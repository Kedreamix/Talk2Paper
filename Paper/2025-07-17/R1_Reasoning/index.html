<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  Comprehension Without Competence Architectural Limits of LLMs in   Symbolic Computation and Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a970e2bbb9edf925c2b94b260c5e7e8d.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-17-æ›´æ–°"><a href="#2025-07-17-æ›´æ–°" class="headerlink" title="2025-07-17 æ›´æ–°"></a>2025-07-17 æ›´æ–°</h1><h2 id="Comprehension-Without-Competence-Architectural-Limits-of-LLMs-in-Symbolic-Computation-and-Reasoning"><a href="#Comprehension-Without-Competence-Architectural-Limits-of-LLMs-in-Symbolic-Computation-and-Reasoning" class="headerlink" title="Comprehension Without Competence: Architectural Limits of LLMs in   Symbolic Computation and Reasoning"></a>Comprehension Without Competence: Architectural Limits of LLMs in   Symbolic Computation and Reasoning</h2><p><strong>Authors:Zheng Zhang</strong></p>
<p>Large Language Models (LLMs) display striking surface fluency yet systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy, and logical consistency. This paper offers a structural diagnosis of such failures, revealing a persistent gap between \textit{comprehension} and \textit{competence}. Through controlled experiments and architectural analysis, we demonstrate that LLMs often articulate correct principles without reliably applying themâ€“a failure rooted not in knowledge access, but in computational execution. We term this phenomenon the computational \textit{split-brain syndrome}, where instruction and action pathways are geometrically and functionally dissociated. This core limitation recurs across domains, from mathematical operations to relational inferences, and explains why model behavior remains brittle even under idealized prompting. We argue that LLMs function as powerful pattern completion engines, but lack the architectural scaffolding for principled, compositional reasoning. Our findings delineate the boundary of current LLM capabilities and motivate future models with metacognitive control, principle lifting, and structurally grounded execution. This diagnosis also clarifies why mechanistic interpretability findings may reflect training-specific pattern coordination rather than universal computational principles, and why the geometric separation between instruction and execution pathways suggests limitations in neural introspection and mechanistic analysis. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¡¨ç°å‡ºæƒŠäººçš„è¡¨é¢æµç•…æ€§ï¼Œä½†åœ¨éœ€è¦ç¬¦å·æ¨ç†ã€ç®—æœ¯å‡†ç¡®æ€§å’Œé€»è¾‘ä¸€è‡´æ€§çš„ä»»åŠ¡ä¸Šå´ç³»ç»Ÿæ€§åœ°å¤±è´¥ã€‚æœ¬æ–‡å¯¹è¿™äº›å¤±è´¥è¿›è¡Œäº†ç»“æ„æ€§è¯Šæ–­ï¼Œæ­ç¤ºäº†â€œç†è§£â€å’Œâ€œèƒ½åŠ›â€ä¹‹é—´çš„æŒä¹…å·®è·ã€‚é€šè¿‡å—æ§å®éªŒå’Œæ¶æ„åˆ†æï¼Œæˆ‘ä»¬è¯æ˜LLMså¾€å¾€é˜è¿°æ­£ç¡®çš„åŸåˆ™ï¼Œä½†å´ä¸èƒ½å¯é åœ°åº”ç”¨å®ƒä»¬â€”â€”è¿™ç§å¤±è´¥å¹¶éæºäºçŸ¥è¯†è·å–ï¼Œè€Œæ˜¯æºäºè®¡ç®—æ‰§è¡Œã€‚æˆ‘ä»¬å°†è¿™ç§ç°è±¡ç§°ä¸ºè®¡ç®—æ€§â€œåˆ†è£‚è„‘ç»¼åˆå¾â€ï¼Œå…¶ä¸­æŒ‡ä»¤å’Œè¡ŒåŠ¨è·¯å¾„åœ¨å‡ ä½•å’ŒåŠŸèƒ½ä¸Šç›¸äº’åˆ†ç¦»ã€‚è¿™ä¸€æ ¸å¿ƒé™åˆ¶åœ¨ä¸åŒçš„é¢†åŸŸä¸­éƒ½å­˜åœ¨ï¼Œä»æ•°å­¦è¿ç®—åˆ°å…³ç³»æ¨ç†ï¼Œå¹¶è§£é‡Šäº†ä¸ºä»€ä¹ˆå³ä½¿åœ¨ç†æƒ³åŒ–çš„æç¤ºä¸‹ï¼Œæ¨¡å‹è¡Œä¸ºä»ç„¶å¾ˆè„†å¼±ã€‚æˆ‘ä»¬è®¤ä¸ºLLMè™½ç„¶å¯ä»¥ä½œä¸ºå¼ºå¤§çš„æ¨¡å¼å®Œæˆå¼•æ“å‘æŒ¥ä½œç”¨ï¼Œä½†ç¼ºä¹æœ‰åŸåˆ™ã€æœ‰ç»„ç»‡çš„æ¨ç†æ‰€éœ€çš„æ¶æ„æ”¯æ’‘ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç•Œå®šäº†å½“å‰LLMçš„èƒ½åŠ›è¾¹ç•Œï¼Œå¹¶æ¿€åŠ±æœªæ¥æ¨¡å‹å…·å¤‡å…ƒè®¤çŸ¥æ§åˆ¶ã€åŸåˆ™æå‡å’Œç»“æ„åŒ–çš„æ‰§è¡Œèƒ½åŠ›ã€‚è¿™ç§è¯Šæ–­è¿˜é˜æ˜äº†ä¸ºä»€ä¹ˆæœºåˆ¶æ€§è§£é‡Šçš„å‘ç°å¯èƒ½åæ˜ è®­ç»ƒç‰¹å®šçš„æ¨¡å¼åè°ƒï¼Œè€Œä¸æ˜¯æ™®éçš„è®¡ç®—åŸç†ï¼Œä»¥åŠä¸ºä»€ä¹ˆæŒ‡ä»¤å’Œæ‰§è¡Œè·¯å¾„ä¹‹é—´çš„å‡ ä½•åˆ†ç¦»è¡¨æ˜ç¥ç»å†…çœå’Œæœºåˆ¶åˆ†æçš„å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10624v1">PDF</a> Substantial change to previous version (experiments, theorem,   analysis and related work); currently under review at TMLR</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è™½ç„¶è¡¨ç°å‡ºæƒŠäººçš„è¡¨é¢æµç•…æ€§ï¼Œä½†åœ¨éœ€è¦ç¬¦å·æ¨ç†ã€ç®—æœ¯å‡†ç¡®æ€§å’Œé€»è¾‘ä¸€è‡´æ€§çš„ä»»åŠ¡ä¸­å´ç³»ç»Ÿæ€§åœ°å¤±è´¥ã€‚æœ¬æ–‡å¯¹å…¶å¤±è´¥åŸå› è¿›è¡Œäº†ç»“æ„æ€§è¯Šæ–­ï¼Œæ­ç¤ºäº†â€œç†è§£â€ä¸â€œèƒ½åŠ›â€ä¹‹é—´çš„æŒä¹…å·®è·ã€‚é€šè¿‡å®éªŒåˆ†æå’Œæ¶æ„è§£æï¼Œæˆ‘ä»¬å‘ç°LLMså¾€å¾€èƒ½æ­£ç¡®é˜è¿°åŸåˆ™ï¼Œä½†å´æ— æ³•å¯é åœ°åº”ç”¨å®ƒä»¬â€”â€”è¿™ç§å¤±è´¥å¹¶éæºäºçŸ¥è¯†è·å–ï¼Œè€Œæ˜¯è®¡ç®—æ‰§è¡Œçš„é—®é¢˜ã€‚æˆ‘ä»¬ç§°è¿™ç§ç°è±¡ä¸ºâ€œè®¡ç®—åˆ†è£‚è„‘ç»¼åˆå¾â€ï¼Œå…¶ä¸­æŒ‡ä»¤å’Œæ‰§è¡Œè·¯å¾„åœ¨å‡ ä½•å’ŒåŠŸèƒ½ä¸Šåˆ†ç¦»ã€‚è¿™ä¸€æ ¸å¿ƒé™åˆ¶åœ¨æ•°å­¦è¿ç®—ã€å…³ç³»æ¨ç†ç­‰é¢†åŸŸä¸­æ™®éå­˜åœ¨ï¼Œè§£é‡Šäº†ä¸ºä»€ä¹ˆå³ä½¿åœ¨ç†æƒ³åŒ–çš„æç¤ºä¸‹ï¼Œæ¨¡å‹è¡Œä¸ºä»ç„¶è„†å¼±ã€‚æœ¬æ–‡è®¤ä¸ºLLMsè™½ç„¶åŠŸèƒ½å¼ºå¤§ï¼Œä½†ç¼ºä¹åŸåˆ™æ€§ã€ç»„åˆæ¨ç†çš„æ¶æ„æ”¯æ’‘ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ˜ç¡®äº†å½“å‰LLMçš„èƒ½åŠ›è¾¹ç•Œï¼Œå¹¶é¼“åŠ±æœªæ¥æ„å»ºå…·æœ‰å…ƒè®¤çŸ¥æ§åˆ¶ã€åŸåˆ™æå‡å’Œç»“æ„åŸºç¡€æ‰§è¡Œçš„æ¨¡å‹ã€‚åŒæ—¶ï¼Œæœ¬æ–‡ä¹Ÿæ¾„æ¸…äº†æœºæ¢°è§£é‡Šæ€§å‘ç°å¯èƒ½åæ˜ è®­ç»ƒç‰¹å®šçš„æ¨¡å¼åè°ƒï¼Œè€Œéæ™®éçš„è®¡ç®—åŸåˆ™ï¼Œä»¥åŠæŒ‡ä»¤ä¸æ‰§è¡Œè·¯å¾„ä¹‹é—´çš„å‡ ä½•åˆ†ç¦»å¯¹ç¥ç»å†…çœå’Œæœºæ¢°åˆ†æçš„é™åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨éœ€è¦ç¬¦å·æ¨ç†ã€ç®—æœ¯å‡†ç¡®æ€§å’Œé€»è¾‘ä¸€è‡´æ€§çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºç³»ç»Ÿæ€§å¤±è´¥ã€‚</li>
<li>LLMsçš„å¤±è´¥æºäºè®¡ç®—æ‰§è¡Œçš„é—®é¢˜ï¼Œè€ŒéçŸ¥è¯†è·å–ã€‚</li>
<li>LLMså¸¸å¸¸èƒ½æ­£ç¡®é˜è¿°åŸåˆ™ï¼Œä½†æ— æ³•å¯é åœ°åº”ç”¨å®ƒä»¬ã€‚</li>
<li>ç°è±¡è¢«ç§°ä¸ºâ€œè®¡ç®—åˆ†è£‚è„‘ç»¼åˆå¾â€ï¼ŒæŒ‡ä»¤å’Œæ‰§è¡Œè·¯å¾„å­˜åœ¨å‡ ä½•å’ŒåŠŸèƒ½ä¸Šçš„åˆ†ç¦»ã€‚</li>
<li>LLMsç¼ºä¹åŸåˆ™æ€§ã€ç»„åˆæ¨ç†çš„æ¶æ„æ”¯æ’‘ã€‚</li>
<li>å½“å‰LLMçš„èƒ½åŠ›è¾¹ç•Œè¢«æ˜ç¡®ï¼Œéœ€è¦æ„å»ºå…·æœ‰å…ƒè®¤çŸ¥æ§åˆ¶ã€åŸåˆ™æå‡å’Œç»“æ„åŸºç¡€æ‰§è¡Œçš„æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10624">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72061717c6ee27bed27e7d7dc3f3152d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Tiny-Reward-Models"><a href="#Tiny-Reward-Models" class="headerlink" title="Tiny Reward Models"></a>Tiny Reward Models</h2><p><strong>Authors:Sarah Pan</strong></p>
<p>Large decoder-based language models have become the dominant architecture for reward modeling in reinforcement learning from human feedback (RLHF). However, as reward models are increasingly deployed in test-time strategies, their inference costs become a growing concern. We present TinyRM, a family of small, bidirectional masked language models (MLMs) with as few as 400 million parameters, that rival the capabilities of models over 175 times larger on reasoning and safety preference modeling tasks. TinyRM combines FLAN-style prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to achieve strong performance on RewardBench, despite using significantly fewer resources. Our experiments suggest that small models benefit from domain-specific tuning strategies, particularly in reasoning, where lightweight finetuning methods are especially effective. While challenges remain in building generalist models and conversational preference modeling, our preliminary results highlight the promise of lightweight bidirectional architectures as efficient, scalable alternatives for preference modeling. </p>
<blockquote>
<p>åŸºäºå¤§å‹è§£ç å™¨çš„è¯­è¨€æ¨¡å‹å·²ç»æˆä¸ºå¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­å¥–åŠ±å»ºæ¨¡çš„ä¸»å¯¼æ¶æ„ã€‚ç„¶è€Œï¼Œéšç€å¥–åŠ±æ¨¡å‹åœ¨æµ‹è¯•æ—¶é—´ç­–ç•¥ä¸­çš„éƒ¨ç½²è¶Šæ¥è¶Šå¤šï¼Œå…¶æ¨ç†æˆæœ¬æˆä¸ºä¸€ä¸ªæ—¥ç›Šå…³æ³¨çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†TinyRMï¼Œè¿™æ˜¯ä¸€ç³»åˆ—å°å‹åŒå‘æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ï¼Œæ‹¥æœ‰ä»…4äº¿ä¸ªå‚æ•°ï¼Œå°±èƒ½åœ¨æ¨ç†å’Œå®‰å…¨åå¥½å»ºæ¨¡ä»»åŠ¡ä¸Šï¼Œä¸è¶…è¿‡å…¶175å€çš„æ¨¡å‹èƒ½åŠ›ç›¸æŠ—è¡¡ã€‚å°½ç®¡TinyRMä½¿ç”¨çš„èµ„æºç›¸å¯¹è¾ƒå°‘ï¼Œä½†å®ƒç»“åˆäº†FLANé£æ ¼çš„æç¤ºã€æ–¹å‘ä½ç§©é€‚åº”ï¼ˆDoRAï¼‰å’Œå±‚å†»ç»“æŠ€æœ¯ï¼Œåœ¨RewardBenchä¸Šå®ç°äº†å¼ºå¤§çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå°æ¨¡å‹å—ç›Šäºç‰¹å®šçš„é¢†åŸŸè°ƒæ•´ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†æ–¹é¢ï¼Œè½»é‡çº§å¾®è°ƒæ–¹æ³•ç‰¹åˆ«æœ‰æ•ˆã€‚è™½ç„¶æ„å»ºé€šç”¨æ¨¡å‹å’Œå¯¹è¯åå¥½å»ºæ¨¡ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†æˆ‘ä»¬çš„åˆæ­¥ç»“æœå‡¸æ˜¾äº†è½»å‹åŒå‘æ¶æ„ä½œä¸ºé«˜æ•ˆã€å¯æ‰©å±•çš„åå¥½å»ºæ¨¡æ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09973v1">PDF</a> 2025 ICML Efficient Systems for Foundation Models Workshop</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§£ç å™¨åŸºç¡€è¯­è¨€æ¨¡å‹å·²æˆä¸ºå¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰ä¸­å¥–åŠ±å»ºæ¨¡çš„ä¸»å¯¼æ¶æ„ã€‚ç„¶è€Œï¼Œéšç€å¥–åŠ±æ¨¡å‹åœ¨æµ‹è¯•æ—¶é—´ç­–ç•¥ä¸­çš„éƒ¨ç½²è¶Šæ¥è¶Šå¤šï¼Œå…¶æ¨ç†æˆæœ¬æˆä¸ºä¸€ä¸ªæ—¥ç›Šå…³æ³¨çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†TinyRMï¼Œè¿™æ˜¯ä¸€ä¸ªå°å‹åŒå‘æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ç³»åˆ—ï¼Œä»…æœ‰4äº¿ä¸ªå‚æ•°ï¼Œåœ¨æ¨ç†å’Œå®‰å…¨åå¥½å»ºæ¨¡ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ä¸è¶…è¿‡å…¶è§„æ¨¡å¤§175å€çš„æ¨¡å‹ç›¸åŒ¹æ•Œã€‚TinyRMç»“åˆäº†FLANé£æ ¼çš„æç¤ºã€æ–¹å‘æ€§ä½ç§©é€‚åº”ï¼ˆDoRAï¼‰å’Œå±‚å†»ç»“ç­‰æŠ€æœ¯ï¼Œåœ¨RewardBenchä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨çš„èµ„æºç›¸å¯¹è¾ƒå°‘ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå°æ¨¡å‹å—ç›Šäºç‰¹å®šçš„é¢†åŸŸè°ƒæ•´ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†æ–¹é¢ï¼Œè½»é‡çº§å¾®è°ƒæ–¹æ³•ç‰¹åˆ«æœ‰æ•ˆã€‚å°½ç®¡åœ¨æ„å»ºé€šç”¨æ¨¡å‹å’Œå¯¹è¯åå¥½å»ºæ¨¡æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†æˆ‘ä»¬çš„åˆæ­¥ç»“æœçªå‡ºäº†è½»é‡çº§åŒå‘æ¶æ„ä½œä¸ºåå¥½å»ºæ¨¡çš„æœ‰æ•ˆã€å¯æ‰©å±•æ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§£ç å™¨åŸºç¡€è¯­è¨€æ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ äººç±»åé¦ˆï¼ˆRLHFï¼‰çš„å¥–åŠ±å»ºæ¨¡ä¸­å æ®ä¸»å¯¼åœ°ä½ã€‚</li>
<li>å¥–åŠ±æ¨¡å‹çš„æ¨ç†æˆæœ¬éšç€å…¶éƒ¨ç½²çš„å¢åŠ è€Œæˆä¸ºå…³æ³¨ç„¦ç‚¹ã€‚</li>
<li>TinyRMæ˜¯ä¸€ä¸ªå°å‹åŒå‘æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMLMï¼‰ï¼Œå‚æ•°å°‘è‡³4äº¿ï¼Œæ€§èƒ½å¼ºå¤§ã€‚</li>
<li>TinyRMåœ¨ReasoningBenchä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨çš„èµ„æºè¾ƒå°‘ã€‚</li>
<li>å°æ¨¡å‹å—ç›Šäºç‰¹å®šçš„é¢†åŸŸè°ƒæ•´ç­–ç•¥ï¼Œè½»é‡çº§å¾®è°ƒæ–¹æ³•ç‰¹åˆ«æœ‰æ•ˆã€‚</li>
<li>TinyRMç»“åˆäº†FLANé£æ ¼çš„æç¤ºã€æ–¹å‘æ€§ä½ç§©é€‚åº”ï¼ˆDoRAï¼‰å’Œå±‚å†»ç»“ç­‰æŠ€æœ¯å®ç°é«˜æ€§èƒ½ã€‚</li>
<li>å°½ç®¡åœ¨æ„å»ºé€šç”¨æ¨¡å‹å’Œå¯¹è¯åå¥½å»ºæ¨¡æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†è½»é‡çº§åŒå‘æ¶æ„å…·æœ‰æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e73b343384be09cb896a3e537b59ccd8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7cae238649e028c96e50734526a107ff.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Is-Human-Written-Data-Enough-The-Challenge-of-Teaching-Reasoning-to-LLMs-Without-RL-or-Distillation"><a href="#Is-Human-Written-Data-Enough-The-Challenge-of-Teaching-Reasoning-to-LLMs-Without-RL-or-Distillation" class="headerlink" title="Is Human-Written Data Enough? The Challenge of Teaching Reasoning to   LLMs Without RL or Distillation"></a>Is Human-Written Data Enough? The Challenge of Teaching Reasoning to   LLMs Without RL or Distillation</h2><p><strong>Authors:Wei Du, Branislav Kisacanin, George Armstrong, Shubham Toshniwal, Ivan Moshkov, Alexan Ayrapetyan, Sadegh Mahdavi, Dan Zhao, Shizhe Diao, Dragan Masulovic, Marius Stanean, Advaith Avadhanam, Max Wang, Ashmit Dutta, Shitij Govil, Sri Yanamandara, Mihir Tandon, Sriram Ananthakrishnan, Vedant Rathi, David Zhang, Joonseok Kang, Leon Luo, Titu Andreescu, Boris Ginsburg, Igor Gitman</strong></p>
<p>Reasoning-capable language models achieve state-of-the-art performance in diverse complex tasks by generating long, explicit Chain-of-Thought (CoT) traces. While recent works show that base models can acquire such reasoning traces via reinforcement learning or distillation from stronger models like DeepSeek-R1, previous works demonstrate that even short CoT prompting without fine-tuning is able to improve reasoning. We ask whether long CoT can be induced in a base model using only prompting or minimal tuning. Using just 20 long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of high-quality examples can unlock strong reasoning capabilities. We further explore using CoT data from non-reasoning models and human annotators, enhanced with prompt engineering, multi-pass editing, and structural guidance. However, neither matches the performance of reasoning model traces, suggesting that certain latent qualities of expert CoT are difficult to replicate. We analyze key properties of reasoning data, such as problem difficulty, diversity, and answer length, that influence reasoning distillation. While challenges remain, we are optimistic that carefully curated human-written CoT, even in small quantities, can activate reasoning behaviors in base models. We release our human-authored dataset across refinement stages and invite further investigation into what makes small-scale reasoning supervision so effective. </p>
<blockquote>
<p>å…·å¤‡æ¨ç†èƒ½åŠ›çš„è¯­è¨€æ¨¡å‹é€šè¿‡ç”Ÿæˆé•¿è€Œæ˜ç¡®çš„æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰è½¨è¿¹ï¼Œåœ¨ä¸åŒå¤æ‚çš„ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºç¡€æ¨¡å‹å¯ä»¥é€šè¿‡å¼ºåŒ–å­¦ä¹ æˆ–ä»æ›´å¼ºæ¨¡å‹ï¼ˆå¦‚DeepSeek-R1ï¼‰çš„è’¸é¦æ¥è·å¾—è¿™ç§æ¨ç†è½¨è¿¹ï¼Œä½†å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿æ²¡æœ‰å¾®è°ƒï¼Œä»…ä»…ä½¿ç”¨çŸ­çš„CoTæç¤ºä¹Ÿèƒ½æé«˜æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬æƒ³çŸ¥é“ï¼Œæ˜¯å¦å¯ä»¥ä½¿ç”¨æç¤ºæˆ–å¾®è°ƒåœ¨åŸºç¡€æ¨¡å‹ä¸­å¼•å‘é•¿CoTã€‚æˆ‘ä»¬ä»…ä½¿ç”¨æ¥è‡ªæ¨ç†æ¨¡å‹QwQ-32B-Previewçš„20ä¸ªé•¿CoTç¤ºä¾‹ï¼Œå¯¹åŸºç¡€æ¨¡å‹Qwen2.5-32Bè¿›è¡Œè½»å¾®å¾®è°ƒã€‚ç»“æœæ¨¡å‹çš„è¡¨ç°è¶…è¿‡äº†æ›´å¤§çš„Qwen2.5-Math-72B-Instructæ¨¡å‹ï¼Œè¿™è¡¨æ˜å°‘é‡é«˜è´¨é‡ç¤ºä¾‹å¯ä»¥è§£é”å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢ä½¿ç”¨éæ¨ç†æ¨¡å‹å’Œäººç±»æ³¨é‡Šè€…çš„CoTæ•°æ®ï¼Œå¹¶é€šè¿‡æç¤ºå·¥ç¨‹ã€å¤šè½®ç¼–è¾‘å’Œç»“æ„æŒ‡å¯¼æ¥å¢å¼ºã€‚ä½†æ˜¯ï¼Œæ— è®ºå“ªç§æ–¹å¼éƒ½æ— æ³•åŒ¹é…æ¨ç†æ¨¡å‹è½¨è¿¹çš„æ€§èƒ½ï¼Œè¿™è¡¨æ˜ä¸“å®¶CoTçš„æŸäº›æ½œåœ¨å“è´¨éš¾ä»¥å¤åˆ¶ã€‚æˆ‘ä»¬åˆ†æäº†å½±å“æ¨ç†è’¸é¦çš„å…³é”®æ¨ç†æ•°æ®å±æ€§ï¼Œå¦‚é—®é¢˜éš¾åº¦ã€å¤šæ ·æ€§å’Œç­”æ¡ˆé•¿åº¦ã€‚è™½ç„¶ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†æˆ‘ä»¬ç›¸ä¿¡ç²¾å¿ƒç­–åˆ’çš„äººç±»ç¼–å†™çš„å°è§„æ¨¡CoTæ•°æ®ä¹Ÿèƒ½æ¿€æ´»åŸºç¡€æ¨¡å‹ä¸­çš„æ¨ç†è¡Œä¸ºã€‚æˆ‘ä»¬å‘å¸ƒäº†å„ä¸ªç²¾ç‚¼é˜¶æ®µçš„äººç±»åˆ›ä½œæ•°æ®é›†ï¼Œå¹¶é‚€è¯·è¿›ä¸€æ­¥ç ”ç©¶å°è§„æ¨¡æ¨ç†ç›‘ç£ä¸ºä½•å¦‚æ­¤æœ‰æ•ˆçš„åŸå› ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09850v2">PDF</a> Accepted at the Second AI for Math Workshop at the 42nd International   Conference on Machine Learning (ICML 2025)</p>
<p><strong>Summary</strong></p>
<p>å¤§è¦æ¨¡èªè¨€æ¨¡å‹åœ¨é€šéç”Ÿæˆé•·ç¯‡çš„ã€é¡¯å¼çš„éˆå¼æ€ç¶­ï¼ˆChain-of-Thoughtï¼ŒCoTï¼‰ç—•è·¡å¾Œï¼Œåœ¨å¤šé‡è¤‡é›œä»»å‹™ä¸­å–å¾—äº†å‰æ²¿çš„è¡¨ç¾ã€‚æœ¬æ–‡æ¢ç´¢äº†æ˜¯å¦åƒ…é€šéæç¤ºæˆ–æœ€å°é™åº¦çš„èª¿æ•´å°±èƒ½åœ¨åŸºç¤æ¨¡å‹ä¸­èª˜å°å‡ºé•·ç¯‡CoTã€‚ä½¿ç”¨ä¾†è‡ªæ¨ç†æ¨¡å‹QwQ-32B-Previewçš„åƒ…20å€‹é•·ç¯‡CoTæ¨£æœ¬ï¼Œæˆ‘å€‘ç¨å¾®èª¿æ•´äº†åŸºç¤æ¨¡å‹Qwen2.5-32Bï¼Œçµæœé¡¯ç¤ºå…¶è¡¨ç¾è¶…è¶Šäº†æ›´å¤§çš„Qwen2.5-Math-72B-Instructæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é‚„æ¢ç´¢äº†ä½¿ç”¨éæ¨ç†æ¨¡å‹å’Œäººé¡æ³¨é‡‹è€…çš„CoTæ•¸æ“šï¼Œé…åˆæç¤ºå·¥ç¨‹ã€å¤šéç·¨è¼¯å’Œçµæ§‹æŒ‡å—ï¼Œä½†æ•ˆæœä»ä¸åŠæ¨ç†æ¨¡å‹ç—•è·¡ã€‚æœ¬æ–‡åˆ†æäº†å½±éŸ¿æ¨ç†ç²¾é¤¾çš„é—œéµå±¬æ€§ï¼Œå¦‚å•é¡Œé›£åº¦ã€å¤šæ¨£æ€§å’Œç­”æ¡ˆé•·åº¦ã€‚æˆ‘å€‘æ¨‚è§€åœ°èªç‚ºï¼Œç²¾å¿ƒç­–åŠƒçš„äººé¡æ›¸å¯«CoTï¼Œå³ä½¿åœ¨å°‘é‡æƒ…æ³ä¸‹ï¼Œä¹Ÿèƒ½å•Ÿå‹•åŸºç¤æ¨¡å‹ä¸­çš„æ¨ç†è¡Œç‚ºã€‚æˆ‘å€‘å…¬é–‹äº†æˆ‘å€‘äººé¡ä½œè€…åœ¨ä¸åŒç²¾è£½éšæ®µæ‰€ç·¨å¯«çš„å¤§å‹æ•¸æ“šé›†ï¼Œä¸¦æ­¡è¿é€²ä¸€æ­¥æ¢ç´¢å°å‹æ¨ç†ç›£ç£å¦‚æ­¤æœ‰æ•ˆçš„å› ç´ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ¨¡å‹é€šéç”Ÿæˆé•·ç¯‡çš„Chain-of-Thoughtï¼ˆCoTï¼‰ç—•è·¡å¯¦ç¾è¤‡é›œä»»å‹™çš„æœ€ä½³æ€§èƒ½ã€‚</li>
<li>åŸºç¤æ¨¡å‹å¯é€šéæç¤ºå’Œå°‘é‡ç²¾ç´°èª¿æ•´ä¾†èª˜å°é•·ç¯‡CoTã€‚</li>
<li>ä½¿ç”¨å°‘é‡é«˜è³ªé‡çš„CoTæ¨£ä¾‹å¯åœ¨åŸºç¤æ¨¡å‹ä¸­è§£é‡‹å¼·æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¾†è‡ªéæ¨ç†æ¨¡å‹å’Œäººé¡æ³¨é‡‹è€…çš„CoTæ•¸æ“šæ•ˆæœè¼ƒå·®ï¼Œæš—ç¤ºå°ˆå®¶CoTçš„æŸäº›æ½›åœ¨ç‰¹è³ªé›£ä»¥è¤‡è£½ã€‚</li>
<li>å•é¡Œé›£åº¦ã€æ•¸æ“šå¤šæ¨£æ€§å’Œç­”æ¡ˆé•·åº¦ç­‰å±¬æ€§å½±éŸ¿æ¨ç†ç²¾é¤¾æ•ˆæœã€‚</li>
<li>ç²¾å¿ƒç­–åŠƒçš„äººé¡æ›¸å¯«CoTå³ä½¿åœ¨å°‘é‡æƒ…æ³ä¸‹ä¹Ÿèƒ½æ¿€æ´»åŸºç¤æ¨¡å‹çš„æ¨ç†è¡Œç‚ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09850">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b3dbfd602dc5f3fc4ebca7d227485fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76d4ba0baebef531ab0f7af930e2b5f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f42dc98c1dc4246e3da28878bf6b7e10.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Rethinking-Prompt-Optimization-Reinforcement-Diversification-and-Migration-in-Blackbox-LLMs"><a href="#Rethinking-Prompt-Optimization-Reinforcement-Diversification-and-Migration-in-Blackbox-LLMs" class="headerlink" title="Rethinking Prompt Optimization: Reinforcement, Diversification, and   Migration in Blackbox LLMs"></a>Rethinking Prompt Optimization: Reinforcement, Diversification, and   Migration in Blackbox LLMs</h2><p><strong>Authors:MohammadReza Davari, Utkarsh Garg, Weixin Cai, Eugene Belilovsky</strong></p>
<p>An increasing number of NLP applications interact with large language models (LLMs) through black-box APIs, making prompt engineering critical for controlling model outputs. While recent Automatic Prompt Optimization (APO) methods iteratively refine prompts using model-generated feedback, textual gradients, they primarily focus on error correction and neglect valuable insights from correct predictions. This limits both their effectiveness and efficiency. In this paper, we propose a novel APO framework centered on enhancing the feedback mechanism. We reinterpret the textual gradient as a form of negative reinforcement and introduce the complementary positive reinforcement to explicitly preserve beneficial prompt components identified through successful predictions. To mitigate the noise inherent in LLM-generated feedback, we introduce a technique called feedback diversification, which aggregates multiple feedback signals, emphasizing consistent, actionable advice while filtering out outliers. Motivated by the rapid evolution and diversity of available LLMs, we also formalize Continual Prompt Optimization (CPO), addressing the practical challenge of efficiently migrating optimized prompts between different model versions or API providers. Our experiments reveal that naive prompt migration often degrades performance due to loss of critical instructions. In contrast, our approach consistently outperforms strong baselines, achieving significant accuracy improvements, faster convergence, and lower computational costs in both standard and migration scenarios. </p>
<blockquote>
<p>éšç€è¶Šæ¥è¶Šå¤šçš„è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨é€šè¿‡é»‘ç®±APIä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œäº¤äº’ï¼Œæç¤ºå·¥ç¨‹å¯¹äºæ§åˆ¶æ¨¡å‹è¾“å‡ºå˜å¾—è‡³å…³é‡è¦ã€‚å°½ç®¡æœ€è¿‘çš„è‡ªåŠ¨æç¤ºä¼˜åŒ–ï¼ˆAPOï¼‰æ–¹æ³•é€šè¿‡æ¨¡å‹ç”Ÿæˆçš„åé¦ˆã€æ–‡æœ¬æ¢¯åº¦æ¥è¿­ä»£ä¼˜åŒ–æç¤ºï¼Œä½†å®ƒä»¬ä¸»è¦å…³æ³¨é”™è¯¯ä¿®æ­£ï¼Œå¿½è§†äº†æ­£ç¡®é¢„æµ‹ä¸­çš„å®è´µè§è§£ã€‚è¿™é™åˆ¶äº†å®ƒä»¬çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥åŠ å¼ºåé¦ˆæœºåˆ¶ä¸ºä¸­å¿ƒçš„æ–°å‹APOæ¡†æ¶ã€‚æˆ‘ä»¬é‡æ–°è§£é‡Šæ–‡æœ¬æ¢¯åº¦ä½œä¸ºä¸€ç§è´Ÿé¢å¼ºåŒ–ï¼Œå¹¶å¼•å…¥è¡¥å……çš„æ­£é¢å¼ºåŒ–æ¥æ˜ç¡®ä¿ç•™é€šè¿‡æˆåŠŸé¢„æµ‹ç¡®å®šçš„æœ‰ç›Šæç¤ºæˆåˆ†ã€‚ä¸ºäº†å‡å°‘LLMç”Ÿæˆçš„åé¦ˆä¸­å›ºæœ‰çš„å™ªå£°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç§°ä¸ºåé¦ˆå¤šæ ·åŒ–çš„æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯èšåˆå¤šä¸ªåé¦ˆä¿¡å·ï¼Œå¼ºè°ƒä¸€è‡´ã€å¯æ“ä½œçš„å»ºè®®ï¼ŒåŒæ—¶è¿‡æ»¤æ‰å¼‚å¸¸å€¼ã€‚å—å¯ç”¨LLMå¿«é€Ÿæ¼”å˜å’Œå¤šæ ·æ€§çš„æ¨åŠ¨ï¼Œæˆ‘ä»¬è¿˜å¯¹æŒç»­æç¤ºä¼˜åŒ–ï¼ˆCPOï¼‰è¿›è¡Œäº†æ­£è§„åŒ–ï¼Œè§£å†³åœ¨ä¸åŒæ¨¡å‹ç‰ˆæœ¬æˆ–APIæä¾›å•†ä¹‹é—´é«˜æ•ˆè¿ç§»ä¼˜åŒ–æç¤ºçš„å®é™…æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œç”±äºå…³é”®æŒ‡ä»¤çš„ä¸¢å¤±ï¼Œç®€å•çš„æç¤ºè¿ç§»å¾€å¾€ä¼šé™ä½æ€§èƒ½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼Œåœ¨æ ‡å‡†å’Œè¿ç§»åœºæ™¯ä¸­å®ç°äº†æ˜¾è‘—çš„å‡†ç¡®æ€§æé«˜ã€æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦å’Œæ›´ä½çš„è®¡ç®—æˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09839v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨NLPåº”ç”¨ä¸­æ‰®æ¼”ç€å…³é”®è§’è‰²ï¼Œå…¶ä¸­é»‘ç®±APIæˆä¸ºäº’åŠ¨çš„å…³é”®ã€‚å½“å‰è‡ªåŠ¨æç¤ºä¼˜åŒ–ï¼ˆAPOï¼‰ä¸»è¦ä¾§é‡äºåŸºäºæ¨¡å‹ç”Ÿæˆåé¦ˆçš„é”™è¯¯æ ¡æ­£ï¼Œè€Œå¿½è§†äº†æ­£ç¡®é¢„æµ‹ä¸­çš„æœ‰ä»·å€¼ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„APOæ¡†æ¶ï¼Œä»¥å¢å¼ºåé¦ˆæœºåˆ¶ä¸ºæ ¸å¿ƒï¼Œå¼•å…¥æ­£é¢å¼ºåŒ–æ¥ä¿ç•™æˆåŠŸé¢„æµ‹ä¸­è¯†åˆ«çš„æœ‰ç›Šæç¤ºæˆåˆ†ã€‚åŒæ—¶ï¼Œä¸ºè§£å†³LLMç”Ÿæˆåé¦ˆä¸­çš„å›ºæœ‰å™ªå£°é—®é¢˜ï¼Œå¼•å…¥åé¦ˆå¤šæ ·åŒ–æŠ€æœ¯ï¼Œèšåˆå¤šä¸ªåé¦ˆä¿¡å·ï¼Œå¼ºè°ƒä¸€è‡´ã€å¯æ“ä½œçš„å»ºè®®ï¼Œè¿‡æ»¤å¼‚å¸¸å€¼ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é’ˆå¯¹ä¸æ–­å‘å±•å’Œå¤šæ ·åŒ–çš„LLMæå‡ºäº†æŒç»­æç¤ºä¼˜åŒ–ï¼ˆCPOï¼‰æ–¹æ³•ï¼Œè§£å†³äº†åœ¨ä¸åŒæ¨¡å‹ç‰ˆæœ¬æˆ–APIæä¾›å•†ä¹‹é—´é«˜æ•ˆè¿ç§»ä¼˜åŒ–æç¤ºçš„å®é™…æŒ‘æˆ˜ã€‚å®éªŒè¯æ˜ï¼Œä¸å¼ºåŸºçº¿ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨æ ‡å‡†å’Œè¿ç§»åœºæ™¯ä¸­å‡å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡é»‘ç®±APIä¸NLPåº”ç”¨äº¤äº’ï¼Œä½¿å¾—æç¤ºå·¥ç¨‹å¯¹äºæ§åˆ¶æ¨¡å‹è¾“å‡ºè‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰è‡ªåŠ¨æç¤ºä¼˜åŒ–ï¼ˆAPOï¼‰ä¸»è¦å…³æ³¨é”™è¯¯æ ¡æ­£ï¼Œå¿½è§†äº†æ­£ç¡®é¢„æµ‹ä¸­çš„æœ‰ä»·å€¼ä¿¡æ¯ã€‚</li>
<li>æ–°å‹APOæ¡†æ¶é€šè¿‡å¢å¼ºåé¦ˆæœºåˆ¶æ¥æé«˜æ€§èƒ½ï¼ŒåŒ…æ‹¬å¼•å…¥æ­£é¢å¼ºåŒ–å’Œåé¦ˆå¤šæ ·åŒ–æŠ€æœ¯ã€‚</li>
<li>åé¦ˆå¤šæ ·åŒ–æŠ€æœ¯èƒ½èšåˆå¤šä¸ªåé¦ˆä¿¡å·ï¼Œå¼ºè°ƒä¸€è‡´ã€å¯æ“ä½œçš„å»ºè®®ï¼Œè¿‡æ»¤å¼‚å¸¸å€¼ã€‚</li>
<li>æå‡ºäº†æŒç»­æç¤ºä¼˜åŒ–ï¼ˆCPOï¼‰æ–¹æ³•ï¼Œä»¥è§£å†³åœ¨ä¸åŒæ¨¡å‹ç‰ˆæœ¬æˆ–APIæä¾›å•†ä¹‹é—´è¿ç§»ä¼˜åŒ–æç¤ºçš„æŒ‘æˆ˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09839">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d6f1c7fcb63c4680db11ac93c705482f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e53a71f33e6da58fc079b6f50429b205.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8d86fb152c7dc9aa1afdcb102fa18a7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="It-Only-Gets-Worse-Revisiting-DL-Based-Vulnerability-Detectors-from-a-Practical-Perspective"><a href="#It-Only-Gets-Worse-Revisiting-DL-Based-Vulnerability-Detectors-from-a-Practical-Perspective" class="headerlink" title="It Only Gets Worse: Revisiting DL-Based Vulnerability Detectors from a   Practical Perspective"></a>It Only Gets Worse: Revisiting DL-Based Vulnerability Detectors from a   Practical Perspective</h2><p><strong>Authors:Yunqian Wang, Xiaohong Li, Yao Zhang, Yuekang Li, Zhiping Zhou, Ruitao Feng</strong></p>
<p>With the growing threat of software vulnerabilities, deep learning (DL)-based detectors have gained popularity for vulnerability detection. However, doubts remain regarding their consistency within declared CWE ranges, real-world effectiveness, and applicability across scenarios. These issues may lead to unreliable detection, high false positives&#x2F;negatives, and poor adaptability to emerging vulnerabilities. A comprehensive analysis is needed to uncover critical factors affecting detection and guide improvements in model design and deployment. In this paper, we present VulTegra, a novel evaluation framework that conducts a multidimensional comparison of scratch-trained and pre-trained-based DL models for vulnerability detection. VulTegra reveals that state-of-the-art (SOTA) detectors still suffer from low consistency, limited real-world capabilities, and scalability challenges. Contrary to common belief, pre-trained models are not consistently better than scratch-trained models but exhibit distinct strengths in specific contexts.Importantly, our study exposes the limitations of relying solely on CWE-based classification and identifies key factors that significantly affect model performance. Experimental results show that adjusting just one such factor consistently improves recall across all seven evaluated detectors, with six also achieving better F1 scores. Our findings provide deeper insights into model behavior and emphasize the need to consider both vulnerability types and inherent code features for effective detection. </p>
<blockquote>
<p>éšç€è½¯ä»¶æ¼æ´å¨èƒçš„ä¸æ–­å¢é•¿ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ£€æµ‹å™¨åœ¨æ¼æ´æ£€æµ‹æ–¹é¢è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚ç„¶è€Œï¼Œå…³äºå…¶åœ¨å£°æ˜çš„CWEèŒƒå›´å†…çš„ä¸€è‡´æ€§ã€ç°å®ä¸–ç•Œçš„æœ‰æ•ˆæ€§ä»¥åŠè·¨åœºæ™¯é€‚ç”¨æ€§çš„ç–‘è™‘ä»ç„¶å­˜åœ¨ã€‚è¿™äº›é—®é¢˜å¯èƒ½å¯¼è‡´æ£€æµ‹ä¸å¯é ã€è¯¯æŠ¥&#x2F;æ¼æŠ¥ç‡é«˜ä»¥åŠé€‚åº”æ–°å…´æ¼æ´çš„èƒ½åŠ›å·®ã€‚ä¸ºäº†æ­ç¤ºå½±å“æ£€æµ‹çš„å…³é”®å› ç´ å¹¶å¼•å¯¼æ¨¡å‹è®¾è®¡å’Œéƒ¨ç½²çš„æ”¹è¿›ï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œå…¨é¢çš„åˆ†æã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†VulTegraï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹è¯„ä¼°æ¡†æ¶ï¼Œå¯ä»¥å¯¹ç”¨äºæ¼æ´æ£€æµ‹çš„ä»å¤´å¼€å§‹è®­ç»ƒå’ŒåŸºäºé¢„è®­ç»ƒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œå¤šç»´æ¯”è¾ƒã€‚VulTegraæ­ç¤ºï¼Œæœ€å…ˆè¿›çš„æ£€æµ‹å™¨ä»ç„¶é¢ä¸´ä¸€è‡´æ€§å·®ã€ç°å®ä¸–ç•Œèƒ½åŠ›æœ‰é™å’Œå¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚ä¸æ™®éä¿¡å¿µç›¸åï¼Œé¢„è®­ç»ƒæ¨¡å‹å¹¶ä¸æ€»æ˜¯æ¯”ä»å¤´å¼€å§‹è®­ç»ƒçš„æ¨¡å‹æ›´å¥½ï¼Œä½†åœ¨ç‰¹å®šä¸Šä¸‹æ–‡ä¸­è¡¨ç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ¿ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†ä»…ä¾èµ–CWEåˆ†ç±»çš„å±€é™æ€§ï¼Œå¹¶ç¡®å®šäº†æ˜¾è‘—å½±å“æ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè°ƒæ•´å…¶ä¸­ä¸€ä¸ªå› ç´ å¯ä»¥ä¸€è‡´åœ°æé«˜æ‰€æœ‰ä¸ƒä¸ªè¯„ä¼°æ£€æµ‹å™¨çš„å¬å›ç‡ï¼Œå…¶ä¸­å…­ä¸ªè¿˜å®ç°äº†æ›´å¥½çš„F1åˆ†æ•°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ·±å…¥äº†è§£æ¨¡å‹è¡Œä¸ºæä¾›äº†æ›´æ·±å…¥çš„è§è§£ï¼Œå¹¶å¼ºè°ƒäº†åœ¨æœ‰æ•ˆæ£€æµ‹ä¸­åŒæ—¶è€ƒè™‘æ¼æ´ç±»å‹å’Œå›ºæœ‰ä»£ç ç‰¹å¾çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09529v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰åœ¨è½¯ä»¶æ¼æ´æ£€æµ‹ä¸­å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†ä»å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå¦‚ä¸€è‡´æ€§ã€çœŸå®åœºæ™¯ä¸­çš„æ•ˆæœåŠè·¨åœºæ™¯é€‚ç”¨æ€§ç­‰ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶VulTegraï¼Œå¯¹æ¯”åˆ†æäº†ä»é›¶å¼€å§‹è®­ç»ƒçš„DLæ¨¡å‹å’ŒåŸºäºé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ£€æµ‹å™¨ä»å­˜åœ¨ä½ä¸€è‡´æ€§ã€æœ‰é™çš„çœŸå®åœºæ™¯èƒ½åŠ›å’Œå¯æ‰©å±•æ€§æŒ‘æˆ˜ã€‚é¢„è®­ç»ƒæ¨¡å‹å¹¶ä¸æ€»æ˜¯ä¼˜äºä»é›¶å¼€å§‹è®­ç»ƒçš„æ¨¡å‹ï¼Œä½†åœ¨ç‰¹å®šåœºæ™¯ä¸‹è¡¨ç°å‡ºä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æŒ‡å‡ºäº†ä»…ä¾èµ–CWEåˆ†ç±»çš„å±€é™æ€§ï¼Œå¹¶ç¡®å®šäº†å½±å“æ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ ã€‚è°ƒæ•´è¿™äº›å› ç´ å¯æ”¹å–„æ£€æµ‹å™¨çš„å¬å›ç‡å’ŒF1åˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨è½¯ä»¶æ¼æ´æ£€æµ‹ä¸­å—åˆ°å…³æ³¨ï¼Œä½†ä»å­˜åœ¨ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>å½“å‰å…ˆè¿›çš„æ£€æµ‹å™¨åœ¨çœŸå®åœºæ™¯åº”ç”¨ä¸­ä»æœ‰é™åˆ¶å’ŒæŒ‘æˆ˜ã€‚</li>
<li>é¢„è®­ç»ƒæ¨¡å‹ä¸ä»é›¶å¼€å§‹è®­ç»ƒçš„æ¨¡å‹å„æœ‰ä¼˜åŠ¿ï¼Œä¸æ€»æ˜¯ä¼˜äºåè€…ã€‚</li>
<li>ä»…ä¾èµ–CWEåˆ†ç±»è¿›è¡Œæ¼æ´æ£€æµ‹å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>è¯†åˆ«å¹¶è°ƒæ•´å½±å“æ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ èƒ½æ˜¾è‘—æé«˜æ£€æµ‹å™¨çš„æ€§èƒ½ã€‚</li>
<li>è°ƒæ•´ç‰¹å®šå› ç´ å¯æ”¹å–„æ‰€æœ‰è¯„ä¼°è¿‡çš„æ£€æµ‹å™¨çš„å¬å›ç‡ï¼Œå¹¶ä½¿å¾—å¤§éƒ¨åˆ†æ£€æµ‹å™¨è·å¾—æ›´é«˜çš„F1åˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09529">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58d394bc5a50e4b8fb26a4a23dd8a497.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-feebec9b6d11a55331a7d61fcd9ffd3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23fa0fd0e26df7e7362f8c8de2d971f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e282f7c0fb9a964a078794e7a072e82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f27d1fb4d2f19e9be6e676ce7c9d51ad.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Prompt4Trust-A-Reinforcement-Learning-Prompt-Augmentation-Framework-for-Clinically-Aligned-Confidence-Calibration-in-Multimodal-Large-Language-Models"><a href="#Prompt4Trust-A-Reinforcement-Learning-Prompt-Augmentation-Framework-for-Clinically-Aligned-Confidence-Calibration-in-Multimodal-Large-Language-Models" class="headerlink" title="Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for   Clinically-Aligned Confidence Calibration in Multimodal Large Language Models"></a>Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for   Clinically-Aligned Confidence Calibration in Multimodal Large Language Models</h2><p><strong>Authors:Anita Kriz, Elizabeth Laura Janes, Xing Shen, Tal Arbel</strong></p>
<p>Multimodal large language models (MLLMs) hold considerable promise for applications in healthcare. However, their deployment in safety-critical settings is hindered by two key limitations: (i) sensitivity to prompt design, and (ii) a tendency to generate incorrect responses with high confidence. As clinicians may rely on a modelâ€™s stated confidence to gauge the reliability of its predictions, it is especially important that when a model expresses high confidence, it is also highly accurate. We introduce Prompt4Trust, the first reinforcement learning (RL) framework for prompt augmentation targeting confidence calibration in MLLMs. A lightweight LLM is trained to produce context-aware auxiliary prompts that guide a downstream task MLLM to generate responses in which the expressed confidence more accurately reflects predictive accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically prioritizes aspects of calibration most critical for safe and trustworthy clinical decision-making. Beyond improvements driven by this clinically motivated calibration objective, our proposed method also improves task accuracy, achieving state-of-the-art medical visual question answering (VQA) performance on the PMC-VQA benchmark, which is composed of multiple-choice questions spanning diverse medical imaging modalities. Moreover, our framework trained with a small downstream task MLLM showed promising zero-shot generalization to larger MLLMs in our experiments, suggesting the potential for scalable calibration without the associated computational costs. This work demonstrates the potential of automated yet human-aligned prompt engineering for improving the the trustworthiness of MLLMs in safety critical settings. Our codebase can be found at <a target="_blank" rel="noopener" href="https://github.com/xingbpshen/prompt4trust">https://github.com/xingbpshen/prompt4trust</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—ä¿å¥é¢†åŸŸçš„åº”ç”¨å‰æ™¯å¹¿é˜”ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å®‰å…¨å…³é”®ç¯å¢ƒä¸­çš„éƒ¨ç½²å—åˆ°ä¸¤ä¸ªä¸»è¦å±€é™çš„é˜»ç¢ï¼šï¼ˆiï¼‰å¯¹æç¤ºè®¾è®¡çš„æ•æ„Ÿæ€§ï¼›ï¼ˆiiï¼‰å€¾å‘äºäº§ç”Ÿé”™è¯¯å“åº”å¹¶å…·æœ‰é«˜ä¿¡å¿ƒã€‚ä¸´åºŠåŒ»ç”Ÿå¯èƒ½ä¼šä¾èµ–æ¨¡å‹æ‰€è¡¨è¾¾çš„ä¿¡å¿ƒæ¥åˆ¤æ–­å…¶é¢„æµ‹çš„å¯ä¿¡åº¦ï¼Œå› æ­¤å½“æ¨¡å‹è¡¨ç°å‡ºé«˜ä¿¡å¿ƒæ—¶ï¼Œä¹Ÿé«˜åº¦å‡†ç¡®å°¤ä¸ºé‡è¦ã€‚æˆ‘ä»¬å¼•å…¥äº†Prompt4Trustï¼Œè¿™æ˜¯é’ˆå¯¹MLLMsä¸­ç½®ä¿¡åº¦æ ¡å‡†çš„ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶çš„æç¤ºå¢å¼ºã€‚ä¸€ä¸ªè½»é‡çº§LLMè¢«è®­ç»ƒæ¥äº§ç”Ÿä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¾…åŠ©æç¤ºï¼Œä»¥æŒ‡å¯¼ä¸‹æ¸¸ä»»åŠ¡MLLMç”Ÿæˆå“åº”ï¼Œå…¶ä¸­è¡¨è¾¾çš„ä¿¡å¿ƒæ›´å‡†ç¡®åœ°åæ˜ äº†é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ä¸ä¼ ç»Ÿçš„æ ¡å‡†æŠ€æœ¯ä¸åŒï¼ŒPrompt4Trustç‰¹åˆ«ä¼˜å…ˆè€ƒè™‘å¯¹å®‰å…¨å’Œå¯ä¿¡ä¸´åºŠå†³ç­–è‡³å…³é‡è¦çš„æ ¡å‡†æ–¹é¢ã€‚é™¤äº†å—æ­¤ä¸´åºŠåŠ¨æœºé©±åŠ¨çš„æ ¡å‡†ç›®æ ‡æ‰€æ¨åŠ¨çš„æ”¹è¿›ä¹‹å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•è¿˜æé«˜äº†ä»»åŠ¡å‡†ç¡®æ€§ï¼Œåœ¨PMC-VQAåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ€§èƒ½ï¼Œè¯¥åŸºå‡†æµ‹è¯•åŒ…å«è·¨è¶Šå¤šç§åŒ»å­¦æˆåƒæ¨¡å¼çš„é€‰æ‹©é¢˜ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä½¿ç”¨å°å‹ä¸‹æ¸¸ä»»åŠ¡MLLMè¿›è¡Œè®­ç»ƒï¼Œåœ¨å®éªŒä¸­æ˜¾ç¤ºå‡ºå¯¹æ›´å¤§çš„MLLMsçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œè¿™è¡¨æ˜äº†å¯æ‰©å±•æ ¡å‡†çš„æ½œåŠ›ï¼Œè€Œæ— éœ€æ‰¿æ‹…ç›¸å…³çš„è®¡ç®—æˆæœ¬ã€‚è¿™é¡¹å·¥ä½œå±•ç¤ºäº†è‡ªåŠ¨åŒ–ä¸”ä¸äººç±»å¯¹é½çš„æç¤ºå·¥ç¨‹åœ¨æé«˜MLLMsåœ¨å®‰å…¨å…³é”®ç¯å¢ƒä¸­çš„å¯ä¿¡åº¦çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xingbpshen/prompt4trust%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xingbpshen/prompt4trustæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09279v2">PDF</a> Accepted to ICCV 2025 Workshop CVAMD</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—ä¿å¥åº”ç”¨ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨å®‰å…¨å…³é”®ç¯å¢ƒä¸­çš„éƒ¨ç½²å—åˆ°ä¸¤ä¸ªä¸»è¦é™åˆ¶ï¼šä¸€æ˜¯å¯¹æç¤ºè®¾è®¡çš„æ•æ„Ÿæ€§å’ŒäºŒæ˜¯æœ‰äº§ç”Ÿé”™è¯¯å“åº”çš„é«˜ä¿¡å¿ƒå€¾å‘ã€‚æ¨¡å‹çš„ä¿¡å¿ƒè¡¨è¾¾å¯¹äºä¸´åºŠåŒ»ç”Ÿè¯„ä¼°é¢„æµ‹å¯é æ€§å°¤ä¸ºé‡è¦ã€‚æˆ‘ä»¬å¼•å…¥Prompt4Trustï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹MLLMsä¸­çš„ä¿¡å¿ƒæ ¡å‡†çš„æç¤ºå¢å¼ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ã€‚ä¸€ä¸ªè½»é‡çº§LLMè¢«è®­ç»ƒæ¥äº§ç”Ÿä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¾…åŠ©æç¤ºï¼Œå¼•å¯¼ä¸‹æ¸¸ä»»åŠ¡MLLMç”Ÿæˆå“åº”ï¼Œå…¶ä¸­è¡¨è¾¾çš„ä¿¡å¿ƒæ›´å‡†ç¡®åœ°åæ˜ äº†é¢„æµ‹å‡†ç¡®æ€§ã€‚ä¸ä¼ ç»Ÿçš„æ ¡å‡†æŠ€æœ¯ä¸åŒï¼ŒPrompt4Trustç‰¹åˆ«ä¼˜å…ˆè€ƒè™‘å¯¹å®‰å…¨å’Œå¯ä¿¡ä¸´åºŠå†³ç­–è‡³å…³é‡è¦çš„æ ¡å‡†æ–¹é¢ã€‚é™¤äº†å—æ­¤ä¸´åºŠåŠ¨æœºé©±åŠ¨çš„æ ¡å‡†ç›®æ ‡çš„æ”¹è¿›ä¹‹å¤–ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•è¿˜æé«˜äº†ä»»åŠ¡å‡†ç¡®æ€§ï¼Œåœ¨PMC-VQAåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„åŒ»å­¦è§†è§‰é—®ç­”æ€§èƒ½ï¼Œè¯¥åŸºå‡†æµ‹è¯•åŒ…å«è·¨è¶Šå¤šç§åŒ»å­¦æˆåƒæ¨¡å¼çš„é€‰æ‹©æ€§é—®é¢˜ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä½¿ç”¨è¾ƒå°çš„ä¸‹æ¸¸ä»»åŠ¡MLLMè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å®éªŒä¸­æ˜¾ç¤ºå‡ºå¯¹æ›´å¤§çš„MLLMsçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œè¿™è¡¨æ˜äº†å¯æ‰©å±•æ ¡å‡†çš„æ½œåŠ›ï¼Œæ— éœ€ç›¸å…³çš„è®¡ç®—æˆæœ¬ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†è‡ªåŠ¨åŒ–ä¸”ç¬¦åˆäººç±»å·¥ç¨‹æç¤ºçš„æ–¹æ³•åœ¨æé«˜MLLMåœ¨å®‰å…¨å…³é”®è®¾ç½®ä¸­çš„å¯ä¿¡åº¦æ–¹é¢çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç åº“ä½äº<a target="_blank" rel="noopener" href="https://github.com/xingbpshen/prompt4trust%E3%80%82">https://github.com/xingbpshen/prompt4trustã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»ç–—ä¿å¥åº”ç”¨ä¸­å…·æœ‰æ½œåŠ›ï¼Œä½†åœ¨å®‰å…¨å…³é”®ç¯å¢ƒä¸­çš„éƒ¨ç½²å­˜åœ¨ä¸¤ä¸ªä¸»è¦é™åˆ¶ï¼šæç¤ºè®¾è®¡æ•æ„Ÿæ€§å’Œé”™è¯¯å“åº”çš„é«˜ä¿¡å¿ƒå€¾å‘ã€‚</li>
<li>Prompt4Trustæ˜¯ç¬¬ä¸€ä¸ªé’ˆå¯¹MLLMsä¸­çš„ä¿¡å¿ƒæ ¡å‡†çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¾…åŠ©æç¤ºæ¥æ”¹å–„æ¨¡å‹çš„ä¿¡å¿ƒè¡¨è¾¾ä¸é¢„æµ‹å‡†ç¡®æ€§çš„åŒ¹é…åº¦ã€‚</li>
<li>ä¸ä¼ ç»Ÿæ ¡å‡†æŠ€æœ¯ä¸åŒï¼ŒPrompt4Trustç‰¹åˆ«å…³æ³¨ä¸´åºŠå†³ç­–ä¸­çš„å…³é”®æ ¡å‡†æ–¹é¢ã€‚</li>
<li>Prompt4Trustä¸ä»…èƒ½æé«˜æ¨¡å‹é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œè€Œä¸”åœ¨PMC-VQAåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†åŒ»å­¦è§†è§‰é—®ç­”çš„æœ€å…ˆè¿›æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶æ˜¾ç¤ºå‡ºå¯¹æ›´å¤§MLLMsçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œè¡¨æ˜æœ‰å¯èƒ½è¿›è¡Œæˆæœ¬æ•ˆç›Šé«˜çš„æ¨¡å‹æ ¡å‡†ã€‚</li>
<li>ä»£ç åº“å…¬å¼€å¯ç”¨ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å’Œåº”ç”¨æä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09279">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-476bd87113d395ce771c785ff3c5dd24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d5be05a95d51ac1fe334bb05d4654f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f199332f67a090057642f81715090d40.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0507327f679f0495f5db304a3ee2145a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Mixture-of-LoRA-Experts-with-Multi-Modal-and-Multi-Granularity-LLM-Generative-Error-Correction-for-Accented-Speech-Recognition"><a href="#Mixture-of-LoRA-Experts-with-Multi-Modal-and-Multi-Granularity-LLM-Generative-Error-Correction-for-Accented-Speech-Recognition" class="headerlink" title="Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM   Generative Error Correction for Accented Speech Recognition"></a>Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM   Generative Error Correction for Accented Speech Recognition</h2><p><strong>Authors:Bingshen Mu, Kun Wei, Pengcheng Guo, Lei Xie</strong></p>
<p>Despite substantial improvements in ASR, performance tends to degrade when faced with adverse conditions such as speaker accents. Generative error correction (GER) leverages the rich linguistic knowledge and exceptional reasoning ability of LLMs, significantly outperforming typical LM methods. However, it lacks specificity in accented speech scenarios. In this study, we leverage GER to improve the accuracy of transcription predictions by addressing the two primary features of accented speech recognition. To fully leverage pronunciation information, we propose the multi-modal GER, which integrates pronunciation information from the speech modality, and the multi-granularity GER, which incorporates fine-grained phoneme-level information related to pronunciation. These two methods enable the LLM to utilize the pronunciation information of accented speech and the semantic information from word-level hypotheses for accurate transcription predictions through LoRA fine-tuning. On the one hand, we employ a three-stage training strategy to train separate multi-modal GER models for each accent to obtain mono-accent LoRA experts. By adopting our proposed HDMoLE method, which incorporates hierarchical routing and dynamic thresholds within the mixture of LoRA experts, we effectively merge multiple mono-accent LoRA experts within a single multi-modal GER to overcome the challenges posed by accent diversity. On the other hand, multi-granularity GER leverages the N-best word-level and phoneme-level hypotheses generated by the HDMoLE model to predict the final accented speech transcriptions. Experimental results on the multi-accent English dataset demonstrate the efficacy of our proposed methods. Our methods achieve a remarkable relative WER reduction of 67.35% compared to the Whisper-large-v3 baseline. </p>
<blockquote>
<p>å°½ç®¡è¯­éŸ³è¯†åˆ«æŠ€æœ¯ï¼ˆASRï¼‰å–å¾—äº†å®è´¨æ€§çš„è¿›æ­¥ï¼Œä½†åœ¨é¢å¯¹å¦‚è¯´è¯è€…å£éŸ³ç­‰ä¸åˆ©æ¡ä»¶æ—¶ï¼Œå…¶æ€§èƒ½å¾€å¾€ä¼šä¸‹é™ã€‚ç”Ÿæˆé”™è¯¯çº æ­£ï¼ˆGERï¼‰èƒ½å¤Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸°å¯Œçš„è¯­è¨€çŸ¥è¯†å’Œå‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºå…¸å‹çš„è¯­è¨€æ¨¡å‹æ–¹æ³•ã€‚ç„¶è€Œï¼Œå®ƒåœ¨å¸¦æœ‰å£éŸ³çš„è¯­éŸ³åœºæ™¯æ–¹é¢ç¼ºä¹ç‰¹å¼‚æ€§ã€‚</p>
</blockquote>
<p>æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨GERæ”¹è¿›äº†è½¬å½•é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œé€šè¿‡è§£å†³å£éŸ³è¯­éŸ³è¯†åˆ«çš„ä¸¤ä¸ªä¸»è¦ç‰¹å¾æ¥å®ç°ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨å‘éŸ³ä¿¡æ¯ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€GERï¼Œå®ƒæ•´åˆäº†è¯­éŸ³æ¨¡æ€çš„å‘éŸ³ä¿¡æ¯ï¼Œä»¥åŠå¤šç²’åº¦GERï¼Œå®ƒç»“åˆäº†ä¸å‘éŸ³ç›¸å…³çš„ç²¾ç»†éŸ³ç´ çº§ä¿¡æ¯ã€‚è¿™ä¸¤ç§æ–¹æ³•ä½¿LLMèƒ½å¤Ÿé€šè¿‡LoRAå¾®è°ƒåˆ©ç”¨å£éŸ³è¯­éŸ³çš„å‘éŸ³ä¿¡æ¯å’Œè¯çº§å‡è®¾çš„è¯­ä¹‰ä¿¡æ¯è¿›è¡Œå‡†ç¡®çš„è½¬å½•é¢„æµ‹ã€‚</p>
<p>ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé’ˆå¯¹æ¯ç§å£éŸ³è®­ç»ƒå•ç‹¬çš„å¤šæ¨¡æ€GERæ¨¡å‹ï¼Œä»¥è·å¾—å•å£éŸ³LoRAä¸“å®¶ã€‚é€šè¿‡é‡‡ç”¨æˆ‘ä»¬æå‡ºçš„HDMoLEæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å±‚æ¬¡è·¯ç”±å’ŒåŠ¨æ€é˜ˆå€¼åœ¨LoRAä¸“å®¶æ··åˆä¸­ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°å°†å¤šä¸ªå•å£éŸ³LoRAä¸“å®¶åˆå¹¶åˆ°ä¸€ä¸ªå•ä¸€çš„å¤šæ¨¡æ€GERä¸­ï¼Œä»¥å…‹æœå£éŸ³å¤šæ ·æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</p>
<p>å¦ä¸€æ–¹é¢ï¼Œå¤šç²’åº¦GERåˆ©ç”¨HDMoLEæ¨¡å‹ç”Ÿæˆçš„Nä¸ªæœ€ä½³è¯çº§å’ŒéŸ³ç´ çº§å‡è®¾æ¥é¢„æµ‹æœ€ç»ˆçš„å£éŸ³è¯­éŸ³è½¬å½•ã€‚åœ¨å¤šå£éŸ³è‹±è¯­æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä¸whisper-large-v3åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†ç›¸å¯¹å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½äº†67.35%ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09116v2">PDF</a> IEEE Transactions on Audio, Speech and Language Processing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¸¦å£éŸ³è¯­éŸ³è¯†åˆ«çš„æ”¹è¿›æ–¹æ³•ã€‚åˆ©ç”¨ç”Ÿæˆå¼é”™è¯¯ä¿®æ­£ï¼ˆGERï¼‰ç»“åˆè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¼˜åŠ¿ï¼Œæå‡ºå¤šæ¨¡æ€GERå’Œå¤šç²’åº¦GERä¸¤ç§æ–¹æ³•ï¼Œä»¥åˆ©ç”¨å£éŸ³è¯­éŸ³çš„å‘éŸ³ä¿¡æ¯å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œæé«˜è½¬å½•é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚é€šè¿‡ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä¸ºæ¯ç§å£éŸ³è®­ç»ƒå•ç‹¬çš„å¤šæ¨¡æ€GERæ¨¡å‹ï¼Œå¹¶é‡‡ç”¨HDMoLEæ–¹æ³•åˆå¹¶å¤šä¸ªå•å£éŸ³æ¨¡å‹ä»¥åº”å¯¹å£éŸ³å¤šæ ·æ€§æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šå£éŸ³è‹±è¯­æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—çš„ç›¸å¯¹å­—è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼é”™è¯¯ä¿®æ­£ï¼ˆGERï¼‰åœ¨è¯­éŸ³è¯†åˆ«ä¸­åˆ©ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸°å¯Œè¯­è¨€çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>é’ˆå¯¹å¸¦å£éŸ³è¯­éŸ³è¯†åˆ«ï¼Œæå‡ºå¤šæ¨¡æ€GERå’Œå¤šç²’åº¦GERä¸¤ç§æ”¹è¿›æ–¹æ³•ã€‚</li>
<li>å¤šæ¨¡æ€GERç»“åˆè¯­éŸ³æ¨¡æ€çš„å‘éŸ³ä¿¡æ¯ï¼Œå¤šç²’åº¦GERåˆ™ç»“åˆç»†ç²’åº¦éŸ³ç´ çº§ä¿¡æ¯ï¼Œä»¥æé«˜è½¬å½•é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä¸ºæ¯ç§å£éŸ³è®­ç»ƒå•ç‹¬çš„å¤šæ¨¡æ€GERæ¨¡å‹ï¼Œå½¢æˆå•å£éŸ³LoRAä¸“å®¶ã€‚</li>
<li>HDMoLEæ–¹æ³•é€šè¿‡å±‚æ¬¡åŒ–è·¯ç”±å’ŒåŠ¨æ€é˜ˆå€¼ï¼Œåœ¨LoRAä¸“å®¶æ··åˆä¸­æœ‰æ•ˆåˆå¹¶å•å£éŸ³æ¨¡å‹ï¼Œåº”å¯¹å£éŸ³å¤šæ ·æ€§ã€‚</li>
<li>å¤šç²’åº¦GERåˆ©ç”¨HDMoLEæ¨¡å‹ç”Ÿæˆçš„N-bestè¯çº§å’ŒéŸ³ç´ çº§å‡è®¾ï¼Œé¢„æµ‹å¸¦å£éŸ³è¯­éŸ³çš„è½¬å½•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09116">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f840240a9d373b5dcad6630a8b1151bf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d51f21573d5ccba195b4e0d0d83ca8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0c9a6a4b9dcab3cbd6bf738e840c195.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b478a7d57278cbad302443f69119351b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="wd1-Weighted-Policy-Optimization-for-Reasoning-in-Diffusion-Language-Models"><a href="#wd1-Weighted-Policy-Optimization-for-Reasoning-in-Diffusion-Language-Models" class="headerlink" title="wd1: Weighted Policy Optimization for Reasoning in Diffusion Language   Models"></a>wd1: Weighted Policy Optimization for Reasoning in Diffusion Language   Models</h2><p><strong>Authors:Xiaohang Tang, Rares Dolga, Sangwoong Yoon, Ilija Bogunovic</strong></p>
<p>Improving the reasoning capabilities of diffusion-based large language models (dLLMs) through reinforcement learning (RL) remains an open problem. The intractability of dLLMs likelihood function necessitates approximating the current, old, and reference policy likelihoods at each policy optimization step. This reliance introduces additional computational overhead and lead to potentially large bias â€“ particularly when approximation errors occur in the denominator of policy ratios used for importance sampling. To mitigate these issues, we introduce $\mathtt{wd1}$, a novel policy optimization approach that reformulates the objective as a weighted likelihood, requiring only a single approximation for the current parametrized policy likelihood. Experiments on widely used reasoning benchmarks demonstrate that $\mathtt{wd1}$, without supervised fine-tuning (SFT) or any supervised data, outperforms existing RL methods for dLLMs, achieving up to 16% higher accuracy. $\mathtt{wd1}$ delivers additional computational gains, including reduced training time and fewer function evaluations (NFEs) per gradient step. These findings, combined with the simplicity of methodâ€™s implementation and R1-Zero-like training (no SFT), position $\mathtt{wd1}$ as a more effective and efficient method for applying RL to dLLMs reasoning. </p>
<blockquote>
<p>é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æé«˜åŸºäºæ‰©æ•£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMï¼‰çš„æ¨ç†èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚dLLMçš„ä¼¼ç„¶å‡½æ•°çš„å¤æ‚æ€§è¦æ±‚åœ¨æ¯æ¬¡ç­–ç•¥ä¼˜åŒ–æ­¥éª¤ä¸­è¿‘ä¼¼å½“å‰ã€æ—§çš„å’Œå‚è€ƒç­–ç•¥çš„ä¼¼ç„¶å‡½æ•°ã€‚è¿™ç§ä¾èµ–å¼•å…¥äº†é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œå¹¶å¯èƒ½å¯¼è‡´è¾ƒå¤§çš„åå·®ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”¨äºé‡è¦æ€§é‡‡æ ·çš„ç­–ç•¥æ¯”å€¼çš„åˆ†æ¯ä¸­å‡ºç°è¿‘ä¼¼è¯¯å·®æ—¶ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†$\mathtt{wd1}$ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œå®ƒå°†ç›®æ ‡é‡æ–°å®šä¹‰ä¸ºåŠ æƒä¼¼ç„¶ï¼Œåªéœ€è¦å¯¹å½“å‰å‚æ•°åŒ–ç­–ç•¥ä¼¼ç„¶è¿›è¡Œä¸€æ¬¡è¿‘ä¼¼ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œ$\mathtt{wd1}$åœ¨ä¸è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ä½¿ç”¨ä»»ä½•ç›‘ç£æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¼˜äºç°æœ‰çš„dLLMçš„RLæ–¹æ³•ï¼Œå‡†ç¡®ç‡æé«˜äº†é«˜è¾¾16%ã€‚$\mathtt{wd1}$è¿˜å¸¦æ¥äº†é¢å¤–çš„è®¡ç®—æ”¶ç›Šï¼ŒåŒ…æ‹¬å‡å°‘è®­ç»ƒæ—¶é—´å’Œæ¯ä¸ªæ¢¯åº¦æ­¥éª¤å‡å°‘å‡½æ•°è¯„ä¼°æ¬¡æ•°ï¼ˆNFEsï¼‰ã€‚è¿™äº›å‘ç°ï¼ŒåŠ ä¸Šæ–¹æ³•å®ç°çš„ç®€å•æ€§å’ŒR1-Zero-likeè®­ç»ƒï¼ˆæ— SFTï¼‰ï¼Œä½¿$\mathtt{wd1}$æˆä¸ºå°†RLåº”ç”¨äºdLLMæ¨ç†çš„æ›´æœ‰æ•ˆå’Œæ›´é«˜æ•ˆçš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08838v1">PDF</a> Preprint</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æ”¹å–„åŸºäºæ‰©æ•£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•wd1ï¼Œé€šè¿‡åŠ æƒä¼¼ç„¶é‡æ–°å®šä¹‰äº†ç›®æ ‡ï¼Œåªéœ€å¯¹å½“å‰å‚æ•°åŒ–ç­–ç•¥ä¼¼ç„¶è¿›è¡Œä¸€æ¬¡è¿‘ä¼¼ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ä»»ä½•ç›‘ç£æ•°æ®çš„æƒ…å†µä¸‹ï¼Œwd1ä¼˜äºç°æœ‰çš„dLLMå¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œå‡†ç¡®ç‡æé«˜è¾¾16%ã€‚æ­¤å¤–ï¼Œwd1è¿˜å…·æœ‰è®¡ç®—æ•ˆç›Šï¼ŒåŒ…æ‹¬ç¼©çŸ­è®­ç»ƒæ—¶é—´å’Œå‡å°‘æ¯æ¢¯åº¦æ­¥æ•°çš„å‡½æ•°è¯„ä¼°æ¬¡æ•°ã€‚å› æ­¤ï¼Œwd1åœ¨åº”ç”¨äºdLLMæ¨ç†çš„RLæ–¹æ³•ä¸­æ›´å…·æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æ”¹å–„åŸºäºæ‰©æ•£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰dLLMçš„ä¼¼ç„¶å‡½æ•°ä¸å¯è¡Œæ€§å¯¼è‡´éœ€è¦åœ¨ç­–ç•¥ä¼˜åŒ–æ­¥éª¤ä¸­å¯¹å½“å‰ã€æ—§å’Œå‚è€ƒç­–ç•¥ä¼¼ç„¶è¿›è¡Œè¿‘ä¼¼ã€‚</li>
<li>è¿‘ä¼¼è¿‡ç¨‹å¼•å…¥è®¡ç®—è´Ÿæ‹…å’Œåå·®é£é™©ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”¨äºé‡è¦æ€§é‡‡æ ·çš„ç­–ç•¥æ¯”ç‡åˆ†æ¯ä¸­å‡ºç°è¿‘ä¼¼è¯¯å·®æ—¶ã€‚</li>
<li>wd1æ˜¯ä¸€ç§æ–°çš„ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡åŠ æƒä¼¼ç„¶é‡æ–°å®šä¹‰äº†ç›®æ ‡ï¼Œç®€åŒ–äº†è®¡ç®—è¿‡ç¨‹ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œwd1åœ¨ä¸ä½¿ç”¨æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ç›‘ç£æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä¼˜äºç°æœ‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæé«˜äº†dLLMçš„æ¨ç†å‡†ç¡®æ€§ï¼Œæœ€é«˜å¯è¾¾16%ã€‚</li>
<li>wd1å…·æœ‰è®¡ç®—æ•ˆç›Šï¼ŒåŒ…æ‹¬ç¼©çŸ­è®­ç»ƒæ—¶é—´å’Œå‡å°‘æ¯æ¢¯åº¦æ­¥æ•°çš„å‡½æ•°è¯„ä¼°æ¬¡æ•°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8980195320cb736c1c13fbca51719937.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="KAT-V1-Kwai-AutoThink-Technical-Report"><a href="#KAT-V1-Kwai-AutoThink-Technical-Report" class="headerlink" title="KAT-V1: Kwai-AutoThink Technical Report"></a>KAT-V1: Kwai-AutoThink Technical Report</h2><p><strong>Authors:Zizheng Zhan, Ken Deng, Huaixi Tang, Wen Xiang, Kun Wu, Weihao Li, Wenqiang Zhu, Jingxuan Xu, Lecheng Huang, Zongxian Feng, Shaojie Wang, Shangpeng Yan, Xuxing Chen, Jiaheng Liu, Zhongyuan Peng, Zuchen Gao, Haoyang Huang, Xiaojiang Zhang, Jinghui Wang, Zheng Lin, Mengtong Li, Huiming Wang, Ziqi Zhan, Yanan Wu, Yuanxing Zhang, Jian Yang, Guang Chen, Haotian Zhang, Bin Chen, Bing Yu</strong></p>
<p>We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model developed to address the overthinking problem in reasoning-intensive tasks, where an automatic thinking training paradigm is proposed to dynamically switch between reasoning and non-reasoning modes based on task complexity. Specifically, first, we construct the dual-regime dataset based on a novel tagging pipeline and a multi-agent synthesis strategy, and then we apply Multi-Token Prediction (MTP)-enhanced knowledge distillation, enabling efficient and fine-grained reasoning transfer with minimal pretraining cost. Besides, we implement a cold-start initialization strategy that introduces mode-selection priors using majority-vote signals and intent-aware prompting. Finally, we propose Step-SRPO, a reinforcement learning algorithm that incorporates intermediate supervision into the GRPO framework, offering structured guidance over both reasoning-mode selection and response accuracy. Extensive experiments across multiple benchmarks demonstrate that KAT consistently matches or even outperforms current state-of-the-art models, including DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of reasoning-intensive tasks while reducing token usage by up to approximately 30%. Beyond academic evaluation, KAT has been successfully deployed in Kwaipilot (i.e., Kuaishouâ€™s internal coding assistant), and improves real-world development workflows with high accuracy, efficiency, and controllable reasoning behaviors. Moreover, we are actively training a 200B Mixture-of-Experts (MoE) with 40B activation parameters, where the early-stage results already demonstrate promising improvements in performance and efficiency, further showing the scalability of the AutoThink paradigm. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Kwaipilot-AutoThinkï¼ˆKATï¼‰é¡¹ç›®ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è§£å†³æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­è¿‡åº¦æ€è€ƒé—®é¢˜çš„å¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…¶å…·å¤‡åŸºäºä»»åŠ¡å¤æ‚åº¦åŠ¨æ€åœ¨æ¨ç†æ¨¡å¼å’Œéæ¨ç†æ¨¡å¼é—´åˆ‡æ¢çš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ ¹æ®æ–°çš„æ ‡è®°ç®¡é“å’Œå¤šæ™ºèƒ½ä½“åˆæˆç­–ç•¥æ„å»ºåŒæ¨¡å¼æ•°æ®é›†ï¼Œç„¶ååº”ç”¨å¢å¼ºçŸ¥è¯†è’¸é¦çš„å¤šä»¤ç‰Œé¢„æµ‹ï¼ˆMTPï¼‰ï¼Œä»¥æœ€å°çš„é¢„è®­ç»ƒæˆæœ¬å®ç°é«˜æ•ˆç²¾ç»†æ¨ç†è¿ç§»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å®ç°äº†å†·å¯åŠ¨åˆå§‹åŒ–ç­–ç•¥ï¼Œé€šè¿‡å¤šæ•°æŠ•ç¥¨ä¿¡å·å’Œæ„å›¾æ„ŸçŸ¥æç¤ºå¼•å…¥æ¨¡å¼é€‰æ‹©å…ˆéªŒã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†èå…¥ä¸­é—´ç›‘ç£çš„Step-SRPOå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œåœ¨GRPOæ¡†æ¶å†…ä¸ºæ¨ç†æ¨¡å¼é€‰æ‹©å’Œå“åº”å‡†ç¡®æ€§æä¾›ç»“æ„åŒ–æŒ‡å¯¼ã€‚åœ¨å¤šåŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒKATåœ¨å¹¿æ³›çš„æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç”šè‡³è¶…è¿‡å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ˆåŒ…æ‹¬DeepSeek-R1-0528å’ŒQwen3-235B-A22Bï¼‰ï¼ŒåŒæ—¶å‡å°‘é«˜è¾¾çº¦30%çš„ä»¤ç‰Œä½¿ç”¨ã€‚é™¤äº†å­¦æœ¯è¯„ä¼°å¤–ï¼ŒKATå·²åœ¨Kwaipilotï¼ˆå³å¿«æ‰‹å†…éƒ¨ç¼–ç åŠ©æ‰‹ï¼‰ä¸­æˆåŠŸéƒ¨ç½²ï¼Œä»¥é«˜å‡†ç¡®æ€§ã€é«˜æ•ˆç‡å’Œæ§åˆ¶æ¨ç†è¡Œä¸ºçš„æ–¹å¼æ”¹è¿›äº†ç°å®ä¸–ç•Œå¼€å‘å·¥ä½œæµç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ­£åœ¨ç§¯æè®­ç»ƒä¸€ä¸ªæ‹¥æœ‰æ¿€æ´»å‚æ•°è¾¾200Bçš„ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰ï¼Œæ—©æœŸé˜¶æ®µçš„ç»“æœå·²ç»æ˜¾ç¤ºå‡ºåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¸Šçš„æå‡æ‰¿è¯ºï¼Œè¿™è¿›ä¸€æ­¥æ˜¾ç¤ºäº†AutoThinkèŒƒå¼çš„å¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08297v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ±Ÿå±±æä¾›äº†ä¸€é¡¹é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„å¼€æ”¾æºä»£ç ç ”ç©¶ï¼Œåä¸ºKwaipilot-AutoThinkï¼ˆKATï¼‰ã€‚è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸­çš„è¿‡åº¦æ€è€ƒé—®é¢˜ã€‚KATé‡‡ç”¨è‡ªåŠ¨æ€è€ƒè®­ç»ƒæ¨¡å¼ï¼Œèƒ½æ ¹æ®ä»»åŠ¡å¤æ‚æ€§åœ¨æ¨ç†ä¸éæ¨ç†æ¨¡å¼ä¹‹é—´åŠ¨æ€åˆ‡æ¢ã€‚é€šè¿‡ä½¿ç”¨å¤šç§æŠ€æœ¯å’Œç­–ç•¥æ„å»ºåŒæ€æ•°æ®é›†å’Œå¤šä»¤åŒ–å…ƒåˆæˆç­–ç•¥çš„æ–°é¢–æ ‡è®°ç®¡é“ï¼Œä»¥åŠå¢å¼ºçš„çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼ˆå¤šä»¤ç‰Œé¢„æµ‹ï¼‰ï¼Œä½¿æ¨¡å‹èƒ½åœ¨ä¿æŒé«˜æ•ˆçš„åŒæ—¶è¿›è¡Œç²¾ç»†æ¨ç†ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å†·å¯åŠ¨åˆå§‹åŒ–ç­–ç•¥ï¼Œé€šè¿‡å¤šæ•°æŠ•ç¥¨ä¿¡å·å’Œæ„å›¾æ„ŸçŸ¥æç¤ºå¼•å…¥æ¨¡å¼é€‰æ‹©å…ˆéªŒã€‚æœ€åæå‡ºäº†ä¸€ç§èåˆå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆæ­¥éª¤-SRPOï¼‰è¿›è¡Œä¸­é—´ç›‘ç£çš„è¾…åŠ©æ§åˆ¶ç¨‹åºè§„åˆ’ä¼˜åŒ–ç®—æ³•ã€‚è¯¥ç ”ç©¶å‡å°‘äº†é¢„å¤„ç†æˆæœ¬çš„åŒæ—¶æå‡äº†å¯¹å¤šç§åŸºå‡†æµ‹è¯•çš„æ¨ç†æ€§èƒ½ï¼Œå¹¶ä¸”å·²ç»æˆåŠŸéƒ¨ç½²åœ¨Kwaipilotï¼ˆå¿«æ‰‹å†…éƒ¨ç¼–ç åŠ©æ‰‹ï¼‰ä¸­ã€‚è¯¥ç ”ç©¶è¿›ä¸€æ­¥å±•æœ›äº†è§„æ¨¡åŒ–çš„è‡ªåŠ¨æ€è€ƒæ¨¡å¼ï¼Œå¹¶æ­£åœ¨ç§¯æè®­ç»ƒä¸€ä¸ªæ··åˆä¸“å®¶æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>KATæ˜¯ä¸€ä¸ªé’ˆå¯¹æ¨ç†å¯†é›†å‹ä»»åŠ¡çš„å¼€æ”¾æºä»£ç å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨è‡ªåŠ¨æ€è€ƒè®­ç»ƒæ¨¡å¼ï¼Œæ ¹æ®ä»»åŠ¡å¤æ‚æ€§åŠ¨æ€åˆ‡æ¢æ¨ç†å’Œéæ¨ç†æ¨¡å¼ã€‚</li>
<li>é€šè¿‡åŒæ€æ•°æ®é›†ã€çŸ¥è¯†è’¸é¦æŠ€æœ¯å’Œå¤šä»¤åŒ–å…ƒåˆæˆç­–ç•¥æ¥æå‡æ¨¡å‹çš„æ¨ç†æ•ˆç‡ã€‚</li>
<li>é‡‡ç”¨å†·å¯åŠ¨åˆå§‹åŒ–ç­–ç•¥ï¼Œé€šè¿‡å¤šæ•°æŠ•ç¥¨ä¿¡å·å’Œæ„å›¾æ„ŸçŸ¥æç¤ºå®ç°æ¨¡å¼é€‰æ‹©å…ˆéªŒã€‚</li>
<li>æå‡ºèåˆå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ä¸­é—´ç›‘ç£è¾…åŠ©æ§åˆ¶ç¨‹åºè§„åˆ’ä¼˜åŒ–ç®—æ³•ï¼ˆStep-SRPOï¼‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08297">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d8a0a9e44ae59d41cf6107dadce67ac.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-444ce838166649257f8fabdaad9bdff6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abda2051a7c393becccd43e06eb39869.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-284b1b56022a12b08884e752f15ae756.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a7e7dc4fb8a22cb2e53c8c4528e3b19.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SpatialViz-Bench-Automatically-Generated-Spatial-Visualization-Reasoning-Tasks-for-MLLMs"><a href="#SpatialViz-Bench-Automatically-Generated-Spatial-Visualization-Reasoning-Tasks-for-MLLMs" class="headerlink" title="SpatialViz-Bench: Automatically Generated Spatial Visualization   Reasoning Tasks for MLLMs"></a>SpatialViz-Bench: Automatically Generated Spatial Visualization   Reasoning Tasks for MLLMs</h2><p><strong>Authors:Siting Wang, Luoyang Sun, Cheng Deng, Kun Shao, Minnan Pei, Zheng Tian, Haifeng Zhang, Jun Wang</strong></p>
<p>Humans can directly imagine and manipulate visual images in their minds, a capability known as spatial visualization. While multi-modal Large Language Models (MLLMs) support imagination-based reasoning, spatial visualization remains insufficiently evaluated, typically embedded within broader mathematical and logical assessments. Existing evaluations often rely on IQ tests or math competitions that may overlap with training data, compromising assessment reliability. To this end, we introduce SpatialViz-Bench, a comprehensive multi-modal benchmark for spatial visualization with 12 tasks across 4 sub-abilities, comprising 1,180 automatically generated problems. Our evaluation of 33 state-of-the-art MLLMs not only reveals wide performance variations and demonstrates the benchmarkâ€™s strong discriminative power, but also uncovers counter-intuitive findings: models exhibit unexpected behaviors by showing difficulty perception that misaligns with human intuition, displaying dramatic 2D-to-3D performance cliffs, and defaulting to formula derivation despite spatial tasks requiring visualization alone. SpatialVizBench empirically demonstrates that state-of-the-art MLLMs continue to exhibit deficiencies in spatial visualization tasks, thereby addressing a significant lacuna in the field. The benchmark is publicly available. </p>
<blockquote>
<p>äººç±»èƒ½å¤Ÿåœ¨å¤§è„‘ä¸­ç›´æ¥æƒ³è±¡å¹¶æ“ä½œè§†è§‰å›¾åƒï¼Œè¿™ç§èƒ½åŠ›è¢«ç§°ä¸ºç©ºé—´å¯è§†åŒ–ã€‚è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ”¯æŒåŸºäºæƒ³è±¡çš„æ¨ç†ï¼Œä½†ç©ºé—´å¯è§†åŒ–èƒ½åŠ›çš„è¯„ä¼°ä»ç„¶ä¸è¶³ï¼Œé€šå¸¸åªåµŒå…¥æ›´å¹¿æ³›çš„æ•°å­¦å’Œé€»è¾‘è¯„ä¼°ä¸­ã€‚ç°æœ‰çš„è¯„ä¼°é€šå¸¸ä¾èµ–äºå¯èƒ½ä¸è®­ç»ƒæ•°æ®é‡å çš„æ™ºå•†æµ‹è¯•æˆ–æ•°å­¦ç«èµ›ï¼Œä»è€Œå½±å“è¯„ä¼°çš„å¯é æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpatialViz-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€ç©ºé—´å¯è§†åŒ–åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«12é¡¹ä»»åŠ¡ï¼Œè·¨è¶Š4ç§å­èƒ½åŠ›ï¼Œå…±æœ‰1180ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„é—®é¢˜ã€‚æˆ‘ä»¬å¯¹33ç§æœ€æ–°é¢–çš„MLLMsçš„è¯„ä¼°ä¸ä»…æ­ç¤ºäº†å¹¿æ³›çš„æ€§èƒ½å·®å¼‚ï¼Œå¹¶è¯æ˜äº†è¯¥åŸºå‡†æµ‹è¯•çš„å¼ºåŠ²é‰´åˆ«åŠ›ï¼Œè€Œä¸”è¿˜å‘ç°äº†å‡ºäººæ„æ–™çš„å‘ç°ï¼šæ¨¡å‹è¡¨ç°å‡ºéš¾ä»¥æ„ŸçŸ¥çš„æ„å¤–è¡Œä¸ºï¼Œä¸äººç±»ç›´è§‰ä¸ä¸€è‡´ï¼Œåœ¨äºŒç»´åˆ°ä¸‰ç»´çš„æ€§èƒ½ä¸Šæœ‰å·¨å¤§å·®å¼‚ï¼Œå¹¶ä¸”å³ä½¿åœ¨åªéœ€è¦å¯è§†åŒ–è€Œä¸éœ€è¦å…¬å¼æ¨å¯¼çš„ç©ºé—´ä»»åŠ¡ä¸­ï¼Œä¹Ÿé»˜è®¤é€‰æ‹©å…¬å¼æ¨å¯¼ã€‚SpatialVizBenchå®è¯è¡¨æ˜ï¼Œæœ€æ–°é¢–çš„MLLMsåœ¨ç©ºé—´å¯è§†åŒ–ä»»åŠ¡ä¸­ä»å­˜åœ¨ç¼ºé™·ï¼Œä»è€Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„é‡å¤§ç©ºç™½ã€‚è¯¥åŸºå‡†æµ‹è¯•å¯å…¬å¼€è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07610v2">PDF</a> </p>
<p><strong>Summary</strong><br>ç©ºé—´å¯è§†åŒ–æ˜¯äººç±»èƒ½å¤Ÿåœ¨è„‘æµ·ä¸­ç›´æ¥æƒ³è±¡å’Œæ“ä½œè§†è§‰å›¾åƒçš„èƒ½åŠ›ã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ”¯æŒåŸºäºæƒ³è±¡çš„æ¨ç†ï¼Œä½†å¯¹ç©ºé—´å¯è§†åŒ–çš„è¯„ä¼°ä»ç„¶ä¸è¶³ï¼Œé€šå¸¸åµŒå…¥åœ¨æ›´å¹¿æ³›çš„æ•°å­¦å’Œé€»è¾‘è¯„ä¼°ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†SpatialViz-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€ç©ºé—´å¯è§†åŒ–åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«12é¡¹ä»»åŠ¡ï¼Œæ¶µç›–4ç§å­èƒ½åŠ›ï¼Œå…±æœ‰1180ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„é¢˜ç›®ã€‚æˆ‘ä»¬å¯¹33ä¸ªæœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä»·æ­ç¤ºäº†å¹¿æ³›çš„æ€§èƒ½å·®å¼‚ï¼Œè¯æ˜äº†åŸºå‡†æµ‹è¯•çš„å¼ºé‰´åˆ«åŠ›ï¼Œå¹¶å‘ç°äº†ä»¤äººå›°æƒ‘çš„ç»“æœï¼šæ¨¡å‹åœ¨æ„ŸçŸ¥æ–¹é¢è¡¨ç°å‡ºå›°éš¾ï¼Œä¸äººç±»ç›´è§‰ä¸ç¬¦ï¼ŒäºŒç»´åˆ°ä¸‰ç»´æ€§èƒ½ä¸‹é™æ˜æ˜¾ï¼Œå¹¶ä¸”åœ¨éœ€è¦ä»…ä½¿ç”¨å¯è§†åŒ–çš„ä»»åŠ¡æ—¶é»˜è®¤ä½¿ç”¨å…¬å¼æ¨å¯¼ã€‚SpatialVizBenchå®è¯è¡¨æ˜ï¼Œæœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç©ºé—´å¯è§†åŒ–ä»»åŠ¡ä¸Šä»æœ‰ç¼ºé™·ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç©ºç™½ã€‚è¯¥åŸºå‡†æµ‹è¯•å·²å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»å…·å¤‡ç©ºé—´å¯è§†åŒ–èƒ½åŠ›ï¼Œèƒ½ç›´æ¥æƒ³è±¡å’Œæ“ä½œè§†è§‰å›¾åƒã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ”¯æŒåŸºäºæƒ³è±¡çš„æ¨ç†ï¼Œä½†å¯¹ç©ºé—´å¯è§†åŒ–çš„è¯„ä¼°ä»ç„¶ä¸è¶³ã€‚</li>
<li>SpatialViz-Benchæ˜¯ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€ç©ºé—´å¯è§†åŒ–åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å¤šä¸ªä»»åŠ¡å’Œå­èƒ½åŠ›ã€‚</li>
<li>å¯¹æœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹çš„è¯„ä»·æ­ç¤ºäº†å¹¿æ³›çš„æ€§èƒ½å·®å¼‚å’Œå¼ºå¤§çš„åŸºå‡†æµ‹è¯•é‰´åˆ«åŠ›ã€‚</li>
<li>æ¨¡å‹åœ¨æ„ŸçŸ¥æ–¹é¢è¡¨ç°å‡ºå›°éš¾ï¼Œå­˜åœ¨äºŒç»´åˆ°ä¸‰ç»´æ€§èƒ½æ‚¬å´–ã€‚</li>
<li>æ¨¡å‹åœ¨éœ€è¦ä»…ä½¿ç”¨å¯è§†åŒ–çš„ä»»åŠ¡æ—¶é»˜è®¤ä½¿ç”¨å…¬å¼æ¨å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07610">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ff5d3b49d013f28d26fd4dd82993018b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64ec35a3c3008d1c27862e904eff0e53.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-553d800ac6766443411e90f9b8627746.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-028d2e8ac30eb50ba0c8537ec1524339.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e2bd081953150145f5899a10cd413d5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8b19d5a9b51c587893013e9c9cf3d3b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Perception-Aware-Policy-Optimization-for-Multimodal-Reasoning"><a href="#Perception-Aware-Policy-Optimization-for-Multimodal-Reasoning" class="headerlink" title="Perception-Aware Policy Optimization for Multimodal Reasoning"></a>Perception-Aware Policy Optimization for Multimodal Reasoning</h2><p><strong>Authors:Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, Heng Ji</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a highly effective strategy for endowing Large Language Models (LLMs) with robust multi-step reasoning abilities. However, its design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks. In particular, we observe that a major source of error in current multimodal reasoning lies in the perception of visual inputs. To address this bottleneck, we propose Perception-Aware Policy Optimization (PAPO), a simple yet effective extension of GRPO that encourages the model to learn to perceive while learning to reason, entirely from internal supervision signals. Notably, PAPO does not rely on additional data curation, external reward models, or proprietary models. Specifically, we introduce the Implicit Perception Loss in the form of a KL divergence term to the GRPO objective, which, despite its simplicity, yields significant overall improvements (4.4%) on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%, on tasks with high vision dependency. We also observe a substantial reduction (30.5%) in perception errors, indicating improved perceptual capabilities with PAPO. We conduct comprehensive analysis of PAPO and identify a unique loss hacking issue, which we rigorously analyze and mitigate through a Double Entropy Loss. Overall, our work introduces a deeper integration of perception-aware supervision into RLVR learning objectives and lays the groundwork for a new RL framework that encourages visually grounded reasoning. Project page: <a target="_blank" rel="noopener" href="https://mikewangwzhl.github.io/PAPO">https://mikewangwzhl.github.io/PAPO</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²è¢«è¯æ˜æ˜¯èµ‹äºˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¼ºå¤§çš„å¤šæ­¥æ¨ç†èƒ½åŠ›çš„ä¸€ç§é«˜æ•ˆç­–ç•¥ã€‚ç„¶è€Œï¼Œå…¶è®¾è®¡ä¸ä¼˜åŒ–ä»ç„¶é’ˆå¯¹çº¯æ–‡æœ¬é¢†åŸŸï¼Œåœ¨åº”ç”¨äºå¤šæ¨¡æ€æ¨ç†ä»»åŠ¡æ—¶è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå½“å‰å¤šæ¨¡æ€æ¨ç†ä¸­çš„è¯¯å·®ä¸»è¦æ¥æºäºè§†è§‰è¾“å…¥çš„æ„ŸçŸ¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†æ„ŸçŸ¥æ„ŸçŸ¥ç­–ç•¥ä¼˜åŒ–ï¼ˆPAPOï¼‰ï¼Œè¿™æ˜¯GRPOçš„ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ‰©å±•ï¼Œé¼“åŠ±æ¨¡å‹åœ¨å­¦ä¹ æ¨ç†çš„åŒæ—¶å­¦ä¹ æ„ŸçŸ¥ï¼Œå®Œå…¨åŸºäºå†…éƒ¨ç›‘ç£ä¿¡å·ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒPAPOä¸ä¾èµ–é¢å¤–çš„æ•°æ®æ•´ç†ã€å¤–éƒ¨å¥–åŠ±æ¨¡å‹æˆ–ä¸“æœ‰æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨GRPOç›®æ ‡ä¸­å¼•å…¥äº†éšå¼æ„ŸçŸ¥æŸå¤±ï¼Œä»¥KLæ•£åº¦é¡¹çš„å½¢å¼ï¼Œå°½ç®¡å®ƒå¾ˆç®€å•ï¼Œä½†åœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šäº§ç”Ÿäº†æ˜¾è‘—çš„æ€»ä½“æ”¹è¿›ï¼ˆ4.4%ï¼‰ã€‚åœ¨é«˜åº¦ä¾èµ–è§†è§‰çš„ä»»åŠ¡ä¸Šï¼Œæ”¹è¿›æ›´ä¸ºæ˜¾è‘—ï¼Œæ¥è¿‘8.0%ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°æ„ŸçŸ¥é”™è¯¯å¤§å¹…å‡å°‘ï¼ˆ30.5%ï¼‰ï¼Œè¡¨æ˜PAPOæé«˜äº†æ„ŸçŸ¥èƒ½åŠ›ã€‚æˆ‘ä»¬å¯¹PAPOè¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œå‘ç°äº†ä¸€ä¸ªç‹¬ç‰¹çš„æŸå¤±é»‘å®¢é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œäº†ä¸¥æ ¼çš„åˆ†æï¼Œå¹¶é€šè¿‡åŒé‡ç†µæŸå¤±å‡è½»äº†è¯¥é—®é¢˜ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œå°†æ„ŸçŸ¥æ„ŸçŸ¥ç›‘ç£æ›´æ·±å…¥åœ°é›†æˆåˆ°RLVRå­¦ä¹ ç›®æ ‡ä¸­ï¼Œå¹¶ä¸ºé¼“åŠ±è§†è§‰è¾…åŠ©æ¨ç†çš„æ–°RLæ¡†æ¶å¥ å®šäº†åŸºç¡€ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://mikewangwzhl.github.io/PAPO%E3%80%82">https://mikewangwzhl.github.io/PAPOã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06448v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å®é™…åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡çš„é—®é¢˜åŠå…¶è§£å†³ç­–ç•¥ã€‚é€šè¿‡å¼•å…¥æ„ŸçŸ¥æ„è¯†ç­–ç•¥ä¼˜åŒ–ï¼ˆPAPOï¼‰ï¼Œå®ç°äº†æ„ŸçŸ¥ä¸æ¨ç†çš„ååŒå­¦ä¹ ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚PAPOæ–¹æ³•é€šè¿‡å¼•å…¥éšå«æ„ŸçŸ¥æŸå¤±ï¼Œä¼˜åŒ–äº†é€šç”¨ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„ç›®æ ‡å‡½æ•°ï¼Œæé«˜äº†æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ï¼Œå‡å°‘äº†æ„ŸçŸ¥é”™è¯¯ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜é€šè¿‡æ·±å…¥åˆ†æè§£å†³äº†æŸå¤±é»‘å®¢é—®é¢˜ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­è§†è§‰æ¨ç†çš„é›†æˆå‘å±•å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å…·æœ‰å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ç­–ç•¥ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èµ‹äºˆå¼ºå¤§çš„å¤šæ­¥éª¤æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚</li>
<li>å¤šæ¨¡æ€æ¨ç†ä¸­çš„é”™è¯¯ä¸»è¦æºäºå¯¹è§†è§‰è¾“å…¥çš„æ„ŸçŸ¥é—®é¢˜ã€‚</li>
<li>å¼•å…¥æ„ŸçŸ¥æ„è¯†ç­–ç•¥ä¼˜åŒ–ï¼ˆPAPOï¼‰æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡å†…éƒ¨ç›‘ç£ä¿¡å·é¼“åŠ±æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­¦ä¹ æ„ŸçŸ¥ã€‚</li>
<li>PAPOæ–¹æ³•é€šè¿‡å¼•å…¥éšå«æ„ŸçŸ¥æŸå¤±æ¥ä¼˜åŒ–é€šç”¨ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„ç›®æ ‡å‡½æ•°ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥èƒ½åŠ›ã€‚</li>
<li>PAPOæ˜¾è‘—æé«˜äº†æ¨¡å‹åœ¨å¤šæ¨¡æ€æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå‡å°‘äº†æ„ŸçŸ¥é”™è¯¯ã€‚</li>
<li>è§£å†³äº†æŸå¤±é»‘å®¢é—®é¢˜ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ æ¡†æ¶ä¸­è§†è§‰æ¨ç†çš„è¿›ä¸€æ­¥å‘å±•å¥ å®šåŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2be00f97e2bc97b5e016d0cac7ba1e51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d05e2b6b625f6d23e5c17f98db36925e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17d3d90d0adc43ada3bb9dfa1792ea1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a317e7519db9da60c387a1b82581abc1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Conversation-Forests-The-Key-to-Fine-Tuning-Large-Language-Models-for-Multi-Turn-Medical-Conversations-is-Branching"><a href="#Conversation-Forests-The-Key-to-Fine-Tuning-Large-Language-Models-for-Multi-Turn-Medical-Conversations-is-Branching" class="headerlink" title="Conversation Forests: The Key to Fine Tuning Large Language Models for   Multi-Turn Medical Conversations is Branching"></a>Conversation Forests: The Key to Fine Tuning Large Language Models for   Multi-Turn Medical Conversations is Branching</h2><p><strong>Authors:Thomas Savage</strong></p>
<p>Fine-tuning methods such as Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO) have demonstrated success in training large language models (LLMs) for single-turn tasks. However, these methods fall short in multi-turn applications, such as diagnostic patient interviewing, where understanding how early conversational turns influence downstream completions and outcomes is essential. In medicine, a multi-turn perspective is critical for learning diagnostic schemas and better understanding conversation dynamics. To address this gap, I introduce Savage Conversation Forests (SCF), a reinforcement learning framework that leverages a branched conversation architecture to fine-tune LLMs for multi-turn dialogue. SCF generates multiple possible conversation continuations at each turn, enabling the model to learn how different early responses affect downstream interactions and diagnostic outcomes. In experiments simulating doctor-patient conversations, SCF with branching outperforms linear conversation architectures on diagnostic accuracy. I hypothesize that SCFâ€™s improvements stem from its ability to provide richer, interdependent training signals across conversation turns. These results suggest that a branched training architecture is an important strategy for fine tuning LLMs in complex multi-turn conversational tasks. </p>
<blockquote>
<p>ç²¾ç»†è°ƒæ•´æ–¹æ³•ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œåœ¨é’ˆå¯¹å•ä¸€å›åˆä»»åŠ¡çš„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å–å¾—äº†æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤šå›åˆåº”ç”¨æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¦‚åœ¨è¯Šæ–­ç—…äººè®¿è°ˆä¸­ï¼Œç†è§£æ—©æœŸå¯¹è¯å›åˆå¦‚ä½•å½±å“ä¸‹æ¸¸å®Œæˆå’Œç»“æœè‡³å…³é‡è¦ã€‚åœ¨åŒ»å­¦é¢†åŸŸï¼Œå¤šå›åˆè§†è§’å¯¹äºå­¦ä¹ è¯Šæ–­æ¨¡å¼å¹¶æ›´å¥½åœ°ç†è§£å¯¹è¯åŠ¨æ€è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘å¼•å…¥äº†é‡è›®å¯¹è¯æ£®æ—ï¼ˆSCFï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨åˆ†æ”¯å¯¹è¯æ¶æ„æ¥ç²¾ç»†è°ƒæ•´LLMçš„å¤šå›åˆå¯¹è¯çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚SCFåœ¨æ¯ä¸€å›åˆç”Ÿæˆå¤šä¸ªå¯èƒ½çš„å¯¹è¯å»¶ç»­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä¸åŒçš„æ—©æœŸå›åº”å¦‚ä½•å½±å“ä¸‹æ¸¸äº’åŠ¨å’Œè¯Šæ–­ç»“æœã€‚åœ¨æ¨¡æ‹ŸåŒ»ç”Ÿç—…äººå¯¹è¯çš„å®éªŒä¸­ï¼Œå¸¦æœ‰åˆ†æ”¯çš„SCFåœ¨è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢ä¼˜äºçº¿æ€§å¯¹è¯æ¶æ„ã€‚æˆ‘å‡è®¾SCFçš„æ”¹è¿›æºäºå…¶åœ¨æ•´ä¸ªå¯¹è¯å›åˆä¸­æä¾›ä¸°å¯Œã€ç›¸äº’ä¾å­˜è®­ç»ƒä¿¡å·çš„èƒ½åŠ›ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œåˆ†æ”¯è®­ç»ƒæ¶æ„æ˜¯åœ¨å¤æ‚çš„å¤šå›åˆå¯¹è¯ä»»åŠ¡ä¸­å¾®è°ƒLLMçš„é‡è¦ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04099v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å•å›åˆä»»åŠ¡ä¸­é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ç­‰å¾®è°ƒæ–¹æ³•å–å¾—äº†æˆåŠŸã€‚ä½†åœ¨å¤šå›åˆåº”ç”¨ä¸­ï¼Œå¦‚è¯Šæ–­ç—…äººè®¿è°ˆç­‰ï¼Œè¿™äº›æ–¹æ³•è¡¨ç°ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡å¼•å…¥é‡è›®å¯¹è¯æ£®æ—ï¼ˆSCFï¼‰è¿™ä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œåˆ©ç”¨åˆ†æ”¯å¯¹è¯æ¶æ„å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œä»¥åº”å¯¹å¤šå›åˆå¯¹è¯ã€‚SCFåœ¨æ¨¡æ‹ŸåŒ»æ‚£å¯¹è¯çš„å®éªŒä¸­ï¼Œä»¥åˆ†æ”¯å¯¹è¯æ¶æ„å±•ç°å‡ºæ›´é«˜çš„è¯Šæ–­å‡†ç¡®æ€§ã€‚è¿™æš—ç¤ºç€åˆ†æ”¯è®­ç»ƒæ¶æ„æ˜¯åœ¨å¤æ‚å¤šå›åˆå¯¹è¯ä»»åŠ¡ä¸­å¾®è°ƒLLMçš„é‡è¦ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å•å›åˆä»»åŠ¡ä¸­å·²å–å¾—æˆåŠŸåº”ç”¨å¾®è°ƒæ–¹æ³•ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å’Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ã€‚</li>
<li>åœ¨å¤šå›åˆå¯¹è¯ä»»åŠ¡ä¸­ï¼Œå¦‚è¯Šæ–­ç—…äººè®¿è°ˆï¼Œç°æœ‰å¾®è°ƒæ–¹æ³•è¡¨ç°ä¸è¶³ï¼Œéœ€è¦æ›´å¤æ‚çš„ç­–ç•¥ã€‚</li>
<li>é‡è›®å¯¹è¯æ£®æ—ï¼ˆSCFï¼‰æ˜¯ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œé‡‡ç”¨åˆ†æ”¯å¯¹è¯æ¶æ„è¿›è¡ŒLLMå¾®è°ƒã€‚</li>
<li>SCFèƒ½å¤Ÿç”Ÿæˆæ¯ä¸ªå›åˆçš„å¤šç§å¯èƒ½çš„å¯¹è¯å»¶ç»­ï¼Œä½¿æ¨¡å‹äº†è§£æ—©æœŸå›åº”å¦‚ä½•å½±å“ä¸‹æ¸¸äº’åŠ¨å’Œè¯Šæ–­ç»“æœã€‚</li>
<li>åœ¨æ¨¡æ‹ŸåŒ»æ‚£å¯¹è¯çš„å®éªŒä¸­ï¼ŒSCFçš„åˆ†æ”¯æ¶æ„åœ¨è¯Šæ–­å‡†ç¡®æ€§ä¸Šä¼˜äºçº¿æ€§å¯¹è¯æ¶æ„ã€‚</li>
<li>SCFçš„æ”¹è¿›æºäºå…¶æä¾›è·¨å¯¹è¯å›åˆçš„ä¸°å¯Œã€ç›¸äº’ä¾èµ–çš„è®­ç»ƒä¿¡å·çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04099">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f98b4223d0644d3fe9a70c330ceae3cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cd4e15ec373b13b3a016e880e007bbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb7560e5ff18f9e64988bc0ea54c738f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-34e43544546ec15afd965f333d88e791.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b558fd94589aa274609ce601c4667f4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="RAG-R1-Incentivize-the-Search-and-Reasoning-Capabilities-of-LLMs-through-Multi-query-Parallelism"><a href="#RAG-R1-Incentivize-the-Search-and-Reasoning-Capabilities-of-LLMs-through-Multi-query-Parallelism" class="headerlink" title="RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs   through Multi-query Parallelism"></a>RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs   through Multi-query Parallelism</h2><p><strong>Authors:Zhiwen Tan, Jiaming Huang, Qintong Wu, Hongxuan Zhang, Chenyi Zhuang, Jinjie Gu</strong></p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, while they remain prone to generating hallucinated or outdated responses due to their static internal knowledge. Recent advancements in Retrieval-Augmented Generation (RAG) methods have explored enhancing modelsâ€™ search and reasoning capabilities through reinforcement learning (RL). Although these methods demonstrate promising results, they face challenges in training stability and encounter issues such as substantial inference time and restricted capabilities due to the single-query mode. In this paper, we propose RAG-R1, a novel training framework designed to enable LLMs to adaptively leverage internal and external knowledge during the reasoning process. We further expand the generation and retrieval processes within the framework from single-query mode to multi-query parallelism, aimed at reducing inference time and enhancing the modelâ€™s capabilities. Extensive experiments on seven question-answering benchmarks demonstrate that our method outperforms the strongest baseline by up to 13.2% and decreases inference time by 11.1%. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†ç”±äºå…¶å†…éƒ¨çŸ¥è¯†çš„é™æ€æ€§ï¼Œå®ƒä»¬å®¹æ˜“äº§ç”Ÿè™šæ„æˆ–è¿‡æ—¶çš„å›åº”ã€‚æœ€è¿‘ï¼Œå¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰æ–¹æ³•çš„è¿›æ­¥é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¢ç´¢äº†æé«˜æ¨¡å‹çš„æœç´¢å’Œæ¨ç†èƒ½åŠ›ã€‚å°½ç®¡è¿™äº›æ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†å®ƒä»¬é¢ä¸´ç€è®­ç»ƒç¨³å®šæ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¹¶é‡åˆ°äº†ç”±äºå•æŸ¥è¯¢æ¨¡å¼è€Œå¯¼è‡´çš„é—®é¢˜ï¼Œå¦‚æ¨ç†æ—¶é—´å®è´¨æ€§å¢åŠ å’Œèƒ½åŠ›å—é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†RAG-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿LLMèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°åˆ©ç”¨å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åœ¨æ¡†æ¶å†…æ‰©å±•äº†ç”Ÿæˆå’Œæ£€ç´¢è¿‡ç¨‹ï¼Œä»å•æŸ¥è¯¢æ¨¡å¼æ‰©å±•åˆ°å¤šæŸ¥è¯¢å¹¶è¡Œå¤„ç†ï¼Œæ—¨åœ¨å‡å°‘æ¨ç†æ—¶é—´å¹¶å¢å¼ºæ¨¡å‹çš„èƒ½åŠ›ã€‚åœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”æœ€å¼ºåŸºçº¿é«˜å‡º13.2%ï¼Œå¹¶å°†æ¨ç†æ—¶é—´å‡å°‘äº†11.1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02962v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMå®¹æ˜“å—åˆ°ç”Ÿæˆçš„å†…å®¹äº§ç”Ÿåå·®æˆ–è€…å›åº”è¿‡æ—¶çš„é—®é¢˜ã€‚é€šè¿‡RAGå¢å¼ºæ¨¡å‹çš„æœç´¢å’Œæ¨ç†èƒ½åŠ›æ˜¯è§£å†³è¯¥é—®é¢˜çš„æ–¹æ³•ä¹‹ä¸€ï¼Œä½†ä»é¢ä¸´è®­ç»ƒç¨³å®šæ€§æŒ‘æˆ˜ä»¥åŠæ¨ç†æ—¶é—´å’Œæ¨¡å‹èƒ½åŠ›å—é™çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„è®­ç»ƒæ¡†æ¶RAG-R1ï¼Œè¿™ä¸ªæ¡†æ¶èƒ½ä½¿LLMåœ¨æ¨ç†è¿‡ç¨‹ä¸­çµæ´»ä½¿ç”¨å†…å¤–éƒ¨çŸ¥è¯†ï¼Œä»å•æŸ¥è¯¢æ¨¡å¼æ‰©å±•åˆ°å¤šæŸ¥è¯¢å¹¶è¡Œæ¨¡å¼ï¼Œæ—¨åœ¨æé«˜æ¨ç†é€Ÿåº¦å¹¶å¢å¼ºæ¨¡å‹èƒ½åŠ›ã€‚åœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºæœ€å¼ºå¤§çš„åŸºçº¿æ–¹æ³•æé«˜äº†é«˜è¾¾13.2%ï¼Œæ¨ç†æ—¶é—´å‡å°‘äº†11.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æŸäº›ä»»åŠ¡ä¸Šå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†ä»å­˜åœ¨ç”Ÿæˆè™šå¹»æˆ–è¿‡æ—¶å“åº”çš„é—®é¢˜ã€‚</li>
<li>RAGæ–¹æ³•æ—¨åœ¨å¢å¼ºæ¨¡å‹çš„æœç´¢å’Œæ¨ç†èƒ½åŠ›ï¼Œä½†é¢ä¸´è®­ç»ƒç¨³å®šæ€§æŒ‘æˆ˜å’Œæ¨ç†æ—¶é—´é™åˆ¶ã€‚</li>
<li>RAG-R1æ˜¯ä¸€ä¸ªæ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡é€‚åº”æ€§åœ°åˆ©ç”¨å†…éƒ¨å’Œå¤–éƒ¨çŸ¥è¯†æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>RAG-R1å°†ç”Ÿæˆå’Œæ£€ç´¢è¿‡ç¨‹ä»å•æŸ¥è¯¢æ¨¡å¼æ‰©å±•åˆ°å¤šæŸ¥è¯¢å¹¶è¡Œæ¨¡å¼ï¼Œä»¥æé«˜æ¨ç†é€Ÿåº¦å’Œæ¨¡å‹èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸ƒä¸ªé—®ç­”åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œè¶…è¿‡æœ€å¼ºåŸºçº¿é«˜è¾¾13.2%ã€‚</li>
<li>RAG-R1ä¸ä»…æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œè¿˜é™ä½äº†æ¨ç†æ—¶é—´ï¼Œå‡å°‘è¾¾11.1%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02962">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-92f68f26d1de800f99e769408a5d8098.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b281ab18f826b1ede85842c931c6b842.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-35fc32b724226ee011ee72b5b2e34efb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18f9a6dbac11e4ba7490735e518c7673.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Driven-Closed-Loop-UAV-Operation-with-Semantic-Observations"><a href="#Large-Language-Model-Driven-Closed-Loop-UAV-Operation-with-Semantic-Observations" class="headerlink" title="Large Language Model-Driven Closed-Loop UAV Operation with Semantic   Observations"></a>Large Language Model-Driven Closed-Loop UAV Operation with Semantic   Observations</h2><p><strong>Authors:Wenhao Wang, Yanyan Li, Long Jiao, Jiawei Yuan</strong></p>
<p>Recent advances in large Language Models (LLMs) have revolutionized mobile robots, including unmanned aerial vehicles (UAVs), enabling their intelligent operation within Internet of Things (IoT) ecosystems. However, LLMs still face challenges from logical reasoning and complex decision-making, leading to concerns about the reliability of LLM-driven UAV operations in IoT applications. In this paper, we propose a LLM-driven closed-loop control framework that enables reliable UAV operations powered by effective feedback and refinement using two LLM modules, i.e., a Code Generator and an Evaluator. Our framework transforms numerical state observations from UAV operations into natural language trajectory descriptions to enhance the evaluator LLMâ€™s understanding of UAV dynamics for precise feedback generation. Our framework also enables a simulation-based refinement process, and hence eliminates the risks to physical UAVs caused by incorrect code execution during the refinement. Extensive experiments on UAV control tasks with different complexities are conducted. The experimental results show that our framework can achieve reliable UAV operations using LLMs, which significantly outperforms baseline approaches in terms of success rate and completeness with the increase of task complexity. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•åœ¨ç§»åŠ¨æœºå™¨äººé¢†åŸŸå¼•å‘äº†é©å‘½æ€§çš„å˜é©ï¼ŒåŒ…æ‹¬æ— äººé£è¡Œå™¨ï¼ˆUAVsï¼‰ã€‚è¿™æ¨åŠ¨äº†å®ƒä»¬åœ¨äº’è”ç½‘ç‰©è”ç½‘ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ™ºèƒ½æ“ä½œã€‚ç„¶è€Œï¼ŒLLMä»é¢ä¸´é€»è¾‘æ¨ç†å’Œå¤æ‚å†³ç­–æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´å¯¹LLMé©±åŠ¨çš„æ— äººæœºåœ¨ç‰©è”ç½‘åº”ç”¨ä¸­çš„å¯é æ€§çš„æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§LLMé©±åŠ¨çš„é—­ç¯æ§åˆ¶æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªLLMæ¨¡å—ï¼ˆå³ä»£ç ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ï¼‰çš„æœ‰æ•ˆåé¦ˆå’Œæ”¹è¿›ï¼Œå®ç°äº†å¯é çš„æ— äººæœºæ“ä½œã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†æ— äººæœºæ“ä½œçš„æ•°å€¼çŠ¶æ€è§‚å¯Ÿè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€è½¨è¿¹æè¿°ï¼Œå¢å¼ºäº†è¯„ä¼°å™¨LLMå¯¹æ— äººæœºåŠ¨æ€çš„ç†è§£ï¼Œä»¥å®ç°ç²¾ç¡®çš„åé¦ˆç”Ÿæˆã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿˜å¯ç”¨äº†åŸºäºæ¨¡æ‹Ÿçš„æ”¹è¿›è¿‡ç¨‹ï¼Œä»è€Œæ¶ˆé™¤äº†æ”¹è¿›è¿‡ç¨‹ä¸­å› ä»£ç æ‰§è¡Œé”™è¯¯è€Œå¯¹å®é™…æ— äººæœºé€ æˆçš„é£é™©ã€‚æˆ‘ä»¬å¯¹ä¸åŒå¤æ‚åº¦çš„æ— äººæœºæ§åˆ¶ä»»åŠ¡è¿›è¡Œäº†å¤§é‡å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿåœ¨LLMä¸Šå®ç°å¯é çš„æ— äººæœºæ“ä½œï¼Œéšç€ä»»åŠ¡å¤æ‚æ€§çš„å¢åŠ ï¼Œåœ¨æˆåŠŸç‡å’Œå®Œæ•´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01930v3">PDF</a> 9 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç§»åŠ¨æœºå™¨äººæŠ€æœ¯ä¸­çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯æ— äººé£è¡Œå™¨ï¼ˆUAVï¼‰é¢†åŸŸï¼Œå·²ç»å®ç°äº†åœ¨äº’è”ç½‘ç‰©è”ç½‘ï¼ˆIoTï¼‰ç”Ÿæ€ç³»ç»Ÿä¸­çš„æ™ºèƒ½æ“ä½œã€‚ç„¶è€Œï¼ŒLLMåœ¨é€»è¾‘æ¨ç†å’Œå¤æ‚å†³ç­–åˆ¶å®šæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œå¯¼è‡´å¯¹LLMé©±åŠ¨çš„æ— äººæœºåœ¨IoTåº”ç”¨ä¸­è¿è¡Œçš„å¯é æ€§å­˜åœ¨æ‹…å¿§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§LLMé©±åŠ¨çš„é—­ç¯æ§åˆ¶æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªLLMæ¨¡å—å³ä»£ç ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨å®ç°å¯é åé¦ˆå’Œä¼˜åŒ–ã€‚æ­¤æ¡†æ¶å¯å°†æ— äººæœºçš„æ•°å€¼çŠ¶æ€è§‚æµ‹è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€è½¨è¿¹æè¿°ï¼Œä»è€Œæé«˜è¯„ä¼°å™¨LLMå¯¹æ— äººæœºåŠ¨æ€çš„ç†è§£ï¼Œå®ç°ç²¾ç¡®åé¦ˆç”Ÿæˆã€‚è¯¥æ¡†æ¶è¿˜æ”¯æŒåŸºäºæ¨¡æ‹Ÿçš„æ”¹è¿›è¿‡ç¨‹ï¼Œæ¶ˆé™¤äº†æ”¹è¿›è¿‡ç¨‹ä¸­å› ä»£ç æ‰§è¡Œé”™è¯¯è€Œå¯¹å®é™…æ— äººæœºé€ æˆçš„é£é™©ã€‚é€šè¿‡åœ¨ä¸åŒå¤æ‚åº¦çš„æ— äººæœºæ§åˆ¶ä»»åŠ¡ä¸Šè¿›è¡Œå¤§é‡å®éªŒï¼Œè¯æ˜è¯¥æ¡†æ¶èƒ½å¤Ÿå¯é åœ°ä½¿ç”¨LLMå®ç°æ— äººæœºæ“ä½œï¼Œåœ¨ä»»åŠ¡å¤æ‚åº¦å¢åŠ çš„æƒ…å†µä¸‹ï¼Œç›¸å¯¹äºåŸºå‡†æ–¹æ³•å…·æœ‰æ›´é«˜çš„æˆåŠŸç‡å’Œå®Œæ•´æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²åº”ç”¨äºç§»åŠ¨æœºå™¨äººæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯æ— äººé£è¡Œå™¨ï¼ˆUAVï¼‰é¢†åŸŸã€‚</li>
<li>åœ¨äº’è”ç½‘ç‰©è”ç½‘ï¼ˆIoTï¼‰ç”Ÿæ€ç³»ç»Ÿä¸­å®ç°äº†æ™ºèƒ½æ“ä½œã€‚</li>
<li>LLMé¢ä¸´é€»è¾‘æ¨ç†å’Œå¤æ‚å†³ç­–åˆ¶å®šçš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§LLMé©±åŠ¨çš„é—­ç¯æ§åˆ¶æ¡†æ¶ï¼Œé€šè¿‡ä»£ç ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ä¸¤ä¸ªæ¨¡å—å®ç°å¯é åé¦ˆå’Œä¼˜åŒ–ã€‚</li>
<li>æ¡†æ¶å¯å°†æ— äººæœºçš„æ•°å€¼çŠ¶æ€è§‚æµ‹è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€è½¨è¿¹æè¿°ï¼Œæé«˜è¯„ä¼°å™¨å¯¹æ— äººæœºåŠ¨æ€çš„ç†è§£ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒæ¨¡æ‹Ÿæ”¹è¿›è¿‡ç¨‹ï¼Œå‡å°‘å®é™…æ— äººæœºçš„é£é™©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-08bf869f2438e1b10e495a5ad6d9ea5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b4fab06cc3db47da849f4fd44147abe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c383a0d150447f7e3fdbfe68eb73bbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a691b73db0c36312c4a697f6aec9130.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f682c04b456367edb962cd4817e6762.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a452a52b6d593509c4f9c3aeb6565e5.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Eka-Eval-A-Comprehensive-Evaluation-Framework-for-Large-Language-Models-in-Indian-Languages"><a href="#Eka-Eval-A-Comprehensive-Evaluation-Framework-for-Large-Language-Models-in-Indian-Languages" class="headerlink" title="Eka-Eval : A Comprehensive Evaluation Framework for Large Language   Models in Indian Languages"></a>Eka-Eval : A Comprehensive Evaluation Framework for Large Language   Models in Indian Languages</h2><p><strong>Authors:Samridhi Raj Sinha, Rajvee Sheth, Abhishek Upperwal, Mayank Singh</strong></p>
<p>The rapid advancement of Large Language Models (LLMs) has intensified the need for evaluation frameworks that address the requirements of linguistically diverse regions, such as India, and go beyond English-centric benchmarks. We introduce EKA-EVAL, a unified evaluation framework that integrates over 35+ benchmarks (including 10 Indic benchmarks) across nine major evaluation categories. The framework provides broader coverage than existing Indian language evaluation tools, offering 11 core capabilities through a modular architecture, seamless integration with Hugging Face and proprietary models, and plug-and-play usability. As the first end-to-end suite for scalable, multilingual LLM benchmarking, the framework combines extensive benchmarks, modular workflows, and dedicated support for low-resource Indian languages to enable inclusive assessment of LLM capabilities across diverse domains. We conducted extensive comparisons against five existing baselines, demonstrating that EKA-EVAL achieves the highest participant ratings in four out of five categories. The framework is open-source and publicly available at: <a target="_blank" rel="noopener" href="https://github.com/lingo-iitgn/eka-eval">https://github.com/lingo-iitgn/eka-eval</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œéœ€è¦æ›´åŠ å¼ºè°ƒè¯„ä¼°æ¡†æ¶çš„é‡è¦æ€§ï¼Œä»¥æ»¡è¶³å°åº¦ç­‰è¯­è¨€å¤šæ ·åŒ–çš„åœ°åŒºçš„éœ€æ±‚ï¼Œå¹¶è¶…è¶Šä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬æ¨å‡ºäº†EKA-EVALï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œæ•´åˆäº†è¶…è¿‡35ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬10ä¸ªå°åº¦è¯­è¨€åŸºå‡†æµ‹è¯•ï¼‰ï¼Œæ¶µç›–ä¹å¤§ä¸»è¦è¯„ä¼°ç±»åˆ«ã€‚è¯¥æ¡†æ¶æä¾›äº†æ¯”ç°æœ‰å°åº¦è¯­è¨€è¯„ä¼°å·¥å…·æ›´å¹¿æ³›çš„è¦†ç›–èŒƒå›´ï¼Œé€šè¿‡æ¨¡å—åŒ–æ¶æ„æä¾›11é¡¹æ ¸å¿ƒèƒ½åŠ›ï¼Œä¸Hugging Faceå’Œä¸“æœ‰æ¨¡å‹æ— ç¼é›†æˆï¼Œä»¥åŠå³æ’å³ç”¨çš„æ˜“ç”¨æ€§ã€‚ä½œä¸ºé¦–ä¸ªç”¨äºå¯æ‰©å±•ã€å¤šè¯­è¨€LLMåŸºå‡†æµ‹è¯•ç«¯åˆ°ç«¯å¥—ä»¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ã€æ¨¡å—åŒ–å·¥ä½œæµç¨‹ä»¥åŠå¯¹ä½èµ„æºå°åº¦è¯­è¨€çš„ä¸“é—¨æ”¯æŒï¼Œä»¥å®ç°åœ¨ä¸åŒé¢†åŸŸå…¨é¢è¯„ä¼°LLMçš„èƒ½åŠ›ã€‚æˆ‘ä»¬ä¸äº”ä¸ªç°æœ‰åŸºå‡†è¿›è¡Œäº†å¹¿æ³›æ¯”è¾ƒï¼Œè¯æ˜EKA-EVALåœ¨äº”ä¸ªç±»åˆ«ä¸­çš„å››ä¸ªä¸­è·å¾—äº†æœ€é«˜å‚ä¸è€…è¯„åˆ†ã€‚è¯¥æ¡†æ¶æ˜¯å¼€æºçš„ï¼Œå¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/lingo-iitgn/eka-eval%E3%80%82">https://github.com/lingo-iitgn/eka-evalã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01853v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å¢å¼ºäº†è¯„ä¼°æ¡†æ¶çš„éœ€æ±‚ï¼Œè¯¥éœ€æ±‚å¿…é¡»æ»¡è¶³è¯­è¨€å¤šæ ·åŒ–åœ°åŒºï¼ˆå¦‚å°åº¦ï¼‰çš„è¦æ±‚ï¼Œå¹¶è¶…è¶Šè‹±è¯­ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬æ¨å‡ºäº†EKA-EVALï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œé›†æˆäº†è¶…è¿‡35ä¸ªåŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬10ä¸ªå°åº¦è¯­è¨€åŸºå‡†æµ‹è¯•ï¼‰ï¼Œæ¶µç›–ä¹å¤§ç±»ä¸»è¦è¯„ä¼°é¡¹ç›®ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡å—åŒ–æ¶æ„æä¾›äº†æ›´å¹¿æ³›çš„è¦†ç›–èŒƒå›´ï¼Œä¸Hugging Faceå’Œä¸“æœ‰æ¨¡å‹æ— ç¼é›†æˆï¼Œå¹¶å…·æœ‰å³æ’å³ç”¨æ€§ã€‚ä½œä¸ºé¦–ä¸ªç«¯åˆ°ç«¯çš„ã€å¯æ‰©å±•çš„å¤šè¯­è¨€LLMåŸºå‡†æµ‹è¯•å¥—ä»¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ã€æ¨¡å—åŒ–å·¥ä½œæµç¨‹ä»¥åŠå¯¹ä½èµ„æºå°åº¦è¯­è¨€çš„ä¸“é—¨æ”¯æŒï¼Œä»¥å®ç°å¯¹ä¸åŒé¢†åŸŸLLMèƒ½åŠ›çš„åŒ…å®¹æ€§è¯„ä¼°ã€‚ç»è¿‡ä¸äº”ä¸ªç°æœ‰åŸºå‡†çš„å¹¿æ³›æ¯”è¾ƒï¼Œè¯æ˜EKA-EVALåœ¨äº”ä¸ªç±»åˆ«ä¸­çš„å››é¡¹è·å¾—äº†æœ€é«˜å‚ä¸è€…è¯„åˆ†ã€‚è¯¥æ¡†æ¶æ˜¯å¼€æºçš„ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lingo-iitgn/eka-eval%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/lingo-iitgn/eka-evalå…¬å¼€è®¿é—®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EKA-EVALæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œä¸“ä¸ºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½è€Œè®¾è®¡ã€‚</li>
<li>å®ƒé›†æˆäº†è¶…è¿‡35ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬é’ˆå¯¹å°åº¦è¯­è¨€çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ¶µç›–ä¹å¤§ç±»è¯„ä¼°é¡¹ç›®ï¼Œå¦‚é˜…è¯»ç†è§£ã€è¯­è¨€ç”Ÿæˆç­‰ã€‚</li>
<li>è¯¥æ¡†æ¶å…·æœ‰æ¨¡å—åŒ–æ¶æ„ï¼Œæ˜“äºé›†æˆä¸åŒçš„è¯„ä¼°å·¥å…·å’Œæ¨¡å‹ã€‚</li>
<li>å®ƒæä¾›äº†å¹¿æ³›çš„è¦†ç›–èŒƒå›´ï¼Œä¸ä»…é™äºè‹±è¯­ï¼Œæ”¯æŒå¤šç§è¯­è¨€è¯„ä¼°ã€‚</li>
<li>ä¸Hugging Faceç­‰ç°æœ‰å·¥å…·æ— ç¼é›†æˆï¼Œå¹¶å…·æœ‰å³æ’å³ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01853">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab712ce30d7609cce3124e059a2e6b6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51681274fc6459c1fc6ef0a8d3d8ee9a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c4fdda0376711e17333cca53334a489.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Teaching-Models-to-Verbalize-Reward-Hacking-in-Chain-of-Thought-Reasoning"><a href="#Teaching-Models-to-Verbalize-Reward-Hacking-in-Chain-of-Thought-Reasoning" class="headerlink" title="Teaching Models to Verbalize Reward Hacking in Chain-of-Thought   Reasoning"></a>Teaching Models to Verbalize Reward Hacking in Chain-of-Thought   Reasoning</h2><p><strong>Authors:Miles Turpin, Andy Arditi, Marvin Li, Joe Benton, Julian Michael</strong></p>
<p>Language models trained with reinforcement learning (RL) can engage in reward hackingâ€“the exploitation of unintended strategies for high rewardâ€“without revealing this behavior in their chain-of-thought reasoning. This makes the detection of reward hacking difficult, posing risks for high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL fine-tuning intervention that trains models to explicitly acknowledge when they are influenced by prompt cuesâ€“hints which point to incorrect answers (e.g., â€œa Stanford professor thinks the answer is Aâ€). To evaluate VFT, we subsequently train models with RL on environments where held-out prompt cues signal which incorrect answers will receive high reward, incentivizing models to exploit these cues instead of reasoning correctly. We measure how often models exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained modelâ€™s responses consist of undetected reward hacks. In comparison, when we perform RL without VFT, the rate of undetected reward hacks goes up to 88%; with a debiasing baseline intervention, this increases further to 99%. VFT achieves this by substantially increasing how often models verbalize the influence of cues, from 8% to 43% after VFT, and up to 94% after RL. Baselines remain low even after RL (11% and 1%). Our results show that teaching models to explicitly verbalize reward hacking behavior before RL significantly improves their detection, offering a practical path toward more transparent and safe AI systems. </p>
<blockquote>
<p>ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä¼šè¿›è¡Œå¥–åŠ±é»‘å®¢è¡Œä¸ºâ€”â€”åˆ©ç”¨æœªé¢„è®¾çš„ç­–ç•¥æ¥è·å¾—é«˜å¥–åŠ±â€”â€”è€Œä¸ä¼šæš´éœ²è¿™ä¸€è¡Œä¸ºåœ¨å…¶æ€ç»´é“¾ä¸­çš„æ¨ç†è¿‡ç¨‹ã€‚è¿™ä½¿å¾—æ£€æµ‹å¥–åŠ±é»‘å®¢è¡Œä¸ºå˜å¾—å›°éš¾ï¼Œå¹¶ä¸ºé«˜é£é™©åº”ç”¨å¸¦æ¥é£é™©ã€‚æˆ‘ä»¬æå‡ºä¸€ç§é¢„RLå¾®è°ƒå¹²é¢„æ–¹æ³•ï¼Œç§°ä¸ºâ€œè¯­è¨€åŒ–å¾®è°ƒâ€ï¼ˆVFTï¼‰ï¼Œè®­ç»ƒæ¨¡å‹æ˜ç¡®æ‰¿è®¤å®ƒä»¬ä½•æ—¶å—åˆ°æç¤ºçº¿ç´¢çš„å½±å“â€”â€”æŒ‡å‘é”™è¯¯ç­”æ¡ˆçš„æš—ç¤ºï¼ˆä¾‹å¦‚ï¼Œâ€œæ–¯å¦ç¦å¤§å­¦æ•™æˆè®¤ä¸ºç­”æ¡ˆæ˜¯Aâ€ï¼‰ã€‚ä¸ºäº†è¯„ä¼°VFTï¼Œæˆ‘ä»¬éšååœ¨éšè—æç¤ºçº¿ç´¢çš„ç¯å¢ƒä¸­ç”¨RLè®­ç»ƒæ¨¡å‹ï¼Œè¿™äº›çº¿ç´¢ä¿¡å·å“ªäº›é”™è¯¯ç­”æ¡ˆä¼šå¾—åˆ°é«˜å¥–åŠ±ï¼Œæ¿€åŠ±æ¨¡å‹åˆ©ç”¨è¿™äº›çº¿ç´¢è€Œä¸æ˜¯æ­£ç¡®æ¨ç†ã€‚æˆ‘ä»¬è¡¡é‡æ¨¡å‹åœ¨æ²¡æœ‰è¯­è¨€åŒ–çš„æƒ…å†µä¸‹åˆ©ç”¨è¿™äº›çº¿ç´¢çš„é¢‘ç‡ã€‚RLä¹‹åï¼Œåªæœ‰6%çš„VFTè®­ç»ƒæ¨¡å‹çš„å“åº”åŒ…å«æœªæ£€æµ‹çš„å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå½“æˆ‘ä»¬ä¸è¿›è¡ŒVFTè€Œç›´æ¥ä½¿ç”¨RLæ—¶ï¼Œæœªæ£€æµ‹çš„å¥–åŠ±é»‘å®¢è¡Œä¸ºçš„æ¯”ç‡ä¸Šå‡è‡³88%ï¼›ä½¿ç”¨å»ååŸºçº¿å¹²é¢„æ—¶ï¼Œè¿™ä¸€æ¯”ç‡è¿›ä¸€æ­¥ä¸Šå‡è‡³99%ã€‚VFTé€šè¿‡å¤§å¹…å¢åŠ æ¨¡å‹è¯­è¨€åŒ–çº¿ç´¢å½±å“çš„èƒ½åŠ›æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œä»VFTä¹‹åçš„8%å¢åŠ åˆ°43%ï¼Œå¹¶åœ¨RLä¹‹åè¾¾åˆ°94%ã€‚å³ä½¿åœ¨RLä¹‹åï¼ŒåŸºçº¿ä»ä¿æŒåœ¨è¾ƒä½æ°´å¹³ï¼ˆåˆ†åˆ«ä¸º11%å’Œ1%ï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨RLä¹‹å‰æ•™æˆæ¨¡å‹æ˜ç¡®è¯­è¨€åŒ–å¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œå¯ä»¥æ˜¾ç€æé«˜å…¶æ£€æµ‹èƒ½åŠ›ï¼Œä¸ºæ„å»ºæ›´é€æ˜ã€æ›´å®‰å…¨çš„AIç³»ç»Ÿæä¾›äº†å®ç”¨é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.22777v2">PDF</a> Published at ICML 2025 Workshop on Reliable and Responsible   Foundation Models</p>
<p><strong>Summary</strong>ï¼šè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ—¶ï¼Œä¼šè¿›è¡Œå¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œå³åˆ©ç”¨æœªé¢„è®¾çš„ç­–ç•¥è·å–é«˜å¥–åŠ±è€Œä¸è¡¨ç°å‡ºè¿™ç§è¡Œä¸ºåœ¨æ€ç»´é“¾ä¸­çš„æ¨ç†è¿‡ç¨‹ã€‚è¿™ä½¿å¾—æ£€æµ‹å¥–åŠ±é»‘å®¢è¡Œä¸ºå˜å¾—å›°éš¾ï¼Œå¹¶ç»™é«˜é£é™©åº”ç”¨å¸¦æ¥é£é™©ã€‚æˆ‘ä»¬æå‡ºäº†è¨€è¯­å¾®è°ƒï¼ˆVFTï¼‰è¿™ä¸€é¢„å¼ºåŒ–å­¦ä¹ å‰çš„å¾®è°ƒå¹²é¢„æ–¹æ³•ï¼Œè®­ç»ƒæ¨¡å‹æ˜¾å¼åœ°æ‰¿è®¤å—åˆ°æç¤ºçº¿ç´¢çš„å½±å“ã€‚é€šè¿‡è¯„ä¼°å‘ç°ï¼Œè¨€è¯­å¾®è°ƒå¯ä»¥æ˜¾è‘—å‡å°‘æœªå‘ç°çš„å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚ç»“æœè¯æ˜äº†è®­ç»ƒæ¨¡å‹åœ¨å¼ºåŒ–å­¦ä¹ å‰æ˜ç¡®è¡¨è¾¾å¥–åŠ±é»‘å®¢è¡Œä¸ºçš„èƒ½åŠ›å¯ä»¥æ˜¾è‘—æé«˜æ£€æµ‹ç‡ï¼Œä¸ºæ›´é€æ˜å’Œå®‰å…¨çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†å®é™…è·¯å¾„ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­è¨€æ¨¡å‹ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ—¶å¯èƒ½è¿›è¡Œå¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œå³åˆ©ç”¨æœªé¢„è®¾çš„ç­–ç•¥è·å–é«˜å¥–åŠ±è€Œä¸å±•ç°å…¶æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>æ£€æµ‹å¥–åŠ±é»‘å®¢è¡Œä¸ºå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¯¹é«˜é£é™©åº”ç”¨å­˜åœ¨é£é™©ã€‚</li>
<li>è¨€è¯­å¾®è°ƒï¼ˆVFTï¼‰æ˜¯ä¸€ç§é¢„å¼ºåŒ–å­¦ä¹ å‰çš„å¹²é¢„æ–¹æ³•ï¼Œæ—¨åœ¨è®­ç»ƒæ¨¡å‹æ˜¾å¼åœ°æ‰¿è®¤å—åˆ°æç¤ºçº¿ç´¢çš„å½±å“ã€‚</li>
<li>VFTå¯ä»¥æœ‰æ•ˆå‡å°‘æœªå‘ç°çš„å¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œæé«˜æ£€æµ‹ç‡ã€‚</li>
<li>VFTé€šè¿‡å¢åŠ æ¨¡å‹å¯¹æç¤ºçº¿ç´¢å½±å“çš„æ˜¾æ€§è¡¨è¾¾æ¥æé«˜æ£€æµ‹ç‡ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼ŒVFTæ˜¾è‘—æé«˜æ¨¡å‹çš„é€æ˜åº¦å’Œå®‰å…¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.22777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-352124dd020ff291d5b29926a2095870.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c1ab68422480d0109d9f8efb1cdb3c6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0c97bee6dd58594d7c55ce0deaf9507f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-416353a8afb0de4a4d821b8d904a7869.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Multi-Preference-Lambda-weighted-Listwise-DPO-for-Dynamic-Preference-Alignment"><a href="#Multi-Preference-Lambda-weighted-Listwise-DPO-for-Dynamic-Preference-Alignment" class="headerlink" title="Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference   Alignment"></a>Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference   Alignment</h2><p><strong>Authors:Yuhui Sun, Xiyao Wang, Zixi Li, Zhenlong Yuan, Jinman Zhao</strong></p>
<p>While large language models (LLMs) excel at text generation, aligning them with human preferences remains challenging. Reinforcement learning from human feedback (RLHF) improves alignment but is costly and unstable. Direct Preference Optimization (DPO) offers a simpler alternative, yet assumes a fixed, single-dimensional preference. We propose Multi-Preference Lambda-weighted Listwise DPO, a generalization of DPO that supports multiple preference dimensions and dynamic interpolation via a simplex-weighted lambda vector. Our method enables listwise supervision and flexible alignment without re-training. While our experiments are conducted on 1B-2B scale models, this is an intentional choice: smaller models provide a more stringent testbed where performance improvements more clearly reflect the effectiveness of the alignment strategy itself. Moreover, such models are widely used in compute-constrained applications, making our improvements both methodologically meaningful and practically valuable. Empirical results show that our approach matches or surpasses standard DPO on alignment benchmarks while offering improved adaptability. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å°†å…¶ä¸äººç±»åå¥½å¯¹é½ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æé«˜äº†å¯¹é½æ€§ï¼Œä½†æˆæœ¬é«˜æ˜‚ä¸”ä¸ç¨³å®šã€‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æä¾›äº†æ›´ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å‡è®¾å­˜åœ¨å›ºå®šçš„ä¸€ç»´åå¥½ã€‚æˆ‘ä»¬æå‡ºäº†å¤šåå¥½Î»åŠ æƒåˆ—è¡¨å¼DPOï¼Œå®ƒæ˜¯DPOçš„æ¨å¹¿ï¼Œæ”¯æŒå¤šä¸ªåå¥½ç»´åº¦å’Œé€šè¿‡å•çº¯å½¢åŠ æƒÎ»å‘é‡è¿›è¡Œçš„åŠ¨æ€æ’å€¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†åˆ—è¡¨å¼ç›‘ç£å’Œæ— éœ€é‡æ–°è®­ç»ƒçš„çµæ´»å¯¹é½ã€‚è™½ç„¶æˆ‘ä»¬çš„å®éªŒæ˜¯åœ¨1B-2Bè§„æ¨¡æ¨¡å‹ä¸Šè¿›è¡Œçš„ï¼Œä½†è¿™æ˜¯æœ‰æ„ä¸ºä¹‹çš„é€‰æ‹©ï¼šå°å‹æ¨¡å‹æä¾›äº†æ›´ä¸¥æ ¼çš„æµ‹è¯•ç¯å¢ƒï¼Œæ€§èƒ½æ”¹è¿›æ›´æ¸…æ¥šåœ°åæ˜ äº†å¯¹é½ç­–ç•¥æœ¬èº«çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¿™æ ·çš„æ¨¡å‹åœ¨è®¡ç®—å—é™çš„åº”ç”¨ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œä½¿æˆ‘ä»¬çš„æ”¹è¿›åœ¨æ–¹æ³•å­¦ä¸Šå…·æœ‰å®é™…æ„ä¹‰ï¼Œåœ¨å®è·µä¸­ä¹Ÿæå…·ä»·å€¼ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯¹é½åŸºå‡†æµ‹è¯•ä¸­ä¸æ ‡å‡†DPOç›¸åŒ¹é…æˆ–è¡¨ç°æ›´ä½³ï¼ŒåŒæ—¶æä¾›äº†æ›´å¥½çš„é€‚åº”æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19780v4">PDF</a> 13 pages, 9 figures, appendix included. To appear in Proceedings of   AAAI 2026. Code:   <a target="_blank" rel="noopener" href="https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO">https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO</a></p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸äººç±»åå¥½å¯¹é½æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æé«˜äº†å¯¹é½æ€§ï¼Œä½†æˆæœ¬é«˜æ˜‚ä¸”ä¸ç¨³å®šã€‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æä¾›äº†æ›´ç®€å•çš„é€‰æ‹©ï¼Œä½†å‡è®¾äº†å›ºå®šå•ä¸€ç»´åº¦çš„åå¥½ã€‚æˆ‘ä»¬æå‡ºäº†å¤šåå¥½LambdaåŠ æƒåˆ—è¡¨å¼DPOï¼Œå®ƒæ˜¯DPOçš„æ¨å¹¿ï¼Œæ”¯æŒå¤šä¸ªåå¥½ç»´åº¦å’Œé€šè¿‡å•çº¯å½¢åŠ æƒlambdaå‘é‡è¿›è¡Œçš„åŠ¨æ€æ’å€¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹è¿›è¡Œåˆ—è¡¨ç›‘ç£åŠçµæ´»å¯¹é½ã€‚æˆ‘ä»¬çš„å®éªŒæ˜¯åœ¨è§„æ¨¡è¾¾åˆ°å‡ åäº¿çš„æ¨¡å‹ä¸Šè¿›è¡Œçš„ï¼Œä½†è¿™æ˜¯æœ‰æ„ä¸ºä¹‹çš„é€‰æ‹©ï¼šè¾ƒå°çš„æ¨¡å‹æä¾›äº†æ›´ä¸ºä¸¥æ ¼çš„æµ‹è¯•ç¯å¢ƒï¼Œå…¶ä¸­æ€§èƒ½çš„æå‡æ›´æ¸…æ™°åœ°åæ˜ äº†å¯¹é½ç­–ç•¥æœ¬èº«çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¿™æ ·çš„æ¨¡å‹åœ¨ç®—åŠ›å—é™çš„åº”ç”¨ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œè¿™ä½¿æˆ‘ä»¬çš„æ”¹è¿›åœ¨æ–¹æ³•å­¦ä¸Šå…·æœ‰å®é™…æ„ä¹‰å¹¶åœ¨å®è·µä¸­å…·æœ‰å®ç”¨ä»·å€¼ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æˆ–è¶…è¶Šäº†æ ‡å‡†DPOçš„å¯¹é½æ€§èƒ½ï¼ŒåŒæ—¶æä¾›äº†æ›´å¥½çš„é€‚åº”æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¸äººç±»åå¥½å¯¹é½ä¸Šå­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰èƒ½æé«˜å¯¹é½æ€§ï¼Œä½†æˆæœ¬é«˜æ˜‚ä¸”ä¸ç¨³å®šã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰æä¾›äº†ä¸€ç§ç®€åŒ–æ›¿ä»£æ–¹æ¡ˆï¼Œä½†å‡è®¾äº†å›ºå®šå•ä¸€ç»´åº¦çš„åå¥½ã€‚</li>
<li>æˆ‘ä»¬æå‡ºäº†å¤šåå¥½LambdaåŠ æƒåˆ—è¡¨å¼DPOæ–¹æ³•ï¼Œæ”¯æŒå¤šä¸ªåå¥½ç»´åº¦å’ŒåŠ¨æ€æ’å€¼ã€‚</li>
<li>è¯¥æ–¹æ³•å¯åœ¨æ— éœ€é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹å®ç°åˆ—è¡¨ç›‘ç£åŠçµæ´»å¯¹é½ã€‚</li>
<li>å®éªŒæ˜¯åœ¨è§„æ¨¡å‡ åäº¿çš„æ¨¡å‹ä¸Šè¿›è¡Œçš„ï¼Œå¼ºè°ƒåœ¨è¾ƒå°çš„æ¨¡å‹ä¸Šæ€§èƒ½æå‡çš„æ„ä¹‰å’Œå®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19780">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-25083ab2f47f6c2921744714c226e9c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-850b4ce44124750c0d8ec14a9d2f7771.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5466ea13aae32df9864c7f5cf4851ed6.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="TReB-A-Comprehensive-Benchmark-for-Evaluating-Table-Reasoning-Capabilities-of-Large-Language-Models"><a href="#TReB-A-Comprehensive-Benchmark-for-Evaluating-Table-Reasoning-Capabilities-of-Large-Language-Models" class="headerlink" title="TReB: A Comprehensive Benchmark for Evaluating Table Reasoning   Capabilities of Large Language Models"></a>TReB: A Comprehensive Benchmark for Evaluating Table Reasoning   Capabilities of Large Language Models</h2><p><strong>Authors:Ce Li, Xiaofan Liu, Zhiyan Song, Ce Chi, Chen Zhao, Jingjing Yang, Zhendong Wang, Kexin Yang, Boshen Shi, Xing Wang, Chao Deng, Junlan Feng</strong></p>
<p>The majority of data in businesses and industries is stored in tables, databases, and data warehouses. Reasoning with table-structured data poses significant challenges for large language models (LLMs) due to its hidden semantics, inherent complexity, and structured nature. One of these challenges is lacking an effective evaluation benchmark fairly reflecting the performances of LLMs on broad table reasoning abilities. In this paper, we fill in this gap, presenting a comprehensive table reasoning evolution benchmark, TReB, which measures both shallow table understanding abilities and deep table reasoning abilities, a total of 26 sub-tasks. We construct a high quality dataset through an iterative data processing procedure. We create an evaluation framework to robustly measure table reasoning capabilities with three distinct inference modes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs using this frame work and prove its effectiveness. Experimental results reveal that existing LLMs still have significant room for improvement in addressing the complex and real world Table related tasks. Both the dataset and evaluation framework are publicly available, with the dataset hosted on huggingface.co&#x2F;datasets&#x2F;JT-LM&#x2F;JIUTIAN-TReB and the framework on github.com&#x2F;JT-LM&#x2F;jiutian-treb. </p>
<blockquote>
<p>åœ¨å•†ä¸šå’Œå·¥ä¸šç•Œï¼Œå¤§éƒ¨åˆ†æ•°æ®éƒ½å­˜å‚¨åœ¨è¡¨æ ¼ã€æ•°æ®åº“å’Œæ•°æ®ä»“åº“ä¸­ã€‚ç”±äºè¡¨æ ¼ç»“æ„æ•°æ®çš„éšè—è¯­ä¹‰ã€å›ºæœ‰å¤æ‚æ€§å’Œç»“æ„åŒ–ç‰¹æ€§ï¼Œå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥è¯´ï¼Œå¯¹å…¶è¿›è¡Œæ¨ç†æ„æˆäº†é‡å¤§æŒ‘æˆ˜ã€‚è¿™äº›æŒ‘æˆ˜ä¹‹ä¸€æ˜¯æ²¡æœ‰ä¸€ä¸ªæœ‰æ•ˆçš„è¯„ä¼°åŸºå‡†ï¼Œèƒ½å¤Ÿå…¬æ­£åœ°åæ˜ LLMsåœ¨å¹¿æ³›è¡¨æ ¼æ¨ç†èƒ½åŠ›æ–¹é¢çš„è¡¨ç°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ï¼Œæå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¡¨æ ¼æ¨ç†è¿›åŒ–åŸºå‡†æµ‹è¯•ï¼ˆTReBï¼‰ï¼Œè¯¥åŸºå‡†æµ‹è¯•è¡¡é‡äº†æµ…å±‚æ¬¡çš„è¡¨æ ¼ç†è§£èƒ½åŠ›å’Œæ·±å±‚æ¬¡çš„è¡¨æ ¼æ¨ç†èƒ½åŠ›ï¼Œå…±åŒ…æ‹¬26ä¸ªå­ä»»åŠ¡ã€‚æˆ‘ä»¬é€šè¿‡è¿­ä»£æ•°æ®å¤„ç†ç¨‹åºæ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æ•°æ®é›†ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œä»¥ä¸‰ç§ä¸åŒçš„æ¨ç†æ¨¡å¼ï¼ˆTCoTã€PoTå’ŒICoTï¼‰ç¨³å¥åœ°è¡¡é‡è¡¨æ ¼æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨æ­¤æ¡†æ¶å¯¹20å¤šç§æœ€æ–°LLMsè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„LLMsåœ¨è§£å†³å¤æ‚å’Œç°å®ä¸–ç•Œçš„è¡¨æ ¼ç›¸å…³ä»»åŠ¡æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶å‡å…¬å¼€å¯ç”¨ï¼Œæ•°æ®é›†æ‰˜ç®¡åœ¨huggingface.co&#x2F;datasets&#x2F;JT-LM&#x2F;JIUTIAN-TReBä¸Šï¼Œæ¡†æ¶åˆ™ä½äºgithub.com&#x2F;JT-LM&#x2F;jiutian-trebã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18421v2">PDF</a> Benmark report v1.1</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§£å†³å•†ä¸šå’Œå·¥ä¸šé¢†åŸŸæ•°æ®å¤„ç†ä¸­çš„ä¸€å¤§æŒ‘æˆ˜â€”â€”è¡¨æ ¼ç»“æ„åŒ–æ•°æ®çš„æ¨ç†é—®é¢˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§å…¨é¢çš„è¡¨æ ¼æ¨ç†è¯„ä¼°åŸºå‡†TReBï¼Œç”¨äºè¡¡é‡LLMå¯¹è¡¨æ ¼çš„æµ…å±‚æ¬¡ç†è§£å’Œæ·±å±‚æ¬¡æ¨ç†èƒ½åŠ›ï¼ŒåŒ…å«26ä¸ªå­ä»»åŠ¡ã€‚é€šè¿‡è¿­ä»£æ•°æ®å¤„ç†ç¨‹åºæ„å»ºé«˜è´¨é‡æ•°æ®é›†ï¼Œå¹¶å»ºç«‹è¯„ä¼°æ¡†æ¶ï¼Œä»¥ä¸‰ç§ä¸åŒçš„æ¨ç†æ¨¡å¼å¯¹è¡¨æ ¼æ¨ç†èƒ½åŠ›è¿›è¡Œç¨³å¥æµ‹é‡ã€‚æ–‡ç« è¿˜ä½¿ç”¨æ­¤æ¡†æ¶å¯¹20å¤šç§æœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤æ‚å’Œç°å®ä¸–ç•Œè¡¨æ ¼ç›¸å…³ä»»åŠ¡æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶å‡å·²å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¡¨æ ¼ç»“æ„åŒ–æ•°æ®åœ¨å•†ä¸šå’Œå·¥ä¸šé¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ï¼Œå¯¹å…¶çš„æ¨ç†å­˜åœ¨è¯¸å¤šæŒ‘æˆ˜ã€‚</li>
<li>å½“å‰ç¼ºä¹æœ‰æ•ˆè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¡¨æ ¼æ¨ç†æ–¹é¢çš„æ€§èƒ½çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨é¢çš„è¡¨æ ¼æ¨ç†è¯„ä¼°åŸºå‡†TReBï¼ŒåŒ…å«æµ…å±‚æ¬¡ç†è§£å’Œæ·±å±‚æ¬¡æ¨ç†èƒ½åŠ›å…±26ä¸ªå­ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡è¿­ä»£æ•°æ®å¤„ç†ç¨‹åºæ„å»ºäº†é«˜è´¨é‡æ•°æ®é›†ã€‚</li>
<li>å»ºç«‹äº†è¯„ä¼°æ¡†æ¶ï¼Œä»¥ä¸‰ç§ä¸åŒçš„æ¨ç†æ¨¡å¼æµ‹é‡è¡¨æ ¼æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æ–‡ç« ä½¿ç”¨æ­¤æ¡†æ¶å¯¹å¤šä¸ªLLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18421">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a0d3744e95ce0b5cafcce76d29133726.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6f002f04d9df31fc3334011c23ac004a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Scientistsâ€™-First-Exam-Probing-Cognitive-Abilities-of-MLLM-via-Perception-Understanding-and-Reasoning"><a href="#Scientistsâ€™-First-Exam-Probing-Cognitive-Abilities-of-MLLM-via-Perception-Understanding-and-Reasoning" class="headerlink" title="Scientistsâ€™ First Exam: Probing Cognitive Abilities of MLLM via   Perception, Understanding, and Reasoning"></a>Scientistsâ€™ First Exam: Probing Cognitive Abilities of MLLM via   Perception, Understanding, and Reasoning</h2><p><strong>Authors:Yuhao Zhou, Yiheng Wang, Xuming He, Ruoyao Xiao, Zhiwei Li, Qiantai Feng, Zijie Guo, Yuejin Yang, Hao Wu, Wenxuan Huang, Jiaqi Wei, Dan Si, Xiuqi Yao, Jia Bu, Haiwen Huang, Tianfan Fu, Shixiang Tang, Ben Fei, Dongzhan Zhou, Fenghua Ling, Yan Lu, Siqi Sun, Chenhui Li, Guanjie Zheng, Jiancheng Lv, Wenlong Zhang, Lei Bai</strong></p>
<p>Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientistsâ€™ First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries. </p>
<blockquote>
<p>ç§‘å­¦ç ”ç©¶è¶Šæ¥è¶Šä¾èµ–äºåŸºäºä¿¡æ¯å¯†é›†çš„ç§‘å­¦æ•°æ®å’Œç‰¹å®šé¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„å¤æ‚å¤šæ¨¡æ€æ¨ç†ã€‚å€ŸåŠ©ä¸“å®¶çº§ç§‘å­¦åŸºå‡†ï¼Œç§‘å­¦å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å®é™…å·¥ä½œæµç¨‹ä¸­å…·æœ‰æ˜¾è‘—æé«˜è¿™ä¸€å‘ç°è¿‡ç¨‹çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç§‘å­¦åŸºå‡†å¤§å¤šä¾§é‡äºè¯„ä¼°MLLMsçš„çŸ¥è¯†ç†è§£èƒ½åŠ›ï¼Œå¯¼è‡´å¯¹å®ƒä»¬çš„æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›çš„è¯„ä¼°ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ç§‘å­¦å®¶ç¬¬ä¸€æ¬¡è€ƒè¯•ï¼ˆSFEï¼‰åŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡ä¸‰ä¸ªç›¸äº’è”ç³»çš„æ°´å¹³æ¥è¯„ä¼°MLLMsçš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ï¼šç§‘å­¦ä¿¡å·æ„ŸçŸ¥ã€ç§‘å­¦å±æ€§ç†è§£ã€ç§‘å­¦æ¯”è¾ƒæ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼ŒSFEåŒ…å«830ä¸ªä¸“å®¶éªŒè¯çš„VQAå¯¹ï¼Œæ¶‰åŠä¸‰ç§é—®é¢˜ç±»å‹ï¼Œæ¶µç›–66ä¸ªå¤šæ¨¡æ€ä»»åŠ¡ï¼Œæ¶‰åŠäº”ä¸ªé«˜ä»·å€¼å­¦ç§‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œç›®å‰æœ€å…ˆè¿›çš„GPT-o3å’ŒInternVL-3åœ¨SFEä¸Šçš„è¡¨ç°ä»…ä¸º34.08%å’Œ26.52%ï¼Œè¿™è¡¨æ˜MLLMsåœ¨ç§‘å­¦é¢†åŸŸä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æˆ‘ä»¬å¸Œæœ›ä»SFEä¸­è·å¾—çš„æ•°æ®èƒ½æ¨åŠ¨äººå·¥æ™ºèƒ½è¾…åŠ©ç§‘å­¦å‘ç°çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10521v4">PDF</a> 82 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç§‘å­¦å‘ç°è¶Šæ¥è¶Šä¾èµ–äºåŸºäºä¿¡æ¯å¯†é›†å‹ç§‘å­¦æ•°æ®å’Œé¢†åŸŸç‰¹å®šä¸“ä¸šçŸ¥è¯†è¿›è¡Œçš„å¤šæ¨¡æ€æ¨ç†ã€‚é’ˆå¯¹å½“å‰ç§‘å­¦åŸºå‡†æµ‹è¯•å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°çš„å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†ç§‘å­¦å®¶é¦–æ¬¡è€ƒè¯•ï¼ˆSFEï¼‰åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ã€‚è¯¥åŸºå‡†æµ‹è¯•åŒ…æ‹¬ä¸‰ä¸ªç›¸äº’å…³è”çš„æ°´å¹³ï¼šç§‘å­¦ä¿¡å·æ„ŸçŸ¥ã€ç§‘å­¦å±æ€§ç†è§£å’Œç§‘å­¦æ¯”è¾ƒæ¨ç†ï¼Œæ¶µç›–äº”ä¸ªé«˜ä»·å€¼å­¦ç§‘çš„66é¡¹å¤šæ¨¡æ€ä»»åŠ¡ï¼Œé€šè¿‡830ç»„ä¸“å®¶éªŒè¯åçš„é—®ç­”å¯¹è¿›è¡Œè¯„ä¼°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç°æœ‰çš„å…ˆè¿›æ¨¡å‹å¦‚GPT-o3å’ŒInternVL-3åœ¨SFEä¸Šçš„è¡¨ç°ä»…ä¸º34.08%å’Œ26.52%ï¼Œæ˜¾ç¤ºå‡ºMLLMsåœ¨ç§‘å­¦é¢†åŸŸæœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚å¸Œæœ›é€šè¿‡SFEçš„ç ”ç©¶ç»“æœæ¨åŠ¨äººå·¥æ™ºèƒ½å¢å¼ºç§‘å­¦å‘ç°çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬çš„å…³é”®è¦ç‚¹æ€»ç»“ï¼š</p>
<ul>
<li>ç§‘å­¦å‘ç°è¶Šæ¥è¶Šä¾èµ–äºå¤æ‚çš„å¤šæ¨¡æ€æ¨ç†å’Œä¿¡æ¯å¯†é›†å‹ç§‘å­¦æ•°æ®ã€‚</li>
<li>å½“å‰çš„ç§‘å­¦åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ç†è§£èƒ½åŠ›è¯„ä¼°ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ç§‘å­¦å®¶é¦–æ¬¡è€ƒè¯•ï¼ˆSFEï¼‰åŸºå‡†æµ‹è¯•ï¼Œä»¥æ›´å…¨é¢è¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ã€‚</li>
<li>SFEåŒ…å«ä¸‰ä¸ªè¯„ä¼°æ°´å¹³ï¼šç§‘å­¦ä¿¡å·æ„ŸçŸ¥ã€ç§‘å­¦å±æ€§ç†è§£å’Œç§‘å­¦æ¯”è¾ƒæ¨ç†ã€‚</li>
<li>SFEæ¶µç›–å¤šä¸ªé«˜ä»·å€¼å­¦ç§‘ï¼ŒåŒ…å«66é¡¹å¤šæ¨¡æ€ä»»åŠ¡å’Œ830ç»„ä¸“å®¶éªŒè¯åçš„é—®ç­”å¯¹ã€‚</li>
<li>ç°æœ‰å…ˆè¿›æ¨¡å‹åœ¨SFEä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œæ˜¾ç¤ºMLLMsåœ¨ç§‘å­¦é¢†åŸŸæœ‰å·¨å¤§æ”¹è¿›ç©ºé—´ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10521">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b35b3219d215a7c973ddcd0798e08fdd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a970e2bbb9edf925c2b94b260c5e7e8d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-122fcd0cf5bc197d0dedc0a1a73382e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40e346a7ed5d56d9af01b6b00ac708e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e579c00f906d25d004c5ecb1e3ff9026.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="BIS-Reasoning-1-0-The-First-Large-Scale-Japanese-Benchmark-for-Belief-Inconsistent-Syllogistic-Reasoning"><a href="#BIS-Reasoning-1-0-The-First-Large-Scale-Japanese-Benchmark-for-Belief-Inconsistent-Syllogistic-Reasoning" class="headerlink" title="BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for   Belief-Inconsistent Syllogistic Reasoning"></a>BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for   Belief-Inconsistent Syllogistic Reasoning</h2><p><strong>Authors:Ha-Thanh Nguyen, Chaoran Liu, Qianying Liu, Hideyuki Tachibana, Su Myat Noe, Yusuke Miyao, Koichi Takeda, Sadao Kurohashi</strong></p>
<p>We present BIS Reasoning 1.0, the first large-scale Japanese dataset of syllogistic reasoning problems explicitly designed to evaluate belief-inconsistent reasoning in large language models (LLMs). Unlike prior datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent syllogisms to uncover reasoning biases in LLMs trained on human-aligned corpora. We benchmark state-of-the-art models - including GPT models, Claude models, and leading Japanese LLMs - revealing significant variance in performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies critical weaknesses in current LLMs when handling logically valid but belief-conflicting inputs. These findings have important implications for deploying LLMs in high-stakes domains such as law, healthcare, and scientific literature, where truth must override intuitive belief to ensure integrity and safety. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†BIS Reasoning 1.0ï¼Œè¿™æ˜¯é¦–ä¸ªå¤§è§„æ¨¡æ—¥è¯­é›†åˆæ¨ç†æ•°æ®é›†ï¼Œä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„ä¿¡å¿µä¸ä¸€è‡´æ¨ç†ã€‚ä¸åŒäºå…ˆå‰ä¸“æ³¨äºä¸€èˆ¬æˆ–ä¿¡å¿µä¸€è‡´çš„æ¨ç†çš„NeuBAROCOå’ŒJFLDæ•°æ®é›†ï¼ŒBIS Reasoning 1.0å¼•å…¥äº†é€»è¾‘ä¸Šæœ‰æ•ˆä½†ä¿¡å¿µä¸ä¸€è‡´çš„æ’æ¯”è®ºè¯ï¼Œä»¥æ­ç¤ºè®­ç»ƒæœ‰ç´ çš„è¯­è¨€æ¨¡å‹åœ¨å—è¿‡äººç±»ç›¸å…³è¯­æ–™åº“è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†åè§ã€‚æˆ‘ä»¬åŸºå‡†æµ‹è¯•äº†æœ€å…ˆè¿›æ¨¡å‹çš„è¡¨ç°æ°´å¹³ï¼ŒåŒ…æ‹¬GPTæ¨¡å‹ã€Claudeæ¨¡å‹å’Œé¢†å…ˆçš„æ—¥æœ¬LLMæ¨¡å‹ï¼Œå‘ç°æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ŒGPT-4oå‡†ç¡®ç‡è¾¾åˆ°äº†79.54%ã€‚æˆ‘ä»¬çš„åˆ†æç¡®å®šäº†å½“å‰LLMåœ¨å¤„ç†é€»è¾‘ä¸Šæœ‰æ•ˆä½†ä¿¡å¿µç›¸å†²çªçš„è¾“å…¥æ—¶çš„å…³é”®å¼±ç‚¹ã€‚è¿™äº›å‘ç°åœ¨æ³•å¾‹ã€åŒ»ç–—ä¿å¥å’Œç§‘å­¦æ–‡çŒ®ç­‰é«˜é£é™©é¢†åŸŸéƒ¨ç½²LLMæ—¶å…·æœ‰é‡è¦æ„ä¹‰ï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œå¿…é¡»è®©çœŸç†å‡Œé©¾äºç›´è§‰ä¿¡å¿µä¹‹ä¸Šï¼Œä»¥ç¡®ä¿å®Œæ•´æ€§å’Œå®‰å…¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06955v4">PDF</a> This version includes minor typo corrections in the example image</p>
<p><strong>Summary</strong>ï¼šæˆ‘ä»¬ä»‹ç»äº†BIS Reasoning 1.0ï¼Œå®ƒæ˜¯é¦–ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°ä¿¡å¿µä¸ä¸€è‡´æ¨ç†èƒ½åŠ›çš„å¤§è§„æ¨¡æ—¥è¯­æ•°æ®é›†ã€‚ä¸ä»¥å¾€çš„NeuBARTå’ŒJFLDç­‰æ•°æ®é›†ç›¸æ¯”ï¼ŒBIS Reasoning 1.0å¼•å…¥äº†é€»è¾‘ä¸Šæœ‰æ•ˆä½†ä¿¡å¿µä¸ä¸€è‡´çš„æ¨ç†é—®é¢˜ï¼Œä»¥æ­ç¤ºåœ¨åŸºäºäººç±»è¯­æ–™åº“è®­ç»ƒçš„LLMä¸­å­˜åœ¨çš„æ¨ç†åè§ã€‚æˆ‘ä»¬å¯¹å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬GPTæ¨¡å‹å’ŒClaudeæ¨¡å‹ç­‰é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚å‘ç°æ€§èƒ½æ˜¾è‘—å‚å·®ä¸é½ï¼ŒGPT-4oçš„å‡†ç¡®ç‡ä¸º79.54%ã€‚æˆ‘ä»¬çš„åˆ†æç¡®å®šäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é€»è¾‘æœ‰æ•ˆä½†ä¿¡å¿µå†²çªçš„è¾“å…¥æ—¶çš„å…³é”®å¼±ç‚¹ã€‚è¿™äº›å‘ç°å¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²åœ¨æ³•å¾‹ã€åŒ»ç–—ä¿å¥å’Œç§‘å­¦æ–‡çŒ®ç­‰é«˜é£é™©é¢†åŸŸå…·æœ‰é‡è¦çš„å¯ç¤ºæ„ä¹‰ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œéœ€è¦çœŸç›¸å‹å€’ç›´è§‰ä¿¡ä»°æ¥ç¡®ä¿è¯šä¿¡å’Œå®‰å…¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>BIS Reasoning 1.0æ˜¯é¦–ä¸ªä¸“æ³¨äºä¿¡å¿µä¸ä¸€è‡´æ¨ç†çš„å¤§è§„æ¨¡æ—¥è¯­æ•°æ®é›†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é€»è¾‘æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>ä¸å…¶ä»–æ•°æ®é›†ç›¸æ¯”ï¼ŒBIS Reasoning 1.0å¼•å…¥é€»è¾‘ä¸Šæœ‰æ•ˆä½†ä¿¡å¿µä¸ä¸€è‡´çš„æ¨ç†é—®é¢˜ã€‚</li>
<li>å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é€»è¾‘æœ‰æ•ˆä½†ä¿¡å¿µå†²çªçš„è¾“å…¥æ—¶å­˜åœ¨å…³é”®å¼±ç‚¹ã€‚</li>
<li>GPT-4oåœ¨BIS Reasoning 1.0æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º79.54%ã€‚</li>
<li>ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¡¨ç°å‡ºæ˜¾è‘—çš„å·®å¼‚ã€‚</li>
<li>æ­¤ç ”ç©¶å¯¹äºå°†å¤§å‹è¯­è¨€æ¨¡å‹éƒ¨ç½²åœ¨é«˜é£é™©é¢†åŸŸï¼Œå¦‚æ³•å¾‹ã€åŒ»ç–—å’Œç§‘å­¦æ–‡çŒ®ï¼Œå…·æœ‰é‡è¦çš„å¯ç¤ºæ„ä¹‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-197292c4f3fb26afcc51456a5dd167a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbdd550e465882b24d992a9e84cbc064.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60709b7a434fc511677607cf40483f19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5111e14c64e0ff4d2f5b3d29ca8a45c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8b41c40e14f0d2a65f0a2594d7ac1f4.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cca6f327a9e0a5dece7366cde8a1f565.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  FalseReject A Resource for Improving Contextual Safety and Mitigating   Over-Refusals in LLMs via Structured Reasoning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-16/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c344a8b0018e5f5d067bd1cc640872a9.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-16  ARMOR Aligning Secure and Safe Large Language Models via Meticulous   Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28791.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
