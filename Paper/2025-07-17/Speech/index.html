<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  FasTUSS Faster Task-Aware Unified Source Separation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-66e76953ba7badc26515e06b5223ce49.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    62 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-17-æ›´æ–°"><a href="#2025-07-17-æ›´æ–°" class="headerlink" title="2025-07-17 æ›´æ–°"></a>2025-07-17 æ›´æ–°</h1><h2 id="FasTUSS-Faster-Task-Aware-Unified-Source-Separation"><a href="#FasTUSS-Faster-Task-Aware-Unified-Source-Separation" class="headerlink" title="FasTUSS: Faster Task-Aware Unified Source Separation"></a>FasTUSS: Faster Task-Aware Unified Source Separation</h2><p><strong>Authors:Francesco Paissan, Gordon Wichern, Yoshiki Masuyama, Ryo Aihara, FranÃ§ois G. Germain, Kohei Saijo, Jonathan Le Roux</strong></p>
<p>Time-Frequency (TF) dual-path models are currently among the best performing audio source separation network architectures, achieving state-of-the-art performance in speech enhancement, music source separation, and cinematic audio source separation. While they are characterized by a relatively low parameter count, they still require a considerable number of operations, implying a higher execution time. This problem is exacerbated by the trend towards bigger models trained on large amounts of data to solve more general tasks, such as the recently introduced task-aware unified source separation (TUSS) model. TUSS, which aims to solve audio source separation tasks using a single, conditional model, is built upon TF-Locoformer, a TF dual-path model combining convolution and attention layers. The task definition comes in the form of a sequence of prompts that specify the number and type of sources to be extracted. In this paper, we analyze the design choices of TUSS with the goal of optimizing its performance-complexity trade-off. We derive two more efficient models, FasTUSS-8.3G and FasTUSS-11.7G that reduce the original modelâ€™s operations by 81% and 73% with minor performance drops of 1.2<del>dB and 0.4</del>dB averaged over all benchmarks, respectively. Additionally, we investigate the impact of prompt conditioning to derive a causal TUSS model. </p>
<blockquote>
<p>æ—¶é—´é¢‘ç‡ï¼ˆTFï¼‰åŒè·¯å¾„æ¨¡å‹æ˜¯ç›®å‰è¡¨ç°æœ€å¥½çš„éŸ³é¢‘æºåˆ†ç¦»ç½‘ç»œæ¶æ„ä¹‹ä¸€ï¼Œåœ¨è¯­éŸ³å¢å¼ºã€éŸ³ä¹æºåˆ†ç¦»å’Œç”µå½±éŸ³é¢‘æºåˆ†ç¦»ç­‰é¢†åŸŸè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½æ°´å¹³ã€‚è™½ç„¶è¿™äº›æ¨¡å‹å‚æ•°ç›¸å¯¹è¾ƒå°‘ï¼Œä½†å®ƒä»¬ä»ç„¶éœ€è¦å¤§é‡çš„è¿ç®—ï¼Œå¯¼è‡´æ‰§è¡Œæ—¶é—´è¾ƒé«˜ã€‚éšç€åœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒæ›´å¤§æ¨¡å‹ä»¥è§£å†³æ›´ä¸€èˆ¬ä»»åŠ¡çš„è¶‹åŠ¿ï¼Œè¿™ä¸€é—®é¢˜è¿›ä¸€æ­¥åŠ å‰§ï¼Œä¾‹å¦‚æœ€è¿‘æ¨å‡ºçš„ä»»åŠ¡æ„ŸçŸ¥ç»Ÿä¸€æºåˆ†ç¦»ï¼ˆTUSSï¼‰æ¨¡å‹ã€‚TUSSæ—¨åœ¨ä½¿ç”¨ä¸€ä¸ªæ¡ä»¶æ¨¡å‹è§£å†³éŸ³é¢‘æºåˆ†ç¦»ä»»åŠ¡ï¼Œå®ƒå»ºç«‹åœ¨TF-Locoformerçš„åŸºç¡€ä¸Šï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†å·ç§¯å’Œæ³¨æ„åŠ›å±‚çš„TFåŒè·¯å¾„æ¨¡å‹ã€‚ä»»åŠ¡å®šä¹‰çš„å½¢å¼æ˜¯ä¸€ç³»åˆ—æç¤ºï¼ŒæŒ‡å®šè¦æå–çš„æºçš„æ•°é‡å’Œç±»å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†TUSSçš„è®¾è®¡é€‰æ‹©ï¼Œæ—¨åœ¨ä¼˜åŒ–å…¶æ€§èƒ½å¤æ‚æ€§æƒè¡¡ã€‚æˆ‘ä»¬æ¨å‡ºäº†ä¸¤ä¸ªæ›´æœ‰æ•ˆçš„æ¨¡å‹FasTUSS-8.3Gå’ŒFasTUSS-11.7Gï¼Œå®ƒä»¬å°†åŸå§‹æ¨¡å‹çš„è¿ç®—é‡åˆ†åˆ«å‡å°‘äº†81%å’Œ73%ï¼Œåœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½å¹³å‡åˆ†åˆ«ä¸‹é™äº†1.2dBå’Œ0.4dBã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†æç¤ºæ¡ä»¶çš„å½±å“ï¼Œä»¥æ¨å¯¼å‡ºå› æœTUSSæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11435v1">PDF</a> Accepted to WASPAA 2025</p>
<p><strong>Summary</strong><br>     æ—¶åºåŒè·¯å¾„æ¨¡å‹æ˜¯ç›®å‰è¡¨ç°æœ€ä½³çš„éŸ³é¢‘æºåˆ†ç¦»ç½‘ç»œæ¶æ„ä¹‹ä¸€ï¼Œå¹¿æ³›åº”ç”¨äºè¯­éŸ³å¢å¼ºã€éŸ³ä¹æºåˆ†ç¦»å’Œç”µå½±éŸ³é¢‘æºåˆ†ç¦»ç­‰é¢†åŸŸã€‚é’ˆå¯¹æ­¤æ¶æ„è¿ç®—é‡å¤§ã€æ‰§è¡Œæ—¶é—´é•¿çš„é—®é¢˜ï¼Œæœ¬æ–‡åˆ†æäº†TUSSæ¨¡å‹çš„è®¾è®¡é€‰æ‹©ï¼Œå¹¶ä¼˜åŒ–å…¶æ€§èƒ½å¤æ‚åº¦æƒè¡¡ï¼Œæ¨å‡ºæ›´åŠ é«˜æ•ˆçš„FasTUSS-8.3Gå’ŒFasTUSS-11.7Gæ¨¡å‹ï¼Œåˆ†åˆ«å°†åŸå§‹æ¨¡å‹è¿ç®—é‡é™ä½äº†81%å’Œ73%ï¼Œåœ¨å„é¡¹åŸºå‡†æµ‹è¯•ä¸­çš„æ€§èƒ½ä¸‹é™åˆ†åˆ«ä¸º1.2dBå’Œ0.4dBã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†åŸºäºæç¤ºæ¡ä»¶çš„å› æœTUSSæ¨¡å‹çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶åºåŒè·¯å¾„æ¨¡å‹æ˜¯ç›®å‰éŸ³é¢‘æºåˆ†ç¦»é¢†åŸŸçš„æœ€ä½³ç½‘ç»œæ¶æ„ä¹‹ä¸€ã€‚</li>
<li>TUSSæ¨¡å‹æ—¨åœ¨ä½¿ç”¨å•ä¸€æ¡ä»¶æ¨¡å‹è§£å†³éŸ³é¢‘æºåˆ†ç¦»ä»»åŠ¡ã€‚</li>
<li>TUSSæ¨¡å‹åŸºäºTF-Locoformeræ¶æ„ï¼Œç»“åˆäº†å·ç§¯å’Œæ³¨æ„åŠ›å±‚ã€‚</li>
<li>æœ¬æ–‡åˆ†æäº†TUSSæ¨¡å‹çš„è®¾è®¡é€‰æ‹©ï¼Œæ—¨åœ¨ä¼˜åŒ–å…¶æ€§èƒ½å¤æ‚åº¦ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>æ¨å‡ºæ›´é«˜æ•ˆçš„FasTUSS-8.3Gå’ŒFasTUSS-11.7Gæ¨¡å‹ï¼Œé™ä½äº†è¿ç®—é‡å¹¶ä¿æŒäº†è¾ƒå¥½çš„æ€§èƒ½ã€‚</li>
<li>FasTUSSæ¨¡å‹æ€§èƒ½ä¸‹é™ç¨‹åº¦åˆ†åˆ«ä¸º1.2dBå’Œ0.4dBã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11435">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a58a6c75f0a03591fc3682877f23b33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e44d662d74e2cd965369b9dfd9242bb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-609a95985acb013702fdcb3db28c7fae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-228908453ec12a25f5a651b86193af97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b13fe91a92ca56ca424bb91f43675b6f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="WhisperKit-On-device-Real-time-ASR-with-Billion-Scale-Transformers"><a href="#WhisperKit-On-device-Real-time-ASR-with-Billion-Scale-Transformers" class="headerlink" title="WhisperKit: On-device Real-time ASR with Billion-Scale Transformers"></a>WhisperKit: On-device Real-time ASR with Billion-Scale Transformers</h2><p><strong>Authors:Atila Orhon, Arda Okan, Berkin Durmus, Zach Nagengast, Eduardo Pacheco</strong></p>
<p>Real-time Automatic Speech Recognition (ASR) is a fundamental building block for many commercial applications of ML, including live captioning, dictation, meeting transcriptions, and medical scribes. Accuracy and latency are the most important factors when companies select a system to deploy. We present WhisperKit, an optimized on-device inference system for real-time ASR that significantly outperforms leading cloud-based systems. We benchmark against server-side systems that deploy a diverse set of models, including a frontier model (OpenAI gpt-4o-transcribe), a proprietary model (Deepgram nova-3), and an open-source model (Fireworks large-v3-turbo).Our results show that WhisperKit matches the lowest latency at 0.46s while achieving the highest accuracy 2.2% WER. The optimizations behind the WhisperKit system are described in detail in this paper. </p>
<blockquote>
<p>å®æ—¶è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ˜¯æœºå™¨å­¦ä¹ è®¸å¤šå•†ä¸šåº”ç”¨çš„åŸºç¡€æ„ä»¶ï¼ŒåŒ…æ‹¬å®æ—¶å­—å¹•ã€å¬å†™ã€ä¼šè®®è®°å½•å’ŒåŒ»ç–—è®°å½•ç­‰ã€‚å…¬å¸åœ¨é€‰æ‹©éƒ¨ç½²ç³»ç»Ÿæ—¶ï¼Œå‡†ç¡®æ€§å’Œå»¶è¿Ÿæ˜¯æœ€é‡è¦çš„å› ç´ ã€‚æˆ‘ä»¬æ¨å‡ºäº†WhisperKitï¼Œè¿™æ˜¯ä¸€ä¸ªä¼˜åŒ–çš„å®æ—¶ASRè®¾å¤‡ç«¯æ¨ç†ç³»ç»Ÿï¼Œå®ƒæ˜¾è‘—ä¼˜äºé¢†å…ˆçš„äº‘ç³»ç»Ÿã€‚æˆ‘ä»¬ä»¥æœåŠ¡å™¨ç«¯ç³»ç»Ÿä¸ºåŸºå‡†è¿›è¡Œæµ‹è¯•ï¼Œè¿™äº›ç³»ç»Ÿéƒ¨ç½²äº†å¤šç§æ¨¡å‹ï¼ŒåŒ…æ‹¬å‰æ²¿æ¨¡å‹ï¼ˆOpenAI gpt-4o-transcribeï¼‰ã€ä¸“æœ‰æ¨¡å‹ï¼ˆDeepgram nova-3ï¼‰å’Œå¼€æºæ¨¡å‹ï¼ˆFireworks large-v3-turboï¼‰ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒWhisperKitåœ¨è¾¾åˆ°æœ€ä½å»¶è¿Ÿ0.46ç§’çš„åŒæ—¶ï¼Œå®ç°äº†æœ€é«˜çš„å‡†ç¡®æ€§ï¼Œè¯é”™è¯¯ç‡ä¸º2.2%ã€‚WhisperKitç³»ç»ŸèƒŒåçš„ä¼˜åŒ–ç»†èŠ‚éƒ½è¯¦ç»†æè¿°åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10860v1">PDF</a> ICML 2025 - On-Device Learning for Foundational Models Workshop</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å®æ—¶è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ˜¯æœºå™¨å­¦ä¹ è®¸å¤šå•†ä¸šåº”ç”¨çš„åŸºç¡€æ„ä»¶ï¼ŒåŒ…æ‹¬å®æ—¶å­—å¹•ã€å¬å†™ã€ä¼šè®®è½¬å½•å’ŒåŒ»ç–—è®°å½•ç­‰ã€‚å…¬å¸åœ¨é€‰æ‹©éƒ¨ç½²ç³»ç»Ÿæ—¶ï¼Œå‡†ç¡®æ€§å’Œå»¶è¿Ÿæ˜¯æœ€é‡è¦çš„å› ç´ ã€‚æœ¬æ–‡ä»‹ç»äº†WhisperKitï¼Œè¿™æ˜¯ä¸€ç§ä¼˜åŒ–çš„å®æ—¶ASRè®¾å¤‡ç«¯æ¨ç†ç³»ç»Ÿï¼Œæ˜¾è‘—ä¼˜äºé¢†å…ˆçš„äº‘ç³»ç»Ÿã€‚æˆ‘ä»¬ä»¥éƒ¨ç½²äº†å„ç§æ¨¡å‹çš„æœåŠ¡å™¨ç«¯ç³»ç»Ÿä¸ºåŸºå‡†è¿›è¡Œäº†è¯„ä¼°ï¼ŒåŒ…æ‹¬å‰æ²¿æ¨¡å‹ï¼ˆOpenAI gpt-4o-transcribeï¼‰ã€ä¸“æœ‰æ¨¡å‹ï¼ˆDeepgram nova-3ï¼‰å’Œå¼€æºæ¨¡å‹ï¼ˆFireworks large-v3-turboï¼‰ã€‚ç»“æœè¡¨æ˜ï¼ŒWhisperKitåœ¨è¾¾åˆ°æœ€ä½å»¶è¿Ÿ0.46ç§’çš„åŒæ—¶å®ç°äº†æœ€é«˜çš„å‡†ç¡®æ€§ï¼Œå­—é”™è¯¯ç‡ä¸º2.2%ã€‚æœ¬æ–‡è¯¦ç»†æè¿°äº†WhisperKitç³»ç»Ÿçš„ä¼˜åŒ–è¿‡ç¨‹ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å®æ—¶è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åœ¨è®¸å¤šå•†ä¸šåº”ç”¨ä¸­å…·æœ‰å…³é”®ä½œç”¨ï¼Œå¦‚å®æ—¶å­—å¹•ã€å¬å†™ç­‰ã€‚</li>
<li>å‡†ç¡®æ€§ä¸å»¶è¿Ÿæ˜¯é€‰æ‹©è¯­éŸ³è¯†åˆ«ç³»ç»Ÿæœ€é‡è¦çš„ä¸¤ä¸ªå› ç´ ã€‚</li>
<li>WhisperKitæ˜¯ä¸€ä¸ªä¼˜åŒ–çš„å®æ—¶ASRæ‰‹æœºç«¯æ¨ç†ç³»ç»Ÿï¼Œè¡¨ç°æ˜¾è‘—ä¼˜äºå¤šæ•°äº‘ç³»ç»Ÿã€‚</li>
<li>WhisperKitçš„åŸºå‡†æµ‹è¯•åŒ…æ‹¬ä¸å¤šç§æœåŠ¡å™¨ç«¯ç³»ç»Ÿå¯¹æ¯”ï¼ŒåŒ…æ‹¬å‰æ²¿æ¨¡å‹å¦‚OpenAI gpt-4o-transcribeã€‚</li>
<li>WhisperKitåœ¨å»¶è¿Ÿå’Œå‡†ç¡®æ€§æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå»¶è¿Ÿè¾¾åˆ°æœ€ä½çš„0.46ç§’ï¼Œå­—é”™è¯¯ç‡ä¸º2.2%ã€‚</li>
<li>WhisperKitçš„ä¼˜åŒ–è¿‡ç¨‹åœ¨è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚</li>
<li>è¯¥ç³»ç»Ÿå¯ä¸ºå•†ä¸šåº”ç”¨æä¾›é«˜æ•ˆã€å‡†ç¡®çš„å®æ—¶è¯­éŸ³è¯†åˆ«æœåŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-53ead9cc8cfaffc7a190ab4dd3d69cbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b941669d3e7b58919a41ac6253cb2019.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-966a8dbb40f81feaf73bed7547fe96a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85c4e2bb1636b25506259b77f802b170.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cab9e6e8c2168ffa49f946ea52327e36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f7b190358e973a1cda5eacf259eb670.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87b53d709485601e807fca202a5b7d1e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Supporting-SENCOTEN-Language-Documentation-Efforts-with-Automatic-Speech-Recognition"><a href="#Supporting-SENCOTEN-Language-Documentation-Efforts-with-Automatic-Speech-Recognition" class="headerlink" title="Supporting SENÄ†OTEN Language Documentation Efforts with Automatic   Speech Recognition"></a>Supporting SENÄ†OTEN Language Documentation Efforts with Automatic   Speech Recognition</h2><p><strong>Authors:Mengzhe Geng, Patrick Littell, Aidan Pine,  PENÃÄ†, Marc Tessier, Roland Kuhn</strong></p>
<p>The SEN&#39;{C}OTEN language, spoken on the Saanich peninsula of southern Vancouver Island, is in the midst of vigorous language revitalization efforts to turn the tide of language loss as a result of colonial language policies. To support these on-the-ground efforts, the community is turning to digital technology. Automatic Speech Recognition (ASR) technology holds great promise for accelerating language documentation and the creation of educational resources. However, developing ASR systems for SEN&#39;{C}OTEN is challenging due to limited data and significant vocabulary variation from its polysynthetic structure and stress-driven metathesis. To address these challenges, we propose an ASR-driven documentation pipeline that leverages augmented speech data from a text-to-speech (TTS) system and cross-lingual transfer learning with Speech Foundation Models (SFMs). An n-gram language model is also incorporated via shallow fusion or n-best restoring to maximize the use of available data. Experiments on the SEN&#39;{C}OTEN dataset show a word error rate (WER) of 19.34% and a character error rate (CER) of 5.09% on the test set with a 57.02% out-of-vocabulary (OOV) rate. After filtering minor cedilla-related errors, WER improves to 14.32% (26.48% on unseen words) and CER to 3.45%, demonstrating the potential of our ASR-driven pipeline to support SEN&#39;{C}OTEN language documentation. </p>
<blockquote>
<p>SENâ€™Câ€™OTENè¯­è¨€æ˜¯è®²äºæ¸©å“¥åå²›å—éƒ¨çš„å¡å°¼åˆ‡åŠå²›çš„ä¸€ç§è¯­è¨€ã€‚ç”±äºæ®–æ°‘è¯­è¨€æ”¿ç­–å¯¼è‡´è¯­è¨€æµå¤±çš„å½¢åŠ¿ä¸¥å³»ï¼Œè¯¥è¯­è¨€æ­£åœ¨ç»å†ç€ä¸€åœºæ¿€çƒˆçš„æŒ¯å…´åŠªåŠ›ã€‚ä¸ºäº†æ”¯æŒè¿™äº›åœ°é¢ä¸Šçš„åŠªåŠ›ï¼Œè¯¥ç¤¾åŒºæ­£è½¬å‘æ•°å­—æŠ€æœ¯ã€‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯å¯¹äºåŠ é€Ÿè¯­è¨€æ–‡çŒ®çš„è®°è½½å’Œæ•™è‚²èµ„æºçš„åˆ›é€ æ–¹é¢æœ‰å¾ˆå¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®æœ‰é™ä»¥åŠç”±èšåˆç»“æ„å’Œå‹åŠ›é©±åŠ¨çš„æ¢è¯å¯¼è‡´çš„æ˜¾è‘—è¯æ±‡å˜åŒ–ï¼Œå¼€å‘é€‚ç”¨äºSENâ€™Câ€™OTENçš„ASRç³»ç»Ÿæ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ASRé©±åŠ¨çš„æ–‡çŒ®è®°å½•ç®¡é“ï¼Œè¯¥ç®¡é“åˆ©ç”¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„å¢å¼ºè¯­éŸ³æ•°æ®ä»¥åŠåŸºäºè¯­éŸ³åŸºç¡€æ¨¡å‹çš„è·¨è¯­è¨€è¿ç§»å­¦ä¹ ã€‚é€šè¿‡æµ…èåˆæˆ–n-bestæ¢å¤æ–¹æ³•èå…¥nå…ƒè¯­è¨€æ¨¡å‹ï¼Œä»¥å……åˆ†åˆ©ç”¨å¯ç”¨æ•°æ®ã€‚åœ¨SENâ€™Câ€™OTENæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œæµ‹è¯•é›†ä¸Šçš„å•è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸º19.34%ï¼Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸º5.09%ï¼Œè¯æ±‡è¡¨å¤–ï¼ˆOOVï¼‰çš„æ¯”ç‡ä¸º57.02%ã€‚è¿‡æ»¤æ‰ä¸cedillaç›¸å…³çš„è½»å¾®é”™è¯¯åï¼ŒWERæé«˜è‡³14.32%ï¼ˆæœªè§è¿‡è¯æ±‡çš„æ¯”ç‡ä¸º26.48%ï¼‰ï¼ŒCERé™è‡³3.45%ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬çš„ASRé©±åŠ¨ç®¡é“åœ¨æ”¯æŒSENâ€™Câ€™OTENè¯­è¨€æ–‡çŒ®è®°å½•æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10827v1">PDF</a> Accepted by ComputEL-8</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½äºæ¸©å“¥åå²›å—éƒ¨è¨å°¼å¥‡åŠå²›çš„æ£®ç§‘ç‰¹æ©è¯­è¨€ï¼ˆSENâ€™C{O}TENï¼‰æ­£å¤„äºæ¿€çƒˆçš„å¤å…´é˜¶æ®µï¼Œä»¥æ‰­è½¬å› æ®–æ°‘è¯­è¨€æ”¿ç­–å¯¼è‡´çš„è¯­è¨€æµå¤±ã€‚ç¤¾åŒºæ­£åœ¨å€ŸåŠ©æ•°å­—æŠ€æœ¯æ¥æ”¯æŒè¿™ä¸€å¤å…´å·¥ä½œï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯æœ‰æœ›åŠ é€Ÿè¯­è¨€è®°å½•å’Œåˆ›å»ºæ•™è‚²èµ„æºã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®æœ‰é™å’Œè¯æ±‡é‡çš„å·¨å¤§å˜åŒ–ï¼ˆæ¥è‡ªå…¶ç»¼åˆç»“æ„å’Œå‹åŠ›é©±åŠ¨çš„å…ƒéŸ³å˜åŒ–ï¼‰ï¼Œå¼€å‘ASRç³»ç»Ÿå¯¹æ£®ç§‘ç‰¹æ©è¯­è¨€å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºASRçš„è¯­è¨€è®°å½•ç®¡é“ï¼Œåˆ©ç”¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„å¢å¼ºè¯­éŸ³æ•°æ®å’Œè·¨è¯­è¨€çš„è¿ç§»å­¦ä¹ ä¸è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMsï¼‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç®¡é“åœ¨æ£®ç§‘ç‰¹æ©æ•°æ®é›†ä¸Šçš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸º14.32%ï¼ˆæœªè§è¯çš„é”™è¯¯ç‡ä¸º26.48%ï¼‰ï¼Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸º3.45%ï¼Œè¯æ˜äº†è¯¥ç®¡é“æ”¯æŒæ£®ç§‘ç‰¹æ©è¯­è¨€è®°å½•çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SENâ€™C{O}TENè¯­è¨€æ­£åœ¨ç»å†å¤å…´ï¼Œä»¥åº”å¯¹æ®–æ°‘è¯­è¨€æ”¿ç­–å¯¼è‡´çš„è¯­è¨€æµå¤±ã€‚</li>
<li>ç¤¾åŒºæ­£åœ¨å€ŸåŠ©æ•°å­—æŠ€æœ¯å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯æ¥æ”¯æŒè¿™ä¸€å¤å…´å·¥ä½œã€‚</li>
<li>å¼€å‘é’ˆå¯¹SENâ€™C{O}TENè¯­è¨€çš„ASRç³»ç»Ÿé¢ä¸´æ•°æ®æœ‰é™å’Œè¯æ±‡å˜åŒ–å¤§çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºASRçš„è¯­è¨€è®°å½•ç®¡é“ï¼Œç»“åˆå¢å¼ºè¯­éŸ³æ•°æ®ã€è·¨è¯­è¨€è¿ç§»å­¦ä¹ å’Œè¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMsï¼‰ã€‚</li>
<li>å®éªŒç»“æœå±•ç¤ºäº†è¯¥ç®¡é“åœ¨æ£®ç§‘ç‰¹æ©æ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œè¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰æœ‰æ‰€é™ä½ã€‚</li>
<li>ç®¡é“å…·æœ‰æ½œåŠ›æ”¯æŒæ£®ç§‘ç‰¹æ©è¯­è¨€çš„è®°å½•å·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d1f45c9a6e0be4d3c16689681cefc0bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1049d4ae6438daf03a9d1aee1979c206.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7faa1cda6a80087ad2fa87826b8ec3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d6c5bc8b128213980f5d37a90da1437.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56d1035616f5dabd94ec322dc72a659a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AudioMAE-learning-better-masked-audio-representations-with-SwiGLU-FFNs"><a href="#AudioMAE-learning-better-masked-audio-representations-with-SwiGLU-FFNs" class="headerlink" title="AudioMAE++: learning better masked audio representations with SwiGLU   FFNs"></a>AudioMAE++: learning better masked audio representations with SwiGLU   FFNs</h2><p><strong>Authors:Sarthak Yadav, Sergios Theodoridis, Zheng-Hua Tan</strong></p>
<p>Masked Autoencoders (MAEs) trained on audio spectrogram patches have emerged as a prominent approach for learning self-supervised audio representations. While several recent papers have evaluated key aspects of training MAEs on audio data, the majority of these approaches still leverage vanilla transformer building blocks, whereas the transformer community has seen steady integration of newer architectural advancements. In this work, we propose AudioMAE++, a revamped audio masked autoencoder with two such enhancements, namely macaron-style transformer blocks with gated linear units. When pretrained on the AudioSet dataset, the proposed AudioMAE++ models outperform existing MAE based approaches on 10 diverse downstream tasks, demonstrating excellent performance on audio classification and speech-based benchmarks. The proposed AudioMAE++ models also demonstrate excellent scaling characteristics, outperforming directly comparable standard MAE baselines with up to 4x more parameters. </p>
<blockquote>
<p>åŸºäºéŸ³é¢‘é¢‘è°±å›¾å—è®­ç»ƒçš„Masked Autoencodersï¼ˆMAEsï¼‰å·²æˆä¸ºå­¦ä¹ è‡ªç›‘ç£éŸ³é¢‘è¡¨ç¤ºçš„ä¸€ç§çªå‡ºæ–¹æ³•ã€‚è™½ç„¶æœ€è¿‘çš„å‡ ç¯‡è®ºæ–‡å·²ç»è¯„ä¼°äº†è®­ç»ƒMAEsåœ¨éŸ³é¢‘æ•°æ®ä¸Šçš„å…³é”®æ–¹é¢ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•ä»ç„¶ä½¿ç”¨æ™®é€šçš„å˜å‹å™¨æ„å»ºå—ï¼Œè€Œå˜å‹å™¨ç¤¾åŒºå·²ç»çœ‹åˆ°äº†æ–°çš„æ¶æ„è¿›æ­¥çš„ç¨³å®šé›†æˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AudioMAE++ï¼Œè¿™æ˜¯ä¸€ä¸ªç¿»æ–°çš„éŸ³é¢‘æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼Œå…·æœ‰ä¸¤ä¸ªè¿™æ ·çš„å¢å¼ºåŠŸèƒ½ï¼Œå³å¸¦æœ‰é—¨æ§çº¿æ€§å•å…ƒçš„å¤¹å¿ƒé¥¼å¹²å¼å˜å‹å™¨å—ã€‚åœ¨AudioSetæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ—¶ï¼Œæ‰€æå‡ºçš„AudioMAE++æ¨¡å‹åœ¨10ä¸ªä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰çš„åŸºäºMAEçš„æ–¹æ³•ï¼Œåœ¨éŸ³é¢‘åˆ†ç±»å’ŒåŸºäºè¯­éŸ³çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„AudioMAE++æ¨¡å‹è¿˜è¡¨ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§ç‰¹å¾ï¼Œåœ¨å‚æ•°æœ€å¤šè¾¾å››å€çš„å¯ç›´æ¥æ¯”è¾ƒçš„æ ‡å‡†MAEåŸºçº¿ä¹‹ä¸Šè¡¨ç°è‰¯å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10464v1">PDF</a> TO APPEAR AT IEEE MLSP 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºéŸ³é¢‘é¢‘è°±å›¾å—çš„Masked Autoencodersï¼ˆMAEsï¼‰å·²æˆä¸ºå­¦ä¹ è‡ªç›‘ç£éŸ³é¢‘è¡¨ç¤ºçš„ä¸»æµæ–¹æ³•ã€‚æœ¬æ–‡æå‡ºAudioMAE++æ¨¡å‹ï¼Œå…¶å¼•å…¥äº†ä¸¤é¡¹æ”¹è¿›ï¼ŒåŒ…æ‹¬ä½¿ç”¨å…·æœ‰é—¨æ§çº¿æ€§å•å…ƒçš„å¤¹å¿ƒé¥¼å¹²é£æ ¼çš„å˜å‹å™¨å—ã€‚åœ¨AudioSetæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒåï¼ŒAudioMAE++æ¨¡å‹åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰çš„MAEæ–¹æ³•ï¼Œå±•ç°äº†å‡ºè‰²çš„éŸ³é¢‘åˆ†ç±»å’Œè¯­éŸ³åŸºå‡†æµ‹è¯•æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒAudioMAE++æ¨¡å‹è¿˜å±•ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§ï¼Œä½¿ç”¨å¤šè¾¾å››å€å‚æ•°çš„å¸¸è§„MAEåŸºçº¿æ¨¡å‹ä¹Ÿèƒ½å®ç°å‡ºè‰²çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Masked Autoencoders (MAEs)æˆä¸ºè‡ªç›‘ç£éŸ³é¢‘è¡¨ç¤ºå­¦ä¹ çš„ä¸»è¦æ–¹æ³•ã€‚</li>
<li>AudioMAE++æ¨¡å‹å¼•å…¥äº†ä¸¤é¡¹æ”¹è¿›ï¼šé‡‡ç”¨å¤¹å¿ƒé¥¼å¹²é£æ ¼çš„å˜å‹å™¨å—å’Œé—¨æ§çº¿æ€§å•å…ƒã€‚</li>
<li>AudioMAE++åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰MAEæ–¹æ³•ã€‚</li>
<li>AudioMAE++æ¨¡å‹åœ¨éŸ³é¢‘åˆ†ç±»å’Œè¯­éŸ³åŸºå‡†æµ‹è¯•ä¸Šå±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>AudioMAE++æ¨¡å‹å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œä½¿ç”¨æ›´å¤šçš„å‚æ•°ä¹Ÿèƒ½å®ç°å‡ºè‰²çš„æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe988b2c20d3c038b1282b6a4e8f18fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dae6f0ccce3de9f07ae720e5429129d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-258bba3fb9928feec33c9cc45cbdbf96.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1599917a6b3fcd4fd8e281e6e9c641ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c003146d37190ee5d1963aa4e117ed72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf7a9ed3466ba614f7ccd13a2f1ca554.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a66ec34256012ec1eff7f8f9e0773a5d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Knowing-When-to-Quit-Probabilistic-Early-Exits-for-Speech-Separation"><a href="#Knowing-When-to-Quit-Probabilistic-Early-Exits-for-Speech-Separation" class="headerlink" title="Knowing When to Quit: Probabilistic Early Exits for Speech Separation"></a>Knowing When to Quit: Probabilistic Early Exits for Speech Separation</h2><p><strong>Authors:Kenny FalkÃ¦r Olsen. Mads Ã˜stergaard, Karl UlbÃ¦k, SÃ¸ren FÃ¸ns Nielsen, Rasmus Malik HÃ¸egh Lindrup, BjÃ¸rn Sand Jensen, Morten MÃ¸rup</strong></p>
<p>In recent years, deep learning-based single-channel speech separation has improved considerably, in large part driven by increasingly compute- and parameter-efficient neural network architectures. Most such architectures are, however, designed with a fixed compute and parameter budget, and consequently cannot scale to varying compute demands or resources, which limits their use in embedded and heterogeneous devices such as mobile phones and hearables. To enable such use-cases we design a neural network architecture for speech separation capable of early-exit, and we propose an uncertainty-aware probabilistic framework to jointly model the clean speech signal and error variance which we use to derive probabilistic early-exit conditions in terms of desired signal-to-noise ratios. We evaluate our methods on both speech separation and enhancement tasks, and we show that a single early-exit model can be competitive with state-of-the-art models trained at many compute and parameter budgets. Our framework enables fine-grained dynamic compute-scaling of speech separation networks while achieving state-of-the-art performance and interpretable exit conditions. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„å•é€šé“è¯­éŸ³åˆ†ç¦»æŠ€æœ¯æœ‰äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ç”±äºç¥ç»ç½‘ç»œæ¶æ„çš„è®¡ç®—å’Œå‚æ•°æ•ˆç‡ä¸æ–­æé«˜ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è¿™æ ·çš„æ¶æ„éƒ½æ˜¯æ ¹æ®å›ºå®šçš„è®¡ç®—å’Œå‚æ•°é¢„ç®—è®¾è®¡çš„ï¼Œå› æ­¤æ— æ³•é€‚åº”å˜åŒ–çš„è®¡ç®—éœ€æ±‚æˆ–èµ„æºï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨åµŒå…¥å¼å’Œå¼‚æ„è®¾å¤‡ï¼ˆå¦‚æ‰‹æœºå’Œå¯ç©¿æˆ´è®¾å¤‡ï¼‰ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†æ”¯æŒè¿™äº›ç”¨ä¾‹ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç”¨äºè¯­éŸ³åˆ†ç¦»çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œè¯¥æ¶æ„å…·å¤‡æå‰é€€å‡ºåŠŸèƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä¸ç¡®å®šæ€§çš„æ¦‚ç‡æ¡†æ¶ï¼Œä»¥è”åˆå»ºæ¨¡å¹²å‡€è¯­éŸ³ä¿¡å·å’Œè¯¯å·®æ–¹å·®ï¼Œå¹¶åˆ©ç”¨å…¶æ¨å¯¼æ¦‚ç‡æå‰é€€å‡ºæ¡ä»¶ï¼Œä»¥è¾¾åˆ°æ‰€éœ€çš„ä¿¡å™ªæ¯”ã€‚æˆ‘ä»¬åœ¨è¯­éŸ³åˆ†ç¦»å’Œå¢å¼ºä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¯æ˜å•ä¸ªæå‰é€€å‡ºæ¨¡å‹å¯ä»¥ä¸åœ¨è®¸å¤šè®¡ç®—å’Œå‚æ•°é¢„ç®—ä¸Šè®­ç»ƒçš„æœ€æ–°æ¨¡å‹ç›¸ç«äº‰ã€‚æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿåœ¨å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°å¯¹è¯­éŸ³åˆ†ç¦»ç½‘ç»œçš„ç²¾ç»†åŠ¨æ€è®¡ç®—ç¼©æ”¾ï¼Œå¹¶æä¾›å¯è§£é‡Šçš„é€€å‡ºæ¡ä»¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09768v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€æ·±åº¦å­¦ä¹ åœ¨å•é€šé“è¯­éŸ³åˆ†ç¦»é¢†åŸŸçš„ä¸æ–­å‘å±•ï¼Œè¿‘å¹´æ¥ç¥ç»ç½‘ç»œæ¶æ„çš„æ•ˆç‡å’Œæ€§èƒ½ä¸æ–­æå‡ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ¶æ„éƒ½æ˜¯é’ˆå¯¹å›ºå®šè®¡ç®—å’Œå‚æ•°é¢„ç®—è®¾è®¡çš„ï¼Œæ— æ³•é€‚åº”ä¸åŒçš„è®¡ç®—éœ€æ±‚æˆ–èµ„æºé™åˆ¶ï¼Œè¿™åœ¨åµŒå…¥å¼å’Œå¼‚æ„è®¾å¤‡ï¼ˆå¦‚æ‰‹æœºå’Œå¯ä½©æˆ´è®¾å¤‡ï¼‰ä¸­çš„åº”ç”¨å—åˆ°é™åˆ¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å…·å¤‡æ—©æœŸé€€å‡ºåŠŸèƒ½çš„è¯­éŸ³åˆ†ç¦»ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå¹¶æå‡ºä¸€ç§ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„æ¦‚ç‡æ¡†æ¶ï¼Œä»¥è”åˆå»ºæ¨¡å¹²å‡€è¯­éŸ³ä¿¡å·å’Œè¯¯å·®æ–¹å·®ï¼Œä»è€Œæ¨å¯¼å‡ºåŸºäºæ‰€éœ€ä¿¡å™ªæ¯”çš„æ¦‚ç‡æ—©æœŸé€€å‡ºæ¡ä»¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è¯­éŸ³åˆ†ç¦»å’Œå¢å¼ºä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œå•ä¸ªæ—©æœŸé€€å‡ºæ¨¡å‹å³å¯ä¸åœ¨å¤šç§è®¡ç®—å’Œå‚æ•°é¢„ç®—ä¸‹è®­ç»ƒçš„æœ€æ–°æ¨¡å‹ç›¸æŠ—è¡¡ã€‚æˆ‘ä»¬çš„æ¡†æ¶å®ç°äº†è¯­éŸ³åˆ†ç¦»ç½‘ç»œçš„ç²¾ç»†åŠ¨æ€è®¡ç®—ç¼©æ”¾ï¼ŒåŒæ—¶è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½å¹¶æä¾›äº†å¯è§£é‡Šçš„æ—©æœŸé€€å‡ºæ¡ä»¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨å•é€šé“è¯­éŸ³åˆ†ç¦»é¢†åŸŸå–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œå½’åŠŸäºæ›´é«˜æ•ˆçš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚</li>
<li>ç°æœ‰æ¶æ„å¤§å¤šé’ˆå¯¹å›ºå®šè®¡ç®—å’Œå‚æ•°é¢„ç®—è®¾è®¡ï¼Œæ— æ³•é€‚åº”ä¸åŒè®¡ç®—éœ€æ±‚æˆ–èµ„æºé™åˆ¶ã€‚</li>
<li>æå‡ºä¸€ç§å…·å¤‡æ—©æœŸé€€å‡ºåŠŸèƒ½çš„è¯­éŸ³åˆ†ç¦»ç¥ç»ç½‘ç»œæ¶æ„ï¼Œé€‚ç”¨äºåµŒå…¥å¼å’Œå¼‚æ„è®¾å¤‡ã€‚</li>
<li>å¼•å…¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„æ¦‚ç‡æ¡†æ¶ï¼Œè”åˆå»ºæ¨¡å¹²å‡€è¯­éŸ³ä¿¡å·å’Œè¯¯å·®æ–¹å·®ã€‚</li>
<li>åŸºäºä¿¡å™ªæ¯”æ¨å¯¼æ¦‚ç‡æ—©æœŸé€€å‡ºæ¡ä»¶ï¼Œå®ç°æ›´ç²¾ç»†çš„åŠ¨æ€è®¡ç®—ç¼©æ”¾ã€‚</li>
<li>æ–¹æ³•åœ¨è¯­éŸ³åˆ†ç¦»å’Œå¢å¼ºä»»åŠ¡ä¸Šè¡¨ç°ç«äº‰åŠ›ï¼Œä¸æœ€æ–°æ¨¡å‹ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09768">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1f08556b0917f17dbaef718fe631e046.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdffe98bfa712205d7f498d59a17a101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd95f6e0475220abb4ba7f61f6035dd2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SC-TSE-Speaker-Consistency-Aware-Target-Speaker-Extraction"><a href="#SC-TSE-Speaker-Consistency-Aware-Target-Speaker-Extraction" class="headerlink" title="SC-TSE: Speaker Consistency-Aware Target Speaker Extraction"></a>SC-TSE: Speaker Consistency-Aware Target Speaker Extraction</h2><p><strong>Authors:Shu Wu, Anbin Qi, Yanzhang Xie, Xiang Xie</strong></p>
<p>Target Speaker Extraction (TSE) uses a reference cue to extract the target speech from a mixture. In TSE systems relying on audio cues, the speaker embedding from the enrolled speech is crucial to performance. However, these embeddings may suffer from speaker identity confusion. Unlike previous studies that focus on improving speaker embedding extraction, we improve TSE performance from the perspective of speaker consistency. In this paper, we propose a speaker consistency-aware target speaker extraction method that incorporates a centroid-based speaker consistency loss. This approach enhances TSE performance by ensuring speaker consistency between the enrolled and extracted speech. In addition, we integrate conditional loss suppression into the training process. The experimental results validate the effectiveness of our proposed methods in advancing the TSE performance. A speech demo is available online.\footnote{<a target="_blank" rel="noopener" href="https://sc-tse.netlify.app/">https://sc-tse.netlify.app/</a> </p>
<blockquote>
<p>ç›®æ ‡è¯´è¯äººæå–ï¼ˆTSEï¼‰ä½¿ç”¨å‚è€ƒçº¿ç´¢ä»æ··åˆè¯­éŸ³ä¸­æå–ç›®æ ‡è¯­éŸ³ã€‚åœ¨ä¾èµ–éŸ³é¢‘çº¿ç´¢çš„TSEç³»ç»Ÿä¸­ï¼Œæ³¨å†Œè¯­éŸ³çš„è¯´è¯äººåµŒå…¥å¯¹æ€§èƒ½è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¿™äº›åµŒå…¥å¯èƒ½ä¼šå—åˆ°è¯´è¯äººèº«ä»½æ··æ·†çš„å½±å“ã€‚ä¸ä»¥å¾€ä¸“æ³¨äºæé«˜è¯´è¯äººåµŒå…¥æå–çš„ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬ä»è¯´è¯äººä¸€è‡´æ€§çš„è§’åº¦æé«˜TSEçš„æ€§èƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè´¨å¿ƒçš„è¯´è¯äººä¸€è‡´æ€§æ„ŸçŸ¥ç›®æ ‡è¯´è¯äººæå–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†åŸºäºè´¨å¿ƒçš„è¯´è¯äººä¸€è‡´æ€§æŸå¤±ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ç¡®ä¿æ³¨å†Œè¯­éŸ³å’Œæå–è¯­éŸ³ä¹‹é—´çš„è¯´è¯äººä¸€è‡´æ€§ï¼Œæé«˜äº†TSEçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ¡ä»¶æŸå¤±æŠ‘åˆ¶é›†æˆåˆ°è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚å®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æé«˜TSEæ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚åœ¨çº¿æä¾›è¯­éŸ³æ¼”ç¤ºã€‚æ³¨ï¼š<a target="_blank" rel="noopener" href="https://sc-tse.netlify.app/%EF%BC%88%E5%9C%A8%E7%BA%BF%E6%BC%94%E7%A4%BA%E9%93%BE%E6%8E%A5%EF%BC%89">https://sc-tse.netlify.app/ï¼ˆåœ¨çº¿æ¼”ç¤ºé“¾æ¥ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09510v1">PDF</a> Accept to Interspeech2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºéŸ³é¢‘æç¤ºçš„ç›®æ ‡è¯­éŸ³æå–ç³»ç»Ÿï¼ˆTarget Speaker Extractionï¼Œç®€ç§°TSEï¼‰ã€‚è¯¥ç³»ç»Ÿä½¿ç”¨åŸºäºè´¨å¿ƒçš„è¯´è¯äººä¸€è‡´æ€§æŸå¤±æ–¹æ³•æé«˜è¯´è¯äººä¸€è‡´æ€§ï¼Œè¿›è€Œæé«˜å·²æ³¨å†Œè¯­éŸ³å’Œç›®æ ‡æå–è¯­éŸ³ä¹‹é—´çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜ç»“åˆäº†æ¡ä»¶æŸå¤±æŠ‘åˆ¶è¿›è¡Œè®­ç»ƒã€‚æœ¬æ–‡æå‡ºçš„ä¸¤ç§æ–¹æ³•æé«˜äº†TSEçš„æ€§èƒ½ã€‚å¯é€šè¿‡åœ¨çº¿è¯­éŸ³æ¼”ç¤ºå±•ç¤ºç›¸å…³å†…å®¹ã€‚å…·ä½“ç½‘å€å·²åœ¨æ–‡ä¸­ç»™å‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TSEç³»ç»Ÿåˆ©ç”¨å‚è€ƒçº¿ç´¢ä»æ··åˆè¯­éŸ³ä¸­æå–ç›®æ ‡è¯­éŸ³ã€‚</li>
<li>éŸ³é¢‘æç¤ºä¸‹çš„TSEç³»ç»Ÿä¸­ï¼Œå·²æ³¨å†Œè¯­éŸ³çš„è¯´è¯äººåµŒå…¥å¯¹æ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>è¯´è¯äººåµŒå…¥å¯èƒ½å—åˆ°è¯´è¯äººèº«ä»½æ··æ·†çš„å½±å“ã€‚</li>
<li>æœ¬æ–‡ä»æé«˜è¯´è¯äººä¸€è‡´æ€§çš„è§’åº¦æ”¹è¿›äº†TSEæ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºè´¨å¿ƒçš„è¯´è¯äººä¸€è‡´æ€§æŸå¤±æ–¹æ³•ï¼Œç¡®ä¿å·²æ³¨å†Œå’Œæå–çš„è¯­éŸ³ä¹‹é—´çš„è¯´è¯äººä¸€è‡´æ€§ã€‚</li>
<li>ç»“åˆæ¡ä»¶æŸå¤±æŠ‘åˆ¶è¿›è¡Œè®­ç»ƒï¼Œè¿›ä¸€æ­¥æé«˜TSEæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-43fc28d510e5870690b262b85afa342e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c667beefa0bfb40f72ddf8d9bb7e7051.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0343895b91edcddd323df34abdc9aaf0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-65faec661cf398db8b629527aa51f395.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="The-DKU-System-for-Multi-Speaker-Automatic-Speech-Recognition-in-MLC-SLM-Challenge"><a href="#The-DKU-System-for-Multi-Speaker-Automatic-Speech-Recognition-in-MLC-SLM-Challenge" class="headerlink" title="The DKU System for Multi-Speaker Automatic Speech Recognition in MLC-SLM   Challenge"></a>The DKU System for Multi-Speaker Automatic Speech Recognition in MLC-SLM   Challenge</h2><p><strong>Authors:Yuke Lin, Ming Cheng, Ze Li, Ming Li</strong></p>
<p>We present the DKU system for Task 2 of the MLC-SLM Challenge, which aims to perform multi-speaker automatic speech recognition directly from raw audio without Oracle speaker labels or time boundaries. Our approach builds upon a diarization-aware framework integrating speaker embeddings and temporal utterance boundaries into a Qwen2.5-based large language model (LLM). Then, we enhance the systemâ€™s multilingual performance by fine-tuning language-specific adapters and LoRA modules within the LLM decoder. Finally, our system achieves the tcpWER of 23.56% and 18.08% on the development and test sets of the MLC-SLM dataset, substantially outperforming the official baseline. </p>
<blockquote>
<p>æˆ‘ä»¬ä¸ºMLC-SLMæŒ‘æˆ˜çš„Task 2æå‡ºäº†DKUç³»ç»Ÿï¼Œæ—¨åœ¨ç›´æ¥ä»åŸå§‹éŸ³é¢‘æ‰§è¡Œå¤šå‘è¨€äººè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼Œæ— éœ€Oracleå‘è¨€äººæ ‡ç­¾æˆ–æ—¶é—´è¾¹ç•Œã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºä¸€ä¸ªèåˆå‘è¨€äººåµŒå…¥å’Œæ—¶åºè¯è¯­è¾¹ç•Œçš„è¯†è¾¨åŒ–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»¥åŸºäºQwen2.5çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåŸºç¡€ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å¾®è°ƒLLMè§£ç å™¨ä¸­çš„è¯­è¨€ç‰¹å®šé€‚é…å™¨å’ŒLoRAæ¨¡å—ï¼Œå¢å¼ºç³»ç»Ÿçš„å¤šè¯­è¨€èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨MLC-SLMæ•°æ®é›†çš„å¼€å‘é›†å’Œæµ‹è¯•é›†ä¸Šå®ç°äº†tcpWERåˆ†åˆ«ä¸º23.56%å’Œ18.08%ï¼Œå¤§å¹…è¶…è¶Šäº†å®˜æ–¹åŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09499v1">PDF</a> Technical Report for MLC-SLM Challenge in Interspeech2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºäº†DKUç³»ç»Ÿï¼Œç”¨äºMLC-SLMæŒ‘æˆ˜ä»»åŠ¡2ï¼Œæ—¨åœ¨ç›´æ¥ä»åŸå§‹éŸ³é¢‘è¿›è¡Œå¤šè¯´è¯è€…è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼Œæ— éœ€Oracleè¯´è¯è€…æ ‡ç­¾æˆ–æ—¶é—´è¾¹ç•Œã€‚ç ”ç©¶é‡‡ç”¨åŸºäºè¯­éŸ³è¯†åˆ«æ¨¡å‹çš„ç»“åˆè¯´è¯è€…åµŒå…¥å’Œä¸´æ—¶è¯è¯­è¾¹ç•Œçš„åŸºäºQwen2.5çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚é€šè¿‡å¾®è°ƒè¯­è¨€ç‰¹å®šé€‚é…å™¨å’ŒLoRAæ¨¡å—ï¼Œå¢å¼ºäº†ç³»ç»Ÿçš„å¤šè¯­è¨€èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œåœ¨MLC-SLMæ•°æ®é›†çš„å¼€å‘å’Œæµ‹è¯•é›†ä¸Šï¼Œè¯¥ç³»ç»Ÿå®ç°äº†tcpWERåˆ†åˆ«ä¸º23.56%å’Œ18.08%ï¼Œæ˜æ˜¾ä¼˜äºå®˜æ–¹åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºDKUç³»ç»Ÿï¼Œé’ˆå¯¹MLC-SLMæŒ‘æˆ˜ä»»åŠ¡2çš„å¤šè¯´è¯è€…è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ã€‚</li>
<li>ç³»ç»Ÿæ— éœ€Oracleè¯´è¯è€…æ ‡ç­¾æˆ–æ—¶é—´è¾¹ç•Œä¿¡æ¯ã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨åŸºäºQwen2.5çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œé›†æˆè¯´è¯è€…åµŒå…¥å’Œä¸´æ—¶è¯è¯­è¾¹ç•Œã€‚</li>
<li>é€šè¿‡å¾®è°ƒè¯­è¨€ç‰¹å®šé€‚é…å™¨å’ŒLoRAæ¨¡å—ï¼Œå¢å¼ºäº†ç³»ç»Ÿçš„å¤šè¯­è¨€èƒ½åŠ›ã€‚</li>
<li>ç³»ç»Ÿåœ¨MLC-SLMæ•°æ®é›†çš„å¼€å‘é›†ä¸Šå®ç°äº†tcpWERä¸º23.56%ã€‚</li>
<li>ç³»ç»Ÿåœ¨MLC-SLMæ•°æ®é›†çš„æµ‹è¯•é›†ä¸Šå®ç°äº†tcpWERä¸º18.08%ï¼Œä¼˜äºå®˜æ–¹åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09499">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3bfd72b308f5f7f3d361c7bd470c5734.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c1de88bda71dcbd687ff39a44ee5058.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Enhancing-ALS-Progression-Tracking-with-Semi-Supervised-ALSFRS-R-Scores-Estimated-from-Ambient-Home-Health-Monitoring"><a href="#Enhancing-ALS-Progression-Tracking-with-Semi-Supervised-ALSFRS-R-Scores-Estimated-from-Ambient-Home-Health-Monitoring" class="headerlink" title="Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores   Estimated from Ambient Home Health Monitoring"></a>Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores   Estimated from Ambient Home Health Monitoring</h2><p><strong>Authors:Noah Marchal, William E. Janes, Mihail Popescu, Xing Song</strong></p>
<p>Clinical monitoring of functional decline in ALS relies on periodic assessments that may miss critical changes occurring between visits. To address this gap, semi-supervised regression models were developed to estimate rates of decline in a case series cohort by targeting ALSFRS- R scale trajectories with continuous in-home sensor monitoring data. Our analysis compared three model paradigms (individual batch learning and cohort-level batch versus incremental fine-tuned transfer learning) across linear slope, cubic polynomial, and ensembled self-attention pseudo-label interpolations. Results revealed cohort homogeneity across functional domains responding to learning methods, with transfer learning improving prediction error for ALSFRS-R subscales in 28 of 32 contrasts (mean RMSE&#x3D;0.20(0.04)), and individual batch learning for predicting the composite scale (mean RMSE&#x3D;3.15(1.25)) in 2 of 3. Self-attention interpolation achieved the lowest prediction error for subscale-level models (mean RMSE&#x3D;0.19(0.06)), capturing complex nonlinear progression patterns, outperforming linear and cubic interpolations in 20 of 32 contrasts, though linear interpolation proved more stable in all ALSFRS-R composite scale models (mean RMSE&#x3D;0.23(0.10)). We identified distinct homogeneity-heterogeneity profiles across functional domains with respiratory and speech exhibiting patient-specific patterns benefiting from personalized incremental adaptation, while swallowing and dressing functions followed cohort-level trajectories suitable for transfer models. These findings suggest that matching learning and pseudo-labeling techniques to functional domain-specific homogeneity-heterogeneity profiles enhances predictive accuracy in ALS progression tracking. Integrating adaptive model selection within sensor monitoring platforms could enable timely interventions and scalable deployment in future multi-center studies. </p>
<blockquote>
<p>ä¸´åºŠç›‘æµ‹è‚Œèç¼©ä¾§ç´¢ç¡¬åŒ–ç—‡ï¼ˆALSï¼‰çš„åŠŸèƒ½è¡°é€€ä¾èµ–äºå®šæœŸè¯„ä¼°ï¼Œå¯èƒ½ä¼šé”™è¿‡å°±è¯ŠæœŸé—´å‘ç”Ÿçš„é‡å¤§å˜åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œå¼€å‘äº†åŠç›‘ç£å›å½’æ¨¡å‹ï¼Œé€šè¿‡é’ˆå¯¹ALSFRS-Ré‡è¡¨è½¨è¿¹ä¸è¿ç»­å±…å®¶ä¼ æ„Ÿå™¨ç›‘æµ‹æ•°æ®ï¼Œä¼°è®¡ç³»åˆ—ç—…ä¾‹çš„è¡°é€€ç‡ã€‚æˆ‘ä»¬çš„åˆ†ææ¯”è¾ƒäº†ä¸‰ç§æ¨¡å‹èŒƒå¼ï¼ˆä¸ªä½“æ‰¹é‡å­¦ä¹ ã€ç¾¤ä½“å±‚é¢æ‰¹é‡å­¦ä¹ ä¸å¢é‡ç²¾ç»†è°ƒæ•´è¿ç§»å­¦ä¹ ï¼‰åœ¨çº¿æ€§æ–œç‡ã€ä¸‰æ¬¡å¤šé¡¹å¼ä»¥åŠé›†æˆè‡ªæ³¨æ„åŠ›ä¼ªæ ‡ç­¾æ’å€¼æ–¹é¢çš„åº”ç”¨ã€‚ç»“æœæ˜¾ç¤ºï¼Œç¾¤ä½“åœ¨å„åŠŸèƒ½é¢†åŸŸå¯¹å­¦ä¹ æ–¹æ³•æœ‰åŒè´¨æ€§ååº”ï¼Œè¿ç§»å­¦ä¹ åœ¨ALSFRS-Rå­é‡è¡¨é¢„æµ‹è¯¯å·®çš„å¯¹æ¯”ä¸­æ”¹å–„äº†28é¡¹ä¸­çš„23é¡¹ï¼ˆå¹³å‡RMSE&#x3D;0.20ï¼ˆ0.04ï¼‰ï¼‰ï¼Œä¸ªä½“æ‰¹é‡å­¦ä¹ åœ¨é¢„æµ‹ç»¼åˆæŒ‡æ ‡æ–¹é¢ä»…åœ¨ä¸¤é¡¹å¯¹æ¯”ä¸­çš„ä¸€é¡¹è¡¨ç°å‡ºè‰²ï¼ˆå¹³å‡RMSE&#x3D;3.15ï¼ˆ1.25ï¼‰ï¼‰ã€‚è‡ªæ³¨æ„åŠ›æ’å€¼åœ¨å­é‡è¡¨çº§åˆ«æ¨¡å‹ä¸­å®ç°äº†æœ€ä½çš„é¢„æµ‹è¯¯å·®ï¼ˆå¹³å‡RMSE&#x3D;0.19ï¼ˆ0.06ï¼‰ï¼‰ï¼Œèƒ½å¤Ÿæ•æ‰å¤æ‚çš„éçº¿æ€§è¿›å±•æ¨¡å¼ï¼Œåœ¨32é¡¹å¯¹æ¯”ä¸­ä¼˜äºçº¿æ€§æ’å€¼å’Œä¸‰æ¬¡æ’å€¼20é¡¹ï¼Œå°½ç®¡çº¿æ€§æ’å€¼åœ¨æ‰€æœ‰ALSFRS-Rç»¼åˆæŒ‡æ ‡æ¨¡å‹ä¸­è¡¨ç°æ›´ç¨³å®šï¼ˆå¹³å‡RMSE&#x3D;0.23ï¼ˆ0.10ï¼‰ï¼‰ã€‚æˆ‘ä»¬ç¡®å®šäº†ä¸åŒåŠŸèƒ½é¢†åŸŸçš„åŒè´¨æ€§-å¼‚è´¨æ€§åˆ†å¸ƒç‰¹å¾ï¼Œå…¶ä¸­å‘¼å¸å’Œè¨€è¯­å…·æœ‰ç‰¹å®šçš„æ‚£è€…æ¨¡å¼ï¼Œå—ç›Šäºä¸ªæ€§åŒ–å¢é‡é€‚åº”ï¼Œè€Œåå’½å’Œç©¿è¡£åŠŸèƒ½éµå¾ªç¾¤ä½“å±‚é¢è½¨è¿¹ï¼Œé€‚åˆè¿ç§»æ¨¡å‹ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå°†å­¦ä¹ ä¸ä¼ªæ ‡ç­¾æŠ€æœ¯ä¸åŠŸèƒ½åŸŸç‰¹å®šçš„åŒè´¨æ€§-å¼‚è´¨æ€§åˆ†å¸ƒç‰¹å¾ç›¸åŒ¹é…ï¼Œå¯æé«˜ALSè¿›å±•è·Ÿè¸ªçš„é¢„æµ‹å‡†ç¡®æ€§ã€‚åœ¨ä¼ æ„Ÿå™¨ç›‘æµ‹å¹³å°ä¸­æ•´åˆè‡ªé€‚åº”æ¨¡å‹é€‰æ‹©å¯ä¸ºæœªæ¥å¤šä¸­å¿ƒç ”ç©¶æä¾›åŠæ—¶çš„å¹²é¢„å’Œå¯è§„æ¨¡åŒ–éƒ¨ç½²çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09460v1">PDF</a> 31 pages, 8 Figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡å¼€å‘åŠç›‘ç£å›å½’æ¨¡å‹ï¼Œä»¥é’ˆå¯¹æ¸å†»äººç—‡ï¼ˆALSï¼‰çš„åŠŸèƒ½è¡°é€€è¿›è¡Œä¸´åºŠç›‘æµ‹ã€‚ç ”ç©¶é‡‡ç”¨è¿ç»­å±…å®¶ä¼ æ„Ÿå™¨ç›‘æµ‹æ•°æ®ï¼Œé€šè¿‡ä¸‰ç§æ¨¡å‹èŒƒå¼ï¼ˆä¸ªä½“æ‰¹é‡å­¦ä¹ ã€ç¾¤ä½“æ°´å¹³æ‰¹é‡å­¦ä¹ ä¸å¢é‡ç²¾ç»†è°ƒæ•´è½¬ç§»å­¦ä¹ ï¼‰åˆ†æALSåŠŸèƒ½è¡°é€€é€Ÿç‡ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œè½¬ç§»å­¦ä¹ åœ¨é¢„æµ‹ALSåŠŸèƒ½æ€§é‡è¡¨ï¼ˆALSFRS-Rï¼‰å­é‡è¡¨æ–¹é¢çš„é¢„æµ‹è¯¯å·®æœ‰æ‰€æ”¹è¿›ï¼Œåœ¨32æ¬¡å¯¹æ¯”ä¸­çš„28æ¬¡è¡¨ç°å‡ºæ›´ä½³çš„å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ã€‚è‡ªæˆ‘æ³¨æ„æ’å€¼åœ¨å­é‡è¡¨çº§åˆ«æ¨¡å‹ä¸­å®ç°æœ€ä½é¢„æµ‹è¯¯å·®ï¼Œå¹¶æ•è·å¤æ‚çš„éçº¿æ€§è¿›å±•æ¨¡å¼ã€‚ç„¶è€Œï¼Œçº¿æ€§æ’å€¼åœ¨ALSFRS-Rç»¼åˆé‡è¡¨æ¨¡å‹ä¸­è¡¨ç°æ›´ç¨³å®šã€‚ç ”ç©¶è¿˜å‘ç°ï¼Œä¸åŒåŠŸèƒ½é¢†åŸŸçš„åŒè´¨æ€§å’Œå¼‚è´¨æ€§ç‰¹å¾å½±å“æ¨¡å‹æ•ˆæœï¼Œå¦‚å‘¼å¸å’Œè¨€è¯­åŠŸèƒ½å—ç›Šäºä¸ªæ€§åŒ–å¢é‡é€‚åº”ï¼Œè€Œåå’½å’Œç©¿è¡£åŠŸèƒ½é€‚åˆä½¿ç”¨è½¬ç§»æ¨¡å‹ã€‚æ€»ä¹‹ï¼Œè¯¥ç ”ç©¶å»ºè®®åŒ¹é…å­¦ä¹ ä¸ä¼ªæ ‡ç­¾æŠ€æœ¯è‡³ç‰¹å®šåŠŸèƒ½é¢†åŸŸçš„åŒè´¨æ€§å’Œå¼‚è´¨æ€§ç‰¹å¾ï¼Œä»¥æé«˜ALSè¿›å±•è·Ÿè¸ªçš„é¢„æµ‹å‡†ç¡®æ€§ã€‚æ•´åˆè‡ªé€‚åº”æ¨¡å‹é€‰æ‹©è¿›å…¥ä¼ æ„Ÿå™¨ç›‘æµ‹å¹³å°å¯å®ç°åŠæ—¶å¹²é¢„å’Œå¤§è§„æ¨¡éƒ¨ç½²ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŠç›‘ç£å›å½’æ¨¡å‹ç”¨äºä¼°è®¡ALSçš„åŠŸèƒ½è¡°é€€é€Ÿç‡ï¼Œä½¿ç”¨è¿ç»­å±…å®¶ä¼ æ„Ÿå™¨ç›‘æµ‹æ•°æ®ã€‚</li>
<li>æ¯”è¾ƒäº†ä¸‰ç§æ¨¡å‹èŒƒå¼ï¼ŒåŒ…æ‹¬ä¸ªä½“æ‰¹é‡å­¦ä¹ ã€ç¾¤ä½“æ°´å¹³æ‰¹é‡å­¦ä¹ ä¸å¢é‡ç²¾ç»†è°ƒæ•´è½¬ç§»å­¦ä¹ ã€‚</li>
<li>è½¬ç§»å­¦ä¹ åœ¨é¢„æµ‹ALSFRS-Rå­é‡è¡¨çš„é¢„æµ‹è¯¯å·®æ–¹é¢è¡¨ç°å‡ºè¾ƒå¥½çš„æ•ˆæœã€‚</li>
<li>è‡ªæˆ‘æ³¨æ„æ’å€¼åœ¨å­é‡è¡¨çº§åˆ«æ¨¡å‹ä¸­å®ç°æœ€ä½é¢„æµ‹è¯¯å·®ï¼Œæ•æ‰å¤æ‚çš„éçº¿æ€§è¿›å±•æ¨¡å¼ã€‚</li>
<li>ä¸åŒåŠŸèƒ½é¢†åŸŸï¼ˆå¦‚å‘¼å¸ã€è¨€è¯­ã€åå’½å’Œç©¿è¡£ï¼‰çš„åŒè´¨æ€§å’Œå¼‚è´¨æ€§ç‰¹å¾å½±å“æ¨¡å‹æ•ˆæœã€‚</li>
<li>ç ”ç©¶å»ºè®®æ ¹æ®åŠŸèƒ½é¢†åŸŸçš„åŒè´¨æ€§å’Œå¼‚è´¨æ€§ç‰¹å¾åŒ¹é…å­¦ä¹ å’Œä¼ªæ ‡ç­¾æŠ€æœ¯ï¼Œä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d205114c2da61d892c8eb50d77894524.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fe68646a1f27403a96f85f7257cdd64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab1cbaf979cf5545f0bab9d12c071a01.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Mixture-of-LoRA-Experts-with-Multi-Modal-and-Multi-Granularity-LLM-Generative-Error-Correction-for-Accented-Speech-Recognition"><a href="#Mixture-of-LoRA-Experts-with-Multi-Modal-and-Multi-Granularity-LLM-Generative-Error-Correction-for-Accented-Speech-Recognition" class="headerlink" title="Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM   Generative Error Correction for Accented Speech Recognition"></a>Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM   Generative Error Correction for Accented Speech Recognition</h2><p><strong>Authors:Bingshen Mu, Kun Wei, Pengcheng Guo, Lei Xie</strong></p>
<p>Despite substantial improvements in ASR, performance tends to degrade when faced with adverse conditions such as speaker accents. Generative error correction (GER) leverages the rich linguistic knowledge and exceptional reasoning ability of LLMs, significantly outperforming typical LM methods. However, it lacks specificity in accented speech scenarios. In this study, we leverage GER to improve the accuracy of transcription predictions by addressing the two primary features of accented speech recognition. To fully leverage pronunciation information, we propose the multi-modal GER, which integrates pronunciation information from the speech modality, and the multi-granularity GER, which incorporates fine-grained phoneme-level information related to pronunciation. These two methods enable the LLM to utilize the pronunciation information of accented speech and the semantic information from word-level hypotheses for accurate transcription predictions through LoRA fine-tuning. On the one hand, we employ a three-stage training strategy to train separate multi-modal GER models for each accent to obtain mono-accent LoRA experts. By adopting our proposed HDMoLE method, which incorporates hierarchical routing and dynamic thresholds within the mixture of LoRA experts, we effectively merge multiple mono-accent LoRA experts within a single multi-modal GER to overcome the challenges posed by accent diversity. On the other hand, multi-granularity GER leverages the N-best word-level and phoneme-level hypotheses generated by the HDMoLE model to predict the final accented speech transcriptions. Experimental results on the multi-accent English dataset demonstrate the efficacy of our proposed methods. Our methods achieve a remarkable relative WER reduction of 67.35% compared to the Whisper-large-v3 baseline. </p>
<blockquote>
<p>å°½ç®¡è¯­éŸ³è¯†åˆ«æŠ€æœ¯ï¼ˆASRï¼‰æœ‰äº†å®è´¨æ€§çš„æ”¹è¿›ï¼Œä½†åœ¨é¢å¯¹å¦‚è¯´è¯è€…å£éŸ³ç­‰ä¸åˆ©æ¡ä»¶æ—¶ï¼Œæ€§èƒ½å¾€å¾€ä¼šæœ‰æ‰€ä¸‹é™ã€‚ç”Ÿæˆé”™è¯¯æ ¡æ­£ï¼ˆGERï¼‰èƒ½å¤Ÿåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸°å¯Œçš„è¯­è¨€çŸ¥è¯†å’Œå‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºå…¸å‹çš„è¯­è¨€æ¨¡å‹æ–¹æ³•ã€‚ç„¶è€Œï¼Œå®ƒåœ¨å¸¦æœ‰å£éŸ³çš„è¯­éŸ³åœºæ™¯ä¸Šç¼ºä¹ç‰¹å¼‚æ€§ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨GERæ¥æé«˜è½¬å½•é¢„æµ‹çš„å‡†ç¡®æ€§ï¼Œé€šè¿‡è§£å†³å¸¦å£éŸ³è¯­éŸ³è¯†åˆ«çš„ä¸¤ä¸ªä¸»è¦ç‰¹å¾æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨å‘éŸ³ä¿¡æ¯ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€GERï¼Œå®ƒç»“åˆäº†è¯­éŸ³æ¨¡æ€çš„å‘éŸ³ä¿¡æ¯ï¼Œä»¥åŠå¤šç²’åº¦GERï¼Œå®ƒç»“åˆäº†ä¸å‘éŸ³ç›¸å…³çš„ç²¾ç»†éŸ³ç´ çº§ä¿¡æ¯ã€‚è¿™ä¸¤ç§æ–¹æ³•ä½¿LLMèƒ½å¤Ÿé€šè¿‡LoRAå¾®è°ƒåˆ©ç”¨å¸¦å£éŸ³è¯­éŸ³çš„å‘éŸ³ä¿¡æ¯å’Œè¯çº§å‡è®¾çš„è¯­ä¹‰ä¿¡æ¯è¿›è¡Œå‡†ç¡®çš„è½¬å½•é¢„æµ‹ã€‚ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé’ˆå¯¹æ¯ç§å£éŸ³è®­ç»ƒå•ç‹¬çš„å¤šæ¨¡æ€GERæ¨¡å‹ï¼Œä»¥è·å¾—å•å£éŸ³LoRAä¸“å®¶ã€‚é€šè¿‡é‡‡ç”¨æˆ‘ä»¬æå‡ºçš„HDMoLEæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å±‚æ¬¡è·¯ç”±å’ŒåŠ¨æ€é˜ˆå€¼åœ¨LoRAä¸“å®¶æ··åˆä½“ä¸­ï¼Œæˆ‘ä»¬æœ‰æ•ˆåœ°å°†å¤šä¸ªå•å£éŸ³LoRAä¸“å®¶åˆå¹¶åˆ°ä¸€ä¸ªå•ä¸€çš„å¤šæ¨¡æ€GERä¸­ï¼Œä»¥å…‹æœå£éŸ³å¤šæ ·æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚å¦ä¸€æ–¹é¢ï¼Œå¤šç²’åº¦GERåˆ©ç”¨HDMoLEæ¨¡å‹ç”Ÿæˆçš„Nä¸ªæœ€ä½³è¯çº§å’ŒéŸ³ç´ çº§å‡è®¾æ¥é¢„æµ‹æœ€ç»ˆçš„å¸¦å£éŸ³è¯­éŸ³è½¬å½•ã€‚åœ¨å¤šå£éŸ³è‹±è¯­æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºWhisper-large-v3åŸºçº¿å®ç°äº†ç›¸å¯¹çš„å­—é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½äº†67.35%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09116v2">PDF</a> IEEE Transactions on Audio, Speech and Language Processing</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶é’ˆå¯¹å¸¦æœ‰å£éŸ³çš„è¯­éŸ³è¯†åˆ«é—®é¢˜ï¼Œæå‡ºäº†å¤šæ¨¡æ€ç”Ÿæˆå¼é”™è¯¯ä¿®æ­£ï¼ˆGERï¼‰æ–¹æ³•å’Œå¤šç²’åº¦GERæ–¹æ³•ï¼Œä»¥æé«˜å‘éŸ³ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œè½¬å½•é¢„æµ‹çš„å­—çº§è¯­ä¹‰ä¿¡æ¯çš„åˆ©ç”¨ã€‚é€šè¿‡LoRAå¾®è°ƒæŠ€æœ¯ï¼Œç»“åˆå£éŸ³çš„å‘éŸ³ä¿¡æ¯å’Œå­—çº§å‡è®¾ä¿¡æ¯ï¼Œå¯¹å¸¦å£éŸ³çš„è¯­éŸ³è¿›è¡Œç²¾å‡†è½¬å½•ã€‚é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œä¸ºæ¯ç§å£éŸ³è®­ç»ƒå•ç‹¬çš„å¤šæ¨¡æ€GERæ¨¡å‹ï¼Œå¹¶åˆ©ç”¨HDMoLEæ–¹æ³•æœ‰æ•ˆåˆå¹¶å¤šä¸ªå•å£éŸ³LoRAä¸“å®¶ï¼Œä»¥åº”å¯¹å£éŸ³å¤šæ ·æ€§å¸¦æ¥çš„æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œå¤šç²’åº¦GERåˆ©ç”¨N-bestçš„å­—çº§å’ŒéŸ³ç´ çº§å‡è®¾è¿›è¡Œæœ€ç»ˆå¸¦å£éŸ³è¯­éŸ³è½¬å½•é¢„æµ‹ã€‚åœ¨å¤šé¡¹è‹±è¯­å£éŸ³æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ç›¸æ¯”åŸºå‡†æ¨¡å‹å®ç°äº†67.35%çš„ç›¸å¯¹WERå‡å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼é”™è¯¯ä¿®æ­£ï¼ˆGERï¼‰åœ¨å¤„ç†å¸¦æœ‰å£éŸ³çš„è¯­éŸ³è¯†åˆ«æ—¶è¡¨ç°çªå‡ºï¼Œç›¸è¾ƒäºä¼ ç»Ÿçš„LMæ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>å¤šæ¨¡æ€GERæ–¹æ³•èåˆäº†è¯­éŸ³æ¨¡æ€çš„å‘éŸ³ä¿¡æ¯ï¼Œæé«˜äº†å‘éŸ³çš„å‡†ç¡®æ€§ã€‚</li>
<li>å¤šç²’åº¦GERæ–¹æ³•ç»“åˆäº†éŸ³ç´ çº§çš„ç²¾ç»†ä¿¡æ¯å’Œå­—çº§çš„è¯­ä¹‰ä¿¡æ¯ï¼Œç”¨äºå‡†ç¡®è½¬å½•å¸¦å£éŸ³çš„è¯­éŸ³ã€‚</li>
<li>é‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œé’ˆå¯¹æ¯ç§å£éŸ³è®­ç»ƒå•ç‹¬çš„å¤šæ¨¡æ€GERæ¨¡å‹ï¼Œä»¥åº”å¯¹å£éŸ³å¤šæ ·æ€§ã€‚</li>
<li>HDMoLEæ–¹æ³•æœ‰æ•ˆåˆå¹¶å¤šä¸ªå•å£éŸ³LoRAä¸“å®¶æ¨¡å‹ï¼Œæé«˜äº†æ¨¡å‹å¤„ç†å¤šç§å£éŸ³çš„èƒ½åŠ›ã€‚</li>
<li>å¤šç²’åº¦GERåˆ©ç”¨N-bestå‡è®¾è¿›è¡Œæœ€ç»ˆè½¬å½•é¢„æµ‹ï¼Œæé«˜äº†é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09116">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f840240a9d373b5dcad6630a8b1151bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d51f21573d5ccba195b4e0d0d83ca8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0c9a6a4b9dcab3cbd6bf738e840c195.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b478a7d57278cbad302443f69119351b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Dynamic-Parameter-Memory-Temporary-LoRA-Enhanced-LLM-for-Long-Sequence-Emotion-Recognition-in-Conversation"><a href="#Dynamic-Parameter-Memory-Temporary-LoRA-Enhanced-LLM-for-Long-Sequence-Emotion-Recognition-in-Conversation" class="headerlink" title="Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence   Emotion Recognition in Conversation"></a>Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence   Emotion Recognition in Conversation</h2><p><strong>Authors:Jialong Mai, Xiaofen Xing, Yawei Li, Zhipeng Li, Jingyuan Xing, Xiangmin Xu</strong></p>
<p>Recent research has focused on applying speech large language model (SLLM) to improve speech emotion recognition (SER). However, the inherently high frame rate in speech modality severely limits the signal processing and understanding capabilities of SLLM. For example, a SLLM with a 4K context window can only process 80 seconds of audio at 50Hz feature sampling rate before reaching its capacity limit. Input token compression methods used in SLLM overlook the continuity and inertia of emotions across multiple conversation turns. This paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual semantics and sentence-level emotion encoding, enabling processing of unlimited-length audio with limited context windows in SLLM. Specifically, DPM progressively encodes sentence-level information and emotions into a temporary LoRA module during inference to effectively â€œmemorizeâ€ the contextual information. We trained an emotion SLLM as a backbone and incorporated our DPM into inference for emotion recognition in conversation (ERC). Experimental results on the IEMOCAP dataset show that DPM significantly improves the emotion recognition capabilities of SLLM when processing long audio sequences, achieving state-of-the-art performance. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶é›†ä¸­åœ¨å°†è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰åº”ç”¨äºæé«˜è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ä¸Šã€‚ç„¶è€Œï¼Œè¯­éŸ³æ¨¡æ€æœ¬èº«çš„é«˜å¸§ç‡ä¸¥é‡é™åˆ¶äº†SLLMçš„ä¿¡å·å¤„ç†å’Œç†è§£èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå…·æœ‰4Kä¸Šä¸‹æ–‡çª—å£çš„SLLMåœ¨50Hzçš„ç‰¹å¾é‡‡æ ·ç‡ä¸‹åªèƒ½å¤„ç†80ç§’çš„éŸ³é¢‘ï¼Œç„¶åå°±ä¼šè¾¾åˆ°å…¶å®¹é‡é™åˆ¶ã€‚SLLMä¸­ä½¿ç”¨çš„è¾“å…¥ä»¤ç‰Œå‹ç¼©æ–¹æ³•å¿½ç•¥äº†æƒ…æ„Ÿåœ¨å¤šè½®å¯¹è¯ä¸­çš„è¿ç»­æ€§å’Œæƒ¯æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å…·æœ‰ä¸Šä¸‹æ–‡è¯­ä¹‰å’Œå¥å­çº§æƒ…æ„Ÿç¼–ç çš„åŠ¨æ€å‚æ•°å†…å­˜ï¼ˆDPMï¼‰æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨SLLMä¸­çš„æœ‰é™ä¸Šä¸‹æ–‡çª—å£ä¸­å¤„ç†æ— é™é•¿åº¦çš„éŸ³é¢‘ã€‚å…·ä½“æ¥è¯´ï¼ŒDPMåœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†å¥å­çº§ä¿¡æ¯å’Œæƒ…æ„Ÿé€æ­¥ç¼–ç åˆ°ä¸´æ—¶çš„LoRAæ¨¡å—ä¸­ï¼Œä»¥æœ‰æ•ˆåœ°â€œè®°å¿†â€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæƒ…æ„ŸSLLMä½œä¸ºä¸»å¹²ï¼Œå¹¶å°†æˆ‘ä»¬çš„DPMç”¨äºå¯¹è¯æƒ…æ„Ÿè¯†åˆ«ï¼ˆERCï¼‰çš„æ¨ç†ã€‚åœ¨IEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤„ç†é•¿éŸ³é¢‘åºåˆ—æ—¶ï¼ŒDPMæ˜¾è‘—æé«˜äº†SLLMçš„æƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09076v1">PDF</a> submitted to EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶å°†è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆSLLMï¼‰åº”ç”¨äºè¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰çš„æ”¹è¿›ã€‚ç„¶è€Œï¼Œè¯­éŸ³æ¨¡æ€çš„é«˜å¸§ç‡ä¸¥é‡é™åˆ¶äº†SLLMçš„ä¿¡å·å¤„ç†å’Œç†è§£èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŠ¨æ€å‚æ•°è®°å¿†ï¼ˆDPMï¼‰æœºåˆ¶ï¼Œç»“åˆä¸Šä¸‹æ–‡è¯­ä¹‰å’Œå¥å­çº§æƒ…æ„Ÿç¼–ç ï¼Œåœ¨æœ‰é™çš„è¯­å¢ƒçª—å£ä¸­å¤„ç†æ— é™é•¿åº¦çš„éŸ³é¢‘ã€‚DPMèƒ½å¤Ÿé€æ­¥å°†å¥å­çº§ä¿¡æ¯å’Œæƒ…æ„Ÿç¼–ç åˆ°ä¸´æ—¶LoRAæ¨¡å—ä¸­ï¼Œæœ‰æ•ˆâ€œè®°å¿†â€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åœ¨IEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDPMåœ¨å¤„ç†é•¿éŸ³é¢‘åºåˆ—æ—¶æ˜¾è‘—æé«˜äº†SLLMçš„æƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›ï¼Œè¾¾åˆ°äº†å…ˆè¿›æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SLLMåœ¨è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ä¸­çš„å±€é™æ€§ï¼šé«˜å¸§ç‡å¯¼è‡´çš„ä¿¡å·å¤„ç†å’Œç†è§£èƒ½åŠ›å—é™ã€‚</li>
<li>DPMæœºåˆ¶ä»‹ç»ï¼šç»“åˆä¸Šä¸‹æ–‡è¯­ä¹‰å’Œå¥å­çº§æƒ…æ„Ÿç¼–ç ï¼Œå¤„ç†æ— é™é•¿åº¦éŸ³é¢‘ã€‚</li>
<li>DPMçš„å·¥ä½œåŸç†ï¼šåœ¨æ¨ç†è¿‡ç¨‹ä¸­é€æ­¥ç¼–ç å¥å­çº§ä¿¡æ¯å’Œæƒ…æ„Ÿåˆ°ä¸´æ—¶LoRAæ¨¡å—ä¸­ã€‚</li>
<li>å®éªŒç»“æœï¼šåœ¨IEMOCAPæ•°æ®é›†ä¸Šï¼ŒDPMæ˜¾è‘—æé«˜SLLMå¤„ç†é•¿éŸ³é¢‘åºåˆ—æ—¶çš„æƒ…æ„Ÿè¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>DPMè¾¾åˆ°å…ˆè¿›æ€§èƒ½æ°´å¹³ã€‚</li>
<li>è¾“å…¥ä»¤ç‰Œå‹ç¼©æ–¹æ³•çš„ä¸è¶³ï¼šå¿½ç•¥æƒ…æ„Ÿçš„è¿ç»­æ€§å’Œæƒ¯æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09076">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-795a820129dd0747a05e68ff98b90a6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29bc00adbfdc6c0488dd37201922ba58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10e2777b2c59f6d48de7b0af7579acc7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SemAlignVC-Enhancing-zero-shot-timbre-conversion-using-semantic-alignment"><a href="#SemAlignVC-Enhancing-zero-shot-timbre-conversion-using-semantic-alignment" class="headerlink" title="SemAlignVC: Enhancing zero-shot timbre conversion using semantic   alignment"></a>SemAlignVC: Enhancing zero-shot timbre conversion using semantic   alignment</h2><p><strong>Authors:Shivam Mehta, Yingru Liu, Zhenyu Tang, Kainan Peng, Vimal Manohar, Shun Zhang, Mike Seltzer, Qing He, Mingbo Ma</strong></p>
<p>Zero-shot voice conversion (VC) synthesizes speech in a target speakerâ€™s voice while preserving linguistic and paralinguistic content. However, timbre leakage-where source speaker traits persist-remains a challenge, especially in neural codec and LLM-based VC, where quantized representations entangle speaker identity with content. We introduce SemAlignVC, an architecture designed to prevent timbre leakage using SemAlign, a novel method that aligns text and audio representations to ensure speaker-independent semantic encoding. This disentangled representation conditions an autoregressive transformer for high-fidelity conversion without explicit speaker embeddings. Experiments show SemAlignVC significantly reduces timbre leakage, outperforming baselines in speaker timbre similarity, intelligibility, and naturalness, making it a robust, privacy-preserving, and generalizable VC solution. Audio samples can be accessed at <a target="_blank" rel="noopener" href="https://shivammehta25.github.io/SemAlignVC/">https://shivammehta25.github.io/SemAlignVC/</a> </p>
<blockquote>
<p>é›¶æ ·æœ¬è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰æŠ€æœ¯èƒ½å¤Ÿåœ¨ç›®æ ‡è¯´è¯è€…çš„å£°éŸ³ä¸­åˆæˆè¯­éŸ³ï¼ŒåŒæ—¶ä¿ç•™è¯­è¨€å’Œå‰¯è¯­è¨€å†…å®¹ã€‚ç„¶è€Œï¼Œå£°çº¹æ³„éœ²ï¼ˆå³æºè¯´è¯è€…çš„ç‰¹å¾æŒç»­å­˜åœ¨ï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºäºç¥ç»ç½‘ç»œç¼–è§£ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„VCä¸­ï¼Œå…¶ä¸­é‡åŒ–è¡¨ç¤ºå°†è¯´è¯äººèº«ä»½ä¸å†…å®¹çº ç¼ åœ¨ä¸€èµ·ã€‚æˆ‘ä»¬å¼•å…¥äº†SemAlignVCï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨é˜²æ­¢å£°çº¹æ³„éœ²çš„æ¶æ„ï¼Œå®ƒä½¿ç”¨äº†SemAlignè¿™ä¸€æ–°æ–¹æ³•ï¼Œé€šè¿‡æ–‡æœ¬å’ŒéŸ³é¢‘è¡¨ç¤ºçš„å¯¹é½æ¥ç¡®ä¿ä¸è¯´è¯äººæ— å…³çš„è¯­ä¹‰ç¼–ç ã€‚è¿™ç§è§£çº ç¼ çš„è¡¨ç¤ºä¸ºä¸€ä¸ªè‡ªå›å½’å˜å‹å™¨æä¾›æ¡ä»¶ï¼Œä»¥å®ç°é«˜ä¿çœŸè½¬æ¢ï¼Œæ— éœ€æ˜ç¡®çš„è¯´è¯äººåµŒå…¥ã€‚å®éªŒè¡¨æ˜ï¼ŒSemAlignVCèƒ½æ˜¾è‘—é™ä½å£°çº¹æ³„éœ²ï¼Œåœ¨è¯´è¯äººå£°çº¹ç›¸ä¼¼æ€§ã€æ¸…æ™°åº¦å’Œè‡ªç„¶åº¦æ–¹é¢è¶…è¶ŠåŸºçº¿ï¼Œæˆä¸ºäº†ä¸€ç§ç¨³å¥ã€ä¿æŠ¤éšç§ä¸”å¯æ¨å¹¿çš„VCè§£å†³æ–¹æ¡ˆã€‚éŸ³é¢‘æ ·æœ¬å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://shivammehta25.github.io/SemAlignVC/%E8%AE%BF%E9%97%AE%E3%80%82">https://shivammehta25.github.io/SemAlignVC/è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09070v1">PDF</a> 6 pages, 2 figures, Accepted at the ISCA Speech Synthesis Workshop   (SSW) 2025</p>
<p><strong>Summary</strong>ï¼šé›¶æ ·æœ¬è¯­éŸ³è½¬æ¢æŠ€æœ¯æ—¨åœ¨åˆæˆç›®æ ‡è¯´è¯äººçš„è¯­éŸ³ï¼ŒåŒæ—¶ä¿ç•™è¯­è¨€å’Œå‰¯è¯­è¨€å†…å®¹ã€‚ç„¶è€Œï¼Œå£°çº¹æ³„éœ²ï¼ˆå³æºè¯´è¯äººçš„ç‰¹å¾æŒç»­å­˜åœ¨ï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¥ç»ç½‘ç»œç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³è½¬æ¢ä¸­ï¼Œé‡åŒ–è¡¨ç¤ºå°†è¯´è¯äººèº«ä»½ä¸å†…å®¹çº ç¼ åœ¨ä¸€èµ·ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SemAlignVCæ¶æ„å’Œå…¶ä¸­çš„SemAlignæ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡å¯¹æ–‡æœ¬å’ŒéŸ³é¢‘è¡¨ç¤ºè¿›è¡Œå¯¹é½æ¥é˜²æ­¢å£°çº¹æ³„éœ²ï¼Œç¡®ä¿ç‹¬ç«‹äºè¯´è¯äººçš„è¯­ä¹‰ç¼–ç ã€‚è¿™ç§åˆ†ç¦»è¡¨ç¤ºæ¡ä»¶è‡ªå›å½’å˜å‹å™¨å¯å®ç°é«˜ä¿çœŸè½¬æ¢ï¼Œæ— éœ€æ˜ç¡®çš„è¯´è¯äººåµŒå…¥ã€‚å®éªŒè¡¨æ˜ï¼ŒSemAlignVCåœ¨å£°çº¹æ³„éœ²æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—é™ä½ï¼Œåœ¨è¯´è¯äººå£°çº¹ç›¸ä¼¼æ€§ã€æ¸…æ™°åº¦å’Œè‡ªç„¶åº¦æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œæˆä¸ºäº†ä¸€ç§ç¨³å¥ã€ä¿æŠ¤éšç§å’Œé€šç”¨çš„è¯­éŸ³è½¬æ¢è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é›¶æ ·æœ¬è¯­éŸ³è½¬æ¢æ—¨åœ¨åˆæˆç›®æ ‡è¯´è¯äººçš„è¯­éŸ³å¹¶ä¿ç•™è¯­è¨€å’Œå‰¯è¯­è¨€å†…å®¹ã€‚</li>
<li>å£°çº¹æ³„éœ²æ˜¯è¯­éŸ³è½¬æ¢æŠ€æœ¯ä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¥ç»ç½‘ç»œç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨ä¸­ã€‚</li>
<li>SemAlignVCæ¶æ„é€šè¿‡SemAlignæ–¹æ³•è§£å†³å£°çº¹æ³„éœ²é—®é¢˜ï¼Œé€šè¿‡å¯¹æ–‡æœ¬å’ŒéŸ³é¢‘è¡¨ç¤ºè¿›è¡Œå¯¹é½æ¥ç¡®ä¿ç‹¬ç«‹äºè¯´è¯äººçš„è¯­ä¹‰ç¼–ç ã€‚</li>
<li>å®éªŒè¯æ˜SemAlignVCåœ¨å£°çº¹æ³„éœ²ã€è¯´è¯äººå£°çº¹ç›¸ä¼¼æ€§ã€æ¸…æ™°åº¦å’Œè‡ªç„¶åº¦æ–¹é¢ä¼˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
<li>SemAlignVCä¸éœ€è¦æ˜ç¡®çš„è¯´è¯äººåµŒå…¥ï¼Œå¯å®ç°é«˜ä¿çœŸè½¬æ¢ã€‚</li>
<li>éŸ³é¢‘æ ·æœ¬å¯é€šè¿‡ç‰¹å®šé“¾æ¥è®¿é—®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8302225aec83c97e3732b81f4df825dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8af15792354acba17c8d2520fc431a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1dd18ce023e2b04f88883d0532afde8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="StreamUni-Achieving-Streaming-Speech-Translation-with-a-Unified-Large-Speech-Language-Model"><a href="#StreamUni-Achieving-Streaming-Speech-Translation-with-a-Unified-Large-Speech-Language-Model" class="headerlink" title="StreamUni: Achieving Streaming Speech Translation with a Unified Large   Speech-Language Model"></a>StreamUni: Achieving Streaming Speech Translation with a Unified Large   Speech-Language Model</h2><p><strong>Authors:Shoutao Guo, Xiang Li, Mengge Liu, Wei Chen, Yang Feng</strong></p>
<p>Streaming speech translation (StreamST) requires determining appropriate timing, known as policy, to generate translations while continuously receiving source speech inputs, balancing low latency with high translation quality. However, existing StreamST methods typically operate on sentence-level speech segments, referred to as simultaneous speech translation (SimulST). In practice, they require collaboration with segmentation models to accomplish StreamST, where the truncated speech segments constrain SimulST models to make policy decisions and generate translations based on limited contextual information. Moreover, SimulST models struggle to learn effective policies due to the complexity of speech inputs and cross-lingual generation. To address these challenges, we propose StreamUni, which achieves StreamST through a unified Large Speech-Language Model (LSLM). Specifically, StreamUni incorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate multi-stage outputs. Leveraging these multi-stage outputs, StreamUni simultaneously accomplishes speech segmentation, policy decision, and translation generation, completing StreamST without requiring massive policy-specific training. Additionally, we propose a streaming CoT training method that enhances low-latency policy decisions and generation capabilities using limited CoT data. Experiments demonstrate that our approach achieves state-of-the-art performance on StreamST tasks. </p>
<blockquote>
<p>æµå¼è¯­éŸ³ç¿»è¯‘ï¼ˆStreamSTï¼‰è¦æ±‚åœ¨è¿ç»­æ¥æ”¶æºè¯­éŸ³è¾“å…¥çš„è¿‡ç¨‹ä¸­ç¡®å®šé€‚å½“çš„ç¿»è¯‘ç”Ÿæˆæ—¶æœºï¼Œè¿™ç§æ—¶æœºè¢«ç§°ä¸ºç­–ç•¥ï¼Œéœ€è¦åœ¨ä½å»¶è¿Ÿå’Œé«˜ç¿»è¯‘è´¨é‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ç„¶è€Œï¼Œç°æœ‰çš„StreamSTæ–¹æ³•é€šå¸¸è¿è¡Œåœ¨å¥å­çº§åˆ«çš„è¯­éŸ³æ®µä¸Šï¼Œç§°ä¸ºåŒæ­¥è¯­éŸ³ç¿»è¯‘ï¼ˆSimulSTï¼‰ã€‚åœ¨å®è·µä¸­ï¼Œå®ƒä»¬éœ€è¦ä¸åˆ†å‰²æ¨¡å‹åä½œæ¥å®ŒæˆStreamSTï¼Œæˆªæ–­çš„è¯­éŸ³æ®µé™åˆ¶SimulSTæ¨¡å‹åœ¨æœ‰é™çš„ä¸Šä¸‹æ–‡ä¿¡æ¯åŸºç¡€ä¸Šåšå‡ºç­–ç•¥å†³ç­–å¹¶ç”Ÿæˆç¿»è¯‘ã€‚è€Œä¸”ï¼Œç”±äºè¯­éŸ³è¾“å…¥çš„å¤æ‚æ€§å’Œè·¨è¯­è¨€ç”Ÿæˆï¼ŒSimulSTæ¨¡å‹åœ¨å­¦ä¹ æœ‰æ•ˆç­–ç•¥æ–¹é¢é¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†StreamUniï¼Œå®ƒé€šè¿‡ç»Ÿä¸€çš„å¤§è§„æ¨¡è¯­éŸ³è¯†åˆ«è¯­è¨€æ¨¡å‹ï¼ˆLSLMï¼‰å®ç°StreamSTã€‚å…·ä½“æ¥è¯´ï¼ŒStreamUniå°†è¯­éŸ³æ€ç»´é“¾ï¼ˆCoTï¼‰èå…¥LSLMï¼Œä»¥æŒ‡å¯¼ç”Ÿæˆå¤šé˜¶æ®µè¾“å‡ºã€‚åˆ©ç”¨è¿™äº›å¤šé˜¶æ®µè¾“å‡ºï¼ŒStreamUniå¯ä»¥åŒæ—¶å®Œæˆè¯­éŸ³åˆ†å‰²ã€ç­–ç•¥å†³ç­–å’Œç¿»è¯‘ç”Ÿæˆï¼Œæ— éœ€å¤§é‡çš„ç‰¹å®šç­–ç•¥è®­ç»ƒå³å¯å®ŒæˆStreamSTã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æµå¼CoTè®­ç»ƒæ–¹æ³•ï¼Œä½¿ç”¨æœ‰é™çš„CoTæ•°æ®å¢å¼ºä½å»¶è¿Ÿçš„ç­–ç•¥å†³ç­–å’Œç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨StreamSTä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07803v2">PDF</a> The code is at <a target="_blank" rel="noopener" href="https://github.com/ictnlp/StreamUni">https://github.com/ictnlp/StreamUni</a>; The model is at   <a target="_blank" rel="noopener" href="https://huggingface.co/ICTNLP/StreamUni-Phi4">https://huggingface.co/ICTNLP/StreamUni-Phi4</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºç»Ÿä¸€å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æµå¼è¯­éŸ³è¯†åˆ«ç¿»è¯‘æ–¹æ³•StreamUniï¼Œé€šè¿‡èå…¥è¯­éŸ³é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç”Ÿæˆå¤šé˜¶æ®µè¾“å‡ºï¼Œå®ç°äº†æ— éœ€å¤§è§„æ¨¡æ”¿ç­–ç‰¹å®šè®­ç»ƒçš„æµå¼è¯­éŸ³è¯†åˆ«ç¿»è¯‘ã€‚æ­¤æ–¹æ³•å¯åŒæ—¶è¿›è¡Œè¯­éŸ³åˆ†æ®µã€æ”¿ç­–å†³ç­–å’Œç¿»è¯‘ç”Ÿæˆï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¸­éœ€è¦åä½œåˆ†æ®µæ¨¡å‹ã€åŸºäºæœ‰é™è¯­å¢ƒä¿¡æ¯åšå‡ºå†³ç­–å’Œç”Ÿæˆç¿»è¯‘çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§å¢å¼ºä½å»¶è¿Ÿæ”¿ç­–å†³ç­–å’Œç”Ÿæˆèƒ½åŠ›çš„æµå¼CoTè®­ç»ƒæ–¹æ³•ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æµå¼è¯­éŸ³è¯†åˆ«ç¿»è¯‘ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ°´å¹³çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æµå¼è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆStreamSTï¼‰æ–¹æ³•é€šå¸¸åŸºäºå¥å­çº§åˆ«çš„è¯­éŸ³æ®µè¿›è¡Œï¼Œç§°ä¸ºåŒæ­¥è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆSimulSTï¼‰ã€‚</li>
<li>SimulSTæ–¹æ³•éœ€è¦ä¸åˆ†æ®µæ¨¡å‹åä½œï¼Œå—æˆªæ–­è¯­éŸ³æ®µçš„é™åˆ¶ï¼ŒåŸºäºæœ‰é™è¯­å¢ƒä¿¡æ¯åšå‡ºå†³ç­–å’Œç”Ÿæˆç¿»è¯‘ã€‚</li>
<li>StreamUnié€šè¿‡ç»Ÿä¸€å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLSLMï¼‰å®ç°StreamSTï¼Œèå…¥è¯­éŸ³é“¾å¼æ€ç»´ï¼ˆCoTï¼‰ç”Ÿæˆå¤šé˜¶æ®µè¾“å‡ºï¼Œå¯åŒæ—¶å®Œæˆè¯­éŸ³åˆ†æ®µã€æ”¿ç­–å†³ç­–å’Œç¿»è¯‘ç”Ÿæˆã€‚</li>
<li>StreamUniæ–¹æ³•æ— éœ€å¤§è§„æ¨¡æ”¿ç­–ç‰¹å®šè®­ç»ƒï¼Œè§£å†³äº†SimulSTæ¨¡å‹åœ¨å¤æ‚è¯­éŸ³è¾“å…¥å’Œè·¨è¯­è¨€ç”Ÿæˆæ–¹é¢å­¦ä¹ æœ‰æ•ˆæ”¿ç­–çš„å›°éš¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¢å¼ºä½å»¶è¿Ÿæ”¿ç­–å†³ç­–å’Œç”Ÿæˆèƒ½åŠ›çš„æµå¼CoTè®­ç»ƒæ–¹æ³•ï¼Œä½¿ç”¨æœ‰é™çš„CoTæ•°æ®ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒStreamUniæ–¹æ³•åœ¨æµå¼è¯­éŸ³è¯†åˆ«ç¿»è¯‘ä»»åŠ¡ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ°´å¹³çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7aa4f9aabf622655863aed051892f6e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb5687da2a0140e0a72d7d68eba827e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39483ed4c1810579c02d11932d5c9584.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96fbccb328040e28f695ae29bb9c45c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a38f2576b62c39aeb1af709c2871479c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DeepGesture-A-conversational-gesture-synthesis-system-based-on-emotions-and-semantics"><a href="#DeepGesture-A-conversational-gesture-synthesis-system-based-on-emotions-and-semantics" class="headerlink" title="DeepGesture: A conversational gesture synthesis system based on emotions   and semantics"></a>DeepGesture: A conversational gesture synthesis system based on emotions   and semantics</h2><p><strong>Authors:Thanh Hoang-Minh</strong></p>
<p>Along with the explosion of large language models, improvements in speech synthesis, advancements in hardware, and the evolution of computer graphics, the current bottleneck in creating digital humans lies in generating character movements that correspond naturally to text or speech inputs.   In this work, we present DeepGesture, a diffusion-based gesture synthesis framework for generating expressive co-speech gestures conditioned on multimodal signals - text, speech, emotion, and seed motion. Built upon the DiffuseStyleGesture model, DeepGesture introduces novel architectural enhancements that improve semantic alignment and emotional expressiveness in generated gestures. Specifically, we integrate fast text transcriptions as semantic conditioning and implement emotion-guided classifier-free diffusion to support controllable gesture generation across affective states. To visualize results, we implement a full rendering pipeline in Unity based on BVH output from the model. Evaluation on the ZeroEGGS dataset shows that DeepGesture produces gestures with improved human-likeness and contextual appropriateness. Our system supports interpolation between emotional states and demonstrates generalization to out-of-distribution speech, including synthetic voices - marking a step forward toward fully multimodal, emotionally aware digital humans.   Project page: <a target="_blank" rel="noopener" href="https://deepgesture.github.io/">https://deepgesture.github.io</a> </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„çˆ†å‘ï¼Œè¯­éŸ³åˆæˆçš„æ”¹è¿›ï¼Œç¡¬ä»¶çš„è¿›æ­¥ä»¥åŠè®¡ç®—æœºå›¾å½¢çš„æ¼”å˜ï¼Œå½“å‰åˆ›å»ºæ•°å­—äººçš„ç“¶é¢ˆåœ¨äºç”Ÿæˆä¸æ–‡æœ¬æˆ–è¯­éŸ³è¾“å…¥ç›¸å¯¹åº”çš„è‡ªç„¶åŠ¨ä½œã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DeepGestureï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ‰‹åŠ¿åˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬ã€è¯­éŸ³ã€æƒ…æ„Ÿå’Œç§å­åŠ¨ä½œç­‰å¤šæ¨¡æ€ä¿¡å·ç”Ÿæˆå¯Œæœ‰è¡¨ç°åŠ›çš„ååŒè¯­éŸ³æ‰‹åŠ¿ã€‚DeepGestureå»ºç«‹åœ¨DiffuseStyleGestureæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œå¼•å…¥äº†æ–°å‹æ¶æ„æ”¹è¿›ï¼Œæé«˜äº†è¯­ä¹‰å¯¹é½å’Œç”Ÿæˆæ‰‹åŠ¿çš„æƒ…æ„Ÿè¡¨ç°åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†å¿«é€Ÿæ–‡æœ¬è½¬å½•ä½œä¸ºè¯­ä¹‰æ¡ä»¶ï¼Œå¹¶å®ç°äº†æƒ…æ„Ÿå¼•å¯¼çš„æ— åˆ†ç±»æ‰©æ•£ï¼Œä»¥æ”¯æŒæƒ…æ„ŸçŠ¶æ€ä¸‹çš„å¯æ§æ‰‹åŠ¿ç”Ÿæˆã€‚ä¸ºäº†å¯è§†åŒ–ç»“æœï¼Œæˆ‘ä»¬åœ¨Unityä¸­å®ç°äº†åŸºäºæ¨¡å‹BVHè¾“å‡ºçš„å®Œæ•´æ¸²æŸ“ç®¡é“ã€‚åœ¨ZeroEGGSæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒDeepGestureäº§ç”Ÿçš„æ‰‹åŠ¿åœ¨äººæ€§åŒ–å’Œä¸Šä¸‹æ–‡æ°å½“æ€§æ–¹é¢æœ‰æ‰€æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿæ”¯æŒæƒ…æ„ŸçŠ¶æ€ä¹‹é—´çš„æ’å€¼ï¼Œå¹¶å±•ç¤ºäº†å¯¹åˆ†å¸ƒå¤–è¯­éŸ³çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¯¹åˆæˆå£°éŸ³çš„æ³›åŒ–â€”â€”æ ‡å¿—ç€æœç€å®Œå…¨å¤šæ¨¡æ€ã€æƒ…æ„Ÿæ„ŸçŸ¥çš„æ•°å­—äººæ–¹å‘è¿ˆå‡ºäº†ä¸€æ­¥ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://deepgesture.github.io/">https://deepgesture.github.io</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03147v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://deepgesture.github.io/">https://deepgesture.github.io</a></p>
<p><strong>Summary</strong><br>æ–‡æœ¬æè¿°äº†DeepGestureç³»ç»ŸåŠå…¶ä¼˜åŠ¿ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹çš„çˆ†ç‚¸å¼å‘å±•ï¼Œä»¥åŠè¯­éŸ³åˆæˆã€ç¡¬ä»¶è¿›æ­¥å’Œè®¡ç®—æœºå›¾å½¢æŠ€æœ¯çš„æå‡ï¼Œå½“å‰åˆ›å»ºæ•°å­—äººç±»çš„ç“¶é¢ˆåœ¨äºç”Ÿæˆè‡ªç„¶å¯¹åº”æ–‡æœ¬æˆ–è¯­éŸ³è¾“å…¥çš„è§’è‰²åŠ¨ä½œã€‚DeepGestureæ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ‰‹åŠ¿åˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æ–‡æœ¬ã€è¯­éŸ³ã€æƒ…æ„Ÿå’Œç§å­åŠ¨ä½œç­‰å¤šæ¨¡å¼ä¿¡å·ç”Ÿæˆè¡¨è¾¾æ€§å…±è¯­æ‰‹åŠ¿ã€‚å®ƒæ”¹è¿›äº†è¯­ä¹‰å¯¹é½å’Œæƒ…ç»ªè¡¨è¾¾ï¼Œæ”¯æŒè·¨æƒ…æ„ŸçŠ¶æ€çš„å¯æ§æ‰‹åŠ¿ç”Ÿæˆã€‚åœ¨Unityä¸­å®ç°çš„å…¨æ¸²æŸ“ç®¡é“ï¼ŒåŸºäºæ¨¡å‹çš„BVHè¾“å‡ºå¯è§†åŒ–ç»“æœã€‚åœ¨ZeroEGGSæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒDeepGestureç”Ÿæˆçš„æ‰‹åŠ¿æ›´å…·äººç±»æ€§å’Œä¸Šä¸‹æ–‡é€‚å½“æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰æ•°å­—äººç±»çš„åˆ›å»ºç“¶é¢ˆåœ¨äºç”Ÿæˆè‡ªç„¶å¯¹åº”æ–‡æœ¬æˆ–è¯­éŸ³è¾“å…¥çš„è§’è‰²åŠ¨ä½œã€‚</li>
<li>DeepGestureæ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ‰‹åŠ¿åˆæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>DeepGestureé›†æˆäº†å¤šæ¨¡å¼ä¿¡å·ï¼Œå¦‚æ–‡æœ¬ã€è¯­éŸ³ã€æƒ…æ„Ÿå’Œç§å­åŠ¨ä½œï¼Œç”¨äºç”Ÿæˆè¡¨è¾¾æ€§å…±è¯­æ‰‹åŠ¿ã€‚</li>
<li>å®ƒé€šè¿‡æ”¹è¿›è¯­ä¹‰å¯¹é½å’Œæƒ…ç»ªè¡¨è¾¾æ¥æé«˜ç”Ÿæˆæ‰‹åŠ¿çš„è‡ªç„¶åº¦å’Œè¡¨è¾¾åŠ›ã€‚</li>
<li>DeepGestureæ”¯æŒè·¨æƒ…æ„ŸçŠ¶æ€çš„å¯æ§æ‰‹åŠ¿ç”Ÿæˆã€‚</li>
<li>åœ¨Unityä¸­å®ç°çš„å…¨æ¸²æŸ“ç®¡é“å¯ä»¥å¯è§†åŒ–ç”Ÿæˆçš„æ‰‹åŠ¿ã€‚</li>
<li>åœ¨ZeroEGGSæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒDeepGestureç”Ÿæˆçš„æ‰‹åŠ¿æ›´å…·äººç±»æ€§å’Œä¸Šä¸‹æ–‡é€‚å½“æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1a5a8b1bcabbcdbe56ee79c054fe0231.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44c1ab7bd87836b3e9a09b1d464cdafd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5d65a590a2074cdf30172ff46b051b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ead5690402d79e82ecadb54846ddc273.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ee73770761afbb7a68fde25caa81cdd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Riemannian-Time-Warping-Multiple-Sequence-Alignment-in-Curved-Spaces"><a href="#Riemannian-Time-Warping-Multiple-Sequence-Alignment-in-Curved-Spaces" class="headerlink" title="Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces"></a>Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces</h2><p><strong>Authors:Julian Richter, Christopher A. ErdÃ¶s, Christian Scheurer, Jochen J. Steil, Niels Dehio</strong></p>
<p>Temporal alignment of multiple signals through time warping is crucial in many fields, such as classification within speech recognition or robot motion learning. Almost all related works are limited to data in Euclidean space. Although an attempt was made in 2011 to adapt this concept to unit quaternions, a general extension to Riemannian manifolds remains absent. Given its importance for numerous applications in robotics and beyond, we introduce Riemannian Time Warping (RTW). This novel approach efficiently aligns multiple signals by considering the geometric structure of the Riemannian manifold in which the data is embedded. Extensive experiments on synthetic and real-world data, including tests with an LBR iiwa robot, demonstrate that RTW consistently outperforms state-of-the-art baselines in both averaging and classification tasks. </p>
<blockquote>
<p>å¤šä¸ªä¿¡å·é€šè¿‡æ—¶é—´æ‰­æ›²çš„æ—¶é—´å¯¹é½åœ¨å¤šä¸ªé¢†åŸŸéƒ½è‡³å…³é‡è¦ï¼Œå¦‚åœ¨è¯­éŸ³è¯†åˆ«æˆ–æœºå™¨äººè¿åŠ¨å­¦ä¹ ä¸­çš„åˆ†ç±»ã€‚å‡ ä¹æ‰€æœ‰ç›¸å…³å·¥ä½œéƒ½å±€é™äºæ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­çš„æ•°æ®ã€‚å°½ç®¡åœ¨2011å¹´æœ‰äººè¯•å›¾å°†è¿™ä¸€æ¦‚å¿µé€‚åº”äºå•ä½å››å…ƒæ•°ï¼Œä½†åˆ°é»æ›¼æµå½¢çš„é€šç”¨æ‰©å±•ä»ç„¶ç¼ºå¤±ã€‚è€ƒè™‘åˆ°å…¶åœ¨æœºå™¨äººæŠ€æœ¯åŠå…¶ä»¥å¤–çš„ä¼—å¤šåº”ç”¨ä¸­çš„é‡è¦æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†é»æ›¼æ—¶é—´æ‰­æ›²ï¼ˆRTWï¼‰ã€‚è¿™ç§æ–¹æ³•é€šè¿‡è€ƒè™‘æ•°æ®åµŒå…¥çš„é»æ›¼æµå½¢çš„å‡ ä½•ç»“æ„ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¯¹é½å¤šä¸ªä¿¡å·ã€‚åœ¨åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬å¯¹LBR iiwaæœºå™¨äººçš„æµ‹è¯•ï¼Œè¯æ˜æ— è®ºæ˜¯åœ¨å¹³å‡ä»»åŠ¡è¿˜æ˜¯åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒRTWå§‹ç»ˆä¼˜äºæœ€æ–°çš„åŸºçº¿æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01635v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æ—¶ç©ºå¼¯æ›²æŠ€æœ¯å¯¹äºè®¸å¤šé¢†åŸŸå¦‚è¯­éŸ³è¯†åˆ«æˆ–æœºå™¨äººè¿åŠ¨å­¦ä¹ ä¸­çš„åˆ†ç±»è‡³å…³é‡è¦ã€‚å°½ç®¡å·²æœ‰å°è¯•å°†æ¦‚å¿µæ‰©å±•åˆ°å•ä½å››å…ƒæ•°ï¼Œä½†ç›®å‰è¿˜ç¼ºä¹å°†å…¶æ‰©å±•åˆ°é»æ›¼æµå½¢çš„ä¸€èˆ¬æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥é»æ›¼æ—¶é—´å¼¯æ›²ï¼ˆRTWï¼‰ï¼Œè¯¥æ–¹æ³•è€ƒè™‘æ•°æ®åµŒå…¥çš„é»æ›¼æµå½¢çš„å‡ ä½•ç»“æ„ï¼Œæœ‰æ•ˆå¯¹é½å¤šä¸ªä¿¡å·ã€‚åœ¨åˆæˆæ•°æ®å’ŒçœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„å¹¿æ³›å®éªŒä»¥åŠå¯¹LBR iiwaæœºå™¨äººçš„æµ‹è¯•è¡¨æ˜ï¼ŒRTWåœ¨å¹³å‡å’Œåˆ†ç±»ä»»åŠ¡ä¸Šå‡ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶ç©ºå¼¯æ›²æŠ€æœ¯åœ¨å¤šä¸ªé¢†åŸŸï¼ˆå¦‚è¯­éŸ³è¯†åˆ«å’Œæœºå™¨äººè¿åŠ¨å­¦ä¹ ï¼‰ä¸­çš„åˆ†ç±»ä»»åŠ¡ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>ç›®å‰å¤§å¤šæ•°ç›¸å…³ç ”ç©¶ä»…é™äºæ¬§å‡ é‡Œå¾—ç©ºé—´å†…çš„æ•°æ®ã€‚</li>
<li>å°½ç®¡å·²æœ‰å°è¯•å°†æ¦‚å¿µæ‰©å±•åˆ°å•ä½å››å…ƒæ•°ï¼Œä½†å°†å…¶ä¸€èˆ¬æ‰©å±•åˆ°é»æ›¼æµå½¢çš„æ–¹æ³•ä»ç„¶ç¼ºå¤±ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹æ³•â€”â€”é»æ›¼æ—¶é—´å¼¯æ›²ï¼ˆRTWï¼‰ï¼Œè¯¥æ–¹æ³•è€ƒè™‘æ•°æ®åµŒå…¥çš„å‡ ä½•ç»“æ„ï¼Œå®ç°å¯¹å¤šä¸ªä¿¡å·çš„æœ‰æ•ˆå¯¹é½ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œé»æ›¼æ—¶é—´å¼¯æ›²åœ¨å¹³å‡å’Œåˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>é»æ›¼æ—¶é—´å¼¯æ›²æŠ€æœ¯çš„å¹¿æ³›åº”ç”¨å¯èƒ½æ¨åŠ¨æœºå™¨äººæŠ€æœ¯ç­‰å¤šä¸ªé¢†åŸŸçš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f19a3f2da8f513cf69dcb3c56ceab3d5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83ae0378cbf3da662aecbd7a75af4895.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19b4aef7943e0bf946ecfb0942eb60c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8edf569a39679f1d05b6f32c032f3104.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3b6a9ee7f143c78e36bedd816a53046.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-171135d0842e0bea48b3d3981853b3fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d612bf694e16f7e4fe76604d00050cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-850b38fd781563baf865fb4c81ce0712.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c3f2f01f840277aabb6cd8c43d49675.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Tiny-Align-Bridging-Automatic-Speech-Recognition-and-Large-Language-Model-on-the-Edge"><a href="#Tiny-Align-Bridging-Automatic-Speech-Recognition-and-Large-Language-Model-on-the-Edge" class="headerlink" title="Tiny-Align: Bridging Automatic Speech Recognition and Large Language   Model on the Edge"></a>Tiny-Align: Bridging Automatic Speech Recognition and Large Language   Model on the Edge</h2><p><strong>Authors:Ruiyang Qin, Dancheng Liu, Gelei Xu, Zheyu Yan, Chenhui Xu, Yuting Hu, Shaocong Wang, X. Sharon Hu, Jinjun Xiong, Yiyu Shi</strong></p>
<p>The combination of Large Language Models (LLM) and Automatic Speech Recognition (ASR), when deployed on edge devices (called edge ASR-LLM), can serve as a powerful personalized assistant to enable audio-based interaction for users. Compared to text-based interaction, edge ASR-LLM allows accessible and natural audio interactions. Unfortunately, existing ASR-LLM models are mainly trained in high-performance computing environments and produce substantial model weights, making them difficult to deploy on edge devices. More importantly, to better serve usersâ€™ personalized needs, the ASR-LLM must be able to learn from each distinct user, given that audio input often contains highly personalized characteristics that necessitate personalized on-device training. Since individually fine-tuning the ASR or LLM often leads to suboptimal results due to modality-specific limitations, end-to-end training ensures seamless integration of audio features and language understanding (cross-modal alignment), ultimately enabling a more personalized and efficient adaptation on edge devices. However, due to the complex training requirements and substantial computational demands of existing approaches, cross-modal alignment between ASR audio and LLM can be challenging on edge devices. In this work, we propose a resource-efficient cross-modal alignment framework that bridges ASR and LLMs on edge devices to handle personalized audio input. Our framework enables efficient ASR-LLM alignment on resource-constrained devices like NVIDIA Jetson Orin (8GB RAM), achieving 50x training time speedup while improving the alignment quality by more than 50%. To the best of our knowledge, this is the first work to study efficient ASR-LLM alignment on resource-constrained edge devices. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„ç»“åˆï¼Œåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ï¼ˆç§°ä¸ºè¾¹ç¼˜ASR-LLMï¼‰æ—¶ï¼Œå¯ä½œä¸ºå¼ºå¤§çš„ä¸ªæ€§åŒ–åŠ©æ‰‹ï¼Œä½¿ç”¨æˆ·èƒ½å¤ŸåŸºäºéŸ³é¢‘è¿›è¡Œäº¤äº’ã€‚ä¸åŸºäºæ–‡æœ¬çš„äº¤äº’ç›¸æ¯”ï¼Œè¾¹ç¼˜ASR-LLMå…è®¸å¯è®¿é—®å’Œè‡ªç„¶çš„éŸ³é¢‘äº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ASR-LLMæ¨¡å‹ä¸»è¦åœ¨é«˜æ€§èƒ½è®¡ç®—ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼Œäº§ç”Ÿå¤§é‡çš„æ¨¡å‹æƒé‡ï¼Œä½¿å¾—å®ƒä»¬éš¾ä»¥åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œä¸ºäº†æ›´å¥½åœ°æ»¡è¶³ç”¨æˆ·çš„ä¸ªæ€§åŒ–éœ€æ±‚ï¼ŒASR-LLMå¿…é¡»èƒ½å¤Ÿä»æ¯ä¸ªä¸åŒçš„ç”¨æˆ·èº«ä¸Šå­¦ä¹ ï¼Œå› ä¸ºéŸ³é¢‘è¾“å…¥é€šå¸¸åŒ…å«é«˜åº¦ä¸ªæ€§åŒ–çš„ç‰¹å¾ï¼Œéœ€è¦è¿›è¡Œä¸ªæ€§åŒ–çš„è®¾å¤‡ç«¯è®­ç»ƒã€‚ç”±äºå•ç‹¬å¾®è°ƒASRæˆ–LLMå¾€å¾€ä¼šå¯¼è‡´æ¬¡ä¼˜ç»“æœï¼Œç«¯åˆ°ç«¯è®­ç»ƒç¡®ä¿éŸ³é¢‘ç‰¹å¾å’Œè¯­è¨€ç†è§£çš„æ— ç¼é›†æˆï¼ˆè·¨æ¨¡æ€å¯¹é½ï¼‰ï¼Œæœ€ç»ˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°æ›´ä¸ªæ€§åŒ–å’Œé«˜æ•ˆçš„é€‚åº”ã€‚ç„¶è€Œï¼Œç”±äºç°æœ‰æ–¹æ³•çš„å¤æ‚è®­ç»ƒè¦æ±‚å’Œå·¨å¤§çš„è®¡ç®—éœ€æ±‚ï¼ŒASRéŸ³é¢‘å’ŒLLMä¹‹é—´çš„è·¨æ¨¡æ€å¯¹é½åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªèµ„æºé«˜æ•ˆçš„è·¨æ¨¡æ€å¯¹é½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°ASRå’ŒLLMçš„è¡”æ¥ï¼Œä»¥å¤„ç†ä¸ªæ€§åŒ–çš„éŸ³é¢‘è¾“å…¥ã€‚æˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿåœ¨åƒNVIDIA Jetson Orinï¼ˆ8GB RAMï¼‰è¿™æ ·çš„èµ„æºå—é™è®¾å¤‡ä¸Šå®ç°é«˜æ•ˆçš„ASR-LLMå¯¹é½ï¼Œå®ç°50å€çš„åŸ¹è®­æ—¶é—´åŠ é€Ÿï¼ŒåŒæ—¶æé«˜å¯¹é½è´¨é‡è¶…è¿‡50%ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šç ”ç©¶é«˜æ•ˆASR-LLMå¯¹é½çš„å·¥ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13766v4">PDF</a> Accepted by ICCADâ€™25</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šéƒ¨ç½²å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„ç»„åˆï¼ˆç§°ä¸ºè¾¹ç¼˜ASR-LLMï¼‰ï¼Œå¯ä½œä¸ºå¼ºå¤§çš„ä¸ªæ€§åŒ–åŠ©æ‰‹ï¼Œå®ç°åŸºäºéŸ³é¢‘çš„ç”¨æˆ·äº¤äº’ã€‚ç›¸è¾ƒäºæ–‡æœ¬äº¤äº’ï¼Œè¾¹ç¼˜ASR-LLMå…è®¸æ›´ä¾¿æ·ã€æ›´è‡ªç„¶çš„éŸ³é¢‘äº¤äº’ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ASR-LLMæ¨¡å‹ä¸»è¦åœ¨é«˜æ€§èƒ½è®¡ç®—ç¯å¢ƒä¸­è®­ç»ƒï¼Œæ¨¡å‹ä½“ç§¯åºå¤§ï¼Œéš¾ä»¥éƒ¨ç½²åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šã€‚æ›´é‡è¦çš„æ˜¯ï¼Œä¸ºäº†æ›´å¥½åœ°æœåŠ¡ç”¨æˆ·çš„ä¸ªæ€§åŒ–éœ€æ±‚ï¼ŒASR-LLMå¿…é¡»å…·å¤‡ä»æ¯ä¸ªç‹¬ç«‹ç”¨æˆ·èº«ä¸Šå­¦ä¹ çš„èƒ½åŠ›ï¼Œå› ä¸ºéŸ³é¢‘è¾“å…¥å¾€å¾€åŒ…å«é«˜åº¦ä¸ªæ€§åŒ–çš„ç‰¹å¾ï¼Œéœ€è¦è¿›è¡Œä¸ªæ€§åŒ–çš„è®¾å¤‡ç«¯è®­ç»ƒã€‚ç”±äºå•ç‹¬å¾®è°ƒASRæˆ–LLMå¾€å¾€å› æ¨¡æ€ç‰¹å®šé™åˆ¶è€Œå¯¼è‡´ç»“æœä¸ä½³ï¼Œç«¯åˆ°ç«¯è®­ç»ƒå¯ç¡®ä¿éŸ³é¢‘ç‰¹å¾ä¸è¯­è¨€ç†è§£çš„æ— ç¼é›†æˆï¼ˆè·¨æ¨¡æ€å¯¹é½ï¼‰ï¼Œæœ€ç»ˆåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°æ›´ä¸ªæ€§åŒ–å’Œé«˜æ•ˆçš„é€‚åº”ã€‚ç„¶è€Œï¼Œç”±äºç°æœ‰æ–¹æ³•çš„å¤æ‚è®­ç»ƒè¦æ±‚å’Œå·¨å¤§çš„è®¡ç®—éœ€æ±‚ï¼Œåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°ASRéŸ³é¢‘å’ŒLLMä¹‹é—´çš„è·¨æ¨¡æ€å¯¹é½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§èµ„æºé«˜æ•ˆçš„è·¨æ¨¡æ€å¯¹é½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šæ¡¥æ¥ASRå’ŒLLMï¼Œå¤„ç†ä¸ªæ€§åŒ–éŸ³é¢‘è¾“å…¥ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šå®ç°é«˜æ•ˆçš„ASR-LLMå¯¹é½ï¼Œå¦‚8GB RAMçš„NVIDIA Jetson Orinï¼Œå®ç°50å€çš„åŸ¹è®­æ—¶é—´åŠ é€Ÿï¼ŒåŒæ—¶æé«˜å¯¹é½è´¨é‡è¶…è¿‡50%ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹åœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šç ”ç©¶é«˜æ•ˆASR-LLMå¯¹é½çš„å·¥ä½œã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¾¹ç¼˜è®¾å¤‡ä¸Šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç»“åˆå¯æä¾›å¼ºå¤§çš„ä¸ªæ€§åŒ–åŠ©æ‰‹åŠŸèƒ½ï¼Œæ”¯æŒéŸ³é¢‘äº¤äº’ã€‚</li>
<li>ç°æœ‰ASR-LLMæ¨¡å‹ä¸»è¦åœ¨é«˜æ€§èƒ½ç¯å¢ƒä¸­è®­ç»ƒï¼Œéš¾ä»¥éƒ¨ç½²åœ¨èµ„æºæœ‰é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šã€‚</li>
<li>ä¸ªæ€§åŒ–éŸ³é¢‘è¾“å…¥éœ€è¦ä¸ªæ€§åŒ–è®¾å¤‡è®­ç»ƒï¼Œå› ä¸ºéŸ³é¢‘åŒ…å«é«˜åº¦ä¸ªæ€§åŒ–çš„ç‰¹å¾ã€‚</li>
<li>å•ç‹¬å¾®è°ƒASRæˆ–LLMå› æ¨¡æ€ç‰¹å®šé™åˆ¶å¯èƒ½æ•ˆæœä¸ä½³ï¼Œç«¯åˆ°ç«¯è®­ç»ƒå¯å®ç°è·¨æ¨¡æ€æ— ç¼é›†æˆï¼ˆè·¨æ¨¡æ€å¯¹é½ï¼‰ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°è·¨æ¨¡æ€å¯¹é½å…·æœ‰æŒ‘æˆ˜ï¼Œå› ä¸ºå¤æ‚çš„è®­ç»ƒè¦æ±‚å’Œè®¡ç®—éœ€æ±‚ã€‚</li>
<li>æå‡ºçš„èµ„æºé«˜æ•ˆè·¨æ¨¡æ€å¯¹é½æ¡†æ¶å¯åœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šæ¡¥æ¥ASRå’ŒLLMï¼Œå®ç°ä¸ªæ€§åŒ–éŸ³é¢‘è¾“å…¥çš„é«˜æ•ˆå¤„ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.13766">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-66e76953ba7badc26515e06b5223ce49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-777a1bedc89f16a499c8ed120fba64f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14077e5b275adf181c92b7c6d9a2b37e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fb780f993d6475ac9ad5e06c367a034.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Speech-Deepfake-Detection"><a href="#A-Survey-on-Speech-Deepfake-Detection" class="headerlink" title="A Survey on Speech Deepfake Detection"></a>A Survey on Speech Deepfake Detection</h2><p><strong>Authors:Menglu Li, Yasaman Ahmadiadli, Xiao-Ping Zhang</strong></p>
<p>The availability of smart devices leads to an exponential increase in multimedia content. However, advancements in deep learning have also enabled the creation of highly sophisticated Deepfake content, including speech Deepfakes, which pose a serious threat by generating realistic voices and spreading misinformation. To combat this, numerous challenges have been organized to advance speech Deepfake detection techniques. In this survey, we systematically analyze more than 200 papers published up to March 2024. We provide a comprehensive review of each component in the detection pipeline, including model architectures, optimization techniques, generalizability, evaluation metrics, performance comparisons, available datasets, and open source availability. For each aspect, we assess recent progress and discuss ongoing challenges. In addition, we explore emerging topics such as partial Deepfake detection, cross-dataset evaluation, and defences against adversarial attacks, while suggesting promising research directions. This survey not only identifies the current state of the art to establish strong baselines for future experiments but also offers clear guidance for researchers aiming to enhance speech Deepfake detection systems. </p>
<blockquote>
<p>æ™ºèƒ½è®¾å¤‡çš„æ™®åŠå¯¼è‡´å¤šåª’ä½“å†…å®¹å‘ˆæŒ‡æ•°çº§å¢é•¿ã€‚ç„¶è€Œï¼Œæ·±åº¦å­¦ä¹ çš„å‘å±•ä¹Ÿå‚¬ç”Ÿäº†é«˜åº¦å¤æ‚çš„æ·±åº¦ä¼ªé€ å†…å®¹ï¼ˆDeepfakeï¼‰çš„ç”Ÿæˆï¼Œå…¶ä¸­åŒ…æ‹¬è¯­éŸ³æ·±åº¦ä¼ªé€ ï¼ˆSpeech Deepfakesï¼‰ï¼Œå®ƒèƒ½ç”Ÿæˆé€¼çœŸçš„å£°éŸ³å¹¶ä¼ æ’­é”™è¯¯ä¿¡æ¯ï¼Œæ„æˆä¸¥é‡å¨èƒã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œå·²ç»ç»„ç»‡äº†å¤šæ¬¡æŒ‘æˆ˜æ¥æ¨è¿›è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹æŠ€æœ¯çš„å‘å±•ã€‚åœ¨è¿™ç¯‡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°åˆ†æäº†æˆªè‡³2024å¹´3æœˆå‘è¡¨çš„è¶…è¿‡200ç¯‡è®ºæ–‡ã€‚æˆ‘ä»¬å¯¹æ£€æµ‹æµç¨‹ä¸­çš„æ¯ä¸ªç»„ä»¶è¿›è¡Œäº†å…¨é¢çš„å›é¡¾ï¼ŒåŒ…æ‹¬æ¨¡å‹æ¶æ„ã€ä¼˜åŒ–æŠ€æœ¯ã€é€šç”¨æ€§ã€è¯„ä»·æŒ‡æ ‡ã€æ€§èƒ½æ¯”è¾ƒã€å¯ç”¨æ•°æ®é›†å’Œå¼€æºå¯ç”¨æ€§ã€‚å¯¹äºæ¯ä¸ªæ–¹é¢ï¼Œæˆ‘ä»¬éƒ½è¯„ä¼°äº†æœ€æ–°çš„è¿›å±•å¹¶è®¨è®ºäº†æŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†æ–°å…´è¯é¢˜ï¼Œå¦‚å±€éƒ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ã€è·¨æ•°æ®é›†è¯„ä¼°å’Œå¯¹æŠ—æ”»å‡»çš„é˜²å¾¡ç­–ç•¥ï¼ŒåŒæ—¶æå‡ºäº†æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚è¿™ç¯‡ç»¼è¿°ä¸ä»…ç¡®å®šäº†å½“å‰çš„ç ”ç©¶ç°çŠ¶ï¼Œä¸ºæœªæ¥çš„å®éªŒå»ºç«‹äº†å¼ºå¤§çš„åŸºå‡†çº¿ï¼Œè€Œä¸”ä¹Ÿä¸ºæ—¨åœ¨æé«˜è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹ç³»ç»Ÿçš„ç ”ç©¶è€…æä¾›äº†æ˜ç¡®çš„æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.13914v2">PDF</a> 38 pages. This paper has been accepted by ACM Computing Surveys</p>
<p><strong>Summary</strong><br>     æ™ºèƒ½è®¾å¤‡çš„å‘å±•å¯¼è‡´å¤šåª’ä½“å†…å®¹å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„è¿›æ­¥ä¹Ÿå‚¬ç”Ÿäº†é«˜åº¦å¤æ‚åŒ–çš„Deepfakeå†…å®¹ï¼ŒåŒ…æ‹¬è¯­éŸ³Deepfakesã€‚å®ƒä»¬é€šè¿‡ç”ŸæˆçœŸå®å£°éŸ³å’Œæ‰©æ•£é”™è¯¯ä¿¡æ¯æ„æˆä¸¥é‡å¨èƒã€‚ä¸ºåº”å¯¹æ­¤æŒ‘æˆ˜ï¼Œå·²ç»„ç»‡å¤šæ¬¡ç«èµ›æ¨åŠ¨è¯­éŸ³Deepfakeæ£€æµ‹æŠ€æœ¯çš„å‘å±•ã€‚æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†è¶…è¿‡200ç¯‡è‡³2024å¹´3æœˆå‘è¡¨çš„è®ºæ–‡ï¼Œå…¨é¢è¯„è¿°æ£€æµ‹æµç¨‹ä¸­çš„æ¯ä¸ªç»„ä»¶ï¼ŒåŒ…æ‹¬æ¨¡å‹æ¶æ„ã€ä¼˜åŒ–æŠ€æœ¯ã€é€šç”¨æ€§ã€è¯„ä¼°æŒ‡æ ‡ã€æ€§èƒ½å¯¹æ¯”ã€å¯ç”¨æ•°æ®é›†å’Œå¼€æºå¯ç”¨æ€§ã€‚åŒæ—¶æ¢è®¨æ–°å…´è¯é¢˜å¦‚éƒ¨åˆ†Deepfakeæ£€æµ‹ã€è·¨æ•°æ®é›†è¯„ä¼°å’Œå¯¹æŠ—æ”»å‡»çš„é˜²å¾¡ç­–ç•¥ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜æ–¹å‘ã€‚æœ¬æ–‡ä¸ä»…ç¡®å®šäº†å½“å‰ç ”ç©¶çš„é¡¶å°–æ°´å¹³ï¼Œè¿˜ä¸ºå¸Œæœ›æé«˜è¯­éŸ³Deepfakeæ£€æµ‹ç³»ç»Ÿçš„ç ”ç©¶äººå‘˜æä¾›äº†æ¸…æ™°çš„æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ™ºèƒ½è®¾å¤‡æ™®åŠå¯¼è‡´å¤šåª’ä½“å†…å®¹æ¿€å¢ï¼ŒåŒ…æ‹¬é«˜åº¦å¤æ‚çš„Deepfakeå†…å®¹ã€‚</li>
<li>è¯­éŸ³Deepfakesé€šè¿‡ç”ŸæˆçœŸå®å£°éŸ³å’Œæ‰©æ•£é”™è¯¯ä¿¡æ¯æ„æˆä¸¥é‡å¨èƒã€‚</li>
<li>ä¸ºåº”å¯¹è¯­éŸ³Deepfakeçš„æŒ‘æˆ˜ï¼Œå·²ç»„ç»‡å¤šæ¬¡ç«èµ›æ¨åŠ¨æ£€æµ‹æŠ€æœ¯å‘å±•ã€‚</li>
<li>æœ¬æ–‡ç³»ç»Ÿåˆ†æäº†è¶…è¿‡200ç¯‡å…³äºè¯­éŸ³Deepfakeæ£€æµ‹çš„è®ºæ–‡ï¼Œå…¨é¢è¯„è¿°æ£€æµ‹æµç¨‹ä¸­çš„å„ä¸ªç»„ä»¶ã€‚</li>
<li>æ–‡ç« æ¢è®¨äº†æ–°å…´è¯é¢˜ï¼Œå¦‚éƒ¨åˆ†Deepfakeæ£€æµ‹ã€è·¨æ•°æ®é›†è¯„ä»·å’Œå¯¹æŠ—æ”»å‡»çš„é˜²å¾¡ç­–ç•¥ã€‚</li>
<li>æœ¬æ–‡ç¡®å®šäº†å½“å‰ç ”ç©¶çš„é¡¶å°–æ°´å¹³ï¼Œä¸ºå»ºç«‹æœªæ¥å®éªŒæä¾›äº†å¼ºåŸºå‡†çº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.13914">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-46c7b82c20f95b9547a3d402c3815579.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ba7fe4604594ff117ec1a81463eda82.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-40a5b62c3e87fc1dce51a7eab8eeec3c.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  SystolicAttention Fusing FlashAttention within a Single Systolic Array
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-742e7ae4eb6b5d512238db3bd8bae61e.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  Focus on Texture Rethinking Pre-training in Masked Autoencoders for   Medical Image Classification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26384.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
