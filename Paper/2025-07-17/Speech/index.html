<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-07-17  FasTUSS Faster Task-Aware Unified Source Separation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-66e76953ba7badc26515e06b5223ce49.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    15.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    62 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-17-更新"><a href="#2025-07-17-更新" class="headerlink" title="2025-07-17 更新"></a>2025-07-17 更新</h1><h2 id="FasTUSS-Faster-Task-Aware-Unified-Source-Separation"><a href="#FasTUSS-Faster-Task-Aware-Unified-Source-Separation" class="headerlink" title="FasTUSS: Faster Task-Aware Unified Source Separation"></a>FasTUSS: Faster Task-Aware Unified Source Separation</h2><p><strong>Authors:Francesco Paissan, Gordon Wichern, Yoshiki Masuyama, Ryo Aihara, François G. Germain, Kohei Saijo, Jonathan Le Roux</strong></p>
<p>Time-Frequency (TF) dual-path models are currently among the best performing audio source separation network architectures, achieving state-of-the-art performance in speech enhancement, music source separation, and cinematic audio source separation. While they are characterized by a relatively low parameter count, they still require a considerable number of operations, implying a higher execution time. This problem is exacerbated by the trend towards bigger models trained on large amounts of data to solve more general tasks, such as the recently introduced task-aware unified source separation (TUSS) model. TUSS, which aims to solve audio source separation tasks using a single, conditional model, is built upon TF-Locoformer, a TF dual-path model combining convolution and attention layers. The task definition comes in the form of a sequence of prompts that specify the number and type of sources to be extracted. In this paper, we analyze the design choices of TUSS with the goal of optimizing its performance-complexity trade-off. We derive two more efficient models, FasTUSS-8.3G and FasTUSS-11.7G that reduce the original model’s operations by 81% and 73% with minor performance drops of 1.2<del>dB and 0.4</del>dB averaged over all benchmarks, respectively. Additionally, we investigate the impact of prompt conditioning to derive a causal TUSS model. </p>
<blockquote>
<p>时间频率（TF）双路径模型是目前表现最好的音频源分离网络架构之一，在语音增强、音乐源分离和电影音频源分离等领域达到了最新技术性能水平。虽然这些模型参数相对较少，但它们仍然需要大量的运算，导致执行时间较高。随着在大量数据上训练更大模型以解决更一般任务的趋势，这一问题进一步加剧，例如最近推出的任务感知统一源分离（TUSS）模型。TUSS旨在使用一个条件模型解决音频源分离任务，它建立在TF-Locoformer的基础上，这是一个结合了卷积和注意力层的TF双路径模型。任务定义的形式是一系列提示，指定要提取的源的数量和类型。在本文中，我们分析了TUSS的设计选择，旨在优化其性能复杂性权衡。我们推出了两个更有效的模型FasTUSS-8.3G和FasTUSS-11.7G，它们将原始模型的运算量分别减少了81%和73%，在所有基准测试上的性能平均分别下降了1.2dB和0.4dB。此外，我们还研究了提示条件的影响，以推导出因果TUSS模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11435v1">PDF</a> Accepted to WASPAA 2025</p>
<p><strong>Summary</strong><br>     时序双路径模型是目前表现最佳的音频源分离网络架构之一，广泛应用于语音增强、音乐源分离和电影音频源分离等领域。针对此架构运算量大、执行时间长的问题，本文分析了TUSS模型的设计选择，并优化其性能复杂度权衡，推出更加高效的FasTUSS-8.3G和FasTUSS-11.7G模型，分别将原始模型运算量降低了81%和73%，在各项基准测试中的性能下降分别为1.2dB和0.4dB。此外，本文还探讨了基于提示条件的因果TUSS模型的影响。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时序双路径模型是目前音频源分离领域的最佳网络架构之一。</li>
<li>TUSS模型旨在使用单一条件模型解决音频源分离任务。</li>
<li>TUSS模型基于TF-Locoformer架构，结合了卷积和注意力层。</li>
<li>本文分析了TUSS模型的设计选择，旨在优化其性能复杂度之间的权衡。</li>
<li>推出更高效的FasTUSS-8.3G和FasTUSS-11.7G模型，降低了运算量并保持了较好的性能。</li>
<li>FasTUSS模型性能下降程度分别为1.2dB和0.4dB。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11435">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4a58a6c75f0a03591fc3682877f23b33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e44d662d74e2cd965369b9dfd9242bb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-609a95985acb013702fdcb3db28c7fae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-228908453ec12a25f5a651b86193af97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b13fe91a92ca56ca424bb91f43675b6f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="WhisperKit-On-device-Real-time-ASR-with-Billion-Scale-Transformers"><a href="#WhisperKit-On-device-Real-time-ASR-with-Billion-Scale-Transformers" class="headerlink" title="WhisperKit: On-device Real-time ASR with Billion-Scale Transformers"></a>WhisperKit: On-device Real-time ASR with Billion-Scale Transformers</h2><p><strong>Authors:Atila Orhon, Arda Okan, Berkin Durmus, Zach Nagengast, Eduardo Pacheco</strong></p>
<p>Real-time Automatic Speech Recognition (ASR) is a fundamental building block for many commercial applications of ML, including live captioning, dictation, meeting transcriptions, and medical scribes. Accuracy and latency are the most important factors when companies select a system to deploy. We present WhisperKit, an optimized on-device inference system for real-time ASR that significantly outperforms leading cloud-based systems. We benchmark against server-side systems that deploy a diverse set of models, including a frontier model (OpenAI gpt-4o-transcribe), a proprietary model (Deepgram nova-3), and an open-source model (Fireworks large-v3-turbo).Our results show that WhisperKit matches the lowest latency at 0.46s while achieving the highest accuracy 2.2% WER. The optimizations behind the WhisperKit system are described in detail in this paper. </p>
<blockquote>
<p>实时自动语音识别（ASR）是机器学习许多商业应用的基础构件，包括实时字幕、听写、会议记录和医疗记录等。公司在选择部署系统时，准确性和延迟是最重要的因素。我们推出了WhisperKit，这是一个优化的实时ASR设备端推理系统，它显著优于领先的云系统。我们以服务器端系统为基准进行测试，这些系统部署了多种模型，包括前沿模型（OpenAI gpt-4o-transcribe）、专有模型（Deepgram nova-3）和开源模型（Fireworks large-v3-turbo）。我们的结果表明，WhisperKit在达到最低延迟0.46秒的同时，实现了最高的准确性，词错误率为2.2%。WhisperKit系统背后的优化细节都详细描述在这篇论文中。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10860v1">PDF</a> ICML 2025 - On-Device Learning for Foundational Models Workshop</p>
<p><strong>摘要</strong></p>
<p>实时语音识别（ASR）是机器学习许多商业应用的基础构件，包括实时字幕、听写、会议转录和医疗记录等。公司在选择部署系统时，准确性和延迟是最重要的因素。本文介绍了WhisperKit，这是一种优化的实时ASR设备端推理系统，显著优于领先的云系统。我们以部署了各种模型的服务器端系统为基准进行了评估，包括前沿模型（OpenAI gpt-4o-transcribe）、专有模型（Deepgram nova-3）和开源模型（Fireworks large-v3-turbo）。结果表明，WhisperKit在达到最低延迟0.46秒的同时实现了最高的准确性，字错误率为2.2%。本文详细描述了WhisperKit系统的优化过程。</p>
<p><strong>要点</strong></p>
<ol>
<li>实时语音识别（ASR）在许多商业应用中具有关键作用，如实时字幕、听写等。</li>
<li>准确性与延迟是选择语音识别系统最重要的两个因素。</li>
<li>WhisperKit是一个优化的实时ASR手机端推理系统，表现显著优于多数云系统。</li>
<li>WhisperKit的基准测试包括与多种服务器端系统对比，包括前沿模型如OpenAI gpt-4o-transcribe。</li>
<li>WhisperKit在延迟和准确性方面表现优异，延迟达到最低的0.46秒，字错误率为2.2%。</li>
<li>WhisperKit的优化过程在论文中有详细描述。</li>
<li>该系统可为商业应用提供高效、准确的实时语音识别服务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10860">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-53ead9cc8cfaffc7a190ab4dd3d69cbc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b941669d3e7b58919a41ac6253cb2019.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-966a8dbb40f81feaf73bed7547fe96a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-85c4e2bb1636b25506259b77f802b170.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cab9e6e8c2168ffa49f946ea52327e36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0f7b190358e973a1cda5eacf259eb670.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87b53d709485601e807fca202a5b7d1e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Supporting-SENCOTEN-Language-Documentation-Efforts-with-Automatic-Speech-Recognition"><a href="#Supporting-SENCOTEN-Language-Documentation-Efforts-with-Automatic-Speech-Recognition" class="headerlink" title="Supporting SENĆOTEN Language Documentation Efforts with Automatic   Speech Recognition"></a>Supporting SENĆOTEN Language Documentation Efforts with Automatic   Speech Recognition</h2><p><strong>Authors:Mengzhe Geng, Patrick Littell, Aidan Pine,  PENÁĆ, Marc Tessier, Roland Kuhn</strong></p>
<p>The SEN&#39;{C}OTEN language, spoken on the Saanich peninsula of southern Vancouver Island, is in the midst of vigorous language revitalization efforts to turn the tide of language loss as a result of colonial language policies. To support these on-the-ground efforts, the community is turning to digital technology. Automatic Speech Recognition (ASR) technology holds great promise for accelerating language documentation and the creation of educational resources. However, developing ASR systems for SEN&#39;{C}OTEN is challenging due to limited data and significant vocabulary variation from its polysynthetic structure and stress-driven metathesis. To address these challenges, we propose an ASR-driven documentation pipeline that leverages augmented speech data from a text-to-speech (TTS) system and cross-lingual transfer learning with Speech Foundation Models (SFMs). An n-gram language model is also incorporated via shallow fusion or n-best restoring to maximize the use of available data. Experiments on the SEN&#39;{C}OTEN dataset show a word error rate (WER) of 19.34% and a character error rate (CER) of 5.09% on the test set with a 57.02% out-of-vocabulary (OOV) rate. After filtering minor cedilla-related errors, WER improves to 14.32% (26.48% on unseen words) and CER to 3.45%, demonstrating the potential of our ASR-driven pipeline to support SEN&#39;{C}OTEN language documentation. </p>
<blockquote>
<p>SEN’C’OTEN语言是讲于温哥华岛南部的塞尼切半岛的一种语言。由于殖民语言政策导致语言流失的形势严峻，该语言正在经历着一场激烈的振兴努力。为了支持这些地面上的努力，该社区正转向数字技术。自动语音识别（ASR）技术对于加速语言文献的记载和教育资源的创造方面有很大潜力。然而，由于数据有限以及由聚合结构和压力驱动的换词导致的显著词汇变化，开发适用于SEN’C’OTEN的ASR系统是一个挑战。为了应对这些挑战，我们提出了一种ASR驱动的文献记录管道，该管道利用文本到语音（TTS）系统的增强语音数据以及基于语音基础模型的跨语言迁移学习。通过浅融合或n-best恢复方法融入n元语言模型，以充分利用可用数据。在SEN’C’OTEN数据集上的实验显示，测试集上的单词错误率（WER）为19.34%，字符错误率（CER）为5.09%，词汇表外（OOV）的比率为57.02%。过滤掉与cedilla相关的轻微错误后，WER提高至14.32%（未见过词汇的比率为26.48%），CER降至3.45%，这证明了我们的ASR驱动管道在支持SEN’C’OTEN语言文献记录方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10827v1">PDF</a> Accepted by ComputEL-8</p>
<p><strong>Summary</strong></p>
<p>本文介绍了位于温哥华岛南部萨尼奇半岛的森科特恩语言（SEN’C{O}TEN）正处于激烈的复兴阶段，以扭转因殖民语言政策导致的语言流失。社区正在借助数字技术来支持这一复兴工作，自动语音识别（ASR）技术有望加速语言记录和创建教育资源。然而，由于数据有限和词汇量的巨大变化（来自其综合结构和压力驱动的元音变化），开发ASR系统对森科特恩语言具有挑战性。为此，本文提出了一种基于ASR的语言记录管道，利用文本到语音（TTS）系统的增强语音数据和跨语言的迁移学习与语音基础模型（SFMs）。实验结果显示，该管道在森科特恩数据集上的词错误率（WER）为14.32%（未见词的错误率为26.48%），字符错误率（CER）为3.45%，证明了该管道支持森科特恩语言记录的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SEN’C{O}TEN语言正在经历复兴，以应对殖民语言政策导致的语言流失。</li>
<li>社区正在借助数字技术和自动语音识别（ASR）技术来支持这一复兴工作。</li>
<li>开发针对SEN’C{O}TEN语言的ASR系统面临数据有限和词汇变化大的挑战。</li>
<li>提出了一种基于ASR的语言记录管道，结合增强语音数据、跨语言迁移学习和语音基础模型（SFMs）。</li>
<li>实验结果展示了该管道在森科特恩数据集上的性能，词错误率（WER）和字符错误率（CER）有所降低。</li>
<li>管道具有潜力支持森科特恩语言的记录工作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10827">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d1f45c9a6e0be4d3c16689681cefc0bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1049d4ae6438daf03a9d1aee1979c206.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b7faa1cda6a80087ad2fa87826b8ec3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d6c5bc8b128213980f5d37a90da1437.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56d1035616f5dabd94ec322dc72a659a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AudioMAE-learning-better-masked-audio-representations-with-SwiGLU-FFNs"><a href="#AudioMAE-learning-better-masked-audio-representations-with-SwiGLU-FFNs" class="headerlink" title="AudioMAE++: learning better masked audio representations with SwiGLU   FFNs"></a>AudioMAE++: learning better masked audio representations with SwiGLU   FFNs</h2><p><strong>Authors:Sarthak Yadav, Sergios Theodoridis, Zheng-Hua Tan</strong></p>
<p>Masked Autoencoders (MAEs) trained on audio spectrogram patches have emerged as a prominent approach for learning self-supervised audio representations. While several recent papers have evaluated key aspects of training MAEs on audio data, the majority of these approaches still leverage vanilla transformer building blocks, whereas the transformer community has seen steady integration of newer architectural advancements. In this work, we propose AudioMAE++, a revamped audio masked autoencoder with two such enhancements, namely macaron-style transformer blocks with gated linear units. When pretrained on the AudioSet dataset, the proposed AudioMAE++ models outperform existing MAE based approaches on 10 diverse downstream tasks, demonstrating excellent performance on audio classification and speech-based benchmarks. The proposed AudioMAE++ models also demonstrate excellent scaling characteristics, outperforming directly comparable standard MAE baselines with up to 4x more parameters. </p>
<blockquote>
<p>基于音频频谱图块训练的Masked Autoencoders（MAEs）已成为学习自监督音频表示的一种突出方法。虽然最近的几篇论文已经评估了训练MAEs在音频数据上的关键方面，但大多数方法仍然使用普通的变压器构建块，而变压器社区已经看到了新的架构进步的稳定集成。在这项工作中，我们提出了AudioMAE++，这是一个翻新的音频掩码自动编码器，具有两个这样的增强功能，即带有门控线性单元的夹心饼干式变压器块。在AudioSet数据集上进行预训练时，所提出的AudioMAE++模型在10个不同的下游任务上优于现有的基于MAE的方法，在音频分类和基于语音的基准测试中表现出卓越的性能。此外，所提出的AudioMAE++模型还表现出卓越的可扩展性特征，在参数最多达四倍的可直接比较的标准MAE基线之上表现良好。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10464v1">PDF</a> TO APPEAR AT IEEE MLSP 2025</p>
<p><strong>Summary</strong></p>
<p>基于音频频谱图块的Masked Autoencoders（MAEs）已成为学习自监督音频表示的主流方法。本文提出AudioMAE++模型，其引入了两项改进，包括使用具有门控线性单元的夹心饼干风格的变压器块。在AudioSet数据集上进行预训练后，AudioMAE++模型在多个下游任务上的性能优于现有的MAE方法，展现了出色的音频分类和语音基准测试性能。此外，AudioMAE++模型还展现出卓越的可扩展性，使用多达四倍参数的常规MAE基线模型也能实现出色的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Masked Autoencoders (MAEs)成为自监督音频表示学习的主要方法。</li>
<li>AudioMAE++模型引入了两项改进：采用夹心饼干风格的变压器块和门控线性单元。</li>
<li>AudioMAE++在多个下游任务上的性能优于现有MAE方法。</li>
<li>AudioMAE++模型在音频分类和语音基准测试上展现出卓越性能。</li>
<li>AudioMAE++模型具有良好的可扩展性，使用更多的参数也能实现出色的性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10464">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fe988b2c20d3c038b1282b6a4e8f18fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dae6f0ccce3de9f07ae720e5429129d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-258bba3fb9928feec33c9cc45cbdbf96.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1599917a6b3fcd4fd8e281e6e9c641ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c003146d37190ee5d1963aa4e117ed72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf7a9ed3466ba614f7ccd13a2f1ca554.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a66ec34256012ec1eff7f8f9e0773a5d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Knowing-When-to-Quit-Probabilistic-Early-Exits-for-Speech-Separation"><a href="#Knowing-When-to-Quit-Probabilistic-Early-Exits-for-Speech-Separation" class="headerlink" title="Knowing When to Quit: Probabilistic Early Exits for Speech Separation"></a>Knowing When to Quit: Probabilistic Early Exits for Speech Separation</h2><p><strong>Authors:Kenny Falkær Olsen. Mads Østergaard, Karl Ulbæk, Søren Føns Nielsen, Rasmus Malik Høegh Lindrup, Bjørn Sand Jensen, Morten Mørup</strong></p>
<p>In recent years, deep learning-based single-channel speech separation has improved considerably, in large part driven by increasingly compute- and parameter-efficient neural network architectures. Most such architectures are, however, designed with a fixed compute and parameter budget, and consequently cannot scale to varying compute demands or resources, which limits their use in embedded and heterogeneous devices such as mobile phones and hearables. To enable such use-cases we design a neural network architecture for speech separation capable of early-exit, and we propose an uncertainty-aware probabilistic framework to jointly model the clean speech signal and error variance which we use to derive probabilistic early-exit conditions in terms of desired signal-to-noise ratios. We evaluate our methods on both speech separation and enhancement tasks, and we show that a single early-exit model can be competitive with state-of-the-art models trained at many compute and parameter budgets. Our framework enables fine-grained dynamic compute-scaling of speech separation networks while achieving state-of-the-art performance and interpretable exit conditions. </p>
<blockquote>
<p>近年来，基于深度学习的单通道语音分离技术有了显著的改进，这在很大程度上是由于神经网络架构的计算和参数效率不断提高。然而，大多数这样的架构都是根据固定的计算和参数预算设计的，因此无法适应变化的计算需求或资源，这限制了它们在嵌入式和异构设备（如手机和可穿戴设备）中的应用。为了支持这些用例，我们设计了一种用于语音分离的神经网络架构，该架构具备提前退出功能。我们提出了一种基于不确定性的概率框架，以联合建模干净语音信号和误差方差，并利用其推导概率提前退出条件，以达到所需的信噪比。我们在语音分离和增强任务上评估了我们的方法，并证明单个提前退出模型可以与在许多计算和参数预算上训练的最新模型相竞争。我们的框架能够在实现最先进的性能的同时，实现对语音分离网络的精细动态计算缩放，并提供可解释的退出条件。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09768v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着深度学习在单通道语音分离领域的不断发展，近年来神经网络架构的效率和性能不断提升。然而，大多数架构都是针对固定计算和参数预算设计的，无法适应不同的计算需求或资源限制，这在嵌入式和异构设备（如手机和可佩戴设备）中的应用受到限制。为此，我们设计了一种具备早期退出功能的语音分离神经网络架构，并提出一种不确定性感知的概率框架，以联合建模干净语音信号和误差方差，从而推导出基于所需信噪比的概率早期退出条件。我们的方法在语音分离和增强任务上表现出竞争力，单个早期退出模型即可与在多种计算和参数预算下训练的最新模型相抗衡。我们的框架实现了语音分离网络的精细动态计算缩放，同时达到了最新性能并提供了可解释的早期退出条件。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习在单通道语音分离领域取得显著进步，归功于更高效的神经网络架构。</li>
<li>现有架构大多针对固定计算和参数预算设计，无法适应不同计算需求或资源限制。</li>
<li>提出一种具备早期退出功能的语音分离神经网络架构，适用于嵌入式和异构设备。</li>
<li>引入不确定性感知的概率框架，联合建模干净语音信号和误差方差。</li>
<li>基于信噪比推导概率早期退出条件，实现更精细的动态计算缩放。</li>
<li>方法在语音分离和增强任务上表现竞争力，与最新模型相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09768">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1f08556b0917f17dbaef718fe631e046.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cdffe98bfa712205d7f498d59a17a101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd95f6e0475220abb4ba7f61f6035dd2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="SC-TSE-Speaker-Consistency-Aware-Target-Speaker-Extraction"><a href="#SC-TSE-Speaker-Consistency-Aware-Target-Speaker-Extraction" class="headerlink" title="SC-TSE: Speaker Consistency-Aware Target Speaker Extraction"></a>SC-TSE: Speaker Consistency-Aware Target Speaker Extraction</h2><p><strong>Authors:Shu Wu, Anbin Qi, Yanzhang Xie, Xiang Xie</strong></p>
<p>Target Speaker Extraction (TSE) uses a reference cue to extract the target speech from a mixture. In TSE systems relying on audio cues, the speaker embedding from the enrolled speech is crucial to performance. However, these embeddings may suffer from speaker identity confusion. Unlike previous studies that focus on improving speaker embedding extraction, we improve TSE performance from the perspective of speaker consistency. In this paper, we propose a speaker consistency-aware target speaker extraction method that incorporates a centroid-based speaker consistency loss. This approach enhances TSE performance by ensuring speaker consistency between the enrolled and extracted speech. In addition, we integrate conditional loss suppression into the training process. The experimental results validate the effectiveness of our proposed methods in advancing the TSE performance. A speech demo is available online.\footnote{<a target="_blank" rel="noopener" href="https://sc-tse.netlify.app/">https://sc-tse.netlify.app/</a> </p>
<blockquote>
<p>目标说话人提取（TSE）使用参考线索从混合语音中提取目标语音。在依赖音频线索的TSE系统中，注册语音的说话人嵌入对性能至关重要。然而，这些嵌入可能会受到说话人身份混淆的影响。与以往专注于提高说话人嵌入提取的研究不同，我们从说话人一致性的角度提高TSE的性能。在本文中，我们提出了一种基于质心的说话人一致性感知目标说话人提取方法，该方法结合了基于质心的说话人一致性损失。这种方法通过确保注册语音和提取语音之间的说话人一致性，提高了TSE的性能。此外，我们将条件损失抑制集成到训练过程中。实验结果验证了我们的方法在提高TSE性能方面的有效性。在线提供语音演示。注：<a target="_blank" rel="noopener" href="https://sc-tse.netlify.app/%EF%BC%88%E5%9C%A8%E7%BA%BF%E6%BC%94%E7%A4%BA%E9%93%BE%E6%8E%A5%EF%BC%89">https://sc-tse.netlify.app/（在线演示链接）</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09510v1">PDF</a> Accept to Interspeech2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于音频提示的目标语音提取系统（Target Speaker Extraction，简称TSE）。该系统使用基于质心的说话人一致性损失方法提高说话人一致性，进而提高已注册语音和目标提取语音之间的性能。此外，还结合了条件损失抑制进行训练。本文提出的两种方法提高了TSE的性能。可通过在线语音演示展示相关内容。具体网址已在文中给出。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TSE系统利用参考线索从混合语音中提取目标语音。</li>
<li>音频提示下的TSE系统中，已注册语音的说话人嵌入对性能至关重要。</li>
<li>说话人嵌入可能受到说话人身份混淆的影响。</li>
<li>本文从提高说话人一致性的角度改进了TSE性能。</li>
<li>提出了一种基于质心的说话人一致性损失方法，确保已注册和提取的语音之间的说话人一致性。</li>
<li>结合条件损失抑制进行训练，进一步提高TSE性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09510">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-43fc28d510e5870690b262b85afa342e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c667beefa0bfb40f72ddf8d9bb7e7051.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0343895b91edcddd323df34abdc9aaf0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-65faec661cf398db8b629527aa51f395.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="The-DKU-System-for-Multi-Speaker-Automatic-Speech-Recognition-in-MLC-SLM-Challenge"><a href="#The-DKU-System-for-Multi-Speaker-Automatic-Speech-Recognition-in-MLC-SLM-Challenge" class="headerlink" title="The DKU System for Multi-Speaker Automatic Speech Recognition in MLC-SLM   Challenge"></a>The DKU System for Multi-Speaker Automatic Speech Recognition in MLC-SLM   Challenge</h2><p><strong>Authors:Yuke Lin, Ming Cheng, Ze Li, Ming Li</strong></p>
<p>We present the DKU system for Task 2 of the MLC-SLM Challenge, which aims to perform multi-speaker automatic speech recognition directly from raw audio without Oracle speaker labels or time boundaries. Our approach builds upon a diarization-aware framework integrating speaker embeddings and temporal utterance boundaries into a Qwen2.5-based large language model (LLM). Then, we enhance the system’s multilingual performance by fine-tuning language-specific adapters and LoRA modules within the LLM decoder. Finally, our system achieves the tcpWER of 23.56% and 18.08% on the development and test sets of the MLC-SLM dataset, substantially outperforming the official baseline. </p>
<blockquote>
<p>我们为MLC-SLM挑战的Task 2提出了DKU系统，旨在直接从原始音频执行多发言人自动语音识别，无需Oracle发言人标签或时间边界。我们的方法基于一个融合发言人嵌入和时序话语边界的识辨化框架，该框架以基于Qwen2.5的大型语言模型（LLM）为基础。然后，我们通过微调LLM解码器中的语言特定适配器和LoRA模块，增强系统的多语言能力。最后，我们的系统在MLC-SLM数据集的开发集和测试集上实现了tcpWER分别为23.56%和18.08%，大幅超越了官方基线。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09499v1">PDF</a> Technical Report for MLC-SLM Challenge in Interspeech2025</p>
<p><strong>Summary</strong><br>     本研究提出了DKU系统，用于MLC-SLM挑战任务2，旨在直接从原始音频进行多说话者自动语音识别，无需Oracle说话者标签或时间边界。研究采用基于语音识别模型的结合说话者嵌入和临时话语边界的基于Qwen2.5的大型语言模型（LLM）。通过微调语言特定适配器和LoRA模块，增强了系统的多语言能力。最终，在MLC-SLM数据集的开发和测试集上，该系统实现了tcpWER分别为23.56%和18.08%，明显优于官方基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究提出DKU系统，针对MLC-SLM挑战任务2的多说话者自动语音识别任务。</li>
<li>系统无需Oracle说话者标签或时间边界信息。</li>
<li>系统采用基于Qwen2.5的大型语言模型（LLM），集成说话者嵌入和临时话语边界。</li>
<li>通过微调语言特定适配器和LoRA模块，增强了系统的多语言能力。</li>
<li>系统在MLC-SLM数据集的开发集上实现了tcpWER为23.56%。</li>
<li>系统在MLC-SLM数据集的测试集上实现了tcpWER为18.08%，优于官方基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09499">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3bfd72b308f5f7f3d361c7bd470c5734.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c1de88bda71dcbd687ff39a44ee5058.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Enhancing-ALS-Progression-Tracking-with-Semi-Supervised-ALSFRS-R-Scores-Estimated-from-Ambient-Home-Health-Monitoring"><a href="#Enhancing-ALS-Progression-Tracking-with-Semi-Supervised-ALSFRS-R-Scores-Estimated-from-Ambient-Home-Health-Monitoring" class="headerlink" title="Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores   Estimated from Ambient Home Health Monitoring"></a>Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores   Estimated from Ambient Home Health Monitoring</h2><p><strong>Authors:Noah Marchal, William E. Janes, Mihail Popescu, Xing Song</strong></p>
<p>Clinical monitoring of functional decline in ALS relies on periodic assessments that may miss critical changes occurring between visits. To address this gap, semi-supervised regression models were developed to estimate rates of decline in a case series cohort by targeting ALSFRS- R scale trajectories with continuous in-home sensor monitoring data. Our analysis compared three model paradigms (individual batch learning and cohort-level batch versus incremental fine-tuned transfer learning) across linear slope, cubic polynomial, and ensembled self-attention pseudo-label interpolations. Results revealed cohort homogeneity across functional domains responding to learning methods, with transfer learning improving prediction error for ALSFRS-R subscales in 28 of 32 contrasts (mean RMSE&#x3D;0.20(0.04)), and individual batch learning for predicting the composite scale (mean RMSE&#x3D;3.15(1.25)) in 2 of 3. Self-attention interpolation achieved the lowest prediction error for subscale-level models (mean RMSE&#x3D;0.19(0.06)), capturing complex nonlinear progression patterns, outperforming linear and cubic interpolations in 20 of 32 contrasts, though linear interpolation proved more stable in all ALSFRS-R composite scale models (mean RMSE&#x3D;0.23(0.10)). We identified distinct homogeneity-heterogeneity profiles across functional domains with respiratory and speech exhibiting patient-specific patterns benefiting from personalized incremental adaptation, while swallowing and dressing functions followed cohort-level trajectories suitable for transfer models. These findings suggest that matching learning and pseudo-labeling techniques to functional domain-specific homogeneity-heterogeneity profiles enhances predictive accuracy in ALS progression tracking. Integrating adaptive model selection within sensor monitoring platforms could enable timely interventions and scalable deployment in future multi-center studies. </p>
<blockquote>
<p>临床监测肌萎缩侧索硬化症（ALS）的功能衰退依赖于定期评估，可能会错过就诊期间发生的重大变化。为了解决这一空白，开发了半监督回归模型，通过针对ALSFRS-R量表轨迹与连续居家传感器监测数据，估计系列病例的衰退率。我们的分析比较了三种模型范式（个体批量学习、群体层面批量学习与增量精细调整迁移学习）在线性斜率、三次多项式以及集成自注意力伪标签插值方面的应用。结果显示，群体在各功能领域对学习方法有同质性反应，迁移学习在ALSFRS-R子量表预测误差的对比中改善了28项中的23项（平均RMSE&#x3D;0.20（0.04）），个体批量学习在预测综合指标方面仅在两项对比中的一项表现出色（平均RMSE&#x3D;3.15（1.25））。自注意力插值在子量表级别模型中实现了最低的预测误差（平均RMSE&#x3D;0.19（0.06）），能够捕捉复杂的非线性进展模式，在32项对比中优于线性插值和三次插值20项，尽管线性插值在所有ALSFRS-R综合指标模型中表现更稳定（平均RMSE&#x3D;0.23（0.10））。我们确定了不同功能领域的同质性-异质性分布特征，其中呼吸和言语具有特定的患者模式，受益于个性化增量适应，而吞咽和穿衣功能遵循群体层面轨迹，适合迁移模型。这些发现表明，将学习与伪标签技术与功能域特定的同质性-异质性分布特征相匹配，可提高ALS进展跟踪的预测准确性。在传感器监测平台中整合自适应模型选择可为未来多中心研究提供及时的干预和可规模化部署的能力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09460v1">PDF</a> 31 pages, 8 Figures</p>
<p><strong>摘要</strong></p>
<p>本文开发半监督回归模型，以针对渐冻人症（ALS）的功能衰退进行临床监测。研究采用连续居家传感器监测数据，通过三种模型范式（个体批量学习、群体水平批量学习与增量精细调整转移学习）分析ALS功能衰退速率。研究结果显示，转移学习在预测ALS功能性量表（ALSFRS-R）子量表方面的预测误差有所改进，在32次对比中的28次表现出更佳的均方根误差（RMSE）。自我注意插值在子量表级别模型中实现最低预测误差，并捕获复杂的非线性进展模式。然而，线性插值在ALSFRS-R综合量表模型中表现更稳定。研究还发现，不同功能领域的同质性和异质性特征影响模型效果，如呼吸和言语功能受益于个性化增量适应，而吞咽和穿衣功能适合使用转移模型。总之，该研究建议匹配学习与伪标签技术至特定功能领域的同质性和异质性特征，以提高ALS进展跟踪的预测准确性。整合自适应模型选择进入传感器监测平台可实现及时干预和大规模部署。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>半监督回归模型用于估计ALS的功能衰退速率，使用连续居家传感器监测数据。</li>
<li>比较了三种模型范式，包括个体批量学习、群体水平批量学习与增量精细调整转移学习。</li>
<li>转移学习在预测ALSFRS-R子量表的预测误差方面表现出较好的效果。</li>
<li>自我注意插值在子量表级别模型中实现最低预测误差，捕捉复杂的非线性进展模式。</li>
<li>不同功能领域（如呼吸、言语、吞咽和穿衣）的同质性和异质性特征影响模型效果。</li>
<li>研究建议根据功能领域的同质性和异质性特征匹配学习和伪标签技术，以提高预测准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09460">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-d205114c2da61d892c8eb50d77894524.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5fe68646a1f27403a96f85f7257cdd64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab1cbaf979cf5545f0bab9d12c071a01.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Mixture-of-LoRA-Experts-with-Multi-Modal-and-Multi-Granularity-LLM-Generative-Error-Correction-for-Accented-Speech-Recognition"><a href="#Mixture-of-LoRA-Experts-with-Multi-Modal-and-Multi-Granularity-LLM-Generative-Error-Correction-for-Accented-Speech-Recognition" class="headerlink" title="Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM   Generative Error Correction for Accented Speech Recognition"></a>Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM   Generative Error Correction for Accented Speech Recognition</h2><p><strong>Authors:Bingshen Mu, Kun Wei, Pengcheng Guo, Lei Xie</strong></p>
<p>Despite substantial improvements in ASR, performance tends to degrade when faced with adverse conditions such as speaker accents. Generative error correction (GER) leverages the rich linguistic knowledge and exceptional reasoning ability of LLMs, significantly outperforming typical LM methods. However, it lacks specificity in accented speech scenarios. In this study, we leverage GER to improve the accuracy of transcription predictions by addressing the two primary features of accented speech recognition. To fully leverage pronunciation information, we propose the multi-modal GER, which integrates pronunciation information from the speech modality, and the multi-granularity GER, which incorporates fine-grained phoneme-level information related to pronunciation. These two methods enable the LLM to utilize the pronunciation information of accented speech and the semantic information from word-level hypotheses for accurate transcription predictions through LoRA fine-tuning. On the one hand, we employ a three-stage training strategy to train separate multi-modal GER models for each accent to obtain mono-accent LoRA experts. By adopting our proposed HDMoLE method, which incorporates hierarchical routing and dynamic thresholds within the mixture of LoRA experts, we effectively merge multiple mono-accent LoRA experts within a single multi-modal GER to overcome the challenges posed by accent diversity. On the other hand, multi-granularity GER leverages the N-best word-level and phoneme-level hypotheses generated by the HDMoLE model to predict the final accented speech transcriptions. Experimental results on the multi-accent English dataset demonstrate the efficacy of our proposed methods. Our methods achieve a remarkable relative WER reduction of 67.35% compared to the Whisper-large-v3 baseline. </p>
<blockquote>
<p>尽管语音识别技术（ASR）有了实质性的改进，但在面对如说话者口音等不利条件时，性能往往会有所下降。生成错误校正（GER）能够利用大型语言模型（LLM）丰富的语言知识和出色的推理能力，显著优于典型的语言模型方法。然而，它在带有口音的语音场景上缺乏特异性。本研究中，我们利用GER来提高转录预测的准确性，通过解决带口音语音识别的两个主要特征来实现这一目标。为了充分利用发音信息，我们提出了多模态GER，它结合了语音模态的发音信息，以及多粒度GER，它结合了与发音相关的精细音素级信息。这两种方法使LLM能够通过LoRA微调利用带口音语音的发音信息和词级假设的语义信息进行准确的转录预测。一方面，我们采用三阶段训练策略，针对每种口音训练单独的多模态GER模型，以获得单口音LoRA专家。通过采用我们提出的HDMoLE方法，该方法结合了层次路由和动态阈值在LoRA专家混合体中，我们有效地将多个单口音LoRA专家合并到一个单一的多模态GER中，以克服口音多样性带来的挑战。另一方面，多粒度GER利用HDMoLE模型生成的N个最佳词级和音素级假设来预测最终的带口音语音转录。在多口音英语数据集上的实验结果表明了我们提出的方法的有效性。我们的方法相较于Whisper-large-v3基线实现了相对的字错误率（WER）降低了67.35%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09116v2">PDF</a> IEEE Transactions on Audio, Speech and Language Processing</p>
<p><strong>Summary</strong></p>
<p>本研究针对带有口音的语音识别问题，提出了多模态生成式错误修正（GER）方法和多粒度GER方法，以提高发音信息的准确性和转录预测的字级语义信息的利用。通过LoRA微调技术，结合口音的发音信息和字级假设信息，对带口音的语音进行精准转录。采用三阶段训练策略，为每种口音训练单独的多模态GER模型，并利用HDMoLE方法有效合并多个单口音LoRA专家，以应对口音多样性带来的挑战。同时，多粒度GER利用N-best的字级和音素级假设进行最终带口音语音转录预测。在多项英语口音数据集上的实验结果显示，该方法相比基准模型实现了67.35%的相对WER减少。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>生成式错误修正（GER）在处理带有口音的语音识别时表现突出，相较于传统的LM方法具有显著优势。</li>
<li>多模态GER方法融合了语音模态的发音信息，提高了发音的准确性。</li>
<li>多粒度GER方法结合了音素级的精细信息和字级的语义信息，用于准确转录带口音的语音。</li>
<li>采用三阶段训练策略，针对每种口音训练单独的多模态GER模型，以应对口音多样性。</li>
<li>HDMoLE方法有效合并多个单口音LoRA专家模型，提高了模型处理多种口音的能力。</li>
<li>多粒度GER利用N-best假设进行最终转录预测，提高了预测的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09116">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f840240a9d373b5dcad6630a8b1151bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d51f21573d5ccba195b4e0d0d83ca8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0c9a6a4b9dcab3cbd6bf738e840c195.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b478a7d57278cbad302443f69119351b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Dynamic-Parameter-Memory-Temporary-LoRA-Enhanced-LLM-for-Long-Sequence-Emotion-Recognition-in-Conversation"><a href="#Dynamic-Parameter-Memory-Temporary-LoRA-Enhanced-LLM-for-Long-Sequence-Emotion-Recognition-in-Conversation" class="headerlink" title="Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence   Emotion Recognition in Conversation"></a>Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence   Emotion Recognition in Conversation</h2><p><strong>Authors:Jialong Mai, Xiaofen Xing, Yawei Li, Zhipeng Li, Jingyuan Xing, Xiangmin Xu</strong></p>
<p>Recent research has focused on applying speech large language model (SLLM) to improve speech emotion recognition (SER). However, the inherently high frame rate in speech modality severely limits the signal processing and understanding capabilities of SLLM. For example, a SLLM with a 4K context window can only process 80 seconds of audio at 50Hz feature sampling rate before reaching its capacity limit. Input token compression methods used in SLLM overlook the continuity and inertia of emotions across multiple conversation turns. This paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual semantics and sentence-level emotion encoding, enabling processing of unlimited-length audio with limited context windows in SLLM. Specifically, DPM progressively encodes sentence-level information and emotions into a temporary LoRA module during inference to effectively “memorize” the contextual information. We trained an emotion SLLM as a backbone and incorporated our DPM into inference for emotion recognition in conversation (ERC). Experimental results on the IEMOCAP dataset show that DPM significantly improves the emotion recognition capabilities of SLLM when processing long audio sequences, achieving state-of-the-art performance. </p>
<blockquote>
<p>最近的研究集中在将语音大语言模型（SLLM）应用于提高语音情感识别（SER）上。然而，语音模态本身的高帧率严重限制了SLLM的信号处理和理解能力。例如，一个具有4K上下文窗口的SLLM在50Hz的特征采样率下只能处理80秒的音频，然后就会达到其容量限制。SLLM中使用的输入令牌压缩方法忽略了情感在多轮对话中的连续性和惯性。本文提出了一种具有上下文语义和句子级情感编码的动态参数内存（DPM）机制，能够在SLLM中的有限上下文窗口中处理无限长度的音频。具体来说，DPM在推理过程中将句子级信息和情感逐步编码到临时的LoRA模块中，以有效地“记忆”上下文信息。我们训练了一个情感SLLM作为主干，并将我们的DPM用于对话情感识别（ERC）的推理。在IEMOCAP数据集上的实验结果表明，在处理长音频序列时，DPM显著提高了SLLM的情感识别能力，达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09076v1">PDF</a> submitted to EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>近期研究将语音大语言模型（SLLM）应用于语音情感识别（SER）的改进。然而，语音模态的高帧率严重限制了SLLM的信号处理和理解能力。本文提出一种动态参数记忆（DPM）机制，结合上下文语义和句子级情感编码，在有限的语境窗口中处理无限长度的音频。DPM能够逐步将句子级信息和情感编码到临时LoRA模块中，有效“记忆”上下文信息。在IEMOCAP数据集上的实验结果表明，DPM在处理长音频序列时显著提高了SLLM的情感识别能力，达到了先进性能水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SLLM在语音情感识别中的局限性：高帧率导致的信号处理和理解能力受限。</li>
<li>DPM机制介绍：结合上下文语义和句子级情感编码，处理无限长度音频。</li>
<li>DPM的工作原理：在推理过程中逐步编码句子级信息和情感到临时LoRA模块中。</li>
<li>实验结果：在IEMOCAP数据集上，DPM显著提高SLLM处理长音频序列时的情感识别能力。</li>
<li>DPM达到先进性能水平。</li>
<li>输入令牌压缩方法的不足：忽略情感的连续性和惯性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09076">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-795a820129dd0747a05e68ff98b90a6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29bc00adbfdc6c0488dd37201922ba58.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10e2777b2c59f6d48de7b0af7579acc7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="SemAlignVC-Enhancing-zero-shot-timbre-conversion-using-semantic-alignment"><a href="#SemAlignVC-Enhancing-zero-shot-timbre-conversion-using-semantic-alignment" class="headerlink" title="SemAlignVC: Enhancing zero-shot timbre conversion using semantic   alignment"></a>SemAlignVC: Enhancing zero-shot timbre conversion using semantic   alignment</h2><p><strong>Authors:Shivam Mehta, Yingru Liu, Zhenyu Tang, Kainan Peng, Vimal Manohar, Shun Zhang, Mike Seltzer, Qing He, Mingbo Ma</strong></p>
<p>Zero-shot voice conversion (VC) synthesizes speech in a target speaker’s voice while preserving linguistic and paralinguistic content. However, timbre leakage-where source speaker traits persist-remains a challenge, especially in neural codec and LLM-based VC, where quantized representations entangle speaker identity with content. We introduce SemAlignVC, an architecture designed to prevent timbre leakage using SemAlign, a novel method that aligns text and audio representations to ensure speaker-independent semantic encoding. This disentangled representation conditions an autoregressive transformer for high-fidelity conversion without explicit speaker embeddings. Experiments show SemAlignVC significantly reduces timbre leakage, outperforming baselines in speaker timbre similarity, intelligibility, and naturalness, making it a robust, privacy-preserving, and generalizable VC solution. Audio samples can be accessed at <a target="_blank" rel="noopener" href="https://shivammehta25.github.io/SemAlignVC/">https://shivammehta25.github.io/SemAlignVC/</a> </p>
<blockquote>
<p>零样本语音转换（VC）技术能够在目标说话者的声音中合成语音，同时保留语言和副语言内容。然而，声纹泄露（即源说话者的特征持续存在）仍然是一个挑战，特别是在基于神经网络编解码器和大型语言模型（LLM）的VC中，其中量化表示将说话人身份与内容纠缠在一起。我们引入了SemAlignVC，这是一种旨在防止声纹泄露的架构，它使用了SemAlign这一新方法，通过文本和音频表示的对齐来确保与说话人无关的语义编码。这种解纠缠的表示为一个自回归变压器提供条件，以实现高保真转换，无需明确的说话人嵌入。实验表明，SemAlignVC能显著降低声纹泄露，在说话人声纹相似性、清晰度和自然度方面超越基线，成为了一种稳健、保护隐私且可推广的VC解决方案。音频样本可通过<a target="_blank" rel="noopener" href="https://shivammehta25.github.io/SemAlignVC/%E8%AE%BF%E9%97%AE%E3%80%82">https://shivammehta25.github.io/SemAlignVC/访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09070v1">PDF</a> 6 pages, 2 figures, Accepted at the ISCA Speech Synthesis Workshop   (SSW) 2025</p>
<p><strong>Summary</strong>：零样本语音转换技术旨在合成目标说话人的语音，同时保留语言和副语言内容。然而，声纹泄露（即源说话人的特征持续存在）仍然是一个挑战，特别是在神经网络编码器和大型语言模型（LLM）的语音转换中，量化表示将说话人身份与内容纠缠在一起。为解决这一问题，我们提出了SemAlignVC架构和其中的SemAlign新方法，该方法通过对文本和音频表示进行对齐来防止声纹泄露，确保独立于说话人的语义编码。这种分离表示条件自回归变压器可实现高保真转换，无需明确的说话人嵌入。实验表明，SemAlignVC在声纹泄露方面表现出显著降低，在说话人声纹相似性、清晰度和自然度方面优于基线方法，成为了一种稳健、保护隐私和通用的语音转换解决方案。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>零样本语音转换旨在合成目标说话人的语音并保留语言和副语言内容。</li>
<li>声纹泄露是语音转换技术中的一个挑战，特别是在神经网络编码器和大型语言模型的应用中。</li>
<li>SemAlignVC架构通过SemAlign方法解决声纹泄露问题，通过对文本和音频表示进行对齐来确保独立于说话人的语义编码。</li>
<li>实验证明SemAlignVC在声纹泄露、说话人声纹相似性、清晰度和自然度方面优于基线方法。</li>
<li>SemAlignVC不需要明确的说话人嵌入，可实现高保真转换。</li>
<li>音频样本可通过特定链接访问。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09070">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8302225aec83c97e3732b81f4df825dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8af15792354acba17c8d2520fc431a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f1dd18ce023e2b04f88883d0532afde8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="StreamUni-Achieving-Streaming-Speech-Translation-with-a-Unified-Large-Speech-Language-Model"><a href="#StreamUni-Achieving-Streaming-Speech-Translation-with-a-Unified-Large-Speech-Language-Model" class="headerlink" title="StreamUni: Achieving Streaming Speech Translation with a Unified Large   Speech-Language Model"></a>StreamUni: Achieving Streaming Speech Translation with a Unified Large   Speech-Language Model</h2><p><strong>Authors:Shoutao Guo, Xiang Li, Mengge Liu, Wei Chen, Yang Feng</strong></p>
<p>Streaming speech translation (StreamST) requires determining appropriate timing, known as policy, to generate translations while continuously receiving source speech inputs, balancing low latency with high translation quality. However, existing StreamST methods typically operate on sentence-level speech segments, referred to as simultaneous speech translation (SimulST). In practice, they require collaboration with segmentation models to accomplish StreamST, where the truncated speech segments constrain SimulST models to make policy decisions and generate translations based on limited contextual information. Moreover, SimulST models struggle to learn effective policies due to the complexity of speech inputs and cross-lingual generation. To address these challenges, we propose StreamUni, which achieves StreamST through a unified Large Speech-Language Model (LSLM). Specifically, StreamUni incorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate multi-stage outputs. Leveraging these multi-stage outputs, StreamUni simultaneously accomplishes speech segmentation, policy decision, and translation generation, completing StreamST without requiring massive policy-specific training. Additionally, we propose a streaming CoT training method that enhances low-latency policy decisions and generation capabilities using limited CoT data. Experiments demonstrate that our approach achieves state-of-the-art performance on StreamST tasks. </p>
<blockquote>
<p>流式语音翻译（StreamST）要求在连续接收源语音输入的过程中确定适当的翻译生成时机，这种时机被称为策略，需要在低延迟和高翻译质量之间取得平衡。然而，现有的StreamST方法通常运行在句子级别的语音段上，称为同步语音翻译（SimulST）。在实践中，它们需要与分割模型协作来完成StreamST，截断的语音段限制SimulST模型在有限的上下文信息基础上做出策略决策并生成翻译。而且，由于语音输入的复杂性和跨语言生成，SimulST模型在学习有效策略方面面临困难。为了解决这些挑战，我们提出了StreamUni，它通过统一的大规模语音识别语言模型（LSLM）实现StreamST。具体来说，StreamUni将语音思维链（CoT）融入LSLM，以指导生成多阶段输出。利用这些多阶段输出，StreamUni可以同时完成语音分割、策略决策和翻译生成，无需大量的特定策略训练即可完成StreamST。此外，我们还提出了一种流式CoT训练方法，使用有限的CoT数据增强低延迟的策略决策和生成能力。实验表明，我们的方法在StreamST任务上达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07803v2">PDF</a> The code is at <a target="_blank" rel="noopener" href="https://github.com/ictnlp/StreamUni">https://github.com/ictnlp/StreamUni</a>; The model is at   <a target="_blank" rel="noopener" href="https://huggingface.co/ICTNLP/StreamUni-Phi4">https://huggingface.co/ICTNLP/StreamUni-Phi4</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于统一大规模语言模型的流式语音识别翻译方法StreamUni，通过融入语音链式思维（CoT）生成多阶段输出，实现了无需大规模政策特定训练的流式语音识别翻译。此方法可同时进行语音分段、政策决策和翻译生成，解决了现有方法中需要协作分段模型、基于有限语境信息做出决策和生成翻译的问题。此外，还提出了一种增强低延迟政策决策和生成能力的流式CoT训练方法。实验证明，该方法在流式语音识别翻译任务上取得了最新技术水平的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有流式语音识别翻译（StreamST）方法通常基于句子级别的语音段进行，称为同步语音识别翻译（SimulST）。</li>
<li>SimulST方法需要与分段模型协作，受截断语音段的限制，基于有限语境信息做出决策和生成翻译。</li>
<li>StreamUni通过统一大规模语言模型（LSLM）实现StreamST，融入语音链式思维（CoT）生成多阶段输出，可同时完成语音分段、政策决策和翻译生成。</li>
<li>StreamUni方法无需大规模政策特定训练，解决了SimulST模型在复杂语音输入和跨语言生成方面学习有效政策的困难。</li>
<li>提出了一种增强低延迟政策决策和生成能力的流式CoT训练方法，使用有限的CoT数据。</li>
<li>实验证明，StreamUni方法在流式语音识别翻译任务上取得了最新技术水平的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07803">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7aa4f9aabf622655863aed051892f6e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb5687da2a0140e0a72d7d68eba827e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39483ed4c1810579c02d11932d5c9584.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96fbccb328040e28f695ae29bb9c45c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a38f2576b62c39aeb1af709c2871479c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DeepGesture-A-conversational-gesture-synthesis-system-based-on-emotions-and-semantics"><a href="#DeepGesture-A-conversational-gesture-synthesis-system-based-on-emotions-and-semantics" class="headerlink" title="DeepGesture: A conversational gesture synthesis system based on emotions   and semantics"></a>DeepGesture: A conversational gesture synthesis system based on emotions   and semantics</h2><p><strong>Authors:Thanh Hoang-Minh</strong></p>
<p>Along with the explosion of large language models, improvements in speech synthesis, advancements in hardware, and the evolution of computer graphics, the current bottleneck in creating digital humans lies in generating character movements that correspond naturally to text or speech inputs.   In this work, we present DeepGesture, a diffusion-based gesture synthesis framework for generating expressive co-speech gestures conditioned on multimodal signals - text, speech, emotion, and seed motion. Built upon the DiffuseStyleGesture model, DeepGesture introduces novel architectural enhancements that improve semantic alignment and emotional expressiveness in generated gestures. Specifically, we integrate fast text transcriptions as semantic conditioning and implement emotion-guided classifier-free diffusion to support controllable gesture generation across affective states. To visualize results, we implement a full rendering pipeline in Unity based on BVH output from the model. Evaluation on the ZeroEGGS dataset shows that DeepGesture produces gestures with improved human-likeness and contextual appropriateness. Our system supports interpolation between emotional states and demonstrates generalization to out-of-distribution speech, including synthetic voices - marking a step forward toward fully multimodal, emotionally aware digital humans.   Project page: <a target="_blank" rel="noopener" href="https://deepgesture.github.io/">https://deepgesture.github.io</a> </p>
<blockquote>
<p>随着大型语言模型的爆发，语音合成的改进，硬件的进步以及计算机图形的演变，当前创建数字人的瓶颈在于生成与文本或语音输入相对应的自然动作。在这项工作中，我们提出了DeepGesture，这是一个基于扩散的手势合成框架，能够根据文本、语音、情感和种子动作等多模态信号生成富有表现力的协同语音手势。DeepGesture建立在DiffuseStyleGesture模型的基础上，引入了新型架构改进，提高了语义对齐和生成手势的情感表现力。具体来说，我们将快速文本转录作为语义条件，并实现了情感引导的无分类扩散，以支持情感状态下的可控手势生成。为了可视化结果，我们在Unity中实现了基于模型BVH输出的完整渲染管道。在ZeroEGGS数据集上的评估表明，DeepGesture产生的手势在人性化和上下文恰当性方面有所改进。我们的系统支持情感状态之间的插值，并展示了对分布外语音的泛化能力，包括对合成声音的泛化——标志着朝着完全多模态、情感感知的数字人方向迈出了一步。项目页面：<a target="_blank" rel="noopener" href="https://deepgesture.github.io/">https://deepgesture.github.io</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03147v2">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://deepgesture.github.io/">https://deepgesture.github.io</a></p>
<p><strong>Summary</strong><br>文本描述了DeepGesture系统及其优势。随着大型语言模型的爆炸式发展，以及语音合成、硬件进步和计算机图形技术的提升，当前创建数字人类的瓶颈在于生成自然对应文本或语音输入的角色动作。DeepGesture是一个基于扩散的手势合成框架，能够根据文本、语音、情感和种子动作等多模式信号生成表达性共语手势。它改进了语义对齐和情绪表达，支持跨情感状态的可控手势生成。在Unity中实现的全渲染管道，基于模型的BVH输出可视化结果。在ZeroEGGS数据集上的评估显示，DeepGesture生成的手势更具人类性和上下文适当性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>当前数字人类的创建瓶颈在于生成自然对应文本或语音输入的角色动作。</li>
<li>DeepGesture是一个基于扩散的手势合成框架，旨在解决这一问题。</li>
<li>DeepGesture集成了多模式信号，如文本、语音、情感和种子动作，用于生成表达性共语手势。</li>
<li>它通过改进语义对齐和情绪表达来提高生成手势的自然度和表达力。</li>
<li>DeepGesture支持跨情感状态的可控手势生成。</li>
<li>在Unity中实现的全渲染管道可以可视化生成的手势。</li>
<li>在ZeroEGGS数据集上的评估显示，DeepGesture生成的手势更具人类性和上下文适当性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03147">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1a5a8b1bcabbcdbe56ee79c054fe0231.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-44c1ab7bd87836b3e9a09b1d464cdafd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5d65a590a2074cdf30172ff46b051b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ead5690402d79e82ecadb54846ddc273.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ee73770761afbb7a68fde25caa81cdd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Riemannian-Time-Warping-Multiple-Sequence-Alignment-in-Curved-Spaces"><a href="#Riemannian-Time-Warping-Multiple-Sequence-Alignment-in-Curved-Spaces" class="headerlink" title="Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces"></a>Riemannian Time Warping: Multiple Sequence Alignment in Curved Spaces</h2><p><strong>Authors:Julian Richter, Christopher A. Erdös, Christian Scheurer, Jochen J. Steil, Niels Dehio</strong></p>
<p>Temporal alignment of multiple signals through time warping is crucial in many fields, such as classification within speech recognition or robot motion learning. Almost all related works are limited to data in Euclidean space. Although an attempt was made in 2011 to adapt this concept to unit quaternions, a general extension to Riemannian manifolds remains absent. Given its importance for numerous applications in robotics and beyond, we introduce Riemannian Time Warping (RTW). This novel approach efficiently aligns multiple signals by considering the geometric structure of the Riemannian manifold in which the data is embedded. Extensive experiments on synthetic and real-world data, including tests with an LBR iiwa robot, demonstrate that RTW consistently outperforms state-of-the-art baselines in both averaging and classification tasks. </p>
<blockquote>
<p>多个信号通过时间扭曲的时间对齐在多个领域都至关重要，如在语音识别或机器人运动学习中的分类。几乎所有相关工作都局限于欧几里得空间中的数据。尽管在2011年有人试图将这一概念适应于单位四元数，但到黎曼流形的通用扩展仍然缺失。考虑到其在机器人技术及其以外的众多应用中的重要性，我们引入了黎曼时间扭曲（RTW）。这种方法通过考虑数据嵌入的黎曼流形的几何结构，可以有效地对齐多个信号。在合成数据和真实世界数据上的大量实验，包括对LBR iiwa机器人的测试，证明无论是在平均任务还是分类任务中，RTW始终优于最新的基线技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.01635v3">PDF</a> </p>
<p><strong>Summary</strong><br>     时空弯曲技术对于许多领域如语音识别或机器人运动学习中的分类至关重要。尽管已有尝试将概念扩展到单位四元数，但目前还缺乏将其扩展到黎曼流形的一般方法。我们引入黎曼时间弯曲（RTW），该方法考虑数据嵌入的黎曼流形的几何结构，有效对齐多个信号。在合成数据和真实世界数据上的广泛实验以及对LBR iiwa机器人的测试表明，RTW在平均和分类任务上均优于最新技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时空弯曲技术在多个领域（如语音识别和机器人运动学习）中的分类任务中起到关键作用。</li>
<li>目前大多数相关研究仅限于欧几里得空间内的数据。</li>
<li>尽管已有尝试将概念扩展到单位四元数，但将其一般扩展到黎曼流形的方法仍然缺失。</li>
<li>引入了一种新的方法——黎曼时间弯曲（RTW），该方法考虑数据嵌入的几何结构，实现对多个信号的有效对齐。</li>
<li>与现有技术相比，黎曼时间弯曲在平均和分类任务上的性能表现更优。</li>
<li>黎曼时间弯曲技术的广泛应用可能推动机器人技术等多个领域的发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.01635">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f19a3f2da8f513cf69dcb3c56ceab3d5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-83ae0378cbf3da662aecbd7a75af4895.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19b4aef7943e0bf946ecfb0942eb60c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8edf569a39679f1d05b6f32c032f3104.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3b6a9ee7f143c78e36bedd816a53046.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-171135d0842e0bea48b3d3981853b3fa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5d612bf694e16f7e4fe76604d00050cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-850b38fd781563baf865fb4c81ce0712.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9c3f2f01f840277aabb6cd8c43d49675.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Tiny-Align-Bridging-Automatic-Speech-Recognition-and-Large-Language-Model-on-the-Edge"><a href="#Tiny-Align-Bridging-Automatic-Speech-Recognition-and-Large-Language-Model-on-the-Edge" class="headerlink" title="Tiny-Align: Bridging Automatic Speech Recognition and Large Language   Model on the Edge"></a>Tiny-Align: Bridging Automatic Speech Recognition and Large Language   Model on the Edge</h2><p><strong>Authors:Ruiyang Qin, Dancheng Liu, Gelei Xu, Zheyu Yan, Chenhui Xu, Yuting Hu, Shaocong Wang, X. Sharon Hu, Jinjun Xiong, Yiyu Shi</strong></p>
<p>The combination of Large Language Models (LLM) and Automatic Speech Recognition (ASR), when deployed on edge devices (called edge ASR-LLM), can serve as a powerful personalized assistant to enable audio-based interaction for users. Compared to text-based interaction, edge ASR-LLM allows accessible and natural audio interactions. Unfortunately, existing ASR-LLM models are mainly trained in high-performance computing environments and produce substantial model weights, making them difficult to deploy on edge devices. More importantly, to better serve users’ personalized needs, the ASR-LLM must be able to learn from each distinct user, given that audio input often contains highly personalized characteristics that necessitate personalized on-device training. Since individually fine-tuning the ASR or LLM often leads to suboptimal results due to modality-specific limitations, end-to-end training ensures seamless integration of audio features and language understanding (cross-modal alignment), ultimately enabling a more personalized and efficient adaptation on edge devices. However, due to the complex training requirements and substantial computational demands of existing approaches, cross-modal alignment between ASR audio and LLM can be challenging on edge devices. In this work, we propose a resource-efficient cross-modal alignment framework that bridges ASR and LLMs on edge devices to handle personalized audio input. Our framework enables efficient ASR-LLM alignment on resource-constrained devices like NVIDIA Jetson Orin (8GB RAM), achieving 50x training time speedup while improving the alignment quality by more than 50%. To the best of our knowledge, this is the first work to study efficient ASR-LLM alignment on resource-constrained edge devices. </p>
<blockquote>
<p>大型语言模型（LLM）与自动语音识别（ASR）的结合，在边缘设备上部署（称为边缘ASR-LLM）时，可作为强大的个性化助手，使用户能够基于音频进行交互。与基于文本的交互相比，边缘ASR-LLM允许可访问和自然的音频交互。然而，现有的ASR-LLM模型主要在高性能计算环境中进行训练，产生大量的模型权重，使得它们难以在边缘设备上部署。更重要的是，为了更好地满足用户的个性化需求，ASR-LLM必须能够从每个不同的用户身上学习，因为音频输入通常包含高度个性化的特征，需要进行个性化的设备端训练。由于单独微调ASR或LLM往往会导致次优结果，端到端训练确保音频特征和语言理解的无缝集成（跨模态对齐），最终在边缘设备上实现更个性化和高效的适应。然而，由于现有方法的复杂训练要求和巨大的计算需求，ASR音频和LLM之间的跨模态对齐在边缘设备上可能具有挑战性。在这项工作中，我们提出了一个资源高效的跨模态对齐框架，该框架可在边缘设备上实现ASR和LLM的衔接，以处理个性化的音频输入。我们的框架能够在像NVIDIA Jetson Orin（8GB RAM）这样的资源受限设备上实现高效的ASR-LLM对齐，实现50倍的培训时间加速，同时提高对齐质量超过50%。据我们所知，这是第一项在资源受限的边缘设备上研究高效ASR-LLM对齐的工作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.13766v4">PDF</a> Accepted by ICCAD’25</p>
<p><strong>摘要</strong></p>
<p>在边缘设备上部署大型语言模型（LLM）与自动语音识别（ASR）的组合（称为边缘ASR-LLM），可作为强大的个性化助手，实现基于音频的用户交互。相较于文本交互，边缘ASR-LLM允许更便捷、更自然的音频交互。然而，现有的ASR-LLM模型主要在高性能计算环境中训练，模型体积庞大，难以部署在边缘设备上。更重要的是，为了更好地服务用户的个性化需求，ASR-LLM必须具备从每个独立用户身上学习的能力，因为音频输入往往包含高度个性化的特征，需要进行个性化的设备端训练。由于单独微调ASR或LLM往往因模态特定限制而导致结果不佳，端到端训练可确保音频特征与语言理解的无缝集成（跨模态对齐），最终在边缘设备上实现更个性化和高效的适应。然而，由于现有方法的复杂训练要求和巨大的计算需求，在边缘设备上实现ASR音频和LLM之间的跨模态对齐具有挑战性。本研究提出一种资源高效的跨模态对齐框架，该框架可在边缘设备上桥接ASR和LLM，处理个性化音频输入。我们的框架可在资源受限的设备上实现高效的ASR-LLM对齐，如8GB RAM的NVIDIA Jetson Orin，实现50倍的培训时间加速，同时提高对齐质量超过50%。据我们所知，这是第一项在资源受限的边缘设备上研究高效ASR-LLM对齐的工作。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>边缘设备上的大型语言模型（LLM）与自动语音识别（ASR）结合可提供强大的个性化助手功能，支持音频交互。</li>
<li>现有ASR-LLM模型主要在高性能环境中训练，难以部署在资源有限的边缘设备上。</li>
<li>个性化音频输入需要个性化设备训练，因为音频包含高度个性化的特征。</li>
<li>单独微调ASR或LLM因模态特定限制可能效果不佳，端到端训练可实现跨模态无缝集成（跨模态对齐）。</li>
<li>现有方法在边缘设备上实现跨模态对齐具有挑战，因为复杂的训练要求和计算需求。</li>
<li>提出的资源高效跨模态对齐框架可在边缘设备上桥接ASR和LLM，实现个性化音频输入的高效处理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.13766">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-66e76953ba7badc26515e06b5223ce49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-777a1bedc89f16a499c8ed120fba64f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14077e5b275adf181c92b7c6d9a2b37e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fb780f993d6475ac9ad5e06c367a034.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="A-Survey-on-Speech-Deepfake-Detection"><a href="#A-Survey-on-Speech-Deepfake-Detection" class="headerlink" title="A Survey on Speech Deepfake Detection"></a>A Survey on Speech Deepfake Detection</h2><p><strong>Authors:Menglu Li, Yasaman Ahmadiadli, Xiao-Ping Zhang</strong></p>
<p>The availability of smart devices leads to an exponential increase in multimedia content. However, advancements in deep learning have also enabled the creation of highly sophisticated Deepfake content, including speech Deepfakes, which pose a serious threat by generating realistic voices and spreading misinformation. To combat this, numerous challenges have been organized to advance speech Deepfake detection techniques. In this survey, we systematically analyze more than 200 papers published up to March 2024. We provide a comprehensive review of each component in the detection pipeline, including model architectures, optimization techniques, generalizability, evaluation metrics, performance comparisons, available datasets, and open source availability. For each aspect, we assess recent progress and discuss ongoing challenges. In addition, we explore emerging topics such as partial Deepfake detection, cross-dataset evaluation, and defences against adversarial attacks, while suggesting promising research directions. This survey not only identifies the current state of the art to establish strong baselines for future experiments but also offers clear guidance for researchers aiming to enhance speech Deepfake detection systems. </p>
<blockquote>
<p>智能设备的普及导致多媒体内容呈指数级增长。然而，深度学习的发展也催生了高度复杂的深度伪造内容（Deepfake）的生成，其中包括语音深度伪造（Speech Deepfakes），它能生成逼真的声音并传播错误信息，构成严重威胁。为了应对这一问题，已经组织了多次挑战来推进语音深度伪造检测技术的发展。在这篇综述中，我们系统地分析了截至2024年3月发表的超过200篇论文。我们对检测流程中的每个组件进行了全面的回顾，包括模型架构、优化技术、通用性、评价指标、性能比较、可用数据集和开源可用性。对于每个方面，我们都评估了最新的进展并讨论了持续存在的挑战。此外，我们还探讨了新兴话题，如局部深度伪造检测、跨数据集评估和对抗攻击的防御策略，同时提出了有前景的研究方向。这篇综述不仅确定了当前的研究现状，为未来的实验建立了强大的基准线，而且也为旨在提高语音深度伪造检测系统的研究者提供了明确的指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.13914v2">PDF</a> 38 pages. This paper has been accepted by ACM Computing Surveys</p>
<p><strong>Summary</strong><br>     智能设备的发展导致多媒体内容呈指数级增长，深度学习技术的进步也催生了高度复杂化的Deepfake内容，包括语音Deepfakes。它们通过生成真实声音和扩散错误信息构成严重威胁。为应对此挑战，已组织多次竞赛推动语音Deepfake检测技术的发展。本文系统分析了超过200篇至2024年3月发表的论文，全面评述检测流程中的每个组件，包括模型架构、优化技术、通用性、评估指标、性能对比、可用数据集和开源可用性。同时探讨新兴话题如部分Deepfake检测、跨数据集评估和对抗攻击的防御策略，并为未来研究指明方向。本文不仅确定了当前研究的顶尖水平，还为希望提高语音Deepfake检测系统的研究人员提供了清晰的指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>智能设备普及导致多媒体内容激增，包括高度复杂的Deepfake内容。</li>
<li>语音Deepfakes通过生成真实声音和扩散错误信息构成严重威胁。</li>
<li>为应对语音Deepfake的挑战，已组织多次竞赛推动检测技术发展。</li>
<li>本文系统分析了超过200篇关于语音Deepfake检测的论文，全面评述检测流程中的各个组件。</li>
<li>文章探讨了新兴话题，如部分Deepfake检测、跨数据集评价和对抗攻击的防御策略。</li>
<li>本文确定了当前研究的顶尖水平，为建立未来实验提供了强基准线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.13914">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-46c7b82c20f95b9547a3d402c3815579.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ba7fe4604594ff117ec1a81463eda82.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-40a5b62c3e87fc1dce51a7eab8eeec3c.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-07-17  SystolicAttention Fusing FlashAttention within a Single Systolic Array
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-742e7ae4eb6b5d512238db3bd8bae61e.jpg" class="responsive-img" alt="医学影像/Breast Ultrasound">
                        
                        <span class="card-title">医学影像/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学影像/Breast Ultrasound 方向最新论文已更新，请持续关注 Update in 2025-07-17  Focus on Texture Rethinking Pre-training in Masked Autoencoders for   Medical Image Classification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    医学影像/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">医学影像/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26384.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
