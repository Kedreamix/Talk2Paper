<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  Alleviating Textual Reliance in Medical Language-guided Segmentation via   Prototype-driven Semantic Approximation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6c30f54fa848454ef5c022bbc33b3c15.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-08
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    49 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-17-æ›´æ–°"><a href="#2025-07-17-æ›´æ–°" class="headerlink" title="2025-07-17 æ›´æ–°"></a>2025-07-17 æ›´æ–°</h1><h2 id="Alleviating-Textual-Reliance-in-Medical-Language-guided-Segmentation-via-Prototype-driven-Semantic-Approximation"><a href="#Alleviating-Textual-Reliance-in-Medical-Language-guided-Segmentation-via-Prototype-driven-Semantic-Approximation" class="headerlink" title="Alleviating Textual Reliance in Medical Language-guided Segmentation via   Prototype-driven Semantic Approximation"></a>Alleviating Textual Reliance in Medical Language-guided Segmentation via   Prototype-driven Semantic Approximation</h2><p><strong>Authors:Shuchang Ye, Usman Naseem, Mingyuan Meng, Jinman Kim</strong></p>
<p>Medical language-guided segmentation, integrating textual clinical reports as auxiliary guidance to enhance image segmentation, has demonstrated significant improvements over unimodal approaches. However, its inherent reliance on paired image-text input, which we refer to as &#96;&#96;textual relianceâ€, presents two fundamental limitations: 1) many medical segmentation datasets lack paired reports, leaving a substantial portion of image-only data underutilized for training; and 2) inference is limited to retrospective analysis of cases with paired reports, limiting its applicability in most clinical scenarios where segmentation typically precedes reporting. To address these limitations, we propose ProLearn, the first Prototype-driven Learning framework for language-guided segmentation that fundamentally alleviates textual reliance. At its core, in ProLearn, we introduce a novel Prototype-driven Semantic Approximation (PSA) module to enable approximation of semantic guidance from textual input. PSA initializes a discrete and compact prototype space by distilling segmentation-relevant semantics from textual reports. Once initialized, it supports a query-and-respond mechanism which approximates semantic guidance for images without textual input, thereby alleviating textual reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG demonstrate that ProLearn outperforms state-of-the-art language-guided methods when limited text is available. </p>
<blockquote>
<p>åŒ»ç–—è¯­è¨€å¼•å¯¼åˆ†å‰²é€šè¿‡å°†æ–‡æœ¬ä¸´åºŠæŠ¥å‘Šä½œä¸ºè¾…åŠ©æŒ‡å¯¼æ¥å¢å¼ºå›¾åƒåˆ†å‰²ï¼Œè¯æ˜å…¶åœ¨å•æ¨¡æ€æ–¹æ³•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ç„¶è€Œï¼Œå…¶å¯¹é…å¯¹å›¾åƒæ–‡æœ¬è¾“å…¥çš„å›ºæœ‰ä¾èµ–ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ–‡æœ¬ä¾èµ–â€ï¼‰å­˜åœ¨ä¸¤ä¸ªåŸºæœ¬å±€é™ï¼š1ï¼‰è®¸å¤šåŒ»ç–—åˆ†å‰²æ•°æ®é›†ç¼ºä¹é…å¯¹æŠ¥å‘Šï¼Œå¯¼è‡´å¤§é‡ä»…åŒ…å«å›¾åƒçš„æ•°æ®æœªèƒ½å¾—åˆ°å……åˆ†åˆ©ç”¨ï¼›2ï¼‰æ¨ç†ä»…é™äºå…·æœ‰é…å¯¹æŠ¥å‘Šçš„ç—…ä¾‹çš„å›é¡¾æ€§åˆ†æï¼Œé™åˆ¶äº†å…¶åœ¨å¤§å¤šæ•°ä¸´åºŠåœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå› ä¸ºåœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œé€šå¸¸å…ˆè¿›è¡Œåˆ†å‰²å†è¿›è¡ŒæŠ¥å‘Šã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ProLearnï¼Œå³é¦–ä¸ªç”¨äºè¯­è¨€å¼•å¯¼åˆ†å‰²çš„åŸå‹é©±åŠ¨å­¦ä¹ æ¡†æ¶ï¼Œä»æ ¹æœ¬ä¸Šå‡è½»äº†æ–‡æœ¬ä¾èµ–ã€‚åœ¨ProLearnçš„æ ¸å¿ƒä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„åŸå‹é©±åŠ¨è¯­ä¹‰è¿‘ä¼¼ï¼ˆPSAï¼‰æ¨¡å—ï¼Œä»¥å®ç°ä»æ–‡æœ¬è¾“å…¥ä¸­è¿‘ä¼¼è¯­ä¹‰æŒ‡å¯¼ã€‚PSAé€šè¿‡ä»æ–‡æœ¬æŠ¥å‘Šä¸­æç‚¼å‡ºä¸åˆ†å‰²ç›¸å…³çš„è¯­ä¹‰æ¥åˆå§‹åŒ–ä¸€ä¸ªç¦»æ•£ä¸”ç´§å‡‘çš„åŸå‹ç©ºé—´ã€‚åˆå§‹åŒ–å®Œæˆåï¼Œå®ƒæ”¯æŒä¸€ç§æŸ¥è¯¢å’Œå“åº”æœºåˆ¶ï¼Œå¯åœ¨æ²¡æœ‰æ–‡æœ¬è¾“å…¥çš„æƒ…å†µä¸‹ä¸ºå›¾åƒè¿‘ä¼¼è¯­ä¹‰æŒ‡å¯¼ï¼Œä»è€Œå‡è½»å¯¹æ–‡æœ¬çš„ä¾èµ–ã€‚åœ¨QaTa-COV19ã€MosMedData+å’ŒKvasir-SEGä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œå½“æ–‡æœ¬æœ‰é™æ—¶ï¼ŒProLearnçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„è¯­è¨€å¼•å¯¼æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11055v1">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong>ï¼šåŒ»å­¦è¯­è¨€å¼•å¯¼åˆ†å‰²æŠ€æœ¯é€šè¿‡æ•´åˆæ–‡æœ¬ä¸´åºŠæŠ¥å‘Šä½œä¸ºè¾…åŠ©æŒ‡å¯¼ï¼Œæé«˜äº†å›¾åƒåˆ†å‰²çš„ç²¾åº¦ã€‚ç„¶è€Œï¼Œè¯¥æŠ€æœ¯ä¾èµ–äºé…å¯¹å›¾åƒæ–‡æœ¬è¾“å…¥ï¼Œå­˜åœ¨æ•°æ®é…å¯¹æŠ¥å‘Šç¼ºå¤±å’Œæ¨ç†åº”ç”¨åœºæ™¯å—é™çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºProLearnåŸå‹é©±åŠ¨å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥åŸå‹é©±åŠ¨è¯­ä¹‰é€¼è¿‘æ¨¡å—å®ç°è¯­ä¹‰æŒ‡å¯¼çš„è¿‘ä¼¼ï¼Œé™ä½å¯¹æ–‡æœ¬è¾“å…¥çš„ä¾èµ–ã€‚å®éªŒè¯æ˜ï¼ŒProLearnåœ¨æœ‰é™æ–‡æœ¬ä¿¡æ¯ä¸‹è¡¨ç°ä¼˜äºç°æœ‰è¯­è¨€å¼•å¯¼æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŒ»å­¦è¯­è¨€å¼•å¯¼åˆ†å‰²æŠ€æœ¯æé«˜äº†å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§ï¼Œé€šè¿‡ç»“åˆæ–‡æœ¬ä¸´åºŠæŠ¥å‘Šä½œä¸ºè¾…åŠ©æŒ‡å¯¼ã€‚</li>
<li>ç°æœ‰æŠ€æœ¯ä¾èµ–äºé…å¯¹å›¾åƒæ–‡æœ¬è¾“å…¥ï¼Œå­˜åœ¨æ•°æ®é…å¯¹æŠ¥å‘Šç¼ºå¤±çš„é—®é¢˜ã€‚</li>
<li>ProLearnæ¡†æ¶è¢«æå‡ºä»¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡å¼•å…¥åŸå‹é©±åŠ¨è¯­ä¹‰é€¼è¿‘æ¨¡å—å®ç°è¯­ä¹‰æŒ‡å¯¼çš„è¿‘ä¼¼ã€‚</li>
<li>ProLearné™ä½äº†å¯¹æ–‡æœ¬è¾“å…¥çš„ä¾èµ–ï¼Œä½¿å¾—åœ¨æ²¡æœ‰æ–‡æœ¬è¾“å…¥çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¿›è¡Œå›¾åƒåˆ†å‰²ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œåœ¨æœ‰é™æ–‡æœ¬ä¿¡æ¯ä¸‹ï¼ŒProLearnçš„è¡¨ç°ä¼˜äºç°æœ‰çš„è¯­è¨€å¼•å¯¼æ–¹æ³•ã€‚</li>
<li>ProLearnå…·æœ‰æ›´å¥½çš„é€‚ç”¨æ€§å’Œçµæ´»æ€§ï¼Œèƒ½å¤Ÿé€‚åº”å¤§å¤šæ•°ä¸´åºŠåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6ae70b8baf3e88f5648b694fcfb02a67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-581a2ac34e56e6d9c43a0fe382a3f143.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfb49aef80910b9762fe28fb9303d90d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07b6727b70a9303b94d9b35023ccabaa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d541bca6ba72270d04eed9ee94be38a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ffae2a1d29c91fb90ad2aa4271470703.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Personalized-OVSS-Understanding-Personal-Concept-in-Open-Vocabulary-Semantic-Segmentation"><a href="#Personalized-OVSS-Understanding-Personal-Concept-in-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="Personalized OVSS: Understanding Personal Concept in Open-Vocabulary   Semantic Segmentation"></a>Personalized OVSS: Understanding Personal Concept in Open-Vocabulary   Semantic Segmentation</h2><p><strong>Authors:Sunghyun Park, Jungsoo Lee, Shubhankar Borse, Munawar Hayat, Sungha Choi, Kyuwoong Hwang, Fatih Porikli</strong></p>
<p>While open-vocabulary semantic segmentation (OVSS) can segment an image into semantic regions based on arbitrarily given text descriptions even for classes unseen during training, it fails to understand personal texts (e.g., <code>my mug cup&#39;) for segmenting regions of specific interest to users. This paper addresses challenges like recognizing </code>my mug cupâ€™ among <code>multiple mug cups&#39;. To overcome this challenge, we introduce a novel task termed \textit&#123;personalized open-vocabulary semantic segmentation&#125; and propose a text prompt tuning-based plug-in method designed to recognize personal visual concepts using a few pairs of images and masks, while maintaining the performance of the original OVSS. Based on the observation that reducing false predictions is essential when applying text prompt tuning to this task, our proposed method employs </code>negative mask proposalâ€™ that captures visual concepts other than the personalized concept. We further improve the performance by enriching the representation of text prompts by injecting visual embeddings of the personal concept into them. This approach enhances personalized OVSS without compromising the original OVSS performance. We demonstrate the superiority of our method on our newly established benchmarks for this task, including FSS$^\text{per}$, CUB$^\text{per}$, and ADE$^\text{per}$. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰å¯ä»¥æ ¹æ®ä»»æ„ç»™å®šçš„æ–‡æœ¬æè¿°å°†å›¾åƒåˆ†å‰²æˆè¯­ä¹‰åŒºåŸŸï¼Œå³ä½¿å¯¹äºè®­ç»ƒæœŸé—´æœªè§è¿‡çš„ç±»åˆ«ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç„¶è€Œï¼Œå®ƒæ— æ³•ç†è§£ä¸ªäººæ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼Œâ€œæˆ‘çš„é©¬å…‹æ¯â€ï¼‰ï¼Œæ— æ³•æ ¹æ®ç”¨æˆ·çš„ç‰¹å®šå…´è¶£åˆ†å‰²åŒºåŸŸã€‚æœ¬æ–‡è§£å†³å¦‚ä»å¤šä¸ªé©¬å…‹æ¯ä¸­è¯†åˆ«å‡ºâ€œæˆ‘çš„é©¬å…‹æ¯â€ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹åä¸ºä¸ªæ€§åŒ–å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„æ–°ä»»åŠ¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬æç¤ºè°ƒæ•´çš„æ’ä»¶æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å°‘é‡å›¾åƒå’Œé®ç½©å¯¹æ¥è¯†åˆ«ä¸ªäººè§†è§‰æ¦‚å¿µï¼ŒåŒæ—¶ä¿æŒåŸå§‹OVSSçš„æ€§èƒ½ã€‚åŸºäºå°†æ–‡æœ¬æç¤ºè°ƒæ•´åº”ç”¨äºæ­¤ä»»åŠ¡æ—¶å‡å°‘é”™è¯¯é¢„æµ‹è‡³å…³é‡è¦çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•é‡‡ç”¨â€œè´Ÿé®ç½©æè®®â€ï¼Œè¯¥æè®®æ•æ‰é™¤ä¸ªæ€§åŒ–æ¦‚å¿µä»¥å¤–çš„è§†è§‰æ¦‚å¿µã€‚æˆ‘ä»¬é€šè¿‡å°†ä¸ªäººæ¦‚å¿µçš„è§†è§‰åµŒå…¥æ³¨å…¥æ–‡æœ¬æç¤ºä¸­ï¼Œä¸°å¯Œæ–‡æœ¬æç¤ºçš„è¡¨ç¤ºï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•åœ¨ä¸æŸå®³åŸå§‹OVSSæ€§èƒ½çš„æƒ…å†µä¸‹å¢å¼ºäº†ä¸ªæ€§åŒ–OVSSã€‚æˆ‘ä»¬åœ¨ä¸ºæ­¤ä»»åŠ¡æ–°å»ºç«‹çš„æ ‡å‡†ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜è¶Šæ€§ï¼ŒåŒ…æ‹¬FSS$^{\text{per}}$ã€CUB$^{\text{per}}$å’ŒADE$^{\text{per}}$ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11030v1">PDF</a> Accepted to ICCV 2025; 15 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰èƒ½å¤Ÿæ ¹æ®ä»»æ„ç»™å®šçš„æ–‡æœ¬æè¿°å¯¹å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œå³ä½¿å¯¹äºè®­ç»ƒæœŸé—´æœªè§è¿‡çš„ç±»åˆ«äº¦æ˜¯å¦‚æ­¤ã€‚ç„¶è€Œï¼Œå®ƒæ— æ³•ç†è§£ä¸ªäººæ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼Œâ€œæˆ‘çš„é©¬å…‹æ¯â€ï¼‰ï¼Œæ— æ³•æ ¹æ®ç”¨æˆ·çš„ç‰¹å®šå…´è¶£åˆ†å‰²åŒºåŸŸã€‚æœ¬æ–‡è§£å†³åœ¨â€œå¤šä¸ªé©¬å…‹æ¯â€ä¸­è¯†åˆ«â€œæˆ‘çš„é©¬å…‹æ¯â€ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹åä¸ºä¸ªæ€§åŒ–å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²çš„æ–°ä»»åŠ¡ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºæ–‡æœ¬æç¤ºè°ƒæ•´çš„æ’ä»¶æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å°‘é‡å›¾åƒå’Œè’™ç‰ˆå¯¹æ¥è¯†åˆ«ä¸ªæ€§åŒ–è§†è§‰æ¦‚å¿µï¼ŒåŒæ—¶ä¿æŒåŸå§‹OVSSçš„æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°å‡å°‘é”™è¯¯é¢„æµ‹åœ¨æ­¤ä»»åŠ¡ä¸­è‡³å…³é‡è¦ï¼Œå› æ­¤æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨â€œè´Ÿè’™ç‰ˆæè®®â€ï¼Œæ•è·é™¤ä¸ªæ€§åŒ–æ¦‚å¿µä¹‹å¤–çš„å…¶ä»–è§†è§‰æ¦‚å¿µã€‚æ­¤å¤–ï¼Œé€šè¿‡å‘æ–‡æœ¬æç¤ºæ³¨å…¥ä¸ªæ€§åŒ–æ¦‚å¿µçš„è§†è§‰åµŒå…¥æ¥ä¸°å¯Œå…¶è¡¨ç¤ºï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•åœ¨ä¸æŸå®³åŸå§‹OVSSæ€§èƒ½çš„æƒ…å†µä¸‹æé«˜äº†ä¸ªæ€§åŒ–OVSSçš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ä¸ºæ­¤ä»»åŠ¡å»ºç«‹çš„æ–°åŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬FSS$^{per}$ã€CUB$^{per}$å’ŒADE$^{per}$ï¼‰ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰èƒ½å¤Ÿæ ¹æ®ä»»æ„æ–‡æœ¬æè¿°å¯¹å›¾åƒè¿›è¡Œè¯­ä¹‰åˆ†å‰²ï¼Œä½†æ— æ³•ç†è§£ä¸ªäººæ–‡æœ¬ã€‚</li>
<li>å¼•å…¥ä¸ªæ€§åŒ–å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨è¯†åˆ«ç”¨æˆ·çš„ç‰¹å®šå…´è¶£åŒºåŸŸã€‚</li>
<li>æå‡ºåŸºäºæ–‡æœ¬æç¤ºè°ƒæ•´çš„æ’ä»¶æ–¹æ³•ï¼Œé€šè¿‡å°‘é‡å›¾åƒå’Œè’™ç‰ˆå¯¹æ¥è¯†åˆ«ä¸ªæ€§åŒ–è§†è§‰æ¦‚å¿µã€‚</li>
<li>æ–¹æ³•é‡‡ç”¨è´Ÿè’™ç‰ˆæè®®ï¼Œä»¥æ•è·é™¤ä¸ªæ€§åŒ–æ¦‚å¿µå¤–çš„å…¶ä»–è§†è§‰æ¦‚å¿µï¼Œå‡å°‘é”™è¯¯é¢„æµ‹ã€‚</li>
<li>é€šè¿‡å‘æ–‡æœ¬æç¤ºæ³¨å…¥ä¸ªæ€§åŒ–æ¦‚å¿µçš„è§†è§‰åµŒå…¥æ¥ä¸°å¯Œè¡¨ç¤ºï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</li>
<li>åœ¨æ–°å»ºç«‹çš„åŸºå‡†æµ‹è¯•ä¸Šå±•ç¤ºäº†æ–¹æ³•ä¼˜è¶Šæ€§ï¼ŒåŒ…æ‹¬FSS$^{per}$ã€CUB$^{per}$å’ŒADE$^{per}$ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸æŸå®³åŸå§‹OVSSæ€§èƒ½çš„æƒ…å†µä¸‹å¢å¼ºäº†ä¸ªæ€§åŒ–OVSSçš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dc9115b4029fccd9ac02542cfef2e518.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6f3d5a3450b446ea20b51a4af50d7b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42846e53c28667efc994ee80465d26b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b3ec2630c9a0a394d2c695d24555a1a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f9b9b7ec46276cefb5721467a92f8d4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Trexplorer-Super-Topologically-Correct-Centerline-Tree-Tracking-of-Tubular-Objects-in-CT-Volumes"><a href="#Trexplorer-Super-Topologically-Correct-Centerline-Tree-Tracking-of-Tubular-Objects-in-CT-Volumes" class="headerlink" title="Trexplorer Super: Topologically Correct Centerline Tree Tracking of   Tubular Objects in CT Volumes"></a>Trexplorer Super: Topologically Correct Centerline Tree Tracking of   Tubular Objects in CT Volumes</h2><p><strong>Authors:Roman Naeem, David Hagerman, Jennifer AlvÃ©n, Lennart Svensson, Fredrik Kahl</strong></p>
<p>Tubular tree structures, such as blood vessels and airways, are essential in human anatomy and accurately tracking them while preserving their topology is crucial for various downstream tasks. Trexplorer is a recurrent model designed for centerline tracking in 3D medical images but it struggles with predicting duplicate branches and terminating tracking prematurely. To address these issues, we present Trexplorer Super, an enhanced version that notably improves performance through novel advancements. However, evaluating centerline tracking models is challenging due to the lack of public datasets. To enable thorough evaluation, we develop three centerline datasets, one synthetic and two real, each with increasing difficulty. Using these datasets, we conduct a comprehensive evaluation of existing state-of-the-art (SOTA) models and compare them with our approach. Trexplorer Super outperforms previous SOTA models on every dataset. Our results also highlight that strong performance on synthetic data does not necessarily translate to real datasets. The code and datasets are available at <a target="_blank" rel="noopener" href="https://github.com/RomStriker/Trexplorer-Super">https://github.com/RomStriker/Trexplorer-Super</a>. </p>
<blockquote>
<p>ç®¡çŠ¶æ ‘çŠ¶ç»“æ„ï¼Œå¦‚è¡€ç®¡å’Œæ°”é“ï¼Œåœ¨äººç±»è§£å‰–å­¦ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œè€Œå‡†ç¡®è·Ÿè¸ªå®ƒä»¬åŒæ—¶ä¿æŒå…¶æ‹“æ‰‘ç»“æ„å¯¹äºå„ç§ä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ã€‚Trexploreræ˜¯ä¸€ç§ä¸º3DåŒ»å­¦å›¾åƒä¸­çš„ä¸­å¿ƒçº¿è·Ÿè¸ªè®¾è®¡çš„é€’å½’æ¨¡å‹ï¼Œä½†å®ƒé¢ä¸´ç€é¢„æµ‹é‡å¤åˆ†æ”¯å’Œè¿‡æ—©ç»ˆæ­¢è·Ÿè¸ªçš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Trexplorer Superï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡æ–°é¢–çš„è¿›æ­¥æ˜¾è‘—æ”¹è¿›æ€§èƒ½çš„æå‡ç‰ˆæœ¬ã€‚ç„¶è€Œï¼Œç”±äºç¼ºå°‘å…¬å…±æ•°æ®é›†ï¼Œè¯„ä¼°ä¸­å¿ƒçº¿è·Ÿè¸ªæ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†å®ç°å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸‰ä¸ªä¸­å¿ƒçº¿æ•°æ®é›†ï¼Œä¸€ä¸ªåˆæˆæ•°æ®é›†å’Œä¸¤ä¸ªçœŸå®æ•°æ®é›†ï¼Œæ¯ä¸ªæ•°æ®é›†çš„éš¾åº¦éƒ½åœ¨å¢åŠ ã€‚ä½¿ç”¨è¿™äº›æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯¹ç°æœ‰çš„æœ€å…ˆè¿›çš„æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶å°†å®ƒä»¬ä¸æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚Trexplorer Superåœ¨æ¯ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°éƒ½è¶…è¿‡äº†ä¹‹å‰çš„æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç»“æœè¿˜è¡¨æ˜ï¼Œåœ¨åˆæˆæ•°æ®ä¸Šçš„å‡ºè‰²è¡¨ç°å¹¶ä¸ä¸€å®šé€‚ç”¨äºçœŸå®æ•°æ®é›†ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RomStriker/Trexplorer-Super">https://github.com/RomStriker/Trexplorer-Super</a>ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10881v1">PDF</a> Submitted Version. Accepted at MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>Trexplorer Superæ˜¯ä¸€æ¬¾é’ˆå¯¹ä¸‰ç»´åŒ»å­¦å›¾åƒä¸­å¿ƒçº¿è·Ÿè¸ªçš„é€’å½’æ¨¡å‹ï¼Œè§£å†³äº†åŸæœ‰Trexploreræ¨¡å‹åœ¨é¢„æµ‹é‡å¤åˆ†æ”¯å’Œè¿‡æ—©ç»ˆæ­¢è·Ÿè¸ªæ–¹é¢çš„é—®é¢˜ã€‚ä¸ºå…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶å›¢é˜Ÿåˆ›å»ºäº†ä¸‰ä¸ªä¸­å¿ƒçº¿æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸€ä¸ªåˆæˆæ•°æ®é›†å’Œä¸¤ä¸ªçœŸå®æ•°æ®é›†ï¼Œéš¾åº¦é€’å¢ã€‚è¯„ä¼°ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹å¹¶ä¸Trexplorer Superè¿›è¡Œå¯¹æ¯”ï¼Œå‘ç°è¯¥æ¨¡å‹åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šå‡è¡¨ç°æœ€ä½³ã€‚åŒæ—¶ï¼Œç ”ç©¶å‘ç°åœ¨åˆæˆæ•°æ®ä¸Šçš„è‰¯å¥½è¡¨ç°ä¸ä¸€å®šèƒ½åœ¨çœŸå®æ•°æ®é›†ä¸­å¾—åˆ°ä½“ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Tubular tree structuresåœ¨äººä½“è§£å‰–å­¦ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œå‡†ç¡®çš„è·Ÿè¸ªå’Œæ‹“æ‰‘ä¿æŒå¯¹äºå„ç§ä¸‹æ¸¸ä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>Trexplorer Superæ˜¯ä¸“ä¸ºä¸‰ç»´åŒ»å­¦å›¾åƒä¸­å¿ƒçº¿è·Ÿè¸ªè®¾è®¡çš„é€’å½’æ¨¡å‹ï¼Œè§£å†³äº†åŸæœ‰æ¨¡å‹çš„é¢„æµ‹é‡å¤åˆ†æ”¯å’Œè¿‡æ—©ç»ˆæ­¢è·Ÿè¸ªé—®é¢˜ã€‚</li>
<li>ä¸ºå…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œåˆ›å»ºäº†ä¸‰ä¸ªä¸åŒéš¾åº¦çš„ä¸­å¿ƒçº¿æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸€ä¸ªåˆæˆæ•°æ®é›†å’Œä¸¤ä¸ªçœŸå®æ•°æ®é›†ã€‚</li>
<li>å¯¹ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶ä¸Trexplorer Superè¿›è¡Œäº†å¯¹æ¯”ã€‚</li>
<li>Trexplorer Superåœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>åœ¨åˆæˆæ•°æ®ä¸Šçš„è‰¯å¥½è¡¨ç°ä¸ä¸€å®šèƒ½åœ¨çœŸå®æ•°æ®é›†ä¸­å¾—åˆ°ä½“ç°ï¼Œè¿™æç¤ºæˆ‘ä»¬åœ¨å®é™…åº”ç”¨ä¸­éœ€è€ƒè™‘æ•°æ®çš„çœŸå®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10881">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-36f2eb0c12068d385422a41efb3eb04b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8c2f5b6fc31fa57419a5ae080ccc80d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-10040be3431ae7fceddc8ef069725c01.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Fine-Grained-Zero-Shot-Object-Detection"><a href="#Fine-Grained-Zero-Shot-Object-Detection" class="headerlink" title="Fine-Grained Zero-Shot Object Detection"></a>Fine-Grained Zero-Shot Object Detection</h2><p><strong>Authors:Hongxu Ma, Chenbo Zhang, Lu Zhang, Jiaogen Zhou, Jihong Guan, Shuigeng Zhou</strong></p>
<p>Zero-shot object detection (ZSD) aims to leverage semantic descriptions to localize and recognize objects of both seen and unseen classes. Existing ZSD works are mainly coarse-grained object detection, where the classes are visually quite different, thus are relatively easy to distinguish. However, in real life we often have to face fine-grained object detection scenarios, where the classes are too similar to be easily distinguished. For example, detecting different kinds of birds, fishes, and flowers.   In this paper, we propose and solve a new problem called Fine-Grained Zero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of different classes with minute differences in details under the ZSD paradigm. We develop an effective method called MSHC for the FG-ZSD task, which is based on an improved two-stage detector and employs a multi-level semantics-aware embedding alignment loss, ensuring tight coupling between the visual and semantic spaces. Considering that existing ZSD datasets are not suitable for the new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds, which contains 148,820 images falling into 36 orders, 140 families, 579 genera and 1432 species. Extensive experiments on FGZSD-Birds show that our method outperforms existing ZSD models. </p>
<blockquote>
<p>é›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆZSDï¼‰æ—¨åœ¨åˆ©ç”¨è¯­ä¹‰æè¿°æ¥å®šä½å’Œè¯†åˆ«å·²è§å’Œæœªè§ç±»åˆ«çš„ç›®æ ‡ã€‚ç°æœ‰çš„ZSDå·¥ä½œä¸»è¦é›†ä¸­åœ¨ç²—ç²’åº¦ç›®æ ‡æ£€æµ‹ä¸Šï¼Œè¿™äº›ç±»åˆ«çš„è§†è§‰å·®å¼‚å¾ˆå¤§ï¼Œå› æ­¤ç›¸å¯¹å®¹æ˜“åŒºåˆ†ã€‚ç„¶è€Œï¼Œåœ¨ç°å®ç”Ÿæ´»ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸é¢ä¸´ç»†ç²’åº¦ç›®æ ‡æ£€æµ‹çš„åœºæ™¯ï¼Œè¿™äº›ç±»åˆ«çš„å·®å¼‚éå¸¸ç»†å¾®ï¼Œéš¾ä»¥åŒºåˆ†ã€‚ä¾‹å¦‚ï¼Œæ£€æµ‹ä¸åŒç§ç±»çš„é¸Ÿç±»ã€é±¼ç±»å’ŒèŠ±å‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºå¹¶è§£å†³äº†ä¸€ä¸ªæ–°çš„é—®é¢˜ï¼Œç§°ä¸ºç»†ç²’åº¦é›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆç®€ç§°FG-ZSDï¼‰ï¼Œæ—¨åœ¨åœ¨ZSDæ¡†æ¶ä¸‹æ£€æµ‹å…·æœ‰ç»†å¾®å·®å¼‚çš„ä¸åŒç±»åˆ«çš„ç›®æ ‡ã€‚æˆ‘ä»¬ä¸ºFG-ZSDä»»åŠ¡å¼€å‘äº†ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç§°ä¸ºMSHCï¼Œè¯¥æ–¹æ³•åŸºäºæ”¹è¿›çš„ä¸¤é˜¶æ®µæ£€æµ‹å™¨ï¼Œå¹¶é‡‡ç”¨å¤šçº§è¯­ä¹‰æ„ŸçŸ¥åµŒå…¥å¯¹é½æŸå¤±ï¼Œç¡®ä¿è§†è§‰å’Œè¯­ä¹‰ç©ºé—´ä¹‹é—´çš„ç´§å¯†è€¦åˆã€‚è€ƒè™‘åˆ°ç°æœ‰çš„ZSDæ•°æ®é›†ä¸é€‚åˆæ–°çš„FG-ZSDä»»åŠ¡ï¼Œæˆ‘ä»¬å»ºç«‹äº†ç¬¬ä¸€ä¸ªFG-ZSDåŸºå‡†æ•°æ®é›†FGZSD-Birdsï¼Œå…¶ä¸­åŒ…å«148820å¼ å›¾åƒï¼Œåˆ†ä¸º36ä¸ªé˜¶ã€140ä¸ªå®¶æ—ã€579ä¸ªå±å’Œ1432ä¸ªç‰©ç§ã€‚åœ¨FGZSD-Birdsä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„ZSDæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10358v1">PDF</a> Accepted by ACM MMâ€™25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ç²¾ç»†ç²’åº¦é›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆFG-ZSDï¼‰çš„æ–°é—®é¢˜ï¼Œæ—¨åœ¨è§£å†³åœ¨é›¶æ ·æœ¬æ£€æµ‹æ¡†æ¶ä¸‹ä¸åŒç±»åˆ«ç›®æ ‡ä¹‹é—´çš„ç»†å¾®å·®å¼‚æ£€æµ‹ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åŸºäºæ”¹è¿›çš„ä¸¤é˜¶æ®µæ£€æµ‹å™¨çš„æ–¹æ³•ï¼Œå¹¶é‡‡ç”¨å¤šå±‚æ¬¡è¯­ä¹‰æ„ŸçŸ¥åµŒå…¥å¯¹é½æŸå¤±ï¼Œç¡®ä¿è§†è§‰å’Œè¯­ä¹‰ç©ºé—´ä¹‹é—´çš„ç´§å¯†è€¦åˆã€‚åŒæ—¶ï¼Œå»ºç«‹äº†é¦–ä¸ªFG-ZSDåŸºå‡†æ•°æ®é›†FGZSD-Birdsï¼ŒåŒ…å«148,820å¼ å›¾åƒï¼Œæ¶µç›–36ä¸ªé˜¶ã€140ä¸ªå®¶æ—ã€579ä¸ªå±å’Œ1432ä¸ªç‰©ç§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰ZSDæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥ç²¾ç»†ç²’åº¦é›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹ï¼ˆFG-ZSDï¼‰é—®é¢˜ï¼Œå…³æ³¨ç±»åˆ«é—´ç»†å¾®å·®å¼‚çš„æ£€æµ‹ã€‚</li>
<li>æå‡ºåŸºäºæ”¹è¿›ä¸¤é˜¶æ®µæ£€æµ‹å™¨çš„æ–¹æ³•ï¼Œé€‚ç”¨äºFG-ZSDä»»åŠ¡ã€‚</li>
<li>é‡‡ç”¨å¤šå±‚æ¬¡è¯­ä¹‰æ„ŸçŸ¥åµŒå…¥å¯¹é½æŸå¤±ï¼Œå®ç°è§†è§‰å’Œè¯­ä¹‰ç©ºé—´çš„ç´§å¯†è€¦åˆã€‚</li>
<li>å»ºç«‹é¦–ä¸ªFG-ZSDåŸºå‡†æ•°æ®é›†FGZSD-Birdsï¼ŒåŒ…å«ä¸°å¯Œçš„å›¾åƒæ•°æ®ã€‚</li>
<li>æ–¹æ³•åœ¨FGZSD-Birdsæ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›å®éªŒéªŒè¯ã€‚</li>
<li>æå‡ºçš„æ–¹æ³•æ€§èƒ½ä¼˜äºç°æœ‰çš„é›¶æ ·æœ¬æ£€æµ‹ï¼ˆZSDï¼‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-74f9c913010d3d5c2d08d1cd45b5d155.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-19ab14fe440f13d210809eead0b2565b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49c36e31108c00d019df101b0ef4a291.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d37510ec79db1a10d9250a24ac961944.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5c6eedbc844b796fc012121251c0ece.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1eb6213437ca3c29a3261708aeb570a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6ca42d67d1dad137f984baf3ee5b019.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Transferring-Styles-for-Reduced-Texture-Bias-and-Improved-Robustness-in-Semantic-Segmentation-Networks"><a href="#Transferring-Styles-for-Reduced-Texture-Bias-and-Improved-Robustness-in-Semantic-Segmentation-Networks" class="headerlink" title="Transferring Styles for Reduced Texture Bias and Improved Robustness in   Semantic Segmentation Networks"></a>Transferring Styles for Reduced Texture Bias and Improved Robustness in   Semantic Segmentation Networks</h2><p><strong>Authors:Ben Hamscher, Edgar Heinert, Annika MÃ¼tze, Kira Maag, Matthias Rottmann</strong></p>
<p>Recent research has investigated the shape and texture biases of deep neural networks (DNNs) in image classification which influence their generalization capabilities and robustness. It has been shown that, in comparison to regular DNN training, training with stylized images reduces texture biases in image classification and improves robustness with respect to image corruptions. In an effort to advance this line of research, we examine whether style transfer can likewise deliver these two effects in semantic segmentation. To this end, we perform style transfer with style varying across artificial image areas. Those random areas are formed by a chosen number of Voronoi cells. The resulting style-transferred data is then used to train semantic segmentation DNNs with the objective of reducing their dependence on texture cues while enhancing their reliance on shape-based features. In our experiments, it turns out that in semantic segmentation, style transfer augmentation reduces texture bias and strongly increases robustness with respect to common image corruptions as well as adversarial attacks. These observations hold for convolutional neural networks and transformer architectures on the Cityscapes dataset as well as on PASCAL Context, showing the generality of the proposed method. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶å·²ç»æ¢è®¨äº†æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨å›¾åƒåˆ†ç±»ä¸­çš„å½¢çŠ¶å’Œçº¹ç†åè§ï¼Œè¿™äº›åè§ä¼šå½±å“å…¶æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä¸å¸¸è§„DNNè®­ç»ƒç›¸æ¯”ï¼Œä½¿ç”¨é£æ ¼åŒ–å›¾åƒè¿›è¡Œè®­ç»ƒå‡å°‘äº†å›¾åƒåˆ†ç±»ä¸­çš„çº¹ç†åè§ï¼Œå¹¶æé«˜äº†å¯¹å›¾åƒè…èš€çš„ç¨³å¥æ€§ã€‚ä¸ºäº†æ¨è¿›è¿™ä¸€ç ”ç©¶é¢†åŸŸï¼Œæˆ‘ä»¬ç ”ç©¶äº†é£æ ¼è½¬æ¢æ˜¯å¦ä¹Ÿèƒ½äº§ç”Ÿè¿™ä¸¤ä¸ªæ•ˆæœåœ¨è¯­ä¹‰åˆ†å‰²ä¸Šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨äººå·¥å›¾åƒåŒºåŸŸä¸Šæ‰§è¡Œäº†é£æ ¼è½¬æ¢ã€‚è¿™äº›éšæœºåŒºåŸŸç”±é€‰æ‹©çš„Voronoiç»†èƒå½¢æˆã€‚ç„¶åï¼Œä½¿ç”¨å¾—åˆ°çš„é£æ ¼è½¬æ¢æ•°æ®æ¥è®­ç»ƒè¯­ä¹‰åˆ†å‰²DNNï¼Œç›®çš„æ˜¯å‡å°‘å…¶å¯¹çº¹ç†çº¿ç´¢çš„ä¾èµ–ï¼ŒåŒæ—¶å¢å¼ºå…¶å¯¹åŸºäºå½¢çŠ¶ç‰¹å¾çš„ä¾èµ–ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œäº‹å®è¯æ˜ï¼Œåœ¨è¯­ä¹‰åˆ†å‰²ä¸­ï¼Œé£æ ¼è½¬æ¢å¢å¼ºå‡å°‘äº†çº¹ç†åè§ï¼Œå¹¶å¤§å¤§æé«˜äº†å¯¹å¸¸è§å›¾åƒè…èš€ä»¥åŠå¯¹æŠ—æ€§æ”»å‡»çš„ç¨³å¥æ€§ã€‚è¿™äº›è§‚å¯Ÿç»“æœåœ¨åŸå¸‚æ™¯è§‚æ•°æ®é›†å’ŒPASCAL Contextä¸Šçš„å·ç§¯ç¥ç»ç½‘ç»œå’Œè½¬æ¢å™¨æ¶æ„ä¸­éƒ½å¾—åˆ°äº†éªŒè¯ï¼Œæ˜¾ç¤ºäº†æ‰€æå‡ºæ–¹æ³•çš„æ™®éæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10239v1">PDF</a> accepted at ECAI 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨å›¾åƒåˆ†ç±»ä¸­çš„å½¢çŠ¶å’Œçº¹ç†åè§å½±å“å…¶æ³›åŒ–èƒ½åŠ›å’Œç¨³å¥æ€§ã€‚é€šè¿‡é‡‡ç”¨é£æ ¼åŒ–å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œå¯å‡å°‘çº¹ç†åè§å¹¶æé«˜å›¾åƒå¤±çœŸé²æ£’æ€§ã€‚æœ¬ç ”ç©¶å°†è¿™ç§è®­ç»ƒç­–ç•¥åº”ç”¨äºè¯­ä¹‰åˆ†å‰²é¢†åŸŸï¼Œåˆ©ç”¨é£æ ¼è½¬ç§»æŠ€æœ¯æ”¹å˜å›¾åƒçš„éƒ¨åˆ†åŒºåŸŸï¼Œè¿™äº›åŒºåŸŸç”±ä¸€ç³»åˆ—æ²ƒç½—è¯ºä¼Šç»†èƒç»„æˆã€‚å®éªŒè¡¨æ˜ï¼Œé£æ ¼è½¬ç§»å¢å¼ºè®­ç»ƒå¯é™ä½è¯­ä¹‰åˆ†å‰²ä¸­çš„çº¹ç†åè§ï¼Œå¹¶æ˜¾è‘—æé«˜å¯¹å¸¸è§å›¾åƒå¤±çœŸä»¥åŠå¯¹æŠ—æ”»å‡»çš„ç¨³å¥æ€§ã€‚è¿™ä¸€è§‚å¯Ÿç»“æœé€‚ç”¨äºå·ç§¯ç¥ç»ç½‘ç»œå’Œè½¬æ¢å™¨æ¶æ„ï¼Œåœ¨Cityscapeså’ŒPASCAL Contextæ•°æ®é›†ä¸Šéƒ½éªŒè¯äº†è¯¥æ–¹æ³•çš„é€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æ·±å…¥æ¢è®¨äº†æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å›¾åƒåˆ†ç±»ä¸­çš„å½¢çŠ¶å’Œçº¹ç†åè§é—®é¢˜åŠå…¶å¯¹æ³›åŒ–å’Œç¨³å¥æ€§çš„å½±å“ã€‚</li>
<li>é€šè¿‡é£æ ¼åŒ–å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œå¯æœ‰æ•ˆå‡å°‘çº¹ç†åè§å¹¶æé«˜æ¨¡å‹å¯¹å›¾åƒå¤±çœŸçš„é²æ£’æ€§ã€‚</li>
<li>ç ”ç©¶å°†é£æ ¼è½¬ç§»æŠ€æœ¯åº”ç”¨äºè¯­ä¹‰åˆ†å‰²é¢†åŸŸï¼Œé€šè¿‡æ”¹å˜å›¾åƒçš„éƒ¨åˆ†åŒºåŸŸï¼ˆç”±æ²ƒç½—è¯ºä¼Šç»†èƒç»„æˆï¼‰è¿›è¡Œè®­ç»ƒã€‚</li>
<li>é£æ ¼è½¬ç§»å¢å¼ºè®­ç»ƒå¯é™ä½è¯­ä¹‰åˆ†å‰²ä¸­çš„çº¹ç†åè§ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—æé«˜æ¨¡å‹å¯¹å¸¸è§å›¾åƒå¤±çœŸä»¥åŠå¯¹æŠ—æ”»å‡»çš„ç¨³å¥æ€§ã€‚</li>
<li>å®éªŒç»“æœé€‚ç”¨äºå·ç§¯ç¥ç»ç½‘ç»œå’Œè½¬æ¢å™¨æ¶æ„ï¼ŒéªŒè¯äº†æ–¹æ³•çš„é€šç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0052d70ef45a1c8f17d107292b7dc2f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33a59ecdd4e595ce5102b2e20e248d54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f6a542b0ef4adc56526b14def16c041e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c30f54fa848454ef5c022bbc33b3c15.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Measuring-the-Impact-of-Rotation-Equivariance-on-Aerial-Object-Detection"><a href="#Measuring-the-Impact-of-Rotation-Equivariance-on-Aerial-Object-Detection" class="headerlink" title="Measuring the Impact of Rotation Equivariance on Aerial Object Detection"></a>Measuring the Impact of Rotation Equivariance on Aerial Object Detection</h2><p><strong>Authors:Xiuyu Wu, Xinhao Wang, Xiubin Zhu, Lan Yang, Jiyuan Liu, Xingchen Hu</strong></p>
<p>Due to the arbitrary orientation of objects in aerial images, rotation equivariance is a critical property for aerial object detectors. However, recent studies on rotation-equivariant aerial object detection remain scarce. Most detectors rely on data augmentation to enable models to learn approximately rotation-equivariant features. A few detectors have constructed rotation-equivariant networks, but due to the breaking of strict rotation equivariance by typical downsampling processes, these networks only achieve approximately rotation-equivariant backbones. Whether strict rotation equivariance is necessary for aerial image object detection remains an open question. In this paper, we implement a strictly rotation-equivariant backbone and neck network with a more advanced network structure and compare it with approximately rotation-equivariant networks to quantitatively measure the impact of rotation equivariance on the performance of aerial image detectors. Additionally, leveraging the inherently grouped nature of rotation-equivariant features, we propose a multi-branch head network that reduces the parameter count while improving detection accuracy. Based on the aforementioned improvements, this study proposes the Multi-branch head rotation-equivariant single-stage Detector (MessDet), which achieves state-of-the-art performance on the challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an exceptionally low parameter count. </p>
<blockquote>
<p>ç”±äºç©ºä¸­å›¾åƒä¸­ç‰©ä½“çš„ä»»æ„æ–¹å‘æ€§ï¼Œæ—‹è½¬ç­‰å˜æ€§å¯¹äºç©ºä¸­ç‰©ä½“æ£€æµ‹å™¨æ˜¯ä¸€ä¸ªå…³é”®å±æ€§ã€‚ç„¶è€Œï¼Œå…³äºæ—‹è½¬ç­‰å˜ç©ºä¸­ç‰©ä½“æ£€æµ‹çš„ç ”ç©¶ä»ç„¶å¾ˆå°‘ã€‚å¤§å¤šæ•°æ£€æµ‹å™¨ä¾èµ–äºæ•°æ®å¢å¼ºæ¥ä½¿æ¨¡å‹å­¦ä¹ è¿‘ä¼¼æ—‹è½¬ç­‰å˜ç‰¹å¾ã€‚ä¸€äº›æ£€æµ‹å™¨å·²ç»æ„å»ºäº†æ—‹è½¬ç­‰å˜ç½‘ç»œï¼Œä½†ç”±äºå…¸å‹ä¸‹é‡‡æ ·è¿‡ç¨‹ç ´åäº†ä¸¥æ ¼çš„æ—‹è½¬ç­‰å˜æ€§ï¼Œè¿™äº›ç½‘ç»œä»…å®ç°äº†è¿‘ä¼¼æ—‹è½¬ç­‰å˜éª¨å¹²ã€‚å¯¹äºç©ºä¸­å›¾åƒç›®æ ‡æ£€æµ‹è€Œè¨€ï¼Œä¸¥æ ¼çš„æ—‹è½¬ç­‰å˜æ€§æ˜¯å¦å¿…è¦ä»ç„¶æ˜¯ä¸€ä¸ªæ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªä¸¥æ ¼çš„æ—‹è½¬ç­‰å˜éª¨å¹²å’Œé¢ˆéƒ¨ç½‘ç»œï¼Œå…·æœ‰æ›´å…ˆè¿›çš„ç½‘ç»œç»“æ„ï¼Œå¹¶ä¸è¿‘ä¼¼æ—‹è½¬ç­‰å˜ç½‘ç»œè¿›è¡Œæ¯”è¾ƒï¼Œä»¥å®šé‡æµ‹é‡æ—‹è½¬ç­‰å˜æ€§å¯¹ç©ºä¸­å›¾åƒæ£€æµ‹å™¨æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨æ—‹è½¬ç­‰å˜ç‰¹å¾çš„å›ºæœ‰åˆ†ç»„ç‰¹æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šåˆ†æ”¯å¤´ç½‘ç»œï¼Œè¯¥ç½‘ç»œåœ¨å‡å°‘å‚æ•°è®¡æ•°çš„åŒæ—¶æé«˜äº†æ£€æµ‹ç²¾åº¦ã€‚åŸºäºä¸Šè¿°æ”¹è¿›ï¼Œæœ¬ç ”ç©¶æå‡ºäº†å¤šåˆ†æ”¯å¤´æ—‹è½¬ç­‰å˜å•é˜¶æ®µæ£€æµ‹å™¨ï¼ˆMessDetï¼‰ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç©ºä¸­å›¾åƒæ•°æ®é›†DOTA-v1.0ã€DOTA-v1.5å’ŒDIOR-Rä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”å‚æ•°æ•°é‡å¼‚å¸¸ä¹‹ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09896v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ—‹è½¬ç­‰å˜æ€§åœ¨é¢å‘èˆªæ‹å›¾åƒçš„ç›®æ ‡æ£€æµ‹ä¸­çš„é‡è¦æ€§ã€‚æ–‡ç« å®ç°äº†ä¸¥æ ¼æ—‹è½¬ç­‰å˜çš„éª¨å¹²ç½‘ç»œå’Œé¢ˆéƒ¨ç½‘ç»œï¼Œä¸è¿‘ä¼¼æ—‹è½¬ç­‰å˜çš„ç½‘ç»œç›¸æ¯”ï¼Œå®šé‡æµ‹é‡äº†æ—‹è½¬ç­‰å˜å¯¹èˆªæ‹å›¾åƒæ£€æµ‹å™¨æ€§èƒ½çš„å½±å“ã€‚æ­¤å¤–ï¼Œæ–‡ç« åˆ©ç”¨æ—‹è½¬ç­‰å˜ç‰¹å¾çš„å›ºæœ‰åˆ†ç»„ç‰¹æ€§ï¼Œæå‡ºäº†å¤šåˆ†æ”¯å¤´ç½‘ç»œï¼Œé™ä½äº†å‚æ•°è®¡æ•°ï¼Œæé«˜äº†æ£€æµ‹ç²¾åº¦ã€‚åŸºäºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†å¤šåˆ†æ”¯å¤´æ—‹è½¬ç­‰å˜å•é˜¶æ®µæ£€æµ‹å™¨ï¼ˆMessDetï¼‰ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„èˆªæ‹å›¾åƒæ•°æ®é›†DOTA-v1.0ã€DOTA-v1.5å’ŒDIOR-Rä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—‹è½¬ç­‰å˜æ€§å¯¹äºèˆªæ‹å›¾åƒä¸­çš„å¯¹è±¡æ£€æµ‹è‡³å…³é‡è¦ï¼Œå› ä¸ºèˆªæ‹å›¾åƒä¸­å¯¹è±¡çš„æœå‘æ˜¯ä»»æ„çš„ã€‚</li>
<li>å¤§å¤šæ•°æ£€æµ‹å™¨é€šè¿‡æ•°æ®å¢å¼ºæ¥ä½¿æ¨¡å‹å­¦ä¹ è¿‘ä¼¼æ—‹è½¬ç­‰å˜ç‰¹å¾ï¼Œä½†ä¸¥æ ¼æ—‹è½¬ç­‰å˜æ€§ä»æœ‰å¾…ç ”ç©¶ã€‚</li>
<li>æ–‡ç« å®ç°äº†ä¸¥æ ¼æ—‹è½¬ç­‰å˜çš„éª¨å¹²ç½‘ç»œå’Œé¢ˆéƒ¨ç½‘ç»œã€‚</li>
<li>ä¸è¿‘ä¼¼æ—‹è½¬ç­‰å˜çš„ç½‘ç»œç›¸æ¯”ï¼Œä¸¥æ ¼æ—‹è½¬ç­‰å˜ç½‘ç»œå…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨æ—‹è½¬ç­‰å˜ç‰¹å¾çš„å›ºæœ‰åˆ†ç»„ç‰¹æ€§ï¼Œæå‡ºäº†å¤šåˆ†æ”¯å¤´ç½‘ç»œï¼Œé™ä½å‚æ•°è®¡æ•°ï¼Œæé«˜æ£€æµ‹ç²¾åº¦ã€‚</li>
<li>åŸºäºä¸Šè¿°æ”¹è¿›ï¼Œæå‡ºäº†å¤šåˆ†æ”¯å¤´æ—‹è½¬ç­‰å˜å•é˜¶æ®µæ£€æµ‹å™¨ï¼ˆMessDetï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09896">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-eba9548c2ecfd6e12e9ba71ffd934f00.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ba4d91167ee4912d6da1e1ac1bd3bf4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7bc373a4588fbe65e5ae54a811faff99.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a13afaa0f1330a9c6504c5b2df3f3a0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0c99db96f79d843f64e04078666d64f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-755d5518303dc7719c388917f34e501a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Stereo-based-3D-Anomaly-Object-Detection-for-Autonomous-Driving-A-New-Dataset-and-Baseline"><a href="#Stereo-based-3D-Anomaly-Object-Detection-for-Autonomous-Driving-A-New-Dataset-and-Baseline" class="headerlink" title="Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New   Dataset and Baseline"></a>Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New   Dataset and Baseline</h2><p><strong>Authors:Shiyi Mu, Zichong Gu, Hanqi Lyu, Yilin Gao, Shugong Xu</strong></p>
<p>3D detection technology is widely used in the field of autonomous driving, with its application scenarios gradually expanding from enclosed highways to open conventional roads. For rare anomaly categories that appear on the road, 3D detection models trained on closed sets often misdetect or fail to detect anomaly objects. To address this risk, it is necessary to enhance the generalization ability of 3D detection models for targets of arbitrary shapes and to possess the capability to filter out anomalies. The generalization of 3D detection is limited by two factors: the coupled training of 2D and 3D, and the insufficient diversity in the scale distribution of training samples. This paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm, which decouples the training strategy of 3D and 2D to release the generalization ability for arbitrary 3D foreground detection, and proposes an anomaly scoring algorithm based on foreground confidence prediction, achieving target-level anomaly scoring. In order to further verify and enhance the generalization of anomaly detection, we use a 3D rendering method to synthesize two augmented reality binocular stereo 3D detection datasets which named KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories as extra training data to address the sparse sample distribution issue. Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not used in training to simulate zero-shot scenarios in real-world settings, solely for evaluating 3D anomaly detection. Finally, the performance of the algorithm and the dataset is verified in the experiments. (Code and dataset can be obtained at <a target="_blank" rel="noopener" href="https://github.com/xxxx/xxx">https://github.com/xxxx/xxx</a>). </p>
<blockquote>
<p>3Dæ£€æµ‹æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸæœ‰ç€å¹¿æ³›åº”ç”¨ï¼Œå…¶åº”ç”¨åœºæ™¯å·²ä»å°é—­çš„é«˜é€Ÿå…¬è·¯é€æ­¥æ‰©å±•åˆ°å¼€æ”¾çš„å¸¸è§„é“è·¯ã€‚å¯¹äºé“è·¯ä¸Šå‡ºç°çš„ç¨€æœ‰å¼‚å¸¸ç±»åˆ«ï¼Œåœ¨å°é—­é›†ä¸Šè®­ç»ƒçš„3Dæ£€æµ‹æ¨¡å‹ç»å¸¸ä¼šå‘ç”Ÿè¯¯æ£€æˆ–æ— æ³•æ£€æµ‹åˆ°å¼‚å¸¸ç‰©ä½“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é£é™©ï¼Œå¿…é¡»æé«˜3Dæ£€æµ‹æ¨¡å‹å¯¹ä»»æ„å½¢çŠ¶çš„ç›®æ ‡çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶éœ€è¦å…·å¤‡è¿‡æ»¤å¼‚å¸¸å€¼çš„èƒ½åŠ›ã€‚3Dæ£€æµ‹çš„æ³›åŒ–å—åˆ°ä¸¤ä¸ªå› ç´ çš„é™åˆ¶ï¼š2Då’Œ3Dçš„è€¦åˆè®­ç»ƒï¼Œä»¥åŠè®­ç»ƒæ ·æœ¬å°ºåº¦åˆ†å¸ƒå¤šæ ·æ€§ä¸è¶³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç«‹ä½“è§†è§‰çš„3Då¼‚å¸¸ç‰©ä½“æ£€æµ‹ï¼ˆS3ADï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•è§£è€¦äº†3Då’Œ2Dçš„è®­ç»ƒç­–ç•¥ï¼Œä»¥é‡Šæ”¾å¯¹ä»»æ„3Då‰æ™¯æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºå‰æ™¯ç½®ä¿¡åº¦é¢„æµ‹çš„å¼‚å¸¸è¯„åˆ†ç®—æ³•ï¼Œå®ç°ç›®æ ‡çº§åˆ«çš„å¼‚å¸¸è¯„åˆ†ã€‚ä¸ºäº†è¿›ä¸€æ­¥éªŒè¯å’Œå¢å¼ºå¼‚å¸¸æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§3Dæ¸²æŸ“æ–¹æ³•æ¥åˆæˆä¸¤ä¸ªå¢å¼ºç°å®åŒç›®ç«‹ä½“3Dæ£€æµ‹æ•°æ®é›†ï¼Œåä¸ºKITTI-ARã€‚KITTI-ARåœ¨KITTIçš„åŸºç¡€ä¸Šå¢åŠ äº†97ä¸ªæ–°ç±»åˆ«ï¼Œæ€»å…±åŒ…å«6000å¯¹ç«‹ä½“å›¾åƒã€‚KITTI-AR-ExDå­é›†åŒ…å«39ä¸ªå¸¸è§ç±»åˆ«ä½œä¸ºé¢å¤–è®­ç»ƒæ•°æ®ï¼Œä»¥è§£å†³æ ·æœ¬åˆ†å¸ƒç¨€ç–çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œ58ä¸ªç½•è§ç±»åˆ«æ„æˆäº†KITTI-AR-OoDå­é›†ï¼Œè¿™äº›ç±»åˆ«ä¸ç”¨äºè®­ç»ƒï¼Œä»¥æ¨¡æ‹Ÿç°å®ä¸–ç•Œä¸­çš„é›¶æ ·æœ¬åœºæ™¯ï¼Œä»…ç”¨äºè¯„ä¼°3Då¼‚å¸¸æ£€æµ‹ã€‚æœ€åï¼Œé€šè¿‡å®éªŒéªŒè¯äº†ç®—æ³•å’Œæ•°æ®é›†çš„æ€§èƒ½ã€‚ï¼ˆä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xxxx/xxx%E8%8E%B7%E5%BE%97%E3%80%82%EF%BC%89">https://github.com/xxxx/xxxè·å¾—ã€‚ï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09214v1">PDF</a> under review</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„ä¸‰ç»´æ£€æµ‹æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨é¢å¯¹å¼€æ”¾é“è·¯ä¸Šçš„ç½•è§å¼‚å¸¸ç±»åˆ«æ—¶å­˜åœ¨è¯¯æ£€æˆ–æ¼æ£€çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç«‹ä½“è§†è§‰çš„3Då¼‚å¸¸ç›®æ ‡æ£€æµ‹ç®—æ³•ï¼ˆS3ADï¼‰ï¼Œé€šè¿‡è§£è€¦äºŒç»´å’Œä¸‰ç»´çš„è®­ç»ƒç­–ç•¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶åŸºäºå‰æ™¯ç½®ä¿¡åº¦é¢„æµ‹è¿›è¡Œå¼‚å¸¸è¯„åˆ†ã€‚ä¸ºäº†éªŒè¯å’Œå¢å¼ºå¼‚å¸¸æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œåˆ©ç”¨ä¸‰ç»´æ¸²æŸ“æ–¹æ³•åˆæˆäº†ä¸¤ä¸ªå¢å¼ºç°å®åŒç›®ç«‹ä½“ä¸‰ç»´æ£€æµ‹æ•°æ®é›†KITTI-ARã€‚å…¶ä¸­ï¼ŒKITTI-AR-ExDå­é›†åŒ…å«é¢å¤–çš„è®­ç»ƒæ•°æ®ä»¥è§£å†³æ ·æœ¬åˆ†å¸ƒç¨€ç–çš„é—®é¢˜ï¼Œè€ŒKITTI-AR-OoDå­é›†åŒ…å«ç”¨äºè¯„ä¼°ä½†ä¸ç”¨äºè®­ç»ƒçš„ç½•è§ç±»åˆ«ï¼Œä»¥æ¨¡æ‹Ÿç°å®ä¸–ç•Œä¸­çš„é›¶æ ·æœ¬åœºæ™¯ã€‚å®éªŒéªŒè¯äº†ç®—æ³•å’Œæ•°æ®é›†çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3Dæ£€æµ‹æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œä½†é¢å¯¹å¼€æ”¾é“è·¯ä¸Šçš„ç½•è§å¼‚å¸¸ç±»åˆ«å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºç«‹ä½“è§†è§‰çš„S3ADç®—æ³•ï¼Œé€šè¿‡è§£è€¦äºŒç»´å’Œä¸‰ç»´è®­ç»ƒæé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥å‰æ™¯ç½®ä¿¡åº¦é¢„æµ‹è¿›è¡Œå¼‚å¸¸è¯„åˆ†ï¼Œå®ç°ç›®æ ‡çº§åˆ«çš„å¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>é‡‡ç”¨äº†ä¸‰ç»´æ¸²æŸ“æ–¹æ³•åˆæˆKITTI-ARæ•°æ®é›†ï¼ŒåŒ…å«å¸¸è§å’Œç½•è§çš„ç±»åˆ«ï¼Œç”¨äºéªŒè¯å’Œå¢å¼ºå¼‚å¸¸æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å…¶ä¸­KITTI-AR-ExDè§£å†³æ ·æœ¬åˆ†å¸ƒç¨€ç–é—®é¢˜ï¼ŒKITTI-AR-OoDæ¨¡æ‹Ÿé›¶æ ·æœ¬åœºæ™¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09214">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0894a225fb573dddff38a7b4e3a6f560.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6300666637b436fa46a7c3f84b6c538c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaedc7f6c6e0a82d9b9df4e6d7b14eb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc474e618a1d84c8b09d53e5fa9f4ef7.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CRISP-SAM2-SAM2-with-Cross-Modal-Interaction-and-Semantic-Prompting-for-Multi-Organ-Segmentation"><a href="#CRISP-SAM2-SAM2-with-Cross-Modal-Interaction-and-Semantic-Prompting-for-Multi-Organ-Segmentation" class="headerlink" title="CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for   Multi-Organ Segmentation"></a>CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for   Multi-Organ Segmentation</h2><p><strong>Authors:Xinlei Yu, Changmiao Wang, Hui Jin, Ahmed Elazab, Gangyong Jia, Xiang Wan, Changqing Zou, Ruiquan Ge</strong></p>
<p>Multi-organ medical segmentation is a crucial component of medical image processing, essential for doctors to make accurate diagnoses and develop effective treatment plans. Despite significant progress in this field, current multi-organ segmentation models often suffer from inaccurate details, dependence on geometric prompts and loss of spatial information. Addressing these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal Interaction and Semantic Prompting based on SAM2. This model represents a promising approach to multi-organ medical segmentation guided by textual descriptions of organs. Our method begins by converting visual and textual inputs into cross-modal contextualized semantics using a progressive cross-attention interaction mechanism. These semantics are then injected into the image encoder to enhance the detailed understanding of visual information. To eliminate reliance on geometric prompts, we use a semantic prompting strategy, replacing the original prompt encoder to sharpen the perception of challenging targets. In addition, a similarity-sorting self-updating strategy for memory and a mask-refining process is applied to further adapt to medical imaging and enhance localized details. Comparative experiments conducted on seven public datasets indicate that CRISP-SAM2 outperforms existing models. Extensive analysis also demonstrates the effectiveness of our method, thereby confirming its superior performance, especially in addressing the limitations mentioned earlier. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/YU-deep/CRISP_SAM2.git">https://github.com/YU-deep/CRISP_SAM2.git</a>. </p>
<blockquote>
<p>åŒ»å­¦å¤šå™¨å®˜åˆ†å‰²æ˜¯åŒ»å­¦å›¾åƒå¤„ç†çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå¯¹äºåŒ»ç”Ÿè¿›è¡Œå‡†ç¡®çš„è¯Šæ–­å’Œåˆ¶å®šæœ‰æ•ˆçš„æ²»ç–—æ–¹æ¡ˆè‡³å…³é‡è¦ã€‚å°½ç®¡è¯¥é¢†åŸŸå–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰çš„å¤šå™¨å®˜åˆ†å‰²æ¨¡å‹é€šå¸¸å­˜åœ¨ç»†èŠ‚ä¸å‡†ç¡®ã€ä¾èµ–å‡ ä½•æç¤ºå’Œä¸¢å¤±ç©ºé—´ä¿¡æ¯ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åä¸ºCRISP-SAM2çš„æ–°æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºSAM2å…·æœ‰è·¨æ¨¡æ€äº¤äº’å’Œè¯­ä¹‰æç¤ºã€‚è¯¥æ¨¡å‹æ˜¯ä¸€ç§å¾ˆæœ‰å‰é€”çš„æ–¹æ³•ï¼Œå¯ä»¥é€šè¿‡å™¨å®˜çš„æ–‡æœ¬æè¿°æ¥æŒ‡å¯¼å¤šå™¨å®˜åŒ»å­¦åˆ†å‰²ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡æ¸è¿›çš„äº¤å‰æ³¨æ„åŠ›äº¤äº’æœºåˆ¶å°†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºè·¨æ¨¡æ€ä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚ç„¶åï¼Œè¿™äº›è¯­ä¹‰è¢«æ³¨å…¥å›¾åƒç¼–ç å™¨ï¼Œä»¥æé«˜å¯¹è§†è§‰ä¿¡æ¯çš„è¯¦ç»†ç†è§£ã€‚ä¸ºäº†å‡å°‘å¯¹å‡ ä½•æç¤ºçš„ä¾èµ–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§è¯­ä¹‰æç¤ºç­–ç•¥ï¼Œä»¥æ›¿ä»£åŸå§‹æç¤ºç¼–ç å™¨ï¼Œæé«˜äº†å¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ç›®æ ‡çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†ç›¸ä¼¼æ€§æ’åºçš„è‡ªæ›´æ–°ç­–ç•¥è¿›è¡Œå†…å­˜ç®¡ç†å’Œæ”¹è¿›äº†æ©è†œè¿‡ç¨‹ï¼Œä»¥è¿›ä¸€æ­¥é€‚åº”åŒ»å­¦å½±åƒå¹¶å¢å¼ºå±€éƒ¨ç»†èŠ‚ã€‚åœ¨ä¸ƒä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¯¹æ¯”å®éªŒè¡¨æ˜ï¼ŒCRISP-SAM2ä¼˜äºç°æœ‰æ¨¡å‹ã€‚å¹¿æ³›çš„åˆ†æä¹Ÿè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä»è€Œè¯å®äº†å…¶å“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³ä¸Šè¿°é™åˆ¶æ–¹é¢ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/YU-deep/CRISP_SAM2.git%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YU-deep/CRISP_SAM2.gitä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23121v3">PDF</a> Accepted By ACMMM25</p>
<p><strong>Summary</strong></p>
<p>ä¸€é¡¹é‡è¦ç ”ç©¶æå‡ºäº†CRISP-SAM2æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€äº¤äº’å’Œè¯­ä¹‰æç¤ºè§£å†³å¤šå™¨å®˜åŒ»å­¦åˆ†å‰²é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ¨¡å‹ç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œé€šè¿‡æ¸è¿›çš„äº¤å‰æ³¨æ„åŠ›äº¤äº’æœºåˆ¶ç”Ÿæˆè·¨æ¨¡æ€ä¸Šä¸‹æ–‡è¯­ä¹‰ï¼Œæ³¨å…¥å›¾åƒç¼–ç å™¨ä»¥æé«˜å¯¹è§†è§‰ä¿¡æ¯çš„è¯¦ç»†ç†è§£ã€‚æ­¤å¤–ï¼Œé‡‡ç”¨è¯­ä¹‰æç¤ºç­–ç•¥å‡å°‘å¯¹å‡ ä½•æç¤ºçš„ä¾èµ–ï¼Œå¹¶ä½¿ç”¨ç›¸ä¼¼æ€§æ’åºçš„è‡ªæ›´æ–°ç­–ç•¥å’Œæ©è†œç»†åŒ–è¿‡ç¨‹è¿›ä¸€æ­¥é€‚åº”åŒ»å­¦æˆåƒå¹¶å¢å¼ºå±€éƒ¨ç»†èŠ‚ã€‚åœ¨ä¸ƒä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¯¹æ¯”å®éªŒè¡¨æ˜ï¼ŒCRISP-SAM2æ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯è§£å†³äº†å…ˆå‰å­˜åœ¨çš„é™åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šå™¨å®˜åŒ»å­¦åˆ†å‰²æ˜¯åŒ»ç–—å›¾åƒå¤„ç†ä¸­çš„å…³é”®éƒ¨åˆ†ï¼Œå¯¹åŒ»ç”Ÿåšå‡ºå‡†ç¡®è¯Šæ–­å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰çš„å¤šå™¨å®˜åˆ†å‰²æ¨¡å‹å­˜åœ¨ä¸å‡†ç¡®ã€ä¾èµ–å‡ ä½•æç¤ºå’ŒæŸå¤±ç©ºé—´ä¿¡æ¯çš„é—®é¢˜ã€‚</li>
<li>CRISP-SAM2æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€äº¤äº’å’Œè¯­ä¹‰æç¤ºè§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æ¨¡å‹ç»“åˆè§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œç”Ÿæˆè·¨æ¨¡æ€ä¸Šä¸‹æ–‡è¯­ä¹‰ï¼Œæé«˜è§†è§‰ä¿¡æ¯çš„è¯¦ç»†ç†è§£ã€‚</li>
<li>é‡‡ç”¨è¯­ä¹‰æç¤ºç­–ç•¥å‡å°‘å‡ ä½•æç¤ºçš„ä¾èµ–ï¼Œå¢å¼ºå¯¹æŒ‘æˆ˜ç›®æ ‡çš„æ„ŸçŸ¥ã€‚</li>
<li>ç›¸ä¼¼æ€§æ’åºçš„è‡ªæ›´æ–°ç­–ç•¥å’Œæ©è†œç»†åŒ–è¿‡ç¨‹è¿›ä¸€æ­¥é€‚åº”åŒ»å­¦æˆåƒå¹¶å¢å¼ºå±€éƒ¨ç»†èŠ‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23121">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6141aab0b9b7a7f839c2b9cb0b758d86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-738db85b49148ecdfe098ba1af5be4e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-482667dec766d3edaafad32624a0618b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56ce20f55be9113de9e269c91d2c1eee.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Stronger-Steadier-Superior-Geometric-Consistency-in-Depth-VFM-Forges-Domain-Generalized-Semantic-Segmentation"><a href="#Stronger-Steadier-Superior-Geometric-Consistency-in-Depth-VFM-Forges-Domain-Generalized-Semantic-Segmentation" class="headerlink" title="Stronger, Steadier &amp; Superior: Geometric Consistency in Depth VFM Forges   Domain Generalized Semantic Segmentation"></a>Stronger, Steadier &amp; Superior: Geometric Consistency in Depth VFM Forges   Domain Generalized Semantic Segmentation</h2><p><strong>Authors:Siyu Chen, Ting Han, Changshe Zhang, Xin Luo, Meiliu Wu, Guorong Cai, Jinhe Su</strong></p>
<p>Vision Foundation Models (VFMs) have delivered remarkable performance in Domain Generalized Semantic Segmentation (DGSS). However, recent methods often overlook the fact that visual cues are susceptible, whereas the underlying geometry remains stable, rendering depth information more robust. In this paper, we investigate the potential of integrating depth information with features from VFMs, to improve the geometric consistency within an image and boost the generalization performance of VFMs. We propose a novel fine-tuning DGSS framework, named DepthForge, which integrates the visual cues from frozen DINOv2 or EVA02 and depth cues from frozen Depth Anything V2. In each layer of the VFMs, we incorporate depth-aware learnable tokens to continuously decouple domain-invariant visual and spatial information, thereby enhancing depth awareness and attention of the VFMs. Finally, we develop a depth refinement decoder and integrate it into the model architecture to adaptively refine multi-layer VFM features and depth-aware learnable tokens. Extensive experiments are conducted based on various DGSS settings and five different datsets as unseen target domains. The qualitative and quantitative results demonstrate that our method significantly outperforms alternative approaches with stronger performance, steadier visual-spatial attention, and superior generalization ability. In particular, DepthForge exhibits outstanding performance under extreme conditions (e.g., night and snow). Code is available at <a target="_blank" rel="noopener" href="https://github.com/anonymouse-xzrptkvyqc/DepthForge">https://github.com/anonymouse-xzrptkvyqc/DepthForge</a>. </p>
<blockquote>
<p>è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰åœ¨åŸŸé€šç”¨è¯­ä¹‰åˆ†å‰²ï¼ˆDGSSï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆç»©ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„æ–¹æ³•å¾€å¾€å¿½ç•¥äº†è¿™æ ·ä¸€ä¸ªäº‹å®ï¼Œå³è§†è§‰çº¿ç´¢æ˜¯æ˜“å˜çš„ï¼Œè€ŒåŸºç¡€å‡ ä½•ç»“æ„æ˜¯ç¨³å®šçš„ï¼Œè¿™ä½¿å¾—æ·±åº¦ä¿¡æ¯æ›´åŠ ç¨³å¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å°†æ·±åº¦ä¿¡æ¯ä¸VFMsçš„ç‰¹å¾ç›¸ç»“åˆï¼Œä»¥æé«˜å›¾åƒå†…çš„å‡ ä½•ä¸€è‡´æ€§å¹¶æå‡VFMsçš„æ³›åŒ–æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å¾®è°ƒDGSSæ¡†æ¶ï¼Œåä¸ºDepthForgeï¼Œå®ƒç»“åˆäº†æ¥è‡ªå†»ç»“çš„DINOv2æˆ–EVA02çš„è§†è§‰çº¿ç´¢å’Œæ¥è‡ªå†»ç»“çš„Depth Anything V2çš„æ·±åº¦çº¿ç´¢ã€‚åœ¨VFMsçš„æ¯ä¸€å±‚ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ·±åº¦æ„ŸçŸ¥çš„å¯å­¦ä¹ ä»¤ç‰Œï¼Œä»¥è¿ç»­åœ°è§£è€¦åŸŸä¸å˜è§†è§‰å’Œç©ºé—´ä¿¡æ¯ï¼Œä»è€Œæé«˜VFMsçš„æ·±åº¦æ„ŸçŸ¥å’Œæ³¨æ„åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ·±åº¦ç»†åŒ–è§£ç å™¨ï¼Œå¹¶å°†å…¶é›†æˆåˆ°æ¨¡å‹æ¶æ„ä¸­ï¼Œä»¥è‡ªé€‚åº”åœ°ç»†åŒ–å¤šå±‚VFMç‰¹å¾å’Œæ·±åº¦æ„ŸçŸ¥å¯å­¦ä¹ ä»¤ç‰Œã€‚åŸºäºå„ç§DGSSè®¾ç½®å’Œäº”ä¸ªä¸åŒæ•°æ®é›†ä½œä¸ºæœªè§è¿‡çš„ç›®æ ‡åŸŸè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚å®šæ€§å’Œå®šé‡ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå…·æœ‰æ›´å¼ºçš„æ€§èƒ½ã€æ›´ç¨³å®šçš„è§†è§‰-ç©ºé—´æ³¨æ„åŠ›å’Œæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯ï¼ŒDepthForgeåœ¨æç«¯æ¡ä»¶ä¸‹ï¼ˆä¾‹å¦‚å¤œæ™šå’Œé›ªåœ°ï¼‰è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/anonymouse-xzrptkvyqc/DepthForge%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/anonymouse-xzrptkvyqc/DepthForgeä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.12753v3">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°†æ·±åº¦ä¿¡æ¯èå…¥è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ä»¥æå‡é¢†åŸŸå¹¿ä¹‰è¯­ä¹‰åˆ†å‰²ï¼ˆDGSSï¼‰æ€§èƒ½çš„æ–¹æ³•ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºDepthForgeçš„æ–°å‹å¾®è°ƒDGSSæ¡†æ¶ï¼Œé€šè¿‡é›†æˆæ¥è‡ªå†»ç»“çš„DINOv2æˆ–EVA02çš„è§†è§‰çº¿ç´¢å’Œæ¥è‡ªå†»ç»“çš„Depth Anything V2çš„æ·±åº¦çº¿ç´¢ï¼Œä»¥å¢å¼ºå›¾åƒå†…çš„å‡ ä½•ä¸€è‡´æ€§å’ŒVFMsçš„æ³›åŒ–æ€§èƒ½ã€‚é€šè¿‡åœ¨å„å±‚èå…¥æ·±åº¦æ„ŸçŸ¥çš„å¯å­¦ä¹ ä»¤ç‰Œï¼Œä»¥æŒç»­è§£è€¦é¢†åŸŸä¸å˜è§†è§‰å’Œç©ºé—´ä¿¡æ¯ï¼Œä»è€Œæé«˜æ·±åº¦æ„ŸçŸ¥å’Œæ³¨æ„åŠ›ã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ä¸€ä¸ªæ·±åº¦ç»†åŒ–è§£ç å™¨ï¼Œå°†å…¶é›†æˆåˆ°æ¨¡å‹æ¶æ„ä¸­ï¼Œä»¥è‡ªé€‚åº”åœ°ä¼˜åŒ–å¤šå±‚VFMç‰¹æ€§å’Œæ·±åº¦æ„ŸçŸ¥å¯å­¦ä¹ ä»¤ç‰Œã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æ›¿ä»£æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ›´å¼ºçš„æ€§èƒ½ã€æ›´ç¨³å®šçš„è§†è§‰ç©ºé—´æ³¨æ„åŠ›å’Œæ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æç«¯æ¡ä»¶ä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡å¼ºè°ƒäº†æ·±åº¦ä¿¡æ¯åœ¨é¢†åŸŸå¹¿ä¹‰è¯­ä¹‰åˆ†å‰²ï¼ˆDGSSï¼‰ä¸­çš„é‡è¦ä½œç”¨ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ•´åˆæ·±åº¦ä¿¡æ¯ä¸è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ç‰¹å¾çš„æ–¹æ³•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºDepthForgeçš„æ–°å‹å¾®è°ƒDGSSæ¡†æ¶ï¼Œèåˆäº†è§†è§‰çº¿ç´¢å’Œæ·±åº¦çº¿ç´¢ã€‚</li>
<li>DepthForgeé€šè¿‡èå…¥æ·±åº¦æ„ŸçŸ¥çš„å¯å­¦ä¹ ä»¤ç‰Œï¼Œæé«˜äº†VFMsçš„æ·±åº¦æ„ŸçŸ¥å’Œæ³¨æ„åŠ›ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªæ·±åº¦ç»†åŒ–è§£ç å™¨ï¼Œä»¥è‡ªé€‚åº”åœ°ä¼˜åŒ–å¤šå±‚VFMç‰¹æ€§å’Œæ·±åº¦æ„ŸçŸ¥å¯å­¦ä¹ ä»¤ç‰Œã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDepthForgeåœ¨å¤šç§DGSSè®¾ç½®å’Œäº”ä¸ªä¸åŒæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>DepthForgeåœ¨æç«¯æ¡ä»¶ä¸‹ï¼ˆå¦‚å¤œæ™šå’Œé›ªåœ°ï¼‰è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.12753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-79abbc18a00a37f7f70113d47ec13816.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fa5b1f9794f6cc27ab146cef66b879e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-269f841d59c1eb9b2910632b0fbaf902.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-502515bb99a920184a040df9820a756a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d21659cbbdf687d341fde494ff41695.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-979cbbe01efc37d3a4c63fb90ca0a264.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="HA-RDet-Hybrid-Anchor-Rotation-Detector-for-Oriented-Object-Detection"><a href="#HA-RDet-Hybrid-Anchor-Rotation-Detector-for-Oriented-Object-Detection" class="headerlink" title="HA-RDet: Hybrid Anchor Rotation Detector for Oriented Object Detection"></a>HA-RDet: Hybrid Anchor Rotation Detector for Oriented Object Detection</h2><p><strong>Authors:Phuc D. A. Nguyen</strong></p>
<p>Oriented object detection in aerial images poses a significant challenge due to their varying sizes and orientations. Current state-of-the-art detectors typically rely on either two-stage or one-stage approaches, often employing Anchor-based strategies, which can result in computationally expensive operations due to the redundant number of generated anchors during training. In contrast, Anchor-free mechanisms offer faster processing but suffer from a reduction in the number of training samples, potentially impacting detection accuracy. To address these limitations, we propose the Hybrid-Anchor Rotation Detector (HA-RDet), which combines the advantages of both anchor-based and anchor-free schemes for oriented object detection. By utilizing only one preset anchor for each location on the feature maps and refining these anchors with our Orientation-Aware Convolution technique, HA-RDet achieves competitive accuracies, including 75.41 mAP on DOTA-v1, 65.3 mAP on DIOR-R, and 90.2 mAP on HRSC2016, against current anchor-based state-of-the-art methods, while significantly reducing computational resources. </p>
<blockquote>
<p>é’ˆå¯¹èˆªæ‹å›¾åƒä¸­çš„å®šå‘ç›®æ ‡æ£€æµ‹é—®é¢˜ï¼Œç”±äºå…¶å°ºå¯¸å’Œæ–¹å‘çš„å¤šæ ·æ€§ï¼Œæ„æˆäº†ä¸€å¤§æŒ‘æˆ˜ã€‚å½“å‰æœ€å…ˆè¿›çš„æ£€æµ‹å™¨é€šå¸¸ä¾èµ–äºä¸¤é˜¶æ®µæˆ–ä¸€é˜¶æ®µçš„æ–¹æ³•ï¼Œç»å¸¸é‡‡ç”¨åŸºäºé”šç‚¹çš„ç­–ç•¥ã€‚ç”±äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­äº§ç”Ÿçš„é”šç‚¹æ•°é‡å†—ä½™ï¼Œè¿™å¯èƒ½å¯¼è‡´è®¡ç®—æˆæœ¬è¾ƒé«˜çš„æ“ä½œã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ— é”šç‚¹æœºåˆ¶å¤„ç†é€Ÿåº¦æ›´å¿«ï¼Œä½†ä¼šé™ä½è®­ç»ƒæ ·æœ¬çš„æ•°é‡ï¼Œä»è€Œå¯èƒ½å½±å“æ£€æµ‹ç²¾åº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†æ··åˆé”šç‚¹æ—‹è½¬æ£€æµ‹å™¨ï¼ˆHA-RDetï¼‰ï¼Œå®ƒå°†åŸºäºé”šç‚¹å’Œæ— é”šç‚¹æ–¹æ¡ˆçš„ä¼˜åŠ¿ç»“åˆèµ·æ¥ï¼Œç”¨äºå®šå‘ç›®æ ‡æ£€æµ‹ã€‚é€šè¿‡åœ¨ç‰¹å¾å›¾çš„æ¯ä¸ªä½ç½®åªä½¿ç”¨ä¸€ä¸ªé¢„è®¾é”šç‚¹ï¼Œå¹¶ç»“åˆæˆ‘ä»¬çš„æ–¹å‘æ„ŸçŸ¥å·ç§¯æŠ€æœ¯è¿›è¡Œç²¾ç»†è°ƒæ•´ï¼ŒHA-RDetå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„ç²¾åº¦ï¼Œåœ¨DOTA-v1ä¸Šè¾¾åˆ°75.41 mAPï¼Œåœ¨DIOR-Rä¸Šè¾¾åˆ°65.3 mAPï¼Œåœ¨HRSC2016ä¸Šè¾¾åˆ°90.2 mAPï¼Œä¸å½“å‰åŸºäºé”šç‚¹çš„æœ€å…ˆè¿›æ–¹æ³•ç›¸æ¯”ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†è®¡ç®—èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14379v2">PDF</a> Bachelor thesis, Accepted to ICCVâ€™25 SEA</p>
<p><strong>Summary</strong></p>
<p>åŸºäºç©ºä¸­å›¾åƒçš„ç›®æ ‡æ£€æµ‹é¢ä¸´å°ºå¯¸å’Œæ–¹ä½å¤šå˜çš„æŒ‘æˆ˜ã€‚å½“å‰é¡¶å°–çš„æ£€æµ‹å™¨ä¸»è¦ä¾èµ–ä¸¤é˜¶æ®µæˆ–ä¸€é˜¶æ®µçš„æ£€æµ‹æ–¹å¼ï¼Œç»å¸¸é‡‡ç”¨åŸºäºé”šçš„ç­–ç•¥ï¼Œè¿™ä¼šç”±äºè®­ç»ƒæ—¶ç”Ÿæˆçš„é”šå†—ä½™è€Œè®¡ç®—æˆæœ¬é«˜ã€‚ä¸æ­¤ç›¸å¯¹çš„æ˜¯ï¼Œæ— é”šæœºåˆ¶å¤„ç†æ›´å¿«ï¼Œä½†å‡å°‘è®­ç»ƒæ ·æœ¬æ•°é‡ï¼Œå¯èƒ½å½±å“æ£€æµ‹ç²¾åº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºäº†æ··åˆé”šæ—‹è½¬æ£€æµ‹å™¨ï¼ˆHA-RDetï¼‰ï¼Œç»“åˆäº†åŸºäºé”šå’Œæ— é”šæ–¹æ¡ˆçš„ä¼˜åŠ¿è¿›è¡Œæ–¹ä½ç›®æ ‡æ£€æµ‹ã€‚é€šè¿‡åœ¨ç‰¹å¾å›¾çš„æ¯ä¸ªä½ç½®åªä½¿ç”¨ä¸€ä¸ªé¢„è®¾é”šï¼Œå¹¶ä½¿ç”¨æˆ‘ä»¬çš„æ–¹å‘æ„ŸçŸ¥å·ç§¯æŠ€æœ¯è¿›è¡Œç²¾ç‚¼ï¼ŒHA-RDetåœ¨DOTA-v1è¾¾åˆ°75.41 mAPã€DIOR-Rè¾¾åˆ°65.3 mAPä»¥åŠHRSC2016è¾¾åˆ°90.2 mAPï¼Œç›¸è¾ƒäºå½“å‰çš„é¡¶å°–åŸºäºé”šçš„æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ï¼ŒåŒæ—¶å¤§å¹…å‡å°‘è®¡ç®—èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©ºä¸­å›¾åƒçš„ç›®æ ‡æ£€æµ‹é¢ä¸´å°ºå¯¸å’Œæ–¹ä½å¤šå˜çš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰é¡¶å°–æ£€æµ‹å™¨é‡‡ç”¨åŸºäºé”šçš„ç­–ç•¥ä¼šæœ‰è®¡ç®—æˆæœ¬é«˜çš„ç¼ºç‚¹ã€‚</li>
<li>æ— é”šæœºåˆ¶å¤„ç†æ›´å¿«ï¼Œä½†å¯èƒ½å½±å“æ£€æµ‹ç²¾åº¦ã€‚</li>
<li>æ··åˆé”šæ—‹è½¬æ£€æµ‹å™¨ï¼ˆHA-RDetï¼‰ç»“åˆäº†åŸºäºé”šå’Œæ— é”šçš„ä¼˜åŠ¿ã€‚</li>
<li>HA-RDetåœ¨æ¯ä¸ªç‰¹å¾å›¾ä½ç½®åªä½¿ç”¨ä¸€ä¸ªé¢„è®¾é”šã€‚</li>
<li>HA-RDetä½¿ç”¨æ–¹å‘æ„ŸçŸ¥å·ç§¯æŠ€æœ¯ç²¾ç‚¼é”šç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14379">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-60a835a3b14df7c67552497f100b0c03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b86da278cf5d36d161680c0363f8462a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f6bfc964ea5915ebd786ab5d28002c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ed835a0bcee12e8bdc7b64862f10039.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CorrCLIP-Reconstructing-Patch-Correlations-in-CLIP-for-Open-Vocabulary-Semantic-Segmentation"><a href="#CorrCLIP-Reconstructing-Patch-Correlations-in-CLIP-for-Open-Vocabulary-Semantic-Segmentation" class="headerlink" title="CorrCLIP: Reconstructing Patch Correlations in CLIP for Open-Vocabulary   Semantic Segmentation"></a>CorrCLIP: Reconstructing Patch Correlations in CLIP for Open-Vocabulary   Semantic Segmentation</h2><p><strong>Authors:Dengke Zhang, Fagui Liu, Quan Tang</strong></p>
<p>Open-vocabulary semantic segmentation aims to assign semantic labels to each pixel without being constrained by a predefined set of categories. While Contrastive Language-Image Pre-training (CLIP) excels in zero-shot classification, it struggles to align image patches with category embeddings because of its incoherent patch correlations. This study reveals that inter-class correlations are the main reason for impairing CLIPâ€™s segmentation performance. Accordingly, we propose CorrCLIP, which reconstructs the scope and value of patch correlations. Specifically, CorrCLIP leverages the Segment Anything Model (SAM) to define the scope of patch interactions, reducing inter-class correlations. To mitigate the problem that SAM-generated masks may contain patches belonging to different classes, CorrCLIP incorporates self-supervised models to compute coherent similarity values, suppressing the weight of inter-class correlations. Additionally, we introduce two additional branches to strengthen patch featuresâ€™ spatial details and semantic representation. Finally, we update segmentation maps with SAM-generated masks to improve spatial consistency. Based on the improvement across patch correlations, feature representations, and segmentation maps, CorrCLIP achieves superior performance across eight benchmarks. Codes are available at: <a target="_blank" rel="noopener" href="https://github.com/zdk258/CorrCLIP">https://github.com/zdk258/CorrCLIP</a>. </p>
<blockquote>
<p>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ—¨åœ¨ä¸ºæ¯ä¸ªåƒç´ åˆ†é…è¯­ä¹‰æ ‡ç­¾ï¼Œè€Œä¸å—é¢„è®¾ç±»åˆ«é›†çš„çº¦æŸã€‚è™½ç„¶å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰åœ¨é›¶æ ·æœ¬åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç”±äºå…¶ä¸è¿è´¯çš„è¡¥ä¸ç›¸å…³æ€§ï¼Œå®ƒå¾ˆéš¾å°†å›¾åƒè¡¥ä¸ä¸ç±»åˆ«åµŒå…¥å¯¹é½ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œç±»é—´ç›¸å…³æ€§æ˜¯æŸå®³CLIPåˆ†å‰²æ€§èƒ½çš„ä¸»è¦åŸå› ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†CorrCLIPï¼Œå®ƒé‡æ„äº†è¡¥ä¸ç›¸å…³æ€§çš„èŒƒå›´å’Œä»·å€¼ã€‚å…·ä½“æ¥è¯´ï¼ŒCorrCLIPåˆ©ç”¨ä¸‡ç‰©å¯åˆ†å‰²æ¨¡å‹ï¼ˆSAMï¼‰å®šä¹‰è¡¥ä¸äº¤äº’çš„èŒƒå›´ï¼Œå‡å°‘ç±»é—´ç›¸å…³æ€§ã€‚ä¸ºäº†è§£å†³SAMç”Ÿæˆçš„è’™ç‰ˆå¯èƒ½åŒ…å«å±äºä¸åŒç±»åˆ«çš„è¡¥ä¸çš„é—®é¢˜ï¼ŒCorrCLIPç»“åˆäº†è‡ªç›‘ç£æ¨¡å‹æ¥è®¡ç®—è¿è´¯çš„ç›¸ä¼¼åº¦å€¼ï¼ŒæŠ‘åˆ¶ç±»é—´ç›¸å…³æ€§çš„æƒé‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å¦å¤–ä¸¤ä¸ªåˆ†æ”¯ï¼Œä»¥å¢å¼ºè¡¥ä¸ç‰¹å¾çš„ç©ºé—´ç»†èŠ‚å’Œè¯­ä¹‰è¡¨ç¤ºã€‚æœ€åï¼Œæˆ‘ä»¬ç”¨SAMç”Ÿæˆçš„è’™ç‰ˆæ›´æ–°åˆ†å‰²å›¾ï¼Œä»¥æé«˜ç©ºé—´ä¸€è‡´æ€§ã€‚åŸºäºè¡¥ä¸ç›¸å…³æ€§ã€ç‰¹å¾è¡¨ç¤ºå’Œåˆ†å‰²å›¾çš„æ”¹è¿›ï¼ŒCorrCLIPåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/zdk258/CorrCLIP%E3%80%82">https://github.com/zdk258/CorrCLIPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10086v2">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºCLIPæ¨¡å‹çš„è¯­ä¹‰åˆ†å‰²é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å…¶å¯¹äºå›¾åƒè¡¥ä¸ä¸ç±»åˆ«åµŒå…¥å¯¹é½çš„å›°éš¾ã€‚ç ”ç©¶å‘ç°ï¼Œç±»é—´å…³è”æ˜¯åˆ¶çº¦CLIPåˆ†å‰²æ€§èƒ½çš„ä¸»è¦åŸå› ã€‚ä¸ºæ­¤ï¼Œæå‡ºCorrCLIPæ¨¡å‹ï¼Œåˆ©ç”¨Segment Anything Modelï¼ˆSAMï¼‰å®šä¹‰è¡¥ä¸äº¤äº’èŒƒå›´ï¼Œé™ä½ç±»é—´å…³è”ã€‚ä¸ºè§£å†³SAMç”Ÿæˆçš„æ©è†œå¯èƒ½åŒ…å«ä¸åŒç±»åˆ«è¡¥ä¸çš„é—®é¢˜ï¼ŒCorrCLIPç»“åˆè‡ªç›‘ç£æ¨¡å‹è®¡ç®—è¿è´¯çš„ç›¸ä¼¼åº¦å€¼ï¼ŒæŠ‘åˆ¶ç±»é—´å…³è”çš„æƒé‡ã€‚æ­¤å¤–ï¼Œå¢åŠ ä¸¤ä¸ªåˆ†æ”¯ä»¥å¢å¼ºè¡¥ä¸ç‰¹å¾çš„ç©ºé—´ç»†èŠ‚å’Œè¯­ä¹‰è¡¨ç¤ºã€‚æœ€åï¼Œä½¿ç”¨SAMç”Ÿæˆçš„æ©è†œæ›´æ–°åˆ†å‰²å›¾ï¼Œæé«˜ç©ºé—´ä¸€è‡´æ€§ã€‚CorrCLIPåœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²æ—¨åœ¨ä¸ºæ¯ä¸ªåƒç´ åˆ†é…è¯­ä¹‰æ ‡ç­¾ï¼Œä¸å—é¢„å®šä¹‰ç±»åˆ«é›†çš„çº¦æŸã€‚</li>
<li>CLIPåœ¨é›¶æ ·æœ¬åˆ†ç±»ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å›¾åƒè¡¥ä¸ä¸ç±»åˆ«åµŒå…¥å¯¹é½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>ç±»é—´å…³è”æ˜¯åˆ¶çº¦CLIPåˆ†å‰²æ€§èƒ½çš„ä¸»è¦åŸå› ã€‚</li>
<li>CorrCLIPæ¨¡å‹åˆ©ç”¨Segment Anything Modelï¼ˆSAMï¼‰é™ä½ç±»é—´å…³è”ï¼Œæé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>SAMç”Ÿæˆçš„æ©è†œå¯èƒ½åŒ…å«ä¸åŒç±»åˆ«çš„è¡¥ä¸ï¼ŒCorrCLIPé€šè¿‡ç»“åˆè‡ªç›‘ç£æ¨¡å‹è®¡ç®—ç›¸ä¼¼åº¦å€¼æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>CorrCLIPå¢åŠ ä¸¤ä¸ªåˆ†æ”¯ä»¥å¢å¼ºè¡¥ä¸ç‰¹å¾çš„ç©ºé—´ç»†èŠ‚å’Œè¯­ä¹‰è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10086">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f7e27c6c75ffb32cb830c8d83cb9ccd0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76bdbe9d6e5fd890d252cd5c5eb62b21.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79c959aae244d0261b6d634b47756ea1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-172c2422f3193e3f05970dd99ddf9e36.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Advancing-Automatic-Photovoltaic-Defect-Detection-using-Semi-Supervised-Semantic-Segmentation-of-Electroluminescence-Images"><a href="#Advancing-Automatic-Photovoltaic-Defect-Detection-using-Semi-Supervised-Semantic-Segmentation-of-Electroluminescence-Images" class="headerlink" title="Advancing Automatic Photovoltaic Defect Detection using Semi-Supervised   Semantic Segmentation of Electroluminescence Images"></a>Advancing Automatic Photovoltaic Defect Detection using Semi-Supervised   Semantic Segmentation of Electroluminescence Images</h2><p><strong>Authors:Abhishek Jha, Yogesh Rawat, Shruti Vyas</strong></p>
<p>Photovoltaic (PV) systems allow us to tap into all abundant solar energy, however they require regular maintenance for high efficiency and to prevent degradation. Traditional manual health check, using Electroluminescence (EL) imaging, is expensive and logistically challenging which makes automated defect detection essential. Current automation approaches require extensive manual expert labeling, which is time-consuming, expensive, and prone to errors. We propose PV-S3 (Photovoltaic-Semi-supervised Semantic Segmentation), a Semi-Supervised Learning approach for semantic segmentation of defects in EL images that reduces reliance on extensive labeling. PV-S3 is an artificial intelligence (AI) model trained using a few labeled images along with numerous unlabeled images. We introduce a novel Semi Cross-Entropy loss function to deal with class imbalance. We evaluate PV-S3 on multiple datasets and demonstrate its effectiveness and adaptability. With merely 20% labeled samples, we achieve an absolute improvement of 9.7% in mean Intersection-over-Union (mIoU), 13.5% in Precision, 29.15% in Recall, and 20.42% in F1-Score over prior state-of-the-art supervised method (which uses 100% labeled samples) on University of Central Florida-Electroluminescence (UCF-EL) dataset (largest dataset available for semantic segmentation of EL images) showing improvement in performance while reducing the annotation costs by 80%. For more details, visit our GitHub repository: <a target="_blank" rel="noopener" href="https://github.com/abj247/PV-S3">https://github.com/abj247/PV-S3</a>. </p>
<blockquote>
<p>å…‰ä¼ï¼ˆPVï¼‰ç³»ç»Ÿä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨æ‰€æœ‰ä¸°å¯Œçš„å¤ªé˜³èƒ½ï¼Œç„¶è€Œï¼Œä¸ºäº†è·å¾—é«˜æ•ˆç‡å¹¶é˜²æ­¢æ€§èƒ½é€€åŒ–ï¼Œå®ƒä»¬éœ€è¦è¿›è¡Œå®šæœŸç»´æŠ¤ã€‚ä¼ ç»Ÿçš„ä½¿ç”¨ç”µè‡´å‘å…‰ï¼ˆELï¼‰æˆåƒè¿›è¡Œå¥åº·æ£€æŸ¥çš„æ–¹æ³•æˆæœ¬é«˜æ˜‚ï¼Œåœ¨åå‹¤æ–¹é¢ä¹Ÿå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™ä½¿å¾—è‡ªåŠ¨åŒ–ç¼ºé™·æ£€æµ‹è‡³å…³é‡è¦ã€‚å½“å‰çš„è‡ªåŠ¨åŒ–æ–¹æ³•éœ€è¦å¹¿æ³›çš„äººå·¥ä¸“å®¶æ ‡è®°ï¼Œè¿™ä¸ä»…è€—æ—¶ã€æˆæœ¬é«˜ï¼Œè€Œä¸”å®¹æ˜“å‡ºé”™ã€‚æˆ‘ä»¬æå‡ºäº†PV-S3ï¼ˆå…‰ä¼åŠç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºELå›¾åƒç¼ºé™·è¯­ä¹‰åˆ†å‰²çš„åŠç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå‡å°‘å¯¹å¤§é‡æ ‡è®°çš„ä¾èµ–ã€‚PV-S3æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¨¡å‹ï¼Œå®ƒä½¿ç”¨å°‘é‡æ ‡è®°å›¾åƒå’Œå¤§é‡æœªæ ‡è®°å›¾åƒè¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„åŠäº¤å‰ç†µæŸå¤±å‡½æ•°æ¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°PV-S3ï¼Œè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œé€‚åº”æ€§ã€‚ä»…ä»…ä½¿ç”¨20%çš„æ ‡è®°æ ·æœ¬ï¼Œæˆ‘ä»¬åœ¨ä½›ç½—é‡Œè¾¾å¤§å­¦ç”µè‡´å‘å…‰ï¼ˆUCF-ELï¼‰æ•°æ®é›†ï¼ˆå¯ç”¨äºELå›¾åƒè¯­ä¹‰åˆ†å‰²çš„æœ€å¤§æ•°æ®é›†ï¼‰ä¸Šçš„å¹³å‡äº¤å¹¶æ¯”ï¼ˆmIoUï¼‰ç»å¯¹æé«˜äº†9.7%ï¼Œç²¾ç¡®åº¦æé«˜äº†13.5%ï¼Œå¬å›ç‡æé«˜äº†29.15%ï¼ŒF1åˆ†æ•°æé«˜äº†20.42%ï¼Œè¶…è¿‡äº†æœ€æ–°ç›‘ç£æ–¹æ³•ï¼ˆä½¿ç”¨100%æ ‡è®°æ ·æœ¬ï¼‰ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„GitHubä»“åº“ï¼š<a target="_blank" rel="noopener" href="https://github.com/abj247/PV-S3%E3%80%82">https://github.com/abj247/PV-S3ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.13693v4">PDF</a> 19 pages, 10 figures</p>
<p><strong>Summary</strong>ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨åŠç›‘ç£å­¦ä¹ æ–¹æ³•å¯¹å¤ªé˜³èƒ½ç”µæ± æ¿çš„ç¼ºé™·è¿›è¡Œè¯­ä¹‰åˆ†å‰²çš„AIæ¨¡å‹â€”â€”PV-S3ç³»ç»Ÿã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–äºå¤§é‡æ‰‹åŠ¨æ ‡ç­¾çš„è‡ªåŠ¨åŒ–ç¼ºé™·æ£€æµ‹æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥ç³»ç»Ÿä»…éœ€å°‘é‡çš„æ ‡ç­¾å›¾åƒå³å¯è¿›è¡Œè®­ç»ƒã€‚é€šè¿‡ä½¿ç”¨ä¸€ç§æ–°å‹çš„åŠäº¤å‰ç†µæŸå¤±å‡½æ•°æ¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼ŒPV-S3ç³»ç»Ÿåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…ˆå‰çš„æ–¹æ³•ã€‚åœ¨UCF-ELæ•°æ®é›†ä¸Šï¼Œä»…ä½¿ç”¨20%çš„æ ‡ç­¾æ ·æœ¬ï¼Œæ€§èƒ½æå‡æ˜¾è‘—ï¼Œä¸”æ ‡æ³¨æˆæœ¬é™ä½äº†80%ã€‚è¯¦æƒ…è¯·è®¿é—®æˆ‘ä»¬çš„GitHubä»“åº“æŸ¥çœ‹ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>PVç³»ç»Ÿè™½ç„¶å¯ä»¥å……åˆ†åˆ©ç”¨å¤ªé˜³èƒ½ï¼Œä½†éœ€è¦å®šæœŸç»´æŠ¤ä»¥ç¡®ä¿é«˜æ•ˆè¿è¡Œå¹¶é˜²æ­¢æ€§èƒ½ä¸‹é™ã€‚</li>
<li>ä¼ ç»Ÿçš„æ‰‹åŠ¨å¥åº·æ£€æŸ¥æ–¹æ³•æˆæœ¬é«˜æ˜‚ä¸”æ“ä½œå›°éš¾ï¼Œå› æ­¤è‡ªåŠ¨åŒ–ç¼ºé™·æ£€æµ‹è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰è‡ªåŠ¨åŒ–æ–¹æ³•è¿‡äºä¾èµ–æ‰‹åŠ¨ä¸“å®¶æ ‡ç­¾ï¼Œæ—¢è´¹æ—¶åˆå®¹æ˜“å‡ºé”™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºåŠç›‘ç£å­¦ä¹ çš„AIæ¨¡å‹PV-S3ï¼Œç”¨äºå¯¹ELå›¾åƒä¸­çš„ç¼ºé™·è¿›è¡Œè¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>PV-S3é€šè¿‡ä½¿ç”¨å°‘é‡æ ‡ç­¾å›¾åƒå’Œå¤§é‡æ— æ ‡ç­¾å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œé™ä½äº†å¯¹å¤§é‡æ‰‹åŠ¨æ ‡ç­¾çš„ä¾èµ–ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŠäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œä»¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.13693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-456cf75a13be9ba608555caca729c82a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41fe5b6d46d38197421d174602ff87db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0896125d7b110c2620fbe7368177728d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1190f6aef8cb30996ca7aa798e389679.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  Attributes Shape the Embedding Space of Face Recognition Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-43d037c500fcef7e128f86f6b3a711ed.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  Comparative Analysis of Vision Transformers and Traditional Deep   Learning Approaches for Automated Pneumonia Detection in Chest X-Rays
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25011.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
