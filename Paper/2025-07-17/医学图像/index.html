<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  U-RWKV Lightweight medical image segmentation with direction-adaptive   RWKV">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-13a01ce61ce047e83414930b4f9d2f89.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-17-æ›´æ–°"><a href="#2025-07-17-æ›´æ–°" class="headerlink" title="2025-07-17 æ›´æ–°"></a>2025-07-17 æ›´æ–°</h1><h2 id="U-RWKV-Lightweight-medical-image-segmentation-with-direction-adaptive-RWKV"><a href="#U-RWKV-Lightweight-medical-image-segmentation-with-direction-adaptive-RWKV" class="headerlink" title="U-RWKV: Lightweight medical image segmentation with direction-adaptive   RWKV"></a>U-RWKV: Lightweight medical image segmentation with direction-adaptive   RWKV</h2><p><strong>Authors:Hongbo Ye, Fenghe Tang, Peiang Zhao, Zhen Huang, Dexin Zhao, Minghao Bian, S. Kevin Zhou</strong></p>
<p>Achieving equity in healthcare accessibility requires lightweight yet high-performance solutions for medical image segmentation, particularly in resource-limited settings. Existing methods like U-Net and its variants often suffer from limited global Effective Receptive Fields (ERFs), hindering their ability to capture long-range dependencies. To address this, we propose U-RWKV, a novel framework leveraging the Recurrent Weighted Key-Value(RWKV) architecture, which achieves efficient long-range modeling at O(N) computational cost. The framework introduces two key innovations: the Direction-Adaptive RWKV Module(DARM) and the Stage-Adaptive Squeeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan mechanisms to aggregate contextual cues across images, mitigating directional bias while preserving global context and maintaining high computational efficiency. SASE dynamically adapts its architecture to different feature extraction stages, balancing high-resolution detail preservation and semantic relationship capture. Experiments demonstrate that U-RWKV achieves state-of-the-art segmentation performance with high computational efficiency, offering a practical solution for democratizing advanced medical imaging technologies in resource-constrained environments. The code is available at <a target="_blank" rel="noopener" href="https://github.com/hbyecoding/U-RWKV">https://github.com/hbyecoding/U-RWKV</a>. </p>
<blockquote>
<p>å®ç°åŒ»ç–—æŠ¤ç†å¯åŠæ€§çš„å…¬å¹³éœ€è¦é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²æå‡ºè½»ä¾¿è€Œé«˜æ€§èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚U-NetåŠå…¶å˜ä½“ï¼Œå¸¸å¸¸å—åˆ°å…¨å±€æœ‰æ•ˆæ„Ÿå—é‡ï¼ˆERFsï¼‰çš„é™åˆ¶ï¼Œé˜»ç¢å…¶æ•æ‰é•¿æœŸä¾èµ–å…³ç³»çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†U-RWKVï¼Œä¸€ä¸ªåˆ©ç”¨å¾ªç¯åŠ æƒé”®å€¼ï¼ˆRWKVï¼‰æ¶æ„çš„æ–°å‹æ¡†æ¶ï¼Œä»¥O(N)çš„è®¡ç®—æˆæœ¬å®ç°é«˜æ•ˆçš„é•¿ç¨‹å»ºæ¨¡ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šæ–¹å‘è‡ªé€‚åº”RWKVæ¨¡å—ï¼ˆDARMï¼‰å’Œé˜¶æ®µè‡ªé€‚åº”æŒ¤å‹-æ¿€å‘æ¨¡å—ï¼ˆSASEï¼‰ã€‚DARMé‡‡ç”¨åŒRWKVå’ŒQuadScanæœºåˆ¶æ¥èšåˆå›¾åƒé—´çš„ä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œå‡è½»æ–¹å‘åå·®ï¼Œä¿ç•™å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå¹¶ä¿æŒè¾ƒé«˜çš„è®¡ç®—æ•ˆç‡ã€‚SASEåŠ¨æ€åœ°é€‚åº”å…¶æ¶æ„åˆ°ä¸åŒçš„ç‰¹å¾æå–é˜¶æ®µï¼Œå¹³è¡¡é«˜åˆ†è¾¨ç‡ç»†èŠ‚ä¿ç•™å’Œè¯­ä¹‰å…³ç³»æ•æ‰ã€‚å®éªŒè¡¨æ˜ï¼ŒU-RWKVä»¥é«˜è®¡ç®—æ•ˆç‡å®ç°äº†æœ€å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ï¼Œä¸ºèµ„æºå—é™ç¯å¢ƒä¸­å…ˆè¿›åŒ»ç–—æˆåƒæŠ€æœ¯çš„æ™®åŠæä¾›äº†å®é™…è§£å†³æ–¹æ¡ˆã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/hbyecoding/U-RWKV%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/hbyecoding/U-RWKVè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11415v1">PDF</a> Accepted by MICCAI2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²æ˜¯å®ç°åŒ»ç–—èµ„æºå…¬å¹³åˆ†é…çš„å…³é”®ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ³•å¦‚U-Netç­‰åœ¨å¤„ç†å…¨çƒæœ‰æ•ˆæ„ŸçŸ¥åœºï¼ˆERFsï¼‰æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºU-RWKVæ¡†æ¶ï¼Œåˆ©ç”¨é€’å½’åŠ æƒé”®å€¼ï¼ˆRWKVï¼‰æ¶æ„å®ç°é«˜æ•ˆçš„é•¿ç¨‹å»ºæ¨¡ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬æ–¹å‘è‡ªé€‚åº”RWKVæ¨¡å—ï¼ˆDARMï¼‰å’Œé˜¶æ®µè‡ªé€‚åº”å‹ç¼©æ¿€å‘æ¨¡å—ï¼ˆSASEï¼‰ã€‚å®éªŒè¯æ˜ï¼ŒU-RWKVå®ç°äº†é«˜æ•ˆä¸”é«˜æ€§èƒ½çš„åˆ†å‰²æ•ˆæœï¼Œç‰¹åˆ«é€‚ç”¨äºèµ„æºå—é™ç¯å¢ƒä¸‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œæœ‰æœ›æ¨å¹¿å…ˆè¿›çš„åŒ»ç–—æˆåƒæŠ€æœ¯åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å®ç°åŒ»ç–—èµ„æºå…¬å¹³åˆ†é…éœ€é«˜æ•ˆä¸”é«˜æ€§èƒ½çš„åŒ»å­¦å›¾åƒåˆ†å‰²è§£å†³æ–¹æ¡ˆã€‚</li>
<li>U-NetåŠå…¶å˜ä½“å­˜åœ¨å…¨çƒæœ‰æ•ˆæ„ŸçŸ¥åœºï¼ˆERFsï¼‰çš„å±€é™æ€§ï¼Œå½±å“é•¿ç¨‹ä¾èµ–æ•æ‰èƒ½åŠ›ã€‚</li>
<li>æå‡ºU-RWKVæ¡†æ¶ï¼Œåˆ©ç”¨é€’å½’åŠ æƒé”®å€¼ï¼ˆRWKVï¼‰æ¶æ„è§£å†³è¯¥é—®é¢˜ã€‚</li>
<li>U-RWKVæ¡†æ¶åŒ…æ‹¬æ–¹å‘è‡ªé€‚åº”RWKVæ¨¡å—ï¼ˆDARMï¼‰å’Œé˜¶æ®µè‡ªé€‚åº”å‹ç¼©æ¿€å‘æ¨¡å—ï¼ˆSASEï¼‰ã€‚</li>
<li>DARMé‡‡ç”¨Dual-RWKVå’ŒQuadScanæœºåˆ¶æ¥èšåˆå›¾åƒä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œå‡å°‘æ–¹å‘åå·®å¹¶ç»´æŒé«˜æ•ˆè®¡ç®—ã€‚</li>
<li>SASEå¯åŠ¨æ€é€‚åº”ä¸åŒç‰¹å¾æå–é˜¶æ®µï¼Œå¹³è¡¡é«˜åˆ†è¾¨ç‡ç»†èŠ‚ä¿ç•™å’Œè¯­ä¹‰å…³ç³»æ•æ‰ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºU-RWKVå®ç°é«˜æ•ˆä¸”é«˜æ€§èƒ½çš„åˆ†å‰²æ•ˆæœï¼Œé€‚ç”¨äºèµ„æºå—é™ç¯å¢ƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11415">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5af8d5f8c6454fd8a9d2c7cbdfc8b68.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b70f0a7eb076120cc771a4845a1fcfd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5ac7d5fb9a71512b3283be2d62ba1198.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="A-highly-compact-and-ultra-fast-homogeneous-electromagnetic-calorimeter-based-on-oriented-lead-tungstate-crystals"><a href="#A-highly-compact-and-ultra-fast-homogeneous-electromagnetic-calorimeter-based-on-oriented-lead-tungstate-crystals" class="headerlink" title="A highly-compact and ultra-fast homogeneous electromagnetic calorimeter   based on oriented lead tungstate crystals"></a>A highly-compact and ultra-fast homogeneous electromagnetic calorimeter   based on oriented lead tungstate crystals</h2><p><strong>Authors:L. Bandiera, V. G. Baryshevsky, N. Canale, S. Carsi, S. Cutini, F. DavÃ¬, D. De Salvador, A. Gianoli, V. Guidi, V. Haurylavets, M. Korjik, A. S. Lobko, L. Malagutti, A. Mazzolari, L. Montalto, P. Monti Guarnieri, M. Moulson, R. Negrello, G. PaternÃ², M. Presti, D. Rinaldi, M. Romagnoni, A. Selmi, F. Sgarbossa, M. Soldani, A. Sytov, V. V. Tikhomirov, E. Vallazza</strong></p>
<p>Progress in high-energy physics has been closely tied to the development of highperformance electromagnetic calorimeters. Recent experiments have demonstrated the possibility to significantly accelerate the development of electromagnetic showers inside scintillating crystals typically used in homogeneous calorimeters based on scintillating crystals when the incident beam is aligned with a crystallographic axis to within a few mrad. In particular, a reduction of the radiation length has been measured when ultrarelativistic electron and photon beams were incident on a high-Z scintillator crystal along one of its main axes. Here, we propose the possibility to exploit this physical effect for the design of a new type of compact e.m. calorimeter, based on oriented ultrafast lead tungstate (PWO-UF) crystals, with a significant reduction in the depth needed to contain electromagnetic showers produced by high-energy particles with respect to the state-of-the-art. We report results from tests of the crystallographic quality of PWO-UF samples via high-resolution X-ray diffraction and photoelastic analysis. We then describe a proof-of-concept calorimeter geometry defined with a Geant4 model including the shower development in oriented crystals. Finally, we discuss the experimental techniques needed for the realization of a matrix of scintillator crystals oriented along a specific crystallographic direction. Since the angular acceptance for e.m. shower acceleration depends little on the particle energy, while the decrease of the shower length remains pronounced at very high energy, an oriented crystal calorimeter will open the way for applications at the maximum energies achievable in current and future experiments. Such applications span from forward calorimeters, to compact beam dumps for the search for light dark matter, to source-pointing space-borne {\gamma}-ray telescopes. </p>
<blockquote>
<p>é«˜èƒ½ç‰©ç†çš„è¿›å±•ä¸é«˜æ€§èƒ½ç”µç£é‡èƒ½å™¨çš„å‘å±•å¯†åˆ‡ç›¸å…³ã€‚æœ€è¿‘çš„å®éªŒè¡¨æ˜ï¼Œå½“å…¥å°„æŸä¸æ™¶ä½“å­¦è½´å¯¹é½åˆ°å‡ æ¯«å¼§åº¦ï¼ˆmradï¼‰æ—¶ï¼Œåœ¨é€šå¸¸ç”¨äºåŸºäºé—ªçƒæ™¶ä½“çš„å‡åŒ€é‡èƒ½å™¨ä¸­çš„é—ªçƒæ™¶ä½“å†…éƒ¨åŠ é€Ÿç”µç£ç°‡å°„çš„å‘å±•æ˜¯å¯èƒ½çš„ã€‚å°¤å…¶æ˜¯ï¼Œå½“ç›¸å¯¹è®ºè¶…é«˜é€Ÿç”µå­å’Œå…‰å­æŸæ²¿å…¶ä¸»è½´ä¹‹ä¸€å…¥å°„æ—¶ï¼Œå·²ç»æµ‹é‡åˆ°è¾å°„é•¿åº¦çš„å‡å°‘ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è¿™ä¸€ç‰©ç†æ•ˆåº”è®¾è®¡æ–°å‹ç´§å‡‘ç”µç£é‡èƒ½å™¨çš„å¯èƒ½æ€§ï¼Œè¯¥é‡èƒ½å™¨åŸºäºå®šå‘è¶…å¿«é’¨é…¸é“…ï¼ˆPWO-UFï¼‰æ™¶ä½“ã€‚ç›¸è¾ƒäºå½“å‰ä¸»æµæŠ€æœ¯ï¼Œå…¶æ·±åº¦å¤§å¤§å‡å°‘äº†ï¼Œèƒ½å¤Ÿå®¹çº³ç”±é«˜èƒ½ç²’å­äº§ç”Ÿçš„ç”µç£ç°‡å°„ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†é€šè¿‡é«˜åˆ†è¾¨ç‡Xå°„çº¿è¡å°„å’Œå…‰ç”µåˆ†ææµ‹è¯•PWO-UFæ ·å“æ™¶ä½“å­¦è´¨é‡çš„ç»“æœã€‚ç„¶åï¼Œæˆ‘ä»¬æè¿°äº†ä¸€ä¸ªæ¦‚å¿µéªŒè¯çš„é‡èƒ½å™¨å‡ ä½•ç»“æ„ï¼Œè¯¥ç»“æ„ç”±Geant4æ¨¡å‹å®šä¹‰ï¼ŒåŒ…æ‹¬å®šå‘æ™¶ä½“ä¸­çš„ç°‡å°„å‘å±•ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å®ç°æ²¿ç‰¹å®šæ™¶ä½“å­¦æ–¹å‘å®šå‘çš„é—ªçƒæ™¶ä½“çŸ©é˜µæ‰€éœ€çš„å®éªŒæŠ€æœ¯ã€‚ç”±äºç”µç£ç°‡å°„åŠ é€Ÿçš„è§’æ¥å—åº¦å‡ ä¹ä¸ä¾èµ–äºç²’å­èƒ½é‡ï¼Œè€Œç°‡å°„é•¿åº¦çš„å‡å°åœ¨é«˜èƒ½é‡æ—¶ä»ç„¶å¾ˆæ˜æ˜¾ï¼Œå› æ­¤å®šå‘æ™¶ä½“é‡èƒ½å™¨å°†ä¸ºå½“å‰å’Œæœªæ¥å®éªŒå¯è¾¾åˆ°çš„æœ€å¤§èƒ½é‡ä¸‹çš„åº”ç”¨å¼€è¾Ÿé“è·¯ã€‚è¿™äº›åº”ç”¨åŒ…æ‹¬æ­£å‘é‡èƒ½å™¨ã€ç”¨äºæœç´¢è½»æš—ç‰©è´¨çš„ç´§å‡‘æŸæµé˜»å°¼å™¨ä»¥åŠæºæŒ‡å‘å¼å¤ªç©ºÎ³å°„çº¿æœ›è¿œé•œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11332v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†é«˜æ€§èƒ½ç”µç£é‡çƒ­è®¡åœ¨é«˜èƒ½ç‰©ç†ç ”ç©¶ä¸­çš„é‡è¦æ€§ï¼Œæå‡ºåˆ©ç”¨å®šå‘è¶…å¿«é“…é’¨é…¸ç›ï¼ˆPWO-UFï¼‰æ™¶ä½“è®¾è®¡æ–°å‹ç´§å‡‘ç”µç£é‡çƒ­è®¡çš„æ„æƒ³ã€‚ç ”ç©¶è¡¨æ˜ï¼Œå½“ç²’å­æŸä¸æ™¶ä½“å­¦è½´å¯¹é½æ—¶ï¼Œå¯ä»¥åŠ é€Ÿç”µç£ç°‡çš„å‘å±•ï¼Œä»è€Œæ˜¾è‘—é™ä½æ‰€éœ€çš„æ™¶ä½“æ·±åº¦ã€‚åŒæ—¶è®¨è®ºäº†å®ç°å®šå‘æ™¶ä½“çŸ©é˜µçš„å®éªŒæŠ€æœ¯ã€‚å®šå‘æ™¶ä½“é‡çƒ­è®¡çš„åº”ç”¨å°†æ¶µç›–å½“å‰å’Œæœªæ¥å®éªŒçš„æœ€å¤§èƒ½é‡èŒƒå›´ï¼ŒåŒ…æ‹¬å‰å‘é‡çƒ­è®¡ã€ç´§å‡‘å…‰æŸè½¬å‚¨å¯»æ‰¾è½»æš—ç‰©è´¨ä»¥åŠæºæŒ‡å‘ç©ºé—´Î³å°„çº¿æœ›è¿œé•œç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜æ€§èƒ½ç”µç£é‡çƒ­è®¡åœ¨é«˜èƒ½ç‰©ç†ç ”ç©¶ä¸­çš„ä½œç”¨çªå‡ºã€‚</li>
<li>é€šè¿‡å®šå‘è¶…å¿«é“…é’¨é…¸ç›ï¼ˆPWO-UFï¼‰æ™¶ä½“è®¾è®¡æ–°å‹ç´§å‡‘ç”µç£é‡çƒ­è®¡çš„æ„æƒ³è¢«æå‡ºã€‚</li>
<li>å½“ç²’å­æŸä¸æ™¶ä½“å­¦è½´å¯¹é½æ—¶ï¼Œå¯ä»¥åŠ é€Ÿç”µç£ç°‡çš„å‘å±•ã€‚</li>
<li>å®šå‘æ™¶ä½“é‡çƒ­è®¡æ‰€éœ€çš„æ™¶ä½“æ·±åº¦æ˜¾è‘—é™ä½ã€‚</li>
<li>å®ç°å®šå‘æ™¶ä½“çŸ©é˜µçš„å®éªŒæŠ€æœ¯è¢«è®¨è®ºã€‚</li>
<li>å®šå‘æ™¶ä½“é‡çƒ­è®¡çš„åº”ç”¨èŒƒå›´å¹¿æ³›ï¼ŒåŒ…æ‹¬å‰å‘é‡çƒ­è®¡ã€å¯»æ‰¾è½»æš—ç‰©è´¨çš„ç´§å‡‘å…‰æŸè½¬å‚¨ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11332">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cea7149511b5672e679060763ea93942.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f90e025f0a5aa2b03989c5ad82f10aa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4987b11a58f50cd9c54ffa48a786f22c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6d643551c2ddd51f2ae778f5e2ea07e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-132004d087e21a264c40a7b05a81fe84.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51013a45eacd76a893ed443f17b7230d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3476877dd1f1d4c48bdac318f1f3ffea.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="An-Explainable-AI-Enhanced-Machine-Learning-Approach-for-Cardiovascular-Disease-Detection-and-Risk-Assessment"><a href="#An-Explainable-AI-Enhanced-Machine-Learning-Approach-for-Cardiovascular-Disease-Detection-and-Risk-Assessment" class="headerlink" title="An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular   Disease Detection and Risk Assessment"></a>An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular   Disease Detection and Risk Assessment</h2><p><strong>Authors:Md. Emon Akter Sourov, Md. Sabbir Hossen, Pabon Shaha, Mohammad Minoar Hossain, Md Sadiq Iqbal</strong></p>
<p>Heart disease remains a major global health concern, particularly in regions with limited access to medical resources and diagnostic facilities. Traditional diagnostic methods often fail to accurately identify and manage heart disease risks, leading to adverse outcomes. Machine learning has the potential to significantly enhance the accuracy, efficiency, and speed of heart disease diagnosis. In this study, we proposed a comprehensive framework that combines classification models for heart disease detection and regression models for risk prediction. We employed the Heart Disease dataset, which comprises 1,035 cases. To address the issue of class imbalance, the Synthetic Minority Oversampling Technique (SMOTE) was applied, resulting in the generation of an additional 100,000 synthetic data points. Performance metrics, including accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to evaluate the modelâ€™s effectiveness. Among the classification models, Random Forest emerged as the standout performer, achieving an accuracy of 97.2% on real data and 97.6% on synthetic data. For regression tasks, Linear Regression demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic datasets, respectively, with the lowest error metrics. Additionally, Explainable AI techniques were employed to enhance the interpretability of the models. This study highlights the potential of machine learning to revolutionize heart disease diagnosis and risk prediction, thereby facilitating early intervention and enhancing clinical decision-making. </p>
<blockquote>
<p>å¿ƒè„ç—…ä»ç„¶æ˜¯ä¸€ä¸ªå…¨çƒæ€§çš„é‡å¤§å¥åº·é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—èµ„æºæœ‰é™å’Œè¯Šæ–­è®¾æ–½ä¸è¶³çš„åœ°åŒºã€‚ä¼ ç»Ÿçš„è¯Šæ–­æ–¹æ³•å¾€å¾€æ— æ³•å‡†ç¡®è¯†åˆ«å’Œç®¡ç†å¿ƒè„ç—…é£é™©ï¼Œå¯¼è‡´ä¸è‰¯åæœã€‚æœºå™¨å­¦ä¹ æœ‰æ½œåŠ›æ˜¾è‘—æé«˜å¿ƒè„ç—…è¯Šæ–­çš„å‡†ç¡®æ€§ã€æ•ˆç‡å’Œé€Ÿåº¦ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç”¨äºå¿ƒè„ç—…æ£€æµ‹çš„åˆ†ç±»æ¨¡å‹å’Œç”¨äºé£é™©é¢„æµ‹çš„å›å½’æ¨¡å‹ã€‚æˆ‘ä»¬é‡‡ç”¨äº†å¿ƒè„ç—…æ•°æ®é›†ï¼ŒåŒ…å«1035ä¸ªç—…ä¾‹ã€‚ä¸ºäº†è§£å†³ç±»åˆ«ä¸å¹³è¡¡çš„é—®é¢˜ï¼Œé‡‡ç”¨äº†åˆæˆå°‘æ•°è¿‡é‡‡æ ·æŠ€æœ¯ï¼ˆSMOTEï¼‰ï¼Œç”Ÿæˆäº†é¢å¤–çš„10ä¸‡ä¸ªåˆæˆæ•°æ®ç‚¹ã€‚æˆ‘ä»¬ä½¿ç”¨æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å‡†ç¡®æ€§ã€ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°ã€Rå¹³æ–¹ã€MSEã€RMSEå’ŒMAEæ¥è¯„ä¼°æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚åœ¨åˆ†ç±»æ¨¡å‹ä¸­ï¼Œéšæœºæ£®æ—è¡¨ç°å‡ºè‰²ï¼Œåœ¨çœŸå®æ•°æ®ä¸Šè¾¾åˆ°äº†97.2%çš„å‡†ç¡®ç‡ï¼Œåœ¨åˆæˆæ•°æ®ä¸Šè¾¾åˆ°äº†97.6%çš„å‡†ç¡®ç‡ã€‚å¯¹äºå›å½’ä»»åŠ¡ï¼Œçº¿æ€§å›å½’åœ¨çœŸå®å’Œåˆæˆæ•°æ®é›†ä¸Šåˆ†åˆ«è·å¾—äº†æœ€é«˜çš„Rå¹³æ–¹å€¼0.992å’Œ0.984ï¼Œå¹¶ä¸”å…·æœ‰æœ€ä½çš„è¯¯å·®æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†å¯è§£é‡Šçš„AIæŠ€æœ¯æ¥æé«˜æ¨¡å‹çš„è§£é‡Šæ€§ã€‚è¿™é¡¹ç ”ç©¶çªå‡ºäº†æœºå™¨å­¦ä¹ åœ¨å¿ƒè„ç—…è¯Šæ–­å’Œæ²»ç–—ä¸­çš„æ½œåŠ›ï¼Œæœ‰åŠ©äºè¿›è¡Œæ—©æœŸå¹²é¢„å¹¶å¢å¼ºä¸´åºŠå†³ç­–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11185v1">PDF</a> This paper has been accepted at the IEEE QPAIN 2025. The final   version will be available in the IEEE Xplore Digital Library</p>
<p><strong>Summary</strong><br>     è¯¥ç ”ç©¶åˆ©ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œæå‡ºä¸€ä¸ªç»¼åˆæ¡†æ¶ç”¨äºå¿ƒè„ç–¾ç—…çš„æ£€æµ‹å’Œé£é™©é¢„æµ‹ã€‚é€šè¿‡åˆæˆå°‘æ•°ç¾¤ä½“è¿‡é‡‡æ ·æŠ€æœ¯ï¼ˆSMOTEï¼‰è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå¹¶ç”Ÿæˆå¤§é‡åˆæˆæ•°æ®ç‚¹ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œéšæœºæ£®æ—åˆ†ç±»æ¨¡å‹åœ¨çœŸå®å’Œåˆæˆæ•°æ®ä¸Šçš„å‡†ç¡®ç‡åˆ†åˆ«è¾¾97.2%å’Œ97.6%ï¼Œçº¿æ€§å›å½’æ¨¡å‹åœ¨å›å½’ä»»åŠ¡ä¸­å…·æœ‰é«˜R2å€¼å’Œä½è¯¯å·®æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜åˆ©ç”¨å¯è§£é‡ŠAIæŠ€æœ¯æé«˜æ¨¡å‹çš„è§£é‡Šæ€§ï¼Œæœ‰æœ›ä¸ºå¿ƒè„ç–¾ç—…çš„æ—©æœŸè¯Šæ–­å’Œé¢„æµ‹æä¾›é©å‘½æ€§å¸®åŠ©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¿ƒè„ç–¾ç—…ä»ç„¶æ˜¯ä¸€ä¸ªå…¨çƒæ€§çš„å¥åº·é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—èµ„æºè¯Šæ–­è®¾æ–½æœ‰é™çš„åœ°åŒºã€‚</li>
<li>ä¼ ç»Ÿè¯Šæ–­æ–¹æ³•åœ¨å‡†ç¡®è¯†åˆ«å’Œç®¡ç†å·¥ä½œå¿ƒè„ç–¾ç—…é£é™©æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æœºå™¨å­¦ä¹ æŠ€æœ¯åœ¨å¿ƒè„ç–¾ç—…è¯Šæ–­å’Œæ²»ç–—ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯ä»¥æé«˜å‡†ç¡®æ€§ã€æ•ˆç‡å’Œé€Ÿåº¦ã€‚</li>
<li>ç ”ç©¶ä¸­æå‡ºäº†ä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼ŒåŒ…æ‹¬åˆ†ç±»æ¨¡å‹ç”¨äºå¿ƒè„ç–¾ç—…æ£€æµ‹ï¼Œå›å½’æ¨¡å‹ç”¨äºé£é™©é¢„æµ‹ã€‚</li>
<li>é‡‡ç”¨åˆæˆå°‘æ•°ç¾¤ä½“è¿‡é‡‡æ ·æŠ€æœ¯ï¼ˆSMOTEï¼‰è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œç”Ÿæˆå¤§é‡åˆæˆæ•°æ®ç‚¹ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>éšæœºæ£®æ—åˆ†ç±»æ¨¡å‹è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œåœ¨çœŸå®å’Œåˆæˆæ•°æ®ä¸Šçš„å‡†ç¡®ç‡å‡è¶…è¿‡97%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11185">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9d2c88a63be38dd4523d1c240e29f759.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20ec969c00f5bd8b2a8eb58e7ceba03b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-203ebaee92f7f7834b0e4e074577d516.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f141d83e290502c325b99789be213c00.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c20615d0cc8c3412b11197be9c7b8df0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e11470492898a62d47900979e03a89fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b14e777e2431b0af076d994f70fb8a5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8835e0bb851d1af9cb397853a60d9dd5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e4b734f3698b138eebd5496d82a5063.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Latent-Space-Consistency-for-Sparse-View-CT-Reconstruction"><a href="#Latent-Space-Consistency-for-Sparse-View-CT-Reconstruction" class="headerlink" title="Latent Space Consistency for Sparse-View CT Reconstruction"></a>Latent Space Consistency for Sparse-View CT Reconstruction</h2><p><strong>Authors:Duoyou Chen, Yunqing Chen, Can Zhang, Zhou Wang, Cheng Chen, Ruoxiu Xiao</strong></p>
<p>Computed Tomography (CT) is a widely utilized imaging modality in clinical settings. Using densely acquired rotational X-ray arrays, CT can capture 3D spatial features. However, it is confronted with challenged such as significant time consumption and high radiation exposure. CT reconstruction methods based on sparse-view X-ray images have garnered substantial attention from researchers as they present a means to mitigate costs and risks. In recent years, diffusion models, particularly the Latent Diffusion Model (LDM), have demonstrated promising potential in the domain of 3D CT reconstruction. Nonetheless, due to the substantial differences between the 2D latent representation of X-ray modalities and the 3D latent representation of CT modalities, the vanilla LDM is incapable of achieving effective alignment within the latent space. To address this issue, we propose the Consistent Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature contrastive learning to efficiently extract latent 3D information from 2D X-ray images and achieve latent space alignment between modalities. Experimental results indicate that CLS-DM outperforms classical and state-of-the-art generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing the effectiveness and economic viability of sparse X-ray reconstructed CT but can also be generalized to other cross-modal transformation tasks, such as text-to-image synthesis. We have made our code publicly available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CLS-DM-50D6/">https://anonymous.4open.science/r/CLS-DM-50D6/</a> to facilitate further research and applications in other domains. </p>
<blockquote>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ˜¯ä¸´åºŠç¯å¢ƒä¸­å¹¿æ³›ä½¿ç”¨çš„æˆåƒæŠ€æœ¯ã€‚é€šè¿‡å¯†é›†è·å–çš„æ—‹è½¬Xå°„çº¿é˜µåˆ—ï¼ŒCTå¯ä»¥æ•æ‰ä¸‰ç»´ç©ºé—´ç‰¹å¾ã€‚ç„¶è€Œï¼Œå®ƒé¢ä¸´ç€æ—¶é—´æ¶ˆè€—å¤§ã€è¾å°„æš´éœ²é«˜ç­‰æŒ‘æˆ˜ã€‚åŸºäºç¨€ç–è§†è§’Xå°„çº¿å›¾åƒçš„CTé‡å»ºæ–¹æ³•å¼•èµ·äº†ç ”ç©¶äººå‘˜çš„å¹¿æ³›å…³æ³¨ï¼Œå› ä¸ºå®ƒä»¬æä¾›äº†ä¸€ç§é™ä½æˆæœ¬å’Œé£é™©çš„æ–¹æ³•ã€‚è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨ä¸‰ç»´CTé‡å»ºé¢†åŸŸæ˜¾ç¤ºå‡ºæœ‰å‰æ™¯çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºXå°„çº¿æ¨¡æ€çš„äºŒç»´æ½œåœ¨è¡¨ç¤ºå’ŒCTæ¨¡æ€çš„ä¸‰ç»´æ½œåœ¨è¡¨ç¤ºä¹‹é—´å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼ŒåŸå§‹çš„LDMæ— æ³•åœ¨æ½œåœ¨ç©ºé—´å†…è¿›è¡Œæœ‰æ•ˆå¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´æ‰©æ•£æ¨¡å‹ï¼ˆCLS-DMï¼‰ï¼Œå®ƒç»“åˆäº†è·¨æ¨¡æ€ç‰¹å¾å¯¹æ¯”å­¦ä¹ ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä»äºŒç»´Xå°„çº¿å›¾åƒä¸­æå–æ½œåœ¨çš„ä¸‰ç»´ä¿¡æ¯ï¼Œå¹¶åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´å®ç°æ½œåœ¨ç©ºé—´å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLS-DMåœ¨LIDC-IDRIå’ŒCTSpine1Kæ•°æ®é›†ä¸Šï¼Œåœ¨æ ‡å‡†çš„ä½“ç´ çº§æŒ‡æ ‡ï¼ˆPSNRï¼ŒSSIMï¼‰æ–¹é¢ä¼˜äºç»å…¸å’Œæœ€æ–°ä¸€ä»£ç”Ÿæˆæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æœ‰åŠ©äºæé«˜ç¨€ç–Xå°„çº¿é‡å»ºCTçš„æœ‰æ•ˆæ€§å’Œç»æµå¯è¡Œæ€§ï¼Œè¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–è·¨æ¨¡æ€è½¬æ¢ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CLS-DM-50D6/%EF%BC%8C%E4%BB%A5%E4%BE%BF%E5%85%B6%E4%BB%96%E5%9C%B0%E7%9B%BE%E7%9A%84%E7%A0%94%E7%A9%B6%E5%92%8C%E5%BA%94%E7%94%A8%E3%80%82">https://anonymous.4open.science/r/CLS-DM-50D6/ï¼Œä»¥ä¾¿å…¶ä»–é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11152v1">PDF</a> ACMMM2025 Accepted</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è®¡ç®—å±‚ææˆåƒï¼ˆCTï¼‰çš„å¹¿æ³›åº”ç”¨åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚æ—¶é—´æ¶ˆè€—å’Œè¾å°„æš´éœ²é—®é¢˜ã€‚ç ”ç©¶è€…æå‡ºåŸºäºç¨€ç–è§†å›¾Xå°„çº¿å›¾åƒçš„CTé‡å»ºæ–¹æ³•ï¼Œä»¥é™ä½æˆæœ¬å’Œé£é™©ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ‰©æ•£æ¨¡å‹â€”â€”ä¸€è‡´æ½œåœ¨ç©ºé—´æ‰©æ•£æ¨¡å‹ï¼ˆCLS-DMï¼‰ï¼Œè¯¥æ¨¡å‹é€šè¿‡è·¨æ¨¡æ€ç‰¹å¾å¯¹æ¯”å­¦ä¹ æœ‰æ•ˆåœ°ä»äºŒç»´Xå°„çº¿å›¾åƒä¸­æå–æ½œåœ¨çš„ä¸‰ç»´ä¿¡æ¯ï¼Œå¹¶åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´å®ç°æ½œåœ¨ç©ºé—´å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLS-DMåœ¨LIDC-IDRIå’ŒCTSpine1Kæ•°æ®é›†ä¸Šä¼˜äºç»å…¸å’Œå…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ï¼Œå¯¹äºç¨€ç–Xå°„çº¿é‡å»ºCTçš„æœ‰æ•ˆæ€§å’Œç»æµæ€§æœ‰æ‰€æå‡ï¼Œå¹¶å¯æ¨å¹¿åˆ°å…¶ä»–è·¨æ¨¡æ€è½¬æ¢ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CTæ˜¯ä¸´åºŠè®¾ç½®ä¸­å¹¿æ³›ä½¿ç”¨çš„æˆåƒæ–¹å¼ï¼Œèƒ½æ•æ‰ä¸‰ç»´ç©ºé—´ç‰¹å¾ï¼Œä½†é¢ä¸´æ—¶é—´æ¶ˆè€—å’Œè¾å°„æš´éœ²çš„æŒ‘æˆ˜ã€‚</li>
<li>åŸºäºç¨€ç–è§†å›¾Xå°„çº¿å›¾åƒçš„CTé‡å»ºæ–¹æ³•å—åˆ°å…³æ³¨ï¼Œä»¥é™ä½æˆæœ¬å’Œé£é™©ã€‚</li>
<li>ä¸€è‡´æ½œåœ¨ç©ºé—´æ‰©æ•£æ¨¡å‹ï¼ˆCLS-DMï¼‰é€šè¿‡è·¨æ¨¡æ€ç‰¹å¾å¯¹æ¯”å­¦ä¹ ä»äºŒç»´Xå°„çº¿å›¾åƒä¸­æå–æ½œåœ¨ä¸‰ç»´ä¿¡æ¯ã€‚</li>
<li>CLS-DMå®ç°äº†åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´çš„æ½œåœ¨ç©ºé—´å¯¹é½ã€‚</li>
<li>CLS-DMåœ¨LIDC-IDRIå’ŒCTSpine1Kæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>CLS-DMæå‡äº†ç¨€ç–Xå°„çº¿é‡å»ºCTçš„æœ‰æ•ˆæ€§å’Œç»æµæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8cc54aad21847a66cc556e599301048c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c2830682701469a6155538e05714611.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6fafce0fe28d73beb76915750c6b179.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bf51a6a8bcdd6772d7272543cebc6e6d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Disk-Instability-Model-for-Quasi-Periodic-Eruptions-Investigating-Period-Dispersion-and-Peak-Temperature"><a href="#Disk-Instability-Model-for-Quasi-Periodic-Eruptions-Investigating-Period-Dispersion-and-Peak-Temperature" class="headerlink" title="Disk Instability Model for Quasi-Periodic Eruptions: Investigating   Period Dispersion and Peak Temperature"></a>Disk Instability Model for Quasi-Periodic Eruptions: Investigating   Period Dispersion and Peak Temperature</h2><p><strong>Authors:Xin Pan, Shuang-Liang Li, Xinwu Cao, Bifang Liu, Weimin Yuan</strong></p>
<p>Quasi-periodic eruptions (QPEs) are a class of X-ray repeating burst phenomena discovered in recent years. Many models have been proposed to study this phenomenon, there remains significant debate regarding the physical origin of QPEs. In our previous work, we developed a disk instability model with a large-scale magnetic field and successfully reproduced the light curves and spectral characteristics of several QPE sources. We further investigate the model in this work, aiming to explain two key observational features: the dispersion in eruption periods and the peak temperatures during eruptions. The model reveals critical thresholds ($\dot{M}<em>{\rm crit}$, $\beta</em>{1,\rm crit}$) that separate systems into stable regimes with minimal period variations and unstable regimes where periods are highly sensitive to accretion rate and magnetic field parameter, while peak temperatures remain nearly constant across the parameter space. This framework successfully explains both the regular eruptions observed in sources like GSN 069 and the stochastic behavior in sources like eRO-QPE1, and simultaneously accounting for the observed temperature stability during long-term QPEs evolution. </p>
<blockquote>
<p>å‡†å‘¨æœŸå–·å‘ï¼ˆQPEsï¼‰æ˜¯è¿‘å¹´æ¥å‘ç°çš„ä¸€ç±»Xå°„çº¿é‡å¤çˆ†å‘ç°è±¡ã€‚å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šæ¨¡å‹æ¥ç ”ç©¶è¿™ä¸€ç°è±¡ï¼Œä½†å…³äºQPEsçš„ç‰©ç†èµ·æºä»å­˜åœ¨é‡å¤§äº‰è®®ã€‚åœ¨æˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå…·æœ‰å¤§è§„æ¨¡ç£åœºçš„ç£ç›˜ä¸ç¨³å®šæ¨¡å‹ï¼Œå¹¶æˆåŠŸå†ç°äº†å¤šä¸ªQPEæºçš„å…‰å˜æ›²çº¿å’Œå…‰è°±ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨è¿™é¡¹å·¥ä½œä¸­è¿›ä¸€æ­¥ç ”ç©¶äº†è¯¥æ¨¡å‹ï¼Œæ—¨åœ¨è§£é‡Šä¸¤ä¸ªå…³é”®çš„è§‚æµ‹ç‰¹å¾ï¼šå–·å‘å‘¨æœŸçš„åˆ†æ•£å’Œå–·å‘æœŸé—´çš„å³°å€¼æ¸©åº¦ã€‚æ¨¡å‹æ­ç¤ºäº†å…³é”®çš„é˜ˆå€¼ï¼ˆ$\dot{M}<em>{\rm crit}$ï¼Œ$\beta</em>{1,\rm crit}$ï¼‰ï¼Œè¿™äº›é˜ˆå€¼å°†ç³»ç»Ÿåˆ’åˆ†ä¸ºå…·æœ‰æœ€å°å‘¨æœŸå˜åŒ–çš„ç¨³å®šçŠ¶æ€å’Œé«˜åº¦æ•æ„Ÿäºå¸ç§¯ç‡å’Œç£åœºå‚æ•°çš„ä¸ç¨³å®šçŠ¶æ€ï¼Œè€Œå³°å€¼æ¸©åº¦åœ¨å‚æ•°ç©ºé—´å†…å‡ ä¹ä¿æŒä¸å˜ã€‚è¿™ä¸€æ¡†æ¶æˆåŠŸè§£é‡Šäº†å¦‚GSN 069ç­‰æºè§‚å¯Ÿåˆ°çš„å®šæœŸå–·å‘å’Œå¦‚eRO-QPE1ç­‰æºçš„éšæœºè¡Œä¸ºï¼ŒåŒæ—¶è¯´æ˜äº†åœ¨é•¿æœŸQPEsæ¼”åŒ–è¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°çš„æ¸©åº¦ç¨³å®šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11100v1">PDF</a> 11 pages, 9 figures, Accepted for publication in the Astrophysical   Journal</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå‘ç°çš„ä¸€ç±»Xå°„çº¿é‡å¤çˆ†å‘ç°è±¡ç§°ä¸ºå‡†å‘¨æœŸçˆ†å‘ï¼ˆQPEsï¼‰ã€‚å°½ç®¡æœ‰è®¸å¤šæ¨¡å‹å¯¹å…¶è¿›è¡Œç ”ç©¶ï¼Œä½†å…³äºå…¶ç‰©ç†èµ·æºä»å­˜åœ¨äº‰è®®ã€‚æˆ‘ä»¬ä¹‹å‰çš„å·¥ä½œå¼€å‘äº†ä¸€ä¸ªå…·æœ‰å¤§å°ºåº¦ç£åœºçš„ç£ç›˜ä¸ç¨³å®šæ¨¡å‹ï¼Œå¹¶æˆåŠŸå†ç°äº†å¤šä¸ªQPEæºçš„å…‰å˜æ›²çº¿å’Œå…‰è°±ç‰¹å¾ã€‚æœ¬æ¬¡å·¥ä½œè¿›ä¸€æ­¥æ¢è®¨è¯¥æ¨¡å‹ï¼Œæ—¨åœ¨è§£é‡Šä¸¤ä¸ªå…³é”®è§‚æµ‹ç‰¹å¾ï¼šçˆ†å‘å‘¨æœŸçš„åˆ†æ•£å’Œçˆ†å‘æœŸé—´çš„å³°å€¼æ¸©åº¦ã€‚æ¨¡å‹æ­ç¤ºäº†å°†ç³»ç»Ÿåˆ†ä¸ºæœ€å°å‘¨æœŸå˜åŒ–ç¨³å®šçš„åˆ¶åº¦åŒºåŸŸå’Œé«˜åº¦æ•æ„Ÿäºå¢è´¨ç‡å’Œç£åœºå‚æ•°çš„ä¸ç¨³å®šåŒºåŸŸçš„ä¸´ç•Œé˜ˆå€¼ï¼ˆ$\dot{M}<em>{\rm crit}$ï¼Œ$\beta</em>{1,\rm crit}$ï¼‰ã€‚åŒæ—¶ï¼Œå³°å€¼æ¸©åº¦åœ¨å‚æ•°ç©ºé—´ä¸­ä¿æŒè¿‘ä¼¼æ’å®šã€‚è¯¥æ¡†æ¶æˆåŠŸè§£é‡Šäº†å¦‚GSN 069ç­‰æ¥æºçš„å¸¸è§„çˆ†å‘å’Œå¦‚eRO-QPE1ç­‰æ¥æºçš„éšæœºè¡Œä¸ºï¼ŒåŒæ—¶è§£é‡Šäº†é•¿æœŸQPEæ¼”åŒ–æœŸé—´è§‚å¯Ÿåˆ°çš„æ¸©åº¦ç¨³å®šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>QPEsæ˜¯ä¸€ç§Xå°„çº¿é‡å¤çˆ†å‘ç°è±¡ï¼Œå¯¹å…¶ç‰©ç†èµ·æºå­˜åœ¨äº‰è®®ã€‚</li>
<li>æ­¤å‰çš„å·¥ä½œæå‡ºäº†ä¸€ä¸ªå…·æœ‰å¤§å°ºåº¦ç£åœºçš„ç£ç›˜ä¸ç¨³å®šæ¨¡å‹ï¼ŒæˆåŠŸæ¨¡æ‹Ÿäº†QPEçš„å…‰å˜æ›²çº¿å’Œå…‰è°±ç‰¹å¾ã€‚</li>
<li>æœ¬æ¬¡ç ”ç©¶è¿›ä¸€æ­¥æ¢è®¨äº†è¯¥æ¨¡å‹ï¼Œæ—¨åœ¨è§£é‡ŠQPEçš„ä¸¤ä¸ªå…³é”®è§‚æµ‹ç‰¹å¾ï¼šçˆ†å‘å‘¨æœŸçš„åˆ†æ•£å’Œå³°å€¼æ¸©åº¦çš„å˜åŒ–ã€‚</li>
<li>æ¨¡å‹æ­ç¤ºäº†ä¸´ç•Œé˜ˆå€¼ï¼ˆ$\dot{M}<em>{\rm crit}$ï¼Œ$\beta</em>{1,\rm crit}$ï¼‰ï¼Œç”¨äºåŒºåˆ†ç³»ç»Ÿçš„ç¨³å®šå’Œä¸ç¨³å®šçš„åˆ¶åº¦åŒºåŸŸã€‚</li>
<li>çˆ†å‘å‘¨æœŸçš„ä¸ç¨³å®šæ€§ä¸å¢è´¨ç‡å’Œç£åœºå‚æ•°çš„æ•æ„Ÿæ€§æœ‰å…³ã€‚</li>
<li>å³°å€¼æ¸©åº¦åœ¨å‚æ•°ç©ºé—´å†…ä¿æŒç›¸å¯¹ç¨³å®šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11100">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-897e72fdfad1d5035c168beccccc3308.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fbeb2cd9fbd75d211656bf9b11ea5edd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c665e35e1ccda28d8d0cb23a11ae70d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-437e434b8502a7a2b56791ac8a983723.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Alleviating-Textual-Reliance-in-Medical-Language-guided-Segmentation-via-Prototype-driven-Semantic-Approximation"><a href="#Alleviating-Textual-Reliance-in-Medical-Language-guided-Segmentation-via-Prototype-driven-Semantic-Approximation" class="headerlink" title="Alleviating Textual Reliance in Medical Language-guided Segmentation via   Prototype-driven Semantic Approximation"></a>Alleviating Textual Reliance in Medical Language-guided Segmentation via   Prototype-driven Semantic Approximation</h2><p><strong>Authors:Shuchang Ye, Usman Naseem, Mingyuan Meng, Jinman Kim</strong></p>
<p>Medical language-guided segmentation, integrating textual clinical reports as auxiliary guidance to enhance image segmentation, has demonstrated significant improvements over unimodal approaches. However, its inherent reliance on paired image-text input, which we refer to as &#96;&#96;textual relianceâ€, presents two fundamental limitations: 1) many medical segmentation datasets lack paired reports, leaving a substantial portion of image-only data underutilized for training; and 2) inference is limited to retrospective analysis of cases with paired reports, limiting its applicability in most clinical scenarios where segmentation typically precedes reporting. To address these limitations, we propose ProLearn, the first Prototype-driven Learning framework for language-guided segmentation that fundamentally alleviates textual reliance. At its core, in ProLearn, we introduce a novel Prototype-driven Semantic Approximation (PSA) module to enable approximation of semantic guidance from textual input. PSA initializes a discrete and compact prototype space by distilling segmentation-relevant semantics from textual reports. Once initialized, it supports a query-and-respond mechanism which approximates semantic guidance for images without textual input, thereby alleviating textual reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG demonstrate that ProLearn outperforms state-of-the-art language-guided methods when limited text is available. </p>
<blockquote>
<p>åŒ»å­¦è¯­è¨€å¼•å¯¼åˆ†å‰²æŠ€æœ¯é€šè¿‡å°†æ–‡æœ¬ä¸´åºŠæŠ¥å‘Šä½œä¸ºè¾…åŠ©æŒ‡å¯¼æ¥å¢å¼ºå›¾åƒåˆ†å‰²ï¼Œè¯æ˜å…¶ç›¸å¯¹äºå•æ¨¡æ€æ–¹æ³•çš„æ˜¾è‘—æ”¹è¿›ã€‚ç„¶è€Œï¼Œå…¶å¯¹é…å¯¹å›¾åƒæ–‡æœ¬è¾“å…¥çš„å›ºæœ‰ä¾èµ–ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ–‡æœ¬ä¾èµ–â€ï¼‰å­˜åœ¨ä¸¤ä¸ªåŸºæœ¬å±€é™ï¼š1ï¼‰è®¸å¤šåŒ»å­¦åˆ†å‰²æ•°æ®é›†ç¼ºå°‘é…å¯¹æŠ¥å‘Šï¼Œå¯¼è‡´å¤§é‡ä»…åŒ…å«å›¾åƒçš„æ•°æ®æœªèƒ½å¾—åˆ°å……åˆ†åˆ©ç”¨ï¼›2ï¼‰æ¨ç†ä»…é™äºå…·æœ‰é…å¯¹æŠ¥å‘Šçš„ç—…ä¾‹çš„å›é¡¾æ€§åˆ†æï¼Œé™åˆ¶äº†å…¶åœ¨å¤§å¤šæ•°ä¸´åºŠåœºæ™¯ä¸­çš„åº”ç”¨ï¼Œå› ä¸ºåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œåˆ†å‰²æ˜¯åœ¨æŠ¥å‘Šä¹‹å‰è¿›è¡Œçš„ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºäº†ProLearnï¼Œå³é¦–ä¸ªç”¨äºè¯­è¨€å¼•å¯¼åˆ†å‰²çš„åŸå‹é©±åŠ¨å­¦ä¹ æ¡†æ¶ï¼Œä»æ ¹æœ¬ä¸Šå‡è½»äº†æ–‡æœ¬ä¾èµ–ã€‚åœ¨ProLearnçš„æ ¸å¿ƒä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸå‹é©±åŠ¨è¯­ä¹‰è¿‘ä¼¼ï¼ˆPSAï¼‰æ¨¡å—ï¼Œä»¥å®ç°ä»æ–‡æœ¬è¾“å…¥è¿›è¡Œè¯­ä¹‰æŒ‡å¯¼çš„è¿‘ä¼¼ã€‚PSAé€šè¿‡è’¸é¦æ¥è‡ªæ–‡æœ¬æŠ¥å‘Šçš„åˆ†å‰²ç›¸å…³è¯­ä¹‰æ¥åˆå§‹åŒ–ä¸€ä¸ªç¦»æ•£ä¸”ç´§å‡‘çš„åŸå‹ç©ºé—´ã€‚åˆå§‹åŒ–å®Œæˆåï¼Œå®ƒæ”¯æŒæŸ¥è¯¢å’Œå“åº”æœºåˆ¶ï¼Œå¯åœ¨æ²¡æœ‰æ–‡æœ¬è¾“å…¥çš„æƒ…å†µä¸‹è¿‘ä¼¼è¯­ä¹‰æŒ‡å¯¼ï¼Œä»è€Œå‡è½»å¯¹æ–‡æœ¬çš„ä¾èµ–ã€‚åœ¨QaTa-COV19ã€MosMedData+å’ŒKvasir-SEGä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œå½“å¯ç”¨æ–‡æœ¬æœ‰é™æ—¶ï¼ŒProLearnçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„è¯­è¨€å¼•å¯¼æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11055v1">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦è¯­è¨€å¼•å¯¼åˆ†å‰²æŠ€æœ¯ç»“åˆæ–‡æœ¬ä¸´åºŠæŠ¥å‘Šä½œä¸ºè¾…åŠ©æŒ‡å¯¼ï¼Œæé«˜äº†å›¾åƒåˆ†å‰²çš„ç²¾åº¦ã€‚ç„¶è€Œï¼Œå…¶å¯¹é…å¯¹å›¾åƒæ–‡æœ¬è¾“å…¥çš„ä¾èµ–å­˜åœ¨ä¸¤å¤§å±€é™ï¼šä¸€æ˜¯è®¸å¤šåŒ»å­¦åˆ†å‰²æ•°æ®é›†ç¼ºä¹é…å¯¹æŠ¥å‘Šï¼Œå¯¼è‡´å¤§é‡ä»…æœ‰å›¾åƒçš„æ•°æ®æœªèƒ½å……åˆ†åˆ©ç”¨ï¼›äºŒæ˜¯æ¨æ–­ä»…é™äºæœ‰é…å¯¹æŠ¥å‘Šçš„ç—…ä¾‹å›é¡¾æ€§åˆ†æï¼Œé™åˆ¶äº†å…¶åœ¨ä¸´åºŠåœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºProLearnâ€”â€”é¦–ä¸ªåŸå‹é©±åŠ¨çš„å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè¯­è¨€å¼•å¯¼åˆ†å‰²ï¼Œä»æ ¹æœ¬ä¸Šå‡è½»äº†æ–‡æœ¬ä¾èµ–ã€‚å¼•å…¥æ–°å‹åŸå‹é©±åŠ¨è¯­ä¹‰é€¼è¿‘æ¨¡å—ï¼Œå®ç°ä»æ–‡æœ¬è¾“å…¥ä¸­è¿‘ä¼¼è¯­ä¹‰æŒ‡å¯¼ã€‚åœ¨QaTa-COV19ã€MosMedData+å’ŒKvasir-SEGä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒProLearnåœ¨æœ‰é™æ–‡æœ¬å¯ç”¨æ—¶ï¼Œè¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„è¯­è¨€å¼•å¯¼æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦è¯­è¨€å¼•å¯¼åˆ†å‰²æŠ€æœ¯ç»“åˆæ–‡æœ¬ä¸´åºŠæŠ¥å‘Šå¯ä»¥æé«˜å›¾åƒåˆ†å‰²çš„ç²¾åº¦ã€‚</li>
<li>åŒ»å­¦åˆ†å‰²æ•°æ®é›†ç¼ºä¹é…å¯¹æŠ¥å‘Šæ˜¯ä¸€å¤§æŒ‘æˆ˜ï¼Œå¯¼è‡´æ•°æ®åˆ©ç”¨ç‡ä¸é«˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å±€é™äºæœ‰é…å¯¹æŠ¥å‘Šçš„ç—…ä¾‹å›é¡¾æ€§åˆ†æã€‚</li>
<li>ProLearnæ˜¯é¦–ä¸ªåŸå‹é©±åŠ¨çš„å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè¯­è¨€å¼•å¯¼åˆ†å‰²ã€‚</li>
<li>ProLearnå¼•å…¥åŸå‹é©±åŠ¨è¯­ä¹‰é€¼è¿‘æ¨¡å—ï¼Œå¯è¿‘ä¼¼å®ç°æ— æ–‡æœ¬è¾“å…¥çš„è¯­ä¹‰æŒ‡å¯¼ã€‚</li>
<li>ProLearnåœ¨æœ‰é™æ–‡æœ¬å¯ç”¨æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºå…¶ä»–å…ˆè¿›çš„è¯­è¨€å¼•å¯¼æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11055">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6ae70b8baf3e88f5648b694fcfb02a67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-581a2ac34e56e6d9c43a0fe382a3f143.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfb49aef80910b9762fe28fb9303d90d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07b6727b70a9303b94d9b35023ccabaa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d541bca6ba72270d04eed9ee94be38a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ffae2a1d29c91fb90ad2aa4271470703.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Focus-on-Texture-Rethinking-Pre-training-in-Masked-Autoencoders-for-Medical-Image-Classification"><a href="#Focus-on-Texture-Rethinking-Pre-training-in-Masked-Autoencoders-for-Medical-Image-Classification" class="headerlink" title="Focus on Texture: Rethinking Pre-training in Masked Autoencoders for   Medical Image Classification"></a>Focus on Texture: Rethinking Pre-training in Masked Autoencoders for   Medical Image Classification</h2><p><strong>Authors:Chetan Madan, Aarjav Satia, Soumen Basu, Pankaj Gupta, Usha Dutta, Chetan Arora</strong></p>
<p>Masked Autoencoders (MAEs) have emerged as a dominant strategy for self-supervised representation learning in natural images, where models are pre-trained to reconstruct masked patches with a pixel-wise mean squared error (MSE) between original and reconstructed RGB values as the loss. We observe that MSE encourages blurred image re-construction, but still works for natural images as it preserves dominant edges. However, in medical imaging, when the texture cues are more important for classification of a visual abnormality, the strategy fails. Taking inspiration from Gray Level Co-occurrence Matrix (GLCM) feature in Radiomics studies, we propose a novel MAE based pre-training framework, GLCM-MAE, using reconstruction loss based on matching GLCM. GLCM captures intensity and spatial relationships in an image, hence proposed loss helps preserve morphological features. Further, we propose a novel formulation to convert matching GLCM matrices into a differentiable loss function. We demonstrate that unsupervised pre-training on medical images with the proposed GLCM loss improves representations for downstream tasks. GLCM-MAE outperforms the current state-of-the-art across four tasks - gallbladder cancer detection from ultrasound images by 2.1%, breast cancer detection from ultrasound by 3.1%, pneumonia detection from x-rays by 0.5%, and COVID detection from CT by 0.6%. Source code and pre-trained models are available at: <a target="_blank" rel="noopener" href="https://github.com/ChetanMadan/GLCM-MAE">https://github.com/ChetanMadan/GLCM-MAE</a>. </p>
<blockquote>
<p>æ©ç è‡ªç¼–ç å™¨ï¼ˆMAEsï¼‰å·²æˆä¸ºè‡ªç„¶å›¾åƒè‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ çš„ä¸»æµç­–ç•¥ï¼Œå…¶ä¸­æ¨¡å‹ç»è¿‡é¢„è®­ç»ƒï¼Œä»¥é‡å»ºæ©ç åŒºåŸŸï¼Œä»¥åŸå§‹å’Œé‡å»ºRGBå€¼ä¹‹é—´çš„åƒç´ çº§å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä½œä¸ºæŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬å‘ç°MSEé¼“åŠ±æ¨¡ç³Šå›¾åƒçš„é‡æ„ï¼Œä½†åœ¨è‡ªç„¶å›¾åƒä¸­ä»ç„¶æœ‰æ•ˆï¼Œå› ä¸ºå®ƒä¿ç•™äº†ä¸»å¯¼è¾¹ç¼˜ã€‚ç„¶è€Œï¼Œåœ¨åŒ»å­¦æˆåƒä¸­ï¼Œå½“çº¹ç†çº¿ç´¢å¯¹äºè§†è§‰å¼‚å¸¸çš„åˆ†ç±»æ›´ä¸ºé‡è¦æ—¶ï¼Œè¯¥ç­–ç•¥ä¼šå¤±æ•ˆã€‚æˆ‘ä»¬ä»æ”¾å°„å­¦ç ”ç©¶ä¸­ç°é˜¶å…±ç”ŸçŸ©é˜µï¼ˆGLCMï¼‰ç‰¹å¾ä¸­æ±²å–çµæ„Ÿï¼Œæå‡ºäº†ä¸€ç§åŸºäºMAEçš„é¢„è®­ç»ƒæ¡†æ¶GLCM-MAEï¼Œä½¿ç”¨åŸºäºåŒ¹é…GLCMçš„é‡æ„æŸå¤±ã€‚GLCMæ•æ‰å›¾åƒä¸­çš„å¼ºåº¦å’Œç©ºé—´å…³ç³»ï¼Œå› æ­¤æ‰€æå‡ºçš„æŸå¤±æœ‰åŠ©äºä¿ç•™å½¢æ€ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å…¬å¼ï¼Œå°†åŒ¹é…çš„GLCMçŸ©é˜µè½¬åŒ–ä¸ºå¯å¾®åˆ†çš„æŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬è¯æ˜ï¼Œä½¿ç”¨æ‰€æå‡ºçš„GLCMæŸå¤±å¯¹åŒ»å­¦å›¾åƒè¿›è¡Œæ— ç›‘ç£é¢„è®­ç»ƒï¼Œå¯ä»¥æ”¹å–„ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ã€‚GLCM-MAEåœ¨å››é¡¹ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†å½“å‰çš„æœ€ä½³æ°´å¹³ï¼šè¶…å£°å›¾åƒèƒ†å›Šç™Œæ£€æµ‹æé«˜2.1%ï¼Œè¶…å£°ä¹³è…ºç™Œæ£€æµ‹æé«˜3.1%ï¼ŒXå…‰è‚ºç‚æ£€æµ‹æé«˜0.5%ï¼ŒCTæ–°å† è‚ºç‚æ£€æµ‹æé«˜0.6%ã€‚æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ChetanMadan/GLCM-MAE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ChetanMadan/GLCM-MAEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10869v1">PDF</a> To appear at MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºç°åº¦å…±ç”ŸçŸ©é˜µï¼ˆGLCMï¼‰çš„Masked Autoencodersï¼ˆMAEsï¼‰åœ¨åŒ»ç–—å›¾åƒè‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒä¸­çš„ä½¿ç”¨ã€‚é€šè¿‡ç»“åˆGLCMçš„ç‰¹å¾ä¸MAEçš„é‡å»ºæŸå¤±ï¼Œå½¢æˆæ–°çš„é¢„è®­ç»ƒæ¡†æ¶GLCM-MAEã€‚æ­¤æ¡†æ¶èƒ½å¤Ÿæ•æ‰å›¾åƒçš„å¼ºåº¦å’Œç©ºé—´å…³ç³»ï¼Œæœ‰åŠ©äºä¿ç•™å½¢æ€å­¦ç‰¹å¾ã€‚åœ¨å››ä¸ªä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒGLCM-MAEåœ¨åŒ»ç–—å›¾åƒä¸‹æ¸¸ä»»åŠ¡ä¸­çš„è¡¨ç°ä¼˜äºå½“å‰çš„æœ€ä¼˜æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MAEså·²æˆä¸ºè‡ªç„¶å›¾åƒè‡ªæˆ‘ç›‘ç£è¡¨ç¤ºå­¦ä¹ çš„ä¸»è¦ç­–ç•¥ï¼Œä½†åœ¨åŒ»ç–—å›¾åƒä¸­ï¼Œå½“çº¹ç†çº¿ç´¢å¯¹è§†è§‰å¼‚å¸¸åˆ†ç±»æ›´é‡è¦æ—¶ï¼ŒåŸºäºMSEçš„ç­–ç•¥ä¼šå¤±æ•ˆã€‚</li>
<li>æå‡ºäº†åŸºäºç°åº¦å…±ç”ŸçŸ©é˜µï¼ˆGLCMï¼‰çš„MAEé¢„è®­ç»ƒæ¡†æ¶GLCM-MAEã€‚</li>
<li>GLCMèƒ½å¤Ÿæ•æ‰å›¾åƒçš„å¼ºåº¦å’Œç©ºé—´å…³ç³»ï¼Œé€šè¿‡åŒ¹é…GLCMçš„é‡å»ºæŸå¤±æœ‰åŠ©äºä¿ç•™å½¢æ€å­¦ç‰¹å¾ã€‚</li>
<li>å°†åŒ¹é…GLCMçŸ©é˜µè½¬åŒ–ä¸ºå¯å¾®åˆ†çš„æŸå¤±å‡½æ•°ã€‚</li>
<li>åœ¨å››ä¸ªä»»åŠ¡ä¸Šï¼ŒGLCM-MAEå‡è¡¨ç°å‡ºè¶…è¶Šå½“å‰æœ€å…ˆè¿›æ–¹æ³•çš„æ•ˆæœï¼ŒåŒ…æ‹¬èƒ†å›Šç™Œã€ä¹³è…ºç™Œæ£€æµ‹çš„è¶…å£°å›¾åƒï¼Œè‚ºç‚æ£€æµ‹çš„Xå…‰å›¾åƒä»¥åŠCOVIDæ£€æµ‹çš„CTå›¾åƒã€‚</li>
<li>å…¬å¼€äº†æºä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œä¾¿äºä»–äººä½¿ç”¨å’Œç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10869">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7292b5944055899c3036d3795b0a888f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d84fa655c22328667c574cf3f51645c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b763685a6da844b81748c9793768e2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-742e7ae4eb6b5d512238db3bd8bae61e.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-New-Dataset-and-Performance-Benchmark-for-Real-time-Spacecraft-Segmentation-in-Onboard-Flight-Computers"><a href="#A-New-Dataset-and-Performance-Benchmark-for-Real-time-Spacecraft-Segmentation-in-Onboard-Flight-Computers" class="headerlink" title="A New Dataset and Performance Benchmark for Real-time Spacecraft   Segmentation in Onboard Flight Computers"></a>A New Dataset and Performance Benchmark for Real-time Spacecraft   Segmentation in Onboard Flight Computers</h2><p><strong>Authors:Jeffrey Joan Sam, Janhavi Sathe, Nikhil Chigali, Naman Gupta, Radhey Ruparel, Yicheng Jiang, Janmajay Singh, James W. Berck, Arko Barman</strong></p>
<p>Spacecraft deployed in outer space are routinely subjected to various forms of damage due to exposure to hazardous environments. In addition, there are significant risks to the subsequent process of in-space repairs through human extravehicular activity or robotic manipulation, incurring substantial operational costs. Recent developments in image segmentation could enable the development of reliable and cost-effective autonomous inspection systems. While these models often require large amounts of training data to achieve satisfactory results, publicly available annotated spacecraft segmentation data are very scarce. Here, we present a new dataset of nearly 64k annotated spacecraft images that was created using real spacecraft models, superimposed on a mixture of real and synthetic backgrounds generated using NASAâ€™s TTALOS pipeline. To mimic camera distortions and noise in real-world image acquisition, we also added different types of noise and distortion to the images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to generate performance benchmarks for the dataset under well-defined hardware and inference time constraints to mimic real-world image segmentation challenges for real-time onboard applications in space on NASAâ€™s inspector spacecraft. The resulting models, when tested under these constraints, achieved a Dice score of 0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second. The dataset and models for performance benchmark are available at <a target="_blank" rel="noopener" href="https://github.com/RiceD2KLab/SWiM">https://github.com/RiceD2KLab/SWiM</a>. </p>
<blockquote>
<p>åœ¨å¤ªç©ºéƒ¨ç½²çš„èˆªå¤©å™¨ç»å¸¸æš´éœ²åœ¨å±é™©ç¯å¢ƒä¸­ï¼Œå› è€Œå—åˆ°å„ç§å½¢å¼çš„æŸå®³ã€‚æ­¤å¤–ï¼Œé€šè¿‡äººç±»èˆ±å¤–æ´»åŠ¨æˆ–æœºå™¨äººæ“çºµè¿›è¡Œçš„å¤ªç©ºä¿®å¤è¿‡ç¨‹å­˜åœ¨é‡å¤§é£é™©ï¼Œå¹¶äº§ç”Ÿå¤§é‡æ“ä½œæˆæœ¬ã€‚å›¾åƒåˆ†å‰²é¢†åŸŸçš„æœ€æ–°å‘å±•å¯èƒ½ä¼šä½¿å¯é ä¸”ç»æµçš„è‡ªä¸»æ£€æŸ¥ç³»ç»Ÿçš„å¼€å‘æˆä¸ºå¯èƒ½ã€‚è™½ç„¶è¿™äº›æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®æ‰èƒ½è¾¾åˆ°æ»¡æ„çš„ç»“æœï¼Œä½†å…¬å¼€å¯ç”¨çš„å¸¦æ³¨é‡Šçš„èˆªå¤©å™¨åˆ†å‰²æ•°æ®éå¸¸ç¨€ç¼ºã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªæ–°æ•°æ®é›†ï¼ŒåŒ…å«è¿‘6.4ä¸‡å¼ ä½¿ç”¨çœŸå®èˆªå¤©å™¨æ¨¡å‹åˆ¶ä½œçš„å¸¦æ³¨é‡Šçš„èˆªå¤©å™¨å›¾åƒã€‚è¿™äº›å›¾åƒå åŠ åœ¨ç”±NASAçš„TTALOSç®¡é“ç”Ÿæˆçš„çœŸå®å’ŒåˆæˆèƒŒæ™¯æ··åˆç‰©ä¸Šã€‚ä¸ºäº†æ¨¡æ‹Ÿç°å®ä¸–ç•Œå›¾åƒé‡‡é›†ä¸­çš„ç›¸æœºå¤±çœŸå’Œå™ªå£°ï¼Œæˆ‘ä»¬è¿˜å‘å›¾åƒä¸­æ·»åŠ äº†ä¸åŒç±»å‹çš„å™ªå£°å’Œå¤±çœŸã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹YOLOv8å’ŒYOLOv11åˆ†å‰²æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œä»¥ç”Ÿæˆé’ˆå¯¹è¯¥æ•°æ®é›†çš„åŸºå‡†æµ‹è¯•æ€§èƒ½ï¼Œè¿™äº›æµ‹è¯•åœ¨æ˜ç¡®çš„ç¡¬ä»¶å’Œæ¨ç†æ—¶é—´çº¦æŸä¸‹è¿›è¡Œï¼Œä»¥æ¨¡æ‹ŸNASAæ£€æŸ¥èˆªå¤©å™¨ä¸Šçš„å®æ—¶æœºä¸Šåº”ç”¨çš„çœŸå®ä¸–ç•Œå›¾åƒåˆ†å‰²æŒ‘æˆ˜ã€‚åœ¨è¿™äº›çº¦æŸæ¡ä»¶ä¸‹æµ‹è¯•çš„æ¨¡å‹å–å¾—äº†Diceç³»æ•°ä¸º0.92ã€Hausdorffè·ç¦»ä¸º0.69ä»¥åŠå¤§çº¦0.5ç§’çš„æ¨ç†æ—¶é—´ã€‚æ•°æ®é›†å’Œæ€§èƒ½åŸºå‡†æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RiceD2KLab/SWiM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/RiceD2KLab/SWiMä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10775v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦æä¾›äº†ä¸€ç§æ–°å‹æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«è¿‘6.4ä¸‡å¼ å¸¦æœ‰æ ‡æ³¨çš„èˆªå¤©å™¨å›¾åƒã€‚æ•°æ®é›†åˆ©ç”¨NASAçš„TTALOSç®¡é“ç”ŸæˆçœŸå®å’ŒåˆæˆèƒŒæ™¯ï¼Œå¹¶æ·»åŠ äº†ä¸åŒç±»å‹çš„å™ªå£°å’Œå¤±çœŸï¼Œä»¥æ¨¡æ‹Ÿå¤ªç©ºç¯å¢ƒä¸­çš„ç›¸æœºå¤±çœŸå’Œå™ªå£°ã€‚ç ”ç©¶äººå‘˜ä½¿ç”¨YOLOv8å’ŒYOLOv11åˆ†å‰²æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥åœ¨è¯¥æ•°æ®é›†ä¸Šç”Ÿæˆæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œä»¥æ¨¡æ‹Ÿå¤ªç©ºä¸­çš„å®æ—¶åœ¨çº¿åº”ç”¨ã€‚æ¨¡å‹åœ¨æµ‹è¯•ä¸‹å–å¾—äº†è¾ƒé«˜çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°å‹æ•°æ®é›†åŒ…å«è¿‘6.4ä¸‡å¼ å¸¦æœ‰æ ‡æ³¨çš„èˆªå¤©å™¨å›¾åƒï¼Œç”¨äºè‡ªä¸»æ£€æµ‹ç³»ç»Ÿçš„å¼€å‘ã€‚</li>
<li>æ•°æ®é›†ç»“åˆäº†çœŸå®å’ŒåˆæˆèƒŒæ™¯ï¼Œæ¨¡æ‹Ÿå¤ªç©ºç¯å¢ƒã€‚</li>
<li>æ·»åŠ äº†ä¸åŒç±»å‹çš„å™ªå£°å’Œå¤±çœŸï¼Œä»¥æ¨¡æ‹Ÿç›¸æœºåœ¨çœŸå®ä¸–ç•Œä¸­çš„å¤±çœŸå’Œå™ªå£°é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨YOLOv8å’ŒYOLOv11åˆ†å‰²æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥åœ¨è¯¥æ•°æ®é›†ä¸Šè¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ¨¡å‹åœ¨æ¨¡æ‹Ÿçš„å¤ªç©ºç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½ï¼Œè¾¾åˆ°äº†è¾ƒé«˜çš„æ€§èƒ½æŒ‡æ ‡ã€‚</li>
<li>æ¨¡å‹çš„æ¨ç†æ—¶é—´çº¦ä¸º0.5ç§’ï¼Œæ»¡è¶³å®æ—¶åº”ç”¨çš„éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10775">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-66257a2b1552f1da636b254c5e36b329.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03717ac2d7288a02510c2088a15559e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9f64c15e283079790c18e3acc63c969c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74ea342f6f96612fbaa2b8397076f20d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38f97a4045c805b69c122d7e3c5471ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d7799076a5c79ee1e4cbc111cc74ebd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d86bd45d0cb67d43b724ff0161ee83e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eb2aaac89b62e38e21d8a09ea3da9870.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6237ea2a1ca55f0fbae74c1c50fb43b0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="RefSTAR-Blind-Facial-Image-Restoration-with-Reference-Selection-Transfer-and-Reconstruction"><a href="#RefSTAR-Blind-Facial-Image-Restoration-with-Reference-Selection-Transfer-and-Reconstruction" class="headerlink" title="RefSTAR: Blind Facial Image Restoration with Reference Selection,   Transfer, and Reconstruction"></a>RefSTAR: Blind Facial Image Restoration with Reference Selection,   Transfer, and Reconstruction</h2><p><strong>Authors:Zhicun Yin, Junjie Chen, Ming Liu, Zhixin Wang, Fan Li, Renjing Pei, Xiaoming Li, Rynson W. H. Lau, Wangmeng Zuo</strong></p>
<p>Blind facial image restoration is highly challenging due to unknown complex degradations and the sensitivity of humans to faces. Although existing methods introduce auxiliary information from generative priors or high-quality reference images, they still struggle with identity preservation problems, mainly due to improper feature introduction on detailed textures. In this paper, we focus on effectively incorporating appropriate features from high-quality reference images, presenting a novel blind facial image restoration method that considers reference selection, transfer, and reconstruction (RefSTAR). In terms of selection, we construct a reference selection (RefSel) module. For training the RefSel module, we construct a RefSel-HQ dataset through a mask generation pipeline, which contains annotating masks for 10,000 ground truth-reference pairs. As for the transfer, due to the trivial solution in vanilla cross-attention operations, a feature fusion paradigm is designed to force the features from the reference to be integrated. Finally, we propose a reference image reconstruction mechanism that further ensures the presence of reference image features in the output image. The cycle consistency loss is also redesigned in conjunction with the mask. Extensive experiments on various backbone models demonstrate superior performance, showing better identity preservation ability and reference feature transfer quality. Source code, dataset, and pre-trained models are available at <a target="_blank" rel="noopener" href="https://github.com/yinzhicun/RefSTAR">https://github.com/yinzhicun/RefSTAR</a>. </p>
<blockquote>
<p>é¢éƒ¨å›¾åƒçš„ç›²ä¿®å¤æ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºæœªçŸ¥çš„å¤æ‚é€€åŒ–å’Œäººç±»å¯¹é¢éƒ¨çš„æ•æ„Ÿæ€§ã€‚å°½ç®¡ç°æœ‰çš„æ–¹æ³•å¼•å…¥äº†ç”Ÿæˆå…ˆéªŒæˆ–é«˜è´¨é‡å‚è€ƒå›¾åƒçš„è¾…åŠ©ä¿¡æ¯ï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´ç€èº«ä»½ä¿ç•™é—®é¢˜ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºç»†èŠ‚çº¹ç†çš„ä¸å½“ç‰¹å¾å¼•å…¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæœ‰æ•ˆåœ°èå…¥é«˜è´¨é‡å‚è€ƒå›¾åƒä¸­çš„é€‚å½“ç‰¹å¾ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„é¢éƒ¨å›¾åƒç›²ä¿®å¤æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è€ƒè™‘äº†å‚è€ƒé€‰æ‹©ã€è½¬ç§»å’Œé‡å»ºï¼ˆRefSTARï¼‰ã€‚åœ¨å‚è€ƒé€‰æ‹©æ–¹é¢ï¼Œæˆ‘ä»¬æ„å»ºäº†å‚è€ƒé€‰æ‹©ï¼ˆRefSelï¼‰æ¨¡å—ã€‚ä¸ºäº†è®­ç»ƒRefSelæ¨¡å—ï¼Œæˆ‘ä»¬é€šè¿‡æ©æ¨¡ç”Ÿæˆç®¡é“æ„å»ºäº†RefSel-HQæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«10,000ä¸ªçœŸå®å‚ç…§å¯¹æ ‡æ³¨æ©æ¨¡ã€‚åœ¨è½¬ç§»æ–¹é¢ï¼Œç”±äºæ™®é€šäº¤å‰æ³¨æ„æ“ä½œå­˜åœ¨å¹³å‡¡è§£ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç‰¹å¾èåˆèŒƒå¼æ¥å¼ºåˆ¶æ•´åˆå‚è€ƒç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å‚è€ƒå›¾åƒé‡å»ºæœºåˆ¶ï¼Œç¡®ä¿å‚è€ƒå›¾åƒç‰¹å¾å‡ºç°åœ¨è¾“å‡ºå›¾åƒä¸­ã€‚å¾ªç¯ä¸€è‡´æ€§æŸå¤±ä¹Ÿä¸æ©æ¨¡é‡æ–°è®¾è®¡ã€‚åœ¨ä¸åŒä¸»å¹²æ¨¡å‹ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å…·æœ‰å“è¶Šçš„æ€§èƒ½ï¼Œè¡¨ç°å‡ºæ›´å¥½çš„èº«ä»½ä¿ç•™èƒ½åŠ›å’Œå‚è€ƒç‰¹å¾è½¬ç§»è´¨é‡ã€‚æºä»£ç ã€æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yinzhicun/RefSTAR%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/yinzhicun/RefSTARæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10470v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç›²é¢éƒ¨å›¾åƒæ¢å¤æ–¹æ³•ï¼Œé€šè¿‡æœ‰æ•ˆèå…¥é«˜è´¨é‡å‚è€ƒå›¾åƒçš„ç‰¹å¾ï¼Œè§£å†³èº«ä»½ä¿ç•™é—®é¢˜ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬å‚è€ƒé€‰æ‹©ã€è½¬ç§»å’Œé‡å»ºï¼ˆRefSTARï¼‰ï¼Œæ„å»ºäº†å‚è€ƒé€‰æ‹©ï¼ˆRefSelï¼‰æ¨¡å—ï¼Œå¹¶é€šè¿‡RefSel-HQæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚è®¾è®¡ç‰¹å¾èåˆèŒƒå¼ï¼Œç¡®ä¿å‚è€ƒç‰¹å¾èå…¥ï¼Œå¹¶æå‡ºå‚è€ƒå›¾åƒé‡å»ºæœºåˆ¶ï¼Œç¡®ä¿è¾“å‡ºå›¾åƒåŒ…å«å‚è€ƒå›¾åƒç‰¹å¾ã€‚è¯¥æ–¹æ³•åœ¨å¤šç§ä¸»å¹²æ¨¡å‹ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œå…·æœ‰æ›´å¥½çš„èº«ä»½ä¿ç•™èƒ½åŠ›å’Œå‚è€ƒç‰¹å¾è½¬ç§»è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç›²é¢éƒ¨å›¾åƒæ¢å¤é¢ä¸´æœªçŸ¥å¤æ‚é€€åŒ–å’Œäººç±»å¯¹é¢éƒ¨çš„æ•æ„Ÿæ€§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¼•å…¥ç”Ÿæˆå…ˆéªŒæˆ–é«˜è´¨é‡å‚è€ƒå›¾åƒä¿¡æ¯ï¼Œä½†ä»å­˜åœ¨èº«ä»½ä¿ç•™é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°ç›²é¢éƒ¨å›¾åƒæ¢å¤æ–¹æ³•ï¼Œé€šè¿‡èå…¥é«˜è´¨é‡å‚è€ƒå›¾åƒçš„ç‰¹å¾è§£å†³èº«ä»½ä¿ç•™é—®é¢˜ã€‚</li>
<li>æ„å»ºå‚è€ƒé€‰æ‹©ï¼ˆRefSelï¼‰æ¨¡å—ï¼Œé€šè¿‡RefSel-HQæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>è®¾è®¡ç‰¹å¾èåˆèŒƒå¼ï¼Œè§£å†³å¹³å‡¡äº¤å‰å…³æ³¨æ“ä½œä¸­çš„ç‰¹å¾è½¬ç§»é—®é¢˜ã€‚</li>
<li>æå‡ºå‚è€ƒå›¾åƒé‡å»ºæœºåˆ¶ï¼Œç¡®ä¿è¾“å‡ºå›¾åƒåŒ…å«å‚è€ƒå›¾åƒç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10470">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5879f47f4934c41d5a8cf549d35db179.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe068c9bae396536e26bb45d6dfe831e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea23917b888d77ee05de82eec3c042d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64090ad4451294b05f40f149e26a2d12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1fc848e7470d6368472212f92ecf255.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DepViT-CAD-Deployable-Vision-Transformer-Based-Cancer-Diagnosis-in-Histopathology"><a href="#DepViT-CAD-Deployable-Vision-Transformer-Based-Cancer-Diagnosis-in-Histopathology" class="headerlink" title="DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in   Histopathology"></a>DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in   Histopathology</h2><p><strong>Authors:Ashkan Shakarami, Lorenzo Nicole, Rocco Cappellesso, Angelo Paolo Dei Tos, Stefano Ghidoni</strong></p>
<p>Accurate and timely cancer diagnosis from histopathological slides is vital for effective clinical decision-making. This paper introduces DepViT-CAD, a deployable AI system for multi-class cancer diagnosis in histopathology. At its core is MAViT, a novel Multi-Attention Vision Transformer designed to capture fine-grained morphological patterns across diverse tumor types. MAViT was trained on expert-annotated patches from 1008 whole-slide images, covering 11 diagnostic categories, including 10 major cancers and non-tumor tissue. DepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer Genome Atlas and 50 routine clinical cases from pathology labs, achieving diagnostic sensitivities of 94.11% and 92%, respectively. By combining state-of-the-art transformer architecture with large-scale real-world validation, DepViT-CAD offers a robust and scalable approach for AI-assisted cancer diagnostics. To support transparency and reproducibility, software and code will be made publicly available at GitHub. </p>
<blockquote>
<p>å‡†ç¡®åŠæ—¶çš„ç™Œç—‡è¯Šæ–­å¯¹äºæœ‰æ•ˆçš„ä¸´åºŠå†³ç­–è‡³å…³é‡è¦ã€‚æœ¬æ–‡ä»‹ç»äº†ç”¨äºç»„ç»‡ç—…ç†å­¦å¤šç±»ç™Œç—‡è¯Šæ–­çš„DepViT-CADå¯éƒ¨ç½²AIç³»ç»Ÿã€‚å…¶æ ¸å¿ƒæ˜¯MAViTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ³¨æ„åŠ›è§†è§‰è½¬æ¢å™¨ï¼Œæ—¨åœ¨æ•è·å¤šç§è‚¿ç˜¤ç±»å‹ä¸­çš„ç²¾ç»†å½¢æ€æ¨¡å¼ã€‚MAViTæ˜¯åœ¨æ¥è‡ª1008å¼ å…¨åˆ‡ç‰‡å›¾åƒçš„ä¸“å®¶æ³¨é‡ŠåŒºåŸŸä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œæ¶µç›–åŒ…æ‹¬10ç§ä¸»è¦ç™Œç—‡å’Œéè‚¿ç˜¤ç»„ç»‡åœ¨å†…çš„11ä¸ªè¯Šæ–­ç±»åˆ«ã€‚DepViT-CADåœ¨ä¸¤ä¸ªç‹¬ç«‹é˜Ÿåˆ—ä¸­è¿›è¡Œäº†éªŒè¯ï¼šæ¥è‡ªç™Œç—‡åŸºå› ç»„å›¾è°±çš„275ä¸ªWSIå’Œæ¥è‡ªç—…ç†å®éªŒå®¤çš„50ä¾‹å¸¸è§„ä¸´åºŠç—…ä¾‹ï¼Œè¯Šæ–­æ•æ„Ÿæ€§åˆ†åˆ«ä¸º94.11%å’Œ92%ã€‚é€šè¿‡å°†æœ€å…ˆè¿›çš„è½¬æ¢æ¶æ„ä¸å¤§è§„æ¨¡ç°å®ä¸–ç•ŒéªŒè¯ç›¸ç»“åˆï¼ŒDepViT-CADä¸ºAIè¾…åŠ©ç™Œç—‡è¯Šæ–­æä¾›äº†ç¨³å¥ä¸”å¯æ‰©å±•çš„æ–¹æ³•ã€‚ä¸ºäº†æ”¯æŒé€æ˜åº¦å’Œå¯é‡å¤æ€§ï¼Œè½¯ä»¶å’Œä»£ç å°†åœ¨GitHubä¸Šå…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10250v1">PDF</a> 25 pages, 15 figures</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸçš„æ–°ç ”ç©¶ä»‹ç»äº†ä¸€ç§åä¸ºDepViT-CADçš„å¯éƒ¨ç½²äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œç”¨äºå¤šç±»åˆ«ç™Œç—‡è¯Šæ–­ã€‚å…¶æ ¸å¿ƒæ˜¯MAViTï¼Œä¸€ç§æ–°å‹çš„å¤šæ³¨æ„åŠ›è§†è§‰è½¬æ¢å™¨ï¼Œèƒ½å¤Ÿæ•æ‰ä¸åŒè‚¿ç˜¤ç±»å‹çš„ç²¾ç»†å½¢æ€æ¨¡å¼ã€‚ç³»ç»Ÿç»è¿‡ä¸“å®¶æ³¨é‡Šçš„è¡¥ä¸è®­ç»ƒï¼Œå¯åœ¨å…¨è§†é‡åˆ‡ç‰‡ä¸Šè¯†åˆ«åä¸€ç§è¯Šæ–­ç±»åˆ«ï¼ŒåŒ…æ‹¬åç§ä¸»è¦ç™Œç—‡å’Œéè‚¿ç˜¤ç»„ç»‡ã€‚åœ¨ç™Œç—‡åŸºå› ç»„å›¾è°±çš„275å¼ å…¨è§†é‡åˆ‡ç‰‡å›¾åƒå’Œç—…ç†å®éªŒå®¤çš„50ä¾‹å¸¸è§„ä¸´åºŠç—…ä¾‹ä¸­è¿›è¡ŒéªŒè¯ï¼Œè¯Šæ–­æ•æ„Ÿæ€§åˆ†åˆ«ä¸º94.11%å’Œ92%ã€‚è¯¥ç³»ç»Ÿç»“åˆäº†æœ€å…ˆè¿›çš„è½¬æ¢å™¨æ¶æ„å’Œå¤§è§„æ¨¡ç°å®ä¸–ç•ŒéªŒè¯ï¼Œä¸ºäººå·¥æ™ºèƒ½è¾…åŠ©ç™Œç—‡è¯Šæ–­æä¾›äº†ç¨³å¥ä¸”å¯æ‰©å±•çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DepViT-CADæ˜¯ä¸€ä¸ªç”¨äºå¤šç±»åˆ«ç™Œç—‡è¯Šæ–­çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚</li>
<li>MAViTæ˜¯è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒï¼Œæ˜¯ä¸€ç§å¤šæ³¨æ„åŠ›è§†è§‰è½¬æ¢å™¨ã€‚</li>
<li>MAViTèƒ½å¤Ÿæ•æ‰å„ç§è‚¿ç˜¤ç±»å‹çš„ç²¾ç»†å½¢æ€æ¨¡å¼ã€‚</li>
<li>ç³»ç»Ÿç»è¿‡ä¸“å®¶æ³¨é‡Šçš„è¡¥ä¸è®­ç»ƒï¼Œå¯è¯†åˆ«åä¸€ç§è¯Šæ–­ç±»åˆ«ã€‚</li>
<li>åœ¨ç™Œç—‡åŸºå› ç»„å›¾è°±å’Œç—…ç†å®éªŒå®¤çš„å¸¸è§„ä¸´åºŠç—…ä¾‹ä¸­è¿›è¡ŒéªŒè¯ï¼Œè¯Šæ–­æ•æ„Ÿæ€§é«˜ã€‚</li>
<li>ç³»ç»Ÿç»“åˆäº†æœ€å…ˆè¿›çš„è½¬æ¢å™¨æ¶æ„å’Œå¤§è§„æ¨¡ç°å®ä¸–ç•ŒéªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10250">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-53d42bfe34c5e4b081aa3784780e2754.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1ceb3de58bd9af0bda446894073b7ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-611d7a09be822e82aa16e1ec616d89b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a011285e8aa1eaa00b81ab2d32c6bb24.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Graph-based-Multi-Modal-Interaction-Lightweight-Network-for-Brain-Tumor-Segmentation-GMLN-BTS-in-Edge-Iterative-MRI-Lesion-Localization-System-EdgeIMLocSys"><a href="#Graph-based-Multi-Modal-Interaction-Lightweight-Network-for-Brain-Tumor-Segmentation-GMLN-BTS-in-Edge-Iterative-MRI-Lesion-Localization-System-EdgeIMLocSys" class="headerlink" title="Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor   Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System   (EdgeIMLocSys)"></a>Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor   Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System   (EdgeIMLocSys)</h2><p><strong>Authors:Guohao Huo, Ruiting Dai, Hao Tang</strong></p>
<p>Brain tumor segmentation plays a critical role in clinical diagnosis and treatment planning, yet the variability in imaging quality across different MRI scanners presents significant challenges to model generalization. To address this, we propose the Edge Iterative MRI Lesion Localization System (EdgeIMLocSys), which integrates Continuous Learning from Human Feedback to adaptively fine-tune segmentation models based on clinician feedback, thereby enhancing robustness to scanner-specific imaging characteristics. Central to this system is the Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS), which employs a Modality-Aware Adaptive Encoder (M2AE) to extract multi-scale semantic features efficiently, and a Graph-based Multi-Modal Collaborative Interaction Module (G2MCIM) to model complementary cross-modal relationships via graph structures. Additionally, we introduce a novel Voxel Refinement UpSampling Module (VRUM) that synergistically combines linear interpolation and multi-scale transposed convolutions to suppress artifacts while preserving high-frequency details, improving segmentation boundary accuracy. Our proposed GMLN-BTS model achieves a Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million parameters, representing a 98% reduction compared to mainstream 3D Transformer models, and significantly outperforms existing lightweight approaches. This work demonstrates a synergistic breakthrough in achieving high-accuracy, resource-efficient brain tumor segmentation suitable for deployment in resource-constrained clinical environments. </p>
<blockquote>
<p>è„‘è‚¿ç˜¤åˆ†å‰²åœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’åˆ¶å®šä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œç„¶è€Œï¼Œä¸åŒMRIæ‰«æä»ªæˆåƒè´¨é‡çš„å·®å¼‚ç»™æ¨¡å‹é€šç”¨åŒ–å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è¾¹ç¼˜è¿­ä»£MRIç—…å˜å®šä½ç³»ç»Ÿï¼ˆEdgeIMLocSysï¼‰ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡æ•´åˆäººç±»åé¦ˆçš„è¿ç»­å­¦ä¹ æ¥è‡ªé€‚åº”å¾®è°ƒåˆ†å‰²æ¨¡å‹ï¼Œæ ¹æ®ä¸´åºŠåŒ»ç”Ÿåé¦ˆæé«˜ç³»ç»Ÿå¯¹æ‰«æä»ªç‰¹å®šæˆåƒç‰¹æ€§çš„ç¨³å¥æ€§ã€‚è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒæ˜¯åŸºäºå›¾çš„è½»é‡çº§å¤šæ¨¡æ€äº¤äº’ç½‘ç»œè„‘è‚¿ç˜¤åˆ†å‰²æ¨¡å‹ï¼ˆGMLN-BTSï¼‰ï¼Œå®ƒé‡‡ç”¨æ¨¡æ€æ„ŸçŸ¥è‡ªé€‚åº”ç¼–ç å™¨ï¼ˆM2AEï¼‰é«˜æ•ˆæå–å¤šå°ºåº¦è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨åŸºäºå›¾çš„è·¨æ¨¡æ€ååŒäº¤äº’æ¨¡å—ï¼ˆG2MCIMï¼‰é€šè¿‡å›¾ç»“æ„å¯¹äº’è¡¥è·¨æ¨¡æ€å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ä½“ç´ ç»†åŒ–ä¸Šé‡‡æ ·æ¨¡å—ï¼ˆVRUMï¼‰ï¼Œè¯¥æ¨¡å—ååŒç»“åˆçº¿æ€§æ’å€¼å’Œå¤šå°ºåº¦è½¬ç½®å·ç§¯ï¼Œåœ¨ä¿ç•™é«˜é¢‘ç»†èŠ‚çš„åŒæ—¶æŠ‘åˆ¶ä¼ªå½±ï¼Œæé«˜äº†åˆ†å‰²è¾¹ç•Œçš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æå‡ºçš„GMLN-BTSæ¨¡å‹åœ¨BraTS2017æ•°æ®é›†ä¸Šå®ç°äº†85.1%çš„Diceå¾—åˆ†ï¼Œå¹¶ä¸”ä»…æœ‰458ä¸‡ä¸ªå‚æ•°ï¼Œç›¸æ¯”ä¸»æµçš„3D Transformeræ¨¡å‹å‡å°‘äº†98%ï¼Œå¹¶ä¸”åœ¨è½»é‡çº§æ–¹æ³•ä¸­æ˜¾è‘—è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†å®ç°é€‚åˆèµ„æºå—é™ä¸´åºŠç¯å¢ƒä¸­çš„é«˜ç²¾åº¦ã€èµ„æºé«˜æ•ˆçš„è„‘è‚¿ç˜¤åˆ†å‰²çš„ååŒçªç ´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09995v2">PDF</a> </p>
<p><strong>Summary</strong><br>    é’ˆå¯¹MRIæ‰«æä»ªä¹‹é—´æˆåƒè´¨é‡å·®å¼‚å¯¹è„‘è‚¿ç˜¤åˆ†å‰²æ¨¡å‹é€šç”¨åŒ–çš„æŒ‘æˆ˜ï¼Œæå‡ºEdgeIMLocSysç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ•´åˆäººç±»åé¦ˆçš„æŒç»­å­¦ä¹ ï¼Œè‡ªé€‚åº”å¾®è°ƒåˆ†å‰²æ¨¡å‹ï¼Œæé«˜å¯¹ä¸åŒæ‰«æä»ªæˆåƒç‰¹æ€§çš„ç¨³å¥æ€§ã€‚æ ¸å¿ƒä¸ºGMLN-BTSç½‘ç»œï¼Œé‡‡ç”¨M2AEæœ‰æ•ˆæå–å¤šå°ºåº¦è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡G2MCIMå›¾ç»“æ„æ¨¡å—è¿›è¡Œè·¨æ¨¡æ€ååŒäº¤äº’ã€‚å¼•å…¥VRUMæ¨¡å—ï¼Œç»“åˆçº¿æ€§æ’å€¼å’Œå¤šå°ºåº¦è½¬ç½®å·ç§¯ï¼Œæé«˜åˆ†å‰²è¾¹ç•Œç²¾åº¦ã€‚GMLN-BTSæ¨¡å‹åœ¨BraTS2017æ•°æ®é›†ä¸Šå®ç°85.1%çš„Diceå¾—åˆ†ï¼Œå‚æ•°ä»…4.58ç™¾ä¸‡ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰è½»é‡åŒ–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EdgeIMLocSysç³»ç»Ÿè§£å†³äº†ä¸åŒMRIæ‰«æä»ªæˆåƒè´¨é‡å·®å¼‚å¯¹è„‘è‚¿ç˜¤åˆ†å‰²æ¨¡å‹é€šç”¨åŒ–çš„æŒ‘æˆ˜ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡æ•´åˆäººç±»åé¦ˆçš„æŒç»­å­¦ä¹ ï¼Œæé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>GMLN-BTSç½‘ç»œæ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒï¼ŒåŒ…å«M2AEå’ŒG2MCIMä¸¤ä¸ªå…³é”®æ¨¡å—ã€‚</li>
<li>M2AEæœ‰æ•ˆæå–å¤šå°ºåº¦è¯­ä¹‰ç‰¹å¾ï¼ŒG2MCIMåˆ™é€šè¿‡å›¾ç»“æ„è¿›è¡Œè·¨æ¨¡æ€ååŒäº¤äº’ã€‚</li>
<li>å¼•å…¥VRUMæ¨¡å—ï¼Œç»“åˆçº¿æ€§æ’å€¼å’Œè½¬ç½®å·ç§¯ï¼Œæé«˜åˆ†å‰²è¾¹ç•Œç²¾åº¦ã€‚</li>
<li>GMLN-BTSæ¨¡å‹åœ¨BraTS2017æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†é«˜å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a610223f8ec735b429d3674866854fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eade698e444a8ff8a9c6dcb2592ca947.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6063bd70440ce86c5a7c06f9d8be2cb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0b807b4dd008cf802ac53a857960e7a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Advanced-U-Net-Architectures-with-CNN-Backbones-for-Automated-Lung-Cancer-Detection-and-Segmentation-in-Chest-CT-Images"><a href="#Advanced-U-Net-Architectures-with-CNN-Backbones-for-Automated-Lung-Cancer-Detection-and-Segmentation-in-Chest-CT-Images" class="headerlink" title="Advanced U-Net Architectures with CNN Backbones for Automated Lung   Cancer Detection and Segmentation in Chest CT Images"></a>Advanced U-Net Architectures with CNN Backbones for Automated Lung   Cancer Detection and Segmentation in Chest CT Images</h2><p><strong>Authors:Alireza Golkarieha, Kiana Kiashemshakib, Sajjad Rezvani Boroujenic, Nasibeh Asadi Isakand</strong></p>
<p>This study investigates the effectiveness of U-Net architectures integrated with various convolutional neural network (CNN) backbones for automated lung cancer detection and segmentation in chest CT images, addressing the critical need for accurate diagnostic tools in clinical settings. A balanced dataset of 832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed using Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to 128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50, VGG16, and Xception, to segment lung regions. After segmentation, CNN-based classifiers and hybrid models combining CNN feature extraction with traditional machine learning classifiers (Support Vector Machine, Random Forest, and Gradient Boosting) were evaluated using 5-fold cross-validation. Metrics included accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC. U-Net with ResNet50 achieved the best performance for cancerous lungs (Dice: 0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for non-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For classification, the CNN model using U-Net with Xception achieved 99.1 percent accuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid CNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent F1-score. Compared to prior methods, our framework consistently outperformed existing models. In conclusion, combining U-Net with advanced CNN backbones provides a powerful method for both segmentation and classification of lung cancer in CT scans, supporting early diagnosis and clinical decision-making. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ç»“åˆä¸åŒå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰éª¨å¹²çš„U-Netæ¶æ„åœ¨èƒ¸éƒ¨CTå›¾åƒä¸­è‡ªåŠ¨æ£€æµ‹ä¸åˆ†å‰²è‚ºç™Œçš„æœ‰æ•ˆæ€§ï¼Œè¿™è§£å†³äº†ä¸´åºŠç¯å¢ƒä¸­å¯¹å‡†ç¡®è¯Šæ–­å·¥å…·çš„å…³é”®éœ€æ±‚ã€‚ä½¿ç”¨å¯¹æ¯”é™åˆ¶è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–ï¼ˆCLAHEï¼‰å¯¹åŒ…å«832å¼ èƒ¸éƒ¨CTå›¾åƒï¼ˆå…¶ä¸­416å¼ ä¸ºç™Œå˜ï¼Œ416å¼ ä¸ºéç™Œå˜ï¼‰çš„å¹³è¡¡æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶å°†å…¶å¤§å°è°ƒæ•´ä¸º128x128åƒç´ ã€‚æœ¬ç ”ç©¶å¼€å‘äº†ä¸‰ç§ä½¿ç”¨ResNet50ã€VGG16å’ŒXceptionä½œä¸ºéª¨å¹²çš„U-Netæ¨¡å‹æ¥åˆ†å‰²è‚ºéƒ¨åŒºåŸŸã€‚åˆ†å‰²åï¼ŒåŸºäºCNNçš„åˆ†ç±»å™¨ä»¥åŠç»“åˆCNNç‰¹å¾æå–ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ†ç±»å™¨ï¼ˆæ”¯æŒå‘é‡æœºã€éšæœºæ£®æ—å’Œæ¢¯åº¦å¢å¼ºï¼‰çš„æ··åˆæ¨¡å‹ï¼Œä½¿ç”¨äº”æŠ˜äº¤å‰éªŒè¯è¿›è¡Œè¯„ä¼°ã€‚è¯„ä»·æŒ‡æ ‡åŒ…æ‹¬å‡†ç¡®åº¦ã€ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°ã€Diceç³»æ•°å’ŒROC-AUCã€‚ä½¿ç”¨ResNet50çš„U-Netåœ¨ç™Œå˜è‚ºéƒ¨æ£€æµ‹æ–¹é¢è¡¨ç°æœ€ä½³ï¼ˆDiceï¼š0.9495ï¼Œå‡†ç¡®åº¦ï¼š0.9735ï¼‰ï¼Œè€Œä½¿ç”¨VGG16çš„U-Netåœ¨éç™Œå˜åˆ†å‰²æ–¹é¢è¡¨ç°æœ€ä½³ï¼ˆDiceï¼š0.9532ï¼Œå‡†ç¡®åº¦ï¼š0.9513ï¼‰ã€‚åœ¨åˆ†ç±»æ–¹é¢ï¼Œä½¿ç”¨Xceptionçš„CNNæ¨¡å‹å®ç°äº†99.1%çš„å‡†ç¡®åº¦ã€99.74%çš„å¬å›ç‡å’Œ99.42%çš„F1åˆ†æ•°ã€‚æ··åˆCNN-SVM-Xceptionæ¨¡å‹å®ç°äº†96.7%çš„å‡†ç¡®åº¦å’Œ97.88%çš„F1åˆ†æ•°ã€‚ä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å§‹ç»ˆä¼˜äºç°æœ‰æ¨¡å‹ã€‚æ€»ä¹‹ï¼Œå°†U-Netä¸å…ˆè¿›çš„CNNéª¨å¹²ç›¸ç»“åˆï¼Œä¸ºCTæ‰«æä¸­çš„è‚ºç™Œåˆ†å‰²å’Œåˆ†ç±»æä¾›äº†ä¸€ç§å¼ºå¤§çš„æ–¹æ³•ï¼Œæ”¯æŒæ—©æœŸè¯Šæ–­å’Œä¸´åºŠå†³ç­–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09898v1">PDF</a> This manuscript has 20 pages and 10 figures. It is submitted to the   Journal â€˜Scientific Reportsâ€™</p>
<p><strong>Summary</strong><br>    æœ¬ç ”ç©¶åˆ©ç”¨U-Netæ¶æ„ç»“åˆä¸åŒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸»å¹²ç½‘ç»œï¼Œå¯¹èƒ¸éƒ¨CTå›¾åƒä¸­çš„è‚ºç™Œè¿›è¡Œè‡ªåŠ¨æ£€æµ‹å’Œåˆ†å‰²ã€‚ç ”ç©¶é‡‡ç”¨å‡è¡¡æ•°æ®é›†ï¼Œå¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†å¹¶è°ƒæ•´å¤§å°ï¼Œä½¿ç”¨ResNet50ã€VGG16å’ŒXceptionä½œä¸ºU-Netçš„ä¸»å¹²è¿›è¡Œè‚ºåŒºåŸŸåˆ†å‰²ã€‚è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬å‡†ç¡®åº¦ã€ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°ã€Diceç³»æ•°å’ŒROC-AUCã€‚U-Netä¸ResNet50çš„ç»“åˆåœ¨ç™Œç—‡è‚ºéƒ¨è¡¨ç°æœ€ä½³ï¼Œè€ŒU-Netä¸VGG16çš„ç»“åˆåœ¨éç™Œç—‡åˆ†å‰²ä¸­è¡¨ç°æœ€ä½³ã€‚åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨U-Netä¸Xceptionçš„CNNæ¨¡å‹è¡¨ç°çªå‡ºã€‚ä¸å…ˆå‰æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬ç ”ç©¶æ¡†æ¶å§‹ç»ˆä¼˜äºç°æœ‰æ¨¡å‹ï¼Œä¸ºè‚ºç™Œçš„æ—©æœŸè¯Šæ–­å’Œä¸´åºŠå†³ç­–æä¾›æœ‰åŠ›æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ä½¿ç”¨U-Netæ¶æ„ä¸å¤šç§CNNä¸»å¹²ç½‘ç»œç»“åˆï¼Œç”¨äºè‡ªåŠ¨æ£€æµ‹ä¸åˆ†å‰²èƒ¸éƒ¨CTå›¾åƒä¸­çš„è‚ºç™Œã€‚</li>
<li>é‡‡ç”¨å‡è¡¡æ•°æ®é›†å¹¶è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥é€‚åº”æ¨¡å‹è®­ç»ƒã€‚</li>
<li>U-Netä¸ResNet50ç»„åˆåœ¨ç™Œç—‡è‚ºéƒ¨åˆ†å‰²ä¸­è¡¨ç°æœ€ä½³ï¼Œè€ŒU-Netä¸VGG16ç»„åˆåœ¨éç™Œç—‡åˆ†å‰²ä¸­æœ€ä½³ã€‚</li>
<li>åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨U-Netä¸Xceptionçš„CNNæ¨¡å‹è¡¨ç°å‡ºé«˜å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶ç›¸è¾ƒäºå…ˆå‰çš„æ–¹æ³•æœ‰ä¼˜å¼‚è¡¨ç°ã€‚</li>
<li>ç»“åˆU-Netä¸å…ˆè¿›çš„CNNä¸»å¹²ç½‘ç»œä¸ºè‚ºç™Œçš„åˆ†å‰²å’Œåˆ†ç±»æä¾›äº†å¼ºå¤§æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-80c526a170325309ac8395c07f7257a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2273e481d8a31768a686eaed183ad748.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62c65105575d87857f58b03d42d57442.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-874c5f37fe9ae71d9b60b3c524619948.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MENTOR-Efficient-Multimodal-Conditioned-Tuning-for-Autoregressive-Vision-Generation-Models"><a href="#MENTOR-Efficient-Multimodal-Conditioned-Tuning-for-Autoregressive-Vision-Generation-Models" class="headerlink" title="MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive   Vision Generation Models"></a>MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive   Vision Generation Models</h2><p><strong>Authors:Haozhe Zhao, Zefan Cai, Shuzheng Si, Liang Chen, Jiuxiang Gu, Wen Xiao, Junjie Hu</strong></p>
<p>Recent text-to-image models produce high-quality results but still struggle with precise visual control, balancing multimodal inputs, and requiring extensive training for complex multimodal image generation. To address these limitations, we propose MENTOR, a novel autoregressive (AR) framework for efficient Multimodal-conditioned Tuning for Autoregressive multimodal image generation. MENTOR combines an AR image generator with a two-stage training paradigm, enabling fine-grained, token-level alignment between multimodal inputs and image outputs without relying on auxiliary adapters or cross-attention modules. The two-stage training consists of: (1) a multimodal alignment stage that establishes robust pixel- and semantic-level alignment, followed by (2) a multimodal instruction tuning stage that balances the integration of multimodal inputs and enhances generation controllability. Despite modest model size, suboptimal base components, and limited training resources, MENTOR achieves strong performance on the DreamBench++ benchmark, outperforming competitive baselines in concept preservation and prompt following. Additionally, our method delivers superior image reconstruction fidelity, broad task adaptability, and improved training efficiency compared to diffusion-based methods. Dataset, code, and models are available at: <a target="_blank" rel="noopener" href="https://github.com/HaozheZhao/MENTOR">https://github.com/HaozheZhao/MENTOR</a> </p>
<blockquote>
<p>æœ€è¿‘çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹äº§ç”Ÿäº†é«˜è´¨é‡çš„ç»“æœï¼Œä½†åœ¨ç²¾ç¡®è§†è§‰æ§åˆ¶ã€å¹³è¡¡å¤šæ¨¡å¼è¾“å…¥å’Œå¤æ‚å¤šæ¨¡å¼å›¾åƒç”Ÿæˆçš„å¹¿æ³›è®­ç»ƒæ–¹é¢ä»å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†MENTORï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºé«˜æ•ˆå¤šæ¨¡å¼æ¡ä»¶è°ƒæ•´çš„è‡ªå›å½’ï¼ˆARï¼‰æ¡†æ¶ï¼Œç”¨äºè‡ªå›å½’å¤šæ¨¡å¼å›¾åƒç”Ÿæˆã€‚MENTORç»“åˆARå›¾åƒç”Ÿæˆå™¨å’Œä¸¤é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–è¾…åŠ©é€‚é…å™¨æˆ–äº¤å‰æ³¨æ„æ¨¡å—çš„æƒ…å†µä¸‹ï¼Œå®ç°å¤šæ¨¡å¼è¾“å…¥å’Œå›¾åƒè¾“å‡ºä¹‹é—´çš„ç»†ç²’åº¦ã€ä»¤ç‰Œçº§å¯¹é½ã€‚ä¸¤é˜¶æ®µè®­ç»ƒåŒ…æ‹¬ï¼šï¼ˆ1ï¼‰å¤šæ¨¡å¼å¯¹é½é˜¶æ®µï¼Œå»ºç«‹ç¨³å¥çš„åƒç´ å’Œè¯­ä¹‰çº§å¯¹é½ï¼Œç„¶åæ˜¯ï¼ˆ2ï¼‰å¤šæ¨¡å¼æŒ‡ä»¤è°ƒæ•´é˜¶æ®µï¼Œå¹³è¡¡å¤šæ¨¡å¼è¾“å…¥çš„é›†æˆï¼Œå¢å¼ºç”Ÿæˆçš„å¯æ§æ€§ã€‚å°½ç®¡æ¨¡å‹è§„æ¨¡é€‚ä¸­ï¼ŒåŸºç¡€ç»„ä»¶ä¸å¤Ÿç†æƒ³ï¼Œä¸”è®­ç»ƒèµ„æºæœ‰é™ï¼Œä½†MENTORåœ¨DreamBench++åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œåœ¨æ¦‚å¿µä¿ç•™å’Œæç¤ºéµå¾ªæ–¹é¢è¶…è¶Šäº†ç«äº‰åŸºå‡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒé‡å»ºä¿çœŸåº¦ã€å¹¿æ³›çš„ä»»åŠ¡é€‚åº”æ€§å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢ä¼˜äºåŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HaozheZhao/MENTOR%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HaozheZhao/MENTORæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09574v1">PDF</a> 24 pages,12 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMENTORçš„æ–°å‹è‡ªå›å½’ï¼ˆARï¼‰æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆçš„å¤šæ¨¡æ€æ¡ä»¶è°ƒèŠ‚è‡ªå›å½’å¤šæ¨¡æ€å›¾åƒç”Ÿæˆã€‚è¯¥æ¡†æ¶ç»“åˆäº†ARå›¾åƒç”Ÿæˆå™¨å’Œä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼Œèƒ½å¤Ÿåœ¨ä¸ä½¿ç”¨è¾…åŠ©é€‚é…å™¨æˆ–äº¤å‰æ³¨æ„åŠ›æ¨¡å—çš„æƒ…å†µä¸‹ï¼Œå®ç°å¤šæ¨¡æ€è¾“å…¥å’Œå›¾åƒè¾“å‡ºä¹‹é—´çš„ç²¾ç»†ç²’åº¦ã€ä»¤ç‰Œçº§å¯¹é½ã€‚è¯¥æ¡†æ¶è§£å†³äº†ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹åœ¨ç²¾ç¡®è§†è§‰æ§åˆ¶ã€å¹³è¡¡å¤šæ¨¡æ€è¾“å…¥å’Œå¤æ‚å¤šæ¨¡æ€å›¾åƒç”Ÿæˆçš„è®­ç»ƒéœ€æ±‚æ–¹é¢çš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MENTORæ˜¯ä¸€ä¸ªè‡ªå›å½’ï¼ˆARï¼‰æ¡†æ¶ï¼Œç”¨äºå¤šæ¨¡æ€å›¾åƒç”Ÿæˆã€‚</li>
<li>å®ƒé€šè¿‡ç»“åˆARå›¾åƒç”Ÿæˆå™¨å’Œä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼å®ç°é«˜æ•ˆçš„å¤šæ¨¡æ€æ¡ä»¶è°ƒèŠ‚ã€‚</li>
<li>MENTORå®ç°äº†å¤šæ¨¡æ€è¾“å…¥å’Œå›¾åƒè¾“å‡ºä¹‹é—´çš„ç²¾ç»†ç²’åº¦ã€ä»¤ç‰Œçº§å¯¹é½ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€å¯¹é½é˜¶æ®µå’Œå¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´é˜¶æ®µã€‚</li>
<li>MENTORåœ¨DreamBench++åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä¼˜äºç«äº‰å¯¹æ‰‹åŸºçº¿ï¼Œåœ¨æ¦‚å¿µä¿ç•™å’Œæç¤ºéµå¾ªæ–¹é¢ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å›¾åƒé‡å»ºä¿çœŸåº¦ã€å¹¿æ³›çš„ä»»åŠ¡é€‚åº”æ€§å’Œè®­ç»ƒæ•ˆç‡æ–¹é¢å‡ä¼˜äºåŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚</li>
<li>æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HaozheZhao/MENTOR%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/HaozheZhao/MENTORä¸Šè·å¾—ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09574">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-258f0efbb7e914dddded9081e53cf895.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cca8e535883a7753dc2b38552284b83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8a1768a0c75ef6c0209965c30036217.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4ad4fdf8ff88ba7ddff5b097b6b6072.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AlphaVAE-Unified-End-to-End-RGBA-Image-Reconstruction-and-Generation-with-Alpha-Aware-Representation-Learning"><a href="#AlphaVAE-Unified-End-to-End-RGBA-Image-Reconstruction-and-Generation-with-Alpha-Aware-Representation-Learning" class="headerlink" title="AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation   with Alpha-Aware Representation Learning"></a>AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation   with Alpha-Aware Representation Learning</h2><p><strong>Authors:Zile Wang, Hao Yu, Jiabo Zhan, Chun Yuan</strong></p>
<p>Recent advances in latent diffusion models have achieved remarkable results in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress and reconstruct pixel data at low computational cost. However, the generation of transparent or layered content (RGBA image) remains largely unexplored, due to the lack of large-scale benchmarks. In this work, we propose ALPHA, the first comprehensive RGBA benchmark that adapts standard RGB metrics to four-channel images via alpha blending over canonical backgrounds. We further introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB VAE by incorporating a dedicated alpha channel. The model is trained with a composite objective that combines alpha-blended pixel reconstruction, patch-level fidelity, perceptual consistency, and dual KL divergence constraints to ensure latent fidelity across both RGB and alpha representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase in SSIM over LayerDiffuse in reconstruction. It also enables superior transparent image generation when fine-tuned within a latent diffusion framework. Our code, data, and models are released on <a target="_blank" rel="noopener" href="https://github.com/o0o0o00o0/AlphaVAE">https://github.com/o0o0o00o0/AlphaVAE</a> for reproducibility. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆlatent diffusion modelsï¼‰çš„è¿›æ­¥åœ¨é«˜ä¿çœŸRGBå›¾åƒåˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒçš„å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEsï¼‰ä»¥è¾ƒä½çš„è®¡ç®—æˆæœ¬å‹ç¼©å’Œé‡å»ºåƒç´ æ•°æ®ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œé€æ˜æˆ–åˆ†å±‚å†…å®¹ï¼ˆRGBAå›¾åƒï¼‰çš„ç”Ÿæˆä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ALPHAï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªé€‚åº”æ ‡å‡†RGBæŒ‡æ ‡çš„å…¨é¢RGBAåŸºå‡†æµ‹è¯•ï¼Œé€šè¿‡æ ‡å‡†èƒŒæ™¯çš„alphaæ··åˆå°†å…¶åº”ç”¨äºå››é€šé“å›¾åƒã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ALPHAVAEï¼Œè¿™æ˜¯ä¸€ç§ç»Ÿä¸€çš„ç«¯åˆ°ç«¯RGBAå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼Œå®ƒé€šè¿‡å¼•å…¥ä¸“ç”¨çš„alphaé€šé“æ‰©å±•äº†é¢„è®­ç»ƒçš„RGB VAEã€‚è¯¥æ¨¡å‹é‡‡ç”¨ç»„åˆç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œç»“åˆäº†alphaæ··åˆåƒç´ é‡å»ºã€è¡¥ä¸çº§åˆ«çš„ä¿çœŸåº¦ã€æ„ŸçŸ¥ä¸€è‡´æ€§å’ŒåŒé‡KLæ•£åº¦çº¦æŸï¼Œä»¥ç¡®ä¿RGBå’Œalphaè¡¨ç¤ºä¸­çš„æ½œåœ¨ä¿çœŸåº¦ã€‚æˆ‘ä»¬çš„RGBA VAEä»…ä½¿ç”¨8Kå›¾åƒè¿›è¡Œè®­ç»ƒï¼Œè€Œå…ˆå‰çš„æ–¹æ³•ä½¿ç”¨äº†1Må›¾åƒã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå®ƒåœ¨PSNRä¸Šæé«˜äº†4.9 dBï¼Œåœ¨SSIMä¸Šæé«˜äº†3.2%ã€‚åœ¨æ½œåœ¨æ‰©æ•£æ¡†æ¶å†…è¿›è¡Œå¾®è°ƒæ—¶ï¼Œå®ƒè¿˜èƒ½å¤Ÿå®ç°æ›´å‡ºè‰²çš„é€æ˜å›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/o0o0o00o0/AlphaVAE%E4%B8%8A%E5%8F%91%E5%B8%83%EF%BC%8C%E4%BB%A5%E7%A1%AE%E4%BF%9D%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E3%80%82">https://github.com/o0o0o00o0/AlphaVAEä¸Šå‘å¸ƒï¼Œä»¥ç¡®ä¿å¯é‡å¤æ€§ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09308v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨RGBAå›¾åƒåˆæˆæ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚æå‡ºäº†ä¸€ç§æ–°çš„RGBAåŸºå‡†æµ‹è¯•ALPHAï¼Œä»¥åŠä¸€ä¸ªç»Ÿä¸€çš„ç«¯åˆ°ç«¯RGBA VAEæ¨¡å‹ALPHAVAEã€‚è¯¥æ¨¡å‹é€šè¿‡èå…¥ä¸“é—¨çš„alphaé€šé“ï¼Œæ‰©å±•äº†é¢„è®­ç»ƒçš„RGB VAEã€‚é€šè¿‡å¤åˆç›®æ ‡å‡½æ•°è®­ç»ƒæ¨¡å‹ï¼Œå®ç°äº†RGBAå’Œalphaè¡¨ç¤ºçš„æ½œåœ¨ä¿çœŸåº¦ã€‚ç›¸è¾ƒäºå…ˆå‰çš„æ–¹æ³•ï¼Œä»…åœ¨8Kå›¾åƒä¸Šè®­ç»ƒçš„ALPHAVAEåœ¨é‡å»ºä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨RGBAå›¾åƒåˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„VAEä»¥ä½è®¡ç®—æˆæœ¬å‹ç¼©å’Œé‡å»ºåƒç´ æ•°æ®ã€‚</li>
<li>ç¼ºä¹å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•æ˜¯é€æ˜æˆ–åˆ†å±‚å†…å®¹ç”Ÿæˆï¼ˆRGBAå›¾åƒï¼‰çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†é¦–ä¸ªRGBAåŸºå‡†æµ‹è¯•ALPHAï¼Œè¯¥æµ‹è¯•é€šè¿‡alphaæ··åˆåœ¨æ ‡å‡†èƒŒæ™¯ä¸Šé€‚åº”æ ‡å‡†RGBæŒ‡æ ‡ã€‚</li>
<li>ä»‹ç»äº†ALPHAVAEï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„ç«¯åˆ°ç«¯RGBA VAEæ¨¡å‹ï¼Œé€šè¿‡èå…¥alphaé€šé“æ‰©å±•äº†é¢„è®­ç»ƒçš„RGB VAEã€‚</li>
<li>ALPHAVAEæ¨¡å‹é€šè¿‡å¤åˆç›®æ ‡å‡½æ•°è¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬alphaæ··åˆåƒç´ é‡å»ºã€è¡¥ä¸çº§åˆ«çš„ä¿çœŸåº¦ã€æ„ŸçŸ¥ä¸€è‡´æ€§å’ŒåŒé‡KLæ•£åº¦çº¦æŸã€‚</li>
<li>åœ¨ä»…ä½¿ç”¨8Kå›¾åƒè®­ç»ƒçš„ALPHAVAEåœ¨é‡å»ºä»»åŠ¡ä¸Šè¾ƒä¹‹å‰çš„æ–¹æ³•æœ‰äº†æ˜¾è‘—æ”¹å–„ï¼Œå®ç°äº†PSNRçš„+4.9 dBæå‡å’ŒSSIMçš„+3.2%å¢é•¿ã€‚</li>
<li>ALPHAVAEåœ¨æ½œåœ¨æ‰©æ•£æ¡†æ¶å†…è¿›è¡Œå¾®è°ƒåï¼Œèƒ½å¤Ÿå®ç°æ›´ä¼˜è´¨çš„é€æ˜å›¾åƒç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1a42053a09e2eca7735644501949e4f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0bdac09e9859e76ee8c3410ae624ed6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97a891b6e3977d2523bc41b13c1f375c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3cdffe223fd29b1b2bbf092e5bc14480.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Calibrated-and-Robust-Foundation-Models-for-Vision-Language-and-Medical-Image-Tasks-Under-Distribution-Shift"><a href="#Calibrated-and-Robust-Foundation-Models-for-Vision-Language-and-Medical-Image-Tasks-Under-Distribution-Shift" class="headerlink" title="Calibrated and Robust Foundation Models for Vision-Language and Medical   Image Tasks Under Distribution Shift"></a>Calibrated and Robust Foundation Models for Vision-Language and Medical   Image Tasks Under Distribution Shift</h2><p><strong>Authors:Behraj Khan, Tahir Syed</strong></p>
<p>Foundation models like CLIP and SAM have transformed computer vision and medical imaging via low-shot transfer learning. However, deployment of these models hindered by two key challenges: \textit{distribution shift} between training and test data, and \textit{confidence misalignment} that leads to overconfident incorrect predictions. These issues manifest differently in vision-language classification and medical segmentation tasks, yet existing solutions remain domain-specific. We propose \textit{StaRFM}, a unified framework addressing both challenges. It introduces a Fisher information penalty (FIP), extended to 3D medical data via patch-wise regularization, to reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence misalignment penalty (CMP), reformulated for voxel-level predictions, calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP minimizes calibration error through Brier score optimization. StaRFM shows consistent performance like \texttt{+}3.5% accuracy and 28% lower ECE on 19 vision datasets (e.g., ImageNet, Office-Home), 84.7% DSC and 4.8mm HD95 in medical segmentation (e.g., BraTS, ATLAS), and 40% lower cross-domain performance gap compared to prior benchmarking methods. The framework is plug-and-play, requiring minimal architectural changes for seamless integration with foundation models. Code and models will be released at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/StaRFM-C0CD/README.md">https://anonymous.4open.science/r/StaRFM-C0CD/README.md</a> </p>
<blockquote>
<p>ç±»ä¼¼CLIPå’ŒSAMçš„åŸºçŸ³æ¨¡å‹å·²é€šè¿‡å°æ ·æœ¬è¿ç§»å­¦ä¹ è½¬å˜äº†è®¡ç®—æœºè§†è§‰å’ŒåŒ»å­¦å½±åƒé¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„éƒ¨ç½²å—åˆ°ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜çš„å½±å“ï¼šè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ä¹‹é—´çš„<em>åˆ†å¸ƒåç§»</em>ï¼Œä»¥åŠå¯¼è‡´è¿‡äºè‡ªä¿¡çš„é¢„æµ‹é”™è¯¯çš„<em>ç½®ä¿¡åº¦ä¸åŒ¹é…</em>ã€‚è¿™äº›é—®é¢˜åœ¨è§†è§‰è¯­è¨€åˆ†ç±»å’ŒåŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸åŒçš„å½¢å¼ï¼Œä½†ç°æœ‰è§£å†³æ–¹æ¡ˆä»ç„¶ä»…é™äºç‰¹å®šé¢†åŸŸã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶<em>StaRFM</em>ï¼Œæ—¨åœ¨è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ã€‚å®ƒå¼•å…¥äº†ä¸€ä¸ªFisherä¿¡æ¯æƒ©ç½šï¼ˆFIPï¼‰ï¼Œé€šè¿‡è¡¥ä¸çº§æ­£åˆ™åŒ–æ‰©å±•åˆ°3DåŒ»å­¦æ•°æ®ï¼Œä»¥å‡å°‘CLIPå’ŒSAMåµŒå…¥ä¸­çš„åå˜é‡åç§»ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ä½“ç´ çº§é¢„æµ‹çš„ç½®ä¿¡åº¦ä¸åŒ¹é…æƒ©ç½šï¼ˆCMPï¼‰æ ¡å‡†äº†åˆ†å‰²ä»»åŠ¡ä¸­çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šæ¨å¯¼äº†PAC-Bayesè¾¹ç•Œï¼Œæ˜¾ç¤ºFIPé€šè¿‡Fisher-RaoèŒƒæ•°æ§åˆ¶æ³›åŒ–ï¼Œè€ŒCMPé€šè¿‡Brierå¾—åˆ†ä¼˜åŒ–æœ€å°åŒ–æ ¡å‡†è¯¯å·®ã€‚StaRFMåœ¨19ä¸ªè§†è§‰æ•°æ®é›†ï¼ˆä¾‹å¦‚ImageNetã€Office-Homeï¼‰ä¸Šè¡¨ç°å‡ºä¸€è‡´çš„æ€§èƒ½ï¼Œå¦‚æé«˜+3.5%çš„å‡†ç¡®æ€§å’Œé™ä½28%çš„ECEï¼Œåœ¨åŒ»å­¦åˆ†å‰²ï¼ˆä¾‹å¦‚BraTSã€ATLASï¼‰æ–¹é¢ï¼Œè¾¾åˆ°84.7%çš„DSCå’Œ4.8mmçš„HD95ï¼Œä¸å…ˆå‰çš„åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè·¨åŸŸæ€§èƒ½å·®è·é™ä½äº†40%ã€‚è¯¥æ¡†æ¶å³æ’å³ç”¨ï¼Œæ— éœ€å¯¹ç°æœ‰æ¶æ„è¿›è¡Œå¤§é‡æ”¹åŠ¨ï¼Œå³å¯æ— ç¼é›†æˆåˆ°åŸºçŸ³æ¨¡å‹ä¸­ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/StaRFM-C0CD/README.md%E5%8F%91%E5%B8%83%E3%80%82">https://anonymous.4open.science/r/StaRFM-C0CD/README.mdå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09222v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºCLIPå’ŒSAMç­‰åŸºç¡€æ¨¡å‹ï¼Œæœ¬æ–‡æå‡ºäº†StaRFMæ¡†æ¶æ¥è§£å†³è®¡ç®—æœºè§†è§‰å’ŒåŒ»å­¦æˆåƒä¸­é‡åˆ°çš„ä¸¤å¤§æŒ‘æˆ˜ï¼šè®­ç»ƒä¸æµ‹è¯•æ•°æ®é—´çš„åˆ†å¸ƒåç§»å’Œä¿¡å¿ƒä¸åŒ¹é…å¯¼è‡´çš„è¿‡åº¦è‡ªä¿¡é”™è¯¯é¢„æµ‹ã€‚é€šè¿‡å¼•å…¥Fisherä¿¡æ¯æƒ©ç½šï¼ˆFIPï¼‰å’Œç½®ä¿¡åº¦ä¸åŒ¹é…æƒ©ç½šï¼ˆCMPï¼‰ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆå‡å°‘äº†åŸºç¡€æ¨¡å‹ä¸­çš„åå˜é‡åç§»ï¼Œå¹¶æ ¡å‡†äº†åˆ†å‰²ä»»åŠ¡ä¸­çš„ä¸ç¡®å®šæ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒStaRFMåœ¨è§†è§‰æ•°æ®é›†å’ŒåŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºç¡€æ¨¡å‹å¦‚CLIPå’ŒSAMå·²æ¨åŠ¨è®¡ç®—æœºè§†è§‰å’ŒåŒ»å­¦æˆåƒçš„è¿›æ­¥ï¼Œä½†å­˜åœ¨åˆ†å¸ƒåç§»å’Œä¿¡å¿ƒä¸åŒ¹é…ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>StaRFMæ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œé€šè¿‡å¼•å…¥Fisherä¿¡æ¯æƒ©ç½šï¼ˆFIPï¼‰å‡å°‘åå˜é‡åç§»ï¼Œå¹¶ç”¨ç½®ä¿¡åº¦ä¸åŒ¹é…æƒ©ç½šï¼ˆCMPï¼‰æ ¡å‡†åˆ†å‰²ä»»åŠ¡ä¸­çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>FIPé€šè¿‡Fisher-RaoèŒƒæ•°æ§åˆ¶æ³›åŒ–ï¼Œè€ŒCMPåˆ™é€šè¿‡Brierå¾—åˆ†ä¼˜åŒ–æ¥æœ€å°åŒ–æ ¡å‡†è¯¯å·®ã€‚</li>
<li>StaRFMåœ¨å¤šä¸ªè§†è§‰æ•°æ®é›†å’ŒåŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¦‚æé«˜å‡†ç¡®ç‡ã€é™ä½è¿‡åº¦è‡ªä¿¡é¢„æµ‹ç­‰ã€‚</li>
<li>è¯¥æ¡†æ¶å…·æœ‰å³æ’å³ç”¨ç‰¹æ€§ï¼Œå¯æ— ç¼é›†æˆäºåŸºç¡€æ¨¡å‹ï¼Œä¸”å¯¹æ¶æ„æ”¹åŠ¨éœ€æ±‚è¾ƒå°ã€‚</li>
<li>ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å‘å¸ƒåœ¨æŒ‡å®šé“¾æ¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ba6b62e4b309e43fd5779cbbacaf75a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a9937f66e4120f0570ece49158f296c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-863982903a1c8c9601b0c440c0cb3a2e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-13a01ce61ce047e83414930b4f9d2f89.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="360-Degree-Full-view-Image-Segmentation-by-Spherical-Convolution-compatible-with-Large-scale-Planar-Pre-trained-Models"><a href="#360-Degree-Full-view-Image-Segmentation-by-Spherical-Convolution-compatible-with-Large-scale-Planar-Pre-trained-Models" class="headerlink" title="360-Degree Full-view Image Segmentation by Spherical Convolution   compatible with Large-scale Planar Pre-trained Models"></a>360-Degree Full-view Image Segmentation by Spherical Convolution   compatible with Large-scale Planar Pre-trained Models</h2><p><strong>Authors:Jingguo Liu, Han Yu, Shigang Li, Jianfeng Li</strong></p>
<p>Due to the current lack of large-scale datasets at the million-scale level, tasks involving panoramic images predominantly rely on existing two-dimensional pre-trained image benchmark models as backbone networks. However, these networks are not equipped to recognize the distortions and discontinuities inherent in panoramic images, which adversely affects their performance in such tasks. In this paper, we introduce a novel spherical sampling method for panoramic images that enables the direct utilization of existing pre-trained models developed for two-dimensional images. Our method employs spherical discrete sampling based on the weights of the pre-trained models, effectively mitigating distortions while achieving favorable initial training values. Additionally, we apply the proposed sampling method to panoramic image segmentation, utilizing features obtained from the spherical model as masks for specific channel attentions, which yields commendable results on commonly used indoor datasets, Stanford2D3D. </p>
<blockquote>
<p>ç”±äºå½“å‰ç¼ºä¹å¤§è§„æ¨¡ç™¾ä¸‡çº§åˆ«çš„æ•°æ®é›†ï¼Œæ¶‰åŠå…¨æ™¯å›¾åƒçš„ä»»åŠ¡ä¸»è¦ä¾èµ–äºç°æœ‰çš„äºŒç»´é¢„è®­ç»ƒå›¾åƒåŸºå‡†æ¨¡å‹ä½œä¸ºéª¨å¹²ç½‘ç»œã€‚ç„¶è€Œï¼Œè¿™äº›ç½‘ç»œå¹¶ä¸å…·å¤‡è¯†åˆ«å…¨æ™¯å›¾åƒå›ºæœ‰çš„å¤±çœŸå’Œæ–­è£‚çš„èƒ½åŠ›ï¼Œè¿™å¯¹å…¶åœ¨æ­¤ç±»ä»»åŠ¡ä¸­çš„æ€§èƒ½äº§ç”Ÿäº†ä¸åˆ©å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸ºå…¨æ™¯å›¾åƒå¼•å…¥äº†ä¸€ç§æ–°å‹çƒé¢é‡‡æ ·æ–¹æ³•ï¼Œä½¿å¾—èƒ½å¤Ÿç›´æ¥ä½¿ç”¨ä¸ºäºŒç»´å›¾åƒå¼€å‘çš„ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡é‡‡ç”¨çƒé¢ç¦»æ•£é‡‡æ ·ï¼Œæœ‰æ•ˆåœ°ç¼“è§£äº†å¤±çœŸé—®é¢˜ï¼ŒåŒæ—¶å®ç°äº†è‰¯å¥½çš„åˆå§‹è®­ç»ƒå€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ‰€æé‡‡æ ·æ–¹æ³•åº”ç”¨äºå…¨æ™¯å›¾åƒåˆ†å‰²ï¼Œåˆ©ç”¨çƒé¢æ¨¡å‹å¾—åˆ°çš„ç‰¹å¾ä½œä¸ºç‰¹å®šé€šé“æ³¨æ„åŠ›çš„æ©è†œï¼Œè¿™åœ¨å¸¸ç”¨çš„å®¤å†…æ•°æ®é›†Stanford2D3Dä¸Šå–å¾—äº†ä»¤äººç§°èµçš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09216v1">PDF</a> This paper is accecpted by ICMEW 2025</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å…¨æ™¯å›¾åƒä»»åŠ¡ç¼ºä¹å¤§è§„æ¨¡æ•°æ®é›†çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çƒå½¢é‡‡æ ·æ–¹æ³•ï¼Œåˆ©ç”¨å·²æœ‰çš„äºŒç»´é¢„è®­ç»ƒå›¾åƒåŸºå‡†æ¨¡å‹ï¼Œé€šè¿‡çƒå½¢ç¦»æ•£é‡‡æ ·å’Œæƒé‡é¢„å¤„ç†ï¼Œæœ‰æ•ˆå‡è½»å…¨æ™¯å›¾åƒçš„å¤±çœŸé—®é¢˜å¹¶å®ç°è¾ƒå¥½çš„åˆå§‹è®­ç»ƒæ•ˆæœã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å°†è¯¥é‡‡æ ·æ–¹æ³•åº”ç”¨äºå…¨æ™¯å›¾åƒåˆ†å‰²ï¼Œåˆ©ç”¨çƒå½¢æ¨¡å‹ç‰¹å¾ä½œä¸ºç‰¹å®šé€šé“æ³¨æ„åŠ›çš„æ©è†œï¼Œåœ¨å¸¸ç”¨çš„å®¤å†…æ•°æ®é›†Stanford2D3Dä¸Šå–å¾—äº†è‰¯å¥½æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¨æ™¯å›¾åƒä»»åŠ¡ç¼ºä¹å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå› æ­¤ä¸»è¦ä¾èµ–ç°æœ‰çš„äºŒç»´é¢„è®­ç»ƒå›¾åƒåŸºå‡†æ¨¡å‹ã€‚</li>
<li>ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹æ— æ³•è¯†åˆ«å…¨æ™¯å›¾åƒçš„æ‰­æ›²å’Œä¸è¿ç»­æ€§ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„çƒå½¢é‡‡æ ·æ–¹æ³•ï¼Œä½¿ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹å¯ç›´æ¥åº”ç”¨äºå…¨æ™¯å›¾åƒã€‚</li>
<li>çƒå½¢é‡‡æ ·åŸºäºé¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡è¿›è¡Œç¦»æ•£é‡‡æ ·ï¼Œæœ‰æ•ˆå‡è½»å…¨æ™¯å›¾åƒçš„å¤±çœŸé—®é¢˜ã€‚</li>
<li>æœ¬æ–‡æ–¹æ³•åœ¨å®ç°è‰¯å¥½åˆå§‹è®­ç»ƒæ•ˆæœçš„åŒæ—¶ï¼Œè¿˜åº”ç”¨äºå…¨æ™¯å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>åˆ©ç”¨çƒå½¢æ¨¡å‹ç‰¹å¾ä½œä¸ºç‰¹å®šé€šé“æ³¨æ„åŠ›çš„æ©è†œï¼Œæé«˜äº†å…¨æ™¯å›¾åƒåˆ†å‰²çš„æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-90e8fe799ba7708b710c15a70e2a5e33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b51f03afb74e88fdfe93fbc6c67a7e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc38a5bd4e82cdcfe8452c024389c1e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6738f8c72975273171a4a408e14fc30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af74fb347f8b837620328f7e277526b6.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="From-Classical-Machine-Learning-to-Emerging-Foundation-Models-Review-on-Multimodal-Data-Integration-for-Cancer-Research"><a href="#From-Classical-Machine-Learning-to-Emerging-Foundation-Models-Review-on-Multimodal-Data-Integration-for-Cancer-Research" class="headerlink" title="From Classical Machine Learning to Emerging Foundation Models: Review on   Multimodal Data Integration for Cancer Research"></a>From Classical Machine Learning to Emerging Foundation Models: Review on   Multimodal Data Integration for Cancer Research</h2><p><strong>Authors:Amgad Muneer, Muhammad Waqas, Maliazurina B Saad, Eman Showkatian, Rukhmini Bandyopadhyay, Hui Xu, Wentao Li, Joe Y Chang, Zhongxing Liao, Cara Haymaker, Luisa Solis Soto, Carol C Wu, Natalie I Vokes, Xiuning Le, Lauren A Byers, Don L Gibbons, John V Heymach, Jianjun Zhang, Jia Wu</strong></p>
<p>Cancer research is increasingly driven by the integration of diverse data modalities, spanning from genomics and proteomics to imaging and clinical factors. However, extracting actionable insights from these vast and heterogeneous datasets remains a key challenge. The rise of foundation models (FMs) â€“ large deep-learning models pretrained on extensive amounts of data serving as a backbone for a wide range of downstream tasks â€“ offers new avenues for discovering biomarkers, improving diagnosis, and personalizing treatment. This paper presents a comprehensive review of widely adopted integration strategies of multimodal data to assist advance the computational approaches for data-driven discoveries in oncology. We examine emerging trends in machine learning (ML) and deep learning (DL), including methodological frameworks, validation protocols, and open-source resources targeting cancer subtype classification, biomarker discovery, treatment guidance, and outcome prediction. This study also comprehensively covers the shift from traditional ML to FMs for multimodal integration. We present a holistic view of recent FMs advancements and challenges faced during the integration of multi-omics with advanced imaging data. We identify the state-of-the-art FMs, publicly available multi-modal repositories, and advanced tools and methods for data integration. We argue that current state-of-the-art integrative methods provide the essential groundwork for developing the next generation of large-scale, pre-trained models poised to further revolutionize oncology. To the best of our knowledge, this is the first review to systematically map the transition from conventional ML to advanced FM for multimodal data integration in oncology, while also framing these developments as foundational for the forthcoming era of large-scale AI models in cancer research. </p>
<blockquote>
<p>ç™Œç—‡ç ”ç©¶æ­£æ—¥ç›Šå—åˆ°å¤šç§æ•°æ®æ¨¡æ€èåˆéœ€æ±‚çš„æ¨åŠ¨ï¼Œæ¶µç›–äº†åŸºå› ç»„å­¦ã€è›‹ç™½è´¨ç»„å­¦ã€æˆåƒå’Œä¸´åºŠå› ç´ ç­‰ã€‚ç„¶è€Œï¼Œä»å¤§é‡ä¸”å¤šæ ·çš„æ•°æ®é›†ä¸­æå–å¯æ“ä½œçš„è§è§£ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚éšç€é¢„è®­ç»ƒå¤§è§„æ¨¡æ•°æ®çš„æ·±åº¦æ¨¡å‹â€”â€”åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰çš„å…´èµ·ï¼Œå®ƒä¸ºå‘ç°ç”Ÿç‰©æ ‡å¿—ç‰©ã€æ”¹å–„è¯Šæ–­å’Œä¸ªæ€§åŒ–æ²»ç–—æä¾›äº†æ–°çš„é€”å¾„ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†å¹¿æ³›é‡‡ç”¨çš„å¤šæ¨¡æ€æ•°æ®èåˆç­–ç•¥ï¼Œæ—¨åœ¨å¸®åŠ©æ¨è¿›è‚¿ç˜¤å­¦æ•°æ®é©±åŠ¨å‹å‘ç°çš„è®¡ç®—æ–¹æ³•ã€‚æˆ‘ä»¬ç ”ç©¶äº†æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„æ–°å…´è¶‹åŠ¿ï¼ŒåŒ…æ‹¬æ–¹æ³•è®ºæ¡†æ¶ã€éªŒè¯åè®®ä»¥åŠé’ˆå¯¹ç™Œç—‡äºšå‹åˆ†ç±»ã€ç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°ã€æ²»ç–—æŒ‡å¯¼å’Œç»“æœé¢„æµ‹çš„å¼€æºèµ„æºã€‚æœ¬æ–‡ä¹Ÿå…¨é¢æ¦‚è¿°äº†ä»ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ åˆ°FMsè¿›è¡Œå¤šæ¨¡æ€èåˆçš„è½¬å˜ã€‚æˆ‘ä»¬æä¾›äº†å…³äºæœ€è¿‘FMså‘å±•å’Œåœ¨å¤šç»„å­¦ä¸é«˜çº§æˆåƒæ•°æ®èåˆè¿‡ç¨‹ä¸­æ‰€é¢ä¸´çš„æŒ‘æˆ˜çš„å…¨é¢äº†è§£ã€‚æˆ‘ä»¬ç¡®å®šäº†æœ€å…ˆè¿›çš„FMsã€å¯å…¬å¼€è®¿é—®çš„å¤šæ¨¡æ€å­˜å‚¨åº“ä»¥åŠæ•°æ®èåˆçš„é«˜çº§å·¥å…·å’Œæ–¹æ³•ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå½“å‰æœ€å…ˆè¿›çš„èåˆæ–¹æ³•æä¾›äº†å¼€å‘ä¸‹ä¸€ä»£å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„å¿…è¦åŸºç¡€ï¼Œæœ‰æœ›è¿›ä¸€æ­¥æ¨åŠ¨è‚¿ç˜¤å­¦çš„é©å‘½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæœ¬æ–‡æ˜¯å¯¹è‚¿ç˜¤å­¦ä¸­ä»ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ åˆ°å…ˆè¿›çš„FMç”¨äºå¤šæ¨¡æ€æ•°æ®èåˆè¿‡æ¸¡çš„é¦–æ¬¡ç³»ç»Ÿæ€§å›é¡¾ï¼ŒåŒæ—¶å°†è¿™äº›å‘å±•ä½œä¸ºæœªæ¥å¤§è§„æ¨¡ç™Œç—‡ç ”ç©¶äººå·¥æ™ºèƒ½æ¨¡å‹çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09028v1">PDF</a> 6 figures, 3 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>ç™Œç—‡ç ”ç©¶æ­£æ—¥ç›Šå—åˆ°å¤šæ¨¡æ€æ•°æ®æ•´åˆçš„é©±åŠ¨ï¼Œæ¶µç›–åŸºå› ç»„å­¦ã€è›‹ç™½è´¨ç»„å­¦ã€æˆåƒå’Œä¸´åºŠå› ç´ ç­‰å¤šä¸ªé¢†åŸŸã€‚ç„¶è€Œï¼Œä»å¤§é‡å¼‚è´¨æ•°æ®é›†ä¸­æå–å¯æ“ä½œçš„è§è§£ä»æ˜¯å…³é”®æŒ‘æˆ˜ã€‚éšç€é¢„è®­ç»ƒå¤§é‡æ•°æ®çš„æ·±åº¦å­¦ä¹ æ¨¡å‹â€”â€”åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰çš„å‡ºç°ï¼Œä¸ºå‘ç°ç”Ÿç‰©æ ‡å¿—ç‰©ã€æ”¹è¿›è¯Šæ–­å’Œæ²»ç–—ä¸ªæ€§åŒ–æä¾›äº†æ–°çš„é€”å¾„ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†å¹¿æ³›é‡‡ç”¨çš„å¤šæ¨¡æ€æ•°æ®æ•´åˆç­–ç•¥ï¼Œä»¥ååŠ©æ¨åŠ¨è‚¿ç˜¤å­¦æ•°æ®é©±åŠ¨å‘ç°çš„è®¡ç®—æ–¹æ³•ã€‚æœ¬æ–‡æ¢è®¨äº†æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„æ–°å…´è¶‹åŠ¿ï¼ŒåŒ…æ‹¬æ–¹æ³•è®ºæ¡†æ¶ã€éªŒè¯åè®®å’Œé’ˆå¯¹ç™Œç—‡äºšå‹åˆ†ç±»ã€ç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°ã€æ²»ç–—æŒ‡å¯¼å’Œç»“æœé¢„æµ‹çš„å¼€æºèµ„æºã€‚æœ¬æ–‡è¿˜æ¶µç›–äº†ä»ä¼ ç»ŸMLåˆ°FMçš„å¤šæ¨¡æ€æ•´åˆè½¬å˜ã€‚æˆ‘ä»¬å±•ç¤ºäº†æœ€è¿‘çš„FMè¿›å±•å’Œé¢ä¸´çš„å¤šç»„å­¦ä¸é«˜çº§æˆåƒæ•°æ®æ•´åˆæŒ‘æˆ˜ã€‚æˆ‘ä»¬ç¡®å®šäº†æœ€å…ˆè¿›çš„FMsã€å¯å…¬å¼€è®¿é—®çš„å¤šæ¨¡å¼å­˜å‚¨åº“ä»¥åŠæ•°æ®æ•´åˆçš„å…ˆè¿›å·¥å…·å’Œæ–¹æ³•ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå½“å‰æœ€å…ˆè¿›çš„æ•´åˆæ–¹æ³•ä¸ºå¼€å‘ä¸‹ä¸€ä»£å¤§å‹é¢„è®­ç»ƒæ¨¡å‹å¥ å®šäº†åŸºç¡€ï¼Œè¿™äº›æ¨¡å‹å°†è¿›ä¸€æ­¥æ¨åŠ¨è‚¿ç˜¤å­¦çš„é©å‘½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ç¯‡ç³»ç»Ÿå›é¡¾ä»ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ è½¬å‘å…ˆè¿›çš„FMç”¨äºè‚¿ç˜¤å­¦å¤šæ¨¡æ€æ•°æ®æ•´åˆçš„è®ºæ–‡ï¼Œå¹¶å°†è¿™äº›å‘å±•ä½œä¸ºå¤§è§„æ¨¡AIæ¨¡å‹åœ¨ç™Œç—‡ç ”ç©¶ä¸­çš„æ–°æ—¶ä»£çš„åŸºçŸ³ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç™Œç—‡ç ”ç©¶æ­£è¶Šæ¥è¶Šå¤šåœ°ä¾èµ–äºå¤šæ¨¡æ€æ•°æ®çš„æ•´åˆï¼ŒåŒ…æ‹¬åŸºå› ç»„å­¦ã€è›‹ç™½è´¨ç»„å­¦ã€æˆåƒå’Œä¸´åºŠå› ç´ ç­‰ã€‚</li>
<li>ä»å¤§é‡å¼‚è´¨æ•°æ®ä¸­æå–å¯æ“ä½œæ€§çš„è§è§£æ˜¯å½“å‰çš„æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>åŸºç¡€æ¨¡å‹ï¼ˆFMsï¼‰çš„å‡ºç°ä¸ºç™Œç—‡ç ”ç©¶ä¸­çš„ç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°ã€è¯Šæ–­å’Œæ²»ç–—ä¸ªæ€§åŒ–æä¾›äº†æ–°çš„é€”å¾„ã€‚</li>
<li>æœ¬æ–‡å…¨é¢å›é¡¾äº†å¤šæ¨¡æ€æ•°æ®æ•´åˆçš„ç­–ç•¥å¹¶æ¢è®¨äº†æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰çš„æ–°å…´è¶‹åŠ¿ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†ä»ä¼ ç»ŸMLåˆ°FMçš„è½¬å˜ï¼Œå¹¶å±•ç¤ºäº†FMåœ¨ç™Œç—‡ç ”ç©¶ä¸­çš„æœ€æ–°è¿›å±•å’ŒæŒ‘æˆ˜ã€‚</li>
<li>å…ˆè¿›çš„FMsã€å¤šæ¨¡æ€å­˜å‚¨åº“ä»¥åŠæ•°æ®æ•´åˆçš„å…ˆè¿›å·¥å…·å’Œæ–¹æ³•çš„å‡ºç°ä¸ºæœªæ¥çš„å¤§è§„æ¨¡AIæ¨¡å‹åœ¨ç™Œç—‡ç ”ç©¶ä¸­çš„åº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚</li>
<li>å½“å‰çš„ç³»ç»Ÿæ€§å›é¡¾æ˜¯é¦–æ¬¡å¯¹è‚¿ç˜¤å­¦å¤šæ¨¡æ€æ•°æ®æ•´åˆä¸­çš„æœºå™¨å­¦ä¹ è½¬å˜è¿›è¡Œæ¢³ç†ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09028">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7c059bf5041a4e9e8bc60f3d6e22a735.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e58ba5a1a277cae03234954b44cbd35c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f57f2023c41e5a8b9c1dc8a428abd55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-152f8161624a8ab9a526e85acb8f7adc.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Understanding-Dataset-Bias-in-Medical-Imaging-A-Case-Study-on-Chest-X-rays"><a href="#Understanding-Dataset-Bias-in-Medical-Imaging-A-Case-Study-on-Chest-X-rays" class="headerlink" title="Understanding Dataset Bias in Medical Imaging: A Case Study on Chest   X-rays"></a>Understanding Dataset Bias in Medical Imaging: A Case Study on Chest   X-rays</h2><p><strong>Authors:Ethan Dack, Chengliang Dai</strong></p>
<p>Recent works have revisited the infamous task &#96;&#96;Name That Datasetâ€™â€™, demonstrating that non-medical datasets contain underlying biases and that the dataset origin task can be solved with high accuracy. In this work, we revisit the same task applied to popular open-source chest X-ray datasets. Medical images are naturally more difficult to release for open-source due to their sensitive nature, which has led to certain open-source datasets being extremely popular for research purposes. By performing the same task, we wish to explore whether dataset bias also exists in these datasets. To extend our work, we apply simple transformations to the datasets, repeat the same task, and perform an analysis to identify and explain any detected biases. Given the importance of AI applications in medical imaging, itâ€™s vital to establish whether modern methods are taking shortcuts or are focused on the relevant pathology. We implement a range of different network architectures on the datasets: NIH, CheXpert, MIMIC-CXR and PadChest. We hope this work will encourage more explainable research being performed in medical imaging and the creation of more open-source datasets in the medical domain. Our code can be found here: <a target="_blank" rel="noopener" href="https://github.com/eedack01/x_ray_ds_bias">https://github.com/eedack01/x_ray_ds_bias</a>. </p>
<blockquote>
<p>è¿‘æœŸçš„ç ”ç©¶é‡æ–°å…³æ³¨äº†â€œå‘½åæ•°æ®é›†â€ä»»åŠ¡ï¼Œè¡¨æ˜éåŒ»å­¦æ•°æ®é›†å­˜åœ¨æ½œåœ¨åè§ï¼Œå¹¶ä¸”æ•°æ®é›†èµ·æºä»»åŠ¡å¯ä»¥é«˜ç²¾åº¦è§£å†³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å…³æ³¨è¯¥ä»»åŠ¡ï¼Œå¹¶åº”ç”¨äºæµè¡Œçš„å¼€æºèƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ã€‚ç”±äºåŒ»å­¦å›¾åƒç”±äºå…¶æ•æ„Ÿæ€§æ€§è´¨è€Œå¤©ç„¶æ›´éš¾ä»¥å¼€æºå‘å¸ƒï¼Œè¿™ä½¿å¾—æŸäº›å¼€æºæ•°æ®é›†æˆä¸ºç ”ç©¶ç”¨é€”çš„æå—æ¬¢è¿çš„èµ„æºã€‚é€šè¿‡æ‰§è¡Œç›¸åŒçš„ä»»åŠ¡ï¼Œæˆ‘ä»¬å¸Œæœ›æ¢ç´¢è¿™äº›æ•°æ®é›†æ˜¯å¦å­˜åœ¨æ•°æ®é›†åè§ã€‚ä¸ºäº†æ‰©å±•æˆ‘ä»¬çš„å·¥ä½œï¼Œæˆ‘ä»¬å¯¹æ•°æ®é›†åº”ç”¨äº†ç®€å•çš„è½¬æ¢ï¼Œé‡å¤ç›¸åŒçš„ä»»åŠ¡ï¼Œå¹¶è¿›è¡Œåˆ†æä»¥è¯†åˆ«å’Œè§£é‡Šä»»ä½•æ£€æµ‹åˆ°çš„åè§ã€‚é‰´äºäººå·¥æ™ºèƒ½åœ¨åŒ»å­¦å½±åƒåº”ç”¨ä¸­çš„é‡è¦æ€§ï¼Œç¡®å®šç°ä»£æ–¹æ³•æ˜¯å¦èµ°æ·å¾„æˆ–ä¸“æ³¨äºç›¸å…³ç—…ç†å­¦è‡³å…³é‡è¦ã€‚æˆ‘ä»¬åœ¨NIHã€CheXpertã€MIMIC-CXRå’ŒPadChestç­‰æ•°æ®é›†ä¸Šå®ç°äº†å¤šç§ä¸åŒçš„ç½‘ç»œæ¶æ„ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œå°†é¼“åŠ±åœ¨åŒ»å­¦å½±åƒé¢†åŸŸè¿›è¡Œæ›´å¤šå¯è§£é‡Šçš„ç ”ç©¶ï¼Œå¹¶åœ¨åŒ»å­¦é¢†åŸŸåˆ›å»ºæ›´å¤šçš„å¼€æºæ•°æ®é›†ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/eedack01/x_ray_ds_bias">https://github.com/eedack01/x_ray_ds_bias</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07722v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡é‡æ–°æ¢è®¨äº†â€œName That Datasetâ€ä»»åŠ¡åœ¨æµè¡Œå¼€æºèƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šçš„åº”ç”¨ï¼Œæ¢ç´¢è¿™äº›æ•°é›†ä¸­æ˜¯å¦å­˜åœ¨æ•°æ®é›†åè§ã€‚é€šè¿‡å¯¹æ•°æ®é›†è¿›è¡Œç®€å•è½¬æ¢å¹¶é‡å¤ä»»åŠ¡ï¼Œåˆ†æå’Œè§£é‡Šæ£€æµ‹åˆ°çš„åè§ã€‚æ–‡ç« æŒ‡å‡ºAIåœ¨åŒ»å­¦å½±åƒé¢†åŸŸåº”ç”¨çš„é‡è¦æ€§ï¼Œæå€¡å»ºç«‹æ›´å¤šåŒ»å­¦å½±åƒé¢†åŸŸçš„å¼€æºæ•°æ®é›†ï¼Œå¹¶è¿›è¡Œæ›´å…·è§£é‡Šæ€§çš„ç ”ç©¶ã€‚ä»£ç å¯é€šè¿‡ç›¸å…³é“¾æ¥è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡æ–°è®¿é—®äº†â€œName That Datasetâ€ä»»åŠ¡ï¼Œå¹¶åº”ç”¨äºæµè¡Œçš„å¼€æºèƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ã€‚</li>
<li>æ¢ç´¢äº†è¿™äº›æ•°æ®é›†æ˜¯å¦å­˜åœ¨æ•°æ®é›†åè§ã€‚</li>
<li>é€šè¿‡ç®€å•è½¬æ¢æ•°æ®é›†å¹¶é‡å¤ä»»åŠ¡æ¥æ£€æµ‹å’Œåˆ†æåè§ã€‚</li>
<li>å¼ºè°ƒäº†AIåœ¨åŒ»å­¦å½±åƒé¢†åŸŸåº”ç”¨çš„é‡è¦æ€§ã€‚</li>
<li>æå€¡å»ºç«‹æ›´å¤šåŒ»å­¦å½±åƒé¢†åŸŸçš„å¼€æºæ•°æ®é›†ã€‚</li>
<li>æå€¡è¿›è¡Œæ›´å…·è§£é‡Šæ€§çš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07722">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f6fa0b0187b0c3e327565a0b43ea3fe0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5c5827e5f40ec7764635cf4bb8654e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b51ea8f27c6234345764b8bd4e99ef62.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9fd039d1cfcaf8e8eedd889b4e02fcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cec86886f9999f4d74cbd6b4cf850119.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Colorectal-Cancer-Tumor-Grade-Segmentation-in-Digital-Histopathology-Images-From-Giga-to-Mini-Challenge"><a href="#Colorectal-Cancer-Tumor-Grade-Segmentation-in-Digital-Histopathology-Images-From-Giga-to-Mini-Challenge" class="headerlink" title="Colorectal Cancer Tumor Grade Segmentation in Digital Histopathology   Images: From Giga to Mini Challenge"></a>Colorectal Cancer Tumor Grade Segmentation in Digital Histopathology   Images: From Giga to Mini Challenge</h2><p><strong>Authors:Alper Bahcekapili, Duygu Arslan, Umut Ozdemir, Berkay Ozkirli, Emre Akbas, Ahmet Acar, Gozde B. Akar, Bingdou He, Shuoyu Xu, Umit Mert Caglar, Alptekin Temizel, Guillaume Picaud, Marc Chaumont, GÃ©rard Subsol, Luc TÃ©ot, Fahad Alsharekh, Shahad Alghannam, Hexiang Mao, Wenhua Zhang</strong></p>
<p>Colorectal cancer (CRC) is the third most diagnosed cancer and the second leading cause of cancer-related death worldwide. Accurate histopathological grading of CRC is essential for prognosis and treatment planning but remains a subjective process prone to observer variability and limited by global shortages of trained pathologists. To promote automated and standardized solutions, we organized the ICIP Grand Challenge on Colorectal Cancer Tumor Grading and Segmentation using the publicly available METU CCTGS dataset. The dataset comprises 103 whole-slide images with expert pixel-level annotations for five tissue classes. Participants submitted segmentation masks via Codalab, evaluated using metrics such as macro F-score and mIoU. Among 39 participating teams, six outperformed the Swin Transformer baseline (62.92 F-score). This paper presents an overview of the challenge, dataset, and the top-performing methods </p>
<blockquote>
<p>ç»“ç›´è‚ ç™Œï¼ˆCRCï¼‰æ˜¯å…¨çƒè¯Šæ–­ç‡ç¬¬ä¸‰é«˜çš„ç™Œç—‡ï¼Œä¹Ÿæ˜¯å¯¼è‡´ç™Œç—‡ç›¸å…³æ­»äº¡çš„ç¬¬äºŒå¤§ä¸»è¦åŸå› ã€‚CRCçš„å‡†ç¡®ç»„ç»‡ç—…ç†å­¦åˆ†çº§å¯¹é¢„åå’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ï¼Œä½†ä»æ˜¯ä¸€ä¸ªä¸»è§‚è¿‡ç¨‹ï¼Œå®¹æ˜“å—è§‚å¯Ÿè€…å˜å¼‚æ€§çš„å½±å“ï¼Œå¹¶å—åˆ°å…¨çƒè®­ç»ƒæœ‰ç´ ç—…ç†å­¦å®¶çŸ­ç¼ºçš„é™åˆ¶ã€‚ä¸ºäº†æ¨å¹¿è‡ªåŠ¨åŒ–å’Œæ ‡å‡†åŒ–çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å…¬å¼€å¯ç”¨çš„METU CCTGSæ•°æ®é›†ï¼Œç»„ç»‡äº†å›½é™…å›¾åƒç—…ç†æŒ‘æˆ˜èµ›ï¼ˆICIP Grand Challengeï¼‰ç»“ç›´è‚ ç™Œè‚¿ç˜¤åˆ†çº§å’Œåˆ†å‰²æŒ‘æˆ˜èµ›ã€‚è¯¥æ•°æ®é›†åŒ…å«103å¼ å…¨ç‰‡å›¾åƒï¼ŒåŒ…æ‹¬äº”ç§ç»„ç»‡çš„ä¸“å®¶åƒç´ çº§æ³¨é‡Šã€‚å‚èµ›è€…é€šè¿‡Codalabæäº¤åˆ†å‰²æ©è†œï¼Œé€šè¿‡å®è§‚Fåˆ†æ•°å’ŒmIoUç­‰æŒ‡æ ‡è¿›è¡Œè¯„ä»·ã€‚åœ¨37æ”¯å‚èµ›é˜Ÿä¼ä¸­ï¼Œæœ‰å…­æ”¯é˜Ÿä¼çš„è¡¨ç°è¶…è¿‡äº†åŸºçº¿Swin Transformerï¼ˆ62.92 Fåˆ†æ•°ï¼‰ã€‚æœ¬æ–‡ä»‹ç»äº†æŒ‘æˆ˜ã€æ•°æ®é›†å’Œè¡¨ç°æœ€å¥½çš„æ–¹æ³•æ¦‚å†µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04681v2">PDF</a> Accepted Grand Challenge Paper ICIP 2025</p>
<p><strong>Summary</strong><br>ç»“ç›´è‚ ç™Œï¼ˆCRCï¼‰æ˜¯å…¨çƒè¯Šæ–­ç‡ç¬¬ä¸‰é«˜çš„ç™Œç—‡ï¼Œä¹Ÿæ˜¯å¯¼è‡´ç™Œç—‡æ­»äº¡çš„ç¬¬äºŒå¤§åŸå› ã€‚å‡†ç¡®çš„ç—…ç†åˆ†çº§å¯¹é¢„åå’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ï¼Œä½†ä»æ˜¯ä¸€ä¸ªä¸»è§‚è¿‡ç¨‹ï¼Œå®¹æ˜“å—è§‚å¯Ÿè€…å·®å¼‚å½±å“ï¼Œå¹¶ä¸”å…¨çƒç—…ç†å­¦å®¶çŸ­ç¼ºã€‚ä¸ºä¿ƒè¿›è‡ªåŠ¨åŒ–å’Œæ ‡å‡†åŒ–è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬ç»„ç»‡äº†åŸºäºå…¬å…±å¯ç”¨METU CCTGSæ•°æ®é›†çš„ç»“ç›´è‚ ç™Œè‚¿ç˜¤åˆ†çº§å’Œåˆ†å‰²çš„ICIP Grand ChallengeæŒ‘æˆ˜ã€‚è¯¥æ•°æ®é›†åŒ…å«åŒ…å«ä¸“å®¶åƒç´ çº§æ³¨é‡Šçš„äº”ä¸ªç»„ç»‡çš„å…¨ç‰‡å›¾åƒå…±è®¡103å¼ ã€‚é€šè¿‡Codalabæäº¤åˆ†å‰²æ©æ¨¡ï¼Œä½¿ç”¨å®è§‚Fåˆ†æ•°å’ŒmIoUç­‰æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚åœ¨å‚ä¸æŒ‘æˆ˜çš„39æ”¯é˜Ÿä¼ä¸­ï¼Œæœ‰å…­æ”¯é˜Ÿä¼çš„è¡¨ç°ä¼˜äºåŸºçº¿æ°´å¹³çš„Swin Transformerï¼ˆ62.92 F-scoreï¼‰ã€‚æœ¬æ–‡ä»‹ç»äº†æŒ‘æˆ˜æ¦‚å†µã€æ•°æ®é›†åŠé¡¶å°–çš„è¡¨ç°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»“ç›´è‚ ç™Œæ˜¯ä¸–ç•Œä¸Šè¯Šæ–­ç‡å’Œæ­»äº¡ç‡è¾ƒé«˜çš„ç™Œç—‡ä¹‹ä¸€ã€‚</li>
<li>ç—…ç†åˆ†çº§åœ¨ç»“ç›´è‚ ç™Œçš„é¢„åå’Œæ²»ç–—è®¡åˆ’ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†ç›®å‰ä»é¢ä¸´è§‚å¯Ÿè€…å·®å¼‚å’Œç—…ç†å­¦å®¶çŸ­ç¼ºçš„é—®é¢˜ã€‚</li>
<li>ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œä¸¾åŠäº†ICIP Grand ChallengeæŒ‘æˆ˜ï¼Œæ—¨åœ¨ä¿ƒè¿›ç»“ç›´è‚ ç™Œè‚¿ç˜¤åˆ†çº§å’Œåˆ†å‰²çš„è‡ªåŠ¨åŒ–å’Œæ ‡å‡†åŒ–è§£å†³æ–¹æ¡ˆã€‚</li>
<li>è¯¥æŒ‘æˆ˜ä½¿ç”¨äº†å…¬å¼€çš„METU CCTGSæ•°æ®é›†ï¼ŒåŒ…å«æœ‰ä¸“å®¶åƒç´ çº§æ³¨é‡Šçš„äº”ç±»ç»„ç»‡çš„å…¨ç‰‡å›¾åƒå…±è®¡103å¼ ã€‚</li>
<li>é€šè¿‡Codalabæäº¤äº†å‚èµ›å›¢é˜Ÿçš„åˆ†å‰²æ©æ¨¡ï¼Œé€šè¿‡å®è§‚Fåˆ†æ•°å’ŒmIoUç­‰æŒ‡æ ‡è¯„ä¼°å‚èµ›é˜Ÿä¼çš„è¡¨ç°ã€‚</li>
<li>æœ€ç»ˆæœ‰å…­æ”¯é˜Ÿä¼çš„è¡¨ç°è¶…è¿‡äº†åŸºçº¿æ°´å¹³çš„Swin Transformeræ¨¡å‹ï¼ˆF-scoreä¸º62.92ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d14a0b9aee40b57242373fef0cd49942.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2fdfd5423d9a978c7cf8d77e5b55b4f7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5545a30067590cf36bb4087683d5716e.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Similarity-Memory-Prior-is-All-You-Need-for-Medical-Image-Segmentation"><a href="#Similarity-Memory-Prior-is-All-You-Need-for-Medical-Image-Segmentation" class="headerlink" title="Similarity Memory Prior is All You Need for Medical Image Segmentation"></a>Similarity Memory Prior is All You Need for Medical Image Segmentation</h2><p><strong>Authors:Hao Tang, Zhiqing Guo, Liejun Wang, Chao Liu</strong></p>
<p>In recent years, it has been found that â€œgrandmother cellsâ€ in the primary visual cortex (V1) of macaques can directly recognize visual input with complex shapes. This inspires us to examine the value of these cells in promoting the research of medical image segmentation. In this paper, we design a Similarity Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically, we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and remembers the category features of specific lesions or organs in medical images through the similarity memory prior in the prototype memory bank, thus helping the network to learn subtle texture changes between categories. DMW-LA also dynamically updates the similarity memory prior in reverse through Weight-Loss Dynamic (W-LD) update strategy, effectively assisting the network directly extract category features. In addition, we propose the Double-Similarity Global Internal Enhancement Module (DS-GIM) to deeply explore the internal differences in the feature distribution of input data through cosine similarity and euclidean distance. Extensive experiments on four public datasets show that Sim-MPNet has better segmentation performance than other state-of-the-art methods. Our code is available on <a target="_blank" rel="noopener" href="https://github.com/vpsg-research/Sim-MPNet">https://github.com/vpsg-research/Sim-MPNet</a>. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç ”ç©¶å‘ç°çŒ•çŒ´åˆçº§è§†è§‰çš®å±‚ï¼ˆV1ï¼‰ä¸­çš„â€œç¥–æ¯ç»†èƒâ€èƒ½ç›´æ¥è¯†åˆ«å…·æœ‰å¤æ‚å½¢çŠ¶çš„è§†è§‰è¾“å…¥ï¼Œè¿™æ¿€å‘äº†æˆ‘ä»¬æ¢ç´¢è¿™äº›ç»†èƒåœ¨æ¨åŠ¨åŒ»å­¦å›¾åƒåˆ†å‰²ç ”ç©¶ä¸­çš„ä»·å€¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„ç›¸ä¼¼æ€§è®°å¿†å…ˆéªŒç½‘ç»œï¼ˆSim-MPNetï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€è®°å¿†æƒé‡æŸå¤±æ³¨æ„åŠ›ï¼ˆDMW-LAï¼‰ï¼Œå®ƒé€šè¿‡åŸå‹è®°å¿†åº“ä¸­çš„ç›¸ä¼¼æ€§è®°å¿†å…ˆéªŒæ¥åŒ¹é…å’Œè®°å¿†åŒ»å­¦å›¾åƒä¸­ç‰¹å®šç—…å˜æˆ–å™¨å®˜çš„åˆ†ç±»ç‰¹å¾ï¼Œä»è€Œå¸®åŠ©ç½‘ç»œå­¦ä¹ ç±»åˆ«ä¹‹é—´çš„ç»†å¾®çº¹ç†å˜åŒ–ã€‚DMW-LAè¿˜é€šè¿‡é‡é‡æŸå¤±åŠ¨æ€ï¼ˆW-LDï¼‰æ›´æ–°ç­–ç•¥åå‘åŠ¨æ€æ›´æ–°ç›¸ä¼¼æ€§è®°å¿†å…ˆéªŒï¼Œæœ‰æ•ˆåœ°å¸®åŠ©ç½‘ç»œç›´æ¥æå–ç±»åˆ«ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†åŒç›¸ä¼¼æ€§å…¨å±€å†…éƒ¨å¢å¼ºæ¨¡å—ï¼ˆDS-GIMï¼‰ï¼Œé€šè¿‡ä½™å¼¦ç›¸ä¼¼æ€§å’Œæ¬§å‡ é‡Œå¾—è·ç¦»æ·±å…¥æ¢ç´¢è¾“å…¥æ•°æ®ç‰¹å¾åˆ†å¸ƒçš„å†…éƒ¨å·®å¼‚ã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSim-MPNetçš„åˆ†å‰²æ€§èƒ½ä¼˜äºå…¶ä»–æœ€æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/vpsg-research/Sim-MPNet%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/vpsg-research/Sim-MPNetä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00585v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŒ»å­¦å›¾åƒåˆ†å‰²é¢†åŸŸçš„æ–°è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§åä¸ºSim-MPNetçš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨åŠ¨æ€è®°å¿†æƒé‡æŸå¤±æ³¨æ„åŠ›æœºåˆ¶ï¼ˆDMW-LAï¼‰å’ŒåŒé‡ç›¸ä¼¼æ€§å…¨å±€å†…éƒ¨å¢å¼ºæ¨¡å—ï¼ˆDS-GIMï¼‰ï¼Œé€šè¿‡åŒ¹é…å’Œè®°å¿†åŒ»å­¦å›¾åƒä¸­ç‰¹å®šç—…å˜æˆ–å™¨å®˜çš„ç‰¹å¾ç±»åˆ«ï¼Œæœ‰æ•ˆå­¦ä¹ ç±»åˆ«é—´çš„ç»†å¾®çº¹ç†å˜åŒ–ï¼Œæå‡äº†åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>â€œç¥–æ¯ç»†èƒâ€åœ¨çŒ•çŒ´åˆçº§è§†è§‰çš®å±‚ï¼ˆV1ï¼‰ä¸­çš„å‘ç°ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†å‰²ç ”ç©¶æä¾›äº†æ–°çš„å¯ç¤ºã€‚</li>
<li>æå‡ºäº†åä¸ºSim-MPNetçš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>Sim-MPNetä¸­çš„DMW-LAæœºåˆ¶èƒ½å¤ŸåŒ¹é…å¹¶è®°å¿†åŒ»å­¦å›¾åƒä¸­ç‰¹å®šç—…å˜æˆ–å™¨å®˜çš„ç‰¹å¾ç±»åˆ«ã€‚</li>
<li>DMW-LAé€šè¿‡åŠ¨æ€æ›´æ–°ç›¸ä¼¼è®°å¿†ä¼˜å…ˆæƒï¼Œæœ‰æ•ˆè¾…åŠ©ç½‘ç»œç›´æ¥æå–ç±»åˆ«ç‰¹å¾ã€‚</li>
<li>DS-GIMæ¨¡å—æ·±å…¥æ¢ç´¢äº†è¾“å…¥æ•°æ®ç‰¹å¾åˆ†å¸ƒçš„å†…éƒ¨å·®å¼‚ï¼Œé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦å’Œæ¬§å‡ é‡Œå¾—è·ç¦»è¿›è¡Œè®¡ç®—ã€‚</li>
<li>Sim-MPNetåœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¶åˆ†å‰²æ€§èƒ½ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00585">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a76583eba60be8db2a887291c90f291e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a141f46d08d3306c7ff02fa9df54290.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3728bf2b4781ef5897204dfc15e654e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-878863d1775997702f5c7ff096bd9293.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03623ce2555efcb668489c7185055423.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7522946d8ad6af48335e690a843a3432.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1049d4ae6438daf03a9d1aee1979c206.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  Supporting SENÄ†OTEN Language Documentation Efforts with Automatic   Speech Recognition
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3a45ec19ecad59aab41880369ef222dc.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  PanoDiff-SR Synthesizing Dental Panoramic Radiographs using Diffusion   and Super-resolution
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
