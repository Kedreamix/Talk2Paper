<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Agent">
    <meta name="description" content="Agent 方向最新论文已更新，请持续关注 Update in 2025-07-17  Journalism-Guided Agentic In-Context Learning for News Stance Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Agent | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-8030ab6220eb9d5af66ba57f0797f4b8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Agent</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Agent/">
                                <span class="chip bg-color">Agent</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                Agent
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-11
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    39 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-17-更新"><a href="#2025-07-17-更新" class="headerlink" title="2025-07-17 更新"></a>2025-07-17 更新</h1><h2 id="Journalism-Guided-Agentic-In-Context-Learning-for-News-Stance-Detection"><a href="#Journalism-Guided-Agentic-In-Context-Learning-for-News-Stance-Detection" class="headerlink" title="Journalism-Guided Agentic In-Context Learning for News Stance Detection"></a>Journalism-Guided Agentic In-Context Learning for News Stance Detection</h2><p><strong>Authors:Dahyun Lee, Jonghyeon Choi, Jiyoung Han, Kunwoo Park</strong></p>
<p>As online news consumption grows, personalized recommendation systems have become integral to digital journalism. However, these systems risk reinforcing filter bubbles and political polarization by failing to incorporate diverse perspectives. Stance detection – identifying a text’s position on a target – can help mitigate this by enabling viewpoint-aware recommendations and data-driven analyses of media bias. Yet, existing stance detection research remains largely limited to short texts and high-resource languages. To address these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for article-level stance detection, comprising 2,000 news articles with article-level and 19,650 segment-level stance annotations across 47 societal issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided \textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that employs a language model agent to predict the stances of key structural segments (e.g., leads, quotes), which are then aggregated to infer the overall article stance. Experiments show that \textsc{JoA-ICL} outperforms existing stance detection methods, highlighting the benefits of segment-level agency in capturing the overall position of long-form news articles. Two case studies further demonstrate its broader utility in promoting viewpoint diversity in news recommendations and uncovering patterns of media bias. </p>
<blockquote>
<p>随着在线新闻消费的增长，个性化推荐系统已成为数字新闻不可或缺的一部分。然而，这些系统因未能融入多元化的观点，存在强化信息茧房和政治极化的风险。立场检测——识别文本对目标的立场——可以通过实现观点感知推荐和媒体偏见的数据驱动分析来帮助缓解这一问题。然而，现有的立场检测研究大多局限于短文本和高资源语言。为了弥补这些不足，我们引入了\textsc{K-News-Stance}，这是首个用于文章级别立场检测的韩语数据集，包含2000篇新闻文章，跨越47个社会问题的文章级别和19650个段落级别的立场注释。我们还提出了\textsc{JoA-ICL}框架，这是一个以新闻为导向的语境学习框架，它采用语言模型代理来预测关键结构段落（如导语、引语）的立场，然后聚合这些立场来推断整篇文章的立场。实验表明，\textsc{JoA-ICL}优于现有的立场检测方法，突显了分段级别的机构在捕捉长篇新闻文章整体立场方面的优势。两个案例研究进一步证明了其在促进新闻推荐中的观点多样性和揭示媒体偏见模式方面的更广泛的实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11049v1">PDF</a> Preprint. 24 pages</p>
<p><strong>Summary</strong><br>随着在线新闻消费的增长，个性化推荐系统在数字新闻中愈发重要，但同时也存在风险，如忽略不同视角可能会加剧资讯茧房和政治极化。立场检测能缓解这些问题，通过对文章或段落进行立场分析识别文章目标立场，进而促进观点感知的推荐和媒体偏见的数据驱动分析。然而现有的立场检测研究多局限于短文本和高资源语言。为填补这些空白，推出首个用于韩国文章级别立场检测的数据集K-News-Stance。同时提出了基于新闻报道指导的JoA-ICL框架，利用语言模型预测关键结构段落的立场，进而推断整体文章立场。实验显示JoA-ICL优于现有立场检测方法，展示了其在捕捉长新闻整体立场方面的优势。在促进新闻推荐的观点多样性和揭示媒体偏见方面也有广泛应用价值。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>在线新闻消费的增长推动了个性化推荐系统在数字新闻中的重要性。</li>
<li>个性化推荐系统若忽略不同视角可能加剧资讯茧房和政治极化问题。</li>
<li>立场检测可识别文本对目标的立场，有助于促进观点感知的推荐和媒体偏见分析。</li>
<li>现有立场检测研究多局限于短文本和高资源语言，存在研究空白。</li>
<li>推出首个韩国文章级别立场检测数据集K-News-Stance，包括2000篇新闻文章和相应立场标注。</li>
<li>提出JoA-ICL框架，通过预测关键结构段落的立场来推断整体文章立场，表现优于现有方法。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11049">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3e723125370f79dc67d56cf82c7e774c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af989a88271b0ec46f0a98a95e94d898.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DS-GT-at-eRisk-2025-From-prompts-to-predictions-benchmarking-early-depression-detection-with-conversational-agent-based-assessments-and-temporal-attention-models"><a href="#DS-GT-at-eRisk-2025-From-prompts-to-predictions-benchmarking-early-depression-detection-with-conversational-agent-based-assessments-and-temporal-attention-models" class="headerlink" title="DS@GT at eRisk 2025: From prompts to predictions, benchmarking early   depression detection with conversational agent based assessments and temporal   attention models"></a>DS@GT at eRisk 2025: From prompts to predictions, benchmarking early   depression detection with conversational agent based assessments and temporal   attention models</h2><p><strong>Authors:Anthony Miyaguchi, David Guecha, Yuwen Chiu, Sidharth Gaur</strong></p>
<p>This Working Note summarizes the participation of the DS@GT team in two eRisk 2025 challenges. For the Pilot Task on conversational depression detection with large language-models (LLMs), we adopted a prompt-engineering strategy in which diverse LLMs conducted BDI-II-based assessments and produced structured JSON outputs. Because ground-truth labels were unavailable, we evaluated cross-model agreement and internal consistency. Our prompt design methodology aligned model outputs with BDI-II criteria and enabled the analysis of conversational cues that influenced the prediction of symptoms. Our best submission, second on the official leaderboard, achieved DCHR &#x3D; 0.50, ADODL &#x3D; 0.89, and ASHR &#x3D; 0.27. </p>
<blockquote>
<p>本工作笔记总结了DS@GT团队参与两项eRisk 2025挑战的情况。在基于大型语言模型（LLMs）的对话式抑郁症检测试点任务中，我们采用了提示工程策略，让多种LLMs进行基于BDI-II的评估，并生成结构化JSON输出。由于无法获取真实标签，我们评估了跨模型的一致性和内部一致性。我们的提示设计方法使模型输出与BDI-II标准相符，并能分析影响症状预测的对话线索。我们的最佳提交成绩在官方排行榜上排名第二，达到DCHR&#x3D;0.50，ADODL&#x3D;0.89和ASHR&#x3D;0.27。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10958v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>DS@GT团队参与了两个eRisk 2025挑战项目的工作笔记总结。在基于大型语言模型（LLMs）的对话式抑郁症检测试点任务中，我们采用了提示工程策略，通过多样的LLMs进行BDI-II基准评估并生成结构化JSON输出。由于缺少真实标签，我们评估了跨模型的一致性和内部一致性。我们的提示设计方法使模型输出与BDI-II标准相符，并能分析影响症状预测的对话线索。我们的最佳提交在官方排行榜上排名第二，达到了DCHR&#x3D;0.50，ADODL&#x3D;0.89和ASHR&#x3D;0.27的成效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DS@GT团队参与了eRisk 2025挑战中的两个任务。</li>
<li>在对话式抑郁症检测任务中，团队采用了基于大型语言模型的提示工程策略。</li>
<li>通过多种LLMs进行BDI-II基准评估并生成结构化输出。</li>
<li>缺乏真实标签的情况下，团队通过评估跨模型的一致性和内部一致性来进行评价。</li>
<li>提示设计使模型输出与BDI-II标准相符，并分析了对话中的线索对症状预测的影响。</li>
<li>最佳提交在官方排行榜上获得第二名。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10958">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-81aba587124c82f78714c28f79b5e6d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3bef381ceab0252cd770b2698e5a2565.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5c7fde487c251866d4379738e65f2d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17f44529bb1ab00eb8a4da46968ca717.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8030ab6220eb9d5af66ba57f0797f4b8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Lessons-Learned-from-Evaluation-of-LLM-based-Multi-agents-in-Safer-Therapy-Recommendation"><a href="#Lessons-Learned-from-Evaluation-of-LLM-based-Multi-agents-in-Safer-Therapy-Recommendation" class="headerlink" title="Lessons Learned from Evaluation of LLM based Multi-agents in Safer   Therapy Recommendation"></a>Lessons Learned from Evaluation of LLM based Multi-agents in Safer   Therapy Recommendation</h2><p><strong>Authors:Yicong Wu, Ting Chen, Irit Hochberg, Zhoujian Sun, Ruth Edry, Zhengxing Huang, Mor Peleg</strong></p>
<p>Therapy recommendation for chronic patients with multimorbidity is challenging due to risks of treatment conflicts. Existing decision support systems face scalability limitations. Inspired by the way in which general practitioners (GP) manage multimorbidity patients, occasionally convening multidisciplinary team (MDT) collaboration, this study investigated the feasibility and value of using a Large Language Model (LLM)-based multi-agent system (MAS) for safer therapy recommendations. We designed a single agent and a MAS framework simulating MDT decision-making by enabling discussion among LLM agents to resolve medical conflicts. The systems were evaluated on therapy planning tasks for multimorbidity patients using benchmark cases. We compared MAS performance with single-agent approaches and real-world benchmarks. An important contribution of our study is the definition of evaluation metrics that go beyond the technical precision and recall and allow the inspection of clinical goals met and medication burden of the proposed advices to a gold standard benchmark. Our results show that with current LLMs, a single agent GP performs as well as MDTs. The best-scoring models provide correct recommendations that address all clinical goals, yet the advices are incomplete. Some models also present unnecessary medications, resulting in unnecessary conflicts between medication and conditions or drug-drug interactions. </p>
<blockquote>
<p>对于患有多种慢性疾病的患者的治疗建议具有挑战性，因为存在治疗冲突的风险。现有的决策支持系统面临可扩展性限制。本研究受到全科医师（GP）管理多病因患者的方式的启发，后者会偶尔召集多学科团队（MDT）进行合作。本研究调查了使用基于大型语言模型（LLM）的多代理系统（MAS）进行更安全治疗建议的可行性和价值。我们设计了一个单一代理和MAS框架，通过模拟LLM代理之间的讨论来解决医学冲突，从而模拟多学科团队的决策制定。我们使用基准病例对用于多病因患者的治疗规划任务的系统进行了评估。我们将MAS的性能与单代理方法和真实世界基准进行了比较。本研究的一个重要贡献是定义了评估指标，这些指标超越了技术上的精确性和召回率，并允许检查所提建议的临床目标达成情况和药物负担与金标准基准的比较。我们的结果表明，使用当前的大型语言模型，单一代理的全科医师的表现与多学科团队相当。得分最高的模型提供了正确的建议，解决了所有临床目标，但建议并不完整。一些模型还提出了不必要的药物，导致药物与条件之间或药物之间的不必要冲突。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10911v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>利用大型语言模型（LLM）构建的多智能体系统（MAS）在针对多病共存患者的治疗推荐方面具有可行性及价值。该研究通过模拟多学科团队（MDT）决策讨论，设计单一智能体和MAS框架解决医疗冲突。评估系统对多病共存患者的治疗规划任务时，与单一智能体方法和真实世界基准进行比较。研究定义的评价指标不仅限于技术精度和召回率，还允许检查临床目标的达成情况和建议的药物负担与黄金标准基准的比较。结果显示，当前LLM的单一智能体表现与MDT相当，最佳模型能提出满足所有临床目标的正确建议，但建议尚不完整，部分模型存在不必要用药，导致药物与条件或药物之间的不必要冲突。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>利用大型语言模型（LLM）构建的多智能体系统（MAS）在处理慢性多病共存患者的治疗推荐时具有挑战性和价值。</li>
<li>多学科团队（MDT）的合作方式被模拟用于解决医疗冲突。</li>
<li>系统评估包括技术精度和召回率，同时考虑临床目标的达成情况和建议的药物负担。</li>
<li>单一智能体的表现与多学科团队相当。</li>
<li>最佳模型能提出正确的建议，满足所有临床目标，但建议尚不完整。</li>
<li>部分模型存在不必要用药的问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10911">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c766d24ad627a0e57932551b603d96b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-645e99d2109256a06d26b6488d3cdffb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76b34f569d0b98dc35beba05c40f677f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bf221f30d5d5c03fdd88c5215d56e40a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Warehouse-Spatial-Question-Answering-with-LLM-Agent"><a href="#Warehouse-Spatial-Question-Answering-with-LLM-Agent" class="headerlink" title="Warehouse Spatial Question Answering with LLM Agent"></a>Warehouse Spatial Question Answering with LLM Agent</h2><p><strong>Authors:Hsiang-Wei Huang, Jen-Hao Cheng, Kuang-Ming Chen, Cheng-Yen Yang, Bahaa Alattar, Yi-Ru Lin, Pyongkun Kim, Sangwon Kim, Kwangju Kim, Chung-I Huang, Jenq-Neng Hwang</strong></p>
<p>Spatial understanding has been a challenging task for existing Multi-modal Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM finetuning to enhance MLLM’s spatial understanding ability. In this paper, we present a data-efficient approach. We propose a LLM agent system with strong and advanced spatial reasoning ability, which can be used to solve the challenging spatial question answering task in complex indoor warehouse scenarios. Our system integrates multiple tools that allow the LLM agent to conduct spatial reasoning and API tools interaction to answer the given complicated spatial question. Extensive evaluations on the 2025 AI City Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that our system achieves high accuracy and efficiency in tasks such as object retrieval, counting, and distance estimation. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/hsiangwei0903/SpatialAgent">https://github.com/hsiangwei0903/SpatialAgent</a> </p>
<blockquote>
<p>空间理解对于现有的多模态大型语言模型（MLLMs）来说一直是一项具有挑战性的任务。之前的方法利用大规模MLLM微调技术来提升MLLM的空间理解能力。在本文中，我们提出了一种数据高效的方法。我们提出了一个具有强大和先进空间推理能力的LLM代理系统，可用于解决复杂室内仓库场景中具有挑战性的空间问答任务。我们的系统集成了多种工具，允许LLM代理进行空间推理和API工具交互，以回答给定的复杂空间问题。在2025年AI城市挑战赛物理AI空间智能仓库数据集上的广泛评估表明，我们的系统在对象检索、计数和距离估计等任务上实现了高准确性和高效率。代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/hsiangwei0903/SpatialAgent">https://github.com/hsiangwei0903/SpatialAgent</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10778v1">PDF</a> 1st Place Solution of the 9th AI City Challenge Track 3</p>
<p><strong>Summary</strong></p>
<p>文本主要描述了一种具有强大空间推理能力的大型语言模型代理系统。该系统采用数据高效的方法解决复杂的室内仓库场景中的空间问答任务，并通过多个工具和API进行空间推理和交互。在AI城市挑战赛物理AI空间智能仓库数据集上的评估表明，该系统在目标检索、计数和距离估计等任务中具有高精度和效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在空间理解方面存在挑战。</li>
<li>提出了一种具有强大空间推理能力的大型语言模型代理系统。</li>
<li>系统通过整合多种工具进行空间推理和API交互以解答复杂的空间问题。</li>
<li>在AI城市挑战赛的物理AI空间智能仓库数据集上进行了广泛评估。</li>
<li>系统在目标检索、计数和距离估计等任务上表现出高准确性和高效率。</li>
<li>该系统可用于复杂的室内仓库场景中的空间问答任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10778">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c6b98ffc58a8a9d1531c3468ae12bfe3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08011589d01c3c860f8d3a986ba425a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f891797b7ab22736e1bf17a36c17192f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Robustifying-3D-Perception-via-Least-Squares-Graphs-for-Multi-Agent-Object-Tracking"><a href="#Robustifying-3D-Perception-via-Least-Squares-Graphs-for-Multi-Agent-Object-Tracking" class="headerlink" title="Robustifying 3D Perception via Least-Squares Graphs for Multi-Agent   Object Tracking"></a>Robustifying 3D Perception via Least-Squares Graphs for Multi-Agent   Object Tracking</h2><p><strong>Authors:Maria Damanaki, Ioulia Kapsali, Nikos Piperigkos, Alexandros Gkillas, Aris S. Lalos</strong></p>
<p>The critical perception capabilities of EdgeAI systems, such as autonomous vehicles, are required to be resilient against adversarial threats, by enabling accurate identification and localization of multiple objects in the scene over time, mitigating their impact. Single-agent tracking offers resilience to adversarial attacks but lacks situational awareness, underscoring the need for multi-agent cooperation to enhance context understanding and robustness. This paper proposes a novel mitigation framework on 3D LiDAR scene against adversarial noise by tracking objects based on least-squares graph on multi-agent adversarial bounding boxes. Specifically, we employ the least-squares graph tool to reduce the induced positional error of each detection’s centroid utilizing overlapped bounding boxes on a fully connected graph via differential coordinates and anchor points. Hence, the multi-vehicle detections are fused and refined mitigating the adversarial impact, and associated with existing tracks in two stages performing tracking to further suppress the adversarial threat. An extensive evaluation study on the real-world V2V4Real dataset demonstrates that the proposed method significantly outperforms both state-of-the-art single and multi-agent tracking frameworks by up to 23.3% under challenging adversarial conditions, operating as a resilient approach without relying on additional defense mechanisms. </p>
<blockquote>
<p>边缘人工智能系统（如自动驾驶汽车）的关键感知能力需要对抗恶意威胁具有韧性。这要求系统能够随着时间的推移，准确识别和定位场景中的多个物体，并减轻其影响。单智能体跟踪虽然可以抵御恶意攻击，但缺乏态势感知能力，这强调了多智能体合作以增强上下文理解和稳健性的必要性。本文提出了一种基于最小二乘图的多智能体对抗边界框跟踪对象的新型缓解框架，以对抗激光雷达场景中的对抗噪声。具体来说，我们利用最小二乘图工具来减少每个检测中心点的感应位置误差，通过在完全连接的图上使用差分坐标和锚点，在重叠的边界框中进行操作。因此，多车辆检测被融合和细化，减轻了对抗影响，并与现有轨迹相结合，分为两个阶段进行追踪，进一步抑制了对抗威胁。在真实世界的V2V4Real数据集上的广泛评估研究表明，在具有挑战性的对抗条件下，该方法明显优于最新的单智能体和多智能体跟踪框架，提升率高达23.3%，作为一种具有韧性的方法，无需依赖额外的防御机制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.04762v2">PDF</a> 6 pages, 3 figures, 4 tables</p>
<p><strong>Summary</strong><br>     边缘智能系统（如自动驾驶汽车）需要具备对抗恶意攻击时的恢复能力，这需要实现对场景中多个物体的准确识别和定位。单智能体追踪虽然可以抵御攻击，但缺乏环境意识，因此需要多智能体合作以增强上下文理解和稳健性。本文提出一种基于最小二乘图和多智能体对抗性边界框的新对抗噪声追踪方法。在连接图中利用重叠的边界框并利用差异坐标和锚点，实现降低对象检测的引入位置误差。通过融合和细化多车辆检测，对抗恶意影响并与现有轨迹关联，进一步抑制恶意威胁。在现实世界V2V4Real数据集上的评估显示，该方法在具有挑战性的对抗条件下显著优于最先进的单智能体和多智能体追踪框架，作为一种不依赖额外防御机制的有恢复力方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EdgeAI系统需要对抗恶意攻击时的恢复能力，以实现对场景中物体的准确识别和定位。</li>
<li>单智能体追踪虽可抵御攻击但缺乏环境意识，强调多智能体合作的必要性以增强稳健性。</li>
<li>论文提出了一种基于最小二乘图和多智能体边界框的追踪方法以抵御恶意噪声攻击。</li>
<li>该方法利用最小二乘图工具减少对象检测引入的位置误差。</li>
<li>多车辆检测融合和细化有助于对抗恶意影响并与现有轨迹关联。</li>
<li>该方法在现实世界数据集上的评估表现优于其他追踪框架。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.04762">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8a615f0467b1ef14e0cf0fa54c77a101.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7bcafd468befa518af46d8882362118e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dcef72b82cff44b9e4180c5564fc12a8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3b8adf703d67655bb32e5e51ce2dd279.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-640fa1c488756879ad93a3b9a9c9fd6b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-44f5351099e5cafac0ff0d72def8d7b4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ccc4105398b87176b117bd8a6bab92b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MATE-LLM-Powered-Multi-Agent-Translation-Environment-for-Accessibility-Applications"><a href="#MATE-LLM-Powered-Multi-Agent-Translation-Environment-for-Accessibility-Applications" class="headerlink" title="MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility   Applications"></a>MATE: LLM-Powered Multi-Agent Translation Environment for Accessibility   Applications</h2><p><strong>Authors:Aleksandr Algazinov, Matt Laing, Paul Laban</strong></p>
<p>Accessibility remains a critical concern in today’s society, as many technologies are not developed to support the full range of user needs. Existing multi-agent systems (MAS) often cannot provide comprehensive assistance for users in need due to the lack of customization stemming from closed-source designs. Consequently, individuals with disabilities frequently encounter significant barriers when attempting to interact with digital environments. We introduce MATE, a multimodal accessibility MAS, which performs the modality conversions based on the user’s needs. The system is useful for assisting people with disabilities by ensuring that data will be converted to an understandable format. For instance, if the user cannot see well and receives an image, the system converts this image to its audio description. MATE can be applied to a wide range of domains, industries, and areas, such as healthcare, and can become a useful assistant for various groups of users. The system supports multiple types of models, ranging from LLM API calling to using custom machine learning (ML) classifiers. This flexibility ensures that the system can be adapted to various needs and is compatible with a wide variety of hardware. Since the system is expected to run locally, it ensures the privacy and security of sensitive information. In addition, the framework can be effectively integrated with institutional technologies (e.g., digital healthcare service) for real-time user assistance. Furthermore, we introduce ModCon-Task-Identifier, a model that is capable of extracting the precise modality conversion task from the user input. Numerous experiments show that ModCon-Task-Identifier consistently outperforms other LLMs and statistical models on our custom data. Our code and data are publicly available at <a target="_blank" rel="noopener" href="https://github.com/AlgazinovAleksandr/Multi-Agent-MATE">https://github.com/AlgazinovAleksandr/Multi-Agent-MATE</a>. </p>
<blockquote>
<p>在当今社会，无障碍性仍然是一个关键问题，因为许多技术并非为了支持各种用户需求而开发。现有的多智能体系统（MAS）往往由于封闭式设计的缺乏定制化而无法为用户提供全面的帮助。因此，残疾人在尝试与数字环境交互时经常遇到重大障碍。我们介绍了MATE，这是一个多模式访问MAS，它可以根据用户的需求进行模式转换。该系统通过确保数据转换为可理解格式，对于帮助残疾人非常有用。例如，如果用户视力不佳并收到图像，系统会将该图像转换为音频描述。MATE可广泛应用于各种领域、行业和地区，如医疗保健，可以成为各种用户的实用助手。该系统支持多种类型的模型，从调用大型语言模型API到使用自定义机器学习（ML）分类器。这种灵活性确保系统可以根据各种需求进行适应，并且与各种硬件兼容。由于系统预期在本地运行，因此确保敏感信息的隐私和安全。此外，该框架可以有效地与机构技术（例如数字医疗服务）集成，以进行实时用户协助。此外，我们引入了ModCon-Task-Identifier模型，它能够精确地从用户输入中提取模式转换任务。大量实验表明，ModCon-Task-Identifier在我们的自定义数据上始终优于其他大型语言模型和统计模型。我们的代码和数据公开可用在<a target="_blank" rel="noopener" href="https://github.com/AlgazinovAleksandr/Multi-Agent-MATE%E3%80%82">https://github.com/AlgazinovAleksandr/Multi-Agent-MATE。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.19502v2">PDF</a> </p>
<p><strong>Summary</strong><br>     多模态访问MAS系统（MATE）能根据用户需求进行模态转换，为残疾人提供全面的帮助。MATE适用于各种领域、行业和用户群体，支持多种类型的模型，可灵活适应各种需求。此外，MATE可本地运行以确保敏感信息的隐私和安全，并能有效地与机构技术集成以实现实时用户协助。我们引入了ModCon-Task-Identifier模型，能够从用户输入中提取精确模态转换任务，并在自定义数据上表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MATE系统采用多模态访问MAS技术，可为用户根据需要提供模态转换。</li>
<li>MATE对于支持残疾人使用数字技术至关重要，可将数据转换为易于理解的形式。</li>
<li>MATE具有广泛的应用范围，适用于多个领域、行业和用户群体。</li>
<li>MATE系统支持多种类型的模型，包括LLM API和自定义机器学习分类器，具有灵活性。</li>
<li>MATE可在本地运行以确保敏感信息的隐私和安全。</li>
<li>MATE能有效集成到机构技术中，提供实时用户协助。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.19502">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-068cbc089cf25737999fe5ad1c6bf9b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e10eeedd1a7556906ee12809c3909891.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4819c1e8d8c8869f9f8c4a738a585022.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d45e3ce87fc03f9ec469eb7640f6247e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a34b6a559c4284b6b0a142d32ebfa700.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-691a20ca5f6fe67c4912dd708578ff11.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MARL-MambaContour-Unleashing-Multi-Agent-Deep-Reinforcement-Learning-for-Active-Contour-Optimization-in-Medical-Image-Segmentation"><a href="#MARL-MambaContour-Unleashing-Multi-Agent-Deep-Reinforcement-Learning-for-Active-Contour-Optimization-in-Medical-Image-Segmentation" class="headerlink" title="MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning   for Active Contour Optimization in Medical Image Segmentation"></a>MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning   for Active Contour Optimization in Medical Image Segmentation</h2><p><strong>Authors:Ruicheng Zhang, Yu Sun, Zeyu Zhang, Jinai Li, Xiaofan Liu, Au Hoi Fan, Haowei Guo, Puxin Yan</strong></p>
<p>We introduce MARL-MambaContour, the first contour-based medical image segmentation framework based on Multi-Agent Reinforcement Learning (MARL). Our approach reframes segmentation as a multi-agent cooperation task focused on generate topologically consistent object-level contours, addressing the limitations of traditional pixel-based methods which could lack topological constraints and holistic structural awareness of anatomical regions. Each contour point is modeled as an autonomous agent that iteratively adjusts its position to align precisely with the target boundary, enabling adaptation to blurred edges and intricate morphologies common in medical images. This iterative adjustment process is optimized by a contour-specific Soft Actor-Critic (SAC) algorithm, further enhanced with the Entropy Regularization Adjustment Mechanism (ERAM) which dynamically balance agent exploration with contour smoothness. Furthermore, the framework incorporates a Mamba-based policy network featuring a novel Bidirectional Cross-attention Hidden-state Fusion Mechanism (BCHFM). This mechanism mitigates potential memory confusion limitations associated with long-range modeling in state space models, thereby facilitating more accurate inter-agent information exchange and informed decision-making. Extensive experiments on five diverse medical imaging datasets demonstrate the state-of-the-art performance of MARL-MambaContour, highlighting its potential as an accurate and robust clinical application. </p>
<blockquote>
<p>我们引入了基于多智能体强化学习（MARL）的首个轮廓基医疗图像分割框架MARL-MambaContour。我们的方法将分割重新构建为一个多智能体合作任务，专注于生成拓扑一致的物体级轮廓，解决了传统像素级方法可能缺乏拓扑约束和整体结构感知解剖区域的局限性。每个轮廓点都被建模为一个自主智能体，可以迭代调整其位置以精确地与目标边界对齐，从而适应医学图像中常见的模糊边缘和复杂形态。这种迭代调整过程通过轮廓特定的柔软演员评论家（SAC）算法优化，进一步通过熵正则化调整机制（ERAM）增强，该机制动态平衡智能体探索与轮廓平滑。此外，该框架采用基于Mamba的策略网络，并引入了一种新颖的双向交叉注意力隐藏状态融合机制（BCHFM）。该机制缓解了状态空间模型中长程建模可能带来的潜在内存混淆限制，从而促进了更准确的跨智能体信息交换和决策。在五个不同的医学成像数据集上的广泛实验证明了MARL-MambaContour的卓越性能，凸显了其在临床应用中作为准确且稳健的潜力的前景。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.18679v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于多智能体强化学习（MARL）的MambaContour轮廓基线医疗图像分割框架。它将分割重新构建为多智能体合作任务，侧重于生成拓扑一致的物体级轮廓，解决了传统像素级方法的局限性。通过轮廓特定的Soft Actor-Critic算法和熵正则化调整机制优化迭代调整过程。引入Mamba策略网络及双向交叉注意力隐藏状态融合机制，提高准确性。在五个医疗成像数据集上的实验证明了其卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MARL-MambaContour是首个基于多智能体强化学习（MARL）的轮廓基线医疗图像分割框架。</li>
<li>将分割任务重新构建为多智能体合作任务，专注于生成拓扑一致的物体级轮廓。</li>
<li>解决了传统像素级方法的局限性，包括缺乏拓扑约束和整体结构感知。</li>
<li>每个轮廓点被建模为自主智能体，可迭代调整位置以精确对齐目标边界，适应模糊边缘和复杂形态。</li>
<li>采用轮廓特定的Soft Actor-Critic算法和熵正则化调整机制优化迭代调整过程。</li>
<li>引入Mamba策略网络和双向交叉注意力隐藏状态融合机制，改善信息交换和决策制定。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.18679">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-44f6b84eb6dbe3591aacc36b5bd2ca31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7b8741f0392cde885ecb4ef733c361b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Agentic-Reasoning-A-Streamlined-Framework-for-Enhancing-LLM-Reasoning-with-Agentic-Tools"><a href="#Agentic-Reasoning-A-Streamlined-Framework-for-Enhancing-LLM-Reasoning-with-Agentic-Tools" class="headerlink" title="Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning   with Agentic Tools"></a>Agentic Reasoning: A Streamlined Framework for Enhancing LLM Reasoning   with Agentic Tools</h2><p><strong>Authors:Junde Wu, Jiayuan Zhu, Yuyuan Liu, Min Xu, Yueming Jin</strong></p>
<p>We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Agentic Reasoning dynamically leverages web search, code execution, and structured memory to address complex problems requiring deep research. A key innovation in our framework is the Mind-Map agent, which constructs a structured knowledge graph to store reasoning context and track logical relationships, ensuring coherence in long reasoning chains with extensive tool usage. Additionally, we conduct a comprehensive exploration of the Web-Search agent, leading to a highly effective search mechanism that surpasses all prior approaches. When deployed on DeepSeek-R1, our method achieves a new state-of-the-art (SOTA) among public models and delivers performance comparable to OpenAI Deep Research, the leading proprietary model in this domain. Extensive ablation studies validate the optimal selection of agentic tools and confirm the effectiveness of our Mind-Map and Web-Search agents in enhancing LLM reasoning. The code is at: <a target="_blank" rel="noopener" href="https://github.com/theworldofagents/Agentic-Reasoning">https://github.com/theworldofagents/Agentic-Reasoning</a> </p>
<blockquote>
<p>我们引入了Agentic Reasoning，这是一个通过整合外部工具使用代理来增强大型语言模型（LLM）推理能力的框架。Agentic Reasoning能够动态利用网页搜索、代码执行和结构化内存来解决需要深入研究的复杂问题。我们框架中的一个关键创新是Mind-Map代理，它构建了一个结构化知识图谱来存储推理上下文并跟踪逻辑关系，确保在长时间使用大量工具的长推理链中保持连贯性。此外，我们对Web-Search代理进行了全面的探索，导致出现了一个高效的搜索机制，超越了所有以前的方法。当部署在DeepSeek-R1上时，我们的方法达到了公共模型中的最新水平，并提供了与此领域领先的专有模型OpenAI Deep Research相当的性能。广泛的消融研究验证了代理工具的优选选择，并证实了我们的Mind-Map和Web-Search代理在提高LLM推理能力方面的有效性。代码位于：<a target="_blank" rel="noopener" href="https://github.com/theworldofagents/Agentic-Reasoning">https://github.com/theworldofagents/Agentic-Reasoning</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04644v2">PDF</a> ACL 2025</p>
<p><strong>Summary</strong><br>大语言模型（LLM）通过引入Agentic Reasoning框架得到增强，该框架集成了外部工具使用代理。Agentic Reasoning通过动态利用Web搜索、代码执行和结构化内存来解决需要深度研究的复杂问题。框架的关键创新在于Mind-Map代理，能够构建结构化知识图谱来存储推理上下文并跟踪逻辑关系，确保在长时间推理链中使用大量工具时的连贯性。此外，我们对Web-Search代理进行了全面的探索，开发出一种高效的搜索机制，超越了先前所有方法。部署在DeepSeek-R1上时，我们的方法达到了公共模型的新水平，性能与领域领先专有模型OpenAI Deep Research相当。广泛的消融研究验证了代理工具的最佳选择，并证实了我们的Mind-Map和Web-Search代理在提高LLM推理方面的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Agentic Reasoning框架集成了外部工具使用代理，增强了大型语言模型（LLM）的推理能力。</li>
<li>该框架通过动态利用Web搜索、代码执行和结构化内存解决复杂问题。</li>
<li>Mind-Map代理是框架的关键创新，能构建结构化知识图谱以存储推理上下文并跟踪逻辑关系。</li>
<li>Web-Search代理经过全面探索，开发出高效的搜索机制，超越先前方法。</li>
<li>该方法在DeepSeek-R1上部署时表现出卓越性能，达到公共模型的新水平。</li>
<li>与领域领先模型OpenAI Deep Research性能相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04644">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c9d944e769b0352de2da75494d09114b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f89aa540b9dfe859e3f5db3c95c9293f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e387f2452a86f11935f9dcc3e4b23a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1e5c5d9e65b2e795e74e7de8c703435.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8b0b30e435ecd607baa89592410d779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f566a41f9ea08a2f8eccadc15f3c7b36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a52320693b6a335fafe0b6af24fb9035.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Online-Intrinsic-Rewards-for-Decision-Making-Agents-from-Large-Language-Model-Feedback"><a href="#Online-Intrinsic-Rewards-for-Decision-Making-Agents-from-Large-Language-Model-Feedback" class="headerlink" title="Online Intrinsic Rewards for Decision Making Agents from Large Language   Model Feedback"></a>Online Intrinsic Rewards for Decision Making Agents from Large Language   Model Feedback</h2><p><strong>Authors:Qinqing Zheng, Mikael Henaff, Amy Zhang, Aditya Grover, Brandon Amos</strong></p>
<p>Automatically synthesizing dense rewards from natural language descriptions is a promising paradigm in reinforcement learning (RL), with applications to sparse reward problems, open-ended exploration, and hierarchical skill design. Recent works have made promising steps by exploiting the prior knowledge of large language models (LLMs). However, these approaches suffer from important limitations: they are either not scalable to problems requiring billions of environment samples, due to requiring LLM annotations for each observation, or they require a diverse offline dataset, which may not exist or be impossible to collect. In this work, we address these limitations through a combination of algorithmic and systems-level contributions. We propose ONI, a distributed architecture that simultaneously learns an RL policy and an intrinsic reward function using LLM feedback. Our approach annotates the agent’s collected experience via an asynchronous LLM server, which is then distilled into an intrinsic reward model. We explore a range of algorithmic choices for reward modeling with varying complexity, including hashing, classification, and ranking models. Our approach achieves state-of-the-art performance across a range of challenging tasks from the NetHack Learning Environment, while removing the need for large offline datasets required by prior work. We make our code available at <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/oni">https://github.com/facebookresearch/oni</a> . </p>
<blockquote>
<p>自动从自然语言描述中合成密集奖励是强化学习（RL）中一个有前途的模式，应用于稀疏奖励问题、开放式探索和分层技能设计。最近的工作通过利用大型语言模型（LLM）的先验知识取得了有希望的进展。然而，这些方法存在重要局限性：它们要么不可扩展至需要数亿环境样本的问题，因为每个观察都需要LLM注释，或者它们需要多样化的离线数据集，这可能不存在或无法收集。在这项工作中，我们通过算法和系统层面的贡献解决了这些局限性。我们提出了ONI，一种分布式架构，可以同时学习RL策略和使用LLM反馈的内在奖励函数。我们的方法通过异步LLM服务器对代理收集的经验进行注释，然后将其蒸馏成内在奖励模型。我们探索了一系列用于奖励建模的算法选择，具有不同的复杂性，包括哈希、分类和排名模型。我们的方法在NetHack学习环境中的一系列具有挑战性的任务上实现了最先进的性能，同时不需要先前工作所需的大量离线数据集。我们在<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/oni%E4%B8%8A%E5%85%AC%E5%BC%80%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/facebookresearch/oni上公开了我们的代码。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.23022v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种结合强化学习和大型语言模型反馈的分布式架构ONI，用于从自然语言描述中自动合成密集奖励。针对现有方法存在的可扩展性和离线数据集需求问题，本文提出了相应的算法和系统级贡献。ONI能够同时学习RL策略和内在奖励函数，通过异步LLM服务器对代理收集的经验进行注释，然后蒸馏成内在奖励模型。本文探索了不同复杂度的奖励建模算法选择，包括哈希、分类和排名模型。在NetHack学习环境的一系列挑战性任务上，该方法取得了最新性能，并去除了先前工作所需的大型离线数据集。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ONI是一种结合强化学习和大型语言模型（LLM）反馈的分布式架构。</li>
<li>ONI能够同时学习RL策略和内在奖励函数。</li>
<li>通过异步LLM服务器对代理收集的经验进行注释，然后蒸馏成内在奖励模型。</li>
<li>本文解决了现有方法在不要求大量环境样本时存在的可扩展性问题。</li>
<li>该方法不需要多样的离线数据集，解决了可能不存在或难以收集的问题。</li>
<li>在NetHack学习环境的挑战性任务上取得了最新性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.23022">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f93f2414f3817bc9ff020fb8994d0fff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-448200f9d07a5643b911488ed8004f68.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-62e4e903c93ef16490f5fdae9f544477.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Trajectory-Imputation-in-Multi-Agent-Sports-with-Derivative-Accumulating-Self-Ensemble"><a href="#Trajectory-Imputation-in-Multi-Agent-Sports-with-Derivative-Accumulating-Self-Ensemble" class="headerlink" title="Trajectory Imputation in Multi-Agent Sports with Derivative-Accumulating   Self-Ensemble"></a>Trajectory Imputation in Multi-Agent Sports with Derivative-Accumulating   Self-Ensemble</h2><p><strong>Authors:Han-Jun Choi, Hyunsung Kim, Minho Lee, Minchul Jeong, Chang-Jo Kim, Jinsung Yoon, Sang-Ki Ko</strong></p>
<p>Multi-agent trajectory data collected from domains such as team sports often suffer from missing values due to various factors. While many imputation methods have been proposed for spatiotemporal data, they are not well-suited for multi-agent sports scenarios where player movements are highly dynamic and inter-agent interactions continuously evolve. To address these challenges, we propose MIDAS (Multi-agent Imputer with Derivative-Accumulating Self-ensemble), a framework that imputes multi-agent trajectories with high accuracy and physical plausibility. It jointly predicts positions, velocities, and accelerations through a Set Transformer-based neural network and generates alternative estimates by recursively accumulating predicted velocity and acceleration values. These predictions are then combined using a learnable weighted ensemble to produce final imputed trajectories. Experiments on three sports datasets demonstrate that MIDAS significantly outperforms existing baselines in both positional accuracy and physical plausibility. Lastly, we showcase use cases of MIDAS, such as approximating total distance and pass success probability, to highlight its applicability to practical downstream tasks that require complete tracking data. </p>
<blockquote>
<p>从团队运动等领域收集的多智能体轨迹数据，由于各种因素往往存在缺失值。虽然许多时空数据的插值方法已经被提出，但它们并不适用于多智能体运动场景，因为球员的动作高度动态化，智能体间的互动也在不断变化。为了应对这些挑战，我们提出了MIDAS（具有导数累积自集成的多智能体插值器），这是一个能够高精度且物理上合理地对多智能体轨迹进行插值的框架。它通过基于集合变换器的神经网络来共同预测位置、速度和加速度，并通过递归累积预测的速度和加速度值来生成替代估计值。然后，这些预测值通过可学习的加权集成组合，生成最终的插值轨迹。在三个体育数据集上的实验表明，MIDAS在位置准确性和物理合理性方面显著优于现有基线。最后，我们展示了MIDAS的应用案例，如估算总距离和传球成功率等，以突出其在需要完整跟踪数据的实际下游任务中的适用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10878v4">PDF</a> Accepted at ECML&#x2F;PKDD 2025</p>
<p><strong>Summary</strong></p>
<p>多源数据缺失是团队运动等领域中常遇到的问题，尤其是在多智能体轨迹数据中。尽管已有许多时空数据插补方法，但它们并不适用于高度动态、交互多变的团队运动场景。本研究提出了MIDAS（基于衍生积累自组织的多智能体插补方法），利用基于集换元神经网络联合预测位置、速度和加速度，通过递归累积预测速度和加速度值生成替代估计值，再采用可学习的加权组合方式生成最终的插补轨迹。在三个运动数据集上的实验表明，MIDAS在位置准确性和物理合理性方面均显著优于现有基线方法。此外，我们还展示了MIDAS在估算总距离和传球成功率等实际应用场景中的应用，突显其在需要完整追踪数据的下游任务中的适用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多智能体轨迹数据缺失问题在团队运动等领域中普遍存在。</li>
<li>传统时空数据插补方法在多智能体运动场景中表现不佳。</li>
<li>MIDAS方法通过集换元神经网络联合预测位置、速度和加速度，具有高准确性和物理合理性。</li>
<li>MIDAS通过递归累积预测值和生成替代估计值来生成最终的插补轨迹。</li>
<li>实验结果表明，MIDAS在位置准确性和物理合理性方面优于现有方法。</li>
<li>MIDAS可应用于估算总距离和传球成功率等实际下游任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10878">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b35d4e9c8c40252557f8103e2e565107.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-068f0b4372900fee6eb7bb61b8d4f81d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-930774469bdaff36645b9d287b247d1b.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Agent/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Agent/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Agent/">
                                    <span class="chip bg-color">Agent</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-df399fd115c07e260f7908d5f08cbff1.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-07-17  CharaConsist Fine-Grained Consistent Character Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-cca6f327a9e0a5dece7366cde8a1f565.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-07-17  FalseReject A Resource for Improving Contextual Safety and Mitigating   Over-Refusals in LLMs via Structured Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
