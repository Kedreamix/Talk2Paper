<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-07-17  Comparative Analysis of Vision Transformers and Traditional Deep   Learning Approaches for Automated Pneumonia Detection in Chest X-Rays">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-43d037c500fcef7e128f86f6b3a711ed.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    45 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-17-更新"><a href="#2025-07-17-更新" class="headerlink" title="2025-07-17 更新"></a>2025-07-17 更新</h1><h2 id="Comparative-Analysis-of-Vision-Transformers-and-Traditional-Deep-Learning-Approaches-for-Automated-Pneumonia-Detection-in-Chest-X-Rays"><a href="#Comparative-Analysis-of-Vision-Transformers-and-Traditional-Deep-Learning-Approaches-for-Automated-Pneumonia-Detection-in-Chest-X-Rays" class="headerlink" title="Comparative Analysis of Vision Transformers and Traditional Deep   Learning Approaches for Automated Pneumonia Detection in Chest X-Rays"></a>Comparative Analysis of Vision Transformers and Traditional Deep   Learning Approaches for Automated Pneumonia Detection in Chest X-Rays</h2><p><strong>Authors:Gaurav Singh</strong></p>
<p>Pneumonia, particularly when induced by diseases like COVID-19, remains a critical global health challenge requiring rapid and accurate diagnosis. This study presents a comprehensive comparison of traditional machine learning and state-of-the-art deep learning approaches for automated pneumonia detection using chest X-rays (CXRs). We evaluate multiple methodologies, ranging from conventional machine learning techniques (PCA-based clustering, Logistic Regression, and Support Vector Classification) to advanced deep learning architectures including Convolutional Neural Networks (Modified LeNet, DenseNet-121) and various Vision Transformer (ViT) implementations (Deep-ViT, Compact Convolutional Transformer, and Cross-ViT). Using a dataset of 5,856 pediatric CXR images, we demonstrate that Vision Transformers, particularly the Cross-ViT architecture, achieve superior performance with 88.25% accuracy and 99.42% recall, surpassing traditional CNN approaches. Our analysis reveals that architectural choices impact performance more significantly than model size, with Cross-ViT’s 75M parameters outperforming larger models. The study also addresses practical considerations including computational efficiency, training requirements, and the critical balance between precision and recall in medical diagnostics. Our findings suggest that Vision Transformers offer a promising direction for automated pneumonia detection, potentially enabling more rapid and accurate diagnosis during health crises. </p>
<blockquote>
<p>肺炎，特别是由COVID-19等疾病引发的肺炎，仍然是全球一个重要的健康挑战，需要快速而准确的诊断。本研究对传统机器学习和最新深度学习方法进行全面比较，使用胸部X射线（CXRs）进行自动肺炎检测。我们评估了多种方法，包括传统机器学习技术（基于PCA的聚类、逻辑回归和支持向量分类）和先进的深度学习架构，包括卷积神经网络（修改后的LeNet、DenseNet-121）和各种视觉变压器（ViT）实现（Deep-ViT、紧凑卷积变压器和Cross-ViT）。我们使用包含5856张儿童CXR图像的数据集证明，视觉变压器，尤其是Cross-ViT架构，具有卓越的性能，准确率为88.25%，召回率为99.42%，超越了传统的CNN方法。我们的分析表明，与模型大小相比，架构选择对性能的影响更为显著，Cross-ViT的75M参数表现优于较大的模型。该研究还涉及实际考量，包括计算效率、培训要求以及在医学诊断中精度和召回率之间的关键平衡。我们的研究结果表明，视觉变压器在自动肺炎检测方面提供了有前景的方向，可能在健康危机期间实现更快速和准确的诊断。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10589v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了传统机器学习与最新深度学习技术在基于胸部X光片的自动化肺炎检测方面的应用对比。研究涵盖了多种方法，包括传统机器学习和先进的深度神经网络架构以及多种视觉转换器（Vision Transformer，ViT）。研究表明，视觉转换器，特别是Cross-ViT架构，表现出卓越性能，准确率高达88.25%，召回率高达99.42%，超越了传统CNN方法。此外，研究还发现模型架构的选择对性能的影响远大于模型大小。这项研究对于实现医疗诊断中的精准和高效有着重要意义，尤其是视觉转换器展现了应对肺炎检测健康危机的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是文中涉及的几个关键观点或发现，用简化中文进行列举：</p>
<ol>
<li>自动化肺炎检测是一个重要的全球健康挑战，特别是在COVID-19等疾病的背景下。</li>
<li>研究对比了传统机器学习和深度学习方法在基于胸部X光片的肺炎检测方面的表现。</li>
<li>视觉转换器（Vision Transformer）方法如Cross-ViT显示出优越性能，准确率较高。</li>
<li>模型架构的选择对性能的影响大于模型大小。</li>
<li>Cross-ViT架构在较小的参数数量（75M）下表现出高性能，超过了较大的模型。</li>
<li>研究考虑了计算效率、训练要求等实际问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10589">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2225a6334bc2ac5f6e8181e7dec30147.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cb539b0551644893c97ee357afa0711.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1844c936a293ce2abdde1d43cd1d3352.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d27d5feb472c8cedb0280399a21f6767.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3dcd6d04a78a951212caef0fcc882f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89a8b8c8db493d12477b0e871da7eee7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01b3dbbfa8762d1189e2cc2f9437f456.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec1e82e0f04d729394d071aeddeb065a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Mind-the-Gap-Aligning-Vision-Foundation-Models-to-Image-Feature-Matching"><a href="#Mind-the-Gap-Aligning-Vision-Foundation-Models-to-Image-Feature-Matching" class="headerlink" title="Mind the Gap: Aligning Vision Foundation Models to Image Feature   Matching"></a>Mind the Gap: Aligning Vision Foundation Models to Image Feature   Matching</h2><p><strong>Authors:Yuhan Liu, Jingwen Fu, Yang Wu, Kangyi Wu, Pengna Li, Jiayi Wu, Sanping Zhou, Jingmin Xin</strong></p>
<p>Leveraging the vision foundation models has emerged as a mainstream paradigm that improves the performance of image feature matching. However, previous works have ignored the misalignment when introducing the foundation models into feature matching. The misalignment arises from the discrepancy between the foundation models focusing on single-image understanding and the cross-image understanding requirement of feature matching. Specifically, 1) the embeddings derived from commonly used foundation models exhibit discrepancies with the optimal embeddings required for feature matching; 2) lacking an effective mechanism to leverage the single-image understanding ability into cross-image understanding. A significant consequence of the misalignment is they struggle when addressing multi-instance feature matching problems. To address this, we introduce a simple but effective framework, called IMD (Image feature Matching with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant solutions employing contrastive-learning based foundation models that emphasize global semantics, we integrate the generative-based diffusion models to effectively capture instance-level details. 2) We leverage the prompt mechanism in generative model as a natural tunnel, propose a novel cross-image interaction prompting module to facilitate bidirectional information interaction between image pairs. To more accurately measure the misalignment, we propose a new benchmark called IMIM, which focuses on multi-instance scenarios. Our proposed IMD establishes a new state-of-the-art in commonly evaluated benchmarks, and the superior improvement 12% in IMIM indicates our method efficiently mitigates the misalignment. </p>
<blockquote>
<p>利用视觉基础模型提高图像特征匹配的性能已成为主流范式。然而，先前的工作在将基础模型引入特征匹配时忽略了不匹配问题。这种不匹配源于基础模型对单图像理解的关注与特征匹配对跨图像理解的要求之间的差异。具体来说，1）常用基础模型产生的嵌入与用于特征匹配的最佳嵌入之间存在差异；2）缺乏一种有效的机制来利用单图像理解能力来实现跨图像理解。不匹配的一个显著后果是它们在解决多实例特征匹配问题时感到困难。为了解决这一问题，我们引入了一个简单有效的框架，称为IMD（带有预训练扩散模型的图像特征匹配），它包含两部分：1）与采用基于对比学习的基础模型的主流解决方案不同，这些基础模型强调全局语义，我们集成了基于生成扩散模型来有效捕获实例级细节。2）我们利用生成模型中的提示机制作为一个自然通道，提出了一种新型的跨图像交互提示模块，以促进图像对之间的双向信息交互。为了更准确地测量不匹配程度，我们提出了一个新的基准测试IMIM，它专注于多实例场景。我们提出的IMD在常用的基准测试中建立了新的最先进的水平，IMIM的12%的优越改进表明我们的方法有效地缓解了不匹配问题。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10318v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong><br>在这个文本中，作者介绍了利用预训练扩散模型进行图像特征匹配的新框架IMD。该框架解决了主流解决方案在处理多实例特征匹配问题时存在的局限性，通过引入生成型扩散模型，并结合跨图像交互提示模块，提高了图像特征匹配的准确性。同时，作者提出了一个新的基准测试IMIM，专注于多实例场景，以更准确地衡量不匹配的程度。IMD框架在常见的基准测试中建立了新的最高水平，并且在IMIM上的优异改进表明该方法有效地缓解了不匹配的问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用预训练扩散模型进行图像特征匹配已成为主流范式。</li>
<li>现有方法在处理多实例特征匹配时存在不匹配问题。</li>
<li>IMD框架通过引入生成型扩散模型解决此问题，强调实例级细节捕捉。</li>
<li>IMD框架结合跨图像交互提示模块，促进图像对之间的双向信息交互。</li>
<li>提出新的基准测试IMIM，专注于多实例场景以衡量不匹配程度。</li>
<li>IMD框架在基准测试中表现优异，并在IMIM上的改进表明其有效缓解不匹配问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10318">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b08340337df7f32f2cf9ff0b19db64b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87b2d554f56b2551404030a197fcdeb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f08b5db22167966a126ded28e63e838.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="When-Schrodinger-Bridge-Meets-Real-World-Image-Dehazing-with-Unpaired-Training"><a href="#When-Schrodinger-Bridge-Meets-Real-World-Image-Dehazing-with-Unpaired-Training" class="headerlink" title="When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired   Training"></a>When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired   Training</h2><p><strong>Authors:Yunwei Lan, Zhigao Cui, Xin Luo, Chang Liu, Nian Wang, Menglin Zhang, Yanzhao Su, Dong Liu</strong></p>
<p>Recent advancements in unpaired dehazing, particularly those using GANs, show promising performance in processing real-world hazy images. However, these methods tend to face limitations due to the generator’s limited transport mapping capability, which hinders the full exploitation of their effectiveness in unpaired training paradigms. To address these challenges, we propose DehazeSB, a novel unpaired dehazing framework based on the Schr&quot;odinger Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges the distributions between hazy and clear images. This enables optimal transport mappings from hazy to clear images in fewer steps, thereby generating high-quality results. To ensure the consistency of structural information and details in the restored images, we introduce detail-preserving regularization, which enforces pixel-level alignment between hazy inputs and dehazed outputs. Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP models in distinguishing hazy images and clear ones, by learning a haze-aware vision-language alignment. Extensive experiments on multiple real-world datasets demonstrate our method’s superiority. Code: <a target="_blank" rel="noopener" href="https://github.com/ywxjm/DehazeSB">https://github.com/ywxjm/DehazeSB</a>. </p>
<blockquote>
<p>近期在无配对去雾技术的最新进展，特别是使用生成对抗网络（GANs）的方法，在处理真实世界中的雾霾图像时表现出良好的性能。然而，这些方法往往面临由于生成器的有限传输映射能力而导致的局限性，这阻碍了它们在无配对训练模式中的完全有效性。为了解决这些挑战，我们提出了基于Schrödinger Bridge的去雾SB（DehazeSB）新型无配对去雾框架。通过利用最优传输（OT）理论，DehazeSB直接连接雾霾图像和清晰图像之间的分布。这能够实现从雾霾图像到清晰图像的最优传输映射，并且步骤更少，从而生成高质量的结果。为了确保恢复图像的结构信息的一致性并保留细节，我们引入了细节保留正则化方法，该方法强制雾霾输入和去雾输出之间的像素级对齐。此外，我们提出了一种新型提示学习法，利用预训练的CLIP模型来区分雾霾图像和清晰的图像，通过学习对雾霾有感知的视语言对齐。在多个真实世界数据集上的广泛实验证明了我们方法的优越性。代码地址：<a target="_blank" rel="noopener" href="https://github.com/ywxjm/DehazeSB">https://github.com/ywxjm/DehazeSB</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09524v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong><br>新一代基于GAN的无配对去雾技术取得显著进展，但仍面临生成器传输映射能力有限的挑战。我们提出一种基于Schrödinger Bridge的去雾框架DehazeSB，通过最优传输理论直接桥接雾天和晴朗图像分布，实现高质量去雾效果。此外，我们引入细节保留正则化和提示学习，提高结构信息的一致性和细节保留能力，并利用预训练CLIP模型进行图像清晰度判别。实验证明，该方法在真实数据集上具有优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GAN在去雾技术中的应用展现了前景，但仍受生成器映射能力限制。</li>
<li>DehazeSB框架基于Schrödinger Bridge和最优传输理论，实现雾天和晴朗图像分布的直接桥接。</li>
<li>DehazeSB能在较少的步骤内完成从雾到晴朗图像的传输映射，生成高质量结果。</li>
<li>引入细节保留正则化，确保恢复图像的结构信息和细节一致性。</li>
<li>提出一种新的提示学习方法，利用预训练的CLIP模型进行图像清晰度判别。</li>
<li>该方法在多个真实数据集上的实验表现优越。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09524">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3b450c8b3ba8321d99523473b6fe8fc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea740b41cb65952010dfdce96d4ea7dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b6563b5a22316ef7a8d97fbe6b99067.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-409e83408c6fe3a2358b635f10bb7f36.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ViT-ProtoNet-for-Few-Shot-Image-Classification-A-Multi-Benchmark-Evaluation"><a href="#ViT-ProtoNet-for-Few-Shot-Image-Classification-A-Multi-Benchmark-Evaluation" class="headerlink" title="ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark   Evaluation"></a>ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark   Evaluation</h2><p><strong>Authors:Abdulvahap Mutlu, Şengül Doğan, Türker Tuncer</strong></p>
<p>The remarkable representational power of Vision Transformers (ViTs) remains underutilized in few-shot image classification. In this work, we introduce ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical Network framework. By averaging class conditional token embeddings from a handful of support examples, ViT-ProtoNet constructs robust prototypes that generalize to novel categories under 5-shot settings. We conduct an extensive empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100, CUB-200, and CIFAR-FS, including overlapped support variants to assess robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based prototypical counterparts, achieving up to a 3.2% improvement in 5-shot accuracy and demonstrating superior feature separability in latent space. Furthermore, it outperforms or is competitive with transformer-based competitors using a more lightweight backbone. Comprehensive ablations examine the impact of transformer depth, patch size, and fine-tuning strategy. To foster reproducibility, we release code and pretrained weights. Our results establish ViT-ProtoNet as a powerful, flexible approach for few-shot classification and set a new baseline for transformer-based meta-learners. </p>
<blockquote>
<p>视觉Transformer（ViT）的表示能力十分强大，但在小样本图像分类中的应用尚未得到充分利用。在此工作中，我们推出了ViT-ProtoNet，它将ViT-Small骨干网集成到原型网络框架中。通过平均少量支持例子的类别条件令牌嵌入，ViT-ProtoNet构建了在5种场景设置下能够推广到新型类别的稳健原型。我们在四个标准数据集上进行了广泛的实证评估：Mini-ImageNet、FC100、CUB-200和CIFAR-FS，包括重叠的支持变体以评估稳健性。在所有拆分中，ViT-ProtoNet始终优于基于CNN的原型对应物，在5次射击的准确度上提高了高达3.2%，并在潜在空间中显示出优越的特征可分离性。此外，使用更轻量级骨干网的基于transformer的竞争对手相比，它具有出色的表现或与之竞争。全面的消融研究检验了transformer深度、补丁大小和微调策略的影响。为了促进可重复性，我们发布了代码和预训练权重。我们的结果确立了ViT-ProtoNet作为小样本分类的强大、灵活的方法，并为基于transformer的元学习者设定了新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09299v1">PDF</a> All codes are available at   <a target="_blank" rel="noopener" href="https://github.com/abdulvahapmutlu/vit-protonet">https://github.com/abdulvahapmutlu/vit-protonet</a></p>
<p><strong>Summary</strong></p>
<p>ViT-ProtoNet将Vision Transformer（ViT）的强大表示能力与Prototypical Network框架相结合，用于少样本图像分类。通过平均少量支持样本的类别条件令牌嵌入，构建可在新型类别中通用的稳健原型。在多个标准基准测试上的广泛实证评估表明，ViT-ProtoNet在5种场景设置中始终优于基于CNN的原型网络，在准确性方面取得了高达3.2%的提升，并显示出强大的特征分离能力。该研究为基于Vision Transformer的少样本分类建立了强大的灵活方法，并为基于transformer的元学习者设定了新的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViT-ProtoNet结合了Vision Transformer（ViT）和Prototypical Network框架，展示了强大的表示能力。</li>
<li>通过平均少量支持样本的类别条件令牌嵌入，构建了稳健的原型。</li>
<li>在多个标准基准测试上，ViT-ProtoNet在少样本图像分类方面表现出优异的性能。</li>
<li>与基于CNN的原型网络相比，ViT-ProtoNet在准确性方面有所提高，达到3.2%。</li>
<li>ViT-ProtoNet在潜在空间中的特征分离能力强大。</li>
<li>研究提供了关于transformer深度、补丁大小和微调策略的综合分析。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09299">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4d0610ed74405bb286dd1346ceae6d14.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Video-Inference-for-Human-Mesh-Recovery-with-Vision-Transformer"><a href="#Video-Inference-for-Human-Mesh-Recovery-with-Vision-Transformer" class="headerlink" title="Video Inference for Human Mesh Recovery with Vision Transformer"></a>Video Inference for Human Mesh Recovery with Vision Transformer</h2><p><strong>Authors:Hanbyel Cho, Jaesung Ahn, Yooshin Cho, Junmo Kim</strong></p>
<p>Human Mesh Recovery (HMR) from an image is a challenging problem because of the inherent ambiguity of the task. Existing HMR methods utilized either temporal information or kinematic relationships to achieve higher accuracy, but there is no method using both. Hence, we propose “Video Inference for Human Mesh Recovery with Vision Transformer (HMR-ViT)” that can take into account both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic Feature Image is constructed using feature vectors obtained from video frames by an image encoder. When generating the feature image, we use a Channel Rearranging Matrix (CRM) so that similar kinematic features could be located spatially close together. The feature image is then further encoded using Vision Transformer, and the SMPL pose and shape parameters are finally inferred using a regression network. Extensive evaluation on the 3DPW and Human3.6M datasets indicates that our method achieves a competitive performance in HMR. </p>
<blockquote>
<p>从图像中进行人体网格恢复（HMR）是一个具有挑战性的问题，因为该任务本身就存在固有的模糊性。现有的HMR方法利用时间信息或运动学关系来提高精度，但没有方法可以同时使用两者。因此，我们提出了“基于视频推理和视觉转换器的人体网格恢复（HMR-ViT）”，该方法可以考虑时间和运动学信息。在HMR-ViT中，使用来自视频帧的特征向量构建一个时间运动学特征图像，该图像由图像编码器生成。在生成特征图像时，我们使用通道重新排列矩阵（CRM），以便将相似的运动学特征在空间上紧密地定位在一起。然后，使用视觉转换器进一步对特征图像进行编码，并最终通过回归网络推断SMPL姿势和形状参数。在3DPW和Human3.6M数据集上的广泛评估表明，我们的方法在HMR中取得了具有竞争力的表现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08981v1">PDF</a> Accepted to IEEE FG 2023</p>
<p><strong>Summary</strong></p>
<p>本文提出一种利用视频信息来进行人体网格恢复（HMR）的新方法，名为“基于视频信息的HMR-ViT人体网格恢复”。该方法结合了时间信息和运动学关系，构建了一个时间运动特征图像，并使用图像编码器和视觉转换器进行特征提取和编码。通过重新安排通道矩阵（CRM），使相似的运动学特征在空间上更接近。最后，使用回归网络推断SMPL姿势和形状参数。在3DPW和Human3.6M数据集上的评估表明，该方法在HMR中取得了具有竞争力的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HMR是一个具有挑战性的问题，因为任务本身就存在固有的模糊性。</li>
<li>目前大多数HMR方法只利用时间信息或运动学关系来提高精度。</li>
<li>本文提出一种新的方法HMR-ViT，能同时考虑时间和运动学信息。</li>
<li>HMR-ViT构建了一个时间运动特征图像，通过图像编码器和视觉转换器进行特征提取和编码。</li>
<li>使用CRM来重新安排通道，使相似的运动学特征在空间上更接近。</li>
<li>通过回归网络推断SMPL姿势和形状参数。</li>
<li>在多个数据集上的评估表明，HMR-ViT在HMR任务中取得了具有竞争力的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08981">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-43d037c500fcef7e128f86f6b3a711ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52be6a03eb161dbb32929abcc9d9088e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ae441ea132046b43fffe0741efb2ef5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc2ad5b70d9565850ff1d32c719b2efa.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PRISM-Reducing-Spurious-Implicit-Biases-in-Vision-Language-Models-with-LLM-Guided-Embedding-Projection"><a href="#PRISM-Reducing-Spurious-Implicit-Biases-in-Vision-Language-Models-with-LLM-Guided-Embedding-Projection" class="headerlink" title="PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with   LLM-Guided Embedding Projection"></a>PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with   LLM-Guided Embedding Projection</h2><p><strong>Authors:Mahdiyar Molahasani, Azadeh Motamedi, Michael Greenspan, Il-Min Kim, Ali Etemad</strong></p>
<p>We introduce Projection-based Reduction of Implicit Spurious bias in vision-language Models (PRISM), a new data-free and task-agnostic solution for bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in their training data, leading to skewed predictions. PRISM is designed to debias VLMs without relying on predefined bias categories or additional external data. It operates in two stages: first, an LLM is prompted with simple class prompts to generate scene descriptions that contain spurious correlations. Next, PRISM uses our novel contrastive-style debiasing loss to learn a projection that maps the embeddings onto a latent space that minimizes spurious correlations while preserving the alignment between image and text embeddings.Extensive experiments demonstrate that PRISM outperforms current debiasing methods on the commonly used Waterbirds and CelebA datasets We make our code public at: <a target="_blank" rel="noopener" href="https://github.com/MahdiyarMM/PRISM">https://github.com/MahdiyarMM/PRISM</a>. </p>
<blockquote>
<p>我们引入了基于投影的隐式偏见减少（PRISM），这是一种新的无数据和任务无关的解决偏见缓解问题的方法，适用于如CLIP这样的视觉语言模型（VLMs）。VLMs通常会继承并放大其训练数据中的偏见，导致预测结果出现偏差。PRISM旨在在不依赖预定义的偏见类别或额外的外部数据的情况下对VLMs进行去偏处理。它分为两个阶段：首先，使用简单的类别提示提示大型语言模型（LLM）生成包含错误关联的场景描述。接下来，PRISM使用我们新颖的对比式去偏损失来学习一种投影方法，将嵌入映射到一个潜在空间，该空间在最小化错误关联的同时，保留图像和文本嵌入之间的对齐。大量实验表明，PRISM在常用的Waterbirds和CelebA数据集上的去偏效果优于当前的去偏方法。我们在以下网站公开了我们的代码：<a target="_blank" rel="noopener" href="https://github.com/MahdiyarMM/PRISM">https://github.com/MahdiyarMM/PRISM</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08979v1">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于投影的隐式偏见减少方法（PRISM），这是一种针对视觉语言模型（如CLIP）偏见缓解的新数据免费和任务无关的解决方案。PRISM旨在不依赖预设偏见类别或额外外部数据对VLMs进行去偏处理。它通过两个阶段进行操作：首先，用简单的类别提示提示大型语言模型生成包含错误相关性的场景描述；接着，PRISM使用新型对比风格去偏损失来学习将嵌入映射到潜在空间中的投影，这个潜在空间最小化错误相关性，同时保留图像和文本嵌入之间的对齐。广泛实验表明，PRISM在常用的小鸟与名人脸部识别数据集（Waterbirds和CelebA）上的去偏效果优于当前方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PRISM是一种针对视觉语言模型（VLMs）如CLIP的偏见缓解新方法。</li>
<li>PRISM是数据免费和任务无关的解决方案，旨在解决VLMs中继承并放大的偏见问题。</li>
<li>PRISM操作分为两个阶段：使用大型语言模型生成场景描述，并使用对比风格去偏损失进行投影映射。</li>
<li>PRISM通过最小化错误相关性并保留图像和文本嵌入之间的对齐来实现去偏。</li>
<li>PRISM在广泛实验中表现出良好的性能，特别是在Waterbirds和CelebA数据集上。</li>
<li>PRISM方法具有公开可用的代码，便于其他研究者使用和改进。</li>
<li>该方法不依赖预设偏见类别或额外外部数据，具有通用性和灵活性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08979">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-15964726ef48fe4b27c024f3ffc5e388.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a80af39f830b087d992dbb07df155700.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4fe5bbf97f22324c14ca61979a4c661.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="High-Fidelity-Differential-information-Driven-Binary-Vision-Transformer"><a href="#High-Fidelity-Differential-information-Driven-Binary-Vision-Transformer" class="headerlink" title="High-Fidelity Differential-information Driven Binary Vision Transformer"></a>High-Fidelity Differential-information Driven Binary Vision Transformer</h2><p><strong>Authors:Tian Gao, Zhiyuan Zhang, Kaijie Yin, Xu-Cheng Zhong, Hui Kong</strong></p>
<p>The binarization of vision transformers (ViTs) offers a promising approach to addressing the trade-off between high computational&#x2F;storage demands and the constraints of edge-device deployment. However, existing binary ViT methods often suffer from severe performance degradation or rely heavily on full-precision modules. To address these issues, we propose DIDB-ViT, a novel binary ViT that is highly informative while maintaining the original ViT architecture and computational efficiency. Specifically, we design an informative attention module incorporating differential information to mitigate information loss caused by binarization and enhance high-frequency retention. To preserve the fidelity of the similarity calculations between binary Q and K tensors, we apply frequency decomposition using the discrete Haar wavelet and integrate similarities across different frequencies. Additionally, we introduce an improved RPReLU activation function to restructure the activation distribution, expanding the model’s representational capacity. Experimental results demonstrate that our DIDB-ViT significantly outperforms state-of-the-art network quantization methods in multiple ViT architectures, achieving superior image classification and segmentation performance. </p>
<blockquote>
<p>视觉变压器的二值化（ViTs）为解决高计算&#x2F;存储需求与边缘设备部署限制之间的权衡提供了有前景的方法。然而，现有的二进制ViT方法常常面临性能严重下降的问题，或者严重依赖于全精度模块。为了解决这些问题，我们提出了DIDB-ViT，这是一种新型的二值ViT，在保持原始ViT架构和计算效率的同时，具有很高的信息量。具体来说，我们设计了一个包含差异信息的有效注意力模块，以减轻二值化引起的信息损失并增强高频保留。为了保留二进制Q和K张量之间相似性计算的保真度，我们采用离散哈尔小波进行频率分解并整合不同频率的相似性。此外，我们引入了一种改进的RPReLU激活函数来重构激活分布，扩大模型的表示能力。实验结果表明，我们的DIDB-ViT在多种ViT架构中显著优于最新的网络量化方法，实现了出色的图像分类和分割性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02222v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一个新型的二值化视觉转换器（DIDB-ViT），它在保持原始ViT架构和计算效率的同时，引入了具有差分信息的高度信息量的注意力模块来缓解二值化引起的信息损失并增强高频保留。同时采用离散小波变换进行频率分解，整合不同频率的相似性计算，改进RPReLU激活函数以重塑激活分布，扩大模型的表征能力。实验结果显示，DIDB-ViT在多个ViT架构上显著优于现有的网络量化方法，实现了出色的图像分类和分割性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DIDB-ViT解决了现有二进制ViT方法面临的性能下降或依赖全精度模块的问题。</li>
<li>引入了一种新型的注意力模块，该模块结合了差分信息以减少二值化带来的信息损失。</li>
<li>使用离散Haar小波变换进行频率分解，用以优化相似度计算。</li>
<li>创新地结合了不同频率下的相似性，增强模型的性能。</li>
<li>提出改进的RPReLU激活函数以重塑激活分布，增强模型的表征能力。</li>
<li>DIDB-ViT在多种ViT架构上实现了优于现有网络量化方法的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02222">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-810868a29d80c1184cc77dc5e31b168e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12c9afcbe17ccc84b42a782efea3f137.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4e6adfd29af2174bf75d9aa16689552.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9b5e2d185f24c0e6106c2370b60ad23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01d5217766a4e49a0d56805ae043c763.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LLM-enhanced-Action-aware-Multi-modal-Prompt-Tuning-for-Image-Text-Matching"><a href="#LLM-enhanced-Action-aware-Multi-modal-Prompt-Tuning-for-Image-Text-Matching" class="headerlink" title="LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text   Matching"></a>LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text   Matching</h2><p><strong>Authors:Mengxiao Tian, Xinxiao Wu, Shuo Yang</strong></p>
<p>Driven by large-scale contrastive vision-language pre-trained models such as CLIP, recent advancements in the image-text matching task have achieved remarkable success in representation learning. Due to image-level visual-language alignment, CLIP falls short in understanding fine-grained details such as object attributes and spatial relationships between objects. Recent efforts have attempted to compel CLIP to acquire structured visual representations by introducing prompt learning to achieve object-level alignment. While achieving promising results, they still lack the capability to perceive actions, which are crucial for describing the states or relationships between objects. Therefore, we propose to endow CLIP with fine-grained action-level understanding by introducing an LLM-enhanced action-aware multi-modal prompt-tuning method, incorporating the action-related external knowledge generated by large language models (LLMs). Specifically, we design an action triplet prompt and an action state prompt to exploit compositional semantic knowledge and state-related causal knowledge implicitly stored in LLMs. Subsequently, we propose an adaptive interaction module to aggregate attentive visual features conditioned on action-aware prompted knowledge for establishing discriminative and action-aware visual representations, which further improves the performance. Comprehensive experimental results on two benchmark datasets demonstrate the effectiveness of our method. </p>
<blockquote>
<p>在大规模对比视觉语言预训练模型（如CLIP）的驱动下，图像文本匹配任务的最新进展在表示学习方面取得了显著的成功。然而，由于图像级别的视觉语言对齐，CLIP在理解精细的细节方面表现不足，如对象属性和对象之间的空间关系。最近的努力尝试通过引入提示学习来迫使CLIP获得结构化视觉表示，以实现对象级别的对齐。虽然取得了一定的成果，但它们仍然缺乏感知动作的能力，这对于描述对象的状态或关系至关重要。因此，我们提出了一种通过引入LLM增强的动作感知多模式提示调整方法，为CLIP赋予精细的动作级别理解，该方法结合了由大型语言模型（LLM）生成的动作相关外部知识。具体来说，我们设计了动作三元组提示和动作状态提示来利用LLM中隐含的组成语义知识和状态相关因果知识。然后，我们提出了一个自适应交互模块，该模块根据动作感知提示知识聚合注意力视觉特征，以建立具有鉴别力和动作感知的视觉表示，这进一步提高了性能。在两个基准数据集上的综合实验结果证明了我们的方法的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23502v2">PDF</a> accepted by ICCV 2025</p>
<p><strong>Summary</strong><br>基于CLIP的大型对比式视觉语言预训练模型在图像文本匹配任务中取得了显著成功，但在理解对象属性、对象间空间关系等细节方面存在不足。为弥补这一缺陷，近期研究尝试引入提示学习来引导CLIP获取结构化视觉表征，实现对象级别的对齐。然而，这些方法仍无法感知动作，对描述对象状态或关系至关重要。因此，我们提出一种结合大型语言模型（LLM）增强的动作感知多模式提示调整方法，引入动作相关的外部知识。通过设计动作三元组提示和动作状态提示来利用LLM中隐含的组成语义知识和状态相关因果知识，并提出自适应交互模块，根据动作感知提示知识聚合注意力视觉特征，建立有判别力和动作感知的视觉表征，进一步提高性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型对比式视觉语言预训练模型（如CLIP）在图像文本匹配任务中取得显著进展。</li>
<li>CLIP在理解图像细节（如对象属性、空间关系）方面存在局限。</li>
<li>为改进CLIP，引入提示学习以实现对象级别的对齐是一种有效方法。</li>
<li>现有方法缺乏动作感知能力，这是描述对象状态或关系的关键。</li>
<li>提出结合大型语言模型（LLM）增强的动作感知多模式提示调整方法。</li>
<li>通过设计动作三元组提示和动作状态提示，利用LLM中的组成语义和状态相关因果知识。</li>
<li>引入自适应交互模块，根据动作感知提示知识聚合注意力视觉特征，提高性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23502">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-75f01832f6f137d119ee70dd1a3e50d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bb09dd63976b61d7f29323e6e6ef995.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78b93b1036fbf876c4bd54f47d444276.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56b24c2c7684fee54457dc2fcad7642a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="BreastDCEDL-A-Comprehensive-Breast-Cancer-DCE-MRI-Dataset-and-Transformer-Implementation-for-Treatment-Response-Prediction"><a href="#BreastDCEDL-A-Comprehensive-Breast-Cancer-DCE-MRI-Dataset-and-Transformer-Implementation-for-Treatment-Response-Prediction" class="headerlink" title="BreastDCEDL: A Comprehensive Breast Cancer DCE-MRI Dataset and   Transformer Implementation for Treatment Response Prediction"></a>BreastDCEDL: A Comprehensive Breast Cancer DCE-MRI Dataset and   Transformer Implementation for Treatment Response Prediction</h2><p><strong>Authors:Naomi Fridman, Bubby Solway, Tomer Fridman, Itamar Barnea, Anat Goldstein</strong></p>
<p>Breast cancer remains a leading cause of cancer-related mortality worldwide, making early detection and accurate treatment response monitoring critical priorities. We present BreastDCEDL, a curated, deep learning-ready dataset comprising pre-treatment 3D Dynamic Contrast-Enhanced MRI (DCE-MRI) scans from 2,070 breast cancer patients drawn from the I-SPY1, I-SPY2, and Duke cohorts, all sourced from The Cancer Imaging Archive. The raw DICOM imaging data were rigorously converted into standardized 3D NIfTI volumes with preserved signal integrity, accompanied by unified tumor annotations and harmonized clinical metadata including pathologic complete response (pCR), hormone receptor (HR), and HER2 status. Although DCE-MRI provides essential diagnostic information and deep learning offers tremendous potential for analyzing such complex data, progress has been limited by lack of accessible, public, multicenter datasets. BreastDCEDL addresses this gap by enabling development of advanced models, including state-of-the-art transformer architectures that require substantial training data. To demonstrate its capacity for robust modeling, we developed the first transformer-based model for breast DCE-MRI, leveraging Vision Transformer (ViT) architecture trained on RGB-fused images from three contrast phases (pre-contrast, early post-contrast, and late post-contrast). Our ViT model achieved state-of-the-art pCR prediction performance in HR+&#x2F;HER2- patients (AUC 0.94, accuracy 0.93). BreastDCEDL includes predefined benchmark splits, offering a framework for reproducible research and enabling clinically meaningful modeling in breast cancer imaging. </p>
<blockquote>
<p>乳腺癌仍然是全球癌症相关死亡的主要原因之一，因此早期检测和精确的治疗反应监测成为至关重要的优先事项。我们推出了BreastDCEDL，这是一个精心策划、为深度学习准备的数据集，包含来自I-SPY1、I-SPY2和Duke队列的2070例乳腺癌患者的治疗前3D动态增强MRI（DCE-MRI）扫描，所有数据均来自癌症成像档案。原始的DICOM成像数据被严格转换为标准化的3DNIfTI体积数据，同时保留了信号完整性，并附有统一的肿瘤注释和协调一致的临床元数据，包括病理完全反应（pCR）、激素受体（HR）和HER2状态。尽管DCE-MRI提供了重要的诊断信息，深度学习在分析此类复杂数据方面拥有巨大潜力，但由于缺乏可访问的公共多中心数据集，进展一直受到限制。BreastDCEDL通过支持开发先进模型来解决这一差距，包括需要大量训练数据的最新变压器架构。为了展示其稳健建模的能力，我们开发了基于变压器的首个乳腺癌DCE-MRI模型，该模型利用在三个对比阶段（预对比、早期后对比和晚期后对比）的RGB融合图像上训练的Vision Transformer（ViT）架构。我们的ViT模型在HR+&#x2F;HER2-患者中实现了最先进的pCR预测性能（AUC 0.94，准确率0.93）。BreastDCEDL包括预定义的基准测试分割，为可重复的研究提供了一个框架，并在乳腺癌成像中实现了具有临床意义的建模。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12190v3">PDF</a> </p>
<p><strong>Summary</strong><br>     乳腺癌仍是全球癌症死亡的主要原因之一，早期检测和准确的治疗反应监测是关键。我们推出BreastDCEDL数据集，包含来自I-SPY1、I-SPY2和Duke队列的2070例乳腺癌患者的预治疗3D动态增强MRI（DCE-MRI）扫描图像，源于癌症成像档案。数据集包含标准化的3DNIfTI体积数据、统一的肿瘤注释和协调的临床元数据，如病理完全反应、激素受体和HER2状态。BreastDCEDL解决了缺乏公共多中心数据集的问题，支持先进模型的开发，包括基于最新Transformer架构的模型。我们开发了基于Vision Transformer的乳腺DCE-MRI模型，在HR+&#x2F;HER2-患者中实现了优异性能。BreastDCEDL包含预设的基准分割，为可重复的研究提供框架，并能在乳腺癌成像中进行临床意义建模。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>乳腺癌仍是全球主要的癌症死亡原因之一，早期检测和精准治疗反应监测是关键挑战。</li>
<li>BreastDCEDL是一个包含预治疗DCE-MRI扫描的深度学习就绪数据集，涵盖了大量患者数据并具备标准化处理的临床元数据。</li>
<li>数据集解决了缺乏公共多中心数据集的难题，为开发先进的模型提供了机会，包括基于Vision Transformer的模型。</li>
<li>首次展示了基于Vision Transformer的乳腺DCE-MRI模型，在特定患者群体中实现了高性能预测。</li>
<li>BreastDCEDL包括预设的基准分割，有助于进行可重复的研究和临床意义建模。</li>
<li>数据集可用于开发预测病理完全反应的模型，对临床决策具有潜在价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12190">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-aec89b4433d9b729a2179ba0a92423f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-100bd057869bb8cf967db06d3455a640.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a9f0ec1c0db9c5ca2eebe0ec3ac097f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b64260b9af43fcc9c67030971d20f4c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Pathfinder-for-Low-altitude-Aircraft-with-Binary-Neural-Network"><a href="#Pathfinder-for-Low-altitude-Aircraft-with-Binary-Neural-Network" class="headerlink" title="Pathfinder for Low-altitude Aircraft with Binary Neural Network"></a>Pathfinder for Low-altitude Aircraft with Binary Neural Network</h2><p><strong>Authors:Kaijie Yin, Tian Gao, Hui Kong</strong></p>
<p>A prior global topological map (e.g., the OpenStreetMap, OSM) can boost the performance of autonomous mapping by a ground mobile robot. However, the prior map is usually incomplete due to lacking labeling in partial paths. To solve this problem, this paper proposes an OSM maker using airborne sensors carried by low-altitude aircraft, where the core of the OSM maker is a novel efficient pathfinder approach based on LiDAR and camera data, i.e., a binary dual-stream road segmentation model. Specifically, a multi-scale feature extraction based on the UNet architecture is implemented for images and point clouds. To reduce the effect caused by the sparsity of point cloud, an attention-guided gated block is designed to integrate image and point-cloud features. To optimize the model for edge deployment that significantly reduces storage footprint and computational demands, we propose a binarization streamline to each model component, including a variant of vision transformer (ViT) architecture as the encoder of the image branch, and new focal and perception losses to optimize the model training. The experimental results on two datasets demonstrate that our pathfinder method achieves SOTA accuracy with high efficiency in finding paths from the low-level airborne sensors, and we can create complete OSM prior maps based on the segmented road skeletons. Code and data are available at: \href{<a target="_blank" rel="noopener" href="https://github.com/IMRL/Pathfinder%7D%7Bhttps://github.com/IMRL/Pathfinder%7D">https://github.com/IMRL/Pathfinder}{https://github.com/IMRL/Pathfinder}</a>. </p>
<blockquote>
<p>先前的全局拓扑地图（例如，OpenStreetMap，OSM）可以通过地面移动机器人提升自主测绘的性能。然而，由于部分路径缺乏标注，先前的地图通常是不完整的。为了解决这一问题，本文提出了一种使用低空飞机携带的机载传感器的OSM制作方法。其中，OSM制作的核心是一种基于激光雷达和相机数据的新型高效路径查找方法，即二进制双流道路分割模型。具体来说，基于UNet架构实现了图像和点云的多尺度特征提取。为了减少点云稀疏造成的影响，设计了一种注意力引导的门控块来融合图像和点云特征。为了优化模型进行边缘部署，以显著降低存储空间和计算需求，我们对模型的每个组件都进行了二值化处理，包括使用变种视觉转换器（ViT）架构作为图像分支的编码器，以及新的焦点和感知损失来优化模型训练。在两个数据集上的实验结果表明，我们的路径查找方法以高效率达到了最先进水平的精度，可以从低级别机载传感器中找到路径，并且我们可以基于分割的道路骨架创建完整的OSM先前地图。代码和数据可在[<a target="_blank" rel="noopener" href="https://github.com/IMRL/Pathfinder]%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/IMRL/Pathfinder]找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08824v4">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>利用先验全局拓扑地图（如OpenStreetMap，OSM）可以提高地面移动机器人的自主映射性能。然而，由于部分路径缺乏标注，先验地图通常是不完整的。为解决此问题，本文提出了一种利用低空飞机携带的机载传感器构建OSM的方法，其核心是一种基于激光雷达和相机数据的新型高效路径查找方法，即二进制双流道路分割模型。具体实现了基于UNet架构的多尺度特征提取，用于图像和点云。为解决点云稀疏引起的影响，设计了注意力引导门控块来整合图像和点云特征。为优化模型进行边缘部署，显著降低存储和计算需求，我们对模型各组件提出了一条二进制化流线，包括使用视觉变压器（ViT）架构作为图像分支的编码器，以及新的焦点和感知损失以优化模型训练。在两个数据集上的实验结果表明，我们的路径查找方法具有高精度和高效率地寻找来自低级别机载传感器的路径，并且我们可以基于分割的道路骨架创建完整的OSM先验地图。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>先验全局拓扑地图（如OpenStreetMap）能提高自主映射性能。</li>
<li>提出的OSM制作方法使用低空飞机的机载传感器。</li>
<li>路径查找方法的核心是二进制双流道路分割模型。</li>
<li>该模型使用基于UNet架构的多尺度特征提取处理图像和点云数据。</li>
<li>为应对点云的稀疏性，结合了图像和点云特征的注意力引导门控块。</li>
<li>为边缘部署优化了模型，包括使用视觉变压器架构和新的焦点及感知损失。</li>
<li>实验结果表明，该方法在寻找低级别机载传感器路径方面达到高精度和高效率，并能创建完整的OSM先验地图。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08824">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7c0f84def7d3dfaa9437c066eed0c43d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce73b9af3e93857e119f615bf970bbe0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1516d7e046ae4a10806301539e39e150.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20950002ef60cd74f23be360b05c251c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f2ed11906713c0e20cfaf73b259df12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ce303143b4b4fcdd428cfaa4f45c7a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de29c8a438da1aa3044540e52009a4fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0563e97ea436d0303a3ce4ee8aff5676.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6394e033381d4ddcfc400af63cc17365.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="The-Utility-of-the-Virtual-Imaging-Trials-Methodology-for-Objective-Characterization-of-AI-Systems-and-Training-Data"><a href="#The-Utility-of-the-Virtual-Imaging-Trials-Methodology-for-Objective-Characterization-of-AI-Systems-and-Training-Data" class="headerlink" title="The Utility of the Virtual Imaging Trials Methodology for Objective   Characterization of AI Systems and Training Data"></a>The Utility of the Virtual Imaging Trials Methodology for Objective   Characterization of AI Systems and Training Data</h2><p><strong>Authors:Fakrul Islam Tushar, Lavsen Dahal, Saman Sotoudeh-Paima, Ehsan Abadi, W. Paul Segars, Ehsan Samei, Joseph Y. Lo</strong></p>
<p>The credibility of Artificial Intelligence (AI) models for medical imaging continues to be a challenge, affected by the diversity of models, the data used to train the models, and applicability of their combination to produce reproducible results for new data. In this work we aimed to explore if the emerging Virtual Imaging Trials (VIT) methodologies can provide an objective resource to approach this challenge. The study was conducted for the case example of COVID-19 diagnosis using clinical and virtual computed tomography (CT) and chest radiography (CXR) processed with convolutional neural networks (CNNs). Multiple AI models were developed and tested using 3D ResNet-like and 2D EfficientNetv2 architectures across diverse datasets. The performance differences were evaluated in terms of the area under the curve (AUC) and the DeLong method for AUC confidence intervals. The models trained on the most diverse datasets showed the highest external testing performance, with AUC values ranging from 0.73 to 0.76 for CT and 0.70 to 0.73 for CXR. Internal testing yielded higher AUC values (0.77 to 0.85 for CT and 0.77 to 1.0 for CXR), highlighting a substantial drop in performance during external validation, which underscores the importance of diverse and comprehensive training and testing data. Most notably, the VIT approach provided objective assessment of the utility of diverse models and datasets while further providing insight into the influence of dataset characteristics, patient factors, and imaging physics on AI efficacy. The VIT approach can be used to enhance model transparency and reliability, offering nuanced insights into the factors driving AI performance and bridging the gap between experimental and clinical settings. </p>
<blockquote>
<p>人工智能（AI）模型在医学影像领域的可信度仍然是一个挑战，受到模型多样性、用于训练模型的数据，以及它们组合应用于新数据产生可重复结果的影响。在这项工作中，我们旨在探索新兴的虚拟成像试验（VIT）方法是否可以为应对这一挑战提供客观资源。该研究是针对COVID-19诊断的案例分析，使用临床和虚拟计算机断层扫描（CT）以及经过卷积神经网络（CNN）处理的胸部X射线（CXR）。使用3D ResNet和2D EfficientNetv2架构开发并测试了多个AI模型，涉及不同的数据集。性能差异是根据曲线下面积（AUC）和AUC置信区间的DeLong方法进行评估的。在最具多样性的数据集上训练的模型在外部测试中的表现最佳，CT的AUC值范围从0.73到0.76，CXR的AUC值范围从0.70到0.73。内部测试的AUC值较高（CT的AUC值为0.77到0.85，CXR的AUC值为0.77到1.0），突显了外部验证中性能的大幅下降，这强调了多样性和全面的训练和测试数据的重要性。值得注意的是，VIT方法为不同的模型和数据集提供了客观评估，同时进一步深入了解数据集特性、患者因素和成像物理学对AI效力的影响。VIT方法可用于提高模型的透明度和可靠性，提供关于驱动AI性能因素的微妙见解，并弥合实验与临床环境之间的差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09730v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文探讨了人工智能模型在医学影像领域的可信度问题，研究了虚拟成像试验（VIT）方法能否为解决此问题提供客观资源。以新冠肺炎诊断为例，研究使用临床和虚拟计算机断层扫描（CT）以及胸部放射摄影（CXR）处理的卷积神经网络（CNN）模型，并在多样数据集上进行训练和测试。研究结果表明，在多样数据集上训练的模型在外部测试中具有最高性能，而内部测试的性能较高，凸显了外部验证时性能的显著下降。虚拟成像试验（VIT）方法为评估不同模型和数据集提供了客观视角，并进一步深入探究了数据集特征、患者因素和成像物理对人工智能效能的影响。此研究方法可以提高模型的透明度和可靠性，并为弥合实验与临床之间的差距提供深刻见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人工智能模型在医学影像领域的可信度面临挑战，主要受模型多样性、训练数据多样性和对新数据产生可重复结果的能力影响。</li>
<li>虚拟成像试验（VIT）方法为评估模型性能提供了新的视角。</li>
<li>使用临床和虚拟计算机断层扫描（CT）以及胸部放射摄影（CXR）处理卷积神经网络（CNN）进行新冠肺炎诊断的模型测试表明多样数据集训练的模型具有最佳性能。</li>
<li>外部验证结果揭示了模型的性能下降，突显了训练和测试数据的多样性和全面性的重要性。</li>
<li>VIT方法能客观地评估不同模型和数据集的性能，提供对AI效能影响因素的深刻见解，如数据集特性、患者因素和成像物理。</li>
<li>VIT方法有助于增强模型的透明度和可靠性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.09730">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-89cf9bc7f742bc73c14f21144f976f7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-479b6f5791168b98df761e27ec55928e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-186bf0c29239a0e9db03a3eaf6a68879.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6c30f54fa848454ef5c022bbc33b3c15.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-07-17  Alleviating Textual Reliance in Medical Language-guided Segmentation via   Prototype-driven Semantic Approximation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-16/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ee0423bb158a1a1596661f062511dc8e.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-07-17  Streaming 4D Visual Geometry Transformer
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30166.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
