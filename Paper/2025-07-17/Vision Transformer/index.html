<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  Comparative Analysis of Vision Transformers and Traditional Deep   Learning Approaches for Automated Pneumonia Detection in Chest X-Rays">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-43d037c500fcef7e128f86f6b3a711ed.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    45 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-17-æ›´æ–°"><a href="#2025-07-17-æ›´æ–°" class="headerlink" title="2025-07-17 æ›´æ–°"></a>2025-07-17 æ›´æ–°</h1><h2 id="Comparative-Analysis-of-Vision-Transformers-and-Traditional-Deep-Learning-Approaches-for-Automated-Pneumonia-Detection-in-Chest-X-Rays"><a href="#Comparative-Analysis-of-Vision-Transformers-and-Traditional-Deep-Learning-Approaches-for-Automated-Pneumonia-Detection-in-Chest-X-Rays" class="headerlink" title="Comparative Analysis of Vision Transformers and Traditional Deep   Learning Approaches for Automated Pneumonia Detection in Chest X-Rays"></a>Comparative Analysis of Vision Transformers and Traditional Deep   Learning Approaches for Automated Pneumonia Detection in Chest X-Rays</h2><p><strong>Authors:Gaurav Singh</strong></p>
<p>Pneumonia, particularly when induced by diseases like COVID-19, remains a critical global health challenge requiring rapid and accurate diagnosis. This study presents a comprehensive comparison of traditional machine learning and state-of-the-art deep learning approaches for automated pneumonia detection using chest X-rays (CXRs). We evaluate multiple methodologies, ranging from conventional machine learning techniques (PCA-based clustering, Logistic Regression, and Support Vector Classification) to advanced deep learning architectures including Convolutional Neural Networks (Modified LeNet, DenseNet-121) and various Vision Transformer (ViT) implementations (Deep-ViT, Compact Convolutional Transformer, and Cross-ViT). Using a dataset of 5,856 pediatric CXR images, we demonstrate that Vision Transformers, particularly the Cross-ViT architecture, achieve superior performance with 88.25% accuracy and 99.42% recall, surpassing traditional CNN approaches. Our analysis reveals that architectural choices impact performance more significantly than model size, with Cross-ViTâ€™s 75M parameters outperforming larger models. The study also addresses practical considerations including computational efficiency, training requirements, and the critical balance between precision and recall in medical diagnostics. Our findings suggest that Vision Transformers offer a promising direction for automated pneumonia detection, potentially enabling more rapid and accurate diagnosis during health crises. </p>
<blockquote>
<p>è‚ºç‚ï¼Œç‰¹åˆ«æ˜¯ç”±COVID-19ç­‰ç–¾ç—…å¼•å‘çš„è‚ºç‚ï¼Œä»ç„¶æ˜¯å…¨çƒä¸€ä¸ªé‡è¦çš„å¥åº·æŒ‘æˆ˜ï¼Œéœ€è¦å¿«é€Ÿè€Œå‡†ç¡®çš„è¯Šæ–­ã€‚æœ¬ç ”ç©¶å¯¹ä¼ ç»Ÿæœºå™¨å­¦ä¹ å’Œæœ€æ–°æ·±åº¦å­¦ä¹ æ–¹æ³•è¿›è¡Œå…¨é¢æ¯”è¾ƒï¼Œä½¿ç”¨èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRsï¼‰è¿›è¡Œè‡ªåŠ¨è‚ºç‚æ£€æµ‹ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼ˆåŸºäºPCAçš„èšç±»ã€é€»è¾‘å›å½’å’Œæ”¯æŒå‘é‡åˆ†ç±»ï¼‰å’Œå…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼ŒåŒ…æ‹¬å·ç§¯ç¥ç»ç½‘ç»œï¼ˆä¿®æ”¹åçš„LeNetã€DenseNet-121ï¼‰å’Œå„ç§è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰å®ç°ï¼ˆDeep-ViTã€ç´§å‡‘å·ç§¯å˜å‹å™¨å’ŒCross-ViTï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨åŒ…å«5856å¼ å„¿ç«¥CXRå›¾åƒçš„æ•°æ®é›†è¯æ˜ï¼Œè§†è§‰å˜å‹å™¨ï¼Œå°¤å…¶æ˜¯Cross-ViTæ¶æ„ï¼Œå…·æœ‰å“è¶Šçš„æ€§èƒ½ï¼Œå‡†ç¡®ç‡ä¸º88.25%ï¼Œå¬å›ç‡ä¸º99.42%ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„CNNæ–¹æ³•ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œä¸æ¨¡å‹å¤§å°ç›¸æ¯”ï¼Œæ¶æ„é€‰æ‹©å¯¹æ€§èƒ½çš„å½±å“æ›´ä¸ºæ˜¾è‘—ï¼ŒCross-ViTçš„75Må‚æ•°è¡¨ç°ä¼˜äºè¾ƒå¤§çš„æ¨¡å‹ã€‚è¯¥ç ”ç©¶è¿˜æ¶‰åŠå®é™…è€ƒé‡ï¼ŒåŒ…æ‹¬è®¡ç®—æ•ˆç‡ã€åŸ¹è®­è¦æ±‚ä»¥åŠåœ¨åŒ»å­¦è¯Šæ–­ä¸­ç²¾åº¦å’Œå¬å›ç‡ä¹‹é—´çš„å…³é”®å¹³è¡¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè§†è§‰å˜å‹å™¨åœ¨è‡ªåŠ¨è‚ºç‚æ£€æµ‹æ–¹é¢æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ï¼Œå¯èƒ½åœ¨å¥åº·å±æœºæœŸé—´å®ç°æ›´å¿«é€Ÿå’Œå‡†ç¡®çš„è¯Šæ–­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10589v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ ä¸æœ€æ–°æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨åŸºäºèƒ¸éƒ¨Xå…‰ç‰‡çš„è‡ªåŠ¨åŒ–è‚ºç‚æ£€æµ‹æ–¹é¢çš„åº”ç”¨å¯¹æ¯”ã€‚ç ”ç©¶æ¶µç›–äº†å¤šç§æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿæœºå™¨å­¦ä¹ å’Œå…ˆè¿›çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ä»¥åŠå¤šç§è§†è§‰è½¬æ¢å™¨ï¼ˆVision Transformerï¼ŒViTï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè§†è§‰è½¬æ¢å™¨ï¼Œç‰¹åˆ«æ˜¯Cross-ViTæ¶æ„ï¼Œè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå‡†ç¡®ç‡é«˜è¾¾88.25%ï¼Œå¬å›ç‡é«˜è¾¾99.42%ï¼Œè¶…è¶Šäº†ä¼ ç»ŸCNNæ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°æ¨¡å‹æ¶æ„çš„é€‰æ‹©å¯¹æ€§èƒ½çš„å½±å“è¿œå¤§äºæ¨¡å‹å¤§å°ã€‚è¿™é¡¹ç ”ç©¶å¯¹äºå®ç°åŒ»ç–—è¯Šæ–­ä¸­çš„ç²¾å‡†å’Œé«˜æ•ˆæœ‰ç€é‡è¦æ„ä¹‰ï¼Œå°¤å…¶æ˜¯è§†è§‰è½¬æ¢å™¨å±•ç°äº†åº”å¯¹è‚ºç‚æ£€æµ‹å¥åº·å±æœºçš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡ä¸­æ¶‰åŠçš„å‡ ä¸ªå…³é”®è§‚ç‚¹æˆ–å‘ç°ï¼Œç”¨ç®€åŒ–ä¸­æ–‡è¿›è¡Œåˆ—ä¸¾ï¼š</p>
<ol>
<li>è‡ªåŠ¨åŒ–è‚ºç‚æ£€æµ‹æ˜¯ä¸€ä¸ªé‡è¦çš„å…¨çƒå¥åº·æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨COVID-19ç­‰ç–¾ç—…çš„èƒŒæ™¯ä¸‹ã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨åŸºäºèƒ¸éƒ¨Xå…‰ç‰‡çš„è‚ºç‚æ£€æµ‹æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>è§†è§‰è½¬æ¢å™¨ï¼ˆVision Transformerï¼‰æ–¹æ³•å¦‚Cross-ViTæ˜¾ç¤ºå‡ºä¼˜è¶Šæ€§èƒ½ï¼Œå‡†ç¡®ç‡è¾ƒé«˜ã€‚</li>
<li>æ¨¡å‹æ¶æ„çš„é€‰æ‹©å¯¹æ€§èƒ½çš„å½±å“å¤§äºæ¨¡å‹å¤§å°ã€‚</li>
<li>Cross-ViTæ¶æ„åœ¨è¾ƒå°çš„å‚æ•°æ•°é‡ï¼ˆ75Mï¼‰ä¸‹è¡¨ç°å‡ºé«˜æ€§èƒ½ï¼Œè¶…è¿‡äº†è¾ƒå¤§çš„æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶è€ƒè™‘äº†è®¡ç®—æ•ˆç‡ã€è®­ç»ƒè¦æ±‚ç­‰å®é™…é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2225a6334bc2ac5f6e8181e7dec30147.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cb539b0551644893c97ee357afa0711.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1844c936a293ce2abdde1d43cd1d3352.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d27d5feb472c8cedb0280399a21f6767.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3dcd6d04a78a951212caef0fcc882f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-89a8b8c8db493d12477b0e871da7eee7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01b3dbbfa8762d1189e2cc2f9437f456.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec1e82e0f04d729394d071aeddeb065a.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Mind-the-Gap-Aligning-Vision-Foundation-Models-to-Image-Feature-Matching"><a href="#Mind-the-Gap-Aligning-Vision-Foundation-Models-to-Image-Feature-Matching" class="headerlink" title="Mind the Gap: Aligning Vision Foundation Models to Image Feature   Matching"></a>Mind the Gap: Aligning Vision Foundation Models to Image Feature   Matching</h2><p><strong>Authors:Yuhan Liu, Jingwen Fu, Yang Wu, Kangyi Wu, Pengna Li, Jiayi Wu, Sanping Zhou, Jingmin Xin</strong></p>
<p>Leveraging the vision foundation models has emerged as a mainstream paradigm that improves the performance of image feature matching. However, previous works have ignored the misalignment when introducing the foundation models into feature matching. The misalignment arises from the discrepancy between the foundation models focusing on single-image understanding and the cross-image understanding requirement of feature matching. Specifically, 1) the embeddings derived from commonly used foundation models exhibit discrepancies with the optimal embeddings required for feature matching; 2) lacking an effective mechanism to leverage the single-image understanding ability into cross-image understanding. A significant consequence of the misalignment is they struggle when addressing multi-instance feature matching problems. To address this, we introduce a simple but effective framework, called IMD (Image feature Matching with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant solutions employing contrastive-learning based foundation models that emphasize global semantics, we integrate the generative-based diffusion models to effectively capture instance-level details. 2) We leverage the prompt mechanism in generative model as a natural tunnel, propose a novel cross-image interaction prompting module to facilitate bidirectional information interaction between image pairs. To more accurately measure the misalignment, we propose a new benchmark called IMIM, which focuses on multi-instance scenarios. Our proposed IMD establishes a new state-of-the-art in commonly evaluated benchmarks, and the superior improvement 12% in IMIM indicates our method efficiently mitigates the misalignment. </p>
<blockquote>
<p>åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹æé«˜å›¾åƒç‰¹å¾åŒ¹é…çš„æ€§èƒ½å·²æˆä¸ºä¸»æµèŒƒå¼ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„å·¥ä½œåœ¨å°†åŸºç¡€æ¨¡å‹å¼•å…¥ç‰¹å¾åŒ¹é…æ—¶å¿½ç•¥äº†ä¸åŒ¹é…é—®é¢˜ã€‚è¿™ç§ä¸åŒ¹é…æºäºåŸºç¡€æ¨¡å‹å¯¹å•å›¾åƒç†è§£çš„å…³æ³¨ä¸ç‰¹å¾åŒ¹é…å¯¹è·¨å›¾åƒç†è§£çš„è¦æ±‚ä¹‹é—´çš„å·®å¼‚ã€‚å…·ä½“æ¥è¯´ï¼Œ1ï¼‰å¸¸ç”¨åŸºç¡€æ¨¡å‹äº§ç”Ÿçš„åµŒå…¥ä¸ç”¨äºç‰¹å¾åŒ¹é…çš„æœ€ä½³åµŒå…¥ä¹‹é—´å­˜åœ¨å·®å¼‚ï¼›2ï¼‰ç¼ºä¹ä¸€ç§æœ‰æ•ˆçš„æœºåˆ¶æ¥åˆ©ç”¨å•å›¾åƒç†è§£èƒ½åŠ›æ¥å®ç°è·¨å›¾åƒç†è§£ã€‚ä¸åŒ¹é…çš„ä¸€ä¸ªæ˜¾è‘—åæœæ˜¯å®ƒä»¬åœ¨è§£å†³å¤šå®ä¾‹ç‰¹å¾åŒ¹é…é—®é¢˜æ—¶æ„Ÿåˆ°å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç§°ä¸ºIMDï¼ˆå¸¦æœ‰é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å›¾åƒç‰¹å¾åŒ¹é…ï¼‰ï¼Œå®ƒåŒ…å«ä¸¤éƒ¨åˆ†ï¼š1ï¼‰ä¸é‡‡ç”¨åŸºäºå¯¹æ¯”å­¦ä¹ çš„åŸºç¡€æ¨¡å‹çš„ä¸»æµè§£å†³æ–¹æ¡ˆä¸åŒï¼Œè¿™äº›åŸºç¡€æ¨¡å‹å¼ºè°ƒå…¨å±€è¯­ä¹‰ï¼Œæˆ‘ä»¬é›†æˆäº†åŸºäºç”Ÿæˆæ‰©æ•£æ¨¡å‹æ¥æœ‰æ•ˆæ•è·å®ä¾‹çº§ç»†èŠ‚ã€‚2ï¼‰æˆ‘ä»¬åˆ©ç”¨ç”Ÿæˆæ¨¡å‹ä¸­çš„æç¤ºæœºåˆ¶ä½œä¸ºä¸€ä¸ªè‡ªç„¶é€šé“ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„è·¨å›¾åƒäº¤äº’æç¤ºæ¨¡å—ï¼Œä»¥ä¿ƒè¿›å›¾åƒå¯¹ä¹‹é—´çš„åŒå‘ä¿¡æ¯äº¤äº’ã€‚ä¸ºäº†æ›´å‡†ç¡®åœ°æµ‹é‡ä¸åŒ¹é…ç¨‹åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•IMIMï¼Œå®ƒä¸“æ³¨äºå¤šå®ä¾‹åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºçš„IMDåœ¨å¸¸ç”¨çš„åŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ï¼ŒIMIMçš„12%çš„ä¼˜è¶Šæ”¹è¿›è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°ç¼“è§£äº†ä¸åŒ¹é…é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10318v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong><br>åœ¨è¿™ä¸ªæ–‡æœ¬ä¸­ï¼Œä½œè€…ä»‹ç»äº†åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒç‰¹å¾åŒ¹é…çš„æ–°æ¡†æ¶IMDã€‚è¯¥æ¡†æ¶è§£å†³äº†ä¸»æµè§£å†³æ–¹æ¡ˆåœ¨å¤„ç†å¤šå®ä¾‹ç‰¹å¾åŒ¹é…é—®é¢˜æ—¶å­˜åœ¨çš„å±€é™æ€§ï¼Œé€šè¿‡å¼•å…¥ç”Ÿæˆå‹æ‰©æ•£æ¨¡å‹ï¼Œå¹¶ç»“åˆè·¨å›¾åƒäº¤äº’æç¤ºæ¨¡å—ï¼Œæé«˜äº†å›¾åƒç‰¹å¾åŒ¹é…çš„å‡†ç¡®æ€§ã€‚åŒæ—¶ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•IMIMï¼Œä¸“æ³¨äºå¤šå®ä¾‹åœºæ™¯ï¼Œä»¥æ›´å‡†ç¡®åœ°è¡¡é‡ä¸åŒ¹é…çš„ç¨‹åº¦ã€‚IMDæ¡†æ¶åœ¨å¸¸è§çš„åŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æ–°çš„æœ€é«˜æ°´å¹³ï¼Œå¹¶ä¸”åœ¨IMIMä¸Šçš„ä¼˜å¼‚æ”¹è¿›è¡¨æ˜è¯¥æ–¹æ³•æœ‰æ•ˆåœ°ç¼“è§£äº†ä¸åŒ¹é…çš„é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒç‰¹å¾åŒ¹é…å·²æˆä¸ºä¸»æµèŒƒå¼ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤šå®ä¾‹ç‰¹å¾åŒ¹é…æ—¶å­˜åœ¨ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>IMDæ¡†æ¶é€šè¿‡å¼•å…¥ç”Ÿæˆå‹æ‰©æ•£æ¨¡å‹è§£å†³æ­¤é—®é¢˜ï¼Œå¼ºè°ƒå®ä¾‹çº§ç»†èŠ‚æ•æ‰ã€‚</li>
<li>IMDæ¡†æ¶ç»“åˆè·¨å›¾åƒäº¤äº’æç¤ºæ¨¡å—ï¼Œä¿ƒè¿›å›¾åƒå¯¹ä¹‹é—´çš„åŒå‘ä¿¡æ¯äº¤äº’ã€‚</li>
<li>æå‡ºæ–°çš„åŸºå‡†æµ‹è¯•IMIMï¼Œä¸“æ³¨äºå¤šå®ä¾‹åœºæ™¯ä»¥è¡¡é‡ä¸åŒ¹é…ç¨‹åº¦ã€‚</li>
<li>IMDæ¡†æ¶åœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨IMIMä¸Šçš„æ”¹è¿›è¡¨æ˜å…¶æœ‰æ•ˆç¼“è§£ä¸åŒ¹é…é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b08340337df7f32f2cf9ff0b19db64b3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-87b2d554f56b2551404030a197fcdeb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f08b5db22167966a126ded28e63e838.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="When-Schrodinger-Bridge-Meets-Real-World-Image-Dehazing-with-Unpaired-Training"><a href="#When-Schrodinger-Bridge-Meets-Real-World-Image-Dehazing-with-Unpaired-Training" class="headerlink" title="When SchrÃ¶dinger Bridge Meets Real-World Image Dehazing with Unpaired   Training"></a>When SchrÃ¶dinger Bridge Meets Real-World Image Dehazing with Unpaired   Training</h2><p><strong>Authors:Yunwei Lan, Zhigao Cui, Xin Luo, Chang Liu, Nian Wang, Menglin Zhang, Yanzhao Su, Dong Liu</strong></p>
<p>Recent advancements in unpaired dehazing, particularly those using GANs, show promising performance in processing real-world hazy images. However, these methods tend to face limitations due to the generatorâ€™s limited transport mapping capability, which hinders the full exploitation of their effectiveness in unpaired training paradigms. To address these challenges, we propose DehazeSB, a novel unpaired dehazing framework based on the Schr&quot;odinger Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges the distributions between hazy and clear images. This enables optimal transport mappings from hazy to clear images in fewer steps, thereby generating high-quality results. To ensure the consistency of structural information and details in the restored images, we introduce detail-preserving regularization, which enforces pixel-level alignment between hazy inputs and dehazed outputs. Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP models in distinguishing hazy images and clear ones, by learning a haze-aware vision-language alignment. Extensive experiments on multiple real-world datasets demonstrate our methodâ€™s superiority. Code: <a target="_blank" rel="noopener" href="https://github.com/ywxjm/DehazeSB">https://github.com/ywxjm/DehazeSB</a>. </p>
<blockquote>
<p>è¿‘æœŸåœ¨æ— é…å¯¹å»é›¾æŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ–¹æ³•ï¼Œåœ¨å¤„ç†çœŸå®ä¸–ç•Œä¸­çš„é›¾éœ¾å›¾åƒæ—¶è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€é¢ä¸´ç”±äºç”Ÿæˆå™¨çš„æœ‰é™ä¼ è¾“æ˜ å°„èƒ½åŠ›è€Œå¯¼è‡´çš„å±€é™æ€§ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨æ— é…å¯¹è®­ç»ƒæ¨¡å¼ä¸­çš„å®Œå…¨æœ‰æ•ˆæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºSchrÃ¶dinger Bridgeçš„å»é›¾SBï¼ˆDehazeSBï¼‰æ–°å‹æ— é…å¯¹å»é›¾æ¡†æ¶ã€‚é€šè¿‡åˆ©ç”¨æœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰ç†è®ºï¼ŒDehazeSBç›´æ¥è¿æ¥é›¾éœ¾å›¾åƒå’Œæ¸…æ™°å›¾åƒä¹‹é—´çš„åˆ†å¸ƒã€‚è¿™èƒ½å¤Ÿå®ç°ä»é›¾éœ¾å›¾åƒåˆ°æ¸…æ™°å›¾åƒçš„æœ€ä¼˜ä¼ è¾“æ˜ å°„ï¼Œå¹¶ä¸”æ­¥éª¤æ›´å°‘ï¼Œä»è€Œç”Ÿæˆé«˜è´¨é‡çš„ç»“æœã€‚ä¸ºäº†ç¡®ä¿æ¢å¤å›¾åƒçš„ç»“æ„ä¿¡æ¯çš„ä¸€è‡´æ€§å¹¶ä¿ç•™ç»†èŠ‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»†èŠ‚ä¿ç•™æ­£åˆ™åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¼ºåˆ¶é›¾éœ¾è¾“å…¥å’Œå»é›¾è¾“å‡ºä¹‹é—´çš„åƒç´ çº§å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æç¤ºå­¦ä¹ æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡å‹æ¥åŒºåˆ†é›¾éœ¾å›¾åƒå’Œæ¸…æ™°çš„å›¾åƒï¼Œé€šè¿‡å­¦ä¹ å¯¹é›¾éœ¾æœ‰æ„ŸçŸ¥çš„è§†è¯­è¨€å¯¹é½ã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/ywxjm/DehazeSB">https://github.com/ywxjm/DehazeSB</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09524v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£åŸºäºGANçš„æ— é…å¯¹å»é›¾æŠ€æœ¯å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä»é¢ä¸´ç”Ÿæˆå™¨ä¼ è¾“æ˜ å°„èƒ½åŠ›æœ‰é™çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºä¸€ç§åŸºäºSchrÃ¶dinger Bridgeçš„å»é›¾æ¡†æ¶DehazeSBï¼Œé€šè¿‡æœ€ä¼˜ä¼ è¾“ç†è®ºç›´æ¥æ¡¥æ¥é›¾å¤©å’Œæ™´æœ—å›¾åƒåˆ†å¸ƒï¼Œå®ç°é«˜è´¨é‡å»é›¾æ•ˆæœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥ç»†èŠ‚ä¿ç•™æ­£åˆ™åŒ–å’Œæç¤ºå­¦ä¹ ï¼Œæé«˜ç»“æ„ä¿¡æ¯çš„ä¸€è‡´æ€§å’Œç»†èŠ‚ä¿ç•™èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒCLIPæ¨¡å‹è¿›è¡Œå›¾åƒæ¸…æ™°åº¦åˆ¤åˆ«ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨çœŸå®æ•°æ®é›†ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GANåœ¨å»é›¾æŠ€æœ¯ä¸­çš„åº”ç”¨å±•ç°äº†å‰æ™¯ï¼Œä½†ä»å—ç”Ÿæˆå™¨æ˜ å°„èƒ½åŠ›é™åˆ¶ã€‚</li>
<li>DehazeSBæ¡†æ¶åŸºäºSchrÃ¶dinger Bridgeå’Œæœ€ä¼˜ä¼ è¾“ç†è®ºï¼Œå®ç°é›¾å¤©å’Œæ™´æœ—å›¾åƒåˆ†å¸ƒçš„ç›´æ¥æ¡¥æ¥ã€‚</li>
<li>DehazeSBèƒ½åœ¨è¾ƒå°‘çš„æ­¥éª¤å†…å®Œæˆä»é›¾åˆ°æ™´æœ—å›¾åƒçš„ä¼ è¾“æ˜ å°„ï¼Œç”Ÿæˆé«˜è´¨é‡ç»“æœã€‚</li>
<li>å¼•å…¥ç»†èŠ‚ä¿ç•™æ­£åˆ™åŒ–ï¼Œç¡®ä¿æ¢å¤å›¾åƒçš„ç»“æ„ä¿¡æ¯å’Œç»†èŠ‚ä¸€è‡´æ€§ã€‚</li>
<li>æå‡ºä¸€ç§æ–°çš„æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡å‹è¿›è¡Œå›¾åƒæ¸…æ™°åº¦åˆ¤åˆ«ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªçœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09524">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3b450c8b3ba8321d99523473b6fe8fc1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea740b41cb65952010dfdce96d4ea7dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b6563b5a22316ef7a8d97fbe6b99067.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-409e83408c6fe3a2358b635f10bb7f36.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ViT-ProtoNet-for-Few-Shot-Image-Classification-A-Multi-Benchmark-Evaluation"><a href="#ViT-ProtoNet-for-Few-Shot-Image-Classification-A-Multi-Benchmark-Evaluation" class="headerlink" title="ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark   Evaluation"></a>ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark   Evaluation</h2><p><strong>Authors:Abdulvahap Mutlu, ÅengÃ¼l DoÄŸan, TÃ¼rker Tuncer</strong></p>
<p>The remarkable representational power of Vision Transformers (ViTs) remains underutilized in few-shot image classification. In this work, we introduce ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical Network framework. By averaging class conditional token embeddings from a handful of support examples, ViT-ProtoNet constructs robust prototypes that generalize to novel categories under 5-shot settings. We conduct an extensive empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100, CUB-200, and CIFAR-FS, including overlapped support variants to assess robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based prototypical counterparts, achieving up to a 3.2% improvement in 5-shot accuracy and demonstrating superior feature separability in latent space. Furthermore, it outperforms or is competitive with transformer-based competitors using a more lightweight backbone. Comprehensive ablations examine the impact of transformer depth, patch size, and fine-tuning strategy. To foster reproducibility, we release code and pretrained weights. Our results establish ViT-ProtoNet as a powerful, flexible approach for few-shot classification and set a new baseline for transformer-based meta-learners. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTï¼‰çš„è¡¨ç¤ºèƒ½åŠ›ååˆ†å¼ºå¤§ï¼Œä½†åœ¨å°æ ·æœ¬å›¾åƒåˆ†ç±»ä¸­çš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ViT-ProtoNetï¼Œå®ƒå°†ViT-Smalléª¨å¹²ç½‘é›†æˆåˆ°åŸå‹ç½‘ç»œæ¡†æ¶ä¸­ã€‚é€šè¿‡å¹³å‡å°‘é‡æ”¯æŒä¾‹å­çš„ç±»åˆ«æ¡ä»¶ä»¤ç‰ŒåµŒå…¥ï¼ŒViT-ProtoNetæ„å»ºäº†åœ¨5ç§åœºæ™¯è®¾ç½®ä¸‹èƒ½å¤Ÿæ¨å¹¿åˆ°æ–°å‹ç±»åˆ«çš„ç¨³å¥åŸå‹ã€‚æˆ‘ä»¬åœ¨å››ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®è¯è¯„ä¼°ï¼šMini-ImageNetã€FC100ã€CUB-200å’ŒCIFAR-FSï¼ŒåŒ…æ‹¬é‡å çš„æ”¯æŒå˜ä½“ä»¥è¯„ä¼°ç¨³å¥æ€§ã€‚åœ¨æ‰€æœ‰æ‹†åˆ†ä¸­ï¼ŒViT-ProtoNetå§‹ç»ˆä¼˜äºåŸºäºCNNçš„åŸå‹å¯¹åº”ç‰©ï¼Œåœ¨5æ¬¡å°„å‡»çš„å‡†ç¡®åº¦ä¸Šæé«˜äº†é«˜è¾¾3.2%ï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´ä¸­æ˜¾ç¤ºå‡ºä¼˜è¶Šçš„ç‰¹å¾å¯åˆ†ç¦»æ€§ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ›´è½»é‡çº§éª¨å¹²ç½‘çš„åŸºäºtransformerçš„ç«äº‰å¯¹æ‰‹ç›¸æ¯”ï¼Œå®ƒå…·æœ‰å‡ºè‰²çš„è¡¨ç°æˆ–ä¸ä¹‹ç«äº‰ã€‚å…¨é¢çš„æ¶ˆèç ”ç©¶æ£€éªŒäº†transformeræ·±åº¦ã€è¡¥ä¸å¤§å°å’Œå¾®è°ƒç­–ç•¥çš„å½±å“ã€‚ä¸ºäº†ä¿ƒè¿›å¯é‡å¤æ€§ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä»£ç å’Œé¢„è®­ç»ƒæƒé‡ã€‚æˆ‘ä»¬çš„ç»“æœç¡®ç«‹äº†ViT-ProtoNetä½œä¸ºå°æ ·æœ¬åˆ†ç±»çš„å¼ºå¤§ã€çµæ´»çš„æ–¹æ³•ï¼Œå¹¶ä¸ºåŸºäºtransformerçš„å…ƒå­¦ä¹ è€…è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09299v1">PDF</a> All codes are available at   <a target="_blank" rel="noopener" href="https://github.com/abdulvahapmutlu/vit-protonet">https://github.com/abdulvahapmutlu/vit-protonet</a></p>
<p><strong>Summary</strong></p>
<p>ViT-ProtoNetå°†Vision Transformerï¼ˆViTï¼‰çš„å¼ºå¤§è¡¨ç¤ºèƒ½åŠ›ä¸Prototypical Networkæ¡†æ¶ç›¸ç»“åˆï¼Œç”¨äºå°‘æ ·æœ¬å›¾åƒåˆ†ç±»ã€‚é€šè¿‡å¹³å‡å°‘é‡æ”¯æŒæ ·æœ¬çš„ç±»åˆ«æ¡ä»¶ä»¤ç‰ŒåµŒå…¥ï¼Œæ„å»ºå¯åœ¨æ–°å‹ç±»åˆ«ä¸­é€šç”¨çš„ç¨³å¥åŸå‹ã€‚åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒViT-ProtoNetåœ¨5ç§åœºæ™¯è®¾ç½®ä¸­å§‹ç»ˆä¼˜äºåŸºäºCNNçš„åŸå‹ç½‘ç»œï¼Œåœ¨å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†é«˜è¾¾3.2%çš„æå‡ï¼Œå¹¶æ˜¾ç¤ºå‡ºå¼ºå¤§çš„ç‰¹å¾åˆ†ç¦»èƒ½åŠ›ã€‚è¯¥ç ”ç©¶ä¸ºåŸºäºVision Transformerçš„å°‘æ ·æœ¬åˆ†ç±»å»ºç«‹äº†å¼ºå¤§çš„çµæ´»æ–¹æ³•ï¼Œå¹¶ä¸ºåŸºäºtransformerçš„å…ƒå­¦ä¹ è€…è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ViT-ProtoNetç»“åˆäº†Vision Transformerï¼ˆViTï¼‰å’ŒPrototypical Networkæ¡†æ¶ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¹³å‡å°‘é‡æ”¯æŒæ ·æœ¬çš„ç±»åˆ«æ¡ä»¶ä»¤ç‰ŒåµŒå…¥ï¼Œæ„å»ºäº†ç¨³å¥çš„åŸå‹ã€‚</li>
<li>åœ¨å¤šä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒViT-ProtoNetåœ¨å°‘æ ·æœ¬å›¾åƒåˆ†ç±»æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</li>
<li>ä¸åŸºäºCNNçš„åŸå‹ç½‘ç»œç›¸æ¯”ï¼ŒViT-ProtoNetåœ¨å‡†ç¡®æ€§æ–¹é¢æœ‰æ‰€æé«˜ï¼Œè¾¾åˆ°3.2%ã€‚</li>
<li>ViT-ProtoNetåœ¨æ½œåœ¨ç©ºé—´ä¸­çš„ç‰¹å¾åˆ†ç¦»èƒ½åŠ›å¼ºå¤§ã€‚</li>
<li>ç ”ç©¶æä¾›äº†å…³äºtransformeræ·±åº¦ã€è¡¥ä¸å¤§å°å’Œå¾®è°ƒç­–ç•¥çš„ç»¼åˆåˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4d0610ed74405bb286dd1346ceae6d14.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Video-Inference-for-Human-Mesh-Recovery-with-Vision-Transformer"><a href="#Video-Inference-for-Human-Mesh-Recovery-with-Vision-Transformer" class="headerlink" title="Video Inference for Human Mesh Recovery with Vision Transformer"></a>Video Inference for Human Mesh Recovery with Vision Transformer</h2><p><strong>Authors:Hanbyel Cho, Jaesung Ahn, Yooshin Cho, Junmo Kim</strong></p>
<p>Human Mesh Recovery (HMR) from an image is a challenging problem because of the inherent ambiguity of the task. Existing HMR methods utilized either temporal information or kinematic relationships to achieve higher accuracy, but there is no method using both. Hence, we propose â€œVideo Inference for Human Mesh Recovery with Vision Transformer (HMR-ViT)â€ that can take into account both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic Feature Image is constructed using feature vectors obtained from video frames by an image encoder. When generating the feature image, we use a Channel Rearranging Matrix (CRM) so that similar kinematic features could be located spatially close together. The feature image is then further encoded using Vision Transformer, and the SMPL pose and shape parameters are finally inferred using a regression network. Extensive evaluation on the 3DPW and Human3.6M datasets indicates that our method achieves a competitive performance in HMR. </p>
<blockquote>
<p>ä»å›¾åƒä¸­è¿›è¡Œäººä½“ç½‘æ ¼æ¢å¤ï¼ˆHMRï¼‰æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå› ä¸ºè¯¥ä»»åŠ¡æœ¬èº«å°±å­˜åœ¨å›ºæœ‰çš„æ¨¡ç³Šæ€§ã€‚ç°æœ‰çš„HMRæ–¹æ³•åˆ©ç”¨æ—¶é—´ä¿¡æ¯æˆ–è¿åŠ¨å­¦å…³ç³»æ¥æé«˜ç²¾åº¦ï¼Œä½†æ²¡æœ‰æ–¹æ³•å¯ä»¥åŒæ—¶ä½¿ç”¨ä¸¤è€…ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†â€œåŸºäºè§†é¢‘æ¨ç†å’Œè§†è§‰è½¬æ¢å™¨çš„äººä½“ç½‘æ ¼æ¢å¤ï¼ˆHMR-ViTï¼‰â€ï¼Œè¯¥æ–¹æ³•å¯ä»¥è€ƒè™‘æ—¶é—´å’Œè¿åŠ¨å­¦ä¿¡æ¯ã€‚åœ¨HMR-ViTä¸­ï¼Œä½¿ç”¨æ¥è‡ªè§†é¢‘å¸§çš„ç‰¹å¾å‘é‡æ„å»ºä¸€ä¸ªæ—¶é—´è¿åŠ¨å­¦ç‰¹å¾å›¾åƒï¼Œè¯¥å›¾åƒç”±å›¾åƒç¼–ç å™¨ç”Ÿæˆã€‚åœ¨ç”Ÿæˆç‰¹å¾å›¾åƒæ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨é€šé“é‡æ–°æ’åˆ—çŸ©é˜µï¼ˆCRMï¼‰ï¼Œä»¥ä¾¿å°†ç›¸ä¼¼çš„è¿åŠ¨å­¦ç‰¹å¾åœ¨ç©ºé—´ä¸Šç´§å¯†åœ°å®šä½åœ¨ä¸€èµ·ã€‚ç„¶åï¼Œä½¿ç”¨è§†è§‰è½¬æ¢å™¨è¿›ä¸€æ­¥å¯¹ç‰¹å¾å›¾åƒè¿›è¡Œç¼–ç ï¼Œå¹¶æœ€ç»ˆé€šè¿‡å›å½’ç½‘ç»œæ¨æ–­SMPLå§¿åŠ¿å’Œå½¢çŠ¶å‚æ•°ã€‚åœ¨3DPWå’ŒHuman3.6Mæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨HMRä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08981v1">PDF</a> Accepted to IEEE FG 2023</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨è§†é¢‘ä¿¡æ¯æ¥è¿›è¡Œäººä½“ç½‘æ ¼æ¢å¤ï¼ˆHMRï¼‰çš„æ–°æ–¹æ³•ï¼Œåä¸ºâ€œåŸºäºè§†é¢‘ä¿¡æ¯çš„HMR-ViTäººä½“ç½‘æ ¼æ¢å¤â€ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ—¶é—´ä¿¡æ¯å’Œè¿åŠ¨å­¦å…³ç³»ï¼Œæ„å»ºäº†ä¸€ä¸ªæ—¶é—´è¿åŠ¨ç‰¹å¾å›¾åƒï¼Œå¹¶ä½¿ç”¨å›¾åƒç¼–ç å™¨å’Œè§†è§‰è½¬æ¢å™¨è¿›è¡Œç‰¹å¾æå–å’Œç¼–ç ã€‚é€šè¿‡é‡æ–°å®‰æ’é€šé“çŸ©é˜µï¼ˆCRMï¼‰ï¼Œä½¿ç›¸ä¼¼çš„è¿åŠ¨å­¦ç‰¹å¾åœ¨ç©ºé—´ä¸Šæ›´æ¥è¿‘ã€‚æœ€åï¼Œä½¿ç”¨å›å½’ç½‘ç»œæ¨æ–­SMPLå§¿åŠ¿å’Œå½¢çŠ¶å‚æ•°ã€‚åœ¨3DPWå’ŒHuman3.6Mæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨HMRä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HMRæ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ï¼Œå› ä¸ºä»»åŠ¡æœ¬èº«å°±å­˜åœ¨å›ºæœ‰çš„æ¨¡ç³Šæ€§ã€‚</li>
<li>ç›®å‰å¤§å¤šæ•°HMRæ–¹æ³•åªåˆ©ç”¨æ—¶é—´ä¿¡æ¯æˆ–è¿åŠ¨å­¦å…³ç³»æ¥æé«˜ç²¾åº¦ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„æ–¹æ³•HMR-ViTï¼Œèƒ½åŒæ—¶è€ƒè™‘æ—¶é—´å’Œè¿åŠ¨å­¦ä¿¡æ¯ã€‚</li>
<li>HMR-ViTæ„å»ºäº†ä¸€ä¸ªæ—¶é—´è¿åŠ¨ç‰¹å¾å›¾åƒï¼Œé€šè¿‡å›¾åƒç¼–ç å™¨å’Œè§†è§‰è½¬æ¢å™¨è¿›è¡Œç‰¹å¾æå–å’Œç¼–ç ã€‚</li>
<li>ä½¿ç”¨CRMæ¥é‡æ–°å®‰æ’é€šé“ï¼Œä½¿ç›¸ä¼¼çš„è¿åŠ¨å­¦ç‰¹å¾åœ¨ç©ºé—´ä¸Šæ›´æ¥è¿‘ã€‚</li>
<li>é€šè¿‡å›å½’ç½‘ç»œæ¨æ–­SMPLå§¿åŠ¿å’Œå½¢çŠ¶å‚æ•°ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒHMR-ViTåœ¨HMRä»»åŠ¡ä¸­å–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08981">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-43d037c500fcef7e128f86f6b3a711ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-52be6a03eb161dbb32929abcc9d9088e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8ae441ea132046b43fffe0741efb2ef5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc2ad5b70d9565850ff1d32c719b2efa.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PRISM-Reducing-Spurious-Implicit-Biases-in-Vision-Language-Models-with-LLM-Guided-Embedding-Projection"><a href="#PRISM-Reducing-Spurious-Implicit-Biases-in-Vision-Language-Models-with-LLM-Guided-Embedding-Projection" class="headerlink" title="PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with   LLM-Guided Embedding Projection"></a>PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with   LLM-Guided Embedding Projection</h2><p><strong>Authors:Mahdiyar Molahasani, Azadeh Motamedi, Michael Greenspan, Il-Min Kim, Ali Etemad</strong></p>
<p>We introduce Projection-based Reduction of Implicit Spurious bias in vision-language Models (PRISM), a new data-free and task-agnostic solution for bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in their training data, leading to skewed predictions. PRISM is designed to debias VLMs without relying on predefined bias categories or additional external data. It operates in two stages: first, an LLM is prompted with simple class prompts to generate scene descriptions that contain spurious correlations. Next, PRISM uses our novel contrastive-style debiasing loss to learn a projection that maps the embeddings onto a latent space that minimizes spurious correlations while preserving the alignment between image and text embeddings.Extensive experiments demonstrate that PRISM outperforms current debiasing methods on the commonly used Waterbirds and CelebA datasets We make our code public at: <a target="_blank" rel="noopener" href="https://github.com/MahdiyarMM/PRISM">https://github.com/MahdiyarMM/PRISM</a>. </p>
<blockquote>
<p>æˆ‘ä»¬å¼•å…¥äº†åŸºäºæŠ•å½±çš„éšå¼åè§å‡å°‘ï¼ˆPRISMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ— æ•°æ®å’Œä»»åŠ¡æ— å…³çš„è§£å†³åè§ç¼“è§£é—®é¢˜çš„æ–¹æ³•ï¼Œé€‚ç”¨äºå¦‚CLIPè¿™æ ·çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ã€‚VLMsé€šå¸¸ä¼šç»§æ‰¿å¹¶æ”¾å¤§å…¶è®­ç»ƒæ•°æ®ä¸­çš„åè§ï¼Œå¯¼è‡´é¢„æµ‹ç»“æœå‡ºç°åå·®ã€‚PRISMæ—¨åœ¨åœ¨ä¸ä¾èµ–é¢„å®šä¹‰çš„åè§ç±»åˆ«æˆ–é¢å¤–çš„å¤–éƒ¨æ•°æ®çš„æƒ…å†µä¸‹å¯¹VLMsè¿›è¡Œå»åå¤„ç†ã€‚å®ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆï¼Œä½¿ç”¨ç®€å•çš„ç±»åˆ«æç¤ºæç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”ŸæˆåŒ…å«é”™è¯¯å…³è”çš„åœºæ™¯æè¿°ã€‚æ¥ä¸‹æ¥ï¼ŒPRISMä½¿ç”¨æˆ‘ä»¬æ–°é¢–çš„å¯¹æ¯”å¼å»åæŸå¤±æ¥å­¦ä¹ ä¸€ç§æŠ•å½±æ–¹æ³•ï¼Œå°†åµŒå…¥æ˜ å°„åˆ°ä¸€ä¸ªæ½œåœ¨ç©ºé—´ï¼Œè¯¥ç©ºé—´åœ¨æœ€å°åŒ–é”™è¯¯å…³è”çš„åŒæ—¶ï¼Œä¿ç•™å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒPRISMåœ¨å¸¸ç”¨çš„Waterbirdså’ŒCelebAæ•°æ®é›†ä¸Šçš„å»åæ•ˆæœä¼˜äºå½“å‰çš„å»åæ–¹æ³•ã€‚æˆ‘ä»¬åœ¨ä»¥ä¸‹ç½‘ç«™å…¬å¼€äº†æˆ‘ä»¬çš„ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/MahdiyarMM/PRISM">https://github.com/MahdiyarMM/PRISM</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08979v1">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæŠ•å½±çš„éšå¼åè§å‡å°‘æ–¹æ³•ï¼ˆPRISMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åè§ç¼“è§£çš„æ–°æ•°æ®å…è´¹å’Œä»»åŠ¡æ— å…³çš„è§£å†³æ–¹æ¡ˆã€‚PRISMæ—¨åœ¨ä¸ä¾èµ–é¢„è®¾åè§ç±»åˆ«æˆ–é¢å¤–å¤–éƒ¨æ•°æ®å¯¹VLMsè¿›è¡Œå»åå¤„ç†ã€‚å®ƒé€šè¿‡ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œæ“ä½œï¼šé¦–å…ˆï¼Œç”¨ç®€å•çš„ç±»åˆ«æç¤ºæç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ç”ŸæˆåŒ…å«é”™è¯¯ç›¸å…³æ€§çš„åœºæ™¯æè¿°ï¼›æ¥ç€ï¼ŒPRISMä½¿ç”¨æ–°å‹å¯¹æ¯”é£æ ¼å»åæŸå¤±æ¥å­¦ä¹ å°†åµŒå…¥æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´ä¸­çš„æŠ•å½±ï¼Œè¿™ä¸ªæ½œåœ¨ç©ºé—´æœ€å°åŒ–é”™è¯¯ç›¸å…³æ€§ï¼ŒåŒæ—¶ä¿ç•™å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„å¯¹é½ã€‚å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPRISMåœ¨å¸¸ç”¨çš„å°é¸Ÿä¸åäººè„¸éƒ¨è¯†åˆ«æ•°æ®é›†ï¼ˆWaterbirdså’ŒCelebAï¼‰ä¸Šçš„å»åæ•ˆæœä¼˜äºå½“å‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PRISMæ˜¯ä¸€ç§é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å¦‚CLIPçš„åè§ç¼“è§£æ–°æ–¹æ³•ã€‚</li>
<li>PRISMæ˜¯æ•°æ®å…è´¹å’Œä»»åŠ¡æ— å…³çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³VLMsä¸­ç»§æ‰¿å¹¶æ”¾å¤§çš„åè§é—®é¢˜ã€‚</li>
<li>PRISMæ“ä½œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆåœºæ™¯æè¿°ï¼Œå¹¶ä½¿ç”¨å¯¹æ¯”é£æ ¼å»åæŸå¤±è¿›è¡ŒæŠ•å½±æ˜ å°„ã€‚</li>
<li>PRISMé€šè¿‡æœ€å°åŒ–é”™è¯¯ç›¸å…³æ€§å¹¶ä¿ç•™å›¾åƒå’Œæ–‡æœ¬åµŒå…¥ä¹‹é—´çš„å¯¹é½æ¥å®ç°å»åã€‚</li>
<li>PRISMåœ¨å¹¿æ³›å®éªŒä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨Waterbirdså’ŒCelebAæ•°æ®é›†ä¸Šã€‚</li>
<li>PRISMæ–¹æ³•å…·æœ‰å…¬å¼€å¯ç”¨çš„ä»£ç ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ä¾èµ–é¢„è®¾åè§ç±»åˆ«æˆ–é¢å¤–å¤–éƒ¨æ•°æ®ï¼Œå…·æœ‰é€šç”¨æ€§å’Œçµæ´»æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15964726ef48fe4b27c024f3ffc5e388.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a80af39f830b087d992dbb07df155700.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4fe5bbf97f22324c14ca61979a4c661.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="High-Fidelity-Differential-information-Driven-Binary-Vision-Transformer"><a href="#High-Fidelity-Differential-information-Driven-Binary-Vision-Transformer" class="headerlink" title="High-Fidelity Differential-information Driven Binary Vision Transformer"></a>High-Fidelity Differential-information Driven Binary Vision Transformer</h2><p><strong>Authors:Tian Gao, Zhiyuan Zhang, Kaijie Yin, Xu-Cheng Zhong, Hui Kong</strong></p>
<p>The binarization of vision transformers (ViTs) offers a promising approach to addressing the trade-off between high computational&#x2F;storage demands and the constraints of edge-device deployment. However, existing binary ViT methods often suffer from severe performance degradation or rely heavily on full-precision modules. To address these issues, we propose DIDB-ViT, a novel binary ViT that is highly informative while maintaining the original ViT architecture and computational efficiency. Specifically, we design an informative attention module incorporating differential information to mitigate information loss caused by binarization and enhance high-frequency retention. To preserve the fidelity of the similarity calculations between binary Q and K tensors, we apply frequency decomposition using the discrete Haar wavelet and integrate similarities across different frequencies. Additionally, we introduce an improved RPReLU activation function to restructure the activation distribution, expanding the modelâ€™s representational capacity. Experimental results demonstrate that our DIDB-ViT significantly outperforms state-of-the-art network quantization methods in multiple ViT architectures, achieving superior image classification and segmentation performance. </p>
<blockquote>
<p>è§†è§‰å˜å‹å™¨çš„äºŒå€¼åŒ–ï¼ˆViTsï¼‰ä¸ºè§£å†³é«˜è®¡ç®—&#x2F;å­˜å‚¨éœ€æ±‚ä¸è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²é™åˆ¶ä¹‹é—´çš„æƒè¡¡æä¾›äº†æœ‰å‰æ™¯çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„äºŒè¿›åˆ¶ViTæ–¹æ³•å¸¸å¸¸é¢ä¸´æ€§èƒ½ä¸¥é‡ä¸‹é™çš„é—®é¢˜ï¼Œæˆ–è€…ä¸¥é‡ä¾èµ–äºå…¨ç²¾åº¦æ¨¡å—ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DIDB-ViTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„äºŒå€¼ViTï¼Œåœ¨ä¿æŒåŸå§‹ViTæ¶æ„å’Œè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå…·æœ‰å¾ˆé«˜çš„ä¿¡æ¯é‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŒ…å«å·®å¼‚ä¿¡æ¯çš„æœ‰æ•ˆæ³¨æ„åŠ›æ¨¡å—ï¼Œä»¥å‡è½»äºŒå€¼åŒ–å¼•èµ·çš„ä¿¡æ¯æŸå¤±å¹¶å¢å¼ºé«˜é¢‘ä¿ç•™ã€‚ä¸ºäº†ä¿ç•™äºŒè¿›åˆ¶Qå’ŒKå¼ é‡ä¹‹é—´ç›¸ä¼¼æ€§è®¡ç®—çš„ä¿çœŸåº¦ï¼Œæˆ‘ä»¬é‡‡ç”¨ç¦»æ•£å“ˆå°”å°æ³¢è¿›è¡Œé¢‘ç‡åˆ†è§£å¹¶æ•´åˆä¸åŒé¢‘ç‡çš„ç›¸ä¼¼æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ”¹è¿›çš„RPReLUæ¿€æ´»å‡½æ•°æ¥é‡æ„æ¿€æ´»åˆ†å¸ƒï¼Œæ‰©å¤§æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DIDB-ViTåœ¨å¤šç§ViTæ¶æ„ä¸­æ˜¾è‘—ä¼˜äºæœ€æ–°çš„ç½‘ç»œé‡åŒ–æ–¹æ³•ï¼Œå®ç°äº†å‡ºè‰²çš„å›¾åƒåˆ†ç±»å’Œåˆ†å‰²æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02222v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°å‹çš„äºŒå€¼åŒ–è§†è§‰è½¬æ¢å™¨ï¼ˆDIDB-ViTï¼‰ï¼Œå®ƒåœ¨ä¿æŒåŸå§‹ViTæ¶æ„å’Œè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå¼•å…¥äº†å…·æœ‰å·®åˆ†ä¿¡æ¯çš„é«˜åº¦ä¿¡æ¯é‡çš„æ³¨æ„åŠ›æ¨¡å—æ¥ç¼“è§£äºŒå€¼åŒ–å¼•èµ·çš„ä¿¡æ¯æŸå¤±å¹¶å¢å¼ºé«˜é¢‘ä¿ç•™ã€‚åŒæ—¶é‡‡ç”¨ç¦»æ•£å°æ³¢å˜æ¢è¿›è¡Œé¢‘ç‡åˆ†è§£ï¼Œæ•´åˆä¸åŒé¢‘ç‡çš„ç›¸ä¼¼æ€§è®¡ç®—ï¼Œæ”¹è¿›RPReLUæ¿€æ´»å‡½æ•°ä»¥é‡å¡‘æ¿€æ´»åˆ†å¸ƒï¼Œæ‰©å¤§æ¨¡å‹çš„è¡¨å¾èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDIDB-ViTåœ¨å¤šä¸ªViTæ¶æ„ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ç½‘ç»œé‡åŒ–æ–¹æ³•ï¼Œå®ç°äº†å‡ºè‰²çš„å›¾åƒåˆ†ç±»å’Œåˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DIDB-ViTè§£å†³äº†ç°æœ‰äºŒè¿›åˆ¶ViTæ–¹æ³•é¢ä¸´çš„æ€§èƒ½ä¸‹é™æˆ–ä¾èµ–å…¨ç²¾åº¦æ¨¡å—çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ³¨æ„åŠ›æ¨¡å—ï¼Œè¯¥æ¨¡å—ç»“åˆäº†å·®åˆ†ä¿¡æ¯ä»¥å‡å°‘äºŒå€¼åŒ–å¸¦æ¥çš„ä¿¡æ¯æŸå¤±ã€‚</li>
<li>ä½¿ç”¨ç¦»æ•£Haarå°æ³¢å˜æ¢è¿›è¡Œé¢‘ç‡åˆ†è§£ï¼Œç”¨ä»¥ä¼˜åŒ–ç›¸ä¼¼åº¦è®¡ç®—ã€‚</li>
<li>åˆ›æ–°åœ°ç»“åˆäº†ä¸åŒé¢‘ç‡ä¸‹çš„ç›¸ä¼¼æ€§ï¼Œå¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>æå‡ºæ”¹è¿›çš„RPReLUæ¿€æ´»å‡½æ•°ä»¥é‡å¡‘æ¿€æ´»åˆ†å¸ƒï¼Œå¢å¼ºæ¨¡å‹çš„è¡¨å¾èƒ½åŠ›ã€‚</li>
<li>DIDB-ViTåœ¨å¤šç§ViTæ¶æ„ä¸Šå®ç°äº†ä¼˜äºç°æœ‰ç½‘ç»œé‡åŒ–æ–¹æ³•çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-810868a29d80c1184cc77dc5e31b168e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12c9afcbe17ccc84b42a782efea3f137.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4e6adfd29af2174bf75d9aa16689552.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9b5e2d185f24c0e6106c2370b60ad23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-01d5217766a4e49a0d56805ae043c763.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="LLM-enhanced-Action-aware-Multi-modal-Prompt-Tuning-for-Image-Text-Matching"><a href="#LLM-enhanced-Action-aware-Multi-modal-Prompt-Tuning-for-Image-Text-Matching" class="headerlink" title="LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text   Matching"></a>LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text   Matching</h2><p><strong>Authors:Mengxiao Tian, Xinxiao Wu, Shuo Yang</strong></p>
<p>Driven by large-scale contrastive vision-language pre-trained models such as CLIP, recent advancements in the image-text matching task have achieved remarkable success in representation learning. Due to image-level visual-language alignment, CLIP falls short in understanding fine-grained details such as object attributes and spatial relationships between objects. Recent efforts have attempted to compel CLIP to acquire structured visual representations by introducing prompt learning to achieve object-level alignment. While achieving promising results, they still lack the capability to perceive actions, which are crucial for describing the states or relationships between objects. Therefore, we propose to endow CLIP with fine-grained action-level understanding by introducing an LLM-enhanced action-aware multi-modal prompt-tuning method, incorporating the action-related external knowledge generated by large language models (LLMs). Specifically, we design an action triplet prompt and an action state prompt to exploit compositional semantic knowledge and state-related causal knowledge implicitly stored in LLMs. Subsequently, we propose an adaptive interaction module to aggregate attentive visual features conditioned on action-aware prompted knowledge for establishing discriminative and action-aware visual representations, which further improves the performance. Comprehensive experimental results on two benchmark datasets demonstrate the effectiveness of our method. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡å¯¹æ¯”è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„é©±åŠ¨ä¸‹ï¼Œå›¾åƒæ–‡æœ¬åŒ¹é…ä»»åŠ¡çš„æœ€æ–°è¿›å±•åœ¨è¡¨ç¤ºå­¦ä¹ æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œç”±äºå›¾åƒçº§åˆ«çš„è§†è§‰è¯­è¨€å¯¹é½ï¼ŒCLIPåœ¨ç†è§£ç²¾ç»†çš„ç»†èŠ‚æ–¹é¢è¡¨ç°ä¸è¶³ï¼Œå¦‚å¯¹è±¡å±æ€§å’Œå¯¹è±¡ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€‚æœ€è¿‘çš„åŠªåŠ›å°è¯•é€šè¿‡å¼•å…¥æç¤ºå­¦ä¹ æ¥è¿«ä½¿CLIPè·å¾—ç»“æ„åŒ–è§†è§‰è¡¨ç¤ºï¼Œä»¥å®ç°å¯¹è±¡çº§åˆ«çš„å¯¹é½ã€‚è™½ç„¶å–å¾—äº†ä¸€å®šçš„æˆæœï¼Œä½†å®ƒä»¬ä»ç„¶ç¼ºä¹æ„ŸçŸ¥åŠ¨ä½œçš„èƒ½åŠ›ï¼Œè¿™å¯¹äºæè¿°å¯¹è±¡çš„çŠ¶æ€æˆ–å…³ç³»è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡å¼•å…¥LLMå¢å¼ºçš„åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡å¼æç¤ºè°ƒæ•´æ–¹æ³•ï¼Œä¸ºCLIPèµ‹äºˆç²¾ç»†çš„åŠ¨ä½œçº§åˆ«ç†è§£ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ç”±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„åŠ¨ä½œç›¸å…³å¤–éƒ¨çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†åŠ¨ä½œä¸‰å…ƒç»„æç¤ºå’ŒåŠ¨ä½œçŠ¶æ€æç¤ºæ¥åˆ©ç”¨LLMä¸­éšå«çš„ç»„æˆè¯­ä¹‰çŸ¥è¯†å’ŒçŠ¶æ€ç›¸å…³å› æœçŸ¥è¯†ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè‡ªé€‚åº”äº¤äº’æ¨¡å—ï¼Œè¯¥æ¨¡å—æ ¹æ®åŠ¨ä½œæ„ŸçŸ¥æç¤ºçŸ¥è¯†èšåˆæ³¨æ„åŠ›è§†è§‰ç‰¹å¾ï¼Œä»¥å»ºç«‹å…·æœ‰é‰´åˆ«åŠ›å’ŒåŠ¨ä½œæ„ŸçŸ¥çš„è§†è§‰è¡¨ç¤ºï¼Œè¿™è¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚åœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒç»“æœè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23502v2">PDF</a> accepted by ICCV 2025</p>
<p><strong>Summary</strong><br>åŸºäºCLIPçš„å¤§å‹å¯¹æ¯”å¼è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åœ¨å›¾åƒæ–‡æœ¬åŒ¹é…ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨ç†è§£å¯¹è±¡å±æ€§ã€å¯¹è±¡é—´ç©ºé—´å…³ç³»ç­‰ç»†èŠ‚æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€ç¼ºé™·ï¼Œè¿‘æœŸç ”ç©¶å°è¯•å¼•å…¥æç¤ºå­¦ä¹ æ¥å¼•å¯¼CLIPè·å–ç»“æ„åŒ–è§†è§‰è¡¨å¾ï¼Œå®ç°å¯¹è±¡çº§åˆ«çš„å¯¹é½ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»æ— æ³•æ„ŸçŸ¥åŠ¨ä½œï¼Œå¯¹æè¿°å¯¹è±¡çŠ¶æ€æˆ–å…³ç³»è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºçš„åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡å¼æç¤ºè°ƒæ•´æ–¹æ³•ï¼Œå¼•å…¥åŠ¨ä½œç›¸å…³çš„å¤–éƒ¨çŸ¥è¯†ã€‚é€šè¿‡è®¾è®¡åŠ¨ä½œä¸‰å…ƒç»„æç¤ºå’ŒåŠ¨ä½œçŠ¶æ€æç¤ºæ¥åˆ©ç”¨LLMä¸­éšå«çš„ç»„æˆè¯­ä¹‰çŸ¥è¯†å’ŒçŠ¶æ€ç›¸å…³å› æœçŸ¥è¯†ï¼Œå¹¶æå‡ºè‡ªé€‚åº”äº¤äº’æ¨¡å—ï¼Œæ ¹æ®åŠ¨ä½œæ„ŸçŸ¥æç¤ºçŸ¥è¯†èšåˆæ³¨æ„åŠ›è§†è§‰ç‰¹å¾ï¼Œå»ºç«‹æœ‰åˆ¤åˆ«åŠ›å’ŒåŠ¨ä½œæ„ŸçŸ¥çš„è§†è§‰è¡¨å¾ï¼Œè¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¯¹æ¯”å¼è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨å›¾åƒæ–‡æœ¬åŒ¹é…ä»»åŠ¡ä¸­å–å¾—æ˜¾è‘—è¿›å±•ã€‚</li>
<li>CLIPåœ¨ç†è§£å›¾åƒç»†èŠ‚ï¼ˆå¦‚å¯¹è±¡å±æ€§ã€ç©ºé—´å…³ç³»ï¼‰æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>ä¸ºæ”¹è¿›CLIPï¼Œå¼•å…¥æç¤ºå­¦ä¹ ä»¥å®ç°å¯¹è±¡çº§åˆ«çš„å¯¹é½æ˜¯ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹åŠ¨ä½œæ„ŸçŸ¥èƒ½åŠ›ï¼Œè¿™æ˜¯æè¿°å¯¹è±¡çŠ¶æ€æˆ–å…³ç³»çš„å…³é”®ã€‚</li>
<li>æå‡ºç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºçš„åŠ¨ä½œæ„ŸçŸ¥å¤šæ¨¡å¼æç¤ºè°ƒæ•´æ–¹æ³•ã€‚</li>
<li>é€šè¿‡è®¾è®¡åŠ¨ä½œä¸‰å…ƒç»„æç¤ºå’ŒåŠ¨ä½œçŠ¶æ€æç¤ºï¼Œåˆ©ç”¨LLMä¸­çš„ç»„æˆè¯­ä¹‰å’ŒçŠ¶æ€ç›¸å…³å› æœçŸ¥è¯†ã€‚</li>
<li>å¼•å…¥è‡ªé€‚åº”äº¤äº’æ¨¡å—ï¼Œæ ¹æ®åŠ¨ä½œæ„ŸçŸ¥æç¤ºçŸ¥è¯†èšåˆæ³¨æ„åŠ›è§†è§‰ç‰¹å¾ï¼Œæé«˜æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23502">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-75f01832f6f137d119ee70dd1a3e50d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bb09dd63976b61d7f29323e6e6ef995.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78b93b1036fbf876c4bd54f47d444276.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56b24c2c7684fee54457dc2fcad7642a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="BreastDCEDL-A-Comprehensive-Breast-Cancer-DCE-MRI-Dataset-and-Transformer-Implementation-for-Treatment-Response-Prediction"><a href="#BreastDCEDL-A-Comprehensive-Breast-Cancer-DCE-MRI-Dataset-and-Transformer-Implementation-for-Treatment-Response-Prediction" class="headerlink" title="BreastDCEDL: A Comprehensive Breast Cancer DCE-MRI Dataset and   Transformer Implementation for Treatment Response Prediction"></a>BreastDCEDL: A Comprehensive Breast Cancer DCE-MRI Dataset and   Transformer Implementation for Treatment Response Prediction</h2><p><strong>Authors:Naomi Fridman, Bubby Solway, Tomer Fridman, Itamar Barnea, Anat Goldstein</strong></p>
<p>Breast cancer remains a leading cause of cancer-related mortality worldwide, making early detection and accurate treatment response monitoring critical priorities. We present BreastDCEDL, a curated, deep learning-ready dataset comprising pre-treatment 3D Dynamic Contrast-Enhanced MRI (DCE-MRI) scans from 2,070 breast cancer patients drawn from the I-SPY1, I-SPY2, and Duke cohorts, all sourced from The Cancer Imaging Archive. The raw DICOM imaging data were rigorously converted into standardized 3D NIfTI volumes with preserved signal integrity, accompanied by unified tumor annotations and harmonized clinical metadata including pathologic complete response (pCR), hormone receptor (HR), and HER2 status. Although DCE-MRI provides essential diagnostic information and deep learning offers tremendous potential for analyzing such complex data, progress has been limited by lack of accessible, public, multicenter datasets. BreastDCEDL addresses this gap by enabling development of advanced models, including state-of-the-art transformer architectures that require substantial training data. To demonstrate its capacity for robust modeling, we developed the first transformer-based model for breast DCE-MRI, leveraging Vision Transformer (ViT) architecture trained on RGB-fused images from three contrast phases (pre-contrast, early post-contrast, and late post-contrast). Our ViT model achieved state-of-the-art pCR prediction performance in HR+&#x2F;HER2- patients (AUC 0.94, accuracy 0.93). BreastDCEDL includes predefined benchmark splits, offering a framework for reproducible research and enabling clinically meaningful modeling in breast cancer imaging. </p>
<blockquote>
<p>ä¹³è…ºç™Œä»ç„¶æ˜¯å…¨çƒç™Œç—‡ç›¸å…³æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œå› æ­¤æ—©æœŸæ£€æµ‹å’Œç²¾ç¡®çš„æ²»ç–—ååº”ç›‘æµ‹æˆä¸ºè‡³å…³é‡è¦çš„ä¼˜å…ˆäº‹é¡¹ã€‚æˆ‘ä»¬æ¨å‡ºäº†BreastDCEDLï¼Œè¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒç­–åˆ’ã€ä¸ºæ·±åº¦å­¦ä¹ å‡†å¤‡çš„æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªI-SPY1ã€I-SPY2å’ŒDukeé˜Ÿåˆ—çš„2070ä¾‹ä¹³è…ºç™Œæ‚£è€…çš„æ²»ç–—å‰3DåŠ¨æ€å¢å¼ºMRIï¼ˆDCE-MRIï¼‰æ‰«æï¼Œæ‰€æœ‰æ•°æ®å‡æ¥è‡ªç™Œç—‡æˆåƒæ¡£æ¡ˆã€‚åŸå§‹çš„DICOMæˆåƒæ•°æ®è¢«ä¸¥æ ¼è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„3DNIfTIä½“ç§¯æ•°æ®ï¼ŒåŒæ—¶ä¿ç•™äº†ä¿¡å·å®Œæ•´æ€§ï¼Œå¹¶é™„æœ‰ç»Ÿä¸€çš„è‚¿ç˜¤æ³¨é‡Šå’Œåè°ƒä¸€è‡´çš„ä¸´åºŠå…ƒæ•°æ®ï¼ŒåŒ…æ‹¬ç—…ç†å®Œå…¨ååº”ï¼ˆpCRï¼‰ã€æ¿€ç´ å—ä½“ï¼ˆHRï¼‰å’ŒHER2çŠ¶æ€ã€‚å°½ç®¡DCE-MRIæä¾›äº†é‡è¦çš„è¯Šæ–­ä¿¡æ¯ï¼Œæ·±åº¦å­¦ä¹ åœ¨åˆ†ææ­¤ç±»å¤æ‚æ•°æ®æ–¹é¢æ‹¥æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ç”±äºç¼ºä¹å¯è®¿é—®çš„å…¬å…±å¤šä¸­å¿ƒæ•°æ®é›†ï¼Œè¿›å±•ä¸€ç›´å—åˆ°é™åˆ¶ã€‚BreastDCEDLé€šè¿‡æ”¯æŒå¼€å‘å…ˆè¿›æ¨¡å‹æ¥è§£å†³è¿™ä¸€å·®è·ï¼ŒåŒ…æ‹¬éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®çš„æœ€æ–°å˜å‹å™¨æ¶æ„ã€‚ä¸ºäº†å±•ç¤ºå…¶ç¨³å¥å»ºæ¨¡çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼€å‘äº†åŸºäºå˜å‹å™¨çš„é¦–ä¸ªä¹³è…ºç™ŒDCE-MRIæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨åœ¨ä¸‰ä¸ªå¯¹æ¯”é˜¶æ®µï¼ˆé¢„å¯¹æ¯”ã€æ—©æœŸåå¯¹æ¯”å’Œæ™šæœŸåå¯¹æ¯”ï¼‰çš„RGBèåˆå›¾åƒä¸Šè®­ç»ƒçš„Vision Transformerï¼ˆViTï¼‰æ¶æ„ã€‚æˆ‘ä»¬çš„ViTæ¨¡å‹åœ¨HR+&#x2F;HER2-æ‚£è€…ä¸­å®ç°äº†æœ€å…ˆè¿›çš„pCRé¢„æµ‹æ€§èƒ½ï¼ˆAUC 0.94ï¼Œå‡†ç¡®ç‡0.93ï¼‰ã€‚BreastDCEDLåŒ…æ‹¬é¢„å®šä¹‰çš„åŸºå‡†æµ‹è¯•åˆ†å‰²ï¼Œä¸ºå¯é‡å¤çš„ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ¡†æ¶ï¼Œå¹¶åœ¨ä¹³è…ºç™Œæˆåƒä¸­å®ç°äº†å…·æœ‰ä¸´åºŠæ„ä¹‰çš„å»ºæ¨¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.12190v3">PDF</a> </p>
<p><strong>Summary</strong><br>     ä¹³è…ºç™Œä»æ˜¯å…¨çƒç™Œç—‡æ­»äº¡çš„ä¸»è¦åŸå› ä¹‹ä¸€ï¼Œæ—©æœŸæ£€æµ‹å’Œå‡†ç¡®çš„æ²»ç–—ååº”ç›‘æµ‹æ˜¯å…³é”®ã€‚æˆ‘ä»¬æ¨å‡ºBreastDCEDLæ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ªI-SPY1ã€I-SPY2å’ŒDukeé˜Ÿåˆ—çš„2070ä¾‹ä¹³è…ºç™Œæ‚£è€…çš„é¢„æ²»ç–—3DåŠ¨æ€å¢å¼ºMRIï¼ˆDCE-MRIï¼‰æ‰«æå›¾åƒï¼Œæºäºç™Œç—‡æˆåƒæ¡£æ¡ˆã€‚æ•°æ®é›†åŒ…å«æ ‡å‡†åŒ–çš„3DNIfTIä½“ç§¯æ•°æ®ã€ç»Ÿä¸€çš„è‚¿ç˜¤æ³¨é‡Šå’Œåè°ƒçš„ä¸´åºŠå…ƒæ•°æ®ï¼Œå¦‚ç—…ç†å®Œå…¨ååº”ã€æ¿€ç´ å—ä½“å’ŒHER2çŠ¶æ€ã€‚BreastDCEDLè§£å†³äº†ç¼ºä¹å…¬å…±å¤šä¸­å¿ƒæ•°æ®é›†çš„é—®é¢˜ï¼Œæ”¯æŒå…ˆè¿›æ¨¡å‹çš„å¼€å‘ï¼ŒåŒ…æ‹¬åŸºäºæœ€æ–°Transformeræ¶æ„çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¼€å‘äº†åŸºäºVision Transformerçš„ä¹³è…ºDCE-MRIæ¨¡å‹ï¼Œåœ¨HR+&#x2F;HER2-æ‚£è€…ä¸­å®ç°äº†ä¼˜å¼‚æ€§èƒ½ã€‚BreastDCEDLåŒ…å«é¢„è®¾çš„åŸºå‡†åˆ†å‰²ï¼Œä¸ºå¯é‡å¤çš„ç ”ç©¶æä¾›æ¡†æ¶ï¼Œå¹¶èƒ½åœ¨ä¹³è…ºç™Œæˆåƒä¸­è¿›è¡Œä¸´åºŠæ„ä¹‰å»ºæ¨¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºç™Œä»æ˜¯å…¨çƒä¸»è¦çš„ç™Œç—‡æ­»äº¡åŸå› ä¹‹ä¸€ï¼Œæ—©æœŸæ£€æµ‹å’Œç²¾å‡†æ²»ç–—ååº”ç›‘æµ‹æ˜¯å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>BreastDCEDLæ˜¯ä¸€ä¸ªåŒ…å«é¢„æ²»ç–—DCE-MRIæ‰«æçš„æ·±åº¦å­¦ä¹ å°±ç»ªæ•°æ®é›†ï¼Œæ¶µç›–äº†å¤§é‡æ‚£è€…æ•°æ®å¹¶å…·å¤‡æ ‡å‡†åŒ–å¤„ç†çš„ä¸´åºŠå…ƒæ•°æ®ã€‚</li>
<li>æ•°æ®é›†è§£å†³äº†ç¼ºä¹å…¬å…±å¤šä¸­å¿ƒæ•°æ®é›†çš„éš¾é¢˜ï¼Œä¸ºå¼€å‘å…ˆè¿›çš„æ¨¡å‹æä¾›äº†æœºä¼šï¼ŒåŒ…æ‹¬åŸºäºVision Transformerçš„æ¨¡å‹ã€‚</li>
<li>é¦–æ¬¡å±•ç¤ºäº†åŸºäºVision Transformerçš„ä¹³è…ºDCE-MRIæ¨¡å‹ï¼Œåœ¨ç‰¹å®šæ‚£è€…ç¾¤ä½“ä¸­å®ç°äº†é«˜æ€§èƒ½é¢„æµ‹ã€‚</li>
<li>BreastDCEDLåŒ…æ‹¬é¢„è®¾çš„åŸºå‡†åˆ†å‰²ï¼Œæœ‰åŠ©äºè¿›è¡Œå¯é‡å¤çš„ç ”ç©¶å’Œä¸´åºŠæ„ä¹‰å»ºæ¨¡ã€‚</li>
<li>æ•°æ®é›†å¯ç”¨äºå¼€å‘é¢„æµ‹ç—…ç†å®Œå…¨ååº”çš„æ¨¡å‹ï¼Œå¯¹ä¸´åºŠå†³ç­–å…·æœ‰æ½œåœ¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.12190">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aec89b4433d9b729a2179ba0a92423f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-100bd057869bb8cf967db06d3455a640.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a9f0ec1c0db9c5ca2eebe0ec3ac097f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b64260b9af43fcc9c67030971d20f4c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Pathfinder-for-Low-altitude-Aircraft-with-Binary-Neural-Network"><a href="#Pathfinder-for-Low-altitude-Aircraft-with-Binary-Neural-Network" class="headerlink" title="Pathfinder for Low-altitude Aircraft with Binary Neural Network"></a>Pathfinder for Low-altitude Aircraft with Binary Neural Network</h2><p><strong>Authors:Kaijie Yin, Tian Gao, Hui Kong</strong></p>
<p>A prior global topological map (e.g., the OpenStreetMap, OSM) can boost the performance of autonomous mapping by a ground mobile robot. However, the prior map is usually incomplete due to lacking labeling in partial paths. To solve this problem, this paper proposes an OSM maker using airborne sensors carried by low-altitude aircraft, where the core of the OSM maker is a novel efficient pathfinder approach based on LiDAR and camera data, i.e., a binary dual-stream road segmentation model. Specifically, a multi-scale feature extraction based on the UNet architecture is implemented for images and point clouds. To reduce the effect caused by the sparsity of point cloud, an attention-guided gated block is designed to integrate image and point-cloud features. To optimize the model for edge deployment that significantly reduces storage footprint and computational demands, we propose a binarization streamline to each model component, including a variant of vision transformer (ViT) architecture as the encoder of the image branch, and new focal and perception losses to optimize the model training. The experimental results on two datasets demonstrate that our pathfinder method achieves SOTA accuracy with high efficiency in finding paths from the low-level airborne sensors, and we can create complete OSM prior maps based on the segmented road skeletons. Code and data are available at: \href{<a target="_blank" rel="noopener" href="https://github.com/IMRL/Pathfinder%7D%7Bhttps://github.com/IMRL/Pathfinder%7D">https://github.com/IMRL/Pathfinder}{https://github.com/IMRL/Pathfinder}</a>. </p>
<blockquote>
<p>å…ˆå‰çš„å…¨å±€æ‹“æ‰‘åœ°å›¾ï¼ˆä¾‹å¦‚ï¼ŒOpenStreetMapï¼ŒOSMï¼‰å¯ä»¥é€šè¿‡åœ°é¢ç§»åŠ¨æœºå™¨äººæå‡è‡ªä¸»æµ‹ç»˜çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºéƒ¨åˆ†è·¯å¾„ç¼ºä¹æ ‡æ³¨ï¼Œå…ˆå‰çš„åœ°å›¾é€šå¸¸æ˜¯ä¸å®Œæ•´çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨ä½ç©ºé£æœºæºå¸¦çš„æœºè½½ä¼ æ„Ÿå™¨çš„OSMåˆ¶ä½œæ–¹æ³•ã€‚å…¶ä¸­ï¼ŒOSMåˆ¶ä½œçš„æ ¸å¿ƒæ˜¯ä¸€ç§åŸºäºæ¿€å…‰é›·è¾¾å’Œç›¸æœºæ•°æ®çš„æ–°å‹é«˜æ•ˆè·¯å¾„æŸ¥æ‰¾æ–¹æ³•ï¼Œå³äºŒè¿›åˆ¶åŒæµé“è·¯åˆ†å‰²æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼ŒåŸºäºUNetæ¶æ„å®ç°äº†å›¾åƒå’Œç‚¹äº‘çš„å¤šå°ºåº¦ç‰¹å¾æå–ã€‚ä¸ºäº†å‡å°‘ç‚¹äº‘ç¨€ç–é€ æˆçš„å½±å“ï¼Œè®¾è®¡äº†ä¸€ç§æ³¨æ„åŠ›å¼•å¯¼çš„é—¨æ§å—æ¥èåˆå›¾åƒå’Œç‚¹äº‘ç‰¹å¾ã€‚ä¸ºäº†ä¼˜åŒ–æ¨¡å‹è¿›è¡Œè¾¹ç¼˜éƒ¨ç½²ï¼Œä»¥æ˜¾è‘—é™ä½å­˜å‚¨ç©ºé—´å’Œè®¡ç®—éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹çš„æ¯ä¸ªç»„ä»¶éƒ½è¿›è¡Œäº†äºŒå€¼åŒ–å¤„ç†ï¼ŒåŒ…æ‹¬ä½¿ç”¨å˜ç§è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¶æ„ä½œä¸ºå›¾åƒåˆ†æ”¯çš„ç¼–ç å™¨ï¼Œä»¥åŠæ–°çš„ç„¦ç‚¹å’Œæ„ŸçŸ¥æŸå¤±æ¥ä¼˜åŒ–æ¨¡å‹è®­ç»ƒã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è·¯å¾„æŸ¥æ‰¾æ–¹æ³•ä»¥é«˜æ•ˆç‡è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³çš„ç²¾åº¦ï¼Œå¯ä»¥ä»ä½çº§åˆ«æœºè½½ä¼ æ„Ÿå™¨ä¸­æ‰¾åˆ°è·¯å¾„ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥åŸºäºåˆ†å‰²çš„é“è·¯éª¨æ¶åˆ›å»ºå®Œæ•´çš„OSMå…ˆå‰åœ°å›¾ã€‚ä»£ç å’Œæ•°æ®å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/IMRL/Pathfinder]%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/IMRL/Pathfinder]æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.08824v4">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åˆ©ç”¨å…ˆéªŒå…¨å±€æ‹“æ‰‘åœ°å›¾ï¼ˆå¦‚OpenStreetMapï¼ŒOSMï¼‰å¯ä»¥æé«˜åœ°é¢ç§»åŠ¨æœºå™¨äººçš„è‡ªä¸»æ˜ å°„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºéƒ¨åˆ†è·¯å¾„ç¼ºä¹æ ‡æ³¨ï¼Œå…ˆéªŒåœ°å›¾é€šå¸¸æ˜¯ä¸å®Œæ•´çš„ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨ä½ç©ºé£æœºæºå¸¦çš„æœºè½½ä¼ æ„Ÿå™¨æ„å»ºOSMçš„æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ˜¯ä¸€ç§åŸºäºæ¿€å…‰é›·è¾¾å’Œç›¸æœºæ•°æ®çš„æ–°å‹é«˜æ•ˆè·¯å¾„æŸ¥æ‰¾æ–¹æ³•ï¼Œå³äºŒè¿›åˆ¶åŒæµé“è·¯åˆ†å‰²æ¨¡å‹ã€‚å…·ä½“å®ç°äº†åŸºäºUNetæ¶æ„çš„å¤šå°ºåº¦ç‰¹å¾æå–ï¼Œç”¨äºå›¾åƒå’Œç‚¹äº‘ã€‚ä¸ºè§£å†³ç‚¹äº‘ç¨€ç–å¼•èµ·çš„å½±å“ï¼Œè®¾è®¡äº†æ³¨æ„åŠ›å¼•å¯¼é—¨æ§å—æ¥æ•´åˆå›¾åƒå’Œç‚¹äº‘ç‰¹å¾ã€‚ä¸ºä¼˜åŒ–æ¨¡å‹è¿›è¡Œè¾¹ç¼˜éƒ¨ç½²ï¼Œæ˜¾è‘—é™ä½å­˜å‚¨å’Œè®¡ç®—éœ€æ±‚ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹å„ç»„ä»¶æå‡ºäº†ä¸€æ¡äºŒè¿›åˆ¶åŒ–æµçº¿ï¼ŒåŒ…æ‹¬ä½¿ç”¨è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰æ¶æ„ä½œä¸ºå›¾åƒåˆ†æ”¯çš„ç¼–ç å™¨ï¼Œä»¥åŠæ–°çš„ç„¦ç‚¹å’Œæ„ŸçŸ¥æŸå¤±ä»¥ä¼˜åŒ–æ¨¡å‹è®­ç»ƒã€‚åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è·¯å¾„æŸ¥æ‰¾æ–¹æ³•å…·æœ‰é«˜ç²¾åº¦å’Œé«˜æ•ˆç‡åœ°å¯»æ‰¾æ¥è‡ªä½çº§åˆ«æœºè½½ä¼ æ„Ÿå™¨çš„è·¯å¾„ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥åŸºäºåˆ†å‰²çš„é“è·¯éª¨æ¶åˆ›å»ºå®Œæ•´çš„OSMå…ˆéªŒåœ°å›¾ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å…ˆéªŒå…¨å±€æ‹“æ‰‘åœ°å›¾ï¼ˆå¦‚OpenStreetMapï¼‰èƒ½æé«˜è‡ªä¸»æ˜ å°„æ€§èƒ½ã€‚</li>
<li>æå‡ºçš„OSMåˆ¶ä½œæ–¹æ³•ä½¿ç”¨ä½ç©ºé£æœºçš„æœºè½½ä¼ æ„Ÿå™¨ã€‚</li>
<li>è·¯å¾„æŸ¥æ‰¾æ–¹æ³•çš„æ ¸å¿ƒæ˜¯äºŒè¿›åˆ¶åŒæµé“è·¯åˆ†å‰²æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹ä½¿ç”¨åŸºäºUNetæ¶æ„çš„å¤šå°ºåº¦ç‰¹å¾æå–å¤„ç†å›¾åƒå’Œç‚¹äº‘æ•°æ®ã€‚</li>
<li>ä¸ºåº”å¯¹ç‚¹äº‘çš„ç¨€ç–æ€§ï¼Œç»“åˆäº†å›¾åƒå’Œç‚¹äº‘ç‰¹å¾çš„æ³¨æ„åŠ›å¼•å¯¼é—¨æ§å—ã€‚</li>
<li>ä¸ºè¾¹ç¼˜éƒ¨ç½²ä¼˜åŒ–äº†æ¨¡å‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨è§†è§‰å˜å‹å™¨æ¶æ„å’Œæ–°çš„ç„¦ç‚¹åŠæ„ŸçŸ¥æŸå¤±ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¯»æ‰¾ä½çº§åˆ«æœºè½½ä¼ æ„Ÿå™¨è·¯å¾„æ–¹é¢è¾¾åˆ°é«˜ç²¾åº¦å’Œé«˜æ•ˆç‡ï¼Œå¹¶èƒ½åˆ›å»ºå®Œæ•´çš„OSMå…ˆéªŒåœ°å›¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.08824">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7c0f84def7d3dfaa9437c066eed0c43d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce73b9af3e93857e119f615bf970bbe0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1516d7e046ae4a10806301539e39e150.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-20950002ef60cd74f23be360b05c251c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f2ed11906713c0e20cfaf73b259df12.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ce303143b4b4fcdd428cfaa4f45c7a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de29c8a438da1aa3044540e52009a4fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0563e97ea436d0303a3ce4ee8aff5676.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6394e033381d4ddcfc400af63cc17365.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="The-Utility-of-the-Virtual-Imaging-Trials-Methodology-for-Objective-Characterization-of-AI-Systems-and-Training-Data"><a href="#The-Utility-of-the-Virtual-Imaging-Trials-Methodology-for-Objective-Characterization-of-AI-Systems-and-Training-Data" class="headerlink" title="The Utility of the Virtual Imaging Trials Methodology for Objective   Characterization of AI Systems and Training Data"></a>The Utility of the Virtual Imaging Trials Methodology for Objective   Characterization of AI Systems and Training Data</h2><p><strong>Authors:Fakrul Islam Tushar, Lavsen Dahal, Saman Sotoudeh-Paima, Ehsan Abadi, W. Paul Segars, Ehsan Samei, Joseph Y. Lo</strong></p>
<p>The credibility of Artificial Intelligence (AI) models for medical imaging continues to be a challenge, affected by the diversity of models, the data used to train the models, and applicability of their combination to produce reproducible results for new data. In this work we aimed to explore if the emerging Virtual Imaging Trials (VIT) methodologies can provide an objective resource to approach this challenge. The study was conducted for the case example of COVID-19 diagnosis using clinical and virtual computed tomography (CT) and chest radiography (CXR) processed with convolutional neural networks (CNNs). Multiple AI models were developed and tested using 3D ResNet-like and 2D EfficientNetv2 architectures across diverse datasets. The performance differences were evaluated in terms of the area under the curve (AUC) and the DeLong method for AUC confidence intervals. The models trained on the most diverse datasets showed the highest external testing performance, with AUC values ranging from 0.73 to 0.76 for CT and 0.70 to 0.73 for CXR. Internal testing yielded higher AUC values (0.77 to 0.85 for CT and 0.77 to 1.0 for CXR), highlighting a substantial drop in performance during external validation, which underscores the importance of diverse and comprehensive training and testing data. Most notably, the VIT approach provided objective assessment of the utility of diverse models and datasets while further providing insight into the influence of dataset characteristics, patient factors, and imaging physics on AI efficacy. The VIT approach can be used to enhance model transparency and reliability, offering nuanced insights into the factors driving AI performance and bridging the gap between experimental and clinical settings. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¨¡å‹åœ¨åŒ»å­¦å½±åƒé¢†åŸŸçš„å¯ä¿¡åº¦ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå—åˆ°æ¨¡å‹å¤šæ ·æ€§ã€ç”¨äºè®­ç»ƒæ¨¡å‹çš„æ•°æ®ï¼Œä»¥åŠå®ƒä»¬ç»„åˆåº”ç”¨äºæ–°æ•°æ®äº§ç”Ÿå¯é‡å¤ç»“æœçš„å½±å“ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨æ¢ç´¢æ–°å…´çš„è™šæ‹Ÿæˆåƒè¯•éªŒï¼ˆVITï¼‰æ–¹æ³•æ˜¯å¦å¯ä»¥ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜æä¾›å®¢è§‚èµ„æºã€‚è¯¥ç ”ç©¶æ˜¯é’ˆå¯¹COVID-19è¯Šæ–­çš„æ¡ˆä¾‹åˆ†æï¼Œä½¿ç”¨ä¸´åºŠå’Œè™šæ‹Ÿè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä»¥åŠç»è¿‡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¤„ç†çš„èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰ã€‚ä½¿ç”¨3D ResNetå’Œ2D EfficientNetv2æ¶æ„å¼€å‘å¹¶æµ‹è¯•äº†å¤šä¸ªAIæ¨¡å‹ï¼Œæ¶‰åŠä¸åŒçš„æ•°æ®é›†ã€‚æ€§èƒ½å·®å¼‚æ˜¯æ ¹æ®æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰å’ŒAUCç½®ä¿¡åŒºé—´çš„DeLongæ–¹æ³•è¿›è¡Œè¯„ä¼°çš„ã€‚åœ¨æœ€å…·å¤šæ ·æ€§çš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨å¤–éƒ¨æµ‹è¯•ä¸­çš„è¡¨ç°æœ€ä½³ï¼ŒCTçš„AUCå€¼èŒƒå›´ä»0.73åˆ°0.76ï¼ŒCXRçš„AUCå€¼èŒƒå›´ä»0.70åˆ°0.73ã€‚å†…éƒ¨æµ‹è¯•çš„AUCå€¼è¾ƒé«˜ï¼ˆCTçš„AUCå€¼ä¸º0.77åˆ°0.85ï¼ŒCXRçš„AUCå€¼ä¸º0.77åˆ°1.0ï¼‰ï¼Œçªæ˜¾äº†å¤–éƒ¨éªŒè¯ä¸­æ€§èƒ½çš„å¤§å¹…ä¸‹é™ï¼Œè¿™å¼ºè°ƒäº†å¤šæ ·æ€§å’Œå…¨é¢çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„é‡è¦æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒVITæ–¹æ³•ä¸ºä¸åŒçš„æ¨¡å‹å’Œæ•°æ®é›†æä¾›äº†å®¢è§‚è¯„ä¼°ï¼ŒåŒæ—¶è¿›ä¸€æ­¥æ·±å…¥äº†è§£æ•°æ®é›†ç‰¹æ€§ã€æ‚£è€…å› ç´ å’Œæˆåƒç‰©ç†å­¦å¯¹AIæ•ˆåŠ›çš„å½±å“ã€‚VITæ–¹æ³•å¯ç”¨äºæé«˜æ¨¡å‹çš„é€æ˜åº¦å’Œå¯é æ€§ï¼Œæä¾›å…³äºé©±åŠ¨AIæ€§èƒ½å› ç´ çš„å¾®å¦™è§è§£ï¼Œå¹¶å¼¥åˆå®éªŒä¸ä¸´åºŠç¯å¢ƒä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09730v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨åŒ»å­¦å½±åƒé¢†åŸŸçš„å¯ä¿¡åº¦é—®é¢˜ï¼Œç ”ç©¶äº†è™šæ‹Ÿæˆåƒè¯•éªŒï¼ˆVITï¼‰æ–¹æ³•èƒ½å¦ä¸ºè§£å†³æ­¤é—®é¢˜æä¾›å®¢è§‚èµ„æºã€‚ä»¥æ–°å† è‚ºç‚è¯Šæ–­ä¸ºä¾‹ï¼Œç ”ç©¶ä½¿ç”¨ä¸´åºŠå’Œè™šæ‹Ÿè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä»¥åŠèƒ¸éƒ¨æ”¾å°„æ‘„å½±ï¼ˆCXRï¼‰å¤„ç†çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¨¡å‹ï¼Œå¹¶åœ¨å¤šæ ·æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨å¤šæ ·æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹åœ¨å¤–éƒ¨æµ‹è¯•ä¸­å…·æœ‰æœ€é«˜æ€§èƒ½ï¼Œè€Œå†…éƒ¨æµ‹è¯•çš„æ€§èƒ½è¾ƒé«˜ï¼Œå‡¸æ˜¾äº†å¤–éƒ¨éªŒè¯æ—¶æ€§èƒ½çš„æ˜¾è‘—ä¸‹é™ã€‚è™šæ‹Ÿæˆåƒè¯•éªŒï¼ˆVITï¼‰æ–¹æ³•ä¸ºè¯„ä¼°ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†æä¾›äº†å®¢è§‚è§†è§’ï¼Œå¹¶è¿›ä¸€æ­¥æ·±å…¥æ¢ç©¶äº†æ•°æ®é›†ç‰¹å¾ã€æ‚£è€…å› ç´ å’Œæˆåƒç‰©ç†å¯¹äººå·¥æ™ºèƒ½æ•ˆèƒ½çš„å½±å“ã€‚æ­¤ç ”ç©¶æ–¹æ³•å¯ä»¥æé«˜æ¨¡å‹çš„é€æ˜åº¦å’Œå¯é æ€§ï¼Œå¹¶ä¸ºå¼¥åˆå®éªŒä¸ä¸´åºŠä¹‹é—´çš„å·®è·æä¾›æ·±åˆ»è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨åŒ»å­¦å½±åƒé¢†åŸŸçš„å¯ä¿¡åº¦é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦å—æ¨¡å‹å¤šæ ·æ€§ã€è®­ç»ƒæ•°æ®å¤šæ ·æ€§å’Œå¯¹æ–°æ•°æ®äº§ç”Ÿå¯é‡å¤ç»“æœçš„èƒ½åŠ›å½±å“ã€‚</li>
<li>è™šæ‹Ÿæˆåƒè¯•éªŒï¼ˆVITï¼‰æ–¹æ³•ä¸ºè¯„ä¼°æ¨¡å‹æ€§èƒ½æä¾›äº†æ–°çš„è§†è§’ã€‚</li>
<li>ä½¿ç”¨ä¸´åºŠå’Œè™šæ‹Ÿè®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä»¥åŠèƒ¸éƒ¨æ”¾å°„æ‘„å½±ï¼ˆCXRï¼‰å¤„ç†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰è¿›è¡Œæ–°å† è‚ºç‚è¯Šæ–­çš„æ¨¡å‹æµ‹è¯•è¡¨æ˜å¤šæ ·æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹å…·æœ‰æœ€ä½³æ€§èƒ½ã€‚</li>
<li>å¤–éƒ¨éªŒè¯ç»“æœæ­ç¤ºäº†æ¨¡å‹çš„æ€§èƒ½ä¸‹é™ï¼Œçªæ˜¾äº†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„å¤šæ ·æ€§å’Œå…¨é¢æ€§çš„é‡è¦æ€§ã€‚</li>
<li>VITæ–¹æ³•èƒ½å®¢è§‚åœ°è¯„ä¼°ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†çš„æ€§èƒ½ï¼Œæä¾›å¯¹AIæ•ˆèƒ½å½±å“å› ç´ çš„æ·±åˆ»è§è§£ï¼Œå¦‚æ•°æ®é›†ç‰¹æ€§ã€æ‚£è€…å› ç´ å’Œæˆåƒç‰©ç†ã€‚</li>
<li>VITæ–¹æ³•æœ‰åŠ©äºå¢å¼ºæ¨¡å‹çš„é€æ˜åº¦å’Œå¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2308.09730">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-89cf9bc7f742bc73c14f21144f976f7c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-479b6f5791168b98df761e27ec55928e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-186bf0c29239a0e9db03a3eaf6a68879.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6c30f54fa848454ef5c022bbc33b3c15.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  Alleviating Textual Reliance in Medical Language-guided Segmentation via   Prototype-driven Semantic Approximation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-16/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-ee0423bb158a1a1596661f062511dc8e.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  Streaming 4D Visual Geometry Transformer
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31879.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
