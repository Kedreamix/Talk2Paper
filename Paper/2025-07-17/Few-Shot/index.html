<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-07-17  CharaConsist Fine-Grained Consistent Character Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-df399fd115c07e260f7908d5f08cbff1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    84 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-17-更新"><a href="#2025-07-17-更新" class="headerlink" title="2025-07-17 更新"></a>2025-07-17 更新</h1><h2 id="CharaConsist-Fine-Grained-Consistent-Character-Generation"><a href="#CharaConsist-Fine-Grained-Consistent-Character-Generation" class="headerlink" title="CharaConsist: Fine-Grained Consistent Character Generation"></a>CharaConsist: Fine-Grained Consistent Character Generation</h2><p><strong>Authors:Mengyu Wang, Henghui Ding, Jianing Peng, Yao Zhao, Yunpeng Chen, Yunchao Wei</strong></p>
<p>In text-to-image generation, producing a series of consistent contents that preserve the same identity is highly valuable for real-world applications. Although a few works have explored training-free methods to enhance the consistency of generated subjects, we observe that they suffer from the following problems. First, they fail to maintain consistent background details, which limits their applicability. Furthermore, when the foreground character undergoes large motion variations, inconsistencies in identity and clothing details become evident. To address these problems, we propose CharaConsist, which employs point-tracking attention and adaptive token merge along with decoupled control of the foreground and background. CharaConsist enables fine-grained consistency for both foreground and background, supporting the generation of one character in continuous shots within a fixed scene or in discrete shots across different scenes. Moreover, CharaConsist is the first consistent generation method tailored for text-to-image DiT model. Its ability to maintain fine-grained consistency, combined with the larger capacity of latest base model, enables it to produce high-quality visual outputs, broadening its applicability to a wider range of real-world scenarios. The source code has been released at <a target="_blank" rel="noopener" href="https://github.com/Murray-Wang/CharaConsist">https://github.com/Murray-Wang/CharaConsist</a> </p>
<blockquote>
<p>在文本到图像生成过程中，生成一系列保持一致的内容并保留相同的身份对于实际应用而言具有极高的价值。尽管已有一些作品探讨了无训练方法来提高生成主题的一致性，但我们观察到它们存在以下问题。首先，它们无法保持背景细节的连贯性，这限制了其适用性。此外，当前景角色经历较大的动作变化时，身份和服装细节的不一致性变得明显。为了解决这些问题，我们提出了CharactConsistency（简称为CharaConsist），它采用点跟踪注意力、自适应令牌合并以及前景和背景的解耦控制。CharaConsist能够实现前景和背景的精细一致性，支持在固定场景内的连续镜头中生成一个角色或在不同场景中的离散镜头中生成角色。此外，CharaConsist是针对文本到图像DiT模型的定制一致性生成方法。其保持精细一致性的能力，结合最新基础模型的更大容量，使其能够产生高质量的视觉输出，扩大了其在更广泛的现实场景中的应用范围。源代码已发布在<a target="_blank" rel="noopener" href="https://github.com/Murray-Wang/CharaConsist%E3%80%82">https://github.com/Murray-Wang/CharaConsist。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11533v1">PDF</a> ICCV 2025 accepted paper, project page:   <a target="_blank" rel="noopener" href="https://murray-wang.github.io/CharaConsist/">https://murray-wang.github.io/CharaConsist/</a></p>
<p><strong>Summary</strong><br>文本中介绍了一种用于解决文本转图像生成中一致性问题的新方法——CharaConsist。该方法通过点跟踪注意力、自适应令牌合并以及前景和背景的独立控制，提高了前景和背景的一致性。它能够生成连续镜头中的单个角色或不同场景中的离散镜头，同时保持高一致性和高质量视觉输出。适用于更广泛的真实世界场景应用。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CharaConsist旨在解决文本转图像生成中的一致性问题。</li>
<li>通过点跟踪注意力机制维持背景细节的一致性。</li>
<li>自适应令牌合并解决了前景角色大动作变化时的一致性问题。</li>
<li>通过前景和背景的独立控制，提高了生成内容的精细一致性。</li>
<li>CharaConsist是专为文本转图像DiT模型设计的首个一致性生成方法。</li>
<li>该方法结合了最新基础模型的更大容量，可产生高质量视觉输出。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11533">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-9a838b555cf47ee90071d01a1fa61ad2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08d78a3634d06a9a8286b16194113dd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bff8cdda366beb93fa86fa3f04b0ffe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da92232f58a9617ef68cee4a7b227b2f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FMC-Formalization-of-Natural-Language-Mathematical-Competition-Problems"><a href="#FMC-Formalization-of-Natural-Language-Mathematical-Competition-Problems" class="headerlink" title="FMC: Formalization of Natural Language Mathematical Competition Problems"></a>FMC: Formalization of Natural Language Mathematical Competition Problems</h2><p><strong>Authors:Jiaxuan Xie, Chengwu Liu, Ye Yuan, Siqi Li, Zhiping Xiao, Ming Zhang</strong></p>
<p>Efficient and accurate autoformalization methods, which leverage large-scale datasets of extensive natural language mathematical problems to construct formal language datasets, are key to advancing formal mathematical reasoning. In this paper, we propose an autoformalization pipeline based on large language models with error feedback, achieving a fully automatic and training-free formalization approach. Using this pipeline, we curate an Olympiad-level dataset aligning natural language problems with Lean formalizations. The dataset comprises $3,922$ mathematical problems in natural language and $9,787$ in Lean, of which $64.46%$ were assessed as at least above-average quality, making it suitable as a benchmark for automated theorem provers. Additionally, we investigate the formalization and reasoning capabilities of various LLMs and empirically demonstrate that few-shot learning, error feedback, and increasing sampling numbers enhance the autoformalization process. Experiments of three automated theorem provers on the \dataset\ dataset also highlight its challenging nature and its value as a benchmark for formal reasoning tasks. </p>
<blockquote>
<p>高效且准确的自动形式化方法，是利用大规模的自然语言数学问题数据集来构建形式化语言数据集的关键，是推动数学形式推理发展的核心。在本文中，我们提出了一种基于大型语言模型的自动形式化流水线，该流水线具有错误反馈功能，实现了全自动且无需训练的形式化方法。使用该流水线，我们整理了一个与Lean形式化对齐的奥林匹亚级别数据集。该数据集包含以自然语言表述的3922个数学问题和以Lean表述的9787个数学问题，其中经过评估，有64.46%至少达到了平均质量以上，适合作为自动化定理证明者的基准测试。此外，我们还探讨了各种大型语言模型的形式化推理能力，并通过实证发现，小样学习、错误反馈和增加采样数量都可以增强自动形式化过程。在特定数据集上进行的三个自动化定理证明器的实验也突显了它的挑战性以及作为基准测试的价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11275v1">PDF</a> Accepted in ICML 2025 AI4MATH Workshop</p>
<p><strong>Summary</strong></p>
<p>大规模自然语言数学问题的数据集对于构建形式语言数据集至关重要，推动了形式化数学推理的发展。本文提出了基于大型语言模型的自动形式化管道，采用错误反馈实现全自动、无需训练的形式化方法。使用该管道，我们整理了一个与Lean形式化对齐的奥赛级别数据集，包含自然语言数学问题3922个，Lean形式化问题9787个，其中64.46%被评为至少中等质量水平以上，适合作为自动化定理证明器的基准测试。此外，本文还探讨了不同大型语言模型的形式化和推理能力，并实证表明少样本学习、错误反馈和增加采样数量可提升自动形式化过程的效果。在\dataset\数据集上进行的三个自动化定理证明器的实验突显了其挑战性，并验证了其作为形式推理任务基准测试的价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大规模数据集在构建形式语言数据集中的作用至关重要，推动了形式化数学推理的进步。</li>
<li>提出了基于大型语言模型的自动形式化管道，实现全自动、无需训练的形式化方法。</li>
<li>整理了一个奥赛级别的数据集，包含自然语言与Lean形式化的数学问题，多数问题质量较高，适合作为自动化定理证明器的基准测试。</li>
<li>探讨并实证了少样本学习、错误反馈和增加采样数量在提升自动形式化过程中的应用效果。</li>
<li>\dataset\数据集具有挑战性，可作为形式推理任务的基准测试。</li>
<li>文中展示了该管道及数据集在自动定理证明任务中的潜力与应用价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11275">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-445b0a2020cd2186c396d2db1d64940b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-130c2923d5f965263e027823c1675af9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-308189fc6f297a63da0befccba9d4a14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00b0e83e56adca02394c83bca0b5af9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be61597329f358d5cd888912e54bdc91.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MSA-at-ImageCLEF-2025-Multimodal-Reasoning-Multilingual-Multimodal-Reasoning-With-Ensemble-Vision-Language-Models"><a href="#MSA-at-ImageCLEF-2025-Multimodal-Reasoning-Multilingual-Multimodal-Reasoning-With-Ensemble-Vision-Language-Models" class="headerlink" title="MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal   Reasoning With Ensemble Vision Language Models"></a>MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal   Reasoning With Ensemble Vision Language Models</h2><p><strong>Authors:Seif Ahmed, Mohamed T. Younes, Abdelrahman Moustafa, Abdelrahman Allam, Hamza Moustafa</strong></p>
<p>We present a robust ensemble-based system for multilingual multimodal reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which handles final answer selection, all coordinated through carefully engineered few-shot and zero-shot prompts. We conducted an extensive ablation study, training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3, Mistral) on an English dataset and its multilingual augmented version. Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for comparison and found it to substantially outperform the trained models. Prompt design also proved critical: enforcing concise, language-normalized formats and prohibiting explanatory text boosted model accuracy on the English validation set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA) achieved first place overall in the multilingual track with 81.4% accuracy, and led 11 out of 13 individual language tracks, with top results such as 95.07% for Croatian and 92.12% for Italian. These findings highlight that lightweight OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual augmentation, can outperform heavier end-to-end models in high-stakes, multilingual educational settings. </p>
<blockquote>
<p>我们针对ImageCLEF 2025 EXAMS V挑战，设计了一个稳健的基于集成系统的多语言多模态推理系统。我们的方法融合了Gemini 2.5 Flash进行视觉描述、Gemini 1.5 Pro进行字幕优化和一致性检查，以及Gemini 2.5 Pro作为推理器进行最终答案选择，所有这些功能都通过精心设计的少量和零样本提示进行协调。我们对多个大型语言模型（Gemini 2.5 Flash、Phi 4、Gemma 3、Mistral）进行了广泛的消融研究训练，训练数据集包括英语数据集及其多语言增强版本。此外，我们还对Gemini 2.5 Flash进行了零样本设置下的比较评估，发现其性能远超训练模型。提示设计也证明是关键：采用简洁、语言规范化的格式，禁止解释性文本，将模型在英语验证集上的准确率从55.9%提高到61.7%。在官方排行榜上，我们的系统（MSA团队）在多语言赛道上总体排名第一，准确率为81.4%，并在13个语言赛道中的11个赛道上领先，如在克罗地亚语赛道上的95.07%和意大利语赛道的92.12%。这些发现表明，当轻量级的OCR-VLM集成系统与精确的提示策略和跨语言增强相结合时，可以在高风险、多语言的教育环境中表现优于更重的端到端模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11114v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一个为ImageCLEF 2025 EXAMS V挑战设计的稳健的基于集成系统的多语言多模态推理系统。该系统通过整合Gemini 2.5 Flash进行视觉描述、Gemini 1.5 Pro进行标题优化和一致性检查、以及Gemini 2.5 Pro作为推理机进行最终答案选择，并通过精心设计的少样本和零样本提示进行协调。研究进行了广泛的消融研究，训练了多个大型语言模型，并在英语数据集及其多语言增强版本上进行了评估。结果显示，在零样本设置下，Gemini 2.5 Flash的表现优于训练模型。提示设计也至关重要：采用简洁、语言规范化的格式，禁止解释性文本，提高了模型在英语验证集上的准确率。该系统在多元语言轨道上获得第一名，准确率高达81.4%，并在13个语言轨道中的11个轨道上领先，如克罗地亚语的95.07%和意大利语的92.12%。这表明在关键的多语言教育环境中，轻量级的OCR-VLM集成系统配合精确的提示策略和跨语言增强技术，可以超越更重的端到端模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>团队MSA提出一种针对ImageCLEF 2025 EXAMS V挑战的多语言多模态推理系统。</li>
<li>系统集成了多个组件，包括用于视觉描述的Gemini 2.5 Flash、用于标题优化的Gemini 1.5 Pro和作为推理机的Gemini 2.5 Pro。</li>
<li>通过精心设计的少样本和零样本提示进行协调各组件。</li>
<li>研究进行了广泛的消融研究，训练了多个大型语言模型，并在英语数据集及其多语言增强版本上评估了系统性能。</li>
<li>Gemini 2.5 Flash在零样本设置下的表现优于训练模型。</li>
<li>提示设计对模型性能有重要影响，采用简洁、语言规范化的格式，禁止解释性文本可以提高模型准确率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11114">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ed0d9efb238edd3ca208a135196be004.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c344a8b0018e5f5d067bd1cc640872a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd12f8f53a91ab2d8889f73aae6a5472.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-028d93bd57c1f26272b8c3342f232bf1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Bridge-Feature-Matching-and-Cross-Modal-Alignment-with-Mutual-filtering-for-Zero-shot-Anomaly-Detection"><a href="#Bridge-Feature-Matching-and-Cross-Modal-Alignment-with-Mutual-filtering-for-Zero-shot-Anomaly-Detection" class="headerlink" title="Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering   for Zero-shot Anomaly Detection"></a>Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering   for Zero-shot Anomaly Detection</h2><p><strong>Authors:Yuhu Bai, Jiangning Zhang, Yunkang Cao, Guangyuan Lu, Qingdong He, Xiangtai Li, Guanzhong Tian</strong></p>
<p>With the advent of vision-language models (e.g., CLIP) in zero- and few-shot settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in recent research, where the rare classes are essential and expected in many applications. This study introduces \textbf{FiSeCLIP} for ZSAD with training-free \textbf{CLIP}, combining the feature matching with the cross-modal alignment. Testing with the entire dataset is impractical, while batch-based testing better aligns with real industrial needs, and images within a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes other images in the same batch as reference information for the current image. However, the lack of labels for these references can introduce ambiguity, we apply text information to \textbf{fi}lter out noisy features. In addition, we further explore CLIP’s inherent potential to restore its local \textbf{se}mantic correlation, adapting it for fine-grained anomaly detection tasks to enable a more accurate filtering process. Our approach exhibits superior performance for both anomaly classification and segmentation on anomaly detection benchmarks, building a stronger baseline for the direction, e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by +4.6%$\uparrow$&#x2F;+5.7%$\uparrow$ in segmentation metrics AU-ROC&#x2F;$F_1$-max. </p>
<blockquote>
<p>随着视觉语言模型（例如CLIP）在零样本和少样本场景的出现，CLIP已被广泛应用于最近的零样本异常检测（ZSAD）研究中，其中稀有类别在许多应用中都是关键和预期的。本研究介绍了一种用于ZSAD的FiSeCLIP，它结合了特征匹配和跨模态对齐，使用无需训练的CLIP。对整个数据集进行测试并不实际，而基于批次的测试更符合实际的工业需求，批次内的图像可以作为相互的参考点。因此，FiSeCLIP利用同一批次中的其他图像作为当前图像的参考信息。然而，这些参考缺乏标签可能会引入模糊性，我们应用文本信息来过滤掉嘈杂的特征。此外，我们进一步探索CLIP的内在潜力来恢复其局部语义相关性，将其适应于精细粒度的异常检测任务，以实现更精确过滤过程。我们的方法在异常检测基准测试上表现出卓越的性能，无论是异常分类还是分割，为这一方向建立了更强的基准。例如，在MVTec-AD上，FiSeCLIP在分割指标AU-ROC&#x2F;F1-max上的表现优于最新技术AdaCLIP，分别提高了+4.6%↑&#x2F;+5.7%↑。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11003v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着视觉语言模型（如CLIP）在零样本和少样本场景的应用，CLIP已被广泛用于零样本异常检测（ZSAD）。本研究提出了FiSeCLIP方法用于ZSAD，采用无需训练的CLIP，结合特征匹配和跨模态对齐。本研究利用同一批次的其他图像作为当前图像的参考信息，并采用文本信息过滤掉噪声特征。此外，研究还探索了CLIP的固有潜力来恢复其局部语义相关性，以适应精细粒度异常检测任务，以实现更准确的过滤过程。此方法在异常检测基准测试中表现出卓越的性能，为未来的研究建立了更强的基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FiSeCLIP被提出用于零样本异常检测（ZSAD），结合了特征匹配和跨模态对齐。</li>
<li>研究利用同一批次的其他图像作为参考信息来检测当前图像中的异常。</li>
<li>文本信息被用来过滤掉由于使用图像参考而产生的噪声特征。</li>
<li>研究探索了CLIP的潜力来恢复其局部语义相关性，以提高异常检测的准确性。</li>
<li>FiSeCLIP在异常分类和分段方面表现出卓越性能，建立了该方向研究的新基线。</li>
<li>在MVTec-AD基准测试中，FiSeCLIP在分割指标AU-ROC和$F_1$-max上超过了现有最佳方法AdaCLIP。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11003">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b0033f38010c548954d4c3ef37487694.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d65d1855db938297605fd42ee77656b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e05ab6cda6dd4ddd82e75b60d7e00dc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8018b3ed88149501b825bcd319143394.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae173447baac2330f386bc0627f6c8e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ccd4241d6f1a0ecd078e2ca1907c299.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Evaluating-Generated-Commit-Messages-with-Large-Language-Models"><a href="#Evaluating-Generated-Commit-Messages-with-Large-Language-Models" class="headerlink" title="Evaluating Generated Commit Messages with Large Language Models"></a>Evaluating Generated Commit Messages with Large Language Models</h2><p><strong>Authors:Qunhong Zeng, Yuxia Zhang, Zexiong Ma, Bo Jiang, Ningyuan Sun, Klaas-Jan Stol, Xingyu Mou, Hui Liu</strong></p>
<p>Commit messages are essential in software development as they serve to document and explain code changes. Yet, their quality often falls short in practice, with studies showing significant proportions of empty or inadequate messages. While automated commit message generation has advanced significantly, particularly with Large Language Models (LLMs), the evaluation of generated messages remains challenging. Traditional reference-based automatic metrics like BLEU, ROUGE-L, and METEOR have notable limitations in assessing commit message quality, as they assume a one-to-one mapping between code changes and commit messages, leading researchers to rely on resource-intensive human evaluation. This study investigates the potential of LLMs as automated evaluators for commit message quality. Through systematic experimentation with various prompt strategies and state-of-the-art LLMs, we demonstrate that LLMs combining Chain-of-Thought reasoning with few-shot demonstrations achieve near human-level evaluation proficiency. Our LLM-based evaluator significantly outperforms traditional metrics while maintaining acceptable reproducibility, robustness, and fairness levels despite some inherent variability. This work conducts a comprehensive preliminary study on using LLMs for commit message evaluation, offering a scalable alternative to human assessment while maintaining high-quality evaluation. </p>
<blockquote>
<p>代码提交信息是软件开发中的关键组成部分，它们用于记录和解释代码更改。然而，在实践中，它们的品质往往不尽如人意，研究表明有很大比例的信息为空或不足。尽管自动生成的提交消息已经有了很大的进步，特别是借助大型语言模型（LLM），但对生成信息的评估仍然具有挑战性。传统的基于参考的自动度量方法，如BLEU、ROUGE-L和METEOR，在评估提交消息质量方面存在明显的局限性，因为它们假设代码更改和提交消息之间存在一一对应关系，导致研究人员不得不依赖资源密集型的人类评估方法。本研究探讨了将大型语言模型作为提交消息质量自动评估器的潜力。通过系统地尝试各种提示策略和最先进的LLM，我们证明了结合“思维链”推理和少量示例的大型语言模型能够实现接近人类水平的评估能力。我们的基于LLM的评估器在保持可接受的再现性、稳健性和公平性的同时，显著优于传统度量标准。尽管存在一些固有的变化性，但这项工作进行了关于使用LLM进行提交信息评估的全面初步研究，提供了一种可扩展的替代人类评估的方法，同时保持了高质量的评价。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10906v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>这篇文章探讨了使用大型语言模型（LLMs）作为自动化评估提交消息质量的潜力。通过系统的实验和提示策略，研究表明结合Chain-of-Thought推理和少量演示的LLMs能够达到接近人类水平的评估能力，显著优于传统指标，同时保持可接受的重复性、稳健性和公平性水平。这为使用LLMs进行提交消息评估提供了可扩展的替代方案，同时保持了高质量的评估。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提交消息在软件开发中具有重要性和作用，但实际执行中其质量常常不足或缺失。</li>
<li>大型语言模型（LLMs）在自动提交消息生成方面取得了进展。</li>
<li>传统自动评估指标（如BLEU、ROUGE-L和METEOR）在评估提交消息质量方面存在局限性。</li>
<li>这项研究探索了使用LLMs作为自动化评估提交消息质量的潜力。</li>
<li>结合Chain-of-Thought推理和少量提示策略的LLMs可以接近人类水平的评估能力。</li>
<li>LLMs的评估器在性能上显著优于传统指标，同时保持了可接受的重复性、稳健性和公平性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10906">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-dfedd21bb1def2f44313781d6f9965c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50aa7c6d7cfd4cc37424d3adb1501106.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abeb38836dc5379baaa34fdedd804c67.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="BioScore-A-Foundational-Scoring-Function-For-Diverse-Biomolecular-Complexes"><a href="#BioScore-A-Foundational-Scoring-Function-For-Diverse-Biomolecular-Complexes" class="headerlink" title="BioScore: A Foundational Scoring Function For Diverse Biomolecular   Complexes"></a>BioScore: A Foundational Scoring Function For Diverse Biomolecular   Complexes</h2><p><strong>Authors:Yuchen Zhu, Jihong Chen, Yitong Li, Xiaomin Fang, Xianbin Ye, Jingzhou He, Xujun Zhang, Jingxuan Ge, Chao Shen, Xiaonan Zhang, Tingjun Hou, Chang-Yu Hsieh</strong></p>
<p>Structural assessment of biomolecular complexes is vital for translating molecular models into functional insights, shaping our understanding of biology and aiding drug discovery. However, current structure-based scoring functions often lack generalizability across diverse biomolecular systems. We present BioScore, a foundational scoring function that addresses key challenges – data sparsity, cross-system representation, and task compatibility – through a dual-scale geometric graph learning framework with tailored modules for structure assessment and affinity prediction. BioScore supports a wide range of tasks, including affinity prediction, conformation ranking, and structure-based virtual screening. Evaluated on 16 benchmarks spanning proteins, nucleic acids, small molecules, and carbohydrates, BioScore consistently outperforms or matches 70 traditional and deep learning methods. Our newly proposed PPI Benchmark further enables comprehensive evaluation of protein-protein complex scoring. BioScore demonstrates broad applicability: (1) pretraining on mixed-structure data boosts protein-protein affinity prediction by up to 40% and antigen-antibody binding correlation by over 90%; (2) cross-system generalizability enables zero- and few-shot prediction with up to 71% correlation gain; and (3) its unified representation captures chemically challenging systems such as cyclic peptides, improving affinity prediction by over 60%. BioScore establishes a robust and generalizable framework for structural assessment across complex biomolecular landscapes. </p>
<blockquote>
<p>生物分子复合物的结构评估对于将分子模型转化为功能见解、塑造我们对生物学的理解以及辅助药物发现至关重要。然而，当前基于结构的评分函数通常在跨不同生物分子系统时缺乏通用性。我们提出了BioScore，这是一个基础评分函数，通过双尺度几何图学习框架和针对结构评估和亲和力预测的定制模块，解决了数据稀疏性、跨系统表示和任务兼容性等关键挑战。BioScore支持各种任务，包括亲和力预测、构象排序和基于结构的虚拟筛选。在涵盖蛋白质、核酸、小分子和碳水化合物的16个基准测试上，BioScore持续优于或与传统深度学习方法的匹配度达到70种以上。我们新提出的PPI基准测试进一步实现了蛋白质-蛋白质复合物评分的全面评估。BioScore展示了广泛的应用性：（1）在混合结构数据上进行预训练，蛋白质-蛋白质亲和力预测提高了高达40%，抗原-抗体结合相关性提高了超过90%；（2）跨系统的通用性实现了零样本和少样本预测，相关性增益高达71%；（3）其统一表示能够捕捉化学上复杂的系统，如环状肽，亲和力预测提高了超过60%。BioScore为跨复杂生物分子景观的结构评估建立了稳健且通用的框架。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10877v1">PDF</a> </p>
<p><strong>Summary</strong>：生物分子复合物的结构评估对于将分子模型转化为功能见解、塑造我们对生物学的理解以及辅助药物发现至关重要。然而，当前的结构评分函数在跨不同生物分子系统时缺乏通用性。本研究提出了BioScore，一个基础评分函数，通过双尺度几何图学习框架和针对结构评估和亲和力预测的定制模块，解决了数据稀疏性、跨系统表示和任务兼容性等关键挑战。BioScore支持多种任务，包括亲和力预测、构象排名和结构基础上的虚拟筛选。在跨越蛋白质、核酸、小分子和碳水化合物的16个基准测试上，BioScore的表现始终优于或相当于70种传统和深度学习方法。此外，新提出的PPI基准测试进一步实现了蛋白质-蛋白质复合物评分的全面评估。BioScore展示了广泛的应用性：预训练混合结构数据可提高蛋白质-蛋白质亲和力预测达40%，抗原-抗体结合相关性提高超过90%；跨系统通用性可实现零样本和少样本预测，相关性提高达71%；其统一表示可捕获化学上具有挑战性的系统，如环状肽，亲和力预测提高超过60%。BioScore为跨复杂生物分子景观的结构评估建立了稳健和通用的框架。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>BioScore解决了生物分子复合物结构评估中的关键挑战，包括数据稀疏性、跨系统表示和任务兼容性。</li>
<li>BioScore支持多种任务，如亲和力预测、构象排名和结构基础上的虚拟筛选。</li>
<li>在多个基准测试中，BioScore表现优异，优于或相当于大多数传统和深度学习方法。</li>
<li>PPI基准测试的实现为蛋白质-蛋白质复合物评分提供了全面评估。</li>
<li>BioScore具有广泛的应用性，预训练混合结构数据可显著提高预测性能。</li>
<li>BioScore实现了零样本和少样本预测，显示出其跨系统的通用性。</li>
<li>BioScore的统一表示可处理化学上挑战性的系统，如环状肽，提高了亲和力预测的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10877">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-50f9bffeae7fcd7c2cb8cd33fd95a3d4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Automated-Thematic-Analyses-Using-LLMs-Xylazine-Wound-Management-Social-Media-Chatter-Use-Case"><a href="#Automated-Thematic-Analyses-Using-LLMs-Xylazine-Wound-Management-Social-Media-Chatter-Use-Case" class="headerlink" title="Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social   Media Chatter Use Case"></a>Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social   Media Chatter Use Case</h2><p><strong>Authors:JaMor Hairston, Ritvik Ranjan, Sahithi Lakamana, Anthony Spadaro, Selen Bozkurt, Jeanmarie Perrone, Abeed Sarker</strong></p>
<p>Background Large language models (LLMs) face challenges in inductive thematic analysis, a task requiring deep interpretive and domain-specific expertise. We evaluated the feasibility of using LLMs to replicate expert-driven thematic analysis of social media data. Methods Using two temporally non-intersecting Reddit datasets on xylazine (n&#x3D;286 and n&#x3D;686, for model optimization and validation, respectively) with twelve expert-derived themes, we evaluated five LLMs against expert coding. We modeled the task as a series of binary classifications, rather than a single, multi-label classification, employing zero-, single-, and few-shot prompting strategies and measuring performance via accuracy, precision, recall, and F1-score. Results On the validation set, GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score: 0.71). For high-prevalence themes, model-derived thematic distributions closely mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use: 16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based approaches can automate thematic analyses, offering a scalable supplement for qualitative research. Keywords: thematic analysis, large language models, natural language processing, qualitative analysis, social media, prompt engineering, public health </p>
<blockquote>
<p>背景：大型语言模型（LLMs）在归纳主题分析方面面临挑战，这是一项需要深度解读和特定领域专业知识的任务。我们评估了使用LLMs复制基于专家的社交媒体数据主题分析的可行性。方法：我们使用了两组时间上没有重叠的关于安定剂（xylazine）的Reddit数据集（分别为286个和686个帖子，用于模型优化和验证），并包含专家得出的十二个主题。我们评估了五种LLMs与专家编码的对比效果。我们将任务建模为一系列二元分类，而不是单一的多标签分类，采用零样本、单样本和少样本提示策略，并通过准确性、精确度、召回率和F1分数来衡量性能。结果：在验证集上，GPT-4o的二次提示表现最佳（准确率：90.9%；F1分数：0.71）。对于高发病率主题，模型衍生的主题分布与专家分类结果非常接近（例如，安定剂使用：模型为13.6% vs 专家为17.8%；MOUD使用：模型为16.5% vs 专家为17.8%）。结论：我们的研究结果表明，基于少样本的LLM方法能够自动化主题分析，为定性研究提供了可扩展的补充。关键词：主题分析、大型语言模型、自然语言处理、定性分析、社交媒体、提示工程、公共卫生。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10803v1">PDF</a> Pages: 19, Abstract word count: 151 words, Manuscript word count:   2185 words, References: 14, Figures: 3, Tables: 2</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在归纳主题分析方面存在挑战，需要深度解读和特定领域的专业知识。研究评估了使用LLMs复制专家驱动的社会媒体数据主题分析的可行性。通过两个关于安定剂Reddit数据集，评估了五个LLMs与专家编码的性能。任务被建模为一系列二元分类，而非单一多元标签分类。采用零、单、少镜头提示策略，并通过准确度、精确度、召回率和F1分数衡量性能。结果显示，GPT-4o在两次提示下的性能最佳（准确度：90.9%；F1分数：0.71）。对于高发病率主题，模型衍生的主题分布与专家分类基本一致。研究结果表明，基于少镜头LLM的方法可以自动进行主题分析，为定性研究提供可扩展的补充。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型面临归纳主题分析的挑战，需要深度解读和特定领域的专业知识。</li>
<li>研究使用LLMs进行社会媒体数据的主题分析，并评估了其可行性。</li>
<li>通过两个关于安定剂的Reddit数据集进行模型评估。</li>
<li>任务被建模为一系列二元分类。</li>
<li>尝试了多种提示策略并评估了性能。</li>
<li>GPT-4o在两次提示下表现最佳。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10803">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-eb36a56d6e1f87b92914e5a16575a7f5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Integrating-Biological-Knowledge-for-Robust-Microscopy-Image-Profiling-on-De-Novo-Cell-Lines"><a href="#Integrating-Biological-Knowledge-for-Robust-Microscopy-Image-Profiling-on-De-Novo-Cell-Lines" class="headerlink" title="Integrating Biological Knowledge for Robust Microscopy Image Profiling   on De Novo Cell Lines"></a>Integrating Biological Knowledge for Robust Microscopy Image Profiling   on De Novo Cell Lines</h2><p><strong>Authors:Jiayuan Chen, Thai-Hoang Pham, Yuanlong Wang, Ping Zhang</strong></p>
<p>High-throughput screening techniques, such as microscopy imaging of cellular responses to genetic and chemical perturbations, play a crucial role in drug discovery and biomedical research. However, robust perturbation screening for \textit{de novo} cell lines remains challenging due to the significant morphological and biological heterogeneity across cell lines. To address this, we propose a novel framework that integrates external biological knowledge into existing pretraining strategies to enhance microscopy image profiling models. Our approach explicitly disentangles perturbation-specific and cell line-specific representations using external biological information. Specifically, we construct a knowledge graph leveraging protein interaction data from STRING and Hetionet databases to guide models toward perturbation-specific features during pretraining. Additionally, we incorporate transcriptomic features from single-cell foundation models to capture cell line-specific representations. By learning these disentangled features, our method improves the generalization of imaging models to \textit{de novo} cell lines. We evaluate our framework on the RxRx database through one-shot fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from the RxRx19a dataset. Experimental results demonstrate that our method enhances microscopy image profiling for \textit{de novo} cell lines, highlighting its effectiveness in real-world phenotype-based drug discovery applications. </p>
<blockquote>
<p>高通量筛选技术，如通过显微镜成像观察细胞对遗传和化学干扰的响应，在药物发现和生物医学研究中发挥着至关重要的作用。然而，由于细胞系之间存在显著的形态和生物异质性，对新型细胞系的稳健干扰筛选仍然是一个挑战。为解决这一问题，我们提出了一种新的框架，它将外部生物学知识整合到现有的预训练策略中，以增强显微镜图像分析模型。我们的方法利用外部生物学信息显式地分解干扰特异性和细胞系特异性表示。具体来说，我们构建了一个知识图谱，利用STRING和Hetionet数据库中的蛋白质相互作用数据来指导模型在预训练过程中关注干扰特异性特征。此外，我们还结合了单细胞基础模型的转录组特征来捕捉细胞系特异性表示。通过学习这些分离的特征，我们的方法提高了成像模型对新型细胞系的泛化能力。我们在RxRx数据库上评估了我们的框架，通过一次微调（RxRx的一个细胞系）和几次微调（RxRx十九甲基的数据集中的细胞系）进行了评估。实验结果表明，我们的方法提高了新型细胞系的显微镜图像分析效果，突显其在基于表型的药物发现应用中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10737v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>在药物发现和生物医学研究中，高通量筛选技术，如利用显微镜成像观察细胞对遗传和化学干扰的响应，发挥着至关重要的作用。然而，对于新型细胞系的稳健性干扰筛选仍具有挑战性。为此，研究团队提出了一种新型框架，该框架将外部生物学知识融入现有的预训练策略中，以提升显微镜图像分析模型的性能。该研究通过明确区分干扰特异性和细胞系特异性表征来解决这一问题，并利用蛋白质相互作用数据和单细胞转录组特征构建知识图谱来指导模型学习。实验结果表明，该方法能提高对新型细胞系的成像模型泛化能力，在基于表型的药物发现应用中具有实际应用价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>高通量筛选技术在药物发现和生物医学研究中具有关键作用。</li>
<li>对新型细胞系的稳健性干扰筛选存在挑战，因为细胞系之间存在显著的形态学和生物学异质性。</li>
<li>研究团队提出了一种结合外部生物学知识的新型框架，以提高显微镜图像分析模型的性能。</li>
<li>该框架通过明确区分干扰特异性和细胞系特异性表征来解决挑战。</li>
<li>研究利用蛋白质相互作用数据和单细胞转录组特征构建知识图谱来指导模型学习。</li>
<li>方法提高了对新型细胞系的成像模型泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10737">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d13bed7ab7b7c789068a7eccbb965f94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66eded3c4a3f59d66a13c327ac2aee11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3fd1462546a903274ddf438cd36f0f43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3455a4889fe753780e917ca0b8a4514a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd81319f3dfe06c1c6e3a32ac510fce7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Machine-learning-inference-of-stellar-properties-using-integrated-photometric-and-spectroscopic-data"><a href="#Machine-learning-inference-of-stellar-properties-using-integrated-photometric-and-spectroscopic-data" class="headerlink" title="Machine-learning inference of stellar properties using integrated   photometric and spectroscopic data"></a>Machine-learning inference of stellar properties using integrated   photometric and spectroscopic data</h2><p><strong>Authors:Ilay Kamai, Alex M. Bronstein, Hagai B. Perets</strong></p>
<p>Stellar astrophysics relies on diverse observational modalities-primarily photometric light curves and spectroscopic data-from which fundamental stellar properties are inferred. While machine learning (ML) has advanced analysis within individual modalities, the complementary information encoded across modalities remains largely underexploited. We present DESA (Dual Embedding model for Stellar Astrophysics), a novel multi-modal foundation model that integrates light curves and spectra to learn a unified, physically meaningful latent space for stars. DESA first trains separate modality-specific encoders using a hybrid supervised&#x2F;self-supervised scheme, and then aligns them through DualFormer, a transformer-based cross-modal integration module tailored for astrophysical data. DualFormer combines cross- and self-attention, a novel dual-projection alignment loss, and a projection-space eigendecomposition that yields physically structured embeddings. We demonstrate that DESA significantly outperforms leading unimodal and self-supervised baselines across a range of tasks. In zero- and few-shot settings, DESA’s learned representations recover stellar color-magnitude and Hertzsprung-Russell diagrams with high fidelity ($R^2 &#x3D; 0.92$ for photometric regressions). In full fine-tuning, DESA achieves state-of-the-art accuracy for binary star detection (AUC &#x3D; $0.99$, AP &#x3D; $1.00$) and stellar age prediction (RMSE &#x3D; $0.94$ Gyr). As a compelling case, DESA naturally separates synchronized binaries from young stars, two populations with nearly identical light curves, purely from their embedded positions in UMAP space, without requiring external kinematic or luminosity information. DESA thus offers a powerful new framework for multimodal, data-driven stellar population analysis, enabling both accurate prediction and novel discovery. </p>
<blockquote>
<p>恒星天文学依赖于多种观测方式，主要是光度光变曲线和光谱数据，从中可以推断出恒星的基本属性。虽然机器学习（ML）已经在单一模式分析方面取得了进展，但跨模式的互补信息仍然在很大程度上被忽视。我们提出了DESA（用于恒星天文学的双重嵌入模型），这是一种新型的多模式基础模型，它结合了光变曲线和光谱数据，学习一个统一、具有物理意义的恒星潜在空间。DESA首先使用混合的监督&#x2F;自监督方案训练特定的模态编码器，然后通过针对天文数据定制的基于变压器的跨模态集成模块DualFormer进行对齐。DualFormer结合了跨注意力和自注意力、新颖的双投影对齐损失以及投影空间特征分解，从而产生具有物理结构的嵌入。我们证明，DESA在一系列任务上显著优于领先的单模态和自我监督的基线。在零样本和少样本情况下，DESA学习的表示能够高度保真地恢复恒星的颜色幅度和赫罗图（$R^2 &#x3D; 0.92$用于光度回归）。在完全微调的情况下，DESA在二元恒星检测（AUC &#x3D; $0.99$，AP &#x3D; $1.00$）和恒星年龄预测（RMSE &#x3D; $0.94$吉年）方面达到了最先进水平。作为一个引人注目的案例，DESA能够自然地分离同步双星和年轻恒星这两个几乎具有相同光变曲线的群体，仅仅基于它们在UMAP空间中的嵌入位置，无需额外的运动学或光度信息。因此，DESA提供了一个强大的新框架，用于多模态、数据驱动的恒星群体分析，能够实现准确的预测和新颖的发现。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10666v1">PDF</a> submitted to ApJ</p>
<p><strong>Summary</strong></p>
<p>本文介绍了DESA模型，这是一种用于恒星天文学的多模态基础模型。DESA通过整合光曲线和光谱数据，学习了一个统一的、具有物理意义的恒星潜在空间。该模型通过混合监督&#x2F;自监督方案训练特定模态的编码器，然后通过基于变压器的跨模态集成模块DualFormer进行对齐。DESA在多种任务上显著优于领先的单模态和自我监督基线。在零样本和少样本情况下，DESA能够恢复高保真度的恒星色标图和赫罗图。在完全微调的情况下，DESA实现了二进制恒星检测和恒星年龄预测的最新准确性。DESA为多元数据驱动恒星群体分析提供了强大的新框架，既可用于准确预测，也可用于新颖发现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DESA是一个多模态模型，用于恒星天文学，整合了光曲线和光谱数据。</li>
<li>DESA通过混合监督&#x2F;自监督方案训练特定模态的编码器。</li>
<li>DESA使用基于变压器的跨模态集成模块DualFormer进行模态对齐。</li>
<li>DESA在各种任务上表现出显著性能，包括回归和分类任务。</li>
<li>在零样本和少样本情况下，DESA能够恢复高保真度的恒星色标图和赫罗图。</li>
<li>DESA实现了二进制恒星检测和恒星年龄预测的最新准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10666">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7233990e26a96403519f0c42d5bee50d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-103629f8d30dd2759efef81249cc7f93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d13a66ce77530e27337fc715c091ad2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MP1-Mean-Flow-Tames-Policy-Learning-in-1-step-for-Robotic-Manipulation"><a href="#MP1-Mean-Flow-Tames-Policy-Learning-in-1-step-for-Robotic-Manipulation" class="headerlink" title="MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation"></a>MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation</h2><p><strong>Authors:Juyi Sheng, Ziyi Wang, Peiming Li, Mengyuan Liu</strong></p>
<p>In robot manipulation, robot learning has become a prevailing approach. However, generative models within this field face a fundamental trade-off between the slow, iterative sampling of diffusion models and the architectural constraints of faster Flow-based methods, which often rely on explicit consistency losses. To address these limitations, we introduce MP1, which pairs 3D point-cloud inputs with the MeanFlow paradigm to generate action trajectories in one network function evaluation (1-NFE). By directly learning the interval-averaged velocity via the MeanFlow Identity, our policy avoids any additional consistency constraints. This formulation eliminates numerical ODE-solver errors during inference, yielding more precise trajectories. MP1 further incorporates CFG for improved trajectory controllability while retaining 1-NFE inference without reintroducing structural constraints. Because subtle scene-context variations are critical for robot learning, especially in few-shot learning, we introduce a lightweight Dispersive Loss that repels state embeddings during training, boosting generalization without slowing inference. We validate our method on the Adroit and Meta-World benchmarks, as well as in real-world scenarios. Experimental results show MP1 achieves superior average task success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its average inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster than FlowPolicy. Our code is available at <a target="_blank" rel="noopener" href="https://mp1-2254.github.io/">https://mp1-2254.github.io/</a>. </p>
<blockquote>
<p>在机器人操作领域，机器人学习已经成为一种流行的方法。然而，该领域中的生成模型面临着扩散模型的缓慢迭代采样和基于流的快速方法的结构约束之间的基本权衡，后者通常依赖于明确的一致性损失。为了解决这些局限性，我们推出了MP1，它将3D点云输入与MeanFlow范式相结合，在一个网络功能评估（1-NFE）中生成行动轨迹。通过直接学习MeanFlow Identity的间隔平均速度，我们的策略避免了任何额外的一致性约束。这种公式消除了推理过程中的数值ODE求解器错误，产生了更精确的轨迹。MP1还结合了CFG，以提高轨迹的可控性，同时保持1-NFE推理，而没有重新引入结构约束。由于微妙的场景上下文变化对机器人学习至关重要，特别是在小样本学习中，我们引入了一个轻量级的分散损失，在训练期间排斥状态嵌入，提高泛化能力而不会减慢推理速度。我们在Adroit和Meta-World基准测试以及真实世界场景中验证了我们的方法。实验结果表明，MP1达到了更高的平均任务成功率，比DP3高出10.2%，比FlowPolicy高出7.3%。其平均推理时间仅为6.8毫秒，比DP3快19倍，比FlowPolicy快近2倍。我们的代码可在<a target="_blank" rel="noopener" href="https://mp1-2254.github.io/">网站链接</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10543v1">PDF</a> </p>
<p><strong>Summary</strong><br>     该文本介绍了机器人操控领域中的机器人学习成为主流方法的情况。为解决生成模型中的扩散模型迭代采样慢与流式方法架构约束之间的权衡问题，提出了MP1方法。它通过结合3D点云输入和MeanFlow范式，在一次网络功能评估中生成动作轨迹，避免了额外的一致性约束，消除了数值ODE求解器在推理过程中的误差，从而生成更精确的轨迹。此外，MP1还结合了CFG，提高了轨迹的可控性，同时保持了1-NFE推理速度，未重新引入结构性约束。为解决机器人学习中场景上下文变化的重要性，特别是few-shot学习中的关键问题，引入了分散损失，在训练期间排斥状态嵌入，提高了泛化能力而不会减慢推理速度。实验结果表明，MP1在Adroit和Meta-World基准测试中取得了更高的任务成功率，平均推理时间也很快。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器人学习在操控领域中被广泛采用，但生成模型面临扩散模型迭代采样慢和流式方法架构约束之间的权衡问题。</li>
<li>MP1方法通过结合3D点云输入和MeanFlow范式在一次网络功能评估中生成动作轨迹，避免了额外的一致性约束。</li>
<li>MP1消除了数值ODE求解器在推理过程中的误差，生成更精确的轨迹。</li>
<li>MP1结合了CFG以提高轨迹可控性，同时保持快速推理速度，未引入新的结构性约束。</li>
<li>场景上下文变化对机器人学习至关重要，特别是few-shot学习中。</li>
<li>MP1引入了分散损失，以提高在训练期间的状态嵌入泛化能力，同时不增加推理时间。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10543">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-ee0a81db793a70d98676fbe9f6e1c35a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-946f4d7b8aa8a9fb7e653a7ef0993433.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ce9667dc018cc8b3d70e840a5a770fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac013d3879bb83ca5d9f95509bdd33e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a0ddc44273978c2f8c4d3d25c9c06e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ef031bbe27112ed658fd61df6672fe0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Graph-World-Model"><a href="#Graph-World-Model" class="headerlink" title="Graph World Model"></a>Graph World Model</h2><p><strong>Authors:Tao Feng, Yexin Wu, Guanyu Lin, Jiaxuan You</strong></p>
<p>World models (WMs) demonstrate strong capabilities in prediction, generation, and planning tasks. Existing WMs primarily focus on unstructured data and cannot leverage the ubiquitous structured data, often represented as graphs, in the digital world. While multiple graph foundation models have been proposed, they focus on graph learning tasks and cannot extend to diverse multi-modal data and interdisciplinary tasks. To address these challenges, we propose the Graph World Model (GWM), a world model that supports both unstructured and graph-structured states with multi-modal information and represents diverse tasks as actions. The core of a GWM is a generic message-passing algorithm to aggregate structured information, either over a unified multi-modal token space by converting multi-modal data into text (GWM-T) or a unified multi-modal embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces action nodes to support diverse tasks, where action nodes are linked to other nodes via direct reference or similarity computation. Extensive experiments on six tasks from diverse domains, including multi-modal generation and matching, recommendation, graph prediction, multi-agent, retrieval-augmented generation, and planning and optimization, show that the same GWM outperforms or matches domain-specific baselines’ performance, benefits from multi-hop structures, and demonstrates strong zero-shot&#x2F;few-shot capabilities on unseen new tasks. Our code for GWM is released at <a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/GWM">https://github.com/ulab-uiuc/GWM</a>. </p>
<blockquote>
<p>世界模型（WMs）在预测、生成和规划任务中展现出强大的能力。现有的WMs主要关注非结构化数据，无法利用普遍存在的结构化数据（通常表示为图）在数字世界中的价值。虽然已提出多个图基础模型，但它们主要关注图学习任务，无法扩展到多样化的多模态数据和跨学科任务。为了应对这些挑战，我们提出了图世界模型（GWM），这是一种支持非结构化图和结构化状态的多模态信息的世界模型，它将各种任务表示为动作。GWM的核心是一个通用消息传递算法，用于聚合结构化信息，无论是在统一的多模态符号空间（通过将多模态数据转换为文本）还是统一的多模态嵌入空间（通过特定模态的编码器）。值得注意的是，GWM引入了动作节点来支持各种任务，动作节点通过直接引用或相似性计算与其他节点链接。在来自不同领域的六个任务上的大量实验表明，同一GWM超越了或匹配了领域特定的基线性能，受益于多跳结构，并在未见的新任务上表现出强大的零样本&#x2F;小样本能力。我们的GWM代码已发布在<a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/GWM%E3%80%82">https://github.com/ulab-uiuc/GWM。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10539v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>世界模型（WMs）在预测、生成和规划任务中表现出强大的能力，但它们主要关注于非结构化数据，无法利用普遍存在的结构化数据（通常以图形表示）。为解决此挑战，提出了图世界模型（GWM），它支持非结构化与图形结构化状态，具备多模式信息，并将各种任务表示为行动。GWM的核心是通用消息传递算法，该算法可通过将多模式数据转换为文本（GWM-T）或在模态特定编码器上的统一多模式嵌入空间（GWM-E）来聚合结构化信息。GWM引入行动节点以支持各种任务，行动节点通过直接引用或相似性计算与其他节点链接。在来自不同领域的六个任务上的实验表明，同一GWM优于或匹配领域特定基准线的性能，受益于多跳结构，并在未见的新任务上表现出强大的零样本&#x2F;小样本能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>世界模型（WMs）具有强大的预测、生成和规划能力，但主要处理非结构化数据。</li>
<li>图世界模型（GWM）支持非结构化和图形结构化数据，并处理多模式信息。</li>
<li>GWM通过通用消息传递算法聚合结构化信息，提供两种实现方式：GWM-T（通过文本转换）和GWM-E（通过模态特定编码器）。</li>
<li>GWM引入行动节点以支持多种任务，这些行动节点与其他节点通过直接引用或相似性计算链接。</li>
<li>GWM在多个领域任务上表现出强大的性能，包括多模式生成与匹配、推荐、图形预测、多智能体、检索增强生成、规划和优化。</li>
<li>GWM具有强大的零样本&#x2F;小样本学习能力，能够在未见的新任务上表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10539">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-6f2068d863c19b175333e6383f0cc4e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b968038a3139062b53b5ec99c27139b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e77798f7a4dc2cff6cf149c9517355d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-270049d6a03e8b6b83728b05c101bc28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25783fdf45ad707b52bfde0f775336fd.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Domain-Borders-Are-There-to-Be-Crossed-With-Federated-Few-Shot-Adaptation"><a href="#Domain-Borders-Are-There-to-Be-Crossed-With-Federated-Few-Shot-Adaptation" class="headerlink" title="Domain Borders Are There to Be Crossed With Federated Few-Shot   Adaptation"></a>Domain Borders Are There to Be Crossed With Federated Few-Shot   Adaptation</h2><p><strong>Authors:Manuel Röder, Christoph Raab, Frank-Michael Schleif</strong></p>
<p>Federated Learning has emerged as a leading paradigm for decentralized, privacy-preserving learning, particularly relevant in the era of interconnected edge devices equipped with sensors. However, the practical implementation of Federated Learning faces three primary challenges: the need for human involvement in costly data labelling processes for target adaptation, covariate shift in client device data collection due to environmental factors affecting sensors, leading to discrepancies between source and target samples, and the impracticality of continuous or regular model updates in resource-constrained environments due to limited data transmission capabilities and technical constraints on channel availability and energy efficiency. To tackle these issues, we expand upon an efficient and scalable Federated Learning framework tailored for real-world client adaptation in industrial settings. This framework leverages a pre-trained source model comprising a deep backbone, an adaptation module, and a classifier running on a powerful server. By freezing the backbone and classifier during client adaptation on resource-constrained devices, we allow the domain adaptive linear layer to handle target domain adaptation, thus minimizing overall computational overhead. Furthermore, this setup, designated as FedAcross+, is extended to encompass the processing of streaming data, thereby rendering the solution suitable for non-stationary environments. Extensive experimental results demonstrate the effectiveness of FedAcross+ in achieving competitive adaptation on low-end client devices with limited target samples, successfully addressing the challenge of domain shift. Moreover, our framework accommodates sporadic model updates within resource-constrained environments, ensuring practical and seamless deployment. </p>
<blockquote>
<p>联邦学习已成为去中心化、保护隐私的学习的主要范式，特别是在配备传感器的互联边缘设备时代。然而，联邦学习的实际应用面临三大挑战：需要人类参与昂贵的数据标记过程以适应目标，由于影响传感器的环境因素导致客户端设备数据收集中的协变量偏移，从而导致源样本和目标样本之间的差异，以及由于数据传输能力有限和通道可用性和能效方面的技术约束，资源受限环境中连续或定期模型更新的不切实际性。为了解决这些问题，我们扩展了一个高效且可扩展的联邦学习框架，该框架旨在适应工业环境中的真实世界客户端。该框架利用预训练的源模型，包括深度主干、适配模块和分类器，在功能强大的服务器上运行。通过在客户端适应资源受限的设备时冻结主干和分类器，我们允许域自适应线性层处理目标域适应，从而最小化总体计算开销。此外，这个被称为FedAcross+的设置被扩展以涵盖流式数据的处理，从而使解决方案适合非静止环境。大量的实验结果表明，FedAcross+在低端客户端设备上实现具有竞争力的适应目标样本方面非常有效，成功解决了领域偏移的挑战。而且，我们的框架能够在资源受限的环境中容纳间歇性模型更新，确保实用且无缝的部署。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10160v1">PDF</a> Extension of <a target="_blank" rel="noopener" href="http://dx.doi.org/10.5220/0012351900003654">http://dx.doi.org/10.5220/0012351900003654</a></p>
<p><strong>Summary</strong><br>     联邦学习已成为去中心化、保护隐私的学习主流范式，尤其在配备传感器的互联边缘设备时代具有重要意义。然而，其实践面临三大挑战。为应对这些挑战，我们扩展了一个高效、可扩展的联邦学习框架，用于工业环境中的真实客户端适应。该框架利用预训练源模型，包括深度主干、适配模块和分类器在强大服务器上运行。通过冻结主干和分类器，在资源受限的设备上进行客户端适配，允许领域自适应线性层处理目标领域适配，从而最小化整体计算开销。此外，FedAcross+ 框架能够处理流式数据，使其适用于非静态环境。实验结果证明了 FedAcross+ 在具有有限目标样本的低端客户端设备上实现竞争适配的有效性，成功解决了领域偏移的挑战。同时，该框架支持资源受限环境中偶尔的模型更新，确保实际部署的实用性和无缝性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>联邦学习已成为去中心化、隐私保护的主流学习范式，尤其在边缘设备时代具有重要意义。</li>
<li>联邦学习的实际应用面临三大挑战：目标适应的数据标注成本、源与靶样本之间的环境差异导致的协变量偏移以及资源受限环境中的连续或定期模型更新的不实用性。</li>
<li>提出了一个针对工业环境中真实客户端适应的高效联邦学习框架——FedAcross+，该框架利用预训练源模型并冻结主干和分类器，以降低计算开销，并实现目标领域适配。</li>
<li>FedAcross+ 能够处理流式数据，适应非静态环境。</li>
<li>实验结果表明，FedAcross+ 在具有有限目标样本的低端客户端设备上实现有效的竞争适配，成功解决领域偏移挑战。</li>
<li>FedAcross+ 框架支持资源受限环境中的偶尔模型更新，确保实际部署的实用性和无缝性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10160">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-85387202a797c5b225fce31a3c99c78c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d16da07d7c3941889995d95245b5f54d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8082bff2bd19e4a0a63a3f075fcdf594.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DEARLi-Decoupled-Enhancement-of-Recognition-and-Localization-for-Semi-supervised-Panoptic-Segmentation"><a href="#DEARLi-Decoupled-Enhancement-of-Recognition-and-Localization-for-Semi-supervised-Panoptic-Segmentation" class="headerlink" title="DEARLi: Decoupled Enhancement of Recognition and Localization for   Semi-supervised Panoptic Segmentation"></a>DEARLi: Decoupled Enhancement of Recognition and Localization for   Semi-supervised Panoptic Segmentation</h2><p><strong>Authors:Ivan Martinović, Josip Šarić, Marin Oršić, Matej Kristan, Siniša Šegvić</strong></p>
<p>Pixel-level annotation is expensive and time-consuming. Semi-supervised segmentation methods address this challenge by learning models on few labeled images alongside a large corpus of unlabeled images. Although foundation models could further account for label scarcity, effective mechanisms for their exploitation remain underexplored. We address this by devising a novel semi-supervised panoptic approach fueled by two dedicated foundation models. We enhance recognition by complementing unsupervised mask-transformer consistency with zero-shot classification of CLIP features. We enhance localization by class-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting decoupled enhancement of recognition and localization (DEARLi) particularly excels in the most challenging semi-supervised scenarios with large taxonomies and limited labeled data. Moreover, DEARLi outperforms the state of the art in semi-supervised semantic segmentation by a large margin while requiring 8x less GPU memory, in spite of being trained only for the panoptic objective. We observe 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/helen1c/DEARLi">https://github.com/helen1c/DEARLi</a>. </p>
<blockquote>
<p>像素级标注既昂贵又耗时。半监督分割方法通过少数标注图像和大量未标注图像数据集上的模型学习来解决这一挑战。虽然基础模型可以进一步解决标签稀缺问题，但对其利用的有效机制仍被忽视。我们通过开发一种新的半监督泛全景方法来解决这个问题，该方法由两个专用的基础模型推动。我们通过将无监督掩膜转换器的一致性补充与CLIP特征的零样本分类相结合，提高识别能力。通过关于SAM伪标签的类别无关解码器预热来提高定位能力。这种去耦的识别和定位增强（DEARLi）技术在标签数据有限的大型分类学中面临最具挑战性的半监督场景时尤其表现出色。此外，尽管DEARLi仅针对全景目标进行训练，但在半监督语义分割方面仍大大优于现有技术，同时需要使用的GPU内存减少了8倍。在ADE20K上，仅使用158个标注图像就实现了29.9的PQ和38.9的mIoU。源代码可在<a target="_blank" rel="noopener" href="https://github.com/helen1c/DEARLi%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/helen1c/DEARLi找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10118v1">PDF</a> ICCV 2025 Findings Workshop</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于半监督学习方法的像素级标注问题解决方案。该方法结合了两种专门的模型进行训练，使用半监督全景分析技术和一系列特定增强手段提高识别与定位能力。该方法在具有大量分类和有限标签数据的挑战性半监督场景中表现优异，显著优于现有技术，同时减少了GPU内存需求。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该方法采用半监督学习方法解决了像素级标注的昂贵和耗时问题。</li>
<li>通过结合两种专门的模型进行训练，提高了模型的性能。</li>
<li>使用半监督全景分析技术增强了识别能力。</li>
<li>通过特定增强手段提高模型的定位能力。</li>
<li>该方法在具有挑战性的半监督场景中表现优异，尤其是那些涉及大量分类和有限标签数据的场景。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10118">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-30c85dec95bfedacc4421be7ef6bba1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-979a0aba0d44ce4779e52797edc2aa13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df399fd115c07e260f7908d5f08cbff1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97904fb6e81c78fdc0e3878fc7be2a62.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f418d4ff981ebc2ba76d54be87328cae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8856e45de90c983eb08628bbacfa8419.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Automating-SPARQL-Query-Translations-between-DBpedia-and-Wikidata"><a href="#Automating-SPARQL-Query-Translations-between-DBpedia-and-Wikidata" class="headerlink" title="Automating SPARQL Query Translations between DBpedia and Wikidata"></a>Automating SPARQL Query Translations between DBpedia and Wikidata</h2><p><strong>Authors:Malte Christian Bartels, Debayan Banerjee, Ricardo Usbeck</strong></p>
<p>This paper investigates whether state-of-the-art Large Language Models (LLMs) can automatically translate SPARQL between popular Knowledge Graph (KG) schemas. We focus on translations between the DBpedia and Wikidata KG, and later on DBLP and OpenAlex KG. This study addresses a notable gap in KG interoperability research by rigorously evaluating LLM performance on SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100 DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and Mistral-Large-Instruct-2407 are selected based on their sizes and architectures and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs were compared with gold answers, and resulting errors were categorized. We find that the performance varies markedly across models and prompting strategies, and that translations for Wikidata to DBpedia work far better than translations for DBpedia to Wikidata. </p>
<blockquote>
<p>本文旨在研究最先进的自然语言大模型是否能够自动翻译知识图谱（KG）之间流行的SPARQL查询。我们重点关注DBpedia和Wikidata KG之间的翻译，后续研究还包括DBLP和OpenAlex KG之间的翻译。本研究通过严格评估LLM在SPARQL-to-SPARQL翻译方面的性能，解决了知识图谱互操作性研究中的一个显著空白。我们构建了两个基准测试集，第一个基准测试集对齐QALD-9-Plus中的100个DBpedia-Wikidata查询；第二个基准测试集包含对齐到OpenAlex的100个DBLP查询，以测试其在百科全书知识图谱之外的泛化能力。基于规模与架构，我们选择三个开源的大型语言模型：Llama-3-8B、DeepSeek-R1-Distill-Llama-70B以及Mistral-Large-Instruct-2407进行试验。试验包括零样本、少样本以及两种思维链方法。输出答案与标准答案进行比较，并对产生的错误进行分类。我们发现不同模型和提示策略之间的性能差异很大，并且从Wikidata到DBpedia的翻译效果远胜于从DBpedia到Wikidata的翻译。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10045v1">PDF</a> 18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference   happening on September 2025</p>
<p><strong>Summary</strong></p>
<p>这篇论文探讨了最先进的自然语言大模型（LLM）是否能自动翻译知识图谱（KG）中的SPARQL查询语句。研究聚焦于DBpedia和Wikidata以及DBLp和OpenAlex两大知识图谱之间的翻译，填补了知识图谱互操作性研究的空白。论文构建了两个基准测试集，评估了三个开源大模型在不同翻译场景下的性能，并发现性能在不同模型和提示策略之间有很大差异，且从Wikidata到DBpedia的翻译效果优于从DBpedia到Wikidata的翻译。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文探讨了使用大语言模型（LLM）自动翻译SPARQL查询语句的可行性。</li>
<li>研究聚焦于DBpedia和Wikidata以及DBLp和OpenAlex两大知识图谱之间的翻译。</li>
<li>论文构建了两个基准测试集，用于评估LLM在SPARQL-to-SPARQL翻译方面的性能。</li>
<li>评估了三个开源大模型，包括Llama-3-8B、DeepSeek-R1-Distill-Llama-70B和Mistral-Large-Instruct-2407。</li>
<li>论文采用了零样本、小样例以及两种思维链方法进行了测试。</li>
<li>性能在不同模型和提示策略之间有很大差异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10045">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e3e97eadff6b6b4eeafde66a265d141c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8ae43574719591c6147893c1bc57fd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a785e66a25e99e28342f236f4fb2468.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Deep-Hidden-Cognition-Facilitates-Reliable-Chain-of-Thought-Reasoning"><a href="#Deep-Hidden-Cognition-Facilitates-Reliable-Chain-of-Thought-Reasoning" class="headerlink" title="Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning"></a>Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning</h2><p><strong>Authors:Zijun Chen, Wenbo Hu, Richang Hong</strong></p>
<p>Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning capabilities in both large language models (LLMs) and multimodal large language models (MLLMs). However, its reliability is often undermined by the accumulation of errors in intermediate steps. This paper introduces an novel approach to calibrate the CoT reasoning accuracy by leveraging the model’s intrinsic veracity encoding. We discover that specific attention head activations reliably reflect the truthfulness of reasoning steps in CoT. Based on this insight, we train a confidence predictor to evaluate the correctness of each reasoning step using these truthfulness-sensitive activations, dynamically selecting the most plausible reasoning path via beam search. Experimental results demonstrate that our method significantly outperforms the state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and commonsense reasoning tasks, exhibiting superior accuracy and reliability in both unimodal and multimodal settings. We further validate the approach on large reasoning models, confirming its applicability to specialized reasoning models. Additionally, we explore the role of the model’s self-correction ability in CoT reasoning. This work provides a novel reliability improvement path for CoT reasoning with broad application potential. </p>
<blockquote>
<p>链式思维（Chain of Thought，简称CoT）推理在大规模语言模型（LLM）和多模态大规模语言模型（MLLM）中都表现出了显著的深度推理能力。然而，其可靠性往往受到中间步骤误差累积的影响。本文介绍了一种利用模型内在真实性编码来校准CoT推理精度的新方法。我们发现特定的注意力头激活能够可靠地反映CoT中推理步骤的真实性。基于这一发现，我们训练了一个置信度预测器，利用这些真实性敏感的激活来评估每个推理步骤的正确性，并通过集束搜索动态选择最合理的推理路径。实验结果表明，我们的方法在数学、符号和常识推理任务上显著优于最先进的基线方法（如少样本CoT、自我一致性、自我评估引导集束搜索等），在单模态和多模态环境中表现出更高的准确性和可靠性。我们在大型推理模型上进一步验证了该方法，证明了它在专业推理模型中的适用性。此外，我们还探索了模型中自我校正能力在CoT推理中的作用。这项工作为CoT推理提供了一种新的可靠性提升路径，具有广泛的应用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10007v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了利用模型内在真实性编码来提升Chain of Thought（CoT）推理准确性的新方法。研究发现了特定注意力头激活能可靠反映CoT推理步骤的真实性，并基于这一发现训练了信心预测器来评估每个推理步骤的正确性。该方法通过动态选择最合理的推理路径，显著提高了数学、符号和常识推理任务的性能，并在单模态和多模态环境中表现出卓越准确性和可靠性。同时，文章还探讨了模型在CoT推理中的自我校正能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入新方法校准CoT推理准确性，通过模型内在真实性编码提升性能。</li>
<li>发现特定注意力头激活能反映CoT推理步骤的真实性。</li>
<li>训练信心预测器评估每个推理步骤的正确性。</li>
<li>方法在多种推理任务上显著优于现有技术，适用于单模态和多模态环境。</li>
<li>方法提高了数学、符号和常识推理的准确性和可靠性。</li>
<li>探索了模型在CoT推理中的自我校正能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10007">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f1545fd35875226550fb557f65be080a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d946da7c9c2b363089e0e5b9b7141f1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-677b408c1a7ca9c66ce9f7a62ad815e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47989b3b81d7934f27fa4f0411cbc085.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Iceberg-Enhancing-HLS-Modeling-with-Synthetic-Data"><a href="#Iceberg-Enhancing-HLS-Modeling-with-Synthetic-Data" class="headerlink" title="Iceberg: Enhancing HLS Modeling with Synthetic Data"></a>Iceberg: Enhancing HLS Modeling with Synthetic Data</h2><p><strong>Authors:Zijian Ding, Tung Nguyen, Weikai Li, Aditya Grover, Yizhou Sun, Jason Cong</strong></p>
<p>Deep learning-based prediction models for High-Level Synthesis (HLS) of hardware designs often struggle to generalize. In this paper, we study how to close the generalizability gap of these models through pretraining on synthetic data and introduce Iceberg, a synthetic data augmentation approach that expands both large language model (LLM)-generated programs and weak labels of unseen design configurations. Our weak label generation method is integrated with an in-context model architecture, enabling meta-learning from actual and proximate labels. Iceberg improves the geometric mean modeling accuracy by $86.4%$ when adapt to six real-world applications with few-shot examples and achieves a $2.47\times$ and a $1.12\times$ better offline DSE performance when adapting to two different test datasets. Our open-sourced code is here: \href{<a target="_blank" rel="noopener" href="https://github.com/UCLA-VAST/iceberg%7D%7Bhttps://github.com/UCLA-VAST/iceberg%7D">https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}</a> </p>
<blockquote>
<p>针对硬件设计的高级综合（HLS）深度学习的预测模型通常面临难以泛化的问题。在本文中，我们研究了如何通过合成数据的预训练来缩小这些模型的泛化差距，并引入了Iceberg，这是一种合成数据增强方法，它扩展了大规模语言模型（LLM）生成的程序和未见设计配置的弱标签。我们的弱标签生成方法与上下文模型架构相结合，实现了从实际和近似标签的元学习。Iceberg在适应六个具有少量示例的实际应用时，提高了几何平均建模精度达86.4%，在适应两个不同的测试数据集时，离线DSE性能分别提高了2.47倍和1.12倍。我们的开源代码在此：<a target="_blank" rel="noopener" href="https://github.com/UCLA-VAST/iceberg">https://github.com/UCLA-VAST/iceberg</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09948v1">PDF</a> 9 pages. accepted to ICLAD’25</p>
<p><strong>Summary</strong><br>深度学习在硬件设计高级综合（HLS）的预测模型通常存在泛化问题。本研究通过预训练合成数据来缩小模型泛化差距，并引入Iceberg方法，扩展了大型语言模型生成的程序和未见设计配置的弱标签。集成弱标签生成方法与上下文模型架构，实现元学习从实际和近似标签中学习。Iceberg在适应少数实例的真实世界应用时，建模精度提高了86.4%，在适应两个不同的测试数据集时，离线DSE性能分别提高了2.47倍和1.12倍。代码已开源。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习在硬件设计高级综合的预测模型存在泛化问题。</li>
<li>通过预训练合成数据来提高模型的泛化能力。</li>
<li>引入Iceberg方法，扩展大型语言模型生成的程序和未见设计配置的弱标签。</li>
<li>集成弱标签生成方法与上下文模型架构，实现元学习。</li>
<li>Iceberg方法在适应少数实例的真实世界应用时，建模精度显著提高。</li>
<li>Iceberg方法提高了离线DSE性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09948">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1836b5ab77c82c73e7bed494e1005a4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d772916e0e70bde9cdf393e3f2ca304.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d023eef157c43c1c3b9a4872f0c63a6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22321854b0db23c5ad4ed0dbc468e4b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a75d5f089d2029aa0fedca806359fc84.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91666131b45024402b03ea13ba19960b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f9ed1e753b7172fc14597ae7eec8104.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aade31a37f027a5e4795578ed7de486.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ViT-ProtoNet-for-Few-Shot-Image-Classification-A-Multi-Benchmark-Evaluation"><a href="#ViT-ProtoNet-for-Few-Shot-Image-Classification-A-Multi-Benchmark-Evaluation" class="headerlink" title="ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark   Evaluation"></a>ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark   Evaluation</h2><p><strong>Authors:Abdulvahap Mutlu, Şengül Doğan, Türker Tuncer</strong></p>
<p>The remarkable representational power of Vision Transformers (ViTs) remains underutilized in few-shot image classification. In this work, we introduce ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical Network framework. By averaging class conditional token embeddings from a handful of support examples, ViT-ProtoNet constructs robust prototypes that generalize to novel categories under 5-shot settings. We conduct an extensive empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100, CUB-200, and CIFAR-FS, including overlapped support variants to assess robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based prototypical counterparts, achieving up to a 3.2% improvement in 5-shot accuracy and demonstrating superior feature separability in latent space. Furthermore, it outperforms or is competitive with transformer-based competitors using a more lightweight backbone. Comprehensive ablations examine the impact of transformer depth, patch size, and fine-tuning strategy. To foster reproducibility, we release code and pretrained weights. Our results establish ViT-ProtoNet as a powerful, flexible approach for few-shot classification and set a new baseline for transformer-based meta-learners. </p>
<blockquote>
<p>视觉Transformer（ViT）的表示能力在少样本图像分类中尚未得到充分利用。在这项工作中，我们引入了ViT-ProtoNet，它将ViT-Small骨干网集成到原型网络框架中。通过平均少量支持样本的类别条件令牌嵌入，ViT-ProtoNet构建了在5个样本设置下能够推广到新型类别的稳健原型。我们在四个标准数据集上进行了广泛的实证评估：Mini-ImageNet、FC100、CUB-200和CIFAR-FS，包括重叠的支持变体以评估稳健性。在所有分割中，ViT-ProtoNet始终优于基于CNN的原型对应模型，在5次射击的准确性上最多提高了3.2%，并在潜在空间中显示出卓越的特征可分离性。此外，使用更轻量级骨干网的情况下，它与基于Transformer的竞争对手相比具有优势或竞争力。全面的研究考察了Transformer深度、补丁大小和微调策略的影响。为促进可重复性，我们发布了代码和预训练权重。我们的结果确立了ViT-ProtoNet在少样本分类中的强大和灵活方法，并为基于Transformer的元学习者设定了新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09299v1">PDF</a> All codes are available at   <a target="_blank" rel="noopener" href="https://github.com/abdulvahapmutlu/vit-protonet">https://github.com/abdulvahapmutlu/vit-protonet</a></p>
<p><strong>Summary</strong><br>     本文介绍了将Vision Transformer（ViT）与Prototypical Network框架相结合的ViT-ProtoNet模型，用于小样本图像分类。通过平均少量支持样本的类别条件令牌嵌入来构建稳健原型，ViT-ProtoNet在5个样本的情况下可推广到新型类别。在四个标准数据集上的实验结果表明，ViT-ProtoNet在基于CNN的原型模型上实现了高达3.2%的改进，并在潜在空间中表现出卓越的特征可分离性。此外，使用更轻量级骨干的基于Transformer的竞争对手相比，ViT-ProtoNet具有出色的性能。代码和预训练权重已发布，以促进可重复性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer（ViT）在小样本图像分类中的代表性能力尚未得到充分利用。</li>
<li>ViT-ProtoNet结合了ViT-Small骨干网络和Prototypical Network框架，用于小样本图像分类。</li>
<li>通过平均少量支持样本的类别条件令牌嵌入，ViT-ProtoNet构建了稳健的原型。</li>
<li>在四个标准数据集上的实验表明，ViT-ProtoNet在5个样本的情况下表现优异，且在潜在空间中具有卓越的特征可分离性。</li>
<li>ViT-ProtoNet在基于CNN的原型模型上实现了显著改进，最高达到3.2%。</li>
<li>与其他基于Transformer的竞争对手相比，使用更轻量级骨干的ViT-ProtoNet表现优秀。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09299">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4d0610ed74405bb286dd1346ceae6d14.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Revisiting-Pool-based-Prompt-Learning-for-Few-shot-Class-incremental-Learning"><a href="#Revisiting-Pool-based-Prompt-Learning-for-Few-shot-Class-incremental-Learning" class="headerlink" title="Revisiting Pool-based Prompt Learning for Few-shot Class-incremental   Learning"></a>Revisiting Pool-based Prompt Learning for Few-shot Class-incremental   Learning</h2><p><strong>Authors:Yongwei Jiang, Yixiong Zou, Yuhua Li, Ruixuan Li</strong></p>
<p>Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data scarcity and incremental learning in real-world scenarios. While pool-based prompting methods have demonstrated success in traditional incremental learning, their effectiveness in FSCIL settings remains unexplored. This paper presents the first study of current prompt pool methods in FSCIL tasks, revealing an unanticipated performance degradation in incremental sessions. Through comprehensive analysis, we identify that this phenomenon stems from token-dimension saturation: with limited data, excessive prompts compete for task-relevant information, leading to model overfitting. Based on this finding, we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively shifts pool-based prompt learning from the token dimension to the spatial dimension. LGSP-Prompt generates spatial prompts by synergistically combining local spatial features and global frequency-domain representations to highlight key patterns in input images. We construct two spatial prompt pools enabling dynamic prompt selection to maintain acquired knowledge while effectively learning novel sessions. Extensive experiments demonstrate that our approach achieves state-of-the-art performance across multiple FSCIL benchmarks, showing significant advantages in both base knowledge preservation and incremental learning. Our implementation is available at <a target="_blank" rel="noopener" href="https://github.com/Jywsuperman/LGSP">https://github.com/Jywsuperman/LGSP</a>. </p>
<blockquote>
<p>在真实场景中，Few-Shot 类增量学习（FSCIL）面临着数据稀缺和增量学习的双重挑战。虽然基于池体的提示方法在传统的增量学习中已经取得了成功，但它们在FSCIL设置中的有效性尚未被探索。本文首次研究了当前提示池方法在FSCIL任务中的应用，揭示了增量会话中预期之外的性能下降。通过综合分析，我们发现这一现象源于token维度的饱和：在有限的数据下，过多的提示会争夺任务相关信息，导致模型过度拟合。基于这一发现，我们提出了LGSP-Prompt（局部-全局空间提示），它创新地将基于池体的提示学习从token维度转移到空间维度。LGSP-Prompt通过协同结合局部空间特征和全局频域表示来生成空间提示，以突出输入图像中的关键模式。我们构建了两个空间提示池，以支持动态提示选择，以维持已获取的知识并有效地学习新的会话。大量实验表明，我们的方法在多个FSCIL基准测试中达到了最先进的性能，在基础知识的保留和增量学习方面都显示出显著的优势。我们的实现可在<a target="_blank" rel="noopener" href="https://github.com/Jywsuperman/LGSP">https://github.com/Jywsuperman/LGSP</a>中找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09183v1">PDF</a> Accepted to ICCV 2025, 11 pages</p>
<p><strong>摘要</strong></p>
<p>该论文针对小样本类别增量学习（FSCIL）领域数据稀缺和增量学习双重挑战进行研究。虽然基于提示池的方法在传统增量学习中取得了成功，但在FSCIL场景中的有效性尚未得到探索。本文首次研究了当前提示池方法在FSCIL任务中的表现，发现增量会话中存在意外的性能下降现象。经过综合分析，我们确定这是由于标记维度饱和导致的：在有限数据下，过多的提示会争夺任务相关信息，导致模型过拟合。基于此发现，我们提出了LGSP-Prompt（局部全局空间提示），创新地将基于提示池的学习从标记维度转向空间维度。LGSP-Prompt通过协同结合局部空间特征和全局频域表示来生成空间提示，从而突出输入图像中的关键模式。我们构建了两个空间提示池，以实现动态提示选择，既保持已获得的知识，又能有效学习新的会话。大量实验表明，我们的方法在多个FSCIL基准测试中达到了最新性能水平，在基础知识保留和增量学习方面都显示出显著优势。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>FSCIL面临数据稀缺和增量学习的双重挑战。</li>
<li>基于提示池的方法在FSCIL任务中表现出意外的性能下降现象。</li>
<li>现象的原因在于标记维度饱和，过量提示导致模型过拟合。</li>
<li>引入LGSP-Prompt方法，将基于提示池的学习从标记维度转向空间维度。</li>
<li>LGSP-Prompt结合局部空间特征和全局频域表示生成空间提示。</li>
<li>构建了两个空间提示池以实现动态提示选择，同时保持知识和学习新会话。</li>
<li>实验表明，该方法在多个FSCIL基准测试中表现最佳，具有显著的保留和增量学习优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09183">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bbeece35ec0a6de2fe73abeb60007f7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0b0e96caedfe38e7868482d51bb06f9d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3aebbe56ecbc3ff22685a3950a379abb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98dba234c37c026e7415c7d11f0259b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc16fab3a36925a28901b51fd325c132.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Beyond-Scale-Small-Language-Models-are-Comparable-to-GPT-4-in-Mental-Health-Understanding"><a href="#Beyond-Scale-Small-Language-Models-are-Comparable-to-GPT-4-in-Mental-Health-Understanding" class="headerlink" title="Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental   Health Understanding"></a>Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental   Health Understanding</h2><p><strong>Authors:Hong Jia, Shiya Fu, Feng Xia, Vassilis Kostakos, Ting Dang</strong></p>
<p>The emergence of Small Language Models (SLMs) as privacy-preserving alternatives for sensitive applications raises a fundamental question about their inherent understanding capabilities compared to Large Language Models (LLMs). This paper investigates the mental health understanding capabilities of current SLMs through systematic evaluation across diverse classification tasks. Employing zero-shot and few-shot learning paradigms, we benchmark their performance against established LLM baselines to elucidate their relative strengths and limitations in this critical domain. We assess five state-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against three LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding tasks. Our findings reveal that SLMs achieve mean performance within 2% of LLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot settings), demonstrating notable competence despite orders of magnitude fewer parameters. Both model categories experience similar degradation on multi-class severity tasks (a drop of over 30%), suggesting that nuanced clinical understanding challenges transcend model scale. Few-shot prompting provides substantial improvements for SLMs (up to 14.6%), while LLM gains are more variable. Our work highlights the potential of SLMs in mental health understanding, showing they can be effective privacy-preserving tools for analyzing sensitive online text data. In particular, their ability to quickly adapt and specialize with minimal data through few-shot learning positions them as promising candidates for scalable mental health screening tools. </p>
<blockquote>
<p>随着小型语言模型（SLMs）作为敏感应用的隐私保护替代方案的出现，关于它们与大型语言模型（LLMs）的内在理解能力的比较，引发了一个根本性的问题。本文通过系统评估各种分类任务，研究当前SLM在心理健康理解方面的能力。我们采用零样本学习和小样本学习范式，将它们的性能与现有的LLM基准进行测试，以阐明这一关键领域中的相对优势和局限性。我们对五个最先进的大型语言模型进行评估，分别是Phi-3、Phi-3.5、Qwen2.5、Llama-3.2和Gemma2等，以及三个大型语言模型GPT-4、FLAN-T5-XXL和Alpaca-7B等六个心理健康理解任务。我们的研究发现，在二元分类任务上，SLM的均值与LLM在短短两个百分点以内达到相对水平（零样本设置中F1分数为0.64对0.66），虽然参数相差多个数量级，但也表现出相当的可信能力。两种模型在多类严重性任务上的性能均出现类似下降（降幅超过百分之三十），这表明细微的临床理解挑战已超越了模型的规模范围。小样本提示（即让机器学习系统的操作显示一些小量样例的能力）可为SLM带来巨大改善（高达百分之十四点六），而大型语言模型的收益则更为不稳定。我们的工作重点突出了SLM在心理健康理解方面的潜力，显示出它们可以成为分析敏感在线文本数据的有效隐私保护工具。特别是它们通过小样本学习快速适应和专门化的能力，使它们成为心理健康筛查工具的可选方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08031v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>当前文本探讨了小型语言模型（SLMs）在心理健康理解方面的能力，通过与大型语言模型（LLMs）的对比实验，展示了SLMs在此领域的潜力和优势。研究发现，SLMs在二元分类任务上的表现与LLMs相近，少数镜头学习技术的引入为SLMs带来了显著的性能提升。尽管存在多类别严重程度任务上的挑战，但SLMs展现了在心理健康理解领域的有效性和潜力。总体来说，SLMs可以作为隐私保护工具应用于敏感文本数据的分析，尤其在快速适应和专业化方面展现出巨大潜力，有望用于可扩展的心理健康筛查工具。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SLMs作为隐私保护替代方案在敏感应用中的兴起，引发了对它们与LLMs在理解能力方面的比较。</li>
<li>通过多种分类任务的系统评估，发现SLMs在心理健康理解方面表现出竞争力。</li>
<li>在二元分类任务上，SLMs的性能与LLMs相近，少数镜头学习技术显著提升SLMs性能。</li>
<li>在多类别严重程度任务上，两种模型均面临挑战，显示微妙的临床理解难度超越模型规模。</li>
<li>SLMs在心理健康理解领域具有潜力和有效性。</li>
<li>SLMs可作为隐私保护工具用于分析敏感文本数据。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08031">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e8666731bba783448672e94f0aff3713.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7dd653e6498b2fddf65cee6e9b4b88a5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f7f81875b75bf5187e5f2e2364b229a2.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Recognizing-Surgical-Phases-Anywhere-Few-Shot-Test-time-Adaptation-and-Task-graph-Guided-Refinement"><a href="#Recognizing-Surgical-Phases-Anywhere-Few-Shot-Test-time-Adaptation-and-Task-graph-Guided-Refinement" class="headerlink" title="Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and   Task-graph Guided Refinement"></a>Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and   Task-graph Guided Refinement</h2><p><strong>Authors:Kun Yuan, Tingxuan Chen, Shi Li, Joel L. Lavanchy, Christian Heiliger, Ege Özsoy, Yiming Huang, Long Bai, Nassir Navab, Vinkle Srivastav, Hongliang Ren, Nicolas Padoy</strong></p>
<p>The complexity and diversity of surgical workflows, driven by heterogeneous operating room settings, institutional protocols, and anatomical variability, present a significant challenge in developing generalizable models for cross-institutional and cross-procedural surgical understanding. While recent surgical foundation models pretrained on large-scale vision-language data offer promising transferability, their zero-shot performance remains constrained by domain shifts, limiting their utility in unseen surgical environments. To address this, we introduce Surgical Phase Anywhere (SPA), a lightweight framework for versatile surgical workflow understanding that adapts foundation models to institutional settings with minimal annotation. SPA leverages few-shot spatial adaptation to align multi-modal embeddings with institution-specific surgical scenes and phases. It also ensures temporal consistency through diffusion modeling, which encodes task-graph priors derived from institutional procedure protocols. Finally, SPA employs dynamic test-time adaptation, exploiting the mutual agreement between multi-modal phase prediction streams to adapt the model to a given test video in a self-supervised manner, enhancing the reliability under test-time distribution shifts. SPA is a lightweight adaptation framework, allowing hospitals to rapidly customize phase recognition models by defining phases in natural language text, annotating a few images with the phase labels, and providing a task graph defining phase transitions. The experimental results show that the SPA framework achieves state-of-the-art performance in few-shot surgical phase recognition across multiple institutions and procedures, even outperforming full-shot models with 32-shot labeled data. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SPA">https://github.com/CAMMA-public/SPA</a> </p>
<blockquote>
<p>手术流程的复杂性和多样性，受到手术室设置、机构协议和解剖结构差异的影响，在为跨机构和跨手术程序手术理解开发可推广模型时面临重大挑战。虽然最近基于大规模视觉语言数据的预训练手术基础模型表现出有希望的迁移能力，但它们的零样本性能仍受到领域差异的限制，在未见过的手术环境中效用有限。为了解决这个问题，我们推出了Surgical Phase Anywhere（SPA），这是一个用于通用手术流程理解的轻便框架，能够借助最少的标注来适应机构环境。SPA利用小样本空间适应，使多模式嵌入与特定机构的手术场景和阶段保持一致。它还通过扩散建模确保时间一致性，根据机构程序协议衍生任务图先验编码。此外，SPA采用动态测试时间自适应，利用多模式阶段预测流之间的相互协议，以自我监督的方式将模型适应给定的测试视频，在测试时间分布变化下提高可靠性。SPA是一个轻量级的适应框架，允许医院通过用自然语言文本定义阶段、对少数图像进行阶段标签注释以及提供定义阶段转换的任务图来快速定制阶段识别模型。实验结果表明，SPA框架在多个机构和程序中的小样本手术阶段识别中达到了最先进的性能，甚至在32个样本标记数据的情况下超过了全样本模型。代码可在<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SPA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CAMMA-public/SPA找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20254v2">PDF</a> Accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对手术流程理解的一个新型框架——Surgical Phase Anywhere（SPA）。该框架旨在解决手术流程复杂性及多样性带来的挑战，通过少量标注数据，适应不同机构的手术环境。SPA利用少样本空间适应、扩散建模以及动态测试时间适应等技术，实现了机构特定手术场景和阶段的对齐、任务图先验的编码以及模型的自适应调整。实验结果显示，SPA框架在跨机构和跨手术程序的少样本手术阶段识别中实现了卓越性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Surgical workflows的复杂性和多样性给跨机构和跨手术程序的理解带来了挑战。</li>
<li>现有的预训练模型在未见过的手术环境中表现受限。</li>
<li>SPA框架通过少样本空间适应，适应不同机构的手术环境。</li>
<li>SPA利用扩散建模实现任务图先验的编码，确保时间一致性。</li>
<li>SPA采用动态测试时间适应，提高模型在测试时分布变化下的可靠性。</li>
<li>SPA框架允许医院通过自然语言文本定义阶段，通过少量图像标注，提供任务图定义阶段转换，实现快速定制阶段识别模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20254">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c8be39de20021b00bf428411d8498ede.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe61f58515d793766ad46c54d05b6773.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-563fcde840d43a3cde2f32ca5768ea63.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c8c2f5b6fc31fa57419a5ae080ccc80d.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-07-17  Trexplorer Super Topologically Correct Centerline Tree Tracking of   Tubular Objects in CT Volumes
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8030ab6220eb9d5af66ba57f0797f4b8.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-07-17  Journalism-Guided Agentic In-Context Learning for News Stance Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30166.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
