<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  CharaConsist Fine-Grained Consistent Character Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-df399fd115c07e260f7908d5f08cbff1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-17-æ›´æ–°"><a href="#2025-07-17-æ›´æ–°" class="headerlink" title="2025-07-17 æ›´æ–°"></a>2025-07-17 æ›´æ–°</h1><h2 id="CharaConsist-Fine-Grained-Consistent-Character-Generation"><a href="#CharaConsist-Fine-Grained-Consistent-Character-Generation" class="headerlink" title="CharaConsist: Fine-Grained Consistent Character Generation"></a>CharaConsist: Fine-Grained Consistent Character Generation</h2><p><strong>Authors:Mengyu Wang, Henghui Ding, Jianing Peng, Yao Zhao, Yunpeng Chen, Yunchao Wei</strong></p>
<p>In text-to-image generation, producing a series of consistent contents that preserve the same identity is highly valuable for real-world applications. Although a few works have explored training-free methods to enhance the consistency of generated subjects, we observe that they suffer from the following problems. First, they fail to maintain consistent background details, which limits their applicability. Furthermore, when the foreground character undergoes large motion variations, inconsistencies in identity and clothing details become evident. To address these problems, we propose CharaConsist, which employs point-tracking attention and adaptive token merge along with decoupled control of the foreground and background. CharaConsist enables fine-grained consistency for both foreground and background, supporting the generation of one character in continuous shots within a fixed scene or in discrete shots across different scenes. Moreover, CharaConsist is the first consistent generation method tailored for text-to-image DiT model. Its ability to maintain fine-grained consistency, combined with the larger capacity of latest base model, enables it to produce high-quality visual outputs, broadening its applicability to a wider range of real-world scenarios. The source code has been released at <a target="_blank" rel="noopener" href="https://github.com/Murray-Wang/CharaConsist">https://github.com/Murray-Wang/CharaConsist</a> </p>
<blockquote>
<p>åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œç”Ÿæˆä¸€ç³»åˆ—ä¿æŒä¸€è‡´çš„å†…å®¹å¹¶ä¿ç•™ç›¸åŒçš„èº«ä»½å¯¹äºå®é™…åº”ç”¨è€Œè¨€å…·æœ‰æé«˜çš„ä»·å€¼ã€‚å°½ç®¡å·²æœ‰ä¸€äº›ä½œå“æ¢è®¨äº†æ— è®­ç»ƒæ–¹æ³•æ¥æé«˜ç”Ÿæˆä¸»é¢˜çš„ä¸€è‡´æ€§ï¼Œä½†æˆ‘ä»¬è§‚å¯Ÿåˆ°å®ƒä»¬å­˜åœ¨ä»¥ä¸‹é—®é¢˜ã€‚é¦–å…ˆï¼Œå®ƒä»¬æ— æ³•ä¿æŒèƒŒæ™¯ç»†èŠ‚çš„è¿è´¯æ€§ï¼Œè¿™é™åˆ¶äº†å…¶é€‚ç”¨æ€§ã€‚æ­¤å¤–ï¼Œå½“å‰æ™¯è§’è‰²ç»å†è¾ƒå¤§çš„åŠ¨ä½œå˜åŒ–æ—¶ï¼Œèº«ä»½å’Œæœè£…ç»†èŠ‚çš„ä¸ä¸€è‡´æ€§å˜å¾—æ˜æ˜¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CharactConsistencyï¼ˆç®€ç§°ä¸ºCharaConsistï¼‰ï¼Œå®ƒé‡‡ç”¨ç‚¹è·Ÿè¸ªæ³¨æ„åŠ›ã€è‡ªé€‚åº”ä»¤ç‰Œåˆå¹¶ä»¥åŠå‰æ™¯å’ŒèƒŒæ™¯çš„è§£è€¦æ§åˆ¶ã€‚CharaConsistèƒ½å¤Ÿå®ç°å‰æ™¯å’ŒèƒŒæ™¯çš„ç²¾ç»†ä¸€è‡´æ€§ï¼Œæ”¯æŒåœ¨å›ºå®šåœºæ™¯å†…çš„è¿ç»­é•œå¤´ä¸­ç”Ÿæˆä¸€ä¸ªè§’è‰²æˆ–åœ¨ä¸åŒåœºæ™¯ä¸­çš„ç¦»æ•£é•œå¤´ä¸­ç”Ÿæˆè§’è‰²ã€‚æ­¤å¤–ï¼ŒCharaConsistæ˜¯é’ˆå¯¹æ–‡æœ¬åˆ°å›¾åƒDiTæ¨¡å‹çš„å®šåˆ¶ä¸€è‡´æ€§ç”Ÿæˆæ–¹æ³•ã€‚å…¶ä¿æŒç²¾ç»†ä¸€è‡´æ€§çš„èƒ½åŠ›ï¼Œç»“åˆæœ€æ–°åŸºç¡€æ¨¡å‹çš„æ›´å¤§å®¹é‡ï¼Œä½¿å…¶èƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡çš„è§†è§‰è¾“å‡ºï¼Œæ‰©å¤§äº†å…¶åœ¨æ›´å¹¿æ³›çš„ç°å®åœºæ™¯ä¸­çš„åº”ç”¨èŒƒå›´ã€‚æºä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Murray-Wang/CharaConsist%E3%80%82">https://github.com/Murray-Wang/CharaConsistã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11533v1">PDF</a> ICCV 2025 accepted paper, project page:   <a target="_blank" rel="noopener" href="https://murray-wang.github.io/CharaConsist/">https://murray-wang.github.io/CharaConsist/</a></p>
<p><strong>Summary</strong><br>æ–‡æœ¬ä¸­ä»‹ç»äº†ä¸€ç§ç”¨äºè§£å†³æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆä¸­ä¸€è‡´æ€§é—®é¢˜çš„æ–°æ–¹æ³•â€”â€”CharaConsistã€‚è¯¥æ–¹æ³•é€šè¿‡ç‚¹è·Ÿè¸ªæ³¨æ„åŠ›ã€è‡ªé€‚åº”ä»¤ç‰Œåˆå¹¶ä»¥åŠå‰æ™¯å’ŒèƒŒæ™¯çš„ç‹¬ç«‹æ§åˆ¶ï¼Œæé«˜äº†å‰æ™¯å’ŒèƒŒæ™¯çš„ä¸€è‡´æ€§ã€‚å®ƒèƒ½å¤Ÿç”Ÿæˆè¿ç»­é•œå¤´ä¸­çš„å•ä¸ªè§’è‰²æˆ–ä¸åŒåœºæ™¯ä¸­çš„ç¦»æ•£é•œå¤´ï¼ŒåŒæ—¶ä¿æŒé«˜ä¸€è‡´æ€§å’Œé«˜è´¨é‡è§†è§‰è¾“å‡ºã€‚é€‚ç”¨äºæ›´å¹¿æ³›çš„çœŸå®ä¸–ç•Œåœºæ™¯åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CharaConsistæ—¨åœ¨è§£å†³æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆä¸­çš„ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç‚¹è·Ÿè¸ªæ³¨æ„åŠ›æœºåˆ¶ç»´æŒèƒŒæ™¯ç»†èŠ‚çš„ä¸€è‡´æ€§ã€‚</li>
<li>è‡ªé€‚åº”ä»¤ç‰Œåˆå¹¶è§£å†³äº†å‰æ™¯è§’è‰²å¤§åŠ¨ä½œå˜åŒ–æ—¶çš„ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡å‰æ™¯å’ŒèƒŒæ™¯çš„ç‹¬ç«‹æ§åˆ¶ï¼Œæé«˜äº†ç”Ÿæˆå†…å®¹çš„ç²¾ç»†ä¸€è‡´æ€§ã€‚</li>
<li>CharaConsistæ˜¯ä¸“ä¸ºæ–‡æœ¬è½¬å›¾åƒDiTæ¨¡å‹è®¾è®¡çš„é¦–ä¸ªä¸€è‡´æ€§ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†æœ€æ–°åŸºç¡€æ¨¡å‹çš„æ›´å¤§å®¹é‡ï¼Œå¯äº§ç”Ÿé«˜è´¨é‡è§†è§‰è¾“å‡ºã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11533">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9a838b555cf47ee90071d01a1fa61ad2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08d78a3634d06a9a8286b16194113dd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bff8cdda366beb93fa86fa3f04b0ffe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da92232f58a9617ef68cee4a7b227b2f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FMC-Formalization-of-Natural-Language-Mathematical-Competition-Problems"><a href="#FMC-Formalization-of-Natural-Language-Mathematical-Competition-Problems" class="headerlink" title="FMC: Formalization of Natural Language Mathematical Competition Problems"></a>FMC: Formalization of Natural Language Mathematical Competition Problems</h2><p><strong>Authors:Jiaxuan Xie, Chengwu Liu, Ye Yuan, Siqi Li, Zhiping Xiao, Ming Zhang</strong></p>
<p>Efficient and accurate autoformalization methods, which leverage large-scale datasets of extensive natural language mathematical problems to construct formal language datasets, are key to advancing formal mathematical reasoning. In this paper, we propose an autoformalization pipeline based on large language models with error feedback, achieving a fully automatic and training-free formalization approach. Using this pipeline, we curate an Olympiad-level dataset aligning natural language problems with Lean formalizations. The dataset comprises $3,922$ mathematical problems in natural language and $9,787$ in Lean, of which $64.46%$ were assessed as at least above-average quality, making it suitable as a benchmark for automated theorem provers. Additionally, we investigate the formalization and reasoning capabilities of various LLMs and empirically demonstrate that few-shot learning, error feedback, and increasing sampling numbers enhance the autoformalization process. Experiments of three automated theorem provers on the \dataset\ dataset also highlight its challenging nature and its value as a benchmark for formal reasoning tasks. </p>
<blockquote>
<p>é«˜æ•ˆä¸”å‡†ç¡®çš„è‡ªåŠ¨å½¢å¼åŒ–æ–¹æ³•ï¼Œæ˜¯åˆ©ç”¨å¤§è§„æ¨¡çš„è‡ªç„¶è¯­è¨€æ•°å­¦é—®é¢˜æ•°æ®é›†æ¥æ„å»ºå½¢å¼åŒ–è¯­è¨€æ•°æ®é›†çš„å…³é”®ï¼Œæ˜¯æ¨åŠ¨æ•°å­¦å½¢å¼æ¨ç†å‘å±•çš„æ ¸å¿ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨å½¢å¼åŒ–æµæ°´çº¿ï¼Œè¯¥æµæ°´çº¿å…·æœ‰é”™è¯¯åé¦ˆåŠŸèƒ½ï¼Œå®ç°äº†å…¨è‡ªåŠ¨ä¸”æ— éœ€è®­ç»ƒçš„å½¢å¼åŒ–æ–¹æ³•ã€‚ä½¿ç”¨è¯¥æµæ°´çº¿ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªä¸Leanå½¢å¼åŒ–å¯¹é½çš„å¥¥æ—åŒ¹äºšçº§åˆ«æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»¥è‡ªç„¶è¯­è¨€è¡¨è¿°çš„3922ä¸ªæ•°å­¦é—®é¢˜å’Œä»¥Leanè¡¨è¿°çš„9787ä¸ªæ•°å­¦é—®é¢˜ï¼Œå…¶ä¸­ç»è¿‡è¯„ä¼°ï¼Œæœ‰64.46%è‡³å°‘è¾¾åˆ°äº†å¹³å‡è´¨é‡ä»¥ä¸Šï¼Œé€‚åˆä½œä¸ºè‡ªåŠ¨åŒ–å®šç†è¯æ˜è€…çš„åŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†å„ç§å¤§å‹è¯­è¨€æ¨¡å‹çš„å½¢å¼åŒ–æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å®è¯å‘ç°ï¼Œå°æ ·å­¦ä¹ ã€é”™è¯¯åé¦ˆå’Œå¢åŠ é‡‡æ ·æ•°é‡éƒ½å¯ä»¥å¢å¼ºè‡ªåŠ¨å½¢å¼åŒ–è¿‡ç¨‹ã€‚åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¿›è¡Œçš„ä¸‰ä¸ªè‡ªåŠ¨åŒ–å®šç†è¯æ˜å™¨çš„å®éªŒä¹Ÿçªæ˜¾äº†å®ƒçš„æŒ‘æˆ˜æ€§ä»¥åŠä½œä¸ºåŸºå‡†æµ‹è¯•çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11275v1">PDF</a> Accepted in ICML 2025 AI4MATH Workshop</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è‡ªç„¶è¯­è¨€æ•°å­¦é—®é¢˜çš„æ•°æ®é›†å¯¹äºæ„å»ºå½¢å¼è¯­è¨€æ•°æ®é›†è‡³å…³é‡è¦ï¼Œæ¨åŠ¨äº†å½¢å¼åŒ–æ•°å­¦æ¨ç†çš„å‘å±•ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨å½¢å¼åŒ–ç®¡é“ï¼Œé‡‡ç”¨é”™è¯¯åé¦ˆå®ç°å…¨è‡ªåŠ¨ã€æ— éœ€è®­ç»ƒçš„å½¢å¼åŒ–æ–¹æ³•ã€‚ä½¿ç”¨è¯¥ç®¡é“ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªä¸Leanå½¢å¼åŒ–å¯¹é½çš„å¥¥èµ›çº§åˆ«æ•°æ®é›†ï¼ŒåŒ…å«è‡ªç„¶è¯­è¨€æ•°å­¦é—®é¢˜3922ä¸ªï¼ŒLeanå½¢å¼åŒ–é—®é¢˜9787ä¸ªï¼Œå…¶ä¸­64.46%è¢«è¯„ä¸ºè‡³å°‘ä¸­ç­‰è´¨é‡æ°´å¹³ä»¥ä¸Šï¼Œé€‚åˆä½œä¸ºè‡ªåŠ¨åŒ–å®šç†è¯æ˜å™¨çš„åŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†ä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹çš„å½¢å¼åŒ–å’Œæ¨ç†èƒ½åŠ›ï¼Œå¹¶å®è¯è¡¨æ˜å°‘æ ·æœ¬å­¦ä¹ ã€é”™è¯¯åé¦ˆå’Œå¢åŠ é‡‡æ ·æ•°é‡å¯æå‡è‡ªåŠ¨å½¢å¼åŒ–è¿‡ç¨‹çš„æ•ˆæœã€‚åœ¨\dataset\æ•°æ®é›†ä¸Šè¿›è¡Œçš„ä¸‰ä¸ªè‡ªåŠ¨åŒ–å®šç†è¯æ˜å™¨çš„å®éªŒçªæ˜¾äº†å…¶æŒ‘æˆ˜æ€§ï¼Œå¹¶éªŒè¯äº†å…¶ä½œä¸ºå½¢å¼æ¨ç†ä»»åŠ¡åŸºå‡†æµ‹è¯•çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è§„æ¨¡æ•°æ®é›†åœ¨æ„å»ºå½¢å¼è¯­è¨€æ•°æ®é›†ä¸­çš„ä½œç”¨è‡³å…³é‡è¦ï¼Œæ¨åŠ¨äº†å½¢å¼åŒ–æ•°å­¦æ¨ç†çš„è¿›æ­¥ã€‚</li>
<li>æå‡ºäº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨å½¢å¼åŒ–ç®¡é“ï¼Œå®ç°å…¨è‡ªåŠ¨ã€æ— éœ€è®­ç»ƒçš„å½¢å¼åŒ–æ–¹æ³•ã€‚</li>
<li>æ•´ç†äº†ä¸€ä¸ªå¥¥èµ›çº§åˆ«çš„æ•°æ®é›†ï¼ŒåŒ…å«è‡ªç„¶è¯­è¨€ä¸Leanå½¢å¼åŒ–çš„æ•°å­¦é—®é¢˜ï¼Œå¤šæ•°é—®é¢˜è´¨é‡è¾ƒé«˜ï¼Œé€‚åˆä½œä¸ºè‡ªåŠ¨åŒ–å®šç†è¯æ˜å™¨çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ¢è®¨å¹¶å®è¯äº†å°‘æ ·æœ¬å­¦ä¹ ã€é”™è¯¯åé¦ˆå’Œå¢åŠ é‡‡æ ·æ•°é‡åœ¨æå‡è‡ªåŠ¨å½¢å¼åŒ–è¿‡ç¨‹ä¸­çš„åº”ç”¨æ•ˆæœã€‚</li>
<li>\dataset\æ•°æ®é›†å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¯ä½œä¸ºå½¢å¼æ¨ç†ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æ–‡ä¸­å±•ç¤ºäº†è¯¥ç®¡é“åŠæ•°æ®é›†åœ¨è‡ªåŠ¨å®šç†è¯æ˜ä»»åŠ¡ä¸­çš„æ½œåŠ›ä¸åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11275">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-445b0a2020cd2186c396d2db1d64940b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-130c2923d5f965263e027823c1675af9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-308189fc6f297a63da0befccba9d4a14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00b0e83e56adca02394c83bca0b5af9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-be61597329f358d5cd888912e54bdc91.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MSA-at-ImageCLEF-2025-Multimodal-Reasoning-Multilingual-Multimodal-Reasoning-With-Ensemble-Vision-Language-Models"><a href="#MSA-at-ImageCLEF-2025-Multimodal-Reasoning-Multilingual-Multimodal-Reasoning-With-Ensemble-Vision-Language-Models" class="headerlink" title="MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal   Reasoning With Ensemble Vision Language Models"></a>MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal   Reasoning With Ensemble Vision Language Models</h2><p><strong>Authors:Seif Ahmed, Mohamed T. Younes, Abdelrahman Moustafa, Abdelrahman Allam, Hamza Moustafa</strong></p>
<p>We present a robust ensemble-based system for multilingual multimodal reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which handles final answer selection, all coordinated through carefully engineered few-shot and zero-shot prompts. We conducted an extensive ablation study, training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3, Mistral) on an English dataset and its multilingual augmented version. Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for comparison and found it to substantially outperform the trained models. Prompt design also proved critical: enforcing concise, language-normalized formats and prohibiting explanatory text boosted model accuracy on the English validation set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA) achieved first place overall in the multilingual track with 81.4% accuracy, and led 11 out of 13 individual language tracks, with top results such as 95.07% for Croatian and 92.12% for Italian. These findings highlight that lightweight OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual augmentation, can outperform heavier end-to-end models in high-stakes, multilingual educational settings. </p>
<blockquote>
<p>æˆ‘ä»¬é’ˆå¯¹ImageCLEF 2025 EXAMS VæŒ‘æˆ˜ï¼Œè®¾è®¡äº†ä¸€ä¸ªç¨³å¥çš„åŸºäºé›†æˆç³»ç»Ÿçš„å¤šè¯­è¨€å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿã€‚æˆ‘ä»¬çš„æ–¹æ³•èåˆäº†Gemini 2.5 Flashè¿›è¡Œè§†è§‰æè¿°ã€Gemini 1.5 Proè¿›è¡Œå­—å¹•ä¼˜åŒ–å’Œä¸€è‡´æ€§æ£€æŸ¥ï¼Œä»¥åŠGemini 2.5 Proä½œä¸ºæ¨ç†å™¨è¿›è¡Œæœ€ç»ˆç­”æ¡ˆé€‰æ‹©ï¼Œæ‰€æœ‰è¿™äº›åŠŸèƒ½éƒ½é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å°‘é‡å’Œé›¶æ ·æœ¬æç¤ºè¿›è¡Œåè°ƒã€‚æˆ‘ä»¬å¯¹å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆGemini 2.5 Flashã€Phi 4ã€Gemma 3ã€Mistralï¼‰è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèç ”ç©¶è®­ç»ƒï¼Œè®­ç»ƒæ•°æ®é›†åŒ…æ‹¬è‹±è¯­æ•°æ®é›†åŠå…¶å¤šè¯­è¨€å¢å¼ºç‰ˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹Gemini 2.5 Flashè¿›è¡Œäº†é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„æ¯”è¾ƒè¯„ä¼°ï¼Œå‘ç°å…¶æ€§èƒ½è¿œè¶…è®­ç»ƒæ¨¡å‹ã€‚æç¤ºè®¾è®¡ä¹Ÿè¯æ˜æ˜¯å…³é”®ï¼šé‡‡ç”¨ç®€æ´ã€è¯­è¨€è§„èŒƒåŒ–çš„æ ¼å¼ï¼Œç¦æ­¢è§£é‡Šæ€§æ–‡æœ¬ï¼Œå°†æ¨¡å‹åœ¨è‹±è¯­éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ä»55.9%æé«˜åˆ°61.7%ã€‚åœ¨å®˜æ–¹æ’è¡Œæ¦œä¸Šï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿï¼ˆMSAå›¢é˜Ÿï¼‰åœ¨å¤šè¯­è¨€èµ›é“ä¸Šæ€»ä½“æ’åç¬¬ä¸€ï¼Œå‡†ç¡®ç‡ä¸º81.4%ï¼Œå¹¶åœ¨13ä¸ªè¯­è¨€èµ›é“ä¸­çš„11ä¸ªèµ›é“ä¸Šé¢†å…ˆï¼Œå¦‚åœ¨å…‹ç½—åœ°äºšè¯­èµ›é“ä¸Šçš„95.07%å’Œæ„å¤§åˆ©è¯­èµ›é“çš„92.12%ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå½“è½»é‡çº§çš„OCR-VLMé›†æˆç³»ç»Ÿä¸ç²¾ç¡®çš„æç¤ºç­–ç•¥å’Œè·¨è¯­è¨€å¢å¼ºç›¸ç»“åˆæ—¶ï¼Œå¯ä»¥åœ¨é«˜é£é™©ã€å¤šè¯­è¨€çš„æ•™è‚²ç¯å¢ƒä¸­è¡¨ç°ä¼˜äºæ›´é‡çš„ç«¯åˆ°ç«¯æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11114v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ä¸ªä¸ºImageCLEF 2025 EXAMS VæŒ‘æˆ˜è®¾è®¡çš„ç¨³å¥çš„åŸºäºé›†æˆç³»ç»Ÿçš„å¤šè¯­è¨€å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡æ•´åˆGemini 2.5 Flashè¿›è¡Œè§†è§‰æè¿°ã€Gemini 1.5 Proè¿›è¡Œæ ‡é¢˜ä¼˜åŒ–å’Œä¸€è‡´æ€§æ£€æŸ¥ã€ä»¥åŠGemini 2.5 Proä½œä¸ºæ¨ç†æœºè¿›è¡Œæœ€ç»ˆç­”æ¡ˆé€‰æ‹©ï¼Œå¹¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬æç¤ºè¿›è¡Œåè°ƒã€‚ç ”ç©¶è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œè®­ç»ƒäº†å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨è‹±è¯­æ•°æ®é›†åŠå…¶å¤šè¯­è¨€å¢å¼ºç‰ˆæœ¬ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼ŒGemini 2.5 Flashçš„è¡¨ç°ä¼˜äºè®­ç»ƒæ¨¡å‹ã€‚æç¤ºè®¾è®¡ä¹Ÿè‡³å…³é‡è¦ï¼šé‡‡ç”¨ç®€æ´ã€è¯­è¨€è§„èŒƒåŒ–çš„æ ¼å¼ï¼Œç¦æ­¢è§£é‡Šæ€§æ–‡æœ¬ï¼Œæé«˜äº†æ¨¡å‹åœ¨è‹±è¯­éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ã€‚è¯¥ç³»ç»Ÿåœ¨å¤šå…ƒè¯­è¨€è½¨é“ä¸Šè·å¾—ç¬¬ä¸€åï¼Œå‡†ç¡®ç‡é«˜è¾¾81.4%ï¼Œå¹¶åœ¨13ä¸ªè¯­è¨€è½¨é“ä¸­çš„11ä¸ªè½¨é“ä¸Šé¢†å…ˆï¼Œå¦‚å…‹ç½—åœ°äºšè¯­çš„95.07%å’Œæ„å¤§åˆ©è¯­çš„92.12%ã€‚è¿™è¡¨æ˜åœ¨å…³é”®çš„å¤šè¯­è¨€æ•™è‚²ç¯å¢ƒä¸­ï¼Œè½»é‡çº§çš„OCR-VLMé›†æˆç³»ç»Ÿé…åˆç²¾ç¡®çš„æç¤ºç­–ç•¥å’Œè·¨è¯­è¨€å¢å¼ºæŠ€æœ¯ï¼Œå¯ä»¥è¶…è¶Šæ›´é‡çš„ç«¯åˆ°ç«¯æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¢é˜ŸMSAæå‡ºä¸€ç§é’ˆå¯¹ImageCLEF 2025 EXAMS VæŒ‘æˆ˜çš„å¤šè¯­è¨€å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿã€‚</li>
<li>ç³»ç»Ÿé›†æˆäº†å¤šä¸ªç»„ä»¶ï¼ŒåŒ…æ‹¬ç”¨äºè§†è§‰æè¿°çš„Gemini 2.5 Flashã€ç”¨äºæ ‡é¢˜ä¼˜åŒ–çš„Gemini 1.5 Proå’Œä½œä¸ºæ¨ç†æœºçš„Gemini 2.5 Proã€‚</li>
<li>é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬æç¤ºè¿›è¡Œåè°ƒå„ç»„ä»¶ã€‚</li>
<li>ç ”ç©¶è¿›è¡Œäº†å¹¿æ³›çš„æ¶ˆèç ”ç©¶ï¼Œè®­ç»ƒäº†å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨è‹±è¯­æ•°æ®é›†åŠå…¶å¤šè¯­è¨€å¢å¼ºç‰ˆæœ¬ä¸Šè¯„ä¼°äº†ç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>Gemini 2.5 Flashåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹çš„è¡¨ç°ä¼˜äºè®­ç»ƒæ¨¡å‹ã€‚</li>
<li>æç¤ºè®¾è®¡å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ï¼Œé‡‡ç”¨ç®€æ´ã€è¯­è¨€è§„èŒƒåŒ–çš„æ ¼å¼ï¼Œç¦æ­¢è§£é‡Šæ€§æ–‡æœ¬å¯ä»¥æé«˜æ¨¡å‹å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11114">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed0d9efb238edd3ca208a135196be004.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c344a8b0018e5f5d067bd1cc640872a9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd12f8f53a91ab2d8889f73aae6a5472.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-028d93bd57c1f26272b8c3342f232bf1.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Bridge-Feature-Matching-and-Cross-Modal-Alignment-with-Mutual-filtering-for-Zero-shot-Anomaly-Detection"><a href="#Bridge-Feature-Matching-and-Cross-Modal-Alignment-with-Mutual-filtering-for-Zero-shot-Anomaly-Detection" class="headerlink" title="Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering   for Zero-shot Anomaly Detection"></a>Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering   for Zero-shot Anomaly Detection</h2><p><strong>Authors:Yuhu Bai, Jiangning Zhang, Yunkang Cao, Guangyuan Lu, Qingdong He, Xiangtai Li, Guanzhong Tian</strong></p>
<p>With the advent of vision-language models (e.g., CLIP) in zero- and few-shot settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in recent research, where the rare classes are essential and expected in many applications. This study introduces \textbf{FiSeCLIP} for ZSAD with training-free \textbf{CLIP}, combining the feature matching with the cross-modal alignment. Testing with the entire dataset is impractical, while batch-based testing better aligns with real industrial needs, and images within a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes other images in the same batch as reference information for the current image. However, the lack of labels for these references can introduce ambiguity, we apply text information to \textbf{fi}lter out noisy features. In addition, we further explore CLIPâ€™s inherent potential to restore its local \textbf{se}mantic correlation, adapting it for fine-grained anomaly detection tasks to enable a more accurate filtering process. Our approach exhibits superior performance for both anomaly classification and segmentation on anomaly detection benchmarks, building a stronger baseline for the direction, e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by +4.6%$\uparrow$&#x2F;+5.7%$\uparrow$ in segmentation metrics AU-ROC&#x2F;$F_1$-max. </p>
<blockquote>
<p>éšç€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚CLIPï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯çš„å‡ºç°ï¼ŒCLIPå·²è¢«å¹¿æ³›åº”ç”¨äºæœ€è¿‘çš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰ç ”ç©¶ä¸­ï¼Œå…¶ä¸­ç¨€æœ‰ç±»åˆ«åœ¨è®¸å¤šåº”ç”¨ä¸­éƒ½æ˜¯å…³é”®å’Œé¢„æœŸçš„ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†ä¸€ç§ç”¨äºZSADçš„FiSeCLIPï¼Œå®ƒç»“åˆäº†ç‰¹å¾åŒ¹é…å’Œè·¨æ¨¡æ€å¯¹é½ï¼Œä½¿ç”¨æ— éœ€è®­ç»ƒçš„CLIPã€‚å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæµ‹è¯•å¹¶ä¸å®é™…ï¼Œè€ŒåŸºäºæ‰¹æ¬¡çš„æµ‹è¯•æ›´ç¬¦åˆå®é™…çš„å·¥ä¸šéœ€æ±‚ï¼Œæ‰¹æ¬¡å†…çš„å›¾åƒå¯ä»¥ä½œä¸ºç›¸äº’çš„å‚è€ƒç‚¹ã€‚å› æ­¤ï¼ŒFiSeCLIPåˆ©ç”¨åŒä¸€æ‰¹æ¬¡ä¸­çš„å…¶ä»–å›¾åƒä½œä¸ºå½“å‰å›¾åƒçš„å‚è€ƒä¿¡æ¯ã€‚ç„¶è€Œï¼Œè¿™äº›å‚è€ƒç¼ºä¹æ ‡ç­¾å¯èƒ½ä¼šå¼•å…¥æ¨¡ç³Šæ€§ï¼Œæˆ‘ä»¬åº”ç”¨æ–‡æœ¬ä¿¡æ¯æ¥è¿‡æ»¤æ‰å˜ˆæ‚çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢CLIPçš„å†…åœ¨æ½œåŠ›æ¥æ¢å¤å…¶å±€éƒ¨è¯­ä¹‰ç›¸å…³æ€§ï¼Œå°†å…¶é€‚åº”äºç²¾ç»†ç²’åº¦çš„å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ï¼Œä»¥å®ç°æ›´ç²¾ç¡®è¿‡æ»¤è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¼‚å¸¸æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œæ— è®ºæ˜¯å¼‚å¸¸åˆ†ç±»è¿˜æ˜¯åˆ†å‰²ï¼Œä¸ºè¿™ä¸€æ–¹å‘å»ºç«‹äº†æ›´å¼ºçš„åŸºå‡†ã€‚ä¾‹å¦‚ï¼Œåœ¨MVTec-ADä¸Šï¼ŒFiSeCLIPåœ¨åˆ†å‰²æŒ‡æ ‡AU-ROC&#x2F;F1-maxä¸Šçš„è¡¨ç°ä¼˜äºæœ€æ–°æŠ€æœ¯AdaCLIPï¼Œåˆ†åˆ«æé«˜äº†+4.6%â†‘&#x2F;+5.7%â†‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11003v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯çš„åº”ç”¨ï¼ŒCLIPå·²è¢«å¹¿æ³›ç”¨äºé›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰ã€‚æœ¬ç ”ç©¶æå‡ºäº†FiSeCLIPæ–¹æ³•ç”¨äºZSADï¼Œé‡‡ç”¨æ— éœ€è®­ç»ƒçš„CLIPï¼Œç»“åˆç‰¹å¾åŒ¹é…å’Œè·¨æ¨¡æ€å¯¹é½ã€‚æœ¬ç ”ç©¶åˆ©ç”¨åŒä¸€æ‰¹æ¬¡çš„å…¶ä»–å›¾åƒä½œä¸ºå½“å‰å›¾åƒçš„å‚è€ƒä¿¡æ¯ï¼Œå¹¶é‡‡ç”¨æ–‡æœ¬ä¿¡æ¯è¿‡æ»¤æ‰å™ªå£°ç‰¹å¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢ç´¢äº†CLIPçš„å›ºæœ‰æ½œåŠ›æ¥æ¢å¤å…¶å±€éƒ¨è¯­ä¹‰ç›¸å…³æ€§ï¼Œä»¥é€‚åº”ç²¾ç»†ç²’åº¦å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ï¼Œä»¥å®ç°æ›´å‡†ç¡®çš„è¿‡æ»¤è¿‡ç¨‹ã€‚æ­¤æ–¹æ³•åœ¨å¼‚å¸¸æ£€æµ‹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶å»ºç«‹äº†æ›´å¼ºçš„åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FiSeCLIPè¢«æå‡ºç”¨äºé›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹ï¼ˆZSADï¼‰ï¼Œç»“åˆäº†ç‰¹å¾åŒ¹é…å’Œè·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨åŒä¸€æ‰¹æ¬¡çš„å…¶ä»–å›¾åƒä½œä¸ºå‚è€ƒä¿¡æ¯æ¥æ£€æµ‹å½“å‰å›¾åƒä¸­çš„å¼‚å¸¸ã€‚</li>
<li>æ–‡æœ¬ä¿¡æ¯è¢«ç”¨æ¥è¿‡æ»¤æ‰ç”±äºä½¿ç”¨å›¾åƒå‚è€ƒè€Œäº§ç”Ÿçš„å™ªå£°ç‰¹å¾ã€‚</li>
<li>ç ”ç©¶æ¢ç´¢äº†CLIPçš„æ½œåŠ›æ¥æ¢å¤å…¶å±€éƒ¨è¯­ä¹‰ç›¸å…³æ€§ï¼Œä»¥æé«˜å¼‚å¸¸æ£€æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>FiSeCLIPåœ¨å¼‚å¸¸åˆ†ç±»å’Œåˆ†æ®µæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå»ºç«‹äº†è¯¥æ–¹å‘ç ”ç©¶çš„æ–°åŸºçº¿ã€‚</li>
<li>åœ¨MVTec-ADåŸºå‡†æµ‹è¯•ä¸­ï¼ŒFiSeCLIPåœ¨åˆ†å‰²æŒ‡æ ‡AU-ROCå’Œ$F_1$-maxä¸Šè¶…è¿‡äº†ç°æœ‰æœ€ä½³æ–¹æ³•AdaCLIPã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11003">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b0033f38010c548954d4c3ef37487694.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d65d1855db938297605fd42ee77656b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e05ab6cda6dd4ddd82e75b60d7e00dc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8018b3ed88149501b825bcd319143394.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae173447baac2330f386bc0627f6c8e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9ccd4241d6f1a0ecd078e2ca1907c299.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Evaluating-Generated-Commit-Messages-with-Large-Language-Models"><a href="#Evaluating-Generated-Commit-Messages-with-Large-Language-Models" class="headerlink" title="Evaluating Generated Commit Messages with Large Language Models"></a>Evaluating Generated Commit Messages with Large Language Models</h2><p><strong>Authors:Qunhong Zeng, Yuxia Zhang, Zexiong Ma, Bo Jiang, Ningyuan Sun, Klaas-Jan Stol, Xingyu Mou, Hui Liu</strong></p>
<p>Commit messages are essential in software development as they serve to document and explain code changes. Yet, their quality often falls short in practice, with studies showing significant proportions of empty or inadequate messages. While automated commit message generation has advanced significantly, particularly with Large Language Models (LLMs), the evaluation of generated messages remains challenging. Traditional reference-based automatic metrics like BLEU, ROUGE-L, and METEOR have notable limitations in assessing commit message quality, as they assume a one-to-one mapping between code changes and commit messages, leading researchers to rely on resource-intensive human evaluation. This study investigates the potential of LLMs as automated evaluators for commit message quality. Through systematic experimentation with various prompt strategies and state-of-the-art LLMs, we demonstrate that LLMs combining Chain-of-Thought reasoning with few-shot demonstrations achieve near human-level evaluation proficiency. Our LLM-based evaluator significantly outperforms traditional metrics while maintaining acceptable reproducibility, robustness, and fairness levels despite some inherent variability. This work conducts a comprehensive preliminary study on using LLMs for commit message evaluation, offering a scalable alternative to human assessment while maintaining high-quality evaluation. </p>
<blockquote>
<p>ä»£ç æäº¤ä¿¡æ¯æ˜¯è½¯ä»¶å¼€å‘ä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œå®ƒä»¬ç”¨äºè®°å½•å’Œè§£é‡Šä»£ç æ›´æ”¹ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œå®ƒä»¬çš„å“è´¨å¾€å¾€ä¸å°½å¦‚äººæ„ï¼Œç ”ç©¶è¡¨æ˜æœ‰å¾ˆå¤§æ¯”ä¾‹çš„ä¿¡æ¯ä¸ºç©ºæˆ–ä¸è¶³ã€‚å°½ç®¡è‡ªåŠ¨ç”Ÿæˆçš„æäº¤æ¶ˆæ¯å·²ç»æœ‰äº†å¾ˆå¤§çš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½†å¯¹ç”Ÿæˆä¿¡æ¯çš„è¯„ä¼°ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¼ ç»Ÿçš„åŸºäºå‚è€ƒçš„è‡ªåŠ¨åº¦é‡æ–¹æ³•ï¼Œå¦‚BLEUã€ROUGE-Lå’ŒMETEORï¼Œåœ¨è¯„ä¼°æäº¤æ¶ˆæ¯è´¨é‡æ–¹é¢å­˜åœ¨æ˜æ˜¾çš„å±€é™æ€§ï¼Œå› ä¸ºå®ƒä»¬å‡è®¾ä»£ç æ›´æ”¹å’Œæäº¤æ¶ˆæ¯ä¹‹é—´å­˜åœ¨ä¸€ä¸€å¯¹åº”å…³ç³»ï¼Œå¯¼è‡´ç ”ç©¶äººå‘˜ä¸å¾—ä¸ä¾èµ–èµ„æºå¯†é›†å‹çš„äººç±»è¯„ä¼°æ–¹æ³•ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†å°†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæäº¤æ¶ˆæ¯è´¨é‡è‡ªåŠ¨è¯„ä¼°å™¨çš„æ½œåŠ›ã€‚é€šè¿‡ç³»ç»Ÿåœ°å°è¯•å„ç§æç¤ºç­–ç•¥å’Œæœ€å…ˆè¿›çš„LLMï¼Œæˆ‘ä»¬è¯æ˜äº†ç»“åˆâ€œæ€ç»´é“¾â€æ¨ç†å’Œå°‘é‡ç¤ºä¾‹çš„å¤§å‹è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå®ç°æ¥è¿‘äººç±»æ°´å¹³çš„è¯„ä¼°èƒ½åŠ›ã€‚æˆ‘ä»¬çš„åŸºäºLLMçš„è¯„ä¼°å™¨åœ¨ä¿æŒå¯æ¥å—çš„å†ç°æ€§ã€ç¨³å¥æ€§å’Œå…¬å¹³æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿåº¦é‡æ ‡å‡†ã€‚å°½ç®¡å­˜åœ¨ä¸€äº›å›ºæœ‰çš„å˜åŒ–æ€§ï¼Œä½†è¿™é¡¹å·¥ä½œè¿›è¡Œäº†å…³äºä½¿ç”¨LLMè¿›è¡Œæäº¤ä¿¡æ¯è¯„ä¼°çš„å…¨é¢åˆæ­¥ç ”ç©¶ï¼Œæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ›¿ä»£äººç±»è¯„ä¼°çš„æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„è¯„ä»·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10906v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æ–‡ç« æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºè‡ªåŠ¨åŒ–è¯„ä¼°æäº¤æ¶ˆæ¯è´¨é‡çš„æ½œåŠ›ã€‚é€šè¿‡ç³»ç»Ÿçš„å®éªŒå’Œæç¤ºç­–ç•¥ï¼Œç ”ç©¶è¡¨æ˜ç»“åˆChain-of-Thoughtæ¨ç†å’Œå°‘é‡æ¼”ç¤ºçš„LLMsèƒ½å¤Ÿè¾¾åˆ°æ¥è¿‘äººç±»æ°´å¹³çš„è¯„ä¼°èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»ŸæŒ‡æ ‡ï¼ŒåŒæ—¶ä¿æŒå¯æ¥å—çš„é‡å¤æ€§ã€ç¨³å¥æ€§å’Œå…¬å¹³æ€§æ°´å¹³ã€‚è¿™ä¸ºä½¿ç”¨LLMsè¿›è¡Œæäº¤æ¶ˆæ¯è¯„ä¼°æä¾›äº†å¯æ‰©å±•çš„æ›¿ä»£æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿æŒäº†é«˜è´¨é‡çš„è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æäº¤æ¶ˆæ¯åœ¨è½¯ä»¶å¼€å‘ä¸­å…·æœ‰é‡è¦æ€§å’Œä½œç”¨ï¼Œä½†å®é™…æ‰§è¡Œä¸­å…¶è´¨é‡å¸¸å¸¸ä¸è¶³æˆ–ç¼ºå¤±ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªåŠ¨æäº¤æ¶ˆæ¯ç”Ÿæˆæ–¹é¢å–å¾—äº†è¿›å±•ã€‚</li>
<li>ä¼ ç»Ÿè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚BLEUã€ROUGE-Lå’ŒMETEORï¼‰åœ¨è¯„ä¼°æäº¤æ¶ˆæ¯è´¨é‡æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>è¿™é¡¹ç ”ç©¶æ¢ç´¢äº†ä½¿ç”¨LLMsä½œä¸ºè‡ªåŠ¨åŒ–è¯„ä¼°æäº¤æ¶ˆæ¯è´¨é‡çš„æ½œåŠ›ã€‚</li>
<li>ç»“åˆChain-of-Thoughtæ¨ç†å’Œå°‘é‡æç¤ºç­–ç•¥çš„LLMså¯ä»¥æ¥è¿‘äººç±»æ°´å¹³çš„è¯„ä¼°èƒ½åŠ›ã€‚</li>
<li>LLMsçš„è¯„ä¼°å™¨åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»ŸæŒ‡æ ‡ï¼ŒåŒæ—¶ä¿æŒäº†å¯æ¥å—çš„é‡å¤æ€§ã€ç¨³å¥æ€§å’Œå…¬å¹³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10906">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dfedd21bb1def2f44313781d6f9965c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-50aa7c6d7cfd4cc37424d3adb1501106.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abeb38836dc5379baaa34fdedd804c67.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="BioScore-A-Foundational-Scoring-Function-For-Diverse-Biomolecular-Complexes"><a href="#BioScore-A-Foundational-Scoring-Function-For-Diverse-Biomolecular-Complexes" class="headerlink" title="BioScore: A Foundational Scoring Function For Diverse Biomolecular   Complexes"></a>BioScore: A Foundational Scoring Function For Diverse Biomolecular   Complexes</h2><p><strong>Authors:Yuchen Zhu, Jihong Chen, Yitong Li, Xiaomin Fang, Xianbin Ye, Jingzhou He, Xujun Zhang, Jingxuan Ge, Chao Shen, Xiaonan Zhang, Tingjun Hou, Chang-Yu Hsieh</strong></p>
<p>Structural assessment of biomolecular complexes is vital for translating molecular models into functional insights, shaping our understanding of biology and aiding drug discovery. However, current structure-based scoring functions often lack generalizability across diverse biomolecular systems. We present BioScore, a foundational scoring function that addresses key challenges â€“ data sparsity, cross-system representation, and task compatibility â€“ through a dual-scale geometric graph learning framework with tailored modules for structure assessment and affinity prediction. BioScore supports a wide range of tasks, including affinity prediction, conformation ranking, and structure-based virtual screening. Evaluated on 16 benchmarks spanning proteins, nucleic acids, small molecules, and carbohydrates, BioScore consistently outperforms or matches 70 traditional and deep learning methods. Our newly proposed PPI Benchmark further enables comprehensive evaluation of protein-protein complex scoring. BioScore demonstrates broad applicability: (1) pretraining on mixed-structure data boosts protein-protein affinity prediction by up to 40% and antigen-antibody binding correlation by over 90%; (2) cross-system generalizability enables zero- and few-shot prediction with up to 71% correlation gain; and (3) its unified representation captures chemically challenging systems such as cyclic peptides, improving affinity prediction by over 60%. BioScore establishes a robust and generalizable framework for structural assessment across complex biomolecular landscapes. </p>
<blockquote>
<p>ç”Ÿç‰©åˆ†å­å¤åˆç‰©çš„ç»“æ„è¯„ä¼°å¯¹äºå°†åˆ†å­æ¨¡å‹è½¬åŒ–ä¸ºåŠŸèƒ½è§è§£ã€å¡‘é€ æˆ‘ä»¬å¯¹ç”Ÿç‰©å­¦çš„ç†è§£ä»¥åŠè¾…åŠ©è¯ç‰©å‘ç°è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰åŸºäºç»“æ„çš„è¯„åˆ†å‡½æ•°é€šå¸¸åœ¨è·¨ä¸åŒç”Ÿç‰©åˆ†å­ç³»ç»Ÿæ—¶ç¼ºä¹é€šç”¨æ€§ã€‚æˆ‘ä»¬æå‡ºäº†BioScoreï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºç¡€è¯„åˆ†å‡½æ•°ï¼Œé€šè¿‡åŒå°ºåº¦å‡ ä½•å›¾å­¦ä¹ æ¡†æ¶å’Œé’ˆå¯¹ç»“æ„è¯„ä¼°å’Œäº²å’ŒåŠ›é¢„æµ‹çš„å®šåˆ¶æ¨¡å—ï¼Œè§£å†³äº†æ•°æ®ç¨€ç–æ€§ã€è·¨ç³»ç»Ÿè¡¨ç¤ºå’Œä»»åŠ¡å…¼å®¹æ€§ç­‰å…³é”®æŒ‘æˆ˜ã€‚BioScoreæ”¯æŒå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬äº²å’ŒåŠ›é¢„æµ‹ã€æ„è±¡æ’åºå’ŒåŸºäºç»“æ„çš„è™šæ‹Ÿç­›é€‰ã€‚åœ¨æ¶µç›–è›‹ç™½è´¨ã€æ ¸é…¸ã€å°åˆ†å­å’Œç¢³æ°´åŒ–åˆç‰©çš„16ä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒBioScoreæŒç»­ä¼˜äºæˆ–ä¸ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ–¹æ³•çš„åŒ¹é…åº¦è¾¾åˆ°70ç§ä»¥ä¸Šã€‚æˆ‘ä»¬æ–°æå‡ºçš„PPIåŸºå‡†æµ‹è¯•è¿›ä¸€æ­¥å®ç°äº†è›‹ç™½è´¨-è›‹ç™½è´¨å¤åˆç‰©è¯„åˆ†çš„å…¨é¢è¯„ä¼°ã€‚BioScoreå±•ç¤ºäº†å¹¿æ³›çš„åº”ç”¨æ€§ï¼šï¼ˆ1ï¼‰åœ¨æ··åˆç»“æ„æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè›‹ç™½è´¨-è›‹ç™½è´¨äº²å’ŒåŠ›é¢„æµ‹æé«˜äº†é«˜è¾¾40%ï¼ŒæŠ—åŸ-æŠ—ä½“ç»“åˆç›¸å…³æ€§æé«˜äº†è¶…è¿‡90%ï¼›ï¼ˆ2ï¼‰è·¨ç³»ç»Ÿçš„é€šç”¨æ€§å®ç°äº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬é¢„æµ‹ï¼Œç›¸å…³æ€§å¢ç›Šé«˜è¾¾71%ï¼›ï¼ˆ3ï¼‰å…¶ç»Ÿä¸€è¡¨ç¤ºèƒ½å¤Ÿæ•æ‰åŒ–å­¦ä¸Šå¤æ‚çš„ç³»ç»Ÿï¼Œå¦‚ç¯çŠ¶è‚½ï¼Œäº²å’ŒåŠ›é¢„æµ‹æé«˜äº†è¶…è¿‡60%ã€‚BioScoreä¸ºè·¨å¤æ‚ç”Ÿç‰©åˆ†å­æ™¯è§‚çš„ç»“æ„è¯„ä¼°å»ºç«‹äº†ç¨³å¥ä¸”é€šç”¨çš„æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10877v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šç”Ÿç‰©åˆ†å­å¤åˆç‰©çš„ç»“æ„è¯„ä¼°å¯¹äºå°†åˆ†å­æ¨¡å‹è½¬åŒ–ä¸ºåŠŸèƒ½è§è§£ã€å¡‘é€ æˆ‘ä»¬å¯¹ç”Ÿç‰©å­¦çš„ç†è§£ä»¥åŠè¾…åŠ©è¯ç‰©å‘ç°è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç»“æ„è¯„åˆ†å‡½æ•°åœ¨è·¨ä¸åŒç”Ÿç‰©åˆ†å­ç³»ç»Ÿæ—¶ç¼ºä¹é€šç”¨æ€§ã€‚æœ¬ç ”ç©¶æå‡ºäº†BioScoreï¼Œä¸€ä¸ªåŸºç¡€è¯„åˆ†å‡½æ•°ï¼Œé€šè¿‡åŒå°ºåº¦å‡ ä½•å›¾å­¦ä¹ æ¡†æ¶å’Œé’ˆå¯¹ç»“æ„è¯„ä¼°å’Œäº²å’ŒåŠ›é¢„æµ‹çš„å®šåˆ¶æ¨¡å—ï¼Œè§£å†³äº†æ•°æ®ç¨€ç–æ€§ã€è·¨ç³»ç»Ÿè¡¨ç¤ºå’Œä»»åŠ¡å…¼å®¹æ€§ç­‰å…³é”®æŒ‘æˆ˜ã€‚BioScoreæ”¯æŒå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬äº²å’ŒåŠ›é¢„æµ‹ã€æ„è±¡æ’åå’Œç»“æ„åŸºç¡€ä¸Šçš„è™šæ‹Ÿç­›é€‰ã€‚åœ¨è·¨è¶Šè›‹ç™½è´¨ã€æ ¸é…¸ã€å°åˆ†å­å’Œç¢³æ°´åŒ–åˆç‰©çš„16ä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒBioScoreçš„è¡¨ç°å§‹ç»ˆä¼˜äºæˆ–ç›¸å½“äº70ç§ä¼ ç»Ÿå’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ–°æå‡ºçš„PPIåŸºå‡†æµ‹è¯•è¿›ä¸€æ­¥å®ç°äº†è›‹ç™½è´¨-è›‹ç™½è´¨å¤åˆç‰©è¯„åˆ†çš„å…¨é¢è¯„ä¼°ã€‚BioScoreå±•ç¤ºäº†å¹¿æ³›çš„åº”ç”¨æ€§ï¼šé¢„è®­ç»ƒæ··åˆç»“æ„æ•°æ®å¯æé«˜è›‹ç™½è´¨-è›‹ç™½è´¨äº²å’ŒåŠ›é¢„æµ‹è¾¾40%ï¼ŒæŠ—åŸ-æŠ—ä½“ç»“åˆç›¸å…³æ€§æé«˜è¶…è¿‡90%ï¼›è·¨ç³»ç»Ÿé€šç”¨æ€§å¯å®ç°é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬é¢„æµ‹ï¼Œç›¸å…³æ€§æé«˜è¾¾71%ï¼›å…¶ç»Ÿä¸€è¡¨ç¤ºå¯æ•è·åŒ–å­¦ä¸Šå…·æœ‰æŒ‘æˆ˜æ€§çš„ç³»ç»Ÿï¼Œå¦‚ç¯çŠ¶è‚½ï¼Œäº²å’ŒåŠ›é¢„æµ‹æé«˜è¶…è¿‡60%ã€‚BioScoreä¸ºè·¨å¤æ‚ç”Ÿç‰©åˆ†å­æ™¯è§‚çš„ç»“æ„è¯„ä¼°å»ºç«‹äº†ç¨³å¥å’Œé€šç”¨çš„æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>BioScoreè§£å†³äº†ç”Ÿç‰©åˆ†å­å¤åˆç‰©ç»“æ„è¯„ä¼°ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®ç¨€ç–æ€§ã€è·¨ç³»ç»Ÿè¡¨ç¤ºå’Œä»»åŠ¡å…¼å®¹æ€§ã€‚</li>
<li>BioScoreæ”¯æŒå¤šç§ä»»åŠ¡ï¼Œå¦‚äº²å’ŒåŠ›é¢„æµ‹ã€æ„è±¡æ’åå’Œç»“æ„åŸºç¡€ä¸Šçš„è™šæ‹Ÿç­›é€‰ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒBioScoreè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºæˆ–ç›¸å½“äºå¤§å¤šæ•°ä¼ ç»Ÿå’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>PPIåŸºå‡†æµ‹è¯•çš„å®ç°ä¸ºè›‹ç™½è´¨-è›‹ç™½è´¨å¤åˆç‰©è¯„åˆ†æä¾›äº†å…¨é¢è¯„ä¼°ã€‚</li>
<li>BioScoreå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ€§ï¼Œé¢„è®­ç»ƒæ··åˆç»“æ„æ•°æ®å¯æ˜¾è‘—æé«˜é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>BioScoreå®ç°äº†é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬é¢„æµ‹ï¼Œæ˜¾ç¤ºå‡ºå…¶è·¨ç³»ç»Ÿçš„é€šç”¨æ€§ã€‚</li>
<li>BioScoreçš„ç»Ÿä¸€è¡¨ç¤ºå¯å¤„ç†åŒ–å­¦ä¸ŠæŒ‘æˆ˜æ€§çš„ç³»ç»Ÿï¼Œå¦‚ç¯çŠ¶è‚½ï¼Œæé«˜äº†äº²å’ŒåŠ›é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10877">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-50f9bffeae7fcd7c2cb8cd33fd95a3d4.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Automated-Thematic-Analyses-Using-LLMs-Xylazine-Wound-Management-Social-Media-Chatter-Use-Case"><a href="#Automated-Thematic-Analyses-Using-LLMs-Xylazine-Wound-Management-Social-Media-Chatter-Use-Case" class="headerlink" title="Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social   Media Chatter Use Case"></a>Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social   Media Chatter Use Case</h2><p><strong>Authors:JaMor Hairston, Ritvik Ranjan, Sahithi Lakamana, Anthony Spadaro, Selen Bozkurt, Jeanmarie Perrone, Abeed Sarker</strong></p>
<p>Background Large language models (LLMs) face challenges in inductive thematic analysis, a task requiring deep interpretive and domain-specific expertise. We evaluated the feasibility of using LLMs to replicate expert-driven thematic analysis of social media data. Methods Using two temporally non-intersecting Reddit datasets on xylazine (n&#x3D;286 and n&#x3D;686, for model optimization and validation, respectively) with twelve expert-derived themes, we evaluated five LLMs against expert coding. We modeled the task as a series of binary classifications, rather than a single, multi-label classification, employing zero-, single-, and few-shot prompting strategies and measuring performance via accuracy, precision, recall, and F1-score. Results On the validation set, GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score: 0.71). For high-prevalence themes, model-derived thematic distributions closely mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use: 16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based approaches can automate thematic analyses, offering a scalable supplement for qualitative research. Keywords: thematic analysis, large language models, natural language processing, qualitative analysis, social media, prompt engineering, public health </p>
<blockquote>
<p>èƒŒæ™¯ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å½’çº³ä¸»é¢˜åˆ†ææ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™æ˜¯ä¸€é¡¹éœ€è¦æ·±åº¦è§£è¯»å’Œç‰¹å®šé¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„ä»»åŠ¡ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä½¿ç”¨LLMså¤åˆ¶åŸºäºä¸“å®¶çš„ç¤¾äº¤åª’ä½“æ•°æ®ä¸»é¢˜åˆ†æçš„å¯è¡Œæ€§ã€‚æ–¹æ³•ï¼šæˆ‘ä»¬ä½¿ç”¨äº†ä¸¤ç»„æ—¶é—´ä¸Šæ²¡æœ‰é‡å çš„å…³äºå®‰å®šå‰‚ï¼ˆxylazineï¼‰çš„Redditæ•°æ®é›†ï¼ˆåˆ†åˆ«ä¸º286ä¸ªå’Œ686ä¸ªå¸–å­ï¼Œç”¨äºæ¨¡å‹ä¼˜åŒ–å’ŒéªŒè¯ï¼‰ï¼Œå¹¶åŒ…å«ä¸“å®¶å¾—å‡ºçš„åäºŒä¸ªä¸»é¢˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†äº”ç§LLMsä¸ä¸“å®¶ç¼–ç çš„å¯¹æ¯”æ•ˆæœã€‚æˆ‘ä»¬å°†ä»»åŠ¡å»ºæ¨¡ä¸ºä¸€ç³»åˆ—äºŒå…ƒåˆ†ç±»ï¼Œè€Œä¸æ˜¯å•ä¸€çš„å¤šæ ‡ç­¾åˆ†ç±»ï¼Œé‡‡ç”¨é›¶æ ·æœ¬ã€å•æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºç­–ç•¥ï¼Œå¹¶é€šè¿‡å‡†ç¡®æ€§ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°æ¥è¡¡é‡æ€§èƒ½ã€‚ç»“æœï¼šåœ¨éªŒè¯é›†ä¸Šï¼ŒGPT-4oçš„äºŒæ¬¡æç¤ºè¡¨ç°æœ€ä½³ï¼ˆå‡†ç¡®ç‡ï¼š90.9%ï¼›F1åˆ†æ•°ï¼š0.71ï¼‰ã€‚å¯¹äºé«˜å‘ç—…ç‡ä¸»é¢˜ï¼Œæ¨¡å‹è¡ç”Ÿçš„ä¸»é¢˜åˆ†å¸ƒä¸ä¸“å®¶åˆ†ç±»ç»“æœéå¸¸æ¥è¿‘ï¼ˆä¾‹å¦‚ï¼Œå®‰å®šå‰‚ä½¿ç”¨ï¼šæ¨¡å‹ä¸º13.6% vs ä¸“å®¶ä¸º17.8%ï¼›MOUDä½¿ç”¨ï¼šæ¨¡å‹ä¸º16.5% vs ä¸“å®¶ä¸º17.8%ï¼‰ã€‚ç»“è®ºï¼šæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºå°‘æ ·æœ¬çš„LLMæ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨åŒ–ä¸»é¢˜åˆ†æï¼Œä¸ºå®šæ€§ç ”ç©¶æä¾›äº†å¯æ‰©å±•çš„è¡¥å……ã€‚å…³é”®è¯ï¼šä¸»é¢˜åˆ†æã€å¤§å‹è¯­è¨€æ¨¡å‹ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€å®šæ€§åˆ†æã€ç¤¾äº¤åª’ä½“ã€æç¤ºå·¥ç¨‹ã€å…¬å…±å«ç”Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10803v1">PDF</a> Pages: 19, Abstract word count: 151 words, Manuscript word count:   2185 words, References: 14, Figures: 3, Tables: 2</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å½’çº³ä¸»é¢˜åˆ†ææ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦æ·±åº¦è§£è¯»å’Œç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ã€‚ç ”ç©¶è¯„ä¼°äº†ä½¿ç”¨LLMså¤åˆ¶ä¸“å®¶é©±åŠ¨çš„ç¤¾ä¼šåª’ä½“æ•°æ®ä¸»é¢˜åˆ†æçš„å¯è¡Œæ€§ã€‚é€šè¿‡ä¸¤ä¸ªå…³äºå®‰å®šå‰‚Redditæ•°æ®é›†ï¼Œè¯„ä¼°äº†äº”ä¸ªLLMsä¸ä¸“å®¶ç¼–ç çš„æ€§èƒ½ã€‚ä»»åŠ¡è¢«å»ºæ¨¡ä¸ºä¸€ç³»åˆ—äºŒå…ƒåˆ†ç±»ï¼Œè€Œéå•ä¸€å¤šå…ƒæ ‡ç­¾åˆ†ç±»ã€‚é‡‡ç”¨é›¶ã€å•ã€å°‘é•œå¤´æç¤ºç­–ç•¥ï¼Œå¹¶é€šè¿‡å‡†ç¡®åº¦ã€ç²¾ç¡®åº¦ã€å¬å›ç‡å’ŒF1åˆ†æ•°è¡¡é‡æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼ŒGPT-4oåœ¨ä¸¤æ¬¡æç¤ºä¸‹çš„æ€§èƒ½æœ€ä½³ï¼ˆå‡†ç¡®åº¦ï¼š90.9%ï¼›F1åˆ†æ•°ï¼š0.71ï¼‰ã€‚å¯¹äºé«˜å‘ç—…ç‡ä¸»é¢˜ï¼Œæ¨¡å‹è¡ç”Ÿçš„ä¸»é¢˜åˆ†å¸ƒä¸ä¸“å®¶åˆ†ç±»åŸºæœ¬ä¸€è‡´ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒåŸºäºå°‘é•œå¤´LLMçš„æ–¹æ³•å¯ä»¥è‡ªåŠ¨è¿›è¡Œä¸»é¢˜åˆ†æï¼Œä¸ºå®šæ€§ç ”ç©¶æä¾›å¯æ‰©å±•çš„è¡¥å……ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´å½’çº³ä¸»é¢˜åˆ†æçš„æŒ‘æˆ˜ï¼Œéœ€è¦æ·±åº¦è§£è¯»å’Œç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨LLMsè¿›è¡Œç¤¾ä¼šåª’ä½“æ•°æ®çš„ä¸»é¢˜åˆ†æï¼Œå¹¶è¯„ä¼°äº†å…¶å¯è¡Œæ€§ã€‚</li>
<li>é€šè¿‡ä¸¤ä¸ªå…³äºå®‰å®šå‰‚çš„Redditæ•°æ®é›†è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚</li>
<li>ä»»åŠ¡è¢«å»ºæ¨¡ä¸ºä¸€ç³»åˆ—äºŒå…ƒåˆ†ç±»ã€‚</li>
<li>å°è¯•äº†å¤šç§æç¤ºç­–ç•¥å¹¶è¯„ä¼°äº†æ€§èƒ½ã€‚</li>
<li>GPT-4oåœ¨ä¸¤æ¬¡æç¤ºä¸‹è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10803">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eb36a56d6e1f87b92914e5a16575a7f5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Integrating-Biological-Knowledge-for-Robust-Microscopy-Image-Profiling-on-De-Novo-Cell-Lines"><a href="#Integrating-Biological-Knowledge-for-Robust-Microscopy-Image-Profiling-on-De-Novo-Cell-Lines" class="headerlink" title="Integrating Biological Knowledge for Robust Microscopy Image Profiling   on De Novo Cell Lines"></a>Integrating Biological Knowledge for Robust Microscopy Image Profiling   on De Novo Cell Lines</h2><p><strong>Authors:Jiayuan Chen, Thai-Hoang Pham, Yuanlong Wang, Ping Zhang</strong></p>
<p>High-throughput screening techniques, such as microscopy imaging of cellular responses to genetic and chemical perturbations, play a crucial role in drug discovery and biomedical research. However, robust perturbation screening for \textit{de novo} cell lines remains challenging due to the significant morphological and biological heterogeneity across cell lines. To address this, we propose a novel framework that integrates external biological knowledge into existing pretraining strategies to enhance microscopy image profiling models. Our approach explicitly disentangles perturbation-specific and cell line-specific representations using external biological information. Specifically, we construct a knowledge graph leveraging protein interaction data from STRING and Hetionet databases to guide models toward perturbation-specific features during pretraining. Additionally, we incorporate transcriptomic features from single-cell foundation models to capture cell line-specific representations. By learning these disentangled features, our method improves the generalization of imaging models to \textit{de novo} cell lines. We evaluate our framework on the RxRx database through one-shot fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from the RxRx19a dataset. Experimental results demonstrate that our method enhances microscopy image profiling for \textit{de novo} cell lines, highlighting its effectiveness in real-world phenotype-based drug discovery applications. </p>
<blockquote>
<p>é«˜é€šé‡ç­›é€‰æŠ€æœ¯ï¼Œå¦‚é€šè¿‡æ˜¾å¾®é•œæˆåƒè§‚å¯Ÿç»†èƒå¯¹é—ä¼ å’ŒåŒ–å­¦å¹²æ‰°çš„å“åº”ï¼Œåœ¨è¯ç‰©å‘ç°å’Œç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç”±äºç»†èƒç³»ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„å½¢æ€å’Œç”Ÿç‰©å¼‚è´¨æ€§ï¼Œå¯¹æ–°å‹ç»†èƒç³»çš„ç¨³å¥å¹²æ‰°ç­›é€‰ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå®ƒå°†å¤–éƒ¨ç”Ÿç‰©å­¦çŸ¥è¯†æ•´åˆåˆ°ç°æœ‰çš„é¢„è®­ç»ƒç­–ç•¥ä¸­ï¼Œä»¥å¢å¼ºæ˜¾å¾®é•œå›¾åƒåˆ†ææ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤–éƒ¨ç”Ÿç‰©å­¦ä¿¡æ¯æ˜¾å¼åœ°åˆ†è§£å¹²æ‰°ç‰¹å¼‚æ€§å’Œç»†èƒç³»ç‰¹å¼‚æ€§è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªçŸ¥è¯†å›¾è°±ï¼Œåˆ©ç”¨STRINGå’ŒHetionetæ•°æ®åº“ä¸­çš„è›‹ç™½è´¨ç›¸äº’ä½œç”¨æ•°æ®æ¥æŒ‡å¯¼æ¨¡å‹åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å…³æ³¨å¹²æ‰°ç‰¹å¼‚æ€§ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç»“åˆäº†å•ç»†èƒåŸºç¡€æ¨¡å‹çš„è½¬å½•ç»„ç‰¹å¾æ¥æ•æ‰ç»†èƒç³»ç‰¹å¼‚æ€§è¡¨ç¤ºã€‚é€šè¿‡å­¦ä¹ è¿™äº›åˆ†ç¦»çš„ç‰¹å¾ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†æˆåƒæ¨¡å‹å¯¹æ–°å‹ç»†èƒç³»çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨RxRxæ•°æ®åº“ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ¡†æ¶ï¼Œé€šè¿‡ä¸€æ¬¡å¾®è°ƒï¼ˆRxRxçš„ä¸€ä¸ªç»†èƒç³»ï¼‰å’Œå‡ æ¬¡å¾®è°ƒï¼ˆRxRxåä¹ç”²åŸºçš„æ•°æ®é›†ä¸­çš„ç»†èƒç³»ï¼‰è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†æ–°å‹ç»†èƒç³»çš„æ˜¾å¾®é•œå›¾åƒåˆ†ææ•ˆæœï¼Œçªæ˜¾å…¶åœ¨åŸºäºè¡¨å‹çš„è¯ç‰©å‘ç°åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10737v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨è¯ç‰©å‘ç°å’Œç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­ï¼Œé«˜é€šé‡ç­›é€‰æŠ€æœ¯ï¼Œå¦‚åˆ©ç”¨æ˜¾å¾®é•œæˆåƒè§‚å¯Ÿç»†èƒå¯¹é—ä¼ å’ŒåŒ–å­¦å¹²æ‰°çš„å“åº”ï¼Œå‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œå¯¹äºæ–°å‹ç»†èƒç³»çš„ç¨³å¥æ€§å¹²æ‰°ç­›é€‰ä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†å¤–éƒ¨ç”Ÿç‰©å­¦çŸ¥è¯†èå…¥ç°æœ‰çš„é¢„è®­ç»ƒç­–ç•¥ä¸­ï¼Œä»¥æå‡æ˜¾å¾®é•œå›¾åƒåˆ†ææ¨¡å‹çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶é€šè¿‡æ˜ç¡®åŒºåˆ†å¹²æ‰°ç‰¹å¼‚æ€§å’Œç»†èƒç³»ç‰¹å¼‚æ€§è¡¨å¾æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¹¶åˆ©ç”¨è›‹ç™½è´¨ç›¸äº’ä½œç”¨æ•°æ®å’Œå•ç»†èƒè½¬å½•ç»„ç‰¹å¾æ„å»ºçŸ¥è¯†å›¾è°±æ¥æŒ‡å¯¼æ¨¡å‹å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æé«˜å¯¹æ–°å‹ç»†èƒç³»çš„æˆåƒæ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨åŸºäºè¡¨å‹çš„è¯ç‰©å‘ç°åº”ç”¨ä¸­å…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜é€šé‡ç­›é€‰æŠ€æœ¯åœ¨è¯ç‰©å‘ç°å’Œç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­å…·æœ‰å…³é”®ä½œç”¨ã€‚</li>
<li>å¯¹æ–°å‹ç»†èƒç³»çš„ç¨³å¥æ€§å¹²æ‰°ç­›é€‰å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºç»†èƒç³»ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„å½¢æ€å­¦å’Œç”Ÿç‰©å­¦å¼‚è´¨æ€§ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§ç»“åˆå¤–éƒ¨ç”Ÿç‰©å­¦çŸ¥è¯†çš„æ–°å‹æ¡†æ¶ï¼Œä»¥æé«˜æ˜¾å¾®é•œå›¾åƒåˆ†ææ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡æ˜ç¡®åŒºåˆ†å¹²æ‰°ç‰¹å¼‚æ€§å’Œç»†èƒç³»ç‰¹å¼‚æ€§è¡¨å¾æ¥è§£å†³æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨è›‹ç™½è´¨ç›¸äº’ä½œç”¨æ•°æ®å’Œå•ç»†èƒè½¬å½•ç»„ç‰¹å¾æ„å»ºçŸ¥è¯†å›¾è°±æ¥æŒ‡å¯¼æ¨¡å‹å­¦ä¹ ã€‚</li>
<li>æ–¹æ³•æé«˜äº†å¯¹æ–°å‹ç»†èƒç³»çš„æˆåƒæ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10737">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d13bed7ab7b7c789068a7eccbb965f94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66eded3c4a3f59d66a13c327ac2aee11.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3fd1462546a903274ddf438cd36f0f43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3455a4889fe753780e917ca0b8a4514a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd81319f3dfe06c1c6e3a32ac510fce7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Machine-learning-inference-of-stellar-properties-using-integrated-photometric-and-spectroscopic-data"><a href="#Machine-learning-inference-of-stellar-properties-using-integrated-photometric-and-spectroscopic-data" class="headerlink" title="Machine-learning inference of stellar properties using integrated   photometric and spectroscopic data"></a>Machine-learning inference of stellar properties using integrated   photometric and spectroscopic data</h2><p><strong>Authors:Ilay Kamai, Alex M. Bronstein, Hagai B. Perets</strong></p>
<p>Stellar astrophysics relies on diverse observational modalities-primarily photometric light curves and spectroscopic data-from which fundamental stellar properties are inferred. While machine learning (ML) has advanced analysis within individual modalities, the complementary information encoded across modalities remains largely underexploited. We present DESA (Dual Embedding model for Stellar Astrophysics), a novel multi-modal foundation model that integrates light curves and spectra to learn a unified, physically meaningful latent space for stars. DESA first trains separate modality-specific encoders using a hybrid supervised&#x2F;self-supervised scheme, and then aligns them through DualFormer, a transformer-based cross-modal integration module tailored for astrophysical data. DualFormer combines cross- and self-attention, a novel dual-projection alignment loss, and a projection-space eigendecomposition that yields physically structured embeddings. We demonstrate that DESA significantly outperforms leading unimodal and self-supervised baselines across a range of tasks. In zero- and few-shot settings, DESAâ€™s learned representations recover stellar color-magnitude and Hertzsprung-Russell diagrams with high fidelity ($R^2 &#x3D; 0.92$ for photometric regressions). In full fine-tuning, DESA achieves state-of-the-art accuracy for binary star detection (AUC &#x3D; $0.99$, AP &#x3D; $1.00$) and stellar age prediction (RMSE &#x3D; $0.94$ Gyr). As a compelling case, DESA naturally separates synchronized binaries from young stars, two populations with nearly identical light curves, purely from their embedded positions in UMAP space, without requiring external kinematic or luminosity information. DESA thus offers a powerful new framework for multimodal, data-driven stellar population analysis, enabling both accurate prediction and novel discovery. </p>
<blockquote>
<p>æ’æ˜Ÿå¤©æ–‡å­¦ä¾èµ–äºå¤šç§è§‚æµ‹æ–¹å¼ï¼Œä¸»è¦æ˜¯å…‰åº¦å…‰å˜æ›²çº¿å’Œå…‰è°±æ•°æ®ï¼Œä»ä¸­å¯ä»¥æ¨æ–­å‡ºæ’æ˜Ÿçš„åŸºæœ¬å±æ€§ã€‚è™½ç„¶æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å·²ç»åœ¨å•ä¸€æ¨¡å¼åˆ†ææ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†è·¨æ¨¡å¼çš„äº’è¡¥ä¿¡æ¯ä»ç„¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¢«å¿½è§†ã€‚æˆ‘ä»¬æå‡ºäº†DESAï¼ˆç”¨äºæ’æ˜Ÿå¤©æ–‡å­¦çš„åŒé‡åµŒå…¥æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡å¼åŸºç¡€æ¨¡å‹ï¼Œå®ƒç»“åˆäº†å…‰å˜æ›²çº¿å’Œå…‰è°±æ•°æ®ï¼Œå­¦ä¹ ä¸€ä¸ªç»Ÿä¸€ã€å…·æœ‰ç‰©ç†æ„ä¹‰çš„æ’æ˜Ÿæ½œåœ¨ç©ºé—´ã€‚DESAé¦–å…ˆä½¿ç”¨æ··åˆçš„ç›‘ç£&#x2F;è‡ªç›‘ç£æ–¹æ¡ˆè®­ç»ƒç‰¹å®šçš„æ¨¡æ€ç¼–ç å™¨ï¼Œç„¶åé€šè¿‡é’ˆå¯¹å¤©æ–‡æ•°æ®å®šåˆ¶çš„åŸºäºå˜å‹å™¨çš„è·¨æ¨¡æ€é›†æˆæ¨¡å—DualFormerè¿›è¡Œå¯¹é½ã€‚DualFormerç»“åˆäº†è·¨æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›ã€æ–°é¢–çš„åŒæŠ•å½±å¯¹é½æŸå¤±ä»¥åŠæŠ•å½±ç©ºé—´ç‰¹å¾åˆ†è§£ï¼Œä»è€Œäº§ç”Ÿå…·æœ‰ç‰©ç†ç»“æ„çš„åµŒå…¥ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒDESAåœ¨ä¸€ç³»åˆ—ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºé¢†å…ˆçš„å•æ¨¡æ€å’Œè‡ªæˆ‘ç›‘ç£çš„åŸºçº¿ã€‚åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æƒ…å†µä¸‹ï¼ŒDESAå­¦ä¹ çš„è¡¨ç¤ºèƒ½å¤Ÿé«˜åº¦ä¿çœŸåœ°æ¢å¤æ’æ˜Ÿçš„é¢œè‰²å¹…åº¦å’Œèµ«ç½—å›¾ï¼ˆ$R^2 &#x3D; 0.92$ç”¨äºå…‰åº¦å›å½’ï¼‰ã€‚åœ¨å®Œå…¨å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒDESAåœ¨äºŒå…ƒæ’æ˜Ÿæ£€æµ‹ï¼ˆAUC &#x3D; $0.99$ï¼ŒAP &#x3D; $1.00$ï¼‰å’Œæ’æ˜Ÿå¹´é¾„é¢„æµ‹ï¼ˆRMSE &#x3D; $0.94$å‰å¹´ï¼‰æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚ä½œä¸ºä¸€ä¸ªå¼•äººæ³¨ç›®çš„æ¡ˆä¾‹ï¼ŒDESAèƒ½å¤Ÿè‡ªç„¶åœ°åˆ†ç¦»åŒæ­¥åŒæ˜Ÿå’Œå¹´è½»æ’æ˜Ÿè¿™ä¸¤ä¸ªå‡ ä¹å…·æœ‰ç›¸åŒå…‰å˜æ›²çº¿çš„ç¾¤ä½“ï¼Œä»…ä»…åŸºäºå®ƒä»¬åœ¨UMAPç©ºé—´ä¸­çš„åµŒå…¥ä½ç½®ï¼Œæ— éœ€é¢å¤–çš„è¿åŠ¨å­¦æˆ–å…‰åº¦ä¿¡æ¯ã€‚å› æ­¤ï¼ŒDESAæä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ–°æ¡†æ¶ï¼Œç”¨äºå¤šæ¨¡æ€ã€æ•°æ®é©±åŠ¨çš„æ’æ˜Ÿç¾¤ä½“åˆ†æï¼Œèƒ½å¤Ÿå®ç°å‡†ç¡®çš„é¢„æµ‹å’Œæ–°é¢–çš„å‘ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10666v1">PDF</a> submitted to ApJ</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DESAæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºæ’æ˜Ÿå¤©æ–‡å­¦çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ã€‚DESAé€šè¿‡æ•´åˆå…‰æ›²çº¿å’Œå…‰è°±æ•°æ®ï¼Œå­¦ä¹ äº†ä¸€ä¸ªç»Ÿä¸€çš„ã€å…·æœ‰ç‰©ç†æ„ä¹‰çš„æ’æ˜Ÿæ½œåœ¨ç©ºé—´ã€‚è¯¥æ¨¡å‹é€šè¿‡æ··åˆç›‘ç£&#x2F;è‡ªç›‘ç£æ–¹æ¡ˆè®­ç»ƒç‰¹å®šæ¨¡æ€çš„ç¼–ç å™¨ï¼Œç„¶åé€šè¿‡åŸºäºå˜å‹å™¨çš„è·¨æ¨¡æ€é›†æˆæ¨¡å—DualFormerè¿›è¡Œå¯¹é½ã€‚DESAåœ¨å¤šç§ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºé¢†å…ˆçš„å•æ¨¡æ€å’Œè‡ªæˆ‘ç›‘ç£åŸºçº¿ã€‚åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æƒ…å†µä¸‹ï¼ŒDESAèƒ½å¤Ÿæ¢å¤é«˜ä¿çœŸåº¦çš„æ’æ˜Ÿè‰²æ ‡å›¾å’Œèµ«ç½—å›¾ã€‚åœ¨å®Œå…¨å¾®è°ƒçš„æƒ…å†µä¸‹ï¼ŒDESAå®ç°äº†äºŒè¿›åˆ¶æ’æ˜Ÿæ£€æµ‹å’Œæ’æ˜Ÿå¹´é¾„é¢„æµ‹çš„æœ€æ–°å‡†ç¡®æ€§ã€‚DESAä¸ºå¤šå…ƒæ•°æ®é©±åŠ¨æ’æ˜Ÿç¾¤ä½“åˆ†ææä¾›äº†å¼ºå¤§çš„æ–°æ¡†æ¶ï¼Œæ—¢å¯ç”¨äºå‡†ç¡®é¢„æµ‹ï¼Œä¹Ÿå¯ç”¨äºæ–°é¢–å‘ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DESAæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹ï¼Œç”¨äºæ’æ˜Ÿå¤©æ–‡å­¦ï¼Œæ•´åˆäº†å…‰æ›²çº¿å’Œå…‰è°±æ•°æ®ã€‚</li>
<li>DESAé€šè¿‡æ··åˆç›‘ç£&#x2F;è‡ªç›‘ç£æ–¹æ¡ˆè®­ç»ƒç‰¹å®šæ¨¡æ€çš„ç¼–ç å™¨ã€‚</li>
<li>DESAä½¿ç”¨åŸºäºå˜å‹å™¨çš„è·¨æ¨¡æ€é›†æˆæ¨¡å—DualFormerè¿›è¡Œæ¨¡æ€å¯¹é½ã€‚</li>
<li>DESAåœ¨å„ç§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½ï¼ŒåŒ…æ‹¬å›å½’å’Œåˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æƒ…å†µä¸‹ï¼ŒDESAèƒ½å¤Ÿæ¢å¤é«˜ä¿çœŸåº¦çš„æ’æ˜Ÿè‰²æ ‡å›¾å’Œèµ«ç½—å›¾ã€‚</li>
<li>DESAå®ç°äº†äºŒè¿›åˆ¶æ’æ˜Ÿæ£€æµ‹å’Œæ’æ˜Ÿå¹´é¾„é¢„æµ‹çš„æœ€æ–°å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10666">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7233990e26a96403519f0c42d5bee50d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-103629f8d30dd2759efef81249cc7f93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0d13a66ce77530e27337fc715c091ad2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MP1-Mean-Flow-Tames-Policy-Learning-in-1-step-for-Robotic-Manipulation"><a href="#MP1-Mean-Flow-Tames-Policy-Learning-in-1-step-for-Robotic-Manipulation" class="headerlink" title="MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation"></a>MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation</h2><p><strong>Authors:Juyi Sheng, Ziyi Wang, Peiming Li, Mengyuan Liu</strong></p>
<p>In robot manipulation, robot learning has become a prevailing approach. However, generative models within this field face a fundamental trade-off between the slow, iterative sampling of diffusion models and the architectural constraints of faster Flow-based methods, which often rely on explicit consistency losses. To address these limitations, we introduce MP1, which pairs 3D point-cloud inputs with the MeanFlow paradigm to generate action trajectories in one network function evaluation (1-NFE). By directly learning the interval-averaged velocity via the MeanFlow Identity, our policy avoids any additional consistency constraints. This formulation eliminates numerical ODE-solver errors during inference, yielding more precise trajectories. MP1 further incorporates CFG for improved trajectory controllability while retaining 1-NFE inference without reintroducing structural constraints. Because subtle scene-context variations are critical for robot learning, especially in few-shot learning, we introduce a lightweight Dispersive Loss that repels state embeddings during training, boosting generalization without slowing inference. We validate our method on the Adroit and Meta-World benchmarks, as well as in real-world scenarios. Experimental results show MP1 achieves superior average task success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its average inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster than FlowPolicy. Our code is available at <a target="_blank" rel="noopener" href="https://mp1-2254.github.io/">https://mp1-2254.github.io/</a>. </p>
<blockquote>
<p>åœ¨æœºå™¨äººæ“ä½œé¢†åŸŸï¼Œæœºå™¨äººå­¦ä¹ å·²ç»æˆä¸ºä¸€ç§æµè¡Œçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¯¥é¢†åŸŸä¸­çš„ç”Ÿæˆæ¨¡å‹é¢ä¸´ç€æ‰©æ•£æ¨¡å‹çš„ç¼“æ…¢è¿­ä»£é‡‡æ ·å’ŒåŸºäºæµçš„å¿«é€Ÿæ–¹æ³•çš„ç»“æ„çº¦æŸä¹‹é—´çš„åŸºæœ¬æƒè¡¡ï¼Œåè€…é€šå¸¸ä¾èµ–äºæ˜ç¡®çš„ä¸€è‡´æ€§æŸå¤±ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†MP1ï¼Œå®ƒå°†3Dç‚¹äº‘è¾“å…¥ä¸MeanFlowèŒƒå¼ç›¸ç»“åˆï¼Œåœ¨ä¸€ä¸ªç½‘ç»œåŠŸèƒ½è¯„ä¼°ï¼ˆ1-NFEï¼‰ä¸­ç”Ÿæˆè¡ŒåŠ¨è½¨è¿¹ã€‚é€šè¿‡ç›´æ¥å­¦ä¹ MeanFlow Identityçš„é—´éš”å¹³å‡é€Ÿåº¦ï¼Œæˆ‘ä»¬çš„ç­–ç•¥é¿å…äº†ä»»ä½•é¢å¤–çš„ä¸€è‡´æ€§çº¦æŸã€‚è¿™ç§å…¬å¼æ¶ˆé™¤äº†æ¨ç†è¿‡ç¨‹ä¸­çš„æ•°å€¼ODEæ±‚è§£å™¨é”™è¯¯ï¼Œäº§ç”Ÿäº†æ›´ç²¾ç¡®çš„è½¨è¿¹ã€‚MP1è¿˜ç»“åˆäº†CFGï¼Œä»¥æé«˜è½¨è¿¹çš„å¯æ§æ€§ï¼ŒåŒæ—¶ä¿æŒ1-NFEæ¨ç†ï¼Œè€Œæ²¡æœ‰é‡æ–°å¼•å…¥ç»“æ„çº¦æŸã€‚ç”±äºå¾®å¦™çš„åœºæ™¯ä¸Šä¸‹æ–‡å˜åŒ–å¯¹æœºå™¨äººå­¦ä¹ è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„åˆ†æ•£æŸå¤±ï¼Œåœ¨è®­ç»ƒæœŸé—´æ’æ–¥çŠ¶æ€åµŒå…¥ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›è€Œä¸ä¼šå‡æ…¢æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬åœ¨Adroitå’ŒMeta-WorldåŸºå‡†æµ‹è¯•ä»¥åŠçœŸå®ä¸–ç•Œåœºæ™¯ä¸­éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMP1è¾¾åˆ°äº†æ›´é«˜çš„å¹³å‡ä»»åŠ¡æˆåŠŸç‡ï¼Œæ¯”DP3é«˜å‡º10.2%ï¼Œæ¯”FlowPolicyé«˜å‡º7.3%ã€‚å…¶å¹³å‡æ¨ç†æ—¶é—´ä»…ä¸º6.8æ¯«ç§’ï¼Œæ¯”DP3å¿«19å€ï¼Œæ¯”FlowPolicyå¿«è¿‘2å€ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://mp1-2254.github.io/">ç½‘ç«™é“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10543v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¯¥æ–‡æœ¬ä»‹ç»äº†æœºå™¨äººæ“æ§é¢†åŸŸä¸­çš„æœºå™¨äººå­¦ä¹ æˆä¸ºä¸»æµæ–¹æ³•çš„æƒ…å†µã€‚ä¸ºè§£å†³ç”Ÿæˆæ¨¡å‹ä¸­çš„æ‰©æ•£æ¨¡å‹è¿­ä»£é‡‡æ ·æ…¢ä¸æµå¼æ–¹æ³•æ¶æ„çº¦æŸä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œæå‡ºäº†MP1æ–¹æ³•ã€‚å®ƒé€šè¿‡ç»“åˆ3Dç‚¹äº‘è¾“å…¥å’ŒMeanFlowèŒƒå¼ï¼Œåœ¨ä¸€æ¬¡ç½‘ç»œåŠŸèƒ½è¯„ä¼°ä¸­ç”ŸæˆåŠ¨ä½œè½¨è¿¹ï¼Œé¿å…äº†é¢å¤–çš„ä¸€è‡´æ€§çº¦æŸï¼Œæ¶ˆé™¤äº†æ•°å€¼ODEæ±‚è§£å™¨åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„è¯¯å·®ï¼Œä»è€Œç”Ÿæˆæ›´ç²¾ç¡®çš„è½¨è¿¹ã€‚æ­¤å¤–ï¼ŒMP1è¿˜ç»“åˆäº†CFGï¼Œæé«˜äº†è½¨è¿¹çš„å¯æ§æ€§ï¼ŒåŒæ—¶ä¿æŒäº†1-NFEæ¨ç†é€Ÿåº¦ï¼Œæœªé‡æ–°å¼•å…¥ç»“æ„æ€§çº¦æŸã€‚ä¸ºè§£å†³æœºå™¨äººå­¦ä¹ ä¸­åœºæ™¯ä¸Šä¸‹æ–‡å˜åŒ–çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯few-shotå­¦ä¹ ä¸­çš„å…³é”®é—®é¢˜ï¼Œå¼•å…¥äº†åˆ†æ•£æŸå¤±ï¼Œåœ¨è®­ç»ƒæœŸé—´æ’æ–¥çŠ¶æ€åµŒå…¥ï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›è€Œä¸ä¼šå‡æ…¢æ¨ç†é€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMP1åœ¨Adroitå’ŒMeta-WorldåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ›´é«˜çš„ä»»åŠ¡æˆåŠŸç‡ï¼Œå¹³å‡æ¨ç†æ—¶é—´ä¹Ÿå¾ˆå¿«ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººå­¦ä¹ åœ¨æ“æ§é¢†åŸŸä¸­è¢«å¹¿æ³›é‡‡ç”¨ï¼Œä½†ç”Ÿæˆæ¨¡å‹é¢ä¸´æ‰©æ•£æ¨¡å‹è¿­ä»£é‡‡æ ·æ…¢å’Œæµå¼æ–¹æ³•æ¶æ„çº¦æŸä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>MP1æ–¹æ³•é€šè¿‡ç»“åˆ3Dç‚¹äº‘è¾“å…¥å’ŒMeanFlowèŒƒå¼åœ¨ä¸€æ¬¡ç½‘ç»œåŠŸèƒ½è¯„ä¼°ä¸­ç”ŸæˆåŠ¨ä½œè½¨è¿¹ï¼Œé¿å…äº†é¢å¤–çš„ä¸€è‡´æ€§çº¦æŸã€‚</li>
<li>MP1æ¶ˆé™¤äº†æ•°å€¼ODEæ±‚è§£å™¨åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„è¯¯å·®ï¼Œç”Ÿæˆæ›´ç²¾ç¡®çš„è½¨è¿¹ã€‚</li>
<li>MP1ç»“åˆäº†CFGä»¥æé«˜è½¨è¿¹å¯æ§æ€§ï¼ŒåŒæ—¶ä¿æŒå¿«é€Ÿæ¨ç†é€Ÿåº¦ï¼Œæœªå¼•å…¥æ–°çš„ç»“æ„æ€§çº¦æŸã€‚</li>
<li>åœºæ™¯ä¸Šä¸‹æ–‡å˜åŒ–å¯¹æœºå™¨äººå­¦ä¹ è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯few-shotå­¦ä¹ ä¸­ã€‚</li>
<li>MP1å¼•å…¥äº†åˆ†æ•£æŸå¤±ï¼Œä»¥æé«˜åœ¨è®­ç»ƒæœŸé—´çš„çŠ¶æ€åµŒå…¥æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶ä¸å¢åŠ æ¨ç†æ—¶é—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10543">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ee0a81db793a70d98676fbe9f6e1c35a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-946f4d7b8aa8a9fb7e653a7ef0993433.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ce9667dc018cc8b3d70e840a5a770fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac013d3879bb83ca5d9f95509bdd33e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a0ddc44273978c2f8c4d3d25c9c06e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ef031bbe27112ed658fd61df6672fe0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Graph-World-Model"><a href="#Graph-World-Model" class="headerlink" title="Graph World Model"></a>Graph World Model</h2><p><strong>Authors:Tao Feng, Yexin Wu, Guanyu Lin, Jiaxuan You</strong></p>
<p>World models (WMs) demonstrate strong capabilities in prediction, generation, and planning tasks. Existing WMs primarily focus on unstructured data and cannot leverage the ubiquitous structured data, often represented as graphs, in the digital world. While multiple graph foundation models have been proposed, they focus on graph learning tasks and cannot extend to diverse multi-modal data and interdisciplinary tasks. To address these challenges, we propose the Graph World Model (GWM), a world model that supports both unstructured and graph-structured states with multi-modal information and represents diverse tasks as actions. The core of a GWM is a generic message-passing algorithm to aggregate structured information, either over a unified multi-modal token space by converting multi-modal data into text (GWM-T) or a unified multi-modal embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces action nodes to support diverse tasks, where action nodes are linked to other nodes via direct reference or similarity computation. Extensive experiments on six tasks from diverse domains, including multi-modal generation and matching, recommendation, graph prediction, multi-agent, retrieval-augmented generation, and planning and optimization, show that the same GWM outperforms or matches domain-specific baselinesâ€™ performance, benefits from multi-hop structures, and demonstrates strong zero-shot&#x2F;few-shot capabilities on unseen new tasks. Our code for GWM is released at <a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/GWM">https://github.com/ulab-uiuc/GWM</a>. </p>
<blockquote>
<p>ä¸–ç•Œæ¨¡å‹ï¼ˆWMsï¼‰åœ¨é¢„æµ‹ã€ç”Ÿæˆå’Œè§„åˆ’ä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç°æœ‰çš„WMsä¸»è¦å…³æ³¨éç»“æ„åŒ–æ•°æ®ï¼Œæ— æ³•åˆ©ç”¨æ™®éå­˜åœ¨çš„ç»“æ„åŒ–æ•°æ®ï¼ˆé€šå¸¸è¡¨ç¤ºä¸ºå›¾ï¼‰åœ¨æ•°å­—ä¸–ç•Œä¸­çš„ä»·å€¼ã€‚è™½ç„¶å·²æå‡ºå¤šä¸ªå›¾åŸºç¡€æ¨¡å‹ï¼Œä½†å®ƒä»¬ä¸»è¦å…³æ³¨å›¾å­¦ä¹ ä»»åŠ¡ï¼Œæ— æ³•æ‰©å±•åˆ°å¤šæ ·åŒ–çš„å¤šæ¨¡æ€æ•°æ®å’Œè·¨å­¦ç§‘ä»»åŠ¡ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†å›¾ä¸–ç•Œæ¨¡å‹ï¼ˆGWMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ”¯æŒéç»“æ„åŒ–å›¾å’Œç»“æ„åŒ–çŠ¶æ€çš„å¤šæ¨¡æ€ä¿¡æ¯çš„ä¸–ç•Œæ¨¡å‹ï¼Œå®ƒå°†å„ç§ä»»åŠ¡è¡¨ç¤ºä¸ºåŠ¨ä½œã€‚GWMçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªé€šç”¨æ¶ˆæ¯ä¼ é€’ç®—æ³•ï¼Œç”¨äºèšåˆç»“æ„åŒ–ä¿¡æ¯ï¼Œæ— è®ºæ˜¯åœ¨ç»Ÿä¸€çš„å¤šæ¨¡æ€ç¬¦å·ç©ºé—´ï¼ˆé€šè¿‡å°†å¤šæ¨¡æ€æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬ï¼‰è¿˜æ˜¯ç»Ÿä¸€çš„å¤šæ¨¡æ€åµŒå…¥ç©ºé—´ï¼ˆé€šè¿‡ç‰¹å®šæ¨¡æ€çš„ç¼–ç å™¨ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒGWMå¼•å…¥äº†åŠ¨ä½œèŠ‚ç‚¹æ¥æ”¯æŒå„ç§ä»»åŠ¡ï¼ŒåŠ¨ä½œèŠ‚ç‚¹é€šè¿‡ç›´æ¥å¼•ç”¨æˆ–ç›¸ä¼¼æ€§è®¡ç®—ä¸å…¶ä»–èŠ‚ç‚¹é“¾æ¥ã€‚åœ¨æ¥è‡ªä¸åŒé¢†åŸŸçš„å…­ä¸ªä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒåŒä¸€GWMè¶…è¶Šäº†æˆ–åŒ¹é…äº†é¢†åŸŸç‰¹å®šçš„åŸºçº¿æ€§èƒ½ï¼Œå—ç›Šäºå¤šè·³ç»“æ„ï¼Œå¹¶åœ¨æœªè§çš„æ–°ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬&#x2F;å°æ ·æœ¬èƒ½åŠ›ã€‚æˆ‘ä»¬çš„GWMä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/GWM%E3%80%82">https://github.com/ulab-uiuc/GWMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10539v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸–ç•Œæ¨¡å‹ï¼ˆWMsï¼‰åœ¨é¢„æµ‹ã€ç”Ÿæˆå’Œè§„åˆ’ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬ä¸»è¦å…³æ³¨äºéç»“æ„åŒ–æ•°æ®ï¼Œæ— æ³•åˆ©ç”¨æ™®éå­˜åœ¨çš„ç»“æ„åŒ–æ•°æ®ï¼ˆé€šå¸¸ä»¥å›¾å½¢è¡¨ç¤ºï¼‰ã€‚ä¸ºè§£å†³æ­¤æŒ‘æˆ˜ï¼Œæå‡ºäº†å›¾ä¸–ç•Œæ¨¡å‹ï¼ˆGWMï¼‰ï¼Œå®ƒæ”¯æŒéç»“æ„åŒ–ä¸å›¾å½¢ç»“æ„åŒ–çŠ¶æ€ï¼Œå…·å¤‡å¤šæ¨¡å¼ä¿¡æ¯ï¼Œå¹¶å°†å„ç§ä»»åŠ¡è¡¨ç¤ºä¸ºè¡ŒåŠ¨ã€‚GWMçš„æ ¸å¿ƒæ˜¯é€šç”¨æ¶ˆæ¯ä¼ é€’ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯é€šè¿‡å°†å¤šæ¨¡å¼æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬ï¼ˆGWM-Tï¼‰æˆ–åœ¨æ¨¡æ€ç‰¹å®šç¼–ç å™¨ä¸Šçš„ç»Ÿä¸€å¤šæ¨¡å¼åµŒå…¥ç©ºé—´ï¼ˆGWM-Eï¼‰æ¥èšåˆç»“æ„åŒ–ä¿¡æ¯ã€‚GWMå¼•å…¥è¡ŒåŠ¨èŠ‚ç‚¹ä»¥æ”¯æŒå„ç§ä»»åŠ¡ï¼Œè¡ŒåŠ¨èŠ‚ç‚¹é€šè¿‡ç›´æ¥å¼•ç”¨æˆ–ç›¸ä¼¼æ€§è®¡ç®—ä¸å…¶ä»–èŠ‚ç‚¹é“¾æ¥ã€‚åœ¨æ¥è‡ªä¸åŒé¢†åŸŸçš„å…­ä¸ªä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒåŒä¸€GWMä¼˜äºæˆ–åŒ¹é…é¢†åŸŸç‰¹å®šåŸºå‡†çº¿çš„æ€§èƒ½ï¼Œå—ç›Šäºå¤šè·³ç»“æ„ï¼Œå¹¶åœ¨æœªè§çš„æ–°ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬&#x2F;å°æ ·æœ¬èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸–ç•Œæ¨¡å‹ï¼ˆWMsï¼‰å…·æœ‰å¼ºå¤§çš„é¢„æµ‹ã€ç”Ÿæˆå’Œè§„åˆ’èƒ½åŠ›ï¼Œä½†ä¸»è¦å¤„ç†éç»“æ„åŒ–æ•°æ®ã€‚</li>
<li>å›¾ä¸–ç•Œæ¨¡å‹ï¼ˆGWMï¼‰æ”¯æŒéç»“æ„åŒ–å’Œå›¾å½¢ç»“æ„åŒ–æ•°æ®ï¼Œå¹¶å¤„ç†å¤šæ¨¡å¼ä¿¡æ¯ã€‚</li>
<li>GWMé€šè¿‡é€šç”¨æ¶ˆæ¯ä¼ é€’ç®—æ³•èšåˆç»“æ„åŒ–ä¿¡æ¯ï¼Œæä¾›ä¸¤ç§å®ç°æ–¹å¼ï¼šGWM-Tï¼ˆé€šè¿‡æ–‡æœ¬è½¬æ¢ï¼‰å’ŒGWM-Eï¼ˆé€šè¿‡æ¨¡æ€ç‰¹å®šç¼–ç å™¨ï¼‰ã€‚</li>
<li>GWMå¼•å…¥è¡ŒåŠ¨èŠ‚ç‚¹ä»¥æ”¯æŒå¤šç§ä»»åŠ¡ï¼Œè¿™äº›è¡ŒåŠ¨èŠ‚ç‚¹ä¸å…¶ä»–èŠ‚ç‚¹é€šè¿‡ç›´æ¥å¼•ç”¨æˆ–ç›¸ä¼¼æ€§è®¡ç®—é“¾æ¥ã€‚</li>
<li>GWMåœ¨å¤šä¸ªé¢†åŸŸä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¤šæ¨¡å¼ç”Ÿæˆä¸åŒ¹é…ã€æ¨èã€å›¾å½¢é¢„æµ‹ã€å¤šæ™ºèƒ½ä½“ã€æ£€ç´¢å¢å¼ºç”Ÿæˆã€è§„åˆ’å’Œä¼˜åŒ–ã€‚</li>
<li>GWMå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬&#x2F;å°æ ·æœ¬å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨æœªè§çš„æ–°ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10539">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6f2068d863c19b175333e6383f0cc4e2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b968038a3139062b53b5ec99c27139b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e77798f7a4dc2cff6cf149c9517355d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-270049d6a03e8b6b83728b05c101bc28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25783fdf45ad707b52bfde0f775336fd.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Domain-Borders-Are-There-to-Be-Crossed-With-Federated-Few-Shot-Adaptation"><a href="#Domain-Borders-Are-There-to-Be-Crossed-With-Federated-Few-Shot-Adaptation" class="headerlink" title="Domain Borders Are There to Be Crossed With Federated Few-Shot   Adaptation"></a>Domain Borders Are There to Be Crossed With Federated Few-Shot   Adaptation</h2><p><strong>Authors:Manuel RÃ¶der, Christoph Raab, Frank-Michael Schleif</strong></p>
<p>Federated Learning has emerged as a leading paradigm for decentralized, privacy-preserving learning, particularly relevant in the era of interconnected edge devices equipped with sensors. However, the practical implementation of Federated Learning faces three primary challenges: the need for human involvement in costly data labelling processes for target adaptation, covariate shift in client device data collection due to environmental factors affecting sensors, leading to discrepancies between source and target samples, and the impracticality of continuous or regular model updates in resource-constrained environments due to limited data transmission capabilities and technical constraints on channel availability and energy efficiency. To tackle these issues, we expand upon an efficient and scalable Federated Learning framework tailored for real-world client adaptation in industrial settings. This framework leverages a pre-trained source model comprising a deep backbone, an adaptation module, and a classifier running on a powerful server. By freezing the backbone and classifier during client adaptation on resource-constrained devices, we allow the domain adaptive linear layer to handle target domain adaptation, thus minimizing overall computational overhead. Furthermore, this setup, designated as FedAcross+, is extended to encompass the processing of streaming data, thereby rendering the solution suitable for non-stationary environments. Extensive experimental results demonstrate the effectiveness of FedAcross+ in achieving competitive adaptation on low-end client devices with limited target samples, successfully addressing the challenge of domain shift. Moreover, our framework accommodates sporadic model updates within resource-constrained environments, ensuring practical and seamless deployment. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ å·²æˆä¸ºå»ä¸­å¿ƒåŒ–ã€ä¿æŠ¤éšç§çš„å­¦ä¹ çš„ä¸»è¦èŒƒå¼ï¼Œç‰¹åˆ«æ˜¯åœ¨é…å¤‡ä¼ æ„Ÿå™¨çš„äº’è”è¾¹ç¼˜è®¾å¤‡æ—¶ä»£ã€‚ç„¶è€Œï¼Œè”é‚¦å­¦ä¹ çš„å®é™…åº”ç”¨é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šéœ€è¦äººç±»å‚ä¸æ˜‚è´µçš„æ•°æ®æ ‡è®°è¿‡ç¨‹ä»¥é€‚åº”ç›®æ ‡ï¼Œç”±äºå½±å“ä¼ æ„Ÿå™¨çš„ç¯å¢ƒå› ç´ å¯¼è‡´å®¢æˆ·ç«¯è®¾å¤‡æ•°æ®æ”¶é›†ä¸­çš„åå˜é‡åç§»ï¼Œä»è€Œå¯¼è‡´æºæ ·æœ¬å’Œç›®æ ‡æ ·æœ¬ä¹‹é—´çš„å·®å¼‚ï¼Œä»¥åŠç”±äºæ•°æ®ä¼ è¾“èƒ½åŠ›æœ‰é™å’Œé€šé“å¯ç”¨æ€§å’Œèƒ½æ•ˆæ–¹é¢çš„æŠ€æœ¯çº¦æŸï¼Œèµ„æºå—é™ç¯å¢ƒä¸­è¿ç»­æˆ–å®šæœŸæ¨¡å‹æ›´æ–°çš„ä¸åˆ‡å®é™…æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ‰©å±•äº†ä¸€ä¸ªé«˜æ•ˆä¸”å¯æ‰©å±•çš„è”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨é€‚åº”å·¥ä¸šç¯å¢ƒä¸­çš„çœŸå®ä¸–ç•Œå®¢æˆ·ç«¯ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„æºæ¨¡å‹ï¼ŒåŒ…æ‹¬æ·±åº¦ä¸»å¹²ã€é€‚é…æ¨¡å—å’Œåˆ†ç±»å™¨ï¼Œåœ¨åŠŸèƒ½å¼ºå¤§çš„æœåŠ¡å™¨ä¸Šè¿è¡Œã€‚é€šè¿‡åœ¨å®¢æˆ·ç«¯é€‚åº”èµ„æºå—é™çš„è®¾å¤‡æ—¶å†»ç»“ä¸»å¹²å’Œåˆ†ç±»å™¨ï¼Œæˆ‘ä»¬å…è®¸åŸŸè‡ªé€‚åº”çº¿æ€§å±‚å¤„ç†ç›®æ ‡åŸŸé€‚åº”ï¼Œä»è€Œæœ€å°åŒ–æ€»ä½“è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œè¿™ä¸ªè¢«ç§°ä¸ºFedAcross+çš„è®¾ç½®è¢«æ‰©å±•ä»¥æ¶µç›–æµå¼æ•°æ®çš„å¤„ç†ï¼Œä»è€Œä½¿è§£å†³æ–¹æ¡ˆé€‚åˆéé™æ­¢ç¯å¢ƒã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFedAcross+åœ¨ä½ç«¯å®¢æˆ·ç«¯è®¾å¤‡ä¸Šå®ç°å…·æœ‰ç«äº‰åŠ›çš„é€‚åº”ç›®æ ‡æ ·æœ¬æ–¹é¢éå¸¸æœ‰æ•ˆï¼ŒæˆåŠŸè§£å†³äº†é¢†åŸŸåç§»çš„æŒ‘æˆ˜ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿåœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­å®¹çº³é—´æ­‡æ€§æ¨¡å‹æ›´æ–°ï¼Œç¡®ä¿å®ç”¨ä¸”æ— ç¼çš„éƒ¨ç½²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10160v1">PDF</a> Extension of <a target="_blank" rel="noopener" href="http://dx.doi.org/10.5220/0012351900003654">http://dx.doi.org/10.5220/0012351900003654</a></p>
<p><strong>Summary</strong><br>     è”é‚¦å­¦ä¹ å·²æˆä¸ºå»ä¸­å¿ƒåŒ–ã€ä¿æŠ¤éšç§çš„å­¦ä¹ ä¸»æµèŒƒå¼ï¼Œå°¤å…¶åœ¨é…å¤‡ä¼ æ„Ÿå™¨çš„äº’è”è¾¹ç¼˜è®¾å¤‡æ—¶ä»£å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼Œå…¶å®è·µé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ‰©å±•äº†ä¸€ä¸ªé«˜æ•ˆã€å¯æ‰©å±•çš„è”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºå·¥ä¸šç¯å¢ƒä¸­çš„çœŸå®å®¢æˆ·ç«¯é€‚åº”ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒæºæ¨¡å‹ï¼ŒåŒ…æ‹¬æ·±åº¦ä¸»å¹²ã€é€‚é…æ¨¡å—å’Œåˆ†ç±»å™¨åœ¨å¼ºå¤§æœåŠ¡å™¨ä¸Šè¿è¡Œã€‚é€šè¿‡å†»ç»“ä¸»å¹²å’Œåˆ†ç±»å™¨ï¼Œåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šè¿›è¡Œå®¢æˆ·ç«¯é€‚é…ï¼Œå…è®¸é¢†åŸŸè‡ªé€‚åº”çº¿æ€§å±‚å¤„ç†ç›®æ ‡é¢†åŸŸé€‚é…ï¼Œä»è€Œæœ€å°åŒ–æ•´ä½“è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼ŒFedAcross+ æ¡†æ¶èƒ½å¤Ÿå¤„ç†æµå¼æ•°æ®ï¼Œä½¿å…¶é€‚ç”¨äºéé™æ€ç¯å¢ƒã€‚å®éªŒç»“æœè¯æ˜äº† FedAcross+ åœ¨å…·æœ‰æœ‰é™ç›®æ ‡æ ·æœ¬çš„ä½ç«¯å®¢æˆ·ç«¯è®¾å¤‡ä¸Šå®ç°ç«äº‰é€‚é…çš„æœ‰æ•ˆæ€§ï¼ŒæˆåŠŸè§£å†³äº†é¢†åŸŸåç§»çš„æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶æ”¯æŒèµ„æºå—é™ç¯å¢ƒä¸­å¶å°”çš„æ¨¡å‹æ›´æ–°ï¼Œç¡®ä¿å®é™…éƒ¨ç½²çš„å®ç”¨æ€§å’Œæ— ç¼æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ å·²æˆä¸ºå»ä¸­å¿ƒåŒ–ã€éšç§ä¿æŠ¤çš„ä¸»æµå­¦ä¹ èŒƒå¼ï¼Œå°¤å…¶åœ¨è¾¹ç¼˜è®¾å¤‡æ—¶ä»£å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>è”é‚¦å­¦ä¹ çš„å®é™…åº”ç”¨é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šç›®æ ‡é€‚åº”çš„æ•°æ®æ ‡æ³¨æˆæœ¬ã€æºä¸é¶æ ·æœ¬ä¹‹é—´çš„ç¯å¢ƒå·®å¼‚å¯¼è‡´çš„åå˜é‡åç§»ä»¥åŠèµ„æºå—é™ç¯å¢ƒä¸­çš„è¿ç»­æˆ–å®šæœŸæ¨¡å‹æ›´æ–°çš„ä¸å®ç”¨æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹å·¥ä¸šç¯å¢ƒä¸­çœŸå®å®¢æˆ·ç«¯é€‚åº”çš„é«˜æ•ˆè”é‚¦å­¦ä¹ æ¡†æ¶â€”â€”FedAcross+ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒæºæ¨¡å‹å¹¶å†»ç»“ä¸»å¹²å’Œåˆ†ç±»å™¨ï¼Œä»¥é™ä½è®¡ç®—å¼€é”€ï¼Œå¹¶å®ç°ç›®æ ‡é¢†åŸŸé€‚é…ã€‚</li>
<li>FedAcross+ èƒ½å¤Ÿå¤„ç†æµå¼æ•°æ®ï¼Œé€‚åº”éé™æ€ç¯å¢ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒFedAcross+ åœ¨å…·æœ‰æœ‰é™ç›®æ ‡æ ·æœ¬çš„ä½ç«¯å®¢æˆ·ç«¯è®¾å¤‡ä¸Šå®ç°æœ‰æ•ˆçš„ç«äº‰é€‚é…ï¼ŒæˆåŠŸè§£å†³é¢†åŸŸåç§»æŒ‘æˆ˜ã€‚</li>
<li>FedAcross+ æ¡†æ¶æ”¯æŒèµ„æºå—é™ç¯å¢ƒä¸­çš„å¶å°”æ¨¡å‹æ›´æ–°ï¼Œç¡®ä¿å®é™…éƒ¨ç½²çš„å®ç”¨æ€§å’Œæ— ç¼æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10160">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-85387202a797c5b225fce31a3c99c78c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d16da07d7c3941889995d95245b5f54d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8082bff2bd19e4a0a63a3f075fcdf594.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="DEARLi-Decoupled-Enhancement-of-Recognition-and-Localization-for-Semi-supervised-Panoptic-Segmentation"><a href="#DEARLi-Decoupled-Enhancement-of-Recognition-and-Localization-for-Semi-supervised-Panoptic-Segmentation" class="headerlink" title="DEARLi: Decoupled Enhancement of Recognition and Localization for   Semi-supervised Panoptic Segmentation"></a>DEARLi: Decoupled Enhancement of Recognition and Localization for   Semi-supervised Panoptic Segmentation</h2><p><strong>Authors:Ivan MartinoviÄ‡, Josip Å ariÄ‡, Marin OrÅ¡iÄ‡, Matej Kristan, SiniÅ¡a Å egviÄ‡</strong></p>
<p>Pixel-level annotation is expensive and time-consuming. Semi-supervised segmentation methods address this challenge by learning models on few labeled images alongside a large corpus of unlabeled images. Although foundation models could further account for label scarcity, effective mechanisms for their exploitation remain underexplored. We address this by devising a novel semi-supervised panoptic approach fueled by two dedicated foundation models. We enhance recognition by complementing unsupervised mask-transformer consistency with zero-shot classification of CLIP features. We enhance localization by class-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting decoupled enhancement of recognition and localization (DEARLi) particularly excels in the most challenging semi-supervised scenarios with large taxonomies and limited labeled data. Moreover, DEARLi outperforms the state of the art in semi-supervised semantic segmentation by a large margin while requiring 8x less GPU memory, in spite of being trained only for the panoptic objective. We observe 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/helen1c/DEARLi">https://github.com/helen1c/DEARLi</a>. </p>
<blockquote>
<p>åƒç´ çº§æ ‡æ³¨æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚åŠç›‘ç£åˆ†å‰²æ–¹æ³•é€šè¿‡å°‘æ•°æ ‡æ³¨å›¾åƒå’Œå¤§é‡æœªæ ‡æ³¨å›¾åƒæ•°æ®é›†ä¸Šçš„æ¨¡å‹å­¦ä¹ æ¥è§£å†³è¿™ä¸€æŒ‘æˆ˜ã€‚è™½ç„¶åŸºç¡€æ¨¡å‹å¯ä»¥è¿›ä¸€æ­¥è§£å†³æ ‡ç­¾ç¨€ç¼ºé—®é¢˜ï¼Œä½†å¯¹å…¶åˆ©ç”¨çš„æœ‰æ•ˆæœºåˆ¶ä»è¢«å¿½è§†ã€‚æˆ‘ä»¬é€šè¿‡å¼€å‘ä¸€ç§æ–°çš„åŠç›‘ç£æ³›å…¨æ™¯æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥æ–¹æ³•ç”±ä¸¤ä¸ªä¸“ç”¨çš„åŸºç¡€æ¨¡å‹æ¨åŠ¨ã€‚æˆ‘ä»¬é€šè¿‡å°†æ— ç›‘ç£æ©è†œè½¬æ¢å™¨çš„ä¸€è‡´æ€§è¡¥å……ä¸CLIPç‰¹å¾çš„é›¶æ ·æœ¬åˆ†ç±»ç›¸ç»“åˆï¼Œæé«˜è¯†åˆ«èƒ½åŠ›ã€‚é€šè¿‡å…³äºSAMä¼ªæ ‡ç­¾çš„ç±»åˆ«æ— å…³è§£ç å™¨é¢„çƒ­æ¥æé«˜å®šä½èƒ½åŠ›ã€‚è¿™ç§å»è€¦çš„è¯†åˆ«å’Œå®šä½å¢å¼ºï¼ˆDEARLiï¼‰æŠ€æœ¯åœ¨æ ‡ç­¾æ•°æ®æœ‰é™çš„å¤§å‹åˆ†ç±»å­¦ä¸­é¢ä¸´æœ€å…·æŒ‘æˆ˜æ€§çš„åŠç›‘ç£åœºæ™¯æ—¶å°¤å…¶è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œå°½ç®¡DEARLiä»…é’ˆå¯¹å…¨æ™¯ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨åŠç›‘ç£è¯­ä¹‰åˆ†å‰²æ–¹é¢ä»å¤§å¤§ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒåŒæ—¶éœ€è¦ä½¿ç”¨çš„GPUå†…å­˜å‡å°‘äº†8å€ã€‚åœ¨ADE20Kä¸Šï¼Œä»…ä½¿ç”¨158ä¸ªæ ‡æ³¨å›¾åƒå°±å®ç°äº†29.9çš„PQå’Œ38.9çš„mIoUã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/helen1c/DEARLi%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/helen1c/DEARLiæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10118v1">PDF</a> ICCV 2025 Findings Workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºåŠç›‘ç£å­¦ä¹ æ–¹æ³•çš„åƒç´ çº§æ ‡æ³¨é—®é¢˜è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ç»“åˆäº†ä¸¤ç§ä¸“é—¨çš„æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨åŠç›‘ç£å…¨æ™¯åˆ†ææŠ€æœ¯å’Œä¸€ç³»åˆ—ç‰¹å®šå¢å¼ºæ‰‹æ®µæé«˜è¯†åˆ«ä¸å®šä½èƒ½åŠ›ã€‚è¯¥æ–¹æ³•åœ¨å…·æœ‰å¤§é‡åˆ†ç±»å’Œæœ‰é™æ ‡ç­¾æ•°æ®çš„æŒ‘æˆ˜æ€§åŠç›‘ç£åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒåŒæ—¶å‡å°‘äº†GPUå†…å­˜éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥æ–¹æ³•é‡‡ç”¨åŠç›‘ç£å­¦ä¹ æ–¹æ³•è§£å†³äº†åƒç´ çº§æ ‡æ³¨çš„æ˜‚è´µå’Œè€—æ—¶é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç»“åˆä¸¤ç§ä¸“é—¨çš„æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨åŠç›‘ç£å…¨æ™¯åˆ†ææŠ€æœ¯å¢å¼ºäº†è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç‰¹å®šå¢å¼ºæ‰‹æ®µæé«˜æ¨¡å‹çš„å®šä½èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŠç›‘ç£åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯é‚£äº›æ¶‰åŠå¤§é‡åˆ†ç±»å’Œæœ‰é™æ ‡ç­¾æ•°æ®çš„åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10118">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-30c85dec95bfedacc4421be7ef6bba1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-979a0aba0d44ce4779e52797edc2aa13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df399fd115c07e260f7908d5f08cbff1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97904fb6e81c78fdc0e3878fc7be2a62.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f418d4ff981ebc2ba76d54be87328cae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8856e45de90c983eb08628bbacfa8419.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Automating-SPARQL-Query-Translations-between-DBpedia-and-Wikidata"><a href="#Automating-SPARQL-Query-Translations-between-DBpedia-and-Wikidata" class="headerlink" title="Automating SPARQL Query Translations between DBpedia and Wikidata"></a>Automating SPARQL Query Translations between DBpedia and Wikidata</h2><p><strong>Authors:Malte Christian Bartels, Debayan Banerjee, Ricardo Usbeck</strong></p>
<p>This paper investigates whether state-of-the-art Large Language Models (LLMs) can automatically translate SPARQL between popular Knowledge Graph (KG) schemas. We focus on translations between the DBpedia and Wikidata KG, and later on DBLP and OpenAlex KG. This study addresses a notable gap in KG interoperability research by rigorously evaluating LLM performance on SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100 DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and Mistral-Large-Instruct-2407 are selected based on their sizes and architectures and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs were compared with gold answers, and resulting errors were categorized. We find that the performance varies markedly across models and prompting strategies, and that translations for Wikidata to DBpedia work far better than translations for DBpedia to Wikidata. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨ç ”ç©¶æœ€å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤§æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿè‡ªåŠ¨ç¿»è¯‘çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¹‹é—´æµè¡Œçš„SPARQLæŸ¥è¯¢ã€‚æˆ‘ä»¬é‡ç‚¹å…³æ³¨DBpediaå’ŒWikidata KGä¹‹é—´çš„ç¿»è¯‘ï¼Œåç»­ç ”ç©¶è¿˜åŒ…æ‹¬DBLPå’ŒOpenAlex KGä¹‹é—´çš„ç¿»è¯‘ã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸¥æ ¼è¯„ä¼°LLMåœ¨SPARQL-to-SPARQLç¿»è¯‘æ–¹é¢çš„æ€§èƒ½ï¼Œè§£å†³äº†çŸ¥è¯†å›¾è°±äº’æ“ä½œæ€§ç ”ç©¶ä¸­çš„ä¸€ä¸ªæ˜¾è‘—ç©ºç™½ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªåŸºå‡†æµ‹è¯•é›†ï¼Œç¬¬ä¸€ä¸ªåŸºå‡†æµ‹è¯•é›†å¯¹é½QALD-9-Plusä¸­çš„100ä¸ªDBpedia-WikidataæŸ¥è¯¢ï¼›ç¬¬äºŒä¸ªåŸºå‡†æµ‹è¯•é›†åŒ…å«å¯¹é½åˆ°OpenAlexçš„100ä¸ªDBLPæŸ¥è¯¢ï¼Œä»¥æµ‹è¯•å…¶åœ¨ç™¾ç§‘å…¨ä¹¦çŸ¥è¯†å›¾è°±ä¹‹å¤–çš„æ³›åŒ–èƒ½åŠ›ã€‚åŸºäºè§„æ¨¡ä¸æ¶æ„ï¼Œæˆ‘ä»¬é€‰æ‹©ä¸‰ä¸ªå¼€æºçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼šLlama-3-8Bã€DeepSeek-R1-Distill-Llama-70Bä»¥åŠMistral-Large-Instruct-2407è¿›è¡Œè¯•éªŒã€‚è¯•éªŒåŒ…æ‹¬é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ä»¥åŠä¸¤ç§æ€ç»´é“¾æ–¹æ³•ã€‚è¾“å‡ºç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼Œå¹¶å¯¹äº§ç”Ÿçš„é”™è¯¯è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬å‘ç°ä¸åŒæ¨¡å‹å’Œæç¤ºç­–ç•¥ä¹‹é—´çš„æ€§èƒ½å·®å¼‚å¾ˆå¤§ï¼Œå¹¶ä¸”ä»Wikidataåˆ°DBpediaçš„ç¿»è¯‘æ•ˆæœè¿œèƒœäºä»DBpediaåˆ°Wikidataçš„ç¿»è¯‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10045v1">PDF</a> 18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference   happening on September 2025</p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†æœ€å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤§æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦èƒ½è‡ªåŠ¨ç¿»è¯‘çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸­çš„SPARQLæŸ¥è¯¢è¯­å¥ã€‚ç ”ç©¶èšç„¦äºDBpediaå’ŒWikidataä»¥åŠDBLpå’ŒOpenAlexä¸¤å¤§çŸ¥è¯†å›¾è°±ä¹‹é—´çš„ç¿»è¯‘ï¼Œå¡«è¡¥äº†çŸ¥è¯†å›¾è°±äº’æ“ä½œæ€§ç ”ç©¶çš„ç©ºç™½ã€‚è®ºæ–‡æ„å»ºäº†ä¸¤ä¸ªåŸºå‡†æµ‹è¯•é›†ï¼Œè¯„ä¼°äº†ä¸‰ä¸ªå¼€æºå¤§æ¨¡å‹åœ¨ä¸åŒç¿»è¯‘åœºæ™¯ä¸‹çš„æ€§èƒ½ï¼Œå¹¶å‘ç°æ€§èƒ½åœ¨ä¸åŒæ¨¡å‹å’Œæç¤ºç­–ç•¥ä¹‹é—´æœ‰å¾ˆå¤§å·®å¼‚ï¼Œä¸”ä»Wikidataåˆ°DBpediaçš„ç¿»è¯‘æ•ˆæœä¼˜äºä»DBpediaåˆ°Wikidataçš„ç¿»è¯‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç¿»è¯‘SPARQLæŸ¥è¯¢è¯­å¥çš„å¯è¡Œæ€§ã€‚</li>
<li>ç ”ç©¶èšç„¦äºDBpediaå’ŒWikidataä»¥åŠDBLpå’ŒOpenAlexä¸¤å¤§çŸ¥è¯†å›¾è°±ä¹‹é—´çš„ç¿»è¯‘ã€‚</li>
<li>è®ºæ–‡æ„å»ºäº†ä¸¤ä¸ªåŸºå‡†æµ‹è¯•é›†ï¼Œç”¨äºè¯„ä¼°LLMåœ¨SPARQL-to-SPARQLç¿»è¯‘æ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>è¯„ä¼°äº†ä¸‰ä¸ªå¼€æºå¤§æ¨¡å‹ï¼ŒåŒ…æ‹¬Llama-3-8Bã€DeepSeek-R1-Distill-Llama-70Bå’ŒMistral-Large-Instruct-2407ã€‚</li>
<li>è®ºæ–‡é‡‡ç”¨äº†é›¶æ ·æœ¬ã€å°æ ·ä¾‹ä»¥åŠä¸¤ç§æ€ç»´é“¾æ–¹æ³•è¿›è¡Œäº†æµ‹è¯•ã€‚</li>
<li>æ€§èƒ½åœ¨ä¸åŒæ¨¡å‹å’Œæç¤ºç­–ç•¥ä¹‹é—´æœ‰å¾ˆå¤§å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10045">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e3e97eadff6b6b4eeafde66a265d141c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c8ae43574719591c6147893c1bc57fd8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a785e66a25e99e28342f236f4fb2468.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Deep-Hidden-Cognition-Facilitates-Reliable-Chain-of-Thought-Reasoning"><a href="#Deep-Hidden-Cognition-Facilitates-Reliable-Chain-of-Thought-Reasoning" class="headerlink" title="Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning"></a>Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning</h2><p><strong>Authors:Zijun Chen, Wenbo Hu, Richang Hong</strong></p>
<p>Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning capabilities in both large language models (LLMs) and multimodal large language models (MLLMs). However, its reliability is often undermined by the accumulation of errors in intermediate steps. This paper introduces an novel approach to calibrate the CoT reasoning accuracy by leveraging the modelâ€™s intrinsic veracity encoding. We discover that specific attention head activations reliably reflect the truthfulness of reasoning steps in CoT. Based on this insight, we train a confidence predictor to evaluate the correctness of each reasoning step using these truthfulness-sensitive activations, dynamically selecting the most plausible reasoning path via beam search. Experimental results demonstrate that our method significantly outperforms the state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and commonsense reasoning tasks, exhibiting superior accuracy and reliability in both unimodal and multimodal settings. We further validate the approach on large reasoning models, confirming its applicability to specialized reasoning models. Additionally, we explore the role of the modelâ€™s self-correction ability in CoT reasoning. This work provides a novel reliability improvement path for CoT reasoning with broad application potential. </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆChain of Thoughtï¼Œç®€ç§°CoTï¼‰æ¨ç†åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­éƒ½è¡¨ç°å‡ºäº†æ˜¾è‘—çš„æ·±åº¦æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶å¯é æ€§å¾€å¾€å—åˆ°ä¸­é—´æ­¥éª¤è¯¯å·®ç´¯ç§¯çš„å½±å“ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æ¨¡å‹å†…åœ¨çœŸå®æ€§ç¼–ç æ¥æ ¡å‡†CoTæ¨ç†ç²¾åº¦çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ç‰¹å®šçš„æ³¨æ„åŠ›å¤´æ¿€æ´»èƒ½å¤Ÿå¯é åœ°åæ˜ CoTä¸­æ¨ç†æ­¥éª¤çš„çœŸå®æ€§ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç½®ä¿¡åº¦é¢„æµ‹å™¨ï¼Œåˆ©ç”¨è¿™äº›çœŸå®æ€§æ•æ„Ÿçš„æ¿€æ´»æ¥è¯„ä¼°æ¯ä¸ªæ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¹¶é€šè¿‡é›†æŸæœç´¢åŠ¨æ€é€‰æ‹©æœ€åˆç†çš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•°å­¦ã€ç¬¦å·å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼ˆå¦‚å°‘æ ·æœ¬CoTã€è‡ªæˆ‘ä¸€è‡´æ€§ã€è‡ªæˆ‘è¯„ä¼°å¼•å¯¼é›†æŸæœç´¢ç­‰ï¼‰ï¼Œåœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€ç¯å¢ƒä¸­è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚æˆ‘ä»¬åœ¨å¤§å‹æ¨ç†æ¨¡å‹ä¸Šè¿›ä¸€æ­¥éªŒè¯äº†è¯¥æ–¹æ³•ï¼Œè¯æ˜äº†å®ƒåœ¨ä¸“ä¸šæ¨ç†æ¨¡å‹ä¸­çš„é€‚ç”¨æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†æ¨¡å‹ä¸­è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›åœ¨CoTæ¨ç†ä¸­çš„ä½œç”¨ã€‚è¿™é¡¹å·¥ä½œä¸ºCoTæ¨ç†æä¾›äº†ä¸€ç§æ–°çš„å¯é æ€§æå‡è·¯å¾„ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10007v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨æ¨¡å‹å†…åœ¨çœŸå®æ€§ç¼–ç æ¥æå‡Chain of Thoughtï¼ˆCoTï¼‰æ¨ç†å‡†ç¡®æ€§çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶å‘ç°äº†ç‰¹å®šæ³¨æ„åŠ›å¤´æ¿€æ´»èƒ½å¯é åæ˜ CoTæ¨ç†æ­¥éª¤çš„çœŸå®æ€§ï¼Œå¹¶åŸºäºè¿™ä¸€å‘ç°è®­ç»ƒäº†ä¿¡å¿ƒé¢„æµ‹å™¨æ¥è¯„ä¼°æ¯ä¸ªæ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€é€‰æ‹©æœ€åˆç†çš„æ¨ç†è·¯å¾„ï¼Œæ˜¾è‘—æé«˜äº†æ•°å­¦ã€ç¬¦å·å’Œå¸¸è¯†æ¨ç†ä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶åœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€ç¯å¢ƒä¸­è¡¨ç°å‡ºå“è¶Šå‡†ç¡®æ€§å’Œå¯é æ€§ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†æ¨¡å‹åœ¨CoTæ¨ç†ä¸­çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ–°æ–¹æ³•æ ¡å‡†CoTæ¨ç†å‡†ç¡®æ€§ï¼Œé€šè¿‡æ¨¡å‹å†…åœ¨çœŸå®æ€§ç¼–ç æå‡æ€§èƒ½ã€‚</li>
<li>å‘ç°ç‰¹å®šæ³¨æ„åŠ›å¤´æ¿€æ´»èƒ½åæ˜ CoTæ¨ç†æ­¥éª¤çš„çœŸå®æ€§ã€‚</li>
<li>è®­ç»ƒä¿¡å¿ƒé¢„æµ‹å™¨è¯„ä¼°æ¯ä¸ªæ¨ç†æ­¥éª¤çš„æ­£ç¡®æ€§ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œé€‚ç”¨äºå•æ¨¡æ€å’Œå¤šæ¨¡æ€ç¯å¢ƒã€‚</li>
<li>æ–¹æ³•æé«˜äº†æ•°å­¦ã€ç¬¦å·å’Œå¸¸è¯†æ¨ç†çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</li>
<li>æ¢ç´¢äº†æ¨¡å‹åœ¨CoTæ¨ç†ä¸­çš„è‡ªæˆ‘æ ¡æ­£èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10007">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f1545fd35875226550fb557f65be080a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d946da7c9c2b363089e0e5b9b7141f1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-677b408c1a7ca9c66ce9f7a62ad815e3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47989b3b81d7934f27fa4f0411cbc085.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Iceberg-Enhancing-HLS-Modeling-with-Synthetic-Data"><a href="#Iceberg-Enhancing-HLS-Modeling-with-Synthetic-Data" class="headerlink" title="Iceberg: Enhancing HLS Modeling with Synthetic Data"></a>Iceberg: Enhancing HLS Modeling with Synthetic Data</h2><p><strong>Authors:Zijian Ding, Tung Nguyen, Weikai Li, Aditya Grover, Yizhou Sun, Jason Cong</strong></p>
<p>Deep learning-based prediction models for High-Level Synthesis (HLS) of hardware designs often struggle to generalize. In this paper, we study how to close the generalizability gap of these models through pretraining on synthetic data and introduce Iceberg, a synthetic data augmentation approach that expands both large language model (LLM)-generated programs and weak labels of unseen design configurations. Our weak label generation method is integrated with an in-context model architecture, enabling meta-learning from actual and proximate labels. Iceberg improves the geometric mean modeling accuracy by $86.4%$ when adapt to six real-world applications with few-shot examples and achieves a $2.47\times$ and a $1.12\times$ better offline DSE performance when adapting to two different test datasets. Our open-sourced code is here: \href{<a target="_blank" rel="noopener" href="https://github.com/UCLA-VAST/iceberg%7D%7Bhttps://github.com/UCLA-VAST/iceberg%7D">https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}</a> </p>
<blockquote>
<p>é’ˆå¯¹ç¡¬ä»¶è®¾è®¡çš„é«˜çº§ç»¼åˆï¼ˆHLSï¼‰æ·±åº¦å­¦ä¹ çš„é¢„æµ‹æ¨¡å‹é€šå¸¸é¢ä¸´éš¾ä»¥æ³›åŒ–çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¦‚ä½•é€šè¿‡åˆæˆæ•°æ®çš„é¢„è®­ç»ƒæ¥ç¼©å°è¿™äº›æ¨¡å‹çš„æ³›åŒ–å·®è·ï¼Œå¹¶å¼•å…¥äº†Icebergï¼Œè¿™æ˜¯ä¸€ç§åˆæˆæ•°æ®å¢å¼ºæ–¹æ³•ï¼Œå®ƒæ‰©å±•äº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆçš„ç¨‹åºå’Œæœªè§è®¾è®¡é…ç½®çš„å¼±æ ‡ç­¾ã€‚æˆ‘ä»¬çš„å¼±æ ‡ç­¾ç”Ÿæˆæ–¹æ³•ä¸ä¸Šä¸‹æ–‡æ¨¡å‹æ¶æ„ç›¸ç»“åˆï¼Œå®ç°äº†ä»å®é™…å’Œè¿‘ä¼¼æ ‡ç­¾çš„å…ƒå­¦ä¹ ã€‚Icebergåœ¨é€‚åº”å…­ä¸ªå…·æœ‰å°‘é‡ç¤ºä¾‹çš„å®é™…åº”ç”¨æ—¶ï¼Œæé«˜äº†å‡ ä½•å¹³å‡å»ºæ¨¡ç²¾åº¦è¾¾86.4%ï¼Œåœ¨é€‚åº”ä¸¤ä¸ªä¸åŒçš„æµ‹è¯•æ•°æ®é›†æ—¶ï¼Œç¦»çº¿DSEæ€§èƒ½åˆ†åˆ«æé«˜äº†2.47å€å’Œ1.12å€ã€‚æˆ‘ä»¬çš„å¼€æºä»£ç åœ¨æ­¤ï¼š<a target="_blank" rel="noopener" href="https://github.com/UCLA-VAST/iceberg">https://github.com/UCLA-VAST/iceberg</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09948v1">PDF</a> 9 pages. accepted to ICLADâ€™25</p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ åœ¨ç¡¬ä»¶è®¾è®¡é«˜çº§ç»¼åˆï¼ˆHLSï¼‰çš„é¢„æµ‹æ¨¡å‹é€šå¸¸å­˜åœ¨æ³›åŒ–é—®é¢˜ã€‚æœ¬ç ”ç©¶é€šè¿‡é¢„è®­ç»ƒåˆæˆæ•°æ®æ¥ç¼©å°æ¨¡å‹æ³›åŒ–å·®è·ï¼Œå¹¶å¼•å…¥Icebergæ–¹æ³•ï¼Œæ‰©å±•äº†å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ç¨‹åºå’Œæœªè§è®¾è®¡é…ç½®çš„å¼±æ ‡ç­¾ã€‚é›†æˆå¼±æ ‡ç­¾ç”Ÿæˆæ–¹æ³•ä¸ä¸Šä¸‹æ–‡æ¨¡å‹æ¶æ„ï¼Œå®ç°å…ƒå­¦ä¹ ä»å®é™…å’Œè¿‘ä¼¼æ ‡ç­¾ä¸­å­¦ä¹ ã€‚Icebergåœ¨é€‚åº”å°‘æ•°å®ä¾‹çš„çœŸå®ä¸–ç•Œåº”ç”¨æ—¶ï¼Œå»ºæ¨¡ç²¾åº¦æé«˜äº†86.4%ï¼Œåœ¨é€‚åº”ä¸¤ä¸ªä¸åŒçš„æµ‹è¯•æ•°æ®é›†æ—¶ï¼Œç¦»çº¿DSEæ€§èƒ½åˆ†åˆ«æé«˜äº†2.47å€å’Œ1.12å€ã€‚ä»£ç å·²å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨ç¡¬ä»¶è®¾è®¡é«˜çº§ç»¼åˆçš„é¢„æµ‹æ¨¡å‹å­˜åœ¨æ³›åŒ–é—®é¢˜ã€‚</li>
<li>é€šè¿‡é¢„è®­ç»ƒåˆæˆæ•°æ®æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥Icebergæ–¹æ³•ï¼Œæ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ç¨‹åºå’Œæœªè§è®¾è®¡é…ç½®çš„å¼±æ ‡ç­¾ã€‚</li>
<li>é›†æˆå¼±æ ‡ç­¾ç”Ÿæˆæ–¹æ³•ä¸ä¸Šä¸‹æ–‡æ¨¡å‹æ¶æ„ï¼Œå®ç°å…ƒå­¦ä¹ ã€‚</li>
<li>Icebergæ–¹æ³•åœ¨é€‚åº”å°‘æ•°å®ä¾‹çš„çœŸå®ä¸–ç•Œåº”ç”¨æ—¶ï¼Œå»ºæ¨¡ç²¾åº¦æ˜¾è‘—æé«˜ã€‚</li>
<li>Icebergæ–¹æ³•æé«˜äº†ç¦»çº¿DSEæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09948">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1836b5ab77c82c73e7bed494e1005a4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d772916e0e70bde9cdf393e3f2ca304.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d023eef157c43c1c3b9a4872f0c63a6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22321854b0db23c5ad4ed0dbc468e4b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a75d5f089d2029aa0fedca806359fc84.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-91666131b45024402b03ea13ba19960b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f9ed1e753b7172fc14597ae7eec8104.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aade31a37f027a5e4795578ed7de486.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="ViT-ProtoNet-for-Few-Shot-Image-Classification-A-Multi-Benchmark-Evaluation"><a href="#ViT-ProtoNet-for-Few-Shot-Image-Classification-A-Multi-Benchmark-Evaluation" class="headerlink" title="ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark   Evaluation"></a>ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark   Evaluation</h2><p><strong>Authors:Abdulvahap Mutlu, ÅengÃ¼l DoÄŸan, TÃ¼rker Tuncer</strong></p>
<p>The remarkable representational power of Vision Transformers (ViTs) remains underutilized in few-shot image classification. In this work, we introduce ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical Network framework. By averaging class conditional token embeddings from a handful of support examples, ViT-ProtoNet constructs robust prototypes that generalize to novel categories under 5-shot settings. We conduct an extensive empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100, CUB-200, and CIFAR-FS, including overlapped support variants to assess robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based prototypical counterparts, achieving up to a 3.2% improvement in 5-shot accuracy and demonstrating superior feature separability in latent space. Furthermore, it outperforms or is competitive with transformer-based competitors using a more lightweight backbone. Comprehensive ablations examine the impact of transformer depth, patch size, and fine-tuning strategy. To foster reproducibility, we release code and pretrained weights. Our results establish ViT-ProtoNet as a powerful, flexible approach for few-shot classification and set a new baseline for transformer-based meta-learners. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTï¼‰çš„è¡¨ç¤ºèƒ½åŠ›åœ¨å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä¸­å°šæœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ViT-ProtoNetï¼Œå®ƒå°†ViT-Smalléª¨å¹²ç½‘é›†æˆåˆ°åŸå‹ç½‘ç»œæ¡†æ¶ä¸­ã€‚é€šè¿‡å¹³å‡å°‘é‡æ”¯æŒæ ·æœ¬çš„ç±»åˆ«æ¡ä»¶ä»¤ç‰ŒåµŒå…¥ï¼ŒViT-ProtoNetæ„å»ºäº†åœ¨5ä¸ªæ ·æœ¬è®¾ç½®ä¸‹èƒ½å¤Ÿæ¨å¹¿åˆ°æ–°å‹ç±»åˆ«çš„ç¨³å¥åŸå‹ã€‚æˆ‘ä»¬åœ¨å››ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®è¯è¯„ä¼°ï¼šMini-ImageNetã€FC100ã€CUB-200å’ŒCIFAR-FSï¼ŒåŒ…æ‹¬é‡å çš„æ”¯æŒå˜ä½“ä»¥è¯„ä¼°ç¨³å¥æ€§ã€‚åœ¨æ‰€æœ‰åˆ†å‰²ä¸­ï¼ŒViT-ProtoNetå§‹ç»ˆä¼˜äºåŸºäºCNNçš„åŸå‹å¯¹åº”æ¨¡å‹ï¼Œåœ¨5æ¬¡å°„å‡»çš„å‡†ç¡®æ€§ä¸Šæœ€å¤šæé«˜äº†3.2%ï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´ä¸­æ˜¾ç¤ºå‡ºå“è¶Šçš„ç‰¹å¾å¯åˆ†ç¦»æ€§ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ›´è½»é‡çº§éª¨å¹²ç½‘çš„æƒ…å†µä¸‹ï¼Œå®ƒä¸åŸºäºTransformerçš„ç«äº‰å¯¹æ‰‹ç›¸æ¯”å…·æœ‰ä¼˜åŠ¿æˆ–ç«äº‰åŠ›ã€‚å…¨é¢çš„ç ”ç©¶è€ƒå¯Ÿäº†Transformeræ·±åº¦ã€è¡¥ä¸å¤§å°å’Œå¾®è°ƒç­–ç•¥çš„å½±å“ã€‚ä¸ºä¿ƒè¿›å¯é‡å¤æ€§ï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä»£ç å’Œé¢„è®­ç»ƒæƒé‡ã€‚æˆ‘ä»¬çš„ç»“æœç¡®ç«‹äº†ViT-ProtoNetåœ¨å°‘æ ·æœ¬åˆ†ç±»ä¸­çš„å¼ºå¤§å’Œçµæ´»æ–¹æ³•ï¼Œå¹¶ä¸ºåŸºäºTransformerçš„å…ƒå­¦ä¹ è€…è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09299v1">PDF</a> All codes are available at   <a target="_blank" rel="noopener" href="https://github.com/abdulvahapmutlu/vit-protonet">https://github.com/abdulvahapmutlu/vit-protonet</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†å°†Vision Transformerï¼ˆViTï¼‰ä¸Prototypical Networkæ¡†æ¶ç›¸ç»“åˆçš„ViT-ProtoNetæ¨¡å‹ï¼Œç”¨äºå°æ ·æœ¬å›¾åƒåˆ†ç±»ã€‚é€šè¿‡å¹³å‡å°‘é‡æ”¯æŒæ ·æœ¬çš„ç±»åˆ«æ¡ä»¶ä»¤ç‰ŒåµŒå…¥æ¥æ„å»ºç¨³å¥åŸå‹ï¼ŒViT-ProtoNetåœ¨5ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹å¯æ¨å¹¿åˆ°æ–°å‹ç±»åˆ«ã€‚åœ¨å››ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒViT-ProtoNetåœ¨åŸºäºCNNçš„åŸå‹æ¨¡å‹ä¸Šå®ç°äº†é«˜è¾¾3.2%çš„æ”¹è¿›ï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´ä¸­è¡¨ç°å‡ºå“è¶Šçš„ç‰¹å¾å¯åˆ†ç¦»æ€§ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ›´è½»é‡çº§éª¨å¹²çš„åŸºäºTransformerçš„ç«äº‰å¯¹æ‰‹ç›¸æ¯”ï¼ŒViT-ProtoNetå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ã€‚ä»£ç å’Œé¢„è®­ç»ƒæƒé‡å·²å‘å¸ƒï¼Œä»¥ä¿ƒè¿›å¯é‡å¤æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformerï¼ˆViTï¼‰åœ¨å°æ ·æœ¬å›¾åƒåˆ†ç±»ä¸­çš„ä»£è¡¨æ€§èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚</li>
<li>ViT-ProtoNetç»“åˆäº†ViT-Smalléª¨å¹²ç½‘ç»œå’ŒPrototypical Networkæ¡†æ¶ï¼Œç”¨äºå°æ ·æœ¬å›¾åƒåˆ†ç±»ã€‚</li>
<li>é€šè¿‡å¹³å‡å°‘é‡æ”¯æŒæ ·æœ¬çš„ç±»åˆ«æ¡ä»¶ä»¤ç‰ŒåµŒå…¥ï¼ŒViT-ProtoNetæ„å»ºäº†ç¨³å¥çš„åŸå‹ã€‚</li>
<li>åœ¨å››ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒViT-ProtoNetåœ¨5ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨æ½œåœ¨ç©ºé—´ä¸­å…·æœ‰å“è¶Šçš„ç‰¹å¾å¯åˆ†ç¦»æ€§ã€‚</li>
<li>ViT-ProtoNetåœ¨åŸºäºCNNçš„åŸå‹æ¨¡å‹ä¸Šå®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œæœ€é«˜è¾¾åˆ°3.2%ã€‚</li>
<li>ä¸å…¶ä»–åŸºäºTransformerçš„ç«äº‰å¯¹æ‰‹ç›¸æ¯”ï¼Œä½¿ç”¨æ›´è½»é‡çº§éª¨å¹²çš„ViT-ProtoNetè¡¨ç°ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4d0610ed74405bb286dd1346ceae6d14.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Revisiting-Pool-based-Prompt-Learning-for-Few-shot-Class-incremental-Learning"><a href="#Revisiting-Pool-based-Prompt-Learning-for-Few-shot-Class-incremental-Learning" class="headerlink" title="Revisiting Pool-based Prompt Learning for Few-shot Class-incremental   Learning"></a>Revisiting Pool-based Prompt Learning for Few-shot Class-incremental   Learning</h2><p><strong>Authors:Yongwei Jiang, Yixiong Zou, Yuhua Li, Ruixuan Li</strong></p>
<p>Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data scarcity and incremental learning in real-world scenarios. While pool-based prompting methods have demonstrated success in traditional incremental learning, their effectiveness in FSCIL settings remains unexplored. This paper presents the first study of current prompt pool methods in FSCIL tasks, revealing an unanticipated performance degradation in incremental sessions. Through comprehensive analysis, we identify that this phenomenon stems from token-dimension saturation: with limited data, excessive prompts compete for task-relevant information, leading to model overfitting. Based on this finding, we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively shifts pool-based prompt learning from the token dimension to the spatial dimension. LGSP-Prompt generates spatial prompts by synergistically combining local spatial features and global frequency-domain representations to highlight key patterns in input images. We construct two spatial prompt pools enabling dynamic prompt selection to maintain acquired knowledge while effectively learning novel sessions. Extensive experiments demonstrate that our approach achieves state-of-the-art performance across multiple FSCIL benchmarks, showing significant advantages in both base knowledge preservation and incremental learning. Our implementation is available at <a target="_blank" rel="noopener" href="https://github.com/Jywsuperman/LGSP">https://github.com/Jywsuperman/LGSP</a>. </p>
<blockquote>
<p>åœ¨çœŸå®åœºæ™¯ä¸­ï¼ŒFew-Shot ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰é¢ä¸´ç€æ•°æ®ç¨€ç¼ºå’Œå¢é‡å­¦ä¹ çš„åŒé‡æŒ‘æˆ˜ã€‚è™½ç„¶åŸºäºæ± ä½“çš„æç¤ºæ–¹æ³•åœ¨ä¼ ç»Ÿçš„å¢é‡å­¦ä¹ ä¸­å·²ç»å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬åœ¨FSCILè®¾ç½®ä¸­çš„æœ‰æ•ˆæ€§å°šæœªè¢«æ¢ç´¢ã€‚æœ¬æ–‡é¦–æ¬¡ç ”ç©¶äº†å½“å‰æç¤ºæ± æ–¹æ³•åœ¨FSCILä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œæ­ç¤ºäº†å¢é‡ä¼šè¯ä¸­é¢„æœŸä¹‹å¤–çš„æ€§èƒ½ä¸‹é™ã€‚é€šè¿‡ç»¼åˆåˆ†æï¼Œæˆ‘ä»¬å‘ç°è¿™ä¸€ç°è±¡æºäºtokenç»´åº¦çš„é¥±å’Œï¼šåœ¨æœ‰é™çš„æ•°æ®ä¸‹ï¼Œè¿‡å¤šçš„æç¤ºä¼šäº‰å¤ºä»»åŠ¡ç›¸å…³ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹è¿‡åº¦æ‹Ÿåˆã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†LGSP-Promptï¼ˆå±€éƒ¨-å…¨å±€ç©ºé—´æç¤ºï¼‰ï¼Œå®ƒåˆ›æ–°åœ°å°†åŸºäºæ± ä½“çš„æç¤ºå­¦ä¹ ä»tokenç»´åº¦è½¬ç§»åˆ°ç©ºé—´ç»´åº¦ã€‚LGSP-Prompté€šè¿‡ååŒç»“åˆå±€éƒ¨ç©ºé—´ç‰¹å¾å’Œå…¨å±€é¢‘åŸŸè¡¨ç¤ºæ¥ç”Ÿæˆç©ºé—´æç¤ºï¼Œä»¥çªå‡ºè¾“å…¥å›¾åƒä¸­çš„å…³é”®æ¨¡å¼ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªç©ºé—´æç¤ºæ± ï¼Œä»¥æ”¯æŒåŠ¨æ€æç¤ºé€‰æ‹©ï¼Œä»¥ç»´æŒå·²è·å–çš„çŸ¥è¯†å¹¶æœ‰æ•ˆåœ°å­¦ä¹ æ–°çš„ä¼šè¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªFSCILåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨åŸºç¡€çŸ¥è¯†çš„ä¿ç•™å’Œå¢é‡å­¦ä¹ æ–¹é¢éƒ½æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Jywsuperman/LGSP">https://github.com/Jywsuperman/LGSP</a>ä¸­æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09183v1">PDF</a> Accepted to ICCV 2025, 11 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¯¥è®ºæ–‡é’ˆå¯¹å°æ ·æœ¬ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰é¢†åŸŸæ•°æ®ç¨€ç¼ºå’Œå¢é‡å­¦ä¹ åŒé‡æŒ‘æˆ˜è¿›è¡Œç ”ç©¶ã€‚è™½ç„¶åŸºäºæç¤ºæ± çš„æ–¹æ³•åœ¨ä¼ ç»Ÿå¢é‡å­¦ä¹ ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨FSCILåœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å°šæœªå¾—åˆ°æ¢ç´¢ã€‚æœ¬æ–‡é¦–æ¬¡ç ”ç©¶äº†å½“å‰æç¤ºæ± æ–¹æ³•åœ¨FSCILä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå‘ç°å¢é‡ä¼šè¯ä¸­å­˜åœ¨æ„å¤–çš„æ€§èƒ½ä¸‹é™ç°è±¡ã€‚ç»è¿‡ç»¼åˆåˆ†æï¼Œæˆ‘ä»¬ç¡®å®šè¿™æ˜¯ç”±äºæ ‡è®°ç»´åº¦é¥±å’Œå¯¼è‡´çš„ï¼šåœ¨æœ‰é™æ•°æ®ä¸‹ï¼Œè¿‡å¤šçš„æç¤ºä¼šäº‰å¤ºä»»åŠ¡ç›¸å…³ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆã€‚åŸºäºæ­¤å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†LGSP-Promptï¼ˆå±€éƒ¨å…¨å±€ç©ºé—´æç¤ºï¼‰ï¼Œåˆ›æ–°åœ°å°†åŸºäºæç¤ºæ± çš„å­¦ä¹ ä»æ ‡è®°ç»´åº¦è½¬å‘ç©ºé—´ç»´åº¦ã€‚LGSP-Prompté€šè¿‡ååŒç»“åˆå±€éƒ¨ç©ºé—´ç‰¹å¾å’Œå…¨å±€é¢‘åŸŸè¡¨ç¤ºæ¥ç”Ÿæˆç©ºé—´æç¤ºï¼Œä»è€Œçªå‡ºè¾“å…¥å›¾åƒä¸­çš„å…³é”®æ¨¡å¼ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªç©ºé—´æç¤ºæ± ï¼Œä»¥å®ç°åŠ¨æ€æç¤ºé€‰æ‹©ï¼Œæ—¢ä¿æŒå·²è·å¾—çš„çŸ¥è¯†ï¼Œåˆèƒ½æœ‰æ•ˆå­¦ä¹ æ–°çš„ä¼šè¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªFSCILåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œåœ¨åŸºç¡€çŸ¥è¯†ä¿ç•™å’Œå¢é‡å­¦ä¹ æ–¹é¢éƒ½æ˜¾ç¤ºå‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>FSCILé¢ä¸´æ•°æ®ç¨€ç¼ºå’Œå¢é‡å­¦ä¹ çš„åŒé‡æŒ‘æˆ˜ã€‚</li>
<li>åŸºäºæç¤ºæ± çš„æ–¹æ³•åœ¨FSCILä»»åŠ¡ä¸­è¡¨ç°å‡ºæ„å¤–çš„æ€§èƒ½ä¸‹é™ç°è±¡ã€‚</li>
<li>ç°è±¡çš„åŸå› åœ¨äºæ ‡è®°ç»´åº¦é¥±å’Œï¼Œè¿‡é‡æç¤ºå¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆã€‚</li>
<li>å¼•å…¥LGSP-Promptæ–¹æ³•ï¼Œå°†åŸºäºæç¤ºæ± çš„å­¦ä¹ ä»æ ‡è®°ç»´åº¦è½¬å‘ç©ºé—´ç»´åº¦ã€‚</li>
<li>LGSP-Promptç»“åˆå±€éƒ¨ç©ºé—´ç‰¹å¾å’Œå…¨å±€é¢‘åŸŸè¡¨ç¤ºç”Ÿæˆç©ºé—´æç¤ºã€‚</li>
<li>æ„å»ºäº†ä¸¤ä¸ªç©ºé—´æç¤ºæ± ä»¥å®ç°åŠ¨æ€æç¤ºé€‰æ‹©ï¼ŒåŒæ—¶ä¿æŒçŸ¥è¯†å’Œå­¦ä¹ æ–°ä¼šè¯ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªFSCILåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œå…·æœ‰æ˜¾è‘—çš„ä¿ç•™å’Œå¢é‡å­¦ä¹ ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bbeece35ec0a6de2fe73abeb60007f7a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0b0e96caedfe38e7868482d51bb06f9d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3aebbe56ecbc3ff22685a3950a379abb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98dba234c37c026e7415c7d11f0259b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc16fab3a36925a28901b51fd325c132.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Beyond-Scale-Small-Language-Models-are-Comparable-to-GPT-4-in-Mental-Health-Understanding"><a href="#Beyond-Scale-Small-Language-Models-are-Comparable-to-GPT-4-in-Mental-Health-Understanding" class="headerlink" title="Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental   Health Understanding"></a>Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental   Health Understanding</h2><p><strong>Authors:Hong Jia, Shiya Fu, Feng Xia, Vassilis Kostakos, Ting Dang</strong></p>
<p>The emergence of Small Language Models (SLMs) as privacy-preserving alternatives for sensitive applications raises a fundamental question about their inherent understanding capabilities compared to Large Language Models (LLMs). This paper investigates the mental health understanding capabilities of current SLMs through systematic evaluation across diverse classification tasks. Employing zero-shot and few-shot learning paradigms, we benchmark their performance against established LLM baselines to elucidate their relative strengths and limitations in this critical domain. We assess five state-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against three LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding tasks. Our findings reveal that SLMs achieve mean performance within 2% of LLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot settings), demonstrating notable competence despite orders of magnitude fewer parameters. Both model categories experience similar degradation on multi-class severity tasks (a drop of over 30%), suggesting that nuanced clinical understanding challenges transcend model scale. Few-shot prompting provides substantial improvements for SLMs (up to 14.6%), while LLM gains are more variable. Our work highlights the potential of SLMs in mental health understanding, showing they can be effective privacy-preserving tools for analyzing sensitive online text data. In particular, their ability to quickly adapt and specialize with minimal data through few-shot learning positions them as promising candidates for scalable mental health screening tools. </p>
<blockquote>
<p>éšç€å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ä½œä¸ºæ•æ„Ÿåº”ç”¨çš„éšç§ä¿æŠ¤æ›¿ä»£æ–¹æ¡ˆçš„å‡ºç°ï¼Œå…³äºå®ƒä»¬ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å†…åœ¨ç†è§£èƒ½åŠ›çš„æ¯”è¾ƒï¼Œå¼•å‘äº†ä¸€ä¸ªæ ¹æœ¬æ€§çš„é—®é¢˜ã€‚æœ¬æ–‡é€šè¿‡ç³»ç»Ÿè¯„ä¼°å„ç§åˆ†ç±»ä»»åŠ¡ï¼Œç ”ç©¶å½“å‰SLMåœ¨å¿ƒç†å¥åº·ç†è§£æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨é›¶æ ·æœ¬å­¦ä¹ å’Œå°æ ·æœ¬å­¦ä¹ èŒƒå¼ï¼Œå°†å®ƒä»¬çš„æ€§èƒ½ä¸ç°æœ‰çš„LLMåŸºå‡†è¿›è¡Œæµ‹è¯•ï¼Œä»¥é˜æ˜è¿™ä¸€å…³é”®é¢†åŸŸä¸­çš„ç›¸å¯¹ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚æˆ‘ä»¬å¯¹äº”ä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œåˆ†åˆ«æ˜¯Phi-3ã€Phi-3.5ã€Qwen2.5ã€Llama-3.2å’ŒGemma2ç­‰ï¼Œä»¥åŠä¸‰ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹GPT-4ã€FLAN-T5-XXLå’ŒAlpaca-7Bç­‰å…­ä¸ªå¿ƒç†å¥åº·ç†è§£ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œåœ¨äºŒå…ƒåˆ†ç±»ä»»åŠ¡ä¸Šï¼ŒSLMçš„å‡å€¼ä¸LLMåœ¨çŸ­çŸ­ä¸¤ä¸ªç™¾åˆ†ç‚¹ä»¥å†…è¾¾åˆ°ç›¸å¯¹æ°´å¹³ï¼ˆé›¶æ ·æœ¬è®¾ç½®ä¸­F1åˆ†æ•°ä¸º0.64å¯¹0.66ï¼‰ï¼Œè™½ç„¶å‚æ•°ç›¸å·®å¤šä¸ªæ•°é‡çº§ï¼Œä½†ä¹Ÿè¡¨ç°å‡ºç›¸å½“çš„å¯ä¿¡èƒ½åŠ›ã€‚ä¸¤ç§æ¨¡å‹åœ¨å¤šç±»ä¸¥é‡æ€§ä»»åŠ¡ä¸Šçš„æ€§èƒ½å‡å‡ºç°ç±»ä¼¼ä¸‹é™ï¼ˆé™å¹…è¶…è¿‡ç™¾åˆ†ä¹‹ä¸‰åï¼‰ï¼Œè¿™è¡¨æ˜ç»†å¾®çš„ä¸´åºŠç†è§£æŒ‘æˆ˜å·²è¶…è¶Šäº†æ¨¡å‹çš„è§„æ¨¡èŒƒå›´ã€‚å°æ ·æœ¬æç¤ºï¼ˆå³è®©æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„æ“ä½œæ˜¾ç¤ºä¸€äº›å°é‡æ ·ä¾‹çš„èƒ½åŠ›ï¼‰å¯ä¸ºSLMå¸¦æ¥å·¨å¤§æ”¹å–„ï¼ˆé«˜è¾¾ç™¾åˆ†ä¹‹åå››ç‚¹å…­ï¼‰ï¼Œè€Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ”¶ç›Šåˆ™æ›´ä¸ºä¸ç¨³å®šã€‚æˆ‘ä»¬çš„å·¥ä½œé‡ç‚¹çªå‡ºäº†SLMåœ¨å¿ƒç†å¥åº·ç†è§£æ–¹é¢çš„æ½œåŠ›ï¼Œæ˜¾ç¤ºå‡ºå®ƒä»¬å¯ä»¥æˆä¸ºåˆ†ææ•æ„Ÿåœ¨çº¿æ–‡æœ¬æ•°æ®çš„æœ‰æ•ˆéšç§ä¿æŠ¤å·¥å…·ã€‚ç‰¹åˆ«æ˜¯å®ƒä»¬é€šè¿‡å°æ ·æœ¬å­¦ä¹ å¿«é€Ÿé€‚åº”å’Œä¸“é—¨åŒ–çš„èƒ½åŠ›ï¼Œä½¿å®ƒä»¬æˆä¸ºå¿ƒç†å¥åº·ç­›æŸ¥å·¥å…·çš„å¯é€‰æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.08031v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å½“å‰æ–‡æœ¬æ¢è®¨äº†å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰åœ¨å¿ƒç†å¥åº·ç†è§£æ–¹é¢çš„èƒ½åŠ›ï¼Œé€šè¿‡ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯¹æ¯”å®éªŒï¼Œå±•ç¤ºäº†SLMsåœ¨æ­¤é¢†åŸŸçš„æ½œåŠ›å’Œä¼˜åŠ¿ã€‚ç ”ç©¶å‘ç°ï¼ŒSLMsåœ¨äºŒå…ƒåˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸LLMsç›¸è¿‘ï¼Œå°‘æ•°é•œå¤´å­¦ä¹ æŠ€æœ¯çš„å¼•å…¥ä¸ºSLMså¸¦æ¥äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å°½ç®¡å­˜åœ¨å¤šç±»åˆ«ä¸¥é‡ç¨‹åº¦ä»»åŠ¡ä¸Šçš„æŒ‘æˆ˜ï¼Œä½†SLMså±•ç°äº†åœ¨å¿ƒç†å¥åº·ç†è§£é¢†åŸŸçš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚æ€»ä½“æ¥è¯´ï¼ŒSLMså¯ä»¥ä½œä¸ºéšç§ä¿æŠ¤å·¥å…·åº”ç”¨äºæ•æ„Ÿæ–‡æœ¬æ•°æ®çš„åˆ†æï¼Œå°¤å…¶åœ¨å¿«é€Ÿé€‚åº”å’Œä¸“ä¸šåŒ–æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œæœ‰æœ›ç”¨äºå¯æ‰©å±•çš„å¿ƒç†å¥åº·ç­›æŸ¥å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SLMsä½œä¸ºéšç§ä¿æŠ¤æ›¿ä»£æ–¹æ¡ˆåœ¨æ•æ„Ÿåº”ç”¨ä¸­çš„å…´èµ·ï¼Œå¼•å‘äº†å¯¹å®ƒä»¬ä¸LLMsåœ¨ç†è§£èƒ½åŠ›æ–¹é¢çš„æ¯”è¾ƒã€‚</li>
<li>é€šè¿‡å¤šç§åˆ†ç±»ä»»åŠ¡çš„ç³»ç»Ÿè¯„ä¼°ï¼Œå‘ç°SLMsåœ¨å¿ƒç†å¥åº·ç†è§£æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ã€‚</li>
<li>åœ¨äºŒå…ƒåˆ†ç±»ä»»åŠ¡ä¸Šï¼ŒSLMsçš„æ€§èƒ½ä¸LLMsç›¸è¿‘ï¼Œå°‘æ•°é•œå¤´å­¦ä¹ æŠ€æœ¯æ˜¾è‘—æå‡SLMsæ€§èƒ½ã€‚</li>
<li>åœ¨å¤šç±»åˆ«ä¸¥é‡ç¨‹åº¦ä»»åŠ¡ä¸Šï¼Œä¸¤ç§æ¨¡å‹å‡é¢ä¸´æŒ‘æˆ˜ï¼Œæ˜¾ç¤ºå¾®å¦™çš„ä¸´åºŠç†è§£éš¾åº¦è¶…è¶Šæ¨¡å‹è§„æ¨¡ã€‚</li>
<li>SLMsåœ¨å¿ƒç†å¥åº·ç†è§£é¢†åŸŸå…·æœ‰æ½œåŠ›å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>SLMså¯ä½œä¸ºéšç§ä¿æŠ¤å·¥å…·ç”¨äºåˆ†ææ•æ„Ÿæ–‡æœ¬æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.08031">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e8666731bba783448672e94f0aff3713.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7dd653e6498b2fddf65cee6e9b4b88a5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f7f81875b75bf5187e5f2e2364b229a2.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Recognizing-Surgical-Phases-Anywhere-Few-Shot-Test-time-Adaptation-and-Task-graph-Guided-Refinement"><a href="#Recognizing-Surgical-Phases-Anywhere-Few-Shot-Test-time-Adaptation-and-Task-graph-Guided-Refinement" class="headerlink" title="Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and   Task-graph Guided Refinement"></a>Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and   Task-graph Guided Refinement</h2><p><strong>Authors:Kun Yuan, Tingxuan Chen, Shi Li, Joel L. Lavanchy, Christian Heiliger, Ege Ã–zsoy, Yiming Huang, Long Bai, Nassir Navab, Vinkle Srivastav, Hongliang Ren, Nicolas Padoy</strong></p>
<p>The complexity and diversity of surgical workflows, driven by heterogeneous operating room settings, institutional protocols, and anatomical variability, present a significant challenge in developing generalizable models for cross-institutional and cross-procedural surgical understanding. While recent surgical foundation models pretrained on large-scale vision-language data offer promising transferability, their zero-shot performance remains constrained by domain shifts, limiting their utility in unseen surgical environments. To address this, we introduce Surgical Phase Anywhere (SPA), a lightweight framework for versatile surgical workflow understanding that adapts foundation models to institutional settings with minimal annotation. SPA leverages few-shot spatial adaptation to align multi-modal embeddings with institution-specific surgical scenes and phases. It also ensures temporal consistency through diffusion modeling, which encodes task-graph priors derived from institutional procedure protocols. Finally, SPA employs dynamic test-time adaptation, exploiting the mutual agreement between multi-modal phase prediction streams to adapt the model to a given test video in a self-supervised manner, enhancing the reliability under test-time distribution shifts. SPA is a lightweight adaptation framework, allowing hospitals to rapidly customize phase recognition models by defining phases in natural language text, annotating a few images with the phase labels, and providing a task graph defining phase transitions. The experimental results show that the SPA framework achieves state-of-the-art performance in few-shot surgical phase recognition across multiple institutions and procedures, even outperforming full-shot models with 32-shot labeled data. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SPA">https://github.com/CAMMA-public/SPA</a> </p>
<blockquote>
<p>æ‰‹æœ¯æµç¨‹çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼Œå—åˆ°æ‰‹æœ¯å®¤è®¾ç½®ã€æœºæ„åè®®å’Œè§£å‰–ç»“æ„å·®å¼‚çš„å½±å“ï¼Œåœ¨ä¸ºè·¨æœºæ„å’Œè·¨æ‰‹æœ¯ç¨‹åºæ‰‹æœ¯ç†è§£å¼€å‘å¯æ¨å¹¿æ¨¡å‹æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶æœ€è¿‘åŸºäºå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ•°æ®çš„é¢„è®­ç»ƒæ‰‹æœ¯åŸºç¡€æ¨¡å‹è¡¨ç°å‡ºæœ‰å¸Œæœ›çš„è¿ç§»èƒ½åŠ›ï¼Œä½†å®ƒä»¬çš„é›¶æ ·æœ¬æ€§èƒ½ä»å—åˆ°é¢†åŸŸå·®å¼‚çš„é™åˆ¶ï¼Œåœ¨æœªè§è¿‡çš„æ‰‹æœ¯ç¯å¢ƒä¸­æ•ˆç”¨æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Surgical Phase Anywhereï¼ˆSPAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé€šç”¨æ‰‹æœ¯æµç¨‹ç†è§£çš„è½»ä¾¿æ¡†æ¶ï¼Œèƒ½å¤Ÿå€ŸåŠ©æœ€å°‘çš„æ ‡æ³¨æ¥é€‚åº”æœºæ„ç¯å¢ƒã€‚SPAåˆ©ç”¨å°æ ·æœ¬ç©ºé—´é€‚åº”ï¼Œä½¿å¤šæ¨¡å¼åµŒå…¥ä¸ç‰¹å®šæœºæ„çš„æ‰‹æœ¯åœºæ™¯å’Œé˜¶æ®µä¿æŒä¸€è‡´ã€‚å®ƒè¿˜é€šè¿‡æ‰©æ•£å»ºæ¨¡ç¡®ä¿æ—¶é—´ä¸€è‡´æ€§ï¼Œæ ¹æ®æœºæ„ç¨‹åºåè®®è¡ç”Ÿä»»åŠ¡å›¾å…ˆéªŒç¼–ç ã€‚æ­¤å¤–ï¼ŒSPAé‡‡ç”¨åŠ¨æ€æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼Œåˆ©ç”¨å¤šæ¨¡å¼é˜¶æ®µé¢„æµ‹æµä¹‹é—´çš„ç›¸äº’åè®®ï¼Œä»¥è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼å°†æ¨¡å‹é€‚åº”ç»™å®šçš„æµ‹è¯•è§†é¢‘ï¼Œåœ¨æµ‹è¯•æ—¶é—´åˆ†å¸ƒå˜åŒ–ä¸‹æé«˜å¯é æ€§ã€‚SPAæ˜¯ä¸€ä¸ªè½»é‡çº§çš„é€‚åº”æ¡†æ¶ï¼Œå…è®¸åŒ»é™¢é€šè¿‡ç”¨è‡ªç„¶è¯­è¨€æ–‡æœ¬å®šä¹‰é˜¶æ®µã€å¯¹å°‘æ•°å›¾åƒè¿›è¡Œé˜¶æ®µæ ‡ç­¾æ³¨é‡Šä»¥åŠæä¾›å®šä¹‰é˜¶æ®µè½¬æ¢çš„ä»»åŠ¡å›¾æ¥å¿«é€Ÿå®šåˆ¶é˜¶æ®µè¯†åˆ«æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPAæ¡†æ¶åœ¨å¤šä¸ªæœºæ„å’Œç¨‹åºä¸­çš„å°æ ·æœ¬æ‰‹æœ¯é˜¶æ®µè¯†åˆ«ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”šè‡³åœ¨32ä¸ªæ ·æœ¬æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹è¶…è¿‡äº†å…¨æ ·æœ¬æ¨¡å‹ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SPA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/CAMMA-public/SPAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20254v2">PDF</a> Accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ‰‹æœ¯æµç¨‹ç†è§£çš„ä¸€ä¸ªæ–°å‹æ¡†æ¶â€”â€”Surgical Phase Anywhereï¼ˆSPAï¼‰ã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³æ‰‹æœ¯æµç¨‹å¤æ‚æ€§åŠå¤šæ ·æ€§å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œé€šè¿‡å°‘é‡æ ‡æ³¨æ•°æ®ï¼Œé€‚åº”ä¸åŒæœºæ„çš„æ‰‹æœ¯ç¯å¢ƒã€‚SPAåˆ©ç”¨å°‘æ ·æœ¬ç©ºé—´é€‚åº”ã€æ‰©æ•£å»ºæ¨¡ä»¥åŠåŠ¨æ€æµ‹è¯•æ—¶é—´é€‚åº”ç­‰æŠ€æœ¯ï¼Œå®ç°äº†æœºæ„ç‰¹å®šæ‰‹æœ¯åœºæ™¯å’Œé˜¶æ®µçš„å¯¹é½ã€ä»»åŠ¡å›¾å…ˆéªŒçš„ç¼–ç ä»¥åŠæ¨¡å‹çš„è‡ªé€‚åº”è°ƒæ•´ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSPAæ¡†æ¶åœ¨è·¨æœºæ„å’Œè·¨æ‰‹æœ¯ç¨‹åºçš„å°‘æ ·æœ¬æ‰‹æœ¯é˜¶æ®µè¯†åˆ«ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Surgical workflowsçš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ç»™è·¨æœºæ„å’Œè·¨æ‰‹æœ¯ç¨‹åºçš„ç†è§£å¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨æœªè§è¿‡çš„æ‰‹æœ¯ç¯å¢ƒä¸­è¡¨ç°å—é™ã€‚</li>
<li>SPAæ¡†æ¶é€šè¿‡å°‘æ ·æœ¬ç©ºé—´é€‚åº”ï¼Œé€‚åº”ä¸åŒæœºæ„çš„æ‰‹æœ¯ç¯å¢ƒã€‚</li>
<li>SPAåˆ©ç”¨æ‰©æ•£å»ºæ¨¡å®ç°ä»»åŠ¡å›¾å…ˆéªŒçš„ç¼–ç ï¼Œç¡®ä¿æ—¶é—´ä¸€è‡´æ€§ã€‚</li>
<li>SPAé‡‡ç”¨åŠ¨æ€æµ‹è¯•æ—¶é—´é€‚åº”ï¼Œæé«˜æ¨¡å‹åœ¨æµ‹è¯•æ—¶åˆ†å¸ƒå˜åŒ–ä¸‹çš„å¯é æ€§ã€‚</li>
<li>SPAæ¡†æ¶å…è®¸åŒ»é™¢é€šè¿‡è‡ªç„¶è¯­è¨€æ–‡æœ¬å®šä¹‰é˜¶æ®µï¼Œé€šè¿‡å°‘é‡å›¾åƒæ ‡æ³¨ï¼Œæä¾›ä»»åŠ¡å›¾å®šä¹‰é˜¶æ®µè½¬æ¢ï¼Œå®ç°å¿«é€Ÿå®šåˆ¶é˜¶æ®µè¯†åˆ«æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c8be39de20021b00bf428411d8498ede.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fe61f58515d793766ad46c54d05b6773.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-563fcde840d43a3cde2f32ca5768ea63.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c8c2f5b6fc31fa57419a5ae080ccc80d.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  Trexplorer Super Topologically Correct Centerline Tree Tracking of   Tubular Objects in CT Volumes
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-8030ab6220eb9d5af66ba57f0797f4b8.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  Journalism-Guided Agentic In-Context Learning for News Stance Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30166.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
