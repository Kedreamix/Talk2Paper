<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  CATVis Context-Aware Thought Visualization">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-a6027536d136c7ce6f1e232c79f51b53.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-19
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-17-æ›´æ–°"><a href="#2025-07-17-æ›´æ–°" class="headerlink" title="2025-07-17 æ›´æ–°"></a>2025-07-17 æ›´æ–°</h1><h2 id="CATVis-Context-Aware-Thought-Visualization"><a href="#CATVis-Context-Aware-Thought-Visualization" class="headerlink" title="CATVis: Context-Aware Thought Visualization"></a>CATVis: Context-Aware Thought Visualization</h2><p><strong>Authors:Tariq Mehmood, Hamza Ahmad, Muhammad Haroon Shakeel, Murtaza Taj</strong></p>
<p>EEG-based brain-computer interfaces (BCIs) have shown promise in various applications, such as motor imagery and cognitive state monitoring. However, decoding visual representations from EEG signals remains a significant challenge due to their complex and noisy nature. We thus propose a novel 5-stage framework for decoding visual representations from EEG signals: (1) an EEG encoder for concept classification, (2) cross-modal alignment of EEG and text embeddings in CLIP feature space, (3) caption refinement via re-ranking, (4) weighted interpolation of concept and caption embeddings for richer semantics, and (5) image generation using a pre-trained Stable Diffusion model. We enable context-aware EEG-to-image generation through cross-modal alignment and re-ranking. Experimental results demonstrate that our method generates high-quality images aligned with visual stimuli, outperforming SOTA approaches by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and reducing Fr&#39;echet Inception Distance by 36.61%, indicating superior semantic alignment and image quality. </p>
<blockquote>
<p>åŸºäºè„‘ç”µå›¾çš„è„‘æœºæ¥å£ï¼ˆBCIsï¼‰åœ¨å¤šç§åº”ç”¨ä¸­æ˜¾ç¤ºå‡ºå¹¿é˜”å‰æ™¯ï¼Œå¦‚è¿åŠ¨æƒ³è±¡å’Œè®¤çŸ¥çŠ¶æ€ç›‘æµ‹ã€‚ç„¶è€Œï¼Œç”±äºè„‘ç”µå›¾ä¿¡å·çš„å¤æ‚æ€§å’Œå™ªå£°å¹²æ‰°ï¼Œä»è„‘ç”µå›¾ä¿¡å·ä¸­è§£ç è§†è§‰è¡¨å¾ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§£ç è§†è§‰è¡¨å¾çš„5é˜¶æ®µæ¡†æ¶ï¼Œä»è„‘ç”µå›¾ä¿¡å·ä¸­æå–ä¿¡æ¯ï¼šï¼ˆ1ï¼‰EEGç¼–ç å™¨ç”¨äºæ¦‚å¿µåˆ†ç±»ï¼›ï¼ˆ2ï¼‰åœ¨CLIPç‰¹å¾ç©ºé—´ä¸­å®ç°EEGå’Œæ–‡æœ¬åµŒå…¥çš„è·¨æ¨¡æ€å¯¹é½ï¼›ï¼ˆ3ï¼‰é€šè¿‡é‡æ–°æ’åºè¿›è¡Œå­—å¹•ä¼˜åŒ–ï¼›ï¼ˆ4ï¼‰å¯¹æ¦‚å¿µåµŒå…¥å’Œå­—å¹•åµŒå…¥è¿›è¡ŒåŠ æƒæ’å€¼ï¼Œä»¥è·å–æ›´ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼›ï¼ˆ5ï¼‰ä½¿ç”¨é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹è¿›è¡Œå›¾åƒç”Ÿæˆã€‚æˆ‘ä»¬é€šè¿‡è·¨æ¨¡æ€å¯¹é½å’Œé‡æ–°æ’åºï¼Œå®ç°äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„EEGåˆ°å›¾åƒç”Ÿæˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆäº†ä¸è§†è§‰åˆºæ¿€å¯¹é½çš„é«˜è´¨é‡å›¾åƒï¼Œåœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸Šè¾ƒå½“å‰æœ€ä½³æ–¹æ³•æé«˜äº†13.43%ï¼Œç”Ÿæˆå‡†ç¡®ç‡æé«˜äº†15.21%ï¼ŒåŒæ—¶å‡å°‘äº†Frechet Inceptionè·ç¦»ï¼ˆFIDï¼‰36.61%ï¼Œè¿™è¡¨æ˜äº†æ›´ä¼˜ç§€çš„è¯­ä¹‰å¯¹é½å’Œå›¾åƒè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11522v1">PDF</a> Accepted at MICCAI 2025. This is the submitted version prior to peer   review. The final Version of Record will appear in the MICCAI 2025   proceedings (Springer LNCS)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºEEGçš„äº”é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºä»è„‘ç”µå›¾ä¿¡å·ä¸­è§£ç è§†è§‰è¡¨å¾ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬æ¦‚å¿µåˆ†ç±»çš„EEGç¼–ç å™¨ã€è·¨æ¨¡æ€å¯¹é½çš„EEGå’Œæ–‡æœ¬åµŒå…¥ã€é€šè¿‡é‡æ–°æ’åºè¿›è¡Œå­—å¹•ä¼˜åŒ–ã€æ¦‚å¿µä¸å­—å¹•åµŒå…¥çš„åŠ æƒæ’å€¼ä»¥ä¸°å¯Œè¯­ä¹‰ï¼Œä»¥åŠä½¿ç”¨é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ç”Ÿæˆå›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ç”Ÿæˆä¸è§†è§‰åˆºæ¿€å¯¹é½çš„é«˜è´¨é‡å›¾åƒï¼Œåœ¨åˆ†ç±»å‡†ç¡®æ€§ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯æ–¹æ³•è¾¾13.43%ï¼Œåœ¨ç”Ÿæˆå‡†ç¡®æ€§ä¸Šæé«˜15.21%ï¼ŒåŒæ—¶å‡å°‘äº†FrÃ©chet Inceptionè·ç¦»ï¼ˆFIDï¼‰è¾¾36.61%ï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„è¯­ä¹‰å¯¹é½å’Œå›¾åƒè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºEEGçš„äº”ä¸ªé˜¶æ®µæ¡†æ¶ç”¨äºè§£ç è§†è§‰è¡¨å¾ã€‚</li>
<li>é€šè¿‡EEGç¼–ç å™¨è¿›è¡Œæ¦‚å¿µåˆ†ç±»æ˜¯æ­¤æ¡†æ¶çš„ç¬¬ä¸€æ­¥ã€‚</li>
<li>åˆ©ç”¨CLIPç‰¹å¾ç©ºé—´è¿›è¡Œè·¨æ¨¡æ€å¯¹é½çš„EEGå’Œæ–‡æœ¬åµŒå…¥ã€‚</li>
<li>é€šè¿‡é‡æ–°æ’åºè¿›è¡Œå­—å¹•ä¼˜åŒ–ï¼Œå¢å¼ºè¯­ä¹‰ä¸°å¯Œæ€§ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„Stable Diffusionæ¨¡å‹ç”Ÿæˆå›¾åƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è¯¥æ¡†æ¶ç”Ÿæˆé«˜è´¨é‡ã€ä¸è§†è§‰åˆºæ¿€å¯¹é½çš„å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11522">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52772f48a3041cd1f8b08f6d56356d28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb5784edf70c5b788efda73b179806d0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c476b4e68896e9da2f9da915050a8380.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Implementing-Adaptations-for-Vision-AutoRegressive-Model"><a href="#Implementing-Adaptations-for-Vision-AutoRegressive-Model" class="headerlink" title="Implementing Adaptations for Vision AutoRegressive Model"></a>Implementing Adaptations for Vision AutoRegressive Model</h2><p><strong>Authors:Kaif Shaikh, Antoni Kowalczuk, Franziska Boenisch, Adam Dziedzic</strong></p>
<p>Vision AutoRegressive model (VAR) was recently introduced as an alternative to Diffusion Models (DMs) in image generation domain. In this work we focus on its adaptations, which aim to fine-tune pre-trained models to perform specific downstream tasks, like medical data generation. While for DMs there exist many techniques, adaptations for VAR remain underexplored. Similarly, differentially private (DP) adaptations-ones that aim to preserve privacy of the adaptation data-have been extensively studied for DMs, while VAR lacks such solutions. In our work, we implement and benchmark many strategies for VAR, and compare them to state-of-the-art DM adaptation strategies. We observe that VAR outperforms DMs for non-DP adaptations, however, the performance of DP suffers, which necessitates further research in private adaptations for VAR. Code is available at <a target="_blank" rel="noopener" href="https://github.com/sprintml/finetuning_var_dp">https://github.com/sprintml/finetuning_var_dp</a>. </p>
<blockquote>
<p>æœ€è¿‘å¼•å…¥äº†Vision AutoRegressiveæ¨¡å‹ï¼ˆVARï¼‰ä½œä¸ºå›¾åƒç”Ÿæˆé¢†åŸŸä¸­Diffusion Modelsï¼ˆDMsï¼‰çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å…³æ³¨å…¶é€‚åº”æ€§çš„è°ƒæ•´ï¼Œæ—¨åœ¨å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ä»¥æ‰§è¡Œç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚åŒ»å­¦æ•°æ®ç”Ÿæˆã€‚è™½ç„¶å¯¹äºDMså­˜åœ¨è®¸å¤šæŠ€æœ¯ï¼Œä½†VARçš„é€‚åº”æ€§è°ƒæ•´ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚ç±»ä¼¼åœ°ï¼Œé’ˆå¯¹DMsçš„å·®åˆ†ç§æœ‰ï¼ˆDPï¼‰é€‚åº”æ€§è°ƒæ•´â€”â€”æ—¨åœ¨ä¿æŠ¤é€‚åº”æ€§æ•°æ®çš„éšç§â€”â€”å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œè€ŒVARç¼ºä¹æ­¤ç±»è§£å†³æ–¹æ¡ˆã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å®ç°äº†å¤šç§VARçš„ç­–ç•¥å¹¶è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å°†å…¶ä¸æœ€æ–°çš„DMé€‚åº”ç­–ç•¥è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬å‘ç°VARåœ¨éDPé€‚åº”æ€§æ–¹é¢è¡¨ç°ä¼˜äºDMsï¼Œä½†æ˜¯DPçš„æ€§èƒ½å´æœ‰æ‰€ä¸‹é™ï¼Œè¿™éœ€è¦åœ¨VARçš„ç§æœ‰é€‚åº”æ€§æ–¹é¢è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/sprintml/finetuning_var_dp%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/sprintml/finetuning_var_dpæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11441v1">PDF</a> Accepted at DIG-BUGS: Data in Generative Models Workshop @ ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Vision AutoRegressiveæ¨¡å‹ï¼ˆVARï¼‰åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸä½œä¸ºDiffusion Modelsï¼ˆDMsï¼‰çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶é‡ç‚¹æ¢è®¨äº†å…¶é’ˆå¯¹ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡çš„é€‚åº”æ€§é—®é¢˜ï¼Œå¦‚åŒ»ç–—æ•°æ®ç”Ÿæˆã€‚æ–‡ç« æ¯”è¾ƒäº†VARä¸DMçš„é€‚åº”ç­–ç•¥ï¼Œå‘ç°VARåœ¨éå·®åˆ†éšç§ï¼ˆDPï¼‰é€‚åº”æ–¹é¢è¡¨ç°ä¼˜äºDMsï¼Œä½†åœ¨å·®åˆ†éšç§é€‚åº”æ–¹é¢æ€§èƒ½æœ‰å¾…æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision AutoRegressiveæ¨¡å‹ï¼ˆVARï¼‰è¢«å¼•å…¥ä½œä¸ºå›¾åƒç”Ÿæˆé¢†åŸŸä¸­çš„Diffusion Modelsï¼ˆDMsï¼‰çš„æ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>æ–‡ç« é‡ç‚¹æ¢è®¨äº†VARæ¨¡å‹çš„é€‚åº”æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹é¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒä»¥æ‰§è¡Œç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚åŒ»ç–—æ•°æ®ç”Ÿæˆã€‚</li>
<li>ä¸DMsçš„é€‚åº”ç­–ç•¥ç›¸æ¯”ï¼ŒVARåœ¨éå·®åˆ†éšç§ï¼ˆDPï¼‰é€‚åº”æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>VARåœ¨å·®åˆ†éšç§ï¼ˆDPï¼‰é€‚åº”æ–¹é¢çš„æ€§èƒ½æœ‰å¾…æé«˜ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
<li>æ–‡ç« æä¾›äº†å¤šç§VARé€‚åº”ç­–ç•¥çš„å®ç°å’ŒåŸºå‡†æµ‹è¯•ã€‚</li>
<li>ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/sprintml/finetuning_var_dp%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/sprintml/finetuning_var_dpè®¿é—®ã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b2117b6d4b4e5d7c1e46083090b84b87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64ce485808ec66f9e5c992e30ff30ac0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-311461f69e6a98e6703515ce61038def.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df783c18657d8e5762de175f03217867.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Latent-Space-Consistency-for-Sparse-View-CT-Reconstruction"><a href="#Latent-Space-Consistency-for-Sparse-View-CT-Reconstruction" class="headerlink" title="Latent Space Consistency for Sparse-View CT Reconstruction"></a>Latent Space Consistency for Sparse-View CT Reconstruction</h2><p><strong>Authors:Duoyou Chen, Yunqing Chen, Can Zhang, Zhou Wang, Cheng Chen, Ruoxiu Xiao</strong></p>
<p>Computed Tomography (CT) is a widely utilized imaging modality in clinical settings. Using densely acquired rotational X-ray arrays, CT can capture 3D spatial features. However, it is confronted with challenged such as significant time consumption and high radiation exposure. CT reconstruction methods based on sparse-view X-ray images have garnered substantial attention from researchers as they present a means to mitigate costs and risks. In recent years, diffusion models, particularly the Latent Diffusion Model (LDM), have demonstrated promising potential in the domain of 3D CT reconstruction. Nonetheless, due to the substantial differences between the 2D latent representation of X-ray modalities and the 3D latent representation of CT modalities, the vanilla LDM is incapable of achieving effective alignment within the latent space. To address this issue, we propose the Consistent Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature contrastive learning to efficiently extract latent 3D information from 2D X-ray images and achieve latent space alignment between modalities. Experimental results indicate that CLS-DM outperforms classical and state-of-the-art generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing the effectiveness and economic viability of sparse X-ray reconstructed CT but can also be generalized to other cross-modal transformation tasks, such as text-to-image synthesis. We have made our code publicly available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CLS-DM-50D6/">https://anonymous.4open.science/r/CLS-DM-50D6/</a> to facilitate further research and applications in other domains. </p>
<blockquote>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æ˜¯ä¸´åºŠç¯å¢ƒä¸­å¹¿æ³›åº”ç”¨çš„æˆåƒæŠ€æœ¯ã€‚å®ƒä½¿ç”¨å¯†é›†é‡‡é›†çš„æ—‹è½¬Xå°„çº¿é˜µåˆ—ï¼Œèƒ½å¤Ÿæ•æ‰ä¸‰ç»´ç©ºé—´ç‰¹å¾ã€‚ç„¶è€Œï¼Œå®ƒé¢ä¸´ç€é‡å¤§æŒ‘æˆ˜ï¼Œå¦‚æ—¶é—´æ¶ˆè€—å·¨å¤§å’Œé«˜è¾å°„æš´éœ²ã€‚åŸºäºç¨€ç–è§†è§’Xå°„çº¿å›¾åƒçš„CTé‡å»ºæ–¹æ³•å·²ç»å¼•èµ·äº†ç ”ç©¶äººå‘˜çš„å¹¿æ³›å…³æ³¨ï¼Œå› ä¸ºå®ƒä»¬æä¾›äº†ä¸€ç§é™ä½æˆæœ¬å’Œé£é™©çš„æ–¹æ³•ã€‚è¿‘å¹´æ¥ï¼Œæ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ï¼Œåœ¨ä¸‰ç»´CTé‡å»ºé¢†åŸŸæ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºXå°„çº¿æ¨¡æ€çš„äºŒç»´æ½œåœ¨è¡¨ç¤ºå’ŒCTæ¨¡æ€çš„ä¸‰ç»´æ½œåœ¨è¡¨ç¤ºä¹‹é—´å­˜åœ¨å·¨å¤§å·®å¼‚ï¼Œæ ‡å‡†çš„LDMæ— æ³•åœ¨æ½œåœ¨ç©ºé—´å†…è¿›è¡Œæœ‰æ•ˆå¯¹é½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨æ¨¡æ€ä¸€è‡´æ½œåœ¨ç©ºé—´æ‰©æ•£æ¨¡å‹ï¼ˆCLS-DMï¼‰ï¼Œå®ƒç»“åˆäº†è·¨æ¨¡æ€ç‰¹å¾å¯¹æ¯”å­¦ä¹ ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä»äºŒç»´Xå°„çº¿å›¾åƒä¸­æå–æ½œåœ¨çš„ä¸‰ç»´ä¿¡æ¯ï¼Œå¹¶åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´å®ç°æ½œåœ¨ç©ºé—´å¯¹é½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLS-DMåœ¨LIDC-IDRIå’ŒCTSpine1Kæ•°æ®é›†ä¸Šï¼Œåœ¨æ ‡å‡†ä½“ç´ çº§æŒ‡æ ‡ï¼ˆPSNRã€SSIMï¼‰æ–¹é¢ä¼˜äºç»å…¸å’Œæœ€æ–°çš„ç”Ÿæˆæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•ä¸ä»…æœ‰åŠ©äºæé«˜ç¨€ç–Xå°„çº¿é‡å»ºCTçš„æœ‰æ•ˆæ€§å’Œç»æµå¯è¡Œæ€§ï¼Œè¿˜å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–è·¨æ¨¡æ€è½¬æ¢ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°å›¾åƒçš„åˆæˆã€‚ä¸ºäº†æ–¹ä¾¿å…¶ä»–é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨ï¼Œæˆ‘ä»¬å·²å°†ä»£ç å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/CLS-DM-50D6/%E4%B8%8A%E3%80%82">https://anonymous.4open.science/r/CLS-DM-50D6/ä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11152v1">PDF</a> ACMMM2025 Accepted</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä¸»è¦æ¢è®¨äº†è®¡ç®—å±‚ææˆåƒï¼ˆCTï¼‰é¢ä¸´çš„æŒ‘æˆ˜ä»¥åŠåŸºäºç¨€ç–è§†è§’Xå°„çº¿å›¾åƒçš„CTé‡å»ºæ–¹æ³•çš„å‘å±•ã€‚ç”±äºCTæˆåƒéœ€è¦å¤§é‡çš„æ—‹è½¬Xå°„çº¿é˜µåˆ—ï¼Œå› æ­¤å­˜åœ¨æ—¶é—´é•¿ã€è¾å°„æš´éœ²é«˜ç­‰é—®é¢˜ã€‚ç ”ç©¶äººå‘˜å…³æ³¨æ‰©æ•£æ¨¡å‹åœ¨CTé‡å»ºä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ã€‚ç„¶è€Œï¼Œç”±äºäºŒç»´Xå°„çº¿æ¨¡æ€çš„æ½œåœ¨è¡¨ç¤ºä¸ä¸‰ç»´CTæ¨¡æ€çš„æ½œåœ¨è¡¨ç¤ºä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼ŒLDMæ— æ³•æœ‰æ•ˆåœ°å¯¹é½æ½œåœ¨ç©ºé—´ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†è·¨æ¨¡æ€ç‰¹å¾å¯¹æ¯”å­¦ä¹ çš„ä¸€è‡´æ½œåœ¨ç©ºé—´æ‰©æ•£æ¨¡å‹ï¼ˆCLS-DMï¼‰ã€‚CLS-DMå¯ä»¥ä»äºŒç»´Xå°„çº¿å›¾åƒä¸­æå–æ½œåœ¨çš„ä¸‰ç»´ä¿¡æ¯ï¼Œå¹¶åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´å®ç°å¯¹é½æ½œåœ¨ç©ºé—´ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLS-DMåœ¨LIDC-IDRIå’ŒCTSpine1Kæ•°æ®é›†ä¸Šä¼˜äºç»å…¸å’Œæœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹çš„æ ‡å‡†ä½“ç´ çº§æŒ‡æ ‡ï¼ˆPSNRã€SSIMï¼‰ã€‚æ­¤æ–¹æ³•ä¸ä»…æé«˜äº†ç¨€ç–Xå°„çº¿é‡å»ºCTçš„æœ‰æ•ˆæ€§å’Œç»æµå¯è¡Œæ€§ï¼Œè¿˜å¯æ¨å¹¿åˆ°å…¶ä»–è·¨æ¨¡æ€è½¬æ¢ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°å›¾åƒåˆæˆã€‚æˆ‘ä»¬çš„ä»£ç å·²å…¬å¼€å¯ç”¨ï¼Œä»¥ä¿ƒè¿›å…¶ä»–é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®¡ç®—å±‚ææˆåƒï¼ˆCTï¼‰é¢ä¸´æ—¶é—´é•¿å’Œè¾å°„æš´éœ²é«˜ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨CTé‡å»ºä¸­å±•ç°å‡ºæ½œåŠ›ã€‚</li>
<li>LDMç”±äºäºŒç»´Xå°„çº¿ä¸ä¸‰ç»´CTçš„æ½œåœ¨è¡¨ç¤ºå·®å¼‚ï¼Œæ— æ³•æœ‰æ•ˆå¯¹é½æ½œåœ¨ç©ºé—´ã€‚</li>
<li>æå‡ºäº†åŸºäºè·¨æ¨¡æ€ç‰¹å¾å¯¹æ¯”å­¦ä¹ çš„ä¸€è‡´æ½œåœ¨ç©ºé—´æ‰©æ•£æ¨¡å‹ï¼ˆCLS-DMï¼‰ã€‚</li>
<li>CLS-DMå¯ä»äºŒç»´Xå°„çº¿å›¾åƒä¸­æå–æ½œåœ¨çš„ä¸‰ç»´ä¿¡æ¯ï¼Œå®ç°æ¨¡æ€é—´çš„æ½œåœ¨ç©ºé—´å¯¹é½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºCLS-DMåœ¨æ ‡å‡†ä½“ç´ çº§æŒ‡æ ‡ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11152">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8cc54aad21847a66cc556e599301048c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c2830682701469a6155538e05714611.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6fafce0fe28d73beb76915750c6b179.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf51a6a8bcdd6772d7272543cebc6e6d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Robust-ID-Specific-Face-Restoration-via-Alignment-Learning"><a href="#Robust-ID-Specific-Face-Restoration-via-Alignment-Learning" class="headerlink" title="Robust ID-Specific Face Restoration via Alignment Learning"></a>Robust ID-Specific Face Restoration via Alignment Learning</h2><p><strong>Authors:Yushun Fang, Lu Liu, Xiang Gao, Qiang Hu, Ning Cao, Jianghe Cui, Gang Chen, Xiaoyun Zhang</strong></p>
<p>The latest developments in Face Restoration have yielded significant advancements in visual quality through the utilization of diverse diffusion priors. Nevertheless, the uncertainty of face identity introduced by identity-obscure inputs and stochastic generative processes remains unresolved. To address this challenge, we present Robust ID-Specific Face Restoration (RIDFR), a novel ID-specific face restoration framework based on diffusion models. Specifically, RIDFR leverages a pre-trained diffusion model in conjunction with two parallel conditioning modules. The Content Injection Module inputs the severely degraded image, while the Identity Injection Module integrates the specific identity from a given image. Subsequently, RIDFR incorporates Alignment Learning, which aligns the restoration results from multiple references with the same identity in order to suppress the interference of ID-irrelevant face semantics (e.g. pose, expression, make-up, hair style). Experiments demonstrate that our framework outperforms the state-of-the-art methods, reconstructing high-quality ID-specific results with high identity fidelity and demonstrating strong robustness. </p>
<blockquote>
<p>é¢éƒ¨ä¿®å¤çš„æœ€æ–°å‘å±•é€šè¿‡åˆ©ç”¨å¤šç§æ‰©æ•£å…ˆéªŒçŸ¥è¯†åœ¨è§†è§‰è´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç”±èº«ä»½æ¨¡ç³Šè¾“å…¥å’Œéšæœºç”Ÿæˆè¿‡ç¨‹å¼•å…¥çš„èº«ä»½ä¸ç¡®å®šæ€§ä»æœªè§£å†³ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„Robust ID-Specific Face Restoration (RIDFR)è¿™ä¸€æ–°å‹èº«ä»½ç‰¹å®šé¢éƒ¨ä¿®å¤æ¡†æ¶ã€‚å…·ä½“è€Œè¨€ï¼ŒRIDFRåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œä¸¤ä¸ªå¹¶è¡Œæ¡ä»¶æ¨¡å—ã€‚å†…å®¹æ³¨å…¥æ¨¡å—è¾“å…¥ä¸¥é‡é€€åŒ–çš„å›¾åƒï¼Œè€Œèº«ä»½æ³¨å…¥æ¨¡å—åˆ™æ•´åˆç»™å®šå›¾åƒä¸­çš„ç‰¹å®šèº«ä»½ã€‚éšåï¼ŒRIDFRç»“åˆäº†å¯¹é½å­¦ä¹ ï¼Œé€šè¿‡å¯¹é½åŒä¸€èº«ä»½çš„å¤šä¸ªå‚è€ƒçš„ä¿®å¤ç»“æœï¼Œä»¥æŠ‘åˆ¶èº«ä»½æ— å…³é¢éƒ¨è¯­ä¹‰ï¼ˆå¦‚å§¿åŠ¿ã€è¡¨æƒ…ã€åŒ–å¦†ã€å‘å‹ï¼‰çš„å¹²æ‰°ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿé‡å»ºé«˜è´¨é‡çš„èº«ä»½ç‰¹å®šç»“æœï¼Œå…·æœ‰é«˜åº¦çš„èº«ä»½ä¿çœŸæ€§å’Œå¼ºå¤§çš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10943v1">PDF</a> 17 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§åä¸ºRobust ID-Specific Face Restorationï¼ˆRIDFRï¼‰çš„æ–°å‹é¢éƒ¨ä¿®å¤æ¡†æ¶ï¼Œç”¨äºè§£å†³èº«ä»½æ¨¡ç³Šè¾“å…¥å’Œéšæœºç”Ÿæˆè¿‡ç¨‹å¸¦æ¥çš„èº«ä»½ä¸ç¡®å®šæ€§é—®é¢˜ã€‚è¯¥æ¡†æ¶ç»“åˆé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸ä¸¤ä¸ªå¹¶è¡Œæ¡ä»¶æ¨¡å—ï¼Œé€šè¿‡å†…å®¹æ³¨å…¥æ¨¡å—å’Œèº«ä»½æ³¨å…¥æ¨¡å—å®ç°é¢éƒ¨ä¿®å¤ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†å¯¹é½å­¦ä¹ ï¼Œé€šè¿‡å¯¹é½å¤šä¸ªç›¸åŒèº«ä»½çš„å‚è€ƒç»“æœï¼ŒæŠ‘åˆ¶äº†ä¸èº«ä»½æ— å…³çš„é¢éƒ¨è¯­ä¹‰çš„å¹²æ‰°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨èº«ä»½ä¸€è‡´æ€§å’Œç¨³å¥æ€§æ–¹é¢è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€æ–°é¢éƒ¨ä¿®å¤æŠ€æœ¯é€šè¿‡åˆ©ç”¨å¤šæ ·çš„æ‰©æ•£å…ˆéªŒä¿¡æ¯åœ¨è§†è§‰è´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚</li>
<li>èº«ä»½æ¨¡ç³Šè¾“å…¥å’Œéšæœºç”Ÿæˆè¿‡ç¨‹å¯¼è‡´çš„èº«ä»½ä¸ç¡®å®šæ€§é—®é¢˜ä»ç„¶æœ‰å¾…è§£å†³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„é¢éƒ¨ä¿®å¤æ¡†æ¶RIDFRï¼ŒåŸºäºæ‰©æ•£æ¨¡å‹è¿›è¡ŒIDç‰¹å®šä¿®å¤ã€‚</li>
<li>RIDFRç»“åˆé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸ä¸¤ä¸ªå¹¶è¡Œæ¡ä»¶æ¨¡å—ï¼šå†…å®¹æ³¨å…¥æ¨¡å—å’Œèº«ä»½æ³¨å…¥æ¨¡å—ã€‚</li>
<li>RIDFRå¼•å…¥äº†å¯¹é½å­¦ä¹ ï¼Œé€šè¿‡å¯¹é½å¤šä¸ªç›¸åŒèº«ä»½çš„å‚è€ƒç»“æœæ¥æé«˜ç¨³å¥æ€§å¹¶æŠ‘åˆ¶æ— å…³è¯­ä¹‰çš„å¹²æ‰°ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒRIDFRæ¡†æ¶åœ¨èº«ä»½ä¸€è‡´æ€§å’Œç¨³å¥æ€§æ–¹é¢è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10943">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ecfb3672f9341e7433e853e783c18270.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b70e2b7a1e3eb697a4a0f8eeff750522.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1591aef3b06be5e572a70b58149578da.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Text-Embedding-Knows-How-to-Quantize-Text-Guided-Diffusion-Models"><a href="#Text-Embedding-Knows-How-to-Quantize-Text-Guided-Diffusion-Models" class="headerlink" title="Text Embedding Knows How to Quantize Text-Guided Diffusion Models"></a>Text Embedding Knows How to Quantize Text-Guided Diffusion Models</h2><p><strong>Authors:Hongjae Lee, Myungjun Son, Dongjea Kang, Seung-Won Jung</strong></p>
<p>Despite the success of diffusion models in image generation tasks such as text-to-image, the enormous computational complexity of diffusion models limits their use in resource-constrained environments. To address this, network quantization has emerged as a promising solution for designing efficient diffusion models. However, existing diffusion model quantization methods do not consider input conditions, such as text prompts, as an essential source of information for quantization. In this paper, we propose a novel quantization method dubbed Quantization of Language-to-Image diffusion models using text Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit precision for every layer at each time step. In addition, QLIP can be seamlessly integrated into existing quantization methods to enhance quantization efficiency. Our extensive experiments demonstrate the effectiveness of QLIP in reducing computational complexity and improving the quality of the generated images across various datasets. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç­‰å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å…¶å·¨å¤§çš„è®¡ç®—å¤æ‚åº¦é™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç½‘ç»œé‡åŒ–ä½œä¸ºè®¾è®¡é«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆè€Œå‡ºç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ‰©æ•£æ¨¡å‹é‡åŒ–æ–¹æ³•æ²¡æœ‰è€ƒè™‘è¾“å…¥æ¡ä»¶ï¼Œå¦‚æ–‡æœ¬æç¤ºï¼Œä½œä¸ºé‡åŒ–çš„é‡è¦ä¿¡æ¯æ¥æºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–æ–¹æ³•ï¼Œç§°ä¸ºä½¿ç”¨æ–‡æœ¬æç¤ºçš„è¯­è¨€åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹é‡åŒ–ï¼ˆQLIPï¼‰ã€‚QLIPåˆ©ç”¨æ–‡æœ¬æç¤ºæ¥æŒ‡å¯¼æ¯ä¸ªæ—¶é—´æ­¥é•¿æ¯ä¸€å±‚çš„ä½ç²¾åº¦çš„é€‰æ‹©ã€‚æ­¤å¤–ï¼ŒQLIPå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰é‡åŒ–æ–¹æ³•ä¸­ï¼Œä»¥æé«˜é‡åŒ–çš„æ•ˆç‡ã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒQLIPåœ¨é™ä½è®¡ç®—å¤æ‚åº¦å’Œæé«˜å„ç§æ•°æ®é›†ç”Ÿæˆçš„å›¾åƒè´¨é‡æ–¹é¢æ˜¯æœ‰æ•ˆçš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10340v2">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ°å›¾åƒï¼‰ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å…¶å·¨å¤§çš„è®¡ç®—å¤æ‚æ€§é™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„ä½¿ç”¨ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œç½‘ç»œé‡åŒ–å·²æˆä¸ºè®¾è®¡é«˜æ•ˆæ‰©æ•£æ¨¡å‹çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ‰©æ•£æ¨¡å‹é‡åŒ–æ–¹æ³•æ²¡æœ‰è€ƒè™‘è¾“å…¥æ¡ä»¶ï¼Œå¦‚æ–‡æœ¬æç¤ºï¼Œä½œä¸ºé‡åŒ–çš„é‡è¦ä¿¡æ¯æ¥æºã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–æ–¹æ³•ï¼Œç§°ä¸ºä½¿ç”¨æ–‡æœ¬æç¤ºçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„é‡åŒ–ï¼ˆQLIPï¼‰ã€‚QLIPåˆ©ç”¨æ–‡æœ¬æç¤ºæ¥æŒ‡å¯¼æ¯ä¸ªæ—¶é—´æ­¥çš„æ¯ä¸€å±‚çš„ä½ç²¾åº¦é€‰æ‹©ã€‚æ­¤å¤–ï¼ŒQLIPå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„é‡åŒ–æ–¹æ³•ä¸­ï¼Œä»¥æé«˜é‡åŒ–çš„æ•ˆç‡ã€‚å®éªŒè¯æ˜ï¼ŒQLIPåœ¨é™ä½è®¡ç®—å¤æ‚åº¦å’Œæé«˜ç”Ÿæˆå›¾åƒè´¨é‡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è®¡ç®—å¤æ‚æ€§é™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚</li>
<li>ç½‘ç»œé‡åŒ–æ˜¯è§£å†³æ‰©æ•£æ¨¡å‹è®¡ç®—å¤æ‚æ€§çš„ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ‰©æ•£æ¨¡å‹é‡åŒ–æ–¹æ³•æœªå……åˆ†åˆ©ç”¨è¾“å…¥æ¡ä»¶ï¼ˆå¦‚æ–‡æœ¬æç¤ºï¼‰ä½œä¸ºé‡åŒ–è¿‡ç¨‹ä¸­çš„é‡è¦ä¿¡æ¯ã€‚</li>
<li>QLIPæ˜¯ä¸€ç§æ–°çš„æ‰©æ•£æ¨¡å‹é‡åŒ–æ–¹æ³•ï¼Œåˆ©ç”¨æ–‡æœ¬æç¤ºæ¥æŒ‡å¯¼ä½ç²¾åº¦çš„é€‰æ‹©ã€‚</li>
<li>QLIPå¯ä»¥æé«˜é‡åŒ–æ•ˆç‡ï¼Œå¹¶å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„é‡åŒ–æ–¹æ³•ä¸­ã€‚</li>
<li>QLIPåœ¨é™ä½è®¡ç®—å¤æ‚åº¦å’Œæé«˜ç”Ÿæˆå›¾åƒè´¨é‡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>å¹¿æ³›å®éªŒè¯æ˜äº†QLIPçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10340">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f7ff2dcb59150289b78add67fea16f49.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e64aa29e95b9721956b28ec3eaf96ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2aa3b77751ac59de3a65f3cd6e077e15.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74ec4d790da943e1abb6c91946f18c89.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7270cdb683832881bd87b1ecbc6665f0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3efef1a659ef791352fd6decd7a2b92a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Mind-the-Gap-Aligning-Vision-Foundation-Models-to-Image-Feature-Matching"><a href="#Mind-the-Gap-Aligning-Vision-Foundation-Models-to-Image-Feature-Matching" class="headerlink" title="Mind the Gap: Aligning Vision Foundation Models to Image Feature   Matching"></a>Mind the Gap: Aligning Vision Foundation Models to Image Feature   Matching</h2><p><strong>Authors:Yuhan Liu, Jingwen Fu, Yang Wu, Kangyi Wu, Pengna Li, Jiayi Wu, Sanping Zhou, Jingmin Xin</strong></p>
<p>Leveraging the vision foundation models has emerged as a mainstream paradigm that improves the performance of image feature matching. However, previous works have ignored the misalignment when introducing the foundation models into feature matching. The misalignment arises from the discrepancy between the foundation models focusing on single-image understanding and the cross-image understanding requirement of feature matching. Specifically, 1) the embeddings derived from commonly used foundation models exhibit discrepancies with the optimal embeddings required for feature matching; 2) lacking an effective mechanism to leverage the single-image understanding ability into cross-image understanding. A significant consequence of the misalignment is they struggle when addressing multi-instance feature matching problems. To address this, we introduce a simple but effective framework, called IMD (Image feature Matching with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant solutions employing contrastive-learning based foundation models that emphasize global semantics, we integrate the generative-based diffusion models to effectively capture instance-level details. 2) We leverage the prompt mechanism in generative model as a natural tunnel, propose a novel cross-image interaction prompting module to facilitate bidirectional information interaction between image pairs. To more accurately measure the misalignment, we propose a new benchmark called IMIM, which focuses on multi-instance scenarios. Our proposed IMD establishes a new state-of-the-art in commonly evaluated benchmarks, and the superior improvement 12% in IMIM indicates our method efficiently mitigates the misalignment. </p>
<blockquote>
<p>åˆ©ç”¨è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆvision foundation modelsï¼‰ä½œä¸ºä¸»æµèŒƒå¼æ¥æé«˜å›¾åƒç‰¹å¾åŒ¹é…çš„æ•ˆèƒ½å·²æˆä¸ºè¡Œä¸šä¸»æµè¶‹åŠ¿ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„å·¥ä½œåœ¨å°†åŸºç¡€æ¨¡å‹å¼•å…¥ç‰¹å¾åŒ¹é…æ—¶å¿½ç•¥äº†é”™é…é—®é¢˜ã€‚è¿™ç§é”™é…æºäºåŸºç¡€æ¨¡å‹å¯¹å•å›¾ç†è§£çš„éœ€æ±‚ä¸ç‰¹å¾åŒ¹é…å¯¹è·¨å›¾åƒç†è§£éœ€æ±‚ä¹‹é—´çš„åˆ†æ­§ã€‚å…·ä½“æ¥è¯´ï¼Œå­˜åœ¨çš„ç—›ç‚¹æœ‰ä¸¤ç‚¹ï¼šä¸€æ˜¯å¸¸ç”¨çš„åŸºç¡€æ¨¡å‹ç”Ÿæˆçš„åµŒå…¥ç‰¹å¾ä¸ä¸ºå›¾åƒç‰¹å¾åŒ¹é…è®¾è®¡æ‰€éœ€è¦çš„æœ€ä¼˜åµŒå…¥ç‰¹å¾å­˜åœ¨åå·®ï¼›äºŒæ˜¯ç¼ºä¹ä¸€ç§æœ‰æ•ˆçš„æœºåˆ¶å°†å•å›¾ç†è§£èƒ½åŠ›è½¬åŒ–ä¸ºè·¨å›¾ç†è§£èƒ½åŠ›ã€‚é”™é…çš„ä¸€ä¸ªæ˜¾è‘—åæœæ˜¯å®ƒä»¬åœ¨å¤„ç†å¤šå®ä¾‹ç‰¹å¾åŒ¹é…é—®é¢˜æ—¶é¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç§°ä¸ºIMDï¼ˆå¸¦æœ‰é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹çš„å›¾åƒç‰¹å¾åŒ¹é…ï¼‰ï¼Œå®ƒåŒ…å«ä¸¤éƒ¨åˆ†ï¼šé¦–å…ˆï¼Œä¸åŒäºé‡‡ç”¨åŸºäºå¯¹æ¯”å­¦ä¹ çš„ä¸»æµåŸºç¡€æ¨¡å‹å¼ºè°ƒå…¨å±€è¯­ä¹‰çš„åšæ³•ï¼Œæˆ‘ä»¬æ•´åˆäº†åŸºäºç”Ÿæˆçš„æ‰©æ•£æ¨¡å‹ï¼Œä»¥æœ‰æ•ˆæ•æ‰å®ä¾‹çº§ç»†èŠ‚ï¼›å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨ç”Ÿæˆæ¨¡å‹ä¸­çš„æç¤ºæœºåˆ¶ä½œä¸ºè‡ªç„¶é€šé“ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„è·¨å›¾åƒäº¤äº’æç¤ºæ¨¡å—ï¼Œä»¥ä¿ƒè¿›å›¾åƒå¯¹ä¹‹é—´çš„åŒå‘ä¿¡æ¯äº¤äº’ã€‚ä¸ºäº†æ›´å‡†ç¡®åœ°æµ‹é‡é”™é…æƒ…å†µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•IMIMï¼Œå®ƒä¸“æ³¨äºå¤šå®ä¾‹åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºçš„IMDåœ¨å¸¸ç”¨çš„åŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå¹¶ä¸”åœ¨IMIMä¸Šçš„å“è¶Šæ”¹è¿›æå‡äº†12%ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°ç¼“è§£äº†é”™é…é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10318v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒç‰¹å¾åŒ¹é…çš„æ–¹æ³•å·²ç»å´­éœ²å¤´è§’ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¿½è§†äº†å¼•å…¥åŸºç¡€æ¨¡å‹æ—¶çš„ç‰¹å¾ä¸åŒ¹é…é—®é¢˜ã€‚è¯¥é—®é¢˜æºäºåŸºç¡€æ¨¡å‹å…³æ³¨å•å›¾ç†è§£ï¼Œè€Œç‰¹å¾åŒ¹é…éœ€è¦è·¨å›¾ç†è§£ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶IMDï¼Œèåˆäº†ç”Ÿæˆå¼çš„æ‰©æ•£æ¨¡å‹æ¥æ•æ‰å®ä¾‹çº§åˆ«çš„ç»†èŠ‚ï¼Œå¹¶åˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„æç¤ºæœºåˆ¶æ¥ä¿ƒè¿›å›¾åƒå¯¹ä¹‹é—´çš„åŒå‘ä¿¡æ¯äº¤æµã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†æ–°çš„åŸºå‡†æµ‹è¯•IMIMæ¥æ›´å‡†ç¡®åœ°è¡¡é‡ä¸åŒ¹é…é—®é¢˜ï¼Œè€ŒIMDåœ¨å¸¸è§„åŸºå‡†æµ‹è¯•ä¸­å»ºç«‹äº†æ–°çš„æœ€é«˜æ°´å¹³ï¼Œå¹¶åœ¨IMIMä¸Šå®ç°äº†é«˜è¾¾12%çš„ä¼˜å¼‚æ”¹è¿›ï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°ç¼“è§£äº†ä¸åŒ¹é…é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹æ”¹è¿›å›¾åƒç‰¹å¾åŒ¹é…å·²æˆä¸ºä¸»æµèŒƒå¼ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½è§†äº†å¼•å…¥åŸºç¡€æ¨¡å‹æ—¶çš„ç‰¹å¾ä¸åŒ¹é…é—®é¢˜ã€‚</li>
<li>ç‰¹å¾ä¸åŒ¹é…æºäºåŸºç¡€æ¨¡å‹å…³æ³¨å•å›¾ç†è§£ï¼Œè€Œç‰¹å¾åŒ¹é…éœ€è¦è·¨å›¾ç†è§£ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„å›¾åƒç‰¹å¾åŒ¹é…æ¡†æ¶IMDï¼Œèåˆäº†ç”Ÿæˆå¼çš„æ‰©æ•£æ¨¡å‹å¹¶å¼•å…¥äº†è·¨å›¾äº¤äº’æç¤ºæ¨¡å—ã€‚</li>
<li>ä¸ºæ›´å‡†ç¡®åœ°è¡¡é‡ä¸åŒ¹é…é—®é¢˜ï¼Œå¼•å…¥äº†æ–°çš„åŸºå‡†æµ‹è¯•IMIMã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10318">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b08340337df7f32f2cf9ff0b19db64b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87b2d554f56b2551404030a197fcdeb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f08b5db22167966a126ded28e63e838.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Synthesizing-Near-Boundary-OOD-Samples-for-Out-of-Distribution-Detection"><a href="#Synthesizing-Near-Boundary-OOD-Samples-for-Out-of-Distribution-Detection" class="headerlink" title="Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection"></a>Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection</h2><p><strong>Authors:Jinglun Li, Kaixun Jiang, Zhaoyu Chen, Bo Lin, Yao Tang, Weifeng Ge, Wenqiang Zhang</strong></p>
<p>Pre-trained vision-language models have exhibited remarkable abilities in detecting out-of-distribution (OOD) samples. However, some challenging OOD samples, which lie close to in-distribution (InD) data in image feature space, can still lead to misclassification. The emergence of foundation models like diffusion models and multimodal large language models (MLLMs) offers a potential solution to this issue. In this work, we propose SynOOD, a novel approach that harnesses foundation models to generate synthetic, challenging OOD data for fine-tuning CLIP models, thereby enhancing boundary-level discrimination between InD and OOD samples. Our method uses an iterative in-painting process guided by contextual prompts from MLLMs to produce nuanced, boundary-aligned OOD samples. These samples are refined through noise adjustments based on gradients from OOD scores like the energy score, effectively sampling from the InD&#x2F;OOD boundary. With these carefully synthesized images, we fine-tune the CLIP image encoder and negative label features derived from the text encoder to strengthen connections between near-boundary OOD samples and a set of negative labels. Finally, SynOOD achieves state-of-the-art performance on the large-scale ImageNet benchmark, with minimal increases in parameters and runtime. Our approach significantly surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by 11.13%. Codes are available in <a target="_blank" rel="noopener" href="https://github.com/Jarvisgivemeasuit/SynOOD">https://github.com/Jarvisgivemeasuit/SynOOD</a>. </p>
<blockquote>
<p>é¢„è®­ç»ƒçš„è¯­è¨€è§†è§‰æ¨¡å‹åœ¨æ£€æµ‹åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ ·æœ¬æ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œä¸€äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„OODæ ·æœ¬ï¼Œåœ¨å›¾åƒç‰¹å¾ç©ºé—´ä¸­ä¸åˆ†å¸ƒå†…ï¼ˆInDï¼‰æ•°æ®ç›¸é‚»ï¼Œä»å¯èƒ½å¯¼è‡´è¯¯åˆ†ç±»ã€‚æ‰©æ•£æ¨¡å‹å’Œå¤šåª’ä½“è¯­è¨€æ¨¡å‹ç­‰åŸºç¡€æ¨¡å‹çš„å…´èµ·ä¸ºè§£å†³è¿™ä¸€é—®é¢˜æä¾›äº†æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SynOODï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆåˆæˆã€å…·æœ‰æŒ‘æˆ˜æ€§çš„OODæ•°æ®ï¼Œå¯¹CLIPæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„æ–°æ–¹æ³•ï¼Œä»è€Œå¢å¼ºInDå’ŒOODæ ·æœ¬ä¹‹é—´è¾¹ç•Œçº§åˆ«çš„åˆ¤åˆ«èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ä¸€ä¸ªç”±å¤šåª’ä½“è¯­è¨€æ¨¡å‹æä¾›çš„ä¸Šä¸‹æ–‡æç¤ºå¼•å¯¼çš„è¿­ä»£å¡«å……è¿‡ç¨‹æ¥äº§ç”Ÿå¾®å¦™çš„ã€ä¸è¾¹ç•Œå¯¹é½çš„OODæ ·æœ¬ã€‚è¿™äº›æ ·æœ¬é€šè¿‡åŸºäºOODåˆ†æ•°ï¼ˆå¦‚èƒ½é‡åˆ†æ•°ï¼‰çš„æ¢¯åº¦è¿›è¡Œå™ªå£°è°ƒæ•´æ¥ç»†åŒ–å¤„ç†ï¼Œæœ‰æ•ˆåœ°ä»InD&#x2F;OODè¾¹ç•Œè¿›è¡Œé‡‡æ ·ã€‚ä½¿ç”¨è¿™äº›ç²¾å¿ƒåˆæˆçš„å›¾åƒï¼Œæˆ‘ä»¬å¾®è°ƒCLIPå›¾åƒç¼–ç å™¨å’Œä»æ–‡æœ¬ç¼–ç å™¨æ´¾ç”Ÿçš„è´Ÿæ ‡ç­¾ç‰¹å¾ï¼Œä»¥åŠ å¼ºè¿‘è¾¹ç•ŒOODæ ·æœ¬ä¸ä¸€ç»„è´Ÿæ ‡ç­¾ä¹‹é—´çš„è”ç³»ã€‚æœ€åï¼ŒSynOODåœ¨å¤§è§„æ¨¡ImageNetåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨å‚æ•°å’Œè¿è¡Œæ—¶å¢åŠ æœ€å°çš„æƒ…å†µä¸‹å®ç°äº†æ˜¾è‘—è¶…è¶Šç°æœ‰æ–¹æ³•çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†AUROCæé«˜äº†2.80%ï¼Œå°†FPR95é™ä½äº†11.13%ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡é“¾æ¥<a target="_blank" rel="noopener" href="https://github.com/Jarvisgivemeasuit/SynOOD%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Jarvisgivemeasuit/SynOODè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10225v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ£€æµ‹åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ ·æœ¬æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å¯¹äºä¸åˆ†å¸ƒå†…ï¼ˆInDï¼‰æ•°æ®åœ¨å›¾åƒç‰¹å¾ç©ºé—´ç›¸è¿‘çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„OODæ ·æœ¬ï¼Œä»å¯èƒ½å¯¼è‡´è¯¯åˆ†ç±»ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åˆ©ç”¨åŸºç¡€æ¨¡å‹å¦‚æ‰©æ•£æ¨¡å‹å’Œå¤šåª’ä½“å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”Ÿæˆåˆæˆã€å…·æœ‰æŒ‘æˆ˜æ€§çš„OODæ•°æ®ï¼Œä»¥å¾®è°ƒCLIPæ¨¡å‹çš„æ–¹æ³•ï¼Œä»è€Œæé«˜InDå’ŒOODæ ·æœ¬ä¹‹é—´çš„è¾¹ç•Œçº§åˆ«åˆ¤åˆ«èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£å¡«å……è¿‡ç¨‹ï¼Œç»“åˆMLLMsçš„ä¸Šä¸‹æ–‡æç¤ºï¼Œç”Ÿæˆå¾®å¦™çš„ã€ä¸è¾¹ç•Œå¯¹é½çš„OODæ ·æœ¬ã€‚è¿™äº›æ ·æœ¬é€šè¿‡åŸºäºOODåˆ†æ•°ï¼ˆå¦‚èƒ½é‡åˆ†æ•°ï¼‰çš„æ¢¯åº¦è¿›è¡Œå™ªå£°è°ƒæ•´ï¼Œæœ‰æ•ˆåœ°ä»InD&#x2F;OODè¾¹ç•Œè¿›è¡Œé‡‡æ ·ã€‚ä½¿ç”¨è¿™äº›ç²¾å¿ƒåˆæˆçš„å›¾åƒï¼Œæˆ‘ä»¬å¾®è°ƒCLIPå›¾åƒç¼–ç å™¨å’Œæ¥è‡ªæ–‡æœ¬ç¼–ç å™¨çš„è´Ÿæ ‡ç­¾ç‰¹å¾ï¼Œä»¥åŠ å¼ºè¿‘è¾¹ç•ŒOODæ ·æœ¬ä¸ä¸€ç»„è´Ÿæ ‡ç­¾ä¹‹é—´çš„è”ç³»ã€‚æœ€ç»ˆï¼ŒSynOODåœ¨å¤§å‹ImageNetåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œå‚æ•°å’Œè¿è¡Œæ—¶ç•¥æœ‰å¢åŠ ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæé«˜äº†AUROCå€¼2.80%ï¼Œå¹¶é™ä½äº†FPR95å€¼11.13%ã€‚ç›¸å…³ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šä¾›å…¬ä¼—å‚è€ƒå’Œåº”ç”¨ã€‚è¯¦æƒ…è¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/Jarvisgivemeasuit/SynOOD">é“¾æ¥åœ°å€</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ£€æµ‹æŸäº›OODæ ·æœ¬æ—¶å­˜åœ¨è¯¯åˆ†ç±»é£é™©ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä¸InDæ•°æ®ç›¸è¿‘çš„æ ·æœ¬ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆ©ç”¨åŸºç¡€æ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹å’ŒMLLMsï¼‰ç”ŸæˆåˆæˆOODæ•°æ®çš„æ–¹æ³•ï¼Œä»¥æ”¹è¿›æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡è¿­ä»£å¡«å……è¿‡ç¨‹å’Œä¸Šä¸‹æ–‡æç¤ºç”Ÿæˆå¾®å¦™çš„ã€ä¸è¾¹ç•Œå¯¹é½çš„OODæ ·æœ¬ã€‚</li>
<li>ä½¿ç”¨OODåˆ†æ•°ï¼ˆå¦‚èƒ½é‡åˆ†æ•°ï¼‰çš„æ¢¯åº¦è¿›è¡Œå™ªå£°è°ƒæ•´ï¼Œæœ‰æ•ˆé‡‡æ ·InD&#x2F;OODè¾¹ç•Œã€‚</li>
<li>é€šè¿‡å¾®è°ƒCLIPæ¨¡å‹çš„å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ï¼Œå¢å¼ºè¿‘è¾¹ç•ŒOODæ ·æœ¬ä¸è´Ÿæ ‡ç­¾çš„è”ç³»ã€‚</li>
<li>SynOODæ–¹æ³•åœ¨ImageNetåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-90d08f79db9d798b76ff65d532dbe5a8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c0228b4aa75a34ccc7a9f777232942a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-014152f40cd33b577bc10e2c7878cd59.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-681933d97e047dcacb3d67817f780db4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="From-Wardrobe-to-Canvas-Wardrobe-Polyptych-LoRA-for-Part-level-Controllable-Human-Image-Generation"><a href="#From-Wardrobe-to-Canvas-Wardrobe-Polyptych-LoRA-for-Part-level-Controllable-Human-Image-Generation" class="headerlink" title="From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level   Controllable Human Image Generation"></a>From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level   Controllable Human Image Generation</h2><p><strong>Authors:Jeongho Kim, Sunghyun Park, Hyoungwoo Park, Sungrack Yun, Jaegul Choo, Seokeon Cho</strong></p>
<p>Recent diffusion models achieve personalization by learning specific subjects, allowing learned attributes to be integrated into generated images. However, personalized human image generation remains challenging due to the need for precise and consistent attribute preservation (e.g., identity, clothing details). Existing subject-driven image generation methods often require either (1) inference-time fine-tuning with few images for each new subject or (2) large-scale dataset training for generalization. Both approaches are computationally expensive and impractical for real-time applications. To address these limitations, we present Wardrobe Polyptych LoRA, a novel part-level controllable model for personalized human image generation. By training only LoRA layers, our method removes the computational burden at inference while ensuring high-fidelity synthesis of unseen subjects. Our key idea is to condition the generation on the subjectâ€™s wardrobe and leverage spatial references to reduce information loss, thereby improving fidelity and consistency. Additionally, we introduce a selective subject region loss, which encourages the model to disregard some of reference images during training. Our loss ensures that generated images better align with text prompts while maintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no additional parameters at the inference stage and performs generation using a single model trained on a few training samples. We construct a new dataset and benchmark tailored for personalized human image generation. Extensive experiments show that our approach significantly outperforms existing techniques in fidelity and consistency, enabling realistic and identity-preserving full-body synthesis. </p>
<blockquote>
<p>æœ€è¿‘çš„æ‰©æ•£æ¨¡å‹é€šè¿‡å­¦ä¹ ç‰¹å®šä¸»é¢˜æ¥å®ç°ä¸ªæ€§åŒ–ï¼Œä½¿å¾—å­¦ä¹ åˆ°çš„å±æ€§å¯ä»¥é›†æˆåˆ°ç”Ÿæˆçš„å›¾åƒä¸­ã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦ç²¾ç¡®ä¸”ä¸€è‡´åœ°ä¿ç•™å±æ€§ï¼ˆä¾‹å¦‚èº«ä»½ã€æœè£…ç»†èŠ‚ï¼‰ï¼Œä¸ªæ€§åŒ–çš„äººç‰©å›¾åƒç”Ÿæˆä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„ä¸»é¢˜é©±åŠ¨å›¾åƒç”Ÿæˆæ–¹æ³•é€šå¸¸éœ€è¦ï¼ˆ1ï¼‰é’ˆå¯¹æ¯ä¸ªæ–°ä¸»é¢˜çš„å°‘é‡å›¾åƒè¿›è¡Œæ¨ç†æ—¶é—´å¾®è°ƒï¼Œæˆ–ï¼ˆ2ï¼‰å¤§è§„æ¨¡æ•°æ®é›†è®­ç»ƒä»¥å®ç°é€šç”¨æ€§ã€‚è¿™ä¸¤ç§æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä¸é€‚ç”¨äºå®æ—¶åº”ç”¨ç¨‹åºã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†Wardrobe Polyptych LoRAï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä¸ªæ€§åŒ–äººç‰©å›¾åƒç”Ÿæˆçš„æ–°å‹å±€éƒ¨å¯æ§æ¨¡å‹ã€‚é€šè¿‡ä»…è®­ç»ƒLoRAå±‚ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¨ç†æ—¶å»é™¤äº†è®¡ç®—è´Ÿæ‹…ï¼ŒåŒæ—¶ç¡®ä¿äº†æœªè§ä¸»ä½“çš„é«˜ä¿çœŸåˆæˆã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯æ ¹æ®ä¸»ä½“çš„è¡£æ©±è¿›è¡Œç”Ÿæˆï¼Œå¹¶åˆ©ç”¨ç©ºé—´å‚è€ƒæ¥å‡å°‘ä¿¡æ¯æŸå¤±ï¼Œä»è€Œæé«˜ä¿çœŸåº¦å’Œä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é€‰æ‹©æ€§ä¸»é¢˜åŒºåŸŸæŸå¤±ï¼Œé¼“åŠ±æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¿½ç•¥éƒ¨åˆ†å‚è€ƒå›¾åƒã€‚æˆ‘ä»¬çš„æŸå¤±ç¡®ä¿ç”Ÿæˆå›¾åƒä¸æ–‡æœ¬æç¤ºæ›´å¥½åœ°å¯¹é½ï¼ŒåŒæ—¶ä¿æŒä¸»é¢˜å®Œæ•´æ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„Wardrobe Polyptych LoRAåœ¨æ¨ç†é˜¶æ®µä¸éœ€è¦æ·»åŠ ä»»ä½•é¢å¤–å‚æ•°ï¼Œå¹¶ä½¿ç”¨å•ä¸ªæ¨¡å‹åœ¨å°‘é‡è®­ç»ƒæ ·æœ¬ä¸Šè¿›è¡Œç”Ÿæˆã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé’ˆå¯¹ä¸ªæ€§åŒ–äººç‰©å›¾åƒç”Ÿæˆçš„æ–°æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿çœŸåº¦å’Œä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œèƒ½å¤Ÿå®ç°çœŸå®ä¸”èº«ä»½ä¿ç•™çš„å…¨èº«åˆæˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10217v1">PDF</a> 10 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºWardrobe Polyptych LoRAçš„æ–°å‹éƒ¨åˆ†å¯æ§æ¨¡å‹ï¼Œç”¨äºä¸ªæ€§åŒ–äººç±»å›¾åƒç”Ÿæˆã€‚è¯¥æ¨¡å‹é€šè¿‡ä»…è®­ç»ƒLoRAå±‚ï¼Œåœ¨æ¨ç†æ—¶å»é™¤è®¡ç®—è´Ÿæ‹…ï¼Œç¡®ä¿å¯¹æœªè§è¿‡çš„ä¸»ä½“è¿›è¡Œé«˜ä¿çœŸåˆæˆã€‚é€šè¿‡æ¡ä»¶ç”Ÿæˆä¸»ä½“çš„è¡£æŸœå¹¶åˆ©ç”¨ç©ºé—´å‚è€ƒï¼Œå‡å°‘äº†ä¿¡æ¯æŸå¤±ï¼Œæé«˜äº†ä¿çœŸåº¦å’Œä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†é€‰æ‹©æ€§ä¸»ä½“åŒºåŸŸæŸå¤±ï¼Œé¼“åŠ±æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¿½ç•¥éƒ¨åˆ†å‚è€ƒå›¾åƒï¼Œä½¿ç”Ÿæˆçš„å›¾åƒæ›´ç¬¦åˆæ–‡æœ¬æç¤ºï¼ŒåŒæ—¶ä¿æŒä¸»ä½“å®Œæ•´æ€§ã€‚è¯¥æ¨¡å‹æ— éœ€åœ¨æ¨ç†é˜¶æ®µæ·»åŠ é¢å¤–å‚æ•°ï¼Œä½¿ç”¨å°‘é‡è®­ç»ƒæ ·æœ¬å³å¯è¿›è¡Œç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Wardrobe Polyptych LoRAæ¨¡å‹é€šè¿‡è®­ç»ƒç‰¹å®šçš„LoRAå±‚ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†æ—¶å®ç°é«˜æ•ˆè®¡ç®—ï¼ŒåŒæ—¶ç¡®ä¿é«˜ä¿çœŸåº¦çš„ä¸ªæ€§åŒ–äººç±»å›¾åƒç”Ÿæˆã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡æ¡ä»¶ç”Ÿæˆä¸»ä½“çš„è¡£æŸœå¹¶åˆ©ç”¨ç©ºé—´å‚è€ƒï¼Œæé«˜äº†å›¾åƒç”Ÿæˆçš„ä¿çœŸåº¦å’Œä¸€è‡´æ€§ã€‚</li>
<li>å¼•å…¥é€‰æ‹©æ€§ä¸»ä½“åŒºåŸŸæŸå¤±ï¼Œä½¿æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿå¿½ç•¥éƒ¨åˆ†å‚è€ƒå›¾åƒï¼Œä½¿ç”Ÿæˆçš„å›¾åƒæ›´ç¬¦åˆæ–‡æœ¬æç¤ºã€‚</li>
<li>Wardrobe Polyptych LoRAæ¨¡å‹åœ¨æ¨ç†é˜¶æ®µæ— éœ€é¢å¤–å‚æ•°ï¼Œä¸”èƒ½ç”¨å•ä¸ªæ¨¡å‹åœ¨å°‘é‡è®­ç»ƒæ ·æœ¬ä¸Šè¿›è¡Œç”Ÿæˆã€‚</li>
<li>æ¨¡å‹åœ¨æ–°æ„å»ºçš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒå¹¶è¯„ä¼°ï¼Œè¯¥æ•°æ®é›†ä¸“ä¸ºä¸ªæ€§åŒ–äººç±»å›¾åƒç”Ÿæˆè€Œè®¾è®¡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ä¿çœŸåº¦å’Œä¸€è‡´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œèƒ½å¤Ÿå®ç°çœŸå®ä¸”èº«ä»½ä¿å…¨çš„å…¨èº«åˆæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10217">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9b570fac2381b6cce71eb86d90cdbe42.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e135971aca4b94e1d03f56f67b568e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4457f578cbcd6f0b8998d04375c5ca40.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f2a89bfd56885b1077f86b642d9361fa.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="FIX-CLIP-Dual-Branch-Hierarchical-Contrastive-Learning-via-Synthetic-Captions-for-Better-Understanding-of-Long-Text"><a href="#FIX-CLIP-Dual-Branch-Hierarchical-Contrastive-Learning-via-Synthetic-Captions-for-Better-Understanding-of-Long-Text" class="headerlink" title="FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic   Captions for Better Understanding of Long Text"></a>FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic   Captions for Better Understanding of Long Text</h2><p><strong>Authors:Bingchao Wang, Zhiwei Ning, Jianyu Ding, Xuanang Gao, Yin Li, Dongsheng Jiang, Jie Yang, Wei Liu</strong></p>
<p>CLIP has shown promising performance across many short-text tasks in a zero-shot manner. However, limited by the input length of the text encoder, CLIP struggles on under-stream tasks with long-text inputs (&gt;77 tokens). To remedy this issue, we propose FIX-CLIP which includes three novel modules: (1) A dual-branch training pipeline that aligns short and long texts with masked and raw images respectively, which boosts the long-text representation while preserving the short-text ability. (2) Multiple learnable regional prompts with unidirectional masks in Transformer layers for regional information extraction. (3) A hierarchical feature alignment module in the intermediate encoder layers to promote the consistency of multi-scale features. Furthermore, we collect 30M images and utilize existing MLLMs to synthesize long-text captions for training. Extensive experiments show that FIX-CLIP achieves state-of-the-art performance on both long-text and short-text retrieval benchmarks. For downstream applications, we reveal that FIX-CLIPâ€™s text encoder delivers promising performance in a plug-and-play manner for diffusion models with long-text input. </p>
<blockquote>
<p>CLIPåœ¨è®¸å¤šçŸ­æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†é›¶æ ·æœ¬æ–¹å¼ä¸‹çš„è‰¯å¥½æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºæ–‡æœ¬ç¼–ç å™¨çš„è¾“å…¥é•¿åº¦é™åˆ¶ï¼ŒCLIPåœ¨å¤„ç†é•¿æ–‡æœ¬è¾“å…¥ï¼ˆ&gt;77ä¸ªä»¤ç‰Œï¼‰çš„ä¸‹æ¸¸ä»»åŠ¡æ—¶é‡åˆ°äº†å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FIX-CLIPï¼Œå®ƒåŒ…æ‹¬ä¸‰ä¸ªæ–°æ¨¡å—ï¼šï¼ˆ1ï¼‰åŒåˆ†æ”¯è®­ç»ƒç®¡é“ï¼Œåˆ†åˆ«ç”¨å¸¦æ©ç å’ŒåŸå§‹å›¾åƒå¯¹é½çŸ­æ–‡æœ¬å’Œé•¿æ–‡æœ¬ï¼Œè¿™æé«˜äº†é•¿æ–‡æœ¬çš„è¡¨ç¤ºèƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™äº†çŸ­æ–‡æœ¬çš„èƒ½åŠ›ã€‚ï¼ˆ2ï¼‰åœ¨Transformerå±‚ä¸­ä½¿ç”¨å¸¦æœ‰å•å‘æ©ç çš„å¯å­¦ä¹ åŒºåŸŸæç¤ºï¼Œç”¨äºåŒºåŸŸä¿¡æ¯æå–ã€‚ï¼ˆ3ï¼‰åœ¨ä¸­é—´ç¼–ç å™¨å±‚ä¸­çš„åˆ†å±‚ç‰¹å¾å¯¹é½æ¨¡å—ï¼Œä»¥ä¿ƒè¿›å¤šå°ºåº¦ç‰¹å¾çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¶é›†äº†3000ä¸‡å¼ å›¾åƒï¼Œå¹¶åˆ©ç”¨ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥åˆæˆé•¿æ–‡æœ¬æè¿°ç¬¦è¿›è¡Œè®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFIX-CLIPåœ¨é•¿æ–‡æœ¬å’ŒçŸ­æ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚å¯¹äºä¸‹æ¸¸åº”ç”¨ï¼Œæˆ‘ä»¬å‘ç°FIX-CLIPçš„æ–‡æœ¬ç¼–ç å™¨åœ¨ä»¥å³æ’å³ç”¨æ–¹å¼åº”ç”¨äºå…·æœ‰é•¿æ–‡æœ¬è¾“å…¥çš„æ‰©æ•£æ¨¡å‹æ—¶è¡¨ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10095v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CLIPåœ¨çŸ­æ–‡æœ¬ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å¤„ç†é•¿æ–‡æœ¬è¾“å…¥æ—¶å› æ–‡æœ¬ç¼–ç å™¨çš„è¾“å…¥é•¿åº¦é™åˆ¶è€Œè¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FIX-CLIPï¼ŒåŒ…æ‹¬ä¸‰ä¸ªæ–°é¢–æ¨¡å—ï¼š1ï¼‰åŒåˆ†æ”¯è®­ç»ƒç®¡é“ï¼Œåˆ†åˆ«å¯¹é½çŸ­æ–‡æœ¬å’Œé•¿æ–‡æœ¬ä¸æ©ç å’ŒåŸå§‹å›¾åƒï¼Œæå‡é•¿æ–‡æœ¬è¡¨å¾èƒ½åŠ›åŒæ—¶ä¿ç•™çŸ­æ–‡æœ¬èƒ½åŠ›ï¼›2ï¼‰åœ¨Transformerå±‚ä½¿ç”¨å¤šä¸ªå¯å­¦ä¹ åŒºåŸŸæç¤ºå’Œå•å‘æ©ç è¿›è¡ŒåŒºåŸŸä¿¡æ¯æå–ï¼›3ï¼‰åœ¨ä¸­é—´ç¼–ç å™¨å±‚ä¸­å¼•å…¥å±‚æ¬¡ç‰¹å¾å¯¹é½æ¨¡å—ï¼Œä¿ƒè¿›å¤šå°ºåº¦ç‰¹å¾çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¶é›†äº†3äº¿å¼ å›¾åƒå¹¶åˆ©ç”¨ç°æœ‰çš„MLLMsåˆæˆé•¿æ–‡æœ¬æè¿°è¿›è¡Œè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒFIX-CLIPåœ¨é•¿æ–‡æœ¬å’ŒçŸ­æ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­å±•ç°äº†å…¶åœ¨æ‰©æ•£æ¨¡å‹ä¸­çš„ä¼˜å¼‚è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLIPåœ¨çŸ­æ–‡æœ¬ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤„ç†é•¿æ–‡æœ¬è¾“å…¥æ—¶å­˜åœ¨é™åˆ¶ã€‚</li>
<li>FIX-CLIPåŒ…å«ä¸‰ä¸ªæ–°é¢–æ¨¡å—ï¼šåŒåˆ†æ”¯è®­ç»ƒç®¡é“ã€åŒºåŸŸä¿¡æ¯æå–æ¨¡å—å’Œå±‚æ¬¡ç‰¹å¾å¯¹é½æ¨¡å—ã€‚</li>
<li>åŒåˆ†æ”¯è®­ç»ƒç®¡é“èƒ½å¤Ÿæå‡é•¿æ–‡æœ¬è¡¨å¾èƒ½åŠ›å¹¶ä¿ç•™çŸ­æ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚</li>
<li>FIX-CLIPé‡‡ç”¨å¤šä¸ªå¯å­¦ä¹ åŒºåŸŸæç¤ºå’Œå•å‘æ©ç ä»¥å¢å¼ºåŒºåŸŸä¿¡æ¯æå–ã€‚</li>
<li>å±‚æ¬¡ç‰¹å¾å¯¹é½æ¨¡å—æœ‰åŠ©äºä¿ƒè¿›å¤šå°ºåº¦ç‰¹å¾çš„ä¸€è‡´æ€§ã€‚</li>
<li>æ”¶é›†å¤§é‡å›¾åƒå¹¶åˆ©ç”¨MLLMsåˆæˆé•¿æ–‡æœ¬æè¿°è¿›è¡Œè®­ç»ƒã€‚</li>
<li>FIX-CLIPåœ¨é•¿æ–‡æœ¬å’ŒçŸ­æ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-96547d5cbe92b9b502894b8d825946b7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-324ceacf2c6957b86a773dbad5cc6233.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f07ed829305c5a7b88e76ea1d9d7fca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-71a0369ee24e9dc3500519a414b784ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81f86c1340e6682e8301e31953f28508.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Frequency-Regulation-for-Exposure-Bias-Mitigation-in-Diffusion-Models"><a href="#Frequency-Regulation-for-Exposure-Bias-Mitigation-in-Diffusion-Models" class="headerlink" title="Frequency Regulation for Exposure Bias Mitigation in Diffusion Models"></a>Frequency Regulation for Exposure Bias Mitigation in Diffusion Models</h2><p><strong>Authors:Meng Yu, Kun Zhan</strong></p>
<p>Diffusion models exhibit impressive generative capabilities but are significantly impacted by exposure bias. In this paper, we make a key observation: the energy of the predicted noisy images decreases during the diffusion process. Building on this, we identify two important findings: 1) The reduction in energy follows distinct patterns in the low-frequency and high-frequency subbands; 2) This energy reduction results in amplitude variations between the network-reconstructed clean data and the real clean data. Based on the first finding, we introduce a frequency-domain regulation mechanism utilizing wavelet transforms, which separately adjusts the low- and high-frequency subbands. Leveraging the second insight, we provide a more accurate analysis of exposure bias in the two subbands. Our method is training-free and plug-and-play, significantly improving the generative quality of various diffusion models and providing a robust solution to exposure bias across different model architectures. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/kunzhan/wpp">https://github.com/kunzhan/wpp</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å—åˆ°æš´éœ²åå·®çš„æ˜¾è‘—å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå…³é”®å‘ç°ï¼šåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œé¢„æµ‹å™ªå£°å›¾åƒçš„èƒ½é‡ä¼šå‡å°‘ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å‘ç°äº†ä¸¤ä¸ªé‡è¦è§‚ç‚¹ï¼š1ï¼‰èƒ½é‡å‡å°‘åœ¨ä½é¢‘é¢‘å¸¦å’Œé«˜é¢‘é¢‘å¸¦ä¸­éµå¾ªä¸åŒçš„æ¨¡å¼ï¼›2ï¼‰è¿™ç§èƒ½é‡å‡å°‘å¯¼è‡´ç½‘ç»œé‡å»ºçš„æ¸…æ´æ•°æ®ä¸çœŸå®æ¸…æ´æ•°æ®ä¹‹é—´çš„æŒ¯å¹…å˜åŒ–ã€‚åŸºäºç¬¬ä¸€ä¸ªå‘ç°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ©ç”¨å°æ³¢å˜æ¢çš„é¢‘ç‡åŸŸè°ƒèŠ‚æœºåˆ¶ï¼Œè¯¥æœºåˆ¶å¯ä»¥åˆ†åˆ«è°ƒæ•´ä½é¢‘é¢‘å¸¦å’Œé«˜é¢‘é¢‘å¸¦ã€‚åˆ©ç”¨ç¬¬äºŒä¸ªè§è§£ï¼Œæˆ‘ä»¬å¯¹ä¸¤ä¸ªå­é¢‘å¸¦çš„æš´éœ²åå·®è¿›è¡Œäº†æ›´å‡†ç¡®çš„åˆ†æã€‚æˆ‘ä»¬çš„æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œå³æ’å³ç”¨ï¼Œå¯æ˜¾è‘—æé«˜å„ç§æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ï¼Œå¹¶ä¸ºä¸åŒæ¨¡å‹æ¶æ„æä¾›ç¨³å¥çš„è§£å†³æš´éœ²åå·®çš„æ–¹æ¡ˆã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kunzhan/wpp%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kunzhan/wppæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10072v1">PDF</a> ACM Multimedia 2025 accepted!</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡è§‚å¯Ÿåˆ°æ‰©æ•£æ¨¡å‹åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­é¢„æµ‹å™ªå£°å›¾åƒçš„èƒ½é‡ä¼šä¸‹é™ï¼Œå¹¶å‘ç°è¿™ä¸€ç°è±¡åœ¨ä½é¢‘é¢‘å¸¦å’Œé«˜é¢‘é¢‘å¸¦è¡¨ç°å‡ºä¸åŒçš„æ¨¡å¼ã€‚åŸºäºæ­¤ï¼Œå¼•å…¥åˆ©ç”¨å°æ³¢å˜æ¢çš„é¢‘ç‡åŸŸè°ƒèŠ‚æœºåˆ¶ï¼Œå¯¹ä½é¢‘å’Œé«˜é¢‘å­å¸¦è¿›è¡Œå•ç‹¬è°ƒæ•´ã€‚è¯¥æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œå³æ’å³ç”¨ï¼Œæ˜¾è‘—æé«˜äº†æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ï¼Œä¸ºä¸åŒæ¨¡å‹ç»“æ„ä¸­çš„æ›å…‰åå·®æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ‰©æ•£è¿‡ç¨‹ä¸­é¢„æµ‹å™ªå£°å›¾åƒçš„èƒ½é‡ä¼šä¸‹é™ã€‚</li>
<li>èƒ½é‡ä¸‹é™åœ¨ä½é¢‘é¢‘å¸¦å’Œé«˜é¢‘é¢‘å¸¦è¡¨ç°å‡ºä¸åŒçš„æ¨¡å¼ã€‚</li>
<li>èƒ½é‡ä¸‹é™å¯¼è‡´ç½‘ç»œé‡å»ºçš„æ¸…æ´æ•°æ®ä¸çœŸå®æ¸…æ´æ•°æ®ä¹‹é—´çš„å¹…åº¦å˜åŒ–ã€‚</li>
<li>å¼•å…¥é¢‘ç‡åŸŸè°ƒèŠ‚æœºåˆ¶ï¼Œåˆ©ç”¨å°æ³¢å˜æ¢å¯¹ä½é¢‘å’Œé«˜é¢‘å­å¸¦è¿›è¡Œå•ç‹¬è°ƒæ•´ã€‚</li>
<li>æ–¹æ³•æ— éœ€è®­ç»ƒï¼Œå³æ’å³ç”¨ï¼Œæé«˜äº†æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºè§£å†³ä¸åŒæ¨¡å‹ç»“æ„ä¸­çš„æ›å…‰åå·®æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cc4f9d224388af0dfb3954b4fa5f9122.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63f11b462066987fbfc05b76c1276558.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-675aa582c7f40f7cb30e88a25dda07b3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3019db3b794b0ccdd22ecd1214af3972.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a08e1dbbda6a409f55dadb1c456ee1ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-76da08f165918e0cca1edb6858a0e54c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-044f3869743dedaf3791d418f1347889.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-332e84404d612c7cc6d0209b48161192.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d0851ea6c397684eb684a91f62a9311a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e54e707173ae442dd883f0b46a158f73.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="IGD-Instructional-Graphic-Design-with-Multimodal-Layer-Generation"><a href="#IGD-Instructional-Graphic-Design-with-Multimodal-Layer-Generation" class="headerlink" title="IGD: Instructional Graphic Design with Multimodal Layer Generation"></a>IGD: Instructional Graphic Design with Multimodal Layer Generation</h2><p><strong>Authors:Yadong Qu, Shancheng Fang, Yuxin Wang, Xiaorui Wang, Zhineng Chen, Hongtao Xie, Yongdong Zhang</strong></p>
<p>Graphic design visually conveys information and data by creating and combining text, images and graphics. Two-stage methods that rely primarily on layout generation lack creativity and intelligence, making graphic design still labor-intensive. Existing diffusion-based methods generate non-editable graphic design files at image level with poor legibility in visual text rendering, which prevents them from achieving satisfactory and practical automated graphic design. In this paper, we propose Instructional Graphic Designer (IGD) to swiftly generate multimodal layers with editable flexibility with only natural language instructions. IGD adopts a new paradigm that leverages parametric rendering and image asset generation. First, we develop a design platform and establish a standardized format for multi-scenario design files, thus laying the foundation for scaling up data. Second, IGD utilizes the multimodal understanding and reasoning capabilities of MLLM to accomplish attribute prediction, sequencing and layout of layers. It also employs a diffusion model to generate image content for assets. By enabling end-to-end training, IGD architecturally supports scalability and extensibility in complex graphic design tasks. The superior experimental results demonstrate that IGD offers a new solution for graphic design. </p>
<blockquote>
<p>å¹³é¢è®¾è®¡é€šè¿‡åˆ›å»ºå’Œç»„åˆæ–‡æœ¬ã€å›¾åƒå’Œå›¾å½¢æ¥è§†è§‰ä¼ è¾¾ä¿¡æ¯å’Œæ•°æ®ã€‚ç°æœ‰çš„ä¸¤é˜¶æ®µæ–¹æ³•ä¸»è¦ä¾èµ–äºå¸ƒå±€ç”Ÿæˆï¼Œç¼ºä¹åˆ›é€ åŠ›å’Œæ™ºèƒ½ï¼Œå¯¼è‡´å¹³é¢è®¾è®¡ä»ç„¶æ˜¯åŠ³åŠ¨å¯†é›†å‹çš„ã€‚ç°æœ‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ç”Ÿæˆéç¼–è¾‘çš„å¹³é¢è®¾è®¡æ–‡ä»¶ï¼Œä»…åœ¨å›¾åƒå±‚é¢å…·æœ‰è‰¯å¥½çš„æ¸…æ™°åº¦ï¼Œè¿™åœ¨è§†è§‰æ–‡æœ¬æ¸²æŸ“æ–¹é¢è¡¨ç°è¾ƒå·®ï¼Œé˜»ç¢äº†å®ƒä»¬å®ç°ä»¤äººæ»¡æ„å’Œå®ç”¨çš„è‡ªåŠ¨åŒ–å¹³é¢è®¾è®¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºæŒ‡ä»¤æ€§å¹³é¢è®¾è®¡å¸ˆï¼ˆIGDï¼‰æ¥å¿«é€Ÿç”Ÿæˆä»…é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å³å¯è¿›è¡Œçµæ´»ç¼–è¾‘çš„å¤šæ¨¡å¼å›¾å±‚ã€‚IGDé‡‡ç”¨äº†ä¸€ç§æ–°çš„èŒƒå¼ï¼Œåˆ©ç”¨å‚æ•°æ¸²æŸ“å’Œå›¾åƒèµ„äº§ç”Ÿæˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè®¾è®¡å¹³å°ï¼Œå»ºç«‹äº†å¤šåœºæ™¯è®¾è®¡æ–‡ä»¶çš„æ ‡å‡†åŒ–æ ¼å¼ï¼Œä»è€Œä¸ºæ‰©å¤§æ•°æ®è§„æ¨¡å¥ å®šäº†åŸºç¡€ã€‚å…¶æ¬¡ï¼ŒIGDåˆ©ç”¨MLLMçš„å¤šæ¨¡å¼ç†è§£å’Œæ¨ç†èƒ½åŠ›æ¥å®Œæˆå±æ€§é¢„æµ‹ã€å›¾å±‚æ’åºå’Œå¸ƒå±€ã€‚å®ƒè¿˜é‡‡ç”¨æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆèµ„äº§å›¾åƒå†…å®¹ã€‚é€šè¿‡æ”¯æŒç«¯åˆ°ç«¯è®­ç»ƒï¼ŒIGDåœ¨å¤æ‚çš„å¹³é¢è®¾è®¡ä»»åŠ¡ä¸­æ”¯æŒå¯ä¼¸ç¼©æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä¼˜å¼‚çš„å®éªŒç»“æœè¯æ˜äº†IGDåœ¨å¹³é¢è®¾è®¡é¢†åŸŸæä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09910v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°å‹å›¾å½¢è®¾è®¡ç³»ç»Ÿâ€”â€”Instructional Graphic Designerï¼ˆIGDï¼‰ã€‚è¯¥ç³»ç»Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¿«é€Ÿç”Ÿæˆå…·æœ‰å¯ç¼–è¾‘çµæ´»æ€§çš„å¤šæ¨¡å¼å±‚ï¼Œå¹¶é‡‡ç”¨å‚æ•°æ¸²æŸ“å’Œå›¾åƒèµ„äº§ç”Ÿæˆçš„æ–°èŒƒå¼ã€‚é€šè¿‡å¼€å‘è®¾è®¡å¹³å°å’Œå»ºç«‹å¤šåœºæ™¯è®¾è®¡æ–‡ä»¶çš„æ ‡å‡†åŒ–æ ¼å¼ï¼ŒIGDå®ç°äº†æ•°æ®è§„æ¨¡åŒ–æ‰©å±•çš„åŸºç¡€ã€‚åŒæ—¶ï¼Œåˆ©ç”¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†èƒ½åŠ›å®Œæˆå±æ€§é¢„æµ‹ã€å±‚åºå’Œå¸ƒå±€ï¼Œå¹¶é‡‡ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒå†…å®¹èµ„äº§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIGDåœ¨å¤æ‚å›¾å½¢è®¾è®¡ä»»åŠ¡ä¸­å…·æœ‰å¯æ‰©å±•æ€§å’Œå¯ä¼¸ç¼©æ€§ï¼Œä¸ºå›¾å½¢è®¾è®¡æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IGDç³»ç»Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤å¿«é€Ÿç”Ÿæˆå¤šæ¨¡å¼å±‚ï¼Œå…·æœ‰å¯ç¼–è¾‘çµæ´»æ€§ã€‚</li>
<li>IGDé‡‡ç”¨å‚æ•°æ¸²æŸ“å’Œå›¾åƒèµ„äº§ç”Ÿæˆçš„æ–°èŒƒå¼ã€‚</li>
<li>è®¾è®¡å¹³å°å’Œå¤šåœºæ™¯è®¾è®¡æ–‡ä»¶çš„æ ‡å‡†åŒ–æ ¼å¼ä¸ºå®ç°æ•°æ®è§„æ¨¡åŒ–æ‰©å±•å¥ å®šåŸºç¡€ã€‚</li>
<li>IGDåˆ©ç”¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†èƒ½åŠ›å®Œæˆå±æ€§é¢„æµ‹ã€å±‚åºå’Œå¸ƒå±€ã€‚</li>
<li>IGDé‡‡ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå›¾åƒå†…å®¹èµ„äº§ï¼Œæ”¯æŒç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚</li>
<li>IGDåœ¨å¤æ‚å›¾å½¢è®¾è®¡ä»»åŠ¡ä¸­å…·æœ‰å¯æ‰©å±•æ€§å’Œå¯ä¼¸ç¼©æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09910">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-033987aa98215a32dfa48225a7631035.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-419d48af863214513af0b22eac146dff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4d8ac50741797d39714b7b1c386242d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb8516aa191f5eb8eeedb521889a7659.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c542ce5c3e231a7fb65d040f7afab011.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Advancing-Text-to-3D-Generation-with-Linearized-Lookahead-Variational-Score-Distillation"><a href="#Advancing-Text-to-3D-Generation-with-Linearized-Lookahead-Variational-Score-Distillation" class="headerlink" title="Advancing Text-to-3D Generation with Linearized Lookahead Variational   Score Distillation"></a>Advancing Text-to-3D Generation with Linearized Lookahead Variational   Score Distillation</h2><p><strong>Authors:Yu Lei, Bingde Liu, Qingsong Xie, Haonan Lu, Zhijie Deng</strong></p>
<p>Text-to-3D generation based on score distillation of pre-trained 2D diffusion models has gained increasing interest, with variational score distillation (VSD) as a remarkable example. VSD proves that vanilla score distillation can be improved by introducing an extra score-based model, which characterizes the distribution of images rendered from 3D models, to correct the distillation gradient. Despite the theoretical foundations, VSD, in practice, is likely to suffer from slow and sometimes ill-posed convergence. In this paper, we perform an in-depth investigation of the interplay between the introduced score model and the 3D model, and find that there exists a mismatching problem between LoRA and 3D distributions in practical implementation. We can simply adjust their optimization order to improve the generation quality. By doing so, the score model looks ahead to the current 3D state and hence yields more reasonable corrections. Nevertheless, naive lookahead VSD may suffer from unstable training in practice due to the potential over-fitting. To address this, we propose to use a linearized variant of the model for score distillation, giving rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD). $L^2$-VSD can be realized efficiently with forward-mode autodiff functionalities of existing deep learning libraries. Extensive experiments validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior score distillation-based methods. We also show that our method can be seamlessly incorporated into any other VSD-based text-to-3D framework. </p>
<blockquote>
<p>åŸºäºé¢„è®­ç»ƒçš„2Dæ‰©æ•£æ¨¡å‹çš„åˆ†æ•°è’¸é¦è¿›è¡Œæ–‡æœ¬åˆ°3Dç”Ÿæˆå·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆVSDï¼‰å°±æ˜¯ä¸€ä¸ªæ˜¾è‘—çš„ä¾‹å­ã€‚VSDè¯æ˜ï¼Œé€šè¿‡å¼•å…¥é¢å¤–çš„åŸºäºåˆ†æ•°çš„æ¨¡å‹æ¥æ”¹å–„æ™®é€šåˆ†æ•°è’¸é¦æ˜¯å¯è¡Œçš„ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæè¿°ä»3Dæ¨¡å‹å‘ˆç°çš„å›¾åƒåˆ†å¸ƒï¼Œä»¥æ ¡æ­£è’¸é¦æ¢¯åº¦ã€‚å°½ç®¡æœ‰ç†è®ºä¸Šçš„åŸºç¡€ï¼Œä½†VSDåœ¨å®è·µä¸­å¯èƒ½ä¼šé‡åˆ°æ”¶æ•›ç¼“æ…¢æˆ–æœ‰æ—¶ä¸é€‚å®šçš„é—®é¢˜ã€‚</p>
</blockquote>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥ç ”ç©¶äº†å¼•å…¥çš„åˆ†æ•°æ¨¡å‹ä¸3Dæ¨¡å‹ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå¹¶å‘ç°åœ¨å®é™…å®æ–½ä¸­å­˜åœ¨LoRAä¸3Dåˆ†å¸ƒä¸åŒ¹é…çš„é—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒæ•´å…¶ä¼˜åŒ–é¡ºåºæ¥æ”¹è¿›ç”Ÿæˆè´¨é‡ã€‚è¿™æ ·åšçš„è¯ï¼Œåˆ†æ•°æ¨¡å‹ä¼šå‰ç»å½“å‰çš„3DçŠ¶æ€ï¼Œä»è€Œæä¾›æ›´åˆç†çš„æ ¡æ­£ã€‚ç„¶è€Œï¼Œç®€å•çš„å‰ç»VSDåœ¨å®è·µä¸­å¯èƒ½ä¼šå› æ½œåœ¨è¿‡æ‹Ÿåˆè€Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09748v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé¢„è®­ç»ƒçš„äºŒç»´æ‰©æ•£æ¨¡å‹çš„åˆ†æ•°è’¸é¦è¿›è¡Œæ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆè¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œå…¶ä¸­çš„å˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆVSDï¼‰æ˜¯ä¸€ä¸ªæ˜¾è‘—ç¤ºä¾‹ã€‚æœ¬æ–‡é€šè¿‡æ·±å…¥ç ”ç©¶æ–°å¼•å…¥çš„åˆ†æ•°æ¨¡å‹ä¸ä¸‰ç»´æ¨¡å‹ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå‘ç°å®é™…åº”ç”¨ä¸­å­˜åœ¨LoRAä¸ä¸‰ç»´åˆ†å¸ƒä¸åŒ¹é…çš„é—®é¢˜ã€‚é€šè¿‡è°ƒæ•´ä¼˜åŒ–é¡ºåºï¼Œå¯ä»¥æé«˜ç”Ÿæˆè´¨é‡ã€‚ç„¶è€Œï¼Œç®€å•çš„å‘å‰æŸ¥çœ‹VSDåœ¨å®è·µä¸­å¯èƒ½ä¼šå› è¿‡æ‹Ÿåˆè€Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†çº¿æ€§åŒ–å˜ä½“æ¨¡å‹ç”¨äºåˆ†æ•°è’¸é¦ï¼Œå³Linearized Lookahead å˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆ$L^2$-VSDï¼‰ã€‚$L^2$-VSDå¯åˆ©ç”¨ç°æœ‰æ·±åº¦å­¦ä¹ åº“çš„å‘å‰æ¨¡å¼è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½é«˜æ•ˆå®ç°ã€‚å¤§é‡å®éªŒéªŒè¯äº†$L^2$-VSDçš„æœ‰æ•ˆæ€§ï¼Œè¡¨æ˜å…¶åœ¨åŸºäºåˆ†æ•°è’¸é¦çš„æ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆæ¡†æ¶ä¸­æ˜æ˜¾ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½æ— ç¼åœ°èå…¥ä»»ä½•å…¶ä»–åŸºäºVSDçš„æ–‡æœ¬åˆ°ä¸‰ç»´æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆVSDï¼‰æ”¹è¿›äº†ä¼ ç»Ÿçš„åˆ†æ•°è’¸é¦æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥é¢å¤–çš„åŸºäºåˆ†æ•°çš„æ¨¡å‹æ¥çº æ­£è’¸é¦æ¢¯åº¦ã€‚</li>
<li>åœ¨å®è·µä¸­ï¼ŒVSDå¯èƒ½ä¼šé­é‡ç¼“æ…¢å’Œä¸åˆé¢„æœŸçš„æ”¶æ•›é—®é¢˜ã€‚</li>
<li>åˆ†æ•°æ¨¡å‹ä¸ä¸‰ç»´æ¨¡å‹ä¹‹é—´å­˜åœ¨ä¸åŒ¹é…é—®é¢˜ï¼Œé€šè¿‡è°ƒæ•´ä¼˜åŒ–é¡ºåºå¯ä»¥æé«˜ç”Ÿæˆè´¨é‡ã€‚</li>
<li>ç®€å•çš„å‘å‰æŸ¥çœ‹VSDå¯èƒ½ä¼šå› è¿‡æ‹Ÿåˆè€Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚</li>
<li>æå‡ºäº†Linearized Lookahead å˜åˆ†åˆ†æ•°è’¸é¦ï¼ˆ$L^2$-VSDï¼‰ï¼Œå®ƒæ˜¯ä¸€ä¸ªé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿåˆ©ç”¨ç°æœ‰æ·±åº¦å­¦ä¹ åº“çš„è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½ã€‚</li>
<li>å®éªŒè¯æ˜$L^2$-VSDåœ¨åŸºäºåˆ†æ•°è’¸é¦çš„æ–‡æœ¬åˆ°ä¸‰ç»´ç”Ÿæˆæ¡†æ¶ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-93f55c002b5febf8e68e1ea9dc6bc676.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f5ec9eb0eceeb1aee9f64dd43d2dd17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-575908068eebc478f6bce3aee4e060f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7427b8ccf3d705761ae2d526d8a4b235.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e20ccfdb716d154e0b0354a7a52e9d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0bb442859710a034d55948035745f0d7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Democratizing-High-Fidelity-Co-Speech-Gesture-Video-Generation"><a href="#Democratizing-High-Fidelity-Co-Speech-Gesture-Video-Generation" class="headerlink" title="Democratizing High-Fidelity Co-Speech Gesture Video Generation"></a>Democratizing High-Fidelity Co-Speech Gesture Video Generation</h2><p><strong>Authors:Xu Yang, Shaoli Huang, Shenbo Xie, Xuelin Chen, Yifei Liu, Changxing Ding</strong></p>
<p>Co-speech gesture video generation aims to synthesize realistic, audio-aligned videos of speakers, complete with synchronized facial expressions and body gestures. This task presents challenges due to the significant one-to-many mapping between audio and visual content, further complicated by the scarcity of large-scale public datasets and high computational demands. We propose a lightweight framework that utilizes 2D full-body skeletons as an efficient auxiliary condition to bridge audio signals with visual outputs. Our approach introduces a diffusion model conditioned on fine-grained audio segments and a skeleton extracted from the speakerâ€™s reference image, predicting skeletal motions through skeleton-audio feature fusion to ensure strict audio coordination and body shape consistency. The generated skeletons are then fed into an off-the-shelf human video generation model with the speakerâ€™s reference image to synthesize high-fidelity videos. To democratize research, we present CSG-405-the first public dataset with 405 hours of high-resolution videos across 71 speech types, annotated with 2D skeletons and diverse speaker demographics. Experiments show that our method exceeds state-of-the-art approaches in visual quality and synchronization while generalizing across speakers and contexts. Code, models, and CSG-405 are publicly released at <a target="_blank" rel="noopener" href="https://mpi-lab.github.io/Democratizing-CSG/">https://mpi-lab.github.io/Democratizing-CSG/</a> </p>
<blockquote>
<p>ååŒè¯­éŸ³æ‰‹åŠ¿è§†é¢‘ç”Ÿæˆæ—¨åœ¨åˆæˆä¸éŸ³é¢‘å¯¹é½çš„ã€é€¼çœŸçš„å‘è¨€äººè§†é¢‘ï¼ŒåŒ…æ‹¬åŒæ­¥çš„é¢éƒ¨è¡¨æƒ…å’Œä½“æ€ã€‚ç”±äºéŸ³é¢‘å’Œè§†è§‰å†…å®¹ä¹‹é—´çš„ä¸€åˆ°å¤šæ˜ å°„å…³ç³»æ˜¾è‘—ï¼Œå†åŠ ä¸Šå¤§è§„æ¨¡å…¬å…±æ•°æ®é›†çš„ç¨€ç¼ºå’Œé«˜è®¡ç®—éœ€æ±‚ï¼Œæ­¤ä»»åŠ¡é¢ä¸´æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ¡†æ¶ï¼Œåˆ©ç”¨äºŒç»´å…¨èº«éª¨æ¶ä½œä¸ºè¿æ¥éŸ³é¢‘ä¿¡å·å’Œè§†è§‰è¾“å‡ºçš„é«˜æ•ˆè¾…åŠ©æ¡ä»¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ ¹æ®ç²¾ç»†çš„éŸ³é¢‘ç‰‡æ®µå’Œä»å‘è¨€äººå‚è€ƒå›¾åƒä¸­æå–çš„éª¨æ¶è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œé€šè¿‡éª¨æ¶éŸ³é¢‘ç‰¹å¾èåˆæ¥é¢„æµ‹éª¨æ¶è¿åŠ¨ï¼Œä»¥ç¡®ä¿ä¸¥æ ¼çš„éŸ³é¢‘åè°ƒå’Œä½“æ€ä¸€è‡´æ€§ã€‚ç”Ÿæˆçš„éª¨æ¶ç„¶åè¾“å…¥åˆ°å¸¦æœ‰å‘è¨€äººå‚è€ƒå›¾åƒçš„å•†å“äººç‰©è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œä»¥åˆæˆé«˜ä¿çœŸè§†é¢‘ã€‚ä¸ºäº†æ™®åŠç ”ç©¶ï¼Œæˆ‘ä»¬æ¨å‡ºäº†CSG-405æ•°æ®é›†â€”â€”ç¬¬ä¸€ä¸ªå…¬å…±æ•°æ®é›†ï¼ŒåŒ…å«71ç§è¯­éŸ³ç±»å‹çš„405å°æ—¶é«˜åˆ†è¾¨ç‡è§†é¢‘ï¼Œæ ‡æ³¨äº†äºŒç»´éª¨æ¶å’Œå¤šæ ·çš„å‘è¨€äººäººå£ç»Ÿè®¡æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’ŒåŒæ­¥æ–¹é¢è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨è·¨å‘è¨€äººå’Œä¸Šä¸‹æ–‡æ–¹é¢å…·æœ‰å¾ˆå¼ºçš„é€šç”¨æ€§ã€‚ä»£ç ã€æ¨¡å‹å’ŒCSG-405æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://mpi-lab.github.io/Democratizing-CSG/">https://mpi-lab.github.io/Democratizing-CSG/</a>å…¬å¼€è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06812v2">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºäºŒç»´å…¨èº«éª¨éª¼çš„é«˜æ•ˆè½»é‡çº§æ¡†æ¶ï¼Œç”¨äºåˆæˆä¸éŸ³é¢‘ä¿¡å·åŒæ­¥çš„è¯´è¯äººæ‰‹åŠ¿è§†é¢‘ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ç²¾ç»†çš„éŸ³é¢‘ç‰‡æ®µå’Œä»å‚è€ƒå›¾åƒä¸­æå–çš„éª¨éª¼ä¿¡æ¯ï¼Œé€šè¿‡éª¨éª¼éŸ³é¢‘ç‰¹å¾èåˆé¢„æµ‹éª¨éª¼è¿åŠ¨ï¼Œç¡®ä¿ä¸¥æ ¼çš„éŸ³é¢‘åè°ƒå’Œèº«ä½“å½¢çŠ¶ä¸€è‡´æ€§ã€‚ç”Ÿæˆçš„éª¨éª¼ä¿¡æ¯å°†è¾“å…¥åˆ°ç°æˆçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œç»“åˆå‚è€ƒå›¾åƒåˆæˆé«˜è´¨é‡è§†é¢‘ã€‚æ­¤å¤–ï¼Œè¿˜å‘å¸ƒäº†é¦–ä¸ªå…¬å…±æ•°æ®é›†CSG-405ï¼ŒåŒ…å«405å°æ—¶çš„é«˜åˆ†è¾¨ç‡è§†é¢‘ï¼Œè·¨è¶Š71ç§è¯­éŸ³ç±»å‹ï¼Œå¹¶é™„æœ‰äºŒç»´éª¨éª¼å’Œå¤šæ ·çš„è¯´è¯è€…äººå£ç»Ÿè®¡ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’ŒåŒæ­¥æ–¹é¢è¶…è¿‡äº†ç°æœ‰æŠ€æœ¯ï¼Œå¹¶åœ¨è·¨è¯´è¯è€…å’Œä¸Šä¸‹æ–‡æ–¹é¢å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ä¸ªåŸºäºäºŒç»´å…¨èº«éª¨éª¼çš„è½»é‡çº§æ¡†æ¶è¿›è¡ŒéŸ³é¢‘å¯¹é½çš„è§†é¢‘ç”Ÿæˆã€‚</li>
<li>é€šè¿‡éª¨éª¼éŸ³é¢‘ç‰¹å¾èåˆé¢„æµ‹éª¨éª¼è¿åŠ¨ï¼Œç¡®ä¿éŸ³é¢‘ä¸è§†è§‰è¾“å‡ºåŒæ­¥ã€‚</li>
<li>åˆ©ç”¨ç°æœ‰çš„è§†é¢‘ç”Ÿæˆæ¨¡å‹å’Œå‚è€ƒå›¾åƒåˆæˆé«˜è´¨é‡è§†é¢‘ã€‚</li>
<li>ä»‹ç»äº†é¦–ä¸ªå…¬å…±æ•°æ®é›†CSG-405ï¼ŒåŒ…å«å¤šç§è¯­éŸ³ç±»å‹å’Œä¸°å¯Œçš„äºŒç»´éª¨éª¼ä¿¡æ¯ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†è§†è§‰è´¨é‡å’ŒåŒæ­¥æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
<li>æ–¹æ³•å…·æœ‰è‰¯å¥½çš„è·¨è¯´è¯è€…å’Œä¸Šä¸‹æ–‡æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06812">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35707a8c57720958718e61bd00585ce0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bb2f58015d7c178821f33a2f8c01770.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-476cc2fb5fb803380038a86e820c4c25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cb3a2d68602d34282c105f2bc933174.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DNF-Intrinsic-Deterministic-Noise-Free-Diffusion-for-Indoor-Inverse-Rendering"><a href="#DNF-Intrinsic-Deterministic-Noise-Free-Diffusion-for-Indoor-Inverse-Rendering" class="headerlink" title="DNF-Intrinsic: Deterministic Noise-Free Diffusion for Indoor Inverse   Rendering"></a>DNF-Intrinsic: Deterministic Noise-Free Diffusion for Indoor Inverse   Rendering</h2><p><strong>Authors:Rongjia Zheng, Qing Zhang, Chengjiang Long, Wei-Shi Zheng</strong></p>
<p>Recent methods have shown that pre-trained diffusion models can be fine-tuned to enable generative inverse rendering by learning image-conditioned noise-to-intrinsic mapping. Despite their remarkable progress, they struggle to robustly produce high-quality results as the noise-to-intrinsic paradigm essentially utilizes noisy images with deteriorated structure and appearance for intrinsic prediction, while it is common knowledge that structure and appearance information in an image are crucial for inverse rendering. To address this issue, we present DNF-Intrinsic, a robust yet efficient inverse rendering approach fine-tuned from a pre-trained diffusion model, where we propose to take the source image rather than Gaussian noise as input to directly predict deterministic intrinsic properties via flow matching. Moreover, we design a generative renderer to constrain that the predicted intrinsic properties are physically faithful to the source image. Experiments on both synthetic and real-world datasets show that our method clearly outperforms existing state-of-the-art methods. </p>
<blockquote>
<p>æœ€æ–°æ–¹æ³•å·²ç»è¡¨æ˜ï¼Œé€šè¿‡å¯¹é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥é€šè¿‡å­¦ä¹ å›¾åƒæ¡ä»¶ä¸‹çš„å™ªå£°åˆ°å†…åœ¨æ˜ å°„æ¥å®ç°ç”Ÿæˆå¼é€†å‘æ¸²æŸ“ã€‚å°½ç®¡å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œå®ƒä»¬åœ¨ç¨³å®šåœ°äº§ç”Ÿé«˜è´¨é‡ç»“æœæ–¹é¢ä»ç„¶é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå™ªå£°åˆ°å†…åœ¨çš„æ¨¡å¼æœ¬è´¨ä¸Šåˆ©ç”¨çš„æ˜¯ç»“æ„æ¶åŒ–å’Œå¤–è§‚é€€åŒ–çš„å™ªå£°å›¾åƒæ¥è¿›è¡Œå†…åœ¨é¢„æµ‹ï¼Œè€Œä¼—æ‰€å‘¨çŸ¥ï¼Œå›¾åƒä¸­çš„ç»“æ„å’Œå¤–è§‚ä¿¡æ¯å¯¹äºé€†å‘æ¸²æŸ“è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DNF-Intrinsicï¼Œè¿™æ˜¯ä¸€ç§ä»é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒçš„ç¨³å¥è€Œé«˜æ•ˆçš„é€†å‘æ¸²æŸ“æ–¹æ³•ï¼Œæˆ‘ä»¬å»ºè®®ä»¥æºå›¾åƒè€Œéé«˜æ–¯å™ªå£°ä¸ºè¾“å…¥ï¼Œé€šè¿‡æµåŒ¹é…ç›´æ¥é¢„æµ‹ç¡®å®šæ€§å†…åœ¨å±æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç”Ÿæˆå¼æ¸²æŸ“å™¨æ¥çº¦æŸé¢„æµ‹çš„å†…åœ¨å±æ€§åœ¨ç‰©ç†ä¸Šå¿ äºæºå›¾åƒã€‚åœ¨åˆæˆæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜æ˜¾ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03924v2">PDF</a> Accepted to ICCV2025</p>
<p><strong>Summary</strong>ï¼šé¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å¯ä»¥é€šè¿‡å­¦ä¹ å›¾åƒæ¡ä»¶å™ªå£°åˆ°å†…åœ¨æ˜ å°„è¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°ç”Ÿæˆå¼é€†å‘æ¸²æŸ“ã€‚ä½†å®ƒä»¬åœ¨äº§ç”Ÿé«˜è´¨é‡ç»“æœæ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå› ä¸ºå™ªå£°åˆ°å†…åœ¨èŒƒå¼æœ¬è´¨ä¸Šåˆ©ç”¨çš„æ˜¯ç»“æ„é€€åŒ–çš„å™ªå£°å›¾åƒè¿›è¡Œå†…åœ¨é¢„æµ‹ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºDNF-Intrinsicæ–¹æ³•ï¼Œä½¿ç”¨æºå›¾åƒè€Œéé«˜æ–¯å™ªå£°ä½œä¸ºè¾“å…¥ï¼Œé€šè¿‡æµåŒ¹é…ç›´æ¥é¢„æµ‹ç¡®å®šæ€§å†…åœ¨å±æ€§ï¼Œå¹¶è®¾è®¡ç”Ÿæˆæ¸²æŸ“å™¨çº¦æŸé¢„æµ‹çš„å†…åœ¨å±æ€§åœ¨ç‰©ç†ä¸Šå¿ äºæºå›¾åƒã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜æ˜¾ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¯ä»¥é€šè¿‡å¾®è°ƒå®ç°ç”Ÿæˆå¼é€†å‘æ¸²æŸ“ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åˆ©ç”¨ç»“æ„é€€åŒ–çš„å™ªå£°å›¾åƒè¿›è¡Œå†…åœ¨é¢„æµ‹ï¼Œå½±å“ç»“æœè´¨é‡ã€‚</li>
<li>DNF-Intrinsicæ–¹æ³•ä»¥æºå›¾åƒä¸ºè¾“å…¥ï¼Œé€šè¿‡æµåŒ¹é…ç›´æ¥é¢„æµ‹ç¡®å®šæ€§å†…åœ¨å±æ€§ã€‚</li>
<li>å¼•å…¥ç”Ÿæˆæ¸²æŸ“å™¨ä»¥çº¦æŸé¢„æµ‹çš„å†…åœ¨å±æ€§ä¸æºå›¾åƒçš„ç‰©ç†ä¸€è‡´æ€§ã€‚</li>
<li>DNF-Intrinsicåœ¨åˆæˆå’ŒçœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</li>
<li>æ–¹æ³•å…·æœ‰é²æ£’æ€§å’Œé«˜æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03924">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d4e3afd189a62f8847a4668cc405760a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a6027536d136c7ce6f1e232c79f51b53.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09a667881b2eb82e4304dba698e1755d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98c93500d59fc77687ccb24f02ae9b95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a05aa23ac9c67c7958a7eb14ecb644b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b56d5d4f1d612df77179453dbbe2e07c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ca8123216ffceb748de15a8df3fa9ad.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Imagine-for-Me-Creative-Conceptual-Blending-of-Real-Images-and-Text-via-Blended-Attention"><a href="#Imagine-for-Me-Creative-Conceptual-Blending-of-Real-Images-and-Text-via-Blended-Attention" class="headerlink" title="Imagine for Me: Creative Conceptual Blending of Real Images and Text via   Blended Attention"></a>Imagine for Me: Creative Conceptual Blending of Real Images and Text via   Blended Attention</h2><p><strong>Authors:Wonwoong Cho, Yanxia Zhang, Yan-Ying Chen, David I. Inouye</strong></p>
<p>Blending visual and textual concepts into a new visual concept is a unique and powerful trait of human beings that can fuel creativity. However, in practice, cross-modal conceptual blending for humans is prone to cognitive biases, like design fixation, which leads to local minima in the design space. In this paper, we propose a T2I diffusion adapter â€œIT-Blenderâ€ that can automate the blending process to enhance human creativity. Prior works related to cross-modal conceptual blending are limited in encoding a real image without loss of details or in disentangling the image and text inputs. To address these gaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend the latent representations of a clean reference image with those of the noisy generated image. Combined with our novel blended attention, IT-Blender encodes the real reference image without loss of details and blends the visual concept with the object specified by the text in a disentangled way. Our experiment results show that IT-Blender outperforms the baselines by a large margin in blending visual and textual concepts, shedding light on the new application of image generative models to augment human creativity. </p>
<blockquote>
<p>å°†è§†è§‰å’Œæ–‡æœ¬æ¦‚å¿µèåˆä¸ºä¸€ä¸ªæ–°çš„è§†è§‰æ¦‚å¿µæ˜¯äººç±»ç‹¬ç‰¹ä¸”å¼ºå¤§çš„ç‰¹è´¨ï¼Œèƒ½å¤Ÿæ¿€å‘åˆ›é€ åŠ›ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œäººç±»çš„è·¨æ¨¡æ€æ¦‚å¿µèåˆå®¹æ˜“å—åˆ°è®¤çŸ¥åè§çš„å½±å“ï¼Œå¦‚è®¾è®¡åƒµåŒ–ï¼Œè¿™ä¼šå¯¼è‡´è®¾è®¡ç©ºé—´ä¸­çš„å±€éƒ¨æœ€å°å€¼ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§T2Iæ‰©æ•£é€‚é…å™¨â€œIT-Blenderâ€ï¼Œå®ƒå¯ä»¥è‡ªåŠ¨åŒ–èåˆè¿‡ç¨‹ä»¥å¢å¼ºäººç±»çš„åˆ›é€ åŠ›ã€‚ä»¥å¾€ä¸è·¨æ¨¡æ€æ¦‚å¿µèåˆç›¸å…³çš„å·¥ä½œåœ¨ç¼–ç çœŸå®å›¾åƒæ—¶å­˜åœ¨ç»†èŠ‚æŸå¤±æˆ–åœ¨åˆ†ç¦»å›¾åƒå’Œæ–‡æœ¬è¾“å…¥æ–¹é¢çš„å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒIT-Blenderåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼ˆSDå’ŒFLUXï¼‰æ¥èåˆå¹²å‡€å‚è€ƒå›¾åƒçš„æ½œåœ¨è¡¨ç¤ºä¸å™ªå£°ç”Ÿæˆå›¾åƒçš„æ½œåœ¨è¡¨ç¤ºã€‚ç»“åˆæˆ‘ä»¬æ–°é¢–çš„èåˆæ³¨æ„åŠ›æœºåˆ¶ï¼ŒIT-Blenderèƒ½å¤Ÿç¼–ç çœŸå®å‚è€ƒå›¾åƒè€Œä¸æŸå¤±ç»†èŠ‚ï¼Œå¹¶ä»¥åˆ†ç¦»çš„æ–¹å¼å°†è§†è§‰æ¦‚å¿µä¸æ–‡æœ¬æŒ‡å®šçš„å¯¹è±¡èåˆåœ¨ä¸€èµ·ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨èåˆè§†è§‰å’Œæ–‡æœ¬æ¦‚å¿µæ–¹é¢ï¼ŒIT-Blenderå¤§å¤§ä¼˜äºåŸºçº¿ï¼Œä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹åœ¨äººç±»åˆ›é€ åŠ›å¢å¼ºæ–¹é¢çš„æ–°åº”ç”¨æä¾›äº†å¯ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.24085v2">PDF</a> Project website is available at <a target="_blank" rel="noopener" href="https://imagineforme.github.io/">https://imagineforme.github.io/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>äººç±»èƒ½å¤Ÿèåˆè§†è§‰å’Œæ–‡æœ¬æ¦‚å¿µä»¥å½¢æˆæ–°çš„è§†è§‰æ¦‚å¿µï¼Œè¿™æ˜¯äººç±»ç‹¬ç‰¹ä¸”å¼ºå¤§çš„åˆ›é€ åŠ›ä½“ç°ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­ï¼Œäººç±»çš„è·¨æ¨¡æ€æ¦‚å¿µèåˆæ˜“å—åˆ°è®¤çŸ¥åè§çš„å½±å“ï¼Œå¦‚è®¾è®¡åƒµåŒ–ï¼Œè¿™ä¼šå¯¼è‡´è®¾è®¡ç©ºé—´ä¸­çš„å±€éƒ¨æœ€å°å€¼ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§T2Iæ‰©æ•£é€‚é…å™¨IT-Blenderï¼Œå¯ä»¥è‡ªåŠ¨åŒ–èåˆè¿‡ç¨‹ä»¥å¢å¼ºäººç±»çš„åˆ›é€ åŠ›ã€‚å…ˆå‰å…³äºè·¨æ¨¡æ€æ¦‚å¿µèåˆçš„ç ”ç©¶å—é™äºæ— æ³•å¯¹çœŸå®å›¾åƒè¿›è¡Œæ— æŸç¼–ç æˆ–æ— æ³•å°†å›¾åƒå’Œæ–‡æœ¬è¾“å…¥åˆ†å¼€ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒIT-Blenderåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼ˆSDå’ŒFLUXï¼‰å°†å¹²å‡€å‚è€ƒå›¾åƒå’Œå™ªå£°ç”Ÿæˆå›¾åƒçš„æ½œåœ¨è¡¨ç¤ºèåˆåœ¨ä¸€èµ·ã€‚ç»“åˆæˆ‘ä»¬æ–°é¢–çš„èåˆæ³¨æ„åŠ›æœºåˆ¶ï¼ŒIT-Blenderèƒ½å¤Ÿå¯¹çœŸå®å‚è€ƒå›¾åƒè¿›è¡Œæ— æŸç¼–ç ï¼Œå¹¶ä»¥åˆ†ç¦»çš„æ–¹å¼å°†è§†è§‰æ¦‚å¿µä¸æ–‡æœ¬æŒ‡å®šçš„å¯¹è±¡èåˆåœ¨ä¸€èµ·ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIT-Blenderåœ¨èåˆè§†è§‰å’Œæ–‡æœ¬æ¦‚å¿µæ–¹é¢è¿œè¶…åŸºçº¿æ–¹æ³•ï¼Œä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹åœ¨äººç±»åˆ›é€ åŠ›æå‡æ–¹é¢çš„åº”ç”¨æä¾›äº†æ–°çš„å¯ç¤ºã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>äººç±»èƒ½å¤Ÿèåˆè§†è§‰å’Œæ–‡æœ¬æ¦‚å¿µä»¥åˆ›é€ æ–°çš„è§†è§‰æ¦‚å¿µï¼Œä½†è¿™ä¸€è¿‡ç¨‹æ˜“å—åˆ°è®¤çŸ¥åè§çš„å½±å“ã€‚</li>
<li>IT-Blenderæ˜¯ä¸€ç§T2Iæ‰©æ•£é€‚é…å™¨ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–èåˆè¿‡ç¨‹ä»¥å¢å¼ºäººç±»åˆ›é€ åŠ›ã€‚</li>
<li>ä»¥å¾€çš„è·¨æ¨¡æ€æ¦‚å¿µèåˆç ”ç©¶åœ¨å›¾åƒç¼–ç å’Œå›¾åƒä¸æ–‡æœ¬è¾“å…¥çš„åˆ†ç¦»æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>IT-Blenderåˆ©ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å’Œèåˆæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œè§†è§‰ä¸æ–‡æœ¬æ¦‚å¿µçš„èåˆã€‚</li>
<li>IT-Blenderèƒ½å¤Ÿå¯¹çœŸå®å‚è€ƒå›¾åƒè¿›è¡Œæ— æŸç¼–ç ï¼Œå¹¶ä»¥åˆ†ç¦»çš„æ–¹å¼å¤„ç†è§†è§‰æ¦‚å¿µå’Œæ–‡æœ¬æŒ‡å®šçš„å¯¹è±¡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒIT-Blenderåœ¨èåˆè§†è§‰å’Œæ–‡æœ¬æ¦‚å¿µæ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œè¿œè¶…åŸºçº¿æ–¹æ³•ã€‚</li>
<li>IT-Blenderçš„åº”ç”¨ä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹åœ¨äººç±»åˆ›é€ åŠ›æå‡æ–¹é¢çš„ä½œç”¨æä¾›äº†æ–°çš„è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.24085">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e370c02eb77b537c21fd11c2d44bf13e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ea465871e2fc971305da33cc7b3652c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-378996105f20254f7ee16cf836739701.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Recognizing-Surgical-Phases-Anywhere-Few-Shot-Test-time-Adaptation-and-Task-graph-Guided-Refinement"><a href="#Recognizing-Surgical-Phases-Anywhere-Few-Shot-Test-time-Adaptation-and-Task-graph-Guided-Refinement" class="headerlink" title="Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and   Task-graph Guided Refinement"></a>Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and   Task-graph Guided Refinement</h2><p><strong>Authors:Kun Yuan, Tingxuan Chen, Shi Li, Joel L. Lavanchy, Christian Heiliger, Ege Ã–zsoy, Yiming Huang, Long Bai, Nassir Navab, Vinkle Srivastav, Hongliang Ren, Nicolas Padoy</strong></p>
<p>The complexity and diversity of surgical workflows, driven by heterogeneous operating room settings, institutional protocols, and anatomical variability, present a significant challenge in developing generalizable models for cross-institutional and cross-procedural surgical understanding. While recent surgical foundation models pretrained on large-scale vision-language data offer promising transferability, their zero-shot performance remains constrained by domain shifts, limiting their utility in unseen surgical environments. To address this, we introduce Surgical Phase Anywhere (SPA), a lightweight framework for versatile surgical workflow understanding that adapts foundation models to institutional settings with minimal annotation. SPA leverages few-shot spatial adaptation to align multi-modal embeddings with institution-specific surgical scenes and phases. It also ensures temporal consistency through diffusion modeling, which encodes task-graph priors derived from institutional procedure protocols. Finally, SPA employs dynamic test-time adaptation, exploiting the mutual agreement between multi-modal phase prediction streams to adapt the model to a given test video in a self-supervised manner, enhancing the reliability under test-time distribution shifts. SPA is a lightweight adaptation framework, allowing hospitals to rapidly customize phase recognition models by defining phases in natural language text, annotating a few images with the phase labels, and providing a task graph defining phase transitions. The experimental results show that the SPA framework achieves state-of-the-art performance in few-shot surgical phase recognition across multiple institutions and procedures, even outperforming full-shot models with 32-shot labeled data. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SPA">https://github.com/CAMMA-public/SPA</a> </p>
<blockquote>
<p>æ‰‹æœ¯æµç¨‹çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ï¼Œå—åˆ°æ‰‹æœ¯å®¤è®¾ç½®ã€æœºæ„åè®®å’Œè§£å‰–ç»“æ„å·®å¼‚çš„å½±å“ï¼Œä¸ºè·¨æœºæ„å’Œè·¨ç¨‹åºçš„æ‰‹æœ¯ç†è§£å¼€å‘é€šç”¨æ¨¡å‹å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚è™½ç„¶æœ€è¿‘åŸºäºå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ•°æ®çš„é¢„è®­ç»ƒæ‰‹æœ¯åŸºç¡€æ¨¡å‹å±•ç°å‡ºæœ‰å‰æ™¯çš„è¿ç§»æ€§ï¼Œä½†å®ƒä»¬çš„é›¶æ ·æœ¬æ€§èƒ½ä»ç„¶å—åˆ°é¢†åŸŸåç§»çš„é™åˆ¶ï¼Œåœ¨æœªè§è¿‡çš„æ‰‹æœ¯ç¯å¢ƒä¸­æ•ˆç”¨æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Surgical Phase Anywhereï¼ˆSPAï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„é€šç”¨æ‰‹æœ¯æµç¨‹ç†è§£æ¡†æ¶ï¼Œèƒ½å¤Ÿé€‚åº”æœºæ„è®¾ç½®ï¼Œå¹¶æœ€å¤§é™åº¦åœ°å‡å°‘æ ‡æ³¨å·¥ä½œã€‚SPAåˆ©ç”¨å°‘æ ·æœ¬ç©ºé—´è‡ªé€‚åº”æ¥å¯¹é½å¤šæ¨¡æ€åµŒå…¥å’Œæœºæ„ç‰¹å®šçš„æ‰‹æœ¯åœºæ™¯å’Œé˜¶æ®µã€‚å®ƒè¿˜é€šè¿‡æ‰©æ•£æ¨¡å‹ç¡®ä¿æ—¶é—´ä¸€è‡´æ€§ï¼Œè¯¥æ¨¡å‹ç¼–ç æ¥è‡ªæœºæ„ç¨‹åºåè®®çš„ä»»åŠ¡å›¾å…ˆéªŒä¿¡æ¯ã€‚æœ€åï¼ŒSPAé‡‡ç”¨åŠ¨æ€æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼Œåˆ©ç”¨å¤šæ¨¡æ€é˜¶æ®µé¢„æµ‹æµä¹‹é—´çš„ç›¸äº’ä¸€è‡´æ€§ï¼Œä»¥è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼é€‚åº”ç»™å®šçš„æµ‹è¯•è§†é¢‘ï¼Œåœ¨æµ‹è¯•æ—¶é—´åˆ†å¸ƒå˜åŒ–çš„æƒ…å†µä¸‹æé«˜å¯é æ€§ã€‚SPAæ˜¯ä¸€ä¸ªè½»é‡çº§çš„é€‚åº”æ¡†æ¶ï¼Œå…è®¸åŒ»é™¢é€šè¿‡ç”¨è‡ªç„¶è¯­è¨€æ–‡æœ¬å®šä¹‰é˜¶æ®µã€å¯¹å°‘æ•°å›¾åƒè¿›è¡Œé˜¶æ®µæ ‡ç­¾æ ‡æ³¨ä»¥åŠæä¾›å®šä¹‰é˜¶æ®µè½¬æ¢çš„ä»»åŠ¡å›¾æ¥å¿«é€Ÿå®šåˆ¶é˜¶æ®µè¯†åˆ«æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPAæ¡†æ¶åœ¨å¤šä¸ªæœºæ„å’Œç¨‹åºä¸­çš„å°‘æ ·æœ¬æ‰‹æœ¯é˜¶æ®µè¯†åˆ«ä¸­è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œå³ä½¿åœ¨æ‹¥æœ‰32ä¸ªå¸¦æ ‡ç­¾æ ·æœ¬çš„å…¨æ ·æœ¬æ¨¡å‹ä¸­ä¹Ÿèƒ½å–å¾—ä¼˜åŠ¿ã€‚ç›¸å…³ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/CAMMA-public/SPA">https://github.com/CAMMA-public/SPA</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.20254v2">PDF</a> Accepted by MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ‰‹æœ¯æµç¨‹ç†è§£çš„è·¨æœºæ„æ¨¡å‹å¼€å‘æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ‰‹æœ¯æµç¨‹ç†è§£æ¡†æ¶â€”â€”Surgical Phase Anywhereï¼ˆSPAï¼‰ã€‚SPAèƒ½å¤Ÿåˆ©ç”¨å°‘é‡çš„æ ·æœ¬æ•°æ®è¿›è¡Œç©ºé—´é€‚åº”ï¼Œä»¥ä¸ç‰¹å®šæœºæ„çš„æ‰‹æœ¯åœºæ™¯å’Œé˜¶æ®µå¯¹é½ã€‚é€šè¿‡æ‰©æ•£å»ºæ¨¡ç¡®ä¿æ—¶é—´ä¸€è‡´æ€§ï¼Œå¹¶ç¼–ç æœºæ„æ‰‹æœ¯åè®®çš„ä»»åŠ¡å›¾å…ˆéªŒã€‚æ­¤å¤–ï¼ŒSPAé‡‡ç”¨åŠ¨æ€æµ‹è¯•æ—¶é—´é€‚åº”ï¼Œåˆ©ç”¨å¤šæ¨¡æ€é˜¶æ®µé¢„æµ‹æµçš„ç›¸äº’ä¸€è‡´æ€§ï¼Œä»¥è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼é€‚åº”ç»™å®šçš„æµ‹è¯•è§†é¢‘ï¼Œæé«˜æµ‹è¯•æ—¶åˆ†å¸ƒå˜åŒ–çš„å¯é æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒSPAæ¡†æ¶åœ¨å¤šä¸ªæœºæ„å’Œç¨‹åºä¸­çš„å°æ ·æœ¬æ‰‹æœ¯é˜¶æ®µè¯†åˆ«ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œç”šè‡³åœ¨æ ‡æ³¨æ•°æ®åªæœ‰32æ¬¡å…¨é•œå¤´çš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°å‡ºä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰‹æœ¯å·¥ä½œæµçš„å¤æ‚æ€§å’Œå¤šæ ·æ€§å¯¹å¼€å‘å¯è·¨æœºæ„å’Œè·¨æ‰‹æœ¯ç†è§£çš„é€šç”¨æ¨¡å‹æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚</li>
<li>Surgical Phase Anywhereï¼ˆSPAï¼‰æ¡†æ¶è¢«å¼•å…¥ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ƒå…è®¸åˆ©ç”¨å°‘é‡çš„æ ·æœ¬æ•°æ®è¿›è¡Œç©ºé—´é€‚åº”ã€‚</li>
<li>SPAé€šè¿‡æ‰©æ•£å»ºæ¨¡ç¡®ä¿æ—¶é—´ä¸€è‡´æ€§ï¼Œå¹¶ç¼–ç ä»»åŠ¡å›¾å…ˆéªŒï¼Œåæ˜ æœºæ„ç‰¹å®šçš„æ‰‹æœ¯åè®®ã€‚</li>
<li>SPAé‡‡ç”¨åŠ¨æ€æµ‹è¯•æ—¶é—´é€‚åº”ï¼Œä»¥æé«˜æ¨¡å‹åœ¨æµ‹è¯•æ—¶é¢å¯¹åˆ†å¸ƒå˜åŒ–çš„å¯é æ€§ã€‚</li>
<li>SPAæ¡†æ¶å…è®¸åŒ»é™¢é€šè¿‡è‡ªç„¶è¯­è¨€æ–‡æœ¬å®šä¹‰é˜¶æ®µã€å¯¹å°‘æ•°å›¾åƒè¿›è¡Œé˜¶æ®µæ ‡ç­¾æ³¨é‡Šä»¥åŠæä¾›å®šä¹‰é˜¶æ®µè½¬æ¢çš„ä»»åŠ¡å›¾ï¼Œæ¥å¿«é€Ÿå®šåˆ¶é˜¶æ®µè¯†åˆ«æ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSPAæ¡†æ¶åœ¨å°‘æ ·æœ¬æ‰‹æœ¯é˜¶æ®µè¯†åˆ«ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œç”šè‡³åœ¨æ ‡æ³¨æ•°æ®é‡è¾ƒå°çš„æƒ…å†µä¸‹ä¹Ÿè¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.20254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c8be39de20021b00bf428411d8498ede.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe61f58515d793766ad46c54d05b6773.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-563fcde840d43a3cde2f32ca5768ea63.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Nexus-Gen-Unified-Image-Understanding-Generation-and-Editing-via-Prefilled-Autoregression-in-Shared-Embedding-Space"><a href="#Nexus-Gen-Unified-Image-Understanding-Generation-and-Editing-via-Prefilled-Autoregression-in-Shared-Embedding-Space" class="headerlink" title="Nexus-Gen: Unified Image Understanding, Generation, and Editing via   Prefilled Autoregression in Shared Embedding Space"></a>Nexus-Gen: Unified Image Understanding, Generation, and Editing via   Prefilled Autoregression in Shared Embedding Space</h2><p><strong>Authors:Hong Zhang, Zhongjie Duan, Xingjun Wang, Yuze Zhao, Weiyi Lu, Zhipeng Di, Yixuan Xu, Yingda Chen, Yu Zhang</strong></p>
<p>Unified multimodal generative models aim to integrate image understanding and generation abilities, offering significant advantages in harnessing multimodal corpora, particularly interleaved text-image data. However, existing unified models exhibit limitations in image synthesis quality, autoregressive error accumulation, and image editing capability. In this work, we propose Nexus-Gen, a novel architecture that unifies image understanding, generation, and editing tasks in a shared image embedding space. This shared space serves as a bridge for the autoregressive and diffusion models, which seamlessly integrates their complementary strengths in cross-modal modeling. To mitigate the severe error accumulation during autoregressive embedding prediction, we propose a novel prefilled autoregression strategy that aligns training-inference dynamics by prefilling input sequences with learnable embeddings. After multi-stage and multi-task training on our constructed large-scale dataset with 26.3 million samples, Nexus-Gen achieves state-of-the-art performance on the evaluation benchmarks spanning image understanding, generation and editing tasks. All models, datasets, and source codes are released in <a target="_blank" rel="noopener" href="https://github.com/modelscope/Nexus-Gen">https://github.com/modelscope/Nexus-Gen</a> to facilitate further advancements across the field. </p>
<blockquote>
<p>ç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹æ—¨åœ¨æ•´åˆå›¾åƒç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œåœ¨åˆ©ç”¨å¤šæ¨¡æ€è¯­æ–™åº“æ–¹é¢ï¼Œç‰¹åˆ«æ˜¯äº¤ç»‡çš„æ–‡æœ¬å›¾åƒæ•°æ®æ–¹é¢ï¼Œå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ç»Ÿä¸€æ¨¡å‹åœ¨å›¾åƒåˆæˆè´¨é‡ã€è‡ªå›å½’è¯¯å·®ç´¯ç§¯å’Œå›¾åƒç¼–è¾‘èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Nexus-Genï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ¶æ„ï¼Œåœ¨å…±äº«çš„å›¾åƒåµŒå…¥ç©ºé—´ä¸­ç»Ÿä¸€äº†å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚è¿™ä¸ªå…±äº«ç©ºé—´ä½œä¸ºè‡ªå›å½’å’Œæ‰©æ•£æ¨¡å‹çš„æ¡¥æ¢ï¼Œæ— ç¼é›†æˆäº†å®ƒä»¬åœ¨è·¨æ¨¡æ€å»ºæ¨¡ä¸­çš„äº’è¡¥ä¼˜åŠ¿ã€‚ä¸ºäº†å‡è½»è‡ªå›å½’åµŒå…¥é¢„æµ‹è¿‡ç¨‹ä¸­çš„ä¸¥é‡è¯¯å·®ç´¯ç§¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é¢„å¡«å……è‡ªå›å½’ç­–ç•¥ï¼Œé€šè¿‡é¢„å¡«å……å¯å­¦ä¹ åµŒå…¥æ¥å¯¹é½è®­ç»ƒæ¨ç†åŠ¨æ€ã€‚åœ¨æˆ‘ä»¬æ„å»ºçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆåŒ…å«2630ä¸‡æ ·æœ¬ï¼‰ä¸Šè¿›è¡Œå¤šé˜¶æ®µå¤šä»»åŠ¡è®­ç»ƒåï¼ŒNexus-Genåœ¨æ¶µç›–å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡çš„è¯„ä¼°åŸºå‡†ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ‰€æœ‰æ¨¡å‹ã€æ•°æ®é›†å’Œæºä»£ç å‡å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/modelscope/Nexus-Gen%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E8%AF%A5%E9%A2%86%E5%9F%9F%E7%9A%84%E8%BF%9B%E4%B8%80%E6%AD%A5%E8%BF%9B%E6%AD%A5%E3%80%82">https://github.com/modelscope/Nexus-Genï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥è¿›æ­¥ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21356v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºNexus-Gençš„ç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨åœ¨ä¸€ä¸ªå…±äº«çš„å›¾åƒåµŒå…¥ç©ºé—´ä¸­ç»Ÿä¸€å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚è¯¥æ¨¡å‹é€šè¿‡æ¡¥æ¥è‡ªå›å½’æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†è·¨æ¨¡æ€å»ºæ¨¡çš„äº’è¡¥ä¼˜åŠ¿ã€‚ä¸ºè§£å†³è‡ªå›å½’åµŒå…¥é¢„æµ‹ä¸­çš„è¯¯å·®ç´¯ç§¯é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é¢„å¡«å……è‡ªå›å½’ç­–ç•¥ï¼Œé€šè¿‡é¢„å¡«å……å­¦ä¹ åµŒå…¥æ¥å¯¹é½è®­ç»ƒå’Œæ¨ç†åŠ¨æ€ã€‚ç»è¿‡åœ¨æ„å»ºçš„å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„å¤šé˜¶æ®µå¤šä»»åŠ¡è®­ç»ƒï¼ŒNexus-Genåœ¨å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡çš„è¯„ä»·æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Nexus-Genæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨åœ¨ä¸€ä¸ªå…±äº«çš„å›¾åƒåµŒå…¥ç©ºé—´ä¸­å®Œæˆå›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>è¯¥æ¨¡å‹é€šè¿‡æ¡¥æ¥è‡ªå›å½’æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œå®ç°äº†è·¨æ¨¡æ€å»ºæ¨¡çš„äº’è¡¥ä¼˜åŠ¿ã€‚</li>
<li>Nexus-Gené‡‡ç”¨é¢„å¡«å……è‡ªå›å½’ç­–ç•¥ï¼Œè§£å†³è‡ªå›å½’åµŒå…¥é¢„æµ‹ä¸­çš„è¯¯å·®ç´¯ç§¯é—®é¢˜ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šè¿›è¡Œå¤šé˜¶æ®µå¤šä»»åŠ¡è®­ç»ƒï¼Œå±•ç¤ºäº†åœ¨å›¾åƒç†è§£ã€ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸Šçš„ä¼˜è¶Šæ€§ã€‚</li>
<li>Nexus-Gençš„æ€§èƒ½åœ¨è¯„ä»·åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>æ‰€æœ‰æ¨¡å‹ã€æ•°æ®é›†å’Œæºä»£ç éƒ½å·²å…¬å¼€å‘å¸ƒï¼Œä»¥ä¾¿ä¿ƒè¿›è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹çš„å‘å±•æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21356">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8cf109789bcee3cede63ec0d896dec59.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51f7deab2aaeac12d5eb63404e35543e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2e295ea37f81102b8ee3341074da0f76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba262d86eea0d6f6902c0bc9cbd091b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95b4a7cc83fddbd34bf5843edacc5f04.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82989e7c14b0085f17daa5fbaabc8117.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0693035f77d694faa184f8de9eafb955.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Diffusion-Models-for-Robotic-Manipulation-A-Survey"><a href="#Diffusion-Models-for-Robotic-Manipulation-A-Survey" class="headerlink" title="Diffusion Models for Robotic Manipulation: A Survey"></a>Diffusion Models for Robotic Manipulation: A Survey</h2><p><strong>Authors:Rosa Wolf, Yitian Shi, Sheng Liu, Rania Rayyes</strong></p>
<p>Diffusion generative models have demonstrated remarkable success in visual domains such as image and video generation. They have also recently emerged as a promising approach in robotics, especially in robot manipulations. Diffusion models leverage a probabilistic framework, and they stand out with their ability to model multi-modal distributions and their robustness to high-dimensional input and output spaces. This survey provides a comprehensive review of state-of-the-art diffusion models in robotic manipulation, including grasp learning, trajectory planning, and data augmentation. Diffusion models for scene and image augmentation lie at the intersection of robotics and computer vision for vision-based tasks to enhance generalizability and data scarcity. This paper also presents the two main frameworks of diffusion models and their integration with imitation learning and reinforcement learning. In addition, it discusses the common architectures and benchmarks and points out the challenges and advantages of current state-of-the-art diffusion-based methods. </p>
<blockquote>
<p>æ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆç­‰è§†è§‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚å®ƒä»¬æœ€è¿‘ä¹Ÿå‡ºç°åœ¨æœºå™¨äººæŠ€æœ¯ä¸­ï¼Œå°¤å…¶æ˜¯æœºå™¨äººæ“æ§æ–¹é¢ï¼Œå±•ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚æ‰©æ•£æ¨¡å‹é‡‡ç”¨æ¦‚ç‡æ¡†æ¶ï¼Œå®ƒä»¬çš„ç‰¹ç‚¹åœ¨äºèƒ½å¤Ÿæ¨¡æ‹Ÿå¤šæ¨¡æ€åˆ†å¸ƒï¼Œå¹¶ä¸”å¯¹é«˜ç»´è¾“å…¥å’Œè¾“å‡ºç©ºé—´å…·æœ‰å¾ˆå¼ºçš„ç¨³å¥æ€§ã€‚è¿™ç¯‡ç»¼è¿°å¯¹æœºå™¨äººæ“æ§ä¸­çš„æœ€æ–°æ‰©æ•£æ¨¡å‹è¿›è¡Œäº†å…¨é¢å›é¡¾ï¼ŒåŒ…æ‹¬æŠ“å–å­¦ä¹ ã€è½¨è¿¹è§„åˆ’å’Œæ•°æ®å¢å¼ºã€‚åœºæ™¯å’Œå›¾åƒå¢å¼ºçš„æ‰©æ•£æ¨¡å‹ä½äºæœºå™¨äººæŠ€æœ¯å’Œè®¡ç®—æœºè§†è§‰çš„äº¤å‰ç‚¹ï¼Œç”¨äºåŸºäºè§†è§‰çš„ä»»åŠ¡ï¼Œä»¥æé«˜é€šç”¨æ€§å’Œè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æœ¬æ–‡è¿˜ä»‹ç»äº†æ‰©æ•£æ¨¡å‹çš„ä¸¤ä¸ªä¸»è¦æ¡†æ¶åŠå…¶ä¸æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„èåˆã€‚æ­¤å¤–ï¼Œå®ƒè¿˜è®¨è®ºäº†å¸¸è§çš„æ¶æ„å’ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰æœ€æ–°æ‰©æ•£æ–¹æ³•çš„ä¼˜åŠ¿å’ŒæŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08438v3">PDF</a> 26 pages, 2 figure, 9 tables</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œæœ€è¿‘åœ¨æœºå™¨äººé¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººæ“ä½œæ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚æœ¬æ–‡å…¨é¢ç»¼è¿°äº†æœºå™¨äººæ“ä½œä¸­çš„æ‰©æ•£æ¨¡å‹ï¼ŒåŒ…æ‹¬æŠ“å–å­¦ä¹ ã€è½¨è¿¹è§„åˆ’å’Œæ•°æ®å¢å¼ºç­‰ã€‚æ‰©æ•£æ¨¡å‹ç»“åˆäº†è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººæŠ€æœ¯ï¼Œç”¨äºåŸºäºè§†è§‰çš„ä»»åŠ¡ï¼Œä»¥æé«˜é€šç”¨æ€§å’Œè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æœ¬æ–‡è¿˜ä»‹ç»äº†æ‰©æ•£æ¨¡å‹çš„ä¸¤ä¸ªä¸»è¦æ¡†æ¶åŠå…¶ä¸æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„ç»“åˆï¼Œè®¨è®ºäº†å¸¸è§çš„æ¶æ„å’ŒåŸºå‡†æµ‹è¯•ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰å…ˆè¿›æ‰©æ•£æ–¹æ³•é¢ä¸´çš„æŒ‘æˆ˜å’Œä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£ç”Ÿæˆæ¨¡å‹åœ¨è§†è§‰é¢†åŸŸå¦‚å›¾åƒå’Œè§†é¢‘ç”Ÿæˆå–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨æœºå™¨äººæ“ä½œé¢†åŸŸå±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç”¨äºæœºå™¨äººæ“ä½œçš„å¤šä¸ªæ–¹é¢ï¼ŒåŒ…æ‹¬æŠ“å–å­¦ä¹ ã€è½¨è¿¹è§„åˆ’å’Œæ•°æ®å¢å¼ºã€‚</li>
<li>æ‰©æ•£æ¨¡å‹ç»“åˆäº†è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººæŠ€æœ¯ï¼Œç”¨äºåŸºäºè§†è§‰çš„ä»»åŠ¡ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹çš„ä¸¤ä¸ªä¸»è¦æ¡†æ¶åŒ…æ‹¬ä¸æ¨¡ä»¿å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ çš„ç»“åˆã€‚</li>
<li>å½“å‰æ‰©æ•£æ¨¡å‹é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼Œä½†ä¹Ÿå…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>æ–‡ç« è®¨è®ºäº†å¸¸è§çš„æ¶æ„å’ŒåŸºå‡†æµ‹è¯•æ¥è¡¡é‡æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08438">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6d0d303fc6d4ca89e29b2b15b287eed2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6b2b5eea1313260c147ae8eef6be00ef.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Conditional-Data-Synthesis-Augmentation"><a href="#Conditional-Data-Synthesis-Augmentation" class="headerlink" title="Conditional Data Synthesis Augmentation"></a>Conditional Data Synthesis Augmentation</h2><p><strong>Authors:Xinyu Tian, Xiaotong Shen</strong></p>
<p>Reliable machine learning and statistical analysis rely on diverse, well-distributed training data. However, real-world datasets are often limited in size and exhibit underrepresentation across key subpopulations, leading to biased predictions and reduced performance, particularly in supervised tasks such as classification. To address these challenges, we propose Conditional Data Synthesis Augmentation (CoDSA), a novel framework that leverages generative models, such as diffusion models, to synthesize high-fidelity data for improving model performance across multimodal domains including tabular, textual, and image data. CoDSA generates synthetic samples that faithfully capture the conditional distributions of the original data, with a focus on under-sampled or high-interest regions. Through transfer learning, CoDSA fine-tunes pre-trained generative models to enhance the realism of synthetic data and increase sample density in sparse areas. This process preserves inter-modal relationships, mitigates data imbalance, improves domain adaptation, and boosts generalization. We also introduce a theoretical framework that quantifies the statistical accuracy improvements enabled by CoDSA as a function of synthetic sample volume and targeted region allocation, providing formal guarantees of its effectiveness. Extensive experiments demonstrate that CoDSA consistently outperforms non-adaptive augmentation strategies and state-of-the-art baselines in both supervised and unsupervised settings. </p>
<blockquote>
<p>å¯é æœºå™¨å­¦ä¹ å’Œç»Ÿè®¡åˆ†æä¾èµ–äºå¤šæ ·ä¸”åˆ†å¸ƒè‰¯å¥½çš„è®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„æ•°æ®é›†é€šå¸¸è§„æ¨¡æœ‰é™ï¼Œå¹¶ä¸”åœ¨å…³é”®å­ç¾¤ä½“ä¸­è¡¨ç°ä»£è¡¨æ€§ä¸è¶³ï¼Œä»è€Œå¯¼è‡´é¢„æµ‹åå·®å’Œæ€§èƒ½ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†ç±»ç­‰ç›‘ç£ä»»åŠ¡ä¸­ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¡ä»¶æ•°æ®åˆæˆå¢å¼ºï¼ˆCoDSAï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰åˆæˆé«˜è´¨é‡æ•°æ®ï¼Œä»¥æ”¹å–„è·¨å¤šæ¨¡å¼åŸŸï¼ˆåŒ…æ‹¬è¡¨æ ¼ã€æ–‡æœ¬å’Œå›¾åƒæ•°æ®ï¼‰çš„æ¨¡å‹æ€§èƒ½ã€‚CoDSAç”Ÿæˆåˆæˆæ ·æœ¬ï¼Œå¿ å®æ•æ‰åŸå§‹æ•°æ®çš„æ¡ä»¶åˆ†å¸ƒï¼Œé‡ç‚¹å…³æ³¨æ¬ é‡‡æ ·æˆ–é«˜å…´è¶£åŒºåŸŸã€‚é€šè¿‡è¿ç§»å­¦ä¹ ï¼ŒCoDSAå¯¹é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥å¢å¼ºåˆæˆæ•°æ®çš„çœŸå®æ€§å’Œç¨€ç–åŒºåŸŸçš„æ ·æœ¬å¯†åº¦ã€‚è¿™ä¸€è¿‡ç¨‹ä¿ç•™äº†è·¨æ¨¡å¼çš„å…³ç³»ï¼Œç¼“è§£äº†æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œæ”¹å–„äº†åŸŸé€‚åº”æ€§å’Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œä»¥é‡åŒ–åˆæˆæ ·æœ¬é‡å’Œç›®æ ‡åŒºåŸŸåˆ†é…æ‰€å®ç°çš„CoDSAçš„ç»Ÿè®¡ç²¾åº¦æ”¹è¿›ï¼Œä»è€Œä¸ºå…¶æä¾›æœ‰æ•ˆçš„æ­£å¼ä¿è¯ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯åœ¨ç›‘ç£è¿˜æ˜¯éç›‘ç£ç¯å¢ƒä¸­ï¼ŒCoDSAå§‹ç»ˆä¼˜äºéè‡ªé€‚åº”å¢å¼ºç­–ç•¥å’Œæœ€æ–°åŸºçº¿æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07426v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºæ¡ä»¶æ•°æ®åˆæˆå¢å¼ºï¼ˆCoDSAï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰åˆæˆé«˜è´¨é‡æ•°æ®ï¼Œä»¥æé«˜åŒ…æ‹¬è¡¨æ ¼ã€æ–‡æœ¬å’Œå›¾åƒæ•°æ®ç­‰å¤šæ¨¡å¼é¢†åŸŸçš„æ¨¡å‹æ€§èƒ½ã€‚CoDSAå…³æ³¨äºæ¬ é‡‡æ ·æˆ–é«˜å…´è¶£åŒºåŸŸçš„åˆæˆæ ·æœ¬ï¼Œé€šè¿‡è¿ç§»å­¦ä¹ å¯¹é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæé«˜åˆæˆæ•°æ®çš„çœŸå®æ€§å’Œç¨€ç–åŒºåŸŸçš„æ ·æœ¬å¯†åº¦ã€‚æ­¤æ¡†æ¶èƒ½å¤Ÿä¿ç•™è·¨æ¨¡æ€å…³ç³»ï¼Œç¼“è§£æ•°æ®ä¸å¹³è¡¡é—®é¢˜ï¼Œæ”¹å–„é¢†åŸŸé€‚åº”æ€§å’Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒCoDSAåœ¨ç›‘ç£å’Œæ— ç›‘ç£è®¾ç½®ä¸­å‡ä¼˜äºéè‡ªé€‚åº”å¢å¼ºç­–ç•¥å’Œæœ€æ–°åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoDSAåˆ©ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰åˆæˆé«˜è´¨é‡æ•°æ®ï¼Œæ—¨åœ¨æé«˜æœºå™¨å­¦ä¹ å’Œç»Ÿè®¡åˆ†æçš„å¯é æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶å…³æ³¨äºæ¬ é‡‡æ ·æˆ–é«˜å…´è¶£åŒºåŸŸçš„åˆæˆæ ·æœ¬ï¼Œä»¥æ”¹å–„æ¨¡å‹åœ¨å…³é”®å­äººç¾¤ä¸­çš„è¡¨ç°ã€‚</li>
<li>é€šè¿‡è¿ç§»å­¦ä¹ ï¼ŒCoDSAå¯¹é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œæé«˜åˆæˆæ•°æ®çš„çœŸå®æ€§å’Œç¨€ç–åŒºåŸŸçš„æ ·æœ¬å¯†åº¦ã€‚</li>
<li>CoDSAèƒ½å¤Ÿä¿ç•™è·¨æ¨¡æ€å…³ç³»ï¼Œç¼“è§£æ•°æ®ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶æé«˜äº†é¢†åŸŸé€‚åº”æ€§å’Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ–‡ä¸­æä¾›äº†ç†è®ºæ¡†æ¶ï¼Œé‡åŒ–CoDSAåœ¨ç»Ÿè®¡å‡†ç¡®æ€§æ–¹é¢çš„æ”¹è¿›ï¼Œå¹¶ç»™å‡ºå…¶æœ‰æ•ˆæ€§çš„æ­£å¼ä¿è¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.07426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-82661f717c913f782228c8090d548785.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Compression-Aware-One-Step-Diffusion-Model-for-JPEG-Artifact-Removal"><a href="#Compression-Aware-One-Step-Diffusion-Model-for-JPEG-Artifact-Removal" class="headerlink" title="Compression-Aware One-Step Diffusion Model for JPEG Artifact Removal"></a>Compression-Aware One-Step Diffusion Model for JPEG Artifact Removal</h2><p><strong>Authors:Jinpei Guo, Zheng Chen, Wenbo Li, Yong Guo, Yulun Zhang</strong></p>
<p>Diffusion models have demonstrated remarkable success in image restoration tasks. However, their multi-step denoising process introduces significant computational overhead, limiting their practical deployment. Furthermore, existing methods struggle to effectively remove severe JPEG artifact, especially in highly compressed images. To address these challenges, we propose CODiff, a compression-aware one-step diffusion model for JPEG artifact removal. The core of CODiff is the compression-aware visual embedder (CaVE), which extracts and leverages JPEG compression priors to guide the diffusion model. We propose a dual learning strategy that combines explicit and implicit learning. Specifically, explicit learning enforces a quality prediction objective to differentiate low-quality images with different compression levels. Implicit learning employs a reconstruction objective that enhances the modelâ€™s generalization. This dual learning allows for a deeper and more comprehensive understanding of JPEG compression. Experimental results demonstrate that CODiff surpasses recent leading methods in both quantitative and visual quality metrics. The code is released at <a target="_blank" rel="noopener" href="https://github.com/jp-guo/CODiff">https://github.com/jp-guo/CODiff</a>. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒæ¢å¤ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå…¶å¤šæ­¥éª¤å»å™ªè¿‡ç¨‹äº§ç”Ÿäº†å¤§é‡çš„è®¡ç®—å¼€é”€ï¼Œé™åˆ¶äº†å…¶å®é™…éƒ¨ç½²çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•åœ¨å»é™¤ä¸¥é‡çš„JPEGä¼ªå½±æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åº¦å‹ç¼©çš„å›¾åƒä¸­ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CODiffï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºJPEGä¼ªå½±å»é™¤çš„å‹ç¼©æ„ŸçŸ¥ä¸€æ­¥æ‰©æ•£æ¨¡å‹ã€‚CODiffçš„æ ¸å¿ƒæ˜¯å‹ç¼©æ„ŸçŸ¥è§†è§‰åµŒå…¥å™¨ï¼ˆCaVEï¼‰ï¼Œå®ƒæå–å¹¶åˆ©ç”¨JPEGå‹ç¼©å…ˆéªŒæ¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆæ˜¾å¼å­¦ä¹ å’Œéšå¼å­¦ä¹ çš„åŒé‡å­¦ä¹ ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œæ˜¾å¼å­¦ä¹ é€šè¿‡å¼ºåˆ¶å®æ–½è´¨é‡é¢„æµ‹ç›®æ ‡æ¥åŒºåˆ†ä¸åŒå‹ç¼©çº§åˆ«çš„ä½è´¨é‡å›¾åƒã€‚éšå¼å­¦ä¹ é‡‡ç”¨é‡å»ºç›®æ ‡ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§åŒé‡å­¦ä¹ å¯ä»¥æ›´æ·±å…¥ã€æ›´å…¨é¢åœ°ç†è§£JPEGå‹ç¼©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCODiffåœ¨å®šé‡å’Œè§†è§‰è´¨é‡æŒ‡æ ‡ä¸Šå‡è¶…è¶Šäº†æœ€æ–°çš„å…ˆè¿›æ–¹æ³•ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/jp-guo/CODiff%E3%80%82">https://github.com/jp-guo/CODiffã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.09873v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCODiffçš„å‹ç¼©æ„ŸçŸ¥ä¸€æ­¥æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºJPEGå›¾åƒå»ä¼ªã€‚è¯¥æ¨¡å‹çš„æ ¸å¿ƒæ˜¯å‹ç¼©æ„ŸçŸ¥è§†è§‰åµŒå…¥å™¨ï¼ˆCaVEï¼‰ï¼Œèƒ½å¤Ÿæå–å’Œåˆ©ç”¨JPEGå‹ç¼©å…ˆéªŒæ¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡ç»“åˆæ˜¾å¼å­¦ä¹ å’Œéšå¼å­¦ä¹ çš„åŒé‡å­¦ä¹ ç­–ç•¥ï¼ŒCODiffå®ç°äº†å¯¹JPEGå‹ç¼©çš„æ·±å…¥å’Œå…¨é¢ç†è§£ï¼Œå¹¶åœ¨å®šé‡å’Œè§†è§‰è´¨é‡æŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰é¢†å…ˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒæ¢å¤ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨å¤šæ­¥å»å™ªè¿‡ç¨‹ä¸­å­˜åœ¨è®¡ç®—å¼€é”€å¤§çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥æœ‰æ•ˆå»é™¤é«˜åº¦å‹ç¼©å›¾åƒä¸­çš„JPEGä¼ªå½±ã€‚</li>
<li>CODiffæ¨¡å‹è¢«æå‡ºä»¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œå®ƒæ˜¯ä¸€ä¸ªå‹ç¼©æ„ŸçŸ¥çš„ä¸€æ­¥æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºJPEGä¼ªå½±å»é™¤ã€‚</li>
<li>CODiffæ¨¡å‹çš„æ ¸å¿ƒæ˜¯å‹ç¼©æ„ŸçŸ¥è§†è§‰åµŒå…¥å™¨ï¼ˆCaVEï¼‰ï¼Œå®ƒèƒ½æå–å’Œåˆ©ç”¨JPEGå‹ç¼©å…ˆéªŒã€‚</li>
<li>CODiffé‡‡ç”¨åŒé‡å­¦ä¹ ç­–ç•¥ï¼Œç»“åˆæ˜¾å¼å­¦ä¹ å’Œéšå¼å­¦ä¹ ï¼Œå®ç°å¯¹JPEGå‹ç¼©çš„æ·±å…¥å’Œå…¨é¢ç†è§£ã€‚</li>
<li>æ˜¾å¼å­¦ä¹ é€šè¿‡è´¨é‡é¢„æµ‹ç›®æ ‡æ¥åŒºåˆ†ä¸åŒå‹ç¼©çº§åˆ«çš„ä½è´¨é‡å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.09873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a7c4dba7301bc84665a242772feb0d3d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-779d99547bbfcf912ebd04eb6106367f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e036c7d7371e04430fe16fccfbbaf32b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a0dd8e954813b8248ce62e19205bf74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d542233630e52b31e459044ffbf0e02b.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-17/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3a45ec19ecad59aab41880369ef222dc.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  PanoDiff-SR Synthesizing Dental Panoramic Radiographs using Diffusion   and Super-resolution
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0b2fc8b2251a1d2ea767a20df9bf662e.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  AI-Enhanced Pediatric Pneumonia Detection A CNN-Based Approach Using   Data Augmentation and Generative Adversarial Networks (GANs)
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32127.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
