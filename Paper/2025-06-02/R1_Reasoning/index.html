<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-06-02  Benchmarking and Rethinking Knowledge Editing for Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-85c8f303c9e344a58f81a1749c5d524f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-06-03
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    83 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-02-更新"><a href="#2025-06-02-更新" class="headerlink" title="2025-06-02 更新"></a>2025-06-02 更新</h1><h2 id="Benchmarking-and-Rethinking-Knowledge-Editing-for-Large-Language-Models"><a href="#Benchmarking-and-Rethinking-Knowledge-Editing-for-Large-Language-Models" class="headerlink" title="Benchmarking and Rethinking Knowledge Editing for Large Language Models"></a>Benchmarking and Rethinking Knowledge Editing for Large Language Models</h2><p><strong>Authors:Guoxiu He, Xin Song, Futing Wang, Aixin Sun</strong></p>
<p>Knowledge editing aims to update the embedded knowledge within Large Language Models (LLMs). However, existing approaches, whether through parameter modification or external memory integration, often suffer from inconsistent evaluation objectives and experimental setups. To address this gap, we conduct a comprehensive benchmarking study. In addition to fact-level datasets, we introduce more complex event-based datasets and general-purpose datasets drawn from other tasks. Our evaluation covers both instruction-tuned and reasoning-oriented LLMs, under a realistic autoregressive inference setting rather than teacher-forced decoding. Beyond single-edit assessments, we also evaluate multi-edit scenarios to better reflect practical demands. We employ four evaluation dimensions, including portability, and compare all recent methods against a simple and straightforward baseline named Selective Contextual Reasoning (SCR). Empirical results reveal that parameter-based editing methods perform poorly under realistic conditions. In contrast, SCR consistently outperforms them across all settings. This study offers new insights into the limitations of current knowledge editing methods and highlights the potential of context-based reasoning as a more robust alternative. </p>
<blockquote>
<p>知识编辑旨在更新大型语言模型（LLM）中的嵌入知识。然而，现有方法，无论是通过参数修改还是外部记忆集成，经常面临评估目标不一致和实验设置不一致的问题。为了弥补这一空白，我们进行了一项全面的基准测试研究。除了事实级的数据集之外，我们还引入了更复杂的事件基础数据集和从其他任务中提取的通用数据集。我们的评估涵盖了指令调优和面向推理的LLM，在一个现实的自回归推理设置下，而不是教师强制解码。除了单编辑评估之外，我们还对多编辑场景进行评估，以更好地反映实际需求。我们采用四个评估维度，包括可移植性，并与名为选择性上下文推理（SCR）的简单直接基线对比所有近期方法。实证结果表明，基于参数的编辑方法在实际情况下的表现较差。相比之下，SCR在所有设置中均表现优异。本研究为当前知识编辑方法的局限性提供了新的见解，并突出了基于上下文的推理作为更稳健的替代方案的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18690v1">PDF</a> arXiv admin note: text overlap with arXiv:2503.05212</p>
<p><strong>总结</strong></p>
<p>本文探讨了大型语言模型中的知识更新问题。当前的知识编辑方法如参数修改和外部记忆集成等存在评价目标不一致的问题。为解决这一问题，研究者进行了一项全面的基准测试研究，引入了更复杂的事件数据集和通用数据集进行评估。评估涵盖了指令调优和推理导向的大型语言模型，采用真实的自动回归推理设置进行评估。此外，还进行了单编辑和多编辑场景的评估以更好地反映实际需求。评估包括便携性在内的四个维度，并对比了名为选择性上下文推理的简单直观基线方法与所有近期方法。实证结果表明，基于参数的编辑方法在真实条件下表现不佳，而选择性上下文推理则始终表现优异。该研究揭示了当前知识编辑方法的局限性，并展示了基于上下文的推理作为更稳健替代方案的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>知识编辑旨在更新大型语言模型中的嵌入知识。</li>
<li>当前的知识编辑方法存在评价目标不一致的问题。</li>
<li>为了解决这一问题，研究者进行了全面的基准测试研究。</li>
<li>研究引入了复杂的事件数据集和通用数据集进行评估。</li>
<li>评估覆盖了指令调优和推理导向的大型语言模型。</li>
<li>选择性上下文推理（SCR）在多种设置下表现优异。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18690">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e743ed70059baa6054d938e498bff081.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6a3f8a0800cfdc0c21016299482f99c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Chain-of-Zoom-Extreme-Super-Resolution-via-Scale-Autoregression-and-Preference-Alignment"><a href="#Chain-of-Zoom-Extreme-Super-Resolution-via-Scale-Autoregression-and-Preference-Alignment" class="headerlink" title="Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and   Preference Alignment"></a>Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and   Preference Alignment</h2><p><strong>Authors:Bryan Sangwoo Kim, Jeongsol Kim, Jong Chul Ye</strong></p>
<p>Modern single-image super-resolution (SISR) models deliver photo-realistic results at the scale factors on which they are trained, but collapse when asked to magnify far beyond that regime. We address this scalability bottleneck with Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an autoregressive chain of intermediate scale-states with multi-scale-aware prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the conditional probability into tractable sub-problems to achieve extreme resolutions without additional training. Because visual cues diminish at high magnifications, we augment each zoom step with multi-scale-aware text prompts generated by a vision-language model (VLM). The prompt extractor itself is fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic VLM, aligning text guidance towards human preference. Experiments show that a standard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement with high perceptual quality and fidelity. Project Page: <a target="_blank" rel="noopener" href="https://bryanswkim.github.io/chain-of-zoom/">https://bryanswkim.github.io/chain-of-zoom/</a> . </p>
<blockquote>
<p>现代单图像超分辨率（SISR）模型在它们所接受的尺度因素上产生逼真的结果，但当被要求放大超出这个范围时，它们就会崩溃。我们采用Chain-of-Zoom（CoZ）来解决这种可扩展性的瓶颈，这是一个通用的模型框架，它将SISR分解为具有多尺度感知提示的自回归中间尺度状态链。CoZ重复利用骨干网SR模型，将条件概率分解为可控制的子问题，以实现极高的分辨率而无需额外的训练。由于在高倍放大时视觉线索会减少，我们利用由视觉语言模型（VLM）生成的多尺度感知文本提示来增强每一步的放大。提示提取器本身使用具有评论家VLM的广义奖励策略优化（GRPO）进行微调，使文本指导符合人类偏好。实验表明，使用CoZ包装的标准4倍扩散SR模型可以实现超过256倍的放大，同时保持高度的感知质量和保真度。项目页面：<a target="_blank" rel="noopener" href="https://bryanswkim.github.io/chain-of-zoom/%E3%80%82">https://bryanswkim.github.io/chain-of-zoom/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18600v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://bryanswkim.github.io/chain-of-zoom/">https://bryanswkim.github.io/chain-of-zoom/</a></p>
<p><strong>Summary</strong></p>
<p>该文针对现代单图像超分辨率（SISR）模型在放大倍数超出训练规模时性能下降的问题，提出了Chain-of-Zoom（CoZ）框架。该框架将SISR分解为中间尺度状态的自回归链，通过多尺度感知提示进行超分辨率重建。CoZ通过重复使用骨干SR模型，将条件概率分解为可解决的子问题，实现极端分辨率而无需额外训练。在高倍放大时，视觉线索减少，因此每个缩放步骤都会借助由视觉语言模型（VLM）生成的多尺度感知文本提示进行增强。提示提取器使用广义奖励策略优化（GRPO）和评论家VLM进行微调，使文本指导符合人类偏好。实验表明，使用CoZ包装的4倍扩散SR模型可实现超过256倍的放大，同时保持高感知质量和保真度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Chain-of-Zoom（CoZ）框架解决了现代SISR模型在超出训练规模时的放大性能瓶颈。</li>
<li>CoZ通过将SISR分解为中间尺度状态的自回归链，利用多尺度感知提示进行超分辨率重建。</li>
<li>CoZ通过重复使用骨干SR模型，分解条件概率为子问题，实现极端分辨率提升。</li>
<li>在高倍放大时，视觉线索减少，因此利用视觉语言模型（VLM）生成的多尺度感知文本提示增强每个缩放步骤。</li>
<li>文本提示提取器通过广义奖励策略优化（GRPO）和评论家VLM进行微调，以符合人类偏好。</li>
<li>实验显示，使用CoZ的SISR模型在超过256倍放大时仍能保持高感知质量和保真度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18600">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e1664d627cd3a58127589dd96264e638.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b2e88bd5dbe2deffd776eea323e77dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51f25eda034c1a4993df4637c421de37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d46191aa00e089f98dfc98e5135f2663.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Generative-RLHF-V-Learning-Principles-from-Multi-modal-Human-Preference"><a href="#Generative-RLHF-V-Learning-Principles-from-Multi-modal-Human-Preference" class="headerlink" title="Generative RLHF-V: Learning Principles from Multi-modal Human Preference"></a>Generative RLHF-V: Learning Principles from Multi-modal Human Preference</h2><p><strong>Authors:Jiayi Zhou, Jiaming Ji, Boyuan Chen, Jiapeng Sun, Wenqi Chen, Donghai Hong, Sirui Han, Yike Guo, Yaodong Yang</strong></p>
<p>Training multi-modal large language models (MLLMs) that align with human intentions is a long-term challenge. Traditional score-only reward models for alignment suffer from low accuracy, weak generalization, and poor interpretability, blocking the progress of alignment methods, e.g., reinforcement learning from human feedback (RLHF). Generative reward models (GRMs) leverage MLLMs’ intrinsic reasoning capabilities to discriminate pair-wise responses, but their pair-wise paradigm makes it hard to generalize to learnable rewards. We introduce Generative RLHF-V, a novel alignment framework that integrates GRMs with multi-modal RLHF. We propose a two-stage pipeline: $\textbf{multi-modal generative reward modeling from RL}$, where RL guides GRMs to actively capture human intention, then predict the correct pair-wise scores; and $\textbf{RL optimization from grouped comparison}$, which enhances multi-modal RL scoring precision by grouped responses comparison. Experimental results demonstrate that, besides out-of-distribution generalization of RM discrimination, our framework improves 4 MLLMs’ performance across 7 benchmarks by $18.1%$, while the baseline RLHF is only $5.3%$. We further validate that Generative RLHF-V achieves a near-linear improvement with an increasing number of candidate responses. Our code and models can be found at <a target="_blank" rel="noopener" href="https://generative-rlhf-v.github.io/">https://generative-rlhf-v.github.io</a>. </p>
<blockquote>
<p>训练符合人类意图的多模态大型语言模型（MLLMs）是一项长期挑战。传统的仅评分奖励模型在对齐方面存在精度低、泛化能力弱和解释性差的缺点，阻碍了对齐方法（例如，基于人类反馈的强化学习（RLHF））的进步。生成奖励模型（GRMs）利用MLLMs的内在推理能力来区分成对响应，但其成对范式难以推广到可学习的奖励。我们引入了生成式RLHF-V，这是一种将GRMs与多模态RLHF相结合的新型对齐框架。我们提出了一个两阶段的流程：<strong>利用强化学习进行多模态生成奖励建模</strong>，其中RL指导GRMs主动捕捉人类意图，然后预测正确的成对分数；和<strong>基于分组比较的RL优化</strong>，通过比较分组响应来提高多模态RL评分精度。实验结果表明，除了RM鉴别的分布外推广，我们的框架在7个基准测试上提高了4个MLLM的性能，提高了18.1%，而基线RLHF仅为5.3%。我们进一步验证，随着候选响应数量的增加，生成式RLHF-V实现了近线性的改进。我们的代码和模型可在<a target="_blank" rel="noopener" href="https://generative-rlhf-v.github.io/">https://generative-rlhf-v.github.io</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18531v1">PDF</a> 9 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>训练多模态大型语言模型（MLLMs）以符合人类意图是一个长期挑战。传统的仅评分奖励模型在对齐方面存在准确性低、泛化能力弱和解释性差的问题，阻碍了如基于人类反馈的强化学习（RLHF）等对齐方法的进展。本文提出了生成奖励模型（GRMs），利用MLLMs的内在推理能力来区分配对响应，但其配对范式难以推广到可学习的奖励。本文介绍了全新的对齐框架——生成RLHF-V，它将GRMs与多模态RLHF相结合。提出了两阶段管道：首先是基于RL的多模态生成奖励建模，其中RL引导GRMs主动捕捉人类意图，然后预测正确的配对分数；其次是基于分组比较的RL优化，通过分组响应比较提高多模态RL评分精度。实验结果表明，除了RM鉴别的分布外推广能力外，我们的框架在7个基准测试中提高了4个MLLM的性能达18.1%，而基线RLHF仅提高5.3%。进一步验证了生成RLHF-V随着候选响应数量的增加实现了近线性的改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>训练多模态大型语言模型与人的意图对齐是一个长期挑战。</li>
<li>传统奖励模型在对齐方面存在局限，如准确性、泛化能力和解释性方面的问题。</li>
<li>引入生成奖励模型（GRMs）利用MLLMs的内在推理能力进行配对响应区分。</li>
<li>提出新的对齐框架——生成RLHF-V，结合GRMs和多模态RLHF。</li>
<li>该框架包含两阶段：基于RL的奖励建模和基于分组比较的RL优化。</li>
<li>实验证明该框架在多个基准测试中显著提高MLLM性能。</li>
<li>随着候选响应数量的增加，该框架实现近线性的改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18531">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8eab29339b0510ec8a977cc0be91f513.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29e3f704dae9bcc57b355a02c77312c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b486715a2a8505be852377172654986.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-544674add46379b8de486ff7cbcae674.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f10fc16fabc00df82a7336ad79187815.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Towards-Revealing-the-Effectiveness-of-Small-Scale-Fine-tuning-in-R1-style-Reinforcement-Learning"><a href="#Towards-Revealing-the-Effectiveness-of-Small-Scale-Fine-tuning-in-R1-style-Reinforcement-Learning" class="headerlink" title="Towards Revealing the Effectiveness of Small-Scale Fine-tuning in   R1-style Reinforcement Learning"></a>Towards Revealing the Effectiveness of Small-Scale Fine-tuning in   R1-style Reinforcement Learning</h2><p><strong>Authors:Yutong Chen, Jiandong Gao, Ji Wu</strong></p>
<p>R1-style Reinforcement Learning (RL) significantly enhances Large Language Models’ reasoning capabilities, yet the mechanism behind rule-based RL remains unclear. We found that small-scale SFT has significant influence on RL but shows poor efficiency. To explain our observations, we propose an analytical framework and compare the efficiency of SFT and RL by measuring sample effect. Hypothetical analysis show that SFT efficiency is limited by training data. Guided by our analysis, we propose Re-distillation, a technique that fine-tunes pretrain model through small-scale distillation from the RL-trained policy. Experiments on Knight &amp; Knave and MATH datasets demonstrate re-distillation’s surprising efficiency: re-distilled models match RL performance with far fewer samples and less computation. Empirical verification shows that sample effect is a good indicator of performance improvements. As a result, on K&amp;K dataset, our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches its instruct-tuned variant without RL. Our work explains several interesting phenomena in R1-style RL, shedding light on the mechanisms behind its empirical success. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/on1262/deep-reasoning">https://github.com/on1262/deep-reasoning</a> </p>
<blockquote>
<p>R1风格的强化学习（RL）显著提高了大型语言模型的推理能力，但其背后的机制仍然不明确。我们发现小规模SFT对RL有很大的影响，但效率不高。为了解释我们的观察结果，我们提出了一个分析框架，并通过测量样本效应来比较SFT和RL的效率。假设分析表明，SFT的效率受到训练数据的限制。在我们的分析指导下，我们提出了再蒸馏技术，这是一种通过小规模蒸馏从RL训练的决策中学习微调预训练模型的方法。在Knight &amp; Knave和MATH数据集上的实验证明了再蒸馏的惊人效率：再蒸馏模型使用较少的样本和计算量就能达到RL的性能。经验验证表明，样本效应是性能改进的良好指标。因此，在K&amp;K数据集上，我们的再蒸馏Qwen2.5-1.5B模型仅使用1K个SFT样本就超过了DeepSeek-V3-0324。在MATH上，使用再蒸馏的500个样本微调Qwen2.5-1.5B模型可以与没有RL的指令调整变体相匹配。我们的工作解释了R1风格RL的几个有趣现象，揭示了其经验成功的机制。代码可用在：<a target="_blank" rel="noopener" href="https://github.com/on1262/deep-reasoning">https://github.com/on1262/deep-reasoning</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17988v2">PDF</a> 11 figs, 3 table, preprint</p>
<p><strong>Summary</strong></p>
<p>R1风格强化学习（RL）可显著提升大型语言模型的推理能力，但其背后的机制尚不清楚。研究发现小规模SFT对RL有很大影响但效率较低。为解释观察结果，本文提出分析框架，通过测量样本效应比较SFT和RL的效率。假设分析显示SFT效率受限于训练数据。基于分析，本文提出再蒸馏技术，该技术通过来自RL训练策略的少量样本微调预训练模型。在Knight &amp; Knave和MATH数据集上的实验显示再蒸馏的惊人效率：再蒸馏模型用更少的样本和计算量匹配RL性能。经验验证显示样本效应是性能改进的良好指标。因此，在K&amp;K数据集上，我们的再蒸馏Qwen2.5-1.5B模型仅使用1KSFT样本就超越了DeepSeek-V3-0324。在MATH上，Qwen2.5-1.5B使用再蒸馏的500个样本进行微调，匹配了没有RL的指令调优变体。本研究解释了R1风格RL中的几个有趣现象，揭示了其经验成功的机制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>R1-style Reinforcement Learning显著增强了大型语言模型的推理能力，但机制尚不清楚。</li>
<li>小规模SFT对RL有影响，但效率较低。</li>
<li>提出了一个分析框架来解释这一观察结果，通过测量样本效应来比较SFT和RL的效率。</li>
<li>假设分析显示，SFT效率受限于训练数据。</li>
<li>提出再蒸馏技术，能有效提高模型的训练效率和性能。</li>
<li>在特定数据集上的实验证明了再蒸馏技术的有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17988">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-da9dc98994f59ee1c05654847fe410ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bebbc683fc0f91d247784da178d4c77.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36a95e010316bfec1d4560b03457c0d4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Plan-R1-Safe-and-Feasible-Trajectory-Planning-as-Language-Modeling"><a href="#Plan-R1-Safe-and-Feasible-Trajectory-Planning-as-Language-Modeling" class="headerlink" title="Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling"></a>Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling</h2><p><strong>Authors:Xiaolong Tang, Meina Kan, Shiguang Shan, Xilin Chen</strong></p>
<p>Safe and feasible trajectory planning is essential for real-world autonomous driving systems. However, existing learning-based planning methods often rely on expert demonstrations, which not only lack explicit safety awareness but also risk inheriting unsafe behaviors such as speeding from suboptimal human driving data. Inspired by the success of large language models, we propose Plan-R1, a novel two-stage trajectory planning framework that formulates trajectory planning as a sequential prediction task, guided by explicit planning principles such as safety, comfort, and traffic rule compliance. In the first stage, we train an autoregressive trajectory predictor via next motion token prediction on expert data. In the second stage, we design rule-based rewards (e.g., collision avoidance, speed limits) and fine-tune the model using Group Relative Policy Optimization (GRPO), a reinforcement learning strategy, to align its predictions with these planning principles. Experiments on the nuPlan benchmark demonstrate that our Plan-R1 significantly improves planning safety and feasibility, achieving state-of-the-art performance. Our code will be made public soon. </p>
<blockquote>
<p>安全可行的轨迹规划对于真实世界的自动驾驶系统至关重要。然而，现有的基于学习的规划方法往往依赖于专家演示，这不仅缺乏明确的安全意识，而且还存在从次优人类驾驶数据中继承不安全行为（如超速）的风险。受大型语言模型成功的启发，我们提出了Plan-R1，这是一种新型的两阶段轨迹规划框架，它将轨迹规划制定为受明确规划原则（如安全、舒适和遵守交通规则）指导的序列预测任务。在第一阶段，我们通过专家数据预测下一个动作标记来训练一个自回归轨迹预测器。在第二阶段，我们设计基于规则的奖励（例如防撞、限速等），并使用一种强化学习策略——群组相对策略优化（GRPO）对模型进行微调，使其预测与这些规划原则保持一致。在nuPlan基准测试上的实验表明，我们的Plan-R1在规划安全性和可行性方面有了显著提高，达到了最先进的性能。我们的代码很快将公开。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17659v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Plan-R1的新型两阶段轨迹规划框架，用于自主驾驶系统的安全可行轨迹规划。该框架将轨迹规划制定为受明确规划原则（如安全、舒适和交通规则遵守）指导的序列预测任务。第一阶段通过专家数据训练自回归轨迹预测器，第二阶段使用基于规则的奖励和强化学习策略进行微调，使其预测与规划原则保持一致。在nuPlan基准测试上的实验表明，Plan-R1在规划安全性和可行性方面显著提高，达到最新技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自主驾驶系统中的轨迹规划必须安全可行。</li>
<li>现有学习规划方法依赖专家演示，缺乏明确的安全意识，并可能继承不安全行为。</li>
<li>Plan-R1框架将轨迹规划制定为序列预测任务，受明确规划原则（如安全、舒适和交通规则遵守）指导。</li>
<li>第一阶段通过专家数据训练自回归轨迹预测器。</li>
<li>第二阶段使用基于规则的奖励和强化学习策略（如碰撞避免、速度限制）对模型进行微调。</li>
<li>在nuPlan基准测试上，Plan-R1表现出显著提高的安全性和可行性，达到最新技术性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17659">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8c53dc38336c4e9f60779412eac22c14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cff5665eea698fa546faf332f6308ab0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Rethinking-the-Sampling-Criteria-in-Reinforcement-Learning-for-LLM-Reasoning-A-Competence-Difficulty-Alignment-Perspective"><a href="#Rethinking-the-Sampling-Criteria-in-Reinforcement-Learning-for-LLM-Reasoning-A-Competence-Difficulty-Alignment-Perspective" class="headerlink" title="Rethinking the Sampling Criteria in Reinforcement Learning for LLM   Reasoning: A Competence-Difficulty Alignment Perspective"></a>Rethinking the Sampling Criteria in Reinforcement Learning for LLM   Reasoning: A Competence-Difficulty Alignment Perspective</h2><p><strong>Authors:Deyang Kong, Qi Guo, Xiangyu Xi, Wei Wang, Jingang Wang, Xunliang Cai, Shikun Zhang, Wei Ye</strong></p>
<p>Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces $\textbf{C}$ompetence-$\textbf{D}$ifficulty $\textbf{A}$lignment $\textbf{S}$ampling ($\textbf{CDAS}$), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the model’s current competence using a fixed-point system. Experimental results across a range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, a competitive strategy in DAPO, which is 2.33 times slower than CDAS. </p>
<blockquote>
<p>强化学习在提高大型语言模型的推理能力方面显示出潜力，但在 rollout 阶段存在样本效率低的问题，难以扩展。现有方法试图通过根据问题难度调度问题来提高效率。然而，这些方法受到问题难度估计不稳定和偏见的影响，无法捕捉模型能力和问题难度在强化学习训练中的对齐情况，从而导致结果不理想。为了解决这些局限性，本文引入了能力-难度对齐采样（CDAS），通过聚合问题的历史性能差异，实现准确稳定的问题难度估计。然后，通过固定点系统量化模型能力，自适应选择难度与模型当前能力相匹配的问题。在多个具有挑战性的数学基准测试上的实验结果表明，CDAS 在准确性和效率方面取得了巨大的改进。CDAS 在平均准确率方面达到了最高水平，与基线相比具有显著优势，并且在与 DAPO 中的动态采样策略相比时表现出明显的速度优势，后者是 CDAS 的 2.33 倍慢。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17652v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>强化学习在提升大型语言模型的推理能力方面具有潜力，但其在rollout阶段的样本效率较低，难以扩展。现有方法试图通过根据问题难度进行调度来提高效率，但存在对问题难度的不稳定、有偏估计，以及无法捕捉模型能力与问题难度在RL训练中的对齐关系，导致结果不尽如人意。为解决这些问题，本文提出一种Competence-Difficulty Alignment Sampling（CDAS）方法。它可以通过汇聚问题的历史表现差异来准确稳定地估计问题难度。随后利用一个定点系统量化模型能力，自适应选择难度与模型当前能力相匹配的问题。在多个挑战性的数学基准测试上进行的实验表明，CDAS在准确性和效率上都取得了巨大的提升。相较于动态采样等竞争性策略，CDAS达到了最高的平均准确率，并且在效率上展现出显著优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习对于提升大型语言模型的推理能力具有潜力，但在实际应用中面临样本效率低的问题。</li>
<li>现有方法通过问题难度调度提高效率，但存在对问题难度的不稳定、有偏估计。</li>
<li>CDAS方法可以准确稳定地估计问题难度，基于模型能力的量化来采样问题。</li>
<li>CDAS通过自适应选择难度与模型当前能力相匹配的问题，提高了训练效率和模型性能。</li>
<li>在多个数学基准测试上，CDAS取得了最高的平均准确率。</li>
<li>与其他采样策略相比，CDAS在效率上展现出显著优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17652">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-437e6107e425b772e36af78a19400ccf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c71e91357f2baf6b63956cd48b4745a1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Pixel-Reasoner-Incentivizing-Pixel-Space-Reasoning-with-Curiosity-Driven-Reinforcement-Learning"><a href="#Pixel-Reasoner-Incentivizing-Pixel-Space-Reasoning-with-Curiosity-Driven-Reinforcement-Learning" class="headerlink" title="Pixel Reasoner: Incentivizing Pixel-Space Reasoning with   Curiosity-Driven Reinforcement Learning"></a>Pixel Reasoner: Incentivizing Pixel-Space Reasoning with   Curiosity-Driven Reinforcement Learning</h2><p><strong>Authors:Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, Wenhu Chen</strong></p>
<p>Chain-of-thought reasoning has significantly improved the performance of Large Language Models (LLMs) across various domains. However, this reasoning process has been confined exclusively to textual space, limiting its effectiveness in visually intensive tasks. To address this limitation, we introduce the concept of reasoning in the pixel-space. Within this novel framework, Vision-Language Models (VLMs) are equipped with a suite of visual reasoning operations, such as zoom-in and select-frame. These operations enable VLMs to directly inspect, interrogate, and infer from visual evidences, thereby enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space reasoning capabilities in VLMs presents notable challenges, including the model’s initially imbalanced competence and its reluctance to adopt the newly introduced pixel-space operations. We address these challenges through a two-phase training approach. The first phase employs instruction tuning on synthesized reasoning traces to familiarize the model with the novel visual operations. Following this, a reinforcement learning (RL) phase leverages a curiosity-driven reward scheme to balance exploration between pixel-space reasoning and textual reasoning. With these visual operations, VLMs can interact with complex visual inputs, such as information-rich images or videos to proactively gather necessary information. We demonstrate that this approach significantly improves VLM performance across diverse visual reasoning benchmarks. Our 7B model, \model, achieves 84% on V* bench, 74% on TallyQA-Complex, and 84% on InfographicsVQA, marking the highest accuracy achieved by any open-source model to date. These results highlight the importance of pixel-space reasoning and the effectiveness of our framework. </p>
<blockquote>
<p>链式思维推理已经显著提高了大型语言模型（LLM）在不同领域的性能。然而，这种推理过程一直被限制在文本空间内，使其在视觉密集型任务中的有效性受到限制。为了解决这一局限性，我们引入了像素空间推理的概念。在这一新颖框架下，视觉语言模型（VLM）配备了一系列视觉推理操作，例如放大和选择帧。这些操作使VLM能够直接检查、质询和从视觉证据中进行推断，从而提高视觉任务的推理保真度。在VLM中培养这种像素空间推理能力面临着显著挑战，包括模型的初始能力不平衡及其对新引入的像素空间操作的抵触情绪。我们通过两阶段训练方法来应对这些挑战。第一阶段采用合成推理轨迹的指令调整，使模型熟悉新型视觉操作。之后，强化学习（RL）阶段利用基于好奇心的奖励方案来平衡像素空间推理和文本推理之间的探索。借助这些视觉操作，VLM可以与复杂的视觉输入（如信息丰富的图像或视频）进行交互，以主动收集必要信息。我们证明，该方法显著提高了VLM在多种视觉推理基准测试上的性能。我们的7B模型实现了V*测试平台上的84%、TallyQA-Complex上的74%和InfographicsVQA上的84%，这是迄今为止任何开源模型所取得的最高精度。这些结果突显了像素空间推理的重要性以及我们框架的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15966v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://tiger-ai-lab.github.io/Pixel-Reasoner/">https://tiger-ai-lab.github.io/Pixel-Reasoner/</a>,   Hands-on Demo: <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/TIGER-Lab/Pixel-Reasoner">https://huggingface.co/spaces/TIGER-Lab/Pixel-Reasoner</a></p>
<p><strong>摘要</strong><br>视觉空间推理的引入显著提升了跨域大型语言模型（LLM）的性能。然而，目前的推理过程主要局限于文本空间，这在视觉任务中限制了其有效性。为解决此局限性，我们提出了像素空间推理的概念。在此新框架下，视觉语言模型（VLM）配备了一系列视觉推理操作，如缩放、选择帧等。这些操作使VLM能够直接从视觉证据中检查、质询和推断，从而提高视觉任务的推理保真度。在VLM中培养这种像素空间推理能力面临着显著挑战，包括模型初始能力的不平衡以及对新引入的像素空间操作的接受度低。我们通过两阶段训练方法来应对这些挑战。第一阶段通过合成推理轨迹进行指令调整，使模型熟悉新的视觉操作。接下来，强化学习（RL）阶段利用好奇驱动的奖励方案来平衡像素空间推理和文本推理之间的探索。借助这些视觉操作，VLM可以与复杂视觉输入（如信息丰富的图像或视频）进行交互，主动收集必要信息。我们的方法显著提高了VLM在多种视觉推理基准测试上的性能。我们的7B模型在V*基准测试上达到84%、TallyQA-Complex上达到74%、InfographicsVQA上达到84%，成为迄今为止公开模型中准确性最高的。这些结果突显了像素空间推理的重要性及我们框架的有效性。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>链式思维推理已显著提高大型语言模型（LLM）在多个领域的性能。</li>
<li>当前推理过程主要局限于文本空间，对视觉任务的有效性有限。</li>
<li>引入像素空间推理概念以解决此局限性，配备视觉语言模型（VLM）以进行视觉推理操作。</li>
<li>面临培养像素空间推理能力的挑战，包括模型初始能力不平衡和新操作接受度低。</li>
<li>采用两阶段训练方法来应对这些挑战，通过指令调整和强化学习来熟悉和提升模型对像素空间推理的掌握。</li>
<li>视觉操作使VLM能够处理复杂视觉输入，如信息丰富的图像或视频。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15966">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b8da341c6535770c2f92380e0df79d31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21b36704bed0c7db46bd8427702f400f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d57b310f88595197e546e7e4c5563795.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15b1106d4ee92d009b154b01b7626b30.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PhyX-Does-Your-Model-Have-the-“Wits”-for-Physical-Reasoning"><a href="#PhyX-Does-Your-Model-Have-the-“Wits”-for-Physical-Reasoning" class="headerlink" title="PhyX: Does Your Model Have the “Wits” for Physical Reasoning?"></a>PhyX: Does Your Model Have the “Wits” for Physical Reasoning?</h2><p><strong>Authors:Hui Shen, Taiqiang Wu, Qi Han, Yunta Hsieh, Jizhou Wang, Yuyue Zhang, Yuxin Cheng, Zijian Hao, Yuansheng Ni, Xin Wang, Zhongwei Wan, Kai Zhang, Wendong Xu, Jing Xiong, Ping Luo, Wenhu Chen, Chaofan Tao, Zhuoqing Mao, Ngai Wong</strong></p>
<p>Existing benchmarks fail to capture a crucial aspect of intelligence: physical reasoning, the integrated ability to combine domain knowledge, symbolic reasoning, and understanding of real-world constraints. To address this gap, we introduce PhyX: the first large-scale benchmark designed to assess models capacity for physics-grounded reasoning in visual scenarios. PhyX includes 3K meticulously curated multimodal questions spanning 6 reasoning types across 25 sub-domains and 6 core physics domains: thermodynamics, electromagnetism, mechanics, modern physics, optics, and wave&amp;acoustics. In our comprehensive evaluation, even state-of-the-art models struggle significantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and GPT-o4-mini achieve only 32.5%, 42.2%, and 45.8% accuracy respectively-performance gaps exceeding 29% compared to human experts. Our analysis exposes critical limitations in current models: over-reliance on memorized disciplinary knowledge, excessive dependence on mathematical formulations, and surface-level visual pattern matching rather than genuine physical understanding. We provide in-depth analysis through fine-grained statistics, detailed case studies, and multiple evaluation paradigms to thoroughly examine physical reasoning capabilities. To ensure reproducibility, we implement a compatible evaluation protocol based on widely-used toolkits such as VLMEvalKit, enabling one-click evaluation. More details are available on our project page: <a target="_blank" rel="noopener" href="https://phyx-bench.github.io/">https://phyx-bench.github.io/</a>. </p>
<blockquote>
<p>现有的基准测试未能捕捉到智力的一个重要方面：物理推理能力，这是一种结合领域知识、符号推理和对现实世界的理解的综合能力。为了弥补这一空白，我们推出了PhyX：这是第一个旨在评估模型中基于物理的推理能力的大规模基准测试，适用于视觉场景。PhyX包括3000个精心策划的多模式问题，涵盖6种推理类型和25个子领域和6个核心物理领域：热力学、电磁学、力学、现代物理学、光学和波动声学。在我们的综合评估中，即使是最先进的模型在物理推理方面也面临巨大挑战。GPT-4o、Claude3.7-Sonnet和GPT-o4-mini的准确率分别为32.5%、42.2%和45.8%，与人类专家相比，性能差距超过29%。我们的分析揭示了当前模型的关键局限性：过于依赖记忆的知识、过度依赖数学公式和表面级的视觉模式匹配，而不是真正的物理理解。我们通过细粒度统计、详细案例研究和多个评估范式进行了深入分析，以彻底检查物理推理能力。为确保可重复性，我们基于广泛使用的工具包（如VLMEvalKit）实施了一个兼容的评估协议，实现一键评估。更多详细信息可在我们的项目页面查看：[网站链接]（<a target="_blank" rel="noopener" href="https://phyx-bench.github.io/%EF%BC%89%E3%80%82">https://phyx-bench.github.io/）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15929v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个名为PhyX的大规模基准测试，旨在评估模型在视觉场景中的物理推理能力。PhyX包含3000个精心策划的多模式问题，涵盖6个推理类型和25个子域以及6个核心物理领域。对现有模型的评估显示，即使在最新技术的模型也存在明显的物理推理困难，与专家相比存在超过29%的性能差距。文章深入分析了当前模型的关键局限性，包括过度依赖学科知识和数学公式，以及表面级的视觉模式匹配而非真正的物理理解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PhyX是首个设计用于评估模型在视觉场景中的物理推理能力的大规模基准测试。</li>
<li>PhyX包含广泛的问题类型，涵盖多个物理领域。</li>
<li>现有模型在物理推理方面存在显著困难，与专家相比存在性能差距。</li>
<li>现有模型的关键局限性包括过度依赖学科知识和数学公式。</li>
<li>模型更依赖于表面级的视觉模式匹配而非真正的物理理解。</li>
<li>文章提供了详细的案例研究，并通过多种评估范式对物理推理能力进行了深入分析。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15929">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-158ae521051800c6f6ac2a2c0c3681cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e75805040cc28574f7fe066615181e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-654a3bfa1557b87d4d0f2454573dd651.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3b6539a791ae55fb7cc8d645f31d4ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7838f49420a15b5f58d7ab38ad5cd9ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdf0a61190d0f356b08fff19fa2bfb51.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d1f2f20de7d0638f881888dab06c48c6.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LEXam-Benchmarking-Legal-Reasoning-on-340-Law-Exams"><a href="#LEXam-Benchmarking-Legal-Reasoning-on-340-Law-Exams" class="headerlink" title="LEXam: Benchmarking Legal Reasoning on 340 Law Exams"></a>LEXam: Benchmarking Legal Reasoning on 340 Law Exams</h2><p><strong>Authors:Yu Fan, Jingwei Ni, Jakob Merane, Etienne Salimbeni, Yang Tian, Yoan Hermstrüwer, Yinya Huang, Mubashara Akhtar, Florian Geering, Oliver Dreyer, Daniel Brunner, Markus Leippold, Mrinmaya Sachan, Alexander Stremitzer, Christoph Engel, Elliott Ash, Joel Niklaus</strong></p>
<p>Long-form legal reasoning remains a key challenge for large language models (LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions. Besides reference answers, the open questions are also accompanied by explicit guidance outlining the expected legal reasoning approach such as issue spotting, rule recall, or rule application. Our evaluation on both open-ended and multiple-choice questions present significant challenges for current LLMs; in particular, they notably struggle with open questions that require structured, multi-step legal reasoning. Moreover, our results underscore the effectiveness of the dataset in differentiating between models with varying capabilities. Adopting an LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate how model-generated reasoning steps can be evaluated consistently and accurately. Our evaluation setup provides a scalable method to assess legal reasoning quality beyond simple accuracy metrics. Project page: <a target="_blank" rel="noopener" href="https://lexam-benchmark.github.io/">https://lexam-benchmark.github.io/</a> </p>
<blockquote>
<p>尽管在测试时缩放方面最近有所进展，但长形式法律推理仍然是大型语言模型（LLM）面临的关键挑战。我们推出了LEXam，这是一个新的基准测试，来源于340场法律考试，跨越116个法律课程，涵盖各种科目和学位层次。该数据集包括4886个英文和德文的法律考试问题，其中包括2841个长形式开放问题和2045个多项选择题。除了参考答案外，开放问题还附有明确的指导，概述了预期的法律推理方法，如发现问题、回忆规则或规则应用。我们对开放问题和多项选择问题的评估显示，当前LLM面临巨大挑战；特别是他们在需要结构化、多步骤法律推理的开放问题上明显挣扎。此外，我们的结果突显了该数据集在区分不同能力模型方面的有效性。通过采用“大型语言模型作为法官”的模式，并进行严格的人类专家验证，我们展示了如何一致且准确地评估模型生成的推理步骤。我们的评估设置提供了一种可扩展的方法，可以评估超越简单准确性指标的法律推理质量。项目页面：<a target="_blank" rel="noopener" href="https://lexam-benchmark.github.io/">https://lexam-benchmark.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12864v2">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>近期虽然大型语言模型（LLMs）在测试时标度上有所进展，但长形式法律推理仍是其面临的关键挑战。为此，研究团队推出了LEXam基准测试，该测试从涵盖多种科目和学历层次的340场法律考试中衍生而来。数据集包含英文和德文的法律考试问题共4886道，其中包括开放式的长回答问题及选择题。除了参考答案外，开放性问题还附有预期的法律推理方法的明确指导。对开放性和选择题的评估显示，当前LLMs面临巨大挑战，特别是在需要结构化、多步骤法律推理的开放性问题上。此外，该数据集在区分不同能力模型方面效果显著。研究团队采用“大型语言模型作为法官”的模式，通过严格的人类专家验证，展示如何一致且准确地评估模型产生的推理步骤。其评估设置提供了一种可评估法律推理质量超越简单准确率的方法。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型（LLMs）在长形式法律推理上仍面临挑战。</li>
<li>LEXam基准测试从多场法律考试中衍生而来，包含多种题型和法律课程。</li>
<li>开放性问题伴随预期的推理方法的指导，以帮助LLMs理解正确的法律推理路径。</li>
<li>LLMs在需要结构化、多步骤的法律推理问题上表现不佳。</li>
<li>数据集在区分不同能力的模型方面效果显著。</li>
<li>采用“大型语言模型作为法官”的模式，通过人类专家验证评估模型的推理步骤。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12864">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f980c40f33f87c2f8d3ac9e462cb9e81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8a74caff7babbbf1b9c3c41d382840a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72f2ec032a4995da68b693da95c57bdc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-690d9670bfbe58ecde4b5b01fad05dc7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Shadow-FT-Tuning-Instruct-via-Base"><a href="#Shadow-FT-Tuning-Instruct-via-Base" class="headerlink" title="Shadow-FT: Tuning Instruct via Base"></a>Shadow-FT: Tuning Instruct via Base</h2><p><strong>Authors:Taiqiang Wu, Runming Yang, Jiayi Li, Pengfei Hu, Ngai Wong, Yujiu Yang</strong></p>
<p>Large language models (LLMs) consistently benefit from further fine-tuning on various tasks. However, we observe that directly tuning the INSTRUCT (i.e., instruction tuned) models often leads to marginal improvements and even performance degeneration. Notably, paired BASE models, the foundation for these INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to tune the INSTRUCT models by leveraging the corresponding BASE models. The key insight is to fine-tune the BASE model, and then directly graft the learned weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no additional parameters, is easy to implement, and significantly improves performance. We conduct extensive experiments on tuning mainstream LLMs, such as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering coding, reasoning, and mathematical tasks. Experimental results demonstrate that Shadow-FT consistently outperforms conventional full-parameter and parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT can be applied to multimodal large language models (MLLMs) and combined with direct preference optimization (DPO). Codes and weights are available at \href{<a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/Shadow-FT%7D%7BGithub%7D">https://github.com/wutaiqiang/Shadow-FT}{Github}</a>. </p>
<blockquote>
<p>大型语言模型（LLM）在各种任务上通过进一步的微调持续受益。然而，我们观察到直接调整INSTRUCT（即指令调整）模型往往只带来微小的改进，甚至性能下降。值得注意的是，这些INSTRUCT变体所依赖的BASE模型包含高度相似的权重值（如Llama 3.1 8B的平均值不到2%）。因此，我们提出了一种新的Shadow-FT框架，利用相应的BASE模型来调整INSTRUCT模型。关键思路是微调BASE模型，然后将学到的权重更新直接移植到INSTRUCT模型。我们提出的Shadow-FT不会引入额外的参数，易于实现，并能显著提高性能。我们在主流的LLM上进行了广泛的实验，如Qwen 3和Llama 3系列，并在涵盖编码、推理和数学任务的19个基准测试上进行了评估。实验结果表明，Shadow-FT持续优于传统的全参数和参数高效调整方法。进一步的分析表明，Shadow-FT可应用于多模态大型语言模型（MLLM），并与直接偏好优化（DPO）相结合。相关代码和权重可在Github上找到（链接：<a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/Shadow-FT%EF%BC%89%E3%80%82">https://github.com/wutaiqiang/Shadow-FT）。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12716v2">PDF</a> 19 pages, 10 tables, 6 figures</p>
<p><strong>Summary</strong>：<br>大型语言模型（LLM）在多种任务上通过微调能获得持续提升，但直接对INSTRUCT模型进行微调常导致性能提升有限甚至退步。研究提出Shadow-FT框架，利用对应的BASE模型来优化INSTRUCT模型的微调。该框架通过微调BASE模型并将学习到的权重更新直接应用到INSTRUCT模型上，无需增加额外参数，易于实现，并能显著提高性能。实验证明，Shadow-FT在主流LLM上表现优异，优于常规的全参数和参数高效微调方法。它还可以应用于多模态大型语言模型（MLLMs）并与直接偏好优化（DPO）结合。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLMs通过微调在多种任务上可获性能提升，但直接对INSTRUCT模型微调效果有限。</li>
<li>BASE模型与INSTRUCT模型权重高度相似。</li>
<li>提出的Shadow-FT框架利用BASE模型来优化INSTRUCT模型的微调，无需增加额外参数。</li>
<li>Shadow-FT通过微调BASE模型并将学习到的权重更新直接应用到INSTRUCT模型，显著提高性能。</li>
<li>实验证明Shadow-FT在主流LLM上表现优异，优于常规的全参数和参数高效微调方法。</li>
<li>Shadow-FT可应用于多模态大型语言模型（MLLMs）。</li>
<li>Shadow-FT可与直接偏好优化（DPO）结合使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12716">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d43d8b4e19d7a5c7707c84fbdf855d45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db1cd1fb6974c657fb8c9fe5a97dbd7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7a6a60633b13a1a8a5fe06c4d09ce7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-171f6aafd714b9fe49957d79b1830234.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3402b8f88c324bb04351c6a1c0aa18d1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Improving-Multilingual-Language-Models-by-Aligning-Representations-through-Steering"><a href="#Improving-Multilingual-Language-Models-by-Aligning-Representations-through-Steering" class="headerlink" title="Improving Multilingual Language Models by Aligning Representations   through Steering"></a>Improving Multilingual Language Models by Aligning Representations   through Steering</h2><p><strong>Authors:Omar Mahmoud, Buddhika Laknath Semage, Thommen George Karimpanal, Santu Rana</strong></p>
<p>In this paper, we investigate how large language models (LLMS) process non-English tokens within their layer representations, an open question despite significant advancements in the field. Using representation steering, specifically by adding a learned vector to a single model layer’s activations, we demonstrate that steering a single model layer can notably enhance performance. Our analysis shows that this approach achieves results comparable to translation baselines and surpasses state of the art prompt optimization methods. Additionally, we highlight how advanced techniques like supervised fine tuning (\textsc{sft}) and reinforcement learning from human feedback (\textsc{rlhf}) improve multilingual capabilities by altering representation spaces. We further illustrate how these methods align with our approach to reshaping LLMS layer representations. </p>
<blockquote>
<p>在这篇论文中，我们研究了大型语言模型（LLMS）是如何在其层表示中处理非英语令牌的，尽管该领域已经取得了重大进展，但这个问题仍然是一个开放的问题。通过使用表示引导法，特别是通过在单个模型层的激活中添加一个学习向量，我们证明引导单个模型层可以显著提高性能。我们的分析表明，该方法可以实现与翻译基线相当的结果，并超越了现有的提示优化方法。此外，我们重点介绍了如何使用先进的监督微调技术（\textbf{sft}）和人类反馈强化学习（\textbf{rlhf}）等高级技术如何通过改变表示空间来提高多语言能力。我们还进一步说明了这些方法如何与重塑LLMS层表示的方法相一致。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12584v1">PDF</a> </p>
<p><strong>Summary</strong><br>这篇论文研究了大型语言模型如何处理非英语符号的层表示。通过表示引导的方法，向单一模型层的激活添加学习向量，可以显著提高性能。分析与翻译基准线相比，该方法取得了相当的结果并超越了现有的提示优化方法。此外，论文还介绍了监督微调（SFT）和强化学习从人类反馈（RLHF）等技术如何改进多语言能力，并探讨了重塑LLMS层表示的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMS）对非英语符号的层表示处理是一个公开问题，尽管该领域已有显著进展。</li>
<li>通过表示引导的方法，增强单一模型层的性能可以显著提高。</li>
<li>引导方法的结果与翻译基准线相当，并超越了现有的提示优化方法。</li>
<li>监督微调（SFT）和强化学习从人类反馈（RLHF）等技术改进了多语言能力。</li>
<li>这些技术通过改变表示空间来改进多语言能力。</li>
<li>论文展示了如何将这些方法与重塑LLMS层表示的方法相结合。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12584">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-6a19e0d28cfd6722c20ea2b8812d5338.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45078bd7c3466355746716f49eb0103e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0599407c8f36833d97c7fd9da9e21ddb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b097e23dd11a7b57e05d6d7aac6f70a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-448d8f7cd7f480b23c575100bedf4fa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b2b6f43d4071716c3bc1b47fa6b1d9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-11ebed427d58fb2cd0ef6d9fa57c5eae.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Towards-Visuospatial-Cognition-via-Hierarchical-Fusion-of-Visual-Experts"><a href="#Towards-Visuospatial-Cognition-via-Hierarchical-Fusion-of-Visual-Experts" class="headerlink" title="Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts"></a>Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</h2><p><strong>Authors:Qi Feng</strong></p>
<p>While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research. </p>
<blockquote>
<p>虽然多模态大型语言模型（MLLMs）在一般的视觉语言任务上表现出色，但对于空间布局、关系和动态的空间推理仍然是巨大的挑战。现有的模型通常缺乏必要的架构组件和精细空间理解的专业训练数据。我们推出了ViCA2（视觉空间认知助手2），这是一种新型的多模态大型语言模型，旨在提高空间推理能力。ViCA2采用双视觉编码器架构，融合了SigLIP的语义和Hiera的空间结构，并采用了高效的标记比率控制机制。我们还开发了ViCA-322K大规模数据集，包含超过32万对空间定位的问题答案对，用于针对性的指令调整。在具有挑战性的VSI-Bench基准测试中，我们的ViCA2-7B模型取得了平均得分56.8分的最新水平成绩，显著超过了更大的开源模型（例如LLaVA-NeXT-Video-72B得分为40.9）和领先的专有模型（Gemini-1.5 Pro得分为45.4）。这证明了我们方法的有效性，即使在紧凑模型下也能实现强大的视觉空间智能。我们发布了ViCA2及其代码库和ViCA-322K数据集，以促进进一步研究。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12363v3">PDF</a> In version 1, Hidetoshi Shimodaira was included as a co-author   without their consent and has been removed from the author list</p>
<p><strong>Summary</strong></p>
<p>多模态大型语言模型（MLLMs）在一般的视觉语言任务上表现出色，但在视觉空间认知方面，即对空间布局、关系和动态推理仍存在挑战。现有模型缺乏必要的架构组件和精细空间理解的专业训练数据。我们推出ViCA2（视觉空间认知助手2），一款专为增强空间推理能力设计的新型MLLM。ViCA2采用双视角编码器架构，集成SigLIP进行语义分析，Hiera进行空间结构解析，并结合令牌比率控制机制以提高效率。我们还开发了ViCA-322K大型数据集，包含超过32万2千个空间定位的问题答案对，用于针对性的指令调优。在具有挑战性的VSI-Bench基准测试中，我们的ViCA2-7B模型达到了平均得分56.8，显著超越了其他大型开源模型（如LLaVA-NeXT-Video-72B，得分40.9）和领先的专业模型（Gemini-1.5 Pro，得分45.4）。这证明了我们在实现强大视觉空间智能的紧凑模型方面的有效性。我们发布ViCA2、其代码库和ViCA-322K数据集以促进进一步研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在视觉空间认知方面存在挑战。</li>
<li>ViCA2是一款新型MLLM，旨在增强空间推理能力。</li>
<li>ViCA2采用双视角编码器架构，集成SigLIP和Hiera进行语义和空间结构解析。</li>
<li>开发出ViCA-322K大型数据集用于针对性的指令调优。</li>
<li>ViCA2在VSI-Bench基准测试中表现优异，平均得分56.8。</li>
<li>ViCA2显著超越了其他大型开源和专业模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12363">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4c0a60dc96f980780755856a47f5f70f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01ddf487f5a21022b3d8cb8abb6ebc1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b49a1d2f8faf93399797eb2a9195519.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94de87aa0b44a40c568d427a06ae798c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Search-and-Refine-During-Think-Autonomous-Retrieval-Augmented-Reasoning-of-LLMs"><a href="#Search-and-Refine-During-Think-Autonomous-Retrieval-Augmented-Reasoning-of-LLMs" class="headerlink" title="Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning   of LLMs"></a>Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning   of LLMs</h2><p><strong>Authors:Yaorui Shi, Sihang Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, Xiang Wang</strong></p>
<p>Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new &#96;&#96;search-and-refine-during-think’’ paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively. </p>
<blockquote>
<p>大规模语言模型已经表现出了令人印象深刻的推理能力，但本质上受限于其知识库。检索增强推理通过允许大型语言模型查询外部资源来缓解这一限制，但现有方法经常检索到不相关或嘈杂的信息，阻碍了准确的推理。在本文中，我们提出了AutoRefine，这是一个采用新型“思考过程中的搜索与优化”范式的强化学习后训练框架。AutoRefine在连续的搜索调用之间引入了明确的知识优化步骤，使模型能够迭代地过滤、提炼和整理证据，然后生成答案。此外，我们通过使用群体相对策略优化，结合了定制的检索特定奖励和答案正确性奖励。在单跳和多跳问答基准测试上的实验表明，AutoRefine显著优于现有方法，特别是在复杂的多跳推理场景中。详细分析表明，AutoRefine能进行频繁的高质量搜索，并能有效地综合证据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11277v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为AutoRefine的强化学习后训练框架，采用新的“搜索与思考中精炼”模式，用于增强大型语言模型的推理能力。该框架在连续搜索调用之间引入显式知识精炼步骤，使模型能够迭代过滤、提炼和整理证据，从而生成更准确的答案。此外，通过结合检索特定奖励和答案正确性奖励，使用群体相对策略优化，AutoRefine在单跳和多跳问答基准测试上的表现均显著优于现有方法，特别是在复杂多跳推理场景中。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型虽然具备强大的推理能力，但其知识库本质上存在局限性。</li>
<li>检索增强推理可通过允许语言模型查询外部资源来减轻这一局限性。</li>
<li>现有方法往往检索到不相关或嘈杂的信息，阻碍准确推理。</li>
<li>AutoRefine框架引入“搜索与思考中精炼”模式，在连续搜索之间加入知识精炼步骤。</li>
<li>AutoRefine通过迭代过滤、提炼和整理证据，提高搜索质量，进而提升答案准确性。</li>
<li>结合检索特定奖励和答案正确性奖励，使用群体相对策略优化，提升模型表现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11277">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4e447565d2bbf874ae46124540158943.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c630524472572e4123adc656bbb11014.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3d42cacf66dffd249ce76343aae3313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4446c1de7272a2a37d42415d65e1827.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5849377557489b948ec35a0555161fd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Learning-When-to-Think-Shaping-Adaptive-Reasoning-in-R1-Style-Models-via-Multi-Stage-RL"><a href="#Learning-When-to-Think-Shaping-Adaptive-Reasoning-in-R1-Style-Models-via-Multi-Stage-RL" class="headerlink" title="Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models   via Multi-Stage RL"></a>Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models   via Multi-Stage RL</h2><p><strong>Authors:Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, Dongbin Zhao</strong></p>
<p>Large reasoning models (LRMs) are proficient at generating explicit, step-by-step reasoning sequences before producing final answers. However, such detailed reasoning can introduce substantial computational overhead and latency, particularly for simple problems. To address this over-thinking problem, we explore how to equip LRMs with adaptive thinking capabilities: enabling them to dynamically decide whether or not to engage in explicit reasoning based on problem complexity. Building on R1-style distilled models, we observe that inserting a simple ellipsis (“…”) into the prompt can stochastically trigger either a thinking or no-thinking mode, revealing a latent controllability in the reasoning behavior. Leveraging this property, we propose AutoThink, a multi-stage reinforcement learning (RL) framework that progressively optimizes reasoning policies via stage-wise reward shaping. AutoThink learns to invoke explicit reasoning only when necessary, while defaulting to succinct responses for simpler tasks. Experiments on five mainstream mathematical benchmarks demonstrate that AutoThink achieves favorable accuracy-efficiency trade-offs compared to recent prompting and RL-based pruning methods. It can be seamlessly integrated into any R1-style model, including both distilled and further fine-tuned variants. Notably, AutoThink improves relative accuracy by 6.4 percent while reducing token usage by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and adaptive reasoning paradigm for LRMs. Project Page: <a target="_blank" rel="noopener" href="https://github.com/ScienceOne-AI/AutoThink">https://github.com/ScienceOne-AI/AutoThink</a>. </p>
<blockquote>
<p>大型推理模型（LRMs）擅长在给出最终答案之前生成明确、循序渐进的推理序列。然而，这样的详细推理可能会引入大量的计算开销和延迟，尤其是在处理简单问题时。为了解决过度思考的问题，我们探讨如何为LRMs配备适应性思考能力：让它们能够动态地决定是否进行显性推理，这取决于问题的复杂性。我们基于R1风格的蒸馏模型观察到，在提示中插入一个简单的省略号（“…”）可以随机触发思考模式或非思考模式，揭示了推理行为中的潜在可控性。利用这一特性，我们提出了AutoThink，这是一个多阶段的强化学习（RL）框架，它通过分阶段奖励塑造来逐步优化推理策略。AutoThink学习只在必要时进行显性推理，而默认为简单的任务提供简洁的回应。在五个主流数学基准测试上的实验表明，与最近的提示和基于RL的修剪方法相比，AutoThink在准确性和效率之间实现了有利的权衡。它可以无缝地集成到任何R1风格的模型中，包括蒸馏和进一步微调后的变体。值得注意的是，AutoThink在DeepSeek-R1-Distill-Qwen-1.5B上相对提高了6.4%的准确率，同时减少了52%的令牌使用，为LRMs建立了可扩展和自适应的推理范式。项目页面：<a target="_blank" rel="noopener" href="https://github.com/ScienceOne-AI/AutoThink%E3%80%82">https://github.com/ScienceOne-AI/AutoThink。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10832v2">PDF</a> Fisrt Submitted on 16 May 2025; Update on 28 May 2025</p>
<p><strong>Summary</strong></p>
<p>大型推理模型（LRMs）能够生成明确、逐步的推理序列以得出最终答案，但这样的详细推理可能会带来较大的计算开销和延迟，尤其是在处理简单问题时。为解决这一问题，本文旨在探讨如何为LRMs配备自适应思考能力，使其能够根据问题复杂度动态决定是否进行显性推理。研究基于R1风格蒸馏模型，发现通过在提示中插入省略号（“…”），可以随机触发思考或非思考模式，揭示出推理行为的潜在可控性。基于此特性，本文提出了AutoThink，这是一个多阶段强化学习（RL）框架，通过阶段性奖励塑造来逐步优化推理策略。AutoThink能够按需进行显性推理，为简单任务提供简洁回应。在五个主流数学基准测试上的实验表明，与最新的提示和基于RL的修剪方法相比，AutoThink在准确性与效率之间取得了有利的权衡。它可以无缝地集成到任何R1风格模型中，包括蒸馏和进一步微调过的变体。尤其是，AutoThink在DeepSeek-R1-Distill-Qwen-1.5B上相对提高了6.4%的准确率，同时减少了52%的令牌使用量，为LRMs建立了可扩展和自适应的推理模式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型推理模型虽然能生成详细的推理步骤，但处理简单问题时可能导致计算开销和延迟。</li>
<li>通过在提示中插入省略号，可以触发模型的思考或非思考模式，表明推理行为具有潜在可控性。</li>
<li>提出AutoThink框架，利用多阶段强化学习优化推理策略，实现按需推理。</li>
<li>AutoThink能够在保持较高准确率的同时，提高模型效率，适用于多种R1风格模型。</li>
<li>AutoThink在DeepSeek-R1-Distill-Qwen-1.5B模型上取得了显著的准确率提升和令牌使用减少。</li>
<li>该研究为大型推理模型建立了一种可扩展和自适应的推理模式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10832">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2023105cedaf55213f3453d0d3400f14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b99f41bd4243d7e267934383d35a6f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f64f6c4e8c4dc47c179962d6a7808f02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc3abd56023e573b251f0edd3c978cda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dbfe7baf7282040d5d1eab12c7bfe42.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Beyond-‘Aha-’-Toward-Systematic-Meta-Abilities-Alignment-in-Large-Reasoning-Models"><a href="#Beyond-‘Aha-’-Toward-Systematic-Meta-Abilities-Alignment-in-Large-Reasoning-Models" class="headerlink" title="Beyond ‘Aha!’: Toward Systematic Meta-Abilities Alignment in Large   Reasoning Models"></a>Beyond ‘Aha!’: Toward Systematic Meta-Abilities Alignment in Large   Reasoning Models</h2><p><strong>Authors:Zhiyuan Hu, Yibo Wang, Hanze Dong, Yuhui Xu, Amrita Saha, Caiming Xiong, Bryan Hooi, Junnan Li</strong></p>
<p>Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referred to as the model’s “aha moment”. However, the timing and consistency of these emergent behaviors remain unpredictable and uncontrollable, limiting the scalability and reliability of LRMs’ reasoning capabilities. To address these limitations, we move beyond reliance on prompts and coincidental “aha moments”. Instead, we explicitly align models with three meta-abilities: deduction, induction, and abduction, using automatically generated, self-verifiable tasks. Our three stage-pipeline individual alignment, parameter-space merging, and domain-specific reinforcement learning, boosting performance by over 10% relative to instruction-tuned baselines. Furthermore, domain-specific RL from the aligned checkpoint yields an additional gain in performance ceiling for both 7B and 32B models across math, coding, and science benchmarks, demonstrating that explicit meta-ability alignment offers a scalable and dependable foundation for reasoning. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/zhiyuanhubj/Meta-Ability-Alignment">https://github.com/zhiyuanhubj/Meta-Ability-Alignment</a> </p>
<blockquote>
<p>大型推理模型（LRMs）已经具备潜在的长链思维推理能力。先前的研究表明，基于结果的强化学习（RL）可以偶然激发高级推理行为，如自我修正、回溯和验证现象，这些常被看作是模型的“顿悟时刻”。然而，这些突发行为的时机和一致性仍然不可预测和不可控制，限制了LRMs推理能力的可扩展性和可靠性。为了解决这些局限性，我们不再依赖提示和偶然的“顿悟时刻”。相反，我们通过使用自动生成的、可自我验证的任务，明确地将模型与三种元能力（演绎、归纳和溯因）对齐。我们的三阶段管道包括个人对齐、参数空间合并和领域特定强化学习，相对于指令调整基准线，性能提高了10%以上。此外，从对齐检查点进行的领域特定RL为7B和32B模型在数学、编码和科学基准测试方面的性能上限带来了额外提升，表明明确的元能力对齐为推理提供了可扩展和可靠的基石。代码可在<a target="_blank" rel="noopener" href="https://github.com/zhiyuanhubj/Meta-Ability-Alignment%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhiyuanhubj/Meta-Ability-Alignment找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10554v2">PDF</a> In Progress</p>
<p><strong>Summary</strong></p>
<p>大型推理模型（LRMs）已具备潜在的长链思维推理能力。先前的研究表明，基于结果的强化学习（RL）可以偶然激发自我修正、回溯和验证等高级推理行为，这些行为通常被视为模型的“顿悟时刻”。然而，这些突发行为的时机和一致性尚不可预测和控制，限制了LRMs推理能力的可扩展性和可靠性。为解决这些局限性，我们不再依赖提示和偶然的“顿悟时刻”，而是明确地将模型与演绎、归纳和溯因三种元能力对齐，使用自动生成的自我验证任务。我们的三阶段管道包括个体对齐、参数空间合并和领域特定强化学习，相较于指令调优基准线，性能提升超过10%。此外，从对齐检查点进行的领域特定RL为数学、编码和科学基准测试中的7B和32B模型提供了额外的性能提升，表明明确的元能力对齐为推理提供了可扩展和可靠的基石。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型推理模型已具有潜在的长链思维推理能力。</li>
<li>基于结果的强化学习可以激发模型的高级推理行为，如自我修正和验证。</li>
<li>当前方法面临推理行为不可预测和难以控制的局限性。</li>
<li>为解决这些问题，提出与演绎、归纳和溯因三种元能力对齐的方法。</li>
<li>使用自动生成的自我验证任务，通过三阶段管道提升模型性能。</li>
<li>领域特定强化学习进一步提高了模型的性能上限，特别是在数学、编码和科学领域。</li>
<li>明确的元能力对齐为推理提供了可扩展和可靠的基石。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10554">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-06ccda022f12fc421fd1d27803c963f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05c9191149f0e07ab5e9327a2ffddf0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67364ee229c668a9d57b2917a5199f2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e47eedfe9ea54088ffebaa4165e57f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa71ae42585f5d0c74b2f959e8c0f52a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DRA-GRPO-Exploring-Diversity-Aware-Reward-Adjustment-for-R1-Zero-Like-Training-of-Large-Language-Models"><a href="#DRA-GRPO-Exploring-Diversity-Aware-Reward-Adjustment-for-R1-Zero-Like-Training-of-Large-Language-Models" class="headerlink" title="DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like   Training of Large Language Models"></a>DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like   Training of Large Language Models</h2><p><strong>Authors:Xiwen Chen, Wenhui Zhu, Peijie Qiu, Xuanzhao Dong, Hao Wang, Haiyu Wu, Huayu Li, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi</strong></p>
<p>Recent advances in reinforcement learning for language model post-training, such as Group Relative Policy Optimization (GRPO), have shown promise in low-resource settings. However, GRPO typically relies on solution-level and scalar reward signals that fail to capture the semantic diversity among sampled completions. This leads to what we identify as a diversity-quality inconsistency, where distinct reasoning paths may receive indistinguishable rewards. To address this limitation, we propose $\textit{Diversity-aware Reward Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity into the reward computation. DRA uses Submodular Mutual Information (SMI) to downweight redundant completions and amplify rewards for diverse ones. This encourages better exploration during learning, while maintaining stable exploitation of high-quality samples. Our method integrates seamlessly with both GRPO and its variant DR.<del>GRPO, resulting in $\textit{DRA-GRPO}$ and $\textit{DGA-DR.</del>GRPO}$. We evaluate our method on five mathematical reasoning benchmarks and find that it outperforms recent strong baselines. It achieves state-of-the-art performance with an average accuracy of 58.2%, using only 7,000 fine-tuning samples and a total training cost of approximately $55. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xiwenc1/DRA-GRPO">https://github.com/xiwenc1/DRA-GRPO</a>. </p>
<blockquote>
<p>最近，强化学习在语言模型后训练方面的进展，如群体相对策略优化（GRPO），在低资源环境中显示出巨大的潜力。然而，GRPO通常依赖于解决方案级别和标量奖励信号，这些信号无法捕获采样完成的语义多样性。这导致了我们所说的多样性质量不一致问题，不同的推理路径可能会获得无法区分的奖励。为了解决这个问题，我们提出了“多样性感知奖励调整”（DRA）方法，该方法显式地将语义多样性纳入奖励计算中。DRA使用子模块互信息（SMI）来降低冗余完成的权重，并放大多样化完成的奖励。这鼓励了更好的学习过程探索，同时保持对高质量样本的稳定利用。我们的方法与GRPO及其变体DR.GRPO无缝集成，形成DRA-GRPO和DGA-DR.GRPO。我们在五个数学推理基准测试上对我们的方法进行了评估，发现它超越了最近的强大基准测试。使用仅7000个微调样本和大约55的总训练成本，它达到了最先进的性能，平均准确率达到了58.2%。代码可在<a target="_blank" rel="noopener" href="https://github.com/xiwenc1/DRA-GRPO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xiwenc1/DRA-GRPO找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09655v2">PDF</a> </p>
<p><strong>Summary</strong><br>强化学习在语言模型后训练方面，最新的发展如GRPO已在低资源设置领域显示出希望。然而，GRPO依赖于解决方案级别的标量奖励信号，无法捕捉采样补全的语义多样性，导致出现多样性质量不一致的问题。为解决这一问题，我们提出了DRA（多样性感知奖励调整）方法，该方法将语义多样性显式地纳入奖励计算中。DRA使用SMI（子模块互信息）来降低冗余补全的权重并放大对多样化补全的奖励。我们的方法能够很好地与GRPO及其变体DR.GRPO相结合，形成DRA-GRPO和DGA-DR.GRPO。我们在五个数学推理基准测试上评估了我们的方法，发现它优于最近的强大基线。使用仅7000个微调样本和约55的总训练成本，它实现了平均准确率为58.2%的最佳性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习在低资源设置中的语言模型后训练展现出潜力，尤其是GRPO方法。</li>
<li>GRPO存在多样性质量不一致的问题，因为它主要依赖解决方案级别的标量奖励信号。</li>
<li>提出的DRA方法通过明确考虑语义多样性来解决这个问题，使用SMI来平衡奖励和调整权重。</li>
<li>DRA与GRPO及其变体无缝集成，形成新的方法DRA-GRPO和DGA-DR.GRPO。</li>
<li>在五个数学推理基准测试上，DRA方法表现出卓越性能，优于其他基线方法。</li>
<li>使用有限的微调样本和较低的训练成本，DRA达到了较高的平均准确率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09655">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5a76552000073b9b82569ecbe8d9b3f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d5792f734516c33d332c216b883d83e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e539a1f490ce62eb9dc36d2d41938e6b.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="RM-R1-Reward-Modeling-as-Reasoning"><a href="#RM-R1-Reward-Modeling-as-Reasoning" class="headerlink" title="RM-R1: Reward Modeling as Reasoning"></a>RM-R1: Reward Modeling as Reasoning</h2><p><strong>Authors:Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, Hanghang Tong, Heng Ji</strong></p>
<p>Reward modeling is essential for aligning large language models with human preferences through reinforcement learning from human feedback. To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. Inspired by recent advances of long chain-of-thought on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RMs interpretability and performance. To this end, we introduce a new class of generative reward models - Reasoning Reward Models (ReasRMs) - which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. RM-R1 features a chain-of-rubrics (CoR) mechanism - self-generating sample-level chat rubrics or math&#x2F;code solutions, and evaluating candidate responses against them. The training of RM-R1 consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. Empirically, our models achieve state-of-the-art performance across three reward model benchmarks on average, outperforming much larger open-weight models (e.g., INF-ORM-Llama3.1-70B) and proprietary ones (e.g., GPT-4o) by up to 4.9%. Beyond final performance, we perform thorough empirical analyses to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six REASRM models along with code and data at <a target="_blank" rel="noopener" href="https://github.com/RM-R1-UIUC/RM-R1">https://github.com/RM-R1-UIUC/RM-R1</a>. </p>
<blockquote>
<p>奖励建模对于通过强化学习从人类反馈中对大型语言模型与人类偏好进行对齐至关重要。为了提供准确的奖励信号，奖励模型（RM）应在分配分数或判断之前激发深入思考并进行可解释的推理。受近期长链思维在推理密集型任务上进步的启发，我们假设并将验证将推理能力融入奖励建模会显著增强RM的可解释性和性能。为此，我们引入了一类新的生成奖励模型——推理奖励模型（ReasRMs），它将奖励建模制定为推理任务。我们提出了面向推理的训练管道，并训练了一系列ReasRMs，即RM-R1。RM-R1的特点是拥有链式提纲（CoR）机制——自我生成样本级聊天提纲或数学&#x2F;代码解决方案，并据此评估候选响应。RM-R1的训练包括两个关键阶段：（1）高质量推理链的蒸馏；（2）可验证奖励的强化学习。经验上，我们的模型在三个奖励模型基准测试上的平均性能达到最新水平，优于较大的开源模型（例如INF-ORM-Llama3.1-70B）和专有模型（例如GPT-4o），最高提升达4.9%。除了最终性能之外，我们还进行了全面的经验分析，以了解成功训练ReasRM的关键要素。为了方便未来研究，我们在<a target="_blank" rel="noopener" href="https://github.com/RM-R1-UIUC/RM-R1%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E5%85%AD%E4%B8%AAREASRM%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E7%9B%B8%E5%85%B3%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/RM-R1-UIUC/RM-R1上发布了六个REASRM模型以及相关的代码和数据。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02387v3">PDF</a> 25 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>奖励建模对于通过强化学习从人类反馈对齐大型语言模型与人类偏好至关重要。通过结合推理能力进行奖励建模，可显著提高奖励模型的可解释性和性能。本文提出了新一代的生成奖励模型——推理奖励模型（ReasRMs），将奖励建模表述为推理任务。通过面向推理的训练管道，训练出一系列ReasRMs，其中RM-R1模型具有链式rubrics（CoR）机制，能够自我生成样本级别的聊天rubrics或数学&#x2F;代码解决方案，并对候选响应进行评估。该模型包括两个阶段：高质量推理链的蒸馏和可验证奖励的强化学习。在三个奖励模型基准测试中，RM-R1模型平均性能达到最新水平，优于大型开源模型（如INF-ORM-Llama 3.1-70B）和专有模型（如GPT-4o），最高提升达4.9%。除了最终性能外，本文还进行了全面的实证研究，以了解成功的ReasRM训练的关键要素。为便于未来研究，我们在<a target="_blank" rel="noopener" href="https://github.com/RM-R1-UIUC/RM-R1%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E5%85%AD%E4%B8%AAREASRM%E6%A8%A1%E5%9E%8B%E5%8F%8A%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/RM-R1-UIUC/RM-R1上发布了六个REASRM模型及相关代码和数据。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>奖励建模对于通过强化学习从人类反馈中使大型语言模型与人类偏好对齐至关重要。</li>
<li>引入了一种新型的生成奖励模型——推理奖励模型（ReasRMs），将奖励建模作为推理任务。</li>
<li>RM-R1模型具有链式rubrics（CoR）机制，能够自我生成样本级别的聊天rubrics或数学&#x2F;代码解决方案。</li>
<li>RM-R1模型的训练包括两个阶段：高质量推理链的蒸馏和强化学习使用可验证奖励。</li>
<li>RM-R1模型在三个奖励模型基准测试中达到最新性能水平，优于其他大型模型。</li>
<li>除了最终性能外，文章还详细探讨了成功的ReasRM训练的关键要素。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02387">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-7cf3c15fc15c72d686cc761873535ea6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e444662af72a79abe2367f82ec9219b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65a7064fab6881124dcbe9a9fb233d53.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b07804023c6b554e5d7de9c03f771c03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b11a41ed5c54519434e59a435e73cda.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="HyperTree-Planning-Enhancing-LLM-Reasoning-via-Hierarchical-Thinking"><a href="#HyperTree-Planning-Enhancing-LLM-Reasoning-via-Hierarchical-Thinking" class="headerlink" title="HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking"></a>HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking</h2><p><strong>Authors:Runquan Gui, Zhihai Wang, Jie Wang, Chi Ma, Huiling Zhen, Mingxuan Yuan, Jianye Hao, Defu Lian, Enhong Chen, Feng Wu</strong></p>
<p>Recent advancements have significantly enhanced the performance of large language models (LLMs) in tackling complex reasoning tasks, achieving notable success in domains like mathematical and logical reasoning. However, these methods encounter challenges with complex planning tasks, primarily due to extended reasoning steps, diverse constraints, and the challenge of handling multiple distinct sub-tasks. To address these challenges, we propose HyperTree Planning (HTP), a novel reasoning paradigm that constructs hypertree-structured planning outlines for effective planning. The hypertree structure enables LLMs to engage in hierarchical thinking by flexibly employing the divide-and-conquer strategy, effectively breaking down intricate reasoning steps, accommodating diverse constraints, and managing multiple distinct sub-tasks in a well-organized manner. We further introduce an autonomous planning framework that completes the planning process by iteratively refining and expanding the hypertree-structured planning outlines. Experiments demonstrate the effectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner benchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement over o1-preview. </p>
<blockquote>
<p>最近的技术进步显著提高了大型语言模型（LLM）在解决复杂推理任务方面的性能，在数学和逻辑推理等领域取得了显著的成功。然而，这些方法在处理复杂的规划任务时面临挑战，主要是由于推理步骤繁多、约束多样以及处理多个不同子任务的挑战。为了解决这些挑战，我们提出了HyperTree Planning（HTP）这一新的推理范式，为有效规划构建超树结构规划大纲。超树结构使LLM能够通过灵活地采用分而治之的策略来进行分层思考，有效地简化复杂的推理步骤，适应各种约束，并以井然有序的方式管理多个不同的子任务。我们还引入了一个自主规划框架，通过迭代精化和扩展超树结构规划大纲来完成规划过程。实验表明HTP的有效性，在TravelPlanner基准测试中使用Gemini-1.5-Pro实现了最先进的准确性，较o1-preview的性能提高了3.6倍。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02322v2">PDF</a> arXiv admin note: text overlap with arXiv:2406.14228 by other authors</p>
<p><strong>Summary</strong>：近期发展显著提升大型语言模型（LLM）在处理复杂推理任务方面的性能，在数学和逻辑推理等领域取得了显著成功。然而，在处理复杂的规划任务时，这些方法面临挑战，如扩展推理步骤、多种约束和应对多个不同子任务的挑战。为应对这些挑战，我们提出HyperTree Planning（HTP）这一新的推理范式，构建用于有效规划的超树结构规划大纲。超树结构使LLM能够通过灵活地采用分而治之的策略进行分层思考，有效地分解复杂的推理步骤，容纳各种约束，并以有序的方式管理多个不同的子任务。我们还引入了一个自主规划框架，通过迭代地细化和扩展超树结构规划大纲来完成规划过程。实验表明HTP的有效性，在TravelPlanner基准测试上使用Gemini-1.5-Pro实现了业界领先精度，相较于o1-preview实现了3.6倍的性能提升。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型（LLM）在处理复杂推理任务方面表现出显著增强的性能。</li>
<li>在数学和逻辑推理等领域，LLM已取得显著成功。</li>
<li>LLM在处理复杂的规划任务时面临挑战，如扩展推理步骤、多种约束和应对多个不同子任务。</li>
<li>提出HyperTree Planning（HTP）这一新的推理范式以应对这些挑战。</li>
<li>HTP利用超树结构进行分层思考，有效分解复杂推理步骤，管理多种约束和多个子任务。</li>
<li>引入自主规划框架完成规划过程，通过迭代优化超树结构规划大纲。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02322">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-85c8f303c9e344a58f81a1749c5d524f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e62f2fbdb5d306912daf110139d1609.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a4d4c3d9cf5dbe68aeaafb6ef9238e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-508404d94a76488f201ecc9fa60753da.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Learning-to-Reason-under-Off-Policy-Guidance"><a href="#Learning-to-Reason-under-Off-Policy-Guidance" class="headerlink" title="Learning to Reason under Off-Policy Guidance"></a>Learning to Reason under Off-Policy Guidance</h2><p><strong>Authors:Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, Yue Zhang</strong></p>
<p>Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning with verifiable rewards~(\textit{RLVR}). However, existing \textit{RLVR} approaches are inherently &#96;&#96;on-policy’’, limiting learning to a model’s own outputs and failing to acquire reasoning abilities beyond its initial capabilities. To address this issue, we introduce \textbf{LUFFY} (\textbf{L}earning to reason \textbf{U}nder o\textbf{FF}-polic\textbf{Y} guidance), a framework that augments \textit{RLVR} with off-policy reasoning traces. LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training. Specifically, LUFFY combines the Mixed-Policy GRPO framework, which has a theoretically guaranteed convergence rate, alongside policy shaping via regularized importance sampling to avoid superficial and rigid imitation during mixed-policy training. Compared with previous RLVR methods, LUFFY achieves an over \textbf{+6.4} average gain across six math benchmarks and an advantage of over \textbf{+6.2} points in out-of-distribution tasks. Most significantly, we show that LUFFY successfully trains weak models in scenarios where on-policy RLVR completely fails. These results provide compelling evidence that LUFFY transcends the fundamental limitations of on-policy RLVR and demonstrates the great potential of utilizing off-policy guidance in RLVR. </p>
<blockquote>
<p>最近的大型推理模型（LRM）的进步表明，通过可验证奖励的强化学习，可以出现多步推理和自我反思等复杂行为（RLVR）。然而，现有的RLVR方法本质上是“基于策略的”，仅限于模型自身的输出，未能获得超出其初始能力的推理能力。为了解决这一问题，我们引入了LUFFY（在偏离策略指导下学习推理），这是一个增强RLVR的框架，配备了偏离策略的推理轨迹。LUFFY通过结合偏离策略的演示和基于策略的滚动输出，在训练过程中实现动态的模仿与探索平衡。具体来说，LUFFY结合了具有理论保证的收敛率的Mixed-Policy GRPO框架，以及通过正则化重要性采样进行策略塑造，以避免混合策略训练过程中的表面和僵化模仿。与之前的RLVR方法相比，LUFFY在六个数学基准测试上平均提高了+6.4分以上，在超出分布的任务中优势超过+6.2分。最重要的是，我们展示了LUFFY在纯基于策略的RLVR完全失败的情况下成功训练弱模型的场景。这些结果提供了强有力的证据表明，LUFFY超越了基于策略的RLVR的根本局限性，并展示了在RLVR中使用偏离策略指导的巨大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14945v4">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>大型推理模型（LRMs）的最新进展表明，通过可验证奖励的强化学习（RLVR）可以产生如多步骤推理和自我反思等复杂行为。然而，现有的RLVR方法本质上是“基于策略的”，仅限于模型自身的输出，无法获得超越初始能力的推理能力。为解决这一问题，我们提出了LUFFY框架，该框架在RLVR中加入了离策略推理轨迹。LUFFY通过结合离策略示范和基于策略的滚动来动态平衡模仿和探索。在六个数学基准测试和跨分布任务中，LUFFY相较于之前的RLVR方法平均提高了+6.4分和在超出分布任务中的优势超过+6.2分。最重要的是，我们在场景模型中展示了LUFFY如何成功地训练弱模型，而在这些场景中基于策略的RLVR完全失效。这表明LUFFY超越了基于策略的RLVR的根本局限性，并展示了在离策略指导中使用强化学习的巨大潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型推理模型（LRMs）可以通过强化学习（RLVR）展现复杂行为，如多步骤推理和自我反思。</li>
<li>现有RLVR方法存在局限性，仅限于模型自身输出，难以获得超越初始能力的推理能力。</li>
<li>LUFFY框架通过结合离策略示范和基于策略的滚动来平衡模仿和探索。</li>
<li>LUFFY实现了在六个数学基准测试和跨分布任务中的显著性能提升。</li>
<li>LUFFY成功训练了弱模型，在基于策略的RLVR完全失效的场景中表现出色。</li>
<li>LUFFY框架具有理论上保证的收敛率，并且可以通过正则化重要性采样进行策略塑造，以避免混合政策训练中的表面和僵化模仿。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14945">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-646c71b1911f2da9f6d26a5a64244332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-294b1e469b3169fe01a5a230e392d7de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a522597f0beaa0d4ec1630f2f1d09bf.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DeepSeek-vs-o3-mini-How-Well-can-Reasoning-LLMs-Evaluate-MT-and-Summarization"><a href="#DeepSeek-vs-o3-mini-How-Well-can-Reasoning-LLMs-Evaluate-MT-and-Summarization" class="headerlink" title="DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and   Summarization?"></a>DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and   Summarization?</h2><p><strong>Authors:Daniil Larionov, Sotaro Takeshita, Ran Zhang, Yanran Chen, Christoph Leiter, Zhipin Wang, Christian Greisinger, Steffen Eger</strong></p>
<p>Reasoning-enabled large language models (LLMs) excel in logical tasks, yet their utility for evaluating natural language generation remains unexplored. This study systematically compares reasoning LLMs with non-reasoning counterparts across machine translation and text summarization evaluation tasks. We evaluate eight models spanning state-of-the-art reasoning models (DeepSeek-R1, OpenAI o3), their distilled variants (8B-70B parameters), and equivalent non-reasoning LLMs. Experiments on WMT23 and SummEval benchmarks reveal architecture and task-dependent benefits: OpenAI o3-mini models show improved performance with increased reasoning on MT, while DeepSeek-R1 and generally underperforms compared to its non-reasoning variant except in summarization consistency evaluation. Correlation analysis demonstrates that reasoning token usage correlates with evaluation quality only in specific models, while almost all models generally allocate more reasoning tokens when identifying more quality issues. Distillation maintains reasonable performance up to 32B parameter models but degrades substantially at 8B scale. This work provides the first assessment of reasoning LLMs for NLG evaluation and comparison to non-reasoning models. We share our code to facilitate further research: <a target="_blank" rel="noopener" href="https://github.com/NL2G/reasoning-eval">https://github.com/NL2G/reasoning-eval</a>. </p>
<blockquote>
<p>具有推理功能的大型语言模型（LLM）在逻辑任务上表现出色，然而它们对于自然语言生成的评估的效用尚未被探索。本研究系统地比较了推理LLM与非推理LLM在机器翻译和文本摘要评估任务上的表现。我们评估了包括最新推理模型（DeepSeek-R1，OpenAI o3）、它们的蒸馏变体（参数范围从8B到70B），以及与它们等效的非推理LLM在内的八个模型。在WMT23和SummEval基准测试上的实验揭示了架构和任务依赖性的好处：OpenAI o3 mini模型在机器翻译方面的性能随着推理能力的提升而增强，而DeepSeek-R1在除了摘要一致性评估之外的任务上通常表现不如其非推理模型。相关性分析表明，推理令牌的使用仅在与特定模型的评估质量相关，而几乎所有模型在识别更多质量问题时都会分配更多的推理令牌。蒸馏技术可以在参数规模达到32B时保持合理的性能，但在8B规模时性能会大幅下降。这项工作首次对用于自然语言生成评估的推理LLM进行了评估，并将其与非推理模型进行了比较。我们分享了我们的代码，以便促进进一步的研究：<a target="_blank" rel="noopener" href="https://github.com/NL2G/reasoning-eval%E3%80%82">https://github.com/NL2G/reasoning-eval。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08120v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了具备推理能力的大型语言模型（LLMs）在自然语言生成评估中的表现。文章系统性地对比了推理LLMs与非推理LLMs在机器翻译和文本摘要评估任务上的差异。实验结果显示，不同模型和任务背景下，推理能力对模型表现的影响有所不同。OpenAI o3-mini模型在机器翻译任务中展现出推理能力的提升，而DeepSeek-R1模型则在摘要一致性评估中表现相对较弱。此外，推理令牌的使用与评价质量的相关性仅在特定模型中体现，且大多数模型在识别质量问题时倾向于使用更多推理令牌。蒸馏技术能在参数规模较小的模型中保持合理性能，但在更大规模参数模型中性能下降明显。本文首次对推理LLMs进行NLG评估并与非推理模型进行比较，相关代码已公开分享，以推动进一步研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>推理LLMs在自然语言生成评估中的表现被系统研究，并与非推理LLMs进行对比。</li>
<li>实验结果显示，不同模型在机器翻译和文本摘要评估任务上，推理能力的影响存在差异。</li>
<li>OpenAI o3-mini模型在机器翻译任务中，推理能力提升明显。</li>
<li>DeepSeek-R1模型在摘要一致性评估中表现相对较弱。</li>
<li>推理令牌的使用与评价质量的相关性仅在特定模型中体现。</li>
<li>大多数模型在识别质量问题时倾向于使用更多推理令牌。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08120">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7048251bd2748b15eeefd8cee4a48eeb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2fda1002c6e988bc23c2399be487b65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2310a1d61a5a13c8c183f4eec908b548.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-02/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-02/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3c64add917f44249521790e958396869.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-06-03  Agent-X Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic   Tasks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-01/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5f65755fd334f8d186705757db0a7c2a.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-06-01  MMGT Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video   Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">19939k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
