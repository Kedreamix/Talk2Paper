<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-02  Benchmarking and Rethinking Knowledge Editing for Large Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-85c8f303c9e344a58f81a1749c5d524f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-03
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-02-æ›´æ–°"><a href="#2025-06-02-æ›´æ–°" class="headerlink" title="2025-06-02 æ›´æ–°"></a>2025-06-02 æ›´æ–°</h1><h2 id="Benchmarking-and-Rethinking-Knowledge-Editing-for-Large-Language-Models"><a href="#Benchmarking-and-Rethinking-Knowledge-Editing-for-Large-Language-Models" class="headerlink" title="Benchmarking and Rethinking Knowledge Editing for Large Language Models"></a>Benchmarking and Rethinking Knowledge Editing for Large Language Models</h2><p><strong>Authors:Guoxiu He, Xin Song, Futing Wang, Aixin Sun</strong></p>
<p>Knowledge editing aims to update the embedded knowledge within Large Language Models (LLMs). However, existing approaches, whether through parameter modification or external memory integration, often suffer from inconsistent evaluation objectives and experimental setups. To address this gap, we conduct a comprehensive benchmarking study. In addition to fact-level datasets, we introduce more complex event-based datasets and general-purpose datasets drawn from other tasks. Our evaluation covers both instruction-tuned and reasoning-oriented LLMs, under a realistic autoregressive inference setting rather than teacher-forced decoding. Beyond single-edit assessments, we also evaluate multi-edit scenarios to better reflect practical demands. We employ four evaluation dimensions, including portability, and compare all recent methods against a simple and straightforward baseline named Selective Contextual Reasoning (SCR). Empirical results reveal that parameter-based editing methods perform poorly under realistic conditions. In contrast, SCR consistently outperforms them across all settings. This study offers new insights into the limitations of current knowledge editing methods and highlights the potential of context-based reasoning as a more robust alternative. </p>
<blockquote>
<p>çŸ¥è¯†ç¼–è¾‘æ—¨åœ¨æ›´æ–°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åµŒå…¥çŸ¥è¯†ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ï¼Œæ— è®ºæ˜¯é€šè¿‡å‚æ•°ä¿®æ”¹è¿˜æ˜¯å¤–éƒ¨è®°å¿†é›†æˆï¼Œç»å¸¸é¢ä¸´è¯„ä¼°ç›®æ ‡ä¸ä¸€è‡´å’Œå®éªŒè®¾ç½®ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å…¨é¢çš„åŸºå‡†æµ‹è¯•ç ”ç©¶ã€‚é™¤äº†äº‹å®çº§çš„æ•°æ®é›†ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†æ›´å¤æ‚çš„äº‹ä»¶åŸºç¡€æ•°æ®é›†å’Œä»å…¶ä»–ä»»åŠ¡ä¸­æå–çš„é€šç”¨æ•°æ®é›†ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ¶µç›–äº†æŒ‡ä»¤è°ƒä¼˜å’Œé¢å‘æ¨ç†çš„LLMï¼Œåœ¨ä¸€ä¸ªç°å®çš„è‡ªå›å½’æ¨ç†è®¾ç½®ä¸‹ï¼Œè€Œä¸æ˜¯æ•™å¸ˆå¼ºåˆ¶è§£ç ã€‚é™¤äº†å•ç¼–è¾‘è¯„ä¼°ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹å¤šç¼–è¾‘åœºæ™¯è¿›è¡Œè¯„ä¼°ï¼Œä»¥æ›´å¥½åœ°åæ˜ å®é™…éœ€æ±‚ã€‚æˆ‘ä»¬é‡‡ç”¨å››ä¸ªè¯„ä¼°ç»´åº¦ï¼ŒåŒ…æ‹¬å¯ç§»æ¤æ€§ï¼Œå¹¶ä¸åä¸ºé€‰æ‹©æ€§ä¸Šä¸‹æ–‡æ¨ç†ï¼ˆSCRï¼‰çš„ç®€å•ç›´æ¥åŸºçº¿å¯¹æ¯”æ‰€æœ‰è¿‘æœŸæ–¹æ³•ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒåŸºäºå‚æ•°çš„ç¼–è¾‘æ–¹æ³•åœ¨å®é™…æƒ…å†µä¸‹çš„è¡¨ç°è¾ƒå·®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒSCRåœ¨æ‰€æœ‰è®¾ç½®ä¸­å‡è¡¨ç°ä¼˜å¼‚ã€‚æœ¬ç ”ç©¶ä¸ºå½“å‰çŸ¥è¯†ç¼–è¾‘æ–¹æ³•çš„å±€é™æ€§æä¾›äº†æ–°çš„è§è§£ï¼Œå¹¶çªå‡ºäº†åŸºäºä¸Šä¸‹æ–‡çš„æ¨ç†ä½œä¸ºæ›´ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18690v1">PDF</a> arXiv admin note: text overlap with arXiv:2503.05212</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„çŸ¥è¯†æ›´æ–°é—®é¢˜ã€‚å½“å‰çš„çŸ¥è¯†ç¼–è¾‘æ–¹æ³•å¦‚å‚æ•°ä¿®æ”¹å’Œå¤–éƒ¨è®°å¿†é›†æˆç­‰å­˜åœ¨è¯„ä»·ç›®æ ‡ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…è¿›è¡Œäº†ä¸€é¡¹å…¨é¢çš„åŸºå‡†æµ‹è¯•ç ”ç©¶ï¼Œå¼•å…¥äº†æ›´å¤æ‚çš„äº‹ä»¶æ•°æ®é›†å’Œé€šç”¨æ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚è¯„ä¼°æ¶µç›–äº†æŒ‡ä»¤è°ƒä¼˜å’Œæ¨ç†å¯¼å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨çœŸå®çš„è‡ªåŠ¨å›å½’æ¨ç†è®¾ç½®è¿›è¡Œè¯„ä¼°ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†å•ç¼–è¾‘å’Œå¤šç¼–è¾‘åœºæ™¯çš„è¯„ä¼°ä»¥æ›´å¥½åœ°åæ˜ å®é™…éœ€æ±‚ã€‚è¯„ä¼°åŒ…æ‹¬ä¾¿æºæ€§åœ¨å†…çš„å››ä¸ªç»´åº¦ï¼Œå¹¶å¯¹æ¯”äº†åä¸ºé€‰æ‹©æ€§ä¸Šä¸‹æ–‡æ¨ç†çš„ç®€å•ç›´è§‚åŸºçº¿æ–¹æ³•ä¸æ‰€æœ‰è¿‘æœŸæ–¹æ³•ã€‚å®è¯ç»“æœè¡¨æ˜ï¼ŒåŸºäºå‚æ•°çš„ç¼–è¾‘æ–¹æ³•åœ¨çœŸå®æ¡ä»¶ä¸‹è¡¨ç°ä¸ä½³ï¼Œè€Œé€‰æ‹©æ€§ä¸Šä¸‹æ–‡æ¨ç†åˆ™å§‹ç»ˆè¡¨ç°ä¼˜å¼‚ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†å½“å‰çŸ¥è¯†ç¼–è¾‘æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶å±•ç¤ºäº†åŸºäºä¸Šä¸‹æ–‡çš„æ¨ç†ä½œä¸ºæ›´ç¨³å¥æ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çŸ¥è¯†ç¼–è¾‘æ—¨åœ¨æ›´æ–°å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„åµŒå…¥çŸ¥è¯†ã€‚</li>
<li>å½“å‰çš„çŸ¥è¯†ç¼–è¾‘æ–¹æ³•å­˜åœ¨è¯„ä»·ç›®æ ‡ä¸ä¸€è‡´çš„é—®é¢˜ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…è¿›è¡Œäº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ç ”ç©¶ã€‚</li>
<li>ç ”ç©¶å¼•å…¥äº†å¤æ‚çš„äº‹ä»¶æ•°æ®é›†å’Œé€šç”¨æ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>è¯„ä¼°è¦†ç›–äº†æŒ‡ä»¤è°ƒä¼˜å’Œæ¨ç†å¯¼å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é€‰æ‹©æ€§ä¸Šä¸‹æ–‡æ¨ç†ï¼ˆSCRï¼‰åœ¨å¤šç§è®¾ç½®ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18690">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e743ed70059baa6054d938e498bff081.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d6a3f8a0800cfdc0c21016299482f99c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Chain-of-Zoom-Extreme-Super-Resolution-via-Scale-Autoregression-and-Preference-Alignment"><a href="#Chain-of-Zoom-Extreme-Super-Resolution-via-Scale-Autoregression-and-Preference-Alignment" class="headerlink" title="Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and   Preference Alignment"></a>Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and   Preference Alignment</h2><p><strong>Authors:Bryan Sangwoo Kim, Jeongsol Kim, Jong Chul Ye</strong></p>
<p>Modern single-image super-resolution (SISR) models deliver photo-realistic results at the scale factors on which they are trained, but collapse when asked to magnify far beyond that regime. We address this scalability bottleneck with Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an autoregressive chain of intermediate scale-states with multi-scale-aware prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the conditional probability into tractable sub-problems to achieve extreme resolutions without additional training. Because visual cues diminish at high magnifications, we augment each zoom step with multi-scale-aware text prompts generated by a vision-language model (VLM). The prompt extractor itself is fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic VLM, aligning text guidance towards human preference. Experiments show that a standard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement with high perceptual quality and fidelity. Project Page: <a target="_blank" rel="noopener" href="https://bryanswkim.github.io/chain-of-zoom/">https://bryanswkim.github.io/chain-of-zoom/</a> . </p>
<blockquote>
<p>ç°ä»£å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰æ¨¡å‹åœ¨å®ƒä»¬æ‰€æ¥å—çš„å°ºåº¦å› ç´ ä¸Šäº§ç”Ÿé€¼çœŸçš„ç»“æœï¼Œä½†å½“è¢«è¦æ±‚æ”¾å¤§è¶…å‡ºè¿™ä¸ªèŒƒå›´æ—¶ï¼Œå®ƒä»¬å°±ä¼šå´©æºƒã€‚æˆ‘ä»¬é‡‡ç”¨Chain-of-Zoomï¼ˆCoZï¼‰æ¥è§£å†³è¿™ç§å¯æ‰©å±•æ€§çš„ç“¶é¢ˆï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„æ¨¡å‹æ¡†æ¶ï¼Œå®ƒå°†SISRåˆ†è§£ä¸ºå…·æœ‰å¤šå°ºåº¦æ„ŸçŸ¥æç¤ºçš„è‡ªå›å½’ä¸­é—´å°ºåº¦çŠ¶æ€é“¾ã€‚CoZé‡å¤åˆ©ç”¨éª¨å¹²ç½‘SRæ¨¡å‹ï¼Œå°†æ¡ä»¶æ¦‚ç‡åˆ†è§£ä¸ºå¯æ§åˆ¶çš„å­é—®é¢˜ï¼Œä»¥å®ç°æé«˜çš„åˆ†è¾¨ç‡è€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚ç”±äºåœ¨é«˜å€æ”¾å¤§æ—¶è§†è§‰çº¿ç´¢ä¼šå‡å°‘ï¼Œæˆ‘ä»¬åˆ©ç”¨ç”±è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”Ÿæˆçš„å¤šå°ºåº¦æ„ŸçŸ¥æ–‡æœ¬æç¤ºæ¥å¢å¼ºæ¯ä¸€æ­¥çš„æ”¾å¤§ã€‚æç¤ºæå–å™¨æœ¬èº«ä½¿ç”¨å…·æœ‰è¯„è®ºå®¶VLMçš„å¹¿ä¹‰å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œå¾®è°ƒï¼Œä½¿æ–‡æœ¬æŒ‡å¯¼ç¬¦åˆäººç±»åå¥½ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨CoZåŒ…è£…çš„æ ‡å‡†4å€æ‰©æ•£SRæ¨¡å‹å¯ä»¥å®ç°è¶…è¿‡256å€çš„æ”¾å¤§ï¼ŒåŒæ—¶ä¿æŒé«˜åº¦çš„æ„ŸçŸ¥è´¨é‡å’Œä¿çœŸåº¦ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://bryanswkim.github.io/chain-of-zoom/%E3%80%82">https://bryanswkim.github.io/chain-of-zoom/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18600v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://bryanswkim.github.io/chain-of-zoom/">https://bryanswkim.github.io/chain-of-zoom/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡é’ˆå¯¹ç°ä»£å•å›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆSISRï¼‰æ¨¡å‹åœ¨æ”¾å¤§å€æ•°è¶…å‡ºè®­ç»ƒè§„æ¨¡æ—¶æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº†Chain-of-Zoomï¼ˆCoZï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†SISRåˆ†è§£ä¸ºä¸­é—´å°ºåº¦çŠ¶æ€çš„è‡ªå›å½’é“¾ï¼Œé€šè¿‡å¤šå°ºåº¦æ„ŸçŸ¥æç¤ºè¿›è¡Œè¶…åˆ†è¾¨ç‡é‡å»ºã€‚CoZé€šè¿‡é‡å¤ä½¿ç”¨éª¨å¹²SRæ¨¡å‹ï¼Œå°†æ¡ä»¶æ¦‚ç‡åˆ†è§£ä¸ºå¯è§£å†³çš„å­é—®é¢˜ï¼Œå®ç°æç«¯åˆ†è¾¨ç‡è€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚åœ¨é«˜å€æ”¾å¤§æ—¶ï¼Œè§†è§‰çº¿ç´¢å‡å°‘ï¼Œå› æ­¤æ¯ä¸ªç¼©æ”¾æ­¥éª¤éƒ½ä¼šå€ŸåŠ©ç”±è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”Ÿæˆçš„å¤šå°ºåº¦æ„ŸçŸ¥æ–‡æœ¬æç¤ºè¿›è¡Œå¢å¼ºã€‚æç¤ºæå–å™¨ä½¿ç”¨å¹¿ä¹‰å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œè¯„è®ºå®¶VLMè¿›è¡Œå¾®è°ƒï¼Œä½¿æ–‡æœ¬æŒ‡å¯¼ç¬¦åˆäººç±»åå¥½ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨CoZåŒ…è£…çš„4å€æ‰©æ•£SRæ¨¡å‹å¯å®ç°è¶…è¿‡256å€çš„æ”¾å¤§ï¼ŒåŒæ—¶ä¿æŒé«˜æ„ŸçŸ¥è´¨é‡å’Œä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Chain-of-Zoomï¼ˆCoZï¼‰æ¡†æ¶è§£å†³äº†ç°ä»£SISRæ¨¡å‹åœ¨è¶…å‡ºè®­ç»ƒè§„æ¨¡æ—¶çš„æ”¾å¤§æ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>CoZé€šè¿‡å°†SISRåˆ†è§£ä¸ºä¸­é—´å°ºåº¦çŠ¶æ€çš„è‡ªå›å½’é“¾ï¼Œåˆ©ç”¨å¤šå°ºåº¦æ„ŸçŸ¥æç¤ºè¿›è¡Œè¶…åˆ†è¾¨ç‡é‡å»ºã€‚</li>
<li>CoZé€šè¿‡é‡å¤ä½¿ç”¨éª¨å¹²SRæ¨¡å‹ï¼Œåˆ†è§£æ¡ä»¶æ¦‚ç‡ä¸ºå­é—®é¢˜ï¼Œå®ç°æç«¯åˆ†è¾¨ç‡æå‡ã€‚</li>
<li>åœ¨é«˜å€æ”¾å¤§æ—¶ï¼Œè§†è§‰çº¿ç´¢å‡å°‘ï¼Œå› æ­¤åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç”Ÿæˆçš„å¤šå°ºåº¦æ„ŸçŸ¥æ–‡æœ¬æç¤ºå¢å¼ºæ¯ä¸ªç¼©æ”¾æ­¥éª¤ã€‚</li>
<li>æ–‡æœ¬æç¤ºæå–å™¨é€šè¿‡å¹¿ä¹‰å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œè¯„è®ºå®¶VLMè¿›è¡Œå¾®è°ƒï¼Œä»¥ç¬¦åˆäººç±»åå¥½ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œä½¿ç”¨CoZçš„SISRæ¨¡å‹åœ¨è¶…è¿‡256å€æ”¾å¤§æ—¶ä»èƒ½ä¿æŒé«˜æ„ŸçŸ¥è´¨é‡å’Œä¿çœŸåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18600">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e1664d627cd3a58127589dd96264e638.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b2e88bd5dbe2deffd776eea323e77dd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-51f25eda034c1a4993df4637c421de37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d46191aa00e089f98dfc98e5135f2663.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Generative-RLHF-V-Learning-Principles-from-Multi-modal-Human-Preference"><a href="#Generative-RLHF-V-Learning-Principles-from-Multi-modal-Human-Preference" class="headerlink" title="Generative RLHF-V: Learning Principles from Multi-modal Human Preference"></a>Generative RLHF-V: Learning Principles from Multi-modal Human Preference</h2><p><strong>Authors:Jiayi Zhou, Jiaming Ji, Boyuan Chen, Jiapeng Sun, Wenqi Chen, Donghai Hong, Sirui Han, Yike Guo, Yaodong Yang</strong></p>
<p>Training multi-modal large language models (MLLMs) that align with human intentions is a long-term challenge. Traditional score-only reward models for alignment suffer from low accuracy, weak generalization, and poor interpretability, blocking the progress of alignment methods, e.g., reinforcement learning from human feedback (RLHF). Generative reward models (GRMs) leverage MLLMsâ€™ intrinsic reasoning capabilities to discriminate pair-wise responses, but their pair-wise paradigm makes it hard to generalize to learnable rewards. We introduce Generative RLHF-V, a novel alignment framework that integrates GRMs with multi-modal RLHF. We propose a two-stage pipeline: $\textbf{multi-modal generative reward modeling from RL}$, where RL guides GRMs to actively capture human intention, then predict the correct pair-wise scores; and $\textbf{RL optimization from grouped comparison}$, which enhances multi-modal RL scoring precision by grouped responses comparison. Experimental results demonstrate that, besides out-of-distribution generalization of RM discrimination, our framework improves 4 MLLMsâ€™ performance across 7 benchmarks by $18.1%$, while the baseline RLHF is only $5.3%$. We further validate that Generative RLHF-V achieves a near-linear improvement with an increasing number of candidate responses. Our code and models can be found at <a target="_blank" rel="noopener" href="https://generative-rlhf-v.github.io/">https://generative-rlhf-v.github.io</a>. </p>
<blockquote>
<p>è®­ç»ƒç¬¦åˆäººç±»æ„å›¾çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ˜¯ä¸€é¡¹é•¿æœŸæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ä»…è¯„åˆ†å¥–åŠ±æ¨¡å‹åœ¨å¯¹é½æ–¹é¢å­˜åœ¨ç²¾åº¦ä½ã€æ³›åŒ–èƒ½åŠ›å¼±å’Œè§£é‡Šæ€§å·®çš„ç¼ºç‚¹ï¼Œé˜»ç¢äº†å¯¹é½æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ï¼‰çš„è¿›æ­¥ã€‚ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGRMsï¼‰åˆ©ç”¨MLLMsçš„å†…åœ¨æ¨ç†èƒ½åŠ›æ¥åŒºåˆ†æˆå¯¹å“åº”ï¼Œä½†å…¶æˆå¯¹èŒƒå¼éš¾ä»¥æ¨å¹¿åˆ°å¯å­¦ä¹ çš„å¥–åŠ±ã€‚æˆ‘ä»¬å¼•å…¥äº†ç”Ÿæˆå¼RLHF-Vï¼Œè¿™æ˜¯ä¸€ç§å°†GRMsä¸å¤šæ¨¡æ€RLHFç›¸ç»“åˆçš„æ–°å‹å¯¹é½æ¡†æ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æµç¨‹ï¼š<strong>åˆ©ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¤šæ¨¡æ€ç”Ÿæˆå¥–åŠ±å»ºæ¨¡</strong>ï¼Œå…¶ä¸­RLæŒ‡å¯¼GRMsä¸»åŠ¨æ•æ‰äººç±»æ„å›¾ï¼Œç„¶åé¢„æµ‹æ­£ç¡®çš„æˆå¯¹åˆ†æ•°ï¼›å’Œ<strong>åŸºäºåˆ†ç»„æ¯”è¾ƒçš„RLä¼˜åŒ–</strong>ï¼Œé€šè¿‡æ¯”è¾ƒåˆ†ç»„å“åº”æ¥æé«˜å¤šæ¨¡æ€RLè¯„åˆ†ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé™¤äº†RMé‰´åˆ«çš„åˆ†å¸ƒå¤–æ¨å¹¿ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨7ä¸ªåŸºå‡†æµ‹è¯•ä¸Šæé«˜äº†4ä¸ªMLLMçš„æ€§èƒ½ï¼Œæé«˜äº†18.1%ï¼Œè€ŒåŸºçº¿RLHFä»…ä¸º5.3%ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥éªŒè¯ï¼Œéšç€å€™é€‰å“åº”æ•°é‡çš„å¢åŠ ï¼Œç”Ÿæˆå¼RLHF-Vå®ç°äº†è¿‘çº¿æ€§çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://generative-rlhf-v.github.io/">https://generative-rlhf-v.github.io</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18531v1">PDF</a> 9 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>è®­ç»ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä»¥ç¬¦åˆäººç±»æ„å›¾æ˜¯ä¸€ä¸ªé•¿æœŸæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ä»…è¯„åˆ†å¥–åŠ±æ¨¡å‹åœ¨å¯¹é½æ–¹é¢å­˜åœ¨å‡†ç¡®æ€§ä½ã€æ³›åŒ–èƒ½åŠ›å¼±å’Œè§£é‡Šæ€§å·®çš„é—®é¢˜ï¼Œé˜»ç¢äº†å¦‚åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ç­‰å¯¹é½æ–¹æ³•çš„è¿›å±•ã€‚æœ¬æ–‡æå‡ºäº†ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGRMsï¼‰ï¼Œåˆ©ç”¨MLLMsçš„å†…åœ¨æ¨ç†èƒ½åŠ›æ¥åŒºåˆ†é…å¯¹å“åº”ï¼Œä½†å…¶é…å¯¹èŒƒå¼éš¾ä»¥æ¨å¹¿åˆ°å¯å­¦ä¹ çš„å¥–åŠ±ã€‚æœ¬æ–‡ä»‹ç»äº†å…¨æ–°çš„å¯¹é½æ¡†æ¶â€”â€”ç”ŸæˆRLHF-Vï¼Œå®ƒå°†GRMsä¸å¤šæ¨¡æ€RLHFç›¸ç»“åˆã€‚æå‡ºäº†ä¸¤é˜¶æ®µç®¡é“ï¼šé¦–å…ˆæ˜¯åŸºäºRLçš„å¤šæ¨¡æ€ç”Ÿæˆå¥–åŠ±å»ºæ¨¡ï¼Œå…¶ä¸­RLå¼•å¯¼GRMsä¸»åŠ¨æ•æ‰äººç±»æ„å›¾ï¼Œç„¶åé¢„æµ‹æ­£ç¡®çš„é…å¯¹åˆ†æ•°ï¼›å…¶æ¬¡æ˜¯åŸºäºåˆ†ç»„æ¯”è¾ƒçš„RLä¼˜åŒ–ï¼Œé€šè¿‡åˆ†ç»„å“åº”æ¯”è¾ƒæé«˜å¤šæ¨¡æ€RLè¯„åˆ†ç²¾åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé™¤äº†RMé‰´åˆ«çš„åˆ†å¸ƒå¤–æ¨å¹¿èƒ½åŠ›å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨7ä¸ªåŸºå‡†æµ‹è¯•ä¸­æé«˜äº†4ä¸ªMLLMçš„æ€§èƒ½è¾¾18.1%ï¼Œè€ŒåŸºçº¿RLHFä»…æé«˜5.3%ã€‚è¿›ä¸€æ­¥éªŒè¯äº†ç”ŸæˆRLHF-Véšç€å€™é€‰å“åº”æ•°é‡çš„å¢åŠ å®ç°äº†è¿‘çº¿æ€§çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®­ç»ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººçš„æ„å›¾å¯¹é½æ˜¯ä¸€ä¸ªé•¿æœŸæŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹åœ¨å¯¹é½æ–¹é¢å­˜åœ¨å±€é™ï¼Œå¦‚å‡†ç¡®æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œè§£é‡Šæ€§æ–¹é¢çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥ç”Ÿæˆå¥–åŠ±æ¨¡å‹ï¼ˆGRMsï¼‰åˆ©ç”¨MLLMsçš„å†…åœ¨æ¨ç†èƒ½åŠ›è¿›è¡Œé…å¯¹å“åº”åŒºåˆ†ã€‚</li>
<li>æå‡ºæ–°çš„å¯¹é½æ¡†æ¶â€”â€”ç”ŸæˆRLHF-Vï¼Œç»“åˆGRMså’Œå¤šæ¨¡æ€RLHFã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…å«ä¸¤é˜¶æ®µï¼šåŸºäºRLçš„å¥–åŠ±å»ºæ¨¡å’ŒåŸºäºåˆ†ç»„æ¯”è¾ƒçš„RLä¼˜åŒ–ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¡†æ¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—æé«˜MLLMæ€§èƒ½ã€‚</li>
<li>éšç€å€™é€‰å“åº”æ•°é‡çš„å¢åŠ ï¼Œè¯¥æ¡†æ¶å®ç°è¿‘çº¿æ€§çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8eab29339b0510ec8a977cc0be91f513.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29e3f704dae9bcc57b355a02c77312c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4b486715a2a8505be852377172654986.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-544674add46379b8de486ff7cbcae674.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f10fc16fabc00df82a7336ad79187815.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Towards-Revealing-the-Effectiveness-of-Small-Scale-Fine-tuning-in-R1-style-Reinforcement-Learning"><a href="#Towards-Revealing-the-Effectiveness-of-Small-Scale-Fine-tuning-in-R1-style-Reinforcement-Learning" class="headerlink" title="Towards Revealing the Effectiveness of Small-Scale Fine-tuning in   R1-style Reinforcement Learning"></a>Towards Revealing the Effectiveness of Small-Scale Fine-tuning in   R1-style Reinforcement Learning</h2><p><strong>Authors:Yutong Chen, Jiandong Gao, Ji Wu</strong></p>
<p>R1-style Reinforcement Learning (RL) significantly enhances Large Language Modelsâ€™ reasoning capabilities, yet the mechanism behind rule-based RL remains unclear. We found that small-scale SFT has significant influence on RL but shows poor efficiency. To explain our observations, we propose an analytical framework and compare the efficiency of SFT and RL by measuring sample effect. Hypothetical analysis show that SFT efficiency is limited by training data. Guided by our analysis, we propose Re-distillation, a technique that fine-tunes pretrain model through small-scale distillation from the RL-trained policy. Experiments on Knight &amp; Knave and MATH datasets demonstrate re-distillationâ€™s surprising efficiency: re-distilled models match RL performance with far fewer samples and less computation. Empirical verification shows that sample effect is a good indicator of performance improvements. As a result, on K&amp;K dataset, our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches its instruct-tuned variant without RL. Our work explains several interesting phenomena in R1-style RL, shedding light on the mechanisms behind its empirical success. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/on1262/deep-reasoning">https://github.com/on1262/deep-reasoning</a> </p>
<blockquote>
<p>R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶èƒŒåçš„æœºåˆ¶ä»ç„¶ä¸æ˜ç¡®ã€‚æˆ‘ä»¬å‘ç°å°è§„æ¨¡SFTå¯¹RLæœ‰å¾ˆå¤§çš„å½±å“ï¼Œä½†æ•ˆç‡ä¸é«˜ã€‚ä¸ºäº†è§£é‡Šæˆ‘ä»¬çš„è§‚å¯Ÿç»“æœï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶ï¼Œå¹¶é€šè¿‡æµ‹é‡æ ·æœ¬æ•ˆåº”æ¥æ¯”è¾ƒSFTå’ŒRLçš„æ•ˆç‡ã€‚å‡è®¾åˆ†æè¡¨æ˜ï¼ŒSFTçš„æ•ˆç‡å—åˆ°è®­ç»ƒæ•°æ®çš„é™åˆ¶ã€‚åœ¨æˆ‘ä»¬çš„åˆ†ææŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†å†è’¸é¦æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡å°è§„æ¨¡è’¸é¦ä»RLè®­ç»ƒçš„å†³ç­–ä¸­å­¦ä¹ å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ã€‚åœ¨Knight &amp; Knaveå’ŒMATHæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†å†è’¸é¦çš„æƒŠäººæ•ˆç‡ï¼šå†è’¸é¦æ¨¡å‹ä½¿ç”¨è¾ƒå°‘çš„æ ·æœ¬å’Œè®¡ç®—é‡å°±èƒ½è¾¾åˆ°RLçš„æ€§èƒ½ã€‚ç»éªŒéªŒè¯è¡¨æ˜ï¼Œæ ·æœ¬æ•ˆåº”æ˜¯æ€§èƒ½æ”¹è¿›çš„è‰¯å¥½æŒ‡æ ‡ã€‚å› æ­¤ï¼Œåœ¨K&amp;Kæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„å†è’¸é¦Qwen2.5-1.5Bæ¨¡å‹ä»…ä½¿ç”¨1Kä¸ªSFTæ ·æœ¬å°±è¶…è¿‡äº†DeepSeek-V3-0324ã€‚åœ¨MATHä¸Šï¼Œä½¿ç”¨å†è’¸é¦çš„500ä¸ªæ ·æœ¬å¾®è°ƒQwen2.5-1.5Bæ¨¡å‹å¯ä»¥ä¸æ²¡æœ‰RLçš„æŒ‡ä»¤è°ƒæ•´å˜ä½“ç›¸åŒ¹é…ã€‚æˆ‘ä»¬çš„å·¥ä½œè§£é‡Šäº†R1é£æ ¼RLçš„å‡ ä¸ªæœ‰è¶£ç°è±¡ï¼Œæ­ç¤ºäº†å…¶ç»éªŒæˆåŠŸçš„æœºåˆ¶ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/on1262/deep-reasoning">https://github.com/on1262/deep-reasoning</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17988v2">PDF</a> 11 figs, 3 table, preprint</p>
<p><strong>Summary</strong></p>
<p>R1é£æ ¼å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶èƒŒåçš„æœºåˆ¶å°šä¸æ¸…æ¥šã€‚ç ”ç©¶å‘ç°å°è§„æ¨¡SFTå¯¹RLæœ‰å¾ˆå¤§å½±å“ä½†æ•ˆç‡è¾ƒä½ã€‚ä¸ºè§£é‡Šè§‚å¯Ÿç»“æœï¼Œæœ¬æ–‡æå‡ºåˆ†ææ¡†æ¶ï¼Œé€šè¿‡æµ‹é‡æ ·æœ¬æ•ˆåº”æ¯”è¾ƒSFTå’ŒRLçš„æ•ˆç‡ã€‚å‡è®¾åˆ†ææ˜¾ç¤ºSFTæ•ˆç‡å—é™äºè®­ç»ƒæ•°æ®ã€‚åŸºäºåˆ†æï¼Œæœ¬æ–‡æå‡ºå†è’¸é¦æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é€šè¿‡æ¥è‡ªRLè®­ç»ƒç­–ç•¥çš„å°‘é‡æ ·æœ¬å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨Knight &amp; Knaveå’ŒMATHæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºå†è’¸é¦çš„æƒŠäººæ•ˆç‡ï¼šå†è’¸é¦æ¨¡å‹ç”¨æ›´å°‘çš„æ ·æœ¬å’Œè®¡ç®—é‡åŒ¹é…RLæ€§èƒ½ã€‚ç»éªŒéªŒè¯æ˜¾ç¤ºæ ·æœ¬æ•ˆåº”æ˜¯æ€§èƒ½æ”¹è¿›çš„è‰¯å¥½æŒ‡æ ‡ã€‚å› æ­¤ï¼Œåœ¨K&amp;Kæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„å†è’¸é¦Qwen2.5-1.5Bæ¨¡å‹ä»…ä½¿ç”¨1KSFTæ ·æœ¬å°±è¶…è¶Šäº†DeepSeek-V3-0324ã€‚åœ¨MATHä¸Šï¼ŒQwen2.5-1.5Bä½¿ç”¨å†è’¸é¦çš„500ä¸ªæ ·æœ¬è¿›è¡Œå¾®è°ƒï¼ŒåŒ¹é…äº†æ²¡æœ‰RLçš„æŒ‡ä»¤è°ƒä¼˜å˜ä½“ã€‚æœ¬ç ”ç©¶è§£é‡Šäº†R1é£æ ¼RLä¸­çš„å‡ ä¸ªæœ‰è¶£ç°è±¡ï¼Œæ­ç¤ºäº†å…¶ç»éªŒæˆåŠŸçš„æœºåˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>R1-style Reinforcement Learningæ˜¾è‘—å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†æœºåˆ¶å°šä¸æ¸…æ¥šã€‚</li>
<li>å°è§„æ¨¡SFTå¯¹RLæœ‰å½±å“ï¼Œä½†æ•ˆç‡è¾ƒä½ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶æ¥è§£é‡Šè¿™ä¸€è§‚å¯Ÿç»“æœï¼Œé€šè¿‡æµ‹é‡æ ·æœ¬æ•ˆåº”æ¥æ¯”è¾ƒSFTå’ŒRLçš„æ•ˆç‡ã€‚</li>
<li>å‡è®¾åˆ†ææ˜¾ç¤ºï¼ŒSFTæ•ˆç‡å—é™äºè®­ç»ƒæ•°æ®ã€‚</li>
<li>æå‡ºå†è’¸é¦æŠ€æœ¯ï¼Œèƒ½æœ‰æ•ˆæé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†å†è’¸é¦æŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-da9dc98994f59ee1c05654847fe410ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bebbc683fc0f91d247784da178d4c77.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-36a95e010316bfec1d4560b03457c0d4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Plan-R1-Safe-and-Feasible-Trajectory-Planning-as-Language-Modeling"><a href="#Plan-R1-Safe-and-Feasible-Trajectory-Planning-as-Language-Modeling" class="headerlink" title="Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling"></a>Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling</h2><p><strong>Authors:Xiaolong Tang, Meina Kan, Shiguang Shan, Xilin Chen</strong></p>
<p>Safe and feasible trajectory planning is essential for real-world autonomous driving systems. However, existing learning-based planning methods often rely on expert demonstrations, which not only lack explicit safety awareness but also risk inheriting unsafe behaviors such as speeding from suboptimal human driving data. Inspired by the success of large language models, we propose Plan-R1, a novel two-stage trajectory planning framework that formulates trajectory planning as a sequential prediction task, guided by explicit planning principles such as safety, comfort, and traffic rule compliance. In the first stage, we train an autoregressive trajectory predictor via next motion token prediction on expert data. In the second stage, we design rule-based rewards (e.g., collision avoidance, speed limits) and fine-tune the model using Group Relative Policy Optimization (GRPO), a reinforcement learning strategy, to align its predictions with these planning principles. Experiments on the nuPlan benchmark demonstrate that our Plan-R1 significantly improves planning safety and feasibility, achieving state-of-the-art performance. Our code will be made public soon. </p>
<blockquote>
<p>å®‰å…¨å¯è¡Œçš„è½¨è¿¹è§„åˆ’å¯¹äºçœŸå®ä¸–ç•Œçš„è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºå­¦ä¹ çš„è§„åˆ’æ–¹æ³•å¾€å¾€ä¾èµ–äºä¸“å®¶æ¼”ç¤ºï¼Œè¿™ä¸ä»…ç¼ºä¹æ˜ç¡®çš„å®‰å…¨æ„è¯†ï¼Œè€Œä¸”è¿˜å­˜åœ¨ä»æ¬¡ä¼˜äººç±»é©¾é©¶æ•°æ®ä¸­ç»§æ‰¿ä¸å®‰å…¨è¡Œä¸ºï¼ˆå¦‚è¶…é€Ÿï¼‰çš„é£é™©ã€‚å—å¤§å‹è¯­è¨€æ¨¡å‹æˆåŠŸçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†Plan-R1ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µè½¨è¿¹è§„åˆ’æ¡†æ¶ï¼Œå®ƒå°†è½¨è¿¹è§„åˆ’åˆ¶å®šä¸ºå—æ˜ç¡®è§„åˆ’åŸåˆ™ï¼ˆå¦‚å®‰å…¨ã€èˆ’é€‚å’Œéµå®ˆäº¤é€šè§„åˆ™ï¼‰æŒ‡å¯¼çš„åºåˆ—é¢„æµ‹ä»»åŠ¡ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬é€šè¿‡ä¸“å®¶æ•°æ®é¢„æµ‹ä¸‹ä¸€ä¸ªåŠ¨ä½œæ ‡è®°æ¥è®­ç»ƒä¸€ä¸ªè‡ªå›å½’è½¨è¿¹é¢„æµ‹å™¨ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬è®¾è®¡åŸºäºè§„åˆ™çš„å¥–åŠ±ï¼ˆä¾‹å¦‚é˜²æ’ã€é™é€Ÿç­‰ï¼‰ï¼Œå¹¶ä½¿ç”¨ä¸€ç§å¼ºåŒ–å­¦ä¹ ç­–ç•¥â€”â€”ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é¢„æµ‹ä¸è¿™äº›è§„åˆ’åŸåˆ™ä¿æŒä¸€è‡´ã€‚åœ¨nuPlanåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„Plan-R1åœ¨è§„åˆ’å®‰å…¨æ€§å’Œå¯è¡Œæ€§æ–¹é¢æœ‰äº†æ˜¾è‘—æé«˜ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¾ˆå¿«å°†å…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17659v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºPlan-R1çš„æ–°å‹ä¸¤é˜¶æ®µè½¨è¿¹è§„åˆ’æ¡†æ¶ï¼Œç”¨äºè‡ªä¸»é©¾é©¶ç³»ç»Ÿçš„å®‰å…¨å¯è¡Œè½¨è¿¹è§„åˆ’ã€‚è¯¥æ¡†æ¶å°†è½¨è¿¹è§„åˆ’åˆ¶å®šä¸ºå—æ˜ç¡®è§„åˆ’åŸåˆ™ï¼ˆå¦‚å®‰å…¨ã€èˆ’é€‚å’Œäº¤é€šè§„åˆ™éµå®ˆï¼‰æŒ‡å¯¼çš„åºåˆ—é¢„æµ‹ä»»åŠ¡ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡ä¸“å®¶æ•°æ®è®­ç»ƒè‡ªå›å½’è½¨è¿¹é¢„æµ‹å™¨ï¼Œç¬¬äºŒé˜¶æ®µä½¿ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±å’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é¢„æµ‹ä¸è§„åˆ’åŸåˆ™ä¿æŒä¸€è‡´ã€‚åœ¨nuPlanåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPlan-R1åœ¨è§„åˆ’å®‰å…¨æ€§å’Œå¯è¡Œæ€§æ–¹é¢æ˜¾è‘—æé«˜ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªä¸»é©¾é©¶ç³»ç»Ÿä¸­çš„è½¨è¿¹è§„åˆ’å¿…é¡»å®‰å…¨å¯è¡Œã€‚</li>
<li>ç°æœ‰å­¦ä¹ è§„åˆ’æ–¹æ³•ä¾èµ–ä¸“å®¶æ¼”ç¤ºï¼Œç¼ºä¹æ˜ç¡®çš„å®‰å…¨æ„è¯†ï¼Œå¹¶å¯èƒ½ç»§æ‰¿ä¸å®‰å…¨è¡Œä¸ºã€‚</li>
<li>Plan-R1æ¡†æ¶å°†è½¨è¿¹è§„åˆ’åˆ¶å®šä¸ºåºåˆ—é¢„æµ‹ä»»åŠ¡ï¼Œå—æ˜ç¡®è§„åˆ’åŸåˆ™ï¼ˆå¦‚å®‰å…¨ã€èˆ’é€‚å’Œäº¤é€šè§„åˆ™éµå®ˆï¼‰æŒ‡å¯¼ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡ä¸“å®¶æ•°æ®è®­ç»ƒè‡ªå›å½’è½¨è¿¹é¢„æµ‹å™¨ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µä½¿ç”¨åŸºäºè§„åˆ™çš„å¥–åŠ±å’Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼ˆå¦‚ç¢°æ’é¿å…ã€é€Ÿåº¦é™åˆ¶ï¼‰å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚</li>
<li>åœ¨nuPlanåŸºå‡†æµ‹è¯•ä¸Šï¼ŒPlan-R1è¡¨ç°å‡ºæ˜¾è‘—æé«˜çš„å®‰å…¨æ€§å’Œå¯è¡Œæ€§ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17659">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8c53dc38336c4e9f60779412eac22c14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cff5665eea698fa546faf332f6308ab0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Rethinking-the-Sampling-Criteria-in-Reinforcement-Learning-for-LLM-Reasoning-A-Competence-Difficulty-Alignment-Perspective"><a href="#Rethinking-the-Sampling-Criteria-in-Reinforcement-Learning-for-LLM-Reasoning-A-Competence-Difficulty-Alignment-Perspective" class="headerlink" title="Rethinking the Sampling Criteria in Reinforcement Learning for LLM   Reasoning: A Competence-Difficulty Alignment Perspective"></a>Rethinking the Sampling Criteria in Reinforcement Learning for LLM   Reasoning: A Competence-Difficulty Alignment Perspective</h2><p><strong>Authors:Deyang Kong, Qi Guo, Xiangyu Xi, Wei Wang, Jingang Wang, Xunliang Cai, Shikun Zhang, Wei Ye</strong></p>
<p>Reinforcement learning exhibits potential in enhancing the reasoning abilities of large language models, yet it is hard to scale for the low sample efficiency during the rollout phase. Existing methods attempt to improve efficiency by scheduling problems based on problem difficulties. However, these approaches suffer from unstable and biased estimations of problem difficulty and fail to capture the alignment between model competence and problem difficulty in RL training, leading to suboptimal results. To tackle these limitations, this paper introduces $\textbf{C}$ompetence-$\textbf{D}$ifficulty $\textbf{A}$lignment $\textbf{S}$ampling ($\textbf{CDAS}$), which enables accurate and stable estimation of problem difficulties by aggregating historical performance discrepancies of problems. Then the model competence is quantified to adaptively select problems whose difficulty is in alignment with the modelâ€™s current competence using a fixed-point system. Experimental results across a range of challenging mathematical benchmarks show that CDAS achieves great improvements in both accuracy and efficiency. CDAS attains the highest average accuracy against baselines and exhibits significant speed advantages compared to Dynamic Sampling, a competitive strategy in DAPO, which is 2.33 times slower than CDAS. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†åœ¨ rollout é˜¶æ®µå­˜åœ¨æ ·æœ¬æ•ˆç‡ä½çš„é—®é¢˜ï¼Œéš¾ä»¥æ‰©å±•ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡æ ¹æ®é—®é¢˜éš¾åº¦è°ƒåº¦é—®é¢˜æ¥æé«˜æ•ˆç‡ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å—åˆ°é—®é¢˜éš¾åº¦ä¼°è®¡ä¸ç¨³å®šå’Œåè§çš„å½±å“ï¼Œæ— æ³•æ•æ‰æ¨¡å‹èƒ½åŠ›å’Œé—®é¢˜éš¾åº¦åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸­çš„å¯¹é½æƒ…å†µï¼Œä»è€Œå¯¼è‡´ç»“æœä¸ç†æƒ³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†èƒ½åŠ›-éš¾åº¦å¯¹é½é‡‡æ ·ï¼ˆCDASï¼‰ï¼Œé€šè¿‡èšåˆé—®é¢˜çš„å†å²æ€§èƒ½å·®å¼‚ï¼Œå®ç°å‡†ç¡®ç¨³å®šçš„é—®é¢˜éš¾åº¦ä¼°è®¡ã€‚ç„¶åï¼Œé€šè¿‡å›ºå®šç‚¹ç³»ç»Ÿé‡åŒ–æ¨¡å‹èƒ½åŠ›ï¼Œè‡ªé€‚åº”é€‰æ‹©éš¾åº¦ä¸æ¨¡å‹å½“å‰èƒ½åŠ›ç›¸åŒ¹é…çš„é—®é¢˜ã€‚åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCDAS åœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡æ–¹é¢å–å¾—äº†å·¨å¤§çš„æ”¹è¿›ã€‚CDAS åœ¨å¹³å‡å‡†ç¡®ç‡æ–¹é¢è¾¾åˆ°äº†æœ€é«˜æ°´å¹³ï¼Œä¸åŸºçº¿ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶ä¸”åœ¨ä¸ DAPO ä¸­çš„åŠ¨æ€é‡‡æ ·ç­–ç•¥ç›¸æ¯”æ—¶è¡¨ç°å‡ºæ˜æ˜¾çš„é€Ÿåº¦ä¼˜åŠ¿ï¼Œåè€…æ˜¯ CDAS çš„ 2.33 å€æ…¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17652v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰æ½œåŠ›ï¼Œä½†å…¶åœ¨rollouté˜¶æ®µçš„æ ·æœ¬æ•ˆç‡è¾ƒä½ï¼Œéš¾ä»¥æ‰©å±•ã€‚ç°æœ‰æ–¹æ³•è¯•å›¾é€šè¿‡æ ¹æ®é—®é¢˜éš¾åº¦è¿›è¡Œè°ƒåº¦æ¥æé«˜æ•ˆç‡ï¼Œä½†å­˜åœ¨å¯¹é—®é¢˜éš¾åº¦çš„ä¸ç¨³å®šã€æœ‰åä¼°è®¡ï¼Œä»¥åŠæ— æ³•æ•æ‰æ¨¡å‹èƒ½åŠ›ä¸é—®é¢˜éš¾åº¦åœ¨RLè®­ç»ƒä¸­çš„å¯¹é½å…³ç³»ï¼Œå¯¼è‡´ç»“æœä¸å°½å¦‚äººæ„ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ç§Competence-Difficulty Alignment Samplingï¼ˆCDASï¼‰æ–¹æ³•ã€‚å®ƒå¯ä»¥é€šè¿‡æ±‡èšé—®é¢˜çš„å†å²è¡¨ç°å·®å¼‚æ¥å‡†ç¡®ç¨³å®šåœ°ä¼°è®¡é—®é¢˜éš¾åº¦ã€‚éšååˆ©ç”¨ä¸€ä¸ªå®šç‚¹ç³»ç»Ÿé‡åŒ–æ¨¡å‹èƒ½åŠ›ï¼Œè‡ªé€‚åº”é€‰æ‹©éš¾åº¦ä¸æ¨¡å‹å½“å‰èƒ½åŠ›ç›¸åŒ¹é…çš„é—®é¢˜ã€‚åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§çš„æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒCDASåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¸Šéƒ½å–å¾—äº†å·¨å¤§çš„æå‡ã€‚ç›¸è¾ƒäºåŠ¨æ€é‡‡æ ·ç­‰ç«äº‰æ€§ç­–ç•¥ï¼ŒCDASè¾¾åˆ°äº†æœ€é«˜çš„å¹³å‡å‡†ç¡®ç‡ï¼Œå¹¶ä¸”åœ¨æ•ˆç‡ä¸Šå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¯¹äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å…·æœ‰æ½œåŠ›ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­é¢ä¸´æ ·æœ¬æ•ˆç‡ä½çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡é—®é¢˜éš¾åº¦è°ƒåº¦æé«˜æ•ˆç‡ï¼Œä½†å­˜åœ¨å¯¹é—®é¢˜éš¾åº¦çš„ä¸ç¨³å®šã€æœ‰åä¼°è®¡ã€‚</li>
<li>CDASæ–¹æ³•å¯ä»¥å‡†ç¡®ç¨³å®šåœ°ä¼°è®¡é—®é¢˜éš¾åº¦ï¼ŒåŸºäºæ¨¡å‹èƒ½åŠ›çš„é‡åŒ–æ¥é‡‡æ ·é—®é¢˜ã€‚</li>
<li>CDASé€šè¿‡è‡ªé€‚åº”é€‰æ‹©éš¾åº¦ä¸æ¨¡å‹å½“å‰èƒ½åŠ›ç›¸åŒ¹é…çš„é—®é¢˜ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šï¼ŒCDASå–å¾—äº†æœ€é«˜çš„å¹³å‡å‡†ç¡®ç‡ã€‚</li>
<li>ä¸å…¶ä»–é‡‡æ ·ç­–ç•¥ç›¸æ¯”ï¼ŒCDASåœ¨æ•ˆç‡ä¸Šå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-437e6107e425b772e36af78a19400ccf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c71e91357f2baf6b63956cd48b4745a1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Pixel-Reasoner-Incentivizing-Pixel-Space-Reasoning-with-Curiosity-Driven-Reinforcement-Learning"><a href="#Pixel-Reasoner-Incentivizing-Pixel-Space-Reasoning-with-Curiosity-Driven-Reinforcement-Learning" class="headerlink" title="Pixel Reasoner: Incentivizing Pixel-Space Reasoning with   Curiosity-Driven Reinforcement Learning"></a>Pixel Reasoner: Incentivizing Pixel-Space Reasoning with   Curiosity-Driven Reinforcement Learning</h2><p><strong>Authors:Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, Wenhu Chen</strong></p>
<p>Chain-of-thought reasoning has significantly improved the performance of Large Language Models (LLMs) across various domains. However, this reasoning process has been confined exclusively to textual space, limiting its effectiveness in visually intensive tasks. To address this limitation, we introduce the concept of reasoning in the pixel-space. Within this novel framework, Vision-Language Models (VLMs) are equipped with a suite of visual reasoning operations, such as zoom-in and select-frame. These operations enable VLMs to directly inspect, interrogate, and infer from visual evidences, thereby enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space reasoning capabilities in VLMs presents notable challenges, including the modelâ€™s initially imbalanced competence and its reluctance to adopt the newly introduced pixel-space operations. We address these challenges through a two-phase training approach. The first phase employs instruction tuning on synthesized reasoning traces to familiarize the model with the novel visual operations. Following this, a reinforcement learning (RL) phase leverages a curiosity-driven reward scheme to balance exploration between pixel-space reasoning and textual reasoning. With these visual operations, VLMs can interact with complex visual inputs, such as information-rich images or videos to proactively gather necessary information. We demonstrate that this approach significantly improves VLM performance across diverse visual reasoning benchmarks. Our 7B model, \model, achieves 84% on V* bench, 74% on TallyQA-Complex, and 84% on InfographicsVQA, marking the highest accuracy achieved by any open-source model to date. These results highlight the importance of pixel-space reasoning and the effectiveness of our framework. </p>
<blockquote>
<p>é“¾å¼æ€ç»´æ¨ç†å·²ç»æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒé¢†åŸŸçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ç§æ¨ç†è¿‡ç¨‹ä¸€ç›´è¢«é™åˆ¶åœ¨æ–‡æœ¬ç©ºé—´å†…ï¼Œä½¿å…¶åœ¨è§†è§‰å¯†é›†å‹ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å—åˆ°é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åƒç´ ç©ºé—´æ¨ç†çš„æ¦‚å¿µã€‚åœ¨è¿™ä¸€æ–°é¢–æ¡†æ¶ä¸‹ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é…å¤‡äº†ä¸€ç³»åˆ—è§†è§‰æ¨ç†æ“ä½œï¼Œä¾‹å¦‚æ”¾å¤§å’Œé€‰æ‹©å¸§ã€‚è¿™äº›æ“ä½œä½¿VLMèƒ½å¤Ÿç›´æ¥æ£€æŸ¥ã€è´¨è¯¢å’Œä»è§†è§‰è¯æ®ä¸­è¿›è¡Œæ¨æ–­ï¼Œä»è€Œæé«˜è§†è§‰ä»»åŠ¡çš„æ¨ç†ä¿çœŸåº¦ã€‚åœ¨VLMä¸­åŸ¹å…»è¿™ç§åƒç´ ç©ºé—´æ¨ç†èƒ½åŠ›é¢ä¸´ç€æ˜¾è‘—æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„åˆå§‹èƒ½åŠ›ä¸å¹³è¡¡åŠå…¶å¯¹æ–°å¼•å…¥çš„åƒç´ ç©ºé—´æ“ä½œçš„æŠµè§¦æƒ…ç»ªã€‚æˆ‘ä»¬é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚ç¬¬ä¸€é˜¶æ®µé‡‡ç”¨åˆæˆæ¨ç†è½¨è¿¹çš„æŒ‡ä»¤è°ƒæ•´ï¼Œä½¿æ¨¡å‹ç†Ÿæ‚‰æ–°å‹è§†è§‰æ“ä½œã€‚ä¹‹åï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µåˆ©ç”¨åŸºäºå¥½å¥‡å¿ƒçš„å¥–åŠ±æ–¹æ¡ˆæ¥å¹³è¡¡åƒç´ ç©ºé—´æ¨ç†å’Œæ–‡æœ¬æ¨ç†ä¹‹é—´çš„æ¢ç´¢ã€‚å€ŸåŠ©è¿™äº›è§†è§‰æ“ä½œï¼ŒVLMå¯ä»¥ä¸å¤æ‚çš„è§†è§‰è¾“å…¥ï¼ˆå¦‚ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæˆ–è§†é¢‘ï¼‰è¿›è¡Œäº¤äº’ï¼Œä»¥ä¸»åŠ¨æ”¶é›†å¿…è¦ä¿¡æ¯ã€‚æˆ‘ä»¬è¯æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†VLMåœ¨å¤šç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„7Bæ¨¡å‹å®ç°äº†V*æµ‹è¯•å¹³å°ä¸Šçš„84%ã€TallyQA-Complexä¸Šçš„74%å’ŒInfographicsVQAä¸Šçš„84%ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢ä»»ä½•å¼€æºæ¨¡å‹æ‰€å–å¾—çš„æœ€é«˜ç²¾åº¦ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åƒç´ ç©ºé—´æ¨ç†çš„é‡è¦æ€§ä»¥åŠæˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15966v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://tiger-ai-lab.github.io/Pixel-Reasoner/">https://tiger-ai-lab.github.io/Pixel-Reasoner/</a>,   Hands-on Demo: <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/TIGER-Lab/Pixel-Reasoner">https://huggingface.co/spaces/TIGER-Lab/Pixel-Reasoner</a></p>
<p><strong>æ‘˜è¦</strong><br>è§†è§‰ç©ºé—´æ¨ç†çš„å¼•å…¥æ˜¾è‘—æå‡äº†è·¨åŸŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç›®å‰çš„æ¨ç†è¿‡ç¨‹ä¸»è¦å±€é™äºæ–‡æœ¬ç©ºé—´ï¼Œè¿™åœ¨è§†è§‰ä»»åŠ¡ä¸­é™åˆ¶äº†å…¶æœ‰æ•ˆæ€§ã€‚ä¸ºè§£å†³æ­¤å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åƒç´ ç©ºé—´æ¨ç†çš„æ¦‚å¿µã€‚åœ¨æ­¤æ–°æ¡†æ¶ä¸‹ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é…å¤‡äº†ä¸€ç³»åˆ—è§†è§‰æ¨ç†æ“ä½œï¼Œå¦‚ç¼©æ”¾ã€é€‰æ‹©å¸§ç­‰ã€‚è¿™äº›æ“ä½œä½¿VLMèƒ½å¤Ÿç›´æ¥ä»è§†è§‰è¯æ®ä¸­æ£€æŸ¥ã€è´¨è¯¢å’Œæ¨æ–­ï¼Œä»è€Œæé«˜è§†è§‰ä»»åŠ¡çš„æ¨ç†ä¿çœŸåº¦ã€‚åœ¨VLMä¸­åŸ¹å…»è¿™ç§åƒç´ ç©ºé—´æ¨ç†èƒ½åŠ›é¢ä¸´ç€æ˜¾è‘—æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡å‹åˆå§‹èƒ½åŠ›çš„ä¸å¹³è¡¡ä»¥åŠå¯¹æ–°å¼•å…¥çš„åƒç´ ç©ºé—´æ“ä½œçš„æ¥å—åº¦ä½ã€‚æˆ‘ä»¬é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚ç¬¬ä¸€é˜¶æ®µé€šè¿‡åˆæˆæ¨ç†è½¨è¿¹è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä½¿æ¨¡å‹ç†Ÿæ‚‰æ–°çš„è§†è§‰æ“ä½œã€‚æ¥ä¸‹æ¥ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é˜¶æ®µåˆ©ç”¨å¥½å¥‡é©±åŠ¨çš„å¥–åŠ±æ–¹æ¡ˆæ¥å¹³è¡¡åƒç´ ç©ºé—´æ¨ç†å’Œæ–‡æœ¬æ¨ç†ä¹‹é—´çš„æ¢ç´¢ã€‚å€ŸåŠ©è¿™äº›è§†è§‰æ“ä½œï¼ŒVLMå¯ä»¥ä¸å¤æ‚è§†è§‰è¾“å…¥ï¼ˆå¦‚ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæˆ–è§†é¢‘ï¼‰è¿›è¡Œäº¤äº’ï¼Œä¸»åŠ¨æ”¶é›†å¿…è¦ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—æé«˜äº†VLMåœ¨å¤šç§è§†è§‰æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„7Bæ¨¡å‹åœ¨V*åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°84%ã€TallyQA-Complexä¸Šè¾¾åˆ°74%ã€InfographicsVQAä¸Šè¾¾åˆ°84%ï¼Œæˆä¸ºè¿„ä»Šä¸ºæ­¢å…¬å¼€æ¨¡å‹ä¸­å‡†ç¡®æ€§æœ€é«˜çš„ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åƒç´ ç©ºé—´æ¨ç†çš„é‡è¦æ€§åŠæˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é“¾å¼æ€ç»´æ¨ç†å·²æ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸçš„æ€§èƒ½ã€‚</li>
<li>å½“å‰æ¨ç†è¿‡ç¨‹ä¸»è¦å±€é™äºæ–‡æœ¬ç©ºé—´ï¼Œå¯¹è§†è§‰ä»»åŠ¡çš„æœ‰æ•ˆæ€§æœ‰é™ã€‚</li>
<li>å¼•å…¥åƒç´ ç©ºé—´æ¨ç†æ¦‚å¿µä»¥è§£å†³æ­¤å±€é™æ€§ï¼Œé…å¤‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä»¥è¿›è¡Œè§†è§‰æ¨ç†æ“ä½œã€‚</li>
<li>é¢ä¸´åŸ¹å…»åƒç´ ç©ºé—´æ¨ç†èƒ½åŠ›çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡å‹åˆå§‹èƒ½åŠ›ä¸å¹³è¡¡å’Œæ–°æ“ä½œæ¥å—åº¦ä½ã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ³•æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œé€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œå¼ºåŒ–å­¦ä¹ æ¥ç†Ÿæ‚‰å’Œæå‡æ¨¡å‹å¯¹åƒç´ ç©ºé—´æ¨ç†çš„æŒæ¡ã€‚</li>
<li>è§†è§‰æ“ä½œä½¿VLMèƒ½å¤Ÿå¤„ç†å¤æ‚è§†è§‰è¾“å…¥ï¼Œå¦‚ä¿¡æ¯ä¸°å¯Œçš„å›¾åƒæˆ–è§†é¢‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15966">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b8da341c6535770c2f92380e0df79d31.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21b36704bed0c7db46bd8427702f400f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d57b310f88595197e546e7e4c5563795.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-15b1106d4ee92d009b154b01b7626b30.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="PhyX-Does-Your-Model-Have-the-â€œWitsâ€-for-Physical-Reasoning"><a href="#PhyX-Does-Your-Model-Have-the-â€œWitsâ€-for-Physical-Reasoning" class="headerlink" title="PhyX: Does Your Model Have the â€œWitsâ€ for Physical Reasoning?"></a>PhyX: Does Your Model Have the â€œWitsâ€ for Physical Reasoning?</h2><p><strong>Authors:Hui Shen, Taiqiang Wu, Qi Han, Yunta Hsieh, Jizhou Wang, Yuyue Zhang, Yuxin Cheng, Zijian Hao, Yuansheng Ni, Xin Wang, Zhongwei Wan, Kai Zhang, Wendong Xu, Jing Xiong, Ping Luo, Wenhu Chen, Chaofan Tao, Zhuoqing Mao, Ngai Wong</strong></p>
<p>Existing benchmarks fail to capture a crucial aspect of intelligence: physical reasoning, the integrated ability to combine domain knowledge, symbolic reasoning, and understanding of real-world constraints. To address this gap, we introduce PhyX: the first large-scale benchmark designed to assess models capacity for physics-grounded reasoning in visual scenarios. PhyX includes 3K meticulously curated multimodal questions spanning 6 reasoning types across 25 sub-domains and 6 core physics domains: thermodynamics, electromagnetism, mechanics, modern physics, optics, and wave&amp;acoustics. In our comprehensive evaluation, even state-of-the-art models struggle significantly with physical reasoning. GPT-4o, Claude3.7-Sonnet, and GPT-o4-mini achieve only 32.5%, 42.2%, and 45.8% accuracy respectively-performance gaps exceeding 29% compared to human experts. Our analysis exposes critical limitations in current models: over-reliance on memorized disciplinary knowledge, excessive dependence on mathematical formulations, and surface-level visual pattern matching rather than genuine physical understanding. We provide in-depth analysis through fine-grained statistics, detailed case studies, and multiple evaluation paradigms to thoroughly examine physical reasoning capabilities. To ensure reproducibility, we implement a compatible evaluation protocol based on widely-used toolkits such as VLMEvalKit, enabling one-click evaluation. More details are available on our project page: <a target="_blank" rel="noopener" href="https://phyx-bench.github.io/">https://phyx-bench.github.io/</a>. </p>
<blockquote>
<p>ç°æœ‰çš„åŸºå‡†æµ‹è¯•æœªèƒ½æ•æ‰åˆ°æ™ºåŠ›çš„ä¸€ä¸ªé‡è¦æ–¹é¢ï¼šç‰©ç†æ¨ç†èƒ½åŠ›ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆé¢†åŸŸçŸ¥è¯†ã€ç¬¦å·æ¨ç†å’Œå¯¹ç°å®ä¸–ç•Œçš„ç†è§£çš„ç»¼åˆèƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PhyXï¼šè¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°æ¨¡å‹ä¸­åŸºäºç‰©ç†çš„æ¨ç†èƒ½åŠ›çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œé€‚ç”¨äºè§†è§‰åœºæ™¯ã€‚PhyXåŒ…æ‹¬3000ä¸ªç²¾å¿ƒç­–åˆ’çš„å¤šæ¨¡å¼é—®é¢˜ï¼Œæ¶µç›–6ç§æ¨ç†ç±»å‹å’Œ25ä¸ªå­é¢†åŸŸå’Œ6ä¸ªæ ¸å¿ƒç‰©ç†é¢†åŸŸï¼šçƒ­åŠ›å­¦ã€ç”µç£å­¦ã€åŠ›å­¦ã€ç°ä»£ç‰©ç†å­¦ã€å…‰å­¦å’Œæ³¢åŠ¨å£°å­¦ã€‚åœ¨æˆ‘ä»¬çš„ç»¼åˆè¯„ä¼°ä¸­ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ç‰©ç†æ¨ç†æ–¹é¢ä¹Ÿé¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚GPT-4oã€Claude3.7-Sonnetå’ŒGPT-o4-miniçš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º32.5%ã€42.2%å’Œ45.8%ï¼Œä¸äººç±»ä¸“å®¶ç›¸æ¯”ï¼Œæ€§èƒ½å·®è·è¶…è¿‡29%ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†å½“å‰æ¨¡å‹çš„å…³é”®å±€é™æ€§ï¼šè¿‡äºä¾èµ–è®°å¿†çš„çŸ¥è¯†ã€è¿‡åº¦ä¾èµ–æ•°å­¦å…¬å¼å’Œè¡¨é¢çº§çš„è§†è§‰æ¨¡å¼åŒ¹é…ï¼Œè€Œä¸æ˜¯çœŸæ­£çš„ç‰©ç†ç†è§£ã€‚æˆ‘ä»¬é€šè¿‡ç»†ç²’åº¦ç»Ÿè®¡ã€è¯¦ç»†æ¡ˆä¾‹ç ”ç©¶å’Œå¤šä¸ªè¯„ä¼°èŒƒå¼è¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œä»¥å½»åº•æ£€æŸ¥ç‰©ç†æ¨ç†èƒ½åŠ›ã€‚ä¸ºç¡®ä¿å¯é‡å¤æ€§ï¼Œæˆ‘ä»¬åŸºäºå¹¿æ³›ä½¿ç”¨çš„å·¥å…·åŒ…ï¼ˆå¦‚VLMEvalKitï¼‰å®æ–½äº†ä¸€ä¸ªå…¼å®¹çš„è¯„ä¼°åè®®ï¼Œå®ç°ä¸€é”®è¯„ä¼°ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯å¯åœ¨æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢æŸ¥çœ‹ï¼š[ç½‘ç«™é“¾æ¥]ï¼ˆ<a target="_blank" rel="noopener" href="https://phyx-bench.github.io/%EF%BC%89%E3%80%82">https://phyx-bench.github.io/ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.15929v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºPhyXçš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨è§†è§‰åœºæ™¯ä¸­çš„ç‰©ç†æ¨ç†èƒ½åŠ›ã€‚PhyXåŒ…å«3000ä¸ªç²¾å¿ƒç­–åˆ’çš„å¤šæ¨¡å¼é—®é¢˜ï¼Œæ¶µç›–6ä¸ªæ¨ç†ç±»å‹å’Œ25ä¸ªå­åŸŸä»¥åŠ6ä¸ªæ ¸å¿ƒç‰©ç†é¢†åŸŸã€‚å¯¹ç°æœ‰æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿åœ¨æœ€æ–°æŠ€æœ¯çš„æ¨¡å‹ä¹Ÿå­˜åœ¨æ˜æ˜¾çš„ç‰©ç†æ¨ç†å›°éš¾ï¼Œä¸ä¸“å®¶ç›¸æ¯”å­˜åœ¨è¶…è¿‡29%çš„æ€§èƒ½å·®è·ã€‚æ–‡ç« æ·±å…¥åˆ†æäº†å½“å‰æ¨¡å‹çš„å…³é”®å±€é™æ€§ï¼ŒåŒ…æ‹¬è¿‡åº¦ä¾èµ–å­¦ç§‘çŸ¥è¯†å’Œæ•°å­¦å…¬å¼ï¼Œä»¥åŠè¡¨é¢çº§çš„è§†è§‰æ¨¡å¼åŒ¹é…è€ŒéçœŸæ­£çš„ç‰©ç†ç†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PhyXæ˜¯é¦–ä¸ªè®¾è®¡ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨è§†è§‰åœºæ™¯ä¸­çš„ç‰©ç†æ¨ç†èƒ½åŠ›çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ã€‚</li>
<li>PhyXåŒ…å«å¹¿æ³›çš„é—®é¢˜ç±»å‹ï¼Œæ¶µç›–å¤šä¸ªç‰©ç†é¢†åŸŸã€‚</li>
<li>ç°æœ‰æ¨¡å‹åœ¨ç‰©ç†æ¨ç†æ–¹é¢å­˜åœ¨æ˜¾è‘—å›°éš¾ï¼Œä¸ä¸“å®¶ç›¸æ¯”å­˜åœ¨æ€§èƒ½å·®è·ã€‚</li>
<li>ç°æœ‰æ¨¡å‹çš„å…³é”®å±€é™æ€§åŒ…æ‹¬è¿‡åº¦ä¾èµ–å­¦ç§‘çŸ¥è¯†å’Œæ•°å­¦å…¬å¼ã€‚</li>
<li>æ¨¡å‹æ›´ä¾èµ–äºè¡¨é¢çº§çš„è§†è§‰æ¨¡å¼åŒ¹é…è€ŒéçœŸæ­£çš„ç‰©ç†ç†è§£ã€‚</li>
<li>æ–‡ç« æä¾›äº†è¯¦ç»†çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œå¹¶é€šè¿‡å¤šç§è¯„ä¼°èŒƒå¼å¯¹ç‰©ç†æ¨ç†èƒ½åŠ›è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.15929">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-158ae521051800c6f6ac2a2c0c3681cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e75805040cc28574f7fe066615181e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-654a3bfa1557b87d4d0f2454573dd651.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3b6539a791ae55fb7cc8d645f31d4ca.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7838f49420a15b5f58d7ab38ad5cd9ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bdf0a61190d0f356b08fff19fa2bfb51.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d1f2f20de7d0638f881888dab06c48c6.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LEXam-Benchmarking-Legal-Reasoning-on-340-Law-Exams"><a href="#LEXam-Benchmarking-Legal-Reasoning-on-340-Law-Exams" class="headerlink" title="LEXam: Benchmarking Legal Reasoning on 340 Law Exams"></a>LEXam: Benchmarking Legal Reasoning on 340 Law Exams</h2><p><strong>Authors:Yu Fan, Jingwei Ni, Jakob Merane, Etienne Salimbeni, Yang Tian, Yoan HermstrÃ¼wer, Yinya Huang, Mubashara Akhtar, Florian Geering, Oliver Dreyer, Daniel Brunner, Markus Leippold, Mrinmaya Sachan, Alexander Stremitzer, Christoph Engel, Elliott Ash, Joel Niklaus</strong></p>
<p>Long-form legal reasoning remains a key challenge for large language models (LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions. Besides reference answers, the open questions are also accompanied by explicit guidance outlining the expected legal reasoning approach such as issue spotting, rule recall, or rule application. Our evaluation on both open-ended and multiple-choice questions present significant challenges for current LLMs; in particular, they notably struggle with open questions that require structured, multi-step legal reasoning. Moreover, our results underscore the effectiveness of the dataset in differentiating between models with varying capabilities. Adopting an LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate how model-generated reasoning steps can be evaluated consistently and accurately. Our evaluation setup provides a scalable method to assess legal reasoning quality beyond simple accuracy metrics. Project page: <a target="_blank" rel="noopener" href="https://lexam-benchmark.github.io/">https://lexam-benchmark.github.io/</a> </p>
<blockquote>
<p>å°½ç®¡åœ¨æµ‹è¯•æ—¶ç¼©æ”¾æ–¹é¢æœ€è¿‘æœ‰æ‰€è¿›å±•ï¼Œä½†é•¿å½¢å¼æ³•å¾‹æ¨ç†ä»ç„¶æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†LEXamï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼Œæ¥æºäº340åœºæ³•å¾‹è€ƒè¯•ï¼Œè·¨è¶Š116ä¸ªæ³•å¾‹è¯¾ç¨‹ï¼Œæ¶µç›–å„ç§ç§‘ç›®å’Œå­¦ä½å±‚æ¬¡ã€‚è¯¥æ•°æ®é›†åŒ…æ‹¬4886ä¸ªè‹±æ–‡å’Œå¾·æ–‡çš„æ³•å¾‹è€ƒè¯•é—®é¢˜ï¼Œå…¶ä¸­åŒ…æ‹¬2841ä¸ªé•¿å½¢å¼å¼€æ”¾é—®é¢˜å’Œ2045ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ã€‚é™¤äº†å‚è€ƒç­”æ¡ˆå¤–ï¼Œå¼€æ”¾é—®é¢˜è¿˜é™„æœ‰æ˜ç¡®çš„æŒ‡å¯¼ï¼Œæ¦‚è¿°äº†é¢„æœŸçš„æ³•å¾‹æ¨ç†æ–¹æ³•ï¼Œå¦‚å‘ç°é—®é¢˜ã€å›å¿†è§„åˆ™æˆ–è§„åˆ™åº”ç”¨ã€‚æˆ‘ä»¬å¯¹å¼€æ”¾é—®é¢˜å’Œå¤šé¡¹é€‰æ‹©é—®é¢˜çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰LLMé¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼›ç‰¹åˆ«æ˜¯ä»–ä»¬åœ¨éœ€è¦ç»“æ„åŒ–ã€å¤šæ­¥éª¤æ³•å¾‹æ¨ç†çš„å¼€æ”¾é—®é¢˜ä¸Šæ˜æ˜¾æŒ£æ‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœçªæ˜¾äº†è¯¥æ•°æ®é›†åœ¨åŒºåˆ†ä¸åŒèƒ½åŠ›æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡é‡‡ç”¨â€œå¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ³•å®˜â€çš„æ¨¡å¼ï¼Œå¹¶è¿›è¡Œä¸¥æ ¼çš„äººç±»ä¸“å®¶éªŒè¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä¸€è‡´ä¸”å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹ç”Ÿæˆçš„æ¨ç†æ­¥éª¤ã€‚æˆ‘ä»¬çš„è¯„ä¼°è®¾ç½®æä¾›äº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•ï¼Œå¯ä»¥è¯„ä¼°è¶…è¶Šç®€å•å‡†ç¡®æ€§æŒ‡æ ‡çš„æ³•å¾‹æ¨ç†è´¨é‡ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://lexam-benchmark.github.io/">https://lexam-benchmark.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12864v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è¿‘æœŸè™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æµ‹è¯•æ—¶æ ‡åº¦ä¸Šæœ‰æ‰€è¿›å±•ï¼Œä½†é•¿å½¢å¼æ³•å¾‹æ¨ç†ä»æ˜¯å…¶é¢ä¸´çš„å…³é”®æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæ¨å‡ºäº†LEXamåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•ä»æ¶µç›–å¤šç§ç§‘ç›®å’Œå­¦å†å±‚æ¬¡çš„340åœºæ³•å¾‹è€ƒè¯•ä¸­è¡ç”Ÿè€Œæ¥ã€‚æ•°æ®é›†åŒ…å«è‹±æ–‡å’Œå¾·æ–‡çš„æ³•å¾‹è€ƒè¯•é—®é¢˜å…±4886é“ï¼Œå…¶ä¸­åŒ…æ‹¬å¼€æ”¾å¼çš„é•¿å›ç­”é—®é¢˜åŠé€‰æ‹©é¢˜ã€‚é™¤äº†å‚è€ƒç­”æ¡ˆå¤–ï¼Œå¼€æ”¾æ€§é—®é¢˜è¿˜é™„æœ‰é¢„æœŸçš„æ³•å¾‹æ¨ç†æ–¹æ³•çš„æ˜ç¡®æŒ‡å¯¼ã€‚å¯¹å¼€æ”¾æ€§å’Œé€‰æ‹©é¢˜çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰LLMsé¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ç»“æ„åŒ–ã€å¤šæ­¥éª¤æ³•å¾‹æ¨ç†çš„å¼€æ”¾æ€§é—®é¢˜ä¸Šã€‚æ­¤å¤–ï¼Œè¯¥æ•°æ®é›†åœ¨åŒºåˆ†ä¸åŒèƒ½åŠ›æ¨¡å‹æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚ç ”ç©¶å›¢é˜Ÿé‡‡ç”¨â€œå¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ³•å®˜â€çš„æ¨¡å¼ï¼Œé€šè¿‡ä¸¥æ ¼çš„äººç±»ä¸“å®¶éªŒè¯ï¼Œå±•ç¤ºå¦‚ä½•ä¸€è‡´ä¸”å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹äº§ç”Ÿçš„æ¨ç†æ­¥éª¤ã€‚å…¶è¯„ä¼°è®¾ç½®æä¾›äº†ä¸€ç§å¯è¯„ä¼°æ³•å¾‹æ¨ç†è´¨é‡è¶…è¶Šç®€å•å‡†ç¡®ç‡çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿å½¢å¼æ³•å¾‹æ¨ç†ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>LEXamåŸºå‡†æµ‹è¯•ä»å¤šåœºæ³•å¾‹è€ƒè¯•ä¸­è¡ç”Ÿè€Œæ¥ï¼ŒåŒ…å«å¤šç§é¢˜å‹å’Œæ³•å¾‹è¯¾ç¨‹ã€‚</li>
<li>å¼€æ”¾æ€§é—®é¢˜ä¼´éšé¢„æœŸçš„æ¨ç†æ–¹æ³•çš„æŒ‡å¯¼ï¼Œä»¥å¸®åŠ©LLMsç†è§£æ­£ç¡®çš„æ³•å¾‹æ¨ç†è·¯å¾„ã€‚</li>
<li>LLMsåœ¨éœ€è¦ç»“æ„åŒ–ã€å¤šæ­¥éª¤çš„æ³•å¾‹æ¨ç†é—®é¢˜ä¸Šè¡¨ç°ä¸ä½³ã€‚</li>
<li>æ•°æ®é›†åœ¨åŒºåˆ†ä¸åŒèƒ½åŠ›çš„æ¨¡å‹æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚</li>
<li>é‡‡ç”¨â€œå¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ³•å®˜â€çš„æ¨¡å¼ï¼Œé€šè¿‡äººç±»ä¸“å®¶éªŒè¯è¯„ä¼°æ¨¡å‹çš„æ¨ç†æ­¥éª¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12864">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f980c40f33f87c2f8d3ac9e462cb9e81.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8a74caff7babbbf1b9c3c41d382840a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72f2ec032a4995da68b693da95c57bdc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-690d9670bfbe58ecde4b5b01fad05dc7.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Shadow-FT-Tuning-Instruct-via-Base"><a href="#Shadow-FT-Tuning-Instruct-via-Base" class="headerlink" title="Shadow-FT: Tuning Instruct via Base"></a>Shadow-FT: Tuning Instruct via Base</h2><p><strong>Authors:Taiqiang Wu, Runming Yang, Jiayi Li, Pengfei Hu, Ngai Wong, Yujiu Yang</strong></p>
<p>Large language models (LLMs) consistently benefit from further fine-tuning on various tasks. However, we observe that directly tuning the INSTRUCT (i.e., instruction tuned) models often leads to marginal improvements and even performance degeneration. Notably, paired BASE models, the foundation for these INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to tune the INSTRUCT models by leveraging the corresponding BASE models. The key insight is to fine-tune the BASE model, and then directly graft the learned weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no additional parameters, is easy to implement, and significantly improves performance. We conduct extensive experiments on tuning mainstream LLMs, such as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering coding, reasoning, and mathematical tasks. Experimental results demonstrate that Shadow-FT consistently outperforms conventional full-parameter and parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT can be applied to multimodal large language models (MLLMs) and combined with direct preference optimization (DPO). Codes and weights are available at \href{<a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/Shadow-FT%7D%7BGithub%7D">https://github.com/wutaiqiang/Shadow-FT}{Github}</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§ä»»åŠ¡ä¸Šé€šè¿‡è¿›ä¸€æ­¥çš„å¾®è°ƒæŒç»­å—ç›Šã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç›´æ¥è°ƒæ•´INSTRUCTï¼ˆå³æŒ‡ä»¤è°ƒæ•´ï¼‰æ¨¡å‹å¾€å¾€åªå¸¦æ¥å¾®å°çš„æ”¹è¿›ï¼Œç”šè‡³æ€§èƒ½ä¸‹é™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›INSTRUCTå˜ä½“æ‰€ä¾èµ–çš„BASEæ¨¡å‹åŒ…å«é«˜åº¦ç›¸ä¼¼çš„æƒé‡å€¼ï¼ˆå¦‚Llama 3.1 8Bçš„å¹³å‡å€¼ä¸åˆ°2%ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„Shadow-FTæ¡†æ¶ï¼Œåˆ©ç”¨ç›¸åº”çš„BASEæ¨¡å‹æ¥è°ƒæ•´INSTRUCTæ¨¡å‹ã€‚å…³é”®æ€è·¯æ˜¯å¾®è°ƒBASEæ¨¡å‹ï¼Œç„¶åå°†å­¦åˆ°çš„æƒé‡æ›´æ–°ç›´æ¥ç§»æ¤åˆ°INSTRUCTæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºçš„Shadow-FTä¸ä¼šå¼•å…¥é¢å¤–çš„å‚æ•°ï¼Œæ˜“äºå®ç°ï¼Œå¹¶èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ä¸»æµçš„LLMä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¦‚Qwen 3å’ŒLlama 3ç³»åˆ—ï¼Œå¹¶åœ¨æ¶µç›–ç¼–ç ã€æ¨ç†å’Œæ•°å­¦ä»»åŠ¡çš„19ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒShadow-FTæŒç»­ä¼˜äºä¼ ç»Ÿçš„å…¨å‚æ•°å’Œå‚æ•°é«˜æ•ˆè°ƒæ•´æ–¹æ³•ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒShadow-FTå¯åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå¹¶ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç›¸ç»“åˆã€‚ç›¸å…³ä»£ç å’Œæƒé‡å¯åœ¨Githubä¸Šæ‰¾åˆ°ï¼ˆé“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/wutaiqiang/Shadow-FT%EF%BC%89%E3%80%82">https://github.com/wutaiqiang/Shadow-FTï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12716v2">PDF</a> 19 pages, 10 tables, 6 figures</p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸Šé€šè¿‡å¾®è°ƒèƒ½è·å¾—æŒç»­æå‡ï¼Œä½†ç›´æ¥å¯¹INSTRUCTæ¨¡å‹è¿›è¡Œå¾®è°ƒå¸¸å¯¼è‡´æ€§èƒ½æå‡æœ‰é™ç”šè‡³é€€æ­¥ã€‚ç ”ç©¶æå‡ºShadow-FTæ¡†æ¶ï¼Œåˆ©ç”¨å¯¹åº”çš„BASEæ¨¡å‹æ¥ä¼˜åŒ–INSTRUCTæ¨¡å‹çš„å¾®è°ƒã€‚è¯¥æ¡†æ¶é€šè¿‡å¾®è°ƒBASEæ¨¡å‹å¹¶å°†å­¦ä¹ åˆ°çš„æƒé‡æ›´æ–°ç›´æ¥åº”ç”¨åˆ°INSTRUCTæ¨¡å‹ä¸Šï¼Œæ— éœ€å¢åŠ é¢å¤–å‚æ•°ï¼Œæ˜“äºå®ç°ï¼Œå¹¶èƒ½æ˜¾è‘—æé«˜æ€§èƒ½ã€‚å®éªŒè¯æ˜ï¼ŒShadow-FTåœ¨ä¸»æµLLMä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºå¸¸è§„çš„å…¨å‚æ•°å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚å®ƒè¿˜å¯ä»¥åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¹¶ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç»“åˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMsé€šè¿‡å¾®è°ƒåœ¨å¤šç§ä»»åŠ¡ä¸Šå¯è·æ€§èƒ½æå‡ï¼Œä½†ç›´æ¥å¯¹INSTRUCTæ¨¡å‹å¾®è°ƒæ•ˆæœæœ‰é™ã€‚</li>
<li>BASEæ¨¡å‹ä¸INSTRUCTæ¨¡å‹æƒé‡é«˜åº¦ç›¸ä¼¼ã€‚</li>
<li>æå‡ºçš„Shadow-FTæ¡†æ¶åˆ©ç”¨BASEæ¨¡å‹æ¥ä¼˜åŒ–INSTRUCTæ¨¡å‹çš„å¾®è°ƒï¼Œæ— éœ€å¢åŠ é¢å¤–å‚æ•°ã€‚</li>
<li>Shadow-FTé€šè¿‡å¾®è°ƒBASEæ¨¡å‹å¹¶å°†å­¦ä¹ åˆ°çš„æƒé‡æ›´æ–°ç›´æ¥åº”ç”¨åˆ°INSTRUCTæ¨¡å‹ï¼Œæ˜¾è‘—æé«˜æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜Shadow-FTåœ¨ä¸»æµLLMä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºå¸¸è§„çš„å…¨å‚æ•°å’Œå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚</li>
<li>Shadow-FTå¯åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚</li>
<li>Shadow-FTå¯ä¸ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ç»“åˆä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12716">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d43d8b4e19d7a5c7707c84fbdf855d45.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-db1cd1fb6974c657fb8c9fe5a97dbd7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7a6a60633b13a1a8a5fe06c4d09ce7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-171f6aafd714b9fe49957d79b1830234.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3402b8f88c324bb04351c6a1c0aa18d1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Improving-Multilingual-Language-Models-by-Aligning-Representations-through-Steering"><a href="#Improving-Multilingual-Language-Models-by-Aligning-Representations-through-Steering" class="headerlink" title="Improving Multilingual Language Models by Aligning Representations   through Steering"></a>Improving Multilingual Language Models by Aligning Representations   through Steering</h2><p><strong>Authors:Omar Mahmoud, Buddhika Laknath Semage, Thommen George Karimpanal, Santu Rana</strong></p>
<p>In this paper, we investigate how large language models (LLMS) process non-English tokens within their layer representations, an open question despite significant advancements in the field. Using representation steering, specifically by adding a learned vector to a single model layerâ€™s activations, we demonstrate that steering a single model layer can notably enhance performance. Our analysis shows that this approach achieves results comparable to translation baselines and surpasses state of the art prompt optimization methods. Additionally, we highlight how advanced techniques like supervised fine tuning (\textsc{sft}) and reinforcement learning from human feedback (\textsc{rlhf}) improve multilingual capabilities by altering representation spaces. We further illustrate how these methods align with our approach to reshaping LLMS layer representations. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMSï¼‰æ˜¯å¦‚ä½•åœ¨å…¶å±‚è¡¨ç¤ºä¸­å¤„ç†éè‹±è¯­ä»¤ç‰Œçš„ï¼Œå°½ç®¡è¯¥é¢†åŸŸå·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†è¿™ä¸ªé—®é¢˜ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨è¡¨ç¤ºå¼•å¯¼æ³•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡åœ¨å•ä¸ªæ¨¡å‹å±‚çš„æ¿€æ´»ä¸­æ·»åŠ ä¸€ä¸ªå­¦ä¹ å‘é‡ï¼Œæˆ‘ä»¬è¯æ˜å¼•å¯¼å•ä¸ªæ¨¡å‹å±‚å¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥å®ç°ä¸ç¿»è¯‘åŸºçº¿ç›¸å½“çš„ç»“æœï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰çš„æç¤ºä¼˜åŒ–æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡ç‚¹ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨å…ˆè¿›çš„ç›‘ç£å¾®è°ƒæŠ€æœ¯ï¼ˆ\textbf{sft}ï¼‰å’Œäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆ\textbf{rlhf}ï¼‰ç­‰é«˜çº§æŠ€æœ¯å¦‚ä½•é€šè¿‡æ”¹å˜è¡¨ç¤ºç©ºé—´æ¥æé«˜å¤šè¯­è¨€èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥è¯´æ˜äº†è¿™äº›æ–¹æ³•å¦‚ä½•ä¸é‡å¡‘LLMSå±‚è¡¨ç¤ºçš„æ–¹æ³•ç›¸ä¸€è‡´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12584v1">PDF</a> </p>
<p><strong>Summary</strong><br>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•å¤„ç†éè‹±è¯­ç¬¦å·çš„å±‚è¡¨ç¤ºã€‚é€šè¿‡è¡¨ç¤ºå¼•å¯¼çš„æ–¹æ³•ï¼Œå‘å•ä¸€æ¨¡å‹å±‚çš„æ¿€æ´»æ·»åŠ å­¦ä¹ å‘é‡ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ€§èƒ½ã€‚åˆ†æä¸ç¿»è¯‘åŸºå‡†çº¿ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å–å¾—äº†ç›¸å½“çš„ç»“æœå¹¶è¶…è¶Šäº†ç°æœ‰çš„æç¤ºä¼˜åŒ–æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ä»‹ç»äº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ç­‰æŠ€æœ¯å¦‚ä½•æ”¹è¿›å¤šè¯­è¨€èƒ½åŠ›ï¼Œå¹¶æ¢è®¨äº†é‡å¡‘LLMSå±‚è¡¨ç¤ºçš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMSï¼‰å¯¹éè‹±è¯­ç¬¦å·çš„å±‚è¡¨ç¤ºå¤„ç†æ˜¯ä¸€ä¸ªå…¬å¼€é—®é¢˜ï¼Œå°½ç®¡è¯¥é¢†åŸŸå·²æœ‰æ˜¾è‘—è¿›å±•ã€‚</li>
<li>é€šè¿‡è¡¨ç¤ºå¼•å¯¼çš„æ–¹æ³•ï¼Œå¢å¼ºå•ä¸€æ¨¡å‹å±‚çš„æ€§èƒ½å¯ä»¥æ˜¾è‘—æé«˜ã€‚</li>
<li>å¼•å¯¼æ–¹æ³•çš„ç»“æœä¸ç¿»è¯‘åŸºå‡†çº¿ç›¸å½“ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰çš„æç¤ºä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰ç­‰æŠ€æœ¯æ”¹è¿›äº†å¤šè¯­è¨€èƒ½åŠ›ã€‚</li>
<li>è¿™äº›æŠ€æœ¯é€šè¿‡æ”¹å˜è¡¨ç¤ºç©ºé—´æ¥æ”¹è¿›å¤šè¯­è¨€èƒ½åŠ›ã€‚</li>
<li>è®ºæ–‡å±•ç¤ºäº†å¦‚ä½•å°†è¿™äº›æ–¹æ³•ä¸é‡å¡‘LLMSå±‚è¡¨ç¤ºçš„æ–¹æ³•ç›¸ç»“åˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12584">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6a19e0d28cfd6722c20ea2b8812d5338.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-45078bd7c3466355746716f49eb0103e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0599407c8f36833d97c7fd9da9e21ddb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b097e23dd11a7b57e05d6d7aac6f70a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-448d8f7cd7f480b23c575100bedf4fa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b2b6f43d4071716c3bc1b47fa6b1d9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-11ebed427d58fb2cd0ef6d9fa57c5eae.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Towards-Visuospatial-Cognition-via-Hierarchical-Fusion-of-Visual-Experts"><a href="#Towards-Visuospatial-Cognition-via-Hierarchical-Fusion-of-Visual-Experts" class="headerlink" title="Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts"></a>Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</h2><p><strong>Authors:Qi Feng</strong></p>
<p>While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research. </p>
<blockquote>
<p>è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸€èˆ¬çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†å¯¹äºç©ºé—´å¸ƒå±€ã€å…³ç³»å’ŒåŠ¨æ€çš„ç©ºé—´æ¨ç†ä»ç„¶æ˜¯å·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ¨¡å‹é€šå¸¸ç¼ºä¹å¿…è¦çš„æ¶æ„ç»„ä»¶å’Œç²¾ç»†ç©ºé—´ç†è§£çš„ä¸“ä¸šè®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬æ¨å‡ºäº†ViCA2ï¼ˆè§†è§‰ç©ºé—´è®¤çŸ¥åŠ©æ‰‹2ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æé«˜ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚ViCA2é‡‡ç”¨åŒè§†è§‰ç¼–ç å™¨æ¶æ„ï¼Œèåˆäº†SigLIPçš„è¯­ä¹‰å’ŒHieraçš„ç©ºé—´ç»“æ„ï¼Œå¹¶é‡‡ç”¨äº†é«˜æ•ˆçš„æ ‡è®°æ¯”ç‡æ§åˆ¶æœºåˆ¶ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ViCA-322Kå¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡32ä¸‡å¯¹ç©ºé—´å®šä½çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œç”¨äºé’ˆå¯¹æ€§çš„æŒ‡ä»¤è°ƒæ•´ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„VSI-BenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„ViCA2-7Bæ¨¡å‹å–å¾—äº†å¹³å‡å¾—åˆ†56.8åˆ†çš„æœ€æ–°æ°´å¹³æˆç»©ï¼Œæ˜¾è‘—è¶…è¿‡äº†æ›´å¤§çš„å¼€æºæ¨¡å‹ï¼ˆä¾‹å¦‚LLaVA-NeXT-Video-72Bå¾—åˆ†ä¸º40.9ï¼‰å’Œé¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ï¼ˆGemini-1.5 Proå¾—åˆ†ä¸º45.4ï¼‰ã€‚è¿™è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå³ä½¿åœ¨ç´§å‡‘æ¨¡å‹ä¸‹ä¹Ÿèƒ½å®ç°å¼ºå¤§çš„è§†è§‰ç©ºé—´æ™ºèƒ½ã€‚æˆ‘ä»¬å‘å¸ƒäº†ViCA2åŠå…¶ä»£ç åº“å’ŒViCA-322Kæ•°æ®é›†ï¼Œä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.12363v3">PDF</a> In version 1, Hidetoshi Shimodaira was included as a co-author   without their consent and has been removed from the author list</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸€èˆ¬çš„è§†è§‰è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§†è§‰ç©ºé—´è®¤çŸ¥æ–¹é¢ï¼Œå³å¯¹ç©ºé—´å¸ƒå±€ã€å…³ç³»å’ŒåŠ¨æ€æ¨ç†ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ç°æœ‰æ¨¡å‹ç¼ºä¹å¿…è¦çš„æ¶æ„ç»„ä»¶å’Œç²¾ç»†ç©ºé—´ç†è§£çš„ä¸“ä¸šè®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬æ¨å‡ºViCA2ï¼ˆè§†è§‰ç©ºé—´è®¤çŸ¥åŠ©æ‰‹2ï¼‰ï¼Œä¸€æ¬¾ä¸“ä¸ºå¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›è®¾è®¡çš„æ–°å‹MLLMã€‚ViCA2é‡‡ç”¨åŒè§†è§’ç¼–ç å™¨æ¶æ„ï¼Œé›†æˆSigLIPè¿›è¡Œè¯­ä¹‰åˆ†æï¼ŒHieraè¿›è¡Œç©ºé—´ç»“æ„è§£æï¼Œå¹¶ç»“åˆä»¤ç‰Œæ¯”ç‡æ§åˆ¶æœºåˆ¶ä»¥æé«˜æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ViCA-322Kå¤§å‹æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡32ä¸‡2åƒä¸ªç©ºé—´å®šä½çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œç”¨äºé’ˆå¯¹æ€§çš„æŒ‡ä»¤è°ƒä¼˜ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„VSI-BenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çš„ViCA2-7Bæ¨¡å‹è¾¾åˆ°äº†å¹³å‡å¾—åˆ†56.8ï¼Œæ˜¾è‘—è¶…è¶Šäº†å…¶ä»–å¤§å‹å¼€æºæ¨¡å‹ï¼ˆå¦‚LLaVA-NeXT-Video-72Bï¼Œå¾—åˆ†40.9ï¼‰å’Œé¢†å…ˆçš„ä¸“ä¸šæ¨¡å‹ï¼ˆGemini-1.5 Proï¼Œå¾—åˆ†45.4ï¼‰ã€‚è¿™è¯æ˜äº†æˆ‘ä»¬åœ¨å®ç°å¼ºå¤§è§†è§‰ç©ºé—´æ™ºèƒ½çš„ç´§å‡‘æ¨¡å‹æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å‘å¸ƒViCA2ã€å…¶ä»£ç åº“å’ŒViCA-322Kæ•°æ®é›†ä»¥ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç©ºé—´è®¤çŸ¥æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ViCA2æ˜¯ä¸€æ¬¾æ–°å‹MLLMï¼Œæ—¨åœ¨å¢å¼ºç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ViCA2é‡‡ç”¨åŒè§†è§’ç¼–ç å™¨æ¶æ„ï¼Œé›†æˆSigLIPå’ŒHieraè¿›è¡Œè¯­ä¹‰å’Œç©ºé—´ç»“æ„è§£æã€‚</li>
<li>å¼€å‘å‡ºViCA-322Kå¤§å‹æ•°æ®é›†ç”¨äºé’ˆå¯¹æ€§çš„æŒ‡ä»¤è°ƒä¼˜ã€‚</li>
<li>ViCA2åœ¨VSI-BenchåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡å¾—åˆ†56.8ã€‚</li>
<li>ViCA2æ˜¾è‘—è¶…è¶Šäº†å…¶ä»–å¤§å‹å¼€æºå’Œä¸“ä¸šæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.12363">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4c0a60dc96f980780755856a47f5f70f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01ddf487f5a21022b3d8cb8abb6ebc1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0b49a1d2f8faf93399797eb2a9195519.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94de87aa0b44a40c568d427a06ae798c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Search-and-Refine-During-Think-Autonomous-Retrieval-Augmented-Reasoning-of-LLMs"><a href="#Search-and-Refine-During-Think-Autonomous-Retrieval-Augmented-Reasoning-of-LLMs" class="headerlink" title="Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning   of LLMs"></a>Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning   of LLMs</h2><p><strong>Authors:Yaorui Shi, Sihang Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, Xiang Wang</strong></p>
<p>Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new &#96;&#96;search-and-refine-during-thinkâ€™â€™ paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹å·²ç»è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ¨ç†èƒ½åŠ›ï¼Œä½†æœ¬è´¨ä¸Šå—é™äºå…¶çŸ¥è¯†åº“ã€‚æ£€ç´¢å¢å¼ºæ¨ç†é€šè¿‡å…è®¸å¤§å‹è¯­è¨€æ¨¡å‹æŸ¥è¯¢å¤–éƒ¨èµ„æºæ¥ç¼“è§£è¿™ä¸€é™åˆ¶ï¼Œä½†ç°æœ‰æ–¹æ³•ç»å¸¸æ£€ç´¢åˆ°ä¸ç›¸å…³æˆ–å˜ˆæ‚çš„ä¿¡æ¯ï¼Œé˜»ç¢äº†å‡†ç¡®çš„æ¨ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†AutoRefineï¼Œè¿™æ˜¯ä¸€ä¸ªé‡‡ç”¨æ–°å‹â€œæ€è€ƒè¿‡ç¨‹ä¸­çš„æœç´¢ä¸ä¼˜åŒ–â€èŒƒå¼çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ¡†æ¶ã€‚AutoRefineåœ¨è¿ç»­çš„æœç´¢è°ƒç”¨ä¹‹é—´å¼•å…¥äº†æ˜ç¡®çš„çŸ¥è¯†ä¼˜åŒ–æ­¥éª¤ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¿­ä»£åœ°è¿‡æ»¤ã€æç‚¼å’Œæ•´ç†è¯æ®ï¼Œç„¶åç”Ÿæˆç­”æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œç»“åˆäº†å®šåˆ¶çš„æ£€ç´¢ç‰¹å®šå¥–åŠ±å’Œç­”æ¡ˆæ­£ç¡®æ€§å¥–åŠ±ã€‚åœ¨å•è·³å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAutoRefineæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„å¤šè·³æ¨ç†åœºæ™¯ä¸­ã€‚è¯¦ç»†åˆ†æè¡¨æ˜ï¼ŒAutoRefineèƒ½è¿›è¡Œé¢‘ç¹çš„é«˜è´¨é‡æœç´¢ï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°ç»¼åˆè¯æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.11277v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºAutoRefineçš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒæ¡†æ¶ï¼Œé‡‡ç”¨æ–°çš„â€œæœç´¢ä¸æ€è€ƒä¸­ç²¾ç‚¼â€æ¨¡å¼ï¼Œç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åœ¨è¿ç»­æœç´¢è°ƒç”¨ä¹‹é—´å¼•å…¥æ˜¾å¼çŸ¥è¯†ç²¾ç‚¼æ­¥éª¤ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¿­ä»£è¿‡æ»¤ã€æç‚¼å’Œæ•´ç†è¯æ®ï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®çš„ç­”æ¡ˆã€‚æ­¤å¤–ï¼Œé€šè¿‡ç»“åˆæ£€ç´¢ç‰¹å®šå¥–åŠ±å’Œç­”æ¡ˆæ­£ç¡®æ€§å¥–åŠ±ï¼Œä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ŒAutoRefineåœ¨å•è·³å’Œå¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚å¤šè·³æ¨ç†åœºæ™¯ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹è™½ç„¶å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶çŸ¥è¯†åº“æœ¬è´¨ä¸Šå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ£€ç´¢å¢å¼ºæ¨ç†å¯é€šè¿‡å…è®¸è¯­è¨€æ¨¡å‹æŸ¥è¯¢å¤–éƒ¨èµ„æºæ¥å‡è½»è¿™ä¸€å±€é™æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€æ£€ç´¢åˆ°ä¸ç›¸å…³æˆ–å˜ˆæ‚çš„ä¿¡æ¯ï¼Œé˜»ç¢å‡†ç¡®æ¨ç†ã€‚</li>
<li>AutoRefineæ¡†æ¶å¼•å…¥â€œæœç´¢ä¸æ€è€ƒä¸­ç²¾ç‚¼â€æ¨¡å¼ï¼Œåœ¨è¿ç»­æœç´¢ä¹‹é—´åŠ å…¥çŸ¥è¯†ç²¾ç‚¼æ­¥éª¤ã€‚</li>
<li>AutoRefineé€šè¿‡è¿­ä»£è¿‡æ»¤ã€æç‚¼å’Œæ•´ç†è¯æ®ï¼Œæé«˜æœç´¢è´¨é‡ï¼Œè¿›è€Œæå‡ç­”æ¡ˆå‡†ç¡®æ€§ã€‚</li>
<li>ç»“åˆæ£€ç´¢ç‰¹å®šå¥–åŠ±å’Œç­”æ¡ˆæ­£ç¡®æ€§å¥–åŠ±ï¼Œä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œæå‡æ¨¡å‹è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.11277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4e447565d2bbf874ae46124540158943.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c630524472572e4123adc656bbb11014.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3d42cacf66dffd249ce76343aae3313.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4446c1de7272a2a37d42415d65e1827.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5849377557489b948ec35a0555161fd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Learning-When-to-Think-Shaping-Adaptive-Reasoning-in-R1-Style-Models-via-Multi-Stage-RL"><a href="#Learning-When-to-Think-Shaping-Adaptive-Reasoning-in-R1-Style-Models-via-Multi-Stage-RL" class="headerlink" title="Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models   via Multi-Stage RL"></a>Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models   via Multi-Stage RL</h2><p><strong>Authors:Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, Dongbin Zhao</strong></p>
<p>Large reasoning models (LRMs) are proficient at generating explicit, step-by-step reasoning sequences before producing final answers. However, such detailed reasoning can introduce substantial computational overhead and latency, particularly for simple problems. To address this over-thinking problem, we explore how to equip LRMs with adaptive thinking capabilities: enabling them to dynamically decide whether or not to engage in explicit reasoning based on problem complexity. Building on R1-style distilled models, we observe that inserting a simple ellipsis (â€œâ€¦â€) into the prompt can stochastically trigger either a thinking or no-thinking mode, revealing a latent controllability in the reasoning behavior. Leveraging this property, we propose AutoThink, a multi-stage reinforcement learning (RL) framework that progressively optimizes reasoning policies via stage-wise reward shaping. AutoThink learns to invoke explicit reasoning only when necessary, while defaulting to succinct responses for simpler tasks. Experiments on five mainstream mathematical benchmarks demonstrate that AutoThink achieves favorable accuracy-efficiency trade-offs compared to recent prompting and RL-based pruning methods. It can be seamlessly integrated into any R1-style model, including both distilled and further fine-tuned variants. Notably, AutoThink improves relative accuracy by 6.4 percent while reducing token usage by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and adaptive reasoning paradigm for LRMs. Project Page: <a target="_blank" rel="noopener" href="https://github.com/ScienceOne-AI/AutoThink">https://github.com/ScienceOne-AI/AutoThink</a>. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰æ“…é•¿åœ¨ç»™å‡ºæœ€ç»ˆç­”æ¡ˆä¹‹å‰ç”Ÿæˆæ˜ç¡®ã€å¾ªåºæ¸è¿›çš„æ¨ç†åºåˆ—ã€‚ç„¶è€Œï¼Œè¿™æ ·çš„è¯¦ç»†æ¨ç†å¯èƒ½ä¼šå¼•å…¥å¤§é‡çš„è®¡ç®—å¼€é”€å’Œå»¶è¿Ÿï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç®€å•é—®é¢˜æ—¶ã€‚ä¸ºäº†è§£å†³è¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œæˆ‘ä»¬æ¢è®¨å¦‚ä½•ä¸ºLRMsé…å¤‡é€‚åº”æ€§æ€è€ƒèƒ½åŠ›ï¼šè®©å®ƒä»¬èƒ½å¤ŸåŠ¨æ€åœ°å†³å®šæ˜¯å¦è¿›è¡Œæ˜¾æ€§æ¨ç†ï¼Œè¿™å–å†³äºé—®é¢˜çš„å¤æ‚æ€§ã€‚æˆ‘ä»¬åŸºäºR1é£æ ¼çš„è’¸é¦æ¨¡å‹è§‚å¯Ÿåˆ°ï¼Œåœ¨æç¤ºä¸­æ’å…¥ä¸€ä¸ªç®€å•çš„çœç•¥å·ï¼ˆâ€œâ€¦â€ï¼‰å¯ä»¥éšæœºè§¦å‘æ€è€ƒæ¨¡å¼æˆ–éæ€è€ƒæ¨¡å¼ï¼Œæ­ç¤ºäº†æ¨ç†è¡Œä¸ºä¸­çš„æ½œåœ¨å¯æ§æ€§ã€‚åˆ©ç”¨è¿™ä¸€ç‰¹æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†AutoThinkï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šé˜¶æ®µçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œå®ƒé€šè¿‡åˆ†é˜¶æ®µå¥–åŠ±å¡‘é€ æ¥é€æ­¥ä¼˜åŒ–æ¨ç†ç­–ç•¥ã€‚AutoThinkå­¦ä¹ åªåœ¨å¿…è¦æ—¶è¿›è¡Œæ˜¾æ€§æ¨ç†ï¼Œè€Œé»˜è®¤ä¸ºç®€å•çš„ä»»åŠ¡æä¾›ç®€æ´çš„å›åº”ã€‚åœ¨äº”ä¸ªä¸»æµæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€è¿‘çš„æç¤ºå’ŒåŸºäºRLçš„ä¿®å‰ªæ–¹æ³•ç›¸æ¯”ï¼ŒAutoThinkåœ¨å‡†ç¡®æ€§å’Œæ•ˆç‡ä¹‹é—´å®ç°äº†æœ‰åˆ©çš„æƒè¡¡ã€‚å®ƒå¯ä»¥æ— ç¼åœ°é›†æˆåˆ°ä»»ä½•R1é£æ ¼çš„æ¨¡å‹ä¸­ï¼ŒåŒ…æ‹¬è’¸é¦å’Œè¿›ä¸€æ­¥å¾®è°ƒåçš„å˜ä½“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒAutoThinkåœ¨DeepSeek-R1-Distill-Qwen-1.5Bä¸Šç›¸å¯¹æé«˜äº†6.4%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å‡å°‘äº†52%çš„ä»¤ç‰Œä½¿ç”¨ï¼Œä¸ºLRMså»ºç«‹äº†å¯æ‰©å±•å’Œè‡ªé€‚åº”çš„æ¨ç†èŒƒå¼ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/ScienceOne-AI/AutoThink%E3%80%82">https://github.com/ScienceOne-AI/AutoThinkã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10832v2">PDF</a> Fisrt Submitted on 16 May 2025; Update on 28 May 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰èƒ½å¤Ÿç”Ÿæˆæ˜ç¡®ã€é€æ­¥çš„æ¨ç†åºåˆ—ä»¥å¾—å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œä½†è¿™æ ·çš„è¯¦ç»†æ¨ç†å¯èƒ½ä¼šå¸¦æ¥è¾ƒå¤§çš„è®¡ç®—å¼€é”€å’Œå»¶è¿Ÿï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç®€å•é—®é¢˜æ—¶ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æ—¨åœ¨æ¢è®¨å¦‚ä½•ä¸ºLRMsé…å¤‡è‡ªé€‚åº”æ€è€ƒèƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ ¹æ®é—®é¢˜å¤æ‚åº¦åŠ¨æ€å†³å®šæ˜¯å¦è¿›è¡Œæ˜¾æ€§æ¨ç†ã€‚ç ”ç©¶åŸºäºR1é£æ ¼è’¸é¦æ¨¡å‹ï¼Œå‘ç°é€šè¿‡åœ¨æç¤ºä¸­æ’å…¥çœç•¥å·ï¼ˆâ€œâ€¦â€ï¼‰ï¼Œå¯ä»¥éšæœºè§¦å‘æ€è€ƒæˆ–éæ€è€ƒæ¨¡å¼ï¼Œæ­ç¤ºå‡ºæ¨ç†è¡Œä¸ºçš„æ½œåœ¨å¯æ§æ€§ã€‚åŸºäºæ­¤ç‰¹æ€§ï¼Œæœ¬æ–‡æå‡ºäº†AutoThinkï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œé€šè¿‡é˜¶æ®µæ€§å¥–åŠ±å¡‘é€ æ¥é€æ­¥ä¼˜åŒ–æ¨ç†ç­–ç•¥ã€‚AutoThinkèƒ½å¤ŸæŒ‰éœ€è¿›è¡Œæ˜¾æ€§æ¨ç†ï¼Œä¸ºç®€å•ä»»åŠ¡æä¾›ç®€æ´å›åº”ã€‚åœ¨äº”ä¸ªä¸»æµæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°çš„æç¤ºå’ŒåŸºäºRLçš„ä¿®å‰ªæ–¹æ³•ç›¸æ¯”ï¼ŒAutoThinkåœ¨å‡†ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´å–å¾—äº†æœ‰åˆ©çš„æƒè¡¡ã€‚å®ƒå¯ä»¥æ— ç¼åœ°é›†æˆåˆ°ä»»ä½•R1é£æ ¼æ¨¡å‹ä¸­ï¼ŒåŒ…æ‹¬è’¸é¦å’Œè¿›ä¸€æ­¥å¾®è°ƒè¿‡çš„å˜ä½“ã€‚å°¤å…¶æ˜¯ï¼ŒAutoThinkåœ¨DeepSeek-R1-Distill-Qwen-1.5Bä¸Šç›¸å¯¹æé«˜äº†6.4%çš„å‡†ç¡®ç‡ï¼ŒåŒæ—¶å‡å°‘äº†52%çš„ä»¤ç‰Œä½¿ç”¨é‡ï¼Œä¸ºLRMså»ºç«‹äº†å¯æ‰©å±•å’Œè‡ªé€‚åº”çš„æ¨ç†æ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹è™½ç„¶èƒ½ç”Ÿæˆè¯¦ç»†çš„æ¨ç†æ­¥éª¤ï¼Œä½†å¤„ç†ç®€å•é—®é¢˜æ—¶å¯èƒ½å¯¼è‡´è®¡ç®—å¼€é”€å’Œå»¶è¿Ÿã€‚</li>
<li>é€šè¿‡åœ¨æç¤ºä¸­æ’å…¥çœç•¥å·ï¼Œå¯ä»¥è§¦å‘æ¨¡å‹çš„æ€è€ƒæˆ–éæ€è€ƒæ¨¡å¼ï¼Œè¡¨æ˜æ¨ç†è¡Œä¸ºå…·æœ‰æ½œåœ¨å¯æ§æ€§ã€‚</li>
<li>æå‡ºAutoThinkæ¡†æ¶ï¼Œåˆ©ç”¨å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨ç†ç­–ç•¥ï¼Œå®ç°æŒ‰éœ€æ¨ç†ã€‚</li>
<li>AutoThinkèƒ½å¤Ÿåœ¨ä¿æŒè¾ƒé«˜å‡†ç¡®ç‡çš„åŒæ—¶ï¼Œæé«˜æ¨¡å‹æ•ˆç‡ï¼Œé€‚ç”¨äºå¤šç§R1é£æ ¼æ¨¡å‹ã€‚</li>
<li>AutoThinkåœ¨DeepSeek-R1-Distill-Qwen-1.5Bæ¨¡å‹ä¸Šå–å¾—äº†æ˜¾è‘—çš„å‡†ç¡®ç‡æå‡å’Œä»¤ç‰Œä½¿ç”¨å‡å°‘ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºå¤§å‹æ¨ç†æ¨¡å‹å»ºç«‹äº†ä¸€ç§å¯æ‰©å±•å’Œè‡ªé€‚åº”çš„æ¨ç†æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10832">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2023105cedaf55213f3453d0d3400f14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b99f41bd4243d7e267934383d35a6f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f64f6c4e8c4dc47c179962d6a7808f02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc3abd56023e573b251f0edd3c978cda.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9dbfe7baf7282040d5d1eab12c7bfe42.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Beyond-â€˜Aha-â€™-Toward-Systematic-Meta-Abilities-Alignment-in-Large-Reasoning-Models"><a href="#Beyond-â€˜Aha-â€™-Toward-Systematic-Meta-Abilities-Alignment-in-Large-Reasoning-Models" class="headerlink" title="Beyond â€˜Aha!â€™: Toward Systematic Meta-Abilities Alignment in Large   Reasoning Models"></a>Beyond â€˜Aha!â€™: Toward Systematic Meta-Abilities Alignment in Large   Reasoning Models</h2><p><strong>Authors:Zhiyuan Hu, Yibo Wang, Hanze Dong, Yuhui Xu, Amrita Saha, Caiming Xiong, Bryan Hooi, Junnan Li</strong></p>
<p>Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referred to as the modelâ€™s â€œaha momentâ€. However, the timing and consistency of these emergent behaviors remain unpredictable and uncontrollable, limiting the scalability and reliability of LRMsâ€™ reasoning capabilities. To address these limitations, we move beyond reliance on prompts and coincidental â€œaha momentsâ€. Instead, we explicitly align models with three meta-abilities: deduction, induction, and abduction, using automatically generated, self-verifiable tasks. Our three stage-pipeline individual alignment, parameter-space merging, and domain-specific reinforcement learning, boosting performance by over 10% relative to instruction-tuned baselines. Furthermore, domain-specific RL from the aligned checkpoint yields an additional gain in performance ceiling for both 7B and 32B models across math, coding, and science benchmarks, demonstrating that explicit meta-ability alignment offers a scalable and dependable foundation for reasoning. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/zhiyuanhubj/Meta-Ability-Alignment">https://github.com/zhiyuanhubj/Meta-Ability-Alignment</a> </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å·²ç»å…·å¤‡æ½œåœ¨çš„é•¿é“¾æ€ç»´æ¨ç†èƒ½åŠ›ã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥å¶ç„¶æ¿€å‘é«˜çº§æ¨ç†è¡Œä¸ºï¼Œå¦‚è‡ªæˆ‘ä¿®æ­£ã€å›æº¯å’ŒéªŒè¯ç°è±¡ï¼Œè¿™äº›å¸¸è¢«çœ‹ä½œæ˜¯æ¨¡å‹çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚ç„¶è€Œï¼Œè¿™äº›çªå‘è¡Œä¸ºçš„æ—¶æœºå’Œä¸€è‡´æ€§ä»ç„¶ä¸å¯é¢„æµ‹å’Œä¸å¯æ§åˆ¶ï¼Œé™åˆ¶äº†LRMsæ¨ç†èƒ½åŠ›çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬ä¸å†ä¾èµ–æç¤ºå’Œå¶ç„¶çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚ç›¸åï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ã€å¯è‡ªæˆ‘éªŒè¯çš„ä»»åŠ¡ï¼Œæ˜ç¡®åœ°å°†æ¨¡å‹ä¸ä¸‰ç§å…ƒèƒ½åŠ›ï¼ˆæ¼”ç»ã€å½’çº³å’Œæº¯å› ï¼‰å¯¹é½ã€‚æˆ‘ä»¬çš„ä¸‰é˜¶æ®µç®¡é“åŒ…æ‹¬ä¸ªäººå¯¹é½ã€å‚æ•°ç©ºé—´åˆå¹¶å’Œé¢†åŸŸç‰¹å®šå¼ºåŒ–å­¦ä¹ ï¼Œç›¸å¯¹äºæŒ‡ä»¤è°ƒæ•´åŸºå‡†çº¿ï¼Œæ€§èƒ½æé«˜äº†10%ä»¥ä¸Šã€‚æ­¤å¤–ï¼Œä»å¯¹é½æ£€æŸ¥ç‚¹è¿›è¡Œçš„é¢†åŸŸç‰¹å®šRLä¸º7Bå’Œ32Bæ¨¡å‹åœ¨æ•°å­¦ã€ç¼–ç å’Œç§‘å­¦åŸºå‡†æµ‹è¯•æ–¹é¢çš„æ€§èƒ½ä¸Šé™å¸¦æ¥äº†é¢å¤–æå‡ï¼Œè¡¨æ˜æ˜ç¡®çš„å…ƒèƒ½åŠ›å¯¹é½ä¸ºæ¨ç†æä¾›äº†å¯æ‰©å±•å’Œå¯é çš„åŸºçŸ³ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhiyuanhubj/Meta-Ability-Alignment%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zhiyuanhubj/Meta-Ability-Alignmentæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10554v2">PDF</a> In Progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å·²å…·å¤‡æ½œåœ¨çš„é•¿é“¾æ€ç»´æ¨ç†èƒ½åŠ›ã€‚å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯ä»¥å¶ç„¶æ¿€å‘è‡ªæˆ‘ä¿®æ­£ã€å›æº¯å’ŒéªŒè¯ç­‰é«˜çº§æ¨ç†è¡Œä¸ºï¼Œè¿™äº›è¡Œä¸ºé€šå¸¸è¢«è§†ä¸ºæ¨¡å‹çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ã€‚ç„¶è€Œï¼Œè¿™äº›çªå‘è¡Œä¸ºçš„æ—¶æœºå’Œä¸€è‡´æ€§å°šä¸å¯é¢„æµ‹å’Œæ§åˆ¶ï¼Œé™åˆ¶äº†LRMsæ¨ç†èƒ½åŠ›çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§ã€‚ä¸ºè§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬ä¸å†ä¾èµ–æç¤ºå’Œå¶ç„¶çš„â€œé¡¿æ‚Ÿæ—¶åˆ»â€ï¼Œè€Œæ˜¯æ˜ç¡®åœ°å°†æ¨¡å‹ä¸æ¼”ç»ã€å½’çº³å’Œæº¯å› ä¸‰ç§å…ƒèƒ½åŠ›å¯¹é½ï¼Œä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„è‡ªæˆ‘éªŒè¯ä»»åŠ¡ã€‚æˆ‘ä»¬çš„ä¸‰é˜¶æ®µç®¡é“åŒ…æ‹¬ä¸ªä½“å¯¹é½ã€å‚æ•°ç©ºé—´åˆå¹¶å’Œé¢†åŸŸç‰¹å®šå¼ºåŒ–å­¦ä¹ ï¼Œç›¸è¾ƒäºæŒ‡ä»¤è°ƒä¼˜åŸºå‡†çº¿ï¼Œæ€§èƒ½æå‡è¶…è¿‡10%ã€‚æ­¤å¤–ï¼Œä»å¯¹é½æ£€æŸ¥ç‚¹è¿›è¡Œçš„é¢†åŸŸç‰¹å®šRLä¸ºæ•°å­¦ã€ç¼–ç å’Œç§‘å­¦åŸºå‡†æµ‹è¯•ä¸­çš„7Bå’Œ32Bæ¨¡å‹æä¾›äº†é¢å¤–çš„æ€§èƒ½æå‡ï¼Œè¡¨æ˜æ˜ç¡®çš„å…ƒèƒ½åŠ›å¯¹é½ä¸ºæ¨ç†æä¾›äº†å¯æ‰©å±•å’Œå¯é çš„åŸºçŸ³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹å·²å…·æœ‰æ½œåœ¨çš„é•¿é“¾æ€ç»´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ å¯ä»¥æ¿€å‘æ¨¡å‹çš„é«˜çº§æ¨ç†è¡Œä¸ºï¼Œå¦‚è‡ªæˆ‘ä¿®æ­£å’ŒéªŒè¯ã€‚</li>
<li>å½“å‰æ–¹æ³•é¢ä¸´æ¨ç†è¡Œä¸ºä¸å¯é¢„æµ‹å’Œéš¾ä»¥æ§åˆ¶çš„å±€é™æ€§ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºä¸æ¼”ç»ã€å½’çº³å’Œæº¯å› ä¸‰ç§å…ƒèƒ½åŠ›å¯¹é½çš„æ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„è‡ªæˆ‘éªŒè¯ä»»åŠ¡ï¼Œé€šè¿‡ä¸‰é˜¶æ®µç®¡é“æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é¢†åŸŸç‰¹å®šå¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ä¸Šé™ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦ã€ç¼–ç å’Œç§‘å­¦é¢†åŸŸã€‚</li>
<li>æ˜ç¡®çš„å…ƒèƒ½åŠ›å¯¹é½ä¸ºæ¨ç†æä¾›äº†å¯æ‰©å±•å’Œå¯é çš„åŸºçŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-06ccda022f12fc421fd1d27803c963f8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05c9191149f0e07ab5e9327a2ffddf0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67364ee229c668a9d57b2917a5199f2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e47eedfe9ea54088ffebaa4165e57f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fa71ae42585f5d0c74b2f959e8c0f52a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DRA-GRPO-Exploring-Diversity-Aware-Reward-Adjustment-for-R1-Zero-Like-Training-of-Large-Language-Models"><a href="#DRA-GRPO-Exploring-Diversity-Aware-Reward-Adjustment-for-R1-Zero-Like-Training-of-Large-Language-Models" class="headerlink" title="DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like   Training of Large Language Models"></a>DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like   Training of Large Language Models</h2><p><strong>Authors:Xiwen Chen, Wenhui Zhu, Peijie Qiu, Xuanzhao Dong, Hao Wang, Haiyu Wu, Huayu Li, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi</strong></p>
<p>Recent advances in reinforcement learning for language model post-training, such as Group Relative Policy Optimization (GRPO), have shown promise in low-resource settings. However, GRPO typically relies on solution-level and scalar reward signals that fail to capture the semantic diversity among sampled completions. This leads to what we identify as a diversity-quality inconsistency, where distinct reasoning paths may receive indistinguishable rewards. To address this limitation, we propose $\textit{Diversity-aware Reward Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity into the reward computation. DRA uses Submodular Mutual Information (SMI) to downweight redundant completions and amplify rewards for diverse ones. This encourages better exploration during learning, while maintaining stable exploitation of high-quality samples. Our method integrates seamlessly with both GRPO and its variant DR.<del>GRPO, resulting in $\textit{DRA-GRPO}$ and $\textit{DGA-DR.</del>GRPO}$. We evaluate our method on five mathematical reasoning benchmarks and find that it outperforms recent strong baselines. It achieves state-of-the-art performance with an average accuracy of 58.2%, using only 7,000 fine-tuning samples and a total training cost of approximately $55. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xiwenc1/DRA-GRPO">https://github.com/xiwenc1/DRA-GRPO</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¼ºåŒ–å­¦ä¹ åœ¨è¯­è¨€æ¨¡å‹åè®­ç»ƒæ–¹é¢çš„è¿›å±•ï¼Œå¦‚ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œåœ¨ä½èµ„æºç¯å¢ƒä¸­æ˜¾ç¤ºå‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒGRPOé€šå¸¸ä¾èµ–äºè§£å†³æ–¹æ¡ˆçº§åˆ«å’Œæ ‡é‡å¥–åŠ±ä¿¡å·ï¼Œè¿™äº›ä¿¡å·æ— æ³•æ•è·é‡‡æ ·å®Œæˆçš„è¯­ä¹‰å¤šæ ·æ€§ã€‚è¿™å¯¼è‡´äº†æˆ‘ä»¬æ‰€è¯´çš„å¤šæ ·æ€§è´¨é‡ä¸ä¸€è‡´é—®é¢˜ï¼Œä¸åŒçš„æ¨ç†è·¯å¾„å¯èƒ½ä¼šè·å¾—æ— æ³•åŒºåˆ†çš„å¥–åŠ±ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œå¤šæ ·æ€§æ„ŸçŸ¥å¥–åŠ±è°ƒæ•´â€ï¼ˆDRAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¾å¼åœ°å°†è¯­ä¹‰å¤šæ ·æ€§çº³å…¥å¥–åŠ±è®¡ç®—ä¸­ã€‚DRAä½¿ç”¨å­æ¨¡å—äº’ä¿¡æ¯ï¼ˆSMIï¼‰æ¥é™ä½å†—ä½™å®Œæˆçš„æƒé‡ï¼Œå¹¶æ”¾å¤§å¤šæ ·åŒ–å®Œæˆçš„å¥–åŠ±ã€‚è¿™é¼“åŠ±äº†æ›´å¥½çš„å­¦ä¹ è¿‡ç¨‹æ¢ç´¢ï¼ŒåŒæ—¶ä¿æŒå¯¹é«˜è´¨é‡æ ·æœ¬çš„ç¨³å®šåˆ©ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸GRPOåŠå…¶å˜ä½“DR.GRPOæ— ç¼é›†æˆï¼Œå½¢æˆDRA-GRPOå’ŒDGA-DR.GRPOã€‚æˆ‘ä»¬åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°å®ƒè¶…è¶Šäº†æœ€è¿‘çš„å¼ºå¤§åŸºå‡†æµ‹è¯•ã€‚ä½¿ç”¨ä»…7000ä¸ªå¾®è°ƒæ ·æœ¬å’Œå¤§çº¦55çš„æ€»è®­ç»ƒæˆæœ¬ï¼Œå®ƒè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹³å‡å‡†ç¡®ç‡è¾¾åˆ°äº†58.2%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiwenc1/DRA-GRPO%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xiwenc1/DRA-GRPOæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.09655v2">PDF</a> </p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨è¯­è¨€æ¨¡å‹åè®­ç»ƒæ–¹é¢ï¼Œæœ€æ–°çš„å‘å±•å¦‚GRPOå·²åœ¨ä½èµ„æºè®¾ç½®é¢†åŸŸæ˜¾ç¤ºå‡ºå¸Œæœ›ã€‚ç„¶è€Œï¼ŒGRPOä¾èµ–äºè§£å†³æ–¹æ¡ˆçº§åˆ«çš„æ ‡é‡å¥–åŠ±ä¿¡å·ï¼Œæ— æ³•æ•æ‰é‡‡æ ·è¡¥å…¨çš„è¯­ä¹‰å¤šæ ·æ€§ï¼Œå¯¼è‡´å‡ºç°å¤šæ ·æ€§è´¨é‡ä¸ä¸€è‡´çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†DRAï¼ˆå¤šæ ·æ€§æ„ŸçŸ¥å¥–åŠ±è°ƒæ•´ï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†è¯­ä¹‰å¤šæ ·æ€§æ˜¾å¼åœ°çº³å…¥å¥–åŠ±è®¡ç®—ä¸­ã€‚DRAä½¿ç”¨SMIï¼ˆå­æ¨¡å—äº’ä¿¡æ¯ï¼‰æ¥é™ä½å†—ä½™è¡¥å…¨çš„æƒé‡å¹¶æ”¾å¤§å¯¹å¤šæ ·åŒ–è¡¥å…¨çš„å¥–åŠ±ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿå¾ˆå¥½åœ°ä¸GRPOåŠå…¶å˜ä½“DR.GRPOç›¸ç»“åˆï¼Œå½¢æˆDRA-GRPOå’ŒDGA-DR.GRPOã€‚æˆ‘ä»¬åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå‘ç°å®ƒä¼˜äºæœ€è¿‘çš„å¼ºå¤§åŸºçº¿ã€‚ä½¿ç”¨ä»…7000ä¸ªå¾®è°ƒæ ·æœ¬å’Œçº¦55çš„æ€»è®­ç»ƒæˆæœ¬ï¼Œå®ƒå®ç°äº†å¹³å‡å‡†ç¡®ç‡ä¸º58.2%çš„æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨ä½èµ„æºè®¾ç½®ä¸­çš„è¯­è¨€æ¨¡å‹åè®­ç»ƒå±•ç°å‡ºæ½œåŠ›ï¼Œå°¤å…¶æ˜¯GRPOæ–¹æ³•ã€‚</li>
<li>GRPOå­˜åœ¨å¤šæ ·æ€§è´¨é‡ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå› ä¸ºå®ƒä¸»è¦ä¾èµ–è§£å†³æ–¹æ¡ˆçº§åˆ«çš„æ ‡é‡å¥–åŠ±ä¿¡å·ã€‚</li>
<li>æå‡ºçš„DRAæ–¹æ³•é€šè¿‡æ˜ç¡®è€ƒè™‘è¯­ä¹‰å¤šæ ·æ€§æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½¿ç”¨SMIæ¥å¹³è¡¡å¥–åŠ±å’Œè°ƒæ•´æƒé‡ã€‚</li>
<li>DRAä¸GRPOåŠå…¶å˜ä½“æ— ç¼é›†æˆï¼Œå½¢æˆæ–°çš„æ–¹æ³•DRA-GRPOå’ŒDGA-DR.GRPOã€‚</li>
<li>åœ¨äº”ä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒDRAæ–¹æ³•è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºå…¶ä»–åŸºçº¿æ–¹æ³•ã€‚</li>
<li>ä½¿ç”¨æœ‰é™çš„å¾®è°ƒæ ·æœ¬å’Œè¾ƒä½çš„è®­ç»ƒæˆæœ¬ï¼ŒDRAè¾¾åˆ°äº†è¾ƒé«˜çš„å¹³å‡å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.09655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5a76552000073b9b82569ecbe8d9b3f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d5792f734516c33d332c216b883d83e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e539a1f490ce62eb9dc36d2d41938e6b.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="RM-R1-Reward-Modeling-as-Reasoning"><a href="#RM-R1-Reward-Modeling-as-Reasoning" class="headerlink" title="RM-R1: Reward Modeling as Reasoning"></a>RM-R1: Reward Modeling as Reasoning</h2><p><strong>Authors:Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, Hanghang Tong, Heng Ji</strong></p>
<p>Reward modeling is essential for aligning large language models with human preferences through reinforcement learning from human feedback. To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. Inspired by recent advances of long chain-of-thought on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RMs interpretability and performance. To this end, we introduce a new class of generative reward models - Reasoning Reward Models (ReasRMs) - which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. RM-R1 features a chain-of-rubrics (CoR) mechanism - self-generating sample-level chat rubrics or math&#x2F;code solutions, and evaluating candidate responses against them. The training of RM-R1 consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. Empirically, our models achieve state-of-the-art performance across three reward model benchmarks on average, outperforming much larger open-weight models (e.g., INF-ORM-Llama3.1-70B) and proprietary ones (e.g., GPT-4o) by up to 4.9%. Beyond final performance, we perform thorough empirical analyses to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six REASRM models along with code and data at <a target="_blank" rel="noopener" href="https://github.com/RM-R1-UIUC/RM-R1">https://github.com/RM-R1-UIUC/RM-R1</a>. </p>
<blockquote>
<p>å¥–åŠ±å»ºæ¨¡å¯¹äºé€šè¿‡å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆä¸­å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½è¿›è¡Œå¯¹é½è‡³å…³é‡è¦ã€‚ä¸ºäº†æä¾›å‡†ç¡®çš„å¥–åŠ±ä¿¡å·ï¼Œå¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰åº”åœ¨åˆ†é…åˆ†æ•°æˆ–åˆ¤æ–­ä¹‹å‰æ¿€å‘æ·±å…¥æ€è€ƒå¹¶è¿›è¡Œå¯è§£é‡Šçš„æ¨ç†ã€‚å—è¿‘æœŸé•¿é“¾æ€ç»´åœ¨æ¨ç†å¯†é›†å‹ä»»åŠ¡ä¸Šè¿›æ­¥çš„å¯å‘ï¼Œæˆ‘ä»¬å‡è®¾å¹¶å°†éªŒè¯å°†æ¨ç†èƒ½åŠ›èå…¥å¥–åŠ±å»ºæ¨¡ä¼šæ˜¾è‘—å¢å¼ºRMçš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç±»æ–°çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹â€”â€”æ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆReasRMsï¼‰ï¼Œå®ƒå°†å¥–åŠ±å»ºæ¨¡åˆ¶å®šä¸ºæ¨ç†ä»»åŠ¡ã€‚æˆ‘ä»¬æå‡ºäº†é¢å‘æ¨ç†çš„è®­ç»ƒç®¡é“ï¼Œå¹¶è®­ç»ƒäº†ä¸€ç³»åˆ—ReasRMsï¼Œå³RM-R1ã€‚RM-R1çš„ç‰¹ç‚¹æ˜¯æ‹¥æœ‰é“¾å¼æçº²ï¼ˆCoRï¼‰æœºåˆ¶â€”â€”è‡ªæˆ‘ç”Ÿæˆæ ·æœ¬çº§èŠå¤©æçº²æˆ–æ•°å­¦&#x2F;ä»£ç è§£å†³æ–¹æ¡ˆï¼Œå¹¶æ®æ­¤è¯„ä¼°å€™é€‰å“åº”ã€‚RM-R1çš„è®­ç»ƒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®é˜¶æ®µï¼šï¼ˆ1ï¼‰é«˜è´¨é‡æ¨ç†é“¾çš„è’¸é¦ï¼›ï¼ˆ2ï¼‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚ç»éªŒä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸‰ä¸ªå¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œä¼˜äºè¾ƒå¤§çš„å¼€æºæ¨¡å‹ï¼ˆä¾‹å¦‚INF-ORM-Llama3.1-70Bï¼‰å’Œä¸“æœ‰æ¨¡å‹ï¼ˆä¾‹å¦‚GPT-4oï¼‰ï¼Œæœ€é«˜æå‡è¾¾4.9%ã€‚é™¤äº†æœ€ç»ˆæ€§èƒ½ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å…¨é¢çš„ç»éªŒåˆ†æï¼Œä»¥äº†è§£æˆåŠŸè®­ç»ƒReasRMçš„å…³é”®è¦ç´ ã€‚ä¸ºäº†æ–¹ä¾¿æœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/RM-R1-UIUC/RM-R1%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E5%85%AD%E4%B8%AAREASRM%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8A%E7%9B%B8%E5%85%B3%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/RM-R1-UIUC/RM-R1ä¸Šå‘å¸ƒäº†å…­ä¸ªREASRMæ¨¡å‹ä»¥åŠç›¸å…³çš„ä»£ç å’Œæ•°æ®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02387v3">PDF</a> 25 pages, 8 figures</p>
<p><strong>Summary</strong></p>
<p>å¥–åŠ±å»ºæ¨¡å¯¹äºé€šè¿‡å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆå¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½è‡³å…³é‡è¦ã€‚é€šè¿‡ç»“åˆæ¨ç†èƒ½åŠ›è¿›è¡Œå¥–åŠ±å»ºæ¨¡ï¼Œå¯æ˜¾è‘—æé«˜å¥–åŠ±æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚æœ¬æ–‡æå‡ºäº†æ–°ä¸€ä»£çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹â€”â€”æ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆReasRMsï¼‰ï¼Œå°†å¥–åŠ±å»ºæ¨¡è¡¨è¿°ä¸ºæ¨ç†ä»»åŠ¡ã€‚é€šè¿‡é¢å‘æ¨ç†çš„è®­ç»ƒç®¡é“ï¼Œè®­ç»ƒå‡ºä¸€ç³»åˆ—ReasRMsï¼Œå…¶ä¸­RM-R1æ¨¡å‹å…·æœ‰é“¾å¼rubricsï¼ˆCoRï¼‰æœºåˆ¶ï¼Œèƒ½å¤Ÿè‡ªæˆ‘ç”Ÿæˆæ ·æœ¬çº§åˆ«çš„èŠå¤©rubricsæˆ–æ•°å­¦&#x2F;ä»£ç è§£å†³æ–¹æ¡ˆï¼Œå¹¶å¯¹å€™é€‰å“åº”è¿›è¡Œè¯„ä¼°ã€‚è¯¥æ¨¡å‹åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé«˜è´¨é‡æ¨ç†é“¾çš„è’¸é¦å’Œå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ã€‚åœ¨ä¸‰ä¸ªå¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRM-R1æ¨¡å‹å¹³å‡æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œä¼˜äºå¤§å‹å¼€æºæ¨¡å‹ï¼ˆå¦‚INF-ORM-Llama 3.1-70Bï¼‰å’Œä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰ï¼Œæœ€é«˜æå‡è¾¾4.9%ã€‚é™¤äº†æœ€ç»ˆæ€§èƒ½å¤–ï¼Œæœ¬æ–‡è¿˜è¿›è¡Œäº†å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œä»¥äº†è§£æˆåŠŸçš„ReasRMè®­ç»ƒçš„å…³é”®è¦ç´ ã€‚ä¸ºä¾¿äºæœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/RM-R1-UIUC/RM-R1%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86%E5%85%AD%E4%B8%AAREASRM%E6%A8%A1%E5%9E%8B%E5%8F%8A%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E3%80%82">https://github.com/RM-R1-UIUC/RM-R1ä¸Šå‘å¸ƒäº†å…­ä¸ªREASRMæ¨¡å‹åŠç›¸å…³ä»£ç å’Œæ•°æ®ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±å»ºæ¨¡å¯¹äºé€šè¿‡å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆä¸­ä½¿å¤§å‹è¯­è¨€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½è‡³å…³é‡è¦ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°å‹çš„ç”Ÿæˆå¥–åŠ±æ¨¡å‹â€”â€”æ¨ç†å¥–åŠ±æ¨¡å‹ï¼ˆReasRMsï¼‰ï¼Œå°†å¥–åŠ±å»ºæ¨¡ä½œä¸ºæ¨ç†ä»»åŠ¡ã€‚</li>
<li>RM-R1æ¨¡å‹å…·æœ‰é“¾å¼rubricsï¼ˆCoRï¼‰æœºåˆ¶ï¼Œèƒ½å¤Ÿè‡ªæˆ‘ç”Ÿæˆæ ·æœ¬çº§åˆ«çš„èŠå¤©rubricsæˆ–æ•°å­¦&#x2F;ä»£ç è§£å†³æ–¹æ¡ˆã€‚</li>
<li>RM-R1æ¨¡å‹çš„è®­ç»ƒåŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé«˜è´¨é‡æ¨ç†é“¾çš„è’¸é¦å’Œå¼ºåŒ–å­¦ä¹ ä½¿ç”¨å¯éªŒè¯å¥–åŠ±ã€‚</li>
<li>RM-R1æ¨¡å‹åœ¨ä¸‰ä¸ªå¥–åŠ±æ¨¡å‹åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼Œä¼˜äºå…¶ä»–å¤§å‹æ¨¡å‹ã€‚</li>
<li>é™¤äº†æœ€ç»ˆæ€§èƒ½å¤–ï¼Œæ–‡ç« è¿˜è¯¦ç»†æ¢è®¨äº†æˆåŠŸçš„ReasRMè®­ç»ƒçš„å…³é”®è¦ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7cf3c15fc15c72d686cc761873535ea6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e444662af72a79abe2367f82ec9219b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65a7064fab6881124dcbe9a9fb233d53.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b07804023c6b554e5d7de9c03f771c03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b11a41ed5c54519434e59a435e73cda.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="HyperTree-Planning-Enhancing-LLM-Reasoning-via-Hierarchical-Thinking"><a href="#HyperTree-Planning-Enhancing-LLM-Reasoning-via-Hierarchical-Thinking" class="headerlink" title="HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking"></a>HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking</h2><p><strong>Authors:Runquan Gui, Zhihai Wang, Jie Wang, Chi Ma, Huiling Zhen, Mingxuan Yuan, Jianye Hao, Defu Lian, Enhong Chen, Feng Wu</strong></p>
<p>Recent advancements have significantly enhanced the performance of large language models (LLMs) in tackling complex reasoning tasks, achieving notable success in domains like mathematical and logical reasoning. However, these methods encounter challenges with complex planning tasks, primarily due to extended reasoning steps, diverse constraints, and the challenge of handling multiple distinct sub-tasks. To address these challenges, we propose HyperTree Planning (HTP), a novel reasoning paradigm that constructs hypertree-structured planning outlines for effective planning. The hypertree structure enables LLMs to engage in hierarchical thinking by flexibly employing the divide-and-conquer strategy, effectively breaking down intricate reasoning steps, accommodating diverse constraints, and managing multiple distinct sub-tasks in a well-organized manner. We further introduce an autonomous planning framework that completes the planning process by iteratively refining and expanding the hypertree-structured planning outlines. Experiments demonstrate the effectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner benchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement over o1-preview. </p>
<blockquote>
<p>æœ€è¿‘çš„æŠ€æœ¯è¿›æ­¥æ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢çš„æ€§èƒ½ï¼Œåœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å¤„ç†å¤æ‚çš„è§„åˆ’ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯ç”±äºæ¨ç†æ­¥éª¤ç¹å¤šã€çº¦æŸå¤šæ ·ä»¥åŠå¤„ç†å¤šä¸ªä¸åŒå­ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†HyperTree Planningï¼ˆHTPï¼‰è¿™ä¸€æ–°çš„æ¨ç†èŒƒå¼ï¼Œä¸ºæœ‰æ•ˆè§„åˆ’æ„å»ºè¶…æ ‘ç»“æ„è§„åˆ’å¤§çº²ã€‚è¶…æ ‘ç»“æ„ä½¿LLMèƒ½å¤Ÿé€šè¿‡çµæ´»åœ°é‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥æ¥è¿›è¡Œåˆ†å±‚æ€è€ƒï¼Œæœ‰æ•ˆåœ°ç®€åŒ–å¤æ‚çš„æ¨ç†æ­¥éª¤ï¼Œé€‚åº”å„ç§çº¦æŸï¼Œå¹¶ä»¥äº•ç„¶æœ‰åºçš„æ–¹å¼ç®¡ç†å¤šä¸ªä¸åŒçš„å­ä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè‡ªä¸»è§„åˆ’æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£ç²¾åŒ–å’Œæ‰©å±•è¶…æ ‘ç»“æ„è§„åˆ’å¤§çº²æ¥å®Œæˆè§„åˆ’è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜HTPçš„æœ‰æ•ˆæ€§ï¼Œåœ¨TravelPlanneråŸºå‡†æµ‹è¯•ä¸­ä½¿ç”¨Gemini-1.5-Proå®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œè¾ƒo1-previewçš„æ€§èƒ½æé«˜äº†3.6å€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02322v2">PDF</a> arXiv admin note: text overlap with arXiv:2406.14228 by other authors</p>
<p><strong>Summary</strong>ï¼šè¿‘æœŸå‘å±•æ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢çš„æ€§èƒ½ï¼Œåœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸã€‚ç„¶è€Œï¼Œåœ¨å¤„ç†å¤æ‚çš„è§„åˆ’ä»»åŠ¡æ—¶ï¼Œè¿™äº›æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ‰©å±•æ¨ç†æ­¥éª¤ã€å¤šç§çº¦æŸå’Œåº”å¯¹å¤šä¸ªä¸åŒå­ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºHyperTree Planningï¼ˆHTPï¼‰è¿™ä¸€æ–°çš„æ¨ç†èŒƒå¼ï¼Œæ„å»ºç”¨äºæœ‰æ•ˆè§„åˆ’çš„è¶…æ ‘ç»“æ„è§„åˆ’å¤§çº²ã€‚è¶…æ ‘ç»“æ„ä½¿LLMèƒ½å¤Ÿé€šè¿‡çµæ´»åœ°é‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„ç­–ç•¥è¿›è¡Œåˆ†å±‚æ€è€ƒï¼Œæœ‰æ•ˆåœ°åˆ†è§£å¤æ‚çš„æ¨ç†æ­¥éª¤ï¼Œå®¹çº³å„ç§çº¦æŸï¼Œå¹¶ä»¥æœ‰åºçš„æ–¹å¼ç®¡ç†å¤šä¸ªä¸åŒçš„å­ä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè‡ªä¸»è§„åˆ’æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£åœ°ç»†åŒ–å’Œæ‰©å±•è¶…æ ‘ç»“æ„è§„åˆ’å¤§çº²æ¥å®Œæˆè§„åˆ’è¿‡ç¨‹ã€‚å®éªŒè¡¨æ˜HTPçš„æœ‰æ•ˆæ€§ï¼Œåœ¨TravelPlanneråŸºå‡†æµ‹è¯•ä¸Šä½¿ç”¨Gemini-1.5-Proå®ç°äº†ä¸šç•Œé¢†å…ˆç²¾åº¦ï¼Œç›¸è¾ƒäºo1-previewå®ç°äº†3.6å€çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¤æ‚æ¨ç†ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—å¢å¼ºçš„æ€§èƒ½ã€‚</li>
<li>åœ¨æ•°å­¦å’Œé€»è¾‘æ¨ç†ç­‰é¢†åŸŸï¼ŒLLMå·²å–å¾—æ˜¾è‘—æˆåŠŸã€‚</li>
<li>LLMåœ¨å¤„ç†å¤æ‚çš„è§„åˆ’ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ‰©å±•æ¨ç†æ­¥éª¤ã€å¤šç§çº¦æŸå’Œåº”å¯¹å¤šä¸ªä¸åŒå­ä»»åŠ¡ã€‚</li>
<li>æå‡ºHyperTree Planningï¼ˆHTPï¼‰è¿™ä¸€æ–°çš„æ¨ç†èŒƒå¼ä»¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>HTPåˆ©ç”¨è¶…æ ‘ç»“æ„è¿›è¡Œåˆ†å±‚æ€è€ƒï¼Œæœ‰æ•ˆåˆ†è§£å¤æ‚æ¨ç†æ­¥éª¤ï¼Œç®¡ç†å¤šç§çº¦æŸå’Œå¤šä¸ªå­ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥è‡ªä¸»è§„åˆ’æ¡†æ¶å®Œæˆè§„åˆ’è¿‡ç¨‹ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–è¶…æ ‘ç»“æ„è§„åˆ’å¤§çº²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02322">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-85c8f303c9e344a58f81a1749c5d524f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e62f2fbdb5d306912daf110139d1609.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a4d4c3d9cf5dbe68aeaafb6ef9238e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-508404d94a76488f201ecc9fa60753da.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Learning-to-Reason-under-Off-Policy-Guidance"><a href="#Learning-to-Reason-under-Off-Policy-Guidance" class="headerlink" title="Learning to Reason under Off-Policy Guidance"></a>Learning to Reason under Off-Policy Guidance</h2><p><strong>Authors:Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, Yue Zhang</strong></p>
<p>Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning with verifiable rewards~(\textit{RLVR}). However, existing \textit{RLVR} approaches are inherently &#96;&#96;on-policyâ€™â€™, limiting learning to a modelâ€™s own outputs and failing to acquire reasoning abilities beyond its initial capabilities. To address this issue, we introduce \textbf{LUFFY} (\textbf{L}earning to reason \textbf{U}nder o\textbf{FF}-polic\textbf{Y} guidance), a framework that augments \textit{RLVR} with off-policy reasoning traces. LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training. Specifically, LUFFY combines the Mixed-Policy GRPO framework, which has a theoretically guaranteed convergence rate, alongside policy shaping via regularized importance sampling to avoid superficial and rigid imitation during mixed-policy training. Compared with previous RLVR methods, LUFFY achieves an over \textbf{+6.4} average gain across six math benchmarks and an advantage of over \textbf{+6.2} points in out-of-distribution tasks. Most significantly, we show that LUFFY successfully trains weak models in scenarios where on-policy RLVR completely fails. These results provide compelling evidence that LUFFY transcends the fundamental limitations of on-policy RLVR and demonstrates the great potential of utilizing off-policy guidance in RLVR. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰çš„è¿›æ­¥è¡¨æ˜ï¼Œé€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»¥å‡ºç°å¤šæ­¥æ¨ç†å’Œè‡ªæˆ‘åæ€ç­‰å¤æ‚è¡Œä¸ºï¼ˆRLVRï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RLVRæ–¹æ³•æœ¬è´¨ä¸Šæ˜¯â€œåŸºäºç­–ç•¥çš„â€ï¼Œä»…é™äºæ¨¡å‹è‡ªèº«çš„è¾“å‡ºï¼Œæœªèƒ½è·å¾—è¶…å‡ºå…¶åˆå§‹èƒ½åŠ›çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LUFFYï¼ˆåœ¨åç¦»ç­–ç•¥æŒ‡å¯¼ä¸‹å­¦ä¹ æ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼ºRLVRçš„æ¡†æ¶ï¼Œé…å¤‡äº†åç¦»ç­–ç•¥çš„æ¨ç†è½¨è¿¹ã€‚LUFFYé€šè¿‡ç»“åˆåç¦»ç­–ç•¥çš„æ¼”ç¤ºå’ŒåŸºäºç­–ç•¥çš„æ»šåŠ¨è¾“å‡ºï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°åŠ¨æ€çš„æ¨¡ä»¿ä¸æ¢ç´¢å¹³è¡¡ã€‚å…·ä½“æ¥è¯´ï¼ŒLUFFYç»“åˆäº†å…·æœ‰ç†è®ºä¿è¯çš„æ”¶æ•›ç‡çš„Mixed-Policy GRPOæ¡†æ¶ï¼Œä»¥åŠé€šè¿‡æ­£åˆ™åŒ–é‡è¦æ€§é‡‡æ ·è¿›è¡Œç­–ç•¥å¡‘é€ ï¼Œä»¥é¿å…æ··åˆç­–ç•¥è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¡¨é¢å’ŒåƒµåŒ–æ¨¡ä»¿ã€‚ä¸ä¹‹å‰çš„RLVRæ–¹æ³•ç›¸æ¯”ï¼ŒLUFFYåœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šå¹³å‡æé«˜äº†+6.4åˆ†ä»¥ä¸Šï¼Œåœ¨è¶…å‡ºåˆ†å¸ƒçš„ä»»åŠ¡ä¸­ä¼˜åŠ¿è¶…è¿‡+6.2åˆ†ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å±•ç¤ºäº†LUFFYåœ¨çº¯åŸºäºç­–ç•¥çš„RLVRå®Œå…¨å¤±è´¥çš„æƒ…å†µä¸‹æˆåŠŸè®­ç»ƒå¼±æ¨¡å‹çš„åœºæ™¯ã€‚è¿™äº›ç»“æœæä¾›äº†å¼ºæœ‰åŠ›çš„è¯æ®è¡¨æ˜ï¼ŒLUFFYè¶…è¶Šäº†åŸºäºç­–ç•¥çš„RLVRçš„æ ¹æœ¬å±€é™æ€§ï¼Œå¹¶å±•ç¤ºäº†åœ¨RLVRä¸­ä½¿ç”¨åç¦»ç­–ç•¥æŒ‡å¯¼çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14945v4">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œé€šè¿‡å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å¯ä»¥äº§ç”Ÿå¦‚å¤šæ­¥éª¤æ¨ç†å’Œè‡ªæˆ‘åæ€ç­‰å¤æ‚è¡Œä¸ºã€‚ç„¶è€Œï¼Œç°æœ‰çš„RLVRæ–¹æ³•æœ¬è´¨ä¸Šæ˜¯â€œåŸºäºç­–ç•¥çš„â€ï¼Œä»…é™äºæ¨¡å‹è‡ªèº«çš„è¾“å‡ºï¼Œæ— æ³•è·å¾—è¶…è¶Šåˆå§‹èƒ½åŠ›çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LUFFYæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨RLVRä¸­åŠ å…¥äº†ç¦»ç­–ç•¥æ¨ç†è½¨è¿¹ã€‚LUFFYé€šè¿‡ç»“åˆç¦»ç­–ç•¥ç¤ºèŒƒå’ŒåŸºäºç­–ç•¥çš„æ»šåŠ¨æ¥åŠ¨æ€å¹³è¡¡æ¨¡ä»¿å’Œæ¢ç´¢ã€‚åœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•å’Œè·¨åˆ†å¸ƒä»»åŠ¡ä¸­ï¼ŒLUFFYç›¸è¾ƒäºä¹‹å‰çš„RLVRæ–¹æ³•å¹³å‡æé«˜äº†+6.4åˆ†å’Œåœ¨è¶…å‡ºåˆ†å¸ƒä»»åŠ¡ä¸­çš„ä¼˜åŠ¿è¶…è¿‡+6.2åˆ†ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨åœºæ™¯æ¨¡å‹ä¸­å±•ç¤ºäº†LUFFYå¦‚ä½•æˆåŠŸåœ°è®­ç»ƒå¼±æ¨¡å‹ï¼Œè€Œåœ¨è¿™äº›åœºæ™¯ä¸­åŸºäºç­–ç•¥çš„RLVRå®Œå…¨å¤±æ•ˆã€‚è¿™è¡¨æ˜LUFFYè¶…è¶Šäº†åŸºäºç­–ç•¥çš„RLVRçš„æ ¹æœ¬å±€é™æ€§ï¼Œå¹¶å±•ç¤ºäº†åœ¨ç¦»ç­–ç•¥æŒ‡å¯¼ä¸­ä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰å¯ä»¥é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å±•ç°å¤æ‚è¡Œä¸ºï¼Œå¦‚å¤šæ­¥éª¤æ¨ç†å’Œè‡ªæˆ‘åæ€ã€‚</li>
<li>ç°æœ‰RLVRæ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œä»…é™äºæ¨¡å‹è‡ªèº«è¾“å‡ºï¼Œéš¾ä»¥è·å¾—è¶…è¶Šåˆå§‹èƒ½åŠ›çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LUFFYæ¡†æ¶é€šè¿‡ç»“åˆç¦»ç­–ç•¥ç¤ºèŒƒå’ŒåŸºäºç­–ç•¥çš„æ»šåŠ¨æ¥å¹³è¡¡æ¨¡ä»¿å’Œæ¢ç´¢ã€‚</li>
<li>LUFFYå®ç°äº†åœ¨å…­ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•å’Œè·¨åˆ†å¸ƒä»»åŠ¡ä¸­çš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>LUFFYæˆåŠŸè®­ç»ƒäº†å¼±æ¨¡å‹ï¼Œåœ¨åŸºäºç­–ç•¥çš„RLVRå®Œå…¨å¤±æ•ˆçš„åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>LUFFYæ¡†æ¶å…·æœ‰ç†è®ºä¸Šä¿è¯çš„æ”¶æ•›ç‡ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æ­£åˆ™åŒ–é‡è¦æ€§é‡‡æ ·è¿›è¡Œç­–ç•¥å¡‘é€ ï¼Œä»¥é¿å…æ··åˆæ”¿ç­–è®­ç»ƒä¸­çš„è¡¨é¢å’ŒåƒµåŒ–æ¨¡ä»¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14945">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-646c71b1911f2da9f6d26a5a64244332.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-294b1e469b3169fe01a5a230e392d7de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a522597f0beaa0d4ec1630f2f1d09bf.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DeepSeek-vs-o3-mini-How-Well-can-Reasoning-LLMs-Evaluate-MT-and-Summarization"><a href="#DeepSeek-vs-o3-mini-How-Well-can-Reasoning-LLMs-Evaluate-MT-and-Summarization" class="headerlink" title="DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and   Summarization?"></a>DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and   Summarization?</h2><p><strong>Authors:Daniil Larionov, Sotaro Takeshita, Ran Zhang, Yanran Chen, Christoph Leiter, Zhipin Wang, Christian Greisinger, Steffen Eger</strong></p>
<p>Reasoning-enabled large language models (LLMs) excel in logical tasks, yet their utility for evaluating natural language generation remains unexplored. This study systematically compares reasoning LLMs with non-reasoning counterparts across machine translation and text summarization evaluation tasks. We evaluate eight models spanning state-of-the-art reasoning models (DeepSeek-R1, OpenAI o3), their distilled variants (8B-70B parameters), and equivalent non-reasoning LLMs. Experiments on WMT23 and SummEval benchmarks reveal architecture and task-dependent benefits: OpenAI o3-mini models show improved performance with increased reasoning on MT, while DeepSeek-R1 and generally underperforms compared to its non-reasoning variant except in summarization consistency evaluation. Correlation analysis demonstrates that reasoning token usage correlates with evaluation quality only in specific models, while almost all models generally allocate more reasoning tokens when identifying more quality issues. Distillation maintains reasonable performance up to 32B parameter models but degrades substantially at 8B scale. This work provides the first assessment of reasoning LLMs for NLG evaluation and comparison to non-reasoning models. We share our code to facilitate further research: <a target="_blank" rel="noopener" href="https://github.com/NL2G/reasoning-eval">https://github.com/NL2G/reasoning-eval</a>. </p>
<blockquote>
<p>å…·æœ‰æ¨ç†åŠŸèƒ½çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é€»è¾‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç„¶è€Œå®ƒä»¬å¯¹äºè‡ªç„¶è¯­è¨€ç”Ÿæˆçš„è¯„ä¼°çš„æ•ˆç”¨å°šæœªè¢«æ¢ç´¢ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¯”è¾ƒäº†æ¨ç†LLMä¸éæ¨ç†LLMåœ¨æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬æ‘˜è¦è¯„ä¼°ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬è¯„ä¼°äº†åŒ…æ‹¬æœ€æ–°æ¨ç†æ¨¡å‹ï¼ˆDeepSeek-R1ï¼ŒOpenAI o3ï¼‰ã€å®ƒä»¬çš„è’¸é¦å˜ä½“ï¼ˆå‚æ•°èŒƒå›´ä»8Båˆ°70Bï¼‰ï¼Œä»¥åŠä¸å®ƒä»¬ç­‰æ•ˆçš„éæ¨ç†LLMåœ¨å†…çš„å…«ä¸ªæ¨¡å‹ã€‚åœ¨WMT23å’ŒSummEvalåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒæ­ç¤ºäº†æ¶æ„å’Œä»»åŠ¡ä¾èµ–æ€§çš„å¥½å¤„ï¼šOpenAI o3 miniæ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘æ–¹é¢çš„æ€§èƒ½éšç€æ¨ç†èƒ½åŠ›çš„æå‡è€Œå¢å¼ºï¼Œè€ŒDeepSeek-R1åœ¨é™¤äº†æ‘˜è¦ä¸€è‡´æ€§è¯„ä¼°ä¹‹å¤–çš„ä»»åŠ¡ä¸Šé€šå¸¸è¡¨ç°ä¸å¦‚å…¶éæ¨ç†æ¨¡å‹ã€‚ç›¸å…³æ€§åˆ†æè¡¨æ˜ï¼Œæ¨ç†ä»¤ç‰Œçš„ä½¿ç”¨ä»…åœ¨ä¸ç‰¹å®šæ¨¡å‹çš„è¯„ä¼°è´¨é‡ç›¸å…³ï¼Œè€Œå‡ ä¹æ‰€æœ‰æ¨¡å‹åœ¨è¯†åˆ«æ›´å¤šè´¨é‡é—®é¢˜æ—¶éƒ½ä¼šåˆ†é…æ›´å¤šçš„æ¨ç†ä»¤ç‰Œã€‚è’¸é¦æŠ€æœ¯å¯ä»¥åœ¨å‚æ•°è§„æ¨¡è¾¾åˆ°32Bæ—¶ä¿æŒåˆç†çš„æ€§èƒ½ï¼Œä½†åœ¨8Bè§„æ¨¡æ—¶æ€§èƒ½ä¼šå¤§å¹…ä¸‹é™ã€‚è¿™é¡¹å·¥ä½œé¦–æ¬¡å¯¹ç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆè¯„ä¼°çš„æ¨ç†LLMè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å°†å…¶ä¸éæ¨ç†æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬åˆ†äº«äº†æˆ‘ä»¬çš„ä»£ç ï¼Œä»¥ä¾¿ä¿ƒè¿›è¿›ä¸€æ­¥çš„ç ”ç©¶ï¼š<a target="_blank" rel="noopener" href="https://github.com/NL2G/reasoning-eval%E3%80%82">https://github.com/NL2G/reasoning-evalã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.08120v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å…·å¤‡æ¨ç†èƒ½åŠ›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆè¯„ä¼°ä¸­çš„è¡¨ç°ã€‚æ–‡ç« ç³»ç»Ÿæ€§åœ°å¯¹æ¯”äº†æ¨ç†LLMsä¸éæ¨ç†LLMsåœ¨æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬æ‘˜è¦è¯„ä¼°ä»»åŠ¡ä¸Šçš„å·®å¼‚ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸åŒæ¨¡å‹å’Œä»»åŠ¡èƒŒæ™¯ä¸‹ï¼Œæ¨ç†èƒ½åŠ›å¯¹æ¨¡å‹è¡¨ç°çš„å½±å“æœ‰æ‰€ä¸åŒã€‚OpenAI o3-miniæ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­å±•ç°å‡ºæ¨ç†èƒ½åŠ›çš„æå‡ï¼Œè€ŒDeepSeek-R1æ¨¡å‹åˆ™åœ¨æ‘˜è¦ä¸€è‡´æ€§è¯„ä¼°ä¸­è¡¨ç°ç›¸å¯¹è¾ƒå¼±ã€‚æ­¤å¤–ï¼Œæ¨ç†ä»¤ç‰Œçš„ä½¿ç”¨ä¸è¯„ä»·è´¨é‡çš„ç›¸å…³æ€§ä»…åœ¨ç‰¹å®šæ¨¡å‹ä¸­ä½“ç°ï¼Œä¸”å¤§å¤šæ•°æ¨¡å‹åœ¨è¯†åˆ«è´¨é‡é—®é¢˜æ—¶å€¾å‘äºä½¿ç”¨æ›´å¤šæ¨ç†ä»¤ç‰Œã€‚è’¸é¦æŠ€æœ¯èƒ½åœ¨å‚æ•°è§„æ¨¡è¾ƒå°çš„æ¨¡å‹ä¸­ä¿æŒåˆç†æ€§èƒ½ï¼Œä½†åœ¨æ›´å¤§è§„æ¨¡å‚æ•°æ¨¡å‹ä¸­æ€§èƒ½ä¸‹é™æ˜æ˜¾ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹æ¨ç†LLMsè¿›è¡ŒNLGè¯„ä¼°å¹¶ä¸éæ¨ç†æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œç›¸å…³ä»£ç å·²å…¬å¼€åˆ†äº«ï¼Œä»¥æ¨åŠ¨è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†LLMsåœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆè¯„ä¼°ä¸­çš„è¡¨ç°è¢«ç³»ç»Ÿç ”ç©¶ï¼Œå¹¶ä¸éæ¨ç†LLMsè¿›è¡Œå¯¹æ¯”ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸åŒæ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘å’Œæ–‡æœ¬æ‘˜è¦è¯„ä¼°ä»»åŠ¡ä¸Šï¼Œæ¨ç†èƒ½åŠ›çš„å½±å“å­˜åœ¨å·®å¼‚ã€‚</li>
<li>OpenAI o3-miniæ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œæ¨ç†èƒ½åŠ›æå‡æ˜æ˜¾ã€‚</li>
<li>DeepSeek-R1æ¨¡å‹åœ¨æ‘˜è¦ä¸€è‡´æ€§è¯„ä¼°ä¸­è¡¨ç°ç›¸å¯¹è¾ƒå¼±ã€‚</li>
<li>æ¨ç†ä»¤ç‰Œçš„ä½¿ç”¨ä¸è¯„ä»·è´¨é‡çš„ç›¸å…³æ€§ä»…åœ¨ç‰¹å®šæ¨¡å‹ä¸­ä½“ç°ã€‚</li>
<li>å¤§å¤šæ•°æ¨¡å‹åœ¨è¯†åˆ«è´¨é‡é—®é¢˜æ—¶å€¾å‘äºä½¿ç”¨æ›´å¤šæ¨ç†ä»¤ç‰Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.08120">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7048251bd2748b15eeefd8cee4a48eeb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a2fda1002c6e988bc23c2399be487b65.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2310a1d61a5a13c8c183f4eec908b548.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-02/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-02/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-03/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3c64add917f44249521790e958396869.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-03  Agent-X Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic   Tasks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-03
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-01/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-5f65755fd334f8d186705757db0a7c2a.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-01  MMGT Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video   Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19939k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
