<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-10-07  No Need to Talk Asynchronous Mixture of Language Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-6d29b747b5dc56d14c00815acb2054c7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-10-07
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    28 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-10-07-更新"><a href="#2024-10-07-更新" class="headerlink" title="2024-10-07 更新"></a>2024-10-07 更新</h1><h2 id="No-Need-to-Talk-Asynchronous-Mixture-of-Language-Models"><a href="#No-Need-to-Talk-Asynchronous-Mixture-of-Language-Models" class="headerlink" title="No Need to Talk: Asynchronous Mixture of Language Models"></a>No Need to Talk: Asynchronous Mixture of Language Models</h2><p><strong>Authors:Anastasiia Filippova, Angelos Katharopoulos, David Grangier, Ronan Collobert</strong></p>
<p>We introduce SmallTalk LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Each model of the mixture specializes in distinct parts of the data distribution, without the need of high-bandwidth communication between the nodes training each model. At inference, a lightweight router directs a given sequence to a single expert, according to a short prefix. This inference scheme naturally uses a fraction of the parameters from the overall mixture model. Our experiments on language modeling demonstrate tha SmallTalk LM achieves significantly lower perplexity than dense model baselines for the same total training FLOPs and an almost identical inference cost. Finally, in our downstream evaluations we outperform the dense baseline on $75%$ of the tasks. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03529v1">PDF</a> 23 pages</p>
<p><strong>Summary</strong><br>提出SmallTalk LM，一种新型异步训练混合语言模型的方法，在保持低推理成本的同时，显著降低困惑度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SmallTalk LM是一种新的语言模型混合训练方法。</li>
<li>模型在数据分布的不同部分各有专长。</li>
<li>无需高带宽通信即可训练模型。</li>
<li>推理时使用轻量级路由器。</li>
<li>使用模型参数远少于整体模型。</li>
<li>SmallTalk LM在语言建模中表现优于密集模型。</li>
<li>在下游任务中，SmallTalk LM优于密集模型75%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>Title: 基于异步混合模型的文本生成技术研究</p>
</li>
<li><p>Authors: Anastasiia Filippova, Angelos Katharopoulos, David Grangier, Ronan Collobert</p>
</li>
<li><p>Affiliation: Apple</p>
</li>
<li><p>Keywords: language modeling, asynchronous training, mixture of experts, efficient inference, large language models</p>
</li>
<li><p>Urls: <a target="_blank" rel="noopener" href="https://github.com/">Github Link: None</a> (Note: The actual Github repository URL for the paper would be provided if available.)</p>
</li>
<li><p>Summary:</p>
<ul>
<li>(1) 研究背景：本文研究大型语言模型（LLM）的训练方法，旨在解决其训练过程中的通信成本问题以及推理效率问题。随着模型规模和训练数据的增加，性能得到了提高，但同时也带来了更高的训练和推理成本。特别是在分布式训练场景下，高带宽通信成为了一个瓶颈。因此，本文旨在探索一种能够在不依赖高速互联的情况下，实现高效训练和推理的方法。</li>
<li>(2) 过去的方法及问题：为了降低通信成本，研究者们已经提出了一些算法，如异步训练和梯度压缩。这些方法在一定程度上减少了通信开销，但仍然需要某种程度的梯度同步，并且与同步每一步的训练方法相比，其生成的模型在困惑度上往往表现较差。针对高效推理，稀疏参数激活技术如Switch Transformer混合专家（MoE）等已受到关注，但它们仍然需要为每个令牌做出路由决策，这要求快速互联并需要访问所有参数在RAM中。本文旨在结合异步训练的优势和混合模型的效率来解决上述问题。</li>
<li>(3) 研究方法：本文提出了一种基于异步混合模型的文本生成方法（SMALLTALK LM）。该方法结合了异步训练方法和稀疏激活技术，通过训练一系列独立的语言模型来构建混合模型。在训练过程中，每个专家专注于数据分布的不同部分，而不需要高带宽的通信。在推理时，一个轻量级的路由器根据短前缀将序列路由到最合适的专家。这种方法显著降低了训练和推理的计算成本，同时保持了模型的性能。</li>
<li>(4) 任务与性能：实验表明，SMALLTALK LM在语言建模任务上实现了更低的困惑度，且在大部分析下游任务上优于密集基线模型。此外，该方法的计算成本接近于密集基线模型，但模型性能得到了显著提高。总的来说，该方法的性能支持了其目标的实现。</li>
</ul>
</li>
<li><p>方法：</p>
</li>
</ol>
<p><em>(1)</em> 研究背景：随着大型语言模型（LLM）的发展，其训练和推理成本逐渐上升，成为实际应用中的瓶颈。特别是在分布式训练场景下，高带宽通信成为了一个主要问题。因此，文章提出了基于异步混合模型的文本生成方法，旨在在不依赖高速互联的情况下，实现高效训练和推理。</p>
<p><em>(2)</em> 方法概述：文章采用了结合异步训练方法和稀疏激活技术的策略。具体来说，它使用一系列独立的语言模型构建混合模型，每个专家专注于数据分布的不同部分。在训练过程中，采用异步方法，无需高带宽通信。在推理时，通过一个轻量级的路由器根据短前缀将序列路由到最合适的专家，从而显著降低了计算和通信成本。</p>
<p><em>(3)</em> 具体实现：文章提出的SMALLTALK LM方法结合了异步训练的优势和混合模型的效率。在训练阶段，采用稀疏激活技术训练多个独立的语言模型，这些模型并行工作并专注于数据的不同部分。在推理阶段，使用路由器选择最合适的模型进行预测，该路由器基于输入序列的前缀做出决策。这种设计显著减少了计算和通信开销，同时保持了模型的性能。</p>
<p><em>(4)</em> 实验验证：文章通过大量的实验验证了该方法的有效性。在语言建模任务上，SMALLTALK LM实现了较低的困惑度，并在大部分下游任务上优于基准模型。此外，该方法的计算成本接近基准模型，但模型性能得到了显著提高。<br>8. Conclusion:</p>
<ul>
<li>(1)该工作的意义在于提出了一种基于异步混合模型的文本生成方法，旨在解决大型语言模型在训练和推理过程中的高成本问题，为文本生成技术在实际应用中的推广提供了有力支持。</li>
<li>(2)创新点：文章结合了异步训练方法和稀疏激活技术，通过训练一系列独立的语言模型构建混合模型，降低了计算和通信成本。在性能上，该方法在语言建模任务上实现了较低的困惑度，并在大部分下游任务上优于基准模型。在工作量方面，文章进行了大量的实验验证，证明了该方法的有效性。然而，该方法的实现依赖于特定的硬件和算法优化，对于普通用户可能存在一定的使用门槛。此外，尽管该方法在降低通信成本方面取得了显著成果，但在分布式训练场景下的通信延迟问题仍有待进一步研究。</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d1e888b5e88c7d8df76efe00b6f6ef35.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-540ea09dc14d2955febf3f1f3c2bd91a.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-637994960114d223dbd91216bbebbff2.jpg" align="middle">
</details>




<h2 id="LaDTalk-Latent-Denoising-for-Synthesizing-Talking-Head-Videos-with-High-Frequency-Details"><a href="#LaDTalk-Latent-Denoising-for-Synthesizing-Talking-Head-Videos-with-High-Frequency-Details" class="headerlink" title="LaDTalk: Latent Denoising for Synthesizing Talking Head Videos with High   Frequency Details"></a>LaDTalk: Latent Denoising for Synthesizing Talking Head Videos with High   Frequency Details</h2><p><strong>Authors:Jian Yang, Xukun Wang, Wentao Wang, Guoming Li, Qihang Fang, Ruihong Yuan, Tianyang Wang, Jason Zhaoxin Fan</strong></p>
<p>Audio-driven talking head generation is a pivotal area within film-making and Virtual Reality. Although existing methods have made significant strides following the end-to-end paradigm, they still encounter challenges in producing videos with high-frequency details due to their limited expressivity in this domain. This limitation has prompted us to explore an effective post-processing approach to synthesize photo-realistic talking head videos. Specifically, we employ a pretrained Wav2Lip model as our foundation model, leveraging its robust audio-lip alignment capabilities. Drawing on the theory of Lipschitz Continuity, we have theoretically established the noise robustness of Vector Quantised Auto Encoders (VQAEs). Our experiments further demonstrate that the high-frequency texture deficiency of the foundation model can be temporally consistently recovered by the Space-Optimised Vector Quantised Auto Encoder (SOVQAE) we introduced, thereby facilitating the creation of realistic talking head videos. We conduct experiments on both the conventional dataset and the High-Frequency TalKing head (HFTK) dataset that we curated. The results indicate that our method, LaDTalk, achieves new state-of-the-art video quality and out-of-domain lip synchronization performance. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.00990v1">PDF</a> </p>
<p><strong>Summary</strong><br>利用预训练模型和空间优化VQAE提升音视频同步生成质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>音视频同步生成在影视和VR领域至关重要。</li>
<li>现有方法存在高频细节表达限制。</li>
<li>采用了预训练的Wav2Lip模型进行音频唇形对齐。</li>
<li>基于Lipschitz连续性理论，验证了VQAE的噪声鲁棒性。</li>
<li>引入SOVQAE修复高频纹理缺陷，提升视频质量。</li>
<li>在传统数据集和HFTK数据集上测试，表现优异。</li>
<li>LaDTalk方法在视频质量和跨域唇形同步上实现新突破。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>Title: 基于潜在表示的音频驱动说话人脸生成技术研究</p>
</li>
<li><p>Authors: (作者名字，这里需要根据实际论文填写)</p>
</li>
<li><p>Affiliation: (作者所在机构，这里需要根据实际论文填写)</p>
</li>
<li><p>Keywords: 音频驱动；说话人脸生成；潜在表示；同步网络；优化向量量化自动编码器</p>
</li>
<li><p>Urls: <a href="%E9%93%BE%E6%8E%A5%E5%9C%B0%E5%9D%80">论文链接</a>, <a href="Github:None">Github代码链接</a></p>
</li>
<li><p>Summary: </p>
<ul>
<li><p>(1)研究背景：随着媒体技术的发展，音频驱动说话人脸生成技术成为计算机视觉和语音处理领域的研究热点。该技术可以应用于电影特效、游戏开发、虚拟主播等领域。</p>
</li>
<li><p>(2)过去的方法及其问题：早期的方法主要基于传统的机器学习技术，但存在分辨率低、同步性差等问题。近期的方法如Wav2Lip虽然取得了较好的唇同步性能，但存在分辨率低和模糊效应等问题。</p>
</li>
<li><p>(3)研究方法：本研究提出了一种基于潜在表示的音频驱动说话人脸生成方法。首先，利用预训练的同步网络（SyncNet）进行音频与脸部的同步。然后，通过优化向量量化自动编码器（VQAE）实现低质量（LQ）脸部到高质量（HQ）脸部的转换。为提高噪声容忍能力，研究采用了特定的优化策略。</p>
</li>
<li><p>(4)任务与性能：本研究在说话人脸生成任务上取得了显著成果，实现了高分辨率、高同步性能的脸部生成。相较于以往的方法，该方法在性能上有了显著提升，尤其是唇同步性能和分辨率方面。实验结果支持了该方法的有效性。</p>
</li>
</ul>
</li>
</ol>
<p>以上内容需要根据实际论文内容进行相应的调整。<br>7. Methods:</p>
<pre><code>- (1) 研究首先介绍了音频驱动说话人脸生成技术的研究背景，概述了其在计算机视觉和语音处理领域的重要性以及潜在应用，如电影特效、游戏开发和虚拟主播等。

- (2) 对过去的研究方法进行了回顾，指出了传统方法存在的问题，如分辨率低和同步性差等。同时，对近期的方法如Wav2Lip进行了简要介绍，指出了其存在的问题，如分辨率低和模糊效应等。

- (3) 针对这些问题，本研究提出了一种基于潜在表示的音频驱动说话人脸生成方法。该方法包括以下步骤：
    a. 利用预训练的同步网络（SyncNet）进行音频与脸部的同步。该网络通过训练学习音频和脸部视频之间的对应关系，从而实现音频信号和脸部动作的同步。
    b. 通过优化向量量化自动编码器（VQAE）实现低质量（LQ）脸部到高质量（HQ）脸部的转换。VQAE是一种生成模型，能够学习脸部图像的有效表示，并通过优化策略将其转换为高质量的脸部图像。
    c. 为提高噪声容忍能力，研究采用了特定的优化策略，包括数据增强和鲁棒性损失函数等，以增强模型在复杂环境下的性能。

- (4) 最后，本研究对所提出的方法进行了实验验证，并在说话人脸生成任务上取得了显著成果。实验结果支持了该方法的有效性，表明该方法在性能上相较于以往的方法有了显著提升，尤其是唇同步性能和分辨率方面。
</code></pre>
<ol start="8">
<li>Conclusion:</li>
</ol>
<ul>
<li><p>(1)该论文研究具有重要的应用价值。音频驱动说话人脸生成技术在电影特效、游戏开发、虚拟主播等领域具有广泛的应用前景。该研究提出了一种新的方法，有助于提高这些领域的技术水平和用户体验。</p>
</li>
<li><p>(2)创新点：该论文提出了一种基于潜在表示的音频驱动说话人脸生成方法，通过结合预训练的同步网络和优化向量量化自动编码器，实现了高质量、高同步性能的脸部生成。该方法的创新点在于利用了潜在表示技术，提高了生成结果的质量和同步性能。</p>
</li>
<li><p>性能：该论文所提出的方法在说话人脸生成任务上取得了显著成果，实现了高分辨率、高同步性能的脸部生成。相较于以往的方法，该方法在性能上有了显著提升，尤其是唇同步性能和分辨率方面。实验结果支持了该方法的有效性。</p>
</li>
<li><p>工作量：该论文进行了充分的实验验证，并对所提出的方法进行了全面的评估。此外，论文还进行了相关的理论分析和推导，证明了所提出方法的有效性和优越性。因此，该论文的工作量较大，具有一定的研究深度。</p>
</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-1516487de9529ba2aab478b3da8d98af.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9f4e2c7129502f06c1ec8236cb9d2704.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-b62baf08dd9ddb6d134b80696fd9867e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-b92cab6c9a4be279765a7020dc7bdbcc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-387b872b1c16054779e978cda7bf4559.jpg" align="middle">
</details>




<h2 id="Alignment-Free-Training-for-Transducer-based-Multi-Talker-ASR"><a href="#Alignment-Free-Training-for-Transducer-based-Multi-Talker-ASR" class="headerlink" title="Alignment-Free Training for Transducer-based Multi-Talker ASR"></a>Alignment-Free Training for Transducer-based Multi-Talker ASR</h2><p><strong>Authors:Takafumi Moriya, Shota Horiguchi, Marc Delcroix, Ryo Masumura, Takanori Ashihara, Hiroshi Sato, Kohei Matsuura, Masato Mimura</strong></p>
<p>Extending the RNN Transducer (RNNT) to recognize multi-talker speech is essential for wider automatic speech recognition (ASR) applications. Multi-talker RNNT (MT-RNNT) aims to achieve recognition without relying on costly front-end source separation. MT-RNNT is conventionally implemented using architectures with multiple encoders or decoders, or by serializing all speakers’ transcriptions into a single output stream. The first approach is computationally expensive, particularly due to the need for multiple encoder processing. In contrast, the second approach involves a complex label generation process, requiring accurate timestamps of all words spoken by all speakers in the mixture, obtained from an external ASR system. In this paper, we propose a novel alignment-free training scheme for the MT-RNNT (MT-RNNT-AFT) that adopts the standard RNNT architecture. The target labels are created by appending a prompt token corresponding to each speaker at the beginning of the transcription, reflecting the order of each speaker’s appearance in the mixtures. Thus, MT-RNNT-AFT can be trained without relying on accurate alignments, and it can recognize all speakers’ speech with just one round of encoder processing. Experiments show that MT-RNNT-AFT achieves performance comparable to that of the state-of-the-art alternatives, while greatly simplifying the training process. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.20301v1">PDF</a> Submitted to ICASSP 2025</p>
<p><strong>Summary</strong><br>提出一种新型MT-RNNT训练方案，简化训练过程，实现多说话人语音识别。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MT-RNNT用于多说话人语音识别，降低前端分离成本。</li>
<li>两种传统MT-RNNT实现方式：多编码器&#x2F;解码器架构或序列化输出。</li>
<li>多编码器方式计算量大，序列化方式需外部ASR系统提供时间戳。</li>
<li>提出MT-RNNT-AFT方案，无需依赖精确对齐。</li>
<li>使用提示标记创建目标标签，反映说话人顺序。</li>
<li>MT-RNNT-AFT只需一轮编码处理即可识别所有说话人。</li>
<li>实验表明，MT-RNNT-AFT性能与现有方案相当。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p><strong>标题</strong>：Alignment-Free Training for Transducer-based Multi-Talker Automatic Speech Recognition（基于转换器的多说话人语音识别中的无对齐训练）<strong>中文翻译</strong>。</p>
</li>
<li><p><strong>作者</strong>：Takafumi Moriya（森雅隆夫）, Shota Horiguchi（广谷昇大）, Marc Delcroix（马克·德洛克洛瓦）, Ryo Masumura（柿子真人）, Takanori Ashihara（白石诚司）, Hiroshi Sato（佐藤宏）, Kohei Matsuura（松浦光辉）, Masato Mimura（海村正人）。他们都是NTT Corporation的成员。</p>
</li>
<li><p><strong>隶属机构</strong>：NTT Corporation（日本电信电话株式会社）。中文翻译。</p>
</li>
<li><p><strong>关键词</strong>：Speech Recognition（语音识别）, End-to-End（端到端技术）、Neural Transducer（神经网络转换器）、Multi-Talker（多说话人）、Alignment-Free Training（无对齐训练）。</p>
</li>
<li><p><strong>链接</strong>：很遗憾，论文尚未在GitHub上公开代码链接，所以填写为“Github: None”。若后续公开了代码链接，可以填写。关于论文链接请查阅相应的学术数据库或该论文发布的期刊官网。</p>
</li>
<li><p><strong>摘要</strong>：</p>
<ul>
<li><p>(1)研究背景：随着语音识别技术的发展，单说话人的语音识别已经取得了显著的进步。然而，在多说话人的场景下，尤其是当多个说话人的声音同时出现时，传统的语音识别方法性能不佳。为此，如何有效识别多个说话人的语音成为了一项重要的研究课题。文章在此背景下展开研究。</p>
</li>
<li><p>(2)过去的方法及其问题：为了解决多说话人语音识别的问题，已经提出了一些方法，包括使用多个编码器和解码器的方法以及序列化所有说话人的转录生成单一输出流的方法等。然而，这些方法存在计算量大、需要外部ASR系统进行精确的时间戳对齐等问题。文中提出的MT-RNNT虽然能在一定程度上解决这个问题，但仍需精确的对齐。所以提出了新方法来简化训练过程并提高识别性能。</p>
</li>
<li><p>(3)研究方法：针对上述问题，本文提出了一种基于转换器架构的无对齐训练方法MT-RNNT-AFT。该方法通过引入一个提示令牌来创建目标标签，该令牌对应于每个说话人的出现顺序。这种方法不需要精确的对齐信息即可训练模型并同时识别所有说话人的语音。在实验中还结合了知识蒸馏和语言模型集成等技术进一步提升识别性能。实验证明了所提出的方法在多说话人自动语音识别任务中的有效性。 </p>
</li>
<li><p>(4)任务与性能：本文的方法在多个说话人的自动语音识别任务上进行了实验验证，并与当前主流方法进行了比较。实验结果表明，所提出的方法在性能和计算效率上均取得了显著的进展。相较于过去的方法，所提出的方法更简单、计算量更小，并实现了与其他方法相近的性能甚至在某些情况下超过了它们。这表明该方法在多说话人自动语音识别任务中具有实际应用价值。</p>
</li>
</ul>
</li>
<li><p>方法论概述：</p>
</li>
</ol>
<p>该文主要研究了基于转换器架构的无对齐训练在多说话人自动语音识别中的应用。具体的方法论如下：</p>
<pre><code>- (1) 研究背景与问题定义：文章首先介绍了多说话人自动语音识别的重要性和挑战，特别是在多个说话人的声音同时出现时的识别问题。提出的方法论是为了解决这些问题而设计的。

- (2) 方法介绍：针对上述问题，文章提出了一种基于转换器架构的无对齐训练方法，名为MT-RNNT-AFT。该方法通过引入提示令牌来创建目标标签，该令牌对应于每个说话人的出现顺序，不需要精确的对齐信息即可训练模型并同时识别所有说话人的语音。同时结合了知识蒸馏和语言模型集成等技术进一步提升识别性能。实验证明了该方法的有效性。 

- (3) 实验设计与实施：文章通过一系列实验验证了所提出方法的有效性。实验设计包括模拟混合语音数据生成过程、模型训练过程以及识别性能评估过程等。同时采用了多种评价指标对模型性能进行定量和定性评估。实验结果表明所提出的方法在性能和计算效率上均取得了显著的进展。 

- (4) 知识蒸馏技术：除了上述方法外，文章还提出了一种基于知识蒸馏的改进方法，以进一步提升MT-RNNT-AFT的性能。该方法利用了模拟混合过程中产生的并行语音数据，通过计算伪标签和预测结果之间的损失函数来优化模型参数，从而提高模型的泛化能力和识别性能。实验结果表明这种改进方法能够有效地提高模型的识别准确率。 
</code></pre>
<p>通过以上步骤和方法论，该文章成功实现了一种基于转换器架构的无对齐训练的多说话人自动语音识别方法，具有实际应用价值。<br>8. Conclusion:</p>
<pre><code>- (1)该工作的重要性在于，它解决了多说话人自动语音识别中的一个重要问题，即在多个说话人的声音同时出现时，如何有效地识别每个说话人的语音。这项工作对于实现更智能、更自然的语音识别系统具有重要意义，可以广泛应用于语音识别、人机交互、智能助理等领域。

- (2)创新点：该文章提出了一种基于转换器架构的无对齐训练方法MT-RNNT-AFT，通过引入提示令牌来解决多说话人自动语音识别中的对齐问题，该方法具有创新性。性能：实验结果表明，该方法在多说话人自动语音识别任务上的性能表现优异，与现有方法相比，该方法更简单、计算量更小，且在某些情况下性能超过它们。工作量：文章通过一系列实验验证了所提出方法的有效性，并采用了多种评价指标对模型性能进行定量和定性评估，工作量较大。
</code></pre>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-e956553657a36fb1865b93f2194d8199.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-44b70d12a5d476518341a5e59f70dffb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-eed1afebc4f9e3cbdfe3e6be4e88b8b9.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-10d5f99e44232ae4f98eee86b254b7b2.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-21b7e2b9eed472735b979e505ebb8bd5.jpg" align="middle">
</details>




<h2 id="Diverse-Code-Query-Learning-for-Speech-Driven-Facial-Animation"><a href="#Diverse-Code-Query-Learning-for-Speech-Driven-Facial-Animation" class="headerlink" title="Diverse Code Query Learning for Speech-Driven Facial Animation"></a>Diverse Code Query Learning for Speech-Driven Facial Animation</h2><p><strong>Authors:Chunzhi Gu, Shigeru Kuriyama, Katsuya Hotta</strong></p>
<p>Speech-driven facial animation aims to synthesize lip-synchronized 3D talking faces following the given speech signal. Prior methods to this task mostly focus on pursuing realism with deterministic systems, yet characterizing the potentially stochastic nature of facial motions has been to date rarely studied. While generative modeling approaches can easily handle the one-to-many mapping by repeatedly drawing samples, ensuring a diverse mode coverage of plausible facial motions on small-scale datasets remains challenging and less explored. In this paper, we propose predicting multiple samples conditioned on the same audio signal and then explicitly encouraging sample diversity to address diverse facial animation synthesis. Our core insight is to guide our model to explore the expressive facial latent space with a diversity-promoting loss such that the desired latent codes for diversification can be ideally identified. To this end, building upon the rich facial prior learned with vector-quantized variational auto-encoding mechanism, our model temporally queries multiple stochastic codes which can be flexibly decoded into a diverse yet plausible set of speech-faithful facial motions. To further allow for control over different facial parts during generation, the proposed model is designed to predict different facial portions of interest in a sequential manner, and compose them to eventually form full-face motions. Our paradigm realizes both diverse and controllable facial animation synthesis in a unified formulation. We experimentally demonstrate that our method yields state-of-the-art performance both quantitatively and qualitatively, especially regarding sample diversity. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.19143v1">PDF</a> </p>
<p><strong>Summary</strong><br>该方法通过条件预测和多样性促进损失，实现了基于语音信号的多样化和可控的3D人脸动画生成。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>语音驱动的人脸动画追求与现实同步的3D人脸。</li>
<li>之前方法主要关注确定性系统的真实感，但面部运动的不确定性研究较少。</li>
<li>生成模型易于处理一对一映射，但在小数据集上实现多样化面部运动覆盖具挑战性。</li>
<li>本文提出基于同一音频信号预测多个样本并鼓励样本多样性。</li>
<li>模型通过多样性促进损失探索表达性面部潜在空间。</li>
<li>建立在向量量化变分自编码机制学习丰富的面部先验基础上。</li>
<li>模型通过时间查询多个随机代码，解码成多样化的面部运动。</li>
<li>模型按顺序预测不同面部部分，形成完整面部运动。</li>
<li>方法实现了多样化和可控的统一面部动画合成。</li>
<li>实验表明，该方法在样本多样性方面具有最佳性能。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<ol>
<li><p>标题：基于多样代码查询学习的面部动画合成研究<br>（标题翻译：Research on Facial Animation Synthesis Based on Diverse Code Query Learning）</p>
</li>
<li><p>作者：Chunzhi Gu（顾宸之），Shigeru Kuriyama（仓山升），Katsuya Hotta（北谷胜也）</p>
</li>
<li><p>隶属机构：顾宸之系日本丰桥技术大学计算机科学与工程系成员，仓山升和北谷胜也分别来自日本的一所大学。</p>
</li>
<li><p>关键词：多样面部动画合成、视听学习、面部部分控制</p>
</li>
<li><p>链接：论文链接（待补充），GitHub代码链接（待补充）</p>
</li>
<li><p>总结：</p>
<ul>
<li><p>(1)研究背景：随着虚拟数字人物在娱乐、游戏等领域的广泛应用，语音驱动的面部动画合成成为了研究热点。此前的方法主要关注于生成真实感的面部动画，但对于面部的多样性以及部分面部控制的研究相对较少。本文旨在解决这一问题。</p>
</li>
<li><p>(2)过去的方法及问题：早期的方法主要依赖手动调整，工作量较大且结果受限。当前主流方法采用深度神经网络进行面部动画合成，但大多为确定性生成，无法捕捉面部的多样性。此外，对面部各部分的独立控制也是一个挑战。</p>
</li>
<li><p>(3)研究方法：本文提出一种基于多样代码查询学习的面部动画合成方法。首先，利用向量量化变分自编码器构建面部先验模型。然后，设计模型以在给定语音信号条件下生成多个面部样本，并鼓励样本多样性。为此，引入了一种促进多样性的损失函数来指导模型探索面部潜在空间。此外，模型按序预测各面部部分，以实现对面部各部分的独立控制。</p>
</li>
<li><p>(4)任务与性能：本文的方法在面部动画合成任务上实现了多样性和可控性的统一。在小型数据集上，模型能够生成多样且逼真的面部动画。此外，通过对面部各部分的独立控制，模型能够生成具有相似唇部动作但上部面部变化多样的谈话面部动画。实验结果证明了该方法的有效性。</p>
</li>
</ul>
</li>
<li><p>方法论概述：</p>
</li>
</ol>
<p>本文提出了一种基于多样代码查询学习的面部动画合成方法。具体步骤如下：</p>
<ul>
<li><p>(1)研究背景：随着虚拟数字人物在娱乐、游戏等领域的广泛应用，语音驱动的面部动画合成成为了研究热点。早期的方法主要关注于生成真实感的面部动画，但对于面部的多样性以及部分面部控制的研究相对较少。本文旨在解决这一问题。</p>
</li>
<li><p>(2)构建面部先验模型：利用向量量化变分自编码器（VQ-VAE）构建面部先验模型。该模型可以学习面部数据的分布并生成高质量的面部纹理。</p>
</li>
<li><p>(3)生成多样面部样本：设计模型以在给定语音信号条件下生成多个面部样本，并鼓励样本多样性。为此，引入了一种促进多样性的损失函数来指导模型探索面部潜在空间。</p>
</li>
<li><p>(4)部分可控合成：将面部动画分解为多个部分（如嘴唇和上半脸），并为每个部分设计独立的模型和控制代码。通过按顺序预测各面部部分，实现对面部各部分的独立控制。</p>
</li>
<li><p>(5)训练策略：使用VQ-VAE优化损失函数，包括自我重建损失和量化损失，以监督模型的训练过程并丰富代码库。</p>
</li>
<li><p>(6)多样性和可控性合成：基于音频输入，模型以时间序列方式预测对应的离散潜在代码作为运动表示。通过引入多样性促进目标和掩码指导策略，实现合成多样性的同时保持音频保真度和对特定面部部分的控制。</p>
</li>
</ul>
<p>本文的方法在面部动画合成任务上实现了多样性和可控性的统一，能够在小型数据集上生成多样且逼真的面部动画。实验结果证明了该方法的有效性。<br>8. Conclusion:</p>
<ul>
<li><p>(1)该作品的意义在于解决了虚拟数字人物面部动画合成中的多样性和部分控制问题，为娱乐、游戏等领域提供更丰富、更真实的面部动画合成方法。</p>
</li>
<li><p>(2)创新点：本文提出了一种基于多样代码查询学习的面部动画合成方法，实现了面部动画合成中的多样性和可控性的统一。<br>性能：在小型数据集上，该方法能够生成多样且逼真的面部动画，且通过对面部各部分的独立控制，能够生成具有相似唇部动作但上部面部变化多样的谈话面部动画。实验结果证明了该方法的有效性。<br>工作量：文章对方法的实现进行了详细的描述，但关于实验的具体实施细节和数据处理量等具体工作量方面未做详细阐述。</p>
</li>
</ul>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-35a73dda42501ac65227235181297437.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-d5e289aecfe6511b453ff9b1a75ef689.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-2b9c5bf6572f8280a07d4dce0029c251.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-677a344324f7766cd4c896d2af6f670d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-694af360d4113e4d121c4ffa811ab1cb.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-6d29b747b5dc56d14c00815acb2054c7.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-10-07/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2024-10-07/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-10-07/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-02d078163a037b73fc794d356891be68.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2024-10-07  Variational Bayes Gaussian Splatting
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-10-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-09-30/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3a74c6c148317ca0fea74487b5271ff3.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-09-30  ReviveDiff A Universal Diffusion Model for Restoring Images in Adverse   Weather Conditions
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-09-30
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">6736.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
