<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  TimeStep Master Asymmetrical Mixture of Timestep LoRA Experts for   Versatile and Efficient Diffusion Models in Vision">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9bbc954f2d09b3c65bf6993d6cb1c1dc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-12-æ›´æ–°"><a href="#2025-03-12-æ›´æ–°" class="headerlink" title="2025-03-12 æ›´æ–°"></a>2025-03-12 æ›´æ–°</h1><h2 id="TimeStep-Master-Asymmetrical-Mixture-of-Timestep-LoRA-Experts-for-Versatile-and-Efficient-Diffusion-Models-in-Vision"><a href="#TimeStep-Master-Asymmetrical-Mixture-of-Timestep-LoRA-Experts-for-Versatile-and-Efficient-Diffusion-Models-in-Vision" class="headerlink" title="TimeStep Master: Asymmetrical Mixture of Timestep LoRA Experts for   Versatile and Efficient Diffusion Models in Vision"></a>TimeStep Master: Asymmetrical Mixture of Timestep LoRA Experts for   Versatile and Efficient Diffusion Models in Vision</h2><p><strong>Authors:Shaobin Zhuang, Yiwei Guo, Yanbo Ding, Kunchang Li, Xinyuan Chen, Yaohui Wang, Fangyikang Wang, Ying Zhang, Chen Li, Yali Wang</strong></p>
<p>Diffusion models have driven the advancement of vision generation over the past years. However, it is often difficult to apply these large models in downstream tasks, due to massive fine-tuning cost. Recently, Low-Rank Adaptation (LoRA) has been applied for efficient tuning of diffusion models. Unfortunately, the capabilities of LoRA-tuned diffusion models are limited, since the same LoRA is used for different timesteps of the diffusion process. To tackle this problem, we introduce a general and concise TimeStep Master (TSM) paradigm with two key fine-tuning stages. In the fostering stage (1-stage), we apply different LoRAs to fine-tune the diffusion model at different timestep intervals. This results in different TimeStep LoRA experts that can effectively capture different noise levels. In the assembling stage (2-stage), we design a novel asymmetrical mixture of TimeStep LoRA experts, via core-context collaboration of experts at multi-scale intervals. For each timestep, we leverage TimeStep LoRA expert within the smallest interval as the core expert without gating, and use experts within the bigger intervals as the context experts with time-dependent gating. Consequently, our TSM can effectively model the noise level via the expert in the finest interval, and adaptively integrate contexts from the experts of other scales, boosting the versatility of diffusion models. To show the effectiveness of our TSM paradigm, we conduct extensive experiments on three typical and popular LoRA-related tasks of diffusion models, including domain adaptation, post-pretraining, and model distillation. Our TSM achieves the state-of-the-art results on all these tasks, throughout various model structures (UNet, DiT and MM-DiT) and visual data modalities (Image, Video), showing its remarkable generalization capacity. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨è¿‡å»å‡ å¹´ä¸­æ¨åŠ¨äº†è§†è§‰ç”Ÿæˆçš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç”±äºå·¨å¤§çš„å¾®è°ƒæˆæœ¬ï¼Œè¿™äº›å¤§å‹æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨å¾€å¾€é¢ä¸´å›°éš¾ã€‚æœ€è¿‘ï¼Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å·²è¢«åº”ç”¨äºæ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆè°ƒæ•´ã€‚ç„¶è€Œï¼ŒLoRAè°ƒæ•´åçš„æ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›æ˜¯æœ‰é™çš„ï¼Œå› ä¸ºç›¸åŒçš„LoRAç”¨äºæ‰©æ•£è¿‡ç¨‹çš„ä¸åŒæ—¶é—´æ­¥é•¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é€šç”¨è€Œç®€æ´çš„TimeStep Masterï¼ˆTSMï¼‰èŒƒå¼ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®çš„å¾®è°ƒé˜¶æ®µã€‚åœ¨ä¿ƒè¿›é˜¶æ®µï¼ˆç¬¬ä¸€é˜¶æ®µï¼‰ï¼Œæˆ‘ä»¬å¯¹ä¸åŒçš„æ—¶é—´æ­¥é•¿é—´éš”åº”ç”¨ä¸åŒçš„LoRAsæ¥å¾®è°ƒæ‰©æ•£æ¨¡å‹ã€‚è¿™å¯¼è‡´äº†èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ä¸åŒå™ªå£°æ°´å¹³çš„ä¸åŒTimeStep LoRAä¸“å®¶ã€‚åœ¨è£…é…é˜¶æ®µï¼ˆç¬¬äºŒé˜¶æ®µï¼‰ï¼Œæˆ‘ä»¬é€šè¿‡å¤šå°ºåº¦é—´éš”çš„ä¸“å®¶æ ¸å¿ƒ-ä¸Šä¸‹æ–‡åä½œï¼Œè®¾è®¡äº†ä¸€ç§æ–°çš„ä¸å¯¹ç§°æ··åˆTimeStep LoRAä¸“å®¶æ–¹æ³•ã€‚å¯¹äºæ¯ä¸ªæ—¶é—´æ­¥é•¿ï¼Œæˆ‘ä»¬åˆ©ç”¨æœ€å°é—´éš”å†…çš„TimeStep LoRAä¸“å®¶ä½œä¸ºæ ¸å¿ƒä¸“å®¶ï¼Œæ²¡æœ‰é—¨æ§æœºåˆ¶ï¼Œå¹¶ä½¿ç”¨è¾ƒå¤§é—´éš”å†…çš„ä¸“å®¶ä½œä¸ºéšæ—¶é—´å˜åŒ–çš„ä¸Šä¸‹æ–‡ä¸“å®¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„TSMå¯ä»¥é€šè¿‡æœ€ç²¾ç»†é—´éš”å†…çš„ä¸“å®¶æœ‰æ•ˆåœ°å»ºæ¨¡å™ªå£°æ°´å¹³ï¼Œå¹¶è‡ªé€‚åº”åœ°é›†æˆæ¥è‡ªå…¶ä»–è§„æ¨¡ä¸“å®¶çš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜äº†æ‰©æ•£æ¨¡å‹çš„é€šç”¨æ€§ã€‚ä¸ºäº†å±•ç¤ºæˆ‘ä»¬TSMèŒƒå¼çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨æ‰©æ•£æ¨¡å‹çš„ä¸‰ä¸ªå…¸å‹ä¸”æµè¡Œçš„LoRAç›¸å…³ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬åŸŸé€‚åº”ã€åé¢„è®­ç»ƒå’Œæ¨¡å‹è’¸é¦ã€‚æˆ‘ä»¬çš„TSMåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½å®ç°äº†æœ€æ–°ç»“æœï¼Œé€‚ç”¨äºå„ç§æ¨¡å‹ç»“æ„ï¼ˆUNetã€DiTå’ŒMM-DiTï¼‰å’Œè§†è§‰æ•°æ®æ¨¡æ€ï¼ˆå›¾åƒã€è§†é¢‘ï¼‰ï¼Œæ˜¾ç¤ºäº†å…¶å“è¶Šçš„æ€»æ‹¬èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07416v1">PDF</a> 17 pages, 5 figures, 13 tables</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨è¿‡å»å‡ å¹´ä¸­æ¨åŠ¨äº†è§†è§‰ç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚ç„¶è€Œï¼Œç”±äºåºå¤§çš„å¾®è°ƒæˆæœ¬ï¼Œè¿™äº›å¤§å‹æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„åº”ç”¨å¾€å¾€é¢ä¸´å›°éš¾ã€‚æœ€è¿‘ï¼Œä½ç§©é€‚åº”ï¼ˆLoRAï¼‰è¢«åº”ç”¨äºæ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆè°ƒæ•´ã€‚ç„¶è€Œï¼ŒLoRAè°ƒæ•´æ‰©æ•£æ¨¡å‹çš„èƒ½åŠ›å—é™äºå…¶åœ¨æ‰©æ•£è¿‡ç¨‹çš„ä¸åŒæ—¶é—´æ­¥é•¿ä¸­ä½¿ç”¨ç›¸åŒçš„LoRAã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºç®€æ´é€šç”¨çš„æ—¶é—´æ­¥é•¿ä¸»ï¼ˆTSMï¼‰èŒƒå¼ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®å¾®è°ƒé˜¶æ®µã€‚åœ¨ä¿ƒè¿›é˜¶æ®µï¼ˆç¬¬ä¸€é˜¶æ®µï¼‰ï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„æ—¶é—´æ­¥é•¿é—´éš”å¯¹æ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåº”ç”¨ä¸åŒçš„LoRAï¼Œå½¢æˆèƒ½å¤Ÿæœ‰æ•ˆæ•æ‰ä¸åŒå™ªå£°æ°´å¹³çš„æ—¶é—´æ­¥é•¿LoRAä¸“å®¶ã€‚åœ¨è£…é…é˜¶æ®µï¼ˆç¬¬äºŒé˜¶æ®µï¼‰ï¼Œæˆ‘ä»¬é€šè¿‡å¤šå°ºåº¦é—´éš”çš„ä¸“å®¶æ ¸å¿ƒ-ä¸Šä¸‹æ–‡åä½œï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹çš„æ—¶é—´æ­¥é•¿LoRAä¸“å®¶ä¸å¯¹ç§°æ··åˆæ–¹æ³•ã€‚å¯¹äºæ¯ä¸ªæ—¶é—´æ­¥é•¿ï¼Œæˆ‘ä»¬åˆ©ç”¨æœ€å°é—´éš”å†…çš„TimeStep LoRAä¸“å®¶ä½œä¸ºæ ¸å¿ƒä¸“å®¶ï¼Œä¸ä½¿ç”¨é—¨æ§æœºåˆ¶ï¼Œå¹¶ä½¿ç”¨è¾ƒå¤§é—´éš”çš„ä¸“å®¶ä½œä¸ºå…·æœ‰æ—¶é—´ä¾èµ–æ€§é—¨æ§æœºåˆ¶çš„ä¸Šä¸‹æ–‡ä¸“å®¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„TSMå¯ä»¥æœ‰æ•ˆåœ°é€šè¿‡æœ€ç²¾ç»†é—´éš”çš„ä¸“å®¶å¯¹å™ªå£°æ°´å¹³è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶è‡ªé€‚åº”åœ°é›†æˆæ¥è‡ªå…¶ä»–è§„æ¨¡ä¸“å®¶çš„ä¸Šä¸‹æ–‡ï¼Œæé«˜äº†æ‰©æ•£æ¨¡å‹çš„é€šç”¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„TSMèŒƒå¼åœ¨ä¸‰ä¸ªå…¸å‹çš„ä¸LoRAç›¸å…³çš„æ‰©æ•£æ¨¡å‹ä»»åŠ¡ä¸Šå®ç°äº†æœ€ä½³æ•ˆæœï¼ŒåŒ…æ‹¬åŸŸé€‚åº”ã€é¢„è®­ç»ƒåå’Œæ¨¡å‹è’¸é¦ã€‚æˆ‘ä»¬çš„TSMåœ¨å„ç§æ¨¡å‹ç»“æ„å’Œè§†è§‰æ•°æ®æ¨¡æ€ä¸Šå®ç°äº†æ˜¾è‘—çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆé¢†åŸŸæœ‰é‡å¤§è´¡çŒ®ï¼Œä½†å¤§è§„æ¨¡å¾®è°ƒæˆæœ¬é«˜ã€‚</li>
<li>LoRAæ–¹æ³•ç”¨äºè°ƒæ•´æ‰©æ•£æ¨¡å‹ï¼Œä½†å…¶åœ¨ä¸åŒæ—¶é—´æ­¥é•¿çš„åº”ç”¨å—é™ã€‚</li>
<li>å¼•å…¥TSMèŒƒå¼ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®å¾®è°ƒé˜¶æ®µï¼šä¿ƒè¿›é˜¶æ®µå’Œè£…é…é˜¶æ®µã€‚</li>
<li>åœ¨ä¿ƒè¿›é˜¶æ®µï¼Œåº”ç”¨ä¸åŒçš„LoRAsåˆ°ä¸åŒçš„æ—¶é—´æ­¥é•¿é—´éš”ï¼Œå½¢æˆTimeStep LoRAä¸“å®¶ã€‚</li>
<li>è£…é…é˜¶æ®µé€šè¿‡æ ¸å¿ƒ-ä¸Šä¸‹æ–‡åä½œå’Œä¸å¯¹ç§°æ··åˆæ–¹æ³•é›†æˆTimeStep LoRAä¸“å®¶ã€‚</li>
<li>TSMèŒƒå¼åœ¨åŸŸé€‚åº”ã€é¢„è®­ç»ƒåå’Œæ¨¡å‹è’¸é¦ç­‰ä»»åŠ¡ä¸Šå®ç°æœ€ä½³æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07416">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-29a66e42689da12770dede2b63b1e182.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a610f19bb8e32a754b7632dd16be4bf3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-773d80c9335350803282c830447f915c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f0ca94d008f9898021419bfce8514f2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29eb325fe961c5a75feb41b8b128cdc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd84e53ad0b119026241064a34267856.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SPEED-Scalable-Precise-and-Efficient-Concept-Erasure-for-Diffusion-Models"><a href="#SPEED-Scalable-Precise-and-Efficient-Concept-Erasure-for-Diffusion-Models" class="headerlink" title="SPEED: Scalable, Precise, and Efficient Concept Erasure for Diffusion   Models"></a>SPEED: Scalable, Precise, and Efficient Concept Erasure for Diffusion   Models</h2><p><strong>Authors:Ouxiang Li, Yuan Wang, Xinting Hu, Houcheng Jiang, Tao Liang, Yanbin Hao, Guojun Ma, Fuli Feng</strong></p>
<p>Erasing concepts from large-scale text-to-image (T2I) diffusion models has become increasingly crucial due to the growing concerns over copyright infringement, offensive content, and privacy violations. However, existing methods either require costly fine-tuning or degrade image quality for non-target concepts (i.e., prior) due to inherent optimization limitations. In this paper, we introduce SPEED, a model editing-based concept erasure approach that leverages null-space constraints for scalable, precise, and efficient erasure. Specifically, SPEED incorporates Influence-based Prior Filtering (IPF) to retain the most affected non-target concepts during erasing, Directed Prior Augmentation (DPA) to expand prior coverage while maintaining semantic consistency, and Invariant Equality Constraints (IEC) to regularize model editing by explicitly preserving key invariants during the T2I generation process. Extensive evaluations across multiple concept erasure tasks demonstrate that SPEED consistently outperforms existing methods in prior preservation while achieving efficient and high-fidelity concept erasure, successfully removing 100 concepts within just 5 seconds. Our code and models are available at: <a target="_blank" rel="noopener" href="https://github.com/Ouxiang-Li/SPEED">https://github.com/Ouxiang-Li/SPEED</a>. </p>
<blockquote>
<p>åœ¨å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ä¸­æ¶ˆé™¤æ¦‚å¿µå˜å¾—è¶Šæ¥è¶Šé‡è¦ï¼Œå› ä¸ºäººä»¬å¯¹ç‰ˆæƒä¾µçŠ¯ã€å†’çŠ¯æ€§å†…å®¹å’Œéšç§æ³„éœ²çš„æ‹…å¿§æ—¥ç›ŠåŠ å‰§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆéœ€è¦å¤§é‡å¾®è°ƒï¼Œè¦ä¹ˆç”±äºå›ºæœ‰çš„ä¼˜åŒ–é™åˆ¶ï¼Œå¯¹éç›®æ ‡æ¦‚å¿µï¼ˆå³å…ˆéªŒï¼‰çš„å›¾åƒè´¨é‡é€ æˆæŸå®³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SPEEDï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ¨¡å‹ç¼–è¾‘çš„æ¦‚å¿µæ¶ˆé™¤æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨é›¶ç©ºé—´çº¦æŸæ¥å®ç°å¯æ‰©å±•ã€ç²¾ç¡®å’Œé«˜æ•ˆçš„æ¶ˆé™¤ã€‚å…·ä½“æ¥è¯´ï¼ŒSPEEDç»“åˆäº†åŸºäºå½±å“çš„å‰ç½®è¿‡æ»¤ï¼ˆIPFï¼‰æ¥ä¿ç•™åœ¨æ¶ˆé™¤è¿‡ç¨‹ä¸­å—å½±å“æœ€å¤§çš„éç›®æ ‡æ¦‚å¿µï¼Œå®šå‘å…ˆéªŒå¢å¼ºï¼ˆDPAï¼‰åœ¨ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§çš„åŒæ—¶æ‰©å¤§å…ˆéªŒè¦†ç›–èŒƒå›´ï¼Œä»¥åŠä¸å˜ç­‰å¼çº¦æŸï¼ˆIECï¼‰é€šè¿‡æ˜¾å¼ä¿ç•™å…³é”®ä¸å˜é‡æ¥è§„èŒƒT2Iç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ¨¡å‹ç¼–è¾‘ã€‚åœ¨å¤šä¸ªæ¦‚å¿µæ¶ˆé™¤ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒSPEEDåœ¨ä¿æŒå…ˆéªŒçš„åŒæ—¶ï¼Œå§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†é«˜æ•ˆå’Œé«˜ä¿çœŸåº¦çš„æ¦‚å¿µæ¶ˆé™¤ï¼Œåªéœ€5ç§’å³å¯æˆåŠŸç§»é™¤100ä¸ªæ¦‚å¿µã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯åœ¨ä»¥ä¸‹ç½‘å€è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/Ouxiang-Li/SPEED">https://github.com/Ouxiang-Li/SPEED</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07392v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ¨¡å‹ç¼–è¾‘çš„æ¦‚å¿µæ¶ˆé™¤æ–¹æ³•SPEEDï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é›¶ç©ºé—´çº¦æŸå®ç°å¯æ‰©å±•ã€ç²¾ç¡®å’Œé«˜æ•ˆçš„æ¦‚å¿µæ¶ˆé™¤ã€‚å®ƒé‡‡ç”¨å½±å“ä¼˜å…ˆè¿‡æ»¤æ¥ä¿ç•™å—å½±å“æœ€å¤§çš„éç›®æ ‡æ¦‚å¿µï¼Œé€šè¿‡å®šå‘ä¼˜å…ˆå¢å¼ºæ¥æ‰©å¤§å…ˆéªŒè¦†ç›–å¹¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ï¼Œä»¥åŠä¸å˜ç­‰å¼çº¦æŸæ¥åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­æ˜¾å¼ä¿ç•™å…³é”®ä¸å˜æ€§ä»¥è§„èŒƒæ¨¡å‹ç¼–è¾‘ã€‚è¯„ä¼°è¡¨æ˜ï¼ŒSPEEDåœ¨ä¿ç•™å…ˆéªŒçš„åŒæ—¶ï¼Œå®ç°äº†é«˜æ•ˆå’Œé«˜ä¿çœŸåº¦çš„æ¦‚å¿µæ¶ˆé™¤ï¼Œèƒ½å¤Ÿåœ¨çŸ­çŸ­5ç§’å†…æˆåŠŸæ¶ˆé™¤100ä¸ªæ¦‚å¿µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SPEEDæ˜¯ä¸€ç§ç”¨äºå¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹çš„æ¦‚å¿µæ¶ˆé™¤æ–¹æ³•ã€‚</li>
<li>å®ƒåˆ©ç”¨é›¶ç©ºé—´çº¦æŸå®ç°å¯æ‰©å±•ã€ç²¾ç¡®å’Œé«˜æ•ˆçš„æ¦‚å¿µæ¶ˆé™¤ã€‚</li>
<li>SPEEDé€šè¿‡å½±å“ä¼˜å…ˆè¿‡æ»¤ä¿ç•™å—å½±å“æœ€å¤§çš„éç›®æ ‡æ¦‚å¿µã€‚</li>
<li>å®šå‘ä¼˜å…ˆå¢å¼ºå¯æ‰©å¤§å…ˆéªŒè¦†ç›–å¹¶ä¿æŒè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>ä¸å˜ç­‰å¼çº¦æŸç”¨äºåœ¨T2Iç”Ÿæˆè¿‡ç¨‹ä¸­æ˜¾å¼ä¿ç•™å…³é”®ä¸å˜æ€§ä»¥è§„èŒƒæ¨¡å‹ç¼–è¾‘ã€‚</li>
<li>SPEEDåœ¨å¤šä¸ªæ¦‚å¿µæ¶ˆé™¤ä»»åŠ¡ä¸­çš„è¯„ä¼°è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½å¤Ÿåœ¨çŸ­æ—¶é—´å†…æ¶ˆé™¤å¤§é‡æ¦‚å¿µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07392">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aafacf58654cba78f5b00ccb7f009745.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bad573d08469fc2fee50aeaa2866fec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c1f2cea336f1d3791c363f12d9b3bb6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7d8481b04e3bf25de68917d1107ada8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1125dff6d304505857d68b3f2cffee1d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73a419b35c01008b4066d6ff8be1a760.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TRCE-Towards-Reliable-Malicious-Concept-Erasure-in-Text-to-Image-Diffusion-Models"><a href="#TRCE-Towards-Reliable-Malicious-Concept-Erasure-in-Text-to-Image-Diffusion-Models" class="headerlink" title="TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image   Diffusion Models"></a>TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image   Diffusion Models</h2><p><strong>Authors:Ruidong Chen, Honglin Guo, Lanjun Wang, Chenyu Zhang, Weizhi Nie, An-An Liu</strong></p>
<p>Recent advances in text-to-image diffusion models enable photorealistic image generation, but they also risk producing malicious content, such as NSFW images. To mitigate risk, concept erasure methods are studied to facilitate the model to unlearn specific concepts. However, current studies struggle to fully erase malicious concepts implicitly embedded in prompts (e.g., metaphorical expressions or adversarial prompts) while preserving the modelâ€™s normal generation capability. To address this challenge, our study proposes TRCE, using a two-stage concept erasure strategy to achieve an effective trade-off between reliable erasure and knowledge preservation. Firstly, TRCE starts by erasing the malicious semantics implicitly embedded in textual prompts. By identifying a critical mapping objective(i.e., the [EoT] embedding), we optimize the cross-attention layers to map malicious prompts to contextually similar prompts but with safe concepts. This step prevents the model from being overly influenced by malicious semantics during the denoising process. Following this, considering the deterministic properties of the sampling trajectory of the diffusion model, TRCE further steers the early denoising prediction toward the safe direction and away from the unsafe one through contrastive learning, thus further avoiding the generation of malicious content. Finally, we conduct comprehensive evaluations of TRCE on multiple malicious concept erasure benchmarks, and the results demonstrate its effectiveness in erasing malicious concepts while better preserving the modelâ€™s original generation ability. The code is available at: <a target="_blank" rel="noopener" href="http://github.com/ddgoodgood/TRCE">http://github.com/ddgoodgood/TRCE</a>. CAUTION: This paper includes model-generated content that may contain offensive material. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¿›å±•ä½¿å¾—èƒ½å¤Ÿç”Ÿæˆé€¼çœŸçš„å›¾åƒï¼Œä½†åŒæ—¶ä¹Ÿå­˜åœ¨ç”Ÿæˆæ¶æ„å†…å®¹çš„é£é™©ï¼Œä¾‹å¦‚ä¸é€‚å®œå…¬å¼€åœºåˆï¼ˆNSFWï¼‰çš„å›¾åƒã€‚ä¸ºäº†å‡è½»é£é™©ï¼Œå¼€å±•äº†æ¦‚å¿µæ¶ˆé™¤æ–¹æ³•çš„ç ”ç©¶ï¼Œä»¥ä¿ƒè¿›æ¨¡å‹å¿˜è®°ç‰¹å®šæ¦‚å¿µã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶åœ¨æ¶ˆé™¤éšå«åœ¨æç¤ºä¸­çš„æ¶æ„æ¦‚å¿µï¼ˆä¾‹å¦‚éšå–»æˆ–å¯¹æŠ—æ€§æç¤ºï¼‰æ—¶é‡åˆ°å›°éš¾ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ­£å¸¸ç”Ÿæˆèƒ½åŠ›ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æå‡ºäº†TRCEï¼Œé‡‡ç”¨ä¸¤é˜¶æ®µæ¦‚å¿µæ¶ˆé™¤ç­–ç•¥ï¼Œåœ¨å¯é æ¶ˆé™¤å’ŒçŸ¥è¯†ä¿ç•™ä¹‹é—´å–å¾—æœ‰æ•ˆå¹³è¡¡ã€‚é¦–å…ˆï¼ŒTRCEä»æ¶ˆé™¤æ–‡æœ¬æç¤ºä¸­éšå«çš„æ¶æ„è¯­ä¹‰å¼€å§‹ã€‚é€šè¿‡ç¡®å®šå…³é”®æ˜ å°„ç›®æ ‡ï¼ˆå³[EoT]åµŒå…¥ï¼‰ï¼Œæˆ‘ä»¬ä¼˜åŒ–äº¤å‰æ³¨æ„å±‚ï¼Œå°†æ¶æ„æç¤ºæ˜ å°„åˆ°ä¸Šä¸‹æ–‡ç›¸ä¼¼çš„æç¤ºä¸Šï¼Œä½†å…·æœ‰å®‰å…¨çš„æ¦‚å¿µã€‚è¿™ä¸€æ­¥é˜²æ­¢äº†æ¨¡å‹åœ¨é™å™ªè¿‡ç¨‹ä¸­å—åˆ°æ¶æ„è¯­ä¹‰çš„è¿‡å¤šå½±å“ã€‚æ¥ä¸‹æ¥ï¼Œè€ƒè™‘åˆ°æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è½¨è¿¹çš„ç¡®å®šæ€§å±æ€§ï¼ŒTRCEè¿›ä¸€æ­¥å¼•å¯¼æ—©æœŸé™å™ªé¢„æµ‹æœç€å®‰å…¨æ–¹å‘è¿›è¡Œï¼Œå¹¶é€šè¿‡å¯¹æ¯”å­¦ä¹ è¿œç¦»ä¸å®‰å…¨æ–¹å‘ï¼Œä»è€Œè¿›ä¸€æ­¥é¿å…ç”Ÿæˆæ¶æ„å†…å®¹ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªæ¶æ„æ¦‚å¿µæ¶ˆé™¤åŸºå‡†æµ‹è¯•ä¸Šå¯¹TRCEè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œç»“æœè¡¨æ˜å®ƒåœ¨æ¶ˆé™¤æ¶æ„æ¦‚å¿µçš„åŒæ—¶æ›´å¥½åœ°ä¿ç•™äº†æ¨¡å‹çš„åŸå§‹ç”Ÿæˆèƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="http://github.com/ddgoodgood/TRCE%E8%8E%B7%E5%8F%96%E3%80%82%E6%B3%A8%E6%84%8F%EF%BC%9A%E6%9C%AC%E6%96%87%E5%8C%85%E5%90%AB%E5%8F%AF%E8%83%BD%E5%90%AB%E6%9C%89%E5%86%92%E7%8A%AF%E6%80%A7%E5%86%85%E5%AE%B9%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%94%9F%E6%88%90%E5%86%85%E5%AE%B9%E3%80%82">http://github.com/ddgoodgood/TRCEè·å–ã€‚æ³¨æ„ï¼šæœ¬æ–‡åŒ…å«å¯èƒ½å«æœ‰å†’çŠ¯æ€§å†…å®¹çš„æ¨¡å‹ç”Ÿæˆå†…å®¹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07389v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå›¾åƒæ—¶å¯èƒ½äº§ç”Ÿçš„é£é™©ï¼Œç‰¹åˆ«æ˜¯ç”Ÿæˆå«æœ‰æ¶æ„å†…å®¹ï¼ˆå¦‚NSFWå›¾åƒï¼‰çš„é—®é¢˜ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºTRCEçš„ä¸¤é˜¶æ®µæ¦‚å¿µæ¶ˆé™¤ç­–ç•¥ï¼Œæ—¨åœ¨æœ‰æ•ˆå¹³è¡¡å¯é æ¶ˆé™¤ä¸çŸ¥è¯†ä¿ç•™ã€‚é¦–å…ˆï¼Œé€šè¿‡è¯†åˆ«å…³é”®æ˜ å°„ç›®æ ‡ï¼ˆå³[EoT]åµŒå…¥ï¼‰ï¼Œä¼˜åŒ–è·¨æ³¨æ„å±‚ï¼Œå°†æ¶æ„æç¤ºæ˜ å°„åˆ°å…·æœ‰å®‰å…¨æ¦‚å¿µä¸”åœ¨ä¸Šä¸‹æ–‡ä¸Šç›¸ä¼¼çš„æç¤ºä¸Šã€‚æ¥ç€ï¼Œè€ƒè™‘åˆ°æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è½¨è¿¹çš„ç¡®å®šæ€§å±æ€§ï¼ŒTRCEé€šè¿‡å¯¹æ¯”å­¦ä¹ è¿›ä¸€æ­¥å¼•å¯¼æ—©æœŸå»å™ªé¢„æµ‹æœç€å®‰å…¨æ–¹å‘è¿›è¡Œï¼Œè¿œç¦»ä¸å®‰å…¨æ–¹å‘ï¼Œä»è€Œé¿å…ç”Ÿæˆæ¶æ„å†…å®¹ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒTRCEåœ¨æ¶ˆé™¤æ¶æ„æ¦‚å¿µçš„åŒæ—¶æ›´å¥½åœ°ä¿ç•™äº†æ¨¡å‹çš„åŸå§‹ç”Ÿæˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å­˜åœ¨ç”Ÿæˆæ¶æ„å†…å®¹çš„é£é™©ã€‚</li>
<li>TRCEæ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µæ¦‚å¿µæ¶ˆé™¤ç­–ç•¥æ¥å¹³è¡¡æ¶ˆé™¤æ¶æ„æ¦‚å¿µå’Œä¿ç•™æ¨¡å‹ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé€šè¿‡ä¼˜åŒ–è·¨æ³¨æ„å±‚ï¼Œå°†æ¶æ„æç¤ºæ˜ å°„åˆ°å…·æœ‰å®‰å…¨æ¦‚å¿µçš„ç›¸ä¼¼æç¤ºä¸Šã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è½¨è¿¹ç¡®å®šæ€§ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å¼•å¯¼å»å™ªé¢„æµ‹æœç€å®‰å…¨æ–¹å‘è¿›è¡Œã€‚</li>
<li>TRCEåœ¨å¤šä¸ªæ¶æ„æ¦‚å¿µæ¶ˆé™¤åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯„ä¼°è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
<li>TRCEæ–¹æ³•çš„ä»£ç å·²å…¬å¼€å¯ä¾›ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07389">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58fe23f639f5b2d8109403ebaf218f3d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07cfa6a3bd5bc53c3420f423af835d95.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a92ac7629c7b7fe8b93f635d211669f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b621c4834f66852e85e14e89f7016ff5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1da987d47c42f0542e0df03bc30bc26d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="AttenST-A-Training-Free-Attention-Driven-Style-Transfer-Framework-with-Pre-Trained-Diffusion-Models"><a href="#AttenST-A-Training-Free-Attention-Driven-Style-Transfer-Framework-with-Pre-Trained-Diffusion-Models" class="headerlink" title="AttenST: A Training-Free Attention-Driven Style Transfer Framework with   Pre-Trained Diffusion Models"></a>AttenST: A Training-Free Attention-Driven Style Transfer Framework with   Pre-Trained Diffusion Models</h2><p><strong>Authors:Bo Huang, Wenlun Xu, Qizhuo Han, Haodong Jing, Ying Li</strong></p>
<p>While diffusion models have achieved remarkable progress in style transfer tasks, existing methods typically rely on fine-tuning or optimizing pre-trained models during inference, leading to high computational costs and challenges in balancing content preservation with style integration. To address these limitations, we introduce AttenST, a training-free attention-driven style transfer framework. Specifically, we propose a style-guided self-attention mechanism that conditions self-attention on the reference style by retaining the query of the content image while substituting its key and value with those from the style image, enabling effective style feature integration. To mitigate style information loss during inversion, we introduce a style-preserving inversion strategy that refines inversion accuracy through multiple resampling steps. Additionally, we propose a content-aware adaptive instance normalization, which integrates content statistics into the normalization process to optimize style fusion while mitigating the content degradation. Furthermore, we introduce a dual-feature cross-attention mechanism to fuse content and style features, ensuring a harmonious synthesis of structural fidelity and stylistic expression. Extensive experiments demonstrate that AttenST outperforms existing methods, achieving state-of-the-art performance in style transfer dataset. </p>
<blockquote>
<p>å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨é£æ ¼è¿ç§»ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒæˆ–ä¼˜åŒ–ï¼Œè¿™å¯¼è‡´äº†è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œä»¥åŠåœ¨å†…å®¹ä¿ç•™ä¸é£æ ¼èåˆä¹‹é—´å–å¾—å¹³è¡¡çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†AttenSTï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒã€ä»¥æ³¨æ„åŠ›é©±åŠ¨çš„é£æ ¼è¿ç§»æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å—é£æ ¼å¼•å¯¼çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡ä¿ç•™å†…å®¹å›¾åƒçš„æŸ¥è¯¢ï¼Œå¹¶ç”¨é£æ ¼å›¾åƒçš„é”®å’Œå€¼æ›¿æ¢å®ƒï¼Œä»è€Œåœ¨è‡ªæ³¨æ„åŠ›ä¸Šè®¾å®šå‚è€ƒé£æ ¼çš„æ¡ä»¶ï¼Œå®ç°æœ‰æ•ˆçš„é£æ ¼ç‰¹å¾èåˆã€‚ä¸ºäº†å‡è½»åè½¬è¿‡ç¨‹ä¸­çš„é£æ ¼ä¿¡æ¯æŸå¤±ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¿é£æ ¼åè½¬ç­–ç•¥ï¼Œé€šè¿‡å¤šæ¬¡é‡é‡‡æ ·æ­¥éª¤æé«˜åè½¬ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å†…å®¹æ„ŸçŸ¥è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–ï¼Œå°†å†…å®¹ç»Ÿè®¡ä¿¡æ¯èå…¥å½’ä¸€åŒ–è¿‡ç¨‹ï¼Œä»¥ä¼˜åŒ–é£æ ¼èåˆï¼ŒåŒæ—¶å‡è½»å†…å®¹é€€åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŒç‰¹å¾äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥èåˆå†…å®¹å’Œé£æ ¼ç‰¹å¾ï¼Œç¡®ä¿ç»“æ„ä¿çœŸå’Œé£æ ¼è¡¨è¾¾çš„å’Œè°èåˆã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAttenSTä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨é£æ ¼è¿ç§»æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07307v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ— è®­ç»ƒé£æ ¼è½¬æ¢æ¡†æ¶AttenSTèƒ½å¤Ÿæœ‰æ•ˆè§£å†³æ‰©æ•£æ¨¡å‹åœ¨é£æ ¼è½¬æ¢ä»»åŠ¡ä¸­çš„é«˜è®¡ç®—æˆæœ¬åŠå†…å®¹ä¿ç•™ä¸é£æ ¼èåˆå¹³è¡¡çš„æŒ‘æˆ˜ã€‚é€šè¿‡é£æ ¼å¼•å¯¼çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€é£æ ¼ä¿ç•™çš„å€’è½¬ç­–ç•¥ã€å†…å®¹æ„ŸçŸ¥è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–ä»¥åŠåŒé‡ç‰¹å¾äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼ŒAttenSTåœ¨é£æ ¼è½¬æ¢æ•°æ®é›†ä¸Šå–å¾—äº†é¢†å…ˆç°æœ‰æ–¹æ³•çš„æœ€ä½³æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨é£æ ¼è½¬æ¢ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»å­˜åœ¨é«˜è®¡ç®—æˆæœ¬å’Œå¹³è¡¡å†…å®¹ä¿ç•™ä¸é£æ ¼èåˆçš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ— è®­ç»ƒé£æ ¼è½¬æ¢æ¡†æ¶AttenSTæ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>AttenSTé‡‡ç”¨é£æ ¼å¼•å¯¼çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡å‚è€ƒé£æ ¼æ¥æ¡ä»¶åŒ–è‡ªæ³¨æ„åŠ›ï¼Œæœ‰æ•ˆæ•´åˆé£æ ¼ç‰¹å¾ã€‚</li>
<li>æå‡ºäº†é£æ ¼ä¿ç•™çš„å€’è½¬ç­–ç•¥ï¼Œé€šè¿‡å¤šæ¬¡é‡é‡‡æ ·æ­¥éª¤æé«˜å€’è½¬ç²¾åº¦ï¼Œå‡å°‘é£æ ¼ä¿¡æ¯æŸå¤±ã€‚</li>
<li>å¼•å…¥å†…å®¹æ„ŸçŸ¥è‡ªé€‚åº”å®ä¾‹å½’ä¸€åŒ–ï¼Œå°†å†…å®¹ç»Ÿè®¡çº³å…¥å½’ä¸€åŒ–è¿‡ç¨‹ï¼Œä¼˜åŒ–é£æ ¼èåˆå¹¶å‡å°‘å†…å®¹é€€åŒ–ã€‚</li>
<li>é‡‡ç”¨åŒé‡ç‰¹å¾äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œèåˆå†…å®¹å’Œé£æ ¼ç‰¹å¾ï¼Œå®ç°ç»“æ„ä¿çœŸå’Œé£æ ¼è¡¨è¾¾çš„å’Œè°èåˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07307">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b218b98949b70fc40e90bdc29a3ab15e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b518b1009de2c344a6b4dc5973d766bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59d20fe039573ed11c55d4e6f762b3ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d61112def1609fdf070463df87e4879c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0f7670a15dece86bc807cf64afd06402.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AnomalyPainter-Vision-Language-Diffusion-Synergy-for-Zero-Shot-Realistic-and-Diverse-Industrial-Anomaly-Synthesis"><a href="#AnomalyPainter-Vision-Language-Diffusion-Synergy-for-Zero-Shot-Realistic-and-Diverse-Industrial-Anomaly-Synthesis" class="headerlink" title="AnomalyPainter: Vision-Language-Diffusion Synergy for Zero-Shot   Realistic and Diverse Industrial Anomaly Synthesis"></a>AnomalyPainter: Vision-Language-Diffusion Synergy for Zero-Shot   Realistic and Diverse Industrial Anomaly Synthesis</h2><p><strong>Authors:Zhangyu Lai, Yilin Lu, Xinyang Li, Jianghang Lin, Yansong Qu, Liujuan Cao, Ming Li, Rongrong Ji</strong></p>
<p>While existing anomaly synthesis methods have made remarkable progress, achieving both realism and diversity in synthesis remains a major obstacle. To address this, we propose AnomalyPainter, a zero-shot framework that breaks the diversity-realism trade-off dilemma through synergizing Vision Language Large Model (VLLM), Latent Diffusion Model (LDM), and our newly introduced texture library Tex-9K. Tex-9K is a professional texture library containing 75 categories and 8,792 texture assets crafted for diverse anomaly synthesis. Leveraging VLLMâ€™s general knowledge, reasonable anomaly text descriptions are generated for each industrial object and matched with relevant diverse textures from Tex-9K. These textures then guide the LDM via ControlNet to paint on normal images. Furthermore, we introduce Texture-Aware Latent Init to stabilize the natural-image-trained ControlNet for industrial images. Extensive experiments show that AnomalyPainter outperforms existing methods in realism, diversity, and generalization, achieving superior downstream performance. </p>
<blockquote>
<p>å°½ç®¡ç°æœ‰çš„å¼‚å¸¸åˆæˆæ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨åˆæˆä¸­å®ç°çœŸå®æ„Ÿå’Œå¤šæ ·æ€§ä»æ˜¯ä¸»è¦éšœç¢ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AnomalyPainterï¼Œè¿™æ˜¯ä¸€ä¸ªé›¶æ ·æœ¬æ¡†æ¶ï¼Œå®ƒé€šè¿‡ååŒè§†è§‰è¯­è¨€å¤§æ¨¡å‹ï¼ˆVLLMï¼‰ã€æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å’Œæˆ‘ä»¬æ–°å¼•å…¥çš„çº¹ç†åº“Tex-9Kï¼Œè§£å†³äº†å¤šæ ·æ€§-çœŸå®æ„Ÿæƒè¡¡çš„å›°å¢ƒã€‚Tex-9Kæ˜¯ä¸€ä¸ªä¸“ä¸šçº¹ç†åº“ï¼ŒåŒ…å«75ä¸ªç±»åˆ«å’Œ8792ä¸ªä¸ºå„ç§å¼‚å¸¸åˆæˆåˆ¶ä½œçš„çº¹ç†èµ„äº§ã€‚åˆ©ç”¨VLLMçš„é€šç”¨çŸ¥è¯†ï¼Œä¸ºæ¯ä¸ªå·¥ä¸šå¯¹è±¡ç”Ÿæˆåˆç†çš„å¼‚å¸¸æ–‡æœ¬æè¿°ï¼Œå¹¶ä¸Tex-9Kä¸­ç›¸å…³çš„å¤šæ ·çº¹ç†ç›¸åŒ¹é…ã€‚è¿™äº›çº¹ç†ç„¶åé€šè¿‡ControlNetåœ¨æ™®é€šå›¾åƒä¸Šè¿›è¡Œç»˜ç”»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†Texture-Aware Latent Initï¼Œä»¥ç¨³å®šé’ˆå¯¹å·¥ä¸šå›¾åƒçš„è‡ªç„¶å›¾åƒè®­ç»ƒControlNetã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒAnomalyPainteråœ¨çœŸå®æ„Ÿã€å¤šæ ·æ€§å’Œæ³›åŒ–æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†å‡ºè‰²çš„ä¸‹æ¸¸æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07253v1">PDF</a> anomaly synthesis,anomaly detection</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºAnomalyPainterçš„é›¶æ ·æœ¬æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ååŒä½¿ç”¨è§†è§‰è¯­è¨€å¤§æ¨¡å‹ï¼ˆVLLMï¼‰ã€æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ä»¥åŠæ–°å¼•å…¥çš„çº¹ç†åº“Tex-9Kï¼Œè§£å†³äº†ç°æœ‰å¼‚å¸¸åˆæˆæ–¹æ³•åœ¨çœŸå®æ€§å’Œå¤šæ ·æ€§æ–¹é¢çš„éš¾é¢˜ã€‚Tex-9KåŒ…å«75ç±»ã€å…±8792ä¸ªä¸“ä¸šçº¹ç†èµ„äº§ï¼Œç”¨äºå¤šæ ·åŒ–çš„å¼‚å¸¸åˆæˆã€‚åˆ©ç”¨VLLMçš„ä¸€èˆ¬çŸ¥è¯†ï¼Œç”Ÿæˆåˆç†çš„å¼‚å¸¸æ–‡æœ¬æè¿°ï¼Œä¸Tex-9Kä¸­çš„ç›¸å…³å¤šæ ·çº¹ç†ç›¸åŒ¹é…ï¼Œå†é€šè¿‡ControlNetåœ¨æ™®é€šå›¾åƒä¸Šè¿›è¡Œç»˜åˆ¶ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†çº¹ç†æ„ŸçŸ¥åˆå§‹æ½œæ€æ¥ç¨³å®šé’ˆå¯¹å·¥ä¸šå›¾åƒçš„è‡ªç„¶å›¾åƒè®­ç»ƒControlNetã€‚å®éªŒè¡¨æ˜ï¼ŒAnomalyPainteråœ¨çœŸå®æ€§ã€å¤šæ ·æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå®ç°äº†å‡ºè‰²çš„ä¸‹æ¸¸æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AnomalyPainteræ˜¯ä¸€ä¸ªé›¶æ ·æœ¬æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å¼‚å¸¸åˆæˆæ–¹æ³•åœ¨çœŸå®æ€§å’Œå¤šæ ·æ€§æ–¹é¢çš„éš¾é¢˜ã€‚</li>
<li>æ¡†æ¶ç»“åˆäº†è§†è§‰è¯­è¨€å¤§æ¨¡å‹ï¼ˆVLLMï¼‰ã€æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ä»¥åŠä¸“ä¸šçº¹ç†åº“Tex-9Kã€‚</li>
<li>Tex-9KåŒ…å«ä¸°å¯Œçš„çº¹ç†èµ„äº§ï¼Œæœ‰åŠ©äºå¤šæ ·åŒ–çš„å¼‚å¸¸åˆæˆã€‚</li>
<li>åˆ©ç”¨VLLMç”Ÿæˆä¸å·¥ä¸šå¯¹è±¡ç›¸å…³çš„åˆç†å¼‚å¸¸æ–‡æœ¬æè¿°ï¼Œå¹¶ä¸Tex-9Kä¸­çš„çº¹ç†åŒ¹é…ã€‚</li>
<li>ControlNetè¢«ç”¨äºåœ¨æ™®é€šå›¾åƒä¸Šè¿›è¡Œç»˜åˆ¶ï¼Œé€šè¿‡å¼•å…¥Texture-Aware Latent Initæ¥é€‚åº”å·¥ä¸šå›¾åƒã€‚</li>
<li>AnomalyPainteråœ¨çœŸå®æ€§ã€å¤šæ ·æ€§å’Œæ³›åŒ–èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07253">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d354b906d12570a02b9b3634867cada5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da893f0feaa29b72612380bf01b9ce47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-735340cb97596fe619e8bd5e1bd6f3af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc1b806e5cbd44ca910ffb415d1feccf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5ac2e009db8168ebab69081cd2075a61.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Effective-and-Efficient-Masked-Image-Generation-Models"><a href="#Effective-and-Efficient-Masked-Image-Generation-Models" class="headerlink" title="Effective and Efficient Masked Image Generation Models"></a>Effective and Efficient Masked Image Generation Models</h2><p><strong>Authors:Zebin You, Jingyang Ou, Xiaolu Zhang, Jun Hu, Jun Zhou, Chongxuan Li</strong></p>
<p>Although masked image generation models and masked diffusion models are designed with different motivations and objectives, we observe that they can be unified within a single framework. Building upon this insight, we carefully explore the design space of training and sampling, identifying key factors that contribute to both performance and efficiency. Based on the improvements observed during this exploration, we develop our model, referred to as eMIGM. Empirically, eMIGM demonstrates strong performance on ImageNet generation, as measured by Fr&#39;echet Inception Distance (FID). In particular, on ImageNet 256x256, with similar number of function evaluations (NFEs) and model parameters, eMIGM outperforms the seminal VAR. Moreover, as NFE and model parameters increase, eMIGM achieves performance comparable to the state-of-the-art continuous diffusion models while requiring less than 40% of the NFE. Additionally, on ImageNet 512x512, with only about 60% of the NFE, eMIGM outperforms the state-of-the-art continuous diffusion models. </p>
<blockquote>
<p>è™½ç„¶æ©è†œå›¾åƒç”Ÿæˆæ¨¡å‹å’Œæ©è†œæ‰©æ•£æ¨¡å‹çš„è®¾è®¡åˆè¡·å’Œç›®æ ‡ä¸åŒï¼Œä½†æˆ‘ä»¬å‘ç°å®ƒä»¬å¯ä»¥åœ¨ä¸€ä¸ªå•ä¸€æ¡†æ¶å†…ç»Ÿä¸€ã€‚åŸºäºè¿™ä¸€æ´å¯Ÿï¼Œæˆ‘ä»¬ä»”ç»†æ¢ç´¢äº†è®­ç»ƒå’Œé‡‡æ ·çš„è®¾è®¡ç©ºé—´ï¼Œè¯†åˆ«å‡ºå¯¹æ€§èƒ½å’Œæ•ˆç‡éƒ½åšå‡ºè´¡çŒ®çš„å…³é”®å› ç´ ã€‚æ ¹æ®åœ¨æ­¤æ¬¡æ¢ç´¢ä¸­è§‚å¯Ÿåˆ°çš„æ”¹è¿›ï¼Œæˆ‘ä»¬å¼€å‘äº†è‡ªå·±çš„æ¨¡å‹ï¼Œç§°ä¸ºeMIGMã€‚ç»éªŒä¸Šï¼ŒeMIGMåœ¨ImageNetç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä»¥FrÃ©chet Inception Distanceï¼ˆFIDï¼‰è¡¡é‡ã€‚ç‰¹åˆ«æ˜¯åœ¨ImageNet 256x256ä¸Šï¼ŒeMIGMåœ¨åŠŸèƒ½è¯„ä¼°æ¬¡æ•°ï¼ˆNFEï¼‰å’Œæ¨¡å‹å‚æ•°ç›¸ä¼¼çš„æƒ…å†µä¸‹ï¼Œè¶…è¿‡äº†æœ€åˆçš„VARæ¨¡å‹ã€‚è€Œä¸”ï¼Œéšç€åŠŸèƒ½è¯„ä¼°æ¬¡æ•°å’Œæ¨¡å‹å‚æ•°çš„å¢åŠ ï¼ŒeMIGMåœ¨æ€§èƒ½ä¸Šè¾¾åˆ°äº†ä¸å½“å‰æœ€å…ˆè¿›çš„è¿ç»­æ‰©æ•£æ¨¡å‹ç›¸å½“çš„æ°´å¹³ï¼ŒåŒæ—¶æ‰€éœ€çš„NFEä¸åˆ°40%ã€‚æ­¤å¤–ï¼Œåœ¨ImageNet 512x512ä¸Šï¼ŒeMIGMåœ¨åªæœ‰å¤§çº¦60%çš„NFEæƒ…å†µä¸‹ï¼Œæ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„è¿ç»­æ‰©æ•£æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07197v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†é¢å…·å›¾åƒç”Ÿæˆæ¨¡å‹ï¼ˆmasked image generation modelsï¼‰å’Œé¢å…·æ‰©æ•£æ¨¡å‹ï¼ˆmasked diffusion modelsï¼‰çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå¹¶æ¢ç´¢äº†è®­ç»ƒå’Œé‡‡æ ·çš„è®¾è®¡ç©ºé—´ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œå¼€å‘å‡ºäº†æ€§èƒ½å¼ºå¤§çš„æ–°æ¨¡å‹eMIGMã€‚åœ¨ImageNetç”Ÿæˆä»»åŠ¡ä¸Šï¼ŒeMIGMè¡¨ç°ä¼˜ç§€ï¼Œç›¸è¾ƒäºæ—©æœŸæ¨¡å‹VARæœ‰æ›´é«˜çš„æ€§èƒ½ã€‚éšç€å‡½æ•°è¯„ä»·å’Œæ¨¡å‹å‚æ•°çš„å¢åŠ ï¼ŒeMIGMçš„æ€§èƒ½è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„è¿ç»­æ‰©æ•£æ¨¡å‹çš„æ°´å¹³ï¼Œä½†æ‰€éœ€çš„å‡½æ•°è¯„ä»·ä½äºè¿™äº›æ¨¡å‹ã€‚åœ¨ImageNet 512x512ä»»åŠ¡ä¸Šï¼ŒeMIGMåœ¨è¾ƒå°‘çš„å‡½æ•°è¯„ä»·ä¸‹å°±èƒ½è¡¨ç°å‡ºè¶…è¶Šç°æœ‰æœ€å…ˆè¿›çš„è¿ç»­æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢å…·å›¾åƒç”Ÿæˆæ¨¡å‹å’Œé¢å…·æ‰©æ•£æ¨¡å‹å¯ä»¥åœ¨ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶å†…æ•´åˆã€‚</li>
<li>é€šè¿‡æ¢ç´¢è®­ç»ƒå’Œé‡‡æ ·çš„è®¾è®¡ç©ºé—´ï¼Œå…³é”®å› ç´ è¢«è¯†åˆ«ä¸ºå¯¹æ€§èƒ½å’Œæ•ˆç‡çš„è´¡çŒ®è€…ã€‚</li>
<li>æ–°å¼€å‘çš„æ¨¡å‹eMIGMåœ¨ImageNetç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>eMIGMç›¸è¾ƒäºæ—©æœŸæ¨¡å‹VARæœ‰æ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>eMIGMçš„æ€§èƒ½è¾¾åˆ°äº†å½“å‰æœ€å…ˆè¿›çš„è¿ç»­æ‰©æ•£æ¨¡å‹çš„æ°´å¹³ï¼Œä½†æ‰€éœ€çš„å‡½æ•°è¯„ä»·æ›´ä½ã€‚</li>
<li>åœ¨ImageNet 512x512ä»»åŠ¡ä¸Šï¼ŒeMIGMåœ¨è¾ƒå°‘çš„å‡½æ•°è¯„ä»·ä¸‹å°±èƒ½è¶…è¶Šç°æœ‰æœ€å…ˆè¿›çš„è¿ç»­æ‰©æ•£æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07197">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1fc9f31466b212dca02a1f1101f968b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fa9a8ea3da12a60f2c69f3b9e2601dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e783bee693e334b0f06a144bfa3e3e67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1219a7dbce0e75cd3d2259498a15e6d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a11006d031ba6443871eefb55d2f7d59.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="TIDE-Temporal-Aware-Sparse-Autoencoders-for-Interpretable-Diffusion-Transformers-in-Image-Generation"><a href="#TIDE-Temporal-Aware-Sparse-Autoencoders-for-Interpretable-Diffusion-Transformers-in-Image-Generation" class="headerlink" title="TIDE : Temporal-Aware Sparse Autoencoders for Interpretable Diffusion   Transformers in Image Generation"></a>TIDE : Temporal-Aware Sparse Autoencoders for Interpretable Diffusion   Transformers in Image Generation</h2><p><strong>Authors:Victor Shea-Jay Huang, Le Zhuo, Yi Xin, Zhaokai Wang, Peng Gao, Hongsheng Li</strong></p>
<p>Diffusion Transformers (DiTs) are a powerful yet underexplored class of generative models compared to U-Net-based diffusion models. To bridge this gap, we introduce TIDE (Temporal-aware Sparse Autoencoders for Interpretable Diffusion transformErs), a novel framework that enhances temporal reconstruction within DiT activation layers across denoising steps. TIDE employs Sparse Autoencoders (SAEs) with a sparse bottleneck layer to extract interpretable and hierarchical features, revealing that diffusion models inherently learn hierarchical features at multiple levels (e.g., 3D, semantic, class) during generative pre-training. Our approach achieves state-of-the-art reconstruction performance, with a mean squared error (MSE) of 1e-3 and a cosine similarity of 0.97, demonstrating superior accuracy in capturing activation dynamics along the denoising trajectory. Beyond interpretability, we showcase TIDEâ€™s potential in downstream applications such as sparse activation-guided image editing and style transfer, enabling improved controllability for generative systems. By providing a comprehensive training and evaluation protocol tailored for DiTs, TIDE contributes to developing more interpretable, transparent, and trustworthy generative models. </p>
<blockquote>
<p>ä¸åŸºäºU-Netçš„æ‰©æ•£æ¨¡å‹ç›¸æ¯”ï¼Œæ‰©æ•£Transformerï¼ˆDiTsï¼‰æ˜¯ä¸€ç±»å¼ºå¤§ä½†å°šæœªè¢«å……åˆ†ç ”ç©¶çš„ç”Ÿæˆæ¨¡å‹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†TIDEï¼ˆç”¨äºå¯è§£é‡Šæ‰©æ•£Transformerçš„æ—¶åºæ„ŸçŸ¥ç¨€ç–è‡ªç¼–ç å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºDiTæ¿€æ´»å±‚å†…çš„æ—¶åºé‡å»ºèƒ½åŠ›ï¼Œè·¨è¶Šå»å™ªæ­¥éª¤ã€‚TIDEé‡‡ç”¨å…·æœ‰ç¨€ç–ç“¶é¢ˆå±‚çš„ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰æ¥æå–å¯è§£é‡Šå’Œåˆ†å±‚ç‰¹å¾ï¼Œæ­ç¤ºæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé¢„è®­ç»ƒè¿‡ç¨‹ä¸­æœ¬è´¨ä¸Šå­¦ä¹ å¤šå±‚ï¼ˆä¾‹å¦‚3Dã€è¯­ä¹‰ã€ç±»åˆ«ï¼‰çš„åˆ†å±‚ç‰¹å¾ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºæ€§èƒ½ï¼Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä¸º1e-3ï¼Œä½™å¼¦ç›¸ä¼¼åº¦ä¸º0.97ï¼Œè¯æ˜äº†åœ¨æ•è·å»å™ªè½¨è¿¹ä¸Šçš„æ¿€æ´»åŠ¨åŠ›å­¦æ–¹é¢çš„å“è¶Šå‡†ç¡®æ€§ã€‚é™¤äº†å¯è§£é‡Šæ€§å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†TIDEåœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œå¦‚ç¨€ç–æ¿€æ´»å¼•å¯¼çš„å›¾åƒç¼–è¾‘å’Œé£æ ¼è½¬æ¢ï¼Œä¸ºç”Ÿæˆç³»ç»Ÿæä¾›æ”¹è¿›çš„å¯æ§æ€§ã€‚é€šè¿‡ä¸ºDiTsæä¾›é‡èº«å®šåˆ¶çš„ç»¼åˆåŸ¹è®­å’Œè¯„ä¼°åè®®ï¼ŒTIDEä¸ºå¼€å‘æ›´å¯è§£é‡Šã€é€æ˜å’Œå¯ä¿¡èµ–çš„ç”Ÿæˆæ¨¡å‹åšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07050v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¨¡å‹çš„ä¸€ç§æ–°æ¡†æ¶â€”â€”TIDEï¼ˆç”¨äºå¯è§£é‡Šæ‰©æ•£è½¬æ¢å™¨çš„æ—¶åºæ„ŸçŸ¥ç¨€ç–è‡ªç¼–ç å™¨ï¼‰ã€‚TIDEé€šè¿‡åœ¨æ‰©æ•£è½¬æ¢å™¨çš„æ¿€æ´»å±‚ä¸­å¢å¼ºæ—¶åºé‡å»ºï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒé‡‡ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰å’Œç¨€ç–ç“¶é¢ˆå±‚æ¥æå–å¯è§£é‡Šå’Œåˆ†å±‚ç‰¹å¾ï¼Œæ­ç¤ºæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé¢„è®­ç»ƒè¿‡ç¨‹ä¸­å†…åœ¨åœ°å­¦ä¹ å¤šçº§ï¼ˆå¦‚3Dã€è¯­ä¹‰ã€ç±»åˆ«ï¼‰çš„åˆ†å±‚ç‰¹å¾ã€‚TIDEå®ç°äº†æœ€å…ˆè¿›çš„é‡å»ºæ€§èƒ½ï¼Œåœ¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰å’Œä½™å¼¦ç›¸ä¼¼æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å±•ç¤ºäº†TIDEåœ¨ä¸‹æ¸¸åº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œå¦‚ç¨€ç–æ¿€æ´»å¼•å¯¼çš„å›¾åƒç¼–è¾‘å’Œé£æ ¼è½¬æ¢ï¼Œä¸ºç”Ÿæˆç³»ç»Ÿæä¾›äº†æ›´å¥½çš„å¯æ§æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TIDEæ˜¯ä¸€ä¸ªç”¨äºå¢å¼ºæ‰©æ•£æ¨¡å‹æ€§èƒ½çš„æ–°æ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‰©æ•£è½¬æ¢å™¨çš„æ¿€æ´»å±‚ä¸­ã€‚</li>
<li>TIDEé‡‡ç”¨ç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰å’Œç¨€ç–ç“¶é¢ˆå±‚ï¼Œä»¥æå–å¯è§£é‡Šå’Œåˆ†å±‚ç‰¹å¾ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé¢„è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ å¤šçº§åˆ†å±‚ç‰¹å¾ã€‚</li>
<li>TIDEå®ç°äº†å…ˆè¿›çš„é‡å»ºæ€§èƒ½ï¼Œå…·æœ‰ä¼˜ç§€çš„å‡†ç¡®æ€§ã€‚</li>
<li>TIDEåœ¨ä¸‹æ¸¸åº”ç”¨ï¼Œå¦‚å›¾åƒç¼–è¾‘å’Œé£æ ¼è½¬æ¢ä¸­å±•ç¤ºäº†æ½œåŠ›ã€‚</li>
<li>TIDEä¸ºç”Ÿæˆç³»ç»Ÿæä¾›äº†æ›´å¥½çš„å¯æ§æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07050">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c5db533000c3d7c84df0ef1c2c7549dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d6d808f7233c1bcfab979a59f5cb761.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79eb551bb895f00cbab58ed9b7158101.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-930bf1edde475527733b012bec245f88.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65bb542a03fe3c32f6f0d3847a61e801.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Recovering-Partially-Corrupted-Major-Objects-through-Tri-modality-Based-Image-Completion"><a href="#Recovering-Partially-Corrupted-Major-Objects-through-Tri-modality-Based-Image-Completion" class="headerlink" title="Recovering Partially Corrupted Major Objects through Tri-modality Based   Image Completion"></a>Recovering Partially Corrupted Major Objects through Tri-modality Based   Image Completion</h2><p><strong>Authors:Yongle Zhang, Yimin Liu, Qiang Wu</strong></p>
<p>Diffusion models have become widely adopted in image completion tasks, with text prompts commonly employed to ensure semantic coherence by providing high-level guidance. However, a persistent challenge arises when an object is partially obscured in the damaged region, yet its remaining parts are still visible in the background. While text prompts offer semantic direction, they often fail to precisely recover fine-grained structural details, such as the objectâ€™s overall posture, ensuring alignment with the visible object information in the background. This limitation stems from the inability of text prompts to provide pixel-level specificity. To address this, we propose supplementing text-based guidance with a novel visual aid: a casual sketch, which can be roughly drawn by anyone based on visible object parts. This sketch supplies critical structural cues, enabling the generative model to produce an object structure that seamlessly integrates with the existing background. We introduce the Visual Sketch Self-Aware (VSSA) model, which integrates the casual sketch into each iterative step of the diffusion process, offering distinct advantages for partially corrupted scenarios. By blending sketch-derived features with those of the corrupted image, and leveraging text prompt guidance, the VSSA assists the diffusion model in generating images that preserve both the intended object semantics and structural consistency across the restored objects and original regions. To support this research, we created two datasets, CUB-sketch and MSCOCO-sketch, each combining images, sketches, and text. Extensive qualitative and quantitative experiments demonstrate that our approach outperforms several state-of-the-art methods. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè¡¥å…¨ä»»åŠ¡ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œé€šå¸¸é‡‡ç”¨æ–‡æœ¬æç¤ºæ¥é€šè¿‡æä¾›é«˜çº§æŒ‡å¯¼æ¥ä¿è¯è¯­ä¹‰è¿è´¯æ€§ã€‚ç„¶è€Œï¼Œå½“ä¸€ä¸ªç‰©ä½“åœ¨æŸååŒºåŸŸä¸­éƒ¨åˆ†è¢«é®æŒ¡ï¼Œä½†å…¶å‰©ä½™éƒ¨åˆ†ä»èƒ½åœ¨èƒŒæ™¯ä¸­å¯è§æ—¶ï¼Œå°±ä¼šå‡ºç°ä¸€ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚è™½ç„¶æ–‡æœ¬æç¤ºæä¾›äº†è¯­ä¹‰æ–¹å‘ï¼Œä½†å®ƒä»¬å¾€å¾€æ— æ³•ç²¾ç¡®æ¢å¤ç‰©ä½“çš„ç²¾ç»†ç»“æ„ç»†èŠ‚ï¼Œå¦‚ç‰©ä½“çš„æ•´ä½“å§¿åŠ¿ï¼Œä»¥åŠä¸èƒŒæ™¯ä¸­å¯è§ç‰©ä½“ä¿¡æ¯çš„å¯¹é½ã€‚è¿™ä¸€å±€é™æ€§æºäºæ–‡æœ¬æç¤ºæ— æ³•æä¾›åƒç´ çº§åˆ«çš„å…·ä½“ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è¡¥å……åŸºäºæ–‡æœ¬çš„æŒ‡å¯¼ä¸ä¸€ç§æ–°çš„è§†è§‰è¾…åŠ©å·¥å…·ï¼šéšæ„è‰å›¾ï¼Œè¯¥è‰å›¾å¯ä»¥æ ¹æ®å¯è§çš„ç‰©ä½“éƒ¨åˆ†ç”±ä»»ä½•äººç²—ç•¥ç»˜åˆ¶ã€‚è‰å›¾æä¾›äº†å…³é”®çš„ç»“æ„çº¿ç´¢ï¼Œä½¿ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿä¸ç°æœ‰èƒŒæ™¯æ— ç¼é›†æˆçš„ç‰©ä½“ç»“æ„ã€‚æˆ‘ä»¬å¼•å…¥äº†è§†è§‰è‰å›¾è‡ªæ„ŸçŸ¥ï¼ˆVSSAï¼‰æ¨¡å‹ï¼Œå®ƒå°†è‰å›¾é›†æˆåˆ°æ‰©æ•£è¿‡ç¨‹çš„æ¯ä¸€æ­¥ä¸­ï¼Œä¸ºéƒ¨åˆ†æŸåçš„åœºæ™¯æä¾›äº†æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚é€šè¿‡èåˆè‰å›¾ç‰¹å¾å’ŒæŸåå›¾åƒçš„ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨æ–‡æœ¬æç¤ºæŒ‡å¯¼ï¼ŒVSSAå¸®åŠ©æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ—¢ä¿ç•™æ„å›¾ç‰©ä½“è¯­ä¹‰åˆä¿æŒæ¢å¤ç‰©ä½“ä¸åŸå§‹åŒºåŸŸç»“æ„ä¸€è‡´æ€§çš„å›¾åƒã€‚ä¸ºäº†æ”¯æŒè¿™é¡¹ç ”ç©¶ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ï¼Œåˆ†åˆ«æ˜¯CUB-sketchå’ŒMSCOCO-sketchï¼Œæ¯ä¸ªæ•°æ®é›†éƒ½ç»“åˆäº†å›¾åƒã€è‰å›¾å’Œæ–‡æœ¬ã€‚å¤§é‡çš„å®šæ€§å’Œå®šé‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºå‡ ç§æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07047v1">PDF</a> 17 pages, 6 page supplementary</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒè¡¥å…¨ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œé’ˆå¯¹éƒ¨åˆ†é®æŒ¡ç‰©ä½“çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæ–‡æœ¬æç¤ºå’Œè‰å›¾è§†è§‰è¾…åŠ©çš„æ–°æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥Visual Sketch Self-Awareï¼ˆVSSAï¼‰æ¨¡å‹ï¼Œç»“åˆè‰å›¾æä¾›çš„ç»“æ„çº¿ç´¢ï¼Œè¾…åŠ©æ‰©æ•£æ¨¡å‹ç”Ÿæˆä¸èƒŒæ™¯æ— ç¼èåˆçš„å¯¹è±¡ç»“æ„ã€‚æ­¤å¤–ï¼Œè¿˜åˆ›å»ºäº†ä¸¤ä¸ªæ•°æ®é›†CUB-sketchå’ŒMSCOCO-sketchä»¥æ”¯æŒç ”ç©¶ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å¹¿æ³›åº”ç”¨äºå›¾åƒè¡¥å…¨ä»»åŠ¡ã€‚</li>
<li>æ–‡æœ¬æç¤ºåœ¨ä¿æŒè¯­ä¹‰è¿è´¯æ€§æ–¹é¢å‘æŒ¥é‡è¦ä½œç”¨ï¼Œä½†åœ¨æ¢å¤ç»†ç²’åº¦ç»“æ„ç»†èŠ‚æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»“åˆæ–‡æœ¬æç¤ºå’Œè‰å›¾è§†è§‰è¾…åŠ©çš„æ–°æ–¹æ³•ï¼Œä»¥è§£å†³éƒ¨åˆ†é®æŒ¡ç‰©ä½“çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥äº†Visual Sketch Self-Awareï¼ˆVSSAï¼‰æ¨¡å‹ï¼Œå°†è‰å›¾èå…¥æ‰©æ•£è¿‡ç¨‹çš„æ¯ä¸€æ­¥ã€‚</li>
<li>è‰å›¾æä¾›å…³é”®ç»“æ„çº¿ç´¢ï¼Œå¸®åŠ©ç”Ÿæˆä¸èƒŒæ™¯æ— ç¼èåˆçš„å¯¹è±¡ç»“æ„ã€‚</li>
<li>åˆ›å»ºäº†ä¸¤ä¸ªæ•°æ®é›†CUB-sketchå’ŒMSCOCO-sketchä»¥æ”¯æŒç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e8022fc28d3a432cdbbe110df4e3855.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c365754806de9f5ca8678f4c527946a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SOYO-A-Tuning-Free-Approach-for-Video-Style-Morphing-via-Style-Adaptive-Interpolation-in-Diffusion-Models"><a href="#SOYO-A-Tuning-Free-Approach-for-Video-Style-Morphing-via-Style-Adaptive-Interpolation-in-Diffusion-Models" class="headerlink" title="SOYO: A Tuning-Free Approach for Video Style Morphing via Style-Adaptive   Interpolation in Diffusion Models"></a>SOYO: A Tuning-Free Approach for Video Style Morphing via Style-Adaptive   Interpolation in Diffusion Models</h2><p><strong>Authors:Haoyu Zheng, Qifan Yu, Binghe Yu, Yang Dai, Wenqiao Zhang, Juncheng Li, Siliang Tang, Yueting Zhuang</strong></p>
<p>Diffusion models have achieved remarkable progress in image and video stylization. However, most existing methods focus on single-style transfer, while video stylization involving multiple styles necessitates seamless transitions between them. We refer to this smooth style transition between video frames as video style morphing. Current approaches often generate stylized video frames with discontinuous structures and abrupt style changes when handling such transitions. To address these limitations, we introduce SOYO, a novel diffusion-based framework for video style morphing. Our method employs a pre-trained text-to-image diffusion model without fine-tuning, combining attention injection and AdaIN to preserve structural consistency and enable smooth style transitions across video frames. Moreover, we notice that applying linear equidistant interpolation directly induces imbalanced style morphing. To harmonize across video frames, we propose a novel adaptive sampling scheduler operating between two style images. Extensive experiments demonstrate that SOYO outperforms existing methods in open-domain video style morphing, better preserving the structural coherence of video frames while achieving stable and smooth style transitions. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘é£æ ¼åŒ–æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å•ä¸€é£æ ¼è½¬æ¢ï¼Œè€Œæ¶‰åŠå¤šç§é£æ ¼çš„è§†é¢‘é£æ ¼åŒ–éœ€è¦é£æ ¼ä¹‹é—´çš„æ— ç¼è¿‡æ¸¡ã€‚æˆ‘ä»¬å°†è§†é¢‘å¸§ä¹‹é—´è¿™ç§å¹³æ»‘çš„é£æ ¼è¿‡æ¸¡ç§°ä¸ºè§†é¢‘é£æ ¼æ¸å˜ã€‚å½“å‰çš„æ–¹æ³•åœ¨å¤„ç†æ­¤ç±»è¿‡æ¸¡æ—¶ï¼Œé€šå¸¸ç”Ÿæˆçš„è§†é¢‘å¸§é£æ ¼åŒ–ç»“æ„ä¸è¿ç»­ï¼Œé£æ ¼å˜åŒ–çªå…€ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†SOYOï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„æ–°å‹è§†é¢‘é£æ ¼æ¸å˜æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œæ— éœ€å¾®è°ƒï¼Œç»“åˆæ³¨æ„åŠ›æ³¨å…¥å’ŒAdaINï¼Œä»¥ä¿ç•™ç»“æ„ä¸€è‡´æ€§å¹¶å®ç°è§†é¢‘å¸§ä¹‹é—´çš„å¹³æ»‘é£æ ¼è¿‡æ¸¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ç›´æ¥åº”ç”¨çº¿æ€§ç­‰è·æ’å€¼ä¼šå¯¼è‡´é£æ ¼æ¸å˜ä¸å¹³è¡¡ã€‚ä¸ºäº†åè°ƒè§†é¢‘å¸§ä¹‹é—´çš„å’Œè°åº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹è‡ªé€‚åº”é‡‡æ ·è°ƒåº¦å™¨ï¼Œåœ¨ä¸¤ç§é£æ ¼å›¾åƒä¹‹é—´è¿è¡Œã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSOYOåœ¨å¼€æ”¾åŸŸè§†é¢‘é£æ ¼æ¸å˜æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ›´å¥½åœ°ä¿ç•™äº†è§†é¢‘å¸§çš„ç»“æ„è¿è´¯æ€§ï¼ŒåŒæ—¶å®ç°äº†ç¨³å®šå’Œå¹³æ»‘çš„é£æ ¼è¿‡æ¸¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06998v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬æŒ‡å‡ºæ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘é£æ ¼åŒ–æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å•é£æ ¼è½¬ç§»ï¼Œå¯¹äºæ¶‰åŠå¤šç§é£æ ¼çš„è§†é¢‘é£æ ¼åŒ–ï¼Œéœ€è¦åœ¨è§†é¢‘å¸§ä¹‹é—´å®ç°å¹³æ»‘çš„é£æ ¼è¿‡æ¸¡ï¼Œç§°ä¸ºè§†é¢‘é£æ ¼æ¸å˜ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™ç§è¿‡æ¸¡æ—¶ï¼Œç”Ÿæˆçš„è§†é¢‘å¸§ç»“æ„ä¸è¿ç»­ï¼Œé£æ ¼å˜åŒ–çªå…€ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼•å…¥äº†SOYOï¼Œä¸€ç§åŸºäºæ‰©æ•£çš„è§†é¢‘é£æ ¼æ¸å˜æ–°æ¡†æ¶ã€‚è¯¥æ–¹æ³•é‡‡ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆæ³¨æ„åŠ›æ³¨å…¥å’ŒAdaINï¼Œæ— éœ€å¾®è°ƒå³å¯ä¿ç•™ç»“æ„ä¸€è‡´æ€§ï¼Œå®ç°è§†é¢‘å¸§ä¹‹é—´çš„å¹³æ»‘é£æ ¼è¿‡æ¸¡ã€‚è¿˜å‘ç°ç›´æ¥åº”ç”¨çº¿æ€§ç­‰è·æ’å€¼ä¼šå¯¼è‡´é£æ ¼æ¸å˜ä¸å¹³è¡¡ï¼Œæå‡ºä¸€ç§æ–°å‹è‡ªé€‚åº”é‡‡æ ·è°ƒåº¦å™¨ï¼Œåœ¨ä¸¤ç§é£æ ¼å›¾åƒä¹‹é—´åè°ƒæ“ä½œã€‚å®éªŒè¯æ˜ï¼ŒSOYOåœ¨å¼€æ”¾åŸŸè§†é¢‘é£æ ¼æ¸å˜æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ›´å¥½åœ°ä¿æŒè§†é¢‘å¸§çš„ç»“æ„è¿è´¯æ€§ï¼Œå®ç°ç¨³å®šå’Œå¹³æ»‘çš„é£æ ¼è¿‡æ¸¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒå’Œè§†é¢‘é£æ ¼åŒ–æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å¤„ç†å¤šé£æ ¼è¿‡æ¸¡æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>è§†é¢‘é£æ ¼æ¸å˜è¦æ±‚åœ¨è§†é¢‘å¸§ä¹‹é—´å®ç°å¹³æ»‘çš„é£æ ¼è¿‡æ¸¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†é£æ ¼è¿‡æ¸¡æ—¶ç”Ÿæˆçš„è§†é¢‘å¸§ç»“æ„ä¸è¿ç»­ï¼Œé£æ ¼å˜åŒ–çªå…€ã€‚</li>
<li>SOYOæ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„è§†é¢‘é£æ ¼æ¸å˜æ–°æ¡†æ¶ï¼Œé‡‡ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹ã€‚</li>
<li>SOYOé€šè¿‡ç»“åˆæ³¨æ„åŠ›æ³¨å…¥å’ŒAdaINï¼Œå®ç°ç»“æ„ä¸€è‡´æ€§å’Œå¹³æ»‘é£æ ¼è¿‡æ¸¡ã€‚</li>
<li>ç›´æ¥åº”ç”¨çº¿æ€§ç­‰è·æ’å€¼ä¼šå¯¼è‡´é£æ ¼æ¸å˜ä¸å¹³è¡¡ï¼ŒSOYOå¼•å…¥æ–°å‹è‡ªé€‚åº”é‡‡æ ·è°ƒåº¦å™¨è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>å®éªŒè¯æ˜SOYOåœ¨å¼€æ”¾åŸŸè§†é¢‘é£æ ¼æ¸å˜æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c8f8513a51d6339304a7a411a928ca39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-921bfb04c77e8718dfb35a0af4aea559.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70d43dc7f274f85d280aab6caa2bdbce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3da3e7fefe3401fb057c11085113b69.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Task-Specific-Knowledge-Distillation-from-the-Vision-Foundation-Model-for-Enhanced-Medical-Image-Segmentation"><a href="#Task-Specific-Knowledge-Distillation-from-the-Vision-Foundation-Model-for-Enhanced-Medical-Image-Segmentation" class="headerlink" title="Task-Specific Knowledge Distillation from the Vision Foundation Model   for Enhanced Medical Image Segmentation"></a>Task-Specific Knowledge Distillation from the Vision Foundation Model   for Enhanced Medical Image Segmentation</h2><p><strong>Authors:Pengchen Liang, Haishan Huang, Bin Pu, Jianguo Chen, Xiang Hua, Jing Zhang, Weibo Ma, Zhuangzhuang Chen, Yiwei Li, Qing Chang</strong></p>
<p>Large-scale pre-trained models, such as Vision Foundation Models (VFMs), have demonstrated impressive performance across various downstream tasks by transferring generalized knowledge, especially when target data is limited. However, their high computational cost and the domain gap between natural and medical images limit their practical application in medical segmentation tasks. Motivated by this, we pose the following important question: â€œHow can we effectively utilize the knowledge of large pre-trained VFMs to train a small, task-specific model for medical image segmentation when training data is limited?â€ To address this problem, we propose a novel and generalizable task-specific knowledge distillation framework. Our method fine-tunes the VFM on the target segmentation task to capture task-specific features before distilling the knowledge to smaller models, leveraging Low-Rank Adaptation (LoRA) to reduce the computational cost of fine-tuning. Additionally, we incorporate synthetic data generated by diffusion models to augment the transfer set, enhancing model performance in data-limited scenarios. Experimental results across five medical image datasets demonstrate that our method consistently outperforms task-agnostic knowledge distillation and self-supervised pretraining approaches like MoCo v3 and Masked Autoencoders (MAE). For example, on the KidneyUS dataset, our method achieved a 28% higher Dice score than task-agnostic KD using 80 labeled samples for fine-tuning. On the CHAOS dataset, it achieved an 11% improvement over MAE with 100 labeled samples. These results underscore the potential of task-specific knowledge distillation to train accurate, efficient models for medical image segmentation in data-constrained settings. </p>
<blockquote>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ï¼Œé€šè¿‡è¿ç§»é€šç”¨çŸ¥è¯†ï¼Œåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ç›®æ ‡æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„é«˜è®¡ç®—æˆæœ¬ä»¥åŠè‡ªç„¶å›¾åƒå’ŒåŒ»å­¦å›¾åƒä¹‹é—´çš„é¢†åŸŸå·®è·é™åˆ¶äº†å®ƒä»¬åœ¨åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥ä¸‹é—®é¢˜ï¼šâ€œå½“è®­ç»ƒæ•°æ®æœ‰é™æ—¶ï¼Œå¦‚ä½•æœ‰æ•ˆåœ°åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒVFMsçš„çŸ¥è¯†æ¥è®­ç»ƒç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„å°è§„æ¨¡ã€ç‰¹å®šä»»åŠ¡æ¨¡å‹ï¼Ÿâ€ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ä¸”é€šç”¨çš„ç‰¹å®šä»»åŠ¡çŸ¥è¯†è’¸é¦æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåœ¨ç›®æ ‡åˆ†å‰²ä»»åŠ¡ä¸Šå¾®è°ƒVFMä»¥æ•è·ç‰¹å®šä»»åŠ¡ç‰¹å¾ï¼Œç„¶åå°†çŸ¥è¯†è’¸é¦åˆ°è¾ƒå°çš„æ¨¡å‹ä¸Šï¼Œå¹¶åˆ©ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰æ¥é™ä½å¾®è°ƒçš„è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®æ¥å¢å¼ºè¿ç§»é›†ï¼Œä»è€Œæé«˜åœ¨æ•°æ®æœ‰é™åœºæ™¯ä¸­çš„æ¨¡å‹æ€§èƒ½ã€‚åœ¨äº”ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºä»»åŠ¡æ— å…³çš„çŸ¥è¯†è’¸é¦å’Œè‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ï¼Œå¦‚MoCo v3å’Œé®ç½©è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨KidneyUSæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨80ä¸ªæ ‡è®°æ ·æœ¬è¿›è¡Œå¾®è°ƒæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ‰€è·å¾—çš„Diceå¾—åˆ†æ¯”ä»»åŠ¡æ— å…³KDé«˜å‡º28%ã€‚åœ¨CHAOSæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨100ä¸ªæ ‡è®°æ ·æœ¬æ—¶ï¼Œä¸MAEç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†11%çš„æ”¹è¿›ã€‚è¿™äº›ç»“æœçªæ˜¾äº†ä»»åŠ¡ç‰¹å®šçŸ¥è¯†è’¸é¦åœ¨æ•°æ®å—é™ç¯å¢ƒä¸­è®­ç»ƒåŒ»å­¦å›¾åƒåˆ†å‰²å‡†ç¡®é«˜æ•ˆæ¨¡å‹çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06976v1">PDF</a> 29 pages, 10 figures, 16 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ï¼Œåœ¨å¤šç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå°¤å…¶åœ¨ç›®æ ‡æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹é€šè¿‡çŸ¥è¯†è¿ç§»å®ç°ã€‚ç„¶è€Œï¼Œå…¶é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ä»¥åŠè‡ªç„¶å›¾åƒä¸åŒ»ç–—å›¾åƒé¢†åŸŸé—´çš„å·®è·ï¼Œé™åˆ¶äº†å…¶åœ¨åŒ»ç–—åˆ†å‰²ä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªé‡è¦é—®é¢˜ï¼šâ€œå¦‚ä½•åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆåˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒVFMsçš„çŸ¥è¯†æ¥è®­ç»ƒå°å‹ã€é’ˆå¯¹åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡çš„æ¨¡å‹ï¼Ÿâ€ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªé€šç”¨ã€é’ˆå¯¹ä»»åŠ¡çš„çŸ¥è¯†è’¸é¦æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆå¯¹VFMè¿›è¡Œç›®æ ‡åˆ†å‰²ä»»åŠ¡çš„å¾®è°ƒï¼Œä»¥æ•è·ä»»åŠ¡ç‰¹å®šç‰¹å¾ï¼Œç„¶åå°†çŸ¥è¯†è’¸é¦åˆ°å°å‹æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å‡å°‘å¾®è°ƒçš„è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®æ¥å¢å¼ºè½¬ç§»é›†ï¼Œæé«˜æ¨¡å‹åœ¨æ•°æ®æœ‰é™åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚åœ¨äº”ä¸ªåŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸€è‡´ä¼˜äºä»»åŠ¡æ— å…³çš„çŸ¥è¯†è’¸é¦å’Œè‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ï¼Œå¦‚MoCo v3å’ŒMasked Autoencodersï¼ˆMAEï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨KidneyUSæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨80ä¸ªæ ·æœ¬è¿›è¡Œå¾®è°ƒæ—¶ï¼ŒDiceå¾—åˆ†æ¯”ä»»åŠ¡æ— å…³KDé«˜å‡º28%ã€‚åœ¨CHAOSæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨100ä¸ªæ ·æœ¬æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”MAEæé«˜äº†11%ã€‚è¿™äº›ç»“æœçªæ˜¾äº†ä»»åŠ¡ç‰¹å®šçŸ¥è¯†è’¸é¦åœ¨æ•°æ®å—é™æƒ…å†µä¸‹è®­ç»ƒåŒ»ç–—å›¾åƒåˆ†å‰²å‡†ç¡®é«˜æ•ˆæ¨¡å‹çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹é¢„è®­ç»ƒæ¨¡å‹åœ¨åŒ»ç–—åˆ†å‰²ä»»åŠ¡ä¸­é¢ä¸´é«˜è®¡ç®—æˆæœ¬å’Œé¢†åŸŸå·®è·é—®é¢˜ã€‚</li>
<li>æå‡ºåˆ©ç”¨ä»»åŠ¡ç‰¹å®šçŸ¥è¯†è’¸é¦æ¡†æ¶æ¥è®­ç»ƒå°å‹åŒ»ç–—å›¾åƒåˆ†å‰²æ¨¡å‹ã€‚</li>
<li>é€šè¿‡å¾®è°ƒVFMæ•è·ä»»åŠ¡ç‰¹å®šç‰¹å¾ï¼Œç„¶åå°†å…¶çŸ¥è¯†è’¸é¦åˆ°å°å‹æ¨¡å‹ã€‚</li>
<li>åˆ©ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å‡å°‘å¾®è°ƒçš„è®¡ç®—æˆæœ¬ã€‚</li>
<li>ç»“åˆæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®å¢å¼ºè½¬ç§»é›†ï¼Œæé«˜åœ¨æ•°æ®æœ‰é™åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªåŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä»»åŠ¡æ— å…³çš„çŸ¥è¯†è’¸é¦å’Œè‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-58ae457d9ee1c9832fe38b8fea10f2c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec3eee9ac1b4bb9c2f9a032481f8d9b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ce061ede3e7180fe56d5a79c0ac96a6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="From-Reusing-to-Forecasting-Accelerating-Diffusion-Models-with-TaylorSeers"><a href="#From-Reusing-to-Forecasting-Accelerating-Diffusion-Models-with-TaylorSeers" class="headerlink" title="From Reusing to Forecasting: Accelerating Diffusion Models with   TaylorSeers"></a>From Reusing to Forecasting: Accelerating Diffusion Models with   TaylorSeers</h2><p><strong>Authors:Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, Linfeng Zhang</strong></p>
<p>Diffusion Transformers (DiT) have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. To solve this problem, feature caching has been proposed to accelerate diffusion models by caching the features in the previous timesteps and then reusing them in the following timesteps. However, at timesteps with significant intervals, the feature similarity in diffusion models decreases substantially, leading to a pronounced increase in errors introduced by feature caching, significantly harming the generation quality. To solve this problem, we propose TaylorSeer, which firstly shows that features of diffusion models at future timesteps can be predicted based on their values at previous timesteps. Based on the fact that features change slowly and continuously across timesteps, TaylorSeer employs a differential method to approximate the higher-order derivatives of features and predict features in future timesteps with Taylor series expansion. Extensive experiments demonstrate its significant effectiveness in both image and video synthesis, especially in high acceleration ratios. For instance, it achieves an almost lossless acceleration of 4.99$\times$ on FLUX and 5.00$\times$ on HunyuanVideo without additional training. On DiT, it achieves $3.41$ lower FID compared with previous SOTA at $4.53$$\times$ acceleration. %Our code is provided in the supplementary materials and will be made publicly available on GitHub. Our codes have been released in Github:<a target="_blank" rel="noopener" href="https://github.com/Shenyi-Z/TaylorSeer">https://github.com/Shenyi-Z/TaylorSeer</a> </p>
<blockquote>
<p>Diffusion Transformersï¼ˆDiTï¼‰å·²ç»å®ç°äº†é«˜ä¿çœŸå›¾åƒå’Œè§†é¢‘åˆæˆçš„é©å‘½æ€§è¿›å±•ï¼Œä½†å…¶è®¡ç®—éœ€æ±‚å¯¹äºå®æ—¶åº”ç”¨æ¥è¯´ä»ç„¶å¾ˆå¤§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ç‰¹å¾ç¼“å­˜æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ï¼Œé€šè¿‡ç¼“å­˜å‰ä¸€æ—¶é—´æ­¥çš„ç‰¹å¾å¹¶åœ¨åç»­æ—¶é—´æ­¥ä¸­é‡å¤ä½¿ç”¨å®ƒä»¬ã€‚ç„¶è€Œï¼Œåœ¨æ—¶é—´æ­¥é•¿è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œæ‰©æ•£æ¨¡å‹ä¸­çš„ç‰¹å¾ç›¸ä¼¼æ€§ä¼šå¤§å¹…ä¸‹é™ï¼Œå¯¼è‡´ç”±ç‰¹å¾ç¼“å­˜å¼•å…¥çš„é”™è¯¯æ˜¾è‘—å¢åŠ ï¼Œä»è€Œä¸¥é‡æŸå®³ç”Ÿæˆè´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†TaylorSeerã€‚å®ƒé¦–å…ˆè¡¨æ˜ï¼Œå¯ä»¥æ ¹æ®ä¹‹å‰æ—¶é—´æ­¥çš„å€¼é¢„æµ‹æ‰©æ•£æ¨¡å‹åœ¨æœªæ¥æ—¶é—´æ­¥çš„ç‰¹å¾ã€‚åŸºäºç‰¹å¾åœ¨æ—¶é—´æ­¥ä¸Šç¼“æ…¢ä¸”è¿ç»­å˜åŒ–çš„ç‰¹æ€§ï¼ŒTaylorSeeré‡‡ç”¨å·®åˆ†æ–¹æ³•è¿‘ä¼¼ç‰¹å¾çš„é«˜é˜¶å¯¼æ•°ï¼Œå¹¶ä½¿ç”¨æ³°å‹’çº§æ•°å±•å¼€é¢„æµ‹æœªæ¥æ—¶é—´æ­¥çš„ç‰¹å¾ã€‚å¤§é‡å®éªŒè¯æ˜ï¼Œå®ƒåœ¨å›¾åƒå’Œè§†é¢‘åˆæˆä¸­éƒ½å–å¾—äº†æ˜¾è‘—çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åŠ é€Ÿæ¯”çš„æƒ…å†µä¸‹ã€‚ä¾‹å¦‚ï¼Œåœ¨FLUXå’ŒHunyuanVideoä¸Šå®ç°äº†è¿‘ä¹æ— æŸçš„4.99å€å’Œ5.00å€çš„åŠ é€Ÿï¼›åœ¨DiTä¸Šï¼Œä»¥4.53å€çš„åŠ é€Ÿå®ç°äº†æ¯”å…ˆå‰æœ€ä½³æ°´å¹³ä½3.41çš„FIDå¾—åˆ†ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šï¼š<a target="_blank" rel="noopener" href="https://github.com/Shenyi-Z/TaylorSeer">https://github.com/Shenyi-Z/TaylorSeer</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06923v1">PDF</a> 13 pages, 14 figures</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰åœ¨é«˜è´¨é‡å›¾åƒå’Œè§†é¢‘åˆæˆé¢†åŸŸå®ç°äº†é©å‘½æ€§çš„è¿›å±•ï¼Œä½†å…¶è®¡ç®—éœ€æ±‚ä»ç„¶å¾ˆå¤§ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶åº”ç”¨çš„è¦æ±‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ç‰¹å¾ç¼“å­˜æ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ï¼Œä½†åœ¨æ—¶é—´æ­¥é•¿è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œç‰¹å¾ç›¸ä¼¼æ€§ä¼šé™ä½ï¼Œå¯¼è‡´ç‰¹å¾ç¼“å­˜å¼•å…¥çš„é”™è¯¯å¢åŠ ï¼Œä¸¥é‡å½±å“ç”Ÿæˆè´¨é‡ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TaylorSeeræ–¹æ³•ã€‚å®ƒåŸºäºæ‰©æ•£æ¨¡å‹åœ¨ä¹‹å‰æ—¶é—´æ­¥çš„ç‰¹å¾å€¼æ¥é¢„æµ‹æœªæ¥æ—¶é—´æ­¥çš„ç‰¹å¾å€¼ã€‚åˆ©ç”¨ç‰¹å¾åœ¨æ—¶åºä¸Šç¼“æ…¢ä¸”è¿ç»­çš„å˜åŒ–ï¼Œé€šè¿‡å¾®åˆ†æ³•è®¡ç®—ç‰¹å¾çš„é«˜é˜¶å¯¼æ•°å¹¶ä½¿ç”¨æ³°å‹’çº§æ•°å±•å¼€è¿›è¡Œé¢„æµ‹ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒå’Œè§†é¢‘åˆæˆä¸­æ•ˆæœæ˜¾è‘—ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åŠ é€Ÿæ¯”çš„æƒ…å†µä¸‹ã€‚ä¾‹å¦‚ï¼Œåœ¨FLUXå’ŒHunyuanVideoä¸Šå®ç°äº†è¿‘ä¹æ— æŸçš„4.99Ã—å’Œ5.00Ã—åŠ é€Ÿï¼›åœ¨DiTä¸Šï¼Œä»¥4.53Ã—çš„åŠ é€Ÿæ¯”å®ç°äº†ç›¸æ¯”ä¹‹å‰æœ€ä½³æ°´å¹³é™ä½3.41çš„FIDã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰åœ¨å›¾åƒå’Œè§†é¢‘åˆæˆä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†è®¡ç®—éœ€æ±‚è¾ƒå¤§ï¼Œä¸åˆ©äºå®æ—¶åº”ç”¨ã€‚</li>
<li>ç‰¹å¾ç¼“å­˜è¢«æå‡ºæ¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹ï¼Œä½†åœ¨æ—¶é—´æ­¥é•¿å¤§çš„æƒ…å†µä¸‹ä¼šå‡ºç°é—®é¢˜ï¼Œå¯¼è‡´ç”Ÿæˆè´¨é‡ä¸‹é™ã€‚</li>
<li>TaylorSeeræ–¹æ³•åŸºäºä¹‹å‰æ—¶é—´æ­¥çš„ç‰¹å¾å€¼é¢„æµ‹æœªæ¥æ—¶é—´æ­¥çš„ç‰¹å¾å€¼ï¼Œè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨ç‰¹å¾åœ¨æ—¶åºä¸Šçš„ç¼“æ…¢ä¸”è¿ç»­å˜åŒ–ï¼Œé€šè¿‡å¾®åˆ†æ³•é¢„æµ‹æœªæ¥ç‰¹å¾ã€‚</li>
<li>TaylorSeeråœ¨å›¾åƒå’Œè§†é¢‘åˆæˆä¸­æ•ˆæœæ˜¾è‘—ï¼Œå®ç°äº†é«˜åŠ é€Ÿæ¯”ä¸‹çš„é«˜è´¨é‡ç”Ÿæˆã€‚</li>
<li>åœ¨FLUXå’ŒHunyuanVideoä¸Šå®ç°äº†è¿‘ä¹æ— æŸçš„åŠ é€Ÿï¼Œä»£ç å·²å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06923">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3607102b64eadf8249b6f4ee8a4966f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-300e931a18026173c30dc092e6a5fdce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08e8e1b14edc0afa45d14e5e2034a149.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-442dbee35784316728882a1725e3ab44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b32ee3bd902268c9ad1779be93de6cb4.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Text-to-Image-Diffusion-Models-Cannot-Count-and-Prompt-Refinement-Cannot-Help"><a href="#Text-to-Image-Diffusion-Models-Cannot-Count-and-Prompt-Refinement-Cannot-Help" class="headerlink" title="Text-to-Image Diffusion Models Cannot Count, and Prompt Refinement   Cannot Help"></a>Text-to-Image Diffusion Models Cannot Count, and Prompt Refinement   Cannot Help</h2><p><strong>Authors:Yuefan Cao, Xuyang Guo, Jiayan Huo, Yingyu Liang, Zhenmei Shi, Zhao Song, Jiahao Zhang, Zhen Zhuang</strong></p>
<p>Generative modeling is widely regarded as one of the most essential problems in todayâ€™s AI community, with text-to-image generation having gained unprecedented real-world impacts. Among various approaches, diffusion models have achieved remarkable success and have become the de facto solution for text-to-image generation. However, despite their impressive performance, these models exhibit fundamental limitations in adhering to numerical constraints in user instructions, frequently generating images with an incorrect number of objects. While several prior works have mentioned this issue, a comprehensive and rigorous evaluation of this limitation remains lacking. To address this gap, we introduce T2ICountBench, a novel benchmark designed to rigorously evaluate the counting ability of state-of-the-art text-to-image diffusion models. Our benchmark encompasses a diverse set of generative models, including both open-source and private systems. It explicitly isolates counting performance from other capabilities, provides structured difficulty levels, and incorporates human evaluations to ensure high reliability.   Extensive evaluations with T2ICountBench reveal that all state-of-the-art diffusion models fail to generate the correct number of objects, with accuracy dropping significantly as the number of objects increases. Additionally, an exploratory study on prompt refinement demonstrates that such simple interventions generally do not improve counting accuracy. Our findings highlight the inherent challenges in numerical understanding within diffusion models and point to promising directions for future improvements. </p>
<blockquote>
<p>ç”Ÿæˆå»ºæ¨¡è¢«å¹¿å¤§AIç¤¾åŒºè§†ä¸ºå½“ä»Šæœ€é‡è¦çš„è¯¾é¢˜ä¹‹ä¸€ï¼Œæ–‡æœ¬åˆ°å›¾åƒçš„ç”Ÿæˆå·²ç»äº§ç”Ÿäº†å‰æ‰€æœªæœ‰çš„ç°å®å½±å“ã€‚åœ¨ä¼—å¤šæ–¹æ³•ä¸­ï¼Œæ‰©æ•£æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„æˆå°±ï¼Œå¹¶æˆä¸ºæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å®é™…è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå°½ç®¡è¿™äº›æ¨¡å‹è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬åœ¨ç”¨æˆ·æŒ‡ä»¤çš„æ•°å€¼çº¦æŸéµå¾ªæ–¹é¢å­˜åœ¨åŸºæœ¬å±€é™ï¼Œç»å¸¸ç”Ÿæˆå…·æœ‰é”™è¯¯å¯¹è±¡æ•°é‡çš„å›¾åƒã€‚å°½ç®¡ä¸€äº›æ—©æœŸçš„å·¥ä½œå·²ç»æåˆ°äº†è¿™ä¸ªé—®é¢˜ï¼Œä½†å¯¹äºè¿™ä¸€å±€é™æ€§çš„å…¨é¢å’Œä¸¥æ ¼è¯„ä¼°ä»ç„¶ç¼ºä¹ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†T2ICountBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ä¸¥æ ¼è¯„ä¼°æœ€æ–°æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è®¡æ•°èƒ½åŠ›ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ¶µç›–äº†å„ç§ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬å¼€æºå’Œç§æœ‰ç³»ç»Ÿã€‚å®ƒæ˜ç¡®åœ°å°†è®¡æ•°æ€§èƒ½ä¸å…¶ä»–èƒ½åŠ›åŒºåˆ†å¼€æ¥ï¼Œæä¾›ç»“æ„åŒ–çš„éš¾åº¦çº§åˆ«ï¼Œå¹¶çº³å…¥äººç±»è¯„ä¼°ä»¥ç¡®ä¿é«˜å¯é æ€§ã€‚ä½¿ç”¨T2ICountBenchè¿›è¡Œçš„å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œæ‰€æœ‰æœ€å…ˆè¿›çš„æ‰©æ•£æ¨¡å‹éƒ½æ— æ³•ç”Ÿæˆæ­£ç¡®æ•°é‡çš„å¯¹è±¡ï¼Œéšç€å¯¹è±¡æ•°é‡çš„å¢åŠ ï¼Œå‡†ç¡®æ€§æ˜¾è‘—é™ä½ã€‚æ­¤å¤–ï¼Œå…³äºæç¤ºç»†åŒ–çš„æ¢ç´¢æ€§ç ”ç©¶è¡¨æ˜ï¼Œè¿™æ ·çš„ç®€å•å¹²é¢„ä¸€èˆ¬ä¸ä¼šæé«˜è®¡æ•°å‡†ç¡®æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†æ‰©æ•£æ¨¡å‹ä¸­æ•°å€¼ç†è§£æ–¹é¢çš„å›ºæœ‰æŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥æ”¹è¿›çš„æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06884v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†ç”Ÿæˆæ¨¡å‹åœ¨AIé¢†åŸŸçš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ–¹é¢ã€‚æ‰©æ•£æ¨¡å‹åœ¨è¿™ä¸€é¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†å­˜åœ¨æ•°å€¼çº¦æŸæ–¹é¢çš„æ ¹æœ¬å±€é™æ€§ï¼Œå³åœ¨éµå¾ªç”¨æˆ·æŒ‡ä»¤ä¸­çš„æ•°é‡æ—¶ç»å¸¸å‡ºç°é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°å·¥å…·T2ICountBenchï¼Œä»¥ä¸¥æ ¼è¯„ä¼°æœ€å…ˆè¿›æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è®¡æ•°èƒ½åŠ›ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œæ‰€æœ‰å…ˆè¿›æ‰©æ•£æ¨¡å‹éƒ½æ— æ³•æ­£ç¡®ç”Ÿæˆå¯¹è±¡æ•°é‡ï¼Œéšç€å¯¹è±¡æ•°é‡å¢åŠ ï¼Œå‡†ç¡®æ€§æ˜¾è‘—ä¸‹é™ã€‚æç¤ºæ”¹è¿›çš„æ¢ç´¢æ€§ç ”ç©¶ä¹Ÿè¡¨æ˜ï¼Œç®€å•çš„å¹²é¢„æªæ–½é€šå¸¸ä¸èƒ½æé«˜è®¡æ•°å‡†ç¡®æ€§ã€‚è¿™æ­ç¤ºäº†æ‰©æ•£æ¨¡å‹ä¸­æ•°å€¼ç†è§£çš„å›ºæœ‰æŒ‘æˆ˜ï¼Œå¹¶ä¸ºæœªæ¥çš„æ”¹è¿›æŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢†åŸŸå–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œæˆä¸ºè¯¥é¢†åŸŸçš„å®é™…è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨éµå¾ªç”¨æˆ·æŒ‡ä»¤ä¸­çš„æ•°é‡æ–¹é¢å­˜åœ¨æ ¹æœ¬å±€é™æ€§ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°å·¥å…·T2ICountBenchï¼Œä»¥ä¸¥æ ¼è¯„ä¼°æœ€å…ˆè¿›æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è®¡æ•°èƒ½åŠ›ã€‚</li>
<li>è¯„ä¼°ç»“æœæ˜¾ç¤ºæ‰€æœ‰å…ˆè¿›æ‰©æ•£æ¨¡å‹éƒ½æ— æ³•æ­£ç¡®ç”ŸæˆæŒ‡å®šæ•°é‡çš„å¯¹è±¡ã€‚</li>
<li>éšç€å¯¹è±¡æ•°é‡çš„å¢åŠ ï¼Œæ¨¡å‹çš„å‡†ç¡®æ€§æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>ç®€å•çš„æç¤ºæ”¹è¿›æªæ–½é€šå¸¸ä¸èƒ½æé«˜æ¨¡å‹çš„è®¡æ•°å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06884">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9c68dbe1e195ab84782bbeb4faa4023f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5570164cba5d686dd88f1a30dbf53579.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Reconstruction-vs-Generation-Taming-Optimization-Dilemma-in-Latent-Diffusion-Models"><a href="#Reconstruction-vs-Generation-Taming-Optimization-Dilemma-in-Latent-Diffusion-Models" class="headerlink" title="Reconstruction vs. Generation: Taming Optimization Dilemma in Latent   Diffusion Models"></a>Reconstruction vs. Generation: Taming Optimization Dilemma in Latent   Diffusion Models</h2><p><strong>Authors:Jingfeng Yao, Bin Yang, Xinggang Wang</strong></p>
<p>Latent diffusion models with Transformer architectures excel at generating high-fidelity images. However, recent studies reveal an optimization dilemma in this two-stage design: while increasing the per-token feature dimension in visual tokenizers improves reconstruction quality, it requires substantially larger diffusion models and more training iterations to achieve comparable generation performance. Consequently, existing systems often settle for sub-optimal solutions, either producing visual artifacts due to information loss within tokenizers or failing to converge fully due to expensive computation costs. We argue that this dilemma stems from the inherent difficulty in learning unconstrained high-dimensional latent spaces. To address this, we propose aligning the latent space with pre-trained vision foundation models when training the visual tokenizers. Our proposed VA-VAE (Vision foundation model Aligned Variational AutoEncoder) significantly expands the reconstruction-generation frontier of latent diffusion models, enabling faster convergence of Diffusion Transformers (DiT) in high-dimensional latent spaces. To exploit the full potential of VA-VAE, we build an enhanced DiT baseline with improved training strategies and architecture designs, termed LightningDiT. The integrated system achieves state-of-the-art (SOTA) performance on ImageNet 256x256 generation with an FID score of 1.35 while demonstrating remarkable training efficiency by reaching an FID score of 2.11 in just 64 epochsâ€“representing an over 21 times convergence speedup compared to the original DiT. Models and codes are available at: <a target="_blank" rel="noopener" href="https://github.com/hustvl/LightningDiT">https://github.com/hustvl/LightningDiT</a>. </p>
<blockquote>
<p>åŸºäºTransformeræ¶æ„çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆé«˜ä¿çœŸå›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶æ­ç¤ºäº†è¿™ç§ä¸¤é˜¶æ®µè®¾è®¡ä¸­çš„ä¼˜åŒ–å›°å¢ƒï¼šè™½ç„¶å¢åŠ è§†è§‰åˆ†è¯å™¨çš„æ¯ä»¤ç‰Œç‰¹å¾ç»´åº¦æé«˜äº†é‡å»ºè´¨é‡ï¼Œä½†éœ€è¦æ›´å¤§è§„æ¨¡çš„æ‰©æ•£æ¨¡å‹å’Œæ›´å¤šçš„è®­ç»ƒè¿­ä»£æ‰èƒ½è¾¾åˆ°ç›¸å½“çš„ç”Ÿæˆæ€§èƒ½ã€‚å› æ­¤ï¼Œç°æœ‰ç³»ç»Ÿé€šå¸¸é€‰æ‹©æ¬¡ä¼˜è§£å†³æ–¹æ¡ˆï¼Œè¦ä¹ˆç”±äºåˆ†è¯å™¨å†…éƒ¨çš„ä¿¡æ¯ä¸¢å¤±è€Œäº§ç”Ÿè§†è§‰ä¼ªå½±ï¼Œè¦ä¹ˆç”±äºè®¡ç®—æˆæœ¬é«˜æ˜‚è€Œæ— æ³•å®Œå…¨æ”¶æ•›ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè¿™ç§å›°å¢ƒæºäºå­¦ä¹ æ— çº¦æŸçš„é«˜ç»´æ½œåœ¨ç©ºé—´çš„å›ºæœ‰å›°éš¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.01423v3">PDF</a> Models and codes are available at:   <a target="_blank" rel="noopener" href="https://github.com/hustvl/LightningDiT">https://github.com/hustvl/LightningDiT</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åŸºäºTransformeræ¶æ„çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹åœ¨é«˜ä¿çœŸå›¾åƒç”Ÿæˆæ–¹é¢çš„ä¼˜åŠ¿ã€‚ç ”ç©¶æ­ç¤ºäº†ä¸¤é˜¶æ®µè®¾è®¡ä¸­çš„ä¼˜åŒ–å›°å¢ƒï¼šæé«˜è§†è§‰ä»¤ç‰Œå™¨ä¸­çš„æ¯ä»¤ç‰Œç‰¹å¾ç»´åº¦å¯ä»¥æé«˜é‡å»ºè´¨é‡ï¼Œä½†éœ€è¦æ›´å¤§çš„æ‰©æ•£æ¨¡å‹å’Œæ›´å¤šçš„è®­ç»ƒè¿­ä»£æ¥å®ç°ç›¸å½“çš„ç”Ÿæˆæ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸€å›°å¢ƒï¼Œæœ¬æ–‡æå‡ºåœ¨è®­ç»ƒè§†è§‰ä»¤ç‰Œå™¨æ—¶ï¼Œä¸é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹å¯¹é½æ½œåœ¨ç©ºé—´ã€‚æå‡ºçš„VA-VAEï¼ˆä¸è§†è§‰åŸºç¡€æ¨¡å‹å¯¹é½çš„å˜åˆ†è‡ªç¼–ç å™¨ï¼‰æ˜¾è‘—æ‰©å±•äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é‡å»º-ç”Ÿæˆè¾¹ç•Œï¼Œä½¿é«˜ç»´æ½œåœ¨ç©ºé—´ä¸­çš„Diffusion Transformersï¼ˆDiTï¼‰æ›´å¿«æ”¶æ•›ã€‚ä¸ºå……åˆ†å‘æŒ¥VA-VAEçš„æ½œåŠ›ï¼Œå»ºç«‹äº†å¢å¼ºå‹DiTåŸºçº¿ï¼Œå…·æœ‰æ”¹è¿›çš„è®­ç»ƒç­–ç•¥å’Œæ¶æ„è®¾è®¡ï¼Œç§°ä¸ºLightningDiTã€‚é›†æˆç³»ç»Ÿåœ¨ImageNet 256x256ç”Ÿæˆæ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒFIDå¾—åˆ†ä¸º1.35ï¼ŒåŒæ—¶å±•ç°å‡ºæƒŠäººçš„è®­ç»ƒæ•ˆç‡ï¼Œä»…64ä¸ªå‘¨æœŸå°±è¾¾åˆ°FID 2.11ï¼Œç›¸è¾ƒäºåŸå§‹DiTå®ç°äº†è¶…è¿‡21å€çš„æ”¶æ•›é€Ÿåº¦æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹èƒ½ç”Ÿæˆé«˜ä¿çœŸå›¾åƒï¼Œä½†å­˜åœ¨ä¼˜åŒ–å›°å¢ƒï¼šæé«˜è§†è§‰ä»¤ç‰Œå™¨ä¸­çš„ç‰¹å¾ç»´åº¦å¯æé«˜é‡å»ºè´¨é‡ï¼Œä½†å¢åŠ æ¨¡å‹å¤§å°å’Œè®­ç»ƒæˆæœ¬ã€‚</li>
<li>å›°å¢ƒæºäºå­¦ä¹ æ— çº¦æŸçš„é«˜ç»´æ½œåœ¨ç©ºé—´çš„å†…åœ¨å›°éš¾ã€‚</li>
<li>æå‡ºVA-VAEæ–¹æ³•ï¼Œé€šè¿‡ä¸é¢„è®­ç»ƒçš„è§†è§‰åŸºç¡€æ¨¡å‹å¯¹é½æ½œåœ¨ç©ºé—´æ¥è§£å†³ä¼˜åŒ–å›°å¢ƒã€‚</li>
<li>VA-VAEæ˜¾è‘—æ‰©å±•äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é‡å»º-ç”Ÿæˆè¾¹ç•Œï¼ŒåŠ é€ŸDiffusion Transformersï¼ˆDiTï¼‰åœ¨é«˜ç»´ç©ºé—´çš„æ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>å»ºç«‹äº†å¢å¼ºå‹DiTåŸºçº¿LightningDiTï¼Œå…·æœ‰æ”¹è¿›çš„è®­ç»ƒç­–ç•¥å’Œæ¶æ„è®¾è®¡ã€‚</li>
<li>LightningDiTåœ¨ImageNet 256x256ç”Ÿæˆæ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼ŒFIDå¾—åˆ†1.35ã€‚</li>
<li>LightningDiTå±•ç°å‡ºæƒŠäººçš„è®­ç»ƒæ•ˆç‡ï¼Œä»…64ä¸ªå‘¨æœŸå°±è¾¾åˆ°FID 2.11ï¼Œç›¸è¾ƒäºåŸå§‹DiTå®ç°äº†è¶…è¿‡21å€çš„æ”¶æ•›é€Ÿåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.01423">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-85213357a5a4693ca98c0227897e08e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a10a542730ad74615533cbde5d6e1ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8877c5be4035d26b5ebd6da24d11bd30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e60dfd4398a443f913d36d2291ba73dd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0320301a76d4f34cf5debcf77a8b2246.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a5491fc00b9cb05f455c3ee8b31acdd.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Rethinking-Diffusion-Based-Image-Generators-for-Fundus-Fluorescein-Angiography-Synthesis-on-Limited-Data"><a href="#Rethinking-Diffusion-Based-Image-Generators-for-Fundus-Fluorescein-Angiography-Synthesis-on-Limited-Data" class="headerlink" title="Rethinking Diffusion-Based Image Generators for Fundus Fluorescein   Angiography Synthesis on Limited Data"></a>Rethinking Diffusion-Based Image Generators for Fundus Fluorescein   Angiography Synthesis on Limited Data</h2><p><strong>Authors:Chengzhou Yu, Huihui Fang, Hongqiu Wang, Ting Deng, Qing Du, Yanwu Xu, Weihua Yang</strong></p>
<p>Fundus imaging is a critical tool in ophthalmology, with different imaging modalities offering unique advantages. For instance, fundus fluorescein angiography (FFA) can accurately identify eye diseases. However, traditional invasive FFA involves the injection of sodium fluorescein, which can cause discomfort and risks. Generating corresponding FFA images from non-invasive fundus images holds significant practical value but also presents challenges. First, limited datasets constrain the performance and effectiveness of models. Second, previous studies have primarily focused on generating FFA for single diseases or single modalities, often resulting in poor performance for patients with various ophthalmic conditions. To address these issues, we propose a novel latent diffusion model-based framework, Diffusion, which introduces a fine-tuning protocol to overcome the challenge of limited medical data and unleash the generative capabilities of diffusion models. Furthermore, we designed a new approach to tackle the challenges of generating across different modalities and disease types. On limited datasets, our framework achieves state-of-the-art results compared to existing methods, offering significant potential to enhance ophthalmic diagnostics and patient care. Our code will be released soon to support further research in this field. </p>
<blockquote>
<p>çœ¼åº•æˆåƒåœ¨çœ¼ç§‘ä¸­æ˜¯ä¸€ç§è‡³å…³é‡è¦çš„å·¥å…·ï¼Œä¸åŒçš„æˆåƒæ¨¡å¼éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿ã€‚ä¾‹å¦‚ï¼Œçœ¼åº•è§å…‰è¡€ç®¡é€ å½±ï¼ˆFFAï¼‰å¯ä»¥å‡†ç¡®è¯†åˆ«çœ¼éƒ¨ç–¾ç—…ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ä¾µå…¥æ€§FFAéœ€è¦æ³¨å°„è§å…‰ç´ é’ ï¼Œå¯èƒ½ä¼šå¼•å‘ä¸é€‚å’Œé£é™©ã€‚ä»éä¾µå…¥æ€§çš„çœ¼åº•å›¾åƒç”Ÿæˆç›¸åº”çš„FFAå›¾åƒå…·æœ‰é‡å¤§çš„å®ç”¨ä»·å€¼ï¼Œä½†ä¹Ÿå­˜åœ¨æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæœ‰é™çš„æ•°æ®é›†é™åˆ¶äº†æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆæœã€‚å…¶æ¬¡ï¼Œä»¥å¾€çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä¸ºå•ä¸€ç–¾ç—…æˆ–å•ä¸€æ¨¡å¼ç”ŸæˆFFAï¼Œå¾€å¾€å¯¼è‡´å¯¹æ‚£æœ‰å¤šç§çœ¼ç§‘ç–¾ç—…çš„æ‚£è€…çš„è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ–°æ¡†æ¶â€”â€”Diffusionï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ç²¾ç»†è°ƒæ•´åè®®ï¼Œä»¥å…‹æœåŒ»å­¦æ•°æ®æœ‰é™çš„æŒ‘æˆ˜ï¼Œå¹¶é‡Šæ”¾æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œä»¥è§£å†³ä¸åŒæ¨¡å¼å’Œç–¾ç—…ç±»å‹ç”Ÿæˆæ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚åœ¨æœ‰é™çš„æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œä¸ºçœ¼ç§‘è¯Šæ–­å’Œæ‚£è€…æŠ¤ç†æä¾›äº†å·¨å¤§çš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å°†å¾ˆå¿«å‘å¸ƒï¼Œä»¥æ”¯æŒè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.12778v2">PDF</a> The first author has a conflict with the data access authority</p>
<p><strong>Summary</strong><br>     åŸºé‡‘çœ¼æˆåƒåœ¨çœ¼ç§‘é¢†åŸŸå…·æœ‰é‡è¦ä½œç”¨ï¼Œä¸åŒçš„æˆåƒæ¨¡å¼å…·æœ‰ç‹¬ç‰¹çš„ä¼˜åŠ¿ã€‚ä¼ ç»Ÿçš„åŸºé‡‘è§å…‰ç´ è¡€ç®¡é€ å½±ï¼ˆFFAï¼‰ä¼šå¸¦æ¥ä¸é€‚å’Œé£é™©ã€‚é€šè¿‡éä¾µå…¥å¼çš„åŸºé‡‘çœ¼å›¾åƒç”Ÿæˆç›¸åº”çš„FFAå›¾åƒå…·æœ‰å®é™…æ„ä¹‰ï¼Œä½†é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ï¼Œé€šè¿‡å¾®è°ƒåè®®å…‹æœæœ‰é™çš„åŒ»ç–—æ•°æ®æŒ‘æˆ˜ï¼Œå¹¶é‡Šæ”¾æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æœ‰é™æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æˆæœã€‚è¯¥æ¡†æ¶å¯¹çœ¼ç§‘è¯Šæ–­å’Œæ‚£è€…æŠ¤ç†æœ‰é‡è¦æ„ä¹‰ã€‚æˆ‘ä»¬å³å°†å‘å¸ƒä»£ç ï¼Œä»¥æ”¯æŒè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºé‡‘çœ¼æˆåƒåœ¨çœ¼ç§‘ä¸­èµ·é‡è¦ä½œç”¨ï¼Œä¸åŒçš„æˆåƒæ¨¡å¼æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿ã€‚</li>
<li>ä¼ ç»ŸFFAæ–¹æ³•å…·æœ‰ä¾µå…¥æ€§ï¼Œå¯èƒ½å¯¼è‡´æ‚£è€…ä¸é€‚å’Œé£é™©ã€‚</li>
<li>éä¾µå…¥å¼ç”ŸæˆFFAå›¾åƒå…·æœ‰å®é™…æ„ä¹‰ï¼Œä½†é¢ä¸´æ•°æ®é™åˆ¶å’Œè·¨ç–¾ç—…ç±»å‹ç”Ÿæˆçš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ¡†æ¶ï¼Œé€šè¿‡å¾®è°ƒåè®®è§£å†³æœ‰é™åŒ»ç–—æ•°æ®é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨æœ‰é™æ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›çš„æˆæœã€‚</li>
<li>è¯¥æ¡†æ¶å¯¹çœ¼ç§‘è¯Šæ–­å’Œæ‚£è€…æŠ¤ç†æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.12778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d982d1ef28fead14cca5cac80d81809.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c0a3825815a33580b489e24de68f4f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cee12aa4e439f9df9678c5e5eeb78e7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Efficient-Dataset-Distillation-via-Diffusion-Driven-Patch-Selection-for-Improved-Generalization"><a href="#Efficient-Dataset-Distillation-via-Diffusion-Driven-Patch-Selection-for-Improved-Generalization" class="headerlink" title="Efficient Dataset Distillation via Diffusion-Driven Patch Selection for   Improved Generalization"></a>Efficient Dataset Distillation via Diffusion-Driven Patch Selection for   Improved Generalization</h2><p><strong>Authors:Xinhao Zhong, Shuoyang Sun, Xulin Gu, Zhaoyang Xu, Yaowei Wang, Min Zhang, Bin Chen</strong></p>
<p>Dataset distillation offers an efficient way to reduce memory and computational costs by optimizing a smaller dataset with performance comparable to the full-scale original. However, for large datasets and complex deep networks (e.g., ImageNet-1K with ResNet-101), the extensive optimization space limits performance, reducing its practicality. Recent approaches employ pre-trained diffusion models to generate informative images directly, avoiding pixel-level optimization and achieving notable results. However, these methods often face challenges due to distribution shifts between pre-trained models and target datasets, along with the need for multiple distillation steps across varying settings. To address these issues, we propose a novel framework orthogonal to existing diffusion-based distillation methods, leveraging diffusion models for selection rather than generation. Our method starts by predicting noise generated by the diffusion model based on input images and text prompts (with or without label text), then calculates the corresponding loss for each pair. With the loss differences, we identify distinctive regions of the original images. Additionally, we perform intra-class clustering and ranking on selected patches to maintain diversity constraints. This streamlined framework enables a single-step distillation process, and extensive experiments demonstrate that our approach outperforms state-of-the-art methods across various metrics. </p>
<blockquote>
<p>æ•°æ®é›†è’¸é¦æä¾›äº†ä¸€ç§é€šè¿‡ä¼˜åŒ–å°å‹æ•°æ®é›†æ¥å‡å°‘å†…å­˜å’Œè®¡ç®—æˆæœ¬çš„æœ‰æ•ˆæ–¹æ³•ï¼Œå…¶æ€§èƒ½å¯ä¸å…¨å°ºå¯¸åŸå§‹æ•°æ®é›†ç›¸å½“ã€‚ç„¶è€Œï¼Œå¯¹äºå¤§å‹æ•°æ®é›†å’Œå¤æ‚çš„æ·±åº¦ç½‘ç»œï¼ˆä¾‹å¦‚ï¼Œå¸¦æœ‰ResNet-101çš„ImageNet-1Kï¼‰ï¼Œå¹¿æ³›çš„ä¼˜åŒ–ç©ºé—´é™åˆ¶äº†æ€§èƒ½ï¼Œé™ä½äº†å…¶å®ç”¨æ€§ã€‚æœ€è¿‘çš„æ–¹æ³•é‡‡ç”¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ç›´æ¥ç”Ÿæˆä¿¡æ¯å›¾åƒï¼Œé¿å…äº†åƒç´ çº§ä¼˜åŒ–ï¼Œå¹¶å–å¾—äº†æ˜¾è‘—çš„ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¸¸å¸¸é¢ä¸´é¢„è®­ç»ƒæ¨¡å‹ä¸ç›®æ ‡æ•°æ®é›†ä¹‹é—´åˆ†å¸ƒå˜åŒ–çš„é—®é¢˜ï¼Œä»¥åŠåœ¨ä¸åŒè®¾ç½®ä¸‹éœ€è¦å¤šæ¬¡è’¸é¦æ­¥éª¤çš„éœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸ç°æœ‰åŸºäºæ‰©æ•£çš„è’¸é¦æ–¹æ³•æ­£äº¤çš„æ–°å‹æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé€‰æ‹©è€Œä¸æ˜¯ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåŸºäºè¾“å…¥å›¾åƒå’Œæ–‡æœ¬æç¤ºï¼ˆå¸¦æˆ–ä¸å¸¦æ ‡ç­¾æ–‡æœ¬ï¼‰é¢„æµ‹ç”±æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„å™ªå£°ï¼Œç„¶åè®¡ç®—æ¯å¯¹å¯¹åº”çš„æŸå¤±ã€‚é€šè¿‡æŸå¤±å·®å¼‚ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºåŸå§‹å›¾åƒçš„ç‹¬ç‰¹åŒºåŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹é€‰å®šçš„å°å—è¿›è¡Œç±»å†…èšç±»å’Œæ’åï¼Œä»¥ä¿æŒå¤šæ ·æ€§çº¦æŸã€‚è¿™ç§ç®€åŒ–çš„æ¡†æ¶å®ç°äº†å•æ­¥è’¸é¦è¿‡ç¨‹ï¼Œå¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æŒ‡æ ‡ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09959v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ•°æ®é›†è’¸é¦çš„æ‰©æ•£æ¨¡å‹ç ”ç©¶æå‡ºäº†æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•çš„é€šè¿‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé€‰æ‹©å’Œé¢„æµ‹ï¼Œé¿å…äº†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„åˆ†å¸ƒåç§»é—®é¢˜ï¼Œå®ç°äº†å•ä¸€æ­¥éª¤è’¸é¦è¿‡ç¨‹ï¼Œå¹¶åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚æ­¤æ–¹æ³•é€šè¿‡è®¡ç®—è¾“å…¥å›¾åƒå’Œæ–‡æœ¬æç¤ºä¸‹çš„å™ªå£°é¢„æµ‹æŸå¤±å·®å¼‚ï¼Œç¡®å®šåŸå§‹å›¾åƒçš„ç‹¬ç‰¹åŒºåŸŸï¼Œå¹¶è¿›è¡Œå†…éƒ¨ç±»åˆ«èšç±»æ’åä»¥ä¿æŒå¤šæ ·æ€§çº¦æŸã€‚æ­¤æ¡†æ¶åœ¨ä¿è¯æ•ˆç‡çš„åŒæ—¶ä¼˜åŒ–æ€§èƒ½è¡¨ç°ï¼Œä»¥å¯¹æŠ—å¤§å‹æ•°æ®é›†å’Œå¤æ‚æ·±åº¦ç½‘ç»œçš„æŒ‘æˆ˜ã€‚ </p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ‰©æ•£æ¨¡å‹æ•°æ®é›†è’¸é¦å¯æœ‰æ•ˆé™ä½å†…å­˜å’Œè®¡ç®—æˆæœ¬ï¼Œé€šè¿‡ä¼˜åŒ–å°å‹æ•°æ®é›†å®ç°ä¸åŸå§‹è§„æ¨¡ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¤§å‹æ•°æ®é›†å’Œå¤æ‚æ·±åº¦ç½‘ç»œï¼ˆå¦‚ImageNet-1Kä¸ResNet-101ï¼‰çš„åº”ç”¨ä¸­ï¼Œå¹¿æ³›çš„ä¼˜åŒ–ç©ºé—´é™åˆ¶äº†æ€§èƒ½ï¼Œå‡å°‘å…¶å®ç”¨æ€§ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç›´æ¥ç”Ÿæˆå›¾åƒæ˜¯ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥é¿å…åƒç´ çº§çš„ä¼˜åŒ–å¹¶è¾¾æˆæ˜¾è‘—ç»“æœï¼Œä½†ä¹Ÿé¢ä¸´åˆ†å¸ƒåç§»å’Œéœ€è¦å¤šä¸ªè’¸é¦æ­¥éª¤çš„æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„è’¸é¦æ¡†æ¶ï¼Œä¸ç°æœ‰æ‰©æ•£è’¸é¦æ–¹æ³•æ­£äº¤ï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹è¿›è¡Œç­›é€‰è€Œéç”Ÿæˆæ¥å¤„ç†ä¸Šè¿°é—®é¢˜ã€‚ </li>
<li>æ–¹æ³•åŒ…æ‹¬åŸºäºè¾“å…¥å›¾åƒå’Œæ–‡æœ¬æç¤ºé¢„æµ‹æ‰©æ•£æ¨¡å‹äº§ç”Ÿçš„å™ªå£°ï¼Œè®¡ç®—æ¯å¯¹æŸå¤±å·®å¼‚ä»¥ç¡®å®šåŸå§‹å›¾åƒçš„ç‹¬ç‰¹åŒºåŸŸã€‚ </li>
<li>é€šè¿‡è¿›è¡Œå†…éƒ¨ç±»åˆ«èšç±»æ’åæ¥ä¿æŒå¤šæ ·æ€§çº¦æŸï¼Œå®ç°å•ä¸€æ­¥éª¤è’¸é¦è¿‡ç¨‹ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b50570252e2ad2a3a8e5b4a4174862a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1cc4c96ad788fd7e73452b326ccb4ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca2860f93f47efd33301a053ad22e258.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2e3b569d9e9dec367d35ac3d55caf4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-137d154fa4dae3f25d35c1f318898f4d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="C-DiffSET-Leveraging-Latent-Diffusion-for-SAR-to-EO-Image-Translation-with-Confidence-Guided-Reliable-Object-Generation"><a href="#C-DiffSET-Leveraging-Latent-Diffusion-for-SAR-to-EO-Image-Translation-with-Confidence-Guided-Reliable-Object-Generation" class="headerlink" title="C-DiffSET: Leveraging Latent Diffusion for SAR-to-EO Image Translation   with Confidence-Guided Reliable Object Generation"></a>C-DiffSET: Leveraging Latent Diffusion for SAR-to-EO Image Translation   with Confidence-Guided Reliable Object Generation</h2><p><strong>Authors:Jeonghyeok Do, Jaehyup Lee, Munchurl Kim</strong></p>
<p>Synthetic Aperture Radar (SAR) imagery provides robust environmental and temporal coverage (e.g., during clouds, seasons, day-night cycles), yet its noise and unique structural patterns pose interpretation challenges, especially for non-experts. SAR-to-EO (Electro-Optical) image translation (SET) has emerged to make SAR images more perceptually interpretable. However, traditional approaches trained from scratch on limited SAR-EO datasets are prone to overfitting. To address these challenges, we introduce Confidence Diffusion for SAR-to-EO Translation, called C-DiffSET, a framework leveraging pretrained Latent Diffusion Model (LDM) extensively trained on natural images, thus enabling effective adaptation to the EO domain. Remarkably, we find that the pretrained VAE encoder aligns SAR and EO images in the same latent space, even with varying noise levels in SAR inputs. To further improve pixel-wise fidelity for SET, we propose a confidence-guided diffusion (C-Diff) loss that mitigates artifacts from temporal discrepancies, such as appearing or disappearing objects, thereby enhancing structural accuracy. C-DiffSET achieves state-of-the-art (SOTA) results on multiple datasets, significantly outperforming the very recent image-to-image translation methods and SET methods with large margins. </p>
<blockquote>
<p>é›·è¾¾å›¾åƒæä¾›ç¨³å¥çš„ç¯å¢ƒå’Œæ—¶é—´è¦†ç›–ï¼ˆä¾‹å¦‚åœ¨äº‘ã€å­£èŠ‚ã€æ˜¼å¤œå¾ªç¯æœŸé—´ï¼‰ï¼Œä½†å…¶å™ªå£°å’Œç‹¬ç‰¹çš„ç»“æ„æ¨¡å¼å¯¹è§£é‡Šæå‡ºäº†æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºéä¸“å®¶è€Œè¨€ã€‚SARåˆ°EOï¼ˆå…‰ç”µï¼‰å›¾åƒç¿»è¯‘ï¼ˆSETï¼‰çš„å‡ºç°ä½¿å¾—SARå›¾åƒæ›´å®¹æ˜“æ„ŸçŸ¥å’Œç†è§£ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•ä»é›¶å¼€å§‹ä½¿ç”¨æœ‰é™çš„SAR-EOæ•°æ®é›†è¿›è¡Œè®­ç»ƒå®¹æ˜“è¿‡åº¦æ‹Ÿåˆã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SARåˆ°EOç¿»è¯‘çš„ç½®ä¿¡åº¦æ‰©æ•£ï¼ˆConfidence Diffusionï¼‰ï¼Œç§°ä¸ºC-DiffSETæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰è¿›è¡Œå¤§é‡è‡ªç„¶å›¾åƒè®­ç»ƒï¼Œä»è€Œå®ç°æœ‰æ•ˆçš„é€‚åº”å…‰ç”µé¢†åŸŸã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°é¢„è®­ç»ƒçš„VAEç¼–ç å™¨èƒ½å¤Ÿåœ¨åŒä¸€æ½œåœ¨ç©ºé—´ä¸­åŒ¹é…SARå’ŒEOå›¾åƒï¼Œå³ä½¿åœ¨SARè¾“å…¥ä¸­å­˜åœ¨ä¸åŒçš„å™ªå£°æ°´å¹³ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜SETçš„åƒç´ çº§ä¿çœŸåº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç½®ä¿¡åº¦å¼•å¯¼æ‰©æ•£ï¼ˆC-Diffï¼‰æŸå¤±ï¼Œè¯¥æŸå¤±å‡è½»äº†ç”±äºæ—¶é—´å·®å¼‚è€Œäº§ç”Ÿçš„ä¼ªå½±ï¼Œå¦‚å‡ºç°çš„ç‰©ä½“æˆ–æ¶ˆå¤±çš„ç‰©ä½“ï¼Œä»è€Œæé«˜ç»“æ„å‡†ç¡®æ€§ã€‚C-DiffSETåœ¨å¤šæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œæ˜¾è‘—ä¼˜äºæœ€æ–°çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ–¹æ³•å’ŒSETæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10788v3">PDF</a> Please visit our project page   <a target="_blank" rel="noopener" href="https://kaist-viclab.github.io/C-DiffSET_site/">https://kaist-viclab.github.io/C-DiffSET_site/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆæˆå­”å¾„é›·è¾¾ï¼ˆSARï¼‰å½±åƒçš„è§£è¯»æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†åŸºäºé¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„SAR-EOå½±åƒç¿»è¯‘æ–¹æ³•â€”â€”C-DiffSETã€‚é€šè¿‡åˆ©ç”¨å¤§è§„æ¨¡è‡ªç„¶å›¾åƒè®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆåœ°é€‚åº”äº†EOé¢†åŸŸã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†ç½®ä¿¡æ‰©æ•£æŸå¤±ï¼Œæé«˜äº†åƒç´ çº§çš„ä¿çœŸåº¦ï¼Œå¢å¼ºäº†ç»“æ„å‡†ç¡®æ€§ã€‚C-DiffSETåœ¨å¤šæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–å›¾åƒè½¬æ¢å’ŒSETæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SARå½±åƒæä¾›ç¨³å¥çš„ç¯å¢ƒå’Œæ—¶åºè¦†ç›–ï¼Œä½†å…¶å™ªå£°å’Œç‹¬ç‰¹ç»“æ„æ¨¡å¼ç»™éä¸“å®¶å¸¦æ¥è§£è¯»æŒ‘æˆ˜ã€‚</li>
<li>SAR-to-EOå›¾åƒç¿»è¯‘ï¼ˆSETï¼‰ä½¿SARå›¾åƒæ›´æ˜“äºæ„ŸçŸ¥å’Œç†è§£ã€‚</li>
<li>ä¼ ç»ŸSETæ–¹æ³•ä»æœ‰é™çš„SAR-EOæ•°æ®é›†ä¸­è®­ç»ƒæ˜“äº§ç”Ÿè¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>C-DiffSETæ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œå¹¿æ³›è®­ç»ƒï¼Œå¹¶èƒ½æœ‰æ•ˆåœ°é€‚åº”EOé¢†åŸŸã€‚</li>
<li>VAEç¼–ç å™¨èƒ½å¤Ÿå°†SARå’ŒEOå½±åƒåœ¨ç›¸åŒæ½œåœ¨ç©ºé—´ä¸­å¯¹é½ï¼Œå³ä½¿SARè¾“å…¥å­˜åœ¨ä¸åŒå™ªå£°çº§åˆ«ã€‚</li>
<li>ç½®ä¿¡æ‰©æ•£æŸå¤±è¢«å¼•å…¥æ¥æé«˜åƒç´ çº§çš„ä¿çœŸåº¦å’Œç»“æ„å‡†ç¡®æ€§ï¼Œå‡å°‘å› æ—¶é—´å·®å¼‚å¯¼è‡´çš„ä¼ªå½±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10788">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9ca8bf84afc01767354a1898fbde7a11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91f427c8b8d482ea9499b0c2fb55e123.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9127f5a6f9b3d619618060abd6d02c6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51678b149dbba80ed6cd688e2dc7e421.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="TDSM-Triplet-Diffusion-for-Skeleton-Text-Matching-in-Zero-Shot-Action-Recognition"><a href="#TDSM-Triplet-Diffusion-for-Skeleton-Text-Matching-in-Zero-Shot-Action-Recognition" class="headerlink" title="TDSM: Triplet Diffusion for Skeleton-Text Matching in Zero-Shot Action   Recognition"></a>TDSM: Triplet Diffusion for Skeleton-Text Matching in Zero-Shot Action   Recognition</h2><p><strong>Authors:Jeonghyeok Do, Munchurl Kim</strong></p>
<p>We firstly present a diffusion-based action recognition with zero-shot learning for skeleton inputs. In zero-shot skeleton-based action recognition, aligning skeleton features with the text features of action labels is essential for accurately predicting unseen actions. Previous methods focus on direct alignment between skeleton and text latent spaces, but the modality gaps between these spaces hinder robust generalization learning. Motivated from the remarkable performance of text-to-image diffusion models, we leverage their alignment capabilities between different modalities mostly by focusing on the training process during reverse diffusion rather than using their generative power. Based on this, our framework is designed as a Triplet Diffusion for Skeleton-Text Matching (TDSM) method which aligns skeleton features with text prompts through reverse diffusion, embedding the prompts into the unified skeleton-text latent space to achieve robust matching. To enhance discriminative power, we introduce a novel triplet diffusion (TD) loss that encourages our TDSM to correct skeleton-text matches while pushing apart incorrect ones. Our TDSM significantly outperforms the very recent state-of-the-art methods with large margins of 2.36%-point to 13.05%-point, demonstrating superior accuracy and scalability in zero-shot settings through effective skeleton-text matching. </p>
<blockquote>
<p>æˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„é›¶æ ·æœ¬å­¦ä¹ åŠ¨ä½œè¯†åˆ«æ–¹æ³•ï¼Œç”¨äºéª¨æ¶è¾“å…¥ã€‚åœ¨é›¶æ ·æœ¬éª¨æ¶åŠ¨ä½œè¯†åˆ«ä¸­ï¼Œå°†éª¨æ¶ç‰¹å¾ä¸åŠ¨ä½œæ ‡ç­¾çš„æ–‡æœ¬ç‰¹å¾å¯¹é½æ˜¯å‡†ç¡®é¢„æµ‹æœªè§åŠ¨ä½œçš„å…³é”®ã€‚ä¹‹å‰çš„æ–¹æ³•ä¾§é‡äºéª¨æ¶å’Œæ–‡æœ¬æ½œåœ¨ç©ºé—´ä¹‹é—´çš„ç›´æ¥å¯¹é½ï¼Œä½†è¿™äº›ç©ºé—´ä¹‹é—´çš„æ¨¡æ€å·®è·é˜»ç¢äº†é²æ£’çš„æ³›åŒ–å­¦ä¹ ã€‚å—æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„å‡ºè‰²æ€§èƒ½çš„å¯å‘ï¼Œæˆ‘ä»¬ä¸»è¦åˆ©ç”¨å®ƒä»¬åœ¨ä¸åŒæ¨¡æ€ä¹‹é—´çš„å¯¹é½èƒ½åŠ›ï¼Œä¾§é‡äºåå‘æ‰©æ•£è¿‡ç¨‹ä¸­çš„è®­ç»ƒè¿‡ç¨‹ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å®ƒä»¬çš„ç”Ÿæˆèƒ½åŠ›ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å°†æ¡†æ¶è®¾è®¡ä¸ºä¸€ç§ç”¨äºéª¨æ¶æ–‡æœ¬åŒ¹é…çš„Tripleæ‰©æ•£æ–¹æ³•ï¼ˆTDSMï¼‰ï¼Œå®ƒé€šè¿‡åå‘æ‰©æ•£å°†éª¨æ¶ç‰¹å¾ä¸æ–‡æœ¬æç¤ºå¯¹é½ï¼Œå°†æç¤ºåµŒå…¥ç»Ÿä¸€çš„éª¨æ¶æ–‡æœ¬æ½œåœ¨ç©ºé—´ä»¥å®ç°ç¨³å¥åŒ¹é…ã€‚ä¸ºäº†æé«˜é‰´åˆ«åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„ä¸‰é‡æ‰©æ•£ï¼ˆTDï¼‰æŸå¤±ï¼Œå®ƒé¼“åŠ±æˆ‘ä»¬çš„TDSMçº æ­£éª¨æ¶æ–‡æœ¬åŒ¹é…ï¼ŒåŒæ—¶æ¨å¼€é”™è¯¯çš„åŒ¹é…ã€‚æˆ‘ä»¬çš„TDSMæ˜¾è‘—ä¼˜äºæœ€æ–°çš„å…ˆè¿›æ–¹æ³•ï¼Œå‡†ç¡®ç‡æé«˜äº†2.36%è‡³13.05%ï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸­é€šè¿‡æœ‰æ•ˆçš„éª¨æ¶æ–‡æœ¬åŒ¹é…å±•ç°äº†å‡ºè‰²çš„å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10745v3">PDF</a> Please visit our project page at   <a target="_blank" rel="noopener" href="https://kaist-viclab.github.io/TDSM_site/">https://kaist-viclab.github.io/TDSM_site/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬å­¦ä¹ éª¨æ¶åŠ¨ä½œè¯†åˆ«æ–¹æ³•ã€‚ä¸ºè§£å†³éª¨æ¶å’Œæ–‡æœ¬ç‰¹å¾å¯¹é½é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå€Ÿé‰´æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›ï¼Œè®¾è®¡äº†ä¸€ç§åä¸ºTriplet Diffusion for Skeleton-Text Matching (TDSM)çš„æ¡†æ¶ã€‚é€šè¿‡åå‘æ‰©æ•£è¿‡ç¨‹å®ç°éª¨æ¶ç‰¹å¾ä¸æ–‡æœ¬æç¤ºçš„å¯¹é½ï¼Œå¹¶å¼•å…¥æ–°å‹çš„ä¸‰å…ƒæ‰©æ•£æŸå¤±å‡½æ•°ï¼Œä»¥æé«˜æ¨¡å‹çš„é‰´åˆ«åŠ›å’ŒåŒ¹é…å‡†ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTDSMæ¡†æ¶åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹æ˜¾è‘—ä¼˜äºæœ€æ–°å…ˆè¿›æ–¹æ³•ï¼Œå±•ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„é›¶æ ·æœ¬å­¦ä¹ éª¨æ¶åŠ¨ä½œè¯†åˆ«ã€‚</li>
<li>éª¨æ¶ç‰¹å¾ä¸æ–‡æœ¬ç‰¹å¾å¯¹é½æ˜¯é¢„æµ‹æœªè§åŠ¨ä½œçš„å…³é”®ã€‚</li>
<li>å€Ÿé‰´æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è·¨æ¨¡æ€å¯¹é½èƒ½åŠ›ã€‚</li>
<li>è®¾è®¡äº†Triplet Diffusion for Skeleton-Text Matching (TDSM)æ¡†æ¶ã€‚</li>
<li>é€šè¿‡åå‘æ‰©æ•£è¿‡ç¨‹å®ç°éª¨æ¶ä¸æ–‡æœ¬çš„å¯¹é½ã€‚</li>
<li>å¼•å…¥æ–°å‹çš„ä¸‰å…ƒæ‰©æ•£æŸå¤±å‡½æ•°ï¼Œæé«˜æ¨¡å‹çš„é‰´åˆ«åŠ›å’ŒåŒ¹é…å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3644e74676b9dc84cbe56eaa9a2be221.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bbc954f2d09b3c65bf6993d6cb1c1dc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea38732c16acdccb925307e4953e445d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="ControlAR-Controllable-Image-Generation-with-Autoregressive-Models"><a href="#ControlAR-Controllable-Image-Generation-with-Autoregressive-Models" class="headerlink" title="ControlAR: Controllable Image Generation with Autoregressive Models"></a>ControlAR: Controllable Image Generation with Autoregressive Models</h2><p><strong>Authors:Zongming Li, Tianheng Cheng, Shoufa Chen, Peize Sun, Haocheng Shen, Longjin Ran, Xiaoxin Chen, Wenyu Liu, Xinggang Wang</strong></p>
<p>Autoregressive (AR) models have reformulated image generation as next-token prediction, demonstrating remarkable potential and emerging as strong competitors to diffusion models. However, control-to-image generation, akin to ControlNet, remains largely unexplored within AR models. Although a natural approach, inspired by advancements in Large Language Models, is to tokenize control images into tokens and prefill them into the autoregressive model before decoding image tokens, it still falls short in generation quality compared to ControlNet and suffers from inefficiency. To this end, we introduce ControlAR, an efficient and effective framework for integrating spatial controls into autoregressive image generation models. Firstly, we explore control encoding for AR models and propose a lightweight control encoder to transform spatial inputs (e.g., canny edges or depth maps) into control tokens. Then ControlAR exploits the conditional decoding method to generate the next image token conditioned on the per-token fusion between control and image tokens, similar to positional encodings. Compared to prefilling tokens, using conditional decoding significantly strengthens the control capability of AR models but also maintains the modelâ€™s efficiency. Furthermore, the proposed ControlAR surprisingly empowers AR models with arbitrary-resolution image generation via conditional decoding and specific controls. Extensive experiments can demonstrate the controllability of the proposed ControlAR for the autoregressive control-to-image generation across diverse inputs, including edges, depths, and segmentation masks. Furthermore, both quantitative and qualitative results indicate that ControlAR surpasses previous state-of-the-art controllable diffusion models, e.g., ControlNet++. Code, models, and demo will soon be available at <a target="_blank" rel="noopener" href="https://github.com/hustvl/ControlAR">https://github.com/hustvl/ControlAR</a>. </p>
<blockquote>
<p>è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹é€šè¿‡å°†å›¾åƒç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼Œå±•ç°äº†å·¨å¤§çš„æ½œåŠ›ï¼Œå¹¶æˆä¸ºæ‰©æ•£æ¨¡å‹çš„å¼ºåŠ²ç«äº‰å¯¹æ‰‹ã€‚ç„¶è€Œï¼Œç±»ä¼¼äºControlNetçš„æ§åˆ¶åˆ°å›¾åƒç”Ÿæˆåœ¨ARæ¨¡å‹ä¸­ä»ç„¶æœªè¢«å¹¿æ³›æ¢ç´¢ã€‚è™½ç„¶å—å¤§å‹è¯­è¨€æ¨¡å‹è¿›å±•çš„å¯å‘ï¼Œå°†æ§åˆ¶å›¾åƒæ ‡è®°åŒ–çš„è‡ªç„¶æ–¹æ³•æ˜¯åœ¨è§£ç å›¾åƒæ ‡è®°ä¹‹å‰å°†å®ƒä»¬é¢„å…ˆå¡«å……åˆ°è‡ªå›å½’æ¨¡å‹ä¸­ï¼Œä½†åœ¨ç”Ÿæˆè´¨é‡æ–¹é¢ä¸æ§åˆ¶ç½‘ç›¸æ¯”ä»ç„¶æœ‰æ‰€ä¸è¶³ï¼Œå¹¶ä¸”å­˜åœ¨æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ControlARï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”æœ‰æ•ˆçš„æ¡†æ¶ï¼Œç”¨äºå°†ç©ºé—´æ§åˆ¶é›†æˆåˆ°è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ¢ç´¢äº†ARæ¨¡å‹çš„æ§åˆ¶ç¼–ç ï¼Œå¹¶æå‡ºäº†ä¸€ç§è½»é‡çº§æ§åˆ¶ç¼–ç å™¨ï¼Œå°†ç©ºé—´è¾“å…¥ï¼ˆå¦‚åå°¼è¾¹ç¼˜æˆ–æ·±åº¦å›¾ï¼‰è½¬æ¢ä¸ºæ§åˆ¶æ ‡è®°ã€‚ç„¶åControlARåˆ©ç”¨æ¡ä»¶è§£ç æ–¹æ³•æ¥ç”Ÿæˆä¸‹ä¸€ä¸ªå›¾åƒæ ‡è®°ï¼Œè¯¥æ ‡è®°æ˜¯åœ¨æ§åˆ¶æ ‡è®°å’Œå›¾åƒæ ‡è®°ä¹‹é—´çš„æ¯ä¸ªæ ‡è®°èåˆä¸Šç”Ÿæˆçš„ï¼Œç±»ä¼¼äºä½ç½®ç¼–ç ã€‚ä¸é¢„å…ˆå¡«å……æ ‡è®°ç›¸æ¯”ï¼Œä½¿ç”¨æ¡ä»¶è§£ç æ˜¾è‘—å¢å¼ºäº†ARæ¨¡å‹çš„æ§åˆ¶èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæ‰€æå‡ºçš„ControlARä»¤äººæƒŠè®¶çš„æ˜¯é€šè¿‡æ¡ä»¶è§£ç å’Œç‰¹å®šæ§åˆ¶ä½¿ARæ¨¡å‹å…·å¤‡ä»»æ„åˆ†è¾¨ç‡çš„å›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚å¤§é‡å®éªŒå¯ä»¥è¯æ˜æ‰€æå‡ºControlARåœ¨è¾¹ç¼˜ã€æ·±åº¦å’Œå„ç§åˆ†å‰²æ©æ¨¡ç­‰å¤šæ ·åŒ–è¾“å…¥ä¸‹çš„è‡ªå›å½’æ§åˆ¶åˆ°å›¾åƒç”Ÿæˆçš„å¯æ§æ€§ã€‚æ­¤å¤–ï¼Œå®šé‡å’Œå®šæ€§ç»“æœå‡è¡¨æ˜ControlARè¶…è¶Šäº†ä¹‹å‰çš„å…ˆè¿›å¯æ§æ‰©æ•£æ¨¡å‹ï¼Œå¦‚ControlNet++ã€‚ä»£ç ã€æ¨¡å‹å’Œæ¼”ç¤ºå°†å¾ˆå¿«åœ¨<a target="_blank" rel="noopener" href="https://github.com/hustvl/ControlAR%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/hustvl/ControlARä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.02705v3">PDF</a> To appear in ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>ARæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ï¼Œæˆä¸ºæ‰©æ•£æ¨¡å‹çš„å¼ºåŠ²ç«äº‰å¯¹æ‰‹ã€‚ç„¶è€Œï¼Œå¯¹äºControlARæ¨¡å‹æ¥è¯´ï¼Œæ§åˆ¶åˆ°å›¾åƒç”Ÿæˆçš„è½¬åŒ–ä»æ˜¯æœªçŸ¥é¢†åŸŸã€‚æœ¬æ–‡æå‡ºControlARæ¡†æ¶ï¼Œé€šè¿‡æ¢ç´¢æ§åˆ¶ç¼–ç å’Œæ¡ä»¶è§£ç æ–¹æ³•ï¼Œå°†ç©ºé—´æ§åˆ¶é›†æˆåˆ°è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­ã€‚ControlARèƒ½å°†ç©ºé—´è¾“å…¥è½¬åŒ–ä¸ºæ§åˆ¶ä»¤ç‰Œï¼Œå¹¶åˆ©ç”¨æ¡ä»¶è§£ç ç”Ÿæˆä¸‹ä¸€ä¸ªå›¾åƒä»¤ç‰Œï¼Œå¢å¼ºäº†ARæ¨¡å‹çš„æ§åˆ¶èƒ½åŠ›å¹¶ä¿æŒæ¨¡å‹æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒControlARè¿˜èƒ½æ”¯æŒä»»æ„åˆ†è¾¨ç‡çš„å›¾åƒç”Ÿæˆã€‚å®éªŒè¡¨æ˜ï¼ŒControlARåœ¨è¾¹ç¼˜ã€æ·±åº¦ã€åˆ†å‰²æ©è†œç­‰å¤šç§è¾“å…¥ä¸Šå…·æœ‰å¯æ§æ€§ï¼Œè¶…è¶Šç°æœ‰çš„å¯æ§æ‰©æ•£æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ARæ¨¡å‹åœ¨å›¾åƒç”Ÿæˆé¢†åŸŸå±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>æ§åˆ¶åˆ°å›¾åƒç”Ÿæˆåœ¨ARæ¨¡å‹ä¸­ä»å±æœªçŸ¥é¢†åŸŸã€‚</li>
<li>ControlARæ¡†æ¶é€šè¿‡æ§åˆ¶ç¼–ç å’Œæ¡ä»¶è§£ç æ–¹æ³•é›†æˆç©ºé—´æ§åˆ¶åˆ°è‡ªå›å½’å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­ã€‚</li>
<li>ControlARèƒ½å°†ç©ºé—´è¾“å…¥è½¬åŒ–ä¸ºæ§åˆ¶ä»¤ç‰Œã€‚</li>
<li>æ¡ä»¶è§£ç å¢å¼ºäº†ARæ¨¡å‹çš„æ§åˆ¶èƒ½åŠ›å¹¶ä¿æŒæ¨¡å‹æ•ˆç‡ã€‚</li>
<li>ControlARæ”¯æŒä»»æ„åˆ†è¾¨ç‡çš„å›¾åƒç”Ÿæˆã€‚</li>
<li>ControlARåœ¨å¤šç§è¾“å…¥ä¸Šå±•ç°å‡ºè¶…è¶Šç°æœ‰å¯æ§æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.02705">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0645fcaaa2d78c179ffcd10be38e3c39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c24a0350df6268c568f274d897f3d661.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d78eeee4e3ee910408334ded8365d9c.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Timestep-Aware-Diffusion-Model-for-Extreme-Image-Rescaling"><a href="#Timestep-Aware-Diffusion-Model-for-Extreme-Image-Rescaling" class="headerlink" title="Timestep-Aware Diffusion Model for Extreme Image Rescaling"></a>Timestep-Aware Diffusion Model for Extreme Image Rescaling</h2><p><strong>Authors:Ce Wang, Zhenyu Hu, Wanjie Sun, Zhenzhong Chen</strong></p>
<p>Image rescaling aims to learn the optimal low-resolution (LR) image that can be accurately reconstructed to its original high-resolution (HR) counterpart, providing an efficient image processing and storage method for ultra-high definition media. However, extreme downscaling factors pose significant challenges to the upscaling process due to its highly ill-posed nature, causing existing image rescaling methods to struggle in generating semantically correct structures and perceptual friendly textures. In this work, we propose a novel framework called Timestep-Aware Diffusion Model (TADM) for extreme image rescaling, which performs rescaling operations in the latent space of a pre-trained autoencoder and effectively leverages powerful natural image priors learned by a pre-trained text-to-image diffusion model. Specifically, TADM adopts a pseudo-invertible module to establish the bidirectional mapping between the latent features of the HR image and the target-sized LR image. Then, the rescaled latent features are enhanced by a pre-trained diffusion model to generate more faithful details. Considering the spatially non-uniform degradation caused by the rescaling operation, we propose a novel time-step alignment strategy, which can adaptively allocate the generative capacity of the diffusion model based on the quality of the reconstructed latent features. Extensive experiments demonstrate the superiority of TADM over previous methods in both quantitative and qualitative evaluations. </p>
<blockquote>
<p>å›¾åƒç¼©æ”¾æ—¨åœ¨å­¦ä¹ æœ€ä½³çš„ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰å›¾åƒï¼Œè¯¥å›¾åƒå¯ä»¥å‡†ç¡®åœ°é‡æ„ä¸ºå…¶åŸå§‹çš„é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰å¯¹åº”ç‰©ï¼Œä¸ºè¶…é«˜æ¸…åª’ä½“æä¾›ä¸€ç§é«˜æ•ˆçš„å›¾åƒå¤„ç†å’Œå­˜å‚¨æ–¹æ³•ã€‚ç„¶è€Œï¼Œæç«¯çš„ç¼©å°å› å­ç»™æ”¾å¤§è¿‡ç¨‹å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œå› ä¸ºå…¶é«˜åº¦ä¸é€‚å®šçš„æ€§è´¨ï¼Œå¯¼è‡´ç°æœ‰çš„å›¾åƒç¼©æ”¾æ–¹æ³•åœ¨ç”Ÿæˆè¯­ä¹‰ä¸Šæ­£ç¡®çš„ç»“æ„å’Œæ„ŸçŸ¥å‹å¥½çš„çº¹ç†æ—¶é‡åˆ°å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºæç«¯å›¾åƒç¼©æ”¾çš„åä¸ºâ€œæ—¶é—´æ„ŸçŸ¥æ‰©æ•£æ¨¡å‹â€ï¼ˆTADMï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨é¢„è®­ç»ƒè‡ªç¼–ç å™¨çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œç¼©æ”¾æ“ä½œï¼Œå¹¶æœ‰æ•ˆåˆ©ç”¨ç”±é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å­¦ä¹ çš„å¼ºå¤§è‡ªç„¶å›¾åƒå…ˆéªŒã€‚å…·ä½“æ¥è¯´ï¼ŒTADMé‡‡ç”¨ä¼ªå¯é€†æ¨¡å—å»ºç«‹é«˜åˆ†è¾¨ç‡å›¾åƒæ½œåœ¨ç‰¹å¾ä¸ç›®æ ‡å¤§å°ä½åˆ†è¾¨ç‡å›¾åƒä¹‹é—´çš„åŒå‘æ˜ å°„ã€‚ç„¶åï¼Œé€šè¿‡é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹å¢å¼ºé‡æ–°ç¼©æ”¾çš„æ½œåœ¨ç‰¹å¾ï¼Œä»¥ç”Ÿæˆæ›´å¿ å®çš„ç»†èŠ‚ã€‚è€ƒè™‘åˆ°ç¼©æ”¾æ“ä½œå¼•èµ·çš„ç©ºé—´éå‡åŒ€é€€åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ—¶é—´æ­¥é•¿å¯¹é½ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥æ ¹æ®é‡æ„çš„æ½œåœ¨ç‰¹å¾çš„è´¨é‡è‡ªé€‚åº”åœ°åˆ†é…æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTADMåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°æ–¹é¢éƒ½ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09151v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬å·¥ä½œæå‡ºä¸€ç§åä¸ºTimestep-Aware Diffusion Modelï¼ˆTADMï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºæç«¯å›¾åƒç¼©æ”¾ã€‚è¯¥æ–¹æ³•åœ¨é¢„è®­ç»ƒè‡ªç¼–ç å™¨çš„æ½œåœ¨ç©ºé—´æ‰§è¡Œç¼©æ”¾æ“ä½œï¼Œå¹¶æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å­¦ä¹ çš„è‡ªç„¶å›¾åƒå…ˆéªŒã€‚TADMé‡‡ç”¨ä¼ªå¯é€†æ¨¡å—å»ºç«‹é«˜åˆ†è¾¨ç‡å›¾åƒæ½œåœ¨ç‰¹å¾ä¸ç›®æ ‡å¤§å°ä½åˆ†è¾¨ç‡å›¾åƒä¹‹é—´çš„åŒå‘æ˜ å°„ï¼Œç„¶åé€šè¿‡æ‰©æ•£æ¨¡å‹å¢å¼ºç¼©æ”¾åçš„æ½œåœ¨ç‰¹å¾ä»¥ç”Ÿæˆæ›´çœŸå®çš„ç»†èŠ‚ã€‚è€ƒè™‘åˆ°ç¼©æ”¾æ“ä½œå¼•èµ·çš„ç©ºé—´éå‡åŒ€é€€åŒ–ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ—¶é—´æ­¥é•¿å¯¹é½ç­–ç•¥ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°æ ¹æ®é‡å»ºçš„æ½œåœ¨ç‰¹å¾è´¨é‡åˆ†é…æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TADMæ¡†æ¶è¢«æå‡ºç”¨äºæç«¯å›¾åƒç¼©æ”¾ï¼Œåœ¨æ½œåœ¨ç©ºé—´æ‰§è¡Œç¼©æ”¾æ“ä½œï¼Œåˆ©ç”¨è‡ªç„¶å›¾åƒå…ˆéªŒã€‚</li>
<li>ä¼ªå¯é€†æ¨¡å—å»ºç«‹é«˜åˆ†è¾¨ç‡å’Œä½åˆ†è¾¨ç‡å›¾åƒä¹‹é—´çš„åŒå‘æ˜ å°„ã€‚</li>
<li>ç¼©æ”¾åçš„æ½œåœ¨ç‰¹å¾é€šè¿‡é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹è¿›è¡Œå¢å¼ºï¼Œä»¥ç”Ÿæˆæ›´çœŸå®çš„ç»†èŠ‚ã€‚</li>
<li>æå‡ºæ—¶é—´æ­¥é•¿å¯¹é½ç­–ç•¥ï¼Œè‡ªé€‚åº”åˆ†é…æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>TADMåœ¨å®šé‡å’Œå®šæ€§è¯„ä¼°ä¸Šéƒ½ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚</li>
<li>æ¡†æ¶èƒ½å¤Ÿå¤„ç†ç”±ç¼©æ”¾æ“ä½œå¼•èµ·çš„ç©ºé—´éå‡åŒ€é€€åŒ–é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.09151">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-81de9271c9cd201a38a6cf3ea5ba6e14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a95efbee27275fdacc45a9f9836b4365.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6359605d3127debe22f56b71e3dfe86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f604ab5b83ce26f0be3d75ad24dd438.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-514fa26ba4dfbc1dfe610e0149be9eb6.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="DiffSG-A-Generative-Solver-for-Network-Optimization-with-Diffusion-Model"><a href="#DiffSG-A-Generative-Solver-for-Network-Optimization-with-Diffusion-Model" class="headerlink" title="DiffSG: A Generative Solver for Network Optimization with Diffusion   Model"></a>DiffSG: A Generative Solver for Network Optimization with Diffusion   Model</h2><p><strong>Authors:Ruihuai Liang, Bo Yang, Zhiwen Yu, Bin Guo, Xuelin Cao, MÃ©rouane Debbah, H. Vincent Poor, Chau Yuen</strong></p>
<p>Generative diffusion models, famous for their performance in image generation, are popular in various cross-domain applications. However, their use in the communication community has been mostly limited to auxiliary tasks like data modeling and feature extraction. These models hold greater promise for fundamental problems in network optimization compared to traditional machine learning methods. Discriminative deep learning often falls short due to its single-step input-output mapping and lack of global awareness of the solution space, especially given the complexity of network optimizationâ€™s objective functions. In contrast, generative diffusion models can consider a broader range of solutions and exhibit stronger generalization by learning parameters that describe the distribution of the underlying solution space, with higher probabilities assigned to better solutions. We propose a new framework Diffusion Model-based Solution Generation (DiffSG), which leverages the intrinsic distribution learning capabilities of generative diffusion models to learn high-quality solution distributions based on given inputs. The optimal solution within this distribution is highly probable, allowing it to be effectively reached through repeated sampling. We validate the performance of DiffSG on several typical network optimization problems, including mixed-integer non-linear programming, convex optimization, and hierarchical non-convex optimization. Our results demonstrate that DiffSG outperforms existing baseline methods not only on in-domain inputs but also on out-of-domain inputs. In summary, we demonstrate the potential of generative diffusion models in tackling complex network optimization problems and outline a promising path for their broader application in the communication community. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/qiyu3816/DiffSG">https://github.com/qiyu3816/DiffSG</a>. </p>
<blockquote>
<p>ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹ä»¥å…¶å›¾åƒç”Ÿæˆæ€§èƒ½è€Œé—»åï¼Œå¹¶å¹¿æ³›åº”ç”¨äºå„ç§è·¨åŸŸåº”ç”¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨é€šä¿¡é¢†åŸŸçš„ä½¿ç”¨å¤§å¤šä»…é™äºæ•°æ®å»ºæ¨¡å’Œç‰¹å¾æå–ç­‰è¾…åŠ©ä»»åŠ¡ã€‚ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¿™äº›æ¨¡å‹åœ¨ç½‘ç»œä¼˜åŒ–ç­‰åŸºç¡€é—®é¢˜ä¸Šæ›´å…·æ½œåŠ›ã€‚åˆ¤åˆ«å¼æ·±åº¦å­¦ä¹ é€šå¸¸ç”±äºå•æ­¥è¾“å…¥è¾“å‡ºæ˜ å°„å’Œç¼ºä¹å¯¹æ•´ä¸ªè§£ç©ºé—´çš„å…¨å±€è®¤è¯†è€Œåœ¨é¢å¯¹å¤æ‚çš„ç½‘ç»œä¼˜åŒ–ç›®æ ‡å‡½æ•°æ—¶è¡¨ç°ä¸è¶³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹å¯ä»¥è€ƒè™‘æ›´å¹¿æ³›çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶é€šè¿‡å­¦ä¹ æè¿°åº•å±‚è§£ç©ºé—´åˆ†å¸ƒçš„å‚æ•°æ¥è¡¨ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºæ›´å¥½çš„è§£å†³æ–¹æ¡ˆåˆ†é…æ›´é«˜çš„æ¦‚ç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆç”Ÿæˆæ¡†æ¶ï¼ˆDiffSGï¼‰ï¼Œå®ƒåˆ©ç”¨ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹çš„å†…åœ¨åˆ†å¸ƒå­¦ä¹ èƒ½åŠ›ï¼ŒåŸºäºç»™å®šè¾“å…¥å­¦ä¹ é«˜è´¨é‡è§£å†³æ–¹æ¡ˆåˆ†å¸ƒã€‚è¿™ä¸ªåˆ†å¸ƒä¸­çš„æœ€ä¼˜è§£å¾ˆå¯èƒ½æ˜¯é«˜åº¦å¯èƒ½çš„ï¼Œå¯ä»¥é€šè¿‡é‡å¤é‡‡æ ·æœ‰æ•ˆåœ°è¾¾åˆ°ã€‚æˆ‘ä»¬åœ¨å…¸å‹çš„ç½‘ç»œä¼˜åŒ–é—®é¢˜ä¸ŠéªŒè¯äº†DiffSGçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬æ··åˆæ•´æ•°éçº¿æ€§è§„åˆ’ã€å‡¸ä¼˜åŒ–å’Œåˆ†å±‚éå‡¸ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒDiffSGä¸ä»…åœ¨é¢†åŸŸå†…éƒ¨è¾“å…¥ä¸Šè€Œä¸”åœ¨é¢†åŸŸå¤–éƒ¨è¾“å…¥ä¸Šéƒ½ä¼˜äºç°æœ‰åŸºå‡†æ–¹æ³•ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç”Ÿæˆæ€§æ‰©æ•£æ¨¡å‹åœ¨è§£å†³å¤æ‚çš„ç½‘ç»œä¼˜åŒ–é—®é¢˜ä¸Šçš„æ½œåŠ›ï¼Œå¹¶ä¸ºå…¶åœ¨é€šä¿¡é¢†åŸŸçš„æ›´å¹¿æ³›åº”ç”¨æç»˜äº†ä¸€æ¡æœ‰å‰é€”çš„é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/qiyu3%E3%80%82%E3%80%82%E3%80%82DiffSG%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/qiyu3 3816/DiffSGä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.06701v2">PDF</a> Accepted by IEEE Communications Magazine</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹åœ¨è§£å†³ç½‘ç»œä¼˜åŒ–é—®é¢˜ä¸­çš„æ½œåŠ›ã€‚ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿè€ƒè™‘æ›´å¹¿æ³›çš„è§£å†³æ–¹æ¡ˆå¹¶å…·æœ‰æ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”åŸºäºæ‰©æ•£æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆç”Ÿæˆï¼ˆDiffSGï¼‰ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹çš„å†…åœ¨åˆ†å¸ƒå­¦ä¹ èƒ½åŠ›ï¼ŒåŸºäºç»™å®šè¾“å…¥å­¦ä¹ é«˜è´¨é‡è§£å†³æ–¹æ¡ˆåˆ†å¸ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffSGåœ¨å¤šç§ç½‘ç»œä¼˜åŒ–é—®é¢˜ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œä¸ä»…åœ¨é¢†åŸŸå†…è¾“å…¥ä¸Šè¡¨ç°è‰¯å¥½ï¼Œåœ¨è·¨é¢†åŸŸè¾“å…¥ä¸Šä¹Ÿæœ‰å‡ºè‰²è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹åœ¨å¤šç§è·¨åŸŸåº”ç”¨ä¸­çš„æ™®åŠï¼Œç‰¹åˆ«æ˜¯åœ¨ç½‘ç»œä¼˜åŒ–é—®é¢˜ä¸Šçš„æ½œåŠ›ã€‚</li>
<li>ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨å¤„ç†ç½‘ç»œä¼˜åŒ–é—®é¢˜æ—¶ï¼Œå¸¸å¸¸å› å•ä¸€æ˜ å°„å’Œç¼ºä¹å…¨å±€è§†é‡è€Œæ˜¾å¾—ä¸è¶³ã€‚</li>
<li>ç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•ï¼Œç”Ÿæˆå¼æ‰©æ•£æ¨¡å‹èƒ½è€ƒè™‘æ›´å¹¿æ³›çš„è§£å†³æ–¹æ¡ˆå¹¶è¡¨ç°å‡ºæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”åŸºäºæ‰©æ•£æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆç”Ÿæˆï¼ˆDiffSGï¼‰ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒå­¦ä¹ èƒ½åŠ›æ¥è§£å†³ç½‘ç»œä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>DiffSGæ¡†æ¶èƒ½å¤Ÿåœ¨ç»™å®šè¾“å…¥åŸºç¡€ä¸Šå­¦ä¹ é«˜è´¨é‡è§£å†³æ–¹æ¡ˆåˆ†å¸ƒï¼Œå¹¶é€šè¿‡é‡å¤é‡‡æ ·æ‰¾åˆ°æœ€ä¼˜è§£ã€‚</li>
<li>DiffSGåœ¨å¤šç§ç½‘ç»œä¼˜åŒ–é—®é¢˜ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰åŸºå‡†æ–¹æ³•ï¼ŒåŒ…æ‹¬æ··åˆæ•´æ•°éçº¿æ€§è§„åˆ’ã€å‡¸ä¼˜åŒ–å’Œåˆ†å±‚éå‡¸ä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.06701">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0a00093f4f428c97d8ccfcd618d9e3be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d078a5755133e6b962bde3bda52ecfae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f0940fb557393a5a1cecb94423c4cdce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75156f26a327f2e12d297861d1f28bea.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-853b5edd0ba7d7e6710b25641dc4e07d.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  Enhanced Pediatric Dental Segmentation Using a Custom SegUNet with VGG19   Backbone on Panoramic Radiographs
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-2b501678aa4879d1456324d975f55982.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  Feature-EndoGaussian Feature Distilled Gaussian Splatting in Surgical   Deformable Scene Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">12990.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
