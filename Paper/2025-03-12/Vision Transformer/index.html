<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  Anatomy-Aware Conditional Image-Text Retrieval">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-196f03cf3132c16b893ba204157ee34a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    58 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-12-æ›´æ–°"><a href="#2025-03-12-æ›´æ–°" class="headerlink" title="2025-03-12 æ›´æ–°"></a>2025-03-12 æ›´æ–°</h1><h2 id="Anatomy-Aware-Conditional-Image-Text-Retrieval"><a href="#Anatomy-Aware-Conditional-Image-Text-Retrieval" class="headerlink" title="Anatomy-Aware Conditional Image-Text Retrieval"></a>Anatomy-Aware Conditional Image-Text Retrieval</h2><p><strong>Authors:Meng Zheng, Jiajin Zhang, Benjamin Planche, Zhongpai Gao, Terrence Chen, Ziyan Wu</strong></p>
<p>Image-Text Retrieval (ITR) finds broad applications in healthcare, aiding clinicians and radiologists by automatically retrieving relevant patient cases in the database given the query image and&#x2F;or report, for more efficient clinical diagnosis and treatment, especially for rare diseases. However conventional ITR systems typically only rely on global image or text representations for measuring patient image&#x2F;report similarities, which overlook local distinctiveness across patient cases. This often results in suboptimal retrieval performance. In this paper, we propose an Anatomical Location-Conditioned Image-Text Retrieval (ALC-ITR) framework, which, given a query image and the associated suspicious anatomical region(s), aims to retrieve similar patient cases exhibiting the same disease or symptoms in the same anatomical region. To perform location-conditioned multimodal retrieval, we learn a medical Relevance-Region-Aligned Vision Language (RRA-VL) model with semantic global-level and region-&#x2F;word-level alignment to produce generalizable, well-aligned multi-modal representations. Additionally, we perform location-conditioned contrastive learning to further utilize cross-pair region-level contrastiveness for improved multi-modal retrieval. We show that our proposed RRA-VL achieves state-of-the-art localization performance in phase-grounding tasks, and satisfying multi-modal retrieval performance with or without location conditioning. Finally, we thoroughly investigate the generalizability and explainability of our proposed ALC-ITR system in providing explanations and preliminary diagnosis reports given retrieved patient cases (conditioned on anatomical regions), with proper off-the-shelf LLM prompts. </p>
<blockquote>
<p>å›¾åƒæ–‡æœ¬æ£€ç´¢ï¼ˆITRï¼‰åœ¨åŒ»ç–—é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ï¼Œå®ƒé€šè¿‡è‡ªåŠ¨æ£€ç´¢æ•°æ®åº“ä¸­çš„ç›¸å…³ç—…ä¾‹ï¼Œå¸®åŠ©ä¸´åºŠåŒ»ç”Ÿå’Œæ”¾å°„ç§‘åŒ»ç”Ÿæ›´é«˜æ•ˆåœ°åšå‡ºè¯Šæ–­å’Œæ²»ç–—ï¼Œå°¤å…¶æ˜¯é’ˆå¯¹ç½•è§ç–¾ç—…ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„ITRç³»ç»Ÿé€šå¸¸åªä¾èµ–äºå…¨å±€å›¾åƒæˆ–æ–‡æœ¬è¡¨ç¤ºæ¥åº¦é‡æ‚£è€…å›¾åƒ&#x2F;æŠ¥å‘Šçš„ç›¸ä¼¼æ€§ï¼Œå¿½ç•¥äº†ä¸åŒæ‚£è€…ç—…ä¾‹ä¹‹é—´çš„å±€éƒ¨å·®å¼‚ã€‚è¿™å¾€å¾€å¯¼è‡´æ£€ç´¢æ€§èƒ½ä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè§£å‰–éƒ¨ä½æ¡ä»¶çš„å›¾åƒæ–‡æœ¬æ£€ç´¢ï¼ˆALC-ITRï¼‰æ¡†æ¶ã€‚ç»™å®šæŸ¥è¯¢å›¾åƒå’Œç›¸å…³çš„å¯ç–‘è§£å‰–åŒºåŸŸï¼Œè¯¥æ¡†æ¶æ—¨åœ¨æ£€ç´¢åœ¨åŒä¸€è§£å‰–åŒºåŸŸè¡¨ç°å‡ºç›¸åŒç–¾ç—…æˆ–ç—‡çŠ¶çš„ç›¸ä¼¼æ‚£è€…ç—…ä¾‹ã€‚ä¸ºäº†æ‰§è¡ŒåŸºäºä½ç½®çš„å¤šåª’ä½“æ£€ç´¢ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ä¸€ç§åŒ»å­¦ç›¸å…³åŒºåŸŸå¯¹é½çš„è§†è§‰è¯­è¨€ï¼ˆRRA-VLï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…·æœ‰è¯­ä¹‰å…¨å±€å’ŒåŒºåŸŸ&#x2F;å•è¯çº§åˆ«çš„å¯¹é½åŠŸèƒ½ï¼Œä»¥äº§ç”Ÿå¯æ¨å¹¿çš„ã€å¯¹é½è‰¯å¥½çš„å¤šæ¨¡æ€è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ‰§è¡ŒåŸºäºä½ç½®çš„å¯¹æ¯”å­¦ä¹ ï¼Œä»¥è¿›ä¸€æ­¥åˆ©ç”¨è·¨å¯¹åŒºåŸŸçº§åˆ«çš„å¯¹æ¯”æ€§ï¼Œæ”¹è¿›å¤šæ¨¡æ€æ£€ç´¢ã€‚æˆ‘ä»¬æ˜¾ç¤ºï¼Œæˆ‘ä»¬æå‡ºçš„RRA-VLåœ¨ç›¸ä½å®šä½ä»»åŠ¡ä¸­å®ç°äº†ä¸€æµçš„å®šä½æ€§èƒ½ï¼Œå¹¶åœ¨æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶ä¸‹å‡å–å¾—äº†ä»¤äººæ»¡æ„çš„å¤šæ¨¡æ€æ£€ç´¢æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬å…¨é¢è°ƒæŸ¥äº†æ‰€æå‡ºçš„ALC-ITRç³»ç»Ÿåœ¨æä¾›è§£é‡Šå’Œåˆæ­¥è¯Šæ–­æŠ¥å‘Šæ–¹é¢çš„é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§ï¼Œæ ¹æ®æ£€ç´¢åˆ°çš„ç—…ä¾‹ï¼ˆä»¥è§£å‰–éƒ¨ä½ä¸ºæ¡ä»¶ï¼‰è¿›è¡Œé€‚å½“çš„ç°æˆå¤§å‹è¯­è¨€æ¨¡å‹æç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07456v1">PDF</a> 16 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºè§£å‰–å­¦å®šä½çš„å›¾æ–‡æ£€ç´¢ï¼ˆALC-ITRï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åº”ç”¨äºåŒ»ç–—é¢†åŸŸï¼Œæ—¨åœ¨æé«˜å›¾åƒå’ŒæŠ¥å‘Šçš„æ£€ç´¢æ€§èƒ½ã€‚è¯¥æ¡†æ¶é€šè¿‡è€ƒè™‘æŸ¥è¯¢å›¾åƒå’Œç›¸å…³çš„å¯ç–‘è§£å‰–å­¦åŒºåŸŸï¼Œæ—¨åœ¨æ£€ç´¢åœ¨åŒä¸€è§£å‰–å­¦åŒºåŸŸä¸­è¡¨ç°å‡ºç›¸åŒç–¾ç—…æˆ–ç—‡çŠ¶çš„ç›¸ä¼¼æ‚£è€…ç—…ä¾‹ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åŒ»å­¦ç›¸å…³åŒºåŸŸå¯¹é½çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆRRA-VLï¼‰ï¼Œå®ç°äº†å…¨å±€å’ŒåŒºåŸŸ&#x2F;è¯çº§åˆ«çš„è¯­ä¹‰å¯¹é½ï¼Œå¹¶åˆ©ç”¨ä½ç½®æ¡ä»¶å¯¹æ¯”å­¦ä¹ è¿›ä¸€æ­¥æé«˜è·¨æ¨¡æ€æ£€ç´¢æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒRRA-VLæ¨¡å‹åœ¨å®šä½ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶ä¸‹çš„å¤šæ¨¡æ€æ£€ç´¢ä¸­å‡è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜èƒ½é€šè¿‡æ£€ç´¢åˆ°çš„ç—…ä¾‹ï¼ˆä»¥è§£å‰–å­¦åŒºåŸŸä¸ºæ¡ä»¶ï¼‰æä¾›è§£é‡Šå’Œåˆæ­¥è¯Šæ–­æŠ¥å‘Šï¼Œå…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§å’Œå¯è§£é‡Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒæ–‡æœ¬æ£€ç´¢ï¼ˆITRï¼‰åœ¨åŒ»ç–—é¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨ä»·å€¼ï¼Œå¦‚è¾…åŠ©åŒ»ç”Ÿè¯Šæ–­ç½•è§ç–¾ç—…ã€‚</li>
<li>ä¼ ç»ŸITRç³»ç»Ÿä¾èµ–å…¨å±€å›¾åƒæˆ–æ–‡æœ¬è¡¨ç¤ºæ¥æµ‹é‡ç›¸ä¼¼æ€§ï¼Œå¿½ç•¥äº†ç—…ä¾‹ä¹‹é—´çš„å±€éƒ¨å·®å¼‚ï¼Œå¯¼è‡´æ£€ç´¢æ€§èƒ½ä¸ä½³ã€‚</li>
<li>æå‡ºçš„ALC-ITRæ¡†æ¶æ—¨åœ¨é€šè¿‡è€ƒè™‘æŸ¥è¯¢å›¾åƒå’Œç›¸å…³çš„è§£å‰–å­¦åŒºåŸŸæ¥æé«˜æ£€ç´¢æ€§èƒ½ã€‚</li>
<li>RRA-VLæ¨¡å‹å®ç°äº†å…¨å±€å’ŒåŒºåŸŸ&#x2F;è¯çº§åˆ«çš„è¯­ä¹‰å¯¹é½ï¼Œæé«˜äº†å¤šæ¨¡æ€æ£€ç´¢çš„é€šç”¨æ€§å’Œå¯¹é½æ€§ã€‚</li>
<li>ä½ç½®æ¡ä»¶å¯¹æ¯”å­¦ä¹ ç”¨äºæé«˜è·¨æ¨¡æ€æ£€ç´¢æ€§èƒ½ã€‚</li>
<li>RRA-VLæ¨¡å‹åœ¨å®šä½ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b6558f3d475177cd8186fbf1b69f26ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07756af073315ec238b374d0e0deb7bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f62aec3b8aeadc943eea0bab397be4ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc638a60be8cfb07cf16ec9901ea4657.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Distilling-Knowledge-into-Quantum-Vision-Transformers-for-Biomedical-Image-Classification"><a href="#Distilling-Knowledge-into-Quantum-Vision-Transformers-for-Biomedical-Image-Classification" class="headerlink" title="Distilling Knowledge into Quantum Vision Transformers for Biomedical   Image Classification"></a>Distilling Knowledge into Quantum Vision Transformers for Biomedical   Image Classification</h2><p><strong>Authors:Thomas Boucher, Evangelos B. Mazomenos</strong></p>
<p>Quantum vision transformers (QViTs) build on vision transformers (ViTs) by replacing linear layers within the self-attention mechanism with parameterised quantum neural networks (QNNs), harnessing quantum mechanical properties to improve feature representation. This hybrid approach aims to achieve superior performance, with significantly reduced model complexity as a result of the enriched feature representation, requiring fewer parameters. This paper proposes a novel QViT model for biomedical image classification and investigates its performance against comparable ViTs across eight diverse datasets, encompassing various modalities and classification tasks. We assess models trained from scratch and those pre-trained using knowledge distillation (KD) from high-quality teacher models. Our findings demonstrate that QViTs outperform comparable ViTs with average ROC AUC (0.863 vs 0.846) and accuracy (0.710 vs 0.687) when trained from scratch, and even compete with state-of-the-art classical models in multiple tasks, whilst being significantly more efficient (89% reduction in GFLOPs and 99.99% in parameter number). Additionally, we find that QViTs and ViTs respond equally well to KD, with QViT pre-training performance scaling with model complexity. This is the first investigation into the efficacy of deploying QViTs with KD for computer-aided diagnosis. Our results highlight the enormous potential of quantum machine learning (QML) in biomedical image analysis. </p>
<blockquote>
<p>é‡å­è§†è§‰è½¬æ¢å™¨ï¼ˆQViTsï¼‰ä»¥è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ä¸ºåŸºç¡€ï¼Œå°†è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„çº¿æ€§å±‚æ›¿æ¢ä¸ºå‚æ•°åŒ–é‡å­ç¥ç»ç½‘ç»œï¼ˆQNNsï¼‰ï¼Œåˆ©ç”¨é‡å­æœºæ¢°å±æ€§æ”¹è¿›ç‰¹å¾è¡¨ç¤ºã€‚è¿™ç§æ··åˆæ–¹æ³•æ—¨åœ¨å®ç°å“è¶Šçš„æ€§èƒ½ï¼Œç”±äºç‰¹å¾è¡¨ç¤ºçš„ä¸°å¯Œæ€§ï¼Œæ¨¡å‹å¤æ‚åº¦æ˜¾è‘—é™ä½ï¼Œå‡å°‘äº†æ‰€éœ€çš„å‚æ•°æ•°é‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„QViTæ¨¡å‹ï¼Œç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ï¼Œå¹¶åœ¨å…«ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šä¸ç±»ä¼¼çš„ViTsæ¯”è¾ƒå…¶æ€§èƒ½ï¼Œè¿™äº›æ•°æ®é›†æ¶µç›–å¤šç§æ¨¡æ€å’Œåˆ†ç±»ä»»åŠ¡ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä»å¤´å¼€å§‹è®­ç»ƒä»¥åŠä½¿ç”¨æ¥è‡ªé«˜è´¨é‡æ•™å¸ˆæ¨¡å‹çš„è’¸é¦çŸ¥è¯†ï¼ˆKDï¼‰è¿›è¡Œé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»å¤´å¼€å§‹è®­ç»ƒçš„QViTsåœ¨å¹³å‡ROC AUCï¼ˆ0.863 vs 0.846ï¼‰å’Œå‡†ç¡®åº¦ï¼ˆ0.710 vs 0.687ï¼‰ä¸Šä¼˜äºç±»ä¼¼çš„ViTsï¼Œå¹¶ä¸”åœ¨å¤šä¸ªä»»åŠ¡ä¸­ç”šè‡³ä¸æœ€å…ˆè¿›çš„ç»å…¸æ¨¡å‹ç«äº‰ï¼ŒåŒæ—¶æ•ˆç‡æ›´é«˜ï¼ˆGFLOPså‡å°‘89%ï¼Œå‚æ•°æ•°é‡å‡å°‘99.99%ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°QViTså’ŒViTså¯¹KDçš„å“åº”åŒæ ·è‰¯å¥½ï¼ŒQViTçš„é¢„è®­ç»ƒæ€§èƒ½éšæ¨¡å‹å¤æ‚åº¦çš„å¢åŠ è€Œæå‡ã€‚è¿™æ˜¯é¦–æ¬¡è°ƒæŸ¥ä½¿ç”¨KDéƒ¨ç½²QViTåœ¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç»“æœçªæ˜¾äº†é‡å­æœºå™¨å­¦ä¹ ï¼ˆQMLï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07294v1">PDF</a> Submitted for MICCAI 2025</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºäº†é‡å­è§†è§‰å˜å‹å™¨ï¼ˆQViTï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºè§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ï¼Œé€šè¿‡å°†è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„çº¿æ€§å±‚æ›¿æ¢ä¸ºå‚æ•°åŒ–é‡å­ç¥ç»ç½‘ç»œï¼ˆQNNï¼‰ï¼Œåˆ©ç”¨é‡å­æœºæ¢°ç‰¹æ€§æ”¹è¿›ç‰¹å¾è¡¨ç¤ºã€‚è¿™ä¸€æ··åˆæ–¹æ³•æ—¨åœ¨å®ç°å“è¶Šçš„æ€§èƒ½ï¼Œç”±äºç‰¹å¾è¡¨ç¤ºçš„ä¸°å¯Œæ€§ï¼Œæ¨¡å‹å¤æ‚åº¦æ˜¾è‘—é™ä½ï¼Œå‚æ•°æ›´å°‘ã€‚è¯¥ç ”ç©¶åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»é¢†åŸŸæå‡ºäº†æ–°çš„QViTæ¨¡å‹ï¼Œå¹¶è¯„ä¼°å…¶ä¸åŒç±»ViTåœ¨å…«ä¸ªä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚ç»“æœæ˜¾ç¤ºï¼Œä»å¤´å¼€å§‹è®­ç»ƒçš„QViTç›¸è¾ƒäºViTåœ¨ROC AUCï¼ˆ0.863 vs 0.846ï¼‰å’Œå‡†ç¡®åº¦ï¼ˆ0.710 vs 0.687ï¼‰ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸­ä¸æœ€å…ˆè¿›çš„ç»å…¸æ¨¡å‹ç«äº‰ï¼ŒåŒæ—¶æ•ˆç‡æ›´é«˜ï¼ˆGFLOPså‡å°‘89%ï¼Œå‚æ•°æ•°é‡å‡å°‘99.99%ï¼‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°QViTå’ŒViTå¯¹è’¸é¦ï¼ˆKDï¼‰çš„å“åº”åŒæ ·è‰¯å¥½ï¼ŒQViTçš„é¢„è®­ç»ƒæ€§èƒ½éšæ¨¡å‹å¤æ‚åº¦è€Œæå‡ã€‚è¿™æ˜¯é¦–æ¬¡æ¢ç´¢å°†QViTä¸KDç»“åˆç”¨äºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ç»“æœçªæ˜¾äº†é‡å­æœºå™¨å­¦ä¹ ï¼ˆQMLï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é‡å­è§†è§‰å˜å‹å™¨ï¼ˆQViTï¼‰ç»“åˆäº†é‡å­ç¥ç»ç½‘ç»œå’Œè§†è§‰å˜å‹å™¨çš„ä¼˜ç‚¹ï¼Œåˆ©ç”¨é‡å­æœºæ¢°ç‰¹æ€§æå‡ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>QViTæ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç›¸è¾ƒäºä¼ ç»ŸViTæ¨¡å‹ï¼Œå…¶ROC AUCå’Œå‡†ç¡®åº¦å‡æœ‰æå‡ã€‚</li>
<li>QViTæ¨¡å‹æ•ˆç‡æ›´é«˜ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—é‡å’Œå‚æ•°æ•°é‡ã€‚</li>
<li>QViTå’ŒViTå¯¹è’¸é¦ï¼ˆKDï¼‰ç­–ç•¥ååº”è‰¯å¥½ï¼Œé¢„è®­ç»ƒæ€§èƒ½éšæ¨¡å‹å¤æ‚åº¦æå‡ã€‚</li>
<li>QViTä¸KDç»“åˆçš„ç­–ç•¥åœ¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡å°†QViTä¸KDç»“åˆåº”ç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æçš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07294">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3f533fcc5222f0ec941b934645917734.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a331eea6ad88bf14f279ea6dab714103.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b93ef2392de589f334d3b432b3523c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6ced9f6a61aa35810369b7069c7f4651.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PathVQ-Reforming-Computational-Pathology-Foundation-Model-for-Whole-Slide-Image-Analysis-via-Vector-Quantization"><a href="#PathVQ-Reforming-Computational-Pathology-Foundation-Model-for-Whole-Slide-Image-Analysis-via-Vector-Quantization" class="headerlink" title="PathVQ: Reforming Computational Pathology Foundation Model for Whole   Slide Image Analysis via Vector Quantization"></a>PathVQ: Reforming Computational Pathology Foundation Model for Whole   Slide Image Analysis via Vector Quantization</h2><p><strong>Authors:Honglin Li, Zhongyi Shui, Yunlong Zhang, Chenglu Zhu, Lin Yang</strong></p>
<p>Computational pathology and whole-slide image (WSI) analysis are pivotal in cancer diagnosis and prognosis. However, the ultra-high resolution of WSIs presents significant modeling challenges. Recent advancements in pathology foundation models have improved performance, yet most approaches rely on [CLS] token representation of tile ViT as slide-level inputs (16x16 pixels is refereed as patch and 224x224 pixels as tile). This discards critical spatial details from patch tokens, limiting downstream WSI analysis tasks. We find that leveraging all spatial patch tokens benefits WSI analysis but incurs nearly 200x higher storage and training costs (e.g., 196 tokens in ViT$_{224}$). To address this, we introduce vector quantized (VQ) distillation on patch feature, which efficiently compresses spatial patch tokens using discrete indices and a decoder. Our method reduces token dimensionality from 1024 to 16, achieving a 64x compression rate while preserving reconstruction fidelity. Furthermore, we employ a multi-scale VQ (MSVQ) strategy, which not only enhances VQ reconstruction performance but also serves as a Self-supervised Learning (SSL) supervision for a seamless slide-level pretraining objective. Built upon the quantized patch features and supervision targets of tile via MSVQ, we develop a progressive convolutional module and slide-level SSL to extract representations with rich spatial-information for downstream WSI tasks. Extensive evaluations on multiple datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance in WSI analysis. Code will be available soon. </p>
<blockquote>
<p>è®¡ç®—ç—…ç†å­¦å’Œå…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†æåœ¨ç™Œç—‡è¯Šæ–­å’Œé¢„åä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼ŒWSIçš„è¶…é«˜åˆ†è¾¨ç‡å¸¦æ¥äº†å·¨å¤§çš„å»ºæ¨¡æŒ‘æˆ˜ã€‚æœ€è¿‘çš„ç—…ç†åŸºç¡€æ¨¡å‹è¿›å±•æé«˜äº†æ€§èƒ½ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•éƒ½ä¾èµ–äºViTçš„[CLS]æ ‡è®°ä½œä¸ºå¹»ç¯ç‰‡çº§åˆ«çš„è¾“å…¥ï¼ˆå°†16x16åƒç´ ç§°ä¸ºè¡¥ä¸ï¼Œè€Œ224x224åƒç´ ç§°ä¸ºç“·ç –ï¼‰ã€‚è¿™ç§æ–¹æ³•èˆå¼ƒäº†è¡¥ä¸æ ‡è®°ä¸­çš„å…³é”®ç©ºé—´ç»†èŠ‚ï¼Œé™åˆ¶äº†ä¸‹æ¸¸çš„WSIåˆ†æä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°åˆ©ç”¨æ‰€æœ‰ç©ºé—´è¡¥ä¸æ ‡è®°å¯¹WSIåˆ†ææ˜¯æœ‰ç›Šçš„ï¼Œä½†è¿™ä¼šå¯¼è‡´å­˜å‚¨å’Œè®­ç»ƒæˆæœ¬å¢åŠ è¿‘200å€ï¼ˆä¾‹å¦‚ï¼ŒViT224ä¸­çš„196ä¸ªæ ‡è®°ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨è¡¥ä¸ç‰¹å¾ä¸Šå¼•å…¥äº†çŸ¢é‡é‡åŒ–ï¼ˆVQï¼‰è’¸é¦æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ä½¿ç”¨ç¦»æ•£ç´¢å¼•å’Œè§£ç å™¨æœ‰æ•ˆåœ°å‹ç¼©ç©ºé—´è¡¥ä¸æ ‡è®°ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æ ‡è®°ç»´åº¦ä»1024å‡å°‘åˆ°16ï¼Œå®ç°äº†64å€çš„å‹ç¼©ç‡ï¼ŒåŒæ—¶ä¿æŒäº†é‡å»ºä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¤šå°ºåº¦VQï¼ˆMSVQï¼‰ç­–ç•¥ï¼Œè¿™ä¸ä»…æé«˜äº†VQé‡å»ºæ€§èƒ½ï¼Œè¿˜ä½œä¸ºæ— ç¼å¹»ç¯ç‰‡çº§é¢„è®­ç»ƒç›®æ ‡çš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ç›‘ç£ã€‚åŸºäºé‡åŒ–çš„è¡¥ä¸ç‰¹å¾å’Œé€šè¿‡MSVQè·å¾—çš„ç“·ç –ç›‘ç£ç›®æ ‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ¸è¿›çš„å·ç§¯æ¨¡å—å’Œå¹»ç¯ç‰‡çº§åˆ«çš„SSLï¼Œä»¥æå–å…·æœ‰ä¸°å¯Œç©ºé—´ä¿¡æ¯çš„è¡¨ç¤ºç”¨äºä¸‹æ¸¸WSIä»»åŠ¡ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œåœ¨å…¨åˆ‡ç‰‡å›¾åƒåˆ†æé¢†åŸŸå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å³å°†å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06482v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è®¡ç®—ç—…ç†å­¦å’Œå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†æåœ¨ç™Œç—‡è¯Šæ–­å’Œæ²»ç–—ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼ŒWSIçš„è¶…é«˜åˆ†è¾¨ç‡å¸¦æ¥äº†å»ºæ¨¡æŒ‘æˆ˜ã€‚è™½ç„¶ç—…ç†å­¦åŸºç¡€æ¨¡å‹çš„æœ€æ–°è¿›å±•æé«˜äº†æ€§èƒ½ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•ä¾èµ–äºCLSæ ‡è®°çš„ç“·ç –ViTä½œä¸ºå¹»ç¯ç‰‡çº§åˆ«çš„è¾“å…¥ï¼Œè¿™ä¼šä¸¢å¼ƒå…³é”®çš„ç©ºtokenæ ‡è®°çš„ç©ºé—´ç»†èŠ‚ä¿¡æ¯é™åˆ¶äº†ä¸‹æ¸¸WSIåˆ†æä»»åŠ¡ã€‚å‘ç°åˆ©ç”¨æ‰€æœ‰ç©ºé—´è¡¥ä¸tokenæœ‰åˆ©äºWSIåˆ†æï¼Œä½†åŒæ—¶ä¹Ÿå¯¼è‡´äº†è¿‘200å€çš„å­˜å‚¨å’Œè®­ç»ƒæˆæœ¬æé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥çŸ¢é‡é‡åŒ–ç‰¹å¾ä¸Šçš„VQè’¸é¦æ³•ï¼Œæœ‰æ•ˆåœ°å‹ç¼©ç©ºé—´è¡¥ä¸tokenå¹¶ä½¿ç”¨ç¦»æ•£ç´¢å¼•å’Œè§£ç å™¨è¿›è¡Œé«˜æ•ˆå¤„ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†tokenç»´åº¦ä»1024å‡å°‘åˆ°16ï¼Œå®ç°äº†é«˜è¾¾64å€çš„å‹ç¼©ç‡ï¼ŒåŒæ—¶ä¿æŒäº†é‡å»ºä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†å¤šå°ºåº¦VQç­–ç•¥ï¼Œä¸ä»…æé«˜äº†VQé‡å»ºæ€§èƒ½ï¼Œè¿˜ä½œä¸ºæ— ç¼å¹»ç¯ç‰‡çº§åˆ«çš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ç›‘ç£æ–¹å¼ã€‚åŸºäºé‡åŒ–çš„è¡¥ä¸ç‰¹å¾å’Œé€šè¿‡MSVQå¾—åˆ°çš„ç“·ç –ç›‘ç£ç›®æ ‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ¸è¿›å¼å·ç§¯æ¨¡å—å’Œå¹»ç¯ç‰‡çº§åˆ«çš„SSLï¼Œä¸ºä¸‹æ¸¸WSIä»»åŠ¡æå–å…·æœ‰ä¸°å¯Œç©ºé—´ä¿¡æ¯çš„è¡¨ç¤ºå½¢å¼ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†æœ€å…ˆè¿›çš„æ•ˆæœã€‚</p>
<p><strong>è¦ç‚¹è§£æ</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡ä¸­è¦ç‚¹ä¿¡æ¯ï¼Œç”¨ç®€åŒ–æ±‰å­—ä¸­æ–‡è¿›è¡Œå½’çº³æ€»ç»“ï¼š</p>
<ul>
<li>è®¡ç®—ç—…ç†å­¦å’Œå…¨å¹»ç¯ç‰‡å›¾åƒåˆ†æåœ¨ç™Œç—‡è¯Šæ–­ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>WSIçš„è¶…é«˜åˆ†è¾¨ç‡ç»™å»ºæ¨¡å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–CLSæ ‡è®°çš„ç“·ç –ViTä½œä¸ºå¹»ç¯ç‰‡çº§åˆ«çš„è¾“å…¥ï¼Œå¯¼è‡´ç©ºé—´ç»†èŠ‚ä¿¡æ¯çš„ä¸¢å¤±ã€‚</li>
<li>åˆ©ç”¨æ‰€æœ‰ç©ºé—´è¡¥ä¸tokenå¯¹WSIåˆ†ææœ‰ç›Šï¼Œä½†å¸¦æ¥æ›´é«˜çš„å­˜å‚¨å’Œè®­ç»ƒæˆæœ¬ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06482">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7532184b623ca7f93ae673fe3e6c469d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56829c1e36c9bc956a8342a6f76a11e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0ec9a893000a2ca22c638cbace53abb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e94115e3d8140c95f62a6db88223ece.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Spectral-State-Space-Model-for-Rotation-InvariantVisualRepresentation-Learning"><a href="#Spectral-State-Space-Model-for-Rotation-InvariantVisualRepresentation-Learning" class="headerlink" title="Spectral State Space Model for   Rotation-InvariantVisualRepresentation~Learning"></a>Spectral State Space Model for   Rotation-Invariant<del>Visual</del>Representation~Learning</h2><p><strong>Authors:Sahar Dastani, Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, David Osowiechi, Gustavo Adolfo Vargas Hakim, Farzad Beizaee, Milad Cheraghalikhani, Arnab Kumar Mondal, Herve Lombaert, Christian Desrosiers</strong></p>
<p>State Space Models (SSMs) have recently emerged as an alternative to Vision Transformers (ViTs) due to their unique ability of modeling global relationships with linear complexity. SSMs are specifically designed to capture spatially proximate relationships of image patches. However, they fail to identify relationships between conceptually related yet not adjacent patches. This limitation arises from the non-causal nature of image data, which lacks inherent directional relationships. Additionally, current vision-based SSMs are highly sensitive to transformations such as rotation. Their predefined scanning directions depend on the original image orientation, which can cause the model to produce inconsistent patch-processing sequences after rotation. To address these limitations, we introduce Spectral VMamba, a novel approach that effectively captures the global structure within an image by leveraging spectral information derived from the graph Laplacian of image patches. Through spectral decomposition, our approach encodes patch relationships independently of image orientation, achieving rotation invariance with the aid of our Rotational Feature Normalizer (RFN) module. Our experiments on classification tasks show that Spectral VMamba outperforms the leading SSM models in vision, such as VMamba, while maintaining invariance to rotations and a providing a similar runtime efficiency. </p>
<blockquote>
<p>çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰å› å…¶ä»¥çº¿æ€§å¤æ‚åº¦å¯¹å…¨å±€å…³ç³»è¿›è¡Œå»ºæ¨¡çš„ç‹¬ç‰¹èƒ½åŠ›ï¼Œæœ€è¿‘ä½œä¸ºè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰çš„æ›¿ä»£å“è€Œå‡ºç°ã€‚SSMsä¸“é—¨è®¾è®¡ç”¨äºæ•æ‰å›¾åƒè¡¥ä¸çš„ç©ºé—´é‚»è¿‘å…³ç³»ã€‚ç„¶è€Œï¼Œå®ƒä»¬æ— æ³•è¯†åˆ«æ¦‚å¿µä¸Šç›¸å…³ä½†å¹¶éç›¸é‚»çš„è¡¥ä¸ä¹‹é—´çš„å…³ç³»ã€‚è¿™ä¸€å±€é™æ€§æºäºå›¾åƒæ•°æ®çš„éå› æœæ€§è´¨ï¼Œå³ç¼ºä¹å›ºæœ‰çš„æ–¹å‘å…³ç³»ã€‚æ­¤å¤–ï¼Œå½“å‰çš„åŸºäºè§†è§‰çš„SSMså¯¹æ—‹è½¬ç­‰è½¬æ¢éå¸¸æ•æ„Ÿã€‚å…¶é¢„å®šä¹‰çš„æ‰«ææ–¹å‘å–å†³äºåŸå§‹å›¾åƒçš„æ–¹å‘ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨æ—‹è½¬åäº§ç”Ÿä¸ä¸€è‡´çš„è¡¥ä¸å¤„ç†åºåˆ—ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†Spectral VMambaè¿™ä¸€æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ä»å›¾åƒè¡¥ä¸çš„å›¾æ‹‰æ™®æ‹‰æ–¯ç®—å­å¾—å‡ºçš„å…‰è°±ä¿¡æ¯ï¼Œæœ‰æ•ˆåœ°æ•æ‰å›¾åƒå†…çš„å…¨å±€ç»“æ„ã€‚é€šè¿‡è°±åˆ†è§£ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç‹¬ç«‹äºå›¾åƒæ–¹å‘åœ°ç¼–ç è¡¥ä¸å…³ç³»ï¼Œå€ŸåŠ©æˆ‘ä»¬çš„æ—‹è½¬ç‰¹å¾å½’ä¸€åŒ–å™¨ï¼ˆRFNï¼‰æ¨¡å—å®ç°æ—‹è½¬ä¸å˜æ€§ã€‚æˆ‘ä»¬åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSpectral VMambaåœ¨è§†è§‰é¢†åŸŸè¶…è¶Šäº†é¢†å…ˆçš„SSMsæ¨¡å‹ï¼ˆå¦‚VMambaï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†å¯¹æ—‹è½¬çš„ä¸å˜æ€§ï¼Œå¹¶æä¾›äº†ç±»ä¼¼çš„è¿è¡Œæ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06369v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼ŒçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰ä½œä¸ºVision Transformersï¼ˆViTsï¼‰çš„æ›¿ä»£æ–¹æ¡ˆå´­éœ²å¤´è§’ï¼Œå…¶ä»¥çº¿æ€§å¤æ‚åº¦å»ºæ¨¡å…¨å±€å…³ç³»çš„èƒ½åŠ›å—åˆ°å…³æ³¨ã€‚ç„¶è€Œï¼Œå®ƒä»¬éš¾ä»¥è¯†åˆ«æ¦‚å¿µç›¸å…³ä½†ä¸ç›¸é‚»çš„è¡¥ä¸é—´çš„å…³ç³»ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Spectral VMambaæ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å›¾åƒè¡¥ä¸çš„å›¾æ‹‰æ™®æ‹‰æ–¯ç®—å­çš„è°±ä¿¡æ¯æ¥æ•è·å›¾åƒå…¨å±€ç»“æ„ï¼Œå¹¶å€ŸåŠ©æ—‹è½¬ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆRFNï¼‰æ¨¡å—å®ç°æ—‹è½¬ä¸å˜æ€§ã€‚å®éªŒè¯æ˜ï¼ŒSpectral VMambaåœ¨åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºé¢†å…ˆçš„SSMæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒå¯¹æ—‹è½¬çš„ä¸å˜æ€§å¹¶ç»´æŒç›¸ä¼¼çš„è¿è¡Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰èƒ½å¤Ÿæ•æ‰å›¾åƒè¡¥ä¸çš„ç©ºé—´é‚»è¿‘å…³ç³»ã€‚</li>
<li>SSMséš¾ä»¥è¯†åˆ«æ¦‚å¿µç›¸å…³ä½†ä¸ç›¸é‚»çš„è¡¥ä¸ä¹‹é—´çš„å…³ç³»ï¼Œè¿™æºäºå›¾åƒæ•°æ®çš„éå› æœæ€§è´¨ã€‚</li>
<li>å½“å‰åŸºäºè§†è§‰çš„SSMå¯¹æ—‹è½¬ç­‰å˜æ¢é«˜åº¦æ•æ„Ÿï¼Œä¾èµ–äºåŸå§‹å›¾åƒæ–¹å‘çš„é¢„å®šæ‰«ææ–¹å‘ä¼šå¯¼è‡´æ¨¡å‹åœ¨æ—‹è½¬åäº§ç”Ÿä¸ä¸€è‡´çš„è¡¥ä¸å¤„ç†åºåˆ—ã€‚</li>
<li>Spectral VMambaæ–¹æ³•é€šè¿‡åˆ©ç”¨å›¾åƒè¡¥ä¸çš„å›¾æ‹‰æ™®æ‹‰æ–¯ç®—å­çš„è°±ä¿¡æ¯æ¥æ•è·å›¾åƒçš„å…¨å±€ç»“æ„ã€‚</li>
<li>Spectral VMambaå®ç°äº†æ—‹è½¬ä¸å˜æ€§ï¼Œå€ŸåŠ©æ—‹è½¬ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆRFNï¼‰æ¨¡å—ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒSpectral VMambaåœ¨åˆ†ç±»ä»»åŠ¡ä¸Šä¼˜äºé¢†å…ˆçš„SSMæ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-122cf626621e1a1af5ded391f80bc4c7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-dd58c3ec27207d5df2084d263bdba660.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7c88466524e44b07c776d5b1e91f2249.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="VORTEX-Challenging-CNNs-at-Texture-Recognition-by-using-Vision-Transformers-with-Orderless-and-Randomized-Token-Encodings"><a href="#VORTEX-Challenging-CNNs-at-Texture-Recognition-by-using-Vision-Transformers-with-Orderless-and-Randomized-Token-Encodings" class="headerlink" title="VORTEX: Challenging CNNs at Texture Recognition by using Vision   Transformers with Orderless and Randomized Token Encodings"></a>VORTEX: Challenging CNNs at Texture Recognition by using Vision   Transformers with Orderless and Randomized Token Encodings</h2><p><strong>Authors:Leonardo Scabini, Kallil M. Zielinski, Emir Konuk, Ricardo T. Fares, Lucas C. Ribas, Kevin Smith, Odemir M. Bruno</strong></p>
<p>Texture recognition has recently been dominated by ImageNet-pre-trained deep Convolutional Neural Networks (CNNs), with specialized modifications and feature engineering required to achieve state-of-the-art (SOTA) performance. However, although Vision Transformers (ViTs) were introduced a few years ago, little is known about their texture recognition ability. Therefore, in this work, we introduce VORTEX (ViTs with Orderless and Randomized Token Encodings for Texture Recognition), a novel method that enables the effective use of ViTs for texture analysis. VORTEX extracts multi-depth token embeddings from pre-trained ViT backbones and employs a lightweight module to aggregate hierarchical features and perform orderless encoding, obtaining a better image representation for texture recognition tasks. This approach allows seamless integration with any ViT with the common transformer architecture. Moreover, no fine-tuning of the backbone is performed, since they are used only as frozen feature extractors, and the features are fed to a linear SVM. We evaluate VORTEX on nine diverse texture datasets, demonstrating its ability to achieve or surpass SOTA performance in a variety of texture analysis scenarios. By bridging the gap between texture recognition with CNNs and transformer-based architectures, VORTEX paves the way for adopting emerging transformer foundation models. Furthermore, VORTEX demonstrates robust computational efficiency when coupled with ViT backbones compared to CNNs with similar costs. The method implementation and experimental scripts are publicly available in our online repository. </p>
<blockquote>
<p>çº¹ç†è¯†åˆ«æœ€è¿‘ä¸»è¦ä¾èµ–äºImageNeté¢„è®­ç»ƒçš„æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œä¸ºäº†å®ç°æœ€å‰æ²¿ï¼ˆSOTAï¼‰æ€§èƒ½ï¼Œéœ€è¦ä¸“ä¸šåŒ–çš„ä¿®æ”¹å’Œç‰¹å¾å·¥ç¨‹ã€‚ç„¶è€Œï¼Œå°½ç®¡è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰å‡ å¹´å‰å°±å·²ç»è¢«å¼•å…¥ï¼Œä½†å…³äºå…¶çº¹ç†è¯†åˆ«èƒ½åŠ›çš„ç ”ç©¶çŸ¥ä¹‹ç”šå°‘ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†VORTEXï¼ˆç”¨äºçº¹ç†è¯†åˆ«çš„å…·æœ‰æ— åºå’Œéšæœºä»¤ç‰Œç¼–ç çš„è§†è§‰å˜å‹å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä½¿è§†è§‰å˜å‹å™¨åœ¨çº¹ç†åˆ†ææ–¹é¢è¿›è¡Œæœ‰æ•ˆä½¿ç”¨çš„æ–°æ–¹æ³•ã€‚VORTEXä»é¢„è®­ç»ƒçš„ViTä¸»å¹²ä¸­æå–å¤šæ·±åº¦ä»¤ç‰ŒåµŒå…¥ï¼Œå¹¶é‡‡ç”¨è½»é‡çº§æ¨¡å—æ¥èšåˆåˆ†å±‚ç‰¹å¾å¹¶æ‰§è¡Œæ— åºç¼–ç ï¼Œä¸ºçº¹ç†è¯†åˆ«ä»»åŠ¡è·å¾—æ›´å¥½çš„å›¾åƒè¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ— ç¼åœ°é›†æˆåˆ°å…·æœ‰é€šç”¨å˜å‹å™¨æ¶æ„çš„ä»»ä½•ViTä¸­ã€‚æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬åªæ˜¯ä½¿ç”¨å†»ç»“çš„ç‰¹å¾æå–å™¨ï¼Œè€Œæ²¡æœ‰å¯¹ä¸»å¹²è¿›è¡Œå¾®è°ƒï¼Œå› æ­¤ç‰¹å¾è¢«è¾“å…¥åˆ°çº¿æ€§SVMä¸­ã€‚æˆ‘ä»¬åœ¨ä¹ä¸ªä¸åŒçš„çº¹ç†æ•°æ®é›†ä¸Šè¯„ä¼°äº†VORTEXï¼Œè¯æ˜äº†å®ƒåœ¨å„ç§çº¹ç†åˆ†æåœºæ™¯ä¸­è¾¾åˆ°æˆ–è¶…è¶ŠSOTAæ€§èƒ½çš„èƒ½åŠ›ã€‚é€šè¿‡ç¼©å°CNNå’ŒåŸºäºå˜å‹å™¨çš„æ¶æ„ä¹‹é—´çš„çº¹ç†è¯†åˆ«å·®è·ï¼ŒVORTEXä¸ºé‡‡ç”¨æ–°å…´çš„å˜å‹å™¨åŸºç¡€æ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚æ­¤å¤–ï¼Œä¸å…·æœ‰ç±»ä¼¼æˆæœ¬çš„CNNç›¸æ¯”ï¼ŒVORTEXä¸ViTä¸»å¹²ç»“åˆæ—¶æ˜¾ç¤ºå‡ºç¨³å¥çš„è®¡ç®—æ•ˆç‡ã€‚æ–¹æ³•å’Œå®éªŒè„šæœ¬å·²åœ¨æˆ‘ä»¬åœ¨çº¿ä»“åº“ä¸­å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06368v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†VORTEXï¼ˆç”¨äºçº¹ç†è¯†åˆ«çš„å¸¦æœ‰æ— åºå’Œéšæœºä»¤ç‰Œç¼–ç çš„æ„¿æ™¯å˜å‹å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä½¿æ„¿æ™¯å˜å‹å™¨æœ‰æ•ˆç”¨äºçº¹ç†åˆ†æçš„æ–°æ–¹æ³•ã€‚VORTEXä»é¢„è®­ç»ƒçš„ViTä¸»å¹²ä¸­æå–å¤šæ·±åº¦ä»¤ç‰ŒåµŒå…¥ï¼Œå¹¶é‡‡ç”¨è½»é‡çº§æ¨¡å—è¿›è¡Œåˆ†å±‚ç‰¹å¾çš„èšåˆå’Œæ— åºç¼–ç ï¼Œä¸ºçº¹ç†è¯†åˆ«ä»»åŠ¡è·å¾—æ›´å¥½çš„å›¾åƒè¡¨ç¤ºã€‚è¯¥æ–¹æ³•å¯æ— ç¼é›†æˆåˆ°å…·æœ‰é€šç”¨è½¬æ¢å™¨æ¶æ„çš„ä»»ä½•ViTä¸­ï¼Œæ— éœ€å¯¹ä¸»å¹²è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬åœ¨ä¹ä¸ªä¸åŒçš„çº¹ç†æ•°æ®é›†ä¸Šè¯„ä¼°äº†VORTEXï¼Œè¯æ˜äº†å…¶åœ¨å„ç§çº¹ç†åˆ†æåœºæ™¯ä¸­çš„èƒ½åŠ›ã€‚VORTEXå¡«è¡¥äº†CNNå’ŒåŸºäºè½¬æ¢å™¨çš„æ¶æ„åœ¨çº¹ç†è¯†åˆ«æ–¹é¢çš„ç©ºç™½ï¼Œä¸ºé‡‡ç”¨æ–°å…´çš„è½¬æ¢å™¨åŸºç¡€æ¨¡å‹é“ºå¹³äº†é“è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VORTEXæ˜¯ä¸€ç§åˆ©ç”¨Vision Transformerï¼ˆViTï¼‰è¿›è¡Œçº¹ç†è¯†åˆ«çš„æ–°æ–¹æ³•ã€‚</li>
<li>VORTEXé€šè¿‡æå–å¤šæ·±åº¦ä»¤ç‰ŒåµŒå…¥å’Œé‡‡ç”¨è½»é‡çº§æ¨¡å—è¿›è¡Œåˆ†å±‚ç‰¹å¾èšåˆä¸æ— åºç¼–ç ï¼Œå®ç°æœ‰æ•ˆçš„çº¹ç†åˆ†æã€‚</li>
<li>VORTEXå¯æ— ç¼é›†æˆåˆ°ä»»ä½•å…·æœ‰é€šç”¨è½¬æ¢å™¨æ¶æ„çš„ViTä¸­ï¼Œä¸”æ— éœ€å¯¹ä¸»å¹²è¿›è¡Œå¾®è°ƒã€‚</li>
<li>åœ¨ä¹ä¸ªä¸åŒçš„çº¹ç†æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒVORTEXåœ¨å¤šç§çº¹ç†åˆ†æåœºæ™¯ä¸­è¾¾åˆ°æˆ–è¶…è¶Šäº†æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
<li>VORTEXå¡«è¡¥äº†CNNå’ŒåŸºäºè½¬æ¢å™¨çš„æ¶æ„åœ¨çº¹ç†è¯†åˆ«æ–¹é¢çš„ç©ºç™½ã€‚</li>
<li>VORTEXç›¸è¾ƒäºå…·æœ‰ç±»ä¼¼æˆæœ¬çš„CNNï¼Œè¡¨ç°å‡ºæ›´å¼ºçš„è®¡ç®—æ•ˆç‡ã€‚</li>
<li>VORTEXçš„æ–¹æ³•å®ç°å’Œå®éªŒè„šæœ¬å·²å…¬å¼€åœ¨çº¿ä»“åº“ä¸­å¯ä¾›ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b21eb0ba161fe0e1280719c1c8a41e61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d2d4408ccc6df3f7e5ed14c638b66f34.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ffcdbde19a126fd0261f2e2779340198.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Treble-Counterfactual-VLMs-A-Causal-Approach-to-Hallucination"><a href="#Treble-Counterfactual-VLMs-A-Causal-Approach-to-Hallucination" class="headerlink" title="Treble Counterfactual VLMs: A Causal Approach to Hallucination"></a>Treble Counterfactual VLMs: A Causal Approach to Hallucination</h2><p><strong>Authors:Li Li, Jiashu Qu, Yuxiao Zhou, Yuehan Qin, Tiankai Yang, Yue Zhao</strong></p>
<p>Vision-Language Models (VLMs) have advanced multi-modal tasks like image captioning, visual question answering, and reasoning. However, they often generate hallucinated outputs inconsistent with the visual context or prompt, limiting reliability in critical applications like autonomous driving and medical imaging. Existing studies link hallucination to statistical biases, language priors, and biased feature learning but lack a structured causal understanding. In this work, we introduce a causal perspective to analyze and mitigate hallucination in VLMs. We hypothesize that hallucination arises from unintended direct influences of either the vision or text modality, bypassing proper multi-modal fusion. To address this, we construct a causal graph for VLMs and employ counterfactual analysis to estimate the Natural Direct Effect (NDE) of vision, text, and their cross-modal interaction on the output. We systematically identify and mitigate these unintended direct effects to ensure that responses are primarily driven by genuine multi-modal fusion. Our approach consists of three steps: (1) designing structural causal graphs to distinguish correct fusion pathways from spurious modality shortcuts, (2) estimating modality-specific and cross-modal NDE using perturbed image representations, hallucinated text embeddings, and degraded visual inputs, and (3) implementing a test-time intervention module to dynamically adjust the modelâ€™s dependence on each modality. Experimental results demonstrate that our method significantly reduces hallucination while preserving task performance, providing a robust and interpretable framework for improving VLM reliability. To enhance accessibility and reproducibility, our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/TREE985/Treble-Counterfactual-VLMs">https://github.com/TREE985/Treble-Counterfactual-VLMs</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œæ¨ç†ç­‰è·¨æ¨¡å¼ä»»åŠ¡ä¸­å–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç»å¸¸äº§ç”Ÿä¸è§†è§‰ä¸Šä¸‹æ–‡æˆ–æç¤ºä¸ä¸€è‡´çš„å¹»è§‰è¾“å‡ºï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨è‡ªåŠ¨é©¾é©¶å’ŒåŒ»å­¦å½±åƒç­‰å…³é”®åº”ç”¨ä¸­çš„å¯é æ€§ã€‚ç°æœ‰ç ”ç©¶å°†å¹»è§‰ä¸ç»Ÿè®¡åè§ã€è¯­è¨€å…ˆéªŒçŸ¥è¯†å’Œæœ‰åç‰¹å¾å­¦ä¹ è”ç³»èµ·æ¥ï¼Œä½†ç¼ºä¹ç»“æ„æ€§çš„å› æœç†è§£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»å› æœè§’åº¦æ¥åˆ†æå¹¶å‡è½»VLMsä¸­çš„å¹»è§‰é—®é¢˜ã€‚æˆ‘ä»¬å‡è®¾å¹»è§‰æ˜¯ç”±äºè§†è§‰æˆ–æ–‡æœ¬æ¨¡æ€çš„æ„å¤–ç›´æ¥å½±å“è€Œäº§ç”Ÿçš„ï¼Œè¿™äº›å½±å“ç»•è¿‡äº†é€‚å½“çš„è·¨æ¨¡å¼èåˆã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä¸ºVLMsæ„å»ºäº†å› æœå›¾ï¼Œå¹¶é‡‡ç”¨åäº‹å®åˆ†ææ¥ä¼°è®¡è§†è§‰ã€æ–‡æœ¬åŠå…¶è·¨æ¨¡å¼äº¤äº’å¯¹è¾“å‡ºçš„è‡ªç„¶ç›´æ¥å½±å“ï¼ˆNDEï¼‰ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°è¯†åˆ«å’Œç¼“è§£è¿™äº›æ„å¤–çš„ç›´æ¥å½±å“ï¼Œä»¥ç¡®ä¿å“åº”ä¸»è¦ç”±çœŸæ­£çš„è·¨æ¨¡å¼èåˆé©±åŠ¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼šï¼ˆ1ï¼‰è®¾è®¡ç»“æ„å› æœå›¾ï¼Œä»¥åŒºåˆ†æ­£ç¡®çš„èåˆé€”å¾„å’Œè™šå‡çš„æ¨¡å¼æ·å¾„ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨æ‰°åŠ¨å›¾åƒè¡¨ç¤ºã€å¹»è§‰æ–‡æœ¬åµŒå…¥å’Œé€€åŒ–è§†è§‰è¾“å…¥æ¥ä¼°è®¡æ¨¡æ€ç‰¹å®šå’Œè·¨æ¨¡æ€çš„NDEï¼›ï¼ˆ3ï¼‰å®ç°æµ‹è¯•æ—¶é—´å¹²é¢„æ¨¡å—ï¼Œä»¥åŠ¨æ€è°ƒæ•´æ¨¡å‹å¯¹æ¯ä¸ªæ¨¡æ€çš„ä¾èµ–æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å‡å°‘å¹»è§‰çš„åŒæ—¶ä¿æŒäº†ä»»åŠ¡æ€§èƒ½ï¼Œä¸ºæé«˜VLMå¯é æ€§æä¾›äº†ç¨³å¥å’Œå¯è§£é‡Šæ€§çš„æ¡†æ¶ã€‚ä¸ºäº†å¢å¼ºå¯è®¿é—®æ€§å’Œå¯é‡å¤æ€§ï¼Œæˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/TREE985/Treble-Counterfactual-VLMs%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/TREE985/Treble-Counterfactual-VLMsä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06169v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä¸­çš„å¹»è§†é—®é¢˜ï¼Œè¯¥é—®é¢˜åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­æ™®éå­˜åœ¨ã€‚æ–‡ç« ä»å› æœè§’åº¦åˆ†æäº†å¹»è§†çš„äº§ç”ŸåŸå› ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„è§£å†³ç­–ç•¥ã€‚é€šè¿‡æ„å»ºå› æœå›¾å’Œä½¿ç”¨åäº‹å®åˆ†æï¼Œæ–‡ç« ç³»ç»Ÿåœ°è¯†åˆ«å¹¶ç¼“è§£äº†å¹»è§†é—®é¢˜ï¼Œæé«˜äº†VLMsçš„å¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­æ™®éå­˜åœ¨å¹»è§†é—®é¢˜ï¼Œå½±å“æ¨¡å‹å¯é æ€§ã€‚</li>
<li>å¹»è§†é—®é¢˜å¯èƒ½ä¸ç»Ÿè®¡åè§ã€è¯­è¨€å…ˆéªŒå’Œç‰¹å¾å­¦ä¹ ä¸­çš„åå·®æœ‰å…³ã€‚</li>
<li>æœ¬æ–‡ä»å› æœè§’åº¦åˆ†æäº†å¹»è§†çš„äº§ç”ŸåŸå› ï¼Œæå‡ºå¹»è§†æ˜¯ç”±äºè§†è§‰æˆ–æ–‡æœ¬æ¨¡æ€çš„ç›´æ¥å½±å“ç»•è¿‡æ­£ç¡®çš„å¤šæ¨¡æ€èåˆæ‰€è‡´ã€‚</li>
<li>é€šè¿‡æ„å»ºå› æœå›¾å’Œä½¿ç”¨åäº‹å®åˆ†æï¼Œæ–‡ç« ç³»ç»Ÿåœ°è¯†åˆ«äº†ç¼“è§£å¹»è§†é—®é¢˜çš„æ–¹æ³•ã€‚</li>
<li>æ–¹æ³•åŒ…æ‹¬è®¾è®¡ç»“æ„å› æœå›¾ã€ä¼°è®¡æ¨¡æ€ç‰¹å®šå’Œè·¨æ¨¡æ€è‡ªç„¶ç›´æ¥æ•ˆåº”ä»¥åŠå®æ–½æµ‹è¯•æ—¶é—´å¹²é¢„æ¨¡å—ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å‡å°‘å¹»è§†çš„åŒæ—¶ä¿ç•™äº†ä»»åŠ¡æ€§èƒ½ï¼Œä¸ºæ”¹è¿›VLMå¯é æ€§æä¾›äº†ç¨³å¥å’Œå¯è§£é‡Šæ€§çš„æ¡†æ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06169">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f618f78950345007f3fa8f94a1ebf896.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbae3b301237c9e0114fe125a26fae54.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e65941cea72f7b971a030c4255ffc7ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a976974b649322a0f7a475381457b5f0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Towards-Universal-Text-driven-CT-Image-Segmentation"><a href="#Towards-Universal-Text-driven-CT-Image-Segmentation" class="headerlink" title="Towards Universal Text-driven CT Image Segmentation"></a>Towards Universal Text-driven CT Image Segmentation</h2><p><strong>Authors:Yuheng Li, Yuxiang Lai, Maria Thor, Deborah Marshall, Zachary Buchwald, David S. Yu, Xiaofeng Yang</strong></p>
<p>Computed tomography (CT) is extensively used for accurate visualization and segmentation of organs and lesions. While deep learning models such as convolutional neural networks (CNNs) and vision transformers (ViTs) have significantly improved CT image analysis, their performance often declines when applied to diverse, real-world clinical data. Although foundation models offer a broader and more adaptable solution, their potential is limited due to the challenge of obtaining large-scale, voxel-level annotations for medical images. In response to these challenges, prompting-based models using visual or text prompts have emerged. Visual-prompting methods, such as the Segment Anything Model (SAM), still require significant manual input and can introduce ambiguity when applied to clinical scenarios. Instead, foundation models that use text prompts offer a more versatile and clinically relevant approach. Notably, current text-prompt models, such as the CLIP-Driven Universal Model, are limited to text prompts already encountered during training and struggle to process the complex and diverse scenarios of real-world clinical applications. Instead of fine-tuning models trained from natural imaging, we propose OpenVocabCT, a vision-language model pretrained on large-scale 3D CT images for universal text-driven segmentation. Using the large-scale CT-RATE dataset, we decompose the diagnostic reports into fine-grained, organ-level descriptions using large language models for multi-granular contrastive learning. We evaluate our OpenVocabCT on downstream segmentation tasks across nine public datasets for organ and tumor segmentation, demonstrating the superior performance of our model compared to existing methods. All code, datasets, and models will be publicly released at <a target="_blank" rel="noopener" href="https://github.com/ricklisz/OpenVocabCT">https://github.com/ricklisz/OpenVocabCT</a>. </p>
<blockquote>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å¹¿æ³›åº”ç”¨äºå™¨å®˜å’Œç—…å˜çš„ç²¾ç¡®å¯è§†åŒ–å’Œåˆ†å‰²ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ï¼Œå·²ç»æ˜¾è‘—æ”¹å–„äº†CTå›¾åƒåˆ†æï¼Œä½†å®ƒä»¬åœ¨åº”ç”¨äºå¤šæ ·ã€ç°å®ä¸–ç•Œä¸´åºŠæ•°æ®æ—¶ï¼Œæ€§èƒ½å¾€å¾€ä¼šä¸‹é™ã€‚è™½ç„¶åŸºç¡€æ¨¡å‹æä¾›äº†æ›´å¹¿æ³›å’Œæ›´çµæ´»çš„è§£å†³æ–¹æ¡ˆï¼Œä½†ç”±äºè·å¾—å¤§è§„æ¨¡åŒ»å­¦å›¾åƒä½“ç´ çº§åˆ«æ³¨é‡Šçš„æŒ‘æˆ˜ï¼Œå®ƒä»¬çš„æ½œåŠ›å—åˆ°é™åˆ¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œå‡ºç°äº†åŸºäºæç¤ºçš„æ¨¡å‹ï¼Œä½¿ç”¨è§†è§‰æˆ–æ–‡æœ¬æç¤ºã€‚è§†è§‰æç¤ºæ–¹æ³•ï¼Œå¦‚Segment Anything Modelï¼ˆSAMï¼‰ï¼Œä»éœ€å¤§é‡çš„äººå·¥è¾“å…¥ï¼Œå¹¶ä¸”åœ¨åº”ç”¨äºä¸´åºŠåœºæ™¯æ—¶å¯èƒ½ä¼šå¼•å…¥æ­§ä¹‰ã€‚ç›¸åï¼Œä½¿ç”¨æ–‡æœ¬æç¤ºçš„åŸºç¡€æ¨¡å‹æä¾›äº†æ›´é€šç”¨å’Œä¸´åºŠä¸Šæ›´ç›¸å…³çš„æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“å‰çš„æ–‡æœ¬æç¤ºæ¨¡å‹ï¼Œå¦‚CLIPé©±åŠ¨çš„é€šç”¨æ¨¡å‹ï¼Œä»…é™äºåœ¨è®­ç»ƒæœŸé—´å·²ç»é‡åˆ°çš„æ–‡æœ¬æç¤ºï¼Œå¹¶éš¾ä»¥å¤„ç†ç°å®ä¸–ç•Œä¸´åºŠåº”ç”¨çš„å¤æ‚å’Œå¤šæ ·åœºæ™¯ã€‚æˆ‘ä»¬æè®®çš„OpenVocabCTæ˜¯ä¸€ç§è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®ƒåœ¨å¤§å‹3D CTå›¾åƒä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç”¨äºé€šç”¨æ–‡æœ¬é©±åŠ¨åˆ†å‰²ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§è§„æ¨¡çš„CT-RATEæ•°æ®é›†å°†è¯Šæ–­æŠ¥å‘Šåˆ†è§£æˆç»†ç²’åº¦çš„å™¨å®˜çº§åˆ«æè¿°ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šç²’åº¦å¯¹æ¯”å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨ä¹ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„OpenVocabCTæ¨¡å‹å’Œè‚¿ç˜¤åˆ†å‰²æ€§èƒ½ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹ç›¸æ¯”ç°æœ‰æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ricklisz/OpenVocabCT%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%A7%E3%80%82">https://github.com/ricklisz/OpenVocabCTä¸Šå…¬å¼€å‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06030v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰å¹¿æ³›åº”ç”¨äºå™¨å®˜å’Œç—…å˜çš„ç²¾ç¡®å¯è§†åŒ–å’Œåˆ†å‰²ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¦‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ï¼Œå·²ç»æ˜¾è‘—æ”¹å–„äº†CTå›¾åƒåˆ†æï¼Œä½†å½“åº”ç”¨äºå¤šæ ·ä¸”çœŸå®çš„ä¸´åºŠæ•°æ®æ—¶ï¼Œå…¶æ€§èƒ½å¾€å¾€ä¼šä¸‹é™ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œå‡ºç°äº†åŸºäºæç¤ºçš„æ¨¡å‹ï¼Œä½¿ç”¨è§†è§‰æˆ–æ–‡æœ¬æç¤ºã€‚è§†è§‰æç¤ºæ–¹æ³•ï¼Œå¦‚Segment Anything Modelï¼ˆSAMï¼‰ï¼Œä»éœ€è¦å¤§é‡çš„äººå·¥è¾“å…¥ï¼Œå¹¶ä¸”åœ¨åº”ç”¨äºä¸´åºŠåœºæ™¯æ—¶å¯èƒ½ä¼šå¼•å…¥æ­§ä¹‰ã€‚ç›¸åï¼Œä½¿ç”¨æ–‡æœ¬æç¤ºçš„åŸºç¡€æ¨¡å‹æä¾›äº†æ›´é€šç”¨å’Œä¸´åºŠç›¸å…³çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–‡æœ¬æç¤ºæ¨¡å‹ï¼Œå¦‚CLIPé©±åŠ¨çš„é€šç”¨æ¨¡å‹ï¼Œä»…é™äºåœ¨è®­ç»ƒæœŸé—´é‡åˆ°çš„æ–‡æœ¬æç¤ºï¼Œéš¾ä»¥å¤„ç†çœŸå®ä¸–ç•Œä¸´åºŠåº”ç”¨çš„å¤æ‚å’Œå¤šæ ·åŒ–åœºæ™¯ã€‚æˆ‘ä»¬æå‡ºäº†OpenVocabCTï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å¤§å‹ä¸‰ç»´CTå›¾åƒä¸Šé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºé€šç”¨çš„æ–‡æœ¬é©±åŠ¨åˆ†å‰²ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§è§„æ¨¡çš„CT-RATEæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå°†è¯Šæ–­æŠ¥å‘Šåˆ†è§£ä¸ºç²¾ç»†çš„å™¨å®˜çº§åˆ«æè¿°ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¤šç²’åº¦å¯¹æ¯”å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨ä¹ä¸ªå…¬å…±æ•°æ®é›†ä¸Šå¯¹OpenVocabCTè¿›è¡Œäº†ä¸‹æ¸¸åˆ†å‰²ä»»åŠ¡è¯„ä¼°ï¼Œå±•ç¤ºäº†å…¶åœ¨å™¨å®˜å’Œè‚¿ç˜¤åˆ†å‰²æ–¹é¢çš„ä¼˜è¶Šæ€§èƒ½ã€‚æ‰€æœ‰ä»£ç ã€æ•°æ®é›†å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ricklisz/OpenVocabCT%E5%85%AC%E5%BC%80%E9%83%A8%E7%BD%AE%E3%80%82">https://github.com/ricklisz/OpenVocabCTå…¬å¼€å‘å¸ƒã€‚</a></p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>CTå›¾åƒåˆ†æåœ¨ä¸´åºŠè¯Šæ–­ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œä½†åº”ç”¨äºçœŸå®ä¸–ç•Œä¸´åºŠæ•°æ®æ—¶æ·±åº¦å­¦ä¹ æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚</li>
<li>ç°æœ‰æ¨¡å‹é¢ä¸´å¤§è§„æ¨¡ã€åƒç´ çº§æ³¨é‡Šçš„è·å–æŒ‘æˆ˜ä»¥åŠå¤„ç†å¤æ‚ã€å¤šæ ·åŒ–ä¸´åºŠåœºæ™¯çš„èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>OpenVocabCTæ˜¯ä¸€ä¸ªåœ¨å¤§å‹ä¸‰ç»´CTå›¾åƒä¸Šé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œç”¨äºé€šç”¨çš„æ–‡æœ¬é©±åŠ¨åˆ†å‰²ï¼Œè§£å†³äº†ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨è¯Šæ–­æŠ¥å‘Šè¿›è¡Œå¤šç²’åº¦å¯¹æ¯”å­¦ä¹ ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>åœ¨ä¹ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¯„ä¼°æ˜¾ç¤ºï¼ŒOpenVocabCTåœ¨å™¨å®˜å’Œè‚¿ç˜¤åˆ†å‰²æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>æ‰€æœ‰ç›¸å…³èµ„æºå°†å…¬å¼€å‘å¸ƒï¼Œä¾¿äºç ”ç©¶è€…å’Œä¸´åºŠåŒ»ç”Ÿè®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bef00c5816968b42a3acf22c66dce32c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa15fba13012cece0b3622828137285f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5fd095b9b4d5f526b618f0da152cb19.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c026e7207aabe8141a5d4f58aebcce3b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf60d76d9cd86253beba75cef1b349aa.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ContextFormer-Redefining-Efficiency-in-Semantic-Segmentation"><a href="#ContextFormer-Redefining-Efficiency-in-Semantic-Segmentation" class="headerlink" title="ContextFormer: Redefining Efficiency in Semantic Segmentation"></a>ContextFormer: Redefining Efficiency in Semantic Segmentation</h2><p><strong>Authors:Mian Muhammad Naeem Abid, Nancy Mehta, Zongwei Wu, Radu Timofte</strong></p>
<p>Semantic segmentation assigns labels to pixels in images, a critical yet challenging task in computer vision. Convolutional methods, although capturing local dependencies well, struggle with long-range relationships. Vision Transformers (ViTs) excel in global context capture but are hindered by high computational demands, especially for high-resolution inputs. Most research optimizes the encoder architecture, leaving the bottleneck underexplored - a key area for enhancing performance and efficiency. We propose ContextFormer, a hybrid framework leveraging the strengths of CNNs and ViTs in the bottleneck to balance efficiency, accuracy, and robustness for real-time semantic segmentation. The frameworkâ€™s efficiency is driven by three synergistic modules: the Token Pyramid Extraction Module (TPEM) for hierarchical multi-scale representation, the Transformer and Branched DepthwiseConv (Trans-BDC) block for dynamic scale-aware feature modeling, and the Feature Merging Module (FMM) for robust integration with enhanced spatial and contextual consistency. Extensive experiments on ADE20K, Pascal Context, CityScapes, and COCO-Stuff datasets show ContextFormer significantly outperforms existing models, achieving state-of-the-art mIoU scores, setting a new benchmark for efficiency and performance. The codes will be made publicly available upon acceptance. </p>
<blockquote>
<p>è¯­ä¹‰åˆ†å‰²æ˜¯ç»™å›¾åƒä¸­çš„åƒç´ åˆ†é…æ ‡ç­¾çš„ä»»åŠ¡ï¼Œè¿™æ˜¯è®¡ç®—æœºè§†è§‰ä¸­è‡³å…³é‡è¦ä¸”å……æ»¡æŒ‘æˆ˜çš„ä»»åŠ¡ã€‚å·ç§¯æ–¹æ³•è™½ç„¶èƒ½å¾ˆå¥½åœ°æ•æ‰å±€éƒ¨ä¾èµ–æ€§ï¼Œä½†åœ¨å¤„ç†é•¿è·ç¦»å…³ç³»æ—¶å´é‡åˆ°å›°éš¾ã€‚è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰æ“…é•¿æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä½†è®¡ç®—éœ€æ±‚è¾ƒé«˜ï¼Œå°¤å…¶æ˜¯å¯¹äºé«˜åˆ†è¾¨ç‡è¾“å…¥ã€‚å¤§å¤šæ•°ç ”ç©¶ä¼˜åŒ–äº†ç¼–ç å™¨æ¶æ„ï¼Œè€Œå¿½ç•¥äº†ä¸€ä¸ªå…³é”®çš„æ€§èƒ½æå‡å’Œæ•ˆç‡æå‡ç“¶é¢ˆã€‚æˆ‘ä»¬æå‡ºäº†ContextFormerï¼Œè¿™æ˜¯ä¸€ä¸ªæ··åˆæ¡†æ¶ï¼Œåˆ©ç”¨CNNå’ŒViTåœ¨ç“¶é¢ˆå¤„çš„ä¼˜åŠ¿ï¼Œåœ¨å®æ—¶è¯­ä¹‰åˆ†å‰²ä¸­å¹³è¡¡æ•ˆç‡ã€å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚è¯¥æ¡†æ¶çš„æ•ˆç‡æ˜¯ç”±ä¸‰ä¸ªååŒæ¨¡å—é©±åŠ¨çš„ï¼šç”¨äºåˆ†å±‚å¤šå°ºåº¦è¡¨ç¤ºçš„Tokené‡‘å­—å¡”æå–æ¨¡å—ï¼ˆTPEMï¼‰ã€ç”¨äºåŠ¨æ€å°ºåº¦æ„ŸçŸ¥ç‰¹å¾å»ºæ¨¡çš„Transformerå’Œåˆ†æ”¯æ·±åº¦å·ç§¯ï¼ˆTrans-BDCï¼‰å—ã€ç”¨äºç¨³å¥é›†æˆçš„ç‰¹å¾åˆå¹¶æ¨¡å—ï¼ˆFMMï¼‰ï¼Œå…·æœ‰å¢å¼ºçš„ç©ºé—´å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚åœ¨ADE20Kã€Pascal Contextã€CityScapeså’ŒCOCO-Stuffæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒContextFormeræ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„mIoUåˆ†æ•°ï¼Œä¸ºæ•ˆç‡å’Œæ€§èƒ½è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚ä»£ç åœ¨æ¥å—åå°†ä¼šå…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19255v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è®¡ç®—æœºè§†è§‰ä¸­çš„è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åä¸ºContextFormerçš„æ··åˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†CNNå’ŒVision Transformerï¼ˆViTï¼‰çš„ä¼˜åŠ¿ï¼Œæ—¨åœ¨æé«˜å®æ—¶è¯­ä¹‰åˆ†å‰²çš„æ•ˆç‡ã€å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚å…¶é€šè¿‡ä¸‰ä¸ªååŒæ¨¡å—å®ç°é«˜æ•ˆæ€§ï¼šToken Pyramid Extraction Moduleï¼ˆTPEMï¼‰ç”¨äºåˆ†å±‚å¤šå°ºåº¦è¡¨ç¤ºï¼ŒTransformerå’ŒBranched DepthwiseConvï¼ˆTrans-BDCï¼‰å—ç”¨äºåŠ¨æ€å°ºåº¦æ„ŸçŸ¥ç‰¹å¾å»ºæ¨¡ï¼Œä»¥åŠFeature Merging Moduleï¼ˆFMMï¼‰ç”¨äºé²æ£’é›†æˆï¼Œå¢å¼ºç©ºé—´ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒContextFormeråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå®ç°äº†æœ€é«˜çš„mIoUåˆ†æ•°ï¼Œä¸ºæ•ˆç‡å’Œæ€§èƒ½è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ContextFormeræ˜¯ä¸€ä¸ªé’ˆå¯¹è¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„æ··åˆæ¡†æ¶ï¼Œæ—¨åœ¨ç»“åˆCNNå’ŒVision Transformerï¼ˆViTï¼‰çš„ä¼˜åŠ¿ã€‚</li>
<li>æ¡†æ¶é€šè¿‡ä¸‰ä¸ªååŒæ¨¡å—å®ç°é«˜æ•ˆæ€§ï¼šTPEMã€Trans-BDCå—å’ŒFMMã€‚</li>
<li>TPEMç”¨äºåˆ†å±‚å¤šå°ºåº¦è¡¨ç¤ºï¼ŒTrans-BDCå—å®ç°åŠ¨æ€å°ºåº¦æ„ŸçŸ¥ç‰¹å¾å»ºæ¨¡ï¼ŒFMMç”¨äºé²æ£’é›†æˆå¹¶å¢å¼ºç©ºé—´ä¸€è‡´æ€§ã€‚</li>
<li>ContextFormeråœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†æœ€é«˜çš„mIoUåˆ†æ•°ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
<li>è¯¥æ¡†æ¶æœ‰åŠ©äºæé«˜è¯­ä¹‰åˆ†å‰²çš„æ•ˆç‡ã€å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>ContextFormerçš„æå‡ºä¸ºè®¡ç®—æœºè§†è§‰é¢†åŸŸè®¾å®šäº†æ–°çš„æ€§èƒ½åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b8a9dd132c2a47a92540b7b47483f5f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5523b56a683d3257ef80b099b50baa8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-833eb5a30852a640bcad4d21cf3e25c2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Vulnerabilities-in-AI-generated-Image-Detection-The-Challenge-of-Adversarial-Attacks"><a href="#Vulnerabilities-in-AI-generated-Image-Detection-The-Challenge-of-Adversarial-Attacks" class="headerlink" title="Vulnerabilities in AI-generated Image Detection: The Challenge of   Adversarial Attacks"></a>Vulnerabilities in AI-generated Image Detection: The Challenge of   Adversarial Attacks</h2><p><strong>Authors:Yunfeng Diao, Naixin Zhai, Changtao Miao, Zitong Yu, Xingxing Wei, Xun Yang, Meng Wang</strong></p>
<p>Recent advancements in image synthesis, particularly with the advent of GAN and Diffusion models, have amplified public concerns regarding the dissemination of disinformation. To address such concerns, numerous AI-generated Image (AIGI) Detectors have been proposed and achieved promising performance in identifying fake images. However, there still lacks a systematic understanding of the adversarial robustness of AIGI detectors. In this paper, we examine the vulnerability of state-of-the-art AIGI detectors against adversarial attack under white-box and black-box settings, which has been rarely investigated so far. To this end, we propose a new method to attack AIGI detectors. First, inspired by the obvious difference between real images and fake images in the frequency domain, we add perturbations under the frequency domain to push the image away from its original frequency distribution. Second, we explore the full posterior distribution of the surrogate model to further narrow this gap between heterogeneous AIGI detectors, e.g. transferring adversarial examples across CNNs and ViTs. This is achieved by introducing a novel post-train Bayesian strategy that turns a single surrogate into a Bayesian one, capable of simulating diverse victim models using one pre-trained surrogate, without the need for re-training. We name our method as Frequency-based Post-train Bayesian Attack, or FPBA. Through FPBA, we show that adversarial attack is truly a real threat to AIGI detectors, because FPBA can deliver successful black-box attacks across models, generators, defense methods, and even evade cross-generator detection, which is a crucial real-world detection scenario. The code will be shared upon acceptance. </p>
<blockquote>
<p>è¿‘æœŸå›¾åƒåˆæˆé¢†åŸŸçš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„å‡ºç°ï¼ŒåŠ å‰§äº†å…¬ä¼—å¯¹è™šå‡ä¿¡æ¯ä¼ æ’­é—®é¢˜çš„æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™äº›æ‹…å¿§ï¼Œå·²ç»æå‡ºäº†è®¸å¤šAIç”Ÿæˆçš„å›¾åƒï¼ˆAIGIï¼‰æ£€æµ‹å™¨ï¼Œå¹¶åœ¨è¯†åˆ«è™šå‡å›¾åƒæ–¹é¢å–å¾—äº†æœ‰å‰æ™¯çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œå¯¹äºAIGIæ£€æµ‹å™¨çš„å¯¹æŠ—æ€§ç¨³å¥æ€§ï¼Œä»ç¼ºä¹ç³»ç»Ÿçš„ç†è§£ã€‚æœ¬æ–‡åœ¨ç™½çš®ä¹¦å’Œé»‘ç®±ç¯å¢ƒä¸­å¯¹æœ€å…ˆè¿›çš„AIGIæ£€æµ‹å™¨è¿›è¡Œå¯¹æŠ—æ€§æ”»å‡»çš„è„†å¼±æ€§è¿›è¡Œäº†è€ƒå¯Ÿï¼Œè¿™ä¸€å†…å®¹è¿„ä»Šä¸ºæ­¢å¾ˆå°‘è¢«ç ”ç©¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥æ”»å‡»AIGIæ£€æµ‹å™¨ã€‚é¦–å…ˆï¼Œå—åˆ°çœŸå®å›¾åƒå’Œè™šå‡å›¾åƒåœ¨é¢‘åŸŸä¸Šæ˜æ˜¾å·®å¼‚çš„å¯å‘ï¼Œæˆ‘ä»¬åœ¨é¢‘åŸŸæ·»åŠ æ‰°åŠ¨æ¥ä½¿å›¾åƒè¿œç¦»å…¶åŸå§‹é¢‘åŸŸåˆ†å¸ƒã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ›¿ä»£æ¨¡å‹çš„åéªŒåˆ†å¸ƒï¼Œä»¥è¿›ä¸€æ­¥ç¼©å°ä¸åŒAIGIæ£€æµ‹å™¨ä¹‹é—´çš„å·®è·ï¼Œä¾‹å¦‚å°†å¯¹æŠ—æ€§ç¤ºä¾‹ä»CNNè½¬ç§»åˆ°ViTsã€‚è¿™æ˜¯é€šè¿‡å¼•å…¥ä¸€ç§æ–°çš„åè®­ç»ƒè´å¶æ–¯ç­–ç•¥æ¥å®ç°çš„ï¼Œå®ƒå°†å•ä¸ªæ›¿ä»£æ¨¡å‹è½¬å˜ä¸ºè´å¶æ–¯æ¨¡å‹ï¼Œèƒ½å¤Ÿä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„æ›¿ä»£æ¨¡å‹æ¨¡æ‹Ÿå¤šç§å—å®³è€…æ¨¡å‹ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ–¹æ³•å‘½åä¸ºåŸºäºé¢‘åŸŸçš„åè®­ç»ƒè´å¶æ–¯æ”»å‡»ï¼ˆFPBAï¼‰ã€‚é€šè¿‡FPBAï¼Œæˆ‘ä»¬è¯æ˜äº†å¯¹æŠ—æ€§æ”»å‡»ç¡®å®å¯¹AIGIæ£€æµ‹å™¨æ„æˆçœŸæ­£çš„å¨èƒï¼Œå› ä¸ºFPBAå¯ä»¥åœ¨ä¸åŒæ¨¡å‹ã€ç”Ÿæˆå™¨ã€é˜²å¾¡æ–¹æ³•å’Œç”šè‡³é¿å…è·¨ç”Ÿæˆå™¨æ£€æµ‹çš„æƒ…å†µä¸‹æˆåŠŸè¿›è¡Œé»‘ç®±æ”»å‡»ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œçš„æ£€æµ‹åœºæ™¯ä¸­è‡³å…³é‡è¦ã€‚ä»£ç å°†åœ¨æ¥å—åå…±äº«ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20836v3">PDF</a> </p>
<p><strong>Summary</strong><br>å…ˆè¿›çš„å›¾åƒåˆæˆæŠ€æœ¯å¼•å‘å…¬ä¼—å¯¹å‡ä¿¡æ¯ä¼ æ’­é—®é¢˜çš„æ‹…å¿§ï¼Œé’ˆå¯¹æ­¤ï¼ŒAIç”Ÿæˆçš„å›¾åƒæ£€æµ‹å™¨åœ¨è¯†åˆ«è™šå‡å›¾åƒæ–¹é¢å±•ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚ç„¶è€Œï¼Œç›®å‰ç¼ºä¹å¯¹å…ˆè¿›AIå›¾åƒæ£€æµ‹å™¨å¯¹æŠ—é²æ£’æ€§çš„ç³»ç»Ÿç†è§£ã€‚æœ¬æ–‡ç ”ç©¶æœ€å…ˆè¿›AIå›¾åƒæ£€æµ‹å™¨åœ¨ç™½ç›’å’Œé»‘ç›’ç¯å¢ƒä¸‹çš„è„†å¼±æ€§ï¼Œå¹¶å¯¹å…¶è¿›è¡Œæ”»å‡»ã€‚é€šè¿‡åœ¨é¢‘ç‡åŸŸæ·»åŠ æ‰°åŠ¨å’Œåˆ©ç”¨ä»£ç†æ¨¡å‹çš„æ•´ä¸ªåéªŒåˆ†å¸ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ”»å‡»æ–¹æ³•â€”â€”åŸºäºé¢‘ç‡çš„åè®­ç»ƒè´å¶æ–¯æ”»å‡»ï¼ˆFPBAï¼‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åŒæ¨¡å‹ã€ç”Ÿæˆå™¨ã€é˜²å¾¡æ–¹æ³•å’Œè·¨ç”Ÿæˆå™¨æ£€æµ‹ä¸­æˆåŠŸè¿›è¡Œé»‘ç›’æ”»å‡»ï¼Œå¹¶ç»•å¼€æ£€æµ‹ï¼Œæ˜¾ç¤ºäº†å¯¹AIå›¾åƒæ£€æµ‹å™¨çš„ä¸¥é‡å¨èƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ˆè¿›çš„å›¾åƒåˆæˆæŠ€æœ¯å¼•å‘å…¬ä¼—å¯¹å‡å›¾åƒä¼ æ’­çš„æ‹…å¿§ï¼ŒAIç”Ÿæˆçš„å›¾åƒæ£€æµ‹å™¨åœ¨è¯†åˆ«è™šå‡å›¾åƒæ–¹é¢å·²å±•ç°å‡ºè‰¯å¥½æ€§èƒ½ã€‚</li>
<li>ç›®å‰ç¼ºä¹å¯¹AIå›¾åƒæ£€æµ‹å™¨å¯¹æŠ—é²æ£’æ€§çš„ç³»ç»Ÿç†è§£ã€‚</li>
<li>æå‡ºäº†åœ¨ç™½ç›’å’Œé»‘ç›’ç¯å¢ƒä¸‹é’ˆå¯¹æœ€å…ˆè¿›AIå›¾åƒæ£€æµ‹å™¨çš„æ”»å‡»æ–¹æ³•ã€‚</li>
<li>æ–°çš„æ”»å‡»æ–¹æ³•â€”â€”åŸºäºé¢‘ç‡çš„åè®­ç»ƒè´å¶æ–¯æ”»å‡»ï¼ˆFPBAï¼‰èƒ½å¤Ÿåœ¨ä¸åŒæ¨¡å‹ã€ç”Ÿæˆå™¨ã€é˜²å¾¡æ–¹æ³•å’Œè·¨ç”Ÿæˆå™¨æ£€æµ‹ä¸­æˆåŠŸè¿›è¡Œé»‘ç›’æ”»å‡»ã€‚</li>
<li>FPBAæ˜¾ç¤ºäº†å¯¹AIå›¾åƒæ£€æµ‹å™¨çš„ä¸¥é‡å¨èƒï¼Œå¹¶ç»•è¿‡äº†æŸäº›æ£€æµ‹æœºåˆ¶ã€‚</li>
<li>é€šè¿‡æ·»åŠ é¢‘ç‡åŸŸçš„æ‰°åŠ¨å’Œæ¨¡æ‹Ÿä¸åŒå—å®³æ¨¡å‹çš„åéªŒåˆ†å¸ƒæ¥æ‰§è¡ŒFPBAæ”»å‡»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.20836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-60580c8b560f7033aa5ec06969a7c423.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c82f2fff667db0fb1123e881d358555a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-811f50070869a29f56c31cff905cb7a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ac7eb11f615f0ad2559cb7713ffc88d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3cb24479ff1a88cc94afe084f610fbc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-196f03cf3132c16b893ba204157ee34a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DEEM-Diffusion-Models-Serve-as-the-Eyes-of-Large-Language-Models-for-Image-Perception"><a href="#DEEM-Diffusion-Models-Serve-as-the-Eyes-of-Large-Language-Models-for-Image-Perception" class="headerlink" title="DEEM: Diffusion Models Serve as the Eyes of Large Language Models for   Image Perception"></a>DEEM: Diffusion Models Serve as the Eyes of Large Language Models for   Image Perception</h2><p><strong>Authors:Run Luo, Yunshui Li, Longze Chen, Wanwei He, Ting-En Lin, Ziqiang Liu, Lei Zhang, Zikai Song, Xiaobo Xia, Tongliang Liu, Min Yang, Binyuan Hui</strong></p>
<p>The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data, such as which can hardly distinguish orientation, quantity, color, structure, etc. This is primarily due to their reliance on image encoders trained to encode images into task-relevant features, which may lead them to disregard irrelevant details. Delving into the modeling capabilities of diffusion models for images naturally prompts the question: Can diffusion models serve as the eyes of large language models for image perception? In this paper, we propose DEEM, a simple but effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. This addresses the drawbacks of previous methods that solely relied on image encoders like CLIP-ViT, thereby enhancing the modelâ€™s resilience against out-of-distribution samples and reducing visual hallucinations. Importantly, this is achieved without requiring additional training modules and with fewer training parameters. We extensively evaluated DEEM on both our newly constructed RobustVQA benchmark and other well-known benchmarks, POPE and MMVP, for visual hallucination and perception. In particular, DEEM improves LMMâ€™s visual perception performance to a large extent (e.g., 4% higher on RobustVQA, 6.5% higher on MMVP and 12.8 % higher on POPE ). Compared to the state-of-the-art interleaved content generation models, DEEM exhibits enhanced robustness and a superior capacity to alleviate model hallucinations while utilizing fewer trainable parameters, less pre-training data (10%), and a smaller base model size. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•æå¤§åœ°æ¨åŠ¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„å‡ºç°ã€‚è™½ç„¶LMMé€šè¿‡ä¿ƒè¿›å¤šæ¨¡æ€ç†è§£å’Œåˆ›ä½œçš„ååŒä½œç”¨å–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œä½†å®ƒä»¬åœ¨é¢å¯¹è¶…å‡ºåˆ†å¸ƒçš„æ•°æ®æ—¶ç»å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œä¾‹å¦‚æ— æ³•åŒºåˆ†æ–¹å‘ã€æ•°é‡ã€é¢œè‰²ã€ç»“æ„ç­‰ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬ä¾èµ–äºè®­ç»ƒæœ‰ç´ çš„å›¾åƒç¼–ç å™¨å°†å›¾åƒç¼–ç ä¸ºä¸ä»»åŠ¡ç›¸å…³çš„ç‰¹å¾ï¼Œè¿™å¯èƒ½å¯¼è‡´å®ƒä»¬å¿½ç•¥ä¸ç›¸å…³çš„ç»†èŠ‚ã€‚æ·±å…¥ç ”ç©¶æ‰©æ•£æ¨¡å‹å¯¹å›¾åƒçš„å»ºæ¨¡èƒ½åŠ›è‡ªç„¶ä¼šå¼•å‘ä¸€ä¸ªé—®é¢˜ï¼šæ‰©æ•£æ¨¡å‹èƒ½å¦ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„â€œçœ¼ç›â€æ¥è¿›è¡Œå›¾åƒæ„ŸçŸ¥ï¼Ÿåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DEEMï¼Œè¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆåé¦ˆæ¥å¯¹é½å›¾åƒç¼–ç å™¨çš„è¯­ä¹‰åˆ†å¸ƒã€‚è¿™è§£å†³äº†ä»¥å‰ä»…ä¾èµ–å›¾åƒç¼–ç å™¨çš„æ–¹æ³•ï¼ˆå¦‚CLIP-ViTï¼‰çš„ç¼ºç‚¹ï¼Œæé«˜äº†æ¨¡å‹å¯¹è¶…å‡ºåˆ†å¸ƒæ ·æœ¬çš„é€‚åº”æ€§ï¼Œå¹¶å‡å°‘äº†è§†è§‰å¹»è§‰ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™æ˜¯åœ¨ä¸æ·»åŠ é¢å¤–çš„è®­ç»ƒæ¨¡å—å’Œæ›´å°‘çš„è®­ç»ƒå‚æ•°çš„æƒ…å†µä¸‹å®ç°çš„ã€‚æˆ‘ä»¬åœ¨æ–°æ„å»ºçš„RobustVQAåŸºå‡†æµ‹è¯•å’Œå…¶ä»–çŸ¥åçš„POPEå’ŒMMVPåŸºå‡†æµ‹è¯•ä¸Šå¯¹DEEMè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œç”¨äºè§†è§‰å¹»è§‰å’Œæ„ŸçŸ¥ã€‚ç‰¹åˆ«æ˜¯ï¼ŒDEEMåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæé«˜äº†LMMçš„è§†è§‰æ„ŸçŸ¥æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼Œåœ¨RobustVQAä¸Šæé«˜äº†4%ï¼Œåœ¨MMVPä¸Šæé«˜äº†6.5%ï¼Œåœ¨POPEä¸Šæé«˜äº†12.8%ï¼‰ã€‚ä¸æœ€å…ˆè¿›çš„äº¤é”™å†…å®¹ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒDEEMå…·æœ‰æ›´å¼ºçš„ç¨³å¥æ€§ï¼Œåœ¨åˆ©ç”¨æ›´å°‘çš„å¯è®­ç»ƒå‚æ•°ã€æ›´å°‘çš„é¢„è®­ç»ƒæ•°æ®ï¼ˆ10%ï¼‰å’Œæ›´å°çš„åŸºæœ¬æ¨¡å‹å¤§å°çš„æƒ…å†µä¸‹ï¼Œå…·æœ‰å‡è½»æ¨¡å‹å¹»è§‰çš„ä¼˜è¶Šèƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15232v4">PDF</a> 25 pages. arXiv admin note: text overlap with arXiv:2401.10208 by   other authors</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•æå¤§åœ°æ¨åŠ¨äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰çš„å‡ºç°ã€‚è™½ç„¶LMMé€šè¿‡ä¿ƒè¿›å¤šæ¨¡æ€ç†è§£å’Œåˆ›é€ ä¹‹é—´çš„ååŒä½œç”¨å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œä½†å½“é¢å¯¹åˆ†å¸ƒå¤–çš„æ•°æ®æ—¶ï¼Œå®ƒä»¬å¸¸å¸¸é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥åŒºåˆ†æ–¹å‘ã€æ•°é‡ã€é¢œè‰²ã€ç»“æ„ç­‰ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•DEEMï¼Œå®ƒåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆåé¦ˆæ¥å¯¹é½å›¾åƒç¼–ç å™¨çš„è¯­ä¹‰åˆ†å¸ƒï¼Œè§£å†³äº†ä¹‹å‰æ–¹æ³•ä»…ä¾èµ–å›¾åƒç¼–ç å™¨ï¼ˆå¦‚CLIP-ViTï¼‰çš„ç¼ºç‚¹ï¼Œå¢å¼ºäº†æ¨¡å‹å¯¹åˆ†å¸ƒå¤–æ ·æœ¬çš„éŸ§æ€§ï¼Œå¹¶å‡å°‘äº†è§†è§‰å¹»è§‰ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæ¨¡å—ï¼Œå¹¶ä¸”è®­ç»ƒå‚æ•°æ›´å°‘ã€‚æˆ‘ä»¬åœ¨æ–°æ„å»ºçš„RobustVQAåŸºå‡†æµ‹è¯•å’Œå…¶ä»–è‘—åçš„POPEå’ŒMMVPåŸºå‡†æµ‹è¯•ä¸Šå¯¹DEEMè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œç”¨äºè§†è§‰å¹»è§‰å’Œæ„ŸçŸ¥ã€‚ç‰¹åˆ«æ˜¯ï¼ŒDEEMåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæé«˜äº†LMMçš„è§†è§‰æ„ŸçŸ¥æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼ŒRobustVQAä¸Šæé«˜äº†4%ï¼ŒMMVPä¸Šæé«˜äº†6.5%ï¼ŒPOPEä¸Šæé«˜äº†12.8%ï¼‰ã€‚ä¸æœ€å…ˆè¿›çš„äº¤æ›¿å†…å®¹ç”Ÿæˆæ¨¡å‹ç›¸æ¯”ï¼ŒDEEMå…·æœ‰æ›´å¼ºçš„ç¨³å¥æ€§ï¼Œåœ¨åˆ©ç”¨æ›´å°‘çš„å¯è®­ç»ƒå‚æ•°ã€é¢„è®­ç»ƒæ•°æ®ï¼ˆå‡å°‘è‡³10%ï¼‰å’Œè¾ƒå°çš„åŸºæœ¬æ¨¡å‹å¤§å°çš„æƒ…å†µä¸‹ï¼Œå…·æœ‰æ›´å‡ºè‰²çš„ç¼“è§£æ¨¡å‹å¹»è§‰çš„èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰åœ¨å¤„ç†åˆ†å¸ƒå¤–çš„å›¾åƒæ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéš¾ä»¥åŒºåˆ†æ–¹å‘ã€æ•°é‡ã€é¢œè‰²ç­‰ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºDEEMçš„æ–°æ–¹æ³•ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆåé¦ˆæ¥æé«˜å›¾åƒç¼–ç å™¨çš„è¯­ä¹‰åˆ†å¸ƒå¯¹é½ã€‚</li>
<li>DEEMå¢å¼ºäº†æ¨¡å‹çš„éŸ§æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°åº”å¯¹åˆ†å¸ƒå¤–çš„æ ·æœ¬ï¼Œå¹¶å‡å°‘äº†è§†è§‰å¹»è§‰ã€‚</li>
<li>DEEMæ–¹æ³•ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒæ¨¡å—ï¼Œå¹¶ä¸”ä½¿ç”¨æ›´å°‘çš„è®­ç»ƒå‚æ•°å³å¯å®ç°æ€§èƒ½æå‡ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šï¼ŒDEEMæ˜¾è‘—æé«˜äº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„è§†è§‰æ„ŸçŸ¥æ€§èƒ½ã€‚</li>
<li>ä¸å…¶ä»–å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”ï¼ŒDEEMå…·æœ‰æ›´å¼ºçš„ç¨³å¥æ€§ï¼Œå¹¶èƒ½æ›´æœ‰æ•ˆåœ°ç¼“è§£æ¨¡å‹å¹»è§‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c0b6103bc7ef9889b013616a33153dac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5911a832e2f068efcd4f1c57fb6c0989.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f388f04ad9850dd89191f6903b1cf64.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Explore-In-Context-Segmentation-via-Latent-Diffusion-Models"><a href="#Explore-In-Context-Segmentation-via-Latent-Diffusion-Models" class="headerlink" title="Explore In-Context Segmentation via Latent Diffusion Models"></a>Explore In-Context Segmentation via Latent Diffusion Models</h2><p><strong>Authors:Chaoyang Wang, Xiangtai Li, Henghui Ding, Lu Qi, Jiangning Zhang, Yunhai Tong, Chen Change Loy, Shuicheng Yan</strong></p>
<p>In-context segmentation has drawn increasing attention with the advent of vision foundation models. Its goal is to segment objects using given reference images. Most existing approaches adopt metric learning or masked image modeling to build the correlation between visual prompts and input image queries. This work approaches the problem from a fresh perspective - unlocking the capability of the latent diffusion model (LDM) for in-context segmentation and investigating different design choices. Specifically, we examine the problem from three angles: instruction extraction, output alignment, and meta-architectures. We design a two-stage masking strategy to prevent interfering information from leaking into the instructions. In addition, we propose an augmented pseudo-masking target to ensure the model predicts without forgetting the original images. Moreover, we build a new and fair in-context segmentation benchmark that covers both image and video datasets. Experiments validate the effectiveness of our approach, demonstrating comparable or even stronger results than previous specialist or visual foundation models. We hope our work inspires others to rethink the unification of segmentation and generation. </p>
<blockquote>
<p>éšç€è§†è§‰åŸºç¡€æ¨¡å‹çš„å‡ºç°ï¼Œä¸Šä¸‹æ–‡åˆ†å‰²å·²ç»å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚å®ƒçš„ç›®æ ‡æ˜¯åˆ©ç”¨ç»™å®šçš„å‚è€ƒå›¾åƒå¯¹å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•é‡‡ç”¨åº¦é‡å­¦ä¹ æˆ–æ©è†œå›¾åƒå»ºæ¨¡ï¼Œåœ¨è§†è§‰æç¤ºå’Œè¾“å…¥å›¾åƒæŸ¥è¯¢ä¹‹é—´å»ºç«‹å…³è”ã€‚è¿™é¡¹å·¥ä½œä»ä¸€ä¸ªå…¨æ–°çš„è§’åº¦æ¥è§£å†³è¿™ä¸ªé—®é¢˜â€”â€”è§£é”æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰åœ¨ä¸Šä¸‹æ–‡åˆ†å‰²æ–¹é¢çš„èƒ½åŠ›ï¼Œå¹¶ç ”ç©¶ä¸åŒçš„è®¾è®¡é€‰æ‹©ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»ä¸‰ä¸ªæ–¹é¢æ¥æ¢è®¨è¿™ä¸ªé—®é¢˜ï¼šæŒ‡ä»¤æå–ã€è¾“å‡ºå¯¹é½å’Œå…ƒæ¶æ„ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ©è†œç­–ç•¥ï¼Œä»¥é˜²æ­¢å¹²æ‰°ä¿¡æ¯æ³„éœ²åˆ°æŒ‡ä»¤ä¸­ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºçš„ä¼ªæ©è†œç›®æ ‡ï¼Œä»¥ç¡®ä¿æ¨¡å‹åœ¨é¢„æµ‹æ—¶ä¸ä¼šå¿˜è®°åŸå§‹å›¾åƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ–°çš„å…¬å¹³ä¸Šä¸‹æ–‡åˆ†å‰²åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–å›¾åƒå’Œè§†é¢‘æ•°æ®é›†ã€‚å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºä¸ä¹‹å‰çš„ä¸“å®¶æˆ–è§†è§‰åŸºç¡€æ¨¡å‹ç›¸å½“ç”šè‡³æ›´å¼ºçš„ç»“æœã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„å·¥ä½œèƒ½æ¿€åŠ±ä»–äººé‡æ–°æ€è€ƒåˆ†å‰²å’Œç”Ÿæˆçš„ç»Ÿä¸€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.09616v2">PDF</a> AAAI 2025</p>
<p><strong>Summary</strong><br>åœ¨æƒ…å¢ƒåˆ†å‰²é¢†åŸŸï¼Œæœ¬æ–‡é‡‡ç”¨æ‰©æ•£æ¨¡å‹è§†è§’ï¼Œä»æŒ‡ä»¤æå–ã€è¾“å‡ºå¯¹é½å’Œå…ƒæ¶æ„ä¸‰ä¸ªè§’åº¦è¿›è¡Œç ”ç©¶ã€‚æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µé®è”½ç­–ç•¥æ¥é˜²æ­¢å¹²æ‰°ä¿¡æ¯æ³„æ¼åˆ°æŒ‡ä»¤ä¸­ï¼Œå¹¶æå‡ºä¸€ç§å¢å¼ºå‹ä¼ªé®è”½ç›®æ ‡æ¥ç¡®ä¿æ¨¡å‹çš„é¢„æµ‹ä¸ä¼šå¿˜è®°åŸå§‹å›¾åƒã€‚åŒæ—¶ï¼Œå»ºç«‹äº†ä¸€ä¸ªæ–°çš„æƒ…å¢ƒåˆ†å‰²åŸºå‡†æµ‹è¯•é›†ï¼Œæ¶µç›–å›¾åƒå’Œè§†é¢‘æ•°æ®é›†ã€‚å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸å…ˆå‰çš„ä¸“ä¸šæˆ–è§†è§‰åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼Œå–å¾—äº†ç›¸å½“ç”šè‡³æ›´å¼ºçš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…å¢ƒåˆ†å‰²æ˜¯ç»™å®šå‚è€ƒå›¾åƒå¯¹ç›®æ ‡è¿›è¡Œåˆ†å‰²çš„é‡è¦ä»»åŠ¡ã€‚æœ¬æ–‡ç ”ç©¶å…¶åœ¨è§†è§‰åŸºç¡€æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†è§’æ¥è§£å†³æƒ…å¢ƒåˆ†å‰²é—®é¢˜ï¼Œå³é€šè¿‡åˆ©ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„èƒ½åŠ›æ¥å®ç°è¿™ä¸€ç‚¹ã€‚</li>
<li>è¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†ä¸åŒçš„è®¾è®¡é€‰æ‹©ï¼Œå¦‚æŒ‡ä»¤æå–ã€è¾“å‡ºå¯¹é½å’Œå…ƒæ¶æ„çš„æ„å»ºã€‚</li>
<li>ä¸ºäº†é˜²æ­¢å¹²æ‰°ä¿¡æ¯æ³„æ¼åˆ°æŒ‡ä»¤ä¸­ï¼Œé‡‡ç”¨äº†ä¸¤é˜¶æ®µé®è”½ç­–ç•¥ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¢å¼ºå‹ä¼ªé®è”½ç›®æ ‡æ¥ç¡®ä¿æ¨¡å‹é¢„æµ‹çš„å‡†ç¡®æ€§å¹¶é¿å…é—å¿˜åŸå§‹å›¾åƒã€‚</li>
<li>å»ºç«‹äº†ä¸€ä¸ªæ–°çš„æƒ…å¢ƒåˆ†å‰²åŸºå‡†æµ‹è¯•é›†ï¼Œæ¶µç›–å›¾åƒå’Œè§†é¢‘æ•°æ®é›†ï¼Œä¸ºè¯„ä¼°æ¨¡å‹æ€§èƒ½æä¾›äº†é‡è¦ä¾æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.09616">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-73c705f1fad3432f5ebcfd607743ecfa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cfccdd4c84c08ee7c9bb8079165b391.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-719569fa7ad23d7df69c744f1cf8ee10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b569853cee96f32ae57d70078fdf6635.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-468c61a22c4f1dc56d88d4913a14ca6d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d1014b3be6b2c87f880bc616ee4e6ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b660283f1fcb5aea9f1a37de12ebe04f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a2227d91c60a66a77c7ad6fa4a7aade.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ComFe-An-Interpretable-Head-for-Vision-Transformers"><a href="#ComFe-An-Interpretable-Head-for-Vision-Transformers" class="headerlink" title="ComFe: An Interpretable Head for Vision Transformers"></a>ComFe: An Interpretable Head for Vision Transformers</h2><p><strong>Authors:Evelyn J. Mannix, Liam Hodgkinson, Howard Bondell</strong></p>
<p>Interpretable computer vision models explain their classifications through comparing the distances between the local embeddings of an image and a set of prototypes that represent the training data. However, these approaches introduce additional hyper-parameters that need to be tuned to apply to new datasets, scale poorly, and are more computationally intensive to train in comparison to black-box approaches. In this work, we introduce Component Features (ComFe), a highly scalable interpretable-by-design image classification head for pretrained Vision Transformers (ViTs) that can obtain competitive performance in comparison to comparable non-interpretable methods. ComFe is the first interpretable head, that we know of, and unlike other interpretable approaches, can be readily applied to large scale datasets such as ImageNet-1K. Additionally, ComFe provides improved robustness and outperforms previous interpretable approaches on key benchmark datasets$\unicode{x2013}$using a consistent set of hyper-parameters and without finetuning the pretrained ViT backbone. With only global image labels and no segmentation or part annotations, ComFe can identify consistent component features within an image and determine which of these features are informative in making a prediction. Code is available at <a target="_blank" rel="noopener" href="https://github.com/emannix/comfe-component-features">https://github.com/emannix/comfe-component-features</a>. </p>
<blockquote>
<p>è§£é‡Šæ€§è®¡ç®—æœºè§†è§‰æ¨¡å‹é€šè¿‡æ¯”è¾ƒå›¾åƒçš„å±€éƒ¨åµŒå…¥ä¸ä»£è¡¨è®­ç»ƒæ•°æ®çš„ä¸€ç»„åŸå‹ä¹‹é—´çš„è·ç¦»æ¥è§£é‡Šå…¶åˆ†ç±»ç»“æœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¼•å…¥äº†ä¸€äº›é¢å¤–çš„è¶…å‚æ•°ï¼Œéœ€è¦é’ˆå¯¹æ–°æ•°æ®é›†è¿›è¡Œè°ƒæ•´ï¼Œæ‰©å±•æ€§å·®ï¼Œå¹¶ä¸”ä¸é»‘ç›’æ–¹æ³•ç›¸æ¯”è®¡ç®—æˆæœ¬æ›´é«˜æ˜‚è¿›è¡Œè®­ç»ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç»„ä»¶ç‰¹å¾ï¼ˆComFeï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸ºé¢„è®­ç»ƒçš„è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰è®¾è®¡çš„å›¾åƒåˆ†ç±»å¤´éƒ¨ï¼Œå®ƒé€šè¿‡è®¾è®¡å…·æœ‰é«˜åº¦å¯è§£é‡Šæ€§ï¼Œå¯ä»¥è·å¾—ä¸éè§£é‡Šæ€§æ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚ComFeæ˜¯æˆ‘ä»¬æ‰€äº†è§£çš„ç¬¬ä¸€ä¸ªè§£é‡Šæ€§å¤´éƒ¨ï¼Œä¸å…¶ä»–è§£é‡Šæ€§æ–¹æ³•ä¸åŒï¼Œå®ƒå¯ä»¥è½»æ¾åº”ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œä¾‹å¦‚ImageNet-1Kã€‚æ­¤å¤–ï¼ŒComFeæé«˜äº†ç¨³å¥æ€§ï¼Œå¹¶åœ¨å…³é”®åŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºä¹‹å‰çš„è§£é‡Šæ€§æ–¹æ³•ï¼Œä½¿ç”¨ä¸€å¥—ä¸€è‡´çš„è¶…å‚æ•°ä¸”æ— éœ€å¾®è°ƒé¢„è®­ç»ƒçš„ViTä¸»å¹²ã€‚ä»…å‡­å…¨å±€å›¾åƒæ ‡ç­¾è€Œæ— éœ€åˆ†æ®µæˆ–éƒ¨åˆ†æ³¨é‡Šï¼ŒComFeå¯ä»¥åœ¨å›¾åƒå†…éƒ¨è¯†åˆ«ä¸€è‡´çš„ç»„ä»¶ç‰¹å¾ï¼Œå¹¶ç¡®å®šå“ªäº›ç‰¹å¾å¯¹åšå‡ºé¢„æµ‹å…·æœ‰ä¿¡æ¯ä»·å€¼ã€‚ä»£ç å¯ä»¥åœ¨ä»¥ä¸‹ç½‘ç«™æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://www.tapceaiwuyouyusanzhejiangyibai.com/">é“¾æ¥åœ°å€https://github.com/emannix/comfe-component-featuresâ€‹â€‹ã€‚</a>â€‹â€‹</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.04125v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºComponent Featuresï¼ˆComFeï¼‰çš„é¢„è®­ç»ƒVision Transformerï¼ˆViTï¼‰å›¾åƒåˆ†ç±»å¤´ï¼Œå…·æœ‰å¯è§£é‡Šæ€§å’Œé«˜åº¦å¯æ‰©å±•æ€§ã€‚ç›¸è¾ƒäºå…¶ä»–è§£é‡Šæ€§æ–¹æ³•ï¼ŒComFeå¯åº”ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†å¦‚ImageNet-1Kï¼Œä½¿ç”¨ä¸€è‡´çš„è¶…å‚æ•°ï¼Œæ— éœ€å¾®è°ƒé¢„è®­ç»ƒçš„ViTä¸»å¹²ï¼Œåœ¨å…³é”®åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå¹¶æé«˜äº†ç¨³å¥æ€§ã€‚ComFeä»…éœ€å…¨å±€å›¾åƒæ ‡ç­¾ï¼Œæ— éœ€åˆ†å‰²æˆ–éƒ¨åˆ†æ³¨é‡Šï¼Œå¯è¯†åˆ«å›¾åƒä¸­çš„ä¸€è‡´ç»„ä»¶ç‰¹å¾ï¼Œå¹¶ç¡®å®šå“ªäº›ç‰¹å¾å¯¹é¢„æµ‹å…·æœ‰ä¿¡æ¯ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ComFeæ˜¯ä¸€ç§ç”¨äºé¢„è®­ç»ƒVision Transformerï¼ˆViTï¼‰çš„é«˜åº¦å¯æ‰©å±•çš„è§£é‡Šæ€§å›¾åƒåˆ†ç±»å¤´ã€‚</li>
<li>ComFeé€šè¿‡æ¯”è¾ƒå›¾åƒå±€éƒ¨åµŒå…¥ä¸ä»£è¡¨è®­ç»ƒæ•°æ®çš„åŸå‹é›†ä¹‹é—´çš„è·ç¦»æ¥è§£é‡Šå…¶åˆ†ç±»ã€‚</li>
<li>ä¸å…¶ä»–è§£é‡Šæ–¹æ³•ç›¸æ¯”ï¼ŒComFeåœ¨å¤§å‹æ•°æ®é›†å¦‚ImageNet-1Kä¸Šçš„è¡¨ç°æ›´å…·ç«äº‰åŠ›ã€‚</li>
<li>ComFeä½¿ç”¨ä¸€è‡´çš„è¶…å‚æ•°ï¼Œæ— éœ€é’ˆå¯¹æ–°æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚</li>
<li>ComFeæé«˜äº†ç¨³å¥æ€§ï¼Œå¹¶åœ¨å…³é”®åŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºä¹‹å‰çš„è§£é‡Šæ–¹æ³•ã€‚</li>
<li>ComFeä»…ä¾èµ–å…¨å±€å›¾åƒæ ‡ç­¾ï¼Œæ— éœ€é¢å¤–çš„åˆ†å‰²æˆ–éƒ¨åˆ†æ³¨é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.04125">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f72572cfa6b33b77d3bdfdcbf7c9cc78.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4b188f2641d88b441f754ded7db85bef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d0d6b9142399df7b7118ebf13df25bf.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Mixture-of-Exemplars-Approach-for-Efficient-Out-of-Distribution-Detection-with-Foundation-Models"><a href="#A-Mixture-of-Exemplars-Approach-for-Efficient-Out-of-Distribution-Detection-with-Foundation-Models" class="headerlink" title="A Mixture of Exemplars Approach for Efficient Out-of-Distribution   Detection with Foundation Models"></a>A Mixture of Exemplars Approach for Efficient Out-of-Distribution   Detection with Foundation Models</h2><p><strong>Authors:Evelyn Mannix, Howard Bondell</strong></p>
<p>One of the early weaknesses identified in deep neural networks trained for image classification tasks was their inability to provide low confidence predictions on out-of-distribution (OOD) data that was significantly different from the in-distribution (ID) data used to train them. Representation learning, where neural networks are trained in specific ways that improve their ability to detect OOD examples, has emerged as a promising solution. However, these approaches require long training times and can add additional overhead to detect OOD examples. Recent developments in Vision Transformer (ViT) foundation models$\unicode{x2013}$large networks trained on large and diverse datasets with self-supervised approaches$\unicode{x2013}$also show strong performance in OOD detection, and could address these challenges. This paper presents Mixture of Exemplars (MoLAR), an efficient approach to tackling OOD detection challenges that is designed to maximise the benefit of training a classifier with a high quality, frozen, pretrained foundation model backbone. MoLAR provides strong OOD performance when only comparing the similarity of OOD examples to the exemplars, a small set of images chosen to be representative of the dataset, leading to up to 30 times faster OOD detection inference over other methods that provide best performance when the full ID dataset is used. In some cases, only using these exemplars actually improves performance with MoLAR. Extensive experiments demonstrate the improved OOD detection performance of MoLAR in comparison to comparable approaches in both supervised and semi-supervised settings, and code is available at github.com&#x2F;emannix&#x2F;molar-mixture-of-exemplars. </p>
<blockquote>
<p>åœ¨æ—©æœŸè¯†åˆ«æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œå›¾åƒåˆ†ç±»ä»»åŠ¡æ—¶çš„ä¸€ä¸ªå¼±ç‚¹æ˜¯å®ƒä»¬å¯¹åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®æä¾›ä½ç½®ä¿¡åº¦é¢„æµ‹çš„èƒ½åŠ›ä¸è¶³ï¼Œè¿™äº›æ•°æ®ä¸ç”¨äºè®­ç»ƒå®ƒä»¬çš„åˆ†å¸ƒå†…ï¼ˆIDï¼‰æ•°æ®æœ‰å¾ˆå¤§çš„ä¸åŒã€‚è¡¨å¾å­¦ä¹ åº”è¿è€Œç”Ÿï¼Œåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œç¥ç»ç½‘ç»œä»¥ç‰¹å®šçš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œæé«˜äº†å®ƒä»¬æ£€æµ‹OODæ ·æœ¬çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•éœ€è¦å¾ˆé•¿çš„è®­ç»ƒæ—¶é—´ï¼Œå¹¶ä¸”å¯èƒ½ä¼šå¢åŠ æ£€æµ‹OODæ ·æœ¬çš„é¢å¤–å¼€é”€ã€‚æœ€è¿‘ï¼ŒVision Transformerï¼ˆViTï¼‰åŸºç¡€æ¨¡å‹çš„å‘å±•â€”â€”åœ¨å¤§å‹å’Œå¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šé‡‡ç”¨è‡ªç›‘ç£æ–¹æ³•è®­ç»ƒçš„å¤§å‹ç½‘ç»œâ€”â€”åœ¨OODæ£€æµ‹æ–¹é¢ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶å¯èƒ½è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†Mixture of Exemplarsï¼ˆMoLARï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è§£å†³OODæ£€æµ‹æŒ‘æˆ˜çš„æœ‰æ•ˆæ–¹æ³•ï¼Œæ—¨åœ¨æœ€å¤§é™åº¦åœ°å‘æŒ¥ä½¿ç”¨é«˜è´¨é‡ã€å†»ç»“çš„é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹éª¨æ¶è®­ç»ƒåˆ†ç±»å™¨çš„ä¼˜åŠ¿ã€‚MoLARä»…é€šè¿‡æ¯”è¾ƒOODç¤ºä¾‹ä¸æ ·æœ¬çš„ç›¸ä¼¼æ€§ï¼Œå³é€‰æ‹©ä¸€å°éƒ¨åˆ†å…·æœ‰ä»£è¡¨æ€§çš„å›¾åƒæ¥ä»£è¡¨æ•°æ®é›†ï¼Œå³å¯å®ç°å¼ºå¤§çš„OODæ€§èƒ½ï¼Œå¯¼è‡´åœ¨ä¸å…¶ä»–æ–¹æ³•è¿›è¡Œæ¯”è¾ƒæ—¶ï¼Œå…¶OODæ£€æµ‹æ¨ç†é€Ÿåº¦æœ€å¿«å¯è¾¾30å€ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä»…ä½¿ç”¨è¿™äº›æ ·æœ¬å®é™…ä¸Šå¯ä»¥æé«˜MoLARçš„æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸å¯æ¯”çš„ç›‘ç£å’ŒåŠç›‘ç£æ–¹æ³•ç›¸æ¯”ï¼ŒMoLARåœ¨OODæ£€æµ‹æ–¹é¢çš„æ€§èƒ½æœ‰æ‰€æé«˜ï¼Œä»£ç å¯åœ¨github.com&#x2F;emannix&#x2F;molar-mixture-of-exemplarsæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.17093v5">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºMoLARçš„æ··åˆèŒƒä¾‹æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­å¯¹å¼‚å¸¸æ•°æ®é¢„æµ‹èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–é¢„è®­ç»ƒæ¨¡å‹æ¶æ„å¹¶åˆ©ç”¨é«˜è´¨èŒƒä¾‹å­é›†æ¥æœ€å¤§åŒ–åˆ†ç±»å™¨æ€§èƒ½ã€‚ç›¸è¾ƒäºä¾èµ–å®Œæ•´æ•°æ®é›†çš„æ–¹æ³•ï¼ŒMoLARåªéœ€ä½¿ç”¨ä»£è¡¨æ•°æ®é›†çš„èŒƒä¾‹å­é›†è¿›è¡Œç›¸ä¼¼æ€§æ¯”è¾ƒï¼Œå®ç°æ›´å¿«çš„å¼‚å¸¸æ£€æµ‹æ¨ç†é€Ÿåº¦ï¼Œå¹¶æé«˜äº†æ€§èƒ½è¡¨ç°ã€‚è¯¥æ–¹æ³•é€‚ç”¨äºç›‘ç£å­¦ä¹ å’ŒåŠç›‘ç£å­¦ä¹ ç¯å¢ƒï¼Œå·²åœ¨å®éªŒä¸­å¾—åˆ°è¯å®æ€§èƒ½ä¼˜åŠ¿ï¼Œä¸”ä»£ç å·²å‘å¸ƒäºgithubã€‚com&#x2F;emannix&#x2F;molar-mixture-of-exemplarsä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MoLARä½œä¸ºä¸€ç§åŸºäºæ··åˆèŒƒä¾‹çš„æ–¹æ³•è¢«æå‡ºæ¥è§£å†³æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­çš„å¼‚å¸¸æ•°æ®æ£€æµ‹é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä¼˜åŒ–é¢„è®­ç»ƒæ¨¡å‹æ¶æ„æå‡å…¶æ€§èƒ½ï¼Œæé«˜æ£€æµ‹å¼‚å¸¸æ•°æ®çš„èƒ½åŠ›ã€‚</li>
<li>MoLARåˆ©ç”¨ä»£è¡¨æ•°æ®é›†çš„èŒƒä¾‹å­é›†è¿›è¡Œç›¸ä¼¼æ€§æ¯”è¾ƒï¼Œå®ç°äº†å¿«é€Ÿçš„å¼‚å¸¸æ£€æµ‹æ¨ç†é€Ÿåº¦ã€‚ç›¸è¾ƒäºä¾èµ–å®Œæ•´æ•°æ®é›†çš„æ–¹æ³•ï¼Œå…¶æ¨ç†é€Ÿåº¦æé«˜äº†é«˜è¾¾30å€ã€‚</li>
<li>MoLARåœ¨ç›‘ç£å­¦ä¹ å’ŒåŠç›‘ç£å­¦ä¹ ç¯å¢ƒä¸­éƒ½å±•ç°å‡ºäº†ä¼˜ç§€çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.17093">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce43247177a08627c1c15d7fdb6c6b27.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b3d347049c36873c6c86500c046344ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a1820e7cfbc788644a4043d2ae2fa9a9.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-39a27e50e75216213ac48fafa49c5162.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  Mitigating Hallucinations in YOLO-based Object Detection Models A   Revisit to Out-of-Distribution Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ef263382987a3050ca07b0aff2682f40.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  ALLVB All-in-One Long Video Understanding Benchmark
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">22950.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
