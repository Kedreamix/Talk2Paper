<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  Brain Inspired Adaptive Memory Dual-Net for Few-Shot Image   Classification">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-7b883c06e632db846504de44b4ac49cf.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    49 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-12-æ›´æ–°"><a href="#2025-03-12-æ›´æ–°" class="headerlink" title="2025-03-12 æ›´æ–°"></a>2025-03-12 æ›´æ–°</h1><h2 id="Brain-Inspired-Adaptive-Memory-Dual-Net-for-Few-Shot-Image-Classification"><a href="#Brain-Inspired-Adaptive-Memory-Dual-Net-for-Few-Shot-Image-Classification" class="headerlink" title="Brain Inspired Adaptive Memory Dual-Net for Few-Shot Image   Classification"></a>Brain Inspired Adaptive Memory Dual-Net for Few-Shot Image   Classification</h2><p><strong>Authors:Kexin Di, Xiuxing Li, Yuyang Han, Ziyu Li, Qing Li, Xia Wu</strong></p>
<p>Few-shot image classification has become a popular research topic for its wide application in real-world scenarios, however the problem of supervision collapse induced by single image-level annotation remains a major challenge. Existing methods aim to tackle this problem by locating and aligning relevant local features. However, the high intra-class variability in real-world images poses significant challenges in locating semantically relevant local regions under few-shot settings. Drawing inspiration from the humanâ€™s complementary learning system, which excels at rapidly capturing and integrating semantic features from limited examples, we propose the generalization-optimized Systems Consolidation Adaptive Memory Dual-Network, SCAM-Net. This approach simulates the systems consolidation of complementary learning system with an adaptive memory module, which successfully addresses the difficulty of identifying meaningful features in few-shot scenarios. Specifically, we construct a Hippocampus-Neocortex dual-network that consolidates structured representation of each category, the structured representation is then stored and adaptively regulated following the generalization optimization principle in a long-term memory inside Neocortex. Extensive experiments on benchmark datasets show that the proposed model has achieved state-of-the-art performance. </p>
<blockquote>
<p>å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ç”±äºå…¶åœ¨å®é™…åœºæ™¯ä¸­çš„å¹¿æ³›åº”ç”¨å·²æˆä¸ºçƒ­é—¨çš„ç ”ç©¶è¯¾é¢˜ï¼Œç„¶è€Œç”±å•å›¾åƒçº§æ ‡æ³¨å¼•èµ·çš„ç›‘ç£å´©æºƒé—®é¢˜ä»æ˜¯ä¸»è¦æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•æ—¨åœ¨é€šè¿‡å®šä½å’ŒåŒ¹é…ç›¸å…³å±€éƒ¨ç‰¹å¾æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œå›¾åƒä¸­çš„é«˜ç±»å†…å˜å¼‚æ€§ç»™å°‘æ ·æœ¬è®¾ç½®ä¸‹å®šä½è¯­ä¹‰ç›¸å…³å±€éƒ¨åŒºåŸŸå¸¦æ¥äº†å·¨å¤§æŒ‘æˆ˜ã€‚å—äººç±»è¾…åŠ©å­¦ä¹ ç³»ç»Ÿçš„å¯å‘ï¼Œè¯¥ç³»ç»Ÿæ“…é•¿ä»æœ‰é™æ ·æœ¬ä¸­å¿«é€Ÿæ•è·å’Œæ•´åˆè¯­ä¹‰ç‰¹å¾ï¼Œæˆ‘ä»¬æå‡ºäº†æ³›åŒ–ä¼˜åŒ–ç³»ç»Ÿå·©å›ºè‡ªé€‚åº”è®°å¿†åŒç½‘ç»œï¼Œå³SCAM-Netã€‚è¯¥æ–¹æ³•é€šè¿‡è‡ªé€‚åº”è®°å¿†æ¨¡å—æ¨¡æ‹Ÿè¾…åŠ©å­¦ä¹ ç³»ç»Ÿçš„ç³»ç»Ÿæ•´åˆï¼ŒæˆåŠŸè§£å†³äº†å°‘æ ·æœ¬åœºæ™¯ä¸­è¯†åˆ«æœ‰æ„ä¹‰ç‰¹å¾çš„å›°éš¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªæµ·é©¬ä½“-æ–°çš®å±‚åŒç½‘ç»œï¼Œè¯¥ç½‘ç»œå·©å›ºäº†æ¯ä¸ªç±»åˆ«çš„ç»“æ„åŒ–è¡¨ç¤ºï¼Œç„¶åæŒ‰ç…§æ–°çš®å±‚ä¸­çš„æ³›åŒ–ä¼˜åŒ–åŸåˆ™ï¼Œå°†ç»“æ„åŒ–è¡¨ç¤ºå­˜å‚¨åœ¨é•¿æœŸè®°å¿†ä¸­å¹¶è¿›è¡Œè‡ªé€‚åº”è°ƒèŠ‚ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹å·²è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07396v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°‘æ ·æœ¬å›¾åƒåˆ†ç±»ä¸­çš„ç›‘ç£å´©æºƒé—®é¢˜ï¼Œå¹¶å—åˆ°äººç±»äº’è¡¥å­¦ä¹ ç³»ç»Ÿçš„å¯å‘ï¼Œæå‡ºäº†åä¸ºSCAM-Netçš„å¹¿ä¹‰ä¼˜åŒ–è®°å¿†è‡ªé€‚åº”è®°å¿†åŒç½‘ç»œã€‚é€šè¿‡è‡ªé€‚åº”è®°å¿†æ¨¡å—æ¨¡æ‹Ÿäººç±»è®°å¿†å·©å›ºè¿‡ç¨‹ï¼Œè§£å†³åœ¨å°‘æ ·æœ¬åœºæ™¯ä¸‹è¯†åˆ«æœ‰æ„ä¹‰ç‰¹å¾çš„é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å°‘æ ·æœ¬å›¾åƒåˆ†ç±»é¢ä¸´çš„é—®é¢˜ï¼šå› å•å¼ å›¾åƒæ ‡æ³¨å¯¼è‡´çš„ç›‘ç£å´©æºƒé—®é¢˜ï¼Œä»¥åŠåœ¨ç°å®å›¾åƒä¸­é«˜ç±»å†…å˜åŒ–å¯¼è‡´çš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶åŠ¨æœºï¼šä»äººç±»å¿«é€Ÿä»æœ‰é™æ ·æœ¬ä¸­æ•è·å’Œæ•´åˆè¯­ä¹‰ç‰¹å¾çš„èƒ½åŠ›ä¸­æ±²å–çµæ„Ÿã€‚</li>
<li>æå‡ºçš„æ–°æ–¹æ³•ï¼šSCAM-Netæ¨¡å‹ï¼ŒåŒ…å«è‡ªé€‚åº”è®°å¿†æ¨¡å—ï¼Œæ¨¡æ‹Ÿäººç±»è®°å¿†å·©å›ºè¿‡ç¨‹ã€‚</li>
<li>æ¨¡å‹ç»“æ„ï¼šæ„å»ºæµ·é©¬ä½“å’Œæ–°çš®å±‚åŒç½‘ç»œï¼Œç”¨äºå·©å›ºæ¯ä¸ªç±»åˆ«çš„ç»“æ„åŒ–è¡¨ç¤ºã€‚è¿™äº›è¡¨ç¤ºè¢«å­˜å‚¨åœ¨é•¿æœŸè®°å¿†ä¸­ï¼Œå¹¶æ ¹æ®ä¼˜åŒ–åŸåˆ™è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚</li>
<li>æ¨¡å‹ä¼˜åŠ¿ï¼šæ¨¡å‹åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-23612b55af067ddb03398ec0fefe0085.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a5fa240169af9d26cfd03e90ca0c0ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e89be017d96fac83d2089d707e7381e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b55dff3a1274c75e6aecea868f13ced.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3acbf586abed77b9bb78960edf4c2081.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc59b493c65ccbe1d46e12353fb98431.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-edfd9104dae1fa74ab66e80bb7c76b8e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SP3D-Boosting-Sparsely-Supervised-3D-Object-Detection-via-Accurate-Cross-Modal-Semantic-Prompts"><a href="#SP3D-Boosting-Sparsely-Supervised-3D-Object-Detection-via-Accurate-Cross-Modal-Semantic-Prompts" class="headerlink" title="SP3D: Boosting Sparsely-Supervised 3D Object Detection via Accurate   Cross-Modal Semantic Prompts"></a>SP3D: Boosting Sparsely-Supervised 3D Object Detection via Accurate   Cross-Modal Semantic Prompts</h2><p><strong>Authors:Shijia Zhao, Qiming Xia, Xusheng Guo, Pufan Zou, Maoji Zheng, Hai Wu, Chenglu Wen, Cheng Wang</strong></p>
<p>Recently, sparsely-supervised 3D object detection has gained great attention, achieving performance close to fully-supervised 3D objectors while requiring only a few annotated instances. Nevertheless, these methods suffer challenges when accurate labels are extremely absent. In this paper, we propose a boosting strategy, termed SP3D, explicitly utilizing the cross-modal semantic prompts generated from Large Multimodal Models (LMMs) to boost the 3D detector with robust feature discrimination capability under sparse annotation settings. Specifically, we first develop a Confident Points Semantic Transfer (CPST) module that generates accurate cross-modal semantic prompts through boundary-constrained center cluster selection. Based on these accurate semantic prompts, which we treat as seed points, we introduce a Dynamic Cluster Pseudo-label Generation (DCPG) module to yield pseudo-supervision signals from the geometry shape of multi-scale neighbor points. Additionally, we design a Distribution Shape score (DS score) that chooses high-quality supervision signals for the initial training of the 3D detector. Experiments on the KITTI dataset and Waymo Open Dataset (WOD) have validated that SP3D can enhance the performance of sparsely supervised detectors by a large margin under meager labeling conditions. Moreover, we verified SP3D in the zero-shot setting, where its performance exceeded that of the state-of-the-art methods. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xmuqimingxia/SP3D">https://github.com/xmuqimingxia/SP3D</a>. </p>
<blockquote>
<p>è¿‘æœŸï¼Œç¨€ç–ç›‘ç£çš„3Dç›®æ ‡æ£€æµ‹å—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œè¿™äº›æ–¹æ³•åœ¨ä»…éœ€å°‘é‡æ ‡æ³¨å®ä¾‹çš„æƒ…å†µä¸‹ï¼Œå°±èƒ½è¾¾åˆ°ä¸å®Œå…¨ç›‘ç£çš„3Dç›®æ ‡æ£€æµ‹å™¨ç›¸è¿‘çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‡†ç¡®æ ‡ç­¾æåº¦ç¼ºå¤±æ—¶ï¼Œè¿™äº›æ–¹æ³•é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§å¢å¼ºç­–ç•¥ï¼Œç§°ä¸ºSP3Dï¼Œå®ƒæ˜¾å¼åœ°åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ç”Ÿæˆçš„è·¨æ¨¡æ€è¯­ä¹‰æç¤ºï¼Œåœ¨ç¨€ç–æ³¨é‡Šè®¾ç½®ä¸‹æé«˜3Dæ£€æµ‹å™¨çš„é²æ£’ç‰¹å¾è¾¨åˆ«èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªç½®ä¿¡ç‚¹è¯­ä¹‰è½¬ç§»ï¼ˆCPSTï¼‰æ¨¡å—ï¼Œé€šè¿‡è¾¹ç•Œçº¦æŸçš„ä¸­å¿ƒèšç±»é€‰æ‹©ç”Ÿæˆå‡†ç¡®çš„è·¨æ¨¡æ€è¯­ä¹‰æç¤ºã€‚åŸºäºè¿™äº›å‡†ç¡®çš„è¯­ä¹‰æç¤ºï¼ˆæˆ‘ä»¬å°†å…¶è§†ä¸ºç§å­ç‚¹ï¼‰ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ¨æ€èšç±»ä¼ªæ ‡ç­¾ç”Ÿæˆï¼ˆDCPGï¼‰æ¨¡å—ï¼Œä»¥ä»å¤šå°ºåº¦é‚»è¿‘ç‚¹çš„å‡ ä½•å½¢çŠ¶ä¸­äº§ç”Ÿä¼ªç›‘ç£ä¿¡å·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåˆ†å¸ƒå½¢çŠ¶å¾—åˆ†ï¼ˆDSå¾—åˆ†ï¼‰ï¼Œä»¥é€‰æ‹©é«˜è´¨é‡çš„ç›‘ç£ä¿¡å·ç”¨äº3Dæ£€æµ‹å™¨çš„åˆå§‹è®­ç»ƒã€‚åœ¨KITTIæ•°æ®é›†å’ŒWaymo Open Datasetï¼ˆWODï¼‰ä¸Šçš„å®éªŒéªŒè¯äº†åœ¨æ ‡æ³¨æ¡ä»¶æœ‰é™çš„æƒ…å†µä¸‹ï¼ŒSP3Då¯ä»¥å¤§å¹…åº¦æé«˜ç¨€ç–ç›‘ç£æ£€æµ‹å™¨çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹éªŒè¯äº†SP3Dï¼Œå…¶æ€§èƒ½è¶…è¿‡äº†æœ€æ–°æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xmuqimingxia/SP3D%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xmuqimingxia/SP3Dæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06467v1">PDF</a> 11 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºSP3Dçš„å¢å¼ºç­–ç•¥ï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆçš„è·¨æ¨¡æ€è¯­ä¹‰æç¤ºï¼Œæå‡ç¨€ç–æ ‡æ³¨ä¸‹3Dæ£€æµ‹å™¨çš„æ€§èƒ½ã€‚é€šè¿‡ç½®ä¿¡ç‚¹è¯­ä¹‰è½¬ç§»æ¨¡å—ç”Ÿæˆå‡†ç¡®è¯­ä¹‰æç¤ºï¼Œå¹¶ä»¥æ­¤ä¸ºåŸºç¡€ï¼Œå¼•å…¥åŠ¨æ€é›†ç¾¤ä¼ªæ ‡ç­¾ç”Ÿæˆæ¨¡å—ï¼Œä»å¤šå°ºåº¦é‚»è¿‘ç‚¹çš„å‡ ä½•å½¢çŠ¶ä¸­äº§ç”Ÿä¼ªç›‘ç£ä¿¡å·ã€‚åœ¨KITTIæ•°æ®é›†å’ŒWaymo Open Datasetä¸Šçš„å®éªŒéªŒè¯äº†SP3Dåœ¨æ ‡æ³¨æ¡ä»¶æœ‰é™çš„æƒ…å†µä¸‹ï¼Œèƒ½æ˜¾è‘—æé«˜ç¨€ç–ç›‘ç£æ£€æµ‹å™¨çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ä¹ŸéªŒè¯äº†å…¶è¶…è¶Šç°æœ‰æ–¹æ³•çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SP3Dç­–ç•¥åˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆçš„è·¨æ¨¡æ€è¯­ä¹‰æç¤ºï¼Œæå‡ç¨€ç–æ ‡æ³¨ä¸‹çš„3Dæ£€æµ‹å™¨æ€§èƒ½ã€‚</li>
<li>CPSTæ¨¡å—é€šè¿‡è¾¹ç•Œçº¦æŸçš„ä¸­å¿ƒèšç±»é€‰æ‹©ç”Ÿæˆå‡†ç¡®çš„è·¨æ¨¡æ€è¯­ä¹‰æç¤ºã€‚</li>
<li>DCPGæ¨¡å—ä»å¤šå°ºåº¦é‚»è¿‘ç‚¹çš„å‡ ä½•å½¢çŠ¶ä¸­äº§ç”Ÿä¼ªç›‘ç£ä¿¡å·ã€‚</li>
<li>DSè¯„åˆ†é€‰æ‹©é«˜è´¨é‡ç›‘ç£ä¿¡å·ç”¨äº3Dæ£€æµ‹å™¨çš„åˆå§‹è®­ç»ƒã€‚</li>
<li>åœ¨KITTIæ•°æ®é›†å’ŒWaymo Open Datasetä¸Šçš„å®éªŒéªŒè¯äº†SP3Dç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>SP3Dç­–ç•¥åœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹æ€§èƒ½è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06467">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-abe374d5828de82c616fcee1701a35f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-99a7f939262725d2bccd89f1c85a992b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b50c27b069f0750fab85f19f903da29d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-37a2666703de95f82138012a6c22b36d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e0e5ec75d4d537baae858bb255ecd80.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="A-Novel-Distributed-PV-Power-Forecasting-Approach-Based-on-Time-LLM"><a href="#A-Novel-Distributed-PV-Power-Forecasting-Approach-Based-on-Time-LLM" class="headerlink" title="A Novel Distributed PV Power Forecasting Approach Based on Time-LLM"></a>A Novel Distributed PV Power Forecasting Approach Based on Time-LLM</h2><p><strong>Authors:Huapeng Lin, Miao Yu</strong></p>
<p>Distributed photovoltaic (DPV) systems are essential for advancing renewable energy applications and achieving energy independence. Accurate DPV power forecasting can optimize power system planning and scheduling while significantly reducing energy loss, thus enhancing overall system efficiency and reliability. However, solar energyâ€™s intermittent nature and DPV systemsâ€™ spatial distribution create significant forecasting challenges. Traditional methods often rely on costly external data, such as numerical weather prediction (NWP) and satellite images, which are difficult to scale for smaller DPV systems. To tackle this issue, this study has introduced an advanced large language model (LLM)-based time series forecasting framework Time-LLM to improve the DPV power forecasting accuracy and generalization ability. By reprogramming, the framework aligns historical power data with natural language modalities, facilitating efficient modeling of time-series data. Then Qwen2.5-3B model is integrated as the backbone LLM to process input data by leveraging its pattern recognition and inference abilities, achieving a balance between efficiency and performance. Finally, by using a flatten and linear projection layer, the LLMâ€™s high-dimensional output is transformed into the final forecasts. Experimental results indicate that Time-LLM outperforms leading recent advanced time series forecasting models, such as Transformer-based methods and MLP-based models, achieving superior accuracy in both short-term and long-term forecasting. Time-LLM also demonstrates exceptional adaptability in few-shot and zero-shot learning scenarios. To the best of the authorsâ€™ knowledge, this study is the first attempt to explore the application of LLMs to DPV power forecasting, which can offer a scalable solution that eliminates reliance on costly external data sources and improve real-world forecasting accuracy. </p>
<blockquote>
<p>åˆ†å¸ƒå¼å…‰ä¼ï¼ˆDPVï¼‰ç³»ç»Ÿå¯¹äºæ¨è¿›å¯å†ç”Ÿèƒ½æºåº”ç”¨å’Œå®ç°èƒ½æºç‹¬ç«‹è‡³å…³é‡è¦ã€‚å‡†ç¡®çš„DPVç”µåŠ›é¢„æµ‹å¯ä»¥ä¼˜åŒ–ç”µåŠ›ç³»ç»Ÿè§„åˆ’å’Œè°ƒåº¦ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘èƒ½æºæŸå¤±ï¼Œä»è€Œæé«˜æ•´ä½“ç³»ç»Ÿæ•ˆç‡å’Œå¯é æ€§ã€‚ç„¶è€Œï¼Œå¤ªé˜³èƒ½çš„é—´æ­‡æ€§å’ŒDPVç³»ç»Ÿçš„ç©ºé—´åˆ†å¸ƒç»™é¢„æµ‹å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•å¾€å¾€ä¾èµ–äºæ˜‚è´µçš„å¤–éƒ¨æ•°æ®ï¼Œå¦‚æ•°å€¼å¤©æ°”é¢„æŠ¥ï¼ˆNWPï¼‰å’Œå«æ˜Ÿå›¾åƒï¼Œè¿™å¯¹äºè¾ƒå°çš„DPVç³»ç»Ÿæ¥è¯´å¾ˆéš¾æ‰©å±•ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºäºæ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶Time-LLMï¼Œä»¥æé«˜DPVç”µåŠ›é¢„æµ‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡é‡æ–°ç¼–ç¨‹ï¼Œè¯¥æ¡†æ¶å°†å†å²ç”µåŠ›æ•°æ®ä¸è‡ªç„¶è¯­è¨€æ¨¡å¼ç›¸ç»“åˆï¼Œä¾¿äºæ—¶é—´åºåˆ—æ•°æ®çš„æœ‰æ•ˆå»ºæ¨¡ã€‚ç„¶åï¼Œé›†æˆQwen2.5-3Bæ¨¡å‹ä½œä¸ºéª¨å¹²LLMæ¥å¤„ç†è¾“å…¥æ•°æ®ï¼Œåˆ©ç”¨å…¶æ¨¡å¼è¯†åˆ«å’Œæ¨ç†èƒ½åŠ›ï¼Œåœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æœ€åï¼Œé€šè¿‡ä½¿ç”¨å¹³é“ºå’Œçº¿æ€§æŠ•å½±å±‚ï¼Œå°†LLMçš„é«˜ç»´è¾“å‡ºè½¬åŒ–ä¸ºæœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTime-LLMåœ¨çŸ­æœŸå’Œé•¿æœŸé¢„æµ‹æ–¹é¢è¡¨ç°å‡ºä¼˜äºå½“å‰å…ˆè¿›çš„æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ï¼Œå¦‚åŸºäºTransformerçš„æ–¹æ³•å’ŒåŸºäºMLPçš„æ¨¡å‹ã€‚Time-LLMè¿˜å±•ç¤ºäº†åœ¨å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­çš„å‡ºè‰²é€‚åº”æ€§ã€‚æ®ä½œè€…æ‰€çŸ¥ï¼Œæœ¬ç ”ç©¶æ˜¯é¦–æ¬¡å°è¯•å°†LLMåº”ç”¨äºDPVç”µåŠ›é¢„æµ‹ï¼Œå®ƒæä¾›äº†ä¸€ç§å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œæ¶ˆé™¤äº†å¯¹æ˜‚è´µå¤–éƒ¨æ•°æ®æºçš„ä¾èµ–ï¼Œæé«˜äº†å®é™…ä¸–ç•Œçš„é¢„æµ‹ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06216v1">PDF</a> 23 pages, 8 figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åˆ†å¸ƒå¼å…‰ä¼ï¼ˆDPVï¼‰ç³»ç»Ÿå¯¹æ¨è¿›å¯å†ç”Ÿèƒ½æºåº”ç”¨å’Œå®ç°èƒ½æºç‹¬ç«‹è‡³å…³é‡è¦ã€‚å‡†ç¡®çš„DPVç”µåŠ›é¢„æµ‹å¯ä»¥ä¼˜åŒ–ç”µåŠ›ç³»ç»Ÿè§„åˆ’å’Œè°ƒåº¦ï¼Œæ˜¾è‘—é™ä½èƒ½æºæŸå¤±ï¼Œä»è€Œæé«˜æ•´ä½“ç³»ç»Ÿæ•ˆç‡å’Œå¯é æ€§ã€‚ç„¶è€Œï¼Œå¤ªé˜³èƒ½èƒ½é‡çš„é—´æ­‡æ€§å’ŒDPVç³»ç»Ÿçš„ç©ºé—´åˆ†å¸ƒç»™é¢„æµ‹å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿæ–¹æ³•å¸¸å¸¸ä¾èµ–æ˜‚è´µçš„å¤–éƒ¨æ•°æ®ï¼Œå¦‚æ•°å€¼å¤©æ°”é¢„æŠ¥ï¼ˆNWPï¼‰å’Œå«æ˜Ÿå›¾åƒï¼Œéš¾ä»¥é€‚ç”¨äºè¾ƒå°çš„DPVç³»ç»Ÿã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶å¼•å…¥å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºäºæ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶Time-LLMï¼Œæé«˜DPVç”µåŠ›é¢„æµ‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡ç¼–ç¨‹ï¼Œè¯¥æ¡†æ¶å°†å†å²ç”µåŠ›æ•°æ®ä¸è‡ªç„¶è¯­è¨€æ¨¡å¼å¯¹é½ï¼Œä¾¿äºæ—¶é—´åºæ•°æ•°æ®çš„æœ‰æ•ˆå»ºæ¨¡ã€‚ç„¶åï¼Œé›†æˆQwen2.5-3Bæ¨¡å‹ä½œä¸ºéª¨å¹²LLMå¤„ç†è¾“å…¥æ•°æ®ï¼Œåˆ©ç”¨å…¶æ¨¡å¼è¯†åˆ«å’Œæ¨ç†èƒ½åŠ›ï¼Œåœ¨æ•ˆç‡å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡ã€‚æœ€åï¼Œé€šè¿‡å¹³å¦åŒ–çº¿æ€§æŠ•å½±å±‚ï¼Œå°†LLMçš„é«˜ç»´è¾“å‡ºè½¬åŒ–ä¸ºæœ€ç»ˆé¢„æµ‹ç»“æœã€‚å®éªŒç»“æœæŒ‡å‡ºï¼ŒTime-LLMè¡¨ç°ä¼˜äºè¿‘æœŸé¢†å…ˆçš„æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ï¼Œå¦‚åŸºäºTransformerçš„æ–¹æ³•å’ŒåŸºäºMLPçš„æ¨¡å‹ï¼Œåœ¨çŸ­æœŸå’Œé•¿æœŸé¢„æµ‹ä¸­éƒ½å®ç°äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚Time-LLMåœ¨å°‘é‡é•œå¤´å’Œé›¶é•œå¤´å­¦ä¹ åœºæ™¯ä¸­è¡¨ç°å‡ºæä½³çš„é€‚åº”æ€§ã€‚æ®ä½œè€…æ‰€çŸ¥ï¼Œæœ¬ç ”ç©¶æ˜¯é¦–æ¬¡æ¢ç´¢å°†LLMåº”ç”¨äºDPVç”µåŠ›é¢„æµ‹ï¼Œå¯æä¾›ä¸€ä¸ªå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œæ¶ˆé™¤å¯¹æ˜‚è´µå¤–éƒ¨æ•°æ®æºçš„ä¾èµ–ï¼Œæé«˜ç°å®ä¸–ç•Œçš„é¢„æµ‹å‡†ç¡®æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ†å¸ƒå¼å…‰ä¼ï¼ˆDPVï¼‰ç³»ç»Ÿåœ¨æ¨è¿›å¯å†ç”Ÿèƒ½æºåº”ç”¨å’Œå®ç°èƒ½æºç‹¬ç«‹æ–¹é¢èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>å‡†ç¡®çš„DPVç”µåŠ›é¢„æµ‹å¯¹äºä¼˜åŒ–ç”µåŠ›ç³»ç»Ÿè§„åˆ’å’Œè°ƒåº¦ã€é™ä½èƒ½æºæŸå¤±è‡³å…³é‡è¦ã€‚</li>
<li>å¤ªé˜³èƒ½çš„é—´æ­‡æ€§å’ŒDPVç³»ç»Ÿçš„ç©ºé—´åˆ†å¸ƒç»™é¢„æµ‹å¸¦æ¥äº†æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿé¢„æµ‹æ–¹æ³•å¸¸å¸¸ä¾èµ–æ˜‚è´µçš„å¤–éƒ¨æ•°æ®ï¼Œéš¾ä»¥é€‚ç”¨äºæ‰€æœ‰åœºæ™¯ã€‚</li>
<li>æœ¬ç ”ç©¶å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰Time-LLMæ¡†æ¶ï¼Œæé«˜DPVç”µåŠ›é¢„æµ‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Time-LLMæ¡†æ¶é€šè¿‡ç»“åˆå†å²ç”µåŠ›æ•°æ®å’Œè‡ªç„¶è¯­è¨€æ¨¡å¼è¿›è¡Œæœ‰æ•ˆå»ºæ¨¡ã€‚</li>
<li>Time-LLMåœ¨çŸ­æœŸå’Œé•¿æœŸé¢„æµ‹ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”é€‚åº”äºä¸åŒå­¦ä¹ åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06216">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d0aee475ed60621af054afbe45ef8c5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71e1d6fcca3fe96c3962dfa382afb7d6.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Vision-aware-Multimodal-Prompt-Tuning-for-Uploadable-Multi-source-Few-shot-Domain-Adaptation"><a href="#Vision-aware-Multimodal-Prompt-Tuning-for-Uploadable-Multi-source-Few-shot-Domain-Adaptation" class="headerlink" title="Vision-aware Multimodal Prompt Tuning for Uploadable Multi-source   Few-shot Domain Adaptation"></a>Vision-aware Multimodal Prompt Tuning for Uploadable Multi-source   Few-shot Domain Adaptation</h2><p><strong>Authors:Kuanghong Liu, Jin Wang, Kangjian He, Dan Xu, Xuejie Zhang</strong></p>
<p>Conventional multi-source domain few-shot adaptation (MFDA) faces the challenge of further reducing the load on edge-side devices in low-resource scenarios. Considering the native language-supervised advantage of CLIP and the plug-and-play nature of prompt to transfer CLIP efficiently, this paper introduces an uploadable multi-source few-shot domain adaptation (UMFDA) schema. It belongs to a decentralized edge collaborative learning in the edge-side models that must maintain a low computational load. And only a limited amount of annotations in source domain data is provided, with most of the data being unannotated. Further, this paper proposes a vision-aware multimodal prompt tuning framework (VAMP) under the decentralized schema, where the vision-aware prompt guides the text domain-specific prompt to maintain semantic discriminability and perceive the domain information. The cross-modal semantic and domain distribution alignment losses optimize each edge-side model, while text classifier consistency and semantic diversity losses promote collaborative learning among edge-side models. Extensive experiments were conducted on OfficeHome and DomainNet datasets to demonstrate the effectiveness of the proposed VAMP in the UMFDA, which outperformed the previous prompt tuning methods. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„å¤šæºåŸŸå°æ ·æœ¬é€‚åº”ï¼ˆMFDAï¼‰é¢ä¸´ç€åœ¨èµ„æºæœ‰é™çš„åœºæ™¯ä¸­è¿›ä¸€æ­¥å‡å°‘è¾¹ç¼˜è®¾å¤‡è´Ÿè½½çš„æŒ‘æˆ˜ã€‚è€ƒè™‘åˆ°CLIPçš„æœ¬åœ°è¯­è¨€ç›‘ç£ä¼˜åŠ¿å’Œpromptçš„å³æ’å³ç”¨ç‰¹æ€§ä»¥é«˜æ•ˆè½¬ç§»CLIPï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§å¯ä¸Šä¼ çš„å¤šæºå°æ ·æœ¬åŸŸé€‚åº”ï¼ˆUMFDAï¼‰æ¶æ„ã€‚å®ƒå±äºè¾¹ç¼˜ä¾§æ¨¡å‹ä¸­å¿…é¡»ä¿æŒä½è®¡ç®—è´Ÿè½½çš„åˆ†å¸ƒå¼è¾¹ç¼˜åä½œå­¦ä¹ ã€‚æºåŸŸæ•°æ®åªæä¾›æœ‰é™é‡çš„æ³¨é‡Šï¼Œè€Œå¤§éƒ¨åˆ†æ•°æ®éƒ½æ˜¯æœªæ³¨é‡Šçš„ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡åœ¨åˆ†å¸ƒå¼æ¶æ„ä¸‹æå‡ºäº†ä¸€ä¸ªè§†è§‰æ„ŸçŸ¥çš„å¤šæ¨¡æ€æç¤ºè°ƒæ•´æ¡†æ¶ï¼ˆVAMPï¼‰ï¼Œå…¶ä¸­è§†è§‰æ„ŸçŸ¥æç¤ºå¼•å¯¼æ–‡æœ¬åŸŸç‰¹å®šæç¤ºä»¥ä¿æŒè¯­ä¹‰åŒºåˆ†å¹¶æ„ŸçŸ¥åŸŸä¿¡æ¯ã€‚è·¨æ¨¡æ€è¯­ä¹‰å’ŒåŸŸåˆ†å¸ƒå¯¹é½æŸå¤±ä¼˜åŒ–äº†æ¯ä¸ªè¾¹ç¼˜ä¾§æ¨¡å‹ï¼Œè€Œæ–‡æœ¬åˆ†ç±»å™¨çš„ä¸€è‡´æ€§å’Œè¯­ä¹‰å¤šæ ·æ€§æŸå¤±ä¿ƒè¿›äº†è¾¹ç¼˜ä¾§æ¨¡å‹ä¹‹é—´çš„åä½œå­¦ä¹ ã€‚åœ¨OfficeHomeå’ŒDomainNetæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œè¯æ˜äº†åœ¨UMFDAä¸­æå‡ºçš„VAMPçš„æœ‰æ•ˆæ€§ï¼Œå…¶æ€§èƒ½ä¼˜äºä»¥å‰çš„æç¤ºè°ƒæ•´æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06106v1">PDF</a> Accepted by AAAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é€‚ç”¨äºè¾¹ç¼˜è®¾å¤‡çš„ä¸Šä¼ å¼å¤šæºå°‘é‡æ•°æ®åŸŸè‡ªé€‚åº”æ¨¡å‹ï¼ˆUMFDAï¼‰ï¼Œæ—¨åœ¨é™ä½ä½èµ„æºåœºæ™¯ä¸‹è¾¹ç¼˜è®¾å¤‡çš„è®¡ç®—è´Ÿè½½ã€‚åˆ©ç”¨CLIPçš„è‡ªç„¶è¯­è¨€ç›‘ç£ä¼˜åŠ¿å’Œpromptçš„æ’æ‹”ç‰¹æ€§ï¼Œé€šè¿‡å¼•å…¥è§†è§‰æ„ŸçŸ¥çš„å¤šæ¨¡æ€æç¤ºè°ƒæ•´æ¡†æ¶ï¼ˆVAMPï¼‰ï¼Œä¼˜åŒ–äº†è¾¹ç¼˜ä¾§æ¨¡å‹çš„æ€§èƒ½ã€‚åœ¨OfficeHomeå’ŒDomainNetæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒVAMPåœ¨UMFDAä¸­çš„è¡¨ç°ä¼˜äºå…ˆå‰çš„æç¤ºè°ƒæ•´æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UMFDAæ¨¡å‹æ—¨åœ¨é™ä½è¾¹ç¼˜è®¾å¤‡åœ¨ä½èµ„æºåœºæ™¯ä¸‹çš„è®¡ç®—è´Ÿè½½ã€‚</li>
<li>CLIPçš„è‡ªç„¶è¯­è¨€ç›‘ç£ä¼˜åŠ¿å’Œpromptçš„æ’æ‹”ç‰¹æ€§è¢«ç”¨äºå¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥è§†è§‰æ„ŸçŸ¥çš„å¤šæ¨¡æ€æç¤ºè°ƒæ•´æ¡†æ¶ï¼ˆVAMPï¼‰ï¼Œä»¥æé«˜è¾¹ç¼˜ä¾§æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>VAMPæ¡†æ¶åŒ…æ‹¬é€šè¿‡è§†è§‰æ„ŸçŸ¥æç¤ºå¼•å¯¼æ–‡æœ¬ç‰¹å®šæç¤ºä»¥ç»´æŒè¯­ä¹‰è¾¨åˆ«èƒ½åŠ›å¹¶æ„ŸçŸ¥åŸŸä¿¡æ¯ã€‚</li>
<li>é€šè¿‡è·¨æ¨¡æ€è¯­ä¹‰å’ŒåŸŸåˆ†å¸ƒå¯¹é½æŸå¤±ä¼˜åŒ–æ¯ä¸ªè¾¹ç¼˜ä¾§æ¨¡å‹ã€‚</li>
<li>é€šè¿‡æ–‡æœ¬åˆ†ç±»å™¨çš„ä¸€è‡´æ€§å’Œè¯­ä¹‰å¤šæ ·æ€§æŸå¤±ä¿ƒè¿›è¾¹ç¼˜ä¾§æ¨¡å‹é—´çš„åä½œå­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06106">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-155124066ed8026932c137c90f66a1ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e83eb0c84a5ed8dee4712591a53fa490.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f3181a7e5ad0af54a03f10ffe7b28fe9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c7df4bb3d30542bdd04b333c4e374cf.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Bridging-Molecular-Graphs-and-Large-Language-Models"><a href="#Bridging-Molecular-Graphs-and-Large-Language-Models" class="headerlink" title="Bridging Molecular Graphs and Large Language Models"></a>Bridging Molecular Graphs and Large Language Models</h2><p><strong>Authors:Runze Wang, Mingqi Yang, Yanming Shen</strong></p>
<p>While Large Language Models (LLMs) have shown exceptional generalization capabilities, their ability to process graph data, such as molecular structures, remains limited. To bridge this gap, this paper proposes Graph2Token, an efficient solution that aligns graph tokens to LLM tokens. The key idea is to represent a graph token with the LLM token vocabulary, without fine-tuning the LLM backbone. To achieve this goal, we first construct a molecule-text paired dataset from multisources, including CHEBI and HMDB, to train a graph structure encoder, which reduces the distance between graphs and texts representations in the feature space. Then, we propose a novel alignment strategy that associates a graph token with LLM tokens. To further unleash the potential of LLMs, we collect molecular IUPAC name identifiers, which are incorporated into the LLM prompts. By aligning molecular graphs as special tokens, we can activate LLM generalization ability to molecular few-shot learning. Extensive experiments on molecular classification and regression tasks demonstrate the effectiveness of our proposed Graph2Token. </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»è¡¨ç°å‡ºå“è¶Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†å¦‚å›¾æ•°æ®ï¼ˆå¦‚åˆ†å­ç»“æ„ï¼‰æ–¹é¢çš„èƒ½åŠ›ä»ç„¶æœ‰é™ã€‚ä¸ºäº†å¼¥è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†Graph2Tokenï¼Œä¸€ç§èƒ½å¤Ÿå°†å›¾æ ‡è®°ä¸LLMæ ‡è®°å¯¹é½çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚å…³é”®æ€æƒ³æ˜¯ä½¿ç”¨LLMæ ‡è®°è¯æ±‡æ¥è¡¨ç¤ºå›¾æ ‡è®°ï¼Œè€Œæ— éœ€å¾®è°ƒLLMçš„ä¸»å¹²ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆä»CHEBIå’ŒHMDBç­‰å¤šæºæ„å»ºåˆ†å­æ–‡æœ¬é…å¯¹æ•°æ®é›†ï¼Œä»¥è®­ç»ƒå›¾ç»“æ„ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨ç¼©å°äº†å›¾ä¸æ–‡æœ¬è¡¨ç¤ºåœ¨ç‰¹å¾ç©ºé—´ä¸­çš„è·ç¦»ã€‚æ¥ç€ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¯¹é½ç­–ç•¥ï¼Œå°†å›¾æ ‡è®°ä¸LLMæ ‡è®°å…³è”èµ·æ¥ã€‚ä¸ºäº†è¿›ä¸€æ­¥é‡Šæ”¾LLMçš„æ½œåŠ›ï¼Œæˆ‘ä»¬æ”¶é›†äº†åˆ†å­IUPACåç§°æ ‡è¯†ç¬¦ï¼Œå¹¶å°†å…¶çº³å…¥LLMæç¤ºä¸­ã€‚é€šè¿‡å¯¹åˆ†å­å›¾è¿›è¡Œç‰¹æ®Šæ ‡è®°å¯¹é½ï¼Œæˆ‘ä»¬å¯ä»¥æ¿€æ´»LLMåœ¨åˆ†å­å°‘æ ·æœ¬å­¦ä¹ ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨åˆ†å­åˆ†ç±»å’Œå›å½’ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„Graph2Tokenæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03135v2">PDF</a> AAAI 2025 camera ready version</p>
<p><strong>Summary</strong></p>
<p>Graph2Tokenè®ºæ–‡æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†å›¾å½¢æ•°æ®ï¼ˆå¦‚åˆ†å­ç»“æ„ï¼‰èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå°†å›¾å½¢ä»¤ç‰Œä¸LLMä»¤ç‰Œå¯¹é½çš„æœ‰æ•ˆè§£å†³æ–¹æ¡ˆï¼Œå¯åœ¨ä¸éœ€è¦å¾®è°ƒLLMä¸»å¹²çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨LLMå¤„ç†å›¾å½¢æ•°æ®ã€‚è¯¥ç ”ç©¶é€šè¿‡æ„å»ºåˆ†å­æ–‡æœ¬é…å¯¹æ•°æ®é›†å’Œæå‡ºæ–°çš„å¯¹é½ç­–ç•¥æ¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Œå¹¶å°†åˆ†å­å›¾çš„IUPACåç§°æ ‡è¯†ç¬¦çº³å…¥LLMæç¤ºä¸­ï¼Œä»è€Œæ¿€æ´»LLMå¯¹åˆ†å­æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¯æ˜Graph2Tokenåœ¨åˆ†å­åˆ†ç±»å’Œå›å½’ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Graph2Tokenè§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†å›¾å½¢æ•°æ®ï¼ˆå¦‚åˆ†å­ç»“æ„ï¼‰çš„å±€é™æ€§ã€‚</li>
<li>é€šè¿‡æ„å»ºåˆ†å­æ–‡æœ¬é…å¯¹æ•°æ®é›†ï¼Œå®ç°äº†å›¾å½¢ä»¤ç‰Œä¸LLMä»¤ç‰Œçš„å¯¹æ¥ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¯¹é½ç­–ç•¥ï¼Œå°†å›¾å½¢ä»¤ç‰Œä¸LLMä»¤ç‰Œå…³è”èµ·æ¥ã€‚</li>
<li>åˆ©ç”¨åˆ†å­IUPACåç§°æ ‡è¯†ç¬¦ï¼Œå°†åˆ†å­å›¾ä½œä¸ºç‰¹æ®Šä»¤ç‰Œçº³å…¥LLMæç¤ºä¸­ã€‚</li>
<li>Graph2Tokenæ¿€æ´»äº†LLMå¯¹åˆ†å­æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜Graph2Tokenåœ¨åˆ†å­åˆ†ç±»å’Œå›å½’ä»»åŠ¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03135">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bd807147b0912573885ebde6c7cc4dee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-268fdc7fc745de10ba8d3912525e9d26.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7086f7ff86995d555fedcfc5831552dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4037ee6ef32338912d281de1935b41b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Learning-to-Animate-Images-from-A-Few-Videos-to-Portray-Delicate-Human-Actions"><a href="#Learning-to-Animate-Images-from-A-Few-Videos-to-Portray-Delicate-Human-Actions" class="headerlink" title="Learning to Animate Images from A Few Videos to Portray Delicate Human   Actions"></a>Learning to Animate Images from A Few Videos to Portray Delicate Human   Actions</h2><p><strong>Authors:Haoxin Li, Yingchen Yu, Qilong Wu, Hanwang Zhang, Song Bai, Boyang Li</strong></p>
<p>Despite recent progress, video generative models still struggle to animate static images into videos that portray delicate human actions, particularly when handling uncommon or novel actions whose training data are limited. In this paper, we explore the task of learning to animate images to portray delicate human actions using a small number of videos â€“ 16 or fewer â€“ which is highly valuable for real-world applications like video and movie production. Learning generalizable motion patterns that smoothly transition from user-provided reference images in a few-shot setting is highly challenging. We propose FLASH (Few-shot Learning to Animate and Steer Humans), which learns generalizable motion patterns by forcing the model to reconstruct a video using the motion features and cross-frame correspondences of another video with the same motion but different appearance. This encourages transferable motion learning and mitigates overfitting to limited training data. Additionally, FLASH extends the decoder with additional layers to propagate details from the reference image to generated frames, improving transition smoothness. Human judges overwhelmingly favor FLASH, with 65.78% of 488 responses prefer FLASH over baselines. We strongly recommend watching the videos in the website: <a target="_blank" rel="noopener" href="https://lihaoxin05.github.io/human_action_animation/">https://lihaoxin05.github.io/human_action_animation/</a>, as motion artifacts are hard to notice from images. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘çš„è¿›å±•æ˜¾è‘—ï¼Œè§†é¢‘ç”Ÿæˆæ¨¡å‹ä»ç„¶éš¾ä»¥å°†é™æ€å›¾åƒè½¬åŒ–ä¸ºæç»˜ç²¾ç»†äººç±»åŠ¨ä½œçš„è§†é¢‘ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†è®­ç»ƒæ•°æ®æœ‰é™çš„ä¸å¸¸è§æˆ–æ–°åŠ¨ä½œæ—¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä½¿ç”¨å°‘é‡è§†é¢‘ï¼ˆ16ä¸ªæˆ–æ›´å°‘ï¼‰å­¦ä¹ ä½¿å›¾åƒåŠ¨ç”»åŒ–ä»¥æç»˜ç²¾ç»†äººç±»åŠ¨ä½œçš„ä»»åŠ¡ï¼Œè¿™å¯¹äºè§†é¢‘å’Œç”µå½±åˆ¶ä½œç­‰å®é™…åº”ç”¨æ¥è¯´éå¸¸æœ‰ä»·å€¼ã€‚åœ¨å°‘æ•°é•œå¤´ä¸‹ï¼Œä»ç”¨æˆ·æä¾›çš„å‚è€ƒå›¾åƒå­¦ä¹ å¯æ¨å¹¿çš„è¿åŠ¨æ¨¡å¼å…·æœ‰å¾ˆå¤§æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬æå‡ºäº†FLASHï¼ˆFew-shot Learning to Animate and Steer Humansï¼‰ï¼Œå®ƒé€šè¿‡å¼ºåˆ¶æ¨¡å‹ä½¿ç”¨å…·æœ‰ç›¸åŒè¿åŠ¨ä½†ä¸åŒå¤–è§‚çš„å¦ä¸€ä¸ªè§†é¢‘çš„è¿åŠ¨ç‰¹å¾å’Œè·¨å¸§å¯¹åº”å…³ç³»æ¥é‡å»ºè§†é¢‘ï¼Œå­¦ä¹ å¯æ¨å¹¿çš„è¿åŠ¨æ¨¡å¼ã€‚è¿™é¼“åŠ±äº†å¯è¿ç§»çš„è¿åŠ¨å­¦ä¹ ï¼Œå¹¶å‡è½»äº†è¿‡åº¦ä¾èµ–æœ‰é™è®­ç»ƒæ•°æ®çš„é—®é¢˜ã€‚æ­¤å¤–ï¼ŒFLASHæ‰©å±•äº†è§£ç å™¨ï¼Œå¢åŠ äº†é¢å¤–çš„å±‚ï¼Œä»¥å°†ç»†èŠ‚ä»å‚è€ƒå›¾åƒä¼ æ’­åˆ°ç”Ÿæˆçš„å¸§ï¼Œæé«˜äº†è¿‡æ¸¡çš„å¹³æ»‘åº¦ã€‚äººç±»è¯„å§”å‹å€’æ€§åœ°é’çFLASHï¼Œåœ¨488ä¸ªå›åº”ä¸­ï¼Œæœ‰65.78%çš„äººè®¤ä¸ºFLASHä¼˜äºåŸºçº¿ã€‚æˆ‘ä»¬å¼ºçƒˆæ¨èåœ¨ç½‘ç«™ä¸Šè§‚çœ‹è§†é¢‘ï¼š<a target="_blank" rel="noopener" href="https://lihaoxin05.github.io/human_action_animation/%EF%BC%8C%E5%9B%A0%E4%B8%BA%E8%BF%90%E5%8A%A8%E4%BC%AA%E7%BB%AA%E6%98%AF%E9%9A%BE%E4%BB%A5%E4%BB%8E%E5%9C%B0%E5%9B%BE%E4%B8%AD%E7%BE%A4%%E3%80%82">https://lihaoxin05.github.io/human_action_animation&#x2F;ï¼Œå› ä¸ºè¿åŠ¨ä¼ªå½±å¾ˆéš¾ä»å›¾åƒä¸­å¯Ÿè§‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00276v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå°‘é‡è§†é¢‘å­¦ä¹ é©±åŠ¨å›¾åƒåŠ¨ç”»ä»¥è¡¨ç°ç²¾ç»†äººç±»åŠ¨ä½œçš„ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºFLASHçš„æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ é€šç”¨è¿åŠ¨æ¨¡å¼å¹¶å€Ÿé‰´å…¶ä»–è§†é¢‘çš„è¿åŠ¨ç‰¹å¾å’Œè·¨å¸§å¯¹åº”å…³ç³»ï¼Œå®ç°ç²¾ç»†äººç±»åŠ¨ä½œçš„åŠ¨ç”»ç”Ÿæˆã€‚FLASHé€šè¿‡æ‰©å±•è§£ç å™¨ï¼Œå°†å‚è€ƒå›¾åƒçš„ç»†èŠ‚ä¼ æ’­åˆ°ç”Ÿæˆçš„å¸§ä¸­ï¼Œæé«˜äº†è¿‡æ¸¡çš„å¹³æ»‘æ€§ã€‚ç»è¿‡å®éªŒéªŒè¯ï¼ŒFLASHåœ¨äººç±»è¯„ä»·ä¸­å–å¾—äº†æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æ¢è®¨äº†å¦‚ä½•åœ¨æœ‰é™è§†é¢‘æ•°æ®ï¼ˆä»…16ä¸ªæˆ–æ›´å°‘ï¼‰ä¸­é©±åŠ¨å›¾åƒåŠ¨ç”»ä»¥å±•ç¤ºç²¾ç»†çš„äººç±»åŠ¨ä½œã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºFLASHçš„æ–¹æ³•ï¼Œé€šè¿‡å€Ÿé‰´å…¶ä»–è§†é¢‘çš„è¿åŠ¨ç‰¹å¾å’Œè·¨å¸§å¯¹åº”å…³ç³»ï¼Œå­¦ä¹ é€šç”¨è¿åŠ¨æ¨¡å¼ã€‚</li>
<li>FLASHæ–¹æ³•é€šè¿‡å­¦ä¹ ä»å‚è€ƒå›¾åƒä¼ æ’­ç»†èŠ‚åˆ°ç”Ÿæˆçš„å¸§ï¼Œæé«˜äº†è¿‡æ¸¡çš„å¹³æ»‘æ€§ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFLASHåœ¨äººç±»è¯„ä»·ä¸­å–å¾—äº†æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¤§å¤šæ•°äººç±»åˆ¤æ–­è€…å€¾å‘äºé€‰æ‹©FLASHç”Ÿæˆçš„åŠ¨ç”»ã€‚</li>
<li>FLASHæ–¹æ³•å¯¹äºçœŸå®ä¸–ç•Œåº”ç”¨ï¼ˆå¦‚è§†é¢‘å’Œç”µå½±åˆ¶ä½œï¼‰å…·æœ‰å¾ˆé«˜çš„ä»·å€¼ã€‚</li>
<li>è¯¥è®ºæ–‡å¼ºè°ƒè§‚çœ‹è§†é¢‘çš„é‡è¦æ€§ï¼Œå› ä¸ºè¿åŠ¨ä¼ªå½±åœ¨å›¾åƒä¸­å¾ˆéš¾å¯Ÿè§‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00276">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cdc2f206a640d2fa63990c9e62216372.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3fac7e858ed269e7415df96877b3002c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4197c7066486b5294f966fafdb196886.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d5b4520db3cfd039f26b4305aea0909.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Continuous-Knowledge-Preserving-Decomposition-for-Few-Shot-Continual-Learning"><a href="#Continuous-Knowledge-Preserving-Decomposition-for-Few-Shot-Continual-Learning" class="headerlink" title="Continuous Knowledge-Preserving Decomposition for Few-Shot Continual   Learning"></a>Continuous Knowledge-Preserving Decomposition for Few-Shot Continual   Learning</h2><p><strong>Authors:Xiaojie Li, Yibo Yang, Jianlong Wu, Jie Liu, Yue Yu, Liqiang Nie, Min Zhang</strong></p>
<p>Few-shot class-incremental learning (FSCIL) involves learning new classes from limited data while retaining prior knowledge, and often results in catastrophic forgetting. Existing methods either freeze backbone networks to preserve knowledge, which limits adaptability, or rely on additional modules or prompts, introducing inference overhead. To this end, we propose Continuous Knowledge-Preserving Decomposition for FSCIL (CKPD-FSCIL), a framework that decomposes a modelâ€™s weights into two parts: one that compacts existing knowledge (knowledge-sensitive components) and another that carries redundant capacity to accommodate new abilities (redundant-capacity components). The decomposition is guided by a covariance matrix from replay samples, ensuring principal components align with classification abilities. During adaptation, we freeze the knowledge-sensitive components and only adapt the redundant-capacity components, fostering plasticity while minimizing interference without changing the architecture or increasing overhead. Additionally, CKPD introduces an adaptive layer selection strategy to identify layers with redundant capacity, dynamically allocating adapters. Experiments on multiple benchmarks show that CKPD-FSCIL outperforms state-of-the-art methods. </p>
<blockquote>
<p>å°‘é‡æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ¶‰åŠä»æœ‰é™æ•°æ®ä¸­å­¦ä¹ æ–°ç±»åˆ«åŒæ—¶ä¿ç•™å…ˆå‰çŸ¥è¯†ï¼Œè¿™å¸¸å¸¸å¯¼è‡´ç¾éš¾æ€§é—å¿˜ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆå†»ç»“ä¸»å¹²ç½‘ç»œä»¥ä¿ç•™çŸ¥è¯†ï¼Œè¿™é™åˆ¶äº†é€‚åº”æ€§ï¼Œè¦ä¹ˆä¾èµ–é™„åŠ æ¨¡å—æˆ–æç¤ºï¼Œå¼•å…¥æ¨ç†å¼€é”€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘FSCILçš„æŒç»­çŸ¥è¯†ä¿ç•™åˆ†è§£ï¼ˆCKPD-FSCILï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å°†æ¨¡å‹æƒé‡åˆ†è§£ä¸ºä¸¤éƒ¨åˆ†çš„æ¡†æ¶ï¼šä¸€éƒ¨åˆ†æ˜¯å‹ç¼©ç°æœ‰çŸ¥è¯†ï¼ˆçŸ¥è¯†æ•æ„Ÿç»„ä»¶ï¼‰ï¼Œå¦ä¸€éƒ¨åˆ†æ˜¯å®¹çº³æ–°èƒ½åŠ›æ‰€éœ€çš„å†—ä½™å®¹é‡ï¼ˆå†—ä½™å®¹é‡ç»„ä»¶ï¼‰ã€‚åˆ†è§£å—å›æ”¾æ ·æœ¬çš„åæ–¹å·®çŸ©é˜µæŒ‡å¯¼ï¼Œç¡®ä¿ä¸»æˆåˆ†ä¸åˆ†ç±»èƒ½åŠ›å¯¹é½ã€‚åœ¨é€‚åº”è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å†»ç»“çŸ¥è¯†æ•æ„Ÿç»„ä»¶ï¼Œåªé€‚åº”å†—ä½™å®¹é‡ç»„ä»¶ï¼Œä¿ƒè¿›å¯å¡‘æ€§ï¼ŒåŒæ—¶æœ€å°åŒ–å¹²æ‰°ï¼Œè€Œæ— éœ€æ›´æ”¹æ¶æ„æˆ–å¢åŠ å¼€é”€ã€‚æ­¤å¤–ï¼ŒCKPDå¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”å±‚é€‰æ‹©ç­–ç•¥ï¼Œä»¥è¯†åˆ«å…·æœ‰å†—ä½™å®¹é‡çš„å±‚ï¼Œå¹¶åŠ¨æ€åˆ†é…é€‚é…å™¨ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCKPD-FSCILä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.05017v2">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/xiaojieli0903/CKPD-FSCIL">https://github.com/xiaojieli0903/CKPD-FSCIL</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœ‰é™æ•°æ®å®ç°ç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰çš„è¿‡ç¨‹ä¸­ï¼Œä¿ç•™å…ˆå‰çŸ¥è¯†å¹¶é¿å…ç¾éš¾æ€§é—å¿˜æ˜¯å…³é”®ã€‚ç°æœ‰æ–¹æ³•è¦ä¹ˆå†»ç»“ä¸»å¹²ç½‘ç»œä»¥ä¿ç•™çŸ¥è¯†ï¼Œé™åˆ¶äº†é€‚åº”æ€§ï¼Œè¦ä¹ˆä¾èµ–é¢å¤–çš„æ¨¡å—æˆ–æç¤ºï¼Œå¢åŠ äº†æ¨ç†å¼€é”€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†é¢å‘FSCILçš„æŒç»­çŸ¥è¯†ä¿ç•™åˆ†è§£ï¼ˆCKPD-FSCILï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†æ¨¡å‹çš„æƒé‡åˆ†è§£ä¸ºä¸¤éƒ¨åˆ†ï¼šä¸€éƒ¨åˆ†æ˜¯å‹ç¼©ç°æœ‰çŸ¥è¯†ï¼ˆçŸ¥è¯†æ•æ„Ÿç»„ä»¶ï¼‰ï¼Œå¦ä¸€éƒ¨åˆ†æ˜¯å®¹çº³æ–°èƒ½åŠ›ï¼ˆå†—ä½™å®¹é‡ç»„ä»¶ï¼‰ã€‚åˆ†è§£è¿‡ç¨‹ç”±å›æ”¾æ ·æœ¬çš„åæ–¹å·®çŸ©é˜µå¼•å¯¼ï¼Œç¡®ä¿ä¸»æˆåˆ†ä¸åˆ†ç±»èƒ½åŠ›å¯¹é½ã€‚åœ¨é€‚åº”è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å†»ç»“çŸ¥è¯†æ•æ„Ÿç»„ä»¶ï¼Œåªé€‚åº”å†—ä½™å®¹é‡ç»„ä»¶ï¼Œä»¥ä¿ƒè¿›å¯å¡‘æ€§å¹¶æœ€å°åŒ–å¹²æ‰°ï¼ŒåŒæ—¶ä¸æ”¹å˜æ¶æ„æˆ–å¢åŠ å¼€é”€ã€‚æ­¤å¤–ï¼ŒCKPDè¿˜å¼•å…¥äº†ä¸€ç§è‡ªé€‚åº”å±‚é€‰æ‹©ç­–ç•¥ï¼Œä»¥è¯†åˆ«å…·æœ‰å†—ä½™å®¹é‡çš„å±‚ï¼Œå¹¶åŠ¨æ€åˆ†é…é€‚é…å™¨ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCKPD-FSCILä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-shot class-incremental learning (FSCIL) æ¶‰åŠä»æ–°æ•°æ®ä¸­å­¦ä¹ å¹¶ä¿ç•™å…ˆå‰çŸ¥è¯†çš„é—®é¢˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¯èƒ½é¢ä¸´ç¾éš¾æ€§é—å¿˜çš„é—®é¢˜ï¼Œè¦ä¹ˆé™åˆ¶é€‚åº”æ€§è¦ä¹ˆå¢åŠ æ¨ç†å¼€é”€ã€‚</li>
<li>CKPD-FSCILæ¡†æ¶é€šè¿‡å°†æ¨¡å‹æƒé‡åˆ†è§£ä¸ºçŸ¥è¯†æ•æ„Ÿç»„ä»¶å’Œå†—ä½™å®¹é‡ç»„ä»¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>çŸ¥è¯†æ•æ„Ÿç»„ä»¶ç”¨äºä¿ç•™ç°æœ‰çŸ¥è¯†ï¼Œå†—ä½™å®¹é‡ç»„ä»¶ç”¨äºé€‚åº”æ–°èƒ½åŠ›ã€‚</li>
<li>CKPDä½¿ç”¨å›æ”¾æ ·æœ¬çš„åæ–¹å·®çŸ©é˜µæ¥å¼•å¯¼åˆ†è§£è¿‡ç¨‹å¹¶ç¡®ä¿åˆ†ç±»èƒ½åŠ›çš„å¯¹é½ã€‚</li>
<li>CKPDé€šè¿‡å†»ç»“çŸ¥è¯†æ•æ„Ÿç»„ä»¶å¹¶ä»…é€‚åº”å†—ä½™å®¹é‡ç»„ä»¶æ¥ä¼˜åŒ–å­¦ä¹ è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.05017">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8d7bfaa7352d96e2034a3b0d851ac896.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ce94058a3b924a3cb209bbcc89b5125.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a86ed531d8fa73887ce859587e1a5839.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="UniVAD-A-Training-free-Unified-Model-for-Few-shot-Visual-Anomaly-Detection"><a href="#UniVAD-A-Training-free-Unified-Model-for-Few-shot-Visual-Anomaly-Detection" class="headerlink" title="UniVAD: A Training-free Unified Model for Few-shot Visual Anomaly   Detection"></a>UniVAD: A Training-free Unified Model for Few-shot Visual Anomaly   Detection</h2><p><strong>Authors:Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, Jinqiao Wang</strong></p>
<p>Visual Anomaly Detection (VAD) aims to identify abnormal samples in images that deviate from normal patterns, covering multiple domains, including industrial, logical, and medical fields. Due to the domain gaps between these fields, existing VAD methods are typically tailored to each domain, with specialized detection techniques and model architectures that are difficult to generalize across different domains. Moreover, even within the same domain, current VAD approaches often follow a â€œone-category-one-modelâ€ paradigm, requiring large amounts of normal samples to train class-specific models, resulting in poor generalizability and hindering unified evaluation across domains. To address this issue, we propose a generalized few-shot VAD method, UniVAD, capable of detecting anomalies across various domains, such as industrial, logical, and medical anomalies, with a training-free unified model. UniVAD only needs few normal samples as references during testing to detect anomalies in previously unseen objects, without training on the specific domain. Specifically, UniVAD employs a Contextual Component Clustering ($C^3$) module based on clustering and vision foundation models to segment components within the image accurately, and leverages Component-Aware Patch Matching (CAPM) and Graph-Enhanced Component Modeling (GECM) modules to detect anomalies at different semantic levels, which are aggregated to produce the final detection result. We conduct experiments on nine datasets spanning industrial, logical, and medical fields, and the results demonstrate that UniVAD achieves state-of-the-art performance in few-shot anomaly detection tasks across multiple domains, outperforming domain-specific anomaly detection models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/FantasticGNU/UniVAD">https://github.com/FantasticGNU/UniVAD</a>. </p>
<blockquote>
<p>è§†è§‰å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰æ—¨åœ¨è¯†åˆ«å›¾åƒä¸­çš„å¼‚å¸¸æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬åç¦»äº†æ­£å¸¸æ¨¡å¼ï¼Œæ¶‰åŠå¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬å·¥ä¸šã€é€»è¾‘å’ŒåŒ»ç–—é¢†åŸŸã€‚ç”±äºè¿™äº›é¢†åŸŸä¹‹é—´å­˜åœ¨é¢†åŸŸå·®è·ï¼Œç°æœ‰çš„VADæ–¹æ³•é€šå¸¸é’ˆå¯¹æ¯ä¸ªé¢†åŸŸè¿›è¡Œå®šåˆ¶ï¼Œå…·æœ‰ä¸“ä¸šçš„æ£€æµ‹æŠ€æœ¯å’Œæ¨¡å‹æ¶æ„ï¼Œéš¾ä»¥åœ¨ä¸åŒé¢†åŸŸä¹‹é—´è¿›è¡Œæ¨å¹¿ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨åŒä¸€é¢†åŸŸå†…ï¼Œå½“å‰çš„VADæ–¹æ³•é€šå¸¸éµå¾ªâ€œä¸€ç±»ä¸€æ¨¡å‹â€çš„æ¨¡å¼ï¼Œéœ€è¦å¤§é‡æ­£å¸¸æ ·æœ¬æ¥è®­ç»ƒç‰¹å®šç±»åˆ«çš„æ¨¡å‹ï¼Œè¿™å¯¼è‡´äº†è¾ƒå·®çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é˜»ç¢äº†è·¨é¢†åŸŸçš„ç»Ÿä¸€è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„Few-Shot VADæ–¹æ³•UniVADï¼Œèƒ½å¤Ÿæ£€æµ‹å„ç§é¢†åŸŸçš„å¼‚å¸¸ï¼Œå¦‚å·¥ä¸šã€é€»è¾‘å’ŒåŒ»å­¦å¼‚å¸¸ï¼Œä½¿ç”¨æ— éœ€è®­ç»ƒçš„ç»Ÿä¸€æ¨¡å‹ã€‚UniVADåªéœ€åœ¨æµ‹è¯•æ—¶ä½¿ç”¨å°‘é‡æ­£å¸¸æ ·æœ¬ä½œä¸ºå‚è€ƒï¼Œå³å¯æ£€æµ‹ä»¥å‰æœªè§è¿‡çš„å¯¹è±¡ä¸­çš„å¼‚å¸¸ï¼Œè€Œæ— éœ€åœ¨ç‰¹å®šé¢†åŸŸä¸Šè¿›è¡Œè®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼ŒUniVADé‡‡ç”¨åŸºäºèšç±»å’Œè§†è§‰åŸºç¡€æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç»„ä»¶èšç±»ï¼ˆC^3ï¼‰æ¨¡å—ï¼Œç²¾ç¡®åœ°åˆ†å‰²å›¾åƒå†…çš„ç»„ä»¶ï¼Œå¹¶åˆ©ç”¨ç»„ä»¶æ„ŸçŸ¥è¡¥ä¸åŒ¹é…ï¼ˆCAPMï¼‰å’Œå›¾å¢å¼ºç»„ä»¶å»ºæ¨¡ï¼ˆGECMï¼‰æ¨¡å—åœ¨ä¸åŒçš„è¯­ä¹‰çº§åˆ«æ£€æµ‹å¼‚å¸¸ï¼Œè¿™äº›å¼‚å¸¸è¢«èšåˆä»¥äº§ç”Ÿæœ€ç»ˆçš„æ£€æµ‹ç»“æœã€‚æˆ‘ä»¬åœ¨ä¹ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œè¿™äº›æ•°æ®é›†æ¶µç›–äº†å·¥ä¸šã€é€»è¾‘å’ŒåŒ»ç–—é¢†åŸŸï¼Œç»“æœè¡¨æ˜UniVADåœ¨è·¨å¤šä¸ªé¢†åŸŸçš„Few-Shotå¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä¼˜äºç‰¹å®šé¢†åŸŸçš„å¼‚å¸¸æ£€æµ‹æ¨¡å‹ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/FantasticGNU/UniVAD%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/FantasticGNU/UniVADè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.03342v3">PDF</a> Accepted by CVPR 2025; Project page: <a target="_blank" rel="noopener" href="https://uni-vad.github.io/">https://uni-vad.github.io/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†è§‰å¼‚å¸¸æ£€æµ‹ï¼ˆVADï¼‰æ—¨åœ¨è¯†åˆ«å›¾åƒä¸­çš„å¼‚å¸¸æ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬åç¦»æ­£å¸¸æ¨¡å¼ï¼Œæ¶‰åŠå·¥ä¸šã€é€»è¾‘å’ŒåŒ»ç–—ç­‰å¤šä¸ªé¢†åŸŸã€‚ç”±äºè¿™äº›é¢†åŸŸä¹‹é—´å­˜åœ¨é¢†åŸŸå·®è·ï¼Œç°æœ‰çš„VADæ–¹æ³•é€šå¸¸é’ˆå¯¹æ¯ä¸ªé¢†åŸŸè¿›è¡Œå®šåˆ¶ï¼Œå…·æœ‰ä¸“ä¸šåŒ–çš„æ£€æµ‹æŠ€æœ¯å’Œæ¨¡å‹æ¶æ„ï¼Œéš¾ä»¥åœ¨ä¸åŒé¢†åŸŸä¹‹é—´è¿›è¡Œæ¨å¹¿ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨åŒä¸€é¢†åŸŸå†…ï¼Œå½“å‰çš„VADæ–¹æ³•ä¹Ÿé€šå¸¸é‡‡ç”¨â€œä¸€ç±»ä¸€æ¨¡å‹â€çš„æ¨¡å¼ï¼Œéœ€è¦å¤§é‡æ­£å¸¸æ ·æœ¬æ¥è®­ç»ƒç‰¹å®šç±»åˆ«çš„æ¨¡å‹ï¼Œå¯¼è‡´æ³›åŒ–æ€§å·®ï¼Œé˜»ç¢äº†è·¨é¢†åŸŸçš„ç»Ÿä¸€è¯„ä¼°ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„å°‘æ ·æœ¬VADæ–¹æ³•UniVADï¼Œèƒ½å¤Ÿæ£€æµ‹å·¥ä¸šã€é€»è¾‘å’ŒåŒ»ç–—ç­‰è·¨é¢†åŸŸçš„å¼‚å¸¸å€¼ã€‚UniVADé‡‡ç”¨æ— è®­ç»ƒçš„ç»Ÿä¸€æ¨¡å‹ï¼Œä»…åœ¨æµ‹è¯•æ—¶éœ€è¦å°‘é‡æ­£å¸¸æ ·æœ¬ä½œä¸ºå‚è€ƒæ¥æ£€æµ‹å…ˆå‰æœªè§è¿‡çš„å¯¹è±¡ä¸­çš„å¼‚å¸¸å€¼ã€‚å…·ä½“æ¥è¯´ï¼ŒUniVADé‡‡ç”¨åŸºäºèšç±»å’Œè§†è§‰åŸºç¡€æ¨¡å‹çš„ä¸Šä¸‹æ–‡ç»„ä»¶èšç±»ï¼ˆC^3ï¼‰æ¨¡å—æ¥å‡†ç¡®åˆ†å‰²å›¾åƒå†…çš„ç»„ä»¶ï¼Œå¹¶åˆ©ç”¨ç»„ä»¶æ„ŸçŸ¥è¡¥ä¸åŒ¹é…ï¼ˆCAPMï¼‰å’Œå›¾å¢å¼ºç»„ä»¶å»ºæ¨¡ï¼ˆGECMï¼‰æ¨¡å—åœ¨ä¸åŒçš„è¯­ä¹‰çº§åˆ«æ£€æµ‹å¼‚å¸¸å€¼ï¼Œæœ€åæ±‡æ€»å¾—å‡ºæœ€ç»ˆçš„æ£€æµ‹ç»“æœã€‚æˆ‘ä»¬åœ¨æ¶µç›–å·¥ä¸šã€é€»è¾‘å’ŒåŒ»ç–—é¢†åŸŸçš„ä¹ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒUniVADåœ¨è·¨é¢†åŸŸçš„å°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œä¼˜äºç‰¹å®šé¢†åŸŸçš„å¼‚å¸¸æ£€æµ‹æ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>VADæ—¨åœ¨è¯†åˆ«ä¸æ­£å¸¸æ¨¡å¼åç¦»çš„å¼‚å¸¸å›¾åƒæ ·æœ¬ï¼Œæ¶‰åŠå¤šä¸ªé¢†åŸŸã€‚</li>
<li>ç°æœ‰VADæ–¹æ³•é€šå¸¸é’ˆå¯¹ç‰¹å®šé¢†åŸŸè®¾è®¡ï¼Œéš¾ä»¥è·¨é¢†åŸŸæ¨å¹¿ã€‚</li>
<li>UniVADæ˜¯ä¸€ç§é€šç”¨çš„å°‘æ ·æœ¬VADæ–¹æ³•ï¼Œå¯æ£€æµ‹ä¸åŒé¢†åŸŸçš„å¼‚å¸¸å€¼ã€‚</li>
<li>UniVADä»…éœ€è¦å°‘é‡æ­£å¸¸æ ·æœ¬ä½œä¸ºå‚è€ƒæ¥æ£€æµ‹æµ‹è¯•ä¸­çš„å¼‚å¸¸å€¼ã€‚</li>
<li>UniVADé‡‡ç”¨ä¸Šä¸‹æ–‡ç»„ä»¶èšç±»ï¼ˆC^3ï¼‰æ¨¡å—å‡†ç¡®åˆ†å‰²å›¾åƒç»„ä»¶ã€‚</li>
<li>UniVADåˆ©ç”¨ç»„ä»¶æ„ŸçŸ¥è¡¥ä¸åŒ¹é…ï¼ˆCAPMï¼‰å’Œå›¾å¢å¼ºç»„ä»¶å»ºæ¨¡ï¼ˆGECMï¼‰æ¨¡å—åœ¨å¤šä¸ªè¯­ä¹‰çº§åˆ«æ£€æµ‹å¼‚å¸¸å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.03342">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f6e7c090f5951ce466dec425e66544b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1258690da287beb7bae925ef19f37a4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fedf9887d8e4a560ad2a7c633088d83d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6fcaca991f91dcc186d626421d880a2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f0caac08fa6b2e27412cb631a7ec29f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8e53d4a9bf60ec112832999ef3cbeed.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DAWN-ICL-Strategic-Planning-of-Problem-solving-Trajectories-for-Zero-Shot-In-Context-Learning"><a href="#DAWN-ICL-Strategic-Planning-of-Problem-solving-Trajectories-for-Zero-Shot-In-Context-Learning" class="headerlink" title="DAWN-ICL: Strategic Planning of Problem-solving Trajectories for   Zero-Shot In-Context Learning"></a>DAWN-ICL: Strategic Planning of Problem-solving Trajectories for   Zero-Shot In-Context Learning</h2><p><strong>Authors:Xinyu Tang, Xiaolei Wang, Wayne Xin Zhao, Ji-Rong Wen</strong></p>
<p>Zero-shot in-context learning (ZS-ICL) aims to conduct in-context learning (ICL) without using human-annotated demonstrations. Most ZS-ICL methods use large language models (LLMs) to generate (input, label) pairs as pseudo-demonstrations and leverage historical pseudo-demonstrations to help solve the current problem. They assume that problems are from the same task and traverse them in a random order. However, in real-world scenarios, problems usually come from diverse tasks, and only a few belong to the same task. The random traversing order may generate unreliable pseudo-demonstrations and lead to error accumulation. To address this problem, we reformulate ZS-ICL as a planning problem and propose a Demonstration-aware Monte Carlo Tree Search (MCTS) approach (DAWN-ICL), which leverages MCTS to strategically plan the problem-solving trajectories for ZS-ICL. In addition, to achieve effective and efficient Q value estimation, we propose a novel demonstration-aware Q-value function and use it to enhance the selection phase and accelerate the expansion and simulation phases in MCTS. Extensive experiments demonstrate the effectiveness and efficiency of DAWN-ICL on in-domain and cross-domain scenarios, and it even outperforms ICL using human-annotated labels. The code is available at <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/MCTS4ZSICL">https://github.com/RUCAIBox/MCTS4ZSICL</a>. </p>
<blockquote>
<p>é›¶æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆZS-ICLï¼‰æ—¨åœ¨è¿›è¡Œæ— éœ€äººå·¥æ ‡æ³¨ç¤ºä¾‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚å¤§å¤šæ•°ZS-ICLæ–¹æ³•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆï¼ˆè¾“å…¥ï¼Œæ ‡ç­¾ï¼‰å¯¹ä½œä¸ºä¼ªæ¼”ç¤ºï¼Œå¹¶åˆ©ç”¨å†å²ä¼ªæ¼”ç¤ºæ¥å¸®åŠ©è§£å†³å½“å‰é—®é¢˜ã€‚å®ƒä»¬å‡è®¾é—®é¢˜æ¥è‡ªåŒä¸€ä»»åŠ¡ï¼Œå¹¶æŒ‰éšæœºé¡ºåºéå†ã€‚ç„¶è€Œï¼Œåœ¨çœŸå®åœºæ™¯ä¸­ï¼Œé—®é¢˜é€šå¸¸æ¥è‡ªå„ç§ä»»åŠ¡ï¼Œåªæœ‰å°‘æ•°å±äºåŒä¸€ä»»åŠ¡ã€‚éšæœºéå†é¡ºåºå¯èƒ½ä¼šç”Ÿæˆä¸å¯é çš„ä¼ªæ¼”ç¤ºï¼Œå¹¶å¯¼è‡´è¯¯å·®ç´¯ç§¯ã€‚ä¸ºäº†è§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å°†ZS-ICLé‡æ–°è¡¨è¿°ä¸ºè§„åˆ’é—®é¢˜ï¼Œå¹¶æå‡ºä¸€ç§åŸºäºæ¼”ç¤ºçš„è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰æ–¹æ³•ï¼ˆDAWN-ICLï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨MCTSä¸ºZS-ICLæˆ˜ç•¥æ€§åœ°è§„åˆ’é—®é¢˜è§£å†³è½¨è¿¹ã€‚æ­¤å¤–ï¼Œä¸ºäº†æœ‰æ•ˆä¸”é«˜æ•ˆåœ°ä¼°è®¡Qå€¼ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºæ¼”ç¤ºçš„Qå€¼å‡½æ•°ï¼Œå¹¶å°†å…¶ç”¨äºå¢å¼ºé€‰æ‹©é˜¶æ®µå¹¶åŠ é€ŸMCTSä¸­çš„æ‰©å±•å’Œæ¨¡æ‹Ÿé˜¶æ®µã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDAWN-ICLåœ¨åŸŸå†…å’Œè·¨åŸŸåœºæ™¯ä¸­éƒ½æœ‰æ•ˆä¸”é«˜æ•ˆï¼Œç”šè‡³è¶…è¶Šäº†ä½¿ç”¨äººå·¥æ ‡æ³¨æ ‡ç­¾çš„ICLã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/MCTS4ZSICL">https://github.com/RUCAIBox/MCTS4ZSICL</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20215v2">PDF</a> NAACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é›¶æ ·æœ¬ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆZS-ICLï¼‰çš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ç°å®ä¸–ç•Œä¸­ä»»åŠ¡å¤šæ ·æ€§çš„é—®é¢˜ã€‚æ–‡ç« æå‡ºå°†ZS-ICLé‡æ–°æ„å»ºä¸ºè§„åˆ’é—®é¢˜ï¼Œå¹¶å¼•å…¥åŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„æ¼”ç¤ºæ„ŸçŸ¥æ–¹æ³•ï¼ˆDAWN-ICLï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡MCTSç­–ç•¥è§„åˆ’é—®é¢˜è§£å†³è½¨è¿¹ï¼Œå¹¶æå‡ºæ¼”ç¤ºæ„ŸçŸ¥Qå€¼å‡½æ•°ä»¥æé«˜é€‰æ‹©é˜¶æ®µæ•ˆç‡ï¼ŒåŠ é€ŸMCTSçš„æ‰©å±•å’Œæ¨¡æ‹Ÿé˜¶æ®µã€‚å®éªŒè¡¨æ˜ï¼ŒDAWN-ICLåœ¨åŸŸå†…å’Œè·¨åŸŸåœºæ™¯ä¸­éƒ½è¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œé«˜æ•ˆæ€§ï¼Œç”šè‡³è¶…è¶Šäº†ä½¿ç”¨äººç±»æ³¨é‡Šæ ‡ç­¾çš„ICLã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZS-ICLæ—¨åœ¨è¿›è¡Œæ— éœ€äººç±»æ³¨é‡Šæ¼”ç¤ºçš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ã€‚</li>
<li>ç°æœ‰ZS-ICLæ–¹æ³•ä¸»è¦ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆï¼ˆè¾“å…¥ï¼Œæ ‡ç­¾ï¼‰å¯¹ä½œä¸ºä¼ªæ¼”ç¤ºã€‚</li>
<li>ç°å®ä¸–ç•Œä¸­ï¼Œé—®é¢˜é€šå¸¸æ¥è‡ªå¤šæ ·åŒ–çš„ä»»åŠ¡ï¼Œåªæœ‰å°‘éƒ¨åˆ†å±äºåŒä¸€ä»»åŠ¡ã€‚</li>
<li>éšæœºéå†é¡ºåºå¯èƒ½ç”Ÿæˆä¸å¯é çš„ä¼ªæ¼”ç¤ºï¼Œå¯¼è‡´è¯¯å·®ç´¯ç§¯ã€‚</li>
<li>æœ¬æ–‡å°†ZS-ICLé‡æ–°æ„å»ºä¸ºè§„åˆ’é—®é¢˜ï¼Œå¹¶æå‡ºåŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰çš„æ¼”ç¤ºæ„ŸçŸ¥æ–¹æ³•ï¼ˆDAWN-ICLï¼‰ã€‚</li>
<li>DAWN-ICLé€šè¿‡MCTSç­–ç•¥è§„åˆ’é—®é¢˜è§£å†³è½¨è¿¹ã€‚</li>
<li>å¼•å…¥æ¼”ç¤ºæ„ŸçŸ¥Qå€¼å‡½æ•°ï¼Œæé«˜é€‰æ‹©é˜¶æ®µæ•ˆç‡ï¼ŒåŠ é€ŸMCTSçš„æ‰©å±•å’Œæ¨¡æ‹Ÿé˜¶æ®µã€‚å®éªŒè¡¨æ˜DAWN-ICLæ–¹æ³•æœ‰æ•ˆä¸”é«˜æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.20215">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8de67ce87c553fecebc5356923143a3a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f9b4e6c3ef0f51586888df6b58862741.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Next-Best-Sense-Guiding-Vision-and-Touch-with-FisherRF-for-3D-Gaussian-Splatting"><a href="#Next-Best-Sense-Guiding-Vision-and-Touch-with-FisherRF-for-3D-Gaussian-Splatting" class="headerlink" title="Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian   Splatting"></a>Next Best Sense: Guiding Vision and Touch with FisherRF for 3D Gaussian   Splatting</h2><p><strong>Authors:Matthew Strong, Boshu Lei, Aiden Swann, Wen Jiang, Kostas Daniilidis, Monroe Kennedy III</strong></p>
<p>We propose a framework for active next best view and touch selection for robotic manipulators using 3D Gaussian Splatting (3DGS). 3DGS is emerging as a useful explicit 3D scene representation for robotics, as it has the ability to represent scenes in a both photorealistic and geometrically accurate manner. However, in real-world, online robotic scenes where the number of views is limited given efficiency requirements, random view selection for 3DGS becomes impractical as views are often overlapping and redundant. We address this issue by proposing an end-to-end online training and active view selection pipeline, which enhances the performance of 3DGS in few-view robotics settings. We first elevate the performance of few-shot 3DGS with a novel semantic depth alignment method using Segment Anything Model 2 (SAM2) that we supplement with Pearson depth and surface normal loss to improve color and depth reconstruction of real-world scenes. We then extend FisherRF, a next-best-view selection method for 3DGS, to select views and touch poses based on depth uncertainty. We perform online view selection on a real robot system during live 3DGS training. We motivate our improvements to few-shot GS scenes, and extend depth-based FisherRF to them, where we demonstrate both qualitative and quantitative improvements on challenging robot scenes. For more information, please see our project page at <a target="_blank" rel="noopener" href="https://arm.stanford.edu/next-best-sense">https://arm.stanford.edu/next-best-sense</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä¸‰ç»´é«˜æ–¯æ’å€¼ï¼ˆ3DGSï¼‰çš„æœºå™¨äººæ“ä½œå™¨æ´»åŠ¨æœ€ä½³ä¸‹ä¸€ä¸ªè§†è§’å’Œè§¦æ‘¸é€‰æ‹©çš„æ¡†æ¶ã€‚3DGSä½œä¸ºä¸€ç§æœ‰ç”¨çš„æœºå™¨äººæŠ€æœ¯ä¸‰ç»´åœºæ™¯è¡¨ç¤ºæ³•ï¼Œå…·æœ‰å†ç°ç°å®åœºæ™¯çš„å…‰ç…§å’Œå‡ ä½•å‡†ç¡®æ€§èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œä¸­çš„åœ¨çº¿æœºå™¨äººåœºæ™¯ä¸­ï¼Œç”±äºæ•ˆç‡è¦æ±‚ï¼Œè§†å›¾æ•°é‡æœ‰é™ï¼Œéšæœºé€‰æ‹©è§†å›¾è¿›è¡Œ3DGSå˜å¾—ä¸åˆ‡å®é™…ï¼Œå› ä¸ºè§†å›¾ç»å¸¸é‡å ä¸”å†—ä½™ã€‚æˆ‘ä»¬é€šè¿‡æå‡ºä¸€ç§ç«¯åˆ°ç«¯çš„åœ¨çº¿è®­ç»ƒå’Œæ´»åŠ¨è§†å›¾é€‰æ‹©ç®¡é“æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥ç®¡é“æé«˜äº†å°‘æ•°è§†è§’æœºå™¨äººè®¾ç½®ä¸­çš„3DGSæ€§èƒ½ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨åˆ†æ®µä»»ä½•æ¨¡å‹ï¼ˆSAM2ï¼‰çš„æ–°é¢–è¯­ä¹‰æ·±åº¦å¯¹é½æ–¹æ³•æ¥æé«˜å°‘é‡å°„å‡»3DGSçš„æ€§èƒ½ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡çš®å°”æ£®æ·±åº¦å’Œè¡¨é¢æ­£å¸¸æŸå¤±æ¥è¡¥å……SAM2ï¼Œä»¥æ”¹å–„çœŸå®åœºæ™¯çš„é¢œè‰²å’Œæ·±åº¦é‡å»ºã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹FisherRFè¿›è¡Œæ‰©å±•ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äº3DGSçš„æœ€ä½³ä¸‹ä¸€ä¸ªè§†å›¾é€‰æ‹©æ–¹æ³•ï¼Œæ ¹æ®æ·±åº¦ä¸ç¡®å®šæ€§é€‰æ‹©è§†å›¾å’Œè§¦æ‘¸å§¿åŠ¿ã€‚æˆ‘ä»¬åœ¨å®æ—¶3DGSè®­ç»ƒæœŸé—´åœ¨çœŸå®çš„æœºå™¨äººç³»ç»Ÿä¸Šæ‰§è¡Œåœ¨çº¿è§†å›¾é€‰æ‹©ã€‚æˆ‘ä»¬æ”¹è¿›äº†å°‘æ•°å°„å‡»GSåœºæ™¯ï¼Œå¹¶å°†åŸºäºæ·±åº¦çš„FisherRFæ‰©å±•åˆ°å®ƒä»¬ä¸Šï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æœºå™¨äººåœºæ™¯ä¸­è¯æ˜äº†æˆ‘ä»¬çš„å®šæ€§å’Œå®šé‡æ”¹è¿›ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://arm.stanford.edu/next-best-sense%E3%80%82">https://arm.stanford.edu/next-best-senseã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.04680v4">PDF</a> To appear in International Conference on Robotics and Automation   (ICRA) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹æœºå™¨äººæ“çºµå™¨çš„ä¸»åŠ¨æœ€ä½³è§†è§’å’Œè§¦æ‘¸é€‰æ‹©çš„æ¡†æ¶ï¼Œåˆ©ç”¨ä¸‰ç»´é«˜æ–¯å–·ç»˜æŠ€æœ¯ï¼ˆ3DGSï¼‰ã€‚é’ˆå¯¹åœ¨çº¿æœºå™¨äººåœºæ™¯ä¸­è§†è§’æ•°é‡æœ‰é™çš„é—®é¢˜ï¼Œæ–‡ç« é€šè¿‡ç«¯åˆ°ç«¯çš„åœ¨çº¿è®­ç»ƒå’Œä¸»åŠ¨è§†è§’é€‰æ‹©ç®¡é“ï¼Œæé«˜äº†ä¸‰ç»´é«˜æ–¯å–·ç»˜åœ¨å°‘é‡è§†è§’ä¸‹çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæ–‡ç« å¼•å…¥äº†ä¸€ç§æ–°å‹è¯­ä¹‰æ·±åº¦å¯¹é½æ–¹æ³•ï¼Œç»“åˆä½©å°”æ£®æ·±åº¦å’Œè¡¨é¢æ­£å¸¸æŸå¤±ï¼Œæ”¹å–„çœŸå®åœºæ™¯çš„é¢œè‰²å’Œæ·±åº¦é‡å»ºã€‚æœ€åï¼Œæ–‡ç« å°†FisherRFæ‰©å±•åˆ°åŸºäºæ·±åº¦ä¸ç¡®å®šæ€§çš„è§†è§’é€‰æ‹©å’Œè§¦æ‘¸å§¿åŠ¿é€‰æ‹©ï¼Œå¹¶åœ¨çœŸå®æœºå™¨äººç³»ç»Ÿä¸Šè¿›è¡Œåœ¨çº¿è§†è§’é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºåˆ©ç”¨ä¸‰ç»´é«˜æ–¯å–·ç»˜æŠ€æœ¯ï¼ˆ3DGSï¼‰çš„æ¡†æ¶ï¼Œå®ç°æœºå™¨äººæ“çºµå™¨çš„ä¸»åŠ¨æœ€ä½³è§†è§’å’Œè§¦æ‘¸é€‰æ‹©ã€‚</li>
<li>3DGSèƒ½å¤Ÿåœ¨ç…§ç‰‡çº§çœŸå®å’Œå‡ ä½•å‡†ç¡®çš„åœºæ™¯è¡¨ç¤ºä¹‹é—´å–å¾—å¹³è¡¡ã€‚</li>
<li>é’ˆå¯¹åœ¨çº¿æœºå™¨äººåœºæ™¯ä¸­è§†è§’æ•°é‡æœ‰é™çš„é—®é¢˜ï¼Œæå‡ºäº†ç«¯åˆ°ç«¯çš„åœ¨çº¿è®­ç»ƒå’Œä¸»åŠ¨è§†è§’é€‰æ‹©ç®¡é“ã€‚</li>
<li>å¼•å…¥æ–°å‹è¯­ä¹‰æ·±åº¦å¯¹é½æ–¹æ³•ï¼Œç»“åˆä½©å°”æ£®æ·±åº¦å’Œè¡¨é¢æ­£å¸¸æŸå¤±ï¼Œæ”¹å–„çœŸå®åœºæ™¯çš„é¢œè‰²å’Œæ·±åº¦é‡å»ºã€‚</li>
<li>å°†FisherRFæ‰©å±•åˆ°åŸºäºæ·±åº¦ä¸ç¡®å®šæ€§çš„è§†è§’é€‰æ‹©å’Œè§¦æ‘¸å§¿åŠ¿é€‰æ‹©ã€‚</li>
<li>åœ¨æŒ‘æˆ˜æ€§çš„æœºå™¨äººåœºæ™¯ä¸­è¿›è¡Œäº†å®šé‡å’Œå®šæ€§çš„æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.04680">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7b883c06e632db846504de44b4ac49cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a717614156def5fe4b96d648c611d6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd1887eef5653d9d242b142541e062e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-197ec846ef39795f6bcf54e5b042a963.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61b848d181da5b2a338f0a1189938ac0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="UniRAG-Universal-Retrieval-Augmentation-for-Large-Vision-Language-Models"><a href="#UniRAG-Universal-Retrieval-Augmentation-for-Large-Vision-Language-Models" class="headerlink" title="UniRAG: Universal Retrieval Augmentation for Large Vision Language   Models"></a>UniRAG: Universal Retrieval Augmentation for Large Vision Language   Models</h2><p><strong>Authors:Sahel Sharifymoghaddam, Shivani Upadhyay, Wenhu Chen, Jimmy Lin</strong></p>
<p>Recently, Large Vision Language Models (LVLMs) have unlocked many complex use cases that require Multi-Modal (MM) understanding (e.g., image captioning or visual question answering) and MM generation (e.g., text-guided image generation or editing) capabilities. To further improve the output fidelityof LVLMs we introduce UniRAG, a plug-and-play technique that adds relevant retrieved information to prompts as few-shot examples during inference. Unlike the common belief that Retrieval Augmentation (RA) mainly improves generation or understanding of uncommon entities, our evaluation results on the MSCOCO dataset with common entities show that both proprietary models like GPT-4o and Gemini-Pro and smaller open-source models like LLaVA, LaVIT, and Emu2 significantly enhance their generation quality when their input prompts are augmented with relevant information retrieved by Vision-Language (VL) retrievers like UniIR models. All the necessary code to reproduce our results is available at <a target="_blank" rel="noopener" href="https://github.com/castorini/UniRAG">https://github.com/castorini/UniRAG</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å·²ç»è§£é”äº†è®¸å¤šå¤æ‚çš„ç”¨ä¾‹ï¼Œè¿™äº›ç”¨ä¾‹éœ€è¦å¤šæ¨¡æ€ï¼ˆMMï¼‰ç†è§£ï¼ˆä¾‹å¦‚ï¼Œå›¾åƒæ ‡é¢˜æˆ–è§†è§‰é—®ç­”ï¼‰å’Œå¤šæ¨¡æ€ç”Ÿæˆï¼ˆä¾‹å¦‚ï¼Œæ–‡æœ¬å¼•å¯¼çš„å›¾åƒç”Ÿæˆæˆ–ç¼–è¾‘ï¼‰åŠŸèƒ½ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜LVLMsçš„è¾“å‡ºä¿çœŸåº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniRAGï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨æŠ€æœ¯ï¼Œå®ƒå°†åœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†ç›¸å…³çš„æ£€ç´¢ä¿¡æ¯ä½œä¸ºå°‘é‡ç¤ºä¾‹æ·»åŠ åˆ°æç¤ºä¸­ã€‚ä¸æ™®éè®¤ä¸ºçš„æ£€ç´¢å¢å¼ºï¼ˆRAï¼‰ä¸»è¦æé«˜ç½•è§å®ä½“çš„ç”Ÿæˆæˆ–ç†è§£ä¸åŒï¼Œæˆ‘ä»¬åœ¨MSCOCOæ•°æ®é›†ä¸Šå¯¹å¸¸è§å®ä½“çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå½“è¾“å…¥æç¤ºé€šè¿‡è§†è§‰è¯­è¨€ï¼ˆVLï¼‰æ£€ç´¢å™¨ï¼ˆå¦‚UniIRæ¨¡å‹ï¼‰æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯è¿›è¡Œå¢å¼ºæ—¶ï¼Œæ— è®ºæ˜¯ä¸“æœ‰æ¨¡å‹ï¼ˆå¦‚GPT-4oå’ŒGemini-Proï¼‰ï¼Œè¿˜æ˜¯è¾ƒå°çš„å¼€æºæ¨¡å‹ï¼ˆå¦‚LLaVAã€LaVITå’ŒEmu2ï¼‰ï¼Œå®ƒä»¬çš„ç”Ÿæˆè´¨é‡éƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚æ‰€æœ‰é‡ç°æˆ‘ä»¬ç»“æœçš„å¿…è¦ä»£ç éƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/castorini/UniRAG">https://github.com/castorini/UniRAG</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.10311v3">PDF</a> 14 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å·²è§£é”è®¸å¤šéœ€è¦å¤šæ¨¡æ€ï¼ˆMMï¼‰ç†è§£å’Œç”Ÿæˆèƒ½åŠ›çš„å¤æ‚ç”¨ä¾‹ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜LVLMsçš„è¾“å‡ºä¿çœŸåº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†UniRAGï¼Œè¿™æ˜¯ä¸€ç§å³æ’å³ç”¨çš„æŠ€æœ¯ï¼Œå¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†ç›¸å…³çš„æ£€ç´¢ä¿¡æ¯ä½œä¸ºå°‘æ•°æ¡ˆä¾‹æ·»åŠ åˆ°æç¤ºä¸­ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œæ— è®ºæ˜¯ä¸“æœ‰æ¨¡å‹å¦‚GPT-4oå’ŒGemini-Proï¼Œè¿˜æ˜¯è¾ƒå°çš„å¼€æºæ¨¡å‹å¦‚LLaVAã€LaVITå’ŒEmu2ï¼Œé€šè¿‡å¢å¼ºè¾“å…¥æç¤ºä¸UniRAGç­‰è§†è§‰è¯­è¨€æ£€ç´¢å™¨æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ç›¸ç»“åˆï¼Œéƒ½å¯ä»¥æ˜¾è‘—æé«˜ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰å…·å¤‡å¤„ç†å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆçš„èƒ½åŠ›ã€‚</li>
<li>UniRAGæ˜¯ä¸€ç§æé«˜LVLMsè¾“å‡ºä¿çœŸåº¦çš„æŠ€æœ¯ï¼Œå®ƒé€šè¿‡æ·»åŠ ç›¸å…³çš„æ£€ç´¢ä¿¡æ¯æ¥å¢å¼ºæç¤ºã€‚</li>
<li>UniRAGæŠ€æœ¯é‡‡ç”¨å³æ’å³ç”¨çš„æ–¹å¼ï¼Œå¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­è½»æ¾é›†æˆåˆ°ç°æœ‰æ¨¡å‹ä¸­ã€‚</li>
<li>è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒUniRAGå¯¹ä¸“æœ‰æ¨¡å‹å’Œå¼€æºæ¨¡å‹éƒ½æœ‰æ˜¾è‘—çš„æå‡æ•ˆæœã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯å¯¹äºå¸¸è§å®ä½“ï¼Œæ£€ç´¢å¢å¼ºä¹Ÿå¯ä»¥æé«˜ç”Ÿæˆè´¨é‡ã€‚</li>
<li>UniRAGçš„å®ç°åœ¨GitHubä¸Šå…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.10311">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bf27d89025c512d16fbbb7452200e935.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-99f4aa83a0b5d1e4983bb377ad5dac67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0589b0de1e1e90f87cf4f57c30dd0f05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8447e376d9b7ff736f1250d0acc9676.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-926595eee4c3e44c71d9ddc7d5194640.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-328a0acff425cb29aa33086d60dcd5d3.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Continual-Adversarial-Defense"><a href="#Continual-Adversarial-Defense" class="headerlink" title="Continual Adversarial Defense"></a>Continual Adversarial Defense</h2><p><strong>Authors:Qian Wang, Hefei Ling, Yingwei Li, Qihao Liu, Ruoxi Jia, Ning Yu</strong></p>
<p>In response to the rapidly evolving nature of adversarial attacks against visual classifiers on a monthly basis, numerous defenses have been proposed to generalize against as many known attacks as possible. However, designing a defense method that generalizes to all types of attacks is not realistic because the environment in which defense systems operate is dynamic and comprises various unique attacks that emerge as time goes on. A well-matched approach to the dynamic environment lies in a defense system that continuously collects adversarial data online to quickly improve itself. Therefore, we put forward a practical defense deployment against a challenging threat model and propose, for the first time, the Continual Adversarial Defense (CAD) framework that adapts to attack sequences under four principles: (1)<del>continual adaptation to new attacks without catastrophic forgetting, (2)</del>few-shot adaptation, (3)<del>memory-efficient adaptation, and (4)</del>high accuracy on both clean and adversarial data. We explore and integrate cutting-edge continual learning, few-shot learning, and ensemble learning techniques to qualify the principles. Extensive experiments validate the effectiveness of our approach against multiple stages of modern adversarial attacks and demonstrate significant improvements over numerous baseline methods. In particular, CAD is capable of quickly adapting with minimal budget and a low cost of defense failure while maintaining good performance against previous attacks. Our research sheds light on a brand-new paradigm for continual defense adaptation against dynamic and evolving attacks. </p>
<blockquote>
<p>é’ˆå¯¹æ¯æœˆéƒ½åœ¨å¿«é€Ÿæ¼”å˜çš„é’ˆå¯¹è§†è§‰åˆ†ç±»å™¨çš„å¯¹æŠ—æ€§æ”»å‡»ï¼Œå·²ç»æå‡ºäº†è®¸å¤šé˜²å¾¡ç­–ç•¥ï¼Œä»¥å°½å¯èƒ½å¯¹æŠ—å¤šç§å·²çŸ¥çš„æ”»å‡»è¿›è¡Œæ¨å¹¿ã€‚ç„¶è€Œï¼Œè®¾è®¡ä¸€ç§èƒ½å¤Ÿåº”å¯¹æ‰€æœ‰ç±»å‹æ”»å‡»çš„é˜²å¾¡æ–¹æ³•å¹¶ä¸ç°å®ï¼Œå› ä¸ºé˜²å¾¡ç³»ç»Ÿæ‰€å¤„çš„ç¯å¢ƒæ˜¯åŠ¨æ€çš„ï¼Œå¹¶ä¸”åŒ…å«éšç€æ—¶é—´æ¨ç§»å‡ºç°çš„å„ç§ç‹¬ç‰¹æ”»å‡»ã€‚ä¸åŠ¨æ€ç¯å¢ƒç›¸åŒ¹é…çš„æ–¹æ³•æ˜¯é˜²å¾¡ç³»ç»Ÿèƒ½å¤ŸæŒç»­åœ¨çº¿æ”¶é›†å¯¹æŠ—æ€§æ•°æ®ï¼Œä»¥å¿«é€Ÿæé«˜è‡ªèº«èƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é’ˆå¯¹å…·æœ‰æŒ‘æˆ˜æ€§çš„å¨èƒæ¨¡å‹æå‡ºäº†å®ç”¨çš„é˜²å¾¡éƒ¨ç½²ï¼Œå¹¶é¦–æ¬¡æå‡ºé€‚åº”æ”»å‡»åºåˆ—çš„Continual Adversarial Defense (CAD)æ¡†æ¶ï¼Œè¯¥æ¡†æ¶éµå¾ªå››é¡¹åŸåˆ™ï¼š(1)æŒç»­é€‚åº”æ–°æ”»å‡»è€Œä¸ä¼šå‡ºç°ç¾éš¾æ€§é—å¿˜ï¼Œ(2)å°æ ·æœ¬é€‚åº”ï¼Œ(3)å†…å­˜é«˜æ•ˆé€‚åº”ï¼Œä»¥åŠ(4)åœ¨å¹²å‡€å’Œå¯¹æŠ—æ€§æ•°æ®ä¸Šéƒ½å…·æœ‰é«˜å‡†ç¡®æ€§ã€‚æˆ‘ä»¬æ¢ç´¢å¹¶æ•´åˆäº†æœ€å…ˆè¿›çš„æŒç»­å­¦ä¹ ã€å°æ ·æœ¬å­¦ä¹ å’Œé›†æˆå­¦ä¹ æŠ€æœ¯æ¥ç¬¦åˆè¿™äº›åŸåˆ™ã€‚å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šé˜¶æ®µç°ä»£å¯¹æŠ—æ€§æ”»å‡»ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜ä¸è®¸å¤šåŸºå‡†æ–¹æ³•ç›¸æ¯”å…·æœ‰æ˜¾è‘—æ”¹è¿›ã€‚ç‰¹åˆ«æ˜¯ï¼ŒCADèƒ½å¤Ÿè¿…é€Ÿé€‚åº”ï¼Œå…·æœ‰æœ€å°çš„é¢„ç®—å’Œè¾ƒä½çš„é˜²å¾¡å¤±è´¥æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒè‰¯å¥½çš„å¯¹æŠ—ä¹‹å‰æ”»å‡»çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºé’ˆå¯¹åŠ¨æ€å’Œä¸æ–­æ¼”å˜çš„æ”»å‡»çš„è¿ç»­é˜²å¾¡é€‚åº”æä¾›äº†æ–°çš„èŒƒä¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.09481v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æå‡ºäº†ä¸€ç§åº”å¯¹åŠ¨æ€å’Œä¸æ–­æ¼”å˜çš„æ”»å‡»çš„æ–°å‹é˜²å¾¡èŒƒå¼â€”â€”æŒç»­å¯¹æŠ—æ€§é˜²å¾¡ï¼ˆCADï¼‰æ¡†æ¶ã€‚CADæ¡†æ¶èƒ½å¤Ÿé€‚åº”æ”»å‡»åºåˆ—ï¼Œå¹¶å…·å¤‡å››é¡¹åŸåˆ™ï¼šæŒç»­é€‚åº”æ–°æ”»å‡»ã€å°‘æ ·æœ¬é€‚åº”ã€å†…å­˜é«˜æ•ˆé€‚åº”ä»¥åŠå¯¹å¹²å‡€å’Œå¯¹æŠ—æ€§æ•°æ®çš„é«˜å‡†ç¡®æ€§ã€‚é€šè¿‡ç»“åˆå‰æ²¿çš„æŒç»­å­¦ä¹ ã€å°æ ·æœ¬å­¦ä¹ å’Œé›†æˆå­¦ä¹ æŠ€æœ¯ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ªé˜¶æ®µçš„ç°ä»£å¯¹æŠ—æ€§æ”»å‡»ä¸­éªŒè¯æœ‰æ•ˆï¼Œç›¸è¾ƒäºå¤šç§åŸºçº¿æ–¹æ³•æœ‰æ˜æ˜¾æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æŠ—æ”»å‡»ä¸æ–­æ¼”åŒ–ï¼Œé€šç”¨é˜²å¾¡æ–¹æ³•ä¸ç°å®ã€‚</li>
<li>é˜²å¾¡ç³»ç»Ÿéœ€è¦èƒ½é€‚åº”æ–°æ”»å‡»ï¼ŒæŒç»­æ”¶é›†å¯¹æŠ—æ•°æ®è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>é¦–æ¬¡æå‡ºæŒç»­å¯¹æŠ—æ€§é˜²å¾¡ï¼ˆCADï¼‰æ¡†æ¶ï¼Œé€‚åº”æ”»å‡»åºåˆ—ã€‚</li>
<li>CADæ¡†æ¶éµå¾ªå››é¡¹åŸåˆ™ï¼šæŒç»­é€‚åº”ã€å°‘æ ·æœ¬é€‚åº”ã€å†…å­˜é«˜æ•ˆã€é«˜å‡†ç¡®æ€§ã€‚</li>
<li>ç»“åˆæŒç»­å­¦ä¹ ã€å°æ ·æœ¬å­¦ä¹ å’Œé›†æˆå­¦ä¹ æŠ€æœ¯æ¥å®ç°CADæ¡†æ¶ã€‚</li>
<li>å®éªŒéªŒè¯CADæ¡†æ¶åœ¨å¤šç§å¯¹æŠ—æ”»å‡»ä¸­çš„æœ‰æ•ˆæ€§ï¼Œè¾ƒåŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.09481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4c157c4d4e945146eb99bb424fe01ba1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6336e7d472714ce255fad8696713c28a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1156df0e592cb40e25cc43735a3fbd4.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9ec00fdcae204d41db27618a87ceed9b.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  LBM Latent Bridge Matching for Fast Image-to-Image Translation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-3c6e299ad16040444d5c2bb0c92c3a51.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  Property Enhanced Instruction Tuning for Multi-task Molecule Generation   with Large Language Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">13632.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
