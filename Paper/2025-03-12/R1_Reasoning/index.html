<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  MedAgentsBench Benchmarking Thinking Models and Agent Frameworks for   Complex Medical Reasoning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b2ab6663be184b5ac86e372483bd9e50.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-12-æ›´æ–°"><a href="#2025-03-12-æ›´æ–°" class="headerlink" title="2025-03-12 æ›´æ–°"></a>2025-03-12 æ›´æ–°</h1><h2 id="MedAgentsBench-Benchmarking-Thinking-Models-and-Agent-Frameworks-for-Complex-Medical-Reasoning"><a href="#MedAgentsBench-Benchmarking-Thinking-Models-and-Agent-Frameworks-for-Complex-Medical-Reasoning" class="headerlink" title="MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for   Complex Medical Reasoning"></a>MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for   Complex Medical Reasoning</h2><p><strong>Authors:Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang, Jinyu Xiang, Fang Wu, Yilun Zhao, Chenglin Wu, Wenqi Shi, Arman Cohan, Mark Gerstein</strong></p>
<p>Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present MedAgentsBench, a benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planning-scenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at <a target="_blank" rel="noopener" href="https://github.com/gersteinlab/medagents-benchmark">https://github.com/gersteinlab/medagents-benchmark</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°æœ‰çš„åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚è¿™ç§é«˜æ€§èƒ½ä½¿å¾—å¯¹å…ˆè¿›æ–¹æ³•è¿›è¡Œæœ‰æ„ä¹‰åœ°è¯„ä¼°å’ŒåŒºåˆ†å˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚æˆ‘ä»¬æ¨å‡ºäº†MedAgentsBenchåŸºå‡†æµ‹è¯•ï¼Œå®ƒä¸“æ³¨äºå…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»ç–—é—®é¢˜ï¼Œè¿™äº›é—®é¢˜éœ€è¦è¿›è¡Œå¤šæ­¥éª¤çš„ä¸´åºŠæ¨ç†ã€è¯Šæ–­åˆ¶å®šå’Œæ²»ç–—è®¡åˆ’åˆ¶å®šåœºæ™¯ï¼Œå°½ç®¡åœ¨æ ‡å‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä½†å½“å‰çš„æ¨¡å‹ä»é¢ä¸´è¿™äº›å›°éš¾ã€‚æˆ‘ä»¬ä»ä¸ƒä¸ªå…¬è®¤çš„åŒ»å­¦æ•°æ®é›†ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•è§£å†³äº†ç°æœ‰è¯„ä¼°ä¸­çš„ä¸‰ä¸ªå…³é”®å±€é™æ€§ï¼šï¼ˆ1ï¼‰ç®€å•é—®é¢˜æ™®éå­˜åœ¨çš„ç°è±¡ï¼Œå…¶ä¸­å³ä½¿æ˜¯åŸºç¡€æ¨¡å‹ä¹Ÿèƒ½å®ç°é«˜æ€§èƒ½ï¼›ï¼ˆ2ï¼‰ä¸åŒç ”ç©¶ä¹‹é—´çš„é‡‡æ ·å’Œè¯„ä¼°åè®®ä¸ä¸€è‡´ï¼›ï¼ˆ3ï¼‰ç¼ºä¹å¯¹æ€§èƒ½ã€æˆæœ¬å’Œæ¨ç†æ—¶é—´ä¹‹é—´ç›¸äº’ä½œç”¨çš„ç³»ç»Ÿåˆ†æã€‚é€šè¿‡å¯¹å„ç§åŸºç¡€æ¨¡å‹å’Œæ¨ç†æ–¹æ³•çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æœ€æ–°çš„æ€ç»´æ¨¡å‹DeepSeek R1å’ŒOpenAI o3åœ¨å¤æ‚çš„åŒ»ç–—æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ä¼ ç»Ÿçš„æœç´¢æ–¹æ³•ç›¸æ¯”ï¼Œå…ˆè¿›çš„åŸºäºæœç´¢çš„ä»£ç†æ–¹æ³•æä¾›äº†éå¸¸æœ‰å‰æ™¯çš„æ€§èƒ½ä¸æˆæœ¬æ¯”ç‡ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸åŒæ¨¡å‹å®¶æ—åœ¨å¤æ‚é—®é¢˜ä¸Šçš„æ€§èƒ½å·®è·ï¼Œå¹¶é’ˆå¯¹ä¸åŒçš„è®¡ç®—çº¦æŸç¡®å®šäº†æœ€ä½³æ¨¡å‹é€‰æ‹©ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/gersteinlab/medagents-benchmark">https://github.com/gersteinlab/medagents-benchmark</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07459v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç°æœ‰çš„åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œè¿™ä½¿å¾—å¯¹å…ˆè¿›æ–¹æ³•çš„è¯„ä¼°å’ŒåŒºåˆ†å˜å¾—æ—¥ç›Šå›°éš¾ã€‚æœ¬æ–‡æå‡ºäº†MedAgentsBenchåŸºå‡†æµ‹è¯•ï¼Œå®ƒä¸“æ³¨äºæŒ‘æˆ˜éœ€è¦å¤šæ­¥éª¤ä¸´åºŠæ¨ç†ã€è¯Šæ–­åˆ¶å®šå’Œæ²»ç–—è®¡åˆ’è®¾è®¡çš„åŒ»ç–—é—®é¢˜ã€‚è¯¥åŸºå‡†æµ‹è¯•è§£å†³äº†ç°æœ‰è¯„ä¼°çš„ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šä»ä¸ƒä¸ªæƒå¨åŒ»å­¦æ•°æ®é›†é€‰å–ï¼Œå…³æ³¨åœ¨åŸºç¡€æ¨¡å‹å°±è¡¨ç°ä¼˜å¼‚çš„ä¸€è‡´æ€§é—®é¢˜ï¼Œä»¥åŠç¼ºä¹é’ˆå¯¹æ€§èƒ½ã€æˆæœ¬å’Œæ¨ç†æ—¶é—´çš„ç³»ç»Ÿåˆ†æã€‚é€šè¿‡åœ¨ä¸åŒåŸºç¡€æ¨¡å‹å’Œæ¨ç†æ–¹æ³•ä¸Šçš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æœ€æ–°æ¨¡å‹DeepSeek R1å’ŒOpenAI o3åœ¨å¤æ‚åŒ»ç–—æ¨ç†ä»»åŠ¡ä¸­çš„å“è¶Šæ€§èƒ½ã€‚æ­¤å¤–ï¼Œé«˜çº§åŸºäºæœç´¢çš„ä»£ç†æ–¹æ³•ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”åœ¨æ€§ä»·æ¯”ä¸Šå…·æœ‰æ½œåœ¨ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸åŒæ¨¡å‹å®¶æ—åœ¨å¤æ‚é—®é¢˜ä¸Šçš„æ€§èƒ½å·®è·ï¼Œå¹¶ä¸ºä¸åŒçš„è®¡ç®—çº¦æŸæä¾›äº†æœ€ä½³æ¨¡å‹é€‰æ‹©å»ºè®®ã€‚åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶å¯åœ¨å…¬å¼€è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºé«˜ç»©æ•ˆã€‚</li>
<li>MedAgentsBenchåŸºå‡†æµ‹è¯•æ—¨åœ¨è§£å†³å¤æ‚åŒ»ç–—é—®é¢˜ï¼Œæ¶‰åŠå¤šæ­¥éª¤çš„ä¸´åºŠæ¨ç†ã€è¯Šæ–­åˆ¶å®šå’Œæ²»ç–—è®¡åˆ’ã€‚</li>
<li>MedAgentsBenchè§£å†³äº†ç°æœ‰è¯„ä¼°çš„ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šå…³æ³¨åŸºç¡€æ¨¡å‹å·²è¡¨ç°å‡ºè‰²çš„ç®€å•é—®é¢˜ã€è¯„ä¼°åè®®çš„ä¸ä¸€è‡´æ€§ä»¥åŠç¼ºä¹é’ˆå¯¹æ€§èƒ½ã€æˆæœ¬å’Œæ¨ç†æ—¶é—´çš„ç³»ç»Ÿåˆ†æã€‚</li>
<li>æœ€æ–°æ¨¡å‹DeepSeek R1å’ŒOpenAI o3åœ¨å¤æ‚åŒ»ç–—æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šã€‚</li>
<li>é«˜çº§åŸºäºæœç´¢çš„ä»£ç†æ–¹æ³•åœ¨æ€§ä»·æ¯”ä¸Šå…·æœ‰æ½œåœ¨ä¼˜åŠ¿ã€‚</li>
<li>ä¸åŒæ¨¡å‹å®¶æ—åœ¨å¤æ‚é—®é¢˜ä¸Šçš„æ€§èƒ½å­˜åœ¨å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2372f71d7c08d0c0447848bda43d1852.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77a8eeb27761ffb85c39029465caf05e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-887da8e131c5d782c91256d66dcf6959.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c22a174577809b271182779a14fd292c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eee2f745610bd4ddbbac7581210e7c03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1ce04950da29119dbc5e6ca26832740d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DRESS-Diffusion-Reasoning-based-Reward-Shaping-Scheme-For-Intelligent-Networks"><a href="#DRESS-Diffusion-Reasoning-based-Reward-Shaping-Scheme-For-Intelligent-Networks" class="headerlink" title="DRESS: Diffusion Reasoning-based Reward Shaping Scheme For Intelligent   Networks"></a>DRESS: Diffusion Reasoning-based Reward Shaping Scheme For Intelligent   Networks</h2><p><strong>Authors:Feiran You, Hongyang Du, Xiangwang Hou, Yong Ren, Kaibin Huang</strong></p>
<p>Network optimization remains fundamental in wireless communications, with Artificial Intelligence (AI)-based solutions gaining widespread adoption. As Sixth-Generation (6G) communication networks pursue full-scenario coverage, optimization in complex extreme environments presents unprecedented challenges. The dynamic nature of these environments, combined with physical constraints, makes it difficult for AI solutions such as Deep Reinforcement Learning (DRL) to obtain effective reward feedback for the training process. However, many existing DRL-based network optimization studies overlook this challenge through idealized environment settings. Inspired by the powerful capabilities of Generative AI (GenAI), especially diffusion models, in capturing complex latent distributions, we introduce a novel Diffusion Reasoning-based Reward Shaping Scheme (DRESS) to achieve robust network optimization. By conditioning on observed environmental states and executed actions, DRESS leverages diffusion modelsâ€™ multi-step denoising process as a form of deep reasoning, progressively refining latent representations to generate meaningful auxiliary reward signals that capture patterns of network systems. Moreover, DRESS is designed for seamless integration with any DRL framework, allowing DRESS-aided DRL (DRESSed-DRL) to enable stable and efficient DRL training even under extreme network environments. Experimental results demonstrate that DRESSed-DRL achieves about 1.5x times faster convergence than its original version in sparse-reward wireless environments and significant performance improvements in multiple general DRL benchmark environments compared to baseline methods. The code of DRESS is available at <a target="_blank" rel="noopener" href="https://github.com/NICE-HKU/DRESS">https://github.com/NICE-HKU/DRESS</a>. </p>
<blockquote>
<p>ç½‘ç»œä¼˜åŒ–åœ¨æ— çº¿é€šä¿¡ä¸­ä»æ˜¯åŸºç¡€æ€§çš„å†…å®¹ï¼ŒåŸºäºäººå·¥æ™ºèƒ½çš„è§£å†³æ–¹æ¡ˆæ­£å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚éšç€ç¬¬å…­ä»£ï¼ˆ6Gï¼‰é€šä¿¡ç½‘ç»œè¿½æ±‚å…¨åœºæ™¯è¦†ç›–ï¼Œå¤æ‚æç«¯ç¯å¢ƒä¸­çš„ä¼˜åŒ–é¢ä¸´ç€å‰æ‰€æœªæœ‰çš„æŒ‘æˆ˜ã€‚è¿™äº›ç¯å¢ƒçš„åŠ¨æ€æ€§ä»¥åŠç‰©ç†çº¦æŸä½¿å¾—äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆï¼ˆå¦‚æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰ï¼‰éš¾ä»¥è·å¾—è®­ç»ƒè¿‡ç¨‹çš„æœ‰æ•ˆå¥–åŠ±åé¦ˆã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰çš„åŸºäºDRLçš„ç½‘ç»œä¼˜åŒ–ç ”ç©¶é€šè¿‡ç†æƒ³åŒ–çš„ç¯å¢ƒè®¾ç½®å¿½ç•¥äº†è¿™ä¸€æŒ‘æˆ˜ã€‚å—ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰å¼ºå¤§èƒ½åŠ›çš„å¯å‘ï¼Œå°¤å…¶æ˜¯æ‰©æ•£æ¨¡å‹åœ¨æ•æ‰å¤æ‚æ½œåœ¨åˆ†å¸ƒæ–¹é¢çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºæ‰©æ•£æ¨ç†çš„å¥–åŠ±å¡‘å½¢æ–¹æ¡ˆï¼ˆDRESSï¼‰æ¥å®ç°ç¨³å¥çš„ç½‘ç»œä¼˜åŒ–ã€‚DRESSé€šè¿‡æ ¹æ®è§‚å¯Ÿåˆ°çš„ç¯å¢ƒçŠ¶æ€å’Œæ‰€æ‰§è¡Œçš„åŠ¨ä½œï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¤šæ­¥å»å™ªè¿‡ç¨‹ä½œä¸ºæ·±åº¦æ¨ç†çš„ä¸€ç§å½¢å¼ï¼Œé€æ­¥ä¼˜åŒ–æ½œåœ¨è¡¨ç¤ºï¼Œç”Ÿæˆæœ‰æ„ä¹‰çš„è¾…åŠ©å¥–åŠ±ä¿¡å·ï¼Œæ•æ‰ç½‘ç»œç³»ç»Ÿæ¨¡å¼ã€‚æ­¤å¤–ï¼ŒDRESSè®¾è®¡ç”¨äºæ— ç¼é›†æˆä»»ä½•DRLæ¡†æ¶ï¼Œå…è®¸DRESSè¾…åŠ©çš„DRLï¼ˆDRESSed-DRLï¼‰å³ä½¿åœ¨æç«¯ç½‘ç»œç¯å¢ƒä¸‹ä¹Ÿèƒ½å®ç°ç¨³å®šå’Œé«˜æ•ˆçš„DRLè®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ç¨€ç–å¥–åŠ±çš„æ— çº¿ç¯å¢ƒä¸­ï¼ŒDRESSed-DRLçš„æ”¶æ•›é€Ÿåº¦æ¯”å…¶åŸå§‹ç‰ˆæœ¬å¿«çº¦1.5å€ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªé€šç”¨DRLåŸºå‡†ç¯å¢ƒä¸­ä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚DRESSçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NICE-HKU/DRESS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NICE-HKU/DRESSæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07433v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>éšç€æ— çº¿é€šè®¯ä¸­å¯¹ç½‘ç»œä¼˜åŒ–çš„åŸºç¡€éœ€æ±‚ä¸æ–­å¢é•¿ï¼ŒåŸºäºäººå·¥æ™ºèƒ½çš„è§£å†³æ–¹æ¡ˆæ­£å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚åœ¨è¿½æ±‚å…¨åœºæ™¯è¦†ç›–çš„6Gé€šè®¯ç½‘ç»œä¸­ï¼Œæç«¯å¤æ‚ç¯å¢ƒä¸‹çš„ä¼˜åŒ–å¸¦æ¥äº†å‰æ‰€æœªæœ‰çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³åŠ¨æ€ç¯å¢ƒå’Œç‰©ç†çº¦æŸå¯¹æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰è®­ç»ƒè¿‡ç¨‹çš„å½±å“ï¼Œæœ¬ç ”ç©¶å—ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰æ‰©æ•£æ¨¡å‹çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºæ‰©æ•£æ¨ç†çš„å¥–åŠ±å¡‘å½¢æ–¹æ¡ˆï¼ˆDRESSï¼‰ã€‚DRESSé€šè¿‡è§‚æµ‹ç¯å¢ƒçŠ¶æ€å’Œè¡ŒåŠ¨ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¤šæ­¥å»å™ªè¿‡ç¨‹è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œé€æ­¥ä¼˜åŒ–æ½œåœ¨è¡¨å¾ï¼Œç”Ÿæˆæœ‰æ„ä¹‰çš„è¾…åŠ©å¥–åŠ±ä¿¡å·ï¼Œæ•æ‰ç½‘ç»œç³»ç»Ÿæ¨¡å¼ã€‚DRESSå¯æ— ç¼é›†æˆä»»ä½•DRLæ¡†æ¶ï¼Œä½¿DRESSè¾…åŠ©çš„DRLï¼ˆDRESSed-DRLï¼‰åœ¨æç«¯ç½‘ç»œç¯å¢ƒä¸‹å®ç°ç¨³å®šå’Œé«˜æ•ˆçš„DRLè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¥–åŠ±ç¨€ç–çš„æ— çº¿ç¯å¢ƒä¸­ï¼ŒDRESSed-DRLçš„æ”¶æ•›é€Ÿåº¦æ¯”åŸç‰ˆå¿«çº¦1.5å€ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªé€šç”¨DRLåŸºå‡†ç¯å¢ƒä¸­ä¸åŸºçº¿æ–¹æ³•ç›¸æ¯”å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç½‘ç»œä¼˜åŒ–åœ¨æ— çº¿é€šè®¯ä¸­ä»å…·æœ‰åŸºç¡€æ€§æ„ä¹‰ï¼Œäººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆåœ¨æ­¤é¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚</li>
<li>6Gé€šè®¯ç½‘ç»œåœ¨è¿½æ±‚å…¨åœºæ™¯è¦†ç›–æ—¶ï¼Œåœ¨æç«¯å¤æ‚ç¯å¢ƒä¸‹çš„ä¼˜åŒ–å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰DRLæ–¹æ¡ˆåœ¨åŠ¨æ€ç¯å¢ƒå’Œç‰©ç†çº¦æŸä¸‹è®­ç»ƒè¿‡ç¨‹å—å½±å“ã€‚</li>
<li>ç ”ç©¶æå‡ºåŸºäºæ‰©æ•£æ¨ç†çš„å¥–åŠ±å¡‘å½¢æ–¹æ¡ˆï¼ˆDRESSï¼‰ä»¥è§£å†³é—®é¢˜ã€‚</li>
<li>DRESSåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„å¤šæ­¥å»å™ªè¿‡ç¨‹è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œç”Ÿæˆè¾…åŠ©å¥–åŠ±ä¿¡å·ã€‚</li>
<li>DRESSå¯é›†æˆä»»ä½•DRLæ¡†æ¶ï¼Œå®ç°ç¨³å®šå’Œé«˜æ•ˆçš„è®­ç»ƒã€‚</li>
<li>DRESSed-DRLåœ¨æ— çº¿ç¯å¢ƒä¸­æœ‰æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†ç¯å¢ƒä¸­æ€§èƒ½æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07433">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-98aa57c2cd7c82fb93802b1dea9c7c79.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-739b517339d233fcd52da77913c3f9ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfab5f64d3ef09c847efba565e97ec2c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MM-Eureka-Exploring-Visual-Aha-Moment-with-Rule-based-Large-scale-Reinforcement-Learning"><a href="#MM-Eureka-Exploring-Visual-Aha-Moment-with-Rule-based-Large-scale-Reinforcement-Learning" class="headerlink" title="MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale   Reinforcement Learning"></a>MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale   Reinforcement Learning</h2><p><strong>Authors:Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, Wenqi Shao</strong></p>
<p>We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMsâ€™ reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at <a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-EUREKA">https://github.com/ModalMinds/MM-EUREKA</a> </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†MM-Eurekaï¼Œè¿™æ˜¯ä¸€ä¸ªæˆåŠŸå°†å¤§è§„æ¨¡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚è™½ç„¶åŸºäºè§„åˆ™çš„RLåœ¨æ–‡æœ¬é¢†åŸŸçš„æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆæ•ˆï¼Œä½†å°†å…¶åº”ç”¨äºå¤šæ¨¡æ€åœºæ™¯ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œåœ¨å¤šæ¨¡æ€ç©ºé—´ä¸­å†ç°äº†åŸºäºæ–‡æœ¬çš„RLç³»ç»Ÿï¼ˆå¦‚DeepSeek-R1ï¼‰çš„å…³é”®ç‰¹æ€§ï¼ŒåŒ…æ‹¬ç²¾åº¦å¥–åŠ±å’Œå“åº”é•¿åº¦çš„ç¨³å®šå¢åŠ ï¼Œä»¥åŠåæ€è¡Œä¸ºçš„å‡ºç°ã€‚æˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡åŸºäºè§„åˆ™çš„RLï¼Œæ— è®ºæ˜¯æŒ‡ä»¤è°ƒæ•´æ¨¡å‹è¿˜æ˜¯é¢„è®­ç»ƒæ¨¡å‹ï¼Œéƒ½å¯ä»¥å¼€å‘å‡ºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œè€Œæ— éœ€ç›‘ç£å¾®è°ƒã€‚ä¸æ›¿ä»£æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾ç¤ºå‡ºå“è¶Šçš„æ•°æ®æ•ˆç‡ã€‚æˆ‘ä»¬å…¬å¼€äº†å®Œæ•´çš„ç®¡é“ï¼Œä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚æˆ‘ä»¬å‘å¸ƒæ‰€æœ‰ä»£ç ã€æ¨¡å‹ã€æ•°æ®ç­‰ï¼Œè¯·è®¿é—®<a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-EUREKA">https://github.com/ModalMinds/MM-EUREKA</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07365v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MM-Eurekaæ˜¯ä¸€æ¬¾æˆåŠŸå°†å¤§è§„æ¨¡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†é¢†åŸŸçš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚å®ƒåœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸‹æˆåŠŸå®ç°äº†å…³é”®æ–‡æœ¬RLç³»ç»Ÿçš„ç‰¹æ€§ï¼ŒåŒ…æ‹¬ç²¾åº¦å¥–åŠ±å’Œå“åº”é•¿åº¦çš„ç¨³å®šå¢é•¿ï¼Œä»¥åŠåæ€è¡Œä¸ºçš„å‡ºç°ã€‚ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼Œæ— è®ºæ˜¯æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹è¿˜æ˜¯é¢„è®­ç»ƒæ¨¡å‹éƒ½èƒ½å‘å±•å‡ºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•å…·æœ‰æ›´é«˜çš„æ•°æ®æ•ˆç‡ã€‚æˆ‘ä»¬å…¬å¼€äº†å®Œæ•´çš„ç®¡é“ä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MM-Eurekaæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼ŒæˆåŠŸå°†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†é¢†åŸŸã€‚</li>
<li>MM-Eurekaåœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸‹æˆåŠŸå®ç°äº†å…³é”®æ–‡æœ¬RLç³»ç»Ÿçš„ç‰¹æ€§ï¼Œå¦‚ç²¾åº¦å¥–åŠ±å’Œå“åº”é•¿åº¦çš„ç¨³å®šå¢é•¿ã€‚</li>
<li>MM-Eurekaå±•ç°äº†åæ€è¡Œä¸ºçš„å‡ºç°ï¼Œè¿™æ˜¯å‘æ›´å¤æ‚ã€æ›´é«˜çº§æ¨ç†èƒ½åŠ›è¿ˆè¿›çš„é‡è¦ä¸€æ­¥ã€‚</li>
<li>é€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼Œæ— è®ºæ˜¯æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹è¿˜æ˜¯é¢„è®­ç»ƒæ¨¡å‹éƒ½èƒ½å‘å±•å‡ºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MM-Eurekaåœ¨æ•°æ®æ•ˆç‡ä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œæ— éœ€å¤§é‡æ ‡æ³¨æ•°æ®å³å¯å®ç°è‰¯å¥½çš„æ€§èƒ½ã€‚</li>
<li>MM-Eurekaçš„å®Œæ•´ç®¡é“è¢«å…¬å¼€ï¼Œä¸ºå…¶ä»–ç ”ç©¶è€…æä¾›äº†ç ”ç©¶åŸºç¡€ï¼Œæœ‰åŠ©äºæ¨åŠ¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-30bfc994f3f0e83fb077a6fce0d19121.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-549019e5dd693c6b13757944df229aa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12dd6c3bc1777f64662cee5033f14e5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7e3ce5614970132a79facbee0c409f2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-161eda93751e8d324b4f96f1d402258c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Application-of-Multiple-Chain-of-Thought-in-Contrastive-Reasoning-for-Implicit-Sentiment-Analysis"><a href="#Application-of-Multiple-Chain-of-Thought-in-Contrastive-Reasoning-for-Implicit-Sentiment-Analysis" class="headerlink" title="Application of Multiple Chain-of-Thought in Contrastive Reasoning for   Implicit Sentiment Analysis"></a>Application of Multiple Chain-of-Thought in Contrastive Reasoning for   Implicit Sentiment Analysis</h2><p><strong>Authors:Liwei Yang, Xinying Wang, Xiaotang Zhou, Zhengchao Wu, Ningning Tan</strong></p>
<p>Implicit sentiment analysis aims to uncover emotions that are subtly expressed, often obscured by ambiguity and figurative language. To accomplish this task, large language models and multi-step reasoning are needed to identify those sentiments that are not explicitly stated. In this study, we propose a novel Dual Reverse Chain Reasoning (DRCR) framework to enhance the performance of implicit sentiment analysis. Inspired by deductive reasoning, the framework consists of three key steps: 1) hypothesize an emotional polarity and derive a reasoning process, 2) negate the initial hypothesis and derive a new reasoning process, and 3) contrast the two reasoning paths to deduce the final sentiment polarity. Building on this, we also introduce a Triple Reverse Chain Reasoning (TRCR) framework to address the limitations of random hypotheses. Both methods combine contrastive mechanisms and multi-step reasoning, significantly improving the accuracy of implicit sentiment classification. Experimental results demonstrate that both approaches outperform existing methods across various model scales, achieving state-of-the-art performance. This validates the effectiveness of combining contrastive reasoning and multi-step reasoning for implicit sentiment analysis. </p>
<blockquote>
<p>éšå¼æƒ…æ„Ÿåˆ†ææ—¨åœ¨å‘ç°é‚£äº›å¾®å¦™è¡¨è¾¾çš„æƒ…æ„Ÿï¼Œè¿™äº›æƒ…æ„Ÿé€šå¸¸ç”±äºæ¨¡æ£±ä¸¤å¯å’Œæ¯”å–»è¯­è¨€è€Œéš¾ä»¥å¯Ÿè§‰ã€‚ä¸ºäº†å®Œæˆè¿™é¡¹ä»»åŠ¡ï¼Œéœ€è¦å¤§å‹è¯­è¨€æ¨¡å‹å’Œåˆ†æ­¥æ¨ç†æ¥è¯†åˆ«é‚£äº›æœªè¢«æ˜ç¡®è¡¨è¾¾çš„æƒ…æ„Ÿã€‚åœ¨ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒé‡é€†å‘é“¾æ¨ç†ï¼ˆDRCRï¼‰æ¡†æ¶ï¼Œä»¥æé«˜éšå¼æƒ…æ„Ÿåˆ†æçš„æ€§èƒ½ã€‚å—æ¼”ç»æ¨ç†çš„å¯å‘ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼š1ï¼‰å‡è®¾æƒ…æ„Ÿææ€§å¹¶å¾—å‡ºæ¨ç†è¿‡ç¨‹ï¼›2ï¼‰å¦å®šåˆå§‹å‡è®¾å¹¶å¾—å‡ºæ–°çš„æ¨ç†è¿‡ç¨‹ï¼›3ï¼‰å¯¹æ¯”ä¸¤æ¡æ¨ç†è·¯å¾„æ¥æ¨æ–­æœ€ç»ˆçš„æƒ…æ„Ÿææ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸‰é‡é€†å‘é“¾æ¨ç†ï¼ˆTRCRï¼‰æ¡†æ¶æ¥è§£å†³éšæœºå‡è®¾çš„å±€é™æ€§é—®é¢˜ã€‚è¿™ä¸¤ç§æ–¹æ³•éƒ½ç»“åˆäº†å¯¹æ¯”æœºåˆ¶å’Œåˆ†æ­¥æ¨ç†ï¼Œå¤§å¤§æé«˜äº†éšå¼æƒ…æ„Ÿåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•åœ¨å„ç§æ¨¡å‹è§„æ¨¡ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚è¿™éªŒè¯äº†å°†å¯¹æ¯”æ¨ç†å’Œå¤šæ­¥æ¨ç†ç›¸ç»“åˆç”¨äºéšå¼æƒ…æ„Ÿåˆ†æçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07140v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšå¼æƒ…æ„Ÿåˆ†ææ—¨åœ¨æ­ç¤ºé€šè¿‡æ¨¡ç³Šå’Œéšå–»è¯­è¨€è¡¨è¾¾çš„æƒ…æ„Ÿã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§æ–°å‹çš„åŒé‡é€†å‘é“¾æ¨ç†ï¼ˆDRCRï¼‰æ¡†æ¶ï¼Œä»¥æé«˜éšå¼æƒ…æ„Ÿåˆ†æçš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶ç”±ä¸‰ä¸ªå…³é”®æ­¥éª¤ç»„æˆï¼šå‡è®¾æƒ…æ„Ÿææ€§å¹¶æ¨å¯¼æ¨ç†è¿‡ç¨‹ã€å¦å®šåˆå§‹å‡è®¾å¹¶æ¨å¯¼æ–°çš„æ¨ç†è¿‡ç¨‹ã€å¯¹æ¯”ä¸¤æ¡æ¨ç†è·¯å¾„æ¥æ¨æ–­æœ€ç»ˆçš„æƒ…æ„Ÿææ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè¿˜å¼•å…¥äº†ä¸‰é‡é€†å‘é“¾æ¨ç†ï¼ˆTRCRï¼‰æ¡†æ¶ï¼Œä»¥è§£å†³éšæœºå‡è®¾çš„å±€é™æ€§ã€‚ä¸¤ç§æ–¹æ³•ç»“åˆå¯¹æ¯”æœºåˆ¶å’Œå¤šæ­¥æ¨ç†ï¼Œæ˜¾è‘—æé«˜éšå¼æƒ…æ„Ÿåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•åœ¨å„ç§æ¨¡å‹è§„æ¨¡ä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ï¼ŒéªŒè¯äº†ç»“åˆå¯¹æ¯”æ¨ç†å’Œå¤šæ­¥æ¨ç†è¿›è¡Œéšå¼æƒ…æ„Ÿåˆ†æçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éšå¼æƒ…æ„Ÿåˆ†æçš„ç›®æ ‡æ˜¯è¯†åˆ«é€šè¿‡æ¨¡ç³Šå’Œéšå–»è¯­è¨€è¡¨è¾¾çš„æƒ…æ„Ÿã€‚</li>
<li>åŒé‡é€†å‘é“¾æ¨ç†ï¼ˆDRCRï¼‰æ¡†æ¶ç”¨äºæé«˜éšå¼æƒ…æ„Ÿåˆ†æçš„æ€§èƒ½ã€‚</li>
<li>DRCRæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šå‡è®¾æƒ…æ„Ÿææ€§ã€å¦å®šåˆå§‹å‡è®¾ã€å¯¹æ¯”ä¸¤æ¡æ¨ç†è·¯å¾„ã€‚</li>
<li>ä¸ºäº†è§£å†³éšæœºå‡è®¾çš„å±€é™æ€§ï¼Œå¼•å…¥äº†ä¸‰é‡é€†å‘é“¾æ¨ç†ï¼ˆTRCRï¼‰æ¡†æ¶ã€‚</li>
<li>TRCRå’ŒDRCRç»“åˆå¯¹æ¯”æœºåˆ¶å’Œé€»è¾‘æ¨ç†ï¼Œå¢å¼ºäº†éšå¼æƒ…æ„Ÿåˆ†ç±»çš„å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°æœ€æ–°æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-592fa7e0ec2e74d2ae6574322b57b14b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="On-the-Generalization-of-Representation-Uncertainty-in-Earth-Observation"><a href="#On-the-Generalization-of-Representation-Uncertainty-in-Earth-Observation" class="headerlink" title="On the Generalization of Representation Uncertainty in Earth Observation"></a>On the Generalization of Representation Uncertainty in Earth Observation</h2><p><strong>Authors:Spyros Kondylatos, Nikolaos Ioannis Bountos, Dimitrios Michail, Xiao Xiang Zhu, Gustau Camps-Valls, Ioannis Papoutsis</strong></p>
<p>Recent advances in Computer Vision have introduced the concept of pretrained representation uncertainty, enabling zero-shot uncertainty estimation. This holds significant potential for Earth Observation (EO), where trustworthiness is critical, yet the complexity of EO data poses challenges to uncertainty-aware methods. In this work, we investigate the generalization of representation uncertainty in EO, considering the domainâ€™s unique semantic characteristics. We pretrain uncertainties on large EO datasets and propose an evaluation framework to assess their zero-shot performance in multi-label classification and segmentation EO tasks. Our findings reveal that, unlike uncertainties pretrained on natural images, EO-pretraining exhibits strong generalization across unseen EO domains, geographic locations, and target granularities, while maintaining sensitivity to variations in ground sampling distance. We demonstrate the practical utility of pretrained uncertainties showcasing their alignment with task-specific uncertainties in downstream tasks, their sensitivity to real-world EO image noise, and their ability to generate spatial uncertainty estimates out-of-the-box. Initiating the discussion on representation uncertainty in EO, our study provides insights into its strengths and limitations, paving the way for future research in the field. Code and weights are available at: <a target="_blank" rel="noopener" href="https://github.com/Orion-AI-Lab/EOUncertaintyGeneralization">https://github.com/Orion-AI-Lab/EOUncertaintyGeneralization</a>. </p>
<blockquote>
<p>è¿‘æœŸè®¡ç®—æœºè§†è§‰çš„è¿›æ­¥å¼•å…¥äº†é¢„è®­ç»ƒè¡¨ç¤ºä¸ç¡®å®šæ€§çš„æ¦‚å¿µï¼Œå®ç°äº†é›¶æ ·æœ¬ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚è¿™åœ¨åœ°çƒè§‚æµ‹ï¼ˆEOï¼‰ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œä¿¡ä»»åº¦åœ¨åœ°çƒè§‚æµ‹ä¸­è‡³å…³é‡è¦ï¼Œç„¶è€Œåœ°çƒè§‚æµ‹æ•°æ®çš„å¤æ‚æ€§ç»™å…·æœ‰æ„ŸçŸ¥ä¸ç¡®å®šæ€§çš„æ–¹æ³•å¸¦æ¥äº†æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è¡¨ç¤ºä¸ç¡®å®šåœ¨åœ°çƒè§‚æµ‹ä¸­çš„é€šç”¨æ€§ï¼Œå¹¶è€ƒè™‘äº†è¯¥é¢†åŸŸç‹¬ç‰¹çš„è¯­ä¹‰ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨å¤§å‹åœ°çƒè§‚æµ‹æ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸ç¡®å®šæ€§é¢„è®­ç»ƒï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°å®ƒä»¬åœ¨å¤šæ ‡ç­¾åˆ†ç±»å’Œåˆ†æ®µåœ°çƒè§‚æµ‹ä»»åŠ¡ä¸­çš„é›¶æ ·æœ¬è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸åœ¨è‡ªç„¶å›¾åƒä¸Šé¢„è®­ç»ƒçš„ä¸ç¡®å®šæ€§ä¸åŒï¼Œåœ°çƒè§‚æµ‹çš„é¢„è®­ç»ƒä¸ç¡®å®šæ€§åœ¨æœªè§è¿‡çš„åœ°çƒè§‚æµ‹é¢†åŸŸã€åœ°ç†ä½ç½®å’Œç›®æ ‡ç²’åº¦ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é€šç”¨æ€§ï¼ŒåŒæ—¶ä¿æŒå¯¹åœ°é‡‡æ ·è·ç¦»å˜åŒ–çš„æ•æ„Ÿæ€§ã€‚æˆ‘ä»¬å±•ç¤ºäº†é¢„è®­ç»ƒä¸ç¡®å®šæ€§çš„å®é™…æ•ˆç”¨ï¼Œå±•ç¤ºå…¶ä¸ä¸‹æ¸¸ä»»åŠ¡ä¸­ä»»åŠ¡ç‰¹å®šä¸ç¡®å®šæ€§çš„å»åˆã€å¯¹ç°å®ä¸–ç•Œåœ°çƒè§‚æµ‹å›¾åƒå™ªå£°çš„æ•æ„Ÿæ€§ä»¥åŠèƒ½å¤Ÿç”Ÿæˆç©ºé—´ä¸ç¡®å®šæ€§ä¼°è®¡çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ç‡å…ˆè®¨è®ºäº†åœ°çƒè§‚æµ‹ä¸­çš„è¡¨ç¤ºä¸ç¡®å®šæ€§é—®é¢˜ï¼Œæœ¬ç ”ç©¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ·±åˆ»çš„è§è§£å’Œé“ºå¹³äº†é“è·¯ã€‚ä»£ç å’Œæƒé‡å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/Orion-AI-Lab/EOUncertaintyGeneralization">Orion AIå®éªŒå®¤çš„ç½‘ç«™é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07082v1">PDF</a> 18 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é¢„è®­ç»ƒè¡¨ç¤ºä¸ç¡®å®šæ€§åœ¨åœ°çƒè§‚æµ‹ï¼ˆEOï¼‰é¢†åŸŸçš„åº”ç”¨ã€‚æ–‡ç« ä»‹ç»äº†é¢„è®­ç»ƒä¸ç¡®å®šæ€§åœ¨EOæ•°æ®ä¸Šçš„ç ”ç©¶ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°å…¶åœ¨å¤šæ ‡ç­¾åˆ†ç±»å’Œåˆ†å‰²EOä»»åŠ¡ä¸­çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œç›¸è¾ƒäºåœ¨è‡ªç„¶å›¾åƒä¸Šé¢„è®­ç»ƒçš„ä¸ç¡®å®šæ€§æ¨¡å‹ï¼ŒEOé¢†åŸŸçš„é¢„è®­ç»ƒä¸ç¡®å®šæ€§å±•ç°å‡ºæ›´å¼ºçš„è·¨æœªè§EOé¢†åŸŸã€åœ°ç†ä½ç½®å’Œç›®æ ‡ç²’åº¦çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶å¯¹åœ°é¢é‡‡æ ·è·ç¦»çš„å˜åŒ–ä¿æŒæ•æ„Ÿã€‚è¯¥ç ”ç©¶ä¸ºåœ°çƒè§‚æµ‹é¢†åŸŸå¼•å…¥é¢„è®­ç»ƒä¸ç¡®å®šæ€§çš„è®¨è®ºï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè¡¨ç¤ºä¸ç¡®å®šæ€§çš„æ¦‚å¿µè¢«å¼•å…¥åœ°çƒè§‚æµ‹ï¼ˆEOï¼‰é¢†åŸŸã€‚</li>
<li>åœ¨EOæ•°æ®ä¸Šé¢„è®­ç»ƒçš„ä¸ç¡®å®šæ€§æ¨¡å‹å±•ç°å‡ºå¼ºæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>EOé¢„è®­ç»ƒçš„ä¸ç¡®å®šæ€§æ¨¡å‹èƒ½åœ¨å¤šæ ‡ç­¾åˆ†ç±»å’Œåˆ†å‰²ä»»åŠ¡ä¸­æœ‰æ•ˆè¯„ä¼°é›¶æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>EOé¢„è®­ç»ƒçš„ä¸ç¡®å®šæ€§æ¨¡å‹å¯¹åœ°é¢é‡‡æ ·è·ç¦»çš„å˜åŒ–æ•æ„Ÿã€‚</li>
<li>å±•ç¤ºé¢„è®­ç»ƒä¸ç¡®å®šæ€§åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å®ç”¨æ€§ï¼Œä¸ä»»åŠ¡ç‰¹å®šä¸ç¡®å®šæ€§å¯¹é½ï¼Œå¯¹ç°å®ä¸–ç•ŒEOå›¾åƒå™ªå£°æ•æ„Ÿï¼Œå¹¶èƒ½ç”Ÿæˆç©ºé—´ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚</li>
<li>ç ”ç©¶æä¾›äº†é¢„è®­ç»ƒè¡¨ç¤ºä¸ç¡®å®šæ€§çš„ä¼˜åŠ¿å’Œå±€é™æ€§çš„è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07082">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b48e149d2eb1e48f25ac067f959d218.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04bfd54ae4e6b498f4caeec3b597ccb3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cf1ee294f956c2f6de9a35611f99956.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Boosting-the-Generalization-and-Reasoning-of-Vision-Language-Models-with-Curriculum-Reinforcement-Learning"><a href="#Boosting-the-Generalization-and-Reasoning-of-Vision-Language-Models-with-Curriculum-Reinforcement-Learning" class="headerlink" title="Boosting the Generalization and Reasoning of Vision Language Models with   Curriculum Reinforcement Learning"></a>Boosting the Generalization and Reasoning of Vision Language Models with   Curriculum Reinforcement Learning</h2><p><strong>Authors:Huilin Deng, Ding Zou, Rui Ma, Hongchen Luo, Yang Cao, Yu Kang</strong></p>
<p>While state-of-the-art vision-language models (VLMs) have demonstrated remarkable capabilities in complex visual-text tasks, their success heavily relies on massive model scaling, limiting their practical deployment. Small-scale VLMs offer a more practical alternative but face significant challenges when trained with traditional supervised fine-tuning (SFT), particularly in two aspects: out-of-domain (OOD) generalization and reasoning abilities, which significantly lags behind the contemporary Large language models (LLMs). To address these challenges, we propose Curriculum Reinforcement Finetuning (Curr-ReFT), a novel post-training paradigm specifically designed for small-scale VLMs. Inspired by the success of reinforcement learning in LLMs, Curr-ReFT comprises two sequential stages: (1) Curriculum Reinforcement Learning, which ensures steady progression of model capabilities through difficulty-aware reward design, transitioning from basic visual perception to complex reasoning tasks; and (2) Rejected Sampling-based Self-improvement, which maintains the fundamental capabilities of VLMs through selective learning from high-quality multimodal and language examples. Extensive experiments demonstrate that models trained with Curr-ReFT paradigm achieve state-of-the-art performance across various visual tasks in both in-domain and out-of-domain settings. Moreover, our Curr-ReFT enhanced 3B model matches the performance of 32B-parameter models, demonstrating that efficient training paradigms can effectively bridge the gap between small and large models. </p>
<blockquote>
<p>è™½ç„¶æœ€å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤æ‚çš„è§†è§‰æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬çš„æˆåŠŸä¸¥é‡ä¾èµ–äºå¤§è§„æ¨¡çš„æ¨¡å‹æ‰©å±•ï¼Œé™åˆ¶äº†å…¶å®é™…éƒ¨ç½²ã€‚å°è§„æ¨¡VLMsæä¾›äº†æ›´å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ä½¿ç”¨ä¼ ç»Ÿçš„æœ‰ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œè®­ç»ƒæ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸¤ä¸ªæ–¹é¢ï¼šåŸŸå¤–ï¼ˆOODï¼‰æ¨å¹¿èƒ½åŠ›å’Œæ¨ç†èƒ½åŠ›ï¼Œè¿™ä¸å½“ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸æ¯”å­˜åœ¨æ˜æ˜¾å·®è·ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†è¯¾ç¨‹å¼ºåŒ–å¾®è°ƒï¼ˆCurr-ReFTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºå°è§„æ¨¡VLMsè®¾è®¡çš„æ–°å‹åè®­ç»ƒèŒƒå¼ã€‚å—å¼ºåŒ–å­¦ä¹ åœ¨LLMsä¸­æˆåŠŸçš„å¯å‘ï¼ŒCurr-ReFTåŒ…å«ä¸¤ä¸ªè¿ç»­çš„é˜¶æ®µï¼šï¼ˆ1ï¼‰è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡éš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡ç¡®ä¿æ¨¡å‹èƒ½åŠ›çš„ç¨³å®šè¿›æ­¥ï¼Œä»åŸºæœ¬çš„è§†è§‰æ„ŸçŸ¥è¿‡æ¸¡åˆ°å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼›ï¼ˆ2ï¼‰åŸºäºæ‹’ç»é‡‡æ ·çš„è‡ªæˆ‘æ”¹è¿›ï¼Œé€šè¿‡é€‰æ‹©æ€§å­¦ä¹ é«˜è´¨é‡çš„å¤šæ¨¡æ€å’Œè¯­è¨€ç¤ºä¾‹ï¼Œä¿æŒVLMsçš„åŸºæœ¬èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨Curr-ReFTèŒƒå¼è®­ç»ƒçš„æ¨¡å‹åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­çš„åŸŸå†…å’ŒåŸŸå¤–è®¾ç½®ä¸­å‡è¾¾åˆ°æœ€æ–°æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡‡ç”¨Curr-ReFTå¢å¼ºçš„3Bæ¨¡å‹ä¸32Bå‚æ•°æ¨¡å‹çš„æ€§èƒ½ç›¸åŒ¹é…ï¼Œè¡¨æ˜æœ‰æ•ˆçš„è®­ç»ƒèŒƒå¼å¯ä»¥æœ‰æ•ˆåœ°ç¼©å°å°å‹å’Œå¤§å‹æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07065v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤æ‚çš„è§†è§‰æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å…¶æˆåŠŸä¸¥é‡ä¾èµ–äºå¤§è§„æ¨¡çš„æ¨¡å‹æ‰©å±•ï¼Œé™åˆ¶äº†å…¶å®è·µéƒ¨ç½²ã€‚å°è§„æ¨¡VLMsè™½ç„¶æä¾›äº†æ›´å®ç”¨çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†åœ¨ä½¿ç”¨ä¼ ç»Ÿçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œè®­ç»ƒæ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸŸå¤–ï¼ˆOODï¼‰æ³›åŒ–å’Œæ¨ç†èƒ½åŠ›æ–¹é¢ï¼Œè¿œè¿œè½åäºå½“ä»£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†è¯¾ç¨‹å¼ºåŒ–å¾®è°ƒï¼ˆCurr-ReFTï¼‰è¿™ä¸€æ–°å‹çš„åè®­ç»ƒèŒƒå¼ï¼Œä¸“ä¸ºå°è§„æ¨¡VLMsè®¾è®¡ã€‚Curr-ReFTç”±ä¸¤ä¸ªé¡ºåºé˜¶æ®µç»„æˆï¼šé¦–å…ˆæ˜¯è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡éš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡ç¡®ä¿æ¨¡å‹èƒ½åŠ›ç¨³æ­¥æé«˜ï¼Œä»åŸºæœ¬çš„è§†è§‰æ„ŸçŸ¥è¿‡æ¸¡åˆ°å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼›ç„¶åæ˜¯æ‹’ç»é‡‡æ ·è‡ªæˆ‘æ”¹è¿›ï¼Œé€šè¿‡é€‰æ‹©æ€§å­¦ä¹ é«˜è´¨é‡çš„å¤šæ¨¡æ€å’Œè¯­è¨€ç¤ºä¾‹æ¥ä¿æŒVLMsçš„åŸºæœ¬èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œé‡‡ç”¨Curr-ReFTè®­ç»ƒçš„æ¨¡å‹åœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œæ— è®ºå¤„äºå†…åŸŸè¿˜æ˜¯å¤–åŸŸç¯å¢ƒä¸­å‡è¡¨ç°çªå‡ºã€‚æ­¤å¤–ï¼Œä½¿ç”¨Curr-ReFTå¢å¼ºçš„3Bæ¨¡å‹ç”šè‡³è¾¾åˆ°äº†32Bå‚æ•°æ¨¡å‹çš„æ€§èƒ½æ°´å¹³ï¼Œè¯æ˜äº†æœ‰æ•ˆè®­ç»ƒèŒƒå¼å¯ä»¥æœ‰æ•ˆç¼©å°å¤§å°æ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å…ˆè¿›çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å¤§è§„æ¨¡æ¨¡å‹æ‰©å±•é™åˆ¶äº†å…¶å®è·µéƒ¨ç½²ã€‚</li>
<li>å°è§„æ¨¡VLMsé¢ä¸´ä¼ ç»Ÿç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸŸå¤–ï¼ˆOODï¼‰æ³›åŒ–å’Œæ¨ç†èƒ½åŠ›æ–¹é¢ã€‚</li>
<li>æå‡ºäº†è¯¾ç¨‹å¼ºåŒ–å¾®è°ƒï¼ˆCurr-ReFTï¼‰è¿™ä¸€æ–°å‹åè®­ç»ƒèŒƒå¼ï¼Œä¸“ä¸ºå°è§„æ¨¡VLMsè®¾è®¡ã€‚</li>
<li>Curr-ReFTåŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šè¯¾ç¨‹å¼ºåŒ–å­¦ä¹ å’Œæ‹’ç»é‡‡æ ·è‡ªæˆ‘æ”¹è¿›ã€‚</li>
<li>è¯¾ç¨‹å¼ºåŒ–å­¦ä¹ é€šè¿‡éš¾åº¦æ„ŸçŸ¥å¥–åŠ±è®¾è®¡æé«˜æ¨¡å‹èƒ½åŠ›ã€‚</li>
<li>æ‹’ç»é‡‡æ ·è‡ªæˆ‘æ”¹è¿›é€šè¿‡é€‰æ‹©æ€§å­¦ä¹ é«˜è´¨é‡ç¤ºä¾‹æ¥ä¿æŒVLMsçš„åŸºæœ¬èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07065">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4434c914cded1407479db447ef57d26d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8e67e29d6df8203527ea4855b6c29a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f515656d0934da60955fe4782e8872e1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1c38b153eeb7cccd7439f6a8c5b1ead.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-960a449709ff575ca779ad1c33a29a8e.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Multimodal-Human-AI-Synergy-for-Medical-Imaging-Quality-Control-A-Hybrid-Intelligence-Framework-with-Adaptive-Dataset-Curation-and-Closed-Loop-Evaluation"><a href="#Multimodal-Human-AI-Synergy-for-Medical-Imaging-Quality-Control-A-Hybrid-Intelligence-Framework-with-Adaptive-Dataset-Curation-and-Closed-Loop-Evaluation" class="headerlink" title="Multimodal Human-AI Synergy for Medical Imaging Quality Control: A   Hybrid Intelligence Framework with Adaptive Dataset Curation and Closed-Loop   Evaluation"></a>Multimodal Human-AI Synergy for Medical Imaging Quality Control: A   Hybrid Intelligence Framework with Adaptive Dataset Curation and Closed-Loop   Evaluation</h2><p><strong>Authors:Zhi Qin, Qianhui Gui, Mouxiao Bian, Rui Wang, Hong Ge, Dandan Yao, Ziying Sun, Yuan Zhao, Yu Zhang, Hui Shi, Dongdong Wang, Chenxin Song, Shenghong Ju, Lihao Liu, Junjun He, Jie Xu, Yuan-Cheng Wang</strong></p>
<p>Medical imaging quality control (QC) is essential for accurate diagnosis, yet traditional QC methods remain labor-intensive and subjective. To address this challenge, in this study, we establish a standardized dataset and evaluation framework for medical imaging QC, systematically assessing large language models (LLMs) in image quality assessment and report standardization. Specifically, we first constructed and anonymized a dataset of 161 chest X-ray (CXR) radiographs and 219 CT reports for evaluation. Then, multiple LLMs, including Gemini 2.0-Flash, GPT-4o, and DeepSeek-R1, were evaluated based on recall, precision, and F1 score to detect technical errors and inconsistencies. Experimental results show that Gemini 2.0-Flash achieved a Macro F1 score of 90 in CXR tasks, demonstrating strong generalization but limited fine-grained performance. DeepSeek-R1 excelled in CT report auditing with a 62.23% recall rate, outperforming other models. However, its distilled variants performed poorly, while InternLM2.5-7B-chat exhibited the highest additional discovery rate, indicating broader but less precise error detection. These findings highlight the potential of LLMs in medical imaging QC, with DeepSeek-R1 and Gemini 2.0-Flash demonstrating superior performance. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒè´¨é‡æ§åˆ¶ï¼ˆQCï¼‰å¯¹äºå‡†ç¡®è¯Šæ–­è‡³å…³é‡è¦ï¼Œç„¶è€Œä¼ ç»Ÿçš„QCæ–¹æ³•ä»ç„¶åŠ³åŠ¨å¼ºåº¦é«˜ä¸”ä¸»è§‚æ€§è¾ƒå¼ºã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶å»ºç«‹äº†ä¸€å¥—æ ‡å‡†åŒ–çš„æ•°æ®é›†å’ŒåŒ»å­¦å½±åƒQCè¯„ä¼°æ¡†æ¶ï¼Œç³»ç»Ÿåœ°è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å›¾åƒè´¨é‡è¯„ä¼°å’ŒæŠ¥å‘Šæ ‡å‡†åŒ–æ–¹é¢çš„è¡¨ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºå¹¶åŒ¿ååŒ–äº†ä¸€ä¸ªåŒ…å«161å¼ èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰ç…§ç‰‡å’Œ219ä»½CTæŠ¥å‘Šçš„æ•°æ®é›†è¿›è¡Œè¯„ä¼°ã€‚ç„¶åï¼ŒåŸºäºå¬å›ç‡ã€ç²¾ç¡®åº¦å’ŒF1åˆ†æ•°ï¼Œå¯¹åŒ…æ‹¬Gemini 2.0-Flashã€GPT-4oå’ŒDeepSeek-R1åœ¨å†…çš„å¤šä¸ªLLMè¿›è¡Œäº†æŠ€æœ¯é”™è¯¯å’Œä¸ä¸€è‡´æ€§æ£€æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGemini 2.0-Flashåœ¨CXRä»»åŠ¡ä¸­çš„å®F1åˆ†æ•°è¾¾åˆ°90ï¼Œè¡¨ç°å‡ºè¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†ç²¾ç»†ç²’åº¦æ€§èƒ½æœ‰é™ã€‚DeepSeek-R1åœ¨CTæŠ¥å‘Šå®¡æ ¸æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¬å›ç‡è¾¾åˆ°62.23%ï¼Œè¶…è¿‡äº†å…¶ä»–æ¨¡å‹ã€‚ç„¶è€Œï¼Œå…¶è’¸é¦å˜ä½“è¡¨ç°ä¸ä½³ï¼Œè€ŒInternLM2.5-7B-chatå±•ç°å‡ºæœ€é«˜çš„é¢å¤–å‘ç°ç‡ï¼Œè¡¨æ˜å…¶é”™è¯¯æ£€æµ‹èŒƒå›´æ›´å¹¿ä½†ç²¾ç¡®åº¦è¾ƒä½ã€‚è¿™äº›å‘ç°çªæ˜¾äº†LLMåœ¨åŒ»å­¦å½±åƒQCä¸­çš„æ½œåŠ›ï¼Œå…¶ä¸­DeepSeek-R1å’ŒGemini 2.0-Flashè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07032v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶å»ºç«‹åŒ»å­¦æˆåƒè´¨é‡æ§åˆ¶ï¼ˆQCï¼‰çš„æ ‡å‡†æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ï¼Œä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å›¾åƒè´¨é‡è¯„ä¼°å’ŒæŠ¥å‘Šæ ‡å‡†åŒ–æ–¹é¢çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGemini 2.0-Flashåœ¨CXRä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒDeepSeek-R1åœ¨CTæŠ¥å‘Šå®¡æ ¸ä¸­å…·æœ‰é«˜å¬å›ç‡å¹¶è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æ­¤ç ”ç©¶çªæ˜¾äº†LLMsåœ¨åŒ»å­¦æˆåƒQCä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å»ºç«‹åŒ»å­¦æˆåƒQCçš„æ ‡å‡†æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ã€‚</li>
<li>é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå›¾åƒè´¨é‡è¯„ä¼°å’ŒæŠ¥å‘Šæ ‡å‡†åŒ–è¯„ä¼°ã€‚</li>
<li>Gemini 2.0-Flashåœ¨CXRä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰å¼ºçš„æ³›åŒ–èƒ½åŠ›ä½†ç²¾ç»†ç²’åº¦æ€§èƒ½æœ‰é™ã€‚</li>
<li>DeepSeek-R1åœ¨CTæŠ¥å‘Šå®¡æ ¸ä¸­è¡¨ç°å‡ºé«˜å¬å›ç‡å’Œå“è¶Šæ€§èƒ½ã€‚</li>
<li>DeepSeek-R1çš„è’¸é¦å˜ä½“è¡¨ç°ä¸ä½³ï¼Œè€ŒInternLM2.5-7B-chatå…·æœ‰æ›´å¹¿æ³›çš„é”™è¯¯æ£€æµ‹èƒ½åŠ›ä½†ç²¾ç¡®åº¦è¾ƒä½ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦æˆåƒQCä¸­çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5e73cd5198e7857542fcc528d14e2f8b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c7dc12b8913f18a800fb426c7ea8c4ae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97086753f39879d7dd051c2d37d8c93b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f161d66af0dcfed4163e75d36e7edcc6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9649930ce86a23ce8395f24baee82868.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Query-Optimization-Method-Utilizing-Large-Language-Models"><a href="#A-Query-Optimization-Method-Utilizing-Large-Language-Models" class="headerlink" title="A Query Optimization Method Utilizing Large Language Models"></a>A Query Optimization Method Utilizing Large Language Models</h2><p><strong>Authors:Zhiming Yao, Haoyang Li, Jing Zhang, Cuiping Li, Hong Chen</strong></p>
<p>Query optimization is a critical task in database systems, focused on determining the most efficient way to execute a query from an enormous set of possible strategies. Traditional approaches rely on heuristic search methods and cost predictions, but these often struggle with the complexity of the search space and inaccuracies in performance estimation, leading to suboptimal plan choices. This paper presents LLMOpt, a novel framework that leverages Large Language Models (LLMs) to address these challenges through two innovative components: (1) LLM for Plan Candidate Generation (LLMOpt(G)), which eliminates heuristic search by utilizing the reasoning abilities of LLMs to directly generate high-quality query plans, and (2) LLM for Plan Candidate Selection (LLMOpt(S)), a list-wise cost model that compares candidates globally to enhance selection accuracy. To adapt LLMs for query optimization, we propose fine-tuning pre-trained models using optimization data collected offline. Experimental results on the JOB, JOB-EXT, and Stack benchmarks show that LLMOpt(G) and LLMOpt(S) outperform state-of-the-art methods, including PostgreSQL, BAO, and HybridQO. Notably, LLMOpt(S) achieves the best practical performance, striking a balance between plan quality and inference efficiency. </p>
<blockquote>
<p>æŸ¥è¯¢ä¼˜åŒ–æ˜¯æ•°æ®åº“ç³»ç»Ÿä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ï¼Œä¸»è¦å…³æ³¨äºä»å¤§é‡å¯èƒ½çš„ç­–ç•¥ä¸­ç¡®å®šæ‰§è¡ŒæŸ¥è¯¢çš„æœ€æœ‰æ•ˆæ–¹å¼ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¯å‘å¼æœç´¢æ–¹æ³•å’Œæˆæœ¬é¢„æµ‹ï¼Œä½†è¿™äº›æ–¹æ³•åœ¨å¤æ‚çš„æœç´¢ç©ºé—´å’Œæ€§èƒ½ä¼°è®¡çš„ä¸å‡†ç¡®æ€§æ–¹é¢å¸¸å¸¸é‡åˆ°æŒ‘æˆ˜ï¼Œä»è€Œå¯¼è‡´é€‰æ‹©æ¬¡ä¼˜è®¡åˆ’ã€‚æœ¬æ–‡æå‡ºäº†LLMOptè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªåˆ›æ–°ç»„ä»¶ï¼šï¼ˆ1ï¼‰LLMè®¡åˆ’å€™é€‰ç”Ÿæˆï¼ˆLLMOptï¼ˆGï¼‰ï¼‰ï¼Œå®ƒåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ç›´æ¥ç”Ÿæˆé«˜è´¨é‡æŸ¥è¯¢è®¡åˆ’ï¼Œä»è€Œæ¶ˆé™¤äº†å¯å‘å¼æœç´¢ï¼›ï¼ˆ2ï¼‰LLMè®¡åˆ’å€™é€‰é€‰æ‹©ï¼ˆLLMOptï¼ˆSï¼‰ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ—è¡¨çº§æˆæœ¬æ¨¡å‹ï¼Œå¯åœ¨å…¨å±€èŒƒå›´å†…æ¯”è¾ƒå€™é€‰è®¡åˆ’ï¼Œä»¥æé«˜é€‰æ‹©å‡†ç¡®æ€§ã€‚ä¸ºäº†å°†å¤§å‹è¯­è¨€æ¨¡å‹é€‚åº”äºæŸ¥è¯¢ä¼˜åŒ–ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡ç¦»çº¿æ”¶é›†çš„ä¼˜åŒ–æ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚åœ¨JOBã€JOB-EXTå’ŒStackåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLLMOptï¼ˆGï¼‰å’ŒLLMOptï¼ˆSï¼‰çš„æ€§èƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ï¼ŒåŒ…æ‹¬PostgreSQLã€BAOå’ŒHybridQOã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLLMOptï¼ˆSï¼‰åœ¨å®é™…æ€§èƒ½ä¸Šè¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ï¼Œåœ¨è®¡åˆ’è´¨é‡å’Œæ¨ç†æ•ˆç‡ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06902v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–æ˜¯æ•°æ®åº“ç³»ç»Ÿä¸­çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä¾èµ–äºå¯å‘å¼æœç´¢æ–¹æ³•å’Œæˆæœ¬é¢„æµ‹ï¼Œä½†å­˜åœ¨æœç´¢ç©ºé—´å¤æ‚å’Œæ€§èƒ½ä¼°ç®—ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œå¯¼è‡´è®¡åˆ’é€‰æ‹©ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶LLMOptï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³è¿™äº›é—®é¢˜ã€‚å®ƒåŒ…æ‹¬ä¸¤ä¸ªåˆ›æ–°ç»„ä»¶ï¼šLLMOpt(G)ç›´æ¥åˆ©ç”¨LLMçš„æ¨ç†èƒ½åŠ›ç”Ÿæˆé«˜è´¨é‡æŸ¥è¯¢è®¡åˆ’ï¼Œæ¶ˆé™¤å¯å‘å¼æœç´¢ï¼›LLMOpt(S)æ˜¯æ¯”è¾ƒå€™é€‰è®¡åˆ’å…¨å±€æˆæœ¬çš„åˆ—è¡¨æ¨¡å‹ï¼Œæé«˜é€‰æ‹©å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒLLMOpt(G)å’ŒLLMOpt(S)ä¼˜äºå½“å‰æœ€ä¼˜æ–¹æ³•ï¼ŒåŒ…æ‹¬PostgreSQLã€BAOå’ŒHybridQOã€‚å°¤å…¶æ˜¯LLMOpt(S)åœ¨å®é™…æ€§èƒ½ä¸Šè¡¨ç°æœ€ä½³ï¼Œåœ¨è®¡åˆ’è´¨é‡å’Œæ¨ç†æ•ˆç‡ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŸ¥è¯¢ä¼˜åŒ–åœ¨æ•°æ®åº“ç³»ç»Ÿä¸­è‡³å…³é‡è¦ï¼Œç›®æ ‡æ˜¯æ‰¾åˆ°æ‰§è¡ŒæŸ¥è¯¢çš„æœ€æœ‰æ•ˆæ–¹å¼ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–å¯å‘å¼æœç´¢å’Œæˆæœ¬é¢„æµ‹ï¼Œä½†å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>LLMOptæ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è§£å†³æŸ¥è¯¢ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>LLMOpt(G)é€šè¿‡ç›´æ¥ç”Ÿæˆé«˜è´¨é‡æŸ¥è¯¢è®¡åˆ’æ¶ˆé™¤å¯å‘å¼æœç´¢ã€‚</li>
<li>LLMOpt(S)é‡‡ç”¨åˆ—è¡¨å¼çš„æˆæœ¬æ¨¡å‹æ¯”è¾ƒå€™é€‰è®¡åˆ’ï¼Œæé«˜é€‰æ‹©å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡ç¦»çº¿æ”¶é›†çš„ä¼˜åŒ–æ•°æ®è¿›è¡Œé¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-487c8a7f824382a870a06d6f1b5e36dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa8db5a821d0364ff862b152a8dfc858.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ace1ea7ebf3e2dd60493a1ce059a6b75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29363adc4c259b6cd625430e076ec1b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52ba9dac0556d40320ac1360b28ba96b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SafePlan-Leveraging-Formal-Logic-and-Chain-of-Thought-Reasoning-for-Enhanced-Safety-in-LLM-based-Robotic-Task-Planning"><a href="#SafePlan-Leveraging-Formal-Logic-and-Chain-of-Thought-Reasoning-for-Enhanced-Safety-in-LLM-based-Robotic-Task-Planning" class="headerlink" title="SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for   Enhanced Safety in LLM-based Robotic Task Planning"></a>SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for   Enhanced Safety in LLM-based Robotic Task Planning</h2><p><strong>Authors:Ike Obi, Vishnunandan L. N. Venkatesh, Weizheng Wang, Ruiqi Wang, Dayoon Suh, Temitope I. Amosa, Wonse Jo, Byung-Cheol Min</strong></p>
<p>Robotics researchers increasingly leverage large language models (LLM) in robotics systems, using them as interfaces to receive task commands, generate task plans, form team coalitions, and allocate tasks among multi-robot and human agents. However, despite their benefits, the growing adoption of LLM in robotics has raised several safety concerns, particularly regarding executing malicious or unsafe natural language prompts. In addition, ensuring that task plans, team formation, and task allocation outputs from LLMs are adequately examined, refined, or rejected is crucial for maintaining system integrity. In this paper, we introduce SafePlan, a multi-component framework that combines formal logic and chain-of-thought reasoners for enhancing the safety of LLM-based robotics systems. Using the components of SafePlan, including Prompt Sanity COT Reasoner and Invariant, Precondition, and Postcondition COT reasoners, we examined the safety of natural language task prompts, task plans, and task allocation outputs generated by LLM-based robotic systems as means of investigating and enhancing system safety profile. Our results show that SafePlan outperforms baseline models by leading to 90.5% reduction in harmful task prompt acceptance while still maintaining reasonable acceptance of safe tasks. </p>
<blockquote>
<p>æœºå™¨äººæŠ€æœ¯ç ”ç©¶è€…è¶Šæ¥è¶Šå¤šåœ°åœ¨æœºå™¨äººç³»ç»Ÿä¸­åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå°†å…¶ä½œä¸ºæ¥å£æ¥æ”¶ä»»åŠ¡å‘½ä»¤ã€ç”Ÿæˆä»»åŠ¡è®¡åˆ’ã€å½¢æˆå›¢é˜Ÿè”ç›Ÿä»¥åŠåœ¨å¤šæœºå™¨äººå’Œäººç±»ä»£ç†äººä¹‹é—´åˆ†é…ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå°½ç®¡æœ‰è¯¸å¤šå¥½å¤„ï¼ŒLLMåœ¨æœºå™¨äººæŠ€æœ¯ä¸­çš„æ—¥ç›Šæ™®åŠä¹Ÿå¼•å‘äº†è‹¥å¹²å®‰å…¨é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å…³äºæ‰§è¡Œæ¶æ„æˆ–å±é™©çš„è‡ªç„¶è¯­è¨€æç¤ºçš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œç¡®ä¿ä»»åŠ¡è®¡åˆ’ã€å›¢é˜Ÿç»„å»ºå’Œä»»åŠ¡åˆ†é…è¾“å‡ºï¼ˆæ¥è‡ªLLMï¼‰å¾—åˆ°å……åˆ†çš„å®¡æŸ¥ã€å®Œå–„æˆ–æ‹’ç»å¯¹äºç»´æŠ¤ç³»ç»Ÿå®Œæ•´æ€§è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†SafePlanï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šç»„ä»¶æ¡†æ¶ï¼Œå®ƒç»“åˆäº†å½¢å¼é€»è¾‘å’Œæ€ç»´é“¾æ¨ç†å™¨ï¼Œä»¥æé«˜åŸºäºLLMçš„æœºå™¨äººç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚ä½¿ç”¨SafePlançš„ç»„ä»¶ï¼ŒåŒ…æ‹¬æç¤ºç†æ™ºæ€ç»´é“¾æ¨ç†å™¨å’Œä¸å˜æ€§ã€å…ˆå†³æ¡ä»¶å’Œåæ¡ä»¶æ€ç»´é“¾æ¨ç†å™¨ï¼Œæˆ‘ä»¬æ£€æŸ¥äº†åŸºäºLLMçš„æœºå™¨äººç³»ç»Ÿç”Ÿæˆçš„è‡ªç„¶è¯­è¨€ä»»åŠ¡æç¤ºã€ä»»åŠ¡è®¡åˆ’å’Œä»»åŠ¡åˆ†é…è¾“å‡ºçš„å®‰å…¨æ€§ï¼Œä½œä¸ºè°ƒæŸ¥å’Œå¢å¼ºç³»ç»Ÿå®‰å…¨æ€§çš„æ‰‹æ®µã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒSafePlanä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¯¼è‡´æœ‰å®³ä»»åŠ¡æç¤ºæ¥å—ç‡é™ä½äº†90.5%ï¼ŒåŒæ—¶ä»ä¿æŒäº†å®‰å…¨ä»»åŠ¡çš„åˆç†æ¥å—ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06892v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœºå™¨äººæŠ€æœ¯ç ”ç©¶ä¸­ï¼Œè¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ¥å£ï¼Œæ‰§è¡Œä»»åŠ¡å‘½ä»¤ã€ç”Ÿæˆä»»åŠ¡è®¡åˆ’ã€ç»„å»ºå›¢é˜Ÿå’Œåˆ†é…å¤šæœºå™¨äººä¸äººç±»ä»£ç†çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼ŒLLMåœ¨æœºå™¨äººæŠ€æœ¯ä¸­çš„å¹¿æ³›åº”ç”¨å¼•å‘äº†å®‰å…¨æ‹…å¿§ï¼Œç‰¹åˆ«æ˜¯æ‰§è¡Œæ¶æ„æˆ–å±é™©çš„è‡ªç„¶è¯­è¨€æç¤ºçš„é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºSafePlançš„å¤šç»„ä»¶æ¡†æ¶ï¼Œå®ƒé€šè¿‡å½¢å¼é€»è¾‘å’Œæ€ç»´é“¾æ¨ç†å™¨æé«˜åŸºäºLLMçš„æœºå™¨äººç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSafePlanå¯å¤§å¹…åº¦é™ä½å±é™©ä»»åŠ¡æç¤ºçš„æ¥å—åº¦ï¼ŒåŒæ—¶ä¿æŒå¯¹å®‰å…¨ä»»åŠ¡çš„åˆç†æ¥å—åº¦ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æ€§èƒ½æå‡è¾¾90.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººæŠ€æœ¯ä¸­è¶Šæ¥è¶Šå¤šåœ°ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºç³»ç»Ÿæ¥å£ï¼Œç”¨äºæ‰§è¡Œå¤šç§ä»»åŠ¡ã€‚</li>
<li>LLMåœ¨æœºå™¨äººæŠ€æœ¯ä¸­çš„åº”ç”¨å¼•å‘äº†å¯¹æ‰§è¡Œæ¶æ„æˆ–å±é™©è‡ªç„¶è¯­è¨€æç¤ºçš„å®‰å…¨æ‹…å¿§ã€‚</li>
<li>SafePlanæ¡†æ¶ç»“åˆäº†å½¢å¼é€»è¾‘å’Œæ€ç»´é“¾æ¨ç†å™¨ï¼Œæ—¨åœ¨æé«˜åŸºäºLLMçš„æœºå™¨äººç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚</li>
<li>SafePlanåŒ…å«å¤šä¸ªç»„ä»¶ï¼Œå¦‚æç¤ºåˆç†æ€§æ€ç»´é“¾æ¨ç†å™¨ä»¥åŠä¸å˜æ€§ã€é¢„ç½®æ¡ä»¶å’Œåç½®æ¡ä»¶æ€ç»´é“¾æ¨ç†å™¨ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨SafePlanæ¡†æ¶ï¼Œç ”ç©¶äººå‘˜èƒ½å¤Ÿæ£€æŸ¥ã€æ”¹è¿›æˆ–æ‹’ç»LLMç”Ÿæˆçš„ä»»åŠ¡è®¡åˆ’ã€å›¢é˜Ÿç»„å»ºå’Œä»»åŠ¡åˆ†é…è¾“å‡ºï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„å®‰å…¨æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSafePlanèƒ½å¤§å¹…åº¦é™ä½å±é™©ä»»åŠ¡æç¤ºçš„æ¥å—åº¦ï¼ŒåŒæ—¶ä¿æŒå¯¹å®‰å…¨ä»»åŠ¡çš„åˆç†æ¥å—åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06892">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-82aedcf1883de45dc1098e9f3bdde324.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9007428b7971c45b1abc86562b73d118.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-decf1091ad389bce59e9bf0ea312477b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2ab6663be184b5ac86e372483bd9e50.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Interactive-Medical-Image-Analysis-with-Concept-based-Similarity-Reasoning"><a href="#Interactive-Medical-Image-Analysis-with-Concept-based-Similarity-Reasoning" class="headerlink" title="Interactive Medical Image Analysis with Concept-based Similarity   Reasoning"></a>Interactive Medical Image Analysis with Concept-based Similarity   Reasoning</h2><p><strong>Authors:Ta Duc Huy, Sen Kim Tran, Phan Nguyen, Nguyen Hoang Tran, Tran Bao Sam, Anton van den Hengel, Zhibin Liao, Johan W. Verjans, Minh-Son To, Vu Minh Hieu Phan</strong></p>
<p>The ability to interpret and intervene model decisions is important for the adoption of computer-aided diagnosis methods in clinical workflows. Recent concept-based methods link the model predictions with interpretable concepts and modify their activation scores to interact with the model. However, these concepts are at the image level, which hinders the model from pinpointing the exact patches the concepts are activated. Alternatively, prototype-based methods learn representations from training image patches and compare these with test image patches, using the similarity scores for final class prediction. However, interpreting the underlying concepts of these patches can be challenging and often necessitates post-hoc guesswork. To address this issue, this paper introduces the novel Concept-based Similarity Reasoning network (CSR), which offers (i) patch-level prototype with intrinsic concept interpretation, and (ii) spatial interactivity. First, the proposed CSR provides localized explanation by grounding prototypes of each concept on image regions. Second, our model introduces novel spatial-level interaction, allowing doctors to engage directly with specific image areas, making it an intuitive and transparent tool for medical imaging. CSR improves upon prior state-of-the-art interpretable methods by up to 4.5% across three biomedical datasets. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/tadeephuy/InteractCSR">https://github.com/tadeephuy/InteractCSR</a>. </p>
<blockquote>
<p>è§£è¯»å¹¶ä»‹å…¥æ¨¡å‹å†³ç­–çš„èƒ½åŠ›å¯¹äºåœ¨ä¸´åºŠå·¥ä½œæµä¸­é‡‡ç”¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­æ–¹æ³•è‡³å…³é‡è¦ã€‚æœ€è¿‘åŸºäºæ¦‚å¿µçš„æ–¹æ³•å°†æ¨¡å‹é¢„æµ‹ä¸å¯è§£é‡Šçš„æ¦‚å¿µè”ç³»èµ·æ¥ï¼Œå¹¶ä¿®æ”¹å…¶æ¿€æ´»åˆ†æ•°ä»¥ä¸æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œè¿™äº›æ¦‚å¿µå¤„äºå›¾åƒå±‚é¢ï¼Œé˜»ç¢äº†æ¨¡å‹ç²¾ç¡®å®šä½æ¦‚å¿µæ¿€æ´»çš„ç¡®åˆ‡åŒºåŸŸã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯åŸºäºåŸå‹çš„æ–¹æ³•ï¼Œå®ƒä»è®­ç»ƒå›¾åƒå—ä¸­å­¦ä¹ è¡¨ç¤ºï¼Œå¹¶å°†å…¶ä¸æµ‹è¯•å›¾åƒå—è¿›è¡Œæ¯”è¾ƒï¼Œä½¿ç”¨ç›¸ä¼¼åº¦åˆ†æ•°è¿›è¡Œæœ€ç»ˆçš„ç±»åˆ«é¢„æµ‹ã€‚ç„¶è€Œï¼Œè§£é‡Šè¿™äº›å›¾åƒå—çš„æ½œåœ¨æ¦‚å¿µå¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œé€šå¸¸éœ€è¦è¿›è¡Œäº‹åçŒœæµ‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº†å…¨æ–°çš„åŸºäºæ¦‚å¿µç›¸ä¼¼æ€§çš„æ¨ç†ç½‘ç»œï¼ˆCSRï¼‰ï¼Œå®ƒæä¾›äº†ï¼ˆiï¼‰å›¾åƒå—çº§çš„åŸå‹å’Œå†…åœ¨æ¦‚å¿µè§£é‡Šï¼Œï¼ˆiiï¼‰ç©ºé—´äº¤äº’æ€§ã€‚é¦–å…ˆï¼Œæ‰€æå‡ºçš„CSRé€šè¿‡åœ¨å›¾åƒåŒºåŸŸä¸Šä¸ºæ¯ä¸ªæ¦‚å¿µæä¾›åŸå‹æ¥æä¾›å±€éƒ¨è§£é‡Šã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¼•å…¥äº†æ–°å‹çš„ç©ºé—´çº§åˆ«äº¤äº’ï¼Œå…è®¸åŒ»ç”Ÿç›´æ¥ä¸ç‰¹å®šçš„å›¾åƒåŒºåŸŸè¿›è¡Œäº¤äº’ï¼Œä½¿å…¶æˆä¸ºåŒ»å­¦æˆåƒä¸­ç›´è§‚ä¸”é€æ˜çš„å·¥å…·ã€‚CSRåœ¨ä¸‰ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šè¾ƒä¹‹å‰çš„æœ€å…ˆè¿›çš„å¯è§£é‡Šæ–¹æ³•æé«˜äº†é«˜è¾¾4.5%ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/tadeephuy/InteractCSR%E3%80%82">https://github.com/tadeephuy/InteractCSRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06873v1">PDF</a> Accepted CVPR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºConcept-based Similarity Reasoningç½‘ç»œï¼ˆCSRï¼‰çš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿé’ˆå¯¹åŒ»å­¦å½±åƒè¯Šæ–­æä¾›æ—¢ç›´è§‚åˆé€æ˜çš„å·¥å…·ã€‚CSRèƒ½å¤Ÿåœ¨å›¾åƒåŒºåŸŸä¸Šå®šä½åŸå‹æ¦‚å¿µï¼Œå¹¶æä¾›ç©ºé—´äº¤äº’åŠŸèƒ½ï¼Œä½¿åŒ»ç”Ÿèƒ½å¤Ÿç›´æ¥ä¸ç‰¹å®šå›¾åƒåŒºåŸŸè¿›è¡Œäº¤äº’ï¼Œä»è€Œæé«˜æ¨¡å‹å†³ç­–çš„å¯è§£é‡Šæ€§å’Œå¹²é¢„èƒ½åŠ›ã€‚æ­¤æ–¹æ³•åœ¨ä¸‰ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„å¯è§£é‡Šæ–¹æ³•ï¼Œå‡†ç¡®ç‡æé«˜äº†é«˜è¾¾4.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CSRç½‘ç»œç»“åˆæ¦‚å¿µåŸºç¡€çš„æ–¹æ³•ï¼Œæä¾›æ¨¡å‹å†³ç­–çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>ä¼ ç»Ÿæ¦‚å¿µåŸºæ–¹æ³•å±€é™äºå›¾åƒå±‚é¢ï¼Œéš¾ä»¥ç¡®å®šæ¦‚å¿µæ¿€æ´»çš„å…·ä½“åŒºåŸŸã€‚</li>
<li>CSRç½‘ç»œé€šè¿‡å®šä½åŸå‹æ¦‚å¿µåœ¨å›¾åƒåŒºåŸŸä¸Šæä¾›å±€éƒ¨è§£é‡Šã€‚</li>
<li>CSRç½‘ç»œå¼•å…¥ç©ºé—´çº§åˆ«çš„äº¤äº’ï¼Œä½¿åŒ»ç”Ÿèƒ½å¤Ÿç›´æ¥ä¸å›¾åƒç‰¹å®šåŒºåŸŸäº’åŠ¨ã€‚</li>
<li>æ­¤æ–¹æ³•æé«˜äº†åŒ»å­¦å½±åƒè¯Šæ–­çš„ç›´è§‚æ€§å’Œé€æ˜åº¦ã€‚</li>
<li>CSRç½‘ç»œåœ¨ä¸‰ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œå‡†ç¡®ç‡æå‡æ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0a060ff375e2265f5c58838bf63d6127.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90bd9f54860aeea029edc540d67492f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aaf48488a6d44f916ac63fb3788990d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f5f8a663e2244983f7b467bc408907fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0eef3f141c3663d8fcbf728486cbdef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbc344d20b38c67b3ddf92be0fba069d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Lost-in-the-Middle-in-Long-Text-Generation-Synthetic-Dataset-Evaluation-Framework-and-Mitigation"><a href="#Lost-in-the-Middle-in-Long-Text-Generation-Synthetic-Dataset-Evaluation-Framework-and-Mitigation" class="headerlink" title="Lost-in-the-Middle in Long-Text Generation: Synthetic Dataset,   Evaluation Framework, and Mitigation"></a>Lost-in-the-Middle in Long-Text Generation: Synthetic Dataset,   Evaluation Framework, and Mitigation</h2><p><strong>Authors:Junhao Zhang, Richong Zhang, Fanshuang Kong, Ziyang Miao, Yanhan Ye, Yaowei Zheng</strong></p>
<p>Existing long-text generation methods primarily concentrate on producing lengthy texts from short inputs, neglecting the long-input and long-output tasks. Such tasks have numerous practical applications while lacking available benchmarks. Moreover, as the input grows in length, existing methods inevitably encounter the â€œlost-in-the-middleâ€ phenomenon. In this paper, we first introduce a Long Input and Output Benchmark (LongInOutBench), including a synthetic dataset and a comprehensive evaluation framework, addressing the challenge of the missing benchmark. We then develop the Retrieval-Augmented Long-Text Writer (RAL-Writer), which retrieves and restates important yet overlooked content, mitigating the â€œlost-in-the-middleâ€ issue by constructing explicit prompts. We finally employ the proposed LongInOutBench to evaluate our RAL-Writer against comparable baselines, and the results demonstrate the effectiveness of our approach. Our code has been released at <a target="_blank" rel="noopener" href="https://github.com/OnlyAR/RAL-Writer">https://github.com/OnlyAR/RAL-Writer</a>. </p>
<blockquote>
<p>å½“å‰çš„é•¿æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ä»çŸ­è¾“å…¥ç”Ÿæˆé•¿ç¯‡æ–‡æœ¬ï¼Œå¿½è§†äº†é•¿è¾“å…¥å’Œé•¿è¾“å‡ºçš„ä»»åŠ¡ã€‚è¿™ç±»ä»»åŠ¡å…·æœ‰è®¸å¤šå®é™…åº”ç”¨ï¼Œä½†ç¼ºä¹å¯ç”¨çš„åŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œéšç€è¾“å…¥çš„å¢é•¿ï¼Œç°æœ‰æ–¹æ³•ä¸å¯é¿å…åœ°ä¼šé‡åˆ°â€œä¸­é—´ä¸¢å¤±â€çš„ç°è±¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†ä¸€ä¸ªé•¿è¾“å…¥è¾“å‡ºåŸºå‡†æµ‹è¯•ï¼ˆLongInOutBenchï¼‰ï¼ŒåŒ…æ‹¬ä¸€ä¸ªåˆæˆæ•°æ®é›†å’Œä¸€ä¸ªç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œä»¥è§£å†³ç¼ºå°‘åŸºå‡†æµ‹è¯•çš„æŒ‘æˆ˜ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼€å‘äº†æ£€ç´¢å¢å¼ºé•¿æ–‡æœ¬å†™ä½œå™¨ï¼ˆRAL-Writerï¼‰ï¼Œå®ƒæ£€ç´¢å¹¶å¤è¿°äº†é‡è¦ä½†è¢«å¿½è§†çš„å†…å®¹ï¼Œé€šè¿‡æ„å»ºæ˜ç¡®çš„æç¤ºæ¥ç¼“è§£â€œä¸­é—´ä¸¢å¤±â€çš„é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬é‡‡ç”¨æå‡ºçš„LongInOutBenchæ¥è¯„ä¼°æˆ‘ä»¬çš„RAL-Writerä¸å¯æ¯”åŸºçº¿çš„æ•ˆæœï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/OnlyAR/RAL-Writer%E3%80%82">https://github.com/OnlyAR/RAL-Writerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06868v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡å¼•å…¥äº†ä¸€ä¸ªé•¿è¾“å…¥å’Œè¾“å‡ºåŸºå‡†æµ‹è¯•ï¼ˆLongInOutBenchï¼‰ï¼ŒåŒ…æ‹¬åˆæˆæ•°æ®é›†å’Œç»¼åˆè¯„ä»·æ¡†æ¶ï¼Œä»¥è§£å†³ç¼ºå°‘åŸºå‡†æµ‹è¯•çš„æŒ‘æˆ˜ã€‚éšåï¼Œå¼€å‘äº†æ£€ç´¢å¢å¼ºé•¿æ–‡æœ¬ç”Ÿæˆå™¨ï¼ˆRAL-Writerï¼‰ï¼Œé€šè¿‡æ£€ç´¢å¹¶å¤è¿°é‡è¦ä½†è¢«å¿½è§†çš„å†…å®¹ï¼Œæ„å»ºæ˜ç¡®çš„æç¤ºï¼Œè§£å†³äº†â€œä¸­é—´ä¸¢å¤±â€çš„é—®é¢˜ã€‚æœ€åï¼Œä½¿ç”¨LongInOutBenchè¯„ä¼°RAL-Writerä¸åŒç±»åŸºçº¿çš„æ•ˆæœï¼Œç»“æœæ˜¾ç¤ºäº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç°æœ‰é•¿æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ä¸»è¦å…³æ³¨çŸ­è¾“å…¥äº§ç”Ÿé•¿æ–‡æœ¬ï¼Œå¿½è§†é•¿è¾“å…¥å’Œé•¿è¾“å‡ºä»»åŠ¡ã€‚</li>
<li>é•¿è¾“å…¥å’Œé•¿è¾“å‡ºä»»åŠ¡å…·æœ‰ä¼—å¤šå®é™…åº”ç”¨ï¼Œä½†ç¼ºä¹å¯ç”¨åŸºå‡†æµ‹è¯•ã€‚</li>
<li>éšç€è¾“å…¥çš„å¢é•¿ï¼Œç°æœ‰æ–¹æ³•ä¼šé‡åˆ°â€œä¸­é—´ä¸¢å¤±â€çš„ç°è±¡ã€‚</li>
<li>å¼•å…¥Long Input and Output Benchmark (LongInOutBench)ï¼ŒåŒ…æ‹¬åˆæˆæ•°æ®é›†å’Œç»¼åˆè¯„ä»·æ¡†æ¶ï¼Œä»¥è§£å†³åŸºå‡†æµ‹è¯•ç¼ºå¤±çš„æŒ‘æˆ˜ã€‚</li>
<li>å¼€å‘äº†Retrieval-Augmented Long-Text Writer (RAL-Writer)ï¼Œé€šè¿‡æ£€ç´¢å¹¶å¤è¿°é‡è¦å†…å®¹ï¼Œæ„å»ºæ˜ç¡®æç¤ºæ¥è§£å†³â€œä¸­é—´ä¸¢å¤±â€é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨LongInOutBenchè¯„ä¼°RAL-Writerä¸åŸºçº¿æ–¹æ³•ï¼Œç»“æœæ˜¾ç¤ºRAL-Writeræ–¹æ³•æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06868">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cb744c2a23e63988d322941a3ae68f39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39c592e9c068c41e9249325e0e8339eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d7ad5850618341464a7eade283035c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6737c4919f7adc29d1863e83fb2e0bdd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8b019deaa558b3f141f4088b396620e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8cc878763272618f4cc78b82852fe82.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Vision-R1-Incentivizing-Reasoning-Capability-in-Multimodal-Large-Language-Models"><a href="#Vision-R1-Incentivizing-Reasoning-Capability-in-Multimodal-Large-Language-Models" class="headerlink" title="Vision-R1: Incentivizing Reasoning Capability in Multimodal Large   Language Models"></a>Vision-R1: Incentivizing Reasoning Capability in Multimodal Large   Language Models</h2><p><strong>Authors:Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, Shaohui Lin</strong></p>
<p>DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the modelâ€™s ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: <a target="_blank" rel="noopener" href="https://github.com/Osilly/Vision-R1">https://github.com/Osilly/Vision-R1</a> . </p>
<blockquote>
<p>DeepSeek-R1-Zeroå·²æˆåŠŸå±•ç¤ºäº†å¤§è¯­è¨€æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å‡ºç°æ¨ç†èƒ½åŠ›ã€‚å—æ­¤çªç ´å¯å‘ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¦‚ä½•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æé«˜å°è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç›´æ¥ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒå¾ˆéš¾åœ¨å°å‹è¯­è¨€æ¨¡å‹ä¸­æ¿€æ´»å¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œå¦‚æé—®å’Œåæ€ï¼Œè¿™æ˜¯å› ä¸ºç¼ºä¹å¤§é‡é«˜è´¨é‡çš„å¤šæ¨¡æ€æ¨ç†æ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€æ¨ç†å°å‹è¯­è¨€æ¨¡å‹â€”â€”Vision-R1ï¼Œä»¥æé«˜å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡åˆ©ç”¨ç°æœ‰çš„å°å‹è¯­è¨€æ¨¡å‹å’ŒDeepSeek-R1è¿›è¡Œæ¨¡æ€æ¡¥æ¥å’Œæ•°æ®è¿‡æ»¤ï¼Œæ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„æ— äººå·¥æ³¨é‡Šçš„å¤šæ¨¡æ€å› æœæ¨ç†æ•°æ®é›†ï¼Œå³Vision-R1å†·å¯åŠ¨æ•°æ®é›†ï¼Œæ•°æ®é›†è§„æ¨¡ä¸º20ä¸‡ã€‚å®ƒä¸ºVision-R1æä¾›äº†å†·å¯åŠ¨åˆå§‹åŒ–æ•°æ®ã€‚ä¸ºäº†ç¼“è§£å†·å¯åŠ¨åè¿‡åº¦æ€è€ƒå¯¼è‡´çš„ä¼˜åŒ–æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¸è¿›æ€è€ƒæŠ‘åˆ¶è®­ç»ƒï¼ˆPTSTï¼‰ç­–ç•¥ï¼Œå¹¶ä½¿ç”¨å…·æœ‰ç¡¬æ ¼å¼åŒ–ç»“æœå¥–åŠ±å‡½æ•°çš„ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œä»¥é€æ­¥æ”¹è¿›æ¨¡å‹åœ¨è§„æ¨¡ä¸º1ä¸‡çš„å¤šæ¨¡æ€æ•°å­¦æ•°æ®é›†ä¸Šå­¦ä¹ æ­£ç¡®ä¸”å¤æ‚æ¨ç†è¿‡ç¨‹çš„èƒ½åŠ›ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å„ç§å¤šæ¨¡æ€æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡å‡†ç¡®ç‡æé«˜äº†çº¦6%ã€‚å…¶ä¸­ï¼ŒVision-R1-7Båœ¨æ•°å­¦è§†è§‰åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡ä¸º73.5%ï¼Œä»…æ¯”é¢†å…ˆçš„æ¨ç†æ¨¡å‹OpenAI O1ä½0.4%ã€‚ç›¸å…³æ•°æ®é›†å’Œä»£ç å°†åœ¨[<a target="_blank" rel="noopener" href="https://github.com/Osilly/Vision-R">https://github.com/Osilly/Vision-R</a> ç»“]é‡Šæ”¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06749v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§æ„ä¸ºï¼šDeepSeek-R1-ZeroæˆåŠŸè¯æ˜äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­èƒ½å¤Ÿæ¶Œç°å‡ºæ¨ç†èƒ½åŠ›ã€‚æœ¬ç ”ç©¶å—å…¶å¯å‘ï¼Œæ—¨åœ¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ æé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹é«˜è´¨é‡çš„å¤šæ¨¡æ€æ¨ç†æ•°æ®ï¼Œç›´æ¥é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¾ˆéš¾æ¿€æ´»å¦‚æé—®å’Œåæ€ç­‰å¤æ‚æ¨ç†èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†å¤šæ¨¡æ€æ¨ç†MLLMæ¨¡å‹â€”â€”Vision-R1ã€‚å…·ä½“æ¥è¯´ï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡æ¨¡æ€æ¡¥æ¥å’Œæ•°æ®è¿‡æ»¤ï¼Œåˆ©ç”¨ç°æœ‰MLLMå’ŒDeepSeek-R1æ„å»ºäº†ä¸€ä¸ªæ— äººå·¥æ ‡æ³¨çš„é«˜è´¨é‡å¤šæ¨¡æ€å› æœæ¨ç†æ•°æ®é›†Vision-R1-coldæ•°æ®é›†ï¼Œç”¨äºå†·å¯åŠ¨åˆå§‹åŒ–ã€‚ä¸ºäº†ç¼“è§£å†·å¯åŠ¨åè¿‡åº¦æ€è€ƒå¸¦æ¥çš„ä¼˜åŒ–æŒ‘æˆ˜ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†æ¸è¿›æ€è€ƒæŠ‘åˆ¶è®­ç»ƒç­–ç•¥ï¼ˆPTSTï¼‰ï¼Œå¹¶é‡‡ç”¨åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ä¸æ ¼å¼åŒ–ç»“æœå¥–åŠ±å‡½æ•°æ¥é€æ­¥æ”¹è¿›æ¨¡å‹åœ¨å¤šæ¨¡æ€æ•°å­¦æ•°æ®é›†ä¸Šçš„æ­£ç¡®æ¨ç†è¿‡ç¨‹ã€‚å®éªŒæ˜¾ç¤ºï¼Œä¸å¤šä¸ªå¤šæ¨¡æ€æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹å¹³å‡æé«˜äº†çº¦6%ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„MathVistaåŸºå‡†æµ‹è¯•ä¸­ï¼ŒVision-R1-7Bæ¨¡å‹çš„å‡†ç¡®ç‡ä¸º73.5%ï¼Œä»…æ¯”é¢†å…ˆçš„æ¨ç†æ¨¡å‹OpenAI O1ä½0.4%ã€‚ç›¸å…³æ•°æ®é›†å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/Osilly/Vision-R1">https://github.com/Osilly/Vision-R1</a>ä¸­å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1-ZeroæˆåŠŸè¯æ˜LLMså¯é€šè¿‡å¼ºåŒ–å­¦ä¹ å±•ç°æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶æ—¨åœ¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å¢å¼ºMLLMsçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ç¼ºä¹é«˜è´¨é‡å¤šæ¨¡æ€æ¨ç†æ•°æ®æ˜¯å¼ºåŒ–å­¦ä¹ è®­ç»ƒMLLMsé¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºVision-R1æ¨¡å‹ä»¥æé«˜å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨ç°æœ‰MLLMå’ŒDeepSeek-R1æ„å»ºæ— äººå·¥æ ‡æ³¨çš„é«˜è´¨é‡å¤šæ¨¡æ€å› æœæ¨ç†æ•°æ®é›†Vision-R1-coldç”¨äºå†·å¯åŠ¨ã€‚</li>
<li>å¼•å…¥æ¸è¿›æ€è€ƒæŠ‘åˆ¶è®­ç»ƒç­–ç•¥å’Œåˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06749">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-69b2ebda1b97576a790224306f7b7d11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7ac22e4798851f1c5a2c8aea0e0c4b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afeb9736b27783feb34cfa2a4df752df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f6885360dce633abc67439687b6600c5.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Reinforcement-Learning-with-Verifiable-Rewards-GRPOâ€™s-Effective-Loss-Dynamics-and-Success-Amplification"><a href="#Reinforcement-Learning-with-Verifiable-Rewards-GRPOâ€™s-Effective-Loss-Dynamics-and-Success-Amplification" class="headerlink" title="Reinforcement Learning with Verifiable Rewards: GRPOâ€™s Effective Loss,   Dynamics, and Success Amplification"></a>Reinforcement Learning with Verifiable Rewards: GRPOâ€™s Effective Loss,   Dynamics, and Success Amplification</h2><p><strong>Authors:Youssef Mroueh</strong></p>
<p>Group Relative Policy Optimization (GRPO) was introduced and used successfully to train DeepSeek R1 models for promoting reasoning capabilities of LLMs using verifiable or binary rewards. We show in this paper that GRPO with verifiable rewards can be written as a Kullback Leibler ($\mathsf{KL}$) regularized contrastive loss, where the contrastive samples are synthetic data sampled from the old policy. The optimal GRPO policy $\pi_{n}$ can be expressed explicitly in terms of the binary reward, as well as the first and second order statistics of the old policy ($\pi_{n-1}$) and the reference policy $\pi_0$. Iterating this scheme, we obtain a sequence of policies $\pi_{n}$ for which we can quantify the probability of success $p_n$. We show that the probability of success of the policy satisfies a recurrence that converges to a fixed point of a function that depends on the initial probability of success $p_0$ and the regularization parameter $\beta$ of the $\mathsf{KL}$ regularizer. We show that the fixed point $p^*$ is guaranteed to be larger than $p_0$, thereby demonstrating that GRPO effectively amplifies the probability of success of the policy. </p>
<blockquote>
<p>ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¢«å¼•å…¥å¹¶æˆåŠŸç”¨äºä½¿ç”¨å¯éªŒè¯æˆ–äºŒè¿›åˆ¶å¥–åŠ±è®­ç»ƒDeepSeek R1æ¨¡å‹ï¼Œä»¥ä¿ƒè¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡å±•ç¤ºäº†ä½¿ç”¨å¯éªŒè¯å¥–åŠ±çš„GRPOå¯ä»¥è¡¨ç¤ºä¸ºKullback Leiblerï¼ˆKLï¼‰æ­£åˆ™åŒ–çš„å¯¹æ¯”æŸå¤±ï¼Œå…¶ä¸­å¯¹æ¯”æ ·æœ¬æ˜¯æ—§ç­–ç•¥é‡‡æ ·çš„åˆæˆæ•°æ®ã€‚æœ€ä¼˜GRPOç­–ç•¥Ï€nå¯ä»¥æ˜ç¡®è¡¨ç¤ºä¸ºäºŒè¿›åˆ¶å¥–åŠ±ä»¥åŠæ—§ç­–ç•¥Ï€n-1å’Œå‚è€ƒç­–ç•¥Ï€0çš„ä¸€é˜¶å’ŒäºŒé˜¶ç»Ÿè®¡é‡ã€‚é€šè¿‡è¿­ä»£æ­¤æ–¹æ¡ˆï¼Œæˆ‘ä»¬è·å¾—ä¸€ç³»åˆ—ç­–ç•¥Ï€nï¼Œå¯ä»¥é‡åŒ–å…¶æˆåŠŸçš„æ¦‚ç‡pnã€‚æˆ‘ä»¬è¯æ˜äº†ç­–ç•¥çš„æˆåŠŸæ¦‚ç‡æ»¡è¶³ä¸€ä¸ªé€’å½’ï¼Œè¯¥é€’å½’æ”¶æ•›äºä¸€ä¸ªä¾èµ–äºåˆå§‹æˆåŠŸæ¦‚ç‡p0å’ŒKLæ­£åˆ™åŒ–çš„æ­£åˆ™åŒ–å‚æ•°Î²çš„å‡½æ•°çš„ä¸åŠ¨ç‚¹ã€‚æˆ‘ä»¬è¯æ˜äº†ä¸åŠ¨ç‚¹p*ä¸€å®šå¤§äºp0ï¼Œä»è€Œè¯æ˜GRPOæœ‰æ•ˆåœ°æ”¾å¤§äº†ç­–ç•¥æˆåŠŸçš„æ¦‚ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06639v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Group Relative Policy Optimizationï¼ˆGRPOï¼‰çš„æˆåŠŸåº”ç”¨äºè®­ç»ƒDeepSeek R1æ¨¡å‹ä»¥ä¿ƒè¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡ä½¿ç”¨å¯éªŒè¯æˆ–äºŒå…ƒå¥–åŠ±ï¼ŒGRPOå¯ä»¥è¡¨ç¤ºä¸ºKullback Leiblerï¼ˆKLï¼‰æ­£åˆ™åŒ–çš„å¯¹æ¯”æŸå¤±ã€‚æœ€ä¼˜GRPOç­–ç•¥å¯æ˜¾å¼è¡¨è¾¾ä¸ºäºŒå…ƒå¥–åŠ±ä»¥åŠæ—§ç­–ç•¥å’Œä¸€é˜¶äºŒé˜¶ç»Ÿè®¡æ•°æ®çš„å‡½æ•°ã€‚é€šè¿‡è¿­ä»£æ­¤æ–¹æ¡ˆï¼Œæˆ‘ä»¬è·å¾—ä¸€ç³»åˆ—ç­–ç•¥ï¼Œå¯ä»¥é‡åŒ–å…¶æˆåŠŸæ¦‚ç‡ã€‚ç ”ç©¶è¡¨æ˜ï¼Œç­–ç•¥çš„æˆåŠŸæ¦‚ç‡æ»¡è¶³ä¸€ä¸ªé€’å½’ï¼Œæ”¶æ•›åˆ°ä¸€ä¸ªä¾èµ–äºåˆå§‹æˆåŠŸæ¦‚ç‡å’ŒKLæ­£åˆ™åŒ–å‚æ•°å‡½æ•°çš„å›ºå®šç‚¹ï¼Œè¯æ˜äº†GRPOæœ‰æ•ˆåœ°æé«˜äº†ç­–ç•¥æˆåŠŸçš„æ¦‚ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Group Relative Policy Optimization (GRPO) ç”¨äºè®­ç»ƒDeepSeek R1æ¨¡å‹ï¼Œæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>GRPOç»“åˆå¯éªŒè¯æˆ–äºŒå…ƒå¥–åŠ±ï¼Œå¯ä»¥è¡¨è¿°ä¸ºKullback Leiblerï¼ˆKLï¼‰æ­£åˆ™åŒ–çš„å¯¹æ¯”æŸå¤±ã€‚</li>
<li>æœ€ä¼˜GRPOç­–ç•¥æ˜¯äºŒå…ƒå¥–åŠ±ã€æ—§ç­–ç•¥çš„ä¸€é˜¶äºŒé˜¶ç»Ÿè®¡æ•°æ®ç­‰å˜é‡çš„å‡½æ•°ã€‚</li>
<li>é€šè¿‡è¿­ä»£ï¼Œè·å¾—ä¸€ç³»åˆ—ç­–ç•¥ï¼Œå¹¶å¯ä»¥é‡åŒ–å…¶æˆåŠŸæ¦‚ç‡ã€‚</li>
<li>ç­–ç•¥çš„æˆåŠŸæ¦‚ç‡æ»¡è¶³ä¸€ä¸ªé€’å½’ï¼Œæ”¶æ•›åˆ°ä¸€ä¸ªå›ºå®šç‚¹ã€‚</li>
<li>è¿™ä¸ªå›ºå®šç‚¹å¤§äºåˆå§‹æˆåŠŸæ¦‚ç‡ï¼Œè¯æ˜äº†GRPOèƒ½æœ‰æ•ˆæé«˜ç­–ç•¥æˆåŠŸçš„æ¦‚ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06639">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-657fcfe2049b0f1ad569b7b1a5bded9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-88d5e84e1662e6affc82e901e59b4568.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Agent-models-Internalizing-Chain-of-Action-Generation-into-Reasoning-models"><a href="#Agent-models-Internalizing-Chain-of-Action-Generation-into-Reasoning-models" class="headerlink" title="Agent models: Internalizing Chain-of-Action Generation into Reasoning   models"></a>Agent models: Internalizing Chain-of-Action Generation into Reasoning   models</h2><p><strong>Authors:Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Xinyan Wen, Jitao Sang</strong></p>
<p>Traditional agentic workflows rely on external prompts to manage interactions with tools and the environment, which limits the autonomy of reasoning models. We position \emph{Large Agent Models (LAMs)} that internalize the generation of \emph{Chain-of-Action (CoA)}, enabling the model to autonomously decide when and how to use external tools. Our proposed AutoCoA framework combines supervised fine-tuning (SFT) and reinforcement learning (RL), allowing the model to seamlessly switch between reasoning and action while efficiently managing environment interactions. Main components include step-level action triggering, trajectory-level CoA optimization, and an internal world model to reduce real-environment interaction costs. Evaluations on open-domain QA tasks demonstrate that AutoCoA-trained agent models significantly outperform ReAct-based workflows in task completion, especially in tasks that require long-term reasoning and multi-step actions. Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/ADaM-BJTU/AutoCoA">https://github.com/ADaM-BJTU/AutoCoA</a> </p>
<blockquote>
<p>ä¼ ç»Ÿçš„ä»£ç†å·¥ä½œæµç¨‹ä¾èµ–äºå¤–éƒ¨æç¤ºæ¥ç®¡ç†å·¥å…·å’Œç¯å¢ƒçš„äº¤äº’ï¼Œè¿™é™åˆ¶äº†æ¨ç†æ¨¡å‹çš„è‡ªä¸»æ€§ã€‚æˆ‘ä»¬å®šä½äº†èƒ½å¤Ÿå†…åŒ–â€œè¡ŒåŠ¨é“¾â€ç”Ÿæˆçš„â€œå¤§å‹ä»£ç†æ¨¡å‹â€ï¼ˆLAMsï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»å†³å®šä½•æ—¶ä»¥åŠå¦‚ä½•ä½¿ç”¨å¤–éƒ¨å·¥å…·ã€‚æˆ‘ä»¬æå‡ºçš„AutoCoAæ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†å’Œè¡ŒåŠ¨ä¹‹é—´æ— ç¼åˆ‡æ¢ï¼ŒåŒæ—¶é«˜æ•ˆåœ°ç®¡ç†ç¯å¢ƒäº¤äº’ã€‚ä¸»è¦ç»„ä»¶åŒ…æ‹¬æ­¥éª¤çº§åŠ¨ä½œè§¦å‘ã€è½¨è¿¹çº§è¡ŒåŠ¨é“¾ä¼˜åŒ–ä»¥åŠä¸€ä¸ªå†…éƒ¨ä¸–ç•Œæ¨¡å‹ï¼Œä»¥å‡å°‘çœŸå®ç¯å¢ƒäº¤äº’æˆæœ¬ã€‚åœ¨å¼€æ”¾åŸŸé—®ç­”ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä½¿ç”¨AutoCoAè®­ç»ƒçš„ä»£ç†æ¨¡å‹åœ¨ä»»åŠ¡å®Œæˆæ–¹é¢æ˜¾è‘—ä¼˜äºåŸºäºReActçš„å·¥ä½œæµç¨‹ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é•¿æœŸæ¨ç†å’Œå¤šæ­¥éª¤è¡ŒåŠ¨çš„ä»»åŠ¡ä¸­è¡¨ç°æ›´ä¼˜ç§€ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/ADaM-BJTU/AutoCoA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ADaM-BJTU/AutoCoAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06580v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹ä»£ç†æ¨¡å‹ï¼ˆLAMsï¼‰é‡‡ç”¨è‡ªä¸»å†³ç­–æµç¨‹ï¼Œèƒ½å¤Ÿè‡ªä¸»å†³å®šä½•æ—¶ä»¥åŠå¦‚ä½•ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼Œå®ç°è¡ŒåŠ¨é“¾ï¼ˆCoAï¼‰çš„ç”Ÿæˆã€‚æå‡ºçš„AutoCoAæ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†å’Œè¡ŒåŠ¨ä¹‹é—´æ— ç¼åˆ‡æ¢ï¼ŒåŒæ—¶æœ‰æ•ˆåœ°ç®¡ç†ç¯å¢ƒäº¤äº’ã€‚å…¶ä¸»è¦ç»„ä»¶åŒ…æ‹¬æ­¥éª¤çº§è¡ŒåŠ¨è§¦å‘ã€è½¨è¿¹çº§è¡ŒåŠ¨é“¾ä¼˜åŒ–å’Œå†…éƒ¨ä¸–ç•Œæ¨¡å‹ï¼Œä»¥å‡å°‘çœŸå®ç¯å¢ƒäº¤äº’æˆæœ¬ã€‚åœ¨å¼€æ”¾åŸŸé—®ç­”ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œä½¿ç”¨AutoCoAè®­ç»ƒçš„ä»£ç†æ¨¡å‹åœ¨ä»»åŠ¡å®Œæˆæ–¹é¢æ˜¾è‘—ä¼˜äºåŸºäºReActçš„å·¥ä½œæµï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦é•¿æœŸæ¨ç†å’Œå¤šæ­¥éª¤è¡ŒåŠ¨çš„ä»»åŠ¡ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹ä»£ç†æ¨¡å‹ï¼ˆLAMsï¼‰èƒ½å¤Ÿè‡ªä¸»å†³å®šä½•æ—¶åŠå¦‚ä½•ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼Œå®ç°è¡ŒåŠ¨é“¾ï¼ˆCoAï¼‰çš„è‡ªåŠ¨ç”Ÿæˆã€‚</li>
<li>AutoCoAæ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œæ”¯æŒæ¨¡å‹åœ¨æ¨ç†å’Œè¡ŒåŠ¨é—´çš„æ— ç¼åˆ‡æ¢ã€‚</li>
<li>AutoCoAæ¡†æ¶åŒ…æ‹¬æ­¥éª¤çº§è¡ŒåŠ¨è§¦å‘ã€è½¨è¿¹çº§è¡ŒåŠ¨é“¾ä¼˜åŒ–ï¼Œä»¥æé«˜ä»»åŠ¡å®Œæˆçš„æ•ˆç‡å’Œæ•ˆæœã€‚</li>
<li>å†…éƒ¨ä¸–ç•Œæ¨¡å‹çš„å¼•å…¥ï¼Œæ—¨åœ¨å‡å°‘ä¸çœŸå®ç¯å¢ƒçš„äº¤äº’æˆæœ¬ã€‚</li>
<li>åœ¨å¼€æ”¾åŸŸé—®ç­”ä»»åŠ¡ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒAutoCoAè®­ç»ƒçš„æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡å®Œæˆæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>AutoCoAæ¡†æ¶ç‰¹åˆ«é€‚ç”¨äºéœ€è¦é•¿æœŸæ¨ç†å’Œå¤šæ­¥éª¤çš„ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-20c3b7901820bda9a98d81ba273d79e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b01551dcdf2834d270bb6b711fcd1e46.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f38343ebc3483ef99f7560f84f9722be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6bcfce62a2f217d2150d955cce421f04.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MetaXCR-Reinforcement-Based-Meta-Transfer-Learning-for-Cross-Lingual-Commonsense-Reasoning"><a href="#MetaXCR-Reinforcement-Based-Meta-Transfer-Learning-for-Cross-Lingual-Commonsense-Reasoning" class="headerlink" title="MetaXCR: Reinforcement-Based Meta-Transfer Learning for Cross-Lingual   Commonsense Reasoning"></a>MetaXCR: Reinforcement-Based Meta-Transfer Learning for Cross-Lingual   Commonsense Reasoning</h2><p><strong>Authors:Jie He, Yu Fu</strong></p>
<p>Commonsense reasoning (CR) has been studied in many pieces of domain and has achieved great progress with the aid of large datasets. Unfortunately, most existing CR datasets are built in English, so most previous work focus on English. Furthermore, as the annotation of commonsense reasoning is costly, it is impossible to build a large dataset for every novel task. Therefore, there are growing appeals for Cross-lingual Low-Resource Commonsense Reasoning, which aims to leverage diverse existed English datasets to help the model adapt to new cross-lingual target datasets with limited labeled data. In this paper, we propose a multi-source adapter for cross-lingual low-resource Commonsense Reasoning (MetaXCR). In this framework, we first extend meta learning by incorporating multiple training datasets to learn a generalized task adapters across different tasks. Then, we further introduce a reinforcement-based sampling strategy to help the model sample the source task that is the most helpful to the target task. Finally, we introduce two types of cross-lingual meta-adaption methods to enhance the performance of models on target languages. Extensive experiments demonstrate MetaXCR is superior over state-of-the-arts, while being trained with fewer parameters than other work. </p>
<blockquote>
<p>å¸¸è¯†æ¨ç†ï¼ˆCRï¼‰åœ¨è®¸å¤šé¢†åŸŸéƒ½å¾—åˆ°äº†ç ”ç©¶ï¼Œå¹¶åœ¨å¤§å‹æ•°æ®é›†çš„å¸®åŠ©ä¸‹å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ã€‚ä¸å¹¸çš„æ˜¯ï¼Œå¤§å¤šæ•°ç°æœ‰çš„CRæ•°æ®é›†éƒ½æ˜¯ç”¨è‹±è¯­æ„å»ºçš„ï¼Œæ‰€ä»¥ä¹‹å‰çš„å¤§å¤šæ•°å·¥ä½œéƒ½é›†ä¸­åœ¨è‹±è¯­ä¸Šã€‚æ­¤å¤–ï¼Œç”±äºå¸¸è¯†æ¨ç†çš„æ ‡æ³¨æˆæœ¬å¾ˆé«˜ï¼Œä¸å¯èƒ½ä¸ºæ¯ä¸€ä¸ªæ–°ä»»åŠ¡éƒ½æ„å»ºä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†ã€‚å› æ­¤ï¼Œå¯¹äºè·¨è¯­è¨€ä½èµ„æºå¸¸è¯†æ¨ç†çš„å‘¼å£°è¶Šæ¥è¶Šé«˜ã€‚å®ƒçš„ç›®æ ‡æ˜¯ä¸ºäº†åˆ©ç”¨å·²å­˜åœ¨çš„è‹±è¯­æ•°æ®é›†æ¥å¸®åŠ©æ¨¡å‹é€‚åº”æ–°çš„è·¨è¯­è¨€ç›®æ ‡æ•°æ®é›†ï¼Œè€Œè¿™äº›ç›®æ ‡æ•°æ®é›†åªæœ‰æœ‰é™çš„æœ‰æ ‡ç­¾æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè·¨è¯­è¨€ä½èµ„æºå¸¸è¯†æ¨ç†çš„å¤šæºé€‚é…å™¨ï¼ˆMetaXCRï¼‰ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡ç»“åˆå¤šä¸ªè®­ç»ƒæ•°æ®é›†æ¥æ‰©å±•å…ƒå­¦ä¹ ï¼Œä»¥å­¦ä¹ ä¸åŒä»»åŠ¡ä¹‹é—´çš„é€šç”¨ä»»åŠ¡é€‚é…å™¨ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§åŸºäºå¼ºåŒ–é‡‡æ ·çš„ç­–ç•¥ï¼Œä»¥å¸®åŠ©æ¨¡å‹é€‰æ‹©å¯¹ç›®æ ‡ä»»åŠ¡æœ€æœ‰å¸®åŠ©çš„ä»»åŠ¡ã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸¤ç§è·¨è¯­è¨€å…ƒé€‚åº”æ–¹æ³•ï¼Œä»¥æé«˜æ¨¡å‹åœ¨ç›®æ ‡è¯­è¨€ä¸Šçš„æ€§èƒ½ã€‚å¤§é‡çš„å®éªŒè¡¨æ˜ï¼ŒMetaXCRåœ¨å‚æ•°è®­ç»ƒè¾ƒå°‘çš„æƒ…å†µä¸‹ä»ä¼˜äºå…¶ä»–å…ˆè¿›æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06531v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨è·¨è¯­è¨€ä½èµ„æºå¸¸è¯†æ¨ç†é¢†åŸŸçš„é—®é¢˜å’ŒæŒ‘æˆ˜ï¼Œæå‡ºä¸€ç§å¤šæºé€‚é…å™¨MetaXCRæ¡†æ¶ã€‚é€šè¿‡èåˆå¤šç§è®­ç»ƒæ•°æ®é›†å’Œå¼ºåŒ–é‡‡æ ·ç­–ç•¥ï¼Œå¢å¼ºæ¨¡å‹å¯¹ä¸åŒä»»åŠ¡çš„é€‚åº”æ€§ï¼Œå¹¶å¼•å…¥ä¸¤ç§è·¨è¯­è¨€å…ƒé€‚åº”æ–¹æ³•æå‡ç›®æ ‡è¯­è¨€çš„æ€§èƒ½ã€‚å®éªŒè¯æ˜MetaXCRä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸”è®­ç»ƒå‚æ•°æ›´å°‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¸¸è¯†æ¨ç†ï¼ˆCRï¼‰ç ”ç©¶å·²å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†å¤šæ•°æ•°æ®é›†ä¸ºè‹±è¯­ï¼Œå¯¼è‡´å¤§å¤šæ•°ç ”ç©¶èšç„¦äºè‹±è¯­ç¯å¢ƒã€‚</li>
<li>è·¨è¯­è¨€ä½èµ„æºå¸¸è¯†æ¨ç†éœ€æ±‚å¢é•¿ï¼Œæ—¨åœ¨åˆ©ç”¨å·²å­˜åœ¨çš„è‹±è¯­æ•°æ®é›†å¸®åŠ©æ¨¡å‹é€‚åº”æ–°çš„è·¨è¯­è¨€ç›®æ ‡æ•°æ®é›†ã€‚</li>
<li>MetaXCRæ¡†æ¶é€šè¿‡æ‰©å±•å…ƒå­¦ä¹ ï¼Œèå…¥å¤šä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œå­¦ä¹ ä¸åŒä»»åŠ¡çš„é€šç”¨ä»»åŠ¡é€‚é…å™¨ã€‚</li>
<li>å¼•å…¥åŸºäºå¼ºåŒ–çš„é‡‡æ ·ç­–ç•¥ï¼Œå¸®åŠ©æ¨¡å‹é€‰æ‹©å¯¹ç›®æ ‡ä»»åŠ¡æœ€æœ‰å¸®åŠ©çš„ä»»åŠ¡ã€‚</li>
<li>å¼•å…¥ä¸¤ç§è·¨è¯­è¨€å…ƒé€‚åº”æ–¹æ³•ï¼Œæå‡æ¨¡å‹åœ¨ç›®æ ‡è¯­è¨€ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒè¯æ˜MetaXCRæ¡†æ¶ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06531">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-df58ce36f7cd57be160ab7d1773e5548.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ac4bae4a181456873d0be842ec52ce6.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Seg-Zero-Reasoning-Chain-Guided-Segmentation-via-Cognitive-Reinforcement"><a href="#Seg-Zero-Reasoning-Chain-Guided-Segmentation-via-Cognitive-Reinforcement" class="headerlink" title="Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive   Reinforcement"></a>Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive   Reinforcement</h2><p><strong>Authors:Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, Jiaya Jia</strong></p>
<p>Traditional methods for reasoning segmentation rely on supervised fine-tuning with categorical labels and simple descriptions, limiting its out-of-domain generalization and lacking explicit reasoning processes. To address these limitations, we propose Seg-Zero, a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement. Seg-Zero introduces a decoupled architecture consisting of a reasoning model and a segmentation model. The reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precious pixel-level masks. We design a sophisticated reward mechanism that integrates both format and accuracy rewards to effectively guide optimization directions. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Experiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on the ReasonSeg benchmark, surpassing the prior LISA-7B by 18%. This significant improvement highlights Seg-Zeroâ€™s ability to generalize across domains while presenting an explicit reasoning process. Code is available at <a target="_blank" rel="noopener" href="https://github.com/dvlab-research/Seg-Zero">https://github.com/dvlab-research/Seg-Zero</a>. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„æ¨ç†åˆ†å‰²æ–¹æ³•ä¾èµ–äºå¸¦æœ‰ç±»åˆ«æ ‡ç­¾å’Œç®€å•æè¿°çš„ç›‘ç£å¾®è°ƒï¼Œè¿™é™åˆ¶äº†å…¶è·¨åŸŸçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ç¼ºä¹æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Seg-Zeroè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒè¡¨ç°å‡ºæ˜¾è‘—çš„å¯æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶é€šè¿‡è®¤çŸ¥å¼ºåŒ–è¿›è¡Œæ˜ç¡®çš„æ€ç»´é“¾æ¨ç†ã€‚Seg-Zeroå¼•å…¥äº†ä¸€ä¸ªåˆ†ç¦»çš„æ¶æ„ï¼ŒåŒ…æ‹¬æ¨ç†æ¨¡å‹å’Œåˆ†å‰²æ¨¡å‹ã€‚æ¨ç†æ¨¡å‹è§£é‡Šç”¨æˆ·æ„å›¾ï¼Œç”Ÿæˆæ˜ç¡®çš„æ¨ç†é“¾ï¼Œå¹¶äº§ç”Ÿä½ç½®æç¤ºï¼Œè¿™äº›æç¤ºéšåè¢«åˆ†å‰²æ¨¡å‹ç”¨äºç”Ÿæˆçè´µçš„åƒç´ çº§æ©ç ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤æ‚çš„å¥–åŠ±æœºåˆ¶ï¼Œå°†æ ¼å¼å’Œå‡†ç¡®åº¦å¥–åŠ±ç»“åˆèµ·æ¥ï¼Œä»¥æœ‰æ•ˆåœ°å¼•å¯¼ä¼˜åŒ–æ–¹å‘ã€‚Seg-Zeroä»…é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨GRPOä¸”æ— éœ€æ˜ç¡®çš„æ¨ç†æ•°æ®ï¼Œå®ç°äº†å¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å±•ç°å‡ºæ–°å…´çš„æµ‹è¯•æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒSeg-Zero-7Båœ¨ReasonSegåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é›¶æ ·æœ¬æ€§èƒ½57.5åˆ†ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„LISA-7Bæ¨¡å‹é«˜å‡º18%ã€‚è¿™ä¸€æ˜¾è‘—çš„æå‡å‡¸æ˜¾äº†Seg-Zeroè·¨åŸŸæ³›åŒ–çš„èƒ½åŠ›ï¼ŒåŒæ—¶å‘ˆç°å‡ºæ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/dvlab-research/Seg-Zero%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/dvlab-research/Seg-Zeroè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06520v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Seg-Zeroæ˜¯ä¸€ä¸ªæ–°çš„æ¨ç†åˆ†å‰²æ¡†æ¶ï¼Œå®ƒé€šè¿‡è®¤çŸ¥å¼ºåŒ–è¿›è¡Œé›¶æ ·æœ¬å­¦ä¹ ï¼Œè¡¨ç°å‡ºå“è¶Šçš„å¯æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å…·å¤‡æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ã€‚å®ƒé‡‡ç”¨åˆ†ç¦»æ¶æ„ï¼ŒåŒ…æ‹¬æ¨ç†æ¨¡å‹å’Œåˆ†å‰²æ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡è®¾è®¡ç²¾ç»†çš„å¥–åŠ±æœºåˆ¶æ¥æŒ‡å¯¼ä¼˜åŒ–æ–¹å‘ï¼Œå®ç°ä»…é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæ— éœ€æ˜ç¡®çš„æ¨ç†æ•°æ®ã€‚åœ¨ReasonSegåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSeg-Zero-7Bå®ç°äº†é›¶æ ·æœ¬æ€§èƒ½57.5ï¼Œè¾ƒä¹‹å‰çš„LISA-7Bæé«˜äº†18%ï¼Œæ˜¾ç¤ºå‡ºäº†è·¨åŸŸæ³›åŒ–å’Œæ˜¾å¼æµ‹è¯•æ—¶æ¨ç†çš„èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Seg-Zeroæ¡†æ¶é€šè¿‡è®¤çŸ¥å¼ºåŒ–è¿›è¡Œæ¨ç†åˆ†å‰²ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¾èµ–æœ‰ç›‘ç£å¾®è°ƒä¸ç®€å•æè¿°çš„é—®é¢˜ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Seg-Zeroå¼•å…¥äº†åˆ†ç¦»æ¶æ„ï¼ŒåŒ…æ‹¬æ¨ç†æ¨¡å‹å’Œåˆ†å‰²æ¨¡å‹ï¼Œå…¶ä¸­æ¨ç†æ¨¡å‹è´Ÿè´£è§£æç”¨æˆ·æ„å›¾å¹¶ç”Ÿæˆæ˜ç¡®çš„æ¨ç†é“¾ï¼Œäº§ç”Ÿå®šä½æç¤ºä¾›åˆ†å‰²æ¨¡å‹ä½¿ç”¨ã€‚</li>
<li>è®¾è®¡äº†ç²¾ç»†çš„å¥–åŠ±æœºåˆ¶ï¼Œæ•´åˆäº†æ ¼å¼å’Œå‡†ç¡®æ€§çš„å¥–åŠ±ï¼Œæœ‰æ•ˆåœ°æŒ‡å¯¼äº†ä¼˜åŒ–æ–¹å‘ã€‚</li>
<li>Seg-Zeroé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€æ˜ç¡®çš„æ¨ç†æ•°æ®ï¼Œå®ç°äº†é›¶æ ·æœ¬å­¦ä¹ å’Œæµ‹è¯•æ—¶çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSeg-Zero-7Båœ¨ReasonSegåŸºå‡†æµ‹è¯•ä¸­çš„é›¶æ ·æœ¬æ€§èƒ½è¾¾åˆ°57.5ï¼Œè¾ƒä¹‹å‰çš„æ–¹æ³•æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>Seg-Zeroçš„ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</li>
<li>Seg-Zeroæ¡†æ¶å…·å¤‡æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œèƒ½å¤Ÿè§£é‡Šæ¨¡å‹åœ¨åˆ†å‰²ä»»åŠ¡ä¸­çš„å†³ç­–è·¯å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06520">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-044f58b9ca2883d585fcc2c81c2b3191.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-225bc13672df9d777a251c0dd28563b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13f7145b3306208cf35bcb71e0b756a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70799effed347dd839d2573234a821eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-426ae3a13b33f9590f20f730ed8845ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d9ec2d77729cb508c1b29fe0f86754eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfb8a1778cba0eaa805c1cf9d4a1c41c.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CalliReader-Contextualizing-Chinese-Calligraphy-via-an-Embedding-Aligned-Vision-Language-Model"><a href="#CalliReader-Contextualizing-Chinese-Calligraphy-via-an-Embedding-Aligned-Vision-Language-Model" class="headerlink" title="CalliReader: Contextualizing Chinese Calligraphy via an   Embedding-Aligned Vision-Language Model"></a>CalliReader: Contextualizing Chinese Calligraphy via an   Embedding-Aligned Vision-Language Model</h2><p><strong>Authors:Yuxuan Luo, Jiaqi Tang, Chenyi Huang, Feiyang Hao, Zhouhui Lian</strong></p>
<p>Chinese calligraphy, a UNESCO Heritage, remains computationally challenging due to visual ambiguity and cultural complexity. Existing AI systems fail to contextualize their intricate scripts, because of limited annotated data and poor visual-semantic alignment. We propose CalliReader, a vision-language model (VLM) that solves the Chinese Calligraphy Contextualization (CC$^2$) problem through three innovations: (1) character-wise slicing for precise character extraction and sorting, (2) CalliAlign for visual-text token compression and alignment, (3) embedding instruction tuning (e-IT) for improving alignment and addressing data scarcity. We also build CalliBench, the first benchmark for full-page calligraphic contextualization, addressing three critical issues in previous OCR and VQA approaches: fragmented context, shallow reasoning, and hallucination. Extensive experiments including user studies have been conducted to verify our CalliReaderâ€™s \textbf{superiority to other state-of-the-art methods and even human professionals in page-level calligraphy recognition and interpretation}, achieving higher accuracy while reducing hallucination. Comparisons with reasoning models highlight the importance of accurate recognition as a prerequisite for reliable comprehension. Quantitative analyses validate CalliReaderâ€™s efficiency; evaluations on document and real-world benchmarks confirm its robust generalization ability. </p>
<blockquote>
<p>ä¸­å›½ä¹¦æ³•ä½œä¸ºè”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡ï¼ˆUNESCOï¼‰é—äº§ï¼Œä»ç„¶å…·æœ‰è®¡ç®—ä¸Šçš„æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå­˜åœ¨è§†è§‰æ¨¡ç³Šå’Œæ–‡åŒ–å¤æ‚æ€§ã€‚ç°æœ‰çš„AIç³»ç»Ÿæ— æ³•å¯¹å…¶å¤æ‚çš„è„šæœ¬è¿›è¡Œè¯­å¢ƒåŒ–ç†è§£ï¼Œè¿™å—é™äºæ ‡æ³¨æ•°æ®çš„æœ‰é™æ€§å’Œè§†è§‰è¯­ä¹‰å¯¹é½çš„ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºCalliReaderï¼Œä¸€ç§è§£å†³ä¸­å›½ä¹¦æ³•è¯­å¢ƒåŒ–ï¼ˆCC$^2$ï¼‰é—®é¢˜çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚å®ƒé€šè¿‡ä¸‰ä¸ªåˆ›æ–°ç‚¹æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼šï¼ˆ1ï¼‰å­—ç¬¦çº§åˆ‡ç‰‡ï¼Œç”¨äºç²¾ç¡®å­—ç¬¦æå–å’Œæ’åºï¼›ï¼ˆ2ï¼‰CalliAlignï¼Œç”¨äºè§†è§‰æ–‡æœ¬ç¬¦å·å‹ç¼©å’Œå¯¹é½ï¼›ï¼ˆ3ï¼‰åµŒå…¥æŒ‡ä»¤å¾®è°ƒï¼ˆe-ITï¼‰ï¼Œç”¨äºæ”¹å–„å¯¹é½å¹¶è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†CalliBenchï¼Œé¦–ä¸ªå…¨é¡µä¹¦æ³•è¯­å¢ƒåŒ–çš„åŸºå‡†æµ‹è¯•ï¼Œè§£å†³äº†ä¹‹å‰OCRå’ŒVQAæ–¹æ³•ä¸­çš„ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–ã€æ¨ç†æµ…æ˜¾å’Œå¹»è§‰ã€‚è¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬ç”¨æˆ·ç ”ç©¶ï¼Œä»¥éªŒè¯æˆ‘ä»¬çš„CalliReaderåœ¨é¡µçº§ä¹¦æ³•è¯†åˆ«å’Œè§£é‡Šæ–¹é¢ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œç”šè‡³ä¼˜äºäººç±»ä¸“ä¸šäººå£«ï¼Œåœ¨æé«˜å‡†ç¡®æ€§çš„åŒæ—¶å‡å°‘äº†å¹»è§‰ã€‚ä¸æ¨ç†æ¨¡å‹çš„æ¯”è¾ƒçªæ˜¾äº†å‡†ç¡®è¯†åˆ«ä½œä¸ºå¯é ç†è§£å…ˆå†³æ¡ä»¶çš„é‡è¦æ€§ã€‚å®šé‡åˆ†æéªŒè¯äº†CalliReaderçš„æ•ˆç‡ï¼›åœ¨æ–‡æ¡£å’Œç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¯å®äº†å…¶ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06472v1">PDF</a> 11 pages</p>
<p><strong>Summary</strong>ï¼š</p>
<p>ä¸­å›½ä¹¦æ³•çš„è¯­å¢ƒåŒ–é—®é¢˜å› å…¶è§†è§‰æ¨¡ç³Šå’Œæ–‡åŒ–å¤æ‚æ€§ä»å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰çš„AIç³»ç»Ÿéš¾ä»¥å¤„ç†å…¶å¤æ‚çš„è„šæœ¬ï¼Œå› ä¸ºæœ‰é™çš„æ ‡æ³¨æ•°æ®å’Œè§†è§‰è¯­ä¹‰å¯¹é½ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†CalliReaderï¼Œä¸€ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œé€šè¿‡ä¸‰é¡¹åˆ›æ–°è§£å†³äº†ä¸­å›½ä¹¦æ³•çš„è¯­å¢ƒåŒ–é—®é¢˜ï¼šå­—ç¬¦çº§çš„åˆ‡ç‰‡æŠ€æœ¯ç”¨äºç²¾ç¡®å­—ç¬¦æå–å’Œæ’åºï¼›CalliAlignç”¨äºè§†è§‰æ–‡æœ¬ç¬¦å·å‹ç¼©å’Œå¯¹é½ï¼›åµŒå…¥æŒ‡ä»¤è°ƒæ•´ï¼ˆe-ITï¼‰æé«˜äº†å¯¹é½æ€§èƒ½å¹¶è§£å†³äº†æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†CalliBenchï¼Œé¦–ä¸ªå…¨é¡µä¹¦æ³•è¯­å¢ƒåŒ–çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œè§£å†³äº†ä¹‹å‰OCRå’ŒVQAæ–¹æ³•ä¸­çš„ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–ã€æ¨ç†æµ…æ˜¾å’Œå¹»è§‰ç°è±¡ã€‚é€šè¿‡å¤§é‡å®éªŒå’Œç”¨æˆ·ç ”ç©¶éªŒè¯äº†CalliReaderåœ¨ä¹¦æ³•è¯†åˆ«å’Œè§£è¯»æ–¹é¢çš„ä¼˜è¶Šæ€§ï¼Œå…¶å‡†ç¡®æ€§é«˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•å’Œä¸“ä¸šäººç±»ä¸“å®¶ï¼ŒåŒæ—¶é™ä½äº†å¹»è§‰ç°è±¡ã€‚ä¸æ¨ç†æ¨¡å‹çš„æ¯”è¾ƒå‡¸æ˜¾äº†å‡†ç¡®è¯†åˆ«ä½œä¸ºå¯é ç†è§£å…ˆå†³æ¡ä»¶çš„é‡è¦æ€§ã€‚å®šé‡åˆ†æéªŒè¯äº†CalliReaderçš„æ•ˆç‡ï¼Œå…¶åœ¨æ–‡æ¡£å’Œç°å®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä¸­å›½ä¹¦æ³•ä½œä¸ºè”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡ï¼ˆUNESCOï¼‰é—äº§ï¼Œå…¶è¯­å¢ƒåŒ–é—®é¢˜ä»å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸»è¦ç”±äºè§†è§‰æ¨¡ç³Šå’Œæ–‡åŒ–å¤æ‚æ€§ã€‚</li>
<li>ç°æœ‰AIç³»ç»Ÿéš¾ä»¥å¤„ç†å¤æ‚ä¹¦æ³•è„šæœ¬ï¼Œå­˜åœ¨æ ‡æ³¨æ•°æ®æœ‰é™å’Œè§†è§‰è¯­ä¹‰å¯¹é½ä¸ä½³çš„é—®é¢˜ã€‚</li>
<li>CalliReaderé€šè¿‡ä¸‰é¡¹åˆ›æ–°è§£å†³ä¸­å›½ä¹¦æ³•è¯­å¢ƒåŒ–é—®é¢˜ï¼šå­—ç¬¦æå–æ’åºã€è§†è§‰æ–‡æœ¬ç¬¦å·å‹ç¼©å¯¹é½åŠåµŒå…¥æŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>CalliBenchä½œä¸ºé¦–ä¸ªå…¨é¡µä¹¦æ³•è¯­å¢ƒåŒ–çš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œè§£å†³äº†ä¹‹å‰OCRå’ŒVQAæ–¹æ³•çš„ä¸‰ä¸ªå…³é”®é—®é¢˜ã€‚</li>
<li>CalliReaderåœ¨ä¹¦æ³•è¯†åˆ«å’Œè§£è¯»æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå‡†ç¡®æ€§é«˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•å’Œä¸“ä¸šäººç±»ä¸“å®¶ï¼Œå¹¶é™ä½äº†å¹»è§‰ç°è±¡ã€‚</li>
<li>å‡†ç¡®è¯†åˆ«æ˜¯å¯é ç†è§£çš„å‰ææ¡ä»¶ï¼Œä¸æ¨ç†æ¨¡å‹çš„æ¯”è¾ƒå‡¸æ˜¾äº†å…¶é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06472">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15d210a1959277cee62dc1a65be47b0f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-568a8ec14eee635810565d5da1f3332e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36be24ed024289de1dc627a3138f98fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c893546b2b2f442e9537babc6e225c3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54a95fc09bdc8be6d33a50a811d85885.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-611f1d22f58d3fe8ea4ca932bc1818fc.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Identifying-Evidence-Subgraphs-for-Financial-Risk-Detection-via-Graph-Counterfactual-and-Factual-Reasoning"><a href="#Identifying-Evidence-Subgraphs-for-Financial-Risk-Detection-via-Graph-Counterfactual-and-Factual-Reasoning" class="headerlink" title="Identifying Evidence Subgraphs for Financial Risk Detection via Graph   Counterfactual and Factual Reasoning"></a>Identifying Evidence Subgraphs for Financial Risk Detection via Graph   Counterfactual and Factual Reasoning</h2><p><strong>Authors:Huaming Du, Lei Yuan, Qing Yang, Xingyan Chen, Yu Zhao, Han Ji, Fuzhen Zhuang, Carl Yang, Gang Kou</strong></p>
<p>Company financial risks pose a significant threat to personal wealth and national economic stability, stimulating increasing attention towards the development of efficient andtimely methods for monitoring them. Current approaches tend to use graph neural networks (GNNs) to model the momentum spillover effect of risks. However, due to the black-box nature of GNNs, these methods leave much to be improved for precise and reliable explanations towards company risks. In this paper, we propose CF3, a novel Counterfactual and Factual learning method for company Financial risk detection, which generates evidence subgraphs on company knowledge graphs to reliably detect and explain company financial risks. Specifically, we first propose a meta-path attribution process based on Granger causality, selecting the meta-paths most relevant to the target node labels to construct an attribution subgraph. Subsequently, we propose anedge-type-aware graph generator to identify important edges, and we also devise a layer-based feature masker to recognize crucial node features. Finally, we utilize counterfactual-factual reasoning and a loss function based on attribution subgraphs to jointly guide the learning of the graph generator and feature masker. Extensive experiments on three real-world datasets demonstrate the superior performance of our method compared to state-of-the-art approaches in the field of financial risk detection. </p>
<blockquote>
<p>å…¬å¸è´¢åŠ¡é£é™©å¯¹ä¸ªäººè´¢å¯Œå’Œå›½å®¶ç»æµç¨³å®šæ„æˆé‡å¤§å¨èƒï¼Œä¿ƒä½¿äººä»¬è¶Šæ¥è¶Šå…³æ³¨å‘å±•æœ‰æ•ˆä¸”åŠæ—¶çš„æ–¹æ³•æ¥ç›‘æ§è¿™äº›é£é™©ã€‚å½“å‰çš„æ–¹æ³•å€¾å‘äºä½¿ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰æ¥æ¨¡æ‹Ÿé£é™©çš„åŠ¨é‡æº¢å‡ºæ•ˆåº”ã€‚ç„¶è€Œï¼Œç”±äºå›¾ç¥ç»ç½‘ç»œçš„é»‘ç®±æ€§è´¨ï¼Œè¿™äº›æ–¹æ³•åœ¨é’ˆå¯¹å…¬å¸é£é™©è¿›è¡Œç²¾ç¡®å’Œå¯é çš„è§£é‡Šæ–¹é¢ä»æœ‰å¾…æ”¹è¿›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CF3ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå…¬å¸è´¢åŠ¡é£é™©æ£€æµ‹çš„æ–°å‹åäº‹å®ä¸äº‹å®å­¦ä¹ æ–¹æ³•ï¼Œå®ƒå¯ä»¥åœ¨å…¬å¸çŸ¥è¯†å›¾è°±ä¸Šç”Ÿæˆè¯æ®å­å›¾ï¼Œä»¥å¯é åœ°æ£€æµ‹å’Œè§£é‡Šå…¬å¸è´¢åŠ¡é£é™©ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåŸºäºæ ¼å…°æ°å› æœå…³ç³»æå‡ºä¸€ç§å…ƒè·¯å¾„å½’å±è¿‡ç¨‹ï¼Œé€‰æ‹©ä¸ç›®æ ‡èŠ‚ç‚¹æ ‡ç­¾æœ€ç›¸å…³çš„å…ƒè·¯å¾„æ¥æ„å»ºå½’å±å­å›¾ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¾¹ç¼˜ç±»å‹æ„ŸçŸ¥å›¾ç”Ÿæˆå™¨æ¥è¯†åˆ«é‡è¦è¾¹ç¼˜ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§åŸºäºå±‚çš„ç‰¹å¾æ©è”½å™¨æ¥è¯†åˆ«å…³é”®èŠ‚ç‚¹ç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ©ç”¨åäº‹å®-äº‹å®æ¨ç†å’ŒåŸºäºå½’å±å­å›¾çš„æŸå¤±å‡½æ•°æ¥å…±åŒæŒ‡å¯¼å›¾ç”Ÿæˆå™¨å’Œç‰¹å¾æ©è”½å™¨çš„å­¦ä¹ ã€‚åœ¨ä¸‰ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è´¢åŠ¡é£é™©æ£€æµ‹é¢†åŸŸçš„è¡¨ç°ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06441v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå›¾ç¥ç»ç½‘ç»œçš„é£é™©æº¢å‡ºèµ„æ•ˆæ¨¡å‹å¯¹å…¬å¸è´¢åŠ¡é£é™©è¿›è¡Œæ£€æµ‹æ—¶ï¼Œå­˜åœ¨ç²¾ç¡®åº¦å’Œå¯é æ€§æ–¹é¢çš„ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCF3çš„æ–°å‹æ··åˆå­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å…¬å¸çŸ¥è¯†å›¾è°±ç”Ÿæˆè¯æ®å­å›¾ï¼Œæ›´å¯é åœ°æ£€æµ‹å’Œè§£é‡Šå…¬å¸è´¢åŠ¡é£é™©ã€‚å®éªŒè¯æ˜ï¼Œä¸ç°æœ‰è´¢åŠ¡é£é™©æ£€æµ‹æ–¹æ³•ç›¸æ¯”ï¼ŒCF3å…·æœ‰å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…¬å¸è´¢åŠ¡é£é™©å¯¹ä¸ªäººè´¢å¯Œå’Œå›½å®¶ç»æµç¨³å®šæ„æˆå¨èƒï¼Œå¼•å‘äº†é£é™©ç›‘æµ‹æ–¹æ³•çš„å…³æ³¨å’Œå‘å±•éœ€æ±‚ã€‚</li>
<li>å½“å‰æ–¹æ³•å€¾å‘äºä½¿ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰æ¥æ¨¡æ‹Ÿé£é™©çš„åŠ¨é‡æº¢å‡ºæ•ˆåº”ï¼Œä½†å­˜åœ¨è§£é‡Šç²¾åº¦å’Œå¯é æ€§é—®é¢˜ã€‚</li>
<li>CF3æ–¹æ³•é€šè¿‡ç”Ÿæˆè¯æ®å­å›¾æ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œè¿™äº›å­å›¾åŸºäºå…¬å¸çŸ¥è¯†å›¾è°±æ„å»ºã€‚</li>
<li>CF3é‡‡ç”¨åŸºäºGrangerå› æœå…³ç³»çš„å…ƒè·¯å¾„å½’å±è¿‡ç¨‹ï¼Œé€‰æ‹©ä¸ç›®æ ‡èŠ‚ç‚¹æ ‡ç­¾æœ€ç›¸å…³çš„å…ƒè·¯å¾„æ¥æ„å»ºå½’å±å­å›¾ã€‚</li>
<li>CF3è¿˜æå‡ºäº†è¾¹ç¼˜ç±»å‹æ„ŸçŸ¥å›¾ç”Ÿæˆå™¨å’ŒåŸºäºå±‚çš„ç‰¹å¾æ©ç å™¨æ¥è¯†åˆ«é‡è¦è¾¹ç¼˜å’Œå…³é”®èŠ‚ç‚¹ç‰¹å¾ã€‚</li>
<li>ä½¿ç”¨å½’å› å­å›¾å¼•å¯¼çš„è”åˆå­¦ä¹ å’ŒæŸå¤±å‡½æ•°å®ç°äº†å¯¹å›¾ç”Ÿæˆå™¨å’Œç‰¹å¾æ©ç å™¨çš„æœ‰æ•ˆå­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06441">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5a6e572be8d70ea9243ece6b5d00db2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03cace22510b9e93f352848812e64f7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab05752fb59817b63252cafecb0f26e0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Pre-Training-Meta-Rule-Selection-Policy-for-Visual-Generative-Abductive-Learning"><a href="#Pre-Training-Meta-Rule-Selection-Policy-for-Visual-Generative-Abductive-Learning" class="headerlink" title="Pre-Training Meta-Rule Selection Policy for Visual Generative Abductive   Learning"></a>Pre-Training Meta-Rule Selection Policy for Visual Generative Abductive   Learning</h2><p><strong>Authors:Yu Jin, Jingming Liu, Zhexu Luo, Yifei Peng, Ziang Qin, Wang-Zhou Dai, Yao-Xiang Ding, Kun Zhou</strong></p>
<p>Visual generative abductive learning studies jointly training symbol-grounded neural visual generator and inducing logic rules from data, such that after learning, the visual generation process is guided by the induced logic rules. A major challenge for this task is to reduce the time cost of logic abduction during learning, an essential step when the logic symbol set is large and the logic rule to induce is complicated. To address this challenge, we propose a pre-training method for obtaining meta-rule selection policy for the recently proposed visual generative learning approach AbdGen [Peng et al., 2023], aiming at significantly reducing the candidate meta-rule set and pruning the search space. The selection model is built based on the embedding representation of both symbol grounding of cases and meta-rules, which can be effectively integrated with both neural model and logic reasoning system. The pre-training process is done on pure symbol data, not involving symbol grounding learning of raw visual inputs, making the entire learning process low-cost. An additional interesting observation is that the selection policy can rectify symbol grounding errors unseen during pre-training, which is resulted from the memorization ability of attention mechanism and the relative stability of symbolic patterns. Experimental results show that our method is able to effectively address the meta-rule selection problem for visual abduction, boosting the efficiency of visual generative abductive learning. Code is available at <a target="_blank" rel="noopener" href="https://github.com/future-item/metarule-select">https://github.com/future-item/metarule-select</a>. </p>
<blockquote>
<p>è§†è§‰ç”Ÿæˆå½’çº³å­¦ä¹ è”åˆè®­ç»ƒç¬¦å·åŸºåœ°ç¥ç»è§†è§‰ç”Ÿæˆå™¨å’Œä»æ•°æ®ä¸­å½’çº³é€»è¾‘è§„åˆ™ï¼Œä»¥ä¾¿åœ¨å­¦ä¹ åï¼Œè§†è§‰ç”Ÿæˆè¿‡ç¨‹å—å½’çº³å¾—åˆ°çš„é€»è¾‘è§„åˆ™æŒ‡å¯¼ã€‚æ­¤ä»»åŠ¡çš„ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯å‡å°‘å­¦ä¹ è¿‡ç¨‹ä¸­çš„é€»è¾‘å½’çº³æ—¶é—´æˆæœ¬ï¼Œå½“é€»è¾‘ç¬¦å·é›†åºå¤§ã€è¦å½’çº³çš„é€»è¾‘è§„åˆ™å¤æ‚æ—¶ï¼Œè¿™æ˜¯è‡³å…³é‡è¦çš„æ­¥éª¤ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é’ˆå¯¹æœ€è¿‘æå‡ºçš„è§†è§‰ç”Ÿæˆå­¦ä¹ æ–¹æ³•AbdGen[Pengç­‰äººï¼Œ2023]æå‡ºäº†å…ƒè§„åˆ™é€‰æ‹©ç­–ç•¥çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æ˜¾è‘—å‡å°‘å€™é€‰å…ƒè§„åˆ™é›†ï¼Œç¼©å°æœç´¢ç©ºé—´ã€‚é€‰æ‹©æ¨¡å‹æ˜¯åŸºäºæ¡ˆä¾‹ç¬¦å·æ¥åœ°å’Œå…ƒè§„åˆ™çš„åµŒå…¥è¡¨ç¤ºçš„ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä¸ç¥ç»æ¨¡å‹å’Œé€»è¾‘æ¨ç†ç³»ç»Ÿç›¸ç»“åˆã€‚é¢„è®­ç»ƒè¿‡ç¨‹æ˜¯åœ¨çº¯ç¬¦å·æ•°æ®ä¸Šè¿›è¡Œçš„ï¼Œä¸æ¶‰åŠåŸå§‹è§†è§‰è¾“å…¥çš„ç¬¦å·æ¥åœ°å­¦ä¹ ï¼Œä½¿æ•´ä¸ªè¿‡ç¨‹æˆæœ¬ä½å»‰ã€‚å¦ä¸€ä¸ªæœ‰è¶£çš„è§‚å¯Ÿç»“æœæ˜¯ï¼Œé€‰æ‹©ç­–ç•¥å¯ä»¥çº æ­£é¢„è®­ç»ƒæœŸé—´æœªè§è¿‡çš„ç¬¦å·æ¥åœ°é”™è¯¯ï¼Œè¿™æ˜¯ç”±æ³¨æ„åŠ›æœºåˆ¶çš„è®°å¿†èƒ½åŠ›å’Œç¬¦å·æ¨¡å¼çš„ç›¸å¯¹ç¨³å®šæ€§æ‰€å¯¼è‡´çš„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³è§†è§‰å½’çº³ä¸­çš„å…ƒè§„åˆ™é€‰æ‹©é—®é¢˜ï¼Œæé«˜è§†è§‰ç”Ÿæˆå½’çº³å­¦ä¹ çš„æ•ˆç‡ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/future-item/metarule-select%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/future-item/metarule-selectè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06427v1">PDF</a> Published as a conference paper at IJCLRâ€™24</p>
<p><strong>Summary</strong><br>è§†è§‰ç”Ÿæˆå½’çº³å­¦ä¹ é€šè¿‡è”åˆè®­ç»ƒç¬¦å·åŒ–ç¥ç»è§†è§‰ç”Ÿæˆå™¨å’Œä»æ•°æ®ä¸­å½’çº³é€»è¾‘è§„åˆ™ï¼ŒæŒ‡å¯¼è§†è§‰ç”Ÿæˆè¿‡ç¨‹ã€‚é’ˆå¯¹é€»è¾‘ç¬¦å·é›†å¤§ã€å½’çº³é€»è¾‘è§„åˆ™å¤æ‚æ—¶é€»è¾‘å½’çº³è€—æ—¶çš„é—®é¢˜ï¼Œæå‡ºä¸€ç§é¢„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨æ˜¾è‘—å‡å°‘å€™é€‰å…ƒè§„åˆ™é›†ï¼Œç¼©å°æœç´¢ç©ºé—´ã€‚é€‰æ‹©æ¨¡å‹åŸºäºæ¡ˆä¾‹å’Œå…ƒè§„åˆ™çš„ç¬¦å·åµŒå…¥è¡¨ç¤ºï¼Œå¯æœ‰æ•ˆåœ°ä¸ç¥ç»æ¨¡å‹å’Œé€»è¾‘æ¨ç†ç³»ç»Ÿç›¸ç»“åˆã€‚é¢„è®­ç»ƒè¿‡ç¨‹ä»…åœ¨ç¬¦å·æ•°æ®ä¸Šè¿›è¡Œï¼Œä¸æ¶‰åŠåŸå§‹è§†è§‰è¾“å…¥çš„ç¬¦å·æ¥åœ°å­¦ä¹ ï¼Œä½¿æ•´ä¸ªè¿‡ç¨‹æˆæœ¬é™ä½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆè§£å†³è§†è§‰å½’çº³å­¦ä¹ çš„å…ƒè§„åˆ™é€‰æ‹©é—®é¢˜ï¼Œæé«˜è§†è§‰ç”Ÿæˆå½’çº³å­¦ä¹ çš„æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰ç”Ÿæˆå½’çº³å­¦ä¹ è”åˆè®­ç»ƒç¬¦å·åŒ–ç¥ç»è§†è§‰ç”Ÿæˆå™¨å’Œä»æ•°æ®ä¸­å½’çº³é€»è¾‘è§„åˆ™ã€‚</li>
<li>é¢ä¸´é€»è¾‘ç¬¦å·é›†å¤§ã€é€»è¾‘è§„åˆ™å¤æ‚æ—¶ï¼Œé€»è¾‘å½’çº³è€—æ—¶çš„é—®é¢˜ã€‚</li>
<li>æå‡ºé¢„è®­ç»ƒæ–¹æ³•ï¼Œæ—¨åœ¨å‡å°‘å€™é€‰å…ƒè§„åˆ™é›†ï¼Œç¼©å°æœç´¢ç©ºé—´ã€‚</li>
<li>é€‰æ‹©æ¨¡å‹åŸºäºç¬¦å·åµŒå…¥è¡¨ç¤ºï¼Œç»“åˆç¥ç»æ¨¡å‹å’Œé€»è¾‘æ¨ç†ç³»ç»Ÿã€‚</li>
<li>é¢„è®­ç»ƒè¿‡ç¨‹åœ¨ç¬¦å·æ•°æ®ä¸Šè¿›è¡Œï¼Œä¸æ¶‰åŠåŸå§‹è§†è§‰è¾“å…¥çš„ç¬¦å·æ¥åœ°å­¦ä¹ ã€‚</li>
<li>é¢„è®­ç»ƒèƒ½å¤Ÿæé«˜è§†è§‰ç”Ÿæˆå½’çº³å­¦ä¹ çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06427">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8497abe5d466a113a9768c0b57b6d43.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-375567501828774e943379776f74dd34.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6208057fb7fc2e27311e97301add940d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ac0ad33dd30f7b72e2e2570c0aa0caa2.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Advancing-Autonomous-Vehicle-Intelligence-Deep-Learning-and-Multimodal-LLM-for-Traffic-Sign-Recognition-and-Robust-Lane-Detection"><a href="#Advancing-Autonomous-Vehicle-Intelligence-Deep-Learning-and-Multimodal-LLM-for-Traffic-Sign-Recognition-and-Robust-Lane-Detection" class="headerlink" title="Advancing Autonomous Vehicle Intelligence: Deep Learning and Multimodal   LLM for Traffic Sign Recognition and Robust Lane Detection"></a>Advancing Autonomous Vehicle Intelligence: Deep Learning and Multimodal   LLM for Traffic Sign Recognition and Robust Lane Detection</h2><p><strong>Authors:Chandan Kumar Sah, Ankit Kumar Shaw, Xiaoli Lian, Arsalan Shahid Baig, Tuopu Wen, Kun Jiang, Mengmeng Yang, Diange Yang</strong></p>
<p>Autonomous vehicles (AVs) require reliable traffic sign recognition and robust lane detection capabilities to ensure safe navigation in complex and dynamic environments. This paper introduces an integrated approach combining advanced deep learning techniques and Multimodal Large Language Models (MLLMs) for comprehensive road perception. For traffic sign recognition, we systematically evaluate ResNet-50, YOLOv8, and RT-DETR, achieving state-of-the-art performance of 99.8% with ResNet-50, 98.0% accuracy with YOLOv8, and achieved 96.6% accuracy in RT-DETR despite its higher computational complexity. For lane detection, we propose a CNN-based segmentation method enhanced by polynomial curve fitting, which delivers high accuracy under favorable conditions. Furthermore, we introduce a lightweight, Multimodal, LLM-based framework that directly undergoes instruction tuning using small yet diverse datasets, eliminating the need for initial pretraining. This framework effectively handles various lane types, complex intersections, and merging zones, significantly enhancing lane detection reliability by reasoning under adverse conditions. Despite constraints in available training resources, our multimodal approach demonstrates advanced reasoning capabilities, achieving a Frame Overall Accuracy (FRM) of 53.87%, a Question Overall Accuracy (QNS) of 82.83%, lane detection accuracies of 99.6% in clear conditions and 93.0% at night, and robust performance in reasoning about lane invisibility due to rain (88.4%) or road degradation (95.6%). The proposed comprehensive framework markedly enhances AV perception reliability, thus contributing significantly to safer autonomous driving across diverse and challenging road scenarios. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVsï¼‰éœ€è¦å¯é çš„äº¤é€šæ ‡å¿—è¯†åˆ«å’Œç¨³å¥çš„è½¦é“æ£€æµ‹èƒ½åŠ›ï¼Œä»¥ç¡®ä¿åœ¨å¤æ‚å’ŒåŠ¨æ€ç¯å¢ƒä¸­çš„å®‰å…¨å¯¼èˆªã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆå…ˆè¿›æ·±åº¦å­¦ä¹ æŠ€æœ¯å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç»¼åˆæ–¹æ³•ï¼Œç”¨äºå…¨é¢çš„é“è·¯æ„ŸçŸ¥ã€‚åœ¨äº¤é€šæ ‡å¿—è¯†åˆ«æ–¹é¢ï¼Œæˆ‘ä»¬å¯¹ResNet-50ã€YOLOv8å’ŒRT-DETRè¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œä½¿ç”¨ResNet-50è¾¾åˆ°äº†99.8%çš„ä¸šç•Œé¢†å…ˆæ€§èƒ½ï¼ŒYOLOv8çš„å‡†ç¡®ç‡ä¸º98.0%ï¼Œå°½ç®¡RT-DETRçš„è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ï¼Œä½†å‡†ç¡®ç‡è¾¾åˆ°äº†96.6%ã€‚å¯¹äºè½¦é“æ£€æµ‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºCNNçš„åˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡å¤šé¡¹å¼æ›²çº¿æ‹Ÿåˆå¢å¼ºï¼Œåœ¨æœ‰åˆ©æ¡ä»¶ä¸‹å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„ã€åŸºäºå¤šæ¨¡æ€LLMçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç›´æ¥ä½¿ç”¨å°å‹ä½†å¤šæ ·åŒ–çš„æ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œæ— éœ€åˆå§‹é¢„è®­ç»ƒã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å„ç§è½¦é“ç±»å‹ã€å¤æ‚çš„äº¤å‰å£å’Œåˆå¹¶åŒºï¼Œé€šè¿‡ä¸è‰¯æ¡ä»¶ä¸‹çš„æ¨ç†ï¼Œæ˜¾è‘—æé«˜è½¦é“æ£€æµ‹çš„å¯é æ€§ã€‚å°½ç®¡å¯ç”¨è®­ç»ƒèµ„æºå­˜åœ¨çº¦æŸï¼Œæˆ‘ä»¬çš„å¤šæ¨¡æ€æ–¹æ³•å±•ç¤ºäº†é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œå®ç°äº†å¸§æ€»ä½“å‡†ç¡®åº¦ï¼ˆFRMï¼‰ä¸º53.87%ï¼Œé—®é¢˜æ€»ä½“å‡†ç¡®åº¦ï¼ˆQNSï¼‰ä¸º82.83%ï¼Œåœ¨æ¸…æ™°æ¡ä»¶ä¸‹çš„è½¦é“æ£€æµ‹å‡†ç¡®ç‡ä¸º99.6%ï¼Œå¤œé—´ä¸º93.0%ï¼Œå¹¶ä¸”åœ¨åº”å¯¹å› é›¨æ°´å¯¼è‡´è½¦é“éšå½¢ï¼ˆ88.4%ï¼‰æˆ–é“è·¯é€€åŒ–ï¼ˆ95.6%ï¼‰çš„æ¨ç†ä¸­è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚æ‰€æå‡ºçš„ç»¼åˆæ¡†æ¶æ˜¾è‘—æé«˜äº†è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥çš„å¯é æ€§ï¼Œä»è€Œä¸ºå„ç§å…·æœ‰æŒ‘æˆ˜çš„é“è·¯åœºæ™¯ä¸‹çš„æ›´å®‰å…¨è‡ªåŠ¨é©¾é©¶åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06313v1">PDF</a> 11 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆæ·±åº¦å­¦ä¹ å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é›†æˆæ–¹æ³•ï¼Œç”¨äºå…¨é¢çš„é“è·¯æ„ŸçŸ¥ã€‚äº¤é€šæ ‡å¿—è¯†åˆ«æ–¹é¢ï¼Œç³»ç»Ÿè¯„ä¼°äº†ResNet-50ã€YOLOv8å’ŒRT-DETRçš„æ€§èƒ½ï¼Œå…¶ä¸­ResNet-50è¾¾åˆ°99.8%çš„å‡†ç¡®ç‡ã€‚è½¦é“æ£€æµ‹æ–¹é¢ï¼Œæå‡ºäº†ä¸€ç§åŸºäºCNNçš„åˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡å¤šé¡¹å¼æ›²çº¿æ‹Ÿåˆå¢å¼ºï¼Œåœ¨æœ‰åˆ©æ¡ä»¶ä¸‹å…·æœ‰é«˜å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†è½»é‡çº§çš„MLLMsæ¡†æ¶ï¼Œå¯ç›´æ¥é€šè¿‡æŒ‡ä»¤è°ƒæ•´ä½¿ç”¨å°å‹çš„å¤šæ ·åŒ–æ•°æ®é›†ï¼Œæ— éœ€åˆå§‹é¢„è®­ç»ƒã€‚è¯¥æ¡†æ¶èƒ½æœ‰æ•ˆåº”å¯¹å„ç§è½¦é“ç±»å‹ã€å¤æ‚äº¤å‰è·¯å£å’Œåˆå¹¶åŒºï¼Œæé«˜äº†è½¦é“æ£€æµ‹çš„å¯é æ€§ã€‚å°½ç®¡è®­ç»ƒèµ„æºæœ‰é™ï¼Œè¯¥å¤šæ¨¡æ€æ–¹æ³•ä»å±•ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œåœ¨æ¸…æ™°å’Œå¤œé—´æ¡ä»¶ä¸‹çš„è½¦é“æ£€æµ‹å‡†ç¡®ç‡åˆ†åˆ«é«˜è¾¾99.6%å’Œ93.0%ï¼Œå¹¶ä¸”åœ¨å¤„ç†å› é™é›¨æˆ–é“è·¯é€€åŒ–å¯¼è‡´çš„è½¦é“éšå½¢é—®é¢˜æ–¹é¢ä¹Ÿè¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„æ„ŸçŸ¥å¯é æ€§æä¾›äº†æ˜¾è‘—è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªä¸»è½¦è¾†éœ€è¦å¯é çš„äº¤é€šæ ‡å¿—è¯†åˆ«å’Œç¨³å¥çš„è½¦é“æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>ç»“åˆæ·±åº¦å­¦ä¹ å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„é›†æˆæ–¹æ³•ç”¨äºå…¨é¢çš„é“è·¯æ„ŸçŸ¥ã€‚</li>
<li>äº¤é€šæ ‡å¿—è¯†åˆ«æ–¹é¢ï¼ŒResNet-50è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œè¾¾åˆ°99.8%çš„å‡†ç¡®ç‡ã€‚</li>
<li>æå‡ºäº†åŸºäºCNNçš„åˆ†å‰²æ–¹æ³•å’Œå¤šé¡¹å¼æ›²çº¿æ‹Ÿåˆå¢å¼ºè½¦é“æ£€æµ‹ã€‚</li>
<li>å¼•å…¥è½»é‡çº§MLLMsæ¡†æ¶ï¼Œå¯ç›´æ¥é€šè¿‡æŒ‡ä»¤è°ƒæ•´ï¼Œé€‚åº”ä¸åŒçš„è½¦é“åœºæ™¯ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å¤šç§æ¡ä»¶ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ¸…æ™°å’Œå¤œé—´æ¡ä»¶ã€å¤æ‚äº¤å‰è·¯å£å’Œåˆå¹¶åŒºã€‚</li>
<li>ç ”ç©¶ä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†çš„æ„ŸçŸ¥å¯é æ€§æä¾›äº†æ˜¾è‘—è´¡çŒ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c438c3c587ae035cf1e81af673c58ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c94dcb2b739034612df654c3877700b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c33a702dca3a97f1dbf2a47c4fa95243.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13e140e95ffb5b785407b50f1f827394.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1f32537be44819f6639d8b2873168cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e7ebf692272209ae4134a8284dbb2aa.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-22c1cab13206bae677ca24ed494c6e63.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  V2Flow Unifying Visual Tokenization and Large Language Model   Vocabularies for Autoregressive Image Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-11/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9072ed3baaec2f60af9032078908f11b.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-11  StickMotion Generating 3D Human Motions by Drawing a Stickman
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26522.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
