<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  V2Flow Unifying Visual Tokenization and Large Language Model   Vocabularies for Autoregressive Image Generation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-22c1cab13206bae677ca24ed494c6e63.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    85 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-12-æ›´æ–°"><a href="#2025-03-12-æ›´æ–°" class="headerlink" title="2025-03-12 æ›´æ–°"></a>2025-03-12 æ›´æ–°</h1><h2 id="V2Flow-Unifying-Visual-Tokenization-and-Large-Language-Model-Vocabularies-for-Autoregressive-Image-Generation"><a href="#V2Flow-Unifying-Visual-Tokenization-and-Large-Language-Model-Vocabularies-for-Autoregressive-Image-Generation" class="headerlink" title="V2Flow: Unifying Visual Tokenization and Large Language Model   Vocabularies for Autoregressive Image Generation"></a>V2Flow: Unifying Visual Tokenization and Large Language Model   Vocabularies for Autoregressive Image Generation</h2><p><strong>Authors:Guiwei Zhang, Tianyu Zhang, Mohan Zhou, Yalong Bai, Biye Li</strong></p>
<p>We propose V2Flow, a novel tokenizer that produces discrete visual tokens capable of high-fidelity reconstruction, while ensuring structural and latent distribution alignment with the vocabulary space of large language models (LLMs). Leveraging this tight visual-vocabulary coupling, V2Flow enables autoregressive visual generation on top of existing LLMs. Our approach formulates visual tokenization as a flow-matching problem, aiming to learn a mapping from a standard normal prior to the continuous image distribution, conditioned on token sequences embedded within the LLMs vocabulary space. The effectiveness of V2Flow stems from two core designs. First, we propose a Visual Vocabulary resampler, which compresses visual data into compact token sequences, with each represented as a soft categorical distribution over LLMâ€™s vocabulary. This allows seamless integration of visual tokens into existing LLMs for autoregressive visual generation. Second, we present a masked autoregressive Rectified-Flow decoder, employing a masked transformer encoder-decoder to refine visual tokens into contextually enriched embeddings. These embeddings then condition a dedicated velocity field for precise reconstruction. Additionally, an autoregressive rectified-flow sampling strategy is incorporated, ensuring flexible sequence lengths while preserving competitive reconstruction quality. Extensive experiments show that V2Flow outperforms mainstream VQ-based tokenizers and facilitates autoregressive visual generation on top of existing. <a target="_blank" rel="noopener" href="https://github.com/zhangguiwei610/V2Flow">https://github.com/zhangguiwei610/V2Flow</a> </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†V2Flowï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åˆ†è¯å™¨ï¼Œèƒ½å¤Ÿäº§ç”Ÿç¦»æ•£çš„å¯è§†åŒ–ä»¤ç‰Œï¼Œå®ç°é«˜ä¿çœŸé‡å»ºï¼ŒåŒæ—¶ç¡®ä¿ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯æ±‡ç©ºé—´çš„ç»“æ„å’Œæ½œåœ¨åˆ†å¸ƒå¯¹é½ã€‚åˆ©ç”¨è¿™ç§ç´§å¯†çš„è§†è§‰è¯æ±‡è€¦åˆï¼ŒV2Flowå¯ä»¥åœ¨ç°æœ‰çš„LLMä¹‹ä¸Šå®ç°è‡ªå›å½’çš„è§†è§‰ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è§†è§‰åˆ†è¯è¡¨è¿°ä¸ºä¸€ä¸ªæµé‡åŒ¹é…é—®é¢˜ï¼Œæ—¨åœ¨ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒå­¦ä¹ æ˜ å°„åˆ°è¿ç»­çš„å›¾åƒåˆ†å¸ƒï¼Œæ ¹æ®åµŒå…¥åœ¨LLMè¯æ±‡ç©ºé—´ä¸­çš„ä»¤ç‰Œåºåˆ—è¿›è¡Œæ¡ä»¶åŒ–ã€‚V2Flowçš„æœ‰æ•ˆæ€§æºäºä¸¤ä¸ªæ ¸å¿ƒè®¾è®¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è§†è§‰è¯æ±‡é‡é‡‡æ ·å™¨ï¼Œå®ƒå°†è§†è§‰æ•°æ®å‹ç¼©æˆç´§å‡‘çš„ä»¤ç‰Œåºåˆ—ï¼Œæ¯ä¸ªåºåˆ—éƒ½è¡¨ç¤ºä¸ºLLMè¯æ±‡ä¸Šçš„è½¯ç±»åˆ«åˆ†å¸ƒã€‚è¿™å…è®¸æ— ç¼åœ°å°†è§†è§‰ä»¤ç‰Œé›†æˆåˆ°ç°æœ‰çš„LLMä¸­ï¼Œç”¨äºè‡ªå›å½’çš„è§†è§‰ç”Ÿæˆã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¸¦æ©ç çš„è‡ªåŠ¨å›å½’ä¿®æ­£æµè§£ç å™¨ï¼Œé‡‡ç”¨å¸¦æ©ç çš„å˜å‹å™¨ç¼–ç å™¨-è§£ç å™¨æ¥ä¼˜åŒ–è§†è§‰ä»¤ç‰Œä¸ºä¸Šä¸‹æ–‡ä¸°å¯Œçš„åµŒå…¥ã€‚è¿™äº›åµŒå…¥ç„¶åä¸ºç²¾ç¡®é‡å»ºè®¾ç½®ä¸€ä¸ªä¸“ç”¨çš„é€Ÿåº¦åœºã€‚æ­¤å¤–ï¼Œè¿˜èå…¥äº†è‡ªå›å½’ä¿®æ­£æµé‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿çµæ´»çš„åºåˆ—é•¿åº¦åŒæ—¶ä¿æŒæœ‰ç«äº‰åŠ›çš„é‡å»ºè´¨é‡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒV2Flowä¼˜äºä¸»æµçš„VQä»¤ç‰Œåˆ†è¯å™¨ï¼Œæ”¯æŒåœ¨ç°æœ‰åŸºç¡€ä¸Šè¿›è¡Œè‡ªå›å½’çš„è§†è§‰ç”Ÿæˆã€‚ç›¸å…³å†…å®¹è¯·æŸ¥é˜…ï¼š<a target="_blank" rel="noopener" href="https://github.com/zhangguiwei610/V2Flow">https://github.com/zhangguiwei610/V2Flow</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07493v1">PDF</a> 11 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>V2Flowæ˜¯ä¸€ç§æ–°å‹è§†è§‰åˆ†è¯å™¨ï¼Œèƒ½å¤Ÿäº§ç”Ÿç¦»æ•£è§†è§‰æ ‡è®°ï¼Œå®ç°é«˜ä¿çœŸé‡å»ºï¼ŒåŒæ—¶ç¡®ä¿ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯æ±‡ç©ºé—´çš„ç»“æ„å’Œæ½œåœ¨åˆ†å¸ƒå¯¹é½ã€‚V2Flowé€šè¿‡å½¢å¼åŒ–è§†è§‰åˆ†è¯ä¸ºæµåŒ¹é…é—®é¢˜ï¼Œå­¦ä¹ ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒåˆ°è¿ç»­å›¾åƒåˆ†å¸ƒä¹‹é—´çš„æ˜ å°„ï¼Œå—LLMè¯æ±‡ç©ºé—´ä¸­çš„æ ‡è®°åºåˆ—æ¡ä»¶å½±å“ã€‚è¯¥æ–¹æ³•çš„ä¼˜åŠ¿æºäºä¸¤ä¸ªæ ¸å¿ƒè®¾è®¡ï¼šè§†è§‰è¯æ±‡é‡é‡‡æ ·å™¨å’Œæ©ç çš„è‡ªåŠ¨å›å½’æ ¡æ­£æµè§£ç å™¨ã€‚å‰è€…å°†è§†è§‰æ•°æ®å‹ç¼©æˆç´§å‡‘çš„æ ‡è®°åºåˆ—ï¼Œæ¯ä¸ªåºåˆ—è¡¨ç¤ºä¸ºLLMè¯æ±‡è¡¨çš„è½¯ç±»åˆ«åˆ†å¸ƒï¼Œä¾¿äºæ— ç¼é›†æˆåˆ°ç°æœ‰çš„LLMä¸­è¿›è¡Œè‡ªåŠ¨å›å½’è§†è§‰ç”Ÿæˆã€‚åè€…é‡‡ç”¨æ©ç çš„è‡ªåŠ¨å›å½’æ ¡æ­£æµé‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿çµæ´»çš„åºåˆ—é•¿åº¦ï¼ŒåŒæ—¶ä¿æŒæœ‰ç«äº‰åŠ›çš„é‡å»ºè´¨é‡ã€‚V2Flowå®éªŒè¡¨æ˜ä¼˜äºä¸»æµVQ-basedçš„åˆ†è¯å™¨å¹¶æ¨åŠ¨äº†è‡ªåŠ¨å›å½’è§†è§‰ç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>V2Flowæ˜¯ä¸€ç§æ–°å‹è§†è§‰åˆ†è¯å™¨ï¼Œä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç´§å¯†ç»“åˆã€‚</li>
<li>é€šè¿‡å½¢å¼åŒ–ä¸ºæµåŒ¹é…é—®é¢˜ï¼ŒV2Flowå­¦ä¹ ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒåˆ°å›¾åƒåˆ†å¸ƒçš„æ˜ å°„ã€‚</li>
<li>è§†è§‰è¯æ±‡é‡é‡‡æ ·å™¨å°†è§†è§‰æ•°æ®è½¬æ¢ä¸ºç´§å‡‘çš„æ ‡è®°åºåˆ—ï¼Œæ˜“äºé›†æˆåˆ°LLMä¸­ã€‚</li>
<li>æ©ç çš„è‡ªåŠ¨å›å½’æ ¡æ­£æµè§£ç å™¨ç”¨äºç»†åŒ–è§†è§‰æ ‡è®°å¹¶ç”Ÿæˆä¸Šä¸‹æ–‡ä¸°å¯Œçš„åµŒå…¥ã€‚</li>
<li>V2Flowé‡‡ç”¨çµæ´»çš„åºåˆ—é•¿åº¦å¹¶ç»´æŒé«˜é‡å»ºè´¨é‡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºV2Flowåœ¨æ€§èƒ½ä¸Šä¼˜äºä¸»æµVQ-basedçš„åˆ†è¯å™¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07493">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5a6bd5cd64be7f84d0eb71b89ba7177.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afe0c632cf9efc32aa8c62ff3539b997.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7ff9be6f2331afbc7b1000520fc107a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a460a9e737134c6acf2942e0464046a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb7f382191a6ca043b3817fa75c196ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ec1b26b2fe1eea78eb5e308ec1b295d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MedAgentsBench-Benchmarking-Thinking-Models-and-Agent-Frameworks-for-Complex-Medical-Reasoning"><a href="#MedAgentsBench-Benchmarking-Thinking-Models-and-Agent-Frameworks-for-Complex-Medical-Reasoning" class="headerlink" title="MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for   Complex Medical Reasoning"></a>MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for   Complex Medical Reasoning</h2><p><strong>Authors:Xiangru Tang, Daniel Shao, Jiwoong Sohn, Jiapeng Chen, Jiayi Zhang, Jinyu Xiang, Fang Wu, Yilun Zhao, Chenglin Wu, Wenqi Shi, Arman Cohan, Mark Gerstein</strong></p>
<p>Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present MedAgentsBench, a benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planning-scenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at <a target="_blank" rel="noopener" href="https://github.com/gersteinlab/medagents-benchmark">https://github.com/gersteinlab/medagents-benchmark</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°æœ‰çš„åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚è¿™ç§é«˜æ€§èƒ½ä½¿å¾—å¯¹å…ˆè¿›æ–¹æ³•è¿›è¡Œæœ‰æ„ä¹‰åœ°è¯„ä¼°å’ŒåŒºåˆ†å˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†MedAgentsBenchåŸºå‡†æµ‹è¯•ï¼Œå®ƒä¸“æ³¨äºå…·æœ‰æŒ‘æˆ˜æ€§çš„åŒ»ç–—é—®é¢˜ï¼Œéœ€è¦å¤šæ­¥éª¤çš„ä¸´åºŠæ¨ç†ã€è¯Šæ–­åˆ¶å®šå’Œæ²»ç–—è®¡åˆ’åˆ¶å®šï¼Œå³ä½¿åœ¨å½“å‰æ¨¡å‹åœ¨æ ‡å‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­ä»ç„¶é¢ä¸´å›°éš¾ã€‚æˆ‘ä»¬ä»ä¸ƒä¸ªå·²å»ºç«‹çš„åŒ»å­¦æ•°æ®é›†ä¸­æ±²å–æ•°æ®ï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•è§£å†³äº†ç°æœ‰è¯„ä¼°ä¸­çš„ä¸‰ä¸ªå…³é”®å±€é™æ€§ï¼šï¼ˆ1ï¼‰ç®€å•é—®é¢˜æ™®éå­˜åœ¨çš„ç°è±¡ï¼Œå³ä½¿åŸºç¡€æ¨¡å‹ä¹Ÿèƒ½å–å¾—è¾ƒé«˜çš„æ€§èƒ½ï¼›ï¼ˆ2ï¼‰ä¸åŒç ”ç©¶ä¹‹é—´é‡‡æ ·å’Œè¯„ä¼°åè®®çš„ä¸ä¸€è‡´æ€§ï¼›ï¼ˆ3ï¼‰ç¼ºä¹å¯¹æ€§èƒ½ã€æˆæœ¬å’Œæ¨ç†æ—¶é—´ä¹‹é—´ç›¸äº’ä½œç”¨çš„ç³»ç»Ÿæ€§åˆ†æã€‚é€šè¿‡å¯¹å„ç§åŸºç¡€æ¨¡å‹å’Œæ¨ç†æ–¹æ³•çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æœ€æ–°çš„æ€è€ƒæ¨¡å‹DeepSeek R1å’ŒOpenAI o3åœ¨å¤æ‚çš„åŒ»ç–—æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ä¼ ç»Ÿçš„æœç´¢æ–¹æ³•ç›¸æ¯”ï¼Œå…ˆè¿›çš„åŸºäºæœç´¢çš„ä»£ç†æ–¹æ³•æä¾›äº†æœ‰å‰æ™¯çš„æ€§èƒ½æˆæœ¬æ¯”ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸åŒæ¨¡å‹å®¶æ—åœ¨å¤æ‚é—®é¢˜ä¸Šçš„æ€§èƒ½å·®è·ï¼Œå¹¶é’ˆå¯¹ä¸åŒçš„è®¡ç®—çº¦æŸæ¡ä»¶é€‰æ‹©äº†æœ€ä½³æ¨¡å‹ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gersteinlab/medagents-benchmark%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/gersteinlab/medagents-benchmarkå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07459v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç°æœ‰çš„åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MedAgentsBenchåŸºå‡†æµ‹è¯•ï¼Œå®ƒä¸“æ³¨äºæŒ‘æˆ˜éœ€è¦å¤šæ­¥éª¤ä¸´åºŠæ¨ç†ã€è¯Šæ–­åˆ¶å®šå’Œæ²»ç–—è®¡åˆ’è§„åˆ’çš„åŒ»ç–—é—®é¢˜ï¼Œè¿™æ˜¯å½“å‰æ¨¡å‹ä»ç„¶é¢ä¸´å›°éš¾çš„åœºæ™¯ã€‚è¯¥åŸºå‡†æµ‹è¯•è§£å†³äº†ç°æœ‰è¯„ä¼°ä¸­çš„ä¸‰ä¸ªå…³é”®é™åˆ¶ã€‚é€šè¿‡è¯•éªŒå„ç§åŸºç¡€æ¨¡å‹å’Œæ¨ç†æ–¹æ³•ï¼Œæˆ‘ä»¬è¯æ˜äº†æœ€æ–°çš„æ€è€ƒæ¨¡å‹åœ¨å¤æ‚çš„åŒ»ç–—æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸åŒæ¨¡å‹å®¶æ—åœ¨å¤æ‚é—®é¢˜ä¸Šçš„æ€§èƒ½å·®è·ï¼Œå¹¶ä¸ºä¸åŒçš„è®¡ç®—çº¦æŸç¡®å®šäº†æœ€ä½³æ¨¡å‹é€‰æ‹©ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/gersteinlab/medagents-benchmark">https://github.com/gersteinlab/medagents-benchmark</a> å…¬å¼€è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMåœ¨åŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä»é¢ä¸´å¤æ‚åŒ»ç–—æ¨ç†ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚</li>
<li>MedAgentsBenchåŸºå‡†æµ‹è¯•è§£å†³äº†ç°æœ‰è¯„ä¼°çš„å…³é”®é™åˆ¶ï¼ŒåŒ…æ‹¬ç®€å•é—®é¢˜è¿‡å¤šã€é‡‡æ ·å’Œè¯„ä»·åè®®ä¸ä¸€è‡´ä»¥åŠç¼ºä¹æ€§èƒ½ã€æˆæœ¬å’Œæ¨ç†æ—¶é—´ä¹‹é—´çš„ç³»ç»Ÿåˆ†æã€‚</li>
<li>æœ€æ–°çš„æ€è€ƒæ¨¡å‹ï¼Œå¦‚DeepSeek R1å’ŒOpenAI o3ï¼Œåœ¨å¤æ‚çš„åŒ»ç–—æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>å…ˆè¿›çš„æœç´¢å‹ä»£ç†æ–¹æ³•ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œæ€§èƒ½ä¸æˆæœ¬æ¯”å…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>ä¸åŒæ¨¡å‹å®¶æ—åœ¨å¤æ‚é—®é¢˜ä¸Šçš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œéœ€è¦æ ¹æ®è®¡ç®—çº¦æŸé€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€‚</li>
<li>MedAgentsBenchåŸºå‡†æµ‹è¯•å’Œè¯„ä¼°æ¡†æ¶å·²å…¬å¼€å‘å¸ƒï¼Œå¯ä¾›ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2372f71d7c08d0c0447848bda43d1852.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77a8eeb27761ffb85c39029465caf05e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-887da8e131c5d782c91256d66dcf6959.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c22a174577809b271182779a14fd292c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eee2f745610bd4ddbbac7581210e7c03.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ce04950da29119dbc5e6ca26832740d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RePO-ReLU-based-Preference-Optimization"><a href="#RePO-ReLU-based-Preference-Optimization" class="headerlink" title="RePO: ReLU-based Preference Optimization"></a>RePO: ReLU-based Preference Optimization</h2><p><strong>Authors:Junkang Wu, Kexin Huang, Xue Wang, Jinyang Gao, Bolin Ding, Jiancan Wu, Xiangnan He, Xiang Wang</strong></p>
<p>Aligning large language models (LLMs) with human preferences is critical for real-world deployment, yet existing methods like RLHF face computational and stability challenges. While DPO establishes an offline paradigm with single hyperparameter $\beta$, subsequent methods like SimPO reintroduce complexity through dual parameters ($\beta$, $\gamma$). We propose {ReLU-based Preference Optimization (RePO)}, a streamlined algorithm that eliminates $\beta$ via two advances: (1) retaining SimPOâ€™s reference-free margins but removing $\beta$ through gradient analysis, and (2) adopting a ReLU-based max-margin loss that naturally filters trivial pairs. Theoretically, RePO is characterized as SimPOâ€™s limiting case ($\beta \to \infty$), where the logistic weighting collapses to binary thresholding, forming a convex envelope of the 0-1 loss. Empirical results on AlpacaEval 2 and Arena-Hard show that RePO outperforms DPO and SimPO across multiple base models, requiring only one hyperparameter to tune. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½å¯¹äºç°å®ä¸–ç•Œéƒ¨ç½²è‡³å…³é‡è¦ï¼Œç„¶è€Œç°æœ‰çš„æ–¹æ³•å¦‚RLHFé¢ä¸´ç€è®¡ç®—å’Œç¨³å®šæ€§æŒ‘æˆ˜ã€‚è™½ç„¶DPOé€šè¿‡å»ºç«‹å•è¶…å‚æ•°Î²çš„ç¦»çº¿èŒƒå¼å–å¾—äº†æˆåŠŸï¼Œä½†åç»­æ–¹æ³•å¦‚SimPOé€šè¿‡åŒå‚æ•°ï¼ˆÎ²ï¼ŒÎ³ï¼‰é‡æ–°å¼•å…¥äº†å¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºReLUçš„åå¥½ä¼˜åŒ–ï¼ˆRePOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ç®—æ³•ï¼Œé€šè¿‡ä¸¤ä¸ªè¿›å±•æ¶ˆé™¤äº†Î²ï¼šï¼ˆ1ï¼‰ä¿ç•™SimPOçš„æ— å‚è€ƒè¾¹è·å¹¶é€šè¿‡æ¢¯åº¦åˆ†ææ¶ˆé™¤Î²ï¼Œï¼ˆ2ï¼‰é‡‡ç”¨åŸºäºReLUçš„æœ€å¤§è¾¹è·æŸå¤±ï¼Œè‡ªç„¶åœ°è¿‡æ»¤æ‰æ— å…³çš„å¯¹ã€‚ä»ç†è®ºä¸Šè®²ï¼ŒRePOè¢«æè¿°ä¸ºSimPOçš„æé™æƒ…å†µï¼ˆÎ²â†’âˆï¼‰ï¼Œåœ¨æ­¤æƒ…å†µä¸‹ï¼Œé€»è¾‘æƒé‡ä¼šé™è‡³é˜ˆå€¼å¤„ç†ï¼Œå½¢æˆ0-1æŸå¤±çš„å‡¸åŒ…ã€‚åœ¨AlpacaEval 2å’ŒArena-Hardä¸Šçš„ç»éªŒç»“æœè¡¨æ˜ï¼ŒRePOåœ¨å¤šä¸ªåŸºç¡€æ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºDPOå’ŒSimPOï¼Œåªéœ€è¦è°ƒæ•´ä¸€ä¸ªè¶…å‚æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07426v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½çš„é‡è¦æ€§ï¼Œé’ˆå¯¹ç°æœ‰æ–¹æ³•å¦‚RLHFé¢ä¸´çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åŸºäºReLUçš„åå¥½ä¼˜åŒ–ï¼ˆRePOï¼‰ç®—æ³•ã€‚è¯¥ç®—æ³•é€šè¿‡æ¢¯åº¦åˆ†æå’ŒReLUåŸºæœ€å¤§è¾¹ç•ŒæŸå¤±æ¥ç®€åŒ–å‚æ•°è®¾ç½®ï¼Œæ¶ˆé™¤äº†Î²å‚æ•°ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡å’Œç¨³å®šæ€§ã€‚åœ¨AlpacaEval 2å’ŒArena-Hardä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRePOåœ¨å¤šä¸ªåŸºç¡€æ¨¡å‹ä¸Šä¼˜äºDPOå’ŒSimPOã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½å¯¹äºå®é™…éƒ¨ç½²è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¦‚RLHFé¢ä¸´è®¡ç®—å’Œç¨³å®šæ€§æŒ‘æˆ˜ã€‚</li>
<li>RePOç®—æ³•é€šè¿‡æ¢¯åº¦åˆ†æå’ŒReLUåŸºæœ€å¤§è¾¹ç•ŒæŸå¤±ç®€åŒ–äº†å‚æ•°è®¾ç½®ï¼Œæ¶ˆé™¤äº†Î²å‚æ•°ã€‚</li>
<li>RePOç®—æ³•æ˜¯SimPOçš„ä¸€ç§ç®€åŒ–ç‰ˆæœ¬ï¼Œè¡¨ç°ä¸ºSimPOçš„æé™æƒ…å†µï¼ˆÎ²â†’âˆï¼‰ã€‚</li>
<li>RePOç®—æ³•æé«˜äº†è®¡ç®—æ•ˆç‡å’Œç¨³å®šæ€§ã€‚</li>
<li>åœ¨AlpacaEval 2å’ŒArena-Hardä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRePOåœ¨å¤šä¸ªåŸºç¡€æ¨¡å‹ä¸Šçš„æ€§èƒ½ä¼˜äºDPOå’ŒSimPOã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07426">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6f85e216e8070bc067557f68574f9a20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a2ee8c44989505458f333929a4a3378.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86e7db4fdca0355aa0fe1b88693f1faf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c9cfeb1d89debcad2752006cbbab4c3.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="REF-VLM-Triplet-Based-Referring-Paradigm-for-Unified-Visual-Decoding"><a href="#REF-VLM-Triplet-Based-Referring-Paradigm-for-Unified-Visual-Decoding" class="headerlink" title="REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding"></a>REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding</h2><p><strong>Authors:Yan Tai, Luhao Zhu, Zhiqiang Chen, Ynan Ding, Yiying Dong, Xiaohong Liu, Guodong Guo</strong></p>
<p>Multimodal Large Language Models (MLLMs) demonstrate robust zero-shot capabilities across diverse vision-language tasks after training on mega-scale datasets. However, dense prediction tasks, such as semantic segmentation and keypoint detection, pose significant challenges for MLLMs when represented solely as text outputs. Simultaneously, current MLLMs utilizing latent embeddings for visual task decoding generally demonstrate limited adaptability to both multi-task learning and multi-granularity scenarios. In this work, we present REF-VLM, an end-to-end framework for unified training of various visual decoding tasks. To address complex visual decoding scenarios, we introduce the Triplet-Based Referring Paradigm (TRP), which explicitly decouples three critical dimensions in visual decoding tasks through a triplet structure: concepts, decoding types, and targets. TRP employs symbolic delimiters to enforce structured representation learning, enhancing the parsability and interpretability of model outputs. Additionally, we construct Visual-Task Instruction Following Dataset (VTInstruct), a large-scale multi-task dataset containing over 100 million multimodal dialogue samples across 25 task types. Beyond text inputs and outputs, VT-Instruct incorporates various visual prompts such as point, box, scribble, and mask, and generates outputs composed of text and visual units like box, keypoint, depth and mask. The combination of different visual prompts and visual units generates a wide variety of task types, expanding the applicability of REF-VLM significantly. Both qualitative and quantitative experiments demonstrate that our REF-VLM outperforms other MLLMs across a variety of standard benchmarks. The code, dataset, and demo available at <a target="_blank" rel="noopener" href="https://github.com/MacavityT/REF-VLM">https://github.com/MacavityT/REF-VLM</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒåï¼Œåœ¨å¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºä»…è¡¨ç°ä¸ºæ–‡æœ¬è¾“å‡ºçš„å¯†é›†é¢„æµ‹ä»»åŠ¡ï¼ˆå¦‚è¯­ä¹‰åˆ†å‰²å’Œå…³é”®ç‚¹æ£€æµ‹ï¼‰ï¼ŒMLLMsé¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œå½“å‰ä½¿ç”¨æ½œåœ¨åµŒå…¥è¿›è¡Œè§†è§‰ä»»åŠ¡è§£ç çš„MLLMé€šå¸¸å¯¹å¤šä»»åŠ¡å­¦ä¹ å’Œå¤šç²’åº¦åœºæ™¯çš„é€‚åº”æ€§æœ‰é™ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†REF-VLMï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€è®­ç»ƒå„ç§è§†è§‰è§£ç ä»»åŠ¡çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚ä¸ºäº†è§£å†³å¤æ‚çš„è§†è§‰è§£ç åœºæ™¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºä¸‰å…ƒç»„çš„å‚ç…§èŒƒå¼ï¼ˆTRPï¼‰ï¼Œå®ƒé€šè¿‡ä¸‰å…ƒç»„ç»“æ„æ˜¾å¼åœ°è§£è€¦è§†è§‰è§£ç ä»»åŠ¡ä¸­çš„ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šæ¦‚å¿µã€è§£ç ç±»å‹å’Œç›®æ ‡ã€‚TRPé‡‡ç”¨ç¬¦å·åˆ†éš”ç¬¦æ¥å¼ºåˆ¶æ‰§è¡Œç»“æ„åŒ–è¡¨ç¤ºå­¦ä¹ ï¼Œæé«˜æ¨¡å‹è¾“å‡ºçš„å¯è§£ææ€§å’Œå¯è§£é‡Šæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†è§†è§‰ä»»åŠ¡æŒ‡ä»¤è·Ÿéšæ•°æ®é›†ï¼ˆVTInstructï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šä»»åŠ¡æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡10äº¿ä¸ªè·¨25ç§ä»»åŠ¡ç±»å‹çš„å¤šæ¨¡æ€å¯¹è¯æ ·æœ¬ã€‚é™¤äº†æ–‡æœ¬è¾“å…¥å’Œè¾“å‡ºå¤–ï¼ŒVT-Instructè¿˜ç»“åˆäº†å„ç§è§†è§‰æç¤ºï¼Œå¦‚ç‚¹ã€æ¡†ã€æ¶‚é¸¦å’Œé®ç½©ï¼Œå¹¶ç”Ÿæˆç”±æ–‡æœ¬å’Œè§†è§‰å•ä½ï¼ˆå¦‚æ¡†ã€å…³é”®ç‚¹ã€æ·±åº¦å’Œé®ç½©ï¼‰ç»„æˆçš„è¾“å‡ºã€‚ä¸åŒçš„è§†è§‰æç¤ºå’Œè§†è§‰å•ä½çš„ç»„åˆäº§ç”Ÿäº†å„ç§ä»»åŠ¡ç±»å‹ï¼Œæå¤§åœ°æ‰©å±•äº†REF-VLMçš„é€‚ç”¨æ€§ã€‚å®šæ€§å’Œå®šé‡å®éªŒå‡è¡¨æ˜ï¼Œæˆ‘ä»¬çš„REF-VLMåœ¨å„ç§æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå…¶ä»–MLLMsã€‚ç›¸å…³ä»£ç ã€æ•°æ®é›†å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MacavityT/REF-VLM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MacavityT/REF-VLMä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07413v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Multimodal Large Language Modelsï¼ˆMLLMsï¼‰åœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡ä¸Šçš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†REF-VLMæ¡†æ¶æ¥ç»Ÿä¸€è®­ç»ƒå„ç§è§†è§‰è§£ç ä»»åŠ¡ã€‚ä¸ºè§£å†³å¤æ‚è§†è§‰è§£ç åœºæ™¯ï¼Œå¼•å…¥äº†Triplet-BasedReferring Paradigmï¼ˆTRPï¼‰ï¼Œé€šè¿‡ä¸‰å…ƒç»“æ„è§£è€¦è§†è§‰è§£ç ä»»åŠ¡ä¸­çš„æ¦‚å¿µã€è§£ç ç±»å‹å’Œç›®æ ‡ã€‚åŒæ—¶æ„å»ºäº†Visual-Task Instruction Following Datasetï¼ˆVTInstructï¼‰ï¼ŒåŒ…å«è¶…è¿‡100ç™¾ä¸‡çš„å¤šæ¨¡æ€å¯¹è¯æ ·æœ¬ã€‚REF-VLMç»“åˆä¸åŒè§†è§‰æç¤ºå’Œè§†è§‰å•ä½ï¼Œæ˜¾è‘—æé«˜äº†åœ¨å¤šç§æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨å¯†é›†é¢„æµ‹ä»»åŠ¡å¦‚è¯­ä¹‰åˆ†å‰²å’Œå…³é”®ç‚¹æ£€æµ‹ä¸Šè¡¨ç°æŒ‘æˆ˜ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„è§†è§‰è§£ç æ–¹æ³•ã€‚</li>
<li>REF-VLMæ¡†æ¶æå‡ºç”¨äºç»Ÿä¸€è®­ç»ƒå„ç§è§†è§‰è§£ç ä»»åŠ¡ï¼Œæé«˜æ¨¡å‹é€‚åº”æ€§å’Œæ€§èƒ½ã€‚</li>
<li>TRPé€šè¿‡ä¸‰å…ƒç»“æ„è§£è€¦è§†è§‰è§£ç ä»»åŠ¡çš„å…³é”®ç»´åº¦ï¼Œæé«˜æ¨¡å‹è¾“å‡ºçš„å¯è§£ææ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>VTInstructæ•°æ®é›†åŒ…å«å¤§é‡å¤šæ¨¡æ€å¯¹è¯æ ·æœ¬ï¼Œç”¨äºè®­ç»ƒå’Œè¯„ä¼°è§†è§‰ä»»åŠ¡æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚</li>
<li>REF-VLMç»“åˆä¸åŒè§†è§‰æç¤ºå’Œè§†è§‰å•ä½ï¼Œå¦‚ç‚¹ã€æ¡†ã€æ¶‚é¸¦å’Œé®ç½©ï¼Œæ‰©å±•äº†åº”ç”¨èŒƒå›´ã€‚</li>
<li>REF-VLMåœ¨å¤šç§æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œè¯æ˜å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07413">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-dbea7c1a706253217a43d071800310ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-167ba77050eed169fe488cbee474c542.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90090431f5e0a743caab17de1f6884eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-afefbccddd8f128b78b4e8e4c9a60fdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73ee5d21a18d2e6401582c7f16922e6c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Process-Supervised-LLM-Recommenders-via-Flow-guided-Tuning"><a href="#Process-Supervised-LLM-Recommenders-via-Flow-guided-Tuning" class="headerlink" title="Process-Supervised LLM Recommenders via Flow-guided Tuning"></a>Process-Supervised LLM Recommenders via Flow-guided Tuning</h2><p><strong>Authors:Chongming Gao, Mengyao Gao, Chenxiao Fan, Shuai Yuan, Wentao Shi, Xiangnan He</strong></p>
<p>While large language models (LLMs) are increasingly adapted for recommendation systems via supervised fine-tuning (SFT), this approach amplifies popularity bias due to its likelihood maximization objective, compromising recommendation diversity and fairness. To address this, we present Flow-guided fine-tuning recommender (Flower), which replaces SFT with a Generative Flow Network (GFlowNet) framework that enacts process supervision through token-level reward propagation. Flowerâ€™s key innovation lies in decomposing item-level rewards into constituent token rewards, enabling direct alignment between token generation probabilities and their reward signals. This mechanism achieves three critical advancements: (1) popularity bias mitigation and fairness enhancement through empirical distribution matching, (2) preservation of diversity through GFlowNetâ€™s proportional sampling, and (3) flexible integration of personalized preferences via adaptable token rewards. Experiments demonstrate Flowerâ€™s superior distribution-fitting capability and its significant advantages over traditional SFT in terms of fairness, diversity, and accuracy, highlighting its potential to improve LLM-based recommendation systems. The implementation is available via <a target="_blank" rel="noopener" href="https://github.com/Mr-Peach0301/Flower">https://github.com/Mr-Peach0301/Flower</a> </p>
<blockquote>
<p>è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é€‚åº”æ¨èç³»ç»Ÿï¼Œä½†ç”±äºå…¶æœ€å¤§åŒ–æ¦‚ç‡çš„ç›®æ ‡ï¼Œè¿™ç§æ–¹æ³•ä¼šåŠ å‰§æµè¡Œåº¦åè§ï¼ŒæŸå®³æ¨èå¤šæ ·æ€§å’Œå…¬å¹³æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æµå¼•å¯¼å¾®è°ƒæ¨èå™¨ï¼ˆFlowerï¼‰ï¼Œå®ƒç”¨ç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetï¼‰æ¡†æ¶æ›¿ä»£äº†SFTï¼Œé€šè¿‡ä»¤ç‰Œçº§åˆ«çš„å¥–åŠ±ä¼ æ’­å®ç°è¿‡ç¨‹ç›‘ç£ã€‚Flowerçš„å…³é”®åˆ›æ–°åœ¨äºå°†é¡¹ç›®çº§åˆ«çš„å¥–åŠ±åˆ†è§£ä¸ºç»„æˆéƒ¨åˆ†çš„ä»¤ç‰Œå¥–åŠ±ï¼Œä½¿å¾—ä»¤ç‰Œç”Ÿæˆæ¦‚ç‡ä¸å…¶å¥–åŠ±ä¿¡å·ä¹‹é—´èƒ½å¤Ÿç›´æ¥å¯¹é½ã€‚è¿™ç§æœºåˆ¶å®ç°äº†ä¸‰ä¸ªå…³é”®è¿›å±•ï¼šï¼ˆ1ï¼‰é€šè¿‡ç»éªŒåˆ†å¸ƒåŒ¹é…ç¼“è§£æµè¡Œåº¦åè§ï¼Œå¢å¼ºå…¬å¹³æ€§ï¼›ï¼ˆ2ï¼‰é€šè¿‡GFlowNetçš„æ¯”ä¾‹é‡‡æ ·ä¿æŒå¤šæ ·æ€§ï¼›ï¼ˆ3ï¼‰é€šè¿‡å¯é€‚åº”çš„ä»¤ç‰Œå¥–åŠ±çµæ´»æ•´åˆä¸ªæ€§åŒ–åå¥½ã€‚å®éªŒè¡¨æ˜ï¼ŒFloweråœ¨åˆ†å¸ƒæ‹Ÿåˆèƒ½åŠ›ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œåœ¨å…¬å¹³æ€§ã€å¤šæ ·æ€§å’Œå‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿçš„SFTï¼Œçªæ˜¾å…¶åœ¨æ”¹è¿›åŸºäºLLMçš„æ¨èç³»ç»Ÿæ–¹é¢çš„æ½œåŠ›ã€‚å…·ä½“å®ç°å¯é€šè¿‡ <a target="_blank" rel="noopener" href="https://github.com/Mr-Peach0301/Flower">https://github.com/Mr-Peach0301/Flower</a> è®¿é—®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07377v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨é€šå¸¸é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ–¹æ³•ï¼Œä½†è¿™ç§æ–¹æ³•ä¼šæ”¾å¤§æµè¡Œåº¦åè§ï¼Œå½±å“æ¨èçš„å¤šæ ·æ€§å’Œå…¬å¹³æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetï¼‰çš„Flowå¼•å¯¼å¾®è°ƒæ¨èå™¨ï¼ˆFlowerï¼‰ã€‚å®ƒé€šè¿‡ä»¤ç‰Œçº§åˆ«çš„å¥–åŠ±ä¼ æ’­æ¥å®ç°è¿‡ç¨‹ç›‘ç£ï¼Œå–ä»£äº†SFTã€‚Flowerçš„å…³é”®åˆ›æ–°ä¹‹å¤„åœ¨äºå°†é¡¹ç›®çº§åˆ«çš„å¥–åŠ±åˆ†è§£ä¸ºç»„æˆéƒ¨åˆ†çš„ä»¤ç‰Œå¥–åŠ±ï¼Œä½¿ä»¤ç‰Œç”Ÿæˆæ¦‚ç‡ä¸å…¶å¥–åŠ±ä¿¡å·ç›´æ¥å¯¹é½ã€‚æ­¤æœºåˆ¶å®ç°äº†ä¸‰ä¸ªå…³é”®è¿›å±•ï¼šä¸€æ˜¯é€šè¿‡ç»éªŒåˆ†å¸ƒåŒ¹é…å‡è½»æµè¡Œåº¦åè§ï¼Œå¢å¼ºå…¬å¹³æ€§ï¼›äºŒæ˜¯é€šè¿‡GFlowNetçš„æ¯”ä¾‹é‡‡æ ·ä¿ç•™å¤šæ ·æ€§ï¼›ä¸‰æ˜¯é€šè¿‡çµæ´»çš„ä¸ªæ€§åŒ–åå¥½é›†æˆæå‡æ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚ç›¸å…³å®éªŒè¡¨æ˜ï¼ŒFloweråœ¨åˆ†å¸ƒæ‹Ÿåˆèƒ½åŠ›ã€å…¬å¹³æ€§å’Œå¤šæ ·æ€§æ–¹é¢å‡ä¼˜äºä¼ ç»Ÿçš„SFTæ–¹æ³•ã€‚å…·ä½“å®ç°å¯é€šè¿‡ç›¸å…³é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”¨äºæ¨èç³»ç»Ÿå¸¸é‡‡ç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä½†ä¼šå¯¼è‡´æµè¡Œåº¦åè§ã€‚</li>
<li>Floweré‡‡ç”¨ç”Ÿæˆæµç½‘ç»œï¼ˆGFlowNetï¼‰æ¡†æ¶æ›¿ä»£SFTï¼Œå®ç°è¿‡ç¨‹ç›‘ç£åŠä»¤ç‰Œçº§åˆ«çš„å¥–åŠ±ä¼ æ’­ã€‚</li>
<li>Floweré€šè¿‡å°†é¡¹ç›®çº§å¥–åŠ±åˆ†è§£ä¸ºä»¤ç‰Œå¥–åŠ±ï¼Œå®ç°äº†å…¬å¹³æ€§ã€å¤šæ ·æ€§å’Œå‡†ç¡®æ€§çš„æå‡ã€‚</li>
<li>é€šè¿‡ç»éªŒåˆ†å¸ƒåŒ¹é…å‡è½»æµè¡Œåº¦åè§ã€‚</li>
<li>GFlowNetçš„æ¯”ä¾‹é‡‡æ ·æœ‰åŠ©äºä¿ç•™å¤šæ ·æ€§ã€‚</li>
<li>Flowerèƒ½å¤Ÿçµæ´»é›†æˆä¸ªæ€§åŒ–åå¥½ï¼Œæå‡æ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07377">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ea9632e95ae21dc8b7c68df014f74401.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0234cf229aaa50807b5c51da0d477fc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ced0c9b429d9f09ddf49b38d618b38e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90a8939ace77651567e4c3a93c2694ed.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-252212925a2f610704b649be36f7e7aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-024f079f932eb91bd89f01e14ec99628.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MM-Eureka-Exploring-Visual-Aha-Moment-with-Rule-based-Large-scale-Reinforcement-Learning"><a href="#MM-Eureka-Exploring-Visual-Aha-Moment-with-Rule-based-Large-scale-Reinforcement-Learning" class="headerlink" title="MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale   Reinforcement Learning"></a>MM-Eureka: Exploring Visual Aha Moment with Rule-based Large-scale   Reinforcement Learning</h2><p><strong>Authors:Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, Wenqi Shao</strong></p>
<p>We present MM-Eureka, a multimodal reasoning model that successfully extends large-scale rule-based reinforcement learning (RL) to multimodal reasoning. While rule-based RL has shown remarkable success in improving LLMsâ€™ reasoning abilities in text domains, its application to multimodal settings has remained challenging. Our work reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, including steady increases in accuracy reward and response length, and the emergence of reflection behaviors. We demonstrate that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at <a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-EUREKA">https://github.com/ModalMinds/MM-EUREKA</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºMM-Eurekaï¼Œè¿™æ˜¯ä¸€æ¬¾æˆåŠŸå°†å¤§è§„æ¨¡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ (RL)æ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ã€‚è™½ç„¶åŸºäºè§„åˆ™çš„RLåœ¨æ–‡æœ¬é¢†åŸŸçš„æé«˜å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å…¶åœ¨å¤šæ¨¡æ€åœºæ™¯çš„åº”ç”¨ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œåœ¨å¤šæ¨¡æ€ç©ºé—´ä¸­å†ç°äº†åŸºäºæ–‡æœ¬çš„RLç³»ç»Ÿï¼ˆå¦‚DeepSeek-R1ï¼‰çš„å…³é”®ç‰¹å¾ï¼ŒåŒ…æ‹¬ç²¾åº¦å¥–åŠ±å’Œå“åº”é•¿åº¦çš„ç¨³å®šå¢åŠ ï¼Œä»¥åŠåæ€è¡Œä¸ºçš„å‡ºç°ã€‚æˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡åŸºäºè§„åˆ™çš„RLï¼Œç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥å‘å±•å‡ºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œæ— éœ€ç›‘ç£å¾®è°ƒï¼Œåœ¨æ•°æ®æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºä¼˜äºå…¶ä»–æ–¹æ³•ã€‚æˆ‘ä»¬å…¬å¼€äº†å®Œæ•´çš„ç®¡é“ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-EUREKA">https://github.com/ModalMinds/MM-EUREKA</a>ä¸Šå‘å¸ƒäº†æˆ‘ä»¬æ‰€æœ‰çš„ä»£ç ã€æ¨¡å‹ã€æ•°æ®ç­‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07365v1">PDF</a> </p>
<p><strong>Summary</strong><br>MM-Eurekaæ˜¯ä¸€ç§å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼ŒæˆåŠŸå°†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†é¢†åŸŸã€‚è¯¥æ¨¡å‹åœ¨æ–‡æœ¬é¢†åŸŸå±•ç¤ºå‡ºè‰²çš„æ¨ç†èƒ½åŠ›ï¼Œç°åœ¨å¯åº”ç”¨äºå¤šæ¨¡æ€ç¯å¢ƒã€‚ç ”ç©¶å±•ç¤ºäº†è¯¥æ¨¡å‹åœ¨ç²¾åº¦å¥–åŠ±å’Œå“åº”é•¿åº¦æ–¹é¢çš„ç¨³æ­¥æé«˜ï¼Œä»¥åŠåæ€è¡Œä¸ºçš„å‡ºç°ã€‚é€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼Œè¯¥æ¨¡å‹æ— éœ€ç›‘ç£å¾®è°ƒå³å¯å‘å±•å‡ºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•å…·æœ‰æ›´é«˜çš„æ•°æ®æ•ˆç‡ã€‚æˆ‘ä»¬å…¬å¼€äº†å®Œæ•´çš„ç®¡é“ä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚æ›´å¤šè¯¦æƒ…å¯é€šè¿‡è®¿é—®é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/ModalMinds/MM-EUREKA">https://github.com/ModalMinds/MM-EUREKA</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MM-EurekaæˆåŠŸå°†åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°å¤šæ¨¡æ€æ¨ç†é¢†åŸŸã€‚</li>
<li>MM-Eurekaåœ¨å¤šæ¨¡æ€ç¯å¢ƒä¸­å±•ç°äº†ä¸æ–‡æœ¬ä¸ºåŸºç¡€çš„RLç³»ç»Ÿç›¸ä¼¼çš„ç‰¹æ€§ï¼Œå¦‚DeepSeek-R1çš„ç²¾åº¦å¥–åŠ±å’Œå“åº”é•¿åº¦çš„ç¨³æ­¥æé«˜ã€‚</li>
<li>MM-Eurekaå±•ç°å‡ºåæ€è¡Œä¸ºçš„å‡ºç°ï¼Œè¿™æ˜¯å¤šæ¨¡æ€æ¨ç†çš„ä¸€ä¸ªé‡è¦ç‰¹å¾ã€‚</li>
<li>é€šè¿‡åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ŒMM-Eurekaåœ¨æ— éœ€ç›‘ç£å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿå‘å±•å‡ºå¼ºå¤§çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MM-Eurekaç›¸è¾ƒäºå…¶ä»–æ–¹æ³•å…·æœ‰æ›´é«˜çš„æ•°æ®æ•ˆç‡ã€‚</li>
<li>MM-Eurekaçš„å®Œæ•´ç®¡é“å·²ç»å¼€æºï¼Œæ—¨åœ¨ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-30bfc994f3f0e83fb077a6fce0d19121.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-549019e5dd693c6b13757944df229aa0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12dd6c3bc1777f64662cee5033f14e5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7e3ce5614970132a79facbee0c409f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-161eda93751e8d324b4f96f1d402258c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Graph-based-Verification-Framework-for-Fact-Checking"><a href="#A-Graph-based-Verification-Framework-for-Fact-Checking" class="headerlink" title="A Graph-based Verification Framework for Fact-Checking"></a>A Graph-based Verification Framework for Fact-Checking</h2><p><strong>Authors:Yani Huang, Richong Zhang, Zhijie Nie, Junfan Chen, Xuefeng Zhang</strong></p>
<p>Fact-checking plays a crucial role in combating misinformation. Existing methods using large language models (LLMs) for claim decomposition face two key limitations: (1) insufficient decomposition, introducing unnecessary complexity to the verification process, and (2) ambiguity of mentions, leading to incorrect verification results. To address these challenges, we suggest introducing a claim graph consisting of triplets to address the insufficient decomposition problem and reduce mention ambiguity through graph structure. Based on this core idea, we propose a graph-based framework, GraphFC, for fact-checking. The framework features three key components: graph construction, which builds both claim and evidence graphs; graph-guided planning, which prioritizes the triplet verification order; and graph-guided checking, which verifies the triples one by one between claim and evidence graphs. Extensive experiments show that GraphFC enables fine-grained decomposition while resolving referential ambiguities through relational constraints, achieving state-of-the-art performance across three datasets. </p>
<blockquote>
<p>äº‹å®æ ¸æŸ¥åœ¨æŠ—å‡»é”™è¯¯ä¿¡æ¯ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç°æœ‰ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå£°æ˜åˆ†è§£çš„æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™ï¼šï¼ˆ1ï¼‰åˆ†è§£ä¸è¶³ï¼Œç»™éªŒè¯è¿‡ç¨‹å¸¦æ¥ä¸å¿…è¦çš„å¤æ‚æ€§ï¼›ï¼ˆ2ï¼‰æåŠçš„æ¨¡ç³Šæ€§ï¼Œå¯¼è‡´éªŒè¯ç»“æœé”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ç”±ä¸‰å…ƒç»„ç»„æˆçš„å£°æ˜å›¾æ¥è§£å†³åˆ†è§£ä¸è¶³çš„é—®é¢˜ï¼Œå¹¶é€šè¿‡å›¾ç»“æ„å‡å°‘æåŠçš„æ¨¡ç³Šæ€§ã€‚åŸºäºè¿™ä¸€æ ¸å¿ƒæ€æƒ³ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºäº‹å®æ ¸æŸ¥çš„å›¾åŸºæ¡†æ¶GraphFCã€‚è¯¥æ¡†æ¶å…·æœ‰ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå›¾æ„å»ºï¼Œæ„å»ºå£°æ˜å’Œè¯æ®å›¾ï¼›å›¾å¼•å¯¼è§„åˆ’ï¼Œä¼˜å…ˆè¿›è¡Œä¸‰å…ƒç»„éªŒè¯é¡ºåºï¼›ä»¥åŠå›¾å¼•å¯¼æ£€æŸ¥ï¼Œé€ä¸€éªŒè¯å£°æ˜å’Œè¯æ®å›¾ä¹‹é—´çš„ä¸‰å…ƒç»„ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGraphFCèƒ½å¤Ÿå®ç°ç²¾ç»†çš„åˆ†è§£ï¼ŒåŒæ—¶é€šè¿‡å…³ç³»çº¦æŸè§£å†³æŒ‡ä»£æ­§ä¹‰ï¼Œåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07282v1">PDF</a> 13pages, 4figures</p>
<p><strong>æ‘˜è¦</strong></p>
<p>äº‹å®æ ¸æŸ¥åœ¨æ‰“å‡»è™šå‡ä¿¡æ¯ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ã€‚ç°æœ‰çš„ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå£°æ˜åˆ†è§£çš„æ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦å±€é™ï¼šä¸€æ˜¯åˆ†è§£ä¸è¶³ï¼Œç»™éªŒè¯è¿‡ç¨‹å¢åŠ äº†ä¸å¿…è¦çš„å¤æ‚æ€§ï¼›äºŒæ˜¯æåŠçš„æ¨¡ç³Šæ€§ï¼Œå¯¼è‡´éªŒè¯ç»“æœä¸æ­£ç¡®ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ ¸å¿ƒæ€æƒ³çš„å›¾åŸºæ¡†æ¶GraphFCã€‚å®ƒé€šè¿‡å»ºç«‹å£°æ˜å’Œè¯æ®çš„å›¾æ¥è§£å†³åˆ†è§£ä¸è¶³çš„é—®é¢˜ï¼Œå¹¶é€šè¿‡å›¾ç»“æ„å‡å°‘æåŠçš„æ¨¡ç³Šæ€§ã€‚GraphFCçš„ç‰¹ç‚¹ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ–¹é¢ï¼šå›¾æ„å»ºã€å›¾å¼•å¯¼è§„åˆ’å’Œå›¾å¼•å¯¼æ£€æŸ¥ã€‚å›¾æ„å»ºè´Ÿè´£æ„å»ºå£°æ˜å’Œè¯æ®çš„å›¾ï¼›å›¾å¼•å¯¼è§„åˆ’åˆ™ä¼˜å…ˆå®‰æ’ä¸‰å…ƒç»„éªŒè¯çš„é¡ºåºï¼›å›¾å¼•å¯¼æ£€æŸ¥åˆ™é€ä¸€éªŒè¯å£°æ˜å’Œè¯æ®å›¾ä¹‹é—´çš„ä¸‰å…ƒç»„ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGraphFCèƒ½å¤Ÿå®ç°ç²¾ç»†çš„åˆ†è§£ï¼Œé€šè¿‡å…³ç³»çº¦æŸè§£å†³æŒ‡ä»£æ­§ä¹‰é—®é¢˜ï¼Œåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šå‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>äº‹å®æ ¸æŸ¥åœ¨æ‰“å‡»è™šå‡ä¿¡æ¯ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>å½“å‰ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå£°æ˜åˆ†è§£çš„æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªä¸»è¦å±€é™ï¼šåˆ†è§£ä¸è¶³å’ŒæåŠçš„æ¨¡ç³Šæ€§ã€‚</li>
<li>å¼•å…¥å£°æ˜å›¾å¯ä»¥è§£å†³åˆ†è§£ä¸è¶³çš„é—®é¢˜ï¼Œå‡å°‘æåŠçš„æ¨¡ç³Šæ€§ã€‚</li>
<li>æå‡ºçš„GraphFCå›¾åŸºæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå›¾æ„å»ºã€å›¾å¼•å¯¼è§„åˆ’å’Œå›¾å¼•å¯¼æ£€æŸ¥ã€‚</li>
<li>å›¾æ„å»ºè´Ÿè´£æ„å»ºå£°æ˜å’Œè¯æ®çš„å›¾ï¼Œä»¥ç®€åŒ–éªŒè¯è¿‡ç¨‹ã€‚</li>
<li>å›¾å¼•å¯¼è§„åˆ’ä¼˜å…ˆå®‰æ’ä¸‰å…ƒç»„éªŒè¯çš„é¡ºåºï¼Œä»¥æé«˜éªŒè¯æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9df210adb747b6db7b36dd8ea26064eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-311bbf8b66a0cddcd9968a6eb0b56016.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CoT-Drive-Efficient-Motion-Forecasting-for-Autonomous-Driving-with-LLMs-and-Chain-of-Thought-Prompting"><a href="#CoT-Drive-Efficient-Motion-Forecasting-for-Autonomous-Driving-with-LLMs-and-Chain-of-Thought-Prompting" class="headerlink" title="CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs   and Chain-of-Thought Prompting"></a>CoT-Drive: Efficient Motion Forecasting for Autonomous Driving with LLMs   and Chain-of-Thought Prompting</h2><p><strong>Authors:Haicheng Liao, Hanlin Kong, Bonan Wang, Chengyue Wang, Wang Ye, Zhengbing He, Chengzhong Xu, Zhenning Li</strong></p>
<p>Accurate motion forecasting is crucial for safe autonomous driving (AD). This study proposes CoT-Drive, a novel approach that enhances motion forecasting by leveraging large language models (LLMs) and a chain-of-thought (CoT) prompting method. We introduce a teacher-student knowledge distillation strategy to effectively transfer LLMsâ€™ advanced scene understanding capabilities to lightweight language models (LMs), ensuring that CoT-Drive operates in real-time on edge devices while maintaining comprehensive scene understanding and generalization capabilities. By leveraging CoT prompting techniques for LLMs without additional training, CoT-Drive generates semantic annotations that significantly improve the understanding of complex traffic environments, thereby boosting the accuracy and robustness of predictions. Additionally, we present two new scene description datasets, Highway-Text and Urban-Text, designed for fine-tuning lightweight LMs to generate context-specific semantic annotations. Comprehensive evaluations of five real-world datasets demonstrate that CoT-Drive outperforms existing models, highlighting its effectiveness and efficiency in handling complex traffic scenarios. Overall, this study is the first to consider the practical application of LLMs in this field. It pioneers the training and use of a lightweight LLM surrogate for motion forecasting, setting a new benchmark and showcasing the potential of integrating LLMs into AD systems. </p>
<blockquote>
<p>ç²¾ç¡®çš„åŠ¨ä½œé¢„æµ‹å¯¹äºå®‰å…¨è‡ªåŠ¨é©¾é©¶ï¼ˆADï¼‰è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•CoT-Driveï¼Œå®ƒå€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ–¹æ³•æ¥å¢å¼ºè¿åŠ¨é¢„æµ‹ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¸ˆå¾’çŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œä»¥æœ‰æ•ˆåœ°å°†LLMçš„é«˜çº§åœºæ™¯ç†è§£èƒ½åŠ›è½¬ç§»åˆ°è½»é‡çº§è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä¸Šï¼Œç¡®ä¿CoT-Driveåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®æ—¶è¿è¡Œï¼ŒåŒæ—¶ä¿æŒå…¨é¢çš„åœºæ™¯ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨CoTæç¤ºæŠ€æœ¯ï¼Œæ— éœ€é¢å¤–è®­ç»ƒï¼ŒCoT-Driveå¯ä»¥ç”Ÿæˆè¯­ä¹‰æ³¨é‡Šï¼Œæå¤§åœ°æé«˜äº†å¯¹å¤æ‚äº¤é€šç¯å¢ƒçš„ç†è§£ï¼Œä»è€Œæé«˜äº†é¢„æµ‹å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸¤æ¬¾æ–°çš„åœºæ™¯æè¿°æ•°æ®é›†â€”â€”Highway-Textå’ŒUrban-Textï¼Œä¸“ä¸ºå¾®è°ƒè½»é‡çº§LMä»¥ç”Ÿæˆç‰¹å®šä¸Šä¸‹æ–‡çš„è¯­ä¹‰æ³¨é‡Šè€Œè®¾è®¡ã€‚å¯¹äº”ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†çš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼ŒCoT-Driveä¼˜äºç°æœ‰æ¨¡å‹ï¼Œçªæ˜¾äº†å…¶åœ¨å¤„ç†å¤æ‚äº¤é€šåœºæ™¯æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬ç ”ç©¶é¦–æ¬¡è€ƒè™‘LLMåœ¨è¯¥é¢†åŸŸçš„å®é™…åº”ç”¨ã€‚å®ƒç‡å…ˆè®­ç»ƒå’Œä½¿ç”¨è½»é‡çº§LLMä»£ç†è¿›è¡ŒåŠ¨ä½œé¢„æµ‹ï¼Œè®¾å®šäº†æ–°çš„åŸºå‡†ï¼Œå±•ç¤ºäº†å°†LLMé›†æˆåˆ°è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07234v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æå‡ºäº†CoT-Driveï¼Œä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ–¹æ³•å¢å¼ºè¿åŠ¨é¢„æµ‹çš„æ–°æ–¹æ³•ã€‚é€šè¿‡å¼•å…¥æ•™å¸ˆ-å­¦ç”ŸçŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œå°†LLMsçš„é«˜çº§åœºæ™¯ç†è§£èƒ½åŠ›æœ‰æ•ˆåœ°è½¬ç§»åˆ°è½»é‡çº§è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰ä¸Šï¼Œç¡®ä¿CoT-Driveåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šå®æ—¶è¿è¡Œï¼ŒåŒæ—¶ä¿æŒå…¨é¢çš„åœºæ™¯ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ã€‚å€ŸåŠ©æ€ç»´é“¾æç¤ºæŠ€æœ¯ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯ç”Ÿæˆè¯­ä¹‰æ³¨é‡Šï¼Œæ˜¾è‘—æé«˜å¯¹å¤æ‚äº¤é€šç¯å¢ƒçš„ç†è§£ï¼Œä»è€Œæé«˜é¢„æµ‹å‡†ç¡®æ€§å’Œç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¨å‡ºä¸¤ä¸ªæ–°åœºæ™¯æè¿°æ•°æ®é›†â€”â€”é«˜é€Ÿå…¬è·¯æ–‡æœ¬å’ŒåŸå¸‚æ–‡æœ¬ï¼Œç”¨äºå¾®è°ƒè½»é‡çº§LMsä»¥ç”Ÿæˆä¸Šä¸‹æ–‡ç‰¹å®šçš„è¯­ä¹‰æ³¨é‡Šã€‚å…¨é¢è¯„ä¼°äº”ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†çš„ç»“æœè¡¨æ˜ï¼ŒCoT-Driveåœ¨åº”å¯¹å¤æ‚äº¤é€šåœºæ™¯æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬ç ”ç©¶é¦–æ¬¡è€ƒè™‘LLMsåœ¨è¯¥é¢†åŸŸçš„å®é™…åº”ç”¨ï¼Œå¼€åˆ›æ€§åœ°è®­ç»ƒå’Œä½¿ç”¨äº†è½»é‡çº§LLMæ›¿ä»£å“è¿›è¡Œè¿åŠ¨é¢„æµ‹ï¼Œæ ‘ç«‹äº†æ–°çš„åŸºå‡†ï¼Œå±•ç¤ºäº†å°†LLMsé›†æˆåˆ°è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°å‹è¿åŠ¨é¢„æµ‹æ–¹æ³•CoT-Driveï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œæ€ç»´é“¾ï¼ˆCoTï¼‰æç¤ºæ–¹æ³•ã€‚</li>
<li>é€šè¿‡æ•™å¸ˆ-å­¦ç”ŸçŸ¥è¯†è’¸é¦ç­–ç•¥ï¼Œå°†LLMsçš„é«˜çº§åœºæ™¯ç†è§£èƒ½åŠ›è½¬ç§»åˆ°è½»é‡çº§è¯­è¨€æ¨¡å‹ä¸Šã€‚</li>
<li>åˆ©ç”¨æ€ç»´é“¾æç¤ºæŠ€æœ¯ç”Ÿæˆè¯­ä¹‰æ³¨é‡Šï¼Œæé«˜å¤æ‚äº¤é€šç¯å¢ƒçš„ç†è§£ã€‚</li>
<li>æ¨å‡ºä¸¤ä¸ªæ–°åœºæ™¯æè¿°æ•°æ®é›†â€”â€”é«˜é€Ÿå…¬è·¯æ–‡æœ¬å’ŒåŸå¸‚æ–‡æœ¬ï¼Œç”¨äºè½»é‡çº§LMsçš„è®­ç»ƒã€‚</li>
<li>åœ¨äº”ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºCoT-Driveå…·æœ‰å“è¶Šçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚</li>
<li>ç ”ç©¶å±•ç¤ºäº†LLMsåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„æ½œåŠ›åŠå®é™…åº”ç”¨ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d7f907262e72df542fd23e7126ed880.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f10e62b7378e811da3f7b726e8a583f1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88bcbbe4b36de681685f327472b375e4.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Post-Training-Quantization-for-Diffusion-Transformer-via-Hierarchical-Timestep-Grouping"><a href="#Post-Training-Quantization-for-Diffusion-Transformer-via-Hierarchical-Timestep-Grouping" class="headerlink" title="Post-Training Quantization for Diffusion Transformer via Hierarchical   Timestep Grouping"></a>Post-Training Quantization for Diffusion Transformer via Hierarchical   Timestep Grouping</h2><p><strong>Authors:Ning Ding, Jing Han, Yuchuan Tian, Chao Xu, Kai Han, Yehui Tang</strong></p>
<p>Diffusion Transformer (DiT) has now become the preferred choice for building image generation models due to its great generation capability. Unlike previous convolution-based UNet models, DiT is purely composed of a stack of transformer blocks, which renders DiT excellent in scalability like large language models. However, the growing model size and multi-step sampling paradigm bring about considerable pressure on deployment and inference. In this work, we propose a post-training quantization framework tailored for Diffusion Transforms to tackle these challenges. We firstly locate that the quantization difficulty of DiT mainly originates from the time-dependent channel-specific outliers. We propose a timestep-aware shift-and-scale strategy to smooth the activation distribution to reduce the quantization error. Secondly, based on the observation that activations of adjacent timesteps have similar distributions, we utilize a hierarchical clustering scheme to divide the denoising timesteps into multiple groups. We further design a re-parameterization scheme which absorbs the quantization parameters into nearby module to avoid redundant computations. Comprehensive experiments demonstrate that out PTQ method successfully quantize the Diffusion Transformer into 8-bit weight and 8-bit activation (W8A8) with state-of-the-art FiD score. And our method can further quantize DiT model into 4-bit weight and 8-bit activation (W4A8) without sacrificing generation quality. </p>
<blockquote>
<p>æ‰©æ•£è½¬æ¢å™¨ï¼ˆDiTï¼‰å› å…¶å‡ºè‰²çš„ç”Ÿæˆèƒ½åŠ›ï¼Œç°å·²æˆä¸ºæ„å»ºå›¾åƒç”Ÿæˆæ¨¡å‹çš„é¦–é€‰ã€‚ä¸ä¹‹å‰çš„åŸºäºå·ç§¯çš„UNetæ¨¡å‹ä¸åŒï¼ŒDiTå®Œå…¨ç”±ä¸€å †è½¬æ¢å™¨å—ç»„æˆï¼Œè¿™ä½¿å¾—DiTåœ¨å¯æ‰©å±•æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°±åƒå¤§å‹è¯­è¨€æ¨¡å‹ä¸€æ ·ã€‚ç„¶è€Œï¼Œä¸æ–­å¢é•¿çš„æ¨¡å‹å¤§å°å’Œåˆ†æ­¥é‡‡æ ·èŒƒå¼ç»™éƒ¨ç½²å’Œæ¨ç†å¸¦æ¥äº†å·¨å¤§çš„å‹åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹æ‰©æ•£è½¬æ¢çš„åè®­ç»ƒé‡åŒ–æ¡†æ¶æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬é¦–å…ˆå‘ç°DiTçš„é‡åŒ–éš¾åº¦ä¸»è¦æºäºæ—¶é—´ä¾èµ–çš„é€šé“ç‰¹å®šå¼‚å¸¸å€¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ—¶é—´æ­¥æ„ŸçŸ¥çš„ç§»ä½å’Œç¼©æ”¾ç­–ç•¥ï¼Œä»¥å¹³æ»‘æ¿€æ´»åˆ†å¸ƒï¼Œä»è€Œå‡å°‘é‡åŒ–è¯¯å·®ã€‚å…¶æ¬¡ï¼ŒåŸºäºç›¸é‚»æ—¶é—´æ­¥çš„æ¿€æ´»å…·æœ‰ç›¸ä¼¼åˆ†å¸ƒçš„è§‚æµ‹ï¼Œæˆ‘ä»¬é‡‡ç”¨åˆ†å±‚èšç±»æ–¹æ¡ˆå°†å»å™ªæ—¶é—´æ­¥åˆ†ä¸ºå¤šä¸ªç»„ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ç§é‡æ–°å‚æ•°åŒ–æ–¹æ¡ˆï¼Œå°†é‡åŒ–å‚æ•°å¸æ”¶åˆ°é™„è¿‘çš„æ¨¡å—ä¸­ï¼Œä»¥é¿å…å†—ä½™è®¡ç®—ã€‚ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„PTQæ–¹æ³•æˆåŠŸåœ°å°†æ‰©æ•£è½¬æ¢å™¨é‡åŒ–åˆ°8ä½æƒé‡å’Œ8ä½æ¿€æ´»ï¼ˆW8A8ï¼‰ï¼Œå…·æœ‰æœ€å…ˆè¿›çš„FIDåˆ†æ•°ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜å¯ä»¥å°†DiTæ¨¡å‹è¿›ä¸€æ­¥é‡åŒ–åˆ°4ä½æƒé‡å’Œ8ä½æ¿€æ´»ï¼ˆW4A8ï¼‰ï¼Œè€Œä¸ä¼šç‰ºç‰²ç”Ÿæˆè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06930v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰çš„å®šåˆ¶åè®­ç»ƒé‡åŒ–æ¡†æ¶ï¼Œè§£å†³äº†æ¨¡å‹ç”Ÿæˆå›¾åƒæ—¶çš„éƒ¨ç½²å’Œæ¨ç†å‹åŠ›é—®é¢˜ã€‚æ–‡ç« æŒ‡å‡ºDiTçš„é‡åŒ–éš¾ç‚¹ä¸»è¦æ¥è‡ªæ—¶é—´ä¾èµ–çš„é€šé“ç‰¹å®šå¼‚å¸¸å€¼ï¼Œå¹¶æå‡ºäº†æ—¶é—´æ„ŸçŸ¥ç§»ä½ç¼©æ”¾ç­–ç•¥å’Œå±‚æ¬¡èšç±»æ–¹æ¡ˆæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜è®¾è®¡äº†ä¸€ç§é‡æ–°å‚æ•°åŒ–æ–¹æ¡ˆï¼Œé¿å…å†—ä½™è®¡ç®—ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸå°†æ‰©æ•£å˜æ¢å™¨é‡åŒ–è‡³8ä½æƒé‡å’Œ8ä½æ¿€æ´»ï¼ˆW8A8ï¼‰ï¼Œå¹¶è¾¾åˆ°äº†å…ˆè¿›çš„FIDå¾—åˆ†ã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•è¿˜èƒ½å°†DiTæ¨¡å‹è¿›ä¸€æ­¥é‡åŒ–è‡³4ä½æƒé‡å’Œ8ä½æ¿€æ´»ï¼ˆW4A8ï¼‰è€Œä¸ç‰ºç‰²ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰å·²æˆä¸ºå›¾åƒç”Ÿæˆæ¨¡å‹çš„ä¼˜é€‰ï¼Œå› å…¶å‡ºè‰²çš„ç”Ÿæˆèƒ½åŠ›å’Œç±»ä¼¼äºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‰¯å¥½å¯æ‰©å±•æ€§ã€‚</li>
<li>DiTçš„é‡åŒ–é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦æºäºæ—¶é—´ä¾èµ–çš„é€šé“ç‰¹å®šå¼‚å¸¸å€¼ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ—¶é—´æ„ŸçŸ¥ç§»ä½ç¼©æ”¾ç­–ç•¥ï¼Œé€šè¿‡å¹³æ»‘æ¿€æ´»åˆ†å¸ƒæ¥å‡å°‘é‡åŒ–è¯¯å·®ã€‚</li>
<li>åˆ©ç”¨å±‚æ¬¡èšç±»æ–¹æ¡ˆå°†å»å™ªæ—¶é—´æ­¥é•¿åˆ†ç»„ï¼Œæé«˜æ¨¡å‹æ•ˆç‡ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§é‡æ–°å‚æ•°åŒ–æ–¹æ¡ˆï¼Œé¿å…å†—ä½™è®¡ç®—ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸå°†DiTæ¨¡å‹é‡åŒ–è‡³W8A8ï¼Œå¹¶ä¿æŒè¾ƒé«˜çš„FIDå¾—åˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5e7b9d1dccbb72da8dc4a930cf77cc82.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-30473515ddbdf7544074ddf20d791574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ded74726a85b45af236fb39b7e3562b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dc52c37b37e796b0e2deafe36b380c57.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CalliReader-Contextualizing-Chinese-Calligraphy-via-an-Embedding-Aligned-Vision-Language-Model"><a href="#CalliReader-Contextualizing-Chinese-Calligraphy-via-an-Embedding-Aligned-Vision-Language-Model" class="headerlink" title="CalliReader: Contextualizing Chinese Calligraphy via an   Embedding-Aligned Vision-Language Model"></a>CalliReader: Contextualizing Chinese Calligraphy via an   Embedding-Aligned Vision-Language Model</h2><p><strong>Authors:Yuxuan Luo, Jiaqi Tang, Chenyi Huang, Feiyang Hao, Zhouhui Lian</strong></p>
<p>Chinese calligraphy, a UNESCO Heritage, remains computationally challenging due to visual ambiguity and cultural complexity. Existing AI systems fail to contextualize their intricate scripts, because of limited annotated data and poor visual-semantic alignment. We propose CalliReader, a vision-language model (VLM) that solves the Chinese Calligraphy Contextualization (CC$^2$) problem through three innovations: (1) character-wise slicing for precise character extraction and sorting, (2) CalliAlign for visual-text token compression and alignment, (3) embedding instruction tuning (e-IT) for improving alignment and addressing data scarcity. We also build CalliBench, the first benchmark for full-page calligraphic contextualization, addressing three critical issues in previous OCR and VQA approaches: fragmented context, shallow reasoning, and hallucination. Extensive experiments including user studies have been conducted to verify our CalliReaderâ€™s \textbf{superiority to other state-of-the-art methods and even human professionals in page-level calligraphy recognition and interpretation}, achieving higher accuracy while reducing hallucination. Comparisons with reasoning models highlight the importance of accurate recognition as a prerequisite for reliable comprehension. Quantitative analyses validate CalliReaderâ€™s efficiency; evaluations on document and real-world benchmarks confirm its robust generalization ability. </p>
<blockquote>
<p>ä¸­å›½ä¹¦æ³•çš„ä¼ æ‰¿ï¼Œå› å…¶ç‹¬ç‰¹çš„è§†è§‰å¤æ‚æ€§å’Œæ–‡åŒ–å› ç´ è€Œæˆä¸ºè”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡çš„ä¸–ç•Œé—äº§ã€‚ç„¶è€Œï¼Œç”±äºç°æœ‰çš„AIç³»ç»Ÿé¢ä¸´è§†è§‰æ¨¡ç³Šå’Œæ–‡åŒ–å¤æ‚æ€§æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œå®ƒä»¬åœ¨è§£æå¤æ‚çš„ä¹¦æ³•ç¬¦å·æ—¶æ— æ³•åšåˆ°å……åˆ†ç†è§£ä¸Šä¸‹æ–‡è¯­å¢ƒã€‚è¿™ç§æƒ…å†µçš„äº§ç”Ÿä¸»è¦æ˜¯ç”±äºæ³¨é‡Šæ•°æ®æœ‰é™ä»¥åŠè§†è§‰è¯­ä¹‰åŒ¹é…æ•ˆæœä¸ä½³æ‰€å¯¼è‡´çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå¹¶å‘½åä¸ºCalliReaderã€‚å®ƒé€šè¿‡ä¸‰é¡¹åˆ›æ–°è§£å†³äº†ä¸­æ–‡ä¹¦æ³•ä¸Šä¸‹æ–‡è§£æï¼ˆCC$^2$ï¼‰é—®é¢˜ï¼šï¼ˆ1ï¼‰å­—ç¬¦çº§åˆ‡ç‰‡æŠ€æœ¯ï¼Œç”¨äºç²¾ç¡®æå–å’Œæ’åºå­—ç¬¦ï¼›ï¼ˆ2ï¼‰CalliAlignç”¨äºè§†è§‰æ–‡æœ¬æ ‡è®°å‹ç¼©å’Œå¯¹é½ï¼›ï¼ˆ3ï¼‰åµŒå…¥æŒ‡ä»¤å¾®è°ƒï¼ˆe-ITï¼‰æŠ€æœ¯ç”¨äºæ”¹è¿›å¯¹é½å¹¶è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†CalliBenchï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºå…¨é¡µä¹¦æ³•ä¸Šä¸‹æ–‡åŒ–çš„åŸºå‡†æµ‹è¯•ï¼Œè§£å†³äº†ä»¥å‰OCRå’ŒVQAæ–¹æ³•ä¸­çš„ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–ã€æ¨ç†æµ…å±‚æ¬¡å’Œå¹»è§‰ç°è±¡ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒå’Œç”¨æˆ·ç ”ç©¶ï¼ŒéªŒè¯äº†CalliReaderåœ¨é¡µçº§ä¹¦æ³•è¯†åˆ«å’Œè§£é‡Šæ–¹é¢ä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•å’Œç”šè‡³ä¸“ä¸šäººç±»ä¸“å®¶çš„ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿåœ¨æé«˜å‡†ç¡®æ€§çš„åŒæ—¶å‡å°‘å¹»è§‰ç°è±¡çš„å‡ºç°ã€‚ä¸æ¨ç†æ¨¡å‹çš„æ¯”è¾ƒçªå‡ºäº†å‡†ç¡®è¯†åˆ«æ˜¯å¯é ç†è§£çš„é‡è¦å…ˆå†³æ¡ä»¶çš„é‡è¦æ€§ã€‚å®šé‡åˆ†æéªŒè¯äº†CalliReaderçš„æ•ˆç‡ï¼›åœ¨æ–‡æ¡£å’ŒçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06472v1">PDF</a> 11 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸­æ–‡ä¹¦æ³•è¿™ä¸€è”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡é—äº§åœ¨è®¡ç®—ä¸Šæ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚è§†è§‰æ¨¡ç³Šå’Œæ–‡åŒ–å¤æ‚æ€§ç­‰ã€‚ç°æœ‰çš„AIç³»ç»Ÿæ— æ³•æ ¹æ®ä¸Šä¸‹æ–‡è§£æå…¶å¤æ‚çš„è„šæœ¬ï¼Œè¿™å—é™äºæœ‰é™æ ‡æ³¨æ•°æ®å’Œè§†è§‰è¯­ä¹‰å¯¹é½ä¸ä½³çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†CalliReaderï¼Œä¸€ç§è§£å†³ä¸­æ–‡ä¹¦æ³•ä¸Šä¸‹æ–‡è§£æé—®é¢˜çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚è¯¥æ¨¡å‹é€šè¿‡ä¸‰é¡¹åˆ›æ–°æŠ€æœ¯æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼šå­—ç¬¦çº§çš„åˆ‡ç‰‡æŠ€æœ¯ä»¥å®ç°ç²¾ç¡®å­—ç¬¦æå–å’Œæ’åºã€CalliAlignè§†è§‰æ–‡æœ¬ä»¤ç‰Œå‹ç¼©å’Œå¯¹é½æŠ€æœ¯ã€åµŒå…¥æŒ‡ä»¤è°ƒæ•´ï¼ˆe-ITï¼‰ä»¥æé«˜å¯¹é½å¹¶è§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ„å»ºäº†CalliBenchï¼Œé¦–ä¸ªå…¨é¡µä¹¦æ³•ä¸Šä¸‹æ–‡è§£æçš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œè§£å†³äº†ä¹‹å‰OCRå’ŒVQAæ–¹æ³•ä¸­çš„ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–ã€æ¨ç†æµ…å±‚å’Œè™šæ„é—®é¢˜ã€‚å®éªŒå’Œç”¨æˆ·ç ”ç©¶è¯æ˜ï¼ŒCalliReaderåœ¨é¡µçº§ä¹¦æ³•è¯†åˆ«å’Œè§£é‡Šæ–¹é¢ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•å’Œä¸“ä¸šäººç±»ï¼Œå®ç°äº†æ›´é«˜çš„å‡†ç¡®æ€§å’Œè¾ƒä½çš„è™šæ„æ€§ã€‚ä¸æ¨ç†æ¨¡å‹çš„æ¯”è¾ƒçªæ˜¾äº†å‡†ç¡®è¯†åˆ«ä½œä¸ºå¯é ç†è§£å…ˆå†³æ¡ä»¶çš„é‡è¦æ€§ã€‚å®šé‡åˆ†æéªŒè¯äº†CalliReaderçš„æ•ˆç‡ï¼Œå¯¹æ–‡æ¡£å’ŒçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•è¯„ä¼°è¯æ˜äº†å…¶ç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸­æ–‡ä¹¦æ³•æ˜¯è”åˆå›½æ•™ç§‘æ–‡ç»„ç»‡è®¤å®šçš„é‡è¦é—äº§ï¼Œé¢ä¸´è§†è§‰æ¨¡ç³Šå’Œæ–‡åŒ–å¤æ‚æ€§å¸¦æ¥çš„è®¡ç®—æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰AIç³»ç»Ÿåœ¨å¤„ç†ä¸­æ–‡ä¹¦æ³•æ—¶å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦æ˜¯å› ä¸ºæ ‡æ³¨æ•°æ®æœ‰é™å’Œè§†è§‰è¯­ä¹‰å¯¹é½ä¸ä½³ã€‚</li>
<li>CalliReaderæ˜¯ä¸€ç§è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œé€šè¿‡ä¸‰é¡¹åˆ›æ–°æŠ€æœ¯è§£å†³ä¸­æ–‡ä¹¦æ³•ä¸Šä¸‹æ–‡è§£æé—®é¢˜ï¼šå­—ç¬¦æå–ã€è§†è§‰æ–‡æœ¬ä»¤ç‰Œå‹ç¼©å’Œå¯¹é½ä»¥åŠåµŒå…¥æŒ‡ä»¤è°ƒæ•´ã€‚</li>
<li>CalliBenchæ˜¯é¦–ä¸ªå…¨é¡µä¹¦æ³•ä¸Šä¸‹æ–‡è§£æçš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œè§£å†³äº†ä¹‹å‰OCRå’ŒVQAæ–¹æ³•çš„ä¸‰ä¸ªå…³é”®é—®é¢˜ã€‚</li>
<li>CalliReaderåœ¨é¡µçº§ä¹¦æ³•è¯†åˆ«å’Œè§£é‡Šæ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•å’Œä¸“ä¸šäººç±»ã€‚</li>
<li>å‡†ç¡®è¯†åˆ«æ˜¯å¯é ç†è§£çš„é‡è¦å…ˆå†³æ¡ä»¶ï¼Œä¸æ¨ç†æ¨¡å‹çš„æ¯”è¾ƒçªæ˜¾äº†è¿™ä¸€è§‚ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06472">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15d210a1959277cee62dc1a65be47b0f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-568a8ec14eee635810565d5da1f3332e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36be24ed024289de1dc627a3138f98fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c893546b2b2f442e9537babc6e225c3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54a95fc09bdc8be6d33a50a811d85885.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-611f1d22f58d3fe8ea4ca932bc1818fc.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Advancing-Autonomous-Vehicle-Intelligence-Deep-Learning-and-Multimodal-LLM-for-Traffic-Sign-Recognition-and-Robust-Lane-Detection"><a href="#Advancing-Autonomous-Vehicle-Intelligence-Deep-Learning-and-Multimodal-LLM-for-Traffic-Sign-Recognition-and-Robust-Lane-Detection" class="headerlink" title="Advancing Autonomous Vehicle Intelligence: Deep Learning and Multimodal   LLM for Traffic Sign Recognition and Robust Lane Detection"></a>Advancing Autonomous Vehicle Intelligence: Deep Learning and Multimodal   LLM for Traffic Sign Recognition and Robust Lane Detection</h2><p><strong>Authors:Chandan Kumar Sah, Ankit Kumar Shaw, Xiaoli Lian, Arsalan Shahid Baig, Tuopu Wen, Kun Jiang, Mengmeng Yang, Diange Yang</strong></p>
<p>Autonomous vehicles (AVs) require reliable traffic sign recognition and robust lane detection capabilities to ensure safe navigation in complex and dynamic environments. This paper introduces an integrated approach combining advanced deep learning techniques and Multimodal Large Language Models (MLLMs) for comprehensive road perception. For traffic sign recognition, we systematically evaluate ResNet-50, YOLOv8, and RT-DETR, achieving state-of-the-art performance of 99.8% with ResNet-50, 98.0% accuracy with YOLOv8, and achieved 96.6% accuracy in RT-DETR despite its higher computational complexity. For lane detection, we propose a CNN-based segmentation method enhanced by polynomial curve fitting, which delivers high accuracy under favorable conditions. Furthermore, we introduce a lightweight, Multimodal, LLM-based framework that directly undergoes instruction tuning using small yet diverse datasets, eliminating the need for initial pretraining. This framework effectively handles various lane types, complex intersections, and merging zones, significantly enhancing lane detection reliability by reasoning under adverse conditions. Despite constraints in available training resources, our multimodal approach demonstrates advanced reasoning capabilities, achieving a Frame Overall Accuracy (FRM) of 53.87%, a Question Overall Accuracy (QNS) of 82.83%, lane detection accuracies of 99.6% in clear conditions and 93.0% at night, and robust performance in reasoning about lane invisibility due to rain (88.4%) or road degradation (95.6%). The proposed comprehensive framework markedly enhances AV perception reliability, thus contributing significantly to safer autonomous driving across diverse and challenging road scenarios. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶è½¦è¾†ï¼ˆAVsï¼‰éœ€è¦å¯é çš„äº¤é€šæ ‡å¿—è¯†åˆ«å’Œç¨³å¥çš„è½¦é“æ£€æµ‹èƒ½åŠ›ï¼Œä»¥ç¡®ä¿åœ¨å¤æ‚å’ŒåŠ¨æ€ç¯å¢ƒä¸­çš„å®‰å…¨å¯¼èˆªã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆå…ˆè¿›æ·±åº¦å­¦ä¹ æŠ€æœ¯å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç»¼åˆæ–¹æ³•ï¼Œç”¨äºå…¨é¢çš„é“è·¯æ„ŸçŸ¥ã€‚å¯¹äºäº¤é€šæ ‡å¿—è¯†åˆ«ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†ResNet-50ã€YOLOv8å’ŒRT-DETRçš„æ€§èƒ½ï¼Œä½¿ç”¨ResNet-50è¾¾åˆ°äº†99.8%çš„å…ˆè¿›æ€§èƒ½ï¼Œä½¿ç”¨YOLOv8è¾¾åˆ°äº†98.0%çš„å‡†ç¡®åº¦ï¼Œå°½ç®¡RT-DETRçš„è®¡ç®—å¤æ‚åº¦è¾ƒé«˜ï¼Œä½†è¾¾åˆ°äº†96.6%çš„å‡†ç¡®åº¦ã€‚å¯¹äºè½¦é“æ£€æµ‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºCNNçš„åˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡å¤šé¡¹å¼æ›²çº¿æ‹Ÿåˆè¿›è¡Œå¢å¼ºï¼Œåœ¨æœ‰åˆ©æ¡ä»¶ä¸‹å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„ã€åŸºäºå¤šæ¨¡æ€LLMçš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡ç›´æ¥ä½¿ç”¨å°å‹ä½†å¤šæ ·åŒ–çš„æ•°æ®é›†è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œæ— éœ€åˆå§‹é¢„è®­ç»ƒã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å„ç§è½¦é“ç±»å‹ã€å¤æ‚äº¤å‰è·¯å£å’Œåˆå¹¶åŒºåŸŸï¼Œé€šè¿‡ä¸è‰¯æ¡ä»¶ä¸‹çš„æ¨ç†ï¼Œæ˜¾è‘—æé«˜è½¦é“æ£€æµ‹çš„å¯é æ€§ã€‚å°½ç®¡å¯ç”¨çš„è®­ç»ƒèµ„æºå­˜åœ¨çº¦æŸï¼Œæˆ‘ä»¬çš„å¤šæ¨¡æ€æ–¹æ³•å±•ç¤ºäº†é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œè¾¾åˆ°äº†å¸§æ•´ä½“å‡†ç¡®ç‡ï¼ˆFRMï¼‰ä¸º53.87%ï¼Œé—®é¢˜æ•´ä½“å‡†ç¡®ç‡ï¼ˆQNSï¼‰ä¸º82.83%ï¼Œåœ¨æ¸…æ™°æ¡ä»¶ä¸‹çš„è½¦é“æ£€æµ‹å‡†ç¡®ç‡ä¸º99.6%ï¼Œå¤œé—´ä¸º93.0%ï¼Œå¹¶ä¸”åœ¨ç†æ€§å…³äºå› é™é›¨ï¼ˆ88.4%ï¼‰æˆ–è·¯é¢é€€åŒ–ï¼ˆ95.6%ï¼‰å¯¼è‡´è½¦é“ä¸å¯è§çš„æƒ…å†µæ—¶è¡¨ç°å‡ºç¨³å¥çš„æ€§èƒ½ã€‚æ‰€æå‡ºçš„ç»¼åˆæ¡†æ¶æ˜¾è‘—æé«˜äº†AVæ„ŸçŸ¥å¯é æ€§ï¼Œä»è€Œä¸ºå„ç§å…·æœ‰æŒ‘æˆ˜çš„é“è·¯åœºæ™¯ä¸­çš„æ›´å®‰å…¨è‡ªåŠ¨é©¾é©¶åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06313v1">PDF</a> 11 pages, 9 figures</p>
<p><strong>Summary</strong></p>
<p>åœ¨è‡ªåŠ¨é©¾é©¶è½¦è¾†ä¸­ï¼Œäº¤é€šæ ‡å¿—è¯†åˆ«å’Œè½¦é“æ£€æµ‹æ˜¯å®ç°å®‰å…¨å¯¼èˆªçš„å…³é”®æŠ€æœ¯ã€‚æœ¬æ–‡ç»“åˆäº†æ·±åº¦å­¦ä¹ å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œæå‡ºäº†ä¸€ç§å…¨é¢çš„é“è·¯æ„ŸçŸ¥æ–¹æ³•ã€‚åœ¨äº¤é€šæ ‡å¿—è¯†åˆ«æ–¹é¢ï¼Œé€šè¿‡ResNet-50ã€YOLOv8å’ŒRT-DETRç­‰æ–¹æ³•å®ç°äº†é«˜è¾¾99.8%ã€98.0%å’Œ96.6%çš„å‡†ç¡®ç‡ã€‚åœ¨è½¦é“æ£€æµ‹æ–¹é¢ï¼Œé€šè¿‡CNNåˆ†å‰²æ–¹æ³•å’Œå¤šé¡¹å¼æ›²çº¿æ‹ŸåˆæŠ€æœ¯æé«˜äº†å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMçš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œå¯ç›´æ¥é€šè¿‡æŒ‡ä»¤è°ƒæ•´ä½¿ç”¨å°å‹å¤šæ ·æ•°æ®é›†ï¼Œæ— éœ€åˆå§‹é¢„è®­ç»ƒã€‚è¯¥æ¡†æ¶åœ¨å„ç§è½¦é“ç±»å‹ã€å¤æ‚äº¤å‰å£å’Œåˆå¹¶åŒºç­‰æ¡ä»¶ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œæé«˜äº†è½¦é“æ£€æµ‹çš„å¯é æ€§ã€‚åœ¨æœ‰é™çš„è®­ç»ƒèµ„æºä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æŒ‘æˆ˜åœºæ™¯ä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†æä¾›äº†å¯é çš„æ„ŸçŸ¥æ”¯æŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨é©¾é©¶è½¦è¾†éœ€è¦å¯é çš„äº¤é€šæ ‡å¿—è¯†åˆ«å’Œè½¦é“æ£€æµ‹æŠ€æœ¯ä»¥ç¡®ä¿å®‰å…¨å¯¼èˆªã€‚</li>
<li>æœ¬æ–‡ç»“åˆäº†æ·±åº¦å­¦ä¹ å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æå‡ºäº†å…¨é¢çš„é“è·¯æ„ŸçŸ¥æ–¹æ³•ã€‚</li>
<li>åœ¨äº¤é€šæ ‡å¿—è¯†åˆ«æ–¹é¢ï¼ŒResNet-50ã€YOLOv8å’ŒRT-DETRç­‰æ–¹æ³•è¡¨ç°å‡ºé«˜å‡†ç¡®ç‡ã€‚</li>
<li>è½¦é“æ£€æµ‹æ–¹é¢é‡‡ç”¨äº†CNNåˆ†å‰²æ–¹æ³•å’Œå¤šé¡¹å¼æ›²çº¿æ‹ŸåˆæŠ€æœ¯ä»¥æé«˜å‡†ç¡®æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºLLMçš„å¤šæ¨¡æ€æ¡†æ¶ï¼Œå¯ç›´æ¥é€šè¿‡æŒ‡ä»¤è°ƒæ•´ä½¿ç”¨å°å‹æ•°æ®é›†ï¼Œå¢å¼ºäº†åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„è½¦é“æ£€æµ‹å¯é æ€§ã€‚</li>
<li>åœ¨æœ‰é™çš„è®­ç»ƒèµ„æºä¸‹ï¼Œè¯¥æ–¹æ³•è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶è½¦è¾†æä¾›äº†å¯é çš„æ„ŸçŸ¥æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9c438c3c587ae035cf1e81af673c58ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c94dcb2b739034612df654c3877700b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c33a702dca3a97f1dbf2a47c4fa95243.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13e140e95ffb5b785407b50f1f827394.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1f32537be44819f6639d8b2873168cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e7ebf692272209ae4134a8284dbb2aa.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="X2I-Seamless-Integration-of-Multimodal-Understanding-into-Diffusion-Transformer-via-Attention-Distillation"><a href="#X2I-Seamless-Integration-of-Multimodal-Understanding-into-Diffusion-Transformer-via-Attention-Distillation" class="headerlink" title="X2I: Seamless Integration of Multimodal Understanding into Diffusion   Transformer via Attention Distillation"></a>X2I: Seamless Integration of Multimodal Understanding into Diffusion   Transformer via Attention Distillation</h2><p><strong>Authors:Jian Ma, Qirong Peng, Xu Guo, Chen Chen, Haonan Lu, Zhenyu Yang</strong></p>
<p>Text-to-image (T2I) models are well known for their ability to produce highly realistic images, while multimodal large language models (MLLMs) are renowned for their proficiency in understanding and integrating multiple modalities. However, currently there is no straightforward and efficient framework to transfer the multimodal comprehension abilities of MLLMs to T2I models to enable them to understand multimodal inputs. In this paper, we propose the X2I framework, which endows Diffusion Transformer (DiT) models with the capability to comprehend various modalities, including multilingual text, screenshot documents, images, videos, and audio. X2I is trained using merely 100K English corpus with 160 GPU hours. Building on the DiT teacher model, we adopt an innovative distillation method to extract the inference capabilities of the teacher model and design a lightweight AlignNet structure to serve as an intermediate bridge. Compared to the teacher model, X2I shows a decrease in performance degradation of less than 1% while gaining various multimodal understanding abilities, including multilingual to image, image to image, image-text to image, video to image, audio to image, and utilizing creative fusion to enhance imagery. Furthermore, it is applicable for LoRA training in the context of image-text to image generation, filling a void in the industry in this area. We further design a simple LightControl to enhance the fidelity of instructional image editing. Finally, extensive experiments demonstrate the effectiveness, efficiency, multifunctionality, and transferability of our X2I. The open-source code and checkpoints for X2I can be found at the following link: <a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/X2I">https://github.com/OPPO-Mente-Lab/X2I</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹ä»¥å…¶ç”Ÿæˆé«˜åº¦é€¼çœŸçš„å›¾åƒçš„èƒ½åŠ›è€Œé—»åï¼Œè€Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åˆ™ä»¥å…¶åœ¨ç†è§£å’Œèåˆå¤šç§æ¨¡æ€æ–¹é¢çš„ä¸“é•¿è€Œè‘—ç§°ã€‚ç„¶è€Œï¼Œç›®å‰æ²¡æœ‰ä¸€ä¸ªç®€å•æœ‰æ•ˆçš„æ¡†æ¶èƒ½å¤Ÿå°†MLLMsçš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›è½¬ç§»åˆ°T2Iæ¨¡å‹ä¸Šï¼Œä»¥ä½¿å…¶èƒ½å¤Ÿç†è§£å¤šæ¨¡æ€è¾“å…¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†X2Iæ¡†æ¶ï¼Œå®ƒèµ‹äºˆäº†Diffusion Transformerï¼ˆDiTï¼‰æ¨¡å‹ç†è§£å¤šç§æ¨¡æ€çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤šè¯­è¨€æ–‡æœ¬ã€æˆªå›¾æ–‡æ¡£ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘ã€‚X2Iä»…ä½¿ç”¨100Kè‹±æ–‡è¯­æ–™åº“å’Œ160 GPUå°æ—¶è¿›è¡Œè®­ç»ƒã€‚åŸºäºDiTæ•™å¸ˆæ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åˆ›æ–°çš„è’¸é¦æ–¹æ³•ï¼Œä»¥æå–æ•™å¸ˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„AlignNetç»“æ„ä½œä¸ºä¸­é—´æ¡¥æ¢ã€‚ä¸æ•™æ•™å¸ˆæ¨¡å‹ç›¸æ¯”ï¼ŒX2Içš„æ€§èƒ½ä¸‹é™ç¨‹åº¦ä¸åˆ°1%ï¼ŒåŒæ—¶è·å¾—äº†å¤šç§å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤šè¯­è¨€åˆ°å›¾åƒã€å›¾åƒåˆ°å›¾åƒã€å›¾æ–‡åˆ°å›¾åƒã€è§†é¢‘åˆ°å›¾åƒã€éŸ³é¢‘åˆ°å›¾åƒï¼Œå¹¶åˆ©ç”¨åˆ›æ„èåˆå¢å¼ºå›¾åƒã€‚æ­¤å¤–ï¼Œå®ƒåœ¨å›¾åƒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„LoRAè®­ç»ƒä¸­ä¹Ÿé€‚ç”¨ï¼Œå¡«è¡¥äº†è¡Œä¸šä¸­çš„ç©ºç™½ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ä¸ªç®€å•çš„LightControlï¼Œä»¥æé«˜æ•™å­¦å›¾åƒç¼–è¾‘çš„ä¿çœŸåº¦ã€‚æœ€åï¼Œå¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„X2Içš„æœ‰æ•ˆæ€§ã€æ•ˆç‡ã€å¤šåŠŸèƒ½æ€§å’Œå¯è¿ç§»æ€§ã€‚X2Içš„å¼€æºä»£ç å’Œæ£€æŸ¥ç‚¹å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/X2I%E3%80%82">https://github.com/OPPO-Mente-Lab/X2Iã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06134v1">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/OPPO-Mente-Lab/X2I">https://github.com/OPPO-Mente-Lab/X2I</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºX2Içš„æ¡†æ¶ï¼Œæ—¨åœ¨å°†å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ä»å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è½¬ç§»åˆ°æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹ä¸Šã€‚é€šè¿‡X2Iæ¡†æ¶ï¼ŒDiffusion Transformerï¼ˆDiTï¼‰æ¨¡å‹èƒ½å¤Ÿç†è§£åŒ…æ‹¬å¤šè¯­è¨€æ–‡æœ¬ã€æˆªå›¾æ–‡æ¡£ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘åœ¨å†…çš„å¤šç§æ¨¡æ€ã€‚è¯¥æ¡†æ¶ä»…ä½¿ç”¨100Kè‹±æ–‡è¯­æ–™åº“å’Œ160ä¸ªGPUå°æ—¶è¿›è¡Œè®­ç»ƒã€‚X2Ié‡‡ç”¨åˆ›æ–°çš„è’¸é¦æ–¹æ³•æå–æ•™å¸ˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶è®¾è®¡äº†ä¸€ä¸ªè½»é‡çº§çš„AlignNetç»“æ„ä½œä¸ºä¸­é—´æ¡¥æ¢ã€‚ä¸æ•™å¸ˆæ¨¡å‹ç›¸æ¯”ï¼ŒX2Iæ€§èƒ½ä¸‹é™ä¸åˆ°1%ï¼ŒåŒæ—¶è·å¾—äº†å¤šç§å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒX2Iè¿˜æ”¯æŒåœ¨å›¾åƒåˆ°å›¾åƒç”Ÿæˆé¢†åŸŸè¿›è¡ŒLoRAè®­ç»ƒï¼Œå¡«è¡¥äº†è¡Œä¸šç©ºç™½ã€‚X2Iæ¡†æ¶è¿˜å…·æœ‰æé«˜æŒ‡ä»¤æ€§å›¾åƒç¼–è¾‘ä¿çœŸåº¦çš„LightControlåŠŸèƒ½ã€‚æ€»ä½“è€Œè¨€ï¼ŒX2Iæ˜¯æœ‰æ•ˆã€é«˜æ•ˆã€å¤šåŠŸèƒ½å’Œå¯è¿ç§»çš„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>X2Iæ¡†æ¶æˆåŠŸå°†å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ä»MLLMsè½¬ç§»åˆ°T2Iæ¨¡å‹ä¸Šï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†å¤šç§æ¨¡æ€è¾“å…¥ã€‚</li>
<li>X2Ié€šè¿‡åˆ›æ–°çš„è’¸é¦æ–¹æ³•å’Œè½»é‡çº§AlignNetç»“æ„å®ç°é«˜æ•ˆè®­ç»ƒã€‚</li>
<li>X2Iåœ¨å°‘é‡è¯­æ–™åº“ï¼ˆ100Kè‹±æ–‡è¯­æ–™åº“ï¼‰å’Œè®¡ç®—èµ„æºï¼ˆ160 GPUå°æ—¶ï¼‰ä¸‹å®ç°è®­ç»ƒã€‚</li>
<li>ä¸æ•™å¸ˆæ¨¡å‹ç›¸æ¯”ï¼ŒX2Iæ€§èƒ½æŸå¤±å°äº1%ï¼Œä½†è·å¾—äº†å¤šç§å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚</li>
<li>X2Iæ”¯æŒå›¾åƒåˆ°å›¾åƒç”Ÿæˆé¢†åŸŸçš„LoRAè®­ç»ƒï¼Œå¡«è¡¥äº†è¡Œä¸šç©ºç™½ã€‚</li>
<li>X2Iæ¡†æ¶å…·æœ‰æé«˜æŒ‡ä»¤æ€§å›¾åƒç¼–è¾‘è´¨é‡çš„LightControlåŠŸèƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-097aea79fb37d977cf0ec9026c534f4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30a22a9bd2af36950b9a83af0f63d501.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-247b4e86452d21d3d3fb4549e938d6de.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="BlackGoose-Rimer-Harnessing-RWKV-7-as-a-Simple-yet-Superior-Replacement-for-Transformers-in-Large-Scale-Time-Series-Modeling"><a href="#BlackGoose-Rimer-Harnessing-RWKV-7-as-a-Simple-yet-Superior-Replacement-for-Transformers-in-Large-Scale-Time-Series-Modeling" class="headerlink" title="BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement   for Transformers in Large-Scale Time Series Modeling"></a>BlackGoose Rimer: Harnessing RWKV-7 as a Simple yet Superior Replacement   for Transformers in Large-Scale Time Series Modeling</h2><p><strong>Authors:Li weile, Liu Xiao</strong></p>
<p>Time series models face significant challenges in scaling to handle large and complex datasets, akin to the scaling achieved by large language models (LLMs). The unique characteristics of time series data and the computational demands of model scaling necessitate innovative approaches. While researchers have explored various architectures such as Transformers, LSTMs, and GRUs to address these challenges, we propose a novel solution using RWKV-7, which incorporates meta-learning into its state update mechanism. By integrating RWKV-7â€™s time mix and channel mix components into the transformer-based time series model Timer, we achieve a substantial performance improvement of approximately 1.13 to 43.3x and a 4.5x reduction in training time with 1&#x2F;23 parameters, all while utilizing fewer parameters. Our code and model weights are publicly available for further research and development at <a target="_blank" rel="noopener" href="https://github.com/Alic-Li/BlackGoose_Rimer">https://github.com/Alic-Li/BlackGoose_Rimer</a>. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—æ¨¡å‹åœ¨æ‰©å±•åˆ°å¤„ç†å¤§è§„æ¨¡ä¸”å¤æ‚çš„æ•°æ®é›†æ—¶é¢ä¸´å·¨å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰€å®ç°çš„è§„æ¨¡æ‰©å±•ç›¸ä¼¼ã€‚æ—¶é—´åºåˆ—æ•°æ®çš„ç‹¬ç‰¹ç‰¹å¾å’Œæ¨¡å‹æ‰©å±•çš„è®¡ç®—éœ€æ±‚éœ€è¦åˆ›æ–°çš„æ–¹æ³•ã€‚è™½ç„¶ç ”ç©¶äººå‘˜å·²ç»æ¢ç´¢äº†è¯¸å¦‚Transformerã€LSTMå’ŒGRUç­‰å„ç§æ¶æ„æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œä½†æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨RWKV-7çš„æ–°è§£å†³æ–¹æ¡ˆï¼Œå®ƒå°†å…ƒå­¦ä¹ çº³å…¥å…¶çŠ¶æ€æ›´æ–°æœºåˆ¶ã€‚é€šè¿‡å°†RWKV-7çš„æ—¶é—´æ··åˆå’Œé€šé“æ··åˆç»„ä»¶é›†æˆåˆ°åŸºäºTransformerçš„æ—¶é—´åºåˆ—æ¨¡å‹Timerä¸­ï¼Œæˆ‘ä»¬åœ¨ä½¿ç”¨æ›´å°‘å‚æ•°çš„æƒ…å†µä¸‹å®ç°äº†çº¦1.13è‡³43.3å€çš„æ˜¾è‘—æ€§èƒ½æå‡ï¼Œå¹¶å®ç°äº†è®­ç»ƒæ—¶é—´çš„4.5å€ç¼©å‡ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹æƒé‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Alic-Li/BlackGoose_Rimer%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%EF%BC%8C%E4%BB%A5%E4%BE%9B%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%E7%A0%94%E7%A9%B6%E5%92%8C%E5%BC%80%E5%8F%91%E3%80%82">https://github.com/Alic-Li/BlackGoose_Rimerä¸Šå…¬å¼€è·å–ï¼Œä»¥ä¾›è¿›ä¸€æ­¥çš„ç ”ç©¶å’Œå¼€å‘ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06121v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹å’Œå¤æ‚æ•°æ®é›†çš„å¤„ç†ä¸Šï¼Œæ—¶é—´åºåˆ—æ¨¡å‹é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œç±»ä¼¼äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ‰©å±•é—®é¢˜ã€‚é‰´äºæ—¶é—´åºåˆ—æ•°æ®çš„ç‹¬ç‰¹ç‰¹æ€§å’Œæ¨¡å‹æ‰©å±•çš„è®¡ç®—éœ€æ±‚ï¼Œéœ€è¦åˆ›æ–°æ–¹æ³•ã€‚ç ”ç©¶äººå‘˜å·²æ¢ç´¢å„ç§æ¶æ„ï¼Œå¦‚Transformerã€LSTMå’ŒGRUç­‰æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§ä½¿ç”¨RWKV-7çš„è§£å†³æ–¹æ³•ï¼Œå®ƒå°†å…ƒå­¦ä¹ èå…¥çŠ¶æ€æ›´æ–°æœºåˆ¶ä¸­ã€‚é€šè¿‡å°†RWKV-7çš„æ—¶é—´æ··åˆå’Œé€šé“æ··åˆæˆåˆ†èå…¥åŸºäºTransformerçš„æ—¶é—´åºåˆ—æ¨¡å‹Timerä¸­ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ˆçº¦1.13è‡³43.3å€ï¼‰ï¼Œè®­ç»ƒæ—¶é—´å‡å°‘äº†4.5å€ï¼ŒåŒæ—¶ä½¿ç”¨çš„å‚æ•°æ›´å°‘ï¼ˆä»…ä¸ºåŸæ¥çš„1&#x2F;23ï¼‰ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹æƒé‡å·²å…¬å¼€å‘å¸ƒäº[é“¾æ¥]ï¼Œä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶å’Œå‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´åºåˆ—æ¨¡å‹åœ¨åº”å¯¹å¤§è§„æ¨¡å¤æ‚æ•°æ®é›†æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦åˆ›æ–°æ–¹æ³•æ¥è§£å†³ã€‚</li>
<li>RWKV-7é€šè¿‡é›†æˆå…ƒå­¦ä¹ æ¥æ”¹è¿›çŠ¶æ€æ›´æ–°æœºåˆ¶ï¼Œä¸ºæ—¶é—´åºåˆ—æ¨¡å‹æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>ç»“åˆRWKV-7çš„æ—¶é—´æ··åˆå’Œé€šé“æ··åˆæˆåˆ†åˆ°åŸºäºTransformerçš„æ—¶é—´åºåˆ—æ¨¡å‹Timerä¸­ï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>ç›¸è¾ƒäºä¼ ç»Ÿæ¨¡å‹ï¼Œä½¿ç”¨RWKV-7çš„è§£å†³æ–¹æ¡ˆèƒ½å¤§å¹…åº¦å‡å°‘è®­ç»ƒæ—¶é—´å¹¶é™ä½å‚æ•°ä½¿ç”¨é‡ã€‚</li>
<li>è¯¥è§£å†³æ–¹æ¡ˆé€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†çš„å¤„ç†ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤„ç†å¤æ‚æ•°æ®æ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
<li>è¯¥ç ”ç©¶å…¬å¼€äº†ä»£ç å’Œæ¨¡å‹æƒé‡ä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶å’Œå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06121">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0eeaa64e63a850100bbc5efadbd0f7d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d8aabb4ea21ce3e9025b69082a003c6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f493f87c85e5335b01c34bb0b7b85cd3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3490678d4fdcc615e4368b16a0075357.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a1826988476887b211abd94a7fc7c2c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9edcf0f18f3c60b7f2f7b14147e9f8a.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DETQUS-Decomposition-Enhanced-Transformers-for-QUery-focused-Summarization"><a href="#DETQUS-Decomposition-Enhanced-Transformers-for-QUery-focused-Summarization" class="headerlink" title="DETQUS: Decomposition-Enhanced Transformers for QUery-focused   Summarization"></a>DETQUS: Decomposition-Enhanced Transformers for QUery-focused   Summarization</h2><p><strong>Authors:Yasir Khan, Xinlei Wu, Sangpil Youm, Justin Ho, Aryaan Shaikh, Jairo Garciga, Rohan Sharma, Bonnie J. Dorr</strong></p>
<p>Query-focused tabular summarization is an emerging task in table-to-text generation that synthesizes a summary response from tabular data based on user queries. Traditional transformer-based approaches face challenges due to token limitations and the complexity of reasoning over large tables. To address these challenges, we introduce DETQUS (Decomposition-Enhanced Transformers for QUery-focused Summarization), a system designed to improve summarization accuracy by leveraging tabular decomposition alongside a fine-tuned encoder-decoder model. DETQUS employs a large language model to selectively reduce table size, retaining only query-relevant columns while preserving essential information. This strategy enables more efficient processing of large tables and enhances summary quality. Our approach, equipped with table-based QA model Omnitab, achieves a ROUGE-L score of 0.4437, outperforming the previous state-of-the-art REFACTOR model (ROUGE-L: 0.422). These results highlight DETQUS as a scalable and effective solution for query-focused tabular summarization, offering a structured alternative to more complex architectures. </p>
<blockquote>
<p>æŸ¥è¯¢èšç„¦çš„è¡¨æ ¼æ‘˜è¦ç”Ÿæˆæ˜¯ä»è¡¨æ ¼åˆ°æ–‡æœ¬ç”Ÿæˆçš„æ–°å…´ä»»åŠ¡ä¹‹ä¸€ï¼Œè¯¥ä»»åŠ¡åŸºäºç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆä»è¡¨æ ¼æ•°æ®åˆæˆçš„æ‘˜è¦å“åº”ã€‚ä¼ ç»Ÿçš„åŸºäºTransformerçš„æ–¹æ³•ç”±äºæ ‡è®°é™åˆ¶å’Œå¤§å‹è¡¨æ ¼æ¨ç†çš„å¤æ‚æ€§è€Œé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DETQUSç³»ç»Ÿï¼ˆç”¨äºæŸ¥è¯¢èšç„¦æ‘˜è¦çš„å¢å¼ºåˆ†è§£è½¬æ¢å™¨ï¼‰ï¼Œè¯¥ç³»ç»Ÿé€šè¿‡åˆ©ç”¨è¡¨æ ¼åˆ†è§£å’Œå¾®è°ƒè¿‡çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹æ¥æé«˜æ‘˜è¦çš„å‡†ç¡®æ€§ã€‚DETQUSåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æœ‰é€‰æ‹©åœ°å‡å°‘è¡¨æ ¼å¤§å°ï¼Œåªä¿ç•™æŸ¥è¯¢ç›¸å…³çš„åˆ—ï¼ŒåŒæ—¶ä¿ç•™é‡è¦ä¿¡æ¯ã€‚è¿™ä¸€ç­–ç•¥ä½¿å¤§å‹è¡¨æ ¼çš„å¤„ç†æ›´åŠ é«˜æ•ˆï¼Œæé«˜äº†æ‘˜è¦çš„è´¨é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•é…å¤‡äº†åŸºäºè¡¨æ ¼çš„é—®ç­”æ¨¡å‹Omnitabï¼Œå®ç°äº†ROUGE-Lè¯„åˆ†0.4437ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„æœ€ä½³æ¨¡å‹REFACTORï¼ˆROUGE-Lï¼š0.422ï¼‰ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†DETQUSåœ¨æŸ¥è¯¢èšç„¦çš„è¡¨æ ¼æ‘˜è¦ç”Ÿæˆä¸­çš„å¯æ‰©å±•æ€§å’Œæœ‰æ•ˆæ€§ï¼Œä¸ºæ›´å¤æ‚çš„æ¶æ„æä¾›äº†ç»“æ„åŒ–çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05935v1">PDF</a> 12 pages, 2 figures, Accepted to NAACL 2025 main conference</p>
<p><strong>Summary</strong>ï¼š<br>åŸºäºç”¨æˆ·æŸ¥è¯¢çš„è¡¨æ ¼æ‘˜è¦ç”Ÿæˆæ˜¯ä¸€é¡¹æ–°å…´ä»»åŠ¡ï¼Œæ—¨åœ¨ä»è¡¨æ ¼æ•°æ®ä¸­ç”Ÿæˆæ‘˜è¦å¼å›ç­”ã€‚é’ˆå¯¹ä¼ ç»ŸåŸºäºTransformerçš„æ–¹æ³•åœ¨å¤„ç†å¤§å‹è¡¨æ ¼æ—¶é¢ä¸´çš„ä»¤ç‰Œé™åˆ¶å’Œæ¨ç†å¤æ‚æ€§æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DETQUSç³»ç»Ÿï¼Œå®ƒé€šè¿‡åˆ©ç”¨è¡¨æ ¼åˆ†è§£å’Œä¸€ä¸ªç²¾ç»†è°ƒæ•´çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹æ¥æé«˜æ‘˜è¦å‡†ç¡®æ€§ã€‚DETQUSé‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é€‰æ‹©æ€§åœ°å‡å°è¡¨æ ¼å¤§å°ï¼Œä»…ä¿ç•™ä¸æŸ¥è¯¢ç›¸å…³çš„åˆ—ï¼ŒåŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•ä½¿å¤§å‹è¡¨æ ¼çš„å¤„ç†æ›´åŠ é«˜æ•ˆï¼Œæé«˜äº†æ‘˜è¦è´¨é‡ã€‚é…å¤‡åŸºäºè¡¨æ ¼çš„é—®ç­”æ¨¡å‹Omnitabçš„DETQUSæ–¹æ³•å®ç°äº†ROUGE-Låˆ†æ•°ä¸º0.4437ï¼Œè¶…è¿‡äº†ä¹‹å‰çš„å…ˆè¿›æ¨¡å‹REFACTORï¼ˆROUGE-Lï¼š0.422ï¼‰ã€‚è¿™è¡¨æ˜DETQUSä¸ºæŸ¥è¯¢èšç„¦çš„è¡¨æ ¼æ‘˜è¦æä¾›äº†å¯æ‰©å±•å’Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºæ›´å¤æ‚çš„æ¶æ„æä¾›äº†ç»“æ„åŒ–æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Query-focused tabular summarizationæ˜¯ä¸€é¡¹æ–°å…´ä»»åŠ¡ï¼Œæ—¨åœ¨åŸºäºç”¨æˆ·æŸ¥è¯¢ä»è¡¨æ ¼æ•°æ®ä¸­ç”Ÿæˆæ‘˜è¦ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•åœ¨å¤„ç†å¤§å‹è¡¨æ ¼æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚ä»¤ç‰Œé™åˆ¶å’Œæ¨ç†å¤æ‚æ€§ã€‚</li>
<li>DETQUSç³»ç»Ÿé€šè¿‡åˆ©ç”¨è¡¨æ ¼åˆ†è§£å’Œç²¾ç»†è°ƒæ•´çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹æ¥æé«˜æ‘˜è¦å‡†ç¡®æ€§ã€‚</li>
<li>DETQUSé‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é€‰æ‹©æ€§åœ°å‡å°è¡¨æ ¼å¤§å°ï¼Œä»…ä¿ç•™ä¸æŸ¥è¯¢ç›¸å…³çš„åˆ—ã€‚</li>
<li>DETQUSæé«˜äº†æ‘˜è¦è´¨é‡ï¼Œå¹¶ä¸”å¤„ç†å¤§å‹è¡¨æ ¼æ›´åŠ é«˜æ•ˆã€‚</li>
<li>DETQUSé…å¤‡äº†åŸºäºè¡¨æ ¼çš„é—®ç­”æ¨¡å‹Omnitabï¼Œå®ç°äº†è¾ƒé«˜çš„ROUGE-Låˆ†æ•°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05935">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-877d3cec9ff3518ce3e41ff72a977c4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36c434bb2dda5e814f3e4ff17325c10a.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="The-Lazy-Studentâ€™s-Dream-ChatGPT-Passing-an-Engineering-Course-on-Its-Own"><a href="#The-Lazy-Studentâ€™s-Dream-ChatGPT-Passing-an-Engineering-Course-on-Its-Own" class="headerlink" title="The Lazy Studentâ€™s Dream: ChatGPT Passing an Engineering Course on Its   Own"></a>The Lazy Studentâ€™s Dream: ChatGPT Passing an Engineering Course on Its   Own</h2><p><strong>Authors:Gokul Puthumanaillam, Melkior Ornik</strong></p>
<p>This paper presents a comprehensive investigation into the capability of Large Language Models (LLMs) to successfully complete a semester-long undergraduate control systems course. Through evaluation of 115 course deliverables, we assess LLM performance using ChatGPT under a &#96;&#96;minimal effortâ€ protocol that simulates realistic student usage patterns. The investigation employs a rigorous testing methodology across multiple assessment formats, from auto-graded multiple choice questions to complex Python programming tasks and long-form analytical writing. Our analysis provides quantitative insights into AIâ€™s strengths and limitations in handling mathematical formulations, coding challenges, and theoretical concepts in control systems engineering. The LLM achieved a B-grade performance (82.24%), approaching but not exceeding the class average (84.99%), with strongest results in structured assignments and greatest limitations in open-ended projects. The findings inform discussions about course design adaptation in response to AI advancement, moving beyond simple prohibition towards thoughtful integration of these tools in engineering education. Additional materials including syllabus, examination papers, design projects, and example responses can be found at the project website: <a target="_blank" rel="noopener" href="https://gradegpt.github.io/">https://gradegpt.github.io</a>. </p>
<blockquote>
<p>æœ¬æ–‡å…¨é¢æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æˆåŠŸå®Œæˆä¸€ä¸ªå­¦æœŸé•¿çš„æœ¬ç§‘æ§åˆ¶ç³»ç»Ÿè¯¾ç¨‹çš„èƒ½åŠ›ã€‚é€šè¿‡å¯¹115ä»½è¯¾ç¨‹äº¤ä»˜æˆæœçš„è¯„ä»·ï¼Œæˆ‘ä»¬åœ¨â€œæœ€å°åŠªåŠ›â€åè®®ä¸‹è¯„ä¼°äº†LLMä½¿ç”¨ChatGPTçš„è¡¨ç°ï¼Œè¯¥åè®®æ¨¡æ‹Ÿäº†ç°å®çš„å­¦ç”Ÿä½¿ç”¨æ¨¡å¼ã€‚è°ƒæŸ¥é‡‡ç”¨äº†ä¸¥æ ¼çš„æµ‹è¯•æ–¹æ³•ï¼Œæ¶µç›–å¤šç§è¯„ä¼°å½¢å¼ï¼Œä»è‡ªåŠ¨åˆ†çº§çš„å¤šä¸ªé€‰æ‹©é¢˜åˆ°å¤æ‚çš„Pythonç¼–ç¨‹ä»»åŠ¡å’Œé•¿ç¯‡åˆ†æå†™ä½œã€‚æˆ‘ä»¬çš„åˆ†ææä¾›äº†å…³äºAIåœ¨å¤„ç†æ§åˆ¶ç³»ç»Ÿå·¥ç¨‹ä¸­çš„æ•°å­¦å…¬å¼ã€ç¼–ç¨‹æŒ‘æˆ˜å’Œç†è®ºæ¦‚å¿µçš„ä¼˜ç‚¹å’Œç¼ºç‚¹çš„å®šé‡è§è§£ã€‚LLMçš„è¡¨ç°è¾¾åˆ°äº†Bçº§ï¼ˆ82.24%ï¼‰ï¼Œæ¥è¿‘ä½†æœªè¶…è¿‡ç­çº§å¹³å‡æ°´å¹³ï¼ˆ84.99%ï¼‰ï¼Œåœ¨ç»“æ„åŒ–çš„ä»»åŠ¡ä¸­è¡¨ç°æœ€å¥½ï¼Œè€Œåœ¨å¼€æ”¾æ€§çš„é¡¹ç›®ä¸­è¡¨ç°æœ‰é™ã€‚ç ”ç©¶ç»“æœå¼•å‘äº†å…³äºé€‚åº”AIå‘å±•è€Œè°ƒæ•´è¯¾ç¨‹è®¾è®¡è®¨è®ºçš„å…´èµ·ï¼Œä»ç®€å•çš„ç¦æ­¢èµ°å‘å¯¹è¿™äº›å·¥å…·åœ¨å·¥ç¨‹æ•™è‚²ä¸­æœ‰æ€æƒ³åœ°æ•´åˆã€‚å…¶ä»–ææ–™åŒ…æ‹¬æ•™å­¦å¤§çº²ã€è€ƒè¯•è¯•å·ã€è®¾è®¡é¡¹ç›®å’Œç¤ºä¾‹ç­”æ¡ˆå¯åœ¨é¡¹ç›®ç½‘ç«™æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://gradegpt.github.io./">https://gradegpt.github.ioã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05760v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ§åˆ¶ç³»ç»Ÿå·¥ç¨‹è¯¾ç¨‹ä¸­å®Œæˆä¸€ä¸ªå­¦æœŸè¯¾ç¨‹çš„æ€§èƒ½ã€‚é€šè¿‡è¯„ä¼°115ä»½è¯¾ç¨‹ä½œä¸šï¼Œåˆ©ç”¨ChatGPTæ¨¡æ‹ŸçœŸå®å­¦ç”Ÿçš„ä½¿ç”¨æ¨¡å¼è¿›è¡Œâ€œæœ€å°åŠªåŠ›â€åè®®ä¸‹çš„æµ‹è¯•ã€‚ç ”ç©¶é‡‡ç”¨ä¸¥æ ¼çš„æµ‹è¯•æ–¹æ³•ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯„åˆ†é€‰æ‹©é¢˜ã€å¤æ‚çš„Pythonç¼–ç¨‹ä»»åŠ¡å’Œé•¿æ–‡åˆ†æå†™ä½œç­‰å¤šç§é¢˜å‹ã€‚åˆ†æç»“æœå®šé‡åœ°æ¢è®¨äº†äººå·¥æ™ºèƒ½åœ¨å¤„ç†æ•°å­¦å…¬å¼ã€ç¼–ç¨‹æŒ‘æˆ˜å’Œç†è®ºæ¦‚å¿µæ–¹é¢çš„ä¼˜åŠ¿ä¸å±€é™æ€§ã€‚LLMçš„è¡¨ç°è¾¾åˆ°Bçº§æ°´å¹³ï¼ˆ82.24%ï¼‰ï¼Œæ¥è¿‘ä½†æœªè¶…è¿‡ç­çº§å¹³å‡æ°´å¹³ï¼ˆ84.99%ï¼‰ï¼Œåœ¨ç»“æ„åŒ–ä½œä¸šæ–¹é¢è¡¨ç°æœ€ä½³ï¼Œåœ¨å¼€æ”¾æ€§é¡¹ç›®æ–¹é¢å­˜åœ¨æœ€å¤§å±€é™æ€§ã€‚è¿™äº›å‘ç°å¼•å‘äº†å…³äºå¦‚ä½•é€‚åº”äººå·¥æ™ºèƒ½å‘å±•è€Œè°ƒæ•´è¯¾ç¨‹è®¾è®¡çš„é—®é¢˜ï¼Œå€¡å¯¼ä»ç®€å•ç¦æ­¢è½¬å‘åœ¨å·¥ç¨‹æ•™è‚²ä¸­æ·±æ€ç†Ÿè™‘åœ°æ•´åˆè¿™äº›å·¥å…·ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ§åˆ¶ç³»ç»Ÿå·¥ç¨‹è¯¾ç¨‹ä¸­çš„è¡¨ç°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨ChatGPTè¿›è¡Œâ€œæœ€å°åŠªåŠ›â€åè®®ä¸‹çš„æµ‹è¯•ï¼Œæ¨¡æ‹ŸçœŸå®å­¦ç”Ÿçš„ä½¿ç”¨æ¨¡å¼ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨å¤šç§é¢˜å‹è¿›è¡Œä¸¥æ ¼çš„æµ‹è¯•æ–¹æ³•ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯„åˆ†é€‰æ‹©é¢˜ã€Pythonç¼–ç¨‹ä»»åŠ¡å’Œé•¿æ–‡åˆ†æå†™ä½œã€‚</li>
<li>LLMåœ¨å¤„ç†æ•°å­¦å…¬å¼ã€ç¼–ç¨‹æŒ‘æˆ˜å’Œç†è®ºæ¦‚å¿µæ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿å’Œå±€é™æ€§ã€‚</li>
<li>LLMçš„è¡¨ç°æ¥è¿‘ç­çº§å¹³å‡æ°´å¹³ï¼Œåœ¨ç»“æ„åŒ–ä½œä¸šæ–¹é¢è¡¨ç°æœ€ä½³ï¼Œä½†åœ¨å¼€æ”¾æ€§é¡¹ç›®ä¸Šä»æœ‰å±€é™ã€‚</li>
<li>ç ”ç©¶ç»“æœå¼•å‘äº†å…³äºå¦‚ä½•é€‚åº”AIå‘å±•è°ƒæ•´è¯¾ç¨‹è®¾è®¡çš„è®¨è®ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b4e76009813bd95d9bfe3f44fbb1f7c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5291117a9dc11b83e9c4e2603839204.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5a296d260a912a60784614a1e2bbc81f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22c1cab13206bae677ca24ed494c6e63.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Every-FLOP-Counts-Scaling-a-300B-Mixture-of-Experts-LING-LLM-without-Premium-GPUs"><a href="#Every-FLOP-Counts-Scaling-a-300B-Mixture-of-Experts-LING-LLM-without-Premium-GPUs" class="headerlink" title="Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without   Premium GPUs"></a>Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without   Premium GPUs</h2><p><strong>Authors: Ling Team, Binwei Zeng, Chao Huang, Chao Zhang, Changxin Tian, Cong Chen, Dingnan Jin, Feng Yu, Feng Zhu, Feng Yuan, Fakang Wang, Gangshan Wang, Guangyao Zhai, Haitao Zhang, Huizhong Li, Jun Zhou, Jia Liu, Junpeng Fang, Junjie Ou, Jun Hu, Ji Luo, Ji Zhang, Jian Liu, Jian Sha, Jianxue Qian, Jiewei Wu, Junping Zhao, Jianguo Li, Jubao Feng, Jingchao Di, Junming Xu, Jinghua Yao, Kuan Xu, Kewei Du, Longfei Li, Lei Liang, Lu Yu, Li Tang, Lin Ju, Peng Xu, Qing Cui, Song Liu, Shicheng Li, Shun Song, Song Yan, Tengwei Cai, Tianyi Chen, Ting Guo, Ting Huang, Tao Feng, Tao Wu, Wei Wu, Xiaolu Zhang, Xueming Yang, Xin Zhao, Xiaobo Hu, Xin Lin, Yao Zhao, Yilong Wang, Yongzhen Guo, Yuanyuan Wang, Yue Yang, Yang Cao, Yuhao Fu, Yi Xiong, Yanzhe Li, Zhe Li, Zhiqiang Zhang, Ziqi Liu, Zhaoxin Huan, Zujie Wen, Zhenhang Sun, Zhuoxuan Du, Zhengyu He</strong></p>
<p>In this technical report, we tackle the challenges of training large-scale Mixture of Experts (MoE) models, focusing on overcoming cost inefficiency and resource limitations prevalent in such systems. To address these issues, we present two differently sized MoE large language models (LLMs), namely Ling-Lite and Ling-Plus (referred to as â€œBailingâ€ in Chinese, spelled B\v{a}il&#39;ing in Pinyin). Ling-Lite contains 16.8 billion parameters with 2.75 billion activated parameters, while Ling-Plus boasts 290 billion parameters with 28.8 billion activated parameters. Both models exhibit comparable performance to leading industry benchmarks. This report offers actionable insights to improve the efficiency and accessibility of AI development in resource-constrained settings, promoting more scalable and sustainable technologies. Specifically, to reduce training costs for large-scale MoE models, we propose innovative methods for (1) optimization of model architecture and training processes, (2) refinement of training anomaly handling, and (3) enhancement of model evaluation efficiency. Additionally, leveraging high-quality data generated from knowledge graphs, our models demonstrate superior capabilities in tool use compared to other models. Ultimately, our experimental findings demonstrate that a 300B MoE LLM can be effectively trained on lower-performance devices while achieving comparable performance to models of a similar scale, including dense and MoE models. Compared to high-performance devices, utilizing a lower-specification hardware system during the pre-training phase demonstrates significant cost savings, reducing computing costs by approximately 20%. The models can be accessed at <a target="_blank" rel="noopener" href="https://huggingface.co/inclusionAI">https://huggingface.co/inclusionAI</a>. </p>
<blockquote>
<p>æœ¬æŠ€æœ¯æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹è®­ç»ƒå¤§è§„æ¨¡ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰çš„æŒ‘æˆ˜å±•å¼€ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨å¦‚ä½•è§£å†³è¿™ç±»ç³»ç»Ÿä¸­æ™®éå­˜åœ¨çš„æˆæœ¬æ•ˆç‡é—®é¢˜å’Œèµ„æºé™åˆ¶é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸¤æ¬¾ä¸åŒè§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹MoEï¼ˆLLMï¼‰ï¼Œå³Ling-Liteå’ŒLing-Plusï¼ˆä¸­æ–‡ç§°ä¸ºâ€œç™¾çµâ€ï¼‰ã€‚Ling-LiteåŒ…å«168äº¿ä¸ªå‚æ•°å’Œ27.5äº¿ä¸ªæ¿€æ´»å‚æ•°ï¼Œè€ŒLing-Plusåˆ™æ‹¥æœ‰é«˜è¾¾29ä¸‡äº¿ä¸ªå‚æ•°å’Œè¶…è¿‡æ•°åäº¿çš„æ¿€æ´»å‚æ•°ã€‚è¿™ä¸¤æ¬¾æ¨¡å‹åœ¨ä¸šå†…é¡¶å°–åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç›¸å½“çš„æ€§èƒ½ã€‚æœ¬æŠ¥å‘Šæä¾›äº†æ”¹è¿›èµ„æºå—é™ç¯å¢ƒä¸­AIå¼€å‘æ•ˆç‡å’Œå¯åŠæ€§çš„å®é™…å»ºè®®ï¼Œæ¨åŠ¨äº†æ›´åŠ å¯æ‰©å±•å’Œå¯æŒç»­çš„æŠ€æœ¯ã€‚ç‰¹åˆ«æ˜¯ä¸ºäº†å‡å°‘å¤§è§„æ¨¡MoEæ¨¡å‹çš„è®­ç»ƒæˆæœ¬ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ›æ–°æ–¹æ³•æ¥ä¼˜åŒ–ï¼ˆ1ï¼‰æ¨¡å‹æ¶æ„å’Œè®­ç»ƒè¿‡ç¨‹ï¼Œï¼ˆ2ï¼‰æ”¹è¿›è®­ç»ƒå¼‚å¸¸å¤„ç†ï¼Œï¼ˆ3ï¼‰æé«˜æ¨¡å‹è¯„ä¼°æ•ˆç‡ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨çŸ¥è¯†å›¾è°±äº§ç”Ÿçš„é«˜è´¨é‡æ•°æ®ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å±•ç°å‡ºè¶…è¶Šå…¶ä»–æ¨¡å‹çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå¯ä»¥åœ¨è¾ƒä½æ€§èƒ½çš„è®¾å¤‡ä¸Šæœ‰æ•ˆè®­ç»ƒå‡ºè§„æ¨¡ä¸ºåƒäº¿çš„MoEå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¹¶å®ç°äº†ä¸ç›¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼ˆåŒ…æ‹¬å¯†é›†æ¨¡å‹å’ŒMoEæ¨¡å‹ï¼‰ç›¸å½“çš„ä¸šç»©ã€‚ç›¸è¾ƒäºé«˜æ€§èƒ½è®¾å¤‡ï¼Œåœ¨é¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨è¾ƒä½è§„æ ¼çš„ç¡¬ä»¶ç³»ç»Ÿå¯ä»¥æ˜¾è‘—èŠ‚çœæˆæœ¬ï¼Œè®¡ç®—æˆæœ¬é™ä½äº†çº¦ç™¾åˆ†ä¹‹äºŒåã€‚å¯ä»¥é€šè¿‡ç½‘å€é“¾æ¥ <a target="_blank" rel="noopener" href="https://huggingface.co/inclusionAI">https://huggingface.co/inclusionAI</a> äº†è§£è®¿é—®å’Œä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05139v2">PDF</a> 34 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æŠ€æœ¯æŠ¥å‘Šä¸“æ³¨äºåº”å¯¹å¤§è§„æ¨¡æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯è§£å†³è¿™äº›ç³»ç»Ÿä¸­çš„æˆæœ¬æ•ˆç›Šå’Œèµ„æºé™åˆ¶é—®é¢˜ã€‚æŠ¥å‘Šä»‹ç»äº†ä¸¤æ¬¾ä¸åŒè§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå³Ling-Liteå’ŒLing-Plusï¼ˆä¸­æ–‡ç§°ä¸ºâ€œBailingâ€ï¼‰ã€‚Ling-Liteæ‹¥æœ‰16.8äº¿å‚æ•°å’Œ2.75äº¿æ¿€æ´»å‚æ•°ï¼Œè€ŒLing-Plusåˆ™æ‹¥æœ‰é«˜è¾¾290äº¿å‚æ•°å’Œ28.8äº¿æ¿€æ´»å‚æ•°ï¼ŒäºŒè€…æ€§èƒ½ä¸è¡Œä¸šé¢†å…ˆåŸºå‡†ç›¸å½“ã€‚æŠ¥å‘Šæä¾›äº†åœ¨èµ„æºå—é™ç¯å¢ƒä¸­æé«˜äººå·¥æ™ºèƒ½å¼€å‘æ•ˆç‡å’Œå¯è®¿é—®æ€§çš„å®ç”¨è§è§£ï¼Œä¿ƒè¿›äº†æ›´å¯ä¼¸ç¼©å’Œå¯æŒç»­çš„æŠ€æœ¯å‘å±•ã€‚ä¸ºå®ç°å¤§è§„æ¨¡MoEæ¨¡å‹çš„é«˜æ•ˆç‡åŸ¹è®­ï¼ŒæŠ¥å‘Šæå‡ºäº†é’ˆå¯¹æ¨¡å‹æ¶æ„å’ŒåŸ¹è®­è¿‡ç¨‹ä¼˜åŒ–ã€è®­ç»ƒå¼‚å¸¸å¤„ç†ä¼˜åŒ–ä»¥åŠæ¨¡å‹è¯„ä¼°æ•ˆç‡æå‡çš„é©æ–°æ–¹æ³•ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨çŸ¥è¯†å›¾è°±äº§ç”Ÿçš„é«˜è´¨é‡æ•°æ®ï¼Œè¿™äº›æ¨¡å‹åœ¨å·¥å…·ä½¿ç”¨èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚æœ€ç»ˆå®éªŒè¡¨æ˜ï¼Œåœ¨è¾ƒä½æ€§èƒ½çš„è®¾å¤‡ä¸Šè®­ç»ƒä¸€ä¸ªè§„æ¨¡ä¸º300Bçš„MoE LLMæ¨¡å‹ï¼Œå…¶æ€§èƒ½å¯ä¸åŒç±»æ¨¡å‹ç›¸åª²ç¾ï¼ŒåŒ…æ‹¬å¯†é›†æ¨¡å‹å’ŒMoEæ¨¡å‹ã€‚ç›¸è¾ƒäºé«˜æ€§èƒ½è®¾å¤‡ï¼Œåœ¨é¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨è¾ƒä½è§„æ ¼çš„ç¡¬ä»¶ç³»ç»Ÿå¯æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼Œå¤§çº¦èŠ‚çœ20%çš„æˆæœ¬ã€‚æ¨¡å‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://huggingface.co/inclusionAI%E8%AE%BF%E9%97%AE%E3%80%82">https://huggingface.co/inclusionAIè®¿é—®ã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æŠ¥å‘Šä»‹ç»äº†ä¸¤æ¬¾å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼ŒLing-Liteå’ŒLing-Plusï¼Œæ—¨åœ¨è§£å†³å¤§è§„æ¨¡æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹çš„æŒ‘æˆ˜ã€‚</li>
<li>è¿™äº›æ¨¡å‹é€šè¿‡ä¼˜åŒ–æ¨¡å‹æ¶æ„å’ŒåŸ¹è®­è¿‡ç¨‹ã€æ”¹è¿›è®­ç»ƒå¼‚å¸¸å¤„ç†å’Œæå‡æ¨¡å‹è¯„ä¼°æ•ˆç‡ç­‰æ–¹æ³•ï¼Œå®ç°äº†èµ„æºå—é™ç¯å¢ƒä¸‹çš„é«˜æ•ˆè®­ç»ƒã€‚</li>
<li>çŸ¥è¯†å›¾è°±äº§ç”Ÿçš„é«˜è´¨é‡æ•°æ®å¢å¼ºäº†æ¨¡å‹çš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œåœ¨è¾ƒä½æ€§èƒ½çš„è®¾å¤‡ä¸Šè®­ç»ƒçš„è§„æ¨¡ä¸º300Bçš„MoE LLMæ€§èƒ½å“è¶Šï¼Œä¸åŒç±»æ¨¡å‹ç›¸å½“ã€‚</li>
<li>ä¸é«˜æ€§èƒ½è®¾å¤‡ç›¸æ¯”ï¼Œé¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨è¾ƒä½è§„æ ¼ç¡¬ä»¶ç³»ç»Ÿå¯æ˜¾è‘—é™ä½è®¡ç®—æˆæœ¬ï¼ŒèŠ‚çœçº¦20%ã€‚</li>
<li>æŠ¥å‘Šæä¾›çš„å®ç”¨è§è§£æœ‰åŠ©äºæé«˜AIå¼€å‘çš„æ•ˆç‡å’Œå¯è®¿é—®æ€§ï¼Œä¿ƒè¿›æ›´å¯ä¼¸ç¼©å’Œå¯æŒç»­çš„æŠ€æœ¯å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05139">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-9435717a5128b2eb0354fad1625c1e62.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a49ba147c1e911d2f3f476d754bb0cb6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c6e32be5ccb8dd1ce00818faffe8c11a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7599552bbd5fad41d577acad9de99a1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f0f5f8a8929877c0902e51ca9437188.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Leveraging-Large-Language-Models-to-Address-Data-Scarcity-in-Machine-Learning-Applications-in-Graphene-Synthesis"><a href="#Leveraging-Large-Language-Models-to-Address-Data-Scarcity-in-Machine-Learning-Applications-in-Graphene-Synthesis" class="headerlink" title="Leveraging Large Language Models to Address Data Scarcity in Machine   Learning: Applications in Graphene Synthesis"></a>Leveraging Large Language Models to Address Data Scarcity in Machine   Learning: Applications in Graphene Synthesis</h2><p><strong>Authors:Devi Dutta Biswajeet, Sara Kadkhodaei</strong></p>
<p>Machine learning in materials science faces challenges due to limited experimental data, as generating synthesis data is costly and time-consuming, especially with in-house experiments. Mining data from existing literature introduces issues like mixed data quality, inconsistent formats, and variations in reporting experimental parameters, complicating the creation of consistent features for the learning algorithm. Additionally, combining continuous and discrete features can hinder the learning process with limited data. Here, we propose strategies that utilize large language models (LLMs) to enhance machine learning performance on a limited, heterogeneous dataset of graphene chemical vapor deposition synthesis compiled from existing literature. These strategies include prompting modalities for imputing missing data points and leveraging large language model embeddings to encode the complex nomenclature of substrates reported in chemical vapor deposition experiments. The proposed strategies enhance graphene layer classification using a support vector machine (SVM) model, increasing binary classification accuracy from 39% to 65% and ternary accuracy from 52% to 72%. We compare the performance of the SVM and a GPT-4 model, both trained and fine-tuned on the same data. Our results demonstrate that the numerical classifier, when combined with LLM-driven data enhancements, outperforms the standalone LLM predictor, highlighting that in data-scarce scenarios, improving predictive learning with LLM strategies requires more than simple fine-tuning on datasets. Instead, it necessitates sophisticated approaches for data imputation and feature space homogenization to achieve optimal performance. The proposed strategies emphasize data enhancement techniques, offering a broadly applicable framework for improving machine learning performance on scarce, inhomogeneous datasets. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ åœ¨ææ–™ç§‘å­¦é¢†åŸŸé¢ä¸´ç€å®éªŒæ•°æ®æœ‰é™çš„æŒ‘æˆ˜ï¼Œå› ä¸ºç”Ÿæˆåˆæˆæ•°æ®æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å†…éƒ¨å®éªŒä¸­ã€‚ä»ç°æœ‰æ–‡çŒ®ä¸­æŒ–æ˜æ•°æ®å¼•å…¥äº†æ··åˆæ•°æ®è´¨é‡ã€æ ¼å¼ä¸ä¸€è‡´ä»¥åŠæŠ¥å‘Šçš„å®éªŒå‚æ•°å˜åŒ–ç­‰é—®é¢˜ï¼Œä¸ºå­¦ä¹ ç®—æ³•åˆ›å»ºä¸€è‡´çš„ç‰¹å¾å¢åŠ äº†å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œåœ¨æœ‰é™çš„æ•°æ®ä¸‹ï¼Œè¿ç»­ç‰¹å¾å’Œç¦»æ•£ç‰¹å¾çš„ç»„åˆå¯èƒ½ä¼šé˜»ç¢å­¦ä¹ è¿‡ç¨‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºæœºå™¨åœ¨çŸ³å¢¨çƒ¯åŒ–å­¦æ°”ç›¸æ²‰ç§¯åˆæˆæ•°æ®é›†ä¸Šå­¦ä¹ æ€§èƒ½çš„ç­–ç•¥ã€‚è¿™äº›æ•°æ®é›†æ˜¯æœ‰é™çš„ã€æ˜¯ä»ç°æœ‰æ–‡çŒ®ä¸­ç¼–è¯‘çš„å¼‚è´¨æ•°æ®é›†ã€‚è¿™äº›ç­–ç•¥åŒ…æ‹¬æç¤ºæ¨¡å¼ä»¥å¡«è¡¥ç¼ºå¤±æ•°æ®ç‚¹ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åµŒå…¥æ¥ç¼–ç åŒ–å­¦æ°”ç›¸æ²‰ç§¯å®éªŒä¸­æŠ¥é“çš„å¤æ‚åº•ææœ¯è¯­ã€‚æ‰€æå‡ºçš„ç­–ç•¥æé«˜äº†ä½¿ç”¨æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰æ¨¡å‹å¯¹çŸ³å¢¨çƒ¯å±‚åˆ†ç±»çš„æ€§èƒ½ï¼Œå°†äºŒå…ƒåˆ†ç±»ç²¾åº¦ä»39%æé«˜åˆ°65%ï¼Œä¸‰å…ƒç²¾åº¦ä»52%æé«˜åˆ°72%ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†åœ¨åŒä¸€æ•°æ®ä¸Šç»è¿‡è®­ç»ƒå’Œç²¾ç»†è°ƒæ•´çš„SVMå’ŒGPT-4æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå½“æ•°å€¼åˆ†ç±»å™¨ä¸LLMé©±åŠ¨çš„æ•°æ®å¢å¼ºç›¸ç»“åˆæ—¶ï¼Œå…¶æ€§èƒ½ä¼˜äºå•ç‹¬çš„LLMé¢„æµ‹å™¨ï¼Œå¼ºè°ƒåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨LLMç­–ç•¥æé«˜é¢„æµ‹å­¦ä¹ æ€§èƒ½ä¸ä»…éœ€è¦ç®€å•çš„æ•°æ®é›†å¾®è°ƒï¼Œè¿˜éœ€è¦è¿›è¡Œæ•°æ®å¡«å……å’Œç‰¹å¾ç©ºé—´å‡è´¨åŒ–çš„å¤æ‚æ–¹æ³•æ¥å®ç°æœ€ä½³æ€§èƒ½ã€‚æ‰€æå‡ºçš„ç­–ç•¥å¼ºè°ƒäº†æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œä¸ºåœ¨ç¨€ç¼ºã€éå‡åŒ€æ•°æ®é›†ä¸Šæé«˜æœºå™¨å­¦ä¹ æ€§èƒ½æä¾›äº†ä¸€ä¸ªå¹¿æ³›é€‚ç”¨çš„æ¡†æ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04870v2">PDF</a> 20 pages, 10 figures, 4 tables; Supplementary Material with 13   figures and 4 tables</p>
<p><strong>æ‘˜è¦</strong><br>    åœ¨ææ–™ç§‘å­¦ä¸­åº”ç”¨æœºå™¨å­¦ä¹ é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯è·å–å®éªŒæ•°æ®çš„å›°éš¾ä¸é«˜æ˜‚æˆæœ¬ã€‚ä»ç°æœ‰æ–‡çŒ®ä¸­æŒ–æ˜æ•°æ®å­˜åœ¨æ•°æ®è´¨é‡ä¸ä¸€ã€æ ¼å¼ä¸ä¸€è‡´åŠå®éªŒå‚æ•°æŠ¥å‘Šå·®å¼‚ç­‰é—®é¢˜ï¼Œç»™æœºå™¨å­¦ä¹ ç®—æ³•çš„ç‰¹å¾æ„å»ºå¸¦æ¥å›°æ‰°ã€‚åŒæ—¶ï¼Œè¿ç»­å’Œç¦»æ•£ç‰¹å¾çš„ç»“åˆåœ¨æœ‰é™æ•°æ®ä¸­é˜»ç¢å­¦ä¹ è¿›ç¨‹ã€‚æœ¬æ–‡æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºæœºå™¨å­¦ä¹ åœ¨æœ‰é™ä¸”å¼‚è´¨çš„çŸ³å¢¨çƒ¯åŒ–å­¦æ°”ç›¸æ²‰ç§¯åˆæˆæ•°æ®é›†ä¸Šçš„æ€§èƒ½çš„ç­–ç•¥ã€‚ç­–ç•¥åŒ…æ‹¬æç¤ºæ¨¡å¼å¡«è¡¥ç¼ºå¤±æ•°æ®ç‚¹ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åµŒå…¥ç¼–ç åŒ–å­¦æ°”ç›¸æ²‰ç§¯å®éªŒä¸­å¤æ‚åº•ç‰©çš„å‘½åã€‚ç­–ç•¥æé«˜äº†çŸ³å¢¨çƒ¯å±‚åˆ†ç±»çš„æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰æ¨¡å‹çš„æ€§èƒ½ï¼Œå°†äºŒå…ƒåˆ†ç±»ç²¾åº¦ä»39%æé«˜åˆ°65%ï¼Œä¸‰å…ƒç²¾åº¦ä»52%æé«˜åˆ°72%ã€‚æ¯”è¾ƒSVMå’ŒGPT-4æ¨¡å‹åœ¨åŒä¸€æ•°æ®é›†ä¸Šçš„è®­ç»ƒå’Œå¾®è°ƒæ€§èƒ½ï¼Œç»“æœè¡¨æ˜ï¼Œæ•°å€¼åˆ†ç±»å™¨ä¸LLMé©±åŠ¨çš„æ•°æ®å¢å¼ºç›¸ç»“åˆï¼Œä¼˜äºå•ç‹¬çš„LLMé¢„æµ‹å™¨ã€‚è¿™è¡¨æ˜åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨LLMç­–ç•¥æé«˜é¢„æµ‹å­¦ä¹ æ€§èƒ½ä¸ä»…éœ€è¦ç®€å•çš„å¾®è°ƒæ•°æ®é›†ï¼Œè¿˜éœ€è¦å¤æ‚çš„æ•°æ®å¡«å……å’Œç‰¹å¾ç©ºé—´åŒè´¨åŒ–æ–¹æ³•æ¥å®ç°æœ€ä½³æ€§èƒ½ã€‚æ‰€æç­–ç•¥å¼ºè°ƒæ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œä¸ºæ”¹å–„æœ‰é™ã€éå‡åŒ€æ•°æ®é›†çš„æœºå™¨å­¦ä¹ æ€§èƒ½æä¾›äº†å¹¿æ³›é€‚ç”¨çš„æ¡†æ¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ åœ¨ææ–™ç§‘å­¦ä¸­é¢ä¸´æœ‰é™å®éªŒæ•°æ®çš„æŒ‘æˆ˜ï¼Œæ•°æ®è·å–æˆæœ¬é«˜æ˜‚ä¸”è€—æ—¶ã€‚</li>
<li>ä»æ–‡çŒ®æŒ–æ˜æ•°æ®å­˜åœ¨è´¨é‡ã€æ ¼å¼å’Œå®éªŒå‚æ•°æŠ¥å‘Šçš„ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>ç»“åˆè¿ç»­å’Œç¦»æ•£ç‰¹å¾åœ¨æœ‰é™æ•°æ®ä¸­å½±å“æœºå™¨å­¦ä¹ æ€§èƒ½ã€‚</li>
<li>æå‡ºåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºæœºå™¨å­¦ä¹ çš„ç­–ç•¥ï¼ŒåŒ…æ‹¬å¡«è¡¥ç¼ºå¤±æ•°æ®å’Œç¼–ç å¤æ‚å‘½åã€‚</li>
<li>ç­–ç•¥æé«˜äº†çŸ³å¢¨çƒ¯å±‚åˆ†ç±»çš„æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¯”è¾ƒäº†SVMå’ŒGPT-4æ¨¡å‹æ€§èƒ½ï¼Œæ˜¾ç¤ºæ•°å€¼åˆ†ç±»å™¨ä¸LLMç»“åˆä¼˜äºå•çº¯LLMé¢„æµ‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04870">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13977125ac5206750aba1ff69d9c2cd4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5978204f4a8e0c63b407e7d519076bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7c65fec78a620527723498e7216d804.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Re-Imagining-Multimodal-Instruction-Tuning-A-Representation-View"><a href="#Re-Imagining-Multimodal-Instruction-Tuning-A-Representation-View" class="headerlink" title="Re-Imagining Multimodal Instruction Tuning: A Representation View"></a>Re-Imagining Multimodal Instruction Tuning: A Representation View</h2><p><strong>Authors:Yiyang Liu, James Chenhao Liang, Ruixiang Tang, Yugyung Lee, Majid Rabbani, Sohail Dianat, Raghuveer Rao, Lifu Huang, Dongfang Liu, Qifan Wang, Cheng Han</strong></p>
<p>Multimodal instruction tuning has proven to be an effective strategy for achieving zero-shot generalization by fine-tuning pre-trained Large Multimodal Models (LMMs) with instruction-following data. However, as the scale of LMMs continues to grow, fully fine-tuning these models has become highly parameter-intensive. Although Parameter-Efficient Fine-Tuning (PEFT) methods have been introduced to reduce the number of tunable parameters, a significant performance gap remains compared to full fine-tuning. Furthermore, existing PEFT approaches are often highly parameterized, making them difficult to interpret and control. In light of this, we introduce Multimodal Representation Tuning (MRT), a novel approach that focuses on directly editing semantically rich multimodal representations to achieve strong performance and provide intuitive control over LMMs. Empirical results show that our method surpasses current state-of-the-art baselines with significant performance gains (e.g., 1580.40 MME score) while requiring substantially fewer tunable parameters (e.g., 0.03% parameters). Additionally, we conduct experiments on editing instrumental tokens within multimodal representations, demonstrating that direct manipulation of these representations enables simple yet effective control over network behavior. </p>
<blockquote>
<p>å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´å·²è¢«è¯æ˜æ˜¯ä¸€ç§é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä»¥å®ç°é›¶æ ·æœ¬æ³›åŒ–çš„æœ‰æ•ˆç­–ç•¥ã€‚ç„¶è€Œï¼Œéšç€LMMè§„æ¨¡çš„æŒç»­å¢é•¿ï¼Œå®Œå…¨å¾®è°ƒè¿™äº›æ¨¡å‹å·²å˜å¾—é«˜åº¦ä¾èµ–å‚æ•°ã€‚è™½ç„¶å·²å¼•å…¥å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•æ¥å‡å°‘å¯è°ƒæ•´å‚æ•°çš„æ•°é‡ï¼Œä½†ä¸å®Œå…¨å¾®è°ƒç›¸æ¯”ï¼Œæ€§èƒ½ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„PEFTæ–¹æ³•é€šå¸¸é«˜åº¦å‚æ•°åŒ–ï¼Œéš¾ä»¥è§£é‡Šå’Œæ§åˆ¶ã€‚é‰´äºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šæ¨¡æ€è¡¨ç¤ºè°ƒæ•´ï¼ˆMRTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œä¸“æ³¨äºç›´æ¥ç¼–è¾‘è¯­ä¹‰ä¸°å¯Œçš„å¤šæ¨¡æ€è¡¨ç¤ºï¼Œä»¥å®ç°å¼ºå¤§çš„æ€§èƒ½å¹¶ä¸ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹æä¾›ç›´è§‚çš„æ§åˆ¶ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¾¾åˆ°æœ€æ–°åŸºå‡†çº¿çš„åŒæ—¶å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ˆä¾‹å¦‚ï¼ŒMMEå¾—åˆ†æé«˜è‡³1580.40ï¼‰ï¼Œå¹¶ä¸”éœ€è¦çš„å¯è°ƒæ•´å‚æ•°å¤§å¤§å‡å°‘ï¼ˆä¾‹å¦‚ï¼Œä»…ä½¿ç”¨ç™¾åˆ†ä¹‹é›¶ç‚¹é›¶ä¸‰çš„å‚æ•°ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ç¼–è¾‘å¤šæ¨¡æ€è¡¨ç¤ºä¸­çš„ä»ªå™¨ç¬¦å·æ–¹é¢è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ç›´æ¥æ“ä½œè¿™äº›è¡¨ç¤ºå¯ä»¥ç®€å•æœ‰æ•ˆåœ°æ§åˆ¶ç½‘ç»œè¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00723v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´æ˜¯ä¸€ç§æœ‰æ•ˆçš„é›¶æ ·æœ¬æ³›åŒ–ç­–ç•¥ï¼Œå®ƒé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æ¥å®ç°ã€‚ç„¶è€Œï¼Œéšç€LMMsè§„æ¨¡çš„æ‰©å¤§ï¼Œå®Œå…¨å¾®è°ƒè¿™äº›æ¨¡å‹éœ€è¦å¤§é‡çš„å‚æ•°è®¡ç®—ã€‚å°½ç®¡å‚æ•°æ•ˆç‡é«˜çš„å¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•å·²ç»å‡å°‘äº†å¯è°ƒæ•´çš„å‚æ•°æ•°é‡ï¼Œä½†ä»ä¸å…¨é‡å¾®è°ƒå­˜åœ¨æ˜æ˜¾çš„æ€§èƒ½å·®è·ã€‚æœ¬æ–‡æå‡ºäº†å¤šæ¨¡æ€è¡¨ç¤ºè°ƒæ•´ï¼ˆMRTï¼‰æ–¹æ³•ï¼Œé€šè¿‡ç›´æ¥ç¼–è¾‘è¯­ä¹‰ä¸°å¯Œçš„å¤šæ¨¡æ€è¡¨ç¤ºæ¥å®ç°å¯¹LMMsçš„å¼ºå¤§æ€§èƒ½å’Œç›´è§‚æ§åˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ˆä¾‹å¦‚ï¼ŒMMEåˆ†æ•°ä¸º1580.40ï¼‰ï¼ŒåŒæ—¶éœ€è¦çš„å¯è°ƒæ•´å‚æ•°å¤§å¤§å‡å°‘ï¼ˆä¾‹å¦‚ï¼Œä»…å æ€»å‚æ•°çš„0.03%ï¼‰ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¯¹å…³é”®æ¨¡æ€ä»¤ç‰Œçš„æ“ä½œå®éªŒè¯æ˜ï¼Œç›´æ¥æ“çºµè¿™äº›è¡¨ç¤ºèƒ½å¤Ÿå®ç°å¯¹ç½‘ç»œè¡Œä¸ºçš„ç®€å•è€Œæœ‰æ•ˆçš„æ§åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æŒ‡ä»¤è°ƒæ•´æ˜¯ä¸€ç§æœ‰æ•ˆçš„é›¶æ ·æœ¬æ³›åŒ–ç­–ç•¥ã€‚</li>
<li>éšç€å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰è§„æ¨¡çš„æ‰©å¤§ï¼Œå®Œå…¨å¾®è°ƒéœ€è¦å¤§é‡å‚æ•°è®¡ç®—ã€‚</li>
<li>å‚æ•°æ•ˆç‡é«˜çš„å¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•è™½ç„¶å‡å°‘äº†å¯è°ƒæ•´å‚æ•°æ•°é‡ï¼Œä½†ä»å­˜åœ¨æ€§èƒ½å·®è·ã€‚</li>
<li>å¤šæ¨¡æ€è¡¨ç¤ºè°ƒæ•´ï¼ˆMRTï¼‰æ–¹æ³•é€šè¿‡ç›´æ¥ç¼–è¾‘è¯­ä¹‰ä¸°å¯Œçš„å¤šæ¨¡æ€è¡¨ç¤ºå®ç°é«˜æ€§èƒ½å’Œç›´è§‚æ§åˆ¶ã€‚</li>
<li>MRTæ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
<li>MRTæ–¹æ³•éœ€è¦çš„å¯è°ƒæ•´å‚æ•°å¤§å¤§å‡å°‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00723">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d54a7c86a7a58f3b177a1d7b98a9b0e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9331da710b49f23cf7564c1bfbf1c1b0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Transformer-Meets-Twicing-Harnessing-Unattended-Residual-Information"><a href="#Transformer-Meets-Twicing-Harnessing-Unattended-Residual-Information" class="headerlink" title="Transformer Meets Twicing: Harnessing Unattended Residual Information"></a>Transformer Meets Twicing: Harnessing Unattended Residual Information</h2><p><strong>Authors:Laziz Abdullaev, Tan M. Nguyen</strong></p>
<p>Transformer-based deep learning models have achieved state-of-the-art performance across numerous language and vision tasks. While the self-attention mechanism, a core component of transformers, has proven capable of handling complex data patterns, it has been observed that the representational capacity of the attention matrix degrades significantly across transformer layers, thereby hurting its overall performance. In this work, we leverage the connection between self-attention computations and low-pass non-local means (NLM) smoothing filters and propose the Twicing Attention, a novel attention mechanism that uses kernel twicing procedure in nonparametric regression to alleviate the low-pass behavior of associated NLM smoothing with compelling theoretical guarantees and enhanced adversarial robustness. This approach enables the extraction and reuse of meaningful information retained in the residuals following the imperfect smoothing operation at each layer. Our proposed method offers two key advantages over standard self-attention: 1) a provably slower decay of representational capacity and 2) improved robustness and accuracy across various data modalities and tasks. We empirically demonstrate the performance gains of our model over baseline transformers on multiple tasks and benchmarks, including image classification and language modeling, on both clean and corrupted data. </p>
<blockquote>
<p>åŸºäºTransformerçš„æ·±åº¦å­¦ä¹ æ¨¡å‹å·²åœ¨ä¼—å¤šè¯­è¨€å’Œè§†è§‰ä»»åŠ¡ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è™½ç„¶Transformerçš„æ ¸å¿ƒç»„ä»¶è‡ªæ³¨æ„åŠ›æœºåˆ¶å·²è¯æ˜èƒ½å¤Ÿå¤„ç†å¤æ‚çš„æ•°æ®æ¨¡å¼ï¼Œä½†äººä»¬è§‚å¯Ÿåˆ°ï¼Œéšç€Transformerå±‚çš„å¢åŠ ï¼Œæ³¨æ„åŠ›çŸ©é˜µçš„è¡¨ç¤ºèƒ½åŠ›ä¼šæ˜¾è‘—ä¸‹é™ï¼Œä»è€ŒæŸå®³å…¶æ€»ä½“æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸ä½é€šéå±€éƒ¨å‡å€¼ï¼ˆNLMï¼‰å¹³æ»‘æ»¤æ³¢å™¨ä¹‹é—´çš„è”ç³»ï¼Œå¹¶æå‡ºäº†Twicing Attentionï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ³¨æ„åŠ›æœºåˆ¶ã€‚å®ƒé€šè¿‡éå‚æ•°å›å½’ä¸­çš„æ ¸Twicingè¿‡ç¨‹æ¥ç¼“è§£ä¸è‡ªæ³¨æ„åŠ›ç›¸å…³çš„NLMå¹³æ»‘çš„ä½é€šè¡Œä¸ºï¼Œå…·æœ‰å¼•äººæ³¨ç›®çš„ç†è®ºä¿è¯å’Œå¢å¼ºçš„å¯¹æŠ—é²æ£’æ€§ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—èƒ½å¤Ÿåœ¨æ¯å±‚çš„ä¸å®Œç¾å¹³æ»‘æ“ä½œåæå–å’Œå†åˆ©ç”¨æ®‹ç•™çš„æœ‰æ„ä¹‰ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›¸å¯¹äºæ ‡å‡†è‡ªæ³¨æ„åŠ›å…·æœ‰ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿ï¼š1ï¼‰è¡¨ç¤ºèƒ½åŠ›çš„è¡°å‡é€Ÿåº¦è¾ƒæ…¢ï¼›2ï¼‰åœ¨å„ç§æ•°æ®æ¨¡å¼å’Œä»»åŠ¡ä¸­æé«˜äº†é²æ£’æ€§å’Œå‡†ç¡®æ€§ã€‚æˆ‘ä»¬é€šè¿‡å®è¯è¯æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬å›¾åƒåˆ†ç±»å’Œè¯­è¨€å»ºæ¨¡ï¼Œä»¥åŠå¯¹å¹²å‡€å’ŒæŸåæ•°æ®çš„æµ‹è¯•ï¼‰ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†åŸºçº¿Transformerã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00687v2">PDF</a> 10 pages in the main text. Published at ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Transformeræ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†è¯­è¨€ä¸è§†è§‰ä»»åŠ¡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ä»£è¡¨æ€§å®¹é‡åœ¨Transformerå±‚é—´æ˜¾è‘—é€€åŒ–çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†Twicing Attentionæœºåˆ¶ï¼Œè¯¥æœºåˆ¶åˆ©ç”¨è‡ªæ³¨æ„åŠ›è®¡ç®—ä¸ä½é€šéå±€éƒ¨å‡å€¼ï¼ˆNLMï¼‰å¹³æ»‘æ»¤æ³¢å™¨ä¹‹é—´çš„è”ç³»ï¼Œå¹¶é‡‡ç”¨éå‚æ•°å›å½’ä¸­çš„æ ¸æ‰­æ›²è¿‡ç¨‹æ¥æ”¹å–„NLMå¹³æ»‘çš„ä½é€šè¡Œä¸ºã€‚æ–°æ–¹æ³•èƒ½å¤Ÿåœ¨æ¯å±‚ä¸å®Œç¾çš„å¹³æ»‘æ“ä½œåæå–å¹¶é‡æ–°åˆ©ç”¨æ®‹ç•™çš„æœ‰æ„ä¹‰ä¿¡æ¯ï¼Œç›¸è¾ƒäºæ ‡å‡†è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå…·æœ‰ä»£è¡¨æ€§å®¹é‡è¡°å‡è¾ƒæ…¢ã€è·¨ä¸åŒæ•°æ®æ¨¡æ€å’Œä»»åŠ¡æé«˜ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ç­‰ä¸¤å¤§ä¼˜åŠ¿ã€‚ä½œè€…åœ¨å¤šä¸ªä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸Šå®è¯äº†æ¨¡å‹åœ¨æ¸…æ´å’ŒæŸåæ•°æ®ä¸Šçš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformeræ¨¡å‹åœ¨è®¸å¤šè¯­è¨€ä¸è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ä»£è¡¨æ€§å®¹é‡åœ¨å±‚é—´å­˜åœ¨æ˜¾è‘—é€€åŒ–é—®é¢˜ã€‚</li>
<li>è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒé—®é¢˜åœ¨äºå…¶å¤„ç†å¤æ‚æ•°æ®æ¨¡å¼æ—¶çš„èƒ½åŠ›ä¸‹é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶â€”â€”Twicing Attentionï¼Œå…¶ç»“åˆéå‚æ•°å›å½’ä¸­çš„æ ¸æ‰­æ›²è¿‡ç¨‹ï¼Œæ”¹å–„äº†ä¸NLMå¹³æ»‘ç›¸å…³çš„ä½é€šè¡Œä¸ºã€‚</li>
<li>Twicing Attentionå…·æœ‰ä¸¤å¤§ä¼˜åŠ¿ï¼šä»£è¡¨æ€§å®¹é‡è¡°å‡è¾ƒæ…¢ã€è·¨ä¸åŒæ•°æ®æ¨¡æ€å’Œä»»åŠ¡æé«˜ç¨³å¥æ€§å’Œå‡†ç¡®æ€§ã€‚</li>
<li>æ–°æ–¹æ³•èƒ½å¤Ÿåœ¨æ¯å±‚ä¸å®Œç¾çš„å¹³æ»‘æ“ä½œåæå–å¹¶é‡æ–°åˆ©ç”¨æ®‹ç•™çš„æœ‰æ„ä¹‰ä¿¡æ¯ã€‚</li>
<li>å®è¯ç ”ç©¶è¡¨æ˜ï¼Œæ–°æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡å’ŒåŸºå‡†æµ‹è¯•ä¸Šï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»å’Œè¯­è¨€å»ºæ¨¡ï¼Œä»¥åŠåœ¨æ¸…æ´å’ŒæŸåæ•°æ®ä¸Šçš„æ€§èƒ½å‡æœ‰æ˜¾è‘—æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-116ddb03d1d22eb2a36c08869c201367.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-33dbab91cb18b20834a539c9c39cbb6f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9da29fd57acbcffe975cd8e5f69f5a2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75b545dbd23498a3ef47b47130e8b6f2.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="M2-omni-Advancing-Omni-MLLM-for-Comprehensive-Modality-Support-with-Competitive-Performance"><a href="#M2-omni-Advancing-Omni-MLLM-for-Comprehensive-Modality-Support-with-Competitive-Performance" class="headerlink" title="M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with   Competitive Performance"></a>M2-omni: Advancing Omni-MLLM for Comprehensive Modality Support with   Competitive Performance</h2><p><strong>Authors:Qingpei Guo, Kaiyou Song, Zipeng Feng, Ziping Ma, Qinglong Zhang, Sirui Gao, Xuzheng Yu, Yunxiao Sun, Tai-Wei Chang, Jingdong Chen, Ming Yang, Jun Zhou</strong></p>
<p>We present M2-omni, a cutting-edge, open-source omni-MLLM that achieves competitive performance to GPT-4o. M2-omni employs a unified multimodal sequence modeling framework, which empowers Large Language Models(LLMs) to acquire comprehensive cross-modal understanding and generation capabilities. Specifically, M2-omni can process arbitrary combinations of audio, video, image, and text modalities as input, generating multimodal sequences interleaving with audio, image, or text outputs, thereby enabling an advanced and interactive real-time experience. The training of such an omni-MLLM is challenged by significant disparities in data quantity and convergence rates across modalities. To address these challenges, we propose a step balance strategy during pre-training to handle the quantity disparities in modality-specific data. Additionally, a dynamically adaptive balance strategy is introduced during the instruction tuning stage to synchronize the modality-wise training progress, ensuring optimal convergence. Notably, we prioritize preserving strong performance on pure text tasks to maintain the robustness of M2-omniâ€™s language understanding capability throughout the training process. To our best knowledge, M2-omni is currently a very competitive open-source model to GPT-4o, characterized by its comprehensive modality and task support, as well as its exceptional performance. We expect M2-omni will advance the development of omni-MLLMs, thus facilitating future research in this domain. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†M2-omniï¼Œè¿™æ˜¯ä¸€æ¬¾å…ˆè¿›çš„å¼€æºé€šç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå…¶æ€§èƒ½ä¸GPT-4oç›¸å½“ã€‚M2-omnié‡‡ç”¨ç»Ÿä¸€çš„å¤šæ¨¡æ€åºåˆ—å»ºæ¨¡æ¡†æ¶ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å…¨é¢çš„è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒM2-omniå¯ä»¥å¤„ç†ä»»æ„ç»„åˆçš„éŸ³é¢‘ã€è§†é¢‘ã€å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€ä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆäº¤æ›¿è¾“å‡ºéŸ³é¢‘ã€å›¾åƒæˆ–æ–‡æœ¬çš„å¤šåª’ä½“åºåˆ—ï¼Œä»è€Œå®ç°å…ˆè¿›ä¸”äº¤äº’å¼çš„å®æ—¶ä½“éªŒã€‚è®­ç»ƒè¿™æ ·çš„é€šç”¨å¤šæ¨¡æ€LLMé¢ä¸´çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ï¼Œä¸åŒæ¨¡æ€çš„æ•°æ®é‡å’Œæ”¶æ•›ç‡å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­æå‡ºäº†ä¸€ç§æ­¥éª¤å¹³è¡¡ç­–ç•¥ï¼Œä»¥å¤„ç†ç‰¹å®šæ¨¡æ€çš„æ•°æ®é‡å·®å¼‚ã€‚æ­¤å¤–ï¼Œåœ¨æŒ‡ä»¤è°ƒæ•´é˜¶æ®µï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€è‡ªé€‚åº”å¹³è¡¡ç­–ç•¥ï¼Œä»¥åŒæ­¥ä¸åŒæ¨¡æ€çš„è®­ç»ƒè¿›åº¦ï¼Œç¡®ä¿æœ€ä½³æ”¶æ•›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä¼˜å…ˆè€ƒè™‘åœ¨çº¯æ–‡æœ¬ä»»åŠ¡ä¸Šä¿æŒå“è¶Šæ€§èƒ½ï¼Œä»¥åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒM2-omniçš„è¯­è¨€ç†è§£èƒ½åŠ›çš„ç¨³å¥æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒM2-omniç›®å‰æ˜¯ä¸€ä¸ªæå…·ç«äº‰åŠ›çš„å¼€æºæ¨¡å‹ï¼Œä¸GPT-4oç›¸æ¯”ï¼Œå®ƒä»¥å…¨é¢çš„æ¨¡æ€å’Œä»»åŠ¡æ”¯æŒä»¥åŠå“è¶Šçš„æ€§èƒ½ä¸ºç‰¹ç‚¹ã€‚æˆ‘ä»¬æœŸæœ›M2-omniå°†æ¨åŠ¨é€šç”¨å¤šæ¨¡æ€LLMçš„å‘å±•ï¼Œä»è€Œæ¨åŠ¨è¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.18778v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    M2-omniæ˜¯ä¸€æ¬¾å…ˆè¿›çš„å¼€æºå¤šåŠŸèƒ½å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ï¼Œå¯å¤„ç†éŸ³é¢‘ã€è§†é¢‘ã€å›¾åƒå’Œæ–‡æœ¬ç­‰å¤šç§æ¨¡æ€çš„è¾“å…¥ï¼Œå¹¶ç”Ÿæˆå¤šåª’ä½“è¾“å‡ºï¼Œå®ç°å…ˆè¿›ä¸”äº¤äº’å¼å®æ—¶ä½“éªŒã€‚ä¸ºè§£å†³ä¸åŒæ¨¡æ€æ•°æ®é‡å’Œæ”¶æ•›ç‡çš„å·®å¼‚å¸¦æ¥çš„è®­ç»ƒæŒ‘æˆ˜ï¼Œç ”ç©¶äººå‘˜åœ¨é¢„è®­ç»ƒé˜¶æ®µé‡‡å–äº†æ­¥éª¤å¹³è¡¡ç­–ç•¥ï¼Œå¹¶åœ¨æŒ‡ä»¤è°ƒæ•´é˜¶æ®µå¼•å…¥äº†åŠ¨æ€è‡ªé€‚åº”å¹³è¡¡ç­–ç•¥ï¼Œç¡®ä¿æœ€ä¼˜æ”¶æ•›ã€‚M2-omniç›®å‰æ˜¯ä¸€ä¸ªç«äº‰åŠ›å¼ºçš„å¼€æºæ¨¡å‹ï¼Œä»¥ç»¼åˆæ¨¡æ€å’Œä»»åŠ¡æ”¯æŒä»¥åŠå“è¶Šæ€§èƒ½ä¸ºç‰¹ç‚¹ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>M2-omniæ˜¯ä¸€æ¬¾å…ˆè¿›çš„å¼€æºå¤šåŠŸèƒ½å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå…·å¤‡è·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>å¯ä»¥å¤„ç†å¤šç§æ¨¡æ€çš„è¾“å…¥ï¼ŒåŒ…æ‹¬éŸ³é¢‘ã€è§†é¢‘ã€å›¾åƒå’Œæ–‡æœ¬ï¼Œç”Ÿæˆå¤šåª’ä½“è¾“å‡ºã€‚</li>
<li>M2-omniå®ç°äº†å…ˆè¿›ä¸”äº¤äº’å¼å®æ—¶ä½“éªŒã€‚</li>
<li>åœ¨é¢„è®­ç»ƒé˜¶æ®µé‡‡å–äº†æ­¥éª¤å¹³è¡¡ç­–ç•¥ï¼Œè§£å†³ä¸åŒæ¨¡æ€æ•°æ®é‡å’Œæ”¶æ•›ç‡çš„å·®å¼‚å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>åœ¨æŒ‡ä»¤è°ƒæ•´é˜¶æ®µå¼•å…¥äº†åŠ¨æ€è‡ªé€‚åº”å¹³è¡¡ç­–ç•¥ï¼Œç¡®ä¿æœ€ä¼˜æ”¶æ•›ã€‚</li>
<li>M2-omniåœ¨çº¯æ–‡æœ¬ä»»åŠ¡ä¸Šä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œç¡®ä¿äº†è¯­è¨€ç†è§£èƒ½åŠ›çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.18778">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9f17b71567bac50aa3a056beb9f22da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03a405de1ce51b7b197c5275237bfa9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f1d74d27e2d236fb414c246230858e65.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88f031d34f0a1a8124fd8656ff379404.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7564fb67c64d3b1baedecb5c9cdec9af.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-536557dd7e4e6f8850f7384f7de041ee.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  MedAgentsBench Benchmarking Thinking Models and Agent Frameworks for   Complex Medical Reasoning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b2ab6663be184b5ac86e372483bd9e50.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  MedAgentsBench Benchmarking Thinking Models and Agent Frameworks for   Complex Medical Reasoning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">17665k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
