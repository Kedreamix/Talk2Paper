<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  Linguistic Knowledge Transfer Learning for Speech Enhancement">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-716b0953ac1258a41d1c0633df8c58dd.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    32 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-12-æ›´æ–°"><a href="#2025-03-12-æ›´æ–°" class="headerlink" title="2025-03-12 æ›´æ–°"></a>2025-03-12 æ›´æ–°</h1><h2 id="Linguistic-Knowledge-Transfer-Learning-for-Speech-Enhancement"><a href="#Linguistic-Knowledge-Transfer-Learning-for-Speech-Enhancement" class="headerlink" title="Linguistic Knowledge Transfer Learning for Speech Enhancement"></a>Linguistic Knowledge Transfer Learning for Speech Enhancement</h2><p><strong>Authors:Kuo-Hsuan Hung, Xugang Lu, Szu-Wei Fu, Huan-Hsin Tseng, Hsin-Yi Lin, Chii-Wann Lin, Yu Tsao</strong></p>
<p>Linguistic knowledge plays a crucial role in spoken language comprehension. It provides essential semantic and syntactic context for speech perception in noisy environments. However, most speech enhancement (SE) methods predominantly rely on acoustic features to learn the mapping relationship between noisy and clean speech, with limited exploration of linguistic integration. While text-informed SE approaches have been investigated, they often require explicit speech-text alignment or externally provided textual data, constraining their practicality in real-world scenarios. Additionally, using text as input poses challenges in aligning linguistic and acoustic representations due to their inherent differences. In this study, we propose the Cross-Modality Knowledge Transfer (CMKT) learning framework, which leverages pre-trained large language models (LLMs) to infuse linguistic knowledge into SE models without requiring text input or LLMs during inference. Furthermore, we introduce a misalignment strategy to improve knowledge transfer. This strategy applies controlled temporal shifts, encouraging the model to learn more robust representations. Experimental evaluations demonstrate that CMKT consistently outperforms baseline models across various SE architectures and LLM embeddings, highlighting its adaptability to different configurations. Additionally, results on Mandarin and English datasets confirm its effectiveness across diverse linguistic conditions, further validating its robustness. Moreover, CMKT remains effective even in scenarios without textual data, underscoring its practicality for real-world applications. By bridging the gap between linguistic and acoustic modalities, CMKT offers a scalable and innovative solution for integrating linguistic knowledge into SE models, leading to substantial improvements in both intelligibility and enhancement performance. </p>
<blockquote>
<p>è¯­è¨€çŸ¥è¯†åœ¨å£è¯­ç†è§£ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚å®ƒä¸ºå˜ˆæ‚ç¯å¢ƒä¸­çš„è¯­éŸ³æ„ŸçŸ¥æä¾›äº†å¿…è¦çš„è¯­ä¹‰å’Œå¥æ³•èƒŒæ™¯ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ–¹æ³•ä¸»è¦ä¾èµ–äºå£°å­¦ç‰¹å¾æ¥å­¦ä¹ å™ªå£°å’Œæ¸…æ´è¯­éŸ³ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œå¯¹è¯­è¨€æ•´åˆçš„æ¢ç´¢æœ‰é™ã€‚è™½ç„¶å·²æœ‰æ–‡æœ¬ä¿¡æ¯å¼•å¯¼çš„SEæ–¹æ³•çš„ç ”ç©¶ï¼Œä½†å®ƒä»¬é€šå¸¸éœ€è¦æ˜ç¡®çš„è¯­éŸ³æ–‡æœ¬å¯¹é½æˆ–å¤–éƒ¨æä¾›çš„æ–‡æœ¬æ•°æ®ï¼Œè¿™åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨å®è·µå—åˆ°é™åˆ¶ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ–‡æœ¬ä½œä¸ºè¾“å…¥ä¼šå¸¦æ¥å¯¹é½è¯­è¨€å’Œå£°å­¦è¡¨ç¤ºçš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬ä¹‹é—´å­˜åœ¨å›ºæœ‰çš„å·®å¼‚ã€‚æœ¬ç ”ç©¶æå‡ºäº†è·¨æ¨¡æ€çŸ¥è¯†è½¬ç§»ï¼ˆCMKTï¼‰å­¦ä¹ æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†è¯­è¨€çŸ¥è¯†æ³¨å…¥SEæ¨¡å‹ï¼Œè€Œä¸éœ€è¦åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨æ–‡æœ¬è¾“å…¥æˆ–LLMã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é”™ä½ç­–ç•¥æ¥æ”¹å–„çŸ¥è¯†è½¬ç§»ã€‚è¯¥ç­–ç•¥åº”ç”¨å—æ§çš„æ—¶é—´åç§»ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ æ›´ç¨³å¥çš„è¡¨ç¤ºã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œåœ¨å„ç§SEæ¶æ„å’ŒLLMåµŒå…¥ä¸­ï¼ŒCMKTå§‹ç»ˆä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œçªæ˜¾å…¶åœ¨ä¸åŒé…ç½®ä¸­çš„é€‚åº”æ€§ã€‚æ­¤å¤–ï¼Œåœ¨æ™®é€šè¯å’Œè‹±è¯­æ•°æ®é›†ä¸Šçš„ç»“æœè¯æ˜äº†å…¶åœ¨å¤šç§è¯­è¨€ç¯å¢ƒä¸‹çš„æœ‰æ•ˆæ€§ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶ç¨³å¥æ€§ã€‚å³ä½¿åœ¨æ²¡æœ‰æ–‡æœ¬æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒCMKTä»ç„¶æœ‰æ•ˆï¼Œè¿™å¼ºè°ƒäº†å…¶åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚é€šè¿‡å¼¥åˆè¯­è¨€å’Œå£°å­¦æ¨¡æ€ä¹‹é—´çš„å·®è·ï¼ŒCMKTä¸ºå°†è¯­è¨€çŸ¥è¯†èå…¥SEæ¨¡å‹æä¾›äº†å¯æ‰©å±•å’Œåˆ›æ–°è§£å†³æ–¹æ¡ˆï¼Œåœ¨å¯æ‡‚åº¦å’Œå¢å¼ºæ€§èƒ½æ–¹é¢éƒ½å®ç°äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07078v1">PDF</a> 11 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æŒ‡å‡ºè¯­è¨€å­¦çŸ¥è¯†åœ¨å£è¯­ç†è§£ä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä»‹ç»äº†è·¨æ¨¡æ€çŸ¥è¯†è½¬ç§»ï¼ˆCMKTï¼‰å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†è¯­è¨€å­¦çŸ¥è¯†æ³¨å…¥è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ¨¡å‹ï¼Œæ— éœ€æ–‡æœ¬è¾“å…¥æˆ–LLMåœ¨æ¨ç†æœŸé—´ä½¿ç”¨ã€‚é€šè¿‡æ§åˆ¶æ—¶é—´åç§»çš„é”™ä½ç­–ç•¥æ”¹è¿›çŸ¥è¯†è½¬ç§»ï¼Œæ¨¡å‹å­¦ä¹ æ›´ç¨³å¥çš„è¡¨ç¤ºã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒCMKTåœ¨å„ç§SEæ¶æ„å’ŒLLMåµŒå…¥ä¸­å§‹ç»ˆä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œä¸”åœ¨æ±‰è¯­å’Œè‹±è¯­æ•°æ®é›†ä¸Šçš„ç»“æœéªŒè¯äº†å…¶è·¨è¯­è¨€æ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œå³ä½¿åœ¨æ— æ–‡æœ¬æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒCMKTä¾ç„¶æœ‰æ•ˆï¼Œå…·æœ‰å®é™…åº”ç”¨çš„å®ç”¨æ€§ã€‚è¯¥æ¡†æ¶å¡«è¡¥äº†è¯­éŸ³ä¸å£°å­¦æ¨¡æ€ä¹‹é—´çš„é¸¿æ²Ÿï¼Œä¸ºå°†è¯­è¨€å­¦çŸ¥è¯†èå…¥SEæ¨¡å‹æä¾›äº†å¯æ‰©å±•å’Œåˆ›æ–°è§£å†³æ–¹æ¡ˆï¼Œå¤§å¤§æé«˜äº†å¯ç†è§£æ€§å’Œå¢å¼ºæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€å­¦çŸ¥è¯†åœ¨å£è¯­ç†è§£ä¸­èµ·å…³é”®ä½œç”¨ï¼Œæä¾›è¯­ä¹‰å’Œå¥æ³•ä¸Šä¸‹æ–‡ï¼Œå°¤å…¶åœ¨å˜ˆæ‚ç¯å¢ƒä¸­ã€‚</li>
<li>ç°æœ‰çš„è¯­éŸ³å¢å¼ºæ–¹æ³•ä¸»è¦ä¾èµ–äºå£°å­¦ç‰¹å¾ï¼Œå¯¹è¯­è¨€å­¦çŸ¥è¯†çš„æ•´åˆæœ‰é™ã€‚</li>
<li>è·¨æ¨¡æ€çŸ¥è¯†è½¬ç§»ï¼ˆCMKTï¼‰å­¦ä¹ æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ³¨å…¥è¯­è¨€å­¦çŸ¥è¯†åˆ°SEæ¨¡å‹ã€‚</li>
<li>CMKTä¸éœ€è¦æ–‡æœ¬è¾“å…¥æˆ–LLMåœ¨æ¨ç†æœŸé—´ä½¿ç”¨ï¼Œæé«˜äº†å…¶å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>é€šè¿‡æ§åˆ¶æ—¶é—´åç§»çš„é”™ä½ç­–ç•¥æ”¹è¿›çŸ¥è¯†è½¬ç§»ï¼Œæé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚</li>
<li>CMKTåœ¨å„ç§SEæ¶æ„å’ŒLLMåµŒå…¥ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”åœ¨æ±‰è¯­å’Œè‹±è¯­æ•°æ®é›†ä¸Šçš„ç»“æœéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07078">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fac88b81f939010ffd9a4195bc860f4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97061d165a00a722e56b459f2ce997ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10808f7a560ceed88c4ad7b38f0f9edc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-268b845c115b34467db527a5da1f6a9b.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Automatic-Speech-Recognition-for-Non-Native-English-Accuracy-and-Disfluency-Handling"><a href="#Automatic-Speech-Recognition-for-Non-Native-English-Accuracy-and-Disfluency-Handling" class="headerlink" title="Automatic Speech Recognition for Non-Native English: Accuracy and   Disfluency Handling"></a>Automatic Speech Recognition for Non-Native English: Accuracy and   Disfluency Handling</h2><p><strong>Authors:Michael McGuire</strong></p>
<p>Automatic speech recognition (ASR) has been an essential component of computer assisted language learning (CALL) and computer assisted language testing (CALT) for many years. As this technology continues to develop rapidly, it is important to evaluate the accuracy of current ASR systems for language learning applications. This study assesses five cutting-edge ASR systemsâ€™ recognition of non-native accented English speech using recordings from the L2-ARCTIC corpus, featuring speakers from six different L1 backgrounds (Arabic, Chinese, Hindi, Korean, Spanish, and Vietnamese), in the form of both read and spontaneous speech. The read speech consisted of 2,400 single sentence recordings from 24 speakers, while the spontaneous speech included narrative recordings from 22 speakers. Results showed that for read speech, Whisper and AssemblyAI achieved the best accuracy with mean Match Error Rates (MER) of 0.054 and 0.056 respectively, approaching human-level accuracy. For spontaneous speech, RevAI performed best with a mean MER of 0.063. The study also examined how each system handled disfluencies such as filler words, repetitions, and revisions, finding significant variation in performance across systems and disfluency types. While processing speed varied considerably between systems, longer processing times did not necessarily correlate with better accuracy. By detailing the performance of several of the most recent, widely-available ASR systems on non-native English speech, this study aims to help language instructors and researchers understand the strengths and weaknesses of each system and identify which may be suitable for specific use cases. </p>
<blockquote>
<p>å¤šå¹´æ¥ï¼Œè¯­éŸ³è¯†åˆ«æŠ€æœ¯ï¼ˆASRï¼‰ä¸€ç›´æ˜¯è®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ï¼ˆCALLï¼‰å’Œè®¡ç®—æœºè¾…åŠ©è¯­è¨€æµ‹è¯•ï¼ˆCALTï¼‰çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚éšç€è¿™é¡¹æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œè¯„ä¼°å½“å‰è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨è¯­è¨€å­¦ä¹ åº”ç”¨ä¸­çš„å‡†ç¡®æ€§è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†äº”ç§å‰æ²¿çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå¯¹éæ¯è¯­è‹±è¯­å£éŸ³çš„è¯†åˆ«èƒ½åŠ›ï¼Œä½¿ç”¨äº†L2-ARCTICè¯­æ–™åº“ä¸­çš„å½•éŸ³ï¼Œè¯¥è¯­æ–™åº“åŒ…å«æ¥è‡ªå…­ä¸ªä¸åŒæ¯è¯­èƒŒæ™¯ï¼ˆé˜¿æ‹‰ä¼¯è¯­ã€ä¸­æ–‡ã€å°åœ°è¯­ã€éŸ©è¯­ã€è¥¿ç­ç‰™è¯­å’Œè¶Šå—è¯­ï¼‰çš„æ¼”è®²è€…ï¼ŒåŒ…æ‹¬æœ—è¯»å’Œè‡ªå‘è¨€è¯­ä¸¤ç§å½¢å¼ã€‚æœ—è¯»éƒ¨åˆ†åŒ…å«æ¥è‡ª24åæ¼”è®²è€…çš„2400ä¸ªå•å¥å½•éŸ³ï¼Œè€Œè‡ªå‘è¨€è¯­åˆ™åŒ…å«æ¥è‡ª22åæ¼”è®²è€…çš„å™è¿°å½•éŸ³ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨æœ—è¯»è¯­éŸ³æ–¹é¢ï¼ŒWhisperå’ŒAssemblyAIå–å¾—äº†æœ€ä½³å‡†ç¡®æ€§ï¼Œå¹³å‡åŒ¹é…é”™è¯¯ç‡ï¼ˆMERï¼‰åˆ†åˆ«ä¸º0.054å’Œ0.056ï¼Œæ¥è¿‘äººç±»æ°´å¹³çš„å‡†ç¡®æ€§ã€‚åœ¨è‡ªå‘è¨€è¯­æ–¹é¢ï¼ŒRevAIè¡¨ç°æœ€ä½³ï¼Œå¹³å‡MERä¸º0.063ã€‚è¯¥ç ”ç©¶è¿˜æ¢è®¨äº†æ¯ä¸ªç³»ç»Ÿå¦‚ä½•å¤„ç†ä¸æµç•…æ€§ï¼Œå¦‚å¡«å……è¯ã€é‡å¤å’Œä¿®æ­£ç­‰ï¼Œå‘ç°ä¸åŒç³»ç»Ÿå’Œä¸åŒä¸æµç•…æ€§ç±»å‹ä¹‹é—´çš„æ€§èƒ½å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚è™½ç„¶ç³»ç»Ÿä¹‹é—´çš„å¤„ç†é€Ÿåº¦å·®å¼‚å¾ˆå¤§ï¼Œä½†å¤„ç†æ—¶é—´çš„é•¿çŸ­å¹¶ä¸ä¸€å®šä¸å‡†ç¡®æ€§ç›¸å…³ã€‚æœ¬ç ”ç©¶é€šè¿‡è¯¦ç»†ä»‹ç»å‡ ç§æœ€æ–°ä¸”å¹¿æ³›å¯ç”¨çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿåœ¨éæ¯è¯­è‹±è¯­è¯­éŸ³ä¸Šçš„è¡¨ç°ï¼Œæ—¨åœ¨å¸®åŠ©è¯­è¨€æ•™å¸ˆå’Œç ”ç©¶è€…äº†è§£æ¯ä¸ªç³»ç»Ÿçš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶ç¡®å®šå“ªäº›ç³»ç»Ÿé€‚ç”¨äºç‰¹å®šçš„ç”¨ä¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06924v1">PDF</a> 33 pages, 10 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†äº”ç§å…ˆè¿›çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå¯¹éæ¯è¯­è‹±è¯­å£éŸ³çš„è¯†åˆ«èƒ½åŠ›ï¼Œä½¿ç”¨äº†L2-ARCTICè¯­æ–™åº“ä¸­çš„å½•éŸ³ï¼ŒåŒ…æ‹¬å…­ç§ä¸åŒæ¯è¯­èƒŒæ™¯çš„å‘éŸ³äººï¼Œæ—¢æœ‰æœ—è¯»ä¹Ÿæœ‰å³å…´æ¼”è®²ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨æœ—è¯»æƒ…å†µä¸‹ï¼ŒWhisperå’ŒAssemblyAIè¡¨ç°æœ€ä½³ï¼Œå¹³å‡åŒ¹é…é”™è¯¯ç‡æ¥è¿‘äººç±»æ°´å¹³ã€‚å³å…´æ¼”è®²æƒ…å†µä¸‹ï¼ŒRevAIè¡¨ç°æœ€ä½³ã€‚åŒæ—¶ï¼Œå„ç³»ç»Ÿå¯¹å‘éŸ³ä¸æµç•…çš„å¤„ç†èƒ½åŠ›ä¹Ÿæœ‰æ˜¾è‘—å·®å¼‚ã€‚æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯å¸®åŠ©è¯­è¨€æ•™å¸ˆå’Œç ”ç©¶è€…äº†è§£å„ç³»ç»Ÿçš„ä¼˜ç¼ºç‚¹ï¼Œä»¥ç¡®å®šå“ªäº›ç³»ç»Ÿé€‚ç”¨äºç‰¹å®šç”¨ä¾‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ˜¯è®¡ç®—æœºè¾…åŠ©è¯­è¨€å­¦ä¹ ï¼ˆCALLï¼‰å’Œè¯­è¨€æµ‹è¯•ï¼ˆCALTï¼‰çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†äº”ç§å…ˆè¿›ASRç³»ç»Ÿå¯¹éæ¯è¯­è‹±è¯­å£éŸ³çš„è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>åœ¨æœ—è¯»æƒ…å†µä¸‹ï¼ŒWhisperå’ŒAssemblyAIè¡¨ç°æœ€ä½³ï¼Œå¹³å‡åŒ¹é…é”™è¯¯ç‡æ¥è¿‘äººç±»æ°´å¹³ã€‚</li>
<li>åœ¨å³å…´æ¼”è®²æƒ…å†µä¸‹ï¼ŒRevAIè¡¨ç°æœ€ä½³ã€‚</li>
<li>å„ç³»ç»Ÿå¤„ç†å‘éŸ³ä¸æµç•…çš„èƒ½åŠ›å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>ç³»ç»Ÿçš„å¤„ç†é€Ÿåº¦ä¸è¯†åˆ«å‡†ç¡®æ€§ä¸ä¸€å®šç›¸å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06924">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cefe3e831ed0adb9e12323c5bac56d76.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Speech-Audio-Generation-from-dynamic-MRI-via-a-Knowledge-Enhanced-Conditional-Variational-Autoencoder"><a href="#Speech-Audio-Generation-from-dynamic-MRI-via-a-Knowledge-Enhanced-Conditional-Variational-Autoencoder" class="headerlink" title="Speech Audio Generation from dynamic MRI via a Knowledge Enhanced   Conditional Variational Autoencoder"></a>Speech Audio Generation from dynamic MRI via a Knowledge Enhanced   Conditional Variational Autoencoder</h2><p><strong>Authors:Yaxuan Li, Han Jiang, Yifei Ma, Shihua Qin, Fangxu Xing</strong></p>
<p>Dynamic Magnetic Resonance Imaging (MRI) of the vocal tract has become an increasingly adopted imaging modality for speech motor studies. Beyond image signals, systematic data loss, noise pollution, and audio file corruption can occur due to the unpredictability of the MRI acquisition environment. In such cases, generating audio from images is critical for data recovery in both clinical and research applications. However, this remains challenging due to hardware constraints, acoustic interference, and data corruption. Existing solutions, such as denoising and multi-stage synthesis methods, face limitations in audio fidelity and generalizability. To address these challenges, we propose a Knowledge Enhanced Conditional Variational Autoencoder (KE-CVAE), a novel two-step â€œknowledge enhancement + variational inferenceâ€ framework for generating speech audio signals from cine dynamic MRI sequences. This approach introduces two key innovations: (1) integration of unlabeled MRI data for knowledge enhancement, and (2) a variational inference architecture to improve generative modeling capacity. To the best of our knowledge, this is one of the first attempts at synthesizing speech audio directly from dynamic MRI video sequences. The proposed method was trained and evaluated on an open-source dynamic vocal tract MRI dataset recorded during speech. Experimental results demonstrate its effectiveness in generating natural speech waveforms while addressing MRI-specific acoustic challenges, outperforming conventional deep learning-based synthesis approaches. </p>
<blockquote>
<p>åŠ¨æ€ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å·²ç»æˆä¸ºè¯­éŸ³è¿åŠ¨ç ”ç©¶è¶Šæ¥è¶Šå—æ¬¢è¿çš„æˆåƒæ–¹å¼ã€‚é™¤äº†å›¾åƒä¿¡å·å¤–ï¼Œç”±äºMRIé‡‡é›†ç¯å¢ƒçš„ä¸ç¡®å®šæ€§ï¼Œè¿˜å¯èƒ½å‘ç”Ÿç³»ç»Ÿæ€§æ•°æ®ä¸¢å¤±ã€å™ªå£°æ±¡æŸ“å’ŒéŸ³é¢‘æ–‡ä»¶æŸåã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»å›¾åƒç”ŸæˆéŸ³é¢‘å¯¹äºä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­çš„æ•°æ®æ¢å¤è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºç¡¬ä»¶é™åˆ¶ã€å£°éŸ³å¹²æ‰°å’Œæ•°æ®æŸåï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰è§£å†³æ–¹æ¡ˆï¼Œå¦‚é™å™ªå’Œå¤šé˜¶æ®µåˆæˆæ–¹æ³•ï¼Œåœ¨éŸ³é¢‘ä¿çœŸåº¦å’Œé€šç”¨æ€§æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§çŸ¥è¯†å¢å¼ºæ¡ä»¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆKE-CVAEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„ä¸¤æ­¥â€œçŸ¥è¯†å¢å¼º+å˜åˆ†æ¨æ–­â€æ¡†æ¶ï¼Œç”¨äºä»ç”µå½±åŠ¨æ€MRIåºåˆ—ç”Ÿæˆè¯­éŸ³éŸ³é¢‘ä¿¡å·ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰å°†æ— æ ‡ç­¾MRIæ•°æ®ç”¨äºçŸ¥è¯†å¢å¼ºï¼›ï¼ˆ2ï¼‰é‡‡ç”¨å˜åˆ†æ¨æ–­æ¶æ„ï¼Œæé«˜ç”Ÿæˆæ¨¡å‹çš„å®¹é‡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡ç›´æ¥ä»åŠ¨æ€MRIè§†é¢‘åºåˆ—ä¸­åˆæˆè¯­éŸ³éŸ³é¢‘çš„å°è¯•ã€‚æ‰€æå‡ºçš„æ–¹æ³•æ˜¯åœ¨å…¬å¼€çš„åŠ¨æ€è¯­éŸ³é“MRIæ•°æ®é›†ä¸Šè®­ç»ƒå’Œè¯„ä¼°çš„ï¼Œè¯¥æ•°æ®é›†æ˜¯åœ¨è¯´è¯æœŸé—´è®°å½•çš„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆè‡ªç„¶è¯­éŸ³æ³¢å½¢çš„åŒæ—¶ï¼Œè§£å†³äº†MRIç‰¹æœ‰çš„å£°å­¦æŒ‘æˆ˜ï¼Œä¼˜äºä¼ ç»Ÿçš„åŸºäºæ·±åº¦å­¦ä¹ çš„åˆæˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06588v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŠ¨æ€ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨è¯­éŸ³è¿åŠ¨ç ”ç©¶ä¸­è¢«è¶Šæ¥è¶Šå¹¿æ³›åœ°é‡‡ç”¨ã€‚ç„¶è€Œï¼ŒMRIé‡‡é›†ç¯å¢ƒä¸­å­˜åœ¨æ•°æ®ä¸¢å¤±ã€å™ªå£°æ±¡æŸ“å’ŒéŸ³é¢‘æ–‡ä»¶æŸåç­‰é—®é¢˜ã€‚ä»å›¾åƒç”ŸæˆéŸ³é¢‘å¯¹äºä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­çš„æ•°æ®æ¢å¤è‡³å…³é‡è¦ã€‚é’ˆå¯¹ç¡¬ä»¶çº¦æŸã€å£°å­¦å¹²æ‰°å’Œæ•°æ®æŸåç­‰æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºçŸ¥è¯†å¢å¼ºçš„æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆKE-CVAEï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¯ä¸€ç§ä¸¤é˜¶æ®µâ€œçŸ¥è¯†å¢å¼º+å˜åˆ†æ¨æ–­â€æ¡†æ¶ï¼Œå¯ä»ç”µå½±åŠ¨æ€MRIåºåˆ—ç”Ÿæˆè¯­éŸ³éŸ³é¢‘ä¿¡å·ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šä¸€æ˜¯å°†æœªæ ‡è®°çš„MRIæ•°æ®è¿›è¡ŒçŸ¥è¯†å¢å¼ºï¼ŒäºŒæ˜¯é‡‡ç”¨å˜åˆ†æ¨æ–­æ¶æ„æé«˜ç”Ÿæˆæ¨¡å‹çš„å®¹é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ç”Ÿæˆè‡ªç„¶è¯­éŸ³æ³¢å½¢çš„åŒæ—¶è§£å†³äº†MRIç‰¹æœ‰çš„å£°å­¦æŒ‘æˆ˜ï¼Œä¼˜äºä¼ ç»Ÿçš„åŸºäºæ·±åº¦å­¦ä¹ çš„åˆæˆæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŠ¨æ€ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å·²å¹¿æ³›åº”ç”¨äºè¯­éŸ³è¿åŠ¨ç ”ç©¶ã€‚</li>
<li>MRIé‡‡é›†ç¯å¢ƒä¸­å­˜åœ¨æ•°æ®ä¸¢å¤±ã€å™ªå£°æ±¡æŸ“å’ŒéŸ³é¢‘æ–‡ä»¶æŸåç­‰é—®é¢˜ã€‚</li>
<li>ä»å›¾åƒç”ŸæˆéŸ³é¢‘å¯¹äºæ•°æ®æ¢å¤åœ¨ä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆå¦‚é™å™ªå’Œå¤šé˜¶æ®µåˆæˆæ–¹æ³•å­˜åœ¨éŸ³é¢‘ä¿çœŸåº¦å’Œé€šç”¨æ€§æ–¹é¢çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºçŸ¥è¯†å¢å¼ºçš„æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆKE-CVAEï¼‰çš„æ–°æ–¹æ³•ï¼Œæ˜¯ä¸€ç§ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨äºä»ç”µå½±åŠ¨æ€MRIåºåˆ—ç”Ÿæˆè¯­éŸ³éŸ³é¢‘ä¿¡å·ã€‚</li>
<li>è¯¥æ–¹æ³•å¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šçŸ¥è¯†å¢å¼ºå’Œå˜åˆ†æ¨æ–­æ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bb8001f256a8073046a24c38a95520bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-045c00a85bd038f7e074831a56add294.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="ProSE-Diffusion-Priors-for-Speech-Enhancement"><a href="#ProSE-Diffusion-Priors-for-Speech-Enhancement" class="headerlink" title="ProSE: Diffusion Priors for Speech Enhancement"></a>ProSE: Diffusion Priors for Speech Enhancement</h2><p><strong>Authors:Sonal Kumar, Sreyan Ghosh, Utkarsh Tyagi, Anton Jeran Ratnarajah, Chandra Kiran Reddy Evuru, Ramani Duraiswami, Dinesh Manocha</strong></p>
<p>Speech enhancement (SE) is the foundational task of enhancing the clarity and quality of speech in the presence of non-stationary additive noise. While deterministic deep learning models have been commonly employed for SE, recent research indicates that generative models, such as denoising diffusion probabilistic models (DDPMs), have shown promise. However, unlike speech generation, SE has a strong constraint in generating results in accordance with the underlying ground-truth signal. Additionally, for a wide variety of applications, SE systems need to be employed in real-time, and traditional diffusion models (DMs) requiring many iterations of a large model during inference are inefficient. To address these issues, we propose ProSE (diffusion-based Priors for SE), a novel methodology based on an alternative framework for applying diffusion models to SE. Specifically, we first apply DDPMs to generate priors in a latent space due to their powerful distribution mapping capabilities. The priors are then integrated into a transformer-based regression model for SE. The priors guide the regression model in the enhancement process. Since the diffusion process is applied to a compact latent space, the diffusion model takes fewer iterations than the traditional DM to obtain accurate estimations. Additionally, using a regression model for SE avoids the distortion issue caused by misaligned details generated by DMs. Our experiments show that ProSE achieves state-of-the-art performance on benchmark datasets with fewer computational costs. </p>
<blockquote>
<p>è¯­éŸ³å¢å¼ºï¼ˆSEï¼‰æ˜¯åœ¨å­˜åœ¨éå¹³ç¨³é™„åŠ å™ªå£°çš„æƒ…å†µä¸‹æé«˜è¯­éŸ³æ¸…æ™°åº¦å’Œè´¨é‡çš„åŸºç¡€ä»»åŠ¡ã€‚è™½ç„¶ç¡®å®šæ€§æ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸è¢«ç”¨äºSEï¼Œä½†æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¦‚å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰ç­‰ç”Ÿæˆæ¨¡å‹æ˜¾ç¤ºå‡ºè‰¯å¥½çš„å‰æ™¯ã€‚ç„¶è€Œï¼Œä¸è¯­éŸ³ç”Ÿæˆä¸åŒï¼ŒSEåœ¨ç”Ÿæˆä¸åº•å±‚çœŸå®ä¿¡å·ç›¸ç¬¦çš„ç»“æœæ–¹é¢å…·æœ‰å¼ºçƒˆçº¦æŸã€‚æ­¤å¤–ï¼Œå¯¹äºå¤šç§åº”ç”¨ï¼ŒSEç³»ç»Ÿéœ€è¦åœ¨å®æ—¶ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œè€Œä¼ ç»Ÿçš„æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­éœ€è¦å¤šæ¬¡è¿­ä»£å¤§å‹æ¨¡å‹ï¼Œå› æ­¤æ•ˆç‡ä¸é«˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„SEå…ˆéªŒï¼ˆProSEï¼‰ã€‚è¿™æ˜¯ä¸€ç§æ–°çš„æ–¹æ³•ï¼ŒåŸºäºæ›¿ä»£æ¡†æ¶å°†æ‰©æ•£æ¨¡å‹åº”ç”¨äºSEã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå°†DDPMåº”ç”¨äºæ½œåœ¨ç©ºé—´ç”Ÿæˆå…ˆéªŒï¼Œç”±äºå…¶å¼ºå¤§çš„åˆ†å¸ƒæ˜ å°„èƒ½åŠ›ã€‚è¿™äº›å…ˆéªŒç„¶åè¢«é›†æˆåˆ°åŸºäºå˜å‹å™¨çš„SEå›å½’æ¨¡å‹ä¸­ã€‚å…ˆéªŒå€¼åœ¨å¢å¼ºè¿‡ç¨‹ä¸­æŒ‡å¯¼å›å½’æ¨¡å‹ã€‚ç”±äºæ‰©æ•£è¿‡ç¨‹åº”ç”¨äºç´§å‡‘çš„æ½œåœ¨ç©ºé—´ï¼Œå› æ­¤æ‰©æ•£æ¨¡å‹è·å¾—å‡†ç¡®ä¼°è®¡æ‰€éœ€çš„è¿­ä»£æ¬¡æ•°å°‘äºä¼ ç»Ÿçš„DMã€‚æ­¤å¤–ï¼Œä½¿ç”¨å›å½’æ¨¡å‹è¿›è¡ŒSEé¿å…äº†ç”±DMç”Ÿæˆçš„ç»†èŠ‚é”™ä½æ‰€å¯¼è‡´çš„å¤±çœŸé—®é¢˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨åŸºå‡†æ•°æ®é›†ä¸Šï¼ŒProSEè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”è®¡ç®—æˆæœ¬æ›´ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06375v1">PDF</a> Accepted at NAACL 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¯­éŸ³å¢å¼ºä»»åŠ¡çš„ä¸€ç§æ–°å‹æ–¹æ³•â€”â€”ProSEã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ‰©æ•£æ¨¡å‹ä¸å›å½’æ¨¡å‹ï¼Œé€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ç”Ÿæˆå…ˆéªŒä¿¡æ¯ï¼Œä»¥æŒ‡å¯¼å›å½’æ¨¡å‹è¿›è¡Œè¯­éŸ³å¢å¼ºè¿‡ç¨‹ã€‚ç›¸è¾ƒäºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼ŒProSEæé«˜äº†è®¡ç®—æ•ˆç‡å¹¶é™ä½äº†å¤±çœŸé£é™©ï¼Œå®ç°äº†åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ProSEæ˜¯ä¸€ç§ç»“åˆæ‰©æ•£æ¨¡å‹å’Œå›å½’æ¨¡å‹çš„æ–°å‹è¯­éŸ³å¢å¼ºæ–¹æ³•ã€‚</li>
<li>ProSEåˆ©ç”¨æ‰©æ•£æ¨¡å‹åœ¨æ½œåœ¨ç©ºé—´ç”Ÿæˆå…ˆéªŒä¿¡æ¯ï¼Œä»¥æŒ‡å¯¼å›å½’æ¨¡å‹è¿›è¡Œè¯­éŸ³å¢å¼ºã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åº”ç”¨äºç´§å‡‘çš„æ½œåœ¨ç©ºé—´ï¼Œå‡å°‘äº†è¿­ä»£æ¬¡æ•°ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚</li>
<li>å›å½’æ¨¡å‹çš„ä½¿ç”¨é¿å…äº†ç”±ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹äº§ç”Ÿçš„ç»†èŠ‚ä¸åŒ¹é…æ‰€å¯¼è‡´çš„å¤±çœŸé—®é¢˜ã€‚</li>
<li>ProSEåœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>ProSEç‰¹åˆ«é€‚ç”¨äºéœ€è¦å®æ—¶å¤„ç†çš„è¯­éŸ³å¢å¼ºåº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1da1b80bbb9f5c4605675c825a00a6d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4d7e50f1a349e9db09d05ccddc9622d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1afdf909a7923585610a492be107880.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9b54786f39d0591f5142aa50c68502b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Adaptive-Audio-Visual-Speech-Recognition-via-Matryoshka-Based-Multimodal-LLMs"><a href="#Adaptive-Audio-Visual-Speech-Recognition-via-Matryoshka-Based-Multimodal-LLMs" class="headerlink" title="Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal   LLMs"></a>Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal   LLMs</h2><p><strong>Authors:Umberto Cappellazzo, Minsu Kim, Stavros Petridis</strong></p>
<p>Audio-Visual Speech Recognition (AVSR) leverages both audio and visual modalities to enhance speech recognition robustness, particularly in noisy environments. Recent advancements in Large Language Models (LLMs) have demonstrated their effectiveness in speech recognition, including AVSR. However, due to the significant length of speech representations, direct integration with LLMs imposes substantial computational costs. Prior approaches address this by compressing speech representations before feeding them into LLMs. However, higher compression ratios often lead to performance degradation, necessitating a trade-off between computational efficiency and recognition accuracy. To address this challenge, we propose Llama-MTSK, the first Matryoshka-based Multimodal LLM for AVSR, which enables flexible adaptation of the audio-visual token allocation based on specific computational constraints while preserving high performance. Our approach, inspired by Matryoshka Representation Learning, encodes audio-visual representations at multiple granularities within a single model, eliminating the need to train separate models for different compression levels. Moreover, to efficiently fine-tune the LLM, we introduce three LoRA-based Matryoshka strategies using global and scale-specific LoRA modules. Extensive evaluations on the two largest AVSR datasets demonstrate that Llama-MTSK achieves state-of-the-art results, matching or surpassing models trained independently at fixed compression levels. </p>
<blockquote>
<p>éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰åˆ©ç”¨éŸ³é¢‘å’Œè§†è§‰ä¸¤ç§æ¨¡å¼æ¥æé«˜è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å™ªå£°ç¯å¢ƒä¸­ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥åœ¨è¯­éŸ³è¯†åˆ«é¢†åŸŸï¼ŒåŒ…æ‹¬AVSRï¼Œéƒ½è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œç”±äºè¯­éŸ³è¡¨ç¤ºçš„é•¿åº¦è¾ƒå¤§ï¼Œç›´æ¥å°†å…¶ä¸LLMé›†æˆä¼šäº§ç”Ÿå·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚ä¹‹å‰çš„æ–¹æ³•é€šè¿‡å‹ç¼©è¯­éŸ³è¡¨ç¤ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç„¶åå†å°†å…¶è¾“å…¥åˆ°LLMä¸­ã€‚ç„¶è€Œï¼Œè¾ƒé«˜çš„å‹ç¼©ç‡å¾€å¾€ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œéœ€è¦åœ¨è®¡ç®—æ•ˆç‡å’Œè¯†åˆ«ç²¾åº¦ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Llama-MTSKï¼Œè¿™æ˜¯åŸºäºMatryoshkaçš„é¦–ä¸ªå¤šæ¨¡æ€AVSR LLMã€‚å®ƒèƒ½å¤Ÿåœ¨ç‰¹å®šçš„è®¡ç®—çº¦æŸä¸‹çµæ´»åœ°é€‚åº”éŸ³é¢‘è§†è§‰ä»¤ç‰Œåˆ†é…ï¼ŒåŒæ—¶ä¿æŒé«˜æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•å—åˆ°Matryoshkaè¡¨ç¤ºå­¦ä¹ çš„å¯å‘ï¼Œåœ¨ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸­ç¼–ç å¤šä¸ªç²’åº¦çš„éŸ³é¢‘è§†è§‰è¡¨ç¤ºï¼Œæ— éœ€ä¸ºä¸åŒçš„å‹ç¼©çº§åˆ«è®­ç»ƒå•ç‹¬çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†æœ‰æ•ˆåœ°å¾®è°ƒLLMï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸‰ç§åŸºäºLoRAçš„Matryoshkaç­–ç•¥ï¼Œä½¿ç”¨å…¨å±€å’Œè§„æ¨¡ç‰¹å®šçš„LoRAæ¨¡å—ã€‚åœ¨ä¸¤å¤§AVSRæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒLlama-MTSKè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼ŒåŒ¹é…æˆ–è¶…è¶Šäº†åœ¨å›ºå®šå‹ç¼©çº§åˆ«ä¸‹ç‹¬ç«‹è®­ç»ƒçš„æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06362v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰åˆ©ç”¨éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€æé«˜è¯­éŸ³è¯†åˆ«åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§ã€‚æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯­éŸ³è¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§å¾—åˆ°è¯æ˜ï¼Œä½†ç›´æ¥é›†æˆåˆ°AVSRä¸­å­˜åœ¨è®¡ç®—æˆæœ¬é«˜çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†åŸºäºMatryoshkaçš„Llama-MTSKå¤šæ¨¡æ€LLMæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ç‰¹å®šçš„è®¡ç®—çº¦æŸä¸‹çµæ´»é€‚åº”éŸ³é¢‘è§†è§‰ä»¤ç‰Œåˆ†é…å¹¶ä¿æŒé«˜æ€§èƒ½ã€‚è¯¥æ¨¡å‹ä½¿ç”¨Matryoshkaè¡¨ç¤ºå­¦ä¹ çš„æ–¹æ³•ï¼Œåœ¨å•ä¸ªæ¨¡å‹å†…éƒ¨ä»¥ä¸åŒç²’åº¦ç¼–ç éŸ³é¢‘è§†è§‰è¡¨ç¤ºå½¢å¼ï¼Œé¿å…ä¸ºä¸åŒå‹ç¼©çº§åˆ«è®­ç»ƒå•ç‹¬æ¨¡å‹çš„éœ€è¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥ä¸‰ç§åŸºäºLoRAçš„Matryoshkaç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜æ•ˆç‡ã€‚åœ¨ä¸¤å¤§AVSRæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒLlama-MTSKè¾¾åˆ°äº†æœ€æ–°çš„ç»“æœï¼Œèƒ½å¤ŸåŒ¹é…æˆ–è¶…è¶Šåœ¨å›ºå®šå‹ç¼©çº§åˆ«ä¸‹ç‹¬ç«‹è®­ç»ƒçš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘è§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰ç»“åˆäº†éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€ï¼Œå¢å¼ºäº†åœ¨å™ªå£°ç¯å¢ƒä¸‹çš„è¯­éŸ³è¯†åˆ«ç¨³å¥æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è¯­éŸ³è¯†åˆ«ä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§ï¼Œä½†ç›´æ¥é›†æˆå­˜åœ¨è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚</li>
<li>Llama-MTSKæ¨¡å‹æ˜¯é¦–ä¸ªé’ˆå¯¹AVSRçš„Matryoshkaå¤šæ¨¡æ€LLMï¼Œèƒ½å¤Ÿåœ¨è®¡ç®—çº¦æŸä¸‹çµæ´»é€‚åº”éŸ³é¢‘è§†è§‰ä»¤ç‰Œåˆ†é…å¹¶ä¿æŒé«˜æ€§èƒ½ã€‚</li>
<li>Llama-MTSKä½¿ç”¨Matryoshkaè¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œä»¥ä¸åŒç²’åº¦ç¼–ç éŸ³é¢‘è§†è§‰è¡¨ç¤ºå½¢å¼ï¼Œé¿å…äº†è®­ç»ƒä¸åŒå‹ç¼©çº§åˆ«ç‹¬ç«‹æ¨¡å‹çš„å¿…è¦æ€§ã€‚</li>
<li>é€šè¿‡å¼•å…¥ä¸‰ç§åŸºäºLoRAçš„Matryoshkaç­–ç•¥è¿›è¡Œå¾®è°ƒï¼Œä»¥æé«˜æ¨¡å‹çš„æ•ˆç‡ã€‚</li>
<li>åœ¨ä¸¤å¤§AVSRæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒLlama-MTSKè¾¾åˆ°äº†æœ€æ–°çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06362">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d9e7cdc9c2818e8f465fd56e6a1c0cef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5236771d5fbec0aaa5095049de393890.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6878213dce25d3dbf912eacbcb829f88.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78bd3e5a6b8e13815c5947f8c0afac2f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Text-Speech-Language-Models-with-Improved-Cross-Modal-Transfer-by-Aligning-Abstraction-Levels"><a href="#Text-Speech-Language-Models-with-Improved-Cross-Modal-Transfer-by-Aligning-Abstraction-Levels" class="headerlink" title="Text-Speech Language Models with Improved Cross-Modal Transfer by   Aligning Abstraction Levels"></a>Text-Speech Language Models with Improved Cross-Modal Transfer by   Aligning Abstraction Levels</h2><p><strong>Authors:Santiago Cuervo, Adel Moumen, Yanis Labrak, Sameer Khurana, Antoine Laurent, Mickael Rouvier, Ricard Marxer</strong></p>
<p>Text-Speech Language Models (TSLMs) â€“ language models trained to jointly process and generate text and speech â€“ aim to enable cross-modal knowledge transfer to overcome the scaling limitations of unimodal speech LMs. The predominant approach to TSLM training expands the vocabulary of a pre-trained text LM by appending new embeddings and linear projections for speech, followed by fine-tuning on speech data. We hypothesize that this method limits cross-modal transfer by neglecting feature compositionality, preventing text-learned functions from being fully leveraged at appropriate abstraction levels. To address this, we propose augmenting vocabulary expansion with modules that better align abstraction levels across layers. Our models, \textsc{SmolTolk}, rival or surpass state-of-the-art TSLMs trained with orders of magnitude more compute. Representation analyses and improved multimodal performance suggest our method enhances cross-modal transfer. </p>
<blockquote>
<p>æ–‡æœ¬-è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆTSLMsï¼‰â€”â€”æ—¨åœ¨è”åˆå¤„ç†å’Œç”Ÿæˆæ–‡æœ¬å’Œè¯­éŸ³çš„è¯­è¨€æ¨¡å‹â€”â€”æ—¨åœ¨å®ç°è·¨æ¨¡æ€çŸ¥è¯†è¿ç§»ï¼Œä»¥å…‹æœå•æ¨¡æ€è¯­éŸ³LMçš„è§„æ¨¡åŒ–é™åˆ¶ã€‚TSLMè®­ç»ƒçš„ä¸»è¦æ–¹æ³•æ˜¯é€šè¿‡åœ¨é¢„è®­ç»ƒçš„æ–‡æœ¬LMçš„è¯æ±‡è¡¨ä¸Šå¢åŠ æ–°çš„è¯­éŸ³åµŒå…¥å’Œçº¿æ€§æŠ•å½±æ¥æ‰©å±•å…¶è¯æ±‡é‡ï¼Œç„¶åé€šè¿‡è¯­éŸ³æ•°æ®å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å‡è®¾è¿™ç§æ–¹æ³•å¿½ç•¥äº†ç‰¹å¾ç»„åˆæ€§ï¼Œé˜»ç¢äº†æ–‡æœ¬å­¦ä¹ åŠŸèƒ½åœ¨é€‚å½“æŠ½è±¡å±‚æ¬¡ä¸Šçš„å……åˆ†åˆ©ç”¨ï¼Œä»è€Œé™åˆ¶äº†è·¨æ¨¡æ€è¿ç§»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡å¢åŠ æ¨¡å—æ¥æ›´å¥½åœ°å¯¹é½å„å±‚ä¹‹é—´çš„æŠ½è±¡å±‚æ¬¡ï¼Œä»¥æ‰©å……è¯æ±‡é‡ã€‚æˆ‘ä»¬çš„æ¨¡å‹SmolTolkä¸æœ€å…ˆè¿›çš„TSLMç›¸å½“æˆ–è¶…è¶Šï¼Œè¿™äº›TSLMçš„è®¡ç®—è®­ç»ƒé‡è¦å¤§å¾—å¤šã€‚è¡¨å¾åˆ†æå’Œæ”¹è¿›çš„å¤šæ¨¡æ€æ€§èƒ½è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¢å¼ºäº†è·¨æ¨¡æ€è¿ç§»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06211v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬-è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆTSLMï¼‰æ—¨åœ¨é€šè¿‡è·¨æ¨¡æ€çŸ¥è¯†è¿ç§»ï¼Œå…‹æœå•ä¸€æ¨¡æ€è¯­éŸ³è¯­è¨€æ¨¡å‹çš„è§„æ¨¡é™åˆ¶ã€‚ç°æœ‰ä¸»æµè®­ç»ƒæ–¹æ³•æ˜¯æ‰©å……é¢„è®­ç»ƒæ–‡æœ¬è¯­è¨€æ¨¡å‹çš„è¯æ±‡ï¼Œå¢åŠ è¯­éŸ³åµŒå…¥å’Œçº¿æ€§æŠ•å½±ï¼Œå†é€šè¿‡è¯­éŸ³æ•°æ®è¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¿½ç•¥äº†ç‰¹å¾ç»„åˆæ€§ï¼Œå¯¼è‡´æ–‡æœ¬å­¦ä¹ åŠŸèƒ½æ— æ³•åœ¨é€‚å½“æŠ½è±¡å±‚é¢è¢«å……åˆ†åˆ©ç”¨ã€‚ä¸ºæ”¹å–„è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€šè¿‡å¢åŠ æ¨¡å—å¯¹é½å„å±‚æŠ½è±¡å±‚é¢æ¥æ‰©å……è¯æ±‡çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¨¡å‹â€œSmolTolkâ€ä¸è®­ç»ƒæ—¶è®¡ç®—é‡è¿œè¶…å…¶çš„å…ˆè¿›TSLMç›¸æ¯”è¡¨ç°ç›¸å½“æˆ–æ›´å¥½ã€‚è¡¨å¾åˆ†æå’Œæ”¹è¿›çš„å¤šæ¨¡æ€æ€§èƒ½è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¢å¼ºäº†è·¨æ¨¡æ€è¿ç§»èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TSLMæ—¨åœ¨é€šè¿‡è·¨æ¨¡æ€çŸ¥è¯†è¿ç§»å…‹æœå•ä¸€æ¨¡æ€è¯­éŸ³è¯­è¨€æ¨¡å‹çš„è§„æ¨¡é™åˆ¶ã€‚</li>
<li>å½“å‰ä¸»æµTSLMè®­ç»ƒæ–¹æ³•æ˜¯æ‰©å……æ–‡æœ¬LMè¯æ±‡å¹¶å¢åŠ è¯­éŸ³åµŒå…¥å’Œçº¿æ€§æŠ•å½±ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥äº†ç‰¹å¾ç»„åˆæ€§ï¼Œå½±å“æ–‡æœ¬å­¦ä¹ åŠŸèƒ½åœ¨é€‚å½“æŠ½è±¡å±‚é¢çš„åˆ©ç”¨ã€‚</li>
<li>ä¸ºæ”¹å–„æ­¤é—®é¢˜ï¼Œæå‡ºäº†ç»“åˆæ‰©å……è¯æ±‡ä¸å¯¹é½å„å±‚æŠ½è±¡å±‚é¢çš„æ–¹æ³•ã€‚</li>
<li>æ¨¡å‹â€œSmolTolkâ€è¡¨ç°ä¼˜å¼‚ï¼Œä¸è®¡ç®—é‡æ›´å¤§çš„å…ˆè¿›TSLMç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>è¡¨å¾åˆ†ææ˜¾ç¤ºæ–°æ–¹æ³•å¢å¼ºäº†è·¨æ¨¡æ€è¿ç§»èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06211">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-98a79d78cdd4eab9d9e74be3f2c98313.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df56725f2517791905af1f0f1c3d052e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d388f95ddb5d4fce0195e361446240f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-716b0953ac1258a41d1c0633df8c58dd.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Bimodal-Connection-Attention-Fusion-for-Speech-Emotion-Recognition"><a href="#Bimodal-Connection-Attention-Fusion-for-Speech-Emotion-Recognition" class="headerlink" title="Bimodal Connection Attention Fusion for Speech Emotion Recognition"></a>Bimodal Connection Attention Fusion for Speech Emotion Recognition</h2><p><strong>Authors:Jiachen Luo, Huy Phan, Lin Wang, Joshua D. Reiss</strong></p>
<p>Multi-modal emotion recognition is challenging due to the difficulty of extracting features that capture subtle emotional differences. Understanding multi-modal interactions and connections is key to building effective bimodal speech emotion recognition systems. In this work, we propose Bimodal Connection Attention Fusion (BCAF) method, which includes three main modules: the interactive connection network, the bimodal attention network, and the correlative attention network. The interactive connection network uses an encoder-decoder architecture to model modality connections between audio and text while leveraging modality-specific features. The bimodal attention network enhances semantic complementation and exploits intra- and inter-modal interactions. The correlative attention network reduces cross-modal noise and captures correlations between audio and text. Experiments on the MELD and IEMOCAP datasets demonstrate that the proposed BCAF method outperforms existing state-of-the-art baselines. </p>
<blockquote>
<p>å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºæå–èƒ½å¤Ÿæ•æ‰å¾®å¦™æƒ…æ„Ÿå·®å¼‚çš„ç‰¹å¾éå¸¸å›°éš¾ã€‚ç†è§£å¤šæ¨¡æ€çš„äº¤äº’å’Œè¿æ¥æ˜¯æ„å»ºæœ‰æ•ˆçš„åŒæ¨¡æ€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿçš„å…³é”®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŒæ¨¡æ€è¿æ¥æ³¨æ„åŠ›èåˆï¼ˆBCAFï¼‰æ–¹æ³•ï¼Œä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šäº¤äº’è¿æ¥ç½‘ç»œã€åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå’Œç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œã€‚äº¤äº’è¿æ¥ç½‘ç»œä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„æ¥å»ºæ¨¡éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€è¿æ¥ï¼ŒåŒæ—¶åˆ©ç”¨æ¨¡æ€ç‰¹å®šç‰¹å¾ã€‚åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå¢å¼ºäº†è¯­ä¹‰è¡¥å……æ€§ï¼Œå¹¶æ¢ç´¢äº†æ¨¡æ€å†…éƒ¨å’Œæ¨¡æ€ä¹‹é—´çš„äº¤äº’ä½œç”¨ã€‚ç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œé™ä½äº†è·¨æ¨¡æ€å™ªå£°ï¼Œå¹¶æ•æ‰äº†éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸å…³æ€§ã€‚åœ¨MELDå’ŒIEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„BCAFæ–¹æ³•ä¼˜äºç°æœ‰çš„æœ€æ–°åŸºçº¿æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05858v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å› æ•æ‰ç»†å¾®æƒ…æ„Ÿå·®å¼‚çš„ç‰¹å¾æå–éš¾åº¦è€Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚æ„å»ºæœ‰æ•ˆçš„åŒæ¨¡æ€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿçš„å…³é”®æ˜¯ç†è§£å¤šæ¨¡æ€äº¤äº’å’Œè¿æ¥ã€‚æœ¬ç ”ç©¶æå‡ºBimodal Connection Attention Fusionï¼ˆBCAFï¼‰æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šäº¤äº’è¿æ¥ç½‘ç»œã€åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå’Œç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œã€‚äº¤äº’è¿æ¥ç½‘ç»œä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„å¯¹éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€è¿æ¥è¿›è¡Œå»ºæ¨¡ï¼ŒåŒæ—¶åˆ©ç”¨æ¨¡æ€ç‰¹å®šç‰¹å¾ã€‚åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå¢å¼ºè¯­ä¹‰äº’è¡¥æ€§å¹¶æŒ–æ˜è·¨æ¨¡æ€å†…å’Œè·¨æ¨¡æ€é—´çš„äº¤äº’ä½œç”¨ã€‚ç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œå‡å°‘è·¨æ¨¡æ€å™ªå£°å¹¶æ•æ‰éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸å…³æ€§ã€‚åœ¨MELDå’ŒIEMOCAPæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„BCAFæ–¹æ³•ä¼˜äºç°æœ‰å…ˆè¿›åŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºæ•æ‰ç»†å¾®æƒ…æ„Ÿå·®å¼‚çš„ç‰¹å¾æå–éš¾åº¦å¤§ã€‚</li>
<li>æ„å»ºæœ‰æ•ˆçš„åŒæ¨¡æ€è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ç³»ç»Ÿçš„å…³é”®æ˜¯ç†è§£å¤šæ¨¡æ€äº¤äº’å’Œè¿æ¥ã€‚</li>
<li>BCAFæ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªä¸»è¦æ¨¡å—ï¼šäº¤äº’è¿æ¥ç½‘ç»œã€åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œå’Œç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œã€‚</li>
<li>äº¤äº’è¿æ¥ç½‘ç»œåˆ©ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„å»ºæ¨¡éŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€è¿æ¥ã€‚</li>
<li>åŒæ¨¡æ€æ³¨æ„åŠ›ç½‘ç»œæå‡è¯­ä¹‰äº’è¡¥æ€§å¹¶æŒ–æ˜æ¨¡æ€é—´çš„äº¤äº’ä½œç”¨ã€‚</li>
<li>ç›¸å…³æ€§æ³¨æ„åŠ›ç½‘ç»œèƒ½å‡å°‘è·¨æ¨¡æ€å™ªå£°ï¼Œå¢å¼ºéŸ³é¢‘å’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸å…³æ€§è¯†åˆ«ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05858">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1e75b1ab170f4cd157a746a8eb1bada8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55103aa927b0494b5221a3dbf79ee5e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da57a5cadbf737501e6170daac787a5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8207e89312885b3e41e6501a4c2dd0af.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="GestureLSM-Latent-Shortcut-based-Co-Speech-Gesture-Generation-with-Spatial-Temporal-Modeling"><a href="#GestureLSM-Latent-Shortcut-based-Co-Speech-Gesture-Generation-with-Spatial-Temporal-Modeling" class="headerlink" title="GestureLSM: Latent Shortcut based Co-Speech Gesture Generation with   Spatial-Temporal Modeling"></a>GestureLSM: Latent Shortcut based Co-Speech Gesture Generation with   Spatial-Temporal Modeling</h2><p><strong>Authors:Pinxin Liu, Luchuan Song, Junhua Huang, Haiyang Liu, Chenliang Xu</strong></p>
<p>Generating full-body human gestures based on speech signals remains challenges on quality and speed. Existing approaches model different body regions such as body, legs and hands separately, which fail to capture the spatial interactions between them and result in unnatural and disjointed movements. Additionally, their autoregressive&#x2F;diffusion-based pipelines show slow generation speed due to dozens of inference steps. To address these two challenges, we propose GestureLSM, a flow-matching-based approach for Co-Speech Gesture Generation with spatial-temporal modeling. Our method i) explicitly model the interaction of tokenized body regions through spatial and temporal attention, for generating coherent full-body gestures. ii) introduce the flow matching to enable more efficient sampling by explicitly modeling the latent velocity space. To overcome the suboptimal performance of flow matching baseline, we propose latent shortcut learning and beta distribution time stamp sampling during training to enhance gesture synthesis quality and accelerate inference. Combining the spatial-temporal modeling and improved flow matching-based framework, GestureLSM achieves state-of-the-art performance on BEAT2 while significantly reducing inference time compared to existing methods, highlighting its potential for enhancing digital humans and embodied agents in real-world applications. Project Page: <a target="_blank" rel="noopener" href="https://andypinxinliu.github.io/GestureLSM">https://andypinxinliu.github.io/GestureLSM</a> </p>
<blockquote>
<p>åŸºäºè¯­éŸ³ä¿¡å·ç”Ÿæˆå…¨èº«äººä½“å§¿æ€åœ¨è´¨é‡å’Œé€Ÿåº¦æ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åˆ†åˆ«å¯¹äººä½“ã€è…¿éƒ¨å’Œæ‰‹éƒ¨ç­‰ä¸åŒèº«ä½“åŒºåŸŸè¿›è¡Œå»ºæ¨¡ï¼Œè¿™æ— æ³•æ•æ‰å®ƒä»¬ä¹‹é—´çš„ç©ºé—´äº¤äº’ï¼Œå¯¼è‡´åŠ¨ä½œä¸è‡ªç„¶å’Œè„±èŠ‚ã€‚æ­¤å¤–ï¼Œä»–ä»¬çš„è‡ªå›å½’&#x2F;æ‰©æ•£åŸºäºçš„ç®¡é“ç”±äºå‡ åæ­¥çš„æ¨ç†æ­¥éª¤è€Œæ˜¾ç¤ºå‡ºç¼“æ…¢ç”Ÿæˆé€Ÿåº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæµåŒ¹é…çš„å…±è¯­å§¿æ€ç”Ÿæˆæ–¹æ³•GestureLSMï¼Œå¹¶è¿›è¡Œäº†æ—¶ç©ºå»ºæ¨¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸€ï¼‰é€šè¿‡ç©ºé—´å’Œæ—¶é—´æ³¨æ„åŠ›æ˜¾å¼åœ°æ¨¡æ‹Ÿæ ‡è®°åŒ–èº«ä½“åŒºåŸŸçš„äº¤äº’ï¼Œä»¥ç”Ÿæˆè¿è´¯çš„å…¨èº«å§¿æ€ã€‚äºŒï¼‰å¼•å…¥æµåŒ¹é…ï¼Œé€šè¿‡æ˜¾å¼åœ°å»ºæ¨¡æ½œåœ¨é€Ÿåº¦ç©ºé—´æ¥å®ç°æ›´æœ‰æ•ˆçš„é‡‡æ ·ã€‚ä¸ºäº†å…‹æœæµåŒ¹é…åŸºçº¿æ€§èƒ½ä¸ä½³çš„é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æå‡ºäº†æ½œåœ¨å¿«æ·æ–¹å¼å­¦ä¹ å’Œbetaåˆ†å¸ƒæ—¶é—´æˆ³é‡‡æ ·ï¼Œä»¥æé«˜å§¿æ€åˆæˆè´¨é‡å’ŒåŠ é€Ÿæ¨ç†ã€‚ç»“åˆæ—¶ç©ºå»ºæ¨¡å’Œæ”¹è¿›çš„åŸºäºæµåŒ¹é…çš„æ¡†æ¶ï¼ŒGestureLSMåœ¨BEAT2ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ï¼Œçªæ˜¾å…¶åœ¨å¢å¼ºæ•°å­—äººç±»å’Œå®ä½“ä»£ç†åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://andypinxinliu.github.io/GestureLSM">https://andypinxinliu.github.io/GestureLSM</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.18898v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¯­éŸ³ä¿¡å·ç”Ÿæˆå…¨èº«äººç±»åŠ¨ä½œåœ¨è´¨é‡å’Œé€Ÿåº¦ä¸Šä»æœ‰æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åˆ†åˆ«å»ºæ¨¡èº«ä½“ä¸åŒéƒ¨ä½ï¼Œå¦‚èº«ä½“ã€è…¿å’Œæ‰‹ï¼Œå¿½ç•¥äº†å®ƒä»¬ä¹‹é—´çš„ç©ºé—´äº¤äº’ï¼Œå¯¼è‡´åŠ¨ä½œä¸è‡ªç„¶ã€ä¸è¿è´¯ã€‚æ­¤å¤–ï¼Œå®ƒä»¬çš„è‡ªå›å½’æˆ–æ‰©æ•£ç”Ÿæˆç®¡é“ç”±äºéœ€è¦å¤§é‡çš„æ¨ç†æ­¥éª¤ï¼Œç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢ã€‚é’ˆå¯¹è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºGestureLSMï¼Œä¸€ç§åŸºäºæµåŒ¹é…çš„è¯­éŸ³å…±æ„ŸåŠ¨ä½œç”Ÿæˆæ–¹æ³•ï¼Œé‡‡ç”¨æ—¶ç©ºå»ºæ¨¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸€ï¼‰é€šè¿‡ç©ºé—´å’Œæ—¶é—´æ³¨æ„åŠ›æ˜¾å¼åœ°æ¨¡æ‹Ÿäº†æ ‡è®°èº«ä½“éƒ¨ä½çš„äº¤äº’ï¼Œä»¥ç”Ÿæˆè¿è´¯çš„å…¨èº«åŠ¨ä½œã€‚äºŒï¼‰å¼•å…¥æµåŒ¹é…ï¼Œé€šè¿‡æ˜¾å¼å»ºæ¨¡æ½œåœ¨é€Ÿåº¦ç©ºé—´ï¼Œä½¿é‡‡æ ·æ›´åŠ é«˜æ•ˆã€‚ä¸ºäº†å…‹æœæµåŒ¹é…åŸºçº¿æ–¹æ³•çš„æ€§èƒ½ä¸è¶³ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†æ½œåœ¨å¿«æ·æ–¹å¼å­¦ä¹ å’Œbetaåˆ†å¸ƒæ—¶é—´æˆ³é‡‡æ ·ï¼Œä»¥æé«˜åŠ¨ä½œåˆæˆçš„è´¨é‡å’ŒåŠ é€Ÿæ¨ç†ã€‚ç»“åˆæ—¶ç©ºå»ºæ¨¡å’Œæ”¹è¿›çš„æµåŒ¹é…æ¡†æ¶ï¼ŒGestureLSMåœ¨BEAT2ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ï¼Œå‡¸æ˜¾å…¶åœ¨å¢å¼ºæ•°å­—äººç±»å’Œå®ä½“ä»£ç†åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰æ–¹æ³•å»ºæ¨¡èº«ä½“ä¸åŒéƒ¨ä½æ—¶å¿½ç•¥äº†ç©ºé—´äº¤äº’ï¼Œå¯¼è‡´åŠ¨ä½œä¸è‡ªç„¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•çš„è‡ªå›å½’&#x2F;æ‰©æ•£ç”Ÿæˆç®¡é“å› æ¨ç†æ­¥éª¤å¤šè€Œç”Ÿæˆé€Ÿåº¦æ…¢ã€‚</li>
<li>GestureLSMé‡‡ç”¨åŸºäºæµåŒ¹é…çš„æ–¹æ³•ï¼Œé€šè¿‡æ—¶ç©ºå»ºæ¨¡ç”Ÿæˆè¿è´¯çš„å…¨èº«åŠ¨ä½œã€‚</li>
<li>GestureLSMæ˜¾å¼åœ°é€šè¿‡ç©ºé—´å’Œæ—¶é—´æ³¨æ„åŠ›å»ºæ¨¡èº«ä½“éƒ¨ä½çš„äº¤äº’ã€‚</li>
<li>GestureLSMå¼•å…¥æµåŒ¹é…å’Œæ½œåœ¨å¿«æ·æ–¹å¼å­¦ä¹ ï¼Œæé«˜åŠ¨ä½œåˆæˆè´¨é‡å’Œæ¨ç†æ•ˆç‡ã€‚</li>
<li>GestureLSMåœ¨BEAT2ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.18898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-eb6d611e85341e02217f0e6d00bac139.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ce768637614b12e044a590ffdd9a3fb4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-35631df10296468d4b29f13d0fe853ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b661ef4f30d2ca2438c9fc36937c2e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-30843cfe5c065d6676ba7ce3b651a22f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-eeb92593924c6f6b1ed647f1869bdcca.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  X-GAN A Generative AI-Powered Unsupervised Model for High-Precision   Segmentation of Retinal Main Vessels toward Early Detection of Glaucoma
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-63e2172cd0a0ea138aee9d585b4ecb93.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  CLICv2 Image Complexity Representation via Content Invariance   Contrastive Learning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29301k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
