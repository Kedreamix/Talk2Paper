<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  LLaVA-RadZ Can Multimodal Large Language Models Effectively Tackle   Zero-shot Radiology Recognition?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-90bd9f54860aeea029edc540d67492f1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-12-æ›´æ–°"><a href="#2025-03-12-æ›´æ–°" class="headerlink" title="2025-03-12 æ›´æ–°"></a>2025-03-12 æ›´æ–°</h1><h2 id="LLaVA-RadZ-Can-Multimodal-Large-Language-Models-Effectively-Tackle-Zero-shot-Radiology-Recognition"><a href="#LLaVA-RadZ-Can-Multimodal-Large-Language-Models-Effectively-Tackle-Zero-shot-Radiology-Recognition" class="headerlink" title="LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle   Zero-shot Radiology Recognition?"></a>LLaVA-RadZ: Can Multimodal Large Language Models Effectively Tackle   Zero-shot Radiology Recognition?</h2><p><strong>Authors:Bangyan Li, Wenxuan Huang, Yunhang Shen, Yeqiang Wang, Shaohui Lin, Jingzhong Lin, Ling You, Yinqi Zhang, Ke Li, Xing Sun, Yuling Sun</strong></p>
<p>Recently, multimodal large models (MLLMs) have demonstrated exceptional capabilities in visual understanding and reasoning across various vision-language tasks. However, MLLMs usually perform poorly in zero-shot medical disease recognition, as they do not fully exploit the captured features and available medical knowledge. To address this challenge, we propose LLaVA-RadZ, a simple yet effective framework for zero-shot medical disease recognition. Specifically, we design an end-to-end training strategy, termed Decoding-Side Feature Alignment Training (DFAT) to take advantage of the characteristics of the MLLM decoder architecture and incorporate modality-specific tokens tailored for different modalities, which effectively utilizes image and text representations and facilitates robust cross-modal alignment. Additionally, we introduce a Domain Knowledge Anchoring Module (DKAM) to exploit the intrinsic medical knowledge of large models, which mitigates the category semantic gap in image-text alignment. DKAM improves category-level alignment, allowing for accurate disease recognition. Extensive experiments on multiple benchmarks demonstrate that our LLaVA-RadZ significantly outperforms traditional MLLMs in zero-shot disease recognition and exhibits the state-of-the-art performance compared to the well-established and highly-optimized CLIP-based approaches. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å„ç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å‡ºè‰²çš„è§†è§‰ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨é›¶æ ·æœ¬åŒ»ç–—ç–¾ç—…è¯†åˆ«æ–¹é¢ï¼ŒMLLMsçš„è¡¨ç°é€šå¸¸ä¸ä½³ï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰å®Œå…¨æŒ–æ˜å‡ºæ•è·çš„ç‰¹å¾å’Œå¯ç”¨çš„åŒ»å­¦çŸ¥è¯†ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†LLaVA-RadZï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºé›¶æ ·æœ¬åŒ»ç–—ç–¾ç—…è¯†åˆ«çš„ç®€å•æœ‰æ•ˆçš„æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç«¯åˆ°ç«¯çš„è®­ç»ƒç­–ç•¥ï¼Œç§°ä¸ºè§£ç ä¾§ç‰¹å¾å¯¹é½è®­ç»ƒï¼ˆDFATï¼‰ï¼Œä»¥åˆ©ç”¨MLLMè§£ç å™¨çš„ç‰¹ç‚¹ï¼Œå¹¶ç»“åˆé’ˆå¯¹ä¸åŒæ¨¡æ€å®šåˆ¶çš„æ¨¡æ€ç‰¹å®šä»¤ç‰Œï¼Œè¿™æœ‰æ•ˆåœ°åˆ©ç”¨äº†å›¾åƒå’Œæ–‡æœ¬è¡¨ç¤ºï¼Œå¹¶ä¿ƒè¿›äº†ç¨³å¥çš„è·¨æ¨¡æ€å¯¹é½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢†åŸŸçŸ¥è¯†é”šå®šæ¨¡å—ï¼ˆDKAMï¼‰ï¼Œä»¥åˆ©ç”¨å¤§å‹æ¨¡å‹çš„å†…åœ¨åŒ»å­¦çŸ¥è¯†ï¼Œè¿™å‡è½»äº†å›¾åƒæ–‡æœ¬å¯¹é½ä¸­çš„ç±»åˆ«è¯­ä¹‰é¸¿æ²Ÿã€‚DKAMæ”¹å–„äº†ç±»åˆ«çº§åˆ«çš„å¯¹é½ï¼Œä»è€Œå®ç°å‡†ç¡®çš„ç–¾ç—…è¯†åˆ«ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„LLaVA-RadZåœ¨é›¶æ ·æœ¬ç–¾ç—…è¯†åˆ«æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„MLLMsï¼Œä¸å»ºç«‹è‰¯å¥½ä¸”é«˜åº¦ä¼˜åŒ–çš„CLIPæ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07487v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å¤šæ¨¡æ€å¤§å‹æ¨¡å‹åœ¨é›¶æ ·æœ¬åŒ»ç–—ç–¾ç—…è¯†åˆ«ä¸­çš„ä¸è¶³ï¼Œæå‡ºLLaVA-RadZæ¡†æ¶ï¼Œé€šè¿‡è§£ç ä¾§ç‰¹å¾å¯¹é½è®­ç»ƒï¼ˆDFATï¼‰å’Œé¢†åŸŸçŸ¥è¯†é”šå®šæ¨¡å—ï¼ˆDKAMï¼‰ï¼Œæœ‰æ•ˆèåˆå›¾åƒå’Œæ–‡æœ¬è¡¨å¾ï¼Œå®ç°ç¨³å¥çš„è·¨æ¨¡æ€å¯¹é½å’Œç–¾ç—…è¯†åˆ«ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»ŸMLLMså’Œé«˜åº¦ä¼˜åŒ–çš„CLIPæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è§†è§‰ç†è§£å’Œæ¨ç†æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨é›¶æ ·æœ¬åŒ»ç–—ç–¾ç—…è¯†åˆ«ä¸­å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>LLaVA-RadZæ¡†æ¶æ—¨åœ¨è§£å†³MLLMsåœ¨é›¶æ ·æœ¬åŒ»ç–—ç–¾ç—…è¯†åˆ«ä¸­çš„ä¸è¶³ã€‚</li>
<li>LLaVA-RadZé‡‡ç”¨è§£ç ä¾§ç‰¹å¾å¯¹é½è®­ç»ƒï¼ˆDFATï¼‰ï¼Œåˆ©ç”¨MLLMè§£ç å™¨æ¶æ„çš„ç‰¹ç‚¹ï¼Œå¹¶èå…¥é’ˆå¯¹ä¸åŒæ¨¡æ€çš„æ¨¡æ€ç‰¹å®šä»¤ç‰Œï¼Œå®ç°å›¾åƒå’Œæ–‡æœ¬è¡¨å¾çš„æœ‰æ•ˆåˆ©ç”¨å’Œç¨³å¥çš„è·¨æ¨¡æ€å¯¹é½ã€‚</li>
<li>å¼•å…¥é¢†åŸŸçŸ¥è¯†é”šå®šæ¨¡å—ï¼ˆDKAMï¼‰ï¼ŒæŒ–æ˜å¤§å‹æ¨¡å‹çš„å†…åœ¨åŒ»å­¦çŸ¥è¯†ï¼Œç¼©å°å›¾åƒ-æ–‡æœ¬å¯¹é½ä¸­çš„ç±»åˆ«è¯­ä¹‰å·®è·ã€‚</li>
<li>DKAMæ”¹è¿›äº†ç±»åˆ«çº§åˆ«çš„å¯¹é½ï¼Œä½¿å¾—ç–¾ç—…è¯†åˆ«æ›´åŠ å‡†ç¡®ã€‚</li>
<li>LLaVA-RadZåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»ŸMLLMsã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07487">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c67d1f9f65428403c30eccc3322aa00c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-473d5621e07f926ec8cabadeda44c5a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-30af04cdb21520c7e5c61e8c29213c2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b15024057d493d86f788160931e20e0d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Distilling-Knowledge-into-Quantum-Vision-Transformers-for-Biomedical-Image-Classification"><a href="#Distilling-Knowledge-into-Quantum-Vision-Transformers-for-Biomedical-Image-Classification" class="headerlink" title="Distilling Knowledge into Quantum Vision Transformers for Biomedical   Image Classification"></a>Distilling Knowledge into Quantum Vision Transformers for Biomedical   Image Classification</h2><p><strong>Authors:Thomas Boucher, Evangelos B. Mazomenos</strong></p>
<p>Quantum vision transformers (QViTs) build on vision transformers (ViTs) by replacing linear layers within the self-attention mechanism with parameterised quantum neural networks (QNNs), harnessing quantum mechanical properties to improve feature representation. This hybrid approach aims to achieve superior performance, with significantly reduced model complexity as a result of the enriched feature representation, requiring fewer parameters. This paper proposes a novel QViT model for biomedical image classification and investigates its performance against comparable ViTs across eight diverse datasets, encompassing various modalities and classification tasks. We assess models trained from scratch and those pre-trained using knowledge distillation (KD) from high-quality teacher models. Our findings demonstrate that QViTs outperform comparable ViTs with average ROC AUC (0.863 vs 0.846) and accuracy (0.710 vs 0.687) when trained from scratch, and even compete with state-of-the-art classical models in multiple tasks, whilst being significantly more efficient (89% reduction in GFLOPs and 99.99% in parameter number). Additionally, we find that QViTs and ViTs respond equally well to KD, with QViT pre-training performance scaling with model complexity. This is the first investigation into the efficacy of deploying QViTs with KD for computer-aided diagnosis. Our results highlight the enormous potential of quantum machine learning (QML) in biomedical image analysis. </p>
<blockquote>
<p>é‡å­è§†è§‰è½¬æ¢å™¨ï¼ˆQViTï¼‰é€šè¿‡æ„å»ºåœ¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„åŸºç¡€ä¸Šï¼Œå°†è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„çº¿æ€§å±‚æ›¿æ¢ä¸ºå‚æ•°åŒ–é‡å­ç¥ç»ç½‘ç»œï¼ˆQNNï¼‰ï¼Œåˆ©ç”¨é‡å­æœºæ¢°ç‰¹æ€§æ”¹è¿›ç‰¹å¾è¡¨ç¤ºã€‚è¿™ç§æ··åˆæ–¹æ³•æ—¨åœ¨å®ç°å“è¶Šçš„æ€§èƒ½ï¼Œå¹¶å› ç‰¹å¾è¡¨ç¤ºæ›´ä¸°å¯Œè€Œå¤§å¤§å‡å°‘æ¨¡å‹å¤æ‚åº¦ï¼Œä»è€Œå‡å°‘äº†å‚æ•°éœ€æ±‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„QViTæ¨¡å‹ï¼Œç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ï¼Œå¹¶ç ”ç©¶äº†å®ƒåœ¨å…«ä¸ªä¸åŒæ•°æ®é›†ä¸Šä¸ç›¸åº”çš„ViTçš„æ€§èƒ½å¯¹æ¯”ã€‚è¿™å…«ä¸ªæ•°æ®é›†æ¶µç›–å¤šç§æ¨¡æ€å’Œåˆ†ç±»ä»»åŠ¡ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä»å¤´å¼€å§‹è®­ç»ƒçš„æ¨¡å‹ä»¥åŠä½¿ç”¨æ¥è‡ªé«˜è´¨é‡æ•™å¸ˆæ¨¡å‹çš„è’¸é¦ï¼ˆKDï¼‰è¿›è¡Œé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä»å¤´å¼€å§‹è®­ç»ƒçš„QViTåœ¨å¹³å‡ROC AUCï¼ˆ0.863 vs 0.846ï¼‰å’Œå‡†ç¡®åº¦ï¼ˆ0.710 vs 0.687ï¼‰ä¸Šè¶…è¿‡äº†ç›¸åº”çš„ViTï¼Œç”šè‡³åœ¨å¤šä¸ªä»»åŠ¡ä¸­ä¸æœ€å…ˆè¿›çš„ç»å…¸æ¨¡å‹ç«äº‰ï¼ŒåŒæ—¶æ•ˆç‡æ›´é«˜ï¼ˆGFLOPså‡å°‘89%ï¼Œå‚æ•°æ•°é‡å‡å°‘99.99%ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°QViTå’ŒViTå¯¹KDçš„å“åº”åŒæ ·è‰¯å¥½ï¼ŒQViTçš„é¢„è®­ç»ƒæ€§èƒ½éšæ¨¡å‹å¤æ‚åº¦çš„å¢åŠ è€Œæé«˜ã€‚è¿™æ˜¯é¦–æ¬¡è°ƒæŸ¥å°†QViTä¸KDç»“åˆç”¨äºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç»“æœçªæ˜¾äº†é‡å­æœºå™¨å­¦ä¹ ï¼ˆQMLï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07294v1">PDF</a> Submitted for MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºé‡å­è®¡ç®—æœºç®—æ³•ç†è®ºçš„é‡å­è§†è§‰å˜å‹å™¨ï¼ˆQViTï¼‰æŠ€æœ¯å–å¾—äº†é‡å¤§çªç ´ã€‚æ­¤æŠ€æœ¯åŸºäºç°æœ‰è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ï¼Œå¹¶åˆ©ç”¨é‡å­ç¥ç»ç½‘ç»œå¼ºåŒ–å…¶ç‰¹å¾è¡¨å¾åŠŸèƒ½ã€‚æœ€æ–°ç ”ç©¶å‘ç°è¿™ç§æ–°æŠ€æœ¯å¯åœ¨æ˜¾è‘—å‡å°‘æ¨¡å‹å¤æ‚æ€§çš„åŒæ—¶æé«˜æ€§èƒ½ï¼Œå°¤å…¶é€‚ç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚åœ¨å¤šç§æ•°æ®é›†ä¸Šè¿›è¡Œçš„æµ‹è¯•è¡¨æ˜ï¼Œä½¿ç”¨QViTæ¨¡å‹çš„åˆ†ç±»å™¨è¡¨ç°ä¼˜äºä¼ ç»Ÿæ¨¡å‹ï¼Œå°¤å…¶åœ¨é«˜æ•ˆè®¡ç®—æ–¹é¢æœ‰å·¨å¤§ä¼˜åŠ¿ã€‚åŒæ—¶ï¼Œæœ¬æ–‡ä¹Ÿæ˜¯é¦–æ¬¡æ¢è®¨å°†QViTæŠ€æœ¯ä¸çŸ¥è¯†è’¸é¦æŠ€æœ¯ç›¸ç»“åˆåº”ç”¨äºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­çš„å¯èƒ½æ€§ã€‚ç ”ç©¶ç»“æœä¸ºé‡å­æœºå™¨å­¦ä¹ åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸçš„æœªæ¥åº”ç”¨å¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é‡å­è§†è§‰å˜å‹å™¨ï¼ˆQViTï¼‰ç»“åˆäº†é‡å­ç¥ç»ç½‘ç»œä¸è§†è§‰å˜å‹å™¨æŠ€æœ¯ã€‚å®ƒé€šè¿‡ä¼˜åŒ–ViTçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥å¢å¼ºå…¶æ€§èƒ½ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è¡¨å¾èƒ½åŠ›ã€‚</li>
<li>QViTæ˜¾è‘—å‡å°‘äº†æ¨¡å‹çš„å¤æ‚æ€§ï¼Œå¹¶åœ¨å¤šé¡¹ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨QViTçš„åˆ†ç±»å™¨æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ¨¡å‹ã€‚å°¤å…¶æ˜¯ä¸ä¼ ç»Ÿçš„ViTç›¸æ¯”ï¼Œå…¶å¹³å‡ROC AUCå’Œå‡†ç¡®ç‡å‡æœ‰æ˜¾è‘—æå‡ã€‚åŒæ—¶å…¶èƒ½æ•ˆæé«˜ï¼Œä¸ä¼ ç»Ÿæ¨¡å‹ç›¸æ¯”å¤§å¹…é™ä½è®¡ç®—å¤æ‚åº¦ã€‚</li>
<li>QViTæ¨¡å‹å¯¹çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼ˆKDï¼‰çš„å“åº”è‰¯å¥½ï¼Œé¢„è®­ç»ƒæ€§èƒ½éšæ¨¡å‹å¤æ‚åº¦çš„å¢åŠ è€Œæå‡ã€‚è¿™ä¸ºæœªæ¥è®¡ç®—æœºè¯Šæ–­ä¸­çš„æ¨¡å‹è®­ç»ƒæä¾›äº†æ–°çš„æ€è·¯ã€‚è¿™æ˜¯é¦–æ¬¡å°è¯•å°†QViTä¸KDç»“åˆåº”ç”¨äºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­çš„ç ”ç©¶ã€‚ç»“åˆä¸¤è€…èƒ½å¤Ÿæé«˜æ¨¡å‹è®­ç»ƒçš„æ•ˆç‡å’Œæ•ˆæœã€‚é€šè¿‡æœ¬æ¬¡è¯•éªŒå‘ç°æ­ç¤ºäº†é‡å­æœºå™¨å­¦ä¹ åœ¨ç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†æé¢†åŸŸå·¨å¤§çš„åº”ç”¨æ½œåŠ›ï¼Œè¯æ˜èåˆä¸åŒæŠ€æœ¯å’Œé¢†åŸŸçš„åˆä½œå‰æ™¯ååˆ†å…‰æ˜ã€‚æˆ‘ä»¬æœ‰æœ›é€šè¿‡é«˜æ•ˆç®—æ³•å®ç°æ›´å‡†ç¡®çš„è¯Šæ–­ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07294">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3f533fcc5222f0ec941b934645917734.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a331eea6ad88bf14f279ea6dab714103.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b93ef2392de589f334d3b432b3523c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ced9f6a61aa35810369b7069c7f4651.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Customized-SAM-2-for-Referring-Remote-Sensing-Image-Segmentation"><a href="#Customized-SAM-2-for-Referring-Remote-Sensing-Image-Segmentation" class="headerlink" title="Customized SAM 2 for Referring Remote Sensing Image Segmentation"></a>Customized SAM 2 for Referring Remote Sensing Image Segmentation</h2><p><strong>Authors:Fu Rong, Meng Lan, Qian Zhang, Lefei Zhang</strong></p>
<p>Referring Remote Sensing Image Segmentation (RRSIS) aims to segment target objects in remote sensing (RS) images based on textual descriptions. Although Segment Anything Model 2 (SAM 2) has shown remarkable performance in various segmentation tasks, its application to RRSIS presents several challenges, including understanding the text-described RS scenes and generating effective prompts from text descriptions. To address these issues, we propose RS2-SAM 2, a novel framework that adapts SAM 2 to RRSIS by aligning the adapted RS features and textual features, providing pseudo-mask-based dense prompts, and enforcing boundary constraints. Specifically, we first employ a union encoder to jointly encode the visual and textual inputs, generating aligned visual and text embeddings as well as multimodal class tokens. Then, we design a bidirectional hierarchical fusion module to adapt SAM 2 to RS scenes and align adapted visual features with the visually enhanced text embeddings, improving the modelâ€™s interpretation of text-described RS scenes. Additionally, a mask prompt generator is introduced to take the visual embeddings and class tokens as input and produce a pseudo-mask as the dense prompt of SAM 2. To further refine segmentation, we introduce a text-guided boundary loss to optimize segmentation boundaries by computing text-weighted gradient differences. Experimental results on several RRSIS benchmarks demonstrate that RS2-SAM 2 achieves state-of-the-art performance. </p>
<blockquote>
<p>è¿œç¨‹é¥æ„Ÿå›¾åƒåˆ†å‰²ï¼ˆRRSISï¼‰æ—¨åœ¨æ ¹æ®æ–‡æœ¬æè¿°å¯¹é¥æ„Ÿï¼ˆRSï¼‰å›¾åƒä¸­çš„ç›®æ ‡å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚å°½ç®¡Segment Anything Model 2ï¼ˆSAM 2ï¼‰åœ¨å„ç§åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†å°†å…¶åº”ç”¨äºRRSISé¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç†è§£æ–‡æœ¬æè¿°çš„RSåœºæ™¯å’Œä»æ–‡æœ¬æè¿°ä¸­äº§ç”Ÿæœ‰æ•ˆçš„æç¤ºã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RS2-SAM 2ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé€šè¿‡é€‚åº”SAM 2åˆ°RRSISï¼Œé€šè¿‡å¯¹é½é€‚åº”çš„RSç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ã€æä¾›åŸºäºä¼ªæ©ç çš„å¯†é›†æç¤ºå’Œæ‰§è¡Œè¾¹ç•Œçº¦æŸæ¥é€‚åº”SAM 2ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆé‡‡ç”¨è”åˆç¼–ç å™¨å¯¹è§†è§‰å’Œæ–‡æœ¬è¾“å…¥è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆå¯¹é½çš„è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ä»¥åŠå¤šæ¨¡æ€ç±»æ ‡è®°ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŒå‘å±‚æ¬¡èåˆæ¨¡å—ï¼Œä½¿SAM 2é€‚åº”RSåœºæ™¯ï¼Œå¹¶ä½¿é€‚åº”çš„è§†è§‰ç‰¹å¾ä¸å¢å¼ºçš„æ–‡æœ¬åµŒå…¥å¯¹é½ï¼Œæé«˜æ¨¡å‹å¯¹æ–‡æœ¬æè¿°çš„RSåœºæ™¯çš„è§£é‡Šèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†æ©è†œæç¤ºç”Ÿæˆå™¨ï¼Œä»¥è§†è§‰åµŒå…¥å’Œç±»æ ‡è®°ä¸ºè¾“å…¥ï¼Œç”Ÿæˆä¼ªæ©è†œä½œä¸ºSAM 2çš„å¯†é›†æç¤ºã€‚ä¸ºäº†è¿›ä¸€æ­¥ç»†åŒ–åˆ†å‰²ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–‡æœ¬å¼•å¯¼è¾¹ç•ŒæŸå¤±ï¼Œé€šè¿‡è®¡ç®—æ–‡æœ¬åŠ æƒæ¢¯åº¦å·®å¼‚æ¥ä¼˜åŒ–åˆ†å‰²è¾¹ç•Œã€‚åœ¨å¤šä¸ªRRSISåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRS2-SAM 2è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07266v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿œç¨‹æ„Ÿåº”å›¾åƒåˆ†å‰²çš„æ–‡æœ¬æè¿°ï¼ˆRRSISï¼‰æ—¨åœ¨æ ¹æ®æ–‡æœ¬æè¿°å¯¹é¥æ„Ÿå›¾åƒçš„ç›®æ ‡å¯¹è±¡è¿›è¡Œåˆ†å‰²ã€‚é’ˆå¯¹SAM 2æ¨¡å‹åœ¨RRSISåº”ç”¨ä¸­çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†RS2-SAM 2æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆé¥æ„Ÿç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾ã€æä¾›åŸºäºä¼ªæ©ç çš„å¯†é›†æç¤ºå¹¶å¼ºåˆ¶æ‰§è¡Œè¾¹ç•Œçº¦æŸï¼Œä»¥é€‚åº”RRSISã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRS2-SAM 2åœ¨å¤šä¸ªRRSISåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RS2-SAM 2æ—¨åœ¨è§£å†³SAM 2åœ¨é¥æ„Ÿå›¾åƒåˆ†æ®µä¸­çš„æ–‡æœ¬æè¿°åº”ç”¨ä¸­çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡è”åˆç¼–ç è§†è§‰å’Œæ–‡æœ¬è¾“å…¥ï¼Œç”Ÿæˆå¯¹é½çš„è§†è§‰å’Œæ–‡æœ¬åµŒå…¥ä»¥åŠå¤šæ¨¡æ€ç±»ä»¤ç‰Œï¼Œä»¥ç†è§£æ–‡æœ¬æè¿°çš„é¥æ„Ÿåœºæ™¯ã€‚</li>
<li>è®¾è®¡çš„åŒå‘å±‚æ¬¡èåˆæ¨¡å—ä½¿SAM 2é€‚åº”é¥æ„Ÿåœºæ™¯ï¼Œå°†é€‚åº”çš„è§†è§‰ç‰¹å¾ä¸è§†è§‰å¢å¼ºçš„æ–‡æœ¬åµŒå…¥å¯¹é½ã€‚</li>
<li>å¼•å…¥æ©è†œæç¤ºç”Ÿæˆå™¨ï¼Œä»¥è§†è§‰åµŒå…¥å’Œç±»ä»¤ç‰Œä¸ºè¾“å…¥ï¼Œç”Ÿæˆä¼ªæ©ç ä½œä¸ºSAM 2çš„å¯†é›†æç¤ºã€‚</li>
<li>é€šè¿‡è®¡ç®—æ–‡æœ¬åŠ æƒçš„æ¢¯åº¦å·®å¼‚æ¥ä¼˜åŒ–åˆ†å‰²è¾¹ç•Œï¼Œè¿›ä¸€æ­¥ç»†åŒ–åˆ†å‰²ã€‚</li>
<li>RS2-SAM 2åœ¨å¤šä¸ªé¥æ„Ÿå›¾åƒåˆ†å‰²åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f2dd96547a6d0b02aaa6eb8844c290f5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d2a37879a18faa7397bfca3fd53235c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0302a6ea4937bc4bbc9bc141c40e74c5.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DeepNuParc-A-Novel-Deep-Clustering-Framework-for-Fine-scale-Parcellation-of-Brain-Nuclei-Using-Diffusion-MRI-Tractography"><a href="#DeepNuParc-A-Novel-Deep-Clustering-Framework-for-Fine-scale-Parcellation-of-Brain-Nuclei-Using-Diffusion-MRI-Tractography" class="headerlink" title="DeepNuParc: A Novel Deep Clustering Framework for Fine-scale   Parcellation of Brain Nuclei Using Diffusion MRI Tractography"></a>DeepNuParc: A Novel Deep Clustering Framework for Fine-scale   Parcellation of Brain Nuclei Using Diffusion MRI Tractography</h2><p><strong>Authors:Haolin He, Ce Zhu, Le Zhang, Yipeng Liu, Xiao Xu, Yuqian Chen, Leo Zekelman, Jarrett Rushmore, Yogesh Rathi, Nikos Makris, Lauren J. Oâ€™Donnell, Fan Zhang</strong></p>
<p>Brain nuclei are clusters of anatomically distinct neurons that serve as important hubs for processing and relaying information in various neural circuits. Fine-scale parcellation of the brain nuclei is vital for a comprehensive understanding of its anatomico-functional correlations. Diffusion MRI tractography is an advanced imaging technique that can estimate the brainâ€™s white matter structural connectivity to potentially reveal the topography of the nuclei of interest for studying its subdivisions. In this work, we present a deep clustering pipeline, namely DeepNuParc, to perform automated, fine-scale parcellation of brain nuclei using diffusion MRI tractography. First, we incorporate a newly proposed deep learning approach to enable accurate segmentation of the nuclei of interest directly on the dMRI data. Next, we design a novel streamline clustering-based structural connectivity feature for a robust representation of voxels within the nuclei. Finally, we improve the popular joint dimensionality reduction and k-means clustering approach to enable nuclei parcellation at a finer scale. We demonstrate DeepNuParc on two important brain structures, i.e. the amygdala and the thalamus, that are known to have multiple anatomically and functionally distinct nuclei subdivisions. Experimental results show that DeepNuParc enables consistent parcellation of the nuclei into multiple parcels across multiple subjects and achieves good correspondence with the widely used coarse-scale atlases. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/HarlandZZC/deep_nuclei_parcellation">https://github.com/HarlandZZC/deep_nuclei_parcellation</a>. </p>
<blockquote>
<p>è„‘æ ¸æ˜¯ç”±è§£å‰–ç»“æ„ç‹¬ç‰¹çš„ç¥ç»å…ƒèšé›†è€Œæˆçš„é‡è¦æ¢çº½ï¼Œç”¨äºå¤„ç†å’Œä¼ é€’å„ç§ç¥ç»ç½‘ç»œä¸­çš„ä¿¡æ¯ã€‚å¯¹è„‘æ ¸è¿›è¡Œç²¾ç»†çš„åˆ†åŒºå¯¹äºå…¨é¢ç†è§£å…¶è§£å‰–åŠŸèƒ½å…³è”è‡³å…³é‡è¦ã€‚æ‰©æ•£MRIè¿½è¸ªæˆåƒæ˜¯ä¸€ç§å…ˆè¿›çš„æˆåƒæŠ€æœ¯ï¼Œå¯ä»¥ä¼°è®¡å¤§è„‘çš„ç™½è‰²ç‰©è´¨ç»“æ„è¿æ¥æ€§ï¼Œä»¥æ­ç¤ºæ„Ÿå…´è¶£æ ¸å›¢çš„æ‹“æ‰‘ç»“æ„ï¼Œä»è€Œç ”ç©¶å…¶äºšåŒºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ·±åº¦èšç±»æµç¨‹ï¼Œå³DeepNuParcï¼Œä½¿ç”¨æ‰©æ•£MRIè¿½è¸ªæˆåƒæŠ€æœ¯è‡ªåŠ¨æ‰§è¡Œè„‘æ ¸çš„ç²¾ç»†åˆ†åŒºã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é‡‡ç”¨æ–°æå‡ºçš„æ·±åº¦å­¦ä¹ æ–¹æ³•è¿›è¡Œå‡†ç¡®åˆ†å‰²ï¼Œç›´æ¥åœ¨dMRIæ•°æ®ä¸Šå¯¹æ„Ÿå…´è¶£çš„æ ¸å›¢è¿›è¡Œåˆ†å‰²ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºæµçº¿èšç±»çš„ç»“æ„è¿æ¥ç‰¹å¾ï¼Œä»¥ç¨³å¥åœ°è¡¨ç¤ºæ ¸å›¢å†…çš„ä½“ç´ ã€‚æœ€åï¼Œæˆ‘ä»¬æ”¹è¿›äº†æµè¡Œçš„è”åˆé™ç»´å’ŒKå‡å€¼èšç±»æ–¹æ³•ï¼Œä»¥å®ç°æ›´ç²¾ç»†çš„æ ¸å›¢åˆ†åŒºã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªé‡è¦çš„è„‘ç»“æ„â€”â€”æä»æ ¸å’Œä¸˜è„‘ä¸Šå±•ç¤ºäº†DeepNuParcçš„åº”ç”¨ï¼Œè¿™ä¸¤ä¸ªç»“æ„å·²çŸ¥å…·æœ‰å¤šä¸ªè§£å‰–å’ŒåŠŸèƒ½ä¸Šç‹¬ç‰¹çš„æ ¸å›¢äºšåŒºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepNuParcèƒ½å¤Ÿåœ¨å¤šä¸ªå—è¯•è€…ä¸­å¯¹æ ¸å›¢è¿›è¡Œä¸€è‡´çš„ç²¾ç»†åˆ†å‰²ï¼Œå¹¶ä¸å¹¿æ³›ä½¿ç”¨çš„ç²—ç•¥å›¾è°±å®ç°äº†è‰¯å¥½çš„å¯¹åº”ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä» <a target="_blank" rel="noopener" href="https://github.com/HarlandZZC/deep_nuclei_parcellation">https://github.com/HarlandZZC/deep_nuclei_parcellation</a> è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07263v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºDeepNuParcçš„æ·±åº¦å­¦ä¹ é©±åŠ¨çš„å¤§è„‘æ ¸å›¢ç²¾ç»†åˆ†å‰²æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†æ‰©æ•£MRIæˆåƒæŠ€æœ¯å’Œæ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå®ç°å¯¹å¤§è„‘æ ¸å›¢è¿›è¡Œè‡ªåŠ¨åŒ–ç²¾ç»†åˆ†å‰²ã€‚DeepNuParcå¯åº”ç”¨äºå¤šä¸ªå·²çŸ¥å…·æœ‰å¤šä¸ªè§£å‰–å’ŒåŠŸèƒ½ä¸Šç‹¬ç‰¹æ ¸å›¢çš„å¤§è„‘ç»“æ„ï¼Œå¦‚æä»æ ¸å’Œä¸˜è„‘ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªå—è¯•è€…ä¹‹é—´å®ç°äº†ç¨³å®šçš„æ ¸å›¢åˆ†å‰²ï¼Œå¹¶ä¸å¹¿æ³›ä½¿ç”¨çš„ç²—å°ºåº¦å›¾è°±å…·æœ‰è‰¯å¥½çš„å¯¹åº”å…³ç³»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ä¸­æåˆ°äº†Brain nucleiä½œä¸ºå¤„ç†ä¿¡æ¯å’Œä¼ é€’ä¿¡æ¯çš„é‡è¦æ¢çº½ï¼Œå…¶ç²¾ç»†åˆ†å‰²å¯¹ç†è§£è§£å‰–åŠŸèƒ½å…³è”è‡³å…³é‡è¦ã€‚</li>
<li>é‡‡ç”¨æ‰©æ•£MRIæˆåƒæŠ€æœ¯ä¼°è®¡å¤§è„‘ç™½è´¨ç»“æ„è¿æ¥æ€§ï¼Œä»¥æ­ç¤ºæ ¸å›¢æ‹“æ‰‘ç»“æ„ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ·±åº¦å­¦ä¹ é©±åŠ¨çš„å¤§è„‘æ ¸å›¢ç²¾ç»†åˆ†å‰²æ–¹æ³•â€”â€”DeepNuParcã€‚</li>
<li>DeepNuParcç»“åˆäº†æ·±åº¦å­¦ä¹ å’Œæ‰©æ•£MRIæ•°æ®ï¼Œå®ç°äº†å¯¹å¤§è„‘æ ¸å›¢çš„ç›´æ¥åˆ†å‰²ã€‚</li>
<li>é€šè¿‡æµçº¿èšç±»æŠ€æœ¯ï¼ŒDeepNuParcå¯æœ‰æ•ˆåœ°è¡¨ç¤ºæ ¸å›¢å†…çš„ä½“ç´ ç‰¹å¾ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒDeepNuParcå¯å®ç°è·¨å¤šä¸ªå—è¯•è€…çš„æ ¸å›¢ä¸€è‡´æ€§åˆ†å‰²ï¼Œå¹¶ä¸ç²—å°ºåº¦å›¾è°±æœ‰è‰¯å¥½çš„å¯¹åº”å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07263">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-baa7d95dda6c5fae54884dd8e125ce0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec125fac39d36231565215589d1f6a9f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="AI-Driven-Automated-Tool-for-Abdominal-CT-Body-Composition-Analysis-in-Gastrointestinal-Cancer-Management"><a href="#AI-Driven-Automated-Tool-for-Abdominal-CT-Body-Composition-Analysis-in-Gastrointestinal-Cancer-Management" class="headerlink" title="AI-Driven Automated Tool for Abdominal CT Body Composition Analysis in   Gastrointestinal Cancer Management"></a>AI-Driven Automated Tool for Abdominal CT Body Composition Analysis in   Gastrointestinal Cancer Management</h2><p><strong>Authors:Xinyu Nan, Meng He, Zifan Chen, Bin Dong, Lei Tang, Li Zhang</strong></p>
<p>The incidence of gastrointestinal cancers remains significantly high, particularly in China, emphasizing the importance of accurate prognostic assessments and effective treatment strategies. Research shows a strong correlation between abdominal muscle and fat tissue composition and patient outcomes. However, existing manual methods for analyzing abdominal tissue composition are time-consuming and costly, limiting clinical research scalability. To address these challenges, we developed an AI-driven tool for automated analysis of abdominal CT scans to effectively identify and segment muscle, subcutaneous fat, and visceral fat. Our tool integrates a multi-view localization model and a high-precision 2D nnUNet-based segmentation model, demonstrating a localization accuracy of 90% and a Dice Score Coefficient of 0.967 for segmentation. Furthermore, it features an interactive interface that allows clinicians to refine the segmentation results, ensuring high-quality outcomes effectively. Our tool offers a standardized method for effectively extracting critical abdominal tissues, potentially enhancing the management and treatment for gastrointestinal cancers. The code is available at <a target="_blank" rel="noopener" href="https://github.com/NanXinyu/AI-Tool4Abdominal-Seg.git%7D%7Bhttps://github.com/NanXinyu/AI-Tool4Abdominal-Seg.git">https://github.com/NanXinyu/AI-Tool4Abdominal-Seg.git}{https://github.com/NanXinyu/AI-Tool4Abdominal-Seg.git</a>. </p>
<blockquote>
<p>èƒƒè‚ é“ç™Œç—‡çš„å‘ç—…ç‡ä»ç„¶å¾ˆé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­å›½ï¼Œè¿™å¼ºè°ƒäº†å‡†ç¡®é¢„åè¯„ä¼°å’Œæœ‰æ•ˆæ²»ç–—ç­–ç•¥çš„é‡è¦æ€§ã€‚ç ”ç©¶è¡¨æ˜ï¼Œè…¹éƒ¨è‚Œè‚‰å’Œè„‚è‚ªç»„ç»‡æˆåˆ†ä¸æ‚£è€…çš„æ²»ç–—æ•ˆæœä¹‹é—´å­˜åœ¨å¼ºçƒˆçš„ç›¸å…³æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åˆ†æè…¹éƒ¨ç»„ç»‡æˆåˆ†çš„æ‰‹å·¥æ–¹æ³•æ—¢è€—æ—¶åˆæˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†ä¸´åºŠç ”ç©¶çš„å¯æ‰©å±•æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§äººå·¥æ™ºèƒ½é©±åŠ¨çš„å·¥å…·ï¼Œç”¨äºè‡ªåŠ¨åˆ†æè…¹éƒ¨CTæ‰«æç»“æœï¼Œæœ‰æ•ˆè¯†åˆ«å¹¶åˆ†å‰²è‚Œè‚‰ã€çš®ä¸‹è„‚è‚ªå’Œå†…è„è„‚è‚ªã€‚æˆ‘ä»¬çš„å·¥å…·é›†æˆäº†ä¸€ä¸ªå¤šè§†å›¾å®šä½æ¨¡å‹å’Œä¸€ä¸ªåŸºäºé«˜ç²¾åº¦äºŒç»´nnUNetçš„åˆ†å‰²æ¨¡å‹ï¼Œå®šä½ç²¾åº¦è¾¾åˆ°90%ï¼Œåˆ†å‰²çš„Diceç³»æ•°è¾¾åˆ°0.967ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å…·æœ‰ä¸€ä¸ªäº¤äº’å¼ç•Œé¢ï¼Œå…è®¸ä¸´åºŠåŒ»ç”Ÿå¯¹åˆ†å‰²ç»“æœè¿›è¡Œå¾®è°ƒï¼Œç¡®ä¿é«˜è´¨é‡çš„ç»“æœã€‚æˆ‘ä»¬çš„å·¥å…·æä¾›äº†ä¸€ç§æœ‰æ•ˆæå–å…³é”®è…¹éƒ¨ç»„ç»‡çš„æ–¹æ³•ï¼Œå¯èƒ½æœ‰åŠ©äºèƒƒè‚ é“ç™Œç—‡çš„ç®¡ç†å’Œæ²»ç–—ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NanXinyu/AI-Tool4Abdominal-Seg.git%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/NanXinyu/AI-Tool4Abdominal-Seg.gitä¸Šè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07248v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸­å›½èƒƒè‚ é“ç™Œç—‡çš„å‘ç—…ç‡ä»ç„¶è¾ƒé«˜ï¼Œå¼ºè°ƒäº†å¯¹æ‚£è€…è¿›è¡Œå‡†ç¡®çš„é¢„åè¯„ä¼°å’Œæœ‰æ•ˆæ²»ç–—ç­–ç•¥çš„é‡è¦æ€§ã€‚ä¸ºè§£å†³ç°æœ‰åˆ†æè…¹éƒ¨ç»„ç»‡æˆåˆ†æ–¹æ³•è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½çš„å·¥å…·ï¼Œå¯è‡ªåŠ¨åˆ†æè…¹éƒ¨CTæ‰«æå›¾åƒï¼Œæœ‰æ•ˆè¯†åˆ«å¹¶åˆ†å‰²è‚Œè‚‰ã€çš®ä¸‹è„‚è‚ªå’Œå†…è„è„‚è‚ªã€‚è¯¥å·¥å…·ç»“åˆäº†å¤šè§†è§’å®šä½æ¨¡å‹å’ŒåŸºäºé«˜ç²¾åº¦äºŒç»´nnUNetçš„åˆ†å‰²æ¨¡å‹ï¼Œå®šä½å‡†ç¡®ç‡é«˜è¾¾90%ï¼Œåˆ†å‰²çš„Diceç³»æ•°è¾¾åˆ°0.967ã€‚æ­¤å¤–ï¼Œå…¶äº¤äº’ç•Œé¢ä½¿ä¸´åºŠåŒ»ç”Ÿèƒ½å¤Ÿä¼˜åŒ–åˆ†å‰²ç»“æœï¼Œç¡®ä¿é«˜è´¨é‡çš„ç»“æœè¾“å‡ºã€‚æ­¤å·¥å…·ä¸ºæ ‡å‡†åŒ–æå–å…³é”®è…¹éƒ¨ç»„ç»‡æä¾›äº†æœ‰æ•ˆæ–¹æ³•ï¼Œæœ‰æœ›æ”¹å–„èƒƒè‚ é“ç™Œç—‡çš„ç®¡ç†å’Œæ²»ç–—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>èƒƒè‚ é“ç™Œç—‡åœ¨ä¸­å›½å‘ç—…ç‡é«˜ï¼Œéœ€è¦å‡†ç¡®çš„é¢„åè¯„ä¼°å’Œæœ‰æ•ˆæ²»ç–—ç­–ç•¥ã€‚</li>
<li>ç°æœ‰åˆ†æè…¹éƒ¨ç»„ç»‡æˆåˆ†çš„æ–¹æ³•è€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚ï¼Œé™åˆ¶äº†ä¸´åºŠç ”ç©¶çš„å‘å±•ã€‚</li>
<li>ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ç§åŸºäºAIçš„å·¥å…·ï¼Œå¯ä»¥è‡ªåŠ¨åˆ†æè…¹éƒ¨CTæ‰«æå›¾åƒï¼Œè¯†åˆ«å¹¶åˆ†å‰²è‚Œè‚‰ã€çš®ä¸‹è„‚è‚ªå’Œå†…è„è„‚è‚ªã€‚</li>
<li>è¯¥å·¥å…·é‡‡ç”¨å¤šè§†è§’å®šä½æ¨¡å‹å’ŒåŸºäºé«˜ç²¾åº¦äºŒç»´nnUNetçš„åˆ†å‰²æ¨¡å‹ï¼Œå…·æœ‰é«˜çš„å®šä½å‡†ç¡®ç‡å’Œåˆ†å‰²æ€§èƒ½ã€‚</li>
<li>å·¥å…·çš„äº¤äº’ç•Œé¢å…è®¸ä¸´åºŠåŒ»ç”Ÿä¼˜åŒ–åˆ†å‰²ç»“æœï¼Œç¡®ä¿é«˜è´¨é‡çš„ç»“æœè¾“å‡ºã€‚</li>
<li>æ­¤å·¥å…·ä¸ºæ ‡å‡†åŒ–æå–å…³é”®è…¹éƒ¨ç»„ç»‡æä¾›äº†æœ‰æ•ˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07248">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-beaa1e650ba780e3afd950594d7e691a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcfc2a09c3a5c711b7d1b6c388899434.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ead33b38d2167d7e21367795a538546.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e427b0293b79de3407d27c9857202efe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c5c240d6b36724846b658346ee6b85f2.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="The-4D-Human-Embryonic-Brain-Atlas-spatiotemporal-atlas-generation-for-rapid-anatomical-changes-using-first-trimester-ultrasound-from-the-Rotterdam-Periconceptional-Cohort"><a href="#The-4D-Human-Embryonic-Brain-Atlas-spatiotemporal-atlas-generation-for-rapid-anatomical-changes-using-first-trimester-ultrasound-from-the-Rotterdam-Periconceptional-Cohort" class="headerlink" title="The 4D Human Embryonic Brain Atlas: spatiotemporal atlas generation for   rapid anatomical changes using first-trimester ultrasound from the Rotterdam   Periconceptional Cohort"></a>The 4D Human Embryonic Brain Atlas: spatiotemporal atlas generation for   rapid anatomical changes using first-trimester ultrasound from the Rotterdam   Periconceptional Cohort</h2><p><strong>Authors:Wietske A. P. Bastiaansen, Melek Rousian, Anton H. J. Koning, Wiro J. Niessen, Bernadette S. de Bakker, RÃ©gine P. M. Steegers-Theunissen, Stefan Klein</strong></p>
<p>Early brain development is crucial for lifelong neurodevelopmental health. However, current clinical practice offers limited knowledge of normal embryonic brain anatomy on ultrasound, despite the brain undergoing rapid changes within the time-span of days. To provide detailed insights into normal brain development and identify deviations, we created the 4D Human Embryonic Brain Atlas using a deep learning-based approach for groupwise registration and spatiotemporal atlas generation. Our method introduced a time-dependent initial atlas and penalized deviations from it, ensuring age-specific anatomy was maintained throughout rapid development. The atlas was generated and validated using 831 3D ultrasound images from 402 subjects in the Rotterdam Periconceptional Cohort, acquired between gestational weeks 8 and 12. We evaluated the effectiveness of our approach with an ablation study, which demonstrated that incorporating a time-dependent initial atlas and penalization produced anatomically accurate results. In contrast, omitting these adaptations led to anatomically incorrect atlas. Visual comparisons with an existing ex-vivo embryo atlas further confirmed the anatomical accuracy of our atlas. In conclusion, the proposed method successfully captures the rapid anotomical development of the embryonic brain. The resulting 4D Human Embryonic Brain Atlas provides a unique insights into this crucial early life period and holds the potential for improving the detection, prevention, and treatment of prenatal neurodevelopmental disorders. </p>
<blockquote>
<p>æ—©æœŸå¤§è„‘å‘è‚²å¯¹ç»ˆèº«ç¥ç»å‘è‚²å¥åº·è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå°½ç®¡å¤§è„‘åœ¨å‡ å¤©å†…ç»å†å¿«é€Ÿå˜åŒ–ï¼Œä½†ç›®å‰çš„ä¸´åºŠå®è·µå¯¹è¶…å£°ä¸‹çš„æ­£å¸¸èƒšèƒå¤§è„‘è§£å‰–ç»“æ„äº†è§£æœ‰é™ã€‚ä¸ºäº†æ·±å…¥äº†è§£æ­£å¸¸å¤§è„‘å‘è‚²å¹¶è¯†åˆ«åå·®ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•åˆ›å»ºå››ç»´äººç±»èƒšèƒå¤§è„‘å›¾è°±è¿›è¡Œç¾¤ä½“æ³¨å†Œå’Œæ—¶ç©ºå›¾è°±ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ä¸ªæ—¶é—´ä¾èµ–çš„åˆå§‹å›¾è°±ï¼Œå¹¶å¯¹åç¦»è¯¥å›¾è°±çš„æƒ…å†µè¿›è¡Œæƒ©ç½šï¼Œä»¥ç¡®ä¿åœ¨å¿«é€Ÿå‘è‚²è¿‡ç¨‹ä¸­ä¿æŒç‰¹å®šå¹´é¾„çš„è§£å‰–å­¦ç‰¹å¾ã€‚è¯¥å›¾è°±æ˜¯ä½¿ç”¨æ¥è‡ªé¹¿ç‰¹ä¸¹èƒšèƒæœŸé˜Ÿåˆ—çš„402åå—è¯•è€…çš„831å¼ ä¸‰ç»´è¶…å£°å›¾åƒç”Ÿæˆå’ŒéªŒè¯çš„ï¼Œè¿™äº›å›¾åƒæ˜¯åœ¨å¦Šå¨ ç¬¬8è‡³ç¬¬12å‘¨æœŸé—´è·å–çš„ã€‚æˆ‘ä»¬é€šè¿‡ä¸€é¡¹æ¶ˆèç ”ç©¶è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œè¯¥ç ”ç©¶è¯æ˜é‡‡ç”¨æ—¶é—´ä¾èµ–çš„åˆå§‹å›¾è°±å’Œæƒ©ç½šæœºåˆ¶èƒ½å¤Ÿäº§ç”Ÿè§£å‰–ç»“æ„å‡†ç¡®çš„ç»“æœã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œçœç•¥è¿™äº›é€‚åº”æ€§è°ƒæ•´ä¼šå¯¼è‡´è§£å‰–ç»“æ„ä¸æ­£ç¡®çš„å›¾è°±ã€‚ä¸ç°æœ‰çš„ä½“å¤–èƒšèƒå›¾è°±çš„è§†è§‰æ¯”è¾ƒè¿›ä¸€æ­¥è¯å®äº†æˆ‘ä»¬å›¾è°±çš„è§£å‰–å­¦å‡†ç¡®æ€§ã€‚æ€»ä¹‹ï¼Œæ‰€æå‡ºçš„æ–¹æ³•æˆåŠŸæ•æ‰äº†èƒšèƒå¤§è„‘çš„å¿«é€Ÿè§£å‰–å­¦å‘è‚²è¿‡ç¨‹ã€‚æ‰€å¾—çš„å››ç»´äººç±»èƒšèƒå¤§è„‘å›¾è°±ä¸ºè¿™ä¸€å…³é”®æ—©æœŸç”Ÿå‘½é˜¶æ®µæä¾›äº†ç‹¬ç‰¹çš„è§è§£ï¼Œå¹¶å…·æœ‰æ”¹å–„äº§å‰ç¥ç»å‘è‚²éšœç¢æ£€æµ‹ã€é¢„é˜²å’Œæ²»ç–—çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07177v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯å¯¹æ—©æœŸèƒšèƒè„‘è¿›è¡Œå››ç»´åŠ¨æ€å›¾åƒç”Ÿæˆä¸åˆ†æï¼ŒæˆåŠŸæ„å»ºäº†é¦–ä¸ªå››äººç±»èƒšèƒè„‘å›¾è°±ã€‚ç ”ç©¶è§£å†³äº†è¶…å£°æ‰«ææŠ€æœ¯å¯¹èƒšèƒè„‘è§£å‰–ç»“æ„çš„å‡†ç¡®è§£æé—®é¢˜ï¼Œé€šè¿‡æ—¶é—´ä¾èµ–çš„åˆå§‹å›¾è°±ä¸åå·®æƒ©ç½šæœºåˆ¶ç¡®ä¿äº†èƒšèƒè„‘å‘è‚²çš„å¿«é€Ÿå˜åŒ–è¿‡ç¨‹ä¸­ä¿æŒç‰¹å®šçš„å¹´é¾„ç»“æ„ç‰¹å¾ã€‚è¯¥ç ”ç©¶æœ‰åŠ©äºæ·±å…¥ç†è§£èƒšèƒè„‘å‘è‚²è¿‡ç¨‹ï¼Œå¯¹äº§å‰ç¥ç»å‘è‚²éšœç¢çš„é¢„æµ‹å’Œæ²»ç–—å…·æœ‰æ½œåœ¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¼ºè°ƒäº†æ—©æœŸå¤§è„‘å‘è‚²å¯¹ç»ˆèº«ç¥ç»å‘è‚²å¥åº·çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰ä¸´åºŠå®è·µä¸­å¯¹èƒšèƒå¤§è„‘å‘è‚²çŸ¥è¯†äº†è§£çš„å±€é™æ€§ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯åˆ›å»ºäº†é¦–ä¸ªå››ç»´äººç±»èƒšèƒè„‘å›¾è°±ï¼Œä»¥æ·±å…¥äº†è§£æ­£å¸¸çš„å¤§è„‘å‘è‚²æƒ…å†µå¹¶è¯†åˆ«åå·®ã€‚</li>
<li>è¯¥ç ”ç©¶è§£å†³äº†è¶…å£°æ‰«ææŠ€æœ¯å¯¹èƒšèƒè„‘è§£å‰–ç»“æ„å‡†ç¡®è§£æçš„é—®é¢˜ï¼Œç¡®ä¿åœ¨èƒšèƒè„‘å¿«é€Ÿå‘è‚²è¿‡ç¨‹ä¸­ä¿æŒç‰¹å®šçš„å¹´é¾„ç»“æ„ç‰¹å¾ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨831å¼ ä¸‰ç»´è¶…å£°å›¾åƒè¿›è¡ŒéªŒè¯ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>ç ”ç©¶é€šè¿‡æ¶ˆèå®éªŒè¯æ˜ï¼ŒåŒ…å«æ—¶é—´ä¾èµ–çš„åˆå§‹å›¾è°±å’Œåå·®æƒ©ç½šæœºåˆ¶çš„æ–¹æ³•èƒ½ç”Ÿæˆè§£å‰–ç»“æ„å‡†ç¡®çš„å›¾è°±ã€‚</li>
<li>ä¸ç°æœ‰çš„ä½“å¤–èƒšèƒå›¾è°±çš„è§†è§‰æ¯”è¾ƒè¿›ä¸€æ­¥è¯å®äº†è¯¥å›¾è°±çš„è§£å‰–å­¦å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07177">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3917198b9d4d9ed294510a1ad603a05c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6ba7dc12f68542ca785240c9dcd09cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f378db077be671e0ed636cb4bc373e4f.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MIRAM-Masked-Image-Reconstruction-Across-Multiple-Scales-for-Breast-Lesion-Risk-Prediction"><a href="#MIRAM-Masked-Image-Reconstruction-Across-Multiple-Scales-for-Breast-Lesion-Risk-Prediction" class="headerlink" title="MIRAM: Masked Image Reconstruction Across Multiple Scales for Breast   Lesion Risk Prediction"></a>MIRAM: Masked Image Reconstruction Across Multiple Scales for Breast   Lesion Risk Prediction</h2><p><strong>Authors:Hung Q. Vo, Pengyu Yuan, Zheng Yin, Kelvin K. Wong, Chika F. Ezeana, Son T. Ly, Stephen T. C. Wong, Hien V. Nguyen</strong></p>
<p>Self-supervised learning (SSL) has garnered substantial interest within the machine learning and computer vision communities. Two prominent approaches in SSL include contrastive-based learning and self-distillation utilizing cropping augmentation. Lately, masked image modeling (MIM) has emerged as a more potent SSL technique, employing image inpainting as a pretext task. MIM creates a strong inductive bias toward meaningful spatial and semantic understanding. This has opened up new opportunities for SSL to contribute not only to classification tasks but also to more complex applications like object detection and image segmentation. Building upon this progress, our research paper introduces a scalable and practical SSL approach centered around more challenging pretext tasks that facilitate the acquisition of robust features. Specifically, we leverage multi-scale image reconstruction from randomly masked input images as the foundation for feature learning. Our hypothesis posits that reconstructing high-resolution images enables the model to attend to finer spatial details, particularly beneficial for discerning subtle intricacies within medical images. The proposed SSL features help improve classification performance on the Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) dataset. In pathology classification, our method demonstrates a 3% increase in average precision (AP) and a 1% increase in the area under the receiver operating characteristic curve (AUC) when compared to state-of-the-art (SOTA) algorithms. Moreover, in mass margins classification, our approach achieves a 4% increase in AP and a 2% increase in AUC. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨æœºå™¨å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸå¼•èµ·äº†æå¤§çš„å…³æ³¨ã€‚è‡ªç›‘ç£å­¦ä¹ çš„ä¸¤ç§ä¸»è¦æ–¹æ³•åŒ…æ‹¬åŸºäºå¯¹æ¯”å­¦ä¹ å’Œä½¿ç”¨è£å‰ªå¢å¼ºçš„è‡ªæˆ‘è’¸é¦ã€‚æœ€è¿‘ï¼ŒåŸºäºå›¾åƒè¡¥å…¨çš„é®ç½©å›¾åƒå»ºæ¨¡ï¼ˆMIMï¼‰ä½œä¸ºä¸€ç§æ›´å¼ºå¤§çš„SSLæŠ€æœ¯è€Œå‡ºç°ã€‚MIMå¯¹æœ‰æ„ä¹‰çš„ç©ºé—´å’Œè¯­ä¹‰ç†è§£äº§ç”Ÿäº†å¼ºçƒˆçš„å½’çº³åè§ã€‚è¿™ä¸ºSSLä¸ä»…ä¸ºåˆ†ç±»ä»»åŠ¡åšå‡ºè´¡çŒ®ï¼Œä¹Ÿä¸ºæ›´å¤æ‚çš„ä»»åŠ¡å¦‚ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ç­‰æä¾›äº†æ–°æœºä¼šã€‚åŸºäºè¿™ä¸€è¿›å±•ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è®ºæ–‡ä»‹ç»äº†ä¸€ç§å¯æ‰©å±•ä¸”å®ç”¨çš„SSLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä»¥æ›´å…·æŒ‘æˆ˜æ€§çš„é¢„æ–‡æœ¬ä»»åŠ¡ä¸ºä¸­å¿ƒï¼Œä¿ƒè¿›ç¨³å¥ç‰¹å¾çš„è·å–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨ä»éšæœºé®ç½©çš„è¾“å…¥å›¾åƒè¿›è¡Œå¤šå°ºåº¦å›¾åƒé‡å»ºä½œä¸ºç‰¹å¾å­¦ä¹ çš„åŸºç¡€ã€‚æˆ‘ä»¬çš„å‡è®¾æ˜¯ï¼Œé‡å»ºé«˜åˆ†è¾¨ç‡å›¾åƒå¯ä»¥ä½¿æ¨¡å‹å…³æ³¨æ›´ç²¾ç»†çš„ç©ºé—´ç»†èŠ‚ï¼Œè¿™å¯¹è¯†åˆ«åŒ»å­¦å›¾åƒä¸­çš„å¾®å¦™ç»†èŠ‚ç‰¹åˆ«æœ‰ç›Šã€‚æ‰€æå‡ºçš„SSLç‰¹å¾æœ‰åŠ©äºæ”¹è¿›æ•°å­—æ•°æ®åº“ç­›é€‰ä¹³è…ºæ‘„å½±æœ¯çš„ç²¾é€‰ä¹³è…ºæˆåƒå­é›†ï¼ˆCBIS-DDSMï¼‰æ•°æ®é›†ä¸Šçš„åˆ†ç±»æ€§èƒ½ã€‚åœ¨ç—…ç†åˆ†ç±»æ–¹é¢ï¼Œä¸æœ€å…ˆè¿›çš„ç®—æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹³å‡ç²¾ç¡®åº¦ä¸Šæé«˜äº†3%ï¼Œåœ¨æ¥æ”¶ç‰¹æ€§æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰ä¸Šæé«˜äº†1%ã€‚æ­¤å¤–ï¼Œåœ¨è‚¿å—è¾¹ç¼˜åˆ†ç±»ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å¹³å‡ç²¾ç¡®åº¦æé«˜4%ï¼ŒAUCæé«˜2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07157v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨æœºå™¨å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­çš„æœ€æ–°ç ”ç©¶è¿›å±•ã€‚é‡ç‚¹ä»‹ç»äº†ä¸€ç§æ–°å‹çš„SSLæ–¹æ³•ï¼Œé€šè¿‡å¤šå°ºåº¦å›¾åƒé‡å»ºä½œä¸ºç‰¹å¾å­¦ä¹ çš„åŸºç¡€ï¼Œåˆ©ç”¨éšæœºæ©ç è¾“å…¥å›¾åƒè¿›è¡Œé‡å»ºã€‚è¯¥æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¹³è…ºå›¾åƒåˆ†ç±»ã€ç—…ç†åˆ†ç±»å’Œè‚¿ç˜¤è¾¹ç¼˜åˆ†ç±»ç­‰æ–¹é¢ï¼Œç›¸è¾ƒäºç°æœ‰ç®—æ³•æœ‰æ‰€æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰å·²æˆä¸ºæœºå™¨å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„çƒ­é—¨ç ”ç©¶æ–¹å‘ã€‚</li>
<li>æ–°å‹SSLæ–¹æ³•åˆ©ç”¨å¤šå°ºåº¦å›¾åƒé‡å»ºä½œä¸ºç‰¹å¾å­¦ä¹ åŸºç¡€ï¼Œé€šè¿‡éšæœºæ©ç è¾“å…¥å›¾åƒè¿›è¡Œé‡å»ºã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆè·å–ç¨³å¥ç‰¹å¾ï¼Œä¸ä»…é€‚ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œè¿˜å¯åº”ç”¨äºæ›´å¤æ‚çš„åº”ç”¨ï¼Œå¦‚ç›®æ ‡æ£€æµ‹å’Œå›¾åƒåˆ†å‰²ã€‚</li>
<li>å‡è®¾é«˜åˆ†è¾¨ç‡å›¾åƒé‡å»ºèƒ½ä¿ƒä½¿æ¨¡å‹å…³æ³¨æ›´ç²¾ç»†çš„ç©ºé—´ç»†èŠ‚ï¼Œå¯¹è¾¨è¯†åŒ»å­¦å›¾åƒä¸­çš„å¾®å¦™ç»†èŠ‚ç‰¹åˆ«æœ‰ç›Šã€‚</li>
<li>åœ¨ä¹³è…ºå›¾åƒåˆ†ç±»æ–¹é¢ï¼Œæ–°æ–¹æ³•ç›¸è¾ƒäºç°æœ‰ç®—æ³•åœ¨å¹³å‡ç²¾åº¦ï¼ˆAPï¼‰ä¸Šæé«˜äº†3%ï¼Œåœ¨å—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰ä¸Šæé«˜äº†1%ã€‚</li>
<li>åœ¨ç—…ç†åˆ†ç±»æ–¹é¢ï¼Œæ–°æ–¹æ³•çš„APæé«˜äº†3%ï¼ŒAUCæé«˜äº†1%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9bc8d2fddaf1fa50c8a075799f98659c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9d631ed2a78ac9024cdf1fbb36a2a17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-35f3a4b21fd2d7429250056dca28e19e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39ef7d2764ed82b132127e7c2705af35.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4a8611053c212ed3fe35996230a147f.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Global-Context-Is-All-You-Need-for-Parallel-Efficient-Tractography-Parcellation"><a href="#Global-Context-Is-All-You-Need-for-Parallel-Efficient-Tractography-Parcellation" class="headerlink" title="Global Context Is All You Need for Parallel Efficient Tractography   Parcellation"></a>Global Context Is All You Need for Parallel Efficient Tractography   Parcellation</h2><p><strong>Authors:Valentin von Bornhaupt, Johannes GrÃ¼n, and Justus Bisten, Tobias Bauer, Theodor RÃ¼ber, Thomas Schultz</strong></p>
<p>Whole-brain tractography in diffusion MRI is often followed by a parcellation in which each streamline is classified as belonging to a specific white matter bundle, or discarded as a false positive. Efficient parcellation is important both in large-scale studies, which have to process huge amounts of data, and in the clinic, where computational resources are often limited. TractCloud is a state-of-the-art approach that aims to maximize accuracy with a local-global representation. We demonstrate that the local context does not contribute to the accuracy of that approach, and is even detrimental when dealing with pathological cases. Based on this observation, we propose PETParc, a new method for Parallel Efficient Tractography Parcellation. PETParc is a transformer-based architecture in which the whole-brain tractogram is randomly partitioned into sub-tractograms whose streamlines are classified in parallel, while serving as global context for each other. This leads to a speedup of up to two orders of magnitude relative to TractCloud, and permits inference even on clinical workstations without a GPU. PETParc accounts for the lack of streamline orientation either via a novel flip-invariant embedding, or by simply using flips as part of data augmentation. Despite the speedup, results are often even better than those of prior methods. The code and pretrained model will be made public upon acceptance. </p>
<blockquote>
<p>åœ¨æ‰©æ•£MRIçš„å…¨è„‘è½¨è¿¹æˆåƒåé€šå¸¸ä¼šè¿›è¡Œåˆ†åŒºï¼Œå°†æ¯æ¡æµçº¿åˆ†ç±»ä¸ºç‰¹å®šçš„ç™½è´¨æŸæˆ–å‰”é™¤ä¸ºå‡é˜³æ€§ã€‚åœ¨å¤§è§„æ¨¡ç ”ç©¶å’Œä¸´åºŠä¸­ï¼Œé«˜æ•ˆåˆ†åŒºéƒ½éå¸¸é‡è¦ï¼Œå› ä¸ºå¿…é¡»å¤„ç†å¤§é‡æ•°æ®ä¸”è®¡ç®—èµ„æºé€šå¸¸æœ‰é™ã€‚TractCloudæ˜¯ä¸€ç§å‰æ²¿æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å±€éƒ¨å…¨å±€è¡¨ç¤ºæ³•æ¥æœ€å¤§åŒ–å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå±€éƒ¨ä¸Šä¸‹æ–‡å¹¶ä¸æœ‰åŠ©äºè¯¥æ–¹æ³•çš„å‡†ç¡®æ€§ï¼Œåœ¨å¤„ç†ç—…ç†ç—…ä¾‹æ—¶ç”šè‡³æ˜¯æœ‰å®³çš„ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†PETParcï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¹¶è¡Œé«˜æ•ˆè½¨è¿¹åˆ†å‰²æ–¹æ³•ã€‚PETParcæ˜¯åŸºäºtransformerçš„æ¶æ„ï¼Œå°†å…¨è„‘è½¨è¿¹å›¾éšæœºåˆ†å‰²æˆå­è½¨è¿¹å›¾ï¼Œæµçº¿åˆ†ç±»å¹¶è¡Œè¿›è¡Œï¼ŒåŒæ—¶ç›¸äº’ä½œä¸ºå…¨å±€ä¸Šä¸‹æ–‡ã€‚è¿™å¯¼è‡´é€Ÿåº¦æé«˜äº†ä¸¤ä¸ªæ•°é‡çº§ï¼Œç›¸å¯¹äºTractCloudï¼Œç”šè‡³å¯ä»¥åœ¨æ²¡æœ‰GPUçš„ä¸´åºŠå·¥ä½œç«™ä¸Šè¿›è¡Œæ¨æ–­ã€‚PETParcé€šè¿‡æ–°å‹ç¿»è½¬ä¸å˜åµŒå…¥æˆ–ä»…ä½¿ç”¨ç¿»è½¬ä½œä¸ºæ•°æ®æ‰©å……çš„ä¸€éƒ¨åˆ†æ¥è§£å†³æµçº¿æ–¹å‘ç¼ºå¤±çš„é—®é¢˜ã€‚å°½ç®¡åŠ å¿«äº†é€Ÿåº¦ï¼Œä½†ç»“æœå¾€å¾€æ¯”ä»¥å¾€çš„æ–¹æ³•æ›´å¥½ã€‚è®ºæ–‡æ¥å—åå°†å…¬å¼€ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07104v1">PDF</a> 8 pages, 2 pages references, 3 figures, 2 tables</p>
<p><strong>Summary</strong><br>     å…¨æ–°å¤§è„‘çº¤ç»´è¿½è¸ªæ–¹æ³•PETParcæå‡ºï¼Œé‡‡ç”¨å¹¶è¡Œé«˜æ•ˆè¿½è¸ªåˆ†å‰²æŠ€æœ¯ï¼ŒåŸºäºtransformeræ¶æ„ï¼Œéšæœºåˆ†å‰²å…¨è„‘çº¤ç»´å›¾æˆå­çº¤ç»´å›¾å¹¶è¡Œå¤„ç†ï¼Œç›¸äº’ä½œä¸ºå…¨å±€èƒŒæ™¯ï¼Œå¤§å¹…æé«˜å¤„ç†é€Ÿåº¦ï¼Œæ”¯æŒä¸´åºŠå·¥ä½œç«™æ— GPUæ¨ç†ã€‚è§£å†³å› æµçº¿æ–¹å‘ç¼ºå¤±çš„é—®é¢˜ï¼Œé€šè¿‡æ–°å‹ç¿»è½¬ä¸å˜åµŒå…¥æˆ–åˆ©ç”¨ç¿»è½¬è¿›è¡Œæ•°æ®å¢å¼ºå®ç°ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PETParcæ˜¯ä¸€ç§åŸºäºå¹¶è¡Œé«˜æ•ˆè¿½è¸ªåˆ†å‰²æŠ€æœ¯çš„æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜å…¨è„‘çº¤ç»´è¿½è¸ªå›¾çš„åˆ†å‰²æ•ˆç‡ã€‚</li>
<li>PETParcé‡‡ç”¨transformeræ¶æ„ï¼Œéšæœºåˆ†å‰²å…¨è„‘çº¤ç»´å›¾ä¸ºå­çº¤ç»´å›¾ï¼Œè¿›è¡Œå¹¶è¡Œå¤„ç†ï¼Œç›¸äº’ä½œä¸ºå…¨å±€èƒŒæ™¯ã€‚</li>
<li>PETParcå¤„ç†é€Ÿåº¦å¤§å¹…æé«˜ï¼Œå¯è¾¾åˆ°TractCloudçš„ä¸¤ç™¾å€é€Ÿï¼Œä¸”èƒ½åœ¨æ— GPUçš„ä¸´åºŠå·¥ä½œç«™è¿›è¡Œæ¨ç†ã€‚</li>
<li>PETParcè§£å†³äº†å› æµçº¿æ–¹å‘ç¼ºå¤±çš„é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡æ–°å‹ç¿»è½¬ä¸å˜åµŒå…¥æˆ–åˆ©ç”¨ç¿»è½¬è¿›è¡Œæ•°æ®å¢å¼ºå®ç°ã€‚</li>
<li>PETParcç›¸å¯¹äºä¹‹å‰çš„æ–¹æ³•ï¼Œç»“æœå¾€å¾€æ›´å¥½ã€‚</li>
<li>PETParcå…¬å¼€ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ä¾›ç”¨æˆ·ä½¿ç”¨å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b2adee16eaf48fc42b58b38d81b9c09a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1dc9640c07d2626db74be0e65f0cf47a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f337e1140daae5a2fbdedcfe43e9d8b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="OmniSAM-Omnidirectional-Segment-Anything-Model-for-UDA-in-Panoramic-Semantic-Segmentation"><a href="#OmniSAM-Omnidirectional-Segment-Anything-Model-for-UDA-in-Panoramic-Semantic-Segmentation" class="headerlink" title="OmniSAM: Omnidirectional Segment Anything Model for UDA in Panoramic   Semantic Segmentation"></a>OmniSAM: Omnidirectional Segment Anything Model for UDA in Panoramic   Semantic Segmentation</h2><p><strong>Authors:Ding Zhong, Xu Zheng, Chenfei Liao, Yuanhuiyi Lyu, Jialei Chen, Shengyang Wu, Linfeng Zhang, Xuming Hu</strong></p>
<p>Segment Anything Model 2 (SAM2) has emerged as a strong base model in various pinhole imaging segmentation tasks. However, when applying it to $360^\circ$ domain, the significant field-of-view (FoV) gap between pinhole ($70^\circ \times 70^\circ$) and panoramic images ($180^\circ \times 360^\circ$) poses unique challenges. Two major concerns for this application includes 1) inevitable distortion and object deformation brought by the large FoV disparity between domains; 2) the lack of pixel-level semantic understanding that the original SAM2 cannot provide. To address these issues, we propose a novel OmniSAM framework, which makes the first attempt to apply SAM2 for panoramic semantic segmentation. Specifically, to bridge the first gap, OmniSAM first divides the panorama into sequences of patches. These patches are then treated as image sequences in similar manners as in video segmentation tasks. We then leverage the SAM2â€™s memory mechanism to extract cross-patch correspondences that embeds the cross-FoV dependencies, improving feature continuity and the prediction consistency along mask boundaries. For the second gap, OmniSAM fine-tunes the pretrained image encoder and reutilize the mask decoder for semantic prediction. An FoV-based prototypical adaptation module with dynamic pseudo label update mechanism is also introduced to facilitate the alignment of memory and backbone features, thereby improving model generalization ability across different sizes of source models. Extensive experimental results demonstrate that OmniSAM outperforms the state-of-the-art methods by large margins, e.g., 79.06% (+10.22%) on SPin8-to-SPan8, 62.46% (+6.58%) on CS13-to-DP13. </p>
<blockquote>
<p>Segment Anything Model 2ï¼ˆSAM2ï¼‰åœ¨å„ç§é’ˆå­”æˆåƒåˆ†å‰²ä»»åŠ¡ä¸­å·²æˆä¸ºå¼ºå¤§çš„åŸºç¡€æ¨¡å‹ã€‚ç„¶è€Œï¼Œå°†å…¶åº”ç”¨äº$360^\circ$åŸŸæ—¶ï¼Œé’ˆå­”ï¼ˆ$70^\circ \times 70^\circ$ï¼‰å’Œå…¨æ™¯å›¾åƒï¼ˆ$180^\circ \times 360^\circ$ï¼‰ä¹‹é—´çš„è§†åœºï¼ˆFoVï¼‰å·®è·å·¨å¤§ï¼Œå¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚å¯¹æ­¤åº”ç”¨çš„ä¸»è¦æ‹…å¿§åŒ…æ‹¬ï¼š1ï¼‰ç”±åŸŸä¹‹é—´çš„å¤§è§†åœºå·®å¼‚å¸¦æ¥çš„ä¸å¯é¿å…çš„å½¢çŠ¶æ‰­æ›²å’Œç‰©ä½“å˜å½¢ï¼›2ï¼‰åŸå§‹SAM2æ— æ³•æä¾›åƒç´ çº§çš„è¯­ä¹‰ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨æ–°çš„OmniSAMæ¡†æ¶ï¼Œå®ƒé¦–æ¬¡å°è¯•å°†SAM2åº”ç”¨äºå…¨æ™¯è¯­ä¹‰åˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å¼¥è¡¥ç¬¬ä¸€ä¸ªå·®è·ï¼ŒOmniSAMé¦–å…ˆæŠŠå…¨æ™¯åˆ†æˆä¸€ç³»åˆ—çš„è¡¥ä¸ã€‚è¿™äº›è¡¥ä¸ç„¶åä»¥ç±»ä¼¼äºè§†é¢‘åˆ†å‰²ä»»åŠ¡çš„æ–¹å¼ä½œä¸ºå›¾åƒåºåˆ—è¿›è¡Œå¤„ç†ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨SAM2çš„è®°å¿†æœºåˆ¶æ¥æå–è·¨è¡¥ä¸å¯¹åº”å…³ç³»ï¼Œè¿™äº›å¯¹åº”å…³ç³»åµŒå…¥äº†è·¨è§†åœºçš„ä¾èµ–å…³ç³»ï¼Œæé«˜äº†ç‰¹å¾è¿ç»­æ€§å’Œæ©è†œè¾¹ç•Œçš„é¢„æµ‹ä¸€è‡´æ€§ã€‚å¯¹äºç¬¬äºŒä¸ªå·®è·ï¼ŒOmniSAMå¯¹é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨è¿›è¡Œå¾®è°ƒï¼Œå¹¶é‡æ–°ä½¿ç”¨æ©è†œè§£ç å™¨è¿›è¡Œè¯­ä¹‰é¢„æµ‹ã€‚è¿˜å¼•å…¥äº†ä¸€ä¸ªåŸºäºè§†åœºçš„åŸå‹è‡ªé€‚åº”æ¨¡å—ï¼Œå¸¦æœ‰åŠ¨æ€ä¼ªæ ‡ç­¾æ›´æ–°æœºåˆ¶ï¼Œä»¥ä¿ƒè¿›è®°å¿†å’Œä¸»å¹²ç‰¹å¾çš„å¯¹é½ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨ä¸åŒå¤§å°æºæ¨¡å‹ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼ŒOmniSAMå¤§å¤§è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä¾‹å¦‚åœ¨SPin8-to-SPan8ä¸Šè¾¾åˆ°79.06%ï¼ˆ+10.22%ï¼‰ï¼Œåœ¨CS13-to-DP13ä¸Šè¾¾åˆ°62.46%ï¼ˆ+6.58%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07098v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å…¨æ™¯å›¾åƒè¯­ä¹‰åˆ†å‰²ä¸­ï¼ŒSAM2æ¨¡å‹å­˜åœ¨è§†é‡ï¼ˆFoVï¼‰å·®è·å’Œåƒç´ çº§è¯­ä¹‰ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºOmniSAMæ¡†æ¶ï¼Œé€šè¿‡åˆ†å‰²å…¨æ™¯å›¾åƒä¸ºåºåˆ—è¡¥ä¸ï¼Œåˆ©ç”¨SAM2çš„è®°å¿†æœºåˆ¶æå–è·¨è¡¥ä¸å¯¹åº”å…³ç³»ï¼Œæ”¹å–„ç‰¹å¾è¿ç»­æ€§å’Œé¢„æµ‹ä¸€è‡´æ€§ã€‚åŒæ—¶ï¼Œå¾®è°ƒé¢„è®­ç»ƒå›¾åƒç¼–ç å™¨å¹¶é‡æ–°åˆ©ç”¨æ©è†œè§£ç å™¨è¿›è¡Œè¯­ä¹‰é¢„æµ‹ï¼Œå¼•å…¥åŸºäºè§†é‡çš„åŸå‹é€‚åº”æ¨¡å—å’ŒåŠ¨æ€ä¼ªæ ‡ç­¾æ›´æ–°æœºåˆ¶ï¼Œæé«˜æ¨¡å‹åœ¨ä¸åŒå°ºå¯¸æºæ¨¡å‹ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜OmniSAMå¤§å¹…ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SAM2æ¨¡å‹åœ¨å…¨æ™¯æˆåƒè¯­ä¹‰åˆ†å‰²ä¸­é¢ä¸´è§†é‡å·®è·å’Œåƒç´ çº§è¯­ä¹‰ç†è§£ä¸è¶³çš„æŒ‘æˆ˜ã€‚</li>
<li>OmniSAMæ¡†æ¶æå‡ºå°†å…¨æ™¯å›¾åƒåˆ†å‰²æˆåºåˆ—è¡¥ä¸ï¼Œä»¥ç¼©å°è§†é‡å·®è·ã€‚</li>
<li>OmniSAMåˆ©ç”¨SAM2çš„è®°å¿†æœºåˆ¶æå–è·¨è¡¥ä¸å¯¹åº”å…³ç³»ï¼Œæ”¹å–„ç‰¹å¾è¿ç»­æ€§å’Œé¢„æµ‹ä¸€è‡´æ€§ã€‚</li>
<li>OmniSAMé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒå›¾åƒç¼–ç å™¨å’Œé‡æ–°åˆ©ç”¨æ©è†œè§£ç å™¨æ¥åº”å¯¹åƒç´ çº§è¯­ä¹‰ç†è§£ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>OmniSAMå¼•å…¥åŸºäºè§†é‡çš„åŸå‹é€‚åº”æ¨¡å—ï¼Œæé«˜æ¨¡å‹åœ¨ä¸åŒå°ºå¯¸æºæ¨¡å‹ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åŠ¨æ€ä¼ªæ ‡ç­¾æ›´æ–°æœºåˆ¶è¢«ç”¨äºä¿ƒè¿›è®°å¿†å’Œä¸»å¹²ç‰¹å¾çš„å¯¹é½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºOmniSAMå¤§å¹…ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¦‚åœ¨SPin8-to-SPan8ä¸Šè¾¾åˆ°79.06%ï¼ˆ+10.22%ï¼‰ï¼Œåœ¨CS13-to-DP13ä¸Šè¾¾åˆ°62.46%ï¼ˆ+6.58%ï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07098">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8da470009b4d1f6300d8bc79263ebebe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-777a52325caf7fceba79cc17e13046d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ca8b89b107ef901d1c3c62e14a6806e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d627d8ece8e0e4bc2af1aed8ebe55d97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a70f0cd429f8be186ee31e46e0c634b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-644c624b22c9d4340ace6db6bc7f3da4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Multimodal-Human-AI-Synergy-for-Medical-Imaging-Quality-Control-A-Hybrid-Intelligence-Framework-with-Adaptive-Dataset-Curation-and-Closed-Loop-Evaluation"><a href="#Multimodal-Human-AI-Synergy-for-Medical-Imaging-Quality-Control-A-Hybrid-Intelligence-Framework-with-Adaptive-Dataset-Curation-and-Closed-Loop-Evaluation" class="headerlink" title="Multimodal Human-AI Synergy for Medical Imaging Quality Control: A   Hybrid Intelligence Framework with Adaptive Dataset Curation and Closed-Loop   Evaluation"></a>Multimodal Human-AI Synergy for Medical Imaging Quality Control: A   Hybrid Intelligence Framework with Adaptive Dataset Curation and Closed-Loop   Evaluation</h2><p><strong>Authors:Zhi Qin, Qianhui Gui, Mouxiao Bian, Rui Wang, Hong Ge, Dandan Yao, Ziying Sun, Yuan Zhao, Yu Zhang, Hui Shi, Dongdong Wang, Chenxin Song, Shenghong Ju, Lihao Liu, Junjun He, Jie Xu, Yuan-Cheng Wang</strong></p>
<p>Medical imaging quality control (QC) is essential for accurate diagnosis, yet traditional QC methods remain labor-intensive and subjective. To address this challenge, in this study, we establish a standardized dataset and evaluation framework for medical imaging QC, systematically assessing large language models (LLMs) in image quality assessment and report standardization. Specifically, we first constructed and anonymized a dataset of 161 chest X-ray (CXR) radiographs and 219 CT reports for evaluation. Then, multiple LLMs, including Gemini 2.0-Flash, GPT-4o, and DeepSeek-R1, were evaluated based on recall, precision, and F1 score to detect technical errors and inconsistencies. Experimental results show that Gemini 2.0-Flash achieved a Macro F1 score of 90 in CXR tasks, demonstrating strong generalization but limited fine-grained performance. DeepSeek-R1 excelled in CT report auditing with a 62.23% recall rate, outperforming other models. However, its distilled variants performed poorly, while InternLM2.5-7B-chat exhibited the highest additional discovery rate, indicating broader but less precise error detection. These findings highlight the potential of LLMs in medical imaging QC, with DeepSeek-R1 and Gemini 2.0-Flash demonstrating superior performance. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒè´¨é‡æ§åˆ¶ï¼ˆQCï¼‰å¯¹äºå‡†ç¡®è¯Šæ–­è‡³å…³é‡è¦ï¼Œç„¶è€Œä¼ ç»Ÿçš„QCæ–¹æ³•ä»ç„¶åŠ³åŠ¨å¼ºåº¦é«˜ä¸”ä¸»è§‚ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæœ¬ç ”ç©¶å»ºç«‹äº†åŒ»å­¦å½±åƒQCçš„æ ‡å‡†æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ï¼Œå¯¹å›¾åƒè´¨é‡è¯„ä¼°å’ŒæŠ¥å‘Šæ ‡å‡†åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆæ„å»ºå¹¶åŒ¿ååŒ–äº†ä¸€ä¸ªåŒ…å«161å¼ èƒ¸éƒ¨Xå°„çº¿ï¼ˆCXRï¼‰æ”¾å°„ç…§ç‰‡å’Œ219ä»½CTæŠ¥å‘Šçš„æ•°æ®é›†ç”¨äºè¯„ä¼°ã€‚ç„¶åï¼ŒåŸºäºå¬å›ç‡ã€ç²¾ç¡®åº¦å’ŒF1åˆ†æ•°ï¼Œå¯¹åŒ…æ‹¬Gemini 2.0-Flashã€GPT-4oå’ŒDeepSeek-R1åœ¨å†…çš„å¤šä¸ªLLMsè¿›è¡Œäº†æŠ€æœ¯é”™è¯¯å’Œä¸ä¸€è‡´æ€§çš„æ£€æµ‹è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGemini 2.0-Flashåœ¨CXRä»»åŠ¡ä¸­è·å¾—äº†90çš„å®è§‚F1åˆ†æ•°ï¼Œè¡¨ç°å‡ºè¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ä½†ç²¾ç»†ç²’åº¦æ€§èƒ½æœ‰é™ã€‚DeepSeek-R1åœ¨CTæŠ¥å‘Šå®¡æ ¸æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¬å›ç‡è¾¾åˆ°62.23%ï¼Œä¼˜äºå…¶ä»–æ¨¡å‹ã€‚ç„¶è€Œï¼Œå…¶è’¸é¦å˜ä½“è¡¨ç°ä¸ä½³ï¼Œè€ŒInternLM2.5-7B-chatè¡¨ç°å‡ºæœ€é«˜çš„é¢å¤–å‘ç°ç‡ï¼Œè¡¨æ˜å…¶é”™è¯¯æ£€æµ‹èŒƒå›´æ›´å¹¿ä½†ç²¾ç¡®åº¦è¾ƒä½ã€‚è¿™äº›å‘ç°çªæ˜¾äº†LLMsåœ¨åŒ»å­¦å½±åƒQCä¸­çš„æ½œåŠ›ï¼Œå…¶ä¸­DeepSeek-R1å’ŒGemini 2.0-Flashè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07032v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†åŒ»å­¦æˆåƒè´¨é‡æ§åˆ¶ï¼ˆQCï¼‰çš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå»ºç«‹äº†ä¸€å¥—æ ‡å‡†åŒ–çš„æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒè´¨é‡è¯„ä¼°å’ŒæŠ¥å‘Šæ ‡å‡†åŒ–æ–¹é¢çš„æ€§èƒ½ã€‚é€šè¿‡æ„å»ºåŒ¿åæ•°æ®é›†å’Œæµ‹è¯•å¤šç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå‘ç°æŸäº›æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸ºåŒ»å­¦æˆåƒQCæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒè´¨é‡æ§åˆ¶å¯¹å‡†ç¡®è¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†ä¼ ç»Ÿæ–¹æ³•åŠ³åŠ¨å¼ºåº¦å¤§ã€ä¸»è§‚æ€§å¼ºã€‚</li>
<li>ç ”ç©¶å»ºç«‹äº†ä¸€å¥—æ ‡å‡†åŒ–çš„æ•°æ®é›†å’Œè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºåŒ»å­¦æˆåƒQCã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å›¾åƒè´¨é‡è¯„ä¼°å’ŒæŠ¥å‘Šæ ‡å‡†åŒ–æ–¹é¢è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚</li>
<li>Gemini 2.0-Flashåœ¨CXRä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºæ³›åŒ–èƒ½åŠ›ï¼Œä½†ç²¾ç»†ç²’åº¦æ€§èƒ½æœ‰é™ã€‚</li>
<li>DeepSeek-R1åœ¨CTæŠ¥å‘Šå®¡æ ¸æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰è¾ƒé«˜çš„å¬å›ç‡ã€‚</li>
<li>InternLM2.5-7B-chatå…·æœ‰æ›´å¹¿æ³›çš„é”™è¯¯æ£€æµ‹èƒ½åŠ›ï¼Œä½†ç²¾ç¡®æ€§è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5e73cd5198e7857542fcc528d14e2f8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c7dc12b8913f18a800fb426c7ea8c4ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97086753f39879d7dd051c2d37d8c93b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f161d66af0dcfed4163e75d36e7edcc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9649930ce86a23ce8395f24baee82868.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Task-Specific-Knowledge-Distillation-from-the-Vision-Foundation-Model-for-Enhanced-Medical-Image-Segmentation"><a href="#Task-Specific-Knowledge-Distillation-from-the-Vision-Foundation-Model-for-Enhanced-Medical-Image-Segmentation" class="headerlink" title="Task-Specific Knowledge Distillation from the Vision Foundation Model   for Enhanced Medical Image Segmentation"></a>Task-Specific Knowledge Distillation from the Vision Foundation Model   for Enhanced Medical Image Segmentation</h2><p><strong>Authors:Pengchen Liang, Haishan Huang, Bin Pu, Jianguo Chen, Xiang Hua, Jing Zhang, Weibo Ma, Zhuangzhuang Chen, Yiwei Li, Qing Chang</strong></p>
<p>Large-scale pre-trained models, such as Vision Foundation Models (VFMs), have demonstrated impressive performance across various downstream tasks by transferring generalized knowledge, especially when target data is limited. However, their high computational cost and the domain gap between natural and medical images limit their practical application in medical segmentation tasks. Motivated by this, we pose the following important question: â€œHow can we effectively utilize the knowledge of large pre-trained VFMs to train a small, task-specific model for medical image segmentation when training data is limited?â€ To address this problem, we propose a novel and generalizable task-specific knowledge distillation framework. Our method fine-tunes the VFM on the target segmentation task to capture task-specific features before distilling the knowledge to smaller models, leveraging Low-Rank Adaptation (LoRA) to reduce the computational cost of fine-tuning. Additionally, we incorporate synthetic data generated by diffusion models to augment the transfer set, enhancing model performance in data-limited scenarios. Experimental results across five medical image datasets demonstrate that our method consistently outperforms task-agnostic knowledge distillation and self-supervised pretraining approaches like MoCo v3 and Masked Autoencoders (MAE). For example, on the KidneyUS dataset, our method achieved a 28% higher Dice score than task-agnostic KD using 80 labeled samples for fine-tuning. On the CHAOS dataset, it achieved an 11% improvement over MAE with 100 labeled samples. These results underscore the potential of task-specific knowledge distillation to train accurate, efficient models for medical image segmentation in data-constrained settings. </p>
<blockquote>
<p>å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ï¼Œé€šè¿‡è¿ç§»é€šç”¨çŸ¥è¯†ï¼Œåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å±•ç°äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ç›®æ ‡æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„é«˜è®¡ç®—æˆæœ¬å’Œè‡ªç„¶å›¾åƒä¸åŒ»å­¦å›¾åƒä¹‹é—´çš„é¢†åŸŸå·®è·é™åˆ¶äº†å®ƒä»¬åœ¨åŒ»å­¦åˆ†å‰²ä»»åŠ¡ä¸­çš„å®é™…åº”ç”¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä»¥ä¸‹é‡è¦é—®é¢˜ï¼šâ€œå½“è®­ç»ƒæ•°æ®æœ‰é™æ—¶ï¼Œå¦‚ä½•æœ‰æ•ˆåˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒVFMsçš„çŸ¥è¯†æ¥è®­ç»ƒä¸€ä¸ªå°è€Œç‰¹å®šçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Ÿâ€ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ä¸”é€šç”¨çš„ä»»åŠ¡ç‰¹å®šçŸ¥è¯†è’¸é¦æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåœ¨ç›®æ ‡åˆ†å‰²ä»»åŠ¡ä¸Šå¾®è°ƒVFMï¼Œä»¥æ•è·ä»»åŠ¡ç‰¹å®šç‰¹å¾ï¼Œç„¶åå°†çŸ¥è¯†è’¸é¦åˆ°è¾ƒå°çš„æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨ä½é˜¶é€‚åº”ï¼ˆLoRAï¼‰æ¥é™ä½å¾®è°ƒçš„è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®æ¥å¢å¼ºè½¬ç§»é›†ï¼Œæé«˜æ¨¡å‹åœ¨æ•°æ®æœ‰é™åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚åœ¨äº”ä¸ªåŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸€ç›´ä¼˜äºä»»åŠ¡æ— å…³çš„çŸ¥è¯†è’¸é¦å’Œè‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ï¼Œå¦‚MoCo v3å’Œæ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨KidneyUSæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨80ä¸ªæ ‡è®°æ ·æœ¬è¿›è¡Œå¾®è°ƒæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¾—åˆ°çš„Diceå¾—åˆ†æ¯”ä»»åŠ¡æ— å…³KDé«˜å‡º28%ã€‚åœ¨CHAOSæ•°æ®é›†ä¸Šï¼Œä½¿ç”¨100ä¸ªæ ‡è®°æ ·æœ¬æ—¶ï¼Œå®ƒæ¯”MAEæé«˜äº†11%ã€‚è¿™äº›ç»“æœçªå‡ºäº†ä»»åŠ¡ç‰¹å®šçŸ¥è¯†è’¸é¦åœ¨æ•°æ®å—é™ç¯å¢ƒä¸­è®­ç»ƒåŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®ã€é«˜æ•ˆæ¨¡å‹çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06976v1">PDF</a> 29 pages, 10 figures, 16 tables</p>
<p><strong>æ‘˜è¦</strong><br>    åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚è§†è§‰åŸºç¡€æ¨¡å‹ï¼‰çš„é€šç”¨çŸ¥è¯†ï¼Œæå‡ºä¸€ç§é’ˆå¯¹åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡çš„æœ‰æ•ˆçŸ¥è¯†è’¸é¦æ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨ç›®æ ‡åˆ†å‰²ä»»åŠ¡ä¸Šå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ•è·ä»»åŠ¡ç‰¹å®šç‰¹å¾ï¼Œå¹¶å°†çŸ¥è¯†è’¸é¦åˆ°å°å‹æ¨¡å‹ä¸­ã€‚é€šè¿‡ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰é™ä½å¾®è°ƒçš„è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®å¢å¼ºè½¬ç§»é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨äº”ä¸ªåŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šçš„æ€§èƒ½å‡ä¼˜äºä»»åŠ¡æ— å…³çš„çŸ¥è¯†è’¸é¦å’Œè‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ï¼Œå¦‚MoCo v3å’Œæ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMsï¼‰ï¼Œåœ¨è¿ç§»å­¦ä¹ æ—¶è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå°¤å…¶åœ¨ç›®æ ‡æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚</li>
<li>é’ˆå¯¹åŒ»ç–—å›¾åƒåˆ†å‰²ä»»åŠ¡ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†æ˜¯å…³é”®ã€‚</li>
<li>æå‡ºä¸€ç§æ–°å‹çš„ä»»åŠ¡ç‰¹å®šçŸ¥è¯†è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹æ¥æ•è·ä»»åŠ¡ç‰¹å®šç‰¹å¾ï¼Œå¹¶å°†çŸ¥è¯†ä¼ é€’ç»™å°å‹æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰é™ä½å¾®è°ƒçš„è®¡ç®—æˆæœ¬ï¼Œæé«˜æ¨¡å‹çš„å®ç”¨æ€§ã€‚</li>
<li>ç»“åˆæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„åˆæˆæ•°æ®å¢å¼ºè½¬ç§»é›†ï¼Œæé«˜æ¨¡å‹åœ¨æ•°æ®æœ‰é™åœºæ™¯ä¸­çš„æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªåŒ»ç–—å›¾åƒæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºä»»åŠ¡æ— å…³çš„çŸ¥è¯†è’¸é¦å’Œè‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>åœ¨è‚¾è„USå’ŒCHAOSæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æœ‰é™çš„æ ‡æ³¨æ•°æ®ä¸‹å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-58ae457d9ee1c9832fe38b8fea10f2c1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec3eee9ac1b4bb9c2f9a032481f8d9b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ce061ede3e7180fe56d5a79c0ac96a6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Interactive-Medical-Image-Analysis-with-Concept-based-Similarity-Reasoning"><a href="#Interactive-Medical-Image-Analysis-with-Concept-based-Similarity-Reasoning" class="headerlink" title="Interactive Medical Image Analysis with Concept-based Similarity   Reasoning"></a>Interactive Medical Image Analysis with Concept-based Similarity   Reasoning</h2><p><strong>Authors:Ta Duc Huy, Sen Kim Tran, Phan Nguyen, Nguyen Hoang Tran, Tran Bao Sam, Anton van den Hengel, Zhibin Liao, Johan W. Verjans, Minh-Son To, Vu Minh Hieu Phan</strong></p>
<p>The ability to interpret and intervene model decisions is important for the adoption of computer-aided diagnosis methods in clinical workflows. Recent concept-based methods link the model predictions with interpretable concepts and modify their activation scores to interact with the model. However, these concepts are at the image level, which hinders the model from pinpointing the exact patches the concepts are activated. Alternatively, prototype-based methods learn representations from training image patches and compare these with test image patches, using the similarity scores for final class prediction. However, interpreting the underlying concepts of these patches can be challenging and often necessitates post-hoc guesswork. To address this issue, this paper introduces the novel Concept-based Similarity Reasoning network (CSR), which offers (i) patch-level prototype with intrinsic concept interpretation, and (ii) spatial interactivity. First, the proposed CSR provides localized explanation by grounding prototypes of each concept on image regions. Second, our model introduces novel spatial-level interaction, allowing doctors to engage directly with specific image areas, making it an intuitive and transparent tool for medical imaging. CSR improves upon prior state-of-the-art interpretable methods by up to 4.5% across three biomedical datasets. Our code is released at <a target="_blank" rel="noopener" href="https://github.com/tadeephuy/InteractCSR">https://github.com/tadeephuy/InteractCSR</a>. </p>
<blockquote>
<p>è§£é‡Šå’Œå¹²é¢„æ¨¡å‹å†³ç­–çš„èƒ½åŠ›å¯¹äºåœ¨ä¸´åºŠå·¥ä½œæµç¨‹ä¸­é‡‡ç”¨è®¡ç®—æœºè¾…åŠ©è¯Šæ–­æ–¹æ³•è‡³å…³é‡è¦ã€‚æœ€è¿‘çš„æ¦‚å¿µçº§æ–¹æ³•å°†æ¨¡å‹é¢„æµ‹ä¸å¯è§£é‡Šçš„æ¦‚å¿µè”ç³»èµ·æ¥ï¼Œå¹¶ä¿®æ”¹å…¶æ¿€æ´»åˆ†æ•°ä»¥ä¸æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œè¿™äº›æ¦‚å¿µä»…é™äºå›¾åƒå±‚é¢ï¼Œé˜»ç¢äº†æ¨¡å‹ç²¾ç¡®è¯†åˆ«æ¦‚å¿µè¢«æ¿€æ´»çš„ç¡®åˆ‡åŒºåŸŸã€‚å¦ä¸€ç§åŸºäºåŸå‹çš„æ–¹æ³•ä»è®­ç»ƒå›¾åƒå—ä¸­å­¦ä¹ è¡¨ç¤ºï¼Œå¹¶å°†å…¶ä¸æµ‹è¯•å›¾åƒå—è¿›è¡Œæ¯”è¾ƒï¼Œä½¿ç”¨ç›¸ä¼¼åº¦åˆ†æ•°è¿›è¡Œæœ€ç»ˆçš„ç±»åˆ«é¢„æµ‹ã€‚ç„¶è€Œï¼Œè§£é‡Šè¿™äº›å›¾åƒå—çš„æ½œåœ¨æ¦‚å¿µå¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œé€šå¸¸éœ€è¦äº‹åçŒœæµ‹ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†æ–°å‹çš„æ¦‚å¿µç›¸ä¼¼æ€§æ¨ç†ç½‘ç»œï¼ˆCSRï¼‰ï¼Œå®ƒæä¾›äº†ï¼ˆiï¼‰å›¾åƒå—çº§çš„åŸå‹ä¸å†…åœ¨æ¦‚å¿µè§£é‡Šï¼Œï¼ˆiiï¼‰ç©ºé—´äº¤äº’æ€§ã€‚é¦–å…ˆï¼Œæ‰€æå‡ºçš„CSRé€šè¿‡åœ¨å›¾åƒåŒºåŸŸä¸Šä¸ºæ¯ä¸ªæ¦‚å¿µæä¾›åŸå‹æ¥æä¾›å±€éƒ¨è§£é‡Šã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¼•å…¥äº†æ–°å‹çš„ç©ºé—´çº§åˆ«äº¤äº’ï¼Œä½¿åŒ»ç”Ÿèƒ½å¤Ÿç›´æ¥ä¸ç‰¹å®šçš„å›¾åƒåŒºåŸŸè¿›è¡Œäº¤äº’ï¼Œä½¿å…¶æˆä¸ºåŒ»å­¦å½±åƒçš„ç›´è§‚å’Œé€æ˜å·¥å…·ã€‚CSRåœ¨ä¸‰ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šè¾ƒä¹‹å‰çš„æœ€å…ˆè¿›çš„å¯è§£é‡Šæ–¹æ³•æé«˜äº†é«˜è¾¾4.5%ã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/tadeephuy/InteractCSR%E3%80%82">https://github.com/tadeephuy/InteractCSRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06873v1">PDF</a> Accepted CVPR2025</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒé¢†åŸŸä¸­çš„æ–°æ¦‚å¿µâ€”â€”åŸºäºç›¸ä¼¼æ€§çš„æ¦‚å¿µæ¨ç†ç½‘ç»œï¼ˆCSRï¼‰ã€‚CSRå¯å®ç°ï¼šï¼ˆä¸€ï¼‰åŒºåŸŸåŒ–çš„æ¦‚å¿µè§£é‡Šï¼Œå³å®šä½åˆ°å›¾åƒåŒºåŸŸçš„æ¦‚å¿µåŸå‹ï¼›ï¼ˆäºŒï¼‰ç©ºé—´äº’åŠ¨ï¼Œä½¿å¾—åŒ»ç”Ÿå¯ç›´è§‚åœ°ç›´æ¥ä¸ç‰¹å®šå›¾åƒåŒºåŸŸäº’åŠ¨ã€‚è¯¥æ–¹æ³•æé«˜äº†åŒ»å­¦å›¾åƒè¯Šæ–­çš„å‡†ç¡®æ€§å’Œé€æ˜åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©è¯Šæ–­æ–¹æ³•çš„é‡‡ç”¨éœ€è¦è§£é‡Šå’Œå¹²é¢„æ¨¡å‹å†³ç­–çš„èƒ½åŠ›ã€‚</li>
<li>å½“å‰æ¦‚å¿µçº§æ–¹æ³•é€šè¿‡è¿æ¥æ¨¡å‹é¢„æµ‹ä¸å¯è§£é‡Šæ¦‚å¿µæ¥ä¼˜åŒ–æ¿€æ´»å¾—åˆ†ï¼Œä»è€Œä¸æ¨¡å‹è¿›è¡Œäº¤äº’ï¼Œä½†è¿™ç§äº¤äº’æ–¹æ³•ä¸èƒ½ç²¾ç¡®åœ°ç¡®å®šæ¦‚å¿µæ¿€æ´»çš„å…·ä½“åŒºåŸŸã€‚</li>
<li>åŸºäºåŸå‹çš„æ–¹æ³•é€šè¿‡å­¦ä¹ ä»è®­ç»ƒå›¾åƒè¡¥ä¸è¡¨ç¤ºå¹¶ä¸æµ‹è¯•å›¾åƒè¡¥ä¸è¿›è¡Œæ¯”è¾ƒæ¥é¢„æµ‹æœ€ç»ˆç±»åˆ«ï¼Œä½†è§£é‡Šè¿™äº›è¡¥ä¸çš„æ½œåœ¨æ¦‚å¿µå¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå¹¶éœ€è¦äº‹åçŒœæµ‹ã€‚</li>
<li>æ–°æå‡ºçš„CSRç½‘ç»œè§£å†³äº†ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡æä¾›åŒºåŸŸåŒ–çš„æ¦‚å¿µè§£é‡Šå’Œç©ºé—´äº’åŠ¨åŠŸèƒ½ï¼Œå®ç°äº†æ›´ç²¾ç¡®çš„æ¨¡å‹é¢„æµ‹å’ŒåŒ»ç”Ÿäº’åŠ¨ã€‚</li>
<li>CSRç½‘ç»œåœ¨ä¸‰ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šè¾ƒä¹‹å‰çš„æœ€å…ˆè¿›çš„å¯è§£é‡Šæ–¹æ³•æé«˜äº†é«˜è¾¾4.5%ã€‚</li>
<li>CSRç½‘ç»œé€šè¿‡å®šä½æ¦‚å¿µåŸå‹åœ¨å›¾åƒåŒºåŸŸä¸­çš„ä½ç½®æä¾›æœ¬åœ°åŒ–è§£é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06873">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a060ff375e2265f5c58838bf63d6127.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90bd9f54860aeea029edc540d67492f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0aaf48488a6d44f916ac63fb3788990d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5f8a663e2244983f7b467bc408907fa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0eef3f141c3663d8fcbf728486cbdef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bbc344d20b38c67b3ddf92be0fba069d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Two-stage-Deep-Denoising-with-Self-guided-Noise-Attention-for-Multimodal-Medical-Images"><a href="#Two-stage-Deep-Denoising-with-Self-guided-Noise-Attention-for-Multimodal-Medical-Images" class="headerlink" title="Two-stage Deep Denoising with Self-guided Noise Attention for Multimodal   Medical Images"></a>Two-stage Deep Denoising with Self-guided Noise Attention for Multimodal   Medical Images</h2><p><strong>Authors:S M A Sharif, Rizwan Ali Naqvi, Woong-Kee Loh</strong></p>
<p>Medical image denoising is considered among the most challenging vision tasks. Despite the real-world implications, existing denoising methods have notable drawbacks as they often generate visual artifacts when applied to heterogeneous medical images. This study addresses the limitation of the contemporary denoising methods with an artificial intelligence (AI)-driven two-stage learning strategy. The proposed method learns to estimate the residual noise from the noisy images. Later, it incorporates a novel noise attention mechanism to correlate estimated residual noise with noisy inputs to perform denoising in a course-to-refine manner. This study also proposes to leverage a multi-modal learning strategy to generalize the denoising among medical image modalities and multiple noise patterns for widespread applications. The practicability of the proposed method has been evaluated with dense experiments. The experimental results demonstrated that the proposed method achieved state-of-the-art performance by significantly outperforming the existing medical image denoising methods in quantitative and qualitative comparisons. Overall, it illustrates a performance gain of 7.64 in Peak Signal-to-Noise Ratio (PSNR), 0.1021 in Structural Similarity Index (SSIM), 0.80 in DeltaE ($\Delta E$), 0.1855 in Visual Information Fidelity Pixel-wise (VIFP), and 18.54 in Mean Squared Error (MSE) metrics. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒå»å™ªè¢«è®¤ä¸ºæ˜¯è§†è§‰ä»»åŠ¡ä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„ä¹‹ä¸€ã€‚å°½ç®¡åœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œç°æœ‰çš„å»å™ªæ–¹æ³•åœ¨åº”ç”¨åˆ°å¼‚è´¨åŒ»å­¦å›¾åƒæ—¶ï¼Œå¾€å¾€ä¼šäº§ç”Ÿè§†è§‰ä¼ªå½±ï¼Œå­˜åœ¨æ˜æ˜¾çš„ç¼ºç‚¹ã€‚æœ¬ç ”ç©¶é€šè¿‡äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰é©±åŠ¨çš„ä¸¤é˜¶æ®µå­¦ä¹ ç­–ç•¥æ¥è§£å†³å½“å‰å»å™ªæ–¹æ³•çš„å±€é™ã€‚æ‰€æå‡ºçš„æ–¹æ³•å­¦ä¹ ä¼°è®¡å™ªå£°å›¾åƒä¸­çš„æ®‹ä½™å™ªå£°ã€‚ç„¶åï¼Œå®ƒå¼•å…¥äº†ä¸€ç§æ–°å‹çš„å™ªå£°æ³¨æ„åŠ›æœºåˆ¶ï¼Œå°†ä¼°è®¡çš„æ®‹ä½™å™ªå£°ä¸å¸¦å™ªå£°çš„è¾“å…¥ç›¸å…³è”ï¼Œä»¥ä»ç²—åˆ°ç»†çš„æ–¹å¼è¿›è¡Œå»å™ªã€‚æœ¬ç ”ç©¶è¿˜æå‡ºåˆ©ç”¨å¤šæ¨¡å¼å­¦ä¹ ç­–ç•¥ï¼Œä»¥æ¨å¹¿ä¸åŒåŒ»å­¦å›¾åƒæ¨¡å¼å’Œå¤šå™ªå£°æ¨¡å¼çš„å»å™ªåº”ç”¨ã€‚æ‰€æå‡ºæ–¹æ³•çš„å®ç”¨æ€§å·²ç»é€šè¿‡å¤§é‡å®éªŒè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§æ¯”è¾ƒä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŒ»å­¦å›¾åƒå»å™ªæ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼Œå®ƒåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ä¸Šè·å¾—äº†7.64çš„æ€§èƒ½æå‡ï¼Œç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ï¼ˆSSIMï¼‰æé«˜äº†0.1021ï¼ŒDeltaEï¼ˆÎ”Eï¼‰é™ä½äº†0.80ï¼Œè§†è§‰ä¿¡æ¯ä¿çœŸåº¦åƒç´ çº§ï¼ˆVIFPï¼‰æé«˜äº†0.1855ï¼Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŒ‡æ ‡å‡å°‘äº†18.54ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06827v1">PDF</a> IEEE Transactions on Radiation and Plasma Medical Sciences (2024)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºäººå·¥æ™ºèƒ½çš„ä¸¤é˜¶æ®µå­¦ä¹ ç­–ç•¥ï¼Œç”¨äºè§£å†³å½“å‰åŒ»å­¦å›¾åƒå»å™ªæ–¹æ³•çš„å±€é™æ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿä¼°è®¡å™ªå£°å›¾åƒä¸­çš„æ®‹ä½™å™ªå£°ï¼Œå¹¶ç»“åˆå™ªå£°å…³æ³¨æœºåˆ¶ï¼Œä»¥ç²—ç•¥åˆ°ç²¾ç»†çš„æ–¹å¼è¿›è¡Œå»å™ªã€‚åŒæ—¶ï¼Œæå‡ºå¤šæ¨¡æ€å­¦ä¹ ç­–ç•¥ï¼Œä½¿å»å™ªæ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åŒåŒ»å­¦å›¾åƒæ¨¡å¼å’Œå¤šå™ªå£°æ¨¡å¼ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å®šé‡å’Œå®šæ€§æ¯”è¾ƒä¸­å‡ä¼˜äºç°æœ‰åŒ»å­¦å›¾åƒå»å™ªæ–¹æ³•ï¼Œå®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒå»å™ªæ˜¯æå…·æŒ‘æˆ˜æ€§çš„è§†è§‰ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰å»å™ªæ–¹æ³•åœ¨åº”ç”¨äºå¼‚è´¨åŒ»å­¦å›¾åƒæ—¶ä¼šäº§ç”Ÿè§†è§‰ä¼ªå½±ã€‚</li>
<li>æœ¬ç ”ç©¶é‡‡ç”¨äººå·¥æ™ºèƒ½ä¸¤é˜¶æ®µå­¦ä¹ ç­–ç•¥æ¥è§£å†³ç°æœ‰å»å™ªæ–¹æ³•çš„å±€é™æ€§ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿä¼°è®¡å™ªå£°å›¾åƒä¸­çš„æ®‹ä½™å™ªå£°ã€‚</li>
<li>ç»“åˆå™ªå£°å…³æ³¨æœºåˆ¶ï¼Œä»¥ç²—ç•¥åˆ°ç²¾ç»†çš„æ–¹å¼è¿›è¡Œå»å™ªã€‚</li>
<li>æå‡ºå¤šæ¨¡æ€å­¦ä¹ ç­–ç•¥ï¼Œæé«˜å»å™ªæ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ï¼Œé€‚ç”¨äºå¤šç§åŒ»å­¦å›¾åƒæ¨¡å¼å’Œå¤šå™ªå£°æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9e4377a007cbd91e9bba1d278cdd5479.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-553195ef6a84922ae800211ba8a91394.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fc727874791ad7d4fe4fcbcb98363a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ffe8269ed00014ad4e91d8f400a50cba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40ca11446802c89b09ac8f4f58e0d72a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6661534c44b4810c22f938e3e3b6c103.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5be77d0e10b239fcbfb0298a7a7a5115.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Interactive-Tumor-Progression-Modeling-via-Sketch-Based-Image-Editing"><a href="#Interactive-Tumor-Progression-Modeling-via-Sketch-Based-Image-Editing" class="headerlink" title="Interactive Tumor Progression Modeling via Sketch-Based Image Editing"></a>Interactive Tumor Progression Modeling via Sketch-Based Image Editing</h2><p><strong>Authors:Gexin Huang, Ruinan Jin, Yucheng Tang, Can Zhao, Tatsuya Harada, Xiaoxiao Li, Gu Lin</strong></p>
<p>Accurately visualizing and editing tumor progression in medical imaging is crucial for diagnosis, treatment planning, and clinical communication. To address the challenges of subjectivity and limited precision in existing methods, we propose SkEditTumor, a sketch-based diffusion model for controllable tumor progression editing. By leveraging sketches as structural priors, our method enables precise modifications of tumor regions while maintaining structural integrity and visual realism. We evaluate SkEditTumor on four public datasets - BraTS, LiTS, KiTS, and MSD-Pancreas - covering diverse organs and imaging modalities. Experimental results demonstrate that our method outperforms state-of-the-art baselines, achieving superior image fidelity and segmentation accuracy. Our contributions include a novel integration of sketches with diffusion models for medical image editing, fine-grained control over tumor progression visualization, and extensive validation across multiple datasets, setting a new benchmark in the field. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒä¸­å‡†ç¡®å¯è§†åŒ–å’Œç¼–è¾‘è‚¿ç˜¤è¿›å±•å¯¹äºè¯Šæ–­ã€æ²»ç–—è®¡åˆ’å’Œä¸´åºŠæ²Ÿé€šè‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ–¹æ³•çš„ä¸»è§‚æ€§å’Œç²¾åº¦æœ‰é™ç­‰æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†SkEditTumorï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè‰å›¾æ‰©æ•£çš„å¯æ§è‚¿ç˜¤è¿›å±•ç¼–è¾‘æ¨¡å‹ã€‚é€šè¿‡åˆ©ç”¨è‰å›¾ä½œä¸ºç»“æ„å…ˆéªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒç»“æ„å®Œæ•´æ€§å’Œè§†è§‰çœŸå®æ„Ÿçš„åŒæ—¶ï¼Œç²¾ç¡®ä¿®æ”¹è‚¿ç˜¤åŒºåŸŸã€‚æˆ‘ä»¬åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ï¼ˆåŒ…æ‹¬è¦†ç›–å¤šä¸ªå™¨å®˜å’Œæˆåƒæ–¹å¼çš„BraTSã€LiTSã€KiTSå’ŒMSD-Pancreasï¼‰ä¸Šè¯„ä¼°äº†SkEditTumorã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°åŸºçº¿ï¼Œåœ¨å›¾åƒä¿çœŸåº¦å’Œåˆ†å‰²ç²¾åº¦æ–¹é¢å–å¾—äº†ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„è´¡çŒ®åŒ…æ‹¬å°†è‰å›¾ä¸æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆè¿›è¡ŒåŒ»å­¦å›¾åƒç¼–è¾‘çš„æ–°é¢–é›†æˆï¼Œå¯¹è‚¿ç˜¤è¿›å±•å¯è§†åŒ–çš„ç²¾ç»†æ§åˆ¶ï¼Œä»¥åŠåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›éªŒè¯ï¼Œä¸ºè¿™ä¸€é¢†åŸŸæ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06809v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè‰å›¾æ‰©æ•£æ¨¡å‹çš„è‚¿ç˜¤è¿›å±•ç¼–è¾‘æ–¹æ³•SkEditTumorï¼Œé€šè¿‡åˆ©ç”¨è‰å›¾ä½œä¸ºç»“æ„å…ˆéªŒï¼Œå®ç°å¯¹è‚¿ç˜¤åŒºåŸŸçš„ç²¾ç¡®ä¿®æ”¹ï¼ŒåŒæ—¶ä¿æŒç»“æ„å®Œæ•´æ€§å’Œè§†è§‰çœŸå®æ€§ã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å›¾åƒä¿çœŸåº¦å’Œåˆ†å‰²ç²¾åº¦æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ï¼Œä¸ºåŒ»å­¦å›¾åƒç¼–è¾‘é¢†åŸŸæ ‘ç«‹äº†æ–°çš„æ ‡æ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SkEditTumoræ˜¯ä¸€ç§åŸºäºè‰å›¾æ‰©æ•£æ¨¡å‹çš„è‚¿ç˜¤è¿›å±•ç¼–è¾‘æ–¹æ³•ï¼Œå¯ç²¾ç¡®ä¿®æ”¹è‚¿ç˜¤åŒºåŸŸã€‚</li>
<li>Sketchesè¢«ç”¨ä½œç»“æ„å…ˆéªŒï¼Œä½¿æ–¹æ³•èƒ½å¤Ÿåœ¨ä¿æŒç»“æ„å®Œæ•´æ€§å’Œè§†è§‰çœŸå®æ€§çš„åŒæ—¶ï¼Œå¯¹è‚¿ç˜¤åŒºåŸŸè¿›è¡Œç²¾ç¡®ä¿®æ”¹ã€‚</li>
<li>SkEditTumoråœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼ŒåŒ…æ‹¬BraTSã€LiTSã€KiTSå’ŒMSD-Pancreasï¼Œè¦†ç›–ä¸åŒå™¨å®˜å’Œæˆåƒæ¨¡å¼ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯åŸºçº¿ç›¸æ¯”ï¼ŒSkEditTumoråœ¨å›¾åƒä¿çœŸåº¦å’Œåˆ†å‰²ç²¾åº¦æ–¹é¢è¡¨ç°å‡ºä¼˜è¶Šæ€§ã€‚</li>
<li>SkEditTumorä¸ºåŒ»å­¦å›¾åƒç¼–è¾‘é¢†åŸŸæ ‘ç«‹äº†æ–°çš„æ ‡æ†ï¼Œç‰¹åˆ«æ˜¯åœ¨è‚¿ç˜¤è¿›å±•å¯è§†åŒ–å’Œç¼–è¾‘æ–¹é¢ã€‚</li>
<li>SkEditTumoré›†æˆäº†è‰å›¾ä¸æ‰©æ•£æ¨¡å‹ï¼Œæä¾›äº†ä¸€ç§æ–°é¢–çš„åŒ»å­¦å›¾åƒç¼–è¾‘æ–¹å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-965f7c035f3789708ada37b78c4772cf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fd0a401db42d18fc5d564333af76d36c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a39d8aa9dc9f77d4e61902c19b1a93c4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-266b353af8ebd460a99b12d85c384380.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="New-CCD-Driving-Technique-to-Suppress-Anomalous-Charge-Intrusion-from-Outside-the-Imaging-Area-for-Soft-X-ray-Imager-of-Xtend-onboard-XRISM"><a href="#New-CCD-Driving-Technique-to-Suppress-Anomalous-Charge-Intrusion-from-Outside-the-Imaging-Area-for-Soft-X-ray-Imager-of-Xtend-onboard-XRISM" class="headerlink" title="New CCD Driving Technique to Suppress Anomalous Charge Intrusion from   Outside the Imaging Area for Soft X-ray Imager of Xtend onboard XRISM"></a>New CCD Driving Technique to Suppress Anomalous Charge Intrusion from   Outside the Imaging Area for Soft X-ray Imager of Xtend onboard XRISM</h2><p><strong>Authors:Hirofumi Noda, Mio Aoyagi, Koji Mori, Hiroshi Tomida, Hiroshi Nakajima, Takaaki Tanaka, Hiromasa Suzuki, Hiroshi Murakami, Hiroyuki Uchida, Takeshi G. Tsuru, Keitaro Miyazaki, Kohei Kusunoki, Yoshiaki Kanemaru, Yuma Aoki, Kumiko Nobukawa, Masayoshi Nobukawa, Kohei Shima, Marina Yoshimoto, Kazunori Asakura, Hironori Matsumoto, Tomokage Yoneyama, Shogo B. Kobayashi, Kouichi Hagino, Hideki Uchiyama, Kiyoshi Hayashida</strong></p>
<p>The Soft X-ray Imager (SXI) is an X-ray CCD camera of the Xtend system onboard the X-Ray Imaging and Spectroscopy Mission (XRISM), which was successfully launched on September 7, 2023 (JST). During ground cooling tests of the CCDs in 2020&#x2F;2021, using the flight-model detector housing, electronic boards, and a mechanical cooler, we encountered an unexpected issue. Anomalous charges appeared outside the imaging area of the CCDs and intruded into the imaging area, causing pulse heights to stick to the maximum value over a wide region. Although this issue has not occurred in subsequent tests or in orbit so far, it could seriously affect the imaging and spectroscopic performance of the SXI if it were to happen in the future. Through experiments with non-flight-model detector components, we successfully reproduced the issue and identified that the anomalous charges intrude via the potential structure created by the charge injection electrode at the top of the imaging area. To prevent anomalous charge intrusion and maintain imaging and spectroscopic performance that satisfies the requirements, even if this issue occurs in orbit, we developed a new CCD driving technique. This technique is different from the normal operation in terms of potential structure and its changes during imaging and charge injection. In this paper, we report an overview of the anomalous charge issue, the related potential structures, the development of the new CCD driving technique to prevent the issue, the imaging and spectroscopic performance of the new technique, and the results of experiments to investigate the cause of anomalous charges. </p>
<blockquote>
<p>è½¯Xå°„çº¿æˆåƒä»ªï¼ˆSXIï¼‰æ˜¯Xå°„çº¿æˆåƒä¸å…‰è°±ä»»åŠ¡ï¼ˆXRISMï¼‰ä¸Šçš„Xå°„çº¿CCDç›¸æœºï¼ŒXRISMäº2023å¹´9æœˆ7æ—¥ï¼ˆæ—¥æœ¬æ ‡å‡†æ—¶é—´ï¼‰æˆåŠŸå‘å°„ã€‚åœ¨2020&#x2F;2021å¹´å¯¹CCDçš„åœ°é¢å†·å´æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é£è¡Œæ¨¡å‹æ£€æµ‹å™¨å¤–å£³ã€ç”µå­æ¿å’Œæœºæ¢°å†·å´å™¨ï¼Œé‡åˆ°äº†ä¸€ä¸ªæ„æ–™ä¹‹å¤–çš„é—®é¢˜ã€‚å¼‚å¸¸ç”µè·å‡ºç°åœ¨CCDæˆåƒåŒºåŸŸä¹‹å¤–ï¼Œå¹¶ä¾µå…¥æˆåƒåŒºåŸŸï¼Œå¯¼è‡´è„‰å†²é«˜åº¦åœ¨å®½åŒºåŸŸå†…è¾¾åˆ°æœ€å¤§å€¼ã€‚è™½ç„¶è¿™ä¸€é—®é¢˜åœ¨éšåçš„æµ‹è¯•æˆ–åœ¨è½¨è¿è¡Œä¸­å°šæœªå‡ºç°ï¼Œä½†å¦‚æœæœªæ¥å†æ¬¡å‘ç”Ÿï¼Œå¯èƒ½ä¼šä¸¥é‡å½±å“SXIçš„æˆåƒå’Œå…‰è°±æ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨éé£è¡Œæ¨¡å‹æ£€æµ‹å™¨ç»„ä»¶è¿›è¡Œå®éªŒï¼ŒæˆåŠŸå¤ç°äº†è¿™ä¸€é—®é¢˜ï¼Œå¹¶ç¡®å®šå¼‚å¸¸ç”µè·æ˜¯é€šè¿‡æˆåƒåŒºåŸŸé¡¶éƒ¨çš„ç”µè·æ³¨å…¥ç”µæäº§ç”Ÿçš„ç”µä½ç»“æ„ä¾µå…¥çš„ã€‚ä¸ºäº†é¢„é˜²å¼‚å¸¸ç”µè·ä¾µå…¥å¹¶æ»¡è¶³æˆåƒå’Œå…‰è°±æ€§èƒ½çš„è¦æ±‚ï¼Œå³ä½¿åœ¨æœªæ¥åœ¨è½¨å‡ºç°è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä¹Ÿå¼€å‘äº†ä¸€ç§æ–°çš„CCDé©±åŠ¨æŠ€æœ¯ã€‚è¿™ç§æŠ€æœ¯åœ¨ç”µä½ç»“æ„å’ŒæˆåƒåŠç”µè·æ³¨å…¥æœŸé—´çš„å˜åŒ–æ–¹é¢ä¸æ­£å¸¸æ“ä½œä¸åŒã€‚æœ¬æ–‡æ¦‚è¿°äº†å¼‚å¸¸ç”µè·é—®é¢˜ã€ç›¸å…³çš„ç”µä½ç»“æ„ã€ä¸ºé˜²æ­¢è¯¥é—®é¢˜è€Œå¼€å‘çš„æ–°CCDé©±åŠ¨æŠ€æœ¯ã€æ–°æŠ€æœ¯çš„æˆåƒå’Œå…‰è°±æ€§èƒ½ä»¥åŠè°ƒæŸ¥å¼‚å¸¸ç”µè·åŸå› çš„è¯•éªŒç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06760v1">PDF</a> 13 pages, 8 figures, Accepted for publication in JATIS XRISM special   issue</p>
<p><strong>æ‘˜è¦</strong><br>    SXIæ˜¯Xå°„çº¿æˆåƒä¸å…‰è°±ä»»åŠ¡ï¼ˆXRISMï¼‰çš„Xtendç³»ç»Ÿæ­è½½çš„ä¸€æ¬¾Xå°„çº¿CCDç›¸æœºï¼Œäºæ—¥æœ¬æ ‡å‡†æ—¶é—´ï¼ˆJSTï¼‰2023å¹´9æœˆ7æ—¥æˆåŠŸå‘å°„ã€‚åœ¨åœ°é¢å†·å´æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬å‘ç°CCDæˆåƒåŒºåŸŸå¤–çš„å¼‚å¸¸ç”µè·ä¾µå…¥æˆåƒåŒºåŸŸï¼Œå¯¼è‡´è„‰å†²é«˜åº¦å¤§é¢ç§¯è¾¾åˆ°æœ€å¤§å€¼ã€‚å°½ç®¡åç»­æµ‹è¯•å’Œè½¨é“è¿è¡Œå‡æœªå‡ºç°æ­¤é—®é¢˜ï¼Œä½†è‹¥æœªæ¥å‘ç”Ÿå°†ä¸¥é‡å½±å“æˆåƒå’Œå…‰è°±æ€§èƒ½ã€‚æˆ‘ä»¬æˆåŠŸé‡ç°äº†è¿™ä¸€é—®é¢˜å¹¶å‘ç°å¼‚å¸¸ç”µè·é€šè¿‡æˆåƒåŒºåŸŸé¡¶éƒ¨çš„ç”µè·æ³¨å…¥ç”µæäº§ç”Ÿçš„æ½œåœ¨ç»“æ„ä¾µå…¥ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜å¹¶ä¿æŒæˆåƒå’Œå…‰è°±æ€§èƒ½æ»¡è¶³è¦æ±‚ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°å‹CCDé©±åŠ¨æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯é€šè¿‡æ”¹å˜æˆåƒå’Œç”µè·æ³¨å…¥æœŸé—´çš„æ½œåœ¨ç»“æ„å’Œå˜åŒ–æ¥é˜²æ­¢å¼‚å¸¸ç”µè·ä¾µå…¥ã€‚æœ¬æ–‡æŠ¥å‘Šäº†å¼‚å¸¸ç”µè·é—®é¢˜æ¦‚è¿°ã€ç›¸å…³æ½œåœ¨ç»“æ„ã€é˜²æ­¢é—®é¢˜çš„æ–°å‹CCDæŠ€æœ¯å¼€å‘ã€æ–°æŠ€æœ¯çš„æˆåƒå’Œå…‰è°±æ€§èƒ½ä»¥åŠè°ƒæŸ¥å¼‚å¸¸ç”µè·åŸå› çš„å®éªŒç»“æœã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SXIæ˜¯XRISMä»»åŠ¡çš„Xå°„çº¿CCDç›¸æœºï¼Œäº2023å¹´æˆåŠŸå‘å°„ã€‚</li>
<li>åœ¨åœ°é¢å†·å´æµ‹è¯•ä¸­å‘ç°äº†å¼‚å¸¸ç”µè·ä¾µå…¥çš„é—®é¢˜ï¼Œå¯¼è‡´æˆåƒåŒºåŸŸå—åˆ°å¹²æ‰°ã€‚</li>
<li>å¼‚å¸¸ç”µè·é€šè¿‡ç”µè·æ³¨å…¥ç”µæäº§ç”Ÿçš„æ½œåœ¨ç»“æ„ä¾µå…¥æˆåƒåŒºåŸŸã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æ–°å‹CCDé©±åŠ¨æŠ€æœ¯ï¼Œä»¥é¢„é˜²å¼‚å¸¸ç”µè·ä¾µå…¥å¹¶ç»´æŒæˆåƒå’Œå…‰è°±æ€§èƒ½ã€‚</li>
<li>æ–°æŠ€æœ¯æ”¹å˜äº†æˆåƒå’Œç”µè·æ³¨å…¥æœŸé—´çš„æ½œåœ¨ç»“æ„å’Œå˜åŒ–ã€‚</li>
<li>è¯¥æŠ€æœ¯æˆåŠŸè§£å†³äº†å¼‚å¸¸ç”µè·ä¾µå…¥çš„é—®é¢˜ï¼Œæé«˜äº†æˆåƒè´¨é‡ã€‚</li>
<li>æ–‡ä¸­è¿˜æŠ¥å‘Šäº†è°ƒæŸ¥å¼‚å¸¸ç”µè·åŸå› çš„å®éªŒç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06760">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d1589d6cb0d990a8d2a9d1ffaaff969c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f81343687de506c5a9b8e5ee49c370f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a67a8c647bb609157fdeb355ebd62614.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1cb263638ded8f0158eadca337406b13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97b3aaee8e476f0326e804b6f55d10fd.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DiffAtlas-GenAI-fying-Atlas-Segmentation-via-Image-Mask-Diffusion"><a href="#DiffAtlas-GenAI-fying-Atlas-Segmentation-via-Image-Mask-Diffusion" class="headerlink" title="DiffAtlas: GenAI-fying Atlas Segmentation via Image-Mask Diffusion"></a>DiffAtlas: GenAI-fying Atlas Segmentation via Image-Mask Diffusion</h2><p><strong>Authors:Hantao Zhang, Yuhe Liu, Jiancheng Yang, Weidong Guo, Xinyuan Wang, Pascal Fua</strong></p>
<p>Accurate medical image segmentation is crucial for precise anatomical delineation. Deep learning models like U-Net have shown great success but depend heavily on large datasets and struggle with domain shifts, complex structures, and limited training samples. Recent studies have explored diffusion models for segmentation by iteratively refining masks. However, these methods still retain the conventional image-to-mask mapping, making them highly sensitive to input data, which hampers stability and generalization. In contrast, we introduce DiffAtlas, a novel generative framework that models both images and masks through diffusion during training, effectively &#96;&#96;GenAI-fyingâ€™â€™ atlas-based segmentation. During testing, the model is guided to generate a specific target image-mask pair, from which the corresponding mask is obtained. DiffAtlas retains the robustness of the atlas paradigm while overcoming its scalability and domain-specific limitations. Extensive experiments on CT and MRI across same-domain, cross-modality, varying-domain, and different data-scale settings using the MMWHS and TotalSegmentator datasets demonstrate that our approach outperforms existing methods, particularly in limited-data and zero-shot modality segmentation. Code is available at <a target="_blank" rel="noopener" href="https://github.com/M3DV/DiffAtlas">https://github.com/M3DV/DiffAtlas</a>. </p>
<blockquote>
<p>ç²¾ç¡®åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºç²¾ç¡®è§£å‰–ç•Œå®šè‡³å…³é‡è¦ã€‚U-Netç­‰æ·±åº¦å­¦ä¹ æ¨¡å‹å·²ç»å–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œä½†å®ƒä»¬ä¸¥é‡ä¾èµ–äºå¤§å‹æ•°æ®é›†ï¼Œåœ¨é¢†åŸŸè½¬ç§»ã€å¤æ‚ç»“æ„å’Œæœ‰é™è®­ç»ƒæ ·æœ¬æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚æœ€è¿‘çš„ç ”ç©¶æ¢ç´¢äº†é€šè¿‡è¿­ä»£ç»†åŒ–æ©è†œè¿›è¡Œåˆ†å‰²çš„æ‰©æ•£æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»ç„¶ä¿ç•™äº†ä¼ ç»Ÿçš„å›¾åƒåˆ°æ©è†œæ˜ å°„ï¼Œä½¿å®ƒä»¬å¯¹è¾“å…¥æ•°æ®é«˜åº¦æ•æ„Ÿï¼Œè¿™é˜»ç¢äº†ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†DiffAtlasï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç”Ÿæˆæ¡†æ¶ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡æ‰©æ•£å¯¹å›¾åƒå’Œæ©è†œè¿›è¡Œå»ºæ¨¡ï¼Œæœ‰æ•ˆåœ°å®ç°äº†åŸºäºå›¾è°±çš„åˆ†å‰²çš„â€œGenAIåŒ–â€ã€‚åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œè¯¥æ¨¡å‹è¢«å¼•å¯¼ç”Ÿæˆç‰¹å®šçš„ç›®æ ‡å›¾åƒ-æ©è†œå¯¹ï¼Œä»ä¸­è·å¾—ç›¸åº”çš„æ©è†œã€‚DiffAtlasä¿ç•™äº†å›¾è°±èŒƒå¼çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶å…‹æœäº†å…¶å¯æ‰©å±•æ€§å’Œé¢†åŸŸç‰¹å®šé™åˆ¶ã€‚åœ¨MMWHSå’ŒTotalSegmentatoræ•°æ®é›†ä¸Šè¿›è¡Œçš„å…³äºCTå’ŒMRIçš„åŒåŸŸã€è·¨æ¨¡æ€ã€å˜åŸŸå’Œä¸åŒæ•°æ®è§„æ¨¡è®¾ç½®çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™æ•°æ®å’Œé›¶æ ·æœ¬æ¨¡æ€åˆ†å‰²æ–¹é¢ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/M3DV/DiffAtlas">https://github.com/M3DV/DiffAtlas</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06748v1">PDF</a> 11 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DiffAtlasï¼Œä¸€ç§æ–°å‹çš„åŒ»å­¦å›¾åƒåˆ†å‰²ç”Ÿæˆæ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡æ‰©æ•£å»ºæ¨¡å›¾åƒå’Œæ©è†œï¼Œæœ‰æ•ˆå®ç°äº†åŸºäºå›¾è°±çš„åˆ†å‰²â€œGenAIåŒ–â€ã€‚é€šè¿‡å®éªŒéªŒè¯ï¼ŒDiffAtlasåœ¨ç›¸åŒé¢†åŸŸã€è·¨æ¨¡æ€ã€ä¸åŒé¢†åŸŸå’Œä¸åŒæ•°æ®è§„æ¨¡è®¾ç½®ä¸‹çš„CTå’ŒMRIå›¾åƒåˆ†å‰²ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™æ•°æ®å’Œé›¶æ ·æœ¬æ¨¡æ€åˆ†å‰²æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§å¯¹äºç²¾ç¡®è§£å‰–ç»“æ„åˆ’åˆ†è‡³å…³é‡è¦ã€‚</li>
<li>U-Netç­‰æ·±åº¦å­¦ä¹ æ¨¡å‹è™½ç„¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†é¢†åŸŸå˜åŒ–ã€å¤æ‚ç»“æ„å’Œæœ‰é™è®­ç»ƒæ ·æœ¬æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆ†å‰²ä¸­çš„åº”ç”¨é€šè¿‡è¿­ä»£ä¼˜åŒ–æ©è†œæ¥æ”¹è¿›åˆ†å‰²æ•ˆæœã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä»ç„¶ä¾èµ–äºä¼ ç»Ÿçš„å›¾åƒåˆ°æ©è†œæ˜ å°„ï¼Œå¯¹è¾“å…¥æ•°æ®æ•æ„Ÿï¼Œå½±å“ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>DiffAtlasæ¡†æ¶é€šè¿‡å»ºæ¨¡å›¾åƒå’Œæ©è†œçš„æ‰©æ•£è¿‡ç¨‹ï¼Œå®ç°äº†åŸºäºå›¾è°±çš„åˆ†å‰²â€œGenAIåŒ–â€ã€‚</li>
<li>DiffAtlasåœ¨å¤šç§è®¾ç½®ä¸‹çš„å®éªŒè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™æ•°æ®å’Œé›¶æ ·æœ¬æ¨¡æ€åˆ†å‰²æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06748">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d6f38a4865872449b09c9932c4b7d522.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-384e61ec0f67df03b564e5a442c27d0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-adf4b3c894cf457c8d52fac0bd98f4d7.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Continuous-Online-Adaptation-Driven-by-User-Interaction-for-Medical-Image-Segmentation"><a href="#Continuous-Online-Adaptation-Driven-by-User-Interaction-for-Medical-Image-Segmentation" class="headerlink" title="Continuous Online Adaptation Driven by User Interaction for Medical   Image Segmentation"></a>Continuous Online Adaptation Driven by User Interaction for Medical   Image Segmentation</h2><p><strong>Authors:Wentian Xu, Ziyun Liang, Harry Anthony, Yasin Ibrahim, Felix Cohen, Guang Yang, Daniel Whitehouse, David Menon, Virginia Newcombe, Konstantinos Kamnitsas</strong></p>
<p>Interactive segmentation models use real-time user interactions, such as mouse clicks, as extra inputs to dynamically refine the model predictions. After model deployment, user corrections of model predictions could be used to adapt the model to the post-deployment data distribution, countering distribution-shift and enhancing reliability. Motivated by this, we introduce an online adaptation framework that enables an interactive segmentation model to continuously learn from user interaction and improve its performance on new data distributions, as it processes a sequence of test images. We introduce the Gaussian Point Loss function to train the model how to leverage user clicks, along with a two-stage online optimization method that adapts the model using the corrected predictions generated via user interactions. We demonstrate that this simple and therefore practical approach is very effective. Experiments on 5 fundus and 4 brain MRI databases demonstrate that our method outperforms existing approaches under various data distribution shifts, including segmentation of image modalities and pathologies not seen during training. </p>
<blockquote>
<p>äº¤äº’å¼åˆ†å‰²æ¨¡å‹ä½¿ç”¨å®æ—¶ç”¨æˆ·äº¤äº’ï¼Œå¦‚é¼ æ ‡ç‚¹å‡»ï¼Œä½œä¸ºé¢å¤–çš„è¾“å…¥æ¥åŠ¨æ€ä¼˜åŒ–æ¨¡å‹é¢„æµ‹ã€‚åœ¨æ¨¡å‹éƒ¨ç½²åï¼Œç”¨æˆ·å¯ä»¥å¯¹æ¨¡å‹é¢„æµ‹è¿›è¡Œä¿®æ­£ï¼Œä½¿æ¨¡å‹é€‚åº”éƒ¨ç½²åçš„æ•°æ®åˆ†å¸ƒï¼Œå¯¹æŠ—åˆ†å¸ƒåç§»ï¼Œæé«˜å¯é æ€§ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåœ¨çº¿é€‚é…æ¡†æ¶ï¼Œä½¿äº¤äº’å¼åˆ†å‰²æ¨¡å‹èƒ½å¤Ÿåœ¨å¤„ç†æµ‹è¯•å›¾åƒåºåˆ—æ—¶ï¼Œä»ç”¨æˆ·äº¤äº’ä¸­æŒç»­å­¦ä¹ ï¼Œå¹¶æé«˜å…¶åœ¨æ–°çš„æ•°æ®åˆ†å¸ƒä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬å¼•å…¥äº†é«˜æ–¯ç‚¹æŸå¤±å‡½æ•°æ¥è®­ç»ƒæ¨¡å‹å¦‚ä½•åˆ©ç”¨ç”¨æˆ·ç‚¹å‡»ï¼Œä»¥åŠä¸€ä¸ªä¸¤é˜¶æ®µçš„åœ¨çº¿ä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ç”¨æˆ·äº¤äº’ç”Ÿæˆçš„ä¿®æ­£é¢„æµ‹æ¥é€‚åº”æ¨¡å‹ã€‚æˆ‘ä»¬è¯æ˜è¿™ç§ç®€å•å®ç”¨çš„æ–¹æ³•éå¸¸æœ‰æ•ˆã€‚åœ¨5ä¸ªçœ¼åº•å’Œ4ä¸ªè„‘éƒ¨MRIæ•°æ®åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§æ•°æ®åˆ†å¸ƒåç§»æƒ…å†µä¸‹ï¼ŒåŒ…æ‹¬å›¾åƒæ¨¡æ€å’Œè®­ç»ƒæœŸé—´æœªè§åˆ°çš„ç—…ç†åˆ†å‰²ï¼Œéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06717v1">PDF</a> </p>
<p><strong>Summary</strong><br>å®æ—¶äº¤äº’åˆ†å‰²æ¨¡å‹é€šè¿‡åˆ©ç”¨ç”¨æˆ·å®æ—¶äº¤äº’ï¼ˆå¦‚é¼ æ ‡ç‚¹å‡»ï¼‰ä½œä¸ºé¢å¤–è¾“å…¥ï¼ŒåŠ¨æ€ä¼˜åŒ–æ¨¡å‹é¢„æµ‹ã€‚éƒ¨ç½²æ¨¡å‹åï¼Œç”¨æˆ·ä¿®æ­£çš„æ¨¡å‹é¢„æµ‹å¯ç”¨äºé€‚åº”éƒ¨ç½²åçš„æ•°æ®åˆ†å¸ƒï¼Œå¯¹æŠ—åˆ†å¸ƒåç§»ï¼Œæé«˜å¯é æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåœ¨çº¿é€‚åº”æ¡†æ¶ï¼Œä½¿äº¤äº’åˆ†å‰²æ¨¡å‹èƒ½å¤Ÿä»ç”¨æˆ·äº¤äº’ä¸­å­¦ä¹ ï¼Œå¹¶åœ¨å¤„ç†æµ‹è¯•å›¾åƒåºåˆ—æ—¶ï¼Œé€‚åº”æ–°æ•°æ®åˆ†å¸ƒå¹¶æ”¹è¿›æ€§èƒ½ã€‚é€šè¿‡é«˜æ–¯ç‚¹æŸå¤±å‡½æ•°è®­ç»ƒæ¨¡å‹åˆ©ç”¨ç”¨æˆ·ç‚¹å‡»ï¼Œå¹¶æå‡ºä¸¤é˜¶æ®µåœ¨çº¿ä¼˜åŒ–æ–¹æ³•ï¼Œä½¿ç”¨ç”¨æˆ·äº¤äº’ç”Ÿæˆçš„ä¿®æ­£é¢„æµ‹æ¥é€‚åº”æ¨¡å‹ã€‚åœ¨5ä¸ªçœ¼åº•å’Œ4ä¸ªè„‘éƒ¨MRIæ•°æ®åº“ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§æ•°æ®åˆ†å¸ƒåç§»ä¸‹çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬å›¾åƒæ¨¡æ€å’Œç—…ç†çš„åˆ†å‰²ï¼Œè¿™äº›åœ¨è®­ç»ƒæ—¶å¹¶æœªè§è¿‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®æ—¶äº¤äº’åˆ†å‰²æ¨¡å‹åˆ©ç”¨ç”¨æˆ·å®æ—¶äº¤äº’ä½œä¸ºé¢å¤–è¾“å…¥æ¥åŠ¨æ€ä¼˜åŒ–æ¨¡å‹é¢„æµ‹ã€‚</li>
<li>ç”¨æˆ·ä¿®æ­£çš„æ¨¡å‹é¢„æµ‹å¯ä»¥ç”¨äºé€‚åº”æ¨¡å‹åˆ°éƒ¨ç½²åçš„æ•°æ®åˆ†å¸ƒã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªåœ¨çº¿é€‚åº”æ¡†æ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»ç”¨æˆ·äº¤äº’ä¸­å­¦ä¹ å¹¶æ”¹è¿›æ€§èƒ½ã€‚</li>
<li>é«˜æ–¯ç‚¹æŸå¤±å‡½æ•°è¢«ç”¨æ¥è®­ç»ƒæ¨¡å‹å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨ç”¨æˆ·ç‚¹å‡»ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„åœ¨çº¿ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡ç”¨æˆ·äº¤äº’ç”Ÿæˆçš„ä¿®æ­£é¢„æµ‹æ¥é€‚åº”æ¨¡å‹ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§æ•°æ®åˆ†å¸ƒåç§»æƒ…å†µä¸‹çš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06717">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d009f51ee509894e8a6f7d0d5889761f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a7b795909d4d573ab8c3f3a80c7c9a48.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Dynamic-Dictionary-Learning-for-Remote-Sensing-Image-Segmentation"><a href="#Dynamic-Dictionary-Learning-for-Remote-Sensing-Image-Segmentation" class="headerlink" title="Dynamic Dictionary Learning for Remote Sensing Image Segmentation"></a>Dynamic Dictionary Learning for Remote Sensing Image Segmentation</h2><p><strong>Authors:Xuechao Zou, Yue Li, Shun Zhang, Kai Li, Shiying Wang, Pin Tao, Junliang Xing, Congyan Lang</strong></p>
<p>Remote sensing image segmentation faces persistent challenges in distinguishing morphologically similar categories and adapting to diverse scene variations. While existing methods rely on implicit representation learning paradigms, they often fail to dynamically adjust semantic embeddings according to contextual cues, leading to suboptimal performance in fine-grained scenarios such as cloud thickness differentiation. This work introduces a dynamic dictionary learning framework that explicitly models class ID embeddings through iterative refinement. The core contribution lies in a novel dictionary construction mechanism, where class-aware semantic embeddings are progressively updated via multi-stage alternating cross-attention querying between image features and dictionary embeddings. This process enables adaptive representation learning tailored to input-specific characteristics, effectively resolving ambiguities in intra-class heterogeneity and inter-class homogeneity. To further enhance discriminability, a contrastive constraint is applied to the dictionary space, ensuring compact intra-class distributions while maximizing inter-class separability. Extensive experiments across both coarse- and fine-grained datasets demonstrate consistent improvements over state-of-the-art methods, particularly in two online test benchmarks (LoveDA and UAVid). Code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/D2LS-8267/">https://anonymous.4open.science/r/D2LS-8267/</a>. </p>
<blockquote>
<p>é¥æ„Ÿå›¾åƒåˆ†å‰²åœ¨åŒºåˆ†å½¢æ€ç›¸ä¼¼ç±»åˆ«å’Œé€‚åº”å¤šç§åœºæ™¯å˜åŒ–æ–¹é¢æŒç»­é¢ä¸´æŒ‘æˆ˜ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•ä¾èµ–äºéšå¼è¡¨ç¤ºå­¦ä¹ èŒƒå¼ï¼Œä½†å®ƒä»¬é€šå¸¸æ— æ³•æ ¹æ®ä¸Šä¸‹æ–‡çº¿ç´¢åŠ¨æ€è°ƒæ•´è¯­ä¹‰åµŒå…¥ï¼Œå¯¼è‡´åœ¨äº‘åšåº¦å·®å¼‚ç­‰ç²¾ç»†åœºæ™¯ä¸­çš„æ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡å¼•å…¥äº†ä¸€ç§åŠ¨æ€å­—å…¸å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£ç»†åŒ–æ˜¾å¼å»ºæ¨¡ç±»IDåµŒå…¥ã€‚æ ¸å¿ƒè´¡çŒ®åœ¨äºä¸€ç§æ–°çš„å­—å…¸æ„å»ºæœºåˆ¶ï¼Œå…¶ä¸­é€šè¿‡å›¾åƒç‰¹å¾å’Œå­—å…¸åµŒå…¥ä¹‹é—´çš„å¤šé˜¶æ®µäº¤æ›¿äº¤å‰æ³¨æ„åŠ›æŸ¥è¯¢ï¼Œé€æ­¥æ›´æ–°ç±»æ„ŸçŸ¥è¯­ä¹‰åµŒå…¥ã€‚è¿™ä¸€è¿‡ç¨‹ä½¿é€‚åº”è¾“å…¥ç‰¹å®šç‰¹æ€§çš„è‡ªé€‚åº”è¡¨ç¤ºå­¦ä¹ æˆä¸ºå¯èƒ½ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ç±»å†…å¼‚è´¨æ€§å’Œç±»é—´åŒè´¨æ€§ä¸­çš„æ­§ä¹‰ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜é‰´åˆ«åŠ›ï¼Œåœ¨å­—å…¸ç©ºé—´åº”ç”¨å¯¹æ¯”çº¦æŸï¼Œç¡®ä¿ç±»å†…åˆ†å¸ƒç´§å‡‘ï¼ŒåŒæ—¶æœ€å¤§åŒ–ç±»é—´å¯åˆ†æ€§ã€‚åœ¨ç²—ç»†ç²’åº¦æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸¤ä¸ªåœ¨çº¿æµ‹è¯•åŸºå‡†ï¼ˆLoveDAå’ŒUAVidï¼‰ä¸Šï¼Œæ”¹è¿›å…·æœ‰ä¸€è‡´æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/D2LS-8267/%E6%89%BE%E5%88%B0%E3%80%82">https://anonymous.4open.science/r/D2LS-8267/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06683v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŠ¨æ€å­—å…¸å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–æ˜¾å¼å»ºæ¨¡ç±»åˆ«IDåµŒå…¥ï¼Œè§£å†³é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­å½¢æ€ç›¸ä¼¼ç±»åˆ«åŒºåˆ†åŠåœºæ™¯å¤šæ ·å˜åŒ–é€‚åº”é—®é¢˜ã€‚æ¡†æ¶æ ¸å¿ƒåœ¨äºæ–°é¢–å­—å…¸æ„å»ºæœºåˆ¶ï¼Œé€šè¿‡å›¾åƒç‰¹å¾ä¸å­—å…¸åµŒå…¥ä¹‹é—´çš„å¤šé˜¶æ®µäº¤æ›¿è·¨æ³¨æ„åŠ›æŸ¥è¯¢ï¼Œé€æ­¥æ›´æ–°ç±»åˆ«æ„ŸçŸ¥è¯­ä¹‰åµŒå…¥ï¼Œå®ç°è‡ªé€‚åº”è¡¨ç¤ºå­¦ä¹ ï¼Œæœ‰æ•ˆè§£å†‘ç±»å†…å¼‚è´¨æ€§å’Œç±»é—´åŒè´¨æ€§å¼•èµ·çš„æ­§ä¹‰ã€‚ä¸ºæå‡é‰´åˆ«åŠ›ï¼Œåº”ç”¨å¯¹æ¯”çº¦æŸäºå­—å…¸ç©ºé—´ï¼Œç¡®ä¿ç±»å†…åˆ†å¸ƒç´§å‡‘å¹¶æœ€å¤§åŒ–ç±»é—´å¯åˆ†æ€§ã€‚åœ¨ç²—ç»†ç²’åº¦æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¾ƒå‰æ²¿æ–¹æ³•æœ‰ä¸€è‡´æ€§æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸¤ä¸ªåœ¨çº¿æµ‹è¯•åŸºå‡†ï¼ˆLoveDAå’ŒUAVidï¼‰ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒåˆ†å‰²é¢ä¸´åŒºåˆ†å½¢æ€ç›¸ä¼¼ç±»åˆ«å’Œé€‚åº”åœºæ™¯å¤šæ ·æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–éšå¼è¡¨ç¤ºå­¦ä¹ ï¼Œæ— æ³•æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´è¯­ä¹‰åµŒå…¥ã€‚</li>
<li>æœ¬æ–‡æå‡ºåŠ¨æ€å­—å…¸å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è¿­ä»£ä¼˜åŒ–æ˜¾å¼å»ºæ¨¡ç±»åˆ«IDåµŒå…¥ã€‚</li>
<li>æ¡†æ¶æ ¸å¿ƒåœ¨äºæ–°é¢–çš„å­—å…¸æ„å»ºæœºåˆ¶ï¼Œé€æ­¥æ›´æ–°ç±»åˆ«æ„ŸçŸ¥è¯­ä¹‰åµŒå…¥ã€‚</li>
<li>å®ç°è‡ªé€‚åº”è¡¨ç¤ºå­¦ä¹ ï¼Œæœ‰æ•ˆè§£å†³ç±»å†…å¼‚è´¨æ€§å’Œç±»é—´åŒè´¨æ€§é—®é¢˜ã€‚</li>
<li>é€šè¿‡åº”ç”¨å¯¹æ¯”çº¦æŸäºå­—å…¸ç©ºé—´ï¼Œæå‡é‰´åˆ«åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06683">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f7593f4dac5e5fb4d13d93718c7dc629.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-506c8b7ba0588ebbed303c9a12769abc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff60fc41b92c0c8fd7c1f53e434c8990.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-72be80bb0be8ea893d5e88e3451b8191.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="LSA-Latent-Style-Augmentation-Towards-Stain-Agnostic-Cervical-Cancer-Screening"><a href="#LSA-Latent-Style-Augmentation-Towards-Stain-Agnostic-Cervical-Cancer-Screening" class="headerlink" title="LSA: Latent Style Augmentation Towards Stain-Agnostic Cervical Cancer   Screening"></a>LSA: Latent Style Augmentation Towards Stain-Agnostic Cervical Cancer   Screening</h2><p><strong>Authors:Jiangdong Cai, Haotian Jiang, Zhenrong Shen, Yonghao Li, Honglin Xiong, Lichi Zhang, Qian Wang</strong></p>
<p>The deployment of computer-aided diagnosis systems for cervical cancer screening using whole slide images (WSIs) faces critical challenges due to domain shifts caused by staining variations across different scanners and imaging environments. While existing stain augmentation methods improve patch-level robustness, they fail to scale to WSIs due to two key limitations: (1) inconsistent stain patterns when extending patch operations to gigapixel slides, and (2) prohibitive computational&#x2F;storage costs from offline processing of augmented WSIs.To address this, we propose Latent Style Augmentation (LSA), a framework that performs efficient, online stain augmentation directly on WSI-level latent features. We first introduce WSAug, a WSI-level stain augmentation method ensuring consistent stain across patches within a WSI. Using offline-augmented WSIs by WSAug, we design and train Stain Transformer, which can simulate targeted style in the latent space, efficiently enhancing the robustness of the WSI-level classifier. We validate our method on a multi-scanner WSI dataset for cervical cancer diagnosis. Despite being trained on data from a single scanner, our approach achieves significant performance improvements on out-of-distribution data from other scanners. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/caijd2000/LSA">https://github.com/caijd2000/LSA</a>. </p>
<blockquote>
<p>åˆ©ç”¨å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰è¿›è¡Œå®«é¢ˆç™Œç­›æŸ¥çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿçš„éƒ¨ç½²ï¼Œç”±äºä¸åŒæ‰«æä»ªå’Œæˆåƒç¯å¢ƒä¸­çš„æŸ“è‰²å˜åŒ–å¯¼è‡´çš„é¢†åŸŸæ¼‚ç§»è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„æŸ“è‰²å¢å¼ºæ–¹æ³•è™½ç„¶æé«˜äº†è¡¥ä¸çº§åˆ«çš„ç¨³å¥æ€§ï¼Œä½†ç”±äºä¸¤ä¸ªå…³é”®å±€é™æ€§è€Œæ— æ³•æ‰©å±•åˆ°å…¨åˆ‡ç‰‡å›¾åƒï¼šä¸€æ˜¯å°†è¡¥ä¸æ“ä½œæ‰©å±•åˆ°åƒå…†åƒç´ åˆ‡ç‰‡æ—¶ï¼ŒæŸ“è‰²æ¨¡å¼ä¸ä¸€è‡´ï¼›äºŒæ˜¯é€šè¿‡ç¦»çº¿å¤„ç†å¢å¼ºå…¨åˆ‡ç‰‡å›¾åƒçš„è®¡ç®—å’Œå­˜å‚¨æˆæœ¬è¿‡é«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨é£æ ¼å¢å¼ºï¼ˆLSAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨å…¨åˆ‡ç‰‡å›¾åƒçº§åˆ«çš„æ½œåœ¨ç‰¹å¾ä¸Šç›´æ¥è¿›è¡Œé«˜æ•ˆåœ¨çº¿æŸ“è‰²å¢å¼ºã€‚æˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†WSAugå…¨åˆ‡ç‰‡å›¾åƒçº§åˆ«çš„æŸ“è‰²å¢å¼ºæ–¹æ³•ï¼Œç¡®ä¿å…¨åˆ‡ç‰‡å›¾åƒå†…å„è¡¥ä¸ä¹‹é—´çš„æŸ“è‰²ä¸€è‡´æ€§ã€‚é€šè¿‡WSAugå¯¹ç¦»çº¿å¢å¼ºçš„å…¨åˆ‡ç‰‡å›¾åƒè¿›è¡Œå¤„ç†ï¼Œæˆ‘ä»¬è®¾è®¡å’Œè®­ç»ƒäº†æŸ“è‰²è½¬æ¢å™¨ï¼Œå¯ä»¥åœ¨æ½œåœ¨ç©ºé—´ä¸­æ¨¡æ‹Ÿç›®æ ‡é£æ ¼ï¼Œæœ‰æ•ˆæé«˜å…¨åˆ‡ç‰‡å›¾åƒçº§åˆ«åˆ†ç±»å™¨çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬åœ¨å¤šæ‰«æä»ªå…¨åˆ‡ç‰‡å›¾åƒæ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œç”¨äºå®«é¢ˆç™Œè¯Šæ–­ã€‚å°½ç®¡æˆ‘ä»¬çš„æ–¹æ³•æ˜¯åœ¨å•ä¸€æ‰«æä»ªçš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œä½†åœ¨å…¶ä»–æ‰«æä»ªçš„ç¦»åˆ†å¸ƒæ•°æ®ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/caijd2000/LSA%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/caijd2000/LSAä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06563v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿåœ¨å®«é¢ˆç™Œç­›æŸ¥ä¸­ä½¿ç”¨å…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰æ—¶é¢ä¸´é¢†åŸŸåç§»çš„æŒ‘æˆ˜ï¼Œå› ä¸ºä¸åŒæ‰«æä»ªå’Œæˆåƒç¯å¢ƒçš„æŸ“è‰²å˜åŒ–å¯¼è‡´çš„ã€‚ç°æœ‰çš„æŸ“è‰²å¢å¼ºæ–¹æ³•åœ¨è¡¥ä¸çº§åˆ«ä¸Šæé«˜äº†é²æ£’æ€§ï¼Œä½†ç”±äºå…³é”®å±€é™æ— æ³•æ‰©å±•åˆ°WSIã€‚æˆ‘ä»¬æå‡ºæ½œæ€æŸ“è‰²å¢å¼ºï¼ˆLSAï¼‰æ¡†æ¶ï¼Œç›´æ¥åœ¨WSIçº§åˆ«çš„æ½œåœ¨ç‰¹å¾ä¸Šè¿›è¡Œé«˜æ•ˆçš„åœ¨çº¿æŸ“è‰²å¢å¼ºã€‚é€šè¿‡å¼•å…¥WSAugæŸ“è‰²å¢å¼ºæ–¹æ³•å’ŒStain Transformerï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªèƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´ä¸­æ¨¡æ‹Ÿç›®æ ‡é£æ ¼çš„æ¨¡å‹ï¼Œæœ‰æ•ˆæé«˜äº†WSIçº§åˆ«åˆ†ç±»å™¨çš„ç¨³å¥æ€§ã€‚åœ¨å¤šä¸ªæ‰«æä»ªçš„WSIæ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¥è‡ªå…¶ä»–æ‰«æä»ªçš„ç¦»ç¾¤æ•°æ®ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè¾…åŠ©è¯Šæ–­ç³»ç»Ÿåœ¨ä½¿ç”¨å…¨åˆ‡ç‰‡å›¾åƒè¿›è¡Œå®«é¢ˆç™Œç­›æŸ¥æ—¶é¢ä¸´æŸ“è‰²å˜åŒ–å¯¼è‡´çš„é¢†åŸŸåç§»æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æŸ“è‰²å¢å¼ºæ–¹æ³•æ— æ³•æ‰©å±•åˆ°å…¨åˆ‡ç‰‡å›¾åƒï¼Œå­˜åœ¨è¡¥ä¸çº§åˆ«çš„ä¸ä¸€è‡´æ€§å’Œé«˜è®¡ç®—å­˜å‚¨æˆæœ¬é—®é¢˜ã€‚</li>
<li>æ½œæ€æŸ“è‰²å¢å¼ºï¼ˆLSAï¼‰æ¡†æ¶æå‡ºåœ¨çº¿æŸ“è‰²å¢å¼ºæ–¹æ³•ï¼Œç›´æ¥åœ¨WSIçº§åˆ«ä¸Šè¿›è¡Œã€‚</li>
<li>WSAugæŸ“è‰²å¢å¼ºæ–¹æ³•ç¡®ä¿å…¨åˆ‡ç‰‡å›¾åƒå†…è¡¥ä¸é—´æŸ“è‰²çš„ä¸€è‡´æ€§ã€‚</li>
<li>Stain Transformerè®¾è®¡ç”¨äºæ¨¡æ‹Ÿæ½œåœ¨ç©ºé—´ä¸­çš„ç›®æ ‡é£æ ¼ï¼Œæé«˜WSIçº§åˆ«åˆ†ç±»å™¨çš„ç¨³å¥æ€§ã€‚</li>
<li>åœ¨å¤šæ‰«æä»ªWSIæ•°æ®é›†ä¸Šçš„éªŒè¯æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ¥è‡ªå…¶ä»–æ‰«æä»ªçš„ç¦»ç¾¤æ•°æ®ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06563">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72ed7127a136eff4d2d0dd561d999dac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10f776fe0a292cb7fc12688326e241f6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbdfcd5027d9e256d82a349301ce1502.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-df8c6c52eb87c5f464725b704c0e6d34.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="PathVQ-Reforming-Computational-Pathology-Foundation-Model-for-Whole-Slide-Image-Analysis-via-Vector-Quantization"><a href="#PathVQ-Reforming-Computational-Pathology-Foundation-Model-for-Whole-Slide-Image-Analysis-via-Vector-Quantization" class="headerlink" title="PathVQ: Reforming Computational Pathology Foundation Model for Whole   Slide Image Analysis via Vector Quantization"></a>PathVQ: Reforming Computational Pathology Foundation Model for Whole   Slide Image Analysis via Vector Quantization</h2><p><strong>Authors:Honglin Li, Zhongyi Shui, Yunlong Zhang, Chenglu Zhu, Lin Yang</strong></p>
<p>Computational pathology and whole-slide image (WSI) analysis are pivotal in cancer diagnosis and prognosis. However, the ultra-high resolution of WSIs presents significant modeling challenges. Recent advancements in pathology foundation models have improved performance, yet most approaches rely on [CLS] token representation of tile ViT as slide-level inputs (16x16 pixels is refereed as patch and 224x224 pixels as tile). This discards critical spatial details from patch tokens, limiting downstream WSI analysis tasks. We find that leveraging all spatial patch tokens benefits WSI analysis but incurs nearly 200x higher storage and training costs (e.g., 196 tokens in ViT$_{224}$). To address this, we introduce vector quantized (VQ) distillation on patch feature, which efficiently compresses spatial patch tokens using discrete indices and a decoder. Our method reduces token dimensionality from 1024 to 16, achieving a 64x compression rate while preserving reconstruction fidelity. Furthermore, we employ a multi-scale VQ (MSVQ) strategy, which not only enhances VQ reconstruction performance but also serves as a Self-supervised Learning (SSL) supervision for a seamless slide-level pretraining objective. Built upon the quantized patch features and supervision targets of tile via MSVQ, we develop a progressive convolutional module and slide-level SSL to extract representations with rich spatial-information for downstream WSI tasks. Extensive evaluations on multiple datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance in WSI analysis. Code will be available soon. </p>
<blockquote>
<p>è®¡ç®—ç—…ç†å­¦å’Œå…¨åˆ‡ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†æåœ¨ç™Œç—‡è¯Šæ–­å’Œæ²»ç–—ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼ŒWSIçš„è¶…é«˜åˆ†è¾¨ç‡ç»™å»ºæ¨¡å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚å°½ç®¡ç—…ç†å­¦åŸºç¡€æ¨¡å‹çš„æœ€æ–°è¿›å±•æé«˜äº†æ€§èƒ½ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•ä»ç„¶ä¾èµ–äºå°†ç“·ç –ViTçš„[CLS]ä»¤ç‰Œè¡¨ç¤ºä½œä¸ºåˆ‡ç‰‡çº§åˆ«çš„è¾“å…¥ï¼ˆå…¶ä¸­ï¼Œå°†16x16åƒç´ ç§°ä¸ºè¡¥ä¸ï¼Œè€Œ224x224åƒç´ ç§°ä¸ºç“·ç –ï¼‰ã€‚è¿™ç§åšæ³•ä¸¢å¼ƒäº†è¡¥ä¸ä»¤ç‰Œä¸­çš„å…³é”®ç©ºé—´ç»†èŠ‚ï¼Œé™åˆ¶äº†åç»­çš„WSIåˆ†æä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°åˆ©ç”¨æ‰€æœ‰ç©ºé—´è¡¥ä¸ä»¤ç‰Œå¯¹WSIåˆ†ææ˜¯æœ‰ç›Šçš„ï¼Œä½†è¿™ä¼šå¯¼è‡´å­˜å‚¨å’Œè®­ç»ƒæˆæœ¬å¢åŠ è¿‘200å€ï¼ˆä¾‹å¦‚ï¼ŒViTä¸­ä½¿ç”¨çš„ä»¤ç‰Œé«˜è¾¾196ä¸ªï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºè¡¥ä¸ç‰¹å¾çš„å‘é‡é‡åŒ–ï¼ˆVQï¼‰è’¸é¦æŠ€æœ¯ã€‚ä½¿ç”¨ç¦»æ•£ç´¢å¼•å’Œè§£ç å™¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é«˜æ•ˆåœ°å‹ç¼©ç©ºé—´è¡¥ä¸ä»¤ç‰Œï¼Œå°†ä»¤ç‰Œç»´åº¦ä»1024é™ä½åˆ°16ï¼Œå®ç°äº†é«˜è¾¾64å€çš„å‹ç¼©ç‡ï¼ŒåŒæ—¶ä¿æŒäº†é‡å»ºä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†å¤šå°ºåº¦VQï¼ˆMSVQï¼‰ç­–ç•¥ï¼Œè¿™ä¸ä»…æé«˜äº†VQé‡å»ºæ€§èƒ½ï¼Œè€Œä¸”è¿˜ä½œä¸ºä¸€ç§è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰çš„ç›‘ç£æ‰‹æ®µï¼Œå®ç°äº†æ— ç¼çš„åˆ‡ç‰‡çº§é¢„è®­ç»ƒç›®æ ‡ã€‚åŸºäºé‡åŒ–è¡¥ä¸ç‰¹å¾å’Œé€šè¿‡MSVQè·å¾—çš„ç“·ç –ç›‘ç£ç›®æ ‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ¸è¿›å¼å·ç§¯æ¨¡å—å’Œåˆ‡ç‰‡çº§SSLï¼Œä»¥æå–å…·æœ‰ä¸°å¯Œç©ºé—´ä¿¡æ¯çš„è¡¨ç¤ºç”¨äºä¸‹æ¸¸WSIä»»åŠ¡ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†æœ€å…ˆè¿›çš„æ•ˆæœã€‚ä»£ç å°†åœ¨è¿‘æœŸå‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06482v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    è®¡ç®—ç—…ç†å­¦å’Œå…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†æåœ¨ç™Œç—‡è¯Šæ–­å’Œæ²»ç–—ä¸­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼ŒWSIçš„è¶…é«˜åˆ†è¾¨ç‡å¸¦æ¥äº†æ˜¾è‘—çš„å»ºæ¨¡æŒ‘æˆ˜ã€‚è™½ç„¶ç—…ç†å­¦åŸºç¡€æ¨¡å‹çš„æœ€æ–°è¿›å±•æé«˜äº†æ€§èƒ½ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•ä»ç„¶ä¾èµ–äº[CLS]æ ‡è®°è¡¨ç¤ºç“·ç –ViTä½œä¸ºå¹»ç¯ç‰‡çº§åˆ«çš„è¾“å…¥ï¼ˆä»¥å°†ä¸€ä¸ªå¹»ç¯ç‰‡åˆ†å‰²æˆè‹¥å¹²å—çš„æ–¹å¼ï¼‰ï¼Œè¿™ç§æ–¹å¼å¿½ç•¥äº†è¡¥ä¸ä»¤ç‰Œçš„ç©ºé—´ç»†èŠ‚ï¼Œé™åˆ¶äº†ä¸‹æ¸¸WSIåˆ†æä»»åŠ¡çš„æ•ˆæœã€‚æœ¬ç ”ç©¶é€šè¿‡åˆ©ç”¨æ‰€æœ‰ç©ºé—´è¡¥ä¸ä»¤ç‰Œå¯¹WSIåˆ†æè¿›è¡Œæ”¹è¿›ï¼Œä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†è¿‘200å€çš„å­˜å‚¨å’Œè®­ç»ƒæˆæœ¬å¢åŠ ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†çŸ¢é‡é‡åŒ–ï¼ˆVQï¼‰è’¸é¦è¡¥ä¸ç‰¹å¾æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ä½¿ç”¨ç¦»æ•£ç´¢å¼•å’Œè§£ç å™¨æœ‰æ•ˆåœ°å‹ç¼©ç©ºé—´è¡¥ä¸ä»¤ç‰Œã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ä»¤ç‰Œç»´åº¦ä»1024å‡å°‘åˆ°16ï¼Œå®ç°äº†64å€çš„å‹ç¼©ç‡ï¼ŒåŒæ—¶ä¿æŒäº†é‡å»ºä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†å¤šå°ºåº¦VQï¼ˆMSVQï¼‰ç­–ç•¥ï¼Œä¸ä»…æé«˜äº†VQé‡å»ºæ€§èƒ½ï¼Œè¿˜ä½œä¸ºæ— ç¼å¹»ç¯ç‰‡çº§åˆ«çš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ç›‘ç£ï¼Œç”¨äºé¢„è®­ç»ƒç›®æ ‡ã€‚åŸºäºé‡åŒ–è¡¥ä¸ç‰¹å¾å’Œé€šè¿‡MSVQè·å¾—çš„ç“·ç –ç›‘ç£ç›®æ ‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ¸è¿›çš„å·ç§¯æ¨¡å—å’Œå¹»ç¯ç‰‡çº§åˆ«çš„SSLï¼Œä»¥æå–å…·æœ‰ä¸°å¯Œç©ºé—´ä¿¡æ¯çš„è¡¨ç¤ºï¼Œç”¨äºä¸‹æ¸¸WSIä»»åŠ¡ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†WSIåˆ†æçš„æœ€å…ˆè¿›æ€§èƒ½ã€‚ä»£ç å°†å¾ˆå¿«å¯ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è®¡ç®—ç—…ç†å­¦åŠå…¨å¹»ç¯ç‰‡å›¾åƒåˆ†æåœ¨ç™Œç—‡è¯Šæ–­ä¸é¢„åè¯„ä¼°ä¸­èµ·å…³é”®ä½œç”¨ã€‚</li>
<li>WSIçš„è¶…é«˜åˆ†è¾¨ç‡å¸¦æ¥äº†å»ºæ¨¡æŒ‘æˆ˜ã€‚</li>
<li>å¤§å¤šæ•°ç°æœ‰çš„æ–¹æ³•ä¾èµ–äºç‰¹å®šçš„æ ‡è®°è¡¨ç¤ºæ–¹å¼ï¼ˆå¦‚ViTæ¨¡å‹ä¸­çš„ç“·ç –çº§åˆ«è¾“å…¥ï¼‰ï¼Œè¿™é™åˆ¶äº†ç©ºé—´ç»†èŠ‚çš„æ•è·å’Œåˆ†ææ•ˆæœã€‚</li>
<li>ç›´æ¥åˆ©ç”¨æ‰€æœ‰ç©ºé—´ç»†èŠ‚ï¼ˆè¡¥ä¸ä»¤ç‰Œï¼‰è™½ç„¶èƒ½æ”¹å–„åˆ†ææ•ˆæœï¼Œä½†æˆæœ¬æ˜¾è‘—å¢åŠ ã€‚</li>
<li>æå‡ºçŸ¢é‡é‡åŒ–ï¼ˆVQï¼‰è’¸é¦è¡¥ä¸ç‰¹å¾æŠ€æœ¯ï¼Œèƒ½åœ¨å‹ç¼©ç©ºé—´è¡¥ä¸ä»¤ç‰Œçš„åŒæ—¶ä¿æŒé‡å»ºçš„ä¿çœŸåº¦ã€‚è¿™ç§æŠ€æœ¯æ˜¾è‘—å‡å°‘äº†å­˜å‚¨å’Œè®­ç»ƒæˆæœ¬ã€‚</li>
<li>å¤šå°ºåº¦VQï¼ˆMSVQï¼‰ç­–ç•¥ä¸ä»…å¢å¼ºäº†VQé‡å»ºæ€§èƒ½ï¼Œè¿˜æä¾›äº†è‡ªç›‘ç£å­¦ä¹ çš„é¢„è®­ç»ƒç›®æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06482">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7532184b623ca7f93ae673fe3e6c469d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56829c1e36c9bc956a8342a6f76a11e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0ec9a893000a2ca22c638cbace53abb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e94115e3d8140c95f62a6db88223ece.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-3b30d3ad12a009fd1a97361cbc68898b.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  Clip-TTS Contrastive Text-content and Mel-spectrogram, A High-Quality   Text-to-Speech Method based on Contextual Semantic Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-853b5edd0ba7d7e6710b25641dc4e07d.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  Enhanced Pediatric Dental Segmentation Using a Custom SegUNet with VGG19   Backbone on Panoramic Radiographs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29580.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
