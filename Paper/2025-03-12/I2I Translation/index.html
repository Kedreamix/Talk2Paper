<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  LBM Latent Bridge Matching for Fast Image-to-Image Translation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-9ec00fdcae204d41db27618a87ceed9b.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    34 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-12-æ›´æ–°"><a href="#2025-03-12-æ›´æ–°" class="headerlink" title="2025-03-12 æ›´æ–°"></a>2025-03-12 æ›´æ–°</h1><h2 id="LBM-Latent-Bridge-Matching-for-Fast-Image-to-Image-Translation"><a href="#LBM-Latent-Bridge-Matching-for-Fast-Image-to-Image-Translation" class="headerlink" title="LBM: Latent Bridge Matching for Fast Image-to-Image Translation"></a>LBM: Latent Bridge Matching for Fast Image-to-Image Translation</h2><p><strong>Authors:ClÃ©ment Chadebec, Onur Tasar, Sanjeev Sreetharan, Benjamin Aubin</strong></p>
<p>In this paper, we introduce Latent Bridge Matching (LBM), a new, versatile and scalable method that relies on Bridge Matching in a latent space to achieve fast image-to-image translation. We show that the method can reach state-of-the-art results for various image-to-image tasks using only a single inference step. In addition to its efficiency, we also demonstrate the versatility of the method across different image translation tasks such as object removal, normal and depth estimation, and object relighting. We also derive a conditional framework of LBM and demonstrate its effectiveness by tackling the tasks of controllable image relighting and shadow generation. We provide an open-source implementation of the method at <a target="_blank" rel="noopener" href="https://github.com/gojasper/LBM">https://github.com/gojasper/LBM</a>. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æ½œåœ¨æ¡¥æ¢åŒ¹é…ï¼ˆLBMï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‡ºç°çš„é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ–¹æ³•ï¼Œå®ƒä¾èµ–äºæ½œåœ¨ç©ºé—´ä¸­çš„æ¡¥æ¢åŒ¹é…æ¥å®ç°å¿«é€Ÿå›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨ä¸€ä¸ªæ¨ç†æ­¥éª¤å°±å¯ä»¥åœ¨å„ç§å›¾åƒåˆ°å›¾åƒçš„ä»»åŠ¡ä¸Šè¾¾åˆ°æœ€æ–°æŠ€æœ¯ç»“æœã€‚é™¤äº†é«˜æ•ˆæ€§ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨ä¸åŒå›¾åƒç¿»è¯‘ä»»åŠ¡ä¸­çš„é€šç”¨æ€§ï¼Œå¦‚ç›®æ ‡ç§»é™¤ã€æ³•çº¿å’Œæ·±åº¦ä¼°è®¡ä»¥åŠç›®æ ‡é‡æ–°ç…§æ˜ç­‰ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†LBMçš„æ¡ä»¶æ¡†æ¶ï¼Œå¹¶é€šè¿‡è§£å†³å¯æ§å›¾åƒé‡æ–°ç…§æ˜å’Œé˜´å½±ç”Ÿæˆä»»åŠ¡æ¥è¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/gojasper/LBM%E4%B8%8A%E6%8F%90%E4%BE%9B%E4%BA%86%E8%AF%A5%E6%96%B9%E6%B3%95%E7%9A%84%E5%BC%80%E6%BA%90%E5%AE%9E%E7%8E%B0%E3%80%82">https://github.com/gojasper/LBMä¸Šæä¾›äº†è¯¥æ–¹æ³•çš„å¼€æºå®ç°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07535v1">PDF</a> </p>
<p><strong>Summary</strong><br>æœ¬è®ºæ–‡æå‡ºä¸€ç§åä¸ºLatent Bridge Matchingï¼ˆLBMï¼‰çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨æ½œåœ¨ç©ºé—´çš„Bridge Matchingå®ç°å¿«é€Ÿå›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ï¼Œå…·æœ‰é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç§å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä»»åŠ¡ä¸­èƒ½è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä»…ä½¿ç”¨å•ä¸ªæ¨ç†æ­¥éª¤å³å¯å®Œæˆã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥ç”¨äºå¤šç§å›¾åƒå¤„ç†ä»»åŠ¡ï¼Œå¦‚å¯¹è±¡å»é™¤ã€å…‰ç…§è°ƒæ•´å’Œæ·±åº¦ä¼°è®¡ç­‰ã€‚æˆ‘ä»¬ä¸ºLBMæ„å»ºäº†ä¸€ä¸ªæ¡ä»¶æ¡†æ¶ï¼Œé€šè¿‡æ§åˆ¶å›¾åƒé‡æ–°ç…§æ˜å’Œé˜´å½±ç”Ÿæˆç­‰ä»»åŠ¡å±•ç¤ºå…¶æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å…¬å¼€äº†LBMæ–¹æ³•çš„å¼€æºå®ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Latent Bridge Matching (LBM)æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼ŒåŸºäºæ½œåœ¨ç©ºé—´çš„Bridge Matchingè¿›è¡Œå¿«é€Ÿå›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ã€‚</li>
<li>LBMå…·æœ‰é€šç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œå¯ä»¥åº”ç”¨äºå¤šç§å›¾åƒç¿»è¯‘ä»»åŠ¡ã€‚</li>
<li>LBMèƒ½åœ¨å„ç§å›¾åƒåˆ°å›¾åƒçš„ç¿»è¯‘ä»»åŠ¡ä¸­è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>LBMä»…éœ€ä¸€ä¸ªæ¨ç†æ­¥éª¤å°±èƒ½å®Œæˆç¿»è¯‘ä»»åŠ¡ï¼Œä½“ç°äº†å…¶é«˜æ•ˆæ€§ã€‚</li>
<li>LBMå¯ç”¨äºå›¾åƒå¤„ç†ä»»åŠ¡ï¼Œå¦‚å¯¹è±¡å»é™¤ã€å…‰ç…§è°ƒæ•´å’Œæ·±åº¦ä¼°è®¡ç­‰ã€‚</li>
<li>LBMå…·æœ‰å¯æ§æ€§ï¼Œé€šè¿‡æ¡ä»¶æ¡†æ¶å®ç°å›¾åƒé‡æ–°ç…§æ˜å’Œé˜´å½±ç”Ÿæˆç­‰ä»»åŠ¡çš„æ§åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ec00fdcae204d41db27618a87ceed9b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c69c581fecd365af93a47f6a2a9452a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-64b8b7b767104850d032d70c3bff4ff2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ca93ad39b6d8cf9cd47a27193fbf3d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61bdcd2239a9fde7fc3688f786f56682.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ebd9a0a974e6dbeba9fa75cb384c227.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SEED-Towards-More-Accurate-Semantic-Evaluation-for-Visual-Brain-Decoding"><a href="#SEED-Towards-More-Accurate-Semantic-Evaluation-for-Visual-Brain-Decoding" class="headerlink" title="SEED: Towards More Accurate Semantic Evaluation for Visual Brain   Decoding"></a>SEED: Towards More Accurate Semantic Evaluation for Visual Brain   Decoding</h2><p><strong>Authors:Juhyeon Park, Peter Yongho Kim, Jiook Cha, Shinjae Yoo, Taesup Moon</strong></p>
<p>We present SEED (\textbf{Se}mantic \textbf{E}valuation for Visual Brain \textbf{D}ecoding), a novel metric for evaluating the semantic decoding performance of visual brain decoding models. It integrates three complementary metrics, each capturing a different aspect of semantic similarity between images. Using carefully crowd-sourced human judgment data, we demonstrate that SEED achieves the highest alignment with human evaluations, outperforming other widely used metrics. Through the evaluation of existing visual brain decoding models, we further reveal that crucial information is often lost in translation, even in state-of-the-art models that achieve near-perfect scores on existing metrics. To facilitate further research, we open-source the human judgment data, encouraging the development of more advanced evaluation methods for brain decoding models. Additionally, we propose a novel loss function designed to enhance semantic decoding performance by leveraging the order of pairwise cosine similarity in CLIP image embeddings. This loss function is compatible with various existing methods and has been shown to consistently improve their semantic decoding performances when used for training, with respect to both existing metrics and SEED. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†SEEDï¼ˆé¢å‘è§†è§‰å¤§è„‘è§£ç çš„è¯­ä¹‰è¯„ä¼°ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè¯„ä¼°è§†è§‰å¤§è„‘è§£ç æ¨¡å‹è¯­ä¹‰è§£ç æ€§èƒ½çš„æ–°å‹æŒ‡æ ‡ã€‚å®ƒé›†æˆäº†ä¸‰ä¸ªäº’è¡¥çš„æŒ‡æ ‡ï¼Œæ¯ä¸ªæŒ‡æ ‡éƒ½æ•æ‰å›¾åƒä¹‹é—´è¯­ä¹‰ç›¸ä¼¼æ€§çš„ä¸åŒæ–¹é¢ã€‚é€šè¿‡ä½¿ç”¨ç²¾å¿ƒæ”¶é›†çš„ä¼—æºäººç±»åˆ¤æ–­æ•°æ®ï¼Œæˆ‘ä»¬è¯æ˜SEEDä¸äººç±»è¯„ä¼°çš„å¥‘åˆåº¦æœ€é«˜ï¼Œä¼˜äºå…¶ä»–å¹¿æ³›ä½¿ç”¨çš„æŒ‡æ ‡ã€‚é€šè¿‡å¯¹ç°æœ‰è§†è§‰å¤§è„‘è§£ç æ¨¡å‹çš„è¯„ä¼°ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å‘ç°ï¼Œå³ä½¿åœ¨æœ€æ–°æ¨¡å‹ä¸­ï¼Œå³ä½¿å…¶åœ¨ç°æœ‰æŒ‡æ ‡ä¸Šçš„å¾—åˆ†æ¥è¿‘å®Œç¾ï¼Œç¿»è¯‘è¿‡ç¨‹ä¸­ä¹Ÿç»å¸¸ä¼šä¸¢å¤±å…³é”®ä¿¡æ¯ã€‚ä¸ºäº†ä¿ƒè¿›è¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬å¼€æºäº†äººç±»åˆ¤æ–­æ•°æ®ï¼Œé¼“åŠ±å¼€å‘æ›´å…ˆè¿›çš„è„‘è§£ç æ¨¡å‹è¯„ä¼°æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨åˆ©ç”¨CLIPå›¾åƒåµŒå…¥ä¸­çš„æˆå¯¹ä½™å¼¦ç›¸ä¼¼æ€§çš„é¡ºåºæ¥æé«˜è¯­ä¹‰è§£ç æ€§èƒ½ã€‚æ­¤æŸå¤±å‡½æ•°ä¸å„ç§ç°æœ‰æ–¹æ³•å…¼å®¹ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒå’Œè¯„ä¼°æ—¶ï¼Œæ— è®ºæ˜¯åœ¨ç°æœ‰æŒ‡æ ‡è¿˜æ˜¯SEEDä¸Šï¼Œéƒ½å·²è¢«è¯æ˜å¯ä»¥æŒç»­æé«˜å…¶è¯­ä¹‰è§£ç æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06437v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>SEEDï¼ˆè¯­ä¹‰è¯„ä¼°ç”¨äºè§†è§‰å¤§è„‘è§£ç ï¼‰æ˜¯ä¸€ç§æ–°å‹æŒ‡æ ‡ï¼Œç”¨äºè¯„ä¼°è§†è§‰å¤§è„‘è§£ç æ¨¡å‹çš„è¯­ä¹‰è§£ç æ€§èƒ½ã€‚å®ƒé€šè¿‡é›†æˆä¸‰ä¸ªäº’è¡¥çš„æŒ‡æ ‡ï¼Œä»å›¾åƒé—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§æ–¹é¢è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚åŸºäºå¹¿æ³›æ”¶é›†çš„äººç±»åˆ¤æ–­æ•°æ®ï¼ŒSEEDè¡¨ç°å‡ºä¸äººç±»è¯„ä»·çš„é«˜åº¦ä¸€è‡´æ€§ï¼Œä¼˜äºå…¶ä»–å¸¸ç”¨æŒ‡æ ‡ã€‚å¯¹ç°æœ‰è§†è§‰å¤§è„‘è§£ç æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œå³ä½¿åœ¨å¾—åˆ†æ¥è¿‘å®Œç¾çš„æ¨¡å‹ä¸­ï¼Œä¹Ÿå­˜åœ¨é‡è¦ä¿¡æ¯çš„ä¸¢å¤±ã€‚æˆ‘ä»¬å…¬å¼€äº†äººç±»åˆ¤æ–­æ•°æ®ï¼Œä»¥ä¿ƒè¿›æ›´å…ˆè¿›çš„è¯„ä¼°æ–¹æ³•çš„å‘å±•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œåˆ©ç”¨CLIPå›¾åƒåµŒå…¥ä¸­çš„æˆå¯¹ä½™å¼¦ç›¸ä¼¼æ€§é¡ºåºï¼Œæ—¨åœ¨æé«˜è¯­ä¹‰è§£ç æ€§èƒ½ã€‚è¯¥æŸå¤±å‡½æ•°ä¸ç°æœ‰æ–¹æ³•å…¼å®¹ï¼Œåœ¨è®­ç»ƒå’Œè¯„ä¼°æ—¶å‡èƒ½æé«˜è¯­ä¹‰è§£ç æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SEEDæ˜¯ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡è§†è§‰å¤§è„‘è§£ç æ¨¡å‹çš„è¯­ä¹‰è§£ç æ€§èƒ½ã€‚</li>
<li>SEEDé›†æˆäº†ä¸‰ä¸ªäº’è¡¥æŒ‡æ ‡ï¼Œåæ˜ å›¾åƒé—´çš„ä¸åŒè¯­ä¹‰ç›¸ä¼¼æ€§æ–¹é¢ã€‚</li>
<li>SEEDåŸºäºå¹¿æ³›æ”¶é›†çš„äººç±»åˆ¤æ–­æ•°æ®ï¼Œä¸äººç±»è¯„ä»·é«˜åº¦ä¸€è‡´ï¼Œä¼˜äºå…¶ä»–å¸¸ç”¨æŒ‡æ ‡ã€‚</li>
<li>å¯¹ç°æœ‰è§†è§‰å¤§è„‘è§£ç æ¨¡å‹çš„è¯„ä¼°å‘ç°ï¼Œå³ä½¿åœ¨é«˜åˆ†æ¨¡å‹ä¸­ï¼Œä¹Ÿå­˜åœ¨é‡è¦ä¿¡æ¯çš„ä¸¢å¤±ã€‚</li>
<li>å…¬å¼€äº†äººç±»åˆ¤æ–­æ•°æ®ä»¥ä¿ƒè¿›æ›´å…ˆè¿›çš„è¯„ä¼°æ–¹æ³•çš„å‘å±•ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œåˆ©ç”¨CLIPå›¾åƒåµŒå…¥ä¸­çš„æˆå¯¹ä½™å¼¦ç›¸ä¼¼æ€§é¡ºåºæ¥æé«˜è¯­ä¹‰è§£ç æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06437">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-42cb57cc08da90038e0311fc215f9d5a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9d862ccbf77af299e6690fca886f84b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c5024a351c8a2b2fc2ebdf157e675877.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-df364130c49d119827d1d4badf271aa0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model"><a href="#PTDiffusion-Free-Lunch-for-Generating-Optical-Illusion-Hidden-Pictures-with-Phase-Transferred-Diffusion-Model" class="headerlink" title="PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model"></a>PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures   with Phase-Transferred Diffusion Model</h2><p><strong>Authors:Xiang Gao, Shuai Yang, Jiaying Liu</strong></p>
<p>Optical illusion hidden picture is an interesting visual perceptual phenomenon where an image is cleverly integrated into another picture in a way that is not immediately obvious to the viewer. Established on the off-the-shelf text-to-image (T2I) diffusion model, we propose a novel training-free text-guided image-to-image (I2I) translation framework dubbed as \textbf{P}hase-\textbf{T}ransferred \textbf{Diffusion} Model (PTDiffusion) for hidden art syntheses. PTDiffusion embeds an input reference image into arbitrary scenes as described by the text prompts, while exhibiting hidden visual cues of the reference image. At the heart of our method is a plug-and-play phase transfer mechanism that dynamically and progressively transplants diffusion featuresâ€™ phase spectrum from the denoising process to reconstruct the reference image into the one to sample the generated illusion image, realizing harmonious fusion of the reference structural information and the textual semantic information. Furthermore, we propose asynchronous phase transfer to enable flexible control to the degree of hidden content discernability. Our method bypasses any model training and fine-tuning, all while substantially outperforming related methods in image quality, text fidelity, visual discernibility, and contextual naturalness for illusion picture synthesis, as demonstrated by extensive qualitative and quantitative experiments. </p>
<blockquote>
<p>éšè—å›¾ç‰‡çš„è§†è§‰é”™è§‰æ˜¯ä¸€ç§æœ‰è¶£çš„è§†è§‰æ„ŸçŸ¥ç°è±¡ï¼Œå…¶ä¸­ä¸€å¼ å›¾ç‰‡è¢«å·§å¦™åœ°èå…¥åˆ°å¦ä¸€å¼ å›¾ç‰‡ä¸­ï¼Œä»¥è‡³äºè§‚ä¼—æ— æ³•ç«‹å³å¯Ÿè§‰ã€‚æˆ‘ä»¬åŸºäºç°æˆçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ–‡å­—å¼•å¯¼å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰è½¬æ¢æ¡†æ¶ï¼Œåä¸ºç›¸ä½è½¬ç§»æ‰©æ•£æ¨¡å‹ï¼ˆPTDiffusionï¼‰ï¼Œç”¨äºåˆæˆéšè—è‰ºæœ¯ã€‚PTDiffusionå°†è¾“å…¥å‚è€ƒå›¾åƒåµŒå…¥åˆ°æ–‡æœ¬æç¤ºæè¿°çš„ä»»æ„åœºæ™¯ä¸­ï¼ŒåŒæ—¶å±•ç¤ºå‚è€ƒå›¾åƒçš„éšè—è§†è§‰çº¿ç´¢ã€‚æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„ç›¸ä½è½¬ç§»æœºåˆ¶ï¼Œå®ƒåŠ¨æ€ä¸”æ¸è¿›åœ°ç§»æ¤æ‰©æ•£ç‰¹å¾çš„ç›¸ä½è°±ï¼Œä»å»å™ªè¿‡ç¨‹ä¸­é‡å»ºå‚è€ƒå›¾åƒåˆ°ç”Ÿæˆé”™è§‰å›¾åƒçš„é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œå®ç°å‚è€ƒç»“æ„ä¿¡æ¯å’Œæ–‡æœ¬è¯­ä¹‰ä¿¡æ¯çš„å’Œè°èåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å¼‚æ­¥ç›¸ä½è½¬ç§»ï¼Œä»¥å®ç°çµæ´»æ§åˆ¶éšè—å†…å®¹å¯è¾¨è¯†åº¦çš„ç¨‹åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¿å¼€äº†ä»»ä½•æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒï¼ŒåŒæ—¶å¤§å¤§æå‡äº†å›¾åƒè´¨é‡ã€æ–‡æœ¬å¿ å®åº¦ã€è§†è§‰è¾¨è¯†åº¦å’Œä¸Šä¸‹æ–‡è‡ªç„¶åº¦ï¼Œåœ¨é”™è§‰å›¾ç‰‡åˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¿™å¾—åˆ°äº†å¹¿æ³›çš„è´¨é‡å’Œæ•°é‡å®éªŒçš„éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06186v1">PDF</a> Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern   Recognition (CVPR 2025)</p>
<p><strong>Summary</strong></p>
<p>å…‰å­¦é”™è§‰éšè—å›¾åƒæ˜¯ä¸€ç§æœ‰è¶£çš„è§†è§‰æ„ŸçŸ¥ç°è±¡ï¼Œå…¶ä¸­å›¾åƒè¢«å·§å¦™åœ°èå…¥å¦ä¸€å¹…å›¾ç‰‡ä¸­ï¼Œè§‚ä¼—æ— æ³•ç«‹å³å¯Ÿè§‰ã€‚æœ¬ç ”ç©¶åŸºäºç°æˆçš„æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–‡å­—å¼•å¯¼å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰è½¬æ¢æ¡†æ¶â€”â€”ç›¸ä½è½¬ç§»æ‰©æ•£æ¨¡å‹ï¼ˆPTDiffusionï¼‰ï¼Œç”¨äºåˆæˆéšè—è‰ºæœ¯å›¾åƒã€‚PTDiffusionå°†è¾“å…¥å‚è€ƒå›¾åƒåµŒå…¥æ–‡æœ¬æç¤ºæè¿°çš„ä»»æ„åœºæ™¯ä¸­ï¼ŒåŒæ—¶å±•ç¤ºå‚è€ƒå›¾åƒçš„éšè—è§†è§‰çº¿ç´¢ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„ç›¸ä½è½¬ç§»æœºåˆ¶ï¼Œå®ƒåŠ¨æ€ä¸”æ¸è¿›åœ°ç§»æ¤æ‰©æ•£ç‰¹å¾çš„ç›¸ä½è°±ï¼Œä»å»å™ªè¿‡ç¨‹ä¸­é‡å»ºå‚è€ƒå›¾åƒï¼Œå°†å…¶èå…¥ç”Ÿæˆçš„é”™è§‰å›¾åƒä¸­ï¼Œå®ç°å‚è€ƒç»“æ„ä¿¡æ¯å’Œæ–‡æœ¬è¯­ä¹‰ä¿¡æ¯çš„å’Œè°èåˆã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æå‡ºäº†å¼‚æ­¥ç›¸ä½è½¬ç§»ï¼Œå®ç°å¯¹éšè—å†…å®¹è¾¨è¯†åº¦çš„çµæ´»æ§åˆ¶ã€‚è¯¥æ–¹æ³•æ— éœ€ä»»ä½•æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒï¼Œåœ¨å›¾åƒè´¨é‡ã€æ–‡æœ¬å¿ å®åº¦ã€è§†è§‰è¾¨è¯†åº¦å’Œä¸Šä¸‹æ–‡è‡ªç„¶åº¦æ–¹é¢å¤§å¤§ä¼˜äºç›¸å…³æ–¹æ³•ï¼Œç”¨äºåˆæˆé”™è§‰å›¾åƒï¼Œå¦‚å¹¿æ³›çš„è´¨é‡å’Œæ•°é‡å®éªŒæ‰€ç¤ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…‰å­¦é”™è§‰éšè—å›¾ç‰‡æ˜¯ä¸€ç§è§†è§‰æ„ŸçŸ¥ç°è±¡ï¼Œå›¾åƒè¢«å·§å¦™åœ°èå…¥å¦ä¸€å¹…å›¾ç‰‡ä¸­ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ— éœ€è®­ç»ƒçš„æ–‡å­—å¼•å¯¼I2Iè½¬æ¢æ¡†æ¶â€”â€”ç›¸ä½è½¬ç§»æ‰©æ•£æ¨¡å‹ï¼ˆPTDiffusionï¼‰ã€‚</li>
<li>PTDiffusionå¯å°†è¾“å…¥å‚è€ƒå›¾åƒåµŒå…¥ä»»æ„åœºæ™¯ä¸­ï¼ŒåŒæ—¶å±•ç¤ºå…¶éšè—è§†è§‰çº¿ç´¢ã€‚</li>
<li>ç›¸ä½è½¬ç§»æœºåˆ¶æ˜¯è¯¥æ¡†æ¶çš„æ ¸å¿ƒï¼Œå¯åŠ¨æ€ã€æ¸è¿›åœ°ç§»æ¤æ‰©æ•£ç‰¹å¾çš„ç›¸ä½è°±ã€‚</li>
<li>å¼‚æ­¥ç›¸ä½è½¬ç§»ä½¿å¯¹éšè—å†…å®¹è¾¨è¯†åº¦çš„æ§åˆ¶æ›´ä¸ºçµæ´»ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒï¼Œåœ¨å›¾åƒè´¨é‡ã€æ–‡æœ¬å¿ å®åº¦ç­‰æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06186">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-83645c00a34d7ffd3790d7f7f076ae96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7eaaf2a9d0ccc30a7efbdd5937b621d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6005a17c2307fd503c9c683c80fb486c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9447fd81ce69dc56cb27f94aa559ab91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4093835e9ee8f1fac03a92910a207a22.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Fish2Mesh-Transformer-3D-Human-Mesh-Recovery-from-Egocentric-Vision"><a href="#Fish2Mesh-Transformer-3D-Human-Mesh-Recovery-from-Egocentric-Vision" class="headerlink" title="Fish2Mesh Transformer: 3D Human Mesh Recovery from Egocentric Vision"></a>Fish2Mesh Transformer: 3D Human Mesh Recovery from Egocentric Vision</h2><p><strong>Authors:David C. Jeong, Aditya Puranik, James Vong, Vrushabh Abhijit Deogirikar, Ryan Fell, Julianna Dietrich, Maria Kyrarini, Christopher Kitts</strong></p>
<p>Egocentric human body estimation allows for the inference of user body pose and shape from a wearable cameraâ€™s first-person perspective. Although research has used pose estimation techniques to overcome self-occlusions and image distortions caused by head-mounted fisheye images, similar advances in 3D human mesh recovery (HMR) techniques have been limited. We introduce Fish2Mesh, a fisheye-aware transformer-based model designed for 3D egocentric human mesh recovery. We propose an egocentric position embedding block to generate an ego-specific position table for the Swin Transformer to reduce fisheye image distortion. Our model utilizes multi-task heads for SMPL parametric regression and camera translations, estimating 3D and 2D joints as auxiliary loss to support model training. To address the scarcity of egocentric camera data, we create a training dataset by employing the pre-trained 4D-Human model and third-person cameras for weak supervision. Our experiments demonstrate that Fish2Mesh outperforms previous state-of-the-art 3D HMR models. </p>
<blockquote>
<p>è‡ªæˆ‘ä¸­å¿ƒçš„äººä½“ä¼°è®¡å…è®¸ä»å¯ç©¿æˆ´ç›¸æœºçš„ç¬¬ä¸€äººç§°è§†è§’æ¨æ–­ç”¨æˆ·çš„èº«ä½“å§¿åŠ¿å’Œå½¢çŠ¶ã€‚å°½ç®¡ç ”ç©¶å·²ç»ä½¿ç”¨å§¿æ€ä¼°è®¡æŠ€æœ¯æ¥å…‹æœå¤´éƒ¨å®‰è£…çš„é±¼çœ¼å›¾åƒå¼•èµ·çš„è‡ªæˆ‘é®æŒ¡å’Œå›¾åƒå¤±çœŸï¼Œä½†åœ¨3Däººä½“ç½‘æ ¼æ¢å¤ï¼ˆHMRï¼‰æŠ€æœ¯æ–¹é¢çš„ç±»ä¼¼è¿›å±•å´å—åˆ°é™åˆ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†Fish2Meshï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºé±¼çœ¼æ„ŸçŸ¥çš„å˜å‹å™¨æ¨¡å‹ï¼Œä¸“ä¸º3Dè‡ªæˆ‘ä¸­å¿ƒäººä½“ç½‘æ ¼æ¢å¤è®¾è®¡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªæˆ‘ä¸­å¿ƒä½ç½®åµŒå…¥å—ï¼Œä»¥ç”Ÿæˆé’ˆå¯¹Swin Transformerçš„è‡ªæˆ‘ç‰¹å®šä½ç½®è¡¨ï¼Œä»¥å‡å°‘é±¼çœ¼å›¾åƒå¤±çœŸã€‚æˆ‘ä»¬çš„æ¨¡å‹åˆ©ç”¨å¤šä»»åŠ¡å¤´è¿›è¡ŒSMPLå‚æ•°å›å½’å’Œç›¸æœºå¹³ç§»ï¼Œä¼°è®¡3Då’Œ2Då…³èŠ‚ä½œä¸ºè¾…åŠ©æŸå¤±ä»¥æ”¯æŒæ¨¡å‹è®­ç»ƒã€‚ä¸ºäº†è§£å†³è‡ªæˆ‘ä¸­å¿ƒç›¸æœºæ•°æ®çš„ç¨€ç¼ºæ€§é—®é¢˜ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„4D-Humanæ¨¡å‹å’Œç¬¬ä¸‰äººç§°ç›¸æœºè¿›è¡Œå¼±ç›‘ç£æ¥åˆ›å»ºè®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒFish2Meshä¼˜äºå…ˆå‰çš„æœ€æ–°3DHMRæ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06089v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶é€šè¿‡å¼•å…¥Fish2Meshæ¨¡å‹ï¼Œåˆ©ç”¨åŸºäºTransformerçš„æŠ€æœ¯ï¼Œå®ç°äº†ä»é±¼çœ¼è§†è§’å¯¹ä¸‰ç»´äººä½“ç½‘æ ¼æ¢å¤çš„ä¼°è®¡ã€‚è¯¥æ¨¡å‹å…·æœ‰è‡ªæˆ‘å®šä½åµŒå…¥å—ï¼Œèƒ½å¤Ÿç”Ÿæˆé’ˆå¯¹Swin Transformerçš„è‡ªæˆ‘ç‰¹å®šä½ç½®è¡¨ï¼Œå‡å°‘é±¼çœ¼å›¾åƒå¤±çœŸã€‚é€šè¿‡å¤šä»»åŠ¡å¤´è¿›è¡ŒSMPLå‚æ•°å›å½’å’Œç›¸æœºå¹³ç§»ä¼°è®¡ï¼ŒåŒæ—¶ä¼°è®¡ä¸‰ç»´å’ŒäºŒç»´å…³èŠ‚ä½œä¸ºè¾…åŠ©æŸå¤±æ”¯æŒæ¨¡å‹è®­ç»ƒã€‚ä¸ºè§£å†³ç¼ºä¹ç¬¬ä¸€äººç§°ç›¸æœºæ•°æ®çš„é—®é¢˜ï¼Œç ”ç©¶ä½¿ç”¨é¢„è®­ç»ƒçš„4D-Humanæ¨¡å‹å’Œç¬¬ä¸‰æ–¹ç›¸æœºè¿›è¡Œå¼±ç›‘ç£åˆ›å»ºè®­ç»ƒæ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼ŒFish2Meshåœ¨ä¸‰ç»´äººä½“ç½‘æ ¼æ¢å¤æ–¹é¢è¶…è¶Šäº†å…ˆå‰æœ€å…ˆè¿›çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Fish2Meshæ¨¡å‹è¢«å¼•å…¥ï¼Œè¯¥æ¨¡å‹ç”¨äºä»é±¼çœ¼è§†è§’è¿›è¡Œä¸‰ç»´äººä½“ç½‘æ ¼æ¢å¤ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨åŸºäºTransformerçš„æŠ€æœ¯ï¼Œå…·æœ‰è‡ªæˆ‘å®šä½åµŒå…¥å—ä»¥å‡å°‘é±¼çœ¼å›¾åƒå¤±çœŸã€‚</li>
<li>å¤šä»»åŠ¡å¤´ç”¨äºSMPLå‚æ•°å›å½’å’Œç›¸æœºå¹³ç§»ä¼°è®¡ã€‚</li>
<li>ä¸‰ç»´å’ŒäºŒç»´å…³èŠ‚ä¼°è®¡ä½œä¸ºè¾…åŠ©æŸå¤±æ”¯æŒæ¨¡å‹è®­ç»ƒã€‚</li>
<li>é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„4D-Humanæ¨¡å‹å’Œç¬¬ä¸‰æ–¹ç›¸æœºè¿›è¡Œå¼±ç›‘ç£åˆ›å»ºè®­ç»ƒæ•°æ®é›†æ¥è§£å†³ç¼ºä¹ç¬¬ä¸€äººç§°ç›¸æœºæ•°æ®çš„é—®é¢˜ã€‚</li>
<li>Fish2Meshåœ¨ä¸‰ç»´äººä½“ç½‘æ ¼æ¢å¤æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06089">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8460009a17df05484fa09a540a170b32.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-81ae232dd8cdbdd8984a2799f51df76d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6a5bb14a02edf397037285a090d97478.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e22fd94a36305aee632ae68b29ae6bca.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LapLoss-Laplacian-Pyramid-based-Multiscale-loss-for-Image-Translation"><a href="#LapLoss-Laplacian-Pyramid-based-Multiscale-loss-for-Image-Translation" class="headerlink" title="LapLoss: Laplacian Pyramid-based Multiscale loss for Image Translation"></a>LapLoss: Laplacian Pyramid-based Multiscale loss for Image Translation</h2><p><strong>Authors:Krish Didwania, Ishaan Gakhar, Prakhar Arya, Sanskriti Labroo</strong></p>
<p>Contrast enhancement, a key aspect of image-to-image translation (I2IT), improves visual quality by adjusting intensity differences between pixels. However, many existing methods struggle to preserve fine-grained details, often leading to the loss of low-level features. This paper introduces LapLoss, a novel approach designed for I2IT contrast enhancement, based on the Laplacian pyramid-centric networks, forming the core of our proposed methodology. The proposed approach employs a multiple discriminator architecture, each operating at a different resolution to capture high-level features, in addition to maintaining low-level details and textures under mixed lighting conditions. The proposed methodology computes the loss at multiple scales, balancing reconstruction accuracy and perceptual quality to enhance overall image generation. The distinct blend of the loss calculation at each level of the pyramid, combined with the architecture of the Laplacian pyramid enables LapLoss to exceed contemporary contrast enhancement techniques. This framework achieves state-of-the-art results, consistently performing well across different lighting conditions in the SICE dataset. </p>
<blockquote>
<p>å›¾åƒåˆ°å›¾åƒç¿»è¯‘ï¼ˆI2ITï¼‰çš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯å¯¹æ¯”åº¦å¢å¼ºï¼Œå®ƒé€šè¿‡è°ƒæ•´åƒç´ ä¹‹é—´çš„å¼ºåº¦å·®å¼‚æ¥æé«˜è§†è§‰è´¨é‡ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰æ–¹æ³•åœ¨ä¿ç•™ç²¾ç»†çº¹ç†æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè¿™å¾€å¾€å¯¼è‡´ä½çº§ç‰¹å¾çš„ä¸¢å¤±ã€‚æœ¬æ–‡ä»‹ç»äº†LapLossï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹I2ITå¯¹æ¯”åº¦å¢å¼ºæå‡ºçš„æ–°å‹æ–¹æ³•ï¼Œå®ƒåŸºäºæ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”ä¸­å¿ƒç½‘ç»œï¼Œæ„æˆæˆ‘ä»¬æå‡ºçš„æ–¹æ³•è®ºçš„æ ¸å¿ƒã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤šé‡é‰´åˆ«å™¨æ¶æ„ï¼Œæ¯ä¸ªé‰´åˆ«å™¨åœ¨ä¸åŒçš„åˆ†è¾¨ç‡ä¸‹è¿è¡Œï¼Œä»¥æ•æ‰é«˜çº§ç‰¹å¾ï¼ŒåŒæ—¶åœ¨æ··åˆç…§æ˜æ¡ä»¶ä¸‹ä¿æŒä½çº§ç»†èŠ‚å’Œçº¹ç†ã€‚è¯¥æ–¹æ³•åœ¨ä¸åŒçš„å°ºåº¦ä¸Šè®¡ç®—æŸå¤±ï¼Œå¹³è¡¡é‡å»ºç²¾åº¦å’Œæ„ŸçŸ¥è´¨é‡ï¼Œä»è€Œæé«˜æ•´ä½“å›¾åƒç”Ÿæˆè´¨é‡ã€‚é‡‘å­—å¡”æ¯ä¸€å±‚æŸå¤±è®¡ç®—çš„ç‹¬ç‰¹èåˆï¼Œç»“åˆæ‹‰æ™®æ‹‰æ–¯é‡‘å­—å¡”çš„æ¶æ„ï¼Œä½¿LapLossè¶…è¶Šäº†å½“å‰çš„å¯¹æ¯”åº¦å¢å¼ºæŠ€æœ¯ã€‚è¯¥æ¡†æ¶åœ¨SICEæ•°æ®é›†çš„ä¸åŒç…§æ˜æ¡ä»¶ä¸‹å‡å–å¾—äº†æœ€æ–°çš„ç»“æœï¼Œè¡¨ç°è‰¯å¥½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05974v1">PDF</a> Accepted at the DeLTa Workshop, ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºLaplaciané‡‘å­—å¡”çš„I2ITå¯¹æ¯”å¢å¼ºæ–°æ–¹æ³•â€”â€”LapLossã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤šåˆ¤åˆ«å™¨æ¶æ„ï¼Œåœ¨ä¸åŒåˆ†è¾¨ç‡ä¸‹æ•æ‰é«˜çº§ç‰¹å¾ï¼Œå¹¶åœ¨æ··åˆå…‰ç…§æ¡ä»¶ä¸‹ä¿æŒä½çº§ç»†èŠ‚å’Œçº¹ç†ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªå°ºåº¦ä¸Šè®¡ç®—æŸå¤±ï¼Œå¹³è¡¡é‡å»ºç²¾åº¦å’Œæ„ŸçŸ¥è´¨é‡ï¼Œæé«˜å›¾åƒç”Ÿæˆçš„æ€»ä½“è´¨é‡ï¼Œè¶…è¶Šç°æœ‰å¯¹æ¯”å¢å¼ºæŠ€æœ¯ï¼Œåœ¨SICEæ•°æ®é›†çš„ä¸åŒå…‰ç…§æ¡ä»¶ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LapLossæ˜¯ä¸€ç§åŸºäºLaplaciané‡‘å­—å¡”çš„æ–°æ–¹æ³•ï¼Œç”¨äºå›¾åƒåˆ°å›¾åƒçš„å¯¹æ¯”å¢å¼ºï¼ˆI2ITï¼‰ã€‚</li>
<li>é‡‡ç”¨å¤šåˆ¤åˆ«å™¨æ¶æ„ï¼Œåœ¨æ··åˆå…‰ç…§æ¡ä»¶ä¸‹æ•æ‰é«˜çº§ç‰¹å¾å’Œä¿æŒä½çº§ç»†èŠ‚å’Œçº¹ç†ã€‚</li>
<li>æ–¹æ³•åœ¨ä¸åŒå°ºåº¦ä¸Šè®¡ç®—æŸå¤±ï¼Œå¹³è¡¡é‡å»ºç²¾åº¦å’Œæ„ŸçŸ¥è´¨é‡ã€‚</li>
<li>LapLossèƒ½æé«˜å›¾åƒç”Ÿæˆçš„æ€»ä½“è´¨é‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨SICEæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œèƒ½é€‚åº”ä¸åŒçš„å…‰ç…§æ¡ä»¶ã€‚</li>
<li>ä¸ç°æœ‰å¯¹æ¯”å¢å¼ºæŠ€æœ¯ç›¸æ¯”ï¼ŒLapLossæœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05974">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fa5b9ff4ab8f788e3be33613224a628a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8a12852e98897728a890d4af0fe3842.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d722390bfcca1a5813be546479bd069.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8edb8b9bce80d151ffcb595755c2fbd9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Intermediate-Domain-guided-Adaptation-for-Unsupervised-Chorioallantoic-Membrane-Vessel-Segmentation"><a href="#Intermediate-Domain-guided-Adaptation-for-Unsupervised-Chorioallantoic-Membrane-Vessel-Segmentation" class="headerlink" title="Intermediate Domain-guided Adaptation for Unsupervised Chorioallantoic   Membrane Vessel Segmentation"></a>Intermediate Domain-guided Adaptation for Unsupervised Chorioallantoic   Membrane Vessel Segmentation</h2><p><strong>Authors:Pengwu Song, Liang Xu, Peng Yao, Shuwei Shen, Pengfei Shao, Mingzhai Sun, Ronald X. Xu</strong></p>
<p>The chorioallantoic membrane (CAM) model is widely employed in angiogenesis research, and distribution of growing blood vessels is the key evaluation indicator. As a result, vessel segmentation is crucial for quantitative assessment based on topology and morphology. However, manual segmentation is extremely time-consuming, labor-intensive, and prone to inconsistency due to its subjective nature. Moreover, research on CAM vessel segmentation algorithms remains limited, and the lack of public datasets contributes to poor prediction performance. To address these challenges, we propose an innovative Intermediate Domain-guided Adaptation (IDA) method, which utilizes the similarity between CAM images and retinal images, along with existing public retinal datasets, to perform unsupervised training on CAM images. Specifically, we introduce a Multi-Resolution Asymmetric Translation (MRAT) strategy to generate intermediate images to promote image-level interaction. Then, an Intermediate Domain-guided Contrastive Learning (IDCL) module is developed to disentangle cross-domain feature representations. This method overcomes the limitations of existing unsupervised domain adaptation (UDA) approaches, which primarily concentrate on directly source-target alignment while neglecting intermediate domain information. Notably, we create the first CAM dataset to validate the proposed algorithm. Extensive experiments on this dataset show that our method outperforms compared approaches. Moreover, it achieves superior performance in UDA tasks across retinal datasets, highlighting its strong generalization capability. The CAM dataset and source codes are available at <a target="_blank" rel="noopener" href="https://github.com/Light-47/IDA">https://github.com/Light-47/IDA</a>. </p>
<blockquote>
<p>ç»’æ¯›è†œå°¿è†œï¼ˆCAMï¼‰æ¨¡å‹åœ¨è¡€ç®¡ç”Ÿæˆç ”ç©¶ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œç”Ÿé•¿è¡€ç®¡çš„åˆ†å¸ƒæ˜¯ä¸»è¦è¯„ä»·æŒ‡æ ‡ã€‚å› æ­¤ï¼ŒåŸºäºæ‹“æ‰‘å’Œå½¢æ€çš„å®šé‡è¯„ä¼°ï¼Œè¡€ç®¡åˆ†å‰²è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨åˆ†å‰²è€—æ—¶è´¹åŠ›ï¼Œä¸”ç”±äºä¸»è§‚æ€§å®¹æ˜“å­˜åœ¨ä¸ä¸€è‡´ã€‚æ­¤å¤–ï¼ŒCAMè¡€ç®¡åˆ†å‰²ç®—æ³•çš„ç ”ç©¶ä»ç„¶æœ‰é™ï¼Œç¼ºä¹å…¬å…±æ•°æ®é›†å¯¼è‡´é¢„æµ‹æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„ä¸­åŸŸå¼•å¯¼é€‚åº”ï¼ˆIDAï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨CAMå›¾åƒå’Œè§†ç½‘è†œå›¾åƒçš„ç›¸ä¼¼æ€§ï¼Œä»¥åŠç°æœ‰çš„å…¬å…±è§†ç½‘è†œæ•°æ®é›†ï¼Œå¯¹CAMå›¾åƒè¿›è¡Œæ— ç›‘ç£è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šåˆ†è¾¨ç‡ä¸å¯¹ç§°ç¿»è¯‘ï¼ˆMRATï¼‰ç­–ç•¥æ¥ç”Ÿæˆä¸­é—´å›¾åƒï¼Œä»¥ä¿ƒè¿›å›¾åƒçº§åˆ«çš„äº¤äº’ã€‚ç„¶åï¼Œå¼€å‘äº†ä¸€ä¸ªä¸­é—´åŸŸå¼•å¯¼å¯¹æ¯”å­¦ä¹ ï¼ˆIDCLï¼‰æ¨¡å—æ¥è§£å¼€è·¨åŸŸç‰¹å¾è¡¨ç¤ºã€‚è¯¥æ–¹æ³•å…‹æœäº†ç°æœ‰æ— ç›‘ç£åŸŸé€‚åº”ï¼ˆUDAï¼‰æ–¹æ³•çš„å±€é™æ€§ï¼Œè¿™äº›æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç›´æ¥æºç›®æ ‡å¯¹é½ä¸Šï¼Œè€Œå¿½ç•¥äº†ä¸­é—´åŸŸä¿¡æ¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ç¬¬ä¸€ä¸ªCAMæ•°æ®é›†æ¥éªŒè¯æ‰€æå‡ºçš„ç®—æ³•ã€‚åœ¨è¯¥æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæ¯”è¾ƒæ–¹æ³•ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨è§†ç½‘è†œæ•°æ®é›†ä¸Šçš„UDAä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œå‡¸æ˜¾äº†å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚CAMæ•°æ®é›†å’Œæºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Light-47/IDA">https://github.com/Light-47/IDA</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.03546v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>åˆ©ç”¨é¸¡èƒšç»’æ¯›è†œå°¿å›Šè†œï¼ˆCAMï¼‰æ¨¡å‹è¿›è¡Œè¡€ç®¡ç”Ÿæˆçš„ç ”ç©¶ä¸­ï¼Œè¡€ç®¡åˆ†å‰²æ˜¯è¯„ä¼°æ‹“æ‰‘å’Œå½¢æ€çš„é‡è¦å…³é”®ç¯èŠ‚ã€‚é’ˆå¯¹CAMè¡€ç®¡åˆ†å‰²ç®—æ³•ç ”ç©¶æœ‰é™åŠç¼ºä¹å…¬å¼€æ•°æ®é›†çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºä¸­é—´åŸŸå¼•å¯¼é€‚é…ï¼ˆIDAï¼‰çš„åˆ›æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨CAMå›¾åƒä¸è§†ç½‘è†œå›¾åƒçš„ç›¸ä¼¼æ€§ï¼Œç»“åˆç°æœ‰çš„å…¬å¼€è§†ç½‘è†œæ•°æ®é›†ï¼Œå¯¹CAMå›¾åƒè¿›è¡Œæ— ç›‘ç£è®­ç»ƒã€‚é€šè¿‡å¼•å…¥å¤šåˆ†è¾¨ç‡ä¸å¯¹ç§°ç¿»è¯‘ï¼ˆMRATï¼‰ç­–ç•¥ç”Ÿæˆä¸­é—´å›¾åƒæ¥ä¿ƒè¿›å›¾åƒçº§äº¤äº’ï¼Œå¹¶å¼€å‘äº†ä¸­é—´åŸŸå¼•å¯¼å¯¹æ¯”å­¦ä¹ ï¼ˆIDCLï¼‰æ¨¡å—æ¥è§£æè·¨åŸŸç‰¹å¾è¡¨ç¤ºã€‚è¯¥æ–¹æ³•åœ¨å…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œå…¶åœ¨CAMæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œä¸”åœ¨è§†ç½‘è†œæ•°æ®é›†ä¸Šçš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ä»»åŠ¡ä¸­å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>CAMæ¨¡å‹åœ¨è¡€ç®¡ç”Ÿæˆç ”ç©¶ä¸­å¹¿æ³›åº”ç”¨ï¼Œè¡€ç®¡åˆ†å¸ƒæ˜¯é‡è¦è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>æ‰‹åŠ¨è¡€ç®¡åˆ†å‰²è€—æ—¶ã€åŠ³åŠ›å¯†é›†ä¸”ä¸»è§‚æ€§å¯¼è‡´ç»“æœä¸ä¸€è‡´ã€‚</li>
<li>ç›®å‰CAMè¡€ç®¡åˆ†å‰²ç®—æ³•ç ”ç©¶æœ‰é™ï¼Œç¼ºä¹å…¬å¼€æ•°æ®é›†å½±å“é¢„æµ‹æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æå‡ºåŸºäºä¸­é—´åŸŸå¼•å¯¼é€‚é…ï¼ˆIDAï¼‰çš„åˆ›æ–°æ–¹æ³•ï¼Œåˆ©ç”¨CAMä¸è§†ç½‘è†œå›¾åƒçš„ç›¸ä¼¼æ€§è¿›è¡Œæ— ç›‘ç£è®­ç»ƒã€‚</li>
<li>å¼•å…¥å¤šåˆ†è¾¨ç‡ä¸å¯¹ç§°ç¿»è¯‘ï¼ˆMRATï¼‰ç­–ç•¥ç”Ÿæˆä¸­é—´å›¾åƒï¼Œä¿ƒè¿›å›¾åƒçº§äº¤äº’ã€‚</li>
<li>å¼€å‘ä¸­é—´åŸŸå¼•å¯¼å¯¹æ¯”å­¦ä¹ ï¼ˆIDCLï¼‰æ¨¡å—ï¼Œä»¥è§£æè·¨åŸŸç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>åœ¨CAMæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä¼˜äºå…¶ä»–å¯¹æ¯”æ–¹æ³•ï¼Œä¸”åœ¨è§†ç½‘è†œæ•°æ®é›†ä¸Šå…·æœ‰è¾ƒå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.03546">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-778ccb1eeaad89aac59c87c9493967ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08dec57e370691d2a4eff5179ca9ecf8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b650e308e82f3e7a9bb9dc2a86def0a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Gotta-Hear-Them-All-Sound-Source-Aware-Vision-to-Audio-Generation"><a href="#Gotta-Hear-Them-All-Sound-Source-Aware-Vision-to-Audio-Generation" class="headerlink" title="Gotta Hear Them All: Sound Source Aware Vision to Audio Generation"></a>Gotta Hear Them All: Sound Source Aware Vision to Audio Generation</h2><p><strong>Authors:Wei Guo, Heng Wang, Jianbo Ma, Weidong Cai</strong></p>
<p>Vision-to-audio (V2A) synthesis has broad applications in multimedia. Recent advancements of V2A methods have made it possible to generate relevant audios from inputs of videos or still images. However, the immersiveness and expressiveness of the generation are limited. One possible problem is that existing methods solely rely on the global scene and overlook details of local sounding objects (i.e., sound sources). To address this issue, we propose a Sound Source-Aware V2A (SSV2A) generator. SSV2A is able to locally perceive multimodal sound sources from a scene with visual detection and cross-modality translation. It then contrastively learns a Cross-Modal Sound Source (CMSS) Manifold to semantically disambiguate each source. Finally, we attentively mix their CMSS semantics into a rich audio representation, from which a pretrained audio generator outputs the sound. To model the CMSS manifold, we curate a novel single-sound-source visual-audio dataset VGGS3 from VGGSound. We also design a Sound Source Matching Score to measure localized audio relevance. By addressing V2A generation at the sound-source level, SSV2A surpasses state-of-the-art methods in both generation fidelity and relevance as evidenced by extensive experiments. We further demonstrate SSV2Aâ€™s ability to achieve intuitive V2A control by compositing vision, text, and audio conditions. Our generation can be tried and heard at <a target="_blank" rel="noopener" href="https://ssv2a.github.io/SSV2A-demo">https://ssv2a.github.io/SSV2A-demo</a> . </p>
<blockquote>
<p>è§†è§‰åˆ°éŸ³é¢‘ï¼ˆV2Aï¼‰åˆæˆåœ¨å¤šåª’ä½“é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚æœ€è¿‘V2Aæ–¹æ³•çš„è¿›å±•ä½¿å¾—ä»è§†é¢‘æˆ–é™æ€å›¾åƒç”Ÿæˆç›¸å…³çš„éŸ³é¢‘æˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œç”Ÿæˆçš„æ²‰æµ¸æ„Ÿå’Œè¡¨è¾¾åŠ›æœ‰é™ã€‚ä¸€ä¸ªé—®é¢˜åœ¨äºç°æœ‰æ–¹æ³•ä»…ä¾èµ–äºå…¨å±€åœºæ™¯ï¼Œè€Œå¿½ç•¥äº†å±€éƒ¨å‘å£°ç‰©ä½“ï¼ˆå³å£°æºï¼‰çš„ç»†èŠ‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Sound Source-Aware V2Aï¼ˆSSV2Aï¼‰ç”Ÿæˆå™¨ã€‚SSV2Aèƒ½å¤Ÿä»åœºæ™¯ä¸­çš„å±€éƒ¨æ„ŸçŸ¥å¤šæ¨¡æ€å£°æºï¼Œé€šè¿‡è§†è§‰æ£€æµ‹å’Œè·¨æ¨¡æ€ç¿»è¯‘æ¥å®ç°ã€‚ç„¶åï¼Œå®ƒå¯¹æ¯”å­¦ä¹ è·¨æ¨¡æ€å£°æºï¼ˆCMSSï¼‰æµå½¢ï¼Œä»¥è¯­ä¹‰ä¸ŠåŒºåˆ†æ¯ä¸ªå£°æºã€‚æœ€åï¼Œæˆ‘ä»¬ä¸“æ³¨äºå°†å…¶CMSSè¯­ä¹‰èå…¥ä¸°å¯Œçš„éŸ³é¢‘è¡¨ç¤ºä¸­ï¼Œé¢„è®­ç»ƒçš„éŸ³é¢‘ç”Ÿæˆå™¨ä»ä¸­è¾“å‡ºå£°éŸ³ã€‚ä¸ºäº†å»ºç«‹CMSSæµå½¢ï¼Œæˆ‘ä»¬ä»VGGSoundä¸­æ•´ç†äº†ä¸€ä¸ªæ–°é¢–çš„å•å£°æºè§†è§‰éŸ³é¢‘æ•°æ®é›†VGGS3ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªå£°æºåŒ¹é…åˆ†æ•°æ¥è¡¡é‡å±€éƒ¨éŸ³é¢‘ç›¸å…³æ€§ã€‚é€šè¿‡åœ¨å£°æºçº§åˆ«è§£å†³V2Aç”Ÿæˆé—®é¢˜ï¼ŒSSV2Aåœ¨ç”Ÿæˆä¿çœŸåº¦å’Œç›¸å…³æ€§æ–¹é¢è¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå¦‚å¹¿æ³›å®éªŒæ‰€ç¤ºã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†SSV2Aå®ç°ç›´è§‚V2Aæ§åˆ¶çš„èƒ½åŠ›ï¼Œé€šè¿‡åˆæˆè§†è§‰ã€æ–‡æœ¬å’ŒéŸ³é¢‘æ¡ä»¶ã€‚æˆ‘ä»¬çš„ç”Ÿæˆä½œå“å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://ssv2a.github.io/SSV2A-demo%E4%B8%8A%E5%AF%BC%E7%94%A8%E5%92%8C%E5%90%B9%E5%94%B1%E3%80%82">https://ssv2a.github.io/SSV2A-demoä¸Šè¯•ç”¨å’Œå¬å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.15447v3">PDF</a> 18 pages, 13 figures, source code available at   <a target="_blank" rel="noopener" href="https://github.com/wguo86/SSV2A">https://github.com/wguo86/SSV2A</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†è§†è§‰åˆ°éŸ³é¢‘ï¼ˆV2Aï¼‰åˆæˆåœ¨å¤šåª’ä½“é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ã€‚ç°æœ‰çš„V2Aæ–¹æ³•å­˜åœ¨æ²‰æµ¸æ„Ÿå’Œè¡¨ç°åŠ›æœ‰é™çš„å±€é™æ€§ï¼Œå› ä¸ºå®ƒä»¬ä»…ä¾èµ–äºå…¨å±€åœºæ™¯è€Œå¿½è§†å±€éƒ¨å£°æºã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å£°éŸ³æºæ„ŸçŸ¥çš„V2Aï¼ˆSSV2Aï¼‰ç”Ÿæˆå™¨ã€‚SSV2Aèƒ½å¤Ÿä»åœºæ™¯ä¸­å±€éƒ¨æ„ŸçŸ¥å¤šæ¨¡æ€å£°æºï¼Œé€šè¿‡è§†è§‰æ£€æµ‹å’Œè·¨æ¨¡æ€ç¿»è¯‘å¯¹æ¯”å­¦ä¹ è·¨æ¨¡æ€å£°æºï¼ˆCMSSï¼‰æµå½¢ä»¥è¯­ä¹‰åœ°è§£ææ¯ä¸ªå£°æºã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†æ–°é¢–çš„å•ä¸€å£°æºè§†è§‰éŸ³é¢‘æ•°æ®é›†VGGS3æ¥è¯„ä¼°å…¶æ€§èƒ½ï¼Œè®¾è®¡äº†å£°æºåŒ¹é…å¾—åˆ†æ¥è¡¡é‡å±€éƒ¨éŸ³é¢‘çš„ç›¸å…³æ€§ã€‚åœ¨å£°éŸ³æºçº§åˆ«çš„V2Aç”Ÿæˆä¸­ï¼ŒSSV2Aåœ¨ç”Ÿæˆä¿çœŸåº¦å’Œç›¸å…³æ€§æ–¹é¢å‡è¶…è¶Šç°æœ‰æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†ç›´è§‚çš„å¯æ§æ€§ã€‚å¯ä»¥é€šè¿‡ç›¸å…³é“¾æ¥ä½“éªŒå…¶æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>V2Aåˆæˆå…·æœ‰å¹¿æ³›çš„åº”ç”¨ä»·å€¼ï¼Œèƒ½å¤Ÿä»è§†é¢‘æˆ–é™æ€å›¾åƒç”Ÿæˆç›¸å…³éŸ³é¢‘ã€‚</li>
<li>ç°æœ‰V2Aæ–¹æ³•å­˜åœ¨æ²‰æµ¸æ„Ÿå’Œè¡¨ç°åŠ›æœ‰é™çš„é—®é¢˜ï¼Œä¸»è¦åŸå› æ˜¯è¿‡äºä¾èµ–å…¨å±€åœºæ™¯è€Œå¿½è§†å±€éƒ¨å£°æºã€‚</li>
<li>æå‡ºäº†ä¸€ç§å£°éŸ³æºæ„ŸçŸ¥çš„V2Aï¼ˆSSV2Aï¼‰ç”Ÿæˆå™¨ï¼Œèƒ½å¤Ÿå±€éƒ¨æ„ŸçŸ¥å¤šæ¨¡æ€å£°æºã€‚</li>
<li>SSV2Aé€šè¿‡å¯¹æ¯”å­¦ä¹ æ„å»ºè·¨æ¨¡æ€å£°æºï¼ˆCMSSï¼‰æµå½¢ä»¥è¯­ä¹‰è§£ææ¯ä¸ªå£°æºã€‚</li>
<li>å¼•å…¥äº†æ–°é¢–çš„å•ä¸€å£°æºè§†è§‰éŸ³é¢‘æ•°æ®é›†VGGS3æ¥è¯„ä¼°SSV2Açš„æ€§èƒ½ã€‚</li>
<li>SSV2Aè®¾è®¡äº†å£°æºåŒ¹é…å¾—åˆ†æ¥è¡¡é‡å±€éƒ¨éŸ³é¢‘çš„ç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.15447">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b1e61e778d808aaa86ad9f3cbab4bac2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7583696e5e79eed607161ba61f2c3fcf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83be6ef37186c022b7bee93a696c47b1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-48e31883f2740a7874b4932d69091414.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="C-DiffSET-Leveraging-Latent-Diffusion-for-SAR-to-EO-Image-Translation-with-Confidence-Guided-Reliable-Object-Generation"><a href="#C-DiffSET-Leveraging-Latent-Diffusion-for-SAR-to-EO-Image-Translation-with-Confidence-Guided-Reliable-Object-Generation" class="headerlink" title="C-DiffSET: Leveraging Latent Diffusion for SAR-to-EO Image Translation   with Confidence-Guided Reliable Object Generation"></a>C-DiffSET: Leveraging Latent Diffusion for SAR-to-EO Image Translation   with Confidence-Guided Reliable Object Generation</h2><p><strong>Authors:Jeonghyeok Do, Jaehyup Lee, Munchurl Kim</strong></p>
<p>Synthetic Aperture Radar (SAR) imagery provides robust environmental and temporal coverage (e.g., during clouds, seasons, day-night cycles), yet its noise and unique structural patterns pose interpretation challenges, especially for non-experts. SAR-to-EO (Electro-Optical) image translation (SET) has emerged to make SAR images more perceptually interpretable. However, traditional approaches trained from scratch on limited SAR-EO datasets are prone to overfitting. To address these challenges, we introduce Confidence Diffusion for SAR-to-EO Translation, called C-DiffSET, a framework leveraging pretrained Latent Diffusion Model (LDM) extensively trained on natural images, thus enabling effective adaptation to the EO domain. Remarkably, we find that the pretrained VAE encoder aligns SAR and EO images in the same latent space, even with varying noise levels in SAR inputs. To further improve pixel-wise fidelity for SET, we propose a confidence-guided diffusion (C-Diff) loss that mitigates artifacts from temporal discrepancies, such as appearing or disappearing objects, thereby enhancing structural accuracy. C-DiffSET achieves state-of-the-art (SOTA) results on multiple datasets, significantly outperforming the very recent image-to-image translation methods and SET methods with large margins. </p>
<blockquote>
<p>é›·è¾¾åˆæˆå­”å¾„å›¾åƒæä¾›äº†ç¨³å¥çš„ç¯å¢ƒå’Œæ—¶é—´è¦†ç›–ï¼ˆä¾‹å¦‚åœ¨äº‘å±‚ã€å­£èŠ‚ã€æ˜¼å¤œå¾ªç¯ä¸­ï¼‰ï¼Œä½†å…¶å™ªå£°å’Œç‹¬ç‰¹çš„ç»“æ„æ¨¡å¼å¯¹è§£é‡Šæå‡ºäº†æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å¯¹éä¸“ä¸šäººå£«æ¥è¯´ã€‚SARåˆ°EOï¼ˆå…‰ç”µï¼‰å›¾åƒç¿»è¯‘ï¼ˆSETï¼‰çš„å‡ºç°ä½¿å¾—SARå›¾åƒæ›´å®¹æ˜“æ„ŸçŸ¥å’Œç†è§£ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„SETæ–¹æ³•ä»é›¶å¼€å§‹ä½¿ç”¨æœ‰é™çš„SAR-EOæ•°æ®é›†è¿›è¡Œè®­ç»ƒå®¹æ˜“è¿‡åº¦æ‹Ÿåˆã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†SARåˆ°EOç¿»è¯‘çš„ç½®ä¿¡åº¦æ‰©æ•£ï¼Œç§°ä¸ºC-DiffSETã€‚è¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¨¡å‹åœ¨è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œäº†å¹¿æ³›çš„è®­ç»ƒï¼Œä»è€Œèƒ½å¤Ÿæœ‰æ•ˆåœ°é€‚åº”EOé¢†åŸŸã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°é¢„è®­ç»ƒçš„VAEç¼–ç å™¨èƒ½å¤Ÿåœ¨åŒä¸€æ½œåœ¨ç©ºé—´ä¸­å¯¹é½SARå’ŒEOå›¾åƒï¼Œå³ä½¿SARè¾“å…¥ä¸­å­˜åœ¨ä¸åŒçº§åˆ«çš„å™ªå£°ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜SETçš„åƒç´ çº§ä¿çœŸåº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç½®ä¿¡åº¦å¼•å¯¼æ‰©æ•£ï¼ˆC-Diffï¼‰æŸå¤±ï¼Œè¯¥æŸå¤±å‡è½»äº†ç”±äºæ—¶é—´å·®å¼‚å¯¼è‡´çš„ä¼ªå½±ï¼Œå¦‚å‡ºç°çš„å¯¹è±¡æˆ–æ¶ˆå¤±çš„å¯¹è±¡ï¼Œä»è€Œæé«˜äº†ç»“æ„å‡†ç¡®æ€§ã€‚C-DiffSETåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°çš„ç»“æœï¼Œå¤§å¤§è¶…è¶Šäº†æœ€æ–°çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ–¹æ³•å’ŒSETæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10788v3">PDF</a> Please visit our project page   <a target="_blank" rel="noopener" href="https://kaist-viclab.github.io/C-DiffSET_site/">https://kaist-viclab.github.io/C-DiffSET_site/</a></p>
<p><strong>Summary</strong></p>
<p>SARå½±åƒå…·æœ‰ç¨³å¥çš„ç¯å¢ƒå’Œæ—¶åºè¦†ç›–èƒ½åŠ›ï¼Œä½†å…¶å™ªå£°å’Œç‹¬ç‰¹ç»“æ„æ¨¡å¼å¯¹éä¸“å®¶æ¥è¯´è§£è¯»å…·æœ‰æŒ‘æˆ˜ã€‚SAR-to-EOå›¾åƒç¿»è¯‘ï¼ˆSETï¼‰æŠ€æœ¯çš„å‡ºç°ä½¿SARå›¾åƒæ›´æ˜“æ„ŸçŸ¥å’Œç†è§£ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•åœ¨å°è§„æ¨¡SAR-EOæ•°æ®é›†ä¸Šè¿›è¡Œä»å¤´è®­ç»ƒå®¹æ˜“è¿‡æ‹Ÿåˆã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºé¢„è®­ç»ƒæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„ä¿¡å¿ƒæ‰©æ•£æŠ€æœ¯ï¼ˆC-DiffSETï¼‰ã€‚è¯¥æŠ€æœ¯å……åˆ†åˆ©ç”¨äº†åœ¨è‡ªç„¶å›¾åƒä¸Šå¹¿æ³›è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ï¼Œå¯æœ‰æ•ˆé€‚åº”EOé¢†åŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°é¢„è®­ç»ƒçš„VAEç¼–ç å™¨èƒ½åœ¨ä¸åŒå™ªå£°æ°´å¹³çš„SARè¾“å…¥ä¸‹ï¼Œå°†SARå’ŒEOå›¾åƒå¯¹é½åœ¨åŒä¸€æ½œåœ¨ç©ºé—´ã€‚ä¸ºè¿›ä¸€æ­¥æé«˜SETçš„åƒç´ çº§ä¿çœŸåº¦ï¼Œæˆ‘ä»¬æå‡ºäº†ä¿¡å¿ƒå¼•å¯¼æ‰©æ•£ï¼ˆC-Diffï¼‰æŸå¤±ï¼Œå‡å°‘å› æ—¶é—´å·®å¼‚é€ æˆçš„ä¼ªå½±ï¼Œå¦‚ç‰©ä½“çš„å‡ºç°æˆ–æ¶ˆå¤±ï¼Œä»è€Œæé«˜ç»“æ„å‡†ç¡®æ€§ã€‚C-DiffSETåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°æœ€ä½³æ•ˆæœï¼Œå¤§å¹…è¶…è¶Šæœ€æ–°å›¾åƒåˆ°å›¾åƒç¿»è¯‘æ–¹æ³•å’ŒSETæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SARå½±åƒæä¾›ç¨³å¥çš„ç¯å¢ƒå’Œæ—¶åºè¦†ç›–èƒ½åŠ›ï¼Œä½†å¯¹éä¸“å®¶è§£è¯»æœ‰æŒ‘æˆ˜ã€‚</li>
<li>SETæŠ€æœ¯æ—¨åœ¨ä½¿SARå›¾åƒæ›´æ˜“æ„ŸçŸ¥å’Œç†è§£ã€‚</li>
<li>ä¼ ç»ŸSETæ–¹æ³•æ˜“åœ¨æœ‰é™æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆã€‚</li>
<li>C-DiffSETåˆ©ç”¨é¢„è®­ç»ƒçš„æ½œåœ¨æ‰©æ•£æ¨¡å‹è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ‰æ•ˆé€‚åº”EOé¢†åŸŸã€‚</li>
<li>é¢„è®­ç»ƒçš„VAEç¼–ç å™¨åœ¨ä¸åŒå™ªå£°æ°´å¹³çš„SARè¾“å…¥ä¸‹å¯¹é½SARå’ŒEOå›¾åƒã€‚</li>
<li>C-DiffæŸå¤±å‡½æ•°å‡å°‘æ—¶é—´å·®å¼‚é€ æˆçš„ä¼ªå½±ï¼Œæé«˜ç»“æ„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10788">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9ca8bf84afc01767354a1898fbde7a11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91f427c8b8d482ea9499b0c2fb55e123.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9127f5a6f9b3d619618060abd6d02c6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51678b149dbba80ed6cd688e2dc7e421.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Online-4D-Ultrasound-Guided-Robotic-Tracking-Enables-3D-Ultrasound-Localisation-Microscopy-with-Large-Tissue-Displacements"><a href="#Online-4D-Ultrasound-Guided-Robotic-Tracking-Enables-3D-Ultrasound-Localisation-Microscopy-with-Large-Tissue-Displacements" class="headerlink" title="Online 4D Ultrasound-Guided Robotic Tracking Enables 3D Ultrasound   Localisation Microscopy with Large Tissue Displacements"></a>Online 4D Ultrasound-Guided Robotic Tracking Enables 3D Ultrasound   Localisation Microscopy with Large Tissue Displacements</h2><p><strong>Authors:Jipeng Yan, Qingyuan Tan, Shusei Kawara, Jingwen Zhu, Bingxue Wang, Matthieu Toulemonde, Honghai Liu, Ying Tan, Meng-Xing Tang</strong></p>
<p>Super-Resolution Ultrasound (SRUS) imaging through localising and tracking microbubbles, also known as Ultrasound Localisation Microscopy (ULM), has demonstrated significant potential for reconstructing microvasculature and flows with sub-diffraction resolution in clinical diagnostics. However, imaging organs with large tissue movements, such as those caused by respiration, presents substantial challenges. Existing methods often require breath holding to maintain accumulation accuracy, which limits data acquisition time and ULM image saturation. To improve image quality in the presence of large tissue movements, this study introduces an approach integrating high-frame-rate ultrasound with online precise robotic probe control. Tested on a microvasculature phantom with translation motions up to 20 mm, twice the aperture size of the matrix array used, our method achieved real-time tracking of the moving phantom and imaging volume rate at 85 Hz, keeping majority of the target volume in the imaging field of view. ULM images of the moving cross channels in the phantom were successfully reconstructed in post-processing, demonstrating the feasibility of super-resolution imaging under large tissue motions. This represents a significant step towards ULM imaging of organs with large motion. </p>
<blockquote>
<p>é€šè¿‡å®šä½å’Œè¿½è¸ªå¾®æ³¡è¿›è¡Œè¶…çº§åˆ†è¾¨ç‡è¶…å£°ï¼ˆSRUSï¼‰æˆåƒï¼Œä¹Ÿè¢«ç§°ä¸ºè¶…å£°å®šä½æ˜¾å¾®é•œï¼ˆULMï¼‰ï¼Œåœ¨ä¸´åºŠè¯Šæ–­ä¸­ä»¥æ¬¡è¡å°„åˆ†è¾¨ç‡é‡å»ºå¾®è¡€ç®¡å’Œç»„ç»‡æµåŠ¨æ–¹é¢è¡¨ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºå…·æœ‰å¤§ç»„ç»‡è¿åŠ¨çš„å™¨å®˜çš„æˆåƒï¼Œä¾‹å¦‚ç”±å‘¼å¸å¼•èµ·çš„è¿åŠ¨ï¼Œå­˜åœ¨ç›¸å½“å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦å±ä½å‘¼å¸ä»¥ä¿æŒç§¯ç´¯ç²¾åº¦ï¼Œè¿™é™åˆ¶äº†æ•°æ®é‡‡é›†æ—¶é—´å’ŒULMå›¾åƒçš„é¥±å’Œåº¦ã€‚ä¸ºäº†æé«˜å¤§ç»„ç»‡è¿åŠ¨ä¸‹çš„å›¾åƒè´¨é‡ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§å°†é«˜å¸§ç‡è¶…å£°ä¸åœ¨çº¿ç²¾å¯†æœºæ¢°æ¢é’ˆæ§åˆ¶ç›¸ç»“åˆçš„æ–¹æ³•ã€‚åœ¨å…·æœ‰æœ€å¤§è¾¾20æ¯«ç±³å¹³ç§»è¿åŠ¨çš„å¾®è¡€ç®¡æ¨¡å‹ä¸Šè¿›è¡Œçš„æµ‹è¯•æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å¯¹ç§»åŠ¨æ¨¡å‹çš„å®æ—¶è·Ÿè¸ªå’Œæ¯ç§’æˆåƒä½“ç§¯ç‡ä¸º85èµ«å…¹ï¼Œå°†å¤§éƒ¨åˆ†ç›®æ ‡ä½“ç§¯ä¿æŒåœ¨æˆåƒè§†é‡ä¸­ã€‚ç»è¿‡åæœŸå¤„ç†æˆåŠŸé‡å»ºäº†ç§»åŠ¨äº¤å‰é€šé“æ¨¡å‹ä¸‹çš„ULMå›¾åƒï¼Œè¯æ˜äº†åœ¨å¤§ç»„ç»‡è¿åŠ¨ä¸‹è¿›è¡Œè¶…çº§åˆ†è¾¨ç‡æˆåƒçš„å¯è¡Œæ€§ã€‚è¿™æ˜¯å‘å¯¹å¤§è¿åŠ¨å™¨å®˜è¿›è¡ŒULMæˆåƒè¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.11391v2">PDF</a> </p>
<p><strong>Summary</strong><br>SRUSæˆåƒé€šè¿‡å®šä½è¿½è¸ªå¾®æ³¡æŠ€æœ¯å±•ç¤ºäº†é‡å»ºå¾®è¡€ç®¡åŠè¡€æµåœ¨å­è¡å°„åˆ†è¾¨ç‡ä¸‹çš„æ½œåŠ›ã€‚é’ˆå¯¹å‘¼å¸ç­‰å¼•èµ·çš„å¤§å™¨å®˜è¿åŠ¨é—®é¢˜ï¼Œè¯¥ç ”ç©¶å°†é«˜é€Ÿç‡è¶…å£°æ³¢ä¸åœ¨çº¿ç²¾å‡†æœºæ¢°æ¢é’ˆæ§åˆ¶ç»“åˆï¼Œæœ‰æ•ˆè¿½è¸ªç§»åŠ¨è¡€ç®¡å¹¶é‡å»ºå›¾åƒã€‚è¯¥æˆæœæœ‰åŠ©äºè§£å†³å™¨å®˜å¤§è¿åŠ¨ä¸‹çš„è¶…åˆ†è¾¨ç‡æˆåƒéš¾é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SRUSæˆåƒé€šè¿‡å®šä½è¿½è¸ªå¾®æ³¡æŠ€æœ¯èƒ½å¤Ÿé‡å»ºå¾®è¡€ç®¡å’Œè¡€æµã€‚</li>
<li>å¤§å™¨å®˜è¿åŠ¨ç»™æˆåƒå¸¦æ¥æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•éœ€è¦æ‚£è€…å±æ°”ã€‚</li>
<li>ç ”ç©¶ç»“åˆäº†é«˜é€Ÿç‡è¶…å£°æ³¢ä¸åœ¨çº¿ç²¾å‡†æœºæ¢°æ¢é’ˆæ§åˆ¶ä»¥æ”¹å–„å›¾åƒè´¨é‡ã€‚</li>
<li>åœ¨æ¨¡æ‹Ÿè¡€ç®¡å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•å®ç°äº†å®æ—¶è¿½è¸ªç§»åŠ¨ç›®æ ‡å¹¶æˆåŠŸé‡å»ºå›¾åƒã€‚</li>
<li>è¯¥æŠ€æœ¯æé«˜äº†è¶…åˆ†è¾¨ç‡æˆåƒåœ¨å™¨å®˜å¤§è¿åŠ¨æƒ…å†µä¸‹çš„å¯è¡Œæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.11391">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e592498a40308b04ef1d312ff34456b6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fb7e286a2006f4efcd1c94be7525bde2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4afe89cecaa57e9b26794d5aba299201.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ef263382987a3050ca07b0aff2682f40.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  ALLVB All-in-One Long Video Understanding Benchmark
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-7b883c06e632db846504de44b4ac49cf.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  Brain Inspired Adaptive Memory Dual-Net for Few-Shot Image   Classification
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27083.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
