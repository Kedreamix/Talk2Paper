<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  Mitigating Hallucinations in YOLO-based Object Detection Models A   Revisit to Out-of-Distribution Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-39a27e50e75216213ac48fafa49c5162.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-12
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    40 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-03-12-æ›´æ–°"><a href="#2025-03-12-æ›´æ–°" class="headerlink" title="2025-03-12 æ›´æ–°"></a>2025-03-12 æ›´æ–°</h1><h2 id="Mitigating-Hallucinations-in-YOLO-based-Object-Detection-Models-A-Revisit-to-Out-of-Distribution-Detection"><a href="#Mitigating-Hallucinations-in-YOLO-based-Object-Detection-Models-A-Revisit-to-Out-of-Distribution-Detection" class="headerlink" title="Mitigating Hallucinations in YOLO-based Object Detection Models: A   Revisit to Out-of-Distribution Detection"></a>Mitigating Hallucinations in YOLO-based Object Detection Models: A   Revisit to Out-of-Distribution Detection</h2><p><strong>Authors:Weicheng He, Changshun Wu, Chih-Hong Cheng, Xiaowei Huang, Saddek Bensalem</strong></p>
<p>Object detection systems must reliably perceive objects of interest without being overly confident to ensure safe decision-making in dynamic environments. Filtering techniques based on out-of-distribution (OoD) detection are commonly added as an extra safeguard to filter hallucinations caused by overconfidence in novel objects. Nevertheless, evaluating YOLO-family detectors and their filters under existing OoD benchmarks often leads to unsatisfactory performance. This paper studies the underlying reasons for performance bottlenecks and proposes a methodology to improve performance fundamentally. Our first contribution is a calibration of all existing evaluation results: Although images in existing OoD benchmark datasets are claimed not to have objects within in-distribution (ID) classes (i.e., categories defined in the training dataset), around 13% of objects detected by the object detector are actually ID objects. Dually, the ID dataset containing OoD objects can also negatively impact the decision boundary of filters. These ultimately lead to a significantly imprecise performance estimation. Our second contribution is to consider the task of hallucination reduction as a joint pipeline of detectors and filters. By developing a methodology to carefully synthesize an OoD dataset that semantically resembles the objects to be detected, and using the crafted OoD dataset in the fine-tuning of YOLO detectors to suppress the objectness score, we achieve a 88% reduction in overall hallucination error with a combined fine-tuned detection and filtering system on the self-driving benchmark BDD-100K. Our code and dataset are available at: <a target="_blank" rel="noopener" href="https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood">https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood</a>. </p>
<blockquote>
<p>å¯¹è±¡æ£€æµ‹ç³»ç»Ÿå¿…é¡»å¯é åœ°æ„ŸçŸ¥åŠ¨æ€ç¯å¢ƒä¸­çš„æ„Ÿå…´è¶£å¯¹è±¡ï¼ŒåŒæ—¶é¿å…è¿‡äºè‡ªä¿¡ï¼Œä»¥ç¡®ä¿åšå‡ºå®‰å…¨å†³ç­–ã€‚åŸºäºç¦»ç¾¤å€¼ï¼ˆOut-of-Distributionï¼ŒOoDï¼‰æ£€æµ‹çš„è¿‡æ»¤æŠ€æœ¯é€šå¸¸è¢«ç”¨ä½œé¢å¤–çš„å®‰å…¨ä¿éšœæªæ–½ï¼Œä»¥è¿‡æ»¤å› è¿‡äºè‡ªä¿¡è€Œå¯¼è‡´çš„æ–°å‹å¯¹è±¡å¹»è§‰ã€‚ç„¶è€Œï¼Œåœ¨ç°æœ‰çš„OoDåŸºå‡†æµ‹è¯•ä¸‹è¯„ä¼°YOLOç³»åˆ—æ£€æµ‹å™¨åŠå…¶è¿‡æ»¤å™¨é€šå¸¸ä¼šå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚æœ¬æ–‡ç ”ç©¶äº†æ€§èƒ½ç“¶é¢ˆçš„æ ¹æœ¬åŸå› ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä»æ ¹æœ¬ä¸Šæé«˜æ€§èƒ½çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªè´¡çŒ®æ˜¯æ ¡å‡†äº†æ‰€æœ‰ç°æœ‰çš„è¯„ä¼°ç»“æœï¼šå°½ç®¡ç°æœ‰OoDåŸºå‡†æ•°æ®é›†å£°ç§°å›¾åƒå†…æ²¡æœ‰å±äºå†…éƒ¨åˆ†å¸ƒï¼ˆIDï¼‰ç±»åˆ«ï¼ˆå³è®­ç»ƒæ•°æ®é›†ä¸­å®šä¹‰çš„ç±»åˆ«ï¼‰çš„å¯¹è±¡ï¼Œä½†å¤§çº¦æœ‰13%çš„å¯¹è±¡å®é™…ä¸Šæ˜¯ç”±å¯¹è±¡æ£€æµ‹å™¨æ£€æµ‹åˆ°çš„IDå¯¹è±¡ã€‚åŒæ ·ï¼ŒåŒ…å«OoDå¯¹è±¡çš„IDæ•°æ®é›†ä¹Ÿå¯èƒ½å¯¹è¿‡æ»¤å™¨çš„å†³ç­–è¾¹ç•Œäº§ç”Ÿè´Ÿé¢å½±å“ã€‚è¿™äº›æœ€ç»ˆå¯¼è‡´äº†æ€§èƒ½ä¼°è®¡çš„ä¸¥é‡ä¸å‡†ç¡®ã€‚æˆ‘ä»¬çš„ç¬¬äºŒä¸ªè´¡çŒ®æ˜¯å°†å¹»è§‰å‡å°‘ä»»åŠ¡è§†ä¸ºæ£€æµ‹å™¨å’Œè¿‡æ»¤å™¨çš„è”åˆç®¡é“ã€‚é€šè¿‡å¼€å‘ä¸€ç§ç²¾å¿ƒåˆæˆçš„OoDæ•°æ®é›†ï¼Œè¯¥æ–¹æ³•è¯­ä¹‰ä¸Šç±»ä¼¼äºè¦æ£€æµ‹çš„å¯¹è±¡ï¼Œå¹¶ä½¿ç”¨å®šåˆ¶åŒ–çš„OoDæ•°æ®é›†å¾®è°ƒYOLOæ£€æµ‹å™¨ä»¥æŠ‘åˆ¶å¯¹è±¡å¾—åˆ†ï¼Œæˆ‘ä»¬åœ¨è‡ªåŠ¨é©¾é©¶åŸºå‡†BDD-100Kä¸Šå®ç°äº†é€šè¿‡ç»“åˆå¾®è°ƒæ£€æµ‹å’Œè¿‡æ»¤ç³»ç»Ÿï¼Œæ•´ä½“å¹»è§‰è¯¯å·®å‡å°‘äº†88%ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood%E3%80%82">https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hoodã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07330v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ç ”ç©¶äº†ç›®æ ‡æ£€æµ‹ç³»ç»Ÿåœ¨é¢å¯¹åŠ¨æ€ç¯å¢ƒæ—¶çš„æ€§èƒ½ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯åœ¨ç°æœ‰åŸºäºå¼‚å¸¸å€¼æ£€æµ‹ï¼ˆOoDï¼‰çš„è¿‡æ»¤å™¨ä¸‹ï¼Œé’ˆå¯¹YOLOç³»åˆ—æ£€æµ‹å™¨çš„é—®é¢˜å°¤ä¸ºçªå‡ºã€‚ç°æœ‰ç ”ç©¶è¡¨æ˜å›¾åƒå¯èƒ½åŒ…å«ç›®æ ‡æ£€æµ‹å™¨å†…è®­ç»ƒæ•°æ®é›†å†…å®šä¹‰ç±»åˆ«çš„ç‰©ä½“ï¼ˆIDå¯¹è±¡ï¼‰ï¼Œä½¿å¾—æ€§èƒ½è¯„ä¼°å­˜åœ¨åå·®ã€‚å› æ­¤ï¼Œä½œè€…æå‡ºäº†åŸºäºæ•°æ®åˆæˆçš„æ ¡æ­£æ–¹æ³•ï¼Œæé«˜äº†æ€§èƒ½ä¼°è®¡çš„ç²¾ç¡®åº¦ã€‚åŒæ—¶ï¼Œä½œè€…è¿˜è€ƒè™‘äº†å°†æ£€æµ‹å™¨å’Œè¿‡æ»¤å™¨ä½œä¸ºæ•´ä½“æµç¨‹ï¼Œåœ¨åˆæˆæ•°æ®é›†ä¸Šå¾®è°ƒYOLOæ£€æµ‹å™¨ä»¥é™ä½é”™è¯¯ç‡ã€‚è¿™äº›æ”¹è¿›åœ¨BDD-100Kè‡ªåŠ¨é©¾é©¶æ•°æ®é›†ä¸Šå®ç°äº†æ˜¾è‘—æˆæœã€‚å¯¹äºç ”ç©¶å’Œå®é™…åº”ç”¨æ¥è¯´æ„ä¹‰é‡å¤§ã€‚è¯¥å·¥ä½œçš„æºä»£ç å’Œä½¿ç”¨çš„æ•°æ®é›†å‡å·²å…¬å¼€å‘å¸ƒåœ¨ç‰¹å®šç½‘ç«™ä¸Šä¾›ç”¨æˆ·ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç›®æ ‡æ£€æµ‹ç³»ç»Ÿéœ€è¦å¹³è¡¡æ„ŸçŸ¥èƒ½åŠ›å’Œè‡ªä¿¡å¿ƒï¼Œç¡®ä¿åœ¨åŠ¨æ€ç¯å¢ƒä¸­åšå‡ºå®‰å…¨å†³ç­–ã€‚å½“å‰é¢ä¸´æ€§èƒ½ç“¶é¢ˆé—®é¢˜ã€‚</li>
<li>åŸºäºå¼‚å¸¸å€¼æ£€æµ‹çš„è¿‡æ»¤æŠ€æœ¯æ—¨åœ¨è¿‡æ»¤ç”±äºè¿‡åº¦è‡ªä¿¡è€Œå¯¼è‡´è¯¯åˆ¤çš„æƒ…å†µã€‚ç„¶è€Œç°æœ‰ç ”ç©¶ä¸­å‘ç°å­˜åœ¨ä¸€å®šåå·®å¯¼è‡´è¯„ä¼°æ€§èƒ½å—åˆ°å½±å“ã€‚ç ”ç©¶å‘ç°çº¦æœ‰è¿‘å››åˆ†ä¹‹ä¸€è¢«æ£€æµ‹çš„ç‰©ä½“æ˜¯å®é™…å­˜åœ¨äºè®­ç»ƒé›†ä¸­çš„ç‰©ä½“ï¼ˆIDå¯¹è±¡ï¼‰ã€‚è¿™å½±å“äº†æ€§èƒ½è¯„ä¼°çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07330">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0547d82e4d7d4d085202f83d90ee81ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bcc708bc2a7345ffa9d27b2dc0ce4d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e8e433bb294cb06e99c3c6de5324b5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da5155564ef4340997e7f3a71cc88318.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a96adb3084a2601e3ab4210b0598d89.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cf483be3eb815cd1d695f2851b7d7701.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-329b91335d445078ac95f2d8244c34a6.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SimROD-A-Simple-Baseline-for-Raw-Object-Detection-with-Global-and-Local-Enhancements"><a href="#SimROD-A-Simple-Baseline-for-Raw-Object-Detection-with-Global-and-Local-Enhancements" class="headerlink" title="SimROD: A Simple Baseline for Raw Object Detection with Global and Local   Enhancements"></a>SimROD: A Simple Baseline for Raw Object Detection with Global and Local   Enhancements</h2><p><strong>Authors:Haiyang Xie, Xi Shen, Shihua Huang, Zheng Wang</strong></p>
<p>Most visual models are designed for sRGB images, yet RAW data offers significant advantages for object detection by preserving sensor information before ISP processing. This enables improved detection accuracy and more efficient hardware designs by bypassing the ISP. However, RAW object detection is challenging due to limited training data, unbalanced pixel distributions, and sensor noise. To address this, we propose SimROD, a lightweight and effective approach for RAW object detection. We introduce a Global Gamma Enhancement (GGE) module, which applies a learnable global gamma transformation with only four parameters, improving feature representation while keeping the model efficient. Additionally, we leverage the green channelâ€™s richer signal to enhance local details, aligning with the human eyeâ€™s sensitivity and Bayer filter design. Extensive experiments on multiple RAW object detection datasets and detectors demonstrate that SimROD outperforms state-of-the-art methods like RAW-Adapter and DIAP while maintaining efficiency. Our work highlights the potential of RAW data for real-world object detection. </p>
<blockquote>
<p>å¤§å¤šæ•°è§†è§‰æ¨¡å‹éƒ½æ˜¯ä¸ºsRGBå›¾åƒè®¾è®¡çš„ï¼Œç„¶è€ŒRAWæ•°æ®åœ¨å¯¹è±¡æ£€æµ‹æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿä¿ç•™ISPå¤„ç†ä¹‹å‰çš„ä¼ æ„Ÿå™¨ä¿¡æ¯ã€‚è¿™å¯ä»¥é€šè¿‡ç»•è¿‡ISPå®ç°æ›´é«˜çš„æ£€æµ‹ç²¾åº¦å’Œæ›´æœ‰æ•ˆçš„ç¡¬ä»¶è®¾è®¡ã€‚ç„¶è€Œï¼ŒRAWæ•°æ®å¯¹è±¡æ£€æµ‹é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼Œå¦‚è®­ç»ƒæ•°æ®æœ‰é™ã€åƒç´ åˆ†å¸ƒä¸å¹³è¡¡å’Œä¼ æ„Ÿå™¨å™ªå£°ç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SimRODï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºRAWæ•°æ®å¯¹è±¡æ£€æµ‹çš„è½»ä¾¿æœ‰æ•ˆæ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†å…¨å±€ä¼½é©¬å¢å¼ºï¼ˆGGEï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—ä»…ä½¿ç”¨å››ä¸ªå‚æ•°è¿›è¡Œå¯å­¦ä¹ çš„å…¨å±€ä¼½é©¬å˜æ¢ï¼Œåœ¨ä¿æŒæ¨¡å‹æ•ˆç‡çš„åŒæ—¶æ”¹è¿›ç‰¹å¾è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨ç»¿è‰²é€šé“æ›´ä¸°å¯Œçš„ä¿¡å·å¢å¼ºå±€éƒ¨ç»†èŠ‚ï¼Œè¿™ä¸äººçœ¼çš„æ•æ„Ÿåº¦å’ŒBayeræ»¤é•œè®¾è®¡ç›¸ä¸€è‡´ã€‚åœ¨å¤šä¸ªRAWå¯¹è±¡æ£€æµ‹æ•°æ®é›†å’Œæ£€æµ‹å™¨ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSimRODåœ¨ä¿æŒé«˜æ•ˆç‡çš„åŒæ—¶ï¼Œä¼˜äºRAW-Adapterå’ŒDIAPç­‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†RAWæ•°æ®åœ¨ç°å®å¯¹è±¡æ£€æµ‹ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07101v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨è§†è§‰æ¨¡å‹ä¸­ï¼Œç›¸è¾ƒäºsRGBå›¾åƒï¼ŒRAWæ•°æ®åœ¨ç‰©ä½“æ£€æµ‹æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚RAWæ•°æ®èƒ½å¤Ÿä¿ç•™ISPå¤„ç†å‰çš„ä¼ æ„Ÿå™¨ä¿¡æ¯ï¼Œä»è€Œæé«˜æ£€æµ‹ç²¾åº¦å¹¶ä¼˜åŒ–ç¡¬ä»¶è®¾è®¡ã€‚å°½ç®¡RAWç‰©ä½“æ£€æµ‹é¢ä¸´è®­ç»ƒæ•°æ®æœ‰é™ã€åƒç´ åˆ†å¸ƒä¸å‡å’Œä¼ æ„Ÿå™¨å™ªå£°ç­‰é—®é¢˜ï¼Œä½†æœ¬æ–‡æå‡ºçš„SimRODæ–¹æ³•ï¼Œé€šè¿‡åº”ç”¨ä»…æœ‰å››ä¸ªå‚æ•°çš„å¯å­¦ä¹ å…¨å±€ä¼½é©¬å˜æ¢å¢å¼ºç‰¹å¾è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨ç»¿è‰²é€šé“çš„ä¸°å¯Œä¿¡å·å¢å¼ºå±€éƒ¨ç»†èŠ‚ï¼Œåœ¨å¤šä¸ªRAWç‰©ä½“æ£€æµ‹æ•°æ®é›†å’Œæ£€æµ‹å™¨ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSimRODåœ¨ä¿æŒé«˜æ•ˆç‡çš„åŒæ—¶ï¼Œä¼˜äºRAW-Adapterå’ŒDIAPç­‰å…ˆè¿›æ–¹æ³•ã€‚æœ¬æ–‡å¼ºè°ƒäº†RAWæ•°æ®åœ¨ç°å®ç‰©ä½“æ£€æµ‹ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAWæ•°æ®åœ¨ç‰©ä½“æ£€æµ‹ä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿä¿ç•™ISPå¤„ç†å‰çš„ä¼ æ„Ÿå™¨ä¿¡æ¯ï¼Œæé«˜æ£€æµ‹ç²¾åº¦å¹¶ä¼˜åŒ–ç¡¬ä»¶è®¾è®¡ã€‚</li>
<li>ç°æœ‰çš„è§†è§‰æ¨¡å‹å¤§å¤šé’ˆå¯¹sRGBå›¾åƒè®¾è®¡ï¼Œè€ŒRAWç‰©ä½“æ£€æµ‹é¢ä¸´è®­ç»ƒæ•°æ®æœ‰é™ã€åƒç´ åˆ†å¸ƒä¸å‡å’Œä¼ æ„Ÿå™¨å™ªå£°ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†SimRODæ–¹æ³•ï¼Œæ˜¯ä¸€ç§è½»é‡çº§ã€æœ‰æ•ˆçš„RAWç‰©ä½“æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>SimRODé€šè¿‡åº”ç”¨å…¨å±€ä¼½é©¬å¢å¼ºæ¨¡å—ï¼Œä½¿ç”¨ä»…å››ä¸ªå‚æ•°çš„å¯å­¦ä¹ å…¨å±€ä¼½é©¬å˜æ¢æ¥æ”¹å–„ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>SimRODåˆ©ç”¨ç»¿è‰²é€šé“çš„ä¸°å¯Œä¿¡å·å¢å¼ºå±€éƒ¨ç»†èŠ‚ï¼Œä¸äººç±»è§†è§‰æ•æ„Ÿåº¦å’ŒBayeræ»¤é•œè®¾è®¡ç›¸ç¬¦ã€‚</li>
<li>åœ¨å¤šä¸ªRAWç‰©ä½“æ£€æµ‹æ•°æ®é›†å’Œæ£€æµ‹å™¨ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSimRODä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼Œå¦‚RAW-Adapterå’ŒDIAPã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07101">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d1781158eca3302c85151bdcd18cd0a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fbb1fd6bccff3f3af62dcf10c594635.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-728a872271640c634a0b459b972dbfdb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5cc325fc95f8936502a9f4e0e90b1047.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f019e90f7fadb985cc23ab14eb687091.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7b86602b4f17a3e9088327f7e558b1ce.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="OmniSAM-Omnidirectional-Segment-Anything-Model-for-UDA-in-Panoramic-Semantic-Segmentation"><a href="#OmniSAM-Omnidirectional-Segment-Anything-Model-for-UDA-in-Panoramic-Semantic-Segmentation" class="headerlink" title="OmniSAM: Omnidirectional Segment Anything Model for UDA in Panoramic   Semantic Segmentation"></a>OmniSAM: Omnidirectional Segment Anything Model for UDA in Panoramic   Semantic Segmentation</h2><p><strong>Authors:Ding Zhong, Xu Zheng, Chenfei Liao, Yuanhuiyi Lyu, Jialei Chen, Shengyang Wu, Linfeng Zhang, Xuming Hu</strong></p>
<p>Segment Anything Model 2 (SAM2) has emerged as a strong base model in various pinhole imaging segmentation tasks. However, when applying it to $360^\circ$ domain, the significant field-of-view (FoV) gap between pinhole ($70^\circ \times 70^\circ$) and panoramic images ($180^\circ \times 360^\circ$) poses unique challenges. Two major concerns for this application includes 1) inevitable distortion and object deformation brought by the large FoV disparity between domains; 2) the lack of pixel-level semantic understanding that the original SAM2 cannot provide. To address these issues, we propose a novel OmniSAM framework, which makes the first attempt to apply SAM2 for panoramic semantic segmentation. Specifically, to bridge the first gap, OmniSAM first divides the panorama into sequences of patches. These patches are then treated as image sequences in similar manners as in video segmentation tasks. We then leverage the SAM2â€™s memory mechanism to extract cross-patch correspondences that embeds the cross-FoV dependencies, improving feature continuity and the prediction consistency along mask boundaries. For the second gap, OmniSAM fine-tunes the pretrained image encoder and reutilize the mask decoder for semantic prediction. An FoV-based prototypical adaptation module with dynamic pseudo label update mechanism is also introduced to facilitate the alignment of memory and backbone features, thereby improving model generalization ability across different sizes of source models. Extensive experimental results demonstrate that OmniSAM outperforms the state-of-the-art methods by large margins, e.g., 79.06% (+10.22%) on SPin8-to-SPan8, 62.46% (+6.58%) on CS13-to-DP13. </p>
<blockquote>
<p>Segment Anything Model 2ï¼ˆSAM2ï¼‰åœ¨å„ç§é’ˆå­”æˆåƒåˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæˆä¸ºå¼ºå¤§çš„åŸºç¡€æ¨¡å‹ã€‚ç„¶è€Œï¼Œå°†å…¶åº”ç”¨äº$360^\circ$é¢†åŸŸæ—¶ï¼Œé’ˆå­”ï¼ˆ$70^\circ \times 70^\circ$ï¼‰ä¸å…¨æ™¯å›¾åƒï¼ˆ$180^\circ \times 360^\circ$ï¼‰ä¹‹é—´è§†é‡ï¼ˆFoVï¼‰çš„æ˜¾è‘—å·®è·å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚è¯¥åº”ç”¨é¢ä¸´ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š1ï¼‰ç”±é¢†åŸŸé—´å¤§è§†é‡å·®å¼‚å¸¦æ¥çš„ä¸å¯é¿å…å¤±çœŸå’Œç‰©ä½“å˜å½¢ï¼›2ï¼‰åŸå§‹SAM2æ— æ³•æä¾›åƒç´ çº§åˆ«çš„è¯­ä¹‰ç†è§£ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å…¨æ–°çš„OmniSAMæ¡†æ¶ï¼Œå®ƒæ˜¯é¦–æ¬¡å°è¯•å°†SAM2åº”ç”¨äºå…¨æ™¯è¯­ä¹‰åˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å¼¥åˆç¬¬ä¸€ä¸ªå·®è·ï¼ŒOmniSAMé¦–å…ˆæŠŠå…¨æ™¯å›¾åƒåˆ†è§£æˆä¸€ç³»åˆ—çš„å›¾å—ã€‚è¿™äº›å›¾å—ç„¶åä»¥ç±»ä¼¼äºè§†é¢‘åˆ†å‰²ä»»åŠ¡çš„æ–¹å¼è¢«å½“ä½œå›¾åƒåºåˆ—è¿›è¡Œå¤„ç†ã€‚æ¥ç€æˆ‘ä»¬åˆ©ç”¨SAM2çš„è®°å¿†æœºåˆ¶æ¥æå–è·¨å›¾å—å¯¹åº”å…³ç³»ï¼Œè¿™äº›å…³ç³»åµŒå…¥è·¨è§†é‡ä¾èµ–ï¼Œæ”¹è¿›äº†ç‰¹å¾è¿ç»­æ€§å’Œé¢„æµ‹çš„ä¸€è‡´æ€§æ²¿ç€è’™ç‰ˆè¾¹ç•Œã€‚ä¸ºäº†å¼¥åˆç¬¬äºŒä¸ªå·®è·ï¼ŒOmniSAMå¯¹é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨è¿›è¡Œäº†å¾®è°ƒï¼Œå¹¶é‡æ–°ä½¿ç”¨è’™ç‰ˆè§£ç å™¨è¿›è¡Œè¯­ä¹‰é¢„æµ‹ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªåŸºäºè§†é‡çš„åŸå‹é€‚åº”æ¨¡å—ï¼Œå¸¦æœ‰åŠ¨æ€ä¼ªæ ‡ç­¾æ›´æ–°æœºåˆ¶ï¼Œä»¥ä¿ƒè¿›è®°å¿†å’Œä¸»å¹²ç‰¹å¾çš„å¯¹é½ï¼Œä»è€Œæé«˜æ¨¡å‹åœ¨ä¸åŒå°ºå¯¸æºæ¨¡å‹ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼ŒOmniSAMç›¸è¾ƒäºæœ€å‰æ²¿çš„æ–¹æ³•è¡¨ç°å‡ºæ›´å¤§çš„ä¼˜åŠ¿ï¼Œä¾‹å¦‚åœ¨SPin8-to-SPan8ä¸Šè¾¾åˆ°79.06%ï¼ˆ+10.22%ï¼‰ï¼Œåœ¨CS13-to-DP13ä¸Šè¾¾åˆ°62.46%ï¼ˆ+6.58%ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07098v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹å…¨æ™¯è¯­ä¹‰åˆ†å‰²ä¸­çš„è§†åœºï¼ˆFoVï¼‰å·®å¼‚é—®é¢˜ï¼ŒOmniSAMæ¡†æ¶é¦–æ¬¡å°è¯•å°†SAM2åº”ç”¨äºå…¨æ™¯åœºæ™¯ã€‚OmniSAMé€šè¿‡åˆ†å‰²å…¨æ™¯å›¾åƒä¸ºå¤šä¸ªè¡¥ä¸ï¼Œåˆ©ç”¨SAM2çš„è®°å¿†æœºåˆ¶æå–è·¨è¡¥ä¸å¯¹åº”å…³ç³»ï¼Œå¹¶å¼•å…¥åŸºäºè§†åœºçš„åŸå‹è‡ªé€‚åº”æ¨¡å—æ¥æ”¹å–„æ¨¡å‹åœ¨ä¸åŒæºæ¨¡å‹å¤§å°ä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜OmniSAMæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SAM2æ¨¡å‹åœ¨å…¨æ™¯è¯­ä¹‰åˆ†å‰²ä¸­é¢ä¸´è§†åœºå·®å¼‚å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>OmniSAMæ¡†æ¶é€šè¿‡åˆ†å‰²å…¨æ™¯å›¾åƒä¸ºè¡¥ä¸æ¥è§£å†³è§†åœºå·®å¼‚é—®é¢˜ï¼Œå¹¶åº”ç”¨SAM2è¿›è¡Œç‰¹å¾æå–ã€‚</li>
<li>OmniSAMåˆ©ç”¨SAM2çš„è®°å¿†æœºåˆ¶æå–è·¨è¡¥ä¸çš„å¯¹åº”å…³ç³»ï¼Œæ”¹å–„ç‰¹å¾è¿ç»­æ€§å’Œé¢„æµ‹ä¸€è‡´æ€§ã€‚</li>
<li>ä¸ºäº†æ”¹å–„è¯­ä¹‰é¢„æµ‹ï¼ŒOmniSAMå¾®è°ƒäº†é¢„è®­ç»ƒå›¾åƒç¼–ç å™¨å¹¶é‡æ–°ä½¿ç”¨æ©è†œè§£ç å™¨ã€‚</li>
<li>å¼•å…¥åŸºäºè§†åœºçš„åŸå‹è‡ªé€‚åº”æ¨¡å—å’ŒåŠ¨æ€ä¼ªæ ‡ç­¾æ›´æ–°æœºåˆ¶ï¼Œæé«˜æ¨¡å‹åœ¨ä¸åŒæºæ¨¡å‹å¤§å°ä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜OmniSAMåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07098">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8da470009b4d1f6300d8bc79263ebebe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-777a52325caf7fceba79cc17e13046d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0ca8b89b107ef901d1c3c62e14a6806e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d627d8ece8e0e4bc2af1aed8ebe55d97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a70f0cd429f8be186ee31e46e0c634b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-644c624b22c9d4340ace6db6bc7f3da4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MemorySAM-Memorize-Modalities-and-Semantics-with-Segment-Anything-Model-2-for-Multi-modal-Semantic-Segmentation"><a href="#MemorySAM-Memorize-Modalities-and-Semantics-with-Segment-Anything-Model-2-for-Multi-modal-Semantic-Segmentation" class="headerlink" title="MemorySAM: Memorize Modalities and Semantics with Segment Anything Model   2 for Multi-modal Semantic Segmentation"></a>MemorySAM: Memorize Modalities and Semantics with Segment Anything Model   2 for Multi-modal Semantic Segmentation</h2><p><strong>Authors:Chenfei Liao, Xu Zheng, Yuanhuiyi Lyu, Haiwei Xue, Yihong Cao, Jiawen Wang, Kailun Yang, Xuming Hu</strong></p>
<p>Research has focused on Multi-Modal Semantic Segmentation (MMSS), where pixel-wise predictions are derived from multiple visual modalities captured by diverse sensors. Recently, the large vision model, Segment Anything Model 2 (SAM2), has shown strong zero-shot segmentation performance on both images and videos. When extending SAM2 to MMSS, two issues arise: 1. How can SAM2 be adapted to multi-modal data? 2. How can SAM2 better understand semantics? Inspired by cross-frame correlation in videos, we propose to treat multi-modal data as a sequence of frames representing the same scene. Our key idea is to â€˜â€™memorizeâ€™â€™ the modality-agnostic information and â€˜memorizeâ€™ the semantics related to the targeted scene. To achieve this, we apply SAM2â€™s memory mechanisms across multi-modal data to capture modality-agnostic features. Meanwhile, to memorize the semantic knowledge, we propose a training-only Semantic Prototype Memory Module (SPMM) to store category-level prototypes across training for facilitating SAM2â€™s transition from instance to semantic segmentation. A prototypical adaptation loss is imposed between global and local prototypes iteratively to align and refine SAM2â€™s semantic understanding. Extensive experimental results demonstrate that our proposed MemorySAM outperforms SoTA methods by large margins on both synthetic and real-world benchmarks (65.38% on DELIVER, 52.88% on MCubeS). Source code will be made publicly available. </p>
<blockquote>
<p>ç ”ç©¶é‡ç‚¹ä¸ºå¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²ï¼ˆMMSSï¼‰ï¼Œå…¶ä¸­åƒç´ çº§é¢„æµ‹æ˜¯ä»ç”±å¤šç§ä¼ æ„Ÿå™¨æ•è·çš„å¤šä¸ªè§†è§‰æ¨¡æ€ä¸­å¾—å‡ºçš„ã€‚æœ€è¿‘ï¼Œå¤§å‹è§†è§‰æ¨¡å‹â€œä¸‡ç‰©åˆ†å‰²æ¨¡å‹2â€ï¼ˆSAM2ï¼‰åœ¨å›¾åƒå’Œè§†é¢‘ä¸Šéƒ½è¡¨ç°å‡ºäº†å¼ºå¤§çš„é›¶æ ·æœ¬åˆ†å‰²æ€§èƒ½ã€‚å½“å°†SAM2æ‰©å±•åˆ°MMSSæ—¶ï¼Œå‡ºç°äº†ä¸¤ä¸ªé—®é¢˜ï¼š1. å¦‚ä½•ä½¿SAM2é€‚åº”å¤šæ¨¡æ€æ•°æ®ï¼Ÿ2. SAM2å¦‚ä½•æ›´å¥½åœ°ç†è§£è¯­ä¹‰ï¼Ÿå—è§†é¢‘ä¸­è·¨å¸§å…³è”çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºå°†å¤šæ¨¡æ€æ•°æ®è§†ä¸ºè¡¨ç¤ºåŒä¸€åœºæ™¯çš„å¸§åºåˆ—ã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡â€œè®°å¿†â€ä¸æ¨¡æ€æ— å…³çš„ä¿¡æ¯å’Œä¸ç›®æ ‡åœºæ™¯ç›¸å…³çš„è¯­ä¹‰æ¥å®ç°è¿™ä¸€ç‚¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åº”ç”¨SAM2çš„è®°å¿†æœºåˆ¶æ¥æ•è·å¤šæ¨¡æ€æ•°æ®çš„æ¨¡æ€æ— å…³ç‰¹å¾ã€‚åŒæ—¶ï¼Œä¸ºäº†è®°å¿†è¯­ä¹‰çŸ¥è¯†ï¼Œæˆ‘ä»¬æå‡ºäº†ä»…ç”¨äºè®­ç»ƒçš„è¯­ä¹‰åŸå‹è®°å¿†æ¨¡å—ï¼ˆSPMMï¼‰ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­˜å‚¨ç±»åˆ«çº§åˆ«çš„åŸå‹ï¼Œä»¥ä¿ƒè¿›SAM2ä»å®ä¾‹åˆ†å‰²åˆ°è¯­ä¹‰åˆ†å‰²çš„è¿‡æ¸¡ã€‚é€šè¿‡å…¨å±€å’Œå±€éƒ¨åŸå‹ä¹‹é—´è¿­ä»£æ–½åŠ åŸå‹é€‚åº”æŸå¤±ï¼Œä»¥å¯¹é½å’Œç»†åŒ–SAM2çš„è¯­ä¹‰ç†è§£ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„MemorySAMåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŸºå‡†æµ‹è¯•ï¼ˆDELIVERä¸Šçš„65.38%ï¼ŒMCubeSä¸Šçš„52.88%ï¼‰ä¸Šéƒ½ä¼˜äºæœ€æ–°æ–¹æ³•çš„å¤§å¹…è¾¹è·ã€‚æºä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06700v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²ï¼ˆMMSSï¼‰çš„ç ”ç©¶ï¼Œé€šè¿‡åˆ©ç”¨Segment Anything Model 2ï¼ˆSAM2ï¼‰çš„é›¶æ ·æœ¬åˆ†å‰²æ€§èƒ½ï¼Œæå‡ºä¸€ç§å°†å¤šæ¨¡æ€æ•°æ®è§†ä¸ºåœºæ™¯å¸§åºåˆ—çš„æ–¹æ³•ã€‚é€šè¿‡è·¨æ¨¡æ€æ•°æ®çš„è®°å¿†æœºåˆ¶æ•è·æ¨¡æ€æ— å…³ç‰¹å¾ï¼Œå¹¶é€šè¿‡è¯­ä¹‰åŸå‹è®°å¿†æ¨¡å—ï¼ˆSPMMï¼‰å®ç°è¯­ä¹‰çŸ¥è¯†çš„è®°å¿†ï¼Œä»è€Œæå‡SAM2å¯¹è¯­ä¹‰çš„ç†è§£ã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ‰€æå‡ºçš„æ–¹æ³•MemorySAMä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>ç ”ç©¶é›†ä¸­åœ¨å¤šæ¨¡æ€è¯­ä¹‰åˆ†å‰²ï¼ˆMMSSï¼‰ä¸Šï¼Œé¢ä¸´å¦‚ä½•é€‚åº”å¤šæ¨¡æ€æ•°æ®å’Œå¢å¼ºè¯­ä¹‰ç†è§£ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>åˆ©ç”¨Segment Anything Model 2ï¼ˆSAM2ï¼‰çš„é›¶æ ·æœ¬åˆ†å‰²æ€§èƒ½ï¼Œå°†å…¶æ‰©å±•åˆ°å¤šæ¨¡æ€åœºæ™¯ã€‚</li>
<li>æå‡ºå°†å¤šæ¨¡æ€æ•°æ®è§†ä¸ºåœºæ™¯å¸§åºåˆ—çš„æ–¹æ³•ï¼Œé€šè¿‡è®°å¿†æœºåˆ¶æ•è·æ¨¡æ€æ— å…³ç‰¹å¾ã€‚</li>
<li>å¼•å…¥è¯­ä¹‰åŸå‹è®°å¿†æ¨¡å—ï¼ˆSPMMï¼‰ï¼Œç”¨äºå­˜å‚¨åœºæ™¯ç±»åˆ«çº§åˆ«çš„åŸå‹ï¼Œä¿ƒè¿›ä»å®ä¾‹åˆ°è¯­ä¹‰åˆ†å‰²çš„è¿‡æ¸¡ã€‚</li>
<li>é€šè¿‡åŸå‹é€‚åº”æŸå¤±æ¥è¿­ä»£åœ°å¯¹é½å’Œç²¾ç‚¼SAM2çš„è¯­ä¹‰ç†è§£ã€‚</li>
<li>æ‰€æå‡ºçš„æ–¹æ³•åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œçš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a89204a14f4adf2cc1e74bdafbbbdecd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fabe0833a60ca5d549e6ddcd2e3b4e3e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b992fb34049f470712eaf9ef6d11d750.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a1307a53f7ebe0a3cf79ccaefeb63d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2cd18d4412f7567f2e26baa54be4482d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Steerable-Pyramid-Weighted-Loss-Multi-Scale-Adaptive-Weighting-for-Semantic-Segmentation"><a href="#Steerable-Pyramid-Weighted-Loss-Multi-Scale-Adaptive-Weighting-for-Semantic-Segmentation" class="headerlink" title="Steerable Pyramid Weighted Loss: Multi-Scale Adaptive Weighting for   Semantic Segmentation"></a>Steerable Pyramid Weighted Loss: Multi-Scale Adaptive Weighting for   Semantic Segmentation</h2><p><strong>Authors:Renhao Lu</strong></p>
<p>Semantic segmentation is a core task in computer vision with applications in biomedical imaging, remote sensing, and autonomous driving. While standard loss functions such as cross-entropy and Dice loss perform well in general cases, they often struggle with fine structures, particularly in tasks involving thin structures or closely packed objects. Various weight map-based loss functions have been proposed to address this issue by assigning higher loss weights to pixels prone to misclassification. However, these methods typically rely on precomputed or runtime-generated weight maps based on distance transforms, which impose significant computational costs and fail to adapt to evolving network predictions. In this paper, we propose a novel steerable pyramid-based weighted (SPW) loss function that efficiently generates adaptive weight maps. Unlike traditional boundary-aware losses that depend on static or iteratively updated distance maps, our method leverages steerable pyramids to dynamically emphasize regions across multiple frequency bands (capturing features at different scales) while maintaining computational efficiency. Additionally, by incorporating network predictions into the weight computation, our approach enables adaptive refinement during training. We evaluate our method on the SNEMI3D, GlaS, and DRIVE datasets, benchmarking it against 11 state-of-the-art loss functions. Our results demonstrate that the proposed SPW loss function achieves superior pixel precision and segmentation accuracy with minimal computational overhead. This work provides an effective and efficient solution for improving semantic segmentation, particularly for applications requiring multiscale feature representation. The code is avaiable at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/SPW-0884">https://anonymous.4open.science/r/SPW-0884</a> </p>
<blockquote>
<p>è¯­ä¹‰åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„æ ¸å¿ƒä»»åŠ¡ï¼Œåœ¨ç”Ÿç‰©åŒ»å­¦æˆåƒã€é¥æ„Ÿå’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚è™½ç„¶äº¤å‰ç†µå’ŒDiceæŸå¤±ç­‰æ ‡å‡†æŸå¤±å‡½æ•°åœ¨ä¸€èˆ¬æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ï¼Œä½†å®ƒä»¬åœ¨å¤„ç†ç²¾ç»†ç»“æ„æ—¶å¸¸å¸¸é‡åˆ°å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠç»†è–„ç»“æ„æˆ–å¯†é›†æ’åˆ—çš„ç‰©ä½“æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå·²ç»æå‡ºäº†å„ç§åŸºäºæƒé‡å›¾çš„æŸå¤±å‡½æ•°ï¼Œé€šè¿‡ç»™å®¹æ˜“è¯¯åˆ†ç±»çš„åƒç´ åˆ†é…æ›´é«˜çš„æŸå¤±æƒé‡æ¥æ”¹è¿›åˆ†å‰²æ•ˆæœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºåŸºäºè·ç¦»å˜æ¢çš„é¢„è®¡ç®—æˆ–è¿è¡Œæ—¶ç”Ÿæˆçš„æƒé‡å›¾ï¼Œè¿™å¸¦æ¥äº†æ˜¾è‘—çš„è®¡ç®—æˆæœ¬ï¼Œå¹¶ä¸”æ— æ³•é€‚åº”ä¸æ–­å˜åŒ–çš„ç½‘ç»œé¢„æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06604v1">PDF</a> 9 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¯å¯¼å‘é‡‘å­—å¡”çš„åŠ æƒï¼ˆSPWï¼‰æŸå¤±å‡½æ•°ï¼Œç”¨äºæ”¹å–„è¯­ä¹‰åˆ†å‰²çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠå¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºçš„åº”ç”¨ä¸­ã€‚è¯¥å‡½æ•°èƒ½å¤ŸåŠ¨æ€å¼ºè°ƒä¸åŒé¢‘ç‡å¸¦çš„åŒºåŸŸï¼Œå¹¶ç»“åˆç½‘ç»œé¢„æµ‹è¿›è¡Œæƒé‡è®¡ç®—ï¼Œä»¥å®ç°è‡ªé€‚åº”ç»†åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPWæŸå¤±å‡½æ•°åœ¨åƒç´ ç²¾åº¦å’Œåˆ†å‰²å‡†ç¡®æ€§æ–¹é¢è¾¾åˆ°ä¼˜è¶Šæ€§èƒ½ï¼ŒåŒæ—¶è®¡ç®—å¼€é”€è¾ƒå°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯­ä¹‰åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰çš„æ ¸å¿ƒä»»åŠ¡ï¼Œåœ¨ç”Ÿç‰©åŒ»å­¦æˆåƒã€é¥æ„Ÿã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>æ ‡å‡†æŸå¤±å‡½æ•°åœ¨ç²¾ç»†ç»“æ„ä¸Šçš„è¡¨ç°æœ‰å¾…æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠè–„ç»“æ„æˆ–å¯†é›†æ’åˆ—ç‰©ä½“çš„ä»»åŠ¡ä¸­ã€‚</li>
<li>åŸºäºæƒé‡å›¾çš„æŸå¤±å‡½æ•°å·²è¢«æå‡ºä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºé¢„è®¡ç®—æˆ–è¿è¡Œæ—¶ç”Ÿæˆçš„æƒé‡å›¾ï¼Œè®¡ç®—æˆæœ¬é«˜ä¸”æ— æ³•é€‚åº”ç½‘ç»œé¢„æµ‹çš„æ¼”å˜ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„SPWæŸå¤±å‡½æ•°ï¼Œåˆ©ç”¨å¯å¯¼å‘é‡‘å­—å¡”æœ‰æ•ˆåœ°ç”Ÿæˆè‡ªé€‚åº”æƒé‡å›¾ï¼Œèƒ½å¤ŸåŠ¨æ€å¼ºè°ƒä¸åŒé¢‘ç‡å¸¦çš„åŒºåŸŸå¹¶ç»´æŒè®¡ç®—æ•ˆç‡ã€‚</li>
<li>SPWæŸå¤±å‡½æ•°ç»“åˆç½‘ç»œé¢„æµ‹è¿›è¡Œæƒé‡è®¡ç®—ï¼Œä½¿è®­ç»ƒè¿‡ç¨‹ä¸­èƒ½å¤Ÿå®ç°è‡ªé€‚åº”ç»†åŒ–ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSPWæŸå¤±å‡½æ•°åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå®ç°äº†è¾ƒé«˜çš„åƒç´ ç²¾åº¦å’Œåˆ†å‰²å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-48af0d028b6877b43b987d0ae5cbc5e9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8adc22c44457042c0879dc605d799c41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a13f6b92c074cfa38cf0746c3cd542d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-750c80ca5ec12cbbc42471661e8bb3ee.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="PointDiffuse-A-Dual-Conditional-Diffusion-Model-for-Enhanced-Point-Cloud-Semantic-Segmentation"><a href="#PointDiffuse-A-Dual-Conditional-Diffusion-Model-for-Enhanced-Point-Cloud-Semantic-Segmentation" class="headerlink" title="PointDiffuse: A Dual-Conditional Diffusion Model for Enhanced Point   Cloud Semantic Segmentation"></a>PointDiffuse: A Dual-Conditional Diffusion Model for Enhanced Point   Cloud Semantic Segmentation</h2><p><strong>Authors:Yong He, Hongshan Yu, Mingtao Feng, Tongjia Chen, Zechuan Li, Anwaar Ulhaq, Saeed Anwar, Ajmal Saeed Mian</strong></p>
<p>Diffusion probabilistic models are traditionally used to generate colors at fixed pixel positions in 2D images. Building on this, we extend diffusion models to point cloud semantic segmentation, where point positions also remain fixed, and the diffusion model generates point labels instead of colors. To accelerate the denoising process in reverse diffusion, we introduce a noisy label embedding mechanism. This approach integrates semantic information into the noisy label, providing an initial semantic reference that improves the reverse diffusion efficiency. Additionally, we propose a point frequency transformer that enhances the adjustment of high-level context in point clouds. To reduce computational complexity, we introduce the position condition into MLP and propose denoising PointNet to process the high-resolution point cloud without sacrificing geometric details. Finally, we integrate the proposed noisy label embedding, point frequency transformer and denoising PointNet in our proposed dual conditional diffusion model-based network (PointDiffuse) to perform large-scale point cloud semantic segmentation. Extensive experiments on five benchmarks demonstrate the superiority of PointDiffuse, achieving the state-of-the-art mIoU of 74.2% on S3DIS Area 5, 81.2% on S3DIS 6-fold and 64.8% on SWAN dataset. </p>
<blockquote>
<p>ä¼ ç»Ÿçš„æ‰©æ•£æ¦‚ç‡æ¨¡å‹ä¸»è¦ç”¨äºåœ¨2Då›¾åƒçš„å›ºå®šåƒç´ ä½ç½®ç”Ÿæˆé¢œè‰²ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å°†æ‰©æ•£æ¨¡å‹æ‰©å±•åˆ°ç‚¹äº‘è¯­ä¹‰åˆ†å‰²ï¼Œå…¶ä¸­ç‚¹ä½ç½®ä¹Ÿæ˜¯å›ºå®šçš„ï¼Œæ‰©æ•£æ¨¡å‹ç”Ÿæˆç‚¹æ ‡ç­¾è€Œä¸æ˜¯é¢œè‰²ã€‚ä¸ºäº†åŠ é€Ÿåå‘æ‰©æ•£ä¸­çš„å»å™ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†å™ªå£°æ ‡ç­¾åµŒå…¥æœºåˆ¶ã€‚è¿™ç§æ–¹æ³•å°†è¯­ä¹‰ä¿¡æ¯é›†æˆåˆ°å™ªå£°æ ‡ç­¾ä¸­ï¼Œä¸ºåå‘æ‰©æ•£æä¾›äº†ä¸€ä¸ªåˆå§‹è¯­ä¹‰å‚è€ƒï¼Œæé«˜äº†åå‘æ‰©æ•£çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç‚¹é¢‘å˜å‹å™¨ï¼Œå®ƒå¢å¼ºäº†ç‚¹äº‘ä¸­é«˜çº§ä¸Šä¸‹æ–‡çš„è°ƒæ•´ã€‚ä¸ºäº†é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œæˆ‘ä»¬å°†ä½ç½®æ¡ä»¶å¼•å…¥å¤šå±‚æ„ŸçŸ¥æœºï¼Œå¹¶æå‡ºå»å™ªPointNetæ¥å¤„ç†é«˜åˆ†è¾¨ç‡ç‚¹äº‘è€Œä¸æŸå¤±å‡ ä½•ç»†èŠ‚ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ‰€æå‡ºçš„å™ªå£°æ ‡ç­¾åµŒå…¥ã€ç‚¹é¢‘å˜å‹å™¨å’Œå»å™ªPointNeté›†æˆåˆ°æˆ‘ä»¬æ‰€æå‡ºçš„åŸºäºåŒæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„ç½‘ç»œï¼ˆPointDiffuseï¼‰ä¸­ï¼Œä»¥æ‰§è¡Œå¤§è§„æ¨¡ç‚¹äº‘è¯­ä¹‰åˆ†å‰²ã€‚åœ¨äº”ç»„åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒPointDiffuseå…·æœ‰ä¼˜è¶Šæ€§ï¼Œåœ¨S3DIS Area 5ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„mIoUä¸º74.2%ï¼Œåœ¨S3DIS 6å€äº¤å‰éªŒè¯ä¸Šä¸º81.2%ï¼Œåœ¨SWANæ•°æ®é›†ä¸Šä¸º64.8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06094v1">PDF</a> 8 pages, 3 figures, 7 tables</p>
<p><strong>Summary</strong>:<br>åŸºäºæ‰©æ•£æ¦‚ç‡æ¨¡å‹ç”ŸæˆäºŒç»´å›¾åƒä¸­çš„å›ºå®šåƒç´ ä½ç½®é¢œè‰²ï¼Œæœ¬æ–‡å°†å…¶æ‰©å±•åˆ°ç‚¹äº‘è¯­ä¹‰åˆ†å‰²é¢†åŸŸã€‚é€šè¿‡å¼•å…¥å™ªå£°æ ‡ç­¾åµŒå…¥æœºåˆ¶å’Œç‚¹é¢‘å˜æ¢å™¨ï¼Œæé«˜äº†åå‘æ‰©æ•£è¿‡ç¨‹çš„æ•ˆç‡å’Œç‚¹äº‘ä¸Šä¸‹æ–‡è°ƒæ•´çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜å¼•å…¥äº†ä½ç½®æ¡ä»¶åˆ°å¤šå±‚æ„ŸçŸ¥æœºä¸­ï¼Œå¹¶æå‡ºäº†å»å™ªPointNetæ¥å¤„ç†é«˜åˆ†è¾¨ç‡ç‚¹äº‘è€Œä¿æŒå‡ ä½•ç»†èŠ‚ã€‚æœ€ç»ˆï¼Œå°†æ‰€æå‡ºçš„ç»„ä»¶é›†æˆåˆ°ä¸€ä¸ªåŸºäºåŒæ¡ä»¶æ‰©æ•£æ¨¡å‹çš„PointDiffuseç½‘ç»œä¸­ï¼Œå®ç°äº†å¤§è§„æ¨¡ç‚¹äº‘è¯­ä¹‰åˆ†å‰²çš„ä¼˜å¼‚æ€§èƒ½ã€‚åœ¨äº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPointDiffuseè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å¹³å‡äº¤å¹¶æ¯”æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å°†æ‰©æ•£æ¦‚ç‡æ¨¡å‹åº”ç”¨äºç‚¹äº‘è¯­ä¹‰åˆ†å‰²é¢†åŸŸã€‚</li>
<li>å¼•å…¥å™ªå£°æ ‡ç­¾åµŒå…¥æœºåˆ¶ä»¥åŠ é€Ÿåå‘æ‰©æ•£è¿‡ç¨‹å¹¶é›†æˆè¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>æå‡ºç‚¹é¢‘å˜æ¢å™¨ä»¥å¢å¼ºç‚¹äº‘ä¸­ä¸Šä¸‹æ–‡ä¿¡æ¯çš„è°ƒæ•´ã€‚</li>
<li>é€šè¿‡å¼•å…¥ä½ç½®æ¡ä»¶åˆ°å¤šå±‚æ„ŸçŸ¥æœºä¸­ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ã€‚</li>
<li>æå‡ºå»å™ªPointNetä»¥å¤„ç†é«˜åˆ†è¾¨ç‡ç‚¹äº‘å¹¶ä¿æŒå‡ ä½•ç»†èŠ‚ã€‚</li>
<li>é›†æˆä¸Šè¿°ç»„ä»¶åˆ°åŒæ¡ä»¶æ‰©æ•£æ¨¡å‹PointDiffuseä¸­ï¼Œå®ç°å¤§è§„æ¨¡ç‚¹äº‘è¯­ä¹‰åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-39a27e50e75216213ac48fafa49c5162.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cfaec9582b08dc0cd3089d9c2a565a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3eadbee93ccb5403bbe052a38f435c05.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Improving-SAM-for-Camouflaged-Object-Detection-via-Dual-Stream-Adapters"><a href="#Improving-SAM-for-Camouflaged-Object-Detection-via-Dual-Stream-Adapters" class="headerlink" title="Improving SAM for Camouflaged Object Detection via Dual Stream Adapters"></a>Improving SAM for Camouflaged Object Detection via Dual Stream Adapters</h2><p><strong>Authors:Jiaming Liu, Linghe Kong, Guihai Chen</strong></p>
<p>Segment anything model (SAM) has shown impressive general-purpose segmentation performance on natural images, but its performance on camouflaged object detection (COD) is unsatisfactory. In this paper, we propose SAM-COD that performs camouflaged object detection for RGB-D inputs. While keeping the SAM architecture intact, dual stream adapters are expanded on the image encoder to learn potential complementary information from RGB images and depth images, and fine-tune the mask decoder and its depth replica to perform dual-stream mask prediction. In practice, the dual stream adapters are embedded into the attention block of the image encoder in a parallel manner to facilitate the refinement and correction of the two types of image embeddings. To mitigate channel discrepancies arising from dual stream embeddings that do not directly interact with each other, we augment the association of dual stream embeddings using bidirectional knowledge distillation including a model distiller and a modal distiller. In addition, to predict the masks for RGB and depth attention maps, we hybridize the two types of image embeddings which are jointly learned with the prompt embeddings to update the initial prompt, and then feed them into the mask decoders to synchronize the consistency of image embeddings and prompt embeddings. Experimental results on four COD benchmarks show that our SAM-COD achieves excellent detection performance gains over SAM and achieves state-of-the-art results with a given fine-tuning paradigm. </p>
<blockquote>
<p>åˆ†æ®µæ¨¡å‹ï¼ˆSAMï¼‰åœ¨è‡ªç„¶å›¾åƒä¸Šçš„é€šç”¨åˆ†å‰²æ€§èƒ½ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†åœ¨ä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼ˆCODï¼‰æ–¹é¢çš„æ€§èƒ½å´ä¸å°½äººæ„ã€‚æœ¬æ–‡æå‡ºäº†é’ˆå¯¹RGB-Dè¾“å…¥çš„ä¼ªè£…ç›®æ ‡æ£€æµ‹SAM-CODã€‚åœ¨ä¿æŒSAMæ¶æ„å®Œæ•´æ€§çš„åŒæ—¶ï¼Œåœ¨å›¾åƒç¼–ç å™¨ä¸Šæ‰©å±•äº†åŒæµé€‚é…å™¨ï¼Œä»¥å­¦ä¹ RGBå›¾åƒå’Œæ·±åº¦å›¾åƒä¹‹é—´æ½œåœ¨çš„äº’è¡¥ä¿¡æ¯ï¼Œå¹¶å¾®è°ƒæ©è†œè§£ç å™¨åŠå…¶æ·±åº¦å‰¯æœ¬ï¼Œä»¥æ‰§è¡ŒåŒæµæ©è†œé¢„æµ‹ã€‚åœ¨å®è·µä¸­ï¼ŒåŒæµé€‚é…å™¨ä»¥å¹¶è¡Œæ–¹å¼åµŒå…¥åˆ°å›¾åƒç¼–ç å™¨çš„æ³¨æ„åŠ›å—ä¸­ï¼Œä»¥ä¿ƒè¿›ä¸¤ç§å›¾åƒåµŒå…¥çš„ç»†åŒ–å’Œæ ¡æ­£ã€‚ä¸ºäº†ç¼“è§£ç”±äºåŒæµåµŒå…¥è€Œäº§ç”Ÿçš„é€šé“å·®å¼‚ï¼Œè¿™äº›åµŒå…¥ä¹‹é—´å¹¶ä¸ç›´æ¥ç›¸äº’ä½œç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†åŒå‘çŸ¥è¯†è’¸é¦å¢å¼ºåŒæµåµŒå…¥çš„å…³è”ï¼ŒåŒ…æ‹¬æ¨¡å‹è’¸é¦å™¨å’Œæ¨¡æ€è’¸é¦å™¨ã€‚æ­¤å¤–ï¼Œä¸ºäº†é¢„æµ‹RGBå’Œæ·±åº¦æ³¨æ„åŠ›å›¾çš„æ©è†œï¼Œæˆ‘ä»¬æ··åˆäº†ä¸¤ç§ç±»å‹çš„å›¾åƒåµŒå…¥ï¼Œè¿™äº›åµŒå…¥ä¸æç¤ºåµŒå…¥ä¸€èµ·è”åˆå­¦ä¹ ä»¥æ›´æ–°åˆå§‹æç¤ºï¼Œç„¶åå°†å…¶è¾“å…¥åˆ°æ©è†œè§£ç å™¨ä¸­ï¼Œä»¥åŒæ­¥å›¾åƒåµŒå…¥å’Œæç¤ºåµŒå…¥çš„ä¸€è‡´æ€§ã€‚åœ¨å››ä¸ªCODåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„SAM-CODåœ¨SAMä¸Šå®ç°äº†å‡ºè‰²çš„æ£€æµ‹æ€§èƒ½æå‡ï¼Œå¹¶åœ¨ç»™å®šçš„å¾®è°ƒèŒƒå¼ä¸‹è¾¾åˆ°äº†æœ€æ–°ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06042v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šSAMæ¨¡å‹åœ¨è‡ªç„¶å›¾åƒåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼ˆCODï¼‰ä¸Šçš„è¡¨ç°ä¸ä½³ã€‚æœ¬æ–‡æå‡ºSAM-CODæ¨¡å‹ï¼Œç”¨äºRGB-Dè¾“å…¥çš„ä¼ªè£…ç›®æ ‡æ£€æµ‹ã€‚é€šè¿‡ä¿ç•™SAMæ¶æ„çš„åŒæ—¶ï¼Œåœ¨å›¾åƒç¼–ç å™¨ä¸Šæ‰©å±•åŒæµé€‚é…å™¨ä»¥å­¦ä¹ RGBå›¾åƒå’Œæ·±åº¦å›¾åƒä¹‹é—´çš„æ½œåœ¨äº’è¡¥ä¿¡æ¯ï¼Œå¹¶å¾®è°ƒæ©è†œè§£ç å™¨åŠå…¶æ·±åº¦å‰¯æœ¬ä»¥æ‰§è¡ŒåŒæµæ©è†œé¢„æµ‹ã€‚é€šè¿‡åµŒå…¥æ³¨æ„åŠ›å—çš„åŒå‘çŸ¥è¯†è’¸é¦å¢å¼ºåŒæµåµŒå…¥çš„å…³è”ï¼Œå¹¶æ··åˆä¸¤ç§å›¾åƒåµŒå…¥æ¥é¢„æµ‹RGBå’Œæ·±åº¦æ³¨æ„åŠ›å›¾çš„æ©è†œã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSAM-CODåœ¨å››ä¸ªCODåŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†å‡ºè‰²çš„æ£€æµ‹æ€§èƒ½æå‡ï¼Œå¹¶åœ¨å¾®è°ƒæ¨¡å¼ä¸‹è¾¾åˆ°äº†å‰æ²¿æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SAMæ¨¡å‹åœ¨è‡ªç„¶å›¾åƒåˆ†å‰²ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼ˆCODï¼‰æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æå‡ºäº†SAM-CODæ¨¡å‹ï¼Œè¯¥æ¨¡å‹é’ˆå¯¹RGB-Dè¾“å…¥è¿›è¡Œä¼ªè£…ç›®æ ‡æ£€æµ‹ã€‚</li>
<li>SAM-CODé€šè¿‡æ‰©å±•åŒæµé€‚é…å™¨å­¦ä¹ RGBå’Œæ·±åº¦å›¾åƒçš„æ½œåœ¨äº’è¡¥ä¿¡æ¯ã€‚</li>
<li>åŒå‘çŸ¥è¯†è’¸é¦ç”¨äºå¢å¼ºåŒæµåµŒå…¥çš„å…³è”ã€‚</li>
<li>åŒæµé€‚é…å™¨åµŒå…¥åˆ°å›¾åƒç¼–ç å™¨çš„æ³¨æ„åŠ›å—ä¸­ä»¥ä¿ƒè¿›ä¸¤ç§å›¾åƒåµŒå…¥çš„å®Œå–„å’Œæ ¡æ­£ã€‚</li>
<li>é€šè¿‡æ··åˆä¸¤ç§å›¾åƒåµŒå…¥å’Œæç¤ºåµŒå…¥æ¥é¢„æµ‹RGBå’Œæ·±åº¦æ³¨æ„åŠ›å›¾çš„æ©è†œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dd8a0e440b4d43cca1c515cf62a93743.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-479e5b1bde5db53fb39683e4e19e6919.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-012410ffb087f588fd8ad22d3703a29b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17c794269379ec97436955f10cc75d30.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Exploring-Token-Level-Augmentation-in-Vision-Transformer-for-Semi-Supervised-Semantic-Segmentation"><a href="#Exploring-Token-Level-Augmentation-in-Vision-Transformer-for-Semi-Supervised-Semantic-Segmentation" class="headerlink" title="Exploring Token-Level Augmentation in Vision Transformer for   Semi-Supervised Semantic Segmentation"></a>Exploring Token-Level Augmentation in Vision Transformer for   Semi-Supervised Semantic Segmentation</h2><p><strong>Authors:Dengke Zhang, Quan Tang, Fagui Liu, Haiqing Mei, C. L. Philip Chen</strong></p>
<p>Semi-supervised semantic segmentation has witnessed remarkable advancements in recent years. However, existing algorithms are based on convolutional neural networks and directly applying them to Vision Transformers poses certain limitations due to conceptual disparities. To this end, we propose TokenMix, a data augmentation technique specifically designed for semi-supervised semantic segmentation with Vision Transformers. TokenMix aligns well with the global attention mechanism by mixing images at the token level, enhancing learning capability for contextual information among image patches. We further incorporate image augmentation and feature augmentation to promote the diversity of augmentation. Moreover, to enhance consistency regularization, we propose a dual-branch framework where each branch applies image and feature augmentation to the input image. We conduct extensive experiments across multiple benchmark datasets, including Pascal VOC 2012, Cityscapes, and COCO. Results suggest that the proposed method outperforms state-of-the-art algorithms with notably observed accuracy improvement, especially under limited fine annotations. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼ŒåŠç›‘ç£è¯­ä¹‰åˆ†å‰²å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œç°æœ‰ç®—æ³•ä¸»è¦åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼Œç›´æ¥å°†å…¶åº”ç”¨äºè§†è§‰å˜å‹å™¨ä¼šé‡åˆ°ä¸€å®šçš„å±€é™æ€§ï¼Œå› ä¸ºä¸¤è€…åœ¨æ¦‚å¿µä¸Šå­˜åœ¨å·®è·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†TokenMixï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨é’ˆå¯¹åŸºäºè§†è§‰å˜å‹å™¨çš„åŠç›‘ç£è¯­ä¹‰åˆ†å‰²è®¾è®¡çš„æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚TokenMixé€šè¿‡åœ¨æ ‡è®°å±‚é¢æ··åˆå›¾åƒä¸å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ç´§å¯†ç»“åˆï¼Œå¢å¼ºäº†å›¾åƒè¡¥ä¸ä¹‹é—´ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å­¦ä¹ èƒ½åŠ›ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥ç»“åˆäº†å›¾åƒå¢å¼ºå’Œç‰¹å¾å¢å¼ºï¼Œä»¥ä¿ƒè¿›å¢å¼ºçš„å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒåˆ†æ”¯æ¡†æ¶ï¼Œæ¯ä¸ªåˆ†æ”¯å¯¹è¾“å…¥å›¾åƒåº”ç”¨å›¾åƒå’Œç‰¹å¾å¢å¼ºã€‚æˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼ŒåŒ…æ‹¬Pascal VOC 2012ã€Cityscapeså’ŒCOCOã€‚ç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¼˜äºæœ€æ–°ç®—æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™çš„ç²¾ç»†æ ‡æ³¨ä¸‹ï¼Œè§‚å¯Ÿåˆ°æ˜æ˜¾çš„å‡†ç¡®ç‡æé«˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02459v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè¿‘å¹´æ¥ï¼ŒåŠç›‘ç£è¯­ä¹‰åˆ†å‰²é¢†åŸŸå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œç°æœ‰ç®—æ³•ä¸»è¦åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼Œç›´æ¥åº”ç”¨äºè§†è§‰Transformerå­˜åœ¨å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†TokenMixï¼Œè¿™æ˜¯ä¸€ç§ä¸“é—¨ä¸ºåŠç›‘ç£è¯­ä¹‰åˆ†å‰²ä¸è§†è§‰Transformerè®¾è®¡çš„æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚TokenMixé€šè¿‡ä¸å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ç»“åˆï¼Œé€šè¿‡åœ¨æ ‡è®°å±‚é¢æ··åˆå›¾åƒï¼Œæé«˜å›¾åƒè¡¥ä¸ä¹‹é—´ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å­¦ä¹ èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜åŠ å…¥äº†å›¾åƒå¢å¼ºå’Œç‰¹å¾å¢å¼ºä»¥ä¿ƒè¿›å¢å¼ºçš„å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†åŠ å¼ºä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒåˆ†æ”¯æ¡†æ¶ï¼Œæ¯ä¸ªåˆ†æ”¯å¯¹è¾“å…¥å›¾åƒåº”ç”¨å›¾åƒå’Œç‰¹å¾å¢å¼ºã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›ç®—æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™ç²¾ç»†æ ‡æ³¨ä¸‹è§‚å¯Ÿåˆ°äº†æ˜¾è‘—çš„å‡†ç¡®æ€§æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŠç›‘ç£è¯­ä¹‰åˆ†å‰²é¢†åŸŸå–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰ç®—æ³•ç›´æ¥åº”ç”¨äºè§†è§‰Transformerå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æå‡ºäº†TokenMixæ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œä¸“é—¨ä¸ºåŠç›‘ç£è¯­ä¹‰åˆ†å‰²ä¸è§†è§‰Transformerè®¾è®¡ã€‚</li>
<li>TokenMixé€šè¿‡ä¸å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ç»“åˆï¼Œé€šè¿‡åœ¨æ ‡è®°å±‚é¢æ··åˆå›¾åƒï¼Œæé«˜ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>åŠ å…¥äº†å›¾åƒå¢å¼ºå’Œç‰¹å¾å¢å¼ºä»¥ä¿ƒè¿›å¢å¼ºçš„å¤šæ ·æ€§ã€‚</li>
<li>æå‡ºåŒåˆ†æ”¯æ¡†æ¶ä»¥åŠ å¼ºä¸€è‡´æ€§æ­£åˆ™åŒ–ï¼Œæ¯ä¸ªåˆ†æ”¯å¯¹è¾“å…¥å›¾åƒåº”ç”¨å›¾åƒå’Œç‰¹å¾å¢å¼ºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºæœ€å…ˆè¿›ç®—æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e59fe33f06b90e3558fc7476a8390708.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-db85d71660905f6b9e2666a1cb01262f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3c9ed3dbd750fb58c20f9591f8458aea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f3602e67cc215ed770f36a49d672da5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cafcbd570b7845fc63fb04eac23ca21c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f3e0e3e89bd211966b214efdce41271.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89d312652e30617a2551296e8b290195.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ContextFormer-Redefining-Efficiency-in-Semantic-Segmentation"><a href="#ContextFormer-Redefining-Efficiency-in-Semantic-Segmentation" class="headerlink" title="ContextFormer: Redefining Efficiency in Semantic Segmentation"></a>ContextFormer: Redefining Efficiency in Semantic Segmentation</h2><p><strong>Authors:Mian Muhammad Naeem Abid, Nancy Mehta, Zongwei Wu, Radu Timofte</strong></p>
<p>Semantic segmentation assigns labels to pixels in images, a critical yet challenging task in computer vision. Convolutional methods, although capturing local dependencies well, struggle with long-range relationships. Vision Transformers (ViTs) excel in global context capture but are hindered by high computational demands, especially for high-resolution inputs. Most research optimizes the encoder architecture, leaving the bottleneck underexplored - a key area for enhancing performance and efficiency. We propose ContextFormer, a hybrid framework leveraging the strengths of CNNs and ViTs in the bottleneck to balance efficiency, accuracy, and robustness for real-time semantic segmentation. The frameworkâ€™s efficiency is driven by three synergistic modules: the Token Pyramid Extraction Module (TPEM) for hierarchical multi-scale representation, the Transformer and Branched DepthwiseConv (Trans-BDC) block for dynamic scale-aware feature modeling, and the Feature Merging Module (FMM) for robust integration with enhanced spatial and contextual consistency. Extensive experiments on ADE20K, Pascal Context, CityScapes, and COCO-Stuff datasets show ContextFormer significantly outperforms existing models, achieving state-of-the-art mIoU scores, setting a new benchmark for efficiency and performance. The codes will be made publicly available upon acceptance. </p>
<blockquote>
<p>è¯­ä¹‰åˆ†å‰²æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹é‡è¦ä¸”å¯Œæœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå®ƒéœ€è¦å¯¹å›¾åƒä¸­çš„åƒç´ è¿›è¡Œæ ‡æ³¨ã€‚å·ç§¯æ–¹æ³•è™½ç„¶èƒ½å¾ˆå¥½åœ°æ•æ‰å±€éƒ¨ä¾èµ–æ€§ï¼Œä½†åœ¨å¤„ç†é•¿è·ç¦»å…³ç³»æ—¶å´é‡åˆ°å›°éš¾ã€‚è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰æ“…é•¿æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ï¼Œä½†è®¡ç®—éœ€æ±‚è¾ƒé«˜ï¼Œå°¤å…¶æ˜¯é«˜åˆ†è¾¨ç‡è¾“å…¥æ—¶ã€‚å¤§å¤šæ•°ç ”ç©¶éƒ½åœ¨ä¼˜åŒ–ç¼–ç å™¨æ¶æ„ï¼Œè€Œå¯¹ç“¶é¢ˆåŒºåŸŸè¿™ä¸ªå¢å¼ºæ€§èƒ½å’Œæ•ˆç‡çš„å…³é”®é¢†åŸŸæ¢ç´¢ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†ContextFormerï¼Œè¿™æ˜¯ä¸€ä¸ªæ··åˆæ¡†æ¶ï¼Œåˆ©ç”¨CNNå’ŒViTåœ¨ç“¶é¢ˆå¤„çš„ä¼˜åŠ¿ï¼Œåœ¨å®æ—¶è¯­ä¹‰åˆ†å‰²ä¸­å¹³è¡¡æ•ˆç‡ã€å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚è¯¥æ¡†æ¶çš„æ•ˆç‡ç”±ä¸‰ä¸ªååŒå·¥ä½œçš„æ¨¡å—é©±åŠ¨ï¼šç”¨äºåˆ†å±‚å¤šå°ºåº¦è¡¨ç¤ºçš„Tokené‡‘å­—å¡”æå–æ¨¡å—ï¼ˆTPEMï¼‰ã€ç”¨äºåŠ¨æ€å°ºåº¦æ„ŸçŸ¥ç‰¹å¾å»ºæ¨¡çš„Transformerå’Œåˆ†æ”¯æ·±åº¦å·ç§¯ï¼ˆTrans-BDCï¼‰å—ï¼Œä»¥åŠç”¨äºç¨³å¥é›†æˆçš„ç‰¹å¾åˆå¹¶æ¨¡å—ï¼ˆFMMï¼‰ï¼Œå…·æœ‰å¢å¼ºçš„ç©ºé—´å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚åœ¨ADE20Kã€Pascal Contextã€CityScapeså’ŒCOCO-Stuffæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒContextFormeræ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„mIoUåˆ†æ•°ï¼Œä¸ºæ•ˆç‡å’Œæ€§èƒ½è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚è®ºæ–‡è¢«æ¥å—åï¼Œä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.19255v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§æ–°å‹çš„è¯­ä¹‰åˆ†å‰²æ¡†æ¶ContextFormerè¢«æå‡ºï¼Œè¯¥æ¡†æ¶ç»“åˆäº†CNNå’ŒViTçš„ä¼˜ç‚¹ï¼Œæ—¨åœ¨æé«˜å®æ—¶è¯­ä¹‰åˆ†å‰²çš„æ•ˆç‡ã€å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚é€šè¿‡ä¸‰ä¸ªååŒæ¨¡å—çš„è®¾è®¡ï¼ŒåŒ…æ‹¬Tokené‡‘å­—å¡”æå–æ¨¡å—ã€Transformerä¸åˆ†æ”¯æ·±åº¦å·ç§¯å—ä»¥åŠç‰¹å¾åˆå¹¶æ¨¡å—ï¼ŒContextFormerå®ç°äº†å‡ºè‰²çš„æ€§èƒ½ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒContextFormeræ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œè¾¾åˆ°äº†æœ€æ–°çš„mIoUå¾—åˆ†ï¼Œä¸ºæ•ˆç‡å’Œæ€§èƒ½è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ContextFormeræ˜¯ä¸€ä¸ªç”¨äºå®æ—¶è¯­ä¹‰åˆ†å‰²çš„æ··åˆæ¡†æ¶ï¼Œç»“åˆäº†CNNå’ŒViTçš„ä¼˜åŠ¿ã€‚</li>
<li>è¯¥æ¡†æ¶æ—¨åœ¨æé«˜è¯­ä¹‰åˆ†å‰²çš„æ•ˆç‡ã€å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚</li>
<li>ContextFormeré€šè¿‡ä¸‰ä¸ªååŒæ¨¡å—å®ç°é«˜æ€§èƒ½ï¼šTokené‡‘å­—å¡”æå–æ¨¡å—ã€Transformerä¸åˆ†æ”¯æ·±åº¦å·ç§¯å—ä»¥åŠç‰¹å¾åˆå¹¶æ¨¡å—ã€‚</li>
<li>ContextFormeræ˜¾è‘—ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œè¾¾åˆ°äº†æœ€æ–°çš„mIoUå¾—åˆ†ã€‚</li>
<li>ContextFormeråœ¨ADE20Kã€Pascal Contextã€CityScapeså’ŒCOCO-Stuffç­‰å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
<li>ContextFormerçš„ä»£ç å°†åœ¨æ¥å—åå…¬å¼€å‘å¸ƒã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºæ•ˆç‡å’Œæ€§èƒ½è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.19255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b8a9dd132c2a47a92540b7b47483f5f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5523b56a683d3257ef80b099b50baa8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-833eb5a30852a640bcad4d21cf3e25c2.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="OVA-Det-Open-Vocabulary-Aerial-Object-Detection-with-Image-Text-Collaboration"><a href="#OVA-Det-Open-Vocabulary-Aerial-Object-Detection-with-Image-Text-Collaboration" class="headerlink" title="OVA-Det: Open Vocabulary Aerial Object Detection with Image-Text   Collaboration"></a>OVA-Det: Open Vocabulary Aerial Object Detection with Image-Text   Collaboration</h2><p><strong>Authors:Guoting Wei, Xia Yuan, Yu Liu, Zhenhao Shang, Xizhe Xue, Peng Wang, Kelu Yao, Chunxia Zhao, Haokui Zhang, Rong Xiao</strong></p>
<p>Aerial object detection plays a crucial role in numerous applications. However, most existing methods focus on detecting predefined object categories, limiting their applicability in real-world open scenarios. In this paper, we extend aerial object detection to open scenarios through image-text collaboration and propose OVA-Det, a highly efficient open-vocabulary detector for aerial scenes. Specifically, we first introduce an image-to-text alignment loss to replace the conventional category regression loss, thereby eliminating category limitations. Next, we propose a lightweight text-guided strategy that enhances the feature extraction process in the encoder and enables queries to focus on class-relevant image features within the decoder, further improving detection accuracy without introducing significant additional costs. Extensive comparison experiments demonstrate that the proposed OVA-Det outperforms state-of-the-art methods on all three widely used benchmark datasets by a large margin. For instance, for zero-shot detection on DIOR, OVA-Det achieves 37.2 mAP and 79.8 Recall, 12.4 and 42.0 higher than that of YOLO-World. In addition, the inference speed of OVA-Det reaches 36 FPS on RTX 4090, meeting the real-time detection requirements for various applications. The code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/GT-Wei/OVA-Det%7D%7Bhttps://github.com/GT-Wei/OVA-Det%7D">https://github.com/GT-Wei/OVA-Det}{https://github.com/GT-Wei/OVA-Det}</a>. </p>
<blockquote>
<p>ç©ºä¸­ç›®æ ‡æ£€æµ‹åœ¨ä¼—å¤šåº”ç”¨ä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•ä¸»è¦é›†ä¸­äºå¯¹é¢„å®šä¹‰ç›®æ ‡ç±»åˆ«çš„æ£€æµ‹ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®å¼€æ”¾åœºæ™¯ä¸­çš„åº”ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å›¾åƒæ–‡æœ¬åä½œå°†ç©ºä¸­ç›®æ ‡æ£€æµ‹æ‰©å±•åˆ°å¼€æ”¾åœºæ™¯ï¼Œå¹¶æå‡ºOVA-Detï¼Œè¿™æ˜¯ä¸€ç§é«˜åº¦æœ‰æ•ˆçš„ç©ºä¸­åœºæ™¯å¼€æ”¾è¯æ±‡è¡¨æ£€æµ‹å™¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥å›¾åƒåˆ°æ–‡æœ¬çš„å¯¹é½æŸå¤±æ¥æ›¿æ¢ä¼ ç»Ÿçš„ç±»åˆ«å›å½’æŸå¤±ï¼Œä»è€Œæ¶ˆé™¤äº†ç±»åˆ«é™åˆ¶ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è½»é‡çº§çš„æ–‡æœ¬å¼•å¯¼ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¢å¼ºäº†ç¼–ç å™¨çš„ç‰¹å¾æå–è¿‡ç¨‹ï¼Œå¹¶ä½¿æŸ¥è¯¢èƒ½å¤Ÿå…³æ³¨è§£ç å™¨ä¸­çš„ç±»ç›¸å…³å›¾åƒç‰¹å¾ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜äº†æ£€æµ‹ç²¾åº¦ï¼Œè€Œæ²¡æœ‰å¼•å…¥é‡å¤§çš„é¢å¤–æˆæœ¬ã€‚å¤§é‡çš„å¯¹æ¯”å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„OVA-Detåœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æ•°æ®é›†ä¸Šå‡å¤§å¹…è¶…è¶Šäº†æœ€æ–°æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨DIORä¸Šçš„é›¶æ ·æœ¬æ£€æµ‹ä¸­ï¼ŒOVA-Detå®ç°äº†37.2çš„mAPå’Œ79.8çš„å¬å›ç‡ï¼Œæ¯”YOLO-Worldé«˜å‡º12.4å’Œ42.0ã€‚æ­¤å¤–ï¼ŒOVA-Detçš„æ¨ç†é€Ÿåº¦åœ¨RTX 4090ä¸Šè¾¾åˆ°36 FPSï¼Œæ»¡è¶³å„ç§åº”ç”¨çš„å®æ—¶æ£€æµ‹è¦æ±‚ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/GT-Wei/OVA-Det">https://github.com/GT-Wei/OVA-Det</a>å¤„è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.12246v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è¯¥è®ºæ–‡å°†ç©ºä¸­ç‰©ä½“æ£€æµ‹æ‰©å±•è‡³å¼€æ”¾åœºæ™¯ï¼Œé€šè¿‡å›¾åƒæ–‡æœ¬ååŒå·¥ä½œï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„å¼€æ”¾è¯æ±‡è¡¨æ£€æµ‹å™¨OVA-Detã€‚æ–°æ–¹æ³•å¼•å…¥å›¾åƒåˆ°æ–‡æœ¬çš„åŒ¹é…æŸå¤±æ¥æ›¿ä»£ä¼ ç»Ÿçš„ç±»åˆ«å›å½’æŸå¤±ï¼Œæ¶ˆé™¤äº†ç±»åˆ«é™åˆ¶ã€‚åŒæ—¶ï¼Œæå‡ºä¸€ç§è½»é‡çº§çš„æ–‡æœ¬å¼•å¯¼ç­–ç•¥ï¼Œæé«˜ç¼–ç å™¨ç‰¹å¾æå–è¿‡ç¨‹ï¼Œå¹¶åœ¨è§£ç å™¨ä¸­ä½¿æŸ¥è¯¢èšç„¦äºç±»ç›¸å…³çš„å›¾åƒç‰¹å¾ï¼Œä»è€Œæé«˜æ£€æµ‹ç²¾åº¦ã€‚å®éªŒè¡¨æ˜ï¼ŒOVA-Detåœ¨ä¸‰ä¸ªå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æ•°æ®é›†ä¸Šå¤§å¹…è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼Œåœ¨DIORä¸Šçš„é›¶æ ·æœ¬æ£€æµ‹ï¼ŒOVA-Detçš„mAPå’ŒRecallåˆ†åˆ«è¾¾åˆ°äº†37.2å’Œ79.8ï¼Œæ¯”YOLO-Worldåˆ†åˆ«é«˜å‡º12.4å’Œ42.0ã€‚æ­¤å¤–ï¼ŒOVA-Detåœ¨RTX 4090ä¸Šçš„æ¨ç†é€Ÿåº¦è¾¾åˆ°36 FPSï¼Œæ»¡è¶³å„ç§åº”ç”¨çš„å®æ—¶æ£€æµ‹è¦æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>è®ºæ–‡å°†ç©ºä¸­ç‰©ä½“æ£€æµ‹æ‰©å±•è‡³å¼€æ”¾åœºæ™¯ï¼Œé€‚åº”äº†æ›´å¹¿æ³›çš„å®é™…åº”ç”¨éœ€æ±‚ã€‚</li>
<li>å¼•å…¥å›¾åƒåˆ°æ–‡æœ¬çš„åŒ¹é…æŸå¤±ï¼Œæ¶ˆé™¤äº†ä¼ ç»Ÿæ£€æµ‹ä¸­çš„ç±»åˆ«é™åˆ¶ã€‚</li>
<li>æå‡ºä¸€ç§è½»é‡çº§çš„æ–‡æœ¬å¼•å¯¼ç­–ç•¥ï¼Œæé«˜ç‰¹å¾æå–å’Œæ£€æµ‹ç²¾åº¦ã€‚</li>
<li>OVA-Detåœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯é›¶æ ·æœ¬æ£€æµ‹ã€‚</li>
<li>OVA-Detçš„mAPå’ŒRecallåœ¨DIORæ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶ŠYOLO-Worldã€‚</li>
<li>OVA-Detçš„æ¨ç†é€Ÿåº¦è¾¾åˆ°36 FPSï¼Œæ»¡è¶³å®æ—¶æ£€æµ‹è¦æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.12246">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7d855c4a7efa3cb31e2443fe48bef397.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b1ff7006019a45699bae8dd6e2fb16d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-09eb59e23c500bfb86e6172a3d4e1672.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-347d94abb8e649f77cc7137439dd9189.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-03-12/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9da9f2ce67bf62b5edd3521993847dac.jpg" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  MSConv Multiplicative and Subtractive Convolution for Face Recognition
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-03-12/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-196f03cf3132c16b893ba204157ee34a.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-03-12  Anatomy-Aware Conditional Image-Text Retrieval
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-03-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">19778.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
