<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-09-16  InfGen A Resolution-Agnostic Paradigm for Scalable Image Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10122v1/page_5_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    35 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-16-更新"><a href="#2025-09-16-更新" class="headerlink" title="2025-09-16 更新"></a>2025-09-16 更新</h1><h2 id="InfGen-A-Resolution-Agnostic-Paradigm-for-Scalable-Image-Synthesis"><a href="#InfGen-A-Resolution-Agnostic-Paradigm-for-Scalable-Image-Synthesis" class="headerlink" title="InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis"></a>InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis</h2><p><strong>Authors:Tao Han, Wanghan Xu, Junchao Gong, Xiaoyu Yue, Song Guo, Luping Zhou, Lei Bai</strong></p>
<p>Arbitrary resolution image generation provides a consistent visual experience across devices, having extensive applications for producers and consumers. Current diffusion models increase computational demand quadratically with resolution, causing 4K image generation delays over 100 seconds. To solve this, we explore the second generation upon the latent diffusion models, where the fixed latent generated by diffusion models is regarded as the content representation and we propose to decode arbitrary resolution images with a compact generated latent using a one-step generator. Thus, we present the \textbf{InfGen}, replacing the VAE decoder with the new generator, for generating images at any resolution from a fixed-size latent without retraining the diffusion models, which simplifies the process, reducing computational complexity and can be applied to any model using the same latent space. Experiments show InfGen is capable of improving many models into the arbitrary high-resolution era while cutting 4K image generation time to under 10 seconds. </p>
<blockquote>
<p>任意分辨率图像生成技术为各种设备提供了一致的视觉体验，对生产者和消费者具有广泛的应用。当前的扩散模型在计算需求上随着分辨率的增加而呈现二次方增长，导致4K图像生成延迟超过100秒。为了解决这个问题，我们在潜在扩散模型的基础上进行了第二代探索，其中由扩散模型生成的固定潜在被视为内容表示。我们提出使用一步生成器，利用紧凑的生成潜在来解码任意分辨率的图像。因此，我们推出了<strong>InfGen</strong>，用新的生成器替换VAE解码器，可以从固定大小的潜在生成任意分辨率的图像，无需重新训练扩散模型，这简化了流程，降低了计算复杂性，并且可以应用于使用相同潜在空间的任何模型。实验表明，InfGen能够将许多模型带入任意高分辨率时代，同时将4K图像生成时间缩短到不到10秒。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10441v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>新一代扩散模型在图像生成方面的应用实现了跨设备的统一视觉体验，特别是在高分辨率图像生成方面有着显著的优势。通过采用固定大小的潜在扩散模型生成的潜在量，我们提出了一种新的解码器来生成任意分辨率的图像。这种名为InfGen的新技术不仅简化了流程，降低了计算复杂度，而且可以应用于使用同一潜在空间的任何模型。实验表明，InfGen可将许多模型提升为任意高分辨率时代，并将4K图像生成时间缩短至不到10秒。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型可实现任意分辨率的图像生成，提供跨设备的统一视觉体验。</li>
<li>当前扩散模型在计算需求上随着分辨率的增加而呈二次方增长，导致高分辨率图像生成时间较长。</li>
<li>第二代扩散模型通过固定潜在量生成，将内容表示与扩散模型相结合。</li>
<li>InfGen技术用于解码任意分辨率的图像，使用紧凑的生成潜在量，无需重新训练扩散模型。</li>
<li>InfGen简化了流程，降低了计算复杂度，可广泛应用于使用同一潜在空间的模型。</li>
<li>实验表明，InfGen技术能够显著提高模型的图像生成能力，进入任意高分辨率时代。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10441">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10441v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10441v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10441v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10441v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10441v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GARD-Gamma-based-Anatomical-Restoration-and-Denoising-for-Retinal-OCT"><a href="#GARD-Gamma-based-Anatomical-Restoration-and-Denoising-for-Retinal-OCT" class="headerlink" title="GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT"></a>GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT</h2><p><strong>Authors:Botond Fazekas, Thomas Pinetz, Guilherme Aresta, Taha Emre, Hrvoje Bogunovic</strong></p>
<p>Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing and monitoring retinal diseases. However, OCT images are inherently degraded by speckle noise, which obscures fine details and hinders accurate interpretation. While numerous denoising methods exist, many struggle to balance noise reduction with the preservation of crucial anatomical structures. This paper introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel deep learning approach for OCT image despeckling that leverages the strengths of diffusion probabilistic models. Unlike conventional diffusion models that assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more accurately reflect the statistical properties of speckle. Furthermore, we introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed, less-noisy image to guide the denoising process. This crucial addition prevents the reintroduction of high-frequency noise. We accelerate the inference process by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans demonstrate that GARD significantly outperforms traditional denoising methods and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE. Qualitative results confirm that GARD produces sharper edges and better preserves fine anatomical details. </p>
<blockquote>
<p>光学相干层析成像（OCT）是诊断视网膜疾病并进行监测的重要成像方式。然而，OCT图像受到斑点噪声的固有干扰，这掩盖了细节并阻碍了准确解释。虽然存在许多去噪方法，但许多方法在平衡噪声降低与关键解剖结构的保留方面存在困难。本文介绍了基于伽马解剖结构恢复和去噪（GARD）技术，这是一种利用扩散概率模型的深度学习方法进行OCT图像去斑处理。与传统的假设高斯噪声的扩散模型不同，GARD采用去噪扩散伽马模型，更准确反映斑点噪声的统计特性。此外，我们引入了降噪保真度术语，利用预处理后的噪声较少的图像来引导去噪过程。这一关键补充避免了高频噪声的再次引入。我们通过将去噪扩散隐模型框架适应到基于伽马的模型来加速推理过程。在具有配对噪声和非噪声OCT B扫描数据集上的实验表明，与传统的去噪方法和最先进的深度学习模型相比，GARD在峰值信噪比（PSNR）、结构相似性度量（SSIM）和均方误差（MSE）方面表现出显著优势。定性结果证实，GARD可以产生更清晰的边缘并更好地保留细微解剖结构细节。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10341v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种基于深度学习的光学相干层析成像（OCT）图像去噪方法——GARD（基于伽马的解剖学恢复和去噪）。该方法利用扩散概率模型的优势，通过采用伽马模型更准确反映散斑的统计特性，并引入去噪保真度项来指导去噪过程，防止高频噪声的重新引入。实验证明，GARD在PSNR、SSIM和MSE指标上显著优于传统去噪方法和最新深度学习模型，能够产生更清晰边缘并更好保留精细解剖学细节。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GARD是一种针对OCT图像去噪的深度学习新方法，基于扩散概率模型。</li>
<li>GARD采用伽马模型更准确反映散斑噪声的统计特性。</li>
<li>引入去噪保真度项，防止高频噪声的重新引入，指导去噪过程。</li>
<li>GARD显著提高了去噪性能，在PSNR、SSIM和MSE指标上优于传统方法和最新深度学习模型。</li>
<li>GARD能够产生更清晰边缘，更好地保留OCT图像中的精细解剖学细节。</li>
<li>GARD方法具有加速推断过程的能力，通过适应隐式扩散模型框架实现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10341">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10341v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Compute-Only-16-Tokens-in-One-Timestep-Accelerating-Diffusion-Transformers-with-Cluster-Driven-Feature-Caching"><a href="#Compute-Only-16-Tokens-in-One-Timestep-Accelerating-Diffusion-Transformers-with-Cluster-Driven-Feature-Caching" class="headerlink" title="Compute Only 16 Tokens in One Timestep: Accelerating Diffusion   Transformers with Cluster-Driven Feature Caching"></a>Compute Only 16 Tokens in One Timestep: Accelerating Diffusion   Transformers with Cluster-Driven Feature Caching</h2><p><strong>Authors:Zhixin Zheng, Xinyu Wang, Chang Zou, Shaobo Wang, Linfeng Zhang</strong></p>
<p>Diffusion transformers have gained significant attention in recent years for their ability to generate high-quality images and videos, yet still suffer from a huge computational cost due to their iterative denoising process. Recently, feature caching has been introduced to accelerate diffusion transformers by caching the feature computation in previous timesteps and reusing it in the following timesteps, which leverage the temporal similarity of diffusion models while ignoring the similarity in the spatial dimension. In this paper, we introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and complementary perspective for previous feature caching. Specifically, ClusCa performs spatial clustering on tokens in each timestep, computes only one token in each cluster and propagates their information to all the other tokens, which is able to reduce the number of tokens by over 90%. Extensive experiments on DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image and text-to-video generation. Besides, it can be directly applied to any diffusion transformer without requirements for training. For instance, ClusCa achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing the original model by 0.51%. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Shenyi-Z/Cache4Diffusion">https://github.com/Shenyi-Z/Cache4Diffusion</a>. </p>
<blockquote>
<p>扩散变压器近年来因其生成高质量图像和视频的能力而备受关注，但由于其迭代去噪过程，仍然面临着巨大的计算成本。最近，特征缓存的引入通过缓存先前的时间步长的特征计算并在后续的时间步长中重复使用它来加速扩散变压器，这利用了扩散模型的时间相似性，同时忽略了空间维度的相似性。本文介绍了一种正交且互补于以前特征缓存的 Cluster-Driven Feature Caching (ClusCa)。具体来说，ClusCa 对每个时间步长的令牌进行空间聚类，每个集群只计算一个令牌，并将其信息传播到所有其他令牌，这能够将令牌数量减少超过 90%。在 DiT、FLUX 和 HunyuanVideo 上的大量实验证明了其在文本到图像和文本到视频生成中的有效性。此外，它可以无需训练直接应用于任何扩散变压器。例如，ClusCa 在 FLUX 上实现了 4.96 倍的加速，ImageReward 为 99.49%，超出原始模型 0.51%。代码可在 <a target="_blank" rel="noopener" href="https://github.com/Shenyi-Z/Cache4Diffusion">https://github.com/Shenyi-Z/Cache4Diffusion</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10312v1">PDF</a> 11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature   caching for diffusion transformers acceleration</p>
<p><strong>摘要</strong><br>扩散模型生成的图像和视频质量高，但由于迭代去噪过程，计算成本仍然很高。最近引入了特征缓存来加速扩散模型，通过在后续时间步重用之前时间步的特征计算，并利用扩散模型的时序相似性来忽略空间维度的相似性。本文介绍了集群驱动特征缓存（ClusCa）作为一种正交且补充的视角。具体来说，ClusCa在每个时间步对令牌进行空间聚类，只计算每个集群中的一个令牌，并将其信息传播到所有其他令牌，能够减少超过90%的令牌数量。在DiT、FLUX和HunyuanVideo上的大量实验证明其在文本到图像和文本到视频生成中的有效性。此外，它可以无需训练直接应用于任何扩散模型。例如，ClusCa在FLUX上实现了4.96倍的加速，ImageReward为99.49%，超越了原始模型0.51%。代码可通过链接获取：<a target="_blank" rel="noopener" href="https://github.com/Shenyi-Z/Cache4Diffusion">https://github.com/Shenyi-Z/Cache4Diffusion</a>。</p>
<p><strong>关键见解</strong></p>
<ul>
<li>扩散模型生成高质量图像和视频但计算成本高。</li>
<li>特征缓存旨在加速扩散模型，通过重用之前时间步的特征计算。</li>
<li>Cluster-Driven Feature Caching（ClusCa）通过空间聚类减少计算负担。</li>
<li>ClusCa能够在保持高质量输出的同时大幅度减少计算令牌数量。</li>
<li>ClusCa方法在各种实验设置中都表现出了有效性。</li>
<li>该方法可以很容易地应用于任何扩散模型，无需额外的训练步骤。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10312">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10312v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10312v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10312v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10312v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10312v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Scalable-Training-for-Vector-Quantized-Networks-with-100-Codebook-Utilization"><a href="#Scalable-Training-for-Vector-Quantized-Networks-with-100-Codebook-Utilization" class="headerlink" title="Scalable Training for Vector-Quantized Networks with 100% Codebook   Utilization"></a>Scalable Training for Vector-Quantized Networks with 100% Codebook   Utilization</h2><p><strong>Authors:Yifan Chang, Jie Qin, Limeng Qiao, Xiaofeng Wang, Zheng Zhu, Lin Ma, Xingang Wang</strong></p>
<p>Vector quantization (VQ) is a key component in discrete tokenizers for image generation, but its training is often unstable due to straight-through estimation bias, one-step-behind updates, and sparse codebook gradients, which lead to suboptimal reconstruction performance and low codebook usage. In this work, we analyze these fundamental challenges and provide a simple yet effective solution. To maintain high codebook usage in VQ networks (VQN) during learning annealing and codebook size expansion, we propose VQBridge, a robust, scalable, and efficient projector based on the map function method. VQBridge optimizes code vectors through a compress-process-recover pipeline, enabling stable and effective codebook training. By combining VQBridge with learning annealing, our VQN achieves full (100%) codebook usage across diverse codebook configurations, which we refer to as FVQ (FullVQ). Through extensive experiments, we demonstrate that FVQ is effective, scalable, and generalizable: it attains 100% codebook usage even with a 262k-codebook, achieves state-of-the-art reconstruction performance, consistently improves with larger codebooks, higher vector channels, or longer training, and remains effective across different VQ variants. Moreover, when integrated with LlamaGen, FVQ significantly enhances image generation performance, surpassing visual autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID, highlighting the importance of high-quality tokenizers for strong autoregressive image generation. </p>
<blockquote>
<p>向量量化（VQ）是图像生成离散分词器中的关键组成部分，但其训练通常不稳定，原因在于直通估计偏差、一步滞后更新和稀疏代码本梯度，导致重建性能不佳和代码本使用率低。在这项工作中，我们分析了这些基本挑战，并提供了一种简单有效的解决方案。为了在学习退火和代码本大小扩展过程中保持VQ网络（VQN）的高代码本使用率，我们提出了VQBridge，这是一种基于映射函数方法的稳健、可扩展和高效的投影仪。VQBridge通过压缩-处理-恢复管道优化代码向量，能够实现稳定和有效的代码本训练。通过将VQBridge与学习退火相结合，我们的VQN在不同代码本配置下实现了100%的代码本使用率，我们称之为FVQ（FullVQ）。通过大量实验，我们证明了FVQ的有效性、可扩展性和通用性：它即使在262k代码本下也能实现100%的代码本使用率，达到了最先进的重建性能，随着更大的代码本、更高的向量通道或更长的训练时间而不断改进，并且在不同的VQ变体中都有效。此外，当与LlamaGen集成时，FVQ显着提高了图像生成性能，以0.5的rFID超过了视觉自回归模型（VAR），以0.2的rFID超过了扩散模型（DiT），突显了高质量分词器对于强大的自回归图像生成的重要性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10140v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了向量量化（VQ）在图像生成离散分词器中的关键作用和存在的训练不稳定性问题。为解决这些问题，提出了一种简单有效的解决方案VQBridge。它通过压缩-处理-恢复管道优化代码向量，实现稳定的代码本训练。结合VQBridge和学习退火技术，实现了全代码本使用率的FVQ（FullVQ）。实验表明，FVQ效果显著，可扩展到大规模代码本，并与其他技术相结合时性能更佳。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>向量量化（VQ）是图像生成离散分词器中的重要组成部分，但其训练存在不稳定问题。</li>
<li>训练不稳定的主要原因包括直通估计偏差、一步滞后更新和稀疏代码本梯度。</li>
<li>VQBridge是一个基于映射函数方法的稳健、可扩展和高效的投影仪，可优化代码向量。</li>
<li>VQBridge通过压缩-处理-恢复管道实现稳定的代码本训练，并结合学习退火技术实现全代码本使用率（FVQ）。</li>
<li>FVQ具有显著效果，可扩展到大规模代码本，并与其他技术结合时表现更佳。</li>
<li>FVQ在多种VQ配置中实现了100%的代码本使用率，并在与LlamaGen集成时显著增强了图像生成性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10140">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10140v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10140v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10140v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10140v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10140v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Realism-Control-One-step-Diffusion-for-Real-World-Image-Super-Resolution"><a href="#Realism-Control-One-step-Diffusion-for-Real-World-Image-Super-Resolution" class="headerlink" title="Realism Control One-step Diffusion for Real-World Image Super-Resolution"></a>Realism Control One-step Diffusion for Real-World Image Super-Resolution</h2><p><strong>Authors:Zongliang Wu, Siming Zheng, Peng-Tao Jiang, Xin Yuan</strong></p>
<p>Pre-trained diffusion models have shown great potential in real-world image super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions. While one-step diffusion (OSD) methods significantly improve efficiency compared to traditional multi-step approaches, they still have limitations in balancing fidelity and realism across diverse scenarios. Since the OSDs for SR are usually trained or distilled by a single timestep, they lack flexible control mechanisms to adaptively prioritize these competing objectives, which are inherently manageable in multi-step methods through adjusting sampling steps. To address this challenge, we propose a Realism Controlled One-step Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping strategy that enables explicit control over fidelity-realism trade-offs during the noise prediction phase with minimal training paradigm modifications and original training data. A degradation-aware sampling strategy is also introduced to align distillation regularization with the grouping strategy and enhance the controlling of trade-offs. Moreover, a visual prompt injection module is used to replace conventional text prompts with degradation-aware visual tokens, enhancing both restoration accuracy and semantic consistency. Our method achieves superior fidelity and perceptual quality while maintaining computational efficiency. Extensive experiments demonstrate that RCOD outperforms state-of-the-art OSD methods in both quantitative metrics and visual qualities, with flexible realism control capabilities in the inference stage. The code will be released. </p>
<blockquote>
<p>预训练的扩散模型在真实世界图像超分辨率（Real-ISR）任务中显示出巨大潜力，能够实现高分辨率重建。与传统的多步方法相比，一步扩散（OSD）方法显著提高了效率，但在平衡不同场景下的保真度和真实感方面仍存在局限性。由于OSD超分辨率通常通过单一时间步长进行训练或蒸馏，它们缺乏灵活的控制机制来适应性地优先考虑这些相互竞争的目标，而这些目标在多步方法中可以通过调整采样步骤来内在管理。为解决这一挑战，我们提出了用于Real-ISR的真实感控制一步扩散（RCOD）框架。RCOD提供了一种潜在领域分组策略，能够在噪声预测阶段通过最小的训练范式修改和原始训练数据实现对保真度-真实感权衡的明确控制。还引入了一种退化感知采样策略，以使蒸馏正则化与分组策略相一致，并增强对权衡的控制。此外，使用视觉提示注入模块来替代传统的文本提示，使用退化感知视觉令牌，提高恢复精度和语义一致性。我们的方法在保持计算效率的同时，实现了优越的保真度和感知质量。大量实验表明，RCOD在定量指标和视觉质量方面优于最新的OSD方法，并在推理阶段具有灵活的真实感控制能力。代码将发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10122v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>预训练扩散模型在真实世界图像超分辨率任务中显示出巨大潜力，通过一步扩散方法提高效率。但一步扩散方法面临保真度和现实感平衡的挑战。为此，我们提出一种名为RCOD的新框架，提供潜在域分组策略，在噪声预测阶段明确控制保真度与现实感的权衡，同时引入降解感知采样策略和视觉提示注入模块，提高恢复精度和语义一致性。RCOD方法在计算效率上实现了高保真和感知质量，并在定量指标和视觉质量上优于最先进的一步扩散方法，具有灵活的推理阶段现实感控制功能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>预训练扩散模型在真实世界图像超分辨率任务中表现出潜力。</li>
<li>一步扩散方法提高了效率，但在平衡保真度和现实感方面存在挑战。</li>
<li>RCOD框架通过潜在域分组策略明确控制保真度与现实感的权衡。</li>
<li>引入降解感知采样策略和视觉提示注入模块提升恢复精度和语义一致性。</li>
<li>RCOD方法在定量指标和视觉质量上表现优异，优于其他先进的一步扩散方法。</li>
<li>RCOD具有灵活的推理阶段现实感控制功能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10122">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10122v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10122v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10122v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10122v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.10122v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Chord-Chain-of-Rendering-Decomposition-for-PBR-Material-Estimation-from-Generated-Texture-Images"><a href="#Chord-Chain-of-Rendering-Decomposition-for-PBR-Material-Estimation-from-Generated-Texture-Images" class="headerlink" title="Chord: Chain of Rendering Decomposition for PBR Material Estimation from   Generated Texture Images"></a>Chord: Chain of Rendering Decomposition for PBR Material Estimation from   Generated Texture Images</h2><p><strong>Authors:Zhi Ying, Boxiang Rong, Jingyu Wang, Maoyuan Xu</strong></p>
<p>Material creation and reconstruction are crucial for appearance modeling but traditionally require significant time and expertise from artists. While recent methods leverage visual foundation models to synthesize PBR materials from user-provided inputs, they often fall short in quality, flexibility, and user control. We propose a novel two-stage generate-and-estimate framework for PBR material generation. In the generation stage, a fine-tuned diffusion model synthesizes shaded, tileable texture images aligned with user input. In the estimation stage, we introduce a chained decomposition scheme that sequentially predicts SVBRDF channels by passing previously extracted representation as input into a single-step image-conditional diffusion model. Our method is efficient, high quality, and enables flexible user control. We evaluate our approach against existing material generation and estimation methods, demonstrating superior performance. Our material estimation method shows strong robustness on both generated textures and in-the-wild photographs. Furthermore, we highlight the flexibility of our framework across diverse applications, including text-to-material, image-to-material, structure-guided generation, and material editing. </p>
<blockquote>
<p>材质创建和重建对于外观建模至关重要，但传统上需要艺术家花费大量时间和专业技能。虽然最近的方法利用视觉基础模型来合成用户提供的输入的物理基础渲染（PBR）材质，但在质量、灵活性和用户控制方面往往存在不足。我们提出了一种用于PBR材质生成的新型两阶段生成和估计框架。在生成阶段，经过微调的分扩散模型合成与用户输入对齐的阴影、可拼接纹理图像。在估计阶段，我们引入了一种链式分解方案，该方案通过将由先前提取的表示作为输入传递给单步图像条件扩散模型来顺序预测SVBRDF通道。我们的方法效率高、质量高，并能实现灵活的用户控制。我们将我们的方法与现有的材质生成和估计方法进行了比较评估，展示了优越的性能。我们的材质估计方法在合成纹理和自然照片上都表现出强大的稳健性。此外，我们强调了我们的框架在各种应用中的灵活性，包括文本到材质、图像到材质、结构引导生成和材质编辑。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09952v1">PDF</a> Accepted to SIGGRAPH Asia 2025. Project page:   <a target="_blank" rel="noopener" href="https://ubisoft-laforge.github.io/world/chord">https://ubisoft-laforge.github.io/world/chord</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于扩散模型的两阶段PBR材质生成框架，包括生成阶段和估计阶段。生成阶段利用精细调整的扩散模型合成与用户输入对齐的带阴影、可贴图的纹理图像；估计阶段则采用链式分解方案，通过单次图像条件扩散模型按顺序预测SVBRDF通道。该方法高效、高质量，可实现灵活的用户控制，并在多种应用场合中展示出色性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种基于扩散模型的两阶段PBR材质生成框架，包括生成和估计两个阶段。</li>
<li>生成阶段利用精细调整的扩散模型合成带阴影、可贴图的纹理图像，与用户输入对齐。</li>
<li>估计阶段采用链式分解方案，通过单次图像条件扩散模型按顺序预测SVBRDF通道。</li>
<li>方法高效、高质量，并实现灵活的用户控制。</li>
<li>与现有材质生成和估计方法相比，该方法表现出卓越性能。</li>
<li>材质估计方法对合成纹理和真实照片均表现出强大稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09952">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.09952v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.09952v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.09952v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2509.09952v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Just-Say-the-Word-Annotation-Free-Fine-Grained-Object-Counting"><a href="#Just-Say-the-Word-Annotation-Free-Fine-Grained-Object-Counting" class="headerlink" title="Just Say the Word: Annotation-Free Fine-Grained Object Counting"></a>Just Say the Word: Annotation-Free Fine-Grained Object Counting</h2><p><strong>Authors:Adriano D’Alessandro, Ali Mahdavi-Amiri, Ghassan Hamarneh</strong></p>
<p>Fine-grained object counting remains a major challenge for class-agnostic counting models, which overcount visually similar but incorrect instances (e.g., jalape~no vs. poblano). Addressing this by annotating new data and fully retraining the model is time-consuming and does not guarantee generalization to additional novel categories at test time. Instead, we propose an alternative paradigm: Given a category name, tune a compact concept embedding derived from the prompt using synthetic images and pseudo-labels generated by a text-to-image diffusion model. This embedding conditions a specialization module that refines raw overcounts from any frozen counter into accurate, category-specific estimates\textemdash without requiring real images or human annotations. We validate our approach on \textsc{Lookalikes}, a challenging new benchmark containing 1,037 images across 27 fine-grained subcategories, and show substantial improvements over strong baselines. Code will be released upon acceptance. Dataset - <a target="_blank" rel="noopener" href="https://dalessandro.dev/datasets/lookalikes/">https://dalessandro.dev/datasets/lookalikes/</a> </p>
<blockquote>
<p>精细对象计数仍然是面向类别无关的计数模型的主要挑战，这些模型会过度计算视觉上相似但错误的实例（例如，辣椒与波布拉诺辣椒）。通过标注新数据并完全重新训练模型来解决这一问题是非常耗时的，并且在测试时并不能保证能够推广到新的类别。相反，我们提出了一种替代方法：给定一个类别名称，使用文本到图像的扩散模型生成合成图像和伪标签来微调由提示派生的紧凑概念嵌入。这个嵌入条件会作用于一个专业模块，该模块能够将来自任何冷冻计数器的原始过度计数调整为精确、针对特定类别的估计值——这不需要真实图像或人工注释。我们在新的挑战基准测试Lookalikes上验证了我们的方法，该基准测试包含涉及27个精细粒度子类别的共1037张图像，并显示出相对于强大基准线的实质性改进。接受后将发布代码。数据集：<a target="_blank" rel="noopener" href="https://dalessandro.dev/datasets/lookalikes/">https://dalessandro.dev/datasets/lookalikes/</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.11705v3">PDF</a> data - <a target="_blank" rel="noopener" href="https://dalessandro.dev/datasets/lookalikes/">https://dalessandro.dev/datasets/lookalikes/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于文本到图像扩散模型的新型计数方法。该方法利用类别名称生成合成图像和伪标签，构建紧凑的概念嵌入，用于调整特定类别的计数模块。该方法可以在不需要真实图像或人工标注的情况下，对任意冻结计数器的粗略计数进行精细化调整，特别适用于精细分类的图像计数挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>精细分类对象计数是一个主要挑战，现有模型容易对视觉上相似但错误的实例进行过度计数。</li>
<li>提议的方法基于文本到图像扩散模型生成合成图像和伪标签，构建概念嵌入。</li>
<li>使用该嵌入来训练特定类别的计数模块，能够调整原始计数，获得更准确的估计。</li>
<li>该方法不需要真实图像或人工标注，为计数任务提供了一个高效、泛化的解决方案。</li>
<li>引入了一个新数据集\textsc{Lookalikes}，包含涵盖多个精细分类的子类别的图像，对于模型验证是一个重要资源。</li>
<li>与现有的强基线相比，新方法在数据集上显示出显著改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.11705">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2504.11705v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2504.11705v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2504.11705v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2504.11705v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2504.11705v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Can-Generative-Geospatial-Diffusion-Models-Excel-as-Discriminative-Geospatial-Foundation-Models"><a href="#Can-Generative-Geospatial-Diffusion-Models-Excel-as-Discriminative-Geospatial-Foundation-Models" class="headerlink" title="Can Generative Geospatial Diffusion Models Excel as Discriminative   Geospatial Foundation Models?"></a>Can Generative Geospatial Diffusion Models Excel as Discriminative   Geospatial Foundation Models?</h2><p><strong>Authors:Yuru Jia, Valerio Marsocci, Ziyang Gong, Xue Yang, Maarten Vergauwen, Andrea Nascetti</strong></p>
<p>Self-supervised learning (SSL) has revolutionized representation learning in Remote Sensing (RS), advancing Geospatial Foundation Models (GFMs) to leverage vast unlabeled satellite imagery for diverse downstream tasks. Currently, GFMs primarily employ objectives like contrastive learning or masked image modeling, owing to their proven success in learning transferable representations. However, generative diffusion models, which demonstrate the potential to capture multi-grained semantics essential for RS tasks during image generation, remain underexplored for discriminative applications. This prompts the question: can generative diffusion models also excel and serve as GFMs with sufficient discriminative power? In this work, we answer this question with SatDiFuser, a framework that transforms a diffusion-based generative geospatial foundation model into a powerful pretraining tool for discriminative RS. By systematically analyzing multi-stage, noise-dependent diffusion features, we develop three fusion strategies to effectively leverage these diverse representations. Extensive experiments on remote sensing benchmarks show that SatDiFuser outperforms state-of-the-art GFMs, achieving gains of up to +5.7% mIoU in semantic segmentation and +7.9% F1-score in classification, demonstrating the capacity of diffusion-based generative foundation models to rival or exceed discriminative GFMs. The source code is available at: <a target="_blank" rel="noopener" href="https://github.com/yurujaja/SatDiFuser">https://github.com/yurujaja/SatDiFuser</a>. </p>
<blockquote>
<p>自监督学习（SSL）已经彻底改变了遥感（RS）中的表示学习，推动了地理空间基础模型（GFMs）利用大量无标签卫星图像用于各种下游任务。目前，GFMs主要使用对比学习或掩膜图像建模等目标，因为它们在学习可转移表示方面取得了成功的证明。然而，生成扩散模型在图像生成过程中显示出捕获对遥感任务至关重要的多粒度语义的潜力，但对于判别性应用仍然探索不足。这引发了以下问题：生成扩散模型是否也能表现出色，并作为具有足够判别力的GFMs？在这项工作中，我们用SatDiFuser回答这个问题，这是一个将基于扩散的生成地理空间基础模型转化为强大的预训练工具框架，用于判别遥感。通过系统分析多阶段、噪声相关的扩散特征，我们开发了三种融合策略，以有效利用这些不同的表示形式。在遥感基准测试上的广泛实验表明，SatDiFuser优于最新GFMs，在语义分割上实现了高达+5.7%的mIoU提升，在分类上实现了+7.9%的F1分数提升，证明了基于扩散的生成基础模型与判别GFMs相匹敌或超越的能力。源代码可在：<a target="_blank" rel="noopener" href="https://github.com/yurujaja/SatDiFuser%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/yurujaja/SatDiFuser获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.07890v2">PDF</a> ICCV 2025, camera ready</p>
<p><strong>Summary</strong>：自监督学习（SSL）在遥感领域引发了表征学习的革命，推动了地理空间基础模型（GFMs）利用大量无标签卫星图像进行各种下游任务。当前GFMs主要使用对比学习或掩膜图像建模等目标，而生成扩散模型在图像生成过程中能够捕捉遥感任务所需的多粒度语义信息，但在判别应用方面仍被低估。本研究通过SatDiFuser框架，将基于扩散的生成型地理空间基础模型转化为强大的预训练工具，用于判别遥感。通过系统分析多阶段噪声相关扩散特征，我们开发了三种融合策略，以有效利用这些不同的表示。在遥感基准测试上的广泛实验表明，SatDiFuser优于最新GFMs，语义分割的mIoU提高了5.7%，分类的F1分数提高了7.9%，证明了扩散基础模型的潜力可与判别GFMs相抗衡或超越。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>自监督学习在遥感领域促进了地理空间基础模型的发展，利用大量无标签卫星图像进行多种下游任务。</li>
<li>当前GFMs主要利用对比学习或掩膜图像建模等方法。</li>
<li>生成扩散模型在捕捉遥感任务所需的多粒度语义信息方面具有潜力，但在判别应用方面尚未得到充分探索。</li>
<li>SatDiFuser框架将基于扩散的生成型地理空间基础模型转化为强大的预训练工具，用于判别遥感任务。</li>
<li>通过系统分析多阶段噪声相关扩散特征，开发了三种融合策略。</li>
<li>在遥感基准测试上，SatDiFuser表现出优于现有GFMs的性能，语义分割和分类任务分别实现了mIoU和F1分数的显著提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.07890">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2503.07890v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2503.07890v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2503.07890v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Your-Image-is-Secretly-the-Last-Frame-of-a-Pseudo-Video"><a href="#Your-Image-is-Secretly-the-Last-Frame-of-a-Pseudo-Video" class="headerlink" title="Your Image is Secretly the Last Frame of a Pseudo Video"></a>Your Image is Secretly the Last Frame of a Pseudo Video</h2><p><strong>Authors:Wenlong Chen, Wenlin Chen, Lapo Rastrelli, Yingzhen Li</strong></p>
<p>Diffusion models, which can be viewed as a special case of hierarchical variational autoencoders (HVAEs), have shown profound success in generating photo-realistic images. In contrast, standard HVAEs often produce images of inferior quality compared to diffusion models. In this paper, we hypothesize that the success of diffusion models can be partly attributed to the additional self-supervision information for their intermediate latent states provided by corrupted images, which along with the original image form a pseudo video. Based on this hypothesis, we explore the possibility of improving other types of generative models with such pseudo videos. Specifically, we first extend a given image generative model to their video generative model counterpart, and then train the video generative model on pseudo videos constructed by applying data augmentation to the original images. Furthermore, we analyze the potential issues of first-order Markov data augmentation methods, which are typically used in diffusion models, and propose to use more expressive data augmentation to construct more useful information in pseudo videos. Our empirical results on the CIFAR10 and CelebA datasets demonstrate that improved image generation quality can be achieved with additional self-supervised information from pseudo videos. </p>
<blockquote>
<p>扩散模型可以看作分层变分自动编码器（HVAEs）的一种特殊情况，在生成逼真的图像方面取得了显著的成功。相比之下，标准的HVAEs产生的图像质量往往不如扩散模型。本文中，我们认为扩散模型的成功部分归功于由损坏图像提供的中间潜在状态的额外自我监督信息，这些与原始图像一起形成伪视频。基于这一假设，我们探索了其他类型的生成模型是否可以通过这种伪视频进行改进的可能性。具体来说，我们首先将从给定的图像生成模型扩展到其对应的视频生成模型，然后在通过原始图像的数据增强构建伪视频的基础上对视频生成模型进行训练。此外，我们分析了扩散模型中常用的一阶马尔可夫数据增强方法的潜在问题，并提出使用更具表现力的数据增强方法在伪视频中构建更有用的信息。我们在CIFAR10和CelebA数据集上的实证结果表明，通过来自伪视频的额外自我监督信息可以实现改进的图像生成质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.20158v3">PDF</a> Presented at the ICLR 2025 Workshop on Deep Generative Model in   Machine Learning: Theory, Principle and Efficacy (DeLTa). 1-frame results for   CIFAR10 in Table 2 corrected. Code released</p>
<p><strong>Summary</strong></p>
<p>本文探讨了扩散模型（作为分层变分自编码器（HVAEs）的一种特殊情况）在生成逼真图像方面的成功，并提出其部分原因在于通过损坏图像提供的中间潜在状态的额外自我监督信息，这些与原始图像一起形成伪视频。基于此假设，文章探讨了将这种伪视频应用于其他类型的生成模型以提高图像生成质量的可行性。通过扩展现有图像生成模型为视频生成模型对应物，并使用原始图像的数据增强构建伪视频来训练视频生成模型。此外，文章分析了扩散模型中常用的一阶马尔可夫数据增强方法的潜在问题，并提出使用更具表现力的数据增强来构建伪视频中更有用的信息。在CIFAR10和CelebA数据集上的实证结果表明，通过伪视频的额外自我监督信息可以提高图像生成质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型能够生成高质量的逼真图像，其成功部分归因于通过损坏图像提供的额外自我监督信息。</li>
<li>这种自我监督信息为扩散模型的中间潜在状态提供了附加信息，与原始图像结合形成伪视频。</li>
<li>文章探讨了将这种伪视频应用于其他生成模型的可行性，以提高图像生成质量。</li>
<li>通过将图像生成模型扩展到视频生成模型并使用数据增强构建伪视频来训练该模型。</li>
<li>文章指出了使用一阶马尔可夫数据增强方法的潜在问题，并提倡使用更具表现力的数据增强方法。</li>
<li>实证研究表明，通过伪视频的自我监督信息能够改进图像生成质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.20158">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2410.20158v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2410.20158v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2410.20158v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Diffusion Models/2410.20158v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-16/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-16/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-16/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_医学图像/2509.10098v1/page_2_1.jpg" class="responsive-img" alt="医学图像">
                        
                        <span class="card-title">医学图像</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            医学图像 方向最新论文已更新，请持续关注 Update in 2025-09-16  Joint X-ray, kinetic Sunyaev-Zeldovich, and weak lensing measurements   toward a consensus picture of efficient gas expulsion from groups and   clusters
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    医学图像
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">医学图像</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-16/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Face Swapping/2509.10409v1/page_5_1.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping 方向最新论文已更新，请持续关注 Update in 2025-09-16  Optimizing Inter-chip Coupler Link Placement for Modular and Chiplet   Quantum Systems
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
