<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-16  MatSKRAFT A framework for large-scale materials knowledge extraction   from scientific tables">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10371v1/page_0_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    90 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-16-æ›´æ–°"><a href="#2025-09-16-æ›´æ–°" class="headerlink" title="2025-09-16 æ›´æ–°"></a>2025-09-16 æ›´æ–°</h1><h2 id="MatSKRAFT-A-framework-for-large-scale-materials-knowledge-extraction-from-scientific-tables"><a href="#MatSKRAFT-A-framework-for-large-scale-materials-knowledge-extraction-from-scientific-tables" class="headerlink" title="MatSKRAFT: A framework for large-scale materials knowledge extraction   from scientific tables"></a>MatSKRAFT: A framework for large-scale materials knowledge extraction   from scientific tables</h2><p><strong>Authors:Kausik Hira, Mohd Zaki,  Mausam, N. M. Anoop Krishnan</strong></p>
<p>Scientific progress increasingly depends on synthesizing knowledge across vast literature, yet most experimental data remains trapped in semi-structured formats that resist systematic extraction and analysis. Here, we present MatSKRAFT, a computational framework that automatically extracts and integrates materials science knowledge from tabular data at unprecedented scale. Our approach transforms tables into graph-based representations processed by constraint-driven GNNs that encode scientific principles directly into model architecture. MatSKRAFT significantly outperforms state-of-the-art large language models, achieving F1 scores of 88.68 for property extraction and 71.35 for composition extraction, while processing data $19$-$496\times$ faster than them (compared to the slowest and the fastest models, respectively) with modest hardware requirements. Applied to nearly 69,000 tables from more than 47,000 research publications, we construct a comprehensive database containing over 535,000 entries, including 104,000 compositions that expand coverage beyond major existing databases, pending manual validation. This systematic approach reveals previously overlooked materials with distinct property combinations and enables data-driven discovery of composition-property relationships forming the cornerstone of materials and scientific discovery. </p>
<blockquote>
<p>ç§‘æŠ€çš„è¿›æ­¥è¶Šæ¥è¶Šä¾èµ–äºå¯¹å¤§é‡æ–‡çŒ®çŸ¥è¯†çš„ç»¼åˆï¼Œç„¶è€Œå¤§éƒ¨åˆ†å®éªŒæ•°æ®ä»ç„¶å›°åœ¨åŠç»“æ„åŒ–æ ¼å¼ä¸­ï¼Œé˜»ç¢äº†ç³»ç»Ÿçš„æå–å’Œåˆ†æã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ¨å‡ºäº†MatSKRAFTï¼Œè¿™æ˜¯ä¸€ä¸ªè®¡ç®—æ¡†æ¶ï¼Œèƒ½å¤Ÿä»¥å‰æ‰€æœªæœ‰çš„è§„æ¨¡è‡ªåŠ¨æå–å’Œæ•´åˆææ–™ç§‘å­¦çŸ¥è¯†è¡¨æ ¼æ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†è¡¨æ ¼è½¬åŒ–ä¸ºåŸºäºå›¾çš„è¡¨ç¤ºå½¢å¼ï¼Œé€šè¿‡çº¦æŸé©±åŠ¨çš„å›¾ç¥ç»ç½‘ç»œè¿›è¡Œå¤„ç†ï¼Œç›´æ¥å°†ç§‘å­¦åŸç†ç¼–ç åˆ°æ¨¡å‹æ¶æ„ä¸­ã€‚MatSKRAFTæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåœ¨å±æ€§æå–æ–¹é¢è¾¾åˆ°88.68çš„F1åˆ†æ•°ï¼Œåœ¨æˆåˆ†æå–æ–¹é¢è¾¾åˆ°71.35çš„F1åˆ†æ•°ï¼ŒåŒæ—¶å¤„ç†æ•°æ®çš„é€Ÿåº¦æ¯”å®ƒä»¬å¿«19åˆ°496å€ï¼ˆåˆ†åˆ«ä¸æœ€æ…¢å’Œæœ€å¿«çš„æ¨¡å‹ç›¸æ¯”ï¼‰ï¼Œå¹¶ä¸”ç¡¬ä»¶éœ€æ±‚é€‚ä¸­ã€‚åº”ç”¨äºæ¥è‡ª47000å¤šç¯‡ç ”ç©¶è®ºæ–‡çš„è¿‘69000ä¸ªè¡¨æ ¼ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡53.5ä¸‡ä¸ªæ¡ç›®çš„ç»¼åˆæ•°æ®åº“ï¼Œå…¶ä¸­åŒ…æ‹¬10.4ä¸‡ä¸ªæˆåˆ†ï¼Œæ‰©å¤§äº†è¦†ç›–èŒƒå›ºè¶…è¿‡äº†ç°æœ‰çš„ä¸»è¦æ•°æ®åº“ï¼ˆå¾…å®šæ‰‹åŠ¨éªŒè¯ï¼‰ã€‚è¿™ç§ç³»ç»Ÿæ–¹æ³•æ­ç¤ºäº†ä»¥å‰è¢«å¿½è§†çš„å…·æœ‰ç‹¬ç‰¹å±æ€§ç»„åˆçš„ææ–™ï¼Œå¹¶é€šè¿‡æ•°æ®é©±åŠ¨çš„å‘ç°æˆåˆ†-å±æ€§å…³ç³»ï¼Œæˆä¸ºææ–™å’Œç§‘å­¦å‘ç°çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10448v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MatSKRAFTæ˜¯ä¸€ä¸ªè®¡ç®—æ¡†æ¶ï¼Œèƒ½å¤Ÿä»è¡¨æ ¼æ•°æ®ä¸­è‡ªåŠ¨æå–å’Œæ•´åˆææ–™ç§‘å­¦çŸ¥è¯†ï¼Œå®ç°å‰æ‰€æœªæœ‰çš„å¤§è§„æ¨¡çŸ¥è¯†æ•´åˆã€‚å®ƒé‡‡ç”¨è¡¨æ ¼è½¬å›¾å½¢è¡¨ç¤ºçš„æ–¹æ³•ï¼Œé€šè¿‡çº¦æŸé©±åŠ¨çš„å›¾ç¥ç»ç½‘ç»œè¿›è¡Œå¤„ç†ï¼Œå¹¶å°†ç§‘å­¦åŸç†ç›´æ¥ç¼–ç è¿›æ¨¡å‹æ¶æ„ä¸­ã€‚ä¸ä¼ ç»Ÿçš„é¡¶çº§è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒMatSKRAFTåœ¨å¤„ç†ææ–™å±æ€§æå–å’Œæˆåˆ†æå–æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ï¼ŒåŒæ—¶å¤„ç†é€Ÿåº¦ä¹Ÿæ›´å¿«ï¼Œç¡¬ä»¶éœ€æ±‚è¾ƒä½ã€‚è¯¥ç ”ç©¶åˆ©ç”¨MatSKRAFTç³»ç»Ÿåœ°å¤„ç†è¿‘6ä¸‡å¼ è¡¨æ ¼çš„æ•°æ®ï¼Œæ„å»ºäº†ä¸€ä¸ªåŒ…å«è¶…è¿‡53ä¸‡æ¡è®°å½•çš„ç»¼åˆæ•°æ®åº“ï¼Œæ­ç¤ºäº†å…ˆå‰è¢«å¿½è§†çš„å…·æœ‰ç‹¬ç‰¹å±æ€§ç»„åˆçš„ææ–™ï¼Œå¹¶ä¸ºåŸºäºæ•°æ®çš„æˆåˆ†å±æ€§å…³ç³»å‘ç°å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MatSKRAFTèƒ½å¤Ÿä»è¡¨æ ¼æ•°æ®ä¸­è‡ªåŠ¨æå–å’Œæ•´åˆææ–™ç§‘å­¦çŸ¥è¯†ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å›¾å½¢è¡¨ç¤ºå¤„ç†å’Œçº¦æŸé©±åŠ¨çš„å›¾ç¥ç»ç½‘ç»œæ•´åˆç§‘å­¦åŸç†ã€‚</li>
<li>MatSKRAFTåœ¨å¤„ç†ææ–™å±æ€§æå–å’Œæˆåˆ†æå–æ–¹é¢è¡¨ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>MatSKRAFTå¤„ç†é€Ÿåº¦æ›´å¿«ï¼Œç¡¬ä»¶éœ€æ±‚è¾ƒä½ã€‚</li>
<li>é€šè¿‡å¤„ç†è¿‘6ä¸‡å¼ è¡¨æ ¼çš„æ•°æ®ï¼Œæ„å»ºäº†ä¸€ä¸ªç»¼åˆæ•°æ®åº“ã€‚</li>
<li>è¯¥ç ”ç©¶æ­ç¤ºäº†å…ˆå‰è¢«å¿½è§†çš„å…·æœ‰ç‹¬ç‰¹å±æ€§ç»„åˆçš„ææ–™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10448v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10448v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10448v1/page_3_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DeepDive-Advancing-Deep-Search-Agents-with-Knowledge-Graphs-and-Multi-Turn-RL"><a href="#DeepDive-Advancing-Deep-Search-Agents-with-Knowledge-Graphs-and-Multi-Turn-RL" class="headerlink" title="DeepDive: Advancing Deep Search Agents with Knowledge Graphs and   Multi-Turn RL"></a>DeepDive: Advancing Deep Search Agents with Knowledge Graphs and   Multi-Turn RL</h2><p><strong>Authors:Rui Lu, Zhenyu Hou, Zihan Wang, Hanchen Zhang, Xiao Liu, Yujiang Li, Shi Feng, Jie Tang, Yuxiao Dong</strong></p>
<p>Augmenting large language models (LLMs) with browsing tools substantially improves their potential as deep search agents to solve complex, real-world tasks. Yet, open LLMs still perform poorly in such settings due to limited long-horizon reasoning capacity with browsing tools and the lack of sufficiently difficult supervised data. To address these challenges, we present DeepDive to advance deep search agents. First, we propose a strategy to automatically synthesize complex, difficult, and hard-to-find questions from open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement learning (RL) to enhance LLMsâ€™ long-horizon reasoning with deep search. Experiments show that DeepDive-32B achieves a new open-source competitive result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and Search-o1. We demonstrate that multi-turn RL training improves deep search ability and significantly contributes to the performance improvements across multiple benchmarks. We observe that DeepDive enables test-time scaling of tool calls and parallel sampling. All datasets, models, and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/THUDM/DeepDive">https://github.com/THUDM/DeepDive</a>. </p>
<blockquote>
<p>é€šè¿‡æµè§ˆå·¥å…·å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ï¼Œèƒ½æ˜¾è‘—æé«˜å…¶ä½œä¸ºæ·±åº¦æœç´¢ä»£ç†è§£å†³å¤æ‚ç°å®ä»»åŠ¡çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œåœ¨å¤æ‚åœºæ™¯ä¸‹ï¼Œå¼€æ”¾LLMè¡¨ç°ä»ç„¶æ¬ ä½³ï¼Œå…¶åŸå› åœ¨äºå…¶æœ‰é™çš„é•¿æœŸè§„åˆ’æ¨ç†èƒ½åŠ›ä¸æµè§ˆå·¥å…·ç›¸ç»“åˆçš„èƒ½åŠ›ä»¥åŠç¼ºä¹è¶³å¤Ÿå›°éš¾çš„ç›‘ç£æ•°æ®ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºDeepDiveæ¥æ¨åŠ¨æ·±åº¦æœç´¢ä»£ç†çš„è¿›æ­¥ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç­–ç•¥ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ä»å¼€æ”¾çŸ¥è¯†å›¾è°±ä¸­åˆæˆå¤æ‚ã€å›°éš¾ä¸”éš¾ä»¥æ‰¾åˆ°çš„é—®é¢˜ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åº”ç”¨ç«¯åˆ°ç«¯çš„å¤šè½®å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æé«˜LLMä¸æ·±åº¦æœç´¢çš„é•¿æœŸè§„åˆ’æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒDeepDive-32Båœ¨BrowseCompä¸Šå–å¾—äº†å¼€æºç«äº‰çš„æ–°æˆæœï¼Œè¶…è¶Šäº†WebSailorã€DeepSeek-R1-Browseå’ŒSearch-o1ã€‚æˆ‘ä»¬è¯æ˜äº†å¤šè½®RLè®­ç»ƒèƒ½æé«˜æ·±åº¦æœç´¢èƒ½åŠ›ï¼Œå¹¶ä¸ºå¤šä¸ªåŸºå‡†æµ‹è¯•çš„æ€§èƒ½æå‡åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°DeepDiveèƒ½å¤Ÿå®ç°æµ‹è¯•æ—¶çš„å·¥å…·è°ƒç”¨æ‰©å±•å’Œå¹¶è¡Œé‡‡æ ·ã€‚æ‰€æœ‰æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/THUDM/DeepDive%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/THUDM/DeepDiveå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10446v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æµè§ˆå·¥å…·è¿›è¡Œå¢å¼ºï¼Œå¯æ˜¾è‘—æé«˜ä½œä¸ºæ·±åº¦æœç´¢ä»£ç†è§£å†³å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¼€æºLLMåœ¨æ­¤ç±»ç¯å¢ƒä¸­è¡¨ç°ä»ç„¶ä¸ä½³ï¼Œé¢ä¸´é•¿æœŸæ¨ç†èƒ½åŠ›æœ‰é™ä»¥åŠä¸æµè§ˆå·¥å…·çš„ç»“åˆé—®é¢˜ï¼Œä»¥åŠç¼ºä¹è¶³å¤Ÿå›°éš¾çš„ç›‘ç£æ•°æ®ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºDeepDiveæ¨è¿›æ·±åº¦æœç´¢ä»£ç†çš„å‘å±•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ç­–ç•¥ï¼Œä»å¼€æ”¾çŸ¥è¯†å›¾è°±ä¸­è‡ªåŠ¨åˆæˆå¤æ‚ã€å›°éš¾ä¸”éš¾ä»¥æ‰¾åˆ°çš„é—®é¢˜ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é‡‡ç”¨ç«¯åˆ°ç«¯çš„å¤šè½®å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æé«˜LLMçš„é•¿æœŸæ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒDeepDive-32Båœ¨BrowseCompä¸Šå–å¾—äº†å¼€æºç«äº‰çš„æ–°æˆæœï¼Œè¶…è¶Šäº†WebSailorã€DeepSeek-R1-Browseå’ŒSearch-o1ã€‚æˆ‘ä»¬è¯æ˜äº†å¤šè½®RLè®­ç»ƒæé«˜äº†æ·±åº¦æœç´¢èƒ½åŠ›ï¼Œå¯¹å¤šä¸ªåŸºå‡†æµ‹è¯•çš„æ€§èƒ½æå‡åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚DeepDiveè¿˜æ”¯æŒæµ‹è¯•æ—¶å·¥å…·è°ƒç”¨çš„æ‰©å±•å’Œå¹¶è¡Œé‡‡æ ·ã€‚æ‰€æœ‰æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/THUDM/DeepDive%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/THUDM/DeepDiveå…¬å¼€è·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æµè§ˆå·¥å…·ç»“åˆå¯ä»¥æ˜¾è‘—æé«˜è§£å†³å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>å¼€æºLLMåœ¨æ·±åº¦æœç´¢ç¯å¢ƒä¸­é¢ä¸´é•¿æœŸæ¨ç†èƒ½åŠ›æœ‰é™ã€ä¸æµè§ˆå·¥å…·ç»“åˆé—®é¢˜ä»¥åŠç¼ºä¹è¶³å¤Ÿçš„ç›‘ç£æ•°æ®ç­‰æŒ‘æˆ˜ã€‚</li>
<li>DeepDiveé€šè¿‡è‡ªåŠ¨åˆæˆå¤æ‚é—®é¢˜å’Œä½¿ç”¨ç«¯åˆ°ç«¯çš„å¤šè½®å¼ºåŒ–å­¦ä¹ æ¥æé«˜LLMçš„æ€§èƒ½ã€‚</li>
<li>DeepDive-32Båœ¨BrowseCompä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œè¶…è¶Šäº†å…¶ä»–å¼€æºæ¨¡å‹ã€‚</li>
<li>å¤šè½®å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¯¹æå‡LLMçš„æ·±åº¦æœç´¢èƒ½åŠ›å’Œæ€§èƒ½è‡³å…³é‡è¦ã€‚</li>
<li>DeepDiveæ”¯æŒæµ‹è¯•æ—¶å·¥å…·è°ƒç”¨çš„æ‰©å±•å’Œå¹¶è¡Œé‡‡æ ·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10446v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10446v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10446v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10446v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10446v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RefactorCoderQA-Benchmarking-LLMs-for-Multi-Domain-Coding-Question-Solutions-in-Cloud-and-Edge-Deployment"><a href="#RefactorCoderQA-Benchmarking-LLMs-for-Multi-Domain-Coding-Question-Solutions-in-Cloud-and-Edge-Deployment" class="headerlink" title="RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question   Solutions in Cloud and Edge Deployment"></a>RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question   Solutions in Cloud and Edge Deployment</h2><p><strong>Authors:Shadikur Rahman, Aroosa Hameed, Gautam Srivastava, Syed Muhammad Danish</strong></p>
<p>To optimize the reasoning and problem-solving capabilities of Large Language Models (LLMs), we propose a novel cloud-edge collaborative architecture that enables a structured, multi-agent prompting framework. This framework comprises three specialized components: GuideLLM, a lightweight model deployed at the edge to provide methodological guidance; SolverLLM, a more powerful model hosted in the cloud responsible for generating code solutions; and JudgeLLM, an automated evaluator for assessing solution correctness and quality. To evaluate and demonstrate the effectiveness of this architecture in realistic settings, we introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate and enhance the performance of Large Language Models (LLMs) across multi-domain coding tasks. Motivated by the limitations of existing benchmarks, RefactorCoderQA systematically covers various technical domains, including Software Engineering, Data Science, Machine Learning, and Natural Language Processing, using authentic coding challenges from Stack Overflow. Extensive experiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves state-of-the-art performance, significantly outperforming leading open-source and commercial baselines with an overall accuracy of 76.84%. Human evaluations further validate the interpretability, accuracy, and practical relevance of the generated solutions. In addition, we evaluate system-level metrics, such as throughput and latency, to gain deeper insights into the performance characteristics and trade-offs of the proposed architecture. </p>
<blockquote>
<p>ä¸ºäº†ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹äº‘è¾¹ååŒæ¶æ„ï¼Œè¯¥æ¶æ„å¯å®ç°ç»“æ„åŒ–ã€å¤šä»£ç†æç¤ºæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªä¸“ä¸šç»„ä»¶ï¼šGuideLLMï¼Œä¸€ä¸ªè½»é‡çº§æ¨¡å‹ï¼Œéƒ¨ç½²åœ¨è¾¹ç¼˜ä»¥æä¾›æ–¹æ³•è®ºæŒ‡å¯¼ï¼›SolverLLMï¼Œä¸€ä¸ªæ›´å¼ºå¤§çš„æ¨¡å‹ï¼Œæ‰˜ç®¡åœ¨äº‘ä¸­ï¼Œè´Ÿè´£ç”Ÿæˆä»£ç è§£å†³æ–¹æ¡ˆï¼›ä»¥åŠJudgeLLMï¼Œä¸€ä¸ªè‡ªåŠ¨åŒ–è¯„ä¼°å™¨ï¼Œç”¨äºè¯„ä¼°è§£å†³æ–¹æ¡ˆçš„æ­£ç¡®æ€§å’Œè´¨é‡ã€‚ä¸ºäº†è¯„ä¼°è¿™ç§æ¶æ„åœ¨çœŸå®ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†RefactorCoderQAï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å’Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šé¢†åŸŸç¼–ç ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚å—åˆ°ç°æœ‰åŸºå‡†æµ‹è¯•çš„å±€é™æ€§çš„å¯å‘ï¼ŒRefactorCoderQAç³»ç»Ÿåœ°æ¶µç›–äº†å„ç§æŠ€æœ¯é¢†åŸŸï¼ŒåŒ…æ‹¬è½¯ä»¶å·¥ç¨‹ã€æ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼Œå¹¶ä½¿ç”¨æ¥è‡ªStack Overflowçš„çœŸå®ç¼–ç æŒ‘æˆ˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬å¾®è°ƒè¿‡çš„æ¨¡å‹RefactorCoder-MoEè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºé¢†å…ˆçš„å¼€æºå’Œå•†ä¸šåŸºå‡†æµ‹è¯•ï¼Œæ€»ä½“å‡†ç¡®åº¦ä¸º76.84%ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†æ‰€ç”Ÿæˆè§£å†³æ–¹æ¡ˆçš„å¯è§£é‡Šæ€§ã€å‡†ç¡®æ€§å’Œå®ç”¨æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†ç³»ç»Ÿçº§æŒ‡æ ‡ï¼Œå¦‚ååé‡å’Œå»¶è¿Ÿï¼Œä»¥æ›´æ·±å…¥åœ°äº†è§£æ‰€æå‡ºæ¶æ„çš„æ€§èƒ½ç‰¹å¾å’Œæƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10436v1">PDF</a> 12 pages, 5 figures, submitted to IEEE Transactions on Services   Computing</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ‘˜è¦æå‡ºä¸€ç§æ–°å‹çš„äº‘è¾¹ç¼˜ååŒæ¶æ„ï¼Œç”¨äºä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚æ¶æ„åŒ…å«ä¸€ä¸ªç»“æ„åŒ–ã€å¤šä»£ç†æç¤ºæ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªä¸“ä¸šç»„ä»¶ï¼šGuideLLMï¼ˆè¾¹ç¼˜éƒ¨ç½²çš„è½»é‡çº§æ¨¡å‹ï¼Œæä¾›æ–¹æ³•è®ºæŒ‡å¯¼ï¼‰ã€SolverLLMï¼ˆäº‘ä¸­æ‰˜ç®¡çš„é«˜æ€§èƒ½æ¨¡å‹ï¼Œè´Ÿè´£ç”Ÿæˆä»£ç è§£å†³æ–¹æ¡ˆï¼‰å’ŒJudgeLLMï¼ˆè¯„ä¼°è§£å†³æ–¹æ¡ˆæ­£ç¡®æ€§å’Œè´¨é‡çš„è‡ªåŠ¨åŒ–è¯„ä¼°å™¨ï¼‰ã€‚ä¸ºè¯„ä¼°è¯¥æ¶æ„åœ¨çœŸå®ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†RefactorCoderQAåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°å’Œå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šé¢†åŸŸç¼–ç ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚è¯¥åŸºå‡†æµ‹è¯•æ¶µç›–äº†è½¯ä»¶å·¥ç¨‹ã€æ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰å¤šä¸ªæŠ€æœ¯é¢†åŸŸï¼Œä½¿ç”¨æ¥è‡ªStack Overflowçš„çœŸå®ç¼–ç¨‹æŒ‘æˆ˜ã€‚å®éªŒè¡¨æ˜ï¼Œç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹RefactorCoder-MoEè¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–å¼€æºå’Œå•†ä¸šåŸºå‡†æµ‹è¯•ï¼Œæ€»ä½“å‡†ç¡®åº¦ä¸º76.84%ã€‚äººç±»è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†ç”Ÿæˆè§£å†³æ–¹æ¡ˆçš„å¯è§£é‡Šæ€§ã€å‡†ç¡®æ€§å’Œå®ç”¨æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†ç³»ç»Ÿçº§çš„ååé‡å»¶è¿Ÿç­‰æŒ‡æ ‡ï¼Œä»¥æ·±å…¥äº†è§£è¯¥æ¶æ„çš„æ€§èƒ½ç‰¹æ€§å’Œæƒè¡¡å–èˆã€‚è¯¥æ¶æ„çš„åˆ›æ–°ç‚¹åœ¨äºç»“åˆäº†äº‘å’Œè¾¹ç¼˜çš„è®¡ç®—èµ„æºï¼Œä¼˜åŒ–äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„åä½œæ•ˆç‡ï¼Œä»è€Œä¸ºAIç¼–ç¨‹é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•æ‰“ä¸‹åŸºç¡€ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„äº‘è¾¹ç¼˜ååŒæ¶æ„ï¼Œæ—¨åœ¨ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚</li>
<li>æ¶æ„åŒ…å«ä¸‰ä¸ªä¸“ä¸šç»„ä»¶ï¼šGuideLLMã€SolverLLMå’ŒJudgeLLMï¼Œåˆ†åˆ«è´Ÿè´£æä¾›æ–¹æ³•è®ºæŒ‡å¯¼ã€ç”Ÿæˆä»£ç è§£å†³æ–¹æ¡ˆå’Œè¯„ä¼°è§£å†³æ–¹æ¡ˆè´¨é‡ã€‚</li>
<li>å¼•å…¥äº†RefactorCoderQAåŸºå‡†æµ‹è¯•ï¼Œä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šé¢†åŸŸç¼–ç ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºï¼Œç»è¿‡å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹RefactorCoder-MoEè¾¾åˆ°ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼Œæ€»ä½“å‡†ç¡®åº¦ä¸º76.84%ã€‚</li>
<li>äººç±»è¯„ä¼°éªŒè¯äº†ç”Ÿæˆè§£å†³æ–¹æ¡ˆçš„å¯è§£é‡Šæ€§ã€å‡†ç¡®æ€§å’Œå®ç”¨æ€§ã€‚</li>
<li>è¯„ä¼°äº†ç³»ç»Ÿçº§çš„ååé‡å»¶è¿Ÿç­‰æŒ‡æ ‡ï¼Œä»¥äº†è§£æ¶æ„çš„æ€§èƒ½ç‰¹æ€§å’Œæƒè¡¡å–èˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10436">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10436v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10436v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10436v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10436v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10436v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10436v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10436v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Inpainting-Guided-Policy-Optimization-for-Diffusion-Large-Language-Models"><a href="#Inpainting-Guided-Policy-Optimization-for-Diffusion-Large-Language-Models" class="headerlink" title="Inpainting-Guided Policy Optimization for Diffusion Large Language   Models"></a>Inpainting-Guided Policy Optimization for Diffusion Large Language   Models</h2><p><strong>Authors:Siyan Zhao, Mengchen Liu, Jing Huang, Miao Liu, Chenyu Wang, Bo Liu, Yuandong Tian, Guan Pang, Sean Bell, Aditya Grover, Feiyu Chen</strong></p>
<p>Masked diffusion large language models (dLLMs) are emerging as promising alternatives to autoregressive LLMs, offering competitive performance while supporting unique generation capabilities such as inpainting. We explore how inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with reinforcement learning faces an exploration challenge: sparse reward signals and sample waste when models fail to discover correct solutions. While this inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunityâ€“their inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided Policy Optimization), an RL framework that strategically inserts partial ground-truth reasoning traces during online sampling. Unlike providing full solutions, inpainting steers exploration toward promising trajectory spaces while preserving self-generated reasoning, bridging supervised fine-tuning and reinforcement learning. We apply IGPO to group-based optimization methods such as GRPO, where exploration failures cause zero advantages and gradients. IGPO restores meaningful gradients while improving sample efficiency. We also propose supervised fine-tuning on synthetically rewritten concise traces that better align with dLLM generation patterns. With additional techniques including entropy-based filtering, our training recipe yields substantial gains across three mathematical benchmarksâ€“GSM8K, Math500, and AMCâ€“achieving new state-of-the-art results for full-attention masked dLLMs. </p>
<blockquote>
<p>æ©ç›–æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä½œä¸ºè‡ªåŠ¨å›å½’LLMsçš„æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆæ­£å´­éœ²å¤´è§’ï¼Œå®ƒä»¬åœ¨æä¾›ç«äº‰åŠ›æ€§èƒ½çš„åŒæ—¶ï¼Œè¿˜æ”¯æŒç‹¬ç‰¹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¦‚æ’ç”»ã€‚æˆ‘ä»¬æ¢è®¨äº†æ’ç”»å¦‚ä½•ä¸ºdLLMsçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•è®¾è®¡æä¾›ä¿¡æ¯ã€‚LLMsä¸å¼ºåŒ–å­¦ä¹ å¯¹é½é¢ä¸´ç€æ¢ç´¢æŒ‘æˆ˜ï¼šç¨€ç–çš„å¥–åŠ±ä¿¡å·å’Œæ¨¡å‹æœªèƒ½å‘ç°æ­£ç¡®è§£å†³æ–¹æ¡ˆæ—¶çš„æ ·æœ¬æµªè´¹ã€‚è™½ç„¶è¿™ç§ä½æ•ˆæ€§å¹¿æ³›å½±å“äº†LLMsï¼Œä½†dLLMsæä¾›äº†ä¸€ä¸ªç‹¬ç‰¹çš„æœºä¼šâ€”â€”å®ƒä»¬çš„æ’ç”»èƒ½åŠ›å¯ä»¥å¼•å¯¼æ¢ç´¢ã€‚æˆ‘ä»¬ä»‹ç»äº†IGPOï¼ˆæ’ç”»å¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨åœ¨çº¿é‡‡æ ·è¿‡ç¨‹ä¸­æˆ˜ç•¥æ€§åœ°æ’å…¥éƒ¨åˆ†çœŸå®æ¨ç†è½¨è¿¹ã€‚ä¸æä¾›å®Œæ•´è§£å†³æ–¹æ¡ˆä¸åŒï¼Œæ’ç”»å¼•å¯¼æ¢ç´¢æœç€æœ‰å¸Œæœ›çš„è½¨è¿¹ç©ºé—´è¿›è¡Œï¼ŒåŒæ—¶ä¿ç•™è‡ªæˆ‘ç”Ÿæˆçš„æ¨ç†ï¼Œå¼¥åˆäº†ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬å°†IGPOåº”ç”¨äºåŸºäºç»„çš„ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚GRPOï¼Œå…¶ä¸­æ¢ç´¢å¤±è´¥å¯¼è‡´é›¶ä¼˜åŠ¿å’Œæ¢¯åº¦ã€‚IGPOæ¢å¤æœ‰æ„ä¹‰çš„æ¢¯åº¦ï¼ŒåŒæ—¶æé«˜æ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åœ¨åˆæˆé‡å†™çš„ç®€æ´è½¨è¿¹ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œè¿™äº›è½¨è¿¹æ›´å¥½åœ°ç¬¦åˆdLLMç”Ÿæˆæ¨¡å¼ã€‚é€šè¿‡åŒ…æ‹¬åŸºäºç†µçš„è¿‡æ»¤ç­‰å…¶ä»–æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„è®­ç»ƒé…æ–¹åœ¨ä¸‰ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ï¼ˆGSM8Kã€Math500å’ŒAMCï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºå…¨æ³¨æ„åŠ›æ©ç›–dLLMsè¾¾åˆ°äº†æ–°çš„æœ€å…ˆè¿›çš„æˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10396v1">PDF</a> preprint; 21 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ‰©æ•£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä½œä¸ºè‡ªå›å½’LLMsçš„æ›¿ä»£æ–¹æ¡ˆå±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œæ”¯æŒç‹¬ç‰¹çš„ç”Ÿæˆèƒ½åŠ›å¦‚è¡¥å…¨ç»˜ç”»ã€‚æœ¬æ–‡æ¢è®¨äº†è¡¥å…¨ç»˜ç”»å¦‚ä½•ä¸ºdLLMsçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•è®¾è®¡æä¾›ä¿¡æ¯ã€‚LLMsä¸å¼ºåŒ–å­¦ä¹ å¯¹é½é¢ä¸´æ¢ç´¢æŒ‘æˆ˜ï¼šç¨€ç–å¥–åŠ±ä¿¡å·å’Œæ¨¡å‹æœªèƒ½å‘ç°æ­£ç¡®è§£å†³æ–¹æ¡ˆæ—¶çš„æ ·æœ¬æµªè´¹ã€‚dLLMsæä¾›äº†ä¸€ä¸ªç‹¬ç‰¹çš„æœºä¼šâ€”â€”å®ƒä»¬çš„è¡¥å…¨ç»˜ç”»èƒ½åŠ›å¯ä»¥å¼•å¯¼æ¢ç´¢ã€‚æœ¬æ–‡å¼•å…¥äº†IGPOï¼ˆè¡¥å…¨ç»˜ç”»å¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼‰è¿™ä¸€RLæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨åœ¨çº¿é‡‡æ ·è¿‡ç¨‹ä¸­æˆ˜ç•¥æ€§åœ°æ’å…¥éƒ¨åˆ†çœŸå®æ¨ç†è½¨è¿¹ã€‚ä¸åŒäºæä¾›å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œè¡¥å…¨ç»˜ç”»å¼•å¯¼æ¢ç´¢èµ°å‘æœ‰å‰æ™¯çš„è½¨è¿¹ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™è‡ªæˆ‘ç”Ÿæˆçš„æ¨ç†ï¼Œæ¡¥æ¥ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Masked diffusion large language models (dLLMs)å±•ç°å‡ºå¯¹è‡ªå›å½’LLMsçš„æ½œåŠ›ï¼Œå¹¶æ”¯æŒç‹¬ç‰¹çš„ç”Ÿæˆèƒ½åŠ›å¦‚è¡¥å…¨ç»˜ç”»ã€‚</li>
<li>LLMsä¸å¼ºåŒ–å­¦ä¹ å¯¹é½å­˜åœ¨æ¢ç´¢æŒ‘æˆ˜ï¼Œå¦‚ç¨€ç–å¥–åŠ±ä¿¡å·å’Œæ ·æœ¬æµªè´¹é—®é¢˜ã€‚</li>
<li>dLLMsçš„è¡¥å…¨ç»˜ç”»èƒ½åŠ›å¯ä»¥ä½œä¸ºä¸€ç§å¼•å¯¼æ¢ç´¢çš„ç­–ç•¥ã€‚</li>
<li>å¼•å…¥IGPOï¼ˆè¡¥å…¨ç»˜ç”»å¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ’å…¥éƒ¨åˆ†çœŸå®æ¨ç†è½¨è¿¹æ¥å¼•å¯¼æ¨¡å‹æ¢ç´¢ã€‚</li>
<li>IGPOèƒ½æé«˜æ ·æœ¬æ•ˆç‡ï¼Œå¹¶åº”ç”¨äºåŸºäºç»„çš„ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚GRPOã€‚</li>
<li>é€šè¿‡å¯¹åˆæˆé‡å†™ç®€æ´è½¨è¿¹çš„ç›‘ç£å¾®è°ƒï¼Œä¸dLLMç”Ÿæˆæ¨¡å¼æ›´å¯¹é½ï¼Œç»“åˆç†µè¿‡æ»¤ç­‰æŠ€æœ¯ï¼Œèƒ½æé«˜æ¨¡å‹åœ¨ä¸‰ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10396v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10396v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MCBP-A-Memory-Compute-Efficient-LLM-Inference-Accelerator-Leveraging-Bit-Slice-enabled-Sparsity-and-Repetitiveness"><a href="#MCBP-A-Memory-Compute-Efficient-LLM-Inference-Accelerator-Leveraging-Bit-Slice-enabled-Sparsity-and-Repetitiveness" class="headerlink" title="MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging   Bit-Slice-enabled Sparsity and Repetitiveness"></a>MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging   Bit-Slice-enabled Sparsity and Repetitiveness</h2><p><strong>Authors:Huizheng Wang, Zichuan Wang, Zhiheng Yue, Yousheng Long, Taiquan Wei, Jianxun Yang, Yang Wang, Chao Li, Shaojun Wei, Yang Hu, Shouyi Yin</strong></p>
<p>Large language models (LLMs) face significant inference latency due to inefficiencies in GEMM operations, weight access, and KV cache access, especially in real-time scenarios. This highlights the need for a versatile compute-memory efficient accelerator. Unfortunately, existing Transformer accelerators struggle to address both aspects simultaneously, as they focus on value-level processing, missing fine-grained opportunities to optimize computation and memory collaboratively. This paper introduces MCBP, a bit-grained compute-memory efficient algorithm-hardware co-design that leverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM inference. MCBP features three key innovations: 1) BS-repetitiveness-enabled computation reduction (BRCR), which eliminates redundant GEMM computations via leveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state coding (BSTC), which reduces weight access via exploiting significant sparsity in high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP), which reduces KV cache access by leveraging early-termination-based bit-grained prediction. These techniques, supported by custom accelerator designs, effectively alleviate the burden in GEMM, weight access, and KV cache access. Extensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up and 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA Transformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than Spatten, FACT and SOFA, respectively. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿›è¡Œé€šç”¨çŸ©é˜µçŸ©é˜µä¹˜æ³•æ“ä½œï¼ˆGEMMï¼‰ã€æƒé‡è®¿é—®å’ŒKVç¼“å­˜è®¿é—®æ—¶å­˜åœ¨æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå› æ­¤åœ¨å®æ—¶åœºæ™¯ä¸‹é¢ä¸´ç€æ¨ç†å»¶è¿Ÿçš„å›°æ‰°ã€‚è¿™å‡¸æ˜¾äº†å¯¹ä¸€ä¸ªé€šç”¨ä¸”è®¡ç®—å†…å­˜é«˜æ•ˆçš„åŠ é€Ÿå™¨çš„éœ€æ±‚ã€‚ç„¶è€Œï¼Œç°æœ‰çš„TransformeråŠ é€Ÿå™¨å¾ˆéš¾åŒæ—¶è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬ä¸“æ³¨äºå€¼çº§å¤„ç†ï¼Œå¿½ç•¥äº†è®¡ç®—å’Œå†…å­˜ååŒä¼˜åŒ–çš„ç²¾ç»†æœºä¼šã€‚æœ¬æ–‡ä»‹ç»äº†MCBPï¼Œè¿™æ˜¯ä¸€ç§ä½ç²¾ç»†è®¡ç®—å†…å­˜é«˜æ•ˆç®—æ³•ç¡¬ä»¶ååŒè®¾è®¡ã€‚MCBPåˆ©ç”¨ä½åˆ‡ç‰‡ï¼ˆBSï¼‰çš„é‡å¤æ€§å’Œç¨€ç–æ€§æ¥åŠ é€ŸLLMæ¨ç†ã€‚MCBPæœ‰ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼š1ï¼‰BSé‡å¤æ€§è®¡ç®—å‡å°‘ï¼ˆBRCRï¼‰ï¼Œå®ƒé€šè¿‡åˆ©ç”¨BSå‘é‡ä¹‹é—´çš„å†—ä½™æ€§æ¥æ¶ˆé™¤å†—ä½™çš„GEMMè®¡ç®—ï¼›2ï¼‰BSç¨€ç–æ€§ä½¿èƒ½çš„ä¸¤æ€ç¼–ç ï¼ˆBSTCï¼‰ï¼Œå®ƒé€šè¿‡åˆ©ç”¨é«˜ä½ä½åˆ‡ç‰‡æƒé‡çš„æ˜¾è‘—ç¨€ç–æ€§æ¥å‡å°‘æƒé‡è®¿é—®ï¼›3ï¼‰ä½ç²¾ç»†æ¸è¿›é¢„æµ‹ï¼ˆBGPPï¼‰ï¼Œå®ƒé€šè¿‡åŸºäºæ—©æœŸç»ˆæ­¢çš„ä½ç²¾ç»†é¢„æµ‹æ¥å‡å°‘KVç¼“å­˜è®¿é—®ã€‚è¿™äº›æŠ€æœ¯è¾…ä»¥å®šåˆ¶çš„åŠ é€Ÿå™¨è®¾è®¡ï¼Œæœ‰æ•ˆåœ°å‡è½»äº†GEMMã€æƒé‡è®¿é—®å’ŒKVç¼“å­˜è®¿é—®çš„è´Ÿæ‹…ã€‚åœ¨26ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMCBPä¸Nvidia A100 GPUç›¸æ¯”ï¼Œå®ç°äº†9.43å€çš„é€Ÿåº¦æå‡å’Œ31.1å€çš„èƒ½æ•ˆæå‡ã€‚ä¸å…ˆè¿›çš„TransformeråŠ é€Ÿå™¨ç›¸æ¯”ï¼ŒMCBPä¸Spattenã€FACTå’ŒSOFAç›¸æ¯”ï¼Œåˆ†åˆ«å®ç°äº†35å€ã€5.2å€å’Œ3.2å€çš„èŠ‚èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10372v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨å»¶è¿Ÿé—®é¢˜ï¼Œä¸»è¦ç”±äºçŸ©é˜µä¹˜æ³•ã€æƒé‡è®¿é—®å’Œé”®å€¼ç¼“å­˜è®¿é—®çš„æ•ˆç‡ä½ä¸‹ã€‚ç°æœ‰TransformeråŠ é€Ÿå™¨éš¾ä»¥åŒæ—¶è§£å†³è¿™ä¸¤æ–¹é¢çš„é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬ä¾§é‡äºä»·å€¼å±‚é¢çš„å¤„ç†ï¼Œå¿½ç•¥äº†è®¡ç®—å’Œå†…å­˜ååŒä¼˜åŒ–çš„ç²¾ç»†æœºä¼šã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ä½ç²’è®¡ç®—çš„åŠ é€Ÿå™¨è®¾è®¡MCBPï¼Œå®ƒé€šè¿‡åˆ©ç”¨ä½åˆ‡ç‰‡ï¼ˆBSï¼‰çš„é‡å¤æ€§å’Œç¨€ç–æ€§æ¥åŠ é€ŸLLMæ¨ç†ã€‚MCBPåŒ…æ‹¬ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šè®¡ç®—å‡å°‘ã€ä¸¤æ€ç¼–ç å’Œæ¸è¿›é¢„æµ‹ã€‚å®ƒåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†é€Ÿåº¦å’Œèƒ½æ•ˆçš„å¤§å¹…æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMé¢ä¸´æ¨ç†å»¶è¿Ÿé—®é¢˜ï¼Œä¸»è¦æºäºçŸ©é˜µä¹˜æ³•ã€æƒé‡è®¿é—®å’Œé”®å€¼ç¼“å­˜è®¿é—®çš„æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>ç°æœ‰TransformeråŠ é€Ÿå™¨éš¾ä»¥åœ¨è®¡ç®—å’Œå†…å­˜æ•ˆç‡ä¸¤æ–¹é¢åŒæ—¶è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>MCBPæ˜¯ä¸€ç§ä½ç²’è®¡ç®—åŠ é€Ÿå™¨è®¾è®¡ï¼Œé€šè¿‡åˆ©ç”¨ä½åˆ‡ç‰‡çš„é‡å¤æ€§å’Œç¨€ç–æ€§æ¥åŠ é€ŸLLMæ¨ç†ã€‚</li>
<li>MCBPåŒ…æ‹¬ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šè®¡ç®—å‡å°‘ï¼ˆBRCRï¼‰ã€ä¸¤æ€ç¼–ç ï¼ˆBSTCï¼‰å’Œæ¸è¿›é¢„æµ‹ï¼ˆBGPPï¼‰ã€‚</li>
<li>MCBPåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡å’Œèƒ½æ•ˆèŠ‚çº¦ï¼Œç›¸æ¯”Nvidia A100 GPUï¼Œé€Ÿåº¦æå‡9.43å€ï¼Œèƒ½æ•ˆæé«˜31.1å€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10372">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10372v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10372v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10372v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10372v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10372v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10372v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10372v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10372v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Characterizing-the-Efficiency-of-Distributed-Training-A-Power-Performance-and-Thermal-Perspective"><a href="#Characterizing-the-Efficiency-of-Distributed-Training-A-Power-Performance-and-Thermal-Perspective" class="headerlink" title="Characterizing the Efficiency of Distributed Training: A Power,   Performance, and Thermal Perspective"></a>Characterizing the Efficiency of Distributed Training: A Power,   Performance, and Thermal Perspective</h2><p><strong>Authors:Seokjin Go, Joongun Park, Spandan More, Hanjiang Wu, Irene Wang, Aaron Jezghani, Tushar Krishna, Divya Mahajan</strong></p>
<p>The rapid scaling of Large Language Models (LLMs) has pushed training workloads far beyond the limits of single-node analysis, demanding a deeper understanding of how these models behave across large-scale, multi-GPU systems. In this paper, we present a comprehensive characterization of LLM training across diverse real-world workloads and hardware platforms, including NVIDIA H100&#x2F;H200 and AMD MI250 GPUs. We analyze dense and sparse models under various parallelism strategies â€“ tensor, pipeline, data, and expert â€“ and evaluate their effects on hardware utilization, power consumption, and thermal behavior. We further evaluate the effectiveness of optimizations such as activation recomputation and compute-communication overlap. Our findings show that performance is not determined solely by scaling hardware capacity. Scale-up systems with fewer, higher-memory GPUs can outperform scale-out systems in communication-bound regimes, but only under carefully tuned configurations; in other cases, scale-out deployments achieve superior throughput. We also show that certain parallelism combinations, such as tensor with pipeline, lead to bandwidth underutilization due to inefficient data chunking, while increasing microbatch sizes beyond a certain point induces bursty execution and peak power excursions that worsen thermal throttling. These insights reveal how training performance is shaped by complex interactions between hardware, system topology, and model execution. We conclude by offering recommendations for system and hardware design to improve the scalability and reliability of future LLM systems and workloads. The source code of this project is available at <a target="_blank" rel="noopener" href="https://github.com/sitar-lab/CharLLM-PPT">https://github.com/sitar-lab/CharLLM-PPT</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•å°†è®­ç»ƒå·¥ä½œé‡æ¨å‘äº†å•ä¸ªèŠ‚ç‚¹åˆ†æçš„æé™ä¹‹å¤–ï¼Œè¿™éœ€è¦æ›´æ·±å…¥åœ°äº†è§£è¿™äº›æ¨¡å‹åœ¨å¤§å‹å¤šGPUç³»ç»Ÿä¸Šçš„è¡Œä¸ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨å„ç§å®é™…å·¥ä½œè´Ÿè½½å’Œç¡¬ä»¶å¹³å°ï¼ˆåŒ…æ‹¬NVIDIA H100&#x2F;H200å’ŒAMD MI250 GPUï¼‰ä¸Šå…¨é¢æè¿°äº†LLMè®­ç»ƒçš„ç‰¹æ€§ã€‚æˆ‘ä»¬åˆ†æäº†å¯†é›†æ¨¡å‹å’Œç¨€ç–æ¨¡å‹åœ¨å„ç§å¹¶è¡Œç­–ç•¥ï¼ˆå¼ é‡ã€ç®¡é“ã€æ•°æ®å’Œä¸“å®¶ï¼‰ä¸‹çš„è¡¨ç°ï¼Œå¹¶è¯„ä¼°äº†å®ƒä»¬å¯¹ç¡¬ä»¶åˆ©ç”¨ç‡ã€åŠŸè€—å’Œçƒ­åŠ›è¡Œä¸ºçš„å½±å“ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯„ä¼°äº†æ¿€æ´»é‡æ–°è®¡ç®—å’Œè®¡ç®—é€šä¿¡é‡å ç­‰ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæ€§èƒ½å¹¶ä¸å®Œå…¨å–å†³äºç¡¬ä»¶å®¹é‡çš„æ‰©å±•ã€‚åœ¨é€šä¿¡å—é™çš„çŠ¶æ€ä¸‹ï¼Œé‡‡ç”¨è¾ƒå°‘ã€å†…å­˜è¾ƒå¤§çš„æ‰©å±•ç³»ç»Ÿå¯èƒ½ä¼šä¼˜äºæ¨ªå‘æ‰©å±•ç³»ç»Ÿï¼Œä½†è¿™ä»…å‘ç”Ÿåœ¨ç»è¿‡ç²¾å¿ƒè°ƒæ•´çš„é…ç½®ä¸‹ï¼›åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œæ¨ªå‘æ‰©å±•éƒ¨ç½²å¯å®ç°æ›´é«˜çš„ååé‡ã€‚æˆ‘ä»¬è¿˜è¡¨æ˜ï¼ŒæŸäº›å¹¶è¡Œç»„åˆï¼ˆå¦‚å¼ é‡ä¸ç®¡é“ï¼‰ä¼šå¯¼è‡´å¸¦å®½åˆ©ç”¨ç‡ä¸è¶³ï¼Œè¿™æ˜¯ç”±äºæ•°æ®å—å¤„ç†æ•ˆç‡ä½ä¸‹é€ æˆçš„ï¼Œè€Œå¾®æ‰¹æ¬¡å¤§å°å¢åŠ åˆ°ä¸€å®šç¨‹åº¦ä¼šå¼•å‘çªå‘æ‰§è¡Œå’Œå³°å€¼åŠŸç‡æ³¢åŠ¨ï¼Œä»è€ŒåŠ å‰§çƒ­åŠ›é™åˆ¶ã€‚è¿™äº›è§è§£æ­ç¤ºäº†è®­ç»ƒæ€§èƒ½æ˜¯å¦‚ä½•å—åˆ°ç¡¬ä»¶ã€ç³»ç»Ÿæ‹“æ‰‘å’Œæ¨¡å‹æ‰§è¡Œä¹‹é—´å¤æ‚äº¤äº’çš„å½±å“ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹ç³»ç»Ÿå’Œç¡¬ä»¶è®¾è®¡çš„å»ºè®®ï¼Œä»¥æ”¹å–„æœªæ¥LLMç³»ç»Ÿå’Œå·¥ä½œè´Ÿè½½çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§ã€‚è¯¥é¡¹ç›®çš„æºä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/sitar-lab/CharLLM-PPT%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/sitar-lab/CharLLM-PPTä¸Šæ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10371v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒå·¥ä½œè´Ÿè½½å·²ç»è¶…è¶Šäº†å•ä¸ªèŠ‚ç‚¹çš„åˆ†ææé™ï¼Œéœ€è¦æ›´æ·±å…¥åœ°äº†è§£è¿™äº›æ¨¡å‹åœ¨å¤§å‹å¤šGPUç³»ç»Ÿä¸Šçš„è¡Œä¸ºã€‚æœ¬æ–‡å…¨é¢æè¿°äº†LLMåœ¨ä¸åŒå®é™…å·¥ä½œè´Ÿè½½å’Œç¡¬ä»¶å¹³å°ï¼ˆåŒ…æ‹¬NVIDIA H100&#x2F;H200å’ŒAMD MI250 GPUï¼‰ä¸Šçš„è®­ç»ƒæƒ…å†µã€‚æ–‡ç« åˆ†æäº†å¯†é›†æ¨¡å‹å’Œç¨€ç–æ¨¡å‹åœ¨ä¸åŒå¹¶è¡Œç­–ç•¥ä¸‹çš„è¡¨ç°ï¼Œå¹¶è¯„ä¼°äº†å…¶å¯¹ç¡¬ä»¶åˆ©ç”¨ç‡ã€åŠŸè€—å’Œæ•£çƒ­è¡Œä¸ºçš„å½±å“ã€‚æ­¤å¤–ï¼Œè¿˜è¯„ä¼°äº†æ¿€æ´»é‡æ–°è®¡ç®—å’Œè®¡ç®—é€šä¿¡é‡å ç­‰ä¼˜åŒ–çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶å‘ç°ï¼Œæ€§èƒ½ä¸ä»…å–å†³äºç¡¬ä»¶å®¹é‡çš„æ‰©å±•ï¼Œè¿˜å—åˆ°ç³»ç»Ÿé…ç½®å’Œæ¨¡å‹æ‰§è¡Œæ–¹å¼çš„å¤æ‚äº¤äº’å½±å“ã€‚æŸäº›å¹¶è¡Œç»„åˆå¯èƒ½å¯¼è‡´å¸¦å®½åˆ©ç”¨ä¸è¶³ï¼Œè€Œå¾®æ‰¹æ¬¡å¤§å°è¿‡å¤§åˆ™å¯èƒ½å¯¼è‡´æ‰§è¡Œè¿‡ç¨‹ä¸­çš„å³°å€¼åŠŸç‡æ³¢åŠ¨å’Œè¿‡çƒ­é—®é¢˜ã€‚æœ€åï¼Œæœ¬æ–‡ç»™å‡ºäº†é’ˆå¯¹ç³»ç»Ÿå’Œç¡¬ä»¶è®¾è®¡çš„å»ºè®®ï¼Œä»¥æé«˜æœªæ¥LLMç³»ç»Ÿå’Œå·¥ä½œè´Ÿè½½çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè®­ç»ƒå·²è¶…è¶Šå•èŠ‚ç‚¹åˆ†æèƒ½åŠ›ï¼Œéœ€è¦å¤šGPUç³»ç»Ÿçš„æ·±å…¥ç†è§£ã€‚</li>
<li>åˆ†æäº†ä¸åŒå¹¶è¡Œç­–ç•¥ï¼ˆå¦‚å¼ é‡ã€ç®¡é“ã€æ•°æ®å’Œä¸“å®¶ï¼‰åœ¨å¯†é›†å’Œç¨€ç–æ¨¡å‹ä¸­çš„è¡¨ç°ã€‚</li>
<li>è¯„ä¼°äº†ç¡¬ä»¶åˆ©ç”¨ç‡ã€åŠŸè€—å’Œæ•£çƒ­è¡Œä¸ºå—åˆ°çš„å½±å“ã€‚</li>
<li>æ€§èƒ½å’Œç¡¬ä»¶å®¹é‡æ‰©å±•å—ç³»ç»Ÿé…ç½®å’Œæ¨¡å‹æ‰§è¡Œæ–¹å¼çš„å¤æ‚äº¤äº’å½±å“ã€‚</li>
<li>æŸäº›å¹¶è¡Œç»„åˆå¯èƒ½å¯¼è‡´å¸¦å®½åˆ©ç”¨ä¸è¶³ï¼Œå¾®æ‰¹æ¬¡å¤§å°éœ€åˆç†è°ƒæ•´ã€‚</li>
<li>ä¼˜åŒ–æªæ–½å¦‚æ¿€æ´»é‡æ–°è®¡ç®—å’Œè®¡ç®—é€šä¿¡é‡å çš„æœ‰æ•ˆæ€§å¾—åˆ°è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10371v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10371v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10371v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10371v1/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10371v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10371v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="The-Morality-of-Probability-How-Implicit-Moral-Biases-in-LLMs-May-Shape-the-Future-of-Human-AI-Symbiosis"><a href="#The-Morality-of-Probability-How-Implicit-Moral-Biases-in-LLMs-May-Shape-the-Future-of-Human-AI-Symbiosis" class="headerlink" title="The Morality of Probability: How Implicit Moral Biases in LLMs May Shape   the Future of Human-AI Symbiosis"></a>The Morality of Probability: How Implicit Moral Biases in LLMs May Shape   the Future of Human-AI Symbiosis</h2><p><strong>Authors:Eoin Oâ€™Doherty, Nicole Weinrauch, Andrew Talone, Uri Klempner, Xiaoyuan Yi, Xing Xie, Yi Zeng</strong></p>
<p>Artificial intelligence (AI) is advancing at a pace that raises urgent questions about how to align machine decision-making with human moral values. This working paper investigates how leading AI systems prioritize moral outcomes and what this reveals about the prospects for human-AI symbiosis. We address two central questions: (1) What moral values do state-of-the-art large language models (LLMs) implicitly favour when confronted with dilemmas? (2) How do differences in model architecture, cultural origin, and explainability affect these moral preferences? To explore these questions, we conduct a quantitative experiment with six LLMs, ranking and scoring outcomes across 18 dilemmas representing five moral frameworks. Our findings uncover strikingly consistent value biases. Across all models, Care and Virtue values outcomes were rated most moral, while libertarian choices were consistently penalized. Reasoning-enabled models exhibited greater sensitivity to context and provided richer explanations, whereas non-reasoning models produced more uniform but opaque judgments. This research makes three contributions: (i) Empirically, it delivers a large-scale comparison of moral reasoning across culturally distinct LLMs; (ii) Theoretically, it links probabilistic model behaviour with underlying value encodings; (iii) Practically, it highlights the need for explainability and cultural awareness as critical design principles to guide AI toward a transparent, aligned, and symbiotic future. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å‘å±•é€Ÿåº¦å¼•å‘äº†å…³äºå¦‚ä½•å°†æœºå™¨å†³ç­–ä¸äººç±»é“å¾·ä»·å€¼è§‚ç›¸ä¸€è‡´çš„ç´§è¿«é—®é¢˜ã€‚æœ¬å·¥ä½œè®ºæ–‡æ—¨åœ¨æ¢è®¨é¢†å…ˆçš„AIç³»ç»Ÿæ˜¯å¦‚ä½•ä¼˜å…ˆè€ƒé‡é“å¾·ç»“æœçš„ï¼Œä»¥åŠè¿™æ­ç¤ºäº†äººç±»ä¸AIå…±ç”Ÿå…³ç³»çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬é’ˆå¯¹ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜å±•å¼€ç ”ç©¶ï¼šï¼ˆ1ï¼‰å½“é¢ä¸´å›°å¢ƒæ—¶ï¼Œæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¼šéšæ€§åœ°æ”¯æŒå“ªäº›é“å¾·ä»·å€¼è§‚ï¼Ÿï¼ˆ2ï¼‰æ¨¡å‹ç»“æ„ã€æ–‡åŒ–èƒŒæ™¯å’Œå¯è§£é‡Šæ€§çš„å·®å¼‚å¦‚ä½•å½±å“è¿™äº›é“å¾·åå¥½ï¼Ÿä¸ºäº†æ¢ç´¢è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¯¹å…­æ¬¾LLMè¿›è¡Œäº†å®šé‡å®éªŒï¼Œå¯¹ä»£è¡¨äº”ç§é“å¾·æ¡†æ¶çš„18ä¸ªå›°å¢ƒè¿›è¡Œæ’åå’Œè¯„åˆ†ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†ä»¤äººæƒŠè®¶çš„ä»·å€¼è§‚å¿µä¸€è‡´æ€§ã€‚åœ¨æ‰€æœ‰æ¨¡å‹ä¸­ï¼Œå…³æ€€å’Œç¾å¾·ä»·å€¼è§‚çš„ç»“æœè¢«è¯„ä¸ºæœ€é“å¾·ï¼Œè€Œè‡ªç”±ä¸»ä¹‰é€‰æ‹©åˆ™å§‹ç»ˆå—åˆ°æƒ©ç½šã€‚å…·æœ‰æ¨ç†èƒ½åŠ›çš„æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡æ›´åŠ æ•æ„Ÿï¼Œå¹¶æä¾›äº†æ›´ä¸°å¯Œçš„è§£é‡Šï¼Œè€Œç¼ºä¹æ¨ç†çš„æ¨¡å‹åˆ™äº§ç”Ÿäº†æ›´ç»Ÿä¸€ä½†æ¨¡ç³Šçš„åˆ¤æ–­ã€‚æœ¬ç ”ç©¶åšå‡ºäº†ä¸‰ä¸ªè´¡çŒ®ï¼šï¼ˆiï¼‰ä»å®è¯è§’åº¦çœ‹ï¼Œå®ƒæä¾›äº†è·¨æ–‡åŒ–LLMé“å¾·æ¨ç†çš„å¤§è§„æ¨¡æ¯”è¾ƒï¼›ï¼ˆiiï¼‰ä»ç†è®ºè§’åº¦çœ‹ï¼Œå®ƒå°†æ¦‚ç‡æ¨¡å‹è¡Œä¸ºä¸åº•å±‚ä»·å€¼ç¼–ç è”ç³»èµ·æ¥ï¼›ï¼ˆiiiï¼‰ä»å®è·µè§’åº¦çœ‹ï¼Œå®ƒå¼ºè°ƒäº†å¯è§£é‡Šæ€§å’Œæ–‡åŒ–æ„è¯†ä½œä¸ºå…³é”®è®¾è®¡åŸåˆ™çš„é‡è¦æ€§ï¼Œä»¥å¼•å¯¼AIèµ°å‘é€æ˜ã€ä¸€è‡´å’Œå…±ç”Ÿçš„æœªæ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10297v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å¦‚ä½•ä¸äººç±»çš„é“å¾·ä»·å€¼è§‚ä¿æŒä¸€è‡´çš„é—®é¢˜ã€‚é€šè¿‡å¯¹å…­ç§é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå®šé‡å®éªŒï¼Œç ”ç©¶å‘ç°åœ¨é¢å¯¹é“å¾·å›°å¢ƒæ—¶ï¼Œè¿™äº›æ¨¡å‹è¡¨ç°å‡ºä¸€è‡´çš„ä»·å€¼è§‚å€¾å‘ï¼Œå…¶ä¸­å…³æ€€å’Œç¾å¾·ä»·å€¼è§‚è¢«è®¤ä¸ºæ˜¯æœ€é“å¾·çš„ï¼Œè€Œè‡ªç”±ä¸»ä¹‰é€‰æ‹©åˆ™å—åˆ°ä¸€è‡´è°´è´£ã€‚å…·å¤‡æ¨ç†èƒ½åŠ›çš„æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡æ›´æ•æ„Ÿï¼Œå¹¶æä¾›æ›´ä¸°å¯Œçš„è§£é‡Šï¼Œè€Œç¼ºä¹æ¨ç†çš„æ¨¡å‹åˆ™äº§ç”Ÿæ›´ç»Ÿä¸€ä½†æ¨¡ç³Šçš„åˆ¤æ–­ã€‚æœ¬æ–‡ä¸ºAIçš„é“å¾·ä¸æ¨ç†ç ”ç©¶åšå‡ºäº†ä¸‰å¤§è´¡çŒ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨å†³ç­–è¿‡ç¨‹ä¸­ä¸äººç±»çš„é“å¾·ä»·å€¼è§‚ä¿æŒä¸€è‡´æ€§é—®é¢˜æ—¥ç›Šç´§è¿«ã€‚</li>
<li>é€šè¿‡å®šé‡å®éªŒï¼Œå‘ç°é¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢å¯¹é“å¾·å›°å¢ƒæ—¶è¡¨ç°å‡ºä¸€è‡´çš„ä»·å€¼è§‚å€¾å‘ã€‚</li>
<li>å…³æ€€å’Œç¾å¾·ä»·å€¼è§‚è¢«è®¤ä¸ºæ˜¯æœ€é“å¾·çš„ï¼Œè‡ªç”±ä¸»ä¹‰é€‰æ‹©å—åˆ°è°´è´£ã€‚</li>
<li>å…·å¤‡æ¨ç†èƒ½åŠ›çš„æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡æ›´æ•æ„Ÿï¼Œè§£é‡Šæ›´ä¸°å¯Œã€‚</li>
<li>ä¸åŒæ¨¡å‹æ¶æ„ã€æ–‡åŒ–èµ·æºå’Œå¯è§£é‡Šæ€§å¯¹é“å¾·åå¥½äº§ç”Ÿå½±å“ã€‚</li>
<li>æœ¬æ–‡ä¸ºå¤§èŒƒå›´æ¯”è¾ƒä¸åŒæ–‡åŒ–èƒŒæ™¯ä¸‹çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„é“å¾·æ¨ç†æä¾›äº†å®è¯æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10297">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10297v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10297v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10297v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="URL2Graph-Unified-Semantic-Structural-Character-Learning-for-Malicious-URL-Detection"><a href="#URL2Graph-Unified-Semantic-Structural-Character-Learning-for-Malicious-URL-Detection" class="headerlink" title="URL2Graph++: Unified Semantic-Structural-Character Learning for   Malicious URL Detection"></a>URL2Graph++: Unified Semantic-Structural-Character Learning for   Malicious URL Detection</h2><p><strong>Authors:Ye Tian, Yifan Jia, Yanbin Wang, Jianguo Sun, Zhiquan Liu, Xiaowen Ling</strong></p>
<p>Malicious URL detection remains a major challenge in cybersecurity, primarily due to two factors: (1) the exponential growth of the Internet has led to an immense diversity of URLs, making generalized detection increasingly difficult; and (2) attackers are increasingly employing sophisticated obfuscation techniques to evade detection. We advocate that addressing these challenges fundamentally requires: (1) obtaining semantic understanding to improve generalization across vast and diverse URL sets, and (2) accurately modeling contextual relationships within the structural composition of URLs. In this paper, we propose a novel malicious URL detection method combining multi-granularity graph learning with semantic embedding to jointly capture semantic, character-level, and structural features for robust URL analysis. To model internal dependencies within URLs, we first construct dual-granularity URL graphs at both subword and character levels, where nodes represent URL tokens&#x2F;characters and edges encode co-occurrence relationships. To obtain fine-grained embeddings, we initialize node representations using a character-level convolutional network. The two graphs are then processed through jointly trained Graph Convolutional Networks to learn consistent graph-level representations, enabling the model to capture complementary structural features that reflect co-occurrence patterns and character-level dependencies. Furthermore, we employ BERT to derive semantic representations of URLs for semantically aware understanding. Finally, we introduce a gated dynamic fusion network to combine the semantically enriched BERT representations with the jointly optimized graph vectors, further enhancing detection performance. We extensively evaluate our method across multiple challenging dimensions. Results show our method exceeds SOTA performance, including against large language models. </p>
<blockquote>
<p>æ¶æ„URLæ£€æµ‹åœ¨ç½‘ç»œå®‰å…¨ä¸­ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºä¸¤ä¸ªå› ç´ é€ æˆçš„ï¼šï¼ˆ1ï¼‰äº’è”ç½‘çš„æŒ‡æ•°çº§å¢é•¿å¯¼è‡´äº†URLçš„æå¤§å¤šæ ·æ€§ï¼Œä½¿å¾—é€šç”¨æ£€æµ‹çš„å›°éš¾è¶Šæ¥è¶Šå¤§ï¼›ï¼ˆ2ï¼‰æ”»å‡»è€…è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨å…ˆè¿›çš„éšè”½æŠ€æœ¯æ¥èº²é¿æ£€æµ‹ã€‚æˆ‘ä»¬ä¸»å¼ ä»æ ¹æœ¬ä¸Šè§£å†³è¿™äº›æŒ‘æˆ˜éœ€è¦ï¼šï¼ˆ1ï¼‰è·å¾—è¯­ä¹‰ç†è§£ï¼Œä»¥æé«˜åœ¨åºå¤§å’Œå¤šæ ·åŒ–çš„URLé›†åˆä¸­çš„é€šç”¨æ€§ï¼›ï¼ˆ2ï¼‰ç²¾ç¡®åœ°æ¨¡æ‹ŸURLç»“æ„ç»„æˆä¸­çš„ä¸Šä¸‹æ–‡å…³ç³»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆå¤šç²’åº¦å›¾å­¦ä¹ ä¸è¯­ä¹‰åµŒå…¥çš„æ–°å‹æ¶æ„URLæ£€æµ‹æ–¹æ³•ï¼Œä»¥è”åˆæ•è·è¯­ä¹‰ã€å­—ç¬¦çº§åˆ«å’Œç»“æ„ç‰¹å¾ï¼Œç”¨äºç¨³å¥çš„URLåˆ†æã€‚ä¸ºäº†æ¨¡æ‹ŸURLå†…éƒ¨çš„ä¾èµ–å…³ç³»ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨å­è¯å’Œå­—ç¬¦çº§åˆ«æ„å»ºåŒç²’åº¦URLå›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨URLä»¤ç‰Œ&#x2F;å­—ç¬¦ï¼Œè¾¹ç¼–ç å…±ç°å…³ç³»ã€‚ä¸ºäº†è·å¾—ç²¾ç»†ç²’åº¦çš„åµŒå…¥ï¼Œæˆ‘ä»¬ä½¿ç”¨å­—ç¬¦çº§å·ç§¯ç½‘ç»œåˆå§‹åŒ–èŠ‚ç‚¹è¡¨ç¤ºã€‚ç„¶åï¼Œè¿™ä¸¤ä¸ªå›¾é€šè¿‡è”åˆè®­ç»ƒçš„å›¾å·ç§¯ç½‘ç»œè¿›è¡Œå¤„ç†ï¼Œä»¥å­¦ä¹ ä¸€è‡´çš„å›¾çº§è¡¨ç¤ºï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•è·åæ˜ å…±ç°æ¨¡å¼å’Œå­—ç¬¦çº§ä¾èµ–å…³ç³»çš„äº’è¡¥ç»“æ„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨BERTæ¨å¯¼å‡ºURLçš„è¯­ä¹‰è¡¨ç¤ºï¼Œä»¥å®ç°è¯­ä¹‰æ„ŸçŸ¥çš„ç†è§£ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé—¨æ§åŠ¨æ€èåˆç½‘ç»œï¼Œå°†è¯­ä¹‰ä¸°å¯Œçš„BERTè¡¨ç¤ºä¸è”åˆä¼˜åŒ–çš„å›¾å‘é‡ç›¸ç»“åˆï¼Œè¿›ä¸€æ­¥æé«˜æ£€æµ‹æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ç»´åº¦ä¸Šå¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¿‡äº†æœ€æ–°æŠ€æœ¯çš„æ€§èƒ½è¡¨ç°ï¼ŒåŒ…æ‹¬å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„è¡¨ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10287v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§ç»“åˆå¤šç²’åº¦å›¾å­¦ä¹ ä¸è¯­ä¹‰åµŒå…¥çš„æ¶æ„URLæ£€æµ‹æ–°æ–¹æ³•ã€‚é€šè¿‡æ„å»ºåŒç²’åº¦URLå›¾æ•æ‰URLå†…éƒ¨ä¾èµ–å…³ç³»ï¼Œå¹¶åˆ©ç”¨å­—ç¬¦çº§å·ç§¯ç½‘ç»œè¿›è¡ŒèŠ‚ç‚¹è¡¨ç¤ºåˆå§‹åŒ–ã€‚æ¥ç€é€šè¿‡è”åˆè®­ç»ƒçš„å›¾å½¢å·ç§¯ç½‘ç»œå­¦ä¹ ä¸€è‡´çš„å›¾çº§åˆ«è¡¨ç¤ºï¼ŒåŒæ—¶å¼•å…¥BERTè¿›è¡ŒURLçš„è¯­ä¹‰è¡¨ç¤ºã€‚æœ€åï¼Œé‡‡ç”¨é—¨æ§åŠ¨æ€èåˆç½‘ç»œç»“åˆä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤ºå’Œè”åˆä¼˜åŒ–çš„å›¾å‘é‡ï¼Œæé«˜æ£€æµ‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‘æˆ˜ç»´åº¦ä¸Šçš„è¡¨ç°å‡è¶…è¿‡ç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ¶æ„URLæ£€æµ‹é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯äº’è”ç½‘æŒ‡æ•°çš„å¢é•¿å’Œæ”»å‡»è€…ä¸æ–­è¿›åŒ–çš„æ¨¡ç³ŠæŠ€æœ¯ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¶æ„URLæ£€æµ‹æ–¹æ³•ï¼Œç»“åˆäº†å¤šç²’åº¦å›¾å­¦ä¹ ä¸è¯­ä¹‰åµŒå…¥ï¼Œè¿›è¡Œç¨³å¥çš„URLåˆ†æã€‚</li>
<li>é€šè¿‡æ„å»ºåŒç²’åº¦URLå›¾æ¥æ•æ‰URLçš„å†…éƒ¨ä¾èµ–å…³ç³»ï¼Œå¹¶é‡‡ç”¨å­—ç¬¦çº§å·ç§¯ç½‘ç»œè¿›è¡ŒèŠ‚ç‚¹è¡¨ç¤ºåˆå§‹åŒ–ã€‚</li>
<li>åˆ©ç”¨BERTè¿›è¡ŒURLçš„è¯­ä¹‰è¡¨ç¤ºï¼Œå®ç°è¯­ä¹‰æ„ŸçŸ¥çš„ç†è§£ã€‚</li>
<li>é‡‡ç”¨é—¨æ§åŠ¨æ€èåˆç½‘ç»œç»“åˆä¸°å¯Œçš„è¯­ä¹‰è¡¨ç¤ºå’Œè”åˆä¼˜åŒ–çš„å›¾å‘é‡ï¼Œè¿›ä¸€æ­¥æé«˜æ£€æµ‹æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10287">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10287v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10287v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SignClip-Leveraging-Mouthing-Cues-for-Sign-Language-Translation-by-Multimodal-Contrastive-Fusion"><a href="#SignClip-Leveraging-Mouthing-Cues-for-Sign-Language-Translation-by-Multimodal-Contrastive-Fusion" class="headerlink" title="SignClip: Leveraging Mouthing Cues for Sign Language Translation by   Multimodal Contrastive Fusion"></a>SignClip: Leveraging Mouthing Cues for Sign Language Translation by   Multimodal Contrastive Fusion</h2><p><strong>Authors:Wenfang Wu, Tingting Yuan, Yupeng Li, Daling Wang, Xiaoming Fu</strong></p>
<p>Sign language translation (SLT) aims to translate natural language from sign language videos, serving as a vital bridge for inclusive communication. While recent advances leverage powerful visual backbones and large language models, most approaches mainly focus on manual signals (hand gestures) and tend to overlook non-manual cues like mouthing. In fact, mouthing conveys essential linguistic information in sign languages and plays a crucial role in disambiguating visually similar signs. In this paper, we propose SignClip, a novel framework to improve the accuracy of sign language translation. It fuses manual and non-manual cues, specifically spatial gesture and lip movement features. Besides, SignClip introduces a hierarchical contrastive learning framework with multi-level alignment objectives, ensuring semantic consistency across sign-lip and visual-text modalities. Extensive experiments on two benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from 24.32 to 24.71, and ROUGE from 46.57 to 48.38. </p>
<blockquote>
<p>æ‰‹è¯­ç¿»è¯‘ï¼ˆSLTï¼‰æ—¨åœ¨ä»æ‰‹è¯­è§†é¢‘ç¿»è¯‘è‡ªç„¶è¯­è¨€ï¼Œæˆä¸ºåŒ…å®¹æ€§æ²Ÿé€šçš„é‡è¦æ¡¥æ¢ã€‚è™½ç„¶æœ€è¿‘çš„è¿›å±•åˆ©ç”¨äº†å¼ºå¤§çš„è§†è§‰éª¨å¹²ç½‘å’Œå¤§è¯­è¨€æ¨¡å‹ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ‰‹åŠ¨ä¿¡å·ï¼ˆæ‰‹åŠ¿ï¼‰ä¸Šï¼Œè€Œå¿½ç•¥äº†éæ‰‹åŠ¨çº¿ç´¢ï¼Œå¦‚å˜´å”‡åŠ¨ä½œã€‚å®é™…ä¸Šï¼Œå˜´å”‡åŠ¨ä½œåœ¨ä¼ é€’æ‰‹è¯­ä¸­çš„åŸºæœ¬è¯­è¨€ä¿¡æ¯æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œåœ¨åŒºåˆ†è§†è§‰ä¸Šç›¸ä¼¼çš„ç¬¦å·æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SignClipï¼Œä¸€ä¸ªæé«˜æ‰‹è¯­ç¿»è¯‘å‡†ç¡®æ€§çš„æ–°å‹æ¡†æ¶ã€‚å®ƒèåˆäº†æ‰‹åŠ¨å’Œéæ‰‹åŠ¨çº¿ç´¢ï¼Œç‰¹åˆ«æ˜¯ç©ºé—´æ‰‹åŠ¿å’Œå˜´å”‡è¿åŠ¨ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒSignClipå¼•å…¥äº†ä¸€ä¸ªåˆ†å±‚å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå…·æœ‰å¤šå±‚æ¬¡å¯¹é½ç›®æ ‡ï¼Œç¡®ä¿æ‰‹è¯­å”‡åŠ¨å’Œè§†è§‰æ–‡æœ¬æ¨¡å¼ä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚åœ¨PHOENIX14Tå’ŒHow2Signä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜è¶Šæ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨PHOENIX14Tçš„Gloss-freeè®¾ç½®ä¸­ï¼ŒSignClipè¶…è¶Šäº†ä¹‹å‰çš„æœ€æ–°æ¨¡å‹SpaMoï¼ŒBLEU-4å¾—åˆ†ä»24.32æé«˜åˆ°24.71ï¼ŒROUGEå¾—åˆ†ä»46.57æé«˜åˆ°48.38ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10266v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æ‰‹åŠ¿è¯­è¨€ç¿»è¯‘ï¼ˆSLTï¼‰æ—¨åœ¨é€šè¿‡è§†é¢‘å°†è‡ªç„¶è¯­è¨€ç¿»è¯‘æˆæ‰‹è¯­ï¼Œä¸ºåŒ…å®¹æ€§æ²Ÿé€šæä¾›é‡è¦æ¡¥æ¢ã€‚å°½ç®¡è¿‘æœŸå‘å±•åˆ©ç”¨å¼ºå¤§çš„è§†è§‰ä¸»å¹²å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•ä¸»è¦å…³æ³¨æ‰‹åŠ¨ä¿¡å·ï¼ˆæ‰‹åŠ¿ï¼‰ï¼Œå¿½è§†äº†éæ‰‹åŠ¨çº¿ç´¢å¦‚å£å‹å˜åŒ–ã€‚å£å‹åœ¨æ‰‹è¯­ä¸­ä¼ é€’é‡è¦çš„è¯­è¨€ä¿¡æ¯ï¼Œå¯¹æ¶ˆé™¤è§†è§‰ç›¸ä¼¼çš„ç¬¦å·èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚æœ¬æ–‡æå‡ºäº†SignClipæ¡†æ¶ï¼Œæ—¨åœ¨æé«˜æ‰‹è¯­ç¿»è¯‘çš„å‡†ç¡®åº¦ï¼Œå®ƒå°†æ‰‹åŠ¨å’Œéæ‰‹åŠ¨çº¿ç´¢ç›¸ç»“åˆï¼Œç‰¹åˆ«æ˜¯ç©ºé—´æ‰‹åŠ¿å’Œå”‡åŠ¨ç‰¹å¾ã€‚æ­¤å¤–ï¼ŒSignClipå¼•å…¥äº†ä¸€ç§å±‚æ¬¡å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œå…·æœ‰å¤šå±‚æ¬¡å¯¹é½ç›®æ ‡ï¼Œç¡®ä¿è·¨æ‰‹è¯­-å”‡è¯­å’Œè§†è§‰-æ–‡æœ¬æ¨¡æ€çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚åœ¨PHOENIX14Tå’ŒHow2Signä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜è¶Šæ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨PHOENIX14Tçš„å…æ ‡ç­¾è®¾ç½®ä¸­ï¼ŒSignClipè¶…è¶Šäº†ä¹‹å‰çš„æœ€ä½³æ¨¡å‹SpaMoï¼ŒBLEU-4å¾—åˆ†ä»24.32æé«˜åˆ°24.71ï¼ŒROUGEå¾—åˆ†ä»46.57æé«˜åˆ°48.38ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ‰‹åŠ¿è¯­è¨€ç¿»è¯‘æ˜¯é€šè¿‡å¯¹è§†é¢‘è¿›è¡Œè‡ªç„¶è¯­è¨€ç¿»è¯‘å®ç°çš„ï¼Œè¿™å¯¹äºå®ç°åŒ…å®¹æ€§æ²Ÿé€šè‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰å¤§å¤šæ•°æ–¹æ³•ä¸»è¦å…³æ³¨æ‰‹åŠ¨ä¿¡å·ï¼ˆæ‰‹åŠ¿ï¼‰ï¼Œå¿½è§†äº†éæ‰‹åŠ¨çº¿ç´¢å¦‚å£å‹å˜åŒ–çš„é‡è¦æ€§ã€‚</li>
<li>å£å‹åœ¨æ‰‹è¯­ä¸­ä¼ é€’é‡è¦çš„è¯­è¨€ä¿¡æ¯ï¼Œæœ‰åŠ©äºæ¶ˆé™¤è§†è§‰ç›¸ä¼¼ç¬¦å·çš„æ­§ä¹‰ã€‚</li>
<li>SignClipæ¡†æ¶ç»“åˆäº†æ‰‹åŠ¨å’Œéæ‰‹åŠ¨çº¿ç´¢ï¼Œç‰¹åˆ«æ˜¯ç©ºé—´æ‰‹åŠ¿å’Œå”‡åŠ¨ç‰¹å¾æ¥æé«˜ç¿»è¯‘å‡†ç¡®åº¦ã€‚</li>
<li>SignClipé‡‡ç”¨å±‚æ¬¡å¯¹æ¯”å­¦ä¹ æ¡†æ¶å’Œå¤šå±‚æ¬¡å¯¹é½ç›®æ ‡ç¡®ä¿è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜äº†SignClipç›¸å¯¹äºä¹‹å‰æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10266">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10266v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10266v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10266v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10266v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10266v1/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10266v1/page_5_1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="SI-FACT-Mitigating-Knowledge-Conflict-via-Self-Improving-Faithfulness-Aware-Contrastive-Tuning"><a href="#SI-FACT-Mitigating-Knowledge-Conflict-via-Self-Improving-Faithfulness-Aware-Contrastive-Tuning" class="headerlink" title="SI-FACT: Mitigating Knowledge Conflict via Self-Improving   Faithfulness-Aware Contrastive Tuning"></a>SI-FACT: Mitigating Knowledge Conflict via Self-Improving   Faithfulness-Aware Contrastive Tuning</h2><p><strong>Authors:Shengqiang Fu</strong></p>
<p>Large Language Models often generate unfaithful responses in knowledge intensive tasks due to knowledge conflict,that is,a preference for relying on internal parametric knowledge rather than the provided context.To address this issue,we propose a novel self improving framework,Self Improving Faithfulness Aware Contrastive Tuning.The framework uses a self instruct mechanism that allows the base LLM to automatically generate high quality,structured contrastive learning data,including anchor samples,semantically equivalent positive samples,and negative samples simulating unfaithful scenarios.This approach significantly reduces the cost of manual annotation.Subsequently,contrastive learning is applied to train the model,enabling it to pull faithful responses closer and push unfaithful responses farther apart in the representation space.Experiments on knowledge conflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT model based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2% over the best baseline method,while significantly reducing dependence on internal memory.The results indicate that SI FACT provides strong effectiveness and high data efficiency in enhancing the contextual faithfulness of LLMs,offering a practical pathway toward building more proactive and trustworthy language models. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­å¸¸å› çŸ¥è¯†å†²çªè€Œäº§ç”Ÿä¸å¿ å®çš„å“åº”ï¼Œå³æ›´å€¾å‘äºä¾èµ–å†…éƒ¨å‚æ•°çŸ¥è¯†è€Œéæä¾›çš„ä¸Šä¸‹æ–‡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è‡ªæ”¹è¿›æ¡†æ¶ï¼Œå³â€œè‡ªæˆ‘æ”¹è¿›å¿ å®æ€§æ„ŸçŸ¥å¯¹æ¯”è°ƒä¼˜â€ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è‡ªæˆ‘æŒ‡ä»¤æœºåˆ¶ï¼Œå…è®¸åŸºç¡€LLMè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ã€ç»“æ„åŒ–çš„å¯¹æ¯”å­¦ä¹ æ•°æ®ï¼ŒåŒ…æ‹¬é”šç‚¹æ ·æœ¬ã€è¯­ä¹‰ç­‰æ•ˆçš„æ­£å‘æ ·æœ¬å’Œæ¨¡æ‹Ÿä¸å¿ å®åœºæ™¯çš„è´Ÿå‘æ ·æœ¬ã€‚è¿™å¤§å¤§é™ä½äº†æ‰‹åŠ¨æ ‡æ³¨çš„æˆæœ¬ã€‚éšåï¼Œåº”ç”¨å¯¹æ¯”å­¦ä¹ æ¥è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶åœ¨è¡¨ç¤ºç©ºé—´ä¸­æ‹‰è¿‘å¿ å®çš„å“åº”å¹¶æ¨è¿œä¸å¿ å®çš„å“åº”ã€‚åœ¨çŸ¥è¯†å†²çªè¯„ä¼°åŸºå‡†ECARE KREå’ŒCOSE KREä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºLlama 3 8B Instructçš„SI FACTæ¨¡å‹åœ¨æœ€ä½³åŸºå‡†æ–¹æ³•çš„åŸºç¡€ä¸Šæé«˜äº†6.2%çš„ä¸Šä¸‹æ–‡å›å¿†ç‡ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘å¯¹å†…éƒ¨å†…å­˜çš„ä¾èµ–ã€‚ç»“æœè¡¨æ˜ï¼ŒSI FACTåœ¨å¢å¼ºLLMçš„ä¸Šä¸‹æ–‡å¿ å®æ€§æ–¹é¢å…·æœ‰å¾ˆå¼ºçš„æœ‰æ•ˆæ€§å’Œé«˜æ•°æ®æ•ˆç‡ï¼Œä¸ºæ„å»ºæ›´ä¸»åŠ¨å’Œå¯ä¿¡èµ–çš„è¯­è¨€æ¨¡å‹æä¾›äº†å®é™…é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10208v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºâ€œè‡ªæˆ‘æå‡ä¿¡ä»°æ„è¯†å¯¹æ¯”è°ƒä¼˜â€çš„æ–°å‹æ¡†æ¶ï¼Œä»¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­å› çŸ¥è¯†å†²çªè€Œäº§ç”Ÿçš„ä¸å¿ å®å›åº”é—®é¢˜ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è‡ªæˆ‘æŒ‡å¯¼æœºåˆ¶ï¼Œä½¿åŸºç¡€è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„ç»“æ„åŒ–å¯¹æ¯”å­¦ä¹ æ•°æ®ï¼ŒåŒ…æ‹¬é”šæ ·æœ¬ã€è¯­ä¹‰ç­‰æ•ˆçš„æ­£æ ·æœ¬å’Œæ¨¡æ‹Ÿä¸å¿ å®åœºæ™¯çš„è´Ÿæ ·æœ¬ï¼Œæ˜¾è‘—é™ä½äº†æ‰‹åŠ¨æ ‡æ³¨çš„æˆæœ¬ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œä½¿å¿ å®å›åº”æ›´æ¥è¿‘ï¼Œä¸å¿ å®çš„å›åº”åœ¨è¡¨ç¤ºç©ºé—´ä¸­æ›´è¿œã€‚åœ¨çŸ¥è¯†å†²çªè¯„ä¼°åŸºå‡†æµ‹è¯•ECARE KREå’ŒCOSE KREä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºLlama3 8B Instructçš„SI FACTæ¨¡å‹æé«˜äº†ä¸Šä¸‹æ–‡å›å¿†ç‡6.2%ï¼Œè¾ƒæœ€ä½³åŸºå‡†æ–¹æ³•æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼ŒåŒæ—¶å‡å°‘äº†å†…éƒ¨å†…å­˜çš„ä¾èµ–ã€‚ç»“æœè¡¨æ˜ï¼ŒSI FACTåœ¨æå‡è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å¿ å®æ€§æ–¹é¢å…·æœ‰å¾ˆå¼ºçš„æœ‰æ•ˆæ€§å’Œæ•°æ®é«˜æ•ˆæ€§ï¼Œä¸ºæ„å»ºæ›´ä¸»åŠ¨å’Œå¯ä¿¡èµ–çš„è¯­è¨€æ¨¡å‹æä¾›äº†å®é™…é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸­å› çŸ¥è¯†å†²çªäº§ç”Ÿä¸å¿ å®å›åº”ã€‚</li>
<li>æå‡ºâ€œè‡ªæˆ‘æå‡ä¿¡ä»°æ„è¯†å¯¹æ¯”è°ƒä¼˜â€æ¡†æ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æ¡†æ¶é‡‡ç”¨è‡ªæˆ‘æŒ‡å¯¼æœºåˆ¶ï¼Œè‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡å¯¹æ¯”å­¦ä¹ æ•°æ®ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œä½¿å¿ å®å›åº”æ›´æ¥è¿‘ï¼Œä¸å¿ å®å›åº”æ›´è¿œã€‚</li>
<li>åœ¨çŸ¥è¯†å†²çªè¯„ä¼°åŸºå‡†æµ‹è¯•ä¸Šï¼ŒSI FACTæ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼Œæé«˜ä¸Šä¸‹æ–‡å›å¿†ç‡ã€‚</li>
<li>SI FACTæ¨¡å‹å‡å°‘äº†å†…éƒ¨å†…å­˜çš„ä¾èµ–ï¼Œå…·æœ‰æ•°æ®é«˜æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10208">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10208v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10208v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10208v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10208v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10208v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Beyond-Token-Limits-Assessing-Language-Model-Performance-on-Long-Text-Classification"><a href="#Beyond-Token-Limits-Assessing-Language-Model-Performance-on-Long-Text-Classification" class="headerlink" title="Beyond Token Limits: Assessing Language Model Performance on Long Text   Classification"></a>Beyond Token Limits: Assessing Language Model Performance on Long Text   Classification</h2><p><strong>Authors:MiklÃ³s SebÅ‘k, Viktor KovÃ¡cs, Martin BÃ¡nÃ³czy, Daniel MÃ¸ller Eriksen, Nathalie Neptune, Philippe Roussille</strong></p>
<p>The most widely used large language models in the social sciences (such as BERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text length that they can process to produce predictions. This is a particularly pressing issue for some classification tasks, where the aim is to handle long input texts. One such area deals with laws and draft laws (bills), which can have a length of multiple hundred pages and, therefore, are not particularly amenable for processing with models that can only handle e.g. 512 tokens. In this paper, we show results from experiments covering 5 languages with XLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass classification task of the Comparative Agendas Project, which has a codebook of 21 policy topic labels from education to health care. Results show no particular advantage for the Longformer model, pre-trained specifically for the purposes of handling long inputs. The comparison between the GPT variants and the best-performing open model yielded an edge for the latter. An analysis of class-level factors points to the importance of support and substance overlaps between specific categories when it comes to performance on long text inputs. </p>
<blockquote>
<p>ç¤¾ä¼šç§‘å­¦é¢†åŸŸæœ€å¸¸ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BERTåŠå…¶è¡ç”Ÿå“RoBERTaï¼‰åœ¨ç”Ÿæˆé¢„æµ‹æ—¶ï¼Œå¯¹è¾“å…¥æ–‡æœ¬çš„é•¿åº¦æœ‰ä¸€å®šçš„é™åˆ¶ã€‚è¿™å¯¹äºä¸€äº›éœ€è¦å¤„ç†é•¿è¾“å…¥æ–‡æœ¬çš„åˆ†ç±»ä»»åŠ¡æ¥è¯´ï¼Œæ˜¯ä¸€ä¸ªç‰¹åˆ«ç´§è¿«çš„é—®é¢˜ã€‚å…¶ä¸­ä¸€ä¸ªé¢†åŸŸæ¶‰åŠæ³•å¾‹å’Œæ³•æ¡ˆè‰æ¡ˆï¼Œè¿™äº›æ³•æ¡ˆå¯èƒ½é•¿è¾¾æ•°ç™¾é¡µï¼Œå› æ­¤ä¸å¤ªé€‚åˆä½¿ç”¨åªèƒ½å¤„ç†ä¾‹å¦‚512ä¸ªæ ‡è®°çš„æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨XLM-RoBERTaã€Longformerã€GPT-3.5å’ŒGPT-4æ¨¡å‹è¿›è¡Œå¤šè¯­è¨€åˆ†ç±»å®éªŒçš„ç»“æœï¼Œè¿™äº›å®éªŒæ¶µç›–äº†äº”ç§è¯­è¨€çš„å¤šç±»åˆ†ç±»ä»»åŠ¡â€”â€”æ¯”è¾ƒè®®ç¨‹é¡¹ç›®ï¼Œè¯¥é¡¹ç›®æœ‰ä»æ•™è‚²åˆ°åŒ»ç–—ä¿å¥çš„21ä¸ªæ”¿ç­–è¯é¢˜æ ‡ç­¾çš„ä»£ç æœ¬ã€‚ç»“æœè¡¨æ˜ï¼Œé’ˆå¯¹å¤„ç†é•¿è¾“å…¥è€Œä¸“é—¨è¿›è¡Œé¢„è®­ç»ƒçš„Longformeræ¨¡å‹å¹¶æ²¡æœ‰ç‰¹åˆ«çš„ä¼˜åŠ¿ã€‚GPTç³»åˆ—æ¨¡å‹ä¸è¡¨ç°æœ€ä½³çš„å¼€æ”¾æ¨¡å‹ä¹‹é—´çš„æ¯”è¾ƒæ˜¾ç¤ºï¼Œåè€…æ›´èƒœä¸€ç­¹ã€‚å¯¹ç±»åˆ«å±‚é¢å› ç´ çš„åˆ†ææŒ‡å‡ºï¼Œåœ¨å¤„ç†é•¿æ–‡æœ¬è¾“å…¥æ—¶ï¼Œç‰¹å®šç±»åˆ«ä¹‹é—´æ”¯æŒå’Œå®è´¨æ€§å†…å®¹çš„é‡å å¯¹äºæ€§èƒ½è‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10199v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç¤¾ä¼šç§‘å­¦é¢†åŸŸé•¿æ–‡æœ¬è¾“å…¥æ—¶çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ³•å¾‹è‰æ¡ˆç­‰å¤šè¾¾æ•°ç™¾é¡µçš„é•¿æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šã€‚å®éªŒæ¯”è¾ƒäº†XLM-RoBERTaã€Longformerã€GPT-3.5å’ŒGPT-4æ¨¡å‹åœ¨å¤šè¯­ç§ç¯å¢ƒä¸‹çš„è¡¨ç°ï¼Œç»“æœæ˜¾ç¤ºGPTç³»åˆ—æ¨¡å‹è¡¨ç°è¾ƒå¥½ï¼Œè€Œé’ˆå¯¹é•¿è¾“å…¥è®¾è®¡çš„Longformeræ¨¡å‹å¹¶æœªæ˜¾ç¤ºå‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚åˆ†æè¿˜å‘ç°ç±»åˆ«é—´çš„æ”¯æŒä¸å®è´¨é‡å å¯¹é•¿æ–‡æœ¬è¾“å…¥æ€§èƒ½çš„å½±å“é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬è¾“å…¥æ—¶å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠæ³•å¾‹è‰æ¡ˆç­‰å¤šè¾¾æ•°ç™¾é¡µçš„åˆ†ç±»ä»»åŠ¡ä¸Šã€‚</li>
<li>å®éªŒæ¯”è¾ƒäº†å¤šç§æ¨¡å‹ï¼ˆåŒ…æ‹¬XLM-RoBERTaã€Longformerã€GPT-3.5å’ŒGPT-4ï¼‰åœ¨å¤šç§è¯­è¨€ç¯å¢ƒä¸‹çš„è¡¨ç°ã€‚</li>
<li>GPTç³»åˆ—æ¨¡å‹åœ¨å¤šç±»åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°è¾ƒå¥½ã€‚</li>
<li>Longformeræ¨¡å‹ï¼Œä¸“ä¸ºå¤„ç†é•¿è¾“å…¥è€Œè®¾è®¡ï¼Œä½†æœªåœ¨å®éªŒä¸­æ˜¾ç¤ºå‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
<li>ç±»åˆ«çš„æ”¯æŒä¸å®è´¨é‡å å¯¹æ¨¡å‹åœ¨é•¿æ–‡æœ¬è¾“å…¥ä¸Šçš„æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚</li>
<li>å®éªŒæ¶‰åŠçš„æ”¿ç­–ä¸»é¢˜æ ‡ç­¾æ¶µç›–ä»æ•™è‚²åˆ°åŒ»ç–—ä¿å¥ç­‰å¤šä¸ªé¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10199">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10199v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10199v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="The-Thinking-Therapist-Training-Large-Language-Models-to-Deliver-Acceptance-and-Commitment-Therapy-using-Supervised-Fine-Tuning-and-Odds-Ratio-Policy-Optimization"><a href="#The-Thinking-Therapist-Training-Large-Language-Models-to-Deliver-Acceptance-and-Commitment-Therapy-using-Supervised-Fine-Tuning-and-Odds-Ratio-Policy-Optimization" class="headerlink" title="The Thinking Therapist: Training Large Language Models to Deliver   Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio   Policy Optimization"></a>The Thinking Therapist: Training Large Language Models to Deliver   Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio   Policy Optimization</h2><p><strong>Authors:Talha Tahir</strong></p>
<p>Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral therapy with emerging evidence of efficacy in several psychiatric conditions. This study investigates the impact of post-training methodology and explicit reasoning on the ability of a small open-weight large language model (LLM) to deliver ACT. Using 50 sets of synthetic ACT transcripts generated by Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches, supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each with and without an explicit chain-of-thought (COT) reasoning step. Performance was evaluated by comparing these four post-trained variants against the base Instruct model. These models were benchmarked in simulated therapy sessions, with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM) and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned on human evaluations. Our findings demonstrate that the ORPO-trained models significantly outperformed both their SFT and Instruct counterparts on ACT fidelity ($\chi^2(5) &#x3D; 185.15, p &lt; .001$) and therapeutic empathy ($\chi^2(5) &#x3D; 140.37, p &lt; .001$). The effect of COT was conditional as it provided a significant benefit to SFT models, improving ACT-FM scores by an average of 2.68 points ($p &lt; .001$), while offering no discernible advantage to the superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO stems from its ability to learn the therapeutic <code>process&#39; over imitating </code>content,â€™ a key aspect of ACT, while COT acts as a necessary scaffold for models trained only via imitation. This study establishes that preference-aligned policy optimization can effectively instill ACT competencies in small LLMs, and that the utility of explicit reasoning is highly dependent on the underlying training paradigm. </p>
<blockquote>
<p>æ¥çº³æ‰¿è¯ºç–—æ³•ï¼ˆACTï¼‰æ˜¯ä¸€ç§æ–°å…´çš„ã€åœ¨å¤šç§ç²¾ç¥ç–¾ç—…ä¸­è¡¨ç°å‡ºç–—æ•ˆçš„ç¬¬ä¸‰ä»£è®¤çŸ¥è¡Œä¸ºç–—æ³•ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†è®­ç»ƒåçš„æ–¹æ³•å’Œæ˜ç¡®æ¨ç†å¯¹å°å‹å¼€æ”¾å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰§è¡ŒACTèƒ½åŠ›çš„å½±å“ã€‚æˆ‘ä»¬ä½¿ç”¨ç”±Mistral-Largeç”Ÿæˆçš„50ç»„åˆæˆACTè½¬å½•æœ¬ï¼Œé‡‡ç”¨ä¸¤ç§ä¸åŒçš„æ–¹æ³•è®­ç»ƒäº†Llama-3.2-3b-Instructæ¨¡å‹ï¼Œå³ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œèµ”ç‡æ¯”ç‡ç­–ç•¥ä¼˜åŒ–ï¼ˆORPOï¼‰ï¼Œæ¯ç§æ–¹æ³•éƒ½å¸¦æœ‰å’Œä¸å¸¦æœ‰æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCOTï¼‰æ¨ç†æ­¥éª¤ã€‚é€šè¿‡ä¸åŸºç¡€Instructæ¨¡å‹å¯¹æ¯”è¿™å››ç§è®­ç»ƒåçš„å˜ä½“æ¥è¯„ä¼°æ€§èƒ½ã€‚è¿™äº›æ¨¡å‹åœ¨æ¨¡æ‹Ÿæ²»ç–—ä¼šè¯ä¸­çš„è¡¨ç°ï¼Œé€šè¿‡æ¥çº³æ‰¿è¯ºç–—æ³•ä¿çœŸåº¦è¡¡é‡æ ‡å‡†ï¼ˆACT-FMï¼‰å’Œå¿ƒç†æ²»ç–—å¸ˆåŒç†å¿ƒé‡è¡¨ï¼ˆTESï¼‰è¿›è¡Œå®šé‡è¯„ä¼°ï¼Œè¯„ä¼°äººå‘˜ä¸ºç»è¿‡äººç±»è¯„ä¼°ç²¾ç»†è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹è£åˆ¤ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»è¿‡ORPOè®­ç»ƒçš„æ¨¡å‹åœ¨ACTä¿çœŸåº¦å’Œæ²»ç–—åŒç†å¿ƒä¸Šå‡æ˜¾è‘—ä¼˜äºSFTå’ŒInstructæ¨¡å‹ï¼ˆÏ‡Â²ï¼ˆ5ï¼‰&#x3D;185.15ï¼Œp &lt; .001 å’Œ Ï‡Â²ï¼ˆ5ï¼‰&#x3D;140.37ï¼Œp &lt; .001ï¼‰ã€‚æ€ç»´é“¾çš„å½±å“æ˜¯æœ‰æ¡ä»¶çš„ï¼Œå› ä¸ºå®ƒä¸ºSFTæ¨¡å‹æä¾›äº†æ˜¾è‘—çš„å¥½å¤„ï¼ŒACT-FMå¾—åˆ†å¹³å‡æé«˜äº†2.68åˆ†ï¼ˆp &lt; .001ï¼‰ï¼Œè€Œå¯¹äºæ›´é«˜çº§çš„ORPOæˆ–ç»æ•™å¯¼è°ƒæ•´çš„å˜ä½“ï¼Œåˆ™æ²¡æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿å¯è¨€ã€‚æˆ‘ä»¬è®¤ä¸ºORPOçš„ä¼˜è¶Šæ€§æ¥è‡ªäºå…¶å­¦ä¹ æ²»ç–—â€œè¿‡ç¨‹â€è€Œéæ¨¡ä»¿â€œå†…å®¹â€çš„èƒ½åŠ›ï¼Œè¿™æ˜¯ACTçš„ä¸€ä¸ªå…³é”®æ–¹é¢ï¼Œè€Œæ€ç»´é“¾å¯¹äºä»…é€šè¿‡æ¨¡ä»¿è®­ç»ƒçš„æ¨¡å‹æ¥è¯´æ˜¯å¿…è¦çš„æ”¯æ¶ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œåå¥½å¯¹é½çš„ç­–ç•¥ä¼˜åŒ–å¯ä»¥æœ‰æ•ˆåœ°èµ‹äºˆå°å‹LLMæ‰§è¡ŒACTçš„èƒ½åŠ›ï¼Œè€Œæ˜ç¡®æ¨ç†çš„å®ç”¨æ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºåŸºæœ¬çš„è®­ç»ƒèŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09712v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æ¥å—ä¸æ‰¿è¯ºç–—æ³•ï¼ˆACTï¼‰æ˜¯ä¸€ç§ç¬¬ä¸‰æ³¢è®¤çŸ¥è¡Œä¸ºç–—æ³•ï¼Œåœ¨å¤šç§ç²¾ç¥ç–¾ç—…ä¸­æœ‰ç–—æ•ˆã€‚æœ¬ç ”ç©¶é€šè¿‡ä¸åŒè®­ç»ƒæ–¹æ³•å’Œæ˜¾å¼æ¨ç†æ¢ç©¶å…¶å¯¹å°å‹å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰§è¡ŒACTèƒ½åŠ›çš„å½±å“ã€‚å®éªŒä½¿ç”¨Mistral-Largeç”Ÿæˆçš„50ç»„åˆæˆACTè½¬å½•æœ¬ï¼Œé‡‡ç”¨ä¸¤ç§ä¸åŒçš„è®­ç»ƒæ–¹æ³•ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œèµ”ç‡æ¯”ç‡ç­–ç•¥ä¼˜åŒ–ï¼ˆORPOï¼‰ï¼Œæ¯ç§æ–¹æ³•éƒ½å¸¦æœ‰å’Œä¸å¸¦æœ‰æ˜¾å¼é“¾å¼æ€ç»´ï¼ˆCOTï¼‰æ¨ç†æ­¥éª¤ã€‚é€šè¿‡æ¨¡æ‹Ÿæ²»ç–—ä¼šè¯è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¹¶é€šè¿‡æ¥å—åº¦æµ‹é‡ï¼ˆACT-FMï¼‰å’Œç–—æ„ˆå¸ˆåŒç†å¿ƒé‡è¡¨ï¼ˆTESï¼‰è¿›è¡Œå®šé‡è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼ŒORPOè®­ç»ƒçš„æ¨¡å‹åœ¨ACTä¿çœŸå’Œç–—æ„ˆæƒ…æ„Ÿæ–¹é¢æ˜¾è‘—ä¼˜äºSFTå’ŒåŸºå‡†æ¨¡å‹ï¼ŒCOTå¯¹SFTæ¨¡å‹æœ‰å¸®åŠ©ä½†å¯¹ORPOæ¨¡å‹åˆ™æ— æ˜æ˜¾ä¼˜åŠ¿ã€‚ç ”ç©¶è®¤ä¸ºORPOçš„ä¼˜è¶Šæ€§åœ¨äºå®ƒèƒ½å­¦ä¹ æ²»ç–—è¿‡ç¨‹è€Œéå•çº¯æ¨¡ä»¿å†…å®¹ï¼Œè€ŒACTçš„æ ¸å¿ƒåœ¨äºè¿‡ç¨‹ï¼›è€ŒCOTå¯¹äºä»…é€šè¿‡æ¨¡ä»¿è®­ç»ƒçš„æ¨¡å‹æ˜¯å¿…è¦çš„æ”¯æ¶ã€‚æ­¤ç ”ç©¶è¡¨æ˜ï¼Œåå¥½å¯¹é½çš„ç­–ç•¥ä¼˜åŒ–å¯ä»¥æœ‰æ•ˆèµ‹äºˆå°å‹LLMæ‰§è¡ŒACTçš„èƒ½åŠ›ï¼Œè€Œæ˜¾å¼æ¨ç†çš„å®ç”¨æ€§å–å†³äºåŸºæœ¬è®­ç»ƒèŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ACTæ˜¯ä¸€ç§æœ‰æ•ˆçš„è®¤çŸ¥è¡Œä¸ºç–—æ³•ï¼Œå¯ç”¨äºæ²»ç–—å¤šç§ç²¾ç¥ç–¾ç—…ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å®éªŒæ¢ç©¶äº†ä¸åŒè®­ç»ƒæ–¹æ³•å’Œæ˜¾å¼æ¨ç†å¯¹å°å‹LLMæ‰§è¡ŒACTèƒ½åŠ›çš„å½±å“ã€‚</li>
<li>ORPOè®­ç»ƒæ–¹æ³•èƒ½æ˜¾è‘—æé«˜LLMæ‰§è¡ŒACTçš„æ•ˆèƒ½ï¼Œä¼˜äºç›‘ç£å¾®è°ƒå’Œå…¶ä»–è®­ç»ƒæ–¹æ³•ã€‚</li>
<li>æ˜¾å¼é“¾å¼æ€ç»´ï¼ˆCOTï¼‰æ¨ç†æ­¥éª¤å¯¹éƒ¨åˆ†è®­ç»ƒæ¨¡å‹æœ‰å¸®åŠ©ï¼Œä½†å¯¹å·²ä¼˜åŒ–çš„æ¨¡å‹åˆ™æ— æ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
<li>ORPOçš„ä¼˜è¶Šæ€§åœ¨äºå®ƒèƒ½å­¦ä¹ æ²»ç–—è¿‡ç¨‹è€Œéå•çº¯æ¨¡ä»¿å†…å®¹ï¼Œç¬¦åˆACTçš„æ ¸å¿ƒè¦ç´ ã€‚</li>
<li>åå¥½å¯¹é½çš„ç­–ç•¥ä¼˜åŒ–æ˜¯èµ‹äºˆå°å‹LLMæ‰§è¡ŒACTèƒ½åŠ›çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.09712v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.09712v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.09712v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.09712v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.09712v1/page_4_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.09712v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning"><a href="#Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning" class="headerlink" title="Parallel-R1: Towards Parallel Thinking via Reinforcement Learning"></a>Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</h2><p><strong>Authors:Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, Dong Yu</strong></p>
<p>Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose \textbf{Parallel-R1}, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the modelâ€™s thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a \textbf{mid-training exploration scaffold}, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at <a target="_blank" rel="noopener" href="https://github.com/zhengkid/Parallel-R1">https://github.com/zhengkid/Parallel-R1</a>. </p>
<blockquote>
<p>å¹¶è¡Œæ€ç»´å·²ç»æˆä¸ºä¸€ç§é€šè¿‡åŒæ—¶æ¢ç´¢å¤šä¸ªæ¨ç†è·¯å¾„æ¥å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ–°æ–¹æ³•ã€‚ç„¶è€Œï¼Œé€šè¿‡è®­ç»ƒæ¿€æ´»è¿™ç§èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºåˆæˆæ•°æ®ä¸Šçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™é¼“åŠ±äº†æ•™å¸ˆå¼ºåˆ¶æ¨¡ä»¿ï¼Œè€Œéæ¢ç´¢å’Œæ³›åŒ–ã€‚ä¸å®ƒä»¬ä¸åŒï¼Œæˆ‘ä»¬æå‡ºäº†<strong>Parallel-R1</strong>ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå¯ç”¨äº†é’ˆå¯¹å¤æ‚ç°å®ä¸–ç•Œæ¨ç†ä»»åŠ¡çš„å¹¶è¡Œæ€ç»´è¡Œä¸ºçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§æ¸è¿›çš„è¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œæ˜ç¡®è§£å†³äº†è®­ç»ƒå¹¶è¡Œæ€ç»´æ—¶çš„å†·å¯åŠ¨é—®é¢˜ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨SFTå¯¹æ¥è‡ªè¾ƒç®€å•ä»»åŠ¡çš„æç¤ºç”Ÿæˆè½¨è¿¹è¿›è¡Œè®­ç»ƒï¼Œä»¥çŒè¾“å¹¶è¡Œæ€ç»´èƒ½åŠ›ï¼Œç„¶åè¿‡æ¸¡åˆ°RLæ¥æ¢ç´¢è¿™é¡¹æŠ€èƒ½å¹¶åœ¨æ›´éš¾çš„é—®é¢˜ä¸Šå®ç°æ³›åŒ–ã€‚åœ¨å„ç§æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼ŒåŒ…æ‹¬MATHã€AMC23å’ŒAIMEï¼Œè¡¨æ˜Parallel-R1æˆåŠŸåŸ¹å…»äº†å¹¶è¡Œæ€ç»´ï¼Œç›¸å¯¹äºç›´æ¥åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šä½¿ç”¨RLè¿›è¡Œé¡ºåºæ€ç»´è®­ç»ƒçš„æ¨¡å‹ï¼Œå…¶å‡†ç¡®åº¦æé«˜äº†8.4%ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ˜¾ç¤ºæ¨¡å‹æ€è€ƒè¡Œä¸ºçš„æ˜æ˜¾è½¬å˜ï¼šåœ¨æ—©æœŸé˜¶æ®µï¼Œå®ƒä½¿ç”¨å¹¶è¡Œæ€ç»´ä½œä¸ºæ¢ç´¢ç­–ç•¥ï¼Œè€Œåœ¨åæœŸé˜¶æ®µï¼Œå®ƒä½¿ç”¨ç›¸åŒçš„èƒ½åŠ›è¿›è¡Œå¤šè§†è§’éªŒè¯ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬éªŒè¯äº†å¹¶è¡Œæ€ç»´ä½œä¸º<strong>ä¸­æœŸè®­ç»ƒæ¢ç´¢è„šæ‰‹æ¶</strong>ï¼Œè¿™ä¸€ä¸´æ—¶æ¢ç´¢é˜¶æ®µåœ¨RLä¹‹åå¼€å¯äº†æ›´é«˜çš„æ€§èƒ½ä¸Šé™ï¼Œåœ¨AIME25ä¸Šç›¸å¯¹äºåŸºå‡†å®ç°äº†42.9%çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhengkid/Parallel-R1%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/zhengkid/Parallel-R1ä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07980v2">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://zhengkid.github.io/Parallel_R1.github.io/">https://zhengkid.github.io/Parallel_R1.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¹¶è¡Œæ€ç»´åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›ä¸­çš„æ–°å…´ä½œç”¨ã€‚æå‡ºä¸€ç§åä¸ºParallel-R1çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å¹¶è¡Œæ€ç»´è§£å†³å¤æ‚ç°å®ä¸–ç•Œæ¨ç†ä»»åŠ¡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ¸è¿›å¼è¯¾ç¨‹ï¼Œè§£å†³å¹¶è¡Œæ€ç»´åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†·å¯åŠ¨é—®é¢˜ã€‚å…ˆé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åŸ¹å…»åŸºç¡€ä»»åŠ¡ä¸­çš„å¹¶è¡Œæ€ç»´èƒ½åŠ›ï¼Œå†è¿‡æ¸¡åˆ°å¼ºåŒ–å­¦ä¹ ä»¥åœ¨æ›´å¤æ‚é—®é¢˜ä¸Šæ¢ç´¢å’Œæ¨å¹¿æ­¤æŠ€èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒParallel-R1æˆåŠŸåŸ¹å…»äº†å¹¶è¡Œæ€ç»´ï¼Œç›¸è¾ƒäºç›´æ¥é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒé¡ºåºæ€è€ƒæ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜äº†8.4%ã€‚è¿›ä¸€æ­¥åˆ†ææ˜¾ç¤ºï¼Œæ¨¡å‹åœ¨ä¸åŒé˜¶æ®µä½¿ç”¨å¹¶è¡Œæ€ç»´çš„ç­–ç•¥ä¸åŒï¼Œæ—©æœŸç”¨äºæ¢ç´¢ç­–ç•¥ï¼ŒåæœŸç”¨äºå¤šè§†è§’éªŒè¯ã€‚éªŒè¯äº†å¹¶è¡Œæ€ç»´ä½œä¸ºè®­ç»ƒä¸­æœŸæ¢ç´¢æ¶æ„çš„ä½œç”¨ï¼Œè¿™ä¸€ä¸´æ—¶æ¢ç´¢é˜¶æ®µä¸ºå¼ºåŒ–å­¦ä¹ åçš„æ€§èƒ½æå‡æ‰“å¼€äº†ä¸Šé™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¹¶è¡Œæ€ç»´æœ‰åŠ©äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºParallel-R1çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œæ”¯æŒå¹¶è¡Œæ€ç»´è§£å†³å¤æ‚ç°å®ä¸–ç•Œæ¨ç†ä»»åŠ¡ã€‚</li>
<li>é‡‡ç”¨æ¸è¿›å¼è¯¾ç¨‹è§£å†³å¹¶è¡Œæ€ç»´åœ¨è®­ç»ƒä¸­çš„å†·å¯åŠ¨é—®é¢˜ã€‚</li>
<li>é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åŸ¹å…»åŸºç¡€ä»»åŠ¡ä¸­çš„å¹¶è¡Œæ€ç»´èƒ½åŠ›ï¼Œå†è¿‡æ¸¡åˆ°å¼ºåŒ–å­¦ä¹ è¿›è¡Œæ›´å¤æ‚çš„æ¢ç´¢å’Œæ¨å¹¿ã€‚</li>
<li>å®éªŒè¡¨æ˜Parallel-R1èƒ½æˆåŠŸåŸ¹å…»å¹¶è¡Œæ€ç»´ï¼Œç›¸è¾ƒäºé¡ºåºæ€è€ƒæ¨¡å‹æœ‰æ›´é«˜çš„å‡†ç¡®ç‡ã€‚</li>
<li>å¹¶è¡Œæ€ç»´åœ¨æ¨¡å‹çš„ä¸åŒé˜¶æ®µæœ‰ä¸åŒçš„åº”ç”¨ç­–ç•¥ï¼Œæ—©æœŸç”¨äºæ¢ç´¢ï¼ŒåæœŸç”¨äºéªŒè¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.07980v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.07980v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.07980v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="MachineLearningLM-Scaling-Many-shot-In-context-Learning-via-Continued-Pretraining"><a href="#MachineLearningLM-Scaling-Many-shot-In-context-Learning-via-Continued-Pretraining" class="headerlink" title="MachineLearningLM: Scaling Many-shot In-context Learning via Continued   Pretraining"></a>MachineLearningLM: Scaling Many-shot In-context Learning via Continued   Pretraining</h2><p><strong>Authors:Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke</strong></p>
<p>Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows.   Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference.   Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‹¥æœ‰å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œå¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ ‡å‡†æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä»»åŠ¡ä¸Šï¼Œå®ƒä»¬å¾ˆéš¾ä»è®¸å¤šä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­å­¦ä¹ ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¬æ— æ³•ä»…å‡­ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰æ¥åˆ©ç”¨å¤šç¤ºä¾‹æ¼”ç¤ºï¼Œè€Œæ— éœ€è¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚æˆ‘ä»¬å¼•å…¥äº†MachineLearningLMï¼Œè¿™æ˜¯ä¸€ä¸ªä¾¿æºçš„ç»§ç»­é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒä¸ºé€šç”¨LLMæä¾›äº†å¼ºå¤§çš„ä¸Šä¸‹æ–‡MLåŠŸèƒ½ï¼ŒåŒæ—¶ä¿ç•™äº†å…¶ä¸€èˆ¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥æ”¯æŒæ›´å¹¿æ³›çš„èŠå¤©å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬çš„é¢„è®­ç»ƒç¨‹åºé€šè¿‡æ•°ç™¾ä¸‡çš„ç»“æ„åŒ–å› æœæ¨¡å‹ï¼ˆSCMï¼‰åˆæˆMLä»»åŠ¡ï¼Œæ¶µç›–æ ·æœ¬é‡è¾¾1024ä¸ªã€‚æˆ‘ä»¬ä»¥éšæœºæ£®æ—æ•™å¸ˆå¼€å§‹ï¼Œå°†åŸºäºæ ‘çš„å†³ç­–ç­–ç•¥è’¸é¦åˆ°LLMä¸­ï¼Œä»¥åŠ å¼ºæ•°å€¼å»ºæ¨¡ä¸­çš„ç¨³å¥æ€§ã€‚æ‰€æœ‰ä»»åŠ¡éƒ½é€šè¿‡é«˜æ•ˆçš„ä»¤ç‰Œæç¤ºè¿›è¡Œåºåˆ—åŒ–ï¼Œå¯ä»¥åœ¨æ¯ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­å®ç°3å€è‡³6å€çš„ç¤ºä¾‹æ•°é‡ï¼Œå¹¶é€šè¿‡æ‰¹é‡æ¨ç†å®ç°é«˜è¾¾50å€çš„æ‘Šé”€ååé‡ã€‚å°½ç®¡æˆ‘ä»¬çš„è®¾ç½®ç›¸å¯¹ç®€å•ï¼ˆä½¿ç”¨LoRAç­‰çº§8çš„Qwen-2.5-7B-Instructï¼‰ï¼Œä½†MachineLearningLMåœ¨é‡‘èã€ç‰©ç†ã€ç”Ÿç‰©å’ŒåŒ»ç–—ä¿å¥é¢†åŸŸçš„éåˆ†å¸ƒå¼è¡¨æ ¼åˆ†ç±»ä¸Šï¼Œå¹³å‡æ¯”å¼ºå¤§çš„LLMåŸºçº¿ï¼ˆä¾‹å¦‚GPT-5 miniï¼‰é«˜å‡ºçº¦15%çš„å‡†ç¡®ç‡ã€‚å®ƒè¡¨ç°å‡ºäº†å¼•äººæ³¨ç›®çš„å¤šç¤ºä¾‹æ‰©å±•å®šå¾‹ï¼šéšç€ä¸Šä¸‹æ–‡æ¼”ç¤ºä»8ä¸ªå¢é•¿åˆ°1024ä¸ªï¼Œå‡†ç¡®ç‡ä¼šå•è°ƒå¢åŠ ã€‚æ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒï¼Œå®ƒåœ¨æ•°ç™¾ä¸ªæ ·æœ¬ç‚¹ä¸Šè¾¾åˆ°äº†éšæœºæ£®æ—çš„ç²¾åº¦ã€‚åŒæ—¶ä¿ç•™äº†é€šç”¨çš„èŠå¤©èƒ½åŠ›ï¼ŒåŒ…æ‹¬çŸ¥è¯†å’Œæ¨ç†ï¼šå®ƒåœ¨MMLUä¸Šè¾¾åˆ°äº†75.4%çš„å‡†ç¡®ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06806v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‹¥æœ‰å¹¿æ³›çš„çŸ¥è¯†å’Œå¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ ‡å‡†æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä»»åŠ¡ä¸Šéš¾ä»¥ä»å¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­å­¦ä¹ ã€‚æˆ‘ä»¬æ¨å‡ºMachineLearningLMï¼Œä¸€ä¸ªä¾¿æºçš„ç»§ç»­é¢„è®­ç»ƒæ¡†æ¶ï¼Œä¸ºé€šç”¨LLMèµ‹äºˆå¼ºå¤§çš„ä¸Šä¸‹æ–‡MLèƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™å…¶ä¸€èˆ¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œé€‚ç”¨äºæ›´å¹¿æ³›çš„èŠå¤©å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬çš„é¢„è®­ç»ƒç¨‹åºé€šè¿‡åˆæˆæ¥è‡ªæ•°ç™¾ä¸‡ç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰çš„MLä»»åŠ¡æ¥å¢å¼ºLLMçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œæ¶µç›–ç¤ºä¾‹æ•°é‡é«˜è¾¾1024ã€‚ç»“åˆéšæœºæ£®æ—æ•™å¸ˆï¼Œå°†æ ‘å½¢å†³ç­–ç­–ç•¥æ³¨å…¥LLMï¼ŒåŠ å¼ºæ•°å€¼å»ºæ¨¡çš„ç¨³å¥æ€§ã€‚æ‰€æœ‰ä»»åŠ¡éƒ½é€šè¿‡é«˜æ•ˆçš„ä»¤ç‰Œæç¤ºè¿›è¡Œåºåˆ—åŒ–ï¼Œèƒ½å¤Ÿåœ¨æ¯ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­æä¾›3è‡³6å€çš„ç¤ºä¾‹ï¼Œå¹¶é€šè¿‡æ‰¹é‡æ¨ç†å®ç°é«˜è¾¾50å€çš„æ‘Šé”€ååé‡ã€‚MachineLearningLMåœ¨é‡‘èã€ç‰©ç†ã€ç”Ÿç‰©å’ŒåŒ»ç–—ä¿å¥ç­‰é¢†åŸŸçš„ç¦»åˆ†å¸ƒè¡¨æ ¼åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œä¼˜äºå¼ºå¤§çš„LLMåŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚GPT-5-miniï¼‰ï¼Œå¹³å‡é«˜å‡ºçº¦15%ã€‚å®ƒè¡¨ç°å‡ºæ˜¾è‘—çš„è®¸å¤šé•œå¤´è§„æ¨¡æ•ˆåº”ï¼šéšç€ä¸Šä¸‹æ–‡æ¼”ç¤ºä»8å¢é•¿åˆ°1024ï¼Œç²¾åº¦ä¸æ–­æé«˜ã€‚æ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡è®­ç»ƒï¼Œå³å¯åœ¨æ•°ç™¾ä¸ªé•œå¤´ä¸Šè¾¾åˆ°éšæœºæ£®æ—çº§åˆ«çš„ç²¾åº¦ã€‚åŒæ—¶ä¿ç•™äº†ä¸€èˆ¬èŠå¤©èƒ½åŠ›ï¼ŒåŒ…æ‹¬çŸ¥è¯†å’Œæ¨ç†ï¼Œåœ¨MMLUä¸Šè¾¾åˆ°75.4%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå…·å¤‡å¹¿æ³›çŸ¥è¯†å’Œå¼ºå¤§é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ ‡å‡†MLä»»åŠ¡ä¸Šéš¾ä»¥ä»å¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­å­¦ä¹ ã€‚</li>
<li>MachineLearningLMæ˜¯ä¸€ä¸ªä¾¿æºçš„ç»§ç»­é¢„è®­ç»ƒæ¡†æ¶ï¼Œä¸ºLLMèµ‹äºˆä¸Šä¸‹æ–‡MLèƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™å…¶ä¸€èˆ¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>é¢„è®­ç»ƒç¨‹åºé€šè¿‡åˆæˆæ¥è‡ªSCMçš„MLä»»åŠ¡æ¥å¢å¼ºLLMçš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œæ¶µç›–ç¤ºä¾‹æ•°é‡é«˜è¾¾1024ã€‚</li>
<li>ä½¿ç”¨éšæœºæ£®æ—æ•™å¸ˆæ¥åŠ å¼ºLLMçš„æ•°å€¼å»ºæ¨¡ç¨³å¥æ€§ã€‚</li>
<li>MachineLearningLMåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­é€šè¿‡é«˜æ•ˆçš„æç¤ºåºåˆ—åŒ–ç¤ºä¾‹ï¼Œæé«˜ååé‡ã€‚</li>
<li>MachineLearningLMåœ¨å¤šä¸ªé¢†åŸŸçš„ç¦»åˆ†å¸ƒè¡¨æ ¼åˆ†ç±»ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–LLMåŸºå‡†æµ‹è¯•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06806">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.06806v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.06806v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.06806v4/page_4_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Cut-Costs-Not-Accuracy-LLM-Powered-Data-Processing-with-Guarantees"><a href="#Cut-Costs-Not-Accuracy-LLM-Powered-Data-Processing-with-Guarantees" class="headerlink" title="Cut Costs, Not Accuracy: LLM-Powered Data Processing with Guarantees"></a>Cut Costs, Not Accuracy: LLM-Powered Data Processing with Guarantees</h2><p><strong>Authors:Sepanta Zeighami, Shreya Shankar, Aditya Parameswaran</strong></p>
<p>Large Language Models (LLMs) are being increasingly used as a building block in data systems to process large text datasets. To do so, LLM model providers offer multiple LLMs with different sizes, spanning various cost-quality trade-offs when processing text at scale. Top-of-the-line LLMs (e.g., GPT-4o, Claude Sonnet) operate with high accuracy but are prohibitively expensive when processing many records. To avoid high costs, more affordable but lower quality LLMs (e.g., GPT-4o-mini, Claude Haiku) can be used to process records, but we need to ensure that the overall accuracy does not deviate substantially from that of the top-of-the-line LLMs. The model cascade framework provides a blueprint to manage this trade-off, by using the confidence of LLMs in their output (e.g., log-probabilities) to decide on which records to use the affordable LLM. However, existing solutions following this framework provide only marginal cost savings and weak theoretical guarantees because of poor estimation of the quality of the affordable LLMâ€™s outputs. We present BARGAIN, a method that judiciously uses affordable LLMs in data processing to significantly reduce cost while providing strong theoretical guarantees on the solution quality. BARGAIN employs a novel adaptive sampling strategy and statistical estimation procedure that uses data and task characteristics and builds on recent statistical tools to make accurate estimations with tight theoretical guarantees. Variants of BARGAIN can support guarantees on accuracy, precision, or recall of the output. Experimental results across 8 real-world datasets show that BARGAIN reduces cost, on average, by up to 86% more than state-of-the-art, while providing stronger theoretical guarantees on accuracy of output, with similar gains when guaranteeing a desired level of precision or recall. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ­£è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨ä½œæ•°æ®ç³»ç»Ÿçš„æ„å»ºå—ï¼Œä»¥å¤„ç†å¤§å‹æ–‡æœ¬æ•°æ®é›†ã€‚ä¸ºæ­¤ï¼ŒLLMæ¨¡å‹æä¾›å•†æä¾›å¤šç§ä¸åŒå¤§å°çš„LLMï¼Œåœ¨å¤„ç†å¤§è§„æ¨¡æ–‡æœ¬æ—¶ï¼Œè·¨è¶Šå„ç§æˆæœ¬è´¨é‡æƒè¡¡ã€‚é¡¶çº§LLMï¼ˆä¾‹å¦‚GPT-4oã€Claude Sonnetï¼‰æ“ä½œå‡†ç¡®åº¦é«˜ï¼Œä½†åœ¨å¤„ç†å¤§é‡è®°å½•æ—¶æˆæœ¬é«˜æ˜‚ã€‚ä¸ºé¿å…é«˜æˆæœ¬ï¼Œå¯ä»¥ä½¿ç”¨æ›´ç»æµå®æƒ ä½†è´¨é‡è¾ƒä½çš„LLMï¼ˆä¾‹å¦‚GPT-4o-miniã€Claude Haikuï¼‰æ¥å¤„ç†è®°å½•ï¼Œä½†æˆ‘ä»¬éœ€è¦ç¡®ä¿æ€»ä½“å‡†ç¡®åº¦ä¸ä¼šä¸é¡¶çº§LLMäº§ç”Ÿå¤ªå¤§åå·®ã€‚æ¨¡å‹çº§è”æ¡†æ¶æä¾›äº†ä¸€ä¸ªç®¡ç†è¿™ç§æƒè¡¡çš„è“å›¾ï¼Œåˆ©ç”¨LLMå¯¹å…¶è¾“å‡ºçš„ä¿¡å¿ƒï¼ˆä¾‹å¦‚æ—¥å¿—æ¦‚ç‡ï¼‰æ¥å†³å®šä½¿ç”¨ç»æµå®æƒ çš„LLMå¤„ç†å“ªäº›è®°å½•ã€‚ç„¶è€Œï¼Œéµå¾ªæ­¤æ¡†æ¶çš„ç°æœ‰è§£å†³æ–¹æ¡ˆä»…æä¾›å¾®ä¸è¶³é“çš„æˆæœ¬èŠ‚çº¦å’Œè–„å¼±çš„ç†è®ºä¿è¯ï¼Œå› ä¸ºå¯¹ç»æµå®æƒ çš„LLMè¾“å‡ºçš„è´¨é‡ä¼°è®¡ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†BARGAINæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å®¡æ…åœ°ä½¿ç”¨ç»æµå®æƒ çš„LLMè¿›è¡Œæ•°æ®å¤„ç†ï¼Œä»¥æ˜¾è‘—é™ä½æˆæœ¬ï¼ŒåŒæ—¶æä¾›å…³äºè§£å†³æ–¹æ¡ˆè´¨é‡çš„å¼ºå¤§ç†è®ºä¿è¯ã€‚BARGAINé‡‡ç”¨äº†ä¸€ç§æ–°å‹è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥å’Œç»Ÿè®¡ä¼°ç®—ç¨‹åºï¼Œè¯¥ç¨‹åºåˆ©ç”¨æ•°æ®å’Œä»»åŠ¡ç‰¹å¾ï¼Œå¹¶åŸºäºæœ€æ–°ç»Ÿè®¡å·¥å…·è¿›è¡Œå‡†ç¡®ä¼°ç®—ï¼Œæä¾›ä¸¥å¯†çš„ç†è®ºä¿è¯ã€‚BARGAINçš„å˜ä½“å¯ä»¥æ”¯æŒè¾“å‡ºå‡†ç¡®æ€§ã€ç²¾åº¦æˆ–å¬å›ç‡çš„ä¿è¯ã€‚åœ¨8ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒBARGAINåœ¨å¹³å‡æˆæœ¬ä¸Šæ¯”æœ€æ–°æŠ€æœ¯æœ€å¤šå‡å°‘86%ï¼ŒåŒæ—¶æä¾›å…³äºè¾“å‡ºå‡†ç¡®æ€§çš„æ›´å¼ºç†è®ºä¿è¯ï¼Œåœ¨ä¿éšœæ‰€éœ€çš„ç²¾åº¦æˆ–å¬å›ç‡æ—¶ä¹Ÿèƒ½å–å¾—ç±»ä¼¼çš„æ”¶ç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.02896v2">PDF</a> To appear in SIGMODâ€™26</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«è¶Šæ¥è¶Šå¤šåœ°ç”¨ä½œæ•°æ®å¤„ç†ç³»ç»Ÿçš„æ„å»ºæ¨¡å—ï¼Œä»¥å¤„ç†å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®ã€‚ä¸åŒè§„æ¨¡å’Œæˆæœ¬çš„LLMæä¾›äº†ä¸åŒçš„æˆæœ¬è´¨é‡æƒè¡¡é€‰æ‹©ã€‚é«˜ç«¯LLMï¼ˆå¦‚GPT-4oã€Claude Sonnetï¼‰å¤„ç†è®°å½•æ—¶ç²¾åº¦é«˜ä½†æˆæœ¬é«˜æ˜‚ã€‚ä¸ºé¿å…é«˜æ˜‚æˆæœ¬ï¼Œå¯ä½¿ç”¨æ€§ä»·æ¯”æ›´é«˜ä½†è´¨é‡è¾ƒä½çš„LLMï¼ˆå¦‚GPT-4o-miniã€Claude Haikuï¼‰å¤„ç†è®°å½•ï¼Œä½†éœ€è¦ç¡®ä¿æ€»ä½“ç²¾åº¦ä¸ä¼šä¸é«˜ç«¯LLMæœ‰è¾ƒå¤§åå·®ã€‚æ¨¡å‹çº§è”æ¡†æ¶é€šè¿‡åˆ©ç”¨LLMå¯¹å…¶è¾“å‡ºçš„ä¿¡å¿ƒï¼ˆå¦‚å¯¹æ•°æ¦‚ç‡ï¼‰æ¥å†³å®šå“ªäº›è®°å½•ä½¿ç”¨æ€§ä»·æ¯”LLMæ¥å¤„ç†ï¼Œä¸ºè§£å†³æ­¤æƒè¡¡æä¾›äº†è“å›¾ã€‚ç„¶è€Œï¼Œç°æœ‰éµå¾ªæ­¤æ¡†æ¶çš„è§£å†³æ–¹æ¡ˆä»…æä¾›æœ‰é™çš„æˆæœ¬èŠ‚çº¦å’Œè–„å¼±çš„ç†è®ºä¿è¯ï¼Œå› ä¸ºå¯¹æ€§ä»·æ¯”LLMçš„è¾“å‡ºè´¨é‡ä¼°ç®—ä¸ä½³ã€‚æœ¬æ–‡æå‡ºBARGAINæ–¹æ³•ï¼Œæ™ºèƒ½åœ°ä½¿ç”¨æ€§ä»·æ¯”LLMè¿›è¡Œæ•°æ®å¤„ç†ï¼Œä»¥é™ä½æˆæœ¬ï¼ŒåŒæ—¶æä¾›å…³äºè§£å†³æ–¹æ¡ˆè´¨é‡çš„å¼ºå¤§ç†è®ºä¿è¯ã€‚BARGAINé‡‡ç”¨æ–°é¢–çš„è‡ªé€‚åº”é‡‡æ ·ç­–ç•¥å’Œç»Ÿè®¡ä¼°ç®—ç¨‹åºï¼Œåˆ©ç”¨æ•°æ®å’Œä»»åŠ¡ç‰¹å¾ï¼Œå¹¶åŸºäºæœ€æ–°ç»Ÿè®¡å·¥å…·è¿›è¡Œå‡†ç¡®ä¼°ç®—ï¼Œæä¾›ä¸¥è°¨çš„ç†è®ºä¿è¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨8ä¸ªçœŸå®æ•°æ®é›†ä¸Šï¼ŒBARGAINåœ¨é™ä½æˆæœ¬æ–¹é¢å¹³å‡æ¯”æœ€æ–°æŠ€æœ¯é«˜å‡ºé«˜è¾¾86%ï¼ŒåŒæ—¶åœ¨è¾“å‡ºå‡†ç¡®æ€§æ–¹é¢æä¾›æ›´å¼ºå¤§çš„ç†è®ºä¿è¯ï¼Œåœ¨ä¿éšœæ‰€éœ€çš„ç²¾åº¦æˆ–å¬å›ç‡æ—¶ä¹Ÿèƒ½å–å¾—ç›¸ä¼¼æ”¶ç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMè¢«ç”¨ä½œæ•°æ®å¤„ç†ç³»ç»Ÿçš„å…³é”®ç»„ä»¶ï¼Œå¤„ç†å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®ã€‚</li>
<li>é«˜ç«¯LLMæä¾›é«˜å‡†ç¡®ç‡ä½†æˆæœ¬é«˜æ˜‚ï¼Œè€Œæ€§ä»·æ¯”LLMå¯ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆã€‚</li>
<li>æ¨¡å‹çº§è”æ¡†æ¶é€šè¿‡åˆ©ç”¨LLMçš„ä¿¡å¿ƒæ¥å†³å®šè®°å½•å¤„ç†ç­–ç•¥ã€‚</li>
<li>ç°æœ‰è§£å†³æ–¹æ¡ˆåœ¨ä¼°ç®—æ€§ä»·æ¯”LLMè¾“å‡ºè´¨é‡æ–¹é¢è¡¨ç°ä¸ä½³ï¼Œå¯¼è‡´æˆæœ¬èŠ‚çº¦æœ‰é™å’Œç†è®ºä¿è¯è–„å¼±ã€‚</li>
<li>BARGAINæ–¹æ³•æ™ºèƒ½ä½¿ç”¨æ€§ä»·æ¯”LLMï¼Œé™ä½æˆæœ¬å¹¶æä¾›å¼ºå¤§çš„ç†è®ºä¿è¯ã€‚</li>
<li>BARGAINé‡‡ç”¨è‡ªé€‚åº”é‡‡æ ·å’Œç»Ÿè®¡ä¼°ç®—ï¼Œåˆ©ç”¨æ•°æ®å’Œä»»åŠ¡ç‰¹æ€§è¿›è¡Œå‡†ç¡®ä¼°ç®—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02896">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.02896v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.02896v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.02896v2/page_1_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.02896v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.02896v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="LMAR-Language-Model-Augmented-Retriever-for-Domain-specific-Knowledge-Indexing"><a href="#LMAR-Language-Model-Augmented-Retriever-for-Domain-specific-Knowledge-Indexing" class="headerlink" title="LMAR: Language Model Augmented Retriever for Domain-specific Knowledge   Indexing"></a>LMAR: Language Model Augmented Retriever for Domain-specific Knowledge   Indexing</h2><p><strong>Authors:Yao Zhao, Yantian Ding, Zhiyue Zhang, Dapeng Yao, Yanxun Xu</strong></p>
<p>Retrieval Augmented Generation (RAG) systems often struggle with domain-specific knowledge due to performance deterioration of pre-trained embeddings and prohibitive computational costs of large language model (LLM)-based retrievers. While fine-tuning data augmentation embedding models offers a promising direction, its effectiveness is limited by the need for high-quality training data and reliable chunking strategies that preserve contextual integrity. We propose LMAR (Language Model Augmented Retriever), a model-agnostic framework that addresses these challenges by combining LLM-guided data synthesis with contrastive embedding adaptation and efficient text clustering. LMAR consists of a two-stage pipeline: (1) Triplet sampling and synthetic data augmentation, where LLMs act as both labeler and validator to ensure high-fidelity supervision throughout the pipeline. Experimental results across multiple domain-specific benchmark datasets demonstrate that LMAR outperforms multiple baseline models, while maintaining moderate hardware requirements and low latency. Its model-agnostic nature further enables seamless integration with emerging RAG architectures and text embedding models, ensuring continual improvements without redesigning the pipeline. These results highlight LMAR as a practical and cost-effective solution for scalable domain-specific adaptation. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿç”±äºé¢„è®­ç»ƒåµŒå…¥æ¨¡å‹æ€§èƒ½ä¸‹é™å’ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢å™¨è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œå¸¸å¸¸é¢ä¸´ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„æŒ‘æˆ˜ã€‚è™½ç„¶å¾®è°ƒæ•°æ®å¢å¼ºåµŒå…¥æ¨¡å‹æä¾›äº†ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ï¼Œä½†å…¶æœ‰æ•ˆæ€§å—é™äºéœ€è¦é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®å’Œå¯é çš„ä¿ç•™ä¸Šä¸‹æ–‡å®Œæ•´æ€§çš„åˆ†å—ç­–ç•¥ã€‚æˆ‘ä»¬æå‡ºäº†LMARï¼ˆè¯­è¨€æ¨¡å‹å¢å¼ºæ£€ç´¢å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å‹æ— å…³æ¡†æ¶ï¼Œé€šè¿‡ç»“åˆLLMå¼•å¯¼çš„æ•°æ®åˆæˆã€å¯¹æ¯”åµŒå…¥é€‚åº”å’Œé«˜æ•ˆæ–‡æœ¬èšç±»æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚LMARç”±ä¸¤é˜¶æ®µç®¡é“ç»„æˆï¼šï¼ˆ1) ä¸‰å…ƒç»„é‡‡æ ·å’Œåˆæˆæ•°æ®å¢å¼ºï¼Œå…¶ä¸­LLMæ—¢ä½œä¸ºæ ‡ç­¾å™¨åˆä½œä¸ºéªŒè¯å™¨ï¼Œä»¥ç¡®ä¿ç®¡é“ä¸­çš„é«˜ä¿çœŸç›‘ç£ã€‚åœ¨å¤šä¸ªç‰¹å®šé¢†åŸŸçš„åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLMARä¼˜äºå¤šä¸ªåŸºå‡†æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒé€‚åº¦çš„ç¡¬ä»¶è¦æ±‚å’Œä½å»¶è¿Ÿã€‚å…¶æ¨¡å‹æ— å…³çš„ç‰¹æ€§è¿›ä¸€æ­¥å®ç°äº†ä¸æ–°å…´RAGæ¶æ„å’Œæ–‡æœ¬åµŒå…¥æ¨¡å‹çš„æ— ç¼é›†æˆï¼Œç¡®ä¿äº†æŒç»­æ”¹è¿›è€Œæ— éœ€é‡æ–°è®¾è®¡ç®¡é“ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†LMARä½œä¸ºå®ç”¨å’Œæˆæœ¬æ•ˆç›Šé«˜çš„å¯æ‰©å±•ç‰¹å®šé¢†åŸŸé€‚åº”è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05672v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LMARæ¡†æ¶é€šè¿‡ç»“åˆLLMå¼•å¯¼çš„æ•°æ®åˆæˆã€å¯¹æ¯”åµŒå…¥é€‚é…å’Œé«˜æ•ˆæ–‡æœ¬èšç±»ï¼Œè§£å†³äº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨ç‰¹å®šé¢†åŸŸçŸ¥è¯†æ–¹é¢é¢ä¸´çš„é—®é¢˜ã€‚LMARé‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“ï¼ŒåŒ…æ‹¬ä¸‰å…ƒç»„é‡‡æ ·å’Œåˆæˆæ•°æ®å¢å¼ºï¼ŒLLMsä½œä¸ºæ ‡ç­¾å™¨å’ŒéªŒè¯å™¨ï¼Œç¡®ä¿ç®¡é“ä¸­çš„é«˜ä¿çœŸç›‘ç£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLMARåœ¨å¤šä¸ªç‰¹å®šé¢†åŸŸçš„åŸºå‡†æ•°æ®é›†ä¸Šä¼˜äºå¤šä¸ªåŸºçº¿æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒé€‚ä¸­çš„ç¡¬ä»¶è¦æ±‚å’Œä½å»¶è¿Ÿã€‚å…¶æ¨¡å‹æ— å…³çš„ç‰¹æ€§ä½¿å¾—å®ƒèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°æ–°å…´çš„RAGæ¶æ„å’Œæ–‡æœ¬åµŒå…¥æ¨¡å‹ä¸­ï¼Œç¡®ä¿æŒç»­æ”¹è¿›è€Œæ— éœ€é‡æ–°è®¾è®¡ç®¡é“ã€‚è¿™äº›ç»“æœå‡¸æ˜¾äº†LMARåœ¨å¯ä¼¸ç¼©çš„ç‰¹å®šé¢†åŸŸé€‚é…ä¸­çš„å®ç”¨æ€§å’Œæˆæœ¬æ•ˆç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LMARæ¡†æ¶è§£å†³äº†RAGç³»ç»Ÿåœ¨ç‰¹å®šé¢†åŸŸçŸ¥è¯†æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>LMARé€šè¿‡ç»“åˆLLMå¼•å¯¼çš„æ•°æ®åˆæˆã€å¯¹æ¯”åµŒå…¥é€‚é…å’Œé«˜æ•ˆæ–‡æœ¬èšç±»å®ç°æ€§èƒ½æå‡ã€‚</li>
<li>LMARé‡‡ç”¨ä¸¤é˜¶æ®µç®¡é“ï¼ŒåŒ…æ‹¬ä¸‰å…ƒç»„é‡‡æ ·å’Œåˆæˆæ•°æ®å¢å¼ºï¼Œç¡®ä¿é«˜ä¿çœŸç›‘ç£ã€‚</li>
<li>LMARåœ¨å¤šä¸ªç‰¹å®šé¢†åŸŸçš„åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜äºåŸºçº¿æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>LMARå…·æœ‰é€‚ä¸­çš„ç¡¬ä»¶è¦æ±‚å’Œä½å»¶è¿Ÿï¼Œä½¿å…¶æ›´å…·å®ç”¨æ€§ã€‚</li>
<li>LMARçš„æ¨¡å‹æ— å…³ç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿæ— ç¼é›†æˆåˆ°æ–°å…´çš„RAGæ¶æ„å’Œæ–‡æœ¬åµŒå…¥æ¨¡å‹ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2508.05672v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2508.05672v2/page_1_0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Dynamic-Motion-Blending-for-Versatile-Motion-Editing"><a href="#Dynamic-Motion-Blending-for-Versatile-Motion-Editing" class="headerlink" title="Dynamic Motion Blending for Versatile Motion Editing"></a>Dynamic Motion Blending for Versatile Motion Editing</h2><p><strong>Authors:Nan Jiang, Hongjie Li, Ziye Yuan, Zimo He, Yixin Chen, Tengyu Liu, Yixin Zhu, Siyuan Huang</strong></p>
<p>Text-guided motion editing enables high-level semantic control and iterative modifications beyond traditional keyframe animation. Existing methods rely on limited pre-collected training triplets, which severely hinders their versatility in diverse editing scenarios. We introduce MotionCutMix, an online data augmentation technique that dynamically generates training triplets by blending body part motions based on input text. While MotionCutMix effectively expands the training distribution, the compositional nature introduces increased randomness and potential body part incoordination. To model such a rich distribution, we present MotionReFit, an auto-regressive diffusion model with a motion coordinator. The auto-regressive architecture facilitates learning by decomposing long sequences, while the motion coordinator mitigates the artifacts of motion composition. Our method handles both spatial and temporal motion edits directly from high-level human instructions, without relying on additional specifications or Large Language Models. Through extensive experiments, we show that MotionReFit achieves state-of-the-art performance in text-guided motion editing. </p>
<blockquote>
<p>æ–‡æœ¬å¼•å¯¼çš„è¿åŠ¨ç¼–è¾‘å®ç°äº†é«˜çº§è¯­ä¹‰æ§åˆ¶å’Œä¼ ç»Ÿå…³é”®å¸§åŠ¨ç”»ä¹‹å¤–çš„è¿­ä»£ä¿®æ”¹ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºæœ‰é™é¢„æ”¶é›†çš„è®­ç»ƒä¸‰å…ƒç»„ï¼Œè¿™ä¸¥é‡é™åˆ¶äº†å…¶åœ¨å¤šæ ·åŒ–ç¼–è¾‘åœºæ™¯ä¸­çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†MotionCutMixï¼Œè¿™æ˜¯ä¸€ç§åœ¨çº¿æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œé€šè¿‡åŸºäºè¾“å…¥æ–‡æœ¬èåˆèº«ä½“éƒ¨ä½çš„è¿åŠ¨æ¥åŠ¨æ€ç”Ÿæˆè®­ç»ƒä¸‰å…ƒç»„ã€‚è™½ç„¶MotionCutMixæœ‰æ•ˆåœ°æ‰©å¤§äº†è®­ç»ƒåˆ†å¸ƒï¼Œä½†ç»„åˆæ€§è´¨å¢åŠ äº†éšæœºæ€§å’Œæ½œåœ¨çš„èº«ä½“éƒ¨ä½ä¸åè°ƒã€‚ä¸ºäº†æ¨¡æ‹Ÿè¿™ç§ä¸°å¯Œçš„åˆ†å¸ƒï¼Œæˆ‘ä»¬æå‡ºäº†MotionReFitï¼Œè¿™æ˜¯ä¸€ç§å¸¦æœ‰è¿åŠ¨åè°ƒå™¨çš„è‡ªå›å½’æ‰©æ•£æ¨¡å‹ã€‚è‡ªå›å½’æ¶æ„é€šè¿‡åˆ†è§£é•¿åºåˆ—æ¥ä¿ƒè¿›å­¦ä¹ ï¼Œè€Œè¿åŠ¨åè°ƒå™¨å‡è½»äº†è¿åŠ¨ç»„åˆçš„ä¼ªå½±ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›´æ¥æ ¹æ®é«˜çº§äººç±»æŒ‡ä»¤å¤„ç†ç©ºé—´å’Œæ—¶é—´çš„è¿åŠ¨ç¼–è¾‘ï¼Œæ— éœ€ä¾èµ–é¢å¤–çš„è§„èŒƒæˆ–å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡å¤§é‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†MotionReFitåœ¨æ–‡æœ¬å¼•å¯¼çš„è¿åŠ¨ç¼–è¾‘æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.20724v2">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬å¼•å¯¼çš„è¿åŠ¨ç¼–è¾‘å®ç°äº†é«˜çº§è¯­ä¹‰æ§åˆ¶å’Œè¿­ä»£ä¿®æ”¹ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿå…³é”®å¸§åŠ¨ç”»çš„é™åˆ¶ã€‚ç°æœ‰çš„æ–¹æ³•ä¾èµ–äºæœ‰é™çš„é¢„æ”¶é›†è®­ç»ƒä¸‰å…ƒç»„ï¼Œä¸¥é‡é™åˆ¶äº†å…¶åœ¨å¤šæ ·åŒ–ç¼–è¾‘åœºæ™¯ä¸­çš„çµæ´»æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†MotionCutMixï¼Œä¸€ç§åŸºäºè¾“å…¥æ–‡æœ¬åœ¨çº¿ç”Ÿæˆè®­ç»ƒä¸‰å…ƒç»„çš„æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚è™½ç„¶MotionCutMixæœ‰æ•ˆåœ°æ‰©å±•äº†è®­ç»ƒåˆ†å¸ƒï¼Œä½†å…¶ç»„åˆæ€§è´¨å¢åŠ äº†éšæœºæ€§å’Œæ½œåœ¨çš„è‚¢ä½“ä¸åè°ƒã€‚ä¸ºäº†æ¨¡æ‹Ÿè¿™ç§ä¸°å¯Œçš„åˆ†å¸ƒï¼Œæˆ‘ä»¬æå‡ºäº†MotionReFitï¼Œä¸€ç§å…·æœ‰è¿åŠ¨åè°ƒåŠŸèƒ½çš„è‡ªå›å½’æ‰©æ•£æ¨¡å‹ã€‚è‡ªå›å½’æ¶æ„é€šè¿‡åˆ†è§£é•¿åºåˆ—ä¿ƒè¿›å­¦ä¹ ï¼Œè€Œè¿åŠ¨åè°ƒå™¨å‡è½»äº†è¿åŠ¨ç»„åˆçš„ä¼ªåƒã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›´æ¥ä»é«˜çº§äººç±»æŒ‡ä»¤å¤„ç†ç©ºé—´å’Œæ—¶é—´è¿åŠ¨ç¼–è¾‘ï¼Œæ— éœ€ä¾èµ–é¢å¤–çš„è§„èŒƒæˆ–å¤§è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†MotionReFitåœ¨æ–‡æœ¬å¼•å¯¼çš„è¿åŠ¨ç¼–è¾‘æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬å¼•å¯¼çš„è¿åŠ¨ç¼–è¾‘è¶…è¶Šäº†ä¼ ç»Ÿå…³é”®å¸§åŠ¨ç”»çš„é™åˆ¶ï¼Œæä¾›äº†é«˜çº§è¯­ä¹‰æ§åˆ¶å’Œè¿­ä»£ä¿®æ”¹çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–äºæœ‰é™çš„é¢„æ”¶é›†è®­ç»ƒä¸‰å…ƒç»„ï¼Œé™åˆ¶äº†å…¶åœ¨å¤šæ ·åŒ–ç¼–è¾‘åœºæ™¯ä¸­çš„çµæ´»æ€§ã€‚</li>
<li>å¼•å…¥äº†MotionCutMixæ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œé€šè¿‡æ··åˆèº«ä½“éƒ¨ä½çš„è¿åŠ¨åŠ¨æ€ç”Ÿæˆè®­ç»ƒä¸‰å…ƒç»„ã€‚</li>
<li>MotionCutMixè™½ç„¶æœ‰æ•ˆæ‰©å±•äº†è®­ç»ƒåˆ†å¸ƒï¼Œä½†å­˜åœ¨éšæœºæ€§å’Œæ½œåœ¨çš„èº«ä½“éƒ¨ä½ä¸åè°ƒé—®é¢˜ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†MotionReFitæ¨¡å‹ï¼Œç»“åˆäº†è‡ªå›å½’æ‰©æ•£æ¨¡å‹å’Œè¿åŠ¨åè°ƒåŠŸèƒ½ã€‚</li>
<li>è‡ªå›å½’æ¶æ„æœ‰åŠ©äºåˆ†è§£é•¿åºåˆ—çš„å­¦ä¹ ï¼Œè€Œè¿åŠ¨åè°ƒå™¨åˆ™å‡è½»äº†è¿åŠ¨ç»„åˆçš„ä¼ªåƒã€‚</li>
<li>Method can handle spatial and temporal motion edits directly from high-level human instructions without relying on additional specifications or Large Language Models, and achieves state-of-the-art performance in text-guided motion editing.</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.20724">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2503.20724v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2503.20724v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2503.20724v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2503.20724v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2503.20724v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Atomic-Fact-Decomposition-Helps-Attributed-Question-Answering"><a href="#Atomic-Fact-Decomposition-Helps-Attributed-Question-Answering" class="headerlink" title="Atomic Fact Decomposition Helps Attributed Question Answering"></a>Atomic Fact Decomposition Helps Attributed Question Answering</h2><p><strong>Authors:Zhichao Yan, Jiapu Wang, Jiaoyan Chen, Xiaoli Li, Ru Li, Jeff Z. Pan</strong></p>
<p>Attributed Question Answering (AQA) aims to provide both a trustworthy answer and a reliable attribution report for a given question. Retrieval is a widely adopted approach, including two general paradigms: Retrieval-Then-Read (RTR) and post-hoc retrieval. Recently, Large Language Models (LLMs) have shown remarkable proficiency, prompting growing interest in AQA among researchers. However, RTR-based AQA often suffers from irrelevant knowledge and rapidly changing information, even when LLMs are adopted, while post-hoc retrieval-based AQA struggles with comprehending long-form answers with complex logic, and precisely identifying the content needing revision and preserving the original intent. To tackle these problems, this paper proposes an Atomic fact decomposition-based Retrieval and Editing (ARE) framework, which decomposes the generated long-form answers into molecular clauses and atomic facts by the instruction-tuned LLMs. Notably, the instruction-tuned LLMs are fine-tuned using a well-constructed dataset, generated from large scale Knowledge Graphs (KGs). This process involves extracting one-hop neighbors from a given set of entities and transforming the result into coherent long-form text. Subsequently, ARE leverages a search engine to retrieve evidences related to atomic facts, inputting these evidences into an LLM-based verifier to determine whether the facts require expansion for re-retrieval or editing. Furthermore, the edited facts are backtracked into the original answer, with evidence aggregated based on the relationship between molecular clauses and atomic facts. Extensive evaluations demonstrate the superior performance of our proposed method over the state-of-the-arts on several datasets, with an additionally proposed new metric $Attr_{p}$ for evaluating the precision of evidence attribution. </p>
<blockquote>
<p>å½’å› é—®ç­”ï¼ˆAQAï¼‰æ—¨åœ¨é’ˆå¯¹ç»™å®šé—®é¢˜æä¾›å¯ä¿¡çš„ç­”æ¡ˆå’Œå¯é çš„å½’å› æŠ¥å‘Šã€‚æ£€ç´¢æ˜¯ä¸€ç§å¹¿æ³›é‡‡ç”¨çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸¤ç§é€šç”¨èŒƒå¼ï¼šå…ˆæ£€ç´¢å†é˜…è¯»ï¼ˆRTRï¼‰å’Œäº‹åæ£€ç´¢ã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ä¸“ä¸šèƒ½åŠ›ï¼Œæ¿€å‘äº†ç ”ç©¶è€…å¯¹AQAçš„å…´è¶£ã€‚ç„¶è€Œï¼ŒåŸºäºRTRçš„AQAç»å¸¸å—åˆ°ä¸ç›¸å…³çŸ¥è¯†å’Œå¿«é€Ÿå˜åŒ–ä¿¡æ¯çš„å½±å“ï¼Œå³ä½¿é‡‡ç”¨LLMï¼Œè€ŒåéªŒæ£€ç´¢çš„AQAåˆ™éš¾ä»¥ç†è§£é•¿å½¢å¼ç­”æ¡ˆä¸­çš„å¤æ‚é€»è¾‘ï¼Œå¹¶å‡†ç¡®è¯†åˆ«éœ€è¦ä¿®è®¢çš„å†…å®¹ä»¥åŠä¿ç•™åŸå§‹æ„å›¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºåŸå­äº‹å®åˆ†è§£çš„æ£€ç´¢å’Œç¼–è¾‘ï¼ˆAREï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æŒ‡ä»¤è°ƒä¼˜çš„LLMå°†ç”Ÿæˆçš„é•¿å½¢å¼ç­”æ¡ˆåˆ†è§£ä¸ºåˆ†å­å­å¥å’ŒåŸå­äº‹å®ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒæŒ‡ä»¤è°ƒä¼˜çš„LLMæ˜¯ä½¿ç”¨ä»å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ç”Ÿæˆçš„æ„å»ºè‰¯å¥½çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚è¿™ä¸€è¿‡ç¨‹æ¶‰åŠä»ç»™å®šå®ä½“é›†ä¸­æå–å•è·³é‚»å±…ï¼Œå¹¶å°†ç»“æœè½¬æ¢ä¸ºè¿è´¯çš„é•¿æ–‡æœ¬å½¢å¼ã€‚éšåï¼ŒAREåˆ©ç”¨æœç´¢å¼•æ“æ£€ç´¢ä¸åŸå­äº‹å®ç›¸å…³çš„è¯æ®ï¼Œå°†è¿™äº›è¯æ®è¾“å…¥åŸºäºLLMçš„éªŒè¯å™¨ï¼Œä»¥ç¡®å®šäº‹å®æ˜¯å¦éœ€è¦æ‰©å±•ä»¥è¿›è¡Œé‡æ–°æ£€ç´¢æˆ–ç¼–è¾‘ã€‚æ­¤å¤–ï¼Œç¼–è¾‘åçš„äº‹å®ä¼šå›æº¯åˆ°åŸå§‹ç­”æ¡ˆä¸­ï¼Œå¹¶æ ¹æ®åˆ†å­å­å¥å’ŒåŸå­äº‹å®ä¹‹é—´çš„å…³ç³»å¯¹è¯æ®è¿›è¡Œèšåˆã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºæœ€æ–°æŠ€æœ¯ï¼Œå¹¶é¢å¤–æå‡ºäº†ä¸€ç§æ–°çš„åº¦é‡æ ‡å‡†$Attr_{p}$æ¥è¯„ä¼°è¯æ®å½’å› çš„å‡†ç¡®æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.16708v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>AQAæ—¨åœ¨é’ˆå¯¹ç»™å®šé—®é¢˜æä¾›å¯ä¿¡çš„ç­”æ¡ˆå’Œå¯é çš„å½’å› æŠ¥å‘Šã€‚æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨AQAæ–¹é¢çš„è¡¨ç°å¼•äººæ³¨ç›®ï¼Œå¼•å‘äº†ç ”ç©¶äººå‘˜çš„å…´è¶£ã€‚ç„¶è€Œï¼ŒåŸºäºRTRçš„AQAå¸¸å¸¸é¢ä¸´çŸ¥è¯†ä¸ç›¸å…³å’Œå¿«é€Ÿå˜åŒ–çš„ä¿¡æ¯é—®é¢˜ï¼Œå³ä½¿é‡‡ç”¨LLMä¹Ÿæ˜¯å¦‚æ­¤ï¼Œè€Œåæ£€ç´¢çš„AQAåˆ™éš¾ä»¥å¤„ç†é€»è¾‘å¤æ‚çš„é•¿ç­”æ¡ˆï¼Œéš¾ä»¥ç²¾ç¡®è¯†åˆ«éœ€è¦ä¿®è®¢çš„å†…å®¹å¹¶ä¿æŒåŸå§‹æ„å›¾ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åŸºäºåŸå­äº‹å®åˆ†è§£çš„æ£€ç´¢å’Œç¼–è¾‘ï¼ˆAREï¼‰æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨æŒ‡ä»¤è®­ç»ƒè¿‡çš„LLMå°†ç”Ÿæˆçš„é•¿ç­”æ¡ˆåˆ†è§£ä¸ºåˆ†å­å­å¥å’ŒåŸå­äº‹å®ï¼Œå¹¶ä½¿ç”¨ä»å¤§è§„æ¨¡çŸ¥è¯†å›¾è°±ç”Ÿæˆçš„å¤§å‹æ•°æ®é›†å¯¹LLMè¿›è¡Œå¾®è°ƒã€‚AREåˆ©ç”¨æœç´¢å¼•æ“æ£€ç´¢ä¸åŸå­äº‹å®ç›¸å…³çš„è¯æ®ï¼Œå¹¶ä½¿ç”¨LLMéªŒè¯å™¨ç¡®å®šäº‹å®æ˜¯å¦éœ€è¦é‡æ–°æ£€ç´¢æˆ–ç¼–è¾‘ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œç›¸è¾ƒäºå‡ ç§æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæœ¬æ–¹æ³•åœ¨æŸäº›æ•°æ®é›†ä¸Šè¡¨ç°æ›´ä½³ï¼Œå¹¶æå‡ºæ–°çš„è¯æ®å½’å› è¯„ä¼°æŒ‡æ ‡$Attr_{p}$ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>AQAçš„ç›®æ ‡æ˜¯æä¾›å¯ä¿¡çš„ç­”æ¡ˆå’Œå¯é çš„å½’å› æŠ¥å‘Šã€‚</li>
<li>LLMåœ¨AQAæ–¹é¢çš„è¡¨ç°å¼•èµ·äº†ç ”ç©¶è€…çš„å¹¿æ³›å…³æ³¨ã€‚</li>
<li>åŸºäºRTRçš„AQAé¢ä¸´çŸ¥è¯†ä¸ç›¸å…³å’Œå¿«é€Ÿå˜åŒ–ä¿¡æ¯çš„æŒ‘æˆ˜ã€‚</li>
<li>åæ£€ç´¢çš„AQAéš¾ä»¥å¤„ç†é€»è¾‘å¤æ‚çš„é•¿ç­”æ¡ˆã€‚</li>
<li>AREæ¡†æ¶åˆ©ç”¨æŒ‡ä»¤è®­ç»ƒè¿‡çš„LLMåˆ†è§£é•¿ç­”æ¡ˆï¼Œå¹¶è¿›è¡ŒçŸ¥è¯†å›¾è°±è¯æ®æœç´¢å’ŒéªŒè¯ã€‚</li>
<li>æœ¬æ–¹æ³•ç›¸è¾ƒäºå…¶ä»–å…ˆè¿›æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.16708">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2410.16708v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2410.16708v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2410.16708v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2410.16708v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2410.16708v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Direct-Judgement-Preference-Optimization"><a href="#Direct-Judgement-Preference-Optimization" class="headerlink" title="Direct Judgement Preference Optimization"></a>Direct Judgement Preference Optimization</h2><p><strong>Authors:Peifeng Wang, Austin Xu, Yilun Zhou, Caiming Xiong, Shafiq Joty</strong></p>
<p>Auto-evaluation is crucial for assessing response quality and offering feedback for model development. Recent studies have explored training large language models (LLMs) as generative judges to evaluate and critique other modelsâ€™ outputs. In this work, we investigate the idea of learning from both positive and negative data with preference optimization to enhance the evaluation capabilities of LLM judges across an array of different use cases. We achieve this by employing three approaches to collect the preference pairs for different use cases, each aimed at improving our generative judge from a different perspective. Our comprehensive study over a wide range of benchmarks demonstrates the effectiveness of our method. In particular, our generative judge achieves the best performance on 10 out of 13 benchmarks, outperforming strong baselines like GPT-4o and specialized judge models. Further analysis show that our judge model robustly counters inherent biases such as position and length bias, flexibly adapts to any evaluation protocol specified by practitioners, and provides helpful language feedback for improving downstream generator models. </p>
<blockquote>
<p>è‡ªåŠ¨è¯„ä¼°å¯¹äºè¯„ä¼°å“åº”è´¨é‡å’Œä¸ºæ¨¡å‹å‘å±•æä¾›åé¦ˆè‡³å…³é‡è¦ã€‚è¿‘æœŸçš„ç ”ç©¶å·²ç»æ¢ç´¢äº†è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºç”Ÿæˆè¯„åˆ¤è€…æ¥è¯„ä¼°å’Œå…¶ä»–æ¨¡å‹çš„è¾“å‡ºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä»æ­£è´Ÿæ ·æœ¬ä¸­å­¦ä¹ å¹¶ç»“åˆåå¥½ä¼˜åŒ–ï¼Œä»¥æé«˜LLMè¯„åˆ¤è€…åœ¨å„ç§ç”¨ä¾‹ä¸­çš„è¯„ä¼°èƒ½åŠ›çš„æƒ³æ³•ã€‚æˆ‘ä»¬é€šè¿‡é‡‡ç”¨ä¸‰ç§æ–¹æ³•ä¸ºä¸åŒçš„ç”¨ä¾‹æ”¶é›†åå¥½å¯¹ï¼Œæ¯ä¸€ç§æ–¹æ³•éƒ½ä»å„è‡ªä¸åŒçš„è§’åº¦æé«˜æˆ‘ä»¬çš„ç”Ÿæˆè¯„åˆ¤è€…çš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å…¨é¢ç ”ç©¶è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„ç”Ÿæˆè¯„åˆ¤è€…åœ¨13ä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„10ä¸ªä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œè¶…è¿‡äº†GPT-4oç­‰å¼ºåŠ²åŸºå‡†æµ‹è¯•ä»¥åŠä¸“é—¨çš„è¯„åˆ¤æ¨¡å‹ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„è¯„åˆ¤æ¨¡å‹ç¨³å¥åœ°åº”å¯¹äº†ä½ç½®åè§å’Œé•¿åº¦åè§ç­‰å›ºæœ‰åè§ï¼Œçµæ´»åœ°é€‚åº”ä»ä¸šè€…æŒ‡å®šçš„ä»»ä½•è¯„ä¼°åè®®ï¼Œå¹¶ä¸ºæ”¹è¿›ä¸‹æ¸¸ç”Ÿæˆæ¨¡å‹æä¾›äº†æœ‰ç›Šçš„è¯­è¨€åé¦ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.14664v3">PDF</a> EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè‡ªåŠ¨è¯„ä¼°çš„é‡è¦æ€§ï¼Œè¯¥ç ”ç©¶æ—¨åœ¨åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„ä¼°ä¸æ‰¹åˆ¤å…¶ä»–æ¨¡å‹è¾“å‡ºçš„è¯„ä»·è€…ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†é€šè¿‡åå¥½ä¼˜åŒ–å­¦ä¹ æ­£è´Ÿæ ·æœ¬æ•°æ®ä»¥å¢å¼ºLLMè¯„ä»·è€…çš„è¯„ä¼°èƒ½åŠ›çš„æ–¹æ³•ã€‚é€šè¿‡ä¸‰ç§æ–¹æ³•ä¸ºä¸åŒçš„ç”¨ä¾‹æ”¶é›†åå¥½é…å¯¹æ•°æ®ï¼Œä»ä¸åŒçš„è§’åº¦æé«˜ç”Ÿæˆå¼è¯„ä»·è€…çš„è¡¨ç°ã€‚ç»è¿‡å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼Œè¯¥æ–¹æ³•è¢«è¯æ˜æœ‰æ•ˆã€‚ç‰¹åˆ«åœ°ï¼Œç”Ÿæˆå¼è¯„ä»·è€…åœ¨13ä¸ªåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€ä½³è¡¨ç°çš„10ä¸ªï¼Œè¶…è¶Šäº†å¦‚GPT-4oç­‰å¼ºåŠ²åŸºçº¿ä»¥åŠä¸“é—¨çš„è¯„ä»·æ¨¡å‹ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œè¯¥è¯„ä»·æ¨¡å‹ç¨³å¥åœ°åº”å¯¹äº†å›ºæœ‰çš„åè§ï¼Œå¦‚ä½ç½®å’Œé•¿åº¦åè§ï¼Œçµæ´»é€‚åº”å®è·µè€…æŒ‡å®šçš„ä»»ä½•è¯„ä¼°åè®®ï¼Œå¹¶ä¸ºæ”¹è¿›ä¸‹æ¸¸ç”Ÿæˆæ¨¡å‹æä¾›æœ‰ç”¨çš„è¯­è¨€åé¦ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯„ä¼°åœ¨è¯„ä¼°å“åº”è´¨é‡å’Œä¸ºæ¨¡å‹å‘å±•æä¾›åé¦ˆæ–¹é¢è‡³å…³é‡è¦ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„ä»·è€…æ¥è¯„ä¼°å…¶ä»–æ¨¡å‹çš„è¾“å‡ºæ˜¯è¿‘æœŸç ”ç©¶çš„è¶‹åŠ¿ã€‚</li>
<li>é€šè¿‡å­¦ä¹ æ­£è´Ÿæ ·æœ¬æ•°æ®å’Œåå¥½ä¼˜åŒ–ï¼Œå¢å¼ºäº†LLMè¯„ä»·è€…çš„è¯„ä¼°èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨äº†ä¸‰ç§æ–¹æ³•æ”¶é›†åå¥½é…å¯¹æ•°æ®ï¼Œä»¥æ”¹è¿›ç”Ÿæˆå¼è¯„ä»·è€…çš„è¡¨ç°ã€‚</li>
<li>åœ¨å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œç”Ÿæˆå¼è¯„ä»·è€…å–å¾—äº†æ˜¾è‘—æˆæœï¼Œè¶…è¶Šäº†å¼ºåŠ²åŸºçº¿ã€‚</li>
<li>è¯¥è¯„ä»·æ¨¡å‹èƒ½å¤Ÿç¨³å¥åœ°åº”å¯¹ä½ç½®å’Œé•¿åº¦åè§ç­‰å†…åœ¨åè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.14664">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2409.14664v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2409.14664v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2409.14664v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2409.14664v3/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2409.14664v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2409.14664v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="UIO-LLMs-Unbiased-Incremental-Optimization-for-Long-Context-LLMs"><a href="#UIO-LLMs-Unbiased-Incremental-Optimization-for-Long-Context-LLMs" class="headerlink" title="UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs"></a>UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs</h2><p><strong>Authors:Wenhao Li, Mingbao Lin, Yunshan Zhong, Shuicheng Yan, Rongrong Ji</strong></p>
<p>Managing long texts is challenging for large language models (LLMs) due to limited context window sizes. This study introduces UIO-LLMs, an unbiased incremental optimization approach for memory-enhanced transformers under long-context settings. We initially conceptualize the process as a streamlined encoder-decoder framework where the weights-shared encoder and decoder respectively encapsulate a context segment into memories and leverage these memories to predict outputs of the subsequent segment. Subsequently, by treating our memory-enhanced transformers as fully-connected recurrent neural networks (RNNs), we refine the training process using the Truncated Backpropagation Through Time (TBPTT) algorithm, which incorporates innovative incremental optimization techniques. These techniques not only diminish time complexity but also address the bias in gradient computation through an unbiased optimization process. UIO-LLMs successfully handle long context, such as extending the context window of Llama2-7b-chat from 4K to 100K tokens with minimal 2% additional parameters, while keeping the inference cost nearly linear as context length increases. </p>
<blockquote>
<p>å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ï¼Œç®¡ç†é•¿æ–‡æœ¬æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºä¸Šä¸‹æ–‡çª—å£å¤§å°æœ‰é™ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†UIO-LLMï¼Œè¿™æ˜¯ä¸€ç§åœ¨é•¿ä¸Šä¸‹æ–‡è®¾ç½®ä¸‹ç”¨äºå¢å¼ºå†…å­˜å˜å‹å™¨çš„æ— åå¢é‡ä¼˜åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬æœ€åˆå°†è¿™ä¸ªè¿‡ç¨‹æ¦‚å¿µåŒ–ä¸ºä¸€ä¸ªç®€åŒ–çš„ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼Œå…¶ä¸­æƒé‡å…±äº«çš„ç¼–ç å™¨å’Œè§£ç å™¨åˆ†åˆ«å°†ä¸Šä¸‹æ–‡æ®µå°è£…åˆ°å†…å­˜ä¸­ï¼Œå¹¶åˆ©ç”¨è¿™äº›å†…å­˜é¢„æµ‹åç»­æ®µçš„è¾“å‡ºã€‚éšåï¼Œæˆ‘ä»¬å°†å¢å¼ºå†…å­˜çš„å˜å‹å™¨è§†ä¸ºå…¨è¿æ¥çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œä½¿ç”¨æˆªæ–­çš„åå‘ä¼ æ’­æ—¶é—´ï¼ˆTBPTTï¼‰ç®—æ³•æ”¹è¿›è®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥ç®—æ³•ç»“åˆäº†åˆ›æ–°çš„å¢é‡ä¼˜åŒ–æŠ€æœ¯ã€‚è¿™äº›æŠ€æœ¯ä¸ä»…é™ä½äº†æ—¶é—´å¤æ‚åº¦ï¼Œè€Œä¸”é€šè¿‡æ— åä¼˜åŒ–è¿‡ç¨‹è§£å†³äº†æ¢¯åº¦è®¡ç®—ä¸­çš„åå·®ã€‚UIO-LLMæˆåŠŸå¤„ç†äº†é•¿ä¸Šä¸‹æ–‡ï¼Œä¾‹å¦‚å°†Llama2-7b-chatçš„ä¸Šä¸‹æ–‡çª—å£ä»4Kæ‰©å±•åˆ°100Kä»¤ç‰Œï¼Œä»…å¢åŠ 2%çš„é¢å¤–å‚æ•°ï¼ŒåŒæ—¶éšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œæ¨ç†æˆæœ¬å‡ ä¹ä¿æŒçº¿æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.18173v3">PDF</a> This article was not accepted, and its quality is not very good.   Therefore, we have decided to withdraw the submission and will not resubmit   it elsewhere</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ‘˜è¦ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶ä»‹ç»äº†UIO-LLMè¿™ä¸€é’ˆå¯¹å†…å­˜å¢å¼ºå‹å˜å‹å™¨çš„æ— åå¢é‡ä¼˜åŒ–æ–¹æ³•ã€‚è¯¥ç ”ç©¶é‡‡ç”¨äº†ä¸€ç§ç®€åŒ–çš„ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶ï¼Œå°†ä¸Šä¸‹æ–‡ç‰‡æ®µå°è£…åœ¨å†…å­˜ä¸­å¹¶åˆ©ç”¨è¿™äº›å†…å­˜é¢„æµ‹åç»­ç‰‡æ®µçš„è¾“å‡ºã€‚é€šè¿‡é‡‡ç”¨æˆªæ–­åå‘ä¼ æ’­æ—¶é—´ï¼ˆTBPTTï¼‰ç®—æ³•è¿›è¡Œè®­ç»ƒè¿‡ç¨‹çš„ä¼˜åŒ–ï¼Œå®ç°äº†æ— åå¢é‡ä¼˜åŒ–æŠ€æœ¯ï¼Œé™ä½äº†æ—¶é—´å¤æ‚åº¦å¹¶è§£å†³äº†æ¢¯åº¦è®¡ç®—ä¸­çš„åå·®é—®é¢˜ã€‚UIO-LLMèƒ½å¤ŸæˆåŠŸå¤„ç†é•¿ä¸Šä¸‹æ–‡å†…å®¹ï¼Œå¦‚åœ¨ä»…å¢åŠ é¢å¤–å‚æ•°çš„ç™¾åˆ†ä¹‹äºŒçš„æƒ…å†µä¸‹æ‰©å±•Llama 2ç³»åˆ—å°çŠ¬çš„æ€§æ ¼å¤§å‹æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£è‡³å¤šè¾¾å››ä¸‡å…­åƒå¤šä¸‡ç¬¦å·ï¼›è€Œéšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œå…¶æ¨ç†æˆæœ¬è¿‘ä¹ä¿æŒçº¿æ€§å¢é•¿è¶‹åŠ¿ã€‚æ­¤é¡¹æŠ€æœ¯æ— ç–‘ä¸ºæœªæ¥è§£å†³é•¿æ–‡æœ¬å»ºæ¨¡çš„å±€é™æ€§é—®é¢˜å¼€è¾Ÿäº†æ–°æ€è·¯ã€‚æ­¤æŠ€æœ¯çš„è¿ç”¨å¯ä»¥è¿›ä¸€æ­¥æ‹“å®½å¤§å‹è¯­è¨€æ¨¡å‹çš„å®é™…åº”ç”¨é¢†åŸŸã€‚æ–‡ä¸­è¯¦ç»†ä»‹ç»å…¶å®ç°åŸç†ï¼Œå¹¶ç»“åˆå…·ä½“å®ä¾‹éªŒè¯äº†å…¶æ€§èƒ½ã€‚æœªæ¥å¯ä»¥æœŸå¾…å…¶å¹¿æ³›åº”ç”¨åœ¨å„ç§åœºæ™¯å¦‚è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸä¸­ï¼Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„ä¼˜åŒ–å…·æœ‰éå¸¸é‡è¦çš„æ„ä¹‰ã€‚è¯¥æ–¹æ³•ä¸ºè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†é•¿æ–‡æœ¬çš„æŒ‘æˆ˜æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚åŒæ—¶æŒ‡å‡ºéšç€æŠ€æœ¯çš„å‘å±•å’Œç ”ç©¶çš„æ·±å…¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å°†åœ¨æœªæ¥å¾—åˆ°æ›´å¹¿æ³›çš„åº”ç”¨å’Œå‘å±•ã€‚æˆ‘ä»¬ç›¸ä¿¡éšç€ç ”ç©¶çš„æ·±å…¥ï¼Œè¯¥æŠ€æœ¯å°†ä¼šä¸æ–­å‘å±•å’Œå®Œå–„ï¼Œåœ¨æœªæ¥çš„NLPç ”ç©¶ä¸­å°†ä¼šæ‰®æ¼”æ›´åŠ é‡è¦çš„è§’è‰²ã€‚é€šè¿‡ä½¿ç”¨åˆ›æ–°çš„å¢é‡ä¼˜åŒ–æŠ€æœ¯å’Œç®€æ´é«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•å…‹æœäº†æ¨¡å‹å¯¹è¶…é•¿åºåˆ—çš„å¤„ç†å›°éš¾å¹¶åœ¨ç°å®ä»»åŠ¡ä¸­å¾—åˆ°æœ‰æ•ˆçš„è¡¨ç°ä¸å®ç°çœŸæ­£çš„è‡ªé€‚åº”æ·±åº¦å­¦ä¹ ç³»ç»Ÿæ›´è¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ€§èƒ½å’Œç¨³å®šæ€§ä»è€Œä¸ºæœªæ¥çš„NLPç ”ç©¶æä¾›äº†é‡è¦çš„æ€è·¯å’Œæ–¹å‘ã€‚è¿™é¡¹ç ”ç©¶æœ‰æœ›ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¸¦æ¥é‡å¤§çªç ´ã€‚æ€»ä½“æ¥è¯´æ˜¯ä¸€é¡¹éå¸¸å…·æœ‰å‰ç»æ€§å’ŒæŒ‘æˆ˜æ€§çš„å·¥ä½œå…·æœ‰é‡è¦çš„ç†è®ºå’Œå®é™…åº”ç”¨ä»·å€¼ã€‚æˆ‘ä»¬ç›¸ä¿¡è¯¥ç ”ç©¶å°†ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å‘å±•å¸¦æ¥é‡è¦çš„å½±å“å¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•è¿›æ­¥ã€‚æ€»çš„æ¥è¯´ï¼ŒUIO-LLMæŠ€æœ¯å¯¹äºè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†é•¿æ–‡æœ¬çš„æŒ‘æˆ˜å…·æœ‰é‡å¤§æ„ä¹‰ã€‚å®ƒæœ‰æœ›ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¸¦æ¥å®è´¨æ€§çš„çªç ´å’Œè¿›æ­¥ã€‚æˆ‘ä»¬æœŸå¾…æœªæ¥æ›´å¤šå…³äºè¿™æ–¹é¢çš„ç ”ç©¶å’Œåº”ç”¨å®è·µçš„å‡ºç°ä»¥æ¨åŠ¨è¯¥é¢†åŸŸçš„æŒç»­å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯ä»æ–‡ä¸­å¾—å‡ºçš„å…³é”®è¦ç‚¹ï¼š</p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯ç”±äºæœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°é™åˆ¶å…¶æ€§èƒ½å‘æŒ¥ã€‚å¯¹æ­¤é—®é¢˜æå‡ºäº†UIO-LLMè§£å†³æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆæ˜¯ä¸€ç§é’ˆå¯¹å†…å­˜å¢å¼ºå‹å˜å‹å™¨çš„æ— åå¢é‡ä¼˜åŒ–æ–¹æ³•ã€‚é‡‡ç”¨ç®€åŒ–çš„ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶æ¥åº”å¯¹é•¿æ–‡æœ¬ä»»åŠ¡å®ç°ä¾¿æ·å¹¶å…·å¤‡è‰¯å¥½çš„æ¨å¹¿æ€§èƒ½åº”ç”¨åœºæ™¯åŒ…æ‹¬ç”Ÿæˆå„ç§æ–‡ç« å†…å®¹å’Œç‰¹å®šä¸»é¢˜æè¿°å†™ä½œæ–¹é¢ç­‰ä»»åŠ¡ä¸Šå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿è¡¨æ˜å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œé‡è¦çš„å®ç”¨ä»·å€¼å¯¹äºæœªæ¥å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨å’Œå‘å±•å…·æœ‰é‡è¦æ„ä¹‰æˆä¸ºè¯¥é¢†åŸŸç ”ç©¶çš„çƒ­ç‚¹ä¹‹ä¸€åŒæ—¶æŒ‡å‡ºäº†è¯¥æŠ€æœ¯çš„ä¼˜åŠ¿æ‰€åœ¨é€šè¿‡å…¶ä¼˜ç§€çš„æ€§èƒ½å’Œå¹¿æ³›çš„åº”ç”¨å‰æ™¯å¸å¼•æ›´å¤šçš„ç ”ç©¶è€…å’Œå·¥ç¨‹å¸ˆå…³æ³¨å¹¶æ¨åŠ¨ç›¸å…³é¢†åŸŸçš„å‘å±•è¿›æ­¥æœªæ¥å¯ä»¥æœŸå¾…å…¶å¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­å®é™…åº”ç”¨çš„æ‰©å±•å¯¹äºæ¨è¿›è¯¥é¢†åŸŸçš„æŠ€æœ¯è¿›æ­¥å’Œå®é™…åº”ç”¨è½åœ°å…·æœ‰éå¸¸é‡è¦çš„æ„ä¹‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.18173">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2406.18173v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2406.18173v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2406.18173v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2406.18173v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2406.18173v3/page_4_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-16/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-16/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-16/Agent/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_Agent/2509.10426v1/page_0_0.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-16  DeepDive Advancing Deep Search Agents with Knowledge Graphs and   Multi-Turn RL
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-16/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.06806v4/page_1_0.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-16  DeepDive Advancing Deep Search Agents with Knowledge Graphs and   Multi-Turn RL
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
