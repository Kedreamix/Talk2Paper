<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-16  DeepDive Advancing Deep Search Agents with Knowledge Graphs and   Multi-Turn RL">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.06806v4/page_1_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-07
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    89 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-16-æ›´æ–°"><a href="#2025-09-16-æ›´æ–°" class="headerlink" title="2025-09-16 æ›´æ–°"></a>2025-09-16 æ›´æ–°</h1><h2 id="DeepDive-Advancing-Deep-Search-Agents-with-Knowledge-Graphs-and-Multi-Turn-RL"><a href="#DeepDive-Advancing-Deep-Search-Agents-with-Knowledge-Graphs-and-Multi-Turn-RL" class="headerlink" title="DeepDive: Advancing Deep Search Agents with Knowledge Graphs and   Multi-Turn RL"></a>DeepDive: Advancing Deep Search Agents with Knowledge Graphs and   Multi-Turn RL</h2><p><strong>Authors:Rui Lu, Zhenyu Hou, Zihan Wang, Hanchen Zhang, Xiao Liu, Yujiang Li, Shi Feng, Jie Tang, Yuxiao Dong</strong></p>
<p>Augmenting large language models (LLMs) with browsing tools substantially improves their potential as deep search agents to solve complex, real-world tasks. Yet, open LLMs still perform poorly in such settings due to limited long-horizon reasoning capacity with browsing tools and the lack of sufficiently difficult supervised data. To address these challenges, we present DeepDive to advance deep search agents. First, we propose a strategy to automatically synthesize complex, difficult, and hard-to-find questions from open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement learning (RL) to enhance LLMsâ€™ long-horizon reasoning with deep search. Experiments show that DeepDive-32B achieves a new open-source competitive result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and Search-o1. We demonstrate that multi-turn RL training improves deep search ability and significantly contributes to the performance improvements across multiple benchmarks. We observe that DeepDive enables test-time scaling of tool calls and parallel sampling. All datasets, models, and code are publicly available at <a target="_blank" rel="noopener" href="https://github.com/THUDM/DeepDive">https://github.com/THUDM/DeepDive</a>. </p>
<blockquote>
<p>é€šè¿‡æµè§ˆå·¥å…·å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›ï¼Œèƒ½æ˜¾è‘—æé«˜å…¶ä½œä¸ºæ·±åº¦æœç´¢ä»£ç†çš„æ½œåŠ›ï¼Œä»è€Œè§£å†³å¤æ‚ã€çœŸå®çš„ä»»åŠ¡ã€‚ç„¶è€Œï¼Œåœ¨ç±»ä¼¼åœºæ™¯ä¸­ï¼Œå¼€æ”¾çš„LLMä»è¡¨ç°ä¸ä½³ï¼Œå…¶åŸå› åœ¨äºä½¿ç”¨æµè§ˆå·¥å…·è¿›è¡Œé•¿æœŸæ¨ç†çš„èƒ½åŠ›æœ‰é™ï¼Œä»¥åŠç¼ºä¹è¶³å¤Ÿå›°éš¾çš„ç›‘ç£æ•°æ®ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºDeepDiveæ¥æ¨åŠ¨æ·±åº¦æœç´¢ä»£ç†çš„å‘å±•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç­–ç•¥ï¼Œå¯ä»¥ä»å¼€æ”¾çŸ¥è¯†å›¾è°±ä¸­è‡ªåŠ¨åˆæˆå¤æ‚ã€å›°éš¾ä¸”éš¾ä»¥æ‰¾åˆ°çš„é—®é¢˜ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åº”ç”¨ç«¯åˆ°ç«¯çš„å¤šè½®å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥æé«˜LLMçš„é•¿æœŸæ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸æ·±åº¦æœç´¢ç›¸ç»“åˆã€‚å®éªŒè¡¨æ˜ï¼ŒDeepDive-32Båœ¨BrowseCompä¸Šå–å¾—äº†å¼€æºç«äº‰çš„æ–°æˆæœï¼Œè¶…è¶Šäº†WebSailorã€DeepSeek-R1-Browseå’ŒSearch-o1ã€‚æˆ‘ä»¬è¯æ˜äº†å¤šè½®RLè®­ç»ƒèƒ½æé«˜æ·±åº¦æœç´¢èƒ½åŠ›ï¼Œå¹¶ä¸ºå¤šä¸ªåŸºå‡†æµ‹è¯•çš„æ€§èƒ½æ”¹è¿›åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°DeepDiveèƒ½å¤Ÿå®ç°æµ‹è¯•æ—¶çš„å·¥å…·è°ƒç”¨æ‰©å±•å’Œå¹¶è¡Œé‡‡æ ·ã€‚æ‰€æœ‰çš„æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç éƒ½å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/THUDM/DeepDive%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/THUDM/DeepDiveå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10446v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡æµè§ˆå·¥å…·è¿›è¡Œå¢å¼ºï¼Œå…¶åœ¨è§£å†³å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡æ—¶ä½œä¸ºæ·±åº¦æœç´¢ä»£ç†çš„æ½œåŠ›å¾—åˆ°æ˜¾è‘—æå‡ã€‚ç„¶è€Œï¼Œç”±äºé•¿æœŸè§„åˆ’æ¨ç†èƒ½åŠ›ä¸æµè§ˆå·¥å…·çš„å±€é™æ€§ä»¥åŠç¼ºä¹è¶³å¤Ÿå›°éš¾çš„ç›‘ç£æ•°æ®ï¼Œå¼€æ”¾LLMsåœ¨è¿™ç§ç¯å¢ƒä¸‹çš„è¡¨ç°ä»ç„¶è¾ƒå·®ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºDeepDiveä»¥æå‡æ·±åº¦æœç´¢ä»£ç†çš„æ€§èƒ½ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ç­–ç•¥ï¼Œå¯ä»¥ä»å¼€æ”¾çŸ¥è¯†å›¾è°±ä¸­è‡ªåŠ¨åˆæˆå¤æ‚ã€å›°éš¾ä¸”éš¾ä»¥æ‰¾åˆ°çš„é—®é¢˜ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é‡‡ç”¨ç«¯åˆ°ç«¯çš„å¤šè½®å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä»¥å¢å¼ºLLMsçš„é•¿æœŸè§„åˆ’æ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒDeepDive-32Båœ¨BrowseCompä¸Šå–å¾—äº†å¼€æºç«äº‰çš„æ–°æˆæœï¼Œè¶…è¶Šäº†WebSailorã€DeepSeek-R1-Browseå’ŒSearch-o1ã€‚æˆ‘ä»¬è¯æ˜äº†å¤šè½®RLè®­ç»ƒæå‡äº†æ·±åº¦æœç´¢èƒ½åŠ›ï¼Œå¹¶å¯¹å¤šä¸ªåŸºå‡†æµ‹è¯•çš„æ€§èƒ½æ”¹è¿›æœ‰æ˜¾è‘—è´¡çŒ®ã€‚DeepDiveè¿˜æ”¯æŒå·¥å…·è°ƒç”¨çš„æµ‹è¯•æ—¶é—´æ‰©å±•å’Œå¹¶è¡Œé‡‡æ ·ã€‚æ‰€æœ‰æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/THUDM/DeepDive%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/THUDM/DeepDiveå…¬å¼€è·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsç»“åˆæµè§ˆå·¥å…·å¯æ˜¾è‘—æé«˜è§£å†³å¤æ‚ç°å®ä¸–ç•Œä»»åŠ¡çš„èƒ½åŠ›ã€‚</li>
<li>å½“å‰LLMsé¢ä¸´é•¿æœŸè§„åˆ’æ¨ç†èƒ½åŠ›ä¸æµè§ˆå·¥å…·çš„å±€é™æ€§ä»¥åŠç¼ºä¹è¶³å¤Ÿå›°éš¾çš„ç›‘ç£æ•°æ®çš„æŒ‘æˆ˜ã€‚</li>
<li>DeepDiveé€šè¿‡è‡ªåŠ¨åˆæˆå¤æ‚é—®é¢˜å¹¶åº”ç”¨ç«¯åˆ°ç«¯çš„å¤šè½®å¼ºåŒ–å­¦ä¹ æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>DeepDive-32Båœ¨BrowseCompä¸Šçš„è¡¨ç°è¶…è¶Šäº†å…¶ä»–å¼€æºæ¨¡å‹ã€‚</li>
<li>å¤šè½®RLè®­ç»ƒæ˜¾è‘—æå‡äº†LLMsçš„æ·±åº¦æœç´¢èƒ½åŠ›å’Œé•¿æœŸè§„åˆ’æ¨ç†èƒ½åŠ›ã€‚</li>
<li>DeepDiveæ”¯æŒå·¥å…·è°ƒç”¨çš„æµ‹è¯•æ—¶é—´æ‰©å±•å’Œå¹¶è¡Œé‡‡æ ·ï¼Œå¢å¼ºäº†å…¶å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10446">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10446v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10446v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10446v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10446v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10446v1/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Inpainting-Guided-Policy-Optimization-for-Diffusion-Large-Language-Models"><a href="#Inpainting-Guided-Policy-Optimization-for-Diffusion-Large-Language-Models" class="headerlink" title="Inpainting-Guided Policy Optimization for Diffusion Large Language   Models"></a>Inpainting-Guided Policy Optimization for Diffusion Large Language   Models</h2><p><strong>Authors:Siyan Zhao, Mengchen Liu, Jing Huang, Miao Liu, Chenyu Wang, Bo Liu, Yuandong Tian, Guan Pang, Sean Bell, Aditya Grover, Feiyu Chen</strong></p>
<p>Masked diffusion large language models (dLLMs) are emerging as promising alternatives to autoregressive LLMs, offering competitive performance while supporting unique generation capabilities such as inpainting. We explore how inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with reinforcement learning faces an exploration challenge: sparse reward signals and sample waste when models fail to discover correct solutions. While this inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunityâ€“their inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided Policy Optimization), an RL framework that strategically inserts partial ground-truth reasoning traces during online sampling. Unlike providing full solutions, inpainting steers exploration toward promising trajectory spaces while preserving self-generated reasoning, bridging supervised fine-tuning and reinforcement learning. We apply IGPO to group-based optimization methods such as GRPO, where exploration failures cause zero advantages and gradients. IGPO restores meaningful gradients while improving sample efficiency. We also propose supervised fine-tuning on synthetically rewritten concise traces that better align with dLLM generation patterns. With additional techniques including entropy-based filtering, our training recipe yields substantial gains across three mathematical benchmarksâ€“GSM8K, Math500, and AMCâ€“achieving new state-of-the-art results for full-attention masked dLLMs. </p>
<blockquote>
<p>æ©ç›–æ‰©æ•£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä½œä¸ºè‡ªåŠ¨å›å½’LLMsçš„æœ‰æœ›æ›¿ä»£æ–¹æ¡ˆæ­£åœ¨å…´èµ·ï¼Œå®ƒä»¬æä¾›äº†ç«äº‰æ€§çš„æ€§èƒ½ï¼ŒåŒæ—¶æ”¯æŒå¦‚æ’ç”»ç­‰ç‹¬ç‰¹çš„ç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘ä»¬æ¢è®¨äº†æ’ç”»å¦‚ä½•ä¸ºdLLMsçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•è®¾è®¡æä¾›ä¿¡æ¯ã€‚LLMsä¸å¼ºåŒ–å­¦ä¹ å¯¹é½é¢ä¸´ç€æ¢ç´¢æŒ‘æˆ˜ï¼šå½“æ¨¡å‹æœªèƒ½å‘ç°æ­£ç¡®è§£å†³æ–¹æ¡ˆæ—¶ï¼Œå¥–åŠ±ä¿¡å·ç¨€ç–å’Œæ ·æœ¬æµªè´¹ã€‚è™½ç„¶è¿™ç§ä½æ•ˆæ€§å¹¿æ³›å½±å“äº†LLMsï¼Œä½†dLLMsæä¾›äº†ç‹¬ç‰¹çš„æœºä¼šâ€”â€”å®ƒä»¬çš„æ’ç”»èƒ½åŠ›å¯ä»¥å¼•å¯¼æ¢ç´¢ã€‚æˆ‘ä»¬ä»‹ç»äº†IGPOï¼ˆæ’ç”»å¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒèƒ½åœ¨åœ¨çº¿é‡‡æ ·è¿‡ç¨‹ä¸­æˆ˜ç•¥æ€§åœ°æ’å…¥éƒ¨åˆ†çœŸå®æ¨ç†è½¨è¿¹ã€‚ä¸æä¾›å®Œæ•´è§£å†³æ–¹æ¡ˆä¸åŒï¼Œæ’ç”»èƒ½å¼•å¯¼æ¢ç´¢èµ°å‘æœ‰å¸Œæœ›çš„è½¨è¿¹ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™è‡ªæˆ‘ç”Ÿæˆçš„æ¨ç†ï¼Œå¼¥åˆäº†ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬å°†IGPOåº”ç”¨äºåŸºäºç»„çš„ä¼˜åŒ–æ–¹æ³•ï¼Œå¦‚GRPOï¼Œå…¶ä¸­æ¢ç´¢å¤±è´¥å¯¼è‡´é›¶ä¼˜åŠ¿å’Œæ¢¯åº¦ã€‚IGPOæ¢å¤äº†æœ‰æ„ä¹‰çš„æ¢¯åº¦ï¼ŒåŒæ—¶æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†åœ¨åˆæˆé‡å†™çš„ç®€æ´è½¨è¿¹ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œä½¿å…¶æ›´å¥½åœ°ä¸dLLMç”Ÿæˆæ¨¡å¼å¯¹é½ã€‚é€šè¿‡åŒ…æ‹¬åŸºäºç†µçš„è¿‡æ»¤ç­‰å…¶ä»–æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„è®­ç»ƒé…æ–¹åœ¨ä¸‰ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ï¼ˆGSM8Kã€Math500å’ŒAMCï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä¸ºå…¨æ³¨æ„åŠ›æ©ç›–dLLMsè¾¾åˆ°äº†æ–°çš„æœ€æ–°ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10396v1">PDF</a> preprint; 21 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡æœ¬å†…å®¹ï¼Œç®€åŒ–çš„ä¸­æ–‡æ‘˜è¦ä¸ºï¼šâ€œæ–°å…´çš„é¢å‘Maskedæ‰©æ•£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰ä¸ºå¼ºåŒ–å­¦ä¹ ç®—æ³•è®¾è®¡æä¾›äº†æ–°æ€è·¯ã€‚dLLMsçš„ç»˜ç”»å¡«å……èƒ½åŠ›å¯æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢è¿‡ç¨‹ã€‚æˆ‘ä»¬å¼•å…¥äº†IGPOï¼ˆç»˜ç”»å¡«å……å¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯åœ¨åœ¨çº¿é‡‡æ ·è¿‡ç¨‹ä¸­æˆ˜ç•¥æ€§åœ°æ’å…¥éƒ¨åˆ†çœŸå®æ¨ç†è½¨è¿¹ã€‚è¿™ä¸åŒäºæä¾›å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œç»˜ç”»å¡«å……å¯ä»¥å¼•å¯¼æ¢ç´¢èµ°å‘æœ‰å¸Œæœ›çš„è½¨è¿¹ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™è‡ªæˆ‘ç”Ÿæˆçš„æ¨ç†ï¼Œå¼¥åˆäº†ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬åœ¨åŸºäºç¾¤ä½“çš„ä¼˜åŒ–æ–¹æ³•ä¸­åº”ç”¨äº†IGPOï¼Œå¦‚GRPOç­‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨åˆæˆé‡å†™çš„ç®€æ´è½¨è¿¹è¿›è¡Œè®­ç»ƒæœ‰åŠ©äºdLLMæ›´å¥½åœ°ç”Ÿæˆæ¨¡å¼ã€‚é€šè¿‡åŒ…æ‹¬åŸºäºç†µçš„è¿‡æ»¤åœ¨å†…çš„é™„åŠ æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„è®­ç»ƒé…æ–¹åœ¨GSM8Kã€Math500å’ŒAMCä¸‰ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚â€è¯·æ³¨æ„è¿™ä¸ªæ‘˜è¦å¯èƒ½è¶…è¿‡äº†ä½ è¦æ±‚çš„å­—æ•°é™åˆ¶ï¼Œè¯·é…Œæƒ…åˆ å‡ä»¥ç¡®ä¿ç¬¦åˆè¦æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æ–‡æœ¬ä¸­çš„å…³é”®è¦ç‚¹ï¼š</p>
<ul>
<li>Maskedæ‰©æ•£çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆdLLMsï¼‰æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä¸€ç§æ–°å…´æ›¿ä»£å½¢å¼ï¼Œæ‹¥æœ‰ç‹¬ç‰¹çš„ç”Ÿæˆèƒ½åŠ›å¦‚ç»˜ç”»å¡«å……åŠŸèƒ½ã€‚è¿™äº›æ¨¡å‹è¡¨ç°å‡ºç«äº‰æ€§çš„æ€§èƒ½æ°´å¹³ã€‚</li>
<li>LLMsåœ¨ä¸å¼ºåŒ–å­¦ä¹ å¯¹é½æ—¶é¢ä¸´æ¢ç´¢æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¨€ç–å¥–åŠ±ä¿¡å·å’Œæ¨¡å‹æœªèƒ½å‘ç°æ­£ç¡®è§£å†³æ–¹æ¡ˆæ—¶çš„æ ·æœ¬æµªè´¹é—®é¢˜ã€‚è¿™ç§ä½æ•ˆæ€§å¹¿æ³›å½±å“LLMsã€‚ç„¶è€Œï¼ŒdLLMsæœ‰æœºä¼šåˆ©ç”¨å…¶ç»˜ç”»å¡«å……èƒ½åŠ›æ¥æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢è¿‡ç¨‹ã€‚ </li>
<li>IGPOï¼ˆç»˜ç”»å¡«å……å¼•å¯¼ç­–ç•¥ä¼˜åŒ–ï¼‰æ¡†æ¶æ˜¯ä¸€ç§æ–°çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒé€šè¿‡æˆ˜ç•¥æ€§åœ°æ’å…¥éƒ¨åˆ†çœŸå®æ¨ç†è½¨è¿¹æ¥å¼•å¯¼æ¨¡å‹æ¢ç´¢è¿‡ç¨‹ã€‚å®ƒæœ‰åŠ©äºå¼¥åˆç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ä¹‹é—´çš„å·®è·ã€‚æ­¤æ¡†æ¶ç‰¹åˆ«é€‚ç”¨äºç¾¤ä½“ä¼˜åŒ–æ–¹æ³•ï¼Œä¾‹å¦‚åœ¨æ¢ç´¢å¤±è´¥å¯¼è‡´æ— æ•ˆæˆæœæˆ–é›¶æ¢¯åº¦æ—¶çš„GRPOç­‰æ–¹æ³•ã€‚ </li>
<li>åœ¨ç›‘ç£å¾®è°ƒä¸­åŠ å…¥åˆæˆé‡å†™çš„ç®€æ´è½¨è¿¹å¯ä»¥æ›´å¥½åœ°é€‚åº”dLLMç”Ÿæˆæ¨¡å¼ã€‚é™„åŠ æŠ€æœ¯å¦‚åŸºäºç†µçš„è¿‡æ»¤èƒ½æ”¹å–„æ ·æœ¬æ•ˆç‡å¹¶æå‡æ¨¡å‹æ€§èƒ½ã€‚è¿™ä¸€è®­ç»ƒç­–ç•¥åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æå‡å’Œæ–°çŠ¶æ€çš„æœ€ä½³ç»“æœã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10396">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10396v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10396v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MagicMirror-A-Large-Scale-Dataset-and-Benchmark-for-Fine-Grained-Artifacts-Assessment-in-Text-to-Image-Generation"><a href="#MagicMirror-A-Large-Scale-Dataset-and-Benchmark-for-Fine-Grained-Artifacts-Assessment-in-Text-to-Image-Generation" class="headerlink" title="MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained   Artifacts Assessment in Text-to-Image Generation"></a>MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained   Artifacts Assessment in Text-to-Image Generation</h2><p><strong>Authors:Jia Wang, Jie Hu, Xiaoqi Ma, Hanghang Ma, Yanbing Zeng, Xiaoming Wei</strong></p>
<p>Text-to-image (T2I) generation has achieved remarkable progress in instruction following and aesthetics. However, a persistent challenge is the prevalence of physical artifacts, such as anatomical and structural flaws, which severely degrade perceptual quality and limit application. Given the diversity and complexity of these artifacts, a systematic and fine-grained evaluation framework is required, which is lacking in current benchmarks. To fill this gap, we introduce MagicMirror, a comprehensive framework for artifacts assessment. We first establish a detailed taxonomy of generated image artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the first human-annotated large-scale dataset of 340K generated images with fine-grained artifact labels. Building on this dataset, we train MagicAssessor, a Vision-Language Model (VLM) that provides detailed assessments and corresponding labels. To overcome challenges like class imbalance and reward hacking, we design a novel data sampling strategy and a multi-level reward system for Group Relative Policy Optimization (GRPO). Finally, we leverage MagicAssessor to construct MagicBench, an automated benchmark for evaluating the image artifacts of current T2I models. Our evaluation with MagicBench reveals that despite their widespread adoption, even top-tier models like GPT-image-1 are consistently plagued by significant artifacts, highlighting artifact reduction as a critical frontier for future T2I development. Project page: <a target="_blank" rel="noopener" href="https://wj-inf.github.io/MagicMirror-page/">https://wj-inf.github.io/MagicMirror-page/</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆåœ¨æŒ‡ä»¤éµå¾ªå’Œç¾å­¦æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œä¸€ä¸ªæŒç»­å­˜åœ¨çš„æŒ‘æˆ˜æ˜¯ç‰©ç†ä¼ªå½±çš„æ™®éå­˜åœ¨ï¼Œä¾‹å¦‚è§£å‰–å’Œç»“æ„ç¼ºé™·ï¼Œè¿™äº›ç¼ºé™·ä¸¥é‡é™ä½äº†æ„ŸçŸ¥è´¨é‡å¹¶é™åˆ¶äº†åº”ç”¨ã€‚è€ƒè™‘åˆ°è¿™äº›ä¼ªå½±çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ï¼Œéœ€è¦ä¸€ä¸ªç³»ç»Ÿå’Œç²¾ç»†çš„è¯„ä»·æ¡†æ¶ï¼Œè€Œå½“å‰åŸºå‡†æµ‹è¯•ä¸­ç¼ºä¹è¿™ä¸€æ¡†æ¶ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†MagicMirrorï¼Œä¸€ä¸ªç”¨äºä¼ªå½±è¯„ä¼°çš„ç»¼åˆæ¡†æ¶ã€‚æˆ‘ä»¬é¦–å…ˆå»ºç«‹äº†ç”Ÿæˆçš„å›¾åƒä¼ªå½±çš„è¯¦ç»†åˆ†ç±»ã€‚åœ¨æ­¤åˆ†ç±»çš„æŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬æ‰‹åŠ¨æ³¨é‡Šäº†MagicData340Kï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”±äººç±»æ³¨é‡Šçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«340Kå¼ ç”Ÿæˆçš„å›¾åƒå’Œç²¾ç»†çš„ä¼ªå½±æ ‡ç­¾ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è®­ç»ƒäº†MagicAssessorï¼Œè¿™æ˜¯ä¸€ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œæä¾›è¯¦ç»†çš„è¯„ä¼°å’Œç›¸åº”æ ‡ç­¾ã€‚ä¸ºäº†å…‹æœç±»ä¸å¹³è¡¡å’Œå¥–åŠ±é»‘å®¢æ”»å‡»ç­‰æŒ‘æˆ˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹æ•°æ®é‡‡æ ·ç­–ç•¥å’Œç”¨äºç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¤šçº§å¥–åŠ±ç³»ç»Ÿã€‚æœ€åï¼Œæˆ‘ä»¬åˆ©ç”¨MagicAssessoræ„å»ºäº†MagicBenchï¼Œä¸€ä¸ªç”¨äºè¯„ä¼°å½“å‰T2Iæ¨¡å‹å›¾åƒä¼ªå½±çš„è‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬åœ¨MagicBenchä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œå°½ç®¡å¹¿æ³›ä½¿ç”¨ï¼Œä½†å³ä½¿æ˜¯é¡¶çº§æ¨¡å‹å¦‚GPT-image-1ä¹Ÿå§‹ç»ˆå­˜åœ¨æ˜¾è‘—çš„ä¼ªå½±é—®é¢˜ï¼Œè¿™çªå‡ºäº†ä¼ªå½±å‡å°‘æ˜¯æœªæ¥T2Iå‘å±•çš„å…³é”®å‰æ²¿ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://wj-inf.github.io/MagicMirror-page/%E3%80%82">https://wj-inf.github.io/MagicMirror-page/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10260v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯ç”Ÿæˆçš„å›¾åƒä¸­çš„ç‰©ç†ç‘•ç–µé—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼•å…¥äº†MagicMirroræ¡†æ¶ï¼ŒåŒ…æ‹¬å»ºç«‹ç‘•ç–µçš„è¯¦ç»†åˆ†ç±»ã€æ‰‹åŠ¨æ ‡æ³¨å¤§è§„æ¨¡æ•°æ®é›†MagicData340Kã€è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹MagicAssessorã€è®¾è®¡æ•°æ®é‡‡æ ·ç­–ç•¥å’Œé‡‡ç”¨å¤šçº§å¥–åŠ±ç³»ç»Ÿè¿›è¡Œç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œå¹¶æ„å»ºè‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•MagicBenchã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå°½ç®¡é¡¶çº§æ¨¡å‹å¦‚GPT-image-1å¹¿æ³›åº”ç”¨ï¼Œä½†ä»å­˜åœ¨å¤§é‡ç‘•ç–µï¼Œè¿™æˆä¸ºæœªæ¥æ–‡æœ¬åˆ°å›¾åƒå‘å±•çš„ä¸€ä¸ªé‡è¦å‰æ²¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆé¢ä¸´ç‰©ç†ç‘•ç–µçš„æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥MagicMirroræ¡†æ¶ä»¥è¯„ä¼°å’Œå‡å°‘ç”Ÿæˆå›¾åƒä¸­çš„ç‘•ç–µã€‚</li>
<li>å»ºç«‹è¯¦ç»†çš„ç‘•ç–µåˆ†ç±»å’Œæ‰‹åŠ¨æ ‡æ³¨çš„å¤§è§„æ¨¡æ•°æ®é›†MagicData340Kã€‚</li>
<li>è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹MagicAssessoræä¾›è¯¦ç»†çš„è¯„ä¼°å’Œç›¸åº”æ ‡ç­¾ã€‚</li>
<li>é‡‡ç”¨æ•°æ®é‡‡æ ·ç­–ç•¥å’Œç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¤šçº§å¥–åŠ±ç³»ç»Ÿã€‚</li>
<li>åˆ©ç”¨MagicAssessoræ„å»ºè‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•MagicBenchã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10260v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10260v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10260v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10260v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10260v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10260v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Empirical-Evaluation-of-Memory-Erasure-Protocols"><a href="#Empirical-Evaluation-of-Memory-Erasure-Protocols" class="headerlink" title="Empirical Evaluation of Memory-Erasure Protocols"></a>Empirical Evaluation of Memory-Erasure Protocols</h2><p><strong>Authors:Reynaldo Gil-Pons, Sjouke Mauw, Rolando Trujillo-Rasua</strong></p>
<p>Software-based memory-erasure protocols are two-party communication protocols where a verifier instructs a computational device to erase its memory and send a proof of erasure. They aim at guaranteeing that low-cost IoT devices are free of malware by putting them back into a safe state without requiring secure hardware or physical manipulation of the device. Several software-based memory-erasure protocols have been introduced and theoretically analysed. Yet, many of them have not been tested for their feasibility, performance and security on real devices, which hinders their industry adoption. This article reports on the first empirical analysis of software-based memory-erasure protocols with respect to their security, erasure guarantees, and performance. The experimental setup consists of 3 modern IoT devices with different computational capabilities, 7 protocols, 6 hash-function implementations, and various performance and security criteria. Our results indicate that existing software-based memory-erasure protocols are feasible, although slow devices may take several seconds to erase their memory and generate a proof of erasure. We found that no protocol dominates across all empirical settings, defined by the computational power and memory size of the device, the network speed, and the required level of security. Interestingly, network speed and hidden constants within the protocol specification played a more prominent role in the performance of these protocols than anticipated based on the related literature. We provide an evaluation framework that, given a desired level of security, determines which protocols offer the best trade-off between performance and erasure guarantees. </p>
<blockquote>
<p>åŸºäºè½¯ä»¶çš„å†…å­˜æ“¦é™¤åè®®æ˜¯ä¸€ç§åŒæ–¹é€šä¿¡åè®®ï¼Œå…¶ä¸­éªŒè¯è€…æŒ‡ä»¤è®¡ç®—è®¾å¤‡æ“¦é™¤å…¶å†…å­˜å¹¶å‘é€æ“¦é™¤è¯æ˜ã€‚å®ƒä»¬çš„ç›®çš„æ˜¯é€šè¿‡ä½¿ä½æˆæœ¬ç‰©è”ç½‘è®¾å¤‡å›åˆ°å®‰å…¨çŠ¶æ€ï¼Œä¿è¯è¿™äº›è®¾å¤‡ä¸å—æ¶æ„è½¯ä»¶çš„ä¾µæ‰°ï¼Œè€Œæ— éœ€ä¾èµ–å®‰å…¨ç¡¬ä»¶æˆ–å¯¹è®¾å¤‡è¿›è¡Œç‰©ç†æ“æ§ã€‚å·²ç»å¼•å…¥å¹¶åˆ†æäº†å¤šä¸ªåŸºäºè½¯ä»¶çš„å†…å­˜æ“¦é™¤åè®®ã€‚ç„¶è€Œï¼Œå…¶ä¸­è®¸å¤šåè®®åœ¨å®é™…è®¾å¤‡ä¸Šçš„å¯è¡Œæ€§ã€æ€§èƒ½å’Œå®‰å…¨æ€§å°šæœªå¾—åˆ°æµ‹è¯•ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨è¡Œä¸šä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹åŸºäºè½¯ä»¶çš„å†…å­˜æ“¦é™¤åè®®è¿›è¡Œå®è¯åˆ†æï¼Œæ¶‰åŠå®‰å…¨æ€§ã€æ“¦é™¤ä¿è¯å’Œæ€§èƒ½ç­‰æ–¹é¢ã€‚å®éªŒè®¾ç½®åŒ…æ‹¬3å°å…·æœ‰ä¸åŒè®¡ç®—èƒ½åŠ›çš„ç°ä»£ç‰©è”ç½‘è®¾å¤‡ã€7ä¸ªåè®®ã€6ä¸ªå“ˆå¸Œå‡½æ•°å®ç°ä»¥åŠå„ç§æ€§èƒ½å’Œå®‰å…¨æ€§æ ‡å‡†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç°æœ‰çš„åŸºäºè½¯ä»¶çš„å†…å­˜æ“¦é™¤åè®®æ˜¯å¯è¡Œçš„ï¼Œå°½ç®¡è¾ƒæ…¢çš„è®¾å¤‡å¯èƒ½éœ€è¦æ•°ç§’æ—¶é—´æ¥æ“¦é™¤å…¶å†…å­˜å¹¶ç”Ÿæˆæ“¦é™¤è¯æ˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œæ²¡æœ‰ä¸€ä¸ªåè®®èƒ½åœ¨æ‰€æœ‰å®è¯ç¯å¢ƒä¸­å æ®ä¸»å¯¼ï¼Œè¿™äº›ç¯å¢ƒç”±è®¾å¤‡çš„è®¡ç®—èƒ½åŠ›å’Œå†…å­˜å¤§å°ã€ç½‘ç»œé€Ÿåº¦ä»¥åŠæ‰€éœ€çš„å®‰å…¨çº§åˆ«æ‰€å®šä¹‰ã€‚æœ‰è¶£çš„æ˜¯ï¼Œç½‘ç»œé€Ÿåº¦å’Œåè®®è§„èŒƒä¸­çš„éšè—å¸¸æ•°å¯¹è¿™äº›åè®®æ€§èƒ½çš„å½±å“æ¯”ç›¸å…³æ–‡çŒ®é¢„æœŸçš„æ›´ä¸ºçªå‡ºã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œåœ¨ç»™å®šçš„å®‰å…¨çº§åˆ«ä¸‹ï¼Œç¡®å®šå“ªäº›åè®®åœ¨æ€§èƒ½å’Œæ“¦é™¤ä¿è¯ä¹‹é—´æä¾›æœ€ä½³æŠ˜è¡·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10224v1">PDF</a> Published at SECRYPT 2025</p>
<p><strong>Summary</strong><br>åœ¨è½¯ä»¶åŸºç¡€çš„è®°å¿†æ¸…é™¤åè®®ä¸­ï¼ŒéªŒè¯è€…ä¼šæŒ‡å¯¼è®¡ç®—è®¾å¤‡æ¸…é™¤å…¶å†…å­˜å¹¶å‘é€æ¸…é™¤è¯æ˜ã€‚è¿™äº›åè®®æ—¨åœ¨ç¡®ä¿ä½æˆæœ¬ç‰©è”ç½‘è®¾å¤‡åœ¨æ²¡æœ‰æ¶æ„è½¯ä»¶çš„æƒ…å†µä¸‹å›å½’å®‰å…¨çŠ¶æ€ï¼Œæ— éœ€ä¾èµ–å®‰å…¨ç¡¬ä»¶æˆ–å¯¹è®¾å¤‡è¿›è¡Œç‰©ç†æ“æ§ã€‚æœ¬æ–‡é¦–æ¬¡å¯¹è½¯ä»¶åŸºç¡€çš„è®°å¿†æ¸…é™¤åè®®è¿›è¡Œå®è¯ç ”ç©¶ï¼Œæ¶‰åŠå®‰å…¨ã€æ¸…é™¤ä¿è¯å’Œæ€§èƒ½ç­‰æ–¹é¢ã€‚å®éªŒè®¾ç½®åŒ…æ‹¬ç°ä»£ç‰©è”ç½‘è®¾å¤‡ã€è®°å¿†æ¸…é™¤åè®®ã€å“ˆå¸Œå‡½æ•°å®æ–½ä»¥åŠå„é¡¹æ€§èƒ½å’Œå®‰å…¨æ ‡å‡†ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ç°æœ‰è½¯ä»¶åŸºç¡€çš„è®°å¿†æ¸…é™¤åè®®å…·æœ‰å¯è¡Œæ€§ï¼Œä½†é€Ÿåº¦è¾ƒæ…¢çš„è®¾å¤‡å¯èƒ½éœ€è¦æ•°ç§’æ¥å®Œæˆå†…å­˜æ¸…é™¤å’Œç”Ÿæˆè¯æ˜ã€‚ä¸åŒåè®®è¡¨ç°å› è®¾å¤‡è®¡ç®—èƒ½åŠ›å’Œå†…å­˜å¤§å°ã€ç½‘ç»œé€Ÿåº¦ä»¥åŠæ‰€éœ€å®‰å…¨ç­‰çº§çš„å·®å¼‚è€Œæœ‰æ‰€ä¸åŒã€‚ç½‘ç»œå’Œåè®®è§„æ ¼ä¸­çš„éšè—å¸¸æ•°å¯¹åè®®æ€§èƒ½çš„å½±å“æ¯”é¢„æœŸæ›´ä¸ºæ˜¾è‘—ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œæ ¹æ®æ‰€éœ€çš„å®‰å…¨ç­‰çº§ï¼Œç¡®å®šå“ªäº›åè®®åœ¨æ€§èƒ½å’Œæ¸…é™¤ä¿è¯ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è½¯ä»¶åŸºç¡€çš„è®°å¿†æ¸…é™¤åè®®æ—¨åœ¨ç¡®ä¿ä½æˆæœ¬ç‰©è”ç½‘è®¾å¤‡çš„å®‰å…¨çŠ¶æ€ï¼Œæ— éœ€ä¾èµ–å®‰å…¨ç¡¬ä»¶æˆ–ç‰©ç†æ“æ§ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡å¯¹è¿™äº›åè®®è¿›è¡Œå®è¯ç ”ç©¶ï¼Œæ¶‰åŠå…¶åœ¨çœŸå®è®¾å¤‡ä¸Šçš„å¯è¡Œæ€§ã€æ€§èƒ½å’Œå®‰å…¨æ€§ã€‚</li>
<li>å®éªŒæ¶µç›–äº†å¤šç§ç°ä»£ç‰©è”ç½‘è®¾å¤‡å’Œè®°å¿†æ¸…é™¤åè®®ï¼Œä»¥åŠå„ç§æ€§èƒ½å’Œå®‰å…¨æ ‡å‡†ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜è½¯ä»¶åŸºç¡€çš„è®°å¿†æ¸…é™¤åè®®å…·æœ‰å¯è¡Œæ€§ï¼Œä½†è¾ƒæ…¢çš„è®¾å¤‡å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´æ¥å®Œæˆæ“ä½œã€‚</li>
<li>ä¸åŒåè®®çš„è¡¨ç°å—åˆ°è®¾å¤‡æ€§èƒ½ã€ç½‘ç»œé€Ÿåº¦å’Œæ‰€éœ€å®‰å…¨ç­‰çº§çš„å½±å“ã€‚</li>
<li>ç½‘ç»œé€Ÿåº¦å’Œåè®®è§„æ ¼ä¸­çš„éšè—å¸¸æ•°å¯¹åè®®æ€§èƒ½çš„å½±å“æ›´ä¸ºæ˜¾è‘—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10224v1/page_0_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Multimodal-Mathematical-Reasoning-Embedded-in-Aerial-Vehicle-Imagery-Benchmarking-Analysis-and-Exploration"><a href="#Multimodal-Mathematical-Reasoning-Embedded-in-Aerial-Vehicle-Imagery-Benchmarking-Analysis-and-Exploration" class="headerlink" title="Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery:   Benchmarking, Analysis, and Exploration"></a>Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery:   Benchmarking, Analysis, and Exploration</h2><p><strong>Authors:Yue Zhou, Litong Feng, Mengcheng Lan, Xue Yang, Qingyun Li, Yiping Ke, Xue Jiang, Wayne Zhang</strong></p>
<p>Mathematical reasoning is critical for tasks such as precise distance and area computations, trajectory estimations, and spatial analysis in unmanned aerial vehicle (UAV) based remote sensing, yet current vision-language models (VLMs) have not been adequately tested in this domain. To address this gap, we introduce AVI-Math, the first benchmark to rigorously evaluate multimodal mathematical reasoning in aerial vehicle imagery, moving beyond simple counting tasks to include domain-specific knowledge in areas such as geometry, logic, and algebra. The dataset comprises 3,773 high-quality vehicle-related questions captured from UAV views, covering 6 mathematical subjects and 20 topics. The data, collected at varying altitudes and from multiple UAV angles, reflects real-world UAV scenarios, ensuring the diversity and complexity of the constructed mathematical problems. In this paper, we benchmark 14 prominent VLMs through a comprehensive evaluation and demonstrate that, despite their success on previous multimodal benchmarks, these models struggle with the reasoning tasks in AVI-Math. Our detailed analysis highlights significant limitations in the mathematical reasoning capabilities of current VLMs and suggests avenues for future research. Furthermore, we explore the use of Chain-of-Thought prompting and fine-tuning techniques, which show promise in addressing the reasoning challenges in AVI-Math. Our findings not only expose the limitations of VLMs in mathematical reasoning but also offer valuable insights for advancing UAV-based trustworthy VLMs in real-world applications. The code, and datasets will be released at <a target="_blank" rel="noopener" href="https://github.com/VisionXLab/avi-math">https://github.com/VisionXLab/avi-math</a> </p>
<blockquote>
<p>åœ¨æ— äººé©¾é©¶èˆªç©ºå™¨ï¼ˆUAVï¼‰åŸºäºé¥æ„ŸæŠ€æœ¯çš„ç²¾ç¡®è·ç¦»å’Œé¢ç§¯è®¡ç®—ã€è½¨è¿¹ä¼°ç®—å’Œç©ºé—´åˆ†æç­‰ä»»åŠ¡ä¸­ï¼Œæ•°å­¦æ¨ç†è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æ­¤é¢†åŸŸå°šæœªå¾—åˆ°å……åˆ†æµ‹è¯•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†AVI-Mathï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä¸¥æ ¼è¯„ä¼°ç©ºä¸­è½¦è¾†å›¾åƒå¤šæ¨¡æ€æ•°å­¦æ¨ç†çš„åŸºå‡†æµ‹è¯•ï¼Œå®ƒè¶…è¶Šäº†ç®€å•çš„è®¡æ•°ä»»åŠ¡ï¼Œæ¶µç›–äº†è¯¸å¦‚å‡ ä½•ã€é€»è¾‘å’Œä»£æ•°ç­‰é¢†åŸŸçš„ç‰¹å®šé¢†åŸŸçŸ¥è¯†ã€‚æ•°æ®é›†åŒ…å«ä»æ— äººæœºè§†è§’æ•è·çš„3773ä¸ªé«˜è´¨é‡è½¦è¾†ç›¸å…³é—®é¢˜ï¼Œæ¶‰åŠ6ä¸ªæ•°å­¦ä¸»é¢˜å’Œ20ä¸ªè¯é¢˜ã€‚è¿™äº›æ•°æ®æ˜¯ä»ä¸åŒé«˜åº¦å’Œå¤šä¸ªæ— äººæœºè§’åº¦æ”¶é›†çš„ï¼Œåæ˜ äº†çœŸå®ä¸–ç•Œçš„æ— äººæœºåœºæ™¯ï¼Œç¡®ä¿äº†æ„å»ºçš„æ•°å­¦é—®é¢˜çš„å¤šæ ·æ€§å’Œå¤æ‚æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å…¨é¢çš„è¯„ä¼°åŸºå‡†æµ‹è¯•äº†14ç§ä¸»æµçš„VLMï¼Œç»“æœè¡¨æ˜ï¼Œå°½ç®¡è¿™äº›æ¨¡å‹åœ¨ä»¥å‰çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨AVI-Mathçš„æ¨ç†ä»»åŠ¡ä¸­å´é‡åˆ°äº†å›°éš¾ã€‚æˆ‘ä»¬çš„è¯¦ç»†åˆ†æçªå‡ºäº†å½“å‰VLMåœ¨æ•°å­¦æ¨ç†èƒ½åŠ›æ–¹é¢çš„é‡å¤§å±€é™æ€§ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä½¿ç”¨æ€ç»´é“¾æç¤ºå’Œå¾®è°ƒæŠ€æœ¯ï¼Œè¿™äº›æ–¹æ³•åœ¨è§£å†³AVI-Mathä¸­çš„æ¨ç†æŒ‘æˆ˜æ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ä»…æ­ç¤ºäº†VLMåœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œè€Œä¸”ä¸ºæ¨è¿›çœŸå®ä¸–ç•Œåº”ç”¨ä¸­åŸºäºUAVçš„å¯ä¿¡VLMæä¾›äº†å®è´µè§è§£ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/VisionXLab/avi-math%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/VisionXLab/avi-mathä¸Šå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10059v1">PDF</a> 17 pages, 16 figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†æ•°å­¦æ¨ç†åœ¨æ— äººæœºé¥æ„Ÿä¸­çš„é‡è¦ä½œç”¨ï¼Œå¹¶æŒ‡å‡ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¯¥é¢†åŸŸçš„ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« å¼•å…¥äº†AVI-MathåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•èƒ½å¤Ÿä¸¥æ ¼è¯„ä¼°æ— äººæœºå›¾åƒä¸­çš„å¤šæ¨¡æ€æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œæ¶µç›–å‡ ä½•ã€é€»è¾‘å’Œä»£æ•°ç­‰é¢†åŸŸã€‚æ–‡ç« é€šè¿‡å¯¹æ¯”è¯„ä¼°äº†å¤šä¸ªé¢†å…ˆçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå‘ç°å®ƒä»¬åœ¨AVI-Mathçš„æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ¢è®¨äº†ä½¿ç”¨Chain-of-Thoughtæç¤ºå’Œå¾®è°ƒæŠ€æœ¯çš„æ½œåŠ›ã€‚ç ”ç©¶ä¸ä»…æ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†ä¸Šçš„å±€é™æ€§ï¼Œè¿˜ä¸ºåœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­æé«˜åŸºäºæ— äººæœºçš„å¯ä¿¡è§†è§‰è¯­è¨€æ¨¡å‹æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ã€‚æ•°æ®é›†å’Œä»£ç å·²å‘å¸ƒåœ¨ç›¸åº”é“¾æ¥ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ•°å­¦æ¨ç†åœ¨æ— äººæœºé¥æ„Ÿä¸­è‡³å…³é‡è¦ï¼Œæ¶‰åŠç²¾ç¡®è·ç¦»å’Œé¢ç§¯è®¡ç®—ã€è½¨è¿¹ä¼°è®¡å’Œç©ºé—´åˆ†æç­‰ä»»åŠ¡ã€‚</li>
<li>å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ— äººæœºå›¾åƒé¢†åŸŸçš„å¤šæ¨¡æ€æ•°å­¦æ¨ç†èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æµ‹è¯•ã€‚</li>
<li>AVI-MathåŸºå‡†æµ‹è¯•çš„å¼•å…¥ï¼Œæ—¨åœ¨ä¸¥æ ¼è¯„ä¼°æ— äººæœºå›¾åƒä¸­çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬å‡ ä½•ã€é€»è¾‘å’Œä»£æ•°ç­‰é¢†åŸŸã€‚</li>
<li>é¢†å…ˆçš„è§†è§‰è¯­è¨€æ¨¡å‹åœ¨AVI-Mathçš„æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ã€‚</li>
<li>Chain-of-Thoughtæç¤ºå’Œå¾®è°ƒæŠ€æœ¯åœ¨è§£å†³AVI-Mathä¸­çš„æ¨ç†æŒ‘æˆ˜æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</li>
<li>ç ”ç©¶æ­ç¤ºäº†å½“å‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10059">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10059v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10059v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10059v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10059v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10059v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10059v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="LaV-CoT-Language-Aware-Visual-CoT-with-Multi-Aspect-Reward-Optimization-for-Real-World-Multilingual-VQA"><a href="#LaV-CoT-Language-Aware-Visual-CoT-with-Multi-Aspect-Reward-Optimization-for-Real-World-Multilingual-VQA" class="headerlink" title="LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization   for Real-World Multilingual VQA"></a>LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization   for Real-World Multilingual VQA</h2><p><strong>Authors:Jing Huang, Zhiya Tan, Shutao Gong, Fanwei Zeng, Jianshu Li</strong></p>
<p>As large vision language models (VLMs) advance, their capabilities in multilingual visual question answering (mVQA) have significantly improved. Chain-of-thought (CoT) reasoning has been proven to enhance interpretability and complex reasoning. However, most existing approaches rely primarily on textual CoT and provide limited support for multilingual multimodal reasoning, constraining their deployment in real-world applications. To address this gap, we introduce \textbf{LaV-CoT}, the first Language-aware Visual CoT framework with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable multi-stage reasoning pipeline consisting of Text Summary with Bounding Box (BBox), Language Identification, Spatial Object-level Captioning, and Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an automated data curation method that generates multilingual CoT annotations through iterative generation, correction, and refinement, enabling scalable and high-quality training data. To improve reasoning and generalization, LaV-CoT adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT) with Language-aware Group Relative Policy Optimization (GRPO), guided by verifiable multi-aspect rewards including language consistency, structural accuracy, and semantic alignment. Extensive evaluations on public datasets including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up to (\sim)9.5% accuracy improvements over open-source baselines of similar size and even surpasses models with 2$\times$ larger scales by (\sim)2.6%. Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513 and Gemini-2.5-flash. We further conducted an online A&#x2F;B test to validate our method on real-world data, highlighting its effectiveness for industrial deployment. Our code is available at this link: \href{<a target="_blank" rel="noopener" href="https://github.com/HJNVR/LaV-CoT%7D">https://github.com/HJNVR/LaV-CoT}</a> </p>
<blockquote>
<p>éšç€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿›æ­¥ï¼Œå®ƒä»¬åœ¨å¤šè¯­è¨€è§†è§‰é—®ç­”ï¼ˆmVQAï¼‰æ–¹é¢çš„èƒ½åŠ›å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†è¢«è¯æ˜å¯ä»¥æé«˜è§£é‡Šæ€§å’Œå¤æ‚æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–æ–‡æœ¬CoTï¼Œå¯¹å¤šè¯­è¨€å¤šæ¨¡æ€æ¨ç†çš„æ”¯æŒæœ‰é™ï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„éƒ¨ç½²ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>LaV-CoT</strong>ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå…·æœ‰å¤šæ–¹é¢å¥–åŠ±ä¼˜åŒ–çš„è¯­è¨€æ„ŸçŸ¥è§†è§‰CoTæ¡†æ¶ã€‚LaV-CoTé‡‡ç”¨äº†ä¸€ä¸ªå¯è§£é‡Šçš„å¤šé˜¶æ®µæ¨ç†ç®¡é“ï¼ŒåŒ…æ‹¬å¸¦æœ‰è¾¹ç•Œæ¡†ï¼ˆBBoxï¼‰çš„æ–‡æœ¬æ‘˜è¦ã€è¯­è¨€è¯†åˆ«ã€ç©ºé—´å¯¹è±¡çº§æè¿°å’Œé€æ­¥é€»è¾‘æ¨ç†ã€‚éµå¾ªè¿™ä¸€æ¨ç†ç®¡é“ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è‡ªåŠ¨åŒ–æ•°æ®æ•´ç†æ–¹æ³•ï¼Œé€šè¿‡è¿­ä»£ç”Ÿæˆã€ä¿®æ­£å’Œç»†åŒ–ç”Ÿæˆå¤šè¯­è¨€CoTæ³¨é‡Šï¼Œå®ç°å¯æ‰©å±•å’Œé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€‚ä¸ºäº†æé«˜æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒLaV-CoTé‡‡ç”¨äº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼Œç»“åˆç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œè¯­è¨€æ„ŸçŸ¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼Œåœ¨å¯éªŒè¯çš„å¤šæ–¹é¢å¥–åŠ±çš„æŒ‡å¯¼ä¸‹è¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬è¯­è¨€ä¸€è‡´æ€§ã€ç»“æ„å‡†ç¡®æ€§å’Œè¯­ä¹‰å¯¹é½ã€‚åœ¨MMMBã€å¤šè¯­è¨€MMBenchå’ŒMTVQAç­‰å…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒLaV-CoTåœ¨ç±»ä¼¼è§„æ¨¡çš„å¼€æºåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†é«˜è¾¾çº¦9.5%çš„å‡†ç¡®ç‡æå‡ï¼Œç”šè‡³è¶…è¿‡äº†è§„æ¨¡æ›´å¤§çš„æ¨¡å‹çº¦2.6%ã€‚æ­¤å¤–ï¼ŒLaV-CoTè¿˜è¶…è¶Šäº†å…ˆè¿›çš„ä¸“æœ‰æ¨¡å‹ï¼Œå¦‚GPT-4o-0513å’ŒGemini-2.5-flashã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¿›è¡Œäº†åœ¨çº¿ABæµ‹è¯•ï¼Œä»¥éªŒè¯æˆ‘ä»¬åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šçš„æ–¹æ³•ï¼Œçªå‡ºäº†å…¶åœ¨å·¥ä¸šéƒ¨ç½²ä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç ä½äºï¼š<a target="_blank" rel="noopener" href="https://github.com/HJNVR/LaV-CoT">https://github.com/HJNVR/LaV-CoT</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10026v1">PDF</a> 12 Pages, 12 Figures, 2 Tables</p>
<p><strong>Summary</strong>ï¼šéšç€å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‘å±•ï¼Œå®ƒä»¬åœ¨å¤šè¯­è¨€è§†è§‰é—®ç­”ï¼ˆmVQAï¼‰ä¸­çš„èƒ½åŠ›æ˜¾è‘—æé«˜ã€‚æœ¬æ–‡ä»‹ç»äº†\textbf{LaV-CoT}ï¼Œä¸€ç§å…·æœ‰å¤šæ–¹é¢å¥–åŠ±ä¼˜åŒ–çš„è¯­è¨€æ„ŸçŸ¥è§†è§‰æ€ç»´é“¾æ¡†æ¶ã€‚å®ƒé€šè¿‡å¯è§£é‡Šçš„å¤šé˜¶æ®µæ¨ç†ç®¡é“ï¼Œç»“åˆæ–‡æœ¬æ‘˜è¦ã€è¯­è¨€è¯†åˆ«ã€ç©ºé—´å¯¹è±¡çº§æè¿°å’Œé€æ­¥é€»è¾‘æ¨ç†ï¼Œæé«˜æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒLaV-CoTåœ¨ç±»ä¼¼è§„æ¨¡çš„å¼€æºåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†é«˜è¾¾9.5ï¼…çš„å‡†ç¡®ç‡æ”¹è¿›ï¼Œç”šè‡³è¶…è¿‡äº†è§„æ¨¡æ›´å¤§çš„æ¨¡å‹çº¦2.6ï¼…ã€‚æ­¤å¤–ï¼ŒLaV-CoTåœ¨åœ¨çº¿A&#x2F;Bæµ‹è¯•ä¸­çš„è¡¨ç°éªŒè¯äº†å…¶åœ¨å·¥ä¸šéƒ¨ç½²ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€è§†è§‰é—®ç­”ä¸Šçš„æ€§èƒ½æ˜¾è‘—æé«˜ã€‚</li>
<li>LaV-CoTæ˜¯é¦–ä¸ªå…·æœ‰å¤šæ–¹é¢å¥–åŠ±ä¼˜åŒ–çš„è¯­è¨€æ„ŸçŸ¥è§†è§‰æ€ç»´é“¾æ¡†æ¶ã€‚</li>
<li>LaV-CoTé€šè¿‡å¤šé˜¶æ®µæ¨ç†ç®¡é“æé«˜ç†è§£å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>LaV-CoTé‡‡ç”¨è‡ªåŠ¨åŒ–æ•°æ®æ•´ç†æ–¹æ³•ç”Ÿæˆå¤šè¯­è¨€æ€ç»´é“¾æ³¨é‡Šã€‚</li>
<li>LaV-CoTé€šè¿‡ç»“åˆç›‘ç£å¾®è°ƒä¸è¯­è¨€æ„ŸçŸ¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ¥æé«˜æ¨ç†èƒ½åŠ›ã€‚</li>
<li>LaV-CoTåœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç±»ä¼¼è§„æ¨¡çš„å¼€æºæ¨¡å‹å’Œå¤§å‹æ¨¡å‹ã€‚</li>
<li>LaV-CoTåœ¨åœ¨çº¿A&#x2F;Bæµ‹è¯•ä¸­çš„æœ‰æ•ˆæ€§éªŒè¯äº†å…¶åœ¨å·¥ä¸šéƒ¨ç½²ä¸­çš„å®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10026v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10026v1/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10026v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10026v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Unsupervised-Hallucination-Detection-by-Inspecting-Reasoning-Processes"><a href="#Unsupervised-Hallucination-Detection-by-Inspecting-Reasoning-Processes" class="headerlink" title="Unsupervised Hallucination Detection by Inspecting Reasoning Processes"></a>Unsupervised Hallucination Detection by Inspecting Reasoning Processes</h2><p><strong>Authors:Ponhvoan Srey, Xiaobao Wu, Anh Tuan Luu</strong></p>
<p>Unsupervised hallucination detection aims to identify hallucinated content generated by large language models (LLMs) without relying on labeled data. While unsupervised methods have gained popularity by eliminating labor-intensive human annotations, they frequently rely on proxy signals unrelated to factual correctness. This misalignment biases detection probes toward superficial or non-truth-related aspects, limiting generalizability across datasets and scenarios. To overcome these limitations, we propose IRIS, an unsupervised hallucination detection framework, leveraging internal representations intrinsic to factual correctness. IRIS prompts the LLM to carefully verify the truthfulness of a given statement, and obtain its contextualized embedding as informative features for training. Meanwhile, the uncertainty of each response is considered a soft pseudolabel for truthfulness. Experimental results demonstrate that IRIS consistently outperforms existing unsupervised methods. Our approach is fully unsupervised, computationally low cost, and works well even with few training data, making it suitable for real-time detection. </p>
<blockquote>
<p>æ— ç›‘ç£çš„å¹»è§‰æ£€æµ‹æ—¨åœ¨è¯†åˆ«å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äº§ç”Ÿçš„å¹»è§‰å†…å®¹ï¼Œè€Œæ— éœ€ä¾èµ–æ ‡æ³¨æ•°æ®ã€‚è™½ç„¶æ— ç›‘ç£æ–¹æ³•é€šè¿‡æ¶ˆé™¤åŠ³åŠ¨å¯†é›†å‹äººç±»æ³¨é‡Šè€Œå¹¿å—æ¬¢è¿ï¼Œä½†å®ƒä»¬ç»å¸¸ä¾èµ–äºä¸äº‹å®æ­£ç¡®æ€§æ— å…³çš„ä»£ç†ä¿¡å·ã€‚è¿™ç§ä¸åŒ¹é…ä¼šä½¿æ£€æµ‹æ¢é’ˆåå‘äºè¡¨é¢æˆ–éçœŸå®ç›¸å…³çš„æ–¹é¢ï¼Œä»è€Œé™åˆ¶äº†è·¨æ•°æ®é›†å’Œåœºæ™¯çš„å¯æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†IRISï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨å†…åœ¨äº‹å®æ­£ç¡®æ€§è¡¨ç¤ºçš„æ— ç›‘ç£å¹»è§‰æ£€æµ‹æ¡†æ¶ã€‚IRISæç¤ºLLMä»”ç»†éªŒè¯ç»™å®šé™ˆè¿°çš„çœŸå®æ€§ï¼Œå¹¶å°†å…¶ä¸Šä¸‹æ–‡åµŒå…¥ä½œä¸ºè®­ç»ƒä¿¡æ¯ç‰¹å¾ã€‚åŒæ—¶ï¼Œæ¯ä¸ªå“åº”çš„ä¸ç¡®å®šæ€§è¢«è§†ä¸ºçœŸå®æ€§çš„è½¯ä¼ªæ ‡ç­¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIRISå§‹ç»ˆåœ¨ç°æœ‰æ— ç›‘ç£æ–¹æ³•ä¸­è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®Œå…¨æ— ç›‘ç£ï¼Œè®¡ç®—æˆæœ¬ä½ï¼Œå³ä½¿åœ¨å°‘é‡è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å¾ˆå¥½åœ°å·¥ä½œï¼Œéå¸¸é€‚åˆå®æ—¶æ£€æµ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.10004v1">PDF</a> To appear in EMNLP 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†æ— ç›‘ç£å¹»è§‰æ£€æµ‹çš„ç›®æ ‡å’Œæ–¹æ³•ã€‚å°½ç®¡æ— ç›‘ç£æ–¹æ³•æ¶ˆé™¤äº†å¯¹å¤§é‡æ ‡æ³¨æ•°æ®çš„ä¾èµ–ï¼Œä½†å®ƒä»¬ç»å¸¸ä¾èµ–äºä¸äº‹å®æ­£ç¡®æ€§æ— å…³çš„ä»£ç†ä¿¡å·ï¼Œå¯¼è‡´æ£€æµ‹ç»“æœåå‘è¡¨é¢æˆ–éçœŸç†ç›¸å…³çš„æ–¹é¢ï¼Œé™åˆ¶äº†å…¶åœ¨ä¸åŒæ•°æ®é›†å’Œåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å†…åœ¨äº‹å®æ­£ç¡®æ€§çš„è¡¨ç¤ºæ¥æ£€æµ‹å¹»è§‰çš„æ¡†æ¶IRISã€‚IRISé€šè¿‡æç¤ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»”ç»†éªŒè¯ç»™å®šé™ˆè¿°çš„çœŸå®æ€§ï¼Œå¹¶è·å–å…¶ä¸Šä¸‹æ–‡åµŒå…¥ä½œä¸ºè®­ç»ƒä¿¡æ¯ç‰¹å¾ã€‚åŒæ—¶ï¼Œæ¯ä¸ªå“åº”çš„ä¸ç¡®å®šæ€§è¢«è§†ä¸ºçœŸå®æ€§çš„è½¯ä¼ªæ ‡ç­¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒIRISåœ¨ç°æœ‰æ— ç›‘ç£æ–¹æ³•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚è¯¥æ–¹æ³•å®Œå…¨æ— ç›‘ç£ï¼Œè®¡ç®—æˆæœ¬ä½ï¼Œå³ä½¿åœ¨å°‘é‡è®­ç»ƒæ•°æ®ä¸‹ä¹Ÿèƒ½è‰¯å¥½å·¥ä½œï¼Œé€‚åˆå®æ—¶æ£€æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ— ç›‘ç£å¹»è§‰æ£€æµ‹æ—¨åœ¨è¯†åˆ«å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å¹»è§‰å†…å®¹ï¼Œæ— éœ€ä¾èµ–æ ‡æ³¨æ•°æ®ã€‚</li>
<li>å½“å‰æ— ç›‘ç£æ–¹æ³•å­˜åœ¨ä¾èµ–ä¸äº‹å®æ­£ç¡®æ€§æ— å…³çš„ä»£ç†ä¿¡å·çš„é—®é¢˜ã€‚</li>
<li>IRISæ¡†æ¶åˆ©ç”¨å†…åœ¨äº‹å®æ­£ç¡®æ€§çš„è¡¨ç¤ºæ¥æ£€æµ‹å¹»è§‰ã€‚</li>
<li>IRISé€šè¿‡æç¤ºLLMéªŒè¯é™ˆè¿°çš„çœŸå®æ€§æ¥è·å–ä¸Šä¸‹æ–‡åµŒå…¥ã€‚</li>
<li>æ¯ä¸ªå“åº”çš„ä¸ç¡®å®šæ€§è¢«è§†ä¸ºçœŸå®æ€§çš„è½¯ä¼ªæ ‡ç­¾ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜IRISåœ¨æ— ç›‘ç£æ–¹æ³•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10004v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10004v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10004v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10004v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.10004v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SmartCoder-R1-Towards-Secure-and-Explainable-Smart-Contract-Generation-with-Security-Aware-Group-Relative-Policy-Optimization"><a href="#SmartCoder-R1-Towards-Secure-and-Explainable-Smart-Contract-Generation-with-Security-Aware-Group-Relative-Policy-Optimization" class="headerlink" title="SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation   with Security-Aware Group Relative Policy Optimization"></a>SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation   with Security-Aware Group Relative Policy Optimization</h2><p><strong>Authors:Lei Yu, Jingyuan Zhang, Xin Wang, Jiajia Ma, Li Yang, Fengjun Zhang</strong></p>
<p>Smart contracts automate the management of high-value assets, where vulnerabilities can lead to catastrophic financial losses. This challenge is amplified in Large Language Models (LLMs) by two interconnected failures: they operate as unauditable â€œblack boxesâ€ lacking a transparent reasoning process, and consequently, generate code riddled with critical security vulnerabilities. To address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a novel framework for secure and explainable smart contract generation. It begins with Continual Pre-training (CPT) to specialize the model. We then apply Long Chain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated reasoning-and-code samples to train the model to emulate human security analysis. Finally, to directly mitigate vulnerabilities, we employ Security-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement learning phase that refines the generation policy by optimizing a weighted reward signal for compilation success, security compliance, and format correctness. Evaluated against 17 baselines on a benchmark of 756 real-world functions, SmartCoder-R1 establishes a new state of the art, achieving top performance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a SafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This FullRate marks a 45.79% relative improvement over the strongest baseline, DeepSeek-R1. Crucially, its generated reasoning also excels in human evaluations, achieving high-quality ratings for Functionality (82.7%), Security (85.3%), and Clarity (90.7%). </p>
<blockquote>
<p>æ™ºèƒ½åˆçº¦è‡ªåŠ¨ç®¡ç†é«˜ä»·å€¼èµ„äº§ï¼Œå…¶ä¸­å­˜åœ¨çš„æ¼æ´å¯èƒ½å¯¼è‡´é‡å¤§è´¢åŠ¡æŸå¤±ã€‚è¿™ä¸€æŒ‘æˆ˜åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­è¢«ä¸¤ä¸ªç›¸äº’å…³è”çš„é—®é¢˜æ‰€æ”¾å¤§ï¼šå®ƒä»¬ä½œä¸ºæ— æ³•å®¡è®¡çš„â€œé»‘ç®±â€ç¼ºä¹é€æ˜çš„æ¨ç†è¿‡ç¨‹ï¼Œå› æ­¤ç”Ÿæˆçš„ä»£ç å……æ–¥ç€å…³é”®çš„å®‰å…¨æ¼æ´ã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºQwen2.5-Coder-7Bçš„æ™ºèƒ½åˆçº¦ç”Ÿæˆæ–°å‹æ¡†æ¶SmartCoder-R1ã€‚å®ƒé¦–å…ˆé€šè¿‡æŒç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰æ¥ä¸“ä¸šåŒ–æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹7998ä¸ªç»è¿‡ä¸“å®¶éªŒè¯çš„æ¨ç†å’Œä»£ç æ ·æœ¬åº”ç”¨é•¿é“¾æ€ç»´ç›‘ç£å¾®è°ƒï¼ˆL-CoT SFTï¼‰ï¼Œä»¥è®­ç»ƒæ¨¡å‹æ¨¡æ‹Ÿäººç±»å®‰å…¨åˆ†æã€‚æœ€åï¼Œä¸ºäº†ç›´æ¥ç¼“è§£æ¼æ´é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨å®‰å…¨æ„ŸçŸ¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆS-GRPOï¼‰å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼Œé€šè¿‡ä¼˜åŒ–ç¼–è¯‘æˆåŠŸã€å®‰å…¨åˆè§„å’Œæ ¼å¼æ­£ç¡®çš„åŠ æƒå¥–åŠ±ä¿¡å·æ¥æ”¹è¿›ç”Ÿæˆç­–ç•¥ã€‚åœ¨åŒ…å«ç°å®ä¸–ç•Œå‡½æ•°756ä¸ªåŸºå‡†ç‚¹çš„æµ‹è¯•é›†ä¸Šï¼Œä¸åä¸ƒæ¡åŸºçº¿ç›¸æ¯”ï¼ŒSmartCoder-R1æ ‘ç«‹äº†æ–°çš„è¡Œä¸šæ ‡å‡†ï¼Œåœ¨äº”ä¸ªå…³é”®æŒ‡æ ‡ä¸Šå–å¾—äº†é¡¶å°–è¡¨ç°ï¼šCompassè¾¾åˆ°87.7%ï¼ŒVulRateä¸º8.6%ï¼ŒSafeAvalä¸º80.16%ï¼ŒFuncRateä¸º53.84%ï¼ŒFullRateä¸º50.53%ã€‚FullRateç›¸å¯¹äºè¡¨ç°æœ€å¼ºçš„åŸºçº¿DeepSeek-R1æé«˜äº†45.79%ã€‚å…³é”®çš„æ˜¯ï¼Œå…¶ç”Ÿæˆçš„æ¨ç†åœ¨äººç±»è¯„ä¼°ä¸­ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œåœ¨åŠŸèƒ½ã€å®‰å…¨æ€§å’Œæ¸…æ™°åº¦æ–¹é¢è·å¾—äº†é«˜è´¨é‡è¯„åˆ†ï¼Œåˆ†åˆ«ä¸º82.7%ã€85.3%å’Œ90.7%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09942v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ™ºèƒ½åˆçº¦åœ¨é«˜ä»·å€¼èµ„äº§çš„ç®¡ç†ä¸­å‘æŒ¥ç€è‡ªåŠ¨åŒ–çš„ä½œç”¨ï¼Œä½†åŒæ—¶ä¹Ÿé¢ä¸´ç€ä¸¥é‡çš„å®‰å…¨æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ­¤é—®é¢˜ä¸Šé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä¸€æ˜¯ç¼ºä¹é€æ˜åº¦ï¼ŒäºŒæ˜¯ç”Ÿæˆçš„ä»£ç å­˜åœ¨ä¸¥é‡çš„å®‰å…¨æ¼æ´ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SmartCoder-R1æ¡†æ¶ï¼Œå®ƒé€šè¿‡è¿ç»­é¢„è®­ç»ƒï¼ˆCPTï¼‰æ¥ä¸“ä¸šåŒ–æ¨¡å‹ï¼Œå¹¶è¿ç”¨é•¿æ€è€ƒé“¾ç›‘ç£å¾®è°ƒï¼ˆL-CoT SFTï¼‰æ¥è®­ç»ƒæ¨¡å‹ä»¥æ¨¡æ‹Ÿäººç±»å®‰å…¨åˆ†æã€‚æ­¤å¤–ï¼Œé€šè¿‡å®‰å…¨æ„ŸçŸ¥çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆS-GRPOï¼‰ç›´æ¥è§£å†³æ¼æ´é—®é¢˜ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒSmartCoder-R1åœ¨äº”ä¸ªå…³é”®æŒ‡æ ‡ä¸Šå‡è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚å®ƒä¸ä»…åœ¨å®‰å…¨æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶åœ¨äººç±»è¯„ä¼°ä¸­ä¹Ÿè·å¾—äº†é«˜åº¦è¯„ä»·ã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥æ¡†æ¶ä¸ºå®ç°æ™ºèƒ½åˆçº¦çš„å¯é ä¸å®‰å…¨åº”ç”¨å¥ å®šäº†åšå®åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ™ºèƒ½åˆçº¦åœ¨èµ„äº§ç®¡ç†ä¸­çš„è‡ªåŠ¨åŒ–ä½œç”¨ï¼Œä½†å…¶å®‰å…¨éšæ‚£ä¸¥é‡ï¼Œå°¤å…¶æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åº”ç”¨ä¸­æ›´ä¸ºçªå‡ºã€‚</li>
<li>LLMsé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šç¼ºä¹é€æ˜åº¦å’Œç”Ÿæˆä»£ç çš„å®‰å…¨æ¼æ´é—®é¢˜ã€‚</li>
<li>SmartCoder-R1æ¡†æ¶é€šè¿‡è¿ç»­é¢„è®­ç»ƒã€é•¿æ€è€ƒé“¾ç›‘ç£å¾®è°ƒå’Œå®‰å…¨æ„ŸçŸ¥çš„ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>SmartCoder-R1åœ¨äº”ä¸ªå…³é”®æŒ‡æ ‡ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨å®‰å…¨æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>è¯¥æ¡†æ¶ç”Ÿæˆçš„æ¨ç†è¿‡ç¨‹åœ¨äººç±»è¯„ä¼°ä¸­ä¹Ÿè·å¾—äº†é«˜è´¨é‡è¯„ä»·ï¼Œä½“ç°äº†å…¶å®é™…åº”ç”¨çš„æ½œåŠ›ã€‚</li>
<li>SmartCoder-R1æ¡†æ¶ä¸ä»…æå‡äº†æ™ºèƒ½åˆçº¦çš„æ€§èƒ½ï¼Œä¹Ÿä¸ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ä¸å®‰å…¨ä¿éšœæä¾›äº†é‡è¦æ”¯æŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09942">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09942v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09942v1/page_4_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Latency-and-Token-Aware-Test-Time-Compute"><a href="#Latency-and-Token-Aware-Test-Time-Compute" class="headerlink" title="Latency and Token-Aware Test-Time Compute"></a>Latency and Token-Aware Test-Time Compute</h2><p><strong>Authors:Jenny Y. Huang, Mehul Damani, Yousef El-Kurdi, Ramon Astudillo, Wei Sun</strong></p>
<p>Inference-time scaling has emerged as a powerful way to improve large language model (LLM) performance by generating multiple candidate responses and selecting among them. However, existing work on dynamic allocation for test-time compute typically considers only parallel generation methods such as best-of-N, overlooking incremental decoding methods like beam search, and has largely ignored latency, focusing only on token usage. We formulate inference-time scaling as a problem of dynamic compute allocation and method selection, where the system must decide which strategy to apply and how much compute to allocate on a per-query basis. Our framework explicitly incorporates both token cost and wall-clock latency, the latter being critical for user experience and particularly for agentic workflows where models must issue multiple queries efficiently. Experiments on reasoning benchmarks show that our approach consistently outperforms static strategies, achieving favorable accuracy-cost trade-offs while remaining practical for deployment. </p>
<blockquote>
<p>æ¨ç†æ—¶é—´ç¼©æ”¾ï¼ˆInference-time scalingï¼‰ä½œä¸ºä¸€ç§å¼ºå¤§çš„æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆå¤šä¸ªå€™é€‰å“åº”å¹¶åœ¨å…¶ä¸­è¿›è¡Œé€‰æ‹©ï¼Œæé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å…³äºæµ‹è¯•æ—¶é—´è®¡ç®—çš„åŠ¨æ€åˆ†é…å·¥ä½œé€šå¸¸åªè€ƒè™‘å¹¶è¡Œç”Ÿæˆæ–¹æ³•ï¼ˆå¦‚æœ€ä½³Né€‰é¡¹ï¼‰ï¼Œå¿½ç•¥äº†å¢é‡è§£ç æ–¹æ³•ï¼ˆå¦‚é›†æŸæœç´¢ï¼‰ï¼Œå¹¶ä¸”ä¸»è¦å¿½ç•¥äº†å»¶è¿Ÿé—®é¢˜ï¼Œåªå…³æ³¨ä»¤ç‰Œä½¿ç”¨æƒ…å†µã€‚æˆ‘ä»¬å°†æ¨ç†æ—¶é—´ç¼©æ”¾å…¬å¼åŒ–ä¸ºä¸€ä¸ªåŠ¨æ€è®¡ç®—åˆ†é…å’Œæ–¹æ³•é€‰æ‹©çš„é—®é¢˜ï¼Œç³»ç»Ÿå¿…é¡»åœ¨æ¯ä¸ªæŸ¥è¯¢çš„åŸºç¡€ä¸Šå†³å®šåº”ç”¨å“ªç§ç­–ç•¥ä»¥åŠåˆ†é…å¤šå°‘è®¡ç®—èµ„æºã€‚æˆ‘ä»¬çš„æ¡†æ¶æ˜¾å¼åœ°åŒ…å«äº†ä»¤ç‰Œæˆæœ¬å’Œå¢™é’Ÿå»¶è¿Ÿï¼Œåè€…å¯¹äºç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¿…é¡»é«˜æ•ˆå‘å‡ºå¤šä¸ªæŸ¥è¯¢çš„ä»£ç†å·¥ä½œæµç¨‹è€Œè¨€ã€‚åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºé™æ€ç­–ç•¥ï¼Œå®ç°äº†æœ‰åˆ©çš„ç²¾åº¦-æˆæœ¬æƒè¡¡ï¼ŒåŒæ—¶åœ¨å®é™…éƒ¨ç½²ä¸­ä¿æŒå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09864v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ¨ç†æ—¶é—´ç¼©æ”¾ä½œä¸ºä¸€ç§å¼ºå¤§çš„æ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆå¤šä¸ªå€™é€‰å“åº”å¹¶é€‰æ‹©å…¶ä¸­çš„æœ€ä½³å“åº”æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æµ‹è¯•æ—¶é—´è®¡ç®—åŠ¨æ€åˆ†é…é€šå¸¸åªè€ƒè™‘å¹¶è¡Œç”Ÿæˆæ–¹æ³•ï¼Œå¦‚æœ€ä½³Né€‰æ‹©æ³•ï¼Œè€Œå¿½è§†äº†å¢é‡è§£ç æ–¹æ³•ï¼Œå¦‚é›†æŸæœç´¢ï¼Œå¹¶ä¸”ä¸»è¦å¿½è§†äº†å»¶è¿Ÿé—®é¢˜ï¼Œåªå…³æ³¨ä»¤ç‰Œä½¿ç”¨é‡ã€‚æœ¬æ–‡å°†æ¨ç†æ—¶é—´ç¼©æ”¾åˆ¶å®šä¸ºåŠ¨æ€è®¡ç®—åˆ†é…å’Œæ–¹æ³•é€‰æ‹©çš„é—®é¢˜ï¼Œç³»ç»Ÿå¿…é¡»åœ¨æ¯ä¸ªæŸ¥è¯¢çš„åŸºç¡€ä¸Šå†³å®šåº”ç”¨å“ªç§ç­–ç•¥ä»¥åŠåˆ†é…å¤šå°‘è®¡ç®—èµ„æºã€‚è¯¥æ¡†æ¶æ˜¾å¼åœ°åŒ…å«äº†ä»¤ç‰Œæˆæœ¬å’Œå¢™é’Ÿå»¶è¿Ÿï¼Œåè€…å¯¹äºç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦æ¨¡å‹é«˜æ•ˆå‘å‡ºå¤šä¸ªæŸ¥è¯¢çš„ä»£ç†å·¥ä½œæµç¨‹ä¸­ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆä¼˜äºé™æ€ç­–ç•¥ï¼Œå®ç°äº†æœ‰åˆ©çš„ç²¾åº¦æˆæœ¬æƒè¡¡ï¼Œå¹¶ä¸”é€‚ç”¨äºéƒ¨ç½²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨ç†æ—¶é—´ç¼©æ”¾èƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ï¼Œé€šè¿‡ç”Ÿæˆå¤šä¸ªå€™é€‰å“åº”å¹¶é€‰æ‹©æœ€ä½³å“åº”ã€‚</li>
<li>ç°æœ‰å·¥ä½œä¸»è¦å…³æ³¨å¹¶è¡Œç”Ÿæˆæ–¹æ³•ï¼Œå¦‚æœ€ä½³Né€‰æ‹©æ³•ï¼Œä½†å¿½è§†äº†å¢é‡è§£ç æ–¹æ³•ï¼Œå¦‚é›†æŸæœç´¢ã€‚</li>
<li>ç°æœ‰çš„æµ‹è¯•æ—¶é—´è®¡ç®—åŠ¨æ€åˆ†é…ç ”ç©¶ä¸»è¦å…³æ³¨ä»¤ç‰Œä½¿ç”¨é‡ï¼Œè€Œå¿½è§†äº†å»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>æœ¬æ–‡å°†æ¨ç†æ—¶é—´ç¼©æ”¾åˆ¶å®šä¸ºåŠ¨æ€è®¡ç®—åˆ†é…å’Œæ–¹æ³•é€‰æ‹©çš„é—®é¢˜ã€‚</li>
<li>ç³»ç»Ÿéœ€å†³å®šåº”ç”¨ä½•ç§ç­–ç•¥åŠåˆ†é…å¤šå°‘è®¡ç®—èµ„æºäºæ¯ä¸ªæŸ¥è¯¢ã€‚</li>
<li>æ¡†æ¶è€ƒè™‘äº†ä»¤ç‰Œæˆæœ¬å’Œå¢™é’Ÿå»¶è¿Ÿï¼Œåè€…åœ¨ä»£ç†å·¥ä½œæµç¨‹ä¸­å°¤å…¶é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09864">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09864v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09864v1/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09864v1/page_3_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Topic-Guided-Reinforcement-Learning-with-LLMs-for-Enhancing-Multi-Document-Summarization"><a href="#Topic-Guided-Reinforcement-Learning-with-LLMs-for-Enhancing-Multi-Document-Summarization" class="headerlink" title="Topic-Guided Reinforcement Learning with LLMs for Enhancing   Multi-Document Summarization"></a>Topic-Guided Reinforcement Learning with LLMs for Enhancing   Multi-Document Summarization</h2><p><strong>Authors:Chuyuan Li, Austin Xu, Shafiq Joty, Giuseppe Carenini</strong></p>
<p>A key challenge in Multi-Document Summarization (MDS) is effectively integrating information from multiple sources while maintaining coherence and topical relevance. While Large Language Models have shown impressive results in single-document summarization, their performance on MDS still leaves room for improvement. In this paper, we propose a topic-guided reinforcement learning approach to improve content selection in MDS. We first show that explicitly prompting models with topic labels enhances the informativeness of the generated summaries. Building on this insight, we propose a novel topic reward within the Group Relative Policy Optimization (GRPO) framework to measure topic alignment between the generated summary and source documents. Experimental results on the Multi-News and Multi-XScience datasets demonstrate that our method consistently outperforms strong baselines, highlighting the effectiveness of leveraging topical cues in MDS. </p>
<blockquote>
<p>å¤šæ–‡æ¡£æ‘˜è¦ï¼ˆMDSï¼‰ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯å¦‚ä½•æœ‰æ•ˆåœ°æ•´åˆå¤šæºä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒè¿è´¯æ€§å’Œä¸»é¢˜ç›¸å…³æ€§ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å•æ–‡æ¡£æ‘˜è¦ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†å®ƒä»¬åœ¨MDSä¸Šçš„è¡¨ç°ä»æœ‰å¾…æé«˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºä¸»é¢˜å¼•å¯¼çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä»¥æ”¹è¿›MDSä¸­çš„å†…å®¹é€‰æ‹©ã€‚æˆ‘ä»¬é¦–å…ˆè¡¨æ˜ï¼Œé€šè¿‡ä¸»é¢˜æ ‡ç­¾æ˜ç¡®æç¤ºæ¨¡å‹å¯ä»¥å¢å¼ºç”Ÿæˆæ‘˜è¦çš„ä¿¡æ¯æ€§ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬åœ¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ¡†æ¶å†…æå‡ºäº†ä¸€ç§æ–°çš„ä¸»é¢˜å¥–åŠ±æªæ–½ï¼Œä»¥è¡¡é‡ç”Ÿæˆæ‘˜è¦ä¸æºæ–‡æ¡£ä¹‹é—´çš„è¯é¢˜å¯¹é½ç¨‹åº¦ã€‚åœ¨Multi-Newså’ŒMulti-XScienceæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºå¼ºå¤§çš„åŸºçº¿æ–¹æ³•ï¼Œçªå‡ºäº†åœ¨MDSä¸­åˆ©ç”¨ä¸»é¢˜çº¿ç´¢çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09852v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šå¤šæ–‡æ¡£æ‘˜è¦ï¼ˆMDSï¼‰çš„å…³é”®æŒ‘æˆ˜åœ¨äºå¦‚ä½•æœ‰æ•ˆæ•´åˆå¤šæºä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒè¿è´¯æ€§å’Œä¸»é¢˜ç›¸å…³æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºè¯é¢˜å¼•å¯¼å¼ºåŒ–å­¦ä¹ çš„æ”¹è¿›å†…å®¹é€‰æ‹©æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨Multi-Newså’ŒMulti-XScienceæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•æŒç»­ä¼˜äºå¼ºåŸºçº¿ï¼Œçªæ˜¾äº†åœ¨MDSä¸­åˆ©ç”¨ä¸»é¢˜çº¿ç´¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ–‡æ¡£æ‘˜è¦ï¼ˆMDSï¼‰é¢ä¸´æœ‰æ•ˆæ•´åˆå¤šæºä¿¡æ¯çš„åŒæ—¶ä¿æŒè¿è´¯æ€§å’Œä¸»é¢˜ç›¸å…³æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å•æ–‡æ¡£æ‘˜è¦ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨MDSæ–¹é¢çš„æ€§èƒ½ä»æœ‰æå‡ç©ºé—´ã€‚</li>
<li>æ˜¾å¼åœ°é€šè¿‡è¯é¢˜æ ‡ç­¾æç¤ºæ¨¡å‹å¯å¢å¼ºç”Ÿæˆæ‘˜è¦çš„ä¿¡æ¯æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºè¯é¢˜å¥–åŠ±çš„Group Relative Policy Optimizationï¼ˆGRPOï¼‰æ¡†æ¶ï¼Œä»¥è¡¡é‡ç”Ÿæˆæ‘˜è¦ä¸æºæ–‡æ¡£ä¹‹é—´çš„è¯é¢˜å¯¹é½ç¨‹åº¦ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨Multi-Newså’ŒMulti-XScienceæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•ä¼˜äºå¼ºåŸºçº¿ã€‚</li>
<li>åˆ©ç”¨è¯é¢˜çº¿ç´¢åœ¨MDSä¸­è‡³å…³é‡è¦ï¼Œå¯æœ‰æ•ˆæé«˜æ‘˜è¦çš„è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09852">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09852v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09852v1/page_1_0.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="The-Thinking-Therapist-Training-Large-Language-Models-to-Deliver-Acceptance-and-Commitment-Therapy-using-Supervised-Fine-Tuning-and-Odds-Ratio-Policy-Optimization"><a href="#The-Thinking-Therapist-Training-Large-Language-Models-to-Deliver-Acceptance-and-Commitment-Therapy-using-Supervised-Fine-Tuning-and-Odds-Ratio-Policy-Optimization" class="headerlink" title="The Thinking Therapist: Training Large Language Models to Deliver   Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio   Policy Optimization"></a>The Thinking Therapist: Training Large Language Models to Deliver   Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio   Policy Optimization</h2><p><strong>Authors:Talha Tahir</strong></p>
<p>Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral therapy with emerging evidence of efficacy in several psychiatric conditions. This study investigates the impact of post-training methodology and explicit reasoning on the ability of a small open-weight large language model (LLM) to deliver ACT. Using 50 sets of synthetic ACT transcripts generated by Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches, supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each with and without an explicit chain-of-thought (COT) reasoning step. Performance was evaluated by comparing these four post-trained variants against the base Instruct model. These models were benchmarked in simulated therapy sessions, with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM) and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned on human evaluations. Our findings demonstrate that the ORPO-trained models significantly outperformed both their SFT and Instruct counterparts on ACT fidelity ($\chi^2(5) &#x3D; 185.15, p &lt; .001$) and therapeutic empathy ($\chi^2(5) &#x3D; 140.37, p &lt; .001$). The effect of COT was conditional as it provided a significant benefit to SFT models, improving ACT-FM scores by an average of 2.68 points ($p &lt; .001$), while offering no discernible advantage to the superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO stems from its ability to learn the therapeutic <code>process&#39; over imitating </code>content,â€™ a key aspect of ACT, while COT acts as a necessary scaffold for models trained only via imitation. This study establishes that preference-aligned policy optimization can effectively instill ACT competencies in small LLMs, and that the utility of explicit reasoning is highly dependent on the underlying training paradigm. </p>
<blockquote>
<p>æ¥çº³æ‰¿è¯ºç–—æ³•ï¼ˆACTï¼‰æ˜¯ä¸€ç§ç¬¬ä¸‰æ³¢è®¤çŸ¥è¡Œä¸ºç–—æ³•ï¼Œåœ¨å¤šç§ç²¾ç¥ç–¾ç—…ä¸­æœ‰æ˜æ˜¾çš„ç–—æ•ˆè¯æ®ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†è®­ç»ƒåçš„æ–¹æ³•å’Œæ˜ç¡®æ¨ç†å¯¹å°å‹å¼€æ”¾å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰§è¡ŒACTèƒ½åŠ›çš„å½±å“ã€‚æˆ‘ä»¬ä½¿ç”¨ç”±Mistral-Largeç”Ÿæˆçš„50ç»„åˆæˆACTè½¬å½•æœ¬ï¼Œé‡‡ç”¨ä¸¤ç§ä¸åŒçš„æ–¹æ³•è®­ç»ƒäº†Llama-3.2-3b-Instructæ¨¡å‹ï¼Œå³ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œèµ”ç‡æ¯”ç‡ç­–ç•¥ä¼˜åŒ–ï¼ˆORPOï¼‰ï¼Œæ¯ç§æ–¹æ³•éƒ½å¸¦æœ‰å’Œä¸å¸¦æœ‰æ˜ç¡®çš„æ€ç»´é“¾ï¼ˆCOTï¼‰æ¨ç†æ­¥éª¤ã€‚é€šè¿‡å°†è¿™äº›è®­ç»ƒåçš„æ¨¡å‹ä¸åŸºç¡€Instructæ¨¡å‹è¿›è¡Œæ¯”è¾ƒæ¥è¯„ä¼°æ€§èƒ½ã€‚è¿™äº›æ¨¡å‹åœ¨æ¨¡æ‹Ÿæ²»ç–—ç¯èŠ‚ä¸­è¿›è¡Œè¯„ä¼°ï¼Œå®šé‡è¯„ä¼°ACTå¿ è¯šåº¦è¡¡é‡æ ‡å‡†ï¼ˆACT-FMï¼‰å’Œæ²»ç–—å¸ˆåŒç†å¿ƒé‡è¡¨ï¼ˆTESï¼‰ï¼Œè¯„ä¼°äººå‘˜æ˜¯ç»è¿‡é’ˆå¯¹äººç±»è¯„ä¼°çš„å¾®è°ƒåçš„LLMæ³•å®˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»è¿‡ORPOè®­ç»ƒçš„æ¨¡å‹åœ¨ACTå¿ è¯šåº¦æ–¹é¢æ˜¾è‘—ä¼˜äºSFTå’ŒInstructæ¨¡å‹ï¼ˆÏ‡Â²ï¼ˆ5ï¼‰&#x3D; 185.15ï¼Œp &lt; .001ï¼‰ï¼Œåœ¨æ²»ç–—åŒç†å¿ƒæ–¹é¢ä¹Ÿæœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼ˆÏ‡Â²ï¼ˆ5ï¼‰&#x3D; 140.37ï¼Œp &lt; .001ï¼‰ã€‚æ€ç»´é“¾æ•ˆåº”æ˜¯æœ‰æ¡ä»¶çš„ï¼Œå› ä¸ºå®ƒä¸ºSFTæ¨¡å‹æä¾›äº†æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œæé«˜äº†ACT-FMå¾—åˆ†å¹³å‡æé«˜2.68åˆ†ï¼ˆp &lt; .001ï¼‰ï¼Œè€Œå¯¹æ›´é«˜çº§çš„ORPOæˆ–ç»è¿‡æŒ‡ä»¤è°ƒæ•´çš„å˜ä½“åˆ™æ²¡æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚æˆ‘ä»¬è®¤ä¸ºORPOçš„ä¼˜åŠ¿åœ¨äºå®ƒèƒ½å¤Ÿå­¦ä¹ æ²»ç–—è¿‡ç¨‹è€Œä¸æ˜¯æ¨¡ä»¿å†…å®¹ï¼Œè¿™æ˜¯ACTçš„ä¸€ä¸ªå…³é”®æ–¹é¢ï¼Œè€Œæ€ç»´é“¾å¯¹äºä»…é€šè¿‡æ¨¡ä»¿è®­ç»ƒçš„æ¨¡å‹æ¥è¯´æ˜¯å¿…è¦çš„æ”¯æ’‘æ¶æ„ã€‚æœ¬ç ”ç©¶ç¡®å®šäº†åå¥½å¯¹é½ç­–ç•¥ä¼˜åŒ–å¯ä»¥æœ‰æ•ˆåœ°å°†ACTæŠ€èƒ½ä¼ æˆç»™å°å‹LLMï¼Œå¹¶ä¸”æ˜ç¡®æ¨ç†çš„å®ç”¨æ€§é«˜åº¦ä¾èµ–äºåŸºç¡€è®­ç»ƒèŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09712v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†æ¥çº³ä¸æ‰¿è¯ºç–—æ³•ï¼ˆACTï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„åº”ç”¨æ•ˆæœã€‚é€šè¿‡ä¸¤ç§ä¸åŒçš„è®­ç»ƒæ–¹æ³•ï¼ˆç›‘ç£ç²¾ç»†è°ƒæ•´ï¼ˆSFTï¼‰å’Œèµ”ç‡æ¯”ç‡ç­–ç•¥ä¼˜åŒ–ï¼ˆORPOï¼‰ï¼‰ä»¥åŠæ˜¯å¦é‡‡ç”¨æ˜¾å¼æ¨ç†é“¾ï¼ˆCOTï¼‰ï¼Œå¯¹LLMè¿›è¡Œäº†è®­ç»ƒã€‚ç»“æœæ˜¾ç¤ºï¼ŒORPOè®­ç»ƒæ¨¡å‹åœ¨ACTçš„å¿ å®åº¦ä¸åŒæƒ…å¿ƒæ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚ç ”ç©¶è®¤ä¸ºï¼ŒORPOçš„ä¼˜åŠ¿åœ¨äºå…¶èƒ½å¤Ÿå­¦ä¹ æ²»ç–—è¿‡ç¨‹è€Œéå•çº¯æ¨¡ä»¿å†…å®¹ï¼Œè¿™æ˜¯ACTçš„æ ¸å¿ƒè¦ç´ ï¼›è€ŒCOTå¯¹äºä»…é€šè¿‡æ¨¡ä»¿è®­ç»ƒçš„æ¨¡å‹æœ‰è¾…åŠ©ä½œç”¨ã€‚æœ¬ç ”ç©¶ç¡®ç«‹äº†åå¥½å¯¹é½ç­–ç•¥ä¼˜åŒ–èƒ½æœ‰æ•ˆèµ‹äºˆå°å‹LLMæ‰§è¡ŒACTçš„èƒ½åŠ›ï¼Œå¹¶æ­ç¤ºäº†æ˜¾å¼æ¨ç†çš„å®ç”¨æ€§å–å†³äºåº•å±‚è®­ç»ƒèŒƒå¼ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ACTä½œä¸ºç¬¬ä¸‰æ³¢è®¤çŸ¥è¡Œä¸ºç–—æ³•ï¼Œåœ¨ç²¾ç¥ç–¾ç—…çš„ç–—æ•ˆæ–¹é¢å­˜åœ¨æ–°å…´è¯æ®ã€‚</li>
<li>é€šè¿‡ä¸¤ç§ä¸åŒçš„è®­ç»ƒæ–¹æ³•ï¼ˆSFTå’ŒORPOï¼‰ä»¥åŠæ˜¯å¦ç»“åˆCOTï¼Œç ”ç©¶äº†ACTåœ¨LLMä¸­çš„åº”ç”¨ã€‚</li>
<li>ORPOè®­ç»ƒæ¨¡å‹åœ¨æ¨¡æ‹Ÿæ²»ç–—ä¼šè¯ä¸­çš„ACTå¿ å®åº¦å’ŒåŒæƒ…å¿ƒæ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚</li>
<li>COTå¯¹ä»…é€šè¿‡æ¨¡ä»¿è®­ç»ƒçš„æ¨¡å‹æœ‰è¾…åŠ©ä½œç”¨ï¼Œä½†å¯¹ç»è¿‡ORPOè®­ç»ƒçš„æ¨¡å‹æ²¡æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>ORPOçš„ä¼˜åŠ¿åœ¨äºå®ƒæ›´æ³¨é‡å­¦ä¹ æ²»ç–—è¿‡ç¨‹è€Œéå†…å®¹æ¨¡ä»¿ï¼Œè¿™æ˜¯ACTçš„æ ¸å¿ƒè¦ç´ ã€‚</li>
<li>åå¥½å¯¹é½ç­–ç•¥ä¼˜åŒ–èƒ½æœ‰æ•ˆèµ‹äºˆå°å‹LLMæ‰§è¡ŒACTçš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09712">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09712v1/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09712v1/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09712v1/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09712v1/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09712v1/page_4_2.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09712v1/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="OmniEVA-Embodied-Versatile-Planner-via-Task-Adaptive-3D-Grounded-and-Embodiment-aware-Reasoning"><a href="#OmniEVA-Embodied-Versatile-Planner-via-Task-Adaptive-3D-Grounded-and-Embodiment-aware-Reasoning" class="headerlink" title="OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and   Embodiment-aware Reasoning"></a>OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and   Embodiment-aware Reasoning</h2><p><strong>Authors:Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yuzheng Zhuang, Bowen Yang, He Zhu, Lingfeng Zhang, Pengwei Xie, David Gamaliel Arcos Bravo, Yingxue Zhang, Jianye Hao, Xingyue Quan</strong></p>
<p>Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible. To address these gaps, we introduce OmniEVA â€“ an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: <a target="_blank" rel="noopener" href="https://omnieva.github.io/">https://omnieva.github.io</a> </p>
<blockquote>
<p>è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›å±•ä¸ºå®ä½“æ™ºèƒ½å¸¦æ¥äº†æ–°çš„æœºä¼šï¼Œå®ç°äº†å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œäº¤äº’ï¼Œä»¥åŠè¿ç»­çš„ç©ºé—´å†³ç­–ã€‚ç„¶è€Œï¼Œå½“å‰åŸºäºMLLMçš„å®ä½“ç³»ç»Ÿé¢ä¸´ä¸¤ä¸ªå…³é”®å±€é™ã€‚é¦–å…ˆï¼Œå‡ ä½•é€‚åº”æ€§å·®è·ï¼šä»…é€šè¿‡2Dè¾“å…¥è¿›è¡Œè®­ç»ƒæˆ–åœ¨3Då‡ ä½•æ³¨å…¥ä¸­ç¡¬ç¼–ç çš„æ¨¡å‹ï¼Œç”±äºç¼ºä¹ç©ºé—´ä¿¡æ¯æˆ–2Dæ³›åŒ–å—é™ï¼Œå¯¼è‡´åœ¨ä¸åŒç©ºé—´éœ€æ±‚çš„ä»»åŠ¡ä¸­çš„é€‚åº”æ€§è¾ƒå·®ã€‚å…¶æ¬¡ï¼Œå®ä½“çº¦æŸå·®è·ï¼šæ—©æœŸçš„ç ”ç©¶å¾€å¾€å¿½è§†äº†çœŸå®æœºå™¨äººçš„ç‰©ç†çº¦æŸå’Œèƒ½åŠ›ï¼Œå¯¼è‡´ä»»åŠ¡è®¡åˆ’åœ¨ç†è®ºä¸Šæœ‰æ•ˆï¼Œä½†åœ¨å®è·µä¸­å´ä¸å¯è¡Œã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†OmniEVAâ€”â€”ä¸€ä¸ªå®ä½“é€šç”¨è§„åˆ’å™¨ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°å®ç°äº†å…ˆè¿›çš„å®ä½“æ¨ç†å’Œä»»åŠ¡è§„åˆ’ï¼šï¼ˆ1ï¼‰ä»»åŠ¡è‡ªé€‚åº”3Dæ¥åœ°æœºåˆ¶ï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªé—¨æ§è·¯ç”±å™¨ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡è¦æ±‚è¿›è¡Œæ˜¾å¼çš„é€‰æ‹©æ€§è°ƒèŠ‚3Dèåˆï¼Œä¸ºå®ç°å¤šæ ·åŒ–çš„å®ä½“ä»»åŠ¡çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥3Dæ¥åœ°ã€‚ ï¼ˆ2ï¼‰å®ä½“æ„ŸçŸ¥æ¨ç†æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ä»»åŠ¡ç›®æ ‡å’Œå®ä½“çº¦æŸçº³å…¥æ¨ç†å¾ªç¯ä¸­ï¼Œä»è€Œåšå‡ºæ—¢ä»¥ç›®æ ‡ä¸ºå¯¼å‘åˆå¯è¡Œçš„è§„åˆ’å†³ç­–ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒOmniEVAä¸ä»…è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å®ä½“æ¨ç†æ€§èƒ½ï¼Œè€Œä¸”åœ¨å¹¿æ³›çš„ä¸‹æ¸¸åœºæ™¯ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„èƒ½åŠ›ã€‚å¯¹ä¸€ç³»åˆ—æå‡ºçš„å®ä½“åŸºå‡†æµ‹è¯•çš„è¯„ä»·ï¼ŒåŒ…æ‹¬åŸºæœ¬ä»»åŠ¡å’Œå¤åˆä»»åŠ¡ï¼Œéƒ½è¯å®äº†å…¶ç¨³å¥å’Œé€šç”¨çš„è§„åˆ’èƒ½åŠ›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://omnieva.github.io/">https://omnieva.github.io</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.09332v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¿‘æœŸå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥ä¸ºå®ä½“æ™ºèƒ½å¸¦æ¥äº†æ–°æœºé‡ï¼Œå®ç°äº†å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œäº¤äº’ï¼Œä»¥åŠè¿ç»­çš„ç©ºé—´å†³ç­–ã€‚ç„¶è€Œï¼Œå½“å‰çš„MLLMå®ä½“ç³»ç»Ÿé¢ä¸´ä¸¤å¤§å±€é™ï¼šä¸€æ˜¯å‡ ä½•é€‚åº”æ€§å·®è·ï¼ŒäºŒæ˜¯å®ä½“çº¦æŸå·®è·ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæå‡ºäº†OmniEVAâ€”â€”ä¸€ä¸ªå®ä½“é€šç”¨è§„åˆ’å™¨ï¼Œé€šè¿‡ä¸¤é¡¹åˆ›æ–°æŠ€æœ¯è§£å†³è¿™äº›é—®é¢˜ï¼šä¸€æ˜¯ä»»åŠ¡é€‚åº”æ€§3Dæ¥åœ°æœºåˆ¶ï¼ŒäºŒæ˜¯å®ä½“æ„ŸçŸ¥æ¨ç†æ¡†æ¶ã€‚OmniEVAä¸ä»…å®ç°äº†å…ˆè¿›çš„å®ä½“æ¨ç†å’Œä»»åŠ¡è§„åˆ’ï¼Œè€Œä¸”è¡¨ç°å‡ºè·¨å¤šç§ä¸‹æ¸¸åœºæ™¯çš„å¼ºå¤§èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è¿›æ­¥æ¨åŠ¨äº†å®ä½“æ™ºèƒ½çš„å‘å±•ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œäº¤äº’ï¼Œä»¥åŠè¿ç»­çš„ç©ºé—´å†³ç­–èƒ½åŠ›ã€‚</li>
<li>å½“å‰MLLMå®ä½“ç³»ç»Ÿå­˜åœ¨ä¸¤å¤§å±€é™ï¼šå‡ ä½•é€‚åº”æ€§å·®è·å’Œå®ä½“çº¦æŸå·®è·ã€‚</li>
<li>OmniEVAæ˜¯ä¸€ä¸ªå®ä½“é€šç”¨è§„åˆ’å™¨ï¼Œé€šè¿‡ä»»åŠ¡é€‚åº”æ€§3Dæ¥åœ°æœºåˆ¶å’Œå®ä½“æ„ŸçŸ¥æ¨ç†æ¡†æ¶ä¸¤é¡¹åˆ›æ–°æŠ€æœ¯æ¥è§£å†³ä¸Šè¿°å±€é™ã€‚</li>
<li>ä»»åŠ¡é€‚åº”æ€§3Dæ¥åœ°æœºåˆ¶æ ¹æ®ä¸Šä¸‹æ–‡éœ€æ±‚è¿›è¡Œ3Dèåˆçš„æ˜¾å¼é€‰æ‹©æ€§è°ƒèŠ‚ï¼Œå®ç°å¤šæ ·åŒ–çš„å®ä½“ä»»åŠ¡çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥3Dæ¥åœ°ã€‚</li>
<li>å®ä½“æ„ŸçŸ¥æ¨ç†æ¡†æ¶å°†ä»»åŠ¡ç›®æ ‡å’Œå®ä½“çº¦æŸçº³å…¥æ¨ç†å¾ªç¯ä¸­ï¼Œä½¿è§„åˆ’å†³ç­–æ—¢å…·æœ‰ç›®æ ‡å¯¼å‘æ€§åˆå¯è¡Œã€‚</li>
<li>OmniEVAä¸ä»…å®ç°äº†å…ˆè¿›çš„å®ä½“æ¨ç†å’Œä»»åŠ¡è§„åˆ’ï¼Œè€Œä¸”åœ¨å¤šç§ä¸‹æ¸¸åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.09332">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09332v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09332v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.09332v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning"><a href="#Parallel-R1-Towards-Parallel-Thinking-via-Reinforcement-Learning" class="headerlink" title="Parallel-R1: Towards Parallel Thinking via Reinforcement Learning"></a>Parallel-R1: Towards Parallel Thinking via Reinforcement Learning</h2><p><strong>Authors:Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, Dong Yu</strong></p>
<p>Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose \textbf{Parallel-R1}, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the modelâ€™s thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a \textbf{mid-training exploration scaffold}, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at <a target="_blank" rel="noopener" href="https://github.com/zhengkid/Parallel-R1">https://github.com/zhengkid/Parallel-R1</a>. </p>
<blockquote>
<p>å¹¶è¡Œæ€è€ƒå·²æˆä¸ºä¸€ç§é€šè¿‡åŒæ—¶æ¢ç´¢å¤šä¸ªæ¨ç†è·¯å¾„æ¥æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›çš„æ–°å‹æ–¹æ³•ã€‚ç„¶è€Œï¼Œé€šè¿‡è®­ç»ƒæ¿€æ´»è¿™ç§èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºåˆæˆæ•°æ®ä¸Šçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™é¼“åŠ±äº†æ•™å¸ˆå¼ºåˆ¶æ¨¡ä»¿ï¼Œè€Œä¸æ˜¯æ¢ç´¢å’Œæ³›åŒ–ã€‚ä¸å®ƒä»¬ä¸åŒï¼Œæˆ‘ä»¬æå‡ºäº†<strong>Parallel-R1</strong>ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿåœ¨å¤æ‚çš„ç°å®ä¸–ç•Œæ¨ç†ä»»åŠ¡ä¸­å¯ç”¨å¹¶è¡Œæ€è€ƒè¡Œä¸ºçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§æ¸è¿›çš„è¯¾ç¨‹å­¦ä¹ ï¼Œæ˜ç¡®è§£å†³äº†è®­ç»ƒå¹¶è¡Œæ€è€ƒæ—¶RLé¢ä¸´çš„å†·å¯åŠ¨é—®é¢˜ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨SFTå¯¹æ¥è‡ªç®€å•ä»»åŠ¡çš„æç¤ºç”Ÿæˆè½¨è¿¹è¿›è¡Œè®­ç»ƒï¼Œä»¥åŸ¹å…»å¹¶è¡Œæ€è€ƒèƒ½åŠ›ï¼Œç„¶åè¿‡æ¸¡åˆ°RLä»¥åœ¨æ›´å¤æ‚çš„é—®é¢˜ä¸Šæ¢ç´¢å’Œæ¨å¹¿è¿™é¡¹æŠ€èƒ½ã€‚åœ¨å„ç§æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒï¼ŒåŒ…æ‹¬MATHã€AMC23å’ŒAIMEï¼Œè¡¨æ˜Parallel-R1æˆåŠŸåŸ¹å…»äº†å¹¶è¡Œæ€è€ƒèƒ½åŠ›ï¼Œç›¸å¯¹äºç›´æ¥åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šä½¿ç”¨RLè®­ç»ƒçš„é¡ºåºæ€è€ƒæ¨¡å‹ï¼Œå…¶å‡†ç¡®æ€§æé«˜äº†8.4%ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œæ¨¡å‹çš„è¡Œä¸ºæœ‰æ˜æ˜¾çš„è½¬å˜ï¼šåœ¨æ—©æœŸé˜¶æ®µï¼Œå®ƒä½¿ç”¨å¹¶è¡Œæ€è€ƒä½œä¸ºæ¢ç´¢ç­–ç•¥ï¼Œè€Œåœ¨åæœŸé˜¶æ®µï¼Œå®ƒä½¿ç”¨ç›¸åŒçš„èƒ½åŠ›è¿›è¡Œå¤šè§†è§’éªŒè¯ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬éªŒè¯äº†å¹¶è¡Œæ€è€ƒä½œä¸º<strong>è®­ç»ƒä¸­æœŸæ¢ç´¢è„šæ‰‹æ¶</strong>ï¼Œè¿™ä¸€ä¸´æ—¶æ¢ç´¢é˜¶æ®µåœ¨RLä¹‹åå¼€å¯äº†æ›´é«˜çš„æ€§èƒ½ä¸Šé™ï¼Œåœ¨AIME25ä¸Šçš„æ”¹è¿›æ¯”åŸºçº¿æé«˜äº†42.9%ã€‚æˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®å’Œä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhengkid/Parallel-R1%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/zhengkid/Parallel-R1ä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07980v2">PDF</a> Project website: <a target="_blank" rel="noopener" href="https://zhengkid.github.io/Parallel_R1.github.io/">https://zhengkid.github.io/Parallel_R1.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¹¶è¡Œæ€ç»´åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸­çš„æ–°å…´ä½œç”¨ã€‚ç„¶è€Œï¼Œé€šè¿‡è®­ç»ƒæ¿€æ´»è¿™ç§èƒ½åŠ›ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸»æµæ–¹æ³•ä¸»è¦ä¾èµ–äºåˆæˆæ•°æ®ä¸Šçš„ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™é¼“åŠ±æ•™å¸ˆå¼ºåˆ¶æ¨¡ä»¿ï¼Œè€Œéæ¢ç´¢å’Œæ³›åŒ–ã€‚ä¸åŒäºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†åä¸ºParallel-R1çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é¦–æ¬¡å®ç°äº†å¤æ‚ç°å®ä¸–ç•Œæ¨ç†ä»»åŠ¡çš„å¹¶è¡Œæ€ç»´è¡Œä¸ºã€‚å®ƒé‡‡ç”¨æ¸è¿›çš„è¯¾ç¨‹ç­–ç•¥æ¥æ˜ç¡®è§£å†³è®­ç»ƒå¹¶è¡Œæ€ç»´ä¸­çš„å†·å¯åŠ¨é—®é¢˜ã€‚é¦–å…ˆä½¿ç”¨åŸºäºç®€å•ä»»åŠ¡çš„æç¤ºç”Ÿæˆè½¨è¿¹è¿›è¡ŒSFTï¼Œä»¥çŒè¾“å¹¶è¡Œæ€ç»´èƒ½åŠ›ï¼Œç„¶åè¿‡æ¸¡åˆ°RLä»¥åœ¨æ­¤æŠ€èƒ½ä¸Šæ¢ç´¢å¹¶æ³›åŒ–éš¾é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒParallel-R1æˆåŠŸåŸ¹å…»äº†å¹¶è¡Œæ€ç»´ï¼Œåœ¨MATHã€AMC23å’ŒAIMEç­‰æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ¯”é€šè¿‡RLç›´æ¥è®­ç»ƒé¡ºåºæ€ç»´çš„æ¨¡å‹é«˜å‡º8.4%çš„å‡†ç¡®ç‡ã€‚è¿›ä¸€æ­¥çš„åˆ†ææ˜¾ç¤ºï¼Œæ¨¡å‹çš„è¡Œä¸ºå‘ç”Ÿäº†æ˜æ˜¾çš„è½¬å˜ï¼šåœ¨æ—©æœŸé˜¶æ®µï¼Œå®ƒä½¿ç”¨å¹¶è¡Œæ€ç»´ä½œä¸ºæ¢ç´¢ç­–ç•¥ï¼Œè€Œåœ¨åæœŸé˜¶æ®µåˆ™å°†å…¶ä½œä¸ºå¤šè§†è§’éªŒè¯çš„èƒ½åŠ›ã€‚æœ€é‡è¦çš„æ˜¯ï¼ŒéªŒè¯äº†å¹¶è¡Œæ€ç»´ä½œä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸´æ—¶æ¢ç´¢æ¶æ„çš„ä½œç”¨ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿåœ¨RLåè§£é”æ›´é«˜çš„æ€§èƒ½ä¸Šé™ï¼Œåœ¨AIME25ä¸Šçš„æ”¹è¿›ç‡è¾¾åˆ°äº†æƒŠäººçš„42.9%ã€‚æ¨¡å‹å’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhengkid/Parallel-R1%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/zhengkid/Parallel-R1ä¸Šå¼€æºã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¹¶è¡Œæ€ç»´æ˜¯ä¸€ç§æ–°å…´æ–¹æ³•ï¼Œç”¨äºå¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ æ¡†æ¶Parallel-R1æ”¯æŒå¤æ‚ç°å®æ¨ç†ä»»åŠ¡çš„å¹¶è¡Œæ€ç»´è¡Œä¸ºã€‚</li>
<li>Parallel-R1é‡‡ç”¨æ¸è¿›çš„è¯¾ç¨‹ç­–ç•¥æ¥è§£å†³è®­ç»ƒå¹¶è¡Œæ€ç»´çš„å†·å¯åŠ¨é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶ç»“åˆäº†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œå…ˆçŒè¾“å¹¶è¡Œæ€ç»´èƒ½åŠ›ï¼Œå†æ¢ç´¢å¹¶æ³›åŒ–éš¾é¢˜ã€‚</li>
<li>å®éªŒè¡¨æ˜Parallel-R1èƒ½æœ‰æ•ˆåŸ¹å…»å¹¶è¡Œæ€ç»´ï¼Œå¹¶åœ¨å¤šä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šå®ç°è¾ƒé«˜å‡†ç¡®ç‡ã€‚</li>
<li>æ¨¡å‹çš„è¡Œä¸ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿå˜åŒ–ï¼Œä»æ—©æœŸä½œä¸ºæ¢ç´¢ç­–ç•¥çš„å¹¶è¡Œæ€ç»´è½¬å˜ä¸ºåæœŸçš„å¤šè§†è§’éªŒè¯èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07980">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.07980v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.07980v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.07980v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="K2-Think-A-Parameter-Efficient-Reasoning-System"><a href="#K2-Think-A-Parameter-Efficient-Reasoning-System" class="headerlink" title="K2-Think: A Parameter-Efficient Reasoning System"></a>K2-Think: A Parameter-Efficient Reasoning System</h2><p><strong>Authors:Zhoujun Cheng, Richard Fan, Shibo Hao, Taylor W. Killian, Haonan Li, Suqi Sun, Hector Ren, Alexander Moreno, Daqian Zhang, Tianjun Zhong, Yuxin Xiong, Yuanzhe Hu, Yutao Xie, Xudong Han, Yuqi Wang, Varad Pimpalkhute, Yonghao Zhuang, Aaryamonvikram Singh, Xuezhi Liang, Anze Xie, Jianshu She, Desai Fan, Chengqian Gao, Liqun Ma, Mikhail Yurochkin, John Maggs, Xuezhe Ma, Guowei He, Zhiting Hu, Zhengzhong Liu, Eric P. Xing</strong></p>
<p>K2-Think is a reasoning system that achieves state-of-the-art performance with a 32B parameter model, matching or surpassing much larger models like GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system shows that smaller models can compete at the highest levels by combining advanced post-training and test-time computation techniques. The approach is based on six key technical pillars: Long Chain-of-thought Supervised Finetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic planning prior to reasoning, Test-time Scaling, Speculative Decoding, and Inference-optimized Hardware, all using publicly available open-source datasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art scores on public benchmarks for open-source models, while also performing strongly in other areas such as Code and Science. Our results confirm that a more parameter-efficient model like K2-Think 32B can compete with state-of-the-art systems through an integrated post-training recipe that includes long chain-of-thought training and strategic inference-time enhancements, making open-source reasoning systems more accessible and affordable. K2-Think is freely available at k2think.ai, offering best-in-class inference speeds of over 2,000 tokens per second per request via the Cerebras Wafer-Scale Engine. </p>
<blockquote>
<p>K2-Thinkæ˜¯ä¸€ä¸ªæ¨ç†ç³»ç»Ÿï¼Œå®ƒå‡­å€Ÿä¸€ä¸ª32Bå‚æ•°çš„æ¨¡å‹å®ç°äº†å‰æ²¿æ€§èƒ½ï¼Œå¯ä¸GPT-OSS 120Bå’ŒDeepSeek v3.1ç­‰å¤§å‹æ¨¡å‹ç›¸åª²ç¾ç”šè‡³è¡¨ç°æ›´ä½³ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿå»ºç«‹åœ¨Qwen2.5åŸºç¡€æ¨¡å‹ä¸Šï¼Œå±•ç¤ºäº†é€šè¿‡ç»“åˆå…ˆè¿›çš„åè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´è®¡ç®—æŠ€æœ¯ï¼Œè¾ƒå°æ¨¡å‹ä¹Ÿèƒ½åœ¨æœ€é«˜å±‚æ¬¡ä¸Šç«äº‰ã€‚è¯¥æ–¹æ³•åŸºäºå…­å¤§å…³é”®æŠ€æœ¯æ”¯æŸ±ï¼šé•¿é“¾æ€ç»´ç›‘ç£å¾®è°ƒã€å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ã€æ¨ç†å‰çš„ä»£ç†è®¡åˆ’ã€æµ‹è¯•æ—¶ç¼©æ”¾ã€æŠ•æœºè§£ç å’Œæ¨ç†ä¼˜åŒ–ç¡¬ä»¶ï¼Œæ‰€æœ‰è¿™äº›éƒ½ä½¿ç”¨å…¬å¼€å¯ç”¨çš„å¼€æºæ•°æ®é›†ã€‚K2-Thinkåœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨å…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å‰æ²¿çš„å¼€æºæ¨¡å‹å¾—åˆ†ï¼ŒåŒæ—¶åœ¨ä»£ç å’Œç§‘å­¦ç­‰å…¶ä»–é¢†åŸŸä¹Ÿè¡¨ç°å¼ºåŠ²ã€‚æˆ‘ä»¬çš„ç»“æœè¯å®ï¼ŒåƒK2-Think 32Bè¿™æ ·æ›´å‚æ•°é«˜æ•ˆçš„æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡é›†æˆåè®­ç»ƒæ–¹æ¡ˆï¼ŒåŒ…æ‹¬é•¿é“¾æ€ç»´è®­ç»ƒå’Œæˆ˜ç•¥æ¨ç†æ—¶é—´å¢å¼ºåŠŸèƒ½ï¼Œä¸æœ€å‰æ²¿ç³»ç»Ÿç›¸ç«äº‰ï¼Œè¿™ä½¿å¾—å¼€æºæ¨ç†ç³»ç»Ÿæ›´åŠ æ˜“äºè®¿é—®å’Œè´Ÿæ‹…å¾—èµ·ã€‚K2-Thinkå·²åœ¨k2think.aiä¸Šå…è´¹æä¾›ï¼Œé€šè¿‡Cerebrasæ™¶åœ†çº§å¼•æ“æä¾›æ¯ç§’è¶…è¿‡2000ä»¤ç‰Œçš„é¡¶çº§æ¨ç†é€Ÿåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.07604v2">PDF</a> To access the K2-Think reasoning system, please visit <a target="_blank" rel="noopener" href="http://www.k2think.ai/">www.k2think.ai</a></p>
<p><strong>Summary</strong></p>
<p>K2-Thinkæ˜¯ä¸€æ¬¾ç»“åˆå…ˆè¿›åè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´è®¡ç®—æŠ€æœ¯çš„æ¨ç†ç³»ç»Ÿï¼Œèƒ½åœ¨é«˜æ°´å¹³çš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚å®ƒåŸºäºQwen2.5åŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡å…­ä¸ªå…³é”®æŠ€æœ¯æ”¯æŸ±å®ç°å…ˆè¿›æ€§èƒ½ï¼ŒåŒ…æ‹¬é•¿é“¾æ€ç»´ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰ã€ä»£ç†è®¡åˆ’æ¨ç†ã€æµ‹è¯•æ—¶é—´ç¼©æ”¾ã€æŠ•æœºè§£ç å’Œæ¨ç†ä¼˜åŒ–ç¡¬ä»¶ç­‰ã€‚K2-Thinkåœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å°¤ä¸ºå‡ºè‰²ï¼ŒåŒæ—¶åœ¨ä»£ç å’Œç§‘å­¦é¢†åŸŸä¹Ÿè¡¨ç°å‡ºå¼ºåŠ²å®åŠ›ã€‚è¯¥ç³»ç»Ÿç°å·²åœ¨k2think.aiä¸Šå…è´¹æä¾›ï¼Œå¹¶å¯é€šè¿‡Cerebrasæ™¶åœ†çº§å¼•æ“å®ç°æ¯ç§’è¶…è¿‡2000ä¸ªä»¤ç‰Œçš„è¯·æ±‚å¤„ç†é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>K2-Thinkæ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ¨ç†ç³»ç»Ÿï¼Œèƒ½å¤ŸåŒ¹é…æˆ–è¶…è¶Šæ›´å¤§è§„æ¨¡çš„æ¨¡å‹ï¼Œå¦‚GPT-OSS 120Bå’ŒDeepSeek v3.1ã€‚</li>
<li>å®ƒåŸºäºQwen2.5åŸºç¡€æ¨¡å‹æ„å»ºï¼Œç»“åˆå…ˆè¿›åè®­ç»ƒå’Œæµ‹è¯•æ—¶é—´è®¡ç®—æŠ€æœ¯å®ç°é«˜æ°´å¹³æ€§èƒ½ã€‚</li>
<li>K2-Thinkçš„å…­å¤§å…³é”®æŠ€æœ¯æ”¯æŸ±åŒ…æ‹¬é•¿é“¾æ€ç»´ç›‘ç£å¾®è°ƒã€å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ç­‰ã€‚</li>
<li>è¯¥ç³»ç»Ÿåœ¨æ•°å­¦æ¨ç†æ–¹é¢è¡¨ç°å“è¶Šï¼ŒåŒæ—¶åœ¨ä»£ç å’Œç§‘å­¦é¢†åŸŸä¹Ÿæ˜¾ç¤ºå‡ºå¼ºå¤§çš„å®åŠ›ã€‚</li>
<li>K2-Thinkæä¾›äº†å…¬å¼€å¯ç”¨çš„å¼€æºæ•°æ®é›†ï¼Œå¹¶ä¸”ç°å·²åœ¨k2think.aiä¸Šå…è´¹æä¾›ã€‚</li>
<li>K2-Thinké€šè¿‡é›†æˆåè®­ç»ƒé…æ–¹å’Œæ¨ç†æ—¶é—´å¢å¼ºæªæ–½ï¼Œå±•ç°äº†ä¸æœ€å…ˆè¿›çš„ç³»ç»Ÿç«äº‰çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.07604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.07604v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.07604v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.07604v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.07604v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="MachineLearningLM-Scaling-Many-shot-In-context-Learning-via-Continued-Pretraining"><a href="#MachineLearningLM-Scaling-Many-shot-In-context-Learning-via-Continued-Pretraining" class="headerlink" title="MachineLearningLM: Scaling Many-shot In-context Learning via Continued   Pretraining"></a>MachineLearningLM: Scaling Many-shot In-context Learning via Continued   Pretraining</h2><p><strong>Authors:Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke</strong></p>
<p>Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows.   Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference.   Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‹¥æœ‰å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œå¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ ‡å‡†æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä»»åŠ¡ä¸Šï¼Œå®ƒä»¬å¾ˆéš¾ä»å¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­å­¦ä¹ ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¬æ— æ³•ä»…é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰åˆ©ç”¨å¤šä¸ªç¤ºä¾‹æ¼”ç¤ºï¼Œè€Œæ— éœ€è¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚æˆ‘ä»¬å¼•å…¥äº†MachineLearningLMï¼Œè¿™æ˜¯ä¸€ä¸ªä¾¿æºå¼æŒç»­é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒä¸ºé€šç”¨LLMé…å¤‡äº†å¼ºå¤§çš„ä¸Šä¸‹æ–‡MLåŠŸèƒ½ï¼ŒåŒæ—¶ä¿ç•™äº†å…¶ä¸€èˆ¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œé€‚ç”¨äºæ›´å¹¿æ³›çš„èŠå¤©å·¥ä½œæµç¨‹ã€‚æˆ‘ä»¬çš„é¢„è®­ç»ƒç¨‹åºä»æ•°ç™¾ä¸‡ä¸ªç»“æ„å› æœæ¨¡å‹ï¼ˆSCMï¼‰ä¸­ç»¼åˆMLä»»åŠ¡ï¼Œæ¶µç›–ç¤ºä¾‹æ•°é‡é«˜è¾¾1024ä¸ªã€‚æˆ‘ä»¬ä»¥éšæœºæ£®æ—æ•™å¸ˆå¼€å§‹ï¼Œå°†åŸºäºæ ‘çš„å†³ç­–ç­–ç•¥è’¸é¦åˆ°LLMä¸­ï¼Œä»¥åŠ å¼ºæ•°å€¼å»ºæ¨¡ä¸­çš„ç¨³å¥æ€§ã€‚æ‰€æœ‰ä»»åŠ¡éƒ½é€šè¿‡é«˜æ•ˆçš„ä»¤ç‰Œæç¤ºè¿›è¡Œåºåˆ—åŒ–ï¼Œå¯ä»¥åœ¨æ¯ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­å®ç°3åˆ°6å€çš„æ›´å¤šç¤ºä¾‹ï¼Œå¹¶é€šè¿‡æ‰¹é‡æ¨ç†å®ç°é«˜è¾¾50å€çš„æ‘Šé”€ååé‡ã€‚å°½ç®¡é…ç½®ç›¸å¯¹ç®€å•ï¼ˆä½¿ç”¨LoRAç­‰çº§8çš„Qwen-2.5-7B-Instructï¼‰ï¼Œä½†MachineLearningLMåœ¨é‡‘èã€ç‰©ç†ã€ç”Ÿç‰©å’ŒåŒ»ç–—ä¿å¥é¢†åŸŸçš„ç¦»åˆ†å¸ƒè¡¨æ ¼åˆ†ç±»æ–¹é¢ï¼Œå¹³å‡æ¯”å¼ºå¤§çš„LLMåŸºå‡†ï¼ˆä¾‹å¦‚GPT-5-miniï¼‰é«˜å‡ºçº¦15%ã€‚å®ƒè¡¨ç°å‡ºå¼•äººæ³¨ç›®çš„å¤šæ¬¡å°„å‡»å®šå¾‹ï¼šéšç€ä¸Šä¸‹æ–‡æ¼”ç¤ºä»8ä¸ªå¢åŠ åˆ°1024ä¸ªï¼Œå‡†ç¡®æ€§å•è°ƒå¢åŠ ã€‚æ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒï¼Œå®ƒåœ¨æ•°ç™¾æ¬¡å°„å‡»ä¸­å°±èƒ½è¾¾åˆ°éšæœºæ£®æ—çš„ç²¾åº¦ã€‚é€šç”¨èŠå¤©åŠŸèƒ½ï¼ˆåŒ…æ‹¬çŸ¥è¯†å’Œæ¨ç†ï¼‰å¾—ä»¥ä¿ç•™ï¼šå®ƒåœ¨MMLUä¸Šè¾¾åˆ°äº†75.4%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.06806v4">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†å’Œå¼ºå¤§çš„é€šç”¨æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨æ ‡å‡†æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä»»åŠ¡ä¸Šå­¦ä¹ å¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹æ—¶é‡åˆ°å›°éš¾ã€‚æˆ‘ä»¬å¼•å…¥äº†MachineLearningLMï¼Œè¿™æ˜¯ä¸€ä¸ªä¾¿æºå¼æŒç»­é¢„è®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸æŸå¤±é€šç”¨çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›çš„æƒ…å†µä¸‹ï¼Œä¸ºé€šç”¨LLMæä¾›å¼ºå¤§çš„ä¸Šä¸‹æ–‡æœºå™¨å­¦ä¹ åŠŸèƒ½ã€‚æˆ‘ä»¬çš„é¢„è®­ç»ƒç¨‹åºé€šè¿‡æ•°ç™¾ä¸‡çš„ç»“æ„åŒ–å› æœæ¨¡å‹ï¼ˆSCMï¼‰åˆæˆMLä»»åŠ¡ï¼Œæ¶µç›–æ ·æœ¬æ•°é«˜è¾¾1024ä¸ªã€‚æˆ‘ä»¬é€šè¿‡éšæœºæ£®æ—æ•™å¸ˆå¼ºåŒ–æ ‘å†³ç­–ç­–ç•¥åˆ°LLMä¸­ï¼Œä»¥æé«˜æ•°å€¼å»ºæ¨¡çš„ç¨³å¥æ€§ã€‚æ‰€æœ‰ä»»åŠ¡éƒ½ä»¥é«˜æ•ˆçš„æç¤ºè¿›è¡Œåºåˆ—åŒ–ï¼Œèƒ½å¤Ÿåœ¨æ¯ä¸ªä¸Šä¸‹æ–‡çª—å£ä¸­å®ç°é¢å¤–ç¤ºä¾‹æ•°é‡çš„ä¸‰å€è‡³å…­å€ï¼Œå¹¶é€šè¿‡æ‰¹é‡æ¨ç†å®ç°é«˜è¾¾äº”åå€çš„æ‘Šé”€ååé‡æå‡ã€‚å°½ç®¡æ¨¡å¼é€‚ä¸­ï¼ˆé…æœ‰LoRAæ’åç¬¬8çš„Qwen-2.5-7B-Instructï¼‰ï¼Œä½†MachineLearningLMåœ¨è´¢ç»ã€ç‰©ç†ã€ç”Ÿç‰©å’ŒåŒ»ç–—ä¿å¥é¢†åŸŸçš„ç¦»åˆ†å¸ƒè¡¨æ ¼åˆ†ç±»ä»»åŠ¡ä¸Šå¹³å‡ä¼˜äºå¼ºå¤§çš„LLMåŸºå‡†æµ‹è¯•ï¼ˆå¦‚GPT-5-miniï¼‰çº¦15%ã€‚å®ƒè¡¨ç°å‡ºæƒŠäººçš„å¤šé•œå¤´ç¼©æ”¾å®šå¾‹ï¼šéšç€ä¸Šä¸‹æ–‡æ¼”ç¤ºä»8ä¸ªå¢é•¿åˆ°1024ä¸ªï¼Œå‡†ç¡®æ€§å•è°ƒå¢åŠ ã€‚æ— éœ€ä»»ä½•ç‰¹å®šä»»åŠ¡è®­ç»ƒï¼Œå³å¯è·å¾—éšæœºæ£®æ—çº§åˆ«çš„ç²¾åº¦ï¼Œè·¨åº¦æ•°ç™¾ä¸ªé•œå¤´ã€‚åŒæ—¶ä¿ç•™äº†ä¸€èˆ¬èŠå¤©èƒ½åŠ›ï¼ŒåŒ…æ‹¬çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼šåœ¨MMLUä¸Šè¾¾åˆ°75.4%ã€‚</p>
<p><strong>è¦ç‚¹å½’çº³</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ ‡å‡†æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ä»»åŠ¡ä¸Šå­¦ä¹ å¤šä¸ªä¸Šä¸‹æ–‡ç¤ºä¾‹æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>ä»‹ç»äº†MachineLearningLMï¼Œä¸€ä¸ªä¾¿æºå¼æŒç»­é¢„è®­ç»ƒæ¡†æ¶ï¼Œå¢å¼ºäº†LLMçš„ä¸Šä¸‹æ–‡æœºå™¨å­¦ä¹ èƒ½åŠ›ï¼ŒåŒæ—¶ä¿ç•™äº†å…¶é€šç”¨çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>é¢„è®­ç»ƒç¨‹åºé€šè¿‡åˆæˆæ¥è‡ªæ•°ç™¾ä¸‡ç»“æ„åŒ–å› æœæ¨¡å‹ï¼ˆSCMï¼‰çš„MLä»»åŠ¡è¿›è¡Œï¼Œæ¶µç›–æ ·æœ¬æ•°é«˜è¾¾1024ä¸ªã€‚</li>
<li>ä½¿ç”¨éšæœºæ£®æ—æ•™å¸ˆæ¥å¼ºåŒ–æ ‘å†³ç­–ç­–ç•¥ï¼Œæé«˜æ•°å€¼å»ºæ¨¡çš„ç¨³å¥æ€§ã€‚</li>
<li>ä¸²è¡ŒåŒ–ä»»åŠ¡ä»¥æœ‰æ•ˆåˆ©ç”¨ä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶é€šè¿‡æ‰¹é‡æ¨ç†æé«˜ååé‡ã€‚</li>
<li>MachineLearningLMåœ¨å¤šç§é¢†åŸŸè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯ç¦»åˆ†å¸ƒè¡¨æ ¼åˆ†ç±»ä»»åŠ¡ä¸Šä¼˜äºå…¶ä»–LLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06806">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.06806v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.06806v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2509.06806v4/page_4_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Input-Time-Scaling"><a href="#Input-Time-Scaling" class="headerlink" title="Input-Time Scaling"></a>Input-Time Scaling</h2><p><strong>Authors:Rapheal Huang, Weilong Guo</strong></p>
<p>Current Large Language Models (LLMs) are usually post-trained on large-scale carefully curated datasets (data &amp; training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Input-Time Scaling, to complement previous scaling methods by putting resources on queries (input time). During training and testing, we utilize meta-knowledge from LLMs to refine inputs with different strategies. We also discover a new phenomenon, train-test co-design. It requires us to apply query strategies during training and testing as a whole. Only applying strategies on training or testing would seriously degrade the performance gained. We are also surprised to find that seemingly low data quality datasets can perform better. We can get the best performance even by adding irrelevant information to the queries, with randomly selected 1k examples from a minimally filtered dataset. These findings contradict the widely held inductive bias, â€œgarbage in, garbage outâ€. Curating datasets with seemingly high-quality data can even potentially limit the performance ceiling. In addition, models trained on more data with similar quality (15k VS 1k) perform worse, the intuition of simply scaling the size should also be carefully inspected. The good news is that our findings are compatible with the Less is More phenomenon. 1K examples are enough to invoke high-level reasoning ability. With experiments on Qwen2.5-32B-Instruct, we are able to reach SOTA performance among 32B models on AIME24(76.7%) and AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B, the result would be 90.0% on AIME24 and 80.0% on AIME25. To facilitate reproducibility and further research, we are working on open-source our datasets, data pipelines, evaluation results, and checkpoints. </p>
<blockquote>
<p>å½“å‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸åœ¨å¤§è§„æ¨¡ç²¾å¿ƒç­–åˆ’çš„æ•°æ®é›†ä¸Šè¿›è¡Œåè®­ç»ƒï¼ˆæ•°æ®å’Œè®­ç»ƒè§„æ¨¡åŒ–ï¼‰ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶é—´ï¼ˆæ¨ç†æ—¶é—´è§„æ¨¡åŒ–ï¼‰è¿›è¡Œæ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§„æ¨¡åŒ–èŒƒå¼â€”â€”è¾“å…¥æ—¶é—´è§„æ¨¡åŒ–ï¼Œä»¥è¡¥å……ä¹‹å‰çš„è§„æ¨¡åŒ–æ–¹æ³•ï¼Œå°†èµ„æºé›†ä¸­åœ¨æŸ¥è¯¢ï¼ˆè¾“å…¥æ—¶é—´ï¼‰ä¸Šã€‚åœ¨è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨LLMçš„å…ƒçŸ¥è¯†ï¼Œé‡‡ç”¨ä¸åŒçš„ç­–ç•¥æ¥ä¼˜åŒ–è¾“å…¥ã€‚æˆ‘ä»¬è¿˜å‘ç°äº†ä¸€ä¸ªæ–°ç°è±¡ï¼Œå³è®­ç»ƒä¸æµ‹è¯•çš„ååŒè®¾è®¡ã€‚å®ƒè¦æ±‚æˆ‘ä»¬åœ¨è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­æ•´ä½“åº”ç”¨æŸ¥è¯¢ç­–ç•¥ã€‚ä»…å¯¹è®­ç»ƒæˆ–æµ‹è¯•åº”ç”¨ç­–ç•¥ä¼šä¸¥é‡é™ä½æ‰€è·å¾—çš„æ€§èƒ½ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿ä½¿ç”¨ä½è´¨é‡çš„æ•°æ®é›†ä¹Ÿèƒ½å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å‘æŸ¥è¯¢ä¸­æ·»åŠ æ— å…³ä¿¡æ¯ï¼Œä»¥åŠä»æœ€å°‘è¿‡æ»¤çš„æ•°æ®é›†ä¸­éšæœºé€‰æ‹©1kä¸ªç¤ºä¾‹æ¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚è¿™äº›å‘ç°ä¸å¹¿ä¸ºæµä¼ çš„å½’çº³åè§â€œåƒåœ¾è¿›ï¼Œåƒåœ¾å‡ºâ€ç›¸çŸ›ç›¾ã€‚ä½¿ç”¨çœ‹ä¼¼é«˜è´¨é‡æ•°æ®çš„æ•°æ®é›†ç”šè‡³å¯èƒ½é™åˆ¶æ€§èƒ½ä¸Šé™ã€‚æ­¤å¤–ï¼Œåœ¨ç›¸ä¼¼è´¨é‡çš„æ•°æ®ä¸Šè®­ç»ƒæ›´å¤šæ•°æ®çš„æ¨¡å‹ï¼ˆ15kå¯¹1kï¼‰è¡¨ç°æ›´å·®ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦è°¨æ…æ£€æŸ¥å•çº¯æ‰©å¤§è§„æ¨¡ç›´è§‰ã€‚å¥½æ¶ˆæ¯æ˜¯ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ä¸â€œå°‘å³æ˜¯å¤šâ€ç°è±¡ç›¸å»åˆã€‚1Kä¸ªä¾‹å­å°±è¶³ä»¥æ¿€å‘é«˜çº§æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨Qwen2.5-32B-Instructä¸Šçš„å®éªŒï¼Œèƒ½å¤Ÿåœ¨AIME24ï¼ˆ76.7%ï¼‰å’ŒAIME25ï¼ˆ76.7%ï¼‰çš„pass@1ä¸Šè¾¾åˆ°32Bæ¨¡å‹ä¸­çš„SOTAæ€§èƒ½ã€‚æˆ‘ä»¬é€šè¿‡ä¸‰ä¸ªæ¨¡å‹çš„å¤§å¤šæ•°æŠ•ç¥¨ï¼Œèƒ½å¤Ÿè¿›ä¸€æ­¥è¾¾åˆ°AIME24ï¼ˆ76.7%ï¼‰å’ŒAIME25ï¼ˆ80%ï¼‰ã€‚ä»DeepSeek-R1-Distill-Qwen-32Bå¼€å§‹ï¼ŒAIME24çš„ç»“æœå°†è¾¾åˆ°90.0%ï¼ŒAIME25å°†è¾¾åˆ°80.0%ã€‚ä¸ºäº†ä¾¿äºå¤åˆ¶å’Œè¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬æ­£åœ¨å¼€æºæˆ‘ä»¬çš„æ•°æ®é›†ã€æ•°æ®ç®¡é“ã€è¯„ä¼°ç»“æœå’Œæ£€æŸ¥ç‚¹ã€‚</p>
</blockquote>
<p><strong>Translation into Simplified Chinese</strong></p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.13654v4">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ¨¡å‹ç¼©æ”¾èŒƒå¼â€”â€”è¾“å…¥æ—¶é—´ç¼©æ”¾ï¼Œå®ƒä¸“æ³¨äºåœ¨æŸ¥è¯¢ï¼ˆè¾“å…¥æ—¶é—´ï¼‰ä¸Šåˆ†é…èµ„æºï¼Œä»¥è¡¥å……ç°æœ‰çš„æ•°æ®è®­ç»ƒç¼©æ”¾å’Œæ¨ç†æ—¶é—´ç¼©æ”¾æ–¹æ³•ã€‚ç ”ç©¶å‘ç°äº†æ–°çš„ç°è±¡è®­ç»ƒæµ‹è¯•ååŒè®¾è®¡ï¼Œéœ€è¦åœ¨è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­åº”ç”¨æŸ¥è¯¢ç­–ç•¥ä»¥æé«˜æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œçœ‹ä¼¼ä½è´¨é‡çš„æ•°æ®é›†è¡¨ç°è‰¯å¥½ï¼Œç”šè‡³å¯ä»¥é€šè¿‡æ·»åŠ ä¸æŸ¥è¯¢ç›¸å…³çš„æ— å…³ä¿¡æ¯æé«˜æ€§èƒ½ã€‚åŒæ—¶æŒ‡å‡ºå•çº¯æ‰©å¤§è§„æ¨¡è€Œå¿½è§†æ•°æ®è´¨é‡å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œè€Œå°‘é‡çš„é«˜è´¨é‡æ•°æ®å¯ä»¥è§¦å‘é«˜çº§æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºæŸäº›æ¨¡å‹èƒ½å¤Ÿè¾¾åˆ°SOTAæ€§èƒ½ã€‚æœªæ¥ä¼šå¼€æºæ•°æ®é›†ã€æ•°æ®ç®¡é“ã€è¯„ä¼°ç»“æœå’Œæ£€æŸ¥ç‚¹ä»¥ä¿ƒè¿›ç ”ç©¶å’Œå¯é‡å¤æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§æ–°çš„æ¨¡å‹ç¼©æ”¾èŒƒå¼â€”â€”è¾“å…¥æ—¶é—´ç¼©æ”¾ï¼Œå…³æ³¨æŸ¥è¯¢èµ„æºçš„åˆ†é…ã€‚</li>
<li>å‘ç°äº†è®­ç»ƒæµ‹è¯•ååŒè®¾è®¡çš„æ–°ç°è±¡ï¼Œå¼ºè°ƒåœ¨è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­åº”ç”¨æŸ¥è¯¢ç­–ç•¥çš„é‡è¦æ€§ã€‚</li>
<li>è¡¨æ˜çœ‹ä¼¼ä½è´¨é‡çš„æ•°æ®é›†å¯èƒ½è¡¨ç°æ›´å¥½ï¼ŒæŒ‘æˆ˜äº†â€œåƒåœ¾è¿›åƒåœ¾å‡ºâ€çš„å½’çº³åè§ã€‚</li>
<li>å•çº¯æ‰©å¤§è§„æ¨¡è€Œä¸è€ƒè™‘æ•°æ®è´¨é‡å¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>å°‘é‡é«˜è´¨é‡æ•°æ®å¯ä»¥è§¦å‘é«˜çº§æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºæŸäº›æ¨¡å‹èƒ½å¤Ÿè¾¾åˆ°SOTAæ€§èƒ½ï¼ŒåŒ…æ‹¬åœ¨æŸäº›ç‰¹å®šä»»åŠ¡ä¸Šçš„é«˜é€šè¿‡ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.13654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2508.13654v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2508.13654v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2508.13654v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2508.13654v4/page_4_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2508.13654v4/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2508.13654v4/page_5_1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AReaL-A-Large-Scale-Asynchronous-Reinforcement-Learning-System-for-Language-Reasoning"><a href="#AReaL-A-Large-Scale-Asynchronous-Reinforcement-Learning-System-for-Language-Reasoning" class="headerlink" title="AReaL: A Large-Scale Asynchronous Reinforcement Learning System for   Language Reasoning"></a>AReaL: A Large-Scale Asynchronous Reinforcement Learning System for   Language Reasoning</h2><p><strong>Authors:Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, Yi Wu</strong></p>
<p>Reinforcement learning (RL) has become a dominant paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous, alternating generation and training in a batch setting where rollouts in each training batch are generated by the same model. This approach stabilizes RL training but suffers from severe system-level inefficiency: generation must wait until the longest output in the batch is completed before model updates, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.77$\times$ training speedup compared to synchronous systems with the same number of GPUs and matched or improved final performance. The code of AReaL is available at <a target="_blank" rel="noopener" href="https://github.com/inclusionAI/AReaL/">https://github.com/inclusionAI/AReaL/</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸»å¯¼èŒƒå¼ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†ä»»åŠ¡ä¸­ã€‚å¯¹äºLLMçš„æœ‰æ•ˆRLéœ€è¦å¤§è§„æ¨¡å¹¶è¡ŒåŒ–ï¼Œå¹¶è¿«åˆ‡éœ€è¦é«˜æ•ˆçš„è®­ç»ƒç³»ç»Ÿã€‚å¤§å¤šæ•°ç°æœ‰çš„å¤§å‹RLç³»ç»Ÿéƒ½æ˜¯åŒæ­¥çš„ï¼Œåˆ†æ‰¹ç”Ÿæˆå’Œè®­ç»ƒäº¤æ›¿è¿›è¡Œï¼Œæ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡ä¸­çš„ç”Ÿæˆéƒ½ç”±åŒä¸€æ¨¡å‹å®Œæˆã€‚è¿™ç§æ–¹æ³•ç¨³å®šäº†RLè®­ç»ƒï¼Œä½†å­˜åœ¨ä¸¥é‡çš„ç³»ç»Ÿçº§æ•ˆç‡ä½ä¸‹ï¼šç”Ÿæˆå¿…é¡»ç­‰å¾…æ‰¹æ¬¡ä¸­æœ€é•¿çš„è¾“å‡ºå®Œæˆæ‰èƒ½è¿›è¡Œæ¨¡å‹æ›´æ–°ï¼Œå¯¼è‡´GPUåˆ©ç”¨ç‡ä¸è¶³ã€‚æˆ‘ä»¬æå‡ºäº†AReaLï¼Œä¸€ä¸ªå®Œå…¨å¼‚æ­¥çš„RLç³»ç»Ÿï¼Œå®ƒå°†ç”Ÿæˆå’Œè®­ç»ƒå®Œå…¨è§£è€¦ã€‚AReaLä¸­çš„ç”Ÿæˆå·¥ä½œå™¨å¯ä»¥æŒç»­ç”Ÿæˆæ–°çš„è¾“å‡ºè€Œæ— éœ€ç­‰å¾…ï¼Œè€Œè®­ç»ƒå·¥ä½œå™¨åˆ™ä¼šåœ¨æ¯æ¬¡æ”¶é›†åˆ°ä¸€æ‰¹æ•°æ®æ—¶æ›´æ–°æ¨¡å‹ã€‚AReaLè¿˜é‡‡ç”¨äº†ä¸€ç³»åˆ—ç³»ç»Ÿçº§ä¼˜åŒ–ï¼Œå¤§å¤§æé«˜äº†GPUåˆ©ç”¨ç‡ã€‚ä¸ºäº†ç¨³å®šRLè®­ç»ƒï¼ŒAReaLå¹³è¡¡äº†ç”Ÿæˆå’Œè®­ç»ƒå·¥ä½œå™¨çš„è´Ÿè½½ï¼Œä»¥æ§åˆ¶æ•°æ®çš„é™ˆæ—§ç¨‹åº¦ï¼Œå¹¶é‡‡ç”¨äº†å¢å¼ºé™ˆæ—§æ€§çš„PPOå˜ä½“æ¥æ›´å¥½åœ°å¤„ç†è¿‡æ—¶çš„è®­ç»ƒæ ·æœ¬ã€‚åœ¨æ•°å­¦å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸å…·æœ‰ç›¸åŒæ•°é‡GPUçš„åŒæ­¥ç³»ç»Ÿç›¸æ¯”ï¼ŒAReaLçš„è®­ç»ƒé€Ÿåº¦æé«˜äº†2.77å€ï¼Œå¹¶ä¸”å…·æœ‰ç›¸åŒ¹é…æˆ–æ›´å¥½çš„æœ€ç»ˆæ€§èƒ½ã€‚AReaLçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/inclusionAI/AReaL/%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/inclusionAI/AReaL/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.24298v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒä¸­çš„ä¸»å¯¼åœ°ä½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³å¤§è§„æ¨¡RLç³»ç»Ÿä¸­å­˜åœ¨çš„ç³»ç»Ÿçº§æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„å¼‚æ­¥RLç³»ç»ŸAReaLã€‚AReaLé€šè¿‡å°†ç”Ÿæˆä¸è®­ç»ƒå®Œå…¨è§£è€¦ï¼Œå®ç°äº†è¿ç»­çš„ç”Ÿæˆè¾“å‡ºå’Œæ¨¡å‹æ›´æ–°ï¼Œä»è€Œå¤§å¤§æé«˜äº†GPUåˆ©ç”¨ç‡ã€‚åŒæ—¶ï¼ŒAReaLè¿˜é€šè¿‡ä¸€ç³»åˆ—ç³»ç»Ÿçº§ä¼˜åŒ–æªæ–½æ¥å¹³è¡¡å·¥ä½œè´Ÿè½½å¹¶æ§åˆ¶æ•°æ®é™ˆæ—§æ€§ï¼Œé‡‡ç”¨äº†ä¸€ç§å¢å¼ºé™ˆæ—§æ€§çš„PPOå˜ä½“ä»¥æ›´å¥½åœ°å¤„ç†è¿‡æ—¶çš„è®­ç»ƒæ ·æœ¬ã€‚åœ¨æ•°å­¦å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒAReaLç›¸è¾ƒäºåŒæ­¥ç³»ç»Ÿå®ç°äº†æœ€é«˜è¾¾2.77å€çš„è®­ç»ƒé€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒäº†åŒ¹é…æˆ–æ›´å¥½çš„æœ€ç»ˆæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å·²æˆä¸ºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒçš„ä¸»è¦æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ¨ç†ä»»åŠ¡æ—¶ã€‚</li>
<li>ç°æœ‰å¤§è§„æ¨¡RLç³»ç»Ÿåœ¨è®­ç»ƒLLMæ—¶å­˜åœ¨ç³»ç»Ÿçº§æ•ˆç‡ä½ä¸‹é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å…¨æ–°çš„å¼‚æ­¥RLç³»ç»ŸAReaLï¼Œå°†ç”Ÿæˆä¸è®­ç»ƒè¿‡ç¨‹å®Œå…¨åˆ†ç¦»ï¼Œå®ç°äº†é«˜æ•ˆçš„GPUåˆ©ç”¨ã€‚</li>
<li>AReaLé€šè¿‡ä¸€ç³»åˆ—ä¼˜åŒ–æªæ–½å¹³è¡¡ç”Ÿæˆä¸è®­ç»ƒçš„å·¥ä½œè´Ÿè½½ï¼Œæ§åˆ¶æ•°æ®é™ˆæ—§æ€§ã€‚</li>
<li>AReaLé‡‡ç”¨äº†ä¸€ç§å¢å¼ºé™ˆæ—§æ€§çš„PPOå˜ä½“ï¼Œä»¥å¤„ç†è¿‡æ—¶çš„è®­ç»ƒæ ·æœ¬ã€‚</li>
<li>åœ¨æ•°å­¦å’Œä»£ç æ¨ç†æµ‹è¯•ä¸­ï¼ŒAReaLç›¸è¾ƒäºåŒæ­¥ç³»ç»Ÿæ˜¾è‘—æå‡äº†è®­ç»ƒé€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.24298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2505.24298v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2505.24298v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2505.24298v3/page_3_1.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Towards-Revealing-the-Effectiveness-of-Small-Scale-Fine-tuning-in-R1-style-Reinforcement-Learning"><a href="#Towards-Revealing-the-Effectiveness-of-Small-Scale-Fine-tuning-in-R1-style-Reinforcement-Learning" class="headerlink" title="Towards Revealing the Effectiveness of Small-Scale Fine-tuning in   R1-style Reinforcement Learning"></a>Towards Revealing the Effectiveness of Small-Scale Fine-tuning in   R1-style Reinforcement Learning</h2><p><strong>Authors:Yutong Chen, Jiandong Gao, Ji Wu</strong></p>
<p>R1-style Reinforcement Learning (RL) significantly enhances Large Language Modelsâ€™ reasoning capabilities, yet the mechanism behind rule-based RL remains unclear. We found that small-scale SFT has substantial influence on RL but shows poor efficiency. To explain our observations, we propose an analytical framework and compare the efficiency of SFT and RL by measuring \textbf{sample effect}. Our hypothetical analysis shows the potential to improve SFT efficiency. Guided by our analysis, we propose \textbf{Re-distillation}, a technique that aims to boost the effectiveness of small-scale distillation by sampling from the RL-trained policy. Re-distillation shows consistent surprising efficiency on three datasets and both Qwen&amp;Llama models: Re-distilled models matched RL performance with far fewer samples and less computation. As a result, on K&amp;K dataset, our re-distilled Qwen-2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT samples. We demonstrate that re-distillation can be used to efficiently balance multiple goals in RL. Our work explains several interesting phenomena in R1-style RL, shedding light on the mechanisms behind its empirical success. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/on1262/deep-reasoning">https://github.com/on1262/deep-reasoning</a>. </p>
<blockquote>
<p>R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¾è‘—å¢å¼ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œç„¶è€ŒåŸºäºè§„åˆ™çš„RLèƒŒåçš„æœºåˆ¶ä»ç„¶ä¸æ¸…æ¥šã€‚æˆ‘ä»¬å‘ç°å°è§„æ¨¡SFTå¯¹RLæœ‰å®è´¨å½±å“ï¼Œä½†æ•ˆç‡è¾ƒä½ã€‚ä¸ºäº†è§£é‡Šæˆ‘ä»¬çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶ï¼Œé€šè¿‡æµ‹é‡â€œæ ·æœ¬æ•ˆåº”â€æ¥æ¯”è¾ƒSFTå’ŒRLçš„æ•ˆç‡ã€‚æˆ‘ä»¬çš„å‡è®¾åˆ†ææ˜¾ç¤ºæé«˜SFTæ•ˆç‡çš„æ½œåŠ›ã€‚åœ¨æˆ‘ä»¬çš„åˆ†ææŒ‡å¯¼ä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†â€œå†è’¸é¦â€æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ—¨åœ¨é€šè¿‡ä»RLè®­ç»ƒçš„ç­–ç•¥ä¸­è¿›è¡Œé‡‡æ ·ï¼Œæé«˜å°è§„æ¨¡è’¸é¦çš„æœ‰æ•ˆæ€§ã€‚å†è’¸é¦åœ¨ä¸‰ä¸ªæ•°æ®é›†å’Œä¸¤ä¸ªQwen&amp;Llamaæ¨¡å‹ä¸Šå‡æ˜¾ç¤ºå‡ºä»¤äººæƒŠè®¶çš„æŒç»­æ€§é«˜æ•ˆã€‚å†è’¸é¦æ¨¡å‹ä½¿ç”¨è¾ƒå°‘çš„æ ·æœ¬å’Œè®¡ç®—èµ„æºå³å¯è¾¾åˆ°RLçš„æ€§èƒ½ã€‚å› æ­¤ï¼Œåœ¨K&amp;Kæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬å†è’¸é¦çš„Qwen-2.5-1.5Bæ¨¡å‹ä»…ä½¿ç”¨1Kä¸ªSFTæ ·æœ¬å°±è¶…è¿‡äº†DeepSeek-V3-0324ã€‚æˆ‘ä»¬è¯æ˜å†è’¸é¦å¯ç”¨äºæœ‰æ•ˆåœ°å¹³è¡¡RLä¸­çš„å¤šä¸ªç›®æ ‡ã€‚æˆ‘ä»¬çš„å·¥ä½œè§£é‡Šäº†R1é£æ ¼RLä¸­çš„å‡ ä¸ªæœ‰è¶£ç°è±¡ï¼Œæ­ç¤ºäº†å…¶ç»éªŒæˆåŠŸçš„æœºåˆ¶ã€‚ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/on1262/deep-reasoning%E3%80%82">https://github.com/on1262/deep-reasoningã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.17988v3">PDF</a> preprint</p>
<p><strong>Summary</strong></p>
<p>R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½å¤Ÿæ˜¾è‘—æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†å…¶èƒŒåçš„æœºåˆ¶å°šä¸æ¸…æ¥šã€‚ç ”ç©¶å‘ç°å°è§„æ¨¡æ ·æœ¬é¢„è®­ç»ƒï¼ˆSFTï¼‰å¯¹RLæœ‰é‡è¦å½±å“ä½†æ•ˆç‡ä¸é«˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶ï¼Œé€šè¿‡æµ‹é‡æ ·æœ¬æ•ˆåº”æ¥æ¯”è¾ƒSFTå’ŒRLçš„æ•ˆç‡ã€‚åŒæ—¶ï¼Œæœ¬æ–‡æå‡ºäº†å†è’¸é¦æŠ€æœ¯ï¼Œæ—¨åœ¨é€šè¿‡ä»RLè®­ç»ƒçš„å†³ç­–ä¸­é‡‡æ ·æ¥æé«˜å°è§„æ¨¡è’¸é¦çš„æœ‰æ•ˆæ€§ã€‚å†è’¸é¦æŠ€æœ¯åœ¨ä¸‰ä¸ªæ•°æ®é›†å’Œä¸¤ä¸ªæ¨¡å‹ä¸Šçš„è¡¨ç°å‡ä»¤äººæƒŠè®¶ï¼Œç”¨æ›´å°‘çš„æ ·æœ¬å’Œè®¡ç®—é‡è¾¾åˆ°äº†ä¸RLç›¸å½“çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œå†è’¸é¦æœ‰æœ›æˆä¸ºé«˜æ•ˆå¹³è¡¡RLä¸­å¤šä¸ªç›®æ ‡çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>R1é£æ ¼çš„å¼ºåŒ–å­¦ä¹ å¤§å¹…æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä½†æœºåˆ¶ä¸æ˜ã€‚</li>
<li>å°è§„æ¨¡æ ·æœ¬é¢„è®­ç»ƒå¯¹å¼ºåŒ–å­¦ä¹ æœ‰é‡è¦å½±å“ï¼Œä½†å…¶æ•ˆç‡ä¸é«˜ã€‚</li>
<li>é€šè¿‡æµ‹é‡æ ·æœ¬æ•ˆåº”ï¼Œæå‡ºäº†ä¸€ä¸ªåˆ†ææ¡†æ¶æ¥æ¯”è¾ƒæ ·æœ¬é¢„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ çš„æ•ˆç‡ã€‚</li>
<li>æå‡ºäº†å†è’¸é¦æŠ€æœ¯ï¼Œæ—¨åœ¨æé«˜å°è§„æ¨¡è’¸é¦çš„æœ‰æ•ˆæ€§ï¼Œé€šè¿‡ä»å¼ºåŒ–å­¦ä¹ çš„å†³ç­–ä¸­é‡‡æ ·ã€‚</li>
<li>å†è’¸é¦æŠ€æœ¯åœ¨å¤šä¸ªæ•°æ®é›†å’Œæ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œç”¨æ›´å°‘çš„æ ·æœ¬å’Œè®¡ç®—é‡è¾¾åˆ°äº†ä¸å¼ºåŒ–å­¦ä¹ ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>å†è’¸é¦æœ‰åŠ©äºé«˜æ•ˆå¹³è¡¡å¼ºåŒ–å­¦ä¹ ä¸­çš„å¤šä¸ªç›®æ ‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.17988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2505.17988v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2505.17988v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2505.17988v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2505.17988v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Prior-Prompt-Engineering-for-Reinforcement-Fine-Tuning"><a href="#Prior-Prompt-Engineering-for-Reinforcement-Fine-Tuning" class="headerlink" title="Prior Prompt Engineering for Reinforcement Fine-Tuning"></a>Prior Prompt Engineering for Reinforcement Fine-Tuning</h2><p><strong>Authors:Pittawat Taveekitworachai, Potsawee Manakul, Sarana Nutanong, Kunat Pipatanakul</strong></p>
<p>This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior promptâ€“the instructions prepended to queries during training to elicit behaviors such as step-by-step reasoningâ€“remains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategiesâ€“reasoning, planning, code-based reasoning, knowledge recall, and null-example utilizationâ€“into corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting a behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as a powerful yet understudied axis for RFT. </p>
<blockquote>
<p>æœ¬æ–‡æ¢è®¨äº†å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰èƒŒæ™¯ä¸‹çš„å‰æœŸæç¤ºå·¥ç¨‹ï¼ˆpPEï¼‰ï¼Œåœ¨RFTä¸­ï¼Œé€šè¿‡å¥–åŠ±ä¿¡å·æ¿€åŠ±è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰è¡¨ç°å‡ºæœ€å¤§åŒ–æ€§èƒ½çš„è¡Œä¸ºã€‚è™½ç„¶ç°æœ‰çš„RFTç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç®—æ³•ã€å¥–åŠ±å¡‘é€ å’Œæ•°æ®é›†æˆä¸Šï¼Œä½†å‰æœŸæç¤ºçš„è®¾è®¡ï¼Œå³åœ¨è®­ç»ƒæœŸé—´é™„åŠ åˆ°æŸ¥è¯¢ä¸­çš„æŒ‡ä»¤ï¼Œä»¥æ¿€å‘å¦‚é€æ­¥æ¨ç†ç­‰è¡Œä¸ºï¼Œä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬è°ƒæŸ¥ä¸åŒçš„pPEæ–¹æ³•æ˜¯å¦èƒ½åœ¨RFTåå¼•å¯¼è¯­è¨€æ¨¡å‹å†…åŒ–ä¸åŒçš„è¡Œä¸ºã€‚æˆ‘ä»¬å—åˆ°æ¨ç†æ—¶é—´æç¤ºå·¥ç¨‹ï¼ˆiPEï¼‰çš„å¯å‘ï¼Œå°†äº”ç§ä»£è¡¨æ€§çš„iPEç­–ç•¥â€”â€”æ¨ç†ã€è§„åˆ’ã€åŸºäºä»£ç æ¨ç†ã€çŸ¥è¯†å›å¿†å’Œç©ºä¾‹åˆ©ç”¨â€”â€”è½¬åŒ–ä¸ºç›¸åº”çš„pPEæ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨æ¯ç§pPEæ–¹æ³•å®éªŒQwen2.5-7Bï¼Œç„¶ååœ¨åŸŸå†…å’ŒåŸŸå¤–åŸºå‡†æµ‹è¯•ï¼ˆä¾‹å¦‚AIME2024ã€HumanEval+å’ŒGPQA-Diamondï¼‰ä¸Šè¯„ä¼°æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ‰€æœ‰pPEè®­ç»ƒçš„æ¨¡å‹éƒ½è¶…è¿‡äº†iPEæç¤ºçš„å¯¹åº”æ¨¡å‹ï¼Œå…¶ä¸­ç©ºä¾‹pPEæ–¹æ³•å–å¾—äº†æœ€å¤§çš„å¹³å‡æ€§èƒ½æå‡ï¼Œå¹¶åœ¨AIME2024å’ŒGPQA-Diamondä¸Šå®ç°äº†æœ€é«˜æ”¹è¿›ï¼Œè¶…è¶Šäº†å¸¸ç”¨çš„æ¨ç†æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé€šè¿‡é€‚åº”è¡Œä¸ºåˆ†ç±»æ¡†æ¶ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸åŒçš„pPEç­–ç•¥ä¼šåœ¨ç»“æœæ¨¡å‹ä¸­å½¢æˆä¸åŒçš„è¡Œä¸ºé£æ ¼ã€‚è¿™äº›å‘ç°å°†pPEå®šä½ä¸ºRFTä¸­å¼ºå¤§è€Œæœªè¢«å……åˆ†ç ”ç©¶çš„è½´ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.14157v2">PDF</a> Accepted at EMNLP 2025, Main; 26 pages, 42 figures</p>
<p><strong>Summary</strong>ï¼šæœ¬æ–‡æ¢è®¨äº†å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰èƒŒæ™¯ä¸‹çš„å‰æœŸæç¤ºå·¥ç¨‹ï¼ˆpPEï¼‰ï¼Œç ”ç©¶äº†å¦‚ä½•é€šè¿‡å¥–åŠ±ä¿¡å·æ¿€åŠ±è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰è¡¨ç°å‡ºæœ€å¤§åŒ–æ€§èƒ½çš„è¡Œä¸ºã€‚æ–‡ç« ä¸»è¦å…³æ³¨å¦‚ä½•å°†æ¨ç†æ—¶é—´æç¤ºå·¥ç¨‹ï¼ˆiPEï¼‰çš„ç­–ç•¥è½¬åŒ–ä¸ºå‰æœŸæç¤ºå·¥ç¨‹ï¼ˆpPEï¼‰çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†ä¸åŒpPEç­–ç•¥åœ¨æŒ‡å¯¼è¯­è¨€æ¨¡å‹å†…åŒ–è¡Œä¸ºæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨pPEè®­ç»ƒçš„æ¨¡å‹åœ¨æ€§èƒ½å’Œé£æ ¼ä¸Šå‡ä¼˜äºé‡‡ç”¨iPEæç¤ºçš„æ¨¡å‹ï¼Œå…¶ä¸­ç©ºä¾‹pPEæ–¹æ³•å–å¾—æœ€å¤§å¹³å‡æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ–‡ç« æ¢è®¨äº†å‰æœŸæç¤ºå·¥ç¨‹ï¼ˆpPEï¼‰åœ¨å¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ä¸­çš„ä½œç”¨ï¼Œç ”ç©¶å¦‚ä½•é€šè¿‡å¥–åŠ±ä¿¡å·ä½¿è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºæœ€å¤§åŒ–æ€§èƒ½çš„è¡Œä¸ºã€‚</li>
<li>æ–‡ç« å°†æ¨ç†æ—¶é—´æç¤ºå·¥ç¨‹ï¼ˆiPEï¼‰çš„ç­–ç•¥è½¬åŒ–ä¸ºpPEçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æ¨ç†ã€è§„åˆ’ã€åŸºäºä»£ç æ¨ç†ã€çŸ¥è¯†å›å¿†å’Œç©ºä¾‹åˆ©ç”¨ç­‰äº”ç§ä»£è¡¨æ€§ç­–ç•¥ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡‡ç”¨pPEè®­ç»ƒçš„æ¨¡å‹åœ¨æ€§èƒ½å’Œé£æ ¼ä¸Šå‡ä¼˜äºä»…ä½¿ç”¨iPEæç¤ºçš„æ¨¡å‹ã€‚</li>
<li>ç©ºä¾‹pPEæ–¹æ³•å–å¾—æœ€å¤§çš„å¹³å‡æ€§èƒ½æå‡ï¼Œå¹¶åœ¨AIME2024å’ŒGPQA-Diamondä¸Šè¡¨ç°æœ€ä½³ã€‚</li>
<li>ä¸åŒpPEç­–ç•¥ä¼šå¯¼è‡´æ¨¡å‹è¡¨ç°å‡ºä¸åŒçš„è¡Œä¸ºé£æ ¼ã€‚</li>
<li>pPEä½œä¸ºå¼ºåŒ–å¾®è°ƒçš„ä¸€ä¸ªå¼ºå¤§è€Œå°šæœªå……åˆ†ç ”ç©¶çš„è½´ï¼Œå…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.14157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2505.14157v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2505.14157v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2505.14157v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2505.14157v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2505.14157v2/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2505.14157v2/page_5_1.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="An-Empirical-Study-of-Position-Bias-in-Modern-Information-Retrieval"><a href="#An-Empirical-Study-of-Position-Bias-in-Modern-Information-Retrieval" class="headerlink" title="An Empirical Study of Position Bias in Modern Information Retrieval"></a>An Empirical Study of Position Bias in Modern Information Retrieval</h2><p><strong>Authors:Ziyang Zeng, Dun Zhang, Jiacheng Li, Panxiang Zou, Yudong Zhou, Shengjie Wang, Yuqing Yang</strong></p>
<p>This study investigates the position bias in information retrieval, where models tend to overemphasize content at the beginning of passages while neglecting semantically relevant information that appears later. To analyze the extent and impact of position bias, we introduce a new evaluation framework consisting of two position-aware retrieval benchmarks (SQuAD-PosQ, FineWeb-PosQ) and an intuitive diagnostic metric, the Position Sensitivity Index (PSI), for quantifying position bias from a worst-case perspective. We conduct a comprehensive evaluation across the full retrieval pipeline, including BM25, dense embedding models, ColBERT-style late-interaction models, and full-interaction reranker models. Our experiments show that when relevant information appears later in the passage, dense embedding models and ColBERT-style models suffer significant performance degradation (an average drop of 15.6%). In contrast, BM25 and reranker models demonstrate greater robustness to such positional variation. These findings provide practical insights into model sensitivity to the position of relevant information and offer guidance for building more position-robust retrieval systems. Code and data are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/NovaSearch-Team/position-bias-in-IR">https://github.com/NovaSearch-Team/position-bias-in-IR</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†ä¿¡æ¯æ£€ç´¢ä¸­çš„ä½ç½®åè§é—®é¢˜ï¼Œå³æ¨¡å‹å¾€å¾€è¿‡åˆ†å¼ºè°ƒæ®µè½å¼€å¤´çš„å†…å®¹ï¼Œè€Œå¿½è§†åé¢å‡ºç°çš„è¯­ä¹‰ç›¸å…³ä¿¡æ¯ã€‚ä¸ºäº†åˆ†æå’Œè¯„ä¼°ä½ç½®åè§çš„å½±å“ç¨‹åº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªä½ç½®æ„ŸçŸ¥æ£€ç´¢åŸºå‡†æµ‹è¯•ï¼ˆSQuAD-PosQï¼ŒFineWeb-PosQï¼‰å’Œä¸€ä¸ªç›´è§‚çš„è¯Šæ–­æŒ‡æ ‡â€”â€”ä½ç½®æ•æ„Ÿæ€§æŒ‡æ•°ï¼ˆPSIï¼‰ï¼Œä»æœ€å·®æƒ…å†µçš„è§’åº¦é‡åŒ–ä½ç½®åè§ã€‚æˆ‘ä»¬å¯¹æ•´ä¸ªæ£€ç´¢æµç¨‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬BM25ã€å¯†é›†åµŒå…¥æ¨¡å‹ã€ColBERTé£æ ¼çš„åæœŸäº¤äº’æ¨¡å‹ä»¥åŠå…¨äº¤äº’é‡æ’æ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼Œå½“ç›¸å…³ä¿¡æ¯å‡ºç°åœ¨æ®µè½åé¢æ—¶ï¼Œå¯†é›†åµŒå…¥æ¨¡å‹å’ŒColBERTé£æ ¼çš„æ¨¡å‹æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ï¼ˆå¹³å‡ä¸‹é™15.6%ï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒBM25å’Œé‡æ’æ¨¡å‹å¯¹è¿™ç§ä½ç½®å˜åŒ–çš„é²æ£’æ€§æ›´å¼ºã€‚è¿™äº›å‘ç°æä¾›äº†å¯¹æ¨¡å‹å¯¹ç›¸å…³ä¿¡æ¯ä½ç½®çš„æ•æ„Ÿæ€§çš„å®é™…è§è§£ï¼Œå¹¶ä¸ºæ„å»ºæ›´ç¨³å¥çš„ä½ç½®æ£€ç´¢ç³»ç»Ÿæä¾›äº†æŒ‡å¯¼ã€‚ä»£ç å’Œæ•°æ®å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/NovaSearch-Team/position-bias-in-IR%E3%80%82">https://github.com/NovaSearch-Team/position-bias-in-IRã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.13950v4">PDF</a> EMNLP 2025 Findings (camera-ready)</p>
<p><strong>Summary</strong><br>åœ¨ä¿¡æ¯æ£€ç´¢ä¸­å­˜åœ¨ä½ç½®åè§é—®é¢˜ï¼Œå³æ¨¡å‹å¾€å¾€è¿‡åˆ†å¼ºè°ƒæ®µè½å¼€å¤´çš„å†…å®¹ï¼Œè€Œå¿½è§†åé¢å‡ºç°çš„è¯­ä¹‰ç›¸å…³ä¿¡æ¯ã€‚ä¸ºåˆ†æå’Œè¯„ä¼°ä½ç½®åè§çš„å½±å“ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†æ–°çš„è¯„ä¼°æ¡†æ¶å’ŒæŒ‡æ ‡ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªä½ç½®æ„ŸçŸ¥æ£€ç´¢åŸºå‡†æµ‹è¯•ï¼ˆSQuAD-PosQã€FineWeb-PosQï¼‰å’Œä¸€ä¸ªç›´è§‚çš„è¯Šæ–­æŒ‡æ ‡â€”â€”ä½ç½®æ•æ„Ÿæ€§æŒ‡æ•°ï¼ˆPSIï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œå½“ç›¸å…³ä¿¡æ¯å‡ºç°åœ¨æ®µè½åé¢æ—¶ï¼Œå¯†é›†åµŒå…¥æ¨¡å‹å’ŒColBERTé£æ ¼æ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼ˆå¹³å‡ä¸‹é™15.6%ï¼‰ï¼Œè€ŒBM25å’Œé‡æ’æ¨¡å‹å¯¹ä½ç½®å˜åŒ–æ›´å…·é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¿¡æ¯æ£€ç´¢ä¸­å­˜åœ¨ä½ç½®åè§é—®é¢˜ï¼Œæ¨¡å‹æ˜“å¿½ç•¥æ®µè½ååŠéƒ¨åˆ†çš„ç›¸å…³ä¿¡æ¯ã€‚</li>
<li>ç ”ç©¶å¼•å…¥æ–°çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªä½ç½®æ„ŸçŸ¥æ£€ç´¢åŸºå‡†æµ‹è¯•å’Œä½ç½®æ•æ„Ÿæ€§æŒ‡æ•°ï¼ˆPSIï¼‰æŒ‡æ ‡ã€‚</li>
<li>å¯†é›†åµŒå…¥æ¨¡å‹å’ŒColBERTé£æ ¼æ¨¡å‹åœ¨ç›¸å…³ä¿¡æ¯é åæ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚</li>
<li>BM25å’Œé‡æ’æ¨¡å‹å¯¹ä½ç½®å˜åŒ–æ›´å…·é²æ£’æ€§ã€‚</li>
<li>ä½ç½®åè§å½±å“è¯„ä¼°æœ‰åŠ©äºäº†è§£æ¨¡å‹å¯¹ä¸åŒä½ç½®ä¿¡æ¯çš„æ•æ„Ÿæ€§ã€‚</li>
<li>ç ”ç©¶ç»“æœæä¾›æ„å»ºæ›´ç¨³å¥çš„æ£€ç´¢ç³»ç»Ÿçš„æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.13950">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2505.13950v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_R1_Reasoning/2505.13950v4/page_3_0.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-16/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-16/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-16/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-16\./crop_LLM/2509.10371v1/page_0_0.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-16  MatSKRAFT A framework for large-scale materials knowledge extraction   from scientific tables
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-14\./crop_åŒ»å­¦å›¾åƒ/2504.07134v2/page_0_0.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-14  On the Encapsulation of Medical Imaging AI Algorithms
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32271.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
