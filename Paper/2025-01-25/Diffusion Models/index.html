<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-25  IMAGINE-E Image Generation Intelligence Evaluation of State-of-the-art   Text-to-Image Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-6c777205d41ebaddd1102cdfd7df7209.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    54 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-25-æ›´æ–°"><a href="#2025-01-25-æ›´æ–°" class="headerlink" title="2025-01-25 æ›´æ–°"></a>2025-01-25 æ›´æ–°</h1><h2 id="IMAGINE-E-Image-Generation-Intelligence-Evaluation-of-State-of-the-art-Text-to-Image-Models"><a href="#IMAGINE-E-Image-Generation-Intelligence-Evaluation-of-State-of-the-art-Text-to-Image-Models" class="headerlink" title="IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art   Text-to-Image Models"></a>IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art   Text-to-Image Models</h2><p><strong>Authors:Jiayi Lei, Renrui Zhang, Xiangfei Hu, Weifeng Lin, Zhen Li, Wenjian Sun, Ruoyi Du, Le Zhuo, Zhongyu Li, Xinyue Li, Shitian Zhao, Ziyu Guo, Yiting Lu, Peng Gao, Hongsheng Li</strong></p>
<p>With the rapid development of diffusion models, text-to-image(T2I) models have made significant progress, showcasing impressive abilities in prompt following and image generation. Recently launched models such as FLUX.1 and Ideogram2.0, along with others like Dall-E3 and Stable Diffusion 3, have demonstrated exceptional performance across various complex tasks, raising questions about whether T2I models are moving towards general-purpose applicability. Beyond traditional image generation, these models exhibit capabilities across a range of fields, including controllable generation, image editing, video, audio, 3D, and motion generation, as well as computer vision tasks like semantic segmentation and depth estimation. However, current evaluation frameworks are insufficient to comprehensively assess these modelsâ€™ performance across expanding domains. To thoroughly evaluate these models, we developed the IMAGINE-E and tested six prominent models: FLUX.1, Ideogram2.0, Midjourney, Dall-E3, Stable Diffusion 3, and Jimeng. Our evaluation is divided into five key domains: structured output generation, realism, and physical consistency, specific domain generation, challenging scenario generation, and multi-style creation tasks. This comprehensive assessment highlights each modelâ€™s strengths and limitations, particularly the outstanding performance of FLUX.1 and Ideogram2.0 in structured and specific domain tasks, underscoring the expanding applications and potential of T2I models as foundational AI tools. This study provides valuable insights into the current state and future trajectory of T2I models as they evolve towards general-purpose usability. Evaluation scripts will be released at <a target="_blank" rel="noopener" href="https://github.com/jylei16/Imagine-e">https://github.com/jylei16/Imagine-e</a>. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œå±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æç¤ºè·Ÿè¸ªå’Œå›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚æœ€è¿‘å‘å¸ƒçš„æ¨¡å‹ï¼Œå¦‚FLUX.1å’ŒIdeogram2.0ï¼Œä»¥åŠDall-E3å’ŒStable Diffusion 3ç­‰å…¶ä»–æ¨¡å‹ï¼Œåœ¨å„ç§å¤æ‚ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå¼•å‘äº†å…³äºT2Iæ¨¡å‹æ˜¯å¦æ­£æœç€é€šç”¨é€‚ç”¨æ€§å‘å±•çš„ç–‘é—®ã€‚è¿™äº›æ¨¡å‹ä¸ä»…å…·å¤‡ä¼ ç»Ÿçš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œè¿˜æ¶‰è¶³å¯æ§ç”Ÿæˆã€å›¾åƒç¼–è¾‘ã€è§†é¢‘ã€éŸ³é¢‘ã€3Då’Œè¿åŠ¨ç”Ÿæˆç­‰é¢†åŸŸï¼Œä»¥åŠè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå¦‚è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯„ä¼°æ¡†æ¶ä¸è¶³ä»¥å…¨é¢è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ä¸æ–­æ‰©å¤§é¢†åŸŸçš„æ€§èƒ½ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°è¿™äº›æ¨¡å‹ï¼Œæˆ‘ä»¬å¼€å‘äº†IMAGINE-Eï¼Œå¹¶æµ‹è¯•äº†å…­ä¸ªçªå‡ºæ¨¡å‹ï¼šFLUX.1ã€Ideogram2.0ã€Midjourneyã€Dall-E3ã€Stable Diffusion 3å’ŒJimengã€‚æˆ‘ä»¬çš„è¯„ä¼°åˆ†ä¸ºäº”ä¸ªå…³é”®é¢†åŸŸï¼šç»“æ„åŒ–è¾“å‡ºç”Ÿæˆã€ç°å®æ„Ÿå’Œç‰©ç†ä¸€è‡´æ€§ã€ç‰¹å®šé¢†åŸŸç”Ÿæˆã€æŒ‘æˆ˜åœºæ™¯ç”Ÿæˆå’Œå¤šé£æ ¼åˆ›å»ºä»»åŠ¡ã€‚è¿™ä»½å…¨é¢çš„è¯„ä¼°çªå‡ºäº†æ¯ä¸ªæ¨¡å‹çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯FLUX.1å’ŒIdeogram2.0åœ¨ç»“æ„å’Œç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­çš„å‡ºè‰²è¡¨ç°ï¼Œçªæ˜¾äº†T2Iæ¨¡å‹ä½œä¸ºåŸºç¡€äººå·¥æ™ºèƒ½å·¥å…·çš„ä¸æ–­æ‰©å¤§çš„åº”ç”¨å’Œæ½œåŠ›ã€‚æœ¬ç ”ç©¶ä¸ºT2Iæ¨¡å‹å½“å‰çŠ¶æ€å’Œæœªæ¥å‘å±•è¶‹åŠ¿æä¾›äº†å®è´µè§è§£ï¼Œå®ƒä»¬æ­£æœç€é€šç”¨å¯ç”¨æ€§å‘å±•ã€‚è¯„ä¼°è„šæœ¬å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/jylei16/Imagine-e">https://github.com/jylei16/Imagine-e</a>å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13920v1">PDF</a> 75 pages, 73 figures, Evaluation scripts:   <a target="_blank" rel="noopener" href="https://github.com/jylei16/Imagine-e">https://github.com/jylei16/Imagine-e</a></p>
<p><strong>Summary</strong></p>
<p>éšç€æ‰©æ•£æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œæ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹åœ¨æç¤ºè·Ÿè¸ªå’Œå›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚æœ€æ–°æ¨å‡ºçš„æ¨¡å‹ï¼Œå¦‚FLUX.1å’ŒIdeogram2.0ç­‰ï¼Œåœ¨å¤šç§å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¼•å‘å…³äºT2Iæ¨¡å‹æ˜¯å¦æ­£æœç€é€šç”¨é€‚ç”¨æ€§å‘å±•çš„ç–‘é—®ã€‚è¿™äº›æ¨¡å‹ä¸ä»…å…·å¤‡ä¼ ç»Ÿçš„å›¾åƒç”Ÿæˆèƒ½åŠ›ï¼Œè¿˜æ¶‰è¶³å¯æ§ç”Ÿæˆã€å›¾åƒç¼–è¾‘ã€è§†é¢‘ã€éŸ³é¢‘ã€3Då’Œè¿åŠ¨ç”Ÿæˆç­‰é¢†åŸŸï¼Œç”šè‡³æ¶µç›–è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œå¦‚è¯­ä¹‰åˆ†å‰²å’Œæ·±åº¦ä¼°è®¡ã€‚ç„¶è€Œï¼Œå½“å‰çš„è¯„ä»·æ¡†æ¶æ— æ³•å…¨é¢è¯„ä¼°è¿™äº›æ¨¡å‹åœ¨ä¸æ–­æ‰©å¤§é¢†åŸŸä¸­çš„è¡¨ç°ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°è¿™äº›æ¨¡å‹ï¼Œæˆ‘ä»¬å¼€å‘äº†IMAGINE-Eï¼Œå¹¶æµ‹è¯•äº†å…­ä¸ªçªå‡ºæ¨¡å‹ã€‚è¯„ä¼°ç»“æœçªæ˜¾äº†æ¯ä¸ªæ¨¡å‹çš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯FLUX.1å’ŒIdeogram2.0åœ¨ç»“æ„å’Œç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­çš„å“è¶Šè¡¨ç°ã€‚æ­¤ç ”ç©¶æ·±å…¥æ´å¯Ÿäº†T2Iæ¨¡å‹å½“å‰çš„çŠ¶å†µå’Œæœªæ¥å‘å±•è¶‹åŠ¿ï¼Œéšç€å®ƒä»¬é€æ­¥é€šç”¨åŒ–è€Œä¸æ–­æ¼”å˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹åœ¨æ‰©æ•£æ¨¡å‹çš„æ¨åŠ¨ä¸‹å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œå±•ç¤ºå‡ºå¼ºå¤§çš„æç¤ºè·Ÿè¸ªå’Œå›¾åƒç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>æ–°å‹T2Iæ¨¡å‹å¦‚FLUX.1å’ŒIdeogram2.0åœ¨å¤šç§å¤æ‚ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>T2Iæ¨¡å‹çš„åº”ç”¨èŒƒå›´ä¸ä»…é™äºä¼ ç»Ÿçš„å›¾åƒç”Ÿæˆï¼Œè¿˜æ‰©å±•åˆ°å¯æ§ç”Ÿæˆã€å›¾åƒç¼–è¾‘ã€è§†é¢‘ã€éŸ³é¢‘ã€3Då’Œè¿åŠ¨ç”Ÿæˆç­‰å¤šä¸ªé¢†åŸŸã€‚</li>
<li>å½“å‰çš„è¯„ä»·æ¡†æ¶æ— æ³•å…¨é¢è¯„ä¼°T2Iæ¨¡å‹åœ¨ä¸æ–­æ‰©å¤§é¢†åŸŸä¸­çš„è¡¨ç°ã€‚</li>
<li>é€šè¿‡IMAGINE-Eè¯„ä¼°ï¼ŒFLUX.1å’ŒIdeogram2.0åœ¨ç»“æ„å’Œç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>T2Iæ¨¡å‹æ­£æœç€é€šç”¨é€‚ç”¨æ€§å‘å±•ï¼Œè¿™ä¸ºå…¶ä½œä¸ºåŸºç¡€AIå·¥å…·çš„åº”ç”¨æä¾›äº†å¹¿é˜”å‰æ™¯ã€‚</li>
<li>ç ”ç©¶æä¾›äº†å¯¹T2Iæ¨¡å‹å½“å‰çŠ¶æ€å’Œæœªæ¥å‘å±•è¶‹åŠ¿çš„æ·±åˆ»è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13920">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bef34c4c046be82b361d732dab413d57.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="One-Prompt-One-Story-Free-Lunch-Consistent-Text-to-Image-Generation-Using-a-Single-Prompt"><a href="#One-Prompt-One-Story-Free-Lunch-Consistent-Text-to-Image-Generation-Using-a-Single-Prompt" class="headerlink" title="One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation   Using a Single Prompt"></a>One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation   Using a Single Prompt</h2><p><strong>Authors:Tao Liu, Kai Wang, Senmao Li, Joost van de Weijer, Fahad Shahbaz Khan, Shiqi Yang, Yaxing Wang, Jian Yang, Ming-Ming Cheng</strong></p>
<p>Text-to-image generation models can create high-quality images from input prompts. However, they struggle to support the consistent generation of identity-preserving requirements for storytelling. Existing approaches to this problem typically require extensive training in large datasets or additional modifications to the original model architectures. This limits their applicability across different domains and diverse diffusion model configurations. In this paper, we first observe the inherent capability of language models, coined context consistency, to comprehend identity through context with a single prompt. Drawing inspiration from the inherent context consistency, we propose a novel training-free method for consistent text-to-image (T2I) generation, termed â€œOne-Prompt-One-Storyâ€ (1Prompt1Story). Our approach 1Prompt1Story concatenates all prompts into a single input for T2I diffusion models, initially preserving character identities. We then refine the generation process using two novel techniques: Singular-Value Reweighting and Identity-Preserving Cross-Attention, ensuring better alignment with the input description for each frame. In our experiments, we compare our method against various existing consistent T2I generation approaches to demonstrate its effectiveness through quantitative metrics and qualitative assessments. Code is available at <a target="_blank" rel="noopener" href="https://github.com/byliutao/1Prompt1Story">https://github.com/byliutao/1Prompt1Story</a>. </p>
<blockquote>
<p>æ–‡æœ¬è½¬å›¾åƒç”Ÿæˆæ¨¡å‹å¯ä»¥ä»è¾“å…¥æç¤ºä¸­åˆ›å»ºé«˜è´¨é‡å›¾åƒã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ”¯æŒæ•…äº‹å™è¿°ä¸­èº«ä»½ä¸€è‡´æ€§çš„æŒç»­ç”Ÿæˆæ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚è§£å†³è¿™ä¸ªé—®é¢˜çš„ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦åœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œå¹¿æ³›è®­ç»ƒæˆ–å¯¹åŸå§‹æ¨¡å‹æ¶æ„è¿›è¡Œé¢å¤–ä¿®æ”¹ã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨ä¸åŒé¢†åŸŸå’Œä¸åŒæ‰©æ•£æ¨¡å‹é…ç½®ä¸­çš„åº”ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè§‚å¯Ÿåˆ°è¯­è¨€æ¨¡å‹çš„å›ºæœ‰èƒ½åŠ›ï¼Œç§°ä¸ºä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼Œå³é€šè¿‡ä¸€ä¸ªå•ä¸€æç¤ºç†è§£èº«ä»½çš„èƒ½åŠ›ã€‚ä»å›ºæœ‰çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒå³å¯å®ç°ä¸€è‡´æ–‡æœ¬è½¬å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆçš„æ–°æ–¹æ³•ï¼Œç§°ä¸ºâ€œä¸€ä¸ªæç¤ºä¸€ä¸ªæ•…äº‹â€ï¼ˆ1Prompt1Storyï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯å°†æ‰€æœ‰æç¤ºä¸²è”èµ·æ¥ä½œä¸ºT2Iæ‰©æ•£æ¨¡å‹çš„å•ä¸ªè¾“å…¥ï¼Œæœ€åˆä¿ç•™è§’è‰²èº«ä»½ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ç§æ–°æŠ€æœ¯å¯¹ç”Ÿæˆè¿‡ç¨‹è¿›è¡Œæ”¹è¿›ï¼šå¥‡å¼‚å€¼é‡åŠ æƒå’Œèº«ä»½ä¿æŒäº¤å‰æ³¨æ„åŠ›ï¼Œç¡®ä¿ä¸æ¯ä¸ªå¸§çš„è¾“å…¥æè¿°æ›´å¥½åœ°å¯¹é½ã€‚æˆ‘ä»¬çš„å®éªŒé€šè¿‡å®šé‡æŒ‡æ ‡å’Œå®šæ€§è¯„ä¼°è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å°†å…¶ä¸å„ç§ç°æœ‰çš„T2Iä¸€è‡´ç”Ÿæˆæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚ä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/byliutao/1Prompt1Story%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/byliutao/1Prompt1Storyæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13554v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿä»è¾“å…¥æç¤ºåˆ›å»ºé«˜è´¨é‡å›¾åƒï¼Œä½†åœ¨æ”¯æŒèº«ä»½ä¿æŒçš„æ•…äº‹å™è¿°ä¸­è¡¨ç°ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†â€œOne-Prompt-One-Storyâ€ï¼ˆç®€ç§°â€œä¸€æç¤ºä¸€æ•…äº‹â€ï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„å†…åœ¨è¯­å¢ƒä¸€è‡´æ€§ç†è§£èº«ä»½ä¿¡æ¯ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯å®ç°ä¸€è‡´çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚é€šè¿‡ä¸²è”æ‰€æœ‰æç¤ºä½œä¸ºå•ä¸€è¾“å…¥ï¼Œåˆæ­¥ä¿ç•™è§’è‰²èº«ä»½ï¼Œå¹¶åº”ç”¨ä¸¤ç§æ–°æŠ€æœ¯è¿›è¡Œå¾®è°ƒï¼Œç¡®ä¿ä¸æ¯ä¸ªå¸§çš„è¾“å…¥æè¿°å¯¹é½ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹åœ¨åˆ›å»ºé«˜è´¨é‡å›¾åƒæ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤„ç†èº«ä»½ä¿æŒçš„æ•…äº‹å™è¿°æ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶è€…æå‡ºâ€œä¸€æç¤ºä¸€æ•…äº‹â€æ–¹æ³•æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨è¯­è¨€æ¨¡å‹çš„å†…åœ¨è¯­å¢ƒä¸€è‡´æ€§ç†è§£èº«ä»½ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡ä¸²è”æ‰€æœ‰æç¤ºä½œä¸ºå•ä¸€è¾“å…¥ï¼Œå®ç°ä¸€è‡´çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚</li>
<li>æ–¹æ³•åº”ç”¨ä¸¤ç§æ–°æŠ€æœ¯è¿›è¡Œå¾®è°ƒï¼Œç¡®ä¿ç”Ÿæˆçš„å›¾åƒä¸è¾“å…¥æè¿°å¯¹é½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1071c5c11407da8dd1603ffe28a0f6b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-637616b950f9f619f3ff0b6512568239.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca232700b4233cb86c6e83199c0db09f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7b0564060d474a014bb7b469dbbd9dd0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LDR-Net-A-Novel-Framework-for-AI-generated-Image-Detection-via-Localized-Discrepancy-Representation"><a href="#LDR-Net-A-Novel-Framework-for-AI-generated-Image-Detection-via-Localized-Discrepancy-Representation" class="headerlink" title="LDR-Net: A Novel Framework for AI-generated Image Detection via   Localized Discrepancy Representation"></a>LDR-Net: A Novel Framework for AI-generated Image Detection via   Localized Discrepancy Representation</h2><p><strong>Authors:JiaXin Chen, Miao Hu, DengYong Zhang, Yun Song, Xin Liao</strong></p>
<p>With the rapid advancement of generative models, the visual quality of generated images has become nearly indistinguishable from the real ones, posing challenges to content authenticity verification. Existing methods for detecting AI-generated images primarily focus on specific forgery clues, which are often tailored to particular generative models like GANs or diffusion models. These approaches struggle to generalize across architectures. Building on the observation that generative images often exhibit local anomalies, such as excessive smoothness, blurred textures, and unnatural pixel variations in small regions, we propose the localized discrepancy representation network (LDR-Net), a novel approach for detecting AI-generated images. LDR-Net captures smoothing artifacts and texture irregularities, which are common but often overlooked. It integrates two complementary modules: local gradient autocorrelation (LGA) which models local smoothing anomalies to detect smoothing anomalies, and local variation pattern (LVP) which captures unnatural regularities by modeling the complexity of image patterns. By merging LGA and LVP features, a comprehensive representation of localized discrepancies can be provided. Extensive experiments demonstrate that our LDR-Net achieves state-of-the-art performance in detecting generated images and exhibits satisfactory generalization across unseen generative models. The code will be released upon acceptance of this paper. </p>
<blockquote>
<p>éšç€ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼Œç”Ÿæˆå›¾åƒçš„å¯è§†è´¨é‡å·²ç»å˜å¾—å‡ ä¹ä¸çœŸå®å›¾åƒæ— æ³•åŒºåˆ†ï¼Œè¿™ç»™å†…å®¹çœŸå®æ€§éªŒè¯å¸¦æ¥äº†æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ£€æµ‹AIç”Ÿæˆå›¾åƒçš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨ç‰¹å®šçš„ä¼ªé€ çº¿ç´¢ä¸Šï¼Œè¿™äº›çº¿ç´¢é€šå¸¸é’ˆå¯¹ç‰¹å®šçš„ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚GANæˆ–æ‰©æ•£æ¨¡å‹ã€‚è¿™äº›æ–¹æ³•åœ¨è·¨æ¶æ„æ–¹é¢çš„æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚åŸºäºè§‚å¯Ÿåˆ°ç”Ÿæˆå›¾åƒé€šå¸¸ä¼šå‡ºç°å±€éƒ¨å¼‚å¸¸ï¼Œå¦‚è¿‡åº¦å¹³æ»‘ã€çº¹ç†æ¨¡ç³Šä»¥åŠå°åŒºåŸŸå†…çš„ä¸è‡ªç„¶åƒç´ å˜åŒ–ï¼Œæˆ‘ä»¬æå‡ºäº†å±€éƒ¨å·®å¼‚è¡¨ç¤ºç½‘ç»œï¼ˆLDR-Netï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹æ–¹æ³•ã€‚LDR-Netæ•æ‰åˆ°äº†å¹³æ»‘ä¼ªå½±å’Œçº¹ç†ä¸è§„åˆ™æ€§è¿™ä¸¤ç§å¸¸è§ä½†å¸¸è¢«å¿½è§†çš„ç‰¹å¾ã€‚å®ƒé›†æˆäº†ä¸¤ä¸ªäº’è¡¥çš„æ¨¡å—ï¼šå±€éƒ¨æ¢¯åº¦è‡ªç›¸å…³ï¼ˆLGAï¼‰æ¨¡å—ç”¨äºæ£€æµ‹å¹³æ»‘å¼‚å¸¸ï¼Œè€Œå±€éƒ¨å˜åŒ–æ¨¡å¼ï¼ˆLVPï¼‰æ¨¡å—åˆ™é€šè¿‡æ¨¡æ‹Ÿå›¾åƒæ¨¡å¼çš„å¤æ‚æ€§æ¥æ•æ‰ä¸è‡ªç„¶çš„è§„å¾‹æ€§ã€‚é€šè¿‡åˆå¹¶LGAå’ŒLVPç‰¹å¾ï¼Œå¯ä»¥æä¾›å¯¹å±€éƒ¨å·®å¼‚çš„å…¨é¢è¡¨ç¤ºã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„LDR-Netåœ¨æ£€æµ‹ç”Ÿæˆå›¾åƒæ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æœªè§è¿‡çš„ç”Ÿæˆæ¨¡å‹ä¹‹é—´è¡¨ç°å‡ºäº†ä»¤äººæ»¡æ„çš„æ³›åŒ–èƒ½åŠ›ã€‚è®ºæ–‡è¢«æ¥å—åå°†å…¬å¸ƒä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13475v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºLDR-Netçš„æ–°å‹ç½‘ç»œï¼Œç”¨äºæ£€æµ‹AIç”Ÿæˆçš„å›¾åƒã€‚è¯¥ç½‘ç»œèƒ½å¤Ÿæ•æ‰ç”Ÿæˆå›¾åƒä¸­å¸¸è§çš„å±€éƒ¨å¼‚å¸¸ï¼Œå¦‚è¿‡åº¦å¹³æ»‘ã€çº¹ç†æ¨¡ç³Šå’Œå±€éƒ¨åƒç´ å˜åŒ–ä¸è‡ªç„¶ç­‰ã€‚é€šè¿‡ç»“åˆå±€éƒ¨æ¢¯åº¦è‡ªç›¸å…³ï¼ˆLGAï¼‰å’Œå±€éƒ¨å˜åŒ–æ¨¡å¼ï¼ˆLVPï¼‰ä¸¤ä¸ªæ¨¡å—ï¼ŒLDR-Netæä¾›äº†å¯¹å±€éƒ¨å·®å¼‚çš„ç»¼åˆè¡¨ç¤ºï¼Œå®ç°äº†å¯¹ç”Ÿæˆå›¾åƒçš„å“è¶Šæ£€æµ‹æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨è·¨æœªè§ç”Ÿæˆæ¨¡å‹çš„æ£€æµ‹ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ä½¿å¾—ç”Ÿæˆå›¾åƒçš„çœŸå®æ€§éªŒè¯é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç‰¹å®šä¼ªé€ çº¿ç´¢ï¼Œä½†éš¾ä»¥æ³›åŒ–åˆ°ä¸åŒæ¶æ„çš„ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>LDR-Netç½‘ç»œèƒ½å¤Ÿæ•æ‰ç”Ÿæˆå›¾åƒä¸­çš„å±€éƒ¨å¼‚å¸¸ï¼Œå¦‚è¿‡åº¦å¹³æ»‘å’Œçº¹ç†æ¨¡ç³Šç­‰ã€‚</li>
<li>LDR-Netç»“åˆLGAå’ŒLVPä¸¤ä¸ªæ¨¡å—ï¼Œæä¾›å¯¹å±€éƒ¨å·®å¼‚çš„ç»¼åˆè¡¨ç¤ºã€‚</li>
<li>LGAæ¨¡å—é€šè¿‡å»ºæ¨¡å±€éƒ¨å¹³æ»‘å¼‚å¸¸æ¥æ£€æµ‹å¹³æ»‘å¼‚å¸¸ã€‚</li>
<li>LVPæ¨¡å—é€šè¿‡å»ºæ¨¡å›¾åƒæ¨¡å¼çš„å¤æ‚æ€§æ¥æ•æ‰ä¸è‡ªç„¶è§„å¾‹ã€‚</li>
<li>LDR-Netåœ¨æ£€æµ‹ç”Ÿæˆå›¾åƒæ–¹é¢è¾¾åˆ°æœ€æ–°æ€§èƒ½ï¼Œå¹¶å±•ç°å‡ºè‰¯å¥½çš„è·¨æœªè§ç”Ÿæˆæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13475">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4e71db5b9e955f23d1c20cb879ee8df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9741697f89077ded5003a04e0e2c9d24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06bf479390cd51f1313d49a3ee38151b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c79ea73019784ebb7b7e967fa7eb6ebf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-96488935f4a78b1e599f3381e3144812.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f426c40e268d5dafe9569ea8a7d4f371.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Retrievals-Can-Be-Detrimental-A-Contrastive-Backdoor-Attack-Paradigm-on-Retrieval-Augmented-Diffusion-Models"><a href="#Retrievals-Can-Be-Detrimental-A-Contrastive-Backdoor-Attack-Paradigm-on-Retrieval-Augmented-Diffusion-Models" class="headerlink" title="Retrievals Can Be Detrimental: A Contrastive Backdoor Attack Paradigm on   Retrieval-Augmented Diffusion Models"></a>Retrievals Can Be Detrimental: A Contrastive Backdoor Attack Paradigm on   Retrieval-Augmented Diffusion Models</h2><p><strong>Authors:Hao Fang, Xiaohang Sui, Hongyao Yu, Jiawei Kong, Sijin Yu, Bin Chen, Hao Wu, Shu-Tao Xia</strong></p>
<p>Diffusion models (DMs) have recently demonstrated remarkable generation capability. However, their training generally requires huge computational resources and large-scale datasets. To solve these, recent studies empower DMs with the advanced Retrieval-Augmented Generation (RAG) technique and propose retrieval-augmented diffusion models (RDMs). By incorporating rich knowledge from an auxiliary database, RAG enhances diffusion modelsâ€™ generation and generalization ability while significantly reducing model parameters. Despite the great success, RAG may introduce novel security issues that warrant further investigation. In this paper, we reveal that the RDM is susceptible to backdoor attacks by proposing a multimodal contrastive attack approach named BadRDM. Our framework fully considers RAGâ€™s characteristics and is devised to manipulate the retrieved items for given text triggers, thereby further controlling the generated contents. Specifically, we first insert a tiny portion of images into the retrieval database as target toxicity surrogates. Subsequently, a malicious variant of contrastive learning is adopted to inject backdoors into the retriever, which builds shortcuts from triggers to the toxicity surrogates. Furthermore, we enhance the attacks through novel entropy-based selection and generative augmentation strategies that can derive better toxicity surrogates. Extensive experiments on two mainstream tasks demonstrate the proposed BadRDM achieves outstanding attack effects while preserving the modelâ€™s benign utility. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰æœ€è¿‘å±•ç°å‡ºæ˜¾è‘—çš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„è®­ç»ƒé€šå¸¸éœ€è¦å·¨å¤§çš„è®¡ç®—èµ„æºå’Œå¤§è§„æ¨¡æ•°æ®é›†ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ€è¿‘çš„ç ”ç©¶ä¸ºDMsèµ‹äºˆäº†å…ˆè¿›çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œå¹¶æå‡ºäº†æ£€ç´¢å¢å¼ºæ‰©æ•£æ¨¡å‹ï¼ˆRDMsï¼‰ã€‚é€šè¿‡èå…¥è¾…åŠ©æ•°æ®åº“ä¸­çš„ä¸°å¯ŒçŸ¥è¯†ï¼ŒRAGæå‡äº†æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå’Œæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†æ¨¡å‹å‚æ•°ã€‚å°½ç®¡å–å¾—äº†å·¨å¤§æˆåŠŸï¼ŒRAGå¯èƒ½ä¼šå¼•å…¥æ–°çš„å®‰å…¨é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥çš„è°ƒæŸ¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºRDMæ˜“å—åé—¨æ”»å‡»ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºBadRDMçš„å¤šæ¨¡æ€å¯¹æ¯”æ”»å‡»æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¡†æ¶å……åˆ†è€ƒè™‘äº†RAGçš„ç‰¹æ€§ï¼Œæ—¨åœ¨æ“çºµç»™å®šæ–‡æœ¬è§¦å‘å™¨çš„æ£€ç´¢é¡¹ç›®ï¼Œä»è€Œè¿›ä¸€æ­¥æ§åˆ¶ç”Ÿæˆçš„å†…å®¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨æ£€ç´¢æ•°æ®åº“ä¸­æ’å…¥ä¸€å°éƒ¨åˆ†å›¾åƒä½œä¸ºç›®æ ‡æ¯’æ€§æ›¿ä»£ç‰©ã€‚éšåï¼Œé‡‡ç”¨ä¸€ç§æ¶æ„çš„å¯¹æ¯”å­¦ä¹ å˜ä½“å°†åé—¨æ³¨å…¥æ£€ç´¢å™¨ï¼Œä»è€Œåœ¨è§¦å‘å™¨å’Œæ¯’æ€§æ›¿ä»£ç‰©ä¹‹é—´å»ºç«‹å¿«æ·æ–¹å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æ–°çš„åŸºäºç†µçš„é€‰æ‹©å’Œç”Ÿæˆå¢å¼ºç­–ç•¥ï¼Œä½¿æ”»å‡»æ›´åŠ æœ‰æ•ˆï¼Œèƒ½å¤Ÿäº§ç”Ÿæ›´å¥½çš„æ¯’æ€§æ›¿ä»£ç‰©ã€‚åœ¨ä¸¤ä¸ªä¸»æµä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„BadRDMåœ¨å®ç°å‡ºè‰²æ”»å‡»æ•ˆæœçš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹çš„è‰¯æ€§æ•ˆç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13340v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å…·æœ‰å‡ºè‰²çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†è®­ç»ƒéœ€è¦å¤§é‡è®¡ç®—èµ„æºå’Œå¤§è§„æ¨¡æ•°æ®é›†ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶äººå‘˜é‡‡ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯èµ‹èƒ½æ‰©æ•£æ¨¡å‹ï¼Œæå‡ºæ£€ç´¢å¢å¼ºæ‰©æ•£æ¨¡å‹ï¼ˆRDMsï¼‰ã€‚RAGé€šè¿‡èå…¥è¾…åŠ©æ•°æ®åº“ä¸­çš„ä¸°å¯ŒçŸ¥è¯†ï¼Œæå‡äº†æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆå’Œæ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶å‡å°‘äº†æ¨¡å‹å‚æ•°ã€‚ç„¶è€Œï¼ŒRAGå¯èƒ½å¼•å…¥æ–°çš„å®‰å…¨é—®é¢˜ã€‚æœ¬æ–‡æ­ç¤ºRDMæ˜“å—åé—¨æ”»å‡»ï¼Œå¹¶æå‡ºä¸€ç§åä¸ºBadRDMçš„å¤šæ¨¡å¼å¯¹æ¯”æ”»å‡»æ–¹æ³•ã€‚è¯¥æ–¹æ³•å……åˆ†è€ƒè™‘äº†RAGçš„ç‰¹æ€§ï¼Œé€šè¿‡æ“çºµæ£€ç´¢åˆ°çš„é¡¹ç›®æ¥å®ç°å¯¹ç”Ÿæˆå†…å®¹çš„æ§åˆ¶ã€‚å®éªŒè¯æ˜ï¼ŒBadRDMåœ¨å®ç°å‡ºè‰²æ”»å‡»æ•ˆæœçš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹çš„è‰¯æ€§æ•ˆç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMsï¼‰å…·å¤‡å‡ºè‰²çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œéœ€è¦å¤§è§„æ¨¡æ•°æ®å’Œè®¡ç®—èµ„æºã€‚</li>
<li>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ç”¨äºæå‡æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶å‡å°‘æ¨¡å‹å‚æ•°ã€‚</li>
<li>RAGæŠ€æœ¯çš„å¼•å…¥å¯èƒ½å¸¦æ¥æ–°çš„å®‰å…¨é—®é¢˜ï¼Œå°¤å…¶æ˜¯åé—¨æ”»å‡»ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºBadRDMçš„å¤šæ¨¡å¼å¯¹æ¯”æ”»å‡»æ–¹æ³•ï¼Œé’ˆå¯¹RDMè¿›è¡Œæ”»å‡»ã€‚</li>
<li>BadRDMé€šè¿‡æ“çºµæ£€ç´¢é¡¹ç›®æ¥æ§åˆ¶ç”Ÿæˆå†…å®¹ã€‚</li>
<li>åœ¨å®éªŒä¸­ï¼ŒBadRDMåœ¨å®ç°å¯¹ä¸»æµä»»åŠ¡çš„å‡ºè‰²æ”»å‡»æ•ˆæœçš„åŒæ—¶ï¼Œä¿æŒäº†æ¨¡å‹çš„è‰¯æ€§æ•ˆç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13340">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2444a6b9f8137896fb170619c0f2c228.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-303ab2c5e77ace764a0e4694d7cc7ac1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-281c6b40d5256b7db1f9d575ca0ddb7f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Graph-Representation-Learning-with-Diffusion-Generative-Models"><a href="#Graph-Representation-Learning-with-Diffusion-Generative-Models" class="headerlink" title="Graph Representation Learning with Diffusion Generative Models"></a>Graph Representation Learning with Diffusion Generative Models</h2><p><strong>Authors:Daniel Wesego</strong></p>
<p>Diffusion models have established themselves as state-of-the-art generative models across various data modalities, including images and videos, due to their ability to accurately approximate complex data distributions. Unlike traditional generative approaches such as VAEs and GANs, diffusion models employ a progressive denoising process that transforms noise into meaningful data over multiple iterative steps. This gradual approach enhances their expressiveness and generation quality. Not only that, diffusion models have also been shown to extract meaningful representations from data while learning to generate samples. Despite their success, the application of diffusion models to graph-structured data remains relatively unexplored, primarily due to the discrete nature of graphs, which necessitates discrete diffusion processes distinct from the continuous methods used in other domains. In this work, we leverage the representational capabilities of diffusion models to learn meaningful embeddings for graph data. By training a discrete diffusion model within an autoencoder framework, we enable both effective autoencoding and representation learning tailored to the unique characteristics of graph-structured data. We only need the encoder at the end to extract representations. Our approach demonstrates the potential of discrete diffusion models to be used for graph representation learning. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å› å…¶å‡†ç¡®é€¼è¿‘å¤æ‚æ•°æ®åˆ†å¸ƒçš„èƒ½åŠ›ï¼Œå·²ç¡®ç«‹è‡ªå·±åœ¨å„ç§æ•°æ®æ¨¡å¼ï¼ˆåŒ…æ‹¬å›¾åƒå’Œè§†é¢‘ï¼‰ä¸­çš„æœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹åœ°ä½ã€‚ä¸ä¼ ç»Ÿç”Ÿæˆæ–¹æ³•ï¼ˆå¦‚å˜åˆ†è‡ªç¼–ç å™¨å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼‰ä¸åŒï¼Œæ‰©æ•£æ¨¡å‹é‡‡ç”¨æ¸è¿›çš„å»å™ªè¿‡ç¨‹ï¼Œé€šè¿‡å¤šä¸ªè¿­ä»£æ­¥éª¤å°†å™ªå£°è½¬åŒ–ä¸ºæœ‰æ„ä¹‰çš„æ•°æ®ã€‚è¿™ç§æ¸è¿›çš„æ–¹æ³•å¢å¼ºäº†å…¶è¡¨ç°åŠ›å’Œç”Ÿæˆè´¨é‡ã€‚ä¸ä»…å¦‚æ­¤ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­èƒ½ä»æ•°æ®ä¸­æå–æœ‰æ„ä¹‰çš„è¡¨ç¤ºã€‚å°½ç®¡å–å¾—äº†æˆåŠŸï¼Œæ‰©æ•£æ¨¡å‹åœ¨å›¾å½¢ç»“æ„æ•°æ®ä¸­çš„åº”ç”¨ä»ç„¶ç›¸å¯¹æœªè¢«æ¢ç´¢ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå›¾å½¢çš„ç¦»æ•£æ€§è´¨éœ€è¦ç‹¬ç‰¹çš„ç¦»æ•£æ‰©æ•£è¿‡ç¨‹ï¼Œè¿™ä¸å…¶ä»–é¢†åŸŸä½¿ç”¨çš„è¿ç»­æ–¹æ³•ä¸åŒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›æ¥å­¦ä¹ å›¾å½¢æ•°æ®çš„æœ‰æ„ä¹‰åµŒå…¥ã€‚é€šè¿‡åœ¨è‡ªç¼–ç æ¡†æ¶å†…è®­ç»ƒç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬å®ç°äº†é’ˆå¯¹å›¾å½¢ç»“æ„æ•°æ®çš„æœ‰æ•ˆè‡ªç¼–ç å’Œè¡¨ç¤ºå­¦ä¹ ã€‚æˆ‘ä»¬æœ€ç»ˆåªéœ€è¦ç¼–ç å™¨æ¥æå–è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•å±•ç¤ºäº†ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨å›¾å½¢è¡¨ç¤ºå­¦ä¹ ä¸­çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13133v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹é€šè¿‡é€æ­¥å»å™ªè¿‡ç¨‹ï¼Œå°†å™ªå£°è½¬åŒ–ä¸ºæœ‰æ„ä¹‰çš„æ•°æ®ï¼Œæé«˜äº†è¡¨è¾¾æ€§å’Œç”Ÿæˆè´¨é‡ï¼Œå·²æˆä¸ºå›¾åƒå’Œè§†é¢‘ç­‰å¤šç§æ•°æ®æ¨¡æ€çš„å…ˆè¿›æŠ€æœ¯ç”Ÿæˆæ¨¡å‹ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨å…¶ä»–é¢†åŸŸå–å¾—äº†æˆåŠŸï¼Œä½†åœ¨å›¾å½¢ç»“æ„åŒ–æ•°æ®çš„åº”ç”¨ä»ç„¶ç›¸å¯¹æœªè¢«æ¢ç´¢ï¼Œä¸»è¦ç”±äºå›¾å½¢çš„ç¦»æ•£æ€§è´¨éœ€è¦ç‹¬ç‰¹çš„ç¦»æ•£æ‰©æ•£è¿‡ç¨‹ã€‚æœ¬æ–‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›å­¦ä¹ å›¾å½¢æ•°æ®çš„æœ‰æ„ä¹‰åµŒå…¥ï¼Œé€šè¿‡è®­ç»ƒç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨è‡ªç¼–ç å™¨æ¡†æ¶å†…ï¼Œå®ç°é’ˆå¯¹å›¾å½¢ç»“æ„åŒ–æ•°æ®çš„æœ‰æ•ˆè‡ªç¼–ç å’Œè¡¨ç¤ºå­¦ä¹ ã€‚æœ€ç»ˆåªéœ€ä½¿ç”¨ç¼–ç å™¨æå–è¡¨ç¤ºã€‚æœ¬ç ”ç©¶å±•ç¤ºäº†ç¦»æ•£æ‰©æ•£æ¨¡å‹åœ¨å›¾å½¢è¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºå›¾åƒå’Œè§†é¢‘ç­‰æ•°æ®æ¨¡æ€çš„å…ˆè¿›æŠ€æœ¯ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹é‡‡ç”¨é€æ­¥å»å™ªè¿‡ç¨‹ï¼Œå°†å™ªå£°è½¬åŒ–ä¸ºæœ‰æ„ä¹‰çš„æ•°æ®ï¼Œæé«˜äº†è¡¨è¾¾æ€§å’Œç”Ÿæˆè´¨é‡ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨å›¾å½¢æ•°æ®ä¸Šçš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œä¸»è¦å› ä¸ºå›¾å½¢çš„ç¦»æ•£æ€§è´¨ã€‚</li>
<li>æœ¬æ–‡åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›å­¦ä¹ å›¾å½¢æ•°æ®çš„æœ‰æ„ä¹‰åµŒå…¥ã€‚</li>
<li>é€šè¿‡åœ¨è‡ªç¼–ç å™¨æ¡†æ¶å†…è®­ç»ƒç¦»æ•£æ‰©æ•£æ¨¡å‹ï¼Œå®ç°æœ‰æ•ˆè‡ªç¼–ç å’Œé’ˆå¯¹å›¾å½¢æ•°æ®çš„è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>è¯¥æ–¹æ³•ä»…éœ€è¦ä½¿ç”¨ç¼–ç å™¨æ¥æå–è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13133">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dad36fd9fe340297e9b45dd413020e17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7af83fcf0c967397217b636132beea1e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Accelerate-High-Quality-Diffusion-Models-with-Inner-Loop-Feedback"><a href="#Accelerate-High-Quality-Diffusion-Models-with-Inner-Loop-Feedback" class="headerlink" title="Accelerate High-Quality Diffusion Models with Inner Loop Feedback"></a>Accelerate High-Quality Diffusion Models with Inner Loop Feedback</h2><p><strong>Authors:Matthew Gwilliam, Han Cai, Di Wu, Abhinav Shrivastava, Zhiyu Cheng</strong></p>
<p>We propose Inner Loop Feedback (ILF), a novel approach to accelerate diffusion modelsâ€™ inference. ILF trains a lightweight module to predict future features in the denoising process by leveraging the outputs from a chosen diffusion backbone block at a given time step. This approach exploits two key intuitions; (1) the outputs of a given block at adjacent time steps are similar, and (2) performing partial computations for a step imposes a lower burden on the model than skipping the step entirely. Our method is highly flexible, since we find that the feedback module itself can simply be a block from the diffusion backbone, with all settings copied. Its influence on the diffusion forward can be tempered with a learnable scaling factor from zero initialization. We train this module using distillation losses; however, unlike some prior work where a full diffusion backbone serves as the student, our model freezes the backbone, training only the feedback module. While many efforts to optimize diffusion models focus on achieving acceptable image quality in extremely few steps (1-4 steps), our emphasis is on matching best case results (typically achieved in 20 steps) while significantly reducing runtime. ILF achieves this balance effectively, demonstrating strong performance for both class-to-image generation with diffusion transformer (DiT) and text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The quality of ILFâ€™s 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP Image Quality Assessment, ImageReward, and qualitative comparisons. Project information is available at <a target="_blank" rel="noopener" href="https://mgwillia.github.io/ilf">https://mgwillia.github.io/ilf</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºInner Loop Feedbackï¼ˆILFï¼‰çš„æ–°å‹æ–¹æ³•ï¼Œä»¥åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚ILFè®­ç»ƒäº†ä¸€ä¸ªè½»é‡çº§æ¨¡å—ï¼Œé€šè¿‡åˆ©ç”¨åœ¨ç»™å®šæ—¶é—´æ­¥é•¿é€‰æ‹©çš„æ‰©æ•£éª¨å¹²ç½‘å—çš„è¾“å‡ºæ¥é¢„æµ‹å»å™ªè¿‡ç¨‹ä¸­çš„æœªæ¥ç‰¹å¾ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨äº†ä¸¤ä¸ªå…³é”®ç›´è§‰ï¼šï¼ˆ1ï¼‰ç»™å®šå—åœ¨ç›¸é‚»æ—¶é—´æ­¥çš„è¾“å‡ºç‰ˆæœ¬ç›¸ä¼¼ï¼›ï¼ˆ2ï¼‰æ‰§è¡Œä¸€æ­¥çš„éƒ¨åˆ†è®¡ç®—æ¯”å®Œå…¨è·³è¿‡æ­¥éª¤ç»™æ¨¡å‹å¸¦æ¥çš„è´Ÿæ‹…æ›´ä½ã€‚æˆ‘ä»¬çš„æ–¹æ³•éå¸¸çµæ´»ï¼Œå› ä¸ºæˆ‘ä»¬å‘ç°åé¦ˆæ¨¡å—æœ¬èº«å¯ä»¥ä»…ä»…æ˜¯æ‰©æ•£éª¨å¹²ç½‘çš„ä¸€ä¸ªå—ï¼Œæ‰€æœ‰è®¾ç½®éƒ½è¢«å¤åˆ¶ã€‚å®ƒå¯¹æ‰©æ•£æ­£å‘çš„å½±å“å¯ä»¥é€šè¿‡ä»é›¶åˆå§‹åŒ–ä¸­å­¦ä¹ åˆ°çš„ç¼©æ”¾å› å­æ¥è°ƒèŠ‚ã€‚æˆ‘ä»¬ä½¿ç”¨è’¸é¦æŸå¤±æ¥è®­ç»ƒè¿™ä¸ªæ¨¡å—ï¼›ç„¶è€Œï¼Œä¸ä¸€äº›å…ˆå‰çš„å·¥ä½œä½¿ç”¨å®Œæ•´çš„æ‰©æ•£éª¨å¹²ç½‘ä½œä¸ºå­¦ç”Ÿä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼šå†»ç»“éª¨å¹²ç½‘ï¼Œåªè®­ç»ƒåé¦ˆæ¨¡å—ã€‚è®¸å¤šä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„åŠªåŠ›éƒ½é›†ä¸­åœ¨ç”¨æå°‘çš„æ­¥éª¤ï¼ˆ1-4æ­¥ï¼‰è¾¾åˆ°å¯æ¥å—çš„å›¾åƒè´¨é‡ä¸Šï¼Œè€Œæˆ‘ä»¬å…³æ³¨çš„é‡ç‚¹æ˜¯åœ¨è¾¾åˆ°æœ€ä½³ç»“æœï¼ˆé€šå¸¸åœ¨ç¬¬20æ­¥è¾¾åˆ°ï¼‰çš„åŒæ—¶æ˜¾è‘—å‡å°‘è¿è¡Œæ—¶é—´ã€‚ILFæœ‰æ•ˆåœ°å®ç°äº†è¿™ç§å¹³è¡¡ï¼Œåœ¨åˆ©ç”¨æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰è¿›è¡Œä»ç±»åˆ«åˆ°å›¾åƒç”Ÿæˆå’ŒåŸºäºDiTçš„PixArt-alphaåŠPixArt-sigmaè¿›è¡Œä»æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä»»åŠ¡æ—¶ï¼Œå‡è¡¨ç°å‡ºå¼ºåŠ²çš„æ€§èƒ½ã€‚ILFçš„1.7x-1.8xçš„åŠ é€Ÿè´¨é‡é€šè¿‡FIDã€CLIPåˆ†æ•°ã€CLIPå›¾åƒè´¨é‡è¯„ä¼°ã€ImageRewardå’Œå®šæ€§æ¯”è¾ƒå¾—åˆ°äº†è¯å®ã€‚é¡¹ç›®ä¿¡æ¯å¯åœ¨<a target="_blank" rel="noopener" href="https://mgwillia.github.io/ilf%E8%8E%B7%E5%8F%96%E3%80%82">https://mgwillia.github.io/ilfè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13107v2">PDF</a> submission currently under review; 20 pages, 17 figures, 6 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†å†…éƒ¨å¾ªç¯åé¦ˆï¼ˆILFï¼‰è¿™ä¸€æ–°æ–¹æ³•ï¼Œæ—¨åœ¨åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ¨æ–­ã€‚ILFè®­ç»ƒäº†ä¸€ä¸ªè½»é‡çº§æ¨¡å—ï¼Œé€šè¿‡åˆ©ç”¨ç»™å®šæ—¶é—´æ­¥é•¿ä¸‹çš„æ‰©æ•£ä¸»å¹²å—çš„è¾“å‡ºæ¥é¢„æµ‹å»å™ªè¿‡ç¨‹ä¸­çš„æœªæ¥ç‰¹å¾ã€‚æ­¤æ–¹æ³•åŸºäºä¸¤ä¸ªå…³é”®ç›´è§‰ï¼šï¼ˆ1ï¼‰ç›¸é‚»æ—¶é—´æ­¥é•¿ä¸‹ç»™å®šå—çš„è¾“å‡ºæ˜¯ç›¸ä¼¼çš„ï¼›ï¼ˆ2ï¼‰å¯¹ä¸€æ­¥è¿›è¡Œéƒ¨åˆ†è®¡ç®—æ¯”å®Œå…¨è·³è¿‡è¯¥æ­¥å¯¹æ¨¡å‹çš„è´Ÿæ‹…æ›´å°ã€‚ILFæ–¹æ³•çµæ´»åº¦é«˜ï¼Œå› ä¸ºåé¦ˆæ¨¡å—æœ¬èº«å¯ä»¥æ˜¯æ‰©æ•£ä¸»å¹²çš„ä»»æ„å—ï¼Œå¹¶ä¸”æ‰€æœ‰è®¾ç½®éƒ½è¢«å¤åˆ¶ã€‚å®ƒå¯¹æ‰©æ•£æ­£å‘çš„å½±å“å¯ä»¥é€šè¿‡ä»é›¶åˆå§‹åŒ–çš„å¯å­¦ä¹ ç¼©æ”¾å› å­æ¥è°ƒèŠ‚ã€‚æˆ‘ä»¬ä½¿ç”¨è’¸é¦æŸå¤±æ¥è®­ç»ƒæ­¤æ¨¡å—ï¼›ç„¶è€Œï¼Œä¸æŸäº›å…ˆå‰çš„å·¥ä½œä¸åŒï¼Œå…¶ä¸­æ•´ä¸ªæ‰©æ•£ä¸»å¹²ä½œä¸ºå­¦ç”Ÿï¼Œæˆ‘ä»¬çš„æ¨¡å‹å†»ç»“äº†ä¸»å¹²ï¼Œåªè®­ç»ƒåé¦ˆæ¨¡å—ã€‚è®¸å¤šä¼˜åŒ–æ‰©æ•£æ¨¡å‹çš„åŠªåŠ›éƒ½é›†ä¸­åœ¨åœ¨æå°‘çš„æ­¥éª¤ï¼ˆ1-4æ­¥ï¼‰å†…è¾¾åˆ°å¯æ¥å—çš„å›¾åƒè´¨é‡ï¼Œè€Œæˆ‘ä»¬çš„é‡ç‚¹æ˜¯åœ¨åŒ¹é…æœ€ä½³ç»“æœï¼ˆé€šå¸¸åœ¨20æ­¥å†…è¾¾åˆ°ï¼‰çš„åŒæ—¶æ˜¾è‘—å‡å°‘è¿è¡Œæ—¶é—´ã€‚ILFæœ‰æ•ˆåœ°å®ç°äº†è¿™ç§å¹³è¡¡ï¼Œåœ¨åŸºäºæ‰©æ•£å˜å‹å™¨çš„ç±»åˆ°å›¾åƒç”Ÿæˆå’ŒåŸºäºDiTçš„PixArt-alphaå’ŒPixArt-sigmaçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä¸­å‡è¡¨ç°å‡ºå¼ºåŠ²æ€§èƒ½ã€‚ILFçš„é€Ÿåº¦æé«˜äº†1.7x-1.8xï¼Œå…¶è´¨é‡å¾—åˆ°äº†FIDã€CLIPåˆ†æ•°ã€CLIPå›¾åƒè´¨é‡è¯„ä¼°ã€ImageRewardå’Œå®šæ€§æ¯”è¾ƒçš„ç¡®è¯ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ILFï¼Œæ—¨åœ¨åŠ é€Ÿæ‰©æ•£æ¨¡å‹çš„æ¨æ–­è¿‡ç¨‹ã€‚</li>
<li>ILFåˆ©ç”¨ç»™å®šæ‰©æ•£æ¨¡å‹çš„ä¸­é—´è¾“å‡ºæ¥é¢„æµ‹æœªæ¥çš„ç‰¹å¾ï¼Œä»è€Œå‡å°‘è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>ILFæ–¹æ³•å…·æœ‰çµæ´»æ€§ï¼Œåé¦ˆæ¨¡å—å¯ä»¥æ˜¯æ‰©æ•£æ¨¡å‹ä¸­çš„ä»»æ„å—ã€‚</li>
<li>é€šè¿‡è’¸é¦æŸå¤±è®­ç»ƒåé¦ˆæ¨¡å—ï¼ŒåŒæ—¶å†»ç»“æ‰©æ•£æ¨¡å‹çš„ä¸»å¹²ã€‚</li>
<li>ILFåœ¨åŒ¹é…æœ€ä½³å›¾åƒè´¨é‡ç»“æœçš„åŒæ—¶ï¼Œå®ç°äº†æ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚</li>
<li>ILFåœ¨ç±»åˆ°å›¾åƒç”Ÿæˆå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>é€šè¿‡å¤šç§è¯„ä¼°æŒ‡æ ‡ï¼ˆåŒ…æ‹¬FIDã€CLIPåˆ†æ•°ç­‰ï¼‰éªŒè¯äº†ILFçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e9998b66b0966abbb2b83cd38f0812cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2cf73f6c7cfea34c5dab9f98cccf6a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dbe75b94a73086ada149a1a8a5288c17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-998beec742d33cdefd3c53d4e64c1c45.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6109b665a0947d1484534ab6f54fc6d3.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Boosting-Diffusion-Guidance-via-Learning-Degradation-Aware-Models-for-Blind-Super-Resolution"><a href="#Boosting-Diffusion-Guidance-via-Learning-Degradation-Aware-Models-for-Blind-Super-Resolution" class="headerlink" title="Boosting Diffusion Guidance via Learning Degradation-Aware Models for   Blind Super Resolution"></a>Boosting Diffusion Guidance via Learning Degradation-Aware Models for   Blind Super Resolution</h2><p><strong>Authors:Shao-Hao Lu, Ren Wang, Ching-Chun Huang, Wei-Chen Chiu</strong></p>
<p>Recently, diffusion-based blind super-resolution (SR) methods have shown great ability to generate high-resolution images with abundant high-frequency detail, but the detail is often achieved at the expense of fidelity. Meanwhile, another line of research focusing on rectifying the reverse process of diffusion models (i.e., diffusion guidance), has demonstrated the power to generate high-fidelity results for non-blind SR. However, these methods rely on known degradation kernels, making them difficult to apply to blind SR. To address these issues, we present DADiff in this paper. DADiff incorporates degradation-aware models into the diffusion guidance framework, eliminating the need to know degradation kernels. Additionally, we propose two novel techniques: input perturbation and guidance scalar, to further improve our performance. Extensive experimental results show that our proposed method has superior performance over state-of-the-art methods on blind SR benchmarks. </p>
<blockquote>
<p>æœ€è¿‘ï¼ŒåŸºäºæ‰©æ•£çš„ç›²è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰æ–¹æ³•æ˜¾ç¤ºå‡ºç”Ÿæˆå…·æœ‰ä¸°å¯Œé«˜é¢‘ç»†èŠ‚çš„é«˜åˆ†è¾¨ç‡å›¾åƒçš„å¼ºå¤§èƒ½åŠ›ï¼Œä½†ç»†èŠ‚çš„å®ç°å¾€å¾€ä»¥ä¿çœŸåº¦çš„æŸå¤±ä¸ºä»£ä»·ã€‚ä¸æ­¤åŒæ—¶ï¼Œå¦ä¸€æ¡ç ”ç©¶çº¿ç´¢é›†ä¸­åœ¨çº æ­£æ‰©æ•£æ¨¡å‹çš„åå‘è¿‡ç¨‹ï¼ˆå³æ‰©æ•£å¼•å¯¼ï¼‰ï¼Œå¹¶å·²è¯æ˜å…¶åœ¨éç›²SRä¸­äº§ç”Ÿé«˜ä¿çœŸç»“æœçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºå·²çŸ¥çš„é€€åŒ–æ ¸ï¼Œä½¿å¾—å®ƒä»¬éš¾ä»¥åº”ç”¨äºç›²SRã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†DADiffã€‚DADiffå°†é€€åŒ–æ„ŸçŸ¥æ¨¡å‹èå…¥æ‰©æ•£å¼•å¯¼æ¡†æ¶ï¼Œæ— éœ€äº†è§£é€€åŒ–æ ¸ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸¤ç§æ–°æŠ€æœ¯ï¼šè¾“å…¥æ‰°åŠ¨å’Œå¼•å¯¼æ ‡é‡ï¼Œä»¥è¿›ä¸€æ­¥æé«˜æˆ‘ä»¬çš„æ€§èƒ½ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨ç›²SRåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¼˜äºæœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.08819v3">PDF</a> To appear in WACV 2025. Code is available at:   <a target="_blank" rel="noopener" href="https://github.com/ryanlu2240/DADiff">https://github.com/ryanlu2240/DADiff</a></p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDADiffçš„æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆäº†é€€åŒ–æ„ŸçŸ¥æ¨¡å‹ä¸æ‰©æ•£å¼•å¯¼æ¡†æ¶ï¼Œæ— éœ€çŸ¥é“é€€åŒ–å†…æ ¸å³å¯å®ç°ç›²è¶…åˆ†è¾¨ç‡é‡å»ºã€‚é€šè¿‡è¾“å…¥æ‰°åŠ¨å’Œå¼•å¯¼æ ‡é‡ä¸¤ç§æ–°æŠ€æœ¯è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œåœ¨ç›²è¶…åˆ†è¾¨ç‡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DADiffç»“åˆäº†é€€åŒ–æ„ŸçŸ¥æ¨¡å‹ä¸æ‰©æ•£å¼•å¯¼æ¡†æ¶ï¼Œè§£å†³äº†ä»¥å¾€æ–¹æ³•éœ€è¦åœ¨è¶…åˆ†è¾¨ç‡è¿‡ç¨‹ä¸­äº†è§£é€€åŒ–å†…æ ¸çš„å±€é™æ€§ã€‚</li>
<li>é€šè¿‡è¾“å…¥æ‰°åŠ¨æŠ€æœ¯è¿›ä¸€æ­¥ä¼˜åŒ–äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æŠ€æœ¯â€”â€”å¼•å¯¼æ ‡é‡ï¼Œè¿›ä¸€æ­¥æå‡äº†æ¨¡å‹åœ¨ç›²è¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>DADiffåœ¨ç›²è¶…åˆ†è¾¨ç‡åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå…·æœ‰ä¸°å¯Œé«˜é¢‘ç»†èŠ‚çš„é«˜åˆ†è¾¨ç‡å›¾åƒã€‚</li>
<li>è¯¥æ–¹æ³•å¹³è¡¡äº†å›¾åƒç»†èŠ‚ä¸ä¿çœŸåº¦ä¹‹é—´çš„å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.08819">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c777205d41ebaddd1102cdfd7df7209.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5be19807ba797a94e8af474d841ff0ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c0cb7cf103785dca203afc355551137.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bfcb5c0167308092bf8db4f5d2801556.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92344895f872238ff4b3e707b02bec12.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RORem-Training-a-Robust-Object-Remover-with-Human-in-the-Loop"><a href="#RORem-Training-a-Robust-Object-Remover-with-Human-in-the-Loop" class="headerlink" title="RORem: Training a Robust Object Remover with Human-in-the-Loop"></a>RORem: Training a Robust Object Remover with Human-in-the-Loop</h2><p><strong>Authors:Ruibin Li, Tao Yang, Song Guo, Lei Zhang</strong></p>
<p>Despite the significant advancements, existing object removal methods struggle with incomplete removal, incorrect content synthesis and blurry synthesized regions, resulting in low success rates. Such issues are mainly caused by the lack of high-quality paired training data, as well as the self-supervised training paradigm adopted in these methods, which forces the model to in-paint the masked regions, leading to ambiguity between synthesizing the masked objects and restoring the background. To address these issues, we propose a semi-supervised learning strategy with human-in-the-loop to create high-quality paired training data, aiming to train a Robust Object Remover (RORem). We first collect 60K training pairs from open-source datasets to train an initial object removal model for generating removal samples, and then utilize human feedback to select a set of high-quality object removal pairs, with which we train a discriminator to automate the following training data generation process. By iterating this process for several rounds, we finally obtain a substantial object removal dataset with over 200K pairs. Fine-tuning the pre-trained stable diffusion model with this dataset, we obtain our RORem, which demonstrates state-of-the-art object removal performance in terms of both reliability and image quality. Particularly, RORem improves the object removal success rate over previous methods by more than 18%. The dataset, source code and trained model are available at <a target="_blank" rel="noopener" href="https://github.com/leeruibin/RORem">https://github.com/leeruibin/RORem</a>. </p>
<blockquote>
<p>å°½ç®¡å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰çš„ç‰©ä½“å»é™¤æ–¹æ³•åœ¨å»é™¤ä¸å®Œæ•´ã€å†…å®¹åˆæˆä¸å‡†ç¡®ä»¥åŠåˆæˆåŒºåŸŸæ¨¡ç³Šç­‰æ–¹é¢ä»å­˜åœ¨å›°éš¾ï¼Œå¯¼è‡´æˆåŠŸç‡è¾ƒä½ã€‚è¿™äº›é—®é¢˜ä¸»è¦æ˜¯ç”±äºç¼ºä¹é«˜è´¨é‡é…å¯¹è®­ç»ƒæ•°æ®ä»¥åŠè¿™äº›æ–¹æ³•æ‰€é‡‡ç”¨çš„è‡ªç›‘ç£è®­ç»ƒèŒƒå¼å¯¼è‡´çš„ã€‚è‡ªç›‘ç£è®­ç»ƒèŒƒå¼è¿«ä½¿æ¨¡å‹å¯¹é®æŒ¡åŒºåŸŸè¿›è¡Œå¡«å……ï¼Œå¯¼è‡´åˆæˆé®æŒ¡ç‰©ä½“å’Œæ¢å¤èƒŒæ™¯ä¹‹é—´å­˜åœ¨æ¨¡ç³Šæ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆäººå·¥å‚ä¸åŠç›‘ç£å­¦ä¹ ç­–ç•¥æ¥åˆ›å»ºé«˜è´¨é‡é…å¯¹è®­ç»ƒæ•°æ®çš„æ–¹æ³•ï¼Œæ—¨åœ¨è®­ç»ƒä¸€ä¸ªç¨³å¥ç‰©ä½“å»é™¤å™¨ï¼ˆRORemï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆä»å…¬å¼€æ•°æ®æºæ”¶é›†6ä¸‡ç»„è®­ç»ƒé…å¯¹æ•°æ®ï¼Œè®­ç»ƒä¸€ä¸ªåˆå§‹ç‰©ä½“å»é™¤æ¨¡å‹æ¥ç”Ÿæˆå»é™¤æ ·æœ¬ï¼Œç„¶ååˆ©ç”¨äººå·¥åé¦ˆé€‰æ‹©ä¸€ç»„é«˜è´¨é‡çš„ç‰©ä½“å»é™¤é…å¯¹æ•°æ®ï¼Œå†ç”¨è¿™äº›æ•°æ®è®­ç»ƒä¸€ä¸ªé‰´åˆ«å™¨ï¼Œä»¥è‡ªåŠ¨åŒ–åç»­çš„è®­ç»ƒæ•°æ®ç”Ÿæˆè¿‡ç¨‹ã€‚ç»è¿‡å‡ è½®è¿­ä»£åï¼Œæˆ‘ä»¬æœ€ç»ˆè·å¾—äº†ä¸€ä¸ªåŒ…å«è¶…è¿‡20ä¸‡ç»„ç‰©ä½“çš„å»é™¤æ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†å¯¹é¢„è®­ç»ƒçš„ç¨³å®šæ‰©æ•£æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¾—åˆ°äº†æˆ‘ä»¬çš„RORemã€‚å®ƒåœ¨å¯é æ€§å’Œå›¾åƒè´¨é‡æ–¹é¢å®ç°äº†æœ€å…ˆè¿›çš„ç‰©ä½“å»é™¤æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å°†ä¹‹å‰æ–¹æ³•çš„ç‰©ä½“å»é™¤æˆåŠŸç‡æé«˜äº†18%ä»¥ä¸Šã€‚æ•°æ®é›†ã€æºä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/leeruibin/RORem">https://github.com/leeruibin/RORem</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.00740v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŠç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œç»“åˆäººå·¥å‚ä¸ï¼Œåˆ›å»ºé«˜è´¨é‡é…å¯¹è®­ç»ƒæ•°æ®ï¼Œæ—¨åœ¨è®­ç»ƒä¸€ä¸ªç¨³å¥çš„å¯¹è±¡ç§»é™¤å™¨ï¼ˆRORemï¼‰ã€‚é€šè¿‡æ”¶é›†å¼€æ”¾æºä»£ç æ•°æ®é›†ç”Ÿæˆåˆå§‹å¯¹è±¡ç§»é™¤æ¨¡å‹æ ·æœ¬ï¼Œå¹¶åˆ©ç”¨äººå·¥åé¦ˆé€‰æ‹©é«˜è´¨é‡çš„å¯¹è±¡ç§»é™¤é…å¯¹æ•°æ®æ¥è®­ç»ƒåˆ¤åˆ«å™¨ï¼Œè‡ªåŠ¨åŒ–åç»­è®­ç»ƒæ•°æ®ç”Ÿæˆè¿‡ç¨‹ã€‚ç»è¿‡å‡ è½®è¿­ä»£ï¼Œè·å¾—å¤§é‡åŒ…å«è¶…è¿‡20ä¸‡å¯¹çš„å¯¹è±¡ç§»é™¤æ•°æ®é›†ã€‚é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼Œå¾—åˆ°æ€§èƒ½å“è¶Šçš„RORemæ¨¡å‹ï¼Œå…¶åœ¨å¯é æ€§å’Œå›¾åƒè´¨é‡æ–¹é¢å‡è¾¾åˆ°ä¸šç•Œæœ€ä½³æ°´å¹³ï¼ŒæˆåŠŸæå‡å¯¹è±¡ç§»é™¤ç‡è¶…è¿‡18%ã€‚æ•°æ®é›†ã€æºä»£ç å’Œè®­ç»ƒæ¨¡å‹å‡å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å¯¹è±¡ç§»é™¤æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼šå°½ç®¡æœ‰é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰å¯¹è±¡ç§»é™¤æ–¹æ³•ä»é¢ä¸´ä¸å®Œå…¨ç§»é™¤ã€å†…å®¹åˆæˆä¸æ­£ç¡®å’ŒåˆæˆåŒºåŸŸæ¨¡ç³Šç­‰é—®é¢˜ï¼Œå¯¼è‡´æˆåŠŸç‡è¾ƒä½ã€‚</li>
<li>ä¸»è¦é—®é¢˜æ ¹æºï¼šç¼ºä¹é«˜è´¨é‡é…å¯¹è®­ç»ƒæ•°æ®å’Œè‡ªç›‘ç£è®­ç»ƒèŒƒå¼æ˜¯å¯¼è‡´è¿™äº›é—®é¢˜çš„ä¸»è¦åŸå› ã€‚</li>
<li>è§£å†³æ–¹æ¡ˆï¼šæå‡ºä¸€ç§åŠç›‘ç£å­¦ä¹ ç­–ç•¥ï¼Œç»“åˆäººå·¥å‚ä¸åˆ›å»ºé«˜è´¨é‡é…å¯¹è®­ç»ƒæ•°æ®ï¼Œæ—¨åœ¨è®­ç»ƒä¸€ä¸ªç¨³å¥çš„å¯¹è±¡ç§»é™¤å™¨ï¼ˆRORemï¼‰ã€‚</li>
<li>æ•°æ®æ”¶é›†ä¸è¿­ä»£ï¼šé€šè¿‡æ”¶é›†å¼€æ”¾æºä»£ç æ•°æ®é›†æ¥ç”Ÿæˆåˆå§‹å¯¹è±¡ç§»é™¤æ¨¡å‹æ ·æœ¬ï¼Œå¹¶åˆ©ç”¨äººå·¥åé¦ˆæ¥é€‰æ‹©é«˜è´¨é‡çš„æ•°æ®æ¥è®­ç»ƒåˆ¤åˆ«å™¨ï¼Œå®ç°åç»­æ•°æ®ç”Ÿæˆçš„è‡ªåŠ¨åŒ–ã€‚ç»è¿‡å¤šè½®è¿­ä»£ï¼Œè·å¾—å¤§é‡å¯¹è±¡ç§»é™¤æ•°æ®é›†ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½å“è¶Šï¼šRORemæ¨¡å‹é€šè¿‡å¾®è°ƒé¢„è®­ç»ƒçš„ç¨³å®šæ‰©æ•£æ¨¡å‹ï¼ŒæˆåŠŸæé«˜å¯¹è±¡ç§»é™¤ç‡è¶…è¿‡18%ï¼Œåœ¨å¯é æ€§å’Œå›¾åƒè´¨é‡æ–¹é¢è¾¾åˆ°ä¸šç•Œæœ€ä½³æ°´å¹³ã€‚</li>
<li>æ•°æ®é›†ä¸èµ„æºå…¬å¼€ï¼šæ•°æ®é›†ã€æºä»£ç å’Œè®­ç»ƒæ¨¡å‹å‡å·²å…¬å¼€åœ¨GitHubä¸Šä¾›å…¬ä¼—è®¿é—®å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.00740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a45ca0726a45ba74615639e0e916a336.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-60878675ad0033dc47bb8471462cf934.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f722f4161e3bb58b6c1261c619403de0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c48cd5acb064bff8293616686cf7ff8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62e631ad6caf2fb30e89a432e95cce18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17980883c30b9aa63f07feb36883739d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca4c0ca7d933c0ed4bb655144410357e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DART-Denoising-Autoregressive-Transformer-for-Scalable-Text-to-Image-Generation"><a href="#DART-Denoising-Autoregressive-Transformer-for-Scalable-Text-to-Image-Generation" class="headerlink" title="DART: Denoising Autoregressive Transformer for Scalable Text-to-Image   Generation"></a>DART: Denoising Autoregressive Transformer for Scalable Text-to-Image   Generation</h2><p><strong>Authors:Jiatao Gu, Yuyang Wang, Yizhe Zhang, Qihang Zhang, Dinghuai Zhang, Navdeep Jaitly, Josh Susskind, Shuangfei Zhai</strong></p>
<p>Diffusion models have become the dominant approach for visual generation. They are trained by denoising a Markovian process which gradually adds noise to the input. We argue that the Markovian property limits the modelâ€™s ability to fully utilize the generation trajectory, leading to inefficiencies during training and inference. In this paper, we propose DART, a transformer-based model that unifies autoregressive (AR) and diffusion within a non-Markovian framework. DART iteratively denoises image patches spatially and spectrally using an AR model that has the same architecture as standard language models. DART does not rely on image quantization, which enables more effective image modeling while maintaining flexibility. Furthermore, DART seamlessly trains with both text and image data in a unified model. Our approach demonstrates competitive performance on class-conditioned and text-to-image generation tasks, offering a scalable, efficient alternative to traditional diffusion models. Through this unified framework, DART sets a new benchmark for scalable, high-quality image synthesis. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºè§†è§‰ç”Ÿæˆçš„ä¸»è¦æ–¹æ³•ã€‚å®ƒä»¬é€šè¿‡å»å™ªé©¬å°”å¯å¤«è¿‡ç¨‹è¿›è¡Œè®­ç»ƒï¼Œè¯¥è¿‡ç¨‹é€æ­¥å‘è¾“å…¥æ·»åŠ å™ªå£°ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œé©¬å°”å¯å¤«å±æ€§é™åˆ¶äº†æ¨¡å‹å……åˆ†åˆ©ç”¨ç”Ÿæˆè½¨è¿¹çš„èƒ½åŠ›ï¼Œå¯¼è‡´è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­çš„æ•ˆç‡ä½ä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DARTï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ï¼Œå®ƒåœ¨éé©¬å°”å¯å¤«æ¡†æ¶å†…ç»Ÿä¸€äº†è‡ªå›å½’ï¼ˆARï¼‰å’Œæ‰©æ•£ã€‚DARTä½¿ç”¨ARæ¨¡å‹è¿­ä»£åœ°å¯¹å›¾åƒå—è¿›è¡Œç©ºé—´å’Œå…‰è°±å»å™ªï¼Œè¯¥ARæ¨¡å‹å…·æœ‰ä¸æ ‡å‡†è¯­è¨€æ¨¡å‹ç›¸åŒçš„æ¶æ„ã€‚DARTä¸ä¾èµ–äºå›¾åƒé‡åŒ–ï¼Œè¿™ä½¿å…¶åœ¨ä¿æŒçµæ´»æ€§çš„åŒæ—¶ï¼Œå®ç°äº†æ›´æœ‰æ•ˆçš„å›¾åƒå»ºæ¨¡ã€‚æ­¤å¤–ï¼ŒDARTå¯ä»¥åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­æ— ç¼åœ°åŒæ—¶ä½¿ç”¨æ–‡æœ¬å’Œå›¾åƒæ•°æ®è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç±»åˆ«æ¡ä»¶å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä¸ºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹æä¾›äº†å¯æ‰©å±•å’Œé«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡è¿™ä¸€ç»Ÿä¸€æ¡†æ¶ï¼ŒDARTä¸ºå¯æ‰©å±•çš„é«˜è´¨é‡å›¾åƒåˆæˆè®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.08159v2">PDF</a> Accepted by ICLR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºéé©¬å°”å¯å¤«æ¡†æ¶çš„ç»Ÿä¸€æ‰©æ•£æ¨¡å‹DARTï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è‡ªå›å½’ï¼ˆARï¼‰å’Œæ‰©æ•£æ¨¡å‹ï¼Œæé«˜äº†å›¾åƒç”Ÿæˆçš„æ•ˆç‡å’Œçµæ´»æ€§ã€‚DARTé€šè¿‡ç©ºé—´ä¸Šå’Œå…‰è°±ä¸Šçš„å›¾åƒå—è‡ªå›å½’æ¨¡å‹è¿›è¡Œè¿­ä»£å»å™ªï¼Œä¸éœ€è¦ä¾èµ–å›¾åƒé‡åŒ–ã€‚æ­¤å¤–ï¼ŒDARTå¯ä»¥åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­æ— ç¼åœ°ä½¿ç”¨æ–‡æœ¬å’Œå›¾åƒæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨ç±»æ¡ä»¶ç”Ÿæˆå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºè§†è§‰ç”Ÿæˆçš„ä¸»è¦æ–¹æ³•ï¼Œé€šè¿‡å»å™ªé©¬å°”å¯å¤«è¿‡ç¨‹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>é©¬å°”å¯å¤«å±æ€§é™åˆ¶äº†æ¨¡å‹åœ¨ç”Ÿæˆè½¨è¿¹ä¸Šçš„å®Œå…¨èƒ½åŠ›ï¼Œå¯¼è‡´è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­çš„æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>æœ¬æ–‡æå‡ºDARTæ¨¡å‹ï¼Œä¸€ä¸ªåŸºäºéé©¬å°”å¯å¤«æ¡†æ¶çš„ç»Ÿä¸€æ‰©æ•£æ¨¡å‹ï¼Œç»“åˆäº†è‡ªå›å½’ï¼ˆARï¼‰å’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>DARTé€šè¿‡ç©ºé—´ä¸Šå’Œå…‰è°±ä¸Šçš„å›¾åƒå—è¿›è¡Œè¿­ä»£å»å™ªï¼Œä½¿ç”¨ä¸æ ‡å‡†è¯­è¨€æ¨¡å‹ç›¸åŒçš„æ¶æ„çš„è‡ªå›å½’æ¨¡å‹ã€‚</li>
<li>DARTä¸éœ€è¦ä¾èµ–å›¾åƒé‡åŒ–ï¼Œä½¿å›¾åƒå»ºæ¨¡æ›´åŠ æœ‰æ•ˆå¹¶ä¿æŒçµæ´»æ€§ã€‚</li>
<li>DARTå¯ä»¥åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­æ— ç¼åœ°ä½¿ç”¨æ–‡æœ¬å’Œå›¾åƒæ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.08159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac9597d65100b353273108723e57a6e0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7716e9df02bc67a94a0995a5d1ad7eaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5793fa0f5d23246015949f10e43e2cfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07de6da17894d400e38ee65f17f4370e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Efficient-Domain-Augmentation-for-Autonomous-Driving-Testing-Using-Diffusion-Models"><a href="#Efficient-Domain-Augmentation-for-Autonomous-Driving-Testing-Using-Diffusion-Models" class="headerlink" title="Efficient Domain Augmentation for Autonomous Driving Testing Using   Diffusion Models"></a>Efficient Domain Augmentation for Autonomous Driving Testing Using   Diffusion Models</h2><p><strong>Authors:Luciano Baresi, Davide Yi Xian Hu, Andrea Stocco, Paolo Tonella</strong></p>
<p>Simulation-based testing is widely used to assess the reliability of Autonomous Driving Systems (ADS), but its effectiveness is limited by the operational design domain (ODD) conditions available in such simulators. To address this limitation, in this work, we explore the integration of generative artificial intelligence techniques with physics-based simulators to enhance ADS system-level testing. Our study evaluates the effectiveness and computational overhead of three generative strategies based on diffusion models, namely instruction-editing, inpainting, and inpainting with refinement. Specifically, we assess these techniquesâ€™ capabilities to produce augmented simulator-generated images of driving scenarios representing new ODDs. We employ a novel automated detector for invalid inputs based on semantic segmentation to ensure semantic preservation and realism of the neural generated images. We then perform system-level testing to evaluate the ADSâ€™s generalization ability to newly synthesized ODDs. Our findings show that diffusion models help increase the ODD coverage for system-level testing of ADS. Our automated semantic validator achieved a percentage of false positives as low as 3%, retaining the correctness and quality of the generated images for testing. Our approach successfully identified new ADS system failures before real-world testing. </p>
<blockquote>
<p>åŸºäºæ¨¡æ‹Ÿå™¨çš„æµ‹è¯•è¢«å¹¿æ³›ç”¨äºè¯„ä¼°è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼ˆADSï¼‰çš„å¯é æ€§ï¼Œä½†å…¶æœ‰æ•ˆæ€§å—é™äºæ¨¡æ‹Ÿå™¨ä¸­å¯ç”¨çš„æ“ä½œè®¾è®¡åŸŸï¼ˆODDï¼‰æ¡ä»¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é™åˆ¶ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿå™¨ä¸ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯çš„é›†æˆï¼Œä»¥æé«˜ADSç³»ç»Ÿçº§æµ‹è¯•çš„æ•ˆæœã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆç­–ç•¥çš„æœ‰æ•ˆæ€§å’Œè®¡ç®—å¼€é”€ï¼Œåˆ†åˆ«æ˜¯æŒ‡ä»¤ç¼–è¾‘ã€å›¾åƒä¿®å¤å’Œå¸¦æœ‰ç²¾ç»†åŒ–çš„å›¾åƒä¿®å¤ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯„ä¼°è¿™äº›æŠ€æœ¯åœ¨ç”Ÿæˆä»£è¡¨æ–°ODDçš„é©¾é©¶åœºæ™¯æ¨¡æ‹Ÿå™¨å›¾åƒæ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åŸºäºè¯­ä¹‰åˆ†å‰²çš„æ–°å‹æ— æ•ˆè¾“å…¥æ£€æµ‹å™¨ï¼Œä»¥ç¡®ä¿ç¥ç»ç”Ÿæˆå›¾åƒè¯­ä¹‰çš„ä¿ç•™å’ŒçœŸå®æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬è¿›è¡Œç³»ç»Ÿçº§æµ‹è¯•ï¼Œä»¥è¯„ä¼°ADSå¯¹æ–°åˆæˆODDçš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£æ¨¡å‹æœ‰åŠ©äºæé«˜ADSç³»ç»Ÿçº§æµ‹è¯•çš„ODDè¦†ç›–ç‡ã€‚æˆ‘ä»¬çš„è‡ªåŠ¨è¯­ä¹‰éªŒè¯å™¨å®ç°äº†é«˜è¾¾3%çš„è¯¯æŠ¥ç‡ï¼Œä¿æŒäº†æµ‹è¯•å›¾åƒç”Ÿæˆçš„è´¨é‡å’Œæ­£ç¡®æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æˆåŠŸåœ°åœ¨çœŸå®ä¸–ç•Œæµ‹è¯•ä¹‹å‰è¯†åˆ«å‡ºæ–°çš„ADSç³»ç»Ÿæ•…éšœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.13661v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ¨¡æ‹Ÿå™¨çš„æµ‹è¯•æ˜¯è¯„ä¼°è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼ˆADSï¼‰å¯é æ€§çš„å¸¸ç”¨æ–¹æ³•ï¼Œä½†å…¶æœ‰æ•ˆæ€§å—é™äºæ¨¡æ‹Ÿå™¨ä¸­çš„æ“ä½œè®¾è®¡åŸŸï¼ˆODDï¼‰æ¡ä»¶ã€‚ä¸ºè§£å†³æ­¤é™åˆ¶ï¼Œæœ¬ç ”ç©¶æ¢ç´¢å°†åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿå™¨å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯è¿›è¡Œé›†æˆï¼Œä»¥æé«˜ADSç³»ç»Ÿçº§åˆ«çš„æµ‹è¯•æ°´å¹³ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆç­–ç•¥çš„æœ‰æ•ˆæ€§å’Œè®¡ç®—å¼€é”€ï¼ŒåŒ…æ‹¬æŒ‡ä»¤ç¼–è¾‘ã€å›¾åƒä¿®å¤å’Œå¸¦ç²¾ä¿®çš„å›¾åƒä¿®å¤ã€‚è¿™äº›æŠ€æœ¯è¢«ç”¨äºç”Ÿæˆä»£è¡¨æ–°ODDçš„é©¾é©¶åœºæ™¯å›¾åƒã€‚æœ¬ç ”ç©¶é‡‡ç”¨åŸºäºè¯­ä¹‰åˆ†å‰²çš„æ–°å‹è‡ªåŠ¨åŒ–æ£€æµ‹å™¨ï¼Œç¡®ä¿ç¥ç»ç”Ÿæˆå›¾åƒä¿æŒè¯­ä¹‰å®Œæ•´æ€§å’Œé€¼çœŸåº¦ã€‚ç³»ç»Ÿçº§æµ‹è¯•çš„ç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹æœ‰åŠ©äºæé«˜ADSç³»ç»Ÿçº§æµ‹è¯•çš„ODDè¦†ç›–ç‡ã€‚è‡ªåŠ¨åŒ–è¯­ä¹‰éªŒè¯å™¨çš„å‡é˜³æ€§ç‡ä½äº3%ï¼Œä¿è¯äº†ç”Ÿæˆå›¾åƒç”¨äºæµ‹è¯•çš„æ­£ç¡®æ€§å’Œè´¨é‡ã€‚è¯¥æ–¹æ³•æˆåŠŸåœ¨çœŸå®ä¸–ç•Œæµ‹è¯•ä¹‹å‰è¯†åˆ«å‡ºæ–°çš„ADSç³»ç»Ÿæ•…éšœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡æ‹Ÿæµ‹è¯•æ˜¯è¯„ä¼°è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼ˆADSï¼‰å¯é æ€§çš„å¸¸ç”¨æ–¹æ³•ï¼Œä½†å—é™äºæ¨¡æ‹Ÿå™¨ä¸­çš„æ“ä½œè®¾è®¡åŸŸï¼ˆODDï¼‰ã€‚</li>
<li>ä¸ºæé«˜æµ‹è¯•æ°´å¹³ï¼Œç»“åˆäº†åŸºäºç‰©ç†çš„æ¨¡æ‹Ÿå™¨å’Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½æŠ€æœ¯ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†ä¸‰ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆç­–ç•¥ï¼šæŒ‡ä»¤ç¼–è¾‘ã€å›¾åƒä¿®å¤å’Œå¸¦ç²¾ä¿®çš„å›¾åƒä¿®å¤ã€‚</li>
<li>è¿™äº›ç­–ç•¥è¢«ç”¨äºç”Ÿæˆä»£è¡¨æ–°ODDçš„é©¾é©¶åœºæ™¯å›¾åƒï¼Œå¢å¼ºäº†ADSçš„ç³»ç»Ÿçº§æµ‹è¯•ã€‚</li>
<li>é‡‡ç”¨æ–°å‹è‡ªåŠ¨åŒ–æ£€æµ‹å™¨ç¡®ä¿ç¥ç»ç”Ÿæˆå›¾åƒçš„è¯­ä¹‰å®Œæ•´æ€§å’Œé€¼çœŸåº¦ã€‚</li>
<li>ç³»ç»Ÿçº§æµ‹è¯•æ˜¾ç¤ºæ‰©æ•£æ¨¡å‹æé«˜äº†ADSç³»ç»Ÿçº§æµ‹è¯•çš„ODDè¦†ç›–ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.13661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a9c72ecb3a8e5b1f3345e6bad2d31786.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4114a7c0c05d712f75e3254ac14ad330.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29218a7d9cbc9b929231fc634512a550.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd9b9230137519698aa25d9c562c845c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b53fe96393e5c606664452fedfc5b80f.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Guided-Reconstruction-with-Conditioned-Diffusion-Models-for-Unsupervised-Anomaly-Detection-in-Brain-MRIs"><a href="#Guided-Reconstruction-with-Conditioned-Diffusion-Models-for-Unsupervised-Anomaly-Detection-in-Brain-MRIs" class="headerlink" title="Guided Reconstruction with Conditioned Diffusion Models for Unsupervised   Anomaly Detection in Brain MRIs"></a>Guided Reconstruction with Conditioned Diffusion Models for Unsupervised   Anomaly Detection in Brain MRIs</h2><p><strong>Authors:Finn Behrendt, Debayan Bhattacharya, Robin Mieling, Lennart Maack, Julia KrÃ¼ger, Roland Opfer, Alexander Schlaefer</strong></p>
<p>The application of supervised models to clinical screening tasks is challenging due to the need for annotated data for each considered pathology. Unsupervised Anomaly Detection (UAD) is an alternative approach that aims to identify any anomaly as an outlier from a healthy training distribution. A prevalent strategy for UAD in brain MRI involves using generative models to learn the reconstruction of healthy brain anatomy for a given input image. As these models should fail to reconstruct unhealthy structures, the reconstruction errors indicate anomalies. However, a significant challenge is to balance the accurate reconstruction of healthy anatomy and the undesired replication of abnormal structures. While diffusion models have shown promising results with detailed and accurate reconstructions, they face challenges in preserving intensity characteristics, resulting in false positives. We propose conditioning the denoising process of diffusion models with additional information derived from a latent representation of the input image. We demonstrate that this conditioning allows for accurate and local adaptation to the general input intensity distribution while avoiding the replication of unhealthy structures. We compare the novel approach to different state-of-the-art methods and for different data sets. Our results show substantial improvements in the segmentation performance, with the Dice score improved by 11.9%, 20.0%, and 44.6%, for the BraTS, ATLAS and MSLUB data sets, respectively, while maintaining competitive performance on the WMH data set. Furthermore, our results indicate effective domain adaptation across different MRI acquisitions and simulated contrasts, an important attribute for general anomaly detection methods. The code for our work is available at <a target="_blank" rel="noopener" href="https://github.com/FinnBehrendt/Conditioned-Diffusion-Models-UAD">https://github.com/FinnBehrendt/Conditioned-Diffusion-Models-UAD</a> </p>
<blockquote>
<p>å°†ç›‘ç£æ¨¡å‹åº”ç”¨äºä¸´åºŠç­›æŸ¥ä»»åŠ¡æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºéœ€è¦è€ƒè™‘é’ˆå¯¹æ¯ç§ç—…ç†çš„æ ‡æ³¨æ•°æ®ã€‚æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼ˆUADï¼‰æ˜¯ä¸€ç§æ›¿ä»£æ–¹æ³•ï¼Œæ—¨åœ¨ä»å¥åº·çš„è®­ç»ƒåˆ†å¸ƒä¸­è¯†åˆ«ä»»ä½•å¼‚å¸¸å€¼ä½œä¸ºç¦»ç¾¤å€¼ã€‚UADåœ¨è„‘éƒ¨MRIä¸­çš„å¸¸è§ç­–ç•¥æ˜¯ä½¿ç”¨ç”Ÿæˆæ¨¡å‹æ¥å­¦ä¹ ç»™å®šè¾“å…¥å›¾åƒçš„è„‘éƒ¨ç»“æ„çš„é‡å»ºã€‚ç”±äºè¿™äº›æ¨¡å‹æ— æ³•é‡å»ºä¸å¥åº·çš„ç»“æ„ï¼Œå› æ­¤é‡å»ºè¯¯å·®æŒ‡ç¤ºäº†å¼‚å¸¸ã€‚ç„¶è€Œï¼Œä¸€ä¸ªé‡å¤§æŒ‘æˆ˜æ˜¯å¹³è¡¡å¥åº·ç»“æ„çš„å‡†ç¡®é‡å»ºå’Œå¼‚å¸¸ç»“æ„çš„éå¿…è¦å¤åˆ¶ã€‚å°½ç®¡æ‰©æ•£æ¨¡å‹åœ¨è¯¦ç»†å’Œå‡†ç¡®çš„é‡å»ºæ–¹é¢æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†å®ƒä»¬é¢ä¸´ç€ä¿æŒå¼ºåº¦ç‰¹æ€§çš„æŒ‘æˆ˜ï¼Œè¿™ä¼šå¯¼è‡´è¯¯æŠ¥ã€‚æˆ‘ä»¬æå‡ºå°†æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ä¸è¾“å…¥å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºæ´¾ç”Ÿå‡ºçš„é™„åŠ ä¿¡æ¯ç›¸ç»“åˆã€‚æˆ‘ä»¬è¯æ˜è¿™ç§æ¡ä»¶åŒ–å…è®¸å¯¹ä¸€èˆ¬è¾“å…¥å¼ºåº¦åˆ†å¸ƒè¿›è¡Œå‡†ç¡®å’Œå±€éƒ¨é€‚åº”ï¼ŒåŒæ—¶é¿å…å¤åˆ¶ä¸å¥åº·ç»“æ„ã€‚æˆ‘ä»¬å°†æ–°æ–¹æ³•ä¸ä¸åŒçš„æœ€æ–°æ–¹æ³•å’Œæ•°æ®é›†è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„åˆ†å‰²æ€§èƒ½æ”¹è¿›ï¼Œå¯¹äºBraTSã€ATLASå’ŒMSLUBæ•°æ®é›†ï¼ŒDiceå¾—åˆ†åˆ†åˆ«æé«˜äº†11.9%ã€20.0%å’Œ44.6%ï¼ŒåŒæ—¶åœ¨WMHæ•°æ®é›†ä¸Šä¿æŒç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨ä¸åŒMRIé‡‡é›†å’Œæ¨¡æ‹Ÿå¯¹æ¯”åº¦ä¹‹é—´å®ç°äº†æœ‰æ•ˆçš„é¢†åŸŸé€‚åº”ï¼Œè¿™å¯¹äºä¸€èˆ¬å¼‚å¸¸æ£€æµ‹æ–¹æ³•è€Œè¨€æ˜¯ä¸€ä¸ªé‡è¦å±æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/FinnBehrendt/Conditioned-Diffusion-Models-UAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/FinnBehrendt/Conditioned-Diffusion-Models-UADæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.04215v2">PDF</a> Preprint: Accepted paper at Combuters in Biology and medicine</p>
<p><strong>Summary</strong><br>     æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒä¸­çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä¿æŒå¼ºåº¦ç‰¹æ€§å’Œé¿å…å¤åˆ¶å¼‚å¸¸ç»“æ„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³é€šè¿‡è¾“å…¥å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºæ¥æ¡ä»¶åŒ–æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ã€‚æ­¤æ–¹æ³•å¯å‡†ç¡®é€‚åº”æ€»ä½“è¾“å…¥å¼ºåº¦åˆ†å¸ƒï¼ŒåŒæ—¶é¿å…å¤åˆ¶ä¸å¥åº·ç»“æ„ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¹…æé«˜äº†åˆ†å‰²æ€§èƒ½ï¼Œå¹¶åœ¨ä¸åŒMRIé‡‡é›†å’Œæ¨¡æ‹Ÿå¯¹æ¯”åº¦ä¸Šå®ç°äº†æœ‰æ•ˆçš„åŸŸé€‚åº”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼ˆUADï¼‰ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€è¦å¹³è¡¡å¥åº·è§£å‰–ç»“æ„çš„å‡†ç¡®é‡å»ºå’Œå¼‚å¸¸ç»“æ„çš„å¤åˆ¶é—®é¢˜ã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨åŒ»å­¦å›¾åƒé‡å»ºä¸­å¯èƒ½æ— æ³•å‡†ç¡®ä¿ç•™å¼ºåº¦ç‰¹æ€§ï¼Œå¯¼è‡´è¯¯æŠ¥ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡è¾“å…¥å›¾åƒçš„æ½œåœ¨è¡¨ç¤ºæ¥æ¡ä»¶åŒ–æ‰©æ•£æ¨¡å‹çš„å»å™ªè¿‡ç¨‹ï¼Œå®ç°æ›´å‡†ç¡®çš„é‡å»ºå¹¶é¿å…å¼‚å¸¸ç»“æ„çš„å¤åˆ¶ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤§å¹…æé«˜åˆ†å‰²æ€§èƒ½ï¼Œå¹¶å®ç°äº†æœ‰æ•ˆçš„åŸŸé€‚åº”ã€‚</li>
<li>æ–¹æ³•åœ¨ä¸åŒMRIé‡‡é›†å’Œæ¨¡æ‹Ÿå¯¹æ¯”åº¦ä¸Šå…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚</li>
<li>å…¬å¼€äº†ç›¸å…³ä»£ç ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
<li>æ­¤æ–¹æ³•æœ‰æœ›æ”¹å–„ä¸´åºŠç­›æŸ¥ä»»åŠ¡ä¸­çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼Œä¸ºåŒ»å­¦å›¾åƒåˆ†ææä¾›æ–°çš„å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.04215">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a58d16688320eeed9231164c23823e34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-26f89ee26535db9b5dc129d187e6f2d1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Flow-Guided-Diffusion-for-Video-Inpainting"><a href="#Flow-Guided-Diffusion-for-Video-Inpainting" class="headerlink" title="Flow-Guided Diffusion for Video Inpainting"></a>Flow-Guided Diffusion for Video Inpainting</h2><p><strong>Authors:Bohai Gu, Yongsheng Yu, Heng Fan, Libo Zhang</strong></p>
<p>Video inpainting has been challenged by complex scenarios like large movements and low-light conditions. Current methods, including emerging diffusion models, face limitations in quality and efficiency. This paper introduces the Flow-Guided Diffusion model for Video Inpainting (FGDVI), a novel approach that significantly enhances temporal consistency and inpainting quality via reusing an off-the-shelf image generation diffusion model. We employ optical flow for precise one-step latent propagation and introduces a model-agnostic flow-guided latent interpolation technique. This technique expedites denoising, seamlessly integrating with any Video Diffusion Model (VDM) without additional training. Our FGDVI demonstrates a remarkable 10% improvement in flow warping error E_warp over existing state-of-the-art methods. Our comprehensive experiments validate superior performance of FGDVI, offering a promising direction for advanced video inpainting. The code and detailed results will be publicly available in <a target="_blank" rel="noopener" href="https://github.com/NevSNev/FGDVI">https://github.com/NevSNev/FGDVI</a>. </p>
<blockquote>
<p>è§†é¢‘è¡¥å…¨æŠ€æœ¯é¢ä¸´å¤æ‚åœºæ™¯ï¼ˆå¦‚å¤§åŠ¨ä½œå’Œä½å…‰ç…§æ¡ä»¶ï¼‰çš„æŒ‘æˆ˜ã€‚å½“å‰çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æ–°å…´çš„æ‰©æ•£æ¨¡å‹åœ¨å†…ï¼Œåœ¨è´¨é‡å’Œæ•ˆç‡æ–¹é¢éƒ½å­˜åœ¨å±€é™æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†è§†é¢‘è¡¥å…¨çš„æµå¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼ˆFGDVIï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œé€šè¿‡å¤ç”¨ç°æˆçš„å›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹æ˜¾è‘—æé«˜äº†æ—¶é—´ä¸€è‡´æ€§å’Œè¡¥å…¨è´¨é‡ã€‚æˆ‘ä»¬é‡‡ç”¨å…‰æµè¿›è¡Œç²¾ç¡®çš„ä¸€æ­¥æ½œåœ¨ä¼ æ’­ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ¨¡å‹æ— å…³çš„æµå¼•å¯¼æ½œåœ¨æ’å€¼æŠ€æœ¯ã€‚è¿™é¡¹æŠ€æœ¯åŠ å¿«äº†å»å™ªè¿‡ç¨‹ï¼Œæ— éœ€é¢å¤–è®­ç»ƒå³å¯æ— ç¼é›†æˆåˆ°ä»»ä½•è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰ä¸­ã€‚æˆ‘ä»¬çš„FGDVIåœ¨æµæ‰­æ›²è¯¯å·®E_warpä¸Šè¾ƒç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„10%æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç»¼åˆå®éªŒéªŒè¯äº†FGDVIçš„ä¼˜è¶Šæ€§èƒ½ï¼Œä¸ºå…ˆè¿›çš„è§†é¢‘è¡¥å…¨æä¾›äº†æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚ä»£ç å’Œè¯¦ç»†ç»“æœå°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NevSNev/FGDVI%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/NevSNev/FGDVIå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.15368v2">PDF</a> This paper has been withdrawn as a new iteration of the work has been   developed, which includes significant improvements and refinements based on   this submission. The withdrawal is made to ensure academic integrity and   compliance with publication standards. If you are interested, please refer to   the updated work at arXiv:2412.00857</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†åŸºäºæµå¼•å¯¼çš„æ‰©æ•£æ¨¡å‹ï¼ˆFGDVIï¼‰çš„è§†é¢‘è¡¥å…¨æ–°æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å…‰å­¦æµè¿›è¡Œç²¾ç¡®çš„ä¸€æ­¥æ½œåœ¨ä¼ æ’­å’Œå¼•å…¥æ¨¡å‹æ— å…³çš„æµå¼•å¯¼æ½œåœ¨æ’å€¼æŠ€æœ¯ï¼Œæ˜¾è‘—æé«˜äº†æ—¶é—´ä¸€è‡´æ€§å’Œè¡¥å…¨è´¨é‡ã€‚è¯¥æ–¹æ³•å¯å¿«é€Ÿå»å™ªï¼Œæ— ç¼é›†æˆåˆ°ä»»ä½•è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼ˆVDMï¼‰ä¸­ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFGDVIåœ¨æµæ‰­æ›²è¯¯å·®E_warpä¸Šè¾ƒç°æœ‰å…ˆè¿›æŠ€æœ¯æœ‰æ˜¾è‘—æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FGDVIæ˜¯ä¸€ç§æ–°é¢–çš„åŸºäºæ‰©æ•£æ¨¡å‹çš„è§†é¢‘è¡¥å…¨æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¼•å…¥å…‰å­¦æµè¿›è¡Œç²¾ç¡®çš„ä¸€æ­¥æ½œåœ¨ä¼ æ’­ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ¨¡å‹æ— å…³çš„æµå¼•å¯¼æ½œåœ¨æ’å€¼æŠ€æœ¯ã€‚</li>
<li>FGDVIæé«˜äº†æ—¶é—´ä¸€è‡´æ€§å’Œè¡¥å…¨è´¨é‡ã€‚</li>
<li>æ–¹æ³•å¯å¿«é€Ÿå»å™ªï¼Œå¹¶èƒ½æ— ç¼é›†æˆåˆ°ä»»ä½•è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­ã€‚</li>
<li>FGDVIåœ¨æµæ‰­æ›²è¯¯å·®ä¸Šè¾ƒç°æœ‰æŠ€æœ¯æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.15368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23965ee5e6bb5a64518f3fb7a16c7f1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08bce2388b1ab66e419d6f1d0c8ac493.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2cc31b87637267534f0e100191e02067.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3d746d480927ec676733740812f605c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b007ad1c9915d828b048ca028ff3019d.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Dequantization-and-Color-Transfer-with-Diffusion-Models"><a href="#Dequantization-and-Color-Transfer-with-Diffusion-Models" class="headerlink" title="Dequantization and Color Transfer with Diffusion Models"></a>Dequantization and Color Transfer with Diffusion Models</h2><p><strong>Authors:Vaibhav Vavilala, Faaris Shaik, David Forsyth</strong></p>
<p>We demonstrate an image dequantizing diffusion model that enables novel edits on natural images. We propose operating on quantized images because they offer easy abstraction for patch-based edits and palette transfer. In particular, we show that color palettes can make the output of the diffusion model easier to control and interpret. We first establish that existing image restoration methods are not sufficient, such as JPEG noise reduction models. We then demonstrate that our model can generate natural images that respect the color palette the user asked for. For palette transfer, we propose a method based on weighted bipartite matching. We then show that our model generates plausible images even after extreme palette transfers, respecting user query. Our method can optionally condition on the source texture in part or all of the image. In doing so, we overcome a common problem in existing image colorization methods that are unable to produce colors with a different luminance than the input. We evaluate several possibilities for texture conditioning and their trade-offs, including luminance, image gradients, and thresholded gradients, the latter of which performed best in maintaining texture and color control simultaneously. Our method can be usefully extended to another practical edit: recoloring patches of an image while respecting the source texture. Our procedure is supported by several qualitative and quantitative evaluations. </p>
<blockquote>
<p>æˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§å›¾åƒå»é‡åŒ–æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯åœ¨è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œæ–°å‹ç¼–è¾‘ã€‚æˆ‘ä»¬æå‡ºåœ¨é‡åŒ–å›¾åƒä¸Šè¿›è¡Œæ“ä½œï¼Œå› ä¸ºé‡åŒ–å›¾åƒä¸ºåŸºäºè¡¥ä¸çš„ç¼–è¾‘å’Œè°ƒè‰²æ¿è½¬æ¢æä¾›äº†ç®€å•çš„æŠ½è±¡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œè‰²å½©è°ƒè‰²æ¿å¯ä»¥ä½¿æ‰©æ•£æ¨¡å‹çš„è¾“å‡ºæ›´å®¹æ˜“æ§åˆ¶å’Œè§£é‡Šã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç¡®å®šç°æœ‰çš„å›¾åƒæ¢å¤æ–¹æ³•ï¼ˆä¾‹å¦‚JPEGé™å™ªæ¨¡å‹ï¼‰æ˜¯ä¸è¶³çš„ã€‚ç„¶åï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ç”Ÿæˆå°Šé‡ç”¨æˆ·è¦æ±‚è‰²å½©è°ƒè‰²æ¿çš„è‡ªç„¶å›¾åƒã€‚å¯¹äºè°ƒè‰²æ¿è½¬æ¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŠ æƒäºŒåˆ†åŒ¹é…çš„æ–¹æ³•ã€‚æ¥ç€ï¼Œæˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ¨¡å‹å³ä½¿åœ¨æç«¯çš„è°ƒè‰²æ¿è½¬æ¢åä¹Ÿèƒ½ç”Ÿæˆåˆç†çš„å›¾åƒï¼Œå°Šé‡ç”¨æˆ·æŸ¥è¯¢ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é€‰æ‹©å¯¹å›¾åƒçš„éƒ¨åˆ†æˆ–å…¨éƒ¨æºçº¹ç†è¿›è¡Œæ¡ä»¶å¤„ç†ã€‚è¿™æ ·åšå¯ä»¥å…‹æœç°æœ‰å›¾åƒç€è‰²æ–¹æ³•ä¸­å¸¸è§çš„æ— æ³•äº§ç”Ÿä¸è¾“å…¥ä¸åŒäº®åº¦çš„é¢œè‰²çš„é—®é¢˜ã€‚æˆ‘ä»¬è¯„ä¼°äº†å‡ ç§çº¹ç†æ¡ä»¶åŠå…¶æƒè¡¡çš„å¯èƒ½æ€§ï¼ŒåŒ…æ‹¬äº®åº¦ã€å›¾åƒæ¢¯åº¦å’Œé˜ˆå€¼æ¢¯åº¦ç­‰ï¼Œåè€…åœ¨åŒæ—¶ä¿æŒçº¹ç†å’Œé¢œè‰²æ§åˆ¶æ–¹é¢è¡¨ç°æœ€ä½³ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ‰©å±•åˆ°å¦ä¸€ç§å®ç”¨çš„ç¼–è¾‘ï¼šåœ¨å°Šé‡æºçº¹ç†çš„åŒæ—¶ï¼Œå¯¹å›¾åƒçš„è¡¥ä¸è¿›è¡Œé‡æ–°ç€è‰²ã€‚æˆ‘ä»¬çš„ç¨‹åºå¾—åˆ°äº†å¤šç§å®šæ€§å’Œå®šé‡è¯„ä¼°çš„æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.02698v5">PDF</a> WACV 2025 23 pages, 21 figures, 4 tables</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡å±•ç¤ºäº†ä¸€ç§å›¾åƒå»é‡åŒ–æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯åœ¨è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œæ–°é¢–ç¼–è¾‘ã€‚æ–‡ç« æå‡ºåœ¨é‡åŒ–å›¾åƒä¸Šè¿›è¡Œæ“ä½œï¼Œå› ä¸ºé‡åŒ–å›¾åƒä¾¿äºåŸºäºè¡¥ä¸çš„ç¼–è¾‘å’Œè°ƒè‰²æ¿è½¬æ¢ã€‚æ–‡ç« è¡¨æ˜ï¼Œé¢œè‰²è°ƒè‰²æ¿å¯ä½¿æ‰©æ•£æ¨¡å‹çš„è¾“å‡ºæ›´æ˜“æ§åˆ¶å’Œè§£é‡Šã€‚æ–‡ç« é¦–å…ˆç¡®å®šç°æœ‰å›¾åƒæ¢å¤æ–¹æ³•ï¼ˆå¦‚JPEGé™å™ªæ¨¡å‹ï¼‰çš„ä¸è¶³ï¼Œç„¶åå±•ç¤ºè¯¥æ¨¡å‹å¯æ ¹æ®ç”¨æˆ·è¦æ±‚ç”Ÿæˆå°Šé‡é¢œè‰²è°ƒè‰²çš„è‡ªç„¶å›¾åƒã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§åŸºäºåŠ æƒäºŒåˆ†åŒ¹é…çš„æ–¹æ³•è¿›è¡Œè°ƒè‰²æ¿è½¬æ¢ï¼Œå¹¶èƒ½å¤Ÿè¿›è¡Œæç«¯çš„è°ƒè‰²æ¿è½¬æ¢åŒæ—¶å°Šé‡ç”¨æˆ·æŸ¥è¯¢ã€‚è¯¥æ¨¡å‹å¯éƒ¨åˆ†æˆ–å…¨éƒ¨ä»¥æºçº¹ç†ä¸ºæ¡ä»¶ï¼Œä»è€Œè§£å†³äº†ç°æœ‰å›¾åƒç€è‰²æ–¹æ³•æ— æ³•äº§ç”Ÿä¸è¾“å…¥ä¸åŒäº®åº¦çš„é¢œè‰²çš„é—®é¢˜ã€‚æ–‡ç« è¯„ä¼°äº†å‡ ç§çº¹ç†æ¡ä»¶åŠå…¶æƒè¡¡ï¼ŒåŒ…æ‹¬äº®åº¦ã€å›¾åƒæ¢¯åº¦å’Œé˜ˆå€¼æ¢¯åº¦ï¼Œåè€…åœ¨ä¿æŒçº¹ç†å’Œé¢œè‰²æ§åˆ¶æ–¹é¢è¡¨ç°æœ€ä½³ã€‚è¯¥æ–¹æ³•å¯æ‰©å±•åˆ°å¦ä¸€ç§å®ç”¨ç¼–è¾‘ï¼šåœ¨å°Šé‡æºçº¹ç†çš„åŒæ—¶é‡æ–°ç€è‰²å›¾åƒçš„è¡¥ä¸ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å±•ç¤ºäº†ä¸€ç§å›¾åƒå»é‡åŒ–æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨è‡ªç„¶å›¾åƒä¸Šè¿›è¡Œæ–°é¢–ç¼–è¾‘ã€‚</li>
<li>é€šè¿‡å¯¹é‡åŒ–å›¾åƒçš„æ“ä½œï¼Œæ–¹ä¾¿äº†åŸºäºè¡¥ä¸çš„ç¼–è¾‘å’Œè°ƒè‰²æ¿è½¬æ¢ã€‚</li>
<li>é¢œè‰²è°ƒè‰²æ¿ä½¿å¾—æ‰©æ•£æ¨¡å‹çš„è¾“å‡ºæ›´å®¹æ˜“æ§åˆ¶å’Œè§£é‡Šã€‚</li>
<li>ç°æœ‰å›¾åƒæ¢å¤æ–¹æ³•çš„ä¸è¶³è¢«æŒ‡å‡ºï¼Œå¹¶å±•ç¤ºäº†æ–°æ¨¡å‹åœ¨ç”Ÿæˆå°Šé‡é¢œè‰²è°ƒè‰²çš„è‡ªç„¶å›¾åƒæ–¹é¢çš„ä¼˜åŠ¿ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºåŠ æƒäºŒåˆ†åŒ¹é…çš„æ–¹æ³•è¿›è¡Œè°ƒè‰²æ¿è½¬æ¢ï¼Œç”šè‡³å¯ä»¥è¿›è¡Œæç«¯è°ƒè‰²æ¿è½¬æ¢ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿä»¥æºçº¹ç†ä¸ºæ¡ä»¶ï¼Œè§£å†³ç°æœ‰å›¾åƒç€è‰²æ–¹æ³•çš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2307.02698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b0acc2a95a8963ca9df34c70cc2000a6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8d88d2a6579943006b63957af68a5c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-20bd098e825c618499869499a4a1bd33.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3b2513a1fd40efafd703fdfe618df66.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-540b72f8ceb9a49644c63dd72aac4be6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3318ec3643c0ee45e1a80bd7ad388565.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-25/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-25/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-65b9cf9e48ea8187bdaca9ec411867dc.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-25  Self-Supervised Diffusion MRI Denoising via Iterative and Stable   Refinement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-25/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-1f3c04d62a25883b7637b37a3c1b5815.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-25  VIGS SLAM IMU-based Large-Scale 3D Gaussian Splatting SLAM
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">14807.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
