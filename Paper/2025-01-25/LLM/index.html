<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-01-25  CRPO Confidence-Reward Driven Preference Optimization for Machine   Translation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-983d629211789b6cd0fd01875e6cf904.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    77 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-25-更新"><a href="#2025-01-25-更新" class="headerlink" title="2025-01-25 更新"></a>2025-01-25 更新</h1><h2 id="CRPO-Confidence-Reward-Driven-Preference-Optimization-for-Machine-Translation"><a href="#CRPO-Confidence-Reward-Driven-Preference-Optimization-for-Machine-Translation" class="headerlink" title="CRPO: Confidence-Reward Driven Preference Optimization for Machine   Translation"></a>CRPO: Confidence-Reward Driven Preference Optimization for Machine   Translation</h2><p><strong>Authors:Guofeng Cui, Pichao Wang, Yang Liu, Zemian Ke, Zhu Liu, Vimal Bhat</strong></p>
<p>Large language models (LLMs) have shown great potential in natural language processing tasks, but their application to machine translation (MT) remains challenging due to pretraining on English-centric data and the complexity of reinforcement learning from human feedback (RLHF). Direct Preference Optimization (DPO) has emerged as a simpler and more efficient alternative, but its performance depends heavily on the quality of preference data. To address this, we propose Confidence-Reward driven Preference Optimization (CRPO), a novel method that combines reward scores with model confidence to improve data selection for fine-tuning. CRPO selects challenging sentence pairs where the model is uncertain or underperforms, leading to more effective learning. While primarily designed for LLMs, CRPO also generalizes to encoder-decoder models like NLLB, demonstrating its versatility. Empirical results show that CRPO outperforms existing methods such as RS-DPO, RSO and MBR score in both translation accuracy and data efficiency. </p>
<blockquote>
<p>大型语言模型（LLM）在自然语言处理任务中显示出巨大潜力，但将其应用于机器翻译（MT）仍然具有挑战性，这主要是由于以英语为中心的预训练数据和从人类反馈中进行强化学习（RLHF）的复杂性。直接偏好优化（DPO）作为一种更简单、更高效的选择方法已经出现，但其性能在很大程度上取决于偏好数据的质量。为了解决这一问题，我们提出了基于信心奖励的偏好优化（CRPO），这是一种结合奖励分数和模型信心以提高微调数据选择的新方法。CRPO选择模型不确定或表现不佳的具有挑战性的句子对，从而实现更有效的学习。虽然CRPO主要设计用于大型语言模型，但它也适用于编码器解码器模型，如NLLB，显示出其通用性。经验结果表明，CRPO在翻译准确性和数据效率方面都优于现有方法，如RS-DPO、RSO和MBR评分。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13927v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型在自然语言处理任务中展现出巨大潜力，但在机器翻译方面的应用仍面临挑战。直接偏好优化（DPO）作为一种更简单高效的方法应运而生，但其性能取决于偏好数据的质量。为解决这一问题，我们提出信心奖励驱动偏好优化（CRPO）方法，结合奖励分数和模型信心改善数据选择，用于微调。CRPO选择模型不确定或表现不佳的难句对，实现更有效的学习。CRPO不仅适用于大型语言模型，也可推广至编码器解码器模型如NLLB，显示出其通用性。实证结果显示，CRPO在翻译准确性和数据效率方面优于现有方法如RS-DPO、RSO和MBR分数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在机器翻译方面应用面临挑战，包括基于英语的数据预训练和强化学习的问题。</li>
<li>直接偏好优化（DPO）作为解决上述问题的简化方法被提出，但性能受限于偏好数据质量。</li>
<li>提出信心奖励驱动偏好优化（CRPO）方法，结合奖励分数和模型信心改善数据选择过程。</li>
<li>CRPO选择模型不确定或表现不佳的难句对进行更有效的学习。</li>
<li>CRPO不仅适用于大型语言模型，也可应用于编码器解码器模型如NLLB。</li>
<li>实证结果显示CRPO在翻译准确性和数据效率方面优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13927">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b6c0f70efb665da67d169cd5201f9a9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-908a67c5457a4a59d82652efddd413c2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Privacy-Preserving-Personalized-Federated-Prompt-Learning-for-Multimodal-Large-Language-Models"><a href="#Privacy-Preserving-Personalized-Federated-Prompt-Learning-for-Multimodal-Large-Language-Models" class="headerlink" title="Privacy-Preserving Personalized Federated Prompt Learning for Multimodal   Large Language Models"></a>Privacy-Preserving Personalized Federated Prompt Learning for Multimodal   Large Language Models</h2><p><strong>Authors:Linh Tran, Wei Sun, Stacy Patterson, Ana Milanova</strong></p>
<p>Multimodal Large Language Models (LLMs) are pivotal in revolutionizing customer support and operations by integrating multiple modalities such as text, images, and audio. Federated Prompt Learning (FPL) is a recently proposed approach that combines pre-trained multimodal LLMs such as vision-language models with federated learning to create personalized, privacy-preserving AI systems. However, balancing the competing goals of personalization, generalization, and privacy remains a significant challenge. Over-personalization can lead to overfitting, reducing generalizability, while stringent privacy measures, such as differential privacy, can hinder both personalization and generalization. In this paper, we propose a Differentially Private Federated Prompt Learning (DP-FPL) approach to tackle this challenge by leveraging a low-rank adaptation scheme to capture generalization while maintaining a residual term that preserves expressiveness for personalization. To ensure privacy, we introduce a novel method where we apply local differential privacy to the two low-rank components of the local prompt, and global differential privacy to the global prompt. Our approach mitigates the impact of privacy noise on the model performance while balancing the tradeoff between personalization and generalization. Extensive experiments demonstrate the effectiveness of our approach over other benchmarks. </p>
<blockquote>
<p>多模态大型语言模型（LLM）通过整合文本、图像和音频等多种模态，在革新客户支持和运营方面发挥着关键作用。联邦提示学习（FPL）是一种最近提出的方法，它将预训练的多模态LLM（如视觉语言模型）与联邦学习相结合，以创建个性化、保护隐私的人工智能系统。然而，在个性化、通用化和隐私之间取得平衡仍然是一个巨大的挑战。过度个性化可能导致过度拟合，降低通用性，而严格的隐私措施（如差分隐私）可能会阻碍个性化和通用化。在本文中，我们提出了一种差分私有联邦提示学习（DP-FPL）方法，通过利用低阶适应方案来捕捉通用性，同时保持一个残留项来保持个性化的表现力，以应对这一挑战。为确保隐私，我们介绍了一种新方法，对局部提示的两个低阶分量应用本地差分隐私，对全局提示应用全局差分隐私。我们的方法减轻了隐私噪声对模型性能的影响，在个性化与通用化之间取得了平衡。大量实验表明，我们的方法在其他基准测试上非常有效。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13904v1">PDF</a> Accepted to ICLR 2025 main conference track</p>
<p><strong>Summary</strong>:<br>多媒体大语言模型通过集成文本、图像和音频等多媒体模式，在客户支持和运营领域掀起革命。本文提出一种基于差分隐私的联邦提示学习方法（DP-FPL），通过低阶适应方案实现个性化与泛化之间的平衡，同时采用本地差分隐私和全局差分隐私保护机制，有效减轻隐私噪声对模型性能的影响。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>多模态大语言模型在客户支持和运营中具有重要作用。</li>
<li>联邦提示学习是一种将预训练的多模态大语言模型与联邦学习相结合的方法，用于创建个性化、保护隐私的人工智能系统。</li>
<li>平衡个性化、泛化和隐私是面临的主要挑战。</li>
<li>过度的个性化可能导致过度拟合，降低泛化能力。</li>
<li>严格的隐私措施，如差分隐私，可能会影响个性化和泛化。</li>
<li>DP-FPL方法通过低阶适应方案平衡个性化和泛化，同时采用本地和全局差分隐私保护机制确保隐私。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13904">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-1ed546cf96c193747e1f1de6d62355b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c65c9f77ad8b75c23932f4e51136cf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a37bb56f457339f3f7f06a07b827ad0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Exploring-Finetuned-Audio-LLM-on-Heart-Murmur-Features"><a href="#Exploring-Finetuned-Audio-LLM-on-Heart-Murmur-Features" class="headerlink" title="Exploring Finetuned Audio-LLM on Heart Murmur Features"></a>Exploring Finetuned Audio-LLM on Heart Murmur Features</h2><p><strong>Authors:Adrian Florea, Xilin Jiang, Nima Mesgarani, Xiaofan Jiang</strong></p>
<p>Large language models (LLMs) for audio have excelled in recognizing and analyzing human speech, music, and environmental sounds. However, their potential for understanding other types of sounds, particularly biomedical sounds, remains largely underexplored despite significant scientific interest. In this study, we focus on diagnosing cardiovascular diseases using phonocardiograms, i.e., heart sounds. Most existing deep neural network (DNN) paradigms are restricted to heart murmur classification (healthy vs unhealthy) and do not predict other acoustic features of the murmur such as timing, grading, harshness, pitch, and quality, which are important in helping physicians diagnose the underlying heart conditions. We propose to finetune an audio LLM, Qwen2-Audio, on the PhysioNet CirCor DigiScope phonocardiogram (PCG) dataset and evaluate its performance in classifying 11 expert-labeled murmur features. Additionally, we aim to achieve more noise-robust and generalizable system by exploring a preprocessing segmentation algorithm using an audio representation model, SSAMBA. Our results indicate that the LLM-based model outperforms state-of-the-art methods in 8 of the 11 features and performs comparably in the remaining 3. Moreover, the LLM successfully classifies long-tail murmur features with limited training data, a task that all previous methods have failed to classify. These findings underscore the potential of audio LLMs as assistants to human cardiologists in enhancing heart disease diagnosis. </p>
<blockquote>
<p>大型语言模型（LLM）在音频领域已经能够出色地识别和解析人类语音、音乐和环境声音。然而，尽管科学界对此有浓厚的兴趣，它们对于理解其他类型的声音，尤其是生物医学声音，其潜力仍被大大低估。在这项研究中，我们专注于使用心音图（即心脏声音）来诊断心血管疾病。现有的大多数深度神经网络（DNN）模式仅限于心脏杂音分类（健康与否），无法预测杂音的其他声学特征，如时间、等级、严厉程度、音高和音质等，这些特征对于帮助医生诊断潜在的心脏状况非常重要。我们提议对音频LLM模型Qwen2-Audio进行微调，并在PhysioNet CirCor DigiScope心音图数据集上评估其性能，以分类专家标注的11种杂音特征。此外，我们还希望通过使用音频表示模型的预处理分割算法SSAMBA，建立一个更加稳健且通用的系统。我们的结果表明，基于LLM的模型在其中的八个特征中表现出优于最新方法的效果，并在其余三个特征中表现相当。此外，LLM能够成功地对长尾杂音特征进行分类，这些特征以前的方法都无法处理。这些发现突显了音频LLM作为人类心脏病学家的辅助工具在增强心脏病诊断方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13884v1">PDF</a> 5 pages, 1 figure, and 3 tables. Submitted to IEEE&#x2F;ACM Conference on   Connected Health: Applications, Systems , and Engineering Technologies</p>
<p><strong>摘要</strong></p>
<p>大型语言模型（LLM）在音频领域已展现出对人脸、音乐和自然环境的优异识别与分析能力。尽管生物医学领域存在对该技术探索的兴趣，但该模型理解其它类型声音如医疗领域生物医学声音的潜力仍未得到广泛研究。本研究关注心脏病诊断的语音图谱领域。多数现有神经网络框架局限于心脏杂音的分类（健康与不健康），且未对其他声学特性如时序性、分级度、猛烈程度等进行预测分析。而这些因素对医生的准确诊断起到重要作用。本研究的首要目的是针对一个名为Qwen2的大型音频语言模型进行优化处理并使其能够符合语音学图像的要求。随后通过一项包含心血管疾病心脏杂音专业标签特征的实验对其进行测试。我们的实验设计目标是创建更为稳健和普遍适用的系统模型，在此过程中采用一种基于音频表现模型的预处理分割算法——SSAMBA。研究结果显示，基于大型语言模型的模型在其中的八项特征上优于当前主流方法，并在其余三项特征上表现相当。更重要的是，大型语言模型在样本数据不足的情况下成功预测了杂音的特征。这表明大型语言模型具有作为心脏病诊断的辅助工具进行应用的潜力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>大型语言模型在音频领域具有广泛的应用潜力，特别是在识别和分析人类语音、音乐和环境声音方面表现出色。</li>
<li>在心脏病诊断中，LLM模型在分类心脏杂音特征方面展现出显著优势，能够预测多种声学特征，如时序性、分级度等。</li>
<li>Qwen2等大型音频语言模型的训练数据集需特定化处理以匹配语音图像集要求以提升模型的适用性。 </li>
<li>相比传统的神经网络框架，大型语言模型可以在缺乏足够的样本数据时仍然展现出稳定的预测能力。这意味着即使在医疗资源不足的情境中，该模型仍然有应用价值。</li>
<li>利用音频表现模型如SSAMBA的预处理分割算法可进一步提升系统的噪声稳健性和普遍性。这一做法可以增强模型处理实际环境中可能出现的不确定性因素的能力。</li>
<li>本研究中提出的大型语言模型可作为一个有力的工具协助心脏病医生进行准确诊断，并有助于改进心脏病诊断和治疗流程的质量。这可能使得医生的诊断更为高效，同时为患者在治疗中节省了时间和资源。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13884">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6f8b6ddbf11cec16873f6f2fa25c950a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb00e99b8c59949eb4fc81fa49f6331d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56c52d31cdbf7d4de1fc9e71ac62e7c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee2fb97a62a470f9eddb6a084f39947.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1bb8140f02f89ac9142c260a937b8402.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Predicting-Compact-Phrasal-Rewrites-with-Large-Language-Models-for-ASR-Post-Editing"><a href="#Predicting-Compact-Phrasal-Rewrites-with-Large-Language-Models-for-ASR-Post-Editing" class="headerlink" title="Predicting Compact Phrasal Rewrites with Large Language Models for ASR   Post Editing"></a>Predicting Compact Phrasal Rewrites with Large Language Models for ASR   Post Editing</h2><p><strong>Authors:Hao Zhang, Felix Stahlberg, Shankar Kumar</strong></p>
<p>Large Language Models (LLMs) excel at rewriting tasks such as text style transfer and grammatical error correction. While there is considerable overlap between the inputs and outputs in these tasks, the decoding cost still increases with output length, regardless of the amount of overlap. By leveraging the overlap between the input and the output, Kaneko and Okazaki (2023) proposed model-agnostic edit span representations to compress the rewrites to save computation. They reported an output length reduction rate of nearly 80% with minimal accuracy impact in four rewriting tasks. In this paper, we propose alternative edit phrase representations inspired by phrase-based statistical machine translation. We systematically compare our phrasal representations with their span representations. We apply the LLM rewriting model to the task of Automatic Speech Recognition (ASR) post editing and show that our target-phrase-only edit representation has the best efficiency-accuracy trade-off. On the LibriSpeech test set, our method closes 50-60% of the WER gap between the edit span model and the full rewrite model while losing only 10-20% of the length reduction rate of the edit span model. </p>
<blockquote>
<p>大型语言模型（LLM）擅长文本重写任务，如文本风格转换和语法错误修正。虽然这些任务的输入和输出之间存在大量重叠，但解码成本仍会随着输出的增长而增加，无论重叠程度如何。Kaneko和Okazaki（2023年）利用输入和输出之间的重叠，提出了与模型无关的编辑跨度表示法，以压缩重写内容，从而节省计算。他们报告称，在四项重写任务中，输出长度缩减率接近80%，且对精度的影响微乎其微。在本文中，我们受到基于短语的统计机器翻译的启发，提出了另一种编辑短语表示法。我们系统地比较了我们的短语表示法与他们的跨度表示法。我们将LLM重写模型应用于自动语音识别（ASR）的后编辑任务，并证明我们的目标短语仅编辑表示法在效率与准确性方面达到了最佳平衡。在LibriSpeech测试集上，我们的方法缩小了编辑跨度模型和全重写模型之间的词错误率（WER）差距的50-60%，同时只失去了编辑跨度模型的长度缩减率的10-20%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13831v1">PDF</a> accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）擅长文本改写任务，如文本风格转换和语法错误修正。尽管输入和输出之间存在大量重叠，解码成本仍随输出长度的增加而增加。Kaneko和Okazaki（2023）提出模型无关的编辑跨度表示法，利用输入和输出之间的重叠来压缩重写内容，以节省计算。他们报告称，在四项重写任务中，输出长度缩减率接近80%，且准确性影响极小。本文受基于短语统计机器翻译的启发，提出替代的编辑短语表示法。我们系统地比较了基于短语的表示法与基于跨度的表示法。我们将LLM重写模型应用于自动语音识别（ASR）的后编辑任务，并发现目标短语仅编辑表示法具有最佳的效率-准确性权衡。在LibriSpeech测试集上，我们的方法缩小了编辑跨度模型和全重写模型之间约50-60%的词错误率差距，同时仅损失了编辑跨度模型的10-20%长度缩减率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在文本改写任务上表现出色，如文本风格转换和语法错误修正。</li>
<li>解码成本随输出长度增加而增加，即使输入和输出存在大量重叠。</li>
<li>Kaneko和Okazaki（2023）提出了模型无关的编辑跨度表示法，利用输入和输出的重叠来压缩重写内容。</li>
<li>输出长度缩减率接近80%，同时准确性影响较小。</li>
<li>本文受短语统计机器翻译的启发，提出了编辑短语表示法。</li>
<li>在ASR后编辑任务中，目标短语仅编辑表示法具有最佳效率-准确性权衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13831">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-299a9baf49c5ae4ddb79f145d25d5ad0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7418ca64b1abcded84714dfc256bf146.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b966b0707850d52930aaa7a75a59d64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b20ad09238d81319cc09492595c4334.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-632b2ec62cf0458bed9cbbcc186163a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e60a33342c813d6e249d05a7d73a8ed3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLMs-for-Governance-with-Human-Oversight-Evaluating-and-Aligning-LLMs-on-Expert-Classification-of-Climate-Misinformation-for-Detecting-False-or-Misleading-Claims-about-Climate-Change"><a href="#Enhancing-LLMs-for-Governance-with-Human-Oversight-Evaluating-and-Aligning-LLMs-on-Expert-Classification-of-Climate-Misinformation-for-Detecting-False-or-Misleading-Claims-about-Climate-Change" class="headerlink" title="Enhancing LLMs for Governance with Human Oversight: Evaluating and   Aligning LLMs on Expert Classification of Climate Misinformation for   Detecting False or Misleading Claims about Climate Change"></a>Enhancing LLMs for Governance with Human Oversight: Evaluating and   Aligning LLMs on Expert Classification of Climate Misinformation for   Detecting False or Misleading Claims about Climate Change</h2><p><strong>Authors:Mowafak Allaham, Ayse D. Lokmanoglu, Sol P. Hart, Erik C. Nisbet</strong></p>
<p>Climate misinformation is a problem that has the potential to be substantially aggravated by the development of Large Language Models (LLMs). In this study we evaluate the potential for LLMs to be part of the solution for mitigating online dis&#x2F;misinformation rather than the problem. Employing a public expert annotated dataset and a curated sample of social media content we evaluate the performance of proprietary vs. open source LLMs on climate misinformation classification task, comparing them to existing climate-focused computer-assisted tools and expert assessments. Results show (1) state-of-the-art (SOTA) open-source models substantially under-perform in classifying climate misinformation compared to proprietary models, (2) existing climate-focused computer-assisted tools leveraging expert-annotated datasets continues to outperform many of proprietary models, including GPT-4o, and (3) demonstrate the efficacy and generalizability of fine-tuning GPT-3.5-turbo on expert annotated dataset in classifying claims about climate change at the equivalency of climate change experts with over 20 years of experience in climate communication. These findings highlight 1) the importance of incorporating human-oversight, such as incorporating expert-annotated datasets in training LLMs, for governance tasks that require subject-matter expertise like classifying climate misinformation, and 2) the potential for LLMs in facilitating civil society organizations to engage in various governance tasks such as classifying false or misleading claims in domains beyond climate change such as politics and health science. </p>
<blockquote>
<p>关于气候误信息的处理是一个可能会因大型语言模型（LLM）的发展而进一步加剧的问题。在这项研究中，我们评估了LLM作为解决方案的一部分，以减轻在线错误信息的程度，而非加剧问题。我们采用公共专家注释的数据集和社交媒体内容的精选样本，评估专有与开源LLM在气候错误信息分类任务上的表现，并将其与现有的专注于气候的计算机辅助工具和专家评估进行比较。结果表明，（1）最先进的开源模型在气候误信息的分类方面显著落后于专有模型；（2）现有的以气候为重点的计算机辅助工具，利用专家注释的数据集继续超越许多专有模型，包括GPT-4o；（3）证明了在专家注释的数据集上微调GPT-3.5 turbo在气候变化主张分类方面的有效性和可推广性，相当于拥有超过二十年气候传播经验的专家水平。这些发现强调了（1）融入人类监督的重要性，例如在训练LLM时融入专家注释的数据集，用于需要专业知识的治理任务如气候信息错误分类；（2）LLM的潜力在于促进公民社会组织参与各种治理任务，如政治和生命科学等领域虚假或误导性主张的分类。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13802v1">PDF</a> Accepted to the AI Governance Workshop at AAAI 2025</p>
<p><strong>Summary</strong>：<br>大型语言模型（LLM）的发展有可能加剧气候谣言问题。本研究评估LLM在缓解在线错误信息方面的作用，而不是加剧问题。通过公共专家注释数据集和社交媒体内容的样本，我们比较了专有与开源LLM在气候谣言分类任务上的性能，以及与现有的气候相关计算机辅助工具和专家评估相比。研究结果表明，先进的开源模型在分类气候谣言方面显著落后于专有模型；现有的气候相关计算机辅助工具仍然优于许多专有模型，包括GPT-4o；精细调整GPT-3.5-turbo在专家注释数据集上可有效且普遍地分类气候变化主张。这些发现强调了融入人类监督（如专家注释数据集）以训练LLM的重要性，特别是在需要专业知识的任务中如分类气候谣言；以及LLM在帮助民间社会组织参与各种治理任务（如分类气候以外领域的虚假或误导性主张）的潜力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>LLM的发展有可能加剧气候谣言问题。</li>
<li>本研究评估LLM在缓解在线错误信息方面的作用。</li>
<li>专有LLM模型在气候谣言分类任务上性能优于开源模型。</li>
<li>现有的气候相关计算机辅助工具表现优于许多专有LLM模型。</li>
<li>精细调整GPT-3.5-turbo在专家注释数据集上可有效分类气候变化主张。</li>
<li>融入人类监督（如专家注释数据集）对于训练LLM至关重要，特别是在需要专业知识的任务中。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13802">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-fbcb4ea7f53c7262f8731852d125e92f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81d830b64e1ecbdaa28ea6f8dbac39a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83fe4a0fd27cd4ade1d39977a927b0e7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Tune-In-Act-Up-Exploring-the-Impact-of-Audio-Modality-Specific-Edits-on-Large-Audio-Language-Models-in-Jailbreak"><a href="#Tune-In-Act-Up-Exploring-the-Impact-of-Audio-Modality-Specific-Edits-on-Large-Audio-Language-Models-in-Jailbreak" class="headerlink" title="Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits   on Large Audio Language Models in Jailbreak"></a>Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits   on Large Audio Language Models in Jailbreak</h2><p><strong>Authors:Erjia Xiao, Hao Cheng, Jing Shao, Jinhao Duan, Kaidi Xu, Le Yang, Jindong Gu, Renjing Xu</strong></p>
<p>Large Language Models (LLMs) demonstrate remarkable zero-shot performance across various natural language processing tasks. The integration of multimodal encoders extends their capabilities, enabling the development of Multimodal Large Language Models that process vision, audio, and text. However, these capabilities also raise significant security concerns, as these models can be manipulated to generate harmful or inappropriate content through jailbreak. While extensive research explores the impact of modality-specific input edits on text-based LLMs and Large Vision-Language Models in jailbreak, the effects of audio-specific edits on Large Audio-Language Models (LALMs) remain underexplored. Hence, this paper addresses this gap by investigating how audio-specific edits influence LALMs inference regarding jailbreak. We introduce the Audio Editing Toolbox (AET), which enables audio-modality edits such as tone adjustment, word emphasis, and noise injection, and the Edited Audio Datasets (EADs), a comprehensive audio jailbreak benchmark. We also conduct extensive evaluations of state-of-the-art LALMs to assess their robustness under different audio edits. This work lays the groundwork for future explorations on audio-modality interactions in LALMs security. </p>
<blockquote>
<p>大型语言模型（LLM）在各种自然语言处理任务中表现出显著的零样本性能。多模态编码器的集成扩展了它们的功能，推动了能够处理视觉、音频和文本的多模态大型语言模型的发展。然而，这些功能也引发了严重的安全担忧，因为这些模型可以通过越狱生成有害或不当内容而受到操纵。虽然大量研究探讨了模态特定输入编辑对基于文本的大型语言模型和大型视觉语言模型的影响，但针对特定音频编辑对大型音频语言模型（LALM）的影响研究仍然不足。因此，本文通过研究特定音频编辑如何影响LALM关于越狱的推理来填补这一空白。我们引入了音频编辑工具箱（AET），它支持音频模态编辑，如音调调整、单词强调和噪声注入，以及编辑后的音频数据集（EAD），这是一个全面的音频越狱基准测试。我们还对最先进的LALM进行了广泛评估，以测试它们在不同的音频编辑下的稳健性。这项工作为未来在LALM安全性方面探索音频模态交互奠定了基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13772v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）展现出跨多种自然语言处理任务的出色零样本性能。通过融入多模态编码器，进一步开发出能够处理视觉、音频和文本的多模态大型语言模型。然而，这些能力也引发了严重的安全担忧，因为模型可能会被操纵以生成有害或不当内容。尽管已有大量研究探讨了模态特定输入编辑对文本型LLM和大型视觉语言模型的影响，但针对音频特定编辑对大型音频语言模型（LALM）的影响的研究仍显不足。本文旨在填补这一空白，研究音频特定编辑如何影响LALM在越狱场景中的推断。引入了音频编辑工具箱（AET）和编辑音频数据集（EADs），并对当前先进的LALM进行了广泛评估，以测试其在不同音频编辑下的稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型展现出跨多种自然语言处理任务的出色零样本性能。</li>
<li>多模态编码器的融入扩展了大型语言模型的能力，包括处理视觉、音频和文本。</li>
<li>大型语言模型存在被操纵生成有害或不当内容的安全风险。</li>
<li>对音频特定编辑影响大型音频语言模型（LALM）的研究仍然不足。</li>
<li>引入音频编辑工具箱（AET）和编辑音频数据集（EADs）为研究提供工具和数据支持。</li>
<li>广泛评估显示，当前先进的LALM在不同音频编辑下的稳健性有待提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13772">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-436e0eff436bb6cfdd3e2e98f5e1e50a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb2cab7ab51b7a127f6c378b5a208f42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f39647dfb72b49f89747a0c0542f7930.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UGMathBench-A-Diverse-and-Dynamic-Benchmark-for-Undergraduate-Level-Mathematical-Reasoning-with-Large-Language-Models"><a href="#UGMathBench-A-Diverse-and-Dynamic-Benchmark-for-Undergraduate-Level-Mathematical-Reasoning-with-Large-Language-Models" class="headerlink" title="UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level   Mathematical Reasoning with Large Language Models"></a>UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level   Mathematical Reasoning with Large Language Models</h2><p><strong>Authors:Xin Xu, Jiaxin Zhang, Tianhao Chen, Zitong Chao, Jishan Hu, Can Yang</strong></p>
<p>Large Language Models (LLMs) have made significant strides in mathematical reasoning, underscoring the need for a comprehensive and fair evaluation of their capabilities. However, existing benchmarks often fall short, either lacking extensive coverage of undergraduate-level mathematical problems or probably suffering from test-set contamination. To address these issues, we introduce UGMathBench, a diverse and dynamic benchmark specifically designed for evaluating undergraduate-level mathematical reasoning with LLMs. UGMathBench comprises 5,062 problems across 16 subjects and 111 topics, featuring 10 distinct answer types. Each problem includes three randomized versions, with additional versions planned for release as leading open-source LLMs become saturated in UGMathBench. Furthermore, we propose two key metrics: effective accuracy (EAcc), which measures the percentage of correctly solved problems across all three versions, and reasoning gap ($\Delta$), which assesses reasoning robustness by calculating the difference between the average accuracy across all versions and EAcc. Our extensive evaluation of 23 leading LLMs reveals that the highest EAcc achieved is 56.3% by OpenAI-o1-mini, with large $\Delta$ values observed across different models. This highlights the need for future research aimed at developing “large reasoning models” with high EAcc and $\Delta &#x3D; 0$. We anticipate that the release of UGMathBench, along with its detailed evaluation codes, will serve as a valuable resource to advance the development of LLMs in solving mathematical problems. </p>
<blockquote>
<p>大型语言模型（LLM）在数学推理方面取得了显著进展，这强调了对它们的能力进行全面公平评估的必要性。然而，现有的基准测试通常存在不足，要么没有广泛覆盖本科数学问题和可能存在的测试集污染。为了解决这些问题，我们推出了UGMathBench，这是一个专门设计用于评估本科数学推理的大型语言模型（LLM）的多元动态基准测试。UGMathBench包含5,062个跨越16个学科和111个主题的问题，包含10种不同的答案类型。每个问题包括三个随机版本，随着领先的开源大型语言模型在UGMathBench中趋于饱和，我们还计划发布更多版本。此外，我们提出了两个关键指标：有效准确率（EAcc），用于衡量所有三个版本中正确解决的问题的百分比；推理差距（Δ），通过计算所有版本平均准确率和EAcc之间的差异来评估推理稳健性。我们对23个领先的大型语言模型的广泛评估表明，OpenAI-o1-mini的最高有效准确率达到了56.3%，不同模型之间观察到较大的Δ值。这强调了未来研究需要致力于开发具有高有效准确率和Δ&#x3D;0的“大型推理模型”。我们预期，UGMathBench及其详细的评估代码的发布将作为推动大型语言模型解决数学问题的重要资源。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13766v1">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>LLMs在数学推理方面取得显著进展，但现有评估标准存在不足。为此，提出UGMathBench评估平台，涵盖5062个问题和多种题型，旨在全面公平地评估LLM解决本科数学问题的能力。引入有效准确度（EAcc）和推理差距（Δ）两个关键指标，对现有23个LLM模型进行评估，发现存在提升空间。期待UGMathBench及其评估代码能促进LLM在数学问题解答方面的进一步发展。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMs在数学推理能力上取得显著进步，需要全面公平的评估标准。</li>
<li>UGMathBench是一个专为评估LLM解决本科数学问题能力的平台，包含多样化和动态的问题库。</li>
<li>引入有效准确度（EAcc）和推理差距（Δ）两个关键指标来评估LLM的推理能力。</li>
<li>UGMathBench对现有23个LLM模型进行了评估，发现最高有效准确度为OpenAI-o1-mini的56.3%，存在提升空间。</li>
<li>需要进一步研究和开发具有高水平有效准确度和零推理差距的“大型推理模型”。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13766">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2d602c434684a466980ffeca8b9fdb13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16337136c74c57cf4736eaf96ee114d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d443191e51ae66d1a979644cf6279b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1f0e37c6af9aff603b3ab498a085240.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RAMQA-A-Unified-Framework-for-Retrieval-Augmented-Multi-Modal-Question-Answering"><a href="#RAMQA-A-Unified-Framework-for-Retrieval-Augmented-Multi-Modal-Question-Answering" class="headerlink" title="RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question   Answering"></a>RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question   Answering</h2><p><strong>Authors:Yang Bai, Christan Earl Grant, Daisy Zhe Wang</strong></p>
<p>Multi-modal retrieval-augmented Question Answering (MRAQA), integrating text and images, has gained significant attention in information retrieval (IR) and natural language processing (NLP). Traditional ranking methods rely on small encoder-based language models, which are incompatible with modern decoder-based generative large language models (LLMs) that have advanced various NLP tasks. To bridge this gap, we propose RAMQA, a unified framework combining learning-to-rank methods with generative permutation-enhanced ranking techniques. We first train a pointwise multi-modal ranker using LLaVA as the backbone. Then, we apply instruction tuning to train a LLaMA model for re-ranking the top-k documents using an innovative autoregressive multi-task learning approach. Our generative ranking model generates re-ranked document IDs and specific answers from document candidates in various permutations. Experiments on two MRAQA benchmarks, WebQA and MultiModalQA, show significant improvements over strong baselines, highlighting the effectiveness of our approach. Code and data are available at: <a target="_blank" rel="noopener" href="https://github.com/TonyBY/RAMQA">https://github.com/TonyBY/RAMQA</a> </p>
<blockquote>
<p>多模态检索增强问答（MRAQA）融合了文本和图像，在信息检索（IR）和自然语言处理（NLP）领域引起了广泛关注。传统排名方法依赖于基于编码器的小型语言模型，这些模型与现代基于解码器的生成式大型语言模型（LLM）不兼容，而LLM已在各种NLP任务中取得了进展。为了弥补这一差距，我们提出了RAMQA，这是一个结合了学习排名方法和生成置换增强排名技术的统一框架。我们首先使用LLaVA作为骨干训练点式多模态排名器。然后，我们应用指令调整来训练一个LLaMA模型，使用创新的自回归多任务学习方法对前k个文档进行重新排名。我们的生成排名模型从文档候选者中生成重新排名的文档ID和特定答案，这些文档以各种排列方式呈现。在WebQA和多模态QA两个MRAQA基准测试上的实验表明，与强大的基线相比，我们的方法有明显的改进，突出了我们的方法的有效性。相关代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/TonyBY/RAMQA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/TonyBY/RAMQA找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13297v1">PDF</a> Accepted by NAACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>多模态检索增强问答（MRAQA）融合了文本和图像，在信息检索（IR）和自然语言处理（NLP）领域引起了广泛关注。传统排序方法依赖于小型编码器基础的语言模型，与现代解码器基础的大型语言模型（LLM）不兼容。本文提出RAMQA框架，结合学习排序方法与生成排列增强排序技术。通过以LLaVA为骨干进行点对多模态排序器训练，再应用指令微调训练LLaMA模型，以创新自回归多任务学习方式进行前k个文档重新排序。生成式排名模型能够生成重新排序的文档ID和来自文档候选者的特定答案，在各种排列中表现优异。在WebQA和MultiModalQA两个MRAQA基准测试上，相较于强基线有显著改进，凸显了该方法的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRAQA融合了文本和图像，在信息检索和自然语言处理中受到重视。</li>
<li>传统排序方法依赖小型编码器语言模型，与大型语言模型不兼容。</li>
<li>RAMQA框架结合了学习排序和生成排列增强排序技术。</li>
<li>使用LLaVA作为骨干进行点对多模态排序器训练。</li>
<li>通过指令微调训练LLaMA模型进行文档重新排序。</li>
<li>生成式排名模型能生成重新排序的文档ID和特定答案。</li>
<li>在MRAQA基准测试上，RAMQA相比基线有显著改善。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13297">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b77a235470449d995af166d6bf2598d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-191ebcf1a1a0be796f0bc9d757ba8f98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c28009cf0b5c7ab370083e4512494e2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33e1bd77b9104d9e6c77d3f46c0913b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91dc97301770f3fa8eb63fc7d90f5975.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ARTEMIS-DA-An-Advanced-Reasoning-and-Transformation-Engine-for-Multi-Step-Insight-Synthesis-in-Data-Analytics"><a href="#ARTEMIS-DA-An-Advanced-Reasoning-and-Transformation-Engine-for-Multi-Step-Insight-Synthesis-in-Data-Analytics" class="headerlink" title="ARTEMIS-DA: An Advanced Reasoning and Transformation Engine for   Multi-Step Insight Synthesis in Data Analytics"></a>ARTEMIS-DA: An Advanced Reasoning and Transformation Engine for   Multi-Step Insight Synthesis in Data Analytics</h2><p><strong>Authors:Atin Sakkeer Hussain</strong></p>
<p>This paper presents the Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework designed to augment Large Language Models (LLMs) for solving complex, multi-step data analytics tasks. ARTEMIS-DA integrates three core components: the Planner, which dissects complex user queries into structured, sequential instructions encompassing data preprocessing, transformation, predictive modeling, and visualization; the Coder, which dynamically generates and executes Python code to implement these instructions; and the Grapher, which interprets generated visualizations to derive actionable insights. By orchestrating the collaboration between these components, ARTEMIS-DA effectively manages sophisticated analytical workflows involving advanced reasoning, multi-step transformations, and synthesis across diverse data modalities. The framework achieves state-of-the-art (SOTA) performance on benchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to tackle intricate analytical tasks with precision and adaptability. By combining the reasoning capabilities of LLMs with automated code generation and execution and visual analysis, ARTEMIS-DA offers a robust, scalable solution for multi-step insight synthesis, addressing a wide range of challenges in data analytics. </p>
<blockquote>
<p>本文介绍了用于数据解析中的多步骤见解合成的先进推理与转换引擎（ARTEMIS-DA），这是一种旨在增强大型语言模型（LLM）以解决复杂的多步骤数据分析任务的新型框架。ARTEMIS-DA集成了三个核心组件：Planner，它将复杂的用户查询划分为结构化、顺序指令，包括数据预处理、转换、预测建模和可视化；Coder，它动态生成并执行Python代码以执行这些指令；以及Grapher，它解释生成的可视化以获取可操作见解。通过协调这些组件之间的协作，ARTEMIS-DA有效地管理涉及高级推理、多步骤转换和跨不同数据模态的合成的高级分析工作流程。该框架在WikiTableQuestions和TabFact等基准测试上实现了最先进的性能，证明了其精确度和适应性处理复杂分析任务的能力。通过将LLM的推理能力与自动化代码生成和执行以及可视化分析相结合，ARTEMIS-DA为多步骤见解合成提供了稳健且可扩展的解决方案，解决了数据分析中的广泛挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14146v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ARTEMIS-DA框架是一种先进的用于多步骤洞察合成数据分析的高级推理与转换引擎。它通过整合规划器、编码器和绘图器三个核心组件，实现复杂的多步骤数据分析任务。ARTEMIS-DA有效管理高级推理、多步骤转换和跨不同数据模态的合成等复杂分析工作流程，并在WikiTableQuestions和TabFact等基准测试中实现最先进的性能表现。它将大型语言模型的推理能力与自动化代码生成和执行以及可视化分析相结合，为复杂的数据分析任务提供了稳健、可扩展的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ARTEMIS-DA是一个旨在增强大型语言模型（LLMs）解决复杂多步骤数据分析任务的新型框架。</li>
<li>它包含三个核心组件：规划器负责将复杂用户查询分解为结构化顺序指令，编码器动态生成并执行Python代码实现这些指令，而绘图器则解释生成的可视化以获取可操作见解。</li>
<li>ARTEMIS-DA通过协同这些组件，有效管理涉及高级推理、多步骤转换和跨不同数据模态的合成等复杂分析工作流程。</li>
<li>ARTEMIS-DA在WikiTableQuestions和TabFact等基准测试中实现了最先进的性能。</li>
<li>该框架将LLMs的推理能力与自动化代码生成和执行以及可视化分析相结合，为复杂的数据分析任务提供了全面解决方案。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14146">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d7378ffa7e36e57eb66a440c8998e630.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-152cb53ea94c8c505412d75c9c95d10d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddf9f19888890e2735f18e5a265d2623.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98388b1a862e9c32483d1e214e3693f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa422c61b4ec8a0eb6de9b04ffc26d61.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Interactive-Cycle-Model-The-Linkage-Combination-among-Automatic-Speech-Recognition-Large-Language-Models-and-Smart-Glasses"><a href="#Interactive-Cycle-Model-The-Linkage-Combination-among-Automatic-Speech-Recognition-Large-Language-Models-and-Smart-Glasses" class="headerlink" title="Interactive Cycle Model: The Linkage Combination among Automatic Speech   Recognition, Large Language Models and Smart Glasses"></a>Interactive Cycle Model: The Linkage Combination among Automatic Speech   Recognition, Large Language Models and Smart Glasses</h2><p><strong>Authors:Libo Wang</strong></p>
<p>This research proposes the interaction loop model “ASR-LLMs-Smart Glasses”, which model combines automatic speech recognition, large language model and smart glasses to facilitate seamless human-computer interaction. And the methodology of this research involves decomposing the interaction process into different stages and elements. Speech is captured and processed by ASR, then analyzed and interpreted by LLMs. The results are then transmitted to smart glasses for display. The feedback loop is complete when the user interacts with the displayed data. Mathematical formulas are used to quantify the performance of the model that revolves around core evaluation points: accuracy, coherence, and latency during ASR speech-to-text conversion. The research results are provided theoretically to test and evaluate the feasibility and performance of the model. Detailed architectural details and experimental process have been uploaded to Github, the link is:<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git">https://github.com/brucewang123456789/GeniusTrail.git</a>. </p>
<blockquote>
<p>本研究提出了“ASR-LLMs-智能眼镜”交互循环模型，该模型结合了自动语音识别、大型语言模型和智能眼镜，促进了无缝的人机交互。本研究的方法论涉及将交互过程分解成不同的阶段和元素。语音被ASR捕获并处理，然后被LLMs分析和解释。结果随后传输到智能眼镜进行显示。当用户与显示的数据交互时，反馈循环完成。本研究使用数学公式来量化模型性能，围绕核心评估点：ASR语音转文本转换过程中的准确性、连贯性和延迟。研究结果在理论上提供了测试和评估该模型可行性和性能的依据。详细的架构细节和实验过程已经上传到Github，链接为：<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git%E3%80%82">https://github.com/brucewang123456789/GeniusTrail.git。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10362v3">PDF</a> OpenReview submitted. 10 pages of text and 2 figures</p>
<p><strong>Summary</strong></p>
<p>该研究提出了“ASR-LLMs-智能眼镜”交互循环模型，该模型结合了自动语音识别、大型语言模型和智能眼镜，促进了无缝的人机交互。研究方法将交互过程分解成不同的阶段和元素。语音由ASR捕获并处理，然后通过LLMs进行分析和解释。结果传输到智能眼镜进行显示。用户与显示的数据进行交互时，反馈循环完成。围绕核心评估点（准确性、连贯性和ASR语音转文本的延迟），使用数学公式对模型性能进行量化。研究成果已在理论上进行了测试并评估了模型的可行性及性能。详细架构和实验过程已上传至Github。有关链接：<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git">https://github.com/brucewang123456789/GeniusTrail.git</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究提出了ASR-LLMs-智能眼镜交互循环模型，整合自动语音识别、大型语言模型和智能眼镜技术。</li>
<li>模型实现了无缝人机互动，通过分解交互过程为不同阶段和元素来优化交互体验。</li>
<li>语音先由ASR处理，然后通过LLMs进行分析和解释，最后将结果展示在智能眼镜上。</li>
<li>模型性能通过数学公式量化，核心评估标准包括准确性、连贯性和延迟。</li>
<li>该模型已经在理论上得到了测试，验证了其可行性和性能表现。</li>
<li>详细的模型架构和实验过程已经公开在Github上供公众查阅。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10362">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-71c7a8f8e1851c32c5a561985b44222a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Reducing-Reasoning-Costs-The-Path-of-Optimization-for-Chain-of-Thought-via-Sparse-Attention-Mechanism"><a href="#Reducing-Reasoning-Costs-The-Path-of-Optimization-for-Chain-of-Thought-via-Sparse-Attention-Mechanism" class="headerlink" title="Reducing Reasoning Costs: The Path of Optimization for Chain of Thought   via Sparse Attention Mechanism"></a>Reducing Reasoning Costs: The Path of Optimization for Chain of Thought   via Sparse Attention Mechanism</h2><p><strong>Authors:Libo Wang</strong></p>
<p>In order to address the chain of thought in the large language model inference cost surge, this research proposes to use a sparse attention mechanism that only focuses on a few relevant tokens. The researcher constructed a new attention mechanism and used GiantRabbit trained with custom GPTs as an experimental tool. The experiment tested and compared the reasoning time, correctness score and chain of thought length of this model and o1 Preview in solving the linear algebra test questions of MIT OpenCourseWare. The results show that GiantRabbit’s reasoning time and chain of thought length are significantly lower than o1 Preview. It verifies the feasibility of sparse attention mechanism for optimizing chain of thought reasoning. Detailed architectural details and experimental process have been uploaded to Github, the link is:<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git">https://github.com/brucewang123456789/GeniusTrail.git</a>. </p>
<blockquote>
<p>为了应对大型语言模型推理成本飙升中的思维链问题，本研究提出了一种稀疏注意力机制，该机制只关注少数相关令牌。研究者构建了一种新的注意力机制，并使用GiantRabbit作为实验工具，GiantRabbit通过自定义GPT进行了训练。实验测试并比较了该模型与o1 Preview在解决MIT OpenCourseWare的线性代数测试问题时的推理时间、正确率和思维链长度。结果表明，GiantRabbit的推理时间和思维链长度明显低于o1 Preview。这验证了稀疏注意力机制在优化思维链推理方面的可行性。详细的架构细节和实验过程已上传到Github，链接为：<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git%E3%80%82">https://github.com/brucewang123456789/GeniusTrail.git。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09111v5">PDF</a> The main text is 5 pages, totaling 9 pages; 4 figures, 1 table. It   have been submitted to NeurIPS 2024 Workshop MusIML and OpenReview</p>
<p><strong>Summary</strong><br>大型语言模型推理成本激增的问题得到了解决。研究提出了稀疏注意力机制来关注关键内容。通过实验发现，新型注意力机制的推理时间和推理思路长度相较于预览都明显更优，且可行。更具体的方案和实验结果在GitHub中给出。详情请参见GitHub链接（<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git">https://github.com/brucewang123456789/GeniusTrail.git</a>）。简洁地表达出来的文章主要内容如下：Sparse Attention机制提高了大语言模型的推理效率；GiantRabbit模型表现优异；GitHub链接提供了详细信息。</p>
<p><strong>Key Takeaways</strong></p>
<p>以下是基于文本提取的七个关键要点：</p>
<ul>
<li>研究者提出使用稀疏注意力机制来解决大型语言模型推理成本上升的问题。</li>
<li>研究中设计了一种新型注意力机制并通过GiantRabbit模型进行实验验证。</li>
<li>GiantRabbit模型是用定制的GPT训练的，作为实验工具进行推理性能测试。</li>
<li>实验通过对比GiantRabbit模型和o1 Preview在解决MIT OpenCourseWare的线性代数测试问题上的表现来评估模型性能。</li>
<li>实验结果显示GiantRabbit模型的推理时间和推理思路长度均显著低于o1 Preview。</li>
<li>实验验证了稀疏注意力机制在优化推理思路方面的可行性。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.09111">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-15674cde4263b5f2f7678ff5c313d490.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54cc398807739192b8c25968dfca40fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0830d96b5f5c26cbd04d076cd6dbb28e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DART-Denoising-Autoregressive-Transformer-for-Scalable-Text-to-Image-Generation"><a href="#DART-Denoising-Autoregressive-Transformer-for-Scalable-Text-to-Image-Generation" class="headerlink" title="DART: Denoising Autoregressive Transformer for Scalable Text-to-Image   Generation"></a>DART: Denoising Autoregressive Transformer for Scalable Text-to-Image   Generation</h2><p><strong>Authors:Jiatao Gu, Yuyang Wang, Yizhe Zhang, Qihang Zhang, Dinghuai Zhang, Navdeep Jaitly, Josh Susskind, Shuangfei Zhai</strong></p>
<p>Diffusion models have become the dominant approach for visual generation. They are trained by denoising a Markovian process which gradually adds noise to the input. We argue that the Markovian property limits the model’s ability to fully utilize the generation trajectory, leading to inefficiencies during training and inference. In this paper, we propose DART, a transformer-based model that unifies autoregressive (AR) and diffusion within a non-Markovian framework. DART iteratively denoises image patches spatially and spectrally using an AR model that has the same architecture as standard language models. DART does not rely on image quantization, which enables more effective image modeling while maintaining flexibility. Furthermore, DART seamlessly trains with both text and image data in a unified model. Our approach demonstrates competitive performance on class-conditioned and text-to-image generation tasks, offering a scalable, efficient alternative to traditional diffusion models. Through this unified framework, DART sets a new benchmark for scalable, high-quality image synthesis. </p>
<blockquote>
<p>扩散模型已经成为视觉生成的主要方法。它们通过去噪马尔可夫过程进行训练，该过程逐步向输入添加噪声。我们认为，马尔可夫属性限制了模型充分利用生成轨迹的能力，从而在训练和推理过程中造成效率低下。在本文中，我们提出了DART，这是一个基于变压器的模型，它在非马尔可夫框架内统一了自回归（AR）和扩散。DART使用与标准语言模型相同的架构的AR模型，在空间上光谱上迭代去噪图像块。DART不依赖于图像量化，能够在保持灵活性的同时实现更有效的图像建模。此外，DART可以在统一模型中无缝地训练文本和图像数据。我们的方法在类条件生成和文本到图像生成任务上表现出有竞争力的性能，为传统扩散模型提供了可扩展且高效的替代方案。通过这一统一框架，DART为可扩展的高质量图像合成设定了新的基准。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.08159v2">PDF</a> Accepted by ICLR2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种基于非马尔可夫框架的统一扩散模型DART，结合了自回归（AR）和扩散模型。DART通过空间上和光谱上的图像块迭代去噪，使用与标准语言模型相同的架构的自回归模型。它不需要依赖图像量化，能在保持灵活性的同时更有效地进行图像建模。此外，DART可以在统一模型中无缝地结合文本和图像数据进行训练。该方法在类条件生成和文本到图像生成任务上表现出竞争力，成为可扩展、高效的传统扩散模型的替代品，为可伸缩高质量图像合成设定了新的基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型已成为视觉生成的主导方法，但其Markovian属性限制了模型在生成轨迹上的完全利用能力，导致训练和推断效率低下。</li>
<li>DART是一个基于非马尔可夫框架的统一模型，结合了自回归（AR）和扩散模型。</li>
<li>DART通过空间上和光谱上的图像块迭代去噪，采用与标准语言模型相同的架构的自回归模型。</li>
<li>DART不需要依赖图像量化，可以更有效地进行图像建模并保持灵活性。</li>
<li>DART能在统一模型中无缝结合文本和图像数据进行训练。</li>
<li>DART在类条件生成和文本到图像生成任务上具有竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.08159">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ac9597d65100b353273108723e57a6e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7716e9df02bc67a94a0995a5d1ad7eaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5793fa0f5d23246015949f10e43e2cfe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-07de6da17894d400e38ee65f17f4370e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="NESTFUL-A-Benchmark-for-Evaluating-LLMs-on-Nested-Sequences-of-API-Calls"><a href="#NESTFUL-A-Benchmark-for-Evaluating-LLMs-on-Nested-Sequences-of-API-Calls" class="headerlink" title="NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API   Calls"></a>NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API   Calls</h2><p><strong>Authors:Kinjal Basu, Ibrahim Abdelaziz, Kiran Kate, Mayank Agarwal, Maxwell Crouse, Yara Rizk, Kelsey Bradford, Asim Munawar, Sadhana Kumaravel, Saurabh Goyal, Xin Wang, Luis A. Lastras, Pavan Kapanipathi</strong></p>
<p>The resurgence of autonomous agents built using large language models (LLMs) to solve complex real-world tasks has brought increased focus on LLMs’ fundamental ability of tool or function calling. At the core of these agents, an LLM must plan, execute, and respond using external tools, APIs, and custom functions. Research on tool calling has gathered momentum, but evaluation benchmarks and datasets representing the complexity of the tasks have lagged behind. In this work, we focus on one such complexity, nested sequencing, with the goal of extending existing benchmarks and evaluation. Specifically, we present NESTFUL, a benchmark to evaluate LLMs on nested sequences of API calls, i.e., sequences where the output of one API call is passed as input to a subsequent call. NESTFUL contains 1800+ nested sequences where all the function calls are executable. Experimental results on multiple models and settings show that the best-performing model on the dataset has a full sequence match accuracy of 25% and win-rate of 34% necessitating a large scope for improvement in the nested sequencing aspect of function calling. Our analysis of these results provides possible future research directions for the community, in addition to a benchmark to track progress. We have released the NESTFUL dataset under the Apache 2.0 license at <a target="_blank" rel="noopener" href="https://github.com/IBM/NESTFUL">https://github.com/IBM/NESTFUL</a>. </p>
<blockquote>
<p>使用大型语言模型（LLM）构建的自主代理人的复苏，用于解决复杂的现实世界任务，这增加了对LLM基本工具或功能调用能力的关注。这些代理人的核心，LLM必须利用外部工具、API和自定义函数进行规划、执行和响应。关于工具调用的研究已经蓄势待发，但是代表任务复杂度的评估基准和数据集却滞后了。在这项工作中，我们专注于其中一种复杂性，即嵌套序列，旨在扩展现有的基准评估和数据集。具体来说，我们提出了NESTFUL基准评估，用于评估LLM在API调用的嵌套序列方面的表现，即一个API调用的输出作为后续调用的输入。NESTFUL包含超过1800个嵌套序列，其中所有函数调用都是可执行的。在多模型和设置上的实验结果表明，该数据集上表现最佳的模型的完整序列匹配准确率为25%，胜率为34%，说明在函数调用中的嵌套序列方面还有很大的改进空间。我们对这些结果的分析为社区提供了可能的未来研究方向，此外还提供了一个基准评估来跟踪进度。我们已在Apache 2.0许可证下发布了NESTFUL数据集：<a target="_blank" rel="noopener" href="https://github.com/IBM/NESTFUL">https://github.com/IBM/NESTFUL</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03797v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文关注大型语言模型（LLM）在解决复杂现实任务中的工具调用能力。针对现有评估基准和数据集在任务复杂性方面的不足，提出了一种新的评估基准NESTFUL，用于评估LLM在处理嵌套序列API调用方面的性能。实验结果表明，现有模型在这方面仍有待提高，为未来研究提供了方向。同时，本文也分享了NESTFUL数据集以便跟踪进展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在解决复杂现实任务中需要调用工具、API和自定义函数。</li>
<li>NESTFUL是一个用于评估LLM在处理嵌套序列API调用性能的基准测试。</li>
<li>NESTFUL包含1800多个可执行的嵌套序列。</li>
<li>目前最佳模型在NESTFUL数据集上的全序列匹配准确率为25%，胜率为34%，表明在这方面仍有很大改进空间。</li>
<li>本文分析了实验结果，为未来的研究提供了方向。</li>
<li>NESTFUL数据集已经发布，可供研究社区使用以跟踪进展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.03797">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fb602a49fb81a27e0180a9425d98354a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a7e3763ac1409c3f82bbb8122b6cf7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c59bef957094a63d121df04278747f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d8fa46acdc16e77da5577372a1b41b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bd1aadd19ea9997ff3658ac5b8c37d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f532d1fa304501c548bd692fef25a7f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="PhishAgent-A-Robust-Multimodal-Agent-for-Phishing-Webpage-Detection"><a href="#PhishAgent-A-Robust-Multimodal-Agent-for-Phishing-Webpage-Detection" class="headerlink" title="PhishAgent: A Robust Multimodal Agent for Phishing Webpage Detection"></a>PhishAgent: A Robust Multimodal Agent for Phishing Webpage Detection</h2><p><strong>Authors:Tri Cao, Chengyu Huang, Yuexin Li, Huilin Wang, Amy He, Nay Oo, Bryan Hooi</strong></p>
<p>Phishing attacks are a major threat to online security, exploiting user vulnerabilities to steal sensitive information. Various methods have been developed to counteract phishing, each with varying levels of accuracy, but they also face notable limitations. In this study, we introduce PhishAgent, a multimodal agent that combines a wide range of tools, integrating both online and offline knowledge bases with Multimodal Large Language Models (MLLMs). This combination leads to broader brand coverage, which enhances brand recognition and recall. Furthermore, we propose a multimodal information retrieval framework designed to extract the relevant top k items from offline knowledge bases, using available information from a webpage, including logos and HTML. Our empirical results, based on three real-world datasets, demonstrate that the proposed framework significantly enhances detection accuracy and reduces both false positives and false negatives, while maintaining model efficiency. Additionally, PhishAgent shows strong resilience against various types of adversarial attacks. </p>
<blockquote>
<p>网络钓鱼攻击是对网络安全的主要威胁之一，它通过利用用户漏洞来窃取敏感信息。尽管已经开发了各种方法来对抗网络钓鱼攻击，每种方法都有不同级别的准确性，但它们也面临着明显的局限性。本研究介绍了PhishAgent，这是一个多模式代理，它结合了多种工具，融合了在线和离线知识库与多模式大型语言模型（MLLMs）。这种结合导致了更广泛的品牌覆盖，从而提高了品牌识别和回忆。此外，我们提出了一种多模式信息检索框架，旨在从离线知识库中提取与网页上可用的信息相关的前k个项目，包括标志和HTML。我们的基于三个真实数据集的实证结果表明，该框架大大提高了检测准确性，降低了误报和漏报的数量，同时保持了模型的效率。此外，PhishAgent对各种对抗性攻击表现出强大的抵抗力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10738v3">PDF</a> Accepted at AAAI 2025 (Oral)</p>
<p><strong>Summary</strong><br>自动化防护受到网络安全领域各种钓邮攻击的威胁影响深远。此研究引入PhishAgent，结合多种工具和多模态大型语言模型（MLLMs），提高品牌覆盖范围和识别能力，更有效地检测和防护钓邮攻击。实证结果表明，该框架显著提高检测准确性并减少误报和漏报，同时保持模型效率。此外，PhishAgent对抗多种对抗性攻击表现出强大的韧性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Phishing攻击是网络安全的主要威胁，旨在利用用户漏洞窃取敏感信息。</li>
<li>当前反钓邮方法存在局限性，需要更有效的解决方案。</li>
<li>PhishAgent是一个多模态agent，结合在线和离线知识库与多模态大型语言模型（MLLMs）。</li>
<li>结合多种工具增强品牌覆盖范围和识别能力。</li>
<li>多模态信息检索框架从离线知识库中提取最相关的前k项信息。</li>
<li>基于三个真实数据集进行的实证研究证明PhishAgent能提高检测准确性并减少误报和漏报。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10738">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5d49a788c15676d3be3babf720157abc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5d84bebe4ac4a0927ecad97cfdbbc13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88321ef2f6d4e369844bb68772bcfa75.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="RegMix-Data-Mixture-as-Regression-for-Language-Model-Pre-training"><a href="#RegMix-Data-Mixture-as-Regression-for-Language-Model-Pre-training" class="headerlink" title="RegMix: Data Mixture as Regression for Language Model Pre-training"></a>RegMix: Data Mixture as Regression for Language Model Pre-training</h2><p><strong>Authors:Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, Min Lin</strong></p>
<p>The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix trains many small models on diverse data mixtures, uses regression to predict performance of unseen mixtures, and applies the best predicted mixture to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, we train 512 models with 1M parameters for 1B tokens to fit the regression model and predict the best data mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Furthermore, RegMix consistently outperforms human selection in experiments involving models up to 7B models trained on 100B tokens, while matching or exceeding DoReMi using just 10% of the computational resources. Our experiments also show that (1) Data mixtures significantly impact performance; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed; (4) Data mixture effects transcend scaling laws. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sail-sg/regmix">https://github.com/sail-sg/regmix</a>. </p>
<blockquote>
<p>数据混合对于大型语言模型的预训练性能有着显著影响，然而如何确定有效的数据混合仍不明确。我们提出RegMix方法，通过将其制定为回归任务来自动确定高性能的数据混合。RegMix在多种数据混合上训练许多小型模型，使用回归来预测未见过的数据混合的性能，并将最佳的预测数据混合应用于训练具有大量计算资源的大型模型。为了实证验证RegMix，我们训练了512个具有1M参数、训练标记为1B的模型来拟合回归模型并预测最佳数据混合。使用这种数据混合，我们训练了一个具有1B参数的模型，进行了高达25B标记的训练（即比具有其他混合物的其他最佳模型规模大10倍，训练时间为其三倍），并且我们发现该模型性能最佳。此外，RegMix在实验中的表现一直优于人工选择方式，即使在涉及高达拥有被训练标记为高达数十亿的更大模型的实验中也是如此。同时，我们的方法匹配或超过了DoReMi方法所使用的计算资源只有其十分之一。我们的实验还表明：（1）数据混合显著影响性能；（2）相较于被认为高质量的维基百科，网络语料库与下游任务的性能之间存在最强的正向关联；（3）各个领域之间的互动错综复杂常常有悖常理常识因此需要进行像RegMix一样的自动化方式来进行评估；（4）数据混合的影响超越了规模定律。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/sail-sg/regmix%E4%B8%8A%E8%8E%B7%E5%8F%96%E6%9F%A5%E9%98%85%E3%80%82">https://github.com/sail-sg/regmix上获取查阅。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.01492v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了大型语言模型预训练数据混合的重要性及其对模型性能的影响。针对如何确定有效数据混合的问题，提出了一种名为RegMix的自动方法。RegMix通过回归任务来预测不同数据混合的性能，并使用最佳预测结果来训练大型模型。实证研究表明，RegMix在多个实验中表现优秀，与人工选择相比具有显著优势，同时匹配或超越了DoReMi方法且仅使用了其十分之一的计算资源。此外，实验还揭示了一些重要见解，如数据混合对性能有重要影响，网络语料库相较于如维基百科等高质量数据对下游性能有更强正面影响等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据混合对大型语言模型的性能具有显著影响。</li>
<li>RegMix方法能够自动确定高性能的数据混合。</li>
<li>通过回归任务，RegMix预测不同数据混合的性能表现。</li>
<li>网络语料库与下游性能之间有强正相关关系。</li>
<li>模型领域间的相互作用复杂，需要自动方法如RegMix来处理。</li>
<li>数据混合的影响超越了规模定律。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.01492">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c686cb4b3276b195b6f6024d69a70d30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14cb73bbb639ffe7496caf361a5a2082.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eccdb003fdfd887278f19472bb704e25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2093eb51db718dbcf5295f80e8baf55a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ce4c1b497b73c80a6f2748458ba8f80.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Evaluating-LLMs-for-Quotation-Attribution-in-Literary-Texts-A-Case-Study-of-LLaMa3"><a href="#Evaluating-LLMs-for-Quotation-Attribution-in-Literary-Texts-A-Case-Study-of-LLaMa3" class="headerlink" title="Evaluating LLMs for Quotation Attribution in Literary Texts: A Case   Study of LLaMa3"></a>Evaluating LLMs for Quotation Attribution in Literary Texts: A Case   Study of LLaMa3</h2><p><strong>Authors:Gaspard Michel, Elena V. Epure, Romain Hennequin, Christophe Cerisara</strong></p>
<p>Large Language Models (LLMs) have shown promising results in a variety of literary tasks, often using complex memorized details of narration and fictional characters. In this work, we evaluate the ability of Llama-3 at attributing utterances of direct-speech to their speaker in novels. The LLM shows impressive results on a corpus of 28 novels, surpassing published results with ChatGPT and encoder-based baselines by a large margin. We then validate these results by assessing the impact of book memorization and annotation contamination. We found that these types of memorization do not explain the large performance gain, making Llama-3 the new state-of-the-art for quotation attribution in English literature. We release publicly our code and data. </p>
<blockquote>
<p>大型语言模型（LLM）在各种文学任务中表现出了令人瞩目的结果，通常利用对叙事和虚构角色的复杂记忆细节。在这项工作中，我们评估了Llama-3在小说中将直接言语归于说话人的能力。该语言模型在28部小说的语料库上表现出令人印象深刻的结果，大幅度超越了ChatGPT和基于编码器的基线模型的已发布结果。然后，我们通过评估书籍记忆和注释污染的影响来验证这些结果。我们发现，这种类型的记忆并不能解释性能的大幅提升，这使得Llama-3成为英语文学中引语归属的新技术顶尖水平。我们公开发布了我们的代码和数据。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11380v2">PDF</a> NAACL 2025 Main Conference – short paper</p>
<p><strong>Summary</strong></p>
<p>LLMs如Llama-3在小说直接引语的说话者归属任务中展现出卓越性能，对28部小说的语料库结果令人印象深刻，超越了ChatGPT和基于编码器的基线模型。验证结果显示，书本记忆和标注污染并不足以解释其性能的大幅提升，Llama-3成为英语文学引用归属任务的新标杆。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Llama-3在小说直接引语的说话者归属任务中表现出卓越性能。</li>
<li>Llama-3在28部小说的语料库上的表现超越了ChatGPT和其他基线模型。</li>
<li>验证结果显示书本记忆和标注污染并不足以解释Llama-3的性能提升。</li>
<li>Llama-3成为英语文学引用归属任务的新标杆。</li>
<li>该研究公开了代码和数据，有助于进一步研究和改进LLM技术。</li>
<li>该研究展示了LLM在文学任务中的复杂记忆能力，包括叙事和角色细节的复杂记忆。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11380">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-77df98bccd76bdb87da600da7e60500c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cb301db410a89e3bc5c67106eacbffc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-983d629211789b6cd0fd01875e6cf904.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0434dc894f0b702ee8f9f50e925f52d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="IrokoBench-A-New-Benchmark-for-African-Languages-in-the-Age-of-Large-Language-Models"><a href="#IrokoBench-A-New-Benchmark-for-African-Languages-in-the-Age-of-Large-Language-Models" class="headerlink" title="IrokoBench: A New Benchmark for African Languages in the Age of Large   Language Models"></a>IrokoBench: A New Benchmark for African Languages in the Age of Large   Language Models</h2><p><strong>Authors:David Ifeoluwa Adelani, Jessica Ojo, Israel Abebe Azime, Jian Yun Zhuang, Jesujoba O. Alabi, Xuanli He, Millicent Ochieng, Sara Hooker, Andiswa Bukula, En-Shiun Annie Lee, Chiamaka Chukwuneke, Happy Buzaaba, Blessing Sibanda, Godson Kalipe, Jonathan Mukiibi, Salomon Kabongo, Foutse Yuehgoh, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Shamsuddeen Hassan Muhammad, Salomey Osei, Sokhar Samb, Tadesse Kebede Guge, Tombekai Vangoni Sherman, Pontus Stenetorp</strong></p>
<p>Despite the widespread adoption of Large language models (LLMs), their remarkable capabilities remain limited to a few high-resource languages. Additionally, many low-resource languages (\eg African languages) are often evaluated only on basic text classification tasks due to the lack of appropriate or comprehensive benchmarks outside of high-resource languages. In this paper, we introduce IrokoBench – a human-translated benchmark dataset for 17 typologically-diverse low-resource African languages covering three tasks: natural language inference<del>(AfriXNLI), mathematical reasoning</del>(AfriMGSM), and multi-choice knowledge-based question answering<del>(AfriMMLU). We use IrokoBench to evaluate zero-shot, few-shot, and translate-test settings</del>(where test sets are translated into English) across 10 open and six proprietary LLMs. Our evaluation reveals a significant performance gap between high-resource languages~(such as English and French) and low-resource African languages. We observe a significant performance gap between open and proprietary models, with the highest performing open model, Gemma 2 27B only at 63% of the best-performing proprietary model GPT-4o performance. In addition, machine translating the test set to English before evaluation helped to close the gap for larger models that are English-centric, such as Gemma 2 27B and LLaMa 3.1 70B. These findings suggest that more efforts are needed to develop and adapt LLMs for African languages. </p>
<blockquote>
<p>尽管大型语言模型（LLM）得到了广泛应用，但它们的出色功能仅限于少数高资源语言。此外，由于高资源语言之外缺乏适当或全面的基准测试，许多低资源语言（例如非洲语言）通常仅在基本的文本分类任务上进行评估。在本文中，我们介绍了IrokoBench——一个为17种类型多样且资源匮乏的非洲语言提供的人类翻译基准数据集，包含三个任务：自然语言推理（AfriXNLI）、数学推理（AfriMGSM）和基于多选知识的问答（AfriMMLU）。我们使用IrokoBench对零样本、少样本以及将测试集翻译至英语的翻译测试环境进行了评估，涉及10个开源和六个专有LLM。我们的评估揭示了高资源语言（如英语和法语）与资源匮乏的非洲语言之间的显著性能差距。我们还观察到开源模型和专有模型之间的显著性能差距，表现最佳的开源模型Gemma 2 27B仅达到最佳专有模型GPT-4o性能的63%。此外，在评估之前将测试集机器翻译至英语有助于缩小以英语为中心的较大模型的差距，如Gemma 2 27B和LLaMa 3.1 70B。这些发现表明，需要更多的努力来开发和适应非洲语言的LLM。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.03368v2">PDF</a> Accepted to NAACL 2025 (main conference)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了IrokoBench——一个为17种语言形态各异的低资源非洲语言设计的人类翻译基准数据集。该数据集涵盖了三项任务：自然语言推理、数学推理和基于知识的问题回答。通过对零样本、少样本和翻译测试环境的评估，发现非洲语言与资源丰富的语言之间存在显著的性能差距。机器翻译测试集到英语后评估有助于缩小与英语为中心的大型模型的差距。这显示出需要更多努力来开发和适应非洲语言的LLM。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IrokoBench是一个针对非洲语言的基准数据集，包含自然语言推理、数学推理和知识问答三项任务。</li>
<li>数据集涵盖17种语言形态各异的低资源非洲语言。</li>
<li>在零样本、少样本和翻译测试环境中评估LLM性能时发现非洲语言与资源丰富的语言之间存在显著差距。</li>
<li>开放模型和专有模型之间存在显著性能差距，最佳开放模型仅达到最佳专有模型性能的63%。</li>
<li>将测试集机器翻译到英语后评估有助于缩小与英语为中心的大型模型的性能差距。</li>
<li>需要更多努力来开发和适应非洲语言的LLM。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.03368">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d3e6e4dba97beb2e0947722992d1d0c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60918a7755b976740f58c6a0cff77b1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ffd8e1a3f25840db5b9f016d689481d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30cf188013e83286dffa3c67454050dd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Solve-longer-Math-Word-Problems-Better"><a href="#Can-LLMs-Solve-longer-Math-Word-Problems-Better" class="headerlink" title="Can LLMs Solve longer Math Word Problems Better?"></a>Can LLMs Solve longer Math Word Problems Better?</h2><p><strong>Authors:Xin Xu, Tong Xiao, Zitong Chao, Zhenya Huang, Can Yang, Yang Wang</strong></p>
<p>Math Word Problems (MWPs) play a vital role in assessing the capabilities of Large Language Models (LLMs), yet current research primarily focuses on questions with concise contexts. The impact of longer contexts on mathematical reasoning remains under-explored. This study pioneers the investigation of Context Length Generalizability (CoLeG), which refers to the ability of LLMs to solve MWPs with extended narratives. We introduce Extended Grade-School Math (E-GSM), a collection of MWPs featuring lengthy narratives, and propose two novel metrics to evaluate the efficacy and resilience of LLMs in tackling these problems. Our analysis of existing zero-shot prompting techniques with proprietary LLMs along with open-source LLMs reveals a general deficiency in CoLeG. To alleviate these issues, we propose tailored approaches for different categories of LLMs. For proprietary LLMs, we introduce a new instructional prompt designed to mitigate the impact of long contexts. For open-source LLMs, we develop a novel auxiliary task for fine-tuning to enhance CoLeG. Our comprehensive results demonstrate the effectiveness of our proposed methods, showing improved performance on E-GSM. Additionally, we conduct an in-depth analysis to differentiate the effects of semantic understanding and reasoning efficacy, showing that our methods improves the latter. We also establish the generalizability of our methods across several other MWP benchmarks. Our findings highlight the limitations of current LLMs and offer practical solutions correspondingly, paving the way for further exploration of model generalizability and training methodologies. </p>
<blockquote>
<p>数学文字题（MWPs）在评估大型语言模型（LLM）的能力方面起着至关重要的作用，然而当前的研究主要集中在简洁语境的问题上。较长语境对数学推理的影响仍被忽视。本研究率先探讨了语境长度泛化（CoLeG）问题，即LLM解决具有扩展叙述的MWPs的能力。我们引入了扩展小学数学（E-GSM），这是一系列带有冗长叙述的MWPs，并提出了两种新颖的指标来评估LLM在处理这些问题时的效果和韧性。我们对现有的零样本提示技术与专有LLM以及开源LLM的分析表明，CoLeG普遍存在不足。为了缓解这些问题，我们为不同类型的LLM提出了针对性的方法。对于专有LLM，我们引入了一种新的指令提示，旨在减轻长语境的影响。对于开源LLM，我们开发了一种用于微调的新型辅助任务，以提高CoLeG。我们的全面结果证明了所提出方法的有效性，显示出在E-GSM上的性能改进。此外，我们进行了深入的分析，以区分语义理解和推理效果的影响，表明我们的方法提高了后者的效果。我们的方法在其他数学文字题基准测试中的通用性也得到了证实。我们的研究突出了当前LLM的局限性，并提供了相应的实用解决方案，为模型的泛化能力和训练方法的进一步探索铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14804v2">PDF</a> Accepted to ICLR 2025</p>
<p><strong>摘要</strong></p>
<p>本文研究了大型语言模型（LLM）在处理带有较长语境的数学字问题（MWPs）时的表现。文章介绍了语境长度泛化（CoLeG）的概念，并探讨了LLM解决具有扩展叙事的问题的能力。文章引入了一种新的数学字问题集Extended Grade-School Math（E-GSM），并提出了两种新指标来评估LLM解决这些问题的效果和韧性。文章分析了现有的零提示提示技术与开源LLM和专有LLM的局限性，并提出了针对性的解决方案。对于专有LLM，引入了一种新的指令提示来缓解长语境的影响。对于开源LLM，我们开发了一种新颖的精细调整辅助任务来增强CoLeG。实验结果显示，所提出的方法在E-GSM上表现出良好的性能提升。此外，文章还深入分析了语义理解和推理效果的不同影响，并建立了方法在其他数学字问题基准测试上的泛化能力。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>当前研究主要关注具有简洁语境的数学字问题，而较长语境对数学推理的影响尚未得到充分探索。</li>
<li>引入Context Length Generalizability（CoLeG）概念，以评估LLM解决具有扩展叙事的数学字问题的能力。</li>
<li>提出Extended Grade-School Math（E-GSM）数据集，包含具有较长叙事的问题。</li>
<li>介绍两种新指标来评估LLM在解决这些问题时的效果和韧性。</li>
<li>分析现有零提示提示技术的局限性，并针对专有LLM和开源LLM提出解决方案。</li>
<li>对于专有LLM，引入新的指令提示来缓解长语境的影响；对于开源LLM，开发新颖的精细调整辅助任务以增强CoLeG。</li>
<li>实验结果显示所提出方法的有效性，并深入分析了语义理解和推理效果的不同影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14804">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-54880d4d1096a4516002091881fb409d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2edbf80dce204251529d371a4fad44e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bd4a2ca917c1950ff8d1eaaabd1851e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-163a1ecb85a76f86ff07c22622d39067.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd7d8b38b8a36b1218de950d016e042e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TT-BLIP-Enhancing-Fake-News-Detection-Using-BLIP-and-Tri-Transformer"><a href="#TT-BLIP-Enhancing-Fake-News-Detection-Using-BLIP-and-Tri-Transformer" class="headerlink" title="TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer"></a>TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer</h2><p><strong>Authors:Eunjee Choi, Jong-Kook Kim</strong></p>
<p>Detecting fake news has received a lot of attention. Many previous methods concatenate independently encoded unimodal data, ignoring the benefits of integrated multimodal information. Also, the absence of specialized feature extraction for text and images further limits these methods. This paper introduces an end-to-end model called TT-BLIP that applies the bootstrapping language-image pretraining for unified vision-language understanding and generation (BLIP) for three types of information: BERT and BLIPTxt for text, ResNet and BLIPImg for images, and bidirectional BLIP encoders for multimodal information. The Multimodal Tri-Transformer fuses tri-modal features using three types of multi-head attention mechanisms, ensuring integrated modalities for enhanced representations and improved multimodal data analysis. The experiments are performed using two fake news datasets, Weibo and Gossipcop. The results indicate TT-BLIP outperforms the state-of-the-art models. </p>
<blockquote>
<p>检测假新闻已经引起了广泛关注。许多以前的方法会连接独立编码的单模态数据，忽略了集成多模态信息的好处。此外，缺少针对文本和图像的专业特征提取进一步限制了这些方法。本文介绍了一种端到端的模型，称为TT-BLIP，该模型应用引导式语言图像预训练进行统一的视觉语言理解和生成（BLIP），涵盖三种类型的信息：BERT和BLIPTxt用于文本，ResNet和BLIPImg用于图像，双向BLIP编码器用于多模态信息。多模态三Transformer通过三种多头注意力机制融合三模态特征，确保集成模式用于增强表示和改进多模态数据分析。实验采用两个假新闻数据集微博和Gossipcop进行。结果表明，TT-BLIP优于最新模型。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.12481v2">PDF</a> 8 pages, Accepted 27th International Conference on Information   Fusion, FUSION 2024</p>
<p><strong>Summary</strong>：</p>
<p>本文介绍了一种名为TT-BLIP的端到端模型，该模型采用预训练的跨语言图像理解生成（BLIP）技术，结合文本和图像的多模态信息，进行统一的视觉和语言理解。模型通过三种信息类型（文本、图像和多模态信息）和三种多头注意力机制融合三模态特征，提高了多模态数据的表示和分析能力。实验结果表明，TT-BLIP在假新闻检测方面优于现有技术。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>许多早期假新闻检测方法忽略多模态信息的融合优势，仅连接独立编码的单模态数据。</li>
<li>TT-BLIP模型采用预训练的BLIP技术，支持统一视觉和语言理解生成。</li>
<li>TT-BLIP支持三种信息类型：文本（BERT和BLIPTxt）、图像（ResNet和BLIPImg）以及多模态信息。</li>
<li>多模态三模态转换器通过三种多头注意力机制融合三模态特征。</li>
<li>模型提高了多模态数据的表示和分析能力。</li>
<li>实验在Weibo和Gossipcop两个假新闻数据集上进行，验证了TT-BLIP模型的性能优越性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.12481">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-4372300d6a87aa87f209b4ca50f71681.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d8fe056969d064ec53baf7696f2cd25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7b9b8b36c4066de88aef78b47cfefab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-487a3fce70536c02986477ba4ccd5ee9.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Transformer-Based-Model-for-Monocular-Visual-Odometry-A-Video-Understanding-Approach"><a href="#Transformer-Based-Model-for-Monocular-Visual-Odometry-A-Video-Understanding-Approach" class="headerlink" title="Transformer-Based Model for Monocular Visual Odometry: A Video   Understanding Approach"></a>Transformer-Based Model for Monocular Visual Odometry: A Video   Understanding Approach</h2><p><strong>Authors:André O. Françani, Marcos R. O. A. Maximo</strong></p>
<p>Estimating the camera’s pose given images from a single camera is a traditional task in mobile robots and autonomous vehicles. This problem is called monocular visual odometry and often relies on geometric approaches that require considerable engineering effort for a specific scenario. Deep learning methods have been shown to be generalizable after proper training and with a large amount of available data. Transformer-based architectures have dominated the state-of-the-art in natural language processing and computer vision tasks, such as image and video understanding. In this work, we deal with the monocular visual odometry as a video understanding task to estimate the 6 degrees of freedom of a camera’s pose. We contribute by presenting the TSformer-VO model based on spatio-temporal self-attention mechanisms to extract features from clips and estimate the motions in an end-to-end manner. Our approach achieved competitive state-of-the-art performance compared with geometry-based and deep learning-based methods on the KITTI visual odometry dataset, outperforming the DeepVO implementation highly accepted in the visual odometry community. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/aofrancani/TSformer-VO">https://github.com/aofrancani/TSformer-VO</a>. </p>
<blockquote>
<p>根据单相机图像估计相机的姿态是移动机器人和自动驾驶汽车中的一个传统任务。这个问题被称为单目视觉里程计，通常依赖于特定场景需要大量工程努力的几何方法。深度学习方法在适当的训练和大量可用数据之后，已显示出可推广性。基于Transformer的架构在诸如图像和视频理解等领域的自然语言处理和计算机视觉任务中占据了最先进的地位。在这项工作中，我们将单目视觉里程计作为视频理解任务来处理，以估计相机的六自由度姿态。我们提出了一种基于时空自注意力机制的TSformer-VO模型，以从片段中提取特征并以端到端的方式估计运动。我们的方法在KITTI视觉里程计数据集上与基于几何和基于深度学习的方法相比，取得了具有竞争力的最先进的性能，并超越了视觉里程计社区中广受欢迎的DeepVO实现。代码可在<a target="_blank" rel="noopener" href="https://github.com/aofrancani/TSformer-VO%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/aofrancani/TSformer-VO上公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.06121v3">PDF</a> This work has been accepted for publication in IEEE Access</p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于Transformer架构的TSformer-VO模型在单目视觉里程计任务中的应用。该模型利用时空自注意力机制提取视频片段特征，以端到端的方式估计相机姿态的六个自由度。在KITTI视觉里程计数据集上的性能表现优异，超过了基于几何和深度学习的方法，包括被广泛接受的DeepVO实现。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TSformer-VO模型基于Transformer架构，用于处理单目视觉里程计任务。</li>
<li>该模型利用时空自注意力机制提取视频片段特征。</li>
<li>TSformer-VO以端到端的方式估计相机姿态的六个自由度。</li>
<li>在KITTI视觉里程计数据集上，TSformer-VO性能表现优异，超过了基于几何和深度学习的方法。</li>
<li>TSformer-VO模型公开可用，网址为<a target="_blank" rel="noopener" href="https://github.com/aofrancani/TSformer-VO%E3%80%82">https://github.com/aofrancani/TSformer-VO。</a></li>
<li>该研究展示了深度学习方法的通用性，经过适当训练和大量数据，可以取得良好性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2305.06121">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-72c2412bebc603839892e153916b420e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca27b6a8de93ec08e45da59c6112986e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-909ace692fddca128899f8d86075a177.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-007cfc1789a67c846bd873d3e24f4fa1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1feda3c7bd60acf4a4f0da003454b10.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-25/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-25/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-25/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6cbb9040a5a327d23167ce5ea5a8fd48.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-01-25  Scalable Safe Multi-Agent Reinforcement Learning for Multi-Agent System
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-24/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-39d0d3f9feb1f6978c65d36425487122.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-01-24  Training Dialogue Systems by AI Feedback for Improving Overall Dialogue   Impression
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25011.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
