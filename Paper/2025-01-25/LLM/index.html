<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-25  CRPO Confidence-Reward Driven Preference Optimization for Machine   Translation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-983d629211789b6cd0fd01875e6cf904.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-01-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-01-25-æ›´æ–°"><a href="#2025-01-25-æ›´æ–°" class="headerlink" title="2025-01-25 æ›´æ–°"></a>2025-01-25 æ›´æ–°</h1><h2 id="CRPO-Confidence-Reward-Driven-Preference-Optimization-for-Machine-Translation"><a href="#CRPO-Confidence-Reward-Driven-Preference-Optimization-for-Machine-Translation" class="headerlink" title="CRPO: Confidence-Reward Driven Preference Optimization for Machine   Translation"></a>CRPO: Confidence-Reward Driven Preference Optimization for Machine   Translation</h2><p><strong>Authors:Guofeng Cui, Pichao Wang, Yang Liu, Zemian Ke, Zhu Liu, Vimal Bhat</strong></p>
<p>Large language models (LLMs) have shown great potential in natural language processing tasks, but their application to machine translation (MT) remains challenging due to pretraining on English-centric data and the complexity of reinforcement learning from human feedback (RLHF). Direct Preference Optimization (DPO) has emerged as a simpler and more efficient alternative, but its performance depends heavily on the quality of preference data. To address this, we propose Confidence-Reward driven Preference Optimization (CRPO), a novel method that combines reward scores with model confidence to improve data selection for fine-tuning. CRPO selects challenging sentence pairs where the model is uncertain or underperforms, leading to more effective learning. While primarily designed for LLMs, CRPO also generalizes to encoder-decoder models like NLLB, demonstrating its versatility. Empirical results show that CRPO outperforms existing methods such as RS-DPO, RSO and MBR score in both translation accuracy and data efficiency. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å°†å…¶åº”ç”¨äºæœºå™¨ç¿»è¯‘ï¼ˆMTï¼‰ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„é¢„è®­ç»ƒæ•°æ®å’Œä»äººç±»åé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„å¤æ‚æ€§ã€‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºä¸€ç§æ›´ç®€å•ã€æ›´é«˜æ•ˆçš„é€‰æ‹©æ–¹æ³•å·²ç»å‡ºç°ï¼Œä½†å…¶æ€§èƒ½åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºåå¥½æ•°æ®çš„è´¨é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¿¡å¿ƒå¥–åŠ±çš„åå¥½ä¼˜åŒ–ï¼ˆCRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆå¥–åŠ±åˆ†æ•°å’Œæ¨¡å‹ä¿¡å¿ƒä»¥æé«˜å¾®è°ƒæ•°æ®é€‰æ‹©çš„æ–°æ–¹æ³•ã€‚CRPOé€‰æ‹©æ¨¡å‹ä¸ç¡®å®šæˆ–è¡¨ç°ä¸ä½³çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„å¥å­å¯¹ï¼Œä»è€Œå®ç°æ›´æœ‰æ•ˆçš„å­¦ä¹ ã€‚è™½ç„¶CRPOä¸»è¦è®¾è®¡ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä½†å®ƒä¹Ÿé€‚ç”¨äºç¼–ç å™¨è§£ç å™¨æ¨¡å‹ï¼Œå¦‚NLLBï¼Œæ˜¾ç¤ºå‡ºå…¶é€šç”¨æ€§ã€‚ç»éªŒç»“æœè¡¨æ˜ï¼ŒCRPOåœ¨ç¿»è¯‘å‡†ç¡®æ€§å’Œæ•°æ®æ•ˆç‡æ–¹é¢éƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¦‚RS-DPOã€RSOå’ŒMBRè¯„åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13927v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨æœºå™¨ç¿»è¯‘æ–¹é¢çš„åº”ç”¨ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºä¸€ç§æ›´ç®€å•é«˜æ•ˆçš„æ–¹æ³•åº”è¿è€Œç”Ÿï¼Œä½†å…¶æ€§èƒ½å–å†³äºåå¥½æ•°æ®çš„è´¨é‡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¿¡å¿ƒå¥–åŠ±é©±åŠ¨åå¥½ä¼˜åŒ–ï¼ˆCRPOï¼‰æ–¹æ³•ï¼Œç»“åˆå¥–åŠ±åˆ†æ•°å’Œæ¨¡å‹ä¿¡å¿ƒæ”¹å–„æ•°æ®é€‰æ‹©ï¼Œç”¨äºå¾®è°ƒã€‚CRPOé€‰æ‹©æ¨¡å‹ä¸ç¡®å®šæˆ–è¡¨ç°ä¸ä½³çš„éš¾å¥å¯¹ï¼Œå®ç°æ›´æœ‰æ•ˆçš„å­¦ä¹ ã€‚CRPOä¸ä»…é€‚ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¹Ÿå¯æ¨å¹¿è‡³ç¼–ç å™¨è§£ç å™¨æ¨¡å‹å¦‚NLLBï¼Œæ˜¾ç¤ºå‡ºå…¶é€šç”¨æ€§ã€‚å®è¯ç»“æœæ˜¾ç¤ºï¼ŒCRPOåœ¨ç¿»è¯‘å‡†ç¡®æ€§å’Œæ•°æ®æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•å¦‚RS-DPOã€RSOå’ŒMBRåˆ†æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨ç¿»è¯‘æ–¹é¢åº”ç”¨é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬åŸºäºè‹±è¯­çš„æ•°æ®é¢„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ çš„é—®é¢˜ã€‚</li>
<li>ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä½œä¸ºè§£å†³ä¸Šè¿°é—®é¢˜çš„ç®€åŒ–æ–¹æ³•è¢«æå‡ºï¼Œä½†æ€§èƒ½å—é™äºåå¥½æ•°æ®è´¨é‡ã€‚</li>
<li>æå‡ºä¿¡å¿ƒå¥–åŠ±é©±åŠ¨åå¥½ä¼˜åŒ–ï¼ˆCRPOï¼‰æ–¹æ³•ï¼Œç»“åˆå¥–åŠ±åˆ†æ•°å’Œæ¨¡å‹ä¿¡å¿ƒæ”¹å–„æ•°æ®é€‰æ‹©è¿‡ç¨‹ã€‚</li>
<li>CRPOé€‰æ‹©æ¨¡å‹ä¸ç¡®å®šæˆ–è¡¨ç°ä¸ä½³çš„éš¾å¥å¯¹è¿›è¡Œæ›´æœ‰æ•ˆçš„å­¦ä¹ ã€‚</li>
<li>CRPOä¸ä»…é€‚ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä¹Ÿå¯åº”ç”¨äºç¼–ç å™¨è§£ç å™¨æ¨¡å‹å¦‚NLLBã€‚</li>
<li>å®è¯ç»“æœæ˜¾ç¤ºCRPOåœ¨ç¿»è¯‘å‡†ç¡®æ€§å’Œæ•°æ®æ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13927">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b6c0f70efb665da67d169cd5201f9a9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-908a67c5457a4a59d82652efddd413c2.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Privacy-Preserving-Personalized-Federated-Prompt-Learning-for-Multimodal-Large-Language-Models"><a href="#Privacy-Preserving-Personalized-Federated-Prompt-Learning-for-Multimodal-Large-Language-Models" class="headerlink" title="Privacy-Preserving Personalized Federated Prompt Learning for Multimodal   Large Language Models"></a>Privacy-Preserving Personalized Federated Prompt Learning for Multimodal   Large Language Models</h2><p><strong>Authors:Linh Tran, Wei Sun, Stacy Patterson, Ana Milanova</strong></p>
<p>Multimodal Large Language Models (LLMs) are pivotal in revolutionizing customer support and operations by integrating multiple modalities such as text, images, and audio. Federated Prompt Learning (FPL) is a recently proposed approach that combines pre-trained multimodal LLMs such as vision-language models with federated learning to create personalized, privacy-preserving AI systems. However, balancing the competing goals of personalization, generalization, and privacy remains a significant challenge. Over-personalization can lead to overfitting, reducing generalizability, while stringent privacy measures, such as differential privacy, can hinder both personalization and generalization. In this paper, we propose a Differentially Private Federated Prompt Learning (DP-FPL) approach to tackle this challenge by leveraging a low-rank adaptation scheme to capture generalization while maintaining a residual term that preserves expressiveness for personalization. To ensure privacy, we introduce a novel method where we apply local differential privacy to the two low-rank components of the local prompt, and global differential privacy to the global prompt. Our approach mitigates the impact of privacy noise on the model performance while balancing the tradeoff between personalization and generalization. Extensive experiments demonstrate the effectiveness of our approach over other benchmarks. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ•´åˆæ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ç­‰å¤šç§æ¨¡æ€ï¼Œåœ¨é©æ–°å®¢æˆ·æ”¯æŒå’Œè¿è¥æ–¹é¢å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚è”é‚¦æç¤ºå­¦ä¹ ï¼ˆFPLï¼‰æ˜¯ä¸€ç§æœ€è¿‘æå‡ºçš„æ–¹æ³•ï¼Œå®ƒå°†é¢„è®­ç»ƒçš„å¤šæ¨¡æ€LLMï¼ˆå¦‚è§†è§‰è¯­è¨€æ¨¡å‹ï¼‰ä¸è”é‚¦å­¦ä¹ ç›¸ç»“åˆï¼Œä»¥åˆ›å»ºä¸ªæ€§åŒ–ã€ä¿æŠ¤éšç§çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚ç„¶è€Œï¼Œåœ¨ä¸ªæ€§åŒ–ã€é€šç”¨åŒ–å’Œéšç§ä¹‹é—´å–å¾—å¹³è¡¡ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚è¿‡åº¦ä¸ªæ€§åŒ–å¯èƒ½å¯¼è‡´è¿‡åº¦æ‹Ÿåˆï¼Œé™ä½é€šç”¨æ€§ï¼Œè€Œä¸¥æ ¼çš„éšç§æªæ–½ï¼ˆå¦‚å·®åˆ†éšç§ï¼‰å¯èƒ½ä¼šé˜»ç¢ä¸ªæ€§åŒ–å’Œé€šç”¨åŒ–ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å·®åˆ†ç§æœ‰è”é‚¦æç¤ºå­¦ä¹ ï¼ˆDP-FPLï¼‰æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨ä½é˜¶é€‚åº”æ–¹æ¡ˆæ¥æ•æ‰é€šç”¨æ€§ï¼ŒåŒæ—¶ä¿æŒä¸€ä¸ªæ®‹ç•™é¡¹æ¥ä¿æŒä¸ªæ€§åŒ–çš„è¡¨ç°åŠ›ï¼Œä»¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚ä¸ºç¡®ä¿éšç§ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå¯¹å±€éƒ¨æç¤ºçš„ä¸¤ä¸ªä½é˜¶åˆ†é‡åº”ç”¨æœ¬åœ°å·®åˆ†éšç§ï¼Œå¯¹å…¨å±€æç¤ºåº”ç”¨å…¨å±€å·®åˆ†éšç§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å‡è½»äº†éšç§å™ªå£°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œåœ¨ä¸ªæ€§åŒ–ä¸é€šç”¨åŒ–ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¶ä»–åŸºå‡†æµ‹è¯•ä¸Šéå¸¸æœ‰æ•ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13904v1">PDF</a> Accepted to ICLR 2025 main conference track</p>
<p><strong>Summary</strong>:<br>å¤šåª’ä½“å¤§è¯­è¨€æ¨¡å‹é€šè¿‡é›†æˆæ–‡æœ¬ã€å›¾åƒå’ŒéŸ³é¢‘ç­‰å¤šåª’ä½“æ¨¡å¼ï¼Œåœ¨å®¢æˆ·æ”¯æŒå’Œè¿è¥é¢†åŸŸæ€èµ·é©å‘½ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå·®åˆ†éšç§çš„è”é‚¦æç¤ºå­¦ä¹ æ–¹æ³•ï¼ˆDP-FPLï¼‰ï¼Œé€šè¿‡ä½é˜¶é€‚åº”æ–¹æ¡ˆå®ç°ä¸ªæ€§åŒ–ä¸æ³›åŒ–ä¹‹é—´çš„å¹³è¡¡ï¼ŒåŒæ—¶é‡‡ç”¨æœ¬åœ°å·®åˆ†éšç§å’Œå…¨å±€å·®åˆ†éšç§ä¿æŠ¤æœºåˆ¶ï¼Œæœ‰æ•ˆå‡è½»éšç§å™ªå£°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨å®¢æˆ·æ”¯æŒå’Œè¿è¥ä¸­å…·æœ‰é‡è¦ä½œç”¨ã€‚</li>
<li>è”é‚¦æç¤ºå­¦ä¹ æ˜¯ä¸€ç§å°†é¢„è®­ç»ƒçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸è”é‚¦å­¦ä¹ ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œç”¨äºåˆ›å»ºä¸ªæ€§åŒ–ã€ä¿æŠ¤éšç§çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿã€‚</li>
<li>å¹³è¡¡ä¸ªæ€§åŒ–ã€æ³›åŒ–å’Œéšç§æ˜¯é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚</li>
<li>è¿‡åº¦çš„ä¸ªæ€§åŒ–å¯èƒ½å¯¼è‡´è¿‡åº¦æ‹Ÿåˆï¼Œé™ä½æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ä¸¥æ ¼çš„éšç§æªæ–½ï¼Œå¦‚å·®åˆ†éšç§ï¼Œå¯èƒ½ä¼šå½±å“ä¸ªæ€§åŒ–å’Œæ³›åŒ–ã€‚</li>
<li>DP-FPLæ–¹æ³•é€šè¿‡ä½é˜¶é€‚åº”æ–¹æ¡ˆå¹³è¡¡ä¸ªæ€§åŒ–å’Œæ³›åŒ–ï¼ŒåŒæ—¶é‡‡ç”¨æœ¬åœ°å’Œå…¨å±€å·®åˆ†éšç§ä¿æŠ¤æœºåˆ¶ç¡®ä¿éšç§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13904">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-1ed546cf96c193747e1f1de6d62355b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6c65c9f77ad8b75c23932f4e51136cf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a37bb56f457339f3f7f06a07b827ad0.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Exploring-Finetuned-Audio-LLM-on-Heart-Murmur-Features"><a href="#Exploring-Finetuned-Audio-LLM-on-Heart-Murmur-Features" class="headerlink" title="Exploring Finetuned Audio-LLM on Heart Murmur Features"></a>Exploring Finetuned Audio-LLM on Heart Murmur Features</h2><p><strong>Authors:Adrian Florea, Xilin Jiang, Nima Mesgarani, Xiaofan Jiang</strong></p>
<p>Large language models (LLMs) for audio have excelled in recognizing and analyzing human speech, music, and environmental sounds. However, their potential for understanding other types of sounds, particularly biomedical sounds, remains largely underexplored despite significant scientific interest. In this study, we focus on diagnosing cardiovascular diseases using phonocardiograms, i.e., heart sounds. Most existing deep neural network (DNN) paradigms are restricted to heart murmur classification (healthy vs unhealthy) and do not predict other acoustic features of the murmur such as timing, grading, harshness, pitch, and quality, which are important in helping physicians diagnose the underlying heart conditions. We propose to finetune an audio LLM, Qwen2-Audio, on the PhysioNet CirCor DigiScope phonocardiogram (PCG) dataset and evaluate its performance in classifying 11 expert-labeled murmur features. Additionally, we aim to achieve more noise-robust and generalizable system by exploring a preprocessing segmentation algorithm using an audio representation model, SSAMBA. Our results indicate that the LLM-based model outperforms state-of-the-art methods in 8 of the 11 features and performs comparably in the remaining 3. Moreover, the LLM successfully classifies long-tail murmur features with limited training data, a task that all previous methods have failed to classify. These findings underscore the potential of audio LLMs as assistants to human cardiologists in enhancing heart disease diagnosis. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éŸ³é¢‘é¢†åŸŸå·²ç»èƒ½å¤Ÿå‡ºè‰²åœ°è¯†åˆ«å’Œè§£æäººç±»è¯­éŸ³ã€éŸ³ä¹å’Œç¯å¢ƒå£°éŸ³ã€‚ç„¶è€Œï¼Œå°½ç®¡ç§‘å­¦ç•Œå¯¹æ­¤æœ‰æµ“åšçš„å…´è¶£ï¼Œå®ƒä»¬å¯¹äºç†è§£å…¶ä»–ç±»å‹çš„å£°éŸ³ï¼Œå°¤å…¶æ˜¯ç”Ÿç‰©åŒ»å­¦å£°éŸ³ï¼Œå…¶æ½œåŠ›ä»è¢«å¤§å¤§ä½ä¼°ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºä½¿ç”¨å¿ƒéŸ³å›¾ï¼ˆå³å¿ƒè„å£°éŸ³ï¼‰æ¥è¯Šæ–­å¿ƒè¡€ç®¡ç–¾ç—…ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰æ¨¡å¼ä»…é™äºå¿ƒè„æ‚éŸ³åˆ†ç±»ï¼ˆå¥åº·ä¸å¦ï¼‰ï¼Œæ— æ³•é¢„æµ‹æ‚éŸ³çš„å…¶ä»–å£°å­¦ç‰¹å¾ï¼Œå¦‚æ—¶é—´ã€ç­‰çº§ã€ä¸¥å‰ç¨‹åº¦ã€éŸ³é«˜å’ŒéŸ³è´¨ç­‰ï¼Œè¿™äº›ç‰¹å¾å¯¹äºå¸®åŠ©åŒ»ç”Ÿè¯Šæ–­æ½œåœ¨çš„å¿ƒè„çŠ¶å†µéå¸¸é‡è¦ã€‚æˆ‘ä»¬æè®®å¯¹éŸ³é¢‘LLMæ¨¡å‹Qwen2-Audioè¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨PhysioNet CirCor DigiScopeå¿ƒéŸ³å›¾æ•°æ®é›†ä¸Šè¯„ä¼°å…¶æ€§èƒ½ï¼Œä»¥åˆ†ç±»ä¸“å®¶æ ‡æ³¨çš„11ç§æ‚éŸ³ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¸Œæœ›é€šè¿‡ä½¿ç”¨éŸ³é¢‘è¡¨ç¤ºæ¨¡å‹çš„é¢„å¤„ç†åˆ†å‰²ç®—æ³•SSAMBAï¼Œå»ºç«‹ä¸€ä¸ªæ›´åŠ ç¨³å¥ä¸”é€šç”¨çš„ç³»ç»Ÿã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºLLMçš„æ¨¡å‹åœ¨å…¶ä¸­çš„å…«ä¸ªç‰¹å¾ä¸­è¡¨ç°å‡ºä¼˜äºæœ€æ–°æ–¹æ³•çš„æ•ˆæœï¼Œå¹¶åœ¨å…¶ä½™ä¸‰ä¸ªç‰¹å¾ä¸­è¡¨ç°ç›¸å½“ã€‚æ­¤å¤–ï¼ŒLLMèƒ½å¤ŸæˆåŠŸåœ°å¯¹é•¿å°¾æ‚éŸ³ç‰¹å¾è¿›è¡Œåˆ†ç±»ï¼Œè¿™äº›ç‰¹å¾ä»¥å‰çš„æ–¹æ³•éƒ½æ— æ³•å¤„ç†ã€‚è¿™äº›å‘ç°çªæ˜¾äº†éŸ³é¢‘LLMä½œä¸ºäººç±»å¿ƒè„ç—…å­¦å®¶çš„è¾…åŠ©å·¥å…·åœ¨å¢å¼ºå¿ƒè„ç—…è¯Šæ–­æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13884v1">PDF</a> 5 pages, 1 figure, and 3 tables. Submitted to IEEE&#x2F;ACM Conference on   Connected Health: Applications, Systems , and Engineering Technologies</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éŸ³é¢‘é¢†åŸŸå·²å±•ç°å‡ºå¯¹äººè„¸ã€éŸ³ä¹å’Œè‡ªç„¶ç¯å¢ƒçš„ä¼˜å¼‚è¯†åˆ«ä¸åˆ†æèƒ½åŠ›ã€‚å°½ç®¡ç”Ÿç‰©åŒ»å­¦é¢†åŸŸå­˜åœ¨å¯¹è¯¥æŠ€æœ¯æ¢ç´¢çš„å…´è¶£ï¼Œä½†è¯¥æ¨¡å‹ç†è§£å…¶å®ƒç±»å‹å£°éŸ³å¦‚åŒ»ç–—é¢†åŸŸç”Ÿç‰©åŒ»å­¦å£°éŸ³çš„æ½œåŠ›ä»æœªå¾—åˆ°å¹¿æ³›ç ”ç©¶ã€‚æœ¬ç ”ç©¶å…³æ³¨å¿ƒè„ç—…è¯Šæ–­çš„è¯­éŸ³å›¾è°±é¢†åŸŸã€‚å¤šæ•°ç°æœ‰ç¥ç»ç½‘ç»œæ¡†æ¶å±€é™äºå¿ƒè„æ‚éŸ³çš„åˆ†ç±»ï¼ˆå¥åº·ä¸ä¸å¥åº·ï¼‰ï¼Œä¸”æœªå¯¹å…¶ä»–å£°å­¦ç‰¹æ€§å¦‚æ—¶åºæ€§ã€åˆ†çº§åº¦ã€çŒ›çƒˆç¨‹åº¦ç­‰è¿›è¡Œé¢„æµ‹åˆ†æã€‚è€Œè¿™äº›å› ç´ å¯¹åŒ»ç”Ÿçš„å‡†ç¡®è¯Šæ–­èµ·åˆ°é‡è¦ä½œç”¨ã€‚æœ¬ç ”ç©¶çš„é¦–è¦ç›®çš„æ˜¯é’ˆå¯¹ä¸€ä¸ªåä¸ºQwen2çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹è¿›è¡Œä¼˜åŒ–å¤„ç†å¹¶ä½¿å…¶èƒ½å¤Ÿç¬¦åˆè¯­éŸ³å­¦å›¾åƒçš„è¦æ±‚ã€‚éšåé€šè¿‡ä¸€é¡¹åŒ…å«å¿ƒè¡€ç®¡ç–¾ç—…å¿ƒè„æ‚éŸ³ä¸“ä¸šæ ‡ç­¾ç‰¹å¾çš„å®éªŒå¯¹å…¶è¿›è¡Œæµ‹è¯•ã€‚æˆ‘ä»¬çš„å®éªŒè®¾è®¡ç›®æ ‡æ˜¯åˆ›å»ºæ›´ä¸ºç¨³å¥å’Œæ™®éé€‚ç”¨çš„ç³»ç»Ÿæ¨¡å‹ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­é‡‡ç”¨ä¸€ç§åŸºäºéŸ³é¢‘è¡¨ç°æ¨¡å‹çš„é¢„å¤„ç†åˆ†å‰²ç®—æ³•â€”â€”SSAMBAã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨¡å‹åœ¨å…¶ä¸­çš„å…«é¡¹ç‰¹å¾ä¸Šä¼˜äºå½“å‰ä¸»æµæ–¹æ³•ï¼Œå¹¶åœ¨å…¶ä½™ä¸‰é¡¹ç‰¹å¾ä¸Šè¡¨ç°ç›¸å½“ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ ·æœ¬æ•°æ®ä¸è¶³çš„æƒ…å†µä¸‹æˆåŠŸé¢„æµ‹äº†æ‚éŸ³çš„ç‰¹å¾ã€‚è¿™è¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰ä½œä¸ºå¿ƒè„ç—…è¯Šæ–­çš„è¾…åŠ©å·¥å…·è¿›è¡Œåº”ç”¨çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨éŸ³é¢‘é¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«å’Œåˆ†æäººç±»è¯­éŸ³ã€éŸ³ä¹å’Œç¯å¢ƒå£°éŸ³æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>åœ¨å¿ƒè„ç—…è¯Šæ–­ä¸­ï¼ŒLLMæ¨¡å‹åœ¨åˆ†ç±»å¿ƒè„æ‚éŸ³ç‰¹å¾æ–¹é¢å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿé¢„æµ‹å¤šç§å£°å­¦ç‰¹å¾ï¼Œå¦‚æ—¶åºæ€§ã€åˆ†çº§åº¦ç­‰ã€‚</li>
<li>Qwen2ç­‰å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†éœ€ç‰¹å®šåŒ–å¤„ç†ä»¥åŒ¹é…è¯­éŸ³å›¾åƒé›†è¦æ±‚ä»¥æå‡æ¨¡å‹çš„é€‚ç”¨æ€§ã€‚ </li>
<li>ç›¸æ¯”ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥åœ¨ç¼ºä¹è¶³å¤Ÿçš„æ ·æœ¬æ•°æ®æ—¶ä»ç„¶å±•ç°å‡ºç¨³å®šçš„é¢„æµ‹èƒ½åŠ›ã€‚è¿™æ„å‘³ç€å³ä½¿åœ¨åŒ»ç–—èµ„æºä¸è¶³çš„æƒ…å¢ƒä¸­ï¼Œè¯¥æ¨¡å‹ä»ç„¶æœ‰åº”ç”¨ä»·å€¼ã€‚</li>
<li>åˆ©ç”¨éŸ³é¢‘è¡¨ç°æ¨¡å‹å¦‚SSAMBAçš„é¢„å¤„ç†åˆ†å‰²ç®—æ³•å¯è¿›ä¸€æ­¥æå‡ç³»ç»Ÿçš„å™ªå£°ç¨³å¥æ€§å’Œæ™®éæ€§ã€‚è¿™ä¸€åšæ³•å¯ä»¥å¢å¼ºæ¨¡å‹å¤„ç†å®é™…ç¯å¢ƒä¸­å¯èƒ½å‡ºç°çš„ä¸ç¡®å®šæ€§å› ç´ çš„èƒ½åŠ›ã€‚</li>
<li>æœ¬ç ”ç©¶ä¸­æå‡ºçš„å¤§å‹è¯­è¨€æ¨¡å‹å¯ä½œä¸ºä¸€ä¸ªæœ‰åŠ›çš„å·¥å…·ååŠ©å¿ƒè„ç—…åŒ»ç”Ÿè¿›è¡Œå‡†ç¡®è¯Šæ–­ï¼Œå¹¶æœ‰åŠ©äºæ”¹è¿›å¿ƒè„ç—…è¯Šæ–­å’Œæ²»ç–—æµç¨‹çš„è´¨é‡ã€‚è¿™å¯èƒ½ä½¿å¾—åŒ»ç”Ÿçš„è¯Šæ–­æ›´ä¸ºé«˜æ•ˆï¼ŒåŒæ—¶ä¸ºæ‚£è€…åœ¨æ²»ç–—ä¸­èŠ‚çœäº†æ—¶é—´å’Œèµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13884">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f8b6ddbf11cec16873f6f2fa25c950a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb00e99b8c59949eb4fc81fa49f6331d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56c52d31cdbf7d4de1fc9e71ac62e7c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee2fb97a62a470f9eddb6a084f39947.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1bb8140f02f89ac9142c260a937b8402.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Predicting-Compact-Phrasal-Rewrites-with-Large-Language-Models-for-ASR-Post-Editing"><a href="#Predicting-Compact-Phrasal-Rewrites-with-Large-Language-Models-for-ASR-Post-Editing" class="headerlink" title="Predicting Compact Phrasal Rewrites with Large Language Models for ASR   Post Editing"></a>Predicting Compact Phrasal Rewrites with Large Language Models for ASR   Post Editing</h2><p><strong>Authors:Hao Zhang, Felix Stahlberg, Shankar Kumar</strong></p>
<p>Large Language Models (LLMs) excel at rewriting tasks such as text style transfer and grammatical error correction. While there is considerable overlap between the inputs and outputs in these tasks, the decoding cost still increases with output length, regardless of the amount of overlap. By leveraging the overlap between the input and the output, Kaneko and Okazaki (2023) proposed model-agnostic edit span representations to compress the rewrites to save computation. They reported an output length reduction rate of nearly 80% with minimal accuracy impact in four rewriting tasks. In this paper, we propose alternative edit phrase representations inspired by phrase-based statistical machine translation. We systematically compare our phrasal representations with their span representations. We apply the LLM rewriting model to the task of Automatic Speech Recognition (ASR) post editing and show that our target-phrase-only edit representation has the best efficiency-accuracy trade-off. On the LibriSpeech test set, our method closes 50-60% of the WER gap between the edit span model and the full rewrite model while losing only 10-20% of the length reduction rate of the edit span model. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿æ–‡æœ¬é‡å†™ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬é£æ ¼è½¬æ¢å’Œè¯­æ³•é”™è¯¯ä¿®æ­£ã€‚è™½ç„¶è¿™äº›ä»»åŠ¡çš„è¾“å…¥å’Œè¾“å‡ºä¹‹é—´å­˜åœ¨å¤§é‡é‡å ï¼Œä½†è§£ç æˆæœ¬ä»ä¼šéšç€è¾“å‡ºçš„å¢é•¿è€Œå¢åŠ ï¼Œæ— è®ºé‡å ç¨‹åº¦å¦‚ä½•ã€‚Kanekoå’ŒOkazakiï¼ˆ2023å¹´ï¼‰åˆ©ç”¨è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„é‡å ï¼Œæå‡ºäº†ä¸æ¨¡å‹æ— å…³çš„ç¼–è¾‘è·¨åº¦è¡¨ç¤ºæ³•ï¼Œä»¥å‹ç¼©é‡å†™å†…å®¹ï¼Œä»è€ŒèŠ‚çœè®¡ç®—ã€‚ä»–ä»¬æŠ¥å‘Šç§°ï¼Œåœ¨å››é¡¹é‡å†™ä»»åŠ¡ä¸­ï¼Œè¾“å‡ºé•¿åº¦ç¼©å‡ç‡æ¥è¿‘80%ï¼Œä¸”å¯¹ç²¾åº¦çš„å½±å“å¾®ä¹å…¶å¾®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å—åˆ°åŸºäºçŸ­è¯­çš„ç»Ÿè®¡æœºå™¨ç¿»è¯‘çš„å¯å‘ï¼Œæå‡ºäº†å¦ä¸€ç§ç¼–è¾‘çŸ­è¯­è¡¨ç¤ºæ³•ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°æ¯”è¾ƒäº†æˆ‘ä»¬çš„çŸ­è¯­è¡¨ç¤ºæ³•ä¸ä»–ä»¬çš„è·¨åº¦è¡¨ç¤ºæ³•ã€‚æˆ‘ä»¬å°†LLMé‡å†™æ¨¡å‹åº”ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„åç¼–è¾‘ä»»åŠ¡ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„ç›®æ ‡çŸ­è¯­ä»…ç¼–è¾‘è¡¨ç¤ºæ³•åœ¨æ•ˆç‡ä¸å‡†ç¡®æ€§æ–¹é¢è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ã€‚åœ¨LibriSpeechæµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¼©å°äº†ç¼–è¾‘è·¨åº¦æ¨¡å‹å’Œå…¨é‡å†™æ¨¡å‹ä¹‹é—´çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å·®è·çš„50-60%ï¼ŒåŒæ—¶åªå¤±å»äº†ç¼–è¾‘è·¨åº¦æ¨¡å‹çš„é•¿åº¦ç¼©å‡ç‡çš„10-20%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13831v1">PDF</a> accepted by ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿æ–‡æœ¬æ”¹å†™ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬é£æ ¼è½¬æ¢å’Œè¯­æ³•é”™è¯¯ä¿®æ­£ã€‚å°½ç®¡è¾“å…¥å’Œè¾“å‡ºä¹‹é—´å­˜åœ¨å¤§é‡é‡å ï¼Œè§£ç æˆæœ¬ä»éšè¾“å‡ºé•¿åº¦çš„å¢åŠ è€Œå¢åŠ ã€‚Kanekoå’ŒOkazakiï¼ˆ2023ï¼‰æå‡ºæ¨¡å‹æ— å…³çš„ç¼–è¾‘è·¨åº¦è¡¨ç¤ºæ³•ï¼Œåˆ©ç”¨è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„é‡å æ¥å‹ç¼©é‡å†™å†…å®¹ï¼Œä»¥èŠ‚çœè®¡ç®—ã€‚ä»–ä»¬æŠ¥å‘Šç§°ï¼Œåœ¨å››é¡¹é‡å†™ä»»åŠ¡ä¸­ï¼Œè¾“å‡ºé•¿åº¦ç¼©å‡ç‡æ¥è¿‘80%ï¼Œä¸”å‡†ç¡®æ€§å½±å“æå°ã€‚æœ¬æ–‡å—åŸºäºçŸ­è¯­ç»Ÿè®¡æœºå™¨ç¿»è¯‘çš„å¯å‘ï¼Œæå‡ºæ›¿ä»£çš„ç¼–è¾‘çŸ­è¯­è¡¨ç¤ºæ³•ã€‚æˆ‘ä»¬ç³»ç»Ÿåœ°æ¯”è¾ƒäº†åŸºäºçŸ­è¯­çš„è¡¨ç¤ºæ³•ä¸åŸºäºè·¨åº¦çš„è¡¨ç¤ºæ³•ã€‚æˆ‘ä»¬å°†LLMé‡å†™æ¨¡å‹åº”ç”¨äºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„åç¼–è¾‘ä»»åŠ¡ï¼Œå¹¶å‘ç°ç›®æ ‡çŸ­è¯­ä»…ç¼–è¾‘è¡¨ç¤ºæ³•å…·æœ‰æœ€ä½³çš„æ•ˆç‡-å‡†ç¡®æ€§æƒè¡¡ã€‚åœ¨LibriSpeechæµ‹è¯•é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç¼©å°äº†ç¼–è¾‘è·¨åº¦æ¨¡å‹å’Œå…¨é‡å†™æ¨¡å‹ä¹‹é—´çº¦50-60%çš„è¯é”™è¯¯ç‡å·®è·ï¼ŒåŒæ—¶ä»…æŸå¤±äº†ç¼–è¾‘è·¨åº¦æ¨¡å‹çš„10-20%é•¿åº¦ç¼©å‡ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ–‡æœ¬æ”¹å†™ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¦‚æ–‡æœ¬é£æ ¼è½¬æ¢å’Œè¯­æ³•é”™è¯¯ä¿®æ­£ã€‚</li>
<li>è§£ç æˆæœ¬éšè¾“å‡ºé•¿åº¦å¢åŠ è€Œå¢åŠ ï¼Œå³ä½¿è¾“å…¥å’Œè¾“å‡ºå­˜åœ¨å¤§é‡é‡å ã€‚</li>
<li>Kanekoå’ŒOkazakiï¼ˆ2023ï¼‰æå‡ºäº†æ¨¡å‹æ— å…³çš„ç¼–è¾‘è·¨åº¦è¡¨ç¤ºæ³•ï¼Œåˆ©ç”¨è¾“å…¥å’Œè¾“å‡ºçš„é‡å æ¥å‹ç¼©é‡å†™å†…å®¹ã€‚</li>
<li>è¾“å‡ºé•¿åº¦ç¼©å‡ç‡æ¥è¿‘80%ï¼ŒåŒæ—¶å‡†ç¡®æ€§å½±å“è¾ƒå°ã€‚</li>
<li>æœ¬æ–‡å—çŸ­è¯­ç»Ÿè®¡æœºå™¨ç¿»è¯‘çš„å¯å‘ï¼Œæå‡ºäº†ç¼–è¾‘çŸ­è¯­è¡¨ç¤ºæ³•ã€‚</li>
<li>åœ¨ASRåç¼–è¾‘ä»»åŠ¡ä¸­ï¼Œç›®æ ‡çŸ­è¯­ä»…ç¼–è¾‘è¡¨ç¤ºæ³•å…·æœ‰æœ€ä½³æ•ˆç‡-å‡†ç¡®æ€§æƒè¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13831">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-299a9baf49c5ae4ddb79f145d25d5ad0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7418ca64b1abcded84714dfc256bf146.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b966b0707850d52930aaa7a75a59d64.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8b20ad09238d81319cc09492595c4334.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-632b2ec62cf0458bed9cbbcc186163a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e60a33342c813d6e249d05a7d73a8ed3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-LLMs-for-Governance-with-Human-Oversight-Evaluating-and-Aligning-LLMs-on-Expert-Classification-of-Climate-Misinformation-for-Detecting-False-or-Misleading-Claims-about-Climate-Change"><a href="#Enhancing-LLMs-for-Governance-with-Human-Oversight-Evaluating-and-Aligning-LLMs-on-Expert-Classification-of-Climate-Misinformation-for-Detecting-False-or-Misleading-Claims-about-Climate-Change" class="headerlink" title="Enhancing LLMs for Governance with Human Oversight: Evaluating and   Aligning LLMs on Expert Classification of Climate Misinformation for   Detecting False or Misleading Claims about Climate Change"></a>Enhancing LLMs for Governance with Human Oversight: Evaluating and   Aligning LLMs on Expert Classification of Climate Misinformation for   Detecting False or Misleading Claims about Climate Change</h2><p><strong>Authors:Mowafak Allaham, Ayse D. Lokmanoglu, Sol P. Hart, Erik C. Nisbet</strong></p>
<p>Climate misinformation is a problem that has the potential to be substantially aggravated by the development of Large Language Models (LLMs). In this study we evaluate the potential for LLMs to be part of the solution for mitigating online dis&#x2F;misinformation rather than the problem. Employing a public expert annotated dataset and a curated sample of social media content we evaluate the performance of proprietary vs. open source LLMs on climate misinformation classification task, comparing them to existing climate-focused computer-assisted tools and expert assessments. Results show (1) state-of-the-art (SOTA) open-source models substantially under-perform in classifying climate misinformation compared to proprietary models, (2) existing climate-focused computer-assisted tools leveraging expert-annotated datasets continues to outperform many of proprietary models, including GPT-4o, and (3) demonstrate the efficacy and generalizability of fine-tuning GPT-3.5-turbo on expert annotated dataset in classifying claims about climate change at the equivalency of climate change experts with over 20 years of experience in climate communication. These findings highlight 1) the importance of incorporating human-oversight, such as incorporating expert-annotated datasets in training LLMs, for governance tasks that require subject-matter expertise like classifying climate misinformation, and 2) the potential for LLMs in facilitating civil society organizations to engage in various governance tasks such as classifying false or misleading claims in domains beyond climate change such as politics and health science. </p>
<blockquote>
<p>å…³äºæ°”å€™è¯¯ä¿¡æ¯çš„å¤„ç†æ˜¯ä¸€ä¸ªå¯èƒ½ä¼šå› å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•è€Œè¿›ä¸€æ­¥åŠ å‰§çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†LLMä½œä¸ºè§£å†³æ–¹æ¡ˆçš„ä¸€éƒ¨åˆ†ï¼Œä»¥å‡è½»åœ¨çº¿é”™è¯¯ä¿¡æ¯çš„ç¨‹åº¦ï¼Œè€ŒéåŠ å‰§é—®é¢˜ã€‚æˆ‘ä»¬é‡‡ç”¨å…¬å…±ä¸“å®¶æ³¨é‡Šçš„æ•°æ®é›†å’Œç¤¾äº¤åª’ä½“å†…å®¹çš„ç²¾é€‰æ ·æœ¬ï¼Œè¯„ä¼°ä¸“æœ‰ä¸å¼€æºLLMåœ¨æ°”å€™é”™è¯¯ä¿¡æ¯åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¹¶å°†å…¶ä¸ç°æœ‰çš„ä¸“æ³¨äºæ°”å€™çš„è®¡ç®—æœºè¾…åŠ©å·¥å…·å’Œä¸“å®¶è¯„ä¼°è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œï¼ˆ1ï¼‰æœ€å…ˆè¿›çš„å¼€æºæ¨¡å‹åœ¨æ°”å€™è¯¯ä¿¡æ¯çš„åˆ†ç±»æ–¹é¢æ˜¾è‘—è½åäºä¸“æœ‰æ¨¡å‹ï¼›ï¼ˆ2ï¼‰ç°æœ‰çš„ä»¥æ°”å€™ä¸ºé‡ç‚¹çš„è®¡ç®—æœºè¾…åŠ©å·¥å…·ï¼Œåˆ©ç”¨ä¸“å®¶æ³¨é‡Šçš„æ•°æ®é›†ç»§ç»­è¶…è¶Šè®¸å¤šä¸“æœ‰æ¨¡å‹ï¼ŒåŒ…æ‹¬GPT-4oï¼›ï¼ˆ3ï¼‰è¯æ˜äº†åœ¨ä¸“å®¶æ³¨é‡Šçš„æ•°æ®é›†ä¸Šå¾®è°ƒGPT-3.5 turboåœ¨æ°”å€™å˜åŒ–ä¸»å¼ åˆ†ç±»æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œå¯æ¨å¹¿æ€§ï¼Œç›¸å½“äºæ‹¥æœ‰è¶…è¿‡äºŒåå¹´æ°”å€™ä¼ æ’­ç»éªŒçš„ä¸“å®¶æ°´å¹³ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ï¼ˆ1ï¼‰èå…¥äººç±»ç›‘ç£çš„é‡è¦æ€§ï¼Œä¾‹å¦‚åœ¨è®­ç»ƒLLMæ—¶èå…¥ä¸“å®¶æ³¨é‡Šçš„æ•°æ®é›†ï¼Œç”¨äºéœ€è¦ä¸“ä¸šçŸ¥è¯†çš„æ²»ç†ä»»åŠ¡å¦‚æ°”å€™ä¿¡æ¯é”™è¯¯åˆ†ç±»ï¼›ï¼ˆ2ï¼‰LLMçš„æ½œåŠ›åœ¨äºä¿ƒè¿›å…¬æ°‘ç¤¾ä¼šç»„ç»‡å‚ä¸å„ç§æ²»ç†ä»»åŠ¡ï¼Œå¦‚æ”¿æ²»å’Œç”Ÿå‘½ç§‘å­¦ç­‰é¢†åŸŸè™šå‡æˆ–è¯¯å¯¼æ€§ä¸»å¼ çš„åˆ†ç±»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13802v1">PDF</a> Accepted to the AI Governance Workshop at AAAI 2025</p>
<p><strong>Summary</strong>ï¼š<br>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‘å±•æœ‰å¯èƒ½åŠ å‰§æ°”å€™è°£è¨€é—®é¢˜ã€‚æœ¬ç ”ç©¶è¯„ä¼°LLMåœ¨ç¼“è§£åœ¨çº¿é”™è¯¯ä¿¡æ¯æ–¹é¢çš„ä½œç”¨ï¼Œè€Œä¸æ˜¯åŠ å‰§é—®é¢˜ã€‚é€šè¿‡å…¬å…±ä¸“å®¶æ³¨é‡Šæ•°æ®é›†å’Œç¤¾äº¤åª’ä½“å†…å®¹çš„æ ·æœ¬ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸“æœ‰ä¸å¼€æºLLMåœ¨æ°”å€™è°£è¨€åˆ†ç±»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä»¥åŠä¸ç°æœ‰çš„æ°”å€™ç›¸å…³è®¡ç®—æœºè¾…åŠ©å·¥å…·å’Œä¸“å®¶è¯„ä¼°ç›¸æ¯”ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå…ˆè¿›çš„å¼€æºæ¨¡å‹åœ¨åˆ†ç±»æ°”å€™è°£è¨€æ–¹é¢æ˜¾è‘—è½åäºä¸“æœ‰æ¨¡å‹ï¼›ç°æœ‰çš„æ°”å€™ç›¸å…³è®¡ç®—æœºè¾…åŠ©å·¥å…·ä»ç„¶ä¼˜äºè®¸å¤šä¸“æœ‰æ¨¡å‹ï¼ŒåŒ…æ‹¬GPT-4oï¼›ç²¾ç»†è°ƒæ•´GPT-3.5-turboåœ¨ä¸“å®¶æ³¨é‡Šæ•°æ®é›†ä¸Šå¯æœ‰æ•ˆä¸”æ™®éåœ°åˆ†ç±»æ°”å€™å˜åŒ–ä¸»å¼ ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†èå…¥äººç±»ç›‘ç£ï¼ˆå¦‚ä¸“å®¶æ³¨é‡Šæ•°æ®é›†ï¼‰ä»¥è®­ç»ƒLLMçš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ä¸“ä¸šçŸ¥è¯†çš„ä»»åŠ¡ä¸­å¦‚åˆ†ç±»æ°”å€™è°£è¨€ï¼›ä»¥åŠLLMåœ¨å¸®åŠ©æ°‘é—´ç¤¾ä¼šç»„ç»‡å‚ä¸å„ç§æ²»ç†ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»æ°”å€™ä»¥å¤–é¢†åŸŸçš„è™šå‡æˆ–è¯¯å¯¼æ€§ä¸»å¼ ï¼‰çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>LLMçš„å‘å±•æœ‰å¯èƒ½åŠ å‰§æ°”å€™è°£è¨€é—®é¢˜ã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°LLMåœ¨ç¼“è§£åœ¨çº¿é”™è¯¯ä¿¡æ¯æ–¹é¢çš„ä½œç”¨ã€‚</li>
<li>ä¸“æœ‰LLMæ¨¡å‹åœ¨æ°”å€™è°£è¨€åˆ†ç±»ä»»åŠ¡ä¸Šæ€§èƒ½ä¼˜äºå¼€æºæ¨¡å‹ã€‚</li>
<li>ç°æœ‰çš„æ°”å€™ç›¸å…³è®¡ç®—æœºè¾…åŠ©å·¥å…·è¡¨ç°ä¼˜äºè®¸å¤šä¸“æœ‰LLMæ¨¡å‹ã€‚</li>
<li>ç²¾ç»†è°ƒæ•´GPT-3.5-turboåœ¨ä¸“å®¶æ³¨é‡Šæ•°æ®é›†ä¸Šå¯æœ‰æ•ˆåˆ†ç±»æ°”å€™å˜åŒ–ä¸»å¼ ã€‚</li>
<li>èå…¥äººç±»ç›‘ç£ï¼ˆå¦‚ä¸“å®¶æ³¨é‡Šæ•°æ®é›†ï¼‰å¯¹äºè®­ç»ƒLLMè‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦ä¸“ä¸šçŸ¥è¯†çš„ä»»åŠ¡ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fbcb4ea7f53c7262f8731852d125e92f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81d830b64e1ecbdaa28ea6f8dbac39a8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-83fe4a0fd27cd4ade1d39977a927b0e7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Tune-In-Act-Up-Exploring-the-Impact-of-Audio-Modality-Specific-Edits-on-Large-Audio-Language-Models-in-Jailbreak"><a href="#Tune-In-Act-Up-Exploring-the-Impact-of-Audio-Modality-Specific-Edits-on-Large-Audio-Language-Models-in-Jailbreak" class="headerlink" title="Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits   on Large Audio Language Models in Jailbreak"></a>Tune In, Act Up: Exploring the Impact of Audio Modality-Specific Edits   on Large Audio Language Models in Jailbreak</h2><p><strong>Authors:Erjia Xiao, Hao Cheng, Jing Shao, Jinhao Duan, Kaidi Xu, Le Yang, Jindong Gu, Renjing Xu</strong></p>
<p>Large Language Models (LLMs) demonstrate remarkable zero-shot performance across various natural language processing tasks. The integration of multimodal encoders extends their capabilities, enabling the development of Multimodal Large Language Models that process vision, audio, and text. However, these capabilities also raise significant security concerns, as these models can be manipulated to generate harmful or inappropriate content through jailbreak. While extensive research explores the impact of modality-specific input edits on text-based LLMs and Large Vision-Language Models in jailbreak, the effects of audio-specific edits on Large Audio-Language Models (LALMs) remain underexplored. Hence, this paper addresses this gap by investigating how audio-specific edits influence LALMs inference regarding jailbreak. We introduce the Audio Editing Toolbox (AET), which enables audio-modality edits such as tone adjustment, word emphasis, and noise injection, and the Edited Audio Datasets (EADs), a comprehensive audio jailbreak benchmark. We also conduct extensive evaluations of state-of-the-art LALMs to assess their robustness under different audio edits. This work lays the groundwork for future explorations on audio-modality interactions in LALMs security. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚å¤šæ¨¡æ€ç¼–ç å™¨çš„é›†æˆæ‰©å±•äº†å®ƒä»¬çš„åŠŸèƒ½ï¼Œæ¨åŠ¨äº†èƒ½å¤Ÿå¤„ç†è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ã€‚ç„¶è€Œï¼Œè¿™äº›åŠŸèƒ½ä¹Ÿå¼•å‘äº†ä¸¥é‡çš„å®‰å…¨æ‹…å¿§ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹å¯ä»¥é€šè¿‡è¶Šç‹±ç”Ÿæˆæœ‰å®³æˆ–ä¸å½“å†…å®¹è€Œå—åˆ°æ“çºµã€‚è™½ç„¶å¤§é‡ç ”ç©¶æ¢è®¨äº†æ¨¡æ€ç‰¹å®šè¾“å…¥ç¼–è¾‘å¯¹åŸºäºæ–‡æœ¬çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å½±å“ï¼Œä½†é’ˆå¯¹ç‰¹å®šéŸ³é¢‘ç¼–è¾‘å¯¹å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„å½±å“ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚å› æ­¤ï¼Œæœ¬æ–‡é€šè¿‡ç ”ç©¶ç‰¹å®šéŸ³é¢‘ç¼–è¾‘å¦‚ä½•å½±å“LALMå…³äºè¶Šç‹±çš„æ¨ç†æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬å¼•å…¥äº†éŸ³é¢‘ç¼–è¾‘å·¥å…·ç®±ï¼ˆAETï¼‰ï¼Œå®ƒæ”¯æŒéŸ³é¢‘æ¨¡æ€ç¼–è¾‘ï¼Œå¦‚éŸ³è°ƒè°ƒæ•´ã€å•è¯å¼ºè°ƒå’Œå™ªå£°æ³¨å…¥ï¼Œä»¥åŠç¼–è¾‘åçš„éŸ³é¢‘æ•°æ®é›†ï¼ˆEADï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„éŸ³é¢‘è¶Šç‹±åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬è¿˜å¯¹æœ€å…ˆè¿›çš„LALMè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œä»¥æµ‹è¯•å®ƒä»¬åœ¨ä¸åŒçš„éŸ³é¢‘ç¼–è¾‘ä¸‹çš„ç¨³å¥æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºæœªæ¥åœ¨LALMå®‰å…¨æ€§æ–¹é¢æ¢ç´¢éŸ³é¢‘æ¨¡æ€äº¤äº’å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13772v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å±•ç°å‡ºè·¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„å‡ºè‰²é›¶æ ·æœ¬æ€§èƒ½ã€‚é€šè¿‡èå…¥å¤šæ¨¡æ€ç¼–ç å™¨ï¼Œè¿›ä¸€æ­¥å¼€å‘å‡ºèƒ½å¤Ÿå¤„ç†è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›èƒ½åŠ›ä¹Ÿå¼•å‘äº†ä¸¥é‡çš„å®‰å…¨æ‹…å¿§ï¼Œå› ä¸ºæ¨¡å‹å¯èƒ½ä¼šè¢«æ“çºµä»¥ç”Ÿæˆæœ‰å®³æˆ–ä¸å½“å†…å®¹ã€‚å°½ç®¡å·²æœ‰å¤§é‡ç ”ç©¶æ¢è®¨äº†æ¨¡æ€ç‰¹å®šè¾“å…¥ç¼–è¾‘å¯¹æ–‡æœ¬å‹LLMå’Œå¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å½±å“ï¼Œä½†é’ˆå¯¹éŸ³é¢‘ç‰¹å®šç¼–è¾‘å¯¹å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„å½±å“çš„ç ”ç©¶ä»æ˜¾ä¸è¶³ã€‚æœ¬æ–‡æ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œç ”ç©¶éŸ³é¢‘ç‰¹å®šç¼–è¾‘å¦‚ä½•å½±å“LALMåœ¨è¶Šç‹±åœºæ™¯ä¸­çš„æ¨æ–­ã€‚å¼•å…¥äº†éŸ³é¢‘ç¼–è¾‘å·¥å…·ç®±ï¼ˆAETï¼‰å’Œç¼–è¾‘éŸ³é¢‘æ•°æ®é›†ï¼ˆEADsï¼‰ï¼Œå¹¶å¯¹å½“å‰å…ˆè¿›çš„LALMè¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œä»¥æµ‹è¯•å…¶åœ¨ä¸åŒéŸ³é¢‘ç¼–è¾‘ä¸‹çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºè·¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„å‡ºè‰²é›¶æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>å¤šæ¨¡æ€ç¼–ç å™¨çš„èå…¥æ‰©å±•äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¤„ç†è§†è§‰ã€éŸ³é¢‘å’Œæ–‡æœ¬ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å­˜åœ¨è¢«æ“çºµç”Ÿæˆæœ‰å®³æˆ–ä¸å½“å†…å®¹çš„å®‰å…¨é£é™©ã€‚</li>
<li>å¯¹éŸ³é¢‘ç‰¹å®šç¼–è¾‘å½±å“å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰çš„ç ”ç©¶ä»ç„¶ä¸è¶³ã€‚</li>
<li>å¼•å…¥éŸ³é¢‘ç¼–è¾‘å·¥å…·ç®±ï¼ˆAETï¼‰å’Œç¼–è¾‘éŸ³é¢‘æ•°æ®é›†ï¼ˆEADsï¼‰ä¸ºç ”ç©¶æä¾›å·¥å…·å’Œæ•°æ®æ”¯æŒã€‚</li>
<li>å¹¿æ³›è¯„ä¼°æ˜¾ç¤ºï¼Œå½“å‰å…ˆè¿›çš„LALMåœ¨ä¸åŒéŸ³é¢‘ç¼–è¾‘ä¸‹çš„ç¨³å¥æ€§æœ‰å¾…æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-436e0eff436bb6cfdd3e2e98f5e1e50a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb2cab7ab51b7a127f6c378b5a208f42.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f39647dfb72b49f89747a0c0542f7930.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UGMathBench-A-Diverse-and-Dynamic-Benchmark-for-Undergraduate-Level-Mathematical-Reasoning-with-Large-Language-Models"><a href="#UGMathBench-A-Diverse-and-Dynamic-Benchmark-for-Undergraduate-Level-Mathematical-Reasoning-with-Large-Language-Models" class="headerlink" title="UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level   Mathematical Reasoning with Large Language Models"></a>UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level   Mathematical Reasoning with Large Language Models</h2><p><strong>Authors:Xin Xu, Jiaxin Zhang, Tianhao Chen, Zitong Chao, Jishan Hu, Can Yang</strong></p>
<p>Large Language Models (LLMs) have made significant strides in mathematical reasoning, underscoring the need for a comprehensive and fair evaluation of their capabilities. However, existing benchmarks often fall short, either lacking extensive coverage of undergraduate-level mathematical problems or probably suffering from test-set contamination. To address these issues, we introduce UGMathBench, a diverse and dynamic benchmark specifically designed for evaluating undergraduate-level mathematical reasoning with LLMs. UGMathBench comprises 5,062 problems across 16 subjects and 111 topics, featuring 10 distinct answer types. Each problem includes three randomized versions, with additional versions planned for release as leading open-source LLMs become saturated in UGMathBench. Furthermore, we propose two key metrics: effective accuracy (EAcc), which measures the percentage of correctly solved problems across all three versions, and reasoning gap ($\Delta$), which assesses reasoning robustness by calculating the difference between the average accuracy across all versions and EAcc. Our extensive evaluation of 23 leading LLMs reveals that the highest EAcc achieved is 56.3% by OpenAI-o1-mini, with large $\Delta$ values observed across different models. This highlights the need for future research aimed at developing â€œlarge reasoning modelsâ€ with high EAcc and $\Delta &#x3D; 0$. We anticipate that the release of UGMathBench, along with its detailed evaluation codes, will serve as a valuable resource to advance the development of LLMs in solving mathematical problems. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™å¼ºè°ƒäº†å¯¹å®ƒä»¬çš„èƒ½åŠ›è¿›è¡Œå…¨é¢å…¬å¹³è¯„ä¼°çš„å¿…è¦æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸å­˜åœ¨ä¸è¶³ï¼Œè¦ä¹ˆæ²¡æœ‰å¹¿æ³›è¦†ç›–æœ¬ç§‘æ•°å­¦é—®é¢˜å’Œå¯èƒ½å­˜åœ¨çš„æµ‹è¯•é›†æ±¡æŸ“ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†UGMathBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨äºè¯„ä¼°æœ¬ç§‘æ•°å­¦æ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¤šå…ƒåŠ¨æ€åŸºå‡†æµ‹è¯•ã€‚UGMathBenchåŒ…å«5,062ä¸ªè·¨è¶Š16ä¸ªå­¦ç§‘å’Œ111ä¸ªä¸»é¢˜çš„é—®é¢˜ï¼ŒåŒ…å«10ç§ä¸åŒçš„ç­”æ¡ˆç±»å‹ã€‚æ¯ä¸ªé—®é¢˜åŒ…æ‹¬ä¸‰ä¸ªéšæœºç‰ˆæœ¬ï¼Œéšç€é¢†å…ˆçš„å¼€æºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨UGMathBenchä¸­è¶‹äºé¥±å’Œï¼Œæˆ‘ä»¬è¿˜è®¡åˆ’å‘å¸ƒæ›´å¤šç‰ˆæœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªå…³é”®æŒ‡æ ‡ï¼šæœ‰æ•ˆå‡†ç¡®ç‡ï¼ˆEAccï¼‰ï¼Œç”¨äºè¡¡é‡æ‰€æœ‰ä¸‰ä¸ªç‰ˆæœ¬ä¸­æ­£ç¡®è§£å†³çš„é—®é¢˜çš„ç™¾åˆ†æ¯”ï¼›æ¨ç†å·®è·ï¼ˆÎ”ï¼‰ï¼Œé€šè¿‡è®¡ç®—æ‰€æœ‰ç‰ˆæœ¬å¹³å‡å‡†ç¡®ç‡å’ŒEAccä¹‹é—´çš„å·®å¼‚æ¥è¯„ä¼°æ¨ç†ç¨³å¥æ€§ã€‚æˆ‘ä»¬å¯¹23ä¸ªé¢†å…ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒOpenAI-o1-miniçš„æœ€é«˜æœ‰æ•ˆå‡†ç¡®ç‡è¾¾åˆ°äº†56.3%ï¼Œä¸åŒæ¨¡å‹ä¹‹é—´è§‚å¯Ÿåˆ°è¾ƒå¤§çš„Î”å€¼ã€‚è¿™å¼ºè°ƒäº†æœªæ¥ç ”ç©¶éœ€è¦è‡´åŠ›äºå¼€å‘å…·æœ‰é«˜æœ‰æ•ˆå‡†ç¡®ç‡å’ŒÎ”&#x3D;0çš„â€œå¤§å‹æ¨ç†æ¨¡å‹â€ã€‚æˆ‘ä»¬é¢„æœŸï¼ŒUGMathBenchåŠå…¶è¯¦ç»†çš„è¯„ä¼°ä»£ç çš„å‘å¸ƒå°†ä½œä¸ºæ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹è§£å†³æ•°å­¦é—®é¢˜çš„é‡è¦èµ„æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13766v1">PDF</a> Accepted to ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>LLMsåœ¨æ•°å­¦æ¨ç†æ–¹é¢å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰è¯„ä¼°æ ‡å‡†å­˜åœ¨ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæå‡ºUGMathBenchè¯„ä¼°å¹³å°ï¼Œæ¶µç›–5062ä¸ªé—®é¢˜å’Œå¤šç§é¢˜å‹ï¼Œæ—¨åœ¨å…¨é¢å…¬å¹³åœ°è¯„ä¼°LLMè§£å†³æœ¬ç§‘æ•°å­¦é—®é¢˜çš„èƒ½åŠ›ã€‚å¼•å…¥æœ‰æ•ˆå‡†ç¡®åº¦ï¼ˆEAccï¼‰å’Œæ¨ç†å·®è·ï¼ˆÎ”ï¼‰ä¸¤ä¸ªå…³é”®æŒ‡æ ‡ï¼Œå¯¹ç°æœ‰23ä¸ªLLMæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°å­˜åœ¨æå‡ç©ºé—´ã€‚æœŸå¾…UGMathBenchåŠå…¶è¯„ä¼°ä»£ç èƒ½ä¿ƒè¿›LLMåœ¨æ•°å­¦é—®é¢˜è§£ç­”æ–¹é¢çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsåœ¨æ•°å­¦æ¨ç†èƒ½åŠ›ä¸Šå–å¾—æ˜¾è‘—è¿›æ­¥ï¼Œéœ€è¦å…¨é¢å…¬å¹³çš„è¯„ä¼°æ ‡å‡†ã€‚</li>
<li>UGMathBenchæ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼°LLMè§£å†³æœ¬ç§‘æ•°å­¦é—®é¢˜èƒ½åŠ›çš„å¹³å°ï¼ŒåŒ…å«å¤šæ ·åŒ–å’ŒåŠ¨æ€çš„é—®é¢˜åº“ã€‚</li>
<li>å¼•å…¥æœ‰æ•ˆå‡†ç¡®åº¦ï¼ˆEAccï¼‰å’Œæ¨ç†å·®è·ï¼ˆÎ”ï¼‰ä¸¤ä¸ªå…³é”®æŒ‡æ ‡æ¥è¯„ä¼°LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>UGMathBenchå¯¹ç°æœ‰23ä¸ªLLMæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°æœ€é«˜æœ‰æ•ˆå‡†ç¡®åº¦ä¸ºOpenAI-o1-miniçš„56.3%ï¼Œå­˜åœ¨æå‡ç©ºé—´ã€‚</li>
<li>éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘å…·æœ‰é«˜æ°´å¹³æœ‰æ•ˆå‡†ç¡®åº¦å’Œé›¶æ¨ç†å·®è·çš„â€œå¤§å‹æ¨ç†æ¨¡å‹â€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13766">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d602c434684a466980ffeca8b9fdb13.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-16337136c74c57cf4736eaf96ee114d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d443191e51ae66d1a979644cf6279b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1f0e37c6af9aff603b3ab498a085240.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="RAMQA-A-Unified-Framework-for-Retrieval-Augmented-Multi-Modal-Question-Answering"><a href="#RAMQA-A-Unified-Framework-for-Retrieval-Augmented-Multi-Modal-Question-Answering" class="headerlink" title="RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question   Answering"></a>RAMQA: A Unified Framework for Retrieval-Augmented Multi-Modal Question   Answering</h2><p><strong>Authors:Yang Bai, Christan Earl Grant, Daisy Zhe Wang</strong></p>
<p>Multi-modal retrieval-augmented Question Answering (MRAQA), integrating text and images, has gained significant attention in information retrieval (IR) and natural language processing (NLP). Traditional ranking methods rely on small encoder-based language models, which are incompatible with modern decoder-based generative large language models (LLMs) that have advanced various NLP tasks. To bridge this gap, we propose RAMQA, a unified framework combining learning-to-rank methods with generative permutation-enhanced ranking techniques. We first train a pointwise multi-modal ranker using LLaVA as the backbone. Then, we apply instruction tuning to train a LLaMA model for re-ranking the top-k documents using an innovative autoregressive multi-task learning approach. Our generative ranking model generates re-ranked document IDs and specific answers from document candidates in various permutations. Experiments on two MRAQA benchmarks, WebQA and MultiModalQA, show significant improvements over strong baselines, highlighting the effectiveness of our approach. Code and data are available at: <a target="_blank" rel="noopener" href="https://github.com/TonyBY/RAMQA">https://github.com/TonyBY/RAMQA</a> </p>
<blockquote>
<p>å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºé—®ç­”ï¼ˆMRAQAï¼‰èåˆäº†æ–‡æœ¬å’Œå›¾åƒï¼Œåœ¨ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ä¼ ç»Ÿæ’åæ–¹æ³•ä¾èµ–äºåŸºäºç¼–ç å™¨çš„å°å‹è¯­è¨€æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ä¸ç°ä»£åŸºäºè§£ç å™¨çš„ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å…¼å®¹ï¼Œè€ŒLLMå·²åœ¨å„ç§NLPä»»åŠ¡ä¸­å–å¾—äº†è¿›å±•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†RAMQAï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†å­¦ä¹ æ’åæ–¹æ³•å’Œç”Ÿæˆç½®æ¢å¢å¼ºæ’åæŠ€æœ¯çš„ç»Ÿä¸€æ¡†æ¶ã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨LLaVAä½œä¸ºéª¨å¹²è®­ç»ƒç‚¹å¼å¤šæ¨¡æ€æ’åå™¨ã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨æŒ‡ä»¤è°ƒæ•´æ¥è®­ç»ƒä¸€ä¸ªLLaMAæ¨¡å‹ï¼Œä½¿ç”¨åˆ›æ–°çš„è‡ªå›å½’å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•å¯¹å‰kä¸ªæ–‡æ¡£è¿›è¡Œé‡æ–°æ’åã€‚æˆ‘ä»¬çš„ç”Ÿæˆæ’åæ¨¡å‹ä»æ–‡æ¡£å€™é€‰è€…ä¸­ç”Ÿæˆé‡æ–°æ’åçš„æ–‡æ¡£IDå’Œç‰¹å®šç­”æ¡ˆï¼Œè¿™äº›æ–‡æ¡£ä»¥å„ç§æ’åˆ—æ–¹å¼å‘ˆç°ã€‚åœ¨WebQAå’Œå¤šæ¨¡æ€QAä¸¤ä¸ªMRAQAåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä¸å¼ºå¤§çš„åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ˜æ˜¾çš„æ”¹è¿›ï¼Œçªå‡ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/TonyBY/RAMQA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/TonyBY/RAMQAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13297v1">PDF</a> Accepted by NAACL 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€æ£€ç´¢å¢å¼ºé—®ç­”ï¼ˆMRAQAï¼‰èåˆäº†æ–‡æœ¬å’Œå›¾åƒï¼Œåœ¨ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ä¼ ç»Ÿæ’åºæ–¹æ³•ä¾èµ–äºå°å‹ç¼–ç å™¨åŸºç¡€çš„è¯­è¨€æ¨¡å‹ï¼Œä¸ç°ä»£è§£ç å™¨åŸºç¡€çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸å…¼å®¹ã€‚æœ¬æ–‡æå‡ºRAMQAæ¡†æ¶ï¼Œç»“åˆå­¦ä¹ æ’åºæ–¹æ³•ä¸ç”Ÿæˆæ’åˆ—å¢å¼ºæ’åºæŠ€æœ¯ã€‚é€šè¿‡ä»¥LLaVAä¸ºéª¨å¹²è¿›è¡Œç‚¹å¯¹å¤šæ¨¡æ€æ’åºå™¨è®­ç»ƒï¼Œå†åº”ç”¨æŒ‡ä»¤å¾®è°ƒè®­ç»ƒLLaMAæ¨¡å‹ï¼Œä»¥åˆ›æ–°è‡ªå›å½’å¤šä»»åŠ¡å­¦ä¹ æ–¹å¼è¿›è¡Œå‰kä¸ªæ–‡æ¡£é‡æ–°æ’åºã€‚ç”Ÿæˆå¼æ’åæ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé‡æ–°æ’åºçš„æ–‡æ¡£IDå’Œæ¥è‡ªæ–‡æ¡£å€™é€‰è€…çš„ç‰¹å®šç­”æ¡ˆï¼Œåœ¨å„ç§æ’åˆ—ä¸­è¡¨ç°ä¼˜å¼‚ã€‚åœ¨WebQAå’ŒMultiModalQAä¸¤ä¸ªMRAQAåŸºå‡†æµ‹è¯•ä¸Šï¼Œç›¸è¾ƒäºå¼ºåŸºçº¿æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œå‡¸æ˜¾äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRAQAèåˆäº†æ–‡æœ¬å’Œå›¾åƒï¼Œåœ¨ä¿¡æ¯æ£€ç´¢å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­å—åˆ°é‡è§†ã€‚</li>
<li>ä¼ ç»Ÿæ’åºæ–¹æ³•ä¾èµ–å°å‹ç¼–ç å™¨è¯­è¨€æ¨¡å‹ï¼Œä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸å…¼å®¹ã€‚</li>
<li>RAMQAæ¡†æ¶ç»“åˆäº†å­¦ä¹ æ’åºå’Œç”Ÿæˆæ’åˆ—å¢å¼ºæ’åºæŠ€æœ¯ã€‚</li>
<li>ä½¿ç”¨LLaVAä½œä¸ºéª¨å¹²è¿›è¡Œç‚¹å¯¹å¤šæ¨¡æ€æ’åºå™¨è®­ç»ƒã€‚</li>
<li>é€šè¿‡æŒ‡ä»¤å¾®è°ƒè®­ç»ƒLLaMAæ¨¡å‹è¿›è¡Œæ–‡æ¡£é‡æ–°æ’åºã€‚</li>
<li>ç”Ÿæˆå¼æ’åæ¨¡å‹èƒ½ç”Ÿæˆé‡æ–°æ’åºçš„æ–‡æ¡£IDå’Œç‰¹å®šç­”æ¡ˆã€‚</li>
<li>åœ¨MRAQAåŸºå‡†æµ‹è¯•ä¸Šï¼ŒRAMQAç›¸æ¯”åŸºçº¿æœ‰æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13297">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b77a235470449d995af166d6bf2598d1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-191ebcf1a1a0be796f0bc9d757ba8f98.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c28009cf0b5c7ab370083e4512494e2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33e1bd77b9104d9e6c77d3f46c0913b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91dc97301770f3fa8eb63fc7d90f5975.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="ARTEMIS-DA-An-Advanced-Reasoning-and-Transformation-Engine-for-Multi-Step-Insight-Synthesis-in-Data-Analytics"><a href="#ARTEMIS-DA-An-Advanced-Reasoning-and-Transformation-Engine-for-Multi-Step-Insight-Synthesis-in-Data-Analytics" class="headerlink" title="ARTEMIS-DA: An Advanced Reasoning and Transformation Engine for   Multi-Step Insight Synthesis in Data Analytics"></a>ARTEMIS-DA: An Advanced Reasoning and Transformation Engine for   Multi-Step Insight Synthesis in Data Analytics</h2><p><strong>Authors:Atin Sakkeer Hussain</strong></p>
<p>This paper presents the Advanced Reasoning and Transformation Engine for Multi-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework designed to augment Large Language Models (LLMs) for solving complex, multi-step data analytics tasks. ARTEMIS-DA integrates three core components: the Planner, which dissects complex user queries into structured, sequential instructions encompassing data preprocessing, transformation, predictive modeling, and visualization; the Coder, which dynamically generates and executes Python code to implement these instructions; and the Grapher, which interprets generated visualizations to derive actionable insights. By orchestrating the collaboration between these components, ARTEMIS-DA effectively manages sophisticated analytical workflows involving advanced reasoning, multi-step transformations, and synthesis across diverse data modalities. The framework achieves state-of-the-art (SOTA) performance on benchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to tackle intricate analytical tasks with precision and adaptability. By combining the reasoning capabilities of LLMs with automated code generation and execution and visual analysis, ARTEMIS-DA offers a robust, scalable solution for multi-step insight synthesis, addressing a wide range of challenges in data analytics. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ç”¨äºæ•°æ®è§£æä¸­çš„å¤šæ­¥éª¤è§è§£åˆæˆçš„å…ˆè¿›æ¨ç†ä¸è½¬æ¢å¼•æ“ï¼ˆARTEMIS-DAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»¥è§£å†³å¤æ‚çš„å¤šæ­¥éª¤æ•°æ®åˆ†æä»»åŠ¡çš„æ–°å‹æ¡†æ¶ã€‚ARTEMIS-DAé›†æˆäº†ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šPlannerï¼Œå®ƒå°†å¤æ‚çš„ç”¨æˆ·æŸ¥è¯¢åˆ’åˆ†ä¸ºç»“æ„åŒ–ã€é¡ºåºæŒ‡ä»¤ï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€è½¬æ¢ã€é¢„æµ‹å»ºæ¨¡å’Œå¯è§†åŒ–ï¼›Coderï¼Œå®ƒåŠ¨æ€ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç ä»¥æ‰§è¡Œè¿™äº›æŒ‡ä»¤ï¼›ä»¥åŠGrapherï¼Œå®ƒè§£é‡Šç”Ÿæˆçš„å¯è§†åŒ–ä»¥è·å–å¯æ“ä½œè§è§£ã€‚é€šè¿‡åè°ƒè¿™äº›ç»„ä»¶ä¹‹é—´çš„åä½œï¼ŒARTEMIS-DAæœ‰æ•ˆåœ°ç®¡ç†æ¶‰åŠé«˜çº§æ¨ç†ã€å¤šæ­¥éª¤è½¬æ¢å’Œè·¨ä¸åŒæ•°æ®æ¨¡æ€çš„åˆæˆçš„é«˜çº§åˆ†æå·¥ä½œæµç¨‹ã€‚è¯¥æ¡†æ¶åœ¨WikiTableQuestionså’ŒTabFactç­‰åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶ç²¾ç¡®åº¦å’Œé€‚åº”æ€§å¤„ç†å¤æ‚åˆ†æä»»åŠ¡çš„èƒ½åŠ›ã€‚é€šè¿‡å°†LLMçš„æ¨ç†èƒ½åŠ›ä¸è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå’Œæ‰§è¡Œä»¥åŠå¯è§†åŒ–åˆ†æç›¸ç»“åˆï¼ŒARTEMIS-DAä¸ºå¤šæ­¥éª¤è§è§£åˆæˆæä¾›äº†ç¨³å¥ä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†æ•°æ®åˆ†æä¸­çš„å¹¿æ³›æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.14146v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ARTEMIS-DAæ¡†æ¶æ˜¯ä¸€ç§å…ˆè¿›çš„ç”¨äºå¤šæ­¥éª¤æ´å¯Ÿåˆæˆæ•°æ®åˆ†æçš„é«˜çº§æ¨ç†ä¸è½¬æ¢å¼•æ“ã€‚å®ƒé€šè¿‡æ•´åˆè§„åˆ’å™¨ã€ç¼–ç å™¨å’Œç»˜å›¾å™¨ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œå®ç°å¤æ‚çš„å¤šæ­¥éª¤æ•°æ®åˆ†æä»»åŠ¡ã€‚ARTEMIS-DAæœ‰æ•ˆç®¡ç†é«˜çº§æ¨ç†ã€å¤šæ­¥éª¤è½¬æ¢å’Œè·¨ä¸åŒæ•°æ®æ¨¡æ€çš„åˆæˆç­‰å¤æ‚åˆ†æå·¥ä½œæµç¨‹ï¼Œå¹¶åœ¨WikiTableQuestionså’ŒTabFactç­‰åŸºå‡†æµ‹è¯•ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚å®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä¸è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå’Œæ‰§è¡Œä»¥åŠå¯è§†åŒ–åˆ†æç›¸ç»“åˆï¼Œä¸ºå¤æ‚çš„æ•°æ®åˆ†æä»»åŠ¡æä¾›äº†ç¨³å¥ã€å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ARTEMIS-DAæ˜¯ä¸€ä¸ªæ—¨åœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è§£å†³å¤æ‚å¤šæ­¥éª¤æ•°æ®åˆ†æä»»åŠ¡çš„æ–°å‹æ¡†æ¶ã€‚</li>
<li>å®ƒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šè§„åˆ’å™¨è´Ÿè´£å°†å¤æ‚ç”¨æˆ·æŸ¥è¯¢åˆ†è§£ä¸ºç»“æ„åŒ–é¡ºåºæŒ‡ä»¤ï¼Œç¼–ç å™¨åŠ¨æ€ç”Ÿæˆå¹¶æ‰§è¡ŒPythonä»£ç å®ç°è¿™äº›æŒ‡ä»¤ï¼Œè€Œç»˜å›¾å™¨åˆ™è§£é‡Šç”Ÿæˆçš„å¯è§†åŒ–ä»¥è·å–å¯æ“ä½œè§è§£ã€‚</li>
<li>ARTEMIS-DAé€šè¿‡ååŒè¿™äº›ç»„ä»¶ï¼Œæœ‰æ•ˆç®¡ç†æ¶‰åŠé«˜çº§æ¨ç†ã€å¤šæ­¥éª¤è½¬æ¢å’Œè·¨ä¸åŒæ•°æ®æ¨¡æ€çš„åˆæˆç­‰å¤æ‚åˆ†æå·¥ä½œæµç¨‹ã€‚</li>
<li>ARTEMIS-DAåœ¨WikiTableQuestionså’ŒTabFactç­‰åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶å°†LLMsçš„æ¨ç†èƒ½åŠ›ä¸è‡ªåŠ¨åŒ–ä»£ç ç”Ÿæˆå’Œæ‰§è¡Œä»¥åŠå¯è§†åŒ–åˆ†æç›¸ç»“åˆï¼Œä¸ºå¤æ‚çš„æ•°æ®åˆ†æä»»åŠ¡æä¾›äº†å…¨é¢è§£å†³æ–¹æ¡ˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.14146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d7378ffa7e36e57eb66a440c8998e630.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-152cb53ea94c8c505412d75c9c95d10d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ddf9f19888890e2735f18e5a265d2623.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-98388b1a862e9c32483d1e214e3693f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa422c61b4ec8a0eb6de9b04ffc26d61.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Interactive-Cycle-Model-The-Linkage-Combination-among-Automatic-Speech-Recognition-Large-Language-Models-and-Smart-Glasses"><a href="#Interactive-Cycle-Model-The-Linkage-Combination-among-Automatic-Speech-Recognition-Large-Language-Models-and-Smart-Glasses" class="headerlink" title="Interactive Cycle Model: The Linkage Combination among Automatic Speech   Recognition, Large Language Models and Smart Glasses"></a>Interactive Cycle Model: The Linkage Combination among Automatic Speech   Recognition, Large Language Models and Smart Glasses</h2><p><strong>Authors:Libo Wang</strong></p>
<p>This research proposes the interaction loop model â€œASR-LLMs-Smart Glassesâ€, which model combines automatic speech recognition, large language model and smart glasses to facilitate seamless human-computer interaction. And the methodology of this research involves decomposing the interaction process into different stages and elements. Speech is captured and processed by ASR, then analyzed and interpreted by LLMs. The results are then transmitted to smart glasses for display. The feedback loop is complete when the user interacts with the displayed data. Mathematical formulas are used to quantify the performance of the model that revolves around core evaluation points: accuracy, coherence, and latency during ASR speech-to-text conversion. The research results are provided theoretically to test and evaluate the feasibility and performance of the model. Detailed architectural details and experimental process have been uploaded to Github, the link is:<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git">https://github.com/brucewang123456789/GeniusTrail.git</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æå‡ºäº†â€œASR-LLMs-æ™ºèƒ½çœ¼é•œâ€äº¤äº’å¾ªç¯æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ™ºèƒ½çœ¼é•œï¼Œä¿ƒè¿›äº†æ— ç¼çš„äººæœºäº¤äº’ã€‚æœ¬ç ”ç©¶çš„æ–¹æ³•è®ºæ¶‰åŠå°†äº¤äº’è¿‡ç¨‹åˆ†è§£æˆä¸åŒçš„é˜¶æ®µå’Œå…ƒç´ ã€‚è¯­éŸ³è¢«ASRæ•è·å¹¶å¤„ç†ï¼Œç„¶åè¢«LLMsåˆ†æå’Œè§£é‡Šã€‚ç»“æœéšåä¼ è¾“åˆ°æ™ºèƒ½çœ¼é•œè¿›è¡Œæ˜¾ç¤ºã€‚å½“ç”¨æˆ·ä¸æ˜¾ç¤ºçš„æ•°æ®äº¤äº’æ—¶ï¼Œåé¦ˆå¾ªç¯å®Œæˆã€‚æœ¬ç ”ç©¶ä½¿ç”¨æ•°å­¦å…¬å¼æ¥é‡åŒ–æ¨¡å‹æ€§èƒ½ï¼Œå›´ç»•æ ¸å¿ƒè¯„ä¼°ç‚¹ï¼šASRè¯­éŸ³è½¬æ–‡æœ¬è½¬æ¢è¿‡ç¨‹ä¸­çš„å‡†ç¡®æ€§ã€è¿è´¯æ€§å’Œå»¶è¿Ÿã€‚ç ”ç©¶ç»“æœåœ¨ç†è®ºä¸Šæä¾›äº†æµ‹è¯•å’Œè¯„ä¼°è¯¥æ¨¡å‹å¯è¡Œæ€§å’Œæ€§èƒ½çš„ä¾æ®ã€‚è¯¦ç»†çš„æ¶æ„ç»†èŠ‚å’Œå®éªŒè¿‡ç¨‹å·²ç»ä¸Šä¼ åˆ°Githubï¼Œé“¾æ¥ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git%E3%80%82">https://github.com/brucewang123456789/GeniusTrail.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10362v3">PDF</a> OpenReview submitted. 10 pages of text and 2 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æå‡ºäº†â€œASR-LLMs-æ™ºèƒ½çœ¼é•œâ€äº¤äº’å¾ªç¯æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ™ºèƒ½çœ¼é•œï¼Œä¿ƒè¿›äº†æ— ç¼çš„äººæœºäº¤äº’ã€‚ç ”ç©¶æ–¹æ³•å°†äº¤äº’è¿‡ç¨‹åˆ†è§£æˆä¸åŒçš„é˜¶æ®µå’Œå…ƒç´ ã€‚è¯­éŸ³ç”±ASRæ•è·å¹¶å¤„ç†ï¼Œç„¶åé€šè¿‡LLMsè¿›è¡Œåˆ†æå’Œè§£é‡Šã€‚ç»“æœä¼ è¾“åˆ°æ™ºèƒ½çœ¼é•œè¿›è¡Œæ˜¾ç¤ºã€‚ç”¨æˆ·ä¸æ˜¾ç¤ºçš„æ•°æ®è¿›è¡Œäº¤äº’æ—¶ï¼Œåé¦ˆå¾ªç¯å®Œæˆã€‚å›´ç»•æ ¸å¿ƒè¯„ä¼°ç‚¹ï¼ˆå‡†ç¡®æ€§ã€è¿è´¯æ€§å’ŒASRè¯­éŸ³è½¬æ–‡æœ¬çš„å»¶è¿Ÿï¼‰ï¼Œä½¿ç”¨æ•°å­¦å…¬å¼å¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œé‡åŒ–ã€‚ç ”ç©¶æˆæœå·²åœ¨ç†è®ºä¸Šè¿›è¡Œäº†æµ‹è¯•å¹¶è¯„ä¼°äº†æ¨¡å‹çš„å¯è¡Œæ€§åŠæ€§èƒ½ã€‚è¯¦ç»†æ¶æ„å’Œå®éªŒè¿‡ç¨‹å·²ä¸Šä¼ è‡³Githubã€‚æœ‰å…³é“¾æ¥ï¼š<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git">https://github.com/brucewang123456789/GeniusTrail.git</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶æå‡ºäº†ASR-LLMs-æ™ºèƒ½çœ¼é•œäº¤äº’å¾ªç¯æ¨¡å‹ï¼Œæ•´åˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ™ºèƒ½çœ¼é•œæŠ€æœ¯ã€‚</li>
<li>æ¨¡å‹å®ç°äº†æ— ç¼äººæœºäº’åŠ¨ï¼Œé€šè¿‡åˆ†è§£äº¤äº’è¿‡ç¨‹ä¸ºä¸åŒé˜¶æ®µå’Œå…ƒç´ æ¥ä¼˜åŒ–äº¤äº’ä½“éªŒã€‚</li>
<li>è¯­éŸ³å…ˆç”±ASRå¤„ç†ï¼Œç„¶åé€šè¿‡LLMsè¿›è¡Œåˆ†æå’Œè§£é‡Šï¼Œæœ€åå°†ç»“æœå±•ç¤ºåœ¨æ™ºèƒ½çœ¼é•œä¸Šã€‚</li>
<li>æ¨¡å‹æ€§èƒ½é€šè¿‡æ•°å­¦å…¬å¼é‡åŒ–ï¼Œæ ¸å¿ƒè¯„ä¼°æ ‡å‡†åŒ…æ‹¬å‡†ç¡®æ€§ã€è¿è´¯æ€§å’Œå»¶è¿Ÿã€‚</li>
<li>è¯¥æ¨¡å‹å·²ç»åœ¨ç†è®ºä¸Šå¾—åˆ°äº†æµ‹è¯•ï¼ŒéªŒè¯äº†å…¶å¯è¡Œæ€§å’Œæ€§èƒ½è¡¨ç°ã€‚</li>
<li>è¯¦ç»†çš„æ¨¡å‹æ¶æ„å’Œå®éªŒè¿‡ç¨‹å·²ç»å…¬å¼€åœ¨Githubä¸Šä¾›å…¬ä¼—æŸ¥é˜…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10362">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71c7a8f8e1851c32c5a561985b44222a.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Reducing-Reasoning-Costs-The-Path-of-Optimization-for-Chain-of-Thought-via-Sparse-Attention-Mechanism"><a href="#Reducing-Reasoning-Costs-The-Path-of-Optimization-for-Chain-of-Thought-via-Sparse-Attention-Mechanism" class="headerlink" title="Reducing Reasoning Costs: The Path of Optimization for Chain of Thought   via Sparse Attention Mechanism"></a>Reducing Reasoning Costs: The Path of Optimization for Chain of Thought   via Sparse Attention Mechanism</h2><p><strong>Authors:Libo Wang</strong></p>
<p>In order to address the chain of thought in the large language model inference cost surge, this research proposes to use a sparse attention mechanism that only focuses on a few relevant tokens. The researcher constructed a new attention mechanism and used GiantRabbit trained with custom GPTs as an experimental tool. The experiment tested and compared the reasoning time, correctness score and chain of thought length of this model and o1 Preview in solving the linear algebra test questions of MIT OpenCourseWare. The results show that GiantRabbitâ€™s reasoning time and chain of thought length are significantly lower than o1 Preview. It verifies the feasibility of sparse attention mechanism for optimizing chain of thought reasoning. Detailed architectural details and experimental process have been uploaded to Github, the link is:<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git">https://github.com/brucewang123456789/GeniusTrail.git</a>. </p>
<blockquote>
<p>ä¸ºäº†åº”å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æˆæœ¬é£™å‡ä¸­çš„æ€ç»´é“¾é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åªå…³æ³¨å°‘æ•°ç›¸å…³ä»¤ç‰Œã€‚ç ”ç©¶è€…æ„å»ºäº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶ä½¿ç”¨GiantRabbitä½œä¸ºå®éªŒå·¥å…·ï¼ŒGiantRabbité€šè¿‡è‡ªå®šä¹‰GPTè¿›è¡Œäº†è®­ç»ƒã€‚å®éªŒæµ‹è¯•å¹¶æ¯”è¾ƒäº†è¯¥æ¨¡å‹ä¸o1 Previewåœ¨è§£å†³MIT OpenCourseWareçš„çº¿æ€§ä»£æ•°æµ‹è¯•é—®é¢˜æ—¶çš„æ¨ç†æ—¶é—´ã€æ­£ç¡®ç‡å’Œæ€ç»´é“¾é•¿åº¦ã€‚ç»“æœè¡¨æ˜ï¼ŒGiantRabbitçš„æ¨ç†æ—¶é—´å’Œæ€ç»´é“¾é•¿åº¦æ˜æ˜¾ä½äºo1 Previewã€‚è¿™éªŒè¯äº†ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶åœ¨ä¼˜åŒ–æ€ç»´é“¾æ¨ç†æ–¹é¢çš„å¯è¡Œæ€§ã€‚è¯¦ç»†çš„æ¶æ„ç»†èŠ‚å’Œå®éªŒè¿‡ç¨‹å·²ä¸Šä¼ åˆ°Githubï¼Œé“¾æ¥ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git%E3%80%82">https://github.com/brucewang123456789/GeniusTrail.gitã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.09111v5">PDF</a> The main text is 5 pages, totaling 9 pages; 4 figures, 1 table. It   have been submitted to NeurIPS 2024 Workshop MusIML and OpenReview</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æˆæœ¬æ¿€å¢çš„é—®é¢˜å¾—åˆ°äº†è§£å†³ã€‚ç ”ç©¶æå‡ºäº†ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶æ¥å…³æ³¨å…³é”®å†…å®¹ã€‚é€šè¿‡å®éªŒå‘ç°ï¼Œæ–°å‹æ³¨æ„åŠ›æœºåˆ¶çš„æ¨ç†æ—¶é—´å’Œæ¨ç†æ€è·¯é•¿åº¦ç›¸è¾ƒäºé¢„è§ˆéƒ½æ˜æ˜¾æ›´ä¼˜ï¼Œä¸”å¯è¡Œã€‚æ›´å…·ä½“çš„æ–¹æ¡ˆå’Œå®éªŒç»“æœåœ¨GitHubä¸­ç»™å‡ºã€‚è¯¦æƒ…è¯·å‚è§GitHubé“¾æ¥ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git">https://github.com/brucewang123456789/GeniusTrail.git</a>ï¼‰ã€‚ç®€æ´åœ°è¡¨è¾¾å‡ºæ¥çš„æ–‡ç« ä¸»è¦å†…å®¹å¦‚ä¸‹ï¼šSparse Attentionæœºåˆ¶æé«˜äº†å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼›GiantRabbitæ¨¡å‹è¡¨ç°ä¼˜å¼‚ï¼›GitHubé“¾æ¥æä¾›äº†è¯¦ç»†ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯åŸºäºæ–‡æœ¬æå–çš„ä¸ƒä¸ªå…³é”®è¦ç‚¹ï¼š</p>
<ul>
<li>ç ”ç©¶è€…æå‡ºä½¿ç”¨ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶æ¥è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†æˆæœ¬ä¸Šå‡çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶ä¸­è®¾è®¡äº†ä¸€ç§æ–°å‹æ³¨æ„åŠ›æœºåˆ¶å¹¶é€šè¿‡GiantRabbitæ¨¡å‹è¿›è¡Œå®éªŒéªŒè¯ã€‚</li>
<li>GiantRabbitæ¨¡å‹æ˜¯ç”¨å®šåˆ¶çš„GPTè®­ç»ƒçš„ï¼Œä½œä¸ºå®éªŒå·¥å…·è¿›è¡Œæ¨ç†æ€§èƒ½æµ‹è¯•ã€‚</li>
<li>å®éªŒé€šè¿‡å¯¹æ¯”GiantRabbitæ¨¡å‹å’Œo1 Previewåœ¨è§£å†³MIT OpenCourseWareçš„çº¿æ€§ä»£æ•°æµ‹è¯•é—®é¢˜ä¸Šçš„è¡¨ç°æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºGiantRabbitæ¨¡å‹çš„æ¨ç†æ—¶é—´å’Œæ¨ç†æ€è·¯é•¿åº¦å‡æ˜¾è‘—ä½äºo1 Previewã€‚</li>
<li>å®éªŒéªŒè¯äº†ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶åœ¨ä¼˜åŒ–æ¨ç†æ€è·¯æ–¹é¢çš„å¯è¡Œæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.09111">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15674cde4263b5f2f7678ff5c313d490.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-54cc398807739192b8c25968dfca40fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0830d96b5f5c26cbd04d076cd6dbb28e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DART-Denoising-Autoregressive-Transformer-for-Scalable-Text-to-Image-Generation"><a href="#DART-Denoising-Autoregressive-Transformer-for-Scalable-Text-to-Image-Generation" class="headerlink" title="DART: Denoising Autoregressive Transformer for Scalable Text-to-Image   Generation"></a>DART: Denoising Autoregressive Transformer for Scalable Text-to-Image   Generation</h2><p><strong>Authors:Jiatao Gu, Yuyang Wang, Yizhe Zhang, Qihang Zhang, Dinghuai Zhang, Navdeep Jaitly, Josh Susskind, Shuangfei Zhai</strong></p>
<p>Diffusion models have become the dominant approach for visual generation. They are trained by denoising a Markovian process which gradually adds noise to the input. We argue that the Markovian property limits the modelâ€™s ability to fully utilize the generation trajectory, leading to inefficiencies during training and inference. In this paper, we propose DART, a transformer-based model that unifies autoregressive (AR) and diffusion within a non-Markovian framework. DART iteratively denoises image patches spatially and spectrally using an AR model that has the same architecture as standard language models. DART does not rely on image quantization, which enables more effective image modeling while maintaining flexibility. Furthermore, DART seamlessly trains with both text and image data in a unified model. Our approach demonstrates competitive performance on class-conditioned and text-to-image generation tasks, offering a scalable, efficient alternative to traditional diffusion models. Through this unified framework, DART sets a new benchmark for scalable, high-quality image synthesis. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»æˆä¸ºè§†è§‰ç”Ÿæˆçš„ä¸»è¦æ–¹æ³•ã€‚å®ƒä»¬é€šè¿‡å»å™ªé©¬å°”å¯å¤«è¿‡ç¨‹è¿›è¡Œè®­ç»ƒï¼Œè¯¥è¿‡ç¨‹é€æ­¥å‘è¾“å…¥æ·»åŠ å™ªå£°ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œé©¬å°”å¯å¤«å±æ€§é™åˆ¶äº†æ¨¡å‹å……åˆ†åˆ©ç”¨ç”Ÿæˆè½¨è¿¹çš„èƒ½åŠ›ï¼Œä»è€Œåœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­é€ æˆæ•ˆç‡ä½ä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DARTï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå˜å‹å™¨çš„æ¨¡å‹ï¼Œå®ƒåœ¨éé©¬å°”å¯å¤«æ¡†æ¶å†…ç»Ÿä¸€äº†è‡ªå›å½’ï¼ˆARï¼‰å’Œæ‰©æ•£ã€‚DARTä½¿ç”¨ä¸æ ‡å‡†è¯­è¨€æ¨¡å‹ç›¸åŒçš„æ¶æ„çš„ARæ¨¡å‹ï¼Œåœ¨ç©ºé—´ä¸Šå…‰è°±ä¸Šè¿­ä»£å»å™ªå›¾åƒå—ã€‚DARTä¸ä¾èµ–äºå›¾åƒé‡åŒ–ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒçµæ´»æ€§çš„åŒæ—¶å®ç°æ›´æœ‰æ•ˆçš„å›¾åƒå»ºæ¨¡ã€‚æ­¤å¤–ï¼ŒDARTå¯ä»¥åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­æ— ç¼åœ°è®­ç»ƒæ–‡æœ¬å’Œå›¾åƒæ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç±»æ¡ä»¶ç”Ÿæˆå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºæœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œä¸ºä¼ ç»Ÿæ‰©æ•£æ¨¡å‹æä¾›äº†å¯æ‰©å±•ä¸”é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡è¿™ä¸€ç»Ÿä¸€æ¡†æ¶ï¼ŒDARTä¸ºå¯æ‰©å±•çš„é«˜è´¨é‡å›¾åƒåˆæˆè®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.08159v2">PDF</a> Accepted by ICLR2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºéé©¬å°”å¯å¤«æ¡†æ¶çš„ç»Ÿä¸€æ‰©æ•£æ¨¡å‹DARTï¼Œç»“åˆäº†è‡ªå›å½’ï¼ˆARï¼‰å’Œæ‰©æ•£æ¨¡å‹ã€‚DARTé€šè¿‡ç©ºé—´ä¸Šå’Œå…‰è°±ä¸Šçš„å›¾åƒå—è¿­ä»£å»å™ªï¼Œä½¿ç”¨ä¸æ ‡å‡†è¯­è¨€æ¨¡å‹ç›¸åŒçš„æ¶æ„çš„è‡ªå›å½’æ¨¡å‹ã€‚å®ƒä¸éœ€è¦ä¾èµ–å›¾åƒé‡åŒ–ï¼Œèƒ½åœ¨ä¿æŒçµæ´»æ€§çš„åŒæ—¶æ›´æœ‰æ•ˆåœ°è¿›è¡Œå›¾åƒå»ºæ¨¡ã€‚æ­¤å¤–ï¼ŒDARTå¯ä»¥åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­æ— ç¼åœ°ç»“åˆæ–‡æœ¬å’Œå›¾åƒæ•°æ®è¿›è¡Œè®­ç»ƒã€‚è¯¥æ–¹æ³•åœ¨ç±»æ¡ä»¶ç”Ÿæˆå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œæˆä¸ºå¯æ‰©å±•ã€é«˜æ•ˆçš„ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹çš„æ›¿ä»£å“ï¼Œä¸ºå¯ä¼¸ç¼©é«˜è´¨é‡å›¾åƒåˆæˆè®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²æˆä¸ºè§†è§‰ç”Ÿæˆçš„ä¸»å¯¼æ–¹æ³•ï¼Œä½†å…¶Markovianå±æ€§é™åˆ¶äº†æ¨¡å‹åœ¨ç”Ÿæˆè½¨è¿¹ä¸Šçš„å®Œå…¨åˆ©ç”¨èƒ½åŠ›ï¼Œå¯¼è‡´è®­ç»ƒå’Œæ¨æ–­æ•ˆç‡ä½ä¸‹ã€‚</li>
<li>DARTæ˜¯ä¸€ä¸ªåŸºäºéé©¬å°”å¯å¤«æ¡†æ¶çš„ç»Ÿä¸€æ¨¡å‹ï¼Œç»“åˆäº†è‡ªå›å½’ï¼ˆARï¼‰å’Œæ‰©æ•£æ¨¡å‹ã€‚</li>
<li>DARTé€šè¿‡ç©ºé—´ä¸Šå’Œå…‰è°±ä¸Šçš„å›¾åƒå—è¿­ä»£å»å™ªï¼Œé‡‡ç”¨ä¸æ ‡å‡†è¯­è¨€æ¨¡å‹ç›¸åŒçš„æ¶æ„çš„è‡ªå›å½’æ¨¡å‹ã€‚</li>
<li>DARTä¸éœ€è¦ä¾èµ–å›¾åƒé‡åŒ–ï¼Œå¯ä»¥æ›´æœ‰æ•ˆåœ°è¿›è¡Œå›¾åƒå»ºæ¨¡å¹¶ä¿æŒçµæ´»æ€§ã€‚</li>
<li>DARTèƒ½åœ¨ç»Ÿä¸€æ¨¡å‹ä¸­æ— ç¼ç»“åˆæ–‡æœ¬å’Œå›¾åƒæ•°æ®è¿›è¡Œè®­ç»ƒã€‚</li>
<li>DARTåœ¨ç±»æ¡ä»¶ç”Ÿæˆå’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸Šå…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.08159">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac9597d65100b353273108723e57a6e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7716e9df02bc67a94a0995a5d1ad7eaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5793fa0f5d23246015949f10e43e2cfe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-07de6da17894d400e38ee65f17f4370e.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="NESTFUL-A-Benchmark-for-Evaluating-LLMs-on-Nested-Sequences-of-API-Calls"><a href="#NESTFUL-A-Benchmark-for-Evaluating-LLMs-on-Nested-Sequences-of-API-Calls" class="headerlink" title="NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API   Calls"></a>NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API   Calls</h2><p><strong>Authors:Kinjal Basu, Ibrahim Abdelaziz, Kiran Kate, Mayank Agarwal, Maxwell Crouse, Yara Rizk, Kelsey Bradford, Asim Munawar, Sadhana Kumaravel, Saurabh Goyal, Xin Wang, Luis A. Lastras, Pavan Kapanipathi</strong></p>
<p>The resurgence of autonomous agents built using large language models (LLMs) to solve complex real-world tasks has brought increased focus on LLMsâ€™ fundamental ability of tool or function calling. At the core of these agents, an LLM must plan, execute, and respond using external tools, APIs, and custom functions. Research on tool calling has gathered momentum, but evaluation benchmarks and datasets representing the complexity of the tasks have lagged behind. In this work, we focus on one such complexity, nested sequencing, with the goal of extending existing benchmarks and evaluation. Specifically, we present NESTFUL, a benchmark to evaluate LLMs on nested sequences of API calls, i.e., sequences where the output of one API call is passed as input to a subsequent call. NESTFUL contains 1800+ nested sequences where all the function calls are executable. Experimental results on multiple models and settings show that the best-performing model on the dataset has a full sequence match accuracy of 25% and win-rate of 34% necessitating a large scope for improvement in the nested sequencing aspect of function calling. Our analysis of these results provides possible future research directions for the community, in addition to a benchmark to track progress. We have released the NESTFUL dataset under the Apache 2.0 license at <a target="_blank" rel="noopener" href="https://github.com/IBM/NESTFUL">https://github.com/IBM/NESTFUL</a>. </p>
<blockquote>
<p>ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„å»ºçš„è‡ªä¸»ä»£ç†äººçš„å¤è‹ï¼Œç”¨äºè§£å†³å¤æ‚çš„ç°å®ä¸–ç•Œä»»åŠ¡ï¼Œè¿™å¢åŠ äº†å¯¹LLMåŸºæœ¬å·¥å…·æˆ–åŠŸèƒ½è°ƒç”¨èƒ½åŠ›çš„å…³æ³¨ã€‚è¿™äº›ä»£ç†äººçš„æ ¸å¿ƒï¼ŒLLMå¿…é¡»åˆ©ç”¨å¤–éƒ¨å·¥å…·ã€APIå’Œè‡ªå®šä¹‰å‡½æ•°è¿›è¡Œè§„åˆ’ã€æ‰§è¡Œå’Œå“åº”ã€‚å…³äºå·¥å…·è°ƒç”¨çš„ç ”ç©¶å·²ç»è“„åŠ¿å¾…å‘ï¼Œä½†æ˜¯ä»£è¡¨ä»»åŠ¡å¤æ‚åº¦çš„è¯„ä¼°åŸºå‡†å’Œæ•°æ®é›†å´æ»åäº†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå…¶ä¸­ä¸€ç§å¤æ‚æ€§ï¼Œå³åµŒå¥—åºåˆ—ï¼Œæ—¨åœ¨æ‰©å±•ç°æœ‰çš„åŸºå‡†è¯„ä¼°å’Œæ•°æ®é›†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†NESTFULåŸºå‡†è¯„ä¼°ï¼Œç”¨äºè¯„ä¼°LLMåœ¨APIè°ƒç”¨çš„åµŒå¥—åºåˆ—æ–¹é¢çš„è¡¨ç°ï¼Œå³ä¸€ä¸ªAPIè°ƒç”¨çš„è¾“å‡ºä½œä¸ºåç»­è°ƒç”¨çš„è¾“å…¥ã€‚NESTFULåŒ…å«è¶…è¿‡1800ä¸ªåµŒå¥—åºåˆ—ï¼Œå…¶ä¸­æ‰€æœ‰å‡½æ•°è°ƒç”¨éƒ½æ˜¯å¯æ‰§è¡Œçš„ã€‚åœ¨å¤šæ¨¡å‹å’Œè®¾ç½®ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ•°æ®é›†ä¸Šè¡¨ç°æœ€ä½³çš„æ¨¡å‹çš„å®Œæ•´åºåˆ—åŒ¹é…å‡†ç¡®ç‡ä¸º25%ï¼Œèƒœç‡ä¸º34%ï¼Œè¯´æ˜åœ¨å‡½æ•°è°ƒç”¨ä¸­çš„åµŒå¥—åºåˆ—æ–¹é¢è¿˜æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æˆ‘ä»¬å¯¹è¿™äº›ç»“æœçš„åˆ†æä¸ºç¤¾åŒºæä¾›äº†å¯èƒ½çš„æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œæ­¤å¤–è¿˜æä¾›äº†ä¸€ä¸ªåŸºå‡†è¯„ä¼°æ¥è·Ÿè¸ªè¿›åº¦ã€‚æˆ‘ä»¬å·²åœ¨Apache 2.0è®¸å¯è¯ä¸‹å‘å¸ƒäº†NESTFULæ•°æ®é›†ï¼š<a target="_blank" rel="noopener" href="https://github.com/IBM/NESTFUL">https://github.com/IBM/NESTFUL</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.03797v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚ç°å®ä»»åŠ¡ä¸­çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°åŸºå‡†å’Œæ•°æ®é›†åœ¨ä»»åŠ¡å¤æ‚æ€§æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°åŸºå‡†NESTFULï¼Œç”¨äºè¯„ä¼°LLMåœ¨å¤„ç†åµŒå¥—åºåˆ—APIè°ƒç”¨æ–¹é¢çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç°æœ‰æ¨¡å‹åœ¨è¿™æ–¹é¢ä»æœ‰å¾…æé«˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚åŒæ—¶ï¼Œæœ¬æ–‡ä¹Ÿåˆ†äº«äº†NESTFULæ•°æ®é›†ä»¥ä¾¿è·Ÿè¸ªè¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³å¤æ‚ç°å®ä»»åŠ¡ä¸­éœ€è¦è°ƒç”¨å·¥å…·ã€APIå’Œè‡ªå®šä¹‰å‡½æ•°ã€‚</li>
<li>NESTFULæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°LLMåœ¨å¤„ç†åµŒå¥—åºåˆ—APIè°ƒç”¨æ€§èƒ½çš„åŸºå‡†æµ‹è¯•ã€‚</li>
<li>NESTFULåŒ…å«1800å¤šä¸ªå¯æ‰§è¡Œçš„åµŒå¥—åºåˆ—ã€‚</li>
<li>ç›®å‰æœ€ä½³æ¨¡å‹åœ¨NESTFULæ•°æ®é›†ä¸Šçš„å…¨åºåˆ—åŒ¹é…å‡†ç¡®ç‡ä¸º25%ï¼Œèƒœç‡ä¸º34%ï¼Œè¡¨æ˜åœ¨è¿™æ–¹é¢ä»æœ‰å¾ˆå¤§æ”¹è¿›ç©ºé—´ã€‚</li>
<li>æœ¬æ–‡åˆ†æäº†å®éªŒç»“æœï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</li>
<li>NESTFULæ•°æ®é›†å·²ç»å‘å¸ƒï¼Œå¯ä¾›ç ”ç©¶ç¤¾åŒºä½¿ç”¨ä»¥è·Ÿè¸ªè¿›å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.03797">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb602a49fb81a27e0180a9425d98354a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a7e3763ac1409c3f82bbb8122b6cf7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c59bef957094a63d121df04278747f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d8fa46acdc16e77da5577372a1b41b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bd1aadd19ea9997ff3658ac5b8c37d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f532d1fa304501c548bd692fef25a7f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="PhishAgent-A-Robust-Multimodal-Agent-for-Phishing-Webpage-Detection"><a href="#PhishAgent-A-Robust-Multimodal-Agent-for-Phishing-Webpage-Detection" class="headerlink" title="PhishAgent: A Robust Multimodal Agent for Phishing Webpage Detection"></a>PhishAgent: A Robust Multimodal Agent for Phishing Webpage Detection</h2><p><strong>Authors:Tri Cao, Chengyu Huang, Yuexin Li, Huilin Wang, Amy He, Nay Oo, Bryan Hooi</strong></p>
<p>Phishing attacks are a major threat to online security, exploiting user vulnerabilities to steal sensitive information. Various methods have been developed to counteract phishing, each with varying levels of accuracy, but they also face notable limitations. In this study, we introduce PhishAgent, a multimodal agent that combines a wide range of tools, integrating both online and offline knowledge bases with Multimodal Large Language Models (MLLMs). This combination leads to broader brand coverage, which enhances brand recognition and recall. Furthermore, we propose a multimodal information retrieval framework designed to extract the relevant top k items from offline knowledge bases, using available information from a webpage, including logos and HTML. Our empirical results, based on three real-world datasets, demonstrate that the proposed framework significantly enhances detection accuracy and reduces both false positives and false negatives, while maintaining model efficiency. Additionally, PhishAgent shows strong resilience against various types of adversarial attacks. </p>
<blockquote>
<p>ç½‘ç»œé’“é±¼æ”»å‡»æ˜¯å¯¹ç½‘ç»œå®‰å…¨çš„ä¸»è¦å¨èƒä¹‹ä¸€ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ç”¨æˆ·æ¼æ´æ¥çªƒå–æ•æ„Ÿä¿¡æ¯ã€‚å°½ç®¡å·²ç»å¼€å‘äº†å„ç§æ–¹æ³•æ¥å¯¹æŠ—ç½‘ç»œé’“é±¼æ”»å‡»ï¼Œæ¯ç§æ–¹æ³•éƒ½æœ‰ä¸åŒçº§åˆ«çš„å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬ä¹Ÿé¢ä¸´ç€æ˜æ˜¾çš„å±€é™æ€§ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†PhishAgentï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ¨¡å¼ä»£ç†ï¼Œå®ƒç»“åˆäº†å¤šç§å·¥å…·ï¼Œèåˆäº†åœ¨çº¿å’Œç¦»çº¿çŸ¥è¯†åº“ä¸å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚è¿™ç§ç»“åˆå¯¼è‡´äº†æ›´å¹¿æ³›çš„å“ç‰Œè¦†ç›–ï¼Œä»è€Œæé«˜äº†å“ç‰Œè¯†åˆ«å’Œå›å¿†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡å¼ä¿¡æ¯æ£€ç´¢æ¡†æ¶ï¼Œæ—¨åœ¨ä»ç¦»çº¿çŸ¥è¯†åº“ä¸­æå–ä¸ç½‘é¡µä¸Šå¯ç”¨çš„ä¿¡æ¯ç›¸å…³çš„å‰kä¸ªé¡¹ç›®ï¼ŒåŒ…æ‹¬æ ‡å¿—å’ŒHTMLã€‚æˆ‘ä»¬çš„åŸºäºä¸‰ä¸ªçœŸå®æ•°æ®é›†çš„å®è¯ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å¤§å¤§æé«˜äº†æ£€æµ‹å‡†ç¡®æ€§ï¼Œé™ä½äº†è¯¯æŠ¥å’Œæ¼æŠ¥çš„æ•°é‡ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒPhishAgentå¯¹å„ç§å¯¹æŠ—æ€§æ”»å‡»è¡¨ç°å‡ºå¼ºå¤§çš„æŠµæŠ—åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.10738v3">PDF</a> Accepted at AAAI 2025 (Oral)</p>
<p><strong>Summary</strong><br>è‡ªåŠ¨åŒ–é˜²æŠ¤å—åˆ°ç½‘ç»œå®‰å…¨é¢†åŸŸå„ç§é’“é‚®æ”»å‡»çš„å¨èƒå½±å“æ·±è¿œã€‚æ­¤ç ”ç©¶å¼•å…¥PhishAgentï¼Œç»“åˆå¤šç§å·¥å…·å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œæé«˜å“ç‰Œè¦†ç›–èŒƒå›´å’Œè¯†åˆ«èƒ½åŠ›ï¼Œæ›´æœ‰æ•ˆåœ°æ£€æµ‹å’Œé˜²æŠ¤é’“é‚®æ”»å‡»ã€‚å®è¯ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æ˜¾è‘—æé«˜æ£€æµ‹å‡†ç¡®æ€§å¹¶å‡å°‘è¯¯æŠ¥å’Œæ¼æŠ¥ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ•ˆç‡ã€‚æ­¤å¤–ï¼ŒPhishAgentå¯¹æŠ—å¤šç§å¯¹æŠ—æ€§æ”»å‡»è¡¨ç°å‡ºå¼ºå¤§çš„éŸ§æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Phishingæ”»å‡»æ˜¯ç½‘ç»œå®‰å…¨çš„ä¸»è¦å¨èƒï¼Œæ—¨åœ¨åˆ©ç”¨ç”¨æˆ·æ¼æ´çªƒå–æ•æ„Ÿä¿¡æ¯ã€‚</li>
<li>å½“å‰åé’“é‚®æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>PhishAgentæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€agentï¼Œç»“åˆåœ¨çº¿å’Œç¦»çº¿çŸ¥è¯†åº“ä¸å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ã€‚</li>
<li>ç»“åˆå¤šç§å·¥å…·å¢å¼ºå“ç‰Œè¦†ç›–èŒƒå›´å’Œè¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>å¤šæ¨¡æ€ä¿¡æ¯æ£€ç´¢æ¡†æ¶ä»ç¦»çº¿çŸ¥è¯†åº“ä¸­æå–æœ€ç›¸å…³çš„å‰ké¡¹ä¿¡æ¯ã€‚</li>
<li>åŸºäºä¸‰ä¸ªçœŸå®æ•°æ®é›†è¿›è¡Œçš„å®è¯ç ”ç©¶è¯æ˜PhishAgentèƒ½æé«˜æ£€æµ‹å‡†ç¡®æ€§å¹¶å‡å°‘è¯¯æŠ¥å’Œæ¼æŠ¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.10738">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d49a788c15676d3be3babf720157abc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a5d84bebe4ac4a0927ecad97cfdbbc13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-88321ef2f6d4e369844bb68772bcfa75.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="RegMix-Data-Mixture-as-Regression-for-Language-Model-Pre-training"><a href="#RegMix-Data-Mixture-as-Regression-for-Language-Model-Pre-training" class="headerlink" title="RegMix: Data Mixture as Regression for Language Model Pre-training"></a>RegMix: Data Mixture as Regression for Language Model Pre-training</h2><p><strong>Authors:Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, Min Lin</strong></p>
<p>The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix trains many small models on diverse data mixtures, uses regression to predict performance of unseen mixtures, and applies the best predicted mixture to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, we train 512 models with 1M parameters for 1B tokens to fit the regression model and predict the best data mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Furthermore, RegMix consistently outperforms human selection in experiments involving models up to 7B models trained on 100B tokens, while matching or exceeding DoReMi using just 10% of the computational resources. Our experiments also show that (1) Data mixtures significantly impact performance; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed; (4) Data mixture effects transcend scaling laws. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/sail-sg/regmix">https://github.com/sail-sg/regmix</a>. </p>
<blockquote>
<p>æ•°æ®æ··åˆå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒæ€§èƒ½æœ‰ç€æ˜¾è‘—å½±å“ï¼Œç„¶è€Œå¦‚ä½•ç¡®å®šæœ‰æ•ˆçš„æ•°æ®æ··åˆä»ä¸æ˜ç¡®ã€‚æˆ‘ä»¬æå‡ºRegMixæ–¹æ³•ï¼Œé€šè¿‡å°†å…¶åˆ¶å®šä¸ºå›å½’ä»»åŠ¡æ¥è‡ªåŠ¨ç¡®å®šé«˜æ€§èƒ½çš„æ•°æ®æ··åˆã€‚RegMixåœ¨å¤šç§æ•°æ®æ··åˆä¸Šè®­ç»ƒè®¸å¤šå°å‹æ¨¡å‹ï¼Œä½¿ç”¨å›å½’æ¥é¢„æµ‹æœªè§è¿‡çš„æ•°æ®æ··åˆçš„æ€§èƒ½ï¼Œå¹¶å°†æœ€ä½³çš„é¢„æµ‹æ•°æ®æ··åˆåº”ç”¨äºè®­ç»ƒå…·æœ‰å¤§é‡è®¡ç®—èµ„æºçš„å¤§å‹æ¨¡å‹ã€‚ä¸ºäº†å®è¯éªŒè¯RegMixï¼Œæˆ‘ä»¬è®­ç»ƒäº†512ä¸ªå…·æœ‰1Må‚æ•°ã€è®­ç»ƒæ ‡è®°ä¸º1Bçš„æ¨¡å‹æ¥æ‹Ÿåˆå›å½’æ¨¡å‹å¹¶é¢„æµ‹æœ€ä½³æ•°æ®æ··åˆã€‚ä½¿ç”¨è¿™ç§æ•°æ®æ··åˆï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå…·æœ‰1Bå‚æ•°çš„æ¨¡å‹ï¼Œè¿›è¡Œäº†é«˜è¾¾25Bæ ‡è®°çš„è®­ç»ƒï¼ˆå³æ¯”å…·æœ‰å…¶ä»–æ··åˆç‰©çš„å…¶ä»–æœ€ä½³æ¨¡å‹è§„æ¨¡å¤§10å€ï¼Œè®­ç»ƒæ—¶é—´ä¸ºå…¶ä¸‰å€ï¼‰ï¼Œå¹¶ä¸”æˆ‘ä»¬å‘ç°è¯¥æ¨¡å‹æ€§èƒ½æœ€ä½³ã€‚æ­¤å¤–ï¼ŒRegMixåœ¨å®éªŒä¸­çš„è¡¨ç°ä¸€ç›´ä¼˜äºäººå·¥é€‰æ‹©æ–¹å¼ï¼Œå³ä½¿åœ¨æ¶‰åŠé«˜è¾¾æ‹¥æœ‰è¢«è®­ç»ƒæ ‡è®°ä¸ºé«˜è¾¾æ•°åäº¿çš„æ›´å¤§æ¨¡å‹çš„å®éªŒä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åŒ¹é…æˆ–è¶…è¿‡äº†DoReMiæ–¹æ³•æ‰€ä½¿ç”¨çš„è®¡ç®—èµ„æºåªæœ‰å…¶ååˆ†ä¹‹ä¸€ã€‚æˆ‘ä»¬çš„å®éªŒè¿˜è¡¨æ˜ï¼šï¼ˆ1ï¼‰æ•°æ®æ··åˆæ˜¾è‘—å½±å“æ€§èƒ½ï¼›ï¼ˆ2ï¼‰ç›¸è¾ƒäºè¢«è®¤ä¸ºé«˜è´¨é‡çš„ç»´åŸºç™¾ç§‘ï¼Œç½‘ç»œè¯­æ–™åº“ä¸ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ä¹‹é—´å­˜åœ¨æœ€å¼ºçš„æ­£å‘å…³è”ï¼›ï¼ˆ3ï¼‰å„ä¸ªé¢†åŸŸä¹‹é—´çš„äº’åŠ¨é”™ç»¼å¤æ‚å¸¸å¸¸æœ‰æ‚–å¸¸ç†å¸¸è¯†å› æ­¤éœ€è¦è¿›è¡ŒåƒRegMixä¸€æ ·çš„è‡ªåŠ¨åŒ–æ–¹å¼æ¥è¿›è¡Œè¯„ä¼°ï¼›ï¼ˆ4ï¼‰æ•°æ®æ··åˆçš„å½±å“è¶…è¶Šäº†è§„æ¨¡å®šå¾‹ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sail-sg/regmix%E4%B8%8A%E8%8E%B7%E5%8F%96%E6%9F%A5%E9%98%85%E3%80%82">https://github.com/sail-sg/regmixä¸Šè·å–æŸ¥é˜…ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.01492v2">PDF</a> ICLR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ•°æ®æ··åˆçš„é‡è¦æ€§åŠå…¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚é’ˆå¯¹å¦‚ä½•ç¡®å®šæœ‰æ•ˆæ•°æ®æ··åˆçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºRegMixçš„è‡ªåŠ¨æ–¹æ³•ã€‚RegMixé€šè¿‡å›å½’ä»»åŠ¡æ¥é¢„æµ‹ä¸åŒæ•°æ®æ··åˆçš„æ€§èƒ½ï¼Œå¹¶ä½¿ç”¨æœ€ä½³é¢„æµ‹ç»“æœæ¥è®­ç»ƒå¤§å‹æ¨¡å‹ã€‚å®è¯ç ”ç©¶è¡¨æ˜ï¼ŒRegMixåœ¨å¤šä¸ªå®éªŒä¸­è¡¨ç°ä¼˜ç§€ï¼Œä¸äººå·¥é€‰æ‹©ç›¸æ¯”å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼ŒåŒæ—¶åŒ¹é…æˆ–è¶…è¶Šäº†DoReMiæ–¹æ³•ä¸”ä»…ä½¿ç”¨äº†å…¶ååˆ†ä¹‹ä¸€çš„è®¡ç®—èµ„æºã€‚æ­¤å¤–ï¼Œå®éªŒè¿˜æ­ç¤ºäº†ä¸€äº›é‡è¦è§è§£ï¼Œå¦‚æ•°æ®æ··åˆå¯¹æ€§èƒ½æœ‰é‡è¦å½±å“ï¼Œç½‘ç»œè¯­æ–™åº“ç›¸è¾ƒäºå¦‚ç»´åŸºç™¾ç§‘ç­‰é«˜è´¨é‡æ•°æ®å¯¹ä¸‹æ¸¸æ€§èƒ½æœ‰æ›´å¼ºæ­£é¢å½±å“ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®æ··åˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½å…·æœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>RegMixæ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨ç¡®å®šé«˜æ€§èƒ½çš„æ•°æ®æ··åˆã€‚</li>
<li>é€šè¿‡å›å½’ä»»åŠ¡ï¼ŒRegMixé¢„æµ‹ä¸åŒæ•°æ®æ··åˆçš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>ç½‘ç»œè¯­æ–™åº“ä¸ä¸‹æ¸¸æ€§èƒ½ä¹‹é—´æœ‰å¼ºæ­£ç›¸å…³å…³ç³»ã€‚</li>
<li>æ¨¡å‹é¢†åŸŸé—´çš„ç›¸äº’ä½œç”¨å¤æ‚ï¼Œéœ€è¦è‡ªåŠ¨æ–¹æ³•å¦‚RegMixæ¥å¤„ç†ã€‚</li>
<li>æ•°æ®æ··åˆçš„å½±å“è¶…è¶Šäº†è§„æ¨¡å®šå¾‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.01492">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c686cb4b3276b195b6f6024d69a70d30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14cb73bbb639ffe7496caf361a5a2082.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eccdb003fdfd887278f19472bb704e25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2093eb51db718dbcf5295f80e8baf55a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ce4c1b497b73c80a6f2748458ba8f80.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Evaluating-LLMs-for-Quotation-Attribution-in-Literary-Texts-A-Case-Study-of-LLaMa3"><a href="#Evaluating-LLMs-for-Quotation-Attribution-in-Literary-Texts-A-Case-Study-of-LLaMa3" class="headerlink" title="Evaluating LLMs for Quotation Attribution in Literary Texts: A Case   Study of LLaMa3"></a>Evaluating LLMs for Quotation Attribution in Literary Texts: A Case   Study of LLaMa3</h2><p><strong>Authors:Gaspard Michel, Elena V. Epure, Romain Hennequin, Christophe Cerisara</strong></p>
<p>Large Language Models (LLMs) have shown promising results in a variety of literary tasks, often using complex memorized details of narration and fictional characters. In this work, we evaluate the ability of Llama-3 at attributing utterances of direct-speech to their speaker in novels. The LLM shows impressive results on a corpus of 28 novels, surpassing published results with ChatGPT and encoder-based baselines by a large margin. We then validate these results by assessing the impact of book memorization and annotation contamination. We found that these types of memorization do not explain the large performance gain, making Llama-3 the new state-of-the-art for quotation attribution in English literature. We release publicly our code and data. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å„ç§æ–‡å­¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä»¤äººç©ç›®çš„ç»“æœï¼Œé€šå¸¸åˆ©ç”¨å¯¹å™äº‹å’Œè™šæ„è§’è‰²çš„å¤æ‚è®°å¿†ç»†èŠ‚ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†Llama-3åœ¨å°è¯´ä¸­å°†ç›´æ¥è¨€è¯­å½’äºè¯´è¯äººçš„èƒ½åŠ›ã€‚è¯¥è¯­è¨€æ¨¡å‹åœ¨28éƒ¨å°è¯´çš„è¯­æ–™åº“ä¸Šè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œå¤§å¹…åº¦è¶…è¶Šäº†ChatGPTå’ŒåŸºäºç¼–ç å™¨çš„åŸºçº¿æ¨¡å‹çš„å·²å‘å¸ƒç»“æœã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡è¯„ä¼°ä¹¦ç±è®°å¿†å’Œæ³¨é‡Šæ±¡æŸ“çš„å½±å“æ¥éªŒè¯è¿™äº›ç»“æœã€‚æˆ‘ä»¬å‘ç°ï¼Œè¿™ç§ç±»å‹çš„è®°å¿†å¹¶ä¸èƒ½è§£é‡Šæ€§èƒ½çš„å¤§å¹…æå‡ï¼Œè¿™ä½¿å¾—Llama-3æˆä¸ºè‹±è¯­æ–‡å­¦ä¸­å¼•è¯­å½’å±çš„æ–°æŠ€æœ¯é¡¶å°–æ°´å¹³ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒäº†æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11380v2">PDF</a> NAACL 2025 Main Conference â€“ short paper</p>
<p><strong>Summary</strong></p>
<p>LLMså¦‚Llama-3åœ¨å°è¯´ç›´æ¥å¼•è¯­çš„è¯´è¯è€…å½’å±ä»»åŠ¡ä¸­å±•ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¯¹28éƒ¨å°è¯´çš„è¯­æ–™åº“ç»“æœä»¤äººå°è±¡æ·±åˆ»ï¼Œè¶…è¶Šäº†ChatGPTå’ŒåŸºäºç¼–ç å™¨çš„åŸºçº¿æ¨¡å‹ã€‚éªŒè¯ç»“æœæ˜¾ç¤ºï¼Œä¹¦æœ¬è®°å¿†å’Œæ ‡æ³¨æ±¡æŸ“å¹¶ä¸è¶³ä»¥è§£é‡Šå…¶æ€§èƒ½çš„å¤§å¹…æå‡ï¼ŒLlama-3æˆä¸ºè‹±è¯­æ–‡å­¦å¼•ç”¨å½’å±ä»»åŠ¡çš„æ–°æ ‡æ†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Llama-3åœ¨å°è¯´ç›´æ¥å¼•è¯­çš„è¯´è¯è€…å½’å±ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>Llama-3åœ¨28éƒ¨å°è¯´çš„è¯­æ–™åº“ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ChatGPTå’Œå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚</li>
<li>éªŒè¯ç»“æœæ˜¾ç¤ºä¹¦æœ¬è®°å¿†å’Œæ ‡æ³¨æ±¡æŸ“å¹¶ä¸è¶³ä»¥è§£é‡ŠLlama-3çš„æ€§èƒ½æå‡ã€‚</li>
<li>Llama-3æˆä¸ºè‹±è¯­æ–‡å­¦å¼•ç”¨å½’å±ä»»åŠ¡çš„æ–°æ ‡æ†ã€‚</li>
<li>è¯¥ç ”ç©¶å…¬å¼€äº†ä»£ç å’Œæ•°æ®ï¼Œæœ‰åŠ©äºè¿›ä¸€æ­¥ç ”ç©¶å’Œæ”¹è¿›LLMæŠ€æœ¯ã€‚</li>
<li>è¯¥ç ”ç©¶å±•ç¤ºäº†LLMåœ¨æ–‡å­¦ä»»åŠ¡ä¸­çš„å¤æ‚è®°å¿†èƒ½åŠ›ï¼ŒåŒ…æ‹¬å™äº‹å’Œè§’è‰²ç»†èŠ‚çš„å¤æ‚è®°å¿†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11380">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-77df98bccd76bdb87da600da7e60500c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1cb301db410a89e3bc5c67106eacbffc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-983d629211789b6cd0fd01875e6cf904.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e0434dc894f0b702ee8f9f50e925f52d.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="IrokoBench-A-New-Benchmark-for-African-Languages-in-the-Age-of-Large-Language-Models"><a href="#IrokoBench-A-New-Benchmark-for-African-Languages-in-the-Age-of-Large-Language-Models" class="headerlink" title="IrokoBench: A New Benchmark for African Languages in the Age of Large   Language Models"></a>IrokoBench: A New Benchmark for African Languages in the Age of Large   Language Models</h2><p><strong>Authors:David Ifeoluwa Adelani, Jessica Ojo, Israel Abebe Azime, Jian Yun Zhuang, Jesujoba O. Alabi, Xuanli He, Millicent Ochieng, Sara Hooker, Andiswa Bukula, En-Shiun Annie Lee, Chiamaka Chukwuneke, Happy Buzaaba, Blessing Sibanda, Godson Kalipe, Jonathan Mukiibi, Salomon Kabongo, Foutse Yuehgoh, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Shamsuddeen Hassan Muhammad, Salomey Osei, Sokhar Samb, Tadesse Kebede Guge, Tombekai Vangoni Sherman, Pontus Stenetorp</strong></p>
<p>Despite the widespread adoption of Large language models (LLMs), their remarkable capabilities remain limited to a few high-resource languages. Additionally, many low-resource languages (\eg African languages) are often evaluated only on basic text classification tasks due to the lack of appropriate or comprehensive benchmarks outside of high-resource languages. In this paper, we introduce IrokoBench â€“ a human-translated benchmark dataset for 17 typologically-diverse low-resource African languages covering three tasks: natural language inference<del>(AfriXNLI), mathematical reasoning</del>(AfriMGSM), and multi-choice knowledge-based question answering<del>(AfriMMLU). We use IrokoBench to evaluate zero-shot, few-shot, and translate-test settings</del>(where test sets are translated into English) across 10 open and six proprietary LLMs. Our evaluation reveals a significant performance gap between high-resource languages~(such as English and French) and low-resource African languages. We observe a significant performance gap between open and proprietary models, with the highest performing open model, Gemma 2 27B only at 63% of the best-performing proprietary model GPT-4o performance. In addition, machine translating the test set to English before evaluation helped to close the gap for larger models that are English-centric, such as Gemma 2 27B and LLaMa 3.1 70B. These findings suggest that more efforts are needed to develop and adapt LLMs for African languages. </p>
<blockquote>
<p>å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä½†å®ƒä»¬çš„å‡ºè‰²åŠŸèƒ½ä»…é™äºå°‘æ•°é«˜èµ„æºè¯­è¨€ã€‚æ­¤å¤–ï¼Œç”±äºé«˜èµ„æºè¯­è¨€ä¹‹å¤–ç¼ºä¹é€‚å½“æˆ–å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œè®¸å¤šä½èµ„æºè¯­è¨€ï¼ˆä¾‹å¦‚éæ´²è¯­è¨€ï¼‰é€šå¸¸ä»…åœ¨åŸºæœ¬çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†IrokoBenchâ€”â€”ä¸€ä¸ªä¸º17ç§ç±»å‹å¤šæ ·ä¸”èµ„æºåŒ®ä¹çš„éæ´²è¯­è¨€æä¾›çš„äººç±»ç¿»è¯‘åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«ä¸‰ä¸ªä»»åŠ¡ï¼šè‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆAfriXNLIï¼‰ã€æ•°å­¦æ¨ç†ï¼ˆAfriMGSMï¼‰å’ŒåŸºäºå¤šé€‰çŸ¥è¯†çš„é—®ç­”ï¼ˆAfriMMLUï¼‰ã€‚æˆ‘ä»¬ä½¿ç”¨IrokoBenchå¯¹é›¶æ ·æœ¬ã€å°‘æ ·æœ¬ä»¥åŠå°†æµ‹è¯•é›†ç¿»è¯‘è‡³è‹±è¯­çš„ç¿»è¯‘æµ‹è¯•ç¯å¢ƒè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶‰åŠ10ä¸ªå¼€æºå’Œå…­ä¸ªä¸“æœ‰LLMã€‚æˆ‘ä»¬çš„è¯„ä¼°æ­ç¤ºäº†é«˜èµ„æºè¯­è¨€ï¼ˆå¦‚è‹±è¯­å’Œæ³•è¯­ï¼‰ä¸èµ„æºåŒ®ä¹çš„éæ´²è¯­è¨€ä¹‹é—´çš„æ˜¾è‘—æ€§èƒ½å·®è·ã€‚æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°å¼€æºæ¨¡å‹å’Œä¸“æœ‰æ¨¡å‹ä¹‹é—´çš„æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œè¡¨ç°æœ€ä½³çš„å¼€æºæ¨¡å‹Gemma 2 27Bä»…è¾¾åˆ°æœ€ä½³ä¸“æœ‰æ¨¡å‹GPT-4oæ€§èƒ½çš„63%ã€‚æ­¤å¤–ï¼Œåœ¨è¯„ä¼°ä¹‹å‰å°†æµ‹è¯•é›†æœºå™¨ç¿»è¯‘è‡³è‹±è¯­æœ‰åŠ©äºç¼©å°ä»¥è‹±è¯­ä¸ºä¸­å¿ƒçš„è¾ƒå¤§æ¨¡å‹çš„å·®è·ï¼Œå¦‚Gemma 2 27Bå’ŒLLaMa 3.1 70Bã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œéœ€è¦æ›´å¤šçš„åŠªåŠ›æ¥å¼€å‘å’Œé€‚åº”éæ´²è¯­è¨€çš„LLMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.03368v2">PDF</a> Accepted to NAACL 2025 (main conference)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†IrokoBenchâ€”â€”ä¸€ä¸ªä¸º17ç§è¯­è¨€å½¢æ€å„å¼‚çš„ä½èµ„æºéæ´²è¯­è¨€è®¾è®¡çš„äººç±»ç¿»è¯‘åŸºå‡†æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†ä¸‰é¡¹ä»»åŠ¡ï¼šè‡ªç„¶è¯­è¨€æ¨ç†ã€æ•°å­¦æ¨ç†å’ŒåŸºäºçŸ¥è¯†çš„é—®é¢˜å›ç­”ã€‚é€šè¿‡å¯¹é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œç¿»è¯‘æµ‹è¯•ç¯å¢ƒçš„è¯„ä¼°ï¼Œå‘ç°éæ´²è¯­è¨€ä¸èµ„æºä¸°å¯Œçš„è¯­è¨€ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ã€‚æœºå™¨ç¿»è¯‘æµ‹è¯•é›†åˆ°è‹±è¯­åè¯„ä¼°æœ‰åŠ©äºç¼©å°ä¸è‹±è¯­ä¸ºä¸­å¿ƒçš„å¤§å‹æ¨¡å‹çš„å·®è·ã€‚è¿™æ˜¾ç¤ºå‡ºéœ€è¦æ›´å¤šåŠªåŠ›æ¥å¼€å‘å’Œé€‚åº”éæ´²è¯­è¨€çš„LLMã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>IrokoBenchæ˜¯ä¸€ä¸ªé’ˆå¯¹éæ´²è¯­è¨€çš„åŸºå‡†æ•°æ®é›†ï¼ŒåŒ…å«è‡ªç„¶è¯­è¨€æ¨ç†ã€æ•°å­¦æ¨ç†å’ŒçŸ¥è¯†é—®ç­”ä¸‰é¡¹ä»»åŠ¡ã€‚</li>
<li>æ•°æ®é›†æ¶µç›–17ç§è¯­è¨€å½¢æ€å„å¼‚çš„ä½èµ„æºéæ´²è¯­è¨€ã€‚</li>
<li>åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œç¿»è¯‘æµ‹è¯•ç¯å¢ƒä¸­è¯„ä¼°LLMæ€§èƒ½æ—¶å‘ç°éæ´²è¯­è¨€ä¸èµ„æºä¸°å¯Œçš„è¯­è¨€ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
<li>å¼€æ”¾æ¨¡å‹å’Œä¸“æœ‰æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—æ€§èƒ½å·®è·ï¼Œæœ€ä½³å¼€æ”¾æ¨¡å‹ä»…è¾¾åˆ°æœ€ä½³ä¸“æœ‰æ¨¡å‹æ€§èƒ½çš„63%ã€‚</li>
<li>å°†æµ‹è¯•é›†æœºå™¨ç¿»è¯‘åˆ°è‹±è¯­åè¯„ä¼°æœ‰åŠ©äºç¼©å°ä¸è‹±è¯­ä¸ºä¸­å¿ƒçš„å¤§å‹æ¨¡å‹çš„æ€§èƒ½å·®è·ã€‚</li>
<li>éœ€è¦æ›´å¤šåŠªåŠ›æ¥å¼€å‘å’Œé€‚åº”éæ´²è¯­è¨€çš„LLMã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.03368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d3e6e4dba97beb2e0947722992d1d0c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-60918a7755b976740f58c6a0cff77b1e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ffd8e1a3f25840db5b9f016d689481d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-30cf188013e83286dffa3c67454050dd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Solve-longer-Math-Word-Problems-Better"><a href="#Can-LLMs-Solve-longer-Math-Word-Problems-Better" class="headerlink" title="Can LLMs Solve longer Math Word Problems Better?"></a>Can LLMs Solve longer Math Word Problems Better?</h2><p><strong>Authors:Xin Xu, Tong Xiao, Zitong Chao, Zhenya Huang, Can Yang, Yang Wang</strong></p>
<p>Math Word Problems (MWPs) play a vital role in assessing the capabilities of Large Language Models (LLMs), yet current research primarily focuses on questions with concise contexts. The impact of longer contexts on mathematical reasoning remains under-explored. This study pioneers the investigation of Context Length Generalizability (CoLeG), which refers to the ability of LLMs to solve MWPs with extended narratives. We introduce Extended Grade-School Math (E-GSM), a collection of MWPs featuring lengthy narratives, and propose two novel metrics to evaluate the efficacy and resilience of LLMs in tackling these problems. Our analysis of existing zero-shot prompting techniques with proprietary LLMs along with open-source LLMs reveals a general deficiency in CoLeG. To alleviate these issues, we propose tailored approaches for different categories of LLMs. For proprietary LLMs, we introduce a new instructional prompt designed to mitigate the impact of long contexts. For open-source LLMs, we develop a novel auxiliary task for fine-tuning to enhance CoLeG. Our comprehensive results demonstrate the effectiveness of our proposed methods, showing improved performance on E-GSM. Additionally, we conduct an in-depth analysis to differentiate the effects of semantic understanding and reasoning efficacy, showing that our methods improves the latter. We also establish the generalizability of our methods across several other MWP benchmarks. Our findings highlight the limitations of current LLMs and offer practical solutions correspondingly, paving the way for further exploration of model generalizability and training methodologies. </p>
<blockquote>
<p>æ•°å­¦æ–‡å­—é¢˜ï¼ˆMWPsï¼‰åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œç„¶è€Œå½“å‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç®€æ´è¯­å¢ƒçš„é—®é¢˜ä¸Šã€‚è¾ƒé•¿è¯­å¢ƒå¯¹æ•°å­¦æ¨ç†çš„å½±å“ä»è¢«å¿½è§†ã€‚æœ¬ç ”ç©¶ç‡å…ˆæ¢è®¨äº†è¯­å¢ƒé•¿åº¦æ³›åŒ–ï¼ˆCoLeGï¼‰é—®é¢˜ï¼Œå³LLMè§£å†³å…·æœ‰æ‰©å±•å™è¿°çš„MWPsçš„èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†æ‰©å±•å°å­¦æ•°å­¦ï¼ˆE-GSMï¼‰ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—å¸¦æœ‰å†—é•¿å™è¿°çš„MWPsï¼Œå¹¶æå‡ºäº†ä¸¤ç§æ–°é¢–çš„æŒ‡æ ‡æ¥è¯„ä¼°LLMåœ¨å¤„ç†è¿™äº›é—®é¢˜æ—¶çš„æ•ˆæœå’ŒéŸ§æ€§ã€‚æˆ‘ä»¬å¯¹ç°æœ‰çš„é›¶æ ·æœ¬æç¤ºæŠ€æœ¯ä¸ä¸“æœ‰LLMä»¥åŠå¼€æºLLMçš„åˆ†æè¡¨æ˜ï¼ŒCoLeGæ™®éå­˜åœ¨ä¸è¶³ã€‚ä¸ºäº†ç¼“è§£è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ä¸ºä¸åŒç±»å‹çš„LLMæå‡ºäº†é’ˆå¯¹æ€§çš„æ–¹æ³•ã€‚å¯¹äºä¸“æœ‰LLMï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æŒ‡ä»¤æç¤ºï¼Œæ—¨åœ¨å‡è½»é•¿è¯­å¢ƒçš„å½±å“ã€‚å¯¹äºå¼€æºLLMï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç”¨äºå¾®è°ƒçš„æ–°å‹è¾…åŠ©ä»»åŠ¡ï¼Œä»¥æé«˜CoLeGã€‚æˆ‘ä»¬çš„å…¨é¢ç»“æœè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæ˜¾ç¤ºå‡ºåœ¨E-GSMä¸Šçš„æ€§èƒ½æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ·±å…¥çš„åˆ†æï¼Œä»¥åŒºåˆ†è¯­ä¹‰ç†è§£å’Œæ¨ç†æ•ˆæœçš„å½±å“ï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†åè€…çš„æ•ˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¶ä»–æ•°å­¦æ–‡å­—é¢˜åŸºå‡†æµ‹è¯•ä¸­çš„é€šç”¨æ€§ä¹Ÿå¾—åˆ°äº†è¯å®ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†å½“å‰LLMçš„å±€é™æ€§ï¼Œå¹¶æä¾›äº†ç›¸åº”çš„å®ç”¨è§£å†³æ–¹æ¡ˆï¼Œä¸ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œè®­ç»ƒæ–¹æ³•çš„è¿›ä¸€æ­¥æ¢ç´¢é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.14804v2">PDF</a> Accepted to ICLR 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤„ç†å¸¦æœ‰è¾ƒé•¿è¯­å¢ƒçš„æ•°å­¦å­—é—®é¢˜ï¼ˆMWPsï¼‰æ—¶çš„è¡¨ç°ã€‚æ–‡ç« ä»‹ç»äº†è¯­å¢ƒé•¿åº¦æ³›åŒ–ï¼ˆCoLeGï¼‰çš„æ¦‚å¿µï¼Œå¹¶æ¢è®¨äº†LLMè§£å†³å…·æœ‰æ‰©å±•å™äº‹çš„é—®é¢˜çš„èƒ½åŠ›ã€‚æ–‡ç« å¼•å…¥äº†ä¸€ç§æ–°çš„æ•°å­¦å­—é—®é¢˜é›†Extended Grade-School Mathï¼ˆE-GSMï¼‰ï¼Œå¹¶æå‡ºäº†ä¸¤ç§æ–°æŒ‡æ ‡æ¥è¯„ä¼°LLMè§£å†³è¿™äº›é—®é¢˜çš„æ•ˆæœå’ŒéŸ§æ€§ã€‚æ–‡ç« åˆ†æäº†ç°æœ‰çš„é›¶æç¤ºæç¤ºæŠ€æœ¯ä¸å¼€æºLLMå’Œä¸“æœ‰LLMçš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†é’ˆå¯¹æ€§çš„è§£å†³æ–¹æ¡ˆã€‚å¯¹äºä¸“æœ‰LLMï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„æŒ‡ä»¤æç¤ºæ¥ç¼“è§£é•¿è¯­å¢ƒçš„å½±å“ã€‚å¯¹äºå¼€æºLLMï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°é¢–çš„ç²¾ç»†è°ƒæ•´è¾…åŠ©ä»»åŠ¡æ¥å¢å¼ºCoLeGã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨E-GSMä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ·±å…¥åˆ†æäº†è¯­ä¹‰ç†è§£å’Œæ¨ç†æ•ˆæœçš„ä¸åŒå½±å“ï¼Œå¹¶å»ºç«‹äº†æ–¹æ³•åœ¨å…¶ä»–æ•°å­¦å­—é—®é¢˜åŸºå‡†æµ‹è¯•ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰ç ”ç©¶ä¸»è¦å…³æ³¨å…·æœ‰ç®€æ´è¯­å¢ƒçš„æ•°å­¦å­—é—®é¢˜ï¼Œè€Œè¾ƒé•¿è¯­å¢ƒå¯¹æ•°å­¦æ¨ç†çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>å¼•å…¥Context Length Generalizabilityï¼ˆCoLeGï¼‰æ¦‚å¿µï¼Œä»¥è¯„ä¼°LLMè§£å†³å…·æœ‰æ‰©å±•å™äº‹çš„æ•°å­¦å­—é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºExtended Grade-School Mathï¼ˆE-GSMï¼‰æ•°æ®é›†ï¼ŒåŒ…å«å…·æœ‰è¾ƒé•¿å™äº‹çš„é—®é¢˜ã€‚</li>
<li>ä»‹ç»ä¸¤ç§æ–°æŒ‡æ ‡æ¥è¯„ä¼°LLMåœ¨è§£å†³è¿™äº›é—®é¢˜æ—¶çš„æ•ˆæœå’ŒéŸ§æ€§ã€‚</li>
<li>åˆ†æç°æœ‰é›¶æç¤ºæç¤ºæŠ€æœ¯çš„å±€é™æ€§ï¼Œå¹¶é’ˆå¯¹ä¸“æœ‰LLMå’Œå¼€æºLLMæå‡ºè§£å†³æ–¹æ¡ˆã€‚</li>
<li>å¯¹äºä¸“æœ‰LLMï¼Œå¼•å…¥æ–°çš„æŒ‡ä»¤æç¤ºæ¥ç¼“è§£é•¿è¯­å¢ƒçš„å½±å“ï¼›å¯¹äºå¼€æºLLMï¼Œå¼€å‘æ–°é¢–çš„ç²¾ç»†è°ƒæ•´è¾…åŠ©ä»»åŠ¡ä»¥å¢å¼ºCoLeGã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºæ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶æ·±å…¥åˆ†æäº†è¯­ä¹‰ç†è§£å’Œæ¨ç†æ•ˆæœçš„ä¸åŒå½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.14804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-54880d4d1096a4516002091881fb409d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2edbf80dce204251529d371a4fad44e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8bd4a2ca917c1950ff8d1eaaabd1851e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-163a1ecb85a76f86ff07c22622d39067.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd7d8b38b8a36b1218de950d016e042e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="TT-BLIP-Enhancing-Fake-News-Detection-Using-BLIP-and-Tri-Transformer"><a href="#TT-BLIP-Enhancing-Fake-News-Detection-Using-BLIP-and-Tri-Transformer" class="headerlink" title="TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer"></a>TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer</h2><p><strong>Authors:Eunjee Choi, Jong-Kook Kim</strong></p>
<p>Detecting fake news has received a lot of attention. Many previous methods concatenate independently encoded unimodal data, ignoring the benefits of integrated multimodal information. Also, the absence of specialized feature extraction for text and images further limits these methods. This paper introduces an end-to-end model called TT-BLIP that applies the bootstrapping language-image pretraining for unified vision-language understanding and generation (BLIP) for three types of information: BERT and BLIPTxt for text, ResNet and BLIPImg for images, and bidirectional BLIP encoders for multimodal information. The Multimodal Tri-Transformer fuses tri-modal features using three types of multi-head attention mechanisms, ensuring integrated modalities for enhanced representations and improved multimodal data analysis. The experiments are performed using two fake news datasets, Weibo and Gossipcop. The results indicate TT-BLIP outperforms the state-of-the-art models. </p>
<blockquote>
<p>æ£€æµ‹å‡æ–°é—»å·²ç»å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚è®¸å¤šä»¥å‰çš„æ–¹æ³•ä¼šè¿æ¥ç‹¬ç«‹ç¼–ç çš„å•æ¨¡æ€æ•°æ®ï¼Œå¿½ç•¥äº†é›†æˆå¤šæ¨¡æ€ä¿¡æ¯çš„å¥½å¤„ã€‚æ­¤å¤–ï¼Œç¼ºå°‘é’ˆå¯¹æ–‡æœ¬å’Œå›¾åƒçš„ä¸“ä¸šç‰¹å¾æå–è¿›ä¸€æ­¥é™åˆ¶äº†è¿™äº›æ–¹æ³•ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼Œç§°ä¸ºTT-BLIPï¼Œè¯¥æ¨¡å‹åº”ç”¨å¼•å¯¼å¼è¯­è¨€å›¾åƒé¢„è®­ç»ƒè¿›è¡Œç»Ÿä¸€çš„è§†è§‰è¯­è¨€ç†è§£å’Œç”Ÿæˆï¼ˆBLIPï¼‰ï¼Œæ¶µç›–ä¸‰ç§ç±»å‹çš„ä¿¡æ¯ï¼šBERTå’ŒBLIPTxtç”¨äºæ–‡æœ¬ï¼ŒResNetå’ŒBLIPImgç”¨äºå›¾åƒï¼ŒåŒå‘BLIPç¼–ç å™¨ç”¨äºå¤šæ¨¡æ€ä¿¡æ¯ã€‚å¤šæ¨¡æ€ä¸‰Transformeré€šè¿‡ä¸‰ç§å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶èåˆä¸‰æ¨¡æ€ç‰¹å¾ï¼Œç¡®ä¿é›†æˆæ¨¡å¼ç”¨äºå¢å¼ºè¡¨ç¤ºå’Œæ”¹è¿›å¤šæ¨¡æ€æ•°æ®åˆ†æã€‚å®éªŒé‡‡ç”¨ä¸¤ä¸ªå‡æ–°é—»æ•°æ®é›†å¾®åšå’ŒGossipcopè¿›è¡Œã€‚ç»“æœè¡¨æ˜ï¼ŒTT-BLIPä¼˜äºæœ€æ–°æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.12481v2">PDF</a> 8 pages, Accepted 27th International Conference on Information   Fusion, FUSION 2024</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTT-BLIPçš„ç«¯åˆ°ç«¯æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨é¢„è®­ç»ƒçš„è·¨è¯­è¨€å›¾åƒç†è§£ç”Ÿæˆï¼ˆBLIPï¼‰æŠ€æœ¯ï¼Œç»“åˆæ–‡æœ¬å’Œå›¾åƒçš„å¤šæ¨¡æ€ä¿¡æ¯ï¼Œè¿›è¡Œç»Ÿä¸€çš„è§†è§‰å’Œè¯­è¨€ç†è§£ã€‚æ¨¡å‹é€šè¿‡ä¸‰ç§ä¿¡æ¯ç±»å‹ï¼ˆæ–‡æœ¬ã€å›¾åƒå’Œå¤šæ¨¡æ€ä¿¡æ¯ï¼‰å’Œä¸‰ç§å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶èåˆä¸‰æ¨¡æ€ç‰¹å¾ï¼Œæé«˜äº†å¤šæ¨¡æ€æ•°æ®çš„è¡¨ç¤ºå’Œåˆ†æèƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTT-BLIPåœ¨å‡æ–°é—»æ£€æµ‹æ–¹é¢ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>è®¸å¤šæ—©æœŸå‡æ–°é—»æ£€æµ‹æ–¹æ³•å¿½ç•¥å¤šæ¨¡æ€ä¿¡æ¯çš„èåˆä¼˜åŠ¿ï¼Œä»…è¿æ¥ç‹¬ç«‹ç¼–ç çš„å•æ¨¡æ€æ•°æ®ã€‚</li>
<li>TT-BLIPæ¨¡å‹é‡‡ç”¨é¢„è®­ç»ƒçš„BLIPæŠ€æœ¯ï¼Œæ”¯æŒç»Ÿä¸€è§†è§‰å’Œè¯­è¨€ç†è§£ç”Ÿæˆã€‚</li>
<li>TT-BLIPæ”¯æŒä¸‰ç§ä¿¡æ¯ç±»å‹ï¼šæ–‡æœ¬ï¼ˆBERTå’ŒBLIPTxtï¼‰ã€å›¾åƒï¼ˆResNetå’ŒBLIPImgï¼‰ä»¥åŠå¤šæ¨¡æ€ä¿¡æ¯ã€‚</li>
<li>å¤šæ¨¡æ€ä¸‰æ¨¡æ€è½¬æ¢å™¨é€šè¿‡ä¸‰ç§å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶èåˆä¸‰æ¨¡æ€ç‰¹å¾ã€‚</li>
<li>æ¨¡å‹æé«˜äº†å¤šæ¨¡æ€æ•°æ®çš„è¡¨ç¤ºå’Œåˆ†æèƒ½åŠ›ã€‚</li>
<li>å®éªŒåœ¨Weiboå’ŒGossipcopä¸¤ä¸ªå‡æ–°é—»æ•°æ®é›†ä¸Šè¿›è¡Œï¼ŒéªŒè¯äº†TT-BLIPæ¨¡å‹çš„æ€§èƒ½ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.12481">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4372300d6a87aa87f209b4ca50f71681.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d8fe056969d064ec53baf7696f2cd25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e7b9b8b36c4066de88aef78b47cfefab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-487a3fce70536c02986477ba4ccd5ee9.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Transformer-Based-Model-for-Monocular-Visual-Odometry-A-Video-Understanding-Approach"><a href="#Transformer-Based-Model-for-Monocular-Visual-Odometry-A-Video-Understanding-Approach" class="headerlink" title="Transformer-Based Model for Monocular Visual Odometry: A Video   Understanding Approach"></a>Transformer-Based Model for Monocular Visual Odometry: A Video   Understanding Approach</h2><p><strong>Authors:AndrÃ© O. FranÃ§ani, Marcos R. O. A. Maximo</strong></p>
<p>Estimating the cameraâ€™s pose given images from a single camera is a traditional task in mobile robots and autonomous vehicles. This problem is called monocular visual odometry and often relies on geometric approaches that require considerable engineering effort for a specific scenario. Deep learning methods have been shown to be generalizable after proper training and with a large amount of available data. Transformer-based architectures have dominated the state-of-the-art in natural language processing and computer vision tasks, such as image and video understanding. In this work, we deal with the monocular visual odometry as a video understanding task to estimate the 6 degrees of freedom of a cameraâ€™s pose. We contribute by presenting the TSformer-VO model based on spatio-temporal self-attention mechanisms to extract features from clips and estimate the motions in an end-to-end manner. Our approach achieved competitive state-of-the-art performance compared with geometry-based and deep learning-based methods on the KITTI visual odometry dataset, outperforming the DeepVO implementation highly accepted in the visual odometry community. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/aofrancani/TSformer-VO">https://github.com/aofrancani/TSformer-VO</a>. </p>
<blockquote>
<p>æ ¹æ®å•ç›¸æœºå›¾åƒä¼°è®¡ç›¸æœºçš„å§¿æ€æ˜¯ç§»åŠ¨æœºå™¨äººå’Œè‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸­çš„ä¸€ä¸ªä¼ ç»Ÿä»»åŠ¡ã€‚è¿™ä¸ªé—®é¢˜è¢«ç§°ä¸ºå•ç›®è§†è§‰é‡Œç¨‹è®¡ï¼Œé€šå¸¸ä¾èµ–äºç‰¹å®šåœºæ™¯éœ€è¦å¤§é‡å·¥ç¨‹åŠªåŠ›çš„å‡ ä½•æ–¹æ³•ã€‚æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨é€‚å½“çš„è®­ç»ƒå’Œå¤§é‡å¯ç”¨æ•°æ®ä¹‹åï¼Œå·²æ˜¾ç¤ºå‡ºå¯æ¨å¹¿æ€§ã€‚åŸºäºTransformerçš„æ¶æ„åœ¨è¯¸å¦‚å›¾åƒå’Œè§†é¢‘ç†è§£ç­‰é¢†åŸŸçš„è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å æ®äº†æœ€å…ˆè¿›çš„åœ°ä½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†å•ç›®è§†è§‰é‡Œç¨‹è®¡ä½œä¸ºè§†é¢‘ç†è§£ä»»åŠ¡æ¥å¤„ç†ï¼Œä»¥ä¼°è®¡ç›¸æœºçš„å…­è‡ªç”±åº¦å§¿æ€ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ—¶ç©ºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„TSformer-VOæ¨¡å‹ï¼Œä»¥ä»ç‰‡æ®µä¸­æå–ç‰¹å¾å¹¶ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼ä¼°è®¡è¿åŠ¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨KITTIè§†è§‰é‡Œç¨‹è®¡æ•°æ®é›†ä¸Šä¸åŸºäºå‡ ä½•å’ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ç›¸æ¯”ï¼Œå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶è¶…è¶Šäº†è§†è§‰é‡Œç¨‹è®¡ç¤¾åŒºä¸­å¹¿å—æ¬¢è¿çš„DeepVOå®ç°ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/aofrancani/TSformer-VO%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/aofrancani/TSformer-VOä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.06121v3">PDF</a> This work has been accepted for publication in IEEE Access</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºTransformeræ¶æ„çš„TSformer-VOæ¨¡å‹åœ¨å•ç›®è§†è§‰é‡Œç¨‹è®¡ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚è¯¥æ¨¡å‹åˆ©ç”¨æ—¶ç©ºè‡ªæ³¨æ„åŠ›æœºåˆ¶æå–è§†é¢‘ç‰‡æ®µç‰¹å¾ï¼Œä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼ä¼°è®¡ç›¸æœºå§¿æ€çš„å…­ä¸ªè‡ªç”±åº¦ã€‚åœ¨KITTIè§†è§‰é‡Œç¨‹è®¡æ•°æ®é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡äº†åŸºäºå‡ ä½•å’Œæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬è¢«å¹¿æ³›æ¥å—çš„DeepVOå®ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TSformer-VOæ¨¡å‹åŸºäºTransformeræ¶æ„ï¼Œç”¨äºå¤„ç†å•ç›®è§†è§‰é‡Œç¨‹è®¡ä»»åŠ¡ã€‚</li>
<li>è¯¥æ¨¡å‹åˆ©ç”¨æ—¶ç©ºè‡ªæ³¨æ„åŠ›æœºåˆ¶æå–è§†é¢‘ç‰‡æ®µç‰¹å¾ã€‚</li>
<li>TSformer-VOä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼ä¼°è®¡ç›¸æœºå§¿æ€çš„å…­ä¸ªè‡ªç”±åº¦ã€‚</li>
<li>åœ¨KITTIè§†è§‰é‡Œç¨‹è®¡æ•°æ®é›†ä¸Šï¼ŒTSformer-VOæ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡äº†åŸºäºå‡ ä½•å’Œæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ã€‚</li>
<li>TSformer-VOæ¨¡å‹å…¬å¼€å¯ç”¨ï¼Œç½‘å€ä¸º<a target="_blank" rel="noopener" href="https://github.com/aofrancani/TSformer-VO%E3%80%82">https://github.com/aofrancani/TSformer-VOã€‚</a></li>
<li>è¯¥ç ”ç©¶å±•ç¤ºäº†æ·±åº¦å­¦ä¹ æ–¹æ³•çš„é€šç”¨æ€§ï¼Œç»è¿‡é€‚å½“è®­ç»ƒå’Œå¤§é‡æ•°æ®ï¼Œå¯ä»¥å–å¾—è‰¯å¥½æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2305.06121">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-72c2412bebc603839892e153916b420e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca27b6a8de93ec08e45da59c6112986e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-909ace692fddca128899f8d86075a177.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-007cfc1789a67c846bd873d3e24f4fa1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1feda3c7bd60acf4a4f0da003454b10.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-25/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-25/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-25/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-6cbb9040a5a327d23167ce5ea5a8fd48.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-25  Scalable Safe Multi-Agent Reinforcement Learning for Multi-Agent System
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-24/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-39d0d3f9feb1f6978c65d36425487122.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-01-24  Training Dialogue Systems by AI Feedback for Improving Overall Dialogue   Impression
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-24
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25011.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
