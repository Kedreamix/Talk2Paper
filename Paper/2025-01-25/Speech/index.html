<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech 方向最新论文已更新，请持续关注 Update in 2025-01-25  Exploring Finetuned Audio-LLM on Heart Murmur Features">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-71c7a8f8e1851c32c5a561985b44222a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-01-25
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    27 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-25-更新"><a href="#2025-01-25-更新" class="headerlink" title="2025-01-25 更新"></a>2025-01-25 更新</h1><h2 id="Exploring-Finetuned-Audio-LLM-on-Heart-Murmur-Features"><a href="#Exploring-Finetuned-Audio-LLM-on-Heart-Murmur-Features" class="headerlink" title="Exploring Finetuned Audio-LLM on Heart Murmur Features"></a>Exploring Finetuned Audio-LLM on Heart Murmur Features</h2><p><strong>Authors:Adrian Florea, Xilin Jiang, Nima Mesgarani, Xiaofan Jiang</strong></p>
<p>Large language models (LLMs) for audio have excelled in recognizing and analyzing human speech, music, and environmental sounds. However, their potential for understanding other types of sounds, particularly biomedical sounds, remains largely underexplored despite significant scientific interest. In this study, we focus on diagnosing cardiovascular diseases using phonocardiograms, i.e., heart sounds. Most existing deep neural network (DNN) paradigms are restricted to heart murmur classification (healthy vs unhealthy) and do not predict other acoustic features of the murmur such as timing, grading, harshness, pitch, and quality, which are important in helping physicians diagnose the underlying heart conditions. We propose to finetune an audio LLM, Qwen2-Audio, on the PhysioNet CirCor DigiScope phonocardiogram (PCG) dataset and evaluate its performance in classifying 11 expert-labeled murmur features. Additionally, we aim to achieve more noise-robust and generalizable system by exploring a preprocessing segmentation algorithm using an audio representation model, SSAMBA. Our results indicate that the LLM-based model outperforms state-of-the-art methods in 8 of the 11 features and performs comparably in the remaining 3. Moreover, the LLM successfully classifies long-tail murmur features with limited training data, a task that all previous methods have failed to classify. These findings underscore the potential of audio LLMs as assistants to human cardiologists in enhancing heart disease diagnosis. </p>
<blockquote>
<p>对于音频的大型语言模型（LLMs）已经在识别和分析人类语音、音乐和环境声音方面表现出卓越的能力。然而，尽管科学界对其充满兴趣，但它们对于理解其他类型的声音，尤其是生物医学声音，还有很大的潜力尚未挖掘。在这项研究中，我们关注于心音图（即心脏声音）的心血管疾病诊断。现有的大多数深度神经网络（DNN）模式仅限于心脏杂音分类（健康与否），并不能预测杂音的其他声学特征，如时间、等级、严厉程度、音高和音质等，这些特征在帮助医生诊断潜在的心脏状况时十分重要。我们提议微调音频LLM Qwen2-Audio在PhysioNet CirCor DigiScope心音图数据集上，并评估其在分类专家标注的11个杂音特征方面的性能。此外，我们还希望通过使用音频表示模型的预处理分割算法SSAMBA，实现更稳健且通用的系统。我们的结果表明，基于LLM的模型在11个特征中有8个优于当前先进技术方法，其余3个表现相当。此外，LLM成功分类了长尾杂音特征，而之前在有限训练数据的情况下，所有方法均未能进行分类。这些发现突显了音频LLM在辅助人类心脏病学家提高心脏病诊断方面的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13884v1">PDF</a> 5 pages, 1 figure, and 3 tables. Submitted to IEEE&#x2F;ACM Conference on   Connected Health: Applications, Systems , and Engineering Technologies</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）的音频技术在识别和分析人类语音、音乐和环境声音方面表现出卓越的能力。然而，它们在理解其他类型的声音，特别是生物医学声音方面的潜力，尽管有大量的科学兴趣，但仍被大大低估。本研究专注于使用心音图（心脏声音）诊断心血管疾病。大多数现有的深度神经网络（DNN）模式仅限于心脏杂音分类（健康与否），并不能预测杂音的其他声学特征，如时间、分级、粗糙度、音高等，这些特征在帮助医生诊断潜在的心脏疾病中非常重要。本研究提出对音频LLM Qwen2-Audio进行微调，在PhysioNet CirCor DigiScope心音图数据集上评估其在分类11种专家标记的杂音特征方面的性能。此外，本研究还旨在通过探索使用音频表示模型的预处理分割算法SSAMBA，实现更抗噪声和更具通用性的系统。结果表明，基于LLM的模型在11个特征中的8个上优于最新方法，并在其余3个上表现相当。此外，LLM成功地对长尾杂音特征进行了分类，而以前的所有方法都无法完成此任务。这些发现强调了音频LLM作为人类心脏病诊断的辅助工具的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在音频领域具有出色的识别和分析能力，但在生物医学声音方面的应用仍被低估。</li>
<li>本研究关注于心音图（心音）在心血管疾病诊断中的应用。</li>
<li>现有深度神经网络（DNN）主要局限于心脏杂音分类，无法预测杂音的其他声学特征。</li>
<li>研究提出对音频LLM Qwen2-Audio进行微调以分类心音图中的杂音特征。</li>
<li>基于LLM的模型在多数杂音特征分类上表现优越，尤其是长尾杂音特征的分类。</li>
<li>预处理分割算法SSAMBA有助于提高系统的噪声鲁棒性和通用性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13884">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6f8b6ddbf11cec16873f6f2fa25c950a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb00e99b8c59949eb4fc81fa49f6331d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56c52d31cdbf7d4de1fc9e71ac62e7c0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ee2fb97a62a470f9eddb6a084f39947.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bb8140f02f89ac9142c260a937b8402.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Predicting-Compact-Phrasal-Rewrites-with-Large-Language-Models-for-ASR-Post-Editing"><a href="#Predicting-Compact-Phrasal-Rewrites-with-Large-Language-Models-for-ASR-Post-Editing" class="headerlink" title="Predicting Compact Phrasal Rewrites with Large Language Models for ASR   Post Editing"></a>Predicting Compact Phrasal Rewrites with Large Language Models for ASR   Post Editing</h2><p><strong>Authors:Hao Zhang, Felix Stahlberg, Shankar Kumar</strong></p>
<p>Large Language Models (LLMs) excel at rewriting tasks such as text style transfer and grammatical error correction. While there is considerable overlap between the inputs and outputs in these tasks, the decoding cost still increases with output length, regardless of the amount of overlap. By leveraging the overlap between the input and the output, Kaneko and Okazaki (2023) proposed model-agnostic edit span representations to compress the rewrites to save computation. They reported an output length reduction rate of nearly 80% with minimal accuracy impact in four rewriting tasks. In this paper, we propose alternative edit phrase representations inspired by phrase-based statistical machine translation. We systematically compare our phrasal representations with their span representations. We apply the LLM rewriting model to the task of Automatic Speech Recognition (ASR) post editing and show that our target-phrase-only edit representation has the best efficiency-accuracy trade-off. On the LibriSpeech test set, our method closes 50-60% of the WER gap between the edit span model and the full rewrite model while losing only 10-20% of the length reduction rate of the edit span model. </p>
<blockquote>
<p>大型语言模型（LLM）擅长文本改写任务，如文本风格转换和语法错误修正。虽然这些任务的输入和输出之间存在大量重叠，但随着输出长度的增加，解码成本仍然会增加，无论重叠程度如何。Kaneko和Okazaki（2023年）利用输入和输出之间的重叠，提出了与模型无关的编辑跨度表示法，以压缩重写内容，从而节省计算量。他们报告称，在四项重写任务中，输出长度缩减率接近80%，且对精度的影响微乎其微。在本文中，我们受到基于短语的统计机器翻译的启发，提出了替代的编辑短语表示法。我们系统地比较了我们的短语表示法与他们的跨度表示法。我们将LLM重写模型应用于语音识别（ASR）的后期编辑任务，并发现我们的目标短语仅编辑表示法具有最佳的效率-准确性权衡。在LibriSpeech测试集上，我们的方法缩小了编辑跨度模型和全重写模型之间词错误率（WER）的差距的50-60%，同时只失去了编辑跨度模型长度缩减率的10-20%。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13831v1">PDF</a> accepted by ICASSP 2025</p>
<p><strong>Summary</strong><br>大语言模型（LLM）在文本风格转换和语法错误纠正等改写任务中表现优异。利用输入和输出之间的重叠性，Kaneko和Okazaki提出了模型无关的编辑跨度表示法来压缩重写内容以节省计算。受短语统计机器翻译的启发，我们提出新的短语编辑表示方法，系统性地对比了短语表示与跨度表示法。我们将LLM重写模型应用于语音识别（ASR）后编辑任务，发现目标短语编辑表示法具有最佳的效率与准确性权衡。在LibriSpeech测试集上，我们的方法缩小了编辑跨度模型和全重写模型之间的词错误率（WER）差距的50-60%，同时仅损失了编辑跨度模型的长度缩减率的10-20%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大语言模型（LLM）擅长文本改写任务，如文本风格转换和语法错误纠正。</li>
<li>Kaneko和Okazaki利用输入和输出间的重叠性，提出模型无关的编辑跨度表示法来压缩重写内容以节省计算成本。</li>
<li>编辑跨度表示法能够实现近80%的输出长度缩减率，且对准确性影响极小。</li>
<li>提出的短语编辑表示法受到短语统计机器翻译的启发，并与跨度表示法进行了系统性对比。</li>
<li>将LLM重写模型应用于语音识别（ASR）后编辑任务时，目标短语编辑表示法展现出最佳的效率与准确性权衡。</li>
<li>在LibriSpeech测试集上，新方法缩小了词错误率（WER）差距的50-60%，相较于编辑跨度模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13831">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-299a9baf49c5ae4ddb79f145d25d5ad0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7418ca64b1abcded84714dfc256bf146.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b966b0707850d52930aaa7a75a59d64.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b20ad09238d81319cc09492595c4334.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-632b2ec62cf0458bed9cbbcc186163a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e60a33342c813d6e249d05a7d73a8ed3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Neural-Vocoders-as-Speech-Enhancers"><a href="#Neural-Vocoders-as-Speech-Enhancers" class="headerlink" title="Neural Vocoders as Speech Enhancers"></a>Neural Vocoders as Speech Enhancers</h2><p><strong>Authors:Andong Li, Zhihang Sun, Fengyuan Hao, Xiaodong Li, Chengshi Zheng</strong></p>
<p>Speech enhancement (SE) and neural vocoding are traditionally viewed as separate tasks. In this work, we observe them under a common thread: the rank behavior of these processes. This observation prompts two key questions: \textit{Can a model designed for one task’s rank degradation be adapted for the other?} and \textit{Is it possible to address both tasks using a unified model?} Our empirical findings demonstrate that existing speech enhancement models can be successfully trained to perform vocoding tasks, and a single model, when jointly trained, can effectively handle both tasks with performance comparable to separately trained models. These results suggest that speech enhancement and neural vocoding can be unified under a broader framework of speech restoration. Code: <a target="_blank" rel="noopener" href="https://github.com/Andong-Li-speech/Neural-Vocoders-as-Speech-Enhancers">https://github.com/Andong-Li-speech/Neural-Vocoders-as-Speech-Enhancers</a>. </p>
<blockquote>
<p>语音增强（SE）和神经声码（Neural Vocoding）传统上被视为两个独立的任务。在这项工作中，我们从这些过程等级行为的共同点进行观察。这一观察结果引发了两个关键问题：一是“为某一任务的等级退化设计的模型是否可以适应另一任务？”二是“是否可以使用统一模型来解决这两个任务？”我们的经验发现表明，现有的语音增强模型可以成功训练以执行声码任务，并且当单一模型进行联合训练时，可以有效地处理这两个任务，其性能可与单独训练的模型相媲美。这些结果表明，语音增强和神经声码可以在更广泛的语音恢复框架下统一起来。代码：<a target="_blank" rel="noopener" href="https://github.com/Andong-Li-speech/Neural-Vocoders-as-Speech-Enhancers%E3%80%82">https://github.com/Andong-Li-speech/Neural-Vocoders-as-Speech-Enhancers。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13465v1">PDF</a> 6 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>本文探讨了语音增强和神经声码器两个传统上被视为独立的任务之间的联系。通过观察它们的等级行为，发现这两个任务具有共同的特点。实证研究结果表明，现有的语音增强模型可以成功训练用于声码器任务，并且一个联合训练模型可以同时处理这两个任务，性能与单独训练的模型相当。这暗示着语音增强和神经声码器可以在更广泛的语音恢复框架下统一起来。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语音增强和神经声码器是两个传统独立任务，但本文通过观察它们的等级行为发现了它们的共同特点。</li>
<li>现有语音增强模型可以成功训练用于声码器任务。</li>
<li>一个联合训练模型可以同时处理语音增强和神经声码器任务。</li>
<li>联合训练模型的性能与单独训练的模型相当。</li>
<li>实证研究证明了统一框架下处理这两个任务的可行性。</li>
<li>语音增强和神经声码器都属于语音恢复的范畴，可以在更广泛的框架下进行统一。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13465">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f631a8b90004ec22a6d1e3145da67de1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e627aea87467f34eafd20bb6ae535ea5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea6cb5c3a2984b276feffdcf5d7cd8ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f805c715fe4b4539f8f5e1bc3263429.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81d766b1a014b86132f2b9af37651041.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Bridging-The-Multi-Modality-Gaps-of-Audio-Visual-and-Linguistic-for-Speech-Enhancement"><a href="#Bridging-The-Multi-Modality-Gaps-of-Audio-Visual-and-Linguistic-for-Speech-Enhancement" class="headerlink" title="Bridging The Multi-Modality Gaps of Audio, Visual and Linguistic for   Speech Enhancement"></a>Bridging The Multi-Modality Gaps of Audio, Visual and Linguistic for   Speech Enhancement</h2><p><strong>Authors:Meng-Ping Lin, Jen-Cheng Hou, Chia-Wei Chen, Shao-Yi Chien, Jun-Cheng Chen, Xugang Lu, Yu Tsao</strong></p>
<p>Speech Enhancement (SE) aims to improve the quality of noisy speech. It has been shown that additional visual cues can further improve performance. Given that speech communication involves audio, visual, and linguistic modalities, it is natural to expect another performance boost by incorporating linguistic information. However, bridging the modality gaps to efficiently incorporate linguistic information, along with audio and visual modalities during knowledge transfer, is a challenging task. In this paper, we propose a novel multi-modality learning framework for SE. In the model framework, a state-of-the-art diffusion Model backbone is utilized for Audio-Visual Speech Enhancement (AVSE) modeling where both audio and visual information are directly captured by microphones and video cameras. Based on this AVSE, the linguistic modality employs a PLM to transfer linguistic knowledge to the visual acoustic modality through a process termed Cross-Modal Knowledge Transfer (CMKT) during AVSE model training. After the model is trained, it is supposed that linguistic knowledge is encoded in the feature processing of the AVSE model by the CMKT, and the PLM will not be involved during inference stage. We carry out SE experiments to evaluate the proposed model framework. Experimental results demonstrate that our proposed AVSE system significantly enhances speech quality and reduces generative artifacts, such as phonetic confusion compared to the state-of-the-art. Moreover, our visualization results demonstrate that our Cross-Modal Knowledge Transfer method further improves the generated speech quality of our AVSE system. These findings not only suggest that Diffusion Model-based techniques hold promise for advancing the state-of-the-art in AVSE but also justify the effectiveness of incorporating linguistic information to improve the performance of Diffusion-based AVSE systems. </p>
<blockquote>
<p>语音增强（SE）旨在提高带噪声语音的质量。研究表明，额外的视觉线索可以进一步提高性能。鉴于语音通信涉及音频、视觉和语言学模式，融入语言学信息来提高性能是自然而然的想法。然而，在知识转移过程中，弥合模式间的差距以有效地融入语言学信息，同时结合音频和视觉模式是一项具有挑战性的任务。在本文中，我们为SE提出了一种新型多模式学习框架。在该模型框架中，我们采用最先进的扩散模型主干进行视听语音增强（AVSE）建模，其中音频和视觉信息由麦克风和摄像机直接捕获。基于AVSE，语言学模式采用PLM（预训练语言模型），通过跨模态知识转移（CMKT）过程在AVSE模型训练阶段将语言知识转移到视觉声学模式。模型训练完成后，假设语言知识通过CMKT编码在AVSE模型的特性处理中，且PLM不会参与推理阶段。我们进行了SE实验来评估所提出的模型框架。实验结果表明，与最先进的水平相比，我们提出的AVSE系统显著提高了语音质量，减少了生成物中的伪影，如语音混淆。此外，我们的可视化结果证明，我们的跨模态知识转移方法进一步提高了AVSE系统的生成语音质量。这些发现不仅表明基于扩散模型的技术在推进AVSE方面前景广阔，而且验证了融入语言学信息以提高基于扩散的AVSE系统性能的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13375v1">PDF</a> </p>
<p><strong>总结</strong></p>
<p>本文主要探讨多模态学习框架在语音增强（SE）中的应用。研究提出一种基于扩散模型的音频视觉语音增强（AVSE）模型框架，该框架结合了音频、视觉和语言三种模态的信息。在模型训练中，通过跨模态知识转移（CMKT）将语言模态的知识转移到视觉声学模态。实验结果表明，该模型能显著提高语音质量和降低生成过程中的失真和音素混淆等副作用。本研究表明扩散模型技术在AVSE方面具有发展潜力，并且结合语言信息能提高扩散模型的AVSE系统性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>多模态学习框架结合了音频、视觉和语言三种模态的信息，以提高语音增强（SE）的性能。</li>
<li>提出了一种基于扩散模型的音频视觉语音增强（AVSE）模型框架，利用麦克风和视频摄像头直接捕获音频和视觉信息。</li>
<li>通过跨模态知识转移（CMKT）在AVSE模型训练中转移语言模态的知识。</li>
<li>实验结果表明，该模型能显著提高语音质量和降低生成过程中的失真和音素混淆。</li>
<li>扩散模型技术在AVSE方面展现出发展潜力。</li>
<li>结合语言信息能提高扩散模型的AVSE系统性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13375">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7533e09e695f9283daca93ccb1b70051.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1bb543d2e4315f907786c7a420809322.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9182da7019b6fafa1b80aef1daf8cd6.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Generative-Data-Augmentation-Challenge-Zero-Shot-Speech-Synthesis-for-Personalized-Speech-Enhancement"><a href="#Generative-Data-Augmentation-Challenge-Zero-Shot-Speech-Synthesis-for-Personalized-Speech-Enhancement" class="headerlink" title="Generative Data Augmentation Challenge: Zero-Shot Speech Synthesis for   Personalized Speech Enhancement"></a>Generative Data Augmentation Challenge: Zero-Shot Speech Synthesis for   Personalized Speech Enhancement</h2><p><strong>Authors:Jae-Sung Bae, Anastasia Kuznetsova, Dinesh Manocha, John Hershey, Trausti Kristjansson, Minje Kim</strong></p>
<p>This paper presents a new challenge that calls for zero-shot text-to-speech (TTS) systems to augment speech data for the downstream task, personalized speech enhancement (PSE), as part of the Generative Data Augmentation workshop at ICASSP 2025. Collecting high-quality personalized data is challenging due to privacy concerns and technical difficulties in recording audio from the test scene. To address these issues, synthetic data generation using generative models has gained significant attention. In this challenge, participants are tasked first with building zero-shot TTS systems to augment personalized data. Subsequently, PSE systems are asked to be trained with this augmented personalized dataset. Through this challenge, we aim to investigate how the quality of augmented data generated by zero-shot TTS models affects PSE model performance. We also provide baseline experiments using open-source zero-shot TTS models to encourage participation and benchmark advancements. Our baseline code implementation and checkpoints are available online. </p>
<blockquote>
<p>本文提出了一个新的挑战，旨在利用零样本文本到语音（TTS）系统为下游任务增强语音数据，作为ICASSP 2025生成数据增强研讨会的一部分。由于隐私担忧和从测试场景中录制音频的技术困难，收集高质量个性化数据具有挑战性。为了解决这些问题，使用生成模型进行合成数据生成已经引起了广泛关注。在此挑战中，首先要求参与者构建零样本TTS系统以增强个性化数据。随后，要求使用此增强的个性化数据集训练PSE系统。通过这一挑战，我们旨在研究零样本TTS模型生成的增强数据质量如何影响PSE模型性能。我们还使用开源零样本TTS模型提供基线实验，以鼓励参与并评估进展。我们的基线代码实现和检查点已在线提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13372v1">PDF</a> Accepted to ICASSP 2025 Satellite Workshop: Generative Data   Augmentation for Real-World Signal Processing Applications</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在ICASSP 2025生成数据增强研讨会上提出的新挑战，即利用零样本文本到语音（TTS）系统增强语音数据，用于下游任务个性化语音增强（PSE）。由于隐私问题和现场录音的技术困难，收集高质量个性化数据具有挑战性。因此，利用生成模型生成合成数据已成为关注的焦点。该挑战要求参与者首先建立零样本TTS系统以增强个性化数据，然后训练使用此增强个性化数据集的PSE系统。此挑战旨在研究零样本TTS模型生成的增强数据质量对PSE模型性能的影响，并提供使用开源零样本TTS模型的基线实验以鼓励参与并评估进展。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文介绍了一个新挑战，旨在利用零样本文本到语音（TTS）系统增强语音数据，以改进个性化语音增强（PSE）系统的性能。</li>
<li>收集高质量个性化数据存在挑战，由于隐私问题和现场录音的技术困难，合成数据生成成为解决方案。</li>
<li>挑战要求参与者建立零样本TTS系统以增强个性化数据，并用此增强数据集训练PSE系统。</li>
<li>该挑战旨在研究零样本TTS模型生成的增强数据质量对PSE模型性能的影响。</li>
<li>提供基线实验，使用开源零样本TTS模型，以鼓励参与并评估进展。</li>
<li>线上提供基线代码实现和检查点。</li>
<li>通过此挑战，期望推动个性化语音增强技术的发展，并深入了解数据质量在其中的作用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13372">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7a5d181fd77f44228bb22569dbcadd6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0858d1a311e26a168baf52bf70b5d701.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-79e8bb4beeaec3d3d9a0443d9a33ef64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0aa2ad5789cff49578319acb1427fbad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4465f5822f3a07095003a0360d31e8ef.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Let-SSMs-be-ConvNets-State-space-Modeling-with-Optimal-Tensor-Contractions"><a href="#Let-SSMs-be-ConvNets-State-space-Modeling-with-Optimal-Tensor-Contractions" class="headerlink" title="Let SSMs be ConvNets: State-space Modeling with Optimal Tensor   Contractions"></a>Let SSMs be ConvNets: State-space Modeling with Optimal Tensor   Contractions</h2><p><strong>Authors:Yan Ru Pei</strong></p>
<p>We introduce Centaurus, a class of networks composed of generalized state-space model (SSM) blocks, where the SSM operations can be treated as tensor contractions during training. The optimal order of tensor contractions can then be systematically determined for every SSM block to maximize training efficiency. This allows more flexibility in designing SSM blocks beyond the depthwise-separable configuration commonly implemented. The new design choices will take inspiration from classical convolutional blocks including group convolutions, full convolutions, and bottleneck blocks. We architect the Centaurus network with a mixture of these blocks, to balance between network size and performance, as well as memory and computational efficiency during both training and inference. We show that this heterogeneous network design outperforms its homogeneous counterparts in raw audio processing tasks including keyword spotting, speech denoising, and automatic speech recognition (ASR). For ASR, Centaurus is the first network with competitive performance that can be made fully state-space based, without using any nonlinear recurrence (LSTMs), explicit convolutions (CNNs), or (surrogate) attention mechanism. Source code is available at github.com&#x2F;Brainchip-Inc&#x2F;Centaurus </p>
<blockquote>
<p>我们介绍了Centaurus网络，它是一类由广义状态空间模型（SSM）块组成的网络。在训练过程中，SSM操作可以被视为张量收缩。然后可以为每个SSM块系统地确定张量收缩的最佳顺序，以最大化训练效率。这允许在设计SSM块时具有更大的灵活性，超越了通常实现的深度可分离配置。新的设计选择将从经典的卷积块中获得灵感，包括分组卷积、全卷积和瓶颈块。我们使用这些块的混合来构建Centaurus网络，以在网络规模和性能之间取得平衡，同时在训练和推理期间的内存和计算效率之间取得平衡。我们证明了这种异构网络设计在原始音频处理任务上的表现优于其同构的对应物，包括关键词识别、语音降噪和自动语音识别（ASR）。对于ASR，Centaurus是第一个具有竞争力的完全基于状态空间的网络，无需使用任何非线性递归（LSTM）、显式卷积（CNN）或（替代）注意力机制。源代码可在github.com&#x2F;Brainchip-Inc&#x2F;Centaurus上找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13230v1">PDF</a> 25 pages, 7 figures</p>
<p><strong>Summary</strong></p>
<p>Centaurus网络结合了广义状态空间模型（SSM）块，通过系统确定SSM块的最优张量收缩顺序以优化训练效率。新设计允许更多灵活性，并超越了深度可分离配置的局限。通过混合不同的SSM块和经典卷积块，实现了网络大小与性能、内存与计算效率的平衡。在原始音频处理任务中，包括关键词识别、语音降噪和自动语音识别（ASR），Centaurus网络表现出优于同类网络的性能。特别是在ASR领域，Centaurus网络成为首个完全基于状态空间模型、无需非线性递归（LSTM）、显式卷积（CNN）或替代注意力机制的网络，表现出强大的竞争力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Centaurus网络结合广义状态空间模型（SSM）块，通过系统优化张量收缩顺序提升训练效率。</li>
<li>SSM块设计更具灵活性，超越深度可分离配置的局限。</li>
<li>结合不同的SSM块和经典卷积块，实现网络大小、性能、内存和计算效率的平衡。</li>
<li>Centaurus网络在原始音频处理任务中表现优越，包括关键词识别、语音降噪和自动语音识别（ASR）。</li>
<li>Centaurus网络在ASR领域的表现尤为突出，成为首个无需非线性递归（LSTM）、显式卷积（CNN）或替代注意力机制的网络。</li>
<li>该网络的源代码可在github.com&#x2F;Brainchip-Inc&#x2F;Centaurus找到。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13230">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-8d3d1e986274f21780876159bfa0dec9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b5c25996edb7d5380db4620fdc4333a6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Interactive-Cycle-Model-The-Linkage-Combination-among-Automatic-Speech-Recognition-Large-Language-Models-and-Smart-Glasses"><a href="#Interactive-Cycle-Model-The-Linkage-Combination-among-Automatic-Speech-Recognition-Large-Language-Models-and-Smart-Glasses" class="headerlink" title="Interactive Cycle Model: The Linkage Combination among Automatic Speech   Recognition, Large Language Models and Smart Glasses"></a>Interactive Cycle Model: The Linkage Combination among Automatic Speech   Recognition, Large Language Models and Smart Glasses</h2><p><strong>Authors:Libo Wang</strong></p>
<p>This research proposes the interaction loop model “ASR-LLMs-Smart Glasses”, which model combines automatic speech recognition, large language model and smart glasses to facilitate seamless human-computer interaction. And the methodology of this research involves decomposing the interaction process into different stages and elements. Speech is captured and processed by ASR, then analyzed and interpreted by LLMs. The results are then transmitted to smart glasses for display. The feedback loop is complete when the user interacts with the displayed data. Mathematical formulas are used to quantify the performance of the model that revolves around core evaluation points: accuracy, coherence, and latency during ASR speech-to-text conversion. The research results are provided theoretically to test and evaluate the feasibility and performance of the model. Detailed architectural details and experimental process have been uploaded to Github, the link is:<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git">https://github.com/brucewang123456789/GeniusTrail.git</a>. </p>
<blockquote>
<p>本研究提出了“ASR-LLMs-智能眼镜”交互循环模型，该模型结合了自动语音识别、大型语言模型和智能眼镜，促进了无缝的人机交互。研究方法涉及将交互过程分解成不同的阶段和元素。语音被ASR捕获并处理，然后通过LLMs进行分析和解释。结果随后传输到智能眼镜进行显示。当用户与显示的数据进行交互时，反馈循环完成。研究使用数学公式来量化模型性能，围绕核心评估点：ASR语音到文本转换过程中的准确性、连贯性和延迟。研究结果从理论上提供了测试和评估模型可行性和性能的依据。详细的架构细节和实验过程已经上传到Github，链接为：<a target="_blank" rel="noopener" href="https://github.com/brucewang123456789/GeniusTrail.git%E3%80%82">https://github.com/brucewang123456789/GeniusTrail.git。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.10362v3">PDF</a> OpenReview submitted. 10 pages of text and 2 figures</p>
<p><strong>Summary</strong>：该研究提出了一个名为“ASR-LLMs-智能眼镜”的互动循环模型，该模型结合了自动语音识别、大型语言模型和智能眼镜，以促进无缝人机交互。研究方法涉及将互动过程分解成不同的阶段和元素。通过ASR捕捉和处理语音，然后由LLMs进行分析和解释。结果将传输到智能眼镜进行显示。在用户与显示的数据进行交互时，反馈循环完成。围绕核心评估点（准确性、连贯性和延迟性），使用数学公式对模型性能进行量化。研究结果在理论上进行了测试和评估模型的可行性及性能。详细的架构和实验过程已上传至GitHub，链接为：[链接地址]。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>提出了一种新型的互动循环模型——“ASR-LLMs-智能眼镜”，整合了自动语音识别、大型语言模型和智能眼镜技术。</li>
<li>模型的工作流程包括：通过ASR捕捉和处理语音，LLMs进行结果分析和解释，然后将结果传输至智能眼镜进行显示。</li>
<li>用户与智能眼镜展示的数据互动完成反馈循环。</li>
<li>模型性能评估主要通过三个核心点：准确性、连贯性和延迟性。</li>
<li>研究结果在理论上进行了测试，证明了模型的可行性及性能。</li>
<li>详细的模型架构和实验过程已公开在GitHub上，供公众查阅和参考。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.10362">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-71c7a8f8e1851c32c5a561985b44222a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-25/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-25/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-25/GAN/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-dad36fd9fe340297e9b45dd413020e17.jpg" class="responsive-img" alt="GAN">
                        
                        <span class="card-title">GAN</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            GAN 方向最新论文已更新，请持续关注 Update in 2025-01-25  PhotoGAN Generative Adversarial Neural Network Acceleration with   Silicon Photonics
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                    GAN
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/GAN/">
                        <span class="chip bg-color">GAN</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-25/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c615036463815b15aea1e1a4884b3611.jpg" class="responsive-img" alt="无监督/半监督/对比学习">
                        
                        <span class="card-title">无监督/半监督/对比学习</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            无监督/半监督/对比学习 方向最新论文已更新，请持续关注 Update in 2025-01-25  Retrievals Can Be Detrimental A Contrastive Backdoor Attack Paradigm on   Retrieval-Augmented Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    无监督/半监督/对比学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">无监督/半监督/对比学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">10160.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
