<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-01-25  Self-Supervised Diffusion MRI Denoising via Iterative and Stable   Refinement">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-65b9cf9e48ea8187bdaca9ec411867dc.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-01-25
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    40 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-01-25-更新"><a href="#2025-01-25-更新" class="headerlink" title="2025-01-25 更新"></a>2025-01-25 更新</h1><h2 id="Self-Supervised-Diffusion-MRI-Denoising-via-Iterative-and-Stable-Refinement"><a href="#Self-Supervised-Diffusion-MRI-Denoising-via-Iterative-and-Stable-Refinement" class="headerlink" title="Self-Supervised Diffusion MRI Denoising via Iterative and Stable   Refinement"></a>Self-Supervised Diffusion MRI Denoising via Iterative and Stable   Refinement</h2><p><strong>Authors:Chenxu Wu, Qingpeng Kong, Zihang Jiang, S. Kevin Zhou</strong></p>
<p>Magnetic Resonance Imaging (MRI), including diffusion MRI (dMRI), serves as a &#96;&#96;microscope’’ for anatomical structures and routinely mitigates the influence of low signal-to-noise ratio scans by compromising temporal or spatial resolution. However, these compromises fail to meet clinical demands for both efficiency and precision. Consequently, denoising is a vital preprocessing step, particularly for dMRI, where clean data is unavailable. In this paper, we introduce Di-Fusion, a fully self-supervised denoising method that leverages the latter diffusion steps and an adaptive sampling process. Unlike previous approaches, our single-stage framework achieves efficient and stable training without extra noise model training and offers adaptive and controllable results in the sampling process. Our thorough experiments on real and simulated data demonstrate that Di-Fusion achieves state-of-the-art performance in microstructure modeling, tractography tracking, and other downstream tasks. </p>
<blockquote>
<p>磁共振成像（MRI），包括扩散MRI（dMRI），可作为解剖结构的“显微镜”，并通过妥协时间或空间分辨率来常规缓解低信噪比扫描的影响。然而，这些妥协无法满足临床对效率和精确度的需求。因此，去噪是一个重要的预处理步骤，特别是对于无法获得清洁数据的dMRI而言尤为关键。在本文中，我们介绍了Di-Fusion，这是一种完全自监督的去噪方法，它利用后续的扩散步骤和自适应采样过程。不同于以前的方法，我们的单阶段框架实现了高效且稳定的训练，无需额外的噪声模型训练，并在采样过程中提供了自适应和可控的结果。我们在真实和模拟数据上的实验表明，Di-Fusion在微观结构建模、轨迹跟踪和其他下游任务中达到了最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13514v1">PDF</a> 39pages, 34figures</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Di-Fusion这一全新的全自监督降噪方法，该方法利用扩散MRI的后扩散步骤和自适应采样过程进行高效稳定的训练，无需额外的噪声模型训练。实验证明，Di-Fusion在微观结构建模、纤维跟踪等下游任务上达到了先进性能，并且能够提供可控的自适应采样结果。此外，对于不可用干净的原始数据或扩散MRI中的难点场景进行清晰的解剖学图像解析尤其有价值。这表明其克服了影响解析准确度的不利因素，并提升了数据质量。Di-Fusion对于改善MRI图像质量、提高诊断精度具有重要意义。 </p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Di-Fusion是一种全自监督的降噪方法，适用于MRI图像的处理。</li>
<li>该方法利用扩散MRI的后扩散步骤和自适应采样过程进行训练，无需额外的噪声模型训练。</li>
<li>Di-Fusion对于解微结构建模、纤维跟踪等下游任务有着良好的性能表现。它在实现图像的高清晰度的同时维持了图像的空间和时间分辨率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13514">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-bbaadbddf45b408765a49ce410e3d8c8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d2d1a42d37888423a30da1944c6810d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-65b9cf9e48ea8187bdaca9ec411867dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed77d961fef18aa3249063739d74c48b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a21867dcc0ec8c1cb4f46729ca73914c.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Modelling-the-energy-dependent-X-ray-variability-of-Mrk-335"><a href="#Modelling-the-energy-dependent-X-ray-variability-of-Mrk-335" class="headerlink" title="Modelling the energy dependent X-ray variability of Mrk 335"></a>Modelling the energy dependent X-ray variability of Mrk 335</h2><p><strong>Authors:K. Akhila, Ranjeev Misra, Rathin Sarma, Savithri H. Ezhikode, K. Jeena</strong></p>
<p>We present a technique which predicts the energy dependent fractional r.m.s for linear correlated variations of a pair of spectral parameters and apply it to an XMM-Newton observation of Mrk 335. The broadband X-ray spectrum can be interpreted as a patchy absorber partially covering the primary emission, a warm and hot coronal emission or a relativistically blurred reflection along with the primary emission. The fractional r.m.s has a non-monotonic behavior with energy for segments of lengths 3 and 6 ksecs. For each spectral model, we consider every pair of spectral parameters and fit the predicted r.m.s with the observed ones, to get the pair which provides the best fit. We find that a variation in at least two parameters is required for all spectral interpretations. For both time segments, variations in the covering fraction of the absorber and the primary power law index gives the best result for the partial covering model, while a variation in the normalization and spectral index of the warm component gives the best fit in the two corona interpretation. For the reflection model, the best fit parameters are different for the two time segment lengths, and the results suggests that more than two parameters are required to explain the data. This, combined with the extreme values of emissivity index and reflection fraction parameters obtained from the spectral analysis, indicates that the blurred reflection model might not be a suitable explanation for the Mrk 335 spectrum. We discuss the results as well as the potential of the technique to be applied to other data sets of different AGN. </p>
<blockquote>
<p>我们提出了一种技术，该技术能够预测一对光谱参数的线性相关变化中的能量相关分数均方根（r.m.s），并将其应用于XMM-牛顿对Mrk 335的观察。宽带X射线光谱可以被解释为部分覆盖主发射区域的斑块吸收器、温暖和炽热的冕发射，或者与主发射区域相伴的相对论性模糊反射。分数均方根（r.m.s）在长度为3和6千秒的时间段内随能量变化表现出非单调行为。对于每个光谱模型，我们考虑每对光谱参数，并用观察到的r.m.s值拟合预测的r.m.s值，以获得提供最佳拟合的配对。我们发现，所有光谱解释都需要至少两个参数的变化。对于两个时间段，吸收器的覆盖分数和主幂律指数的变化为部分覆盖模型提供了最佳结果，而暖成分的标准化和光谱指数的变化为两冕解释提供了最佳拟合。对于反射模型，两个时间段长度的最佳拟合参数不同，结果暗示需要超过两个参数来解释数据。结合从光谱分析获得的发射率指数和反射分数参数的极端值，表明模糊的反射模型可能不适合解释Mrk 335的光谱。我们讨论了结果以及该技术应用于其他不同活跃星系核（AGN）数据集的可能性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13458v1">PDF</a> Accepted for publication in JHEAP</p>
<p><strong>Summary</strong></p>
<p>该文介绍了一种预测线性相关光谱参数变化分数的能量依赖性的技术，并将其应用于Mrk 335的XMM-Newton观测。文章探讨了部分遮挡模型、暖冠与热冠发射模型以及相对论的模糊反射模型等三种光谱解释。其中，部分遮挡模型的最佳拟合参数为遮挡物覆盖分数和主要幂律指数的变化；在暖冠模型中，归一化和光谱指数的变化提供了最佳拟合。反射模型的最佳拟合参数随时间长度变化而不同，且结果暗示需要超过两个参数来解释数据。结合从光谱分析获得的极端发射率指数和反射因子参数值，模糊的反射模型可能不适合解释Mrk 335光谱。文章结果对于未来分析不同活动星系核（AGN）的数据集具有潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文章介绍了一种预测能量相关分数的技术，并应用于Mrk 335的XMM-Newton观测数据。</li>
<li>文章探讨了三种光谱解释模型：部分遮挡模型、暖冠与热冠发射模型以及相对论的模糊反射模型。</li>
<li>部分遮挡模型的最佳拟合参数是遮挡物覆盖分数和主要幂律指数的变化。</li>
<li>在暖冠模型中，归一化和光谱指数的变化提供了最佳拟合结果。</li>
<li>反射模型的最佳拟合参数随时间长度变化而不同，且需要超过两个参数来解释数据。<br>6.模糊的反射模型可能不适合解释Mrk 335光谱，结合从光谱分析获得的参数值得出此结论。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13458">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cc23eff77b751dd211a21cec4b07db1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14b196582f5eb261d726cbef5d6dd3e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f3678af98b3832294605d06423a450c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c92e3597e4ed75d63fabbc67d7e07f9b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-245d2894ac91a585645a5a63514eec0b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca3775c7e3b54f49fe07fe8875073e14.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ff7568b268cb166bb72c5f9f2c9a553.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Unraveling-Normal-Anatomy-via-Fluid-Driven-Anomaly-Randomization"><a href="#Unraveling-Normal-Anatomy-via-Fluid-Driven-Anomaly-Randomization" class="headerlink" title="Unraveling Normal Anatomy via Fluid-Driven Anomaly Randomization"></a>Unraveling Normal Anatomy via Fluid-Driven Anomaly Randomization</h2><p><strong>Authors:Peirong Liu, Ana Lawry Aguila, Juan E. Iglesias</strong></p>
<p>Data-driven machine learning has made significant strides in medical image analysis. However, most existing methods are tailored to specific modalities and assume a particular resolution (often isotropic). This limits their generalizability in clinical settings, where variations in scan appearance arise from differences in sequence parameters, resolution, and orientation. Furthermore, most general-purpose models are designed for healthy subjects and suffer from performance degradation when pathology is present. We introduce UNA (Unraveling Normal Anatomy), the first modality-agnostic learning approach for normal brain anatomy reconstruction that can handle both healthy scans and cases with pathology. We propose a fluid-driven anomaly randomization method that generates an unlimited number of realistic pathology profiles on-the-fly. UNA is trained on a combination of synthetic and real data, and can be applied directly to real images with potential pathology without the need for fine-tuning. We demonstrate UNA’s effectiveness in reconstructing healthy brain anatomy and showcase its direct application to anomaly detection, using both simulated and real images from 3D healthy and stroke datasets, including CT and MRI scans. By bridging the gap between healthy and diseased images, UNA enables the use of general-purpose models on diseased images, opening up new opportunities for large-scale analysis of uncurated clinical images in the presence of pathology. Code is available at <a target="_blank" rel="noopener" href="https://github.com/peirong26/UNA">https://github.com/peirong26/UNA</a>. </p>
<blockquote>
<p>数据驱动的机器学习在医学图像分析方面取得了重大进展。然而，大多数现有方法都是针对特定模态的，并假设了特定的分辨率（通常为等距）。这限制了它们在临床环境中的通用性，因为扫描外观的变化来自于序列参数、分辨率和方位的差异。此外，大多数通用模型都是为健康受试者设计的，在有病理情况出现时会出现性能下降。我们引入了UNA（解开正常解剖结构），这是一种用于正常大脑解剖结构重建的模态无关学习方法，它可以处理健康扫描和带有病理情况的病例。我们提出了一种流体驱动异常随机化方法，可以实时生成无限数量的现实病理特征。UNA是在合成数据和真实数据的组合上进行训练的，可以直接应用于潜在病理的真实图像，无需微调。我们展示了UNA在重建健康大脑解剖结构方面的有效性，并展示了其在异常检测方面的直接应用，使用来自3D健康和中风数据集的模拟和真实图像，包括CT和MRI扫描。通过缩小健康图像与疾病图像之间的差距，UNA使得通用模型能够在疾病图像上使用，为存在病理情况的未整理临床图像的大规模分析提供了新的机会。代码可在<a target="_blank" rel="noopener" href="https://github.com/peirong26/UNA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/peirong26/UNA找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13370v1">PDF</a> 16 pages, 6 figures</p>
<p><strong>Summary</strong></p>
<p>数据驱动的机器学习在医学图像分析方面取得了显著进展，但大多数现有方法针对特定模态并假设特定分辨率，限制了其在临床环境中的通用性。本文介绍了一种通用的、模态无关的学习方法UNA（Unraveling Normal Anatomy），用于重建正常脑结构，可处理健康扫描和病理情况。UNA采用流体驱动异常随机化方法，可实时生成无限数量的逼真病理特征。UNA经过合成和真实数据的训练，可直接应用于潜在病理的真实图像，无需微调。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据驱动的机器学习在医学图像分析中的应用进展。</li>
<li>现有方法的局限性：针对特定模态和分辨率，限制了其在临床环境中的通用性。</li>
<li>UNA是一种通用的、模态无关的学习方法，用于重建正常脑结构。</li>
<li>UNA可以处理健康和病理情况下的扫描。</li>
<li>UNA采用流体驱动异常随机化方法，生成逼真的病理特征。</li>
<li>UNA经过合成和真实数据的训练，可直接应用于真实图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13370">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-666eceb86ae1aa8fc1cf31ef7273b04b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b7af2996b1d4ea7a0a0f1aa7181a1b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31235e911381ef250ca7bbb625f1894a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a49cb35b90b031dc4a4e72a26627fe23.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MEDFORM-A-Foundation-Model-for-Contrastive-Learning-of-CT-Imaging-and-Clinical-Numeric-Data-in-Multi-Cancer-Analysis"><a href="#MEDFORM-A-Foundation-Model-for-Contrastive-Learning-of-CT-Imaging-and-Clinical-Numeric-Data-in-Multi-Cancer-Analysis" class="headerlink" title="MEDFORM: A Foundation Model for Contrastive Learning of CT Imaging and   Clinical Numeric Data in Multi-Cancer Analysis"></a>MEDFORM: A Foundation Model for Contrastive Learning of CT Imaging and   Clinical Numeric Data in Multi-Cancer Analysis</h2><p><strong>Authors:Daeun Jung, Jaehyeok Jang, Sooyoung Jang, Yu Rang Park</strong></p>
<p>Computed tomography (CT) and clinical numeric data are essential modalities for cancer evaluation, but building large-scale multimodal training datasets for developing medical foundation models remains challenging due to the structural complexity of multi-slice CT data and high cost of expert annotation. In this study, we propose MEDFORM, a multimodal pre-training strategy that guides CT image representation learning using complementary information from clinical data for medical foundation model development. MEDFORM efficiently processes CT slice through multiple instance learning (MIL) and adopts a dual pre-training strategy: first pretraining the CT slice feature extractor using SimCLR-based self-supervised learning, then aligning CT and clinical modalities through cross-modal contrastive learning. Our model was pre-trained on three different cancer types: lung cancer (141,171 slices), breast cancer (8,100 slices), colorectal cancer (10,393 slices). The experimental results demonstrated that this dual pre-training strategy improves cancer classification performance and maintains robust performance in few-shot learning scenarios. Code available at <a target="_blank" rel="noopener" href="https://github.com/DigitalHealthcareLab/25MultiModalFoundationModel.git">https://github.com/DigitalHealthcareLab/25MultiModalFoundationModel.git</a> </p>
<blockquote>
<p>计算机断层扫描（CT）和临床数值数据对于癌症评估至关重要，但由于多层CT数据的结构复杂性以及专家标注的高成本，构建用于开发医疗基础模型的大规模多模式训练数据集仍然是一个挑战。在本研究中，我们提出了MEDFORM，这是一种多模式预训练策略，通过利用临床数据的补充信息来指导CT图像表示学习，以开发医疗基础模型。MEDFORM通过多实例学习（MIL）有效地处理CT切片，并采用双重预训练策略：首先使用基于SimCLR的自我监督学习对CT切片特征提取器进行预训练，然后通过跨模式对比学习对齐CT和临床模式。我们的模型在三种不同类型的癌症上进行了预训练：肺癌（141,171切片）、乳腺癌（8,100切片）和结肠癌（10,393切片）。实验结果表明，这种双重预训练策略提高了癌症分类性能，并在小样本学习场景中保持了稳健的性能。代码可通过以下网址获取：<a target="_blank" rel="noopener" href="https://github.com/DigitalHealthcareLab/25MultiModalFoundationModel.git">网址</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13277v1">PDF</a> 8 pages, 1 figure</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为MEDFORM的多模态预训练策略，该策略利用临床数据中的互补信息指导CT图像表示学习，用于开发医学基础模型。通过采用多实例学习（MIL）处理CT切片，并采用基于SimCLR的自监督学习和跨模态对比学习的双预训练策略，模型在肺癌、乳腺癌和结肠癌三种不同癌症类型上的预训练表现优异。实验结果证明，该双预训练策略能提高癌症分类性能，并在小样本学习场景下保持稳健性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MEDFORM是一种多模态预训练策略，用于医学基础模型的开发，结合CT图像和临床数据。</li>
<li>MEDFORM采用多实例学习（MIL）处理CT切片，以提高模型的效率和性能。</li>
<li>该策略包括两个阶段的预训练：首先使用基于SimCLR的自监督学习预训练CT切片特征提取器，然后通过跨模态对比学习对齐CT和临床模态。</li>
<li>模型在肺癌、乳腺癌和结肠癌三种癌症类型上进行预训练。</li>
<li>实验结果表明，MEDFORM策略能提高癌症分类性能。</li>
<li>该策略在小样本学习场景下表现出稳健的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13277">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4fcab9de5bbc4422f438b1e058ed7acf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba86564eb0e2a231ab18b51566b28d32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-125a42217eaa579cc3e76791eb3e8d48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c615036463815b15aea1e1a4884b3611.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Novel-Scene-Coupling-Semantic-Mask-Network-for-Remote-Sensing-Image-Segmentation"><a href="#A-Novel-Scene-Coupling-Semantic-Mask-Network-for-Remote-Sensing-Image-Segmentation" class="headerlink" title="A Novel Scene Coupling Semantic Mask Network for Remote Sensing Image   Segmentation"></a>A Novel Scene Coupling Semantic Mask Network for Remote Sensing Image   Segmentation</h2><p><strong>Authors:Xiaowen Ma, Rongrong Lian, Zhenkai Wu, Renxiang Guan, Tingfeng Hong, Mengjiao Zhao, Mengting Ma, Jiangtao Nie, Zhenhong Du, Siyang Song, Wei Zhang</strong></p>
<p>As a common method in the field of computer vision, spatial attention mechanism has been widely used in semantic segmentation of remote sensing images due to its outstanding long-range dependency modeling capability. However, remote sensing images are usually characterized by complex backgrounds and large intra-class variance that would degrade their analysis performance. While vanilla spatial attention mechanisms are based on dense affine operations, they tend to introduce a large amount of background contextual information and lack of consideration for intrinsic spatial correlation. To deal with such limitations, this paper proposes a novel scene-Coupling semantic mask network, which reconstructs the vanilla attention with scene coupling and local global semantic masks strategies. Specifically, scene coupling module decomposes scene information into global representations and object distributions, which are then embedded in the attention affinity processes. This Strategy effectively utilizes the intrinsic spatial correlation between features so that improve the process of attention modeling. Meanwhile, local global semantic masks module indirectly correlate pixels with the global semantic masks by using the local semantic mask as an intermediate sensory element, which reduces the background contextual interference and mitigates the effect of intra-class variance. By combining the above two strategies, we propose the model SCSM, which not only can efficiently segment various geospatial objects in complex scenarios, but also possesses inter-clean and elegant mathematical representations. Experimental results on four benchmark datasets demonstrate the the effectiveness of the above two strategies for improving the attention modeling of remote sensing images. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/xwmaxwma/rssegmentation">https://github.com/xwmaxwma/rssegmentation</a> </p>
<blockquote>
<p>作为计算机视觉领域的常见方法，空间注意力机制因其出色的长程依赖建模能力而广泛应用于遥感图像的语义分割。然而，遥感图像通常具有复杂的背景和较大的类内差异，这可能会降低其分析性能。虽然普通空间注意力机制是基于密集的仿射运算，但它们往往引入大量背景上下文信息，并且没有考虑到内在的空间相关性。为了应对这些局限性，本文提出了一种新的场景耦合语义掩码网络，该网络通过场景耦合和局部全局语义掩码策略重建了普通注意力。具体来说，场景耦合模块将场景信息分解为全局表示和对象分布，然后将其嵌入到注意力亲和过程中。该策略有效地利用了特征之间的内在空间相关性，从而改进了注意力建模的过程。同时，局部全局语义掩码模块通过利用局部语义掩码作为中间感知元素，间接地将像素与全局语义掩码相关联，这减少了背景上下文的干扰并减轻了类内差异的影响。通过结合上述两种策略，我们提出了SCSM模型，该模型不仅可以在复杂场景中有效地分割各种地理空间对象，而且还具有简洁优雅的数学表示。在四个基准数据集上的实验结果表明，上述两种策略在改进遥感图像的注意力建模方面非常有效。数据集和代码可通过<a target="_blank" rel="noopener" href="https://github.com/xwmaxwma/rssegmentation%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/xwmaxwma/rssegmentation获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.13130v1">PDF</a> Accepted by ISPRS Journal of Photogrammetry and Remote Sensing</p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的场景耦合语义掩膜网络，通过场景耦合和局部全局语义掩膜策略，改进了基于密集仿射运算的传统空间注意机制。该策略能更有效地处理遥感图像语义分割中的复杂背景和类内差异问题，提升注意力建模过程。实验结果表明，该策略在四个基准数据集上均有效。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>空间注意机制在遥感图像语义分割中得到广泛应用，但存在处理复杂背景和类内差异时的性能下降问题。</li>
<li>现有空间注意机制基于密集仿射运算，易引入大量背景上下文信息，且未充分考虑特征间的内在空间相关性。</li>
<li>本文提出了一种新型的场景耦合语义掩膜网络，通过场景耦合模块和局部全局语义掩膜策略，改进了注意力建模过程。</li>
<li>场景耦合模块将场景信息分解为全局表示和对象分布，并将其嵌入注意力亲和过程中，有效利用特征间的内在空间相关性。</li>
<li>局部全局语义掩膜模块通过局部语义掩膜作为中间感知元素，间接地将像素与全局语义掩膜相关联，减少了背景上下文干扰和类内差异的影响。</li>
<li>结合上述两个策略，提出的SCSM模型不仅能有效地分割复杂场景中的各类地理空间对象，还具有简洁优雅的数学表示。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.13130">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-67b4eac1bca56d7112c4e0142171054d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a42a0ff498e03212e21e65cf7fb2a24.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c164c31f1b1a008e90b7bdb29b430f48.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7142e701882014649216af4280f1bc4c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74534a7c471000c0adb729543cf541ca.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UNSURE-self-supervised-learning-with-Unknown-Noise-level-and-Stein’s-Unbiased-Risk-Estimate"><a href="#UNSURE-self-supervised-learning-with-Unknown-Noise-level-and-Stein’s-Unbiased-Risk-Estimate" class="headerlink" title="UNSURE: self-supervised learning with Unknown Noise level and Stein’s   Unbiased Risk Estimate"></a>UNSURE: self-supervised learning with Unknown Noise level and Stein’s   Unbiased Risk Estimate</h2><p><strong>Authors:Julián Tachella, Mike Davies, Laurent Jacques</strong></p>
<p>Recently, many self-supervised learning methods for image reconstruction have been proposed that can learn from noisy data alone, bypassing the need for ground-truth references. Most existing methods cluster around two classes: i) Stein’s Unbiased Risk Estimate (SURE) and similar approaches that assume full knowledge of the distribution, and ii) Noise2Self and similar cross-validation methods that require very mild knowledge about the noise distribution. The first class of methods tends to be impractical, as the noise level is often unknown in real-world applications, and the second class is often suboptimal compared to supervised learning. In this paper, we provide a theoretical framework that characterizes this expressivity-robustness trade-off and propose a new approach based on SURE, but unlike the standard SURE, does not require knowledge about the noise level. Throughout a series of experiments, we show that the proposed estimator outperforms other existing self-supervised methods on various imaging inverse problems. </p>
<blockquote>
<p>最近，许多用于图像重建的自我监督学习方法已被提出，这些方法可以仅从噪声数据中学习，无需真实参考。现有的大多数方法主要集中在两类上：i) Stein的无偏风险估计（SURE）和假设对分布有充分了解的类似方法，以及ii) Noise2Self和关于噪声分布只需非常轻微了解的类似交叉验证方法。第一类方法往往不切实际，因为在现实世界的实际应用中往往不知道噪声水平，第二类方法与监督学习相比通常表现不佳。在本文中，我们提供了一个理论框架，该框架描述了表达性稳健性权衡的特点，并提出了一种基于SURE的新方法，但与标准的SURE不同，它不需要了解噪声水平。通过一系列实验，我们表明，所提出的估计器在各种成像反问题上优于其他现有的自监督方法。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.01985v3">PDF</a> </p>
<p><strong>Summary</strong><br>医学图像重建中的自监督学习方法近年来备受关注。现有方法多依赖于噪声水平分布知识，实际应用中难以实现或效果不理想。本文提出一种基于Stein的无偏风险估计的新方法，无需了解噪声水平信息，实验证明在多种图像逆问题上表现优异。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>自监督学习方法在医学图像重建中逐渐受到重视，能够从噪声数据中学习，无需参考真实图像。</li>
<li>现有方法主要分为两类：基于Stein的无偏风险估计（SURE）和其他假设对噪声分布有充分了解的方法。</li>
<li>基于SURE的方法在实际应用中由于噪声水平未知而显得不实用。</li>
<li>Noise2Self等交叉验证方法虽然对噪声分布知识需求较轻，但效果常不如监督学习。</li>
<li>本文提出的新方法基于SURE但无需了解噪声水平信息，解决了现有方法的局限性。</li>
<li>实验证明，新方法在各种图像逆问题上的表现优于其他自监督方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.01985">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0d9efca56c0e0a13ef26e5c22e41b096.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adf5a04399bf9bd8d3f6a101d2c05b34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b392b6d93ba06f19d94a41bf0d0e68ac.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="ST-USleepNet-A-Spatial-Temporal-Coupling-Prominence-Network-for-Multi-Channel-Sleep-Staging"><a href="#ST-USleepNet-A-Spatial-Temporal-Coupling-Prominence-Network-for-Multi-Channel-Sleep-Staging" class="headerlink" title="ST-USleepNet: A Spatial-Temporal Coupling Prominence Network for   Multi-Channel Sleep Staging"></a>ST-USleepNet: A Spatial-Temporal Coupling Prominence Network for   Multi-Channel Sleep Staging</h2><p><strong>Authors:Jingying Ma, Qika Lin, Ziyu Jia, Mengling Feng</strong></p>
<p>Sleep staging is critical to assess sleep quality and diagnose disorders. Despite advancements in artificial intelligence enabling automated sleep staging, significant challenges remain: (1) Simultaneously extracting prominent temporal and spatial sleep features from multi-channel raw signals, including characteristic sleep waveforms and salient spatial brain networks. (2) Capturing the spatial-temporal coupling patterns essential for accurate sleep staging. To address these challenges, we propose a novel framework named ST-USleepNet, comprising a spatial-temporal graph construction module (ST) and a U-shaped sleep network (USleepNet). The ST module converts raw signals into a spatial-temporal graph based on signal similarity, temporal, and spatial relationships to model spatial-temporal coupling patterns. The USleepNet employs a U-shaped structure for both the temporal and spatial streams, mirroring its original use in image segmentation to isolate significant targets. Applied to raw sleep signals and graph data from the ST module, USleepNet effectively segments these inputs, simultaneously extracting prominent temporal and spatial sleep features. Testing on three datasets demonstrates that ST-USleepNet outperforms existing baselines, and model visualizations confirm its efficacy in extracting prominent sleep features and temporal-spatial coupling patterns across various sleep stages. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/Majy-Yuji/ST-USleepNet.git">https://github.com/Majy-Yuji/ST-USleepNet.git</a>. </p>
<blockquote>
<p>睡眠分期对于评估睡眠质量和诊断睡眠障碍至关重要。尽管人工智能的进步已经实现了自动化的睡眠分期，但仍存在重大挑战：(1) 从多通道原始信号中同时提取突出的时间和空间睡眠特征，包括特征睡眠波形和显著的脑空间网络。(2)捕捉对准确睡眠分期至关重要的时空耦合模式。为了解决这些挑战，我们提出了一种名为ST-USleepNet的新型框架，它包含一个时空图构建模块(ST)和一个U型睡眠网络(USleepNet)。ST模块根据信号相似性、时间和空间关系将原始信号转换为时空图，以模拟时空耦合模式。USleepNet采用U型结构，用于时间和空间流，模仿其在图像分割中的原始用途，以隔离重要目标。应用于来自ST模块的原始睡眠信号和图形数据，USleepNet有效地分割了这些输入，同时提取了突出的时间性和空间性睡眠特征。在三个数据集上的测试表明，ST-USleepNet的性能超过了现有基线，模型可视化证实了其在提取各睡眠阶段的重要睡眠特征和时空耦合模式方面的有效性。代码可在<a target="_blank" rel="noopener" href="https://github.com/Majy-Yuji/ST-USleepNet.git">https://github.com/Majy-Yuji/ST-USleepNet.git</a>处获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.11884v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为ST-USleepNet的新型框架，用于解决睡眠分期中从多通道原始信号中提取重要时空睡眠特征以及捕捉时空耦合模式的问题。该框架包括一个时空图构建模块（ST）和一个U型睡眠网络（USleepNet）。该框架在三个数据集上的测试表现优于现有基线，可有效提取重要睡眠特征和时空耦合模式。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>睡眠分期是评估睡眠质量和诊断睡眠障碍的关键。</li>
<li>目前在睡眠分期中面临从多通道原始信号提取重要时空睡眠特征和捕捉时空耦合模式的挑战。</li>
<li>ST-USleepNet框架由时空图构建模块（ST）和U型睡眠网络（USleepNet）组成，用于解决这些挑战。</li>
<li>ST模块将原始信号转换为基于信号相似性的时空图，以模拟时空耦合模式。</li>
<li>USleepNet采用U型结构，同时处理时空流，有效隔离显著目标，并从原始睡眠信号和ST模块的图数据中提取重要特征。</li>
<li>在三个数据集上的测试表明，ST-USleepNet的性能优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.11884">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-848c4ab4e5ff421828f3903cda3da0c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56b1e3e239fc71af13be7e3f22188382.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfd1120fbd5e69389e344ce677c842a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2bccd8ec792026cf3d40d2f90feb7377.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Robust-Simultaneous-Multislice-MRI-Reconstruction-Using-Deep-Generative-Priors"><a href="#Robust-Simultaneous-Multislice-MRI-Reconstruction-Using-Deep-Generative-Priors" class="headerlink" title="Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative   Priors"></a>Robust Simultaneous Multislice MRI Reconstruction Using Deep Generative   Priors</h2><p><strong>Authors:Shoujin Huang, Guanxiong Luo, Yunlin Zhao, Yilong Liu, Yuwan Wang, Kexin Yang, Jingzhe Liu, Hua Guo, Min Wang, Lingyan Zhang, Mengye Lyu</strong></p>
<p>Simultaneous multislice (SMS) imaging is a powerful technique for accelerating magnetic resonance imaging (MRI) acquisitions. However, SMS reconstruction remains challenging due to complex signal interactions between and within the excited slices. In this study, we introduce ROGER, a robust SMS MRI reconstruction method based on deep generative priors. Utilizing denoising diffusion probabilistic models (DDPM), ROGER begins with Gaussian noise and gradually recovers individual slices through reverse diffusion iterations while enforcing data consistency from measured k-space data within the readout concatenation framework. The posterior sampling procedure is designed such that the DDPM training can be performed on single-slice images without requiring modifications for SMS tasks. Additionally, our method incorporates a low-frequency enhancement (LFE) module to address the practical issue that SMS-accelerated fast spin echo (FSE) and echo planar imaging (EPI) sequences cannot easily embed fully-sampled autocalibration signals. Extensive experiments on both retrospectively and prospectively accelerated datasets demonstrate that ROGER consistently outperforms existing methods, enhancing both anatomical and functional imaging with strong out-of-distribution generalization. The source code and sample data for ROGER are available at <a target="_blank" rel="noopener" href="https://github.com/Solor-pikachu/ROGER">https://github.com/Solor-pikachu/ROGER</a>. </p>
<blockquote>
<p>层叠多切片（Simultaneous Multislice，简称SMS）成像是一种加速磁共振成像（MRI）采集的强大技术。然而，由于激发切片之间和切片内部的复杂信号相互作用，SMS重建仍然是一个挑战。在这项研究中，我们引入了基于深度生成先验的稳健SMS MRI重建方法——ROGER。利用降噪扩散概率模型（DDPM），ROGER从高斯噪声开始，通过反向扩散迭代逐渐恢复单个切片，同时在读出拼接框架内强制实施来自测量k空间数据的数据一致性。后采样程序的设计使得可以在单切片图像上对DDPM进行训练，而无需对SMS任务进行修改。此外，我们的方法还融入了一个低频增强（LFE）模块，以解决一个实际问题，即SMS加速的快速自旋回波（FSE）和回波平面成像（EPI）序列无法轻松嵌入完全采样的自动校准信号。对回顾性和前瞻性加速数据集的大量实验表明，ROGER始终优于现有方法，在解剖和功能性成像方面表现出强大的泛化能力。ROGER的源代码和样本数据可在<a target="_blank" rel="noopener" href="https://github.com/Solor-pikachu/ROGER%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Solor-pikachu/ROGER找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.21600v2">PDF</a> Submitted to Medical Image Analysis. New fMRI analysis and figures   are added since v1</p>
<p><strong>Summary</strong></p>
<p>基于深度生成先验的ROGER方法在SMS MRI重建中具有强大的性能。该方法利用去噪扩散概率模型（DDPM），从高斯噪声开始逐步恢复切片，同时通过反向扩散迭代在读取拼接框架中强制实施数据一致性。此外，ROGER方法还融入了低频增强模块，解决了SMS加速的快速自旋回波（FSE）和回声平面成像序列难以嵌入全采样自动校准信号的实际问题。实验证明，ROGER在回顾性和前瞻性加速的数据集上均表现出优于现有方法的效果，能够增强解剖和功能性成像的外部分布泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SMS成像是一种加速磁共振成像（MRI）的技术。</li>
<li>ROGER是一种基于深度生成先验的稳健的SMS MRI重建方法。</li>
<li>ROGER利用去噪扩散概率模型（DDPM）从高斯噪声开始逐步恢复切片。</li>
<li>ROGER通过反向扩散迭代在读取拼接框架中实施数据一致性。</li>
<li>ROGER融入了低频增强模块，解决了SMS加速成像中的实际问题。</li>
<li>ROGER在回顾性和前瞻性加速的数据集上表现出卓越性能。</li>
<li>ROGER增强了MRI的解剖和功能性成像效果，并具有强大的外部分布泛化能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.21600">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-0114b767b052f4316821fd86e8c39493.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dda360acc5e4809dbbd74fdd842906fe.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="How-to-Efficiently-Annotate-Images-for-Best-Performing-Deep-Learning-Based-Segmentation-Models-An-Empirical-Study-with-Weak-and-Noisy-Annotations-and-Segment-Anything-Model"><a href="#How-to-Efficiently-Annotate-Images-for-Best-Performing-Deep-Learning-Based-Segmentation-Models-An-Empirical-Study-with-Weak-and-Noisy-Annotations-and-Segment-Anything-Model" class="headerlink" title="How to Efficiently Annotate Images for Best-Performing Deep Learning   Based Segmentation Models: An Empirical Study with Weak and Noisy Annotations   and Segment Anything Model"></a>How to Efficiently Annotate Images for Best-Performing Deep Learning   Based Segmentation Models: An Empirical Study with Weak and Noisy Annotations   and Segment Anything Model</h2><p><strong>Authors:Yixin Zhang, Shen Zhao, Hanxue Gu, Maciej A. Mazurowski</strong></p>
<p>Deep neural networks (DNNs) have demonstrated exceptional performance across various image segmentation tasks. However, the process of preparing datasets for training segmentation DNNs is both labor-intensive and costly, as it typically requires pixel-level annotations for each object of interest. To mitigate this challenge, alternative approaches such as using weak labels (e.g., bounding boxes or scribbles) or less precise (noisy) annotations can be employed. Noisy and weak labels are significantly quicker to generate, allowing for more annotated images within the same time frame. However, the potential decrease in annotation quality may adversely impact the segmentation performance of the resulting model. In this study, we conducted a comprehensive cost-effectiveness evaluation on six variants of annotation strategies (9~10 sub-variants in total) across 4 datasets and conclude that the common practice of precisely outlining objects of interest is virtually never the optimal approach when annotation budget is limited. Both noisy and weak annotations showed usage cases that yield similar performance to the perfectly annotated counterpart, yet had significantly better cost-effectiveness. We hope our findings will help researchers be aware of the different available options and use their annotation budgets more efficiently, especially in cases where accurately acquiring labels for target objects is particularly costly. Our code will be made available on <a target="_blank" rel="noopener" href="https://github.com/yzluka/AnnotationEfficiency2D">https://github.com/yzluka/AnnotationEfficiency2D</a>. </p>
<blockquote>
<p>深度神经网络（DNNs）在各种图像分割任务中表现出了卓越的性能。然而，为训练分割DNNs准备数据集的过程既劳动密集又成本高昂，因为它通常需要对每个感兴趣的对象进行像素级的注释。为了缓解这一挑战，可以采用使用弱标签（如边界框或涂鸦）或不太精确（带有噪声）的注释等替代方法。带噪声和弱标签的生成速度要快得多，可以在同一时间框架内生成更多注释图像。然而，注释质量可能的下降可能会给所得模型的分割性能带来不利影响。本研究中，我们对4个数据集的6种注释策略变体（总计9~10个子变体）进行了全面的成本效益评估，并得出结论：当注释预算有限时，精确描绘感兴趣对象的一般做法几乎从未是最佳方法。带噪声和弱标签的用例产生的性能与完美注释的对应物相似，但成本效益却显著提高。我们希望我们的研究能帮助研究人员了解不同的可用选项，并更有效地利用他们的注释预算，特别是在为目标对象准确获取标签特别昂贵的情况下。我们的代码将在 <a target="_blank" rel="noopener" href="https://github.com/yzluka/AnnotationEfficiency2D">https://github.com/yzluka/AnnotationEfficiency2D</a> 上提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.10600v3">PDF</a> Supplemental information is in appendix</p>
<p><strong>Summary</strong><br>     深度神经网络在图像分割任务中表现出卓越性能，但训练分割深度神经网络的数据集制备既劳力密集又成本高昂，通常需要为每个目标对象进行像素级注释。为缓解这一问题，可采用弱标签（如边界框或涂鸦）或不精确的（噪声）注释等替代方法。噪声和弱标签生成更快，可在相同时间内生成更多注释图像。然而，注释质量的潜在下降可能会对所得模型的分割性能产生不利影响。本研究对六种注释策略（共9至10种子策略）进行了全面的成本效益评估，涉及四个数据集，得出结论：当注释预算有限时，精确描绘目标对象的常规做法几乎从未是最优选择。噪声和弱注释的使用案例产生了与完美注释相当的性能，但成本效益显著提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度神经网络在图像分割任务中表现出卓越性能。</li>
<li>数据集制备需要像素级注释，过程既劳力密集又成本高昂。</li>
<li>为缓解高成本问题，可采用弱标签或不精确的注释等替代方法。</li>
<li>噪声和弱标签生成更快，能在有限时间内生成更多注释图像。</li>
<li>注释质量的潜在下降可能影响模型的分割性能。</li>
<li>研究表明，在注释预算有限时，不必精确描绘目标对象。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.10600">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b1a06ab293ea93c15f70729ce1084640.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4248f4ff1fe1c0e00ebfcfed8552763.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89284c288c9cf5bc425f70e7ea93fd75.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Guided-Reconstruction-with-Conditioned-Diffusion-Models-for-Unsupervised-Anomaly-Detection-in-Brain-MRIs"><a href="#Guided-Reconstruction-with-Conditioned-Diffusion-Models-for-Unsupervised-Anomaly-Detection-in-Brain-MRIs" class="headerlink" title="Guided Reconstruction with Conditioned Diffusion Models for Unsupervised   Anomaly Detection in Brain MRIs"></a>Guided Reconstruction with Conditioned Diffusion Models for Unsupervised   Anomaly Detection in Brain MRIs</h2><p><strong>Authors:Finn Behrendt, Debayan Bhattacharya, Robin Mieling, Lennart Maack, Julia Krüger, Roland Opfer, Alexander Schlaefer</strong></p>
<p>The application of supervised models to clinical screening tasks is challenging due to the need for annotated data for each considered pathology. Unsupervised Anomaly Detection (UAD) is an alternative approach that aims to identify any anomaly as an outlier from a healthy training distribution. A prevalent strategy for UAD in brain MRI involves using generative models to learn the reconstruction of healthy brain anatomy for a given input image. As these models should fail to reconstruct unhealthy structures, the reconstruction errors indicate anomalies. However, a significant challenge is to balance the accurate reconstruction of healthy anatomy and the undesired replication of abnormal structures. While diffusion models have shown promising results with detailed and accurate reconstructions, they face challenges in preserving intensity characteristics, resulting in false positives. We propose conditioning the denoising process of diffusion models with additional information derived from a latent representation of the input image. We demonstrate that this conditioning allows for accurate and local adaptation to the general input intensity distribution while avoiding the replication of unhealthy structures. We compare the novel approach to different state-of-the-art methods and for different data sets. Our results show substantial improvements in the segmentation performance, with the Dice score improved by 11.9%, 20.0%, and 44.6%, for the BraTS, ATLAS and MSLUB data sets, respectively, while maintaining competitive performance on the WMH data set. Furthermore, our results indicate effective domain adaptation across different MRI acquisitions and simulated contrasts, an important attribute for general anomaly detection methods. The code for our work is available at <a target="_blank" rel="noopener" href="https://github.com/FinnBehrendt/Conditioned-Diffusion-Models-UAD">https://github.com/FinnBehrendt/Conditioned-Diffusion-Models-UAD</a> </p>
<blockquote>
<p>将监督模型应用于临床筛查任务是一项挑战，因为需要考虑的每种病理学都需要标注数据。无监督异常检测（UAD）是一种旨在从健康的训练分布中识别任何异常值的替代方法。在脑部MRI的UAD中，一种常见的策略是使用生成模型来学习给定输入图像的脑部结构的重建。由于这些模型无法重建不健康结构，因此重建误差会指示异常。然而，一个重大挑战是如何平衡健康解剖结构的准确重建和异常结构的非期望复制。虽然扩散模型在详细和准确的重建方面显示出有前景的结果，但它们面临着保持强度特征的挑战，从而导致误报。我们提出通过输入图像的潜在表示的派生信息对扩散模型的去噪过程进行条件处理。我们证明这种条件处理允许对一般输入强度分布进行准确和局部适应，同时避免复制不健康结构。我们将新方法与不同的最新技术和数据集进行了比较。我们的结果表明，在分割性能方面取得了重大改进，对于BraTS、ATLAS和MSLUB数据集，Dice得分分别提高了11.9%、20.0%和44.6%，同时在WMH数据集上保持了竞争性能。此外，我们的结果还表明，我们的方法在MRI采集和不同模拟对比度之间实现了有效的域适应，这对于一般的异常检测方法是一个重要的属性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/FinnBehrendt/Conditioned-Diffusion-Models-UAD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/FinnBehrendt/Conditioned-Diffusion-Models-UAD找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.04215v2">PDF</a> Preprint: Accepted paper at Combuters in Biology and medicine</p>
<p><strong>Summary</strong><br>     使用无监督异常检测（UAD）方法，通过生成模型学习健康脑结构的重建以发现异常。扩散模型在重建健康结构方面表现出良好性能，但在保持强度特征方面存在挑战。本研究提出将输入图像的潜在表示信息纳入扩散模型的去噪过程，以提高异常检测的准确性并避免复制不健康结构。研究结果显示，该方法在多个数据集上的分割性能显著提高，同时具有良好的跨不同MRI采集和模拟对比的域适应能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>监督模型应用于临床筛查任务时面临标注数据需求挑战。</li>
<li>无监督异常检测（UAD）旨在从健康训练分布中识别异常值。</li>
<li>在脑MRI的UAD中，生成模型用于学习健康脑结构的重建。</li>
<li>扩散模型在重建健康结构方面表现良好，但保持强度特征方面存在挑战。</li>
<li>将输入图像的潜在表示信息纳入扩散模型的去噪过程，以提高异常检测的准确性。</li>
<li>方法在多个数据集上的分割性能显著提高，并具有良好的域适应能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.04215">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a58d16688320eeed9231164c23823e34.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-26f89ee26535db9b5dc129d187e6f2d1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-01-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-01-25/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-25/TTS/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-79e8bb4beeaec3d3d9a0443d9a33ef64.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-01-25  Generative Data Augmentation Challenge Zero-Shot Speech Synthesis for   Personalized Speech Enhancement
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-01-25
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-01-25/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-6c777205d41ebaddd1102cdfd7df7209.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-01-25  IMAGINE-E Image Generation Intelligence Evaluation of State-of-the-art   Text-to-Image Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-01-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">12603.7k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
