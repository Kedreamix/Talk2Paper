<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-02-09  OmniBal Towards Fast Instruct-tuning for Vision-Language Models via   Omniverse Computation Balance">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5c7ad26b8fba8b43b39e61fddc5802f5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    12.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    50 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-09-更新"><a href="#2025-02-09-更新" class="headerlink" title="2025-02-09 更新"></a>2025-02-09 更新</h1><h2 id="OmniBal-Towards-Fast-Instruct-tuning-for-Vision-Language-Models-via-Omniverse-Computation-Balance"><a href="#OmniBal-Towards-Fast-Instruct-tuning-for-Vision-Language-Models-via-Omniverse-Computation-Balance" class="headerlink" title="OmniBal: Towards Fast Instruct-tuning for Vision-Language Models via   Omniverse Computation Balance"></a>OmniBal: Towards Fast Instruct-tuning for Vision-Language Models via   Omniverse Computation Balance</h2><p><strong>Authors:Yongqiang Yao, Jingru Tan, Jiahao Hu, Feizhao Zhang, Xin Jin, Bo Li, Ruihao Gong, Pengfei Liu</strong></p>
<p>Recently, vision-language instruct-tuning models have made significant progress due to their more comprehensive understanding of the world. In this work, we discovered that large-scale 3D parallel training on those models leads to an imbalanced computation load across different devices. The vision and language parts are inherently heterogeneous: their data distribution and model architecture differ significantly, which affects distributed training efficiency. We rebalanced the computational loads from data, model, and memory perspectives to address this issue, achieving more balanced computation across devices. These three components are not independent but are closely connected, forming an omniverse balanced training framework. Specifically, for the data, we grouped instances into new balanced mini-batches within and across devices. For the model, we employed a search-based method to achieve a more balanced partitioning. For memory optimization, we adaptively adjusted the re-computation strategy for each partition to utilize the available memory fully. We conducted extensive experiments to validate the effectiveness of our method. Compared with the open-source training code of InternVL-Chat, we significantly reduced GPU days, achieving about 1.8x speed-up. Our method’s efficacy and generalizability were further demonstrated across various models and datasets. Codes will be released at <a target="_blank" rel="noopener" href="https://github.com/ModelTC/OmniBal">https://github.com/ModelTC/OmniBal</a>. </p>
<blockquote>
<p>最近，由于视觉语言指令微调模型对世界的理解更加全面，它们取得了重大进展。在这项工作中，我们发现这些模型的大规模3D并行训练会导致不同设备之间的计算负载不平衡。视觉和语言部分本质上是异构的：它们的数据分布和模型架构存在很大差异，这影响了分布式训练的效率。我们从数据、模型和内存三个方面重新平衡计算负载，以解决这一问题，实现跨设备的更平衡计算。这三个组件不是独立的，而是紧密相联的，形成了一个全方位平衡的训练框架。具体来说，对于数据，我们将实例分组为跨设备内外的新平衡小批量。对于模型，我们采用基于搜索的方法来实现更平衡的划分。对于内存优化，我们自适应地调整每个分区的重新计算策略，以充分利用可用内存。我们进行了大量实验来验证我们方法的有效性。与InternVL-Chat的开源训练代码相比，我们大幅减少了GPU天数，实现了约1.8倍的速度提升。我们的方法在各种模型和数据集上的有效性和通用性得到了进一步证明。代码将在<a target="_blank" rel="noopener" href="https://github.com/ModelTC/OmniBal%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/ModelTC/OmniBal发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20761v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模三维并行训练在视觉语言指令微调模型上的不均衡计算负载问题被解决。通过从数据、模型和内存三个方面进行平衡调整，实现了更均衡的计算负载分配。该研究提高了分布式训练效率，减少了GPU天数，并在不同的模型和数据集上验证了其有效性和通用性。相关代码将在GitHub上发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言指令微调模型在大规模三维并行训练中存在计算负载不均衡的问题。</li>
<li>视觉和语言部分在数据分布和模型架构上存在固有差异，影响分布式训练效率。</li>
<li>为解决计算负载不均衡问题，从数据、模型和内存三个方面进行了平衡调整。</li>
<li>通过平衡数据的实例分组和模型搜索方法实现更均衡的计算负载分配。</li>
<li>优化内存管理策略以充分利用可用内存，适应不同的分区需求。</li>
<li>该方法显著提高了训练效率，减少了GPU天数，实现了约1.8倍的速度提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.20761">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-cccb5394e5083bf508b55deae5029833.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36c292c3621e8dc0e7e891c637fb7b94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e94b6e3107dfced375733b6222d8fca0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2445de10961c1b5aa637e4d36199e6d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e5090351b95da9e2f7ac0fab0532bb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-761c8541addedf7416be192890ca6629.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Advantage-Alignment-Algorithms"><a href="#Advantage-Alignment-Algorithms" class="headerlink" title="Advantage Alignment Algorithms"></a>Advantage Alignment Algorithms</h2><p><strong>Authors:Juan Agustin Duque, Milad Aghajohari, Tim Cooijmans, Razvan Ciuca, Tianyu Zhang, Gauthier Gidel, Aaron Courville</strong></p>
<p>Artificially intelligent agents are increasingly being integrated into human decision-making: from large language model (LLM) assistants to autonomous vehicles. These systems often optimize their individual objective, leading to conflicts, particularly in general-sum games where naive reinforcement learning agents empirically converge to Pareto-suboptimal Nash equilibria. To address this issue, opponent shaping has emerged as a paradigm for finding socially beneficial equilibria in general-sum games. In this work, we introduce Advantage Alignment, a family of algorithms derived from first principles that perform opponent shaping efficiently and intuitively. We achieve this by aligning the advantages of interacting agents, increasing the probability of mutually beneficial actions when their interaction has been positive. We prove that existing opponent shaping methods implicitly perform Advantage Alignment. Compared to these methods, Advantage Alignment simplifies the mathematical formulation of opponent shaping, reduces the computational burden and extends to continuous action domains. We demonstrate the effectiveness of our algorithms across a range of social dilemmas, achieving state-of-the-art cooperation and robustness against exploitation. </p>
<blockquote>
<p>人工智能代理正越来越多地融入人类的决策过程：从大型语言模型（LLM）助理到自动驾驶汽车。这些系统通常优化各自的目标，从而导致冲突，特别是在总收益游戏中的冲突尤为明显，其中天真的强化学习代理通常会收敛到帕累托次优的纳什均衡。为解决这一问题，对手塑形已成为在总收益游戏中寻找社会利益均衡的一种范式。在这项工作中，我们介绍了优势对齐（Advantage Alignment）算法家族，这些算法基于基本原理，能够高效直观地进行对手塑形。我们通过调整交互代理的优势来实现这一点，当它们的交互呈现正面时，增加了互惠行为的可能性。我们证明了现有的对手塑形方法隐含地执行优势对齐。与这些方法相比，优势对齐简化了对手塑形的数学公式，减轻了计算负担，并扩展到了连续动作领域。我们在一系列社会困境中展示了算法的有效性，实现了最先进的合作和对抗剥削的稳健性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14662v3">PDF</a> 25 Pages, 8 figures</p>
<p><strong>Summary</strong><br>：人工智能代理正越来越多地融入人类决策过程，从大型语言模型（LLM）助理到自动驾驶汽车。这些系统通常优化各自的目标，导致冲突，特别是在总和游戏中尤为明显，其中简单的强化学习代理往往会收敛到帕累托次优的纳什均衡。为解决这一问题，对手塑造已成为寻找一般总和游戏中社会利益均衡的一种范式。我们引入了一种优势对齐算法，这是一种从第一性原则派生出来的算法家族，能够高效直观地实现对手塑造。我们通过调整交互代理的优势来实现这一点，即在交互积极时增加互益行动的概率。我们证明了现有的对手塑造方法隐式地执行优势对齐。与这些方法相比，优势对齐简化了对手塑造的数学公式，降低了计算负担，并扩展到了连续动作领域。我们在一系列社会困境中展示了算法的有效性，实现了最先进的合作和对利用的稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人工智能代理正融入人类决策，引发冲突问题。</li>
<li>对手塑造是解决人工智能代理冲突的一种范式。</li>
<li>引入优势对齐算法家族，能够高效直观地实现对手塑造。</li>
<li>优势对齐通过调整交互代理的优势来实现互益行动的概率增加。</li>
<li>现有对手塑造方法隐式执行优势对齐。</li>
<li>优势对齐简化了对手塑造的数学公式，降低计算负担。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14662">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-65cb775cf4ebd8222a462464ec22721d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89e093ac0ec79b682c295ec784cb144e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Demystifying-Language-Model-Forgetting-with-Low-rank-Example-Associations"><a href="#Demystifying-Language-Model-Forgetting-with-Low-rank-Example-Associations" class="headerlink" title="Demystifying Language Model Forgetting with Low-rank Example   Associations"></a>Demystifying Language Model Forgetting with Low-rank Example   Associations</h2><p><strong>Authors:Xisen Jin, Xiang Ren</strong></p>
<p>Large Language models (LLMs) suffer from forgetting of upstream data when fine-tuned. Despite efforts on mitigating forgetting, few have investigated whether, and how forgotten upstream examples are dependent on newly learned tasks. Insights on such dependencies enable efficient and targeted mitigation of forgetting. In this paper, we empirically analyze forgetting that occurs in $N$ upstream examples of language modeling or instruction-tuning after fine-tuning LLMs on one of $M$ new tasks, visualized in $M\times N$ matrices. We show that the matrices are often well-approximated with low-rank matrices, indicating the dominance of simple associations between the learned tasks and forgotten upstream examples. Leveraging the analysis, we predict forgetting of upstream examples when fine-tuning on unseen tasks with matrix completion over the empirical associations. This enables fast identification of most forgotten examples without expensive inference on the entire upstream data. The approach, despite simplicity, outperforms prior approaches that learn semantic relationships of learned tasks and upstream examples with LMs for predicting forgetting. We demonstrate the practical utility of our analysis by showing statistically significantly reduced forgetting as we upweight predicted examples for replay at fine-tuning. Project page: <a target="_blank" rel="noopener" href="https://inklab.usc.edu/lm-forgetting-prediction/">https://inklab.usc.edu/lm-forgetting-prediction/</a> </p>
<blockquote>
<p>大型语言模型（LLM）在微调时会遗忘上游数据。尽管有人努力减轻遗忘，但很少有人研究遗忘的上游示例是否以及如何依赖于新学习的任务。对这种依赖性的洞察能够使人们有效地有针对性地缓解遗忘。在本文中，我们对在一个新的M任务上微调LLM后发生的上游语言建模或指令调整中的N个上游示例的遗忘进行了实证分析，并以$M\times N$矩阵的形式进行可视化。我们发现这些矩阵通常可以用低阶矩阵很好地逼近，这表明学到的任务和遗忘的上游示例之间存在简单的关联占据主导地位。利用分析，我们通过在经验关联上应用矩阵补全来预测在未见任务上进行微调时的上游示例遗忘。这能够在不耗费昂贵的整个上游数据的推理情况下快速识别出最容易遗忘的例子。尽管我们的方法很简单，但它超越了以前的方法，后者使用语言模型来学习学习到的任务和上游示例之间的语义关系来预测遗忘。我们通过展示在微调时通过回放预测的例子来显著减少遗忘，证明了我们的分析的实际效用。项目页面：[<a target="_blank" rel="noopener" href="https://inklab.usc.edu/lm-forgetting-prediction/]">https://inklab.usc.edu/lm-forgetting-prediction/]</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14026v4">PDF</a> 8 pages; preprint</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在微调时会遗忘上游数据。本文实证分析了在微调LLM后，对上游数据示例的遗忘情况，并探讨了这种遗忘与新增任务之间的关系。通过矩阵可视化分析，发现遗忘矩阵通常可以用低阶矩阵近似表示，表明新任务与遗忘的上游数据示例之间存在简单的关联。利用这种分析，可以通过矩阵补全对经验关联进行预测，从而快速识别最易遗忘的示例，无需在整个上游数据集上进行昂贵的推理操作。通过加权预测示例进行回放的方法显示，在减少遗忘方面表现出显著的实际效果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在微调时会遗忘上游数据。</li>
<li>遗忘与新增任务之间存在关联。</li>
<li>通过矩阵可视化分析遗忘情况。</li>
<li>遗忘矩阵可以用低阶矩阵近似表示。</li>
<li>利用矩阵补全预测遗忘。</li>
<li>可快速识别最易遗忘的示例，无需全数据集推理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14026">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ccffad4aa26b9de7b7461b289e5cd768.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd249bb0d6b5b3e5ed6f9d9bcfb16fce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b36019530b6e976f7e21e03b8718db74.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d0c7fe028f5de826dc20f779e16df0e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c7ad26b8fba8b43b39e61fddc5802f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-440967d431f24aa5a4324cfc1d684f85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-076de3fa9b49e4d1259254ef4466c1ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a8714858485c9fc5f2b25cf0b58b94b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SituationalLLM-Proactive-language-models-with-scene-awareness-for-dynamic-contextual-task-guidance"><a href="#SituationalLLM-Proactive-language-models-with-scene-awareness-for-dynamic-contextual-task-guidance" class="headerlink" title="SituationalLLM: Proactive language models with scene awareness for   dynamic, contextual task guidance"></a>SituationalLLM: Proactive language models with scene awareness for   dynamic, contextual task guidance</h2><p><strong>Authors:Muhammad Saif Ullah Khan, Muhammad Zeshan Afzal, Didier Stricker</strong></p>
<p>Large language models (LLMs) have achieved remarkable success in text-based tasks but often struggle to provide actionable guidance in real-world physical environments. This is because of their inability to recognize their limited understanding of the user’s physical context. We present SituationalLLM, a novel approach that integrates structured scene information into an LLM to deliver proactive, context-aware assistance. By encoding objects, attributes, and relationships in a custom Scene Graph Language, SituationalLLM actively identifies gaps in environmental context and seeks clarifications during user interactions. This behavior emerges from training on the Situational Awareness Database for Instruct-Tuning (SAD-Instruct), which combines diverse, scenario-specific scene graphs with iterative, dialogue-based refinements. Experimental results indicate that SituationalLLM outperforms generic LLM baselines in task specificity, reliability, and adaptability, paving the way for environment-aware AI assistants capable of delivering robust, user-centric guidance under real-world constraints. </p>
<blockquote>
<p>大型语言模型（LLM）在文本任务中取得了显著的成功，但在现实世界的物理环境中往往难以提供可操作的指导。这是因为它们无法识别自己对用户物理环境的有限理解。我们提出了SituationalLLM，这是一种新型方法，通过将结构化场景信息集成到LLM中，以提供主动、基于上下文的任务辅助。通过自定义场景图语言对物体、属性和关系进行编码，SituationalLLM能够主动识别环境上下文中的空白，并在用户交互过程中寻求澄清。这种行为源于对情境意识数据库指令微调（SAD-Instruct）的训练，它将多样化的场景特定场景图与迭代式的对话式微调相结合。实验结果表明，在任务特异性、可靠性和适应性方面，SituationalLLM优于通用LLM基线，为环境感知AI助理铺平了道路，能够在现实世界的约束下提供稳健的、以用户为中心的指导。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.13302v3">PDF</a> Revised Submission to Open Research Europe</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在文本任务中取得了显著成功，但在现实世界的物理环境中往往难以提供可操作的指导。本文提出了一种名为SituationalLLM的新方法，该方法将结构化场景信息集成到LLM中，以提供主动、基于上下文的帮助。通过自定义场景图语言编码对象、属性和关系，SituationalLLM能够主动识别环境上下文中的差距，在用户交互过程中寻求澄清。这种行为来自于对情境意识数据库指令训练（SAD-Instruct）的训练，它将多样化的场景特定场景图与迭代式的对话式改进相结合。实验结果表明，SituationalLLM在任务特异性、可靠性和适应性方面优于通用LLM基线，为环境感知AI助理开辟了道路，能够在现实世界的约束条件下提供稳健、以用户为中心的指导。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在现实世界的物理环境中提供指导时存在局限性，无法识别用户物理上下文的理解限制。</li>
<li>SituationalLLM是一种将结构化场景信息集成到LLM中的新方法，以提供基于上下文的主动帮助。</li>
<li>SituationalLLM通过自定义场景图语言编码对象、属性和关系。</li>
<li>SituationalLLM能够主动识别环境上下文中的差距，并在用户交互中寻求澄清。</li>
<li>SituationalLLM的训练基于情境意识数据库指令训练（SAD-Instruct），结合了场景特定场景图和对话式改进。</li>
<li>实验结果表明，SituationalLLM在任务特异性、可靠性和适应性方面优于通用LLM基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.13302">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7e0c8b84439d1bc1251c8ad8ceacc0b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85ba8a692f7895dde90b88dd2b23c8b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23008b77b1d17ca780646c675c52d74e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b033f402171a2cb1beb87dd5d9fcaa72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5db7b3601505ba4c6c40257c299197cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e7f78f8f855b25f16ad3ecae1def4b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77daf73d945ec4bc7a150a07c07eb4a3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="How-Out-of-Distribution-Detection-Learning-Theory-Enhances-Transformer-Learnability-and-Reliability"><a href="#How-Out-of-Distribution-Detection-Learning-Theory-Enhances-Transformer-Learnability-and-Reliability" class="headerlink" title="How Out-of-Distribution Detection Learning Theory Enhances Transformer:   Learnability and Reliability"></a>How Out-of-Distribution Detection Learning Theory Enhances Transformer:   Learnability and Reliability</h2><p><strong>Authors:Yijin Zhou, Yutang Ge, Xiaowen Dong, Yuguang Wang</strong></p>
<p>Transformer networks excel in natural language processing and computer vision tasks. However, they still face challenges in generalizing to Out-of-Distribution (OOD) datasets, i.e. data whose distribution differs from that seen during training. The OOD detection aims to distinguish outliers while preserving in-distribution (ID) data performance. This paper introduces the OOD detection Probably Approximately Correct (PAC) Theory for transformers, which establishes the conditions for data distribution and model configurations for the learnability of transformers in terms of OOD detection. The theory demonstrates that outliers can be accurately represented and distinguished with sufficient data. The theoretical implications highlight the trade-off between theoretical principles and practical training paradigms. By examining this trade-off, we naturally derived the rationale for leveraging auxiliary outliers to enhance OOD detection. Our theory suggests that by penalizing the misclassification of outliers within the loss function and strategically generating soft synthetic outliers, one can robustly bolster the reliability of transformer networks. This approach yields a novel algorithm that ensures learnability and refines the decision boundaries between inliers and outliers. In practice, the algorithm consistently achieves state-of-the-art performance across various data formats. </p>
<blockquote>
<p>Transformer网络在自然语言处理和计算机视觉任务上表现出色。然而，它们在泛化到分布外（OOD）数据集时仍面临挑战，即数据分布与训练期间所见的数据分布不同。OOD检测旨在区分异常值，同时保持分布内（ID）数据性能。本文引入了针对变压器的OOD检测可能近似正确（PAC）理论，该理论建立了数据分布和模型配置的条件，以在OOD检测方面学习变压器的知识。该理论表明，使用足够的数据可以准确表示并区分异常值。理论上的含义突出了理论原则和实践训练范式之间的权衡。通过考察这种权衡，我们自然地得出了利用辅助异常值来提高OOD检测的合理性。我们的理论认为，通过惩罚损失函数中异常值的误分类，并战略性地生成柔软的合成异常值，可以稳健地提高变压器网络的可靠性。这种方法产生了一种新型算法，该算法确保可学习性并细化了内聚点和异常值之间的决策边界。在实践中，该算法在各种数据格式上始终实现最先进的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12915v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了针对Transformer网络在Out-of-Distribution（OOD）数据集上的挑战，提出了基于PAC理论的OOD检测方案。该方案通过建立数据分布和模型配置的条件，展示了Transformer在OOD检测方面的可学习性，并强调了利用辅助异常值提高OOD检测性能的重要性。通过惩罚损失函数中异常值的误分类并生成软合成异常值，能够增强Transformer网络的可靠性。该算法在实践中达到了各种数据格式的最新性能水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformer网络在自然语言处理和计算机视觉任务中表现出色，但在处理与训练数据分布不同的OOD数据集时仍面临挑战。</li>
<li>OOD检测旨在区分异常值同时保持ID数据性能。</li>
<li>本文提出了基于PAC理论的OOD检测方案，该方案为Transformer在OOD检测方面的可学习性建立了条件。</li>
<li>理论强调利用辅助异常值提高OOD检测性能的重要性。</li>
<li>通过惩罚损失函数中异常值的误分类并生成软合成异常值，可以增强Transformer网络的可靠性。</li>
<li>该方案在实践中实现了各种数据格式的最新性能水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.12915">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-769f1a7a077a51e8cf5aaa163a62208b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fce1dddae5ea10d3e15b321c6ab7367.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c830169efc108e27333aaf3dd204a73e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Unelicitable-Backdoors-in-Language-Models-via-Cryptographic-Transformer-Circuits"><a href="#Unelicitable-Backdoors-in-Language-Models-via-Cryptographic-Transformer-Circuits" class="headerlink" title="Unelicitable Backdoors in Language Models via Cryptographic Transformer   Circuits"></a>Unelicitable Backdoors in Language Models via Cryptographic Transformer   Circuits</h2><p><strong>Authors:Andis Draguns, Andrew Gritsevskiy, Sumeet Ramesh Motwani, Charlie Rogers-Smith, Jeffrey Ladish, Christian Schroeder de Witt</strong></p>
<p>The rapid proliferation of open-source language models significantly increases the risks of downstream backdoor attacks. These backdoors can introduce dangerous behaviours during model deployment and can evade detection by conventional cybersecurity monitoring systems. In this paper, we introduce a novel class of backdoors in transformer models, that, in contrast to prior art, are unelicitable in nature. Unelicitability prevents the defender from triggering the backdoor, making it impossible to properly evaluate ahead of deployment even if given full white-box access and using automated techniques, such as red-teaming or certain formal verification methods. We show that our novel construction is not only unelicitable thanks to using cryptographic techniques, but also has favourable robustness properties. We confirm these properties in empirical investigations, and provide evidence that our backdoors can withstand state-of-the-art mitigation strategies. Additionally, we expand on previous work by showing that our universal backdoors, while not completely undetectable in white-box settings, can be harder to detect than some existing designs. By demonstrating the feasibility of seamlessly integrating backdoors into transformer models, this paper fundamentally questions the efficacy of pre-deployment detection strategies. This offers new insights into the offence-defence balance in AI safety and security. </p>
<blockquote>
<p>开源语言模型的快速增殖显著增加了下游后门攻击的风险。这些后门可以在模型部署期间引入危险行为，并且可以逃避传统网络安全监控系统的检测。在本文中，我们介绍了变压器模型中一类新型后门，与现有技术相比，这些后门具有不可激发的性质。不可激发性可以防止防御者触发后门，即使在部署前给予完全的白盒访问并使用自动化技术（如红队或某些形式验证方法），也无法对其进行适当评估。我们表明，我们的新型构造不仅由于使用了加密技术而不可激发，而且还具有有利的稳健性属性。我们通过实证研究证实了这些属性，并提供证据表明我们的后门可以抵御最先进的缓解策略。此外，我们通过展示我们的通用后门虽然在白盒设置下并非完全不可检测，但比某些现有设计更难检测，从而扩展了以前的工作。本文通过展示将后门无缝集成到变压器模型中的可行性，从根本上质疑了预部署检测策略的有效性。这为人工智能安全和防御攻防平衡提供了新的见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02619v2">PDF</a> 19 pages, 7 figures</p>
<p><strong>Summary</strong><br>     开源语言模型的快速增殖增加了下游后门攻击的风险。本文介绍了一种新型的后门攻击方法，通过在转换器模型中植入后门来改变模型的行为。后门攻击具有隐蔽性，使得防御者在部署前无法触发后门，即使拥有完全的白盒访问权限并使用自动化技术也无法检测。这种后门攻击具有强大的稳健性，可以抵御最先进的缓解策略。本文展示了后门攻击的设计，对预部署检测策略的有效性提出了质疑，为人工智能安全和防御平衡提供了新的见解。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>开源语言模型的快速普及增加了下游后门攻击的风险。</li>
<li>介绍了一种新型的后门攻击方法，该方法是隐性的，可改变模型的行为。</li>
<li>后门攻击具有隐蔽性，使得防御者在部署前无法检测和触发后门。</li>
<li>后门攻击具有强大的稳健性，能够抵御先进的缓解策略。</li>
<li>该攻击方法的存在对预部署检测策略的有效性提出了质疑。</li>
<li>后门攻击设计展示了对转换器模型的完美集成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02619">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-457fef31ea4e46bf41179180091992d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75cc82f441da761e259993a097be5db3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c85f2f69019780b37c78eeeadd58ec23.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Quality-Assessment-for-AI-Generated-Images-with-Instruction-Tuning"><a href="#Quality-Assessment-for-AI-Generated-Images-with-Instruction-Tuning" class="headerlink" title="Quality Assessment for AI Generated Images with Instruction Tuning"></a>Quality Assessment for AI Generated Images with Instruction Tuning</h2><p><strong>Authors:Jiarui Wang, Huiyu Duan, Guangtao Zhai, Xiongkuo Min</strong></p>
<p>Artificial Intelligence Generated Content (AIGC) has grown rapidly in recent years, among which AI-based image generation has gained widespread attention due to its efficient and imaginative image creation ability. However, AI-generated Images (AIGIs) may not satisfy human preferences due to their unique distortions, which highlights the necessity to understand and evaluate human preferences for AIGIs. To this end, in this paper, we first establish a novel Image Quality Assessment (IQA) database for AIGIs, termed AIGCIQA2023+, which provides human visual preference scores and detailed preference explanations from three perspectives including quality, authenticity, and correspondence. Then, based on the constructed AIGCIQA2023+ database, this paper presents a MINT-IQA model to evaluate and explain human preferences for AIGIs from Multi-perspectives with INstruction Tuning. Specifically, the MINT-IQA model first learn and evaluate human preferences for AI-generated Images from multi-perspectives, then via the vision-language instruction tuning strategy, MINT-IQA attains powerful understanding and explanation ability for human visual preference on AIGIs, which can be used for feedback to further improve the assessment capabilities. Extensive experimental results demonstrate that the proposed MINT-IQA model achieves state-of-the-art performance in understanding and evaluating human visual preferences for AIGIs, and the proposed model also achieves competing results on traditional IQA tasks compared with state-of-the-art IQA models. The AIGCIQA2023+ database and MINT-IQA model are available at: <a target="_blank" rel="noopener" href="https://github.com/IntMeGroup/MINT-IQA">https://github.com/IntMeGroup/MINT-IQA</a>. </p>
<blockquote>
<p>人工智能生成内容（AIGC）近年来发展迅速，其中基于人工智能的图像生成因其高效和富有创造力的图像创作能力而受到广泛关注。然而，由于人工智能生成的图像（AIGI）具有独特的失真，可能无法满足人类的偏好，这强调了了解和评估人类对AIGI的偏好的必要性。为此，本文首先建立了一个新型的人工智能生成图像质量评估（IQA）数据库，名为AIGCIQA2023+，该数据库提供从质量、真实性和对应性三个方面的人类视觉偏好分数和详细的偏好解释。然后，基于构建的AIGCIQA2023+数据库，本文提出了一个MINT-IQA模型，该模型能够从多角度评估并解释人类对AIGI的偏好，并且能够通过指令微调策略增强理解和解释能力。具体来说，MINT-IQA模型首先学习并评估人类对AI生成的图像的多角度偏好，然后通过视觉语言指令微调策略，MINT-IQA获得了对AIGI的人类视觉偏好的强大理解和解释能力，可用于反馈以进一步提高评估能力。大量的实验结果证明，所提出的MINT-IQA模型在理解和评估人类对AIGI的视觉偏好方面达到了最先进的性能，并且在传统的IQA任务上也取得了与最先进的IQA模型相竞争的结果。AIGCIQA2023+数据库和MINT-IQA模型可在<a target="_blank" rel="noopener" href="https://github.com/IntMeGroup/MINT-IQA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/IntMeGroup/MINT-IQA获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.07346v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本文首先构建了名为AIGCIQA2023+的新型AI生成图像质量评估数据库，提供从质量、真实性和对应性三个方面的人类视觉偏好分数和详细偏好解释。然后，基于该数据库，提出了一个名为MINT-IQA的多角度评价模型，该模型通过指令微调策略，能够强大地理解和解释人类对AI生成图像的多角度视觉偏好。实验结果显示，MINT-IQA模型在理解和评估人类对AI生成图像视觉偏好方面取得了最佳性能，并在传统图像质量评估任务上取得了与最新模型相当的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>建立了名为AIGCIQA2023+的AI生成图像质量评估数据库，包含人类视觉偏好分数和详细偏好解释。</li>
<li>从质量、真实性和对应性三个方面评估AI生成图像。</li>
<li>提出了基于MINT-IQA的多角度评价模型，能够理解和解释人类对AI生成图像的多角度视觉偏好。</li>
<li>MINT-IQA模型通过指令微调策略，具有强大的理解和解释能力。</li>
<li>MINT-IQA模型在理解和评估人类对AI生成图像视觉偏好方面取得最佳性能。</li>
<li>MINT-IQA模型在传统图像质量评估任务上取得了与最新模型相当的结果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.07346">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3711bc0be5e89d6c9141b427a6bf1364.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-170523c3fe63deda8197f732a2658c5b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5eaf802786557e584f849b940bf51b86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8a368ea769c566905b1ebe899c34a4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd91aaf0f7f78314cb8e2932c606cc04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-883a65254b7b5230233c29ec65afaaa9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="HMT-Hierarchical-Memory-Transformer-for-Efficient-Long-Context-Language-Processing"><a href="#HMT-Hierarchical-Memory-Transformer-for-Efficient-Long-Context-Language-Processing" class="headerlink" title="HMT: Hierarchical Memory Transformer for Efficient Long Context Language   Processing"></a>HMT: Hierarchical Memory Transformer for Efficient Long Context Language   Processing</h2><p><strong>Authors:Zifan He, Yingqi Cao, Zongyue Qin, Neha Prakriya, Yizhou Sun, Jason Cong</strong></p>
<p>Transformer-based large language models (LLM) have been widely used in language processing applications. However, due to the memory constraints of the devices, most of them restrict the context window. Even though recurrent models in previous works can memorize past tokens to enable unlimited context and maintain effectiveness, they have &#96;&#96;flat’’ memory architectures. Such architectures have limitations in selecting and filtering information. Since humans are good at learning and self-adjustment, we believe that imitating brain memory hierarchy is beneficial for model memorization. Thus, we propose the Hierarchical Memory Transformer (HMT), a novel framework that facilitates a model’s long-context processing ability by imitating human memorization behavior. Leveraging memory-augmented segment-level recurrence, we organize the memory hierarchy by preserving tokens from early input segments, passing memory embeddings along the sequence, and recalling relevant information from history. Evaluating general language modeling, question-answering tasks, and the summarization task, we show that HMT consistently improves the long-context processing ability of existing models. Furthermore, HMT achieves a comparable or superior generation quality to long-context LLMs with $2 \sim 57\times$ fewer parameters and $2.5 \sim 116\times$ less inference memory, significantly outperforming previous memory-augmented models. Code on Github: <a target="_blank" rel="noopener" href="https://github.com/OswaldHe/HMT-pytorch">https://github.com/OswaldHe/HMT-pytorch</a>. </p>
<blockquote>
<p>基于Transformer的大型语言模型（LLM）已广泛应用于语言处理应用。然而，由于设备的内存限制，大多数模型都限制了上下文窗口。尽管之前的工作中的循环模型可以记住过去的标记来实现无限上下文并保持有效性，但它们具有“平面”内存架构。这种架构在选择和过滤信息方面存在局限性。由于人类擅长学习和自我调整，我们相信模仿大脑记忆层次结构对模型记忆是有益的。因此，我们提出了分层记忆转换器（HMT），这是一个新的框架，通过模仿人类的记忆行为，促进模型的长期上下文处理能力。通过利用增强内存的段级递归，我们保留早期输入段的标记来组织记忆层次结构，将记忆嵌入物沿着序列传递，并从历史中回忆相关信息。通过对通用语言建模、问答任务和摘要任务进行评估，我们证明了HMT在改进现有模型的长上下文处理能力方面的一致性。此外，HMT在长上下文LLM的生成质量方面达到了相当或更高的水平，使用了较少的参数（2至57倍）和较少的推理内存（2.5至116倍），显著优于之前的增强记忆模型。GitHub上的代码为：<a target="_blank" rel="noopener" href="https://github.com/OswaldHe/HMT-pytorch">https://github.com/OswaldHe/HMT-pytorch</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.06067v3">PDF</a> NAACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为Hierarchical Memory Transformer（HMT）的新型框架，该框架通过模仿人类记忆机制来提高模型的长期上下文处理能力。该框架通过分段级别的记忆增强复现机制构建记忆层次结构，保存早期输入片段的令牌，传递内存嵌入序列，并从历史中回忆相关信息。实验表明，HMT在通用语言建模、问答任务和摘要任务中能够一致提高现有模型的长期上下文处理能力，并且在较少的参数和较低推理内存的情况下达到或超过长上下文大型语言模型（LLM）的生成质量。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Hierarchical Memory Transformer（HMT）框架模仿人类记忆机制来提高模型的长期上下文处理能力。</li>
<li>HMT通过分段级别的记忆增强复现机制构建记忆层次结构。</li>
<li>HMT能够保存早期输入片段的令牌，传递内存嵌入序列，并回忆相关信息。</li>
<li>HMT在通用语言建模、问答任务和摘要任务中表现出提高现有模型的长期上下文处理能力的效果。</li>
<li>HMT在较少的参数和较低推理内存的情况下达到或超过长上下文大型语言模型（LLM）的生成质量。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.06067">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-1bbe297d4cf9d9f554fef0d656316382.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f7baf2f14f821d0f754b0c4bc0d1d10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a0b165651a466c7dec1786d0bd86c7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d04ec4cc8a424c07909c4809634b4b41.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CollagePrompt-A-Benchmark-for-Budget-Friendly-Visual-Recognition-with-GPT-4V"><a href="#CollagePrompt-A-Benchmark-for-Budget-Friendly-Visual-Recognition-with-GPT-4V" class="headerlink" title="CollagePrompt: A Benchmark for Budget-Friendly Visual Recognition with   GPT-4V"></a>CollagePrompt: A Benchmark for Budget-Friendly Visual Recognition with   GPT-4V</h2><p><strong>Authors:Siyu Xu, Yunke Wang, Daochang Liu, Bo Du, Chang Xu</strong></p>
<p>Recent advancements in generative AI have suggested that by taking visual prompts, GPT-4V can demonstrate significant proficiency in visual recognition tasks. Despite its impressive capabilities, the financial cost associated with GPT-4V’s inference presents a substantial barrier to its wide use. To address this challenge, we propose a budget-friendly collage prompting task that collages multiple images into a single visual prompt and makes GPT-4V perform visual recognition on several images simultaneously, thereby reducing the cost. We collect a dataset of various collage prompts to assess its performance in GPT-4V’s visual recognition. Our evaluations reveal several key findings: 1) Recognition accuracy varies with different positions in the collage. 2) Grouping images of the same category together leads to better visual recognition results. 3) Incorrect labels often come from adjacent images. These findings highlight the importance of image arrangement within collage prompt. To this end, we construct a benchmark called CollagePrompt, which offers a platform for designing collage prompt to achieve more cost-effective visual recognition with GPT-4V. A baseline method derived from genetic algorithms to optimize collage layouts is proposed and two metrics are introduced to measure the efficiency of the optimized collage prompt. Our benchmark enables researchers to better optimize collage prompts, thus making GPT-4V more cost-effective in visual recognition. The code and data are available at this project page <a target="_blank" rel="noopener" href="https://collageprompting.github.io/">https://collageprompting.github.io/</a>. </p>
<blockquote>
<p>最近生成式人工智能的进步表明，通过视觉提示，GPT-4V在视觉识别任务中表现出了显著的专业能力。尽管其能力令人印象深刻，但GPT-4V推理的财务成本却成为其广泛使用的巨大障碍。为了应对这一挑战，我们提出了一种经济实惠的拼贴提示任务，该任务将多张图像拼贴成单个视觉提示，使GPT-4V能够同时对多张图像进行视觉识别，从而降低推理成本。我们收集了各种拼贴提示数据集来评估GPT-4V的视觉识别性能。我们的评估揭示了几个关键发现：1）识别准确率因拼贴中的不同位置而异。2）将同一类别的图像分组在一起有助于获得更好的视觉识别结果。3）错误的标签往往来自相邻的图像。这些发现突显了拼贴提示中图像排列的重要性。为此，我们构建了一个名为CollagePrompt的基准测试平台，该平台为设计拼贴提示以实现更具成本效益的GPT-4V视觉识别提供了平台。提出了一种基于遗传算法优化拼贴布局的基础方法，并引入了两个指标来衡量优化拼贴提示的效率。我们的基准测试使研究人员能够更好地优化拼贴提示，从而使GPT-4V在视觉识别方面更具成本效益。代码和数据可在项目页面<a target="_blank" rel="noopener" href="https://collageprompting.github.io/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://collageprompting.github.io/上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11468v2">PDF</a> Accepted by NAACL2025 Findings</p>
<p><strong>Summary</strong></p>
<p>GPT-4V在视觉识别任务中展现出显著的能力，但其推理成本高昂。为降低成本，研究提出一种经济高效的拼贴提示任务，将多个图像拼贴成单一视觉提示，使GPT-4V能同时执行多个图像的视觉识别。研究评估发现拼贴位置、图像分组和相邻图像影响识别结果。为此构建了一个名为CollagePrompt的基准测试平台，提供设计拼贴提示以实现更具成本效益的GPT-4V视觉识别。提出一种基于遗传算法的基线方法来优化拼贴布局，并引入两个指标来衡量优化拼贴提示的效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4V具备强大的视觉识别能力。</li>
<li>GPT-4V推理成本高昂，成为其广泛应用的障碍。</li>
<li>提出拼贴提示任务，通过拼贴多个图像降低GPT-4V的视觉识别成本。</li>
<li>评估发现拼贴位置、图像分组和相邻图像对识别结果有影响。</li>
<li>构建CollagePrompt基准测试平台，为优化拼贴提示提供工具。</li>
<li>提出基于遗传算法的基线方法来优化拼贴布局。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.11468">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b43de732cceaf07f47a287125c4327d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd484734827281341b4199f66469989b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dce236898f0dd87d84953bb4e19ee9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a065ddd1bdfc44f47601b90b269819b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-523d1d698d6d446ee8b18ab277ca61ff.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Separate-Instructions-From-Data-And-What-Do-We-Even-Mean-By-That"><a href="#Can-LLMs-Separate-Instructions-From-Data-And-What-Do-We-Even-Mean-By-That" class="headerlink" title="Can LLMs Separate Instructions From Data? And What Do We Even Mean By   That?"></a>Can LLMs Separate Instructions From Data? And What Do We Even Mean By   That?</h2><p><strong>Authors:Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, Christoph H. Lampert</strong></p>
<p>Instruction-tuned Large Language Models (LLMs) show impressive results in numerous practical applications, but they lack essential safety features that are common in other areas of computer science, particularly an explicit separation of instructions and data. This makes them vulnerable to manipulations such as indirect prompt injections and generally unsuitable for safety-critical tasks. Surprisingly, there is currently no established definition or benchmark to quantify this phenomenon. In this work, we close this gap by introducing a formal measure for instruction-data separation and an empirical variant that is calculable from a model’s outputs. We also present a new dataset, SEP, that allows estimating the measure for real-world models. Our results on various LLMs show that the problem of instruction-data separation is real: all models fail to achieve high separation, and canonical mitigation techniques, such as prompt engineering and fine-tuning, either fail to substantially improve separation or reduce model utility. The source code and SEP dataset are openly accessible at <a target="_blank" rel="noopener" href="https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed">https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed</a>. </p>
<blockquote>
<p>指令优化的大型语言模型（LLM）在多个实际应用中表现出令人印象深刻的结果，但它们缺少计算机科学其他领域中常见的关键安全功能，特别是指令和数据的明确分离。这使得它们容易受到间接提示注入等操作的操纵，并且通常不适合执行安全关键任务。令人惊讶的是，目前尚没有既定的定义或基准来衡量这种现象。在这项工作中，我们通过引入指令与数据分离的正式度量标准以及可从模型输出中计算的经验变体来填补这一空白。我们还提供了一个新的数据集SEP，可以用来估计真实世界模型中的度量标准。我们在各种LLM上的结果表明，指令与数据分离的问题确实存在：所有模型的分离程度均不高，常见的缓解技术（如提示工程和微调）要么无法实质性提高分离程度，要么会降低模型的实用性。源代码和SEP数据集可在<a target="_blank" rel="noopener" href="https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.06833v3">PDF</a> Published as a conference paper at ICLR 2025, GitHub:   <a target="_blank" rel="noopener" href="https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed">https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed</a>. 10 pages main   text, 30 pages in total</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在诸多实际应用中表现出色，但缺乏计算机科学其他领域常见的安全特性，特别是缺少指令与数据的明确分离。这使其容易受到间接提示注入等操纵，并不适合安全关键任务。为解决此问题，本研究提出了指令与数据分离的正式衡量标准及其可计算的实证变体，并引入SEP数据集来估计真实模型的指标。结果表明所有模型都存在指令与数据分离问题，常规缓解技术如提示工程和微调要么无法实质改善分离，要么会降低模型效用。数据集源代码和SEP数据集已在GitHub上公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在多个实际应用中展现优异性能，但缺乏必要的安全特性。</li>
<li>LLM缺少指令与数据的明确分离，容易受到操纵，不适合安全关键任务。</li>
<li>研究提出了指令与数据分离的正式衡量标准和实证变体。</li>
<li>引入SEP数据集用于估计真实模型的指令与数据分离程度。</li>
<li>研究表明所有被测试的LLM都存在指令与数据分离问题。</li>
<li>常见缓解技术如提示工程和微调在改善指令与数据分离方面效果有限。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.06833">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-da6bc34fa03914663e8edabc7ff2fd24.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f9f9c8fac84c097262fbe8430ade31d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebab7749c9455438c76e128e6c9e09d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c82b81d4ec904a0268d7f4971e0bdcc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae59c5768f383534e869a5777fed039d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="What-Makes-for-Good-Visual-Instructions-Synthesizing-Complex-Visual-Reasoning-Instructions-for-Visual-Instruction-Tuning"><a href="#What-Makes-for-Good-Visual-Instructions-Synthesizing-Complex-Visual-Reasoning-Instructions-for-Visual-Instruction-Tuning" class="headerlink" title="What Makes for Good Visual Instructions? Synthesizing Complex Visual   Reasoning Instructions for Visual Instruction Tuning"></a>What Makes for Good Visual Instructions? Synthesizing Complex Visual   Reasoning Instructions for Visual Instruction Tuning</h2><p><strong>Authors:Yifan Du, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Jinpeng Wang, Chuyuan Wang, Mingchen Cai, Ruihua Song, Ji-Rong Wen</strong></p>
<p>Visual instruction tuning is crucial for enhancing the zero-shot generalization capability of Multi-modal Large Language Models (MLLMs). In this paper, we aim to investigate a fundamental question: ‘’what makes for good visual instructions’’. Through a comprehensive empirical study, we find that instructions focusing on complex visual reasoning tasks are particularly effective in improving the performance of MLLMs, with results correlating to instruction complexity. Based on this insight, we develop a systematic approach to automatically create high-quality complex visual reasoning instructions. Our approach employs a synthesize-complicate-reformulate paradigm, leveraging multiple stages to gradually increase the complexity of the instructions while guaranteeing quality. Based on this approach, we create the ComVint dataset with 32K examples, and fine-tune four MLLMs on it. Experimental results consistently demonstrate the enhanced performance of all compared MLLMs, such as a 27.86% and 27.60% improvement for LLaVA on MME-Perception and MME-Cognition, respectively. Our code and data are publicly available at the link: <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/ComVint">https://github.com/RUCAIBox/ComVint</a>. </p>
<blockquote>
<p>视觉指令调整对于提高多模态大型语言模型（MLLMs）的零样本泛化能力至关重要。本文旨在探究一个基本问题：“什么样的视觉指令是好的”。通过全面的实证研究，我们发现专注于复杂视觉推理任务的指令在提高MLLM性能方面特别有效，结果与指令复杂性相关。基于此见解，我们开发了一种系统的方法，自动创建高质量的复杂视觉推理指令。我们的方法采用合成-复杂化-重构的模式，利用多个阶段来逐步增加指令的复杂性，同时保证质量。基于此方法，我们创建了包含32K实例的ComVint数据集，并在其上微调了四个MLLM。实验结果一致表明所有对比MLLM的性能都有所提高，例如LLaVA在MME-Perception和MME-Cognition上的改进分别为27.86%和27.60%。我们的代码和数据在以下链接公开可用：<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/ComVint">https://github.com/RUCAIBox/ComVint</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01487v2">PDF</a> Accepted by COLING2025</p>
<p><strong>Summary</strong><br>视觉指令调优对于提升多模态大型语言模型的零样本泛化能力至关重要。本文旨在探究“什么是好的视觉指令”这一基本问题。通过全面的实证研究，我们发现关注复杂视觉推理任务的指令在提高MLLM性能方面特别有效，并且结果与指令的复杂性相关。基于此，我们开发了一种自动创建高质量复杂视觉推理指令的系统方法。使用合成-复杂化-改写的模式，通过多个阶段逐步增加指令的复杂性同时保证质量。基于该方法，我们创建了包含32K实例的ComVint数据集，并在其上微调了四个MLLM。实验结果一致表明所有对比的MLLM性能有所提升，如在MME-Perception和MME-Cognition上LLaVA分别提升了27.86%和27.60%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉指令调优对增强多模态大型语言模型的零样本泛化能力至关重要。</li>
<li>复杂视觉推理任务的指令对提高语言模型性能效果显著。</li>
<li>指令的复杂性对模型性能有直接影响。</li>
<li>提出了一种自动创建高质量复杂视觉推理指令的系统方法。</li>
<li>创建了包含32K实例的ComVint数据集用于模型训练。</li>
<li>使用ComVint数据集微调的MLLM性能有所提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.01487">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-09c09c48971fb47779f2e27794c440e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c30945e4fb5612a4981f435c6e919710.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-19db595d345a9b03702881619a2853af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dde0aad0d319d7ac7a70761f344dde35.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Chain-of-Factors-Paper-Reviewer-Matching"><a href="#Chain-of-Factors-Paper-Reviewer-Matching" class="headerlink" title="Chain-of-Factors Paper-Reviewer Matching"></a>Chain-of-Factors Paper-Reviewer Matching</h2><p><strong>Authors:Yu Zhang, Yanzhen Shen, SeongKu Kang, Xiusi Chen, Bowen Jin, Jiawei Han</strong></p>
<p>With the rapid increase in paper submissions to academic conferences, the need for automated and accurate paper-reviewer matching is more critical than ever. Previous efforts in this area have considered various factors to assess the relevance of a reviewer’s expertise to a paper, such as the semantic similarity, shared topics, and citation connections between the paper and the reviewer’s previous works. However, most of these studies focus on only one factor, resulting in an incomplete evaluation of the paper-reviewer relevance. To address this issue, we propose a unified model for paper-reviewer matching that jointly considers semantic, topic, and citation factors. To be specific, during training, we instruction-tune a contextualized language model shared across all factors to capture their commonalities and characteristics; during inference, we chain the three factors to enable step-by-step, coarse-to-fine search for qualified reviewers given a submission. Experiments on four datasets (one of which is newly contributed by us) spanning various fields such as machine learning, computer vision, information retrieval, and data mining consistently demonstrate the effectiveness of our proposed Chain-of-Factors model in comparison with state-of-the-art paper-reviewer matching methods and scientific pre-trained language models. </p>
<blockquote>
<p>随着提交到学术会议的论文数量迅速增加，对自动化和准确的论文审稿人匹配的需求比以往任何时候都更为关键。在此领域的先前研究已经考虑了各种因素来评估审稿人的专业知识与论文的相关性，例如语义相似性、共同的主题以及论文与审稿人之前作品之间的引用联系。然而，大多数这些研究只专注于一个因素，导致对论文与审稿人相关性的评估不完整。为了解决这一问题，我们提出了一种统一的论文审稿人匹配模型，该模型同时考虑语义、主题和引用因素。具体来说，在训练过程中，我们对所有因素共享一个上下文化的语言模型进行微调，以捕捉它们的共同点和特征；在推理过程中，我们将这三个因素串联起来，以在给定的提交中进行逐步的、从粗到细的搜索合格审稿人。在涵盖机器学习、计算机视觉、信息检索和数据挖掘等不同领域的四个数据集上进行实验，与我们提出的Chain-of-Factors模型相比，一贯证明了其在论文审稿人匹配方法与科学预训练语言模型中的有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14483v3">PDF</a> 10 pages; Accepted to WWW 2025 (Code:   <a target="_blank" rel="noopener" href="https://github.com/yuzhimanhua/CoF">https://github.com/yuzhimanhua/CoF</a>)</p>
<p><strong>Summary</strong><br>学术会议论文提交数量激增，对自动化精准匹配审稿人的需求愈发重要。先前研究主要聚焦于单一因素评估审稿人与论文相关性，如语义相似性、共同主题和引用联系等。本研究提出一种统一的论文与审稿人匹配模型，同时考虑语义、主题和引用因素。实验结果表明，Chain-of-Factors模型在多个数据集上的表现优于当前先进的论文审稿匹配方法和科学预训练语言模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>学术会议论文提交数量迅速增长，对自动化精准匹配审稿人的需求愈发迫切。</li>
<li>先前研究主要聚焦于单一因素评估审稿人与论文的相关性。</li>
<li>本研究提出一种统一的论文与审稿人匹配模型，整合语义、主题和引用因素。</li>
<li>模型采用分阶段粗到细的搜索策略寻找合格审稿人。</li>
<li>模型在多个涵盖不同领域的数据集上进行了实验验证。</li>
<li>实验结果表明，Chain-of-Factors模型在论文与审稿人匹配方面的表现优于当前先进的匹配方法和预训练语言模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.14483">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-5149a61eb0aac6d834d962d383067f39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07448aad390e9da5f51a4cbb32f013b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee814dfeed1f56a6bc9a3c367c9a30c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79428ec79aa7e1b4d5ed4808df144029.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Preserving-Knowledge-Invariance-Rethinking-Robustness-Evaluation-of-Open-Information-Extraction"><a href="#Preserving-Knowledge-Invariance-Rethinking-Robustness-Evaluation-of-Open-Information-Extraction" class="headerlink" title="Preserving Knowledge Invariance: Rethinking Robustness Evaluation of   Open Information Extraction"></a>Preserving Knowledge Invariance: Rethinking Robustness Evaluation of   Open Information Extraction</h2><p><strong>Authors:Ji Qi, Chuchun Zhang, Xiaozhi Wang, Kaisheng Zeng, Jifan Yu, Jinxin Liu, Jiuding Sun, Yuxiang Chen, Lei Hou, Juanzi Li, Bin Xu</strong></p>
<p>The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial measurement of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a popular large language model, the results show that the existing successful models exhibit a frustrating degradation, with a maximum drop of 23.43 F1 score. Our resources and code are available at <a target="_blank" rel="noopener" href="https://github.com/qijimrc/ROBUST">https://github.com/qijimrc/ROBUST</a>. </p>
<blockquote>
<p>模型的分布变化鲁棒性保证了自然语言处理模型能够在现实世界中成功应用，特别是在信息提取任务中。然而，大多数先前的评估基准测试主要集中在验证配对匹配的正确性，忽略了鲁棒性的关键度量。在本文中，我们提出了第一个模拟现实世界开放信息提取模型评估的基准测试，其中相同知识含义下的语法和表达分布可能会发生变化。我们设计并标注了一个大规模测试平台，其中的每个例子都是一个知识不变的小团体，由具有相同含义的结构化知识的句子组成，但具有不同的语法和表达形式。通过进一步阐述鲁棒性指标，如果一个模型在整个群体中的性能始终准确，则该模型被认为是稳健的。我们在过去十年中发布的典型模型以及流行的大型语言模型上进行了实验，结果表明，现有成功模型的性能令人沮丧地下降，最大F1分数下降了23.43分。我们的资源和代码可在<a target="_blank" rel="noopener" href="https://github.com/qijimrc/ROBUST%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/qijimrc/ROBUST找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.13981v3">PDF</a> Accepted by EMNLP 2023 Main Conference</p>
<p><strong>Summary</strong></p>
<p>本文提出了首个模拟现实世界开放信息提取模型评估的基准测试，重点关注模型在语法和表达分布变化下的稳健性。通过构建大规模测试平台，对模型在不同语法和表达形式但意义相同的知识集合上进行评估。实验结果显示，现有成功模型在稳健性方面存在显著下降，最大F1分数下降23.43。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本文强调了NLP模型在现实世界中应用时，对分布变化稳健性的重要性，特别是在信息提取任务中。</li>
<li>提出了首个模拟现实世界开放信息提取模型评估的基准测试，关注模型在知识意义相同但语法和表达分布变化下的性能。</li>
<li>构建了一个大规模测试平台，每个例子都是一个知识不变集合，包含结构化知识相同但不同语法和表达形式的句子。</li>
<li>定义了模型的稳健性评估标准，即模型在整体集合上的性能表现要稳定准确。</li>
<li>实验结果显示，现有成功模型在稳健性方面存在显著不足，最大F1分数下降23.43。</li>
<li>提供了研究资源和代码公开链接，便于其他研究者使用和进一步探索。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2305.13981">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f77d875d108e5dc6e0bd4d2b7f87cce0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97274f37882fce1f7611a0b97eb4c552.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd1d8c41d6c44b40a2526672d03979a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2280d0bf90421137c1f0394839c094d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-216a0e5d24fa3e61fb3594991e064b94.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-11/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-820e620c53c7c61f13abf341d6049f71.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-02-11  Long-VITA Scaling Large Multi-modal Models to 1 Million Tokens with   Leading Short-Context Accuray
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-08/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9083c84f5176d4d6b25603c42b11c618.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive 方向最新论文已更新，请持续关注 Update in 2025-02-08  ReactEmbed A Cross-Domain Framework for Protein-Molecule Representation   Learning via Biochemical Reaction Networks
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">16573k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
