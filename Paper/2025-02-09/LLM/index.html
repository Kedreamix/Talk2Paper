<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-09  OmniBal Towards Fast Instruct-tuning for Vision-Language Models via   Omniverse Computation Balance">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-5c7ad26b8fba8b43b39e61fddc5802f5.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-09
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    50 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-09-æ›´æ–°"><a href="#2025-02-09-æ›´æ–°" class="headerlink" title="2025-02-09 æ›´æ–°"></a>2025-02-09 æ›´æ–°</h1><h2 id="OmniBal-Towards-Fast-Instruct-tuning-for-Vision-Language-Models-via-Omniverse-Computation-Balance"><a href="#OmniBal-Towards-Fast-Instruct-tuning-for-Vision-Language-Models-via-Omniverse-Computation-Balance" class="headerlink" title="OmniBal: Towards Fast Instruct-tuning for Vision-Language Models via   Omniverse Computation Balance"></a>OmniBal: Towards Fast Instruct-tuning for Vision-Language Models via   Omniverse Computation Balance</h2><p><strong>Authors:Yongqiang Yao, Jingru Tan, Jiahao Hu, Feizhao Zhang, Xin Jin, Bo Li, Ruihao Gong, Pengfei Liu</strong></p>
<p>Recently, vision-language instruct-tuning models have made significant progress due to their more comprehensive understanding of the world. In this work, we discovered that large-scale 3D parallel training on those models leads to an imbalanced computation load across different devices. The vision and language parts are inherently heterogeneous: their data distribution and model architecture differ significantly, which affects distributed training efficiency. We rebalanced the computational loads from data, model, and memory perspectives to address this issue, achieving more balanced computation across devices. These three components are not independent but are closely connected, forming an omniverse balanced training framework. Specifically, for the data, we grouped instances into new balanced mini-batches within and across devices. For the model, we employed a search-based method to achieve a more balanced partitioning. For memory optimization, we adaptively adjusted the re-computation strategy for each partition to utilize the available memory fully. We conducted extensive experiments to validate the effectiveness of our method. Compared with the open-source training code of InternVL-Chat, we significantly reduced GPU days, achieving about 1.8x speed-up. Our methodâ€™s efficacy and generalizability were further demonstrated across various models and datasets. Codes will be released at <a target="_blank" rel="noopener" href="https://github.com/ModelTC/OmniBal">https://github.com/ModelTC/OmniBal</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç”±äºè§†è§‰è¯­è¨€æŒ‡ä»¤å¾®è°ƒæ¨¡å‹å¯¹ä¸–ç•Œçš„ç†è§£æ›´åŠ å…¨é¢ï¼Œå®ƒä»¬å–å¾—äº†é‡å¤§è¿›å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹çš„å¤§è§„æ¨¡3Då¹¶è¡Œè®­ç»ƒä¼šå¯¼è‡´ä¸åŒè®¾å¤‡ä¹‹é—´çš„è®¡ç®—è´Ÿè½½ä¸å¹³è¡¡ã€‚è§†è§‰å’Œè¯­è¨€éƒ¨åˆ†æœ¬è´¨ä¸Šæ˜¯å¼‚æ„çš„ï¼šå®ƒä»¬çš„æ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹æ¶æ„å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œè¿™å½±å“äº†åˆ†å¸ƒå¼è®­ç»ƒçš„æ•ˆç‡ã€‚æˆ‘ä»¬ä»æ•°æ®ã€æ¨¡å‹å’Œå†…å­˜ä¸‰ä¸ªæ–¹é¢é‡æ–°å¹³è¡¡è®¡ç®—è´Ÿè½½ï¼Œä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ç°è·¨è®¾å¤‡çš„æ›´å¹³è¡¡è®¡ç®—ã€‚è¿™ä¸‰ä¸ªç»„ä»¶ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œè€Œæ˜¯ç´§å¯†ç›¸è”çš„ï¼Œå½¢æˆäº†ä¸€ä¸ªå…¨æ–¹ä½å¹³è¡¡çš„è®­ç»ƒæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ•°æ®ï¼Œæˆ‘ä»¬å°†å®ä¾‹åˆ†ç»„ä¸ºè·¨è®¾å¤‡å†…å¤–çš„æ–°å¹³è¡¡å°æ‰¹é‡ã€‚å¯¹äºæ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºæœç´¢çš„æ–¹æ³•æ¥å®ç°æ›´å¹³è¡¡çš„åˆ’åˆ†ã€‚å¯¹äºå†…å­˜ä¼˜åŒ–ï¼Œæˆ‘ä»¬è‡ªé€‚åº”åœ°è°ƒæ•´æ¯ä¸ªåˆ†åŒºçš„é‡æ–°è®¡ç®—ç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨å¯ç”¨å†…å­˜ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒæ¥éªŒè¯æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä¸InternVL-Chatçš„å¼€æºè®­ç»ƒä»£ç ç›¸æ¯”ï¼Œæˆ‘ä»¬å¤§å¹…å‡å°‘äº†GPUå¤©æ•°ï¼Œå®ç°äº†çº¦1.8å€çš„é€Ÿåº¦æå‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§å¾—åˆ°äº†è¿›ä¸€æ­¥è¯æ˜ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ModelTC/OmniBal%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/ModelTC/OmniBalå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20761v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡ä¸‰ç»´å¹¶è¡Œè®­ç»ƒåœ¨è§†è§‰è¯­è¨€æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ä¸Šçš„ä¸å‡è¡¡è®¡ç®—è´Ÿè½½é—®é¢˜è¢«è§£å†³ã€‚é€šè¿‡ä»æ•°æ®ã€æ¨¡å‹å’Œå†…å­˜ä¸‰ä¸ªæ–¹é¢è¿›è¡Œå¹³è¡¡è°ƒæ•´ï¼Œå®ç°äº†æ›´å‡è¡¡çš„è®¡ç®—è´Ÿè½½åˆ†é…ã€‚è¯¥ç ”ç©¶æé«˜äº†åˆ†å¸ƒå¼è®­ç»ƒæ•ˆç‡ï¼Œå‡å°‘äº†GPUå¤©æ•°ï¼Œå¹¶åœ¨ä¸åŒçš„æ¨¡å‹å’Œæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚ç›¸å…³ä»£ç å°†åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æŒ‡ä»¤å¾®è°ƒæ¨¡å‹åœ¨å¤§è§„æ¨¡ä¸‰ç»´å¹¶è¡Œè®­ç»ƒä¸­å­˜åœ¨è®¡ç®—è´Ÿè½½ä¸å‡è¡¡çš„é—®é¢˜ã€‚</li>
<li>è§†è§‰å’Œè¯­è¨€éƒ¨åˆ†åœ¨æ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹æ¶æ„ä¸Šå­˜åœ¨å›ºæœ‰å·®å¼‚ï¼Œå½±å“åˆ†å¸ƒå¼è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>ä¸ºè§£å†³è®¡ç®—è´Ÿè½½ä¸å‡è¡¡é—®é¢˜ï¼Œä»æ•°æ®ã€æ¨¡å‹å’Œå†…å­˜ä¸‰ä¸ªæ–¹é¢è¿›è¡Œäº†å¹³è¡¡è°ƒæ•´ã€‚</li>
<li>é€šè¿‡å¹³è¡¡æ•°æ®çš„å®ä¾‹åˆ†ç»„å’Œæ¨¡å‹æœç´¢æ–¹æ³•å®ç°æ›´å‡è¡¡çš„è®¡ç®—è´Ÿè½½åˆ†é…ã€‚</li>
<li>ä¼˜åŒ–å†…å­˜ç®¡ç†ç­–ç•¥ä»¥å……åˆ†åˆ©ç”¨å¯ç”¨å†…å­˜ï¼Œé€‚åº”ä¸åŒçš„åˆ†åŒºéœ€æ±‚ã€‚</li>
<li>è¯¥æ–¹æ³•æ˜¾è‘—æé«˜äº†è®­ç»ƒæ•ˆç‡ï¼Œå‡å°‘äº†GPUå¤©æ•°ï¼Œå®ç°äº†çº¦1.8å€çš„é€Ÿåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.20761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cccb5394e5083bf508b55deae5029833.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36c292c3621e8dc0e7e891c637fb7b94.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e94b6e3107dfced375733b6222d8fca0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2445de10961c1b5aa637e4d36199e6d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e5090351b95da9e2f7ac0fab0532bb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-761c8541addedf7416be192890ca6629.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Advantage-Alignment-Algorithms"><a href="#Advantage-Alignment-Algorithms" class="headerlink" title="Advantage Alignment Algorithms"></a>Advantage Alignment Algorithms</h2><p><strong>Authors:Juan Agustin Duque, Milad Aghajohari, Tim Cooijmans, Razvan Ciuca, Tianyu Zhang, Gauthier Gidel, Aaron Courville</strong></p>
<p>Artificially intelligent agents are increasingly being integrated into human decision-making: from large language model (LLM) assistants to autonomous vehicles. These systems often optimize their individual objective, leading to conflicts, particularly in general-sum games where naive reinforcement learning agents empirically converge to Pareto-suboptimal Nash equilibria. To address this issue, opponent shaping has emerged as a paradigm for finding socially beneficial equilibria in general-sum games. In this work, we introduce Advantage Alignment, a family of algorithms derived from first principles that perform opponent shaping efficiently and intuitively. We achieve this by aligning the advantages of interacting agents, increasing the probability of mutually beneficial actions when their interaction has been positive. We prove that existing opponent shaping methods implicitly perform Advantage Alignment. Compared to these methods, Advantage Alignment simplifies the mathematical formulation of opponent shaping, reduces the computational burden and extends to continuous action domains. We demonstrate the effectiveness of our algorithms across a range of social dilemmas, achieving state-of-the-art cooperation and robustness against exploitation. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ä»£ç†æ­£è¶Šæ¥è¶Šå¤šåœ°èå…¥äººç±»çš„å†³ç­–è¿‡ç¨‹ï¼šä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŠ©ç†åˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€‚è¿™äº›ç³»ç»Ÿé€šå¸¸ä¼˜åŒ–å„è‡ªçš„ç›®æ ‡ï¼Œä»è€Œå¯¼è‡´å†²çªï¼Œç‰¹åˆ«æ˜¯åœ¨æ€»æ”¶ç›Šæ¸¸æˆä¸­çš„å†²çªå°¤ä¸ºæ˜æ˜¾ï¼Œå…¶ä¸­å¤©çœŸçš„å¼ºåŒ–å­¦ä¹ ä»£ç†é€šå¸¸ä¼šæ”¶æ•›åˆ°å¸•ç´¯æ‰˜æ¬¡ä¼˜çš„çº³ä»€å‡è¡¡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¯¹æ‰‹å¡‘å½¢å·²æˆä¸ºåœ¨æ€»æ”¶ç›Šæ¸¸æˆä¸­å¯»æ‰¾ç¤¾ä¼šåˆ©ç›Šå‡è¡¡çš„ä¸€ç§èŒƒå¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¼˜åŠ¿å¯¹é½ï¼ˆAdvantage Alignmentï¼‰ç®—æ³•å®¶æ—ï¼Œè¿™äº›ç®—æ³•åŸºäºåŸºæœ¬åŸç†ï¼Œèƒ½å¤Ÿé«˜æ•ˆç›´è§‚åœ°è¿›è¡Œå¯¹æ‰‹å¡‘å½¢ã€‚æˆ‘ä»¬é€šè¿‡è°ƒæ•´äº¤äº’ä»£ç†çš„ä¼˜åŠ¿æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œå½“å®ƒä»¬çš„äº¤äº’å‘ˆç°æ­£é¢æ—¶ï¼Œå¢åŠ äº†äº’æƒ è¡Œä¸ºçš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬è¯æ˜äº†ç°æœ‰çš„å¯¹æ‰‹å¡‘å½¢æ–¹æ³•éšå«åœ°æ‰§è¡Œä¼˜åŠ¿å¯¹é½ã€‚ä¸è¿™äº›æ–¹æ³•ç›¸æ¯”ï¼Œä¼˜åŠ¿å¯¹é½ç®€åŒ–äº†å¯¹æ‰‹å¡‘å½¢çš„æ•°å­¦å…¬å¼ï¼Œå‡è½»äº†è®¡ç®—è´Ÿæ‹…ï¼Œå¹¶æ‰©å±•åˆ°äº†è¿ç»­åŠ¨ä½œé¢†åŸŸã€‚æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—ç¤¾ä¼šå›°å¢ƒä¸­å±•ç¤ºäº†ç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„åˆä½œå’Œå¯¹æŠ—å‰¥å‰Šçš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14662v3">PDF</a> 25 Pages, 8 figures</p>
<p><strong>Summary</strong><br>ï¼šäººå·¥æ™ºèƒ½ä»£ç†æ­£è¶Šæ¥è¶Šå¤šåœ°èå…¥äººç±»å†³ç­–è¿‡ç¨‹ï¼Œä»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŠ©ç†åˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ã€‚è¿™äº›ç³»ç»Ÿé€šå¸¸ä¼˜åŒ–å„è‡ªçš„ç›®æ ‡ï¼Œå¯¼è‡´å†²çªï¼Œç‰¹åˆ«æ˜¯åœ¨æ€»å’Œæ¸¸æˆä¸­å°¤ä¸ºæ˜æ˜¾ï¼Œå…¶ä¸­ç®€å•çš„å¼ºåŒ–å­¦ä¹ ä»£ç†å¾€å¾€ä¼šæ”¶æ•›åˆ°å¸•ç´¯æ‰˜æ¬¡ä¼˜çš„çº³ä»€å‡è¡¡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œå¯¹æ‰‹å¡‘é€ å·²æˆä¸ºå¯»æ‰¾ä¸€èˆ¬æ€»å’Œæ¸¸æˆä¸­ç¤¾ä¼šåˆ©ç›Šå‡è¡¡çš„ä¸€ç§èŒƒå¼ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¼˜åŠ¿å¯¹é½ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä»ç¬¬ä¸€æ€§åŸåˆ™æ´¾ç”Ÿå‡ºæ¥çš„ç®—æ³•å®¶æ—ï¼Œèƒ½å¤Ÿé«˜æ•ˆç›´è§‚åœ°å®ç°å¯¹æ‰‹å¡‘é€ ã€‚æˆ‘ä»¬é€šè¿‡è°ƒæ•´äº¤äº’ä»£ç†çš„ä¼˜åŠ¿æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œå³åœ¨äº¤äº’ç§¯ææ—¶å¢åŠ äº’ç›Šè¡ŒåŠ¨çš„æ¦‚ç‡ã€‚æˆ‘ä»¬è¯æ˜äº†ç°æœ‰çš„å¯¹æ‰‹å¡‘é€ æ–¹æ³•éšå¼åœ°æ‰§è¡Œä¼˜åŠ¿å¯¹é½ã€‚ä¸è¿™äº›æ–¹æ³•ç›¸æ¯”ï¼Œä¼˜åŠ¿å¯¹é½ç®€åŒ–äº†å¯¹æ‰‹å¡‘é€ çš„æ•°å­¦å…¬å¼ï¼Œé™ä½äº†è®¡ç®—è´Ÿæ‹…ï¼Œå¹¶æ‰©å±•åˆ°äº†è¿ç»­åŠ¨ä½œé¢†åŸŸã€‚æˆ‘ä»¬åœ¨ä¸€ç³»åˆ—ç¤¾ä¼šå›°å¢ƒä¸­å±•ç¤ºäº†ç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„åˆä½œå’Œå¯¹åˆ©ç”¨çš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½ä»£ç†æ­£èå…¥äººç±»å†³ç­–ï¼Œå¼•å‘å†²çªé—®é¢˜ã€‚</li>
<li>å¯¹æ‰‹å¡‘é€ æ˜¯è§£å†³äººå·¥æ™ºèƒ½ä»£ç†å†²çªçš„ä¸€ç§èŒƒå¼ã€‚</li>
<li>å¼•å…¥ä¼˜åŠ¿å¯¹é½ç®—æ³•å®¶æ—ï¼Œèƒ½å¤Ÿé«˜æ•ˆç›´è§‚åœ°å®ç°å¯¹æ‰‹å¡‘é€ ã€‚</li>
<li>ä¼˜åŠ¿å¯¹é½é€šè¿‡è°ƒæ•´äº¤äº’ä»£ç†çš„ä¼˜åŠ¿æ¥å®ç°äº’ç›Šè¡ŒåŠ¨çš„æ¦‚ç‡å¢åŠ ã€‚</li>
<li>ç°æœ‰å¯¹æ‰‹å¡‘é€ æ–¹æ³•éšå¼æ‰§è¡Œä¼˜åŠ¿å¯¹é½ã€‚</li>
<li>ä¼˜åŠ¿å¯¹é½ç®€åŒ–äº†å¯¹æ‰‹å¡‘é€ çš„æ•°å­¦å…¬å¼ï¼Œé™ä½è®¡ç®—è´Ÿæ‹…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14662">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-65cb775cf4ebd8222a462464ec22721d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89e093ac0ec79b682c295ec784cb144e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Demystifying-Language-Model-Forgetting-with-Low-rank-Example-Associations"><a href="#Demystifying-Language-Model-Forgetting-with-Low-rank-Example-Associations" class="headerlink" title="Demystifying Language Model Forgetting with Low-rank Example   Associations"></a>Demystifying Language Model Forgetting with Low-rank Example   Associations</h2><p><strong>Authors:Xisen Jin, Xiang Ren</strong></p>
<p>Large Language models (LLMs) suffer from forgetting of upstream data when fine-tuned. Despite efforts on mitigating forgetting, few have investigated whether, and how forgotten upstream examples are dependent on newly learned tasks. Insights on such dependencies enable efficient and targeted mitigation of forgetting. In this paper, we empirically analyze forgetting that occurs in $N$ upstream examples of language modeling or instruction-tuning after fine-tuning LLMs on one of $M$ new tasks, visualized in $M\times N$ matrices. We show that the matrices are often well-approximated with low-rank matrices, indicating the dominance of simple associations between the learned tasks and forgotten upstream examples. Leveraging the analysis, we predict forgetting of upstream examples when fine-tuning on unseen tasks with matrix completion over the empirical associations. This enables fast identification of most forgotten examples without expensive inference on the entire upstream data. The approach, despite simplicity, outperforms prior approaches that learn semantic relationships of learned tasks and upstream examples with LMs for predicting forgetting. We demonstrate the practical utility of our analysis by showing statistically significantly reduced forgetting as we upweight predicted examples for replay at fine-tuning. Project page: <a target="_blank" rel="noopener" href="https://inklab.usc.edu/lm-forgetting-prediction/">https://inklab.usc.edu/lm-forgetting-prediction/</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¾®è°ƒæ—¶ä¼šé—å¿˜ä¸Šæ¸¸æ•°æ®ã€‚å°½ç®¡æœ‰äººåŠªåŠ›å‡è½»é—å¿˜ï¼Œä½†å¾ˆå°‘æœ‰äººç ”ç©¶é—å¿˜çš„ä¸Šæ¸¸ç¤ºä¾‹æ˜¯å¦ä»¥åŠå¦‚ä½•ä¾èµ–äºæ–°å­¦ä¹ çš„ä»»åŠ¡ã€‚å¯¹è¿™ç§ä¾èµ–æ€§çš„æ´å¯Ÿèƒ½å¤Ÿä½¿äººä»¬æœ‰æ•ˆåœ°æœ‰é’ˆå¯¹æ€§åœ°ç¼“è§£é—å¿˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹åœ¨ä¸€ä¸ªæ–°çš„Mä»»åŠ¡ä¸Šå¾®è°ƒLLMåå‘ç”Ÿçš„ä¸Šæ¸¸è¯­è¨€å»ºæ¨¡æˆ–æŒ‡ä»¤è°ƒæ•´ä¸­çš„Nä¸ªä¸Šæ¸¸ç¤ºä¾‹çš„é—å¿˜è¿›è¡Œäº†å®è¯åˆ†æï¼Œå¹¶ä»¥$M\times N$çŸ©é˜µçš„å½¢å¼è¿›è¡Œå¯è§†åŒ–ã€‚æˆ‘ä»¬å‘ç°è¿™äº›çŸ©é˜µé€šå¸¸å¯ä»¥ç”¨ä½é˜¶çŸ©é˜µå¾ˆå¥½åœ°é€¼è¿‘ï¼Œè¿™è¡¨æ˜å­¦åˆ°çš„ä»»åŠ¡å’Œé—å¿˜çš„ä¸Šæ¸¸ç¤ºä¾‹ä¹‹é—´å­˜åœ¨ç®€å•çš„å…³è”å æ®ä¸»å¯¼åœ°ä½ã€‚åˆ©ç”¨åˆ†æï¼Œæˆ‘ä»¬é€šè¿‡åœ¨ç»éªŒå…³è”ä¸Šåº”ç”¨çŸ©é˜µè¡¥å…¨æ¥é¢„æµ‹åœ¨æœªè§ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒæ—¶çš„ä¸Šæ¸¸ç¤ºä¾‹é—å¿˜ã€‚è¿™èƒ½å¤Ÿåœ¨ä¸è€—è´¹æ˜‚è´µçš„æ•´ä¸ªä¸Šæ¸¸æ•°æ®çš„æ¨ç†æƒ…å†µä¸‹å¿«é€Ÿè¯†åˆ«å‡ºæœ€å®¹æ˜“é—å¿˜çš„ä¾‹å­ã€‚å°½ç®¡æˆ‘ä»¬çš„æ–¹æ³•å¾ˆç®€å•ï¼Œä½†å®ƒè¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ï¼Œåè€…ä½¿ç”¨è¯­è¨€æ¨¡å‹æ¥å­¦ä¹ å­¦ä¹ åˆ°çš„ä»»åŠ¡å’Œä¸Šæ¸¸ç¤ºä¾‹ä¹‹é—´çš„è¯­ä¹‰å…³ç³»æ¥é¢„æµ‹é—å¿˜ã€‚æˆ‘ä»¬é€šè¿‡å±•ç¤ºåœ¨å¾®è°ƒæ—¶é€šè¿‡å›æ”¾é¢„æµ‹çš„ä¾‹å­æ¥æ˜¾è‘—å‡å°‘é—å¿˜ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„åˆ†æçš„å®é™…æ•ˆç”¨ã€‚é¡¹ç›®é¡µé¢ï¼š[<a target="_blank" rel="noopener" href="https://inklab.usc.edu/lm-forgetting-prediction/]">https://inklab.usc.edu/lm-forgetting-prediction/]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.14026v4">PDF</a> 8 pages; preprint</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¾®è°ƒæ—¶ä¼šé—å¿˜ä¸Šæ¸¸æ•°æ®ã€‚æœ¬æ–‡å®è¯åˆ†æäº†åœ¨å¾®è°ƒLLMåï¼Œå¯¹ä¸Šæ¸¸æ•°æ®ç¤ºä¾‹çš„é—å¿˜æƒ…å†µï¼Œå¹¶æ¢è®¨äº†è¿™ç§é—å¿˜ä¸æ–°å¢ä»»åŠ¡ä¹‹é—´çš„å…³ç³»ã€‚é€šè¿‡çŸ©é˜µå¯è§†åŒ–åˆ†æï¼Œå‘ç°é—å¿˜çŸ©é˜µé€šå¸¸å¯ä»¥ç”¨ä½é˜¶çŸ©é˜µè¿‘ä¼¼è¡¨ç¤ºï¼Œè¡¨æ˜æ–°ä»»åŠ¡ä¸é—å¿˜çš„ä¸Šæ¸¸æ•°æ®ç¤ºä¾‹ä¹‹é—´å­˜åœ¨ç®€å•çš„å…³è”ã€‚åˆ©ç”¨è¿™ç§åˆ†æï¼Œå¯ä»¥é€šè¿‡çŸ©é˜µè¡¥å…¨å¯¹ç»éªŒå…³è”è¿›è¡Œé¢„æµ‹ï¼Œä»è€Œå¿«é€Ÿè¯†åˆ«æœ€æ˜“é—å¿˜çš„ç¤ºä¾‹ï¼Œæ— éœ€åœ¨æ•´ä¸ªä¸Šæ¸¸æ•°æ®é›†ä¸Šè¿›è¡Œæ˜‚è´µçš„æ¨ç†æ“ä½œã€‚é€šè¿‡åŠ æƒé¢„æµ‹ç¤ºä¾‹è¿›è¡Œå›æ”¾çš„æ–¹æ³•æ˜¾ç¤ºï¼Œåœ¨å‡å°‘é—å¿˜æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„å®é™…æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¾®è°ƒæ—¶ä¼šé—å¿˜ä¸Šæ¸¸æ•°æ®ã€‚</li>
<li>é—å¿˜ä¸æ–°å¢ä»»åŠ¡ä¹‹é—´å­˜åœ¨å…³è”ã€‚</li>
<li>é€šè¿‡çŸ©é˜µå¯è§†åŒ–åˆ†æé—å¿˜æƒ…å†µã€‚</li>
<li>é—å¿˜çŸ©é˜µå¯ä»¥ç”¨ä½é˜¶çŸ©é˜µè¿‘ä¼¼è¡¨ç¤ºã€‚</li>
<li>åˆ©ç”¨çŸ©é˜µè¡¥å…¨é¢„æµ‹é—å¿˜ã€‚</li>
<li>å¯å¿«é€Ÿè¯†åˆ«æœ€æ˜“é—å¿˜çš„ç¤ºä¾‹ï¼Œæ— éœ€å…¨æ•°æ®é›†æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.14026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ccffad4aa26b9de7b7461b289e5cd768.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd249bb0d6b5b3e5ed6f9d9bcfb16fce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b36019530b6e976f7e21e03b8718db74.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d0c7fe028f5de826dc20f779e16df0e5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5c7ad26b8fba8b43b39e61fddc5802f5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-440967d431f24aa5a4324cfc1d684f85.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-076de3fa9b49e4d1259254ef4466c1ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a8714858485c9fc5f2b25cf0b58b94b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SituationalLLM-Proactive-language-models-with-scene-awareness-for-dynamic-contextual-task-guidance"><a href="#SituationalLLM-Proactive-language-models-with-scene-awareness-for-dynamic-contextual-task-guidance" class="headerlink" title="SituationalLLM: Proactive language models with scene awareness for   dynamic, contextual task guidance"></a>SituationalLLM: Proactive language models with scene awareness for   dynamic, contextual task guidance</h2><p><strong>Authors:Muhammad Saif Ullah Khan, Muhammad Zeshan Afzal, Didier Stricker</strong></p>
<p>Large language models (LLMs) have achieved remarkable success in text-based tasks but often struggle to provide actionable guidance in real-world physical environments. This is because of their inability to recognize their limited understanding of the userâ€™s physical context. We present SituationalLLM, a novel approach that integrates structured scene information into an LLM to deliver proactive, context-aware assistance. By encoding objects, attributes, and relationships in a custom Scene Graph Language, SituationalLLM actively identifies gaps in environmental context and seeks clarifications during user interactions. This behavior emerges from training on the Situational Awareness Database for Instruct-Tuning (SAD-Instruct), which combines diverse, scenario-specific scene graphs with iterative, dialogue-based refinements. Experimental results indicate that SituationalLLM outperforms generic LLM baselines in task specificity, reliability, and adaptability, paving the way for environment-aware AI assistants capable of delivering robust, user-centric guidance under real-world constraints. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„ç‰©ç†ç¯å¢ƒä¸­å¾€å¾€éš¾ä»¥æä¾›å¯æ“ä½œçš„æŒ‡å¯¼ã€‚è¿™æ˜¯å› ä¸ºå®ƒä»¬æ— æ³•è¯†åˆ«è‡ªå·±å¯¹ç”¨æˆ·ç‰©ç†ç¯å¢ƒçš„æœ‰é™ç†è§£ã€‚æˆ‘ä»¬æå‡ºäº†SituationalLLMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œé€šè¿‡å°†ç»“æ„åŒ–åœºæ™¯ä¿¡æ¯é›†æˆåˆ°LLMä¸­ï¼Œä»¥æä¾›ä¸»åŠ¨ã€åŸºäºä¸Šä¸‹æ–‡çš„ä»»åŠ¡è¾…åŠ©ã€‚é€šè¿‡è‡ªå®šä¹‰åœºæ™¯å›¾è¯­è¨€å¯¹ç‰©ä½“ã€å±æ€§å’Œå…³ç³»è¿›è¡Œç¼–ç ï¼ŒSituationalLLMèƒ½å¤Ÿä¸»åŠ¨è¯†åˆ«ç¯å¢ƒä¸Šä¸‹æ–‡ä¸­çš„ç©ºç™½ï¼Œå¹¶åœ¨ç”¨æˆ·äº¤äº’è¿‡ç¨‹ä¸­å¯»æ±‚æ¾„æ¸…ã€‚è¿™ç§è¡Œä¸ºæºäºå¯¹æƒ…å¢ƒæ„è¯†æ•°æ®åº“æŒ‡ä»¤å¾®è°ƒï¼ˆSAD-Instructï¼‰çš„è®­ç»ƒï¼Œå®ƒå°†å¤šæ ·åŒ–çš„åœºæ™¯ç‰¹å®šåœºæ™¯å›¾ä¸è¿­ä»£å¼çš„å¯¹è¯å¼å¾®è°ƒç›¸ç»“åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä»»åŠ¡ç‰¹å¼‚æ€§ã€å¯é æ€§å’Œé€‚åº”æ€§æ–¹é¢ï¼ŒSituationalLLMä¼˜äºé€šç”¨LLMåŸºçº¿ï¼Œä¸ºç¯å¢ƒæ„ŸçŸ¥AIåŠ©ç†é“ºå¹³äº†é“è·¯ï¼Œèƒ½å¤Ÿåœ¨ç°å®ä¸–ç•Œçš„çº¦æŸä¸‹æä¾›ç¨³å¥çš„ã€ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.13302v3">PDF</a> Revised Submission to Open Research Europe</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨ç°å®ä¸–ç•Œçš„ç‰©ç†ç¯å¢ƒä¸­å¾€å¾€éš¾ä»¥æä¾›å¯æ“ä½œçš„æŒ‡å¯¼ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSituationalLLMçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†ç»“æ„åŒ–åœºæ™¯ä¿¡æ¯é›†æˆåˆ°LLMä¸­ï¼Œä»¥æä¾›ä¸»åŠ¨ã€åŸºäºä¸Šä¸‹æ–‡çš„å¸®åŠ©ã€‚é€šè¿‡è‡ªå®šä¹‰åœºæ™¯å›¾è¯­è¨€ç¼–ç å¯¹è±¡ã€å±æ€§å’Œå…³ç³»ï¼ŒSituationalLLMèƒ½å¤Ÿä¸»åŠ¨è¯†åˆ«ç¯å¢ƒä¸Šä¸‹æ–‡ä¸­çš„å·®è·ï¼Œåœ¨ç”¨æˆ·äº¤äº’è¿‡ç¨‹ä¸­å¯»æ±‚æ¾„æ¸…ã€‚è¿™ç§è¡Œä¸ºæ¥è‡ªäºå¯¹æƒ…å¢ƒæ„è¯†æ•°æ®åº“æŒ‡ä»¤è®­ç»ƒï¼ˆSAD-Instructï¼‰çš„è®­ç»ƒï¼Œå®ƒå°†å¤šæ ·åŒ–çš„åœºæ™¯ç‰¹å®šåœºæ™¯å›¾ä¸è¿­ä»£å¼çš„å¯¹è¯å¼æ”¹è¿›ç›¸ç»“åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSituationalLLMåœ¨ä»»åŠ¡ç‰¹å¼‚æ€§ã€å¯é æ€§å’Œé€‚åº”æ€§æ–¹é¢ä¼˜äºé€šç”¨LLMåŸºçº¿ï¼Œä¸ºç¯å¢ƒæ„ŸçŸ¥AIåŠ©ç†å¼€è¾Ÿäº†é“è·¯ï¼Œèƒ½å¤Ÿåœ¨ç°å®ä¸–ç•Œçš„çº¦æŸæ¡ä»¶ä¸‹æä¾›ç¨³å¥ã€ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨ç°å®ä¸–ç•Œçš„ç‰©ç†ç¯å¢ƒä¸­æä¾›æŒ‡å¯¼æ—¶å­˜åœ¨å±€é™æ€§ï¼Œæ— æ³•è¯†åˆ«ç”¨æˆ·ç‰©ç†ä¸Šä¸‹æ–‡çš„ç†è§£é™åˆ¶ã€‚</li>
<li>SituationalLLMæ˜¯ä¸€ç§å°†ç»“æ„åŒ–åœºæ™¯ä¿¡æ¯é›†æˆåˆ°LLMä¸­çš„æ–°æ–¹æ³•ï¼Œä»¥æä¾›åŸºäºä¸Šä¸‹æ–‡çš„ä¸»åŠ¨å¸®åŠ©ã€‚</li>
<li>SituationalLLMé€šè¿‡è‡ªå®šä¹‰åœºæ™¯å›¾è¯­è¨€ç¼–ç å¯¹è±¡ã€å±æ€§å’Œå…³ç³»ã€‚</li>
<li>SituationalLLMèƒ½å¤Ÿä¸»åŠ¨è¯†åˆ«ç¯å¢ƒä¸Šä¸‹æ–‡ä¸­çš„å·®è·ï¼Œå¹¶åœ¨ç”¨æˆ·äº¤äº’ä¸­å¯»æ±‚æ¾„æ¸…ã€‚</li>
<li>SituationalLLMçš„è®­ç»ƒåŸºäºæƒ…å¢ƒæ„è¯†æ•°æ®åº“æŒ‡ä»¤è®­ç»ƒï¼ˆSAD-Instructï¼‰ï¼Œç»“åˆäº†åœºæ™¯ç‰¹å®šåœºæ™¯å›¾å’Œå¯¹è¯å¼æ”¹è¿›ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒSituationalLLMåœ¨ä»»åŠ¡ç‰¹å¼‚æ€§ã€å¯é æ€§å’Œé€‚åº”æ€§æ–¹é¢ä¼˜äºé€šç”¨LLMåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.13302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e0c8b84439d1bc1251c8ad8ceacc0b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-85ba8a692f7895dde90b88dd2b23c8b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23008b77b1d17ca780646c675c52d74e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b033f402171a2cb1beb87dd5d9fcaa72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5db7b3601505ba4c6c40257c299197cc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0e7f78f8f855b25f16ad3ecae1def4b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77daf73d945ec4bc7a150a07c07eb4a3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="How-Out-of-Distribution-Detection-Learning-Theory-Enhances-Transformer-Learnability-and-Reliability"><a href="#How-Out-of-Distribution-Detection-Learning-Theory-Enhances-Transformer-Learnability-and-Reliability" class="headerlink" title="How Out-of-Distribution Detection Learning Theory Enhances Transformer:   Learnability and Reliability"></a>How Out-of-Distribution Detection Learning Theory Enhances Transformer:   Learnability and Reliability</h2><p><strong>Authors:Yijin Zhou, Yutang Ge, Xiaowen Dong, Yuguang Wang</strong></p>
<p>Transformer networks excel in natural language processing and computer vision tasks. However, they still face challenges in generalizing to Out-of-Distribution (OOD) datasets, i.e. data whose distribution differs from that seen during training. The OOD detection aims to distinguish outliers while preserving in-distribution (ID) data performance. This paper introduces the OOD detection Probably Approximately Correct (PAC) Theory for transformers, which establishes the conditions for data distribution and model configurations for the learnability of transformers in terms of OOD detection. The theory demonstrates that outliers can be accurately represented and distinguished with sufficient data. The theoretical implications highlight the trade-off between theoretical principles and practical training paradigms. By examining this trade-off, we naturally derived the rationale for leveraging auxiliary outliers to enhance OOD detection. Our theory suggests that by penalizing the misclassification of outliers within the loss function and strategically generating soft synthetic outliers, one can robustly bolster the reliability of transformer networks. This approach yields a novel algorithm that ensures learnability and refines the decision boundaries between inliers and outliers. In practice, the algorithm consistently achieves state-of-the-art performance across various data formats. </p>
<blockquote>
<p>Transformerç½‘ç»œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ³›åŒ–åˆ°åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®é›†æ—¶ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå³æ•°æ®åˆ†å¸ƒä¸è®­ç»ƒæœŸé—´æ‰€è§çš„æ•°æ®åˆ†å¸ƒä¸åŒã€‚OODæ£€æµ‹æ—¨åœ¨åŒºåˆ†å¼‚å¸¸å€¼ï¼ŒåŒæ—¶ä¿æŒåˆ†å¸ƒå†…ï¼ˆIDï¼‰æ•°æ®æ€§èƒ½ã€‚æœ¬æ–‡å¼•å…¥äº†é’ˆå¯¹å˜å‹å™¨çš„OODæ£€æµ‹å¯èƒ½è¿‘ä¼¼æ­£ç¡®ï¼ˆPACï¼‰ç†è®ºï¼Œè¯¥ç†è®ºå»ºç«‹äº†æ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹é…ç½®çš„æ¡ä»¶ï¼Œä»¥åœ¨OODæ£€æµ‹æ–¹é¢å­¦ä¹ å˜å‹å™¨çš„çŸ¥è¯†ã€‚è¯¥ç†è®ºè¡¨æ˜ï¼Œä½¿ç”¨è¶³å¤Ÿçš„æ•°æ®å¯ä»¥å‡†ç¡®è¡¨ç¤ºå¹¶åŒºåˆ†å¼‚å¸¸å€¼ã€‚ç†è®ºä¸Šçš„å«ä¹‰çªå‡ºäº†ç†è®ºåŸåˆ™å’Œå®è·µè®­ç»ƒèŒƒå¼ä¹‹é—´çš„æƒè¡¡ã€‚é€šè¿‡è€ƒå¯Ÿè¿™ç§æƒè¡¡ï¼Œæˆ‘ä»¬è‡ªç„¶åœ°å¾—å‡ºäº†åˆ©ç”¨è¾…åŠ©å¼‚å¸¸å€¼æ¥æé«˜OODæ£€æµ‹çš„åˆç†æ€§ã€‚æˆ‘ä»¬çš„ç†è®ºè®¤ä¸ºï¼Œé€šè¿‡æƒ©ç½šæŸå¤±å‡½æ•°ä¸­å¼‚å¸¸å€¼çš„è¯¯åˆ†ç±»ï¼Œå¹¶æˆ˜ç•¥æ€§åœ°ç”ŸæˆæŸ”è½¯çš„åˆæˆå¼‚å¸¸å€¼ï¼Œå¯ä»¥ç¨³å¥åœ°æé«˜å˜å‹å™¨ç½‘ç»œçš„å¯é æ€§ã€‚è¿™ç§æ–¹æ³•äº§ç”Ÿäº†ä¸€ç§æ–°å‹ç®—æ³•ï¼Œè¯¥ç®—æ³•ç¡®ä¿å¯å­¦ä¹ æ€§å¹¶ç»†åŒ–äº†å†…èšç‚¹å’Œå¼‚å¸¸å€¼ä¹‹é—´çš„å†³ç­–è¾¹ç•Œã€‚åœ¨å®è·µä¸­ï¼Œè¯¥ç®—æ³•åœ¨å„ç§æ•°æ®æ ¼å¼ä¸Šå§‹ç»ˆå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12915v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹Transformerç½‘ç»œåœ¨Out-of-Distributionï¼ˆOODï¼‰æ•°æ®é›†ä¸Šçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†åŸºäºPACç†è®ºçš„OODæ£€æµ‹æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé€šè¿‡å»ºç«‹æ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹é…ç½®çš„æ¡ä»¶ï¼Œå±•ç¤ºäº†Transformeråœ¨OODæ£€æµ‹æ–¹é¢çš„å¯å­¦ä¹ æ€§ï¼Œå¹¶å¼ºè°ƒäº†åˆ©ç”¨è¾…åŠ©å¼‚å¸¸å€¼æé«˜OODæ£€æµ‹æ€§èƒ½çš„é‡è¦æ€§ã€‚é€šè¿‡æƒ©ç½šæŸå¤±å‡½æ•°ä¸­å¼‚å¸¸å€¼çš„è¯¯åˆ†ç±»å¹¶ç”Ÿæˆè½¯åˆæˆå¼‚å¸¸å€¼ï¼Œèƒ½å¤Ÿå¢å¼ºTransformerç½‘ç»œçš„å¯é æ€§ã€‚è¯¥ç®—æ³•åœ¨å®è·µä¸­è¾¾åˆ°äº†å„ç§æ•°æ®æ ¼å¼çš„æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Transformerç½‘ç»œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†ä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸åŒçš„OODæ•°æ®é›†æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>OODæ£€æµ‹æ—¨åœ¨åŒºåˆ†å¼‚å¸¸å€¼åŒæ—¶ä¿æŒIDæ•°æ®æ€§èƒ½ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºPACç†è®ºçš„OODæ£€æµ‹æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆä¸ºTransformeråœ¨OODæ£€æµ‹æ–¹é¢çš„å¯å­¦ä¹ æ€§å»ºç«‹äº†æ¡ä»¶ã€‚</li>
<li>ç†è®ºå¼ºè°ƒåˆ©ç”¨è¾…åŠ©å¼‚å¸¸å€¼æé«˜OODæ£€æµ‹æ€§èƒ½çš„é‡è¦æ€§ã€‚</li>
<li>é€šè¿‡æƒ©ç½šæŸå¤±å‡½æ•°ä¸­å¼‚å¸¸å€¼çš„è¯¯åˆ†ç±»å¹¶ç”Ÿæˆè½¯åˆæˆå¼‚å¸¸å€¼ï¼Œå¯ä»¥å¢å¼ºTransformerç½‘ç»œçš„å¯é æ€§ã€‚</li>
<li>è¯¥æ–¹æ¡ˆåœ¨å®è·µä¸­å®ç°äº†å„ç§æ•°æ®æ ¼å¼çš„æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.12915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-769f1a7a077a51e8cf5aaa163a62208b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7fce1dddae5ea10d3e15b321c6ab7367.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c830169efc108e27333aaf3dd204a73e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Unelicitable-Backdoors-in-Language-Models-via-Cryptographic-Transformer-Circuits"><a href="#Unelicitable-Backdoors-in-Language-Models-via-Cryptographic-Transformer-Circuits" class="headerlink" title="Unelicitable Backdoors in Language Models via Cryptographic Transformer   Circuits"></a>Unelicitable Backdoors in Language Models via Cryptographic Transformer   Circuits</h2><p><strong>Authors:Andis Draguns, Andrew Gritsevskiy, Sumeet Ramesh Motwani, Charlie Rogers-Smith, Jeffrey Ladish, Christian Schroeder de Witt</strong></p>
<p>The rapid proliferation of open-source language models significantly increases the risks of downstream backdoor attacks. These backdoors can introduce dangerous behaviours during model deployment and can evade detection by conventional cybersecurity monitoring systems. In this paper, we introduce a novel class of backdoors in transformer models, that, in contrast to prior art, are unelicitable in nature. Unelicitability prevents the defender from triggering the backdoor, making it impossible to properly evaluate ahead of deployment even if given full white-box access and using automated techniques, such as red-teaming or certain formal verification methods. We show that our novel construction is not only unelicitable thanks to using cryptographic techniques, but also has favourable robustness properties. We confirm these properties in empirical investigations, and provide evidence that our backdoors can withstand state-of-the-art mitigation strategies. Additionally, we expand on previous work by showing that our universal backdoors, while not completely undetectable in white-box settings, can be harder to detect than some existing designs. By demonstrating the feasibility of seamlessly integrating backdoors into transformer models, this paper fundamentally questions the efficacy of pre-deployment detection strategies. This offers new insights into the offence-defence balance in AI safety and security. </p>
<blockquote>
<p>å¼€æºè¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå¢æ®–æ˜¾è‘—å¢åŠ äº†ä¸‹æ¸¸åé—¨æ”»å‡»çš„é£é™©ã€‚è¿™äº›åé—¨å¯ä»¥åœ¨æ¨¡å‹éƒ¨ç½²æœŸé—´å¼•å…¥å±é™©è¡Œä¸ºï¼Œå¹¶ä¸”å¯ä»¥é€ƒé¿ä¼ ç»Ÿç½‘ç»œå®‰å…¨ç›‘æ§ç³»ç»Ÿçš„æ£€æµ‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å˜å‹å™¨æ¨¡å‹ä¸­ä¸€ç±»æ–°å‹åé—¨ï¼Œä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¿™äº›åé—¨å…·æœ‰ä¸å¯æ¿€å‘çš„æ€§è´¨ã€‚ä¸å¯æ¿€å‘æ€§å¯ä»¥é˜²æ­¢é˜²å¾¡è€…è§¦å‘åé—¨ï¼Œå³ä½¿åœ¨éƒ¨ç½²å‰ç»™äºˆå®Œå…¨çš„ç™½ç›’è®¿é—®å¹¶ä½¿ç”¨è‡ªåŠ¨åŒ–æŠ€æœ¯ï¼ˆå¦‚çº¢é˜Ÿæˆ–æŸäº›å½¢å¼éªŒè¯æ–¹æ³•ï¼‰ï¼Œä¹Ÿæ— æ³•å¯¹å…¶è¿›è¡Œé€‚å½“è¯„ä¼°ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–°å‹æ„é€ ä¸ä»…ç”±äºä½¿ç”¨äº†åŠ å¯†æŠ€æœ¯è€Œä¸å¯æ¿€å‘ï¼Œè€Œä¸”è¿˜å…·æœ‰æœ‰åˆ©çš„ç¨³å¥æ€§å±æ€§ã€‚æˆ‘ä»¬é€šè¿‡å®è¯ç ”ç©¶è¯å®äº†è¿™äº›å±æ€§ï¼Œå¹¶æä¾›è¯æ®è¡¨æ˜æˆ‘ä»¬çš„åé—¨å¯ä»¥æŠµå¾¡æœ€å…ˆè¿›çš„ç¼“è§£ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡å±•ç¤ºæˆ‘ä»¬çš„é€šç”¨åé—¨è™½ç„¶åœ¨ç™½ç›’è®¾ç½®ä¸‹å¹¶éå®Œå…¨ä¸å¯æ£€æµ‹ï¼Œä½†æ¯”æŸäº›ç°æœ‰è®¾è®¡æ›´éš¾æ£€æµ‹ï¼Œä»è€Œæ‰©å±•äº†ä»¥å‰çš„å·¥ä½œã€‚æœ¬æ–‡é€šè¿‡å±•ç¤ºå°†åé—¨æ— ç¼é›†æˆåˆ°å˜å‹å™¨æ¨¡å‹ä¸­çš„å¯è¡Œæ€§ï¼Œä»æ ¹æœ¬ä¸Šè´¨ç–‘äº†é¢„éƒ¨ç½²æ£€æµ‹ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚è¿™ä¸ºäººå·¥æ™ºèƒ½å®‰å…¨å’Œé˜²å¾¡æ”»é˜²å¹³è¡¡æä¾›äº†æ–°çš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.02619v2">PDF</a> 19 pages, 7 figures</p>
<p><strong>Summary</strong><br>     å¼€æºè¯­è¨€æ¨¡å‹çš„å¿«é€Ÿå¢æ®–å¢åŠ äº†ä¸‹æ¸¸åé—¨æ”»å‡»çš„é£é™©ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åé—¨æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡åœ¨è½¬æ¢å™¨æ¨¡å‹ä¸­æ¤å…¥åé—¨æ¥æ”¹å˜æ¨¡å‹çš„è¡Œä¸ºã€‚åé—¨æ”»å‡»å…·æœ‰éšè”½æ€§ï¼Œä½¿å¾—é˜²å¾¡è€…åœ¨éƒ¨ç½²å‰æ— æ³•è§¦å‘åé—¨ï¼Œå³ä½¿æ‹¥æœ‰å®Œå…¨çš„ç™½ç›’è®¿é—®æƒé™å¹¶ä½¿ç”¨è‡ªåŠ¨åŒ–æŠ€æœ¯ä¹Ÿæ— æ³•æ£€æµ‹ã€‚è¿™ç§åé—¨æ”»å‡»å…·æœ‰å¼ºå¤§çš„ç¨³å¥æ€§ï¼Œå¯ä»¥æŠµå¾¡æœ€å…ˆè¿›çš„ç¼“è§£ç­–ç•¥ã€‚æœ¬æ–‡å±•ç¤ºäº†åé—¨æ”»å‡»çš„è®¾è®¡ï¼Œå¯¹é¢„éƒ¨ç½²æ£€æµ‹ç­–ç•¥çš„æœ‰æ•ˆæ€§æå‡ºäº†è´¨ç–‘ï¼Œä¸ºäººå·¥æ™ºèƒ½å®‰å…¨å’Œé˜²å¾¡å¹³è¡¡æä¾›äº†æ–°çš„è§è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼€æºè¯­è¨€æ¨¡å‹çš„å¿«é€Ÿæ™®åŠå¢åŠ äº†ä¸‹æ¸¸åé—¨æ”»å‡»çš„é£é™©ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åé—¨æ”»å‡»æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¯éšæ€§çš„ï¼Œå¯æ”¹å˜æ¨¡å‹çš„è¡Œä¸ºã€‚</li>
<li>åé—¨æ”»å‡»å…·æœ‰éšè”½æ€§ï¼Œä½¿å¾—é˜²å¾¡è€…åœ¨éƒ¨ç½²å‰æ— æ³•æ£€æµ‹å’Œè§¦å‘åé—¨ã€‚</li>
<li>åé—¨æ”»å‡»å…·æœ‰å¼ºå¤§çš„ç¨³å¥æ€§ï¼Œèƒ½å¤ŸæŠµå¾¡å…ˆè¿›çš„ç¼“è§£ç­–ç•¥ã€‚</li>
<li>è¯¥æ”»å‡»æ–¹æ³•çš„å­˜åœ¨å¯¹é¢„éƒ¨ç½²æ£€æµ‹ç­–ç•¥çš„æœ‰æ•ˆæ€§æå‡ºäº†è´¨ç–‘ã€‚</li>
<li>åé—¨æ”»å‡»è®¾è®¡å±•ç¤ºäº†å¯¹è½¬æ¢å™¨æ¨¡å‹çš„å®Œç¾é›†æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.02619">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-457fef31ea4e46bf41179180091992d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-75cc82f441da761e259993a097be5db3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c85f2f69019780b37c78eeeadd58ec23.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Quality-Assessment-for-AI-Generated-Images-with-Instruction-Tuning"><a href="#Quality-Assessment-for-AI-Generated-Images-with-Instruction-Tuning" class="headerlink" title="Quality Assessment for AI Generated Images with Instruction Tuning"></a>Quality Assessment for AI Generated Images with Instruction Tuning</h2><p><strong>Authors:Jiarui Wang, Huiyu Duan, Guangtao Zhai, Xiongkuo Min</strong></p>
<p>Artificial Intelligence Generated Content (AIGC) has grown rapidly in recent years, among which AI-based image generation has gained widespread attention due to its efficient and imaginative image creation ability. However, AI-generated Images (AIGIs) may not satisfy human preferences due to their unique distortions, which highlights the necessity to understand and evaluate human preferences for AIGIs. To this end, in this paper, we first establish a novel Image Quality Assessment (IQA) database for AIGIs, termed AIGCIQA2023+, which provides human visual preference scores and detailed preference explanations from three perspectives including quality, authenticity, and correspondence. Then, based on the constructed AIGCIQA2023+ database, this paper presents a MINT-IQA model to evaluate and explain human preferences for AIGIs from Multi-perspectives with INstruction Tuning. Specifically, the MINT-IQA model first learn and evaluate human preferences for AI-generated Images from multi-perspectives, then via the vision-language instruction tuning strategy, MINT-IQA attains powerful understanding and explanation ability for human visual preference on AIGIs, which can be used for feedback to further improve the assessment capabilities. Extensive experimental results demonstrate that the proposed MINT-IQA model achieves state-of-the-art performance in understanding and evaluating human visual preferences for AIGIs, and the proposed model also achieves competing results on traditional IQA tasks compared with state-of-the-art IQA models. The AIGCIQA2023+ database and MINT-IQA model are available at: <a target="_blank" rel="noopener" href="https://github.com/IntMeGroup/MINT-IQA">https://github.com/IntMeGroup/MINT-IQA</a>. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹ï¼ˆAIGCï¼‰è¿‘å¹´æ¥å‘å±•è¿…é€Ÿï¼Œå…¶ä¸­åŸºäºäººå·¥æ™ºèƒ½çš„å›¾åƒç”Ÿæˆå› å…¶é«˜æ•ˆå’Œå¯Œæœ‰åˆ›é€ åŠ›çš„å›¾åƒåˆ›ä½œèƒ½åŠ›è€Œå—åˆ°å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äºäººå·¥æ™ºèƒ½ç”Ÿæˆçš„å›¾åƒï¼ˆAIGIï¼‰å…·æœ‰ç‹¬ç‰¹çš„å¤±çœŸï¼Œå¯èƒ½æ— æ³•æ»¡è¶³äººç±»çš„åå¥½ï¼Œè¿™å¼ºè°ƒäº†äº†è§£å’Œè¯„ä¼°äººç±»å¯¹AIGIçš„åå¥½çš„å¿…è¦æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªæ–°å‹çš„äººå·¥æ™ºèƒ½ç”Ÿæˆå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰æ•°æ®åº“ï¼Œåä¸ºAIGCIQA2023+ï¼Œè¯¥æ•°æ®åº“æä¾›ä»è´¨é‡ã€çœŸå®æ€§å’Œå¯¹åº”æ€§ä¸‰ä¸ªæ–¹é¢çš„äººç±»è§†è§‰åå¥½åˆ†æ•°å’Œè¯¦ç»†çš„åå¥½è§£é‡Šã€‚ç„¶åï¼ŒåŸºäºæ„å»ºçš„AIGCIQA2023+æ•°æ®åº“ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªMINT-IQAæ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»å¤šè§’åº¦è¯„ä¼°å¹¶è§£é‡Šäººç±»å¯¹AIGIçš„åå¥½ï¼Œå¹¶ä¸”èƒ½å¤Ÿé€šè¿‡æŒ‡ä»¤å¾®è°ƒç­–ç•¥å¢å¼ºç†è§£å’Œè§£é‡Šèƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼ŒMINT-IQAæ¨¡å‹é¦–å…ˆå­¦ä¹ å¹¶è¯„ä¼°äººç±»å¯¹AIç”Ÿæˆçš„å›¾åƒçš„å¤šè§’åº¦åå¥½ï¼Œç„¶åé€šè¿‡è§†è§‰è¯­è¨€æŒ‡ä»¤å¾®è°ƒç­–ç•¥ï¼ŒMINT-IQAè·å¾—äº†å¯¹AIGIçš„äººç±»è§†è§‰åå¥½çš„å¼ºå¤§ç†è§£å’Œè§£é‡Šèƒ½åŠ›ï¼Œå¯ç”¨äºåé¦ˆä»¥è¿›ä¸€æ­¥æé«˜è¯„ä¼°èƒ½åŠ›ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼Œæ‰€æå‡ºçš„MINT-IQAæ¨¡å‹åœ¨ç†è§£å’Œè¯„ä¼°äººç±»å¯¹AIGIçš„è§†è§‰åå¥½æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä¼ ç»Ÿçš„IQAä»»åŠ¡ä¸Šä¹Ÿå–å¾—äº†ä¸æœ€å…ˆè¿›çš„IQAæ¨¡å‹ç›¸ç«äº‰çš„ç»“æœã€‚AIGCIQA2023+æ•°æ®åº“å’ŒMINT-IQAæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/IntMeGroup/MINT-IQA%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/IntMeGroup/MINT-IQAè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.07346v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡é¦–å…ˆæ„å»ºäº†åä¸ºAIGCIQA2023+çš„æ–°å‹AIç”Ÿæˆå›¾åƒè´¨é‡è¯„ä¼°æ•°æ®åº“ï¼Œæä¾›ä»è´¨é‡ã€çœŸå®æ€§å’Œå¯¹åº”æ€§ä¸‰ä¸ªæ–¹é¢çš„äººç±»è§†è§‰åå¥½åˆ†æ•°å’Œè¯¦ç»†åå¥½è§£é‡Šã€‚ç„¶åï¼ŒåŸºäºè¯¥æ•°æ®åº“ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºMINT-IQAçš„å¤šè§’åº¦è¯„ä»·æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡æŒ‡ä»¤å¾®è°ƒç­–ç•¥ï¼Œèƒ½å¤Ÿå¼ºå¤§åœ°ç†è§£å’Œè§£é‡Šäººç±»å¯¹AIç”Ÿæˆå›¾åƒçš„å¤šè§’åº¦è§†è§‰åå¥½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMINT-IQAæ¨¡å‹åœ¨ç†è§£å’Œè¯„ä¼°äººç±»å¯¹AIç”Ÿæˆå›¾åƒè§†è§‰åå¥½æ–¹é¢å–å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œå¹¶åœ¨ä¼ ç»Ÿå›¾åƒè´¨é‡è¯„ä¼°ä»»åŠ¡ä¸Šå–å¾—äº†ä¸æœ€æ–°æ¨¡å‹ç›¸å½“çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å»ºç«‹äº†åä¸ºAIGCIQA2023+çš„AIç”Ÿæˆå›¾åƒè´¨é‡è¯„ä¼°æ•°æ®åº“ï¼ŒåŒ…å«äººç±»è§†è§‰åå¥½åˆ†æ•°å’Œè¯¦ç»†åå¥½è§£é‡Šã€‚</li>
<li>ä»è´¨é‡ã€çœŸå®æ€§å’Œå¯¹åº”æ€§ä¸‰ä¸ªæ–¹é¢è¯„ä¼°AIç”Ÿæˆå›¾åƒã€‚</li>
<li>æå‡ºäº†åŸºäºMINT-IQAçš„å¤šè§’åº¦è¯„ä»·æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£å’Œè§£é‡Šäººç±»å¯¹AIç”Ÿæˆå›¾åƒçš„å¤šè§’åº¦è§†è§‰åå¥½ã€‚</li>
<li>MINT-IQAæ¨¡å‹é€šè¿‡æŒ‡ä»¤å¾®è°ƒç­–ç•¥ï¼Œå…·æœ‰å¼ºå¤§çš„ç†è§£å’Œè§£é‡Šèƒ½åŠ›ã€‚</li>
<li>MINT-IQAæ¨¡å‹åœ¨ç†è§£å’Œè¯„ä¼°äººç±»å¯¹AIç”Ÿæˆå›¾åƒè§†è§‰åå¥½æ–¹é¢å–å¾—æœ€ä½³æ€§èƒ½ã€‚</li>
<li>MINT-IQAæ¨¡å‹åœ¨ä¼ ç»Ÿå›¾åƒè´¨é‡è¯„ä¼°ä»»åŠ¡ä¸Šå–å¾—äº†ä¸æœ€æ–°æ¨¡å‹ç›¸å½“çš„ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.07346">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3711bc0be5e89d6c9141b427a6bf1364.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-170523c3fe63deda8197f732a2658c5b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5eaf802786557e584f849b940bf51b86.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8a368ea769c566905b1ebe899c34a4b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd91aaf0f7f78314cb8e2932c606cc04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-883a65254b7b5230233c29ec65afaaa9.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="HMT-Hierarchical-Memory-Transformer-for-Efficient-Long-Context-Language-Processing"><a href="#HMT-Hierarchical-Memory-Transformer-for-Efficient-Long-Context-Language-Processing" class="headerlink" title="HMT: Hierarchical Memory Transformer for Efficient Long Context Language   Processing"></a>HMT: Hierarchical Memory Transformer for Efficient Long Context Language   Processing</h2><p><strong>Authors:Zifan He, Yingqi Cao, Zongyue Qin, Neha Prakriya, Yizhou Sun, Jason Cong</strong></p>
<p>Transformer-based large language models (LLM) have been widely used in language processing applications. However, due to the memory constraints of the devices, most of them restrict the context window. Even though recurrent models in previous works can memorize past tokens to enable unlimited context and maintain effectiveness, they have &#96;&#96;flatâ€™â€™ memory architectures. Such architectures have limitations in selecting and filtering information. Since humans are good at learning and self-adjustment, we believe that imitating brain memory hierarchy is beneficial for model memorization. Thus, we propose the Hierarchical Memory Transformer (HMT), a novel framework that facilitates a modelâ€™s long-context processing ability by imitating human memorization behavior. Leveraging memory-augmented segment-level recurrence, we organize the memory hierarchy by preserving tokens from early input segments, passing memory embeddings along the sequence, and recalling relevant information from history. Evaluating general language modeling, question-answering tasks, and the summarization task, we show that HMT consistently improves the long-context processing ability of existing models. Furthermore, HMT achieves a comparable or superior generation quality to long-context LLMs with $2 \sim 57\times$ fewer parameters and $2.5 \sim 116\times$ less inference memory, significantly outperforming previous memory-augmented models. Code on Github: <a target="_blank" rel="noopener" href="https://github.com/OswaldHe/HMT-pytorch">https://github.com/OswaldHe/HMT-pytorch</a>. </p>
<blockquote>
<p>åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²å¹¿æ³›åº”ç”¨äºè¯­è¨€å¤„ç†åº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºè®¾å¤‡çš„å†…å­˜é™åˆ¶ï¼Œå¤§å¤šæ•°æ¨¡å‹éƒ½é™åˆ¶äº†ä¸Šä¸‹æ–‡çª—å£ã€‚å°½ç®¡ä¹‹å‰çš„å·¥ä½œä¸­çš„å¾ªç¯æ¨¡å‹å¯ä»¥è®°ä½è¿‡å»çš„æ ‡è®°æ¥å®ç°æ— é™ä¸Šä¸‹æ–‡å¹¶ä¿æŒæœ‰æ•ˆæ€§ï¼Œä½†å®ƒä»¬å…·æœ‰â€œå¹³é¢â€å†…å­˜æ¶æ„ã€‚è¿™ç§æ¶æ„åœ¨é€‰æ‹©å’Œè¿‡æ»¤ä¿¡æ¯æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ç”±äºäººç±»æ“…é•¿å­¦ä¹ å’Œè‡ªæˆ‘è°ƒæ•´ï¼Œæˆ‘ä»¬ç›¸ä¿¡æ¨¡ä»¿å¤§è„‘è®°å¿†å±‚æ¬¡ç»“æ„å¯¹æ¨¡å‹è®°å¿†æ˜¯æœ‰ç›Šçš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†å±‚è®°å¿†è½¬æ¢å™¨ï¼ˆHMTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œé€šè¿‡æ¨¡ä»¿äººç±»çš„è®°å¿†è¡Œä¸ºï¼Œä¿ƒè¿›æ¨¡å‹çš„é•¿æœŸä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ã€‚é€šè¿‡åˆ©ç”¨å¢å¼ºå†…å­˜çš„æ®µçº§é€’å½’ï¼Œæˆ‘ä»¬ä¿ç•™æ—©æœŸè¾“å…¥æ®µçš„æ ‡è®°æ¥ç»„ç»‡è®°å¿†å±‚æ¬¡ç»“æ„ï¼Œå°†è®°å¿†åµŒå…¥ç‰©æ²¿ç€åºåˆ—ä¼ é€’ï¼Œå¹¶ä»å†å²ä¸­å›å¿†ç›¸å…³ä¿¡æ¯ã€‚é€šè¿‡å¯¹é€šç”¨è¯­è¨€å»ºæ¨¡ã€é—®ç­”ä»»åŠ¡å’Œæ‘˜è¦ä»»åŠ¡è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†HMTåœ¨æ”¹è¿›ç°æœ‰æ¨¡å‹çš„é•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›æ–¹é¢çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼ŒHMTåœ¨é•¿ä¸Šä¸‹æ–‡LLMçš„ç”Ÿæˆè´¨é‡æ–¹é¢è¾¾åˆ°äº†ç›¸å½“æˆ–æ›´é«˜çš„æ°´å¹³ï¼Œä½¿ç”¨äº†è¾ƒå°‘çš„å‚æ•°ï¼ˆ2è‡³57å€ï¼‰å’Œè¾ƒå°‘çš„æ¨ç†å†…å­˜ï¼ˆ2.5è‡³116å€ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºä¹‹å‰çš„å¢å¼ºè®°å¿†æ¨¡å‹ã€‚GitHubä¸Šçš„ä»£ç ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/OswaldHe/HMT-pytorch">https://github.com/OswaldHe/HMT-pytorch</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.06067v3">PDF</a> NAACL 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºHierarchical Memory Transformerï¼ˆHMTï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ¨¡ä»¿äººç±»è®°å¿†æœºåˆ¶æ¥æé«˜æ¨¡å‹çš„é•¿æœŸä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†æ®µçº§åˆ«çš„è®°å¿†å¢å¼ºå¤ç°æœºåˆ¶æ„å»ºè®°å¿†å±‚æ¬¡ç»“æ„ï¼Œä¿å­˜æ—©æœŸè¾“å…¥ç‰‡æ®µçš„ä»¤ç‰Œï¼Œä¼ é€’å†…å­˜åµŒå…¥åºåˆ—ï¼Œå¹¶ä»å†å²ä¸­å›å¿†ç›¸å…³ä¿¡æ¯ã€‚å®éªŒè¡¨æ˜ï¼ŒHMTåœ¨é€šç”¨è¯­è¨€å»ºæ¨¡ã€é—®ç­”ä»»åŠ¡å’Œæ‘˜è¦ä»»åŠ¡ä¸­èƒ½å¤Ÿä¸€è‡´æé«˜ç°æœ‰æ¨¡å‹çš„é•¿æœŸä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨è¾ƒå°‘çš„å‚æ•°å’Œè¾ƒä½æ¨ç†å†…å­˜çš„æƒ…å†µä¸‹è¾¾åˆ°æˆ–è¶…è¿‡é•¿ä¸Šä¸‹æ–‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Hierarchical Memory Transformerï¼ˆHMTï¼‰æ¡†æ¶æ¨¡ä»¿äººç±»è®°å¿†æœºåˆ¶æ¥æé«˜æ¨¡å‹çš„é•¿æœŸä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ã€‚</li>
<li>HMTé€šè¿‡åˆ†æ®µçº§åˆ«çš„è®°å¿†å¢å¼ºå¤ç°æœºåˆ¶æ„å»ºè®°å¿†å±‚æ¬¡ç»“æ„ã€‚</li>
<li>HMTèƒ½å¤Ÿä¿å­˜æ—©æœŸè¾“å…¥ç‰‡æ®µçš„ä»¤ç‰Œï¼Œä¼ é€’å†…å­˜åµŒå…¥åºåˆ—ï¼Œå¹¶å›å¿†ç›¸å…³ä¿¡æ¯ã€‚</li>
<li>HMTåœ¨é€šç”¨è¯­è¨€å»ºæ¨¡ã€é—®ç­”ä»»åŠ¡å’Œæ‘˜è¦ä»»åŠ¡ä¸­è¡¨ç°å‡ºæé«˜ç°æœ‰æ¨¡å‹çš„é•¿æœŸä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›çš„æ•ˆæœã€‚</li>
<li>HMTåœ¨è¾ƒå°‘çš„å‚æ•°å’Œè¾ƒä½æ¨ç†å†…å­˜çš„æƒ…å†µä¸‹è¾¾åˆ°æˆ–è¶…è¿‡é•¿ä¸Šä¸‹æ–‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆè´¨é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.06067">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1bbe297d4cf9d9f554fef0d656316382.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1f7baf2f14f821d0f754b0c4bc0d1d10.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1a0b165651a466c7dec1786d0bd86c7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d04ec4cc8a424c07909c4809634b4b41.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CollagePrompt-A-Benchmark-for-Budget-Friendly-Visual-Recognition-with-GPT-4V"><a href="#CollagePrompt-A-Benchmark-for-Budget-Friendly-Visual-Recognition-with-GPT-4V" class="headerlink" title="CollagePrompt: A Benchmark for Budget-Friendly Visual Recognition with   GPT-4V"></a>CollagePrompt: A Benchmark for Budget-Friendly Visual Recognition with   GPT-4V</h2><p><strong>Authors:Siyu Xu, Yunke Wang, Daochang Liu, Bo Du, Chang Xu</strong></p>
<p>Recent advancements in generative AI have suggested that by taking visual prompts, GPT-4V can demonstrate significant proficiency in visual recognition tasks. Despite its impressive capabilities, the financial cost associated with GPT-4Vâ€™s inference presents a substantial barrier to its wide use. To address this challenge, we propose a budget-friendly collage prompting task that collages multiple images into a single visual prompt and makes GPT-4V perform visual recognition on several images simultaneously, thereby reducing the cost. We collect a dataset of various collage prompts to assess its performance in GPT-4Vâ€™s visual recognition. Our evaluations reveal several key findings: 1) Recognition accuracy varies with different positions in the collage. 2) Grouping images of the same category together leads to better visual recognition results. 3) Incorrect labels often come from adjacent images. These findings highlight the importance of image arrangement within collage prompt. To this end, we construct a benchmark called CollagePrompt, which offers a platform for designing collage prompt to achieve more cost-effective visual recognition with GPT-4V. A baseline method derived from genetic algorithms to optimize collage layouts is proposed and two metrics are introduced to measure the efficiency of the optimized collage prompt. Our benchmark enables researchers to better optimize collage prompts, thus making GPT-4V more cost-effective in visual recognition. The code and data are available at this project page <a target="_blank" rel="noopener" href="https://collageprompting.github.io/">https://collageprompting.github.io/</a>. </p>
<blockquote>
<p>æœ€è¿‘ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„è¿›æ­¥è¡¨æ˜ï¼Œé€šè¿‡è§†è§‰æç¤ºï¼ŒGPT-4Våœ¨è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„ä¸“ä¸šèƒ½åŠ›ã€‚å°½ç®¡å…¶èƒ½åŠ›ä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†GPT-4Væ¨ç†çš„è´¢åŠ¡æˆæœ¬å´æˆä¸ºå…¶å¹¿æ³›ä½¿ç”¨çš„å·¨å¤§éšœç¢ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»æµå®æƒ çš„æ‹¼è´´æç¤ºä»»åŠ¡ï¼Œè¯¥ä»»åŠ¡å°†å¤šå¼ å›¾åƒæ‹¼è´´æˆå•ä¸ªè§†è§‰æç¤ºï¼Œä½¿GPT-4Vèƒ½å¤ŸåŒæ—¶å¯¹å¤šå¼ å›¾åƒè¿›è¡Œè§†è§‰è¯†åˆ«ï¼Œä»è€Œé™ä½æ¨ç†æˆæœ¬ã€‚æˆ‘ä»¬æ”¶é›†äº†å„ç§æ‹¼è´´æç¤ºæ•°æ®é›†æ¥è¯„ä¼°GPT-4Vçš„è§†è§‰è¯†åˆ«æ€§èƒ½ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ­ç¤ºäº†å‡ ä¸ªå…³é”®å‘ç°ï¼š1ï¼‰è¯†åˆ«å‡†ç¡®ç‡å› æ‹¼è´´ä¸­çš„ä¸åŒä½ç½®è€Œå¼‚ã€‚2ï¼‰å°†åŒä¸€ç±»åˆ«çš„å›¾åƒåˆ†ç»„åœ¨ä¸€èµ·æœ‰åŠ©äºè·å¾—æ›´å¥½çš„è§†è§‰è¯†åˆ«ç»“æœã€‚3ï¼‰é”™è¯¯çš„æ ‡ç­¾å¾€å¾€æ¥è‡ªç›¸é‚»çš„å›¾åƒã€‚è¿™äº›å‘ç°çªæ˜¾äº†æ‹¼è´´æç¤ºä¸­å›¾åƒæ’åˆ—çš„é‡è¦æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåä¸ºCollagePromptçš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œè¯¥å¹³å°ä¸ºè®¾è®¡æ‹¼è´´æç¤ºä»¥å®ç°æ›´å…·æˆæœ¬æ•ˆç›Šçš„GPT-4Vè§†è§‰è¯†åˆ«æä¾›äº†å¹³å°ã€‚æå‡ºäº†ä¸€ç§åŸºäºé—ä¼ ç®—æ³•ä¼˜åŒ–æ‹¼è´´å¸ƒå±€çš„åŸºç¡€æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†ä¸¤ä¸ªæŒ‡æ ‡æ¥è¡¡é‡ä¼˜åŒ–æ‹¼è´´æç¤ºçš„æ•ˆç‡ã€‚æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•ä½¿ç ”ç©¶äººå‘˜èƒ½å¤Ÿæ›´å¥½åœ°ä¼˜åŒ–æ‹¼è´´æç¤ºï¼Œä»è€Œä½¿GPT-4Våœ¨è§†è§‰è¯†åˆ«æ–¹é¢æ›´å…·æˆæœ¬æ•ˆç›Šã€‚ä»£ç å’Œæ•°æ®å¯åœ¨é¡¹ç›®é¡µé¢<a target="_blank" rel="noopener" href="https://collageprompting.github.io/%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://collageprompting.github.io/ä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.11468v2">PDF</a> Accepted by NAACL2025 Findings</p>
<p><strong>Summary</strong></p>
<p>GPT-4Våœ¨è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­å±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œä½†å…¶æ¨ç†æˆæœ¬é«˜æ˜‚ã€‚ä¸ºé™ä½æˆæœ¬ï¼Œç ”ç©¶æå‡ºä¸€ç§ç»æµé«˜æ•ˆçš„æ‹¼è´´æç¤ºä»»åŠ¡ï¼Œå°†å¤šä¸ªå›¾åƒæ‹¼è´´æˆå•ä¸€è§†è§‰æç¤ºï¼Œä½¿GPT-4Vèƒ½åŒæ—¶æ‰§è¡Œå¤šä¸ªå›¾åƒçš„è§†è§‰è¯†åˆ«ã€‚ç ”ç©¶è¯„ä¼°å‘ç°æ‹¼è´´ä½ç½®ã€å›¾åƒåˆ†ç»„å’Œç›¸é‚»å›¾åƒå½±å“è¯†åˆ«ç»“æœã€‚ä¸ºæ­¤æ„å»ºäº†ä¸€ä¸ªåä¸ºCollagePromptçš„åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæä¾›è®¾è®¡æ‹¼è´´æç¤ºä»¥å®ç°æ›´å…·æˆæœ¬æ•ˆç›Šçš„GPT-4Vè§†è§‰è¯†åˆ«ã€‚æå‡ºä¸€ç§åŸºäºé—ä¼ ç®—æ³•çš„åŸºçº¿æ–¹æ³•æ¥ä¼˜åŒ–æ‹¼è´´å¸ƒå±€ï¼Œå¹¶å¼•å…¥ä¸¤ä¸ªæŒ‡æ ‡æ¥è¡¡é‡ä¼˜åŒ–æ‹¼è´´æç¤ºçš„æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GPT-4Vå…·å¤‡å¼ºå¤§çš„è§†è§‰è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>GPT-4Væ¨ç†æˆæœ¬é«˜æ˜‚ï¼Œæˆä¸ºå…¶å¹¿æ³›åº”ç”¨çš„éšœç¢ã€‚</li>
<li>æå‡ºæ‹¼è´´æç¤ºä»»åŠ¡ï¼Œé€šè¿‡æ‹¼è´´å¤šä¸ªå›¾åƒé™ä½GPT-4Vçš„è§†è§‰è¯†åˆ«æˆæœ¬ã€‚</li>
<li>è¯„ä¼°å‘ç°æ‹¼è´´ä½ç½®ã€å›¾åƒåˆ†ç»„å’Œç›¸é‚»å›¾åƒå¯¹è¯†åˆ«ç»“æœæœ‰å½±å“ã€‚</li>
<li>æ„å»ºCollagePromptåŸºå‡†æµ‹è¯•å¹³å°ï¼Œä¸ºä¼˜åŒ–æ‹¼è´´æç¤ºæä¾›å·¥å…·ã€‚</li>
<li>æå‡ºåŸºäºé—ä¼ ç®—æ³•çš„åŸºçº¿æ–¹æ³•æ¥ä¼˜åŒ–æ‹¼è´´å¸ƒå±€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.11468">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b43de732cceaf07f47a287125c4327d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cd484734827281341b4199f66469989b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7dce236898f0dd87d84953bb4e19ee9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a065ddd1bdfc44f47601b90b269819b2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-523d1d698d6d446ee8b18ab277ca61ff.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Can-LLMs-Separate-Instructions-From-Data-And-What-Do-We-Even-Mean-By-That"><a href="#Can-LLMs-Separate-Instructions-From-Data-And-What-Do-We-Even-Mean-By-That" class="headerlink" title="Can LLMs Separate Instructions From Data? And What Do We Even Mean By   That?"></a>Can LLMs Separate Instructions From Data? And What Do We Even Mean By   That?</h2><p><strong>Authors:Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, Christoph H. Lampert</strong></p>
<p>Instruction-tuned Large Language Models (LLMs) show impressive results in numerous practical applications, but they lack essential safety features that are common in other areas of computer science, particularly an explicit separation of instructions and data. This makes them vulnerable to manipulations such as indirect prompt injections and generally unsuitable for safety-critical tasks. Surprisingly, there is currently no established definition or benchmark to quantify this phenomenon. In this work, we close this gap by introducing a formal measure for instruction-data separation and an empirical variant that is calculable from a modelâ€™s outputs. We also present a new dataset, SEP, that allows estimating the measure for real-world models. Our results on various LLMs show that the problem of instruction-data separation is real: all models fail to achieve high separation, and canonical mitigation techniques, such as prompt engineering and fine-tuning, either fail to substantially improve separation or reduce model utility. The source code and SEP dataset are openly accessible at <a target="_blank" rel="noopener" href="https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed">https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed</a>. </p>
<blockquote>
<p>æŒ‡ä»¤ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªå®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†å®ƒä»¬ç¼ºå°‘è®¡ç®—æœºç§‘å­¦å…¶ä»–é¢†åŸŸä¸­å¸¸è§çš„å…³é”®å®‰å…¨åŠŸèƒ½ï¼Œç‰¹åˆ«æ˜¯æŒ‡ä»¤å’Œæ•°æ®çš„æ˜ç¡®åˆ†ç¦»ã€‚è¿™ä½¿å¾—å®ƒä»¬å®¹æ˜“å—åˆ°é—´æ¥æç¤ºæ³¨å…¥ç­‰æ“ä½œçš„æ“çºµï¼Œå¹¶ä¸”é€šå¸¸ä¸é€‚åˆæ‰§è¡Œå®‰å…¨å…³é”®ä»»åŠ¡ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œç›®å‰å°šæ²¡æœ‰æ—¢å®šçš„å®šä¹‰æˆ–åŸºå‡†æ¥è¡¡é‡è¿™ç§ç°è±¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥æŒ‡ä»¤ä¸æ•°æ®åˆ†ç¦»çš„æ­£å¼åº¦é‡æ ‡å‡†ä»¥åŠå¯ä»æ¨¡å‹è¾“å‡ºä¸­è®¡ç®—çš„ç»éªŒå˜ä½“æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†SEPï¼Œå¯ä»¥ç”¨æ¥ä¼°è®¡çœŸå®ä¸–ç•Œæ¨¡å‹ä¸­çš„åº¦é‡æ ‡å‡†ã€‚æˆ‘ä»¬åœ¨å„ç§LLMä¸Šçš„ç»“æœè¡¨æ˜ï¼ŒæŒ‡ä»¤ä¸æ•°æ®åˆ†ç¦»çš„é—®é¢˜ç¡®å®å­˜åœ¨ï¼šæ‰€æœ‰æ¨¡å‹çš„åˆ†ç¦»ç¨‹åº¦å‡ä¸é«˜ï¼Œå¸¸è§çš„ç¼“è§£æŠ€æœ¯ï¼ˆå¦‚æç¤ºå·¥ç¨‹å’Œå¾®è°ƒï¼‰è¦ä¹ˆæ— æ³•å®è´¨æ€§æé«˜åˆ†ç¦»ç¨‹åº¦ï¼Œè¦ä¹ˆä¼šé™ä½æ¨¡å‹çš„å®ç”¨æ€§ã€‚æºä»£ç å’ŒSEPæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/egozverev/Shold-It-Be-Executed-Or-Processedå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.06833v3">PDF</a> Published as a conference paper at ICLR 2025, GitHub:   <a target="_blank" rel="noopener" href="https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed">https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed</a>. 10 pages main   text, 30 pages in total</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¯¸å¤šå®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ç¼ºä¹è®¡ç®—æœºç§‘å­¦å…¶ä»–é¢†åŸŸå¸¸è§çš„å®‰å…¨ç‰¹æ€§ï¼Œç‰¹åˆ«æ˜¯ç¼ºå°‘æŒ‡ä»¤ä¸æ•°æ®çš„æ˜ç¡®åˆ†ç¦»ã€‚è¿™ä½¿å…¶å®¹æ˜“å—åˆ°é—´æ¥æç¤ºæ³¨å…¥ç­‰æ“çºµï¼Œå¹¶ä¸é€‚åˆå®‰å…¨å…³é”®ä»»åŠ¡ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†æŒ‡ä»¤ä¸æ•°æ®åˆ†ç¦»çš„æ­£å¼è¡¡é‡æ ‡å‡†åŠå…¶å¯è®¡ç®—çš„å®è¯å˜ä½“ï¼Œå¹¶å¼•å…¥SEPæ•°æ®é›†æ¥ä¼°è®¡çœŸå®æ¨¡å‹çš„æŒ‡æ ‡ã€‚ç»“æœè¡¨æ˜æ‰€æœ‰æ¨¡å‹éƒ½å­˜åœ¨æŒ‡ä»¤ä¸æ•°æ®åˆ†ç¦»é—®é¢˜ï¼Œå¸¸è§„ç¼“è§£æŠ€æœ¯å¦‚æç¤ºå·¥ç¨‹å’Œå¾®è°ƒè¦ä¹ˆæ— æ³•å®è´¨æ”¹å–„åˆ†ç¦»ï¼Œè¦ä¹ˆä¼šé™ä½æ¨¡å‹æ•ˆç”¨ã€‚æ•°æ®é›†æºä»£ç å’ŒSEPæ•°æ®é›†å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªå®é™…åº”ç”¨ä¸­å±•ç°ä¼˜å¼‚æ€§èƒ½ï¼Œä½†ç¼ºä¹å¿…è¦çš„å®‰å…¨ç‰¹æ€§ã€‚</li>
<li>LLMç¼ºå°‘æŒ‡ä»¤ä¸æ•°æ®çš„æ˜ç¡®åˆ†ç¦»ï¼Œå®¹æ˜“å—åˆ°æ“çºµï¼Œä¸é€‚åˆå®‰å…¨å…³é”®ä»»åŠ¡ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†æŒ‡ä»¤ä¸æ•°æ®åˆ†ç¦»çš„æ­£å¼è¡¡é‡æ ‡å‡†å’Œå®è¯å˜ä½“ã€‚</li>
<li>å¼•å…¥SEPæ•°æ®é›†ç”¨äºä¼°è®¡çœŸå®æ¨¡å‹çš„æŒ‡ä»¤ä¸æ•°æ®åˆ†ç¦»ç¨‹åº¦ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜æ‰€æœ‰è¢«æµ‹è¯•çš„LLMéƒ½å­˜åœ¨æŒ‡ä»¤ä¸æ•°æ®åˆ†ç¦»é—®é¢˜ã€‚</li>
<li>å¸¸è§ç¼“è§£æŠ€æœ¯å¦‚æç¤ºå·¥ç¨‹å’Œå¾®è°ƒåœ¨æ”¹å–„æŒ‡ä»¤ä¸æ•°æ®åˆ†ç¦»æ–¹é¢æ•ˆæœæœ‰é™ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.06833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-da6bc34fa03914663e8edabc7ff2fd24.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f9f9c8fac84c097262fbe8430ade31d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ebab7749c9455438c76e128e6c9e09d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c82b81d4ec904a0268d7f4971e0bdcc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ae59c5768f383534e869a5777fed039d.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="What-Makes-for-Good-Visual-Instructions-Synthesizing-Complex-Visual-Reasoning-Instructions-for-Visual-Instruction-Tuning"><a href="#What-Makes-for-Good-Visual-Instructions-Synthesizing-Complex-Visual-Reasoning-Instructions-for-Visual-Instruction-Tuning" class="headerlink" title="What Makes for Good Visual Instructions? Synthesizing Complex Visual   Reasoning Instructions for Visual Instruction Tuning"></a>What Makes for Good Visual Instructions? Synthesizing Complex Visual   Reasoning Instructions for Visual Instruction Tuning</h2><p><strong>Authors:Yifan Du, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Jinpeng Wang, Chuyuan Wang, Mingchen Cai, Ruihua Song, Ji-Rong Wen</strong></p>
<p>Visual instruction tuning is crucial for enhancing the zero-shot generalization capability of Multi-modal Large Language Models (MLLMs). In this paper, we aim to investigate a fundamental question: â€˜â€™what makes for good visual instructionsâ€™â€™. Through a comprehensive empirical study, we find that instructions focusing on complex visual reasoning tasks are particularly effective in improving the performance of MLLMs, with results correlating to instruction complexity. Based on this insight, we develop a systematic approach to automatically create high-quality complex visual reasoning instructions. Our approach employs a synthesize-complicate-reformulate paradigm, leveraging multiple stages to gradually increase the complexity of the instructions while guaranteeing quality. Based on this approach, we create the ComVint dataset with 32K examples, and fine-tune four MLLMs on it. Experimental results consistently demonstrate the enhanced performance of all compared MLLMs, such as a 27.86% and 27.60% improvement for LLaVA on MME-Perception and MME-Cognition, respectively. Our code and data are publicly available at the link: <a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/ComVint">https://github.com/RUCAIBox/ComVint</a>. </p>
<blockquote>
<p>è§†è§‰æŒ‡ä»¤è°ƒæ•´å¯¹äºæé«˜å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç©¶ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼šâ€œä»€ä¹ˆæ ·çš„è§†è§‰æŒ‡ä»¤æ˜¯å¥½çš„â€ã€‚é€šè¿‡å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ä¸“æ³¨äºå¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡çš„æŒ‡ä»¤åœ¨æé«˜MLLMæ€§èƒ½æ–¹é¢ç‰¹åˆ«æœ‰æ•ˆï¼Œç»“æœä¸æŒ‡ä»¤å¤æ‚æ€§ç›¸å…³ã€‚åŸºäºæ­¤è§è§£ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç³»ç»Ÿçš„æ–¹æ³•ï¼Œè‡ªåŠ¨åˆ›å»ºé«˜è´¨é‡çš„å¤æ‚è§†è§‰æ¨ç†æŒ‡ä»¤ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨åˆæˆ-å¤æ‚åŒ–-é‡æ„çš„æ¨¡å¼ï¼Œåˆ©ç”¨å¤šä¸ªé˜¶æ®µæ¥é€æ­¥å¢åŠ æŒ‡ä»¤çš„å¤æ‚æ€§ï¼ŒåŒæ—¶ä¿è¯è´¨é‡ã€‚åŸºäºæ­¤æ–¹æ³•ï¼Œæˆ‘ä»¬åˆ›å»ºäº†åŒ…å«32Kå®ä¾‹çš„ComVintæ•°æ®é›†ï¼Œå¹¶åœ¨å…¶ä¸Šå¾®è°ƒäº†å››ä¸ªMLLMã€‚å®éªŒç»“æœä¸€è‡´è¡¨æ˜æ‰€æœ‰å¯¹æ¯”MLLMçš„æ€§èƒ½éƒ½æœ‰æ‰€æé«˜ï¼Œä¾‹å¦‚LLaVAåœ¨MME-Perceptionå’ŒMME-Cognitionä¸Šçš„æ”¹è¿›åˆ†åˆ«ä¸º27.86%å’Œ27.60%ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®åœ¨ä»¥ä¸‹é“¾æ¥å…¬å¼€å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/RUCAIBox/ComVint">https://github.com/RUCAIBox/ComVint</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.01487v2">PDF</a> Accepted by COLING2025</p>
<p><strong>Summary</strong><br>è§†è§‰æŒ‡ä»¤è°ƒä¼˜å¯¹äºæå‡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ—¨åœ¨æ¢ç©¶â€œä»€ä¹ˆæ˜¯å¥½çš„è§†è§‰æŒ‡ä»¤â€è¿™ä¸€åŸºæœ¬é—®é¢˜ã€‚é€šè¿‡å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°å…³æ³¨å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡çš„æŒ‡ä»¤åœ¨æé«˜MLLMæ€§èƒ½æ–¹é¢ç‰¹åˆ«æœ‰æ•ˆï¼Œå¹¶ä¸”ç»“æœä¸æŒ‡ä»¤çš„å¤æ‚æ€§ç›¸å…³ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§è‡ªåŠ¨åˆ›å»ºé«˜è´¨é‡å¤æ‚è§†è§‰æ¨ç†æŒ‡ä»¤çš„ç³»ç»Ÿæ–¹æ³•ã€‚ä½¿ç”¨åˆæˆ-å¤æ‚åŒ–-æ”¹å†™çš„æ¨¡å¼ï¼Œé€šè¿‡å¤šä¸ªé˜¶æ®µé€æ­¥å¢åŠ æŒ‡ä»¤çš„å¤æ‚æ€§åŒæ—¶ä¿è¯è´¨é‡ã€‚åŸºäºè¯¥æ–¹æ³•ï¼Œæˆ‘ä»¬åˆ›å»ºäº†åŒ…å«32Kå®ä¾‹çš„ComVintæ•°æ®é›†ï¼Œå¹¶åœ¨å…¶ä¸Šå¾®è°ƒäº†å››ä¸ªMLLMã€‚å®éªŒç»“æœä¸€è‡´è¡¨æ˜æ‰€æœ‰å¯¹æ¯”çš„MLLMæ€§èƒ½æœ‰æ‰€æå‡ï¼Œå¦‚åœ¨MME-Perceptionå’ŒMME-Cognitionä¸ŠLLaVAåˆ†åˆ«æå‡äº†27.86%å’Œ27.60%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰æŒ‡ä»¤è°ƒä¼˜å¯¹å¢å¼ºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>å¤æ‚è§†è§‰æ¨ç†ä»»åŠ¡çš„æŒ‡ä»¤å¯¹æé«˜è¯­è¨€æ¨¡å‹æ€§èƒ½æ•ˆæœæ˜¾è‘—ã€‚</li>
<li>æŒ‡ä»¤çš„å¤æ‚æ€§å¯¹æ¨¡å‹æ€§èƒ½æœ‰ç›´æ¥å½±å“ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è‡ªåŠ¨åˆ›å»ºé«˜è´¨é‡å¤æ‚è§†è§‰æ¨ç†æŒ‡ä»¤çš„ç³»ç»Ÿæ–¹æ³•ã€‚</li>
<li>åˆ›å»ºäº†åŒ…å«32Kå®ä¾‹çš„ComVintæ•°æ®é›†ç”¨äºæ¨¡å‹è®­ç»ƒã€‚</li>
<li>ä½¿ç”¨ComVintæ•°æ®é›†å¾®è°ƒçš„MLLMæ€§èƒ½æœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.01487">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-09c09c48971fb47779f2e27794c440e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c30945e4fb5612a4981f435c6e919710.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-19db595d345a9b03702881619a2853af.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dde0aad0d319d7ac7a70761f344dde35.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Chain-of-Factors-Paper-Reviewer-Matching"><a href="#Chain-of-Factors-Paper-Reviewer-Matching" class="headerlink" title="Chain-of-Factors Paper-Reviewer Matching"></a>Chain-of-Factors Paper-Reviewer Matching</h2><p><strong>Authors:Yu Zhang, Yanzhen Shen, SeongKu Kang, Xiusi Chen, Bowen Jin, Jiawei Han</strong></p>
<p>With the rapid increase in paper submissions to academic conferences, the need for automated and accurate paper-reviewer matching is more critical than ever. Previous efforts in this area have considered various factors to assess the relevance of a reviewerâ€™s expertise to a paper, such as the semantic similarity, shared topics, and citation connections between the paper and the reviewerâ€™s previous works. However, most of these studies focus on only one factor, resulting in an incomplete evaluation of the paper-reviewer relevance. To address this issue, we propose a unified model for paper-reviewer matching that jointly considers semantic, topic, and citation factors. To be specific, during training, we instruction-tune a contextualized language model shared across all factors to capture their commonalities and characteristics; during inference, we chain the three factors to enable step-by-step, coarse-to-fine search for qualified reviewers given a submission. Experiments on four datasets (one of which is newly contributed by us) spanning various fields such as machine learning, computer vision, information retrieval, and data mining consistently demonstrate the effectiveness of our proposed Chain-of-Factors model in comparison with state-of-the-art paper-reviewer matching methods and scientific pre-trained language models. </p>
<blockquote>
<p>éšç€æäº¤åˆ°å­¦æœ¯ä¼šè®®çš„è®ºæ–‡æ•°é‡è¿…é€Ÿå¢åŠ ï¼Œå¯¹è‡ªåŠ¨åŒ–å’Œå‡†ç¡®çš„è®ºæ–‡å®¡ç¨¿äººåŒ¹é…çš„éœ€æ±‚æ¯”ä»¥å¾€ä»»ä½•æ—¶å€™éƒ½æ›´ä¸ºå…³é”®ã€‚åœ¨æ­¤é¢†åŸŸçš„å…ˆå‰ç ”ç©¶å·²ç»è€ƒè™‘äº†å„ç§å› ç´ æ¥è¯„ä¼°å®¡ç¨¿äººçš„ä¸“ä¸šçŸ¥è¯†ä¸è®ºæ–‡çš„ç›¸å…³æ€§ï¼Œä¾‹å¦‚è¯­ä¹‰ç›¸ä¼¼æ€§ã€å…±åŒçš„ä¸»é¢˜ä»¥åŠè®ºæ–‡ä¸å®¡ç¨¿äººä¹‹å‰ä½œå“ä¹‹é—´çš„å¼•ç”¨è”ç³»ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°è¿™äº›ç ”ç©¶åªä¸“æ³¨äºä¸€ä¸ªå› ç´ ï¼Œå¯¼è‡´å¯¹è®ºæ–‡ä¸å®¡ç¨¿äººç›¸å…³æ€§çš„è¯„ä¼°ä¸å®Œæ•´ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è®ºæ–‡å®¡ç¨¿äººåŒ¹é…æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŒæ—¶è€ƒè™‘è¯­ä¹‰ã€ä¸»é¢˜å’Œå¼•ç”¨å› ç´ ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰å› ç´ å…±äº«ä¸€ä¸ªä¸Šä¸‹æ–‡åŒ–çš„è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä»¥æ•æ‰å®ƒä»¬çš„å…±åŒç‚¹å’Œç‰¹å¾ï¼›åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†è¿™ä¸‰ä¸ªå› ç´ ä¸²è”èµ·æ¥ï¼Œä»¥åœ¨ç»™å®šçš„æäº¤ä¸­è¿›è¡Œé€æ­¥çš„ã€ä»ç²—åˆ°ç»†çš„æœç´¢åˆæ ¼å®¡ç¨¿äººã€‚åœ¨æ¶µç›–æœºå™¨å­¦ä¹ ã€è®¡ç®—æœºè§†è§‰ã€ä¿¡æ¯æ£€ç´¢å’Œæ•°æ®æŒ–æ˜ç­‰ä¸åŒé¢†åŸŸçš„å››ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œä¸æˆ‘ä»¬æå‡ºçš„Chain-of-Factorsæ¨¡å‹ç›¸æ¯”ï¼Œä¸€è´¯è¯æ˜äº†å…¶åœ¨è®ºæ–‡å®¡ç¨¿äººåŒ¹é…æ–¹æ³•ä¸ç§‘å­¦é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14483v3">PDF</a> 10 pages; Accepted to WWW 2025 (Code:   <a target="_blank" rel="noopener" href="https://github.com/yuzhimanhua/CoF">https://github.com/yuzhimanhua/CoF</a>)</p>
<p><strong>Summary</strong><br>å­¦æœ¯ä¼šè®®è®ºæ–‡æäº¤æ•°é‡æ¿€å¢ï¼Œå¯¹è‡ªåŠ¨åŒ–ç²¾å‡†åŒ¹é…å®¡ç¨¿äººçš„éœ€æ±‚æ„ˆå‘é‡è¦ã€‚å…ˆå‰ç ”ç©¶ä¸»è¦èšç„¦äºå•ä¸€å› ç´ è¯„ä¼°å®¡ç¨¿äººä¸è®ºæ–‡ç›¸å…³æ€§ï¼Œå¦‚è¯­ä¹‰ç›¸ä¼¼æ€§ã€å…±åŒä¸»é¢˜å’Œå¼•ç”¨è”ç³»ç­‰ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§ç»Ÿä¸€çš„è®ºæ–‡ä¸å®¡ç¨¿äººåŒ¹é…æ¨¡å‹ï¼ŒåŒæ—¶è€ƒè™‘è¯­ä¹‰ã€ä¸»é¢˜å’Œå¼•ç”¨å› ç´ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒChain-of-Factorsæ¨¡å‹åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå½“å‰å…ˆè¿›çš„è®ºæ–‡å®¡ç¨¿åŒ¹é…æ–¹æ³•å’Œç§‘å­¦é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å­¦æœ¯ä¼šè®®è®ºæ–‡æäº¤æ•°é‡è¿…é€Ÿå¢é•¿ï¼Œå¯¹è‡ªåŠ¨åŒ–ç²¾å‡†åŒ¹é…å®¡ç¨¿äººçš„éœ€æ±‚æ„ˆå‘è¿«åˆ‡ã€‚</li>
<li>å…ˆå‰ç ”ç©¶ä¸»è¦èšç„¦äºå•ä¸€å› ç´ è¯„ä¼°å®¡ç¨¿äººä¸è®ºæ–‡çš„ç›¸å…³æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºä¸€ç§ç»Ÿä¸€çš„è®ºæ–‡ä¸å®¡ç¨¿äººåŒ¹é…æ¨¡å‹ï¼Œæ•´åˆè¯­ä¹‰ã€ä¸»é¢˜å’Œå¼•ç”¨å› ç´ ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨åˆ†é˜¶æ®µç²—åˆ°ç»†çš„æœç´¢ç­–ç•¥å¯»æ‰¾åˆæ ¼å®¡ç¨¿äººã€‚</li>
<li>æ¨¡å‹åœ¨å¤šä¸ªæ¶µç›–ä¸åŒé¢†åŸŸçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒChain-of-Factorsæ¨¡å‹åœ¨è®ºæ–‡ä¸å®¡ç¨¿äººåŒ¹é…æ–¹é¢çš„è¡¨ç°ä¼˜äºå½“å‰å…ˆè¿›çš„åŒ¹é…æ–¹æ³•å’Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2310.14483">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5149a61eb0aac6d834d962d383067f39.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07448aad390e9da5f51a4cbb32f013b0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee814dfeed1f56a6bc9a3c367c9a30c1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79428ec79aa7e1b4d5ed4808df144029.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Preserving-Knowledge-Invariance-Rethinking-Robustness-Evaluation-of-Open-Information-Extraction"><a href="#Preserving-Knowledge-Invariance-Rethinking-Robustness-Evaluation-of-Open-Information-Extraction" class="headerlink" title="Preserving Knowledge Invariance: Rethinking Robustness Evaluation of   Open Information Extraction"></a>Preserving Knowledge Invariance: Rethinking Robustness Evaluation of   Open Information Extraction</h2><p><strong>Authors:Ji Qi, Chuchun Zhang, Xiaozhi Wang, Kaisheng Zeng, Jifan Yu, Jinxin Liu, Jiuding Sun, Yuxiang Chen, Lei Hou, Juanzi Li, Bin Xu</strong></p>
<p>The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial measurement of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a popular large language model, the results show that the existing successful models exhibit a frustrating degradation, with a maximum drop of 23.43 F1 score. Our resources and code are available at <a target="_blank" rel="noopener" href="https://github.com/qijimrc/ROBUST">https://github.com/qijimrc/ROBUST</a>. </p>
<blockquote>
<p>æ¨¡å‹çš„åˆ†å¸ƒå˜åŒ–é²æ£’æ€§ä¿è¯äº†è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹èƒ½å¤Ÿåœ¨ç°å®ä¸–ç•Œä¸­æˆåŠŸåº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡æ¯æå–ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…ˆå‰çš„è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨éªŒè¯é…å¯¹åŒ¹é…çš„æ­£ç¡®æ€§ï¼Œå¿½ç•¥äº†é²æ£’æ€§çš„å…³é”®åº¦é‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªæ¨¡æ‹Ÿç°å®ä¸–ç•Œå¼€æ”¾ä¿¡æ¯æå–æ¨¡å‹è¯„ä¼°çš„åŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­ç›¸åŒçŸ¥è¯†å«ä¹‰ä¸‹çš„è¯­æ³•å’Œè¡¨è¾¾åˆ†å¸ƒå¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ã€‚æˆ‘ä»¬è®¾è®¡å¹¶æ ‡æ³¨äº†ä¸€ä¸ªå¤§è§„æ¨¡æµ‹è¯•å¹³å°ï¼Œå…¶ä¸­çš„æ¯ä¸ªä¾‹å­éƒ½æ˜¯ä¸€ä¸ªçŸ¥è¯†ä¸å˜çš„å°å›¢ä½“ï¼Œç”±å…·æœ‰ç›¸åŒå«ä¹‰çš„ç»“æ„åŒ–çŸ¥è¯†çš„å¥å­ç»„æˆï¼Œä½†å…·æœ‰ä¸åŒçš„è¯­æ³•å’Œè¡¨è¾¾å½¢å¼ã€‚é€šè¿‡è¿›ä¸€æ­¥é˜è¿°é²æ£’æ€§æŒ‡æ ‡ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹åœ¨æ•´ä¸ªç¾¤ä½“ä¸­çš„æ€§èƒ½å§‹ç»ˆå‡†ç¡®ï¼Œåˆ™è¯¥æ¨¡å‹è¢«è®¤ä¸ºæ˜¯ç¨³å¥çš„ã€‚æˆ‘ä»¬åœ¨è¿‡å»åå¹´ä¸­å‘å¸ƒçš„å…¸å‹æ¨¡å‹ä»¥åŠæµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œç°æœ‰æˆåŠŸæ¨¡å‹çš„æ€§èƒ½ä»¤äººæ²®ä¸§åœ°ä¸‹é™ï¼Œæœ€å¤§F1åˆ†æ•°ä¸‹é™äº†23.43åˆ†ã€‚æˆ‘ä»¬çš„èµ„æºå’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/qijimrc/ROBUST%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/qijimrc/ROBUSTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2305.13981v3">PDF</a> Accepted by EMNLP 2023 Main Conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†é¦–ä¸ªæ¨¡æ‹Ÿç°å®ä¸–ç•Œå¼€æ”¾ä¿¡æ¯æå–æ¨¡å‹è¯„ä¼°çš„åŸºå‡†æµ‹è¯•ï¼Œé‡ç‚¹å…³æ³¨æ¨¡å‹åœ¨è¯­æ³•å’Œè¡¨è¾¾åˆ†å¸ƒå˜åŒ–ä¸‹çš„ç¨³å¥æ€§ã€‚é€šè¿‡æ„å»ºå¤§è§„æ¨¡æµ‹è¯•å¹³å°ï¼Œå¯¹æ¨¡å‹åœ¨ä¸åŒè¯­æ³•å’Œè¡¨è¾¾å½¢å¼ä½†æ„ä¹‰ç›¸åŒçš„çŸ¥è¯†é›†åˆä¸Šè¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æˆåŠŸæ¨¡å‹åœ¨ç¨³å¥æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸‹é™ï¼Œæœ€å¤§F1åˆ†æ•°ä¸‹é™23.43ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡å¼ºè°ƒäº†NLPæ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­åº”ç”¨æ—¶ï¼Œå¯¹åˆ†å¸ƒå˜åŒ–ç¨³å¥æ€§çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿¡æ¯æå–ä»»åŠ¡ä¸­ã€‚</li>
<li>æå‡ºäº†é¦–ä¸ªæ¨¡æ‹Ÿç°å®ä¸–ç•Œå¼€æ”¾ä¿¡æ¯æå–æ¨¡å‹è¯„ä¼°çš„åŸºå‡†æµ‹è¯•ï¼Œå…³æ³¨æ¨¡å‹åœ¨çŸ¥è¯†æ„ä¹‰ç›¸åŒä½†è¯­æ³•å’Œè¡¨è¾¾åˆ†å¸ƒå˜åŒ–ä¸‹çš„æ€§èƒ½ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡æµ‹è¯•å¹³å°ï¼Œæ¯ä¸ªä¾‹å­éƒ½æ˜¯ä¸€ä¸ªçŸ¥è¯†ä¸å˜é›†åˆï¼ŒåŒ…å«ç»“æ„åŒ–çŸ¥è¯†ç›¸åŒä½†ä¸åŒè¯­æ³•å’Œè¡¨è¾¾å½¢å¼çš„å¥å­ã€‚</li>
<li>å®šä¹‰äº†æ¨¡å‹çš„ç¨³å¥æ€§è¯„ä¼°æ ‡å‡†ï¼Œå³æ¨¡å‹åœ¨æ•´ä½“é›†åˆä¸Šçš„æ€§èƒ½è¡¨ç°è¦ç¨³å®šå‡†ç¡®ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰æˆåŠŸæ¨¡å‹åœ¨ç¨³å¥æ€§æ–¹é¢å­˜åœ¨æ˜¾è‘—ä¸è¶³ï¼Œæœ€å¤§F1åˆ†æ•°ä¸‹é™23.43ã€‚</li>
<li>æä¾›äº†ç ”ç©¶èµ„æºå’Œä»£ç å…¬å¼€é“¾æ¥ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œè¿›ä¸€æ­¥æ¢ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2305.13981">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f77d875d108e5dc6e0bd4d2b7f87cce0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97274f37882fce1f7611a0b97eb4c552.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd1d8c41d6c44b40a2526672d03979a0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c2280d0bf90421137c1f0394839c094d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-216a0e5d24fa3e61fb3594991e064b94.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-09/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-09/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-11/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-820e620c53c7c61f13abf341d6049f71.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-11  Long-VITA Scaling Large Multi-modal Models to 1 Million Tokens with   Leading Short-Context Accuray
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-08/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-9083c84f5176d4d6b25603c42b11c618.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-08  ReactEmbed A Cross-Domain Framework for Protein-Molecule Representation   Learning via Biochemical Reaction Networks
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">16573k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
