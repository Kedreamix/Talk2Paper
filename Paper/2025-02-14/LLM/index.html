<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-02-14  HoVLE Unleashing the Power of Monolithic Vision-Language Models with   Holistic Vision-Language Embedding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-95de6cc92752fcfd0c90c2f7fce87553.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    54 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-02-14-更新"><a href="#2025-02-14-更新" class="headerlink" title="2025-02-14 更新"></a>2025-02-14 更新</h1><h2 id="HoVLE-Unleashing-the-Power-of-Monolithic-Vision-Language-Models-with-Holistic-Vision-Language-Embedding"><a href="#HoVLE-Unleashing-the-Power-of-Monolithic-Vision-Language-Models-with-Holistic-Vision-Language-Embedding" class="headerlink" title="HoVLE: Unleashing the Power of Monolithic Vision-Language Models with   Holistic Vision-Language Embedding"></a>HoVLE: Unleashing the Power of Monolithic Vision-Language Models with   Holistic Vision-Language Embedding</h2><p><strong>Authors:Chenxin Tao, Shiqian Su, Xizhou Zhu, Chenyu Zhang, Zhe Chen, Jiawen Liu, Wenhai Wang, Lewei Lu, Gao Huang, Yu Qiao, Jifeng Dai</strong></p>
<p>The rapid advance of Large Language Models (LLMs) has catalyzed the development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid modality-specific encoders, offer a promising alternative to the compositional ones but face the challenge of inferior performance. Most existing monolithic VLMs require tuning pre-trained LLMs to acquire vision abilities, which may degrade their language capabilities. To address this dilemma, this paper presents a novel high-performance monolithic VLM named HoVLE. We note that LLMs have been shown capable of interpreting images, when image embeddings are aligned with text embeddings. The challenge for current monolithic VLMs actually lies in the lack of a holistic embedding module for both vision and language inputs. Therefore, HoVLE introduces a holistic embedding module that converts visual and textual inputs into a shared space, allowing LLMs to process images in the same way as texts. Furthermore, a multi-stage training strategy is carefully designed to empower the holistic embedding module. It is first trained to distill visual features from a pre-trained vision encoder and text embeddings from the LLM, enabling large-scale training with unpaired random images and text tokens. The whole model further undergoes next-token prediction on multi-modal data to align the embeddings. Finally, an instruction-tuning stage is incorporated. Our experiments show that HoVLE achieves performance close to leading compositional models on various benchmarks, outperforming previous monolithic models by a large margin. Model available at <a target="_blank" rel="noopener" href="https://huggingface.co/OpenGVLab/HoVLE">https://huggingface.co/OpenGVLab/HoVLE</a>. </p>
<blockquote>
<p>大型语言模型（LLM）的快速发展推动了视觉语言模型（VLM）的进步。一体式VLM避免了模态特定编码器，为组合式VLM提供了有前景的替代方案，但面临性能较差的挑战。大多数现有的一体式VLM需要调整预训练的LLM以获得视觉能力，这可能会降低其语言功能。为了解决这一困境，本文提出了一种新型高性能一体式VLM，名为HoVLE。我们注意到，当图像嵌入与文本嵌入对齐时，LLM已被证明能够解释图像。当前一体式VLM的挑战实际上在于缺乏一个用于视觉和语言输入的全面嵌入模块。因此，HoVLE引入了一个全面嵌入模块，该模块将视觉和文本输入转换为共享空间，使LLM能够以相同的方式处理图像和文本。此外，精心设计了一种多阶段训练策略来增强全面嵌入模块。它首先经过训练，从预训练的视觉编码器和LLM中提炼出视觉特征和文本嵌入，使模型能够使用大量的未配对随机图像和文本标记进行大规模训练。整个模型进一步进行多模态数据的下一个标记预测，以对齐嵌入。最后，加入了指令调整阶段。我们的实验表明，HoVLE在各种基准测试中实现了接近领先组合模型的表现，大大超过了以前的一体式模型。模型可在<a target="_blank" rel="noopener" href="https://huggingface.co/OpenGVLab/HoVLE%E4%B8%8B%E8%BD%BD%E4%BD%BF%E7%94%A8%E3%80%82">https://huggingface.co/OpenGVLab/HoVLE下载使用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16158v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模语言模型（LLM）的快速发展推动了视觉语言模型（VLM）的进步。本文提出了一种新型高性能单体VLM——HoVLE，旨在解决现有单体VLM需要在预训练LLM上调整以获取视觉能力而导致的语言能力下降的问题。HoVLE引入了一个整体嵌入模块，将视觉和文本输入转换为共享空间，使LLM能够以相同的方式处理图像和文本。通过多阶段训练策略赋能整体嵌入模块，首先进行预训练视觉编码器和LLM的文本嵌入对齐，然后进行大规模未配对随机图像和文本标记的训练，最后进行多模态数据的下一个标记预测以对齐嵌入。实验表明，HoVLE在各种基准测试上的性能接近领先的组合模型，并且大大超过了以前的单体模型。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM的快速发展推动了VLM的进步，单体VLM作为一种有前景的替代方案出现，旨在解决组合模型的挑战。</li>
<li>现有单体VLM需要在预训练的LLM上调整以获取视觉能力，可能导致语言能力的退化。</li>
<li>HoVLE通过引入整体嵌入模块，将视觉和文本输入转换为共享空间，使LLM能同时处理图像和文本。</li>
<li>HoVLE采用多阶段训练策略，包括预训练视觉编码器和LLM的文本嵌入对齐、大规模未配对随机图像和文本标记的训练、以及多模态数据的下一个标记预测。</li>
<li>HoVLE在各种基准测试上的性能接近领先的组合模型，且大幅超越了之前的单体模型。</li>
<li>HoVLE模型可在huggingface.co&#x2F;OpenGVLab&#x2F;HoVLE上获取。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16158">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-7d8f0eb7e1984ee8e43b6c4cc638615b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-282cd19ee053d426e5026824038d06e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e058202889054595b27c1343b7b44a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6605b3a25999c28f229189dd7b69313.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Filter-then-Generate-Large-Language-Models-with-Structure-Text-Adapter-for-Knowledge-Graph-Completion"><a href="#Filter-then-Generate-Large-Language-Models-with-Structure-Text-Adapter-for-Knowledge-Graph-Completion" class="headerlink" title="Filter-then-Generate: Large Language Models with Structure-Text Adapter   for Knowledge Graph Completion"></a>Filter-then-Generate: Large Language Models with Structure-Text Adapter   for Knowledge Graph Completion</h2><p><strong>Authors:Ben Liu, Jihai Zhang, Fangquan Lin, Cheng Yang, Min Peng</strong></p>
<p>Large Language Models (LLMs) present massive inherent knowledge and superior semantic comprehension capability, which have revolutionized various tasks in natural language processing. Despite their success, a critical gap remains in enabling LLMs to perform knowledge graph completion (KGC). Empirical evidence suggests that LLMs consistently perform worse than conventional KGC approaches, even through sophisticated prompt design or tailored instruction-tuning. Fundamentally, applying LLMs on KGC introduces several critical challenges, including a vast set of entity candidates, hallucination issue of LLMs, and under-exploitation of the graph structure. To address these challenges, we propose a novel instruction-tuning-based method, namely FtG. Specifically, we present a filter-then-generate paradigm and formulate the KGC task into a multiple-choice question format. In this way, we can harness the capability of LLMs while mitigating the issue casused by hallucinations. Moreover, we devise a flexible ego-graph serialization prompt and employ a structure-text adapter to couple structure and text information in a contextualized manner. Experimental results demonstrate that FtG achieves substantial performance gain compared to existing state-of-the-art methods. The instruction dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/LB0828/FtG">https://github.com/LB0828/FtG</a>. </p>
<blockquote>
<p>大型语言模型（LLM）拥有庞大的内在知识和出色的语义理解能力，已经彻底改变了自然语言处理的各项任务。尽管取得了成功，但在知识图谱补全（KGC）方面，LLM仍存在明显不足。经验证据表明，即使在通过复杂提示设计或针对性指令调整的情况下，LLM的表现仍一直逊于传统的KGC方法。从根本上说，将LLM应用于KGC面临着几个关键挑战，包括大量的实体候选对象、LLM的幻觉问题以及对图结构利用不足等。为了解决这些挑战，我们提出了一种基于指令调整的新方法，称为FtG。具体来说，我们提出了一个“过滤然后生成”的模式，并将KGC任务转化为一个选择题格式。通过这种方式，我们可以利用LLM的能力，同时减轻幻觉问题带来的影响。此外，我们设计了一个灵活的自我图序列化提示，并采用了结构文本适配器，以语境化的方式将结构和文本信息进行结合。实验结果表明，与现有最先进的方法相比，FtG实现了显著的性能提升。指令数据集和代码可通过<a target="_blank" rel="noopener" href="https://github.com/LB0828/FtG%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/LB0828/FtG获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09094v3">PDF</a> COLING 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>大规模语言模型（LLM）具备丰富的内在知识和高级语义理解能，在自然语言处理的各种任务中带来了革命性的变化。然而，在知识图谱补全（KGC）方面，LLM的应用仍存在显著差距。尽管通过巧妙的提示设计或指令微调来优化，LLM的表现仍逊于传统KGC方法。为实现LLM在KGC任务中的有效应用，我们面临实体候选集庞大、LLM的虚构问题以及图形结构利用不足等挑战。为应对这些挑战，我们提出了一种基于指令调优的方法FtG，采用先过滤后生成的范式，将KGC任务转化为多选问题形式，从而利用LLM的优势并减轻虚构问题的影响。此外，我们还设计了灵活的自我图谱序列化提示，并采用结构文本适配器以语境化的方式结合结构和文本信息。实验结果表明，FtG相较于现有先进方法实现了显著的性能提升。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM具备强大的内在知识和语义理解能力，但在知识图谱补全任务中表现不佳。</li>
<li>LLM在KGC面临实体候选集庞大、虚构问题以及图形结构利用不足等挑战。</li>
<li>提出了一种新的方法FtG，基于指令调优，采用先过滤后生成的范式处理KGC任务。</li>
<li>FtG将KGC任务转化为多选问题形式，减轻LLM的虚构问题。</li>
<li>FtG设计了灵活的自我图谱序列化提示，并结合结构和文本信息。</li>
<li>实验证明，FtG在KGC任务上较现有方法有明显性能提升。</li>
<li>FtG的指令数据集和代码已公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09094">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c3aedb45c0983cfb1d157a22b6cdb2e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e631baf4c0f5193d3c9a4c22d055e29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb0c88c965b9abf65e9d9c1ee53a680d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c37c0d21abe441fac0f95ba65f21c6d8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TimeSuite-Improving-MLLMs-for-Long-Video-Understanding-via-Grounded-Tuning"><a href="#TimeSuite-Improving-MLLMs-for-Long-Video-Understanding-via-Grounded-Tuning" class="headerlink" title="TimeSuite: Improving MLLMs for Long Video Understanding via Grounded   Tuning"></a>TimeSuite: Improving MLLMs for Long Video Understanding via Grounded   Tuning</h2><p><strong>Authors:Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, Yali Wang, Yu Qiao, Limin Wang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated impressive performance in short video understanding. However, understanding long-form videos still remains challenging for MLLMs. This paper proposes TimeSuite, a collection of new designs to adapt the existing short-form video MLLMs for long video understanding, including a simple yet efficient framework to process long video sequence, a high-quality video dataset for grounded tuning of MLLMs, and a carefully-designed instruction tuning task to explicitly incorporate the grounding supervision in the traditional QA format. Specifically, based on VideoChat, we propose our long-video MLLM, coined as VideoChat-T, by implementing a token shuffling to compress long video tokens and introducing Temporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of visual representation. Meanwhile, we introduce the TimePro, a comprehensive grounding-centric instruction tuning dataset composed of 9 tasks and 349k high-quality grounded annotations. Notably, we design a new instruction tuning task type, called Temporal Grounded Caption, to peform detailed video descriptions with the corresponding time stamps prediction. This explicit temporal location prediction will guide MLLM to correctly attend on the visual content when generating description, and thus reduce the hallucination risk caused by the LLMs. Experimental results demonstrate that our TimeSuite provides a successful solution to enhance the long video understanding capability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the benchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T exhibits robust zero-shot temporal grounding capabilities, significantly outperforming the existing state-of-the-art MLLMs. After fine-tuning, it performs on par with the traditional supervised expert models. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）在短视频理解方面表现出了令人印象深刻的性能。然而，对于MLLMs来说，理解长视频仍然是一个挑战。本文针对现有短视频MLLMs的长视频理解适应性不足的问题，提出了TimeSuite方案，包括一个简单而高效的长视频序列处理框架、一个用于MLLMs精准调整的高质量视频数据集以及一个精心设计的任务指令调整任务，以明确将传统的问答格式中的基础监督纳入其中。具体来说，基于VideoChat，我们提出了名为VideoChat-T的长视频MLLM，它通过实现令牌混洗来压缩长视频令牌并引入时间自适应位置编码（TAPE）来提高视觉表示的时间意识。同时，我们引入了TimePro，这是一个全面的以基础为中心的任务指令数据集，包含9个任务和34.9万高质量的基础注释。值得一提的是，我们设计了一种新的任务指令类型，称为时间基础字幕，以进行详细的视频描述和相应的时间戳预测。这种明确的时间位置预测将指导MLLM在生成描述时正确关注视觉内容，从而降低LLM产生的幻觉风险。实验结果表明，我们的TimeSuite方案成功地提高了短视频MLLM对长视频的理解能力，在Egoschema和VideoMME基准测试上的改进分别为5.6%和6.8%。此外，VideoChat-T展现出强大的零样本时间基础能力，显著优于现有的最先进的MLLMs。经过微调后，其性能与传统的监督专家模型相当。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19702v2">PDF</a> Accepted by ICLR2025</p>
<p><strong>Summary</strong></p>
<p>在本文中，研究者提出了TimeSuite方案，通过设计新颖的方法和框架来解决长视频理解问题。其中提出了基于VideoChat的VideoChat-T模型，使用标记置换技术和时间自适应位置编码（TAPE）增强模型对长视频的感知能力。同时，引入了TimePro数据集，包含多种任务类型与标注信息用于提升模型性能。研究结果显示TimeSuite方法有效提高了长视频理解效果，成功扩展了原有模型性能，相较于现有模型具有显著优势。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TimeSuite方案旨在解决多模态大型语言模型在长视频理解上的挑战。</li>
<li>VideoChat-T模型通过标记置换和时间自适应位置编码技术实现长视频理解。</li>
<li>TimePro数据集为MLLM提供了全面而准确的任务导向标注数据，用以指导模型的训练和测试。</li>
<li>设计了一种名为Temporal Grounded Caption的新任务类型，用于预测时间戳并减少模型产生的幻觉风险。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.19702">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c0fbfa33971e5434cf1d9e7e23c7e52c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e691accdb034b7c00206e5b2639f7fe6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a311d5a8610845b8d3ace50ee054b25d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Diffusion-Transformer-Policy-Scaling-Diffusion-Transformer-for-Generalist-Visual-Language-Action-Learning"><a href="#Diffusion-Transformer-Policy-Scaling-Diffusion-Transformer-for-Generalist-Visual-Language-Action-Learning" class="headerlink" title="Diffusion Transformer Policy: Scaling Diffusion Transformer for   Generalist Visual-Language-Action Learning"></a>Diffusion Transformer Policy: Scaling Diffusion Transformer for   Generalist Visual-Language-Action Learning</h2><p><strong>Authors:Zhi Hou, Tianyi Zhang, Yuwen Xiong, Hengjun Pu, Chengyang Zhao, Ronglei Tong, Yu Qiao, Jifeng Dai, Yuntao Chen</strong></p>
<p>Recent large visual-language action models pretrained on diverse robot datasets have demonstrated the potential for generalizing to new environments with a few in-domain data. However, those approaches usually predict individual discretized or continuous action by a small action head, which limits the ability in handling diverse action spaces. In contrast, we model the continuous action sequence with a large multi-modal diffusion transformer, dubbed as Diffusion Transformer Policy, in which we directly denoise action chunks by a large transformer model rather than a small action head for action embedding. By leveraging the scaling capability of transformers, the proposed approach can effectively model continuous end-effector actions across large diverse robot datasets, and achieve better generalization performance. Extensive experiments demonstrate the effectiveness and generalization of Diffusion Transformer Policy on Maniskill2, Libero, Calvin and SimplerEnv, as well as the real-world Franka arm, achieving consistent better performance on Real-to-Sim benchmark SimplerEnv, real-world Franka Arm and Libero compared to OpenVLA and Octo. Specifically, without bells and whistles, the proposed approach achieves state-of-the-art performance with only a single third-view camera stream in the Calvin task ABC-&gt;D, improving the average number of tasks completed in a row of 5 to 3.6, and the pretraining stage significantly facilitates the success sequence length on the Calvin by over 1.2. Project Page: <a target="_blank" rel="noopener" href="https://zhihou7.github.io/dit_policy_vla/">https://zhihou7.github.io/dit_policy_vla/</a> </p>
<blockquote>
<p>最近的大型视觉语言行动模型在多种机器人数据集上进行预训练，并已证明其在少量领域内数据下适应新环境的潜力。然而，这些方法通常通过小型动作头预测个体离散或连续动作，这限制了处理多样动作空间的能力。相比之下，我们采用大型多模态扩散变压器来模拟连续动作序列，称为扩散变压器策略。在该策略中，我们通过一个大型变压器模型直接对动作块进行去噪，而不是通过小型动作头进行动作嵌入。通过利用变压器的可扩展能力，所提出的方法可以有效地模拟跨越大型多样机器人数据集的连续末端执行器动作，并实现了更好的泛化性能。大量实验表明，扩散变压器策略在Maniskill2、Libero、Calvin和SimplerEnv等多个数据集上的有效性和泛化性。此外，与OpenVLA和Octo相比，它在现实世界的Franka手臂、Libero以及SimplerEnv等基准测试上表现更出色。具体来说，在不使用任何花哨技巧的情况下，所提出的方法仅在Calvin任务的ABC-&gt;D中使用单一第三视角相机流即实现了最先进的性能，将连续完成任务的平均数量从5提高到3.6。预训练阶段显著增加了Calvin任务的成功序列长度超过1.2。项目页面：<a target="_blank" rel="noopener" href="https://zhihou7.github.io/dit_policy_vla/">https://zhihou7.github.io/dit_policy_vla/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15959v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>本文介绍了使用大型多模态扩散变压器（Diffusion Transformer Policy）对连续动作序列进行建模的方法。该方法直接通过大型变压器模型去噪动作块，而不是通过小动作头进行动作嵌入。利用变压器的可扩展性，该方法可以有效地对大型多样机器人数据集进行连续末端执行器动作建模，并实现了更好的泛化性能。实验表明，该方法在多个机器人任务上实现了卓越的性能和泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了使用大型多模态扩散变压器（Diffusion Transformer Policy）处理机器人连续动作序列的方法。</li>
<li>该方法通过大型变压器模型直接对动作块进行去噪，而非使用小动作头进行动作嵌入。</li>
<li>变压器模型的扩展性使得方法能够有效地对大型多样的机器人数据集进行建模。</li>
<li>方法实现了更好的泛化性能，在多个机器人任务上表现出卓越的性能。</li>
<li>在Real-to-Sim基准测试SimplerEnv、真实世界的Franka手臂和Libero任务上，该方法相较于OpenVLA和Octo具有更好的性能。</li>
<li>在Calvin任务的ABC-&gt;D中，该方法仅使用单第三人称视角相机流即实现了先进性能，完成了平均任务行数从5到3.6的提升。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15959">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c3954fd59d22b86cb46cfbb71920af2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00a652f829a46d1224ec081fdfa93a59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a48abcee3297f1afd349d62ba9241b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-338b40674e9e90bfe793622a8441e73b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83ca834836b706621f4326e23adfbacb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1368f54c1ebf7e7419c4451fc07e173.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TimeDiT-General-purpose-Diffusion-Transformers-for-Time-Series-Foundation-Model"><a href="#TimeDiT-General-purpose-Diffusion-Transformers-for-Time-Series-Foundation-Model" class="headerlink" title="TimeDiT: General-purpose Diffusion Transformers for Time Series   Foundation Model"></a>TimeDiT: General-purpose Diffusion Transformers for Time Series   Foundation Model</h2><p><strong>Authors:Defu Cao, Wen Ye, Yizhou Zhang, Yan Liu</strong></p>
<p>Foundation models, particularly Large Language Models (LLMs), have revolutionized text and video processing, yet time series data presents distinct challenges for such approaches due to domain-specific features such as missing values, multi-resolution characteristics, etc. Furthermore, the de-facto autoregressive transformers tend to learn deterministic temporal dependencies within pre-trained data while overlooking inherent uncertainties and lacking integration of physical constraints. In this paper, we introduce TimeDiT, a diffusion transformer model that synergistically combines transformer-based temporal dependency learning with diffusion-based probabilistic sampling. TimeDiT employs a unified masking mechanism to harmonize the training and inference process across diverse tasks while introducing a theoretically grounded, finetuning-free model editing strategy that enables flexible integration of external knowledge during sampling. Acknowledging the challenges of unifying multiple downstream tasks under a single model, our systematic evaluation demonstrates TimeDiT’s effectiveness both in fundamental tasks, i.e., forecasting and imputation, through zero-shot&#x2F;fine-tuning; and in domain tasks, i.e., multi-resolution forecasting, anomaly detection, and data generation, establishing it as a \textit{proto-foundation model} that bridges the gap between general-purpose and domain-specific models. </p>
<blockquote>
<p>基于模型的革命性变化，特别是大型语言模型（LLM）在文本和视频处理方面的应用，时间序列数据因其特有的缺失值、多分辨率特性等特征给这些方法带来了独特的挑战。此外，当前的自回归transformer模型倾向于在预训练数据中学习确定性时间依赖关系，而忽略了固有的不确定性并缺乏物理约束的集成。在本文中，我们介绍了TimeDiT，这是一个结合了基于转换器的时间依赖关系学习与基于扩散的概率采样技术的扩散转换器模型。TimeDiT采用统一的掩码机制来协调不同任务的训练和推理过程，同时引入了一种有理论基础的、无需微调的模型编辑策略，可以在采样过程中灵活地集成外部知识。我们认识到在一个单一模型下统一多个下游任务所面临的挑战，通过系统的评估表明，TimeDiT在基础任务（如预测和插值）以及领域任务（如多分辨率预测、异常检测和数据生成）中的表现都非常出色，成为连接通用模型和特定领域模型的桥梁。TimeDiT作为一个原初的基础模型建立起了这个桥梁。它填补了通用模型和特定领域模型之间的空白。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02322v2">PDF</a> 31 Pages, 11 Figures, 22 Tables. First present at ICML 2024 Workshop   on Foundation Models in the Wild</p>
<p><strong>Summary</strong></p>
<p>文本介绍了针对时间序列数据的新模型TimeDiT，它结合了基于变压器的时序依赖性学习和基于扩散的概率采样技术。TimeDiT采用统一的掩码机制，在多样化任务中协调训练和推理过程，并引入了一种理论基础的、无需微调模型编辑策略，可在采样过程中灵活集成外部知识。TimeDiT解决了统一多个下游任务在单一模型下的挑战，在基础任务和领域任务中都展现出有效性和实用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TimeDiT是一种针对时间序列数据的扩散变压器模型，结合了基于变压器的时序依赖性学习和基于扩散的概率采样技术。</li>
<li>TimeDiT采用统一的掩码机制，用于在多样化任务中协调训练和推理过程。</li>
<li>该模型引入了一种无需微调模型编辑策略，能够在采样过程中灵活集成外部知识。</li>
<li>TimeDiT解决了在单一模型中统一多个下游任务的挑战。</li>
<li>TimeDiT在基础任务（如预测和插补）和领域任务（如多分辨率预测、异常检测和生成数据）中都表现出有效性。</li>
<li>TimeDiT填补了通用模型和特定领域模型之间的空白。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02322">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4c21b86e5514c8b18a6608e4725e7e80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91983f17113cf0b00b89b2b48069d574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe5c486bd7c35b2ee776f5b806fbcccf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Improving-and-Assessing-the-Fidelity-of-Large-Language-Models-Alignment-to-Online-Communities"><a href="#Improving-and-Assessing-the-Fidelity-of-Large-Language-Models-Alignment-to-Online-Communities" class="headerlink" title="Improving and Assessing the Fidelity of Large Language Models Alignment   to Online Communities"></a>Improving and Assessing the Fidelity of Large Language Models Alignment   to Online Communities</h2><p><strong>Authors:Minh Duc Chu, Zihao He, Rebecca Dorn, Kristina Lerman</strong></p>
<p>Large language models (LLMs) have shown promise in representing individuals and communities, offering new ways to study complex social dynamics. However, effectively aligning LLMs with specific human groups and systematically assessing the fidelity of the alignment remains a challenge. This paper presents a robust framework for aligning LLMs with online communities via instruction-tuning and comprehensively evaluating alignment across various aspects of language, including authenticity, emotional tone, toxicity, and harm. We demonstrate the utility of our approach by applying it to online communities centered on dieting and body image. We administer an eating disorder psychometric test to the aligned LLMs to reveal unhealthy beliefs and successfully differentiate communities with varying levels of eating disorder risk. Our results highlight the potential of LLMs in automated moderation and broader applications in public health and social science research. </p>
<blockquote>
<p>大型语言模型（LLM）在代表个人和社区方面展现出潜力，为人们研究复杂的社交动态提供了新的途径。然而，如何有效地将LLM与特定的人群进行匹配，以及如何系统地评估匹配程度的保真度，仍然是一个挑战。本文针对LLM如何通过网络社区中的指令微调来进行适配这一问题提出了一个可靠的框架，并对语言各领域的匹配程度进行了全面的评估，包括真实性、情感基调、攻击性以及有害性。本文通过在以节食和体型为主的网络社区中实施该策略来展示其效用。我们对调整后的LLM进行了一项饮食障碍心理测试，揭示了其不健康的信念，并成功区分了不同饮食障碍风险的社区。我们的研究结果突显了LLM在自动化监管以及公共卫生和社会科学研究的广泛应用潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09366v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在代表个人和社区方面展现出潜力，为复杂的社会动态研究提供了新的途径。然而，如何将LLMs与特定人类群体有效对齐，并系统评估对齐的保真度仍存在挑战。本文通过构建稳健的框架，利用指令微调将LLMs与在线社区对齐，并全面评估语言各方面的对齐情况，包括真实性、情感基调、毒性和危害。我们通过对以饮食和体形为中心的在线社区进行演示，展示了我们方法的有效性。结果揭示了LLMs在自动管理和公共卫生及社会科学研究中的潜在应用。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMs为复杂社会动态研究提供了新的途径。</li>
<li>LLMs与特定人类群体的有效对齐是一个挑战。</li>
<li>通过指令微调，可以将LLMs与在线社区对齐。</li>
<li>对齐的评估需全面考虑语言的真实性、情感基调、毒性和危害。</li>
<li>以饮食和体形为中心的在线社区作为演示展示了方法的有效性。</li>
<li>LLMs在自动管理和公共卫生及社会科学研究中具有潜在应用。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.09366">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e36fb2a081da349f933c5835b2593cce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01b80b4fd357b7da71b0d0fe1221901b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa9ac2ba6822a0c0d1ffcb091474a8b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3be36e1ffa99c05e2a93e188fa0180b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e6995ad44d770b81fd816b9922c04ee.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Why-Are-My-Prompts-Leaked-Unraveling-Prompt-Extraction-Threats-in-Customized-Large-Language-Models"><a href="#Why-Are-My-Prompts-Leaked-Unraveling-Prompt-Extraction-Threats-in-Customized-Large-Language-Models" class="headerlink" title="Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in   Customized Large Language Models"></a>Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in   Customized Large Language Models</h2><p><strong>Authors:Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, Haoyang Li</strong></p>
<p>The drastic increase of large language models’ (LLMs) parameters has led to a new research direction of fine-tuning-free downstream customization by prompts, i.e., task descriptions. While these prompt-based services (e.g. OpenAI’s GPTs) play an important role in many businesses, there has emerged growing concerns about the prompt leakage, which undermines the intellectual properties of these services and causes downstream attacks. In this paper, we analyze the underlying mechanism of prompt leakage, which we refer to as prompt memorization, and develop corresponding defending strategies. By exploring the scaling laws in prompt extraction, we analyze key attributes that influence prompt extraction, including model sizes, prompt lengths, as well as the types of prompts. Then we propose two hypotheses that explain how LLMs expose their prompts. The first is attributed to the perplexity, i.e. the familiarity of LLMs to texts, whereas the second is based on the straightforward token translation path in attention matrices. To defend against such threats, we investigate whether alignments can undermine the extraction of prompts. We find that current LLMs, even those with safety alignments like GPT-4, are highly vulnerable to prompt extraction attacks, even under the most straightforward user attacks. Therefore, we put forward several defense strategies with the inspiration of our findings, which achieve 83.8% and 71.0% drop in the prompt extraction rate for Llama2-7B and GPT-3.5, respectively. Source code is avaliable at <a target="_blank" rel="noopener" href="https://github.com/liangzid/PromptExtractionEval">https://github.com/liangzid/PromptExtractionEval</a>. </p>
<blockquote>
<p>随着大型语言模型（LLM）参数的大规模增长，通过提示（即任务描述）进行微调以外的下游定制已经成为了一个新的研究方向。虽然这些基于提示的服务（例如OpenAI的GPT系列）在许多业务中发挥着重要作用，但关于提示泄露的担忧也在日益增长。提示泄露破坏了这些服务的知识产权并导致了下游攻击。在本文中，我们分析了提示泄露的内在机制，我们称之为提示记忆，并开发了相应的防御策略。通过探索提示提取中的规模定律，我们分析了影响提示提取的关键属性，包括模型大小、提示长度以及提示类型。然后，我们提出了两个假设来解释LLM如何暴露其提示。第一个假设归因于困惑度，即LLM对文本的熟悉程度，而第二个假设则是基于注意力矩阵中的直接令牌翻译路径。为了应对这些威胁，我们调查了对齐是否能阻碍提示的提取。我们发现，即使是具有安全对齐的当前LLM（如GPT-4）也非常容易受到提示提取攻击，即使在最简单的用户攻击下也是如此。因此，我们根据研究结果提出了几种防御策略，实现了Llama2-7B和GPT-3.5的提示提取率分别下降了83.8%和71.0%。源代码可在<a target="_blank" rel="noopener" href="https://github.com/liangzid/PromptExtractionEval%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/liangzid/PromptExtractionEval找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.02416v2">PDF</a> Source Code: <a target="_blank" rel="noopener" href="https://github.com/liangzid/PromptExtractionEval">https://github.com/liangzid/PromptExtractionEval</a></p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）参数剧增推动了无需精细调整的下游定制方向，即通过任务描述提示来完成。尽管提示服务在许多商业领域扮演重要角色，但提示泄露问题逐渐浮现，侵犯了服务的知识产权并引发下游攻击。本文分析了提示泄露的内在机制——提示记忆化，并发展了相应的防御策略。通过探索提示提取中的规模效应规律，分析了影响提示提取的关键因素，包括模型大小、提示长度和提示类型等。本文提出两个假设来解释LLM如何暴露其提示：一是与困惑度有关，二是基于注意力矩阵中的直接标记翻译路径。为应对这些威胁，本文调查了对齐是否能阻止提示提取。研究发现，即使是带有安全对齐的当前LLM（如GPT-4）也极易受到用户简单的提示提取攻击。受本研究结果启发，提出的防御策略使Llama2-7B和GPT-3.5的提示提取率分别下降了83.8%和71.0%。相关源代码可通过链接获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）通过任务描述提示实现下游定制，但提示泄露问题日益严重。</li>
<li>提示泄露的内在机制是提示记忆化，这威胁到LLM服务的知识产权。</li>
<li>影响提示提取的关键因素包括模型大小、提示长度和类型。</li>
<li>LLM暴露其提示的两个假设：与困惑度有关和基于注意力矩阵的直接标记翻译路径。</li>
<li>当前LLM（如GPT-4）易受简单用户攻击的威胁，存在严重的提示泄露问题。</li>
<li>受研究结果启发，提出的防御策略显著降低了Llama和GPT的提示提取率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.02416">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4615815ec3a7b171bff42eda674f9602.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7859bfcb3611914e713c6c733af550d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa19e5455776ad3af9654e49e8f3a10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9a05d58c560cb26832f25d93c040c7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-618a5b28ee5635dfddb7323d77bfae30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78ccdc5a5cff9c5adbc720251eb151e2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="OmniBal-Towards-Fast-Instruct-tuning-for-Vision-Language-Models-via-Omniverse-Computation-Balance"><a href="#OmniBal-Towards-Fast-Instruct-tuning-for-Vision-Language-Models-via-Omniverse-Computation-Balance" class="headerlink" title="OmniBal: Towards Fast Instruct-tuning for Vision-Language Models via   Omniverse Computation Balance"></a>OmniBal: Towards Fast Instruct-tuning for Vision-Language Models via   Omniverse Computation Balance</h2><p><strong>Authors:Yongqiang Yao, Jingru Tan, Jiahao Hu, Feizhao Zhang, Yazhe Niu, Xin Jin, Bo Li, Ruihao Gong, Pengfei Liu, Dahua Lin, Ningyi Xu</strong></p>
<p>Recently, vision-language instruct-tuning models have made significant progress due to their more comprehensive understanding of the world. In this work, we discovered that large-scale 3D parallel training on those models leads to an imbalanced computation load across different devices. The vision and language parts are inherently heterogeneous: their data distribution and model architecture differ significantly, which affects distributed training efficiency. We rebalanced the computational loads from data, model, and memory perspectives to address this issue, achieving more balanced computation across devices. These three components are not independent but are closely connected, forming an omniverse balanced training framework. Specifically, for the data, we grouped instances into new balanced mini-batches within and across devices. For the model, we employed a search-based method to achieve a more balanced partitioning. For memory optimization, we adaptively adjusted the re-computation strategy for each partition to utilize the available memory fully. We conducted extensive experiments to validate the effectiveness of our method. Compared with the open-source training code of InternVL-Chat, we significantly reduced GPU days, achieving about 1.8x speed-up. Our method’s efficacy and generalizability were further demonstrated across various models and datasets. Codes will be released at <a target="_blank" rel="noopener" href="https://github.com/ModelTC/OmniBal">https://github.com/ModelTC/OmniBal</a>. </p>
<blockquote>
<p>最近，由于视觉语言指令微调模型对世界的理解更加全面，它们取得了显著的进展。在这项工作中，我们发现对这些模型进行大规模3D并行训练会导致不同设备之间的计算负载不平衡。视觉和语言部分是固有地异质的：它们的数据分布和模型架构存在很大差异，这影响了分布式训练的效率。我们从数据、模型和内存三个方面重新平衡计算负载，以解决此问题，实现跨设备的更平衡计算。这三个组件不是独立的，而是紧密相关的，形成了一个全方位平衡的训练框架。具体来说，对于数据，我们将实例分为设备和跨设备之间的新平衡小批量。对于模型，我们采用基于搜索的方法来实现更平衡的分区。对于内存优化，我们自适应地调整每个分区的重新计算策略，以充分利用可用内存。我们进行了大量实验来验证我们的方法的有效性。与开源训练代码InternVL-Chat相比，我们大幅减少了GPU天数，实现了约1.8倍的速度提升。我们的方法在各种模型和数据集上的有效性和通用性得到了进一步证明。代码将在<a target="_blank" rel="noopener" href="https://github.com/ModelTC/OmniBal%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/ModelTC/OmniBal发布。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20761v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大规模三维并行训练在视觉语言指令微调模型上会导致计算负载不均衡。为解决这一问题，我们从数据、模型和内存三个方面重新平衡计算负载，形成了一个全方位平衡的训练框架。通过平衡数据分布、优化模型分区和调整内存使用策略，我们提高了分布式训练效率，减少了GPU天数，实现了跨模型和数据集的有效性和泛化性提升。相关代码将在GitHub上发布。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言指令微调模型在大规模三维并行训练时存在计算负载不均衡问题。</li>
<li>视觉和语言部分在数据分布和模型架构上存在固有差异，影响分布式训练效率。</li>
<li>为解决计算负载不均衡问题，从数据、模型和内存三个方面进行了重新平衡。</li>
<li>通过平衡数据分布、优化模型分区和调整内存使用策略，提高了训练效率。</li>
<li>与开源训练代码相比，减少了GPU天数，实现了约1.8倍的速度提升。</li>
<li>该方法在各种模型和数据集上表现出有效性和泛化性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.20761">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c4c737ee94e2efdb79f2fa4b3a187d30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36c292c3621e8dc0e7e891c637fb7b94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e94b6e3107dfced375733b6222d8fca0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2445de10961c1b5aa637e4d36199e6d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e5090351b95da9e2f7ac0fab0532bb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-761c8541addedf7416be192890ca6629.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="From-Loops-to-Oops-Fallback-Behaviors-of-Language-Models-Under-Uncertainty"><a href="#From-Loops-to-Oops-Fallback-Behaviors-of-Language-Models-Under-Uncertainty" class="headerlink" title="From Loops to Oops: Fallback Behaviors of Language Models Under   Uncertainty"></a>From Loops to Oops: Fallback Behaviors of Language Models Under   Uncertainty</h2><p><strong>Authors:Maor Ivgi, Ori Yoran, Jonathan Berant, Mor Geva</strong></p>
<p>Large language models (LLMs) often exhibit undesirable behaviors, such as hallucinations and sequence repetitions. We propose to view these behaviors as fallbacks that models exhibit under epistemic uncertainty, and investigate the connection between them. We categorize fallback behaviors - sequence repetitions, degenerate text, and hallucinations - and extensively analyze them in models from the same family that differ by the amount of pretraining tokens, parameter count, or the inclusion of instruction-following training. Our experiments reveal a clear and consistent ordering of fallback behaviors, across all these axes: the more advanced an LLM is (i.e., trained on more tokens, has more parameters, or instruction-tuned), its fallback behavior shifts from sequence repetitions, to degenerate text, and then to hallucinations. Moreover, the same ordering is observed during the generation of a single sequence, even for the best-performing models; as uncertainty increases, models shift from generating hallucinations to producing degenerate text and finally sequence repetitions. Lastly, we demonstrate that while common decoding techniques, such as random sampling, alleviate unwanted behaviors like sequence repetitions, they increase harder-to-detect hallucinations. </p>
<blockquote>
<p>大型语言模型（LLM）常常表现出一些不理想的行为，如幻觉和序列重复。我们提出将这些行为视为模型在认知不确定性下表现出的回退行为，并研究它们之间的联系。我们对回退行为进行分类，包括序列重复、文本退化以及幻觉，并在同一家族的模型中进行了广泛的分析，这些模型在预训练令牌的数量、参数计数或是否包含指令跟随训练方面有所不同。我们的实验揭示了各种轴上的回退行为的清晰且一致排序：一个LLM越先进（即训练了更多的令牌、有更多的参数或经过指令调整），其回退行为从序列重复转变为文本退化，然后到幻觉。此外，在生成单个序列的过程中，即使对于性能最佳的模型，也可以观察到相同的排序；随着不确定性的增加，模型从产生幻觉转变为生成文本退化并最终出现序列重复。最后，我们证明常见的解码技术（如随机采样）虽然可以减轻不希望出现的行为（如序列重复），但它们会增加更难检测的幻觉。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.06071v2">PDF</a> NeurIPS Workshop on Attributing Model Behavior at Scale (ATTRIB 2024)</p>
<p><strong>总结</strong></p>
<p>本文探讨了大型语言模型（LLM）在面临知识不确定性时产生的退化行为，如序列重复、文本退化和幻觉。文章对模型进行归类，并通过实验分析不同预训练标记量、参数数量或是否包含指令训练等因素对其行为的影响。实验揭示了一个明确的退化行为顺序：随着模型的发展（即更多的预训练标记、参数或指令调整），其退化行为从序列重复转变为文本退化，最终出现幻觉。此外，在同一序列生成过程中也观察到相同的顺序。最后，文章指出常见的解码技术如随机采样虽然可以减轻序列重复等不期望的行为，但它们可能增加更难检测的幻觉问题。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>LLM在知识不确定性下会表现出退化行为，包括序列重复、文本退化和幻觉。</li>
<li>退化行为在不同模型之间存在一致性，并与模型的预训练标记量、参数数量以及是否经过指令训练等因素有关。</li>
<li>随着模型先进程度的提高，其退化行为呈现出一种可预测的顺序变化。</li>
<li>在生成单一序列时，随着不确定性的增加，模型的退化行为也呈现上述顺序变化。</li>
<li>常见的解码技术如随机采样虽能减轻某些不期望的行为，但可能增加其他类型的风险，如幻觉。</li>
<li>LLM的退化行为可能与模型在不确定情境下的决策策略有关。</li>
<li>对LLM的进一步优化可能需要考虑如何在不确定情境下平衡模型的决策效率和准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.06071">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6f355bf5359c4004e15fe182a47b94b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d91c687b53df5e5d7046379b589769ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61647c96f395f54551a92bc0007f571d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9664280add2caffaab91bf2b0ba3204c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81bee329c30f4acac336108b85441a78.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CollabStory-Multi-LLM-Collaborative-Story-Generation-and-Authorship-Analysis"><a href="#CollabStory-Multi-LLM-Collaborative-Story-Generation-and-Authorship-Analysis" class="headerlink" title="CollabStory: Multi-LLM Collaborative Story Generation and Authorship   Analysis"></a>CollabStory: Multi-LLM Collaborative Story Generation and Authorship   Analysis</h2><p><strong>Authors:Saranya Venkatraman, Nafis Irtiza Tripto, Dongwon Lee</strong></p>
<p>The rise of unifying frameworks that enable seamless interoperability of Large Language Models (LLMs) has made LLM-LLM collaboration for open-ended tasks a possibility. Despite this, there have not been efforts to explore such collaborative writing. We take the next step beyond human-LLM collaboration to explore this multi-LLM scenario by generating the first exclusively LLM-generated collaborative stories dataset called CollabStory. We focus on single-author to multi-author (up to 5 LLMs) scenarios, where multiple LLMs co-author stories. We generate over 32k stories using open-source instruction-tuned LLMs. Further, we take inspiration from the PAN tasks that have set the standard for human-human multi-author writing tasks and analysis. We extend their authorship-related tasks for multi-LLM settings and present baselines for LLM-LLM collaboration. We find that current baselines are not able to handle this emerging scenario. Thus, CollabStory is a resource that could help propel an understanding as well as the development of new techniques to discern the use of multiple LLMs. This is crucial to study in the context of writing tasks since LLM-LLM collaboration could potentially overwhelm ongoing challenges related to plagiarism detection, credit assignment, maintaining academic integrity in educational settings, and addressing copyright infringement concerns. We make our dataset and code available at <a target="_blank" rel="noopener" href="https://github.com/saranya-venkatraman/CollabStory">https://github.com/saranya-venkatraman/CollabStory</a>. </p>
<blockquote>
<p>随着能够使大型语言模型无缝互操作的统一框架的兴起，开放式任务中的多语言模型协同合作成为可能。然而，人们尚未尝试过探索这种协同写作的方式。我们在人类与语言模型合作的基础上迈出一步，探索这种多语言模型协同合作情景，生成了首个由语言模型生成的协作故事数据集，名为CollabStory。我们关注单人到多人作者（最多五个语言模型）的场景，其中多个语言模型共同创作故事。我们使用开源指令调整的语言模型生成超过三万两千个故事。此外，我们从为多人合作写作任务设定标准的PAN任务中汲取灵感，并将其扩展到多语言模型设置的作者相关任务上，为语言模型的协同合作提供了基准线。我们发现当前的基准线无法应对这一新兴场景。因此，CollabStory可以帮助推进对这种情境的理解，以及开发能够识别多个语言模型使用的新技术。在写任务的环境中研究这一点至关重要，因为语言模型的协同合作可能会带来一些挑战，如抄袭检测、信用分配、维护教育环境中的学术诚信以及解决版权侵权问题。我们的数据集和代码可以在<a target="_blank" rel="noopener" href="https://github.com/saranya-venkatraman/CollabStory%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/saranya-venkatraman/CollabStory中找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12665v3">PDF</a> Accepted to NAACL Findings 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了多大型语言模型（LLM）协同写作的故事数据集CollabStory的生成与探索。重点研究了单个作者到多个作者（最多5个LLM）的场景，其中多个LLM共同创作故事。通过开放源代码指令调整的LLM生成超过32k个故事。同时，从人类人类多作者写作任务的标杆任务PAN中汲取灵感，为多重LLM设置扩展了作者相关任务，并为LLM-LLM协作提供了基准线。研究发现当前基线无法应对这一新兴场景，因此CollabStory资源有助于推进对多个LLM使用理解以及新技术开发，解决写作任务中的挑战，如抄袭检测、学分分配、维护学术诚信以及版权侵权问题等。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）之间的无缝互操作性已成为可能，并生成了首个名为CollabStory的LLM协同创作故事数据集。</li>
<li>CollabStory专注于单一作者到多作者场景，最多包含五个LLM共同创作故事。</li>
<li>生成了超过32k个故事，使用了开源指令调整过的LLM。</li>
<li>从人类多作者写作任务的标杆任务PAN中汲取灵感，为多重LLM设置扩展了作者相关任务。</li>
<li>当前基线无法应对多LLM协作场景，需要新的技术和方法。</li>
<li>CollabStory资源有助于解决写作任务中的挑战，如抄袭检测、维护学术诚信等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.12665">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a5906c4d75b9bb48ffc691ec17ef0281.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14524dfd41ae948e41f4d5fa98bd7ea7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-489b998a5b5f2a29e05a8a2bf0397a0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95de6cc92752fcfd0c90c2f7fce87553.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-020302bc52b16fa2071b2d00778eb4e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f40d0397b393afb37220cd295b943629.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="On-the-Impacts-of-Contexts-on-Repository-Level-Code-Generation"><a href="#On-the-Impacts-of-Contexts-on-Repository-Level-Code-Generation" class="headerlink" title="On the Impacts of Contexts on Repository-Level Code Generation"></a>On the Impacts of Contexts on Repository-Level Code Generation</h2><p><strong>Authors:Nam Le Hai, Dung Manh Nguyen, Nghi D. Q. Bui</strong></p>
<p>CodeLLMs have gained widespread adoption for code generation tasks, yet their capacity to handle repository-level code generation with complex contextual dependencies remains underexplored. Our work underscores the critical importance of leveraging repository-level contexts to generate executable and functionally correct code. We present RepoExec, a novel benchmark designed to evaluate repository-level code generation, with a focus on three key aspects: executability, functional correctness through comprehensive test case generation, and accurate utilization of cross-file contexts. Our study examines a controlled scenario where developers specify essential code dependencies (contexts), challenging models to integrate them effectively. Additionally, we introduce an instruction-tuned dataset that enhances CodeLLMs’ ability to leverage dependencies, along with a new metric, Dependency Invocation Rate (DIR), to quantify context utilization. Experimental results reveal that while pretrained LLMs demonstrate superior performance in terms of correctness, instruction-tuned models excel in context utilization and debugging capabilities. RepoExec offers a comprehensive evaluation framework for assessing code functionality and alignment with developer intent, thereby advancing the development of more reliable CodeLLMs for real-world applications. The dataset and source code are available at <a target="_blank" rel="noopener" href="https://github.com/FSoft-AI4Code/RepoExec">https://github.com/FSoft-AI4Code/RepoExec</a>. </p>
<blockquote>
<p>代码LLM（CodeLLMs）已经在代码生成任务中得到广泛应用，然而它们在处理具有复杂上下文依赖的仓库级别代码生成方面的能力尚未得到充分探索。我们的工作强调了利用仓库级别上下文生成可执行和功能性正确的代码的重要性。我们提出了RepoExec，这是一个用于评估仓库级别代码生成的新型基准测试，重点关注三个方面：可执行性、通过全面的测试用例生成的功能正确性，以及跨文件上下文的准确利用。我们的研究考察了一个开发人员指定关键代码依赖关系（上下文）的受控场景，对模型有效地集成它们提出了挑战。此外，我们引入了一个指令调优数据集，以提高CodeLLM利用依赖项的能力，以及一个新的指标——依赖调用率（DIR），以量化上下文的利用率。实验结果表明，虽然预训练的LLM在正确性方面表现出卓越的性能，但指令调优的模型在上下文利用和调试能力方面表现出色。RepoExec为评估代码功能和与开发人员意图的契合度提供了一个全面的评估框架，从而推动了更可靠的CodeLLM在现实世界应用中的开发。数据集和源代码可在&lt;<a target="_blank" rel="noopener" href="https://github.com/FSoft-AI4Code">https://github.com/FSoft-AI4Code</a> 展开▼Code&gt;上找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11927v4">PDF</a> Accepted to NAACL 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了CodeLLM在处理仓库级别代码生成方面的关键能力，强调了利用仓库级别上下文生成可执行且功能正确的代码的重要性。作者提出了RepoExec基准测试，该测试主要评估仓库级别代码生成的三个方面：可执行性、通过综合测试用例生成的功能正确性，以及准确使用跨文件上下文。文章还介绍了指令微调数据集和新的依赖调用率指标，以量化上下文利用情况。实验结果表明，预训练LLM在正确性方面表现出卓越性能，而指令微调模型在上下文利用和调试能力方面表现更出色。RepoExec提供了一个全面的评估框架，用于评估代码功能和与开发者意图的契合度，推动了更可靠的CodeLLM在现实世界应用中的开发。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CodeLLMs在处理仓库级别代码生成的能力受到关注，但其在处理复杂上下文依赖方面的能力尚未得到充分探索。</li>
<li>RepoExec基准测试被设计用来评估仓库级别代码生成，主要关注可执行性、功能正确性和跨文件上下文的准确使用。</li>
<li>指令微调数据集被介绍来增强CodeLLMs利用依赖关系的能力。</li>
<li>依赖调用率（DIR）是一个新的指标，用于量化上下文的利用情况。</li>
<li>预训练LLM在正确性方面表现出卓越性能，而指令微调模型在上下文利用和调试能力方面更出色。</li>
<li>RepoExec提供了一个全面的评估框架，有助于评估代码功能和与开发者意图的契合度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11927">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a37aa4dc45530a3785e58e634b0ab701.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6fdff72ce7fa6063bcf3d2172daa800c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ad20bd94d6f29956d7f763d8a4145204.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb9b1c3049f5750a7cffdff912252995.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e808b7ae8f032d79be6db8fbcc30453b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Enhancing-Visual-Language-Modality-Alignment-in-Large-Vision-Language-Models-via-Self-Improvement"><a href="#Enhancing-Visual-Language-Modality-Alignment-in-Large-Vision-Language-Models-via-Self-Improvement" class="headerlink" title="Enhancing Visual-Language Modality Alignment in Large Vision Language   Models via Self-Improvement"></a>Enhancing Visual-Language Modality Alignment in Large Vision Language   Models via Self-Improvement</h2><p><strong>Authors:Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Furong Huang, Cao Xiao</strong></p>
<p>Large vision-language models (LVLMs) have achieved impressive results in visual question-answering and reasoning tasks through vision instruction tuning on specific datasets. However, there remains significant room for improvement in aligning visual and language modalities. Existing methods often depend on external models or data, leading to uncontrollable and unstable alignment results. In this paper, we propose SIMA, a self-improvement framework that enhances visual and language modality alignment without external dependencies. SIMA leverages existing vision instruction tuning datasets to self-generate responses, incorporating an in-context self-critic mechanism that constructs preference pairs for tuning. Crucially, our approach allows LVLMs to act as critics by designing effective critic prompts, eliminating the need for additional fine-tuning with external instruction data. We introduce three novel visual metrics within the self-critic process to guide judgment, significantly improving the accuracy of self-critic. Through extensive experiments across 14 hallucination and comprehensive benchmarks, we demonstrate that SIMA significantly improves LVLM’s performance and outperforms previous approaches, achieving superior modality alignment. </p>
<blockquote>
<p>大型视觉语言模型（LVLMs）通过特定数据集上的视觉指令调整，在视觉问答和推理任务中取得了令人印象深刻的结果。然而，在视觉和语言模态对齐方面仍有很大的改进空间。现有方法往往依赖于外部模型或数据，导致对齐结果不可控制和不稳定。在本文中，我们提出了SIMA，一个无需依赖外部因素即可提高视觉和语言模态对齐的自我改进框架。SIMA利用现有的视觉指令调整数据集进行自我生成响应，并结合上下文自我批判机制，构建偏好对进行调整。关键的是，我们的方法允许LVLMs通过设计有效的批评提示来充当批评者，从而无需使用外部指令数据进行额外的微调。我们在自我批判过程中引入了三种新型视觉指标来指导判断，大大提高了自我批判的准确性。通过对1.4个幻觉和全面基准的广泛实验，我们证明了SIMA能显著提高LVLM的性能，并优于以前的方法，实现了卓越的模式对齐。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15973v4">PDF</a> NAACL 2025 Findings</p>
<p><strong>Summary</strong><br>大规模视觉语言模型（LVLM）通过特定数据集上的视觉指令调整，在视觉问答和推理任务中取得了显著成果。然而，仍存在视觉和语言模态对齐的改进空间。现有方法常依赖外部模型或数据，导致对齐结果不可控制和不稳定。本文提出SIMA，一种无需外部依赖的自我改进框架，增强视觉和语言模态对齐。SIMA利用现有视觉指令调整数据集自我生成响应，引入上下文自我批判机制，构建偏好对进行调整。通过设计有效的批判提示，使LVLM成为批判者，无需额外的外部指令数据精细调整。在自我批判过程中引入三项新颖的视觉指标，以指导判断，显著提高自我批判的准确性。经过广泛的实验验证，SIMA显著提高了LVLM的性能，超越了之前的方法，实现了优越的模态对齐。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVLM在视觉问答和推理任务中表现出色，但仍需改进视觉和语言模态的对齐。</li>
<li>现有方法依赖外部模型和数据，导致对齐结果不稳定。</li>
<li>SIMA框架被提出，实现视觉和语言模态的自我改进对齐，无需外部依赖。</li>
<li>SIMA利用视觉指令调整数据集自我生成响应，引入上下文自我批判机制。</li>
<li>LVLM被设计为批判者，通过有效的批判提示进行自我调整，无需额外精细调整。</li>
<li>在自我批判过程中引入三项视觉指标，提高判断的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15973">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e58bc5bd70e10d97b02ff19849d61982.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-880f88645aa88c18d449df8d28b2b49f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da96408765d5f60f1ab4a85640817986.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-689006edca2617e0156d59f55b0595a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed4aa6b104b681873573fda9dcef812c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Alpaca-against-Vicuna-Using-LLMs-to-Uncover-Memorization-of-LLMs"><a href="#Alpaca-against-Vicuna-Using-LLMs-to-Uncover-Memorization-of-LLMs" class="headerlink" title="Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs"></a>Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs</h2><p><strong>Authors:Aly M. Kassem, Omar Mahmoud, Niloofar Mireshghallah, Hyunwoo Kim, Yulia Tsvetkov, Yejin Choi, Sherif Saad, Santu Rana</strong></p>
<p>In this paper, we introduce a black-box prompt optimization method that uses an attacker LLM agent to uncover higher levels of memorization in a victim agent, compared to what is revealed by prompting the target model with the training data directly, which is the dominant approach of quantifying memorization in LLMs. We use an iterative rejection-sampling optimization process to find instruction-based prompts with two main characteristics: (1) minimal overlap with the training data to avoid presenting the solution directly to the model, and (2) maximal overlap between the victim model’s output and the training data, aiming to induce the victim to spit out training data. We observe that our instruction-based prompts generate outputs with 23.7% higher overlap with training data compared to the baseline prefix-suffix measurements. Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, if not more so, (2) contexts other than the original training data can lead to leakage, and (3) using instructions proposed by other LLMs can open a new avenue of automated attacks that we should further study and explore. The code can be found at <a target="_blank" rel="noopener" href="https://github.com/Alymostafa/Instruction_based_attack">https://github.com/Alymostafa/Instruction_based_attack</a> . </p>
<blockquote>
<p>在这篇论文中，我们介绍了一种黑盒提示优化方法，该方法使用攻击者LLM代理来揭示受害者代理中更高层次的记忆，与直接通过训练数据提示目标模型相比，这是目前在LLM中量化记忆的主要方法。我们使用迭代拒绝采样优化过程来找到具有两个主要特征的基于指令的提示：（1）与训练数据的最小重叠，以避免直接向模型呈现解决方案；（2）受害者模型输出与训练数据之间的最大重叠，旨在诱导受害者吐出训练数据。我们观察到，与基准前缀-后缀测量相比，我们的基于指令的提示产生了与训练数据重叠高出23.7%的输出。我们的研究结果表明：（1）指令调整模型可以暴露预训练数据与基础模型一样多，甚至更多；（2）除了原始训练数据以外的上下文也可能导致泄漏；（3）使用其他LLM提出的指令可以为我们的自动化攻击打开一条新途径，值得我们进一步研究和探索。代码可在<a target="_blank" rel="noopener" href="https://github.com/Alymostafa/Instruction_based_attack%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Alymostafa/Instruction_based_attack找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.04801v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种利用攻击型LLM代理发现受害者代理中更高层次记忆的方法。该方法通过迭代拒绝采样优化过程来寻找基于指令的提示，这些提示具有两个主要特点：一是与训练数据的最小重叠，以避免直接向模型呈现解决方案；二是受害者模型输出与训练数据之间的最大重叠，旨在诱导受害者泄露训练数据。研究发现，与基线前缀后缀测量相比，基于指令的提示生成输出的训练数据重叠率提高了23.7%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>引入了一种黑盒提示优化方法，使用攻击型LLM代理来揭示受害者代理中的记忆。</li>
<li>通过迭代拒绝采样优化过程寻找基于指令的提示。</li>
<li>基于指令的提示具有两个特点：与训练数据的最小重叠和受害者模型输出与训练数据的最大重叠。</li>
<li>基于指令的模型可以暴露预训练数据与基础模型相当甚至更多的数据。</li>
<li>除原始训练数据外的上下文也可能导致泄露。</li>
<li>使用其他LLM提出的指令可以开启新的自动化攻击途径，需要进一步研究和探索。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.04801">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-21e4f91c7df734c01a1d9953037c06a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f625dd0d21354c30b103661b6de2112.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7164f3adcba4cce04ea4b40aae6254db.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-14\./crop_LLM/2403.04801v3/page_4_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-815218ba364ff5f67cf21de10b2f485e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-14/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-14/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-14/Speech/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-471d015769f14450ab3948258e259f0f.jpg" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech 方向最新论文已更新，请持续关注 Update in 2025-02-14  Contextual Gesture Co-Speech Gesture Video Generation through   Context-aware Gesture Representation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-64ade2d79e4c962dfe3f417da7c73ff1.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2025-02-13  What Is That Talk About? A Video-to-Text Summarization Dataset for   Scientific Presentations
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25243.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
