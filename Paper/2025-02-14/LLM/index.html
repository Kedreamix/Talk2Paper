<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-14  HoVLE Unleashing the Power of Monolithic Vision-Language Models with   Holistic Vision-Language Embedding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-95de6cc92752fcfd0c90c2f7fce87553.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    54 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-14-æ›´æ–°"><a href="#2025-02-14-æ›´æ–°" class="headerlink" title="2025-02-14 æ›´æ–°"></a>2025-02-14 æ›´æ–°</h1><h2 id="HoVLE-Unleashing-the-Power-of-Monolithic-Vision-Language-Models-with-Holistic-Vision-Language-Embedding"><a href="#HoVLE-Unleashing-the-Power-of-Monolithic-Vision-Language-Models-with-Holistic-Vision-Language-Embedding" class="headerlink" title="HoVLE: Unleashing the Power of Monolithic Vision-Language Models with   Holistic Vision-Language Embedding"></a>HoVLE: Unleashing the Power of Monolithic Vision-Language Models with   Holistic Vision-Language Embedding</h2><p><strong>Authors:Chenxin Tao, Shiqian Su, Xizhou Zhu, Chenyu Zhang, Zhe Chen, Jiawen Liu, Wenhai Wang, Lewei Lu, Gao Huang, Yu Qiao, Jifeng Dai</strong></p>
<p>The rapid advance of Large Language Models (LLMs) has catalyzed the development of Vision-Language Models (VLMs). Monolithic VLMs, which avoid modality-specific encoders, offer a promising alternative to the compositional ones but face the challenge of inferior performance. Most existing monolithic VLMs require tuning pre-trained LLMs to acquire vision abilities, which may degrade their language capabilities. To address this dilemma, this paper presents a novel high-performance monolithic VLM named HoVLE. We note that LLMs have been shown capable of interpreting images, when image embeddings are aligned with text embeddings. The challenge for current monolithic VLMs actually lies in the lack of a holistic embedding module for both vision and language inputs. Therefore, HoVLE introduces a holistic embedding module that converts visual and textual inputs into a shared space, allowing LLMs to process images in the same way as texts. Furthermore, a multi-stage training strategy is carefully designed to empower the holistic embedding module. It is first trained to distill visual features from a pre-trained vision encoder and text embeddings from the LLM, enabling large-scale training with unpaired random images and text tokens. The whole model further undergoes next-token prediction on multi-modal data to align the embeddings. Finally, an instruction-tuning stage is incorporated. Our experiments show that HoVLE achieves performance close to leading compositional models on various benchmarks, outperforming previous monolithic models by a large margin. Model available at <a target="_blank" rel="noopener" href="https://huggingface.co/OpenGVLab/HoVLE">https://huggingface.co/OpenGVLab/HoVLE</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¿›æ­¥ã€‚ä¸€ä½“å¼VLMé¿å…äº†æ¨¡æ€ç‰¹å®šç¼–ç å™¨ï¼Œä¸ºç»„åˆå¼VLMæä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†é¢ä¸´æ€§èƒ½è¾ƒå·®çš„æŒ‘æˆ˜ã€‚å¤§å¤šæ•°ç°æœ‰çš„ä¸€ä½“å¼VLMéœ€è¦è°ƒæ•´é¢„è®­ç»ƒçš„LLMä»¥è·å¾—è§†è§‰èƒ½åŠ›ï¼Œè¿™å¯èƒ½ä¼šé™ä½å…¶è¯­è¨€åŠŸèƒ½ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å›°å¢ƒï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹é«˜æ€§èƒ½ä¸€ä½“å¼VLMï¼Œåä¸ºHoVLEã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œå½“å›¾åƒåµŒå…¥ä¸æ–‡æœ¬åµŒå…¥å¯¹é½æ—¶ï¼ŒLLMå·²è¢«è¯æ˜èƒ½å¤Ÿè§£é‡Šå›¾åƒã€‚å½“å‰ä¸€ä½“å¼VLMçš„æŒ‘æˆ˜å®é™…ä¸Šåœ¨äºç¼ºä¹ä¸€ä¸ªç”¨äºè§†è§‰å’Œè¯­è¨€è¾“å…¥çš„å…¨é¢åµŒå…¥æ¨¡å—ã€‚å› æ­¤ï¼ŒHoVLEå¼•å…¥äº†ä¸€ä¸ªå…¨é¢åµŒå…¥æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºå…±äº«ç©ºé—´ï¼Œä½¿LLMèƒ½å¤Ÿä»¥ç›¸åŒçš„æ–¹å¼å¤„ç†å›¾åƒå’Œæ–‡æœ¬ã€‚æ­¤å¤–ï¼Œç²¾å¿ƒè®¾è®¡äº†ä¸€ç§å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥æ¥å¢å¼ºå…¨é¢åµŒå…¥æ¨¡å—ã€‚å®ƒé¦–å…ˆç»è¿‡è®­ç»ƒï¼Œä»é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å’ŒLLMä¸­æç‚¼å‡ºè§†è§‰ç‰¹å¾å’Œæ–‡æœ¬åµŒå…¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä½¿ç”¨å¤§é‡çš„æœªé…å¯¹éšæœºå›¾åƒå’Œæ–‡æœ¬æ ‡è®°è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒã€‚æ•´ä¸ªæ¨¡å‹è¿›ä¸€æ­¥è¿›è¡Œå¤šæ¨¡æ€æ•°æ®çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ï¼Œä»¥å¯¹é½åµŒå…¥ã€‚æœ€åï¼ŒåŠ å…¥äº†æŒ‡ä»¤è°ƒæ•´é˜¶æ®µã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒHoVLEåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ¥è¿‘é¢†å…ˆç»„åˆæ¨¡å‹çš„è¡¨ç°ï¼Œå¤§å¤§è¶…è¿‡äº†ä»¥å‰çš„ä¸€ä½“å¼æ¨¡å‹ã€‚æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/OpenGVLab/HoVLE%E4%B8%8B%E8%BD%BD%E4%BD%BF%E7%94%A8%E3%80%82">https://huggingface.co/OpenGVLab/HoVLEä¸‹è½½ä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16158v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„è¿›æ­¥ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹é«˜æ€§èƒ½å•ä½“VLMâ€”â€”HoVLEï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å•ä½“VLMéœ€è¦åœ¨é¢„è®­ç»ƒLLMä¸Šè°ƒæ•´ä»¥è·å–è§†è§‰èƒ½åŠ›è€Œå¯¼è‡´çš„è¯­è¨€èƒ½åŠ›ä¸‹é™çš„é—®é¢˜ã€‚HoVLEå¼•å…¥äº†ä¸€ä¸ªæ•´ä½“åµŒå…¥æ¨¡å—ï¼Œå°†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºå…±äº«ç©ºé—´ï¼Œä½¿LLMèƒ½å¤Ÿä»¥ç›¸åŒçš„æ–¹å¼å¤„ç†å›¾åƒå’Œæ–‡æœ¬ã€‚é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥èµ‹èƒ½æ•´ä½“åµŒå…¥æ¨¡å—ï¼Œé¦–å…ˆè¿›è¡Œé¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨å’ŒLLMçš„æ–‡æœ¬åµŒå…¥å¯¹é½ï¼Œç„¶åè¿›è¡Œå¤§è§„æ¨¡æœªé…å¯¹éšæœºå›¾åƒå’Œæ–‡æœ¬æ ‡è®°çš„è®­ç»ƒï¼Œæœ€åè¿›è¡Œå¤šæ¨¡æ€æ•°æ®çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ä»¥å¯¹é½åµŒå…¥ã€‚å®éªŒè¡¨æ˜ï¼ŒHoVLEåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½æ¥è¿‘é¢†å…ˆçš„ç»„åˆæ¨¡å‹ï¼Œå¹¶ä¸”å¤§å¤§è¶…è¿‡äº†ä»¥å‰çš„å•ä½“æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMçš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†VLMçš„è¿›æ­¥ï¼Œå•ä½“VLMä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆå‡ºç°ï¼Œæ—¨åœ¨è§£å†³ç»„åˆæ¨¡å‹çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰å•ä½“VLMéœ€è¦åœ¨é¢„è®­ç»ƒçš„LLMä¸Šè°ƒæ•´ä»¥è·å–è§†è§‰èƒ½åŠ›ï¼Œå¯èƒ½å¯¼è‡´è¯­è¨€èƒ½åŠ›çš„é€€åŒ–ã€‚</li>
<li>HoVLEé€šè¿‡å¼•å…¥æ•´ä½“åµŒå…¥æ¨¡å—ï¼Œå°†è§†è§‰å’Œæ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºå…±äº«ç©ºé—´ï¼Œä½¿LLMèƒ½åŒæ—¶å¤„ç†å›¾åƒå’Œæ–‡æœ¬ã€‚</li>
<li>HoVLEé‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒè§†è§‰ç¼–ç å™¨å’ŒLLMçš„æ–‡æœ¬åµŒå…¥å¯¹é½ã€å¤§è§„æ¨¡æœªé…å¯¹éšæœºå›¾åƒå’Œæ–‡æœ¬æ ‡è®°çš„è®­ç»ƒã€ä»¥åŠå¤šæ¨¡æ€æ•°æ®çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹ã€‚</li>
<li>HoVLEåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½æ¥è¿‘é¢†å…ˆçš„ç»„åˆæ¨¡å‹ï¼Œä¸”å¤§å¹…è¶…è¶Šäº†ä¹‹å‰çš„å•ä½“æ¨¡å‹ã€‚</li>
<li>HoVLEæ¨¡å‹å¯åœ¨huggingface.co&#x2F;OpenGVLab&#x2F;HoVLEä¸Šè·å–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16158">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7d8f0eb7e1984ee8e43b6c4cc638615b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-282cd19ee053d426e5026824038d06e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e058202889054595b27c1343b7b44a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6605b3a25999c28f229189dd7b69313.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Filter-then-Generate-Large-Language-Models-with-Structure-Text-Adapter-for-Knowledge-Graph-Completion"><a href="#Filter-then-Generate-Large-Language-Models-with-Structure-Text-Adapter-for-Knowledge-Graph-Completion" class="headerlink" title="Filter-then-Generate: Large Language Models with Structure-Text Adapter   for Knowledge Graph Completion"></a>Filter-then-Generate: Large Language Models with Structure-Text Adapter   for Knowledge Graph Completion</h2><p><strong>Authors:Ben Liu, Jihai Zhang, Fangquan Lin, Cheng Yang, Min Peng</strong></p>
<p>Large Language Models (LLMs) present massive inherent knowledge and superior semantic comprehension capability, which have revolutionized various tasks in natural language processing. Despite their success, a critical gap remains in enabling LLMs to perform knowledge graph completion (KGC). Empirical evidence suggests that LLMs consistently perform worse than conventional KGC approaches, even through sophisticated prompt design or tailored instruction-tuning. Fundamentally, applying LLMs on KGC introduces several critical challenges, including a vast set of entity candidates, hallucination issue of LLMs, and under-exploitation of the graph structure. To address these challenges, we propose a novel instruction-tuning-based method, namely FtG. Specifically, we present a filter-then-generate paradigm and formulate the KGC task into a multiple-choice question format. In this way, we can harness the capability of LLMs while mitigating the issue casused by hallucinations. Moreover, we devise a flexible ego-graph serialization prompt and employ a structure-text adapter to couple structure and text information in a contextualized manner. Experimental results demonstrate that FtG achieves substantial performance gain compared to existing state-of-the-art methods. The instruction dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/LB0828/FtG">https://github.com/LB0828/FtG</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‹¥æœ‰åºå¤§çš„å†…åœ¨çŸ¥è¯†å’Œå‡ºè‰²çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œå·²ç»å½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†çš„å„é¡¹ä»»åŠ¡ã€‚å°½ç®¡å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ–¹é¢ï¼ŒLLMä»å­˜åœ¨æ˜æ˜¾ä¸è¶³ã€‚ç»éªŒè¯æ®è¡¨æ˜ï¼Œå³ä½¿åœ¨é€šè¿‡å¤æ‚æç¤ºè®¾è®¡æˆ–é’ˆå¯¹æ€§æŒ‡ä»¤è°ƒæ•´çš„æƒ…å†µä¸‹ï¼ŒLLMçš„è¡¨ç°ä»ä¸€ç›´é€Šäºä¼ ç»Ÿçš„KGCæ–¹æ³•ã€‚ä»æ ¹æœ¬ä¸Šè¯´ï¼Œå°†LLMåº”ç”¨äºKGCé¢ä¸´ç€å‡ ä¸ªå…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤§é‡çš„å®ä½“å€™é€‰å¯¹è±¡ã€LLMçš„å¹»è§‰é—®é¢˜ä»¥åŠå¯¹å›¾ç»“æ„åˆ©ç”¨ä¸è¶³ç­‰ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæŒ‡ä»¤è°ƒæ•´çš„æ–°æ–¹æ³•ï¼Œç§°ä¸ºFtGã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªâ€œè¿‡æ»¤ç„¶åç”Ÿæˆâ€çš„æ¨¡å¼ï¼Œå¹¶å°†KGCä»»åŠ¡è½¬åŒ–ä¸ºä¸€ä¸ªé€‰æ‹©é¢˜æ ¼å¼ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨LLMçš„èƒ½åŠ›ï¼ŒåŒæ—¶å‡è½»å¹»è§‰é—®é¢˜å¸¦æ¥çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªçµæ´»çš„è‡ªæˆ‘å›¾åºåˆ—åŒ–æç¤ºï¼Œå¹¶é‡‡ç”¨äº†ç»“æ„æ–‡æœ¬é€‚é…å™¨ï¼Œä»¥è¯­å¢ƒåŒ–çš„æ–¹å¼å°†ç»“æ„å’Œæ–‡æœ¬ä¿¡æ¯è¿›è¡Œç»“åˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒFtGå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æŒ‡ä»¤æ•°æ®é›†å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/LB0828/FtG%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/LB0828/FtGè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09094v3">PDF</a> COLING 2025 Main Conference</p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·å¤‡ä¸°å¯Œçš„å†…åœ¨çŸ¥è¯†å’Œé«˜çº§è¯­ä¹‰ç†è§£èƒ½ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†çš„å„ç§ä»»åŠ¡ä¸­å¸¦æ¥äº†é©å‘½æ€§çš„å˜åŒ–ã€‚ç„¶è€Œï¼Œåœ¨çŸ¥è¯†å›¾è°±è¡¥å…¨ï¼ˆKGCï¼‰æ–¹é¢ï¼ŒLLMçš„åº”ç”¨ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚å°½ç®¡é€šè¿‡å·§å¦™çš„æç¤ºè®¾è®¡æˆ–æŒ‡ä»¤å¾®è°ƒæ¥ä¼˜åŒ–ï¼ŒLLMçš„è¡¨ç°ä»é€Šäºä¼ ç»ŸKGCæ–¹æ³•ã€‚ä¸ºå®ç°LLMåœ¨KGCä»»åŠ¡ä¸­çš„æœ‰æ•ˆåº”ç”¨ï¼Œæˆ‘ä»¬é¢ä¸´å®ä½“å€™é€‰é›†åºå¤§ã€LLMçš„è™šæ„é—®é¢˜ä»¥åŠå›¾å½¢ç»“æ„åˆ©ç”¨ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæŒ‡ä»¤è°ƒä¼˜çš„æ–¹æ³•FtGï¼Œé‡‡ç”¨å…ˆè¿‡æ»¤åç”Ÿæˆçš„èŒƒå¼ï¼Œå°†KGCä»»åŠ¡è½¬åŒ–ä¸ºå¤šé€‰é—®é¢˜å½¢å¼ï¼Œä»è€Œåˆ©ç”¨LLMçš„ä¼˜åŠ¿å¹¶å‡è½»è™šæ„é—®é¢˜çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†çµæ´»çš„è‡ªæˆ‘å›¾è°±åºåˆ—åŒ–æç¤ºï¼Œå¹¶é‡‡ç”¨ç»“æ„æ–‡æœ¬é€‚é…å™¨ä»¥è¯­å¢ƒåŒ–çš„æ–¹å¼ç»“åˆç»“æ„å’Œæ–‡æœ¬ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFtGç›¸è¾ƒäºç°æœ‰å…ˆè¿›æ–¹æ³•å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMå…·å¤‡å¼ºå¤§çš„å†…åœ¨çŸ¥è¯†å’Œè¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œä½†åœ¨çŸ¥è¯†å›¾è°±è¡¥å…¨ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚</li>
<li>LLMåœ¨KGCé¢ä¸´å®ä½“å€™é€‰é›†åºå¤§ã€è™šæ„é—®é¢˜ä»¥åŠå›¾å½¢ç»“æ„åˆ©ç”¨ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•FtGï¼ŒåŸºäºæŒ‡ä»¤è°ƒä¼˜ï¼Œé‡‡ç”¨å…ˆè¿‡æ»¤åç”Ÿæˆçš„èŒƒå¼å¤„ç†KGCä»»åŠ¡ã€‚</li>
<li>FtGå°†KGCä»»åŠ¡è½¬åŒ–ä¸ºå¤šé€‰é—®é¢˜å½¢å¼ï¼Œå‡è½»LLMçš„è™šæ„é—®é¢˜ã€‚</li>
<li>FtGè®¾è®¡äº†çµæ´»çš„è‡ªæˆ‘å›¾è°±åºåˆ—åŒ–æç¤ºï¼Œå¹¶ç»“åˆç»“æ„å’Œæ–‡æœ¬ä¿¡æ¯ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒFtGåœ¨KGCä»»åŠ¡ä¸Šè¾ƒç°æœ‰æ–¹æ³•æœ‰æ˜æ˜¾æ€§èƒ½æå‡ã€‚</li>
<li>FtGçš„æŒ‡ä»¤æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09094">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3aedb45c0983cfb1d157a22b6cdb2e8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e631baf4c0f5193d3c9a4c22d055e29.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb0c88c965b9abf65e9d9c1ee53a680d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c37c0d21abe441fac0f95ba65f21c6d8.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TimeSuite-Improving-MLLMs-for-Long-Video-Understanding-via-Grounded-Tuning"><a href="#TimeSuite-Improving-MLLMs-for-Long-Video-Understanding-via-Grounded-Tuning" class="headerlink" title="TimeSuite: Improving MLLMs for Long Video Understanding via Grounded   Tuning"></a>TimeSuite: Improving MLLMs for Long Video Understanding via Grounded   Tuning</h2><p><strong>Authors:Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, Yali Wang, Yu Qiao, Limin Wang</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated impressive performance in short video understanding. However, understanding long-form videos still remains challenging for MLLMs. This paper proposes TimeSuite, a collection of new designs to adapt the existing short-form video MLLMs for long video understanding, including a simple yet efficient framework to process long video sequence, a high-quality video dataset for grounded tuning of MLLMs, and a carefully-designed instruction tuning task to explicitly incorporate the grounding supervision in the traditional QA format. Specifically, based on VideoChat, we propose our long-video MLLM, coined as VideoChat-T, by implementing a token shuffling to compress long video tokens and introducing Temporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of visual representation. Meanwhile, we introduce the TimePro, a comprehensive grounding-centric instruction tuning dataset composed of 9 tasks and 349k high-quality grounded annotations. Notably, we design a new instruction tuning task type, called Temporal Grounded Caption, to peform detailed video descriptions with the corresponding time stamps prediction. This explicit temporal location prediction will guide MLLM to correctly attend on the visual content when generating description, and thus reduce the hallucination risk caused by the LLMs. Experimental results demonstrate that our TimeSuite provides a successful solution to enhance the long video understanding capability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the benchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T exhibits robust zero-shot temporal grounding capabilities, significantly outperforming the existing state-of-the-art MLLMs. After fine-tuning, it performs on par with the traditional supervised expert models. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨çŸ­è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹äºMLLMsæ¥è¯´ï¼Œç†è§£é•¿è§†é¢‘ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡é’ˆå¯¹ç°æœ‰çŸ­è§†é¢‘MLLMsçš„é•¿è§†é¢‘ç†è§£é€‚åº”æ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†TimeSuiteæ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä¸€ä¸ªç®€å•è€Œé«˜æ•ˆçš„é•¿è§†é¢‘åºåˆ—å¤„ç†æ¡†æ¶ã€ä¸€ä¸ªç”¨äºMLLMsç²¾å‡†è°ƒæ•´çš„é«˜è´¨é‡è§†é¢‘æ•°æ®é›†ä»¥åŠä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡æŒ‡ä»¤è°ƒæ•´ä»»åŠ¡ï¼Œä»¥æ˜ç¡®å°†ä¼ ç»Ÿçš„é—®ç­”æ ¼å¼ä¸­çš„åŸºç¡€ç›‘ç£çº³å…¥å…¶ä¸­ã€‚å…·ä½“æ¥è¯´ï¼ŒåŸºäºVideoChatï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºVideoChat-Tçš„é•¿è§†é¢‘MLLMï¼Œå®ƒé€šè¿‡å®ç°ä»¤ç‰Œæ··æ´—æ¥å‹ç¼©é•¿è§†é¢‘ä»¤ç‰Œå¹¶å¼•å…¥æ—¶é—´è‡ªé€‚åº”ä½ç½®ç¼–ç ï¼ˆTAPEï¼‰æ¥æé«˜è§†è§‰è¡¨ç¤ºçš„æ—¶é—´æ„è¯†ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†TimeProï¼Œè¿™æ˜¯ä¸€ä¸ªå…¨é¢çš„ä»¥åŸºç¡€ä¸ºä¸­å¿ƒçš„ä»»åŠ¡æŒ‡ä»¤æ•°æ®é›†ï¼ŒåŒ…å«9ä¸ªä»»åŠ¡å’Œ34.9ä¸‡é«˜è´¨é‡çš„åŸºç¡€æ³¨é‡Šã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„ä»»åŠ¡æŒ‡ä»¤ç±»å‹ï¼Œç§°ä¸ºæ—¶é—´åŸºç¡€å­—å¹•ï¼Œä»¥è¿›è¡Œè¯¦ç»†çš„è§†é¢‘æè¿°å’Œç›¸åº”çš„æ—¶é—´æˆ³é¢„æµ‹ã€‚è¿™ç§æ˜ç¡®çš„æ—¶é—´ä½ç½®é¢„æµ‹å°†æŒ‡å¯¼MLLMåœ¨ç”Ÿæˆæè¿°æ—¶æ­£ç¡®å…³æ³¨è§†è§‰å†…å®¹ï¼Œä»è€Œé™ä½LLMäº§ç”Ÿçš„å¹»è§‰é£é™©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„TimeSuiteæ–¹æ¡ˆæˆåŠŸåœ°æé«˜äº†çŸ­è§†é¢‘MLLMå¯¹é•¿è§†é¢‘çš„ç†è§£èƒ½åŠ›ï¼Œåœ¨Egoschemaå’ŒVideoMMEåŸºå‡†æµ‹è¯•ä¸Šçš„æ”¹è¿›åˆ†åˆ«ä¸º5.6%å’Œ6.8%ã€‚æ­¤å¤–ï¼ŒVideoChat-Tå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ—¶é—´åŸºç¡€èƒ½åŠ›ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›çš„MLLMsã€‚ç»è¿‡å¾®è°ƒåï¼Œå…¶æ€§èƒ½ä¸ä¼ ç»Ÿçš„ç›‘ç£ä¸“å®¶æ¨¡å‹ç›¸å½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.19702v2">PDF</a> Accepted by ICLR2025</p>
<p><strong>Summary</strong></p>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œç ”ç©¶è€…æå‡ºäº†TimeSuiteæ–¹æ¡ˆï¼Œé€šè¿‡è®¾è®¡æ–°é¢–çš„æ–¹æ³•å’Œæ¡†æ¶æ¥è§£å†³é•¿è§†é¢‘ç†è§£é—®é¢˜ã€‚å…¶ä¸­æå‡ºäº†åŸºäºVideoChatçš„VideoChat-Tæ¨¡å‹ï¼Œä½¿ç”¨æ ‡è®°ç½®æ¢æŠ€æœ¯å’Œæ—¶é—´è‡ªé€‚åº”ä½ç½®ç¼–ç ï¼ˆTAPEï¼‰å¢å¼ºæ¨¡å‹å¯¹é•¿è§†é¢‘çš„æ„ŸçŸ¥èƒ½åŠ›ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†TimeProæ•°æ®é›†ï¼ŒåŒ…å«å¤šç§ä»»åŠ¡ç±»å‹ä¸æ ‡æ³¨ä¿¡æ¯ç”¨äºæå‡æ¨¡å‹æ€§èƒ½ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºTimeSuiteæ–¹æ³•æœ‰æ•ˆæé«˜äº†é•¿è§†é¢‘ç†è§£æ•ˆæœï¼ŒæˆåŠŸæ‰©å±•äº†åŸæœ‰æ¨¡å‹æ€§èƒ½ï¼Œç›¸è¾ƒäºç°æœ‰æ¨¡å‹å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TimeSuiteæ–¹æ¡ˆæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿è§†é¢‘ç†è§£ä¸Šçš„æŒ‘æˆ˜ã€‚</li>
<li>VideoChat-Tæ¨¡å‹é€šè¿‡æ ‡è®°ç½®æ¢å’Œæ—¶é—´è‡ªé€‚åº”ä½ç½®ç¼–ç æŠ€æœ¯å®ç°é•¿è§†é¢‘ç†è§£ã€‚</li>
<li>TimeProæ•°æ®é›†ä¸ºMLLMæä¾›äº†å…¨é¢è€Œå‡†ç¡®çš„ä»»åŠ¡å¯¼å‘æ ‡æ³¨æ•°æ®ï¼Œç”¨ä»¥æŒ‡å¯¼æ¨¡å‹çš„è®­ç»ƒå’Œæµ‹è¯•ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§åä¸ºTemporal Grounded Captionçš„æ–°ä»»åŠ¡ç±»å‹ï¼Œç”¨äºé¢„æµ‹æ—¶é—´æˆ³å¹¶å‡å°‘æ¨¡å‹äº§ç”Ÿçš„å¹»è§‰é£é™©ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.19702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c0fbfa33971e5434cf1d9e7e23c7e52c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e691accdb034b7c00206e5b2639f7fe6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a311d5a8610845b8d3ace50ee054b25d.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Diffusion-Transformer-Policy-Scaling-Diffusion-Transformer-for-Generalist-Visual-Language-Action-Learning"><a href="#Diffusion-Transformer-Policy-Scaling-Diffusion-Transformer-for-Generalist-Visual-Language-Action-Learning" class="headerlink" title="Diffusion Transformer Policy: Scaling Diffusion Transformer for   Generalist Visual-Language-Action Learning"></a>Diffusion Transformer Policy: Scaling Diffusion Transformer for   Generalist Visual-Language-Action Learning</h2><p><strong>Authors:Zhi Hou, Tianyi Zhang, Yuwen Xiong, Hengjun Pu, Chengyang Zhao, Ronglei Tong, Yu Qiao, Jifeng Dai, Yuntao Chen</strong></p>
<p>Recent large visual-language action models pretrained on diverse robot datasets have demonstrated the potential for generalizing to new environments with a few in-domain data. However, those approaches usually predict individual discretized or continuous action by a small action head, which limits the ability in handling diverse action spaces. In contrast, we model the continuous action sequence with a large multi-modal diffusion transformer, dubbed as Diffusion Transformer Policy, in which we directly denoise action chunks by a large transformer model rather than a small action head for action embedding. By leveraging the scaling capability of transformers, the proposed approach can effectively model continuous end-effector actions across large diverse robot datasets, and achieve better generalization performance. Extensive experiments demonstrate the effectiveness and generalization of Diffusion Transformer Policy on Maniskill2, Libero, Calvin and SimplerEnv, as well as the real-world Franka arm, achieving consistent better performance on Real-to-Sim benchmark SimplerEnv, real-world Franka Arm and Libero compared to OpenVLA and Octo. Specifically, without bells and whistles, the proposed approach achieves state-of-the-art performance with only a single third-view camera stream in the Calvin task ABC-&gt;D, improving the average number of tasks completed in a row of 5 to 3.6, and the pretraining stage significantly facilitates the success sequence length on the Calvin by over 1.2. Project Page: <a target="_blank" rel="noopener" href="https://zhihou7.github.io/dit_policy_vla/">https://zhihou7.github.io/dit_policy_vla/</a> </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è§†è§‰è¯­è¨€è¡ŒåŠ¨æ¨¡å‹åœ¨å¤šç§æœºå™¨äººæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶å·²è¯æ˜å…¶åœ¨å°‘é‡é¢†åŸŸå†…æ•°æ®ä¸‹é€‚åº”æ–°ç¯å¢ƒçš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸é€šè¿‡å°å‹åŠ¨ä½œå¤´é¢„æµ‹ä¸ªä½“ç¦»æ•£æˆ–è¿ç»­åŠ¨ä½œï¼Œè¿™é™åˆ¶äº†å¤„ç†å¤šæ ·åŠ¨ä½œç©ºé—´çš„èƒ½åŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤§å‹å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨æ¥æ¨¡æ‹Ÿè¿ç»­åŠ¨ä½œåºåˆ—ï¼Œç§°ä¸ºæ‰©æ•£å˜å‹å™¨ç­–ç•¥ã€‚åœ¨è¯¥ç­–ç•¥ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå¤§å‹å˜å‹å™¨æ¨¡å‹ç›´æ¥å¯¹åŠ¨ä½œå—è¿›è¡Œå»å™ªï¼Œè€Œä¸æ˜¯é€šè¿‡å°å‹åŠ¨ä½œå¤´è¿›è¡ŒåŠ¨ä½œåµŒå…¥ã€‚é€šè¿‡åˆ©ç”¨å˜å‹å™¨çš„å¯æ‰©å±•èƒ½åŠ›ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°æ¨¡æ‹Ÿè·¨è¶Šå¤§å‹å¤šæ ·æœºå™¨äººæ•°æ®é›†çš„è¿ç»­æœ«ç«¯æ‰§è¡Œå™¨åŠ¨ä½œï¼Œå¹¶å®ç°äº†æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰©æ•£å˜å‹å™¨ç­–ç•¥åœ¨Maniskill2ã€Liberoã€Calvinå’ŒSimplerEnvç­‰å¤šä¸ªæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–æ€§ã€‚æ­¤å¤–ï¼Œä¸OpenVLAå’ŒOctoç›¸æ¯”ï¼Œå®ƒåœ¨ç°å®ä¸–ç•Œçš„Frankaæ‰‹è‡‚ã€Liberoä»¥åŠSimplerEnvç­‰åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°æ›´å‡ºè‰²ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ä¸ä½¿ç”¨ä»»ä½•èŠ±å“¨æŠ€å·§çš„æƒ…å†µä¸‹ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä»…åœ¨Calvinä»»åŠ¡çš„ABC-&gt;Dä¸­ä½¿ç”¨å•ä¸€ç¬¬ä¸‰è§†è§’ç›¸æœºæµå³å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå°†è¿ç»­å®Œæˆä»»åŠ¡çš„å¹³å‡æ•°é‡ä»5æé«˜åˆ°3.6ã€‚é¢„è®­ç»ƒé˜¶æ®µæ˜¾è‘—å¢åŠ äº†Calvinä»»åŠ¡çš„æˆåŠŸåºåˆ—é•¿åº¦è¶…è¿‡1.2ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://zhihou7.github.io/dit_policy_vla/">https://zhihou7.github.io/dit_policy_vla/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.15959v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä½¿ç”¨å¤§å‹å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆDiffusion Transformer Policyï¼‰å¯¹è¿ç»­åŠ¨ä½œåºåˆ—è¿›è¡Œå»ºæ¨¡çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•ç›´æ¥é€šè¿‡å¤§å‹å˜å‹å™¨æ¨¡å‹å»å™ªåŠ¨ä½œå—ï¼Œè€Œä¸æ˜¯é€šè¿‡å°åŠ¨ä½œå¤´è¿›è¡ŒåŠ¨ä½œåµŒå…¥ã€‚åˆ©ç”¨å˜å‹å™¨çš„å¯æ‰©å±•æ€§ï¼Œè¯¥æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°å¯¹å¤§å‹å¤šæ ·æœºå™¨äººæ•°æ®é›†è¿›è¡Œè¿ç»­æœ«ç«¯æ‰§è¡Œå™¨åŠ¨ä½œå»ºæ¨¡ï¼Œå¹¶å®ç°äº†æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæœºå™¨äººä»»åŠ¡ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä½¿ç”¨å¤§å‹å¤šæ¨¡æ€æ‰©æ•£å˜å‹å™¨ï¼ˆDiffusion Transformer Policyï¼‰å¤„ç†æœºå™¨äººè¿ç»­åŠ¨ä½œåºåˆ—çš„æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å¤§å‹å˜å‹å™¨æ¨¡å‹ç›´æ¥å¯¹åŠ¨ä½œå—è¿›è¡Œå»å™ªï¼Œè€Œéä½¿ç”¨å°åŠ¨ä½œå¤´è¿›è¡ŒåŠ¨ä½œåµŒå…¥ã€‚</li>
<li>å˜å‹å™¨æ¨¡å‹çš„æ‰©å±•æ€§ä½¿å¾—æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å¯¹å¤§å‹å¤šæ ·çš„æœºå™¨äººæ•°æ®é›†è¿›è¡Œå»ºæ¨¡ã€‚</li>
<li>æ–¹æ³•å®ç°äº†æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œåœ¨å¤šä¸ªæœºå™¨äººä»»åŠ¡ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>åœ¨Real-to-SimåŸºå‡†æµ‹è¯•SimplerEnvã€çœŸå®ä¸–ç•Œçš„Frankaæ‰‹è‡‚å’ŒLiberoä»»åŠ¡ä¸Šï¼Œè¯¥æ–¹æ³•ç›¸è¾ƒäºOpenVLAå’ŒOctoå…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>åœ¨Calvinä»»åŠ¡çš„ABC-&gt;Dä¸­ï¼Œè¯¥æ–¹æ³•ä»…ä½¿ç”¨å•ç¬¬ä¸‰äººç§°è§†è§’ç›¸æœºæµå³å®ç°äº†å…ˆè¿›æ€§èƒ½ï¼Œå®Œæˆäº†å¹³å‡ä»»åŠ¡è¡Œæ•°ä»5åˆ°3.6çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.15959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3954fd59d22b86cb46cfbb71920af2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00a652f829a46d1224ec081fdfa93a59.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a48abcee3297f1afd349d62ba9241b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-338b40674e9e90bfe793622a8441e73b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-83ca834836b706621f4326e23adfbacb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1368f54c1ebf7e7419c4451fc07e173.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TimeDiT-General-purpose-Diffusion-Transformers-for-Time-Series-Foundation-Model"><a href="#TimeDiT-General-purpose-Diffusion-Transformers-for-Time-Series-Foundation-Model" class="headerlink" title="TimeDiT: General-purpose Diffusion Transformers for Time Series   Foundation Model"></a>TimeDiT: General-purpose Diffusion Transformers for Time Series   Foundation Model</h2><p><strong>Authors:Defu Cao, Wen Ye, Yizhou Zhang, Yan Liu</strong></p>
<p>Foundation models, particularly Large Language Models (LLMs), have revolutionized text and video processing, yet time series data presents distinct challenges for such approaches due to domain-specific features such as missing values, multi-resolution characteristics, etc. Furthermore, the de-facto autoregressive transformers tend to learn deterministic temporal dependencies within pre-trained data while overlooking inherent uncertainties and lacking integration of physical constraints. In this paper, we introduce TimeDiT, a diffusion transformer model that synergistically combines transformer-based temporal dependency learning with diffusion-based probabilistic sampling. TimeDiT employs a unified masking mechanism to harmonize the training and inference process across diverse tasks while introducing a theoretically grounded, finetuning-free model editing strategy that enables flexible integration of external knowledge during sampling. Acknowledging the challenges of unifying multiple downstream tasks under a single model, our systematic evaluation demonstrates TimeDiTâ€™s effectiveness both in fundamental tasks, i.e., forecasting and imputation, through zero-shot&#x2F;fine-tuning; and in domain tasks, i.e., multi-resolution forecasting, anomaly detection, and data generation, establishing it as a \textit{proto-foundation model} that bridges the gap between general-purpose and domain-specific models. </p>
<blockquote>
<p>åŸºäºæ¨¡å‹çš„é©å‘½æ€§å˜åŒ–ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬å’Œè§†é¢‘å¤„ç†æ–¹é¢çš„åº”ç”¨ï¼Œæ—¶é—´åºåˆ—æ•°æ®å› å…¶ç‰¹æœ‰çš„ç¼ºå¤±å€¼ã€å¤šåˆ†è¾¨ç‡ç‰¹æ€§ç­‰ç‰¹å¾ç»™è¿™äº›æ–¹æ³•å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå½“å‰çš„è‡ªå›å½’transformeræ¨¡å‹å€¾å‘äºåœ¨é¢„è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ ç¡®å®šæ€§æ—¶é—´ä¾èµ–å…³ç³»ï¼Œè€Œå¿½ç•¥äº†å›ºæœ‰çš„ä¸ç¡®å®šæ€§å¹¶ç¼ºä¹ç‰©ç†çº¦æŸçš„é›†æˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†TimeDiTï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº†åŸºäºè½¬æ¢å™¨çš„æ—¶é—´ä¾èµ–å…³ç³»å­¦ä¹ ä¸åŸºäºæ‰©æ•£çš„æ¦‚ç‡é‡‡æ ·æŠ€æœ¯çš„æ‰©æ•£è½¬æ¢å™¨æ¨¡å‹ã€‚TimeDiTé‡‡ç”¨ç»Ÿä¸€çš„æ©ç æœºåˆ¶æ¥åè°ƒä¸åŒä»»åŠ¡çš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ï¼ŒåŒæ—¶å¼•å…¥äº†ä¸€ç§æœ‰ç†è®ºåŸºç¡€çš„ã€æ— éœ€å¾®è°ƒçš„æ¨¡å‹ç¼–è¾‘ç­–ç•¥ï¼Œå¯ä»¥åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­çµæ´»åœ°é›†æˆå¤–éƒ¨çŸ¥è¯†ã€‚æˆ‘ä»¬è®¤è¯†åˆ°åœ¨ä¸€ä¸ªå•ä¸€æ¨¡å‹ä¸‹ç»Ÿä¸€å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œé€šè¿‡ç³»ç»Ÿçš„è¯„ä¼°è¡¨æ˜ï¼ŒTimeDiTåœ¨åŸºç¡€ä»»åŠ¡ï¼ˆå¦‚é¢„æµ‹å’Œæ’å€¼ï¼‰ä»¥åŠé¢†åŸŸä»»åŠ¡ï¼ˆå¦‚å¤šåˆ†è¾¨ç‡é¢„æµ‹ã€å¼‚å¸¸æ£€æµ‹å’Œæ•°æ®ç”Ÿæˆï¼‰ä¸­çš„è¡¨ç°éƒ½éå¸¸å‡ºè‰²ï¼Œæˆä¸ºè¿æ¥é€šç”¨æ¨¡å‹å’Œç‰¹å®šé¢†åŸŸæ¨¡å‹çš„æ¡¥æ¢ã€‚TimeDiTä½œä¸ºä¸€ä¸ªåŸåˆçš„åŸºç¡€æ¨¡å‹å»ºç«‹èµ·äº†è¿™ä¸ªæ¡¥æ¢ã€‚å®ƒå¡«è¡¥äº†é€šç”¨æ¨¡å‹å’Œç‰¹å®šé¢†åŸŸæ¨¡å‹ä¹‹é—´çš„ç©ºç™½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.02322v2">PDF</a> 31 Pages, 11 Figures, 22 Tables. First present at ICML 2024 Workshop   on Foundation Models in the Wild</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹æ—¶é—´åºåˆ—æ•°æ®çš„æ–°æ¨¡å‹TimeDiTï¼Œå®ƒç»“åˆäº†åŸºäºå˜å‹å™¨çš„æ—¶åºä¾èµ–æ€§å­¦ä¹ å’ŒåŸºäºæ‰©æ•£çš„æ¦‚ç‡é‡‡æ ·æŠ€æœ¯ã€‚TimeDiTé‡‡ç”¨ç»Ÿä¸€çš„æ©ç æœºåˆ¶ï¼Œåœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­åè°ƒè®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ç†è®ºåŸºç¡€çš„ã€æ— éœ€å¾®è°ƒæ¨¡å‹ç¼–è¾‘ç­–ç•¥ï¼Œå¯åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­çµæ´»é›†æˆå¤–éƒ¨çŸ¥è¯†ã€‚TimeDiTè§£å†³äº†ç»Ÿä¸€å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡åœ¨å•ä¸€æ¨¡å‹ä¸‹çš„æŒ‘æˆ˜ï¼Œåœ¨åŸºç¡€ä»»åŠ¡å’Œé¢†åŸŸä»»åŠ¡ä¸­éƒ½å±•ç°å‡ºæœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TimeDiTæ˜¯ä¸€ç§é’ˆå¯¹æ—¶é—´åºåˆ—æ•°æ®çš„æ‰©æ•£å˜å‹å™¨æ¨¡å‹ï¼Œç»“åˆäº†åŸºäºå˜å‹å™¨çš„æ—¶åºä¾èµ–æ€§å­¦ä¹ å’ŒåŸºäºæ‰©æ•£çš„æ¦‚ç‡é‡‡æ ·æŠ€æœ¯ã€‚</li>
<li>TimeDiTé‡‡ç”¨ç»Ÿä¸€çš„æ©ç æœºåˆ¶ï¼Œç”¨äºåœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­åè°ƒè®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚</li>
<li>è¯¥æ¨¡å‹å¼•å…¥äº†ä¸€ç§æ— éœ€å¾®è°ƒæ¨¡å‹ç¼–è¾‘ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨é‡‡æ ·è¿‡ç¨‹ä¸­çµæ´»é›†æˆå¤–éƒ¨çŸ¥è¯†ã€‚</li>
<li>TimeDiTè§£å†³äº†åœ¨å•ä¸€æ¨¡å‹ä¸­ç»Ÿä¸€å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚</li>
<li>TimeDiTåœ¨åŸºç¡€ä»»åŠ¡ï¼ˆå¦‚é¢„æµ‹å’Œæ’è¡¥ï¼‰å’Œé¢†åŸŸä»»åŠ¡ï¼ˆå¦‚å¤šåˆ†è¾¨ç‡é¢„æµ‹ã€å¼‚å¸¸æ£€æµ‹å’Œç”Ÿæˆæ•°æ®ï¼‰ä¸­éƒ½è¡¨ç°å‡ºæœ‰æ•ˆæ€§ã€‚</li>
<li>TimeDiTå¡«è¡¥äº†é€šç”¨æ¨¡å‹å’Œç‰¹å®šé¢†åŸŸæ¨¡å‹ä¹‹é—´çš„ç©ºç™½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.02322">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4c21b86e5514c8b18a6608e4725e7e80.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-91983f17113cf0b00b89b2b48069d574.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe5c486bd7c35b2ee776f5b806fbcccf.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Improving-and-Assessing-the-Fidelity-of-Large-Language-Models-Alignment-to-Online-Communities"><a href="#Improving-and-Assessing-the-Fidelity-of-Large-Language-Models-Alignment-to-Online-Communities" class="headerlink" title="Improving and Assessing the Fidelity of Large Language Models Alignment   to Online Communities"></a>Improving and Assessing the Fidelity of Large Language Models Alignment   to Online Communities</h2><p><strong>Authors:Minh Duc Chu, Zihao He, Rebecca Dorn, Kristina Lerman</strong></p>
<p>Large language models (LLMs) have shown promise in representing individuals and communities, offering new ways to study complex social dynamics. However, effectively aligning LLMs with specific human groups and systematically assessing the fidelity of the alignment remains a challenge. This paper presents a robust framework for aligning LLMs with online communities via instruction-tuning and comprehensively evaluating alignment across various aspects of language, including authenticity, emotional tone, toxicity, and harm. We demonstrate the utility of our approach by applying it to online communities centered on dieting and body image. We administer an eating disorder psychometric test to the aligned LLMs to reveal unhealthy beliefs and successfully differentiate communities with varying levels of eating disorder risk. Our results highlight the potential of LLMs in automated moderation and broader applications in public health and social science research. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£è¡¨ä¸ªäººå’Œç¤¾åŒºæ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä¸ºäººä»¬ç ”ç©¶å¤æ‚çš„ç¤¾äº¤åŠ¨æ€æä¾›äº†æ–°çš„é€”å¾„ã€‚ç„¶è€Œï¼Œå¦‚ä½•æœ‰æ•ˆåœ°å°†LLMä¸ç‰¹å®šçš„äººç¾¤è¿›è¡ŒåŒ¹é…ï¼Œä»¥åŠå¦‚ä½•ç³»ç»Ÿåœ°è¯„ä¼°åŒ¹é…ç¨‹åº¦çš„ä¿çœŸåº¦ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡é’ˆå¯¹LLMå¦‚ä½•é€šè¿‡ç½‘ç»œç¤¾åŒºä¸­çš„æŒ‡ä»¤å¾®è°ƒæ¥è¿›è¡Œé€‚é…è¿™ä¸€é—®é¢˜æå‡ºäº†ä¸€ä¸ªå¯é çš„æ¡†æ¶ï¼Œå¹¶å¯¹è¯­è¨€å„é¢†åŸŸçš„åŒ¹é…ç¨‹åº¦è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬çœŸå®æ€§ã€æƒ…æ„ŸåŸºè°ƒã€æ”»å‡»æ€§ä»¥åŠæœ‰å®³æ€§ã€‚æœ¬æ–‡é€šè¿‡åœ¨ä»¥èŠ‚é£Ÿå’Œä½“å‹ä¸ºä¸»çš„ç½‘ç»œç¤¾åŒºä¸­å®æ–½è¯¥ç­–ç•¥æ¥å±•ç¤ºå…¶æ•ˆç”¨ã€‚æˆ‘ä»¬å¯¹è°ƒæ•´åçš„LLMè¿›è¡Œäº†ä¸€é¡¹é¥®é£Ÿéšœç¢å¿ƒç†æµ‹è¯•ï¼Œæ­ç¤ºäº†å…¶ä¸å¥åº·çš„ä¿¡å¿µï¼Œå¹¶æˆåŠŸåŒºåˆ†äº†ä¸åŒé¥®é£Ÿéšœç¢é£é™©çš„ç¤¾åŒºã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªæ˜¾äº†LLMåœ¨è‡ªåŠ¨åŒ–ç›‘ç®¡ä»¥åŠå…¬å…±å«ç”Ÿå’Œç¤¾ä¼šç§‘å­¦ç ”ç©¶çš„å¹¿æ³›åº”ç”¨æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.09366v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ä»£è¡¨ä¸ªäººå’Œç¤¾åŒºæ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä¸ºå¤æ‚çš„ç¤¾ä¼šåŠ¨æ€ç ”ç©¶æä¾›äº†æ–°çš„é€”å¾„ã€‚ç„¶è€Œï¼Œå¦‚ä½•å°†LLMsä¸ç‰¹å®šäººç±»ç¾¤ä½“æœ‰æ•ˆå¯¹é½ï¼Œå¹¶ç³»ç»Ÿè¯„ä¼°å¯¹é½çš„ä¿çœŸåº¦ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡é€šè¿‡æ„å»ºç¨³å¥çš„æ¡†æ¶ï¼Œåˆ©ç”¨æŒ‡ä»¤å¾®è°ƒå°†LLMsä¸åœ¨çº¿ç¤¾åŒºå¯¹é½ï¼Œå¹¶å…¨é¢è¯„ä¼°è¯­è¨€å„æ–¹é¢çš„å¯¹é½æƒ…å†µï¼ŒåŒ…æ‹¬çœŸå®æ€§ã€æƒ…æ„ŸåŸºè°ƒã€æ¯’æ€§å’Œå±å®³ã€‚æˆ‘ä»¬é€šè¿‡å¯¹ä»¥é¥®é£Ÿå’Œä½“å½¢ä¸ºä¸­å¿ƒçš„åœ¨çº¿ç¤¾åŒºè¿›è¡Œæ¼”ç¤ºï¼Œå±•ç¤ºäº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæ­ç¤ºäº†LLMsåœ¨è‡ªåŠ¨ç®¡ç†å’Œå…¬å…±å«ç”ŸåŠç¤¾ä¼šç§‘å­¦ç ”ç©¶ä¸­çš„æ½œåœ¨åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsä¸ºå¤æ‚ç¤¾ä¼šåŠ¨æ€ç ”ç©¶æä¾›äº†æ–°çš„é€”å¾„ã€‚</li>
<li>LLMsä¸ç‰¹å®šäººç±»ç¾¤ä½“çš„æœ‰æ•ˆå¯¹é½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡æŒ‡ä»¤å¾®è°ƒï¼Œå¯ä»¥å°†LLMsä¸åœ¨çº¿ç¤¾åŒºå¯¹é½ã€‚</li>
<li>å¯¹é½çš„è¯„ä¼°éœ€å…¨é¢è€ƒè™‘è¯­è¨€çš„çœŸå®æ€§ã€æƒ…æ„ŸåŸºè°ƒã€æ¯’æ€§å’Œå±å®³ã€‚</li>
<li>ä»¥é¥®é£Ÿå’Œä½“å½¢ä¸ºä¸­å¿ƒçš„åœ¨çº¿ç¤¾åŒºä½œä¸ºæ¼”ç¤ºå±•ç¤ºäº†æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>LLMsåœ¨è‡ªåŠ¨ç®¡ç†å’Œå…¬å…±å«ç”ŸåŠç¤¾ä¼šç§‘å­¦ç ”ç©¶ä¸­å…·æœ‰æ½œåœ¨åº”ç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.09366">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e36fb2a081da349f933c5835b2593cce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01b80b4fd357b7da71b0d0fe1221901b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa9ac2ba6822a0c0d1ffcb091474a8b4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3be36e1ffa99c05e2a93e188fa0180b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e6995ad44d770b81fd816b9922c04ee.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Why-Are-My-Prompts-Leaked-Unraveling-Prompt-Extraction-Threats-in-Customized-Large-Language-Models"><a href="#Why-Are-My-Prompts-Leaked-Unraveling-Prompt-Extraction-Threats-in-Customized-Large-Language-Models" class="headerlink" title="Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in   Customized Large Language Models"></a>Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in   Customized Large Language Models</h2><p><strong>Authors:Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, Haoyang Li</strong></p>
<p>The drastic increase of large language modelsâ€™ (LLMs) parameters has led to a new research direction of fine-tuning-free downstream customization by prompts, i.e., task descriptions. While these prompt-based services (e.g. OpenAIâ€™s GPTs) play an important role in many businesses, there has emerged growing concerns about the prompt leakage, which undermines the intellectual properties of these services and causes downstream attacks. In this paper, we analyze the underlying mechanism of prompt leakage, which we refer to as prompt memorization, and develop corresponding defending strategies. By exploring the scaling laws in prompt extraction, we analyze key attributes that influence prompt extraction, including model sizes, prompt lengths, as well as the types of prompts. Then we propose two hypotheses that explain how LLMs expose their prompts. The first is attributed to the perplexity, i.e. the familiarity of LLMs to texts, whereas the second is based on the straightforward token translation path in attention matrices. To defend against such threats, we investigate whether alignments can undermine the extraction of prompts. We find that current LLMs, even those with safety alignments like GPT-4, are highly vulnerable to prompt extraction attacks, even under the most straightforward user attacks. Therefore, we put forward several defense strategies with the inspiration of our findings, which achieve 83.8% and 71.0% drop in the prompt extraction rate for Llama2-7B and GPT-3.5, respectively. Source code is avaliable at <a target="_blank" rel="noopener" href="https://github.com/liangzid/PromptExtractionEval">https://github.com/liangzid/PromptExtractionEval</a>. </p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‚æ•°çš„å¤§è§„æ¨¡å¢é•¿ï¼Œé€šè¿‡æç¤ºï¼ˆå³ä»»åŠ¡æè¿°ï¼‰è¿›è¡Œå¾®è°ƒä»¥å¤–çš„ä¸‹æ¸¸å®šåˆ¶å·²ç»æˆä¸ºäº†ä¸€ä¸ªæ–°çš„ç ”ç©¶æ–¹å‘ã€‚è™½ç„¶è¿™äº›åŸºäºæç¤ºçš„æœåŠ¡ï¼ˆä¾‹å¦‚OpenAIçš„GPTç³»åˆ—ï¼‰åœ¨è®¸å¤šä¸šåŠ¡ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä½†å…³äºæç¤ºæ³„éœ²çš„æ‹…å¿§ä¹Ÿåœ¨æ—¥ç›Šå¢é•¿ã€‚æç¤ºæ³„éœ²ç ´åäº†è¿™äº›æœåŠ¡çš„çŸ¥è¯†äº§æƒå¹¶å¯¼è‡´äº†ä¸‹æ¸¸æ”»å‡»ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†æç¤ºæ³„éœ²çš„å†…åœ¨æœºåˆ¶ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºæç¤ºè®°å¿†ï¼Œå¹¶å¼€å‘äº†ç›¸åº”çš„é˜²å¾¡ç­–ç•¥ã€‚é€šè¿‡æ¢ç´¢æç¤ºæå–ä¸­çš„è§„æ¨¡å®šå¾‹ï¼Œæˆ‘ä»¬åˆ†æäº†å½±å“æç¤ºæå–çš„å…³é”®å±æ€§ï¼ŒåŒ…æ‹¬æ¨¡å‹å¤§å°ã€æç¤ºé•¿åº¦ä»¥åŠæç¤ºç±»å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªå‡è®¾æ¥è§£é‡ŠLLMå¦‚ä½•æš´éœ²å…¶æç¤ºã€‚ç¬¬ä¸€ä¸ªå‡è®¾å½’å› äºå›°æƒ‘åº¦ï¼Œå³LLMå¯¹æ–‡æœ¬çš„ç†Ÿæ‚‰ç¨‹åº¦ï¼Œè€Œç¬¬äºŒä¸ªå‡è®¾åˆ™æ˜¯åŸºäºæ³¨æ„åŠ›çŸ©é˜µä¸­çš„ç›´æ¥ä»¤ç‰Œç¿»è¯‘è·¯å¾„ã€‚ä¸ºäº†åº”å¯¹è¿™äº›å¨èƒï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†å¯¹é½æ˜¯å¦èƒ½é˜»ç¢æç¤ºçš„æå–ã€‚æˆ‘ä»¬å‘ç°ï¼Œå³ä½¿æ˜¯å…·æœ‰å®‰å…¨å¯¹é½çš„å½“å‰LLMï¼ˆå¦‚GPT-4ï¼‰ä¹Ÿéå¸¸å®¹æ˜“å—åˆ°æç¤ºæå–æ”»å‡»ï¼Œå³ä½¿åœ¨æœ€ç®€å•çš„ç”¨æˆ·æ”»å‡»ä¸‹ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ ¹æ®ç ”ç©¶ç»“æœæå‡ºäº†å‡ ç§é˜²å¾¡ç­–ç•¥ï¼Œå®ç°äº†Llama2-7Bå’ŒGPT-3.5çš„æç¤ºæå–ç‡åˆ†åˆ«ä¸‹é™äº†83.8%å’Œ71.0%ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/liangzid/PromptExtractionEval%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/liangzid/PromptExtractionEvalæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.02416v2">PDF</a> Source Code: <a target="_blank" rel="noopener" href="https://github.com/liangzid/PromptExtractionEval">https://github.com/liangzid/PromptExtractionEval</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‚æ•°å‰§å¢æ¨åŠ¨äº†æ— éœ€ç²¾ç»†è°ƒæ•´çš„ä¸‹æ¸¸å®šåˆ¶æ–¹å‘ï¼Œå³é€šè¿‡ä»»åŠ¡æè¿°æç¤ºæ¥å®Œæˆã€‚å°½ç®¡æç¤ºæœåŠ¡åœ¨è®¸å¤šå•†ä¸šé¢†åŸŸæ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†æç¤ºæ³„éœ²é—®é¢˜é€æ¸æµ®ç°ï¼Œä¾µçŠ¯äº†æœåŠ¡çš„çŸ¥è¯†äº§æƒå¹¶å¼•å‘ä¸‹æ¸¸æ”»å‡»ã€‚æœ¬æ–‡åˆ†æäº†æç¤ºæ³„éœ²çš„å†…åœ¨æœºåˆ¶â€”â€”æç¤ºè®°å¿†åŒ–ï¼Œå¹¶å‘å±•äº†ç›¸åº”çš„é˜²å¾¡ç­–ç•¥ã€‚é€šè¿‡æ¢ç´¢æç¤ºæå–ä¸­çš„è§„æ¨¡æ•ˆåº”è§„å¾‹ï¼Œåˆ†æäº†å½±å“æç¤ºæå–çš„å…³é”®å› ç´ ï¼ŒåŒ…æ‹¬æ¨¡å‹å¤§å°ã€æç¤ºé•¿åº¦å’Œæç¤ºç±»å‹ç­‰ã€‚æœ¬æ–‡æå‡ºä¸¤ä¸ªå‡è®¾æ¥è§£é‡ŠLLMå¦‚ä½•æš´éœ²å…¶æç¤ºï¼šä¸€æ˜¯ä¸å›°æƒ‘åº¦æœ‰å…³ï¼ŒäºŒæ˜¯åŸºäºæ³¨æ„åŠ›çŸ©é˜µä¸­çš„ç›´æ¥æ ‡è®°ç¿»è¯‘è·¯å¾„ã€‚ä¸ºåº”å¯¹è¿™äº›å¨èƒï¼Œæœ¬æ–‡è°ƒæŸ¥äº†å¯¹é½æ˜¯å¦èƒ½é˜»æ­¢æç¤ºæå–ã€‚ç ”ç©¶å‘ç°ï¼Œå³ä½¿æ˜¯å¸¦æœ‰å®‰å…¨å¯¹é½çš„å½“å‰LLMï¼ˆå¦‚GPT-4ï¼‰ä¹Ÿææ˜“å—åˆ°ç”¨æˆ·ç®€å•çš„æç¤ºæå–æ”»å‡»ã€‚å—æœ¬ç ”ç©¶ç»“æœå¯å‘ï¼Œæå‡ºçš„é˜²å¾¡ç­–ç•¥ä½¿Llama2-7Bå’ŒGPT-3.5çš„æç¤ºæå–ç‡åˆ†åˆ«ä¸‹é™äº†83.8%å’Œ71.0%ã€‚ç›¸å…³æºä»£ç å¯é€šè¿‡é“¾æ¥è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä»»åŠ¡æè¿°æç¤ºå®ç°ä¸‹æ¸¸å®šåˆ¶ï¼Œä½†æç¤ºæ³„éœ²é—®é¢˜æ—¥ç›Šä¸¥é‡ã€‚</li>
<li>æç¤ºæ³„éœ²çš„å†…åœ¨æœºåˆ¶æ˜¯æç¤ºè®°å¿†åŒ–ï¼Œè¿™å¨èƒåˆ°LLMæœåŠ¡çš„çŸ¥è¯†äº§æƒã€‚</li>
<li>å½±å“æç¤ºæå–çš„å…³é”®å› ç´ åŒ…æ‹¬æ¨¡å‹å¤§å°ã€æç¤ºé•¿åº¦å’Œç±»å‹ã€‚</li>
<li>LLMæš´éœ²å…¶æç¤ºçš„ä¸¤ä¸ªå‡è®¾ï¼šä¸å›°æƒ‘åº¦æœ‰å…³å’ŒåŸºäºæ³¨æ„åŠ›çŸ©é˜µçš„ç›´æ¥æ ‡è®°ç¿»è¯‘è·¯å¾„ã€‚</li>
<li>å½“å‰LLMï¼ˆå¦‚GPT-4ï¼‰æ˜“å—ç®€å•ç”¨æˆ·æ”»å‡»çš„å¨èƒï¼Œå­˜åœ¨ä¸¥é‡çš„æç¤ºæ³„éœ²é—®é¢˜ã€‚</li>
<li>å—ç ”ç©¶ç»“æœå¯å‘ï¼Œæå‡ºçš„é˜²å¾¡ç­–ç•¥æ˜¾è‘—é™ä½äº†Llamaå’ŒGPTçš„æç¤ºæå–ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.02416">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4615815ec3a7b171bff42eda674f9602.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7859bfcb3611914e713c6c733af550d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa19e5455776ad3af9654e49e8f3a10.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a9a05d58c560cb26832f25d93c040c7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-618a5b28ee5635dfddb7323d77bfae30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-78ccdc5a5cff9c5adbc720251eb151e2.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="OmniBal-Towards-Fast-Instruct-tuning-for-Vision-Language-Models-via-Omniverse-Computation-Balance"><a href="#OmniBal-Towards-Fast-Instruct-tuning-for-Vision-Language-Models-via-Omniverse-Computation-Balance" class="headerlink" title="OmniBal: Towards Fast Instruct-tuning for Vision-Language Models via   Omniverse Computation Balance"></a>OmniBal: Towards Fast Instruct-tuning for Vision-Language Models via   Omniverse Computation Balance</h2><p><strong>Authors:Yongqiang Yao, Jingru Tan, Jiahao Hu, Feizhao Zhang, Yazhe Niu, Xin Jin, Bo Li, Ruihao Gong, Pengfei Liu, Dahua Lin, Ningyi Xu</strong></p>
<p>Recently, vision-language instruct-tuning models have made significant progress due to their more comprehensive understanding of the world. In this work, we discovered that large-scale 3D parallel training on those models leads to an imbalanced computation load across different devices. The vision and language parts are inherently heterogeneous: their data distribution and model architecture differ significantly, which affects distributed training efficiency. We rebalanced the computational loads from data, model, and memory perspectives to address this issue, achieving more balanced computation across devices. These three components are not independent but are closely connected, forming an omniverse balanced training framework. Specifically, for the data, we grouped instances into new balanced mini-batches within and across devices. For the model, we employed a search-based method to achieve a more balanced partitioning. For memory optimization, we adaptively adjusted the re-computation strategy for each partition to utilize the available memory fully. We conducted extensive experiments to validate the effectiveness of our method. Compared with the open-source training code of InternVL-Chat, we significantly reduced GPU days, achieving about 1.8x speed-up. Our methodâ€™s efficacy and generalizability were further demonstrated across various models and datasets. Codes will be released at <a target="_blank" rel="noopener" href="https://github.com/ModelTC/OmniBal">https://github.com/ModelTC/OmniBal</a>. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œç”±äºè§†è§‰è¯­è¨€æŒ‡ä»¤å¾®è°ƒæ¨¡å‹å¯¹ä¸–ç•Œçš„ç†è§£æ›´åŠ å…¨é¢ï¼Œå®ƒä»¬å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç°å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå¤§è§„æ¨¡3Då¹¶è¡Œè®­ç»ƒä¼šå¯¼è‡´ä¸åŒè®¾å¤‡ä¹‹é—´çš„è®¡ç®—è´Ÿè½½ä¸å¹³è¡¡ã€‚è§†è§‰å’Œè¯­è¨€éƒ¨åˆ†æ˜¯å›ºæœ‰åœ°å¼‚è´¨çš„ï¼šå®ƒä»¬çš„æ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹æ¶æ„å­˜åœ¨å¾ˆå¤§å·®å¼‚ï¼Œè¿™å½±å“äº†åˆ†å¸ƒå¼è®­ç»ƒçš„æ•ˆç‡ã€‚æˆ‘ä»¬ä»æ•°æ®ã€æ¨¡å‹å’Œå†…å­˜ä¸‰ä¸ªæ–¹é¢é‡æ–°å¹³è¡¡è®¡ç®—è´Ÿè½½ï¼Œä»¥è§£å†³æ­¤é—®é¢˜ï¼Œå®ç°è·¨è®¾å¤‡çš„æ›´å¹³è¡¡è®¡ç®—ã€‚è¿™ä¸‰ä¸ªç»„ä»¶ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œè€Œæ˜¯ç´§å¯†ç›¸å…³çš„ï¼Œå½¢æˆäº†ä¸€ä¸ªå…¨æ–¹ä½å¹³è¡¡çš„è®­ç»ƒæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ•°æ®ï¼Œæˆ‘ä»¬å°†å®ä¾‹åˆ†ä¸ºè®¾å¤‡å’Œè·¨è®¾å¤‡ä¹‹é—´çš„æ–°å¹³è¡¡å°æ‰¹é‡ã€‚å¯¹äºæ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨åŸºäºæœç´¢çš„æ–¹æ³•æ¥å®ç°æ›´å¹³è¡¡çš„åˆ†åŒºã€‚å¯¹äºå†…å­˜ä¼˜åŒ–ï¼Œæˆ‘ä»¬è‡ªé€‚åº”åœ°è°ƒæ•´æ¯ä¸ªåˆ†åŒºçš„é‡æ–°è®¡ç®—ç­–ç•¥ï¼Œä»¥å……åˆ†åˆ©ç”¨å¯ç”¨å†…å­˜ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒæ¥éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä¸å¼€æºè®­ç»ƒä»£ç InternVL-Chatç›¸æ¯”ï¼Œæˆ‘ä»¬å¤§å¹…å‡å°‘äº†GPUå¤©æ•°ï¼Œå®ç°äº†çº¦1.8å€çš„é€Ÿåº¦æå‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§æ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§å¾—åˆ°äº†è¿›ä¸€æ­¥è¯æ˜ã€‚ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/ModelTC/OmniBal%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/ModelTC/OmniBalå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.20761v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§è§„æ¨¡ä¸‰ç»´å¹¶è¡Œè®­ç»ƒåœ¨è§†è§‰è¯­è¨€æŒ‡ä»¤å¾®è°ƒæ¨¡å‹ä¸Šä¼šå¯¼è‡´è®¡ç®—è´Ÿè½½ä¸å‡è¡¡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä»æ•°æ®ã€æ¨¡å‹å’Œå†…å­˜ä¸‰ä¸ªæ–¹é¢é‡æ–°å¹³è¡¡è®¡ç®—è´Ÿè½½ï¼Œå½¢æˆäº†ä¸€ä¸ªå…¨æ–¹ä½å¹³è¡¡çš„è®­ç»ƒæ¡†æ¶ã€‚é€šè¿‡å¹³è¡¡æ•°æ®åˆ†å¸ƒã€ä¼˜åŒ–æ¨¡å‹åˆ†åŒºå’Œè°ƒæ•´å†…å­˜ä½¿ç”¨ç­–ç•¥ï¼Œæˆ‘ä»¬æé«˜äº†åˆ†å¸ƒå¼è®­ç»ƒæ•ˆç‡ï¼Œå‡å°‘äº†GPUå¤©æ•°ï¼Œå®ç°äº†è·¨æ¨¡å‹å’Œæ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–æ€§æå‡ã€‚ç›¸å…³ä»£ç å°†åœ¨GitHubä¸Šå‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æŒ‡ä»¤å¾®è°ƒæ¨¡å‹åœ¨å¤§è§„æ¨¡ä¸‰ç»´å¹¶è¡Œè®­ç»ƒæ—¶å­˜åœ¨è®¡ç®—è´Ÿè½½ä¸å‡è¡¡é—®é¢˜ã€‚</li>
<li>è§†è§‰å’Œè¯­è¨€éƒ¨åˆ†åœ¨æ•°æ®åˆ†å¸ƒå’Œæ¨¡å‹æ¶æ„ä¸Šå­˜åœ¨å›ºæœ‰å·®å¼‚ï¼Œå½±å“åˆ†å¸ƒå¼è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>ä¸ºè§£å†³è®¡ç®—è´Ÿè½½ä¸å‡è¡¡é—®é¢˜ï¼Œä»æ•°æ®ã€æ¨¡å‹å’Œå†…å­˜ä¸‰ä¸ªæ–¹é¢è¿›è¡Œäº†é‡æ–°å¹³è¡¡ã€‚</li>
<li>é€šè¿‡å¹³è¡¡æ•°æ®åˆ†å¸ƒã€ä¼˜åŒ–æ¨¡å‹åˆ†åŒºå’Œè°ƒæ•´å†…å­˜ä½¿ç”¨ç­–ç•¥ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>ä¸å¼€æºè®­ç»ƒä»£ç ç›¸æ¯”ï¼Œå‡å°‘äº†GPUå¤©æ•°ï¼Œå®ç°äº†çº¦1.8å€çš„é€Ÿåº¦æå‡ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å„ç§æ¨¡å‹å’Œæ•°æ®é›†ä¸Šè¡¨ç°å‡ºæœ‰æ•ˆæ€§å’Œæ³›åŒ–æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.20761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c4c737ee94e2efdb79f2fa4b3a187d30.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-36c292c3621e8dc0e7e891c637fb7b94.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e94b6e3107dfced375733b6222d8fca0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2445de10961c1b5aa637e4d36199e6d2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e5090351b95da9e2f7ac0fab0532bb0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-761c8541addedf7416be192890ca6629.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="From-Loops-to-Oops-Fallback-Behaviors-of-Language-Models-Under-Uncertainty"><a href="#From-Loops-to-Oops-Fallback-Behaviors-of-Language-Models-Under-Uncertainty" class="headerlink" title="From Loops to Oops: Fallback Behaviors of Language Models Under   Uncertainty"></a>From Loops to Oops: Fallback Behaviors of Language Models Under   Uncertainty</h2><p><strong>Authors:Maor Ivgi, Ori Yoran, Jonathan Berant, Mor Geva</strong></p>
<p>Large language models (LLMs) often exhibit undesirable behaviors, such as hallucinations and sequence repetitions. We propose to view these behaviors as fallbacks that models exhibit under epistemic uncertainty, and investigate the connection between them. We categorize fallback behaviors - sequence repetitions, degenerate text, and hallucinations - and extensively analyze them in models from the same family that differ by the amount of pretraining tokens, parameter count, or the inclusion of instruction-following training. Our experiments reveal a clear and consistent ordering of fallback behaviors, across all these axes: the more advanced an LLM is (i.e., trained on more tokens, has more parameters, or instruction-tuned), its fallback behavior shifts from sequence repetitions, to degenerate text, and then to hallucinations. Moreover, the same ordering is observed during the generation of a single sequence, even for the best-performing models; as uncertainty increases, models shift from generating hallucinations to producing degenerate text and finally sequence repetitions. Lastly, we demonstrate that while common decoding techniques, such as random sampling, alleviate unwanted behaviors like sequence repetitions, they increase harder-to-detect hallucinations. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¸å¸¸è¡¨ç°å‡ºä¸€äº›ä¸ç†æƒ³çš„è¡Œä¸ºï¼Œå¦‚å¹»è§‰å’Œåºåˆ—é‡å¤ã€‚æˆ‘ä»¬æå‡ºå°†è¿™äº›è¡Œä¸ºè§†ä¸ºæ¨¡å‹åœ¨è®¤çŸ¥ä¸ç¡®å®šæ€§ä¸‹è¡¨ç°å‡ºçš„å›é€€è¡Œä¸ºï¼Œå¹¶ç ”ç©¶å®ƒä»¬ä¹‹é—´çš„è”ç³»ã€‚æˆ‘ä»¬å¯¹å›é€€è¡Œä¸ºè¿›è¡Œåˆ†ç±»ï¼ŒåŒ…æ‹¬åºåˆ—é‡å¤ã€æ–‡æœ¬é€€åŒ–ä»¥åŠå¹»è§‰ï¼Œå¹¶åœ¨åŒä¸€å®¶æ—çš„æ¨¡å‹ä¸­è¿›è¡Œäº†å¹¿æ³›çš„åˆ†æï¼Œè¿™äº›æ¨¡å‹åœ¨é¢„è®­ç»ƒä»¤ç‰Œçš„æ•°é‡ã€å‚æ•°è®¡æ•°æˆ–æ˜¯å¦åŒ…å«æŒ‡ä»¤è·Ÿéšè®­ç»ƒæ–¹é¢æœ‰æ‰€ä¸åŒã€‚æˆ‘ä»¬çš„å®éªŒæ­ç¤ºäº†å„ç§è½´ä¸Šçš„å›é€€è¡Œä¸ºçš„æ¸…æ™°ä¸”ä¸€è‡´æ’åºï¼šä¸€ä¸ªLLMè¶Šå…ˆè¿›ï¼ˆå³è®­ç»ƒäº†æ›´å¤šçš„ä»¤ç‰Œã€æœ‰æ›´å¤šçš„å‚æ•°æˆ–ç»è¿‡æŒ‡ä»¤è°ƒæ•´ï¼‰ï¼Œå…¶å›é€€è¡Œä¸ºä»åºåˆ—é‡å¤è½¬å˜ä¸ºæ–‡æœ¬é€€åŒ–ï¼Œç„¶ååˆ°å¹»è§‰ã€‚æ­¤å¤–ï¼Œåœ¨ç”Ÿæˆå•ä¸ªåºåˆ—çš„è¿‡ç¨‹ä¸­ï¼Œå³ä½¿å¯¹äºæ€§èƒ½æœ€ä½³çš„æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥è§‚å¯Ÿåˆ°ç›¸åŒçš„æ’åºï¼›éšç€ä¸ç¡®å®šæ€§çš„å¢åŠ ï¼Œæ¨¡å‹ä»äº§ç”Ÿå¹»è§‰è½¬å˜ä¸ºç”Ÿæˆæ–‡æœ¬é€€åŒ–å¹¶æœ€ç»ˆå‡ºç°åºåˆ—é‡å¤ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜å¸¸è§çš„è§£ç æŠ€æœ¯ï¼ˆå¦‚éšæœºé‡‡æ ·ï¼‰è™½ç„¶å¯ä»¥å‡è½»ä¸å¸Œæœ›å‡ºç°çš„è¡Œä¸ºï¼ˆå¦‚åºåˆ—é‡å¤ï¼‰ï¼Œä½†å®ƒä»¬ä¼šå¢åŠ æ›´éš¾æ£€æµ‹çš„å¹»è§‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.06071v2">PDF</a> NeurIPS Workshop on Attributing Model Behavior at Scale (ATTRIB 2024)</p>
<p><strong>æ€»ç»“</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é¢ä¸´çŸ¥è¯†ä¸ç¡®å®šæ€§æ—¶äº§ç”Ÿçš„é€€åŒ–è¡Œä¸ºï¼Œå¦‚åºåˆ—é‡å¤ã€æ–‡æœ¬é€€åŒ–å’Œå¹»è§‰ã€‚æ–‡ç« å¯¹æ¨¡å‹è¿›è¡Œå½’ç±»ï¼Œå¹¶é€šè¿‡å®éªŒåˆ†æä¸åŒé¢„è®­ç»ƒæ ‡è®°é‡ã€å‚æ•°æ•°é‡æˆ–æ˜¯å¦åŒ…å«æŒ‡ä»¤è®­ç»ƒç­‰å› ç´ å¯¹å…¶è¡Œä¸ºçš„å½±å“ã€‚å®éªŒæ­ç¤ºäº†ä¸€ä¸ªæ˜ç¡®çš„é€€åŒ–è¡Œä¸ºé¡ºåºï¼šéšç€æ¨¡å‹çš„å‘å±•ï¼ˆå³æ›´å¤šçš„é¢„è®­ç»ƒæ ‡è®°ã€å‚æ•°æˆ–æŒ‡ä»¤è°ƒæ•´ï¼‰ï¼Œå…¶é€€åŒ–è¡Œä¸ºä»åºåˆ—é‡å¤è½¬å˜ä¸ºæ–‡æœ¬é€€åŒ–ï¼Œæœ€ç»ˆå‡ºç°å¹»è§‰ã€‚æ­¤å¤–ï¼Œåœ¨åŒä¸€åºåˆ—ç”Ÿæˆè¿‡ç¨‹ä¸­ä¹Ÿè§‚å¯Ÿåˆ°ç›¸åŒçš„é¡ºåºã€‚æœ€åï¼Œæ–‡ç« æŒ‡å‡ºå¸¸è§çš„è§£ç æŠ€æœ¯å¦‚éšæœºé‡‡æ ·è™½ç„¶å¯ä»¥å‡è½»åºåˆ—é‡å¤ç­‰ä¸æœŸæœ›çš„è¡Œä¸ºï¼Œä½†å®ƒä»¬å¯èƒ½å¢åŠ æ›´éš¾æ£€æµ‹çš„å¹»è§‰é—®é¢˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LLMåœ¨çŸ¥è¯†ä¸ç¡®å®šæ€§ä¸‹ä¼šè¡¨ç°å‡ºé€€åŒ–è¡Œä¸ºï¼ŒåŒ…æ‹¬åºåˆ—é‡å¤ã€æ–‡æœ¬é€€åŒ–å’Œå¹»è§‰ã€‚</li>
<li>é€€åŒ–è¡Œä¸ºåœ¨ä¸åŒæ¨¡å‹ä¹‹é—´å­˜åœ¨ä¸€è‡´æ€§ï¼Œå¹¶ä¸æ¨¡å‹çš„é¢„è®­ç»ƒæ ‡è®°é‡ã€å‚æ•°æ•°é‡ä»¥åŠæ˜¯å¦ç»è¿‡æŒ‡ä»¤è®­ç»ƒç­‰å› ç´ æœ‰å…³ã€‚</li>
<li>éšç€æ¨¡å‹å…ˆè¿›ç¨‹åº¦çš„æé«˜ï¼Œå…¶é€€åŒ–è¡Œä¸ºå‘ˆç°å‡ºä¸€ç§å¯é¢„æµ‹çš„é¡ºåºå˜åŒ–ã€‚</li>
<li>åœ¨ç”Ÿæˆå•ä¸€åºåˆ—æ—¶ï¼Œéšç€ä¸ç¡®å®šæ€§çš„å¢åŠ ï¼Œæ¨¡å‹çš„é€€åŒ–è¡Œä¸ºä¹Ÿå‘ˆç°ä¸Šè¿°é¡ºåºå˜åŒ–ã€‚</li>
<li>å¸¸è§çš„è§£ç æŠ€æœ¯å¦‚éšæœºé‡‡æ ·è™½èƒ½å‡è½»æŸäº›ä¸æœŸæœ›çš„è¡Œä¸ºï¼Œä½†å¯èƒ½å¢åŠ å…¶ä»–ç±»å‹çš„é£é™©ï¼Œå¦‚å¹»è§‰ã€‚</li>
<li>LLMçš„é€€åŒ–è¡Œä¸ºå¯èƒ½ä¸æ¨¡å‹åœ¨ä¸ç¡®å®šæƒ…å¢ƒä¸‹çš„å†³ç­–ç­–ç•¥æœ‰å…³ã€‚</li>
<li>å¯¹LLMçš„è¿›ä¸€æ­¥ä¼˜åŒ–å¯èƒ½éœ€è¦è€ƒè™‘å¦‚ä½•åœ¨ä¸ç¡®å®šæƒ…å¢ƒä¸‹å¹³è¡¡æ¨¡å‹çš„å†³ç­–æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.06071">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6f355bf5359c4004e15fe182a47b94b8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d91c687b53df5e5d7046379b589769ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61647c96f395f54551a92bc0007f571d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9664280add2caffaab91bf2b0ba3204c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81bee329c30f4acac336108b85441a78.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="CollabStory-Multi-LLM-Collaborative-Story-Generation-and-Authorship-Analysis"><a href="#CollabStory-Multi-LLM-Collaborative-Story-Generation-and-Authorship-Analysis" class="headerlink" title="CollabStory: Multi-LLM Collaborative Story Generation and Authorship   Analysis"></a>CollabStory: Multi-LLM Collaborative Story Generation and Authorship   Analysis</h2><p><strong>Authors:Saranya Venkatraman, Nafis Irtiza Tripto, Dongwon Lee</strong></p>
<p>The rise of unifying frameworks that enable seamless interoperability of Large Language Models (LLMs) has made LLM-LLM collaboration for open-ended tasks a possibility. Despite this, there have not been efforts to explore such collaborative writing. We take the next step beyond human-LLM collaboration to explore this multi-LLM scenario by generating the first exclusively LLM-generated collaborative stories dataset called CollabStory. We focus on single-author to multi-author (up to 5 LLMs) scenarios, where multiple LLMs co-author stories. We generate over 32k stories using open-source instruction-tuned LLMs. Further, we take inspiration from the PAN tasks that have set the standard for human-human multi-author writing tasks and analysis. We extend their authorship-related tasks for multi-LLM settings and present baselines for LLM-LLM collaboration. We find that current baselines are not able to handle this emerging scenario. Thus, CollabStory is a resource that could help propel an understanding as well as the development of new techniques to discern the use of multiple LLMs. This is crucial to study in the context of writing tasks since LLM-LLM collaboration could potentially overwhelm ongoing challenges related to plagiarism detection, credit assignment, maintaining academic integrity in educational settings, and addressing copyright infringement concerns. We make our dataset and code available at <a target="_blank" rel="noopener" href="https://github.com/saranya-venkatraman/CollabStory">https://github.com/saranya-venkatraman/CollabStory</a>. </p>
<blockquote>
<p>éšç€èƒ½å¤Ÿä½¿å¤§å‹è¯­è¨€æ¨¡å‹æ— ç¼äº’æ“ä½œçš„ç»Ÿä¸€æ¡†æ¶çš„å…´èµ·ï¼Œå¼€æ”¾å¼ä»»åŠ¡ä¸­çš„å¤šè¯­è¨€æ¨¡å‹ååŒåˆä½œæˆä¸ºå¯èƒ½ã€‚ç„¶è€Œï¼Œäººä»¬å°šæœªå°è¯•è¿‡æ¢ç´¢è¿™ç§ååŒå†™ä½œçš„æ–¹å¼ã€‚æˆ‘ä»¬åœ¨äººç±»ä¸è¯­è¨€æ¨¡å‹åˆä½œçš„åŸºç¡€ä¸Šè¿ˆå‡ºä¸€æ­¥ï¼Œæ¢ç´¢è¿™ç§å¤šè¯­è¨€æ¨¡å‹ååŒåˆä½œæƒ…æ™¯ï¼Œç”Ÿæˆäº†é¦–ä¸ªç”±è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åä½œæ•…äº‹æ•°æ®é›†ï¼Œåä¸ºCollabStoryã€‚æˆ‘ä»¬å…³æ³¨å•äººåˆ°å¤šäººä½œè€…ï¼ˆæœ€å¤šäº”ä¸ªè¯­è¨€æ¨¡å‹ï¼‰çš„åœºæ™¯ï¼Œå…¶ä¸­å¤šä¸ªè¯­è¨€æ¨¡å‹å…±åŒåˆ›ä½œæ•…äº‹ã€‚æˆ‘ä»¬ä½¿ç”¨å¼€æºæŒ‡ä»¤è°ƒæ•´çš„è¯­è¨€æ¨¡å‹ç”Ÿæˆè¶…è¿‡ä¸‰ä¸‡ä¸¤åƒä¸ªæ•…äº‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»ä¸ºå¤šäººåˆä½œå†™ä½œä»»åŠ¡è®¾å®šæ ‡å‡†çš„PANä»»åŠ¡ä¸­æ±²å–çµæ„Ÿï¼Œå¹¶å°†å…¶æ‰©å±•åˆ°å¤šè¯­è¨€æ¨¡å‹è®¾ç½®çš„ä½œè€…ç›¸å…³ä»»åŠ¡ä¸Šï¼Œä¸ºè¯­è¨€æ¨¡å‹çš„ååŒåˆä½œæä¾›äº†åŸºå‡†çº¿ã€‚æˆ‘ä»¬å‘ç°å½“å‰çš„åŸºå‡†çº¿æ— æ³•åº”å¯¹è¿™ä¸€æ–°å…´åœºæ™¯ã€‚å› æ­¤ï¼ŒCollabStoryå¯ä»¥å¸®åŠ©æ¨è¿›å¯¹è¿™ç§æƒ…å¢ƒçš„ç†è§£ï¼Œä»¥åŠå¼€å‘èƒ½å¤Ÿè¯†åˆ«å¤šä¸ªè¯­è¨€æ¨¡å‹ä½¿ç”¨çš„æ–°æŠ€æœ¯ã€‚åœ¨å†™ä»»åŠ¡çš„ç¯å¢ƒä¸­ç ”ç©¶è¿™ä¸€ç‚¹è‡³å…³é‡è¦ï¼Œå› ä¸ºè¯­è¨€æ¨¡å‹çš„ååŒåˆä½œå¯èƒ½ä¼šå¸¦æ¥ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚æŠ„è¢­æ£€æµ‹ã€ä¿¡ç”¨åˆ†é…ã€ç»´æŠ¤æ•™è‚²ç¯å¢ƒä¸­çš„å­¦æœ¯è¯šä¿¡ä»¥åŠè§£å†³ç‰ˆæƒä¾µæƒé—®é¢˜ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/saranya-venkatraman/CollabStory%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/saranya-venkatraman/CollabStoryä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12665v3">PDF</a> Accepted to NAACL Findings 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ååŒå†™ä½œçš„æ•…äº‹æ•°æ®é›†CollabStoryçš„ç”Ÿæˆä¸æ¢ç´¢ã€‚é‡ç‚¹ç ”ç©¶äº†å•ä¸ªä½œè€…åˆ°å¤šä¸ªä½œè€…ï¼ˆæœ€å¤š5ä¸ªLLMï¼‰çš„åœºæ™¯ï¼Œå…¶ä¸­å¤šä¸ªLLMå…±åŒåˆ›ä½œæ•…äº‹ã€‚é€šè¿‡å¼€æ”¾æºä»£ç æŒ‡ä»¤è°ƒæ•´çš„LLMç”Ÿæˆè¶…è¿‡32kä¸ªæ•…äº‹ã€‚åŒæ—¶ï¼Œä»äººç±»äººç±»å¤šä½œè€…å†™ä½œä»»åŠ¡çš„æ ‡æ†ä»»åŠ¡PANä¸­æ±²å–çµæ„Ÿï¼Œä¸ºå¤šé‡LLMè®¾ç½®æ‰©å±•äº†ä½œè€…ç›¸å…³ä»»åŠ¡ï¼Œå¹¶ä¸ºLLM-LLMåä½œæä¾›äº†åŸºå‡†çº¿ã€‚ç ”ç©¶å‘ç°å½“å‰åŸºçº¿æ— æ³•åº”å¯¹è¿™ä¸€æ–°å…´åœºæ™¯ï¼Œå› æ­¤CollabStoryèµ„æºæœ‰åŠ©äºæ¨è¿›å¯¹å¤šä¸ªLLMä½¿ç”¨ç†è§£ä»¥åŠæ–°æŠ€æœ¯å¼€å‘ï¼Œè§£å†³å†™ä½œä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚æŠ„è¢­æ£€æµ‹ã€å­¦åˆ†åˆ†é…ã€ç»´æŠ¤å­¦æœ¯è¯šä¿¡ä»¥åŠç‰ˆæƒä¾µæƒé—®é¢˜ç­‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹é—´çš„æ— ç¼äº’æ“ä½œæ€§å·²æˆä¸ºå¯èƒ½ï¼Œå¹¶ç”Ÿæˆäº†é¦–ä¸ªåä¸ºCollabStoryçš„LLMååŒåˆ›ä½œæ•…äº‹æ•°æ®é›†ã€‚</li>
<li>CollabStoryä¸“æ³¨äºå•ä¸€ä½œè€…åˆ°å¤šä½œè€…åœºæ™¯ï¼Œæœ€å¤šåŒ…å«äº”ä¸ªLLMå…±åŒåˆ›ä½œæ•…äº‹ã€‚</li>
<li>ç”Ÿæˆäº†è¶…è¿‡32kä¸ªæ•…äº‹ï¼Œä½¿ç”¨äº†å¼€æºæŒ‡ä»¤è°ƒæ•´è¿‡çš„LLMã€‚</li>
<li>ä»äººç±»å¤šä½œè€…å†™ä½œä»»åŠ¡çš„æ ‡æ†ä»»åŠ¡PANä¸­æ±²å–çµæ„Ÿï¼Œä¸ºå¤šé‡LLMè®¾ç½®æ‰©å±•äº†ä½œè€…ç›¸å…³ä»»åŠ¡ã€‚</li>
<li>å½“å‰åŸºçº¿æ— æ³•åº”å¯¹å¤šLLMåä½œåœºæ™¯ï¼Œéœ€è¦æ–°çš„æŠ€æœ¯å’Œæ–¹æ³•ã€‚</li>
<li>CollabStoryèµ„æºæœ‰åŠ©äºè§£å†³å†™ä½œä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ï¼Œå¦‚æŠ„è¢­æ£€æµ‹ã€ç»´æŠ¤å­¦æœ¯è¯šä¿¡ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.12665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a5906c4d75b9bb48ffc691ec17ef0281.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-14524dfd41ae948e41f4d5fa98bd7ea7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-489b998a5b5f2a29e05a8a2bf0397a0d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-95de6cc92752fcfd0c90c2f7fce87553.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-020302bc52b16fa2071b2d00778eb4e2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f40d0397b393afb37220cd295b943629.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="On-the-Impacts-of-Contexts-on-Repository-Level-Code-Generation"><a href="#On-the-Impacts-of-Contexts-on-Repository-Level-Code-Generation" class="headerlink" title="On the Impacts of Contexts on Repository-Level Code Generation"></a>On the Impacts of Contexts on Repository-Level Code Generation</h2><p><strong>Authors:Nam Le Hai, Dung Manh Nguyen, Nghi D. Q. Bui</strong></p>
<p>CodeLLMs have gained widespread adoption for code generation tasks, yet their capacity to handle repository-level code generation with complex contextual dependencies remains underexplored. Our work underscores the critical importance of leveraging repository-level contexts to generate executable and functionally correct code. We present RepoExec, a novel benchmark designed to evaluate repository-level code generation, with a focus on three key aspects: executability, functional correctness through comprehensive test case generation, and accurate utilization of cross-file contexts. Our study examines a controlled scenario where developers specify essential code dependencies (contexts), challenging models to integrate them effectively. Additionally, we introduce an instruction-tuned dataset that enhances CodeLLMsâ€™ ability to leverage dependencies, along with a new metric, Dependency Invocation Rate (DIR), to quantify context utilization. Experimental results reveal that while pretrained LLMs demonstrate superior performance in terms of correctness, instruction-tuned models excel in context utilization and debugging capabilities. RepoExec offers a comprehensive evaluation framework for assessing code functionality and alignment with developer intent, thereby advancing the development of more reliable CodeLLMs for real-world applications. The dataset and source code are available at <a target="_blank" rel="noopener" href="https://github.com/FSoft-AI4Code/RepoExec">https://github.com/FSoft-AI4Code/RepoExec</a>. </p>
<blockquote>
<p>ä»£ç LLMï¼ˆCodeLLMsï¼‰å·²ç»åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œç„¶è€Œå®ƒä»¬åœ¨å¤„ç†å…·æœ‰å¤æ‚ä¸Šä¸‹æ–‡ä¾èµ–çš„ä»“åº“çº§åˆ«ä»£ç ç”Ÿæˆæ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†åˆ©ç”¨ä»“åº“çº§åˆ«ä¸Šä¸‹æ–‡ç”Ÿæˆå¯æ‰§è¡Œå’ŒåŠŸèƒ½æ€§æ­£ç¡®çš„ä»£ç çš„é‡è¦æ€§ã€‚æˆ‘ä»¬æå‡ºäº†RepoExecï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°ä»“åº“çº§åˆ«ä»£ç ç”Ÿæˆçš„æ–°å‹åŸºå‡†æµ‹è¯•ï¼Œé‡ç‚¹å…³æ³¨ä¸‰ä¸ªæ–¹é¢ï¼šå¯æ‰§è¡Œæ€§ã€é€šè¿‡å…¨é¢çš„æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§ï¼Œä»¥åŠè·¨æ–‡ä»¶ä¸Šä¸‹æ–‡çš„å‡†ç¡®åˆ©ç”¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶è€ƒå¯Ÿäº†ä¸€ä¸ªå¼€å‘äººå‘˜æŒ‡å®šå…³é”®ä»£ç ä¾èµ–å…³ç³»ï¼ˆä¸Šä¸‹æ–‡ï¼‰çš„å—æ§åœºæ™¯ï¼Œå¯¹æ¨¡å‹æœ‰æ•ˆåœ°é›†æˆå®ƒä»¬æå‡ºäº†æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæŒ‡ä»¤è°ƒä¼˜æ•°æ®é›†ï¼Œä»¥æé«˜CodeLLMåˆ©ç”¨ä¾èµ–é¡¹çš„èƒ½åŠ›ï¼Œä»¥åŠä¸€ä¸ªæ–°çš„æŒ‡æ ‡â€”â€”ä¾èµ–è°ƒç”¨ç‡ï¼ˆDIRï¼‰ï¼Œä»¥é‡åŒ–ä¸Šä¸‹æ–‡çš„åˆ©ç”¨ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè™½ç„¶é¢„è®­ç»ƒçš„LLMåœ¨æ­£ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†æŒ‡ä»¤è°ƒä¼˜çš„æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡åˆ©ç”¨å’Œè°ƒè¯•èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚RepoExecä¸ºè¯„ä¼°ä»£ç åŠŸèƒ½å’Œä¸å¼€å‘äººå‘˜æ„å›¾çš„å¥‘åˆåº¦æä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œä»è€Œæ¨åŠ¨äº†æ›´å¯é çš„CodeLLMåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¼€å‘ã€‚æ•°æ®é›†å’Œæºä»£ç å¯åœ¨&lt;<a target="_blank" rel="noopener" href="https://github.com/FSoft-AI4Code">https://github.com/FSoft-AI4Code</a> å±•å¼€â–¼Code&gt;ä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11927v4">PDF</a> Accepted to NAACL 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†CodeLLMåœ¨å¤„ç†ä»“åº“çº§åˆ«ä»£ç ç”Ÿæˆæ–¹é¢çš„å…³é”®èƒ½åŠ›ï¼Œå¼ºè°ƒäº†åˆ©ç”¨ä»“åº“çº§åˆ«ä¸Šä¸‹æ–‡ç”Ÿæˆå¯æ‰§è¡Œä¸”åŠŸèƒ½æ­£ç¡®çš„ä»£ç çš„é‡è¦æ€§ã€‚ä½œè€…æå‡ºäº†RepoExecåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•ä¸»è¦è¯„ä¼°ä»“åº“çº§åˆ«ä»£ç ç”Ÿæˆçš„ä¸‰ä¸ªæ–¹é¢ï¼šå¯æ‰§è¡Œæ€§ã€é€šè¿‡ç»¼åˆæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§ï¼Œä»¥åŠå‡†ç¡®ä½¿ç”¨è·¨æ–‡ä»¶ä¸Šä¸‹æ–‡ã€‚æ–‡ç« è¿˜ä»‹ç»äº†æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†å’Œæ–°çš„ä¾èµ–è°ƒç”¨ç‡æŒ‡æ ‡ï¼Œä»¥é‡åŒ–ä¸Šä¸‹æ–‡åˆ©ç”¨æƒ…å†µã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒLLMåœ¨æ­£ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè€ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹åœ¨ä¸Šä¸‹æ–‡åˆ©ç”¨å’Œè°ƒè¯•èƒ½åŠ›æ–¹é¢è¡¨ç°æ›´å‡ºè‰²ã€‚RepoExecæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°ä»£ç åŠŸèƒ½å’Œä¸å¼€å‘è€…æ„å›¾çš„å¥‘åˆåº¦ï¼Œæ¨åŠ¨äº†æ›´å¯é çš„CodeLLMåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¼€å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CodeLLMsåœ¨å¤„ç†ä»“åº“çº§åˆ«ä»£ç ç”Ÿæˆçš„èƒ½åŠ›å—åˆ°å…³æ³¨ï¼Œä½†å…¶åœ¨å¤„ç†å¤æ‚ä¸Šä¸‹æ–‡ä¾èµ–æ–¹é¢çš„èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>RepoExecåŸºå‡†æµ‹è¯•è¢«è®¾è®¡ç”¨æ¥è¯„ä¼°ä»“åº“çº§åˆ«ä»£ç ç”Ÿæˆï¼Œä¸»è¦å…³æ³¨å¯æ‰§è¡Œæ€§ã€åŠŸèƒ½æ­£ç¡®æ€§å’Œè·¨æ–‡ä»¶ä¸Šä¸‹æ–‡çš„å‡†ç¡®ä½¿ç”¨ã€‚</li>
<li>æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†è¢«ä»‹ç»æ¥å¢å¼ºCodeLLMsåˆ©ç”¨ä¾èµ–å…³ç³»çš„èƒ½åŠ›ã€‚</li>
<li>ä¾èµ–è°ƒç”¨ç‡ï¼ˆDIRï¼‰æ˜¯ä¸€ä¸ªæ–°çš„æŒ‡æ ‡ï¼Œç”¨äºé‡åŒ–ä¸Šä¸‹æ–‡çš„åˆ©ç”¨æƒ…å†µã€‚</li>
<li>é¢„è®­ç»ƒLLMåœ¨æ­£ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè€ŒæŒ‡ä»¤å¾®è°ƒæ¨¡å‹åœ¨ä¸Šä¸‹æ–‡åˆ©ç”¨å’Œè°ƒè¯•èƒ½åŠ›æ–¹é¢æ›´å‡ºè‰²ã€‚</li>
<li>RepoExecæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ¡†æ¶ï¼Œæœ‰åŠ©äºè¯„ä¼°ä»£ç åŠŸèƒ½å’Œä¸å¼€å‘è€…æ„å›¾çš„å¥‘åˆåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.11927">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a37aa4dc45530a3785e58e634b0ab701.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6fdff72ce7fa6063bcf3d2172daa800c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ad20bd94d6f29956d7f763d8a4145204.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bb9b1c3049f5750a7cffdff912252995.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e808b7ae8f032d79be6db8fbcc30453b.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Enhancing-Visual-Language-Modality-Alignment-in-Large-Vision-Language-Models-via-Self-Improvement"><a href="#Enhancing-Visual-Language-Modality-Alignment-in-Large-Vision-Language-Models-via-Self-Improvement" class="headerlink" title="Enhancing Visual-Language Modality Alignment in Large Vision Language   Models via Self-Improvement"></a>Enhancing Visual-Language Modality Alignment in Large Vision Language   Models via Self-Improvement</h2><p><strong>Authors:Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Furong Huang, Cao Xiao</strong></p>
<p>Large vision-language models (LVLMs) have achieved impressive results in visual question-answering and reasoning tasks through vision instruction tuning on specific datasets. However, there remains significant room for improvement in aligning visual and language modalities. Existing methods often depend on external models or data, leading to uncontrollable and unstable alignment results. In this paper, we propose SIMA, a self-improvement framework that enhances visual and language modality alignment without external dependencies. SIMA leverages existing vision instruction tuning datasets to self-generate responses, incorporating an in-context self-critic mechanism that constructs preference pairs for tuning. Crucially, our approach allows LVLMs to act as critics by designing effective critic prompts, eliminating the need for additional fine-tuning with external instruction data. We introduce three novel visual metrics within the self-critic process to guide judgment, significantly improving the accuracy of self-critic. Through extensive experiments across 14 hallucination and comprehensive benchmarks, we demonstrate that SIMA significantly improves LVLMâ€™s performance and outperforms previous approaches, achieving superior modality alignment. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰é€šè¿‡ç‰¹å®šæ•°æ®é›†ä¸Šçš„è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼Œåœ¨è§†è§‰é—®ç­”å’Œæ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚ç„¶è€Œï¼Œåœ¨è§†è§‰å’Œè¯­è¨€æ¨¡æ€å¯¹é½æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå¤–éƒ¨æ¨¡å‹æˆ–æ•°æ®ï¼Œå¯¼è‡´å¯¹é½ç»“æœä¸å¯æ§åˆ¶å’Œä¸ç¨³å®šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†SIMAï¼Œä¸€ä¸ªæ— éœ€ä¾èµ–å¤–éƒ¨å› ç´ å³å¯æé«˜è§†è§‰å’Œè¯­è¨€æ¨¡æ€å¯¹é½çš„è‡ªæˆ‘æ”¹è¿›æ¡†æ¶ã€‚SIMAåˆ©ç”¨ç°æœ‰çš„è§†è§‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†è¿›è¡Œè‡ªæˆ‘ç”Ÿæˆå“åº”ï¼Œå¹¶ç»“åˆä¸Šä¸‹æ–‡è‡ªæˆ‘æ‰¹åˆ¤æœºåˆ¶ï¼Œæ„å»ºåå¥½å¯¹è¿›è¡Œè°ƒæ•´ã€‚å…³é”®çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸LVLMsé€šè¿‡è®¾è®¡æœ‰æ•ˆçš„æ‰¹è¯„æç¤ºæ¥å……å½“æ‰¹è¯„è€…ï¼Œä»è€Œæ— éœ€ä½¿ç”¨å¤–éƒ¨æŒ‡ä»¤æ•°æ®è¿›è¡Œé¢å¤–çš„å¾®è°ƒã€‚æˆ‘ä»¬åœ¨è‡ªæˆ‘æ‰¹åˆ¤è¿‡ç¨‹ä¸­å¼•å…¥äº†ä¸‰ç§æ–°å‹è§†è§‰æŒ‡æ ‡æ¥æŒ‡å¯¼åˆ¤æ–­ï¼Œå¤§å¤§æé«˜äº†è‡ªæˆ‘æ‰¹åˆ¤çš„å‡†ç¡®æ€§ã€‚é€šè¿‡å¯¹1.4ä¸ªå¹»è§‰å’Œå…¨é¢åŸºå‡†çš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†SIMAèƒ½æ˜¾è‘—æé«˜LVLMçš„æ€§èƒ½ï¼Œå¹¶ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œå®ç°äº†å“è¶Šçš„æ¨¡å¼å¯¹é½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.15973v4">PDF</a> NAACL 2025 Findings</p>
<p><strong>Summary</strong><br>å¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰é€šè¿‡ç‰¹å®šæ•°æ®é›†ä¸Šçš„è§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼Œåœ¨è§†è§‰é—®ç­”å’Œæ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚ç„¶è€Œï¼Œä»å­˜åœ¨è§†è§‰å’Œè¯­è¨€æ¨¡æ€å¯¹é½çš„æ”¹è¿›ç©ºé—´ã€‚ç°æœ‰æ–¹æ³•å¸¸ä¾èµ–å¤–éƒ¨æ¨¡å‹æˆ–æ•°æ®ï¼Œå¯¼è‡´å¯¹é½ç»“æœä¸å¯æ§åˆ¶å’Œä¸ç¨³å®šã€‚æœ¬æ–‡æå‡ºSIMAï¼Œä¸€ç§æ— éœ€å¤–éƒ¨ä¾èµ–çš„è‡ªæˆ‘æ”¹è¿›æ¡†æ¶ï¼Œå¢å¼ºè§†è§‰å’Œè¯­è¨€æ¨¡æ€å¯¹é½ã€‚SIMAåˆ©ç”¨ç°æœ‰è§†è§‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†è‡ªæˆ‘ç”Ÿæˆå“åº”ï¼Œå¼•å…¥ä¸Šä¸‹æ–‡è‡ªæˆ‘æ‰¹åˆ¤æœºåˆ¶ï¼Œæ„å»ºåå¥½å¯¹è¿›è¡Œè°ƒæ•´ã€‚é€šè¿‡è®¾è®¡æœ‰æ•ˆçš„æ‰¹åˆ¤æç¤ºï¼Œä½¿LVLMæˆä¸ºæ‰¹åˆ¤è€…ï¼Œæ— éœ€é¢å¤–çš„å¤–éƒ¨æŒ‡ä»¤æ•°æ®ç²¾ç»†è°ƒæ•´ã€‚åœ¨è‡ªæˆ‘æ‰¹åˆ¤è¿‡ç¨‹ä¸­å¼•å…¥ä¸‰é¡¹æ–°é¢–çš„è§†è§‰æŒ‡æ ‡ï¼Œä»¥æŒ‡å¯¼åˆ¤æ–­ï¼Œæ˜¾è‘—æé«˜è‡ªæˆ‘æ‰¹åˆ¤çš„å‡†ç¡®æ€§ã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒéªŒè¯ï¼ŒSIMAæ˜¾è‘—æé«˜äº†LVLMçš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼Œå®ç°äº†ä¼˜è¶Šçš„æ¨¡æ€å¯¹é½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LVLMåœ¨è§†è§‰é—®ç­”å’Œæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä»éœ€æ”¹è¿›è§†è§‰å’Œè¯­è¨€æ¨¡æ€çš„å¯¹é½ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–å¤–éƒ¨æ¨¡å‹å’Œæ•°æ®ï¼Œå¯¼è‡´å¯¹é½ç»“æœä¸ç¨³å®šã€‚</li>
<li>SIMAæ¡†æ¶è¢«æå‡ºï¼Œå®ç°è§†è§‰å’Œè¯­è¨€æ¨¡æ€çš„è‡ªæˆ‘æ”¹è¿›å¯¹é½ï¼Œæ— éœ€å¤–éƒ¨ä¾èµ–ã€‚</li>
<li>SIMAåˆ©ç”¨è§†è§‰æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†è‡ªæˆ‘ç”Ÿæˆå“åº”ï¼Œå¼•å…¥ä¸Šä¸‹æ–‡è‡ªæˆ‘æ‰¹åˆ¤æœºåˆ¶ã€‚</li>
<li>LVLMè¢«è®¾è®¡ä¸ºæ‰¹åˆ¤è€…ï¼Œé€šè¿‡æœ‰æ•ˆçš„æ‰¹åˆ¤æç¤ºè¿›è¡Œè‡ªæˆ‘è°ƒæ•´ï¼Œæ— éœ€é¢å¤–ç²¾ç»†è°ƒæ•´ã€‚</li>
<li>åœ¨è‡ªæˆ‘æ‰¹åˆ¤è¿‡ç¨‹ä¸­å¼•å…¥ä¸‰é¡¹è§†è§‰æŒ‡æ ‡ï¼Œæé«˜åˆ¤æ–­çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.15973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e58bc5bd70e10d97b02ff19849d61982.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-880f88645aa88c18d449df8d28b2b49f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da96408765d5f60f1ab4a85640817986.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-689006edca2617e0156d59f55b0595a7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ed4aa6b104b681873573fda9dcef812c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Alpaca-against-Vicuna-Using-LLMs-to-Uncover-Memorization-of-LLMs"><a href="#Alpaca-against-Vicuna-Using-LLMs-to-Uncover-Memorization-of-LLMs" class="headerlink" title="Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs"></a>Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs</h2><p><strong>Authors:Aly M. Kassem, Omar Mahmoud, Niloofar Mireshghallah, Hyunwoo Kim, Yulia Tsvetkov, Yejin Choi, Sherif Saad, Santu Rana</strong></p>
<p>In this paper, we introduce a black-box prompt optimization method that uses an attacker LLM agent to uncover higher levels of memorization in a victim agent, compared to what is revealed by prompting the target model with the training data directly, which is the dominant approach of quantifying memorization in LLMs. We use an iterative rejection-sampling optimization process to find instruction-based prompts with two main characteristics: (1) minimal overlap with the training data to avoid presenting the solution directly to the model, and (2) maximal overlap between the victim modelâ€™s output and the training data, aiming to induce the victim to spit out training data. We observe that our instruction-based prompts generate outputs with 23.7% higher overlap with training data compared to the baseline prefix-suffix measurements. Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, if not more so, (2) contexts other than the original training data can lead to leakage, and (3) using instructions proposed by other LLMs can open a new avenue of automated attacks that we should further study and explore. The code can be found at <a target="_blank" rel="noopener" href="https://github.com/Alymostafa/Instruction_based_attack">https://github.com/Alymostafa/Instruction_based_attack</a> . </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§é»‘ç›’æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨æ”»å‡»è€…LLMä»£ç†æ¥æ­ç¤ºå—å®³è€…ä»£ç†ä¸­æ›´é«˜å±‚æ¬¡çš„è®°å¿†ï¼Œä¸ç›´æ¥é€šè¿‡è®­ç»ƒæ•°æ®æç¤ºç›®æ ‡æ¨¡å‹ç›¸æ¯”ï¼Œè¿™æ˜¯ç›®å‰åœ¨LLMä¸­é‡åŒ–è®°å¿†çš„ä¸»è¦æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨è¿­ä»£æ‹’ç»é‡‡æ ·ä¼˜åŒ–è¿‡ç¨‹æ¥æ‰¾åˆ°å…·æœ‰ä¸¤ä¸ªä¸»è¦ç‰¹å¾çš„åŸºäºæŒ‡ä»¤çš„æç¤ºï¼šï¼ˆ1ï¼‰ä¸è®­ç»ƒæ•°æ®çš„æœ€å°é‡å ï¼Œä»¥é¿å…ç›´æ¥å‘æ¨¡å‹å‘ˆç°è§£å†³æ–¹æ¡ˆï¼›ï¼ˆ2ï¼‰å—å®³è€…æ¨¡å‹è¾“å‡ºä¸è®­ç»ƒæ•°æ®ä¹‹é—´çš„æœ€å¤§é‡å ï¼Œæ—¨åœ¨è¯±å¯¼å—å®³è€…åå‡ºè®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œä¸åŸºå‡†å‰ç¼€-åç¼€æµ‹é‡ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„åŸºäºæŒ‡ä»¤çš„æç¤ºäº§ç”Ÿäº†ä¸è®­ç»ƒæ•°æ®é‡å é«˜å‡º23.7%çš„è¾“å‡ºã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼šï¼ˆ1ï¼‰æŒ‡ä»¤è°ƒæ•´æ¨¡å‹å¯ä»¥æš´éœ²é¢„è®­ç»ƒæ•°æ®ä¸åŸºç¡€æ¨¡å‹ä¸€æ ·å¤šï¼Œç”šè‡³æ›´å¤šï¼›ï¼ˆ2ï¼‰é™¤äº†åŸå§‹è®­ç»ƒæ•°æ®ä»¥å¤–çš„ä¸Šä¸‹æ–‡ä¹Ÿå¯èƒ½å¯¼è‡´æ³„æ¼ï¼›ï¼ˆ3ï¼‰ä½¿ç”¨å…¶ä»–LLMæå‡ºçš„æŒ‡ä»¤å¯ä»¥ä¸ºæˆ‘ä»¬çš„è‡ªåŠ¨åŒ–æ”»å‡»æ‰“å¼€ä¸€æ¡æ–°é€”å¾„ï¼Œå€¼å¾—æˆ‘ä»¬è¿›ä¸€æ­¥ç ”ç©¶å’Œæ¢ç´¢ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Alymostafa/Instruction_based_attack%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Alymostafa/Instruction_based_attackæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.04801v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æ”»å‡»å‹LLMä»£ç†å‘ç°å—å®³è€…ä»£ç†ä¸­æ›´é«˜å±‚æ¬¡è®°å¿†çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£æ‹’ç»é‡‡æ ·ä¼˜åŒ–è¿‡ç¨‹æ¥å¯»æ‰¾åŸºäºæŒ‡ä»¤çš„æç¤ºï¼Œè¿™äº›æç¤ºå…·æœ‰ä¸¤ä¸ªä¸»è¦ç‰¹ç‚¹ï¼šä¸€æ˜¯ä¸è®­ç»ƒæ•°æ®çš„æœ€å°é‡å ï¼Œä»¥é¿å…ç›´æ¥å‘æ¨¡å‹å‘ˆç°è§£å†³æ–¹æ¡ˆï¼›äºŒæ˜¯å—å®³è€…æ¨¡å‹è¾“å‡ºä¸è®­ç»ƒæ•°æ®ä¹‹é—´çš„æœ€å¤§é‡å ï¼Œæ—¨åœ¨è¯±å¯¼å—å®³è€…æ³„éœ²è®­ç»ƒæ•°æ®ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŸºçº¿å‰ç¼€åç¼€æµ‹é‡ç›¸æ¯”ï¼ŒåŸºäºæŒ‡ä»¤çš„æç¤ºç”Ÿæˆè¾“å‡ºçš„è®­ç»ƒæ•°æ®é‡å ç‡æé«˜äº†23.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥äº†ä¸€ç§é»‘ç›’æç¤ºä¼˜åŒ–æ–¹æ³•ï¼Œä½¿ç”¨æ”»å‡»å‹LLMä»£ç†æ¥æ­ç¤ºå—å®³è€…ä»£ç†ä¸­çš„è®°å¿†ã€‚</li>
<li>é€šè¿‡è¿­ä»£æ‹’ç»é‡‡æ ·ä¼˜åŒ–è¿‡ç¨‹å¯»æ‰¾åŸºäºæŒ‡ä»¤çš„æç¤ºã€‚</li>
<li>åŸºäºæŒ‡ä»¤çš„æç¤ºå…·æœ‰ä¸¤ä¸ªç‰¹ç‚¹ï¼šä¸è®­ç»ƒæ•°æ®çš„æœ€å°é‡å å’Œå—å®³è€…æ¨¡å‹è¾“å‡ºä¸è®­ç»ƒæ•°æ®çš„æœ€å¤§é‡å ã€‚</li>
<li>åŸºäºæŒ‡ä»¤çš„æ¨¡å‹å¯ä»¥æš´éœ²é¢„è®­ç»ƒæ•°æ®ä¸åŸºç¡€æ¨¡å‹ç›¸å½“ç”šè‡³æ›´å¤šçš„æ•°æ®ã€‚</li>
<li>é™¤åŸå§‹è®­ç»ƒæ•°æ®å¤–çš„ä¸Šä¸‹æ–‡ä¹Ÿå¯èƒ½å¯¼è‡´æ³„éœ²ã€‚</li>
<li>ä½¿ç”¨å…¶ä»–LLMæå‡ºçš„æŒ‡ä»¤å¯ä»¥å¼€å¯æ–°çš„è‡ªåŠ¨åŒ–æ”»å‡»é€”å¾„ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶å’Œæ¢ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.04801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-21e4f91c7df734c01a1d9953037c06a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f625dd0d21354c30b103661b6de2112.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7164f3adcba4cce04ea4b40aae6254db.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-02-14\./crop_LLM/2403.04801v3/page_4_0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-815218ba364ff5f67cf21de10b2f485e.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-14/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-14/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-14/Speech/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-471d015769f14450ab3948258e259f0f.jpg" class="responsive-img" alt="Speech">
                        
                        <span class="card-title">Speech</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-14  Contextual Gesture Co-Speech Gesture Video Generation through   Context-aware Gesture Representation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-14
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                    Speech
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Speech/">
                        <span class="chip bg-color">Speech</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-13/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-64ade2d79e4c962dfe3f417da7c73ff1.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-13  What Is That Talk About? A Video-to-Text Summarization Dataset for   Scientific Presentations
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
