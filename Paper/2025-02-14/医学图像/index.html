<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-14  A General Pipeline for Glomerulus Whole-Slide Image Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-fb6803249d476e3ed1a138e741d792b8.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-02-14
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-05-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    41 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-02-14-æ›´æ–°"><a href="#2025-02-14-æ›´æ–°" class="headerlink" title="2025-02-14 æ›´æ–°"></a>2025-02-14 æ›´æ–°</h1><h2 id="A-General-Pipeline-for-Glomerulus-Whole-Slide-Image-Segmentation"><a href="#A-General-Pipeline-for-Glomerulus-Whole-Slide-Image-Segmentation" class="headerlink" title="A General Pipeline for Glomerulus Whole-Slide Image Segmentation"></a>A General Pipeline for Glomerulus Whole-Slide Image Segmentation</h2><p><strong>Authors:Quan Huu Cap</strong></p>
<p>Whole-slide images (WSI) glomerulus segmentation is essential for accurately diagnosing kidney diseases. In this work, we propose a general and practical pipeline for glomerulus segmentation that effectively enhances both patch-level and WSI-level segmentation tasks. Our approach leverages stitching on overlapping patches, increasing the detection coverage, especially when glomeruli are located near patch image borders. In addition, we conduct comprehensive evaluations from different segmentation models across two large and diverse datasets with over 30K glomerulus annotations. Experimental results demonstrate that models using our pipeline outperform the previous state-of-the-art method, achieving superior results across both datasets and setting a new benchmark for glomerulus segmentation in WSIs. The code and pre-trained models are available at <a target="_blank" rel="noopener" href="https://github.com/huuquan1994/wsi_glomerulus_seg">https://github.com/huuquan1994/wsi_glomerulus_seg</a>. </p>
<blockquote>
<p>å…¨æ™¯å›¾åƒï¼ˆWSIï¼‰ä¸­çš„è‚¾å°çƒåˆ†å‰²å¯¹äºå‡†ç¡®è¯Šæ–­è‚¾è„ç–¾ç—…è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºè‚¾å°çƒåˆ†å‰²çš„é€šç”¨å®ç”¨æµç¨‹ï¼Œè¯¥æµç¨‹å¯æœ‰æ•ˆæé«˜è¡¥ä¸çº§åˆ«å’Œå…¨æ™¯å›¾åƒçº§åˆ«çš„åˆ†å‰²ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨é‡å è¡¥ä¸ä¸Šçš„æ‹¼æ¥æŠ€æœ¯ï¼Œæé«˜äº†æ£€æµ‹è¦†ç›–ç‡ï¼Œå°¤å…¶æ˜¯å½“è‚¾å°çƒä½äºè¡¥ä¸å›¾åƒè¾¹ç¼˜æ—¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªå¤§å‹ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šå¯¹ä¸åŒåˆ†å‰²æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæ•°æ®é›†åŒ…å«è¶…è¿‡ä¸‰ä¸‡å¼ è‚¾å°çƒæ³¨é‡Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬æµç¨‹çš„æ¨¡å‹ä¼˜äºä»¥å‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šéƒ½å–å¾—äº†ä¼˜è¶Šçš„ç»“æœï¼Œå¹¶ä¸ºå…¨æ™¯å›¾åƒä¸­çš„è‚¾å°çƒåˆ†å‰²è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/huuquan1994/wsi_glomerulus_seg">https://github.com/huuquan1994/wsi_glomerulus_seg</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.04782v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§é€šç”¨ä¸”å®ç”¨çš„è‚¾å°çƒåˆ†å‰²æµæ°´çº¿ï¼Œå¯æœ‰æ•ˆæå‡è¡¥ä¸çº§åˆ«å’Œå…¨å¹»ç¯ç‰‡çº§åˆ«å›¾åƒçš„è‚¾å°çƒåˆ†å‰²ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡é‡å è¡¥ä¸çš„æ‹¼æ¥ï¼Œæé«˜äº†æ£€æµ‹è¦†ç›–ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è‚¾å°çƒä½äºè¡¥ä¸å›¾åƒè¾¹ç¼˜æ—¶ã€‚åŒæ—¶ï¼Œæœ¬æ–‡åœ¨ä¸¤ä¸ªå¤§å‹ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼ŒåŒ…å«è¶…è¿‡3ä¸‡å¼ è‚¾å°çƒæ ‡æ³¨å›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨æœ¬æ–‡æµæ°´çº¿çš„æ¨¡å‹ä¼˜äºå…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šéƒ½å–å¾—äº†ä¼˜å¼‚çš„ç»“æœï¼Œä¸ºå…¨å¹»ç¯ç‰‡å›¾åƒä¸­çš„è‚¾å°çƒåˆ†å‰²è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§ç”¨äºè‚¾å°çƒåˆ†å‰²çš„é€šç”¨å’Œå®ç”¨æµæ°´çº¿ã€‚</li>
<li>é€šè¿‡é‡å è¡¥ä¸çš„æ‹¼æ¥ï¼Œæé«˜äº†è‚¾å°çƒæ£€æµ‹çš„è¦†ç›–ç‡ã€‚</li>
<li>æµæ°´çº¿è®¾è®¡æœ‰æ•ˆæå‡äº†è¡¥ä¸çº§åˆ«å’Œå…¨å¹»ç¯ç‰‡çº§åˆ«å›¾åƒçš„è‚¾å°çƒåˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>åœ¨ä¸¤ä¸ªå¤§å‹ã€å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜è¯¥æµæ°´çº¿æ¨¡å‹åœ¨åˆ†å‰²æ•ˆæœä¸Šä¼˜äºå…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>å…¬å¼€äº†ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.04782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b225b530c92de6935b42a692d1cdc2e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-735258659471fa25b4477d440c6a35e9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52719337264bc0418ee0387bae11b48f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-68528d032f9f83094f627cce38b47500.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f27dff63f51291b162b311b597b90a30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f86a34fa905245951b0d3eb73027e7d7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b745907958a21895a61116f078c16f02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f2f67623ac46bd5d51f3c3f5a1430d9.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="UNSURE-self-supervised-learning-with-Unknown-Noise-level-and-Steinâ€™s-Unbiased-Risk-Estimate"><a href="#UNSURE-self-supervised-learning-with-Unknown-Noise-level-and-Steinâ€™s-Unbiased-Risk-Estimate" class="headerlink" title="UNSURE: self-supervised learning with Unknown Noise level and Steinâ€™s   Unbiased Risk Estimate"></a>UNSURE: self-supervised learning with Unknown Noise level and Steinâ€™s   Unbiased Risk Estimate</h2><p><strong>Authors:JuliÃ¡n Tachella, Mike Davies, Laurent Jacques</strong></p>
<p>Recently, many self-supervised learning methods for image reconstruction have been proposed that can learn from noisy data alone, bypassing the need for ground-truth references. Most existing methods cluster around two classes: i) Steinâ€™s Unbiased Risk Estimate (SURE) and similar approaches that assume full knowledge of the noise distribution, and ii) Noise2Self and similar cross-validation methods that require very mild knowledge about the noise distribution. The first class of methods tends to be impractical, as the noise level is often unknown in real-world applications, and the second class is often suboptimal compared to supervised learning. In this paper, we provide a theoretical framework that characterizes this expressivity-robustness trade-off and propose a new approach based on SURE, but unlike the standard SURE, does not require knowledge about the noise level. Throughout a series of experiments, we show that the proposed estimator outperforms other existing self-supervised methods on various imaging inverse problems. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œè®¸å¤šç”¨äºå›¾åƒé‡å»ºçš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹æ³•å·²è¢«æå‡ºï¼Œè¿™äº›å­¦ä¹ æ–¹æ³•å¯ä»¥ä»å•ç‹¬çš„å™ªå£°æ•°æ®ä¸­å­¦ä¹ ï¼Œæ— éœ€çœŸå®å‚è€ƒæ•°æ®ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•é›†ä¸­åœ¨ä¸¤ç±»ä¸Šï¼ši) Steinçš„æ— åé£é™©ä¼°è®¡ï¼ˆSUREï¼‰å’Œå‡è®¾å¯¹å™ªå£°åˆ†å¸ƒæœ‰å……åˆ†äº†è§£çš„ç±»ä¼¼æ–¹æ³•ï¼›ä»¥åŠii) Noise2Selfå’Œéœ€è¦è½»å¾®äº†è§£å™ªå£°åˆ†å¸ƒçš„ç±»ä¼¼äº¤å‰éªŒè¯æ–¹æ³•ã€‚ç¬¬ä¸€ç±»æ–¹æ³•å¾€å¾€ä¸åˆ‡å®é™…ï¼Œå› ä¸ºåœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­ï¼Œå™ªå£°æ°´å¹³é€šå¸¸æ˜¯æœªçŸ¥çš„ï¼›è€Œç¬¬äºŒç±»æ–¹æ³•ä¸ç›‘ç£å­¦ä¹ ç›¸æ¯”å¾€å¾€è¡¨ç°ä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç†è®ºæ¡†æ¶ï¼Œæè¿°äº†è¿™ç§è¡¨è¾¾æ€§ç¨³å¥æ€§æƒè¡¡çš„ç‰¹ç‚¹ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºSUREçš„æ–°æ–¹æ³•ï¼Œä½†ä¸æ ‡å‡†çš„SUREä¸åŒçš„æ˜¯ï¼Œå®ƒä¸éœ€è¦äº†è§£å™ªå£°æ°´å¹³ã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºçš„ä¼°è®¡å™¨åœ¨å„ç§æˆåƒé€†é—®é¢˜ä¸Šä¼˜äºå…¶ä»–ç°æœ‰çš„è‡ªç›‘ç£æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.01985v4">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºSteinçš„æ— åé£é™©ä¼°è®¡ï¼ˆSUREï¼‰çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ç”¨äºå›¾åƒé‡å»ºï¼Œæ— éœ€äº†è§£å™ªå£°æ°´å¹³ä¿¡æ¯å³å¯å®ç°è‡ªç›‘ç£å­¦ä¹ ï¼Œå¹¶åœ¨å„ç§æˆåƒé€†é—®é¢˜ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« è®¨è®ºäº†ç°æœ‰çš„è‡ªç›‘ç£å­¦ä¹ åœ¨å›¾åƒé‡å»ºä¸­çš„åº”ç”¨ï¼ŒåŒ…æ‹¬åŸºäºSteinçš„Unbiased Risk Estimateçš„æ–¹æ³•å’ŒNoise2Selfç­‰äº¤å‰éªŒè¯æ–¹æ³•ã€‚</li>
<li>ç¬¬ä¸€ç§æ–¹æ³•éœ€è¦äº†è§£å™ªå£°åˆ†å¸ƒçš„å…¨è²Œï¼Œè¿™åœ¨ç°å®åº”ç”¨ä¸­å¾€å¾€éš¾ä»¥å®ç°ï¼›ç¬¬äºŒç§æ–¹æ³•ç›¸è¾ƒäºæœ‰ç›‘ç£å­¦ä¹ è¡¨ç°å¸¸æ¬ ä½³ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºç†è®ºæ¡†æ¶çš„è¡¨å™çµæ´»æ€§ä¸ç¨³å¥æ€§ä¹‹é—´çš„æƒè¡¡ï¼Œå¹¶ä¸ºä¸€ç§æ–°çš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•æä¾›äº†ç†è®ºåŸºç¡€ã€‚</li>
<li>æ–°æ–¹æ³•ä¸æ ‡å‡†çš„SUREæ–¹æ³•ä¸åŒï¼Œæ— éœ€äº†è§£å™ªå£°æ°´å¹³ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡ä¸€ç³»åˆ—å®éªŒéªŒè¯ï¼Œæ–°æ–¹æ³•åœ¨å„ç§æˆåƒé€†é—®é¢˜ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–ç°æœ‰è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>æ­¤æ–¹æ³•èƒ½å¤Ÿåœ¨æ— å‚è€ƒçœŸå®æ•°æ®çš„æƒ…å†µä¸‹å­¦ä¹ ä»å™ªå£°æ•°æ®ä¸­é‡å»ºå›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.01985">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4de73cb9130d7028d92a32434acdc8e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-adf5a04399bf9bd8d3f6a101d2c05b34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e499061f566eb2349833dfe526aaf32.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Visual-Prompt-Engineering-for-Vision-Language-Models-in-Radiology"><a href="#Visual-Prompt-Engineering-for-Vision-Language-Models-in-Radiology" class="headerlink" title="Visual Prompt Engineering for Vision Language Models in Radiology"></a>Visual Prompt Engineering for Vision Language Models in Radiology</h2><p><strong>Authors:Stefan Denner, Markus Bujotzek, Dimitrios Bounias, David Zimmerer, Raphael Stock, Klaus Maier-Hein</strong></p>
<p>Medical image classification plays a crucial role in clinical decision-making, yet most models are constrained to a fixed set of predefined classes, limiting their adaptability to new conditions. Contrastive Language-Image Pretraining (CLIP) offers a promising solution by enabling zero-shot classification through multimodal large-scale pretraining. However, while CLIP effectively captures global image content, radiology requires a more localized focus on specific pathology regions to enhance both interpretability and diagnostic accuracy. To address this, we explore the potential of incorporating visual cues into zero-shot classification, embedding visual markers $\unicode{x2013}$ such as arrows, bounding boxes, and circles $\unicode{x2013}$ directly into radiological images to guide model attention. Evaluating across four public chest X-ray datasets, we demonstrate that visual markers improve AUROC by up to 0.185, highlighting their effectiveness in enhancing classification performance. Furthermore, attention map analysis confirms that visual cues help models focus on clinically relevant areas, leading to more interpretable predictions. To support further research, we use public datasets and will release our code and preprocessing pipeline, providing a reference point for future work on localized classification in medical imaging. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†ç±»åœ¨ä¸´åºŠå†³ç­–ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹ä»…é™äºä¸€ç»„é¢„å®šä¹‰çš„ç±»åˆ«ï¼Œè¿™ä½¿å¾—å®ƒä»¬å¯¹æ–°æ¡ä»¶çš„é€‚åº”æ€§å—åˆ°é™åˆ¶ã€‚å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰é€šè¿‡å¤šæ¨¡å¼å¤§è§„æ¨¡é¢„è®­ç»ƒæä¾›é›¶æ ·æœ¬åˆ†ç±»çš„æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œè™½ç„¶CLIPæœ‰æ•ˆåœ°æ•è·äº†å…¨å±€å›¾åƒå†…å®¹ï¼Œä½†æ”¾å°„å­¦éœ€è¦æ›´åŠ ä¾§é‡äºç‰¹å®šç—…ç†åŒºåŸŸçš„æœ¬åœ°åŒ–å…³æ³¨ï¼Œä»¥æé«˜è§£é‡Šèƒ½åŠ›å’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å°†è§†è§‰çº¿ç´¢èå…¥é›¶æ ·æœ¬åˆ†ç±»çš„æ½œåŠ›ï¼Œé€šè¿‡åœ¨æ”¾å°„å›¾åƒä¸­åµŒå…¥è§†è§‰æ ‡è®°ï¼ˆå¦‚ç®­å¤´ã€è¾¹ç•Œæ¡†å’Œåœ†åœˆï¼‰æ¥å¼•å¯¼æ¨¡å‹æ³¨æ„åŠ›ã€‚åœ¨å››ä¸ªå…¬å…±èƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè§†è§‰æ ‡è®°æé«˜äº†AUROCå€¼é«˜è¾¾0.185ï¼Œçªæ˜¾äº†å®ƒä»¬åœ¨æé«˜åˆ†ç±»æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæ³¨æ„åŠ›åœ°å›¾åˆ†æè¯å®ï¼Œè§†è§‰çº¿ç´¢æœ‰åŠ©äºæ¨¡å‹å…³æ³¨ä¸´åºŠç›¸å…³åŒºåŸŸï¼Œä»è€Œåšå‡ºæ›´å¯è§£é‡Šçš„é¢„æµ‹ã€‚ä¸ºäº†æ”¯æŒè¿›ä¸€æ­¥ç ”ç©¶ï¼Œæˆ‘ä»¬ä½¿ç”¨å…¬å…±æ•°æ®é›†ï¼Œå¹¶å°†å‘å¸ƒæˆ‘ä»¬çš„ä»£ç å’Œé¢„å¤„ç†ç®¡é“ï¼Œä¸ºæœªæ¥åŒ»å­¦æˆåƒä¸­å±€éƒ¨åŒ–åˆ†ç±»çš„ç ”ç©¶æä¾›å‚è€ƒç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.15802v2">PDF</a> Accepted at ECCV 2024 Workshop on Emergent Visual Abilities and   Limits of Foundation Models</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒåˆ†ç±»åœ¨ä¸´åºŠå†³ç­–ä¸­è‡³å…³é‡è¦ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹å—é™äºé¢„è®¾ç±»åˆ«ï¼Œéš¾ä»¥é€‚åº”æ–°æƒ…å†µã€‚å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰é€šè¿‡å¤šæ¨¡æ€å¤§è§„æ¨¡é¢„è®­ç»ƒå®ç°é›¶æ ·æœ¬åˆ†ç±»ï¼Œä¸ºè§£å†³æ­¤é—®é¢˜æä¾›äº†å¸Œæœ›ã€‚ç„¶è€Œï¼ŒCLIPä¸»è¦æ•æ‰å›¾åƒå…¨å±€å†…å®¹ï¼Œè€Œæ”¾å°„å­¦éœ€è¦æ›´ä¾§é‡äºç‰¹å®šç—…ç†åŒºåŸŸçš„è§†è§‰æç¤ºï¼Œä»¥æé«˜è§£é‡Šæ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚æœ¬ç ”ç©¶æ¢ç´¢å°†è§†è§‰æç¤ºèå…¥é›¶æ ·æœ¬åˆ†ç±»çš„æ½œåŠ›ï¼Œé€šè¿‡åœ¨æ”¾å°„å›¾åƒä¸­åµŒå…¥è§†è§‰æ ‡è®°ï¼ˆå¦‚ç®­å¤´ã€è¾¹ç•Œæ¡†å’Œåœ†åœˆï¼‰æ¥å¼•å¯¼æ¨¡å‹æ³¨æ„åŠ›ã€‚åœ¨å››ä¸ªå…¬å…±èƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œè§†è§‰æ ‡è®°æé«˜äº†AUROCå€¼è¾¾0.185ï¼Œè¯æ˜äº†å…¶åœ¨æé«˜åˆ†ç±»æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæ³¨æ„åŠ›åœ°å›¾åˆ†æè¯å®è§†è§‰çº¿ç´¢æœ‰åŠ©äºæ¨¡å‹å…³æ³¨ä¸´åºŠç›¸å…³åŒºåŸŸï¼Œäº§ç”Ÿæ›´å¯è§£é‡Šçš„é¢„æµ‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†ç±»åœ¨ä¸´åºŠå†³ç­–ä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œä½†æ¨¡å‹å—é™äºé¢„è®¾ç±»åˆ«ï¼Œéœ€è¦é€‚åº”æ–°æƒ…å†µçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰å¯ä»¥å®ç°é›¶æ ·æœ¬åˆ†ç±»ï¼Œä½†å…¶åœ¨æ•æ‰å›¾åƒå…¨å±€å†…å®¹æ–¹é¢æœ‰æ‰€å±€é™ã€‚</li>
<li>æ”¾å°„å­¦éœ€è¦æ›´å…³æ³¨ç‰¹å®šç—…ç†åŒºåŸŸçš„è§†è§‰æç¤ºï¼Œä»¥æé«˜è§£é‡Šæ€§å’Œè¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡åœ¨æ”¾å°„å›¾åƒä¸­åµŒå…¥è§†è§‰æ ‡è®°ï¼ˆå¦‚ç®­å¤´ã€è¾¹ç•Œæ¡†å’Œåœ†åœˆï¼‰æ¥å¼•å¯¼æ¨¡å‹æ³¨æ„åŠ›æ˜¯ä¸€ç§æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>åœ¨å››ä¸ªå…¬å…±èƒ¸éƒ¨Xå°„çº¿æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè§†è§‰æ ‡è®°æé«˜äº†åˆ†ç±»æ€§èƒ½ã€‚</li>
<li>æ³¨æ„åŠ›åœ°å›¾åˆ†æè¯å®è§†è§‰çº¿ç´¢æœ‰åŠ©äºæ¨¡å‹å…³æ³¨ä¸´åºŠç›¸å…³åŒºåŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.15802">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fb6803249d476e3ed1a138e741d792b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0bcfeac055c90e5dec3328a82969b2bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a393ae5ea5d0d3d54efb0070aeda083c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Exploiting-Precision-Mapping-and-Component-Specific-Feature-Enhancement-for-Breast-Cancer-Segmentation-and-Identification"><a href="#Exploiting-Precision-Mapping-and-Component-Specific-Feature-Enhancement-for-Breast-Cancer-Segmentation-and-Identification" class="headerlink" title="Exploiting Precision Mapping and Component-Specific Feature Enhancement   for Breast Cancer Segmentation and Identification"></a>Exploiting Precision Mapping and Component-Specific Feature Enhancement   for Breast Cancer Segmentation and Identification</h2><p><strong>Authors:Pandiyaraju V, Shravan Venkatraman, Pavan Kumar S, Santhosh Malarvannan, Kannan A</strong></p>
<p>Breast cancer is one of the leading causes of death globally, and thus there is an urgent need for early and accurate diagnostic techniques. Although ultrasound imaging is a widely used technique for breast cancer screening, it faces challenges such as poor boundary delineation caused by variations in tumor morphology and reduced diagnostic accuracy due to inconsistent image quality. To address these challenges, we propose novel Deep Learning (DL) frameworks for breast lesion segmentation and classification. We introduce a precision mapping mechanism (PMM) for a precision mapping and attention-driven LinkNet (PMAD-LinkNet) segmentation framework that dynamically adapts spatial mappings through morphological variation analysis, enabling precise pixel-level refinement of tumor boundaries. Subsequently, we introduce a component-specific feature enhancement module (CSFEM) for a component-specific feature-enhanced classifier (CSFEC-Net). Through a multi-level attention approach, the CSFEM magnifies distinguishing features of benign, malignant, and normal tissues. The proposed frameworks are evaluated against existing literature and a diverse set of state-of-the-art Convolutional Neural Network (CNN) architectures. The obtained results show that our segmentation model achieves an accuracy of 98.1%, an IoU of 96.9%, and a Dice Coefficient of 97.2%. For the classification model, an accuracy of 99.2% is achieved with F1-score, precision, and recall values of 99.1%, 99.3%, and 99.1%, respectively. </p>
<blockquote>
<p>ä¹³è…ºç™Œæ˜¯å…¨çƒä¸»è¦çš„è‡´æ­»åŸå› ä¹‹ä¸€ï¼Œå› æ­¤äºŸéœ€æ—©æœŸå‡†ç¡®è¯Šæ–­æŠ€æœ¯ã€‚è¶…å£°æˆåƒè™½ç„¶å¹¿æ³›åº”ç”¨äºä¹³è…ºç™Œç­›æŸ¥ï¼Œä½†ä»é¢ä¸´å› è‚¿ç˜¤å½¢æ€å˜åŒ–å¯¼è‡´çš„è¾¹ç•Œç•Œå®šä¸æ¸…ä»¥åŠå›¾åƒè´¨é‡ä¸ä¸€è‡´å¯¼è‡´çš„è¯Šæ–­å‡†ç¡®æ€§é™ä½ç­‰æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºä¹³è…ºç—…ç¶åˆ†å‰²å’Œåˆ†ç±»çš„æ–°å‹æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¡†æ¶ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç²¾åº¦æ˜ å°„æœºåˆ¶ï¼ˆPMMï¼‰ï¼Œç”¨äºç²¾åº¦æ˜ å°„å’Œæ³¨æ„åŠ›é©±åŠ¨LinkNetï¼ˆPMAD-LinkNetï¼‰åˆ†å‰²æ¡†æ¶ï¼Œé€šè¿‡å½¢æ€å˜åŒ–åˆ†æåŠ¨æ€é€‚åº”ç©ºé—´æ˜ å°„ï¼Œå®ç°å¯¹è‚¿ç˜¤è¾¹ç•Œçš„ç²¾ç¡®åƒç´ çº§ç»†åŒ–ã€‚æ¥ç€ï¼Œæˆ‘ä»¬é’ˆå¯¹ç»„ä»¶ç‰¹å®šç‰¹å¾å¢å¼ºåˆ†ç±»å™¨ï¼ˆCSFEC-Netï¼‰å¼•å…¥äº†ä¸€ä¸ªç»„ä»¶ç‰¹å®šç‰¹å¾å¢å¼ºæ¨¡å—ï¼ˆCSFEMï¼‰ã€‚é€šè¿‡å¤šçº§æ³¨æ„åŠ›æ–¹æ³•ï¼ŒCSFEMæ”¾å¤§äº†è‰¯æ€§ã€æ¶æ€§åŠæ­£å¸¸ç»„ç»‡çš„åŒºåˆ«ç‰¹å¾ã€‚æ‰€æå‡ºçš„æ¡†æ¶ä¸ç°æœ‰æ–‡çŒ®å’Œä¸€ç³»åˆ—å…ˆè¿›çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¶æ„è¿›è¡Œäº†è¯„ä¼°æ¯”è¾ƒã€‚è·å¾—çš„ç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„åˆ†å‰²æ¨¡å‹å‡†ç¡®ç‡è¾¾åˆ°äº†98.1%ï¼ŒIoUä¸º96.9%ï¼ŒDiceç³»æ•°ä¸º97.2%ã€‚åˆ†ç±»æ¨¡å‹åˆ™å®ç°äº†99.2%çš„å‡†ç¡®ç‡ï¼ŒF1åˆ†æ•°ã€ç²¾ç¡®åº¦å’Œå¬å›ç‡åˆ†åˆ«ä¸º99.1%ã€99.3%å’Œ99.1%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.02844v6">PDF</a> 27 pages, 18 figures, 6 tables</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹ä¹³è…ºç™Œè¶…å£°æˆåƒä¸­é¢ä¸´çš„è¾¹ç•Œæ¨¡ç³Šå’Œè¯Šæ–­å‡†ç¡®æ€§é—®é¢˜ï¼Œæå‡ºåŸºäºæ·±åº¦å­¦ä¹ çš„æ–°æ¡†æ¶è¿›è¡Œç—…ç¶åˆ†å‰²å’Œåˆ†ç±»ã€‚é€šè¿‡ç²¾ç¡®æ˜ å°„æœºåˆ¶å’Œç»„ä»¶ç‰¹å®šç‰¹å¾å¢å¼ºæ¨¡å—ï¼Œå®ç°å¯¹è‚¿ç˜¤è¾¹ç•Œçš„ç²¾ç¡®åƒç´ çº§åˆ†å‰²å’Œå¯¹è‰¯æ¶æ€§åŠæ­£å¸¸ç»„ç»‡çš„ç‰¹å¾å¢å¼ºåˆ†ç±»ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œåˆ†å‰²æ¨¡å‹å‡†ç¡®ç‡ä¸º98.1%ï¼ŒIoUä¸º96.9%ï¼ŒDiceç³»æ•°ä¸º97.2%ï¼›åˆ†ç±»æ¨¡å‹å‡†ç¡®ç‡ä¸º99.2%ï¼ŒF1åˆ†æ•°ã€ç²¾ç¡®åº¦å’Œå¬å›ç‡åˆ†åˆ«ä¸º99.1%ã€99.3%å’Œ99.1%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä¹³è…ºç™Œæ˜¯å…¨çƒä¸»è¦çš„æ­»äº¡åŸå› ä¹‹ä¸€ï¼Œéœ€è¦æ—©æœŸå’Œå‡†ç¡®çš„è¯Šæ–­æŠ€æœ¯ã€‚</li>
<li>è¶…å£°æˆåƒåœ¨ä¹³è…ºç™Œç­›æŸ¥ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†å­˜åœ¨è¾¹ç•Œæ¨¡ç³Šå’Œå›¾åƒè´¨é‡ä¸ä¸€è‡´ç­‰æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥åŸºäºæ·±åº¦å­¦ä¹ çš„æ–°æ¡†æ¶ï¼ŒåŒ…æ‹¬ç²¾ç¡®æ˜ å°„æœºåˆ¶å’Œç»„ä»¶ç‰¹å®šç‰¹å¾å¢å¼ºæ¨¡å—ï¼Œä»¥æé«˜è¯Šæ–­å‡†ç¡®æ€§ã€‚</li>
<li>ç²¾ç¡®æ˜ å°„æœºåˆ¶èƒ½å¤ŸåŠ¨æ€é€‚åº”ç©ºé—´æ˜ å°„ï¼Œå®ç°è‚¿ç˜¤è¾¹ç•Œçš„ç²¾ç¡®åƒç´ çº§åˆ†å‰²ã€‚</li>
<li>ç»„ä»¶ç‰¹å®šç‰¹å¾å¢å¼ºæ¨¡å—é€šè¿‡å¤šçº§åˆ«æ³¨æ„åŠ›æ–¹æ³•æ”¾å¤§è‰¯æ¶æ€§åŠæ­£å¸¸ç»„ç»‡é—´çš„ç‰¹å¾å·®å¼‚ã€‚</li>
<li>åˆ†å‰²æ¨¡å‹å‡†ç¡®ç‡é«˜è¾¾98.1%ï¼ŒIoUä¸º96.9%ï¼ŒDiceç³»æ•°ä¸º97.2%ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.02844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3e5a1e588af5ef14b95acb729625f1ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e73a377ac493de01906c5aad1842257f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="LOGCAN-Adaptive-Local-global-class-aware-network-for-semantic-segmentation-of-remote-sensing-imagery"><a href="#LOGCAN-Adaptive-Local-global-class-aware-network-for-semantic-segmentation-of-remote-sensing-imagery" class="headerlink" title="LOGCAN++: Adaptive Local-global class-aware network for semantic   segmentation of remote sensing imagery"></a>LOGCAN++: Adaptive Local-global class-aware network for semantic   segmentation of remote sensing imagery</h2><p><strong>Authors:Xiaowen Ma, Rongrong Lian, Zhenkai Wu, Hongbo Guo, Mengting Ma, Sensen Wu, Zhenhong Du, Siyang Song, Wei Zhang</strong></p>
<p>Remote sensing images usually characterized by complex backgrounds, scale and orientation variations, and large intra-class variance. General semantic segmentation methods usually fail to fully investigate the above issues, and thus their performances on remote sensing image segmentation are limited. In this paper, we propose our LOGCAN++, a semantic segmentation model customized for remote sensing images, which is made up of a Global Class Awareness (GCA) module and several Local Class Awareness (LCA) modules. The GCA module captures global representations for class-level context modeling to reduce the interference of background noise. The LCA module generates local class representations as intermediate perceptual elements to indirectly associate pixels with the global class representations, targeting at dealing with the large intra-class variance problem. In particular, we introduce affine transformations in the LCA module for adaptive extraction of local class representations to effectively tolerate scale and orientation variations in remotely sensed images. Extensive experiments on three benchmark datasets show that our LOGCAN++ outperforms current mainstream general and remote sensing semantic segmentation methods and achieves a better trade-off between speed and accuracy. Code is available at <a target="_blank" rel="noopener" href="https://github.com/xwmaxwma/rssegmentation">https://github.com/xwmaxwma/rssegmentation</a>. </p>
<blockquote>
<p>é¥æ„Ÿå›¾åƒé€šå¸¸å…·æœ‰å¤æ‚çš„èƒŒæ™¯ã€å°ºåº¦å’Œæ–¹å‘å˜åŒ–ä»¥åŠè¾ƒå¤§çš„ç±»å†…å·®å¼‚ã€‚ä¸€èˆ¬çš„è¯­ä¹‰åˆ†å‰²æ–¹æ³•é€šå¸¸æ— æ³•å®Œå…¨è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå› æ­¤åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²æ–¹é¢çš„æ€§èƒ½æœ‰é™ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹é¥æ„Ÿå›¾åƒçš„è¯­ä¹‰åˆ†å‰²æ¨¡å‹LOGCAN++ï¼Œå®ƒç”±å…¨å±€ç±»æ„è¯†ï¼ˆGCAï¼‰æ¨¡å—å’Œå¤šä¸ªå±€éƒ¨ç±»æ„è¯†ï¼ˆLCAï¼‰æ¨¡å—ç»„æˆã€‚GCAæ¨¡å—æ•è·å…¨å±€è¡¨ç¤ºæ¥è¿›è¡Œç±»ä¸Šä¸‹æ–‡å»ºæ¨¡ï¼Œä»¥å‡å°‘èƒŒæ™¯å™ªå£°çš„å¹²æ‰°ã€‚LCAæ¨¡å—ç”Ÿæˆå±€éƒ¨ç±»è¡¨ç¤ºä½œä¸ºä¸­é—´æ„ŸçŸ¥å…ƒç´ ï¼Œé—´æ¥åœ°å°†åƒç´ ä¸å…¨å±€ç±»è¡¨ç¤ºå…³è”èµ·æ¥ï¼Œä»¥å¤„ç†è¾ƒå¤§çš„ç±»å†…å·®å¼‚é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åœ¨LCAæ¨¡å—ä¸­å¼•å…¥äº†ä»¿å°„å˜æ¢ï¼Œä»¥è¿›è¡Œå±€éƒ¨ç±»è¡¨ç¤ºçš„è‡ªé€‚åº”æå–ï¼Œä»è€Œæœ‰æ•ˆåœ°å®¹å¿é¥æ„Ÿå›¾åƒä¸­çš„å°ºåº¦å’Œæ–¹å‘å˜åŒ–ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„LOGCAN++åœ¨é€Ÿåº¦å’Œå‡†ç¡®æ€§ä¹‹é—´å–å¾—äº†æ›´å¥½çš„æƒè¡¡ï¼Œå¹¶è¶…è¶Šäº†å½“å‰ä¸»æµçš„ä¸€èˆ¬å’Œé¥æ„Ÿè¯­ä¹‰åˆ†å‰²æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xwmaxwma/rssegmentation%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xwmaxwma/rssegmentationä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16502v3">PDF</a> Accepted by TGRS2025</p>
<p><strong>Summary</strong><br>è¿œç¨‹é¥æ„Ÿå›¾åƒå…·æœ‰å¤æ‚èƒŒæ™¯ã€å°ºåº¦ä¸æ–¹å‘å˜åŒ–ä»¥åŠå¤§ç±»å†…å·®å¼‚å¤§çš„ç‰¹ç‚¹ã€‚é€šç”¨è¯­ä¹‰åˆ†å‰²æ–¹æ³•éš¾ä»¥å……åˆ†åº”å¯¹ä¸Šè¿°é—®é¢˜ï¼Œåœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„è¡¨ç°å—é™ã€‚æœ¬æ–‡æå‡ºé’ˆå¯¹é¥æ„Ÿå›¾åƒçš„å®šåˆ¶è¯­ä¹‰åˆ†å‰²æ¨¡å‹LOGCAN++ï¼ŒåŒ…æ‹¬å…¨å±€ç±»åˆ«æ„ŸçŸ¥æ¨¡å—å’Œå¤šä¸ªå±€éƒ¨ç±»åˆ«æ„ŸçŸ¥æ¨¡å—ã€‚å…¨å±€ç±»åˆ«æ„ŸçŸ¥æ¨¡å—æ•æ‰å…¨å±€ç±»åˆ«ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œé™ä½èƒŒæ™¯å™ªå£°å¹²æ‰°ï¼›å±€éƒ¨ç±»åˆ«æ„ŸçŸ¥æ¨¡å—ç”Ÿæˆå±€éƒ¨ç±»åˆ«è¡¨ç¤ºï¼Œä½œä¸ºä¸­é—´æ„ŸçŸ¥å…ƒç´ é—´æ¥å…³è”åƒç´ ä¸å…¨å±€ç±»åˆ«è¡¨ç¤ºï¼Œä»¥è§£å†³å¤§ç±»å†…å·®å¼‚é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯æˆ‘ä»¬åœ¨å±€éƒ¨ç±»åˆ«æ„ŸçŸ¥æ¨¡å—ä¸­å¼•å…¥äº†ä»¿å°„å˜æ¢ï¼Œç”¨äºè‡ªé€‚åº”æå–å±€éƒ¨ç±»åˆ«è¡¨ç¤ºï¼Œæœ‰æ•ˆå®¹å¿é¥æ„Ÿå›¾åƒä¸­çš„å°ºåº¦å’Œæ–¹å‘å˜åŒ–ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„LOGCAN++åœ¨ä¸»æµé€šç”¨å’Œé¥æ„Ÿè¯­ä¹‰åˆ†å‰²æ–¹æ³•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå®ç°äº†é€Ÿåº¦å’Œå‡†ç¡®åº¦çš„è‰¯å¥½å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¥æ„Ÿå›¾åƒå…·æœ‰å¤æ‚èƒŒæ™¯ã€å°ºåº¦ä¸æ–¹å‘å˜åŒ–ä»¥åŠå¤§ç±»å†…å·®å¼‚å¤§çš„ç‰¹æ€§ã€‚</li>
<li>é€šç”¨è¯­ä¹‰åˆ†å‰²æ–¹æ³•åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­è¡¨ç°å—é™ã€‚</li>
<li>LOGCAN++æ˜¯ä¸€ä¸ªé’ˆå¯¹é¥æ„Ÿå›¾åƒçš„å®šåˆ¶è¯­ä¹‰åˆ†å‰²æ¨¡å‹ï¼ŒåŒ…å«å…¨å±€ç±»åˆ«æ„ŸçŸ¥æ¨¡å—å’Œå±€éƒ¨ç±»åˆ«æ„ŸçŸ¥æ¨¡å—ã€‚</li>
<li>å…¨å±€ç±»åˆ«æ„ŸçŸ¥æ¨¡å—æ•æ‰å…¨å±€ç±»åˆ«ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œé™ä½èƒŒæ™¯å™ªå£°å¹²æ‰°ã€‚</li>
<li>å±€éƒ¨ç±»åˆ«æ„ŸçŸ¥æ¨¡å—è§£å†³å¤§ç±»å†…å·®å¼‚é—®é¢˜ï¼Œé€šè¿‡ç”Ÿæˆå±€éƒ¨ç±»åˆ«è¡¨ç¤ºé—´æ¥å…³è”åƒç´ ä¸å…¨å±€ç±»åˆ«è¡¨ç¤ºã€‚</li>
<li>å±€éƒ¨ç±»åˆ«æ„ŸçŸ¥æ¨¡å—ä¸­å¼•å…¥ä»¿å°„å˜æ¢ï¼Œç”¨äºè‡ªé€‚åº”æå–å±€éƒ¨ç±»åˆ«è¡¨ç¤ºï¼Œæœ‰æ•ˆåº”å¯¹é¥æ„Ÿå›¾åƒä¸­çš„å°ºåº¦å’Œæ–¹å‘å˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.16502">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d220bf93655691a8bcc6f23f6d46ac09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4925ec0f7ce95e87a1d80ccdd1c31731.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd374110d73ec55f2cf46f420c485897.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-708bae23da1df723a0f36f3b315386e0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Weakly-Supervised-PET-Anomaly-Detection-using-Implicitly-Guided-Attention-Conditional-Counterfactual-Diffusion-Modeling-a-Multi-Center-Multi-Cancer-and-Multi-Tracer-Study"><a href="#Weakly-Supervised-PET-Anomaly-Detection-using-Implicitly-Guided-Attention-Conditional-Counterfactual-Diffusion-Modeling-a-Multi-Center-Multi-Cancer-and-Multi-Tracer-Study" class="headerlink" title="Weakly-Supervised PET Anomaly Detection using Implicitly-Guided   Attention-Conditional Counterfactual Diffusion Modeling: a Multi-Center,   Multi-Cancer, and Multi-Tracer Study"></a>Weakly-Supervised PET Anomaly Detection using Implicitly-Guided   Attention-Conditional Counterfactual Diffusion Modeling: a Multi-Center,   Multi-Cancer, and Multi-Tracer Study</h2><p><strong>Authors:Shadab Ahamed, Arman Rahmim</strong></p>
<p>Minimizing the need for pixel-level annotated data to train PET lesion detection and segmentation networks is highly desired and can be transformative, given time and cost constraints associated with expert annotations. Current un-&#x2F;weakly-supervised anomaly detection methods rely on autoencoder or generative adversarial networks trained only on healthy data; however GAN-based networks are more challenging to train due to issues with simultaneous optimization of two competing networks, mode collapse, etc. In this paper, we present the weakly-supervised Implicitly guided COuNterfactual diffusion model for Detecting Anomalies in PET images (IgCONDA-PET). The solution is developed and validated using PET scans from six retrospective cohorts consisting of a total of 2652 cases containing both local and public datasets. The training is conditioned on image class labels (healthy vs. unhealthy) via attention modules, and we employ implicit diffusion guidance. We perform counterfactual generation which facilitates â€œunhealthy-to-healthyâ€ domain translation by generating a synthetic, healthy version of an unhealthy input image, enabling the detection of anomalies through the calculated differences. The performance of our method was compared against several other deep learning based weakly-supervised or unsupervised methods as well as traditional methods like 41% SUVmax thresholding. We also highlight the importance of incorporating attention modules in our network for the detection of small anomalies. The code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ahxmeds/IgCONDA-PET.git">https://github.com/ahxmeds/IgCONDA-PET.git</a>. </p>
<blockquote>
<p>æœ€å°åŒ–å¯¹åƒç´ çº§æ ‡æ³¨æ•°æ®çš„éœ€æ±‚ï¼Œä»¥è®­ç»ƒPETå›¾åƒä¸­çš„ç—…å˜æ£€æµ‹å’Œåˆ†å‰²ç½‘ç»œæ˜¯éå¸¸ç†æƒ³çš„ï¼Œå¹¶èƒ½å¤Ÿåœ¨æ—¶é—´å’Œæˆæœ¬å—é™çš„ä¸“å®¶æ ‡æ³¨ä¸‹äº§ç”Ÿé©å‘½æ€§å˜åŒ–ã€‚å½“å‰çš„æ— ç›‘ç£æˆ–å¼±ç›‘ç£å¼‚å¸¸æ£€æµ‹æ–¹æ³•ä¾èµ–äºä»…ä½¿ç”¨å¥åº·æ•°æ®çš„è‡ªç¼–ç å™¨æˆ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼›ç„¶è€Œï¼ŒåŸºäºGANçš„ç½‘ç»œç”±äºåŒæ—¶ä¼˜åŒ–ä¸¤ä¸ªç«äº‰ç½‘ç»œã€æ¨¡å¼å´©æºƒç­‰é—®é¢˜è€Œæ›´å…·æŒ‘æˆ˜æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºPETå›¾åƒå¼‚å¸¸æ£€æµ‹çš„å¼±ç›‘ç£éšå¼å¼•å¯¼è®¡æ•°å™¨æ‰©æ•£æ¨¡å‹ï¼ˆIgCONDA-PETï¼‰ã€‚è¯¥è§£å†³æ–¹æ¡ˆä½¿ç”¨æ¥è‡ªå…­ä¸ªå›é¡¾æ€§é˜Ÿåˆ—çš„PETæ‰«æè¿›è¡Œå¼€å‘å’ŒéªŒè¯ï¼Œå…¶ä¸­åŒ…æ‹¬æœ¬åœ°å’Œå…¬å¼€æ•°æ®é›†æ€»å…±åŒ…å«2652ä¸ªç—…ä¾‹ã€‚è®­ç»ƒé€šè¿‡æ³¨æ„åŠ›æ¨¡å—å¯¹å›¾åƒç±»åˆ«æ ‡ç­¾ï¼ˆå¥åº·ä¸å¦ï¼‰è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œå¹¶é‡‡ç”¨éšå¼æ‰©æ•£å¼•å¯¼ã€‚æˆ‘ä»¬æ‰§è¡Œåäº‹å®ç”Ÿæˆï¼Œé€šè¿‡ç”Ÿæˆä¸å¥åº·è¾“å…¥å›¾åƒçš„åˆæˆå¥åº·ç‰ˆæœ¬ï¼Œä¿ƒè¿›â€œä¸å¥åº·åˆ°å¥åº·â€çš„é¢†åŸŸè½¬æ¢ï¼Œå¹¶é€šè¿‡è®¡ç®—å·®å¼‚æ¥æ£€æµ‹å¼‚å¸¸ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸åŸºäºæ·±åº¦å­¦ä¹ çš„å…¶ä»–å¼±ç›‘ç£æˆ–æ— ç›‘ç£æ–¹æ³•ä»¥åŠä¼ ç»Ÿçš„å¦‚SUVmaxé˜ˆå€¼æ³•ç­‰æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬è¿˜å¼ºè°ƒäº†åœ¨ç½‘ç»œä¸­èå…¥æ³¨æ„åŠ›æ¨¡å—å¯¹å°å¼‚å¸¸æ£€æµ‹çš„é‡è¦æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ahxmeds/IgCONDA-PET.git%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/ahxmeds/IgCONDA-PET.gitä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.00239v2">PDF</a> 32 pages, 6 figures, 4 tables</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¼±ç›‘ç£çš„PETå›¾åƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹IgCONDA-PETã€‚è¯¥æ¨¡å‹é‡‡ç”¨æ³¨æ„åŠ›æ¨¡å—ï¼Œåˆ©ç”¨éšæ‰©æ•£æŒ‡å¯¼æœºåˆ¶è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€åƒç´ çº§æ ‡æ³¨æ•°æ®ã€‚æ¨¡å‹å¯ç”Ÿæˆåˆæˆå¥åº·å›¾åƒå¹¶æ¯”è¾ƒå…¶ä¸å¼‚å¸¸è¾“å…¥å›¾åƒçš„å·®å¼‚æ¥æ£€æµ‹å¼‚å¸¸ã€‚æ¨¡å‹æ€§èƒ½å·²ä¸å…¶ä»–å¼±ç›‘ç£æˆ–æ— ç›‘ç£æ·±åº¦å­¦ä¹ æ–¹æ³•åŠä¼ ç»Ÿæ–¹æ³•è¿›è¡Œæ¯”è¾ƒéªŒè¯ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¼±ç›‘ç£çš„PETå›¾åƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹IgCONDA-PETã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨æ³¨æ„åŠ›æ¨¡å—ï¼Œåˆ©ç”¨éšæ‰©æ•£æŒ‡å¯¼æœºåˆ¶è¿›è¡Œè®­ç»ƒï¼Œæ— éœ€åƒç´ çº§æ ‡æ³¨æ•°æ®ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ç”Ÿæˆåˆæˆå¥åº·å›¾åƒå¹¶æ¯”è¾ƒå…¶ä¸å¼‚å¸¸è¾“å…¥å›¾åƒçš„å·®å¼‚æ¥æ£€æµ‹å¼‚å¸¸ã€‚</li>
<li>æ¨¡å‹æ€§èƒ½å·²ä¸å…¶ä»–å¼±ç›‘ç£æˆ–æ— ç›‘ç£æ·±åº¦å­¦ä¹ æ–¹æ³•åŠä¼ ç»Ÿæ–¹æ³•è¿›è¡Œæ¯”è¾ƒéªŒè¯ã€‚</li>
<li>å…¬å¼€äº†æ¨¡å‹ä»£ç ä»¥ä¾¿ä»–äººä½¿ç”¨å’Œç ”ç©¶ã€‚</li>
<li>å¼ºè°ƒäº†åœ¨ç½‘ç»œæ£€æµ‹å°å‹å¼‚å¸¸æ—¶ï¼Œæ³¨æ„åŠ›æ¨¡å—çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.00239">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-06bd0e3399d3339c12d8463dea2e76ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8b183a6f692786db96ae3b6122c14876.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="X-Diffusion-Generating-Detailed-3D-MRI-Volumes-From-a-Single-Image-Using-Cross-Sectional-Diffusion-Models"><a href="#X-Diffusion-Generating-Detailed-3D-MRI-Volumes-From-a-Single-Image-Using-Cross-Sectional-Diffusion-Models" class="headerlink" title="X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image   Using Cross-Sectional Diffusion Models"></a>X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image   Using Cross-Sectional Diffusion Models</h2><p><strong>Authors:Emmanuelle Bourigault, Abdullah Hamdi, Amir Jamaludin</strong></p>
<p>Magnetic Resonance Imaging (MRI) is a crucial diagnostic tool, but high-resolution scans are often slow and expensive due to extensive data acquisition requirements. Traditional MRI reconstruction methods aim to expedite this process by filling in missing frequency components in the K-space, performing 3D-to-3D reconstructions that demand full 3D scans. In contrast, we introduce X-Diffusion, a novel cross-sectional diffusion model that reconstructs detailed 3D MRI volumes from extremely sparse spatial-domain inputs, achieving 2D-to-3D reconstruction from as little as a single 2D MRI slice or few slices. A key aspect of X-Diffusion is that it models MRI data as holistic 3D volumes during the cross-sectional training and inference, unlike previous learning approaches that treat MRI scans as collections of 2D slices in standard planes (coronal, axial, sagittal). We evaluated X-Diffusion on brain tumor MRIs from the BRATS dataset and full-body MRIs from the UK Biobank dataset. Our results demonstrate that X-Diffusion not only surpasses state-of-the-art methods in quantitative accuracy (PSNR) on unseen data but also preserves critical anatomical features such as tumor profiles, spine curvature, and brain volume. Remarkably, the model generalizes beyond the training domain, successfully reconstructing knee MRIs despite being trained exclusively on brain data. Medical expert evaluations further confirm the clinical relevance and fidelity of the generated images.To our knowledge, X-Diffusion is the first method capable of producing detailed 3D MRIs from highly limited 2D input data, potentially accelerating MRI acquisition and reducing associated costs. The code is available on the project website <a target="_blank" rel="noopener" href="https://emmanuelleb985.github.io/XDiffusion/">https://emmanuelleb985.github.io/XDiffusion/</a> . </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ˜¯ä¸€ç§é‡è¦çš„è¯Šæ–­å·¥å…·ï¼Œä½†é«˜åˆ†è¾¨ç‡æ‰«æé€šå¸¸ç”±äºéœ€è¦å¤§é‡æ•°æ®é‡‡é›†è€Œç¼“æ…¢ä¸”æ˜‚è´µã€‚ä¼ ç»Ÿçš„MRIé‡å»ºæ–¹æ³•æ—¨åœ¨é€šè¿‡å¡«å……Kç©ºé—´ä¸­çš„ç¼ºå¤±é¢‘ç‡æˆåˆ†æ¥åŠ å¿«è¿™ä¸€è¿‡ç¨‹ï¼Œè¿›è¡Œéœ€è¦å®Œæ•´3Dæ‰«æçš„3D-to-3Dé‡å»ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†X-Diffusionï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æˆªé¢æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿä»æç¨€ç–çš„ç©ºé—´åŸŸè¾“å…¥ä¸­é‡å»ºè¯¦ç»†çš„3DMRIä½“ç§¯ï¼Œä»…ä½¿ç”¨å°‘é‡æˆ–å•ä¸ª2DMRIåˆ‡ç‰‡å³å¯å®ç°2D-to-3Dé‡å»ºã€‚X-Diffusionçš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯ï¼Œå®ƒåœ¨æˆªé¢è®­ç»ƒå’Œæ¨æ–­è¿‡ç¨‹ä¸­å°†MRIæ•°æ®å»ºæ¨¡ä¸ºæ•´ä½“3Dä½“ç§¯ï¼Œä¸åŒäºä»¥å¾€çš„å­¦ä¹ æ–¹æ³•å°†MRIæ‰«æè§†ä¸ºæ ‡å‡†å¹³é¢ï¼ˆå† çŠ¶é¢ã€è½´é¢ã€çŸ¢çŠ¶é¢ï¼‰ä¸­çš„2Dåˆ‡ç‰‡é›†åˆã€‚æˆ‘ä»¬å¯¹BRATSæ•°æ®é›†ä¸­çš„è„‘è‚¿ç˜¤MRIå’ŒUK Biobankæ•°æ®é›†ä¸­çš„å…¨èº«MRIè¿›è¡Œäº†X-Diffusionè¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒX-Diffusionä¸ä»…åœ¨æœªè§æ•°æ®ä¸Šçš„å®šé‡å‡†ç¡®æ€§ï¼ˆPSNRï¼‰ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œè€Œä¸”è¿˜ä¿ç•™äº†å…³é”®è§£å‰–ç‰¹å¾ï¼Œå¦‚è‚¿ç˜¤æ¦‚å†µã€è„ŠæŸ±æ›²åº¦å’Œè„‘å®¹é‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥æ¨¡å‹åœ¨è®­ç»ƒé¢†åŸŸä¹‹å¤–ä¹Ÿå…·æœ‰æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤ŸæˆåŠŸé‡å»ºè†ç›–MRIï¼Œå°½ç®¡å®ƒåªæ¥å—è„‘éƒ¨æ•°æ®è®­ç»ƒã€‚åŒ»å­¦ä¸“å®¶è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†ç”Ÿæˆå›¾åƒçš„ä¸´åºŠç›¸å…³æ€§å’Œä¿çœŸåº¦ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒX-Diffusionæ˜¯ç¬¬ä¸€ç§èƒ½å¤Ÿä»é«˜åº¦æœ‰é™çš„2Dè¾“å…¥æ•°æ®ç”Ÿæˆè¯¦ç»†çš„3DMRIçš„æ–¹æ³•ï¼Œæœ‰æœ›åŠ é€ŸMRIé‡‡é›†å¹¶é™ä½ç›¸å…³æˆæœ¬ã€‚ä»£ç å¯åœ¨é¡¹ç›®ç½‘ç«™<a target="_blank" rel="noopener" href="https://emmanuelleb985.github.io/XDiffusion/%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://emmanuelleb985.github.io/XDiffusion/ä¸Šè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2404.19604v2">PDF</a> preprint, project website:   <a target="_blank" rel="noopener" href="https://emmanuelleb985.github.io/XDiffusion/">https://emmanuelleb985.github.io/XDiffusion/</a></p>
<p><strong>Summary</strong><br>     åŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„X-Diffusionæ¨¡å‹ï¼Œèƒ½å¤Ÿä»æå°‘çš„äºŒç»´MRIåˆ‡ç‰‡é‡å»ºå‡ºè¯¦ç»†çš„ä¸‰ç»´å›¾åƒï¼Œå®ç°å¿«é€Ÿä¸”ç»æµçš„ä¸‰ç»´MRIé‡å»ºã€‚è¯¥æ–¹æ³•çªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„å±€é™ï¼Œä¸ºMRIè¯Šæ–­æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>X-Diffusionæ˜¯ä¸€ç§æ–°å‹äº¤å‰æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿä»æç¨€ç–çš„ç©ºé—´åŸŸè¾“å…¥é‡å»ºè¯¦ç»†çš„ä¸‰ç»´MRIä½“ç§¯ã€‚</li>
<li>è¯¥æ–¹æ³•å®ç°äº†ä»å•ä¸ªæˆ–å°‘æ•°äºŒç»´MRIåˆ‡ç‰‡çš„äºŒç»´åˆ°ä¸‰ç»´é‡å»ºã€‚</li>
<li>X-Diffusionå°†MRIæ•°æ®è§†ä¸ºæ•´ä½“ä¸‰ç»´ä½“ç§¯è¿›è¡Œè®­ç»ƒå’Œæ¨æ–­ï¼Œä¸ä»¥å¾€å°†MRIæ‰«æè§†ä¸ºäºŒç»´åˆ‡ç‰‡é›†åˆçš„å­¦ä¹ æ–¹æ³•ä¸åŒã€‚</li>
<li>åœ¨BRATSå’ŒUK Biobankæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒX-Diffusionåœ¨å®šé‡å‡†ç¡®æ€§ï¼ˆPSNRï¼‰ä¸Šè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¿ç•™äº†å…³é”®è§£å‰–ç‰¹å¾ã€‚</li>
<li>X-Diffusionèƒ½å¤Ÿæ¨å¹¿åˆ°è®­ç»ƒåŸŸä¹‹å¤–ï¼ŒæˆåŠŸé‡å»ºè†ç›–MRIï¼Œå°½ç®¡åªç»è¿‡å¤§è„‘æ•°æ®è®­ç»ƒã€‚</li>
<li>åŒ»ç–—ä¸“å®¶è¯„ä¼°è¿›ä¸€æ­¥è¯å®äº†ç”Ÿæˆå›¾åƒçš„ä¸´åºŠç›¸å…³æ€§å’Œä¿çœŸåº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2404.19604">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ee3e95325fd3c67d7ad5956791eafc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d5442c69b1523a6f6c8ab9eac63a7a1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4a150c0c505a622d1cd8aa45655a21c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a3a4e56f753e6f104d072e2da06b8d6a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51550ff5ad461c0aa6b204a94b79cd05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53a6d4aace4c2b3c2d0a17c3cfcf8757.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Deep-Spatiotemporal-Clutter-Filtering-of-Transthoracic-Echocardiographic-Images-Leveraging-Contextual-Attention-and-Residual-Learning"><a href="#Deep-Spatiotemporal-Clutter-Filtering-of-Transthoracic-Echocardiographic-Images-Leveraging-Contextual-Attention-and-Residual-Learning" class="headerlink" title="Deep Spatiotemporal Clutter Filtering of Transthoracic Echocardiographic   Images: Leveraging Contextual Attention and Residual Learning"></a>Deep Spatiotemporal Clutter Filtering of Transthoracic Echocardiographic   Images: Leveraging Contextual Attention and Residual Learning</h2><p><strong>Authors:Mahdi Tabassian, Somayeh Akbari, Sandro QueirÃ³s, Jan Dâ€™hooge</strong></p>
<p>This study presents a deep convolutional autoencoder network for filtering reverberation clutter from transthoracic echocardiographic (TTE) image sequences. Given the spatiotemporal nature of this type of clutter, the filtering network employs 3D convolutional layers to suppress it throughout the cardiac cycle. The design of the network incorporates two key features that contribute to the effectiveness of the filter: 1) an attention mechanism for focusing on cluttered regions and leveraging contextual information, and 2) residual learning for preserving fine image structures. To train the network, a diverse set of artifact patterns was simulated and superimposed onto ultra-realistic synthetic TTE sequences from six ultrasound vendors, generating input for the filtering network. The artifact-free sequences served as ground-truth. Performance of the filtering network was evaluated using unseen synthetic and in vivo artifactual sequences. Results from the in vivo dataset confirmed the networkâ€™s strong generalization capabilities, despite being trained solely on synthetic data and simulated artifacts. The suitability of the filtered sequences for downstream processing was assessed by computing segmental strain curves. A significant reduction in the discrepancy between strain profiles computed from cluttered and clutter-free segments was observed after filtering the cluttered sequences with the proposed network. The trained network processes a TTE sequence in a fraction of a second, enabling real-time clutter filtering and potentially improving the precision of clinically relevant indices derived from TTE sequences. The source code of the proposed method and example video files of the filtering results are available at: \href{<a target="_blank" rel="noopener" href="https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main%7D%7Bhttps://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main%7D">https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main}{https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main}</a>. </p>
<blockquote>
<p>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ·±åº¦å·ç§¯è‡ªç¼–ç å™¨ç½‘ç»œï¼Œç”¨äºä»èƒ¸éƒ¨è¶…å£°å¿ƒåŠ¨å›¾ï¼ˆTTEï¼‰å›¾åƒåºåˆ—ä¸­è¿‡æ»¤å›å£°å¹²æ‰°ã€‚è€ƒè™‘åˆ°è¿™ç§å¹²æ‰°çš„æ—¶ç©ºç‰¹æ€§ï¼Œè¿‡æ»¤ç½‘ç»œé‡‡ç”¨3Då·ç§¯å±‚ï¼Œåœ¨æ•´ä¸ªå¿ƒåŠ¨å‘¨æœŸå†…æŠ‘åˆ¶å¹²æ‰°ã€‚è¯¥ç½‘ç»œçš„è®¾è®¡ç»“åˆäº†ä¸¤ä¸ªå…³é”®ç‰¹å¾ï¼Œä¸ºè¿‡æ»¤å™¨çš„æœ‰æ•ˆæ€§åšå‡ºäº†è´¡çŒ®ï¼š1ï¼‰ä¸€ç§å…³æ³¨å¹²æ‰°åŒºåŸŸçš„æ³¨æ„åŠ›æœºåˆ¶å’Œåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼›2ï¼‰æ®‹å·®å­¦ä¹ ï¼Œä»¥ä¿ç•™ç²¾ç»†çš„å›¾åƒç»“æ„ã€‚ä¸ºäº†è®­ç»ƒç½‘ç»œï¼Œæ¨¡æ‹Ÿäº†å„ç§ä¼ªå½±æ¨¡å¼å¹¶å°†å…¶å åŠ åœ¨æ¥è‡ªå…­ä¸ªè¶…å£°ä¾›åº”å•†çš„è¶…é«˜çœŸå®åº¦åˆæˆTTEåºåˆ—ä¸Šï¼Œä»¥ä¸ºè¿‡æ»¤ç½‘ç»œæä¾›è¾“å…¥ã€‚æ— ä¼ªå½±åºåˆ—ä½œä¸ºåœ°é¢å®å†µï¼ˆground-truthï¼‰ã€‚è¿‡æ»¤ç½‘ç»œçš„æ€§èƒ½é€šè¿‡æœªè§è¿‡çš„åˆæˆå’Œä½“å†…ä¼ªå½±åºåˆ—è¿›è¡Œäº†è¯„ä¼°ã€‚ä½“å†…æ•°æ®é›†çš„ç»“æœè¯å®äº†è¯¥ç½‘ç»œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå°½ç®¡å®ƒåªæ¥å—åˆæˆæ•°æ®å’Œæ¨¡æ‹Ÿä¼ªå½±çš„è®­ç»ƒã€‚é€šè¿‡è®¡ç®—åˆ†æ®µåº”å˜æ›²çº¿è¯„ä¼°äº†è¿‡æ»¤åºåˆ—å¯¹äºä¸‹æ¸¸å¤„ç†çš„é€‚ç”¨æ€§ã€‚è§‚å¯Ÿå‘ç°ï¼Œåœ¨è¿‡æ»¤å¸¦æœ‰å¹²æ‰°çš„åºåˆ—åï¼Œä¸æ— å¹²æ‰°ç‰‡æ®µè®¡ç®—çš„åº”å˜æ›²çº¿ç›¸æ¯”ï¼Œå¹²æ‰°æ›²çº¿ä¹‹é—´çš„å·®å¼‚æ˜¾è‘—å‡å°‘ã€‚è®­ç»ƒåçš„ç½‘ç»œèƒ½å¤Ÿåœ¨å‡ åˆ†ä¹‹ä¸€ç§’å†…å¤„ç†ä¸€ä¸ªTTEåºåˆ—ï¼Œå®ç°äº†å®æ—¶å¹²æ‰°è¿‡æ»¤ï¼Œå¹¶å¯èƒ½æé«˜äº†ä»TTEåºåˆ—ä¸­å¾—å‡ºçš„ä¸´åºŠç›¸å…³æŒ‡æ ‡çš„ç²¾åº¦ã€‚æ‰€æå‡ºæ–¹æ³•çš„æºä»£ç å’Œè¿‡æ»¤ç»“æœçš„ç¤ºä¾‹è§†é¢‘æ–‡ä»¶å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main">https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2401.13147v2">PDF</a> 19 pages, 14 figures</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ·±åº¦å·ç§¯è‡ªç¼–ç å™¨ç½‘ç»œï¼Œç”¨äºä»èƒ¸éƒ¨è¶…å£°å¿ƒåŠ¨å›¾ï¼ˆTTEï¼‰å›¾åƒåºåˆ—ä¸­è¿‡æ»¤æ··å“å¹²æ‰°ã€‚è¯¥ç½‘ç»œé‡‡ç”¨ä¸‰ç»´å·ç§¯å±‚æ¥æŠ‘åˆ¶æ•´ä¸ªå¿ƒåŠ¨å‘¨æœŸä¸­çš„æ—¶ç©ºå¹²æ‰°ã€‚ç½‘ç»œè®¾è®¡èå…¥äº†ä¸¤ä¸ªå…³é”®ç‰¹æ€§ï¼Œæé«˜äº†æ»¤æ³¢æ•ˆæœï¼šä¸€æ˜¯å…³æ³¨å¹²æ‰°åŒºåŸŸçš„æ³¨æ„åŠ›æœºåˆ¶å’Œåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒäºŒæ˜¯ä¿ç•™å›¾åƒç»†èŠ‚çš„æ®‹å·®å­¦ä¹ ã€‚ä¸ºè®­ç»ƒç½‘ç»œï¼Œæ¨¡æ‹Ÿäº†å¤šç§ä¼ªå½±æ¨¡å¼å¹¶å°†å…¶å åŠ åœ¨æ¥è‡ªå…­ä¸ªè¶…å£°ä¾›åº”å•†çš„è¶…é«˜çœŸå®åº¦åˆæˆTTEåºåˆ—ä¸Šï¼Œä½œä¸ºæ»¤æ³¢ç½‘ç»œçš„è¾“å…¥ã€‚æ— ä¼ªå½±åºåˆ—ä½œä¸ºåœ°é¢çœŸå®æƒ…å†µæ¥è¯„ä¼°æ»¤æ³¢ç½‘ç»œæ€§èƒ½ã€‚é€šè¿‡æœªç»éªŒè¿‡çš„åˆæˆå’Œä½“å†…å¹²æ‰°åºåˆ—çš„è¯„ä¼°ç»“æœè¯å®ï¼Œè¯¥ç½‘ç»œåœ¨ä»…ä½¿ç”¨åˆæˆæ•°æ®å’Œæ¨¡æ‹Ÿä¼ªå½±è¿›è¡Œè®­ç»ƒçš„æƒ…å†µä¸‹ä»å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡è®¡ç®—åˆ†æ®µåº”å˜æ›²çº¿è¯„ä¼°è¿‡æ»¤åºåˆ—çš„ä¸‹æ¸¸å¤„ç†é€‚å®œæ€§ã€‚è§‚å¯Ÿå‘ç°ï¼Œä¸ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•å¤„ç†çš„å¹²æ‰°åºåˆ—ç›¸æ¯”ï¼Œä½¿ç”¨æ‰€æç½‘ç»œè¿‡æ»¤åï¼Œä»å¹²æ‰°æ®µè®¡ç®—çš„åº”å˜æ›²çº¿ä¹‹é—´çš„åå·®æ˜¾è‘—é™ä½ã€‚æ‰€è®­ç»ƒçš„ç½‘ç»œå¯åœ¨å‡ åˆ†ä¹‹ä¸€ç§’å†…å¤„ç†TTEåºåˆ—ï¼Œå®ç°å®æ—¶å¹²æ‰°æ»¤æ³¢ï¼Œå¹¶å¯èƒ½æé«˜ä»TTEåºåˆ—æ´¾ç”Ÿçš„ä¸´åºŠç›¸å…³æŒ‡æ ‡çš„ç²¾åº¦ã€‚æ‰€æå‡ºæ–¹æ³•çš„æºä»£ç å’Œè¿‡æ»¤ç»“æœç¤ºä¾‹è§†é¢‘æ–‡ä»¶å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main">https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main</a>è·å¾—ã€‚</p>
<p><strong>è¦ç‚¹è§£æ</strong></p>
<ol>
<li>æœ¬ç ”ç©¶è®¾è®¡äº†ä¸€ç§æ·±åº¦å·ç§¯è‡ªç¼–ç å™¨ç½‘ç»œï¼Œæ—¨åœ¨ä»èƒ¸éƒ¨è¶…å£°å¿ƒåŠ¨å›¾ï¼ˆTTEï¼‰å›¾åƒåºåˆ—ä¸­å»é™¤æ··å“å¹²æ‰°ã€‚</li>
<li>ç½‘ç»œåˆ©ç”¨ä¸‰ç»´å·ç§¯å±‚åœ¨å¿ƒè„å‘¨æœŸä¸­æŠ‘åˆ¶æ—¶ç©ºå¹²æ‰°ã€‚</li>
<li>ç½‘ç»œè®¾è®¡çš„ä¸¤å¤§å…³é”®ç‰¹æ€§ï¼šæ³¨æ„åŠ›æœºåˆ¶å’Œæ®‹å·®å­¦ä¹ ï¼Œåˆ†åˆ«ç”¨äºèšç„¦å¹²æ‰°åŒºåŸŸã€åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯åŠä¿ç•™å›¾åƒç»†èŠ‚ã€‚</li>
<li>ä¸ºè®­ç»ƒç½‘ç»œï¼Œç ”ç©¶æ¨¡æ‹Ÿå¤šç§ä¼ªå½±æ¨¡å¼å¹¶å°†å…¶æ·»åŠ åˆ°åˆæˆTTEåºåˆ—ä¸­ã€‚</li>
<li>é€šè¿‡åˆæˆå’Œä½“å†…æ•°æ®è¯„ä¼°ç½‘ç»œæ€§èƒ½ï¼Œæ˜¾ç¤ºå…¶åœ¨ä»…ä½¿ç”¨åˆæˆæ•°æ®å’Œæ¨¡æ‹Ÿä¼ªå½±è®­ç»ƒæ—¶çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ»¤æ³¢åçš„åºåˆ—åœ¨ä¸‹æ¸¸å¤„ç†ä¸­çš„é€‚ç”¨æ€§é€šè¿‡è®¡ç®—åˆ†æ®µåº”å˜æ›²çº¿è¿›è¡Œè¯„ä¼°ï¼Œæ˜¾ç¤ºæ˜æ˜¾æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2401.13147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-64a5cb687cd0ff7098dd103f4ecb6985.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-389e55621d1f535ffdee9d817865dee9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bf14393a246e26d75b6dd33181a628f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d998273236807cb2ad70e69bf3c6e733.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1c6a37500867f2bfc0047b8716adff60.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ec214808eb3d467ca15bd094139d23f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c43249fedc6955f1121ea5ce8d45a6e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SegVol-Universal-and-Interactive-Volumetric-Medical-Image-Segmentation"><a href="#SegVol-Universal-and-Interactive-Volumetric-Medical-Image-Segmentation" class="headerlink" title="SegVol: Universal and Interactive Volumetric Medical Image Segmentation"></a>SegVol: Universal and Interactive Volumetric Medical Image Segmentation</h2><p><strong>Authors:Yuxin Du, Fan Bai, Tiejun Huang, Bo Zhao</strong></p>
<p>Precise image segmentation provides clinical study with instructive information. Despite the remarkable progress achieved in medical image segmentation, there is still an absence of a 3D foundation segmentation model that can segment a wide range of anatomical categories with easy user interaction. In this paper, we propose a 3D foundation segmentation model, named SegVol, supporting universal and interactive volumetric medical image segmentation. By scaling up training data to 90K unlabeled Computed Tomography (CT) volumes and 6K labeled CT volumes, this foundation model supports the segmentation of over 200 anatomical categories using semantic and spatial prompts. To facilitate efficient and precise inference on volumetric images, we design a zoom-out-zoom-in mechanism. Extensive experiments on 22 anatomical segmentation tasks verify that SegVol outperforms the competitors in 19 tasks, with improvements up to 37.24% compared to the runner-up methods. We demonstrate the effectiveness and importance of specific designs by ablation study. We expect this foundation model can promote the development of volumetric medical image analysis. The model and code are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/BAAI-DCAI/SegVol">https://github.com/BAAI-DCAI/SegVol</a>. </p>
<blockquote>
<p>ç²¾ç¡®å›¾åƒåˆ†å‰²ä¸ºä¸´åºŠç ”ç©¶æä¾›äº†æŒ‡å¯¼æ€§ä¿¡æ¯ã€‚å°½ç®¡åŒ»å­¦å›¾åƒåˆ†å‰²å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†ä»ç„¶ç¼ºä¹ä¸€ç§3DåŸºç¡€åˆ†å‰²æ¨¡å‹ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨å¹¿æ³›çš„è§£å‰–ç±»åˆ«ä¸­è¿›è¡Œåˆ†å‰²ï¼Œå¹¶å…·å¤‡ç®€å•çš„ç”¨æˆ·äº¤äº’åŠŸèƒ½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºSegVolçš„3DåŸºç¡€åˆ†å‰²æ¨¡å‹ï¼Œæ”¯æŒé€šç”¨å’Œäº¤äº’å¼ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚é€šè¿‡æ‰©å¤§è®­ç»ƒæ•°æ®è‡³9ä¸‡ä»½æ— æ ‡ç­¾è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä½“ç§¯æ•°æ®å’Œ6åƒä»½æ ‡è®°CTä½“ç§¯æ•°æ®ï¼Œè¯¥åŸºç¡€æ¨¡å‹æ”¯æŒè¶…è¿‡200ä¸ªè§£å‰–ç±»åˆ«çš„åˆ†å‰²ï¼Œä½¿ç”¨è¯­ä¹‰å’Œç©ºé—´æç¤ºã€‚ä¸ºäº†æ–¹ä¾¿å¯¹ä½“ç§¯å›¾åƒè¿›è¡Œé«˜æ•ˆå’Œç²¾ç¡®çš„æ¨ç†ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç¼©æ”¾é€€å‡ºå’Œç¼©æ”¾è¿›å…¥æœºåˆ¶ã€‚åœ¨22ä¸ªè§£å‰–åˆ†å‰²ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSegVolåœ¨19ä¸ªä»»åŠ¡ä¸­è¶…è¿‡äº†ç«äº‰å¯¹æ‰‹ï¼Œç›¸è¾ƒäºæ’åç¬¬äºŒçš„æ–¹æ³•æ”¹è¿›äº†é«˜è¾¾37.24%ã€‚æˆ‘ä»¬é€šè¿‡æ¶ˆèç ”ç©¶è¯æ˜äº†ç‰¹å®šè®¾è®¡çš„æœ‰æ•ˆæ€§å’Œé‡è¦æ€§ã€‚æˆ‘ä»¬é¢„è®¡è¿™ä¸ªåŸºç¡€æ¨¡å‹å¯ä»¥ä¿ƒè¿›ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†æçš„å‘å±•ã€‚æ¨¡å‹å’Œä»£ç å…¬å¼€å¯ç”¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/BAAI-DCAI/SegVol%E3%80%82">https://github.com/BAAI-DCAI/SegVolã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.13385v5">PDF</a> NeurIPS 2024 Spotlight</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠç ”ç©¶ä¸­æä¾›é‡è¦ä¿¡æ¯ã€‚å°½ç®¡åŒ»å­¦å›¾åƒåˆ†å‰²å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ä»ç¼ºä¹ä¸€ä¸ªèƒ½åœ¨å¹¿æ³›è§£å‰–ç±»åˆ«ä¸­è¿›è¡Œåˆ†å‰²çš„3DåŸºç¡€åˆ†å‰²æ¨¡å‹ï¼Œä¸”éœ€è¦æ˜“äºç”¨æˆ·äº¤äº’ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSegVolçš„3DåŸºç¡€åˆ†å‰²æ¨¡å‹ï¼Œæ”¯æŒé€šç”¨å’Œäº¤äº’å¼ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚é€šè¿‡æ‰©å¤§è®­ç»ƒæ•°æ®è‡³9ä¸‡æ— æ ‡ç­¾è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ä½“ç§¯å’Œ6åƒæ ‡è®°CTä½“ç§¯ï¼Œè¯¥åŸºç¡€æ¨¡å‹ä½¿ç”¨è¯­ä¹‰å’Œç©ºé—´æç¤ºæ”¯æŒè¶…è¿‡200ä¸ªè§£å‰–ç±»åˆ«çš„åˆ†å‰²ã€‚ä¸ºåœ¨ä½“ç§¯å›¾åƒä¸Šè¿›è¡Œé«˜æ•ˆå’Œç²¾ç¡®æ¨ç†ï¼Œæˆ‘ä»¬è®¾è®¡äº†ç¼©æ”¾æœºåˆ¶ã€‚åœ¨22ä¸ªè§£å‰–åˆ†å‰²ä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒSegVolåœ¨19ä¸ªä»»åŠ¡ä¸­ä¼˜äºç«äº‰å¯¹æ‰‹ï¼Œç›¸è¾ƒäºæ¬¡ä¼˜æ–¹æ³•ï¼Œæ”¹è¿›ç‡é«˜è¾¾37.24%ã€‚æˆ‘ä»¬é€šè¿‡æ¶ˆèç ”ç©¶è¯æ˜äº†ç‰¹å®šè®¾è®¡çš„æœ‰æ•ˆæ€§å’Œé‡è¦æ€§ã€‚æœŸæœ›æ­¤åŸºç¡€æ¨¡å‹èƒ½ä¿ƒè¿›ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†æçš„å‘å±•ã€‚æ¨¡å‹å’Œä»£ç å…¬å¼€äºï¼š<a target="_blank" rel="noopener" href="https://github.com/BAAI-DCAI/SegVol%E3%80%82">https://github.com/BAAI-DCAI/SegVolã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸ºä¸´åºŠç ”ç©¶æä¾›å…³é”®ä¿¡æ¯ã€‚</li>
<li>å½“å‰ç¼ºä¹ä¸€ä¸ªèƒ½åœ¨å¹¿æ³›è§£å‰–ç±»åˆ«ä¸­è¿›è¡Œåˆ†å‰²çš„3DåŸºç¡€åˆ†å‰²æ¨¡å‹ï¼Œéœ€æ”¹è¿›ä»¥æ˜“äºç”¨æˆ·äº¤äº’ã€‚</li>
<li>SegVolæ˜¯ä¸€ä¸ªæ–°çš„3DåŸºç¡€åˆ†å‰²æ¨¡å‹ï¼Œæ”¯æŒé€šç”¨å’Œäº¤äº’å¼ä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>SegVolé€šè¿‡æ‰©å¤§è®­ç»ƒæ•°æ®è‡³å¤§é‡CTä½“ç§¯å›¾åƒæ¥æé«˜æ€§èƒ½ã€‚</li>
<li>SegVolåœ¨å¤šæ•°è§£å‰–åˆ†å‰²ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºç«äº‰å¯¹æ‰‹ï¼Œæ”¹è¿›æ•ˆæœæ˜¾è‘—ã€‚</li>
<li>SegVolè®¾è®¡åŒ…æ‹¬ç¼©æ”¾æœºåˆ¶ä»¥é«˜æ•ˆå’Œç²¾ç¡®åœ°è¿›è¡Œä½“ç§¯å›¾åƒæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2311.13385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-067d0e50b4cf897131f2ff539f5d6fbc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-58bd5a2e56ee41239fc9218b504a7776.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-355e5dd30be73d085d1303a7a330275d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05e97499c3487a0da36dd306955a8f03.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Volumetric-medical-image-segmentation-through-dual-self-distillation-in-U-shaped-networks"><a href="#Volumetric-medical-image-segmentation-through-dual-self-distillation-in-U-shaped-networks" class="headerlink" title="Volumetric medical image segmentation through dual self-distillation in   U-shaped networks"></a>Volumetric medical image segmentation through dual self-distillation in   U-shaped networks</h2><p><strong>Authors:Soumyanil Banerjee, Nicholas Summerfield, Ming Dong, Carri Glide-Hurst</strong></p>
<p>U-shaped networks and its variants have demonstrated exceptional results for medical image segmentation. In this paper, we propose a novel dual self-distillation (DSD) framework in U-shaped networks for volumetric medical image segmentation. DSD distills knowledge from the ground-truth segmentation labels to the decoder layers. Additionally, DSD also distills knowledge from the deepest decoder and encoder layer to the shallower decoder and encoder layers respectively of a single U-shaped network. DSD is a generalized training strategy that could be attached to the backbone architecture of any U-shaped network to further improve its segmentation performance. We attached DSD on several state-of-the-art U-shaped backbones, and extensive experiments on various public 3D medical image segmentation datasets (cardiac substructure, brain tumor and Hippocampus) demonstrated significant improvement over the same backbones without DSD. On average, after attaching DSD to the U-shaped backbones, we observed an increase of 2.82%, 4.53% and 1.3% in Dice similarity score, a decrease of 7.15 mm, 6.48 mm and 0.76 mm in the Hausdorff distance, for cardiac substructure, brain tumor and Hippocampus segmentation, respectively. These improvements were achieved with negligible increase in the number of trainable parameters and training time. Our proposed DSD framework also led to significant qualitative improvements for cardiac substructure, brain tumor and Hippocampus segmentation over the U-shaped backbones. The source code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/soumbane/DualSelfDistillation">https://github.com/soumbane/DualSelfDistillation</a>. </p>
<blockquote>
<p>Uå½¢ç½‘ç»œåŠå…¶å˜ä½“åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„ç»“æœã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨Uå½¢ç½‘ç»œä¸­æå‡ºäº†ä¸€ç§æ–°å‹çš„åŒè‡ªè’¸é¦ï¼ˆDSDï¼‰æ¡†æ¶ï¼Œç”¨äºä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚DSDä»çœŸå®çš„åˆ†å‰²æ ‡ç­¾ä¸­æç‚¼çŸ¥è¯†å¹¶å°†å…¶ä¼ é€’ç»™è§£ç å™¨å±‚ã€‚æ­¤å¤–ï¼ŒDSDè¿˜ä»æœ€æ·±å±‚çš„è§£ç å™¨å’Œç¼–ç å™¨å±‚æç‚¼çŸ¥è¯†ï¼Œå¹¶å°†å…¶åˆ†åˆ«ä¼ é€’ç»™è¾ƒæµ…å±‚çš„è§£ç å™¨å’Œç¼–ç å™¨å±‚ï¼Œåœ¨åŒä¸€Uå½¢ç½‘ç»œä¸­ã€‚DSDæ˜¯ä¸€ç§é€šç”¨çš„è®­ç»ƒç­–ç•¥ï¼Œå¯ä»¥é™„åŠ åˆ°ä»»ä½•Uå½¢ç½‘ç»œçš„éª¨å¹²æ¶æ„ä¸Šï¼Œä»¥è¿›ä¸€æ­¥æé«˜å…¶åˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ä¸€äº›æœ€å…ˆè¿›çš„Uå½¢éª¨å¹²ç½‘ç»œä¸Šé™„åŠ äº†DSDï¼Œåœ¨å„ç§å…¬å…±3DåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ï¼ˆå¿ƒè„å­ç»“æ„ã€è„‘è‚¿ç˜¤å’Œæµ·é©¬ä½“ï¼‰ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æ²¡æœ‰DSDçš„ç›¸åŒéª¨å¹²ç½‘ç»œç›¸æ¯”ï¼ŒDSDå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚å¹³å‡è€Œè¨€ï¼Œåœ¨Uå½¢éª¨å¹²ç½‘ç»œä¸Šé™„åŠ DSDåï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°Diceç›¸ä¼¼åº¦å¾—åˆ†æé«˜äº†2.82%ã€4.53%å’Œ1.3%ï¼Œå¿ƒè„å­ç»“æ„ã€è„‘è‚¿ç˜¤å’Œæµ·é©¬ä½“åˆ†å‰²çš„Hausdorffè·ç¦»åˆ†åˆ«å‡å°‘äº†7.15æ¯«ç±³ã€6.48æ¯«ç±³å’Œ0.76æ¯«ç±³ã€‚è¿™äº›æ”¹è¿›æ˜¯åœ¨å¯è®­ç»ƒå‚æ•°æ•°é‡å’Œè®­ç»ƒæ—¶é—´å¢åŠ ç”šå¾®çš„æƒ…å†µä¸‹å®ç°çš„ã€‚æˆ‘ä»¬æå‡ºçš„DSDæ¡†æ¶è¿˜å¯¼è‡´å¿ƒè„å­ç»“æ„ã€è„‘è‚¿ç˜¤å’Œæµ·é©¬ä½“åˆ†å‰²çš„å®šæ€§æ˜¾è‘—æ”¹è¿›ï¼Œè¶…è¿‡äº†Uå½¢éª¨å¹²ç½‘ç»œã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/soumbane/DualSelfDistillation%E5%85%AC%E5%BC%B9%E5%8F%91%E5%A3%B0%E3%80%82">https://github.com/soumbane/DualSelfDistillationå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.03271v2">PDF</a> 27 pages, 5 figures, 7 tables, preliminary version accepted at IEEE   ISBI 2024</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„åŒé‡è‡ªè’¸é¦ï¼ˆDSDï¼‰æ¡†æ¶ï¼Œåº”ç”¨äºUå‹ç½‘ç»œï¼Œç”¨äºä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚DSDä»åœ°é¢çœŸå®åˆ†å‰²æ ‡ç­¾ä¸­æç‚¼çŸ¥è¯†å¹¶ä¼ è¾“åˆ°è§£ç å™¨å±‚ï¼ŒåŒæ—¶ä¹Ÿå°†æœ€æ·±è§£ç å™¨å’Œç¼–ç å™¨çš„çŸ¥è¯†ä¼ è¾“åˆ°è¾ƒæµ…çš„è§£ç å™¨å’Œç¼–ç å™¨å±‚ã€‚DSDæ˜¯ä¸€ç§é€šç”¨è®­ç»ƒç­–ç•¥ï¼Œå¯é™„åŠ åˆ°ä»»ä½•Uå‹ç½‘ç»œçš„éª¨å¹²æ¶æ„ä¸Šï¼Œä»¥è¿›ä¸€æ­¥æé«˜å…¶åˆ†å‰²æ€§èƒ½ã€‚åœ¨å¤šä¸ªå…¬å…±3DåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸æ²¡æœ‰DSDçš„ç›¸åŒéª¨å¹²ç›¸æ¯”ï¼Œæ·»åŠ DSDåæ€§èƒ½æ˜¾è‘—æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒé‡è‡ªè’¸é¦ï¼ˆDSDï¼‰æ¡†æ¶è¢«æå‡ºå¹¶åº”ç”¨äºUå‹ç½‘ç»œï¼Œç”¨äºä½“ç§¯åŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</li>
<li>DSDä»åœ°é¢çœŸå®åˆ†å‰²æ ‡ç­¾å’Œæœ€æ·±è§£ç å™¨ä¸ç¼–ç å™¨å±‚ä¸­æç‚¼çŸ¥è¯†ã€‚</li>
<li>DSDå¯é™„åŠ åˆ°ä»»ä½•Uå‹ç½‘ç»œçš„éª¨å¹²æ¶æ„ä¸Šï¼Œä»¥æé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å…±3DåŒ»å­¦å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šï¼Œæ·»åŠ DSDåï¼ŒDiceç›¸ä¼¼åº¦å¾—åˆ†å¹³å‡æé«˜2.82%ã€4.53%å’Œ1.3%ï¼ŒHausdorffè·ç¦»å¹³å‡å‡å°‘7.15mmã€6.48mmå’Œ0.76mmã€‚</li>
<li>DSDå¯¼è‡´çš„å‚æ•°å¢åŠ å’Œè®­ç»ƒæ—¶é—´å»¶é•¿å¾®ä¹å…¶å¾®ã€‚</li>
<li>DSDæ¡†æ¶åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²è´¨é‡ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>æºä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/soumbane/DualSelfDistillation%E3%80%82">https://github.com/soumbane/DualSelfDistillationã€‚</a></li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2306.03271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-02d75bfebf4e0061cd8eb89641230ded.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-02-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-02-14/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-15/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-f0371212a75e39cb72fce6ceda00ddd2.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-15  MME-CoT Benchmarking Chain-of-Thought in Large Multimodal Models for   Reasoning Quality, Robustness, and Efficiency
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-02-14/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cef17cc1d3d1bc04ccca8508377d280a.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-02-14  SAM-DiffSR Structure-Modulated Diffusion Model for Image   Super-Resolution
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-14
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">24231k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
