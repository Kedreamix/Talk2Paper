<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS 方向最新论文已更新，请持续关注 Update in 2025-08-02  Gaussian Variation Field Diffusion for High-fidelity Video-to-4D   Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c20ea647f542e878705d061131ccfb2a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    76 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-02-更新"><a href="#2025-08-02-更新" class="headerlink" title="2025-08-02 更新"></a>2025-08-02 更新</h1><h2 id="Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis"><a href="#Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis" class="headerlink" title="Gaussian Variation Field Diffusion for High-fidelity Video-to-4D   Synthesis"></a>Gaussian Variation Field Diffusion for High-fidelity Video-to-4D   Synthesis</h2><p><strong>Authors:Bowen Zhang, Sicheng Xu, Chuxin Wang, Jiaolong Yang, Feng Zhao, Dong Chen, Baining Guo</strong></p>
<p>In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: <a target="_blank" rel="noopener" href="https://gvfdiffusion.github.io/">https://gvfdiffusion.github.io/</a>. </p>
<blockquote>
<p>在本文中，我们提出了一个用于视频到4D生成的新型框架，该框架能够从单个视频输入中创建高质量动态3D内容。直接4D扩散建模由于数据构建成本高昂以及同时表示3D形状、外观和运动的维数过高而极具挑战性。我们通过引入Direct 4DMesh-to-GS变化场VAE来解决这些挑战，该VAE能够直接从3D动画数据对规范高斯Splats（GS）及其时间变化进行编码，而无需逐个实例进行拟合，并将高维动画压缩到紧凑的潜在空间中。在此基础上，我们训练了一个基于时间感知扩散Transformer的高斯变化场扩散模型，该模型根据输入视频和规范GS进行训练。该模型在精心挑选的来自Objaverse数据集的动画3D对象上进行训练，与现有方法相比，其生成质量更胜一筹。尽管该模型仅在合成数据上进行训练，但它对天然视频输入的泛化能力仍然十分显著，为生成高质量动画3D内容铺平了道路。项目页面：<a target="_blank" rel="noopener" href="https://gvfdiffusion.github.io/%E3%80%82">https://gvfdiffusion.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23785v1">PDF</a> ICCV 2025. Project page: <a target="_blank" rel="noopener" href="https://gvfdiffusion.github.io/">https://gvfdiffusion.github.io/</a></p>
<p><strong>Summary</strong><br>本文提出了一种新颖的框架，可从单个视频输入生成高质量动态3D内容。通过引入Direct 4DMesh-to-GS Variation Field VAE，直接对规范高斯Splats（GS）及其时间变化进行编码，无需对每个实例进行拟合，并将高维动画压缩到紧凑的潜在空间。在此基础上，训练了一个基于高斯变异场扩散模型的具有时间感知扩散变压器的模型，以输入视频和规范GS为条件。在Objaverse数据集的可动画3D对象上进行训练，与现有方法相比，该模型展现出更高的生成质量，并且对野生视频输入具有出色的泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>论文提出了一种新的视频到4D生成框架，可从单个视频创建高质量动态3D内容。</li>
<li>引入Direct 4DMesh-to-GS Variation Field VAE，有效表示和压缩3D动画数据。</li>
<li>采用高斯变异场扩散模型，结合时间感知扩散变压器进行训练。</li>
<li>模型在Objaverse数据集上进行训练，展现出优秀的生成质量。</li>
<li>模型具有良好的泛化能力，能处理来自野生环境的视频输入。</li>
<li>该模型能够直接处理规范高斯Splats（GS）及其时间变化，无需对每个实例进行拟合。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23785">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a0a1b89aa26ce3d68dfdd6a9bfc7138c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c831b3064b5b518234a2cb590c26c41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b3a04415c0d6138eb44fddde3539229.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c20ea647f542e878705d061131ccfb2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-102ab5b4a8dfd7bbabc5cae8a1d38e20.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SeqAffordSplat-Scene-level-Sequential-Affordance-Reasoning-on-3D-Gaussian-Splatting"><a href="#SeqAffordSplat-Scene-level-Sequential-Affordance-Reasoning-on-3D-Gaussian-Splatting" class="headerlink" title="SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D   Gaussian Splatting"></a>SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D   Gaussian Splatting</h2><p><strong>Authors:Di Li, Jie Feng, Jiahao Chen, Weisheng Dong, Guanbin Li, Yuhui Zheng, Mingtao Feng, Guangming Shi</strong></p>
<p>3D affordance reasoning, the task of associating human instructions with the functional regions of 3D objects, is a critical capability for embodied agents. Current methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited to single-object, single-step interactions, a paradigm that falls short of addressing the long-horizon, multi-object tasks required for complex real-world applications. To bridge this gap, we introduce the novel task of Sequential 3D Gaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale benchmark featuring 1800+ scenes to support research on long-horizon affordance understanding in complex 3DGS environments. We then propose SeqSplatNet, an end-to-end framework that directly maps an instruction to a sequence of 3D affordance masks. SeqSplatNet employs a large language model that autoregressively generates text interleaved with special segmentation tokens, guiding a conditional decoder to produce the corresponding 3D mask. To handle complex scene geometry, we introduce a pre-training strategy, Conditional Geometric Reconstruction, where the model learns to reconstruct complete affordance region masks from known geometric observations, thereby building a robust geometric prior. Furthermore, to resolve semantic ambiguities, we design a feature injection mechanism that lifts rich semantic features from 2D Vision Foundation Models (VFM) and fuses them into the 3D decoder at multiple scales. Extensive experiments demonstrate that our method sets a new state-of-the-art on our challenging benchmark, effectively advancing affordance reasoning from single-step interactions to complex, sequential tasks at the scene level. </p>
<blockquote>
<p>3D作用推理是一项将人类指令与3D对象的功能区域相关联的重要任务，对于实体代理而言是一项至关重要的能力。当前基于3D高斯泼溅（3DGS）的方法根本上仅限于单对象、单步骤交互的模式，这种模式无法应对复杂现实世界应用所需的长周期、多对象任务。为了弥补这一差距，我们引入了序列3D高斯作用推理这一新任务，并建立了SeqAffordSplat大规模基准测试，包含1800多个场景，以支持在复杂3DGS环境中对长周期作用理解的研究。然后，我们提出了SeqSplatNet端到端框架，该框架直接将指令映射到一系列3D作用掩膜。SeqSplatNet采用大型语言模型，通过自回归生成与特殊分割令牌交织的文本，指导条件解码器生成相应的3D掩膜。为了处理复杂的场景几何结构，我们引入了预训练策略，即条件几何重建，在该策略中，模型学习从已知的几何观测中重建完整的作用区域掩膜，从而建立稳健的几何先验。此外，为了解决语义歧义问题，我们设计了一种特征注入机制，从二维视觉基础模型（VFM）中提取丰富的语义特征，并将它们融合到多个尺度的三维解码器中。大量实验表明，我们的方法在我们的具有挑战性的基准测试上达到了最新水平，有效地将作用推理从单步骤交互推进到复杂的场景级序列任务。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23772v1">PDF</a> </p>
<p><strong>摘要</strong></p>
<p>本文介绍了三维高斯分割技术（3DGS）在智能体执行任务中的局限性，特别是在处理复杂现实世界中的长期多目标任务时的不足。为解决这一问题，本文提出了序列三维高斯功能推理（Sequential 3D Gaussian Affordance Reasoning）的新任务，并建立了SeqAffordSplat大规模基准测试平台，包含超过1800个场景。此外，本文还提出了一种名为SeqSplatNet的端到端框架，该框架可直接将指令映射到一系列三维功能掩膜。SeqSplatNet采用大型语言模型进行预训练，并通过条件解码器生成相应的三维掩膜。为解决场景几何复杂性和语义模糊性问题，本文引入了条件几何重建策略和特征注入机制。实验证明，SeqSplatNet在复杂场景级别的长期多目标任务上表现优异，将功能推理从单步交互推向了新的高度。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>当前基于三维高斯分割（3DGS）的方法在处理长期多目标任务时存在局限性。</li>
<li>引入了序列三维高斯功能推理的新任务，以应对复杂现实世界中的长期多目标任务。</li>
<li>建立了SeqAffordSplat大规模基准测试平台，包含多样化场景，支持长期功能理解的研究。</li>
<li>提出了SeqSplatNet框架，能直接处理指令并生成一系列三维功能掩膜。</li>
<li>采用大型语言模型进行预训练，解决场景几何复杂性问题。</li>
<li>引入条件几何重建策略，提升模型对完整功能区域的重建能力。</li>
<li>设计了特征注入机制，融合二维视觉基础模型的丰富语义特征，提高语义理解的准确性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23772">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-52fcd06f4e24d5923434944fdc122204.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42723816d289ea4eb2cdc248de4cff9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2be20f2dc5a62b2156870f4d01e4ab69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06f335d5aec48beb87d4479869f0e46d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MoGA-3D-Generative-Avatar-Prior-for-Monocular-Gaussian-Avatar-Reconstruction"><a href="#MoGA-3D-Generative-Avatar-Prior-for-Monocular-Gaussian-Avatar-Reconstruction" class="headerlink" title="MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar   Reconstruction"></a>MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar   Reconstruction</h2><p><strong>Authors:Zijian Dong, Longteng Duan, Jie Song, Michael J. Black, Andreas Geiger</strong></p>
<p>We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian avatars from a single-view image. The main challenge lies in inferring unseen appearance and geometric details while ensuring 3D consistency and realism. Most previous methods rely on 2D diffusion models to synthesize unseen views; however, these generated views are sparse and inconsistent, resulting in unrealistic 3D artifacts and blurred appearance. To address these limitations, we leverage a generative avatar model, that can generate diverse 3D avatars by sampling deformed Gaussians from a learned prior distribution. Due to the limited amount of 3D training data such a 3D model alone cannot capture all image details of unseen identities. Consequently, we integrate it as a prior, ensuring 3D consistency by projecting input images into its latent space and enforcing additional 3D appearance and geometric constraints. Our novel approach formulates Gaussian avatar creation as a model inversion process by fitting the generative avatar to synthetic views from 2D diffusion models. The generative avatar provides a meaningful initialization for model fitting, enforces 3D regularization, and helps in refining pose estimation. Experiments show that our method surpasses state-of-the-art techniques and generalizes well to real-world scenarios. Our Gaussian avatars are also inherently animatable </p>
<blockquote>
<p>我们提出了MoGA，这是一种从单视图图像重建高保真3D高斯头像的新方法。主要挑战在于推断出看不见的外观和几何细节，同时确保3D的一致性和真实性。大多数之前的方法依赖于2D扩散模型来合成未见的视图；然而，这些生成的视图是稀疏且不一致的，导致3D人工制品不真实和外观模糊。为了解决这些局限性，我们利用生成头像模型，通过从学习的先验分布中采样变形高斯来生成各种3D头像。由于3D训练数据的数量有限，仅使用这样的3D模型无法捕获未见身份的所有图像细节。因此，我们将其整合为优先事项，通过将输入图像投影到其潜在空间并强制额外的3D外观和几何约束来确保3D一致性。我们的新方法将高斯头像创建公式化为一个模型反转过程，通过将生成的头像拟合到来自2D扩散模型的合成视图。生成的头像为模型拟合提供了一个有意义的初始化，强制进行3D正则化，并有助于改进姿势估计。实验表明，我们的方法超越了最新技术，并能很好地推广到现实世界场景。我们的高斯头像本质上是可动画的。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23597v1">PDF</a> ICCV 2025 (Highlight), Project Page: <a target="_blank" rel="noopener" href="https://zj-dong.github.io/MoGA/">https://zj-dong.github.io/MoGA/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出了MoGA，一种从单视角图像重建高保真3D高斯头像的新方法。主要挑战在于推断出隐藏的外观和几何细节，同时确保3D的一致性和真实性。通过结合生成头像模型和投影输入图像到潜在空间等技术，解决了以往方法生成的视图稀疏、不一致、导致3D效果不真实的问题。新方法将高斯头像创建表述为模型反演过程，通过拟合生成头像到来自二维扩散模型的合成视图。实验表明，该方法超越现有技术，并良好地推广至真实世界场景。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的方法MoGA，能够从单视角图像重建高保真3D高斯头像。</li>
<li>解决了以往方法生成的视图稀疏、不一致的问题，提高了3D头像的真实性和一致性。</li>
<li>通过结合生成头像模型，采用从学习到的先验分布中采样变形高斯的方式，生成多样的3D头像。</li>
<li>将生成头像模型作为先验，通过投影输入图像到潜在空间，加强3D一致性。</li>
<li>将高斯头像创建表述为模型反演过程，通过拟合生成头像到二维扩散模型的合成视图。</li>
<li>MoGA方法超越了现有技术，并在真实世界场景中具有良好的推广性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23597">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-36299fa1d2de31244961425d9c2f9e03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92c4cfb63cc8def479f57a52e048adf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4030795a97d1504b3de20f5c85aeca83.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Gaussian-Splatting-Feature-Fields-for-Privacy-Preserving-Visual-Localization"><a href="#Gaussian-Splatting-Feature-Fields-for-Privacy-Preserving-Visual-Localization" class="headerlink" title="Gaussian Splatting Feature Fields for Privacy-Preserving Visual   Localization"></a>Gaussian Splatting Feature Fields for Privacy-Preserving Visual   Localization</h2><p><strong>Authors:Maxime Pietrantoni, Gabriela Csurka, Torsten Sattler</strong></p>
<p>Visual localization is the task of estimating a camera pose in a known environment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based representations for accurate and privacy-preserving visual localization. We propose Gaussian Splatting Feature Fields (GSFFs), a scene representation for visual localization that combines an explicit geometry model (3DGS) with an implicit feature field. We leverage the dense geometric information and differentiable rasterization algorithm from 3DGS to learn robust feature representations grounded in 3D. In particular, we align a 3D scale-aware feature field and a 2D feature encoder in a common embedding space through a contrastive framework. Using a 3D structure-informed clustering procedure, we further regularize the representation learning and seamlessly convert the features to segmentations, which can be used for privacy-preserving visual localization. Pose refinement, which involves aligning either feature maps or segmentations from a query image with those rendered from the GSFFs scene representation, is used to achieve localization. The resulting privacy- and non-privacy-preserving localization pipelines, evaluated on multiple real-world datasets, show state-of-the-art performances. </p>
<blockquote>
<p>视觉定位是在已知环境中估计相机姿态的任务。在本文中，我们利用基于3D高斯拼贴（3DGS）的表示来进行精确且保护隐私的视觉定位。我们提出高斯拼贴特征场（GSFFs），这是一种视觉定位的场景表示，它将显式的几何模型（3DGS）与隐式的特征场相结合。我们利用3DGS的密集几何信息和可微栅格化算法，学习基于3D的稳健特征表示。特别是，我们通过对比框架在3D尺度感知特征场和2D特征编码器之间建立一个通用的嵌入空间。通过利用3D结构信息的聚类程序，我们进一步正则化表示学习，无缝地将特征转换为分割，这可用于保护隐私的视觉定位。姿态修正涉及将查询图像的特征图或分割与从GSFFs场景表示呈现的特征进行对齐，以实现定位。在多个真实世界数据集上评估的隐私和非隐私保护定位流程均显示出最新技术性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23569v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>本文利用基于三维高斯拼贴（3DGS）的表示方法进行精确且保护隐私的视觉定位。提出高斯拼贴特征场（GSFFs），结合显式几何模型（3DGS）和隐式特征场，用于视觉定位的场景表示。通过利用3DGS的密集几何信息和可微分栅格化算法，学习基于三维的稳健特征表示。通过对比框架，将三维尺度感知特征场和二维特征编码器对齐到公共嵌入空间。利用三维结构感知聚类程序，进一步规范表示学习，无缝转换特征为分段，可用于保护隐私的视觉定位。通过姿势微调，实现定位，即在查询图像的特征图或分段与GSFFs场景表示所呈现的特征图或分段之间进行对齐。在多个真实世界数据集上的评估结果证明了隐私和非隐私保护定位管道均达到最新技术水平。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>利用三维高斯拼贴（3DGS）进行视觉定位，实现精确性并保护隐私。</li>
<li>提出高斯拼贴特征场（GSFFs），结合显式几何模型与隐式特征场。</li>
<li>利用3DGS的密集几何信息和可微分栅格化算法，学习稳健的三维特征表示。</li>
<li>通过对比框架对齐三维尺度感知特征场和二维特征编码器。</li>
<li>采用三维结构感知聚类程序，实现特征到分段的转换，用于隐私保护定位。</li>
<li>通过姿势微调实现定位，通过对齐查询图像的特征图或分段与场景表示的特征图或分段来完成。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23569">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-c742e0212b82cdb83a8e4fcc62fd7f91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05694e9146f8a22aae5ebbb28cf8226d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3505621d0bc150550397b2c4d62c76a2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="NeRF-Is-a-Valuable-Assistant-for-3D-Gaussian-Splatting"><a href="#NeRF-Is-a-Valuable-Assistant-for-3D-Gaussian-Splatting" class="headerlink" title="NeRF Is a Valuable Assistant for 3D Gaussian Splatting"></a>NeRF Is a Valuable Assistant for 3D Gaussian Splatting</h2><p><strong>Authors:Shuangkang Fang, I-Chao Shen, Takeo Igarashi, Yufeng Wang, ZeSheng Wang, Yi Yang, Wenrui Ding, Shuchang Zhou</strong></p>
<p>We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation. </p>
<blockquote>
<p>我们介绍了NeRF-GS，这是一个新型框架，它联合优化了神经辐射场（NeRF）和3D高斯拼贴（3DGS）。该框架利用NeRF的固有连续空间表示来减轻3DGS的几个局限性，包括高斯初始化的敏感性、空间感知的局限性以及高斯间关联较弱等问题，从而提高了其性能。在NeRF-GS中，我们重新设计了3DGS，并逐步将其空间特征与NeRF对齐，使两种表示形式能够通过共享的三维空间信息在同一场景中进行优化。我们进一步通过优化隐特征和高斯位置的残差向量来解决两种方法之间的形式差异，以增强3DGS的个性化能力。在基准数据集上的实验结果表明，NeRF-GS超越了现有方法，达到了最先进的性能。这一结果证实，NeRF和3DGS是互补的而不是竞争的，为结合3DGS和NeRF的高效3D场景表示提供了混合方法的全新见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23374v1">PDF</a> Accepted by ICCV</p>
<p><strong>摘要</strong></p>
<p>NeRF-GS是一种结合了神经辐射场（NeRF）和三维高斯拼贴（3DGS）的新型框架。它通过利用NeRF的内在连续空间表示来克服3DGS的几个局限性，包括高斯初始化的敏感性、空间感知的有限性以及高斯间关联性的微弱。在NeRF-GS中，重新设计了3DGS，逐步将其空间特征与NeRF对齐，使两种表示方法能够在同一场景中进行优化，通过共享三维空间信息来增强性能。通过优化隐式特征和高斯位置的残差向量，解决了两者之间的形式差异，提高了3DGS的个性化能力。在基准数据集上的实验结果表明，NeRF-GS超越了现有方法，达到了最先进的性能。这证明了NeRF和3DGS是互补的而不是竞争的，为结合3DGS和NeRF的有效三维场景表示提供了新见解。</p>
<p><strong>要点</strong></p>
<ol>
<li>NeRF-GS结合了神经辐射场（NeRF）和三维高斯拼贴（3DGS）的新型框架。</li>
<li>NeRF-GS克服了3DGS的几个局限性，包括高斯初始化的敏感性、空间感知的有限性以及高斯间关联性的微弱。</li>
<li>通过共享三维空间信息，NeRF和3DGS在优化中可以相互补充。</li>
<li>重新设计了3DGS的空间特征，使其与NeRF对齐。</li>
<li>通过优化隐式特征和高斯位置的残差向量，解决了NeRF和3DGS之间的形式差异。</li>
<li>实验结果表明，NeRF-GS达到了最先进的性能，超越了现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23374">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-171cb64b2dc7ed8ead6e62a2bc7dc785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dad08707822011a404d304345bd3a4a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92a93dbadca0a7676aa1e22e393da7a5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="iLRM-An-Iterative-Large-3D-Reconstruction-Model"><a href="#iLRM-An-Iterative-Large-3D-Reconstruction-Model" class="headerlink" title="iLRM: An Iterative Large 3D Reconstruction Model"></a>iLRM: An Iterative Large 3D Reconstruction Model</h2><p><strong>Authors:Gyeongjin Kang, Seungtae Nam, Xiangyu Sun, Sameh Khamis, Abdelrahman Mohamed, Eunbyung Park</strong></p>
<p>Feed-forward 3D modeling has emerged as a promising approach for rapid and high-quality 3D reconstruction. In particular, directly generating explicit 3D representations, such as 3D Gaussian splatting, has attracted significant attention due to its fast and high-quality rendering, as well as numerous applications. However, many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases. Toward a scalable and efficient feed-forward 3D reconstruction, we introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction. Experimental results on widely used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms existing methods in both reconstruction quality and speed. Notably, iLRM exhibits superior scalability, delivering significantly higher reconstruction quality under comparable computational cost by efficiently leveraging a larger number of input views. </p>
<blockquote>
<p>前馈3D建模已成为一种有前景的快速、高质量3D重建方法。特别是直接生成显式3D表示（如3D高斯散斑）引起了广泛关注，因其快速、高质量的渲染以及众多的应用。然而，许多最先进的方法主要基于transformer架构，存在严重的可扩展性问题，因为它们依赖于来自多个输入视图的图像标记的全注意力，随着视图数量或图像分辨率的增加，计算成本成为禁止性成本。为了实现可伸缩和高效的前馈3D重建，我们引入了一种迭代式大型3D重建模型（iLRM），该模型通过迭代细化机制生成3D高斯表示，由三个核心原则指导：（1）将场景表示与输入视图图像解耦，以实现紧凑的3D表示；（2）将全注意力多视图交互分解为两阶段注意力方案，以降低计算成本；（3）在每一层注入高分辨率信息，以实现高保真重建。在RE10K和DL3DV等常用数据集上的实验结果表明，iLRM在重建质量和速度方面均优于现有方法。值得注意的是，iLRM表现出卓越的可扩展性，通过有效利用更多的输入视图，在可比较的计算成本下实现更高的重建质量。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23277v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://gynjn.github.io/iLRM/">https://gynjn.github.io/iLRM/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于迭代的大型三维重建模型（iLRM）在三维重建中的应用。iLRM通过迭代优化机制生成三维高斯表示，采用三个核心原则实现高效的三维重建：解耦场景表示与输入视图图像，实现紧凑的三维表示；将全注意力多视图交互分解为两阶段注意力方案以降低计算成本；在每一层注入高分辨率信息以实现高保真重建。实验结果表明，iLRM在重建质量和速度方面均优于现有方法，展现出优越的扩展性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>文中提出利用迭代的大型三维重建模型（iLRM）实现高效的三维重建。该模型采用迭代优化机制生成三维高斯表示。</li>
<li>iLRM通过三个核心原则实现紧凑的三维表示和高效计算：解耦场景表示与输入视图图像；采用两阶段注意力方案降低计算成本；注入高分辨率信息以提升重建质量。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23277">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-222ee592da0aa00e9b4a41e213db2fc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3532ca4b0494f05a9cabfddc19e66960.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf3d562fe0bcd403789d249c4c9f60d0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="GSFusion-Globally-Optimized-LiDAR-Inertial-Visual-Mapping-for-Gaussian-Splatting"><a href="#GSFusion-Globally-Optimized-LiDAR-Inertial-Visual-Mapping-for-Gaussian-Splatting" class="headerlink" title="GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian   Splatting"></a>GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian   Splatting</h2><p><strong>Authors:Jaeseok Park, Chanoh Park, Minsu Kim, Soohwan Kim</strong></p>
<p>While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping, conventional approaches based on camera sensor, even RGB-D, suffer from fundamental limitations such as high computational load, failure in environments with poor texture or illumination, and short operational ranges. LiDAR emerges as a robust alternative, but its integration with 3DGS introduces new challenges, such as the need for exceptional global alignment for photorealistic quality and prolonged optimization times caused by sparse data. To address these challenges, we propose GSFusion, an online LiDAR-Inertial-Visual mapping system that ensures high-precision map consistency through a surfel-to-surfel constraint in the global pose-graph optimization. To handle sparse data, our system employs a pixel-aware Gaussian initialization strategy for efficient representation and a bounded sigmoid constraint to prevent uncontrolled Gaussian growth. Experiments on public and our datasets demonstrate our system outperforms existing 3DGS SLAM systems in terms of rendering quality and map-building efficiency. </p>
<blockquote>
<p>虽然3D高斯延展（3DGS）已经实现了对真实感映射的革命性改变，但基于相机传感器的传统方法，甚至是RGB-D，仍然存在一些基本局限，如计算负载高、在纹理或照明不良的环境中失效以及操作范围短等。激光雷达作为一种稳健的替代方案而出现，但将其与3DGS集成带来了新的挑战，例如为了实现真实感质量而需要进行出色的全局对齐以及由于稀疏数据而导致的优化时间延长。为了解决这些挑战，我们提出了GSFusion，这是一种在线激光雷达惯性视觉映射系统，它通过全局姿态图优化中的surfel-to-surfel约束确保高精度地图一致性。为了处理稀疏数据，我们的系统采用像素感知高斯初始化策略进行高效表示，并使用有界sigmoid约束来防止高斯增长失控。在公共数据集和我们自己的数据集上的实验表明，我们的系统在渲染质量和地图构建效率方面超过了现有的3DGS SLAM系统。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23273v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于LiDAR的在线三维映射系统GSFusion，该系统结合了LiDAR、惯性测量和视觉传感器技术，解决了传统方法在光照和纹理不佳环境下存在的问题，同时通过surfels全局姿态优化算法保证了高质量的三维重建精度和地图一致性。采用像素感知高斯初始化策略应对稀疏数据挑战，并在优化过程中应用边界sigmoid约束以避免高斯增长失控。实验证明，该系统在渲染质量和地图构建效率方面优于现有的基于3DGS的SLAM系统。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>传统的基于相机传感器的RGB-D方法在光照和纹理不佳环境下存在局限性。</li>
<li>LiDAR作为一种替代方法具有稳健性，但与3DGS集成带来全局对齐挑战和稀疏数据处理问题。</li>
<li>GSFusion系统结合LiDAR、惯性测量和视觉传感器技术，实现了高精度地图一致性。</li>
<li>GSFusion通过surfels全局姿态优化算法解决了精准性问题。</li>
<li>系统采用像素感知高斯初始化策略应对稀疏数据挑战。</li>
<li>采用边界sigmoid约束防止高斯增长失控。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23273">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f7acb216c6d6b0cfa1b4805bfcac0993.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5b8d880ba25496f2706dad94cfc18bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-befbd51fb45746ec23c183dd7b1cac9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f58c15ce61dfcbdff17835e1e5057829.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e244bb4207fa88a0a6ee6066318221a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89efbbfc154dbcd64635bf5647df6e7b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Robust-and-Efficient-3D-Gaussian-Splatting-for-Urban-Scene-Reconstruction"><a href="#Robust-and-Efficient-3D-Gaussian-Splatting-for-Urban-Scene-Reconstruction" class="headerlink" title="Robust and Efficient 3D Gaussian Splatting for Urban Scene   Reconstruction"></a>Robust and Efficient 3D Gaussian Splatting for Urban Scene   Reconstruction</h2><p><strong>Authors:Zhensheng Yuan, Haozhi Huang, Zhen Xiong, Di Wang, Guanghua Yang</strong></p>
<p>We present a framework that enables fast reconstruction and real-time rendering of urban-scale scenes while maintaining robustness against appearance variations across multi-view captures. Our approach begins with scene partitioning for parallel training, employing a visibility-based image selection strategy to optimize training efficiency. A controllable level-of-detail (LOD) strategy explicitly regulates Gaussian density under a user-defined budget, enabling efficient training and rendering while maintaining high visual fidelity. The appearance transformation module mitigates the negative effects of appearance inconsistencies across images while enabling flexible adjustments. Additionally, we utilize enhancement modules, such as depth regularization, scale regularization, and antialiasing, to improve reconstruction fidelity. Experimental results demonstrate that our method effectively reconstructs urban-scale scenes and outperforms previous approaches in both efficiency and quality. The source code is available at: <a target="_blank" rel="noopener" href="https://yzslab.github.io/REUrbanGS">https://yzslab.github.io/REUrbanGS</a>. </p>
<blockquote>
<p>我们提出了一种框架，能够在多视角捕获中快速重建和实时渲染城市场景，同时保持对各种外观变化的稳健性。我们的方法从场景分区并行训练开始，采用基于可见性的图像选择策略来优化训练效率。可控的层次细节（LOD）策略在用户定义的预算下明确调节高斯密度，能够在保持高视觉保真度的同时实现高效的训练和渲染。外观变换模块减轻了图像间外观不一致的负面影响，同时实现灵活调整。此外，我们还使用增强模块，如深度正则化、尺度正则化和抗锯齿，以提高重建的保真度。实验结果表明，我们的方法在重建城市场景方面非常有效，在效率和质量方面都优于以前的方法。源代码可在：<a target="_blank" rel="noopener" href="https://yzslab.github.io/REUrbanGS%E3%80%82">https://yzslab.github.io/REUrbanGS。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23006v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>该框架支持城市级场景的快速重建和实时渲染，能应对多视角捕捉中的外观变化。采用基于可见性的图像选择策略进行场景分区并行训练，优化训练效率。可控的细节层次（LOD）策略在用户定义预算下调控高斯密度，实现高效训练和渲染的同时保持高视觉保真度。外观变换模块减轻了图像间外观不一致的负面影响，并实现了灵活调整。此外，还使用深度正则化、尺度正则化和抗锯齿等增强模块，提高重建的保真度。实验结果证明，该方法在城市级场景的重建中表现优异，在效率和质量上均超过之前的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>框架支持城市级场景的快速重建和实时渲染。</li>
<li>采用场景分区并行训练和基于可见性的图像选择策略优化训练效率。</li>
<li>细节层次（LOD）策略调控高斯密度，实现高效训练和渲染。</li>
<li>外观变换模块应对多视角捕捉中的外观变化。</li>
<li>增强模块如深度正则化、尺度正则化和抗锯齿提高重建的保真度。</li>
<li>框架性能优越，在重建效率和质量上超越之前的方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23006">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f4125890218d476f06350f1155194a7c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1db203a777ffb9ef2cc9344c48e4fb7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20d54897e4d3000957d4ccd78aef5c59.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DISCOVERSE-Efficient-Robot-Simulation-in-Complex-High-Fidelity-Environments"><a href="#DISCOVERSE-Efficient-Robot-Simulation-in-Complex-High-Fidelity-Environments" class="headerlink" title="DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity   Environments"></a>DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity   Environments</h2><p><strong>Authors:Yufei Jia, Guangyu Wang, Yuhang Dong, Junzhe Wu, Yupei Zeng, Haonan Lin, Zifan Wang, Haizhou Ge, Weibin Gu, Kairui Ding, Zike Yan, Yunjie Cheng, Yue Li, Ziming Wang, Chuxuan Li, Wei Sui, Lu Shi, Guanzhong Tian, Ruqi Huang, Guyue Zhou</strong></p>
<p>We present the first unified, modular, open-source 3DGS-based simulation framework for Real2Sim2Real robot learning. It features a holistic Real2Sim pipeline that synthesizes hyper-realistic geometry and appearance of complex real-world scenarios, paving the way for analyzing and bridging the Sim2Real gap. Powered by Gaussian Splatting and MuJoCo, Discoverse enables massively parallel simulation of multiple sensor modalities and accurate physics, with inclusive supports for existing 3D assets, robot models, and ROS plugins, empowering large-scale robot learning and complex robotic benchmarks. Through extensive experiments on imitation learning, Discoverse demonstrates state-of-the-art zero-shot Sim2Real transfer performance compared to existing simulators. For code and demos: <a target="_blank" rel="noopener" href="https://air-discoverse.github.io/">https://air-discoverse.github.io/</a>. </p>
<blockquote>
<p>我们推出了首个统一、模块化、开源的基于3DGS的Real2Sim2Real机器人学习仿真框架。它具备全面的Real2Sim管道，能够合成超逼真的复杂现实世界场景的几何和外观，为分析和弥合Sim2Real差距铺平了道路。借助高斯Splatting和MuJoCo，Discoverse能够实现多种传感器模式的大规模并行仿真和精确的物理模拟，同时支持现有的3D资产、机器人模型和ROS插件，为大规模的机器人学习和复杂的机器人基准测试提供支持。通过模仿学习的广泛实验，Discoverse展现出与现有模拟器相比的卓越零射击Sim2Real传输性能。有关代码和演示，请访问：<a target="_blank" rel="noopener" href="https://air-discoverse.github.io/%E3%80%82">https://air-discoverse.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21981v1">PDF</a> 8pages, IROS2025 (Camera Ready)</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个基于3DGS的统一、模块化、开源的仿真框架Discoverse，用于Real2Sim2Real机器人学习。该框架具备完整的Real2Sim管道，能合成复杂真实场景的超高仿真几何与外观，为分析并缩小Sim2Real差距铺平了道路。借助高斯拼贴和MuJoCo，Discoverse支持多种传感器模式的并行仿真和精确物理计算，并支持现有3D资产、机器人模型和ROS插件，为大规模机器人学习和复杂机器人基准测试提供了支持。通过模仿学习的广泛实验，Discoverse展现出相较于其他模拟器的卓越零射击Sim2Real转移性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>基于3DGS的仿真框架：Discoverse为Real2Sim2Real机器人学习提供了首个统一、模块化、开源的仿真框架。</li>
<li>Real2Sim管道：该框架具备完整的Real2Sim管道，能够合成高度逼真的几何和外观，有助于缩小Sim2Real差距。</li>
<li>支持多种传感器模式：Discoverse支持多种传感器模式的并行仿真。</li>
<li>精确物理计算：借助高斯拼贴和MuJoCo，Discoverse实现了精确的物理计算。</li>
<li>广泛的支持性：Discoverse支持现有3D资产、机器人模型和ROS插件，为大规模机器人学习和基准测试提供了支持。</li>
<li>卓越性能表现：在模仿学习实验中，Discoverse展现出卓越的性能表现，尤其是零射击Sim2Real转移性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21981">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-bc72e3353bde17d1437b78a901a296cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a6dd341fd0e24c14e038e0ac6987ae5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-164ee04c5f2137bf5f32f3386d0012ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6388b8a6cdeb972d9f70bce8a860e12.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bba66a930b0d9c926b427e7719081a24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59e78eace38459789503c87d616e39f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e7e290f0a2cb46afe0a4082cac4a34f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RaGS-Unleashing-3D-Gaussian-Splatting-from-4D-Radar-and-Monocular-Cues-for-3D-Object-Detection"><a href="#RaGS-Unleashing-3D-Gaussian-Splatting-from-4D-Radar-and-Monocular-Cues-for-3D-Object-Detection" class="headerlink" title="RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues   for 3D Object Detection"></a>RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues   for 3D Object Detection</h2><p><strong>Authors:Xiaokai Bai, Chenxu Zhou, Lianqing Zheng, Si-Yuan Cao, Jianan Liu, Xiaohan Zhang, Zhengzhuang Zhang, Hui-liang Shen</strong></p>
<p>4D millimeter-wave radar has emerged as a promising sensor for autonomous driving, but effective 3D object detection from both 4D radar and monocular images remains a challenge. Existing fusion approaches typically rely on either instance-based proposals or dense BEV grids, which either lack holistic scene understanding or are limited by rigid grid structures. To address these, we propose RaGS, the first framework to leverage 3D Gaussian Splatting (GS) as representation for fusing 4D radar and monocular cues in 3D object detection. 3D GS naturally suits 3D object detection by modeling the scene as a field of Gaussians, dynamically allocating resources on foreground objects and providing a flexible, resource-efficient solution. RaGS uses a cascaded pipeline to construct and refine the Gaussian field. It starts with the Frustum-based Localization Initiation (FLI), which unprojects foreground pixels to initialize coarse 3D Gaussians positions. Then, the Iterative Multimodal Aggregation (IMA) fuses semantics and geometry, refining the limited Gaussians to the regions of interest. Finally, the Multi-level Gaussian Fusion (MGF) renders the Gaussians into multi-level BEV features for 3D object detection. By dynamically focusing on sparse objects within scenes, RaGS enable object concentrating while offering comprehensive scene perception. Extensive experiments on View-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate its state-of-the-art performance. Code will be released. </p>
<blockquote>
<p>随着自动驾驶的兴起，4D毫米波雷达已经成为了一个有前景的传感器，但如何从雷达的雷达数据和单目图像进行高效3D物体检测仍然是一个挑战。现有的融合方法大多依赖于实例化的提议或密集的BEV网格，它们要么缺乏全局场景理解，要么受限于刚性的网格结构。为了解决这些问题，我们提出了RaGS框架，首次使用三维高斯平铺（GS）作为融合四维雷达和单目视觉提示的代表，用于三维物体检测。三维高斯表示法自然地适合三维物体检测，它将场景建模为高斯场，动态分配资源于前景物体，并提供灵活、资源高效的解决方案。RaGS使用级联管道来构建和细化高斯场。它以基于视锥的定位初始化（FLI）开始，将前景像素反投影以初始化粗略的三维高斯位置。然后，迭代多模态聚合（IMA）融合语义和几何信息，将有限的高斯细分到感兴趣区域。最后，多级高斯融合（MGF）将高斯渲染成多级BEV特征用于三维物体检测。通过动态关注场景中的稀疏物体，RaGS能够在物体集中时提供全面的场景感知。在View-of-Delft、TJ4DRadSet和OmniHD-Scenes基准测试上的大量实验证明了其卓越的性能。代码将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19856v2">PDF</a> 9 pages, 6 figures, conference</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为RaGS的新框架，用于在自主驾驶中的四维雷达和单目图像融合的三维对象检测。它通过采用三维高斯模板作为场景表示方法，并通过一系列模块来构建和优化高斯场实现精准的三维对象检测。RaGS的创新点在于其对前景对象进行动态资源分配并提供灵活、资源高效的解决方案，最终通过多层次高斯融合（MGF）实现先进性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>4D雷达已成为自主驾驶中有前景的传感器，但融合雷达和图像实现三维物体检测仍然存在挑战。</li>
<li>目前融合方法主要依赖于实例提议或密集BEV网格，存在对场景理解不全面或受网格结构限制的问题。</li>
<li>RaGS框架首次利用三维高斯模板（GS）作为表示方法，融合四维雷达和单目图像进行三维对象检测。</li>
<li>三维高斯模板自然适用于三维对象检测，通过动态资源分配对前景对象进行建模，提供灵活且资源高效的解决方案。</li>
<li>RaGS使用级联管道构建和优化高斯场，包括基于视锥的定位启动（FLI）、迭代多模式聚合（IMA）和多层次高斯融合（MGF）。</li>
<li>通过聚焦场景中的稀疏对象，RaGS实现了对象集中并提供了全面的场景感知。</li>
<li>在多个基准测试中，RaGS表现出卓越性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19856">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0edc511009429e77c152020ee011a982.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-557174558a450f85c8820260f0f9ac55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d093c386ad98b81e155e79307803ffa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b7884256c56c8ff35df1bf5e8575871.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acbab909692d1066dfa6f86b11ee8862.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GS-Occ3D-Scaling-Vision-only-Occupancy-Reconstruction-for-Autonomous-Driving-with-Gaussian-Splatting"><a href="#GS-Occ3D-Scaling-Vision-only-Occupancy-Reconstruction-for-Autonomous-Driving-with-Gaussian-Splatting" class="headerlink" title="GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous   Driving with Gaussian Splatting"></a>GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous   Driving with Gaussian Splatting</h2><p><strong>Authors:Baijun Ye, Minghui Qin, Saining Zhang, Moonjun Gong, Shaoting Zhu, Zebang Shen, Luan Zhang, Lu Zhang, Hao Zhao, Hang Zhao</strong></p>
<p>Occupancy is crucial for autonomous driving, providing essential geometric priors for perception and planning. However, existing methods predominantly rely on LiDAR-based occupancy annotations, which limits scalability and prevents leveraging vast amounts of potential crowdsourced data for auto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only framework that directly reconstructs occupancy. Vision-only occupancy reconstruction poses significant challenges due to sparse viewpoints, dynamic scene elements, severe occlusions, and long-horizon motion. Existing vision-based methods primarily rely on mesh representation, which suffer from incomplete geometry and additional post-processing, limiting scalability. To overcome these issues, GS-Occ3D optimizes an explicit occupancy representation using an Octree-based Gaussian Surfel formulation, ensuring efficiency and scalability. Additionally, we decompose scenes into static background, ground, and dynamic objects, enabling tailored modeling strategies: (1) Ground is explicitly reconstructed as a dominant structural element, significantly improving large-area consistency; (2) Dynamic vehicles are separately modeled to better capture motion-related occupancy patterns. Extensive experiments on the Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry reconstruction results. By curating vision-only binary occupancy labels from diverse urban scenes, we show their effectiveness for downstream occupancy models on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes. It highlights the potential of large-scale vision-based occupancy reconstruction as a new paradigm for scalable auto-labeling. Project Page: <a target="_blank" rel="noopener" href="https://gs-occ3d.github.io/">https://gs-occ3d.github.io/</a> </p>
<blockquote>
<p>占用信息对于自动驾驶至关重要，它为感知和规划提供了必要的几何先验信息。然而，现有方法主要依赖于基于激光雷达的占用标注，这限制了可扩展性，并阻止了利用大量潜在的众包数据进行自动标注。为了解决这一问题，我们提出了GS-Occ3D，这是一个可扩展的仅视觉框架，可直接重建占用信息。仅使用视觉的占用信息重建由于稀疏的视点、动态场景元素、严重的遮挡和长距离运动而面临重大挑战。现有的基于视觉的方法主要依赖于网格表示，这导致了几何不完整性和额外的后处理，从而限制了可扩展性。为了克服这些问题，GS-Occ3D使用基于八叉树的高斯Surfel表示法优化明确的占用表示，确保效率和可扩展性。此外，我们将场景分解为静态背景、地面和动态物体，以实现定制建模策略：（1）地面被明确重建为一个主要结构元素，这显著提高了大面积的一致性；（2）动态车辆被单独建模，以更好地捕捉与运动相关的占用模式。在Waymo数据集上的广泛实验表明，GS-Occ3D实现了最先进的几何重建结果。我们通过从各种城市场景中整理仅使用视觉的二进制占用标签，展示了它们在Occ3D-Waymo上的下游占用模型的有效性，以及在Occ3D-nuScenes上的零样本泛化优势。它突出了大规模基于视觉的占用信息重建作为新的可扩展自动标注范式的潜力。项目页面：<a target="_blank" rel="noopener" href="https://gs-occ3d.github.io/">https://gs-occ3d.github.io/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19451v2">PDF</a> ICCV 2025. Project Page: <a target="_blank" rel="noopener" href="https://gs-occ3d.github.io/">https://gs-occ3d.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为GS-Occ3D的仅视觉的占有率重建框架，用于直接重建自主驾驶中的占有率。该框架采用基于Octree的Gaussian Surfel公式进行明确的占有率表示，确保效率和可扩展性。通过分解场景为静态背景、地面和动态物体，实现有针对性的建模策略，并在Waymo数据集上达到最先进的几何重建结果。此外，从多样的城市场景中提取仅视觉的二元占有率标签，展示其在下游占有率模型中的有效性，并强调大规模视觉基础占有率重建作为可伸缩自动标注的新范式潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GS-Occ3D是一个仅视觉的占有率重建框架，可直接重建自主驾驶中的占有率。</li>
<li>该框架采用基于Octree的Gaussian Surfel公式，优化明确的占有率表示，确保效率和可扩展性。</li>
<li>场景被分解为静态背景、地面和动态物体，以实现有针对性的建模策略。</li>
<li>在Waymo数据集上，GS-Occ3D实现了最先进的几何重建结果。</li>
<li>从多样的城市场景中提取仅视觉的二元占有率标签，这些标签对于下游占有率模型非常有效。</li>
<li>GS-Occ3D显示出大规模视觉基础占有率重建作为可伸缩自动标注新范式的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19451">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-87b3a67d38fd4a97e9b53de62040d0c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5337e1b374c7dc1047f328770ee538cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9874a664ee8860ec1f69c92d5e11e58a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MVG4D-Image-Matrix-Based-Multi-View-and-Motion-Generation-for-4D-Content-Creation-from-a-Single-Image"><a href="#MVG4D-Image-Matrix-Based-Multi-View-and-Motion-Generation-for-4D-Content-Creation-from-a-Single-Image" class="headerlink" title="MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D   Content Creation from a Single Image"></a>MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D   Content Creation from a Single Image</h2><p><strong>Authors:DongFu Yin, Xiaotian Chen, Fei Richard Yu, Xuanchen Li, Xinhao Zhang</strong></p>
<p>Advances in generative modeling have significantly enhanced digital content creation, extending from 2D images to complex 3D and 4D scenes. Despite substantial progress, producing high-fidelity and temporally consistent dynamic 4D content remains a challenge. In this paper, we propose MVG4D, a novel framework that generates dynamic 4D content from a single still image by combining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core, MVG4D employs an image matrix module that synthesizes temporally coherent and spatially diverse multi-view images, providing rich supervisory signals for downstream 3D and 4D reconstruction. These multi-view images are used to optimize a 3D Gaussian point cloud, which is further extended into the temporal domain via a lightweight deformation network. Our method effectively enhances temporal consistency, geometric fidelity, and visual realism, addressing key challenges in motion discontinuity and background degradation that affect prior 4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate that MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and time efficiency. Notably, it reduces flickering artifacts and sharpens structural details across views and time, enabling more immersive AR&#x2F;VR experiences. MVG4D sets a new direction for efficient and controllable 4D generation from minimal inputs. </p>
<blockquote>
<p>生成模型的进步极大推动了数字内容的创作，从二维图像扩展到复杂的三维甚至四维场景。尽管取得了很大进展，但生成高保真且时间一致的动态四维内容仍是一个挑战。本文提出了一种新型框架MVG4D，通过结合多视图合成与四维高斯斑点（4D GS），从单一静态图像生成动态四维内容。MVG4D的核心在于采用图像矩阵模块，合成时间连贯且空间多样的多视图图像，为下游的三维和四维重建提供丰富的监督信号。这些多视图图像用于优化三维高斯点云，通过轻量级变形网络进一步扩展到时间域。我们的方法有效地提高了时间一致性、几何保真度和视觉逼真度，解决了影响基于四维GS的方法的运动不连续和背景退化等关键挑战。在Objaverse数据集上的大量实验表明，MVG4D在CLIP-I、PSNR、FVD和时间效率方面优于最新基线。值得注意的是，它减少了闪烁伪影，提高了跨视图和时间的结构细节清晰度，为增强AR&#x2F;VR体验提供了更多沉浸式体验。MVG4D为从最小输入实现高效可控的四维生成设置了新的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18371v2">PDF</a> </p>
<p><strong>Summary</strong><br>     本文提出MVG4D框架，通过结合多视角合成与四维高斯点云技术，从单一静态图像生成动态四维内容。该框架采用图像矩阵模块，合成时空连贯且视角多样的图像，为下游三维和四维重建提供丰富的监督信号。通过优化三维高斯点云，并在时间域进行轻量化变形网络扩展，有效提升了时序一致性、几何精度和视觉真实感。在Objaverse数据集上的实验表明，MVG4D在CLIP-I、PSNR、FVD和时间效率等方面优于现有基线，减少了闪烁伪影，提高了结构细节清晰度，为AR&#x2F;VR体验提供更沉浸式体验。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MVG4D框架结合了多视角合成与四维高斯点云技术，能够从单一静态图像生成动态四维内容。</li>
<li>图像矩阵模块用于合成时空连贯且视角多样的图像，为下游三维和四维重建提供丰富的监督信号。</li>
<li>通过优化三维高斯点云，并在时间域进行轻量化变形网络扩展，提升了时序一致性、几何精度和视觉真实感。</li>
<li>MVG4D解决了基于四维高斯点云方法存在的运动不连续和背景退化等关键挑战。</li>
<li>在Objaverse数据集上的实验表明，MVG4D在多个指标上优于现有方法，包括CLIP-I、PSNR、FVD等。</li>
<li>MVG4D减少了生成内容中的闪烁伪影，提高了结构细节的清晰度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18371">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-419620538e551ccfda567b7a926b9c83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9705e45b591bb75895c8e2aa8cb98687.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9427d8233b1b668390f94e35ebf34fd6.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GeoAvatar-Adaptive-Geometrical-Gaussian-Splatting-for-3D-Head-Avatar"><a href="#GeoAvatar-Adaptive-Geometrical-Gaussian-Splatting-for-3D-Head-Avatar" class="headerlink" title="GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar"></a>GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar</h2><p><strong>Authors:SeungJun Moon, Hah Min Lew, Seungeun Lee, Ji-Su Kang, Gyeong-Moon Park</strong></p>
<p>Despite recent progress in 3D head avatar generation, balancing identity preservation, i.e., reconstruction, with novel poses and expressions, i.e., animation, remains a challenge. Existing methods struggle to adapt Gaussians to varying geometrical deviations across facial regions, resulting in suboptimal quality. To address this, we propose GeoAvatar, a framework for adaptive geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation Stage (APS), an unsupervised method that segments Gaussians into rigid and flexible sets for adaptive offset regularization. Then, based on mouth anatomy and dynamics, we introduce a novel mouth structure and the part-wise deformation strategy to enhance the animation fidelity of the mouth. Finally, we propose a regularization loss for precise rigging between Gaussians and 3DMM faces. Moreover, we release DynamicFace, a video dataset with highly expressive facial motions. Extensive experiments show the superiority of GeoAvatar compared to state-of-the-art methods in reconstruction and novel animation scenarios. </p>
<blockquote>
<p>尽管最近在3D头像生成方面取得了进展，但在平衡身份保留（即重建）与新颖姿势和表情（即动画）之间仍然存在挑战。现有方法难以适应面部各区域不同的几何偏差，导致质量不佳。为了解决这一问题，我们提出了GeoAvatar，一个自适应几何高斯拼贴框架。GeoAvatar利用自适应预分配阶段（APS），这是一种无监督方法，将高斯分割成刚性和柔性集合，用于自适应偏移正则化。然后，基于嘴巴结构和动态特性，我们引入了一种新的嘴巴结构和部分变形策略，以提高嘴巴的动画保真度。最后，我们提出了一种针对高斯和3DMM面孔之间的精确骨架的正则化损失。此外，我们发布了DynamicFace，这是一个具有高度表达性面部动作的视频数据集。大量实验表明，GeoAvatar在重建和新颖动画场景中相较于最先进的方法具有优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18155v1">PDF</a> ICCV 2025, Project page: <a target="_blank" rel="noopener" href="https://hahminlew.github.io/geoavatar/">https://hahminlew.github.io/geoavatar/</a></p>
<p><strong>Summary</strong></p>
<p>该文探讨了在3D头像生成领域中的一项挑战，即如何在保持身份不变的同时实现新的姿态和表情。为解决现有方法在面部几何形变上的不足，提出GeoAvatar框架，利用自适应预分配阶段和基于嘴巴解剖结构和动态特性的创新技术，提升头像质量。此外，还引入了一种新的动态面部数据集DynamicFace。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D头像生成中保持身份、重建和动画平衡仍是挑战。</li>
<li>现有方法难以适应面部几何形变，导致质量不佳。</li>
<li>GeoAvatar框架通过自适应几何高斯点分布解决此问题。</li>
<li>利用自适应预分配阶段（APS）进行高斯分割，实现自适应偏移正则化。</li>
<li>基于嘴巴解剖结构和动态特性引入新型嘴巴结构和部分变形策略，提升动画逼真度。</li>
<li>引入正则化损失，精确调整高斯和3DMM面部之间的关联。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18155">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-f17f65e62b20cd25d392adba8e0daebc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ba7e89bc4b3b9648de01db46af3d015.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4532f8b7796c6218f83f1ee0ae03a99d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de3c27123c0082f233ed6945ac6023d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4780f90f36cb275c32235c1c57d6585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7f942e51a4d8e6bf812dce3334525b1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="StreamME-Simplify-3D-Gaussian-Avatar-within-Live-Stream"><a href="#StreamME-Simplify-3D-Gaussian-Avatar-within-Live-Stream" class="headerlink" title="StreamME: Simplify 3D Gaussian Avatar within Live Stream"></a>StreamME: Simplify 3D Gaussian Avatar within Live Stream</h2><p><strong>Authors:Luchuan Song, Yang Zhou, Zhan Xu, Yi Zhou, Deepali Aneja, Chenliang Xu</strong></p>
<p>We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: <a target="_blank" rel="noopener" href="https://songluchuan.github.io/StreamME/">https://songluchuan.github.io/StreamME/</a>. </p>
<blockquote>
<p>我们提出了StreamME方法，它专注于快速3D化身重建。StreamME同步记录并从实时视频流中重建头部化身，无需任何预先缓存的数据，使得重建的外观能够无缝地集成到下游应用中。我们称之为即时训练的超快训练策略是我们方法的核心。我们的方法基于3D高斯展开技术（3DGS），摒弃了可变形3DGS中对多层感知机（MLPs）的依赖，只依赖几何结构，这极大地提高了对面部表情的适应速度。为了确保即时训练的高效率，我们引入了一种基于主要点的简化策略，该策略在面部表面更稀疏地分布点云，在保持渲染质量的同时优化了点的数量。借助即时训练功能，我们的方法保护了面部的隐私并降低了VR系统或在线会议中的通信带宽。此外，它可以直接应用于动画、卡通化和重新照明等下游应用。想了解更多详情，请访问我们的项目页面：<a target="_blank" rel="noopener" href="https://songluchuan.github.io/StreamME/%E3%80%82">https://songluchuan.github.io/StreamME/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17029v1">PDF</a> 12 pages, 15 Figures</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为StreamME的快速3D头像重建方法，它同步记录并从实时视频流中重建头像，无需预先缓存数据。该方法以实时训练为核心，建立于3D高斯展开技术之上，简化了点云分布，提高渲染效率，保护面部隐私并减少虚拟实境系统或在线会议中的通信带宽需求。可直接应用于动画、卡通渲染和重光照等下游应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StreamME是一种快速3D头像重建方法，可从实时视频流中同步记录和重建头像。</li>
<li>该方法采用实时训练策略，摒弃了对多层感知机的依赖，仅依赖几何信息，提高了对面部表情的适应速度。</li>
<li>StreamME基于主要点简化策略，优化了点云分布，在保持渲染质量的同时减少了点数。</li>
<li>该方法能提高渲染效率，保护面部隐私，降低虚拟实境系统或在线会议的通信带宽需求。</li>
<li>StreamME可直接应用于动画、卡通渲染和重光照等下游应用。</li>
<li>文章提到的3D高斯展开技术是该方法的重要基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17029">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-02629449751a7b8e7335e258911c30d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-facb3a2875aa1ff49609f4ed51701e2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39db404d7510185eb41364a776ca8189.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a17537a8404bc1a4a156373cfa1392d0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DCHM-Depth-Consistent-Human-Modeling-for-Multiview-Detection"><a href="#DCHM-Depth-Consistent-Human-Modeling-for-Multiview-Detection" class="headerlink" title="DCHM: Depth-Consistent Human Modeling for Multiview Detection"></a>DCHM: Depth-Consistent Human Modeling for Multiview Detection</h2><p><strong>Authors:Jiahao Ma, Tianyu Wang, Miaomiao Liu, David Ahmedt-Aristizabal, Chuong Nguyen</strong></p>
<p>Multiview pedestrian detection typically involves two stages: human modeling and pedestrian localization. Human modeling represents pedestrians in 3D space by fusing multiview information, making its quality crucial for detection accuracy. However, existing methods often introduce noise and have low precision. While some approaches reduce noise by fitting on costly multiview 3D annotations, they often struggle to generalize across diverse scenes. To eliminate reliance on human-labeled annotations and accurately model humans, we propose Depth-Consistent Human Modeling (DCHM), a framework designed for consistent depth estimation and multiview fusion in global coordinates. Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting achieves multiview depth consistency in sparse-view, large-scaled, and crowded scenarios, producing precise point clouds for pedestrian localization. Extensive validations demonstrate that our method significantly reduces noise during human modeling, outperforming previous state-of-the-art baselines. Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians and perform multiview segmentation in such a challenging setting. Code is available on the \href{<a target="_blank" rel="noopener" href="https://jiahao-ma.github.io/DCHM/%7D%7Bproject">https://jiahao-ma.github.io/DCHM/}{project</a> page}. </p>
<blockquote>
<p>多视角行人检测通常涉及两个阶段：人体建模和行人定位。人体建模通过融合多视角信息来在三维空间中表示行人，因此其质量对检测精度至关重要。然而，现有方法往往会引入噪声且精度较低。虽然一些方法通过适应成本高昂的多视角三维标注来减少噪声，但它们往往在跨不同场景时推广困难。为了消除对人类标注的依赖并实现精准的人体建模，我们提出了深度一致人体建模（DCHM），这是一个用于全局坐标系中的一致深度估计和多视角融合的框架。具体来说，我们提出的具有超像素级高斯平铺的管道在稀疏视角、大规模和拥挤的场景中实现了多视角深度一致性，为行人定位生成精确的点云。广泛的验证表明，我们的方法在人建建模过程中显著降低了噪声，优于最新的基线。此外，据我们所知，DCHM是在如此具有挑战性的环境中重建行人和进行多视角分割的第一种方法。代码可在项目页面找到：[<a target="_blank" rel="noopener" href="https://jiahao-ma.github.io/DCHM/]">https://jiahao-ma.github.io/DCHM/]</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14505v1">PDF</a> multi-view detection, sparse-view reconstruction</p>
<p><strong>Summary</strong><br>三维多视角行人检测包括人体建模和行人定位两个阶段。现有的人体建模方法融合多视角信息表示行人，但引入噪声并降低精度。我们提出Depth-Consistent Human Modeling（DCHM）框架，旨在实现深度一致性多视角融合。通过超像素级的高斯延展技术，在稀疏视角、大规模和拥挤场景中实现深度一致性，为行人定位生成精确的点云。验证表明，我们的方法显著降低了人体建模中的噪声，优于现有技术。DCHM是首个在此类复杂环境中重建行人和进行多视角分割的方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Multiview pedestrian detection涉及人体建模和行人定位两个阶段。</li>
<li>现有方法融合多视角信息表示行人，但存在引入噪声和精度低的问题。</li>
<li>Depth-Consistent Human Modeling（DCHM）框架旨在实现深度一致性多视角融合。</li>
<li>DCHM通过超像素级的高斯延展技术实现深度一致性。</li>
<li>DCHM在稀疏视角、大规模和拥挤场景中表现优越。</li>
<li>DCHM生成精确的点云，有助于行人定位。</li>
<li>与现有技术相比，DCHM在人体建模中显著降低噪声。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14505">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4b29812ea7e62b748445886392140771.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-939756d4e40f9a5bb1e69d8b7cb18d63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bd3b491e98a0e74de66309568cd3679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37b42a900efc069fa6431b75c5963107.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efb1fa5b0b6cdb638ab7019466a58bb9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e1f3e9ce2af567fe471dbad8dea5053.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Advances-in-Feed-Forward-3D-Reconstruction-and-View-Synthesis-A-Survey"><a href="#Advances-in-Feed-Forward-3D-Reconstruction-and-View-Synthesis-A-Survey" class="headerlink" title="Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey"></a>Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey</h2><p><strong>Authors:Jiahui Zhang, Yuelei Li, Anpei Chen, Muyu Xu, Kunhao Liu, Jianyuan Wang, Xiao-Xiao Long, Hanxue Liang, Zexiang Xu, Hao Su, Christian Theobalt, Christian Rupprecht, Andrea Vedaldi, Hanspeter Pfister, Shijian Lu, Fangneng Zhan</strong></p>
<p>3D reconstruction and view synthesis are foundational problems in computer vision, graphics, and immersive technologies such as augmented reality (AR), virtual reality (VR), and digital twins. Traditional methods rely on computationally intensive iterative optimization in a complex chain, limiting their applicability in real-world scenarios. Recent advances in feed-forward approaches, driven by deep learning, have revolutionized this field by enabling fast and generalizable 3D reconstruction and view synthesis. This survey offers a comprehensive review of feed-forward techniques for 3D reconstruction and view synthesis, with a taxonomy according to the underlying representation architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural Radiance Fields (NeRF), etc. We examine key tasks such as pose-free reconstruction, dynamic 3D reconstruction, and 3D-aware image and video synthesis, highlighting their applications in digital humans, SLAM, robotics, and beyond. In addition, we review commonly used datasets with detailed statistics, along with evaluation protocols for various downstream tasks. We conclude by discussing open research challenges and promising directions for future work, emphasizing the potential of feed-forward approaches to advance the state of the art in 3D vision. </p>
<blockquote>
<p>3D重建和视图合成是计算机视觉、图形学和沉浸式技术（如增强现实(AR)、虚拟现实(VR)和数字孪生）中的基础问题。传统方法依赖于复杂链中的计算密集型迭代优化，这在现实场景的应用中存在一定的局限性。最近，深度学习驱动的前馈方法的进步已经彻底改变了这一领域，实现了快速和通用的3D重建和视图合成。这篇综述全面介绍了前馈技术在3D重建和视图合成方面的应用，并根据底层表示架构进行了分类，包括点云、3D高斯溅射（3DGS）、神经辐射场（NeRF）等。我们研究了关键任务，如姿态自由重建、动态3D重建和3D感知图像和视频合成，突出了它们在数字人类、SLAM、机器人技术等领域的应用以及其他更广泛的应用。此外，我们还回顾了常用的数据集及其详细统计数据，以及各种下游任务的评估协议。最后，我们讨论了当前的研究挑战以及未来工作的有前途的方向，强调了前馈方法在推动3D视觉技术前沿的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14501v2">PDF</a> A project page associated with this survey is available at   <a target="_blank" rel="noopener" href="https://fnzhan.com/projects/Feed-Forward-3D">https://fnzhan.com/projects/Feed-Forward-3D</a></p>
<p><strong>Summary</strong></p>
<p>本文综述了基于深度学习的Feed-forward技术在三维重建和视图合成方面的应用，涵盖了各种底层架构，如点云、三维高斯渲染技术和神经网络辐射场等。本文详细讨论了三维重建中的关键任务，包括无姿态重建、动态三维重建和三维图像和视频合成等，并强调了这些技术在数字人、即时定位与地图构建（SLAM）、机器人等领域的应用。此外，本文还介绍了常用的数据集及其详细统计信息，以及各种下游任务的评估协议。最后，本文探讨了开放的研究挑战和未来研究的有前途的方向，强调了Feed-forward技术在推动三维视觉技术前沿的潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Feed-forward技术推动了三维重建和视图合成的革命性发展，使快速且通用的三维重建成为可能。</li>
<li>点云、三维高斯渲染技术和神经网络辐射场等是主要的底层架构。</li>
<li>无姿态重建、动态三维重建和三维图像和视频合成是三维重建中的关键任务。</li>
<li>这些技术在数字人、即时定位与地图构建（SLAM）、机器人等领域有广泛应用。</li>
<li>常用的数据集及其详细统计信息对于研究和应用至关重要。</li>
<li>评估协议对于各种下游任务非常重要，有助于衡量技术性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14501">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-49a484bb983abad5bc0f6fb7f24c2ea8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af67e8f5ffb0c8ebb2831103199b100a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bfa97f29caebec405ae1e8e652e7e2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-950245207a32bfbb23e9d14eb54d7f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e8f9d80f8c8e9a5d566a28ba69100a3.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DreamScene-3D-Gaussian-based-End-to-end-Text-to-3D-Scene-Generation"><a href="#DreamScene-3D-Gaussian-based-End-to-end-Text-to-3D-Scene-Generation" class="headerlink" title="DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation"></a>DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation</h2><p><strong>Authors:Haoran Li, Yuli Tian, Kun Lan, Yong Liao, Lin Wang, Pan Hui, Peng Yuan Zhou</strong></p>
<p>Generating 3D scenes from natural language holds great promise for applications in gaming, film, and design. However, existing methods struggle with automation, 3D consistency, and fine-grained control. We present DreamScene, an end-to-end framework for high-quality and editable 3D scene generation from text or dialogue. DreamScene begins with a scene planning module, where a GPT-4 agent infers object semantics and spatial constraints to construct a hybrid graph. A graph-based placement algorithm then produces a structured, collision-free layout. Based on this layout, Formation Pattern Sampling (FPS) generates object geometry using multi-timestep sampling and reconstructive optimization, enabling fast and realistic synthesis. To ensure global consistent, DreamScene employs a progressive camera sampling strategy tailored to both indoor and outdoor settings. Finally, the system supports fine-grained scene editing, including object movement, appearance changes, and 4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior methods in quality, consistency, and flexibility, offering a practical solution for open-domain 3D content creation. Code and demos are available at <a target="_blank" rel="noopener" href="https://jahnsonblack.github.io/DreamScene-Full/">https://jahnsonblack.github.io/DreamScene-Full/</a>. </p>
<blockquote>
<p>从自然语言生成3D场景在游戏、电影和设计等领域的应用具有巨大潜力。然而，现有方法在自动化、3D一致性和精细控制方面存在挑战。我们提出了DreamScene，一个从文本或对话中进行高质量和可编辑的3D场景生成的端到端框架。DreamScene从场景规划模块开始，GPT-4智能体会推断对象语义和空间约束来构建混合图。基于图的放置算法然后产生结构化的、无碰撞的布局。基于该布局，Formation Pattern Sampling（FPS）使用多时步采样和重建优化生成对象几何，实现快速和逼真的合成。为确保全局一致性，DreamScene采用针对室内和室外环境的渐进式相机采样策略。最后，该系统支持精细的场景编辑，包括对象移动、外观变化和4D动态运动。实验表明，DreamScene在质量、一致性和灵活性方面超越了之前的方法，为开放域3D内容创建提供了实用解决方案。代码和演示可在<a target="_blank" rel="noopener" href="https://jahnsonblack.github.io/DreamScene-Full/%E6%89%BE%E5%88%B0%E3%80%82">https://jahnsonblack.github.io/DreamScene-Full/找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13985v2">PDF</a> Extended version of ECCV 2024 paper “DreamScene”</p>
<p><strong>Summary</strong><br>文本描述了一个名为DreamScene的端到端框架，它能从文本或对话中生成高质量且可编辑的3D场景。该框架通过场景规划模块开始，使用GPT-4代理推断对象语义和空间约束来构建混合图。基于图的放置算法生成结构化的无碰撞布局，然后通过Formation Pattern Sampling生成对象几何结构。为确保全局一致性，DreamScene采用了面向室内和室外环境的渐进式摄像机采样策略。该系统支持精细的场景编辑，如对象移动、外观变化和四维动态运动。实验表明，DreamScene在质量、一致性和灵活性方面超越了以前的方法，为开放领域的3D内容创建提供了切实可行的解决方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DreamScene是一个用于从文本生成高质量且可编辑的3D场景的端到端框架。</li>
<li>它使用GPT-4代理进行场景规划，通过推断对象语义和空间约束来构建混合图。</li>
<li>基于图的放置算法生成无碰撞的布局结构。</li>
<li>Formation Pattern Sampling用于生成对象几何结构，确保快速且逼真的合成。</li>
<li>DreamScene采用渐进式摄像机采样策略，确保全局一致性，适用于室内和室外环境。</li>
<li>该系统支持精细的场景编辑功能，如对象移动、外观变化和动态运动。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13985">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-375f39cf35861cff679adf7329566123.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33d15e421b3e52be80c28d1faecef16c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76721972be8bbfc93e3d4391ac6cd3ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36c5713a94daef7d7027b4c84354bdb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f4658cb0438977f5373dc24447aaf8f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="VisualSpeaker-Visually-Guided-3D-Avatar-Lip-Synthesis"><a href="#VisualSpeaker-Visually-Guided-3D-Avatar-Lip-Synthesis" class="headerlink" title="VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis"></a>VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis</h2><p><strong>Authors:Alexandre Symeonidis-Herzig, Özge Mercanoğlu Sincan, Richard Bowden</strong></p>
<p>Realistic, high-fidelity 3D facial animations are crucial for expressive avatar systems in human-computer interaction and accessibility. Although prior methods show promising quality, their reliance on the mesh domain limits their ability to fully leverage the rapid visual innovations seen in 2D computer vision and graphics. We propose VisualSpeaker, a novel method that bridges this gap using photorealistic differentiable rendering, supervised by visual speech recognition, for improved 3D facial animation. Our contribution is a perceptual lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting avatar renders through a pre-trained Visual Automatic Speech Recognition model during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker improves both the standard Lip Vertex Error metric by 56.1% and the perceptual quality of the generated animations, while retaining the controllability of mesh-driven animation. This perceptual focus naturally supports accurate mouthings, essential cues that disambiguate similar manual signs in sign language avatars. </p>
<blockquote>
<p>真实、高保真度的三维面部动画对于人机交互和可访问性中的表情符号系统至关重要。尽管先前的方法显示出有希望的品质，但它们对网格域的依赖限制了它们充分利用二维计算机视觉和图形中快速视觉创新的能力。我们提出了VisualSpeaker，这是一种新方法，通过真实可渲染的技术缩小这一差距，由视觉语音识别进行监督，以提高三维面部动画的质量。我们的贡献在于感知唇读损失，它是通过在训练过程中将通过预训练的视觉自动语音识别模型的真实三维高斯变形人脸渲染传递，从而派生出来的。在MEAD数据集上的评估表明，VisualSpeaker不仅将标准唇顶点误差度量提高了56.1%，而且提高了生成动画的感知质量，同时还保持了网格驱动动画的可控性。这种感知重点自然支持准确的嘴部动作，这是区分类似手动符号的手语表情符号的重要线索。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06060v2">PDF</a> Accepted in International Conference on Computer Vision (ICCV)   Workshops</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为VisualSpeaker的新型方法，通过采用逼真的可微分渲染技术，结合视觉语音识别的监督，改进了3D面部动画。该方法使用感知唇读损失，通过在训练期间将通过逼真渲染的3D高斯喷溅人像呈现给预训练的视觉自动语音识别模型来得到。在MEAD数据集上的评估显示，VisualSpeaker改进了唇顶点误差指标，提高了动画生成的质量，同时保留了网格驱动的动画的可控性。这种方法为真实、高质量的3D面部动画提供了新的可能性，对人机交互和访问控制中的表情符号系统尤为重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VisualSpeaker是一种改进3D面部动画的新型方法，结合了逼真的可微分渲染和视觉语音识别的监督。</li>
<li>该方法通过使用感知唇读损失来提高动画质量，该损失是通过将逼真的3D面部渲染呈现给预训练的视觉自动语音识别模型来计算的。</li>
<li>VisualSpeaker在MEAD数据集上的表现优于传统方法，唇顶点误差指标改进了56.1%。</li>
<li>该方法提高了动画的感知质量，同时保留了网格驱动动画的可控性。</li>
<li>VisualSpeaker对于人机交互和访问控制中的表情符号系统尤为重要。</li>
<li>该方法能够支持准确的嘴部动作，这对于区分手动符号语言中的相似手动标志至关重要。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06060">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2de8e4f5fb567b48dced7da6064d8de6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de8752550d8ec2f96109383cbfaa8632.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b2d05bafcd46d8dd7e08954747a41d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0eb7b5361dc63950890a9f289619896.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3354b9aacd688e1c39bcbec517277f7e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FOCI-Trajectory-Optimization-on-Gaussian-Splats"><a href="#FOCI-Trajectory-Optimization-on-Gaussian-Splats" class="headerlink" title="FOCI: Trajectory Optimization on Gaussian Splats"></a>FOCI: Trajectory Optimization on Gaussian Splats</h2><p><strong>Authors:Mario Gomez Andreu, Maximum Wilder-Smith, Victor Klemm, Vaishakh Patil, Jesus Tordesillas, Marco Hutter</strong></p>
<p>3D Gaussian Splatting (3DGS) has recently gained popularity as a faster alternative to Neural Radiance Fields (NeRFs) in 3D reconstruction and view synthesis methods. Leveraging the spatial information encoded in 3DGS, this work proposes FOCI (Field Overlap Collision Integral), an algorithm that is able to optimize trajectories directly on the Gaussians themselves. FOCI leverages a novel and interpretable collision formulation for 3DGS using the notion of the overlap integral between Gaussians. Contrary to other approaches, which represent the robot with conservative bounding boxes that underestimate the traversability of the environment, we propose to represent the environment and the robot as Gaussian Splats. This not only has desirable computational properties, but also allows for orientation-aware planning, allowing the robot to pass through very tight and narrow spaces. We extensively test our algorithm in both synthetic and real Gaussian Splats, showcasing that collision-free trajectories for the ANYmal legged robot that can be computed in a few seconds, even with hundreds of thousands of Gaussians making up the environment. The project page and code are available at <a target="_blank" rel="noopener" href="https://rffr.leggedrobotics.com/works/foci/">https://rffr.leggedrobotics.com/works/foci/</a> </p>
<blockquote>
<p>三维高斯散斑法（3DGS）最近成为了在三维重建和视角合成方法中，神经网络辐射场（NeRFs）的一种更快替代方法而受到欢迎。利用三维高斯散斑法中的空间信息编码，这项工作提出了FOCI（高斯域碰撞积分），这是一种能在高斯函数本身上直接优化轨迹的算法。FOCI利用一个新颖的可解释的碰撞公式来描述三维高斯散斑法，采用高斯之间的重叠积分概念。与其他使用保守边界框表示机器人，从而低估环境通行性的方法不同，我们提出将环境和机器人表示为高斯散斑。这不仅具有理想的计算特性，还允许具有方向感知的规划，使机器人能够穿过非常狭窄的空间。我们在合成和真实的高斯散斑上都测试了我们的算法，展示了即使在由数十万高斯组成的环境中，也可以在几秒内计算出适合人马机器人且无碰撞的轨迹。项目页面和代码可以在 <a target="_blank" rel="noopener" href="https://rffr.leggedrobotics.com/works/foci/">https://rffr.leggedrobotics.com/works/foci/</a> 找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08510v2">PDF</a> 8 pages, 8 figures, Mario Gomez Andreu and Maximum Wilder-Smith   contributed equally</p>
<p><strong>Summary</strong></p>
<p>3D高斯混合（3DGS）在三维重建和视图合成方法中作为神经网络辐射场（NeRFs）的更快替代方案而受到关注。本研究提出FOCI（基于场重叠碰撞积分）算法，该算法能够在高斯分布本身上直接优化轨迹。FOCI利用高斯之间重叠积分的概念，采用一种新颖且可解释性强的碰撞公式用于优化机器人与环境之间的交互。与其他方法不同，FOCI将机器人和环境表示为高斯混合，这不仅具有理想的计算属性，还允许进行方向感知规划，使机器人能够穿过非常紧凑和狭窄的空间。我们的算法在合成和真实的高斯混合中都经过了广泛测试，展示了即使在由数十万高斯组成的环境中，也能在几秒钟内计算出ANYmal步行机器人的无碰撞轨迹。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGS作为NeRFs的替代方法，在三维重建和视图合成中受到关注。</li>
<li>FOCI算法利用高斯之间的空间信息优化轨迹。</li>
<li>FOCI采用新颖的碰撞公式，基于高斯之间的重叠积分。</li>
<li>与其他方法不同，FOCI将机器人和环境表示为高斯混合，提高了环境遍历能力的估计。</li>
<li>高斯混合表示允许方向感知规划，适用于狭窄空间。</li>
<li>在合成和真实环境中广泛测试了FOCI算法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08510">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2cd80fb72740fbfbe260b17f41ea682c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da25f21a2f3b282eac96ea984475e9cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-722fd984a1aa4b3dd0a53cd3620d55bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97370c4728c76c97f079a6d7f97112e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-111b00395c3add5361c6281356bef41d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64682faa2230a9f038a40d4b24a1ec53.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Sparfels-Fast-Reconstruction-from-Sparse-Unposed-Imagery"><a href="#Sparfels-Fast-Reconstruction-from-Sparse-Unposed-Imagery" class="headerlink" title="Sparfels: Fast Reconstruction from Sparse Unposed Imagery"></a>Sparfels: Fast Reconstruction from Sparse Unposed Imagery</h2><p><strong>Authors:Shubhendu Jena, Amine Ouasfi, Mae Younes, Adnane Boukhayma</strong></p>
<p>We present a method for Sparse view reconstruction with surface element splatting that runs within 3 minutes on a consumer grade GPU. While few methods address sparse radiance field learning from noisy or unposed sparse cameras, shape recovery remains relatively underexplored in this setting. Several radiance and shape learning test-time optimization methods address the sparse posed setting by learning data priors or using combinations of external monocular geometry priors. Differently, we propose an efficient and simple pipeline harnessing a single recent 3D foundation model. We leverage its various task heads, notably point maps and camera initializations to instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image correspondences to guide camera optimization midst 2DGS training. Key to our contribution is a novel formulation of splatted color variance along rays, which can be computed efficiently. Reducing this moment in training leads to more accurate shape reconstructions. We demonstrate state-of-the-art performances in the sparse uncalibrated setting in reconstruction and novel view benchmarks based on established multi-view datasets. </p>
<blockquote>
<p>我们提出了一种利用表面元素喷绘进行稀疏视角重建的方法，该方法在消费级GPU上运行时间不超过3分钟。尽管已有少数方法解决了从带噪声或无姿态的稀疏相机学习稀疏辐射场的问题，但在此环境中，形状恢复仍然相对研究不足。几种辐射和形状学习的测试时间优化方法通过数据先验或结合外部单眼几何先验来解决稀疏设定的姿态问题。与之不同，我们提出了一种高效且简单的流程，利用最新的单一3D基础模型。我们利用其各种任务头，特别是点图和相机初始化来实例化调整束的二维高斯喷绘（2DGS）模型，并利用图像对应关系来指导在二维GS训练过程中的相机优化。我们贡献的关键在于沿光线喷绘颜色方差的新公式，该公式可以高效计算。在训练中减少这一点会导致更准确的形状重建。我们在稀疏未校准环境中展示了最先进的重建和基于多视角数据集的新视角基准测试性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02178v4">PDF</a> ICCV 2025. Project page :   <a target="_blank" rel="noopener" href="https://shubhendu-jena.github.io/Sparfels-web/">https://shubhendu-jena.github.io/Sparfels-web/</a></p>
<p><strong>摘要</strong></p>
<p>本文提出了一种基于表面元素拼贴技术的稀疏视角重建方法，该方法在消费级GPU上运行时间不超过3分钟。尽管已有一些方法解决了稀疏辐射场学习的问题，但在噪声或无固定姿势的稀疏摄像机条件下，形状恢复仍是相对未被充分探索的领域。本文通过高效简洁的管道流程提出解决方案，该流程依赖于单个最新的三维基础模型。我们利用它的多个任务头部，特别是点图和相机初始化来建立束调整二维高斯拼贴模型，并利用图像对应关系来指导相机优化与二维GS训练。本文的关键贡献是沿射线拼贴颜色方差的新公式，该公式可以高效计算。在训练中减少这一时刻会导致更准确的形状重建。我们在稀疏未校准设置中重建和新颖视图基准上展示了最先进的性能表现，这些基准基于公认的多视角数据集。</p>
<p><strong>要点摘要</strong></p>
<p>一、提出了一种基于表面元素拼贴技术的稀疏视角重建方法，运行时间短。<br>二、在消费级GPU上实现。<br>三、在噪声或无固定姿势的稀疏摄像机条件下，解决了形状恢复的难题。<br>四、提出了一个高效且简洁的管道流程解决方案，依赖单个最新的三维基础模型。<br>五、利用点图和相机初始化等任务头部建立束调整二维高斯拼贴模型。<br>六、通过图像对应关系指导相机优化与二维GS训练。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02178">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-be77ff73478c99374883cd78b9f50e91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf25476d8c5619f994d28881ef64681c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a707278172627a1fb74721ec2f325f76.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-25fa6963933556854a815a540ef9bf76.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF 方向最新论文已更新，请持续关注 Update in 2025-08-02  NeRF Is a Valuable Assistant for 3D Gaussian Splatting
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ffab9676fc57bdedb660e09e9f610722.jpg" class="responsive-img" alt="元宇宙/虚拟人">
                        
                        <span class="card-title">元宇宙/虚拟人</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            元宇宙/虚拟人 方向最新论文已更新，请持续关注 Update in 2025-08-02  MoGA 3D Generative Avatar Prior for Monocular Gaussian Avatar   Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    元宇宙/虚拟人
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">元宇宙/虚拟人</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
