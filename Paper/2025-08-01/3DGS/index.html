<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="3DGS">
    <meta name="description" content="3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  Gaussian Variation Field Diffusion for High-fidelity Video-to-4D   Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>3DGS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c20ea647f542e878705d061131ccfb2a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">3DGS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/3DGS/">
                                <span class="chip bg-color">3DGS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                3DGS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-02-æ›´æ–°"><a href="#2025-08-02-æ›´æ–°" class="headerlink" title="2025-08-02 æ›´æ–°"></a>2025-08-02 æ›´æ–°</h1><h2 id="Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis"><a href="#Gaussian-Variation-Field-Diffusion-for-High-fidelity-Video-to-4D-Synthesis" class="headerlink" title="Gaussian Variation Field Diffusion for High-fidelity Video-to-4D   Synthesis"></a>Gaussian Variation Field Diffusion for High-fidelity Video-to-4D   Synthesis</h2><p><strong>Authors:Bowen Zhang, Sicheng Xu, Chuxin Wang, Jiaolong Yang, Feng Zhao, Dong Chen, Baining Guo</strong></p>
<p>In this paper, we present a novel framework for video-to-4D generation that creates high-quality dynamic 3D content from single video inputs. Direct 4D diffusion modeling is extremely challenging due to costly data construction and the high-dimensional nature of jointly representing 3D shape, appearance, and motion. We address these challenges by introducing a Direct 4DMesh-to-GS Variation Field VAE that directly encodes canonical Gaussian Splats (GS) and their temporal variations from 3D animation data without per-instance fitting, and compresses high-dimensional animations into a compact latent space. Building upon this efficient representation, we train a Gaussian Variation Field diffusion model with temporal-aware Diffusion Transformer conditioned on input videos and canonical GS. Trained on carefully-curated animatable 3D objects from the Objaverse dataset, our model demonstrates superior generation quality compared to existing methods. It also exhibits remarkable generalization to in-the-wild video inputs despite being trained exclusively on synthetic data, paving the way for generating high-quality animated 3D content. Project page: <a target="_blank" rel="noopener" href="https://gvfdiffusion.github.io/">https://gvfdiffusion.github.io/</a>. </p>
<blockquote>
<p>åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç”¨äºè§†é¢‘åˆ°4Dç”Ÿæˆçš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»å•ä¸ªè§†é¢‘è¾“å…¥ä¸­åˆ›å»ºé«˜è´¨é‡åŠ¨æ€3Då†…å®¹ã€‚ç›´æ¥4Dæ‰©æ•£å»ºæ¨¡ç”±äºæ•°æ®æ„å»ºæˆæœ¬é«˜æ˜‚ä»¥åŠåŒæ—¶è¡¨ç¤º3Då½¢çŠ¶ã€å¤–è§‚å’Œè¿åŠ¨çš„ç»´æ•°è¿‡é«˜è€Œæå…·æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥Direct 4DMesh-to-GSå˜åŒ–åœºVAEæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¯¥VAEèƒ½å¤Ÿç›´æ¥ä»3DåŠ¨ç”»æ•°æ®å¯¹è§„èŒƒé«˜æ–¯Splatsï¼ˆGSï¼‰åŠå…¶æ—¶é—´å˜åŒ–è¿›è¡Œç¼–ç ï¼Œè€Œæ— éœ€é€ä¸ªå®ä¾‹è¿›è¡Œæ‹Ÿåˆï¼Œå¹¶å°†é«˜ç»´åŠ¨ç”»å‹ç¼©åˆ°ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ä¸­ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªåŸºäºæ—¶é—´æ„ŸçŸ¥æ‰©æ•£Transformerçš„é«˜æ–¯å˜åŒ–åœºæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ ¹æ®è¾“å…¥è§†é¢‘å’Œè§„èŒƒGSè¿›è¡Œè®­ç»ƒã€‚è¯¥æ¨¡å‹åœ¨ç²¾å¿ƒæŒ‘é€‰çš„æ¥è‡ªObjaverseæ•°æ®é›†çš„åŠ¨ç”»3Då¯¹è±¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå…¶ç”Ÿæˆè´¨é‡æ›´èƒœä¸€ç­¹ã€‚å°½ç®¡è¯¥æ¨¡å‹ä»…åœ¨åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†å®ƒå¯¹å¤©ç„¶è§†é¢‘è¾“å…¥çš„æ³›åŒ–èƒ½åŠ›ä»ç„¶ååˆ†æ˜¾è‘—ï¼Œä¸ºç”Ÿæˆé«˜è´¨é‡åŠ¨ç”»3Då†…å®¹é“ºå¹³äº†é“è·¯ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://gvfdiffusion.github.io/%E3%80%82">https://gvfdiffusion.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23785v1">PDF</a> ICCV 2025. Project page: <a target="_blank" rel="noopener" href="https://gvfdiffusion.github.io/">https://gvfdiffusion.github.io/</a></p>
<p><strong>Summary</strong><br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œå¯ä»å•ä¸ªè§†é¢‘è¾“å…¥ç”Ÿæˆé«˜è´¨é‡åŠ¨æ€3Då†…å®¹ã€‚é€šè¿‡å¼•å…¥Direct 4DMesh-to-GS Variation Field VAEï¼Œç›´æ¥å¯¹è§„èŒƒé«˜æ–¯Splatsï¼ˆGSï¼‰åŠå…¶æ—¶é—´å˜åŒ–è¿›è¡Œç¼–ç ï¼Œæ— éœ€å¯¹æ¯ä¸ªå®ä¾‹è¿›è¡Œæ‹Ÿåˆï¼Œå¹¶å°†é«˜ç»´åŠ¨ç”»å‹ç¼©åˆ°ç´§å‡‘çš„æ½œåœ¨ç©ºé—´ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œè®­ç»ƒäº†ä¸€ä¸ªåŸºäºé«˜æ–¯å˜å¼‚åœºæ‰©æ•£æ¨¡å‹çš„å…·æœ‰æ—¶é—´æ„ŸçŸ¥æ‰©æ•£å˜å‹å™¨çš„æ¨¡å‹ï¼Œä»¥è¾“å…¥è§†é¢‘å’Œè§„èŒƒGSä¸ºæ¡ä»¶ã€‚åœ¨Objaverseæ•°æ®é›†çš„å¯åŠ¨ç”»3Då¯¹è±¡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹å±•ç°å‡ºæ›´é«˜çš„ç”Ÿæˆè´¨é‡ï¼Œå¹¶ä¸”å¯¹é‡ç”Ÿè§†é¢‘è¾“å…¥å…·æœ‰å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘åˆ°4Dç”Ÿæˆæ¡†æ¶ï¼Œå¯ä»å•ä¸ªè§†é¢‘åˆ›å»ºé«˜è´¨é‡åŠ¨æ€3Då†…å®¹ã€‚</li>
<li>å¼•å…¥Direct 4DMesh-to-GS Variation Field VAEï¼Œæœ‰æ•ˆè¡¨ç¤ºå’Œå‹ç¼©3DåŠ¨ç”»æ•°æ®ã€‚</li>
<li>é‡‡ç”¨é«˜æ–¯å˜å¼‚åœºæ‰©æ•£æ¨¡å‹ï¼Œç»“åˆæ—¶é—´æ„ŸçŸ¥æ‰©æ•£å˜å‹å™¨è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ¨¡å‹åœ¨Objaverseæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå±•ç°å‡ºä¼˜ç§€çš„ç”Ÿæˆè´¨é‡ã€‚</li>
<li>æ¨¡å‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œèƒ½å¤„ç†æ¥è‡ªé‡ç”Ÿç¯å¢ƒçš„è§†é¢‘è¾“å…¥ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½å¤Ÿç›´æ¥å¤„ç†è§„èŒƒé«˜æ–¯Splatsï¼ˆGSï¼‰åŠå…¶æ—¶é—´å˜åŒ–ï¼Œæ— éœ€å¯¹æ¯ä¸ªå®ä¾‹è¿›è¡Œæ‹Ÿåˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23785">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0a1b89aa26ce3d68dfdd6a9bfc7138c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c831b3064b5b518234a2cb590c26c41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b3a04415c0d6138eb44fddde3539229.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c20ea647f542e878705d061131ccfb2a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-102ab5b4a8dfd7bbabc5cae8a1d38e20.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SeqAffordSplat-Scene-level-Sequential-Affordance-Reasoning-on-3D-Gaussian-Splatting"><a href="#SeqAffordSplat-Scene-level-Sequential-Affordance-Reasoning-on-3D-Gaussian-Splatting" class="headerlink" title="SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D   Gaussian Splatting"></a>SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D   Gaussian Splatting</h2><p><strong>Authors:Di Li, Jie Feng, Jiahao Chen, Weisheng Dong, Guanbin Li, Yuhui Zheng, Mingtao Feng, Guangming Shi</strong></p>
<p>3D affordance reasoning, the task of associating human instructions with the functional regions of 3D objects, is a critical capability for embodied agents. Current methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited to single-object, single-step interactions, a paradigm that falls short of addressing the long-horizon, multi-object tasks required for complex real-world applications. To bridge this gap, we introduce the novel task of Sequential 3D Gaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale benchmark featuring 1800+ scenes to support research on long-horizon affordance understanding in complex 3DGS environments. We then propose SeqSplatNet, an end-to-end framework that directly maps an instruction to a sequence of 3D affordance masks. SeqSplatNet employs a large language model that autoregressively generates text interleaved with special segmentation tokens, guiding a conditional decoder to produce the corresponding 3D mask. To handle complex scene geometry, we introduce a pre-training strategy, Conditional Geometric Reconstruction, where the model learns to reconstruct complete affordance region masks from known geometric observations, thereby building a robust geometric prior. Furthermore, to resolve semantic ambiguities, we design a feature injection mechanism that lifts rich semantic features from 2D Vision Foundation Models (VFM) and fuses them into the 3D decoder at multiple scales. Extensive experiments demonstrate that our method sets a new state-of-the-art on our challenging benchmark, effectively advancing affordance reasoning from single-step interactions to complex, sequential tasks at the scene level. </p>
<blockquote>
<p>3Dä½œç”¨æ¨ç†æ˜¯ä¸€é¡¹å°†äººç±»æŒ‡ä»¤ä¸3Då¯¹è±¡çš„åŠŸèƒ½åŒºåŸŸç›¸å…³è”çš„é‡è¦ä»»åŠ¡ï¼Œå¯¹äºå®ä½“ä»£ç†è€Œè¨€æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„èƒ½åŠ›ã€‚å½“å‰åŸºäº3Dé«˜æ–¯æ³¼æº…ï¼ˆ3DGSï¼‰çš„æ–¹æ³•æ ¹æœ¬ä¸Šä»…é™äºå•å¯¹è±¡ã€å•æ­¥éª¤äº¤äº’çš„æ¨¡å¼ï¼Œè¿™ç§æ¨¡å¼æ— æ³•åº”å¯¹å¤æ‚ç°å®ä¸–ç•Œåº”ç”¨æ‰€éœ€çš„é•¿å‘¨æœŸã€å¤šå¯¹è±¡ä»»åŠ¡ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†åºåˆ—3Dé«˜æ–¯ä½œç”¨æ¨ç†è¿™ä¸€æ–°ä»»åŠ¡ï¼Œå¹¶å»ºç«‹äº†SeqAffordSplatå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1800å¤šä¸ªåœºæ™¯ï¼Œä»¥æ”¯æŒåœ¨å¤æ‚3DGSç¯å¢ƒä¸­å¯¹é•¿å‘¨æœŸä½œç”¨ç†è§£çš„ç ”ç©¶ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†SeqSplatNetç«¯åˆ°ç«¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç›´æ¥å°†æŒ‡ä»¤æ˜ å°„åˆ°ä¸€ç³»åˆ—3Dä½œç”¨æ©è†œã€‚SeqSplatNeté‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡è‡ªå›å½’ç”Ÿæˆä¸ç‰¹æ®Šåˆ†å‰²ä»¤ç‰Œäº¤ç»‡çš„æ–‡æœ¬ï¼ŒæŒ‡å¯¼æ¡ä»¶è§£ç å™¨ç”Ÿæˆç›¸åº”çš„3Dæ©è†œã€‚ä¸ºäº†å¤„ç†å¤æ‚çš„åœºæ™¯å‡ ä½•ç»“æ„ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢„è®­ç»ƒç­–ç•¥ï¼Œå³æ¡ä»¶å‡ ä½•é‡å»ºï¼Œåœ¨è¯¥ç­–ç•¥ä¸­ï¼Œæ¨¡å‹å­¦ä¹ ä»å·²çŸ¥çš„å‡ ä½•è§‚æµ‹ä¸­é‡å»ºå®Œæ•´çš„ä½œç”¨åŒºåŸŸæ©è†œï¼Œä»è€Œå»ºç«‹ç¨³å¥çš„å‡ ä½•å…ˆéªŒã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³è¯­ä¹‰æ­§ä¹‰é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç‰¹å¾æ³¨å…¥æœºåˆ¶ï¼Œä»äºŒç»´è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰ä¸­æå–ä¸°å¯Œçš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶å°†å®ƒä»¬èåˆåˆ°å¤šä¸ªå°ºåº¦çš„ä¸‰ç»´è§£ç å™¨ä¸­ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æˆ‘ä»¬çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œæœ‰æ•ˆåœ°å°†ä½œç”¨æ¨ç†ä»å•æ­¥éª¤äº¤äº’æ¨è¿›åˆ°å¤æ‚çš„åœºæ™¯çº§åºåˆ—ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23772v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸‰ç»´é«˜æ–¯åˆ†å‰²æŠ€æœ¯ï¼ˆ3DGSï¼‰åœ¨æ™ºèƒ½ä½“æ‰§è¡Œä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚ç°å®ä¸–ç•Œä¸­çš„é•¿æœŸå¤šç›®æ ‡ä»»åŠ¡æ—¶çš„ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†åºåˆ—ä¸‰ç»´é«˜æ–¯åŠŸèƒ½æ¨ç†ï¼ˆSequential 3D Gaussian Affordance Reasoningï¼‰çš„æ–°ä»»åŠ¡ï¼Œå¹¶å»ºç«‹äº†SeqAffordSplatå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«è¶…è¿‡1800ä¸ªåœºæ™¯ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§åä¸ºSeqSplatNetçš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ç›´æ¥å°†æŒ‡ä»¤æ˜ å°„åˆ°ä¸€ç³»åˆ—ä¸‰ç»´åŠŸèƒ½æ©è†œã€‚SeqSplatNeté‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡æ¡ä»¶è§£ç å™¨ç”Ÿæˆç›¸åº”çš„ä¸‰ç»´æ©è†œã€‚ä¸ºè§£å†³åœºæ™¯å‡ ä½•å¤æ‚æ€§å’Œè¯­ä¹‰æ¨¡ç³Šæ€§é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†æ¡ä»¶å‡ ä½•é‡å»ºç­–ç•¥å’Œç‰¹å¾æ³¨å…¥æœºåˆ¶ã€‚å®éªŒè¯æ˜ï¼ŒSeqSplatNetåœ¨å¤æ‚åœºæ™¯çº§åˆ«çš„é•¿æœŸå¤šç›®æ ‡ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°†åŠŸèƒ½æ¨ç†ä»å•æ­¥äº¤äº’æ¨å‘äº†æ–°çš„é«˜åº¦ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰åŸºäºä¸‰ç»´é«˜æ–¯åˆ†å‰²ï¼ˆ3DGSï¼‰çš„æ–¹æ³•åœ¨å¤„ç†é•¿æœŸå¤šç›®æ ‡ä»»åŠ¡æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>å¼•å…¥äº†åºåˆ—ä¸‰ç»´é«˜æ–¯åŠŸèƒ½æ¨ç†çš„æ–°ä»»åŠ¡ï¼Œä»¥åº”å¯¹å¤æ‚ç°å®ä¸–ç•Œä¸­çš„é•¿æœŸå¤šç›®æ ‡ä»»åŠ¡ã€‚</li>
<li>å»ºç«‹äº†SeqAffordSplatå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«å¤šæ ·åŒ–åœºæ™¯ï¼Œæ”¯æŒé•¿æœŸåŠŸèƒ½ç†è§£çš„ç ”ç©¶ã€‚</li>
<li>æå‡ºäº†SeqSplatNetæ¡†æ¶ï¼Œèƒ½ç›´æ¥å¤„ç†æŒ‡ä»¤å¹¶ç”Ÿæˆä¸€ç³»åˆ—ä¸‰ç»´åŠŸèƒ½æ©è†œã€‚</li>
<li>é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œè§£å†³åœºæ™¯å‡ ä½•å¤æ‚æ€§é—®é¢˜ã€‚</li>
<li>å¼•å…¥æ¡ä»¶å‡ ä½•é‡å»ºç­–ç•¥ï¼Œæå‡æ¨¡å‹å¯¹å®Œæ•´åŠŸèƒ½åŒºåŸŸçš„é‡å»ºèƒ½åŠ›ã€‚</li>
<li>è®¾è®¡äº†ç‰¹å¾æ³¨å…¥æœºåˆ¶ï¼ŒèåˆäºŒç»´è§†è§‰åŸºç¡€æ¨¡å‹çš„ä¸°å¯Œè¯­ä¹‰ç‰¹å¾ï¼Œæé«˜è¯­ä¹‰ç†è§£çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52fcd06f4e24d5923434944fdc122204.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42723816d289ea4eb2cdc248de4cff9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2be20f2dc5a62b2156870f4d01e4ab69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06f335d5aec48beb87d4479869f0e46d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MoGA-3D-Generative-Avatar-Prior-for-Monocular-Gaussian-Avatar-Reconstruction"><a href="#MoGA-3D-Generative-Avatar-Prior-for-Monocular-Gaussian-Avatar-Reconstruction" class="headerlink" title="MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar   Reconstruction"></a>MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar   Reconstruction</h2><p><strong>Authors:Zijian Dong, Longteng Duan, Jie Song, Michael J. Black, Andreas Geiger</strong></p>
<p>We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian avatars from a single-view image. The main challenge lies in inferring unseen appearance and geometric details while ensuring 3D consistency and realism. Most previous methods rely on 2D diffusion models to synthesize unseen views; however, these generated views are sparse and inconsistent, resulting in unrealistic 3D artifacts and blurred appearance. To address these limitations, we leverage a generative avatar model, that can generate diverse 3D avatars by sampling deformed Gaussians from a learned prior distribution. Due to the limited amount of 3D training data such a 3D model alone cannot capture all image details of unseen identities. Consequently, we integrate it as a prior, ensuring 3D consistency by projecting input images into its latent space and enforcing additional 3D appearance and geometric constraints. Our novel approach formulates Gaussian avatar creation as a model inversion process by fitting the generative avatar to synthetic views from 2D diffusion models. The generative avatar provides a meaningful initialization for model fitting, enforces 3D regularization, and helps in refining pose estimation. Experiments show that our method surpasses state-of-the-art techniques and generalizes well to real-world scenarios. Our Gaussian avatars are also inherently animatable </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†MoGAï¼Œè¿™æ˜¯ä¸€ç§ä»å•è§†å›¾å›¾åƒé‡å»ºé«˜ä¿çœŸ3Dé«˜æ–¯å¤´åƒçš„æ–°æ–¹æ³•ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºæ¨æ–­å‡ºçœ‹ä¸è§çš„å¤–è§‚å’Œå‡ ä½•ç»†èŠ‚ï¼ŒåŒæ—¶ç¡®ä¿3Dçš„ä¸€è‡´æ€§å’ŒçœŸå®æ€§ã€‚å¤§å¤šæ•°ä¹‹å‰çš„æ–¹æ³•ä¾èµ–äº2Dæ‰©æ•£æ¨¡å‹æ¥åˆæˆæœªè§çš„è§†å›¾ï¼›ç„¶è€Œï¼Œè¿™äº›ç”Ÿæˆçš„è§†å›¾æ˜¯ç¨€ç–ä¸”ä¸ä¸€è‡´çš„ï¼Œå¯¼è‡´3Däººå·¥åˆ¶å“ä¸çœŸå®å’Œå¤–è§‚æ¨¡ç³Šã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨ç”Ÿæˆå¤´åƒæ¨¡å‹ï¼Œé€šè¿‡ä»å­¦ä¹ çš„å…ˆéªŒåˆ†å¸ƒä¸­é‡‡æ ·å˜å½¢é«˜æ–¯æ¥ç”Ÿæˆå„ç§3Då¤´åƒã€‚ç”±äº3Dè®­ç»ƒæ•°æ®çš„æ•°é‡æœ‰é™ï¼Œä»…ä½¿ç”¨è¿™æ ·çš„3Dæ¨¡å‹æ— æ³•æ•è·æœªè§èº«ä»½çš„æ‰€æœ‰å›¾åƒç»†èŠ‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å…¶æ•´åˆä¸ºä¼˜å…ˆäº‹é¡¹ï¼Œé€šè¿‡å°†è¾“å…¥å›¾åƒæŠ•å½±åˆ°å…¶æ½œåœ¨ç©ºé—´å¹¶å¼ºåˆ¶é¢å¤–çš„3Då¤–è§‚å’Œå‡ ä½•çº¦æŸæ¥ç¡®ä¿3Dä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–°æ–¹æ³•å°†é«˜æ–¯å¤´åƒåˆ›å»ºå…¬å¼åŒ–ä¸ºä¸€ä¸ªæ¨¡å‹åè½¬è¿‡ç¨‹ï¼Œé€šè¿‡å°†ç”Ÿæˆçš„å¤´åƒæ‹Ÿåˆåˆ°æ¥è‡ª2Dæ‰©æ•£æ¨¡å‹çš„åˆæˆè§†å›¾ã€‚ç”Ÿæˆçš„å¤´åƒä¸ºæ¨¡å‹æ‹Ÿåˆæä¾›äº†ä¸€ä¸ªæœ‰æ„ä¹‰çš„åˆå§‹åŒ–ï¼Œå¼ºåˆ¶è¿›è¡Œ3Dæ­£åˆ™åŒ–ï¼Œå¹¶æœ‰åŠ©äºæ”¹è¿›å§¿åŠ¿ä¼°è®¡ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†æœ€æ–°æŠ€æœ¯ï¼Œå¹¶èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°ç°å®ä¸–ç•Œåœºæ™¯ã€‚æˆ‘ä»¬çš„é«˜æ–¯å¤´åƒæœ¬è´¨ä¸Šæ˜¯å¯åŠ¨ç”»çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23597v1">PDF</a> ICCV 2025 (Highlight), Project Page: <a target="_blank" rel="noopener" href="https://zj-dong.github.io/MoGA/">https://zj-dong.github.io/MoGA/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†MoGAï¼Œä¸€ç§ä»å•è§†è§’å›¾åƒé‡å»ºé«˜ä¿çœŸ3Dé«˜æ–¯å¤´åƒçš„æ–°æ–¹æ³•ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºæ¨æ–­å‡ºéšè—çš„å¤–è§‚å’Œå‡ ä½•ç»†èŠ‚ï¼ŒåŒæ—¶ç¡®ä¿3Dçš„ä¸€è‡´æ€§å’ŒçœŸå®æ€§ã€‚é€šè¿‡ç»“åˆç”Ÿæˆå¤´åƒæ¨¡å‹å’ŒæŠ•å½±è¾“å…¥å›¾åƒåˆ°æ½œåœ¨ç©ºé—´ç­‰æŠ€æœ¯ï¼Œè§£å†³äº†ä»¥å¾€æ–¹æ³•ç”Ÿæˆçš„è§†å›¾ç¨€ç–ã€ä¸ä¸€è‡´ã€å¯¼è‡´3Dæ•ˆæœä¸çœŸå®çš„é—®é¢˜ã€‚æ–°æ–¹æ³•å°†é«˜æ–¯å¤´åƒåˆ›å»ºè¡¨è¿°ä¸ºæ¨¡å‹åæ¼”è¿‡ç¨‹ï¼Œé€šè¿‡æ‹Ÿåˆç”Ÿæˆå¤´åƒåˆ°æ¥è‡ªäºŒç»´æ‰©æ•£æ¨¡å‹çš„åˆæˆè§†å›¾ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•è¶…è¶Šç°æœ‰æŠ€æœ¯ï¼Œå¹¶è‰¯å¥½åœ°æ¨å¹¿è‡³çœŸå®ä¸–ç•Œåœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•MoGAï¼Œèƒ½å¤Ÿä»å•è§†è§’å›¾åƒé‡å»ºé«˜ä¿çœŸ3Dé«˜æ–¯å¤´åƒã€‚</li>
<li>è§£å†³äº†ä»¥å¾€æ–¹æ³•ç”Ÿæˆçš„è§†å›¾ç¨€ç–ã€ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œæé«˜äº†3Då¤´åƒçš„çœŸå®æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>é€šè¿‡ç»“åˆç”Ÿæˆå¤´åƒæ¨¡å‹ï¼Œé‡‡ç”¨ä»å­¦ä¹ åˆ°çš„å…ˆéªŒåˆ†å¸ƒä¸­é‡‡æ ·å˜å½¢é«˜æ–¯çš„æ–¹å¼ï¼Œç”Ÿæˆå¤šæ ·çš„3Då¤´åƒã€‚</li>
<li>å°†ç”Ÿæˆå¤´åƒæ¨¡å‹ä½œä¸ºå…ˆéªŒï¼Œé€šè¿‡æŠ•å½±è¾“å…¥å›¾åƒåˆ°æ½œåœ¨ç©ºé—´ï¼ŒåŠ å¼º3Dä¸€è‡´æ€§ã€‚</li>
<li>å°†é«˜æ–¯å¤´åƒåˆ›å»ºè¡¨è¿°ä¸ºæ¨¡å‹åæ¼”è¿‡ç¨‹ï¼Œé€šè¿‡æ‹Ÿåˆç”Ÿæˆå¤´åƒåˆ°äºŒç»´æ‰©æ•£æ¨¡å‹çš„åˆæˆè§†å›¾ã€‚</li>
<li>MoGAæ–¹æ³•è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­å…·æœ‰è‰¯å¥½çš„æ¨å¹¿æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-36299fa1d2de31244961425d9c2f9e03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92c4cfb63cc8def479f57a52e048adf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4030795a97d1504b3de20f5c85aeca83.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Gaussian-Splatting-Feature-Fields-for-Privacy-Preserving-Visual-Localization"><a href="#Gaussian-Splatting-Feature-Fields-for-Privacy-Preserving-Visual-Localization" class="headerlink" title="Gaussian Splatting Feature Fields for Privacy-Preserving Visual   Localization"></a>Gaussian Splatting Feature Fields for Privacy-Preserving Visual   Localization</h2><p><strong>Authors:Maxime Pietrantoni, Gabriela Csurka, Torsten Sattler</strong></p>
<p>Visual localization is the task of estimating a camera pose in a known environment. In this paper, we utilize 3D Gaussian Splatting (3DGS)-based representations for accurate and privacy-preserving visual localization. We propose Gaussian Splatting Feature Fields (GSFFs), a scene representation for visual localization that combines an explicit geometry model (3DGS) with an implicit feature field. We leverage the dense geometric information and differentiable rasterization algorithm from 3DGS to learn robust feature representations grounded in 3D. In particular, we align a 3D scale-aware feature field and a 2D feature encoder in a common embedding space through a contrastive framework. Using a 3D structure-informed clustering procedure, we further regularize the representation learning and seamlessly convert the features to segmentations, which can be used for privacy-preserving visual localization. Pose refinement, which involves aligning either feature maps or segmentations from a query image with those rendered from the GSFFs scene representation, is used to achieve localization. The resulting privacy- and non-privacy-preserving localization pipelines, evaluated on multiple real-world datasets, show state-of-the-art performances. </p>
<blockquote>
<p>è§†è§‰å®šä½æ˜¯åœ¨å·²çŸ¥ç¯å¢ƒä¸­ä¼°è®¡ç›¸æœºå§¿æ€çš„ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨åŸºäº3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰çš„è¡¨ç¤ºæ¥è¿›è¡Œç²¾ç¡®ä¸”ä¿æŠ¤éšç§çš„è§†è§‰å®šä½ã€‚æˆ‘ä»¬æå‡ºé«˜æ–¯æ‹¼è´´ç‰¹å¾åœºï¼ˆGSFFsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è§†è§‰å®šä½çš„åœºæ™¯è¡¨ç¤ºï¼Œå®ƒå°†æ˜¾å¼çš„å‡ ä½•æ¨¡å‹ï¼ˆ3DGSï¼‰ä¸éšå¼çš„ç‰¹å¾åœºç›¸ç»“åˆã€‚æˆ‘ä»¬åˆ©ç”¨3DGSçš„å¯†é›†å‡ ä½•ä¿¡æ¯å’Œå¯å¾®æ …æ ¼åŒ–ç®—æ³•ï¼Œå­¦ä¹ åŸºäº3Dçš„ç¨³å¥ç‰¹å¾è¡¨ç¤ºã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹æ¯”æ¡†æ¶åœ¨3Då°ºåº¦æ„ŸçŸ¥ç‰¹å¾åœºå’Œ2Dç‰¹å¾ç¼–ç å™¨ä¹‹é—´å»ºç«‹ä¸€ä¸ªé€šç”¨çš„åµŒå…¥ç©ºé—´ã€‚é€šè¿‡åˆ©ç”¨3Dç»“æ„ä¿¡æ¯çš„èšç±»ç¨‹åºï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ­£åˆ™åŒ–è¡¨ç¤ºå­¦ä¹ ï¼Œæ— ç¼åœ°å°†ç‰¹å¾è½¬æ¢ä¸ºåˆ†å‰²ï¼Œè¿™å¯ç”¨äºä¿æŠ¤éšç§çš„è§†è§‰å®šä½ã€‚å§¿æ€ä¿®æ­£æ¶‰åŠå°†æŸ¥è¯¢å›¾åƒçš„ç‰¹å¾å›¾æˆ–åˆ†å‰²ä¸ä»GSFFsåœºæ™¯è¡¨ç¤ºå‘ˆç°çš„ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œä»¥å®ç°å®šä½ã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¯„ä¼°çš„éšç§å’Œééšç§ä¿æŠ¤å®šä½æµç¨‹å‡æ˜¾ç¤ºå‡ºæœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23569v1">PDF</a> CVPR 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ©ç”¨åŸºäºä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰çš„è¡¨ç¤ºæ–¹æ³•è¿›è¡Œç²¾ç¡®ä¸”ä¿æŠ¤éšç§çš„è§†è§‰å®šä½ã€‚æå‡ºé«˜æ–¯æ‹¼è´´ç‰¹å¾åœºï¼ˆGSFFsï¼‰ï¼Œç»“åˆæ˜¾å¼å‡ ä½•æ¨¡å‹ï¼ˆ3DGSï¼‰å’Œéšå¼ç‰¹å¾åœºï¼Œç”¨äºè§†è§‰å®šä½çš„åœºæ™¯è¡¨ç¤ºã€‚é€šè¿‡åˆ©ç”¨3DGSçš„å¯†é›†å‡ ä½•ä¿¡æ¯å’Œå¯å¾®åˆ†æ …æ ¼åŒ–ç®—æ³•ï¼Œå­¦ä¹ åŸºäºä¸‰ç»´çš„ç¨³å¥ç‰¹å¾è¡¨ç¤ºã€‚é€šè¿‡å¯¹æ¯”æ¡†æ¶ï¼Œå°†ä¸‰ç»´å°ºåº¦æ„ŸçŸ¥ç‰¹å¾åœºå’ŒäºŒç»´ç‰¹å¾ç¼–ç å™¨å¯¹é½åˆ°å…¬å…±åµŒå…¥ç©ºé—´ã€‚åˆ©ç”¨ä¸‰ç»´ç»“æ„æ„ŸçŸ¥èšç±»ç¨‹åºï¼Œè¿›ä¸€æ­¥è§„èŒƒè¡¨ç¤ºå­¦ä¹ ï¼Œæ— ç¼è½¬æ¢ç‰¹å¾ä¸ºåˆ†æ®µï¼Œå¯ç”¨äºä¿æŠ¤éšç§çš„è§†è§‰å®šä½ã€‚é€šè¿‡å§¿åŠ¿å¾®è°ƒï¼Œå®ç°å®šä½ï¼Œå³åœ¨æŸ¥è¯¢å›¾åƒçš„ç‰¹å¾å›¾æˆ–åˆ†æ®µä¸GSFFsåœºæ™¯è¡¨ç¤ºæ‰€å‘ˆç°çš„ç‰¹å¾å›¾æˆ–åˆ†æ®µä¹‹é—´è¿›è¡Œå¯¹é½ã€‚åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¯æ˜äº†éšç§å’Œééšç§ä¿æŠ¤å®šä½ç®¡é“å‡è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨ä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰è¿›è¡Œè§†è§‰å®šä½ï¼Œå®ç°ç²¾ç¡®æ€§å¹¶ä¿æŠ¤éšç§ã€‚</li>
<li>æå‡ºé«˜æ–¯æ‹¼è´´ç‰¹å¾åœºï¼ˆGSFFsï¼‰ï¼Œç»“åˆæ˜¾å¼å‡ ä½•æ¨¡å‹ä¸éšå¼ç‰¹å¾åœºã€‚</li>
<li>åˆ©ç”¨3DGSçš„å¯†é›†å‡ ä½•ä¿¡æ¯å’Œå¯å¾®åˆ†æ …æ ¼åŒ–ç®—æ³•ï¼Œå­¦ä¹ ç¨³å¥çš„ä¸‰ç»´ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>é€šè¿‡å¯¹æ¯”æ¡†æ¶å¯¹é½ä¸‰ç»´å°ºåº¦æ„ŸçŸ¥ç‰¹å¾åœºå’ŒäºŒç»´ç‰¹å¾ç¼–ç å™¨ã€‚</li>
<li>é‡‡ç”¨ä¸‰ç»´ç»“æ„æ„ŸçŸ¥èšç±»ç¨‹åºï¼Œå®ç°ç‰¹å¾åˆ°åˆ†æ®µçš„è½¬æ¢ï¼Œç”¨äºéšç§ä¿æŠ¤å®šä½ã€‚</li>
<li>é€šè¿‡å§¿åŠ¿å¾®è°ƒå®ç°å®šä½ï¼Œé€šè¿‡å¯¹é½æŸ¥è¯¢å›¾åƒçš„ç‰¹å¾å›¾æˆ–åˆ†æ®µä¸åœºæ™¯è¡¨ç¤ºçš„ç‰¹å¾å›¾æˆ–åˆ†æ®µæ¥å®Œæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23569">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-c742e0212b82cdb83a8e4fcc62fd7f91.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-05694e9146f8a22aae5ebbb28cf8226d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3505621d0bc150550397b2c4d62c76a2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="NeRF-Is-a-Valuable-Assistant-for-3D-Gaussian-Splatting"><a href="#NeRF-Is-a-Valuable-Assistant-for-3D-Gaussian-Splatting" class="headerlink" title="NeRF Is a Valuable Assistant for 3D Gaussian Splatting"></a>NeRF Is a Valuable Assistant for 3D Gaussian Splatting</h2><p><strong>Authors:Shuangkang Fang, I-Chao Shen, Takeo Igarashi, Yufeng Wang, ZeSheng Wang, Yi Yang, Wenrui Ding, Shuchang Zhou</strong></p>
<p>We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†NeRF-GSï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒè”åˆä¼˜åŒ–äº†ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œ3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰ã€‚è¯¥æ¡†æ¶åˆ©ç”¨NeRFçš„å›ºæœ‰è¿ç»­ç©ºé—´è¡¨ç¤ºæ¥å‡è½»3DGSçš„å‡ ä¸ªå±€é™æ€§ï¼ŒåŒ…æ‹¬é«˜æ–¯åˆå§‹åŒ–çš„æ•æ„Ÿæ€§ã€ç©ºé—´æ„ŸçŸ¥çš„å±€é™æ€§ä»¥åŠé«˜æ–¯é—´å…³è”è¾ƒå¼±ç­‰é—®é¢˜ï¼Œä»è€Œæé«˜äº†å…¶æ€§èƒ½ã€‚åœ¨NeRF-GSä¸­ï¼Œæˆ‘ä»¬é‡æ–°è®¾è®¡äº†3DGSï¼Œå¹¶é€æ­¥å°†å…¶ç©ºé—´ç‰¹å¾ä¸NeRFå¯¹é½ï¼Œä½¿ä¸¤ç§è¡¨ç¤ºå½¢å¼èƒ½å¤Ÿé€šè¿‡å…±äº«çš„ä¸‰ç»´ç©ºé—´ä¿¡æ¯åœ¨åŒä¸€åœºæ™¯ä¸­è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡ä¼˜åŒ–éšç‰¹å¾å’Œé«˜æ–¯ä½ç½®çš„æ®‹å·®å‘é‡æ¥è§£å†³ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„å½¢å¼å·®å¼‚ï¼Œä»¥å¢å¼º3DGSçš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒNeRF-GSè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™ä¸€ç»“æœè¯å®ï¼ŒNeRFå’Œ3DGSæ˜¯äº’è¡¥çš„è€Œä¸æ˜¯ç«äº‰çš„ï¼Œä¸ºç»“åˆ3DGSå’ŒNeRFçš„é«˜æ•ˆ3Dåœºæ™¯è¡¨ç¤ºæä¾›äº†æ··åˆæ–¹æ³•çš„å…¨æ–°è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23374v1">PDF</a> Accepted by ICCV</p>
<p><strong>æ‘˜è¦</strong></p>
<p>NeRF-GSæ˜¯ä¸€ç§ç»“åˆäº†ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰çš„æ–°å‹æ¡†æ¶ã€‚å®ƒé€šè¿‡åˆ©ç”¨NeRFçš„å†…åœ¨è¿ç»­ç©ºé—´è¡¨ç¤ºæ¥å…‹æœ3DGSçš„å‡ ä¸ªå±€é™æ€§ï¼ŒåŒ…æ‹¬é«˜æ–¯åˆå§‹åŒ–çš„æ•æ„Ÿæ€§ã€ç©ºé—´æ„ŸçŸ¥çš„æœ‰é™æ€§ä»¥åŠé«˜æ–¯é—´å…³è”æ€§çš„å¾®å¼±ã€‚åœ¨NeRF-GSä¸­ï¼Œé‡æ–°è®¾è®¡äº†3DGSï¼Œé€æ­¥å°†å…¶ç©ºé—´ç‰¹å¾ä¸NeRFå¯¹é½ï¼Œä½¿ä¸¤ç§è¡¨ç¤ºæ–¹æ³•èƒ½å¤Ÿåœ¨åŒä¸€åœºæ™¯ä¸­è¿›è¡Œä¼˜åŒ–ï¼Œé€šè¿‡å…±äº«ä¸‰ç»´ç©ºé—´ä¿¡æ¯æ¥å¢å¼ºæ€§èƒ½ã€‚é€šè¿‡ä¼˜åŒ–éšå¼ç‰¹å¾å’Œé«˜æ–¯ä½ç½®çš„æ®‹å·®å‘é‡ï¼Œè§£å†³äº†ä¸¤è€…ä¹‹é—´çš„å½¢å¼å·®å¼‚ï¼Œæé«˜äº†3DGSçš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒNeRF-GSè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™è¯æ˜äº†NeRFå’Œ3DGSæ˜¯äº’è¡¥çš„è€Œä¸æ˜¯ç«äº‰çš„ï¼Œä¸ºç»“åˆ3DGSå’ŒNeRFçš„æœ‰æ•ˆä¸‰ç»´åœºæ™¯è¡¨ç¤ºæä¾›äº†æ–°è§è§£ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>NeRF-GSç»“åˆäº†ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰çš„æ–°å‹æ¡†æ¶ã€‚</li>
<li>NeRF-GSå…‹æœäº†3DGSçš„å‡ ä¸ªå±€é™æ€§ï¼ŒåŒ…æ‹¬é«˜æ–¯åˆå§‹åŒ–çš„æ•æ„Ÿæ€§ã€ç©ºé—´æ„ŸçŸ¥çš„æœ‰é™æ€§ä»¥åŠé«˜æ–¯é—´å…³è”æ€§çš„å¾®å¼±ã€‚</li>
<li>é€šè¿‡å…±äº«ä¸‰ç»´ç©ºé—´ä¿¡æ¯ï¼ŒNeRFå’Œ3DGSåœ¨ä¼˜åŒ–ä¸­å¯ä»¥ç›¸äº’è¡¥å……ã€‚</li>
<li>é‡æ–°è®¾è®¡äº†3DGSçš„ç©ºé—´ç‰¹å¾ï¼Œä½¿å…¶ä¸NeRFå¯¹é½ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–éšå¼ç‰¹å¾å’Œé«˜æ–¯ä½ç½®çš„æ®‹å·®å‘é‡ï¼Œè§£å†³äº†NeRFå’Œ3DGSä¹‹é—´çš„å½¢å¼å·®å¼‚ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒNeRF-GSè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23374">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-171cb64b2dc7ed8ead6e62a2bc7dc785.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dad08707822011a404d304345bd3a4a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92a93dbadca0a7676aa1e22e393da7a5.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="iLRM-An-Iterative-Large-3D-Reconstruction-Model"><a href="#iLRM-An-Iterative-Large-3D-Reconstruction-Model" class="headerlink" title="iLRM: An Iterative Large 3D Reconstruction Model"></a>iLRM: An Iterative Large 3D Reconstruction Model</h2><p><strong>Authors:Gyeongjin Kang, Seungtae Nam, Xiangyu Sun, Sameh Khamis, Abdelrahman Mohamed, Eunbyung Park</strong></p>
<p>Feed-forward 3D modeling has emerged as a promising approach for rapid and high-quality 3D reconstruction. In particular, directly generating explicit 3D representations, such as 3D Gaussian splatting, has attracted significant attention due to its fast and high-quality rendering, as well as numerous applications. However, many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases. Toward a scalable and efficient feed-forward 3D reconstruction, we introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction. Experimental results on widely used datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms existing methods in both reconstruction quality and speed. Notably, iLRM exhibits superior scalability, delivering significantly higher reconstruction quality under comparable computational cost by efficiently leveraging a larger number of input views. </p>
<blockquote>
<p>å‰é¦ˆ3Då»ºæ¨¡å·²æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„å¿«é€Ÿã€é«˜è´¨é‡3Dé‡å»ºæ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ç›´æ¥ç”Ÿæˆæ˜¾å¼3Dè¡¨ç¤ºï¼ˆå¦‚3Dé«˜æ–¯æ•£æ–‘ï¼‰å¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå› å…¶å¿«é€Ÿã€é«˜è´¨é‡çš„æ¸²æŸ“ä»¥åŠä¼—å¤šçš„åº”ç”¨ã€‚ç„¶è€Œï¼Œè®¸å¤šæœ€å…ˆè¿›çš„æ–¹æ³•ä¸»è¦åŸºäºtransformeræ¶æ„ï¼Œå­˜åœ¨ä¸¥é‡çš„å¯æ‰©å±•æ€§é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬ä¾èµ–äºæ¥è‡ªå¤šä¸ªè¾“å…¥è§†å›¾çš„å›¾åƒæ ‡è®°çš„å…¨æ³¨æ„åŠ›ï¼Œéšç€è§†å›¾æ•°é‡æˆ–å›¾åƒåˆ†è¾¨ç‡çš„å¢åŠ ï¼Œè®¡ç®—æˆæœ¬æˆä¸ºç¦æ­¢æ€§æˆæœ¬ã€‚ä¸ºäº†å®ç°å¯ä¼¸ç¼©å’Œé«˜æ•ˆçš„å‰é¦ˆ3Dé‡å»ºï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¿­ä»£å¼å¤§å‹3Dé‡å»ºæ¨¡å‹ï¼ˆiLRMï¼‰ï¼Œè¯¥æ¨¡å‹é€šè¿‡è¿­ä»£ç»†åŒ–æœºåˆ¶ç”Ÿæˆ3Dé«˜æ–¯è¡¨ç¤ºï¼Œç”±ä¸‰ä¸ªæ ¸å¿ƒåŸåˆ™æŒ‡å¯¼ï¼šï¼ˆ1ï¼‰å°†åœºæ™¯è¡¨ç¤ºä¸è¾“å…¥è§†å›¾å›¾åƒè§£è€¦ï¼Œä»¥å®ç°ç´§å‡‘çš„3Dè¡¨ç¤ºï¼›ï¼ˆ2ï¼‰å°†å…¨æ³¨æ„åŠ›å¤šè§†å›¾äº¤äº’åˆ†è§£ä¸ºä¸¤é˜¶æ®µæ³¨æ„åŠ›æ–¹æ¡ˆï¼Œä»¥é™ä½è®¡ç®—æˆæœ¬ï¼›ï¼ˆ3ï¼‰åœ¨æ¯ä¸€å±‚æ³¨å…¥é«˜åˆ†è¾¨ç‡ä¿¡æ¯ï¼Œä»¥å®ç°é«˜ä¿çœŸé‡å»ºã€‚åœ¨RE10Kå’ŒDL3DVç­‰å¸¸ç”¨æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒiLRMåœ¨é‡å»ºè´¨é‡å’Œé€Ÿåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒiLRMè¡¨ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§ï¼Œé€šè¿‡æœ‰æ•ˆåˆ©ç”¨æ›´å¤šçš„è¾“å…¥è§†å›¾ï¼Œåœ¨å¯æ¯”è¾ƒçš„è®¡ç®—æˆæœ¬ä¸‹å®ç°æ›´é«˜çš„é‡å»ºè´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23277v1">PDF</a> Project page: <a target="_blank" rel="noopener" href="https://gynjn.github.io/iLRM/">https://gynjn.github.io/iLRM/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºè¿­ä»£çš„å¤§å‹ä¸‰ç»´é‡å»ºæ¨¡å‹ï¼ˆiLRMï¼‰åœ¨ä¸‰ç»´é‡å»ºä¸­çš„åº”ç”¨ã€‚iLRMé€šè¿‡è¿­ä»£ä¼˜åŒ–æœºåˆ¶ç”Ÿæˆä¸‰ç»´é«˜æ–¯è¡¨ç¤ºï¼Œé‡‡ç”¨ä¸‰ä¸ªæ ¸å¿ƒåŸåˆ™å®ç°é«˜æ•ˆçš„ä¸‰ç»´é‡å»ºï¼šè§£è€¦åœºæ™¯è¡¨ç¤ºä¸è¾“å…¥è§†å›¾å›¾åƒï¼Œå®ç°ç´§å‡‘çš„ä¸‰ç»´è¡¨ç¤ºï¼›å°†å…¨æ³¨æ„åŠ›å¤šè§†å›¾äº¤äº’åˆ†è§£ä¸ºä¸¤é˜¶æ®µæ³¨æ„åŠ›æ–¹æ¡ˆä»¥é™ä½è®¡ç®—æˆæœ¬ï¼›åœ¨æ¯ä¸€å±‚æ³¨å…¥é«˜åˆ†è¾¨ç‡ä¿¡æ¯ä»¥å®ç°é«˜ä¿çœŸé‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒiLRMåœ¨é‡å»ºè´¨é‡å’Œé€Ÿåº¦æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°å‡ºä¼˜è¶Šçš„æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ–‡ä¸­æå‡ºåˆ©ç”¨è¿­ä»£çš„å¤§å‹ä¸‰ç»´é‡å»ºæ¨¡å‹ï¼ˆiLRMï¼‰å®ç°é«˜æ•ˆçš„ä¸‰ç»´é‡å»ºã€‚è¯¥æ¨¡å‹é‡‡ç”¨è¿­ä»£ä¼˜åŒ–æœºåˆ¶ç”Ÿæˆä¸‰ç»´é«˜æ–¯è¡¨ç¤ºã€‚</li>
<li>iLRMé€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒåŸåˆ™å®ç°ç´§å‡‘çš„ä¸‰ç»´è¡¨ç¤ºå’Œé«˜æ•ˆè®¡ç®—ï¼šè§£è€¦åœºæ™¯è¡¨ç¤ºä¸è¾“å…¥è§†å›¾å›¾åƒï¼›é‡‡ç”¨ä¸¤é˜¶æ®µæ³¨æ„åŠ›æ–¹æ¡ˆé™ä½è®¡ç®—æˆæœ¬ï¼›æ³¨å…¥é«˜åˆ†è¾¨ç‡ä¿¡æ¯ä»¥æå‡é‡å»ºè´¨é‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-222ee592da0aa00e9b4a41e213db2fc9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3532ca4b0494f05a9cabfddc19e66960.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cf3d562fe0bcd403789d249c4c9f60d0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="GSFusion-Globally-Optimized-LiDAR-Inertial-Visual-Mapping-for-Gaussian-Splatting"><a href="#GSFusion-Globally-Optimized-LiDAR-Inertial-Visual-Mapping-for-Gaussian-Splatting" class="headerlink" title="GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian   Splatting"></a>GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian   Splatting</h2><p><strong>Authors:Jaeseok Park, Chanoh Park, Minsu Kim, Soohwan Kim</strong></p>
<p>While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping, conventional approaches based on camera sensor, even RGB-D, suffer from fundamental limitations such as high computational load, failure in environments with poor texture or illumination, and short operational ranges. LiDAR emerges as a robust alternative, but its integration with 3DGS introduces new challenges, such as the need for exceptional global alignment for photorealistic quality and prolonged optimization times caused by sparse data. To address these challenges, we propose GSFusion, an online LiDAR-Inertial-Visual mapping system that ensures high-precision map consistency through a surfel-to-surfel constraint in the global pose-graph optimization. To handle sparse data, our system employs a pixel-aware Gaussian initialization strategy for efficient representation and a bounded sigmoid constraint to prevent uncontrolled Gaussian growth. Experiments on public and our datasets demonstrate our system outperforms existing 3DGS SLAM systems in terms of rendering quality and map-building efficiency. </p>
<blockquote>
<p>è™½ç„¶3Dé«˜æ–¯å»¶å±•ï¼ˆ3DGSï¼‰å·²ç»å®ç°äº†å¯¹çœŸå®æ„Ÿæ˜ å°„çš„é©å‘½æ€§æ”¹å˜ï¼Œä½†åŸºäºç›¸æœºä¼ æ„Ÿå™¨çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œç”šè‡³æ˜¯RGB-Dï¼Œä»ç„¶å­˜åœ¨ä¸€äº›åŸºæœ¬å±€é™ï¼Œå¦‚è®¡ç®—è´Ÿè½½é«˜ã€åœ¨çº¹ç†æˆ–ç…§æ˜ä¸è‰¯çš„ç¯å¢ƒä¸­å¤±æ•ˆä»¥åŠæ“ä½œèŒƒå›´çŸ­ç­‰ã€‚æ¿€å…‰é›·è¾¾ä½œä¸ºä¸€ç§ç¨³å¥çš„æ›¿ä»£æ–¹æ¡ˆè€Œå‡ºç°ï¼Œä½†å°†å…¶ä¸3DGSé›†æˆå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚ä¸ºäº†å®ç°çœŸå®æ„Ÿè´¨é‡è€Œéœ€è¦è¿›è¡Œå‡ºè‰²çš„å…¨å±€å¯¹é½ä»¥åŠç”±äºç¨€ç–æ•°æ®è€Œå¯¼è‡´çš„ä¼˜åŒ–æ—¶é—´å»¶é•¿ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†GSFusionï¼Œè¿™æ˜¯ä¸€ç§åœ¨çº¿æ¿€å…‰é›·è¾¾æƒ¯æ€§è§†è§‰æ˜ å°„ç³»ç»Ÿï¼Œå®ƒé€šè¿‡å…¨å±€å§¿æ€å›¾ä¼˜åŒ–ä¸­çš„surfel-to-surfelçº¦æŸç¡®ä¿é«˜ç²¾åº¦åœ°å›¾ä¸€è‡´æ€§ã€‚ä¸ºäº†å¤„ç†ç¨€ç–æ•°æ®ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿé‡‡ç”¨åƒç´ æ„ŸçŸ¥é«˜æ–¯åˆå§‹åŒ–ç­–ç•¥è¿›è¡Œé«˜æ•ˆè¡¨ç¤ºï¼Œå¹¶ä½¿ç”¨æœ‰ç•Œsigmoidçº¦æŸæ¥é˜²æ­¢é«˜æ–¯å¢é•¿å¤±æ§ã€‚åœ¨å…¬å…±æ•°æ®é›†å’Œæˆ‘ä»¬è‡ªå·±çš„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨æ¸²æŸ“è´¨é‡å’Œåœ°å›¾æ„å»ºæ•ˆç‡æ–¹é¢è¶…è¿‡äº†ç°æœ‰çš„3DGS SLAMç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23273v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºLiDARçš„åœ¨çº¿ä¸‰ç»´æ˜ å°„ç³»ç»ŸGSFusionï¼Œè¯¥ç³»ç»Ÿç»“åˆäº†LiDARã€æƒ¯æ€§æµ‹é‡å’Œè§†è§‰ä¼ æ„Ÿå™¨æŠ€æœ¯ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨å…‰ç…§å’Œçº¹ç†ä¸ä½³ç¯å¢ƒä¸‹å­˜åœ¨çš„é—®é¢˜ï¼ŒåŒæ—¶é€šè¿‡surfelså…¨å±€å§¿æ€ä¼˜åŒ–ç®—æ³•ä¿è¯äº†é«˜è´¨é‡çš„ä¸‰ç»´é‡å»ºç²¾åº¦å’Œåœ°å›¾ä¸€è‡´æ€§ã€‚é‡‡ç”¨åƒç´ æ„ŸçŸ¥é«˜æ–¯åˆå§‹åŒ–ç­–ç•¥åº”å¯¹ç¨€ç–æ•°æ®æŒ‘æˆ˜ï¼Œå¹¶åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åº”ç”¨è¾¹ç•Œsigmoidçº¦æŸä»¥é¿å…é«˜æ–¯å¢é•¿å¤±æ§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨æ¸²æŸ“è´¨é‡å’Œåœ°å›¾æ„å»ºæ•ˆç‡æ–¹é¢ä¼˜äºç°æœ‰çš„åŸºäº3DGSçš„SLAMç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä¼ ç»Ÿçš„åŸºäºç›¸æœºä¼ æ„Ÿå™¨çš„RGB-Dæ–¹æ³•åœ¨å…‰ç…§å’Œçº¹ç†ä¸ä½³ç¯å¢ƒä¸‹å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>LiDARä½œä¸ºä¸€ç§æ›¿ä»£æ–¹æ³•å…·æœ‰ç¨³å¥æ€§ï¼Œä½†ä¸3DGSé›†æˆå¸¦æ¥å…¨å±€å¯¹é½æŒ‘æˆ˜å’Œç¨€ç–æ•°æ®å¤„ç†é—®é¢˜ã€‚</li>
<li>GSFusionç³»ç»Ÿç»“åˆLiDARã€æƒ¯æ€§æµ‹é‡å’Œè§†è§‰ä¼ æ„Ÿå™¨æŠ€æœ¯ï¼Œå®ç°äº†é«˜ç²¾åº¦åœ°å›¾ä¸€è‡´æ€§ã€‚</li>
<li>GSFusioné€šè¿‡surfelså…¨å±€å§¿æ€ä¼˜åŒ–ç®—æ³•è§£å†³äº†ç²¾å‡†æ€§é—®é¢˜ã€‚</li>
<li>ç³»ç»Ÿé‡‡ç”¨åƒç´ æ„ŸçŸ¥é«˜æ–¯åˆå§‹åŒ–ç­–ç•¥åº”å¯¹ç¨€ç–æ•°æ®æŒ‘æˆ˜ã€‚</li>
<li>é‡‡ç”¨è¾¹ç•Œsigmoidçº¦æŸé˜²æ­¢é«˜æ–¯å¢é•¿å¤±æ§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23273">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f7acb216c6d6b0cfa1b4805bfcac0993.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5b8d880ba25496f2706dad94cfc18bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-befbd51fb45746ec23c183dd7b1cac9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f58c15ce61dfcbdff17835e1e5057829.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3e244bb4207fa88a0a6ee6066318221a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-89efbbfc154dbcd64635bf5647df6e7b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Robust-and-Efficient-3D-Gaussian-Splatting-for-Urban-Scene-Reconstruction"><a href="#Robust-and-Efficient-3D-Gaussian-Splatting-for-Urban-Scene-Reconstruction" class="headerlink" title="Robust and Efficient 3D Gaussian Splatting for Urban Scene   Reconstruction"></a>Robust and Efficient 3D Gaussian Splatting for Urban Scene   Reconstruction</h2><p><strong>Authors:Zhensheng Yuan, Haozhi Huang, Zhen Xiong, Di Wang, Guanghua Yang</strong></p>
<p>We present a framework that enables fast reconstruction and real-time rendering of urban-scale scenes while maintaining robustness against appearance variations across multi-view captures. Our approach begins with scene partitioning for parallel training, employing a visibility-based image selection strategy to optimize training efficiency. A controllable level-of-detail (LOD) strategy explicitly regulates Gaussian density under a user-defined budget, enabling efficient training and rendering while maintaining high visual fidelity. The appearance transformation module mitigates the negative effects of appearance inconsistencies across images while enabling flexible adjustments. Additionally, we utilize enhancement modules, such as depth regularization, scale regularization, and antialiasing, to improve reconstruction fidelity. Experimental results demonstrate that our method effectively reconstructs urban-scale scenes and outperforms previous approaches in both efficiency and quality. The source code is available at: <a target="_blank" rel="noopener" href="https://yzslab.github.io/REUrbanGS">https://yzslab.github.io/REUrbanGS</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å¤šè§†è§’æ•è·ä¸­å¿«é€Ÿé‡å»ºå’Œå®æ—¶æ¸²æŸ“åŸå¸‚åœºæ™¯ï¼ŒåŒæ—¶ä¿æŒå¯¹å„ç§å¤–è§‚å˜åŒ–çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»åœºæ™¯åˆ†åŒºå¹¶è¡Œè®­ç»ƒå¼€å§‹ï¼Œé‡‡ç”¨åŸºäºå¯è§æ€§çš„å›¾åƒé€‰æ‹©ç­–ç•¥æ¥ä¼˜åŒ–è®­ç»ƒæ•ˆç‡ã€‚å¯æ§çš„å±‚æ¬¡ç»†èŠ‚ï¼ˆLODï¼‰ç­–ç•¥åœ¨ç”¨æˆ·å®šä¹‰çš„é¢„ç®—ä¸‹æ˜ç¡®è°ƒèŠ‚é«˜æ–¯å¯†åº¦ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒé«˜è§†è§‰ä¿çœŸåº¦çš„åŒæ—¶å®ç°é«˜æ•ˆçš„è®­ç»ƒå’Œæ¸²æŸ“ã€‚å¤–è§‚å˜æ¢æ¨¡å—å‡è½»äº†å›¾åƒé—´å¤–è§‚ä¸ä¸€è‡´çš„è´Ÿé¢å½±å“ï¼ŒåŒæ—¶å®ç°çµæ´»è°ƒæ•´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨å¢å¼ºæ¨¡å—ï¼Œå¦‚æ·±åº¦æ­£åˆ™åŒ–ã€å°ºåº¦æ­£åˆ™åŒ–å’ŒæŠ—é”¯é½¿ï¼Œä»¥æé«˜é‡å»ºçš„ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é‡å»ºåŸå¸‚åœºæ™¯æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œåœ¨æ•ˆç‡å’Œè´¨é‡æ–¹é¢éƒ½ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚æºä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://yzslab.github.io/REUrbanGS%E3%80%82">https://yzslab.github.io/REUrbanGSã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23006v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ¡†æ¶æ”¯æŒåŸå¸‚çº§åœºæ™¯çš„å¿«é€Ÿé‡å»ºå’Œå®æ—¶æ¸²æŸ“ï¼Œèƒ½åº”å¯¹å¤šè§†è§’æ•æ‰ä¸­çš„å¤–è§‚å˜åŒ–ã€‚é‡‡ç”¨åŸºäºå¯è§æ€§çš„å›¾åƒé€‰æ‹©ç­–ç•¥è¿›è¡Œåœºæ™¯åˆ†åŒºå¹¶è¡Œè®­ç»ƒï¼Œä¼˜åŒ–è®­ç»ƒæ•ˆç‡ã€‚å¯æ§çš„ç»†èŠ‚å±‚æ¬¡ï¼ˆLODï¼‰ç­–ç•¥åœ¨ç”¨æˆ·å®šä¹‰é¢„ç®—ä¸‹è°ƒæ§é«˜æ–¯å¯†åº¦ï¼Œå®ç°é«˜æ•ˆè®­ç»ƒå’Œæ¸²æŸ“çš„åŒæ—¶ä¿æŒé«˜è§†è§‰ä¿çœŸåº¦ã€‚å¤–è§‚å˜æ¢æ¨¡å—å‡è½»äº†å›¾åƒé—´å¤–è§‚ä¸ä¸€è‡´çš„è´Ÿé¢å½±å“ï¼Œå¹¶å®ç°äº†çµæ´»è°ƒæ•´ã€‚æ­¤å¤–ï¼Œè¿˜ä½¿ç”¨æ·±åº¦æ­£åˆ™åŒ–ã€å°ºåº¦æ­£åˆ™åŒ–å’ŒæŠ—é”¯é½¿ç­‰å¢å¼ºæ¨¡å—ï¼Œæé«˜é‡å»ºçš„ä¿çœŸåº¦ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸå¸‚çº§åœºæ™¯çš„é‡å»ºä¸­è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨æ•ˆç‡å’Œè´¨é‡ä¸Šå‡è¶…è¿‡ä¹‹å‰çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¡†æ¶æ”¯æŒåŸå¸‚çº§åœºæ™¯çš„å¿«é€Ÿé‡å»ºå’Œå®æ—¶æ¸²æŸ“ã€‚</li>
<li>é‡‡ç”¨åœºæ™¯åˆ†åŒºå¹¶è¡Œè®­ç»ƒå’ŒåŸºäºå¯è§æ€§çš„å›¾åƒé€‰æ‹©ç­–ç•¥ä¼˜åŒ–è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>ç»†èŠ‚å±‚æ¬¡ï¼ˆLODï¼‰ç­–ç•¥è°ƒæ§é«˜æ–¯å¯†åº¦ï¼Œå®ç°é«˜æ•ˆè®­ç»ƒå’Œæ¸²æŸ“ã€‚</li>
<li>å¤–è§‚å˜æ¢æ¨¡å—åº”å¯¹å¤šè§†è§’æ•æ‰ä¸­çš„å¤–è§‚å˜åŒ–ã€‚</li>
<li>å¢å¼ºæ¨¡å—å¦‚æ·±åº¦æ­£åˆ™åŒ–ã€å°ºåº¦æ­£åˆ™åŒ–å’ŒæŠ—é”¯é½¿æé«˜é‡å»ºçš„ä¿çœŸåº¦ã€‚</li>
<li>æ¡†æ¶æ€§èƒ½ä¼˜è¶Šï¼Œåœ¨é‡å»ºæ•ˆç‡å’Œè´¨é‡ä¸Šè¶…è¶Šä¹‹å‰çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4125890218d476f06350f1155194a7c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1db203a777ffb9ef2cc9344c48e4fb7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20d54897e4d3000957d4ccd78aef5c59.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="DISCOVERSE-Efficient-Robot-Simulation-in-Complex-High-Fidelity-Environments"><a href="#DISCOVERSE-Efficient-Robot-Simulation-in-Complex-High-Fidelity-Environments" class="headerlink" title="DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity   Environments"></a>DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity   Environments</h2><p><strong>Authors:Yufei Jia, Guangyu Wang, Yuhang Dong, Junzhe Wu, Yupei Zeng, Haonan Lin, Zifan Wang, Haizhou Ge, Weibin Gu, Kairui Ding, Zike Yan, Yunjie Cheng, Yue Li, Ziming Wang, Chuxuan Li, Wei Sui, Lu Shi, Guanzhong Tian, Ruqi Huang, Guyue Zhou</strong></p>
<p>We present the first unified, modular, open-source 3DGS-based simulation framework for Real2Sim2Real robot learning. It features a holistic Real2Sim pipeline that synthesizes hyper-realistic geometry and appearance of complex real-world scenarios, paving the way for analyzing and bridging the Sim2Real gap. Powered by Gaussian Splatting and MuJoCo, Discoverse enables massively parallel simulation of multiple sensor modalities and accurate physics, with inclusive supports for existing 3D assets, robot models, and ROS plugins, empowering large-scale robot learning and complex robotic benchmarks. Through extensive experiments on imitation learning, Discoverse demonstrates state-of-the-art zero-shot Sim2Real transfer performance compared to existing simulators. For code and demos: <a target="_blank" rel="noopener" href="https://air-discoverse.github.io/">https://air-discoverse.github.io/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†é¦–ä¸ªç»Ÿä¸€ã€æ¨¡å—åŒ–ã€å¼€æºçš„åŸºäº3DGSçš„Real2Sim2Realæœºå™¨äººå­¦ä¹ ä»¿çœŸæ¡†æ¶ã€‚å®ƒå…·å¤‡å…¨é¢çš„Real2Simç®¡é“ï¼Œèƒ½å¤Ÿåˆæˆè¶…é€¼çœŸçš„å¤æ‚ç°å®ä¸–ç•Œåœºæ™¯çš„å‡ ä½•å’Œå¤–è§‚ï¼Œä¸ºåˆ†æå’Œå¼¥åˆSim2Realå·®è·é“ºå¹³äº†é“è·¯ã€‚å€ŸåŠ©é«˜æ–¯Splattingå’ŒMuJoCoï¼ŒDiscoverseèƒ½å¤Ÿå®ç°å¤šç§ä¼ æ„Ÿå™¨æ¨¡å¼çš„å¤§è§„æ¨¡å¹¶è¡Œä»¿çœŸå’Œç²¾ç¡®çš„ç‰©ç†æ¨¡æ‹Ÿï¼ŒåŒæ—¶æ”¯æŒç°æœ‰çš„3Dèµ„äº§ã€æœºå™¨äººæ¨¡å‹å’ŒROSæ’ä»¶ï¼Œä¸ºå¤§è§„æ¨¡çš„æœºå™¨äººå­¦ä¹ å’Œå¤æ‚çš„æœºå™¨äººåŸºå‡†æµ‹è¯•æä¾›æ”¯æŒã€‚é€šè¿‡æ¨¡ä»¿å­¦ä¹ çš„å¹¿æ³›å®éªŒï¼ŒDiscoverseå±•ç°å‡ºä¸ç°æœ‰æ¨¡æ‹Ÿå™¨ç›¸æ¯”çš„å“è¶Šé›¶å°„å‡»Sim2Realä¼ è¾“æ€§èƒ½ã€‚æœ‰å…³ä»£ç å’Œæ¼”ç¤ºï¼Œè¯·è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://air-discoverse.github.io/%E3%80%82">https://air-discoverse.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21981v1">PDF</a> 8pages, IROS2025 (Camera Ready)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªåŸºäº3DGSçš„ç»Ÿä¸€ã€æ¨¡å—åŒ–ã€å¼€æºçš„ä»¿çœŸæ¡†æ¶Discoverseï¼Œç”¨äºReal2Sim2Realæœºå™¨äººå­¦ä¹ ã€‚è¯¥æ¡†æ¶å…·å¤‡å®Œæ•´çš„Real2Simç®¡é“ï¼Œèƒ½åˆæˆå¤æ‚çœŸå®åœºæ™¯çš„è¶…é«˜ä»¿çœŸå‡ ä½•ä¸å¤–è§‚ï¼Œä¸ºåˆ†æå¹¶ç¼©å°Sim2Realå·®è·é“ºå¹³äº†é“è·¯ã€‚å€ŸåŠ©é«˜æ–¯æ‹¼è´´å’ŒMuJoCoï¼ŒDiscoverseæ”¯æŒå¤šç§ä¼ æ„Ÿå™¨æ¨¡å¼çš„å¹¶è¡Œä»¿çœŸå’Œç²¾ç¡®ç‰©ç†è®¡ç®—ï¼Œå¹¶æ”¯æŒç°æœ‰3Dèµ„äº§ã€æœºå™¨äººæ¨¡å‹å’ŒROSæ’ä»¶ï¼Œä¸ºå¤§è§„æ¨¡æœºå™¨äººå­¦ä¹ å’Œå¤æ‚æœºå™¨äººåŸºå‡†æµ‹è¯•æä¾›äº†æ”¯æŒã€‚é€šè¿‡æ¨¡ä»¿å­¦ä¹ çš„å¹¿æ³›å®éªŒï¼ŒDiscoverseå±•ç°å‡ºç›¸è¾ƒäºå…¶ä»–æ¨¡æ‹Ÿå™¨çš„å“è¶Šé›¶å°„å‡»Sim2Realè½¬ç§»æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäº3DGSçš„ä»¿çœŸæ¡†æ¶ï¼šDiscoverseä¸ºReal2Sim2Realæœºå™¨äººå­¦ä¹ æä¾›äº†é¦–ä¸ªç»Ÿä¸€ã€æ¨¡å—åŒ–ã€å¼€æºçš„ä»¿çœŸæ¡†æ¶ã€‚</li>
<li>Real2Simç®¡é“ï¼šè¯¥æ¡†æ¶å…·å¤‡å®Œæ•´çš„Real2Simç®¡é“ï¼Œèƒ½å¤Ÿåˆæˆé«˜åº¦é€¼çœŸçš„å‡ ä½•å’Œå¤–è§‚ï¼Œæœ‰åŠ©äºç¼©å°Sim2Realå·®è·ã€‚</li>
<li>æ”¯æŒå¤šç§ä¼ æ„Ÿå™¨æ¨¡å¼ï¼šDiscoverseæ”¯æŒå¤šç§ä¼ æ„Ÿå™¨æ¨¡å¼çš„å¹¶è¡Œä»¿çœŸã€‚</li>
<li>ç²¾ç¡®ç‰©ç†è®¡ç®—ï¼šå€ŸåŠ©é«˜æ–¯æ‹¼è´´å’ŒMuJoCoï¼ŒDiscoverseå®ç°äº†ç²¾ç¡®çš„ç‰©ç†è®¡ç®—ã€‚</li>
<li>å¹¿æ³›çš„æ”¯æŒæ€§ï¼šDiscoverseæ”¯æŒç°æœ‰3Dèµ„äº§ã€æœºå™¨äººæ¨¡å‹å’ŒROSæ’ä»¶ï¼Œä¸ºå¤§è§„æ¨¡æœºå™¨äººå­¦ä¹ å’ŒåŸºå‡†æµ‹è¯•æä¾›äº†æ”¯æŒã€‚</li>
<li>å“è¶Šæ€§èƒ½è¡¨ç°ï¼šåœ¨æ¨¡ä»¿å­¦ä¹ å®éªŒä¸­ï¼ŒDiscoverseå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½è¡¨ç°ï¼Œå°¤å…¶æ˜¯é›¶å°„å‡»Sim2Realè½¬ç§»æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21981">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-bc72e3353bde17d1437b78a901a296cb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a6dd341fd0e24c14e038e0ac6987ae5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-164ee04c5f2137bf5f32f3386d0012ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6388b8a6cdeb972d9f70bce8a860e12.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bba66a930b0d9c926b427e7719081a24.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59e78eace38459789503c87d616e39f4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6e7e290f0a2cb46afe0a4082cac4a34f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="RaGS-Unleashing-3D-Gaussian-Splatting-from-4D-Radar-and-Monocular-Cues-for-3D-Object-Detection"><a href="#RaGS-Unleashing-3D-Gaussian-Splatting-from-4D-Radar-and-Monocular-Cues-for-3D-Object-Detection" class="headerlink" title="RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues   for 3D Object Detection"></a>RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues   for 3D Object Detection</h2><p><strong>Authors:Xiaokai Bai, Chenxu Zhou, Lianqing Zheng, Si-Yuan Cao, Jianan Liu, Xiaohan Zhang, Zhengzhuang Zhang, Hui-liang Shen</strong></p>
<p>4D millimeter-wave radar has emerged as a promising sensor for autonomous driving, but effective 3D object detection from both 4D radar and monocular images remains a challenge. Existing fusion approaches typically rely on either instance-based proposals or dense BEV grids, which either lack holistic scene understanding or are limited by rigid grid structures. To address these, we propose RaGS, the first framework to leverage 3D Gaussian Splatting (GS) as representation for fusing 4D radar and monocular cues in 3D object detection. 3D GS naturally suits 3D object detection by modeling the scene as a field of Gaussians, dynamically allocating resources on foreground objects and providing a flexible, resource-efficient solution. RaGS uses a cascaded pipeline to construct and refine the Gaussian field. It starts with the Frustum-based Localization Initiation (FLI), which unprojects foreground pixels to initialize coarse 3D Gaussians positions. Then, the Iterative Multimodal Aggregation (IMA) fuses semantics and geometry, refining the limited Gaussians to the regions of interest. Finally, the Multi-level Gaussian Fusion (MGF) renders the Gaussians into multi-level BEV features for 3D object detection. By dynamically focusing on sparse objects within scenes, RaGS enable object concentrating while offering comprehensive scene perception. Extensive experiments on View-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate its state-of-the-art performance. Code will be released. </p>
<blockquote>
<p>éšç€è‡ªåŠ¨é©¾é©¶çš„å…´èµ·ï¼Œ4Dæ¯«ç±³æ³¢é›·è¾¾å·²ç»æˆä¸ºäº†ä¸€ä¸ªæœ‰å‰æ™¯çš„ä¼ æ„Ÿå™¨ï¼Œä½†å¦‚ä½•ä»é›·è¾¾çš„é›·è¾¾æ•°æ®å’Œå•ç›®å›¾åƒè¿›è¡Œé«˜æ•ˆ3Dç‰©ä½“æ£€æµ‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„èåˆæ–¹æ³•å¤§å¤šä¾èµ–äºå®ä¾‹åŒ–çš„æè®®æˆ–å¯†é›†çš„BEVç½‘æ ¼ï¼Œå®ƒä»¬è¦ä¹ˆç¼ºä¹å…¨å±€åœºæ™¯ç†è§£ï¼Œè¦ä¹ˆå—é™äºåˆšæ€§çš„ç½‘æ ¼ç»“æ„ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RaGSæ¡†æ¶ï¼Œé¦–æ¬¡ä½¿ç”¨ä¸‰ç»´é«˜æ–¯å¹³é“ºï¼ˆGSï¼‰ä½œä¸ºèåˆå››ç»´é›·è¾¾å’Œå•ç›®è§†è§‰æç¤ºçš„ä»£è¡¨ï¼Œç”¨äºä¸‰ç»´ç‰©ä½“æ£€æµ‹ã€‚ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºæ³•è‡ªç„¶åœ°é€‚åˆä¸‰ç»´ç‰©ä½“æ£€æµ‹ï¼Œå®ƒå°†åœºæ™¯å»ºæ¨¡ä¸ºé«˜æ–¯åœºï¼ŒåŠ¨æ€åˆ†é…èµ„æºäºå‰æ™¯ç‰©ä½“ï¼Œå¹¶æä¾›çµæ´»ã€èµ„æºé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚RaGSä½¿ç”¨çº§è”ç®¡é“æ¥æ„å»ºå’Œç»†åŒ–é«˜æ–¯åœºã€‚å®ƒä»¥åŸºäºè§†é”¥çš„å®šä½åˆå§‹åŒ–ï¼ˆFLIï¼‰å¼€å§‹ï¼Œå°†å‰æ™¯åƒç´ åæŠ•å½±ä»¥åˆå§‹åŒ–ç²—ç•¥çš„ä¸‰ç»´é«˜æ–¯ä½ç½®ã€‚ç„¶åï¼Œè¿­ä»£å¤šæ¨¡æ€èšåˆï¼ˆIMAï¼‰èåˆè¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯ï¼Œå°†æœ‰é™çš„é«˜æ–¯ç»†åˆ†åˆ°æ„Ÿå…´è¶£åŒºåŸŸã€‚æœ€åï¼Œå¤šçº§é«˜æ–¯èåˆï¼ˆMGFï¼‰å°†é«˜æ–¯æ¸²æŸ“æˆå¤šçº§BEVç‰¹å¾ç”¨äºä¸‰ç»´ç‰©ä½“æ£€æµ‹ã€‚é€šè¿‡åŠ¨æ€å…³æ³¨åœºæ™¯ä¸­çš„ç¨€ç–ç‰©ä½“ï¼ŒRaGSèƒ½å¤Ÿåœ¨ç‰©ä½“é›†ä¸­æ—¶æä¾›å…¨é¢çš„åœºæ™¯æ„ŸçŸ¥ã€‚åœ¨View-of-Delftã€TJ4DRadSetå’ŒOmniHD-ScenesåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†å…¶å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19856v2">PDF</a> 9 pages, 6 figures, conference</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºRaGSçš„æ–°æ¡†æ¶ï¼Œç”¨äºåœ¨è‡ªä¸»é©¾é©¶ä¸­çš„å››ç»´é›·è¾¾å’Œå•ç›®å›¾åƒèåˆçš„ä¸‰ç»´å¯¹è±¡æ£€æµ‹ã€‚å®ƒé€šè¿‡é‡‡ç”¨ä¸‰ç»´é«˜æ–¯æ¨¡æ¿ä½œä¸ºåœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œå¹¶é€šè¿‡ä¸€ç³»åˆ—æ¨¡å—æ¥æ„å»ºå’Œä¼˜åŒ–é«˜æ–¯åœºå®ç°ç²¾å‡†çš„ä¸‰ç»´å¯¹è±¡æ£€æµ‹ã€‚RaGSçš„åˆ›æ–°ç‚¹åœ¨äºå…¶å¯¹å‰æ™¯å¯¹è±¡è¿›è¡ŒåŠ¨æ€èµ„æºåˆ†é…å¹¶æä¾›çµæ´»ã€èµ„æºé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œæœ€ç»ˆé€šè¿‡å¤šå±‚æ¬¡é«˜æ–¯èåˆï¼ˆMGFï¼‰å®ç°å…ˆè¿›æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>4Dé›·è¾¾å·²æˆä¸ºè‡ªä¸»é©¾é©¶ä¸­æœ‰å‰æ™¯çš„ä¼ æ„Ÿå™¨ï¼Œä½†èåˆé›·è¾¾å’Œå›¾åƒå®ç°ä¸‰ç»´ç‰©ä½“æ£€æµ‹ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç›®å‰èåˆæ–¹æ³•ä¸»è¦ä¾èµ–äºå®ä¾‹æè®®æˆ–å¯†é›†BEVç½‘æ ¼ï¼Œå­˜åœ¨å¯¹åœºæ™¯ç†è§£ä¸å…¨é¢æˆ–å—ç½‘æ ¼ç»“æ„é™åˆ¶çš„é—®é¢˜ã€‚</li>
<li>RaGSæ¡†æ¶é¦–æ¬¡åˆ©ç”¨ä¸‰ç»´é«˜æ–¯æ¨¡æ¿ï¼ˆGSï¼‰ä½œä¸ºè¡¨ç¤ºæ–¹æ³•ï¼Œèåˆå››ç»´é›·è¾¾å’Œå•ç›®å›¾åƒè¿›è¡Œä¸‰ç»´å¯¹è±¡æ£€æµ‹ã€‚</li>
<li>ä¸‰ç»´é«˜æ–¯æ¨¡æ¿è‡ªç„¶é€‚ç”¨äºä¸‰ç»´å¯¹è±¡æ£€æµ‹ï¼Œé€šè¿‡åŠ¨æ€èµ„æºåˆ†é…å¯¹å‰æ™¯å¯¹è±¡è¿›è¡Œå»ºæ¨¡ï¼Œæä¾›çµæ´»ä¸”èµ„æºé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>RaGSä½¿ç”¨çº§è”ç®¡é“æ„å»ºå’Œä¼˜åŒ–é«˜æ–¯åœºï¼ŒåŒ…æ‹¬åŸºäºè§†é”¥çš„å®šä½å¯åŠ¨ï¼ˆFLIï¼‰ã€è¿­ä»£å¤šæ¨¡å¼èšåˆï¼ˆIMAï¼‰å’Œå¤šå±‚æ¬¡é«˜æ–¯èåˆï¼ˆMGFï¼‰ã€‚</li>
<li>é€šè¿‡èšç„¦åœºæ™¯ä¸­çš„ç¨€ç–å¯¹è±¡ï¼ŒRaGSå®ç°äº†å¯¹è±¡é›†ä¸­å¹¶æä¾›äº†å…¨é¢çš„åœºæ™¯æ„ŸçŸ¥ã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRaGSè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19856">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0edc511009429e77c152020ee011a982.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-557174558a450f85c8820260f0f9ac55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d093c386ad98b81e155e79307803ffa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b7884256c56c8ff35df1bf5e8575871.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acbab909692d1066dfa6f86b11ee8862.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GS-Occ3D-Scaling-Vision-only-Occupancy-Reconstruction-for-Autonomous-Driving-with-Gaussian-Splatting"><a href="#GS-Occ3D-Scaling-Vision-only-Occupancy-Reconstruction-for-Autonomous-Driving-with-Gaussian-Splatting" class="headerlink" title="GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous   Driving with Gaussian Splatting"></a>GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous   Driving with Gaussian Splatting</h2><p><strong>Authors:Baijun Ye, Minghui Qin, Saining Zhang, Moonjun Gong, Shaoting Zhu, Zebang Shen, Luan Zhang, Lu Zhang, Hao Zhao, Hang Zhao</strong></p>
<p>Occupancy is crucial for autonomous driving, providing essential geometric priors for perception and planning. However, existing methods predominantly rely on LiDAR-based occupancy annotations, which limits scalability and prevents leveraging vast amounts of potential crowdsourced data for auto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only framework that directly reconstructs occupancy. Vision-only occupancy reconstruction poses significant challenges due to sparse viewpoints, dynamic scene elements, severe occlusions, and long-horizon motion. Existing vision-based methods primarily rely on mesh representation, which suffer from incomplete geometry and additional post-processing, limiting scalability. To overcome these issues, GS-Occ3D optimizes an explicit occupancy representation using an Octree-based Gaussian Surfel formulation, ensuring efficiency and scalability. Additionally, we decompose scenes into static background, ground, and dynamic objects, enabling tailored modeling strategies: (1) Ground is explicitly reconstructed as a dominant structural element, significantly improving large-area consistency; (2) Dynamic vehicles are separately modeled to better capture motion-related occupancy patterns. Extensive experiments on the Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry reconstruction results. By curating vision-only binary occupancy labels from diverse urban scenes, we show their effectiveness for downstream occupancy models on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes. It highlights the potential of large-scale vision-based occupancy reconstruction as a new paradigm for scalable auto-labeling. Project Page: <a target="_blank" rel="noopener" href="https://gs-occ3d.github.io/">https://gs-occ3d.github.io/</a> </p>
<blockquote>
<p>å ç”¨ä¿¡æ¯å¯¹äºè‡ªåŠ¨é©¾é©¶è‡³å…³é‡è¦ï¼Œå®ƒä¸ºæ„ŸçŸ¥å’Œè§„åˆ’æä¾›äº†å¿…è¦çš„å‡ ä½•å…ˆéªŒä¿¡æ¯ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºåŸºäºæ¿€å…‰é›·è¾¾çš„å ç”¨æ ‡æ³¨ï¼Œè¿™é™åˆ¶äº†å¯æ‰©å±•æ€§ï¼Œå¹¶é˜»æ­¢äº†åˆ©ç”¨å¤§é‡æ½œåœ¨çš„ä¼—åŒ…æ•°æ®è¿›è¡Œè‡ªåŠ¨æ ‡æ³¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GS-Occ3Dï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„ä»…è§†è§‰æ¡†æ¶ï¼Œå¯ç›´æ¥é‡å»ºå ç”¨ä¿¡æ¯ã€‚ä»…ä½¿ç”¨è§†è§‰çš„å ç”¨ä¿¡æ¯é‡å»ºç”±äºç¨€ç–çš„è§†ç‚¹ã€åŠ¨æ€åœºæ™¯å…ƒç´ ã€ä¸¥é‡çš„é®æŒ¡å’Œé•¿è·ç¦»è¿åŠ¨è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºäºè§†è§‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºç½‘æ ¼è¡¨ç¤ºï¼Œè¿™å¯¼è‡´äº†å‡ ä½•ä¸å®Œæ•´æ€§å’Œé¢å¤–çš„åå¤„ç†ï¼Œä»è€Œé™åˆ¶äº†å¯æ‰©å±•æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼ŒGS-Occ3Dä½¿ç”¨åŸºäºå…«å‰æ ‘çš„é«˜æ–¯Surfelè¡¨ç¤ºæ³•ä¼˜åŒ–æ˜ç¡®çš„å ç”¨è¡¨ç¤ºï¼Œç¡®ä¿æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†åœºæ™¯åˆ†è§£ä¸ºé™æ€èƒŒæ™¯ã€åœ°é¢å’ŒåŠ¨æ€ç‰©ä½“ï¼Œä»¥å®ç°å®šåˆ¶å»ºæ¨¡ç­–ç•¥ï¼šï¼ˆ1ï¼‰åœ°é¢è¢«æ˜ç¡®é‡å»ºä¸ºä¸€ä¸ªä¸»è¦ç»“æ„å…ƒç´ ï¼Œè¿™æ˜¾è‘—æé«˜äº†å¤§é¢ç§¯çš„ä¸€è‡´æ€§ï¼›ï¼ˆ2ï¼‰åŠ¨æ€è½¦è¾†è¢«å•ç‹¬å»ºæ¨¡ï¼Œä»¥æ›´å¥½åœ°æ•æ‰ä¸è¿åŠ¨ç›¸å…³çš„å ç”¨æ¨¡å¼ã€‚åœ¨Waymoæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒGS-Occ3Då®ç°äº†æœ€å…ˆè¿›çš„å‡ ä½•é‡å»ºç»“æœã€‚æˆ‘ä»¬é€šè¿‡ä»å„ç§åŸå¸‚åœºæ™¯ä¸­æ•´ç†ä»…ä½¿ç”¨è§†è§‰çš„äºŒè¿›åˆ¶å ç”¨æ ‡ç­¾ï¼Œå±•ç¤ºäº†å®ƒä»¬åœ¨Occ3D-Waymoä¸Šçš„ä¸‹æ¸¸å ç”¨æ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠåœ¨Occ3D-nuScenesä¸Šçš„é›¶æ ·æœ¬æ³›åŒ–ä¼˜åŠ¿ã€‚å®ƒçªå‡ºäº†å¤§è§„æ¨¡åŸºäºè§†è§‰çš„å ç”¨ä¿¡æ¯é‡å»ºä½œä¸ºæ–°çš„å¯æ‰©å±•è‡ªåŠ¨æ ‡æ³¨èŒƒå¼çš„æ½œåŠ›ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://gs-occ3d.github.io/">https://gs-occ3d.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19451v2">PDF</a> ICCV 2025. Project Page: <a target="_blank" rel="noopener" href="https://gs-occ3d.github.io/">https://gs-occ3d.github.io/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºGS-Occ3Dçš„ä»…è§†è§‰çš„å æœ‰ç‡é‡å»ºæ¡†æ¶ï¼Œç”¨äºç›´æ¥é‡å»ºè‡ªä¸»é©¾é©¶ä¸­çš„å æœ‰ç‡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºOctreeçš„Gaussian Surfelå…¬å¼è¿›è¡Œæ˜ç¡®çš„å æœ‰ç‡è¡¨ç¤ºï¼Œç¡®ä¿æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚é€šè¿‡åˆ†è§£åœºæ™¯ä¸ºé™æ€èƒŒæ™¯ã€åœ°é¢å’ŒåŠ¨æ€ç‰©ä½“ï¼Œå®ç°æœ‰é’ˆå¯¹æ€§çš„å»ºæ¨¡ç­–ç•¥ï¼Œå¹¶åœ¨Waymoæ•°æ®é›†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„å‡ ä½•é‡å»ºç»“æœã€‚æ­¤å¤–ï¼Œä»å¤šæ ·çš„åŸå¸‚åœºæ™¯ä¸­æå–ä»…è§†è§‰çš„äºŒå…ƒå æœ‰ç‡æ ‡ç­¾ï¼Œå±•ç¤ºå…¶åœ¨ä¸‹æ¸¸å æœ‰ç‡æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å¼ºè°ƒå¤§è§„æ¨¡è§†è§‰åŸºç¡€å æœ‰ç‡é‡å»ºä½œä¸ºå¯ä¼¸ç¼©è‡ªåŠ¨æ ‡æ³¨çš„æ–°èŒƒå¼æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GS-Occ3Dæ˜¯ä¸€ä¸ªä»…è§†è§‰çš„å æœ‰ç‡é‡å»ºæ¡†æ¶ï¼Œå¯ç›´æ¥é‡å»ºè‡ªä¸»é©¾é©¶ä¸­çš„å æœ‰ç‡ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºOctreeçš„Gaussian Surfelå…¬å¼ï¼Œä¼˜åŒ–æ˜ç¡®çš„å æœ‰ç‡è¡¨ç¤ºï¼Œç¡®ä¿æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>åœºæ™¯è¢«åˆ†è§£ä¸ºé™æ€èƒŒæ™¯ã€åœ°é¢å’ŒåŠ¨æ€ç‰©ä½“ï¼Œä»¥å®ç°æœ‰é’ˆå¯¹æ€§çš„å»ºæ¨¡ç­–ç•¥ã€‚</li>
<li>åœ¨Waymoæ•°æ®é›†ä¸Šï¼ŒGS-Occ3Då®ç°äº†æœ€å…ˆè¿›çš„å‡ ä½•é‡å»ºç»“æœã€‚</li>
<li>ä»å¤šæ ·çš„åŸå¸‚åœºæ™¯ä¸­æå–ä»…è§†è§‰çš„äºŒå…ƒå æœ‰ç‡æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾å¯¹äºä¸‹æ¸¸å æœ‰ç‡æ¨¡å‹éå¸¸æœ‰æ•ˆã€‚</li>
<li>GS-Occ3Dæ˜¾ç¤ºå‡ºå¤§è§„æ¨¡è§†è§‰åŸºç¡€å æœ‰ç‡é‡å»ºä½œä¸ºå¯ä¼¸ç¼©è‡ªåŠ¨æ ‡æ³¨æ–°èŒƒå¼çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19451">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87b3a67d38fd4a97e9b53de62040d0c5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5337e1b374c7dc1047f328770ee538cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9874a664ee8860ec1f69c92d5e11e58a.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MVG4D-Image-Matrix-Based-Multi-View-and-Motion-Generation-for-4D-Content-Creation-from-a-Single-Image"><a href="#MVG4D-Image-Matrix-Based-Multi-View-and-Motion-Generation-for-4D-Content-Creation-from-a-Single-Image" class="headerlink" title="MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D   Content Creation from a Single Image"></a>MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D   Content Creation from a Single Image</h2><p><strong>Authors:DongFu Yin, Xiaotian Chen, Fei Richard Yu, Xuanchen Li, Xinhao Zhang</strong></p>
<p>Advances in generative modeling have significantly enhanced digital content creation, extending from 2D images to complex 3D and 4D scenes. Despite substantial progress, producing high-fidelity and temporally consistent dynamic 4D content remains a challenge. In this paper, we propose MVG4D, a novel framework that generates dynamic 4D content from a single still image by combining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core, MVG4D employs an image matrix module that synthesizes temporally coherent and spatially diverse multi-view images, providing rich supervisory signals for downstream 3D and 4D reconstruction. These multi-view images are used to optimize a 3D Gaussian point cloud, which is further extended into the temporal domain via a lightweight deformation network. Our method effectively enhances temporal consistency, geometric fidelity, and visual realism, addressing key challenges in motion discontinuity and background degradation that affect prior 4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate that MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and time efficiency. Notably, it reduces flickering artifacts and sharpens structural details across views and time, enabling more immersive AR&#x2F;VR experiences. MVG4D sets a new direction for efficient and controllable 4D generation from minimal inputs. </p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹çš„è¿›æ­¥æå¤§æ¨åŠ¨äº†æ•°å­—å†…å®¹çš„åˆ›ä½œï¼Œä»äºŒç»´å›¾åƒæ‰©å±•åˆ°å¤æ‚çš„ä¸‰ç»´ç”šè‡³å››ç»´åœºæ™¯ã€‚å°½ç®¡å–å¾—äº†å¾ˆå¤§è¿›å±•ï¼Œä½†ç”Ÿæˆé«˜ä¿çœŸä¸”æ—¶é—´ä¸€è‡´çš„åŠ¨æ€å››ç»´å†…å®¹ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶MVG4Dï¼Œé€šè¿‡ç»“åˆå¤šè§†å›¾åˆæˆä¸å››ç»´é«˜æ–¯æ–‘ç‚¹ï¼ˆ4D GSï¼‰ï¼Œä»å•ä¸€é™æ€å›¾åƒç”ŸæˆåŠ¨æ€å››ç»´å†…å®¹ã€‚MVG4Dçš„æ ¸å¿ƒåœ¨äºé‡‡ç”¨å›¾åƒçŸ©é˜µæ¨¡å—ï¼Œåˆæˆæ—¶é—´è¿è´¯ä¸”ç©ºé—´å¤šæ ·çš„å¤šè§†å›¾å›¾åƒï¼Œä¸ºä¸‹æ¸¸çš„ä¸‰ç»´å’Œå››ç»´é‡å»ºæä¾›ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ã€‚è¿™äº›å¤šè§†å›¾å›¾åƒç”¨äºä¼˜åŒ–ä¸‰ç»´é«˜æ–¯ç‚¹äº‘ï¼Œé€šè¿‡è½»é‡çº§å˜å½¢ç½‘ç»œè¿›ä¸€æ­¥æ‰©å±•åˆ°æ—¶é—´åŸŸã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°æé«˜äº†æ—¶é—´ä¸€è‡´æ€§ã€å‡ ä½•ä¿çœŸåº¦å’Œè§†è§‰é€¼çœŸåº¦ï¼Œè§£å†³äº†å½±å“åŸºäºå››ç»´GSçš„æ–¹æ³•çš„è¿åŠ¨ä¸è¿ç»­å’ŒèƒŒæ™¯é€€åŒ–ç­‰å…³é”®æŒ‘æˆ˜ã€‚åœ¨Objaverseæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMVG4Dåœ¨CLIP-Iã€PSNRã€FVDå’Œæ—¶é—´æ•ˆç‡æ–¹é¢ä¼˜äºæœ€æ–°åŸºçº¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒå‡å°‘äº†é—ªçƒä¼ªå½±ï¼Œæé«˜äº†è·¨è§†å›¾å’Œæ—¶é—´çš„ç»“æ„ç»†èŠ‚æ¸…æ™°åº¦ï¼Œä¸ºå¢å¼ºAR&#x2F;VRä½“éªŒæä¾›äº†æ›´å¤šæ²‰æµ¸å¼ä½“éªŒã€‚MVG4Dä¸ºä»æœ€å°è¾“å…¥å®ç°é«˜æ•ˆå¯æ§çš„å››ç»´ç”Ÿæˆè®¾ç½®äº†æ–°çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18371v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºMVG4Dæ¡†æ¶ï¼Œé€šè¿‡ç»“åˆå¤šè§†è§’åˆæˆä¸å››ç»´é«˜æ–¯ç‚¹äº‘æŠ€æœ¯ï¼Œä»å•ä¸€é™æ€å›¾åƒç”ŸæˆåŠ¨æ€å››ç»´å†…å®¹ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å›¾åƒçŸ©é˜µæ¨¡å—ï¼Œåˆæˆæ—¶ç©ºè¿è´¯ä¸”è§†è§’å¤šæ ·çš„å›¾åƒï¼Œä¸ºä¸‹æ¸¸ä¸‰ç»´å’Œå››ç»´é‡å»ºæä¾›ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ã€‚é€šè¿‡ä¼˜åŒ–ä¸‰ç»´é«˜æ–¯ç‚¹äº‘ï¼Œå¹¶åœ¨æ—¶é—´åŸŸè¿›è¡Œè½»é‡åŒ–å˜å½¢ç½‘ç»œæ‰©å±•ï¼Œæœ‰æ•ˆæå‡äº†æ—¶åºä¸€è‡´æ€§ã€å‡ ä½•ç²¾åº¦å’Œè§†è§‰çœŸå®æ„Ÿã€‚åœ¨Objaverseæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMVG4Dåœ¨CLIP-Iã€PSNRã€FVDå’Œæ—¶é—´æ•ˆç‡ç­‰æ–¹é¢ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå‡å°‘äº†é—ªçƒä¼ªå½±ï¼Œæé«˜äº†ç»“æ„ç»†èŠ‚æ¸…æ™°åº¦ï¼Œä¸ºAR&#x2F;VRä½“éªŒæä¾›æ›´æ²‰æµ¸å¼ä½“éªŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MVG4Dæ¡†æ¶ç»“åˆäº†å¤šè§†è§’åˆæˆä¸å››ç»´é«˜æ–¯ç‚¹äº‘æŠ€æœ¯ï¼Œèƒ½å¤Ÿä»å•ä¸€é™æ€å›¾åƒç”ŸæˆåŠ¨æ€å››ç»´å†…å®¹ã€‚</li>
<li>å›¾åƒçŸ©é˜µæ¨¡å—ç”¨äºåˆæˆæ—¶ç©ºè¿è´¯ä¸”è§†è§’å¤šæ ·çš„å›¾åƒï¼Œä¸ºä¸‹æ¸¸ä¸‰ç»´å’Œå››ç»´é‡å»ºæä¾›ä¸°å¯Œçš„ç›‘ç£ä¿¡å·ã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–ä¸‰ç»´é«˜æ–¯ç‚¹äº‘ï¼Œå¹¶åœ¨æ—¶é—´åŸŸè¿›è¡Œè½»é‡åŒ–å˜å½¢ç½‘ç»œæ‰©å±•ï¼Œæå‡äº†æ—¶åºä¸€è‡´æ€§ã€å‡ ä½•ç²¾åº¦å’Œè§†è§‰çœŸå®æ„Ÿã€‚</li>
<li>MVG4Dè§£å†³äº†åŸºäºå››ç»´é«˜æ–¯ç‚¹äº‘æ–¹æ³•å­˜åœ¨çš„è¿åŠ¨ä¸è¿ç»­å’ŒèƒŒæ™¯é€€åŒ–ç­‰å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>åœ¨Objaverseæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMVG4Dåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒ…æ‹¬CLIP-Iã€PSNRã€FVDç­‰ã€‚</li>
<li>MVG4Då‡å°‘äº†ç”Ÿæˆå†…å®¹ä¸­çš„é—ªçƒä¼ªå½±ï¼Œæé«˜äº†ç»“æ„ç»†èŠ‚çš„æ¸…æ™°åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-419620538e551ccfda567b7a926b9c83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9705e45b591bb75895c8e2aa8cb98687.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9427d8233b1b668390f94e35ebf34fd6.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="GeoAvatar-Adaptive-Geometrical-Gaussian-Splatting-for-3D-Head-Avatar"><a href="#GeoAvatar-Adaptive-Geometrical-Gaussian-Splatting-for-3D-Head-Avatar" class="headerlink" title="GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar"></a>GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar</h2><p><strong>Authors:SeungJun Moon, Hah Min Lew, Seungeun Lee, Ji-Su Kang, Gyeong-Moon Park</strong></p>
<p>Despite recent progress in 3D head avatar generation, balancing identity preservation, i.e., reconstruction, with novel poses and expressions, i.e., animation, remains a challenge. Existing methods struggle to adapt Gaussians to varying geometrical deviations across facial regions, resulting in suboptimal quality. To address this, we propose GeoAvatar, a framework for adaptive geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation Stage (APS), an unsupervised method that segments Gaussians into rigid and flexible sets for adaptive offset regularization. Then, based on mouth anatomy and dynamics, we introduce a novel mouth structure and the part-wise deformation strategy to enhance the animation fidelity of the mouth. Finally, we propose a regularization loss for precise rigging between Gaussians and 3DMM faces. Moreover, we release DynamicFace, a video dataset with highly expressive facial motions. Extensive experiments show the superiority of GeoAvatar compared to state-of-the-art methods in reconstruction and novel animation scenarios. </p>
<blockquote>
<p>å°½ç®¡æœ€è¿‘åœ¨3Då¤´åƒç”Ÿæˆæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†åœ¨å¹³è¡¡èº«ä»½ä¿ç•™ï¼ˆå³é‡å»ºï¼‰ä¸æ–°é¢–å§¿åŠ¿å’Œè¡¨æƒ…ï¼ˆå³åŠ¨ç”»ï¼‰ä¹‹é—´ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•éš¾ä»¥é€‚åº”é¢éƒ¨å„åŒºåŸŸä¸åŒçš„å‡ ä½•åå·®ï¼Œå¯¼è‡´è´¨é‡ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GeoAvatarï¼Œä¸€ä¸ªè‡ªé€‚åº”å‡ ä½•é«˜æ–¯æ‹¼è´´æ¡†æ¶ã€‚GeoAvataråˆ©ç”¨è‡ªé€‚åº”é¢„åˆ†é…é˜¶æ®µï¼ˆAPSï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— ç›‘ç£æ–¹æ³•ï¼Œå°†é«˜æ–¯åˆ†å‰²æˆåˆšæ€§å’ŒæŸ”æ€§é›†åˆï¼Œç”¨äºè‡ªé€‚åº”åç§»æ­£åˆ™åŒ–ã€‚ç„¶åï¼ŒåŸºäºå˜´å·´ç»“æ„å’ŒåŠ¨æ€ç‰¹æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å˜´å·´ç»“æ„å’Œéƒ¨åˆ†å˜å½¢ç­–ç•¥ï¼Œä»¥æé«˜å˜´å·´çš„åŠ¨ç”»ä¿çœŸåº¦ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹é«˜æ–¯å’Œ3DMMé¢å­”ä¹‹é—´çš„ç²¾ç¡®éª¨æ¶çš„æ­£åˆ™åŒ–æŸå¤±ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘å¸ƒäº†DynamicFaceï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰é«˜åº¦è¡¨è¾¾æ€§é¢éƒ¨åŠ¨ä½œçš„è§†é¢‘æ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒGeoAvataråœ¨é‡å»ºå’Œæ–°é¢–åŠ¨ç”»åœºæ™¯ä¸­ç›¸è¾ƒäºæœ€å…ˆè¿›çš„æ–¹æ³•å…·æœ‰ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18155v1">PDF</a> ICCV 2025, Project page: <a target="_blank" rel="noopener" href="https://hahminlew.github.io/geoavatar/">https://hahminlew.github.io/geoavatar/</a></p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†åœ¨3Då¤´åƒç”Ÿæˆé¢†åŸŸä¸­çš„ä¸€é¡¹æŒ‘æˆ˜ï¼Œå³å¦‚ä½•åœ¨ä¿æŒèº«ä»½ä¸å˜çš„åŒæ—¶å®ç°æ–°çš„å§¿æ€å’Œè¡¨æƒ…ã€‚ä¸ºè§£å†³ç°æœ‰æ–¹æ³•åœ¨é¢éƒ¨å‡ ä½•å½¢å˜ä¸Šçš„ä¸è¶³ï¼Œæå‡ºGeoAvataræ¡†æ¶ï¼Œåˆ©ç”¨è‡ªé€‚åº”é¢„åˆ†é…é˜¶æ®µå’ŒåŸºäºå˜´å·´è§£å‰–ç»“æ„å’ŒåŠ¨æ€ç‰¹æ€§çš„åˆ›æ–°æŠ€æœ¯ï¼Œæå‡å¤´åƒè´¨é‡ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„åŠ¨æ€é¢éƒ¨æ•°æ®é›†DynamicFaceã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Då¤´åƒç”Ÿæˆä¸­ä¿æŒèº«ä»½ã€é‡å»ºå’ŒåŠ¨ç”»å¹³è¡¡ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥é€‚åº”é¢éƒ¨å‡ ä½•å½¢å˜ï¼Œå¯¼è‡´è´¨é‡ä¸ä½³ã€‚</li>
<li>GeoAvataræ¡†æ¶é€šè¿‡è‡ªé€‚åº”å‡ ä½•é«˜æ–¯ç‚¹åˆ†å¸ƒè§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨è‡ªé€‚åº”é¢„åˆ†é…é˜¶æ®µï¼ˆAPSï¼‰è¿›è¡Œé«˜æ–¯åˆ†å‰²ï¼Œå®ç°è‡ªé€‚åº”åç§»æ­£åˆ™åŒ–ã€‚</li>
<li>åŸºäºå˜´å·´è§£å‰–ç»“æ„å’ŒåŠ¨æ€ç‰¹æ€§å¼•å…¥æ–°å‹å˜´å·´ç»“æ„å’Œéƒ¨åˆ†å˜å½¢ç­–ç•¥ï¼Œæå‡åŠ¨ç”»é€¼çœŸåº¦ã€‚</li>
<li>å¼•å…¥æ­£åˆ™åŒ–æŸå¤±ï¼Œç²¾ç¡®è°ƒæ•´é«˜æ–¯å’Œ3DMMé¢éƒ¨ä¹‹é—´çš„å…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f17f65e62b20cd25d392adba8e0daebc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ba7e89bc4b3b9648de01db46af3d015.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4532f8b7796c6218f83f1ee0ae03a99d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de3c27123c0082f233ed6945ac6023d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4780f90f36cb275c32235c1c57d6585.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e7f942e51a4d8e6bf812dce3334525b1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="StreamME-Simplify-3D-Gaussian-Avatar-within-Live-Stream"><a href="#StreamME-Simplify-3D-Gaussian-Avatar-within-Live-Stream" class="headerlink" title="StreamME: Simplify 3D Gaussian Avatar within Live Stream"></a>StreamME: Simplify 3D Gaussian Avatar within Live Stream</h2><p><strong>Authors:Luchuan Song, Yang Zhou, Zhan Xu, Yi Zhou, Deepali Aneja, Chenliang Xu</strong></p>
<p>We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: <a target="_blank" rel="noopener" href="https://songluchuan.github.io/StreamME/">https://songluchuan.github.io/StreamME/</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†StreamMEæ–¹æ³•ï¼Œå®ƒä¸“æ³¨äºå¿«é€Ÿ3DåŒ–èº«é‡å»ºã€‚StreamMEåŒæ­¥è®°å½•å¹¶ä»å®æ—¶è§†é¢‘æµä¸­é‡å»ºå¤´éƒ¨åŒ–èº«ï¼Œæ— éœ€ä»»ä½•é¢„å…ˆç¼“å­˜çš„æ•°æ®ï¼Œä½¿å¾—é‡å»ºçš„å¤–è§‚èƒ½å¤Ÿæ— ç¼åœ°é›†æˆåˆ°ä¸‹æ¸¸åº”ç”¨ä¸­ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºå³æ—¶è®­ç»ƒçš„è¶…å¿«è®­ç»ƒç­–ç•¥æ˜¯æˆ‘ä»¬æ–¹æ³•çš„æ ¸å¿ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäº3Dé«˜æ–¯å±•å¼€æŠ€æœ¯ï¼ˆ3DGSï¼‰ï¼Œæ‘’å¼ƒäº†å¯å˜å½¢3DGSä¸­å¯¹å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPsï¼‰çš„ä¾èµ–ï¼Œåªä¾èµ–å‡ ä½•ç»“æ„ï¼Œè¿™æå¤§åœ°æé«˜äº†å¯¹é¢éƒ¨è¡¨æƒ…çš„é€‚åº”é€Ÿåº¦ã€‚ä¸ºäº†ç¡®ä¿å³æ—¶è®­ç»ƒçš„é«˜æ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºä¸»è¦ç‚¹çš„ç®€åŒ–ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨é¢éƒ¨è¡¨é¢æ›´ç¨€ç–åœ°åˆ†å¸ƒç‚¹äº‘ï¼Œåœ¨ä¿æŒæ¸²æŸ“è´¨é‡çš„åŒæ—¶ä¼˜åŒ–äº†ç‚¹çš„æ•°é‡ã€‚å€ŸåŠ©å³æ—¶è®­ç»ƒåŠŸèƒ½ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¿æŠ¤äº†é¢éƒ¨çš„éšç§å¹¶é™ä½äº†VRç³»ç»Ÿæˆ–åœ¨çº¿ä¼šè®®ä¸­çš„é€šä¿¡å¸¦å®½ã€‚æ­¤å¤–ï¼Œå®ƒå¯ä»¥ç›´æ¥åº”ç”¨äºåŠ¨ç”»ã€å¡é€šåŒ–å’Œé‡æ–°ç…§æ˜ç­‰ä¸‹æ¸¸åº”ç”¨ã€‚æƒ³äº†è§£æ›´å¤šè¯¦æƒ…ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://songluchuan.github.io/StreamME/%E3%80%82">https://songluchuan.github.io/StreamME/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17029v1">PDF</a> 12 pages, 15 Figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºStreamMEçš„å¿«é€Ÿ3Då¤´åƒé‡å»ºæ–¹æ³•ï¼Œå®ƒåŒæ­¥è®°å½•å¹¶ä»å®æ—¶è§†é¢‘æµä¸­é‡å»ºå¤´åƒï¼Œæ— éœ€é¢„å…ˆç¼“å­˜æ•°æ®ã€‚è¯¥æ–¹æ³•ä»¥å®æ—¶è®­ç»ƒä¸ºæ ¸å¿ƒï¼Œå»ºç«‹äº3Dé«˜æ–¯å±•å¼€æŠ€æœ¯ä¹‹ä¸Šï¼Œç®€åŒ–äº†ç‚¹äº‘åˆ†å¸ƒï¼Œæé«˜æ¸²æŸ“æ•ˆç‡ï¼Œä¿æŠ¤é¢éƒ¨éšç§å¹¶å‡å°‘è™šæ‹Ÿå®å¢ƒç³»ç»Ÿæˆ–åœ¨çº¿ä¼šè®®ä¸­çš„é€šä¿¡å¸¦å®½éœ€æ±‚ã€‚å¯ç›´æ¥åº”ç”¨äºåŠ¨ç”»ã€å¡é€šæ¸²æŸ“å’Œé‡å…‰ç…§ç­‰ä¸‹æ¸¸åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>StreamMEæ˜¯ä¸€ç§å¿«é€Ÿ3Då¤´åƒé‡å»ºæ–¹æ³•ï¼Œå¯ä»å®æ—¶è§†é¢‘æµä¸­åŒæ­¥è®°å½•å’Œé‡å»ºå¤´åƒã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨å®æ—¶è®­ç»ƒç­–ç•¥ï¼Œæ‘’å¼ƒäº†å¯¹å¤šå±‚æ„ŸçŸ¥æœºçš„ä¾èµ–ï¼Œä»…ä¾èµ–å‡ ä½•ä¿¡æ¯ï¼Œæé«˜äº†å¯¹é¢éƒ¨è¡¨æƒ…çš„é€‚åº”é€Ÿåº¦ã€‚</li>
<li>StreamMEåŸºäºä¸»è¦ç‚¹ç®€åŒ–ç­–ç•¥ï¼Œä¼˜åŒ–äº†ç‚¹äº‘åˆ†å¸ƒï¼Œåœ¨ä¿æŒæ¸²æŸ“è´¨é‡çš„åŒæ—¶å‡å°‘äº†ç‚¹æ•°ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½æé«˜æ¸²æŸ“æ•ˆç‡ï¼Œä¿æŠ¤é¢éƒ¨éšç§ï¼Œé™ä½è™šæ‹Ÿå®å¢ƒç³»ç»Ÿæˆ–åœ¨çº¿ä¼šè®®çš„é€šä¿¡å¸¦å®½éœ€æ±‚ã€‚</li>
<li>StreamMEå¯ç›´æ¥åº”ç”¨äºåŠ¨ç”»ã€å¡é€šæ¸²æŸ“å’Œé‡å…‰ç…§ç­‰ä¸‹æ¸¸åº”ç”¨ã€‚</li>
<li>æ–‡ç« æåˆ°çš„3Dé«˜æ–¯å±•å¼€æŠ€æœ¯æ˜¯è¯¥æ–¹æ³•çš„é‡è¦åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17029">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02629449751a7b8e7335e258911c30d1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-facb3a2875aa1ff49609f4ed51701e2a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39db404d7510185eb41364a776ca8189.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a17537a8404bc1a4a156373cfa1392d0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="DCHM-Depth-Consistent-Human-Modeling-for-Multiview-Detection"><a href="#DCHM-Depth-Consistent-Human-Modeling-for-Multiview-Detection" class="headerlink" title="DCHM: Depth-Consistent Human Modeling for Multiview Detection"></a>DCHM: Depth-Consistent Human Modeling for Multiview Detection</h2><p><strong>Authors:Jiahao Ma, Tianyu Wang, Miaomiao Liu, David Ahmedt-Aristizabal, Chuong Nguyen</strong></p>
<p>Multiview pedestrian detection typically involves two stages: human modeling and pedestrian localization. Human modeling represents pedestrians in 3D space by fusing multiview information, making its quality crucial for detection accuracy. However, existing methods often introduce noise and have low precision. While some approaches reduce noise by fitting on costly multiview 3D annotations, they often struggle to generalize across diverse scenes. To eliminate reliance on human-labeled annotations and accurately model humans, we propose Depth-Consistent Human Modeling (DCHM), a framework designed for consistent depth estimation and multiview fusion in global coordinates. Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting achieves multiview depth consistency in sparse-view, large-scaled, and crowded scenarios, producing precise point clouds for pedestrian localization. Extensive validations demonstrate that our method significantly reduces noise during human modeling, outperforming previous state-of-the-art baselines. Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians and perform multiview segmentation in such a challenging setting. Code is available on the \href{<a target="_blank" rel="noopener" href="https://jiahao-ma.github.io/DCHM/%7D%7Bproject">https://jiahao-ma.github.io/DCHM/}{project</a> page}. </p>
<blockquote>
<p>å¤šè§†è§’è¡Œäººæ£€æµ‹é€šå¸¸æ¶‰åŠä¸¤ä¸ªé˜¶æ®µï¼šäººä½“å»ºæ¨¡å’Œè¡Œäººå®šä½ã€‚äººä½“å»ºæ¨¡é€šè¿‡èåˆå¤šè§†è§’ä¿¡æ¯æ¥åœ¨ä¸‰ç»´ç©ºé—´ä¸­è¡¨ç¤ºè¡Œäººï¼Œå› æ­¤å…¶è´¨é‡å¯¹æ£€æµ‹ç²¾åº¦è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€ä¼šå¼•å…¥å™ªå£°ä¸”ç²¾åº¦è¾ƒä½ã€‚è™½ç„¶ä¸€äº›æ–¹æ³•é€šè¿‡é€‚åº”æˆæœ¬é«˜æ˜‚çš„å¤šè§†è§’ä¸‰ç»´æ ‡æ³¨æ¥å‡å°‘å™ªå£°ï¼Œä½†å®ƒä»¬å¾€å¾€åœ¨è·¨ä¸åŒåœºæ™¯æ—¶æ¨å¹¿å›°éš¾ã€‚ä¸ºäº†æ¶ˆé™¤å¯¹äººç±»æ ‡æ³¨çš„ä¾èµ–å¹¶å®ç°ç²¾å‡†çš„äººä½“å»ºæ¨¡ï¼Œæˆ‘ä»¬æå‡ºäº†æ·±åº¦ä¸€è‡´äººä½“å»ºæ¨¡ï¼ˆDCHMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå…¨å±€åæ ‡ç³»ä¸­çš„ä¸€è‡´æ·±åº¦ä¼°è®¡å’Œå¤šè§†è§’èåˆçš„æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºçš„å…·æœ‰è¶…åƒç´ çº§é«˜æ–¯å¹³é“ºçš„ç®¡é“åœ¨ç¨€ç–è§†è§’ã€å¤§è§„æ¨¡å’Œæ‹¥æŒ¤çš„åœºæ™¯ä¸­å®ç°äº†å¤šè§†è§’æ·±åº¦ä¸€è‡´æ€§ï¼Œä¸ºè¡Œäººå®šä½ç”Ÿæˆç²¾ç¡®çš„ç‚¹äº‘ã€‚å¹¿æ³›çš„éªŒè¯è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨äººå»ºå»ºæ¨¡è¿‡ç¨‹ä¸­æ˜¾è‘—é™ä½äº†å™ªå£°ï¼Œä¼˜äºæœ€æ–°çš„åŸºçº¿ã€‚æ­¤å¤–ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒDCHMæ˜¯åœ¨å¦‚æ­¤å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­é‡å»ºè¡Œäººå’Œè¿›è¡Œå¤šè§†è§’åˆ†å‰²çš„ç¬¬ä¸€ç§æ–¹æ³•ã€‚ä»£ç å¯åœ¨é¡¹ç›®é¡µé¢æ‰¾åˆ°ï¼š[<a target="_blank" rel="noopener" href="https://jiahao-ma.github.io/DCHM/]">https://jiahao-ma.github.io/DCHM/]</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14505v1">PDF</a> multi-view detection, sparse-view reconstruction</p>
<p><strong>Summary</strong><br>ä¸‰ç»´å¤šè§†è§’è¡Œäººæ£€æµ‹åŒ…æ‹¬äººä½“å»ºæ¨¡å’Œè¡Œäººå®šä½ä¸¤ä¸ªé˜¶æ®µã€‚ç°æœ‰çš„äººä½“å»ºæ¨¡æ–¹æ³•èåˆå¤šè§†è§’ä¿¡æ¯è¡¨ç¤ºè¡Œäººï¼Œä½†å¼•å…¥å™ªå£°å¹¶é™ä½ç²¾åº¦ã€‚æˆ‘ä»¬æå‡ºDepth-Consistent Human Modelingï¼ˆDCHMï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°æ·±åº¦ä¸€è‡´æ€§å¤šè§†è§’èåˆã€‚é€šè¿‡è¶…åƒç´ çº§çš„é«˜æ–¯å»¶å±•æŠ€æœ¯ï¼Œåœ¨ç¨€ç–è§†è§’ã€å¤§è§„æ¨¡å’Œæ‹¥æŒ¤åœºæ™¯ä¸­å®ç°æ·±åº¦ä¸€è‡´æ€§ï¼Œä¸ºè¡Œäººå®šä½ç”Ÿæˆç²¾ç¡®çš„ç‚¹äº‘ã€‚éªŒè¯è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—é™ä½äº†äººä½“å»ºæ¨¡ä¸­çš„å™ªå£°ï¼Œä¼˜äºç°æœ‰æŠ€æœ¯ã€‚DCHMæ˜¯é¦–ä¸ªåœ¨æ­¤ç±»å¤æ‚ç¯å¢ƒä¸­é‡å»ºè¡Œäººå’Œè¿›è¡Œå¤šè§†è§’åˆ†å‰²çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Multiview pedestrian detectionæ¶‰åŠäººä½“å»ºæ¨¡å’Œè¡Œäººå®šä½ä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>ç°æœ‰æ–¹æ³•èåˆå¤šè§†è§’ä¿¡æ¯è¡¨ç¤ºè¡Œäººï¼Œä½†å­˜åœ¨å¼•å…¥å™ªå£°å’Œç²¾åº¦ä½çš„é—®é¢˜ã€‚</li>
<li>Depth-Consistent Human Modelingï¼ˆDCHMï¼‰æ¡†æ¶æ—¨åœ¨å®ç°æ·±åº¦ä¸€è‡´æ€§å¤šè§†è§’èåˆã€‚</li>
<li>DCHMé€šè¿‡è¶…åƒç´ çº§çš„é«˜æ–¯å»¶å±•æŠ€æœ¯å®ç°æ·±åº¦ä¸€è‡´æ€§ã€‚</li>
<li>DCHMåœ¨ç¨€ç–è§†è§’ã€å¤§è§„æ¨¡å’Œæ‹¥æŒ¤åœºæ™¯ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>DCHMç”Ÿæˆç²¾ç¡®çš„ç‚¹äº‘ï¼Œæœ‰åŠ©äºè¡Œäººå®šä½ã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒDCHMåœ¨äººä½“å»ºæ¨¡ä¸­æ˜¾è‘—é™ä½å™ªå£°ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b29812ea7e62b748445886392140771.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-939756d4e40f9a5bb1e69d8b7cb18d63.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bd3b491e98a0e74de66309568cd3679.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-37b42a900efc069fa6431b75c5963107.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efb1fa5b0b6cdb638ab7019466a58bb9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5e1f3e9ce2af567fe471dbad8dea5053.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Advances-in-Feed-Forward-3D-Reconstruction-and-View-Synthesis-A-Survey"><a href="#Advances-in-Feed-Forward-3D-Reconstruction-and-View-Synthesis-A-Survey" class="headerlink" title="Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey"></a>Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey</h2><p><strong>Authors:Jiahui Zhang, Yuelei Li, Anpei Chen, Muyu Xu, Kunhao Liu, Jianyuan Wang, Xiao-Xiao Long, Hanxue Liang, Zexiang Xu, Hao Su, Christian Theobalt, Christian Rupprecht, Andrea Vedaldi, Hanspeter Pfister, Shijian Lu, Fangneng Zhan</strong></p>
<p>3D reconstruction and view synthesis are foundational problems in computer vision, graphics, and immersive technologies such as augmented reality (AR), virtual reality (VR), and digital twins. Traditional methods rely on computationally intensive iterative optimization in a complex chain, limiting their applicability in real-world scenarios. Recent advances in feed-forward approaches, driven by deep learning, have revolutionized this field by enabling fast and generalizable 3D reconstruction and view synthesis. This survey offers a comprehensive review of feed-forward techniques for 3D reconstruction and view synthesis, with a taxonomy according to the underlying representation architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural Radiance Fields (NeRF), etc. We examine key tasks such as pose-free reconstruction, dynamic 3D reconstruction, and 3D-aware image and video synthesis, highlighting their applications in digital humans, SLAM, robotics, and beyond. In addition, we review commonly used datasets with detailed statistics, along with evaluation protocols for various downstream tasks. We conclude by discussing open research challenges and promising directions for future work, emphasizing the potential of feed-forward approaches to advance the state of the art in 3D vision. </p>
<blockquote>
<p>3Dé‡å»ºå’Œè§†å›¾åˆæˆæ˜¯è®¡ç®—æœºè§†è§‰ã€å›¾å½¢å­¦å’Œæ²‰æµ¸å¼æŠ€æœ¯ï¼ˆå¦‚å¢å¼ºç°å®(AR)ã€è™šæ‹Ÿç°å®(VR)å’Œæ•°å­—å­ªç”Ÿï¼‰ä¸­çš„åŸºç¡€é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¤æ‚é“¾ä¸­çš„è®¡ç®—å¯†é›†å‹è¿­ä»£ä¼˜åŒ–ï¼Œè¿™åœ¨ç°å®åœºæ™¯çš„åº”ç”¨ä¸­å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ã€‚æœ€è¿‘ï¼Œæ·±åº¦å­¦ä¹ é©±åŠ¨çš„å‰é¦ˆæ–¹æ³•çš„è¿›æ­¥å·²ç»å½»åº•æ”¹å˜äº†è¿™ä¸€é¢†åŸŸï¼Œå®ç°äº†å¿«é€Ÿå’Œé€šç”¨çš„3Dé‡å»ºå’Œè§†å›¾åˆæˆã€‚è¿™ç¯‡ç»¼è¿°å…¨é¢ä»‹ç»äº†å‰é¦ˆæŠ€æœ¯åœ¨3Dé‡å»ºå’Œè§†å›¾åˆæˆæ–¹é¢çš„åº”ç”¨ï¼Œå¹¶æ ¹æ®åº•å±‚è¡¨ç¤ºæ¶æ„è¿›è¡Œäº†åˆ†ç±»ï¼ŒåŒ…æ‹¬ç‚¹äº‘ã€3Dé«˜æ–¯æº…å°„ï¼ˆ3DGSï¼‰ã€ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ç­‰ã€‚æˆ‘ä»¬ç ”ç©¶äº†å…³é”®ä»»åŠ¡ï¼Œå¦‚å§¿æ€è‡ªç”±é‡å»ºã€åŠ¨æ€3Dé‡å»ºå’Œ3Dæ„ŸçŸ¥å›¾åƒå’Œè§†é¢‘åˆæˆï¼Œçªå‡ºäº†å®ƒä»¬åœ¨æ•°å­—äººç±»ã€SLAMã€æœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸçš„åº”ç”¨ä»¥åŠå…¶ä»–æ›´å¹¿æ³›çš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å›é¡¾äº†å¸¸ç”¨çš„æ•°æ®é›†åŠå…¶è¯¦ç»†ç»Ÿè®¡æ•°æ®ï¼Œä»¥åŠå„ç§ä¸‹æ¸¸ä»»åŠ¡çš„è¯„ä¼°åè®®ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å½“å‰çš„ç ”ç©¶æŒ‘æˆ˜ä»¥åŠæœªæ¥å·¥ä½œçš„æœ‰å‰é€”çš„æ–¹å‘ï¼Œå¼ºè°ƒäº†å‰é¦ˆæ–¹æ³•åœ¨æ¨åŠ¨3Dè§†è§‰æŠ€æœ¯å‰æ²¿çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14501v2">PDF</a> A project page associated with this survey is available at   <a target="_blank" rel="noopener" href="https://fnzhan.com/projects/Feed-Forward-3D">https://fnzhan.com/projects/Feed-Forward-3D</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç»¼è¿°äº†åŸºäºæ·±åº¦å­¦ä¹ çš„Feed-forwardæŠ€æœ¯åœ¨ä¸‰ç»´é‡å»ºå’Œè§†å›¾åˆæˆæ–¹é¢çš„åº”ç”¨ï¼Œæ¶µç›–äº†å„ç§åº•å±‚æ¶æ„ï¼Œå¦‚ç‚¹äº‘ã€ä¸‰ç»´é«˜æ–¯æ¸²æŸ“æŠ€æœ¯å’Œç¥ç»ç½‘ç»œè¾å°„åœºç­‰ã€‚æœ¬æ–‡è¯¦ç»†è®¨è®ºäº†ä¸‰ç»´é‡å»ºä¸­çš„å…³é”®ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ— å§¿æ€é‡å»ºã€åŠ¨æ€ä¸‰ç»´é‡å»ºå’Œä¸‰ç»´å›¾åƒå’Œè§†é¢‘åˆæˆç­‰ï¼Œå¹¶å¼ºè°ƒäº†è¿™äº›æŠ€æœ¯åœ¨æ•°å­—äººã€å³æ—¶å®šä½ä¸åœ°å›¾æ„å»ºï¼ˆSLAMï¼‰ã€æœºå™¨äººç­‰é¢†åŸŸçš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜ä»‹ç»äº†å¸¸ç”¨çš„æ•°æ®é›†åŠå…¶è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ï¼Œä»¥åŠå„ç§ä¸‹æ¸¸ä»»åŠ¡çš„è¯„ä¼°åè®®ã€‚æœ€åï¼Œæœ¬æ–‡æ¢è®¨äº†å¼€æ”¾çš„ç ”ç©¶æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶çš„æœ‰å‰é€”çš„æ–¹å‘ï¼Œå¼ºè°ƒäº†Feed-forwardæŠ€æœ¯åœ¨æ¨åŠ¨ä¸‰ç»´è§†è§‰æŠ€æœ¯å‰æ²¿çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Feed-forwardæŠ€æœ¯æ¨åŠ¨äº†ä¸‰ç»´é‡å»ºå’Œè§†å›¾åˆæˆçš„é©å‘½æ€§å‘å±•ï¼Œä½¿å¿«é€Ÿä¸”é€šç”¨çš„ä¸‰ç»´é‡å»ºæˆä¸ºå¯èƒ½ã€‚</li>
<li>ç‚¹äº‘ã€ä¸‰ç»´é«˜æ–¯æ¸²æŸ“æŠ€æœ¯å’Œç¥ç»ç½‘ç»œè¾å°„åœºç­‰æ˜¯ä¸»è¦çš„åº•å±‚æ¶æ„ã€‚</li>
<li>æ— å§¿æ€é‡å»ºã€åŠ¨æ€ä¸‰ç»´é‡å»ºå’Œä¸‰ç»´å›¾åƒå’Œè§†é¢‘åˆæˆæ˜¯ä¸‰ç»´é‡å»ºä¸­çš„å…³é”®ä»»åŠ¡ã€‚</li>
<li>è¿™äº›æŠ€æœ¯åœ¨æ•°å­—äººã€å³æ—¶å®šä½ä¸åœ°å›¾æ„å»ºï¼ˆSLAMï¼‰ã€æœºå™¨äººç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>å¸¸ç”¨çš„æ•°æ®é›†åŠå…¶è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯å¯¹äºç ”ç©¶å’Œåº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>è¯„ä¼°åè®®å¯¹äºå„ç§ä¸‹æ¸¸ä»»åŠ¡éå¸¸é‡è¦ï¼Œæœ‰åŠ©äºè¡¡é‡æŠ€æœ¯æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-49a484bb983abad5bc0f6fb7f24c2ea8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-af67e8f5ffb0c8ebb2831103199b100a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bfa97f29caebec405ae1e8e652e7e2d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-950245207a32bfbb23e9d14eb54d7f7a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e8f9d80f8c8e9a5d566a28ba69100a3.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DreamScene-3D-Gaussian-based-End-to-end-Text-to-3D-Scene-Generation"><a href="#DreamScene-3D-Gaussian-based-End-to-end-Text-to-3D-Scene-Generation" class="headerlink" title="DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation"></a>DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation</h2><p><strong>Authors:Haoran Li, Yuli Tian, Kun Lan, Yong Liao, Lin Wang, Pan Hui, Peng Yuan Zhou</strong></p>
<p>Generating 3D scenes from natural language holds great promise for applications in gaming, film, and design. However, existing methods struggle with automation, 3D consistency, and fine-grained control. We present DreamScene, an end-to-end framework for high-quality and editable 3D scene generation from text or dialogue. DreamScene begins with a scene planning module, where a GPT-4 agent infers object semantics and spatial constraints to construct a hybrid graph. A graph-based placement algorithm then produces a structured, collision-free layout. Based on this layout, Formation Pattern Sampling (FPS) generates object geometry using multi-timestep sampling and reconstructive optimization, enabling fast and realistic synthesis. To ensure global consistent, DreamScene employs a progressive camera sampling strategy tailored to both indoor and outdoor settings. Finally, the system supports fine-grained scene editing, including object movement, appearance changes, and 4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior methods in quality, consistency, and flexibility, offering a practical solution for open-domain 3D content creation. Code and demos are available at <a target="_blank" rel="noopener" href="https://jahnsonblack.github.io/DreamScene-Full/">https://jahnsonblack.github.io/DreamScene-Full/</a>. </p>
<blockquote>
<p>ä»è‡ªç„¶è¯­è¨€ç”Ÿæˆ3Dåœºæ™¯åœ¨æ¸¸æˆã€ç”µå½±å’Œè®¾è®¡ç­‰é¢†åŸŸçš„åº”ç”¨å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨è‡ªåŠ¨åŒ–ã€3Dä¸€è‡´æ€§å’Œç²¾ç»†æ§åˆ¶æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºäº†DreamSceneï¼Œä¸€ä¸ªä»æ–‡æœ¬æˆ–å¯¹è¯ä¸­è¿›è¡Œé«˜è´¨é‡å’Œå¯ç¼–è¾‘çš„3Dåœºæ™¯ç”Ÿæˆçš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚DreamSceneä»åœºæ™¯è§„åˆ’æ¨¡å—å¼€å§‹ï¼ŒGPT-4æ™ºèƒ½ä½“ä¼šæ¨æ–­å¯¹è±¡è¯­ä¹‰å’Œç©ºé—´çº¦æŸæ¥æ„å»ºæ··åˆå›¾ã€‚åŸºäºå›¾çš„æ”¾ç½®ç®—æ³•ç„¶åäº§ç”Ÿç»“æ„åŒ–çš„ã€æ— ç¢°æ’çš„å¸ƒå±€ã€‚åŸºäºè¯¥å¸ƒå±€ï¼ŒFormation Pattern Samplingï¼ˆFPSï¼‰ä½¿ç”¨å¤šæ—¶æ­¥é‡‡æ ·å’Œé‡å»ºä¼˜åŒ–ç”Ÿæˆå¯¹è±¡å‡ ä½•ï¼Œå®ç°å¿«é€Ÿå’Œé€¼çœŸçš„åˆæˆã€‚ä¸ºç¡®ä¿å…¨å±€ä¸€è‡´æ€§ï¼ŒDreamSceneé‡‡ç”¨é’ˆå¯¹å®¤å†…å’Œå®¤å¤–ç¯å¢ƒçš„æ¸è¿›å¼ç›¸æœºé‡‡æ ·ç­–ç•¥ã€‚æœ€åï¼Œè¯¥ç³»ç»Ÿæ”¯æŒç²¾ç»†çš„åœºæ™¯ç¼–è¾‘ï¼ŒåŒ…æ‹¬å¯¹è±¡ç§»åŠ¨ã€å¤–è§‚å˜åŒ–å’Œ4DåŠ¨æ€è¿åŠ¨ã€‚å®éªŒè¡¨æ˜ï¼ŒDreamSceneåœ¨è´¨é‡ã€ä¸€è‡´æ€§å’Œçµæ´»æ€§æ–¹é¢è¶…è¶Šäº†ä¹‹å‰çš„æ–¹æ³•ï¼Œä¸ºå¼€æ”¾åŸŸ3Då†…å®¹åˆ›å»ºæä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆã€‚ä»£ç å’Œæ¼”ç¤ºå¯åœ¨<a target="_blank" rel="noopener" href="https://jahnsonblack.github.io/DreamScene-Full/%E6%89%BE%E5%88%B0%E3%80%82">https://jahnsonblack.github.io/DreamScene-Full/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13985v2">PDF</a> Extended version of ECCV 2024 paper â€œDreamSceneâ€</p>
<p><strong>Summary</strong><br>æ–‡æœ¬æè¿°äº†ä¸€ä¸ªåä¸ºDreamSceneçš„ç«¯åˆ°ç«¯æ¡†æ¶ï¼Œå®ƒèƒ½ä»æ–‡æœ¬æˆ–å¯¹è¯ä¸­ç”Ÿæˆé«˜è´¨é‡ä¸”å¯ç¼–è¾‘çš„3Dåœºæ™¯ã€‚è¯¥æ¡†æ¶é€šè¿‡åœºæ™¯è§„åˆ’æ¨¡å—å¼€å§‹ï¼Œä½¿ç”¨GPT-4ä»£ç†æ¨æ–­å¯¹è±¡è¯­ä¹‰å’Œç©ºé—´çº¦æŸæ¥æ„å»ºæ··åˆå›¾ã€‚åŸºäºå›¾çš„æ”¾ç½®ç®—æ³•ç”Ÿæˆç»“æ„åŒ–çš„æ— ç¢°æ’å¸ƒå±€ï¼Œç„¶åé€šè¿‡Formation Pattern Samplingç”Ÿæˆå¯¹è±¡å‡ ä½•ç»“æ„ã€‚ä¸ºç¡®ä¿å…¨å±€ä¸€è‡´æ€§ï¼ŒDreamSceneé‡‡ç”¨äº†é¢å‘å®¤å†…å’Œå®¤å¤–ç¯å¢ƒçš„æ¸è¿›å¼æ‘„åƒæœºé‡‡æ ·ç­–ç•¥ã€‚è¯¥ç³»ç»Ÿæ”¯æŒç²¾ç»†çš„åœºæ™¯ç¼–è¾‘ï¼Œå¦‚å¯¹è±¡ç§»åŠ¨ã€å¤–è§‚å˜åŒ–å’Œå››ç»´åŠ¨æ€è¿åŠ¨ã€‚å®éªŒè¡¨æ˜ï¼ŒDreamSceneåœ¨è´¨é‡ã€ä¸€è‡´æ€§å’Œçµæ´»æ€§æ–¹é¢è¶…è¶Šäº†ä»¥å‰çš„æ–¹æ³•ï¼Œä¸ºå¼€æ”¾é¢†åŸŸçš„3Då†…å®¹åˆ›å»ºæä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DreamSceneæ˜¯ä¸€ä¸ªç”¨äºä»æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡ä¸”å¯ç¼–è¾‘çš„3Dåœºæ™¯çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚</li>
<li>å®ƒä½¿ç”¨GPT-4ä»£ç†è¿›è¡Œåœºæ™¯è§„åˆ’ï¼Œé€šè¿‡æ¨æ–­å¯¹è±¡è¯­ä¹‰å’Œç©ºé—´çº¦æŸæ¥æ„å»ºæ··åˆå›¾ã€‚</li>
<li>åŸºäºå›¾çš„æ”¾ç½®ç®—æ³•ç”Ÿæˆæ— ç¢°æ’çš„å¸ƒå±€ç»“æ„ã€‚</li>
<li>Formation Pattern Samplingç”¨äºç”Ÿæˆå¯¹è±¡å‡ ä½•ç»“æ„ï¼Œç¡®ä¿å¿«é€Ÿä¸”é€¼çœŸçš„åˆæˆã€‚</li>
<li>DreamSceneé‡‡ç”¨æ¸è¿›å¼æ‘„åƒæœºé‡‡æ ·ç­–ç•¥ï¼Œç¡®ä¿å…¨å±€ä¸€è‡´æ€§ï¼Œé€‚ç”¨äºå®¤å†…å’Œå®¤å¤–ç¯å¢ƒã€‚</li>
<li>è¯¥ç³»ç»Ÿæ”¯æŒç²¾ç»†çš„åœºæ™¯ç¼–è¾‘åŠŸèƒ½ï¼Œå¦‚å¯¹è±¡ç§»åŠ¨ã€å¤–è§‚å˜åŒ–å’ŒåŠ¨æ€è¿åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13985">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-375f39cf35861cff679adf7329566123.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-33d15e421b3e52be80c28d1faecef16c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-76721972be8bbfc93e3d4391ac6cd3ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36c5713a94daef7d7027b4c84354bdb9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f4658cb0438977f5373dc24447aaf8f.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="VisualSpeaker-Visually-Guided-3D-Avatar-Lip-Synthesis"><a href="#VisualSpeaker-Visually-Guided-3D-Avatar-Lip-Synthesis" class="headerlink" title="VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis"></a>VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis</h2><p><strong>Authors:Alexandre Symeonidis-Herzig, Ã–zge MercanoÄŸlu Sincan, Richard Bowden</strong></p>
<p>Realistic, high-fidelity 3D facial animations are crucial for expressive avatar systems in human-computer interaction and accessibility. Although prior methods show promising quality, their reliance on the mesh domain limits their ability to fully leverage the rapid visual innovations seen in 2D computer vision and graphics. We propose VisualSpeaker, a novel method that bridges this gap using photorealistic differentiable rendering, supervised by visual speech recognition, for improved 3D facial animation. Our contribution is a perceptual lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting avatar renders through a pre-trained Visual Automatic Speech Recognition model during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker improves both the standard Lip Vertex Error metric by 56.1% and the perceptual quality of the generated animations, while retaining the controllability of mesh-driven animation. This perceptual focus naturally supports accurate mouthings, essential cues that disambiguate similar manual signs in sign language avatars. </p>
<blockquote>
<p>çœŸå®ã€é«˜ä¿çœŸåº¦çš„ä¸‰ç»´é¢éƒ¨åŠ¨ç”»å¯¹äºäººæœºäº¤äº’å’Œå¯è®¿é—®æ€§ä¸­çš„è¡¨æƒ…ç¬¦å·ç³»ç»Ÿè‡³å…³é‡è¦ã€‚å°½ç®¡å…ˆå‰çš„æ–¹æ³•æ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„å“è´¨ï¼Œä½†å®ƒä»¬å¯¹ç½‘æ ¼åŸŸçš„ä¾èµ–é™åˆ¶äº†å®ƒä»¬å……åˆ†åˆ©ç”¨äºŒç»´è®¡ç®—æœºè§†è§‰å’Œå›¾å½¢ä¸­å¿«é€Ÿè§†è§‰åˆ›æ–°çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†VisualSpeakerï¼Œè¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡çœŸå®å¯æ¸²æŸ“çš„æŠ€æœ¯ç¼©å°è¿™ä¸€å·®è·ï¼Œç”±è§†è§‰è¯­éŸ³è¯†åˆ«è¿›è¡Œç›‘ç£ï¼Œä»¥æé«˜ä¸‰ç»´é¢éƒ¨åŠ¨ç”»çš„è´¨é‡ã€‚æˆ‘ä»¬çš„è´¡çŒ®åœ¨äºæ„ŸçŸ¥å”‡è¯»æŸå¤±ï¼Œå®ƒæ˜¯é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†é€šè¿‡é¢„è®­ç»ƒçš„è§†è§‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„çœŸå®ä¸‰ç»´é«˜æ–¯å˜å½¢äººè„¸æ¸²æŸ“ä¼ é€’ï¼Œä»è€Œæ´¾ç”Ÿå‡ºæ¥çš„ã€‚åœ¨MEADæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒVisualSpeakerä¸ä»…å°†æ ‡å‡†å”‡é¡¶ç‚¹è¯¯å·®åº¦é‡æé«˜äº†56.1%ï¼Œè€Œä¸”æé«˜äº†ç”ŸæˆåŠ¨ç”»çš„æ„ŸçŸ¥è´¨é‡ï¼ŒåŒæ—¶è¿˜ä¿æŒäº†ç½‘æ ¼é©±åŠ¨åŠ¨ç”»çš„å¯æ§æ€§ã€‚è¿™ç§æ„ŸçŸ¥é‡ç‚¹è‡ªç„¶æ”¯æŒå‡†ç¡®çš„å˜´éƒ¨åŠ¨ä½œï¼Œè¿™æ˜¯åŒºåˆ†ç±»ä¼¼æ‰‹åŠ¨ç¬¦å·çš„æ‰‹è¯­è¡¨æƒ…ç¬¦å·çš„é‡è¦çº¿ç´¢ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.06060v2">PDF</a> Accepted in International Conference on Computer Vision (ICCV)   Workshops</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºVisualSpeakerçš„æ–°å‹æ–¹æ³•ï¼Œé€šè¿‡é‡‡ç”¨é€¼çœŸçš„å¯å¾®åˆ†æ¸²æŸ“æŠ€æœ¯ï¼Œç»“åˆè§†è§‰è¯­éŸ³è¯†åˆ«çš„ç›‘ç£ï¼Œæ”¹è¿›äº†3Dé¢éƒ¨åŠ¨ç”»ã€‚è¯¥æ–¹æ³•ä½¿ç”¨æ„ŸçŸ¥å”‡è¯»æŸå¤±ï¼Œé€šè¿‡åœ¨è®­ç»ƒæœŸé—´å°†é€šè¿‡é€¼çœŸæ¸²æŸ“çš„3Dé«˜æ–¯å–·æº…äººåƒå‘ˆç°ç»™é¢„è®­ç»ƒçš„è§†è§‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹æ¥å¾—åˆ°ã€‚åœ¨MEADæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒVisualSpeakeræ”¹è¿›äº†å”‡é¡¶ç‚¹è¯¯å·®æŒ‡æ ‡ï¼Œæé«˜äº†åŠ¨ç”»ç”Ÿæˆçš„è´¨é‡ï¼ŒåŒæ—¶ä¿ç•™äº†ç½‘æ ¼é©±åŠ¨çš„åŠ¨ç”»çš„å¯æ§æ€§ã€‚è¿™ç§æ–¹æ³•ä¸ºçœŸå®ã€é«˜è´¨é‡çš„3Dé¢éƒ¨åŠ¨ç”»æä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œå¯¹äººæœºäº¤äº’å’Œè®¿é—®æ§åˆ¶ä¸­çš„è¡¨æƒ…ç¬¦å·ç³»ç»Ÿå°¤ä¸ºé‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VisualSpeakeræ˜¯ä¸€ç§æ”¹è¿›3Dé¢éƒ¨åŠ¨ç”»çš„æ–°å‹æ–¹æ³•ï¼Œç»“åˆäº†é€¼çœŸçš„å¯å¾®åˆ†æ¸²æŸ“å’Œè§†è§‰è¯­éŸ³è¯†åˆ«çš„ç›‘ç£ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ä½¿ç”¨æ„ŸçŸ¥å”‡è¯»æŸå¤±æ¥æé«˜åŠ¨ç”»è´¨é‡ï¼Œè¯¥æŸå¤±æ˜¯é€šè¿‡å°†é€¼çœŸçš„3Dé¢éƒ¨æ¸²æŸ“å‘ˆç°ç»™é¢„è®­ç»ƒçš„è§†è§‰è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹æ¥è®¡ç®—çš„ã€‚</li>
<li>VisualSpeakeråœ¨MEADæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œå”‡é¡¶ç‚¹è¯¯å·®æŒ‡æ ‡æ”¹è¿›äº†56.1%ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†åŠ¨ç”»çš„æ„ŸçŸ¥è´¨é‡ï¼ŒåŒæ—¶ä¿ç•™äº†ç½‘æ ¼é©±åŠ¨åŠ¨ç”»çš„å¯æ§æ€§ã€‚</li>
<li>VisualSpeakerå¯¹äºäººæœºäº¤äº’å’Œè®¿é—®æ§åˆ¶ä¸­çš„è¡¨æƒ…ç¬¦å·ç³»ç»Ÿå°¤ä¸ºé‡è¦ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿæ”¯æŒå‡†ç¡®çš„å˜´éƒ¨åŠ¨ä½œï¼Œè¿™å¯¹äºåŒºåˆ†æ‰‹åŠ¨ç¬¦å·è¯­è¨€ä¸­çš„ç›¸ä¼¼æ‰‹åŠ¨æ ‡å¿—è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.06060">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2de8e4f5fb567b48dced7da6064d8de6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-de8752550d8ec2f96109383cbfaa8632.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b2d05bafcd46d8dd7e08954747a41d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0eb7b5361dc63950890a9f289619896.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3354b9aacd688e1c39bcbec517277f7e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="FOCI-Trajectory-Optimization-on-Gaussian-Splats"><a href="#FOCI-Trajectory-Optimization-on-Gaussian-Splats" class="headerlink" title="FOCI: Trajectory Optimization on Gaussian Splats"></a>FOCI: Trajectory Optimization on Gaussian Splats</h2><p><strong>Authors:Mario Gomez Andreu, Maximum Wilder-Smith, Victor Klemm, Vaishakh Patil, Jesus Tordesillas, Marco Hutter</strong></p>
<p>3D Gaussian Splatting (3DGS) has recently gained popularity as a faster alternative to Neural Radiance Fields (NeRFs) in 3D reconstruction and view synthesis methods. Leveraging the spatial information encoded in 3DGS, this work proposes FOCI (Field Overlap Collision Integral), an algorithm that is able to optimize trajectories directly on the Gaussians themselves. FOCI leverages a novel and interpretable collision formulation for 3DGS using the notion of the overlap integral between Gaussians. Contrary to other approaches, which represent the robot with conservative bounding boxes that underestimate the traversability of the environment, we propose to represent the environment and the robot as Gaussian Splats. This not only has desirable computational properties, but also allows for orientation-aware planning, allowing the robot to pass through very tight and narrow spaces. We extensively test our algorithm in both synthetic and real Gaussian Splats, showcasing that collision-free trajectories for the ANYmal legged robot that can be computed in a few seconds, even with hundreds of thousands of Gaussians making up the environment. The project page and code are available at <a target="_blank" rel="noopener" href="https://rffr.leggedrobotics.com/works/foci/">https://rffr.leggedrobotics.com/works/foci/</a> </p>
<blockquote>
<p>ä¸‰ç»´é«˜æ–¯æ•£æ–‘æ³•ï¼ˆ3DGSï¼‰æœ€è¿‘æˆä¸ºäº†åœ¨ä¸‰ç»´é‡å»ºå’Œè§†è§’åˆæˆæ–¹æ³•ä¸­ï¼Œç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFsï¼‰çš„ä¸€ç§æ›´å¿«æ›¿ä»£æ–¹æ³•è€Œå—åˆ°æ¬¢è¿ã€‚åˆ©ç”¨ä¸‰ç»´é«˜æ–¯æ•£æ–‘æ³•ä¸­çš„ç©ºé—´ä¿¡æ¯ç¼–ç ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†FOCIï¼ˆé«˜æ–¯åŸŸç¢°æ’ç§¯åˆ†ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§èƒ½åœ¨é«˜æ–¯å‡½æ•°æœ¬èº«ä¸Šç›´æ¥ä¼˜åŒ–è½¨è¿¹çš„ç®—æ³•ã€‚FOCIåˆ©ç”¨ä¸€ä¸ªæ–°é¢–çš„å¯è§£é‡Šçš„ç¢°æ’å…¬å¼æ¥æè¿°ä¸‰ç»´é«˜æ–¯æ•£æ–‘æ³•ï¼Œé‡‡ç”¨é«˜æ–¯ä¹‹é—´çš„é‡å ç§¯åˆ†æ¦‚å¿µã€‚ä¸å…¶ä»–ä½¿ç”¨ä¿å®ˆè¾¹ç•Œæ¡†è¡¨ç¤ºæœºå™¨äººï¼Œä»è€Œä½ä¼°ç¯å¢ƒé€šè¡Œæ€§çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬æå‡ºå°†ç¯å¢ƒå’Œæœºå™¨äººè¡¨ç¤ºä¸ºé«˜æ–¯æ•£æ–‘ã€‚è¿™ä¸ä»…å…·æœ‰ç†æƒ³çš„è®¡ç®—ç‰¹æ€§ï¼Œè¿˜å…è®¸å…·æœ‰æ–¹å‘æ„ŸçŸ¥çš„è§„åˆ’ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿç©¿è¿‡éå¸¸ç‹­çª„çš„ç©ºé—´ã€‚æˆ‘ä»¬åœ¨åˆæˆå’ŒçœŸå®çš„é«˜æ–¯æ•£æ–‘ä¸Šéƒ½æµ‹è¯•äº†æˆ‘ä»¬çš„ç®—æ³•ï¼Œå±•ç¤ºäº†å³ä½¿åœ¨ç”±æ•°åä¸‡é«˜æ–¯ç»„æˆçš„ç¯å¢ƒä¸­ï¼Œä¹Ÿå¯ä»¥åœ¨å‡ ç§’å†…è®¡ç®—å‡ºé€‚åˆäººé©¬æœºå™¨äººä¸”æ— ç¢°æ’çš„è½¨è¿¹ã€‚é¡¹ç›®é¡µé¢å’Œä»£ç å¯ä»¥åœ¨ <a target="_blank" rel="noopener" href="https://rffr.leggedrobotics.com/works/foci/">https://rffr.leggedrobotics.com/works/foci/</a> æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08510v2">PDF</a> 8 pages, 8 figures, Mario Gomez Andreu and Maximum Wilder-Smith   contributed equally</p>
<p><strong>Summary</strong></p>
<p>3Dé«˜æ–¯æ··åˆï¼ˆ3DGSï¼‰åœ¨ä¸‰ç»´é‡å»ºå’Œè§†å›¾åˆæˆæ–¹æ³•ä¸­ä½œä¸ºç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFsï¼‰çš„æ›´å¿«æ›¿ä»£æ–¹æ¡ˆè€Œå—åˆ°å…³æ³¨ã€‚æœ¬ç ”ç©¶æå‡ºFOCIï¼ˆåŸºäºåœºé‡å ç¢°æ’ç§¯åˆ†ï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿåœ¨é«˜æ–¯åˆ†å¸ƒæœ¬èº«ä¸Šç›´æ¥ä¼˜åŒ–è½¨è¿¹ã€‚FOCIåˆ©ç”¨é«˜æ–¯ä¹‹é—´é‡å ç§¯åˆ†çš„æ¦‚å¿µï¼Œé‡‡ç”¨ä¸€ç§æ–°é¢–ä¸”å¯è§£é‡Šæ€§å¼ºçš„ç¢°æ’å…¬å¼ç”¨äºä¼˜åŒ–æœºå™¨äººä¸ç¯å¢ƒä¹‹é—´çš„äº¤äº’ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒFOCIå°†æœºå™¨äººå’Œç¯å¢ƒè¡¨ç¤ºä¸ºé«˜æ–¯æ··åˆï¼Œè¿™ä¸ä»…å…·æœ‰ç†æƒ³çš„è®¡ç®—å±æ€§ï¼Œè¿˜å…è®¸è¿›è¡Œæ–¹å‘æ„ŸçŸ¥è§„åˆ’ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿç©¿è¿‡éå¸¸ç´§å‡‘å’Œç‹­çª„çš„ç©ºé—´ã€‚æˆ‘ä»¬çš„ç®—æ³•åœ¨åˆæˆå’ŒçœŸå®çš„é«˜æ–¯æ··åˆä¸­éƒ½ç»è¿‡äº†å¹¿æ³›æµ‹è¯•ï¼Œå±•ç¤ºäº†å³ä½¿åœ¨ç”±æ•°åä¸‡é«˜æ–¯ç»„æˆçš„ç¯å¢ƒä¸­ï¼Œä¹Ÿèƒ½åœ¨å‡ ç§’é’Ÿå†…è®¡ç®—å‡ºANYmalæ­¥è¡Œæœºå™¨äººçš„æ— ç¢°æ’è½¨è¿¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3DGSä½œä¸ºNeRFsçš„æ›¿ä»£æ–¹æ³•ï¼Œåœ¨ä¸‰ç»´é‡å»ºå’Œè§†å›¾åˆæˆä¸­å—åˆ°å…³æ³¨ã€‚</li>
<li>FOCIç®—æ³•åˆ©ç”¨é«˜æ–¯ä¹‹é—´çš„ç©ºé—´ä¿¡æ¯ä¼˜åŒ–è½¨è¿¹ã€‚</li>
<li>FOCIé‡‡ç”¨æ–°é¢–çš„ç¢°æ’å…¬å¼ï¼ŒåŸºäºé«˜æ–¯ä¹‹é—´çš„é‡å ç§¯åˆ†ã€‚</li>
<li>ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼ŒFOCIå°†æœºå™¨äººå’Œç¯å¢ƒè¡¨ç¤ºä¸ºé«˜æ–¯æ··åˆï¼Œæé«˜äº†ç¯å¢ƒéå†èƒ½åŠ›çš„ä¼°è®¡ã€‚</li>
<li>é«˜æ–¯æ··åˆè¡¨ç¤ºå…è®¸æ–¹å‘æ„ŸçŸ¥è§„åˆ’ï¼Œé€‚ç”¨äºç‹­çª„ç©ºé—´ã€‚</li>
<li>åœ¨åˆæˆå’ŒçœŸå®ç¯å¢ƒä¸­å¹¿æ³›æµ‹è¯•äº†FOCIç®—æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2cd80fb72740fbfbe260b17f41ea682c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-da25f21a2f3b282eac96ea984475e9cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-722fd984a1aa4b3dd0a53cd3620d55bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97370c4728c76c97f079a6d7f97112e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-111b00395c3add5361c6281356bef41d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-64682faa2230a9f038a40d4b24a1ec53.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Sparfels-Fast-Reconstruction-from-Sparse-Unposed-Imagery"><a href="#Sparfels-Fast-Reconstruction-from-Sparse-Unposed-Imagery" class="headerlink" title="Sparfels: Fast Reconstruction from Sparse Unposed Imagery"></a>Sparfels: Fast Reconstruction from Sparse Unposed Imagery</h2><p><strong>Authors:Shubhendu Jena, Amine Ouasfi, Mae Younes, Adnane Boukhayma</strong></p>
<p>We present a method for Sparse view reconstruction with surface element splatting that runs within 3 minutes on a consumer grade GPU. While few methods address sparse radiance field learning from noisy or unposed sparse cameras, shape recovery remains relatively underexplored in this setting. Several radiance and shape learning test-time optimization methods address the sparse posed setting by learning data priors or using combinations of external monocular geometry priors. Differently, we propose an efficient and simple pipeline harnessing a single recent 3D foundation model. We leverage its various task heads, notably point maps and camera initializations to instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image correspondences to guide camera optimization midst 2DGS training. Key to our contribution is a novel formulation of splatted color variance along rays, which can be computed efficiently. Reducing this moment in training leads to more accurate shape reconstructions. We demonstrate state-of-the-art performances in the sparse uncalibrated setting in reconstruction and novel view benchmarks based on established multi-view datasets. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨è¡¨é¢å…ƒç´ å–·ç»˜è¿›è¡Œç¨€ç–è§†è§’é‡å»ºçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œæ—¶é—´ä¸è¶…è¿‡3åˆ†é’Ÿã€‚å°½ç®¡å·²æœ‰å°‘æ•°æ–¹æ³•è§£å†³äº†ä»å¸¦å™ªå£°æˆ–æ— å§¿æ€çš„ç¨€ç–ç›¸æœºå­¦ä¹ ç¨€ç–è¾å°„åœºçš„é—®é¢˜ï¼Œä½†åœ¨æ­¤ç¯å¢ƒä¸­ï¼Œå½¢çŠ¶æ¢å¤ä»ç„¶ç›¸å¯¹ç ”ç©¶ä¸è¶³ã€‚å‡ ç§è¾å°„å’Œå½¢çŠ¶å­¦ä¹ çš„æµ‹è¯•æ—¶é—´ä¼˜åŒ–æ–¹æ³•é€šè¿‡æ•°æ®å…ˆéªŒæˆ–ç»“åˆå¤–éƒ¨å•çœ¼å‡ ä½•å…ˆéªŒæ¥è§£å†³ç¨€ç–è®¾å®šçš„å§¿æ€é—®é¢˜ã€‚ä¸ä¹‹ä¸åŒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆä¸”ç®€å•çš„æµç¨‹ï¼Œåˆ©ç”¨æœ€æ–°çš„å•ä¸€3DåŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬åˆ©ç”¨å…¶å„ç§ä»»åŠ¡å¤´ï¼Œç‰¹åˆ«æ˜¯ç‚¹å›¾å’Œç›¸æœºåˆå§‹åŒ–æ¥å®ä¾‹åŒ–è°ƒæ•´æŸçš„äºŒç»´é«˜æ–¯å–·ç»˜ï¼ˆ2DGSï¼‰æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨å›¾åƒå¯¹åº”å…³ç³»æ¥æŒ‡å¯¼åœ¨äºŒç»´GSè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç›¸æœºä¼˜åŒ–ã€‚æˆ‘ä»¬è´¡çŒ®çš„å…³é”®åœ¨äºæ²¿å…‰çº¿å–·ç»˜é¢œè‰²æ–¹å·®çš„æ–°å…¬å¼ï¼Œè¯¥å…¬å¼å¯ä»¥é«˜æ•ˆè®¡ç®—ã€‚åœ¨è®­ç»ƒä¸­å‡å°‘è¿™ä¸€ç‚¹ä¼šå¯¼è‡´æ›´å‡†ç¡®çš„å½¢çŠ¶é‡å»ºã€‚æˆ‘ä»¬åœ¨ç¨€ç–æœªæ ¡å‡†ç¯å¢ƒä¸­å±•ç¤ºäº†æœ€å…ˆè¿›çš„é‡å»ºå’ŒåŸºäºå¤šè§†è§’æ•°æ®é›†çš„æ–°è§†è§’åŸºå‡†æµ‹è¯•æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02178v4">PDF</a> ICCV 2025. Project page :   <a target="_blank" rel="noopener" href="https://shubhendu-jena.github.io/Sparfels-web/">https://shubhendu-jena.github.io/Sparfels-web/</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè¡¨é¢å…ƒç´ æ‹¼è´´æŠ€æœ¯çš„ç¨€ç–è§†è§’é‡å»ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œæ—¶é—´ä¸è¶…è¿‡3åˆ†é’Ÿã€‚å°½ç®¡å·²æœ‰ä¸€äº›æ–¹æ³•è§£å†³äº†ç¨€ç–è¾å°„åœºå­¦ä¹ çš„é—®é¢˜ï¼Œä½†åœ¨å™ªå£°æˆ–æ— å›ºå®šå§¿åŠ¿çš„ç¨€ç–æ‘„åƒæœºæ¡ä»¶ä¸‹ï¼Œå½¢çŠ¶æ¢å¤ä»æ˜¯ç›¸å¯¹æœªè¢«å……åˆ†æ¢ç´¢çš„é¢†åŸŸã€‚æœ¬æ–‡é€šè¿‡é«˜æ•ˆç®€æ´çš„ç®¡é“æµç¨‹æå‡ºè§£å†³æ–¹æ¡ˆï¼Œè¯¥æµç¨‹ä¾èµ–äºå•ä¸ªæœ€æ–°çš„ä¸‰ç»´åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬åˆ©ç”¨å®ƒçš„å¤šä¸ªä»»åŠ¡å¤´éƒ¨ï¼Œç‰¹åˆ«æ˜¯ç‚¹å›¾å’Œç›¸æœºåˆå§‹åŒ–æ¥å»ºç«‹æŸè°ƒæ•´äºŒç»´é«˜æ–¯æ‹¼è´´æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨å›¾åƒå¯¹åº”å…³ç³»æ¥æŒ‡å¯¼ç›¸æœºä¼˜åŒ–ä¸äºŒç»´GSè®­ç»ƒã€‚æœ¬æ–‡çš„å…³é”®è´¡çŒ®æ˜¯æ²¿å°„çº¿æ‹¼è´´é¢œè‰²æ–¹å·®çš„æ–°å…¬å¼ï¼Œè¯¥å…¬å¼å¯ä»¥é«˜æ•ˆè®¡ç®—ã€‚åœ¨è®­ç»ƒä¸­å‡å°‘è¿™ä¸€æ—¶åˆ»ä¼šå¯¼è‡´æ›´å‡†ç¡®çš„å½¢çŠ¶é‡å»ºã€‚æˆ‘ä»¬åœ¨ç¨€ç–æœªæ ¡å‡†è®¾ç½®ä¸­é‡å»ºå’Œæ–°é¢–è§†å›¾åŸºå‡†ä¸Šå±•ç¤ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œè¿™äº›åŸºå‡†åŸºäºå…¬è®¤çš„å¤šè§†è§’æ•°æ®é›†ã€‚</p>
<p><strong>è¦ç‚¹æ‘˜è¦</strong></p>
<p>ä¸€ã€æå‡ºäº†ä¸€ç§åŸºäºè¡¨é¢å…ƒç´ æ‹¼è´´æŠ€æœ¯çš„ç¨€ç–è§†è§’é‡å»ºæ–¹æ³•ï¼Œè¿è¡Œæ—¶é—´çŸ­ã€‚<br>äºŒã€åœ¨æ¶ˆè´¹çº§GPUä¸Šå®ç°ã€‚<br>ä¸‰ã€åœ¨å™ªå£°æˆ–æ— å›ºå®šå§¿åŠ¿çš„ç¨€ç–æ‘„åƒæœºæ¡ä»¶ä¸‹ï¼Œè§£å†³äº†å½¢çŠ¶æ¢å¤çš„éš¾é¢˜ã€‚<br>å››ã€æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆä¸”ç®€æ´çš„ç®¡é“æµç¨‹è§£å†³æ–¹æ¡ˆï¼Œä¾èµ–å•ä¸ªæœ€æ–°çš„ä¸‰ç»´åŸºç¡€æ¨¡å‹ã€‚<br>äº”ã€åˆ©ç”¨ç‚¹å›¾å’Œç›¸æœºåˆå§‹åŒ–ç­‰ä»»åŠ¡å¤´éƒ¨å»ºç«‹æŸè°ƒæ•´äºŒç»´é«˜æ–¯æ‹¼è´´æ¨¡å‹ã€‚<br>å…­ã€é€šè¿‡å›¾åƒå¯¹åº”å…³ç³»æŒ‡å¯¼ç›¸æœºä¼˜åŒ–ä¸äºŒç»´GSè®­ç»ƒã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02178">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-be77ff73478c99374883cd78b9f50e91.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cf25476d8c5619f994d28881ef64681c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a707278172627a1fb74721ec2f325f76.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/3DGS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/3DGS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/3DGS/">
                                    <span class="chip bg-color">3DGS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-25fa6963933556854a815a540ef9bf76.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  NeRF Is a Valuable Assistant for 3D Gaussian Splatting
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ffab9676fc57bdedb660e09e9f610722.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  MoGA 3D Generative Avatar Prior for Monocular Gaussian Avatar   Reconstruction
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
