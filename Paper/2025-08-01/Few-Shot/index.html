<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-08-01  Rule2Text Natural Language Explanation of Logical Rules in Knowledge   Graphs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1107db2ae4fd74f81995026a3cd2516c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    13k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    52 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-01-更新"><a href="#2025-08-01-更新" class="headerlink" title="2025-08-01 更新"></a>2025-08-01 更新</h1><h2 id="Rule2Text-Natural-Language-Explanation-of-Logical-Rules-in-Knowledge-Graphs"><a href="#Rule2Text-Natural-Language-Explanation-of-Logical-Rules-in-Knowledge-Graphs" class="headerlink" title="Rule2Text: Natural Language Explanation of Logical Rules in Knowledge   Graphs"></a>Rule2Text: Natural Language Explanation of Logical Rules in Knowledge   Graphs</h2><p><strong>Authors:Nasim Shirvani-Mahdavi, Devin Wingfield, Amin Ghasemi, Chengkai Li</strong></p>
<p>Knowledge graphs (KGs) often contain sufficient information to support the inference of new facts. Identifying logical rules not only improves the completeness of a knowledge graph but also enables the detection of potential errors, reveals subtle data patterns, and enhances the overall capacity for reasoning and interpretation. However, the complexity of such rules, combined with the unique labeling conventions of each KG, can make them difficult for humans to understand. In this paper, we explore the potential of large language models to generate natural language explanations for logical rules. Specifically, we extract logical rules using the AMIE 3.5.1 rule discovery algorithm from the benchmark dataset FB15k-237 and two large-scale datasets, FB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including zero- and few-shot prompting, including variable entity types, and chain-of-thought reasoning. We conduct a comprehensive human evaluation of the generated explanations based on correctness, clarity, and hallucination, and also assess the use of large language models as automatic judges. Our results demonstrate promising performance in terms of explanation correctness and clarity, although several challenges remain for future research. All scripts and data used in this study are publicly available at <a target="_blank" rel="noopener" href="https://github.com/idirlab/KGRule2NL%7D%7Bhttps://github.com/idirlab/KGRule2NL">https://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL</a>. </p>
<blockquote>
<p>知识图谱（KG）通常包含支持新事实推断的充足信息。识别逻辑规则不仅提高了知识图谱的完整性，还能够检测潜在错误，揭示细微的数据模式，并提高了整体推理和解释能力。然而，这些规则的复杂性以及每个知识图谱独特的标签约定使得人类难以理解。在本文中，我们探索了大型语言模型为逻辑规则生成自然语言解释的能力。具体来说，我们使用AMIE 3.5.1规则发现算法从基准数据集FB1intablea范围内的新下载更新将我的数值保存在a myproject所在目录中更改现有目录下上传的数据模型b户脚本也已被迁移到mytest的新位置或该项目以及两个大规模数据集FB-CVT-REV和FB+CVT-REV中提取逻辑规则。我们研究了各种提示策略，包括零次和一次式提示策略、可变实体类型和思维链推理等。我们对生成的解释进行了基于正确性、清晰度和幻觉的综合人类评估，并对大型语言模型作为自动裁判的使用进行了评估。我们的研究结果在解释的准确性和清晰度方面表现出有前途的性能，尽管仍然存在一些挑战，未来需要进行进一步的研究。本研究所使用的所有脚本和数据均可在<a target="_blank" rel="noopener" href="https://github.com/idirlab/KGRule2NL%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/idirlab/KGRule2NL上公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23740v1">PDF</a> </p>
<p><strong>摘要</strong><br>     本文探讨了大型语言模型在知识图谱逻辑规则解释中的潜力。研究利用AMIE 3.5.1规则发现算法从基准数据集FB15k-237和两个大规模数据集FB-CVT-REV和FB+CVT-REV中提取逻辑规则，并探索了不同的提示策略，包括零样本和少样本提示、可变实体类型和链式思维推理。研究对生成的解释进行了全面的人类评估，评估依据为正确性、清晰度和幻觉现象，同时评估了大型语言模型作为自动裁判的使用情况。研究结果表明，在解释的正确性和清晰度方面表现出有希望的性能，尽管仍存在一些挑战，需要未来进一步研究。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>知识图谱包含支持新事实推断的充足信息，逻辑规则的识别不仅提高了知识图谱的完整性，还有助于检测潜在错误、揭示微妙的数据模式并增强推理和解释的总体能力。</li>
<li>大型语言模型具有生成自然语言解释知识图谱逻辑规则的潜力。</li>
<li>研究使用了AMIE 3.5.1规则发现算法，从基准数据集FB15k-237以及两个大规模数据集FB-CVT-REV和FB+CVT-REV中提取逻辑规则。</li>
<li>研究探索了多种提示策略，包括零样本和少样本提示，考虑了可变实体类型和链式思维推理。</li>
<li>生成解释的人类评估考虑了正确性、清晰度和幻觉现象等方面。</li>
<li>大型语言模型在作为自动裁判方面的使用得到了评估。</li>
<li>尽管在解释的正确性和清晰度方面表现出有希望的结果，但仍存在一些挑战，需要未来的进一步研究。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23740">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c65cf7a10aaaac71c41580b04b3a2ca6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f632b6fd2056565c2f2191dd8acb01c4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DivControl-Knowledge-Diversion-for-Controllable-Image-Generation"><a href="#DivControl-Knowledge-Diversion-for-Controllable-Image-Generation" class="headerlink" title="DivControl: Knowledge Diversion for Controllable Image Generation"></a>DivControl: Knowledge Diversion for Controllable Image Generation</h2><p><strong>Authors:Yucheng Xie, Fu Feng, Ruixiao Shi, Jing Wang, Yong Rui, Xin Geng</strong></p>
<p>Diffusion models have advanced from text-to-image (T2I) to image-to-image (I2I) generation by incorporating structured inputs such as depth maps, enabling fine-grained spatial control. However, existing methods either train separate models for each condition or rely on unified architectures with entangled representations, resulting in poor generalization and high adaptation costs for novel conditions. To this end, we propose DivControl, a decomposable pretraining framework for unified controllable generation and efficient adaptation. DivControl factorizes ControlNet via SVD into basic components-pairs of singular vectors-which are disentangled into condition-agnostic learngenes and condition-specific tailors through knowledge diversion during multi-condition training. Knowledge diversion is implemented via a dynamic gate that performs soft routing over tailors based on the semantics of condition instructions, enabling zero-shot generalization and parameter-efficient adaptation to novel conditions. To further improve condition fidelity and training efficiency, we introduce a representation alignment loss that aligns condition embeddings with early diffusion features. Extensive experiments demonstrate that DivControl achieves state-of-the-art controllability with 36.4$\times$ less training cost, while simultaneously improving average performance on basic conditions. It also delivers strong zero-shot and few-shot performance on unseen conditions, demonstrating superior scalability, modularity, and transferability. </p>
<blockquote>
<p>扩散模型已经通过结合结构化输入（例如深度图）从文本到图像（T2I）的生成发展到图像到图像（I2I）的生成，实现了精细的空间控制。然而，现有方法要么针对每种条件训练单独的模型，要么依赖于具有纠缠表示的统一架构，导致对新颖条件的泛化能力较差和较高的适应成本。为此，我们提出了DivControl，这是一个可分解的预训练框架，用于统一的可控生成和高效适应。DivControl通过SVD将ControlNet进行分解，得到基本组件——一对奇异向量，通过多任务训练中的知识分流将其分离为条件无关的学习基因和条件特定的裁缝。知识分流是通过动态门实现的，该门根据条件指令的语义对裁缝进行软路由选择，实现零样本泛化和对新颖条件的参数高效适应。为了进一步提高条件保真度和训练效率，我们引入了一种表示对齐损失，将条件嵌入与早期扩散特征对齐。大量实验表明，DivControl在减少36.4倍训练成本的同时，实现了最先进的可控性，同时提高了基本条件下的平均性能。它在未见过的条件上表现出强大的零样本和少样本性能，证明了其可扩展性、模块化和可迁移性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23620v1">PDF</a> </p>
<p><strong>Summary</strong><br>扩散模型已从文本到图像（T2I）发展到图像到图像（I2I）生成，通过融入结构化输入如深度图，实现了精细的空间控制。然而，现有方法要么为每种条件训练单独模型，要么依赖于统一架构，导致表征纠缠，对新条件的泛化性能差和适应成本高。为此，我们提出DivControl，一种可分解的预训练框架，用于统一的可控生成和高效适应。DivControl通过SVD将ControlNet分解为基本组件——奇异向量对，并在多条件训练期间通过知识分流将其分解为条件无关的学习基因和条件特定的裁缝。知识分流通过动态门实现，根据条件指令的语义对裁缝进行软路由选择，实现零样本泛化和对新条件的参数高效适应。为提高条件保真度和训练效率，我们引入了表征对齐损失，使条件嵌入与早期扩散特征对齐。实验表明，DivControl实现了先进可控性，减少了36.4倍的训练成本，同时提高了基本条件下的平均性能，并在未见条件下表现出强大的零样本和少样本性能，展现出卓越的可扩展性、模块性和可迁移性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型已从文本到图像生成扩展到图像到图像生成，融入结构化输入如深度图实现精细空间控制。</li>
<li>现有方法存在对新条件泛化性能差和适应成本高的问题。</li>
<li>DivControl框架通过SVD分解ControlNet为基本组件，并通过知识分流实现条件无关的学习和条件特定的适应。</li>
<li>知识分流通过动态门实现软路由选择，提高零样本泛化和对新条件的参数效率。</li>
<li>引入表征对齐损失以提高条件保真度和训练效率。</li>
<li>DivControl实现了先进可控性，并显著减少训练成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23620">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-856b018d780e0f6fffdd810a998144e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62deb42a30faab0d997b4045393e24a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec0fcb208626ea3b1b013fbe1d6b78c7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="H-RDT-Human-Manipulation-Enhanced-Bimanual-Robotic-Manipulation"><a href="#H-RDT-Human-Manipulation-Enhanced-Bimanual-Robotic-Manipulation" class="headerlink" title="H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation"></a>H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation</h2><p><strong>Authors:Hongzhe Bi, Lingxuan Wu, Tianwei Lin, Hengkai Tan, Zhizhong Su, Hang Su, Jun Zhu</strong></p>
<p>Imitation learning for robotic manipulation faces a fundamental challenge: the scarcity of large-scale, high-quality robot demonstration data. Recent robotic foundation models often pre-train on cross-embodiment robot datasets to increase data scale, while they face significant limitations as the diverse morphologies and action spaces across different robot embodiments make unified training challenging. In this paper, we present H-RDT (Human to Robotics Diffusion Transformer), a novel approach that leverages human manipulation data to enhance robot manipulation capabilities. Our key insight is that large-scale egocentric human manipulation videos with paired 3D hand pose annotations provide rich behavioral priors that capture natural manipulation strategies and can benefit robotic policy learning. We introduce a two-stage training paradigm: (1) pre-training on large-scale egocentric human manipulation data, and (2) cross-embodiment fine-tuning on robot-specific data with modular action encoders and decoders. Built on a diffusion transformer architecture with 2B parameters, H-RDT uses flow matching to model complex action distributions. Extensive evaluations encompassing both simulation and real-world experiments, single-task and multitask scenarios, as well as few-shot learning and robustness assessments, demonstrate that H-RDT outperforms training from scratch and existing state-of-the-art methods, including Pi0 and RDT, achieving significant improvements of 13.9% and 40.5% over training from scratch in simulation and real-world experiments, respectively. The results validate our core hypothesis that human manipulation data can serve as a powerful foundation for learning bimanual robotic manipulation policies. </p>
<blockquote>
<p>机器人操纵模仿学习面临一个根本挑战：大规模高质量机器人演示数据的稀缺性。最近的机器人基础模型通常会在跨形态机器人数据集上进行预训练，以增加数据规模，然而，由于不同机器人形态之间的形态多样性和动作空间差异，统一训练面临重大挑战。在本文中，我们提出了H-RDT（人类到机器人的扩散转换器），这是一种利用人类操作数据增强机器人操作能力的新方法。我们的关键见解是，大规模的第一人称人类操作视频配上3D手部姿势注释，提供了丰富的行为先验，能够捕捉自然的操作策略，并有益于机器人策略学习。我们引入了一种两阶段训练模式：（1）在第一人称人类操作数据上进行大规模预训练；（2）使用模块化动作编码器和解码器，在机器人特定数据上进行跨形态微调。H-RDT建立在具有20亿参数的扩散转换器架构上，使用流程匹配来模拟复杂的动作分布。全面的评估包括模拟和真实世界的实验、单任务和多任务场景，以及小样本学习和稳健性评估，结果表明，H-RDT优于从头开始训练和现有的最先进的方法，包括Pi0和RDT，在模拟和真实世界实验中分别比从头开始训练提高了13.9%和40.5%。结果验证了我们的核心假设，即人类操作数据可以作为学习双手机器人操作策略的有力基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23523v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文主要介绍了一种新型机器人操作方法H-RDT，该方法利用人类操作数据增强机器人操作能力。研究的关键见解是，大规模第一人称人类操作视频与配套的3D手势注释提供了丰富的行为先验，捕捉自然操作策略，有益于机器人政策学习。该方法采用两阶段训练模式，首先在大量第一人称人类操作数据上进行预训练，然后在特定机器人数据上进行跨体态微调，使用模块化动作编码器和解码器以及扩散转换器架构。实验结果表明，H-RDT在模拟和真实世界实验、单任务和多任务场景、小样本学习和稳健性评估等方面均优于从头开始训练和现有先进技术方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>H-RDT是一种新型机器人操作方法，利用人类操作数据增强机器人操作能力。</li>
<li>大规模第一人称人类操作视频与配套的3D手势注释提供了丰富的行为先验。</li>
<li>H-RDT采用两阶段训练模式，包括预训练和跨体态微调。</li>
<li>模块化动作编码器和解码器以及扩散转换器架构是H-RDT的关键组成部分。</li>
<li>H-RDT在模拟和真实世界实验中表现出优异性能，显著优于从头开始训练和现有技术方法。</li>
<li>H-RDT在单任务和多任务场景、小样本学习等方面具有优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23523">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cc0690c939ce2f7909d2f1470f0370ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8cfdeb6b6785a9dd986ab3e337b815e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5f57d6a4e7c02d79f106bb2dc4e9a94c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-466daa9457146bde3464889b66654493.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Ambiguity-Guided-Learnable-Distribution-Calibration-for-Semi-Supervised-Few-Shot-Class-Incremental-Learning"><a href="#Ambiguity-Guided-Learnable-Distribution-Calibration-for-Semi-Supervised-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Ambiguity-Guided Learnable Distribution Calibration for Semi-Supervised   Few-Shot Class-Incremental Learning"></a>Ambiguity-Guided Learnable Distribution Calibration for Semi-Supervised   Few-Shot Class-Incremental Learning</h2><p><strong>Authors:Fan Lyu, Linglan Zhao, Chengyan Liu, Yinying Mei, Zhang Zhang, Jian Zhang, Fuyuan Hu, Liang Wang</strong></p>
<p>Few-Shot Class-Incremental Learning (FSCIL) focuses on models learning new concepts from limited data while retaining knowledge of previous classes. Recently, many studies have started to leverage unlabeled samples to assist models in learning from few-shot samples, giving rise to the field of Semi-supervised Few-shot Class-Incremental Learning (Semi-FSCIL). However, these studies often assume that the source of unlabeled data is only confined to novel classes of the current session, which presents a narrow perspective and cannot align well with practical scenarios. To better reflect real-world scenarios, we redefine Semi-FSCIL as Generalized Semi-FSCIL (GSemi-FSCIL) by incorporating both base and all the ever-seen novel classes in the unlabeled set. This change in the composition of unlabeled samples poses a new challenge for existing methods, as they struggle to distinguish between unlabeled samples from base and novel classes. To address this issue, we propose an Ambiguity-guided Learnable Distribution Calibration (ALDC) strategy. ALDC dynamically uses abundant base samples to correct biased feature distributions for few-shot novel classes. Experiments on three benchmark datasets show that our method outperforms existing works, setting new state-of-the-art results. </p>
<blockquote>
<p>少量类别增量学习（FSCIL）关注模型从有限数据中学习新概念的同时，保留对之前类别的知识。最近，许多研究开始利用未标记的样本来帮助模型从少量样本中学习，从而出现了半监督少量类别增量学习（Semi-FSCIL）领域。然而，这些研究通常假设未标记数据的来源仅限于当前会话的新类别，这呈现了一种狭隘的视角，并不能很好地与实际应用场景相符。为了更好地反映真实世界场景，我们通过将基础类别和所有已见过的新类别纳入未标记集，将Semi-FSCIL重新定义为广义Semi-FSCIL（GSemi-FSCIL）。未标记样本组成的这种变化给现有方法带来了新的挑战，因为它们很难区分来自基础类别和新类别的未标记样本。为了解决这一问题，我们提出了一种模糊引导的可学习分布校准（ALDC）策略。ALDC动态使用大量的基础样本纠正少量新类别的特征分布偏差。在三个基准数据集上的实验表明，我们的方法优于现有工作，创下了新的最高性能记录。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23237v1">PDF</a> 6 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>半监督小样本类增量学习（GSemi-FSCIL）重新定义了半监督场景下的少样本类增量学习问题，将基础类和所有已见过的新类的未标记样本纳入未标记集，对现有的方法提出了新的挑战。针对这一问题，提出了基于模糊度引导的可学习分布校准（ALDC）策略，利用丰富的基础样本对少数新类的特征分布进行校正。实验表明，该方法在三个基准数据集上取得了优于现有方法的结果。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>半监督小样本类增量学习（GSemi-FSCIL）扩展了问题的定义，将基础类和所有已见过的新类的未标记样本纳入考虑。</li>
<li>现有方法在区分基础类和新类的未标记样本时面临挑战。</li>
<li>提出了一种基于模糊度引导的可学习分布校准（ALDC）策略来解决上述问题。</li>
<li>ALDC策略利用丰富的基础样本对少数新类的特征分布进行校正。</li>
<li>该方法在三个基准数据集上进行了实验验证。</li>
<li>实验结果表明，该方法在性能上优于现有方法，达到了新的先进水平。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23237">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0924047d3c58e99e2ecd5b11b6e16b07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3e0fc40f1845714c609c404e2c6f851.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bc38c449f39a8983df0228674a42bdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5d819ddd2ee979a15b773d7847b5ca1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c55033190908cbe9c770d4fd64c4b7ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e58dba9079369060dc097e115ca558de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8424b1fad07b95be2e552aecc543d62.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Vocabulary-free-Fine-grained-Visual-Recognition-via-Enriched-Contextually-Grounded-Vision-Language-Model"><a href="#Vocabulary-free-Fine-grained-Visual-Recognition-via-Enriched-Contextually-Grounded-Vision-Language-Model" class="headerlink" title="Vocabulary-free Fine-grained Visual Recognition via Enriched   Contextually Grounded Vision-Language Model"></a>Vocabulary-free Fine-grained Visual Recognition via Enriched   Contextually Grounded Vision-Language Model</h2><p><strong>Authors:Dmitry Demidov, Zaigham Zaheer, Omkar Thawakar, Salman Khan, Fahad Shahbaz Khan</strong></p>
<p>Fine-grained image classification, the task of distinguishing between visually similar subcategories within a broader category (e.g., bird species, car models, flower types), is a challenging computer vision problem. Traditional approaches rely heavily on fixed vocabularies and closed-set classification paradigms, limiting their scalability and adaptability in real-world settings where novel classes frequently emerge. Recent research has demonstrated that combining large language models (LLMs) with vision-language models (VLMs) makes open-set recognition possible without the need for predefined class labels. However, the existing methods are often limited in harnessing the power of LLMs at the classification phase, and also rely heavily on the guessed class names provided by an LLM without thorough analysis and refinement. To address these bottlenecks, we propose our training-free method, Enriched-FineR (or E-FineR for short), which demonstrates state-of-the-art results in fine-grained visual recognition while also offering greater interpretability, highlighting its strong potential in real-world scenarios and new domains where expert annotations are difficult to obtain. Additionally, we demonstrate the application of our proposed approach to zero-shot and few-shot classification, where it demonstrated performance on par with the existing SOTA while being training-free and not requiring human interventions. Overall, our vocabulary-free framework supports the shift in image classification from rigid label prediction to flexible, language-driven understanding, enabling scalable and generalizable systems for real-world applications. Well-documented code is available on <a target="_blank" rel="noopener" href="https://github.com/demidovd98/e-finer">https://github.com/demidovd98/e-finer</a>. </p>
<blockquote>
<p>细粒度图像分类是在一个更广泛的类别中区分视觉上相似的子类别（例如鸟类、汽车型号、花卉类型）的任务，这是一个具有挑战性的计算机视觉问题。传统的方法严重依赖于固定的词汇表和封闭集分类范式，这在现实世界环境中存在局限性，因为新的类别经常会出现。最近的研究表明，将大型语言模型（LLMs）与视觉语言模型（VLMs）相结合，可以在无需预先定义的类别标签的情况下实现开放集识别。然而，现有方法往往无法充分利用LLMs在分类阶段的潜力，并且严重依赖于LLM提供的猜测类名，而没有进行深入的分析和细化。为了解决这些瓶颈，我们提出了无需训练的方法Enriched-FineR（或简称为E-FineR），该方法在细粒度视觉识别方面达到了最先进的技术成果，同时提供了更高的可解释性，突显了其在难以获得专家注释的现实场景和新领域中的强大潜力。此外，我们展示了所提出的方法在零样本和少样本分类中的应用，该方法在无需训练和人为干预的情况下达到了与现有技术相当的性能。总的来说，我们的无词汇表框架支持图像分类从僵化的标签预测转向灵活的语言驱动理解，为实际应用提供了可扩展和通用的系统。相关代码已详细记录在<a target="_blank" rel="noopener" href="https://github.com/demidovd98/e-finer%E4%B8%8A%E3%80%82">https://github.com/demidovd98/e-finer上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23070v1">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了细粒度图像分类中的挑战，包括如何区分广泛类别中的视觉相似子类（如鸟类物种、汽车型号和花卉类型）。传统的分类方法严重依赖于固定词汇表和封闭集分类模式，限制了其在现实世界中的可扩展性和适应性，因为现实世界经常出现新的类别。近期的研究通过将大型语言模型（LLMs）与视觉语言模型（VLMs）相结合，实现了开放式集识别，无需预先定义的类别标签。然而，现有方法往往无法充分利用LLMs在分类阶段的潜力，并严重依赖于LLMs提供的猜测类名，缺乏深入的分析和改进。针对这些瓶颈，本文提出了一种无需训练的Enriched-FineR方法（简称E-FineR），在细粒度视觉识别方面取得了最新成果，并提供了更高的可解释性，突显其在难以获得专家注释的现实世界场景和新领域中的强大潜力。此外，本文展示了该方法在零样本和少样本分类中的应用，其性能与现有技术相当，无需训练且无需人工干预。总体而言，本文的词汇表外框架支持图像分类从刚性标签预测向灵活的语言驱动理解的转变，为实现可扩展和通用的真实世界应用系统提供了可能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>细粒度图像分类是区分广泛类别中视觉相似子类的问题，具有挑战性。</li>
<li>传统方法依赖于固定词汇和封闭集分类模式，限制了其在现实世界的适应性。</li>
<li>结合大型语言模型和视觉语言模型可实现开放式集识别。</li>
<li>现有方法无法充分利用语言模型在分类阶段的潜力并依赖猜测类名。</li>
<li>提出的Enriched-FineR方法无需训练，在细粒度视觉识别方面取得最新成果。</li>
<li>Enriched-FineR提供了更高的可解释性，适用于现实世界场景和新领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23070">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3892788a36ddda3836ac5cbbe74c6857.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-529e7bb5d809d189358488530da8a953.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15c62a8616ca0d3cbbe3ee778d1c35fe.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Empirical-Evaluation-of-Concept-Drift-in-ML-Based-Android-Malware-Detection"><a href="#Empirical-Evaluation-of-Concept-Drift-in-ML-Based-Android-Malware-Detection" class="headerlink" title="Empirical Evaluation of Concept Drift in ML-Based Android Malware   Detection"></a>Empirical Evaluation of Concept Drift in ML-Based Android Malware   Detection</h2><p><strong>Authors:Ahmed Sabbah, Radi Jarrar, Samer Zein, David Mohaisen</strong></p>
<p>Despite outstanding results, machine learning-based Android malware detection models struggle with concept drift, where rapidly evolving malware characteristics degrade model effectiveness. This study examines the impact of concept drift on Android malware detection, evaluating two datasets and nine machine learning and deep learning algorithms, as well as Large Language Models (LLMs). Various feature types–static, dynamic, hybrid, semantic, and image-based–were considered. The results showed that concept drift is widespread and significantly affects model performance. Factors influencing the drift include feature types, data environments, and detection methods. Balancing algorithms helped with class imbalance but did not fully address concept drift, which primarily stems from the dynamic nature of the malware landscape. No strong link was found between the type of algorithm used and concept drift, the impact was relatively minor compared to other variables since hyperparameters were not fine-tuned, and the default algorithm configurations were used. While LLMs using few-shot learning demonstrated promising detection performance, they did not fully mitigate concept drift, highlighting the need for further investigation. </p>
<blockquote>
<p>尽管结果出色，但基于机器学习的Android恶意软件检测模型仍然受到概念漂移的困扰，其中快速演变的恶意软件特征会降低模型的有效性。本研究探讨了概念漂移对Android恶意软件检测的影响，评估了两个数据集和九个机器学习和深度学习算法以及大型语言模型（LLM）。考虑了各种特征类型，包括静态、动态、混合、语义和图像。结果表明，概念漂移普遍存在，严重影响模型性能。影响漂移的因素包括特征类型、数据环境和检测方法。平衡算法有助于解决类不平衡问题，但没有完全解决概念漂移问题，这主要源于恶意软件景观的动态性。没有发现所使用的算法类型与概念漂移之间存在强关联，由于未对超参数进行微调并使用默认算法配置，与其他变量相比，其影响相对较小。虽然使用少量样本学习的LLM显示出有希望的检测性能，但它们并没有完全缓解概念漂移问题，这凸显了需要进一步调查的需要。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22772v1">PDF</a> 18 pages, 12 tables, 14 figures, paper under review</p>
<p><strong>Summary</strong></p>
<p>本文研究了概念漂移对基于机器学习的Android恶意软件检测模型的影响。通过对两个数据集和九种机器学习与深度学习算法以及大型语言模型（LLMs）的评估发现，概念漂移普遍存在并严重影响模型性能。影响概念漂移的因素包括特征类型、数据环境和检测方法。虽然平衡算法有助于解决类别不平衡问题，但未能完全解决概念漂移问题，其主要源于恶意软件景观的动态性。LLMs的少数学习虽表现出良好的检测性能，但未完全缓解概念漂移问题，需要进一步研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>概念漂移在Android恶意软件检测中是普遍存在的，且严重影响模型性能。</li>
<li>特征类型、数据环境和检测方法都是影响概念漂移的重要因素。</li>
<li>平衡算法有助于解决类别不平衡问题，但无法完全解决概念漂移。</li>
<li>概念漂移主要源于恶意软件景观的动态性。</li>
<li>使用的算法类型与概念漂移之间未发现有强烈联系。</li>
<li>大型语言模型（LLMs）的少数学习虽表现出检测潜力，但未能完全缓解概念漂移。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22772">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-2f917111f371689d566de469a9ed6ed2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f7cbcf914e7644af445a4a24a037459.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-535063fed6c2420e27caa8a622229baf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e8d1be5fa58e1e2eae802476cb75c6c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MOVE-Motion-Guided-Few-Shot-Video-Object-Segmentation"><a href="#MOVE-Motion-Guided-Few-Shot-Video-Object-Segmentation" class="headerlink" title="MOVE: Motion-Guided Few-Shot Video Object Segmentation"></a>MOVE: Motion-Guided Few-Shot Video Object Segmentation</h2><p><strong>Authors:Kaining Ying, Hengrui Hu, Henghui Ding</strong></p>
<p>This work addresses motion-guided few-shot video object segmentation (FSVOS), which aims to segment dynamic objects in videos based on a few annotated examples with the same motion patterns. Existing FSVOS datasets and methods typically focus on object categories, which are static attributes that ignore the rich temporal dynamics in videos, limiting their application in scenarios requiring motion understanding. To fill this gap, we introduce MOVE, a large-scale dataset specifically designed for motion-guided FSVOS. Based on MOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different related tasks across 2 experimental settings. Our results reveal that current methods struggle to address motion-guided FSVOS, prompting us to analyze the associated challenges and propose a baseline method, Decoupled Motion Appearance Network (DMA). Experiments demonstrate that our approach achieves superior performance in few shot motion understanding, establishing a solid foundation for future research in this direction. </p>
<blockquote>
<p>本文涉及运动引导的小样本视频对象分割（FSVOS），旨在基于具有相同运动模式的少量注释示例来分割视频中的动态对象。现有的FSVOS数据集和方法通常关注对象类别，这些静态属性忽略了视频中的丰富时间动态，从而限制了它们在需要运动理解场景中的应用。为了填补这一空白，我们引入了专为运动引导FSVOS设计的大规模数据集MOVE。基于MOVE，我们在两个实验环境下全面评估了来自三个不同任务的6种最新方法。结果表明，当前方法难以解决运动引导FSVOS问题，促使我们分析相关挑战并提出基线方法，即解耦运动外观网络（DMA）。实验表明，我们的方法在少量运动理解中取得了卓越的性能，为未来在这一方向的研究奠定了坚实的基础。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22061v1">PDF</a> ICCV 2025, Project Page: <a target="_blank" rel="noopener" href="https://henghuiding.com/MOVE/">https://henghuiding.com/MOVE/</a></p>
<p><strong>Summary</strong><br>视频中的动态物体可通过其运动模式进行识别分割，而现有的数据集和方法多侧重于静态属性的物体类别而忽视视频中的丰富时间动态。为此，我们引入了MOVE数据集并设计了用于运动引导下的少样本视频对象分割（FSVOS）任务。我们评估了多个先进方法和任务在MOVE数据集上的表现，发现现有方法在处理运动引导下的FSVOS时面临挑战。因此，我们提出了一个基线方法——解耦运动外观网络（DMA），在少量样本运动理解上取得了优异性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该研究针对运动引导下的少样本视频对象分割（FSVOS）任务展开研究。</li>
<li>现有FSVOS数据集和方法多侧重于静态属性的物体类别，忽视了视频中的丰富时间动态。</li>
<li>我们引入了MOVE数据集用于运动引导下的FSVOS任务，具有大规模特性。</li>
<li>对多种先进方法和任务在MOVE数据集上的表现进行了评估。</li>
<li>实验结果显示现有方法在处理运动引导下的FSVOS时存在挑战。</li>
<li>提出了一种基线方法——解耦运动外观网络（DMA）。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22061">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8d2fcd8635c31f8f17dd782c4f83822b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87ead665d44e32541af66cae96ef5755.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-390b2af84df1e8ea090051f13ddedcae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8e7aec777c074f58a7a4ab0145b8607.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ac4e150fdf9e3470b17449d9f96f702.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1107db2ae4fd74f81995026a3cd2516c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-153995ac4d932871366d21985bc366ed.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Shallow-Deep-Learning-Can-Still-Excel-in-Fine-Grained-Few-Shot-Learning"><a href="#Shallow-Deep-Learning-Can-Still-Excel-in-Fine-Grained-Few-Shot-Learning" class="headerlink" title="Shallow Deep Learning Can Still Excel in Fine-Grained Few-Shot Learning"></a>Shallow Deep Learning Can Still Excel in Fine-Grained Few-Shot Learning</h2><p><strong>Authors:Chaofei Qi, Chao Ye, Zhitai Liu, Weiyang Lin, Jianbin Qiu</strong></p>
<p>Deep learning has witnessed the extensive utilization across a wide spectrum of domains, including fine-grained few-shot learning (FGFSL) which heavily depends on deep backbones. Nonetheless, shallower deep backbones such as ConvNet-4, are not commonly preferred because they’re prone to extract a larger quantity of non-abstract visual attributes. In this paper, we initially re-evaluate the relationship between network depth and the ability to fully encode few-shot instances, and delve into whether shallow deep architecture could effectuate comparable or superior performance to mainstream deep backbone. Fueled by the inspiration from vanilla ConvNet-4, we introduce a location-aware constellation network (LCN-4), equipped with a cutting-edge location-aware feature clustering module. This module can proficiently encoder and integrate spatial feature fusion, feature clustering, and recessive feature location, thereby significantly minimizing the overall loss. Specifically, we innovatively put forward a general grid position encoding compensation to effectively address the issue of positional information missing during the feature extraction process of specific ordinary convolutions. Additionally, we further propose a general frequency domain location embedding technique to offset for the location loss in clustering features. We have carried out validation procedures on three representative fine-grained few-shot benchmarks. Relevant experiments have established that LCN-4 notably outperforms the ConvNet-4 based State-of-the-Arts and achieves performance that is on par with or superior to most ResNet12-based methods, confirming the correctness of our conjecture. </p>
<blockquote>
<p>深度学习已在多个领域得到广泛应用，其中包括精细的少量学习（FGFSL），这严重依赖于深度骨干网络。然而，较浅的深度骨干网络（如ConvNet-4）通常不受欢迎，因为它们容易提取大量的非抽象视觉属性。在本文中，我们重新评估了网络深度与完全编码少量实例的能力之间的关系，并探讨了浅层深度架构是否能够实现与主流深度骨干网络相当或更好的性能。受普通ConvNet-4的启发，我们引入了一种位置感知星座网络（LCN-4），配备了一种尖端的位置感知特征聚类模块。该模块能够熟练地编码和集成空间特征融合、特征聚类和隐性特征位置，从而显著减少总体损失。具体来说，我们创新地提出了一种通用网格位置编码补偿，以有效解决普通卷积在特征提取过程中缺失位置信息的问题。此外，我们进一步提出了一种通用的频域位置嵌入技术，以补偿聚类特征中的位置损失。我们在三个代表性的精细少量基准测试上进行了验证程序。相关实验表明，LCN-4显著优于基于ConvNet-4的现有技术，并且性能与大多数基于ResNet12的方法相当或更好，这证实了我们的猜想。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22041v1">PDF</a> </p>
<p><strong>Summary</strong><br>    本文探讨了深度学习在精细粒度小样本学习（FGFSL）中的应用，对网络深度与编码小样本实例能力之间的关系进行了重新评估。提出了一种基于位置感知的天秤网络（LCN-4），配备了先进的位置感知特征聚类模块，能有效编码和整合空间特征融合、特征聚类和隐性特征位置，从而显著减少总体损失。通过通用网格位置编码补偿和频率域位置嵌入技术，解决了普通卷积在特征提取过程中丢失位置信息的问题。在三个代表性的小样本精细粒度基准测试上进行了验证，表明LCN-4显著优于基于ConvNet-4的现有技术，性能与基于ResNet12的方法相当或更优。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>探讨了网络深度在精细粒度小样本学习中的重要性。</li>
<li>重新评估了网络深度与编码小样本实例能力之间的关系。</li>
<li>介绍了位置感知的天秤网络（LCN-4），结合了先进的位置感知特征聚类模块。</li>
<li>LCN-4能有效编码和整合空间特征融合、特征聚类和隐性特征位置。</li>
<li>通过通用网格位置编码补偿解决了普通卷积中位置信息丢失的问题。</li>
<li>提出了一种频率域位置嵌入技术，以弥补聚类特征中的位置损失。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22041">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5bf30760da7f6733c4a4681dd08fed60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75e745bd02245fc4777a6a021faba204.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76a981f05e8b1d78076aedd2afc6176d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a2f40b5aef0952da2ae477fca47e602.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-532010c276a07243470dc8d3044d64a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-057b0b952c1e275119d71913c6596e52.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MSGCoOp-Multiple-Semantic-Guided-Context-Optimization-for-Few-Shot-Learning"><a href="#MSGCoOp-Multiple-Semantic-Guided-Context-Optimization-for-Few-Shot-Learning" class="headerlink" title="MSGCoOp: Multiple Semantic-Guided Context Optimization for Few-Shot   Learning"></a>MSGCoOp: Multiple Semantic-Guided Context Optimization for Few-Shot   Learning</h2><p><strong>Authors:Zhaolong Wang, Tongfeng Sun, Mingzheng Du, Yachao Huang</strong></p>
<p>Vision-language pre-trained models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, and prompt learning has emerged as an efficient alternative to full fine-tuning. However, existing methods often struggle with generalization to novel classes, a phenomenon attributed to overfitting on seen classes and forgetting general knowledge. Furthermore, recent approaches that improve generalization often introduce complex architectures or heavy computational overhead. In this paper, we propose a Multiple Semantic-Guided Context Optimization (MSGCoOp) framework to enhance few-shot generalization while maintaining computational efficiency. Our approach leverages an ensemble of parallel learnable context vectors to capture diverse semantic aspects. To enrich these prompts, we introduce a semantic guidance mechanism that aligns them with comprehensive class descriptions automatically generated by a Large Language Model (LLM). Furthermore, a diversity regularization loss encourages the prompts to learn complementary and orthogonal features, preventing them from collapsing into redundant representations. Extensive experiments on 11 benchmark datasets show that MSGCoOp significantly improves performance on base-to-novel generalization, achieving an average harmonic mean improvement of 1.10% over the strong KgCoOp baseline. Our method also demonstrates enhanced robustness in cross-domain generalization tasks. Our code is avaliable at: \href{<a target="_blank" rel="noopener" href="https://github.com/Rain-Bus/MSGCoOp%7D%7Bhttps://github.com/Rain-Bus/MSGCoOp%7D">https://github.com/Rain-Bus/MSGCoOp}{https://github.com/Rain-Bus/MSGCoOp}</a>. </p>
<blockquote>
<p>视觉语言预训练模型（如CLIP）已经展现出惊人的零样本泛化能力，而提示学习已经成为全微调的一个有效替代方案。然而，现有方法在泛化到新类别时往往表现不佳，这种现象被归因于对所见类别的过拟合以及遗忘通用知识。此外，最近一些提高泛化的方法往往引入复杂的架构或大量的计算开销。在本文中，我们提出了一个多重语义引导上下文优化（MSGCoOp）框架，旨在提高少样本泛化能力的同时保持计算效率。我们的方法利用一组并行可学习的上下文向量来捕捉各种语义方面。为了丰富这些提示，我们引入了一种语义引导机制，使其与大型语言模型（LLM）自动生成的全面类别描述相吻合。此外，多样性正则化损失鼓励提示学习互补和正交特征，防止它们陷入冗余表示。在11个基准数据集上的广泛实验表明，MSGCoOp在基础到新颖的泛化方面显著提高性能，相较于强大的KgCoOp基线，平均调和均值提高了1.10%。我们的方法在跨域泛化任务中也表现出增强的稳健性。我们的代码可用在：<a target="_blank" rel="noopener" href="https://github.com/Rain-Bus/MSGCoOp">https://github.com/Rain-Bus/MSGCoOp</a>。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21786v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为MSGCoOp的语义引导上下文优化框架，旨在提高小样本泛化能力同时保持计算效率。通过利用并行学习上下文向量的集合来捕捉多样化的语义方面，并引入语义引导机制使提示与大型语言模型自动生成的全面类别描述对齐。此外，通过多样性正则化损失鼓励提示学习互补和正交特征，防止其陷入冗余表示。在多个基准数据集上的实验表明，MSGCoOp在基础到新颖的泛化任务上取得了显著的性能提升，平均调和平均数较强大的KgCoOp基线提高了1.10%。此外，该方法在跨域泛化任务中也表现出增强的稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MSGCoOp框架旨在提高小样本泛化能力并保持计算效率。</li>
<li>通过利用并行学习上下文向量的集合捕捉多样化的语义方面。</li>
<li>引入语义引导机制，使提示与大型语言模型自动生成的全面类别描述对齐。</li>
<li>多样性正则化损失鼓励提示学习互补和正交特征，避免冗余表示。</li>
<li>MSGCoOp在基础到新颖的泛化任务上取得了显著性能提升。</li>
<li>平均调和平均数较强大的KgCoOp基线提高了1.10%。</li>
<li>MSGCoOp方法在跨域泛化任务中表现出增强的稳健性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21786">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-11fb79cb66f4367173514a573e7f6ba5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52861d4077f7152a1e9bb270f9a6d028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bed97d02037ea59b9f436118160879fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eded70cfa206c5d693caa66f0b224b23.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Vision-Language-Reasoning-for-Satellite-Imagery-via-Verifiable-Rewards"><a href="#Few-Shot-Vision-Language-Reasoning-for-Satellite-Imagery-via-Verifiable-Rewards" class="headerlink" title="Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable   Rewards"></a>Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable   Rewards</h2><p><strong>Authors:Aybora Koksal, A. Aydin Alatan</strong></p>
<p>Recent advances in large language and vision-language models have enabled strong reasoning capabilities, yet they remain impractical for specialized domains like remote sensing, where annotated data is scarce and expensive. We present the first few-shot reinforcement learning with verifiable reward (RLVR) framework for satellite imagery that eliminates the need for caption supervision–relying solely on lightweight, rule-based binary or IoU-based rewards. Adapting the “1-shot RLVR” paradigm from language models to vision-language models, we employ policy-gradient optimization with as few as one curated example to align model outputs for satellite reasoning tasks. Comprehensive experiments across multiple remote sensing benchmarks–including classification, visual question answering, and grounding–show that even a single example yields substantial improvements over the base model. Scaling to 128 examples matches or exceeds models trained on thousands of annotated samples. While the extreme one-shot setting can induce mild, task-specific overfitting, our approach consistently demonstrates robust generalization and efficiency across diverse tasks. Further, we find that prompt design and loss weighting significantly influence training stability and final accuracy. Our method enables cost-effective and data-efficient development of domain-specialist vision-language reasoning models, offering a pragmatic recipe for data-scarce fields: start from a compact VLM, curate a handful of reward-checkable cases, and train via RLVR. </p>
<blockquote>
<p>近期大型语言和视觉语言模型的进步赋予了强大的推理能力，但在遥感等特定领域仍不切实际，因为这些领域缺乏标注数据且价格昂贵。我们首次提出一种无需注释监督的基于可验证奖励的强化学习（RLVR）框架，该框架专门用于卫星图像，仅依赖于轻量级、基于规则的二元奖励或基于IoU的奖励。我们将语言模型的“一次强化学习（RLVR）”范式应用于视觉语言模型，采用策略梯度优化法，仅凭一个精选示例就能对卫星推理任务进行模型输出匹配。跨越多个遥感基准的综合性实验——包括分类、视觉问答和定位——表明，即使在单次学习中，相较于基础模型也有显著的改进。扩展到128个样本的性能与在数千个标注样本上训练的模型相匹配或更好。虽然极端的一次性学习设置可能会导致特定任务的轻微过拟合，但我们的方法在各种任务中始终展现出稳健的泛化能力和效率。此外，我们发现提示设计和损失权重对训练稳定性和最终精度有显著影响。我们的方法实现了领域特定视觉语言推理模型的低成本高效开发，为数据稀缺领域提供了一个务实方案：从紧凑的VLM开始，挑选一些可奖励验证的案例，然后通过RLVR进行训练。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21745v1">PDF</a> ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL). 10   pages, 3 figures, 6 tables. Our model, training code and dataset will be at   <a target="_blank" rel="noopener" href="https://github.com/aybora/FewShotReasoning">https://github.com/aybora/FewShotReasoning</a></p>
<p><strong>Summary</strong></p>
<p>大型语言和视觉语言模型的最新进展赋予了强大的推理能力，但在遥感等特定领域仍不适用，缺乏标注数据且成本高昂。本研究提出了首个无需监督标注的少量强化学习验证奖励（RLVR）框架，用于卫星图像。该框架仅依赖于轻量级、基于规则的二元或IoU奖励，无需字幕监督。通过适应语言模型的“一次强化学习验证奖励”（RLVR）范式，采用基于策略的梯度优化，仅使用一个精选示例即可对齐卫星推理任务模型输出。实验表明，即使在单次示例下，该模型也实现了对基准模型的显著改进。扩展到128个示例与在数千个标注样本上训练的模型相匹配或表现更佳。虽然极端单次训练可能会引发特定任务的过度拟合，但该方法在多样化任务中始终表现出稳健的泛化能力和效率。此外，研究发现提示设计和损失权重对训练稳定性和最终精度有重大影响。该方法为数据稀缺领域提供了经济高效的数据驱动视觉语言推理模型开发实用方案：从紧凑的VLM开始，精选少量可验证奖励的案例，并通过RLVR进行训练。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了一种新的少量强化学习验证奖励（RLVR）框架，适用于卫星图像，无需字幕监督。</li>
<li>适应了语言模型的“一次强化学习验证奖励”（RLVR）范式，并将其应用于视觉语言模型。</li>
<li>通过策略梯度优化，仅使用一个精选示例即可对齐卫星推理任务的模型输出。</li>
<li>实验表明，该方法在多个遥感基准测试中实现了显著改进，即使使用少量示例。</li>
<li>方法的泛化能力和效率在多样化任务中得到了验证。</li>
<li>提示设计和损失权重对训练稳定性和最终精度有重要影响。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21745">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-2c770f7e9c6a350ebdd948b0468571be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-523cadb83435b7fe50117ac9758c77eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9da7cf5bd8e6c3b9062f2351e76d97a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2521a48ab6d958e0c2842dd6e3b642c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="An-h-space-Based-Adversarial-Attack-for-Protection-Against-Few-shot-Personalization"><a href="#An-h-space-Based-Adversarial-Attack-for-Protection-Against-Few-shot-Personalization" class="headerlink" title="An h-space Based Adversarial Attack for Protection Against Few-shot   Personalization"></a>An h-space Based Adversarial Attack for Protection Against Few-shot   Personalization</h2><p><strong>Authors:Xide Xu, Sandesh Kamath, Muhammad Atif Butt, Bogdan Raducanu</strong></p>
<p>The versatility of diffusion models in generating customized images from few samples raises significant privacy concerns, particularly regarding unauthorized modifications of private content. This concerning issue has renewed the efforts in developing protection mechanisms based on adversarial attacks, which generate effective perturbations to poison diffusion models. Our work is motivated by the observation that these models exhibit a high degree of abstraction within their semantic latent space (&#96;h-space’), which encodes critical high-level features for generating coherent and meaningful content. In this paper, we propose a novel anti-customization approach, called HAAD (h-space based Adversarial Attack for Diffusion models), that leverages adversarial attacks to craft perturbations based on the h-space that can efficiently degrade the image generation process. Building upon HAAD, we further introduce a more efficient variant, HAAD-KV, that constructs perturbations solely based on the KV parameters of the h-space. This strategy offers a stronger protection, that is computationally less expensive. Despite their simplicity, our methods outperform state-of-the-art adversarial attacks, highlighting their effectiveness. </p>
<blockquote>
<p>扩散模型的通用性能够从少量样本中生成定制图像，这引发了关于未经授权的私人内容修改的严重隐私担忧。这一令人担忧的问题促使我们基于对抗性攻击发展保护机制，生成能有效干扰扩散模型的扰动。我们的工作受到观察启发，观察到这些模型在其语义潜在空间（h空间）内表现出高度抽象，该空间编码了生成连贯和有意义内容的关键高级特征。在本文中，我们提出了一种新的反定制方法，称为HAAD（基于h空间的扩散模型对抗性攻击），它利用对抗性攻击来基于h空间制造扰动，可以有效地破坏图像生成过程。基于HAAD，我们进一步介绍了一种更高效的变体，即仅基于h空间的KV参数构建扰动的HAAD-KV。该策略提供了更强大的保护，同时计算成本更低。尽管方法简单，但我们的方法优于最新先进的对抗性攻击方法，突出了其有效性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17554v1">PDF</a> 32 pages, 15 figures. Accepted by ACM Multimedia 2025</p>
<p><strong>Summary</strong><br>扩散模型具备从少量样本生成定制化图像的能力，但其多功能性引发了关于未经授权的私人内容修改的重大隐私担忧。本研究受到观察启发，观察到这些模型在其语义潜在空间（h空间）内存在高度抽象，该空间编码了生成连贯和有意义内容的关键高级特征。本文提出了一种新型的反定制方法HAAD（基于h空间的扩散模型对抗攻击法），利用对抗性攻击制造基于h空间的扰动，可有效破坏图像生成过程。在HAAD的基础上，我们进一步引入了更高效的HAAD-KV变体，仅基于h空间的KV参数构建扰动。该方法在保护方面更强大且计算成本更低。尽管方法简单，但我们的方法在性能上优于最先进的对抗性攻击，突显其有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型具备生成定制化图像的能力，引发关于未经授权修改内容的隐私担忧。</li>
<li>语义潜在空间（h空间）在扩散模型中起到关键作用，编码了生成连贯图像的高级特征。</li>
<li>提出了新型反定制方法HAAD，利用对抗性攻击制造基于h空间的扰动。</li>
<li>HAAD的改进版本HAAD-KV更加高效，仅使用h空间的KV参数构建扰动。</li>
<li>HAAD和HAAD-KV方法在计算效率和保护效果方面表现出优势。</li>
<li>与现有对抗性攻击相比，HAAD和HAAD-KV方法展现出更高的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17554">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-cd6f70167903873c81e4c3648cdfbf2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5709e64652c63e2bf5b5cf9c69f6b54d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9854802c4c60c7778189d2f936be1911.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8408dd9d57bde868981d9be807cd232.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CMP-A-Composable-Meta-Prompt-for-SAM-Based-Cross-Domain-Few-Shot-Segmentation"><a href="#CMP-A-Composable-Meta-Prompt-for-SAM-Based-Cross-Domain-Few-Shot-Segmentation" class="headerlink" title="CMP: A Composable Meta Prompt for SAM-Based Cross-Domain Few-Shot   Segmentation"></a>CMP: A Composable Meta Prompt for SAM-Based Cross-Domain Few-Shot   Segmentation</h2><p><strong>Authors:Shuai Chen, Fanman Meng, Chunjin Yang, Haoran Wei, Chenhao Wu, Qingbo Wu, Hongliang Li</strong></p>
<p>Cross-Domain Few-Shot Segmentation (CD-FSS) remains challenging due to limited data and domain shifts. Recent foundation models like the Segment Anything Model (SAM) have shown remarkable zero-shot generalization capability in general segmentation tasks, making it a promising solution for few-shot scenarios. However, adapting SAM to CD-FSS faces two critical challenges: reliance on manual prompt and limited cross-domain ability. Therefore, we propose the Composable Meta-Prompt (CMP) framework that introduces three key modules: (i) the Reference Complement and Transformation (RCT) module for semantic expansion, (ii) the Composable Meta-Prompt Generation (CMPG) module for automated meta-prompt synthesis, and (iii) the Frequency-Aware Interaction (FAI) module for domain discrepancy mitigation. Evaluations across four cross-domain datasets demonstrate CMP’s state-of-the-art performance, achieving 71.8% and 74.5% mIoU in 1-shot and 5-shot scenarios respectively. </p>
<blockquote>
<p>跨域小样本分割（CD-FSS）由于数据有限和领域偏移而仍然具有挑战性。最近的基石模型，如分割任何模型（SAM），在一般分割任务中表现出了惊人的零样本泛化能力，使其成为小样本场景的很有前途的解决方案。然而，将SAM适应于CD-FSS面临两个关键挑战：依赖手动提示和有限的跨域能力。因此，我们提出了可组合元提示（CMP）框架，该框架引入了三个关键模块：（i）参考补全和转换（RCT）模块，用于语义扩展；（ii）可组合元提示生成（CMPG）模块，用于自动元提示合成；（iii）频率感知交互（FAI）模块，用于领域差异缓解。在四个跨域数据集上的评估证明了CMP的卓越性能，在1次和5次拍摄的场景中分别实现了71.8%和74.5%的mIoU。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16753v1">PDF</a> 3 figures</p>
<p><strong>Summary</strong><br>跨域小样本分割（CD-FSS）面临有限数据和域差异的挑战。最近的分割通用模型，如Segment Anything Model（SAM），在一般分割任务中显示出惊人的零样本泛化能力，对于小样本场景具有潜力。然而，将SAM适应于CD-FSS面临两个关键问题：依赖手动提示和有限的跨域能力。因此，我们提出了Composable Meta-Prompt（CMP）框架，引入三个关键模块：用于语义扩展的Reference Complement and Transformation（RCT）模块、用于自动元提示合成的Composable Meta-Prompt Generation（CMPG）模块、以及用于域差异缓解的频率感知交互（FAI）模块。在四个跨域数据集上的评估显示，CMP具有最先进的性能，在1-shot和5-shot场景下分别实现了71.8%和74.5%的mIoU。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CD-FSS面临有限数据和域差异的挑战。</li>
<li>SAM模型在一般分割任务中表现出零样本泛化能力。</li>
<li>将SAM适应于CD-FSS存在两个关键挑战：依赖手动提示和有限的跨域能力。</li>
<li>提出的CMP框架包含三个关键模块：RCT、CMPG和FAI。</li>
<li>CMP框架旨在解决语义扩展、自动元提示合成和域差异缓解的问题。</li>
<li>CMP框架在四个跨域数据集上的评估表现优异，达到最先进的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16753">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-609ffc3331d2ed490546129e60b61e82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-512108a3d0a63097d0bf2f633dedffc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2454b66da8061de60d8c8270d705a5e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2589ce79f418878ee6b83135275cc7d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0df37e9df94d3f5cfdebf8879ddc753.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ec7704fefe16578ed325921a47580aa.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Doodle-Your-Keypoints-Sketch-Based-Few-Shot-Keypoint-Detection"><a href="#Doodle-Your-Keypoints-Sketch-Based-Few-Shot-Keypoint-Detection" class="headerlink" title="Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection"></a>Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection</h2><p><strong>Authors:Subhajit Maity, Ayan Kumar Bhunia, Subhadeep Koley, Pinaki Nath Chowdhury, Aneeshan Sain, Yi-Zhe Song</strong></p>
<p>Keypoint detection, integral to modern machine perception, faces challenges in few-shot learning, particularly when source data from the same distribution as the query is unavailable. This gap is addressed by leveraging sketches, a popular form of human expression, providing a source-free alternative. However, challenges arise in mastering cross-modal embeddings and handling user-specific sketch styles. Our proposed framework overcomes these hurdles with a prototypical setup, combined with a grid-based locator and prototypical domain adaptation. We also demonstrate success in few-shot convergence across novel keypoints and classes through extensive experiments. </p>
<blockquote>
<p>关键点检测是现代机器感知的核心，在少样本学习中面临挑战，尤其是在无法获取与查询相同分布的源数据时。为解决这一空白，我们利用素描这一流行的人类表达方式，提供一种无需源数据的替代方案。然而，掌握跨模态嵌入和处理用户特定的素描风格却存在挑战。我们提出的框架通过结合原型设置、基于网格的定位器和原型域适应，克服了这些障碍。我们还通过大量实验证明了在新关键点和新类别上的少样本收敛的成功。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07994v3">PDF</a> Accepted at ICCV 2025. Project Page: <a target="_blank" rel="noopener" href="https://subhajitmaity.me/DYKp">https://subhajitmaity.me/DYKp</a></p>
<p><strong>Summary</strong>：<br>现代机器感知中的关键点检测在少样本学习中面临挑战，特别是在没有与查询相同分布的源数据的情况下。通过利用人类表达的一种流行形式——草图作为源数据的替代方案来解决这一问题。然而，掌握跨模态嵌入和处理用户特定的草图风格仍存在挑战。提出的框架通过原型设置和基于网格的定位器以及原型域适应来克服这些障碍，并通过广泛的实验证明了在新型关键点和类别上的少样本收敛的成功。</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>现代机器感知中的关键点检测在少样本学习环境下具有挑战。</li>
<li>当缺乏与查询相同的源数据分布时，利用草图作为替代方案来解决这一问题。</li>
<li>掌握跨模态嵌入和处理用户特定草图风格是面临的挑战。</li>
<li>提出的框架采用原型设置来解决这些挑战。</li>
<li>基于网格的定位器有助于处理草图数据。</li>
<li>通过原型域适应来提高模型的适应性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07994">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-b9a6b61914b9819369f3d062535c5be5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6956a2cd400538e97c5a19d28bdc5bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e7800874675488123f5524a13531d87f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69996e32d6d73a0e13c0d807d83a82d2.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4ece756ddbcbb9cb90df7645521e10d1.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-08-01  Explainable Image Classification with Reduced Overconfidence for Tissue   Characterisation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8aa4d08019ca21c0fb06fa5cc13801f1.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT 方向最新论文已更新，请持续关注 Update in 2025-08-01  iLSU-T an Open Dataset for Uruguayan Sign Language Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25691.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
