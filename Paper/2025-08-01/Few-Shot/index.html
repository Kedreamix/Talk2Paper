<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  Rule2Text Natural Language Explanation of Logical Rules in Knowledge   Graphs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1107db2ae4fd74f81995026a3cd2516c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    52 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-01-æ›´æ–°"><a href="#2025-08-01-æ›´æ–°" class="headerlink" title="2025-08-01 æ›´æ–°"></a>2025-08-01 æ›´æ–°</h1><h2 id="Rule2Text-Natural-Language-Explanation-of-Logical-Rules-in-Knowledge-Graphs"><a href="#Rule2Text-Natural-Language-Explanation-of-Logical-Rules-in-Knowledge-Graphs" class="headerlink" title="Rule2Text: Natural Language Explanation of Logical Rules in Knowledge   Graphs"></a>Rule2Text: Natural Language Explanation of Logical Rules in Knowledge   Graphs</h2><p><strong>Authors:Nasim Shirvani-Mahdavi, Devin Wingfield, Amin Ghasemi, Chengkai Li</strong></p>
<p>Knowledge graphs (KGs) often contain sufficient information to support the inference of new facts. Identifying logical rules not only improves the completeness of a knowledge graph but also enables the detection of potential errors, reveals subtle data patterns, and enhances the overall capacity for reasoning and interpretation. However, the complexity of such rules, combined with the unique labeling conventions of each KG, can make them difficult for humans to understand. In this paper, we explore the potential of large language models to generate natural language explanations for logical rules. Specifically, we extract logical rules using the AMIE 3.5.1 rule discovery algorithm from the benchmark dataset FB15k-237 and two large-scale datasets, FB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including zero- and few-shot prompting, including variable entity types, and chain-of-thought reasoning. We conduct a comprehensive human evaluation of the generated explanations based on correctness, clarity, and hallucination, and also assess the use of large language models as automatic judges. Our results demonstrate promising performance in terms of explanation correctness and clarity, although several challenges remain for future research. All scripts and data used in this study are publicly available at <a target="_blank" rel="noopener" href="https://github.com/idirlab/KGRule2NL%7D%7Bhttps://github.com/idirlab/KGRule2NL">https://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL</a>. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰é€šå¸¸åŒ…å«æ”¯æŒæ–°äº‹å®æ¨æ–­çš„å……è¶³ä¿¡æ¯ã€‚è¯†åˆ«é€»è¾‘è§„åˆ™ä¸ä»…æé«˜äº†çŸ¥è¯†å›¾è°±çš„å®Œæ•´æ€§ï¼Œè¿˜èƒ½å¤Ÿæ£€æµ‹æ½œåœ¨é”™è¯¯ï¼Œæ­ç¤ºç»†å¾®çš„æ•°æ®æ¨¡å¼ï¼Œå¹¶æé«˜äº†æ•´ä½“æ¨ç†å’Œè§£é‡Šèƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›è§„åˆ™çš„å¤æ‚æ€§ä»¥åŠæ¯ä¸ªçŸ¥è¯†å›¾è°±ç‹¬ç‰¹çš„æ ‡ç­¾çº¦å®šä½¿å¾—äººç±»éš¾ä»¥ç†è§£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºé€»è¾‘è§„åˆ™ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šçš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨AMIE 3.5.1è§„åˆ™å‘ç°ç®—æ³•ä»åŸºå‡†æ•°æ®é›†FB1intableaèŒƒå›´å†…çš„æ–°ä¸‹è½½æ›´æ–°å°†æˆ‘çš„æ•°å€¼ä¿å­˜åœ¨a myprojectæ‰€åœ¨ç›®å½•ä¸­æ›´æ”¹ç°æœ‰ç›®å½•ä¸‹ä¸Šä¼ çš„æ•°æ®æ¨¡å‹bæˆ·è„šæœ¬ä¹Ÿå·²è¢«è¿ç§»åˆ°mytestçš„æ–°ä½ç½®æˆ–è¯¥é¡¹ç›®ä»¥åŠä¸¤ä¸ªå¤§è§„æ¨¡æ•°æ®é›†FB-CVT-REVå’ŒFB+CVT-REVä¸­æå–é€»è¾‘è§„åˆ™ã€‚æˆ‘ä»¬ç ”ç©¶äº†å„ç§æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶æ¬¡å’Œä¸€æ¬¡å¼æç¤ºç­–ç•¥ã€å¯å˜å®ä½“ç±»å‹å’Œæ€ç»´é“¾æ¨ç†ç­‰ã€‚æˆ‘ä»¬å¯¹ç”Ÿæˆçš„è§£é‡Šè¿›è¡Œäº†åŸºäºæ­£ç¡®æ€§ã€æ¸…æ™°åº¦å’Œå¹»è§‰çš„ç»¼åˆäººç±»è¯„ä¼°ï¼Œå¹¶å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè‡ªåŠ¨è£åˆ¤çš„ä½¿ç”¨è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœåœ¨è§£é‡Šçš„å‡†ç¡®æ€§å’Œæ¸…æ™°åº¦æ–¹é¢è¡¨ç°å‡ºæœ‰å‰é€”çš„æ€§èƒ½ï¼Œå°½ç®¡ä»ç„¶å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼Œæœªæ¥éœ€è¦è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚æœ¬ç ”ç©¶æ‰€ä½¿ç”¨çš„æ‰€æœ‰è„šæœ¬å’Œæ•°æ®å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/idirlab/KGRule2NL%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/idirlab/KGRule2NLä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23740v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>     æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å›¾è°±é€»è¾‘è§„åˆ™è§£é‡Šä¸­çš„æ½œåŠ›ã€‚ç ”ç©¶åˆ©ç”¨AMIE 3.5.1è§„åˆ™å‘ç°ç®—æ³•ä»åŸºå‡†æ•°æ®é›†FB15k-237å’Œä¸¤ä¸ªå¤§è§„æ¨¡æ•°æ®é›†FB-CVT-REVå’ŒFB+CVT-REVä¸­æå–é€»è¾‘è§„åˆ™ï¼Œå¹¶æ¢ç´¢äº†ä¸åŒçš„æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºã€å¯å˜å®ä½“ç±»å‹å’Œé“¾å¼æ€ç»´æ¨ç†ã€‚ç ”ç©¶å¯¹ç”Ÿæˆçš„è§£é‡Šè¿›è¡Œäº†å…¨é¢çš„äººç±»è¯„ä¼°ï¼Œè¯„ä¼°ä¾æ®ä¸ºæ­£ç¡®æ€§ã€æ¸…æ™°åº¦å’Œå¹»è§‰ç°è±¡ï¼ŒåŒæ—¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè‡ªåŠ¨è£åˆ¤çš„ä½¿ç”¨æƒ…å†µã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨è§£é‡Šçš„æ­£ç¡®æ€§å’Œæ¸…æ™°åº¦æ–¹é¢è¡¨ç°å‡ºæœ‰å¸Œæœ›çš„æ€§èƒ½ï¼Œå°½ç®¡ä»å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼Œéœ€è¦æœªæ¥è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>çŸ¥è¯†å›¾è°±åŒ…å«æ”¯æŒæ–°äº‹å®æ¨æ–­çš„å……è¶³ä¿¡æ¯ï¼Œé€»è¾‘è§„åˆ™çš„è¯†åˆ«ä¸ä»…æé«˜äº†çŸ¥è¯†å›¾è°±çš„å®Œæ•´æ€§ï¼Œè¿˜æœ‰åŠ©äºæ£€æµ‹æ½œåœ¨é”™è¯¯ã€æ­ç¤ºå¾®å¦™çš„æ•°æ®æ¨¡å¼å¹¶å¢å¼ºæ¨ç†å’Œè§£é‡Šçš„æ€»ä½“èƒ½åŠ›ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡ŠçŸ¥è¯†å›¾è°±é€»è¾‘è§„åˆ™çš„æ½œåŠ›ã€‚</li>
<li>ç ”ç©¶ä½¿ç”¨äº†AMIE 3.5.1è§„åˆ™å‘ç°ç®—æ³•ï¼Œä»åŸºå‡†æ•°æ®é›†FB15k-237ä»¥åŠä¸¤ä¸ªå¤§è§„æ¨¡æ•°æ®é›†FB-CVT-REVå’ŒFB+CVT-REVä¸­æå–é€»è¾‘è§„åˆ™ã€‚</li>
<li>ç ”ç©¶æ¢ç´¢äº†å¤šç§æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºï¼Œè€ƒè™‘äº†å¯å˜å®ä½“ç±»å‹å’Œé“¾å¼æ€ç»´æ¨ç†ã€‚</li>
<li>ç”Ÿæˆè§£é‡Šçš„äººç±»è¯„ä¼°è€ƒè™‘äº†æ­£ç¡®æ€§ã€æ¸…æ™°åº¦å’Œå¹»è§‰ç°è±¡ç­‰æ–¹é¢ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½œä¸ºè‡ªåŠ¨è£åˆ¤æ–¹é¢çš„ä½¿ç”¨å¾—åˆ°äº†è¯„ä¼°ã€‚</li>
<li>å°½ç®¡åœ¨è§£é‡Šçš„æ­£ç¡®æ€§å’Œæ¸…æ™°åº¦æ–¹é¢è¡¨ç°å‡ºæœ‰å¸Œæœ›çš„ç»“æœï¼Œä½†ä»å­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼Œéœ€è¦æœªæ¥çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c65cf7a10aaaac71c41580b04b3a2ca6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f632b6fd2056565c2f2191dd8acb01c4.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DivControl-Knowledge-Diversion-for-Controllable-Image-Generation"><a href="#DivControl-Knowledge-Diversion-for-Controllable-Image-Generation" class="headerlink" title="DivControl: Knowledge Diversion for Controllable Image Generation"></a>DivControl: Knowledge Diversion for Controllable Image Generation</h2><p><strong>Authors:Yucheng Xie, Fu Feng, Ruixiao Shi, Jing Wang, Yong Rui, Xin Geng</strong></p>
<p>Diffusion models have advanced from text-to-image (T2I) to image-to-image (I2I) generation by incorporating structured inputs such as depth maps, enabling fine-grained spatial control. However, existing methods either train separate models for each condition or rely on unified architectures with entangled representations, resulting in poor generalization and high adaptation costs for novel conditions. To this end, we propose DivControl, a decomposable pretraining framework for unified controllable generation and efficient adaptation. DivControl factorizes ControlNet via SVD into basic components-pairs of singular vectors-which are disentangled into condition-agnostic learngenes and condition-specific tailors through knowledge diversion during multi-condition training. Knowledge diversion is implemented via a dynamic gate that performs soft routing over tailors based on the semantics of condition instructions, enabling zero-shot generalization and parameter-efficient adaptation to novel conditions. To further improve condition fidelity and training efficiency, we introduce a representation alignment loss that aligns condition embeddings with early diffusion features. Extensive experiments demonstrate that DivControl achieves state-of-the-art controllability with 36.4$\times$ less training cost, while simultaneously improving average performance on basic conditions. It also delivers strong zero-shot and few-shot performance on unseen conditions, demonstrating superior scalability, modularity, and transferability. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»é€šè¿‡ç»“åˆç»“æ„åŒ–è¾“å…¥ï¼ˆä¾‹å¦‚æ·±åº¦å›¾ï¼‰ä»æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„ç”Ÿæˆå‘å±•åˆ°å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰çš„ç”Ÿæˆï¼Œå®ç°äº†ç²¾ç»†çš„ç©ºé—´æ§åˆ¶ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆé’ˆå¯¹æ¯ç§æ¡ä»¶è®­ç»ƒå•ç‹¬çš„æ¨¡å‹ï¼Œè¦ä¹ˆä¾èµ–äºå…·æœ‰çº ç¼ è¡¨ç¤ºçš„ç»Ÿä¸€æ¶æ„ï¼Œå¯¼è‡´å¯¹æ–°é¢–æ¡ä»¶çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®å’Œè¾ƒé«˜çš„é€‚åº”æˆæœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DivControlï¼Œè¿™æ˜¯ä¸€ä¸ªå¯åˆ†è§£çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºç»Ÿä¸€çš„å¯æ§ç”Ÿæˆå’Œé«˜æ•ˆé€‚åº”ã€‚DivControlé€šè¿‡SVDå°†ControlNetè¿›è¡Œåˆ†è§£ï¼Œå¾—åˆ°åŸºæœ¬ç»„ä»¶â€”â€”ä¸€å¯¹å¥‡å¼‚å‘é‡ï¼Œé€šè¿‡å¤šä»»åŠ¡è®­ç»ƒä¸­çš„çŸ¥è¯†åˆ†æµå°†å…¶åˆ†ç¦»ä¸ºæ¡ä»¶æ— å…³çš„å­¦ä¹ åŸºå› å’Œæ¡ä»¶ç‰¹å®šçš„è£ç¼ã€‚çŸ¥è¯†åˆ†æµæ˜¯é€šè¿‡åŠ¨æ€é—¨å®ç°çš„ï¼Œè¯¥é—¨æ ¹æ®æ¡ä»¶æŒ‡ä»¤çš„è¯­ä¹‰å¯¹è£ç¼è¿›è¡Œè½¯è·¯ç”±é€‰æ‹©ï¼Œå®ç°é›¶æ ·æœ¬æ³›åŒ–å’Œå¯¹æ–°é¢–æ¡ä»¶çš„å‚æ•°é«˜æ•ˆé€‚åº”ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¡ä»¶ä¿çœŸåº¦å’Œè®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¡¨ç¤ºå¯¹é½æŸå¤±ï¼Œå°†æ¡ä»¶åµŒå…¥ä¸æ—©æœŸæ‰©æ•£ç‰¹å¾å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDivControlåœ¨å‡å°‘36.4å€è®­ç»ƒæˆæœ¬çš„åŒæ—¶ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„å¯æ§æ€§ï¼ŒåŒæ—¶æé«˜äº†åŸºæœ¬æ¡ä»¶ä¸‹çš„å¹³å‡æ€§èƒ½ã€‚å®ƒåœ¨æœªè§è¿‡çš„æ¡ä»¶ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ï¼Œè¯æ˜äº†å…¶å¯æ‰©å±•æ€§ã€æ¨¡å—åŒ–å’Œå¯è¿ç§»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23620v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹å·²ä»æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰å‘å±•åˆ°å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç”Ÿæˆï¼Œé€šè¿‡èå…¥ç»“æ„åŒ–è¾“å…¥å¦‚æ·±åº¦å›¾ï¼Œå®ç°äº†ç²¾ç»†çš„ç©ºé—´æ§åˆ¶ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆä¸ºæ¯ç§æ¡ä»¶è®­ç»ƒå•ç‹¬æ¨¡å‹ï¼Œè¦ä¹ˆä¾èµ–äºç»Ÿä¸€æ¶æ„ï¼Œå¯¼è‡´è¡¨å¾çº ç¼ ï¼Œå¯¹æ–°æ¡ä»¶çš„æ³›åŒ–æ€§èƒ½å·®å’Œé€‚åº”æˆæœ¬é«˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºDivControlï¼Œä¸€ç§å¯åˆ†è§£çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºç»Ÿä¸€çš„å¯æ§ç”Ÿæˆå’Œé«˜æ•ˆé€‚åº”ã€‚DivControlé€šè¿‡SVDå°†ControlNetåˆ†è§£ä¸ºåŸºæœ¬ç»„ä»¶â€”â€”å¥‡å¼‚å‘é‡å¯¹ï¼Œå¹¶åœ¨å¤šæ¡ä»¶è®­ç»ƒæœŸé—´é€šè¿‡çŸ¥è¯†åˆ†æµå°†å…¶åˆ†è§£ä¸ºæ¡ä»¶æ— å…³çš„å­¦ä¹ åŸºå› å’Œæ¡ä»¶ç‰¹å®šçš„è£ç¼ã€‚çŸ¥è¯†åˆ†æµé€šè¿‡åŠ¨æ€é—¨å®ç°ï¼Œæ ¹æ®æ¡ä»¶æŒ‡ä»¤çš„è¯­ä¹‰å¯¹è£ç¼è¿›è¡Œè½¯è·¯ç”±é€‰æ‹©ï¼Œå®ç°é›¶æ ·æœ¬æ³›åŒ–å’Œå¯¹æ–°æ¡ä»¶çš„å‚æ•°é«˜æ•ˆé€‚åº”ã€‚ä¸ºæé«˜æ¡ä»¶ä¿çœŸåº¦å’Œè®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†è¡¨å¾å¯¹é½æŸå¤±ï¼Œä½¿æ¡ä»¶åµŒå…¥ä¸æ—©æœŸæ‰©æ•£ç‰¹å¾å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒDivControlå®ç°äº†å…ˆè¿›å¯æ§æ€§ï¼Œå‡å°‘äº†36.4å€çš„è®­ç»ƒæˆæœ¬ï¼ŒåŒæ—¶æé«˜äº†åŸºæœ¬æ¡ä»¶ä¸‹çš„å¹³å‡æ€§èƒ½ï¼Œå¹¶åœ¨æœªè§æ¡ä»¶ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ï¼Œå±•ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§ã€æ¨¡å—æ€§å’Œå¯è¿ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å·²ä»æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ‰©å±•åˆ°å›¾åƒåˆ°å›¾åƒç”Ÿæˆï¼Œèå…¥ç»“æ„åŒ–è¾“å…¥å¦‚æ·±åº¦å›¾å®ç°ç²¾ç»†ç©ºé—´æ§åˆ¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨å¯¹æ–°æ¡ä»¶æ³›åŒ–æ€§èƒ½å·®å’Œé€‚åº”æˆæœ¬é«˜çš„é—®é¢˜ã€‚</li>
<li>DivControlæ¡†æ¶é€šè¿‡SVDåˆ†è§£ControlNetä¸ºåŸºæœ¬ç»„ä»¶ï¼Œå¹¶é€šè¿‡çŸ¥è¯†åˆ†æµå®ç°æ¡ä»¶æ— å…³çš„å­¦ä¹ å’Œæ¡ä»¶ç‰¹å®šçš„é€‚åº”ã€‚</li>
<li>çŸ¥è¯†åˆ†æµé€šè¿‡åŠ¨æ€é—¨å®ç°è½¯è·¯ç”±é€‰æ‹©ï¼Œæé«˜é›¶æ ·æœ¬æ³›åŒ–å’Œå¯¹æ–°æ¡ä»¶çš„å‚æ•°æ•ˆç‡ã€‚</li>
<li>å¼•å…¥è¡¨å¾å¯¹é½æŸå¤±ä»¥æé«˜æ¡ä»¶ä¿çœŸåº¦å’Œè®­ç»ƒæ•ˆç‡ã€‚</li>
<li>DivControlå®ç°äº†å…ˆè¿›å¯æ§æ€§ï¼Œå¹¶æ˜¾è‘—å‡å°‘è®­ç»ƒæˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23620">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-856b018d780e0f6fffdd810a998144e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62deb42a30faab0d997b4045393e24a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec0fcb208626ea3b1b013fbe1d6b78c7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="H-RDT-Human-Manipulation-Enhanced-Bimanual-Robotic-Manipulation"><a href="#H-RDT-Human-Manipulation-Enhanced-Bimanual-Robotic-Manipulation" class="headerlink" title="H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation"></a>H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation</h2><p><strong>Authors:Hongzhe Bi, Lingxuan Wu, Tianwei Lin, Hengkai Tan, Zhizhong Su, Hang Su, Jun Zhu</strong></p>
<p>Imitation learning for robotic manipulation faces a fundamental challenge: the scarcity of large-scale, high-quality robot demonstration data. Recent robotic foundation models often pre-train on cross-embodiment robot datasets to increase data scale, while they face significant limitations as the diverse morphologies and action spaces across different robot embodiments make unified training challenging. In this paper, we present H-RDT (Human to Robotics Diffusion Transformer), a novel approach that leverages human manipulation data to enhance robot manipulation capabilities. Our key insight is that large-scale egocentric human manipulation videos with paired 3D hand pose annotations provide rich behavioral priors that capture natural manipulation strategies and can benefit robotic policy learning. We introduce a two-stage training paradigm: (1) pre-training on large-scale egocentric human manipulation data, and (2) cross-embodiment fine-tuning on robot-specific data with modular action encoders and decoders. Built on a diffusion transformer architecture with 2B parameters, H-RDT uses flow matching to model complex action distributions. Extensive evaluations encompassing both simulation and real-world experiments, single-task and multitask scenarios, as well as few-shot learning and robustness assessments, demonstrate that H-RDT outperforms training from scratch and existing state-of-the-art methods, including Pi0 and RDT, achieving significant improvements of 13.9% and 40.5% over training from scratch in simulation and real-world experiments, respectively. The results validate our core hypothesis that human manipulation data can serve as a powerful foundation for learning bimanual robotic manipulation policies. </p>
<blockquote>
<p>æœºå™¨äººæ“çºµæ¨¡ä»¿å­¦ä¹ é¢ä¸´ä¸€ä¸ªæ ¹æœ¬æŒ‘æˆ˜ï¼šå¤§è§„æ¨¡é«˜è´¨é‡æœºå™¨äººæ¼”ç¤ºæ•°æ®çš„ç¨€ç¼ºæ€§ã€‚æœ€è¿‘çš„æœºå™¨äººåŸºç¡€æ¨¡å‹é€šå¸¸ä¼šåœ¨è·¨å½¢æ€æœºå™¨äººæ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥å¢åŠ æ•°æ®è§„æ¨¡ï¼Œç„¶è€Œï¼Œç”±äºä¸åŒæœºå™¨äººå½¢æ€ä¹‹é—´çš„å½¢æ€å¤šæ ·æ€§å’ŒåŠ¨ä½œç©ºé—´å·®å¼‚ï¼Œç»Ÿä¸€è®­ç»ƒé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†H-RDTï¼ˆäººç±»åˆ°æœºå™¨äººçš„æ‰©æ•£è½¬æ¢å™¨ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨äººç±»æ“ä½œæ•°æ®å¢å¼ºæœºå™¨äººæ“ä½œèƒ½åŠ›çš„æ–°æ–¹æ³•ã€‚æˆ‘ä»¬çš„å…³é”®è§è§£æ˜¯ï¼Œå¤§è§„æ¨¡çš„ç¬¬ä¸€äººç§°äººç±»æ“ä½œè§†é¢‘é…ä¸Š3Dæ‰‹éƒ¨å§¿åŠ¿æ³¨é‡Šï¼Œæä¾›äº†ä¸°å¯Œçš„è¡Œä¸ºå…ˆéªŒï¼Œèƒ½å¤Ÿæ•æ‰è‡ªç„¶çš„æ“ä½œç­–ç•¥ï¼Œå¹¶æœ‰ç›Šäºæœºå™¨äººç­–ç•¥å­¦ä¹ ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼šï¼ˆ1ï¼‰åœ¨ç¬¬ä¸€äººç§°äººç±»æ“ä½œæ•°æ®ä¸Šè¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒï¼›ï¼ˆ2ï¼‰ä½¿ç”¨æ¨¡å—åŒ–åŠ¨ä½œç¼–ç å™¨å’Œè§£ç å™¨ï¼Œåœ¨æœºå™¨äººç‰¹å®šæ•°æ®ä¸Šè¿›è¡Œè·¨å½¢æ€å¾®è°ƒã€‚H-RDTå»ºç«‹åœ¨å…·æœ‰20äº¿å‚æ•°çš„æ‰©æ•£è½¬æ¢å™¨æ¶æ„ä¸Šï¼Œä½¿ç”¨æµç¨‹åŒ¹é…æ¥æ¨¡æ‹Ÿå¤æ‚çš„åŠ¨ä½œåˆ†å¸ƒã€‚å…¨é¢çš„è¯„ä¼°åŒ…æ‹¬æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œçš„å®éªŒã€å•ä»»åŠ¡å’Œå¤šä»»åŠ¡åœºæ™¯ï¼Œä»¥åŠå°æ ·æœ¬å­¦ä¹ å’Œç¨³å¥æ€§è¯„ä¼°ï¼Œç»“æœè¡¨æ˜ï¼ŒH-RDTä¼˜äºä»å¤´å¼€å§‹è®­ç»ƒå’Œç°æœ‰çš„æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬Pi0å’ŒRDTï¼Œåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­åˆ†åˆ«æ¯”ä»å¤´å¼€å§‹è®­ç»ƒæé«˜äº†13.9%å’Œ40.5%ã€‚ç»“æœéªŒè¯äº†æˆ‘ä»¬çš„æ ¸å¿ƒå‡è®¾ï¼Œå³äººç±»æ“ä½œæ•°æ®å¯ä»¥ä½œä¸ºå­¦ä¹ åŒæ‰‹æœºå™¨äººæ“ä½œç­–ç•¥çš„æœ‰åŠ›åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23523v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ç§æ–°å‹æœºå™¨äººæ“ä½œæ–¹æ³•H-RDTï¼Œè¯¥æ–¹æ³•åˆ©ç”¨äººç±»æ“ä½œæ•°æ®å¢å¼ºæœºå™¨äººæ“ä½œèƒ½åŠ›ã€‚ç ”ç©¶çš„å…³é”®è§è§£æ˜¯ï¼Œå¤§è§„æ¨¡ç¬¬ä¸€äººç§°äººç±»æ“ä½œè§†é¢‘ä¸é…å¥—çš„3Dæ‰‹åŠ¿æ³¨é‡Šæä¾›äº†ä¸°å¯Œçš„è¡Œä¸ºå…ˆéªŒï¼Œæ•æ‰è‡ªç„¶æ“ä½œç­–ç•¥ï¼Œæœ‰ç›Šäºæœºå™¨äººæ”¿ç­–å­¦ä¹ ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼Œé¦–å…ˆåœ¨å¤§é‡ç¬¬ä¸€äººç§°äººç±»æ“ä½œæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨ç‰¹å®šæœºå™¨äººæ•°æ®ä¸Šè¿›è¡Œè·¨ä½“æ€å¾®è°ƒï¼Œä½¿ç”¨æ¨¡å—åŒ–åŠ¨ä½œç¼–ç å™¨å’Œè§£ç å™¨ä»¥åŠæ‰©æ•£è½¬æ¢å™¨æ¶æ„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒH-RDTåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œå®éªŒã€å•ä»»åŠ¡å’Œå¤šä»»åŠ¡åœºæ™¯ã€å°æ ·æœ¬å­¦ä¹ å’Œç¨³å¥æ€§è¯„ä¼°ç­‰æ–¹é¢å‡ä¼˜äºä»å¤´å¼€å§‹è®­ç»ƒå’Œç°æœ‰å…ˆè¿›æŠ€æœ¯æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>H-RDTæ˜¯ä¸€ç§æ–°å‹æœºå™¨äººæ“ä½œæ–¹æ³•ï¼Œåˆ©ç”¨äººç±»æ“ä½œæ•°æ®å¢å¼ºæœºå™¨äººæ“ä½œèƒ½åŠ›ã€‚</li>
<li>å¤§è§„æ¨¡ç¬¬ä¸€äººç§°äººç±»æ“ä½œè§†é¢‘ä¸é…å¥—çš„3Dæ‰‹åŠ¿æ³¨é‡Šæä¾›äº†ä¸°å¯Œçš„è¡Œä¸ºå…ˆéªŒã€‚</li>
<li>H-RDTé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæ¨¡å¼ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒå’Œè·¨ä½“æ€å¾®è°ƒã€‚</li>
<li>æ¨¡å—åŒ–åŠ¨ä½œç¼–ç å™¨å’Œè§£ç å™¨ä»¥åŠæ‰©æ•£è½¬æ¢å™¨æ¶æ„æ˜¯H-RDTçš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚</li>
<li>H-RDTåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ä¸–ç•Œå®éªŒä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºä»å¤´å¼€å§‹è®­ç»ƒå’Œç°æœ‰æŠ€æœ¯æ–¹æ³•ã€‚</li>
<li>H-RDTåœ¨å•ä»»åŠ¡å’Œå¤šä»»åŠ¡åœºæ™¯ã€å°æ ·æœ¬å­¦ä¹ ç­‰æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23523">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cc0690c939ce2f7909d2f1470f0370ea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e8cfdeb6b6785a9dd986ab3e337b815e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5f57d6a4e7c02d79f106bb2dc4e9a94c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-466daa9457146bde3464889b66654493.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Ambiguity-Guided-Learnable-Distribution-Calibration-for-Semi-Supervised-Few-Shot-Class-Incremental-Learning"><a href="#Ambiguity-Guided-Learnable-Distribution-Calibration-for-Semi-Supervised-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Ambiguity-Guided Learnable Distribution Calibration for Semi-Supervised   Few-Shot Class-Incremental Learning"></a>Ambiguity-Guided Learnable Distribution Calibration for Semi-Supervised   Few-Shot Class-Incremental Learning</h2><p><strong>Authors:Fan Lyu, Linglan Zhao, Chengyan Liu, Yinying Mei, Zhang Zhang, Jian Zhang, Fuyuan Hu, Liang Wang</strong></p>
<p>Few-Shot Class-Incremental Learning (FSCIL) focuses on models learning new concepts from limited data while retaining knowledge of previous classes. Recently, many studies have started to leverage unlabeled samples to assist models in learning from few-shot samples, giving rise to the field of Semi-supervised Few-shot Class-Incremental Learning (Semi-FSCIL). However, these studies often assume that the source of unlabeled data is only confined to novel classes of the current session, which presents a narrow perspective and cannot align well with practical scenarios. To better reflect real-world scenarios, we redefine Semi-FSCIL as Generalized Semi-FSCIL (GSemi-FSCIL) by incorporating both base and all the ever-seen novel classes in the unlabeled set. This change in the composition of unlabeled samples poses a new challenge for existing methods, as they struggle to distinguish between unlabeled samples from base and novel classes. To address this issue, we propose an Ambiguity-guided Learnable Distribution Calibration (ALDC) strategy. ALDC dynamically uses abundant base samples to correct biased feature distributions for few-shot novel classes. Experiments on three benchmark datasets show that our method outperforms existing works, setting new state-of-the-art results. </p>
<blockquote>
<p>å°‘é‡ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰å…³æ³¨æ¨¡å‹ä»æœ‰é™æ•°æ®ä¸­å­¦ä¹ æ–°æ¦‚å¿µçš„åŒæ—¶ï¼Œä¿ç•™å¯¹ä¹‹å‰ç±»åˆ«çš„çŸ¥è¯†ã€‚æœ€è¿‘ï¼Œè®¸å¤šç ”ç©¶å¼€å§‹åˆ©ç”¨æœªæ ‡è®°çš„æ ·æœ¬æ¥å¸®åŠ©æ¨¡å‹ä»å°‘é‡æ ·æœ¬ä¸­å­¦ä¹ ï¼Œä»è€Œå‡ºç°äº†åŠç›‘ç£å°‘é‡ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆSemi-FSCILï¼‰é¢†åŸŸã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶é€šå¸¸å‡è®¾æœªæ ‡è®°æ•°æ®çš„æ¥æºä»…é™äºå½“å‰ä¼šè¯çš„æ–°ç±»åˆ«ï¼Œè¿™å‘ˆç°äº†ä¸€ç§ç‹­éš˜çš„è§†è§’ï¼Œå¹¶ä¸èƒ½å¾ˆå¥½åœ°ä¸å®é™…åº”ç”¨åœºæ™¯ç›¸ç¬¦ã€‚ä¸ºäº†æ›´å¥½åœ°åæ˜ çœŸå®ä¸–ç•Œåœºæ™¯ï¼Œæˆ‘ä»¬é€šè¿‡å°†åŸºç¡€ç±»åˆ«å’Œæ‰€æœ‰å·²è§è¿‡çš„æ–°ç±»åˆ«çº³å…¥æœªæ ‡è®°é›†ï¼Œå°†Semi-FSCILé‡æ–°å®šä¹‰ä¸ºå¹¿ä¹‰Semi-FSCILï¼ˆGSemi-FSCILï¼‰ã€‚æœªæ ‡è®°æ ·æœ¬ç»„æˆçš„è¿™ç§å˜åŒ–ç»™ç°æœ‰æ–¹æ³•å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬å¾ˆéš¾åŒºåˆ†æ¥è‡ªåŸºç¡€ç±»åˆ«å’Œæ–°ç±»åˆ«çš„æœªæ ‡è®°æ ·æœ¬ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¨¡ç³Šå¼•å¯¼çš„å¯å­¦ä¹ åˆ†å¸ƒæ ¡å‡†ï¼ˆALDCï¼‰ç­–ç•¥ã€‚ALDCåŠ¨æ€ä½¿ç”¨å¤§é‡çš„åŸºç¡€æ ·æœ¬çº æ­£å°‘é‡æ–°ç±»åˆ«çš„ç‰¹å¾åˆ†å¸ƒåå·®ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰å·¥ä½œï¼Œåˆ›ä¸‹äº†æ–°çš„æœ€é«˜æ€§èƒ½è®°å½•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23237v1">PDF</a> 6 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŠç›‘ç£å°æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆGSemi-FSCILï¼‰é‡æ–°å®šä¹‰äº†åŠç›‘ç£åœºæ™¯ä¸‹çš„å°‘æ ·æœ¬ç±»å¢é‡å­¦ä¹ é—®é¢˜ï¼Œå°†åŸºç¡€ç±»å’Œæ‰€æœ‰å·²è§è¿‡çš„æ–°ç±»çš„æœªæ ‡è®°æ ·æœ¬çº³å…¥æœªæ ‡è®°é›†ï¼Œå¯¹ç°æœ‰çš„æ–¹æ³•æå‡ºäº†æ–°çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åŸºäºæ¨¡ç³Šåº¦å¼•å¯¼çš„å¯å­¦ä¹ åˆ†å¸ƒæ ¡å‡†ï¼ˆALDCï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨ä¸°å¯Œçš„åŸºç¡€æ ·æœ¬å¯¹å°‘æ•°æ–°ç±»çš„ç‰¹å¾åˆ†å¸ƒè¿›è¡Œæ ¡æ­£ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜äºç°æœ‰æ–¹æ³•çš„ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£å°æ ·æœ¬ç±»å¢é‡å­¦ä¹ ï¼ˆGSemi-FSCILï¼‰æ‰©å±•äº†é—®é¢˜çš„å®šä¹‰ï¼Œå°†åŸºç¡€ç±»å’Œæ‰€æœ‰å·²è§è¿‡çš„æ–°ç±»çš„æœªæ ‡è®°æ ·æœ¬çº³å…¥è€ƒè™‘ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨åŒºåˆ†åŸºç¡€ç±»å’Œæ–°ç±»çš„æœªæ ‡è®°æ ·æœ¬æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ¨¡ç³Šåº¦å¼•å¯¼çš„å¯å­¦ä¹ åˆ†å¸ƒæ ¡å‡†ï¼ˆALDCï¼‰ç­–ç•¥æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>ALDCç­–ç•¥åˆ©ç”¨ä¸°å¯Œçš„åŸºç¡€æ ·æœ¬å¯¹å°‘æ•°æ–°ç±»çš„ç‰¹å¾åˆ†å¸ƒè¿›è¡Œæ ¡æ­£ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æ–°çš„å…ˆè¿›æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23237">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0924047d3c58e99e2ecd5b11b6e16b07.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f3e0fc40f1845714c609c404e2c6f851.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5bc38c449f39a8983df0228674a42bdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f5d819ddd2ee979a15b773d7847b5ca1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c55033190908cbe9c770d4fd64c4b7ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e58dba9079369060dc097e115ca558de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d8424b1fad07b95be2e552aecc543d62.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Vocabulary-free-Fine-grained-Visual-Recognition-via-Enriched-Contextually-Grounded-Vision-Language-Model"><a href="#Vocabulary-free-Fine-grained-Visual-Recognition-via-Enriched-Contextually-Grounded-Vision-Language-Model" class="headerlink" title="Vocabulary-free Fine-grained Visual Recognition via Enriched   Contextually Grounded Vision-Language Model"></a>Vocabulary-free Fine-grained Visual Recognition via Enriched   Contextually Grounded Vision-Language Model</h2><p><strong>Authors:Dmitry Demidov, Zaigham Zaheer, Omkar Thawakar, Salman Khan, Fahad Shahbaz Khan</strong></p>
<p>Fine-grained image classification, the task of distinguishing between visually similar subcategories within a broader category (e.g., bird species, car models, flower types), is a challenging computer vision problem. Traditional approaches rely heavily on fixed vocabularies and closed-set classification paradigms, limiting their scalability and adaptability in real-world settings where novel classes frequently emerge. Recent research has demonstrated that combining large language models (LLMs) with vision-language models (VLMs) makes open-set recognition possible without the need for predefined class labels. However, the existing methods are often limited in harnessing the power of LLMs at the classification phase, and also rely heavily on the guessed class names provided by an LLM without thorough analysis and refinement. To address these bottlenecks, we propose our training-free method, Enriched-FineR (or E-FineR for short), which demonstrates state-of-the-art results in fine-grained visual recognition while also offering greater interpretability, highlighting its strong potential in real-world scenarios and new domains where expert annotations are difficult to obtain. Additionally, we demonstrate the application of our proposed approach to zero-shot and few-shot classification, where it demonstrated performance on par with the existing SOTA while being training-free and not requiring human interventions. Overall, our vocabulary-free framework supports the shift in image classification from rigid label prediction to flexible, language-driven understanding, enabling scalable and generalizable systems for real-world applications. Well-documented code is available on <a target="_blank" rel="noopener" href="https://github.com/demidovd98/e-finer">https://github.com/demidovd98/e-finer</a>. </p>
<blockquote>
<p>ç»†ç²’åº¦å›¾åƒåˆ†ç±»æ˜¯åœ¨ä¸€ä¸ªæ›´å¹¿æ³›çš„ç±»åˆ«ä¸­åŒºåˆ†è§†è§‰ä¸Šç›¸ä¼¼çš„å­ç±»åˆ«ï¼ˆä¾‹å¦‚é¸Ÿç±»ã€æ±½è½¦å‹å·ã€èŠ±å‰ç±»å‹ï¼‰çš„ä»»åŠ¡ï¼Œè¿™æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„è®¡ç®—æœºè§†è§‰é—®é¢˜ã€‚ä¼ ç»Ÿçš„æ–¹æ³•ä¸¥é‡ä¾èµ–äºå›ºå®šçš„è¯æ±‡è¡¨å’Œå°é—­é›†åˆ†ç±»èŒƒå¼ï¼Œè¿™åœ¨ç°å®ä¸–ç•Œç¯å¢ƒä¸­å­˜åœ¨å±€é™æ€§ï¼Œå› ä¸ºæ–°çš„ç±»åˆ«ç»å¸¸ä¼šå‡ºç°ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç›¸ç»“åˆï¼Œå¯ä»¥åœ¨æ— éœ€é¢„å…ˆå®šä¹‰çš„ç±»åˆ«æ ‡ç­¾çš„æƒ…å†µä¸‹å®ç°å¼€æ”¾é›†è¯†åˆ«ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å……åˆ†åˆ©ç”¨LLMsåœ¨åˆ†ç±»é˜¶æ®µçš„æ½œåŠ›ï¼Œå¹¶ä¸”ä¸¥é‡ä¾èµ–äºLLMæä¾›çš„çŒœæµ‹ç±»åï¼Œè€Œæ²¡æœ‰è¿›è¡Œæ·±å…¥çš„åˆ†æå’Œç»†åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›ç“¶é¢ˆï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€è®­ç»ƒçš„æ–¹æ³•Enriched-FineRï¼ˆæˆ–ç®€ç§°ä¸ºE-FineRï¼‰ï¼Œè¯¥æ–¹æ³•åœ¨ç»†ç²’åº¦è§†è§‰è¯†åˆ«æ–¹é¢è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æŠ€æœ¯æˆæœï¼ŒåŒæ—¶æä¾›äº†æ›´é«˜çš„å¯è§£é‡Šæ€§ï¼Œçªæ˜¾äº†å…¶åœ¨éš¾ä»¥è·å¾—ä¸“å®¶æ³¨é‡Šçš„ç°å®åœºæ™¯å’Œæ–°é¢†åŸŸä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ‰€æå‡ºçš„æ–¹æ³•åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åˆ†ç±»ä¸­çš„åº”ç”¨ï¼Œè¯¥æ–¹æ³•åœ¨æ— éœ€è®­ç»ƒå’Œäººä¸ºå¹²é¢„çš„æƒ…å†µä¸‹è¾¾åˆ°äº†ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ— è¯æ±‡è¡¨æ¡†æ¶æ”¯æŒå›¾åƒåˆ†ç±»ä»åƒµåŒ–çš„æ ‡ç­¾é¢„æµ‹è½¬å‘çµæ´»çš„è¯­è¨€é©±åŠ¨ç†è§£ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†å¯æ‰©å±•å’Œé€šç”¨çš„ç³»ç»Ÿã€‚ç›¸å…³ä»£ç å·²è¯¦ç»†è®°å½•åœ¨<a target="_blank" rel="noopener" href="https://github.com/demidovd98/e-finer%E4%B8%8A%E3%80%82">https://github.com/demidovd98/e-finerä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23070v1">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç»†ç²’åº¦å›¾åƒåˆ†ç±»ä¸­çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¦‚ä½•åŒºåˆ†å¹¿æ³›ç±»åˆ«ä¸­çš„è§†è§‰ç›¸ä¼¼å­ç±»ï¼ˆå¦‚é¸Ÿç±»ç‰©ç§ã€æ±½è½¦å‹å·å’ŒèŠ±å‰ç±»å‹ï¼‰ã€‚ä¼ ç»Ÿçš„åˆ†ç±»æ–¹æ³•ä¸¥é‡ä¾èµ–äºå›ºå®šè¯æ±‡è¡¨å’Œå°é—­é›†åˆ†ç±»æ¨¡å¼ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œä¸­çš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ï¼Œå› ä¸ºç°å®ä¸–ç•Œç»å¸¸å‡ºç°æ–°çš„ç±»åˆ«ã€‚è¿‘æœŸçš„ç ”ç©¶é€šè¿‡å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç›¸ç»“åˆï¼Œå®ç°äº†å¼€æ”¾å¼é›†è¯†åˆ«ï¼Œæ— éœ€é¢„å…ˆå®šä¹‰çš„ç±»åˆ«æ ‡ç­¾ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€æ— æ³•å……åˆ†åˆ©ç”¨LLMsåœ¨åˆ†ç±»é˜¶æ®µçš„æ½œåŠ›ï¼Œå¹¶ä¸¥é‡ä¾èµ–äºLLMsæä¾›çš„çŒœæµ‹ç±»åï¼Œç¼ºä¹æ·±å…¥çš„åˆ†æå’Œæ”¹è¿›ã€‚é’ˆå¯¹è¿™äº›ç“¶é¢ˆï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„Enriched-FineRæ–¹æ³•ï¼ˆç®€ç§°E-FineRï¼‰ï¼Œåœ¨ç»†ç²’åº¦è§†è§‰è¯†åˆ«æ–¹é¢å–å¾—äº†æœ€æ–°æˆæœï¼Œå¹¶æä¾›äº†æ›´é«˜çš„å¯è§£é‡Šæ€§ï¼Œçªæ˜¾å…¶åœ¨éš¾ä»¥è·å¾—ä¸“å®¶æ³¨é‡Šçš„ç°å®ä¸–ç•Œåœºæ™¯å’Œæ–°é¢†åŸŸä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åˆ†ç±»ä¸­çš„åº”ç”¨ï¼Œå…¶æ€§èƒ½ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“ï¼Œæ— éœ€è®­ç»ƒä¸”æ— éœ€äººå·¥å¹²é¢„ã€‚æ€»ä½“è€Œè¨€ï¼Œæœ¬æ–‡çš„è¯æ±‡è¡¨å¤–æ¡†æ¶æ”¯æŒå›¾åƒåˆ†ç±»ä»åˆšæ€§æ ‡ç­¾é¢„æµ‹å‘çµæ´»çš„è¯­è¨€é©±åŠ¨ç†è§£çš„è½¬å˜ï¼Œä¸ºå®ç°å¯æ‰©å±•å’Œé€šç”¨çš„çœŸå®ä¸–ç•Œåº”ç”¨ç³»ç»Ÿæä¾›äº†å¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»†ç²’åº¦å›¾åƒåˆ†ç±»æ˜¯åŒºåˆ†å¹¿æ³›ç±»åˆ«ä¸­è§†è§‰ç›¸ä¼¼å­ç±»çš„é—®é¢˜ï¼Œå…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå›ºå®šè¯æ±‡å’Œå°é—­é›†åˆ†ç±»æ¨¡å¼ï¼Œé™åˆ¶äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„é€‚åº”æ€§ã€‚</li>
<li>ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹å¯å®ç°å¼€æ”¾å¼é›†è¯†åˆ«ã€‚</li>
<li>ç°æœ‰æ–¹æ³•æ— æ³•å……åˆ†åˆ©ç”¨è¯­è¨€æ¨¡å‹åœ¨åˆ†ç±»é˜¶æ®µçš„æ½œåŠ›å¹¶ä¾èµ–çŒœæµ‹ç±»åã€‚</li>
<li>æå‡ºçš„Enriched-FineRæ–¹æ³•æ— éœ€è®­ç»ƒï¼Œåœ¨ç»†ç²’åº¦è§†è§‰è¯†åˆ«æ–¹é¢å–å¾—æœ€æ–°æˆæœã€‚</li>
<li>Enriched-FineRæä¾›äº†æ›´é«˜çš„å¯è§£é‡Šæ€§ï¼Œé€‚ç”¨äºç°å®ä¸–ç•Œåœºæ™¯å’Œæ–°é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23070">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3892788a36ddda3836ac5cbbe74c6857.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-529e7bb5d809d189358488530da8a953.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15c62a8616ca0d3cbbe3ee778d1c35fe.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Empirical-Evaluation-of-Concept-Drift-in-ML-Based-Android-Malware-Detection"><a href="#Empirical-Evaluation-of-Concept-Drift-in-ML-Based-Android-Malware-Detection" class="headerlink" title="Empirical Evaluation of Concept Drift in ML-Based Android Malware   Detection"></a>Empirical Evaluation of Concept Drift in ML-Based Android Malware   Detection</h2><p><strong>Authors:Ahmed Sabbah, Radi Jarrar, Samer Zein, David Mohaisen</strong></p>
<p>Despite outstanding results, machine learning-based Android malware detection models struggle with concept drift, where rapidly evolving malware characteristics degrade model effectiveness. This study examines the impact of concept drift on Android malware detection, evaluating two datasets and nine machine learning and deep learning algorithms, as well as Large Language Models (LLMs). Various feature typesâ€“static, dynamic, hybrid, semantic, and image-basedâ€“were considered. The results showed that concept drift is widespread and significantly affects model performance. Factors influencing the drift include feature types, data environments, and detection methods. Balancing algorithms helped with class imbalance but did not fully address concept drift, which primarily stems from the dynamic nature of the malware landscape. No strong link was found between the type of algorithm used and concept drift, the impact was relatively minor compared to other variables since hyperparameters were not fine-tuned, and the default algorithm configurations were used. While LLMs using few-shot learning demonstrated promising detection performance, they did not fully mitigate concept drift, highlighting the need for further investigation. </p>
<blockquote>
<p>å°½ç®¡ç»“æœå‡ºè‰²ï¼Œä½†åŸºäºæœºå™¨å­¦ä¹ çš„Androidæ¶æ„è½¯ä»¶æ£€æµ‹æ¨¡å‹ä»ç„¶å—åˆ°æ¦‚å¿µæ¼‚ç§»çš„å›°æ‰°ï¼Œå…¶ä¸­å¿«é€Ÿæ¼”å˜çš„æ¶æ„è½¯ä»¶ç‰¹å¾ä¼šé™ä½æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æ¦‚å¿µæ¼‚ç§»å¯¹Androidæ¶æ„è½¯ä»¶æ£€æµ‹çš„å½±å“ï¼Œè¯„ä¼°äº†ä¸¤ä¸ªæ•°æ®é›†å’Œä¹ä¸ªæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ç®—æ³•ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è€ƒè™‘äº†å„ç§ç‰¹å¾ç±»å‹ï¼ŒåŒ…æ‹¬é™æ€ã€åŠ¨æ€ã€æ··åˆã€è¯­ä¹‰å’Œå›¾åƒã€‚ç»“æœè¡¨æ˜ï¼Œæ¦‚å¿µæ¼‚ç§»æ™®éå­˜åœ¨ï¼Œä¸¥é‡å½±å“æ¨¡å‹æ€§èƒ½ã€‚å½±å“æ¼‚ç§»çš„å› ç´ åŒ…æ‹¬ç‰¹å¾ç±»å‹ã€æ•°æ®ç¯å¢ƒå’Œæ£€æµ‹æ–¹æ³•ã€‚å¹³è¡¡ç®—æ³•æœ‰åŠ©äºè§£å†³ç±»ä¸å¹³è¡¡é—®é¢˜ï¼Œä½†æ²¡æœ‰å®Œå…¨è§£å†³æ¦‚å¿µæ¼‚ç§»é—®é¢˜ï¼Œè¿™ä¸»è¦æºäºæ¶æ„è½¯ä»¶æ™¯è§‚çš„åŠ¨æ€æ€§ã€‚æ²¡æœ‰å‘ç°æ‰€ä½¿ç”¨çš„ç®—æ³•ç±»å‹ä¸æ¦‚å¿µæ¼‚ç§»ä¹‹é—´å­˜åœ¨å¼ºå…³è”ï¼Œç”±äºæœªå¯¹è¶…å‚æ•°è¿›è¡Œå¾®è°ƒå¹¶ä½¿ç”¨é»˜è®¤ç®—æ³•é…ç½®ï¼Œä¸å…¶ä»–å˜é‡ç›¸æ¯”ï¼Œå…¶å½±å“ç›¸å¯¹è¾ƒå°ã€‚è™½ç„¶ä½¿ç”¨å°‘é‡æ ·æœ¬å­¦ä¹ çš„LLMæ˜¾ç¤ºå‡ºæœ‰å¸Œæœ›çš„æ£€æµ‹æ€§èƒ½ï¼Œä½†å®ƒä»¬å¹¶æ²¡æœ‰å®Œå…¨ç¼“è§£æ¦‚å¿µæ¼‚ç§»é—®é¢˜ï¼Œè¿™å‡¸æ˜¾äº†éœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥çš„éœ€è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22772v1">PDF</a> 18 pages, 12 tables, 14 figures, paper under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†æ¦‚å¿µæ¼‚ç§»å¯¹åŸºäºæœºå™¨å­¦ä¹ çš„Androidæ¶æ„è½¯ä»¶æ£€æµ‹æ¨¡å‹çš„å½±å“ã€‚é€šè¿‡å¯¹ä¸¤ä¸ªæ•°æ®é›†å’Œä¹ç§æœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ ç®—æ³•ä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¯„ä¼°å‘ç°ï¼Œæ¦‚å¿µæ¼‚ç§»æ™®éå­˜åœ¨å¹¶ä¸¥é‡å½±å“æ¨¡å‹æ€§èƒ½ã€‚å½±å“æ¦‚å¿µæ¼‚ç§»çš„å› ç´ åŒ…æ‹¬ç‰¹å¾ç±»å‹ã€æ•°æ®ç¯å¢ƒå’Œæ£€æµ‹æ–¹æ³•ã€‚è™½ç„¶å¹³è¡¡ç®—æ³•æœ‰åŠ©äºè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œä½†æœªèƒ½å®Œå…¨è§£å†³æ¦‚å¿µæ¼‚ç§»é—®é¢˜ï¼Œå…¶ä¸»è¦æºäºæ¶æ„è½¯ä»¶æ™¯è§‚çš„åŠ¨æ€æ€§ã€‚LLMsçš„å°‘æ•°å­¦ä¹ è™½è¡¨ç°å‡ºè‰¯å¥½çš„æ£€æµ‹æ€§èƒ½ï¼Œä½†æœªå®Œå…¨ç¼“è§£æ¦‚å¿µæ¼‚ç§»é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¦‚å¿µæ¼‚ç§»åœ¨Androidæ¶æ„è½¯ä»¶æ£€æµ‹ä¸­æ˜¯æ™®éå­˜åœ¨çš„ï¼Œä¸”ä¸¥é‡å½±å“æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç‰¹å¾ç±»å‹ã€æ•°æ®ç¯å¢ƒå’Œæ£€æµ‹æ–¹æ³•éƒ½æ˜¯å½±å“æ¦‚å¿µæ¼‚ç§»çš„é‡è¦å› ç´ ã€‚</li>
<li>å¹³è¡¡ç®—æ³•æœ‰åŠ©äºè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œä½†æ— æ³•å®Œå…¨è§£å†³æ¦‚å¿µæ¼‚ç§»ã€‚</li>
<li>æ¦‚å¿µæ¼‚ç§»ä¸»è¦æºäºæ¶æ„è½¯ä»¶æ™¯è§‚çš„åŠ¨æ€æ€§ã€‚</li>
<li>ä½¿ç”¨çš„ç®—æ³•ç±»å‹ä¸æ¦‚å¿µæ¼‚ç§»ä¹‹é—´æœªå‘ç°æœ‰å¼ºçƒˆè”ç³»ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å°‘æ•°å­¦ä¹ è™½è¡¨ç°å‡ºæ£€æµ‹æ½œåŠ›ï¼Œä½†æœªèƒ½å®Œå…¨ç¼“è§£æ¦‚å¿µæ¼‚ç§»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2f917111f371689d566de469a9ed6ed2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f7cbcf914e7644af445a4a24a037459.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-535063fed6c2420e27caa8a622229baf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8e8d1be5fa58e1e2eae802476cb75c6c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MOVE-Motion-Guided-Few-Shot-Video-Object-Segmentation"><a href="#MOVE-Motion-Guided-Few-Shot-Video-Object-Segmentation" class="headerlink" title="MOVE: Motion-Guided Few-Shot Video Object Segmentation"></a>MOVE: Motion-Guided Few-Shot Video Object Segmentation</h2><p><strong>Authors:Kaining Ying, Hengrui Hu, Henghui Ding</strong></p>
<p>This work addresses motion-guided few-shot video object segmentation (FSVOS), which aims to segment dynamic objects in videos based on a few annotated examples with the same motion patterns. Existing FSVOS datasets and methods typically focus on object categories, which are static attributes that ignore the rich temporal dynamics in videos, limiting their application in scenarios requiring motion understanding. To fill this gap, we introduce MOVE, a large-scale dataset specifically designed for motion-guided FSVOS. Based on MOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different related tasks across 2 experimental settings. Our results reveal that current methods struggle to address motion-guided FSVOS, prompting us to analyze the associated challenges and propose a baseline method, Decoupled Motion Appearance Network (DMA). Experiments demonstrate that our approach achieves superior performance in few shot motion understanding, establishing a solid foundation for future research in this direction. </p>
<blockquote>
<p>æœ¬æ–‡æ¶‰åŠè¿åŠ¨å¼•å¯¼çš„å°æ ·æœ¬è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆFSVOSï¼‰ï¼Œæ—¨åœ¨åŸºäºå…·æœ‰ç›¸åŒè¿åŠ¨æ¨¡å¼çš„å°‘é‡æ³¨é‡Šç¤ºä¾‹æ¥åˆ†å‰²è§†é¢‘ä¸­çš„åŠ¨æ€å¯¹è±¡ã€‚ç°æœ‰çš„FSVOSæ•°æ®é›†å’Œæ–¹æ³•é€šå¸¸å…³æ³¨å¯¹è±¡ç±»åˆ«ï¼Œè¿™äº›é™æ€å±æ€§å¿½ç•¥äº†è§†é¢‘ä¸­çš„ä¸°å¯Œæ—¶é—´åŠ¨æ€ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬åœ¨éœ€è¦è¿åŠ¨ç†è§£åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸“ä¸ºè¿åŠ¨å¼•å¯¼FSVOSè®¾è®¡çš„å¤§è§„æ¨¡æ•°æ®é›†MOVEã€‚åŸºäºMOVEï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªå®éªŒç¯å¢ƒä¸‹å…¨é¢è¯„ä¼°äº†æ¥è‡ªä¸‰ä¸ªä¸åŒä»»åŠ¡çš„6ç§æœ€æ–°æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å‰æ–¹æ³•éš¾ä»¥è§£å†³è¿åŠ¨å¼•å¯¼FSVOSé—®é¢˜ï¼Œä¿ƒä½¿æˆ‘ä»¬åˆ†æç›¸å…³æŒ‘æˆ˜å¹¶æå‡ºåŸºçº¿æ–¹æ³•ï¼Œå³è§£è€¦è¿åŠ¨å¤–è§‚ç½‘ç»œï¼ˆDMAï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å°‘é‡è¿åŠ¨ç†è§£ä¸­å–å¾—äº†å“è¶Šçš„æ€§èƒ½ï¼Œä¸ºæœªæ¥åœ¨è¿™ä¸€æ–¹å‘çš„ç ”ç©¶å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22061v1">PDF</a> ICCV 2025, Project Page: <a target="_blank" rel="noopener" href="https://henghuiding.com/MOVE/">https://henghuiding.com/MOVE/</a></p>
<p><strong>Summary</strong><br>è§†é¢‘ä¸­çš„åŠ¨æ€ç‰©ä½“å¯é€šè¿‡å…¶è¿åŠ¨æ¨¡å¼è¿›è¡Œè¯†åˆ«åˆ†å‰²ï¼Œè€Œç°æœ‰çš„æ•°æ®é›†å’Œæ–¹æ³•å¤šä¾§é‡äºé™æ€å±æ€§çš„ç‰©ä½“ç±»åˆ«è€Œå¿½è§†è§†é¢‘ä¸­çš„ä¸°å¯Œæ—¶é—´åŠ¨æ€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†MOVEæ•°æ®é›†å¹¶è®¾è®¡äº†ç”¨äºè¿åŠ¨å¼•å¯¼ä¸‹çš„å°‘æ ·æœ¬è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆFSVOSï¼‰ä»»åŠ¡ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šä¸ªå…ˆè¿›æ–¹æ³•å’Œä»»åŠ¡åœ¨MOVEæ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼Œå‘ç°ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿åŠ¨å¼•å¯¼ä¸‹çš„FSVOSæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºçº¿æ–¹æ³•â€”â€”è§£è€¦è¿åŠ¨å¤–è§‚ç½‘ç»œï¼ˆDMAï¼‰ï¼Œåœ¨å°‘é‡æ ·æœ¬è¿åŠ¨ç†è§£ä¸Šå–å¾—äº†ä¼˜å¼‚æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶é’ˆå¯¹è¿åŠ¨å¼•å¯¼ä¸‹çš„å°‘æ ·æœ¬è§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆFSVOSï¼‰ä»»åŠ¡å±•å¼€ç ”ç©¶ã€‚</li>
<li>ç°æœ‰FSVOSæ•°æ®é›†å’Œæ–¹æ³•å¤šä¾§é‡äºé™æ€å±æ€§çš„ç‰©ä½“ç±»åˆ«ï¼Œå¿½è§†äº†è§†é¢‘ä¸­çš„ä¸°å¯Œæ—¶é—´åŠ¨æ€ã€‚</li>
<li>æˆ‘ä»¬å¼•å…¥äº†MOVEæ•°æ®é›†ç”¨äºè¿åŠ¨å¼•å¯¼ä¸‹çš„FSVOSä»»åŠ¡ï¼Œå…·æœ‰å¤§è§„æ¨¡ç‰¹æ€§ã€‚</li>
<li>å¯¹å¤šç§å…ˆè¿›æ–¹æ³•å’Œä»»åŠ¡åœ¨MOVEæ•°æ®é›†ä¸Šçš„è¡¨ç°è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿åŠ¨å¼•å¯¼ä¸‹çš„FSVOSæ—¶å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºçº¿æ–¹æ³•â€”â€”è§£è€¦è¿åŠ¨å¤–è§‚ç½‘ç»œï¼ˆDMAï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22061">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8d2fcd8635c31f8f17dd782c4f83822b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-87ead665d44e32541af66cae96ef5755.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-390b2af84df1e8ea090051f13ddedcae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8e7aec777c074f58a7a4ab0145b8607.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ac4e150fdf9e3470b17449d9f96f702.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1107db2ae4fd74f81995026a3cd2516c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-153995ac4d932871366d21985bc366ed.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Shallow-Deep-Learning-Can-Still-Excel-in-Fine-Grained-Few-Shot-Learning"><a href="#Shallow-Deep-Learning-Can-Still-Excel-in-Fine-Grained-Few-Shot-Learning" class="headerlink" title="Shallow Deep Learning Can Still Excel in Fine-Grained Few-Shot Learning"></a>Shallow Deep Learning Can Still Excel in Fine-Grained Few-Shot Learning</h2><p><strong>Authors:Chaofei Qi, Chao Ye, Zhitai Liu, Weiyang Lin, Jianbin Qiu</strong></p>
<p>Deep learning has witnessed the extensive utilization across a wide spectrum of domains, including fine-grained few-shot learning (FGFSL) which heavily depends on deep backbones. Nonetheless, shallower deep backbones such as ConvNet-4, are not commonly preferred because theyâ€™re prone to extract a larger quantity of non-abstract visual attributes. In this paper, we initially re-evaluate the relationship between network depth and the ability to fully encode few-shot instances, and delve into whether shallow deep architecture could effectuate comparable or superior performance to mainstream deep backbone. Fueled by the inspiration from vanilla ConvNet-4, we introduce a location-aware constellation network (LCN-4), equipped with a cutting-edge location-aware feature clustering module. This module can proficiently encoder and integrate spatial feature fusion, feature clustering, and recessive feature location, thereby significantly minimizing the overall loss. Specifically, we innovatively put forward a general grid position encoding compensation to effectively address the issue of positional information missing during the feature extraction process of specific ordinary convolutions. Additionally, we further propose a general frequency domain location embedding technique to offset for the location loss in clustering features. We have carried out validation procedures on three representative fine-grained few-shot benchmarks. Relevant experiments have established that LCN-4 notably outperforms the ConvNet-4 based State-of-the-Arts and achieves performance that is on par with or superior to most ResNet12-based methods, confirming the correctness of our conjecture. </p>
<blockquote>
<p>æ·±åº¦å­¦ä¹ å·²åœ¨å¤šä¸ªé¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œå…¶ä¸­åŒ…æ‹¬ç²¾ç»†çš„å°‘é‡å­¦ä¹ ï¼ˆFGFSLï¼‰ï¼Œè¿™ä¸¥é‡ä¾èµ–äºæ·±åº¦éª¨å¹²ç½‘ç»œã€‚ç„¶è€Œï¼Œè¾ƒæµ…çš„æ·±åº¦éª¨å¹²ç½‘ç»œï¼ˆå¦‚ConvNet-4ï¼‰é€šå¸¸ä¸å—æ¬¢è¿ï¼Œå› ä¸ºå®ƒä»¬å®¹æ˜“æå–å¤§é‡çš„éæŠ½è±¡è§†è§‰å±æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡æ–°è¯„ä¼°äº†ç½‘ç»œæ·±åº¦ä¸å®Œå…¨ç¼–ç å°‘é‡å®ä¾‹çš„èƒ½åŠ›ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶æ¢è®¨äº†æµ…å±‚æ·±åº¦æ¶æ„æ˜¯å¦èƒ½å¤Ÿå®ç°ä¸ä¸»æµæ·±åº¦éª¨å¹²ç½‘ç»œç›¸å½“æˆ–æ›´å¥½çš„æ€§èƒ½ã€‚å—æ™®é€šConvNet-4çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä½ç½®æ„ŸçŸ¥æ˜Ÿåº§ç½‘ç»œï¼ˆLCN-4ï¼‰ï¼Œé…å¤‡äº†ä¸€ç§å°–ç«¯çš„ä½ç½®æ„ŸçŸ¥ç‰¹å¾èšç±»æ¨¡å—ã€‚è¯¥æ¨¡å—èƒ½å¤Ÿç†Ÿç»ƒåœ°ç¼–ç å’Œé›†æˆç©ºé—´ç‰¹å¾èåˆã€ç‰¹å¾èšç±»å’Œéšæ€§ç‰¹å¾ä½ç½®ï¼Œä»è€Œæ˜¾è‘—å‡å°‘æ€»ä½“æŸå¤±ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ›æ–°åœ°æå‡ºäº†ä¸€ç§é€šç”¨ç½‘æ ¼ä½ç½®ç¼–ç è¡¥å¿ï¼Œä»¥æœ‰æ•ˆè§£å†³æ™®é€šå·ç§¯åœ¨ç‰¹å¾æå–è¿‡ç¨‹ä¸­ç¼ºå¤±ä½ç½®ä¿¡æ¯çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§é€šç”¨çš„é¢‘åŸŸä½ç½®åµŒå…¥æŠ€æœ¯ï¼Œä»¥è¡¥å¿èšç±»ç‰¹å¾ä¸­çš„ä½ç½®æŸå¤±ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªä»£è¡¨æ€§çš„ç²¾ç»†å°‘é‡åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†éªŒè¯ç¨‹åºã€‚ç›¸å…³å®éªŒè¡¨æ˜ï¼ŒLCN-4æ˜¾è‘—ä¼˜äºåŸºäºConvNet-4çš„ç°æœ‰æŠ€æœ¯ï¼Œå¹¶ä¸”æ€§èƒ½ä¸å¤§å¤šæ•°åŸºäºResNet12çš„æ–¹æ³•ç›¸å½“æˆ–æ›´å¥½ï¼Œè¿™è¯å®äº†æˆ‘ä»¬çš„çŒœæƒ³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22041v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æ¢è®¨äº†æ·±åº¦å­¦ä¹ åœ¨ç²¾ç»†ç²’åº¦å°æ ·æœ¬å­¦ä¹ ï¼ˆFGFSLï¼‰ä¸­çš„åº”ç”¨ï¼Œå¯¹ç½‘ç»œæ·±åº¦ä¸ç¼–ç å°æ ·æœ¬å®ä¾‹èƒ½åŠ›ä¹‹é—´çš„å…³ç³»è¿›è¡Œäº†é‡æ–°è¯„ä¼°ã€‚æå‡ºäº†ä¸€ç§åŸºäºä½ç½®æ„ŸçŸ¥çš„å¤©ç§¤ç½‘ç»œï¼ˆLCN-4ï¼‰ï¼Œé…å¤‡äº†å…ˆè¿›çš„ä½ç½®æ„ŸçŸ¥ç‰¹å¾èšç±»æ¨¡å—ï¼Œèƒ½æœ‰æ•ˆç¼–ç å’Œæ•´åˆç©ºé—´ç‰¹å¾èåˆã€ç‰¹å¾èšç±»å’Œéšæ€§ç‰¹å¾ä½ç½®ï¼Œä»è€Œæ˜¾è‘—å‡å°‘æ€»ä½“æŸå¤±ã€‚é€šè¿‡é€šç”¨ç½‘æ ¼ä½ç½®ç¼–ç è¡¥å¿å’Œé¢‘ç‡åŸŸä½ç½®åµŒå…¥æŠ€æœ¯ï¼Œè§£å†³äº†æ™®é€šå·ç§¯åœ¨ç‰¹å¾æå–è¿‡ç¨‹ä¸­ä¸¢å¤±ä½ç½®ä¿¡æ¯çš„é—®é¢˜ã€‚åœ¨ä¸‰ä¸ªä»£è¡¨æ€§çš„å°æ ·æœ¬ç²¾ç»†ç²’åº¦åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œè¡¨æ˜LCN-4æ˜¾è‘—ä¼˜äºåŸºäºConvNet-4çš„ç°æœ‰æŠ€æœ¯ï¼Œæ€§èƒ½ä¸åŸºäºResNet12çš„æ–¹æ³•ç›¸å½“æˆ–æ›´ä¼˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¢è®¨äº†ç½‘ç»œæ·±åº¦åœ¨ç²¾ç»†ç²’åº¦å°æ ·æœ¬å­¦ä¹ ä¸­çš„é‡è¦æ€§ã€‚</li>
<li>é‡æ–°è¯„ä¼°äº†ç½‘ç»œæ·±åº¦ä¸ç¼–ç å°æ ·æœ¬å®ä¾‹èƒ½åŠ›ä¹‹é—´çš„å…³ç³»ã€‚</li>
<li>ä»‹ç»äº†ä½ç½®æ„ŸçŸ¥çš„å¤©ç§¤ç½‘ç»œï¼ˆLCN-4ï¼‰ï¼Œç»“åˆäº†å…ˆè¿›çš„ä½ç½®æ„ŸçŸ¥ç‰¹å¾èšç±»æ¨¡å—ã€‚</li>
<li>LCN-4èƒ½æœ‰æ•ˆç¼–ç å’Œæ•´åˆç©ºé—´ç‰¹å¾èåˆã€ç‰¹å¾èšç±»å’Œéšæ€§ç‰¹å¾ä½ç½®ã€‚</li>
<li>é€šè¿‡é€šç”¨ç½‘æ ¼ä½ç½®ç¼–ç è¡¥å¿è§£å†³äº†æ™®é€šå·ç§¯ä¸­ä½ç½®ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é¢‘ç‡åŸŸä½ç½®åµŒå…¥æŠ€æœ¯ï¼Œä»¥å¼¥è¡¥èšç±»ç‰¹å¾ä¸­çš„ä½ç½®æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22041">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5bf30760da7f6733c4a4681dd08fed60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75e745bd02245fc4777a6a021faba204.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-76a981f05e8b1d78076aedd2afc6176d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a2f40b5aef0952da2ae477fca47e602.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-532010c276a07243470dc8d3044d64a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-057b0b952c1e275119d71913c6596e52.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MSGCoOp-Multiple-Semantic-Guided-Context-Optimization-for-Few-Shot-Learning"><a href="#MSGCoOp-Multiple-Semantic-Guided-Context-Optimization-for-Few-Shot-Learning" class="headerlink" title="MSGCoOp: Multiple Semantic-Guided Context Optimization for Few-Shot   Learning"></a>MSGCoOp: Multiple Semantic-Guided Context Optimization for Few-Shot   Learning</h2><p><strong>Authors:Zhaolong Wang, Tongfeng Sun, Mingzheng Du, Yachao Huang</strong></p>
<p>Vision-language pre-trained models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, and prompt learning has emerged as an efficient alternative to full fine-tuning. However, existing methods often struggle with generalization to novel classes, a phenomenon attributed to overfitting on seen classes and forgetting general knowledge. Furthermore, recent approaches that improve generalization often introduce complex architectures or heavy computational overhead. In this paper, we propose a Multiple Semantic-Guided Context Optimization (MSGCoOp) framework to enhance few-shot generalization while maintaining computational efficiency. Our approach leverages an ensemble of parallel learnable context vectors to capture diverse semantic aspects. To enrich these prompts, we introduce a semantic guidance mechanism that aligns them with comprehensive class descriptions automatically generated by a Large Language Model (LLM). Furthermore, a diversity regularization loss encourages the prompts to learn complementary and orthogonal features, preventing them from collapsing into redundant representations. Extensive experiments on 11 benchmark datasets show that MSGCoOp significantly improves performance on base-to-novel generalization, achieving an average harmonic mean improvement of 1.10% over the strong KgCoOp baseline. Our method also demonstrates enhanced robustness in cross-domain generalization tasks. Our code is avaliable at: \href{<a target="_blank" rel="noopener" href="https://github.com/Rain-Bus/MSGCoOp%7D%7Bhttps://github.com/Rain-Bus/MSGCoOp%7D">https://github.com/Rain-Bus/MSGCoOp}{https://github.com/Rain-Bus/MSGCoOp}</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚CLIPï¼‰å·²ç»å±•ç°å‡ºæƒŠäººçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œè€Œæç¤ºå­¦ä¹ å·²ç»æˆä¸ºå…¨å¾®è°ƒçš„ä¸€ä¸ªæœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•åœ¨æ³›åŒ–åˆ°æ–°ç±»åˆ«æ—¶å¾€å¾€è¡¨ç°ä¸ä½³ï¼Œè¿™ç§ç°è±¡è¢«å½’å› äºå¯¹æ‰€è§ç±»åˆ«çš„è¿‡æ‹Ÿåˆä»¥åŠé—å¿˜é€šç”¨çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæœ€è¿‘ä¸€äº›æé«˜æ³›åŒ–çš„æ–¹æ³•å¾€å¾€å¼•å…¥å¤æ‚çš„æ¶æ„æˆ–å¤§é‡çš„è®¡ç®—å¼€é”€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šé‡è¯­ä¹‰å¼•å¯¼ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆMSGCoOpï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å°‘æ ·æœ¬æ³›åŒ–èƒ½åŠ›çš„åŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨ä¸€ç»„å¹¶è¡Œå¯å­¦ä¹ çš„ä¸Šä¸‹æ–‡å‘é‡æ¥æ•æ‰å„ç§è¯­ä¹‰æ–¹é¢ã€‚ä¸ºäº†ä¸°å¯Œè¿™äº›æç¤ºï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¯­ä¹‰å¼•å¯¼æœºåˆ¶ï¼Œä½¿å…¶ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆçš„å…¨é¢ç±»åˆ«æè¿°ç›¸å»åˆã€‚æ­¤å¤–ï¼Œå¤šæ ·æ€§æ­£åˆ™åŒ–æŸå¤±é¼“åŠ±æç¤ºå­¦ä¹ äº’è¡¥å’Œæ­£äº¤ç‰¹å¾ï¼Œé˜²æ­¢å®ƒä»¬é™·å…¥å†—ä½™è¡¨ç¤ºã€‚åœ¨11ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMSGCoOpåœ¨åŸºç¡€åˆ°æ–°é¢–çš„æ³›åŒ–æ–¹é¢æ˜¾è‘—æé«˜æ€§èƒ½ï¼Œç›¸è¾ƒäºå¼ºå¤§çš„KgCoOpåŸºçº¿ï¼Œå¹³å‡è°ƒå’Œå‡å€¼æé«˜äº†1.10%ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è·¨åŸŸæ³›åŒ–ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºå¢å¼ºçš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯ç”¨åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Rain-Bus/MSGCoOp">https://github.com/Rain-Bus/MSGCoOp</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21786v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMSGCoOpçš„è¯­ä¹‰å¼•å¯¼ä¸Šä¸‹æ–‡ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å°æ ·æœ¬æ³›åŒ–èƒ½åŠ›åŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚é€šè¿‡åˆ©ç”¨å¹¶è¡Œå­¦ä¹ ä¸Šä¸‹æ–‡å‘é‡çš„é›†åˆæ¥æ•æ‰å¤šæ ·åŒ–çš„è¯­ä¹‰æ–¹é¢ï¼Œå¹¶å¼•å…¥è¯­ä¹‰å¼•å¯¼æœºåˆ¶ä½¿æç¤ºä¸å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆçš„å…¨é¢ç±»åˆ«æè¿°å¯¹é½ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¤šæ ·æ€§æ­£åˆ™åŒ–æŸå¤±é¼“åŠ±æç¤ºå­¦ä¹ äº’è¡¥å’Œæ­£äº¤ç‰¹å¾ï¼Œé˜²æ­¢å…¶é™·å…¥å†—ä½™è¡¨ç¤ºã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMSGCoOpåœ¨åŸºç¡€åˆ°æ–°é¢–çš„æ³›åŒ–ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œå¹³å‡è°ƒå’Œå¹³å‡æ•°è¾ƒå¼ºå¤§çš„KgCoOpåŸºçº¿æé«˜äº†1.10%ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨è·¨åŸŸæ³›åŒ–ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºå¢å¼ºçš„ç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MSGCoOpæ¡†æ¶æ—¨åœ¨æé«˜å°æ ·æœ¬æ³›åŒ–èƒ½åŠ›å¹¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨å¹¶è¡Œå­¦ä¹ ä¸Šä¸‹æ–‡å‘é‡çš„é›†åˆæ•æ‰å¤šæ ·åŒ–çš„è¯­ä¹‰æ–¹é¢ã€‚</li>
<li>å¼•å…¥è¯­ä¹‰å¼•å¯¼æœºåˆ¶ï¼Œä½¿æç¤ºä¸å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆçš„å…¨é¢ç±»åˆ«æè¿°å¯¹é½ã€‚</li>
<li>å¤šæ ·æ€§æ­£åˆ™åŒ–æŸå¤±é¼“åŠ±æç¤ºå­¦ä¹ äº’è¡¥å’Œæ­£äº¤ç‰¹å¾ï¼Œé¿å…å†—ä½™è¡¨ç¤ºã€‚</li>
<li>MSGCoOpåœ¨åŸºç¡€åˆ°æ–°é¢–çš„æ³›åŒ–ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>å¹³å‡è°ƒå’Œå¹³å‡æ•°è¾ƒå¼ºå¤§çš„KgCoOpåŸºçº¿æé«˜äº†1.10%ã€‚</li>
<li>MSGCoOpæ–¹æ³•åœ¨è·¨åŸŸæ³›åŒ–ä»»åŠ¡ä¸­è¡¨ç°å‡ºå¢å¼ºçš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21786">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-11fb79cb66f4367173514a573e7f6ba5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52861d4077f7152a1e9bb270f9a6d028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bed97d02037ea59b9f436118160879fc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-eded70cfa206c5d693caa66f0b224b23.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Few-Shot-Vision-Language-Reasoning-for-Satellite-Imagery-via-Verifiable-Rewards"><a href="#Few-Shot-Vision-Language-Reasoning-for-Satellite-Imagery-via-Verifiable-Rewards" class="headerlink" title="Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable   Rewards"></a>Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable   Rewards</h2><p><strong>Authors:Aybora Koksal, A. Aydin Alatan</strong></p>
<p>Recent advances in large language and vision-language models have enabled strong reasoning capabilities, yet they remain impractical for specialized domains like remote sensing, where annotated data is scarce and expensive. We present the first few-shot reinforcement learning with verifiable reward (RLVR) framework for satellite imagery that eliminates the need for caption supervisionâ€“relying solely on lightweight, rule-based binary or IoU-based rewards. Adapting the â€œ1-shot RLVRâ€ paradigm from language models to vision-language models, we employ policy-gradient optimization with as few as one curated example to align model outputs for satellite reasoning tasks. Comprehensive experiments across multiple remote sensing benchmarksâ€“including classification, visual question answering, and groundingâ€“show that even a single example yields substantial improvements over the base model. Scaling to 128 examples matches or exceeds models trained on thousands of annotated samples. While the extreme one-shot setting can induce mild, task-specific overfitting, our approach consistently demonstrates robust generalization and efficiency across diverse tasks. Further, we find that prompt design and loss weighting significantly influence training stability and final accuracy. Our method enables cost-effective and data-efficient development of domain-specialist vision-language reasoning models, offering a pragmatic recipe for data-scarce fields: start from a compact VLM, curate a handful of reward-checkable cases, and train via RLVR. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›æ­¥èµ‹äºˆäº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨é¥æ„Ÿç­‰ç‰¹å®šé¢†åŸŸä»ä¸åˆ‡å®é™…ï¼Œå› ä¸ºè¿™äº›é¢†åŸŸç¼ºä¹æ ‡æ³¨æ•°æ®ä¸”ä»·æ ¼æ˜‚è´µã€‚æˆ‘ä»¬é¦–æ¬¡æå‡ºä¸€ç§æ— éœ€æ³¨é‡Šç›‘ç£çš„åŸºäºå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸“é—¨ç”¨äºå«æ˜Ÿå›¾åƒï¼Œä»…ä¾èµ–äºè½»é‡çº§ã€åŸºäºè§„åˆ™çš„äºŒå…ƒå¥–åŠ±æˆ–åŸºäºIoUçš„å¥–åŠ±ã€‚æˆ‘ä»¬å°†è¯­è¨€æ¨¡å‹çš„â€œä¸€æ¬¡å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰â€èŒƒå¼åº”ç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–æ³•ï¼Œä»…å‡­ä¸€ä¸ªç²¾é€‰ç¤ºä¾‹å°±èƒ½å¯¹å«æ˜Ÿæ¨ç†ä»»åŠ¡è¿›è¡Œæ¨¡å‹è¾“å‡ºåŒ¹é…ã€‚è·¨è¶Šå¤šä¸ªé¥æ„ŸåŸºå‡†çš„ç»¼åˆæ€§å®éªŒâ€”â€”åŒ…æ‹¬åˆ†ç±»ã€è§†è§‰é—®ç­”å’Œå®šä½â€”â€”è¡¨æ˜ï¼Œå³ä½¿åœ¨å•æ¬¡å­¦ä¹ ä¸­ï¼Œç›¸è¾ƒäºåŸºç¡€æ¨¡å‹ä¹Ÿæœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚æ‰©å±•åˆ°128ä¸ªæ ·æœ¬çš„æ€§èƒ½ä¸åœ¨æ•°åƒä¸ªæ ‡æ³¨æ ·æœ¬ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸åŒ¹é…æˆ–æ›´å¥½ã€‚è™½ç„¶æç«¯çš„ä¸€æ¬¡æ€§å­¦ä¹ è®¾ç½®å¯èƒ½ä¼šå¯¼è‡´ç‰¹å®šä»»åŠ¡çš„è½»å¾®è¿‡æ‹Ÿåˆï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å„ç§ä»»åŠ¡ä¸­å§‹ç»ˆå±•ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°æç¤ºè®¾è®¡å’ŒæŸå¤±æƒé‡å¯¹è®­ç»ƒç¨³å®šæ€§å’Œæœ€ç»ˆç²¾åº¦æœ‰æ˜¾è‘—å½±å“ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†é¢†åŸŸç‰¹å®šè§†è§‰è¯­è¨€æ¨ç†æ¨¡å‹çš„ä½æˆæœ¬é«˜æ•ˆå¼€å‘ï¼Œä¸ºæ•°æ®ç¨€ç¼ºé¢†åŸŸæä¾›äº†ä¸€ä¸ªåŠ¡å®æ–¹æ¡ˆï¼šä»ç´§å‡‘çš„VLMå¼€å§‹ï¼ŒæŒ‘é€‰ä¸€äº›å¯å¥–åŠ±éªŒè¯çš„æ¡ˆä¾‹ï¼Œç„¶åé€šè¿‡RLVRè¿›è¡Œè®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21745v1">PDF</a> ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL). 10   pages, 3 figures, 6 tables. Our model, training code and dataset will be at   <a target="_blank" rel="noopener" href="https://github.com/aybora/FewShotReasoning">https://github.com/aybora/FewShotReasoning</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€å’Œè§†è§‰è¯­è¨€æ¨¡å‹çš„æœ€æ–°è¿›å±•èµ‹äºˆäº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨é¥æ„Ÿç­‰ç‰¹å®šé¢†åŸŸä»ä¸é€‚ç”¨ï¼Œç¼ºä¹æ ‡æ³¨æ•°æ®ä¸”æˆæœ¬é«˜æ˜‚ã€‚æœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªæ— éœ€ç›‘ç£æ ‡æ³¨çš„å°‘é‡å¼ºåŒ–å­¦ä¹ éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ¡†æ¶ï¼Œç”¨äºå«æ˜Ÿå›¾åƒã€‚è¯¥æ¡†æ¶ä»…ä¾èµ–äºè½»é‡çº§ã€åŸºäºè§„åˆ™çš„äºŒå…ƒæˆ–IoUå¥–åŠ±ï¼Œæ— éœ€å­—å¹•ç›‘ç£ã€‚é€šè¿‡é€‚åº”è¯­è¨€æ¨¡å‹çš„â€œä¸€æ¬¡å¼ºåŒ–å­¦ä¹ éªŒè¯å¥–åŠ±â€ï¼ˆRLVRï¼‰èŒƒå¼ï¼Œé‡‡ç”¨åŸºäºç­–ç•¥çš„æ¢¯åº¦ä¼˜åŒ–ï¼Œä»…ä½¿ç”¨ä¸€ä¸ªç²¾é€‰ç¤ºä¾‹å³å¯å¯¹é½å«æ˜Ÿæ¨ç†ä»»åŠ¡æ¨¡å‹è¾“å‡ºã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨å•æ¬¡ç¤ºä¾‹ä¸‹ï¼Œè¯¥æ¨¡å‹ä¹Ÿå®ç°äº†å¯¹åŸºå‡†æ¨¡å‹çš„æ˜¾è‘—æ”¹è¿›ã€‚æ‰©å±•åˆ°128ä¸ªç¤ºä¾‹ä¸åœ¨æ•°åƒä¸ªæ ‡æ³¨æ ·æœ¬ä¸Šè®­ç»ƒçš„æ¨¡å‹ç›¸åŒ¹é…æˆ–è¡¨ç°æ›´ä½³ã€‚è™½ç„¶æç«¯å•æ¬¡è®­ç»ƒå¯èƒ½ä¼šå¼•å‘ç‰¹å®šä»»åŠ¡çš„è¿‡åº¦æ‹Ÿåˆï¼Œä½†è¯¥æ–¹æ³•åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­å§‹ç»ˆè¡¨ç°å‡ºç¨³å¥çš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æç¤ºè®¾è®¡å’ŒæŸå¤±æƒé‡å¯¹è®­ç»ƒç¨³å®šæ€§å’Œæœ€ç»ˆç²¾åº¦æœ‰é‡å¤§å½±å“ã€‚è¯¥æ–¹æ³•ä¸ºæ•°æ®ç¨€ç¼ºé¢†åŸŸæä¾›äº†ç»æµé«˜æ•ˆçš„æ•°æ®é©±åŠ¨è§†è§‰è¯­è¨€æ¨ç†æ¨¡å‹å¼€å‘å®ç”¨æ–¹æ¡ˆï¼šä»ç´§å‡‘çš„VLMå¼€å§‹ï¼Œç²¾é€‰å°‘é‡å¯éªŒè¯å¥–åŠ±çš„æ¡ˆä¾‹ï¼Œå¹¶é€šè¿‡RLVRè¿›è¡Œè®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å°‘é‡å¼ºåŒ–å­¦ä¹ éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ¡†æ¶ï¼Œé€‚ç”¨äºå«æ˜Ÿå›¾åƒï¼Œæ— éœ€å­—å¹•ç›‘ç£ã€‚</li>
<li>é€‚åº”äº†è¯­è¨€æ¨¡å‹çš„â€œä¸€æ¬¡å¼ºåŒ–å­¦ä¹ éªŒè¯å¥–åŠ±â€ï¼ˆRLVRï¼‰èŒƒå¼ï¼Œå¹¶å°†å…¶åº”ç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é€šè¿‡ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ï¼Œä»…ä½¿ç”¨ä¸€ä¸ªç²¾é€‰ç¤ºä¾‹å³å¯å¯¹é½å«æ˜Ÿæ¨ç†ä»»åŠ¡çš„æ¨¡å‹è¾“å‡ºã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé¥æ„ŸåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—æ”¹è¿›ï¼Œå³ä½¿ä½¿ç”¨å°‘é‡ç¤ºä¾‹ã€‚</li>
<li>æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›å’Œæ•ˆç‡åœ¨å¤šæ ·åŒ–ä»»åŠ¡ä¸­å¾—åˆ°äº†éªŒè¯ã€‚</li>
<li>æç¤ºè®¾è®¡å’ŒæŸå¤±æƒé‡å¯¹è®­ç»ƒç¨³å®šæ€§å’Œæœ€ç»ˆç²¾åº¦æœ‰é‡è¦å½±å“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21745">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2c770f7e9c6a350ebdd948b0468571be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-523cadb83435b7fe50117ac9758c77eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9da7cf5bd8e6c3b9062f2351e76d97a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2521a48ab6d958e0c2842dd6e3b642c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="An-h-space-Based-Adversarial-Attack-for-Protection-Against-Few-shot-Personalization"><a href="#An-h-space-Based-Adversarial-Attack-for-Protection-Against-Few-shot-Personalization" class="headerlink" title="An h-space Based Adversarial Attack for Protection Against Few-shot   Personalization"></a>An h-space Based Adversarial Attack for Protection Against Few-shot   Personalization</h2><p><strong>Authors:Xide Xu, Sandesh Kamath, Muhammad Atif Butt, Bogdan Raducanu</strong></p>
<p>The versatility of diffusion models in generating customized images from few samples raises significant privacy concerns, particularly regarding unauthorized modifications of private content. This concerning issue has renewed the efforts in developing protection mechanisms based on adversarial attacks, which generate effective perturbations to poison diffusion models. Our work is motivated by the observation that these models exhibit a high degree of abstraction within their semantic latent space (&#96;h-spaceâ€™), which encodes critical high-level features for generating coherent and meaningful content. In this paper, we propose a novel anti-customization approach, called HAAD (h-space based Adversarial Attack for Diffusion models), that leverages adversarial attacks to craft perturbations based on the h-space that can efficiently degrade the image generation process. Building upon HAAD, we further introduce a more efficient variant, HAAD-KV, that constructs perturbations solely based on the KV parameters of the h-space. This strategy offers a stronger protection, that is computationally less expensive. Despite their simplicity, our methods outperform state-of-the-art adversarial attacks, highlighting their effectiveness. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹çš„é€šç”¨æ€§èƒ½å¤Ÿä»å°‘é‡æ ·æœ¬ä¸­ç”Ÿæˆå®šåˆ¶å›¾åƒï¼Œè¿™å¼•å‘äº†å…³äºæœªç»æˆæƒçš„ç§äººå†…å®¹ä¿®æ”¹çš„ä¸¥é‡éšç§æ‹…å¿§ã€‚è¿™ä¸€ä»¤äººæ‹…å¿§çš„é—®é¢˜ä¿ƒä½¿æˆ‘ä»¬åŸºäºå¯¹æŠ—æ€§æ”»å‡»å‘å±•ä¿æŠ¤æœºåˆ¶ï¼Œç”Ÿæˆèƒ½æœ‰æ•ˆå¹²æ‰°æ‰©æ•£æ¨¡å‹çš„æ‰°åŠ¨ã€‚æˆ‘ä»¬çš„å·¥ä½œå—åˆ°è§‚å¯Ÿå¯å‘ï¼Œè§‚å¯Ÿåˆ°è¿™äº›æ¨¡å‹åœ¨å…¶è¯­ä¹‰æ½œåœ¨ç©ºé—´ï¼ˆhç©ºé—´ï¼‰å†…è¡¨ç°å‡ºé«˜åº¦æŠ½è±¡ï¼Œè¯¥ç©ºé—´ç¼–ç äº†ç”Ÿæˆè¿è´¯å’Œæœ‰æ„ä¹‰å†…å®¹çš„å…³é”®é«˜çº§ç‰¹å¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åå®šåˆ¶æ–¹æ³•ï¼Œç§°ä¸ºHAADï¼ˆåŸºäºhç©ºé—´çš„æ‰©æ•£æ¨¡å‹å¯¹æŠ—æ€§æ”»å‡»ï¼‰ï¼Œå®ƒåˆ©ç”¨å¯¹æŠ—æ€§æ”»å‡»æ¥åŸºäºhç©ºé—´åˆ¶é€ æ‰°åŠ¨ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ç ´åå›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚åŸºäºHAADï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä»‹ç»äº†ä¸€ç§æ›´é«˜æ•ˆçš„å˜ä½“ï¼Œå³ä»…åŸºäºhç©ºé—´çš„KVå‚æ•°æ„å»ºæ‰°åŠ¨çš„HAAD-KVã€‚è¯¥ç­–ç•¥æä¾›äº†æ›´å¼ºå¤§çš„ä¿æŠ¤ï¼ŒåŒæ—¶è®¡ç®—æˆæœ¬æ›´ä½ã€‚å°½ç®¡æ–¹æ³•ç®€å•ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€æ–°å…ˆè¿›çš„å¯¹æŠ—æ€§æ”»å‡»æ–¹æ³•ï¼Œçªå‡ºäº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17554v1">PDF</a> 32 pages, 15 figures. Accepted by ACM Multimedia 2025</p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹å…·å¤‡ä»å°‘é‡æ ·æœ¬ç”Ÿæˆå®šåˆ¶åŒ–å›¾åƒçš„èƒ½åŠ›ï¼Œä½†å…¶å¤šåŠŸèƒ½æ€§å¼•å‘äº†å…³äºæœªç»æˆæƒçš„ç§äººå†…å®¹ä¿®æ”¹çš„é‡å¤§éšç§æ‹…å¿§ã€‚æœ¬ç ”ç©¶å—åˆ°è§‚å¯Ÿå¯å‘ï¼Œè§‚å¯Ÿåˆ°è¿™äº›æ¨¡å‹åœ¨å…¶è¯­ä¹‰æ½œåœ¨ç©ºé—´ï¼ˆhç©ºé—´ï¼‰å†…å­˜åœ¨é«˜åº¦æŠ½è±¡ï¼Œè¯¥ç©ºé—´ç¼–ç äº†ç”Ÿæˆè¿è´¯å’Œæœ‰æ„ä¹‰å†…å®¹çš„å…³é”®é«˜çº§ç‰¹å¾ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„åå®šåˆ¶æ–¹æ³•HAADï¼ˆåŸºäºhç©ºé—´çš„æ‰©æ•£æ¨¡å‹å¯¹æŠ—æ”»å‡»æ³•ï¼‰ï¼Œåˆ©ç”¨å¯¹æŠ—æ€§æ”»å‡»åˆ¶é€ åŸºäºhç©ºé—´çš„æ‰°åŠ¨ï¼Œå¯æœ‰æ•ˆç ´åå›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚åœ¨HAADçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†æ›´é«˜æ•ˆçš„HAAD-KVå˜ä½“ï¼Œä»…åŸºäºhç©ºé—´çš„KVå‚æ•°æ„å»ºæ‰°åŠ¨ã€‚è¯¥æ–¹æ³•åœ¨ä¿æŠ¤æ–¹é¢æ›´å¼ºå¤§ä¸”è®¡ç®—æˆæœ¬æ›´ä½ã€‚å°½ç®¡æ–¹æ³•ç®€å•ï¼Œä½†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šä¼˜äºæœ€å…ˆè¿›çš„å¯¹æŠ—æ€§æ”»å‡»ï¼Œçªæ˜¾å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å…·å¤‡ç”Ÿæˆå®šåˆ¶åŒ–å›¾åƒçš„èƒ½åŠ›ï¼Œå¼•å‘å…³äºæœªç»æˆæƒä¿®æ”¹å†…å®¹çš„éšç§æ‹…å¿§ã€‚</li>
<li>è¯­ä¹‰æ½œåœ¨ç©ºé—´ï¼ˆhç©ºé—´ï¼‰åœ¨æ‰©æ•£æ¨¡å‹ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼Œç¼–ç äº†ç”Ÿæˆè¿è´¯å›¾åƒçš„é«˜çº§ç‰¹å¾ã€‚</li>
<li>æå‡ºäº†æ–°å‹åå®šåˆ¶æ–¹æ³•HAADï¼Œåˆ©ç”¨å¯¹æŠ—æ€§æ”»å‡»åˆ¶é€ åŸºäºhç©ºé—´çš„æ‰°åŠ¨ã€‚</li>
<li>HAADçš„æ”¹è¿›ç‰ˆæœ¬HAAD-KVæ›´åŠ é«˜æ•ˆï¼Œä»…ä½¿ç”¨hç©ºé—´çš„KVå‚æ•°æ„å»ºæ‰°åŠ¨ã€‚</li>
<li>HAADå’ŒHAAD-KVæ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡å’Œä¿æŠ¤æ•ˆæœæ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</li>
<li>ä¸ç°æœ‰å¯¹æŠ—æ€§æ”»å‡»ç›¸æ¯”ï¼ŒHAADå’ŒHAAD-KVæ–¹æ³•å±•ç°å‡ºæ›´é«˜çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17554">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cd6f70167903873c81e4c3648cdfbf2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5709e64652c63e2bf5b5cf9c69f6b54d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9854802c4c60c7778189d2f936be1911.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8408dd9d57bde868981d9be807cd232.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CMP-A-Composable-Meta-Prompt-for-SAM-Based-Cross-Domain-Few-Shot-Segmentation"><a href="#CMP-A-Composable-Meta-Prompt-for-SAM-Based-Cross-Domain-Few-Shot-Segmentation" class="headerlink" title="CMP: A Composable Meta Prompt for SAM-Based Cross-Domain Few-Shot   Segmentation"></a>CMP: A Composable Meta Prompt for SAM-Based Cross-Domain Few-Shot   Segmentation</h2><p><strong>Authors:Shuai Chen, Fanman Meng, Chunjin Yang, Haoran Wei, Chenhao Wu, Qingbo Wu, Hongliang Li</strong></p>
<p>Cross-Domain Few-Shot Segmentation (CD-FSS) remains challenging due to limited data and domain shifts. Recent foundation models like the Segment Anything Model (SAM) have shown remarkable zero-shot generalization capability in general segmentation tasks, making it a promising solution for few-shot scenarios. However, adapting SAM to CD-FSS faces two critical challenges: reliance on manual prompt and limited cross-domain ability. Therefore, we propose the Composable Meta-Prompt (CMP) framework that introduces three key modules: (i) the Reference Complement and Transformation (RCT) module for semantic expansion, (ii) the Composable Meta-Prompt Generation (CMPG) module for automated meta-prompt synthesis, and (iii) the Frequency-Aware Interaction (FAI) module for domain discrepancy mitigation. Evaluations across four cross-domain datasets demonstrate CMPâ€™s state-of-the-art performance, achieving 71.8% and 74.5% mIoU in 1-shot and 5-shot scenarios respectively. </p>
<blockquote>
<p>è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰ç”±äºæ•°æ®æœ‰é™å’Œé¢†åŸŸåç§»è€Œä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘çš„åŸºçŸ³æ¨¡å‹ï¼Œå¦‚åˆ†å‰²ä»»ä½•æ¨¡å‹ï¼ˆSAMï¼‰ï¼Œåœ¨ä¸€èˆ¬åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æƒŠäººçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶æˆä¸ºå°æ ·æœ¬åœºæ™¯çš„å¾ˆæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå°†SAMé€‚åº”äºCD-FSSé¢ä¸´ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¾èµ–æ‰‹åŠ¨æç¤ºå’Œæœ‰é™çš„è·¨åŸŸèƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¯ç»„åˆå…ƒæç¤ºï¼ˆCMPï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼šï¼ˆiï¼‰å‚è€ƒè¡¥å…¨å’Œè½¬æ¢ï¼ˆRCTï¼‰æ¨¡å—ï¼Œç”¨äºè¯­ä¹‰æ‰©å±•ï¼›ï¼ˆiiï¼‰å¯ç»„åˆå…ƒæç¤ºç”Ÿæˆï¼ˆCMPGï¼‰æ¨¡å—ï¼Œç”¨äºè‡ªåŠ¨å…ƒæç¤ºåˆæˆï¼›ï¼ˆiiiï¼‰é¢‘ç‡æ„ŸçŸ¥äº¤äº’ï¼ˆFAIï¼‰æ¨¡å—ï¼Œç”¨äºé¢†åŸŸå·®å¼‚ç¼“è§£ã€‚åœ¨å››ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†CMPçš„å“è¶Šæ€§èƒ½ï¼Œåœ¨1æ¬¡å’Œ5æ¬¡æ‹æ‘„çš„åœºæ™¯ä¸­åˆ†åˆ«å®ç°äº†71.8%å’Œ74.5%çš„mIoUã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16753v1">PDF</a> 3 figures</p>
<p><strong>Summary</strong><br>è·¨åŸŸå°æ ·æœ¬åˆ†å‰²ï¼ˆCD-FSSï¼‰é¢ä¸´æœ‰é™æ•°æ®å’ŒåŸŸå·®å¼‚çš„æŒ‘æˆ˜ã€‚æœ€è¿‘çš„åˆ†å‰²é€šç”¨æ¨¡å‹ï¼Œå¦‚Segment Anything Modelï¼ˆSAMï¼‰ï¼Œåœ¨ä¸€èˆ¬åˆ†å‰²ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæƒŠäººçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ï¼Œå¯¹äºå°æ ·æœ¬åœºæ™¯å…·æœ‰æ½œåŠ›ã€‚ç„¶è€Œï¼Œå°†SAMé€‚åº”äºCD-FSSé¢ä¸´ä¸¤ä¸ªå…³é”®é—®é¢˜ï¼šä¾èµ–æ‰‹åŠ¨æç¤ºå’Œæœ‰é™çš„è·¨åŸŸèƒ½åŠ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Composable Meta-Promptï¼ˆCMPï¼‰æ¡†æ¶ï¼Œå¼•å…¥ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼šç”¨äºè¯­ä¹‰æ‰©å±•çš„Reference Complement and Transformationï¼ˆRCTï¼‰æ¨¡å—ã€ç”¨äºè‡ªåŠ¨å…ƒæç¤ºåˆæˆçš„Composable Meta-Prompt Generationï¼ˆCMPGï¼‰æ¨¡å—ã€ä»¥åŠç”¨äºåŸŸå·®å¼‚ç¼“è§£çš„é¢‘ç‡æ„ŸçŸ¥äº¤äº’ï¼ˆFAIï¼‰æ¨¡å—ã€‚åœ¨å››ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒCMPå…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨1-shotå’Œ5-shotåœºæ™¯ä¸‹åˆ†åˆ«å®ç°äº†71.8%å’Œ74.5%çš„mIoUã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CD-FSSé¢ä¸´æœ‰é™æ•°æ®å’ŒåŸŸå·®å¼‚çš„æŒ‘æˆ˜ã€‚</li>
<li>SAMæ¨¡å‹åœ¨ä¸€èˆ¬åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å°†SAMé€‚åº”äºCD-FSSå­˜åœ¨ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šä¾èµ–æ‰‹åŠ¨æç¤ºå’Œæœ‰é™çš„è·¨åŸŸèƒ½åŠ›ã€‚</li>
<li>æå‡ºçš„CMPæ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®æ¨¡å—ï¼šRCTã€CMPGå’ŒFAIã€‚</li>
<li>CMPæ¡†æ¶æ—¨åœ¨è§£å†³è¯­ä¹‰æ‰©å±•ã€è‡ªåŠ¨å…ƒæç¤ºåˆæˆå’ŒåŸŸå·®å¼‚ç¼“è§£çš„é—®é¢˜ã€‚</li>
<li>CMPæ¡†æ¶åœ¨å››ä¸ªè·¨åŸŸæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-609ffc3331d2ed490546129e60b61e82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-512108a3d0a63097d0bf2f633dedffc2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2454b66da8061de60d8c8270d705a5e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2589ce79f418878ee6b83135275cc7d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0df37e9df94d3f5cfdebf8879ddc753.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ec7704fefe16578ed325921a47580aa.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Doodle-Your-Keypoints-Sketch-Based-Few-Shot-Keypoint-Detection"><a href="#Doodle-Your-Keypoints-Sketch-Based-Few-Shot-Keypoint-Detection" class="headerlink" title="Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection"></a>Doodle Your Keypoints: Sketch-Based Few-Shot Keypoint Detection</h2><p><strong>Authors:Subhajit Maity, Ayan Kumar Bhunia, Subhadeep Koley, Pinaki Nath Chowdhury, Aneeshan Sain, Yi-Zhe Song</strong></p>
<p>Keypoint detection, integral to modern machine perception, faces challenges in few-shot learning, particularly when source data from the same distribution as the query is unavailable. This gap is addressed by leveraging sketches, a popular form of human expression, providing a source-free alternative. However, challenges arise in mastering cross-modal embeddings and handling user-specific sketch styles. Our proposed framework overcomes these hurdles with a prototypical setup, combined with a grid-based locator and prototypical domain adaptation. We also demonstrate success in few-shot convergence across novel keypoints and classes through extensive experiments. </p>
<blockquote>
<p>å…³é”®ç‚¹æ£€æµ‹æ˜¯ç°ä»£æœºå™¨æ„ŸçŸ¥çš„æ ¸å¿ƒï¼Œåœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨æ— æ³•è·å–ä¸æŸ¥è¯¢ç›¸åŒåˆ†å¸ƒçš„æºæ•°æ®æ—¶ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬åˆ©ç”¨ç´ æè¿™ä¸€æµè¡Œçš„äººç±»è¡¨è¾¾æ–¹å¼ï¼Œæä¾›ä¸€ç§æ— éœ€æºæ•°æ®çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼ŒæŒæ¡è·¨æ¨¡æ€åµŒå…¥å’Œå¤„ç†ç”¨æˆ·ç‰¹å®šçš„ç´ æé£æ ¼å´å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬æå‡ºçš„æ¡†æ¶é€šè¿‡ç»“åˆåŸå‹è®¾ç½®ã€åŸºäºç½‘æ ¼çš„å®šä½å™¨å’ŒåŸå‹åŸŸé€‚åº”ï¼Œå…‹æœäº†è¿™äº›éšœç¢ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å¤§é‡å®éªŒè¯æ˜äº†åœ¨æ–°å…³é”®ç‚¹å’Œæ–°ç±»åˆ«ä¸Šçš„å°‘æ ·æœ¬æ”¶æ•›çš„æˆåŠŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07994v3">PDF</a> Accepted at ICCV 2025. Project Page: <a target="_blank" rel="noopener" href="https://subhajitmaity.me/DYKp">https://subhajitmaity.me/DYKp</a></p>
<p><strong>Summary</strong>ï¼š<br>ç°ä»£æœºå™¨æ„ŸçŸ¥ä¸­çš„å…³é”®ç‚¹æ£€æµ‹åœ¨å°‘æ ·æœ¬å­¦ä¹ ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰ä¸æŸ¥è¯¢ç›¸åŒåˆ†å¸ƒçš„æºæ•°æ®çš„æƒ…å†µä¸‹ã€‚é€šè¿‡åˆ©ç”¨äººç±»è¡¨è¾¾çš„ä¸€ç§æµè¡Œå½¢å¼â€”â€”è‰å›¾ä½œä¸ºæºæ•°æ®çš„æ›¿ä»£æ–¹æ¡ˆæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼ŒæŒæ¡è·¨æ¨¡æ€åµŒå…¥å’Œå¤„ç†ç”¨æˆ·ç‰¹å®šçš„è‰å›¾é£æ ¼ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æå‡ºçš„æ¡†æ¶é€šè¿‡åŸå‹è®¾ç½®å’ŒåŸºäºç½‘æ ¼çš„å®šä½å™¨ä»¥åŠåŸå‹åŸŸé€‚åº”æ¥å…‹æœè¿™äº›éšœç¢ï¼Œå¹¶é€šè¿‡å¹¿æ³›çš„å®éªŒè¯æ˜äº†åœ¨æ–°å‹å…³é”®ç‚¹å’Œç±»åˆ«ä¸Šçš„å°‘æ ·æœ¬æ”¶æ•›çš„æˆåŠŸã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>ç°ä»£æœºå™¨æ„ŸçŸ¥ä¸­çš„å…³é”®ç‚¹æ£€æµ‹åœ¨å°‘æ ·æœ¬å­¦ä¹ ç¯å¢ƒä¸‹å…·æœ‰æŒ‘æˆ˜ã€‚</li>
<li>å½“ç¼ºä¹ä¸æŸ¥è¯¢ç›¸åŒçš„æºæ•°æ®åˆ†å¸ƒæ—¶ï¼Œåˆ©ç”¨è‰å›¾ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>æŒæ¡è·¨æ¨¡æ€åµŒå…¥å’Œå¤„ç†ç”¨æˆ·ç‰¹å®šè‰å›¾é£æ ¼æ˜¯é¢ä¸´çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶é‡‡ç”¨åŸå‹è®¾ç½®æ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚</li>
<li>åŸºäºç½‘æ ¼çš„å®šä½å™¨æœ‰åŠ©äºå¤„ç†è‰å›¾æ•°æ®ã€‚</li>
<li>é€šè¿‡åŸå‹åŸŸé€‚åº”æ¥æé«˜æ¨¡å‹çš„é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07994">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b9a6b61914b9819369f3d062535c5be5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b6956a2cd400538e97c5a19d28bdc5bb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e7800874675488123f5524a13531d87f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-69996e32d6d73a0e13c0d807d83a82d2.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4ece756ddbcbb9cb90df7645521e10d1.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  Explainable Image Classification with Reduced Overconfidence for Tissue   Characterisation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/MMT/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8aa4d08019ca21c0fb06fa5cc13801f1.jpg" class="responsive-img" alt="MMT">
                        
                        <span class="card-title">MMT</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            MMT æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  iLSU-T an Open Dataset for Uruguayan Sign Language Translation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/MMT/" class="post-category">
                                    MMT
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/MMT/">
                        <span class="chip bg-color">MMT</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25691.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
