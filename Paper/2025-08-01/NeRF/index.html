<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="NeRF">
    <meta name="description" content="NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  NeRF Is a Valuable Assistant for 3D Gaussian Splatting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>NeRF | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-25fa6963933556854a815a540ef9bf76.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">NeRF</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/NeRF/">
                                <span class="chip bg-color">NeRF</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                NeRF
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    74 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-02-æ›´æ–°"><a href="#2025-08-02-æ›´æ–°" class="headerlink" title="2025-08-02 æ›´æ–°"></a>2025-08-02 æ›´æ–°</h1><h2 id="NeRF-Is-a-Valuable-Assistant-for-3D-Gaussian-Splatting"><a href="#NeRF-Is-a-Valuable-Assistant-for-3D-Gaussian-Splatting" class="headerlink" title="NeRF Is a Valuable Assistant for 3D Gaussian Splatting"></a>NeRF Is a Valuable Assistant for 3D Gaussian Splatting</h2><p><strong>Authors:Shuangkang Fang, I-Chao Shen, Takeo Igarashi, Yufeng Wang, ZeSheng Wang, Yi Yang, Wenrui Ding, Shuchang Zhou</strong></p>
<p>We introduce NeRF-GS, a novel framework that jointly optimizes Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework leverages the inherent continuous spatial representation of NeRF to mitigate several limitations of 3DGS, including sensitivity to Gaussian initialization, limited spatial awareness, and weak inter-Gaussian correlations, thereby enhancing its performance. In NeRF-GS, we revisit the design of 3DGS and progressively align its spatial features with NeRF, enabling both representations to be optimized within the same scene through shared 3D spatial information. We further address the formal distinctions between the two approaches by optimizing residual vectors for both implicit features and Gaussian positions to enhance the personalized capabilities of 3DGS. Experimental results on benchmark datasets show that NeRF-GS surpasses existing methods and achieves state-of-the-art performance. This outcome confirms that NeRF and 3DGS are complementary rather than competing, offering new insights into hybrid approaches that combine 3DGS and NeRF for efficient 3D scene representation. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†NeRF-GSï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒè”åˆä¼˜åŒ–äº†ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œ3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰ã€‚è¯¥æ¡†æ¶åˆ©ç”¨NeRFçš„å›ºæœ‰è¿ç»­ç©ºé—´è¡¨ç¤ºæ¥å‡è½»3DGSçš„å‡ ä¸ªå±€é™æ€§ï¼ŒåŒ…æ‹¬é«˜æ–¯åˆå§‹åŒ–çš„æ•æ„Ÿæ€§ã€ç©ºé—´æ„ŸçŸ¥çš„æœ‰é™æ€§ä»¥åŠé«˜æ–¯ä¹‹é—´çš„å¼±ç›¸å…³æ€§ï¼Œä»è€Œæé«˜äº†å…¶æ€§èƒ½ã€‚åœ¨NeRF-GSä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†3DGSçš„è®¾è®¡ï¼Œå¹¶é€ä¸€å¯¹é½å…¶ç©ºé—´ç‰¹å¾ä¸NeRFï¼Œä½¿è¿™ä¸¤ç§è¡¨ç¤ºå½¢å¼èƒ½å¤Ÿé€šè¿‡å…±äº«3Dç©ºé—´ä¿¡æ¯åœ¨åŒä¸€åœºæ™¯ä¸­è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é€šè¿‡ä¼˜åŒ–éšå¼ç‰¹å¾å’Œé«˜æ–¯ä½ç½®çš„æ®‹å·®å‘é‡æ¥è§£å†³ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„å½¢å¼åŒºåˆ«ï¼Œä»¥å¢å¼º3DGSçš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒNeRF-GSè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚è¿™ä¸€ç»“æœè¯å®ï¼ŒNeRFå’Œ3DGSæ˜¯äº’è¡¥çš„è€Œä¸æ˜¯ç«äº‰çš„ï¼Œä¸ºç»“åˆ3DGSå’ŒNeRFçš„æ··åˆæ–¹æ³•æä¾›äº†æ–°è§è§£ï¼Œä»¥å®ç°æœ‰æ•ˆçš„3Dåœºæ™¯è¡¨ç¤ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23374v1">PDF</a> Accepted by ICCV</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æˆ‘ä»¬æ¨å‡ºNeRF-GSï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆNeural Radiance Fieldsï¼ˆNeRFï¼‰ä¸3D Gaussian Splattingï¼ˆ3DGSï¼‰çš„æ–°å‹æ¡†æ¶ã€‚å®ƒé€šè¿‡NeRFçš„å†…åœ¨è¿ç»­ç©ºé—´è¡¨ç¤ºï¼Œå‡è½»äº†3DGSçš„å‡ ä¸ªå±€é™æ€§ï¼ŒåŒ…æ‹¬é«˜æ–¯åˆå§‹åŒ–çš„æ•æ„Ÿæ€§ã€ç©ºé—´æ„ŸçŸ¥çš„æœ‰é™æ€§ä»¥åŠé«˜æ–¯é—´ç›¸å…³æ€§è¾ƒå¼±çš„é—®é¢˜ï¼Œä»è€Œæé«˜äº†å…¶æ€§èƒ½ã€‚åœ¨NeRF-GSä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†3DGSçš„è®¾è®¡ï¼Œé€æ­¥å°†å…¶ç©ºé—´ç‰¹å¾ä¸NeRFå¯¹é½ï¼Œä½¿ä¸¤ç§è¡¨ç¤ºå½¢å¼èƒ½å¤Ÿé€šè¿‡å…±äº«çš„ä¸‰ç»´ç©ºé—´ä¿¡æ¯åœ¨åŒä¸€åœºæ™¯ä¸­è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬é€šè¿‡ä¼˜åŒ–éšå¼ç‰¹å¾å’Œé«˜æ–¯ä½ç½®çš„æ®‹å·®å‘é‡ï¼Œè§£å†³äº†ä¸¤è€…ä¹‹é—´çš„å½¢å¼å·®å¼‚ï¼Œå¢å¼ºäº†3DGSçš„ä¸ªæ€§åŒ–èƒ½åŠ›ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒNeRF-GSè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚è¿™è¯æ˜NeRFå’Œ3DGSæ˜¯äº’è¡¥çš„è€Œéç«äº‰çš„ï¼Œä¸ºç»“åˆ3DGSå’ŒNeRFçš„é«˜æ•ˆ3Dåœºæ™¯è¡¨ç¤ºæä¾›äº†æ–°è§è§£ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>NeRF-GSç»“åˆäº†NeRFä¸3DGSï¼Œå½¢æˆäº†ä¸€ç§æ–°å‹æ¡†æ¶ã€‚</li>
<li>NeRFçš„è¿ç»­ç©ºé—´è¡¨ç¤ºæœ‰åŠ©äºè§£å†³3DGSçš„å‡ ä¸ªå…³é”®å±€é™æ€§ã€‚</li>
<li>åœ¨NeRF-GSä¸­ï¼Œ3DGSçš„ç©ºé—´ç‰¹å¾è®¾è®¡è¢«é‡æ–°æ£€è§†ï¼Œä¸NeRFé€æ­¥å¯¹é½ã€‚</li>
<li>é€šè¿‡å…±äº«ä¸‰ç»´ç©ºé—´ä¿¡æ¯ï¼Œä¸¤ç§è¡¨ç¤ºå½¢å¼å¯ä»¥åœ¨åŒä¸€åœºæ™¯ä¸­è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>ä¼˜åŒ–äº†éšå¼ç‰¹å¾å’Œé«˜æ–¯ä½ç½®çš„æ®‹å·®å‘é‡ï¼Œç¼©å°äº†NeRFå’Œ3DGSä¹‹é—´çš„å½¢å¼å·®å¼‚ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒNeRF-GSè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</li>
<li>NeRFå’Œ3DGSæ˜¯äº’è¡¥çš„ï¼Œè¿™ä¸ºç»“åˆä¸¤è€…çš„æ··åˆæ–¹æ³•æä¾›äº†æ–°è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23374">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-171cb64b2dc7ed8ead6e62a2bc7dc785.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dad08707822011a404d304345bd3a4a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92a93dbadca0a7676aa1e22e393da7a5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="NeuroVoxel-LM-Language-Aligned-3D-Perception-via-Dynamic-Voxelization-and-Meta-Embedding"><a href="#NeuroVoxel-LM-Language-Aligned-3D-Perception-via-Dynamic-Voxelization-and-Meta-Embedding" class="headerlink" title="NeuroVoxel-LM: Language-Aligned 3D Perception via Dynamic Voxelization   and Meta-Embedding"></a>NeuroVoxel-LM: Language-Aligned 3D Perception via Dynamic Voxelization   and Meta-Embedding</h2><p><strong>Authors:Shiyu Liu, Lianlei Shan</strong></p>
<p>Recent breakthroughs in Visual Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have significantly advanced 3D scene perception towards language-driven cognition. However, existing 3D language models struggle with sparse, large-scale point clouds due to slow feature extraction and limited representation accuracy. To address these challenges, we propose NeuroVoxel-LM, a novel framework that integrates Neural Radiance Fields (NeRF) with dynamic resolution voxelization and lightweight meta-embedding. Specifically, we introduce a Dynamic Resolution Multiscale Voxelization (DR-MSV) technique that adaptively adjusts voxel granularity based on geometric and structural complexity, reducing computational cost while preserving reconstruction fidelity. In addition, we propose the Token-level Adaptive Pooling for Lightweight Meta-Embedding (TAP-LME) mechanism, which enhances semantic representation through attention-based weighting and residual fusion. Experimental results demonstrate that DR-MSV significantly improves point cloud feature extraction efficiency and accuracy, while TAP-LME outperforms conventional max-pooling in capturing fine-grained semantics from NeRF weights. </p>
<blockquote>
<p>æœ€è¿‘çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„çªç ´ï¼Œæå¤§åœ°æ¨åŠ¨äº†3Dåœºæ™¯æ„ŸçŸ¥å‘è¯­è¨€é©±åŠ¨è®¤çŸ¥çš„å‘å±•ã€‚ç„¶è€Œï¼Œç”±äºç‰¹å¾æå–é€Ÿåº¦æ…¢å’Œè¡¨ç¤ºç²¾åº¦æœ‰é™ï¼Œç°æœ‰çš„3Dè¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç¨€ç–çš„å¤§è§„æ¨¡ç‚¹äº‘æ—¶é‡åˆ°äº†å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†NeuroVoxel-LMè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ä¸åŠ¨æ€åˆ†è¾¨ç‡ä½“ç´ åŒ–å’Œè½»é‡çº§å…ƒåµŒå…¥ç›¸ç»“åˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€åˆ†è¾¨ç‡å¤šå°ºåº¦ä½“ç´ åŒ–ï¼ˆDR-MSVï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯ä»¥æ ¹æ®å‡ ä½•å’Œç»“æ„å¤æ‚æ€§è‡ªé€‚åº”åœ°è°ƒæ•´ä½“ç´ ç²’åº¦ï¼Œä»è€Œåœ¨é™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒé‡å»ºçš„ä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºè½»é‡çº§å…ƒåµŒå…¥çš„æ ‡è®°çº§è‡ªé€‚åº”æ± åŒ–ï¼ˆTAP-LMEï¼‰æœºåˆ¶ï¼Œå®ƒé€šè¿‡åŸºäºæ³¨æ„åŠ›çš„åŠ æƒå’Œæ®‹å·®èåˆæ¥å¢å¼ºè¯­ä¹‰è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDR-MSVèƒ½æ˜¾è‘—æé«˜ç‚¹äº‘ç‰¹å¾æå–çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œè€ŒTAP-LMEåœ¨æ•è·NeRFæƒé‡çš„ç²¾ç»†è¯­ä¹‰æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„æœ€å¤§æ± åŒ–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20110v1">PDF</a> **14 pages, 3 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æŠ€æœ¯ç»“åˆåŠ¨æ€åˆ†è¾¨ç‡ä½“ç´ åŒ–å’Œè½»é‡çº§å…ƒåµŒå…¥çš„NeuroVoxel-LMæ¡†æ¶ï¼Œèƒ½æœ‰æ•ˆè§£å†³ç°æœ‰3Dè¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç¨€ç–å¤§è§„æ¨¡ç‚¹äº‘æ—¶é¢ä¸´çš„ç‰¹å¾æå–æ…¢å’Œè¡¨ç¤ºç²¾åº¦æœ‰é™çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€åˆ†è¾¨ç‡å¤šå°ºåº¦ä½“ç´ åŒ–ï¼ˆDR-MSVï¼‰æŠ€æœ¯è‡ªé€‚åº”è°ƒæ•´ä½“ç´ ç²’åº¦ï¼ŒåŸºäºå‡ ä½•å’Œç»“æ„å¤æ‚åº¦ï¼Œé™ä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ä¿æŒé‡å»ºä¿çœŸåº¦ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ç”¨äºè½»é‡çº§å…ƒåµŒå…¥çš„æ ‡è®°çº§åˆ«è‡ªé€‚åº”æ± åŒ–ï¼ˆTAP-LMEï¼‰æœºåˆ¶ï¼Œé€šè¿‡æ³¨æ„åŠ›åŠ æƒçš„æ®‹å·®èåˆå¢å¼ºè¯­ä¹‰è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDR-MSVèƒ½æ˜¾è‘—æé«˜ç‚¹äº‘ç‰¹å¾æå–çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒTAP-LMEåœ¨æ•è·NeRFæƒé‡çš„ç²¾ç»†è¯­ä¹‰æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„æœ€å¤§æ± åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeuroVoxel-LMæ¡†æ¶ç»“åˆäº†NeRFæŠ€æœ¯ã€åŠ¨æ€åˆ†è¾¨ç‡ä½“ç´ åŒ–å’Œè½»é‡çº§å…ƒåµŒå…¥ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰3Dè¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç‚¹äº‘æ—¶çš„æŒ‘æˆ˜ã€‚</li>
<li>DR-MSVæŠ€æœ¯èƒ½å¤Ÿè‡ªé€‚åº”è°ƒæ•´ä½“ç´ ç²’åº¦ï¼ŒåŸºäºå‡ ä½•å’Œç»“æ„å¤æ‚åº¦ï¼Œä»¥æé«˜ç‰¹å¾æå–æ•ˆç‡å’Œé‡å»ºä¿çœŸåº¦ã€‚</li>
<li>TAP-LMEæœºåˆ¶é€šè¿‡æ³¨æ„åŠ›åŠ æƒçš„æ®‹å·®èåˆå¢å¼ºè¯­ä¹‰è¡¨ç¤ºï¼Œåœ¨æ•è·NeRFæƒé‡çš„ç²¾ç»†è¯­ä¹‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>æ¡†æ¶è§£å†³äº†ç°æœ‰3Dè¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¤§è§„æ¨¡ç¨€ç–ç‚¹äº‘æ—¶çš„è®¡ç®—æˆæœ¬é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒNeuroVoxel-LMåœ¨ç‚¹äº‘ç‰¹å¾æå–å’Œè¡¨ç¤ºæ–¹é¢å…·æœ‰è¾ƒé«˜çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>è¯¥æ¡†æ¶çš„åº”ç”¨å‰æ™¯å¹¿æ³›ï¼Œå¯æ¨åŠ¨è¯­è¨€é©±åŠ¨çš„è®¤çŸ¥3Dåœºæ™¯æ„ŸçŸ¥çš„å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20110">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-270aebcfc487b212ac1a0fd254162db2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a18cd4d5ba5d1c05ddb98f8a7f1e13c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ea111f70326a64c7aab18f648c72b52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0ab5199a7110c0ac95bf1d915075ee4b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DINO-SLAM-DINO-informed-RGB-D-SLAM-for-Neural-Implicit-and-Explicit-Representations"><a href="#DINO-SLAM-DINO-informed-RGB-D-SLAM-for-Neural-Implicit-and-Explicit-Representations" class="headerlink" title="DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit   Representations"></a>DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit   Representations</h2><p><strong>Authors:Ziren Gong, Xiaohan Li, Fabio Tosi, Youmin Zhang, Stefano Mattoccia, Jun Wu, Matteo Poggi</strong></p>
<p>This paper presents DINO-SLAM, a DINO-informed design strategy to enhance neural implicit (Neural Radiance Field â€“ NeRF) and explicit representations (3D Gaussian Splatting â€“ 3DGS) in SLAM systems through more comprehensive scene representations. Purposely, we rely on a Scene Structure Encoder (SSE) that enriches DINO features into Enhanced DINO ones (EDINO) to capture hierarchical scene elements and their structural relationships. Building upon it, we propose two foundational paradigms for NeRF and 3DGS SLAM systems integrating EDINO features. Our DINO-informed pipelines achieve superior performance on the Replica, ScanNet, and TUM compared to state-of-the-art methods. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†DINO-SLAMï¼Œè¿™æ˜¯ä¸€ç§ä»¥DINOä¸ºæŒ‡å¯¼çš„è®¾è®¡ç­–ç•¥ï¼Œé€šè¿‡æ›´å…¨é¢çš„åœºæ™¯è¡¨ç¤ºï¼Œå¢å¼ºSLAMç³»ç»Ÿä¸­çš„ç¥ç»éšå¼ï¼ˆç¥ç»è¾å°„åœº-NeRFï¼‰å’Œæ˜¾å¼è¡¨ç¤ºï¼ˆä¸‰ç»´é«˜æ–¯æ‹¼è´´-3DGSï¼‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä¾èµ–åœºæ™¯ç»“æ„ç¼–ç å™¨ï¼ˆSSEï¼‰ï¼Œå®ƒå°†DINOç‰¹å¾ä¸°å¯Œä¸ºå¢å¼ºå‹DINOç‰¹å¾ï¼ˆEDINOï¼‰ï¼Œä»¥æ•è·åˆ†å±‚åœºæ™¯å…ƒç´ åŠå…¶ç»“æ„å…³ç³»ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä¸ºNeRFå’Œ3DGS SLAMç³»ç»Ÿæå‡ºäº†ä¸¤ç§èåˆEDINOç‰¹å¾çš„åŸºç¡€èŒƒå¼ã€‚æˆ‘ä»¬çš„ä»¥DINOä¸ºæŒ‡å¯¼çš„ç®¡é“åœ¨Replicaã€ScanNetå’ŒTUMä¸Šä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19474v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DINO-SLAMï¼Œè¿™æ˜¯ä¸€ç§åŸºäºDINOæŠ€æœ¯çš„è®¾è®¡ç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡æ›´å…¨é¢çš„åœºæ™¯è¡¨ç¤ºå¢å¼ºç¥ç»éšå¼ï¼ˆNeural Radiance Field - NeRFï¼‰å’Œæ˜¾å¼è¡¨ç¤ºï¼ˆ3D Gaussian Splatting - 3DGSï¼‰åœ¨SLAMç³»ç»Ÿä¸­çš„è¡¨ç°ã€‚è¯¥ç ”ç©¶é€šè¿‡åœºæ™¯ç»“æ„ç¼–ç å™¨ï¼ˆSSEï¼‰ä¸°å¯ŒDINOç‰¹å¾ï¼Œå½¢æˆå¢å¼ºå‹DINOç‰¹å¾ï¼ˆEDINOï¼‰ï¼Œä»¥æ•æ‰åœºæ™¯å…ƒç´ çš„å±‚æ¬¡ç»“æ„å’Œå®ƒä»¬ä¹‹é—´çš„ç»“æ„å…³ç³»ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç ”ç©¶æå‡ºäº†ä¸¤ä¸ªé’ˆå¯¹NeRFå’Œ3DGS SLAMç³»ç»Ÿçš„åŸºæœ¬èŒƒå¼ï¼Œæ•´åˆEDINOç‰¹å¾ã€‚åœ¨Replicaã€ScanNetå’ŒTUMä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDINO-SLAMçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DINO-SLAMæ˜¯åŸºäºDINOæŠ€æœ¯çš„è®¾è®¡ç­–ç•¥ï¼Œæ—¨åœ¨å¢å¼ºç¥ç»éšå¼å’Œæ˜¾å¼è¡¨ç¤ºåœ¨SLAMç³»ç»Ÿä¸­çš„è¡¨ç°ã€‚</li>
<li>é€šè¿‡åœºæ™¯ç»“æ„ç¼–ç å™¨ï¼ˆSSEï¼‰ä¸°å¯ŒDINOç‰¹å¾ï¼Œå½¢æˆå¢å¼ºå‹DINOç‰¹å¾ï¼ˆEDINOï¼‰ã€‚</li>
<li>EDINOç‰¹å¾ç”¨äºæ•æ‰åœºæ™¯å…ƒç´ çš„å±‚æ¬¡ç»“æ„å’Œç»“æ„å…³ç³»ã€‚</li>
<li>æå‡ºäº†ä¸¤ä¸ªé’ˆå¯¹NeRFå’Œ3DGS SLAMç³»ç»Ÿçš„åŸºæœ¬èŒƒå¼ï¼Œæ•´åˆEDINOç‰¹å¾ã€‚</li>
<li>DINO-SLAMåœ¨Replicaã€ScanNetå’ŒTUMç­‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥ç­–ç•¥èƒ½å¤Ÿæä¾›æ›´åŠ å…¨é¢çš„åœºæ™¯è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5c80e57949dac334fe909d50b2b70755.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-632f443cc7fd99eb97801bf77962ac5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-53bbe5bfa45d89a4867a6cdc012c2658.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d56e993bfcfb6f81a3966dd543f1e7ee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-974d5ae1d3d51a7de22c8ed9c1593e20.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d6408930b90c3d7a7a0a5c913e8100c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="NerT-CA-Efficient-Dynamic-Reconstruction-from-Sparse-view-X-ray-Coronary-Angiography"><a href="#NerT-CA-Efficient-Dynamic-Reconstruction-from-Sparse-view-X-ray-Coronary-Angiography" class="headerlink" title="NerT-CA: Efficient Dynamic Reconstruction from Sparse-view X-ray   Coronary Angiography"></a>NerT-CA: Efficient Dynamic Reconstruction from Sparse-view X-ray   Coronary Angiography</h2><p><strong>Authors:Kirsten W. H. Maas, Danny Ruijters, Nicola Pezzotti, Anna Vilanova</strong></p>
<p>Three-dimensional (3D) and dynamic 3D+time (4D) reconstruction of coronary arteries from X-ray coronary angiography (CA) has the potential to improve clinical procedures. However, there are multiple challenges to be addressed, most notably, blood-vessel structure sparsity, poor background and blood vessel distinction, sparse-views, and intra-scan motion. State-of-the-art reconstruction approaches rely on time-consuming manual or error-prone automatic segmentations, limiting clinical usability. Recently, approaches based on Neural Radiance Fields (NeRF) have shown promise for automatic reconstructions in the sparse-view setting. However, they suffer from long training times due to their dependence on MLP-based representations. We propose NerT-CA, a hybrid approach of Neural and Tensorial representations for accelerated 4D reconstructions with sparse-view CA. Building on top of the previous NeRF-based work, we model the CA scene as a decomposition of low-rank and sparse components, utilizing fast tensorial fields for low-rank static reconstruction and neural fields for dynamic sparse reconstruction. Our approach outperforms previous works in both training time and reconstruction accuracy, yielding reasonable reconstructions from as few as three angiogram views. We validate our approach quantitatively and qualitatively on representative 4D phantom datasets. </p>
<blockquote>
<p>ä»Xå°„çº¿å† çŠ¶åŠ¨è„‰é€ å½±ï¼ˆCAï¼‰è¿›è¡Œå† çŠ¶åŠ¨è„‰çš„ä¸‰ç»´ï¼ˆ3Dï¼‰å’ŒåŠ¨æ€3D+æ—¶é—´ï¼ˆ4Dï¼‰é‡å»ºå…·æœ‰æ”¹å–„ä¸´åºŠæµç¨‹çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œè¿˜å­˜åœ¨è®¸å¤šå¾…è§£å†³çš„é—®é¢˜ï¼Œæœ€æ˜¾è‘—çš„æ˜¯è¡€ç®¡ç»“æ„ç¨€ç–ã€èƒŒæ™¯ä¸è¡€ç®¡åŒºåˆ†åº¦å·®ã€è§†è§’ç¨€ç–ä»¥åŠæ‰«æå†…è¿åŠ¨ã€‚æœ€å…ˆè¿›çš„é‡å»ºæ–¹æ³•ä¾èµ–äºè€—æ—¶çš„äººå·¥æˆ–æ˜“å‡ºé”™çš„è‡ªåŠ¨åˆ†å‰²ï¼Œè¿™é™åˆ¶äº†å…¶åœ¨ä¸´åºŠä¸Šçš„å¯ç”¨æ€§ã€‚æœ€è¿‘ï¼ŒåŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æ–¹æ³•åœ¨ç¨€ç–è§†è§’ä¸‹çš„è‡ªåŠ¨é‡å»ºæ–¹é¢æ˜¾ç¤ºå‡ºå¸Œæœ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç”±äºä¾èµ–äºMLPè¡¨ç¤ºè€Œå…·æœ‰è¾ƒé•¿çš„è®­ç»ƒæ—¶é—´ã€‚æˆ‘ä»¬æå‡ºäº†NerT-CAï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºåŠ é€Ÿç¨€ç–è§†è§’CAçš„4Dé‡å»ºçš„ç¥ç»å’Œå¼ é‡è¡¨å¾çš„æ··åˆæ–¹æ³•ã€‚åŸºäºä¹‹å‰çš„NeRFç›¸å…³å·¥ä½œï¼Œæˆ‘ä»¬å°†CAåœºæ™¯å»ºæ¨¡ä¸ºä½é˜¶å’Œç¨€ç–æˆåˆ†çš„åˆ†è§£ï¼Œåˆ©ç”¨å¿«é€Ÿçš„å¼ é‡åœºè¿›è¡Œä½é˜¶é™æ€é‡å»ºå’Œç¥ç»åœºè¿›è¡ŒåŠ¨æ€ç¨€ç–é‡å»ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è®­ç»ƒæ—¶é—´å’Œé‡å»ºç²¾åº¦ä¸Šéƒ½ä¼˜äºä»¥å‰çš„å·¥ä½œï¼Œä»ä»…ä¸‰ä¸ªè¡€ç®¡é€ å½±è§†è§’å°±èƒ½å¾—åˆ°åˆç†çš„é‡å»ºç»“æœã€‚æˆ‘ä»¬åœ¨å…¸å‹çš„4D Phantomæ•°æ®é›†ä¸Šä»æ•°é‡å’Œæ€§è´¨ä¸¤ä¸ªæ–¹é¢éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19328v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºNeRFçš„å† çŠ¶åŠ¨è„‰ä¸‰ç»´ï¼ˆ3Dï¼‰åŠå››ç»´ï¼ˆåŠ¨æ€3D+æ—¶é—´ï¼‰é‡å»ºæ–¹æ³•é¢ä¸´å¤šç§æŒ‘æˆ˜ï¼Œå¦‚è¡€ç®¡ç»“æ„ç¨€ç–ã€èƒŒæ™¯ä¸è¡€ç®¡åŒºåˆ†ä¸æ¸…ç­‰ã€‚ä¸ºåŠ å¿«è®­ç»ƒé€Ÿåº¦å’Œæå‡é‡å»ºå‡†ç¡®æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œä¸å¼ é‡è¡¨ç¤ºçš„æ··åˆæ–¹æ³•NerT-CAã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¿«é€Ÿå¼ é‡åœºè¿›è¡Œé™æ€ä½ç§©é‡å»ºï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œè¿›è¡ŒåŠ¨æ€ç¨€ç–é‡å»ºã€‚åœ¨ä»£è¡¨æ€§å››ç»´å¹»å½±æ•°æ®é›†ä¸Šè¿›è¡Œå®šé‡å’Œå®šæ€§éªŒè¯ï¼Œè¡¨æ˜è¯¥æ–¹æ³•åœ¨è®­ç»ƒæ—¶é—´å’Œé‡å»ºå‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºä»¥å‰çš„å·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºNeRFçš„å† çŠ¶åŠ¨è„‰é‡å»ºæ–¹æ³•å…·æœ‰æ½œåŠ›æ”¹å–„ä¸´åºŠè¿‡ç¨‹ã€‚</li>
<li>å½“å‰é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬è¡€ç®¡ç»“æ„ç¨€ç–ã€èƒŒæ™¯ä¸è¡€ç®¡åŒºåˆ†ä¸æ¸…ç­‰ã€‚</li>
<li>NerT-CAæ˜¯ä¸€ç§åŸºäºç¥ç»ç½‘ç»œä¸å¼ é‡è¡¨ç¤ºçš„æ··åˆæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>NerT-CAåˆ©ç”¨å¿«é€Ÿå¼ é‡åœºè¿›è¡Œé™æ€ä½ç§©é‡å»ºï¼Œç¥ç»ç½‘ç»œè¿›è¡ŒåŠ¨æ€ç¨€ç–é‡å»ºã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨è®­ç»ƒæ—¶é—´å’Œé‡å»ºå‡†ç¡®æ€§æ–¹é¢å‡ä¼˜äºä»¥å‰çš„å·¥ä½œã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿä»æå°‘çš„é€ å½±è§†è§’ï¼ˆå¦‚ä¸‰ä¸ªè§†è§’ï¼‰è¿›è¡Œåˆç†é‡å»ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19328">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c5223c256592b57755a339fbb50fcf36.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6193aab56df31ac8e72e451990353ef4.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="SaLF-Sparse-Local-Fields-for-Multi-Sensor-Rendering-in-Real-Time"><a href="#SaLF-Sparse-Local-Fields-for-Multi-Sensor-Rendering-in-Real-Time" class="headerlink" title="SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time"></a>SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time</h2><p><strong>Authors:Yun Chen, Matthew Haines, Jingkang Wang, Krzysztof Baron-Lis, Sivabalan Manivasagam, Ze Yang, Raquel Urtasun</strong></p>
<p>High-fidelity sensor simulation of light-based sensors such as cameras and LiDARs is critical for safe and accurate autonomy testing. Neural radiance field (NeRF)-based methods that reconstruct sensor observations via ray-casting of implicit representations have demonstrated accurate simulation of driving scenes, but are slow to train and render, hampering scale. 3D Gaussian Splatting (3DGS) has demonstrated faster training and rendering times through rasterization, but is primarily restricted to pinhole camera sensors, preventing usage for realistic multi-sensor autonomy evaluation. Moreover, both NeRF and 3DGS couple the representation with the rendering procedure (implicit networks for ray-based evaluation, particles for rasterization), preventing interoperability, which is key for general usage. In this work, we present Sparse Local Fields (SaLF), a novel volumetric representation that supports rasterization and raytracing. SaLF represents volumes as a sparse set of 3D voxel primitives, where each voxel is a local implicit field. SaLF has fast training (&lt;30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS LiDAR), has adaptive pruning and densification to easily handle large scenes, and can support non-pinhole cameras and spinning LiDARs. We demonstrate that SaLF has similar realism as existing self-driving sensor simulation methods while improving efficiency and enhancing capabilities, enabling more scalable simulation. <a target="_blank" rel="noopener" href="https://waabi.ai/salf/">https://waabi.ai/salf/</a> </p>
<blockquote>
<p>åŸºäºæ‘„åƒå¤´å’Œæ¿€å…‰é›·è¾¾ç­‰å…‰åŸºä¼ æ„Ÿå™¨çš„é«˜ä¿çœŸä¼ æ„Ÿå™¨æ¨¡æ‹Ÿå¯¹äºå®‰å…¨å’Œå‡†ç¡®çš„è‡ªä¸»æ€§æµ‹è¯•è‡³å…³é‡è¦ã€‚åŸºäºç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æ–¹æ³•é€šè¿‡å…‰çº¿æŠ•å°„éšå¼è¡¨ç¤ºæ¥é‡å»ºä¼ æ„Ÿå™¨è§‚æµ‹ï¼Œå·²è¯æ˜å¯ä»¥å‡†ç¡®æ¨¡æ‹Ÿé©¾é©¶åœºæ™¯ï¼Œä½†è®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦è¾ƒæ…¢ï¼Œå½±å“äº†å…¶æ‰©å±•æ€§ã€‚3Dé«˜æ–¯å–·æ¶‚ï¼ˆ3DGSï¼‰é€šè¿‡æ …æ ¼åŒ–å®ç°äº†æ›´å¿«çš„è®­ç»ƒå’Œæ¸²æŸ“æ—¶é—´ï¼Œä½†ä¸»è¦å±€é™äºé’ˆå­”ç›¸æœºä¼ æ„Ÿå™¨ï¼Œæ— æ³•ç”¨äºç°å®çš„å¤šä¼ æ„Ÿå™¨è‡ªä¸»æ€§è¯„ä¼°ã€‚æ­¤å¤–ï¼ŒNeRFå’Œ3DGSéƒ½å°†è¡¨ç¤ºä¸æ¸²æŸ“ç¨‹åºç›¸ç»“åˆï¼ˆåŸºäºå°„çº¿çš„éšå¼ç½‘ç»œè¯„ä¼°ã€ç²’å­æ …æ ¼åŒ–ï¼‰ï¼Œå¦¨ç¢äº†äº’æ“ä½œæ€§ï¼Œè¿™å¯¹äºé€šç”¨ä½¿ç”¨è‡³å…³é‡è¦ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¨€ç–å±€éƒ¨åœºï¼ˆSaLFï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„ä½“ç§¯è¡¨ç¤ºæ³•ï¼Œæ”¯æŒæ …æ ¼åŒ–å’Œå…‰çº¿è¿½è¸ªã€‚SaLFå°†ä½“ç§¯è¡¨ç¤ºä¸ºä¸€ç»„ç¨€ç–çš„3Dä½“ç´ åŸè¯­ï¼Œå…¶ä¸­æ¯ä¸ªä½“ç´ éƒ½æ˜¯ä¸€ä¸ªå±€éƒ¨éšå¼åœºã€‚SaLFå…·æœ‰å¿«é€Ÿè®­ç»ƒï¼ˆ&lt;30åˆ†é’Ÿï¼‰å’Œæ¸²æŸ“èƒ½åŠ›ï¼ˆæ‘„åƒå¤´50+ FPSå’Œæ¿€å…‰é›·è¾¾600+ FPSï¼‰ï¼Œå…·æœ‰è‡ªé€‚åº”çš„ä¿®å‰ªå’Œç»†åŒ–ï¼Œå¯è½»æ¾å¤„ç†å¤§åœºæ™¯ï¼Œå¹¶ä¸”å¯ä»¥æ”¯æŒéé’ˆå­”ç›¸æœºå’Œæ—‹è½¬æ¿€å…‰é›·è¾¾ã€‚æˆ‘ä»¬è¯æ˜ï¼ŒSaLFå…·æœ‰ä¸ç°æœ‰è‡ªåŠ¨é©¾é©¶ä¼ æ„Ÿå™¨æ¨¡æ‹Ÿæ–¹æ³•ç±»ä¼¼çš„é€¼çœŸåº¦ï¼ŒåŒæ—¶æé«˜äº†æ•ˆç‡å’Œå¢å¼ºäº†åŠŸèƒ½ï¼Œä½¿æ¨¡æ‹Ÿæ›´åŠ å¯æ‰©å±•ã€‚è¯¦æƒ…è¯·è®¿é—®<a target="_blank" rel="noopener" href="https://waabi.ai/salf/%E4%BA%86%E8%A7%A3%E3%80%82">https://waabi.ai/salf/äº†è§£ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18713v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Sparse Local Fieldsï¼ˆSaLFï¼‰è¿™ä¸€æ–°å‹ä½“ç§¯è¡¨ç¤ºæ³•ï¼Œå®ƒæ”¯æŒå…‰çº¿è¿½è¸ªå’Œæ …æ ¼åŒ–ï¼Œå¯ç”¨äºé«˜ä¿çœŸä¼ æ„Ÿå™¨æ¨¡æ‹Ÿã€‚SaLFå…·æœ‰å¿«é€Ÿè®­ç»ƒå’Œæ¸²æŸ“èƒ½åŠ›ï¼Œèƒ½è½»æ¾å¤„ç†å¤§åœºæ™¯ï¼Œå¹¶æ”¯æŒéé’ˆå­”ç›¸æœºå’Œæ—‹è½¬æ¿€å…‰é›·è¾¾ã€‚å®ƒæé«˜äº†è‡ªåŠ¨é©¾é©¶ä¼ æ„Ÿå™¨æ¨¡æ‹Ÿçš„æ•ˆç‡å¹¶å¢å¼ºäº†å…¶åŠŸèƒ½ï¼ŒåŒæ—¶ä¿æŒä¸ç°å®åœºæ™¯çš„é€¼çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜ä¿çœŸä¼ æ„Ÿå™¨æ¨¡æ‹Ÿå¯¹å®‰å…¨å’Œç²¾ç¡®çš„è‡ªåŠ¨é©¾é©¶æµ‹è¯•è‡³å…³é‡è¦ã€‚</li>
<li>Neural Radiance Fieldï¼ˆNeRFï¼‰æ–¹æ³•èƒ½é€šè¿‡å°„çº¿æŠ•å°„éšå¼è¡¨ç¤ºé‡å»ºä¼ æ„Ÿå™¨è§‚å¯Ÿï¼Œä½†è®­ç»ƒå’Œæ¸²æŸ“é€Ÿåº¦æ…¢ï¼Œé™åˆ¶äº†å…¶è§„æ¨¡åº”ç”¨ã€‚</li>
<li>3D Gaussian Splattingï¼ˆ3DGSï¼‰æ–¹æ³•é€šè¿‡æ …æ ¼åŒ–å®ç°æ›´å¿«çš„è®­ç»ƒå’Œæ¸²æŸ“æ—¶é—´ï¼Œä½†ä¸»è¦å±€é™äºé’ˆå­”ç›¸æœºä¼ æ„Ÿå™¨ï¼Œæ— æ³•ç”¨äºç°å®çš„å¤šä¼ æ„Ÿå™¨è‡ªåŠ¨é©¾é©¶è¯„ä¼°ã€‚</li>
<li>ç°æœ‰çš„æ–¹æ³•å°†è¡¨ç¤ºä¸æ¸²æŸ“è¿‡ç¨‹ç›¸ç»“åˆï¼Œé˜»ç¢äº†äº’æ“ä½œæ€§ï¼Œè¿™æ˜¯é€šç”¨ç”¨é€”çš„å…³é”®ã€‚</li>
<li>SaLFæ˜¯ä¸€ç§æ–°å‹ä½“ç§¯è¡¨ç¤ºæ³•ï¼Œæ”¯æŒæ …æ ¼åŒ–å’Œå…‰çº¿è¿½è¸ªï¼Œå…·æœ‰å¿«é€Ÿè®­ç»ƒå’Œæ¸²æŸ“èƒ½åŠ›ï¼Œå¹¶èƒ½å¤„ç†å¤§åœºæ™¯ã€‚</li>
<li>SaLFå¯ä»¥æ”¯æŒéé’ˆå­”ç›¸æœºå’Œæ—‹è½¬æ¿€å…‰é›·è¾¾ï¼Œæé«˜äº†ç°å®æ„Ÿå¹¶ä¿æŒé«˜æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b2fd13f8293827cba16f7c2d70e4f723.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b8a83763155a603b60d38b28cddb7e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e654e28e7aa40c300990168708828cf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3de98db6fd302e9568599fc5d211f8a9.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Facial-Demorphing-from-a-Single-Morph-Using-a-Latent-Conditional-GAN"><a href="#Facial-Demorphing-from-a-Single-Morph-Using-a-Latent-Conditional-GAN" class="headerlink" title="Facial Demorphing from a Single Morph Using a Latent Conditional GAN"></a>Facial Demorphing from a Single Morph Using a Latent Conditional GAN</h2><p><strong>Authors:Nitish Shukla, Arun Ross</strong></p>
<p>A morph is created by combining two (or more) face images from two (or more) identities to create a composite image that is highly similar to all constituent identities, allowing the forged morph to be biometrically associated with more than one individual. Morph Attack Detection (MAD) can be used to detect a morph, but does not reveal the constituent images. Demorphing - the process of deducing the constituent images - is thus vital to provide additional evidence about a morph. Existing demorphing methods suffer from the morph replication problem, where the outputs tend to look very similar to the morph itself, or assume that train and test morphs are generated using the same morph technique. The proposed method overcomes these issues. The method decomposes a morph in latent space allowing it to demorph images created from unseen morph techniques and face styles. We train our method on morphs created from synthetic faces and test on morphs created from real faces using different morph techniques. Our method outperforms existing methods by a considerable margin and produces high fidelity demorphed face images. </p>
<blockquote>
<p>é€šè¿‡ç»“åˆä¸¤ä¸ªï¼ˆæˆ–æ›´å¤šï¼‰èº«ä»½çš„ä¸¤ä¸ªï¼ˆæˆ–æ›´å¤šï¼‰é¢éƒ¨å›¾åƒï¼Œåˆ›å»ºä¸€ä¸ªå½¢æ€ï¼Œä»¥åˆ›å»ºä¸€ä¸ªä¸æ‰€æœ‰ç»„æˆèº«ä»½é«˜åº¦ç›¸ä¼¼çš„åˆæˆå›¾åƒï¼Œä»è€Œä½¿ä¼ªé€ å½¢æ€èƒ½å¤Ÿä¸å¤šä¸ªä¸ªä½“è¿›è¡Œç”Ÿç‰©ç‰¹å¾å…³è”ã€‚å½¢æ€æ”»å‡»æ£€æµ‹ï¼ˆMADï¼‰å¯ç”¨äºæ£€æµ‹å½¢æ€ï¼Œä½†ä¸ä¼šæ˜¾ç¤ºç»„æˆå›¾åƒã€‚å› æ­¤ï¼Œå½¢æ€æ¨å¯¼ï¼ˆè¿˜åŸç»„æˆå›¾åƒçš„è¿‡ç¨‹ï¼‰å¯¹äºæä¾›æœ‰å…³å½¢æ€çš„é¢å¤–è¯æ®è‡³å…³é‡è¦ã€‚ç°æœ‰å½¢æ€æ¨å¯¼æ–¹æ³•å­˜åœ¨å½¢æ€å¤åˆ¶é—®é¢˜ï¼Œè¾“å‡ºçš„ç»“æœå¾€å¾€ä¸å½¢æ€æœ¬èº«éå¸¸ç›¸ä¼¼ï¼Œæˆ–è€…å‡è®¾è®­ç»ƒå’Œæµ‹è¯•å½¢æ€æ˜¯ä½¿ç”¨ç›¸åŒçš„å½¢æ€æŠ€æœ¯ç”Ÿæˆçš„ã€‚æ‰€æå‡ºçš„æ–¹æ³•å…‹æœäº†è¿™äº›é—®é¢˜ã€‚è¯¥æ–¹æ³•åœ¨æ½œåœ¨ç©ºé—´å†…å¯¹å½¢æ€è¿›è¡Œåˆ†è§£ï¼Œä½¿å…¶èƒ½å¤Ÿè¿˜åŸç”±æœªè§è¿‡çš„å½¢æ€æŠ€æœ¯å’Œé¢éƒ¨é£æ ¼æ‰€åˆ›å»ºçš„å›¾åƒã€‚æˆ‘ä»¬åœ¨ç”±åˆæˆé¢éƒ¨åˆ›å»ºçš„å½¢æ€ä¸Šè®­ç»ƒæˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶åœ¨ä½¿ç”¨ä¸åŒå½¢æ€æŠ€æœ¯ç”±çœŸå®é¢éƒ¨åˆ›å»ºçš„å½¢æ€ä¸Šè¿›è¡Œæµ‹è¯•ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå¤§å¤§è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶äº§ç”Ÿäº†é«˜ä¿çœŸåº¦çš„è¿˜åŸé¢éƒ¨å›¾åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18566v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é€šè¿‡ç»“åˆä¸¤ä¸ªæˆ–å¤šä¸ªèº«ä»½çš„é¢åƒåˆ›å»ºå¤åˆå›¾åƒçš„æŠ€æœ¯ï¼Œç§°ä¸ºå½¢æ€æ”»å‡»ï¼ˆMorph Attackï¼‰ã€‚å½¢æ€æ”»å‡»æ£€æµ‹ï¼ˆMADï¼‰å¯ä»¥æ£€æµ‹å½¢æ€æ”»å‡»ï¼Œä½†ä¸æ­ç¤ºæ„æˆå›¾åƒã€‚å› æ­¤ï¼Œå»å½¢æ€åŒ–ï¼ˆDemorphingï¼‰è¿‡ç¨‹â€”â€”æ¨æ–­æ„æˆå›¾åƒçš„è¿‡ç¨‹â€”â€”å¯¹äºæä¾›å½¢æ€æ”»å‡»çš„é¢å¤–è¯æ®è‡³å…³é‡è¦ã€‚ç°æœ‰å»å½¢æ€åŒ–æ–¹æ³•å­˜åœ¨å½¢æ€å¤åˆ¶é—®é¢˜ï¼Œè¾“å‡ºçš„å›¾åƒå¾€å¾€ä¸åŸå§‹å½¢æ€ç›¸ä¼¼æˆ–å‡è®¾è®­ç»ƒå’Œæµ‹è¯•å½¢æ€ä½¿ç”¨ç›¸åŒçš„å½¢æ€æŠ€æœ¯ã€‚è€Œæ–°æ–¹æ³•å…‹æœäº†è¿™äº›é—®é¢˜ï¼Œå®ƒåœ¨æ½œåœ¨ç©ºé—´ä¸­åˆ†è§£å½¢æ€ï¼Œä½¿å…¶èƒ½å¤Ÿå¯¹ç”±æœªè§è¿‡çš„å½¢æ€æŠ€æœ¯å’Œé¢éƒ¨é£æ ¼åˆ›å»ºçš„å›¾åƒè¿›è¡Œå»å½¢æ€åŒ–ã€‚ç»è¿‡åœ¨åˆæˆé¢éƒ¨å’ŒçœŸå®é¢éƒ¨ä¸Šåˆ›å»ºçš„ä¸åŒå½¢æ€æŠ€æœ¯çš„æµ‹è¯•ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶äº§ç”Ÿé«˜ä¿çœŸåº¦çš„å»å½¢æ€åŒ–é¢éƒ¨å›¾åƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½¢æ€æ”»å‡»æ˜¯é€šè¿‡ç»“åˆå¤šä¸ªèº«ä»½çš„é¢åƒåˆ›å»ºå¤åˆå›¾åƒçš„æŠ€æœ¯ã€‚</li>
<li>å½¢æ€æ”»å‡»æ£€æµ‹ï¼ˆMADï¼‰èƒ½å¤Ÿæ£€æµ‹å½¢æ€æ”»å‡»ï¼Œä½†ä¸æ­ç¤ºæ„æˆå›¾åƒã€‚</li>
<li>å»å½¢æ€åŒ–ï¼ˆDemorphingï¼‰æ˜¯æ¨æ–­æ„æˆå›¾åƒçš„è¿‡ç¨‹ï¼Œå¯¹äºæ­ç¤ºå½¢æ€æ”»å‡»çš„è¯æ®è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰å»å½¢æ€åŒ–æ–¹æ³•å­˜åœ¨å½¢æ€å¤åˆ¶é—®é¢˜ï¼Œå³è¾“å‡ºå›¾åƒä¸åŸå§‹å½¢æ€ç›¸ä¼¼ã€‚</li>
<li>æ–°æ–¹æ³•èƒ½å¤Ÿåœ¨æ½œåœ¨ç©ºé—´ä¸­åˆ†è§£å½¢æ€ï¼Œå…‹æœç°æœ‰é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿå¤„ç†ç”±æœªè§è¿‡çš„å½¢æ€æŠ€æœ¯å’Œé¢éƒ¨é£æ ¼åˆ›å»ºçš„å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b0ba164074e9d2c6a31b7250ab9f7966.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d88c7929c19ee01bb92e8620377e4ff8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-509f519005210de162806cfb7b20bff4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b016dc6c769e0cf626246d6e0ae017cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3fbd7d6d8b5c2f52dfefc1d1e033c6f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03ca086a3e053fc72a24ff1b796d7d7d.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="High-fidelity-3D-Gaussian-Inpainting-preserving-multi-view-consistency-and-photorealistic-details"><a href="#High-fidelity-3D-Gaussian-Inpainting-preserving-multi-view-consistency-and-photorealistic-details" class="headerlink" title="High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency   and photorealistic details"></a>High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency   and photorealistic details</h2><p><strong>Authors:Jun Zhou, Dinghao Li, Nannan Li, Mingjie Wang</strong></p>
<p>Recent advancements in multi-view 3D reconstruction and novel-view synthesis, particularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have greatly enhanced the fidelity and efficiency of 3D content creation. However, inpainting 3D scenes remains a challenging task due to the inherent irregularity of 3D structures and the critical need for maintaining multi-view consistency. In this work, we propose a novel 3D Gaussian inpainting framework that reconstructs complete 3D scenes by leveraging sparse inpainted views. Our framework incorporates an automatic Mask Refinement Process and region-wise Uncertainty-guided Optimization. Specifically, we refine the inpainting mask using a series of operations, including Gaussian scene filtering and back-projection, enabling more accurate localization of occluded regions and realistic boundary restoration. Furthermore, our Uncertainty-guided Fine-grained Optimization strategy, which estimates the importance of each region across multi-view images during training, alleviates multi-view inconsistencies and enhances the fidelity of fine details in the inpainted results. Comprehensive experiments conducted on diverse datasets demonstrate that our approach outperforms existing state-of-the-art methods in both visual quality and view consistency. </p>
<blockquote>
<p>è¿‘æœŸå¤šè§†è§’ä¸‰ç»´é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆæ–¹é¢çš„è¿›å±•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰å’Œä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰ï¼Œæå¤§åœ°æé«˜äº†ä¸‰ç»´å†…å®¹åˆ›å»ºçš„çœŸå®æ€§å’Œæ•ˆç‡ã€‚ç„¶è€Œï¼Œç”±äºä¸‰ç»´ç»“æ„æœ¬èº«çš„ä¸è§„åˆ™æ€§å’Œä¿æŒå¤šè§†è§’ä¸€è‡´æ€§çš„å…³é”®éœ€æ±‚ï¼Œå¯¹ä¸‰ç»´åœºæ™¯çš„ä¸Šè‰²å¡«å……ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¸‰ç»´é«˜æ–¯å¡«å……æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨ç¨€ç–å¡«å……è§†å›¾é‡å»ºå®Œæ•´çš„ä¸‰ç»´åœºæ™¯ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»“åˆäº†è‡ªåŠ¨è’™ç‰ˆç»†åŒ–è¿‡ç¨‹å’ŒåŒºåŸŸä¸ç¡®å®šæ€§å¼•å¯¼ä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç³»åˆ—æ“ä½œæ¥ä¼˜åŒ–å¡«å……è’™ç‰ˆï¼ŒåŒ…æ‹¬é«˜æ–¯åœºæ™¯æ»¤æ³¢å™¨å’Œåå‘æŠ•å½±ï¼Œä»è€Œæ›´å‡†ç¡®åœ°å®šä½é®æŒ¡åŒºåŸŸå¹¶å®ç°é€¼çœŸçš„è¾¹ç•Œæ¢å¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§å¼•å¯¼ç²¾ç»†ä¼˜åŒ–ç­–ç•¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼°è®¡å¤šè§†è§’å›¾åƒä¸­æ¯ä¸ªåŒºåŸŸçš„é‡è¦æ€§ï¼Œå‡è½»äº†å¤šè§†è§’çš„ä¸ä¸€è‡´æ€§ï¼Œæé«˜äº†å¡«å……ç»“æœä¸­ç»†èŠ‚çš„çœŸå®æ€§ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œè§†è§’ä¸€è‡´æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18023v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>     è¿‘æœŸå¤šè§†è§’ä¸‰ç»´é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆï¼ˆå°¤å…¶æ˜¯é€šè¿‡ç¥ç»è¾å°„åœºå’Œä¸‰ç»´é«˜æ–¯æ‹¼è´´æŠ€æœ¯ï¼‰çš„è¿›æ­¥å¤§å¤§æé«˜äº†ä¸‰ç»´å†…å®¹åˆ›ä½œçš„ä¿çœŸåº¦å’Œæ•ˆç‡ã€‚ç„¶è€Œï¼Œå¯¹ä¸‰ç»´åœºæ™¯çš„è¡¥å…¨ä¾ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºä¸‰ç»´ç»“æ„çš„ä¸è§„åˆ™æ€§å’Œä¿æŒå¤šè§†è§’ä¸€è‡´æ€§çš„å…³é”®éœ€æ±‚ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸‰ç»´é«˜æ–¯è¡¥å…¨æ¡†æ¶ï¼Œé€šè¿‡åˆ©ç”¨ç¨€ç–çš„è¡¥å…¨è§†è§’æ¥é‡å»ºå®Œæ•´çš„ä¸‰ç»´åœºæ™¯ã€‚æ­¤æ¡†æ¶ç»“åˆäº†è‡ªåŠ¨æ©è†œç»†åŒ–è¿‡ç¨‹å’ŒåŒºåŸŸä¸ç¡®å®šæ€§å¼•å¯¼ä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ç³»åˆ—æ“ä½œå¦‚é«˜æ–¯åœºæ™¯è¿‡æ»¤å’Œåå‘æŠ•å½±æ¥ä¼˜åŒ–è¡¥å…¨æ©è†œï¼Œå¯ä»¥æ›´å‡†ç¡®åœ°å®šä½é®æŒ¡åŒºåŸŸå¹¶å®ç°é€¼çœŸçš„è¾¹ç•Œæ¢å¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§å¼•å¯¼ç²¾ç»†ä¼˜åŒ–ç­–ç•¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼°è®¡æ¯ä¸ªåŒºåŸŸåœ¨å¤šè§†è§’å›¾åƒä¸­çš„é‡è¦æ€§ï¼Œå‡è½»äº†å¤šè§†è§’çš„ä¸ä¸€è‡´æ€§ï¼Œæé«˜äº†è¡¥å…¨ç»“æœçš„ç»†èŠ‚ä¿çœŸåº¦ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œè§†è§’ä¸€è‡´æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åˆ©ç”¨å¤šè§†è§’ä¸‰ç»´é‡å»ºå’Œæ–°é¢–è§†è§’åˆæˆæŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œå¢å¼ºäº†ä¸‰ç»´å†…å®¹åˆ›ä½œçš„ä¿çœŸåº¦å’Œæ•ˆç‡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸‰ç»´é«˜æ–¯è¡¥å…¨æ¡†æ¶ï¼Œåˆ©ç”¨ç¨€ç–çš„è¡¥å…¨è§†è§’é‡å»ºå®Œæ•´ä¸‰ç»´åœºæ™¯ã€‚</li>
<li>æ¡†æ¶åŒ…å«è‡ªåŠ¨æ©è†œç»†åŒ–è¿‡ç¨‹ï¼Œé€šè¿‡é«˜æ–¯åœºæ™¯è¿‡æ»¤å’Œåå‘æŠ•å½±ç­‰æ“ä½œä¼˜åŒ–è¡¥å…¨æ©è†œã€‚</li>
<li>å¼•å…¥åŒºåŸŸä¸ç¡®å®šæ€§å¼•å¯¼ä¼˜åŒ–ç­–ç•¥ï¼Œä¼°è®¡æ¯ä¸ªåŒºåŸŸåœ¨å¤šè§†è§’å›¾åƒä¸­çš„é‡è¦æ€§ï¼Œå‡å°‘å¤šè§†è§’ä¸ä¸€è‡´æ€§ã€‚</li>
<li>ç»¼åˆå®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§†è§‰è´¨é‡å’Œè§†è§’ä¸€è‡´æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½æœ‰æ•ˆå¤„ç†ä¸‰ç»´ç»“æ„çš„ä¸è§„åˆ™æ€§ï¼Œå®ç°æ›´å‡†ç¡®çš„é®æŒ¡åŒºåŸŸå®šä½å’Œé€¼çœŸçš„è¾¹ç•Œæ¢å¤ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-caecbd7239c5c938c3c9bdccb613efff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aaaf44df6323e63af790e79f3cc674f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42145126ca9d061f4bcca2a70eec9345.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c79b8735ebd6391bd0cb5c6a13b20c63.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6d1ade62955d2c8b2b7fefb9d64d9587.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4863ef50e589b7658e0225a4ee8b0aa5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Exploring-Active-Learning-for-Label-Efficient-Training-of-Semantic-Neural-Radiance-Field"><a href="#Exploring-Active-Learning-for-Label-Efficient-Training-of-Semantic-Neural-Radiance-Field" class="headerlink" title="Exploring Active Learning for Label-Efficient Training of Semantic   Neural Radiance Field"></a>Exploring Active Learning for Label-Efficient Training of Semantic   Neural Radiance Field</h2><p><strong>Authors:Yuzhe Zhu, Lile Cai, Kangkang Lu, Fayao Liu, Xulei Yang</strong></p>
<p>Neural Radiance Field (NeRF) models are implicit neural scene representation methods that offer unprecedented capabilities in novel view synthesis. Semantically-aware NeRFs not only capture the shape and radiance of a scene, but also encode semantic information of the scene. The training of semantically-aware NeRFs typically requires pixel-level class labels, which can be prohibitively expensive to collect. In this work, we explore active learning as a potential solution to alleviate the annotation burden. We investigate various design choices for active learning of semantically-aware NeRF, including selection granularity and selection strategies. We further propose a novel active learning strategy that takes into account 3D geometric constraints in sample selection. Our experiments demonstrate that active learning can effectively reduce the annotation cost of training semantically-aware NeRF, achieving more than 2X reduction in annotation cost compared to random sampling. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æ¨¡å‹æ˜¯ä¸€ç§éšå¼ç¥ç»åœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œä¸ºæ–°è§†è§’åˆæˆæä¾›äº†å‰æ‰€æœªæœ‰çš„èƒ½åŠ›ã€‚è¯­ä¹‰æ„ŸçŸ¥NeRFä¸ä»…æ•æ‰åœºæ™¯çš„å½¢çŠ¶å’Œè¾å°„åº¦ï¼Œè¿˜ç¼–ç åœºæ™¯è¯­ä¹‰ä¿¡æ¯ã€‚è¯­ä¹‰æ„ŸçŸ¥NeRFçš„è®­ç»ƒé€šå¸¸éœ€è¦åƒç´ çº§ç±»åˆ«æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾çš„æ”¶é›†å¯èƒ½éå¸¸æ˜‚è´µã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸»åŠ¨å­¦ä¹ ä½œä¸ºå‡è½»æ ‡æ³¨è´Ÿæ‹…çš„æ½œåœ¨è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬ç ”ç©¶äº†è¯­ä¹‰æ„ŸçŸ¥NeRFä¸»åŠ¨å­¦ä¹ çš„å„ç§è®¾è®¡é€‰æ‹©ï¼ŒåŒ…æ‹¬é€‰æ‹©ç²’åº¦å’Œé€‰æ‹©ç­–ç•¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ–°çš„ä¸»åŠ¨å­¦ä¹ ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨æ ·æœ¬é€‰æ‹©æ—¶è€ƒè™‘äº†3Då‡ ä½•çº¦æŸã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸»åŠ¨å­¦ä¹ å¯ä»¥æœ‰æ•ˆåœ°é™ä½è®­ç»ƒè¯­ä¹‰æ„ŸçŸ¥NeRFçš„æ ‡æ³¨æˆæœ¬ï¼Œä¸éšæœºé‡‡æ ·ç›¸æ¯”ï¼Œæ ‡æ³¨æˆæœ¬é™ä½äº†ä¸¤å€ä»¥ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17351v1">PDF</a> Accepted to ICME 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Neural Radiance Fieldï¼ˆNeRFï¼‰æ¨¡å‹çš„è¯­ä¹‰åŒ–é—®é¢˜ï¼Œä»‹ç»äº†ä¸€ç§ç»“åˆä¸»åŠ¨å­¦ä¹ æ–¹æ³•å‡å°‘è¯­ä¹‰æ„ŸçŸ¥NeRFè®­ç»ƒä¸­çš„æ ‡æ³¨æˆæœ¬çš„æ–¹å¼ã€‚ç ”ç©¶é€šè¿‡è®¾è®¡ä¸åŒé€‰æ‹©ç²’åº¦åŠç­–ç•¥æ¥æ¢è®¨ä¸»åŠ¨å­¦ä¹ çš„å¯è¡Œæ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„ä¸»åŠ¨å­¦ä¹ ç­–ç•¥ï¼Œå³åœ¨æ ·æœ¬é€‰æ‹©æ—¶è€ƒè™‘åˆ°ä¸‰ç»´å‡ ä½•çº¦æŸã€‚å®éªŒè¡¨æ˜ï¼Œä¸»åŠ¨å­¦ä¹ å¯ä»¥æœ‰æ•ˆé™ä½è¯­ä¹‰æ„ŸçŸ¥NeRFè®­ç»ƒçš„æ ‡æ³¨æˆæœ¬ï¼Œç›¸è¾ƒäºéšæœºé‡‡æ ·ï¼Œæ ‡æ³¨æˆæœ¬é™ä½äº†ä¸¤å€ä»¥ä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFæ¨¡å‹æ˜¯éšå¼ç¥ç»ç½‘ç»œåœºæ™¯è¡¨ç¤ºæ–¹æ³•ï¼Œèƒ½è¿›è¡Œå‰æ‰€æœªæœ‰çš„æ–°è§†è§’åˆæˆã€‚</li>
<li>è¯­ä¹‰æ„ŸçŸ¥NeRFä¸ä»…èƒ½æ•æ‰åœºæ™¯çš„å½¢çŠ¶å’Œè¾å°„åº¦ï¼Œè¿˜èƒ½ç¼–ç åœºæ™¯è¯­ä¹‰ä¿¡æ¯ã€‚</li>
<li>è¯­ä¹‰æ„ŸçŸ¥NeRFè®­ç»ƒé€šå¸¸éœ€è¦åƒç´ çº§åˆ«çš„ç±»æ ‡ç­¾ï¼Œè¿™ä¼šå¯¼è‡´æ ‡æ³¨æˆæœ¬è¿‡é«˜ã€‚</li>
<li>ä¸»åŠ¨å­¦ä¹ æ˜¯ä¸€ç§é™ä½æ ‡æ³¨æˆæœ¬çš„æœ‰æ•ˆæ–¹å¼ã€‚</li>
<li>ç ”ç©¶é€šè¿‡è®¾è®¡ä¸åŒé€‰æ‹©ç²’åº¦åŠç­–ç•¥æ¥æ¢è®¨ä¸»åŠ¨å­¦ä¹ çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„ä¸»åŠ¨å­¦ä¹ ç­–ç•¥ï¼Œè€ƒè™‘ä¸‰ç»´å‡ ä½•çº¦æŸåœ¨æ ·æœ¬é€‰æ‹©ä¸­çš„ä½œç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17351">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1dfe495172dcbf8489c5eeb378723f9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e21987a0bd9fb8b44fda04fa819300cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-17aa1b95426ae5d6b61703bb0531338a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e8005fa325b7db9a65b2000ce65dfba.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="GeMix-Conditional-GAN-Based-Mixup-for-Improved-Medical-Image-Augmentation"><a href="#GeMix-Conditional-GAN-Based-Mixup-for-Improved-Medical-Image-Augmentation" class="headerlink" title="GeMix: Conditional GAN-Based Mixup for Improved Medical Image   Augmentation"></a>GeMix: Conditional GAN-Based Mixup for Improved Medical Image   Augmentation</h2><p><strong>Authors:Hugo Carlesso, Maria Eliza Patulea, Moncef Garouani, Radu Tudor Ionescu, Josiane Mothe</strong></p>
<p>Mixup has become a popular augmentation strategy for image classification, yet its naive pixel-wise interpolation often produces unrealistic images that can hinder learning, particularly in high-stakes medical applications. We propose GeMix, a two-stage framework that replaces heuristic blending with a learned, label-aware interpolation powered by class-conditional GANs. First, a StyleGAN2-ADA generator is trained on the target dataset. During augmentation, we sample two label vectors from Dirichlet priors biased toward different classes and blend them via a Beta-distributed coefficient. Then, we condition the generator on this soft label to synthesize visually coherent images that lie along a continuous class manifold. We benchmark GeMix on the large-scale COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101, EfficientNet-B0). When combined with real data, our method increases macro-F1 over traditional mixup for all backbones, reducing the false negative rate for COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup, delivering stronger regularization and greater semantic fidelity, without disrupting existing training pipelines. We publicly release our code at <a target="_blank" rel="noopener" href="https://github.com/hugocarlesso/GeMix">https://github.com/hugocarlesso/GeMix</a> to foster reproducibility and further research. </p>
<blockquote>
<p>æ•°æ®å¢å¼ºç­–ç•¥Mixupåœ¨å›¾åƒåˆ†ç±»ä¸­å—åˆ°å¹¿æ³›æ¬¢è¿ï¼Œä½†å…¶ç®€å•çš„åƒç´ çº§æ’å€¼ç»å¸¸ç”Ÿæˆä¸çœŸå®çš„å›¾åƒï¼Œå¯èƒ½é˜»ç¢å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é£é™©çš„åŒ»ç–—åº”ç”¨ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†GeMixï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼Œå®ƒç”¨åŸºäºç±»åˆ«æ¡ä»¶GANçš„å­¦ä¹ æ„ŸçŸ¥æ’å€¼æ›¿æ¢å¯å‘å¼æ··åˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šè®­ç»ƒStyleGAN2-ADAç”Ÿæˆå™¨ã€‚åœ¨æ•°æ®å¢å¼ºè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»åå‘ä¸åŒç±»åˆ«çš„Dirichletå…ˆéªŒä¸­é‡‡æ ·ä¸¤ä¸ªæ ‡ç­¾å‘é‡ï¼Œå¹¶é€šè¿‡Betaåˆ†å¸ƒç³»æ•°å°†å®ƒä»¬æ··åˆã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ç”Ÿæˆå™¨è®¾ç½®åœ¨è¿™ä¸ªè½¯æ ‡ç­¾ä¸Šï¼Œåˆæˆè§†è§‰ä¸Šè¿è´¯çš„å›¾åƒï¼Œè¿™äº›å›¾åƒä½äºè¿ç»­çš„ç±»åˆ«æµå½¢ä¸Šã€‚æˆ‘ä»¬åœ¨å¤§è§„æ¨¡çš„COVIDx-CT-3æ•°æ®é›†ä¸Šä½¿ç”¨ä¸‰ç§éª¨å¹²ç½‘ç»œï¼ˆResNet-50ã€ResNet-101ã€EfficientNet-B0ï¼‰å¯¹GeMixè¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚å½“ä¸çœŸå®æ•°æ®ç»“åˆæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰éª¨å¹²ç½‘ç»œä¸Šç›¸å¯¹äºä¼ ç»ŸMixupæé«˜äº†å®è§‚F1åˆ†æ•°ï¼Œé™ä½äº†æ–°å† è‚ºç‚æ£€æµ‹çš„è¯¯æŠ¥ç‡ã€‚å› æ­¤ï¼ŒGeMixå¯ä½œä¸ºåƒç´ ç©ºé—´Mixupçš„æ›¿ä»£å“ï¼Œæä¾›æ›´å¼ºçš„æ­£åˆ™åŒ–å’Œæ›´é«˜çš„è¯­ä¹‰ä¿çœŸåº¦ï¼Œè€Œä¸ä¼šç ´åç°æœ‰çš„è®­ç»ƒç®¡é“ã€‚æˆ‘ä»¬å…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„ä»£ç åœ¨<a target="_blank" rel="noopener" href="https://github.com/hugocarlesso/GeMix%E4%B8%8A%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E5%92%8C%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/hugocarlesso/GeMixä¸Šï¼Œä»¥ä¿ƒè¿›å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15577v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ•°æ®å¢å¼ºç­–ç•¥Mixupåœ¨å›¾åƒåˆ†ç±»ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨é«˜é£é™©çš„åŒ»ç–—åº”ç”¨ä¸­ï¼Œå…¶ç®€å•çš„åƒç´ çº§æ’å€¼ä¼šäº§ç”Ÿä¸çœŸå®å›¾åƒï¼Œå½±å“å­¦ä¹ æ•ˆæœã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºGeMixï¼Œä¸€ä¸ªä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨åŸºäºç±»åˆ«æ¡ä»¶GANçš„æ ‡è®°æ„ŸçŸ¥æ’å€¼æ›¿æ¢å¯å‘å¼æ··åˆã€‚GeMixåœ¨COVIDx-CT-3æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œèƒ½æé«˜å®è§‚F1åˆ†æ•°ï¼Œé™ä½COVID-19æ£€æµ‹çš„è¯¯æŠ¥ç‡ã€‚GeMixå¯æ›¿ä»£åƒç´ ç©ºé—´Mixupï¼Œæä¾›æ›´å¼ºå¤§çš„æ­£åˆ™åŒ–å’Œæ›´é«˜çš„è¯­ä¹‰ä¿çœŸåº¦ï¼Œä¸”ä¸ä¼šå¹²æ‰°ç°æœ‰è®­ç»ƒæµç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mixupä½œä¸ºä¸€ç§æ•°æ®å¢å¼ºç­–ç•¥åœ¨å›¾åƒåˆ†ç±»ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨åŒ»ç–—é¢†åŸŸå­˜åœ¨å±€é™æ€§ã€‚</li>
<li>GeMixæ¡†æ¶é€šè¿‡å¼•å…¥åŸºäºç±»åˆ«æ¡ä»¶GANçš„æ ‡è®°æ„ŸçŸ¥æ’å€¼ï¼Œæ”¹è¿›äº†Mixupçš„å±€é™æ€§ã€‚</li>
<li>GeMixä½¿ç”¨StyleGAN2-ADAç”Ÿæˆå™¨ç”Ÿæˆè§†è§‰è¿è´¯çš„å›¾åƒï¼Œè¿™äº›å›¾åƒä½äºè¿ç»­çš„ç±»åˆ«æµå½¢ä¸Šã€‚</li>
<li>åœ¨COVIDx-CT-3æ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒGeMixç›¸è¾ƒäºä¼ ç»ŸMixupæé«˜äº†å®è§‚F1åˆ†æ•°ï¼Œé™ä½äº†è¯¯æŠ¥ç‡ã€‚</li>
<li>GeMixå¯æ›¿ä»£åƒç´ ç©ºé—´Mixupï¼Œæä¾›æ›´å¼ºå¤§çš„æ­£åˆ™åŒ–å’Œæ›´é«˜çš„è¯­ä¹‰ä¿çœŸåº¦ã€‚</li>
<li>GeMixä¸ä¼šå¯¹ç°æœ‰è®­ç»ƒæµç¨‹é€ æˆå¹²æ‰°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15577">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3c9ca6e24e4b0c7b658685d4cfb5b12e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51fbb2895b1fc4e719f7f1afada9d889.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ae986152fc258fc0bd1c5463b01c4ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3fb36e1dcfe1a28d91cc76e1334fc30.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GCC-Spam-Spam-Detection-via-GAN-Contrastive-Learning-and-Character-Similarity-Networks"><a href="#GCC-Spam-Spam-Detection-via-GAN-Contrastive-Learning-and-Character-Similarity-Networks" class="headerlink" title="GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character   Similarity Networks"></a>GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character   Similarity Networks</h2><p><strong>Authors:Zhijie Wang, Zixin Xu, Zhiyuan Pan</strong></p>
<p>The exponential growth of spam text on the Internet necessitates robust detection mechanisms to mitigate risks such as information leakage and social instability. This work addresses two principal challenges: adversarial strategies employed by spammers and the scarcity of labeled data. We propose a novel spam-text detection framework GCC-Spam, which integrates three core innovations. First, a character similarity network captures orthographic and phonetic features to counter character-obfuscation attacks and furthermore produces sentence embeddings for downstream classification. Second, contrastive learning enhances discriminability by optimizing the latent-space distance between spam and normal texts. Third, a Generative Adversarial Network (GAN) generates realistic pseudo-spam samples to alleviate data scarcity while improving model robustness and classification accuracy. Extensive experiments on real-world datasets demonstrate that our model outperforms baseline approaches, achieving higher detection rates with significantly fewer labeled examples. </p>
<blockquote>
<p>éšç€äº’è”ç½‘ä¸Šåƒåœ¾æ–‡æœ¬å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œä¸ºäº†å‡å°‘ä¿¡æ¯æ³„éœ²å’Œç¤¾ä¼šä¸ç¨³å®šç­‰é£é™©ï¼Œéœ€è¦å¼ºå¤§çš„æ£€æµ‹æœºåˆ¶ã€‚è¿™é¡¹å·¥ä½œè§£å†³äº†ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šåƒåœ¾é‚®ä»¶å‘é€è€…é‡‡ç”¨çš„å¯¹æŠ—ç­–ç•¥å’Œæ ‡è®°æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åƒåœ¾æ–‡æœ¬æ£€æµ‹æ¡†æ¶GCC-Spamï¼Œå®ƒèåˆäº†ä¸‰ä¸ªæ ¸å¿ƒåˆ›æ–°ç‚¹ã€‚é¦–å…ˆï¼Œä¸€ä¸ªå­—ç¬¦ç›¸ä¼¼ç½‘ç»œèƒ½å¤Ÿæ•è·æ­£å­—å’Œè¯­éŸ³ç‰¹å¾æ¥å¯¹æŠ—å­—ç¬¦æ··æ·†æ”»å‡»ï¼Œå¹¶ä¸ºä¸‹æ¸¸åˆ†ç±»äº§ç”Ÿå¥å­åµŒå…¥ã€‚å…¶æ¬¡ï¼Œå¯¹æ¯”å­¦ä¹ é€šè¿‡ä¼˜åŒ–åƒåœ¾æ–‡æœ¬å’Œæ­£å¸¸æ–‡æœ¬ä¹‹é—´çš„æ½œåœ¨ç©ºé—´è·ç¦»æ¥æé«˜é‰´åˆ«åŠ›ã€‚æœ€åï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç”Ÿæˆé€¼çœŸçš„ä¼ªåƒåœ¾é‚®ä»¶æ ·æœ¬ï¼Œä»¥ç¼“è§£æ•°æ®ç¨€ç¼ºé—®é¢˜ï¼ŒåŒæ—¶æé«˜æ¨¡å‹çš„ç¨³å¥æ€§å’Œåˆ†ç±»å‡†ç¡®æ€§ã€‚åœ¨ç°å®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œåœ¨æ›´å°‘çš„æ ‡è®°æ ·æœ¬çš„æƒ…å†µä¸‹å®ç°äº†æ›´é«˜çš„æ£€æµ‹ç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14679v2">PDF</a> </p>
<p><strong>Summary</strong><br>äº’è”ç½‘ä¸Šçš„åƒåœ¾æ–‡æœ¬æŒ‡æ•°çº§å¢é•¿ï¼Œå¸¦æ¥ä¿¡æ¯æ³„éœ²å’Œç¤¾ä¼šä¸ç¨³å®šé£é™©ã€‚é’ˆå¯¹æ­¤é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†GCC-Spamæ¡†æ¶ï¼Œæ•´åˆä¸‰å¤§åˆ›æ–°ç­–ç•¥ï¼šå­—ç¬¦ç›¸ä¼¼ç½‘ç»œåº”å¯¹å­—ç¬¦æ··æ·†æ”»å‡»å¹¶ç”Ÿæˆå¥å­åµŒå…¥ï¼›å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–åƒåœ¾æ–‡æœ¬ä¸æ­£å¸¸æ–‡æœ¬çš„æ½œåœ¨ç©ºé—´è·ç¦»ï¼›ç”Ÿæˆå¯¹æŠ—ç½‘ç»œç”Ÿæˆé€¼çœŸçš„ä¼ªåƒåœ¾æ–‡æœ¬æ ·æœ¬ä»¥ç¼“è§£æ•°æ®çŸ­ç¼ºé—®é¢˜ï¼ŒåŒæ—¶æé«˜æ¨¡å‹ç¨³å¥æ€§å’Œåˆ†ç±»ç²¾åº¦ã€‚å®éªŒè¯æ˜è¯¥æ¨¡å‹åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„æ£€æµ‹ç‡é«˜äºåŸºçº¿æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>äº’è”ç½‘åƒåœ¾æ–‡æœ¬å¢é•¿å¼•å‘é£é™©ï¼Œéœ€å¼ºåŒ–æ£€æµ‹æœºåˆ¶ã€‚</li>
<li>GCC-Spamæ¡†æ¶æ•´åˆä¸‰å¤§åˆ›æ–°ç­–ç•¥åº”å¯¹åƒåœ¾æ–‡æœ¬æ£€æµ‹æŒ‘æˆ˜ã€‚</li>
<li>å­—ç¬¦ç›¸ä¼¼ç½‘ç»œåº”å¯¹å­—ç¬¦æ··æ·†æ”»å‡»å¹¶ç”Ÿæˆå¥å­åµŒå…¥ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ ä¼˜åŒ–æ½œåœ¨ç©ºé—´è·ç¦»ï¼Œæé«˜è¯†åˆ«ç²¾åº¦ã€‚</li>
<li>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œç”Ÿæˆä¼ªåƒåœ¾æ–‡æœ¬æ ·æœ¬ï¼Œç¼“è§£æ•°æ®çŸ­ç¼ºé—®é¢˜ã€‚</li>
<li>GCC-Spamæ¡†æ¶åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„æ£€æµ‹ç‡é«˜äºåŸºçº¿æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-946ae3f7a355e27ba3ad72078b8f472e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3204f53e725902a3f3986b4c8d5b8cde.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c51c1c9b29244e7e5cbc765e1330243.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e34008d54764134bc4c7f97d03c834b1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DiSCO-3D-Discovering-and-segmenting-Sub-Concepts-from-Open-vocabulary-queries-in-NeRF"><a href="#DiSCO-3D-Discovering-and-segmenting-Sub-Concepts-from-Open-vocabulary-queries-in-NeRF" class="headerlink" title="DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary   queries in NeRF"></a>DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary   queries in NeRF</h2><p><strong>Authors:Doriand Petit, Steve Bourgeois, Vincent Gay-Bellile, Florian Chabot, LoÃ¯c Barthe</strong></p>
<p>3D semantic segmentation provides high-level scene understanding for applications in robotics, autonomous systems, \textit{etc}. Traditional methods adapt exclusively to either task-specific goals (open-vocabulary segmentation) or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts Discovery, which aims to provide a 3D semantic segmentation that adapts to both the scene and user queries. We build DiSCO-3D on Neural Fields representations, combining unsupervised segmentation with weak open-vocabulary guidance. Our evaluations demonstrate that DiSCO-3D achieves effective performance in Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in the edge cases of both open-vocabulary and unsupervised segmentation. </p>
<blockquote>
<p>3Dè¯­ä¹‰åˆ†å‰²ä¸ºæœºå™¨äººæŠ€æœ¯ã€è‡ªä¸»ç³»ç»Ÿç­‰é¢†åŸŸçš„åº”ç”¨æä¾›äº†é«˜çº§åœºæ™¯ç†è§£ã€‚ä¼ ç»Ÿæ–¹æ³•åªé€‚åº”äºç‰¹å®šä»»åŠ¡çš„ç‰¹å®šç›®æ ‡ï¼ˆå¼€æ”¾å¼è¯æ±‡åˆ†å‰²ï¼‰æˆ–åœºæ™¯å†…å®¹ï¼ˆæ— ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼‰ã€‚æˆ‘ä»¬æå‡ºDiSCO-3Dï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªè§£å†³æ›´å¹¿æ³›çš„å¼€æ”¾å¼è¯æ±‡å­æ¦‚å¿µå‘ç°é—®é¢˜çš„ä¸‰ç»´æ–¹æ³•ï¼Œæ—¨åœ¨æä¾›ä¸€ç§é€‚åº”åœºæ™¯å’Œç”¨æˆ·æŸ¥è¯¢çš„ä¸‰ç»´è¯­ä¹‰åˆ†å‰²ã€‚æˆ‘ä»¬åœ¨ç¥ç»åœºè¡¨ç¤ºçš„åŸºç¡€ä¸Šæ„å»ºDiSCO-3Dï¼Œå°†æ— ç›‘ç£åˆ†å‰²ä¸å¼±å¼€æ”¾å¼è¯æ±‡å¼•å¯¼ç›¸ç»“åˆã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒDiSCO-3Dåœ¨å¼€æ”¾å¼è¯æ±‡å­æ¦‚å¿µå‘ç°æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å¼€æ”¾å¼è¯æ±‡å’Œæ— ç›‘ç£åˆ†å‰²çš„æç«¯æƒ…å†µä¸‹éƒ½å¤„äºä¸€æµæ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14596v1">PDF</a> Published at ICCVâ€™25</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†DiSCO-3Dæ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³3Då¼€æ”¾è¯æ±‡å­æ¦‚å¿µå‘ç°è¿™ä¸€æ›´å¹¿æ³›çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ç»“åˆäº†æ— ç›‘ç£åˆ†å‰²ä¸å¼±å¼€æ”¾è¯æ±‡æŒ‡å¯¼ï¼Œæ—¨åœ¨ä¸ºåœºæ™¯å’Œç”¨æˆ·æŸ¥è¯¢æä¾›è‡ªé€‚åº”çš„3Dè¯­ä¹‰åˆ†å‰²ã€‚è¯„ä»·æ˜¾ç¤ºï¼ŒDiSCO-3Dåœ¨å¼€æ”¾è¯æ±‡å­æ¦‚å¿µå‘ç°ä¸­å–å¾—äº†æœ‰æ•ˆæ€§èƒ½ï¼Œå¹¶åœ¨å¼€æ”¾è¯æ±‡å’Œæ— ç›‘ç£åˆ†å‰²çš„è¾¹ç¼˜æƒ…å†µä¸‹å±•ç°äº†æœ€ä½³ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3Dè¯­ä¹‰åˆ†å‰²ä¸ºæœºå™¨äººã€è‡ªä¸»ç³»ç»Ÿç­‰åº”ç”¨æä¾›äº†é«˜çº§åœºæ™¯ç†è§£ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é€‚åº”äºä»»åŠ¡ç‰¹å®šç›®æ ‡æˆ–åœºæ™¯å†…å®¹ã€‚</li>
<li>DiSCO-3Dæ˜¯é¦–ä¸ªè§£å†³3Då¼€æ”¾è¯æ±‡å­æ¦‚å¿µå‘ç°é—®é¢˜çš„åŠæ³•ã€‚</li>
<li>DiSCO-3Dæ—¨åœ¨æä¾›è‡ªé€‚åº”äºåœºæ™¯å’Œç”¨æˆ·æŸ¥è¯¢çš„3Dè¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>DiSCO-3DåŸºäºç¥ç»åœºè¡¨ç¤ºï¼Œç»“åˆäº†æ— ç›‘ç£åˆ†å‰²ä¸å¼±å¼€æ”¾è¯æ±‡æŒ‡å¯¼ã€‚</li>
<li>DiSCO-3Dåœ¨å¼€æ”¾è¯æ±‡å­æ¦‚å¿µå‘ç°ä¸­è¡¨ç°æœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14596">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c46d7863e50584851d7106800748939.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e92f671dc1e8e5086abd0a66e443dd5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ca5f03a3ec621c92426ad7641109234.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Advances-in-Feed-Forward-3D-Reconstruction-and-View-Synthesis-A-Survey"><a href="#Advances-in-Feed-Forward-3D-Reconstruction-and-View-Synthesis-A-Survey" class="headerlink" title="Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey"></a>Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey</h2><p><strong>Authors:Jiahui Zhang, Yuelei Li, Anpei Chen, Muyu Xu, Kunhao Liu, Jianyuan Wang, Xiao-Xiao Long, Hanxue Liang, Zexiang Xu, Hao Su, Christian Theobalt, Christian Rupprecht, Andrea Vedaldi, Hanspeter Pfister, Shijian Lu, Fangneng Zhan</strong></p>
<p>3D reconstruction and view synthesis are foundational problems in computer vision, graphics, and immersive technologies such as augmented reality (AR), virtual reality (VR), and digital twins. Traditional methods rely on computationally intensive iterative optimization in a complex chain, limiting their applicability in real-world scenarios. Recent advances in feed-forward approaches, driven by deep learning, have revolutionized this field by enabling fast and generalizable 3D reconstruction and view synthesis. This survey offers a comprehensive review of feed-forward techniques for 3D reconstruction and view synthesis, with a taxonomy according to the underlying representation architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural Radiance Fields (NeRF), etc. We examine key tasks such as pose-free reconstruction, dynamic 3D reconstruction, and 3D-aware image and video synthesis, highlighting their applications in digital humans, SLAM, robotics, and beyond. In addition, we review commonly used datasets with detailed statistics, along with evaluation protocols for various downstream tasks. We conclude by discussing open research challenges and promising directions for future work, emphasizing the potential of feed-forward approaches to advance the state of the art in 3D vision. </p>
<blockquote>
<p>3Dé‡å»ºå’Œè§†å›¾åˆæˆæ˜¯è®¡ç®—æœºè§†è§‰ã€å›¾å½¢å­¦å’Œæ²‰æµ¸å¼æŠ€æœ¯ï¼ˆå¦‚å¢å¼ºç°å®ï¼ˆARï¼‰ã€è™šæ‹Ÿç°å®ï¼ˆVRï¼‰å’Œæ•°å­—å­ªç”Ÿï¼‰ä¸­çš„åŸºç¡€é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•ä¾èµ–äºå¤æ‚é“¾ä¸­çš„è®¡ç®—å¯†é›†å‹è¿­ä»£ä¼˜åŒ–ï¼Œè¿™åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨å…·æœ‰ä¸€å®šçš„å±€é™æ€§ã€‚æœ€è¿‘ï¼Œç”±æ·±åº¦å­¦ä¹ é©±åŠ¨çš„å‰é¦ˆæ–¹æ³•çš„æœ€æ–°è¿›å±•å·²ç»å½»åº•æ”¹å˜äº†è¿™ä¸€é¢†åŸŸï¼Œå®ç°äº†å¿«é€Ÿå’Œé€šç”¨çš„3Dé‡å»ºå’Œè§†å›¾åˆæˆã€‚è¿™ç¯‡ç»¼è¿°å¯¹å‰é¦ˆæŠ€æœ¯åœ¨3Dé‡å»ºå’Œè§†å›¾åˆæˆæ–¹é¢çš„åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„å›é¡¾ï¼Œå¹¶æ ¹æ®åŸºç¡€è¡¨ç¤ºæ¶æ„è¿›è¡Œäº†åˆ†ç±»ï¼ŒåŒ…æ‹¬ç‚¹äº‘ã€3Dé«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰ã€ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰ç­‰ã€‚æˆ‘ä»¬ç ”ç©¶äº†å…³é”®ä»»åŠ¡ï¼Œå¦‚å§¿æ€è‡ªç”±é‡å»ºã€åŠ¨æ€3Dé‡å»ºå’Œ3Dæ„ŸçŸ¥å›¾åƒå’Œè§†é¢‘åˆæˆï¼Œé‡ç‚¹ä»‹ç»äº†å®ƒä»¬åœ¨æ•°å­—äººç±»ã€SLAMã€æœºå™¨äººæŠ€æœ¯ç­‰é¢†åŸŸçš„åº”ç”¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å›é¡¾äº†å¸¸ç”¨æ•°æ®é›†åŠå…¶è¯¦ç»†ç»Ÿè®¡æ•°æ®ï¼Œä»¥åŠå„ç§ä¸‹æ¸¸ä»»åŠ¡çš„è¯„ä¼°åè®®ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å½“å‰çš„ç ”ç©¶æŒ‘æˆ˜ä»¥åŠæœªæ¥å·¥ä½œçš„æœ‰å‰é€”çš„æ–¹å‘ï¼Œå¼ºè°ƒäº†å‰é¦ˆæ–¹æ³•åœ¨æ¨åŠ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå‘å±•çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14501v2">PDF</a> A project page associated with this survey is available at   <a target="_blank" rel="noopener" href="https://fnzhan.com/projects/Feed-Forward-3D">https://fnzhan.com/projects/Feed-Forward-3D</a></p>
<p><strong>Summary</strong><br>ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰åœ¨ä¸‰ç»´é‡å»ºå’Œè§†å›¾åˆæˆä¸­çš„ç»¼åˆç ”ç©¶ã€‚è¯¥æ–‡å›é¡¾äº†åŸºäºç‚¹äº‘ã€ä¸‰ç»´é«˜æ–¯å–·æº…ï¼ˆNeRFï¼‰ã€æ·±åº¦å­¦ä¹ ç­‰æ–°å…´æŠ€æœ¯çš„é¢å‘ä»»åŠ¡æ–¹æ³•çš„æ¼”å˜åŠä¼˜åŠ£ç‚¹ã€‚å¼ºè°ƒåº”ç”¨åŒ…æ‹¬åŠ¨æ€ä¸‰ç»´é‡å»ºå’Œåœºæ™¯è®¤çŸ¥çš„è§†é¢‘å›¾åƒåˆæˆç­‰ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜è®¨è®ºäº†æ•°æ®é›†å’Œè¯„ä¼°åè®®ï¼Œå¹¶å±•æœ›äº†æœªæ¥ç ”ç©¶æŒ‘æˆ˜å’Œæ½œåœ¨æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä»‹ç»äº†åŸºäºæ·±åº¦å­¦ä¹ çš„å³æ—¶æ–¹æ³•ï¼ˆfeed-forward approachesï¼‰åœ¨ä¸‰ç»´é‡å»ºå’Œè§†å›¾åˆæˆä¸­çš„ä¼˜åŠ¿ã€‚ä¸ä¼ ç»Ÿçš„å¤æ‚è¿­ä»£ä¼˜åŒ–æ–¹æ³•ç›¸æ¯”ï¼Œè¿™äº›æ–¹æ³•æ›´ä¸ºå¿«é€Ÿå’Œæ™®éé€‚ç”¨ã€‚</li>
<li>æä¾›äº†ä¸€ç§å¯¹åŸºäºç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰å’Œå…¶ä»–æŠ€æœ¯ï¼ˆå¦‚ç‚¹äº‘å’Œä¸‰ç»´é«˜æ–¯å–·æº…ï¼‰çš„åˆ†ç±»æ–¹æ³•ï¼Œå¹¶å¯¹æ¯ç§æŠ€æœ¯çš„ç‰¹ç‚¹è¿›è¡Œäº†æ¦‚è¿°ã€‚</li>
<li>è®¨è®ºäº†å…³é”®ä»»åŠ¡ï¼Œå¦‚å§¿åŠ¿æ— å…³é‡å»ºã€åŠ¨æ€ä¸‰ç»´é‡å»ºå’Œä¸‰ç»´æ„ŸçŸ¥å›¾åƒå’Œè§†é¢‘åˆæˆç­‰ï¼Œå¹¶å±•ç¤ºäº†å®ƒä»¬åœ¨æ•°å­—äººç±»ã€SLAMï¼ˆå³æ—¶å®šä½å’Œåœ°å›¾æ„å»ºï¼‰ã€æœºå™¨äººç­‰é¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>åˆ†æäº†ç°æœ‰æ•°æ®é›†å¹¶æä¾›äº†è¯¦ç»†çš„ç»Ÿè®¡æ•°æ®ï¼ŒåŒæ—¶æ¢è®¨äº†ç”¨äºå„ç§ä¸‹æ¸¸ä»»åŠ¡çš„è¯„ä¼°åè®®ã€‚</li>
<li>æœ€åï¼Œæ–‡ç« æ€»ç»“äº†å½“å‰ç ”ç©¶çš„å¼€æ”¾æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶æ–¹å‘ï¼Œå¹¶å¼ºè°ƒäº†é¢å‘å³æ—¶æ–¹æ³•åœ¨æ¨åŠ¨ä¸‰ç»´è§†è§‰é¢†åŸŸä¸­çš„æ½œåŠ›ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-49a484bb983abad5bc0f6fb7f24c2ea8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af67e8f5ffb0c8ebb2831103199b100a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4bfa97f29caebec405ae1e8e652e7e2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-950245207a32bfbb23e9d14eb54d7f7a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e8f9d80f8c8e9a5d566a28ba69100a3.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="EPSilon-Efficient-Point-Sampling-for-Lightening-of-Hybrid-based-3D-Avatar-Generation"><a href="#EPSilon-Efficient-Point-Sampling-for-Lightening-of-Hybrid-based-3D-Avatar-Generation" class="headerlink" title="EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D   Avatar Generation"></a>EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D   Avatar Generation</h2><p><strong>Authors:Seungjun Moon, Sangjoon Yu, Gyeong-Moon Park</strong></p>
<p>The rapid advancement of neural radiance fields (NeRF) has paved the way to generate animatable human avatars from a monocular video. However, the sole usage of NeRF suffers from a lack of details, which results in the emergence of hybrid representation that utilizes SMPL-based mesh together with NeRF representation. While hybrid-based models show photo-realistic human avatar generation qualities, they suffer from extremely slow inference due to their deformation scheme: to be aligned with the mesh, hybrid-based models use the deformation based on SMPL skinning weights, which needs high computational costs on each sampled point. We observe that since most of the sampled points are located in empty space, they do not affect the generation quality but result in inference latency with deformation. In light of this observation, we propose EPSilon, a hybrid-based 3D avatar generation scheme with novel efficient point sampling strategies that boost both training and inference. In EPSilon, we propose two methods to omit empty points at rendering; empty ray omission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that progress through the empty space. Then, EIO narrows down the sampling interval on the ray, which wipes out the region not occupied by either clothes or mesh. The delicate sampling scheme of EPSilon enables not only great computational cost reduction during deformation but also the designation of the important regions to be sampled, which enables a single-stage NeRF structure without hierarchical sampling. Compared to existing methods, EPSilon maintains the generation quality while using only 3.9% of sampled points and achieves around 20 times faster inference, together with 4 times faster training convergence. We provide video results on <a target="_blank" rel="noopener" href="https://github.com/seungjun-moon/epsilon">https://github.com/seungjun-moon/epsilon</a>. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºå®ç°ä»å•ç›®è§†é¢‘ä¸­ç”Ÿæˆå¯åŠ¨ç”»çš„äººç±»åŒ–èº«é“ºå¹³äº†é“è·¯ã€‚ç„¶è€Œï¼Œä»…ä»…ä½¿ç”¨NeRFä¼šå¯¼è‡´ç»†èŠ‚ä¸è¶³ï¼Œè¿™å¯¼è‡´äº†åˆ©ç”¨åŸºäºSMPLçš„ç½‘æ ¼ä¸NeRFè¡¨ç¤ºç›¸ç»“åˆçš„æ··åˆè¡¨ç¤ºçš„å‡ºç°ã€‚è™½ç„¶æ··åˆæ¨¡å‹è¡¨ç°å‡ºé€¼çœŸçš„äººç±»åŒ–èº«ç”Ÿæˆè´¨é‡ï¼Œä½†ç”±äºå…¶å˜å½¢æ–¹æ¡ˆï¼Œå®ƒä»¬é­å—äº†ææ…¢çš„æ¨ç†è¿‡ç¨‹ï¼šä¸ºäº†ä¸ç½‘æ ¼å¯¹é½ï¼Œæ··åˆæ¨¡å‹ä½¿ç”¨åŸºäºSMPLè’™çš®æƒé‡çš„å˜å½¢ï¼Œè¿™åœ¨æ¯ä¸ªé‡‡æ ·ç‚¹ä¸Šéƒ½éœ€è¦å¾ˆé«˜çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œç”±äºå¤§å¤šæ•°é‡‡æ ·ç‚¹ä½äºç©ºç™½ç©ºé—´ä¸­ï¼Œå®ƒä»¬å¹¶ä¸å½±å“ç”Ÿæˆè´¨é‡ï¼Œä½†å´å¯¼è‡´äº†å…·æœ‰å˜å½¢çš„æ¨ç†å»¶è¿Ÿã€‚é‰´äºæ­¤è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†EPSILONï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ··åˆçš„3DåŒ–èº«ç”Ÿæˆæ–¹æ¡ˆï¼Œå…·æœ‰æ–°é¢–çš„é«˜æ•ˆç‚¹é‡‡æ ·ç­–ç•¥ï¼Œå¯ä»¥åŠ é€Ÿè®­ç»ƒå’Œæ¨ç†ã€‚åœ¨EPSILONä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–¹æ³•æ¥çœç•¥æ¸²æŸ“ä¸­çš„ç©ºç™½ç‚¹ï¼šç©ºç™½å°„çº¿çœç•¥ï¼ˆEROï¼‰å’Œç©ºç™½é—´éš”çœç•¥ï¼ˆEIOï¼‰ã€‚EROæ¶ˆé™¤äº†ç©¿è¿‡ç©ºç™½ç©ºé—´çš„å°„çº¿ã€‚ç„¶åï¼ŒEIOç¼©å°äº†å°„çº¿ä¸Šçš„é‡‡æ ·é—´éš”ï¼Œä»è€Œæ¶ˆé™¤äº†æœªè¢«è¡£æœæˆ–ç½‘æ ¼å æ®çš„åŒºåŸŸã€‚EPSILONçš„ç²¾ç»†é‡‡æ ·æ–¹æ¡ˆä¸ä»…å¤§å¤§é™ä½äº†å˜å½¢è¿‡ç¨‹ä¸­çš„è®¡ç®—æˆæœ¬ï¼Œè¿˜èƒ½æŒ‡å®šè¦é‡‡æ ·çš„é‡è¦åŒºåŸŸï¼Œä»è€Œå®ç°æ— éœ€åˆ†å±‚é‡‡æ ·çš„å•é˜¶æ®µNeRFç»“æ„ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒEPSILONåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶ï¼Œä»…ä½¿ç”¨3.9%çš„é‡‡æ ·ç‚¹ï¼Œå®ç°äº†çº¦20å€çš„å¿«é€Ÿæ¨ç†ï¼Œä»¥åŠ4å€çš„è®­ç»ƒæ”¶æ•›é€Ÿåº¦ã€‚è§†é¢‘ç»“æœå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/seungjun-moon/epsilon">https://github.com/seungjun-moon/epsilon</a>ä¸ŠæŸ¥çœ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13648v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºNeRFæŠ€æœ¯çš„åŠ¨ç”»äººç‰©ç”Ÿæˆæ–¹æ³•ã€‚ä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨NeRFç»“åˆSMPLç½‘æ ¼æ¨¡å‹ç”Ÿæˆäººç‰©ï¼Œä½†å­˜åœ¨ç»†èŠ‚ä¸è¶³å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºEPSilonçš„æ··åˆæ¨¡å‹ï¼Œé‡‡ç”¨é«˜æ•ˆçš„ç‚¹é‡‡æ ·ç­–ç•¥æå‡è®­ç»ƒå’Œæ¨ç†é€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡çš„ç”Ÿæˆæ•ˆæœã€‚EPSILONé€šè¿‡å‰”é™¤æ— æ•ˆé‡‡æ ·ç‚¹ï¼Œå®ç°äº†å¿«é€Ÿå˜å½¢å’Œå•é˜¶æ®µNeRFç»“æ„ï¼Œæ˜¾è‘—æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFæŠ€æœ¯ç»“åˆSMPLç½‘æ ¼æ¨¡å‹ç”ŸæˆåŠ¨ç”»äººç‰©é¢ä¸´ç»†èŠ‚ä¸è¶³å’Œè®¡ç®—æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</li>
<li>EPSILONæ¨¡å‹é‡‡ç”¨é«˜æ•ˆçš„ç‚¹é‡‡æ ·ç­–ç•¥ï¼Œå‰”é™¤æ— æ•ˆé‡‡æ ·ç‚¹ä»¥æé«˜è®¡ç®—å’Œè®­ç»ƒæ•ˆç‡ã€‚</li>
<li>EPSILONå®ç°äº†å¿«é€Ÿå˜å½¢å’Œå•é˜¶æ®µNeRFç»“æ„ï¼Œæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚</li>
<li>EPSILONåœ¨ä¿æŒé«˜è´¨é‡ç”Ÿæˆæ•ˆæœçš„åŒæ—¶ï¼Œå®ç°äº†çº¦20å€åŠ é€Ÿçš„æ¨ç†é€Ÿåº¦å’Œ4å€åŠ é€Ÿçš„è®­ç»ƒæ”¶æ•›é€Ÿåº¦ã€‚</li>
<li>EPSILONæ¨¡å‹åœ¨GitHubä¸Šæä¾›äº†è§†é¢‘ç»“æœï¼Œä¾›å…¬ä¼—æŸ¥çœ‹å’Œå‚è€ƒã€‚</li>
<li>EPSILONæ¨¡å‹çš„æå‡ºè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­çš„ä¸€äº›é—®é¢˜ï¼Œå¦‚é‡‡æ ·ç‚¹å¤šã€è®¡ç®—é‡å¤§ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13648">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-35fe569c74b5451b0808d6943d05bf01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4d37b04335d7a85c6e8cb20a5e5e3a09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9863210d15f80304d8a29b420b591442.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="VolSegGS-Segmentation-and-Tracking-in-Dynamic-Volumetric-Scenes-via-Deformable-3D-Gaussians"><a href="#VolSegGS-Segmentation-and-Tracking-in-Dynamic-Volumetric-Scenes-via-Deformable-3D-Gaussians" class="headerlink" title="VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via   Deformable 3D Gaussians"></a>VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via   Deformable 3D Gaussians</h2><p><strong>Authors:Siyuan Yao, Chaoli Wang</strong></p>
<p>Visualization of large-scale time-dependent simulation data is crucial for domain scientists to analyze complex phenomena, but it demands significant I&#x2F;O bandwidth, storage, and computational resources. To enable effective visualization on local, low-end machines, recent advances in view synthesis techniques, such as neural radiance fields, utilize neural networks to generate novel visualizations for volumetric scenes. However, these methods focus on reconstruction quality rather than facilitating interactive visualization exploration, such as feature extraction and tracking. We introduce VolSegGS, a novel Gaussian splatting framework that supports interactive segmentation and tracking in dynamic volumetric scenes for exploratory visualization and analysis. Our approach utilizes deformable 3D Gaussians to represent a dynamic volumetric scene, allowing for real-time novel view synthesis. For accurate segmentation, we leverage the view-independent colors of Gaussians for coarse-level segmentation and refine the results with an affinity field network for fine-level segmentation. Additionally, by embedding segmentation results within the Gaussians, we ensure that their deformation enables continuous tracking of segmented regions over time. We demonstrate the effectiveness of VolSegGS with several time-varying datasets and compare our solutions against state-of-the-art methods. With the ability to interact with a dynamic scene in real time and provide flexible segmentation and tracking capabilities, VolSegGS offers a powerful solution under low computational demands. This framework unlocks exciting new possibilities for time-varying volumetric data analysis and visualization. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ—¶å˜ä»¿çœŸæ•°æ®çš„å¯è§†åŒ–å¯¹äºé¢†åŸŸç§‘å­¦å®¶åˆ†æå¤æ‚ç°è±¡è‡³å…³é‡è¦ï¼Œä½†è¿™éœ€è¦å·¨å¤§çš„è¾“å…¥&#x2F;è¾“å‡ºå¸¦å®½ã€å­˜å‚¨å’Œè®¡ç®—èµ„æºã€‚ä¸ºäº†åœ¨æœ¬åœ°ä½ç«¯æœºå™¨ä¸Šå®ç°æœ‰æ•ˆçš„å¯è§†åŒ–ï¼Œæœ€è¿‘çš„è§†å›¾åˆæˆæŠ€æœ¯ï¼ˆå¦‚ç¥ç»è¾å°„åœºï¼‰åˆ©ç”¨ç¥ç»ç½‘ç»œä¸ºä½“ç§¯åœºæ™¯ç”Ÿæˆæ–°å‹å¯è§†åŒ–ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¾§é‡äºé‡å»ºè´¨é‡ï¼Œè€Œä¸æ˜¯ä¿ƒè¿›äº¤äº’å¼å¯è§†åŒ–æ¢ç´¢ï¼Œå¦‚ç‰¹å¾æå–å’Œè·Ÿè¸ªã€‚æˆ‘ä»¬å¼•å…¥äº†VolSegGSï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„é«˜æ–¯å–·æ¶‚æ¡†æ¶ï¼Œæ”¯æŒåŠ¨æ€ä½“ç§¯åœºæ™¯ä¸­çš„äº¤äº’å¼åˆ†å‰²å’Œè·Ÿè¸ªï¼Œç”¨äºæ¢ç´¢æ€§å¯è§†åŒ–å’Œåˆ†æã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¯å˜å½¢çš„ä¸‰ç»´é«˜æ–¯æ¥è¡¨ç¤ºåŠ¨æ€ä½“ç§¯åœºæ™¯ï¼Œä»è€Œå®ç°å®æ—¶çš„æ–°å‹è§†å›¾åˆæˆã€‚ä¸ºäº†è·å¾—å‡†ç¡®çš„åˆ†å‰²ï¼Œæˆ‘ä»¬åˆ©ç”¨é«˜æ–¯è§†å›¾ç‹¬ç«‹é¢œè‰²è¿›è¡Œç²—ç•¥åˆ†å‰²ï¼Œå¹¶ä½¿ç”¨äº²å’Œåœºç½‘ç»œå¯¹ç»“æœè¿›è¡Œç»†åŒ–ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†åˆ†å‰²ç»“æœåµŒå…¥é«˜æ–¯ä¸­ï¼Œæˆ‘ä»¬ç¡®ä¿å®ƒä»¬çš„å˜å½¢èƒ½å¤Ÿå®æ—¶è·Ÿè¸ªåˆ†å‰²åŒºåŸŸçš„è¿ç»­å˜åŒ–ã€‚æˆ‘ä»¬é€šè¿‡å¤šä¸ªæ—¶å˜æ•°æ®é›†å±•ç¤ºäº†VolSegGSçš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„è§£å†³æ–¹æ¡ˆè¿›è¡Œäº†æ¯”è¾ƒã€‚VolSegGSèƒ½å¤Ÿåœ¨å®æ—¶ä¸åŠ¨æ€åœºæ™¯è¿›è¡Œäº¤äº’ï¼Œå¹¶æä¾›çµæ´»çš„åˆ†å‰²å’Œè·Ÿè¸ªåŠŸèƒ½ï¼Œå®ƒåœ¨ä½è®¡ç®—éœ€æ±‚ä¸‹æä¾›äº†å¼ºå¤§çš„è§£å†³æ–¹æ¡ˆã€‚è¿™ä¸€æ¡†æ¶ä¸ºæ—¶å˜ä½“ç§¯æ•°æ®åˆ†æå’Œå¯è§†åŒ–å¼€å¯äº†ä»¤äººå…´å¥‹çš„æ–°å¯èƒ½æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12667v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§è§„æ¨¡æ—¶å˜ä»¿çœŸæ•°æ®å¯è§†åŒ–çš„ä¸€ç§æ–°æ–¹æ³•â€”â€”VolSegGSã€‚è¯¥æ–¹æ³•é‡‡ç”¨é«˜æ–¯å–·ç»˜æ¡†æ¶ï¼Œæ”¯æŒåŠ¨æ€ä½“ç§¯åœºæ™¯ä¸­çš„äº¤äº’å¼åˆ†å‰²å’Œè·Ÿè¸ªï¼Œä¸ºæ¢ç´¢æ€§å¯è§†åŒ–å’Œåˆ†ææä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚é€šè¿‡åˆ©ç”¨å¯å˜å½¢ä¸‰ç»´é«˜æ–¯æ•°è¡¨ç¤ºåŠ¨æ€ä½“ç§¯åœºæ™¯ï¼Œå®ç°å®æ—¶æ–°é¢–è§†å›¾åˆæˆã€‚è¯¥æ–¹æ³•åˆ©ç”¨é«˜æ–¯è§†ç‹¬ç«‹é¢œè‰²è¿›è¡Œç²—ç•¥åˆ†å‰²ï¼Œå¹¶é€šè¿‡äº²å’Œåœºç½‘ç»œè¿›è¡Œç²¾ç»†åˆ†å‰²ã€‚é€šè¿‡å°†åˆ†å‰²ç»“æœåµŒå…¥é«˜æ–¯æ•°ä¸­ï¼Œç¡®ä¿äº†åˆ†å‰²åŒºåŸŸçš„è¿ç»­è·Ÿè¸ªã€‚VolSegGSä¸ºæ—¶é—´å˜åŒ–æ•°æ®é›†çš„åˆ†æå’Œå¯è§†åŒ–æä¾›äº†å®æ—¶äº¤äº’ã€çµæ´»åˆ†å‰²å’Œè·Ÿè¸ªèƒ½åŠ›çš„å¼ºå¤§è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VolSegGSæ˜¯ä¸€ç§åŸºäºé«˜æ–¯å–·ç»˜æ¡†æ¶çš„æ–¹æ³•ï¼Œç”¨äºæ”¯æŒåŠ¨æ€ä½“ç§¯åœºæ™¯ä¸­çš„äº¤äº’å¼åˆ†å‰²å’Œè·Ÿè¸ªã€‚</li>
<li>æ–¹æ³•å®ç°äº†å®æ—¶æ–°é¢–è§†å›¾åˆæˆï¼Œé€šè¿‡åˆ©ç”¨å¯å˜å½¢ä¸‰ç»´é«˜æ–¯æ•°è¡¨ç¤ºåŠ¨æ€ä½“ç§¯åœºæ™¯ã€‚</li>
<li>åˆ©ç”¨é«˜æ–¯è§†ç‹¬ç«‹é¢œè‰²è¿›è¡Œç²—ç•¥åˆ†å‰²ï¼Œå¹¶ç»“åˆäº²å’Œåœºç½‘ç»œè¿›è¡Œç²¾ç»†åˆ†å‰²ã€‚</li>
<li>é€šè¿‡å°†åˆ†å‰²ç»“æœåµŒå…¥é«˜æ–¯æ•°ä¸­ï¼Œç¡®ä¿äº†åˆ†å‰²åŒºåŸŸçš„è¿ç»­è·Ÿè¸ªã€‚</li>
<li>VolSegGSå¯¹äºæ—¶é—´å˜åŒ–æ•°æ®é›†çš„åˆ†æå’Œå¯è§†åŒ–å…·æœ‰å¼ºå¤§çš„å®æ—¶äº¤äº’èƒ½åŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ»¡è¶³ä½è®¡ç®—éœ€æ±‚çš„åŒæ—¶ï¼Œæä¾›äº†çµæ´»çš„åˆ†å‰²å’Œè·Ÿè¸ªåŠŸèƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12667">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d039db9815d8967d5e13fa587adfcb7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c46d0fa884ad5e3eea34ab7436b9a20.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74a3107d81cad5f44ddb01fcfdd3a2fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-666922b0a73226da497a8cefc25ed06b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7a1c46cdc03992f2425b15016316f0bb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7351a63a7e171bfc123949af3e8ae96.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba695c8435195320e64ebcc320647a76.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="BRUM-Robust-3D-Vehicle-Reconstruction-from-360-Sparse-Images"><a href="#BRUM-Robust-3D-Vehicle-Reconstruction-from-360-Sparse-Images" class="headerlink" title="BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images"></a>BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images</h2><p><strong>Authors:Davide Di Nucci, Matteo Tomei, Guido Borghi, Luca Ciuffreda, Roberto Vezzani, Rita Cucchiara</strong></p>
<p>Accurate 3D reconstruction of vehicles is vital for applications such as vehicle inspection, predictive maintenance, and urban planning. Existing methods like Neural Radiance Fields and Gaussian Splatting have shown impressive results but remain limited by their reliance on dense input views, which hinders real-world applicability. This paper addresses the challenge of reconstructing vehicles from sparse-view inputs, leveraging depth maps and a robust pose estimation architecture to synthesize novel views and augment training data. Specifically, we enhance Gaussian Splatting by integrating a selective photometric loss, applied only to high-confidence pixels, and replacing standard Structure-from-Motion pipelines with the DUSt3R architecture to improve camera pose estimation. Furthermore, we present a novel dataset featuring both synthetic and real-world public transportation vehicles, enabling extensive evaluation of our approach. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, showcasing the methodâ€™s ability to achieve high-quality reconstructions even under constrained input conditions. </p>
<blockquote>
<p>è½¦è¾†çš„ç²¾ç¡®3Dé‡å»ºå¯¹äºè½¦è¾†æ£€æµ‹ã€é¢„æµ‹æ€§ç»´æŠ¤å’ŒåŸå¸‚è§„åˆ’ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚ç°æœ‰çš„æ–¹æ³•ï¼Œå¦‚ç¥ç»ç½‘ç»œè¾å°„åœºå’Œé«˜æ–¯æ‹¼è´´æ³•ï¼Œå·²ç»å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œä½†å®ƒä»¬ä»ç„¶å—é™äºå¯¹å¯†é›†è¾“å…¥è§†å›¾çš„ä¾èµ–ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨ã€‚æœ¬æ–‡é’ˆå¯¹ç¨€ç–è§†å›¾è¾“å…¥ä¸‹çš„è½¦è¾†é‡å»ºé—®é¢˜ï¼Œåˆ©ç”¨æ·±åº¦å›¾å’Œç¨³å¥çš„å§¿æ€ä¼°è®¡æ¶æ„æ¥åˆæˆæ–°è§†å›¾å¹¶å¢å¼ºè®­ç»ƒæ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹é«˜ç½®ä¿¡åº¦åƒç´ ä»…åº”ç”¨é€‰æ‹©æ€§å…‰åº¦æŸå¤±ï¼Œæ”¹è¿›äº†é«˜æ–¯æ‹¼è´´æ³•ï¼Œå¹¶ç”¨DUSt3Ræ¶æ„å–ä»£äº†æ ‡å‡†ä»è¿åŠ¨æ¢å¤ç»“æ„ç®¡é“ï¼Œä»¥æé«˜ç›¸æœºå§¿æ€ä¼°è®¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ä¸€ä¸ªæ–°å‹æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«åˆæˆå’ŒçœŸå®ä¸–ç•Œçš„å…¬å…±äº¤é€šè½¦è¾†ï¼Œèƒ½å¤Ÿå¹¿æ³›è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå±•ç¤ºäº†è¯¥æ–¹æ³•åœ¨å—é™è¾“å…¥æ¡ä»¶ä¸‹å®ç°é«˜è´¨é‡é‡å»ºçš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12095v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>è¯¥è®ºæ–‡èšç„¦äºåˆ©ç”¨ç¨€ç–è§†å›¾è¿›è¡Œè½¦è¾†é‡å»ºçš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„é«˜æ–¯æ¸²æŸ“æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯é›†æˆäº†é€‰æ‹©æ€§å…‰åº¦æŸå¤±ï¼Œåªå¯¹é«˜ç½®ä¿¡åº¦åƒç´ åº”ç”¨æŸå¤±ã€‚è®ºæ–‡åŒæ—¶é‡‡ç”¨äº†æ·±åº¦åœ°å›¾å’Œä¸€ä¸ªå¼ºå¤§çš„å§¿æ€ä¼°è®¡æ¶æ„æ¥åˆæˆæ–°è§†è§’å¹¶æ‰©å……è®­ç»ƒæ•°æ®ã€‚å®éªŒç»“æœåœ¨å¤šä¸ªäººé€ è½¦è¾†æ•°æ®é›†ä¸Šå±•ç¤ºäº†å‰æ²¿æ€§èƒ½ï¼Œè¯æ˜äº†å³ä½¿åœ¨æœ‰é™çš„è¾“å…¥æ¡ä»¶ä¸‹ä¹Ÿèƒ½å®ç°é«˜è´¨é‡é‡å»ºçš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°å‹æ•°æ®é›†ï¼ŒåŒ…å«åˆæˆå’ŒçœŸå®å…¬å…±äº¤é€šè½¦è¾†æ•°æ®ï¼Œä¸ºè¯„ä¼°æ–¹æ³•æä¾›äº†æ›´å…¨é¢çš„è§†è§’ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é’ˆå¯¹è½¦è¾†çš„ä¸‰ç»´é‡å»ºè¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå¯¹å®é™…åº”ç”¨å¦‚è½¦è¾†æ£€æµ‹ã€é¢„æµ‹æ€§ç»´æŠ¤å’ŒåŸå¸‚è§„åˆ’å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ”¹è¿›çš„é«˜æ–¯æ¸²æŸ“æŠ€æœ¯ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•ä¾èµ–å¯†é›†è¾“å…¥è§†å›¾çš„é—®é¢˜ï¼Œå¢å¼ºäº†ç¨€ç–è§†å›¾è¾“å…¥ä¸‹çš„è½¦è¾†é‡å»ºèƒ½åŠ›ã€‚</li>
<li>é€šè¿‡æ·±åº¦åœ°å›¾å’Œå§¿æ€ä¼°è®¡æ¶æ„çš„åˆæˆæ–°è§†è§’æŠ€æœ¯ï¼Œæ‰©å……äº†è®­ç»ƒæ•°æ®ï¼Œæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å¼•å…¥é€‰æ‹©æ€§å…‰åº¦æŸå¤±ï¼Œä¼˜åŒ–é«˜ç½®ä¿¡åº¦åƒç´ çš„é‡å»ºè´¨é‡ã€‚</li>
<li>æ›¿ä»£äº†ä¼ ç»Ÿçš„ç»“æ„ä»è¿åŠ¨ï¼ˆSfMï¼‰ç®¡é“ï¼Œé‡‡ç”¨äº†DUSt3Ræ¶æ„è¿›è¡Œç›¸æœºå§¿æ€ä¼°è®¡ï¼Œæé«˜äº†ä¼°è®¡çš„å‡†ç¡®æ€§ã€‚</li>
<li>å¼€å‘äº†ä¸€ç§æ–°å‹æ•°æ®é›†ï¼ŒåŒ…å«åˆæˆå’ŒçœŸå®å…¬å…±äº¤é€šè½¦è¾†æ•°æ®ï¼Œä¸ºè¯„ä¼°è½¦è¾†é‡å»ºæ–¹æ³•æä¾›äº†æ›´å…¨é¢çš„è§†è§’ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3eb38d0f5c19b552cb3e6d84a3147a7b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8f02da53fc985a03d18d6bec63da7bf1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b50533b6b6a202a1f56263d39eb11cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc56db0b7993700e8b8d1c9a107f1819.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb58ba27e638589dd14db3e49f4e5a80.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Tile-and-Slide-A-New-Framework-for-Scaling-NeRF-from-Local-to-Global-3D-Earth-Observation"><a href="#Tile-and-Slide-A-New-Framework-for-Scaling-NeRF-from-Local-to-Global-3D-Earth-Observation" class="headerlink" title="Tile and Slide : A New Framework for Scaling NeRF from Local to Global   3D Earth Observation"></a>Tile and Slide : A New Framework for Scaling NeRF from Local to Global   3D Earth Observation</h2><p><strong>Authors:Camille Billouard, Dawa Derksen, Alexandre Constantin, Bruno Vallet</strong></p>
<p>Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D reconstruction from multiview satellite imagery. However, state-of-the-art NeRF methods are typically constrained to small scenes due to the memory footprint during training, which we study in this paper. Previous work on large-scale NeRFs palliate this by dividing the scene into NeRFs. This paper introduces Snake-NeRF, a framework that scales to large scenes. Our out-of-core method eliminates the need to load all images and networks simultaneously, and operates on a single device. We achieve this by dividing the region of interest into NeRFs that 3D tile without overlap. Importantly, we crop the images with overlap to ensure each NeRFs is trained with all the necessary pixels. We introduce a novel $2\times 2$ 3D tile progression strategy and segmented sampler, which together prevent 3D reconstruction errors along the tile edges. Our experiments conclude that large satellite images can effectively be processed with linear time complexity, on a single GPU, and without compromise in quality. </p>
<blockquote>
<p>ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æœ€è¿‘å·²æˆä¸ºä»å¤šè§†è§’å«æ˜Ÿå›¾åƒè¿›è¡Œ3Dé‡å»ºçš„ä¸€ç§èŒƒå¼ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„NeRFæ–¹æ³•é€šå¸¸å—é™äºå°åœºæ™¯ï¼Œå› ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜å ç”¨è¾ƒå¤§ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­å¯¹æ­¤è¿›è¡Œäº†ç ”ç©¶ã€‚ä¹‹å‰å…³äºå¤§è§„æ¨¡NeRFçš„å·¥ä½œé€šè¿‡å°†åœºæ™¯åˆ’åˆ†ä¸ºNeRFæ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»äº†Snake-NeRFæ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯æ‰©å±•åˆ°å¤§å‹åœºæ™¯ã€‚æˆ‘ä»¬çš„å¤–ç½®å­˜å‚¨æ–¹æ³•æ— éœ€åŒæ—¶åŠ è½½æ‰€æœ‰å›¾åƒå’Œç½‘ç»œï¼Œå¯åœ¨å•ä¸ªè®¾å¤‡ä¸Šè¿è¡Œã€‚æˆ‘ä»¬é€šè¿‡å°†æ„Ÿå…´è¶£åŒºåŸŸåˆ’åˆ†ä¸ºæ— é‡å çš„NeRFæ¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œé‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è£å‰ªäº†æœ‰é‡å çš„å›¾åƒï¼Œä»¥ç¡®ä¿æ¯ä¸ªNeRFéƒ½èƒ½ä½¿ç”¨æ‰€æœ‰å¿…è¦çš„åƒç´ è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„$ 2\times 2$ 3Dç“¦ç‰‡é€’è¿›ç­–ç•¥å’Œåˆ†æ®µé‡‡æ ·å™¨ï¼Œå®ƒä»¬å…±åŒé˜²æ­¢äº†ç“¦ç‰‡è¾¹ç¼˜å¤„çš„3Dé‡å»ºé”™è¯¯ã€‚æˆ‘ä»¬çš„å®éªŒå¾—å‡ºç»“è®ºï¼Œå¯ä»¥åœ¨å•ä¸ªGPUä¸Šä»¥çº¿æ€§æ—¶é—´å¤æ‚åº¦æœ‰æ•ˆåœ°å¤„ç†å¤§å‹å«æ˜Ÿå›¾åƒï¼Œä¸”ä¸ä¼šé™ä½è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01631v2">PDF</a> Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D   Vision Across Altitudes). Our code will be made public after the conference   at <a target="_blank" rel="noopener" href="https://github.com/Ellimac0/Snake-NeRF">https://github.com/Ellimac0/Snake-NeRF</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Neural Radiance Fieldsï¼ˆNeRFï¼‰åœ¨å¤§è§„æ¨¡åœºæ™¯ä¸‰ç»´é‡å»ºä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹NeRFåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜å ç”¨é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSnake-NeRFçš„æ¡†æ¶ï¼Œé€šè¿‡åˆ’åˆ†æ„Ÿå…´è¶£åŒºåŸŸä¸ºæ— é‡å çš„NeRFsï¼Œå®ç°äº†å¤§è§„æ¨¡åœºæ™¯ä¸‹çš„NeRFé‡å»ºã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„$2\times 2$ä¸‰ç»´å¹³é“ºè¿›å±•ç­–ç•¥å’Œåˆ†æ®µé‡‡æ ·å™¨ï¼Œæœ‰æ•ˆé¿å…äº†åœ¨å¹³é“ºè¾¹ç¼˜å‡ºç°ä¸‰ç»´é‡å»ºé”™è¯¯ã€‚å®éªŒè¯æ˜ï¼Œå¯ä»¥åœ¨å•ä¸ªGPUä¸Šä»¥çº¿æ€§æ—¶é—´å¤æ‚åº¦å¤„ç†å¤§è§„æ¨¡å«æ˜Ÿå›¾åƒï¼Œä¸”ä¸å½±å“è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeRFè¢«åº”ç”¨äºå¤§è§„æ¨¡åœºæ™¯çš„ä¸‰ç»´é‡å»ºã€‚</li>
<li>Snake-NeRFæ¡†æ¶è§£å†³äº†NeRFåœ¨å¤„ç†å¤§è§„æ¨¡åœºæ™¯æ—¶çš„å†…å­˜å ç”¨é—®é¢˜ã€‚</li>
<li>Snake-NeRFé€šè¿‡å°†æ„Ÿå…´è¶£åŒºåŸŸåˆ’åˆ†ä¸ºæ— é‡å çš„NeRFsæ¥å®ç°å¤§è§„æ¨¡åœºæ™¯ä¸‹çš„NeRFé‡å»ºã€‚</li>
<li>Snake-NeRFé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„$2\times 2$ä¸‰ç»´å¹³é“ºè¿›å±•ç­–ç•¥ï¼Œæœ‰æ•ˆé¿å…äº†åœ¨å¹³é“ºè¾¹ç¼˜çš„ä¸‰ç»´é‡å»ºé”™è¯¯ã€‚</li>
<li>Snake-NeRFé€šè¿‡åˆ†æ®µé‡‡æ ·å™¨æé«˜äº†é‡å»ºè´¨é‡ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒSnake-NeRFå¯ä»¥åœ¨å•ä¸ªGPUä¸Šä»¥çº¿æ€§æ—¶é—´å¤æ‚åº¦å¤„ç†å¤§è§„æ¨¡å«æ˜Ÿå›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0ae336841b772112ec01a5195e923df7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ea99bc0927ffd3340f79fd512d2ffdfe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d336537fb08acda13d414dccb6f5325c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4696274ef189063314c463eaf0844fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b3e3fbef7325075b2076ad53a6d2d67.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="FOCI-Trajectory-Optimization-on-Gaussian-Splats"><a href="#FOCI-Trajectory-Optimization-on-Gaussian-Splats" class="headerlink" title="FOCI: Trajectory Optimization on Gaussian Splats"></a>FOCI: Trajectory Optimization on Gaussian Splats</h2><p><strong>Authors:Mario Gomez Andreu, Maximum Wilder-Smith, Victor Klemm, Vaishakh Patil, Jesus Tordesillas, Marco Hutter</strong></p>
<p>3D Gaussian Splatting (3DGS) has recently gained popularity as a faster alternative to Neural Radiance Fields (NeRFs) in 3D reconstruction and view synthesis methods. Leveraging the spatial information encoded in 3DGS, this work proposes FOCI (Field Overlap Collision Integral), an algorithm that is able to optimize trajectories directly on the Gaussians themselves. FOCI leverages a novel and interpretable collision formulation for 3DGS using the notion of the overlap integral between Gaussians. Contrary to other approaches, which represent the robot with conservative bounding boxes that underestimate the traversability of the environment, we propose to represent the environment and the robot as Gaussian Splats. This not only has desirable computational properties, but also allows for orientation-aware planning, allowing the robot to pass through very tight and narrow spaces. We extensively test our algorithm in both synthetic and real Gaussian Splats, showcasing that collision-free trajectories for the ANYmal legged robot that can be computed in a few seconds, even with hundreds of thousands of Gaussians making up the environment. The project page and code are available at <a target="_blank" rel="noopener" href="https://rffr.leggedrobotics.com/works/foci/">https://rffr.leggedrobotics.com/works/foci/</a> </p>
<blockquote>
<p>è¿‘æœŸï¼Œä¸‰ç»´é«˜æ–¯æ‰©å±•ï¼ˆ3DGSï¼‰ä½œä¸ºä¸‰ç»´é‡å»ºå’Œè§†è§’åˆæˆæ–¹æ³•ä¸­ç¥ç»è¾å°„åœºï¼ˆNeRFsï¼‰çš„å¿«é€Ÿæ›¿ä»£æ–¹æ¡ˆè€Œå¤‡å—å…³æ³¨ã€‚å€ŸåŠ©ç¼–ç åœ¨ä¸‰ç»´é«˜æ–¯æ‰©å±•ä¸­çš„ç©ºé—´ä¿¡æ¯ï¼Œæœ¬ç ”ç©¶æå‡ºäº†FOCIï¼ˆåŸºäºåœºé‡å ç¢°æ’ç§¯åˆ†ç®—æ³•ï¼‰ï¼Œå®ƒèƒ½å¤Ÿç›´æ¥åœ¨é«˜æ–¯å‡½æ•°æœ¬èº«ä¸Šä¼˜åŒ–è½¨è¿¹ã€‚FOCIåˆ©ç”¨é«˜æ–¯ä¹‹é—´é‡å ç§¯åˆ†çš„æ¦‚å¿µï¼Œä¸ºä¸‰ç»´é«˜æ–¯æ‰©å±•æå‡ºäº†ä¸€ç§æ–°é¢–ä¸”å¯è§£é‡Šæ€§çš„ç¢°æ’å…¬å¼ã€‚ä¸å…¶ä»–æ–¹æ³•ä¸åŒï¼Œè¿™äº›æ–¹æ³•ä½¿ç”¨ä¿å®ˆçš„è¾¹ç•Œæ¡†æ¥è¡¨ç¤ºæœºå™¨äººï¼Œä½ä¼°äº†ç¯å¢ƒçš„å¯é€šè¡Œæ€§ï¼Œæˆ‘ä»¬æè®®å°†ç¯å¢ƒå’Œæœºå™¨äººè¡¨ç¤ºä¸ºé«˜æ–¯æ‰©å±•ã€‚è¿™ä¸ä»…å…·æœ‰ç†æƒ³åŒ–çš„è®¡ç®—å±æ€§ï¼Œè€Œä¸”å…è®¸æ ¹æ®æ–¹å‘è¿›è¡Œè§„åˆ’ï¼Œå…è®¸æœºå™¨äººåœ¨éå¸¸ç‹­çª„çš„ç©ºé—´ä¸­é€šè¿‡ã€‚æˆ‘ä»¬åœ¨åˆæˆå’ŒçœŸå®çš„é«˜æ–¯æ‰©å±•ä¸­éƒ½æµ‹è¯•äº†æˆ‘ä»¬çš„ç®—æ³•ï¼Œå±•ç¤ºäº†å³ä½¿åœ¨ç”±æ•°åä¸‡é«˜æ–¯ç»„æˆçš„ç¯å¢ƒä¸­ï¼Œä¹Ÿèƒ½åœ¨å‡ ç§’é’Ÿå†…è®¡ç®—å‡ºANYmalæ­¥è¡Œæœºå™¨äººçš„æ— ç¢°æ’è½¨è¿¹ã€‚é¡¹ç›®é¡µé¢å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://rffr.leggedrobotics.com/works/foci/%E6%89%BE%E5%88%B0%E3%80%82">https://rffr.leggedrobotics.com/works/foci/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.08510v2">PDF</a> 8 pages, 8 figures, Mario Gomez Andreu and Maximum Wilder-Smith   contributed equally</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºä¸‰ç»´é«˜æ–¯æ’å€¼ï¼ˆ3DGSï¼‰æŠ€æœ¯çš„FOCIç®—æ³•ï¼Œè¯¥ç®—æ³•ä¼˜åŒ–äº†è½¨è¿¹ç”Ÿæˆçš„æ•ˆç‡ï¼Œåœ¨ä¸‰ç»´é‡å»ºå’Œè§†å›¾åˆæˆæ–¹æ³•ä¸­æˆä¸ºä¸€ç§å¿«é€Ÿæ›¿ä»£ç¥ç»ç½‘ç»œè¾å°„åœºï¼ˆNeRFï¼‰çš„æ–¹æ³•ã€‚FOCIåˆ©ç”¨æ–°é¢–çš„ç¢°æ’å…¬å¼å¯¹Gaussiansè¿›è¡Œé‡å ç§¯åˆ†å¤„ç†ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ–¹æ³•æ›´ç²¾ç¡®åœ°ä»£è¡¨ç¯å¢ƒå’Œæœºå™¨äººã€‚æ­¤æ–¹æ³•å®ç°äº†å¿«é€Ÿã€ç²¾ç¡®çš„ç¯å¢ƒæ„ŸçŸ¥å’Œæœºå™¨äººè½¨è¿¹è§„åˆ’ï¼Œå°¤å…¶é€‚ç”¨äºç´§å‡‘ç‹­çª„ç©ºé—´å†…çš„è§„åˆ’ä»»åŠ¡ã€‚åœ¨åˆæˆå’ŒçœŸå®çš„é«˜æ–¯æ’å€¼ç¯å¢ƒä¸­è¿›è¡Œäº†å¹¿æ³›çš„æµ‹è¯•ï¼Œå±•ç¤ºäº†ç®—æ³•çš„é«˜æ•ˆæ€§ã€‚é¡¹ç›®é¡µé¢å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://rffr.leggedrobotics.com/works/foci/%E6%89%BE%E5%88%B0%E3%80%82">https://rffr.leggedrobotics.com/works/foci/æ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>FOCIç®—æ³•åŸºäºä¸‰ç»´é«˜æ–¯æ’å€¼æŠ€æœ¯ä¼˜åŒ–äº†è½¨è¿¹ç”Ÿæˆã€‚</li>
<li>åˆ©ç”¨æ–°é¢–çš„ç¢°æ’å…¬å¼è¿›è¡Œé«˜æ–¯é‡å ç§¯åˆ†å¤„ç†ã€‚</li>
<li>å°†ç¯å¢ƒå’Œæœºå™¨äººè¡¨ç¤ºä¸ºé«˜æ–¯æ’å€¼ï¼Œè€Œéä¿å®ˆçš„è¾¹ç•Œæ¡†ï¼Œæé«˜äº†ç¯å¢ƒæ„ŸçŸ¥çš„å‡†ç¡®æ€§ã€‚</li>
<li>å®ç°å¿«é€Ÿã€ç²¾ç¡®çš„ç¯å¢ƒæ„ŸçŸ¥å’Œæœºå™¨äººè½¨è¿¹è§„åˆ’ã€‚</li>
<li>é€‚ç”¨äºç´§å‡‘ç‹­çª„ç©ºé—´å†…çš„è§„åˆ’ä»»åŠ¡ã€‚</li>
<li>åœ¨åˆæˆå’ŒçœŸå®ç¯å¢ƒä¸­è¿›è¡Œäº†å¹¿æ³›çš„æµ‹è¯•ï¼ŒéªŒè¯äº†ç®—æ³•çš„é«˜æ•ˆæ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.08510">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2cd80fb72740fbfbe260b17f41ea682c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da25f21a2f3b282eac96ea984475e9cc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-722fd984a1aa4b3dd0a53cd3620d55bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97370c4728c76c97f079a6d7f97112e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-111b00395c3add5361c6281356bef41d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-64682faa2230a9f038a40d4b24a1ec53.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LookCloser-Frequency-aware-Radiance-Field-for-Tiny-Detail-Scene"><a href="#LookCloser-Frequency-aware-Radiance-Field-for-Tiny-Detail-Scene" class="headerlink" title="LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene"></a>LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene</h2><p><strong>Authors:Xiaoyu Zhang, Weihong Pan, Chong Bao, Xiyu Zhang, Xiaojun Xiang, Hanqing Jiang, Hujun Bao</strong></p>
<p>Humans perceive and comprehend their surroundings through information spanning multiple frequencies. In immersive scenes, people naturally scan their environment to grasp its overall structure while examining fine details of objects that capture their attention. However, current NeRF frameworks primarily focus on modeling either high-frequency local views or the broad structure of scenes with low-frequency information, which is limited to balancing both. We introduce FA-NeRF, a novel frequency-aware framework for view synthesis that simultaneously captures the overall scene structure and high-definition details within a single NeRF model. To achieve this, we propose a 3D frequency quantification method that analyzes the sceneâ€™s frequency distribution, enabling frequency-aware rendering. Our framework incorporates a frequency grid for fast convergence and querying, a frequency-aware feature re-weighting strategy to balance features across different frequency contents. Extensive experiments show that our method significantly outperforms existing approaches in modeling entire scenes while preserving fine details. Project page: <a target="_blank" rel="noopener" href="https://coscatter.github.io/LookCloser/">https://coscatter.github.io/LookCloser/</a> </p>
<blockquote>
<p>äººç±»é€šè¿‡è·¨è¶Šå¤šä¸ªé¢‘ç‡çš„ä¿¡æ¯æ¥æ„ŸçŸ¥å’Œç†è§£å‘¨å›´ç¯å¢ƒã€‚åœ¨æ²‰æµ¸å¼åœºæ™¯ä¸­ï¼Œäººä»¬è‡ªç„¶åœ°ä¼šæ‰«æç¯å¢ƒï¼Œä»¥æŠŠæ¡å…¶æ•´ä½“ç»“æ„ï¼ŒåŒæ—¶å…³æ³¨å¸å¼•ä»–ä»¬çš„ç‰©ä½“çš„ç»†èŠ‚ã€‚ç„¶è€Œï¼Œå½“å‰çš„NeRFæ¡†æ¶ä¸»è¦ä¾§é‡äºå¯¹é«˜é¢‘å±€éƒ¨è§†å›¾æˆ–åœºæ™¯å¹¿æ³›ç»“æ„çš„å»ºæ¨¡ï¼Œæ¶‰åŠä½é¢‘ä¿¡æ¯ï¼Œè¿™åœ¨å¹³è¡¡ä¸¤è€…æ—¶å­˜åœ¨å±€é™æ€§ã€‚æˆ‘ä»¬ä»‹ç»äº†FA-NeRFï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹é¢‘ç‡æ„ŸçŸ¥æ¡†æ¶ï¼Œç”¨äºè§†å›¾åˆæˆï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªNeRFæ¨¡å‹ä¸­åŒæ—¶æ•æ‰åœºæ™¯çš„æ•´ä½“ç»“æ„å’Œé«˜æ¸…ç»†èŠ‚ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸‰ç»´é¢‘ç‡é‡åŒ–æ–¹æ³•ï¼Œåˆ†æåœºæ™¯çš„é¢‘ç‡åˆ†å¸ƒï¼Œå®ç°é¢‘ç‡æ„ŸçŸ¥æ¸²æŸ“ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç»“åˆäº†é¢‘ç‡ç½‘æ ¼ä»¥å®ç°å¿«é€Ÿæ”¶æ•›å’ŒæŸ¥è¯¢ï¼Œä»¥åŠé¢‘ç‡æ„ŸçŸ¥ç‰¹å¾é‡åŠ æƒç­–ç•¥ï¼Œä»¥å¹³è¡¡ä¸åŒé¢‘ç‡å†…å®¹çš„åŠŸèƒ½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å»ºæ¨¡æ•´ä¸ªåœºæ™¯æ—¶æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ŒåŒæ—¶ä¿ç•™äº†ç»†èŠ‚ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://coscatter.github.io/LookCloser/">https://coscatter.github.io/LookCloser/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.18513v3">PDF</a> Accepted to CVPR 2025. Project page:   <a target="_blank" rel="noopener" href="https://coscatter.github.io/LookCloser">https://coscatter.github.io/LookCloser</a></p>
<p><strong>Summary</strong><br>NeRFæŠ€æœ¯é€šå¸¸åªèƒ½å¹³è¡¡å¯¹åœºæ™¯çš„æ•´ä½“ç»“æ„å’Œé«˜é¢‘ç»†èŠ‚å»ºæ¨¡çš„å…³æ³¨ï¼Œä½†æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé¢‘ç‡æ„ŸçŸ¥çš„NeRFæ¡†æ¶ï¼Œå¯åŒæ—¶æ•æ‰åœºæ™¯çš„æ•´ä½“ç»“æ„å’Œé«˜æ¸…ç»†èŠ‚ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„é¢‘ç‡é‡åŒ–æ–¹æ³•ï¼Œå¹¶é‡‡ç”¨äº†é¢‘ç‡ç½‘æ ¼å’Œç‰¹å¾é‡åŠ æƒç­–ç•¥ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œèƒ½æ›´å‡†ç¡®åœ°å»ºæ¨¡æ•´ä¸ªåœºæ™¯å¹¶ä¿ç•™ç»†èŠ‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å½“å‰NeRFæŠ€æœ¯éš¾ä»¥åŒæ—¶å…³æ³¨åœºæ™¯çš„æ•´ä½“ç»“æ„å’Œé«˜é¢‘ç»†èŠ‚å»ºæ¨¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é¢‘ç‡æ„ŸçŸ¥çš„NeRFæ¡†æ¶ï¼ˆFA-NeRFï¼‰ï¼Œå¯åŒæ—¶æ•æ‰åœºæ™¯çš„æ•´ä½“ç»“æ„å’Œé«˜æ¸…ç»†èŠ‚ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„é¢‘ç‡é‡åŒ–æ–¹æ³•ï¼Œç”¨äºåˆ†æåœºæ™¯çš„é¢‘ç‡åˆ†å¸ƒå¹¶å®ç°é¢‘ç‡æ„ŸçŸ¥æ¸²æŸ“ã€‚</li>
<li>é‡‡ç”¨é¢‘ç‡ç½‘æ ¼å®ç°å¿«é€Ÿæ”¶æ•›å’ŒæŸ¥è¯¢ã€‚</li>
<li>é‡‡ç”¨ç‰¹å¾é‡åŠ æƒç­–ç•¥å¹³è¡¡ä¸åŒé¢‘ç‡å†…å®¹ä¹‹é—´çš„ç‰¹å¾ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.18513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-357ba4ea334b198adbcdaedd109910f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0d6af326c03133e7876a4526aa26da6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b92560f81720ceacb5474f277e835d72.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-de36ecc4767a264f0af183c8b34f5f4a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="DeGauss-Dynamic-Static-Decomposition-with-Gaussian-Splatting-for-Distractor-free-3D-Reconstruction"><a href="#DeGauss-Dynamic-Static-Decomposition-with-Gaussian-Splatting-for-Distractor-free-3D-Reconstruction" class="headerlink" title="DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for   Distractor-free 3D Reconstruction"></a>DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for   Distractor-free 3D Reconstruction</h2><p><strong>Authors:Rui Wang, Quentin Lohmeyer, Mirko Meboldt, Siyu Tang</strong></p>
<p>Reconstructing clean, distractor-free 3D scenes from real-world captures remains a significant challenge, particularly in highly dynamic and cluttered settings such as egocentric videos. To tackle this problem, we introduce DeGauss, a simple and robust self-supervised framework for dynamic scene reconstruction based on a decoupled dynamic-static Gaussian Splatting design. DeGauss models dynamic elements with foreground Gaussians and static content with background Gaussians, using a probabilistic mask to coordinate their composition and enable independent yet complementary optimization. DeGauss generalizes robustly across a wide range of real-world scenarios, from casual image collections to long, dynamic egocentric videos, without relying on complex heuristics or extensive supervision. Experiments on benchmarks including NeRF-on-the-go, ADT, AEA, Hot3D, and EPIC-Fields demonstrate that DeGauss consistently outperforms existing methods, establishing a strong baseline for generalizable, distractor-free 3D reconstructionin highly dynamic, interaction-rich environments. Project page: <a target="_blank" rel="noopener" href="https://batfacewayne.github.io/DeGauss.io/">https://batfacewayne.github.io/DeGauss.io/</a> </p>
<blockquote>
<p>ä»çœŸå®ä¸–ç•Œçš„æ•æ‰ä¸­é‡å»ºå¹²å‡€ã€æ— å¹²æ‰°ç‰©çš„3Dåœºæ™¯ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜åº¦åŠ¨æ€å’Œæ‚ä¹±çš„ç¯å¢ƒä¸­ï¼Œå¦‚ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†DeGaussï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•è€Œç¨³å¥çš„è‡ªç›‘ç£åŠ¨æ€åœºæ™¯é‡å»ºæ¡†æ¶ï¼ŒåŸºäºè§£è€¦çš„åŠ¨æ€é™æ€é«˜æ–¯æ‹¼è´´è®¾è®¡ã€‚DeGaussä½¿ç”¨å‰æ™¯é«˜æ–¯å¯¹åŠ¨æ€å…ƒç´ è¿›è¡Œå»ºæ¨¡ï¼Œä½¿ç”¨èƒŒæ™¯é«˜æ–¯å¯¹é™æ€å†…å®¹è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä½¿ç”¨æ¦‚ç‡æ©è†œæ¥åè°ƒå®ƒä»¬çš„ç»„åˆï¼Œä»¥å®ç°ç‹¬ç«‹ä½†äº’è¡¥çš„ä¼˜åŒ–ã€‚DeGaussåœ¨å¹¿æ³›çš„å„ç§çœŸå®åœºæ™¯ä¸­è¡¨ç°ç¨³å¥ï¼Œä»éšæ„çš„å›¾åƒé›†åˆåˆ°æ¼«é•¿çš„åŠ¨æ€ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„è§†é¢‘ï¼Œæ— éœ€ä¾èµ–å¤æ‚çš„å¯å‘å¼æ–¹æ³•æˆ–å¹¿æ³›çš„ç›‘ç£ã€‚åœ¨åŒ…æ‹¬NeRF-on-the-goã€ADTã€AEAã€Hot3Då’ŒEPIC-Fieldsç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDeGausså§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºé«˜åº¦åŠ¨æ€ã€äº¤äº’ä¸°å¯Œçš„ç¯å¢ƒä¸­çš„é€šç”¨ã€æ— å¹²æ‰°ç‰©3Dé‡å»ºå»ºç«‹äº†å¼ºå¤§çš„åŸºçº¿ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://batfacewayne.github.io/DeGauss.io/">https://batfacewayne.github.io/DeGauss.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.13176v3">PDF</a> Accepted by ICCV 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>DeGaussæ˜¯ä¸€ç§ç®€å•è€Œç¨³å¥çš„è‡ªç›‘ç£åŠ¨æ€åœºæ™¯é‡å»ºæ¡†æ¶ï¼Œé€‚ç”¨äºä»çœŸå®ä¸–ç•Œæ•æ‰çš„å›¾åƒä¸­é‡å»ºå¹²å‡€ã€æ— å¹²æ‰°çš„3Dåœºæ™¯ã€‚è¯¥æ¡†æ¶é‡‡ç”¨è§£è€¦çš„åŠ¨æ€é™æ€é«˜æ–¯å–·ç»˜è®¾è®¡ï¼Œç”¨å‰æ™¯é«˜æ–¯å»ºæ¨¡åŠ¨æ€å…ƒç´ ï¼Œç”¨èƒŒæ™¯é«˜æ–¯å»ºæ¨¡é™æ€å†…å®¹ã€‚é€šè¿‡æ¦‚ç‡æ©ç åè°ƒå®ƒä»¬çš„ç»„åˆï¼Œå®ç°ç‹¬ç«‹ä½†äº’è¡¥çš„ä¼˜åŒ–ã€‚DeGaussåœ¨å¤šç§çœŸå®åœºæ™¯ï¼Œå¦‚æ—¥å¸¸å›¾åƒé›†å’ŒåŠ¨æ€ç¬¬ä¸€äººç§°è§†é¢‘ä¸­å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€ä¾èµ–å¤æ‚çš„å¯å‘å¼æ–¹æ³•æˆ–å¹¿æ³›çš„ç›‘ç£å³å¯å®ç°ã€‚åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDeGaussåœ¨é«˜åº¦åŠ¨æ€ã€äº¤äº’ä¸°å¯Œçš„ç¯å¢ƒä¸­å§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºæ— å¹²æ‰°çš„3Dé‡å»ºå»ºç«‹äº†å¼ºæœ‰åŠ›çš„åŸºçº¿ã€‚</p>
<p><strong>è¦ç‚¹è§£æ</strong></p>
<ol>
<li>DeGaussæ˜¯ä¸€ä¸ªé’ˆå¯¹çœŸå®ä¸–ç•Œæ•æ‰å›¾åƒçš„åŠ¨æ€åœºæ™¯é‡å»ºæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¹²å‡€ã€æ— å¹²æ‰°çš„3Dåœºæ™¯é‡å»ºé—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨åŠ¨æ€é™æ€è§£è€¦çš„é«˜æ–¯å–·ç»˜è®¾è®¡ï¼Œä»¥åŒºåˆ†åŠ¨æ€å…ƒç´ å’Œé™æ€å†…å®¹ã€‚</li>
<li>DeGaussåˆ©ç”¨æ¦‚ç‡æ©ç æ¥åè°ƒå‰æ™¯é«˜æ–¯å’ŒèƒŒæ™¯é«˜æ–¯ç»„åˆçš„æ„å»ºã€‚</li>
<li>è¯¥æ¡†æ¶å®ç°äº†ç‹¬ç«‹ä½†äº’è¡¥çš„ä¼˜åŒ–ï¼Œæé«˜äº†åœºæ™¯é‡å»ºçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚</li>
<li>DeGaussåœ¨å„ç§çœŸå®åœºæ™¯ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ—¥å¸¸å›¾åƒé›†å’ŒåŠ¨æ€ç¬¬ä¸€äººç§°è§†é¢‘ã€‚</li>
<li>å¯¹æ¯”å¤šä¸ªåŸºå‡†æµ‹è¯•çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDeGaussåœ¨é«˜åº¦åŠ¨æ€ã€äº¤äº’ä¸°å¯Œçš„ç¯å¢ƒä¸­æ€§èƒ½å“è¶Šï¼Œä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.13176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c801cc964a28e7d770fdac0f50082686.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25fa6963933556854a815a540ef9bf76.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1675db56c26089a87805becb4c76ae9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1e7b5969bc0735620e3292ff7bb7798d.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Category-level-Meta-learned-NeRF-Priors-for-Efficient-Object-Mapping"><a href="#Category-level-Meta-learned-NeRF-Priors-for-Efficient-Object-Mapping" class="headerlink" title="Category-level Meta-learned NeRF Priors for Efficient Object Mapping"></a>Category-level Meta-learned NeRF Priors for Efficient Object Mapping</h2><p><strong>Authors:Saad Ejaz, Hriday Bavle, Laura Ribeiro, Holger Voos, Jose Luis Sanchez-Lopez</strong></p>
<p>In 3D object mapping, category-level priors enable efficient object reconstruction and canonical pose estimation, requiring only a single prior per semantic category (e.g., chair, book, laptop, etc.). DeepSDF has been used predominantly as a category-level shape prior, but it struggles to reconstruct sharp geometry and is computationally expensive. In contrast, NeRFs capture fine details but have yet to be effectively integrated with category-level priors in a real-time multi-object mapping framework. To bridge this gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper that integrates category-level priors with object-level NeRFs to enhance reconstruction efficiency and enable canonical object pose estimation. PRENOM gets to know objects on a first-name basis by meta-learning on synthetic reconstruction tasks generated from open-source shape datasets. To account for object category variations, it employs a multi-objective genetic algorithm to optimize the NeRF architecture for each category, balancing reconstruction quality and training time. Additionally, prior-based probabilistic ray sampling directs sampling toward expected object regions, accelerating convergence and improving reconstruction quality under constrained resources. Experimental results highlight the ability of PRENOM to achieve high-quality reconstructions while maintaining computational feasibility. Specifically, comparisons with prior-free NeRF-based approaches on a synthetic dataset show a 21% lower Chamfer distance. Furthermore, evaluations against other approaches using shape priors on a noisy real-world dataset indicate a 13% improvement averaged across all reconstruction metrics, and comparable pose and size estimation accuracy, while being trained for 5$\times$ less time. Code available at: <a target="_blank" rel="noopener" href="https://github.com/snt-arg/PRENOM">https://github.com/snt-arg/PRENOM</a> </p>
<blockquote>
<p>åœ¨3Då¯¹è±¡æ˜ å°„ä¸­ï¼Œç±»åˆ«çº§å…ˆéªŒçŸ¥è¯†èƒ½å¤Ÿå®ç°é«˜æ•ˆçš„å¯¹è±¡é‡å»ºå’Œè§„èŒƒåŒ–å§¿æ€ä¼°è®¡ï¼Œåªéœ€è¦æ¯ä¸ªè¯­ä¹‰ç±»åˆ«ï¼ˆä¾‹å¦‚æ¤…å­ã€ä¹¦ç±ã€ç¬”è®°æœ¬ç”µè„‘ç­‰ï¼‰çš„ä¸€ä¸ªå…ˆéªŒçŸ¥è¯†å³å¯ã€‚DeepSDFä¸»è¦ç”¨ä½œç±»åˆ«çº§åˆ«çš„å½¢çŠ¶å…ˆéªŒï¼Œä½†åœ¨é‡å»ºé”åˆ©å‡ ä½•ç»“æ„æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè€Œä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒNeRFå¯ä»¥æ•æ‰ç²¾ç»†çš„ç»†èŠ‚ï¼Œä½†å°šæœªåœ¨å®æ—¶å¤šå¯¹è±¡æ˜ å°„æ¡†æ¶ä¸­æœ‰æ•ˆåœ°ä¸ç±»åˆ«çº§å…ˆéªŒç›¸ç»“åˆã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†PRENOMï¼ˆåŸºäºå…ˆéªŒçš„é«˜æ•ˆç¥ç»å¯¹è±¡æ˜ å°„å™¨ï¼‰ï¼Œå®ƒå°†ç±»åˆ«çº§å…ˆéªŒä¸å¯¹è±¡çº§çš„NeRFç›¸ç»“åˆï¼Œæé«˜äº†é‡å»ºæ•ˆç‡ï¼Œå¹¶å®ç°äº†è§„èŒƒåŒ–å¯¹è±¡å§¿æ€ä¼°è®¡ã€‚PRENOMé€šè¿‡åœ¨ä»å¼€æºå½¢çŠ¶æ•°æ®é›†ç”Ÿæˆçš„åˆæˆé‡å»ºä»»åŠ¡ä¸Šè¿›è¡Œå…ƒå­¦ä¹ æ¥æ·±å…¥äº†è§£å¯¹è±¡ã€‚ä¸ºäº†åº”å¯¹å¯¹è±¡ç±»åˆ«å˜åŒ–ï¼Œå®ƒé‡‡ç”¨äº†ä¸€ç§å¤šç›®æ ‡é—ä¼ ç®—æ³•æ¥é’ˆå¯¹æ¯ä¸ªç±»åˆ«ä¼˜åŒ–NeRFæ¶æ„ï¼Œå¹³è¡¡é‡å»ºè´¨é‡å’Œè®­ç»ƒæ—¶é—´ã€‚æ­¤å¤–ï¼ŒåŸºäºå…ˆéªŒçš„æ¦‚ç‡å°„çº¿é‡‡æ ·å°†é‡‡æ ·æŒ‡å‘é¢„æœŸçš„å¯¹è±¡åŒºåŸŸï¼ŒåŠ å¿«äº†æ”¶æ•›é€Ÿåº¦ï¼Œå¹¶åœ¨å—é™èµ„æºä¸‹æé«˜äº†é‡å»ºè´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPRENOMåœ¨é«˜è´¨é‡å»ºçš„åŒæ—¶ä¿æŒè®¡ç®—å¯è¡Œæ€§ã€‚å…·ä½“è€Œè¨€ï¼Œä¸åˆæˆæ•°æ®é›†ä¸Šæ— éœ€å…ˆéªŒçš„NeRFæ–¹æ³•ç›¸æ¯”ï¼ŒChamferè·ç¦»é™ä½äº†21%ã€‚æ­¤å¤–ï¼Œåœ¨å™ªå£°ç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šä¸ä½¿ç”¨å½¢çŠ¶å…ˆéªŒçš„å…¶ä»–æ–¹æ³•è¿›è¡Œçš„è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨æ‰€æœ‰é‡å»ºæŒ‡æ ‡ä¸Šçš„å¹³å‡æ”¹è¿›ä¸º13%ï¼Œå§¿æ€å’Œå¤§å°ä¼°è®¡çš„å‡†ç¡®æ€§ç›¸å½“ï¼ŒåŒæ—¶è®­ç»ƒæ—¶é—´å‡å°‘äº†5å€ã€‚ç›¸å…³ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/snt-arg/PRENOM%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/snt-arg/PRENOMè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.01582v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç±»åˆ«å…ˆéªŒçš„é«˜æ•ˆç¥ç»ç½‘ç»œå¯¹è±¡æ˜ å°„å™¨ï¼ˆPRENOMï¼‰ç»“åˆäº†ç±»åˆ«çº§åˆ«çš„å…ˆéªŒçŸ¥è¯†å’Œå¯¹è±¡çº§åˆ«çš„NeRFæŠ€æœ¯ï¼Œæé«˜äº†é‡å»ºæ•ˆç‡å¹¶å®ç°äº†å¯¹è±¡çš„æ ‡å‡†å§¿æ€ä¼°è®¡ã€‚PRENOMé€šè¿‡å…ƒå­¦ä¹ åœ¨å¼€æºé‡å»ºä»»åŠ¡æ•°æ®é›†ä¸Šäº†è§£å¯¹è±¡ï¼Œå¹¶åˆ©ç”¨å¤šç›®æ ‡é—ä¼ ç®—æ³•ä¼˜åŒ–NeRFæ¶æ„ä»¥å¹³è¡¡é‡å»ºè´¨é‡å’Œè®­ç»ƒæ—¶é—´ã€‚åŒæ—¶ï¼Œå€ŸåŠ©åŸºäºå…ˆéªŒçš„æ¦‚ç‡å°„çº¿é‡‡æ ·æŠ€æœ¯åŠ é€Ÿæ”¶æ•›ï¼Œåœ¨æœ‰é™çš„èµ„æºä¸‹æé«˜é‡å»ºè´¨é‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPRENOMåœ¨åˆæˆæ•°æ®é›†ä¸Šå®ç°äº†é«˜è´¨é‡çš„é‡å»ºï¼Œç›¸è¾ƒäºæ— å…ˆéªŒNeRFæ–¹æ³•é™ä½äº†21%çš„Chamferè·ç¦»ã€‚åœ¨å™ªå£°ç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä¸å…¶ä»–ä½¿ç”¨å½¢çŠ¶å…ˆéªŒçš„æ–¹æ³•ç›¸æ¯”ï¼ŒPRENOMå¹³å‡æé«˜äº†13%çš„é‡å»ºæŒ‡æ ‡å‡†ç¡®åº¦ï¼ŒåŒæ—¶å§¿æ€å’Œå¤§å°ä¼°è®¡ç²¾åº¦ç›¸å½“ï¼Œå¹¶ä¸”è®­ç»ƒæ—¶é—´ç¼©çŸ­äº†5å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PRENOMç»“åˆäº†ç±»åˆ«çº§åˆ«çš„å…ˆéªŒçŸ¥è¯†å’Œå¯¹è±¡çº§åˆ«çš„NeRFæŠ€æœ¯ï¼Œç”¨äº3Då¯¹è±¡æ˜ å°„å’Œå§¿æ€ä¼°è®¡ã€‚</li>
<li>PRENOMé€šè¿‡å…ƒå­¦ä¹ åœ¨åˆæˆé‡å»ºä»»åŠ¡æ•°æ®é›†ä¸Šäº†è§£å¯¹è±¡ã€‚</li>
<li>åˆ©ç”¨å¤šç›®æ ‡é—ä¼ ç®—æ³•é’ˆå¯¹æ¯ä¸ªç±»åˆ«ä¼˜åŒ–NeRFæ¶æ„ã€‚</li>
<li>åŸºäºå…ˆéªŒçš„æ¦‚ç‡å°„çº¿é‡‡æ ·æŠ€æœ¯ç”¨äºåŠ é€Ÿæ”¶æ•›å¹¶æ”¹å–„é‡å»ºè´¨é‡ã€‚</li>
<li>åœ¨åˆæˆæ•°æ®é›†ä¸Šï¼ŒPRENOMç›¸è¾ƒäºæ— å…ˆéªŒNeRFæ–¹æ³•é™ä½äº†Chamferè·ç¦»ã€‚</li>
<li>åœ¨ç°å®ä¸–ç•Œæ•°æ®é›†ä¸Šï¼ŒPRENOMåœ¨é‡å»ºæŒ‡æ ‡ã€å§¿æ€å’Œå¤§å°ä¼°è®¡ç²¾åº¦æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼ŒåŒæ—¶è®­ç»ƒæ—¶é—´æ›´çŸ­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.01582">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-561b3319e68f3ddf5fa7da142e7903eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48aa89303fa0694e928014127511fdb7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f161b720b57d485e0723f2a247cff76b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2e41285960c3cb7dd8eff1ef16df5ea4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-005dcdfd726fec6c9339210343b7ce95.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fc04fc999bd6beb2bab4a8c5bbaefbc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6a50548ab5ac8dd98228bac35498c8d.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/NeRF/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/NeRF/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/NeRF/">
                                    <span class="chip bg-color">NeRF</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-08d623e6c661aeacba7ab6308f09487f.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  UniLDiff Unlocking the Power of Diffusion Priors for All-in-One Image   Restoration
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c20ea647f542e878705d061131ccfb2a.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  Gaussian Variation Field Diffusion for High-fidelity Video-to-4D   Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33125.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
