<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  UniLDiff Unlocking the Power of Diffusion Priors for All-in-One Image   Restoration">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-08d623e6c661aeacba7ab6308f09487f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    71 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-02-æ›´æ–°"><a href="#2025-08-02-æ›´æ–°" class="headerlink" title="2025-08-02 æ›´æ–°"></a>2025-08-02 æ›´æ–°</h1><h2 id="UniLDiff-Unlocking-the-Power-of-Diffusion-Priors-for-All-in-One-Image-Restoration"><a href="#UniLDiff-Unlocking-the-Power-of-Diffusion-Priors-for-All-in-One-Image-Restoration" class="headerlink" title="UniLDiff: Unlocking the Power of Diffusion Priors for All-in-One Image   Restoration"></a>UniLDiff: Unlocking the Power of Diffusion Priors for All-in-One Image   Restoration</h2><p><strong>Authors:Zihan Cheng, Liangtai Zhou, Dian Chen, Ni Tang, Xiaotong Luo, Yanyun Qu</strong></p>
<p>All-in-One Image Restoration (AiOIR) has emerged as a promising yet challenging research direction. To address its core challenges, we propose a novel unified image restoration framework based on latent diffusion models (LDMs). Our approach structurally integrates low-quality visual priors into the diffusion process, unlocking the powerful generative capacity of diffusion models for diverse degradations. Specifically, we design a Degradation-Aware Feature Fusion (DAFF) module to enable adaptive handling of diverse degradation types. Furthermore, to mitigate detail loss caused by the high compression and iterative sampling of LDMs, we design a Detail-Aware Expert Module (DAEM) in the decoder to enhance texture and fine-structure recovery. Extensive experiments across multi-task and mixed degradation settings demonstrate that our method consistently achieves state-of-the-art performance, highlighting the practical potential of diffusion priors for unified image restoration. Our code will be released. </p>
<blockquote>
<p>å…¨èƒ½å›¾åƒä¿®å¤ï¼ˆAiOIRï¼‰å·²æˆä¸ºä¸€ä¸ªå‰æ™¯å¹¿é˜”ä½†å……æ»¡æŒ‘æˆ˜çš„ç ”ç©¶æ–¹å‘ã€‚ä¸ºäº†è§£å†³å…¶æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰çš„ç»Ÿä¸€å›¾åƒä¿®å¤æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†ä½è´¨é‡è§†è§‰å…ˆéªŒç»“æ„æ€§åœ°èå…¥æ‰©æ•£è¿‡ç¨‹ï¼Œè§£é”æ‰©æ•£æ¨¡å‹å¯¹å„ç§é€€åŒ–çš„å¼ºå¤§ç”Ÿæˆèƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé€€åŒ–æ„ŸçŸ¥ç‰¹å¾èåˆï¼ˆDAFFï¼‰æ¨¡å—ï¼Œä»¥å®ç°å¯¹å¤šç§é€€åŒ–ç±»å‹çš„è‡ªé€‚åº”å¤„ç†ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡è½»ç”±äºLDMçš„é«˜å‹ç¼©å’Œè¿­ä»£é‡‡æ ·é€ æˆçš„ç»†èŠ‚æŸå¤±ï¼Œæˆ‘ä»¬åœ¨è§£ç å™¨ä¸­è®¾è®¡äº†ä¸€ä¸ªç»†èŠ‚æ„ŸçŸ¥ä¸“å®¶æ¨¡å—ï¼ˆDAEMï¼‰ä»¥å¢å¼ºçº¹ç†å’Œç²¾ç»†ç»“æ„çš„æ¢å¤ã€‚è·¨å¤šä»»åŠ¡å’Œæ··åˆé€€åŒ–ç¯å¢ƒçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œçªæ˜¾äº†ç»Ÿä¸€å›¾åƒä¿®å¤ä¸­æ‰©æ•£å…ˆéªŒçš„å®é™…æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å°†äºˆä»¥å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23685v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç»Ÿä¸€å›¾åƒæ¢å¤æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†ä½è´¨é‡è§†è§‰å…ˆéªŒçŸ¥è¯†èå…¥åˆ°æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œä»¥åº”å¯¹å¤šæ ·é™è§£çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚è®¾è®¡Degradation-Aware Feature Fusionï¼ˆDAFFï¼‰æ¨¡å—ï¼Œå®ç°è‡ªé€‚åº”å¤„ç†å¤šç§é™è§£ç±»å‹ã€‚æ­¤å¤–ï¼Œåœ¨è§£ç å™¨ä¸­è®¾è®¡Detail-Aware Expert Moduleï¼ˆDAEMï¼‰ä»¥å‡å°‘æ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é«˜å‹ç¼©å’Œè¿­ä»£é‡‡æ ·å¼•èµ·çš„ç»†èŠ‚æŸå¤±ï¼Œå¢å¼ºçº¹ç†å’Œç²¾ç»†ç»“æ„çš„æ¢å¤ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä»»åŠ¡å’Œæ··åˆé™è§£è®¾ç½®ä¸­å§‹ç»ˆå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰çš„ç»Ÿä¸€å›¾åƒæ¢å¤æ¡†æ¶ã€‚</li>
<li>æ¡†æ¶èåˆäº†ä½è´¨é‡è§†è§‰å…ˆéªŒçŸ¥è¯†ä»¥åº”å¯¹å›¾åƒæ¢å¤çš„å¤šæ ·é™è§£æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>é‡‡ç”¨äº†Degradation-Aware Feature Fusionï¼ˆDAFFï¼‰æ¨¡å—ï¼Œå®ç°è‡ªé€‚åº”å¤„ç†å¤šç§é™è§£ç±»å‹ã€‚</li>
<li>è®¾è®¡äº†Detail-Aware Expert Moduleï¼ˆDAEMï¼‰ä»¥å‡å°‘ç»†èŠ‚æŸå¤±å¹¶å¢å¼ºçº¹ç†å’Œç²¾ç»†ç»“æ„çš„æ¢å¤ã€‚</li>
<li>æ–¹æ³•åœ¨å¤šä»»åŠ¡å’Œæ··åˆé™è§£è®¾ç½®ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰å®é™…åº”ç”¨çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0bd87546ad808a899042c03d4bc56e52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae4649b02c77d03ef31ad095027e1ee5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-08d623e6c661aeacba7ab6308f09487f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e38d1a400028769161008ff31069953e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-634fa2101363ca158dc3acdffba5b9b5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f583d0d2d40d571b18e0c4612193dd34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7eff37ec735ba8144fb72b50b419b75.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Adaptively-Distilled-ControlNet-Accelerated-Training-and-Superior-Sampling-for-Medical-Image-Synthesis"><a href="#Adaptively-Distilled-ControlNet-Accelerated-Training-and-Superior-Sampling-for-Medical-Image-Synthesis" class="headerlink" title="Adaptively Distilled ControlNet: Accelerated Training and Superior   Sampling for Medical Image Synthesis"></a>Adaptively Distilled ControlNet: Accelerated Training and Superior   Sampling for Medical Image Synthesis</h2><p><strong>Authors:Kunpeng Qiu, Zhiying Zhou, Yongxin Guo</strong></p>
<p>Medical image annotation is constrained by privacy concerns and labor-intensive labeling, significantly limiting the performance and generalization of segmentation models. While mask-controllable diffusion models excel in synthesis, they struggle with precise lesion-mask alignment. We propose \textbf{Adaptively Distilled ControlNet}, a task-agnostic framework that accelerates training and optimization through dual-model distillation. Specifically, during training, a teacher model, conditioned on mask-image pairs, regularizes a mask-only student model via predicted noise alignment in parameter space, further enhanced by adaptive regularization based on lesion-background ratios. During sampling, only the student model is used, enabling privacy-preserving medical image generation. Comprehensive evaluations on two distinct medical datasets demonstrate state-of-the-art performance: TransUNet improves mDice&#x2F;mIoU by 2.4%&#x2F;4.2% on KiTS19, while SANet achieves 2.6%&#x2F;3.5% gains on Polyps, highlighting its effectiveness and superiority. Code is available at GitHub. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒæ ‡æ³¨å—åˆ°éšç§æ‹…å¿§å’ŒåŠ³åŠ¨å¯†é›†å‹æ ‡æ³¨çš„é™åˆ¶ï¼Œè¿™æå¤§åœ°é™åˆ¶äº†åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚è™½ç„¶å¯æ§åˆ¶çš„æ©è†œæ‰©æ•£æ¨¡å‹åœ¨åˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç²¾ç¡®çš„ç—…ç¶æ©è†œå¯¹é½æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†<strong>é€‚åº”è’¸é¦ControlNet</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªä»»åŠ¡æ— å…³çš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡åŒæ¨¡å‹è’¸é¦åŠ é€Ÿè®­ç»ƒå’Œä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»¥æ•™å¸ˆæ¨¡å‹ï¼ˆä»¥æ©è†œå›¾åƒå¯¹ä¸ºæ¡ä»¶ï¼‰é€šè¿‡å‚æ•°ç©ºé—´çš„é¢„æµ‹å™ªå£°å¯¹é½æ¥è§„èŒƒä»…ä½¿ç”¨æ©è†œçš„å­¦ç”Ÿæ¨¡å‹ï¼Œå¹¶è¿›ä¸€æ­¥é€šè¿‡åŸºäºç—…ç¶èƒŒæ™¯æ¯”çš„è‡ªé€‚åº”æ­£åˆ™åŒ–æ¥å¢å¼ºã€‚åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œåªä½¿ç”¨å­¦ç”Ÿæ¨¡å‹ï¼Œå®ç°äº†ä¿æŠ¤éšç§çš„åŒ»å­¦å½±åƒç”Ÿæˆã€‚åœ¨ä¸¤ä¸ªä¸åŒçš„åŒ»å­¦æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œå…¶æ€§èƒ½è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼šTransUNetåœ¨KiTS19ä¸Šçš„mDice&#x2F;mIoUæé«˜äº†2.4%&#x2F;4.2%ï¼Œè€ŒSANetåœ¨Polypsä¸Šçš„å¢ç›Šè¾¾åˆ°2.6%&#x2F;3.5%ï¼Œçªæ˜¾äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚ä»£ç å¯åœ¨GitHubä¸Šè·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23652v1">PDF</a> Accepted by MICCAI2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºAdaptively Distilled ControlNetçš„ä»»åŠ¡æ— å…³æ¡†æ¶ï¼Œé€šè¿‡åŒæ¨¡å‹è’¸é¦åŠ é€Ÿè®­ç»ƒå’Œä¼˜åŒ–ã€‚è¯¥æ¡†æ¶è§£å†³äº†åŒ»å­¦å›¾åƒæ ‡æ³¨ä¸­éšç§å’Œæ ‡æ³¨å·¥ä½œé‡çš„é—®é¢˜ï¼Œå¹¶åœ¨ç²¾ç¡®ç—…å˜è’™ç‰ˆå¯¹é½æ–¹é¢å–å¾—è¿›å±•ã€‚æ¡†æ¶åŒ…æ‹¬è®­ç»ƒé˜¶æ®µä½¿ç”¨ä»¥æ•™å¸ˆæ¨¡å‹ä¸ºä¸»ã€åŸºäºè’™ç‰ˆå›¾åƒå¯¹çš„é¢„æµ‹å™ªå£°å¯¹é½è¿›è¡Œå‚æ•°ç©ºé—´çš„æ­£åˆ™åŒ–ï¼Œä»¥åŠåœ¨é‡‡æ ·é˜¶æ®µä»…ä½¿ç”¨å­¦ç”Ÿæ¨¡å‹è¿›è¡Œéšç§ä¿æŠ¤çš„åŒ»å­¦å›¾åƒç”Ÿæˆã€‚åœ¨ä¸¤ç§ä¸åŒåŒ»å­¦æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¯æ˜äº†å…¶å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæ ‡æ³¨å—é™äºéšç§å…³æ³¨å’ŒåŠ³åŠ¨å¯†é›†å‹æ ‡æ³¨ï¼Œå½±å“åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºAdaptively Distilled ControlNetæ¡†æ¶ï¼Œé€šè¿‡åŒæ¨¡å‹è’¸é¦åŠ é€Ÿè®­ç»ƒå’Œä¼˜åŒ–ã€‚</li>
<li>æ•™å¸ˆæ¨¡å‹åŸºäºè’™ç‰ˆå›¾åƒå¯¹è¿›è¡Œé¢„æµ‹å™ªå£°å¯¹é½çš„å‚æ•°ç©ºé—´æ­£åˆ™åŒ–ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬è‡ªé€‚åº”æ­£åˆ™åŒ–ï¼Œæ ¹æ®ç—…å˜èƒŒæ™¯æ¯”è¿›è¡Œè°ƒæ•´ã€‚</li>
<li>é‡‡æ ·é˜¶æ®µä»…ä½¿ç”¨å­¦ç”Ÿæ¨¡å‹ï¼Œå®ç°éšç§ä¿æŠ¤çš„åŒ»å­¦å›¾åƒç”Ÿæˆã€‚</li>
<li>åœ¨ä¸¤ä¸ªä¸åŒåŒ»å­¦æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶å“è¶Šæ€§èƒ½ï¼ŒTransUNetåœ¨KiTS19æ•°æ®é›†ä¸Šæ”¹è¿›äº†mDice&#x2F;mIoUæŒ‡æ ‡ï¼ŒSANetåœ¨Polypsæ•°æ®é›†ä¸Šä¹Ÿå–å¾—äº†æ˜¾è‘—æˆç»©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-27aec319c555cb84a1217756aabdbf5f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fca6d88b0b96d07154dbe597e3d0a766.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff68157b30c53504a18bac4945fe0a0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f75ee3bdc0bb53e7e3064e7bfa4a24a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-167f062263edb8b321cc1e920c59188a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c97563aae3e07801d61783fcd81aa12e.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DivControl-Knowledge-Diversion-for-Controllable-Image-Generation"><a href="#DivControl-Knowledge-Diversion-for-Controllable-Image-Generation" class="headerlink" title="DivControl: Knowledge Diversion for Controllable Image Generation"></a>DivControl: Knowledge Diversion for Controllable Image Generation</h2><p><strong>Authors:Yucheng Xie, Fu Feng, Ruixiao Shi, Jing Wang, Yong Rui, Xin Geng</strong></p>
<p>Diffusion models have advanced from text-to-image (T2I) to image-to-image (I2I) generation by incorporating structured inputs such as depth maps, enabling fine-grained spatial control. However, existing methods either train separate models for each condition or rely on unified architectures with entangled representations, resulting in poor generalization and high adaptation costs for novel conditions. To this end, we propose DivControl, a decomposable pretraining framework for unified controllable generation and efficient adaptation. DivControl factorizes ControlNet via SVD into basic components-pairs of singular vectors-which are disentangled into condition-agnostic learngenes and condition-specific tailors through knowledge diversion during multi-condition training. Knowledge diversion is implemented via a dynamic gate that performs soft routing over tailors based on the semantics of condition instructions, enabling zero-shot generalization and parameter-efficient adaptation to novel conditions. To further improve condition fidelity and training efficiency, we introduce a representation alignment loss that aligns condition embeddings with early diffusion features. Extensive experiments demonstrate that DivControl achieves state-of-the-art controllability with 36.4$\times$ less training cost, while simultaneously improving average performance on basic conditions. It also delivers strong zero-shot and few-shot performance on unseen conditions, demonstrating superior scalability, modularity, and transferability. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹å·²ç»ä»æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„ç”Ÿæˆå‘å±•åˆ°äº†å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰çš„ç”Ÿæˆï¼Œé€šè¿‡èå…¥æ·±åº¦æ˜ å°„ç­‰ç»“æ„åŒ–è¾“å…¥ï¼Œå®ç°äº†ç²¾ç»†çš„ç©ºé—´æ§åˆ¶ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•è¦ä¹ˆé’ˆå¯¹æ¯ç§æ¡ä»¶è®­ç»ƒå•ç‹¬çš„æ¨¡å‹ï¼Œè¦ä¹ˆä¾èµ–äºå…·æœ‰çº ç¼ è¡¨ç¤ºçš„ç»Ÿä¸€æ¶æ„ï¼Œå¯¼è‡´å¯¹æ–°æ¡ä»¶çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®å’Œè¾ƒé«˜çš„é€‚åº”æˆæœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†DivControlï¼Œè¿™æ˜¯ä¸€ä¸ªå¯åˆ†è§£çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºç»Ÿä¸€çš„å¯æ§ç”Ÿæˆå’Œé«˜æ•ˆé€‚åº”ã€‚DivControlé€šè¿‡SVDå°†ControlNetè¿›è¡Œå› å­åˆ†è§£ï¼Œå¾—åˆ°åŸºæœ¬ç»„ä»¶â€”â€”å¥‡å¼‚å‘é‡å¯¹ï¼Œç„¶åé€šè¿‡å¤šæ¡ä»¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„çŸ¥è¯†åˆ†æµï¼Œå°†å…¶åˆ†ç¦»ä¸ºä¸æ¡ä»¶æ— å…³çš„å­¦ä¹ åŸºå› å’Œä¸æ¡ä»¶ç‰¹å®šçš„è£ç¼ã€‚çŸ¥è¯†åˆ†æµæ˜¯é€šè¿‡ä¸€ä¸ªåŠ¨æ€é—¨å®ç°çš„ï¼Œè¯¥é—¨æ ¹æ®æ¡ä»¶æŒ‡ä»¤çš„è¯­ä¹‰å¯¹è£ç¼è¿›è¡Œè½¯è·¯ç”±é€‰æ‹©ï¼Œä»è€Œå®ç°é›¶æ ·æœ¬æ³›åŒ–å’Œå¯¹æ–°æ¡ä»¶çš„å‚æ•°é«˜æ•ˆé€‚åº”ã€‚ä¸ºäº†æé«˜æ¡ä»¶ä¿çœŸåº¦å’Œè®­ç»ƒæ•ˆç‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¡¨ç¤ºå¯¹é½æŸå¤±ï¼Œå°†æ¡ä»¶åµŒå…¥ä¸æ—©æœŸæ‰©æ•£ç‰¹å¾å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDivControlåœ¨å‡å°‘36.4å€è®­ç»ƒæˆæœ¬çš„åŒæ—¶ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„å¯æ§æ€§ï¼ŒåŒæ—¶æé«˜äº†åŸºæœ¬æ¡ä»¶ä¸‹çš„å¹³å‡æ€§èƒ½ã€‚å®ƒåœ¨æœªè§è¿‡çš„æ¡ä»¶ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ï¼Œè¯æ˜äº†å…¶å¯æ‰©å±•æ€§ã€æ¨¡å—åŒ–å’Œå¯è¿ç§»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23620v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Diffusionæ¨¡å‹ä»æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆå‘å±•åˆ°å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç”Ÿæˆçš„è¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥ç»“æ„åŒ–è¾“å…¥ï¼ˆå¦‚æ·±åº¦å›¾ï¼‰ï¼Œå®ç°äº†ç²¾ç»†çš„ç©ºé—´æ§åˆ¶ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨è®­ç»ƒæ¯ä¸ªæ¡ä»¶éœ€è¦å•ç‹¬æ¨¡å‹æˆ–ä¾èµ–ç»Ÿä¸€æ¶æ„ä½†è¡¨ç¤ºçº ç¼ çš„é—®é¢˜ï¼Œå¯¼è‡´å¯¹æ–°æ¡ä»¶çš„æ³›åŒ–èƒ½åŠ›è¾ƒå·®å’Œè¾ƒé«˜çš„é€‚åº”æˆæœ¬ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†DivControlï¼Œä¸€ä¸ªå¯åˆ†è§£çš„é¢„è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºç»Ÿä¸€å¯æ§ç”Ÿæˆå’Œé«˜æ•ˆé€‚åº”ã€‚DivControlé€šè¿‡SVDå°†ControlNetåˆ†è§£ä¸ºåŸºæœ¬ç»„ä»¶ï¼Œå³å¥‡å¼‚å‘é‡å¯¹ï¼Œå¹¶åœ¨å¤šæ¡ä»¶è®­ç»ƒæœŸé—´é€šè¿‡çŸ¥è¯†åˆ†æµå°†å…¶åˆ†ç¦»ä¸ºæ¡ä»¶æ— å…³çš„å­¦ä¹ åŸºå› å’Œæ¡ä»¶ç‰¹å®šçš„å®šåˆ¶è€…ã€‚çŸ¥è¯†åˆ†æµé€šè¿‡åŠ¨æ€é—¨å®ç°ï¼Œæ ¹æ®æ¡ä»¶æŒ‡ä»¤çš„è¯­ä¹‰å¯¹å®šåˆ¶è€…è¿›è¡Œè½¯è·¯ç”±é€‰æ‹©ï¼Œå®ç°é›¶æ ·æœ¬æ³›åŒ–å’Œå¯¹æ–°æ¡ä»¶çš„å‚æ•°é«˜æ•ˆé€‚åº”ã€‚ä¸ºæé«˜æ¡ä»¶ä¿çœŸåº¦å’Œè®­ç»ƒæ•ˆç‡ï¼Œè¿˜å¼•å…¥äº†è¡¨ç¤ºå¯¹é½æŸå¤±ï¼Œå°†æ¡ä»¶åµŒå…¥ä¸æ—©æœŸæ‰©æ•£ç‰¹å¾å¯¹é½ã€‚å®éªŒè¡¨æ˜ï¼ŒDivControlåœ¨é™ä½36.4å€è®­ç»ƒæˆæœ¬çš„åŒæ—¶ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„å¯æ§æ€§ï¼Œå¹¶æé«˜äº†åŸºæœ¬æ¡ä»¶ä¸‹çš„å¹³å‡æ€§èƒ½ï¼ŒåŒæ—¶åœ¨æœªè§è¿‡çš„æ¡ä»¶ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ï¼Œå±•ç°äº†å‡ºè‰²çš„å¯æ‰©å±•æ€§ã€æ¨¡å—æ€§å’Œå¯è½¬ç§»æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Diffusionæ¨¡å‹å·²å‘å±•åˆ°å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰ç”Ÿæˆï¼Œé€šè¿‡ç»“æ„åŒ–è¾“å…¥å®ç°ç²¾ç»†ç©ºé—´æ§åˆ¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨å¯¹æ–°æ¡ä»¶æ³›åŒ–èƒ½åŠ›å·®å’Œé€‚åº”æˆæœ¬é«˜çš„é—®é¢˜ã€‚</li>
<li>DivControlæ¡†æ¶é€šè¿‡åˆ†è§£ControlNetæé«˜å¯æ§ç”Ÿæˆå’Œé€‚åº”æ•ˆç‡ã€‚</li>
<li>DivControlå®ç°çŸ¥è¯†åˆ†æµï¼Œé€šè¿‡åŠ¨æ€é—¨é€‰æ‹©å®šåˆ¶è€…ï¼Œå®ç°é›¶æ ·æœ¬æ³›åŒ–å’Œå‚æ•°é«˜æ•ˆé€‚åº”ã€‚</li>
<li>å¼•å…¥è¡¨ç¤ºå¯¹é½æŸå¤±ä»¥æé«˜æ¡ä»¶ä¿çœŸåº¦å’Œè®­ç»ƒæ•ˆç‡ã€‚</li>
<li>DivControlå®ç°äº†å…ˆè¿›çš„å¯æ§æ€§ï¼Œå¹¶æé«˜äº†åŸºæœ¬æ¡ä»¶ä¸‹çš„å¹³å‡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23620">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-856b018d780e0f6fffdd810a998144e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62deb42a30faab0d997b4045393e24a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec0fcb208626ea3b1b013fbe1d6b78c7.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="MoGA-3D-Generative-Avatar-Prior-for-Monocular-Gaussian-Avatar-Reconstruction"><a href="#MoGA-3D-Generative-Avatar-Prior-for-Monocular-Gaussian-Avatar-Reconstruction" class="headerlink" title="MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar   Reconstruction"></a>MoGA: 3D Generative Avatar Prior for Monocular Gaussian Avatar   Reconstruction</h2><p><strong>Authors:Zijian Dong, Longteng Duan, Jie Song, Michael J. Black, Andreas Geiger</strong></p>
<p>We present MoGA, a novel method to reconstruct high-fidelity 3D Gaussian avatars from a single-view image. The main challenge lies in inferring unseen appearance and geometric details while ensuring 3D consistency and realism. Most previous methods rely on 2D diffusion models to synthesize unseen views; however, these generated views are sparse and inconsistent, resulting in unrealistic 3D artifacts and blurred appearance. To address these limitations, we leverage a generative avatar model, that can generate diverse 3D avatars by sampling deformed Gaussians from a learned prior distribution. Due to the limited amount of 3D training data such a 3D model alone cannot capture all image details of unseen identities. Consequently, we integrate it as a prior, ensuring 3D consistency by projecting input images into its latent space and enforcing additional 3D appearance and geometric constraints. Our novel approach formulates Gaussian avatar creation as a model inversion process by fitting the generative avatar to synthetic views from 2D diffusion models. The generative avatar provides a meaningful initialization for model fitting, enforces 3D regularization, and helps in refining pose estimation. Experiments show that our method surpasses state-of-the-art techniques and generalizes well to real-world scenarios. Our Gaussian avatars are also inherently animatable </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºMoGAçš„æ–°æ–¹æ³•ï¼Œå¯ä»¥ä»å•è§†å›¾å›¾åƒé‡å»ºé«˜ä¿çœŸ3Dé«˜æ–¯åŒ–èº«ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºæ¨æ–­æœªè§çš„å¤–è§‚å’Œå‡ ä½•ç»†èŠ‚ï¼ŒåŒæ—¶ç¡®ä¿3Dä¸€è‡´æ€§å’Œé€¼çœŸåº¦ã€‚å¤§å¤šæ•°ä¹‹å‰çš„æ–¹æ³•ä¾èµ–äº2Dæ‰©æ•£æ¨¡å‹æ¥åˆæˆæœªè§çš„è§†å›¾ï¼›ç„¶è€Œï¼Œè¿™äº›ç”Ÿæˆçš„è§†å›¾ç¨€ç–ä¸”ä¸ä¸€è‡´ï¼Œå¯¼è‡´3Dä¼ªå½±å’Œæ¨¡ç³Šçš„å¤–è§‚ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨ç”ŸæˆåŒ–èº«æ¨¡å‹ï¼Œé€šè¿‡ä»å­¦ä¹ çš„å…ˆéªŒåˆ†å¸ƒä¸­é‡‡æ ·å˜å½¢é«˜æ–¯å‡½æ•°æ¥ç”Ÿæˆå„ç§3DåŒ–èº«ã€‚ç”±äº3Dè®­ç»ƒæ•°æ®é‡æœ‰é™ï¼Œå•ä¸€çš„3Dæ¨¡å‹æ— æ³•æ•è·æœªè§èº«ä»½çš„æ‰€æœ‰å›¾åƒç»†èŠ‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å…¶æ•´åˆä¸ºä¼˜å…ˆçº§ï¼Œé€šè¿‡å°†è¾“å…¥å›¾åƒæŠ•å½±åˆ°å…¶æ½œåœ¨ç©ºé—´å¹¶æ–½åŠ é¢å¤–çš„3Då¤–è§‚å’Œå‡ ä½•çº¦æŸæ¥ç¡®ä¿3Dä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–°æ–¹æ³•å°†é«˜æ–¯åŒ–èº«çš„åˆ›å»ºåˆ¶å®šä¸ºä¸€ä¸ªæ¨¡å‹åè½¬è¿‡ç¨‹ï¼Œé€šè¿‡å°†ç”ŸæˆåŒ–èº«æ‹Ÿåˆåˆ°æ¥è‡ª2Dæ‰©æ•£æ¨¡å‹çš„åˆæˆè§†å›¾ã€‚ç”ŸæˆåŒ–èº«ä¸ºæ¨¡å‹æ‹Ÿåˆæä¾›äº†æœ‰æ„ä¹‰çš„åˆå§‹åŒ–ï¼Œå¼ºåˆ¶æ‰§è¡Œ3Dæ­£åˆ™åŒ–ï¼Œå¹¶æœ‰åŠ©äºæ”¹è¿›å§¿æ€ä¼°è®¡ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¶…è¶Šäº†æœ€æ–°æŠ€æœ¯å¹¶åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­å…·æœ‰è‰¯å¥½çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬çš„é«˜æ–¯åŒ–èº«ä¹Ÿå…·æœ‰å†…åœ¨çš„åŠ¨ç”»æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23597v1">PDF</a> ICCV 2025 (Highlight), Project Page: <a target="_blank" rel="noopener" href="https://zj-dong.github.io/MoGA/">https://zj-dong.github.io/MoGA/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MoGAæ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§ä»å•è§†è§’å›¾åƒé‡å»ºé«˜ä¿çœŸ3Dé«˜æ–¯å¤´åƒçš„æ–°æŠ€æœ¯ã€‚å®ƒä¸»è¦è§£å†³äº†åœ¨æ¨æ–­æœªè§çš„å¤–è§‚å’Œå‡ ä½•ç»†èŠ‚æ—¶ï¼Œå¦‚ä½•ç¡®ä¿3Dä¸€è‡´æ€§å’ŒçœŸå®æ€§çš„é—®é¢˜ã€‚ä¸å¤§å¤šæ•°ä¾èµ–2Dæ‰©æ•£æ¨¡å‹åˆæˆæœªè§è§†å›¾çš„æ–¹æ³•ä¸åŒï¼ŒMoGAé‡‡ç”¨ç”Ÿæˆå¼å¤´åƒæ¨¡å‹ï¼Œé€šè¿‡ä»å­¦ä¹ åˆ°çš„å…ˆéªŒåˆ†å¸ƒä¸­é‡‡æ ·å˜å½¢é«˜æ–¯æ¥ç”Ÿæˆå¤šæ ·åŒ–çš„3Då¤´åƒã€‚ç»“åˆ3Dæ¨¡å‹å’Œè¾“å…¥å›¾åƒçš„æŠ•å½±ï¼Œç¡®ä¿äº†3Dä¸€è‡´æ€§ï¼Œå¹¶å®æ–½äº†é¢å¤–çš„3Då¤–è§‚å’Œå‡ ä½•çº¦æŸã€‚MoGAå°†é«˜æ–¯å¤´åƒåˆ›å»ºåˆ¶å®šä¸ºæ¨¡å‹åæ¼”è¿‡ç¨‹ï¼Œé€šè¿‡å°†ç”Ÿæˆå¼å¤´åƒé€‚é…äºæ¥è‡ª2Dæ‰©æ•£æ¨¡å‹çš„åˆæˆè§†å›¾æ¥å®ç°ã€‚æ­¤æ–¹æ³•è¶…è¶Šäº†å…ˆè¿›æŠ€æœ¯ï¼Œå¹¶å¾ˆå¥½åœ°æ¨å¹¿è‡³çœŸå®ä¸–ç•Œåœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MoGAæ˜¯ä¸€ç§ä»å•è§†è§’å›¾åƒé‡å»ºé«˜ä¿çœŸ3Dé«˜æ–¯å¤´åƒçš„æ–°æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸»è¦è§£å†³åœ¨æ¨æ–­æœªè§çš„å¤–è§‚å’Œå‡ ä½•ç»†èŠ‚æ—¶çš„3Dä¸€è‡´æ€§å’ŒçœŸå®æ€§é—®é¢˜ã€‚</li>
<li>ä¸ä¾èµ–2Dæ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ä¸åŒï¼ŒMoGAé‡‡ç”¨ç”Ÿæˆå¼å¤´åƒæ¨¡å‹æ¥ç”Ÿæˆå¤šæ ·åŒ–çš„3Då¤´åƒã€‚</li>
<li>MoGAç»“åˆ3Dæ¨¡å‹å’Œè¾“å…¥å›¾åƒæŠ•å½±ï¼Œç¡®ä¿3Dä¸€è‡´æ€§ï¼Œå¹¶å®æ–½é¢å¤–çš„3Då¤–è§‚å’Œå‡ ä½•çº¦æŸã€‚</li>
<li>MoGAå°†é«˜æ–¯å¤´åƒåˆ›å»ºåˆ¶å®šä¸ºä¸€ä¸ªæ¨¡å‹åæ¼”è¿‡ç¨‹ï¼Œé€‚é…ç”Ÿæˆå¼å¤´åƒè‡³åˆæˆè§†å›¾ã€‚</li>
<li>MoGAæ–¹æ³•è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­å…·æœ‰è‰¯å¥½çš„æ¨å¹¿æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-36299fa1d2de31244961425d9c2f9e03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-92c4cfb63cc8def479f57a52e048adf5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4030795a97d1504b3de20f5c85aeca83.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Out-of-Distribution-Detection-in-Medical-Imaging-via-Diffusion-Trajectories"><a href="#Out-of-Distribution-Detection-in-Medical-Imaging-via-Diffusion-Trajectories" class="headerlink" title="Out-of-Distribution Detection in Medical Imaging via Diffusion   Trajectories"></a>Out-of-Distribution Detection in Medical Imaging via Diffusion   Trajectories</h2><p><strong>Authors:Lemar Abdi, Francisco Caetano, Amaan Valiuddin, Christiaan Viviers, Hamdi Joudeh, Fons van der Sommen</strong></p>
<p>In medical imaging, unsupervised out-of-distribution (OOD) detection offers an attractive approach for identifying pathological cases with extremely low incidence rates. In contrast to supervised methods, OOD-based approaches function without labels and are inherently robust to data imbalances. Current generative approaches often rely on likelihood estimation or reconstruction error, but these methods can be computationally expensive, unreliable, and require retraining if the inlier data changes. These limitations hinder their ability to distinguish nominal from anomalous inputs efficiently, consistently, and robustly. We propose a reconstruction-free OOD detection method that leverages the forward diffusion trajectories of a Stein score-based denoising diffusion model (SBDDM). By capturing trajectory curvature via the estimated Stein score, our approach enables accurate anomaly scoring with only five diffusion steps. A single SBDDM pre-trained on a large, semantically aligned medical dataset generalizes effectively across multiple Near-OOD and Far-OOD benchmarks, achieving state-of-the-art performance while drastically reducing computational cost during inference. Compared to existing methods, SBDDM achieves a relative improvement of up to 10.43% and 18.10% for Near-OOD and Far-OOD detection, making it a practical building block for real-time, reliable computer-aided diagnosis. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒä¸­ï¼Œæ— ç›‘ç£çš„ç¦»ç¾¤åˆ†å¸ƒï¼ˆOODï¼‰æ£€æµ‹æä¾›äº†ä¸€ç§æå…·å¸å¼•åŠ›çš„æ–¹æ³•æ¥è¯†åˆ«å‘ç—…ç‡æä½çš„ç—…ç†ç—…ä¾‹ã€‚ä¸ç›‘ç£æ–¹æ³•ç›¸æ¯”ï¼ŒåŸºäºOODçš„æ–¹æ³•æ— éœ€æ ‡ç­¾å°±èƒ½è¿è¡Œï¼Œå¹¶ä¸”å¤©ç„¶åœ°èƒ½å¤Ÿåº”å¯¹æ•°æ®ä¸å¹³è¡¡çš„é—®é¢˜ã€‚å½“å‰çš„ç”Ÿæˆæ–¹æ³•é€šå¸¸ä¾èµ–äºå¯èƒ½æ€§ä¼°è®¡æˆ–é‡å»ºè¯¯å·®ï¼Œä½†è¿™äº›æ–¹æ³•è®¡ç®—æˆæœ¬é«˜æ˜‚ã€å¯é æ€§ä¸è¶³ï¼Œå¦‚æœå†…éƒ¨æ•°æ®å‘ç”Ÿå˜åŒ–ï¼Œè¿˜éœ€è¦é‡æ–°è®­ç»ƒã€‚è¿™äº›å±€é™æ€§é˜»ç¢äº†å®ƒä»¬åœ¨é«˜æ•ˆã€æŒç»­å’Œç¨³å¥åœ°åŒºåˆ†æ­£å¸¸è¾“å…¥å’Œå¼‚å¸¸è¾“å…¥æ–¹é¢çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€é‡å»ºçš„OODæ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŸºäºSteinå¾—åˆ†çš„é™å™ªæ‰©æ•£æ¨¡å‹çš„å‘å‰æ‰©æ•£è½¨è¿¹ã€‚é€šè¿‡ä¼°è®¡Steinå¾—åˆ†æ¥æ•æ‰è½¨è¿¹æ›²ç‡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨äº”ä¸ªæ‰©æ•£æ­¥éª¤å³å¯å®ç°ç²¾ç¡®å¼‚å¸¸è¯„åˆ†ã€‚å•ä¸ªåŸºäºSteinå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹ï¼ˆSBDDMï¼‰é¢„è®­ç»ƒåœ¨å¤§å‹ã€è¯­ä¹‰å¯¹é½çš„åŒ»ç–—æ•°æ®é›†ä¸Šï¼Œèƒ½å¤Ÿåœ¨å¤šä¸ªè¿‘OODå’Œè¿œOODåŸºå‡†æµ‹è¯•ä¸Šå®ç°æœ‰æ•ˆæ³›åŒ–ï¼ŒåŒæ—¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¤§å¹…é™ä½è®¡ç®—æˆæœ¬ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSBDDMåœ¨è¿‘OODå’Œè¿œOODæ£€æµ‹æ–¹é¢çš„ç›¸å¯¹æ”¹è¿›ç‡åˆ†åˆ«é«˜è¾¾10.43%å’Œ18.10%ï¼Œä½¿å…¶æˆä¸ºå®æ—¶å¯é è®¡ç®—æœºè¾…åŠ©è¯Šæ–­çš„å®ç”¨ç»„æˆéƒ¨åˆ†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23411v1">PDF</a> Accepted at Uncertainty for Safe Utilization of Machine Learning in   Medical Imaging, MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºSteinå¾—åˆ†æ‰©æ•£æ¨¡å‹çš„éç›‘ç£åˆ†å¸ƒå¤–æ£€æµ‹ï¼ˆOODï¼‰ä¸ºåŒ»å­¦å›¾åƒä¸­ä½é¢‘ç—…ä¾‹æ£€æµ‹æä¾›äº†æœ‰æ•ˆçš„æ‰‹æ®µã€‚è¯¥æ–¹æ¡ˆé€šè¿‡æ•è·æ‰©æ•£è½¨è¿¹çš„æ›²ç‡æ¥å®ç°ç²¾ç¡®å¼‚å¸¸è¯„åˆ†ï¼Œåªéœ€äº”æ­¥æ‰©æ•£å³å¯å®Œæˆï¼Œç›¸æ¯”ä¾èµ–é‡æ„è¯¯å·®çš„ç”Ÿæˆæ¨¡å‹ï¼Œæ›´ç»æµã€å¯é ï¼Œé€‚åº”æ€§å¼ºã€‚è¯¥æ–¹æ³•ä»…éœ€åœ¨ä¸€å¤§å¼ è¯­ä¹‰å¯¹é½çš„åŒ»ç–—æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªSBDDMæ¨¡å‹å³å¯åº”å¯¹å¤šä¸ªè¿‘OODå’Œè¿œOODæ ‡å‡†æ£€æµ‹ï¼Œå–å¾—äº†å‰æ‰€æœªæœ‰çš„æ•ˆæœï¼Œå¤§å¤§å‡å°‘äº†æ¨ç†é˜¶æ®µçš„è®¡ç®—æˆæœ¬ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSBDDMåœ¨è¿‘OODå’Œè¿œOODæ£€æµ‹æ–¹é¢çš„ç›¸å¯¹æ”¹è¿›åˆ†åˆ«è¾¾åˆ°äº†é«˜è¾¾10.43%å’Œé«˜è¾¾18.10%ï¼Œæˆä¸ºäº†å®é™…ç”¨äºå¯é çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­çš„å…³é”®æ„å»ºå—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ— ç›‘ç£åˆ†å¸ƒå¤–æ£€æµ‹ï¼ˆOODï¼‰æ˜¯è¯†åˆ«ä½é¢‘ç—…ä¾‹çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
<li>åŸºäºSteinå¾—åˆ†æ‰©æ•£æ¨¡å‹çš„æ£€æµ‹æ–¹æ³•è®¡ç®—æˆæœ¬ä½ä¸”å¯é æ€§é«˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c0452124bcbd9dca9215499bc41d539.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-73ca0c27df4df270c134f4dbf2508b2b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-74ac3eb8886ed21642ace5edcd84a046.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UniEmo-Unifying-Emotional-Understanding-and-Generation-with-Learnable-Expert-Queries"><a href="#UniEmo-Unifying-Emotional-Understanding-and-Generation-with-Learnable-Expert-Queries" class="headerlink" title="UniEmo: Unifying Emotional Understanding and Generation with Learnable   Expert Queries"></a>UniEmo: Unifying Emotional Understanding and Generation with Learnable   Expert Queries</h2><p><strong>Authors:Yijie Zhu, Lingsen Zhang, Zitong Yu, Rui Shao, Tao Tan, Liqiang Nie</strong></p>
<p>Emotional understanding and generation are often treated as separate tasks, yet they are inherently complementary and can mutually enhance each other. In this paper, we propose the UniEmo, a unified framework that seamlessly integrates these two tasks. The key challenge lies in the abstract nature of emotions, necessitating the extraction of visual representations beneficial for both tasks. To address this, we propose a hierarchical emotional understanding chain with learnable expert queries that progressively extracts multi-scale emotional features, thereby serving as a foundational step for unification. Simultaneously, we fuse these expert queries and emotional representations to guide the diffusion model in generating emotion-evoking images. To enhance the diversity and fidelity of the generated emotional images, we further introduce the emotional correlation coefficient and emotional condition loss into the fusion process. This step facilitates fusion and alignment for emotional generation guided by the understanding. In turn, we demonstrate that joint training allows the generation component to provide implicit feedback to the understanding part. Furthermore, we propose a novel data filtering algorithm to select high-quality and diverse emotional images generated by the well-trained model, which explicitly feedback into the understanding part. Together, these generation-driven dual feedback processes enhance the modelâ€™s understanding capacity. Extensive experiments show that UniEmo significantly outperforms state-of-the-art methods in both emotional understanding and generation tasks. The code for the proposed method is available at <a target="_blank" rel="noopener" href="https://github.com/JiuTian-VL/UniEmo">https://github.com/JiuTian-VL/UniEmo</a>. </p>
<blockquote>
<p>æƒ…æ„Ÿç†è§£å’Œç”Ÿæˆé€šå¸¸è¢«è§†ä¸ºä¸¤ä¸ªå•ç‹¬çš„ä»»åŠ¡ï¼Œä½†å®ƒä»¬æœ¬è´¨ä¸Šæ˜¯äº’è¡¥çš„ï¼Œå¯ä»¥ç›¸äº’å¢å¼ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†UniEmoï¼Œè¿™æ˜¯ä¸€ä¸ªæ— ç¼é›†æˆè¿™ä¸¤ä¸ªä»»åŠ¡çš„ç»Ÿä¸€æ¡†æ¶ã€‚å…³é”®æŒ‘æˆ˜åœ¨äºæƒ…æ„Ÿçš„æŠ½è±¡æ€§è´¨ï¼Œéœ€è¦æå–æœ‰ç›Šäºè¿™ä¸¤ä¸ªä»»åŠ¡çš„è§†è§‰è¡¨ç¤ºã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ†çº§æƒ…æ„Ÿç†è§£é“¾ï¼Œé€šè¿‡å¯å­¦ä¹ çš„ä¸“å®¶æŸ¥è¯¢é€æ­¥æå–å¤šå°ºåº¦æƒ…æ„Ÿç‰¹å¾ï¼Œä»è€Œä¸ºç»Ÿä¸€åŒ–å¥ å®šåŸºç¡€ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å°†è¿™äº›ä¸“å®¶æŸ¥è¯¢å’Œæƒ…æ„Ÿè¡¨ç¤ºèåˆï¼Œä»¥æŒ‡å¯¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¼•å‘æƒ…æ„Ÿå›¾åƒã€‚ä¸ºæé«˜ç”Ÿæˆæƒ…æ„Ÿå›¾åƒçš„å¤šæ ·æ€§å’Œä¿çœŸåº¦ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å°†æƒ…æ„Ÿç›¸å…³ç³»æ•°å’Œæƒ…æ„Ÿæ¡ä»¶æŸå¤±å¼•å…¥åˆ°èåˆè¿‡ç¨‹ä¸­ã€‚è¿™ä¸€æ­¥æœ‰åŠ©äºèåˆå’Œå¯¹é½æƒ…æ„Ÿç”Ÿæˆä¸ç†è§£ä¹‹é—´çš„æŒ‡å¯¼ã€‚åè¿‡æ¥ï¼Œæˆ‘ä»¬è¯æ˜äº†è”åˆè®­ç»ƒå¯ä»¥è®©ç”Ÿæˆç»„ä»¶ä¸ºç†è§£éƒ¨åˆ†æä¾›éšæ€§åé¦ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®è¿‡æ»¤ç®—æ³•ï¼Œç”¨äºé€‰æ‹©é«˜è´¨é‡å’Œå¤šæ ·åŒ–çš„æƒ…æ„Ÿå›¾åƒï¼Œè¿™äº›å›¾åƒç”±è®­ç»ƒè‰¯å¥½çš„æ¨¡å‹ç”Ÿæˆï¼Œå¹¶æ˜ç¡®åé¦ˆåˆ°ç†è§£éƒ¨åˆ†ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç”Ÿæˆé©±åŠ¨çš„åŒé‡åé¦ˆè¿‡ç¨‹æé«˜äº†æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒUniEmoåœ¨æƒ…æ„Ÿç†è§£å’Œç”Ÿæˆä»»åŠ¡ä¸Šéƒ½æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æ‰€æå‡ºæ–¹æ³•çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JiuTian-VL/UniEmo">https://github.com/JiuTian-VL/UniEmo</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23372v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºUniEmoçš„ç»Ÿä¸€æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†æƒ…æ„Ÿç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚å…¶æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºæƒ…æ„Ÿçš„æŠ½è±¡æ€§è´¨ï¼Œéœ€è¦æå–å¯¹è¿™ä¸¤ä¸ªä»»åŠ¡éƒ½æœ‰ç›Šçš„è§†è§‰è¡¨ç¤ºã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†å±‚æ¬¡åŒ–çš„æƒ…æ„Ÿç†è§£é“¾ï¼Œåˆ©ç”¨å¯å­¦ä¹ çš„ä¸“å®¶æŸ¥è¯¢é€æ­¥æå–å¤šå°ºåº¦æƒ…æ„Ÿç‰¹å¾ï¼Œå¹¶ä¸ºèåˆè¿™äº›ç‰¹å¾æä¾›äº†åŸºç¡€ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¼•å…¥æƒ…æ„Ÿå…³è”ç³»æ•°å’Œæƒ…æ„Ÿæ¡ä»¶æŸå¤±ï¼Œä¼˜åŒ–äº†æƒ…æ„Ÿç”Ÿæˆå›¾åƒçš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚è”åˆè®­ç»ƒä½¿å¾—ç”Ÿæˆç»„ä»¶ä¸ºç†è§£éƒ¨åˆ†æä¾›éšæ€§åé¦ˆï¼ŒåŒæ—¶æå‡ºçš„æ•°æ®è¿‡æ»¤ç®—æ³•èƒ½å¤Ÿé€‰æ‹©é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æƒ…æ„Ÿå›¾åƒï¼Œä¸ºç†è§£éƒ¨åˆ†æä¾›æ˜ç¡®åé¦ˆã€‚æ•´ä½“ä¸Šï¼Œè¿™äº›ç”Ÿæˆé©±åŠ¨çš„åŒé‡åé¦ˆè¿‡ç¨‹æé«˜äº†æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniEmoæ¡†æ¶èåˆäº†æƒ…æ„Ÿç†è§£å’Œç”Ÿæˆä»»åŠ¡ï¼Œè®¤ä¸ºä¸¤è€…æ˜¯äº’è¡¥çš„ï¼Œå¯ä»¥ç›¸äº’å¢å¼ºã€‚</li>
<li>å±‚æ¬¡åŒ–çš„æƒ…æ„Ÿç†è§£é“¾å’Œå¯å­¦ä¹ çš„ä¸“å®¶æŸ¥è¯¢ç”¨äºæå–å¤šå°ºåº¦æƒ…æ„Ÿç‰¹å¾ã€‚</li>
<li>é€šè¿‡å¼•å…¥æƒ…æ„Ÿå…³è”ç³»æ•°å’Œæƒ…æ„Ÿæ¡ä»¶æŸå¤±ï¼Œä¼˜åŒ–äº†æƒ…æ„Ÿç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚</li>
<li>è”åˆè®­ç»ƒä½¿å¾—ç”Ÿæˆç»„ä»¶ä¸ºç†è§£éƒ¨åˆ†æä¾›éšæ€§åé¦ˆã€‚</li>
<li>æ•°æ®è¿‡æ»¤ç®—æ³•èƒ½å¤Ÿé€‰æ‹©é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æƒ…æ„Ÿå›¾åƒï¼Œä¸ºç†è§£éƒ¨åˆ†æä¾›æ˜ç¡®åé¦ˆã€‚</li>
<li>åŒé‡åé¦ˆè¿‡ç¨‹æé«˜äº†æ¨¡å‹å¯¹æƒ…æ„Ÿçš„ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23372">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7245a4880e34ba3279951b4d425b847d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55f85a75f556318c676041e25d25b149.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cfef4318514ac11be60f6c8cb6885884.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-12e2376b3dd3e5daaa5723a49d5c6527.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29ce831152a6500079c12cada22ae9a9.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="IN45023-Neural-Network-Design-Patterns-in-Computer-Vision-Seminar-Report-Summer-2025"><a href="#IN45023-Neural-Network-Design-Patterns-in-Computer-Vision-Seminar-Report-Summer-2025" class="headerlink" title="IN45023 Neural Network Design Patterns in Computer Vision Seminar   Report, Summer 2025"></a>IN45023 Neural Network Design Patterns in Computer Vision Seminar   Report, Summer 2025</h2><p><strong>Authors:Radu-Andrei Bourceanu, Neil De La Fuente, Jan Grimm, Andrei Jardan, Andriy Manucharyan, Cornelius Weiss, Roman Pflugfelder</strong></p>
<p>This report analyzes the evolution of key design patterns in computer vision by examining six influential papers. The analy- sis begins with foundational architectures for image recognition. We review ResNet, which introduced residual connections to overcome the vanishing gradient problem and enable effective training of significantly deeper convolutional networks. Subsequently, we examine the Vision Transformer (ViT), which established a new paradigm by applying the Transformer ar- chitecture to sequences of image patches, demonstrating the efficacy of attention-based models for large-scale image recogni- tion. Building on these visual representation backbones, we investigate generative models. Generative Adversarial Networks (GANs) are analyzed for their novel adversarial training process, which challenges a generator against a discriminator to learn complex data distributions. Then, Latent Diffusion Models (LDMs) are covered, which improve upon prior generative methods by performing a sequential denoising process in a perceptually compressed latent space. LDMs achieve high-fidelity synthesis with greater computational efficiency, representing the current state-of-the-art for image generation. Finally, we explore self-supervised learning techniques that reduce dependency on labeled data. DINO is a self-distillation framework in which a student network learns to match the output of a momentum-updated teacher, yielding features with strong k-NN classification performance. We conclude with Masked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design to reconstruct heavily masked inputs, providing a highly scalable and effective method for pre-training large-scale vision models. </p>
<blockquote>
<p>æœ¬æŠ¥å‘Šé€šè¿‡åˆ†æå…­ç¯‡æœ‰å½±å“åŠ›çš„è®ºæ–‡ï¼Œæ¢è®¨äº†è®¡ç®—æœºè§†è§‰ä¸­å…³é”®è®¾è®¡æ¨¡å¼çš„æ¼”å˜ã€‚åˆ†æä»å›¾åƒè¯†åˆ«çš„åŸºæœ¬æ¶æ„å¼€å§‹ã€‚æˆ‘ä»¬å›é¡¾äº†ResNetï¼Œå®ƒå¼•å…¥äº†æ®‹å·®è¿æ¥ï¼Œå…‹æœäº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå®ç°äº†å¯¹æ·±åº¦å·ç§¯ç¥ç»ç½‘è·¯çš„æœ‰æ•ˆè®­ç»ƒã€‚ä¹‹åï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒè¡¥ä¸åºåˆ—çš„Vision Transformerï¼ˆViTï¼‰ï¼Œè¿™å¼€åˆ›äº†æ–°çš„æ¨¡å¼ï¼Œå¹¶è¯æ˜äº†æ³¨æ„åŠ›æ¨¡å‹åœ¨å¤§è§„æ¨¡å›¾åƒè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚åŸºäºè¿™äº›è§†è§‰è¡¨å¾éª¨å¹²ç½‘ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”Ÿæˆæ¨¡å‹ã€‚åˆ†æäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ–°é¢–å¯¹æŠ—è®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹é€šè¿‡ç”Ÿæˆå™¨ä¸é‰´åˆ«å™¨çš„å¯¹æŠ—æ¥å­¦ä¹ å¤æ‚çš„æ•°æ®åˆ†å¸ƒã€‚ç„¶åï¼Œä»‹ç»äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ï¼Œé€šè¿‡æ„ŸçŸ¥å‹ç¼©æ½œåœ¨ç©ºé—´ä¸­çš„é¡ºåºå»å™ªè¿‡ç¨‹æ”¹è¿›äº†å…ˆå‰çš„ç”Ÿæˆæ–¹æ³•ã€‚LDMså®ç°äº†é«˜ä¿çœŸåˆæˆï¼Œå…·æœ‰æ›´é«˜çš„è®¡ç®—æ•ˆç‡ï¼Œä»£è¡¨äº†å½“å‰å›¾åƒç”Ÿæˆçš„æœ€æ–°æŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢ç´¢äº†å‡å°‘å¯¹æ ‡å®šæ•°æ®ä¾èµ–æ€§çš„è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ã€‚DINOæ˜¯ä¸€ç§è‡ªè’¸é¦æ¡†æ¶ï¼Œå­¦ç”Ÿç½‘ç»œå­¦ä¹ åŒ¹é…åŠ¨é‡æ›´æ–°åçš„æ•™å¸ˆè¾“å‡ºï¼Œäº§ç”Ÿå…·æœ‰å¼ºå¤§k-NNåˆ†ç±»æ€§èƒ½çš„ç‰¹å¾ã€‚æœ€åä»¥ä½¿ç”¨ä¸å¯¹ç§°ç¼–ç å™¨-è§£ç å™¨è®¾è®¡é‡å»ºé«˜åº¦é®æŒ¡è¾“å…¥çš„Masked Autoencodersï¼ˆMAEï¼‰ä¸ºä¾‹ï¼Œæä¾›äº†ä¸€ç§é«˜åº¦å¯æ‰©å±•å’Œæœ‰æ•ˆçš„é¢„è®­ç»ƒå¤§è§„æ¨¡è§†è§‰æ¨¡å‹çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23357v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡åˆ†æäº†è®¡ç®—æœºè§†è§‰ä¸­å…³é”®è®¾è®¡æ¨¡å¼çš„æ¼”å˜ï¼Œé€šè¿‡è€ƒå¯Ÿå…­ç¯‡æœ‰å½±å“åŠ›çš„è®ºæ–‡è¿›è¡Œäº†è¯¦ç»†çš„ç ”ç©¶ã€‚æ–‡ç« ä»å›¾åƒè¯†åˆ«çš„åŸºæœ¬æ¶æ„å¼€å§‹ï¼Œä»‹ç»äº†ResNetã€Vision Transformerç­‰å…³é”®æŠ€æœ¯ã€‚éšåï¼Œæ¢è®¨äº†ç”Ÿæˆæ¨¡å‹ï¼ŒåŒ…æ‹¬å¯¹æŠ—æ€§è®­ç»ƒè¿‡ç¨‹çš„Generative Adversarial Networks (GANs)å’Œåœ¨æ„ŸçŸ¥å‹ç¼©æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œé¡ºåºå»å™ªè¿‡ç¨‹çš„Latent Diffusion Models (LDMs)ã€‚æœ€åï¼Œä»‹ç»äº†è‡ªæˆ‘ç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼ŒåŒ…æ‹¬DINOå’ŒMasked Autoencodersç­‰ï¼Œè¿™äº›æŠ€æœ¯å‡å°‘äº†å¯¹äºæ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè§†è§‰ä¸­çš„è®¾è®¡æ¨¡å¼ä¸æ–­æ¼”å˜ï¼Œä»å›¾åƒè¯†åˆ«çš„åŸºæœ¬æ¶æ„åˆ°ç”Ÿæˆæ¨¡å‹å’Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ æŠ€æœ¯ã€‚</li>
<li>ResNeté€šè¿‡å¼•å…¥æ®‹å·®è¿æ¥å…‹æœäº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½¿æ·±åº¦å·ç§¯ç½‘ç»œçš„æœ‰æ•ˆè®­ç»ƒæˆä¸ºå¯èƒ½ã€‚</li>
<li>Vision Transformer (ViT) å°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒè¡¥ä¸åºåˆ—ï¼Œè¯æ˜äº†æ³¨æ„åŠ›æ¨¡å‹åœ¨å¤§è§„æ¨¡å›¾åƒè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>Generative Adversarial Networks (GANs) é€šè¿‡å¯¹æŠ—è®­ç»ƒè¿‡ç¨‹æŒ‘æˆ˜ç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨ï¼Œå­¦ä¼šäº†å¤æ‚çš„æ•°æ®åˆ†å¸ƒã€‚</li>
<li>Latent Diffusion Models (LDMs) åœ¨æ„ŸçŸ¥å‹ç¼©æ½œåœ¨ç©ºé—´ä¸­æ‰§è¡Œå»å™ªè¿‡ç¨‹ï¼Œå®ç°äº†é«˜ä¿çœŸåˆæˆå’Œæ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚</li>
<li>DINOæ˜¯ä¸€ç§è‡ªè’¸é¦æ¡†æ¶ï¼Œå­¦ç”Ÿç½‘ç»œå­¦ä¹ åŒ¹é…åŠ¨é‡æ›´æ–°çš„æ•™å¸ˆè¾“å‡ºï¼Œäº§ç”Ÿå…·æœ‰å¼ºå¤§k-NNåˆ†ç±»æ€§èƒ½çš„ç‰¹å¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23357">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f55b25dd93b83437b5e43038bd3e0a8f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-cb308890c31345f2aab07dd14fc1774c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43c7b7523d1996c598c4c86034ca318b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-498ae1a3c68c14971d462838ba0e1fa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a42b24e9ff5111ddf2f9f617b925344.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models"><a href="#The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models" class="headerlink" title="The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in   Text-to-Image Models"></a>The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in   Text-to-Image Models</h2><p><strong>Authors:Alfio Ferrara, Sergio Picascia, Elisabetta Rocchetti</strong></p>
<p>Text-to-image diffusion models have demonstrated remarkable capabilities in generating artistic content by learning from billions of images, including popular artworks. However, the fundamental question of how these models internally represent concepts, such as content and style in paintings, remains unexplored. Traditional computer vision assumes content and style are orthogonal, but diffusion models receive no explicit guidance about this distinction during training. In this work, we investigate how transformer-based text-to-image diffusion models encode content and style concepts when generating artworks. We leverage cross-attention heatmaps to attribute pixels in generated images to specific prompt tokens, enabling us to isolate image regions influenced by content-describing versus style-describing tokens. Our findings reveal that diffusion models demonstrate varying degrees of content-style separation depending on the specific artistic prompt and style requested. In many cases, content tokens primarily influence object-related regions while style tokens affect background and texture areas, suggesting an emergent understanding of the content-style distinction. These insights contribute to our understanding of how large-scale generative models internally represent complex artistic concepts without explicit supervision. We share the code and dataset, together with an exploratory tool for visualizing attention maps at <a target="_blank" rel="noopener" href="https://github.com/umilISLab/artistic-prompt-interpretation">https://github.com/umilISLab/artistic-prompt-interpretation</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å·²ç»æ˜¾ç¤ºå‡ºä»æ•°åäº¿å›¾åƒï¼ˆåŒ…æ‹¬æµè¡Œè‰ºæœ¯ä½œå“ï¼‰ç”Ÿæˆè‰ºæœ¯å†…å®¹çš„æ˜¾è‘—èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¦‚ä½•åœ¨å†…éƒ¨è¡¨ç¤ºç»˜ç”»ä¸­çš„å†…å®¹å’Œé£æ ¼ç­‰æ¦‚å¿µçš„åŸºç¡€é—®é¢˜ä»æœªè¢«æ¢ç´¢ã€‚ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰å‡è®¾å†…å®¹å’Œé£æ ¼æ˜¯æ­£äº¤çš„ï¼Œä½†æ‰©æ•£æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰å…³äºè¿™ç§åŒºåˆ«çš„æ˜ç¡®æŒ‡å¯¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†åŸºäºè½¬æ¢å™¨çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¦‚ä½•åœ¨ç”Ÿæˆè‰ºæœ¯ä½œå“æ—¶ç¼–ç å†…å®¹å’Œé£æ ¼æ¦‚å¿µã€‚æˆ‘ä»¬åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›çƒ­å›¾å°†ç”Ÿæˆå›¾åƒçš„åƒç´ å½’å› äºç‰¹å®šçš„æç¤ºæ ‡è®°ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿéš”ç¦»å—å†…å®¹æè¿°æ ‡è®°ä¸é£æ ¼æè¿°æ ‡è®°å½±å“çš„å›¾åƒåŒºåŸŸã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£æ¨¡å‹åœ¨ä¸åŒè‰ºæœ¯æç¤ºå’Œæ‰€è¯·æ±‚çš„é£æ ¼çš„ç‰¹å®šæƒ…å†µä¸‹è¡¨ç°å‡ºä¸åŒç¨‹åº¦çš„å†…å®¹ä¸é£æ ¼çš„åˆ†ç¦»ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œå†…å®¹æ ‡è®°ä¸»è¦å½±å“ä¸å¯¹è±¡ç›¸å…³çš„åŒºåŸŸï¼Œè€Œé£æ ¼æ ‡è®°å½±å“èƒŒæ™¯å’Œçº¹ç†åŒºåŸŸï¼Œè¿™è¡¨æ˜å¯¹å†…å®¹å’Œé£æ ¼åŒºåˆ«çš„éšæ€§ç†è§£ã€‚è¿™äº›è§è§£æœ‰åŠ©äºæˆ‘ä»¬äº†è§£å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹å¦‚ä½•åœ¨æ²¡æœ‰æ˜¾å¼ç›‘ç£çš„æƒ…å†µä¸‹å†…éƒ¨è¡¨ç¤ºå¤æ‚è‰ºæœ¯æ¦‚å¿µã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/umilISLab/artistic-prompt-interpretation">https://github.com/umilISLab/artistic-prompt-interpretation</a>ä¸Šåˆ†äº«äº†ä»£ç ã€æ•°æ®é›†ä»¥åŠç”¨äºå¯è§†åŒ–æ³¨æ„åŠ›å›¾çš„æ¢ç´¢å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23313v1">PDF</a> to be published in: Applications of AI in the Analysis of Cultural   and Artistic Heritage, organized within the 35th IEEE International Workshop   on Machine Learning for Signal Processing (MLSP) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åŸºäºå˜å‹å™¨çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè‰ºæœ¯ä½œå“æ—¶å¦‚ä½•ç¼–ç å†…å®¹å’Œé£æ ¼æ¦‚å¿µã€‚é€šè¿‡åˆ©ç”¨è·¨æ³¨æ„åŠ›çƒ­å›¾ï¼Œå°†ç”Ÿæˆçš„å›¾åƒä¸­çš„åƒç´ å½’å±äºç‰¹å®šçš„æç¤ºæ ‡è®°ï¼Œä»è€Œéš”ç¦»å—å†…å®¹æè¿°å’Œé£æ ¼æè¿°æ ‡è®°å½±å“çš„å›¾åƒåŒºåŸŸã€‚ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£æ¨¡å‹åœ¨å†…å®¹å’Œé£æ ¼ä¸Šè¡¨ç°å‡ºä¸åŒç¨‹åº¦çš„åˆ†ç¦»ï¼Œè¿™å–å†³äºç‰¹å®šçš„è‰ºæœ¯æç¤ºå’Œæ‰€è¦æ±‚çš„é£æ ¼ã€‚è¿™äº›å‘ç°æœ‰åŠ©äºç†è§£å¤§å‹ç”Ÿæˆæ¨¡å‹å¦‚ä½•åœ¨æ²¡æœ‰æ˜¾å¼ç›‘ç£çš„æƒ…å†µä¸‹å†…éƒ¨è¡¨ç¤ºå¤æ‚è‰ºæœ¯æ¦‚å¿µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¯ä»¥ç”Ÿæˆå…·æœ‰è‰ºæœ¯æ€§çš„å†…å®¹ï¼Œä½†å…¶å†…éƒ¨å¦‚ä½•ä»£è¡¨æ¦‚å¿µï¼Œå¦‚ç»˜ç”»ä¸­çš„å†…å®¹å’Œé£æ ¼ï¼Œå°šæœªè¢«æ¢ç´¢ã€‚</li>
<li>ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰å‡è®¾å†…å®¹å’Œé£æ ¼æ˜¯æ­£äº¤çš„ï¼Œä½†æ‰©æ•£æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰å…³äºè¿™ç§åŒºåˆ«çš„æ˜ç¡®æŒ‡å¯¼ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨è·¨æ³¨æ„åŠ›çƒ­å›¾ï¼Œå¯ä»¥è¿½è¸ªç”Ÿæˆå›¾åƒä¸­çš„åƒç´ ä¸ç‰¹å®šæç¤ºæ ‡è®°çš„å…³ç³»ï¼Œä»è€Œéš”ç¦»å†…å®¹å’Œé£æ ¼çš„å½±å“åŒºåŸŸã€‚</li>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè‰ºæœ¯ä½œå“æ—¶ï¼Œå†…å®¹å’Œé£æ ¼çš„å½±å“ç¨‹åº¦ä¼šæœ‰æ‰€ä¸åŒï¼Œè¿™å–å†³äºè‰ºæœ¯æç¤ºå’Œæ‰€è¦æ±‚çš„é£æ ¼ã€‚</li>
<li>åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œå†…å®¹æ ‡è®°ä¸»è¦å½±å“å¯¹è±¡ç›¸å…³åŒºåŸŸï¼Œè€Œé£æ ¼æ ‡è®°å½±å“èƒŒæ™¯å’Œçº¹ç†åŒºåŸŸï¼Œè¡¨æ˜æ¨¡å‹å¯¹å†…å®¹å’Œé£æ ¼çš„åŒºåˆ†æœ‰ä¸€å®šçš„è‡ªæˆ‘æ„è¯†ã€‚</li>
<li>è¿™äº›å‘ç°å¢åŠ äº†å¯¹å¤§å‹ç”Ÿæˆæ¨¡å‹å¦‚ä½•å†…éƒ¨è¡¨ç¤ºå¤æ‚è‰ºæœ¯æ¦‚å¿µçš„ç†è§£ï¼Œè¿™äº›æ¨¡å‹åœ¨æ— éœ€æ˜¾å¼ç›‘ç£çš„æƒ…å†µä¸‹è¡¨ç°å‡ºå†…å®¹å’Œé£æ ¼çš„åˆ†ç¦»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cbaab2d5dc74bf7f8805253b79ab041f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec4ca46180fe1eced4488586f5557e1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c3b12316863cb073736cf4fd773429d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4059378c7a88da83cb74610a9ce77be.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Adversarial-Guided-Diffusion-for-Multimodal-LLM-Attacks"><a href="#Adversarial-Guided-Diffusion-for-Multimodal-LLM-Attacks" class="headerlink" title="Adversarial-Guided Diffusion for Multimodal LLM Attacks"></a>Adversarial-Guided Diffusion for Multimodal LLM Attacks</h2><p><strong>Authors:Chengwei Xia, Fan Ma, Ruijie Quan, Kun Zhan, Yi Yang</strong></p>
<p>This paper addresses the challenge of generating adversarial image using a diffusion model to deceive multimodal large language models (MLLMs) into generating the targeted responses, while avoiding significant distortion of the clean image. To address the above challenges, we propose an adversarial-guided diffusion (AGD) approach for adversarial attack MLLMs. We introduce adversarial-guided noise to ensure attack efficacy. A key observation in our design is that, unlike most traditional adversarial attacks which embed high-frequency perturbations directly into the clean image, AGD injects target semantics into the noise component of the reverse diffusion. Since the added noise in a diffusion model spans the entire frequency spectrum, the adversarial signal embedded within it also inherits this full-spectrum property. Importantly, during reverse diffusion, the adversarial image is formed as a linear combination of the clean image and the noise. Thus, when applying defenses such as a simple low-pass filtering, which act independently on each component, the adversarial image within the noise component is less likely to be suppressed, as it is not confined to the high-frequency band. This makes AGD inherently robust to variety defenses. Extensive experiments demonstrate that our AGD outperforms state-of-the-art methods in attack performance as well as in model robustness to some defenses. </p>
<blockquote>
<p>æœ¬æ–‡æ—¨åœ¨è§£å†³ä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆå¯¹æŠ—å›¾åƒï¼Œä»¥æ¬ºéª—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰äº§ç”Ÿç›®æ ‡å“åº”çš„æŒ‘æˆ˜ï¼ŒåŒæ—¶é¿å…å¹²å‡€å›¾åƒå‡ºç°é‡å¤§å¤±çœŸã€‚ä¸ºäº†åº”å¯¹ä¸Šè¿°æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå¯¹æŠ—MLLMsçš„å¯¹æŠ—æ€§å¼•å¯¼æ‰©æ•£ï¼ˆAGDï¼‰æ–¹æ³•ã€‚æˆ‘ä»¬å¼•å…¥å¯¹æŠ—æ€§å¼•å¯¼å™ªå£°ä»¥ç¡®ä¿æ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„è®¾è®¡ä¸­çš„ä¸€ä¸ªå…³é”®è§‚å¯Ÿæ˜¯ï¼Œä¸åŒäºå¤§å¤šæ•°ä¼ ç»Ÿå¯¹æŠ—æ€§æ”»å‡»ç›´æ¥å°†é«˜é¢‘æ‰°åŠ¨åµŒå…¥å¹²å‡€å›¾åƒï¼ŒAGDå°†ç›®æ ‡è¯­ä¹‰æ³¨å…¥åå‘æ‰©æ•£çš„å™ªå£°æˆåˆ†ã€‚ç”±äºæ‰©æ•£æ¨¡å‹ä¸­æ·»åŠ çš„å™ªå£°è·¨è¶Šäº†æ•´ä¸ªé¢‘è°±ï¼ŒåµŒå…¥å…¶ä¸­çš„å¯¹æŠ—ä¿¡å·ä¹Ÿç»§æ‰¿äº†è¿™ç§å…¨é¢‘è°±ç‰¹æ€§ã€‚é‡è¦çš„æ˜¯ï¼Œåœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œå¯¹æŠ—æ€§å›¾åƒæ˜¯å¹²å‡€å›¾åƒå’Œå™ªå£°çš„çº¿æ€§ç»„åˆã€‚å› æ­¤ï¼Œå½“åº”ç”¨å¦‚ç®€å•çš„ä½é€šæ»¤æ³¢ç­‰é˜²å¾¡æªæ–½æ—¶ï¼Œè¿™äº›æªæ–½ç‹¬ç«‹ä½œç”¨äºæ¯ä¸ªç»„ä»¶ï¼Œå™ªå£°æˆåˆ†ä¸­çš„å¯¹æŠ—å›¾åƒä¸å¤ªå¯èƒ½è¢«æŠ‘åˆ¶ï¼Œå› ä¸ºå®ƒå¹¶ä¸å±€é™äºé«˜é¢‘å¸¦ã€‚è¿™ä½¿å¾—AGDå¯¹å¤šç§é˜²å¾¡æªæ–½å…·æœ‰å†…åœ¨çš„é²æ£’æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„AGDåœ¨æ”»å‡»æ€§èƒ½å’Œæ¨¡å‹å¯¹æŸäº›é˜²å¾¡çš„ç¨³å¥æ€§æ–¹é¢éƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23202v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å¯¹æŠ—æ€§æ”»å‡»æ–¹æ³•ï¼Œæ—¨åœ¨æ¬ºéª—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç›®æ ‡å“åº”ï¼ŒåŒæ—¶é¿å…å¯¹å¹²å‡€å›¾åƒé€ æˆé‡å¤§å¤±çœŸã€‚æ–‡ç« ä»‹ç»äº†å¯¹æŠ—æ€§å¼•å¯¼æ‰©æ•£ï¼ˆAGDï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å¯¹æŠ—æ€§å¼•å¯¼å™ªå£°ç¡®ä¿æ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚ä¸åŒäºå¤§å¤šæ•°ä¼ ç»Ÿå¯¹æŠ—æ€§æ”»å‡»ç›´æ¥å°†é«˜é¢‘æ‰°åŠ¨åµŒå…¥å¹²å‡€å›¾åƒï¼ŒAGDå°†ç›®æ ‡è¯­ä¹‰æ³¨å…¥åå‘æ‰©æ•£çš„å™ªå£°æˆåˆ†ä¸­ã€‚è¿™ç§åµŒå…¥çš„å¯¹æŠ—ä¿¡å·ç»§æ‰¿äº†æ•´ä¸ªé¢‘è°±å±æ€§ã€‚åœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œå¯¹æŠ—æ€§å›¾åƒæ˜¯ç”±å¹²å‡€å›¾åƒå’Œå™ªå£°çš„çº¿æ€§ç»„åˆå½¢æˆçš„ã€‚å› æ­¤ï¼Œå½“åº”ç”¨å¦‚ç®€å•ä½é€šæ»¤æ³¢ç­‰é˜²å¾¡æ‰‹æ®µæ—¶ï¼ŒAGDä¸­çš„å¯¹æŠ—æ€§å›¾åƒåœ¨å™ªå£°æˆåˆ†ä¸­ä¸å¤ªå¯èƒ½è¢«æŠ‘åˆ¶ï¼Œå› ä¸ºå®ƒä¸é™äºé«˜é¢‘å¸¦ï¼Œè¿™ä½¿å¾—AGDå¯¹å¤šç§é˜²å¾¡æ‰‹æ®µå…·æœ‰å†…åœ¨é²æ£’æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒAGDåœ¨æ”»å‡»æ€§èƒ½å’Œæ¨¡å‹å¯¹æŸäº›é˜²å¾¡çš„é²æ£’æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„å¯¹æŠ—æ€§æ”»å‡»æ–¹æ³•â€”â€”å¯¹æŠ—æ€§å¼•å¯¼æ‰©æ•£ï¼ˆAGDï¼‰ï¼Œç”¨äºæ¬ºéª—å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>AGDé€šè¿‡å¼•å…¥å¯¹æŠ—æ€§å¼•å¯¼å™ªå£°ç¡®ä¿æ”»å‡»çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>AGDå°†ç›®æ ‡è¯­ä¹‰æ³¨å…¥åå‘æ‰©æ•£çš„å™ªå£°æˆåˆ†ä¸­ï¼Œä¸åŒäºä¼ ç»Ÿçš„é«˜é¢‘æ‰°åŠ¨æ–¹æ³•ã€‚</li>
<li>å¯¹æŠ—æ€§ä¿¡å·åµŒå…¥çš„å™ªå£°å…·æœ‰æ•´ä¸ªé¢‘è°±å±æ€§ã€‚</li>
<li>åœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­ï¼Œå¯¹æŠ—æ€§å›¾åƒæ˜¯å¹²å‡€å›¾åƒå’Œå™ªå£°çš„çº¿æ€§ç»„åˆã€‚</li>
<li>AGDå…·æœ‰å¯¹å¤šç§é˜²å¾¡æ‰‹æ®µçš„å†…åœ¨é²æ£’æ€§ï¼Œå› ä¸ºå¯¹æŠ—æ€§å›¾åƒåœ¨å™ªå£°ä¸­ä¸æ˜“è¢«ç®€å•ä½é€šæ»¤æ³¢ç­‰é˜²å¾¡æ‰‹æ®µæŠ‘åˆ¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cb2a17df38cd360d48c0adacfe42ff57.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4000c5d88f3382623661a6c007a637c7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2200c11603f0d0188dc15ac431a3cd5e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e0df70108c52cdf7e55d9a9702f5e853.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8d29fe897ddfd05fafc8a851db2e00b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Hate-in-Plain-Sight-On-the-Risks-of-Moderating-AI-Generated-Hateful-Illusions"><a href="#Hate-in-Plain-Sight-On-the-Risks-of-Moderating-AI-Generated-Hateful-Illusions" class="headerlink" title="Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful   Illusions"></a>Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful   Illusions</h2><p><strong>Authors:Yiting Qu, Ziqing Yang, Yihan Ma, Michael Backes, Savvas Zannettou, Yang Zhang</strong></p>
<p>Recent advances in text-to-image diffusion models have enabled the creation of a new form of digital art: optical illusionsâ€“visual tricks that create different perceptions of reality. However, adversaries may misuse such techniques to generate hateful illusions, which embed specific hate messages into harmless scenes and disseminate them across web communities. In this work, we take the first step toward investigating the risks of scalable hateful illusion generation and the potential for bypassing current content moderation models. Specifically, we generate 1,860 optical illusions using Stable Diffusion and ControlNet, conditioned on 62 hate messages. Of these, 1,571 are hateful illusions that successfully embed hate messages, either overtly or subtly, forming the Hateful Illusion dataset. Using this dataset, we evaluate the performance of six moderation classifiers and nine vision language models (VLMs) in identifying hateful illusions. Experimental results reveal significant vulnerabilities in existing moderation models: the detection accuracy falls below 0.245 for moderation classifiers and below 0.102 for VLMs. We further identify a critical limitation in their vision encoders, which mainly focus on surface-level image details while overlooking the secondary layer of information, i.e., hidden messages. To address this risk, we explore preliminary mitigation measures and identify the most effective approaches from the perspectives of image transformations and training-level strategies. </p>
<blockquote>
<p>è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¿›å±•ä¸ºæ•°å­—è‰ºæœ¯åˆ›é€ äº†ä¸€ç§æ–°çš„å½¢å¼ï¼šå…‰å­¦é”™è§‰â€”â€”é€šè¿‡è§†è§‰æŠ€å·§åˆ›é€ ä¸åŒçš„ç°å®æ„ŸçŸ¥ã€‚ç„¶è€Œï¼Œå¯¹æ‰‹å¯èƒ½ä¼šæ»¥ç”¨è¿™äº›æŠ€æœ¯æ¥ç”Ÿæˆä»‡æ¨é”™è§‰ï¼Œå°†ç‰¹å®šçš„ä»‡æ¨ä¿¡æ¯åµŒå…¥åˆ°æ— å®³çš„åœºæ™¯ä¸­ï¼Œå¹¶åœ¨ç½‘ç»œç¤¾åŒºä¸­ä¼ æ’­å®ƒä»¬ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡å¯¹å¯æ‰©å±•ä»‡æ¨é”™è§‰ç”Ÿæˆçš„é£é™©å’Œç»•è¿‡å½“å‰å†…å®¹ç®¡ç†æ¨¡å‹çš„èƒ½åŠ›è¿›è¡Œäº†è°ƒæŸ¥ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨Stable Diffusionå’ŒControlNetç”Ÿæˆäº†1860ä¸ªå…‰å­¦é”™è§‰ï¼ŒåŸºäº62æ¡ä»‡æ¨ä¿¡æ¯ã€‚å…¶ä¸­ï¼Œæœ‰1571ä¸ªä»‡æ¨é”™è§‰æˆåŠŸåœ°å°†ä»‡æ¨ä¿¡æ¯åµŒå…¥å…¶ä¸­ï¼Œæ— è®ºæ˜¯æ˜æ˜¾è¿˜æ˜¯å¾®å¦™åœ°å½¢æˆäº†ä»‡æ¨é”™è§‰æ•°æ®é›†ã€‚ä½¿ç”¨è¯¥æ•°æ®é›†ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å…­ä¸ªç®¡ç†åˆ†ç±»å™¨å’Œä¹ä¸ªè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨è¯†åˆ«ä»‡æ¨é”™è§‰æ–¹é¢çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ç°æœ‰ç®¡ç†æ¨¡å‹å­˜åœ¨é‡å¤§æ¼æ´ï¼šç®¡ç†åˆ†ç±»å™¨çš„æ£€æµ‹å‡†ç¡®ç‡ä½äº0.245ï¼ŒVLMçš„å‡†ç¡®ç‡ä½äº0.102ã€‚æˆ‘ä»¬è¿˜å‘ç°äº†ä»–ä»¬è§†è§‰ç¼–ç å™¨çš„ä¸€ä¸ªå…³é”®å±€é™æ€§ï¼Œå®ƒä»¬ä¸»è¦å…³æ³¨å›¾åƒçš„è¡¨é¢ç»†èŠ‚ï¼Œè€Œå¿½è§†äº†æ¬¡è¦å±‚çš„ä¿¡æ¯ï¼Œå³éšè—ä¿¡æ¯ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é£é™©ï¼Œæˆ‘ä»¬æ¢ç´¢äº†åˆæ­¥ç¼“è§£æªæ–½ï¼Œå¹¶ä»å›¾åƒè½¬æ¢å’ŒåŸ¹è®­å±‚é¢ç­–ç•¥çš„è§’åº¦ç¡®å®šäº†æœ€æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22617v1">PDF</a> Accepted at ICCV 2025</p>
<p><strong>Summary</strong><br>     è¿‘æœŸæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„è¿›å±•ä¸ºåˆ›é€ æ–°å‹æ•°å­—è‰ºæœ¯â€”â€”å…‰å­¦é”™è§‰æä¾›äº†å¯èƒ½ï¼Œè¿™äº›é”™è§‰èƒ½åœ¨æ— å®³çš„åœºæ™¯ä¸­åµŒå…¥ç‰¹å®šä»‡æ¨ä¿¡æ¯å¹¶ä¼ æ’­åˆ°ç½‘ç»œç¤¾åŒºä¸­ã€‚æœ¬ç ”ç©¶é¦–æ¬¡æ¢è®¨äº†å¯ä¼¸ç¼©ä»‡æ¨é”™è§‰ç”Ÿæˆçš„é£é™©å’Œç»•è¿‡å½“å‰å†…å®¹å®¡æ ¸æ¨¡å‹çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨Stable Diffusionå’ŒControlNetç”Ÿæˆäº†1860ä¸ªå…‰å­¦é”™è§‰å›¾åƒï¼Œå…¶ä¸­åŒ…å«62æ¡ä»‡æ¨ä¿¡æ¯ã€‚å®éªŒå‘ç°ç°æœ‰å®¡æ ¸æ¨¡å‹å­˜åœ¨æ˜¾è‘—æ¼æ´ï¼Œæ£€æµ‹å‡†ç¡®ç‡ä½äº0.245ã€‚æ­¤å¤–ï¼Œæ¨¡å‹åœ¨è¯†åˆ«å›¾åƒä¸­çš„éšè—ä¿¡æ¯æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬åˆæ­¥æ¢è®¨äº†ç¼“è§£æªæ–½ï¼Œå¹¶ä»å›¾åƒè½¬æ¢å’ŒåŸ¹è®­ç­–ç•¥ä¸¤æ–¹é¢æå‡ºäº†æœ€æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹çš„æœ€æ–°è¿›å±•ä¸ºåˆ›å»ºå…‰å­¦é”™è§‰æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚</li>
<li>å…‰å­¦é”™è§‰æŠ€æœ¯å¯è¢«ç”¨äºç”Ÿæˆä»‡æ¨é”™è§‰ï¼Œå°†ä»‡æ¨ä¿¡æ¯åµŒå…¥æ— å®³åœºæ™¯ä¸­å¹¶åœ¨ç½‘ç»œç¤¾åŒºä¸­ä¼ æ’­ã€‚</li>
<li>é¦–æ¬¡è°ƒæŸ¥äº†å¯ä¼¸ç¼©ä»‡æ¨é”™è§‰ç”Ÿæˆçš„é£é™©å’Œå¯¹ç°æœ‰å†…å®¹å®¡æ ¸æ¨¡å‹çš„æ½œåœ¨å¨èƒã€‚</li>
<li>ç”Ÿæˆäº†åŒ…å«ä»‡æ¨ä¿¡æ¯çš„1,571ä¸ªä»‡æ¨é”™è§‰å›¾åƒï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªæ•°æ®é›†ã€‚</li>
<li>å®éªŒå‘ç°ç°æœ‰å®¡æ ¸æ¨¡å‹å­˜åœ¨æ˜¾è‘—æ¼æ´ï¼Œæ£€æµ‹å‡†ç¡®ç‡ä½äº0.245ã€‚</li>
<li>ç°æœ‰å®¡æ ¸æ¨¡å‹åœ¨è¯†åˆ«å›¾åƒä¸­çš„éšè—ä¿¡æ¯æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œä¸»è¦å…³æ³¨è¡¨é¢ç»†èŠ‚è€Œå¿½è§†æ¬¡è¦ä¿¡æ¯å±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-74d6112d3e033c6979c2fde4e2c592ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f6e0f839781b36695ecb31a14fff7a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f66625b32784967d39e256df7116d41e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b6723a3cbbacb5abd1a2a73e696b5a55.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-75ab518029df39b604c55fb9e16de3fc.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="DACA-Net-A-Degradation-Aware-Conditional-Diffusion-Network-for-Underwater-Image-Enhancement"><a href="#DACA-Net-A-Degradation-Aware-Conditional-Diffusion-Network-for-Underwater-Image-Enhancement" class="headerlink" title="DACA-Net: A Degradation-Aware Conditional Diffusion Network for   Underwater Image Enhancement"></a>DACA-Net: A Degradation-Aware Conditional Diffusion Network for   Underwater Image Enhancement</h2><p><strong>Authors:Chang Huang, Jiahang Cao, Jun Ma, Kieren Yu, Cong Li, Huayong Yang, Kaishun Wu</strong></p>
<p>Underwater images typically suffer from severe colour distortions, low visibility, and reduced structural clarity due to complex optical effects such as scattering and absorption, which greatly degrade their visual quality and limit the performance of downstream visual perception tasks. Existing enhancement methods often struggle to adaptively handle diverse degradation conditions and fail to leverage underwater-specific physical priors effectively. In this paper, we propose a degradation-aware conditional diffusion model to enhance underwater images adaptively and robustly. Given a degraded underwater image as input, we first predict its degradation level using a lightweight dual-stream convolutional network, generating a continuous degradation score as semantic guidance. Based on this score, we introduce a novel conditional diffusion-based restoration network with a Swin UNet backbone, enabling adaptive noise scheduling and hierarchical feature refinement. To incorporate underwater-specific physical priors, we further propose a degradation-guided adaptive feature fusion module and a hybrid loss function that combines perceptual consistency, histogram matching, and feature-level contrast. Comprehensive experiments on benchmark datasets demonstrate that our method effectively restores underwater images with superior colour fidelity, perceptual quality, and structural details. Compared with SOTA approaches, our framework achieves significant improvements in both quantitative metrics and qualitative visual assessments. </p>
<blockquote>
<p>æ°´ä¸‹å›¾åƒé€šå¸¸å—åˆ°ä¸¥é‡çš„è‰²å½©å¤±çœŸã€ä½å¯è§åº¦å’Œç»“æ„æ¸…æ™°åº¦é™ä½çš„å½±å“ï¼Œè¿™æ˜¯ç”±äºæ•£å°„å’Œå¸æ”¶ç­‰å¤æ‚çš„å…‰å­¦æ•ˆåº”å¯¼è‡´çš„ã€‚è¿™å¤§å¤§é™ä½äº†æ°´ä¸‹å›¾åƒçš„å¯è§†è´¨é‡ï¼Œå¹¶é™åˆ¶äº†åç»­è§†è§‰æ„ŸçŸ¥ä»»åŠ¡çš„æ€§èƒ½ã€‚ç°æœ‰çš„å¢å¼ºæ–¹æ³•å¾€å¾€éš¾ä»¥é€‚åº”ä¸åŒçš„é€€åŒ–æ¡ä»¶ï¼Œå¹¶ä¸”æœªèƒ½æœ‰æ•ˆåœ°åˆ©ç”¨æ°´ä¸‹ç‰¹å®šçš„ç‰©ç†å…ˆéªŒä¿¡æ¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ„è¯†åˆ°çš„é€€åŒ–æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œä»¥è‡ªé€‚åº”å’Œç¨³å¥åœ°å¢å¼ºæ°´ä¸‹å›¾åƒã€‚ç»™å®šä¸€ä¸ªé€€åŒ–æ°´ä¸‹å›¾åƒä½œä¸ºè¾“å…¥ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨è½»é‡çº§åŒæµå·ç§¯ç½‘ç»œé¢„æµ‹å…¶é€€åŒ–ç¨‹åº¦ï¼Œç”Ÿæˆè¿ç»­çš„é€€åŒ–åˆ†æ•°ä½œä¸ºè¯­ä¹‰æŒ‡å¯¼ã€‚åŸºäºæ­¤åˆ†æ•°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹çš„æ¡ä»¶æ‰©æ•£æ¢å¤ç½‘ç»œï¼Œå…·æœ‰Swin UNetä¸»å¹²ï¼Œèƒ½å¤Ÿå®ç°è‡ªé€‚åº”å™ªå£°è°ƒåº¦å’Œåˆ†å±‚ç‰¹å¾ç»†åŒ–ã€‚ä¸ºäº†èå…¥æ°´ä¸‹ç‰¹å®šçš„ç‰©ç†å…ˆéªŒä¿¡æ¯ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªç”±é€€åŒ–å¼•å¯¼çš„è‡ªé€‚åº”ç‰¹å¾èåˆæ¨¡å—å’Œä¸€ä¸ªæ··åˆæŸå¤±å‡½æ•°ï¼Œè¯¥æŸå¤±å‡½æ•°ç»“åˆäº†æ„ŸçŸ¥ä¸€è‡´æ€§ã€ç›´æ–¹å›¾åŒ¹é…å’Œç‰¹å¾çº§å¯¹æ¯”åº¦ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°æ¢å¤äº†æ°´ä¸‹å›¾åƒï¼Œå…·æœ‰å‡ºè‰²çš„è‰²å½©ä¿çœŸåº¦ã€æ„ŸçŸ¥è´¨é‡å’Œç»“æ„ç»†èŠ‚ã€‚ä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§è§†è§‰è¯„ä¼°æ–¹é¢éƒ½å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22501v1">PDF</a> accepted by ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé™è§£æ„ŸçŸ¥çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè‡ªé€‚åº”å’Œç¨³å¥åœ°å¢å¼ºæ°´ä¸‹å›¾åƒã€‚é¦–å…ˆåˆ©ç”¨è½»é‡çº§åŒæµä¼ å·ç§¯ç½‘ç»œé¢„æµ‹å›¾åƒé€€åŒ–ç¨‹åº¦ï¼Œç”Ÿæˆè¿ç»­é€€åŒ–åˆ†æ•°ä½œä¸ºè¯­ä¹‰æŒ‡å¯¼ã€‚åŸºäºè¯¥åˆ†æ•°ï¼Œå¼•å…¥æ–°å‹æ¡ä»¶æ‰©æ•£å¼æ¢å¤ç½‘ç»œï¼Œç»“åˆSwin UNetä¸»å¹²ç½‘ï¼Œå®ç°è‡ªé€‚åº”å™ªå£°è°ƒåº¦å’Œåˆ†å±‚ç‰¹å¾ç»†åŒ–ã€‚é€šè¿‡æ°´ä¸‹ç‰¹å®šç‰©ç†å…ˆéªŒçš„èå…¥ï¼Œè¿›ä¸€æ­¥æå‡ºé™è§£å¼•å¯¼è‡ªé€‚åº”ç‰¹å¾èåˆæ¨¡å—å’Œæ··åˆæŸå¤±å‡½æ•°ï¼Œç»“åˆæ„ŸçŸ¥ä¸€è‡´æ€§ã€ç›´æ–¹å›¾åŒ¹é…å’Œç‰¹å¾çº§å¯¹æ¯”ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æœ‰æ•ˆæ¢å¤äº†æ°´ä¸‹å›¾åƒï¼Œå…·æœ‰å‡ºè‰²çš„è‰²å½©ä¿çœŸåº¦ã€æ„ŸçŸ¥è´¨é‡å’Œç»“æ„ç»†èŠ‚ã€‚ä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ¡†æ¶åœ¨å®šé‡æŒ‡æ ‡å’Œå®šæ€§è§†è§‰è¯„ä¼°æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹å›¾åƒå¸¸å—åˆ°ä¸¥é‡è‰²å½©å¤±çœŸã€ä½å¯è§åº¦å’Œç»“æ„æ¸…æ™°åº¦é™ä½çš„å½±å“ã€‚</li>
<li>å¤æ‚çš„å…‰å­¦æ•ˆåº”ï¼ˆå¦‚æ•£å°„å’Œå¸æ”¶ï¼‰å¯¼è‡´å›¾åƒè§†è§‰è´¨é‡ä¸‹é™ï¼Œå½±å“ä¸‹æ¸¸è§†è§‰æ„ŸçŸ¥ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>ç°æœ‰çš„å¢å¼ºæ–¹æ³•å¾€å¾€éš¾ä»¥é€‚åº”ä¸åŒçš„é€€åŒ–æ¡ä»¶ï¼Œæœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ°´ä¸‹ç‰¹å®šçš„ç‰©ç†å…ˆéªŒã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§é™è§£æ„ŸçŸ¥çš„æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè‡ªé€‚åº”å’Œç¨³å¥åœ°å¢å¼ºæ°´ä¸‹å›¾åƒã€‚</li>
<li>é€šè¿‡è½»é‡çº§åŒæµä¼ å·ç§¯ç½‘ç»œé¢„æµ‹å›¾åƒé€€åŒ–ç¨‹åº¦ï¼Œç”Ÿæˆè¿ç»­é€€åŒ–åˆ†æ•°ä½œä¸ºè¯­ä¹‰æŒ‡å¯¼ã€‚</li>
<li>å¼•å…¥æ–°å‹æ¡ä»¶æ‰©æ•£å¼æ¢å¤ç½‘ç»œï¼Œç»“åˆSwin UNetä¸»å¹²ç½‘ï¼Œå®ç°è‡ªé€‚åº”å™ªå£°å¤„ç†å’Œåˆ†å±‚ç‰¹å¾ç»†åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22501">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a35c6094f531e7bda892506585b34738.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f25a06b76d9a14d3b89261231640698.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f8198489c49a83aeb9af1c19ee0e1ac.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Visual-Language-Models-as-Zero-Shot-Deepfake-Detectors"><a href="#Visual-Language-Models-as-Zero-Shot-Deepfake-Detectors" class="headerlink" title="Visual Language Models as Zero-Shot Deepfake Detectors"></a>Visual Language Models as Zero-Shot Deepfake Detectors</h2><p><strong>Authors:Viacheslav Pirogov</strong></p>
<p>The contemporary phenomenon of deepfakes, utilizing GAN or diffusion models for face swapping, presents a substantial and evolving threat in digital media, identity verification, and a multitude of other systems. The majority of existing methods for detecting deepfakes rely on training specialized classifiers to distinguish between genuine and manipulated images, focusing only on the image domain without incorporating any auxiliary tasks that could enhance robustness. In this paper, inspired by the zero-shot capabilities of Vision Language Models, we propose a novel VLM-based approach to image classification and then evaluate it for deepfake detection. Specifically, we utilize a new high-quality deepfake dataset comprising 60,000 images, on which our zero-shot models demonstrate superior performance to almost all existing methods. Subsequently, we compare the performance of the best-performing architecture, InstructBLIP, on the popular deepfake dataset DFDC-P against traditional methods in two scenarios: zero-shot and in-domain fine-tuning. Our results demonstrate the superiority of VLMs over traditional classifiers. </p>
<blockquote>
<p>ç°ä»£æ·±åº¦ä¼ªé€ ç°è±¡ï¼Œåˆ©ç”¨GANæˆ–æ‰©æ•£æ¨¡å‹è¿›è¡Œé¢éƒ¨æ›¿æ¢ï¼Œç»™æ•°å­—åª’ä½“ã€èº«ä»½éªŒè¯ä»¥åŠå…¶ä»–å¤šç§ç³»ç»Ÿå¸¦æ¥äº†æ˜¾è‘—ä¸”ä¸æ–­æ¼”å˜çš„å¨èƒã€‚å¤§å¤šæ•°ç°æœ‰çš„æ£€æµ‹æ·±åº¦ä¼ªé€ çš„æ–¹æ³•éƒ½ä¾èµ–äºè®­ç»ƒä¸“é—¨çš„åˆ†ç±»å™¨æ¥åŒºåˆ†çœŸå®å’Œæ“çºµè¿‡çš„å›¾åƒï¼Œåªä¸“æ³¨äºå›¾åƒé¢†åŸŸï¼Œè€Œæ²¡æœ‰èå…¥ä»»ä½•å¯ä»¥å¢å¼ºç¨³å¥æ€§çš„è¾…åŠ©ä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å—åˆ°è§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å›¾åƒåˆ†ç±»æ–°æ–¹æ³•ï¼Œå¹¶è¯„ä¼°äº†å…¶åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„åº”ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†ä¸€ä¸ªæ–°çš„é«˜è´¨é‡æ·±åº¦ä¼ªé€ æ•°æ®é›†ï¼ŒåŒ…å«6ä¸‡å¼ å›¾åƒï¼Œæˆ‘ä»¬çš„é›¶æ ·æœ¬æ¨¡å‹åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ ä¹è¶…è¿‡äº†æ‰€æœ‰ç°æœ‰æ–¹æ³•ã€‚éšåï¼Œæˆ‘ä»¬åœ¨æµè¡Œçš„æ·±åº¦ä¼ªé€ æ•°æ®é›†DFDC-Pä¸Šå¯¹æ¯”äº†è¡¨ç°æœ€ä½³çš„æ¶æ„InstructBLIPä¸ä¼ ç»Ÿæ–¹æ³•åœ¨é›¶æ ·æœ¬å’ŒåŸŸå†…å¾®è°ƒä¸¤ç§æƒ…å†µä¸‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†è§†è§‰è¯­è¨€æ¨¡å‹ç›¸è¾ƒäºä¼ ç»Ÿåˆ†ç±»å™¨çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22469v1">PDF</a> Accepted to the ICML 2025 Workshop on Reliable and Responsible   Foundation Models</p>
<p><strong>Summary</strong><br>     åˆ©ç”¨GANæˆ–æ‰©æ•£æ¨¡å‹è¿›è¡Œé¢éƒ¨æ›¿æ¢çš„Deepfakesç°è±¡å¯¹æ•°å­—åª’ä½“ã€èº«ä»½éªŒè¯ç­‰å¤šä¸ªç³»ç»Ÿæ„æˆé‡å¤§ä¸”ä¸æ–­å‘å±•çš„å¨èƒã€‚ç°æœ‰çš„å¤§å¤šæ•°æ£€æµ‹æ·±åº¦ä¼ªé€ çš„æ–¹æ³•ä¾èµ–äºè®­ç»ƒä¸“é—¨çš„åˆ†ç±»å™¨æ¥åŒºåˆ†çœŸå®å’Œæ“çºµè¿‡çš„å›¾åƒï¼Œä½†ä»…é™äºå›¾åƒé¢†åŸŸï¼Œæ²¡æœ‰å¼•å…¥ä»»ä½•å¯ä»¥å¢å¼ºç¨³å¥æ€§çš„è¾…åŠ©ä»»åŠ¡ã€‚æœ¬æ–‡å—è§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºVLMçš„å›¾åƒåˆ†ç±»æ–¹æ³•ï¼Œå¹¶è¯„ä¼°å…¶åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„åº”ç”¨ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨æ–°å‹é«˜è´¨é‡æ·±åº¦ä¼ªé€ æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„é›¶æ ·æœ¬æ¨¡å‹å‡ ä¹åœ¨æ‰€æœ‰ç°æœ‰æ–¹æ³•ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æœ€ä½³æ€§èƒ½æ¶æ„InstructBLIPåœ¨æµè¡Œçš„æ·±åº¦ä¼ªé€ æ•°æ®é›†DFDC-Pä¸Šä¸ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œäº†é›¶æ ·æœ¬å’ŒåŸŸå†…å¾®è°ƒä¸¤ç§åœºæ™¯ä¸‹çš„æ€§èƒ½å¯¹æ¯”ï¼Œç»“æœè¯æ˜äº†VLMsç›¸è¾ƒäºä¼ ç»Ÿåˆ†ç±»å™¨çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Deepfakesç°è±¡ä½¿ç”¨GANæˆ–æ‰©æ•£æ¨¡å‹è¿›è¡Œé¢éƒ¨æ›¿æ¢ï¼Œå¯¹æ•°å­—åª’ä½“å’Œèº«ä»½éªŒè¯ç³»ç»Ÿæ„æˆå¨èƒã€‚</li>
<li>ç°æœ‰çš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾èµ–ä¸“é—¨çš„åˆ†ç±»å™¨æ¥åŒºåˆ†çœŸå®å’Œæ“çºµè¿‡çš„å›¾åƒï¼Œä½†ç¼ºä¹è¾…åŠ©ä»»åŠ¡ä»¥å¢å¼ºå…¶ç¨³å¥æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„é›¶æ ·æœ¬èƒ½åŠ›è¿›è¡Œå›¾åƒåˆ†ç±»çš„æ–°æ–¹æ³•ã€‚</li>
<li>åœ¨æ–°å‹é«˜è´¨é‡æ·±åº¦ä¼ªé€ æ•°æ®é›†ä¸Šï¼Œé›¶æ ·æœ¬æ¨¡å‹è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ã€‚</li>
<li>å¯¹æ¯”äº†æœ€ä½³æ€§èƒ½æ¶æ„InstructBLIPä¸ä¼ ç»Ÿæ–¹æ³•åœ¨æ·±åº¦ä¼ªé€ æ•°æ®é›†DFDC-Pä¸Šçš„æ€§èƒ½ã€‚</li>
<li>InstructBLIPåœ¨é›¶æ ·æœ¬å’ŒåŸŸå†…å¾®è°ƒä¸¤ç§åœºæ™¯ä¸‹å‡è¡¨ç°å‡ºä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½ã€‚</li>
<li>éªŒè¯äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„ä¼˜è¶Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22469">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4574ad67e7a06d38d2ff41cdc97f08b2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3a0a994545b478dbbad8fe299d7328cf.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="TopoLiDM-Topology-Aware-LiDAR-Diffusion-Models-for-Interpretable-and-Realistic-LiDAR-Point-Cloud-Generation"><a href="#TopoLiDM-Topology-Aware-LiDAR-Diffusion-Models-for-Interpretable-and-Realistic-LiDAR-Point-Cloud-Generation" class="headerlink" title="TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and   Realistic LiDAR Point Cloud Generation"></a>TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and   Realistic LiDAR Point Cloud Generation</h2><p><strong>Authors:Jiuming Liu, Zheng Huang, Mengmeng Liu, Tianchen Deng, Francesco Nex, Hao Cheng, Hesheng Wang</strong></p>
<p>LiDAR scene generation is critical for mitigating real-world LiDAR data collection costs and enhancing the robustness of downstream perception tasks in autonomous driving. However, existing methods commonly struggle to capture geometric realism and global topological consistency. Recent LiDAR Diffusion Models (LiDMs) predominantly embed LiDAR points into the latent space for improved generation efficiency, which limits their interpretable ability to model detailed geometric structures and preserve global topological consistency. To address these challenges, we propose TopoLiDM, a novel framework that integrates graph neural networks (GNNs) with diffusion models under topological regularization for high-fidelity LiDAR generation. Our approach first trains a topological-preserving VAE to extract latent graph representations by graph construction and multiple graph convolutional layers. Then we freeze the VAE and generate novel latent topological graphs through the latent diffusion models. We also introduce 0-dimensional persistent homology (PH) constraints, ensuring the generated LiDAR scenes adhere to real-world global topological structures. Extensive experiments on the KITTI-360 dataset demonstrate TopoLiDMâ€™s superiority over state-of-the-art methods, achieving improvements of 22.6% lower Frechet Range Image Distance (FRID) and 9.2% lower Minimum Matching Distance (MMD). Notably, our model also enables fast generation speed with an average inference time of 1.68 samples&#x2F;s, showcasing its scalability for real-world applications. We will release the related codes at <a target="_blank" rel="noopener" href="https://github.com/IRMVLab/TopoLiDM">https://github.com/IRMVLab/TopoLiDM</a>. </p>
<blockquote>
<p>æ¿€å…‰é›·è¾¾åœºæ™¯ç”Ÿæˆå¯¹äºé™ä½çœŸå®ä¸–ç•Œæ¿€å…‰é›·è¾¾æ•°æ®é‡‡é›†æˆæœ¬ã€æé«˜è‡ªåŠ¨é©¾é©¶ä¸‹æ¸¸æ„ŸçŸ¥ä»»åŠ¡çš„ç¨³å¥æ€§è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸éš¾ä»¥æ•æ‰å‡ ä½•ç°å®å’Œå…¨å±€æ‹“æ‰‘ä¸€è‡´æ€§ã€‚æœ€è¿‘çš„æ¿€å…‰é›·è¾¾æ‰©æ•£æ¨¡å‹ï¼ˆLiDMï¼‰ä¸»è¦å°†æ¿€å…‰é›·è¾¾ç‚¹åµŒå…¥æ½œåœ¨ç©ºé—´ä»¥æé«˜ç”Ÿæˆæ•ˆç‡ï¼Œè¿™é™åˆ¶äº†å…¶å»ºæ¨¡è¯¦ç»†å‡ ä½•ç»“æ„å’Œä¿æŒå…¨å±€æ‹“æ‰‘ä¸€è‡´æ€§çš„è§£é‡Šèƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†TopoLiDMï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰å’Œæ‹“æ‰‘æ­£åˆ™åŒ–ä¸‹çš„æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºé«˜ä¿çœŸåº¦çš„æ¿€å…‰é›·è¾¾ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆè®­ç»ƒä¸€ä¸ªä¿ç•™æ‹“æ‰‘ç»“æ„çš„VAEï¼Œé€šè¿‡å›¾æ„å»ºå’Œå¤šä¸ªå›¾å·ç§¯å±‚æå–æ½œåœ¨å›¾è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬å†»ç»“VAEï¼Œå¹¶é€šè¿‡æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ–°çš„æ½œåœ¨æ‹“æ‰‘å›¾ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†é›¶ç»´æŒä¹…åŒä½™ï¼ˆPHï¼‰çº¦æŸï¼Œç¡®ä¿ç”Ÿæˆçš„æ¿€å…‰é›·è¾¾åœºæ™¯ç¬¦åˆç°å®ä¸–ç•Œçš„å…¨å±€æ‹“æ‰‘ç»“æ„ã€‚åœ¨KITTI-360æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTopoLiDMä¼˜äºæœ€æ–°æ–¹æ³•ï¼ŒFrechet Range Image Distance (FRID)é™ä½äº†22.6%ï¼ŒMinimum Matching Distance (MMD)é™ä½äº†9.2%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜å®ç°äº†å¿«é€Ÿçš„ç”Ÿæˆé€Ÿåº¦ï¼Œå¹³å‡æ¨ç†æ—¶é—´ä¸º1.68æ ·æœ¬&#x2F;ç§’ï¼Œå±•ç¤ºäº†å…¶é€‚ç”¨äºå®é™…åº”ç”¨çš„å¯æ‰©å±•æ€§ã€‚æˆ‘ä»¬å°†ä¼šåœ¨<a target="_blank" rel="noopener" href="https://github.com/IRMVLab/TopoLiDM%E5%8F%91%E5%B8%83%E7%9B%B8%E5%85%B3%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/IRMVLab/TopoLiDMå‘å¸ƒç›¸å…³ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22454v1">PDF</a> Accepted by IROS 2025. Code:<a target="_blank" rel="noopener" href="https://github.com/IRMVLab/TopoLiDM">https://github.com/IRMVLab/TopoLiDM</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†TopoLiDMæ¡†æ¶ï¼Œç»“åˆäº†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ä¸æ‰©æ•£æ¨¡å‹ï¼Œç”¨äºè¿›è¡Œæ‹“æ‰‘æ­£åˆ™åŒ–çš„é«˜ä¿çœŸLiDARåœºæ™¯ç”Ÿæˆã€‚è¯¥æ¡†æ¶é€šè¿‡è®­ç»ƒæ‹“æ‰‘ä¿æŒå¼VAEæå–æ½œåœ¨å›¾è¡¨ç¤ºï¼Œç„¶åé€šè¿‡æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ–°çš„æ½œåœ¨æ‹“æ‰‘å›¾ã€‚æ­¤å¤–ï¼Œå¼•å…¥é›¶ç»´æŒä¹…æ€§åŒæºæ€§ï¼ˆPHï¼‰çº¦æŸä»¥ç¡®ä¿ç”Ÿæˆçš„LiDARåœºæ™¯ç¬¦åˆç°å®ä¸–ç•Œå…¨å±€æ‹“æ‰‘ç»“æ„ã€‚å®éªŒè¯æ˜ï¼ŒTopoLiDMåœ¨KITTI-360æ•°æ®é›†ä¸Šè¾ƒç°æœ‰æ–¹æ³•è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œæé«˜äº†ç”Ÿæˆé€Ÿåº¦å’Œæ„ŸçŸ¥ä»»åŠ¡çš„é²æ£’æ€§ã€‚å°†åœ¨IRMVLabçš„GitHubä¸Šå‘å¸ƒç›¸å…³ä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LiDARåœºæ™¯ç”Ÿæˆå¯¹é™ä½çœŸå®ä¸–ç•Œæ•°æ®æ”¶é›†æˆæœ¬å’Œæé«˜è‡ªåŠ¨é©¾é©¶ä¸‹æ¸¸æ„ŸçŸ¥ä»»åŠ¡ç¨³å¥æ€§è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥æ•æ‰å‡ ä½•çœŸå®æ€§å’Œå…¨å±€æ‹“æ‰‘ä¸€è‡´æ€§ã€‚</li>
<li>TopoLiDMæ¡†æ¶ç»“åˆäº†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ä¸æ‰©æ•£æ¨¡å‹ï¼Œå®ç°é«˜ä¿çœŸLiDARç”Ÿæˆã€‚</li>
<li>ä½¿ç”¨æ‹“æ‰‘ä¿æŒå¼VAEæå–æ½œåœ¨å›¾è¡¨ç¤ºï¼Œé€šè¿‡æ½œåœ¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ–°æ‹“æ‰‘å›¾ã€‚</li>
<li>å¼•å…¥é›¶ç»´æŒä¹…æ€§åŒæºæ€§ï¼ˆPHï¼‰çº¦æŸä»¥ç¡®ä¿ç”Ÿæˆçš„LiDARåœºæ™¯ç¬¦åˆç°å®ä¸–ç•Œå…¨å±€ç»“æ„ã€‚</li>
<li>åœ¨KITTI-360æ•°æ®é›†ä¸Šï¼ŒTopoLiDMè¾ƒç°æœ‰æ–¹æ³•æ€§èƒ½ä¼˜è¶Šï¼Œé™ä½Frechet Range Image Distance (FRID)å’ŒMinimum Matching Distance (MMD)ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22454">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d77417fecd03ca16cecdc5a66307b295.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecf8ebbdfe8ba13a9b1bb9b7c9b82e29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c45b6d85ea8ce39f94def9e2b071b15.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0d6c13d917ffed9dd5be5244868c4aec.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-25585702c242e81201029d65aaaf9220.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad31fb9e755ace647d14f89540452865.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Learning-to-See-in-the-Extremely-Dark"><a href="#Learning-to-See-in-the-Extremely-Dark" class="headerlink" title="Learning to See in the Extremely Dark"></a>Learning to See in the Extremely Dark</h2><p><strong>Authors:Hai Jiang, Binhao Guan, Zhen Liu, Xiaohong Liu, Jian Yu, Zheng Liu, Songchen Han, Shuaicheng Liu</strong></p>
<p>Learning-based methods have made promising advances in low-light RAW image enhancement, while their capability to extremely dark scenes where the environmental illuminance drops as low as 0.0001 lux remains to be explored due to the lack of corresponding datasets. To this end, we propose a paired-to-paired data synthesis pipeline capable of generating well-calibrated extremely low-light RAW images at three precise illuminance ranges of 0.01-0.1 lux, 0.001-0.01 lux, and 0.0001-0.001 lux, together with high-quality sRGB references to comprise a large-scale paired dataset named See-in-the-Extremely-Dark (SIED) to benchmark low-light RAW image enhancement approaches. Furthermore, we propose a diffusion-based framework that leverages the generative ability and intrinsic denoising property of diffusion models to restore visually pleasing results from extremely low-SNR RAW inputs, in which an Adaptive Illumination Correction Module (AICM) and a color consistency loss are introduced to ensure accurate exposure correction and color restoration. Extensive experiments on the proposed SIED and publicly available benchmarks demonstrate the effectiveness of our method. The code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/JianghaiSCU/SIED">https://github.com/JianghaiSCU/SIED</a>. </p>
<blockquote>
<p>åŸºäºå­¦ä¹ çš„æ–¹æ³•åœ¨ä½å…‰ç¯å¢ƒä¸‹çš„RAWå›¾åƒå¢å¼ºæ–¹é¢å·²å–å¾—äº†æœ‰å‰æ™¯çš„è¿›å±•ï¼Œç„¶è€Œï¼Œç”±äºç¼ºä¹ç›¸åº”çš„æ•°æ®é›†ï¼Œå®ƒä»¬åœ¨ç¯å¢ƒç…§åº¦é™è‡³0.0001å‹’å…‹æ–¯çš„ææš—åœºæ™¯ä¸‹çš„èƒ½åŠ›å°šå¾…æ¢ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é…å¯¹åˆ°é…å¯¹çš„æ•°æ®åˆæˆç®¡é“ï¼Œèƒ½å¤Ÿç”Ÿæˆä¸‰ä¸ªç²¾ç¡®ç…§åº¦èŒƒå›´å†…çš„æ ¡å‡†è‰¯å¥½çš„æä½å…‰RAWå›¾åƒï¼Œå³0.01-0.1å‹’å…‹æ–¯ã€0.001-0.01å‹’å…‹æ–¯å’Œ0.0001-0.001å‹’å…‹æ–¯ï¼Œä»¥åŠé«˜è´¨é‡sRGBå‚è€ƒå›¾åƒï¼Œå…±åŒç»„æˆå¤§è§„æ¨¡é…å¯¹æ•°æ®é›†ï¼Œåä¸ºâ€œè§ææš—â€ï¼ˆSIEDï¼‰ï¼Œä½œä¸ºä½å…‰RAWå›¾åƒå¢å¼ºæ–¹æ³•çš„åŸºå‡†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œå†…åœ¨é™å™ªå±æ€§ï¼Œä»æä½çš„SNR RAWè¾“å…¥ä¸­æ¢å¤å‡ºè§†è§‰ä¸Šçš„æ„‰æ‚¦ç»“æœï¼Œå…¶ä¸­å¼•å…¥äº†è‡ªé€‚åº”ç…§æ˜æ ¡æ­£æ¨¡å—ï¼ˆAICMï¼‰å’Œé¢œè‰²ä¸€è‡´æ€§æŸå¤±ï¼Œä»¥ç¡®ä¿å‡†ç¡®çš„æ›å…‰æ ¡æ­£å’Œé¢œè‰²æ¢å¤ã€‚åœ¨æå‡ºçš„SIEDå’Œå…¬å¼€å¯ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JianghaiSCU/SIED%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/JianghaiSCU/SIEDæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.21132v2">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ•°æ®åˆæˆæ–¹æ³•ï¼Œç”Ÿæˆç²¾ç¡®æ ¡å‡†çš„æä½å…‰ç…§RAWå›¾åƒåŠå…¶sRGBå‚è€ƒçš„å¤§å‹é…å¯¹æ•°æ®é›†â€œææš—ç¯å¢ƒä¸‹çš„è§†è§‰æ„ŸçŸ¥â€ï¼ˆSee-in-the-Extremely-Darkï¼Œç®€ç§°SIEDï¼‰ã€‚æ­¤å¤–ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œå†…åœ¨å»å™ªç‰¹æ€§ï¼Œæå‡ºä¸€ç§åŸºäºæ‰©æ•£çš„æ¡†æ¶ï¼Œç”¨äºä»æä½ä¿¡å™ªæ¯”çš„RAWè¾“å…¥ä¸­æ¢å¤å‡ºè§†è§‰ä¸Šä»¤äººæ„‰æ‚¦çš„ç»“æœã€‚å…¶ä¸­å¼•å…¥äº†è‡ªé€‚åº”å…‰ç…§æ ¡æ­£æ¨¡å—å’Œè‰²å½©ä¸€è‡´æ€§æŸå¤±ï¼Œç¡®ä¿å‡†ç¡®çš„æ›å…‰æ ¡æ­£å’Œè‰²å½©æ¢å¤ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨æå‡ºçš„SIEDå’Œå…¬å¼€åŸºå‡†æµ‹è¯•é›†ä¸Šå‡æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¼ºä¹å¯¹åº”æ•°æ®é›†é™åˆ¶äº†å­¦ä¹ ç®—æ³•åœ¨æä½å…‰ç…§åœºæ™¯ï¼ˆç¯å¢ƒç…§åº¦ä½è‡³0.0001 luxï¼‰çš„åº”ç”¨ã€‚</li>
<li>æå‡ºä¸€ç§æ•°æ®åˆæˆæ–¹æ³•ç”Ÿæˆæç«¯ä½å…‰RAWå›¾åƒåŠå…¶sRGBå‚è€ƒçš„å¤§å‹é…å¯¹æ•°æ®é›†â€”â€”ææš—ç¯å¢ƒä¸‹çš„è§†è§‰æ„ŸçŸ¥ï¼ˆSIEï¼‰ã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œå†…åœ¨å»å™ªç‰¹æ€§ï¼Œæå‡ºåŸºäºæ‰©æ•£çš„æ¡†æ¶ç”¨äºRAWå›¾åƒå¢å¼ºã€‚</li>
<li>å¼•å…¥è‡ªé€‚åº”å…‰ç…§æ ¡æ­£æ¨¡å—å’Œè‰²å½©ä¸€è‡´æ€§æŸå¤±ç¡®ä¿å‡†ç¡®æ›å…‰æ ¡æ­£å’Œè‰²å½©æ¢å¤ã€‚</li>
<li>åœ¨æå‡ºçš„SIEDæ•°æ®é›†å’Œå…¬å¼€åŸºå‡†æµ‹è¯•é›†ä¸Šçš„å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•å’Œæ•°æ®é›†å¯ç”¨äºè¯„ä¼°ä½å…‰RAWå›¾åƒå¢å¼ºæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.21132">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4d0eee733273c2f1c72895c096c4414a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82d90ff222464e4b9628118a3208fabe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2dee007b4f1b83b39acd7f6741c0eeba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8d9214ba8203ab33a0c4eb07c9523848.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f6b127c4f7b703ebe304543adbc2a61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc3a4ad2fd49a46e352e4b875766ae02.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Diffusion-based-Adversarial-Identity-Manipulation-for-Facial-Privacy-Protection"><a href="#Diffusion-based-Adversarial-Identity-Manipulation-for-Facial-Privacy-Protection" class="headerlink" title="Diffusion-based Adversarial Identity Manipulation for Facial Privacy   Protection"></a>Diffusion-based Adversarial Identity Manipulation for Facial Privacy   Protection</h2><p><strong>Authors:Liqin Wang, Qianyue Hu, Wei Lu, Xiangyang Luo</strong></p>
<p>The success of face recognition (FR) systems has led to serious privacy concerns due to potential unauthorized surveillance and user tracking on social networks. Existing methods for enhancing privacy fail to generate natural face images that can protect facial privacy. In this paper, we propose diffusion-based adversarial identity manipulation (DiffAIM) to generate natural and highly transferable adversarial faces against malicious FR systems. To be specific, we manipulate facial identity within the low-dimensional latent space of a diffusion model. This involves iteratively injecting gradient-based adversarial identity guidance during the reverse diffusion process, progressively steering the generation toward the desired adversarial faces. The guidance is optimized for identity convergence towards a target while promoting semantic divergence from the source, facilitating effective impersonation while maintaining visual naturalness. We further incorporate structure-preserving regularization to preserve facial structure consistency during manipulation. Extensive experiments on both face verification and identification tasks demonstrate that compared with the state-of-the-art, DiffAIM achieves stronger black-box attack transferability while maintaining superior visual quality. We also demonstrate the effectiveness of the proposed approach for commercial FR APIs, including Face++ and Aliyun. </p>
<blockquote>
<p>äººè„¸è¯†åˆ«ï¼ˆFRï¼‰ç³»ç»Ÿçš„æˆåŠŸå¼•å‘äº†å¯¹ä¸ªäººéšç§çš„ä¸¥é‡æ‹…å¿§ï¼Œå› ä¸ºæœ‰å¯èƒ½å¯¼è‡´æœªç»æˆæƒçš„ç›‘è§†å’Œç¤¾ä¼šç½‘ç»œä¸Šçš„ç”¨æˆ·è¿½è¸ªã€‚ç°æœ‰çš„æé«˜éšç§ä¿æŠ¤çš„æ–¹æ³•æ— æ³•ç”Ÿæˆèƒ½å¤Ÿä¿æŠ¤é¢éƒ¨éšç§çš„è‡ªç„¶é¢éƒ¨å›¾åƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºåŸºäºæ‰©æ•£çš„å¯¹æŠ—æ€§èº«ä»½æ“æ§ï¼ˆDiffAIMï¼‰æ¥ç”Ÿæˆè‡ªç„¶çš„ã€é«˜åº¦å¯è½¬ç§»çš„å¯¹æŠ—æ€§é¢éƒ¨å›¾åƒï¼Œä»¥å¯¹æŠ—æ¶æ„çš„äººè„¸è¯†åˆ«ç³»ç»Ÿã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æ‰©æ•£æ¨¡å‹çš„ä½ç»´æ½œåœ¨ç©ºé—´å†…å¯¹èº«ä»½è¿›è¡Œæ“æ§ã€‚è¿™æ¶‰åŠåœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­è¿­ä»£åœ°æ³¨å…¥åŸºäºæ¢¯åº¦çš„å¯¹æŠ—æ€§èº«ä»½æŒ‡å¯¼ï¼Œé€æ­¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ä»¥äº§ç”Ÿæ‰€éœ€çš„å¯¹æŠ—æ€§é¢éƒ¨å›¾åƒã€‚æŒ‡å¯¼ä¼˜åŒ–æ—¨åœ¨ä½¿èº«ä»½æœå‘ç›®æ ‡æ”¶æ•›ï¼ŒåŒæ—¶ä¿ƒè¿›ä¸æºæ•°æ®çš„è¯­ä¹‰åˆ†æ­§ï¼Œä»¥å®ç°æœ‰æ•ˆçš„ä¼ªè£…ï¼ŒåŒæ—¶ä¿æŒè§†è§‰è‡ªç„¶æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥èå…¥äº†ç»“æ„ä¿æŒæ­£åˆ™åŒ–ï¼Œä»¥ä¿æŒé¢éƒ¨ç»“æ„çš„ä¸€è‡´æ€§åœ¨æ“æ§è¿‡ç¨‹ä¸­ã€‚å¯¹é¢éƒ¨éªŒè¯å’Œè¯†åˆ«ä»»åŠ¡çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼ŒDiffAIMå®ç°äº†æ›´å¼ºçš„é»‘ç›’æ”»å‡»è½¬ç§»èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº†ä¼˜è¶Šçš„è§†è§‰è´¨é‡ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†æ‰€æå‡ºæ–¹æ³•åœ¨åŒ…æ‹¬Face++å’Œé˜¿é‡Œäº‘åœ¨å†…å•†ç”¨FR APIä¸Šçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.21646v3">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>äººè„¸è¯†åˆ«ï¼ˆFRï¼‰ç³»ç»Ÿçš„æˆåŠŸå¼•å‘äº†å…³äºéšç§çš„æ‹…å¿§ï¼Œå› ä¸ºå¯èƒ½å­˜åœ¨çš„æœªç»æˆæƒçš„ç›‘æ§å’Œç”¨æˆ·ç¤¾äº¤ç½‘ç»œçš„è¿½è¸ªè¡Œä¸ºã€‚ç°æœ‰çš„éšç§ä¿æŠ¤æ–¹æ³•æ— æ³•ç”Ÿæˆèƒ½ä¿æŠ¤é¢éƒ¨éšç§çš„è‡ªç„¶é¢éƒ¨å›¾åƒã€‚æœ¬æ–‡æå‡ºäº†åŸºäºæ‰©æ•£å¯¹æŠ—èº«ä»½æ“æ§ï¼ˆDiffAIMï¼‰çš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆè‡ªç„¶ä¸”é«˜åº¦å¯è¿ç§»çš„å¯¹æŠ—é¢éƒ¨å›¾åƒï¼Œä»¥å¯¹æŠ—æ¶æ„FRç³»ç»Ÿã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨æ‰©æ•£æ¨¡å‹çš„ä½ç»´æ½œåœ¨ç©ºé—´å†…æ“çºµé¢éƒ¨èº«ä»½ã€‚è¿™æ¶‰åŠåœ¨åå‘æ‰©æ•£è¿‡ç¨‹ä¸­è¿­ä»£æ³¨å…¥åŸºäºæ¢¯åº¦çš„å¯¹æŠ—èº«ä»½æŒ‡å¯¼ï¼Œé€æ­¥å¼•å¯¼ç”Ÿæˆèµ°å‘æœŸæœ›çš„å¯¹æŠ—é¢éƒ¨ã€‚æŒ‡å¯¼ä¼˜åŒ–æ—¨åœ¨ä½¿èº«ä»½æ”¶æ•›äºç›®æ ‡ï¼ŒåŒæ—¶ä¿ƒè¿›ä¸æºä¹‹é—´çš„è¯­ä¹‰åˆ†æ­§ï¼Œä»¥å®ç°æœ‰æ•ˆçš„ä¼ªè£…ï¼ŒåŒæ—¶ä¿æŒè§†è§‰è‡ªç„¶æ€§ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥èå…¥äº†ç»“æ„ä¿æŒæ­£åˆ™åŒ–ï¼Œä»¥ä¿æŒé¢éƒ¨ç»“æ„åœ¨æ“æ§è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ã€‚å¯¹é¢éƒ¨éªŒè¯å’Œè¯†åˆ«ä»»åŠ¡çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼ŒDiffAIMå®ç°äº†æ›´å¼ºçš„é»‘ç›’æ”»å‡»è¿ç§»æ€§ï¼ŒåŒæ—¶ä¿æŒäº†å“è¶Šçš„å¯è§†è´¨é‡ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†è¯¥æ–¹æ³•å¯¹åŒ…æ‹¬Face++å’Œé˜¿é‡Œäº‘åœ¨å†…çš„å•†ä¸šFR APIçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>äººè„¸è¯†åˆ«ç³»ç»Ÿçš„æˆåŠŸå¼•å‘äº†å…³äºéšç§ä¿æŠ¤çš„ä¸¥é‡å…³åˆ‡ã€‚</li>
<li>ç°æœ‰éšç§å¢å¼ºæ–¹æ³•æ— æ³•ç”Ÿæˆè‡ªç„¶çš„é¢éƒ¨å›¾åƒä»¥ä¿æŠ¤é¢éƒ¨éšç§ã€‚</li>
<li>æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„å¯¹æŠ—èº«ä»½æ“æ§ï¼ˆDiffAIMï¼‰æ–¹æ³•ã€‚</li>
<li>åœ¨ä½ç»´æ½œåœ¨ç©ºé—´å†…æ“æ§é¢éƒ¨èº«ä»½ï¼Œé€šè¿‡åå‘æ‰©æ•£è¿‡ç¨‹é€æ­¥ç”Ÿæˆå¯¹æŠ—é¢éƒ¨å›¾åƒã€‚</li>
<li>é€šè¿‡ä¼˜åŒ–æŒ‡å¯¼å®ç°èº«ä»½æ”¶æ•›ä¸è¯­ä¹‰åˆ†æ­§çš„å¹³è¡¡ï¼Œç¡®ä¿è‡ªç„¶æ€§å’Œæœ‰æ•ˆæ€§ã€‚</li>
<li>èå…¥ç»“æ„ä¿æŒæ­£åˆ™åŒ–ï¼Œä¿æŒé¢éƒ¨ç»“æ„ä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒDiffAIMç›¸æ¯”æœ€æ–°æŠ€æœ¯ï¼Œåœ¨é¢éƒ¨éªŒè¯å’Œè¯†åˆ«ä»»åŠ¡ä¸Šå®ç°äº†æ›´å¼ºçš„é»‘ç›’æ”»å‡»è¿ç§»æ€§ï¼Œå¹¶ä¿æŒä¼˜ç§€è§†è§‰è´¨é‡ï¼Œå¯¹å•†ä¸šFR APIæœ‰æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.21646">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fa849968e3526929fdf77a9a7b2def18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-272a7669fee5045433f717aef13777e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-577fc993560b251244b4b6e8dd16d79f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52818510866ad8d3710bff8fd9120c69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-63ae440f5725af946f1b2bdbc3940d76.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d305d7a30c98393dc905dba094c12643.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="VistaDepth-Frequency-Modulation-with-Bias-Reweighting-for-Enhanced-Far-range-Depth-Estimation"><a href="#VistaDepth-Frequency-Modulation-with-Bias-Reweighting-for-Enhanced-Far-range-Depth-Estimation" class="headerlink" title="VistaDepth: Frequency Modulation with Bias Reweighting for Enhanced   Far-range Depth Estimation"></a>VistaDepth: Frequency Modulation with Bias Reweighting for Enhanced   Far-range Depth Estimation</h2><p><strong>Authors:Mingxia Zhan, Li Zhang, Xiaomeng Chu, Beibei Wang, Yanyong Zhang</strong></p>
<p>Monocular depth estimation predicts per-pixel depth from a single RGB image. While recent methods have shown promise by leveraging diffusion models, they often struggle to accurately reconstruct far-range regions. This difficulty stems from two compounding factors. First, the standard spatially uniform diffusion objective fails to adapt to the varying frequency content across a depth map. Second, the long-tail depth distribution heavily biases models toward near-range regions. To address these limitations, we introduce VistaDepth, a novel framework named for its ability to accurately reconstruct far-range vistas, which integrates adaptive frequency-domain feature processing with an adaptive loss-balancing mechanism into the diffusion pipeline. Central to our approach is the Latent Frequency Modulation module, which dynamically refines spectral responses in the latent feature space, effectively preserving structural detail. Additionally, we introduce BiasMap, a mechanism that applies adaptive weights directly to the diffusion loss in the latent space, focusing supervision on under-represented far-range regions. These innovations collectively achieve superior depth perception performance across near- and far-range depths while preserving fine detail. Experiments show that VistaDepth achieves state-of-the-art performance for diffusion-based MDE, particularly excelling in reconstructing detailed and accurate depth in far-range regions. </p>
<blockquote>
<p>å•çœ¼æ·±åº¦ä¼°è®¡æ˜¯ä»å•ä¸€RGBå›¾åƒé¢„æµ‹æ¯ä¸ªåƒç´ çš„æ·±åº¦ã€‚è™½ç„¶æœ€è¿‘çš„æ–¹æ³•åˆ©ç”¨æ‰©æ•£æ¨¡å‹æ˜¾ç¤ºå‡ºäº†ä¸€å®šçš„å‰æ™¯ï¼Œä½†å®ƒä»¬å¾€å¾€éš¾ä»¥å‡†ç¡®é‡å»ºè¿œè·ç¦»åŒºåŸŸã€‚è¿™ç§å›°éš¾æºäºä¸¤ä¸ªç›¸äº’å…³è”çš„å› ç´ ã€‚é¦–å…ˆï¼Œæ ‡å‡†çš„ç©ºé—´å‡åŒ€æ‰©æ•£ç›®æ ‡ä¸èƒ½é€‚åº”æ·±åº¦å›¾çš„ä¸åŒé¢‘ç‡å†…å®¹ã€‚å…¶æ¬¡ï¼Œé•¿å°¾å·´æ·±åº¦åˆ†å¸ƒä¸¥é‡ä½¿æ¨¡å‹åå‘äºè¿‘è·ç¦»åŒºåŸŸã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†VistaDepthï¼Œè¿™æ˜¯ä¸€ä¸ªåä¸ºè¿œè§†é‡å»ºçš„æ–°æ¡†æ¶ï¼Œå®ƒå°†è‡ªé€‚åº”é¢‘åŸŸç‰¹å¾å¤„ç†ä¸è‡ªé€‚åº”æŸå¤±å¹³è¡¡æœºåˆ¶é›†æˆåˆ°æ‰©æ•£ç®¡é“ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯æ½œåœ¨é¢‘ç‡è°ƒåˆ¶æ¨¡å—ï¼Œå®ƒåŠ¨æ€åœ°ä¼˜åŒ–æ½œåœ¨ç‰¹å¾ç©ºé—´ä¸­çš„å…‰è°±å“åº”ï¼Œæœ‰æ•ˆåœ°ä¿ç•™ç»“æ„ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†BiasMapæœºåˆ¶ï¼Œè¯¥æœºåˆ¶ç›´æ¥åœ¨æ½œåœ¨ç©ºé—´ä¸­å¯¹æ‰©æ•£æŸå¤±åº”ç”¨è‡ªé€‚åº”æƒé‡ï¼Œå°†ç›‘ç£é‡ç‚¹æ”¾åœ¨è¡¨ç¤ºä¸è¶³çš„è¿œè·ç¦»åŒºåŸŸã€‚è¿™äº›åˆ›æ–°å…±åŒå®ç°äº†å“è¶Šçš„æ·±åº¦æ„ŸçŸ¥æ€§èƒ½ï¼Œæ¶µç›–è¿‘è·å’Œè¿œè·ç¦»æ·±åº¦ï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†ç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼ŒVistaDepthåœ¨åŸºäºæ‰©æ•£çš„MDEä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡å»ºè¿œè·ç¦»åŒºåŸŸçš„è¯¦ç»†å’Œå‡†ç¡®æ·±åº¦æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15095v4">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå•ç›®æ·±åº¦ä¼°è®¡çš„å•RGBå›¾åƒåƒç´ çº§æ·±åº¦é¢„æµ‹æŠ€æœ¯åœ¨æœ€è¿‘é‡‡ç”¨æ‰©æ•£æ¨¡å‹çš„æ–¹æ³•ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†é‡å»ºè¿œè·ç¦»åŒºåŸŸæ—¶ä»å­˜åœ¨å›°éš¾ã€‚ä¸»è¦é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šæ ‡å‡†ç©ºé—´å‡åŒ€æ‰©æ•£ç›®æ ‡æ— æ³•é€‚åº”æ·±åº¦å›¾ä¸­ä¸åŒçš„é¢‘ç‡å†…å®¹ï¼›æ·±åº¦åˆ†å¸ƒçš„é•¿å°¾ç°è±¡å¯¼è‡´æ¨¡å‹ä¸¥é‡åå‘äºè¿‘è·ç¦»åŒºåŸŸã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºVistaDepthæ¡†æ¶ï¼Œé€šè¿‡è‡ªé€‚åº”é¢‘åŸŸç‰¹å¾å¤„ç†å’Œè‡ªé€‚åº”æŸå¤±å¹³è¡¡æœºåˆ¶æå‡æ‰©æ•£ç®¡é“çš„æ€§èƒ½ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬æ½œåœ¨é¢‘ç‡è°ƒåˆ¶æ¨¡å—å’ŒBiasMapæœºåˆ¶ï¼Œå‰è€…åŠ¨æ€ä¼˜åŒ–æ½œåœ¨ç‰¹å¾ç©ºé—´çš„é¢‘è°±å“åº”ï¼Œæœ‰æ•ˆä¿ç•™ç»“æ„ç»†èŠ‚ï¼›åè€…ç›´æ¥å¯¹æ‰©æ•£æŸå¤±æ–½åŠ è‡ªé€‚åº”æƒé‡ï¼Œé‡ç‚¹ç›‘ç£è¿œè·ç¦»åŒºåŸŸçš„è¡¨ç¤ºä¸è¶³é—®é¢˜ã€‚ç»¼åˆåº”ç”¨è¿™äº›åˆ›æ–°æŠ€æœ¯ï¼ŒVistaDepthåœ¨è¿‘è·ç¦»å’Œè¿œè·ç¦»æ·±åº¦æ„ŸçŸ¥æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶ä¿ç•™ç²¾ç»†ç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼ŒVistaDepthåœ¨åŸºäºæ‰©æ•£çš„MDEä¸­è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œå°¤å…¶åœ¨é‡å»ºè¿œè·ç¦»åŒºåŸŸçš„è¯¦ç»†å’Œå‡†ç¡®æ·±åº¦æ–¹é¢è¡¨ç°çªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å•ç›®æ·±åº¦ä¼°è®¡é€šè¿‡å•RGBå›¾åƒè¿›è¡Œåƒç´ çº§æ·±åº¦é¢„æµ‹ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è™½å€ŸåŠ©æ‰©æ•£æ¨¡å‹å±•ç°æ½œåŠ›ï¼Œä½†åœ¨é‡å»ºè¿œè·ç¦»åŒºåŸŸæ—¶é‡åˆ°å›°éš¾ã€‚</li>
<li>ä¸»è¦æŒ‘æˆ˜åœ¨äºæ ‡å‡†ç©ºé—´å‡åŒ€æ‰©æ•£ç›®æ ‡æ— æ³•é€‚åº”æ·±åº¦å›¾ä¸­çš„é¢‘ç‡å˜åŒ–å’Œæ·±åº¦åˆ†å¸ƒçš„é•¿å°¾ç°è±¡ã€‚</li>
<li>VistaDepthæ¡†æ¶é€šè¿‡è‡ªé€‚åº”é¢‘åŸŸç‰¹å¾å¤„ç†å’ŒæŸå¤±å¹³è¡¡æœºåˆ¶è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æ½œåœ¨é¢‘ç‡è°ƒåˆ¶æ¨¡å—åŠ¨æ€ä¼˜åŒ–æ½œåœ¨ç‰¹å¾ç©ºé—´çš„é¢‘è°±å“åº”ï¼Œä¿ç•™ç»“æ„ç»†èŠ‚ã€‚</li>
<li>BiasMapæœºåˆ¶å¯¹æ‰©æ•£æŸå¤±æ–½åŠ è‡ªé€‚åº”æƒé‡ï¼Œå¼ºåŒ–è¿œè·ç¦»åŒºåŸŸçš„è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1e78ef3e5e48738088d828bf36a85baf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-936606aa1df2b650c2abe654e5400e5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f75f8b3e00baf6914855b89bda6f9275.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Learning-to-Align-and-Refine-A-Foundation-to-Diffusion-Framework-for-Occlusion-Robust-Two-Hand-Reconstruction"><a href="#Learning-to-Align-and-Refine-A-Foundation-to-Diffusion-Framework-for-Occlusion-Robust-Two-Hand-Reconstruction" class="headerlink" title="Learning to Align and Refine: A Foundation-to-Diffusion Framework for   Occlusion-Robust Two-Hand Reconstruction"></a>Learning to Align and Refine: A Foundation-to-Diffusion Framework for   Occlusion-Robust Two-Hand Reconstruction</h2><p><strong>Authors:Gaoge Han, Yongkang Cheng, Zhe Chen, Shaoli Huang, Tongliang Liu</strong></p>
<p>Two-hand reconstruction from monocular images faces persistent challenges due to complex and dynamic hand postures and occlusions, causing significant difficulty in achieving plausible interaction alignment. Existing approaches struggle with such alignment issues, often resulting in misalignment and penetration artifacts. To tackle this, we propose a dual-stage Foundation-to-Diffusion framework that precisely align 2D prior guidance from vision foundation models and diffusion-based generative 3D interaction refinement to achieve occlusion-robust two-hand reconstruction. First, we introduce a lightweight fusion alignment encoder that aligns fused multimodal 2D priors like key points, segmentation maps, and depth cues from vision foundation models during training. This provides robust structured guidance, further enabling efficient inference without heavy foundation model encoders at test time while maintaining high reconstruction accuracy. Second, we implement a two-hand diffusion model explicitly trained to convert interpenetrated 3D poses into plausible, penetration-free counterparts. Through collision gradient-guided denoising, the model rectifies artifacts while preserving natural spatial relationships between hands. Extensive evaluations demonstrate that our method achieves state-of-the-art performance on InterHand2.6M, HIC, and FreiHAND datasets, significantly advancing occlusion handling and interaction robustness. Our code will be publicly released. </p>
<blockquote>
<p>ä»å•ç›®å›¾åƒè¿›è¡ŒåŒæ‰‹é‡å»ºé¢ä¸´ç€æŒä¹…çš„æŒ‘æˆ˜ï¼Œç”±äºæ‰‹åŠ¿å¤æ‚ã€åŠ¨æ€ä¸”å¤šå˜ï¼Œä»¥åŠé®æŒ¡é—®é¢˜ï¼Œå¯¼è‡´å®ç°åˆç†çš„äº¤äº’å¯¹é½å˜å¾—éå¸¸å›°éš¾ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™ç±»å¯¹é½é—®é¢˜æ—¶å¾€å¾€åŠ›ä¸ä»å¿ƒï¼Œç»å¸¸å‡ºç°é”™ä½å’Œç©¿é€ä¼ªå½±ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µçš„Foundation-to-Diffusionæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç²¾ç¡®åœ°å¯¹é½æ¥è‡ªè§†è§‰åŸºç¡€æ¨¡å‹çš„2Då…ˆéªŒæŒ‡å¯¼å’ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆå¼3Däº¤äº’ç»†åŒ–ï¼Œä»¥å®ç°é®æŒ¡ç¨³å¥çš„åŒæ‰‹é‡å»ºã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è½»é‡çº§çš„èåˆå¯¹é½ç¼–ç å™¨ï¼Œå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹é½èåˆå¤šæ¨¡æ€çš„2Då…ˆéªŒï¼Œå¦‚å…³é”®ç‚¹ã€åˆ†å‰²å›¾å’Œæ·±åº¦çº¿ç´¢ç­‰ï¼Œè¿™äº›æ¥è‡ªè§†è§‰åŸºç¡€æ¨¡å‹çš„å…ˆéªŒä¿¡æ¯ã€‚è¿™æä¾›äº†ç¨³å¥çš„ç»“æ„åŒ–æŒ‡å¯¼ï¼Œè¿›ä¸€æ­¥ä½¿æµ‹è¯•æ—¶çš„æ¨ç†æ›´é«˜æ•ˆï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„é‡å»ºç²¾åº¦ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªåŒæ‰‹æ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»è¿‡ä¸“é—¨è®­ç»ƒï¼Œèƒ½å¤Ÿå°†ç›¸äº’ç©¿é€çš„3Då§¿åŠ¿è½¬æ¢ä¸ºåˆç†çš„ã€æ— ç©¿é€çš„å¯¹åº”å§¿åŠ¿ã€‚é€šè¿‡ç¢°æ’æ¢¯åº¦å¼•å¯¼çš„é™å™ªï¼Œè¯¥æ¨¡å‹åœ¨ä¿®æ­£ä¼ªå½±çš„åŒæ—¶ï¼Œä¿ç•™äº†åŒæ‰‹ä¹‹é—´çš„è‡ªç„¶ç©ºé—´å…³ç³»ã€‚åœ¨InterHand2.6Mã€HICå’ŒFreiHANDæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œåœ¨é®æŒ¡å¤„ç†å’Œäº¤äº’ç¨³å¥æ€§æ–¹é¢æœ‰äº†æ˜¾è‘—çš„æå‡ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17788v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>å•ç›®å›¾åƒçš„ä¸¤æ‰‹é‡å»ºé¢ä¸´è¯¸å¤šæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ‰‹éƒ¨å§¿æ€çš„å¤æ‚æ€§å’ŒåŠ¨æ€å˜åŒ–ä»¥åŠé®æŒ¡é—®é¢˜ï¼Œå¯¼è‡´äº’åŠ¨å¯¹é½éš¾ä»¥å®ç°ã€‚ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è¿™ç±»å¯¹é½é—®é¢˜æ—¶å¾€å¾€å‡ºç°è¯¯å¯¹é½å’Œç©¿é€ä¼ªå½±ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„Foundation-to-Diffusionæ¡†æ¶ï¼Œé€šè¿‡ç²¾ç¡®å¯¹é½æ¥è‡ªè§†è§‰åŸºç¡€æ¨¡å‹çš„2Då…ˆéªŒæŒ‡å¯¼å’ŒåŸºäºæ‰©æ•£çš„ç”Ÿæˆå¼3Däº’åŠ¨ä¼˜åŒ–ï¼Œå®ç°é®æŒ¡é²æ£’çš„ä¸¤æ‰‹é‡å»ºã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„èåˆå¯¹é½ç¼–ç å™¨ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹é½èåˆçš„å¤šæ¨¡æ€2Då…ˆéªŒï¼Œå¦‚å…³é”®ç‚¹ã€åˆ†å‰²å›¾å’Œæ·±åº¦çº¿ç´¢ç­‰ï¼Œæ¥è‡ªè§†è§‰åŸºç¡€æ¨¡å‹ã€‚è¿™æä¾›äº†ç¨³å¥çš„ç»“æ„åŒ–æŒ‡å¯¼ï¼Œè¿›ä¸€æ­¥å®ç°äº†æµ‹è¯•æ—¶çš„é«˜æ•ˆæ¨ç†ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„é‡å»ºç²¾åº¦ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªä¸“é—¨ç”¨äºå°†ç›¸äº’ç©¿é€çš„3Då§¿åŠ¿è½¬æ¢ä¸ºåˆç†ã€æ— ç©¿é€å¯¹åº”å§¿åŠ¿çš„ä¸¤æ‰‹æ‰©æ•£æ¨¡å‹ã€‚é€šè¿‡ç¢°æ’æ¢¯åº¦å¼•å¯¼çš„é™å™ªï¼Œè¯¥æ¨¡å‹åœ¨ä¿®æ­£ä¼ªå½±çš„åŒæ—¶ï¼Œä¿ç•™äº†æ‰‹éƒ¨ä¹‹é—´çš„è‡ªç„¶ç©ºé—´å…³ç³»ã€‚å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨InterHand2.6Mã€HICå’ŒFreiHANDæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼Œæ˜¾è‘—æé«˜äº†é®æŒ¡å¤„ç†å’Œäº’åŠ¨ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„Foundation-to-Diffusionæ¡†æ¶ï¼Œç”¨äºä¸¤æ‰‹é‡å»ºä»»åŠ¡ã€‚</li>
<li>å¼•å…¥è½»é‡çº§èåˆå¯¹é½ç¼–ç å™¨ï¼Œå¯¹é½æ¥è‡ªè§†è§‰åŸºç¡€æ¨¡å‹çš„2Då…ˆéªŒæŒ‡å¯¼ã€‚</li>
<li>å®ç°äº†ä¸€ä¸ªä¸¤æ‰‹æ‰©æ•£æ¨¡å‹ï¼Œèƒ½å¤Ÿè½¬æ¢ç›¸äº’ç©¿é€çš„3Då§¿åŠ¿ä¸ºæ— ç©¿é€çš„å¯¹åº”å§¿åŠ¿ã€‚</li>
<li>é€šè¿‡ç¢°æ’æ¢¯åº¦å¼•å¯¼çš„é™å™ªï¼Œæ¨¡å‹èƒ½å¤Ÿä¿®æ­£ä¼ªå½±å¹¶ä¿ç•™æ‰‹éƒ¨é—´çš„è‡ªç„¶ç©ºé—´å…³ç³»ã€‚</li>
<li>æ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ï¼ŒåŒ…æ‹¬InterHand2.6Mã€HICå’ŒFreiHANDã€‚</li>
<li>æ¡†æ¶å…·æœ‰é®æŒ¡å¤„ç†å’Œäº’åŠ¨ç¨³å¥æ€§ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17788">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dffb7421b9c44bcc78c3ed5c51985225.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b36c3c7608b8fa348605128af3c6bedf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b50481b0514b30abaf31f645ba45de74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47a877e8be222fac6061742625f2f759.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-85a44268f989da37344b06a1df760c17.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0b4c17d0d9eaed3026acfad0b7f895ce.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e0aa87973b8e89f7287f66e505ae9a2f.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  PerioDet Large-Scale Panoramic Radiograph Benchmark for   Clinical-Oriented Apical Periodontitis Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-25fa6963933556854a815a540ef9bf76.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  NeRF Is a Valuable Assistant for 3D Gaussian Splatting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
