<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="è§†é¢‘ç†è§£">
    <meta name="description" content="è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  Towards Video Thinking Test A Holistic Benchmark for Advanced Video   Reasoning and Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>è§†é¢‘ç†è§£ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-46c694119abd21be5efe2a3d91cd6519.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">è§†é¢‘ç†è§£</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                è§†é¢‘ç†è§£
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    11.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    45 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-01-æ›´æ–°"><a href="#2025-08-01-æ›´æ–°" class="headerlink" title="2025-08-01 æ›´æ–°"></a>2025-08-01 æ›´æ–°</h1><h2 id="Towards-Video-Thinking-Test-A-Holistic-Benchmark-for-Advanced-Video-Reasoning-and-Understanding"><a href="#Towards-Video-Thinking-Test-A-Holistic-Benchmark-for-Advanced-Video-Reasoning-and-Understanding" class="headerlink" title="Towards Video Thinking Test: A Holistic Benchmark for Advanced Video   Reasoning and Understanding"></a>Towards Video Thinking Test: A Holistic Benchmark for Advanced Video   Reasoning and Understanding</h2><p><strong>Authors:Yuanhan Zhang, Yunice Chew, Yuhao Dong, Aria Leo, Bo Hu, Ziwei Liu</strong></p>
<p>Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video large language models (video LLMs), existing benchmarks inadequately reflect the gap between these models and human intelligence in maintaining correctness and robustness in video interpretation. We introduce the Video Thinking Test (Video-TT), to assess if video LLMs can interpret real-world videos as effectively as humans. Video-TT reflects genuine gaps in understanding complex visual narratives, and evaluates robustness against natural adversarial questions. Video-TT comprises 1,000 YouTube Shorts videos, each with one open-ended question and four adversarial questions that probe visual and narrative complexity. Our evaluation shows a significant gap between video LLMs and human performance. </p>
<blockquote>
<p>äººç±»çš„æ™ºèƒ½éœ€è¦æ­£ç¡®æ€§å’Œç¨³å¥æ€§ï¼Œå‰è€…æ˜¯åè€…çš„åŸºç¡€ã€‚åœ¨è§†é¢‘ç†è§£ä¸­ï¼Œæ­£ç¡®æ€§ç¡®ä¿å¯¹è§†è§‰å†…å®¹çš„å‡†ç¡®è§£é‡Šï¼Œè€Œç¨³å¥æ€§åˆ™ç»´æŒäº†åœ¨æŒ‘æˆ˜æ¡ä»¶ä¸‹çš„ç¨³å®šè¡¨ç°ã€‚å°½ç®¡è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo LLMsï¼‰æœ‰æ‰€è¿›å±•ï¼Œä½†ç°æœ‰åŸºå‡†æµ‹è¯•å¹¶æœªå……åˆ†åæ˜ å‡ºè¿™äº›æ¨¡å‹åœ¨ç»´æŒè§†é¢‘è§£è¯»ä¸­çš„æ­£ç¡®æ€§å’Œç¨³å¥æ€§ä¸äººç±»æ™ºèƒ½ä¹‹é—´çš„å·®è·ã€‚æˆ‘ä»¬æ¨å‡ºè§†é¢‘æ€ç»´æµ‹è¯•ï¼ˆVideo-TTï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°è§†é¢‘LLMsæ˜¯å¦èƒ½åƒäººç±»ä¸€æ ·æœ‰æ•ˆåœ°è§£è¯»ç°å®ä¸–ç•Œçš„è§†é¢‘ã€‚Video-TTåæ˜ äº†åœ¨ç†è§£å¤æ‚è§†è§‰å™äº‹æ–¹é¢çš„çœŸå®å·®è·ï¼Œå¹¶è¯„ä¼°äº†é¢å¯¹è‡ªç„¶å¯¹æŠ—æ€§é—®é¢˜æ—¶çš„ç¨³å¥æ€§ã€‚Video-TTåŒ…å«1000ä¸ªYouTubeçŸ­è§†é¢‘ï¼Œæ¯ä¸ªè§†é¢‘éƒ½æœ‰ä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜ä»¥åŠå››ä¸ªé’ˆå¯¹è§†è§‰å’Œå™äº‹å¤æ‚æ€§çš„å¯¹æŠ—æ€§é—®é¢˜ã€‚æˆ‘ä»¬çš„è¯„ä¼°æ˜¾ç¤ºï¼Œè§†é¢‘LLMsä¸äººç±»æ€§èƒ½ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15028v1">PDF</a> ICCV 2025; Project page: <a target="_blank" rel="noopener" href="https://zhangyuanhan-ai.github.io/video-tt/">https://zhangyuanhan-ai.github.io/video-tt/</a></p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘ç†è§£é¢†åŸŸä¸­ï¼Œäººç±»æ™ºèƒ½éœ€è¦æ­£ç¡®æ€§ä¸ç¨³å¥æ€§ï¼Œå…¶ä¸­æ­£ç¡®æ€§ä¸ºåŸºç¡€ã€‚åœ¨è§†é¢‘ç†è§£ä¸­ï¼Œæ­£ç¡®æ€§ç¡®ä¿è§†è§‰å†…å®¹çš„å‡†ç¡®è§£è¯»ï¼Œè€Œç¨³å¥æ€§åˆ™ç»´æŒäº†åœ¨å¤æ‚ç¯å¢ƒä¸‹çš„ç¨³å®šè¡¨ç°ã€‚å°½ç®¡è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆvideo LLMsï¼‰æœ‰æ‰€å‘å±•ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•å¹¶ä¸èƒ½å……åˆ†åæ˜ è¿™äº›æ¨¡å‹åœ¨ç»´æŒè§†é¢‘è§£è¯»çš„æ­£ç¡®æ€§ä¸ç¨³å¥æ€§æ–¹é¢ä¸äººç±»æ™ºèƒ½ä¹‹é—´çš„å·®è·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºè§†é¢‘æ€ç»´æµ‹è¯•ï¼ˆVideo-TTï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°è§†é¢‘LLMsæ˜¯å¦å¦‚äººç±»èˆ¬æœ‰æ•ˆåœ°è§£è¯»ç°å®ä¸–ç•Œçš„è§†é¢‘ã€‚Video-TTåæ˜ äº†åœ¨ç†è§£å¤æ‚è§†è§‰å™äº‹æ–¹é¢çš„çœŸå®å·®è·ï¼Œå¹¶è¯„ä¼°äº†é¢å¯¹è‡ªç„¶å¯¹æŠ—æ€§é—®é¢˜çš„ç¨³å¥æ€§ã€‚è¯¥æµ‹è¯•åŒ…å«1000ä¸ªYouTubeçŸ­è§†é¢‘ï¼Œæ¯ä¸ªè§†é¢‘é™„å¸¦ä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜åŠå››ä¸ªé’ˆå¯¹è§†è§‰å’Œå™äº‹å¤æ‚æ€§çš„å¯¹æŠ—æ€§é—®é¢˜ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºè§†é¢‘LLMsä¸äººç±»æ€§èƒ½ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»æ™ºèƒ½åœ¨è§†é¢‘ç†è§£ä¸­éœ€è¦æ­£ç¡®æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•æœªèƒ½å……åˆ†åæ˜ è§†é¢‘LLMsä¸äººç±»åœ¨è§†é¢‘ç†è§£ä¸Šçš„å·®è·ã€‚</li>
<li>æ¨å‡ºè§†é¢‘æ€ç»´æµ‹è¯•ï¼ˆVideo-TTï¼‰ï¼Œä»¥è¯„ä¼°è§†é¢‘LLMsçš„è§£è¯»èƒ½åŠ›ã€‚</li>
<li>Video-TTåŒ…å«1000ä¸ªçŸ­è§†é¢‘ï¼Œæ¯ä¸ªè§†é¢‘é™„å¸¦ä¸€ä¸ªå¼€æ”¾æ€§é—®é¢˜åŠå››ä¸ªå¯¹æŠ—æ€§é—®é¢˜ã€‚</li>
<li>Video-TTæ—¨åœ¨æµ‹è¯•æ¨¡å‹åœ¨ç†è§£å¤æ‚è§†è§‰å™äº‹å’Œé¢å¯¹å¯¹æŠ—æ€§é—®é¢˜æ—¶çš„è¡¨ç°ã€‚</li>
<li>è§†é¢‘LLMsåœ¨è§†é¢‘ç†è§£æ–¹é¢ä¸äººç±»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15028">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3857dfd91ef9e7de3aea4b189c68621f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-156dc4247e7de063839e214331f86807.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e569d1aa3636f69d8f87b0a0a7a5986.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a20801336ec0cb8d807aa69ca9f6a9fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea9e6a0b0cada6842fcd7bcd5dabf696.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b1bc72c7ac32a240648d02aa556339e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VideoITG-Multimodal-Video-Understanding-with-Instructed-Temporal-Grounding"><a href="#VideoITG-Multimodal-Video-Understanding-with-Instructed-Temporal-Grounding" class="headerlink" title="VideoITG: Multimodal Video Understanding with Instructed Temporal   Grounding"></a>VideoITG: Multimodal Video Understanding with Instructed Temporal   Grounding</h2><p><strong>Authors:Shihao Wang, Guo Chen, De-an Huang, Zhiqi Li, Minghan Li, Guilin Li, Jose M. Alvarez, Lei Zhang, Zhiding Yu</strong></p>
<p>Recent studies have revealed that selecting informative and relevant video frames can significantly improve the performance of Video Large Language Models (Video-LLMs). Current methods, such as reducing inter-frame redundancy, employing separate models for image-text relevance assessment, or utilizing temporal video grounding for event localization, substantially adopt unsupervised learning paradigms, whereas they struggle to address the complex scenarios in long video understanding. We propose Instructed Temporal Grounding for Videos (VideoITG), featuring customized frame sampling aligned with user instructions. The core of VideoITG is the VidThinker pipeline, an automated annotation framework that explicitly mimics the human annotation process. First, it generates detailed clip-level captions conditioned on the instruction; then, it retrieves relevant video segments through instruction-guided reasoning; finally, it performs fine-grained frame selection to pinpoint the most informative visual evidence. Leveraging VidThinker, we construct the VideoITG-40K dataset, containing 40K videos and 500K instructed temporal grounding annotations. We then design a plug-and-play VideoITG model, which takes advantage of visual language alignment and reasoning capabilities of Video-LLMs, for effective frame selection in a discriminative manner. Coupled with Video-LLMs, VideoITG achieves consistent performance improvements across multiple multimodal video understanding benchmarks, showing its superiority and great potentials for video understanding. </p>
<blockquote>
<p>è¿‘æœŸç ”ç©¶æŒ‡å‡ºï¼Œé€‰å–å…·æœ‰ä¿¡æ¯é‡å’Œç›¸å…³æ€§çš„è§†é¢‘å¸§ï¼Œèƒ½æ˜¾è‘—æå‡è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMï¼‰çš„æ€§èƒ½ã€‚å½“å‰çš„æ–¹æ³•ï¼Œå¦‚å‡å°‘å¸§é—´å†—ä½™ã€é‡‡ç”¨å•ç‹¬çš„æ¨¡å‹è¿›è¡Œå›¾æ–‡ç›¸å…³æ€§è¯„ä¼°ï¼Œæˆ–ä½¿ç”¨åŸºäºæ—¶é—´çš„è§†é¢‘å®šä½è¿›è¡Œäº‹ä»¶å®šä½ç­‰ï¼Œä¸»è¦é‡‡ç”¨æ— ç›‘ç£å­¦ä¹ æ¨¡å¼ï¼Œéš¾ä»¥åº”å¯¹é•¿è§†é¢‘ç†è§£ä¸­çš„å¤æ‚åœºæ™¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºåŸºäºç”¨æˆ·æŒ‡ä»¤çš„è§†é¢‘æŒ‡ä»¤åŒ–æ—¶é—´å®šä½æ–¹æ³•ï¼ˆVideoITGï¼‰ï¼Œå…·æœ‰è‡ªå®šä¹‰å¸§é‡‡æ ·ä¸ç”¨æˆ·æŒ‡ä»¤å¯¹é½çš„ç‰¹ç‚¹ã€‚VideoITGçš„æ ¸å¿ƒæ˜¯VidThinkerç®¡é“ï¼Œä¸€ä¸ªæ¨¡ä»¿äººç±»æ ‡æ³¨è¿‡ç¨‹çš„è‡ªåŠ¨åŒ–æ ‡æ³¨æ¡†æ¶ã€‚é¦–å…ˆï¼Œå®ƒæ ¹æ®æŒ‡ä»¤ç”Ÿæˆè¯¦ç»†çš„ç‰‡æ®µçº§æè¿°ï¼›ç„¶åï¼Œé€šè¿‡æŒ‡ä»¤å¼•å¯¼æ¨ç†æ£€ç´¢ç›¸å…³è§†é¢‘ç‰‡æ®µï¼›æœ€åï¼Œè¿›è¡Œç²¾ç»†å¸§é€‰æ‹©ï¼Œç¡®å®šæœ€å…·ä¿¡æ¯é‡çš„è§†è§‰è¯æ®ã€‚å€ŸåŠ©VidThinkerï¼Œæˆ‘ä»¬æ„å»ºäº†åŒ…å«4ä¸‡è§†é¢‘å’Œ50ä¸‡ä¸ªæŒ‡ä»¤åŒ–æ—¶é—´å®šä½æ ‡æ³¨çš„VideoITG-40Kæ•°æ®é›†ã€‚æ¥ç€ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€æ¬¾å³æ’å³ç”¨çš„VideoITGæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰è¯­è¨€å¯¹é½å’Œæ¨ç†èƒ½åŠ›ï¼Œä»¥åˆ¤åˆ«æ–¹å¼è¿›è¡Œæœ‰æ•ˆå¸§é€‰æ‹©ã€‚é…åˆè§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ŒVideoITGåœ¨å¤šä¸ªå¤šåª’ä½“è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ€§èƒ½æŒç»­æå‡ï¼Œè¯æ˜äº†å…¶åœ¨è§†é¢‘ç†è§£é¢†åŸŸçš„ä¼˜è¶Šæ€§åŠå·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13353v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹è§†é¢‘çš„æŒ‡ä»¤åŒ–æ—¶é—´å®šä½æ–¹æ³•ï¼ˆVideoITGï¼‰ï¼Œé€šè¿‡æ¨¡ä»¿äººç±»æ ‡æ³¨è¿‡ç¨‹ï¼Œæ„å»ºè‡ªåŠ¨åŒ–æ ‡æ³¨æ¡†æ¶VidThinkerï¼Œç”¨äºç”ŸæˆåŸºäºæŒ‡ä»¤çš„è¯¦ç»†ç‰‡æ®µçº§æè¿°ã€é€šè¿‡æŒ‡ä»¤å¼•å¯¼çš„æ¨ç†ä»¥åŠç²¾ç»†åŒ–çš„å¸§é€‰æ‹©ã€‚åˆ©ç”¨VideoITGï¼Œæ„å»ºVideoITG-40Kæ•°æ®é›†ï¼Œå¹¶è®¾è®¡ä¸€æ¬¾å³æ’å³ç”¨çš„VideoITGæ¨¡å‹ï¼Œç»“åˆè§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMï¼‰çš„è§†è§‰è¯­è¨€å¯¹é½å’Œæ¨ç†èƒ½åŠ›ï¼Œå®ç°æœ‰æ•ˆçš„å¸§é€‰æ‹©ã€‚è¯¥æ–¹æ³•åœ¨å¤šä¸ªå¤šåª’ä½“è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå…·æœ‰å·¨å¤§çš„è§†é¢‘ç†è§£æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸç ”ç©¶å‘ç°é€‰æ‹©æœ‰ä¿¡æ¯é‡å’Œç›¸å…³æ€§çš„è§†é¢‘å¸§èƒ½æ˜¾è‘—æå‡è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMï¼‰æ€§èƒ½ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦é‡‡å–æ— ç›‘ç£å­¦ä¹ æ¨¡å¼ï¼Œä½†åœ¨å¤„ç†é•¿è§†é¢‘å¤æ‚åœºæ™¯æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>æå‡ºçš„VideoITGæ–¹æ³•é€šè¿‡æ¨¡ä»¿äººç±»æ ‡æ³¨è¿‡ç¨‹ï¼Œæ„å»ºè‡ªåŠ¨åŒ–æ ‡æ³¨æ¡†æ¶VidThinkerã€‚</li>
<li>VidThinkeråŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼šåŸºäºæŒ‡ä»¤ç”Ÿæˆè¯¦ç»†ç‰‡æ®µçº§æè¿°ã€é€šè¿‡æŒ‡ä»¤å¼•å¯¼çš„æ¨ç†ä»¥åŠç²¾ç»†åŒ–çš„å¸§é€‰æ‹©ã€‚</li>
<li>åˆ©ç”¨VideoITGæ„å»ºVideoITG-40Kæ•°æ®é›†ï¼ŒåŒ…å«40Kè§†é¢‘å’Œ500KæŒ‡ä»¤åŒ–æ—¶é—´å®šä½æ ‡æ³¨ã€‚</li>
<li>è®¾è®¡çš„VideoITGæ¨¡å‹ç»“åˆVideo-LLMçš„è§†è§‰è¯­è¨€å¯¹é½å’Œæ¨ç†èƒ½åŠ›ï¼Œå®ç°æœ‰æ•ˆçš„å¸§é€‰æ‹©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13353">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81c9d9dec3eb7f6f847e772ef481f6b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a16ec2c16960901e8083fbfa6eef88f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bdcd1464b78e432b41f8525d54ecada.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-467a1197a64f2585b9ad956bde53c5ea.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Watch-Listen-Understand-Mislead-Tri-modal-Adversarial-Attacks-on-Short-Videos-for-Content-Appropriateness-Evaluation"><a href="#Watch-Listen-Understand-Mislead-Tri-modal-Adversarial-Attacks-on-Short-Videos-for-Content-Appropriateness-Evaluation" class="headerlink" title="Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on   Short Videos for Content Appropriateness Evaluation"></a>Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on   Short Videos for Content Appropriateness Evaluation</h2><p><strong>Authors:Sahid Hossain Mustakim, S M Jishanul Islam, Ummay Maria Muna, Montasir Chowdhury, Mohammed Jawwadul Islam, Sadia Ahmmed, Tashfia Sikder, Syed Tasdid Azam Dhrubo, Swakkhar Shatabda</strong></p>
<p>Multimodal Large Language Models (MLLMs) are increasingly used for content moderation, yet their robustness in short-form video contexts remains underexplored. Current safety evaluations often rely on unimodal attacks, failing to address combined attack vulnerabilities. In this paper, we introduce a comprehensive framework for evaluating the tri-modal safety of MLLMs. First, we present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising diverse short-form videos with human-guided synthetic adversarial attacks. Second, we propose ChimeraBreak, a novel tri-modal attack strategy that simultaneously challenges visual, auditory, and semantic reasoning pathways. Extensive experiments on state-of-the-art MLLMs reveal significant vulnerabilities with high Attack Success Rates (ASR). Our findings uncover distinct failure modes, showing model biases toward misclassifying benign or policy-violating content. We assess results using LLM-as-a-judge, demonstrating attack reasoning efficacy. Our dataset and findings provide crucial insights for developing more robust and safe MLLMs. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°ç”¨äºå†…å®¹å®¡æ ¸ï¼Œä½†å…¶åœ¨çŸ­è§†é¢‘ä¸Šä¸‹æ–‡ä¸­çš„ç¨³å¥æ€§ä»ç„¶è¢«å¿½è§†ã€‚å½“å‰çš„å®‰å…¨è¯„ä¼°é€šå¸¸ä¾èµ–äºå•æ¨¡æ€æ”»å‡»ï¼Œæ— æ³•åº”å¯¹ç»„åˆæ”»å‡»æ¼æ´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå…¨é¢æ¡†æ¶ï¼Œä»¥è¯„ä¼°MLLMsçš„ä¸‰æ¨¡æ€å®‰å…¨æ€§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†çŸ­è§†é¢‘å¤šæ¨¡æ€å¯¹æŠ—ï¼ˆSVMAï¼‰æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¤šç§å¸¦æœ‰ç”±äººç±»å¼•å¯¼çš„åˆæˆå¯¹æŠ—æ€§æ”»å‡»çš„çŸ­è§†é¢‘ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸‰æ¨¡æ€æ”»å‡»ç­–ç•¥â€”â€” ChimeraBreakï¼Œå¯ä»¥åŒæ—¶æŒ‘æˆ˜è§†è§‰ã€å¬è§‰å’Œè¯­ä¹‰æ¨ç†è·¯å¾„ã€‚åœ¨æœ€æ–°MLLMsä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜å­˜åœ¨æ˜¾è‘—æ¼æ´ï¼Œæ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰å¾ˆé«˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†ä¸åŒçš„å¤±è´¥æ¨¡å¼ï¼Œæ˜¾ç¤ºå‡ºæ¨¡å‹åå‘äºè¯¯åˆ¤è‰¯æ€§æˆ–è¿åæ”¿ç­–çš„å†…å®¹ã€‚æˆ‘ä»¬ä½¿ç”¨LLMä½œä¸ºæ³•å®˜è¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜äº†æ”»å‡»æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œç ”ç©¶ç»“æœå¯¹äºå¼€å‘æ›´ç¨³å¥å’Œå®‰å…¨çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†å…³é”®è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11968v1">PDF</a> Accepted as long paper, SVU Workshop at ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨çŸ­è§†é¢‘å†…å®¹å®¡æ ¸ä¸­çš„åº”ç”¨åŠå…¶å­˜åœ¨çš„å®‰å…¨é—®é¢˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶æ¥è¯„ä¼°MLLMsçš„ä¸‰æ¨¡æ€å®‰å…¨æ€§ï¼Œå¹¶ä»‹ç»äº†Short-Video Multimodal Adversarialï¼ˆSVMAï¼‰æ•°æ®é›†å’ŒChimeraBreakæ–°å‹ä¸‰æ¨¡æ€æ”»å‡»ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒMLLMså­˜åœ¨æ˜¾è‘—çš„å®‰å…¨æ¼æ´ï¼Œå¯¹è‰¯æ€§æˆ–è¿è§„å†…å®¹çš„è¯¯åˆ¤è¡¨ç°å‡ºæ¨¡å‹åè§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨çŸ­è§†é¢‘å†…å®¹å®¡æ ¸ä¸­çš„ä½¿ç”¨è¶Šæ¥è¶Šæ™®éï¼Œä½†å…¶ç¨³å¥æ€§ä»å¾…æ¢ç´¢ã€‚</li>
<li>å½“å‰çš„å®‰å…¨è¯„ä¼°ä¸»è¦ä¾èµ–å•æ¨¡æ€æ”»å‡»ï¼Œæ— æ³•åº”å¯¹ç»„åˆæ”»å‡»æ¼æ´ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªè¯„ä¼°MLLMsä¸‰æ¨¡æ€å®‰å…¨æ€§çš„å…¨é¢æ¡†æ¶ã€‚</li>
<li>ä»‹ç»äº†Short-Video Multimodal Adversarialï¼ˆSVMAï¼‰æ•°æ®é›†ï¼ŒåŒ…å«å¸¦æœ‰äººå·¥åˆæˆå¯¹æŠ—æ”»å‡»çš„çŸ­è§†é¢‘ã€‚</li>
<li>æå‡ºäº†ChimeraBreakæ–°å‹ä¸‰æ¨¡æ€æ”»å‡»ç­–ç•¥ï¼ŒåŒæ—¶æŒ‘æˆ˜è§†è§‰ã€å¬è§‰å’Œè¯­ä¹‰æ¨ç†è·¯å¾„ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒMLLMså­˜åœ¨æ˜¾è‘—çš„å®‰å…¨æ¼æ´ï¼Œæ”»å‡»æˆåŠŸç‡è¾ƒé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11968">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-34d4c0f5eb4b8eb8e9561989c634116a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-881ae9b9d4272710de7fcd61933ace9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-053153bcb4f20a7517c1a705c94ac0a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7eb0e8cd25d1e8641ce0d554ab57d8ad.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VRU-Accident-A-Vision-Language-Benchmark-for-Video-Question-Answering-and-Dense-Captioning-for-Accident-Scene-Understanding"><a href="#VRU-Accident-A-Vision-Language-Benchmark-for-Video-Question-Answering-and-Dense-Captioning-for-Accident-Scene-Understanding" class="headerlink" title="VRU-Accident: A Vision-Language Benchmark for Video Question Answering   and Dense Captioning for Accident Scene Understanding"></a>VRU-Accident: A Vision-Language Benchmark for Video Question Answering   and Dense Captioning for Accident Scene Understanding</h2><p><strong>Authors:Younggun Kim, Ahmed S. Abdelrahman, Mohamed Abdel-Aty</strong></p>
<p>Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and cyclists, is a critical challenge for autonomous driving systems, as crashes involving VRUs often result in severe or fatal consequences. While multimodal large language models (MLLMs) have shown promise in enhancing scene understanding and decision making in autonomous vehicles, there is currently no standardized benchmark to quantitatively evaluate their reasoning abilities in complex, safety-critical scenarios involving VRUs. To address this gap, we present VRU-Accident, a large-scale vision-language benchmark designed to evaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident comprises 1K real-world dashcam accident videos, annotated with 6K multiple-choice question-answer pairs across six safety-critical categories (with 24K candidate options and 3.4K unique answer choices), as well as 1K dense scene descriptions. Unlike prior works, our benchmark focuses explicitly on VRU-vehicle accidents, providing rich, fine-grained annotations that capture both spatial-temporal dynamics and causal semantics of accidents. To assess the current landscape of MLLMs, we conduct a comprehensive evaluation of 17 state-of-the-art models on the multiple-choice VQA task and on the dense captioning task. Our findings reveal that while MLLMs perform reasonably well on visually grounded attributes, they face significant challenges in reasoning and describing accident causes, types, and preventability. </p>
<blockquote>
<p>ç¡®ä¿è„†å¼±é“è·¯ä½¿ç”¨è€…ï¼ˆå¦‚æ­¥è¡Œè€…å’Œéª‘è‡ªè¡Œè½¦çš„äººï¼‰çš„å®‰å…¨å¯¹äºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿæ¥è¯´æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå› ä¸ºæ¶‰åŠè„†å¼±é“è·¯ä½¿ç”¨è€…çš„äº¤é€šäº‹æ•…å¸¸å¸¸ä¼šå¯¼è‡´ä¸¥é‡æˆ–è‡´å‘½çš„åæœã€‚å°½ç®¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¢å¼ºè‡ªåŠ¨é©¾é©¶åœºæ™¯ç†è§£å’Œå†³ç­–åˆ¶å®šæ–¹é¢è¡¨ç°å‡ºäº†æ½œåŠ›ï¼Œä½†ç›®å‰è¿˜æ²¡æœ‰ä¸€ä¸ªæ ‡å‡†åŒ–çš„åŸºå‡†æ¥å®šé‡è¯„ä¼°å®ƒä»¬åœ¨æ¶‰åŠè„†å¼±é“è·¯ä½¿ç”¨è€…çš„å¤æ‚ã€å®‰å…¨å…³é”®çš„åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VRU-Accidentï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨è¯„ä¼°æ¶‰åŠè„†å¼±é“è·¯ä½¿ç”¨è€…çš„é«˜é£é™©äº¤é€šåœºæ™¯ä¸­çš„MLLMsã€‚VRU-AccidentåŒ…å«1000ä¸ªçœŸå®ä¸–ç•Œçš„è¡Œè½¦è®°å½•ä»ªäº‹æ•…è§†é¢‘ï¼Œè¿™äº›è§†é¢‘è¢«æ ‡æ³¨äº†6000ä¸ªé€‰æ‹©é¢˜ç­”æ¡ˆå¯¹ï¼Œè·¨è¶Šå…­ä¸ªå®‰å…¨å…³é”®ç±»åˆ«ï¼ˆåŒ…å«24000ä¸ªå€™é€‰é€‰é¡¹å’Œ3400ä¸ªç‹¬ç‰¹ç­”æ¡ˆï¼‰ï¼Œä»¥åŠ1000ä¸ªå¯†é›†åœºæ™¯æè¿°ã€‚ä¸ä»¥å‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•æ˜ç¡®åœ°å…³æ³¨äºVRU-è½¦è¾†äº‹æ•…ï¼Œæä¾›ä¸°å¯Œã€ç²¾ç»†çš„æ ‡æ³¨ï¼Œæ•æ‰äº‹æ•…çš„æ—¶ç©ºåŠ¨æ€å’Œå› æœè¯­ä¹‰ã€‚ä¸ºäº†è¯„ä¼°å½“å‰MLLMsçš„çŠ¶å†µï¼Œæˆ‘ä»¬å¯¹17ä¸ªæœ€æ–°æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„å¤šé€‰é¢˜é—®ç­”ä»»åŠ¡å’Œå¯†é›†æè¿°ä»»åŠ¡è¯„ä¼°ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼Œè™½ç„¶MLLMsåœ¨è§†è§‰åŸºç¡€ä¸Šçš„å±æ€§è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ¨ç†å’Œæè¿°äº‹æ•…åŸå› ã€ç±»å‹å’Œå¯é¢„é˜²æ€§æ–¹é¢ä»é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09815v2">PDF</a> 22 pages, 11 figures, 5 tables</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹è‡ªä¸»é©¾é©¶ç³»ç»Ÿä¸­ä¿éšœè„†å¼±é“è·¯ä½¿ç”¨è€…ï¼ˆVRUsï¼‰å¦‚è¡ŒäººåŠéª‘è¡Œè€…çš„å®‰å…¨é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºVRU-Accidentå¤§å‹è§†è§‰è¯­è¨€åŸºå‡†æµ‹è¯•ã€‚è¯¥æµ‹è¯•åŒ…å«åƒä½™çœŸå®è¡Œè½¦è®°å½•äº‹æ•…è§†é¢‘ï¼Œæ¶µç›–å…­å¤§å…³é”®å®‰å…¨ç±»åˆ«ï¼ŒåŒ…å«é—®ç­”é…å¯¹åŠå¯†é›†åœºæ™¯æè¿°ç­‰ä¸°å¯Œç²¾ç»†æ ‡æ³¨ã€‚è¯¥åŸºå‡†æµ‹è¯•ä¸“æ³¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¤„ç†æ¶‰åŠVRUsçš„äº‹æ•…æ—¶çš„æ€§èƒ½æŒ‘æˆ˜ï¼Œè¯„ä¼°å…¶æè¿°äº‹æ•…èµ·å› ã€ç±»å‹åŠå¯é¢„é˜²æ€§çš„èƒ½åŠ›ã€‚åˆ†æå‘ç°ï¼Œè™½ç„¶MLLMsåœ¨è§†è§‰å±æ€§æ–¹é¢çš„è¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨äº‹æ•…æ¨ç†å’Œæè¿°æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è‡ªä¸»é©¾é©¶ç³»ç»Ÿé¢ä¸´ä¿éšœè„†å¼±é“è·¯ä½¿ç”¨è€…çš„é‡å¤§æŒ‘æˆ˜ã€‚æ¶‰åŠVRUsçš„äº‹æ•…å¸¸å¯¼è‡´ä¸¥é‡æˆ–è‡´å‘½åæœã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¢å¼ºè‡ªä¸»è½¦è¾†åœºæ™¯ç†è§£å’Œå†³ç­–æ–¹é¢å±•ç°æ½œåŠ›ã€‚</li>
<li>ç›®å‰ç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•æ¥è¯„ä¼°MLLMsåœ¨å¤„ç†æ¶‰åŠVRUsçš„å¤æ‚ã€å®‰å…¨å…³é”®åœºæ™¯ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>VRU-AccidentåŸºå‡†æµ‹è¯•åŒ…å«å¤§é‡çœŸå®äº‹æ•…è§†é¢‘ï¼Œå¹¶é…å¤‡ä¸°å¯Œçš„æ ‡æ³¨ä¿¡æ¯ï¼Œç€é‡å…³æ³¨æ¶‰åŠVRUsçš„äº‹æ•…ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•æ—¨åœ¨è¯„ä¼°MLLMsåœ¨å¤„ç†äº‹æ•…çš„ç©ºé—´æ—¶é—´åŠ¨æ€åŠå› æœè¯­ä¹‰æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>åˆ†æå‘ç°ï¼Œè™½ç„¶MLLMsåœ¨è§†è§‰å±æ€§æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨äº‹æ•…æ¨ç†å’Œæè¿°æ–¹é¢å­˜åœ¨æ˜¾è‘—æŒ‘æˆ˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-98e98ad480cd539da4c6a22fea7d4bb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25ec7c46685294f6bf7d3d3e6f64a9eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a11c3c677dd3ad243bb23db92a1a84fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dac55ea750916477473703046c59ad04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1e4c5137ac4a9d77ebc67b212c15c58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-969f95eb3e31e5d8c4fc0d51dbe8837f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Infinite-Video-Understanding"><a href="#Infinite-Video-Understanding" class="headerlink" title="Infinite Video Understanding"></a>Infinite Video Understanding</h2><p><strong>Authors:Dell Zhang, Xiangyu Chen, Jixiang Luo, Mengxi Jia, Changzhi Sun, Ruilong Ren, Jingren Liu, Hao Sun, Xuelong Li</strong></p>
<p>The rapid advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have ushered in remarkable progress in video understanding. However, a fundamental challenge persists: effectively processing and comprehending video content that extends beyond minutes or hours. While recent efforts like Video-XL-2 have demonstrated novel architectural solutions for extreme efficiency, and advancements in positional encoding such as HoPE and VideoRoPE++ aim to improve spatio-temporal understanding over extensive contexts, current state-of-the-art models still encounter significant computational and memory constraints when faced with the sheer volume of visual tokens from lengthy sequences. Furthermore, maintaining temporal coherence, tracking complex events, and preserving fine-grained details over extended periods remain formidable hurdles, despite progress in agentic reasoning systems like Deep Video Discovery. This position paper posits that a logical, albeit ambitious, next frontier for multimedia research is Infinite Video Understanding â€“ the capability for models to continuously process, understand, and reason about video data of arbitrary, potentially never-ending duration. We argue that framing Infinite Video Understanding as a blue-sky research objective provides a vital north star for the multimedia, and the wider AI, research communities, driving innovation in areas such as streaming architectures, persistent memory mechanisms, hierarchical and adaptive representations, event-centric reasoning, and novel evaluation paradigms. Drawing inspiration from recent work on long&#x2F;ultra-long video understanding and several closely related fields, we outline the core challenges and key research directions towards achieving this transformative capability. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŠå…¶å¤šæ¨¡æ€æ‰©å±•ï¼ˆMLLMï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºè§†é¢‘ç†è§£å¸¦æ¥äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ï¼šæœ‰æ•ˆå¤„ç†å’Œç†è§£è¶…è¿‡åˆ†é’Ÿæˆ–å°æ—¶çš„è§†é¢‘å†…å®¹ã€‚å°½ç®¡æœ€è¿‘çš„åŠªåŠ›ï¼Œå¦‚Video-XL-2ï¼Œå·²ç»å±•ç¤ºäº†æç«¯æ•ˆç‡çš„æ–°å‹æ¶æ„è§£å†³æ–¹æ¡ˆï¼Œä»¥åŠå¦‚HoPEå’ŒVideoRoPE++ç­‰ä½ç½®ç¼–ç çš„è¿›å±•æ—¨åœ¨æ”¹è¿›å¹¿æ³›ä¸Šä¸‹æ–‡ä¸­çš„æ—¶ç©ºç†è§£ï¼Œä½†å½“å‰æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨é¢å¯¹é•¿æ—¶é—´åºåˆ—çš„å¤§é‡è§†è§‰æ ‡è®°æ—¶ï¼Œä»é¢ä¸´é‡å¤§çš„è®¡ç®—å’Œå†…å­˜çº¦æŸã€‚æ­¤å¤–ï¼Œå°½ç®¡åœ¨è¯¸å¦‚Deep Video Discoveryç­‰æ™ºèƒ½æ¨ç†ç³»ç»Ÿæ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç»´æŒæ—¶é—´è¿è´¯æ€§ã€è¿½è¸ªå¤æ‚äº‹ä»¶ä»¥åŠåœ¨å»¶é•¿æ—¶æœŸä¿æŒç²¾ç»†ç»†èŠ‚ä»ç„¶æ˜¯éå¸¸å·¨å¤§çš„æŒ‘æˆ˜ã€‚è¿™ç¯‡ç«‹åœºè®ºæ–‡è®¤ä¸ºï¼Œå¤šåª’ä½“ç ”ç©¶çš„ä¸€ä¸ªåˆä¹é€»è¾‘ä½†é›„å¿ƒå‹ƒå‹ƒçš„ä¸‹ä¸€ä¸ªå‰æ²¿æ˜¯æ— é™è§†é¢‘ç†è§£â€”â€”æ¨¡å‹è¿ç»­å¤„ç†ã€ç†è§£å’Œæ¨ç†ä»»æ„æ½œåœ¨æ— é™æŒç»­æ—¶é—´çš„è§†é¢‘æ•°æ®çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå°†æ— é™è§†é¢‘ç†è§£ä½œä¸ºè“å¤©ç ”ç©¶ç›®æ ‡ï¼Œä¸ºå¤šåª’ä½“å’Œæ›´å¹¿æ³›çš„AIç ”ç©¶ç¤¾åŒºæä¾›äº†ä¸€ä¸ªé‡è¦çš„åŒ—ææ˜Ÿï¼Œæ¨åŠ¨æµåª’ä½“æ¶æ„ã€æŒä¹…æ€§è®°å¿†æœºåˆ¶ã€åˆ†å±‚å’Œè‡ªé€‚åº”è¡¨ç¤ºã€ä»¥äº‹ä»¶ä¸ºä¸­å¿ƒçš„æ¨ç†ä»¥åŠæ–°å‹è¯„ä¼°èŒƒå¼ç­‰é¢†åŸŸçš„åˆ›æ–°ã€‚æˆ‘ä»¬ä»æœ€è¿‘å…³äºé•¿&#x2F;è¶…é•¿è§†é¢‘ç†è§£å’Œå‡ ä¸ªç›¸å…³é¢†åŸŸçš„ä½œå“ä¸­æ±²å–çµæ„Ÿï¼Œæ¦‚è¿°äº†å®ç°è¿™ä¸€å˜é©æ€§èƒ½åŠ›çš„æ ¸å¿ƒæŒ‘æˆ˜å’Œå…³é”®ç ”ç©¶æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09068v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åŠå…¶å¤šæ¨¡æ€æ‰©å±•ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œè§†é¢‘ç†è§£é¢†åŸŸå·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œå¤„ç†å’Œç†è§£è¶…è¿‡åˆ†é’Ÿæˆ–å°æ—¶çš„è§†é¢‘å†…å®¹çš„æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ã€‚å½“å‰æœ€å…ˆè¿›çš„æŠ€æœ¯åœ¨é¢ä¸´å¤§é‡è§†è§‰æ ‡è®°æ—¶ä»é¢ä¸´è®¡ç®—èƒ½åŠ›å’Œå†…å­˜çº¦æŸã€‚å°½ç®¡æœ‰æ·±åº¦è§†é¢‘å‘ç°ç­‰è¿›æ­¥ï¼Œä½†ç»´æŒæ—¶é—´è¿è´¯æ€§ã€è¿½è¸ªå¤æ‚äº‹ä»¶ä»¥åŠåœ¨é•¿æœŸå†…ä¿ç•™ç²¾ç»†ç»†èŠ‚ä»æ˜¯è‰°å·¨çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºæ— é™è§†é¢‘ç†è§£æ˜¯å¤šåª’ä½“ç ”ç©¶çš„æ–°å‰æ²¿ï¼Œå¹¶è®¨è®ºäº†å®ç°è¿™ä¸€å˜é©æ€§èƒ½åŠ›çš„æ ¸å¿ƒæŒ‘æˆ˜å’Œå…³é”®ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€æ‰©å±•ï¼ˆMLLMsï¼‰çš„å¿«é€Ÿå‘å±•æ¨åŠ¨äº†è§†é¢‘ç†è§£é¢†åŸŸçš„æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>å¤„ç†å’Œç†è§£è¶…è¿‡åˆ†é’Ÿæˆ–å°æ—¶çš„è§†é¢‘å†…å®¹ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æŠ€æœ¯é¢ä¸´è®¡ç®—èƒ½åŠ›å’Œå†…å­˜çº¦æŸï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§é‡è§†è§‰æ ‡è®°æ—¶ã€‚</li>
<li>ç»´æŒæ—¶é—´è¿è´¯æ€§ã€è¿½è¸ªå¤æ‚äº‹ä»¶ä»¥åŠåœ¨é•¿æœŸå†…ä¿ç•™ç²¾ç»†ç»†èŠ‚æ˜¯è‰°å·¨çš„æŒ‘æˆ˜ã€‚</li>
<li>æ— é™è§†é¢‘ç†è§£ä½œä¸ºå¤šåª’ä½“ç ”ç©¶çš„æ–°å‰æ²¿ï¼Œéœ€è¦è§£å†³æ ¸å¿ƒæŒ‘æˆ˜ã€‚</li>
<li>å®ç°æ— é™è§†é¢‘ç†è§£çš„å…³é”®ç ”ç©¶æ–¹å‘åŒ…æ‹¬æµåª’ä½“æ¶æ„ã€æŒä¹…è®°å¿†æœºåˆ¶ã€åˆ†å±‚å’Œè‡ªé€‚åº”è¡¨ç¤ºã€äº‹ä»¶å¯¼å‘æ¨ç†å’Œæ–°å‹è¯„ä¼°èŒƒå¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09068">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ac9732a9fbbf3db84487a19cdd0afa73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93596e57588dda68fd15a9e5afdc3ceb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AuroraLong-Bringing-RNNs-Back-to-Efficient-Open-Ended-Video-Understanding"><a href="#AuroraLong-Bringing-RNNs-Back-to-Efficient-Open-Ended-Video-Understanding" class="headerlink" title="AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video   Understanding"></a>AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video   Understanding</h2><p><strong>Authors:Weili Xu, Enxin Song, Wenhao Chai, Xuexiang Wen, Tian Ye, Gaoang Wang</strong></p>
<p>The challenge of long video understanding lies in its high computational complexity and prohibitive memory cost, since the memory and computation required by transformer-based LLMs scale quadratically with input sequence length. We propose AuroraLong to address this challenge by replacing the LLM component in MLLMs with a linear RNN language model that handles input sequence of arbitrary length with constant-size hidden states. To further increase throughput and efficiency, we combine visual token merge with linear RNN models by reordering the visual tokens by their sizes in ascending order. Despite having only 2B parameters and being trained exclusively on public data, AuroraLong achieves performance comparable to Transformer-based models of similar size trained on private datasets across multiple video benchmarks. This demonstrates the potential of efficient, linear RNNs to democratize long video understanding by lowering its computational entry barrier. To our best knowledge, we are the first to use a linear RNN based LLM backbone in a LLaVA-like model for open-ended video understanding. </p>
<blockquote>
<p>é•¿è§†é¢‘ç†è§£çš„æŒ‘æˆ˜åœ¨äºå…¶è¾ƒé«˜çš„è®¡ç®—å¤æ‚åº¦å’Œå·¨å¤§çš„å†…å­˜æˆæœ¬ï¼Œå› ä¸ºåŸºäºå˜å‹å™¨çš„LLMæ‰€éœ€çš„å†…å­˜å’Œè®¡ç®—é‡éšè¾“å…¥åºåˆ—é•¿åº¦å‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†AuroraLongæ–¹æ¡ˆï¼Œç”¨çº¿æ€§RNNè¯­è¨€æ¨¡å‹æ›¿æ¢MLLMä¸­çš„LLMç»„ä»¶ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿä»¥æ’å®šå¤§å°çš„éšè—çŠ¶æ€å¤„ç†ä»»æ„é•¿åº¦çš„è¾“å…¥åºåˆ—ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜ååé‡å’Œæ•ˆç‡ï¼Œæˆ‘ä»¬é€šè¿‡æŒ‰è§†è§‰ä»¤ç‰Œå¤§å°å‡åºé‡æ–°æ’åºï¼Œå°†è§†è§‰ä»¤ç‰Œåˆå¹¶ä¸çº¿æ€§RNNæ¨¡å‹ç›¸ç»“åˆã€‚å°½ç®¡AuroraLongåªæœ‰2Bä¸ªå‚æ•°ï¼Œå¹¶ä¸”åªæ¥å—å…¬å¼€æ•°æ®çš„è®­ç»ƒï¼Œä½†åœ¨å¤šä¸ªè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶æ€§èƒ½ä¸åœ¨ç§æœ‰æ•°æ®é›†ä¸Šè®­ç»ƒçš„ç±»ä¼¼å¤§å°çš„åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ç›¸å½“ã€‚è¿™è¯æ˜äº†é«˜æ•ˆçš„çº¿æ€§RNNæœ‰æ½œåŠ›é€šè¿‡é™ä½è®¡ç®—å…¥é—¨é—¨æ§›æ¥å®ç°é•¿è§†é¢‘ç†è§£çš„æ™®åŠã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªåœ¨ç”¨äºå¼€æ”¾å¼è§†é¢‘ç†è§£çš„LLaVAç±»æ¨¡å‹ä¸­ï¼Œä½¿ç”¨åŸºäºçº¿æ€§RNNçš„LLMä¸»å¹²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02591v3">PDF</a> ICCV 2025 Camera Ready</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹é•¿è§†é¢‘ç†è§£é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¦‚é«˜è®¡ç®—å¤æ‚åº¦å’Œé«˜æ˜‚çš„å†…å­˜æˆæœ¬ï¼Œæˆ‘ä»¬æå‡ºäº†AuroraLongæ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆé€šè¿‡ç”¨çº¿æ€§RNNè¯­è¨€æ¨¡å‹æ›¿æ¢MLLMsä¸­çš„LLMç»„ä»¶ï¼Œä»¥å¤„ç†ä»»æ„é•¿åº¦çš„è¾“å…¥åºåˆ—å¹¶ä¿æŒæ’å®šå¤§å°çš„éšè—çŠ¶æ€ï¼Œä»è€Œè§£å†³è¿™ä¸€é—®é¢˜ã€‚ä¸ºè¿›ä¸€æ­¥å¢åŠ ååé‡å’Œæ•ˆç‡ï¼Œæˆ‘ä»¬é€šè¿‡æŒ‰è§†è§‰ä»¤ç‰Œå¤§å°å‡åºé‡æ–°æ’åºï¼Œå°†è§†è§‰ä»¤ç‰Œåˆå¹¶ä¸çº¿æ€§RNNæ¨¡å‹ç›¸ç»“åˆã€‚å°½ç®¡AuroraLongä»…æ‹¥æœ‰2Bå‚æ•°ï¼Œä¸”ä»…åœ¨å…¬å…±æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†åœ¨å¤šä¸ªè§†é¢‘åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶æ€§èƒ½ä¸ç±»ä¼¼å¤§å°çš„åŸºäºTransformerçš„æ¨¡å‹ç›¸å½“ï¼Œè¿™äº›æ¨¡å‹æ˜¯åœ¨ç§æœ‰æ•°æ®é›†ä¸Šè®­ç»ƒçš„ã€‚è¿™è¯æ˜äº†é«˜æ•ˆçš„çº¿æ€§RNNåœ¨é™ä½é•¿è§†é¢‘ç†è§£çš„è®¡ç®—å…¥é—¨é—¨æ§›æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡åœ¨LLaVAç±»æ¨¡å‹ä¸­ï¼Œä½¿ç”¨åŸºäºçº¿æ€§RNNçš„LLMä¸»å¹²è¿›è¡Œå¼€æ”¾å¼è§†é¢‘ç†è§£ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é•¿è§†é¢‘ç†è§£é¢ä¸´é«˜è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜æˆæœ¬æŒ‘æˆ˜ã€‚</li>
<li>AuroraLongé€šè¿‡çº¿æ€§RNNè¯­è¨€æ¨¡å‹å¤„ç†ä»»æ„é•¿åº¦è¾“å…¥åºåˆ—æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä¿æŒæ’å®šå¤§å°çš„éšè—çŠ¶æ€ã€‚</li>
<li>é€šè¿‡é‡æ–°æ’åºè§†è§‰ä»¤ç‰Œå¤§å°ï¼Œç»“åˆäº†è§†è§‰ä»¤ç‰Œåˆå¹¶ä¸çº¿æ€§RNNæ¨¡å‹ï¼Œæé«˜äº†ååé‡å’Œæ•ˆç‡ã€‚</li>
<li>AuroraLongåœ¨å…¬å…±æ•°æ®ä¸Šè®­ç»ƒï¼Œå‚æ•°è§„æ¨¡ä»…ä¸º2Bï¼Œæ€§èƒ½å´ä¸ç§æœ‰æ•°æ®é›†ä¸Šè®­ç»ƒçš„ç±»ä¼¼å¤§å°çš„åŸºäºTransformerçš„æ¨¡å‹ç›¸å½“ã€‚</li>
<li>å®éªŒç»“æœè¯æ˜äº†é«˜æ•ˆçº¿æ€§RNNåœ¨é•¿è§†é¢‘ç†è§£é¢†åŸŸçš„æ½œåŠ›ã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡ä½¿ç”¨åŸºäºçº¿æ€§RNNçš„LLMä¸»å¹²åœ¨LLaVAç±»æ¨¡å‹ä¸­è¿›è¡Œå¼€æ”¾å¼è§†é¢‘ç†è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02591">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f94fd0b872e2a5c09e11830b3ea3959c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ffd4ffe4215adbf029537af0dcc5ee0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5221689666f7aac6f7071c6db76c91ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d102226cc30ca32858e1f2c5ff9f6e84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efb382a7f4256e52da5aa193cd8652fe.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Flash-VStream-Efficient-Real-Time-Understanding-for-Long-Video-Streams"><a href="#Flash-VStream-Efficient-Real-Time-Understanding-for-Long-Video-Streams" class="headerlink" title="Flash-VStream: Efficient Real-Time Understanding for Long Video Streams"></a>Flash-VStream: Efficient Real-Time Understanding for Long Video Streams</h2><p><strong>Authors:Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Xiaojie Jin</strong></p>
<p>Benefiting from the advances in large language models and cross-modal alignment, existing multimodal large language models have achieved prominent performance in image and short video understanding. However, the understanding of long videos is still challenging, as their long-context nature results in significant computational and memory overhead. Most existing work treats long videos in the same way as short videos, which is inefficient for real-world applications and hard to generalize to even longer videos. To address these issues, we propose Flash-VStream, an efficient video language model capable of processing extremely long videos and responding to user queries in real time. Particularly, we design a Flash Memory module, containing a low-capacity context memory to aggregate long-context temporal information and model the distribution of information density, and a high-capacity augmentation memory to retrieve detailed spatial information based on this distribution. Compared to existing models, Flash-VStream achieves significant reductions in inference latency. Extensive experiments on long video benchmarks and comprehensive video benchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate the state-of-the-art performance and outstanding efficiency of our method. Code is available at <a target="_blank" rel="noopener" href="https://github.com/IVGSZ/Flash-VStream">https://github.com/IVGSZ/Flash-VStream</a>. </p>
<blockquote>
<p>å¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹å’Œè·¨æ¨¡æ€å¯¹é½æŠ€æœ¯çš„è¿›æ­¥ï¼Œç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å›¾åƒå’ŒçŸ­è§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹é•¿è§†é¢‘çš„ç†è§£ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºé•¿è§†é¢‘çš„é•¿ä¸Šä¸‹æ–‡ç‰¹æ€§å¯¼è‡´è®¡ç®—å’Œå†…å­˜å¼€é”€æ˜¾è‘—å¢åŠ ã€‚ç°æœ‰çš„å¤§å¤šæ•°å·¥ä½œéƒ½å°†é•¿è§†é¢‘ä¸çŸ­è§†é¢‘ä»¥ç›¸åŒçš„æ–¹å¼å¤„ç†ï¼Œè¿™å¯¹äºå®é™…åº”ç”¨æ¥è¯´æ•ˆç‡ä½ä¸‹ï¼Œå¹¶ä¸”éš¾ä»¥æ¨å¹¿åˆ°æ›´é•¿çš„è§†é¢‘ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Flash-VStreamï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„è§†é¢‘è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†æé•¿çš„è§†é¢‘å¹¶åœ¨å®æ—¶ä¸­å“åº”ç”¨æˆ·æŸ¥è¯¢ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªFlash Memoryæ¨¡å—ï¼ŒåŒ…å«ä¸€ä¸ªä½å®¹é‡çš„ä¸Šä¸‹æ–‡å†…å­˜ï¼Œç”¨äºèšåˆé•¿ä¸Šä¸‹æ–‡çš„ä¸´æ—¶ä¿¡æ¯å¹¶æ¨¡æ‹Ÿä¿¡æ¯å¯†åº¦çš„åˆ†å¸ƒï¼Œä»¥åŠä¸€ä¸ªåŸºäºè¿™ç§åˆ†å¸ƒçš„é«˜å®¹é‡å¢å¼ºå†…å­˜ï¼Œç”¨äºæ£€ç´¢è¯¦ç»†çš„ç©ºé—´ä¿¡æ¯ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒFlash-VStreamåœ¨æ¨ç†å»¶è¿Ÿæ–¹é¢å®ç°äº†æ˜¾è‘—å‡å°‘ã€‚åœ¨Long Video Benchmarkså’Œå…¨é¢çš„è§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆå³EgoSchemaã€MLVUã€LVBenchã€MVBenchå’Œè§†é¢‘MMEï¼‰ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰æœ€å…ˆè¿›çš„æ€§èƒ½å’Œå‡ºè‰²çš„æ•ˆç‡ã€‚ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/IVGSZ/Flash-VStream%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/IVGSZ/Flash-VStreamè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23825v2">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹å’Œè·¨æ¨¡æ€å¯¹é½æŠ€æœ¯çš„è¿›æ­¥ï¼Œç°æœ‰çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å›¾åƒå’ŒçŸ­è§†é¢‘ç†è§£æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—æ€§èƒ½ã€‚ç„¶è€Œï¼Œé•¿è§†é¢‘ç†è§£ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºå…¶é•¿ä¸Šä¸‹æ–‡ç‰¹æ€§å¯¼è‡´è®¡ç®—å’Œå†…å­˜å¼€é”€è¾ƒå¤§ã€‚å¤§å¤šæ•°ç°æœ‰å·¥ä½œå°†é•¿è§†é¢‘ä¸çŸ­è§†é¢‘ä¸€è§†åŒä»ï¼Œè¿™å¯¹äºå®é™…åº”ç”¨æ•ˆç‡ä½ä¸‹ï¼Œéš¾ä»¥æ¨å¹¿åˆ°æ›´é•¿çš„è§†é¢‘ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Flash-VStreamé«˜æ•ˆè§†é¢‘è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†æé•¿è§†é¢‘å¹¶å®æ—¶å“åº”ç”¨æˆ·æŸ¥è¯¢ã€‚é€šè¿‡è®¾è®¡Flash Memoryæ¨¡å—ï¼ŒåŒ…å«ä½å®¹é‡ä¸Šä¸‹æ–‡å†…å­˜ä»¥èšé›†é•¿ä¸Šä¸‹æ–‡ä¸´æ—¶ä¿¡æ¯å¹¶æ¨¡æ‹Ÿä¿¡æ¯å¯†åº¦åˆ†å¸ƒï¼Œä»¥åŠé«˜å®¹é‡å¢å¼ºå†…å­˜ä»¥åŸºäºè¯¥åˆ†å¸ƒæ£€ç´¢è¯¦ç»†çš„ç©ºé—´ä¿¡æ¯ã€‚ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”ï¼ŒFlash-VStreamå®ç°äº†æ¨ç†å»¶è¿Ÿçš„æ˜¾è‘—é™ä½ã€‚åœ¨é•¿æŒ‰è§†é¢‘åŸºå‡†æµ‹è¯•å’Œç»¼åˆè§†é¢‘åŸºå‡†æµ‹è¯•ï¼ˆå¦‚EgoSchemaã€MLVUã€LVBenchã€MVBenchå’Œè§†é¢‘MMEï¼‰ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ€æ–°æ€§èƒ½å’Œå‡ºè‰²æ•ˆç‡ã€‚ç›¸å…³ä»£ç å¯è®¿é—®äº[ç½‘å€]ï¼ˆä¸­æ–‡è¯­å¢ƒä¸‹ï¼Œå¯ä½¿ç”¨ä¸­æ–‡ç½‘å€ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å’Œè·¨æ¨¡æ€å¯¹é½çš„è¿›æ­¥æ¨åŠ¨äº†è§†é¢‘ç†è§£çš„å‘å±•ã€‚</li>
<li>é•¿è§†é¢‘ç†è§£é¢ä¸´è®¡ç®—ä¸å†…å­˜å¼€é”€å¤§çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤„ç†é•¿è§†é¢‘æ—¶æ•ˆç‡è¾ƒä½ï¼Œéš¾ä»¥æ¨å¹¿è‡³æ›´é•¿è§†é¢‘ã€‚</li>
<li>Flash-VStreamæ¨¡å‹é€šè¿‡è®¾è®¡Flash Memoryæ¨¡å—å¤„ç†é•¿è§†é¢‘ï¼ŒåŒ…å«ä½å®¹é‡ä¸Šä¸‹æ–‡å†…å­˜å’Œé«˜å®¹é‡å¢å¼ºå†…å­˜ã€‚</li>
<li>ä½å®¹é‡ä¸Šä¸‹æ–‡å†…å­˜å¯èšé›†é•¿ä¸Šä¸‹æ–‡ä¿¡æ¯å¹¶æ¨¡æ‹Ÿä¿¡æ¯å¯†åº¦åˆ†å¸ƒã€‚</li>
<li>é«˜å®¹é‡å¢å¼ºå†…å­˜åŸºäºä¿¡æ¯å¯†åº¦åˆ†å¸ƒæ£€ç´¢è¯¦ç»†ç©ºé—´ä¿¡æ¯ã€‚</li>
<li>Flash-VStreamç›¸æ¯”ç°æœ‰æ¨¡å‹é™ä½äº†æ¨ç†å»¶è¿Ÿã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23825">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-14fbe5b9223859f2bdb09f6ac40e7d5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90d011f8cdbd95919a8f60d9155fd78c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0d51d77d9fa70ded555cbabdb3bfa7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d2cfcdf709b5e17573c13834d831b01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b7a18f9cec814869398406db285a691.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80fba04ff3b5c21d4bdb4fc2b8f5db79.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Deep-Video-Discovery-Agentic-Search-with-Tool-Use-for-Long-form-Video-Understanding"><a href="#Deep-Video-Discovery-Agentic-Search-with-Tool-Use-for-Long-form-Video-Understanding" class="headerlink" title="Deep Video Discovery: Agentic Search with Tool Use for Long-form Video   Understanding"></a>Deep Video Discovery: Agentic Search with Tool Use for Long-form Video   Understanding</h2><p><strong>Authors:Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, Yan Lu</strong></p>
<p>Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery agent to leverage an agentic search strategy over segmented video clips. Different from previous video agents manually designing a rigid workflow, our approach emphasizes the autonomous nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools, formulates appropriate parameters for actions, and iteratively refines its internal reasoning in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates the advantage of the entire system design. Our DVD agent achieves SOTA performance, significantly surpassing prior works by a large margin on the challenging LVBench dataset. Comprehensive ablation studies and in-depth tool analyses are also provided, yielding insights to further advance intelligent agents tailored for long-form video understanding tasks. The code has been released in <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepVideoDiscovery">https://github.com/microsoft/DeepVideoDiscovery</a>. </p>
<blockquote>
<p>é•¿è§†é¢‘ç†è§£ç”±äºå·¨å¤§çš„æ—¶ç©ºå¤æ‚æ€§å’Œåœ¨å¦‚æ­¤æ‰©å±•çš„ä¸Šä¸‹æ–‡ä¸‹è¿›è¡Œé—®ç­”çš„å›°éš¾è€Œé¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§†é¢‘åˆ†æèƒ½åŠ›å’Œé•¿ä¸Šä¸‹æ–‡å¤„ç†æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨å¤„ç†ä¿¡æ¯å¯†é›†çš„å°æ—¶é•¿çš„è§†é¢‘æ—¶ï¼Œå®ƒä»¬ä»ç„¶è¡¨ç°å‡ºå±€é™æ€§ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Deep Video Discoveryä»£ç†ï¼Œé‡‡ç”¨åŸºäºåˆ†å‰²çš„è§†é¢‘ç‰‡æ®µçš„ä»£ç†æœç´¢ç­–ç•¥ã€‚ä¸åŒäºä»¥å‰çš„æ‰‹åŠ¨è®¾è®¡åˆšæ€§å·¥ä½œæµç¨‹çš„è§†é¢‘ä»£ç†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼ºè°ƒä»£ç†çš„è‡ªä¸»æ€§ã€‚é€šè¿‡åœ¨å¤šç²’åº¦è§†é¢‘æ•°æ®åº“ä¸Šæä¾›ä¸€å¥—ä»¥æœç´¢ä¸ºä¸­å¿ƒçš„å·¥å…·ï¼Œæˆ‘ä»¬çš„DVDä»£ç†åˆ©ç”¨LLMçš„é«˜çº§æ¨ç†èƒ½åŠ›æ¥è§„åˆ’å…¶å½“å‰è§‚å¯ŸçŠ¶æ€ï¼Œæˆ˜ç•¥æ€§åœ°é€‰æ‹©å·¥å…·ï¼Œä¸ºè¡ŒåŠ¨åˆ¶å®šé€‚å½“å‚æ•°ï¼Œå¹¶æ ¹æ®æ”¶é›†çš„ä¿¡æ¯è¿­ä»£åœ°ä¼˜åŒ–å…¶å†…éƒ¨æ¨ç†ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸Šå¯¹ç³»ç»Ÿè¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œè¯æ˜äº†æ•´ä¸ªç³»ç»Ÿè®¾è®¡çš„ä¼˜åŠ¿ã€‚æˆ‘ä»¬çš„DVDä»£ç†è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„LVBenchæ•°æ®é›†ä¸Šå¤§å¤§è¶…è¶Šäº†ä»¥å‰çš„å·¥ä½œã€‚è¿˜æä¾›äº†å…¨é¢çš„æ¶ˆèç ”ç©¶å’Œæ·±å…¥çš„å·¥å…·åˆ†æï¼Œä¸ºé’ˆå¯¹é•¿è§†é¢‘ç†è§£ä»»åŠ¡è¿›ä¸€æ­¥æ”¹è¿›æ™ºèƒ½ä»£ç†æä¾›äº†è§è§£ã€‚ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepVideoDiscovery%E3%80%82">https://github.com/microsoft/DeepVideoDiscoveryã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18079v3">PDF</a> V3 draft. Under review</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹é•¿è§†é¢‘ç†è§£çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦è§†é¢‘å‘ç°ä»£ç†çš„æœç´¢ç­–ç•¥ã€‚è¯¥ç­–ç•¥å…·å¤‡è‡ªä¸»å­¦ä¹ èƒ½åŠ›ï¼Œå¯ä»¥åœ¨å¤šç²’åº¦è§†é¢‘æ•°æ®åº“ä¸Šå®ç°çµæ´»çš„è§†é¢‘å‰ªè¾‘æœç´¢ï¼Œæ˜¾è‘—æå‡äº†åœ¨å¤æ‚é•¿è§†é¢‘ä¸­çš„ä¿¡æ¯å¤„ç†èƒ½åŠ›ã€‚è¯¥ä»£ç†åœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨LVBenchæ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—è¶…è¶Šå…ˆå‰å·¥ä½œçš„æ€§èƒ½ã€‚ä»£ç å·²å‘å¸ƒåœ¨å¾®è½¯GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é•¿è§†é¢‘ç†è§£é¢ä¸´å·¨å¤§çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¤æ‚çš„æ—¶ç©ºå¤æ‚æ€§å’Œé•¿ä¸Šä¸‹æ–‡ä¸­çš„é—®ç­”éš¾åº¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é•¿è§†é¢‘å¤„ç†ä¸Šä»å­˜åœ¨å±€é™ï¼Œè€Œæ–°æå‡ºçš„Deep Video Discoveryä»£ç†å¯ä»¥å…‹æœè¿™äº›é™åˆ¶ã€‚</li>
<li>Deep Video Discoveryä»£ç†ä½¿ç”¨ä¸€ç§è‡ªä¸»çš„å­¦ä¹ ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨å¤šç²’åº¦è§†é¢‘æ•°æ®åº“ä¸Šè¿›è¡Œçµæ´»çš„æœç´¢ã€‚</li>
<li>è¯¥ä»£ç†å…·å¤‡å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿæ ¹æ®å½“å‰è§‚å¯ŸçŠ¶æ€è¿›è¡Œè§„åˆ’ï¼Œé€‰æ‹©å·¥å…·å¹¶è®¾ç½®é€‚å½“çš„å‚æ•°ã€‚</li>
<li>åœ¨å¤šä¸ªé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œè¯æ˜äº†è¯¥ç³»ç»Ÿçš„ä¼˜åŠ¿ã€‚ç‰¹åˆ«æ˜¯åœ¨LVBenchæ•°æ®é›†ä¸Šçš„è¡¨ç°æ˜¾è‘—è¶…è¶Šäº†å…ˆå‰çš„ä½œå“ã€‚</li>
<li>ç ”ç©¶è€…æä¾›äº†å…¨é¢çš„æ¶ˆèç ”ç©¶å’Œæ·±å…¥çš„å·¥å…·åˆ†æï¼Œä¸ºè¿›ä¸€æ­¥æ”¹è¿›é€‚åˆé•¿è§†é¢‘ç†è§£ä»»åŠ¡çš„æ™ºèƒ½ä»£ç†æä¾›äº†è§è§£ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18079">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8ba0251dee6f1d44ec7f8ae55caf1006.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-953a2e0735dc1523c356980ffc004edb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bb3d4505eeba5bafd3e3b5fe7e6761c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Vidi-Large-Multimodal-Models-for-Video-Understanding-and-Editing"><a href="#Vidi-Large-Multimodal-Models-for-Video-Understanding-and-Editing" class="headerlink" title="Vidi: Large Multimodal Models for Video Understanding and Editing"></a>Vidi: Large Multimodal Models for Video Understanding and Editing</h2><p><strong>Authors: Vidi Team, Celong Liu, Chia-Wen Kuo, Dawei Du, Fan Chen, Guang Chen, Jiamin Yuan, Lingxi Zhang, Lu Guo, Lusha Li, Longyin Wen, Qingyu Chen, Rachel Deng, Sijie Zhu, Stuart Siew, Tong Jin, Wei Lu, Wen Zhong, Xiaohui Shen, Xin Gu, Xing Mei, Xueqiong Qu, Zhenfang Chen</strong></p>
<p>Humans naturally share information with those they are connected to, and video has become one of the dominant mediums for communication and expression on the Internet. To support the creation of high-quality large-scale video content, a modern pipeline requires a comprehensive understanding of both the raw input materials (e.g., the unedited footage captured by cameras) and the editing components (e.g., visual effects). In video editing scenarios, models must process multiple modalities (e.g., vision, audio, text) with strong background knowledge and handle flexible input lengths (e.g., hour-long raw videos), which poses significant challenges for traditional models. In this report, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a wide range of video understand editing scenarios. The first release focuses on temporal retrieval, i.e., identifying the time ranges within the input videos corresponding to a given text query, which plays a critical role in intelligent editing. The model is capable of processing hour-long videos with strong temporal understanding capability, e.g., retrieve time ranges for certain queries. To support a comprehensive evaluation in real-world scenarios, we also present the VUE-TR benchmark, which introduces five key advancements. 1) Video duration: significantly longer than videos of existing temporal retrival datasets, 2) Audio support: includes audio-based queries, 3) Query format: diverse query lengths&#x2F;formats, 4) Annotation quality: ground-truth time ranges are manually annotated. 5) Evaluation metric: a refined IoU metric to support evaluation over multiple time ranges. Remarkably, Vidi significantly outperforms leading proprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task, indicating its superiority in video editing scenarios. </p>
<blockquote>
<p>äººç±»è‡ªç„¶ä¸å…¶æ‰€è”ç³»çš„äººåˆ†äº«ä¿¡æ¯ï¼Œè§†é¢‘å·²æˆä¸ºäº’è”ç½‘ä¸Šè¿›è¡Œäº¤æµå’Œè¡¨è¾¾çš„ä¸»å¯¼åª’ä»‹ä¹‹ä¸€ã€‚ä¸ºäº†æ”¯æŒé«˜è´¨é‡å¤§è§„æ¨¡è§†é¢‘å†…å®¹çš„åˆ›ä½œï¼Œç°ä»£æµæ°´çº¿éœ€è¦å…¨é¢ç†è§£åŸå§‹è¾“å…¥ææ–™ï¼ˆä¾‹å¦‚æ‘„åƒæœºæ‹æ‘„æœªç»ç¼–è¾‘çš„ç´ æï¼‰å’Œç¼–è¾‘ç»„ä»¶ï¼ˆä¾‹å¦‚è§†è§‰æ•ˆæœï¼‰ã€‚åœ¨è§†é¢‘ç¼–è¾‘åœºæ™¯ä¸­ï¼Œæ¨¡å‹å¿…é¡»å¤„ç†å¤šç§æ¨¡æ€ï¼ˆä¾‹å¦‚è§†è§‰ã€éŸ³é¢‘ã€æ–‡æœ¬ï¼‰å¹¶å…·å¤‡å¼ºå¤§çš„èƒŒæ™¯çŸ¥è¯†ï¼ŒåŒæ—¶å¤„ç†çµæ´»è¾“å…¥é•¿åº¦ï¼ˆä¾‹å¦‚é•¿è¾¾æ•°å°æ—¶çš„åŸè§†é¢‘ï¼‰ï¼Œè¿™å¯¹ä¼ ç»Ÿæ¨¡å‹æå‡ºäº†é‡å¤§æŒ‘æˆ˜ã€‚åœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Vidiï¼Œè¿™æ˜¯ä¸€ç³»åˆ—ç”¨äºå¹¿æ³›è§†é¢‘ç†è§£ç¼–è¾‘åœºæ™¯çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ã€‚é¦–æ¬¡å‘å¸ƒé‡ç‚¹å…³æ³¨æ—¶é—´æ£€ç´¢ï¼Œå³è¯†åˆ«ä¸ç»™å®šæ–‡æœ¬æŸ¥è¯¢å¯¹åº”è¾“å…¥è§†é¢‘ä¸­çš„æ—¶é—´æ®µï¼Œè¿™åœ¨æ™ºèƒ½ç¼–è¾‘ä¸­èµ·ç€å…³é”®ä½œç”¨ã€‚è¯¥æ¨¡å‹å…·æœ‰å¤„ç†é•¿è¾¾æ•°å°æ—¶çš„è§†é¢‘çš„å¼ºçƒˆæ—¶é—´ç†è§£èƒ½åŠ›ï¼Œä¾‹å¦‚æ£€ç´¢æŸäº›æŸ¥è¯¢çš„æ—¶é—´èŒƒå›´ã€‚ä¸ºäº†æ”¯æŒçœŸå®åœºæ™¯ä¸­çš„å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†VUE-TRåŸºå‡†æµ‹è¯•ï¼Œå®ƒå¼•å…¥äº†äº”ä¸ªå…³é”®è¿›å±•ã€‚1ï¼‰è§†é¢‘æ—¶é•¿ï¼šæ˜¾è‘—é•¿äºç°æœ‰æ—¶é—´æ£€ç´¢æ•°æ®é›†çš„è§†é¢‘ï¼›2ï¼‰éŸ³é¢‘æ”¯æŒï¼šåŒ…å«åŸºäºéŸ³é¢‘çš„æŸ¥è¯¢ï¼›3ï¼‰æŸ¥è¯¢æ ¼å¼ï¼šå¤šæ ·çš„æŸ¥è¯¢é•¿åº¦&#x2F;æ ¼å¼ï¼›4ï¼‰æ³¨é‡Šè´¨é‡ï¼šé€šè¿‡æ‰‹åŠ¨æ ‡æ³¨çœŸå®çš„æ—¶é—´èŒƒå›´ï¼›5ï¼‰è¯„ä¼°æŒ‡æ ‡ï¼šç²¾ç»†çš„IoUæŒ‡æ ‡ä»¥æ”¯æŒå¤šä¸ªæ—¶é—´èŒƒå›´çš„è¯„ä¼°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒVidiåœ¨æ—¶é—´ä¸Šæ£€ç´¢ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºé¢†å…ˆçš„ä¸“æœ‰æ¨¡å‹ï¼Œå¦‚GPT-4oå’ŒGeminiï¼Œè¿™è¡¨æ˜å…¶åœ¨è§†é¢‘ç¼–è¾‘åœºæ™¯ä¸­çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15681v3">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>è§†é¢‘åœ¨äº’è”ç½‘ä¸Šå·²æˆä¸ºä¸»å¯¼çš„ä¿¡æ¯ä¼ æ’­åª’ä»‹ä¹‹ä¸€ã€‚ä¸ºæ”¯æŒé«˜è´¨é‡çš„å¤§è§„æ¨¡è§†é¢‘å†…å®¹çš„åˆ›ä½œï¼Œå¯¹åŸå§‹ç´ æå’Œç¼–è¾‘ç»„ä»¶çš„ç»¼åˆç†è§£è‡³å…³é‡è¦ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼ŒæŠ¥å‘Šä»‹ç»äº†Vidiç³»åˆ—å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ï¼Œé’ˆå¯¹å¹¿æ³›çš„è§†é¢‘ç†è§£ç¼–è¾‘åœºæ™¯ã€‚é¦–æ¬¾å‘å¸ƒçš„äº§å“ä¾§é‡äºæ—¶é—´æ£€ç´¢ï¼Œå³æ ¹æ®æ–‡æœ¬æŸ¥è¯¢è¯†åˆ«è¾“å…¥è§†é¢‘ä¸­çš„æ—¶é—´èŒƒå›´ï¼Œè¿™åœ¨æ™ºèƒ½ç¼–è¾‘ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚è¯¥æ¨¡å‹å…·å¤‡å¤„ç†é•¿è¾¾æ•°å°æ—¶çš„è§†é¢‘çš„å¼ºå¤§æ—¶é—´ç†è§£èƒ½åŠ›ã€‚ä¸ºæ”¯æŒçœŸå®åœºæ™¯ä¸‹çš„å…¨é¢è¯„ä¼°ï¼ŒæŠ¥å‘Šè¿˜æ¨å‡ºäº†VUE-TRåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«äº”å¤§å…³é”®è¿›å±•ã€‚æŠ¥å‘Šæœ€åå±•ç¤ºäº†Vidiæ¨¡å‹åœ¨æ€§èƒ½ä¸Šçš„ä¼˜è¶Šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨æ—¶é—´æ£€ç´¢ä»»åŠ¡ä¸Šï¼Œæ˜¾è‘—ä¼˜äºGPT-4oå’ŒGeminiç­‰ä¸»æµæ¨¡å‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†é¢‘åœ¨äº’è”ç½‘æ²Ÿé€šä¸­çš„é‡è¦æ€§æ—¥ç›Šå‡¸æ˜¾ï¼Œæˆä¸ºä¸»å¯¼çš„ä¿¡æ¯ä¼ æ’­åª’ä»‹ä¹‹ä¸€ã€‚</li>
<li>ç°ä»£è§†é¢‘å†…å®¹åˆ›ä½œéœ€è¦ç†è§£åŸå§‹ç´ æå’Œç¼–è¾‘ç»„ä»¶çš„ç»¼åˆæ¨¡å‹ã€‚</li>
<li>Vidiç³»åˆ—å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æ—¨åœ¨æ”¯æŒå¹¿æ³›çš„è§†é¢‘ç†è§£ç¼–è¾‘åœºæ™¯ã€‚</li>
<li>æŠ¥å‘Šå…³æ³¨çš„æ—¶é—´æ£€ç´¢åœ¨æ™ºèƒ½ç¼–è¾‘ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚</li>
<li>Vidiæ¨¡å‹èƒ½å¤Ÿå¤„ç†é•¿è¾¾æ•°å°æ—¶çš„è§†é¢‘å¹¶å…·æœ‰å¼ºå¤§çš„æ—¶é—´ç†è§£èƒ½åŠ›ã€‚</li>
<li>VUE-TRåŸºå‡†æµ‹è¯•åŒ…å«äº”å¤§å…³é”®è¿›å±•ï¼Œç”¨äºæ”¯æŒçœŸå®åœºæ™¯ä¸‹çš„å…¨é¢è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15681">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-01ad689ae2f9d32b321047d29e7c172a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca6122a960cfbc1abf56b5d0a747e0e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74c5d173d5b354d3f1ff8c2272029b64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06e190fdcb035cf8cf86d17145da5cd2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6d4f87c7f38f588dbae5c1d5a5a4a693.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Vamba-Understanding-Hour-Long-Videos-with-Hybrid-Mamba-Transformers"><a href="#Vamba-Understanding-Hour-Long-Videos-with-Hybrid-Mamba-Transformers" class="headerlink" title="Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers"></a>Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers</h2><p><strong>Authors:Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, Wenhu Chen</strong></p>
<p>State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the number of video tokens but often incur information loss and remain inefficient for extremely long sequences. In this paper, we explore an orthogonal direction to build a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to encode video tokens with linear complexity. Without any token reduction, VAMBA can encode more than 1024 frames (640$\times$360) on a single GPU, while transformer-based models can only encode 256 frames. On long video input, VAMBA achieves at least 50% reduction in GPU memory usage during training and inference, and nearly doubles the speed per training step compared to transformer-based LMMs. Our experimental results demonstrate that VAMBA improves accuracy by 4.3% on the challenging hour-long video understanding benchmark LVBench over prior efficient video LMMs, and maintains strong performance on a broad spectrum of long and short video understanding tasks. </p>
<blockquote>
<p>å½“å‰æœ€å…ˆè¿›çš„åŸºäºtransformerçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨å¤„ç†é•¿è¾¾æ•°å°æ—¶çš„è§†é¢‘è¾“å…¥æ—¶é¢ä¸´å›°éš¾ï¼Œå› ä¸ºå› æœè‡ªæ³¨æ„åŠ›æ“ä½œçš„äºŒæ¬¡å¤æ‚æ€§å¯¼è‡´äº†è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ç°æœ‰çš„åŸºäºä»¤ç‰Œå‹ç¼©çš„æ–¹æ³•å‡å°‘äº†è§†é¢‘ä»¤ç‰Œçš„æ•°é‡ï¼Œä½†å¾€å¾€ä¼šé€ æˆä¿¡æ¯ä¸¢å¤±ï¼Œå¹¶ä¸”å¯¹äºæé•¿çš„åºåˆ—æ¥è¯´ä»ç„¶æ•ˆç‡ä½ä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ä¸ªå»ºç«‹æ··åˆMamba-Transformeræ¨¡å‹ï¼ˆVAMBAï¼‰çš„æ­£äº¤æ–¹å‘ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨Mamba-2å—ä»¥çº¿æ€§å¤æ‚åº¦ç¼–ç è§†é¢‘ä»¤ç‰Œã€‚åœ¨ä¸å‡å°‘ä»¤ç‰Œçš„æƒ…å†µä¸‹ï¼ŒVAMBAå¯ä»¥åœ¨å•ä¸ªGPUä¸Šç¼–ç è¶…è¿‡1024å¸§ï¼ˆ640Ã—360ï¼‰ï¼Œè€ŒåŸºäºtransformerçš„æ¨¡å‹åªèƒ½ç¼–ç 256å¸§ã€‚å¯¹äºé•¿è§†é¢‘è¾“å…¥ï¼ŒVAMBAåœ¨è®­ç»ƒå’Œæ¨ç†æœŸé—´å®ç°äº†GPUå†…å­˜ä½¿ç”¨è‡³å°‘å‡å°‘50%ï¼Œå¹¶ä¸”ä¸åŸºäºtransformerçš„LMMç›¸æ¯”ï¼Œæ¯æ­¥è®­ç»ƒé€Ÿåº¦å‡ ä¹æé«˜ä¸€å€ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•LVBenchä¸Šï¼ŒVAMBAåœ¨ä¹‹å‰çš„æ•ˆç‡è§†é¢‘LMMçš„åŸºç¡€ä¸Šæé«˜äº†4.3%çš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”åœ¨å¹¿æ³›çš„é•¿æœŸå’ŒçŸ­æœŸè§†é¢‘ç†è§£ä»»åŠ¡ä¸Šä¿æŒäº†å¼ºå¤§çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11579v2">PDF</a> ICCV 2025 Camera Ready Version. Project Page:   <a target="_blank" rel="noopener" href="https://tiger-ai-lab.github.io/Vamba/">https://tiger-ai-lab.github.io/Vamba/</a></p>
<p><strong>Summary</strong><br>è§†é¢‘ç†è§£é¢†åŸŸçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘è¾“å…¥æ—¶é¢ä¸´è®¡ç®—æˆæœ¬é«˜çš„é—®é¢˜ã€‚ç°æœ‰ç ”ç©¶å°è¯•é€šè¿‡å‡å°‘è§†é¢‘ä»¤ç‰Œæ•°é‡æ¥é™ä½è®¡ç®—æˆæœ¬ï¼Œä½†è¿™æ ·åšä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±ï¼Œä¸”å¯¹äºè¶…é•¿åºåˆ—ä»ä¸å¤Ÿé«˜æ•ˆã€‚æœ¬æ–‡æ¢ç´¢äº†ä¸€ç§æ„å»ºæ··åˆMamba-Transformeræ¨¡å‹ï¼ˆVAMBAï¼‰çš„æ–°æ–¹å‘ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨å…·æœ‰çº¿æ€§å¤æ‚åº¦çš„Mamba-2å—å¯¹è§†é¢‘ä»¤ç‰Œè¿›è¡Œç¼–ç ã€‚æ— éœ€å‡å°‘ä»¤ç‰Œæ•°é‡ï¼ŒVAMBAèƒ½åœ¨å•ä¸ªGPUä¸Šç¼–ç è¶…è¿‡1024å¸§ï¼ˆ640Ã—360ï¼‰çš„è§†é¢‘ï¼Œè€ŒåŸºäºTransformerçš„æ¨¡å‹åªèƒ½ç¼–ç 256å¸§ã€‚åœ¨é•¿è§†é¢‘è¾“å…¥æ–¹é¢ï¼ŒVAMBAåœ¨è®­ç»ƒå’Œæ¨ç†æœŸé—´å®ç°äº†GPUå†…å­˜ä½¿ç”¨è‡³å°‘å‡å°‘50%ï¼Œå¹¶ä¸”ä¸åŸºäºTransformerçš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ç›¸æ¯”ï¼Œè®­ç»ƒæ­¥éª¤é€Ÿåº¦å‡ ä¹ç¿»å€ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVAMBAåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•LVBenchä¸Šçš„å‡†ç¡®ç‡æé«˜äº†4.3%ï¼Œå¹¶ä¸”åœ¨å¹¿æ³›çš„é•¿çŸ­è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å¼ºåŠ²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤„ç†é•¿è§†é¢‘æ—¶é¢ä¸´è®¡ç®—æˆæœ¬é«˜çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šè¿‡å‡å°‘è§†é¢‘ä»¤ç‰Œæ•°é‡æ¥é™ä½æˆæœ¬ï¼Œä½†ä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±å’Œæ•ˆç‡é—®é¢˜ã€‚</li>
<li>VAMBAæ¨¡å‹é‡‡ç”¨Mamba-2å—è¿›è¡Œè§†é¢‘ä»¤ç‰Œç¼–ç ï¼Œå…·æœ‰çº¿æ€§å¤æ‚åº¦ï¼Œæ— éœ€å‡å°‘ä»¤ç‰Œæ•°é‡ã€‚</li>
<li>VAMBAèƒ½åœ¨å•ä¸ªGPUä¸Šå¤„ç†æ›´å¤šçš„è§†é¢‘å¸§ã€‚</li>
<li>VAMBAæ˜¾è‘—å‡å°‘äº†è®­ç»ƒå’Œæ¨ç†æœŸé—´çš„GPUå†…å­˜ä½¿ç”¨ã€‚</li>
<li>VAMBAçš„è®­ç»ƒæ­¥éª¤é€Ÿåº¦å‡ ä¹æ˜¯ç°æœ‰æ¨¡å‹çš„ä¸¤å€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11579">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4df28503317ee664cbb3e4b2604de11f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46c694119abd21be5efe2a3d91cd6519.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d9738788c57d13cc36e78cc64f9e8f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7474219fbd196e5f3a0c590afcae8fea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-638560325336cdc855be26bc43034a32.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LVAgent-Long-Video-Understanding-by-Multi-Round-Dynamical-Collaboration-of-MLLM-Agents"><a href="#LVAgent-Long-Video-Understanding-by-Multi-Round-Dynamical-Collaboration-of-MLLM-Agents" class="headerlink" title="LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration   of MLLM Agents"></a>LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration   of MLLM Agents</h2><p><strong>Authors:Boyu Chen, Zhengrong Yue, Siran Chen, Zikang Wang, Yang Liu, Peng Li, Yali Wang</strong></p>
<p>Existing MLLMs encounter significant challenges in modeling the temporal context within long videos. Currently, mainstream Agent-based methods use external tools to assist a single MLLM in answering long video questions. Despite such tool-based support, a solitary MLLM still offers only a partial understanding of long videos, resulting in limited performance. In order to better address long video tasks, we introduce LVAgent, the first framework enabling multi-round dynamic collaboration of MLLM agents in long video understanding. Our method consists of four key steps: 1) Selection: We pre-select appropriate agents from the model library to form optimal agent teams based on different tasks. 2) Perception: We design an effective retrieval scheme for long videos to improve the coverage of critical temporal segments while maintaining computational efficiency. 3) Action: Agents answer long video questions and exchange reasons. 4) Reflection: We evaluate each agentâ€™s performance in each round of discussion and optimize the agent team for dynamic collaboration. The agents iteratively refine their answers by multi-round dynamical collaboration of MLLM agents. LVAgent is the first agent system method that outperforms all closed-source models (like GPT-4o) and open-source models (like InternVL-2.5 and Qwen2-VL) in the long video understanding tasks. Our LVAgent achieves an accuracy of 80% on four mainstream long video understanding tasks. Notably, LVAgent improves accuracy by 13.3% on LongVideoBench. Code is available at <a target="_blank" rel="noopener" href="https://github.com/64327069/LVAgent">https://github.com/64327069/LVAgent</a>. </p>
<blockquote>
<p>ç°æœ‰çš„MLLMåœ¨å¤„ç†é•¿è§†é¢‘ä¸­çš„æ—¶é—´ä¸Šä¸‹æ–‡å»ºæ¨¡æ—¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç›®å‰ä¸»æµçš„åŸºäºä»£ç†çš„æ–¹æ³•ä½¿ç”¨å¤–éƒ¨å·¥å…·æ¥ååŠ©å•ä¸ªMLLMå›ç­”é•¿è§†é¢‘é—®é¢˜ã€‚å°½ç®¡æœ‰åŸºäºå·¥å…·çš„æ”¯æŒï¼Œå•ä¸ªMLLMä»ç„¶åªèƒ½å¯¹é•¿è§†é¢‘æœ‰å±€éƒ¨ç†è§£ï¼Œå¯¼è‡´æ€§èƒ½æœ‰é™ã€‚ä¸ºäº†æ›´å¥½åœ°å¤„ç†é•¿è§†é¢‘ä»»åŠ¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†LVAgentï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªä½¿MLLMä»£ç†è¿›è¡Œå¤šè½®åŠ¨æ€åä½œçš„é•¿è§†é¢‘ç†è§£æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºå››ä¸ªå…³é”®æ­¥éª¤ï¼š1ï¼‰é€‰æ‹©ï¼šæˆ‘ä»¬ä»æ¨¡å‹åº“ä¸­é¢„å…ˆé€‰æ‹©é€‚å½“çš„ä»£ç†ï¼Œæ ¹æ®ä¸åŒçš„ä»»åŠ¡å½¢æˆæœ€ä¼˜ä»£ç†å›¢é˜Ÿã€‚2ï¼‰æ„ŸçŸ¥ï¼šæˆ‘ä»¬ä¸ºé•¿è§†é¢‘è®¾è®¡äº†ä¸€ç§æœ‰æ•ˆçš„æ£€ç´¢æ–¹æ¡ˆï¼Œä»¥æé«˜å…³é”®æ—¶é—´æ®µçš„è¦†ç›–ç‡ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—æ•ˆç‡ã€‚3ï¼‰è¡ŒåŠ¨ï¼šä»£ç†å›ç­”é•¿è§†é¢‘é—®é¢˜å¹¶äº¤æµç†ç”±ã€‚4ï¼‰åæ€ï¼šæˆ‘ä»¬è¯„ä¼°æ¯ä¸ªä»£ç†åœ¨æ¯è½®è®¨è®ºä¸­çš„è¡¨ç°ï¼Œä¼˜åŒ–ä»£ç†å›¢é˜Ÿè¿›è¡ŒåŠ¨æ€åä½œã€‚é€šè¿‡MLLMä»£ç†çš„å¤šè½®åŠ¨æ€åä½œï¼Œä»£ç†èƒ½å¤Ÿä¸æ–­åœ°å®Œå–„å…¶ç­”æ¡ˆã€‚LVAgentæ˜¯é¦–ä¸ªåœ¨é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸­ä¼˜äºæ‰€æœ‰é—­æºæ¨¡å‹ï¼ˆå¦‚GPT-4oï¼‰å’Œå¼€æºæ¨¡å‹ï¼ˆå¦‚InternVL-2.5å’ŒQwen2-VLï¼‰çš„ä»£ç†ç³»ç»Ÿæ–¹æ³•ã€‚æˆ‘ä»¬çš„LVAgentåœ¨å››ä¸ªä¸»æµçš„é•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šè¾¾åˆ°äº†80%çš„å‡†ç¡®ç‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒLVAgentåœ¨LongVideoBenchä¸Šçš„å‡†ç¡®ç‡æé«˜äº†13.3%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/64327069/LVAgent">https://github.com/64327069/LVAgent</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10200v4">PDF</a> accepted in ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å¤„ç†é•¿è§†é¢‘ç†è§£ä»»åŠ¡æ—¶ï¼Œç°æœ‰MLLMé¢ä¸´çš„æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†LVAgentæ¡†æ¶ï¼Œé€šè¿‡å¤šè½®åŠ¨æ€åä½œçš„MLLMä»£ç†å®ç°é•¿è§†é¢‘ç†è§£ã€‚LVAgentåŒ…æ‹¬é€‰æ‹©ã€æ„ŸçŸ¥ã€è¡ŒåŠ¨å’Œåæ€å››ä¸ªå…³é”®æ­¥éª¤ï¼Œèƒ½æœ‰æ•ˆæé«˜é•¿è§†é¢‘ä»»åŠ¡çš„æ€§èƒ½ã€‚LVAgentåœ¨ä¸»æµé•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°80%ï¼Œåœ¨LongVideoBenchä¸Šçš„å‡†ç¡®ç‡æé«˜13.3%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMåœ¨å¤„ç†é•¿è§†é¢‘ç†è§£ä»»åŠ¡æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>LVAgentæ¡†æ¶é€šè¿‡å¤šè½®åŠ¨æ€åä½œçš„MLLMä»£ç†è§£å†³æ­¤é—®é¢˜ã€‚</li>
<li>LVAgentåŒ…æ‹¬é€‰æ‹©ã€æ„ŸçŸ¥ã€è¡ŒåŠ¨å’Œåæ€å››ä¸ªå…³é”®æ­¥éª¤ã€‚</li>
<li>LVAgentèƒ½æœ‰æ•ˆæé«˜é•¿è§†é¢‘ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>LVAgentåœ¨ä¸»æµé•¿è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šçš„å‡†ç¡®ç‡ä¸º80%ã€‚</li>
<li>LVAgentåœ¨LongVideoBenchä¸Šçš„å‡†ç¡®ç‡è¾ƒå…¶ä»–æ¨¡å‹æé«˜13.3%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10200">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e34b0ef9b490ae3b13fd9fc259dd7a1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36e8b00642d51f709422bb252ce89a9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae0c83d1766c0da7464ec73448d189a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e89c31255333c481402fb81b6441c4d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7faa39017b8f9feb6d8ac447406c0e74.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a1e85749bc723dbdb5f43298501e3667.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  IN45023 Neural Network Design Patterns in Computer Vision Seminar   Report, Summer 2025
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4ece756ddbcbb9cb90df7645521e10d1.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  Explainable Image Classification with Reduced Overconfidence for Tissue   Characterisation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27197.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
