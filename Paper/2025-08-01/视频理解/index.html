<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="视频理解">
    <meta name="description" content="视频理解 方向最新论文已更新，请持续关注 Update in 2025-08-01  Towards Video Thinking Test A Holistic Benchmark for Advanced Video   Reasoning and Understanding">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>视频理解 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-46c694119abd21be5efe2a3d91cd6519.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">视频理解</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                <span class="chip bg-color">视频理解</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                视频理解
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    45 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-01-更新"><a href="#2025-08-01-更新" class="headerlink" title="2025-08-01 更新"></a>2025-08-01 更新</h1><h2 id="Towards-Video-Thinking-Test-A-Holistic-Benchmark-for-Advanced-Video-Reasoning-and-Understanding"><a href="#Towards-Video-Thinking-Test-A-Holistic-Benchmark-for-Advanced-Video-Reasoning-and-Understanding" class="headerlink" title="Towards Video Thinking Test: A Holistic Benchmark for Advanced Video   Reasoning and Understanding"></a>Towards Video Thinking Test: A Holistic Benchmark for Advanced Video   Reasoning and Understanding</h2><p><strong>Authors:Yuanhan Zhang, Yunice Chew, Yuhao Dong, Aria Leo, Bo Hu, Ziwei Liu</strong></p>
<p>Human intelligence requires correctness and robustness, with the former being foundational for the latter. In video understanding, correctness ensures the accurate interpretation of visual content, and robustness maintains consistent performance in challenging conditions. Despite advances in video large language models (video LLMs), existing benchmarks inadequately reflect the gap between these models and human intelligence in maintaining correctness and robustness in video interpretation. We introduce the Video Thinking Test (Video-TT), to assess if video LLMs can interpret real-world videos as effectively as humans. Video-TT reflects genuine gaps in understanding complex visual narratives, and evaluates robustness against natural adversarial questions. Video-TT comprises 1,000 YouTube Shorts videos, each with one open-ended question and four adversarial questions that probe visual and narrative complexity. Our evaluation shows a significant gap between video LLMs and human performance. </p>
<blockquote>
<p>人类的智能需要正确性和稳健性，前者是后者的基础。在视频理解中，正确性确保对视觉内容的准确解释，而稳健性则维持了在挑战条件下的稳定表现。尽管视频大型语言模型（Video LLMs）有所进展，但现有基准测试并未充分反映出这些模型在维持视频解读中的正确性和稳健性与人类智能之间的差距。我们推出视频思维测试（Video-TT），旨在评估视频LLMs是否能像人类一样有效地解读现实世界的视频。Video-TT反映了在理解复杂视觉叙事方面的真实差距，并评估了面对自然对抗性问题时的稳健性。Video-TT包含1000个YouTube短视频，每个视频都有一个开放性问题以及四个针对视觉和叙事复杂性的对抗性问题。我们的评估显示，视频LLMs与人类性能之间存在显著差距。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15028v1">PDF</a> ICCV 2025; Project page: <a target="_blank" rel="noopener" href="https://zhangyuanhan-ai.github.io/video-tt/">https://zhangyuanhan-ai.github.io/video-tt/</a></p>
<p><strong>Summary</strong></p>
<p>视频理解领域中，人类智能需要正确性与稳健性，其中正确性为基础。在视频理解中，正确性确保视觉内容的准确解读，而稳健性则维持了在复杂环境下的稳定表现。尽管视频大型语言模型（video LLMs）有所发展，现有基准测试并不能充分反映这些模型在维持视频解读的正确性与稳健性方面与人类智能之间的差距。为此，我们推出视频思维测试（Video-TT），旨在评估视频LLMs是否如人类般有效地解读现实世界的视频。Video-TT反映了在理解复杂视觉叙事方面的真实差距，并评估了面对自然对抗性问题的稳健性。该测试包含1000个YouTube短视频，每个视频附带一个开放性问题及四个针对视觉和叙事复杂性的对抗性问题。评估结果显示视频LLMs与人类性能之间存在显著差距。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人类智能在视频理解中需要正确性和稳健性。</li>
<li>现有基准测试未能充分反映视频LLMs与人类在视频理解上的差距。</li>
<li>推出视频思维测试（Video-TT），以评估视频LLMs的解读能力。</li>
<li>Video-TT包含1000个短视频，每个视频附带一个开放性问题及四个对抗性问题。</li>
<li>Video-TT旨在测试模型在理解复杂视觉叙事和面对对抗性问题时的表现。</li>
<li>视频LLMs在视频理解方面与人类存在显著差距。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15028">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3857dfd91ef9e7de3aea4b189c68621f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-156dc4247e7de063839e214331f86807.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e569d1aa3636f69d8f87b0a0a7a5986.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a20801336ec0cb8d807aa69ca9f6a9fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea9e6a0b0cada6842fcd7bcd5dabf696.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b1bc72c7ac32a240648d02aa556339e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="VideoITG-Multimodal-Video-Understanding-with-Instructed-Temporal-Grounding"><a href="#VideoITG-Multimodal-Video-Understanding-with-Instructed-Temporal-Grounding" class="headerlink" title="VideoITG: Multimodal Video Understanding with Instructed Temporal   Grounding"></a>VideoITG: Multimodal Video Understanding with Instructed Temporal   Grounding</h2><p><strong>Authors:Shihao Wang, Guo Chen, De-an Huang, Zhiqi Li, Minghan Li, Guilin Li, Jose M. Alvarez, Lei Zhang, Zhiding Yu</strong></p>
<p>Recent studies have revealed that selecting informative and relevant video frames can significantly improve the performance of Video Large Language Models (Video-LLMs). Current methods, such as reducing inter-frame redundancy, employing separate models for image-text relevance assessment, or utilizing temporal video grounding for event localization, substantially adopt unsupervised learning paradigms, whereas they struggle to address the complex scenarios in long video understanding. We propose Instructed Temporal Grounding for Videos (VideoITG), featuring customized frame sampling aligned with user instructions. The core of VideoITG is the VidThinker pipeline, an automated annotation framework that explicitly mimics the human annotation process. First, it generates detailed clip-level captions conditioned on the instruction; then, it retrieves relevant video segments through instruction-guided reasoning; finally, it performs fine-grained frame selection to pinpoint the most informative visual evidence. Leveraging VidThinker, we construct the VideoITG-40K dataset, containing 40K videos and 500K instructed temporal grounding annotations. We then design a plug-and-play VideoITG model, which takes advantage of visual language alignment and reasoning capabilities of Video-LLMs, for effective frame selection in a discriminative manner. Coupled with Video-LLMs, VideoITG achieves consistent performance improvements across multiple multimodal video understanding benchmarks, showing its superiority and great potentials for video understanding. </p>
<blockquote>
<p>近期研究指出，选取具有信息量和相关性的视频帧，能显著提升视频大语言模型（Video-LLM）的性能。当前的方法，如减少帧间冗余、采用单独的模型进行图文相关性评估，或使用基于时间的视频定位进行事件定位等，主要采用无监督学习模式，难以应对长视频理解中的复杂场景。为此，我们提出基于用户指令的视频指令化时间定位方法（VideoITG），具有自定义帧采样与用户指令对齐的特点。VideoITG的核心是VidThinker管道，一个模仿人类标注过程的自动化标注框架。首先，它根据指令生成详细的片段级描述；然后，通过指令引导推理检索相关视频片段；最后，进行精细帧选择，确定最具信息量的视觉证据。借助VidThinker，我们构建了包含4万视频和50万个指令化时间定位标注的VideoITG-40K数据集。接着，我们设计了一款即插即用的VideoITG模型，该模型利用视频大语言模型的视觉语言对齐和推理能力，以判别方式进行有效帧选择。配合视频大语言模型，VideoITG在多个多媒体视频理解基准测试中实现了性能持续提升，证明了其在视频理解领域的优越性及巨大潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13353v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong></p>
<p>本文提出一种针对视频的指令化时间定位方法（VideoITG），通过模仿人类标注过程，构建自动化标注框架VidThinker，用于生成基于指令的详细片段级描述、通过指令引导的推理以及精细化的帧选择。利用VideoITG，构建VideoITG-40K数据集，并设计一款即插即用的VideoITG模型，结合视频大型语言模型（Video-LLM）的视觉语言对齐和推理能力，实现有效的帧选择。该方法在多个多媒体视频理解基准测试中表现优越，具有巨大的视频理解潜力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>近期研究发现选择有信息量和相关性的视频帧能显著提升视频大型语言模型（Video-LLM）性能。</li>
<li>当前方法主要采取无监督学习模式，但在处理长视频复杂场景时存在困难。</li>
<li>提出的VideoITG方法通过模仿人类标注过程，构建自动化标注框架VidThinker。</li>
<li>VidThinker包含三个阶段：基于指令生成详细片段级描述、通过指令引导的推理以及精细化的帧选择。</li>
<li>利用VideoITG构建VideoITG-40K数据集，包含40K视频和500K指令化时间定位标注。</li>
<li>设计的VideoITG模型结合Video-LLM的视觉语言对齐和推理能力，实现有效的帧选择。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13353">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-81c9d9dec3eb7f6f847e772ef481f6b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a16ec2c16960901e8083fbfa6eef88f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bdcd1464b78e432b41f8525d54ecada.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-467a1197a64f2585b9ad956bde53c5ea.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Watch-Listen-Understand-Mislead-Tri-modal-Adversarial-Attacks-on-Short-Videos-for-Content-Appropriateness-Evaluation"><a href="#Watch-Listen-Understand-Mislead-Tri-modal-Adversarial-Attacks-on-Short-Videos-for-Content-Appropriateness-Evaluation" class="headerlink" title="Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on   Short Videos for Content Appropriateness Evaluation"></a>Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on   Short Videos for Content Appropriateness Evaluation</h2><p><strong>Authors:Sahid Hossain Mustakim, S M Jishanul Islam, Ummay Maria Muna, Montasir Chowdhury, Mohammed Jawwadul Islam, Sadia Ahmmed, Tashfia Sikder, Syed Tasdid Azam Dhrubo, Swakkhar Shatabda</strong></p>
<p>Multimodal Large Language Models (MLLMs) are increasingly used for content moderation, yet their robustness in short-form video contexts remains underexplored. Current safety evaluations often rely on unimodal attacks, failing to address combined attack vulnerabilities. In this paper, we introduce a comprehensive framework for evaluating the tri-modal safety of MLLMs. First, we present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising diverse short-form videos with human-guided synthetic adversarial attacks. Second, we propose ChimeraBreak, a novel tri-modal attack strategy that simultaneously challenges visual, auditory, and semantic reasoning pathways. Extensive experiments on state-of-the-art MLLMs reveal significant vulnerabilities with high Attack Success Rates (ASR). Our findings uncover distinct failure modes, showing model biases toward misclassifying benign or policy-violating content. We assess results using LLM-as-a-judge, demonstrating attack reasoning efficacy. Our dataset and findings provide crucial insights for developing more robust and safe MLLMs. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）越来越多地用于内容审核，但其在短视频上下文中的稳健性仍然被忽视。当前的安全评估通常依赖于单模态攻击，无法应对组合攻击漏洞。在本文中，我们引入了一个全面框架，以评估MLLMs的三模态安全性。首先，我们提出了短视频多模态对抗（SVMA）数据集，其中包含多种带有由人类引导的合成对抗性攻击的短视频。其次，我们提出了一种新型的三模态攻击策略—— ChimeraBreak，可以同时挑战视觉、听觉和语义推理路径。在最新MLLMs上的广泛实验表明存在显著漏洞，攻击成功率（ASR）很高。我们的研究发现了不同的失败模式，显示出模型偏向于误判良性或违反政策的内容。我们使用LLM作为法官进行评估，证明了攻击推理的有效性。我们的数据集和研究结果对于开发更稳健和安全的多模态大型语言模型提供了关键见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11968v1">PDF</a> Accepted as long paper, SVU Workshop at ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了多模态大型语言模型（MLLMs）在短视频内容审核中的应用及其存在的安全问题。文章提出了一个全面的框架来评估MLLMs的三模态安全性，并介绍了Short-Video Multimodal Adversarial（SVMA）数据集和ChimeraBreak新型三模态攻击策略。实验表明，MLLMs存在显著的安全漏洞，对良性或违规内容的误判表现出模型偏见。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在短视频内容审核中的使用越来越普遍，但其稳健性仍待探索。</li>
<li>当前的安全评估主要依赖单模态攻击，无法应对组合攻击漏洞。</li>
<li>提出了一个评估MLLMs三模态安全性的全面框架。</li>
<li>介绍了Short-Video Multimodal Adversarial（SVMA）数据集，包含带有人工合成对抗攻击的短视频。</li>
<li>提出了ChimeraBreak新型三模态攻击策略，同时挑战视觉、听觉和语义推理路径。</li>
<li>实验表明，MLLMs存在显著的安全漏洞，攻击成功率较高。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11968">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-34d4c0f5eb4b8eb8e9561989c634116a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-881ae9b9d4272710de7fcd61933ace9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-053153bcb4f20a7517c1a705c94ac0a5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7eb0e8cd25d1e8641ce0d554ab57d8ad.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="VRU-Accident-A-Vision-Language-Benchmark-for-Video-Question-Answering-and-Dense-Captioning-for-Accident-Scene-Understanding"><a href="#VRU-Accident-A-Vision-Language-Benchmark-for-Video-Question-Answering-and-Dense-Captioning-for-Accident-Scene-Understanding" class="headerlink" title="VRU-Accident: A Vision-Language Benchmark for Video Question Answering   and Dense Captioning for Accident Scene Understanding"></a>VRU-Accident: A Vision-Language Benchmark for Video Question Answering   and Dense Captioning for Accident Scene Understanding</h2><p><strong>Authors:Younggun Kim, Ahmed S. Abdelrahman, Mohamed Abdel-Aty</strong></p>
<p>Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and cyclists, is a critical challenge for autonomous driving systems, as crashes involving VRUs often result in severe or fatal consequences. While multimodal large language models (MLLMs) have shown promise in enhancing scene understanding and decision making in autonomous vehicles, there is currently no standardized benchmark to quantitatively evaluate their reasoning abilities in complex, safety-critical scenarios involving VRUs. To address this gap, we present VRU-Accident, a large-scale vision-language benchmark designed to evaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident comprises 1K real-world dashcam accident videos, annotated with 6K multiple-choice question-answer pairs across six safety-critical categories (with 24K candidate options and 3.4K unique answer choices), as well as 1K dense scene descriptions. Unlike prior works, our benchmark focuses explicitly on VRU-vehicle accidents, providing rich, fine-grained annotations that capture both spatial-temporal dynamics and causal semantics of accidents. To assess the current landscape of MLLMs, we conduct a comprehensive evaluation of 17 state-of-the-art models on the multiple-choice VQA task and on the dense captioning task. Our findings reveal that while MLLMs perform reasonably well on visually grounded attributes, they face significant challenges in reasoning and describing accident causes, types, and preventability. </p>
<blockquote>
<p>确保脆弱道路使用者（如步行者和骑自行车的人）的安全对于自动驾驶系统来说是一个关键挑战，因为涉及脆弱道路使用者的交通事故常常会导致严重或致命的后果。尽管多模态大型语言模型（MLLMs）在增强自动驾驶场景理解和决策制定方面表现出了潜力，但目前还没有一个标准化的基准来定量评估它们在涉及脆弱道路使用者的复杂、安全关键的场景中的推理能力。为了弥补这一空白，我们推出了VRU-Accident，这是一个大规模视觉语言基准测试，旨在评估涉及脆弱道路使用者的高风险交通场景中的MLLMs。VRU-Accident包含1000个真实世界的行车记录仪事故视频，这些视频被标注了6000个选择题答案对，跨越六个安全关键类别（包含24000个候选选项和3400个独特答案），以及1000个密集场景描述。与以前的工作不同，我们的基准测试明确地关注于VRU-车辆事故，提供丰富、精细的标注，捕捉事故的时空动态和因果语义。为了评估当前MLLMs的状况，我们对17个最新模型进行了全面的多选题问答任务和密集描述任务评估。我们的研究发现，虽然MLLMs在视觉基础上的属性表现良好，但在推理和描述事故原因、类型和可预防性方面仍面临重大挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09815v2">PDF</a> 22 pages, 11 figures, 5 tables</p>
<p><strong>摘要</strong></p>
<p>针对自主驾驶系统中保障脆弱道路使用者（VRUs）如行人及骑行者的安全问题，本文提出VRU-Accident大型视觉语言基准测试。该测试包含千余真实行车记录事故视频，涵盖六大关键安全类别，包含问答配对及密集场景描述等丰富精细标注。该基准测试专注于评估多模态大型语言模型（MLLMs）在处理涉及VRUs的事故时的性能挑战，评估其描述事故起因、类型及可预防性的能力。分析发现，虽然MLLMs在视觉属性方面的表现较好，但在事故推理和描述方面仍存在挑战。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>自主驾驶系统面临保障脆弱道路使用者的重大挑战。涉及VRUs的事故常导致严重或致命后果。</li>
<li>多模态大型语言模型（MLLMs）在增强自主车辆场景理解和决策方面展现潜力。</li>
<li>目前缺乏标准化基准测试来评估MLLMs在处理涉及VRUs的复杂、安全关键场景中的推理能力。</li>
<li>VRU-Accident基准测试包含大量真实事故视频，并配备丰富的标注信息，着重关注涉及VRUs的事故。</li>
<li>该基准测试旨在评估MLLMs在处理事故的空间时间动态及因果语义方面的能力。</li>
<li>分析发现，虽然MLLMs在视觉属性方面表现良好，但在事故推理和描述方面存在显著挑战。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09815">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-98e98ad480cd539da4c6a22fea7d4bb3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-25ec7c46685294f6bf7d3d3e6f64a9eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a11c3c677dd3ad243bb23db92a1a84fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dac55ea750916477473703046c59ad04.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e1e4c5137ac4a9d77ebc67b212c15c58.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-969f95eb3e31e5d8c4fc0d51dbe8837f.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Infinite-Video-Understanding"><a href="#Infinite-Video-Understanding" class="headerlink" title="Infinite Video Understanding"></a>Infinite Video Understanding</h2><p><strong>Authors:Dell Zhang, Xiangyu Chen, Jixiang Luo, Mengxi Jia, Changzhi Sun, Ruilong Ren, Jingren Liu, Hao Sun, Xuelong Li</strong></p>
<p>The rapid advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have ushered in remarkable progress in video understanding. However, a fundamental challenge persists: effectively processing and comprehending video content that extends beyond minutes or hours. While recent efforts like Video-XL-2 have demonstrated novel architectural solutions for extreme efficiency, and advancements in positional encoding such as HoPE and VideoRoPE++ aim to improve spatio-temporal understanding over extensive contexts, current state-of-the-art models still encounter significant computational and memory constraints when faced with the sheer volume of visual tokens from lengthy sequences. Furthermore, maintaining temporal coherence, tracking complex events, and preserving fine-grained details over extended periods remain formidable hurdles, despite progress in agentic reasoning systems like Deep Video Discovery. This position paper posits that a logical, albeit ambitious, next frontier for multimedia research is Infinite Video Understanding – the capability for models to continuously process, understand, and reason about video data of arbitrary, potentially never-ending duration. We argue that framing Infinite Video Understanding as a blue-sky research objective provides a vital north star for the multimedia, and the wider AI, research communities, driving innovation in areas such as streaming architectures, persistent memory mechanisms, hierarchical and adaptive representations, event-centric reasoning, and novel evaluation paradigms. Drawing inspiration from recent work on long&#x2F;ultra-long video understanding and several closely related fields, we outline the core challenges and key research directions towards achieving this transformative capability. </p>
<blockquote>
<p>大型语言模型（LLM）及其多模态扩展（MLLM）的快速发展为视频理解带来了显著的进步。然而，一个基本挑战仍然存在：有效处理和理解超过分钟或小时的视频内容。尽管最近的努力，如Video-XL-2，已经展示了极端效率的新型架构解决方案，以及如HoPE和VideoRoPE++等位置编码的进展旨在改进广泛上下文中的时空理解，但当前最先进的模型在面对长时间序列的大量视觉标记时，仍面临重大的计算和内存约束。此外，尽管在诸如Deep Video Discovery等智能推理系统方面取得了进展，但维持时间连贯性、追踪复杂事件以及在延长时期保持精细细节仍然是非常巨大的挑战。这篇立场论文认为，多媒体研究的一个合乎逻辑但雄心勃勃的下一个前沿是无限视频理解——模型连续处理、理解和推理任意潜在无限持续时间的视频数据的能力。我们认为，将无限视频理解作为蓝天研究目标，为多媒体和更广泛的AI研究社区提供了一个重要的北极星，推动流媒体架构、持久性记忆机制、分层和自适应表示、以事件为中心的推理以及新型评估范式等领域的创新。我们从最近关于长&#x2F;超长视频理解和几个相关领域的作品中汲取灵感，概述了实现这一变革性能力的核心挑战和关键研究方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.09068v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着大型语言模型（LLMs）及其多模态扩展（MLLMs）的快速发展，视频理解领域已经取得了显著的进步。然而，处理和理解超过分钟或小时的视频内容的挑战仍然存在。当前最先进的技术在面临大量视觉标记时仍面临计算能力和内存约束。尽管有深度视频发现等进步，但维持时间连贯性、追踪复杂事件以及在长期内保留精细细节仍是艰巨的挑战。本文提出无限视频理解是多媒体研究的新前沿，并讨论了实现这一变革性能力的核心挑战和关键研究方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）和多模态扩展（MLLMs）的快速发展推动了视频理解领域的显著进步。</li>
<li>处理和理解超过分钟或小时的视频内容仍存在挑战。</li>
<li>当前技术面临计算能力和内存约束，尤其是在处理大量视觉标记时。</li>
<li>维持时间连贯性、追踪复杂事件以及在长期内保留精细细节是艰巨的挑战。</li>
<li>无限视频理解作为多媒体研究的新前沿，需要解决核心挑战。</li>
<li>实现无限视频理解的关键研究方向包括流媒体架构、持久记忆机制、分层和自适应表示、事件导向推理和新型评估范式。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.09068">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-ac9732a9fbbf3db84487a19cdd0afa73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93596e57588dda68fd15a9e5afdc3ceb.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AuroraLong-Bringing-RNNs-Back-to-Efficient-Open-Ended-Video-Understanding"><a href="#AuroraLong-Bringing-RNNs-Back-to-Efficient-Open-Ended-Video-Understanding" class="headerlink" title="AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video   Understanding"></a>AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video   Understanding</h2><p><strong>Authors:Weili Xu, Enxin Song, Wenhao Chai, Xuexiang Wen, Tian Ye, Gaoang Wang</strong></p>
<p>The challenge of long video understanding lies in its high computational complexity and prohibitive memory cost, since the memory and computation required by transformer-based LLMs scale quadratically with input sequence length. We propose AuroraLong to address this challenge by replacing the LLM component in MLLMs with a linear RNN language model that handles input sequence of arbitrary length with constant-size hidden states. To further increase throughput and efficiency, we combine visual token merge with linear RNN models by reordering the visual tokens by their sizes in ascending order. Despite having only 2B parameters and being trained exclusively on public data, AuroraLong achieves performance comparable to Transformer-based models of similar size trained on private datasets across multiple video benchmarks. This demonstrates the potential of efficient, linear RNNs to democratize long video understanding by lowering its computational entry barrier. To our best knowledge, we are the first to use a linear RNN based LLM backbone in a LLaVA-like model for open-ended video understanding. </p>
<blockquote>
<p>长视频理解的挑战在于其较高的计算复杂度和巨大的内存成本，因为基于变压器的LLM所需的内存和计算量随输入序列长度呈二次方增长。为了解决这一挑战，我们提出了AuroraLong方案，用线性RNN语言模型替换MLLM中的LLM组件，该模型能够以恒定大小的隐藏状态处理任意长度的输入序列。为了进一步提高吞吐量和效率，我们通过按视觉令牌大小升序重新排序，将视觉令牌合并与线性RNN模型相结合。尽管AuroraLong只有2B个参数，并且只接受公开数据的训练，但在多个视频基准测试中，其性能与在私有数据集上训练的类似大小的基于转换器的模型相当。这证明了高效的线性RNN有潜力通过降低计算入门门槛来实现长视频理解的普及。据我们所知，我们是第一个在用于开放式视频理解的LLaVA类模型中，使用基于线性RNN的LLM主干。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.02591v3">PDF</a> ICCV 2025 Camera Ready</p>
<p><strong>摘要</strong></p>
<p>针对长视频理解面临的挑战，如高计算复杂度和高昂的内存成本，我们提出了AuroraLong方案。该方案通过用线性RNN语言模型替换MLLMs中的LLM组件，以处理任意长度的输入序列并保持恒定大小的隐藏状态，从而解决这一问题。为进一步增加吞吐量和效率，我们通过按视觉令牌大小升序重新排序，将视觉令牌合并与线性RNN模型相结合。尽管AuroraLong仅拥有2B参数，且仅在公共数据上进行训练，但在多个视频基准测试中，其性能与类似大小的基于Transformer的模型相当，这些模型是在私有数据集上训练的。这证明了高效的线性RNN在降低长视频理解的计算入门门槛方面具有潜力。据我们所知，我们是首次在LLaVA类模型中，使用基于线性RNN的LLM主干进行开放式视频理解。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>长视频理解面临高计算复杂度和内存成本挑战。</li>
<li>AuroraLong通过线性RNN语言模型处理任意长度输入序列来解决这一问题，保持恒定大小的隐藏状态。</li>
<li>通过重新排序视觉令牌大小，结合了视觉令牌合并与线性RNN模型，提高了吞吐量和效率。</li>
<li>AuroraLong在公共数据上训练，参数规模仅为2B，性能却与私有数据集上训练的类似大小的基于Transformer的模型相当。</li>
<li>实验结果证明了高效线性RNN在长视频理解领域的潜力。</li>
<li>这是首次使用基于线性RNN的LLM主干在LLaVA类模型中进行开放式视频理解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.02591">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-f94fd0b872e2a5c09e11830b3ea3959c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ffd4ffe4215adbf029537af0dcc5ee0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5221689666f7aac6f7071c6db76c91ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d102226cc30ca32858e1f2c5ff9f6e84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efb382a7f4256e52da5aa193cd8652fe.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Flash-VStream-Efficient-Real-Time-Understanding-for-Long-Video-Streams"><a href="#Flash-VStream-Efficient-Real-Time-Understanding-for-Long-Video-Streams" class="headerlink" title="Flash-VStream: Efficient Real-Time Understanding for Long Video Streams"></a>Flash-VStream: Efficient Real-Time Understanding for Long Video Streams</h2><p><strong>Authors:Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Xiaojie Jin</strong></p>
<p>Benefiting from the advances in large language models and cross-modal alignment, existing multimodal large language models have achieved prominent performance in image and short video understanding. However, the understanding of long videos is still challenging, as their long-context nature results in significant computational and memory overhead. Most existing work treats long videos in the same way as short videos, which is inefficient for real-world applications and hard to generalize to even longer videos. To address these issues, we propose Flash-VStream, an efficient video language model capable of processing extremely long videos and responding to user queries in real time. Particularly, we design a Flash Memory module, containing a low-capacity context memory to aggregate long-context temporal information and model the distribution of information density, and a high-capacity augmentation memory to retrieve detailed spatial information based on this distribution. Compared to existing models, Flash-VStream achieves significant reductions in inference latency. Extensive experiments on long video benchmarks and comprehensive video benchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate the state-of-the-art performance and outstanding efficiency of our method. Code is available at <a target="_blank" rel="noopener" href="https://github.com/IVGSZ/Flash-VStream">https://github.com/IVGSZ/Flash-VStream</a>. </p>
<blockquote>
<p>得益于大型语言模型和跨模态对齐技术的进步，现有的多模态大型语言模型在图像和短视频理解方面取得了显著的性能。然而，对长视频的理解仍然具有挑战性，因为长视频的长上下文特性导致计算和内存开销显著增加。现有的大多数工作都将长视频与短视频以相同的方式处理，这对于实际应用来说效率低下，并且难以推广到更长的视频。为了解决这些问题，我们提出了Flash-VStream，这是一种高效的视频语言模型，能够处理极长的视频并在实时中响应用户查询。特别是，我们设计了一个Flash Memory模块，包含一个低容量的上下文内存，用于聚合长上下文的临时信息并模拟信息密度的分布，以及一个基于这种分布的高容量增强内存，用于检索详细的空间信息。与现有模型相比，Flash-VStream在推理延迟方面实现了显著减少。在Long Video Benchmarks和全面的视频基准测试（即EgoSchema、MLVU、LVBench、MVBench和视频MME）上的大量实验表明，我们的方法具有最先进的性能和出色的效率。代码可通过<a target="_blank" rel="noopener" href="https://github.com/IVGSZ/Flash-VStream%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/IVGSZ/Flash-VStream获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.23825v2">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型和跨模态对齐技术的进步，现有的多模态大型语言模型在图像和短视频理解方面表现出显著性能。然而，长视频理解仍然存在挑战，因为其长上下文特性导致计算和内存开销较大。大多数现有工作将长视频与短视频一视同仁，这对于实际应用效率低下，难以推广到更长的视频。为解决这些问题，我们提出了Flash-VStream高效视频语言模型，能够处理极长视频并实时响应用户查询。通过设计Flash Memory模块，包含低容量上下文内存以聚集长上下文临时信息并模拟信息密度分布，以及高容量增强内存以基于该分布检索详细的空间信息。与现有模型相比，Flash-VStream实现了推理延迟的显著降低。在长按视频基准测试和综合视频基准测试（如EgoSchema、MLVU、LVBench、MVBench和视频MME）上的广泛实验证明了我们的方法的最新性能和出色效率。相关代码可访问于[网址]（中文语境下，可使用中文网址）。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型和跨模态对齐的进步推动了视频理解的发展。</li>
<li>长视频理解面临计算与内存开销大的挑战。</li>
<li>现有方法处理长视频时效率较低，难以推广至更长视频。</li>
<li>Flash-VStream模型通过设计Flash Memory模块处理长视频，包含低容量上下文内存和高容量增强内存。</li>
<li>低容量上下文内存可聚集长上下文信息并模拟信息密度分布。</li>
<li>高容量增强内存基于信息密度分布检索详细空间信息。</li>
<li>Flash-VStream相比现有模型降低了推理延迟。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.23825">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-14fbe5b9223859f2bdb09f6ac40e7d5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-90d011f8cdbd95919a8f60d9155fd78c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0d51d77d9fa70ded555cbabdb3bfa7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d2cfcdf709b5e17573c13834d831b01.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b7a18f9cec814869398406db285a691.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80fba04ff3b5c21d4bdb4fc2b8f5db79.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Deep-Video-Discovery-Agentic-Search-with-Tool-Use-for-Long-form-Video-Understanding"><a href="#Deep-Video-Discovery-Agentic-Search-with-Tool-Use-for-Long-form-Video-Understanding" class="headerlink" title="Deep Video Discovery: Agentic Search with Tool Use for Long-form Video   Understanding"></a>Deep Video Discovery: Agentic Search with Tool Use for Long-form Video   Understanding</h2><p><strong>Authors:Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, Yan Lu</strong></p>
<p>Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery agent to leverage an agentic search strategy over segmented video clips. Different from previous video agents manually designing a rigid workflow, our approach emphasizes the autonomous nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools, formulates appropriate parameters for actions, and iteratively refines its internal reasoning in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates the advantage of the entire system design. Our DVD agent achieves SOTA performance, significantly surpassing prior works by a large margin on the challenging LVBench dataset. Comprehensive ablation studies and in-depth tool analyses are also provided, yielding insights to further advance intelligent agents tailored for long-form video understanding tasks. The code has been released in <a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepVideoDiscovery">https://github.com/microsoft/DeepVideoDiscovery</a>. </p>
<blockquote>
<p>长视频理解由于巨大的时空复杂性和在如此扩展的上下文下进行问答的困难而面临重大挑战。虽然大型语言模型（LLM）在视频分析能力和长上下文处理方面取得了显著的进步，但在处理信息密集的小时长的视频时，它们仍然表现出局限性。为了克服这些局限性，我们提出了Deep Video Discovery代理，采用基于分割的视频片段的代理搜索策略。不同于以前的手动设计刚性工作流程的视频代理，我们的方法强调代理的自主性。通过在多粒度视频数据库上提供一套以搜索为中心的工具，我们的DVD代理利用LLM的高级推理能力来规划其当前观察状态，战略性地选择工具，为行动制定适当参数，并根据收集的信息迭代地优化其内部推理。我们在多个长视频理解基准测试上对系统进行了全面评估，证明了整个系统设计的优势。我们的DVD代理达到了最先进的性能，在具有挑战性的LVBench数据集上大大超越了以前的工作。还提供了全面的消融研究和深入的工具分析，为针对长视频理解任务进一步改进智能代理提供了见解。代码已发布在<a target="_blank" rel="noopener" href="https://github.com/microsoft/DeepVideoDiscovery%E3%80%82">https://github.com/microsoft/DeepVideoDiscovery。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.18079v3">PDF</a> V3 draft. Under review</p>
<p><strong>Summary</strong>：针对长视频理解的挑战，提出了一种基于深度视频发现代理的搜索策略。该策略具备自主学习能力，可以在多粒度视频数据库上实现灵活的视频剪辑搜索，显著提升了在复杂长视频中的信息处理能力。该代理在多个长视频理解基准测试中表现优异，特别是在LVBench数据集上取得了显著超越先前工作的性能。代码已发布在微软GitHub上。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>长视频理解面临巨大的挑战，包括复杂的时空复杂性和长上下文中的问答难度。</li>
<li>大型语言模型在长视频处理上仍存在局限，而新提出的Deep Video Discovery代理可以克服这些限制。</li>
<li>Deep Video Discovery代理使用一种自主的学习策略，能够在多粒度视频数据库上进行灵活的搜索。</li>
<li>该代理具备强大的推理能力，能够根据当前观察状态进行规划，选择工具并设置适当的参数。</li>
<li>在多个长视频理解基准测试中进行了全面的评估，证明了该系统的优势。特别是在LVBench数据集上的表现显著超越了先前的作品。</li>
<li>研究者提供了全面的消融研究和深入的工具分析，为进一步改进适合长视频理解任务的智能代理提供了见解。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.18079">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8ba0251dee6f1d44ec7f8ae55caf1006.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-953a2e0735dc1523c356980ffc004edb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9bb3d4505eeba5bafd3e3b5fe7e6761c.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Vidi-Large-Multimodal-Models-for-Video-Understanding-and-Editing"><a href="#Vidi-Large-Multimodal-Models-for-Video-Understanding-and-Editing" class="headerlink" title="Vidi: Large Multimodal Models for Video Understanding and Editing"></a>Vidi: Large Multimodal Models for Video Understanding and Editing</h2><p><strong>Authors: Vidi Team, Celong Liu, Chia-Wen Kuo, Dawei Du, Fan Chen, Guang Chen, Jiamin Yuan, Lingxi Zhang, Lu Guo, Lusha Li, Longyin Wen, Qingyu Chen, Rachel Deng, Sijie Zhu, Stuart Siew, Tong Jin, Wei Lu, Wen Zhong, Xiaohui Shen, Xin Gu, Xing Mei, Xueqiong Qu, Zhenfang Chen</strong></p>
<p>Humans naturally share information with those they are connected to, and video has become one of the dominant mediums for communication and expression on the Internet. To support the creation of high-quality large-scale video content, a modern pipeline requires a comprehensive understanding of both the raw input materials (e.g., the unedited footage captured by cameras) and the editing components (e.g., visual effects). In video editing scenarios, models must process multiple modalities (e.g., vision, audio, text) with strong background knowledge and handle flexible input lengths (e.g., hour-long raw videos), which poses significant challenges for traditional models. In this report, we introduce Vidi, a family of Large Multimodal Models (LMMs) for a wide range of video understand editing scenarios. The first release focuses on temporal retrieval, i.e., identifying the time ranges within the input videos corresponding to a given text query, which plays a critical role in intelligent editing. The model is capable of processing hour-long videos with strong temporal understanding capability, e.g., retrieve time ranges for certain queries. To support a comprehensive evaluation in real-world scenarios, we also present the VUE-TR benchmark, which introduces five key advancements. 1) Video duration: significantly longer than videos of existing temporal retrival datasets, 2) Audio support: includes audio-based queries, 3) Query format: diverse query lengths&#x2F;formats, 4) Annotation quality: ground-truth time ranges are manually annotated. 5) Evaluation metric: a refined IoU metric to support evaluation over multiple time ranges. Remarkably, Vidi significantly outperforms leading proprietary models, e.g., GPT-4o and Gemini, on the temporal retrieval task, indicating its superiority in video editing scenarios. </p>
<blockquote>
<p>人类自然与其所联系的人分享信息，视频已成为互联网上进行交流和表达的主导媒介之一。为了支持高质量大规模视频内容的创作，现代流水线需要全面理解原始输入材料（例如摄像机拍摄未经编辑的素材）和编辑组件（例如视觉效果）。在视频编辑场景中，模型必须处理多种模态（例如视觉、音频、文本）并具备强大的背景知识，同时处理灵活输入长度（例如长达数小时的原视频），这对传统模型提出了重大挑战。在本报告中，我们介绍了Vidi，这是一系列用于广泛视频理解编辑场景的大型多模态模型（LMMs）。首次发布重点关注时间检索，即识别与给定文本查询对应输入视频中的时间段，这在智能编辑中起着关键作用。该模型具有处理长达数小时的视频的强烈时间理解能力，例如检索某些查询的时间范围。为了支持真实场景中的全面评估，我们还推出了VUE-TR基准测试，它引入了五个关键进展。1）视频时长：显著长于现有时间检索数据集的视频；2）音频支持：包含基于音频的查询；3）查询格式：多样的查询长度&#x2F;格式；4）注释质量：通过手动标注真实的时间范围；5）评估指标：精细的IoU指标以支持多个时间范围的评估。值得注意的是，Vidi在时间上检索任务上显著优于领先的专有模型，如GPT-4o和Gemini，这表明其在视频编辑场景中的优越性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.15681v3">PDF</a> </p>
<p><strong>摘要</strong><br>视频在互联网上已成为主导的信息传播媒介之一。为支持高质量的大规模视频内容的创作，对原始素材和编辑组件的综合理解至关重要。在此背景下，报告介绍了Vidi系列大型多模态模型（LMMs），针对广泛的视频理解编辑场景。首款发布的产品侧重于时间检索，即根据文本查询识别输入视频中的时间范围，这在智能编辑中发挥着关键作用。该模型具备处理长达数小时的视频的强大时间理解能力。为支持真实场景下的全面评估，报告还推出了VUE-TR基准测试，包含五大关键进展。报告最后展示了Vidi模型在性能上的优越性，尤其是在时间检索任务上，显著优于GPT-4o和Gemini等主流模型。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>视频在互联网沟通中的重要性日益凸显，成为主导的信息传播媒介之一。</li>
<li>现代视频内容创作需要理解原始素材和编辑组件的综合模型。</li>
<li>Vidi系列大型多模态模型（LMMs）旨在支持广泛的视频理解编辑场景。</li>
<li>报告关注的时间检索在智能编辑中发挥着关键作用。</li>
<li>Vidi模型能够处理长达数小时的视频并具有强大的时间理解能力。</li>
<li>VUE-TR基准测试包含五大关键进展，用于支持真实场景下的全面评估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.15681">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-01ad689ae2f9d32b321047d29e7c172a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ca6122a960cfbc1abf56b5d0a747e0e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74c5d173d5b354d3f1ff8c2272029b64.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06e190fdcb035cf8cf86d17145da5cd2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6d4f87c7f38f588dbae5c1d5a5a4a693.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Vamba-Understanding-Hour-Long-Videos-with-Hybrid-Mamba-Transformers"><a href="#Vamba-Understanding-Hour-Long-Videos-with-Hybrid-Mamba-Transformers" class="headerlink" title="Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers"></a>Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers</h2><p><strong>Authors:Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, Wenhu Chen</strong></p>
<p>State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the number of video tokens but often incur information loss and remain inefficient for extremely long sequences. In this paper, we explore an orthogonal direction to build a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to encode video tokens with linear complexity. Without any token reduction, VAMBA can encode more than 1024 frames (640$\times$360) on a single GPU, while transformer-based models can only encode 256 frames. On long video input, VAMBA achieves at least 50% reduction in GPU memory usage during training and inference, and nearly doubles the speed per training step compared to transformer-based LMMs. Our experimental results demonstrate that VAMBA improves accuracy by 4.3% on the challenging hour-long video understanding benchmark LVBench over prior efficient video LMMs, and maintains strong performance on a broad spectrum of long and short video understanding tasks. </p>
<blockquote>
<p>当前最先进的基于transformer的大型多模态模型（LMMs）在处理长达数小时的视频输入时面临困难，因为因果自注意力操作的二次复杂性导致了训练和推理过程中的计算成本高昂。现有的基于令牌压缩的方法减少了视频令牌的数量，但往往会造成信息丢失，并且对于极长的序列来说仍然效率低下。在本文中，我们探索了一个建立混合Mamba-Transformer模型（VAMBA）的正交方向，该模型采用Mamba-2块以线性复杂度编码视频令牌。在不减少令牌的情况下，VAMBA可以在单个GPU上编码超过1024帧（640×360），而基于transformer的模型只能编码256帧。对于长视频输入，VAMBA在训练和推理期间实现了GPU内存使用至少减少50%，并且与基于transformer的LMM相比，每步训练速度几乎提高一倍。我们的实验结果表明，在具有挑战性的长达一小时的视频理解基准测试LVBench上，VAMBA在之前的效率视频LMM的基础上提高了4.3%的准确率，并且在广泛的长期和短期视频理解任务上保持了强大的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11579v2">PDF</a> ICCV 2025 Camera Ready Version. Project Page:   <a target="_blank" rel="noopener" href="https://tiger-ai-lab.github.io/Vamba/">https://tiger-ai-lab.github.io/Vamba/</a></p>
<p><strong>Summary</strong><br>视频理解领域的大型多模态模型在处理长达一小时的视频输入时面临计算成本高的问题。现有研究尝试通过减少视频令牌数量来降低计算成本，但这样做会导致信息损失，且对于超长序列仍不够高效。本文探索了一种构建混合Mamba-Transformer模型（VAMBA）的新方向，该模型采用具有线性复杂度的Mamba-2块对视频令牌进行编码。无需减少令牌数量，VAMBA能在单个GPU上编码超过1024帧（640×360）的视频，而基于Transformer的模型只能编码256帧。在长视频输入方面，VAMBA在训练和推理期间实现了GPU内存使用至少减少50%，并且与基于Transformer的大型多模态模型相比，训练步骤速度几乎翻倍。实验结果表明，VAMBA在具有挑战性的长达一小时的视频理解基准测试LVBench上的准确率提高了4.3%，并且在广泛的长短视频理解任务上表现强劲。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型多模态模型在处理长视频时面临计算成本高的挑战。</li>
<li>现有方法通过减少视频令牌数量来降低成本，但会导致信息损失和效率问题。</li>
<li>VAMBA模型采用Mamba-2块进行视频令牌编码，具有线性复杂度，无需减少令牌数量。</li>
<li>VAMBA能在单个GPU上处理更多的视频帧。</li>
<li>VAMBA显著减少了训练和推理期间的GPU内存使用。</li>
<li>VAMBA的训练步骤速度几乎是现有模型的两倍。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11579">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4df28503317ee664cbb3e4b2604de11f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46c694119abd21be5efe2a3d91cd6519.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d9738788c57d13cc36e78cc64f9e8f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7474219fbd196e5f3a0c590afcae8fea.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-638560325336cdc855be26bc43034a32.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LVAgent-Long-Video-Understanding-by-Multi-Round-Dynamical-Collaboration-of-MLLM-Agents"><a href="#LVAgent-Long-Video-Understanding-by-Multi-Round-Dynamical-Collaboration-of-MLLM-Agents" class="headerlink" title="LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration   of MLLM Agents"></a>LVAgent: Long Video Understanding by Multi-Round Dynamical Collaboration   of MLLM Agents</h2><p><strong>Authors:Boyu Chen, Zhengrong Yue, Siran Chen, Zikang Wang, Yang Liu, Peng Li, Yali Wang</strong></p>
<p>Existing MLLMs encounter significant challenges in modeling the temporal context within long videos. Currently, mainstream Agent-based methods use external tools to assist a single MLLM in answering long video questions. Despite such tool-based support, a solitary MLLM still offers only a partial understanding of long videos, resulting in limited performance. In order to better address long video tasks, we introduce LVAgent, the first framework enabling multi-round dynamic collaboration of MLLM agents in long video understanding. Our method consists of four key steps: 1) Selection: We pre-select appropriate agents from the model library to form optimal agent teams based on different tasks. 2) Perception: We design an effective retrieval scheme for long videos to improve the coverage of critical temporal segments while maintaining computational efficiency. 3) Action: Agents answer long video questions and exchange reasons. 4) Reflection: We evaluate each agent’s performance in each round of discussion and optimize the agent team for dynamic collaboration. The agents iteratively refine their answers by multi-round dynamical collaboration of MLLM agents. LVAgent is the first agent system method that outperforms all closed-source models (like GPT-4o) and open-source models (like InternVL-2.5 and Qwen2-VL) in the long video understanding tasks. Our LVAgent achieves an accuracy of 80% on four mainstream long video understanding tasks. Notably, LVAgent improves accuracy by 13.3% on LongVideoBench. Code is available at <a target="_blank" rel="noopener" href="https://github.com/64327069/LVAgent">https://github.com/64327069/LVAgent</a>. </p>
<blockquote>
<p>现有的MLLM在处理长视频中的时间上下文建模时面临重大挑战。目前主流的基于代理的方法使用外部工具来协助单个MLLM回答长视频问题。尽管有基于工具的支持，单个MLLM仍然只能对长视频有局部理解，导致性能有限。为了更好地处理长视频任务，我们引入了LVAgent，这是第一个使MLLM代理进行多轮动态协作的长视频理解框架。我们的方法分为四个关键步骤：1）选择：我们从模型库中预先选择适当的代理，根据不同的任务形成最优代理团队。2）感知：我们为长视频设计了一种有效的检索方案，以提高关键时间段的覆盖率，同时保持计算效率。3）行动：代理回答长视频问题并交流理由。4）反思：我们评估每个代理在每轮讨论中的表现，优化代理团队进行动态协作。通过MLLM代理的多轮动态协作，代理能够不断地完善其答案。LVAgent是首个在长视频理解任务中优于所有闭源模型（如GPT-4o）和开源模型（如InternVL-2.5和Qwen2-VL）的代理系统方法。我们的LVAgent在四个主流的长视频理解任务上达到了80%的准确率。值得注意的是，LVAgent在LongVideoBench上的准确率提高了13.3%。代码可在<a target="_blank" rel="noopener" href="https://github.com/64327069/LVAgent">https://github.com/64327069/LVAgent</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.10200v4">PDF</a> accepted in ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了在处理长视频理解任务时，现有MLLM面临的挑战。为此，提出了LVAgent框架，通过多轮动态协作的MLLM代理实现长视频理解。LVAgent包括选择、感知、行动和反思四个关键步骤，能有效提高长视频任务的性能。LVAgent在主流长视频理解任务上的准确率达到80%，在LongVideoBench上的准确率提高13.3%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLM在处理长视频理解任务时面临挑战。</li>
<li>LVAgent框架通过多轮动态协作的MLLM代理解决此问题。</li>
<li>LVAgent包括选择、感知、行动和反思四个关键步骤。</li>
<li>LVAgent能有效提高长视频任务的性能。</li>
<li>LVAgent在主流长视频理解任务上的准确率为80%。</li>
<li>LVAgent在LongVideoBench上的准确率较其他模型提高13.3%。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.10200">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e34b0ef9b490ae3b13fd9fc259dd7a1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36e8b00642d51f709422bb252ce89a9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae0c83d1766c0da7464ec73448d189a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e89c31255333c481402fb81b6441c4d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7faa39017b8f9feb6d8ac447406c0e74.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                                    <span class="chip bg-color">视频理解</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a1e85749bc723dbdb5f43298501e3667.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-08-01  IN45023 Neural Network Design Patterns in Computer Vision Seminar   Report, Summer 2025
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4ece756ddbcbb9cb90df7645521e10d1.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-08-01  Explainable Image Classification with Reduced Overconfidence for Tissue   Characterisation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">27197.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
