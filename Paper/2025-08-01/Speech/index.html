<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  Identifying Hearing Difficulty Moments in Conversational Audio">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-8849d6872b3140822eef10a0f30459a9.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    77 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-02-æ›´æ–°"><a href="#2025-08-02-æ›´æ–°" class="headerlink" title="2025-08-02 æ›´æ–°"></a>2025-08-02 æ›´æ–°</h1><h2 id="Identifying-Hearing-Difficulty-Moments-in-Conversational-Audio"><a href="#Identifying-Hearing-Difficulty-Moments-in-Conversational-Audio" class="headerlink" title="Identifying Hearing Difficulty Moments in Conversational Audio"></a>Identifying Hearing Difficulty Moments in Conversational Audio</h2><p><strong>Authors:Jack Collins, Adrian Buzea, Chris Collier, Alejandro Ballesta Rosen, Julian Maclaren, Richard F. Lyon, Simon Carlile</strong></p>
<p>Individuals regularly experience Hearing Difficulty Moments in everyday conversation. Identifying these moments of hearing difficulty has particular significance in the field of hearing assistive technology where timely interventions are key for realtime hearing assistance. In this paper, we propose and compare machine learning solutions for continuously detecting utterances that identify these specific moments in conversational audio. We show that audio language models, through their multimodal reasoning capabilities, excel at this task, significantly outperforming a simple ASR hotword heuristic and a more conventional fine-tuning approach with Wav2Vec, an audio-only input architecture that is state-of-the-art for automatic speech recognition (ASR). </p>
<blockquote>
<p>åœ¨æ—¥å¸¸å¯¹è¯ä¸­ï¼Œä¸ªäººç»å¸¸ä¼šé‡åˆ°å¬åŠ›å›°éš¾æ—¶åˆ»ã€‚è¯†åˆ«è¿™äº›å¬åŠ›å›°éš¾æ—¶åˆ»å¯¹äºåŠ©å¬æŠ€æœ¯é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå› ä¸ºåŠæ—¶çš„å¹²é¢„å¯¹äºå®æ—¶å¬åŠ›è¾…åŠ©è‡³å…³é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºå¹¶æ¯”è¾ƒäº†ç”¨äºè¿ç»­æ£€æµ‹å¯¹è¯éŸ³é¢‘ä¸­ç‰¹å®šæ—¶åˆ»çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒéŸ³é¢‘è¯­è¨€æ¨¡å‹é€šè¿‡å…¶å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›åœ¨è¿™æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œæ˜¾è‘—ä¼˜äºç®€å•çš„ASRçƒ­è¯å¯å‘å¼æ–¹æ³•å’Œæ›´ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ï¼Œä½¿ç”¨Wav2Vecï¼ˆä¸€ç§ä»…é€‚ç”¨äºéŸ³é¢‘çš„è¾“å…¥æ¶æ„ï¼Œæ˜¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æœ€æ–°æŠ€æœ¯ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23590v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºå¹¶æ¯”è¾ƒäº†ç”¨äºæŒç»­æ£€æµ‹æ—¥å¸¸å¯¹è¯ä¸­çš„å¬åŠ›å›°éš¾æ—¶åˆ»çš„æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚ç ”ç©¶å‘ç°ï¼Œå…·å¤‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹åœ¨æ­¤ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºç®€å•çš„ASRçƒ­è¯å¯å‘å¼æ–¹æ³•å’Œä½¿ç”¨Wav2Vecç­‰ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ã€‚è¿™é¡¹ç ”ç©¶å¯¹äºå¬åŠ›è¾…åŠ©æŠ€æœ¯é¢†åŸŸå…·æœ‰é‡è¦æ„ä¹‰ï¼Œå› ä¸ºå®æ—¶å¹²é¢„å¯¹äºæä¾›å®æ—¶å¬åŠ›æ´åŠ©è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¬åŠ›å›°éš¾æ—¶åˆ»çš„è¯†åˆ«åœ¨æ—¥å¸¸å¯¹è¯ä¸­å…·æœ‰é‡è¦ä»·å€¼ï¼Œç‰¹åˆ«æ˜¯åœ¨å¬åŠ›è¾…åŠ©æŠ€æœ¯é¢†åŸŸã€‚</li>
<li>æœ¬æ–‡æå‡ºå¹¶æ¯”è¾ƒäº†å¤šç§æœºå™¨å­¦ä¹ æ–¹æ³•æ¥æ£€æµ‹å¬åŠ›å›°éš¾æ—¶åˆ»ã€‚</li>
<li>éŸ³é¢‘è¯­è¨€æ¨¡å‹é€šè¿‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›åœ¨æ­¤ä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºã€‚</li>
<li>éŸ³é¢‘è¯­è¨€æ¨¡å‹æ˜¾è‘—ä¼˜äºç®€å•çš„ASRçƒ­è¯å¯å‘å¼æ–¹æ³•å’Œä½¿ç”¨Wav2Vecç­‰ä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ã€‚</li>
<li>å®æ—¶å¹²é¢„å¯¹äºæä¾›å®æ—¶å¬åŠ›æ´åŠ©è‡³å…³é‡è¦ã€‚</li>
<li>æœºå™¨å­¦ä¹ çš„åº”ç”¨ä¸ºå¬åŠ›è¾…åŠ©æŠ€æœ¯æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23590">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16d1650b93c7136f0dcaf1194f5dde97.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1918d3c2dfc768a09cfe2d9dc674034d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ff9a43ec69afbdd7aff4a5e0d59fc97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8387e3c58d20fadb30de69e0e9f1f73d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Feature-Importance-across-Domains-for-Improving-Non-Intrusive-Speech-Intelligibility-Prediction-in-Hearing-Aids"><a href="#Feature-Importance-across-Domains-for-Improving-Non-Intrusive-Speech-Intelligibility-Prediction-in-Hearing-Aids" class="headerlink" title="Feature Importance across Domains for Improving Non-Intrusive Speech   Intelligibility Prediction in Hearing Aids"></a>Feature Importance across Domains for Improving Non-Intrusive Speech   Intelligibility Prediction in Hearing Aids</h2><p><strong>Authors:Ryandhimas E. Zezario, Sabato M. Siniscalchi, Fei Chen, Hsin-Min Wang, Yu Tsao</strong></p>
<p>Given the critical role of non-intrusive speech intelligibility assessment in hearing aids (HA), this paper enhances its performance by introducing Feature Importance across Domains (FiDo). We estimate feature importance on spectral and time-domain acoustic features as well as latent representations of Whisper. Importance weights are calculated per frame, and based on these weights, features are projected into new spaces, allowing the model to focus on important areas early. Next, feature concatenation is performed to combine the features before the assessment module processes them. Experimental results show that when FiDo is incorporated into the improved multi-branched speech intelligibility model MBI-Net+, RMSE can be reduced by 7.62% (from 26.10 to 24.11). MBI-Net+ with FiDo also achieves a relative RMSE reduction of 3.98% compared to the best system in the 2023 Clarity Prediction Challenge. These results validate FiDoâ€™s effectiveness in enhancing neural speech assessment in HA. </p>
<blockquote>
<p>è€ƒè™‘åˆ°éä¾µå…¥æ€§è¯­éŸ³æ¸…æ™°åº¦è¯„ä¼°åœ¨åŠ©å¬å™¨ï¼ˆHAï¼‰ä¸­çš„å…³é”®ä½œç”¨ï¼Œæœ¬æ–‡é€šè¿‡å¼•å…¥è·¨åŸŸç‰¹å¾é‡è¦æ€§ï¼ˆFiDoï¼‰æ¥æé«˜å…¶æ€§èƒ½ã€‚æˆ‘ä»¬ä¼°è®¡äº†é¢‘è°±å’Œæ—¶é—´åŸŸå£°å­¦ç‰¹å¾ä»¥åŠè€³è¯­æ½œåœ¨è¡¨ç¤ºçš„ç‰¹å¾é‡è¦æ€§ã€‚é‡è¦æ€§æƒé‡æ˜¯æŒ‰å¸§è®¡ç®—çš„ï¼ŒåŸºäºè¿™äº›æƒé‡ï¼Œç‰¹å¾è¢«æŠ•å°„åˆ°æ–°çš„ç©ºé—´ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ—©æœŸå…³æ³¨é‡è¦åŒºåŸŸã€‚æ¥ä¸‹æ¥ï¼Œè¿›è¡Œç‰¹å¾æ‹¼æ¥ï¼Œä»¥åœ¨è¯„ä¼°æ¨¡å—å¤„ç†ä¹‹å‰ç»“åˆç‰¹å¾ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå½“FiDoè¢«çº³å…¥åˆ°æ”¹è¿›çš„å¤šåˆ†æ”¯è¯­éŸ³æ¸…æ™°åº¦æ¨¡å‹MBI-Net+ä¸­æ—¶ï¼Œå‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰å¯ä»¥é™ä½7.62%ï¼ˆä»26.10é™è‡³24.11ï¼‰ã€‚ä¸2023å¹´æ¸…æ™°åº¦é¢„æµ‹æŒ‘æˆ˜èµ›ä¸­çš„æœ€ä½³ç³»ç»Ÿç›¸æ¯”ï¼ŒMBI-Net+ç»“åˆFiDoè¿˜å®ç°äº†ç›¸å¯¹RMSEé™ä½3.98%ã€‚è¿™äº›ç»“æœéªŒè¯äº†FiDoåœ¨æé«˜åŠ©å¬å™¨çš„ç¥ç»è¯­éŸ³è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23223v1">PDF</a> Accepted to Interspeech 2025</p>
<p><strong>Summary</strong><br>æœ¬è®ºæ–‡é€šè¿‡å¼•å…¥è·¨åŸŸç‰¹å¾é‡è¦æ€§ï¼ˆFiDoï¼‰æŠ€æœ¯ï¼Œæé«˜äº†å¬åŠ›è¾…åŠ©è®¾å¤‡ä¸­éä¾µå…¥æ€§è¯­éŸ³æ¸…æ™°åº¦è¯„ä¼°çš„æ€§èƒ½ã€‚è®ºæ–‡å¯¹é¢‘è°±å’Œæ—¶åŸŸå£°å­¦ç‰¹å¾ä»¥åŠwhisperçš„æ½œåœ¨è¡¨ç°è¿›è¡Œäº†ç‰¹å¾é‡è¦æ€§ä¼°ç®—ã€‚åŸºäºæ¯å¸§çš„é‡è¦æ€§æƒé‡ï¼Œå°†ç‰¹å¾æŠ•å½±åˆ°æ–°çš„ç©ºé—´ä¸­ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæå‰å…³æ³¨é‡è¦åŒºåŸŸã€‚ç„¶åï¼Œé€šè¿‡ç‰¹å¾æ‹¼æ¥å°†ç‰¹å¾åˆå¹¶ï¼Œä»¥ä¾›è¯„ä¼°æ¨¡å—å¤„ç†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°†FiDoçº³å…¥æ”¹è¿›çš„å¤šåˆ†æ”¯è¯­éŸ³æ¸…æ™°åº¦æ¨¡å‹MBI-Net+åï¼ŒRMSEå¯é™ä½7.62%ï¼ˆä»26.10é™è‡³24.11ï¼‰ã€‚ä¸2023å¹´æ¸…æ™°åº¦é¢„æµ‹æŒ‘æˆ˜èµ›çš„æœ€ä½³ç³»ç»Ÿç›¸æ¯”ï¼ŒMBI-Net+ä¸FiDoçš„ç»“åˆå®ç°äº†ç›¸å¯¹RMSEé™ä½3.98%ã€‚ç»“æœéªŒè¯äº†FiDoæŠ€æœ¯åœ¨æé«˜å¬åŠ›è¾…åŠ©è®¾å¤‡ä¸­ç¥ç»ç½‘ç»œè¯­éŸ³è¯„ä¼°æ•ˆæœæ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥è·¨åŸŸç‰¹å¾é‡è¦æ€§ï¼ˆFiDoï¼‰æŠ€æœ¯ï¼Œç”¨äºæé«˜éä¾µå…¥æ€§è¯­éŸ³æ¸…æ™°åº¦è¯„ä¼°åœ¨å¬åŠ›è¾…åŠ©è®¾å¤‡ä¸­çš„æ€§èƒ½ã€‚</li>
<li>ä¼°ç®—é¢‘è°±å’Œæ—¶åŸŸå£°å­¦ç‰¹å¾ä»¥åŠwhisperæ½œåœ¨è¡¨ç°çš„ç‰¹å¾é‡è¦æ€§ã€‚</li>
<li>åŸºäºæ¯å¸§çš„é‡è¦æ€§æƒé‡ï¼Œå°†ç‰¹å¾æŠ•å½±åˆ°æ–°çš„ç©ºé—´ä»¥çªå‡ºé‡è¦åŒºåŸŸã€‚</li>
<li>é€šè¿‡ç‰¹å¾æ‹¼æ¥åˆå¹¶ç‰¹å¾ï¼Œä»¥ä¾›è¯„ä¼°æ¨¡å—å¤„ç†ã€‚</li>
<li>FiDoçº³å…¥MBI-Net+åï¼ŒRMSEæ€§èƒ½é™ä½7.62%ã€‚</li>
<li>ä¸ç°æœ‰æœ€ä½³ç³»ç»Ÿç›¸æ¯”ï¼ŒMBI-Net+ä¸FiDoç»“åˆå®ç°äº†ç›¸å¯¹RMSEé™ä½3.98%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23223">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-76bc7039e00f50576a4977fc2ecba136.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fc287c567c7d839d14740257ccd8488.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e6f0a2a6d0ec56c008c39cf6e3b43560.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1fe7f2bd167e4887b39080132ca12a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13576ca9ce92707c572e01d2af359f2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3700bfbac399485ec6b65f20179fa65.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="The-Interspeech-2025-Speech-Accessibility-Project-Challenge"><a href="#The-Interspeech-2025-Speech-Accessibility-Project-Challenge" class="headerlink" title="The Interspeech 2025 Speech Accessibility Project Challenge"></a>The Interspeech 2025 Speech Accessibility Project Challenge</h2><p><strong>Authors:Xiuwen Zheng, Bornali Phukon, Jonghwan Na, Ed Cutrell, Kyu Han, Mark Hasegawa-Johnson, Pan-Pan Jiang, Aadhrik Kuila, Colin Lea, Bob MacDonald, Gautam Mantena, Venkatesh Ravichandran, Leda Sari, Katrin Tomanek, Chang D. Yoo, Chris Zwilling</strong></p>
<p>While the last decade has witnessed significant advancements in Automatic Speech Recognition (ASR) systems, performance of these systems for individuals with speech disabilities remains inadequate, partly due to limited public training data. To bridge this gap, the 2025 Interspeech Speech Accessibility Project (SAP) Challenge was launched, utilizing over 400 hours of SAP data collected and transcribed from more than 500 individuals with diverse speech disabilities. Hosted on EvalAI and leveraging the remote evaluation pipeline, the SAP Challenge evaluates submissions based on Word Error Rate and Semantic Score. Consequently, 12 out of 22 valid teams outperformed the whisper-large-v2 baseline in terms of WER, while 17 teams surpassed the baseline on SemScore. Notably, the top team achieved the lowest WER of 8.11%, and the highest SemScore of 88.44% at the same time, setting new benchmarks for future ASR systems in recognizing impaired speech. </p>
<blockquote>
<p>å°½ç®¡è¿‡å»åå¹´ä¸­è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å¯¹äºæœ‰è¨€è¯­éšœç¢çš„ä¸ªä½“çš„æ€§èƒ½ä»ç„¶ä¸è¶³ï¼Œéƒ¨åˆ†åŸå› æ˜¯å…¬å…±è®­ç»ƒæ•°æ®æœ‰é™ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œ2025å¹´Interspeechè¯­éŸ³æ— éšœç¢é¡¹ç›®ï¼ˆSAPï¼‰æŒ‘æˆ˜èµ›åº”è¿è€Œç”Ÿï¼Œè¯¥æŒ‘æˆ˜èµ›ä½¿ç”¨äº†è¶…è¿‡400å°æ—¶çš„SAPæ•°æ®ï¼Œè¿™äº›æ•°æ®æ˜¯ä»500å¤šåå…·æœ‰ä¸åŒè¨€è¯­éšœç¢çš„ä¸ªä½“èº«ä¸Šæ”¶é›†å¹¶è¿›è¡Œè½¬å½•çš„ã€‚è¯¥æŒ‘æˆ˜èµ›åœ¨EvalAIä¸Šæ‰˜ç®¡ï¼Œå¹¶åˆ©ç”¨è¿œç¨‹è¯„ä¼°ç®¡é“è¿›è¡Œï¼Œæ ¹æ®å•è¯é”™è¯¯ç‡å’Œè¯­ä¹‰åˆ†æ•°å¯¹æäº¤çš„å†…å®¹è¿›è¡Œè¯„ä¼°ã€‚å› æ­¤ï¼Œåœ¨WERæ–¹é¢ï¼Œæœ‰12æ”¯é˜Ÿä¼è¶…è¿‡äº†æœ‰æ•ˆçš„22æ”¯é˜Ÿä¼ä¸­çš„whisper-large-v2åŸºçº¿æ°´å¹³ï¼›è€Œåœ¨SemScoreä¸Šï¼Œæœ‰17æ”¯é˜Ÿä¼è¶…è¿‡äº†åŸºçº¿æ°´å¹³ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ’åç¬¬ä¸€çš„é˜Ÿä¼åŒæ—¶è¾¾åˆ°äº†æœ€ä½çš„WERä¸º8.11%ï¼Œä»¥åŠæœ€é«˜çš„SemScoreä¸º88.44%ï¼Œä¸ºæœªæ¥ASRç³»ç»Ÿåœ¨è¯†åˆ«å—æŸè¯­éŸ³æ–¹é¢æ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22047v1">PDF</a> To appear in Proceedings of Interspeech, 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œå°½ç®¡è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å¯¹äºæœ‰è¨€è¯­éšœç¢çš„ä¸ªä½“çš„æ€§èƒ½ä»ç„¶ä¸è¶³ï¼Œéƒ¨åˆ†åŸå› åœ¨äºå…¬å…±è®­ç»ƒæ•°æ®çš„æœ‰é™æ€§ã€‚ä¸ºå¼¥è¡¥è¿™ä¸€å·®è·ï¼Œ2025å¹´Interspeechä¸¾åŠäº†è¯­éŸ³æ— éšœç¢é¡¹ç›®ï¼ˆSAPï¼‰æŒ‘æˆ˜èµ›ï¼Œä½¿ç”¨è¶…è¿‡400å°æ—¶çš„SAPæ•°æ®ï¼Œè¿™äº›æ•°æ®æ¥è‡ª500å¤šåæ‹¥æœ‰ä¸åŒè¨€è¯­éšœç¢çš„ä¸ªä½“ã€‚è¯¥æŒ‘æˆ˜èµ›åŸºäºWord Error Rateå’ŒSemantic Scoreå¯¹æäº¤çš„å†…å®¹è¿›è¡Œè¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œæœ‰12æ”¯é˜Ÿä¼åœ¨WERæ–¹é¢è¶…è¶Šäº†whisper-large-v2åŸºçº¿ï¼Œæœ‰17æ”¯é˜Ÿä¼åœ¨SemScoreä¸Šè¶…è¿‡äº†åŸºçº¿ã€‚å°¤å…¶å€¼å¾—å…³æ³¨çš„æ˜¯ï¼Œæœ€ä½³å›¢é˜ŸåŒæ—¶å–å¾—äº†æœ€ä½çš„WERï¼ˆ8.11%ï¼‰å’Œæœ€é«˜çš„SemScoreï¼ˆ88.44%ï¼‰ï¼Œä¸ºæœªæ¥ASRç³»ç»Ÿè¯†åˆ«å—æŸè¯­éŸ³è®¾å®šäº†æ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿè™½æœ‰æ‰€è¿›æ­¥ï¼Œä½†å¯¹æœ‰è¨€è¯­éšœç¢çš„ä¸ªä½“çš„æ€§èƒ½ä»ç„¶ä¸è¶³ã€‚</li>
<li>å…¬å…±è®­ç»ƒæ•°æ®çš„æœ‰é™æ€§æ˜¯å¯¼è‡´è¿™ä¸€å·®è·çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚</li>
<li>2025å¹´Interspeechè¯­éŸ³æ— éšœç¢é¡¹ç›®ï¼ˆSAPï¼‰æŒ‘æˆ˜èµ›æ—¨åœ¨ç¼©å°è¿™ä¸€å·®è·ã€‚</li>
<li>SAPæŒ‘æˆ˜èµ›ä½¿ç”¨äº†æ¥è‡ª500å¤šåæ‹¥æœ‰ä¸åŒè¨€è¯­éšœç¢ä¸ªä½“çš„è¶…è¿‡400å°æ—¶çš„æ•°æ®ã€‚</li>
<li>æŒ‘æˆ˜èµ›åŸºäºWord Error Rateå’ŒSemantic Scoreå¯¹æäº¤å†…å®¹è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>åœ¨SAPæŒ‘æˆ˜èµ›ä¸­ï¼Œæœ‰é˜Ÿä¼åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†ç°æœ‰çš„whisper-large-v2åŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22047">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-de3177dcdcc3f77cb4ec87ba81aace7b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1fcc8efc0fdc4628db19eff4090b5ff4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5934a0e9696a24b930119c795e821a9e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Whilter-A-Whisper-based-Data-Filter-for-â€œIn-the-Wildâ€-Speech-Corpora-Using-Utterance-level-Multi-Task-Classification"><a href="#Whilter-A-Whisper-based-Data-Filter-for-â€œIn-the-Wildâ€-Speech-Corpora-Using-Utterance-level-Multi-Task-Classification" class="headerlink" title="Whilter: A Whisper-based Data Filter for â€œIn-the-Wildâ€ Speech Corpora   Using Utterance-level Multi-Task Classification"></a>Whilter: A Whisper-based Data Filter for â€œIn-the-Wildâ€ Speech Corpora   Using Utterance-level Multi-Task Classification</h2><p><strong>Authors:William Ravenscroft, George Close, Kit Bower-Morris, Jamie Stacey, Dmitry Sityaev, Kris Y. Hong</strong></p>
<p>Large-scale in-the-wild speech datasets have become more prevalent in recent years due to increased interest in models that can learn useful features from unlabelled data for tasks such as speech recognition or synthesis. These datasets often contain undesirable features, such as multiple speakers, non-target languages, and music, which may impact model learning. The Whilter model is proposed as a multitask solution to identify these undesirable samples. Whilter uses a Whisper encoder with an attention-based classifier to solve five diverse classification problems at once. In addition, an annotated dataset is published for a subset of two popular in-the-wild corpora. Whilter achieves F1 scores above 85% and equal error rates of 6.5% to 7.8% for three of five subtasks, outperforming a state-of-the-art BEATs classifier on speech-specific classes, with a notable decrease in processing time compared to a combination of single-task alternatives. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç”±äºæ¨¡å‹å¯¹ä»éæ ‡è®°æ•°æ®ä¸­å­¦ä¹ æœ‰ç”¨ç‰¹å¾ä»¥ç”¨äºè¯­éŸ³è¯†åˆ«æˆ–åˆæˆç­‰ä»»åŠ¡çš„å…´è¶£å¢åŠ ï¼Œå¤§è§„æ¨¡é‡å¤–è¯­éŸ³æ•°æ®é›†å˜å¾—æ›´åŠ æ™®éã€‚è¿™äº›æ•°æ®é›†å¾€å¾€åŒ…å«ä¸€äº›ä¸è‰¯ç‰¹å¾ï¼Œå¦‚å¤šä¸ªè¯´è¯è€…ã€éç›®æ ‡è¯­è¨€å’ŒéŸ³ä¹ç­‰ï¼Œå¯èƒ½ä¼šå½±å“æ¨¡å‹å­¦ä¹ ã€‚å› æ­¤æå‡ºäº†Whilteræ¨¡å‹ä½œä¸ºå¤šä»»åŠ¡è§£å†³æ–¹æ¡ˆæ¥è¯†åˆ«è¿™äº›ä¸è‰¯æ ·æœ¬ã€‚Whilterä½¿ç”¨ä¸€ä¸ªå¸¦æœ‰æ³¨æ„åŠ›åˆ†ç±»å™¨çš„Whisperç¼–ç å™¨åŒæ—¶è§£å†³äº”ä¸ªä¸åŒçš„åˆ†ç±»é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜ä¸ºä¸¤ä¸ªæµè¡Œé‡å¤–è¯­æ–™åº“çš„å­é›†å‘å¸ƒäº†ä¸€ä¸ªå¸¦æ³¨é‡Šçš„æ•°æ®é›†ã€‚å¯¹äºäº”ä¸ªå­ä»»åŠ¡ä¸­çš„ä¸‰ä¸ªä»»åŠ¡ï¼ŒWhilterçš„F1å¾—åˆ†é«˜äº85%ï¼Œå¹¶ä¸”å¯¹äºä¸‰ä¸ªå­ä»»åŠ¡çš„é”™è¯¯ç‡è¾¾åˆ°äº†6.5%è‡³7.8%ï¼Œåœ¨è¯­éŸ³ç‰¹å®šç±»åˆ«ä¸Šè¶…è¿‡äº†æœ€å…ˆè¿›çš„BEATsåˆ†ç±»å™¨ï¼Œå¹¶ä¸”ç›¸è¾ƒäºå•ä¸€ä»»åŠ¡çš„ç»„åˆå¤„ç†æ–¹å¼ï¼Œå¤„ç†æ—¶é—´æ˜¾è‘—å‡å°‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21642v1">PDF</a> Accepted for Interspeech 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€å¯¹èƒ½ä»æœªæ ‡è®°æ•°æ®ä¸­å­¦ä¹ æœ‰ç”¨ç‰¹å¾çš„æ¨¡å‹çš„å…´è¶£å¢åŠ ï¼Œç”¨äºè¯­éŸ³è¯†åˆ«æˆ–åˆæˆç­‰ä»»åŠ¡çš„å¤§è§„æ¨¡é‡å¤–è¯­éŸ³æ•°æ®é›†è¿‘å¹´æ¥å˜å¾—æ›´åŠ æ™®éã€‚è¿™äº›æ•°æ®é›†å¾€å¾€åŒ…å«ä¸åˆ©çš„ç‰¹å¾ï¼Œå¦‚å¤šä¸ªè¯´è¯è€…ã€éç›®æ ‡è¯­è¨€å’ŒéŸ³ä¹ç­‰ï¼Œå¯èƒ½ä¼šå½±å“æ¨¡å‹çš„å­¦ä¹ ã€‚Whilteræ¨¡å‹è¢«æå‡ºä½œä¸ºä¸€ç§å¤šä»»åŠ¡è§£å†³æ–¹æ¡ˆï¼Œç”¨äºè¯†åˆ«è¿™äº›ä¸åˆ©çš„æ ·æœ¬ã€‚Whilterä½¿ç”¨ä¸€ä¸ªWhisperç¼–ç å™¨å’Œä¸€ä¸ªåŸºäºæ³¨æ„åŠ›çš„åˆ†ç±»å™¨æ¥è§£å†³äº”ä¸ªä¸åŒçš„åˆ†ç±»é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿˜ä¸ºä¸¤ä¸ªæµè¡Œçš„é‡å¤–è¯­æ–™åº“çš„ä¸€ä¸ªå­é›†å‘å¸ƒäº†ä¸€ä¸ªæ³¨é‡Šæ•°æ®é›†ã€‚Whifteråœ¨äº”ä¸ªå­ä»»åŠ¡ä¸­çš„ä¸‰ä¸ªä¸Šå®ç°äº†è¶…è¿‡85%çš„F1åˆ†æ•°å’Œ6.5%è‡³7.8%çš„ç­‰æ•ˆé”™è¯¯ç‡ï¼Œåœ¨è¯­éŸ³ç‰¹å®šç±»åˆ«ä¸Šä¼˜äºæœ€æ–°çš„BEATsåˆ†ç±»å™¨ï¼Œä¸å¤„ç†å•ä¸ªä»»åŠ¡çš„ç»„åˆç›¸æ¯”ï¼Œå¤„ç†æ—¶é—´æ˜¾è‘—å‡å°‘ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>å¤§è§„æ¨¡é‡å¤–è¯­éŸ³æ•°æ®é›†çš„æ™®åŠæ¨åŠ¨äº†å¯¹èƒ½ä»æœªæ ‡è®°æ•°æ®ä¸­å­¦ä¹ æœ‰ç”¨ç‰¹å¾çš„æ¨¡å‹çš„éœ€æ±‚ã€‚</li>
<li>è¿™äº›æ•°æ®é›†å¸¸å¸¸åŒ…å«ä¸åˆ©çš„ç‰¹å¾ï¼Œå¦‚å¤šä¸ªè¯´è¯è€…ã€éç›®æ ‡è¯­è¨€å’ŒéŸ³ä¹ã€‚</li>
<li>Whilteræ¨¡å‹æ˜¯ä¸€ç§å¤šä»»åŠ¡è§£å†³æ–¹æ¡ˆï¼Œç”¨äºè¯†åˆ«è¿™äº›ä¸åˆ©çš„æ ·æœ¬ã€‚</li>
<li>Whilterä½¿ç”¨Whisperç¼–ç å™¨å’ŒåŸºäºæ³¨æ„åŠ›çš„åˆ†ç±»å™¨æ¥è§£å†³äº”ä¸ªä¸åŒçš„åˆ†ç±»é—®é¢˜ã€‚</li>
<li>ä¸€ä¸ªæ³¨é‡Šæ•°æ®é›†è¢«å‘å¸ƒç”¨äºä¸¤ä¸ªæµè¡Œé‡å¤–è¯­æ–™åº“çš„ä¸€ä¸ªå­é›†ã€‚</li>
<li>Whifteråœ¨å¤šä¸ªå…³é”®æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå®ç°äº†é«˜F1åˆ†æ•°å’Œè¾ƒä½çš„ç­‰æ•ˆé”™è¯¯ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21642">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2c77d12a0646a1e20b8ac9694af8024a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc3d957096091423d714d51cf43f9dc8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c1d0a3e7dbdcaf2e19e229881e5a509.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf9928700545cec4918245bcf418d6e3.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Real-Time-Audio-Visual-Speech-Enhancement-Using-Pre-trained-Visual-Representations"><a href="#Real-Time-Audio-Visual-Speech-Enhancement-Using-Pre-trained-Visual-Representations" class="headerlink" title="Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual   Representations"></a>Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual   Representations</h2><p><strong>Authors: Teng,  Ma, Sile Yin, Li-Chia Yang, Shuo Zhang</strong></p>
<p>Speech enhancement in audio-only settings remains challenging, particularly in the presence of interfering speakers. This paper presents a simple yet effective real-time audio-visual speech enhancement (AVSE) system, RAVEN, which isolates and enhances the on-screen target speaker while suppressing interfering speakers and background noise. We investigate how visual embeddings learned from audio-visual speech recognition (AVSR) and active speaker detection (ASD) contribute to AVSE across different SNR conditions and numbers of interfering speakers. Our results show concatenating embeddings from AVSR and ASD models provides the greatest improvement in low-SNR, multi-speaker environments, while AVSR embeddings alone perform best in noise-only scenarios. In addition, we develop a real-time streaming system that operates on a computer CPU and we provide a video demonstration and code repository. To our knowledge, this is the first open-source implementation of a real-time AVSE system. </p>
<blockquote>
<p>åœ¨åªæœ‰éŸ³é¢‘çš„è®¾å®šä¸­ï¼Œè¯­éŸ³å¢å¼ºä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨å¹²æ‰°è¯´è¯è€…çš„æƒ…å†µä¸‹ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å®æ—¶è§†å¬è¯­éŸ³å¢å¼ºï¼ˆAVSEï¼‰ç³»ç»Ÿï¼Œåä¸ºRAVENï¼Œå®ƒèƒ½éš”ç¦»å¹¶å¢å¼ºå±å¹•ä¸Šçš„ç›®æ ‡è¯´è¯è€…ï¼ŒåŒæ—¶æŠ‘åˆ¶å¹²æ‰°è¯´è¯è€…å’ŒèƒŒæ™¯å™ªéŸ³ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä»è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰å’Œä¸»åŠ¨è¯´è¯è€…æ£€æµ‹ï¼ˆASDï¼‰ä¸­å­¦ä¹ åˆ°çš„è§†è§‰åµŒå…¥å¦‚ä½•åœ¨ä¸åŒä¿¡å™ªæ¯”æ¡ä»¶å’Œå¹²æ‰°è¯´è¯è€…æ•°é‡ä¸‹å¯¹AVSEåšå‡ºè´¡çŒ®ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨ä¿¡å™ªæ¯”ä½ã€å¤šè¯´è¯è€…çš„ç¯å¢ƒä¸­ï¼Œæ¥è‡ªAVSRå’ŒASDæ¨¡å‹çš„åµŒå…¥ç‰©ä¸²è”æä¾›äº†æœ€å¤§çš„æ”¹è¿›ï¼Œè€Œåœ¨åªæœ‰å™ªéŸ³çš„æƒ…å†µä¸‹ï¼ŒAVSRåµŒå…¥ç‰©å•ç‹¬è¡¨ç°æœ€å¥½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯åœ¨è®¡ç®—æœºCPUä¸Šè¿è¡Œçš„å®æ—¶æµåª’ä½“ç³»ç»Ÿï¼Œå¹¶æä¾›äº†è§†é¢‘æ¼”ç¤ºå’Œä»£ç ä»“åº“ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®æ—¶AVSEç³»ç»Ÿçš„å¼€æºå®ç°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21448v1">PDF</a> Accepted into Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„å®æ—¶è§†å¬è¯­éŸ³å¢å¼ºç³»ç»Ÿï¼ˆAVSEï¼‰â€”â€”RAVENï¼Œå®ƒèƒ½éš”ç¦»å¹¶å¢å¼ºå±å¹•ä¸Šçš„ç›®æ ‡è¯´è¯è€…å£°éŸ³ï¼ŒåŒæ—¶æŠ‘åˆ¶å¹²æ‰°è¯´è¯è€…å’ŒèƒŒæ™¯å™ªéŸ³ã€‚ç ”ç©¶æ¢è®¨äº†è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰å’Œä¸»åŠ¨è¯´è¯è€…æ£€æµ‹ï¼ˆASDï¼‰çš„è§†è§‰åµŒå…¥å¯¹AVSEåœ¨ä¸åŒä¿¡å™ªæ¯”æ¡ä»¶å’Œå¹²æ‰°è¯´è¯è€…æ•°é‡ä¸‹çš„è´¡çŒ®ã€‚ç»“æœæ˜¾ç¤ºï¼Œåœ¨ä¿¡å™ªæ¯”ä½ã€å¤šè¯´è¯äººçš„ç¯å¢ƒä¸­ï¼Œç»“åˆAVSRå’ŒASDæ¨¡å‹çš„åµŒå…¥ä¿¡æ¯è¡¨ç°æœ€ä½³ï¼›è€Œä»…ä½¿ç”¨AVSRåµŒå…¥ä¿¡æ¯åœ¨ä»…å­˜åœ¨å™ªå£°çš„åœºæ™¯ä¸­è¡¨ç°æœ€å¥½ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼€å‘äº†ä¸€ä¸ªå®æ—¶æµå¼ç³»ç»Ÿï¼Œå¯åœ¨è®¡ç®—æœºCPUä¸Šè¿è¡Œï¼Œå¹¶æä¾›è§†é¢‘æ¼”ç¤ºå’Œä»£ç åº“ã€‚è¿™æ˜¯é¦–ä¸ªå¼€æºçš„å®æ—¶AVSEç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³å¢å¼ºåœ¨ä»…æœ‰éŸ³é¢‘çš„ç¯å¢ƒä¸‹ä»å…·æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å­˜åœ¨å¹²æ‰°è¯´è¯è€…çš„æƒ…å†µä¸‹ã€‚</li>
<li>RAVENç³»ç»Ÿæ˜¯ä¸€ä¸ªå®æ—¶è§†å¬è¯­éŸ³å¢å¼ºç³»ç»Ÿï¼Œèƒ½å¤Ÿéš”ç¦»å¹¶å¢å¼ºç›®æ ‡è¯´è¯è€…çš„å£°éŸ³ã€‚</li>
<li>ç ”ç©¶é‡ç‚¹æ¢è®¨äº†è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰å’Œä¸»åŠ¨è¯´è¯è€…æ£€æµ‹ï¼ˆASDï¼‰çš„è§†è§‰åµŒå…¥å¦‚ä½•å¯¹AVSEåšå‡ºè´¡çŒ®ã€‚</li>
<li>åœ¨ä¸åŒçš„ä¿¡å™ªæ¯”æ¡ä»¶å’Œå¹²æ‰°è¯´è¯è€…æ•°é‡ä¸‹ï¼Œç»“åˆAVSRå’ŒASDæ¨¡å‹çš„åµŒå…¥ä¿¡æ¯è¡¨ç°æœ€ä½³ã€‚</li>
<li>ä»…ä½¿ç”¨AVSRåµŒå…¥ä¿¡æ¯åœ¨ä»…å­˜åœ¨å™ªå£°çš„åœºæ™¯ä¸­è¡¨ç°æœ€å¥½ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªå¯åœ¨è®¡ç®—æœºCPUä¸Šè¿è¡Œçš„å®æ—¶æµå¼ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c2d8fc9d14064fa434b10ea777d0218b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccb7a4863ac98099675f2e8d020b1d26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-52a76c793b5edeeafae6751fd28e31a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-027184c5e56c67cc641549bf7039f859.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="A-Deep-Learning-Automatic-Speech-Recognition-Model-for-Shona-Language"><a href="#A-Deep-Learning-Automatic-Speech-Recognition-Model-for-Shona-Language" class="headerlink" title="A Deep Learning Automatic Speech Recognition Model for Shona Language"></a>A Deep Learning Automatic Speech Recognition Model for Shona Language</h2><p><strong>Authors:Leslie Wellington Sirora, Mainford Mutandavari</strong></p>
<p>This study presented the development of a deep learning-based Automatic Speech Recognition system for Shona, a low-resource language characterized by unique tonal and grammatical complexities. The research aimed to address the challenges posed by limited training data, lack of labelled data, and the intricate tonal nuances present in Shona speech, with the objective of achieving significant improvements in recognition accuracy compared to traditional statistical models. The research first explored the feasibility of using deep learning to develop an accurate ASR system for Shona. Second, it investigated the specific challenges involved in designing and implementing deep learning architectures for Shona speech recognition and proposed strategies to mitigate these challenges. Lastly, it compared the performance of the deep learning-based model with existing statistical models in terms of accuracy. The developed ASR system utilized a hybrid architecture consisting of a Convolutional Neural Network for acoustic modelling and a Long Short-Term Memory network for language modelling. To overcome the scarcity of data, data augmentation techniques and transfer learning were employed. Attention mechanisms were also incorporated to accommodate the tonal nature of Shona speech. The resulting ASR system achieved impressive results, with a Word Error Rate of 29%, Phoneme Error Rate of 12%, and an overall accuracy of 74%. These metrics indicated the potential of deep learning to enhance ASR accuracy for under-resourced languages like Shona. This study contributed to the advancement of ASR technology for under-resourced languages like Shona, ultimately fostering improved accessibility and communication for Shona speakers worldwide. </p>
<blockquote>
<p>æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„ç”¨äºè¯†åˆ«ç»çº³è¯­çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿã€‚ç»çº³è¯­æ˜¯ä¸€ç§å…·æœ‰ç‹¬ç‰¹è¯­è°ƒåŠè¯­æ³•å¤æ‚æ€§çš„èµ„æºåŒ®ä¹çš„è¯­è¨€ã€‚ç ”ç©¶æ—¨åœ¨è§£å†³ç”±æœ‰é™çš„è®­ç»ƒæ•°æ®å’Œç¼ºä¹æ ‡è®°æ•°æ®æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä»¥åŠç»çº³è¯­ä¸­å¤æ‚çš„è¯­è°ƒç»†å¾®å·®åˆ«ï¼Œç›®æ ‡æ˜¯å®ç°ä¸ä¼ ç»Ÿç»Ÿè®¡æ¨¡å‹ç›¸æ¯”åœ¨è¯†åˆ«å‡†ç¡®æ€§æ–¹é¢çš„æ˜¾è‘—æ”¹è¿›ã€‚è¯¥ç ”ç©¶é¦–å…ˆæ¢è®¨äº†ä½¿ç”¨æ·±åº¦å­¦ä¹ å¼€å‘å‡†ç¡®çš„ç»çº³è¯­è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿçš„å¯è¡Œæ€§ã€‚å…¶æ¬¡ï¼Œå®ƒç ”ç©¶äº†åœ¨è®¾è®¡å’Œå®æ–½ç»çº³è¯­éŸ³è¯†åˆ«çš„æ·±åº¦å­¦ä¹ æ¶æ„è¿‡ç¨‹ä¸­æ‰€é¢ä¸´çš„ç‰¹å®šæŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜çš„ç­–ç•¥ã€‚æœ€åï¼Œåœ¨å‡†ç¡®æ€§æ–¹é¢ï¼Œå°†åŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹ä¸ç°æœ‰çš„ç»Ÿè®¡æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚å¼€å‘çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿé‡‡ç”¨æ··åˆæ¶æ„ï¼ŒåŒ…æ‹¬ç”¨äºå£°å­¦å»ºæ¨¡çš„å·ç§¯ç¥ç»ç½‘ç»œå’Œç”¨äºè¯­è¨€å»ºæ¨¡çš„é•¿çŸ­æœŸè®°å¿†ç½‘ç»œã€‚ä¸ºäº†å…‹æœæ•°æ®ç¼ºä¹çš„é—®é¢˜ï¼Œé‡‡ç”¨äº†æ•°æ®å¢å¼ºæŠ€æœ¯å’Œè¿ç§»å­¦ä¹ ã€‚è¿˜èå…¥äº†æ³¨æ„åŠ›æœºåˆ¶ä»¥é€‚åº”ç»çº³è¯­çš„è¯­è°ƒç‰¹ç‚¹ã€‚ç»“æœå¾—åˆ°çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœï¼Œè¯é”™è¯¯ç‡ä¸º29%ï¼ŒéŸ³ç´ é”™è¯¯ç‡ä¸º12%ï¼Œæ€»ä½“å‡†ç¡®ç‡ä¸º74%ã€‚è¿™äº›æŒ‡æ ‡è¡¨æ˜æ·±åº¦å­¦ä¹ åœ¨æé«˜èµ„æºåŒ®ä¹è¯­è¨€ï¼ˆå¦‚ç»çº³è¯­ï¼‰çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å‡†ç¡®æ€§æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚æœ¬ç ”ç©¶æ¨åŠ¨äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯åœ¨èµ„æºåŒ®ä¹è¯­è¨€ï¼ˆå¦‚ç»çº³è¯­ï¼‰é¢†åŸŸçš„å‘å±•ï¼Œæœ€ç»ˆä¿ƒè¿›äº†å…¨çƒç»çº³è¯­ä½¿ç”¨è€…çš„å¯è®¿é—®æ€§å’Œäº¤æµã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21331v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œç”¨äºå¯¹å…·æœ‰ç‹¬ç‰¹éŸ³è°ƒå’Œè¯­æ³•å¤æ‚æ€§çš„ä½èµ„æºè¯­è¨€Shonaè¿›è¡Œè¯†åˆ«ã€‚ç ”ç©¶æ—¨åœ¨è§£å†³ç”±æœ‰é™è®­ç»ƒæ•°æ®å’Œç¼ºä¹æ ‡ç­¾æ•°æ®æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œä»¥åŠShonaè¯­éŸ³ä¸­å¤æ‚çš„éŸ³è°ƒç»†å¾®å·®åˆ«ã€‚é€šè¿‡ä¸ä¼ ç»Ÿçš„ç»Ÿè®¡æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥ç ”ç©¶åœ¨è¯†åˆ«å‡†ç¡®æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚ç ”ç©¶è¿˜æ¢è®¨äº†ä¸ºShonaè¯­éŸ³è¯†åˆ«è®¾è®¡å’Œå®ç°æ·±åº¦å­¦ä¹ æ¶æ„æ‰€é¢ä¸´çš„ç‰¹å®šæŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†åº”å¯¹ç­–ç•¥ã€‚æœ€ç»ˆå¼€å‘çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿé‡‡ç”¨æ··åˆæ¶æ„ï¼ŒåŒ…æ‹¬ç”¨äºå£°å­¦å»ºæ¨¡çš„å·ç§¯ç¥ç»ç½‘ç»œå’Œç”¨äºè¯­è¨€å»ºæ¨¡çš„é•¿çŸ­æœŸè®°å¿†ç½‘ç»œã€‚é€šè¿‡æ•°æ®å¢å¼ºæŠ€æœ¯å’Œè¿ç§»å­¦ä¹ æ¥å…‹æœæ•°æ®ç¨€ç¼ºé—®é¢˜ï¼Œå¹¶èå…¥æ³¨æ„åŠ›æœºåˆ¶ä»¥é€‚åº”Shonaè¯­éŸ³çš„éŸ³è°ƒç‰¹æ€§ã€‚ç»“æœå–å¾—çš„å•è¯é”™è¯¯ç‡ä¸º29%ï¼ŒéŸ³ç´ é”™è¯¯ç‡ä¸º12%ï¼Œæ€»ä½“å‡†ç¡®ç‡ä¸º74%ï¼Œè¡¨æ˜äº†æ·±åº¦å­¦ä¹ åœ¨æé«˜ä½èµ„æºè¯­è¨€è¯­éŸ³è¯†åˆ«å‡†ç¡®æ€§æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³Shonaè¯­è¨€ï¼ˆä¸€ç§å…·æœ‰ç‹¬ç‰¹éŸ³è°ƒå’Œè¯­æ³•å¤æ‚æ€§çš„ä½èµ„æºè¯­è¨€ï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶é€šè¿‡æ·±åº¦å­¦ä¹ æ–¹æ³•å¼€å‘äº†ä¸€ç§æ–°çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œé’ˆå¯¹Shonaè¯­è¨€çš„ç‰¹æ€§è¿›è¡Œäº†ä¼˜åŒ–ã€‚</li>
<li>ç ”ç©¶é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬æœ‰é™è®­ç»ƒæ•°æ®ã€ç¼ºä¹æ ‡ç­¾æ•°æ®å’ŒShonaè¯­éŸ³ä¸­çš„éŸ³è°ƒç»†å¾®å·®åˆ«ã€‚</li>
<li>ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œç ”ç©¶é‡‡ç”¨äº†æ•°æ®å¢å¼ºæŠ€æœ¯å’Œè¿ç§»å­¦ä¹ ï¼Œå¹¶èå…¥äº†æ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>å¼€å‘çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå–å¾—äº†æ˜¾è‘—çš„è¯†åˆ«å‡†ç¡®æ€§ï¼Œå•è¯é”™è¯¯ç‡ä¸º29%ï¼ŒéŸ³ç´ é”™è¯¯ç‡ä¸º12%ï¼Œæ€»ä½“å‡†ç¡®ç‡ä¸º74%ã€‚</li>
<li>è¯¥ç ”ç©¶å±•ç¤ºäº†æ·±åº¦å­¦ä¹ åœ¨ä½èµ„æºè¯­è¨€è¯­éŸ³è¯†åˆ«æ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21331">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8849d6872b3140822eef10a0f30459a9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dbdb60e43b30d86f3502299b7f27dee.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-126f3e23e77635844705df32df2dd1d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d93820c366fb1f1a3b128f4f31cf2a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b536d85718b592147f9ec9adb2082aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-303fea7c439996998f2e8f95b9eeba52.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="JAM-A-Tiny-Flow-based-Song-Generator-with-Fine-grained-Controllability-and-Aesthetic-Alignment"><a href="#JAM-A-Tiny-Flow-based-Song-Generator-with-Fine-grained-Controllability-and-Aesthetic-Alignment" class="headerlink" title="JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability   and Aesthetic Alignment"></a>JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability   and Aesthetic Alignment</h2><p><strong>Authors:Renhang Liu, Chia-Yu Hung, Navonil Majumder, Taylor Gautreaux, Amir Ali Bagherzadeh, Chuan Li, Dorien Herremans, Soujanya Poria</strong></p>
<p>Diffusion and flow-matching models have revolutionized automatic text-to-audio generation in recent times. These models are increasingly capable of generating high quality and faithful audio outputs capturing to speech and acoustic events. However, there is still much room for improvement in creative audio generation that primarily involves music and songs. Recent open lyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an acceptable standard in automatic song generation for recreational use. However, these models lack fine-grained word-level controllability often desired by musicians in their workflows. To the best of our knowledge, our flow-matching-based JAM is the first effort toward endowing word-level timing and duration control in song generation, allowing fine-grained vocal control. To enhance the quality of generated songs to better align with human preferences, we implement aesthetic alignment through Direct Preference Optimization, which iteratively refines the model using a synthetic dataset, eliminating the need or manual data annotations. Furthermore, we aim to standardize the evaluation of such lyrics-to-song models through our public evaluation dataset JAME. We show that JAM outperforms the existing models in terms of the music-specific attributes. </p>
<blockquote>
<p>æ‰©æ•£å’ŒæµåŒ¹é…æ¨¡å‹æœ€è¿‘å½»åº•æ”¹å˜äº†è‡ªåŠ¨æ–‡æœ¬åˆ°éŸ³é¢‘çš„ç”Ÿæˆã€‚è¿™äº›æ¨¡å‹è¶Šæ¥è¶Šèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡å’Œå¿ å®çš„éŸ³é¢‘è¾“å‡ºï¼Œæ•æ‰åˆ°è¯­éŸ³å’Œå£°éŸ³äº‹ä»¶ã€‚ç„¶è€Œï¼Œåœ¨ä¸»è¦æ¶‰åŠéŸ³ä¹å’Œæ­Œæ›²çš„åˆ›é€ æ€§éŸ³é¢‘ç”Ÿæˆæ–¹é¢ï¼Œä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æœ€è¿‘çš„å¼€æ”¾æ­Œè¯åˆ°æ­Œæ›²æ¨¡å‹ï¼Œå¦‚DiffRhythmã€ACE-Stepå’ŒLeVoï¼Œåœ¨å¨±ä¹ç”¨é€”çš„è‡ªåŠ¨æ­Œæ›²ç”Ÿæˆæ–¹é¢å·²ç»è®¾å®šäº†å¯æ¥å—çš„æ ‡å‡†ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ç¼ºä¹éŸ³ä¹å®¶åœ¨å…¶å·¥ä½œæµç¨‹ä¸­é€šå¸¸æ‰€éœ€çš„ç²¾ç»†çš„è¯è¯­çº§æ§åˆ¶åŠ›ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„åŸºäºæµåŒ¹é…çš„JAMæ˜¯é¦–æ¬¡åŠªåŠ›èµ‹äºˆæ­Œæ›²ç”Ÿæˆä¸­è¯è¯­çº§çš„æ—¶åºå’ŒæŒç»­æ—¶é—´æ§åˆ¶ï¼Œå…è®¸ç²¾ç»†çš„è¯­éŸ³æ§åˆ¶ã€‚ä¸ºäº†æé«˜ç”Ÿæˆæ­Œæ›²çš„è´¨é‡ï¼Œä»¥æ›´å¥½åœ°ç¬¦åˆäººç±»åå¥½ï¼Œæˆ‘ä»¬é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–å®ç°äº†å®¡ç¾å¯¹é½ï¼Œè¿™è¿­ä»£åœ°æ”¹è¿›äº†æ¨¡å‹ï¼Œä½¿ç”¨åˆæˆæ•°æ®é›†ï¼Œæ¶ˆé™¤äº†å¯¹æ‰‹åŠ¨æ•°æ®æ³¨é‡Šçš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡æˆ‘ä»¬çš„å…¬å…±è¯„ä¼°æ•°æ®é›†JAMEæ¥æ ‡å‡†åŒ–æ­¤ç±»æ­Œè¯åˆ°æ­Œæ›²æ¨¡å‹çš„è¯„ä¼°ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨ç‰¹å®šçš„éŸ³ä¹å±æ€§æ–¹é¢ï¼ŒJAMçš„è¡¨ç°è¶…è¿‡äº†ç°æœ‰æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20880v1">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/declare-lab/jamify">https://github.com/declare-lab/jamify</a></p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸï¼Œæ‰©æ•£å’Œæµå¼åŒ¹é…æ¨¡å‹åœ¨è‡ªåŠ¨æ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆé¢†åŸŸå–å¾—äº†é©å‘½æ€§çš„è¿›å±•ã€‚è¿™äº›æ¨¡å‹èƒ½ç”Ÿæˆé«˜è´¨é‡ã€é€¼çœŸçš„éŸ³é¢‘è¾“å‡ºï¼Œä½†åœ¨éŸ³ä¹åˆ›ä½œç­‰åˆ›é€ æ€§éŸ³é¢‘ç”Ÿæˆæ–¹é¢ä»æœ‰æå‡ç©ºé—´ã€‚ç°æœ‰çš„è‡ªåŠ¨æ­Œæ›²ç”Ÿæˆæ¨¡å‹å¦‚DiffRhythmã€ACE-Stepå’ŒLeVoç­‰è™½å·²è®¾å®šäº†å¨±ä¹ç”¨é€”çš„è‡ªåŠ¨æ­Œæ›²ç”Ÿæˆæ ‡å‡†ï¼Œä½†å®ƒä»¬ç¼ºä¹éŸ³ä¹å®¶å·¥ä½œæµç¨‹ä¸­é€šå¸¸æ‰€éœ€çš„ç²¾ç»†è¯çº§æ§åˆ¶ã€‚æˆ‘ä»¬çš„åŸºäºæµå¼åŒ¹é…çš„JAMæ¨¡å‹é¦–æ¬¡å®ç°äº†æ­Œæ›²ç”Ÿæˆä¸­çš„è¯çº§æ—¶åºå’ŒæŒç»­æ—¶é—´æ§åˆ¶ï¼Œå®ç°äº†ç²¾ç»†çš„å£°ä¹æ§åˆ¶ã€‚ä¸ºæé«˜ç”Ÿæˆæ­Œæ›²çš„è´¨é‡ä»¥æ›´å¥½åœ°ç¬¦åˆäººç±»åå¥½ï¼Œæˆ‘ä»¬é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–å®æ–½ç¾å­¦å¯¹é½ï¼Œä½¿ç”¨åˆæˆæ•°æ®é›†è¿­ä»£ä¼˜åŒ–æ¨¡å‹ï¼Œæ— éœ€æ‰‹åŠ¨æ•°æ®æ ‡æ³¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è‡´åŠ›äºé€šè¿‡å…¬å¼€è¯„ä¼°æ•°æ®é›†JAMEæ¥ç»Ÿä¸€æ­¤ç±»æ­Œè¯åˆ°æ­Œæ›²æ¨¡å‹çš„è¯„ä¼°æ ‡å‡†ã€‚å®éªŒè¡¨æ˜ï¼ŒJAMåœ¨éŸ³ä¹ç‰¹å®šå±æ€§æ–¹é¢ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£å’Œæµå¼åŒ¹é…æ¨¡å‹åœ¨è‡ªåŠ¨æ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆä¸­å–å¾—æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯­éŸ³å’Œå£°éŸ³äº‹ä»¶çš„ç”Ÿæˆæ–¹é¢ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨æ­Œæ›²ç”Ÿæˆæ¨¡å‹å¦‚DiffRhythmã€ACE-Stepå’ŒLeVoç­‰åœ¨å¨±ä¹ç”¨é€”ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†ç¼ºä¹è¯çº§æ§åˆ¶åŠŸèƒ½ã€‚</li>
<li>JAMæ¨¡å‹é¦–æ¬¡å®ç°æ­Œæ›²ç”Ÿæˆä¸­çš„è¯çº§æ—¶åºå’ŒæŒç»­æ—¶é—´æ§åˆ¶ï¼Œä¸ºéŸ³ä¹å®¶æä¾›æ›´ç²¾ç»†çš„å£°ä¹æ§åˆ¶ã€‚</li>
<li>é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–å®æ–½ç¾å­¦å¯¹é½ï¼Œä½¿ç”¨åˆæˆæ•°æ®é›†è¿­ä»£ä¼˜åŒ–æ¨¡å‹ï¼Œæé«˜ç”Ÿæˆæ­Œæ›²è´¨é‡ã€‚</li>
<li>æå‡ºå…¬å¼€è¯„ä¼°æ•°æ®é›†JAMEï¼Œä»¥ç»Ÿä¸€æ­Œè¯åˆ°æ­Œæ›²æ¨¡å‹çš„è¯„ä¼°æ ‡å‡†ã€‚</li>
<li>JAMæ¨¡å‹åœ¨éŸ³ä¹ç‰¹å®šå±æ€§æ–¹é¢ä¼˜äºç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4fa05f2e488cb828871f350df8352497.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b1a9a0bb7edb52f25d2bb395e43ce77e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-10f92e8a2018f719f4afefef871c6e6c.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Learning-Phonetic-Context-Dependent-Viseme-for-Enhancing-Speech-Driven-3D-Facial-Animation"><a href="#Learning-Phonetic-Context-Dependent-Viseme-for-Enhancing-Speech-Driven-3D-Facial-Animation" class="headerlink" title="Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven   3D Facial Animation"></a>Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven   3D Facial Animation</h2><p><strong>Authors:Hyung Kyu Kim, Hak Gu Kim</strong></p>
<p>Speech-driven 3D facial animation aims to generate realistic facial movements synchronized with audio. Traditional methods primarily minimize reconstruction loss by aligning each frame with ground-truth. However, this frame-wise approach often fails to capture the continuity of facial motion, leading to jittery and unnatural outputs due to coarticulation. To address this, we propose a novel phonetic context-aware loss, which explicitly models the influence of phonetic context on viseme transitions. By incorporating a viseme coarticulation weight, we assign adaptive importance to facial movements based on their dynamic changes over time, ensuring smoother and perceptually consistent animations. Extensive experiments demonstrate that replacing the conventional reconstruction loss with ours improves both quantitative metrics and visual quality. It highlights the importance of explicitly modeling phonetic context-dependent visemes in synthesizing natural speech-driven 3D facial animation. Project page: <a target="_blank" rel="noopener" href="https://cau-irislab.github.io/interspeech25/">https://cau-irislab.github.io/interspeech25/</a> </p>
<blockquote>
<p>è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»æ—¨åœ¨ç”Ÿæˆä¸éŸ³é¢‘åŒæ­¥çš„çœŸå®é¢éƒ¨è¿åŠ¨ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é€šè¿‡å°†æ¯ä¸€å¸§ä¸åœ°é¢çœŸå®æ•°æ®è¿›è¡Œæ¯”å¯¹æ¥æœ€å°åŒ–é‡å»ºæŸå¤±ã€‚ç„¶è€Œï¼Œè¿™ç§é€å¸§çš„æ–¹æ³•å¾€å¾€æ— æ³•æ•æ‰åˆ°é¢éƒ¨è¿åŠ¨çš„è¿ç»­æ€§ï¼Œå¯¼è‡´ç”±äºååŒå‘éŸ³è€Œäº§ç”ŸæŠ–åŠ¨å’Œä¸è‡ªç„¶çš„è¾“å‡ºã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹è¯­éŸ³ä¸Šä¸‹æ–‡æ„ŸçŸ¥æŸå¤±ï¼Œè¯¥æŸå¤±èƒ½æ˜ç¡®å»ºæ¨¡è¯­éŸ³ä¸Šä¸‹æ–‡å¯¹å‘éŸ³è¿‡æ¸¡çš„å½±å“ã€‚é€šè¿‡å¼•å…¥å‘éŸ³ååŒæƒé‡ï¼Œæˆ‘ä»¬æ ¹æ®é¢éƒ¨è¿åŠ¨éšæ—¶é—´å˜åŒ–çš„åŠ¨æ€å˜åŒ–ä¸ºå…¶åˆ†é…è‡ªé€‚åº”é‡è¦æ€§ï¼Œç¡®ä¿æ›´å¹³æ»‘å’Œæ„ŸçŸ¥ä¸€è‡´çš„åŠ¨ç”»ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œç”¨æˆ‘ä»¬çš„æ–¹æ³•æ›¿æ¢ä¼ ç»Ÿçš„é‡å»ºæŸå¤±å¯ä»¥æé«˜å®šé‡æŒ‡æ ‡å’Œè§†è§‰è´¨é‡ã€‚å®ƒå¼ºè°ƒäº†æ˜ç¡®å»ºæ¨¡è¯­éŸ³ä¸Šä¸‹æ–‡ç›¸å…³çš„å‘éŸ³åœ¨åˆæˆè‡ªç„¶è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»ä¸­çš„é‡è¦æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cau-irislab.github.io/interspeech25/">https://cau-irislab.github.io/interspeech25/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20568v1">PDF</a> Accepted for Interspeech 2025 Project Page:   <a target="_blank" rel="noopener" href="https://cau-irislab.github.io/interspeech25/">https://cau-irislab.github.io/interspeech25/</a></p>
<p><strong>Summary</strong></p>
<p>è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»æ—¨åœ¨ç”Ÿæˆä¸éŸ³é¢‘åŒæ­¥çš„çœŸå®é¢éƒ¨åŠ¨ä½œã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é€šè¿‡æœ€å°åŒ–é‡å»ºæŸå¤±æ¥å¯¹é½æ¯ä¸€å¸§ä¸çœŸå®å€¼ï¼Œä½†è¿™ç§é€å¸§çš„æ–¹æ³•å¾€å¾€æ— æ³•æ•æ‰é¢éƒ¨åŠ¨ä½œçš„è¿ç»­æ€§ï¼Œå¯¼è‡´è¾“å‡ºåŠ¨ä½œå‡ºç°æŠ–åŠ¨å’Œä¸è‡ªç„¶çš„ç°è±¡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¯­éŸ³å­¦è¯­å¢ƒæ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°æ˜¾å¼åœ°å»ºæ¨¡è¯­éŸ³å­¦è¯­å¢ƒå¯¹é¢éƒ¨åŠ¨ä½œçš„å½±å“ã€‚é€šè¿‡å¼•å…¥é¢éƒ¨åŠ¨ä½œæƒé‡ï¼Œæˆ‘ä»¬æ ¹æ®æ—¶é—´åŠ¨æ€å˜åŒ–å¯¹é¢éƒ¨åŠ¨ä½œè¿›è¡Œè‡ªé€‚åº”é‡è¦æ€§åˆ†é…ï¼Œç¡®ä¿åŠ¨ç”»æ›´åŠ æµç•…ä¸”è§†è§‰æ„ŸçŸ¥ä¸€è‡´ã€‚å®éªŒè¡¨æ˜ï¼Œå°†ä¼ ç»Ÿçš„é‡å»ºæŸå¤±å‡½æ•°æ›¿æ¢ä¸ºæˆ‘ä»¬çš„å‡½æ•°åï¼Œæ—¢èƒ½æå‡å®šé‡æŒ‡æ ‡ï¼Œä¹Ÿèƒ½æé«˜è§†è§‰è´¨é‡ã€‚è¿™å¼ºè°ƒäº†æ˜ç¡®å»ºæ¨¡ä¾èµ–äºè¯­éŸ³å­¦è¯­å¢ƒçš„é¢éƒ¨åŠ¨ä½œåœ¨åˆæˆè‡ªç„¶è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»æ—¨åœ¨ç”Ÿæˆä¸éŸ³é¢‘åŒæ­¥çš„çœŸå®é¢éƒ¨åŠ¨ä½œã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é€šè¿‡å¯¹é½æ¯ä¸€å¸§ä¸çœŸå®å€¼æ¥æœ€å°åŒ–é‡å»ºæŸå¤±ï¼Œä½†è¿™ç§æ–¹æ³•æ— æ³•æ•æ‰é¢éƒ¨åŠ¨ä½œçš„è¿ç»­æ€§ã€‚</li>
<li>æå‡ºçš„è¯­éŸ³å­¦è¯­å¢ƒæ„ŸçŸ¥æŸå¤±å‡½æ•°èƒ½æ˜¾å¼åœ°å»ºæ¨¡è¯­éŸ³å­¦è¯­å¢ƒå¯¹é¢éƒ¨åŠ¨ä½œçš„å½±å“ã€‚</li>
<li>é€šè¿‡å¼•å…¥é¢éƒ¨åŠ¨ä½œæƒé‡ï¼Œè¯¥å‡½æ•°èƒ½ç¡®ä¿åŠ¨ç”»æ›´åŠ æµç•…ä¸”è§†è§‰æ„ŸçŸ¥ä¸€è‡´ã€‚</li>
<li>æ›¿æ¢ä¼ ç»Ÿé‡å»ºæŸå¤±å‡½æ•°åï¼Œæ–°çš„å‡½æ•°èƒ½æé«˜åŠ¨ç”»çš„å®šé‡æŒ‡æ ‡å’Œè§†è§‰è´¨é‡ã€‚</li>
<li>æ˜ç¡®å»ºæ¨¡ä¾èµ–äºè¯­éŸ³å­¦è¯­å¢ƒçš„é¢éƒ¨åŠ¨ä½œå¯¹äºåˆæˆè‡ªç„¶è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20568">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3ce8d4c2f35c5ea3a21f8bc004dda592.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d834e69b32ee4cd770df812a61025d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ff5507140b2cd6e367d151d3a6b2f60.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="MemoryTalker-Personalized-Speech-Driven-3D-Facial-Animation-via-Audio-Guided-Stylization"><a href="#MemoryTalker-Personalized-Speech-Driven-3D-Facial-Animation-via-Audio-Guided-Stylization" class="headerlink" title="MemoryTalker: Personalized Speech-Driven 3D Facial Animation via   Audio-Guided Stylization"></a>MemoryTalker: Personalized Speech-Driven 3D Facial Animation via   Audio-Guided Stylization</h2><p><strong>Authors:Hyung Kyu Kim, Sangmin Lee, Hak Gu Kim</strong></p>
<p>Speech-driven 3D facial animation aims to synthesize realistic facial motion sequences from given audio, matching the speakerâ€™s speaking style. However, previous works often require priors such as class labels of a speaker or additional 3D facial meshes at inference, which makes them fail to reflect the speaking style and limits their practical use. To address these issues, we propose MemoryTalker which enables realistic and accurate 3D facial motion synthesis by reflecting speaking style only with audio input to maximize usability in applications. Our framework consists of two training stages: 1-stage is storing and retrieving general motion (i.e., Memorizing), and 2-stage is to perform the personalized facial motion synthesis (i.e., Animating) with the motion memory stylized by the audio-driven speaking style feature. In this second stage, our model learns about which facial motion types should be emphasized for a particular piece of audio. As a result, our MemoryTalker can generate a reliable personalized facial animation without additional prior information. With quantitative and qualitative evaluations, as well as user study, we show the effectiveness of our model and its performance enhancement for personalized facial animation over state-of-the-art methods. </p>
<blockquote>
<p>è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»æ—¨åœ¨ä»ç»™å®šçš„éŸ³é¢‘ä¸­åˆæˆé€¼çœŸçš„é¢éƒ¨è¿åŠ¨åºåˆ—ï¼Œä»¥åŒ¹é…è¯´è¯è€…çš„è¯´è¯é£æ ¼ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„å·¥ä½œé€šå¸¸éœ€è¦å…ˆéªŒä¿¡æ¯ï¼Œå¦‚è¯´è¯è€…çš„ç±»åˆ«æ ‡ç­¾æˆ–æ¨ç†æ—¶çš„é¢å¤–3Dé¢éƒ¨ç½‘æ ¼ï¼Œè¿™ä½¿å¾—å®ƒä»¬æ— æ³•åæ˜ è¯´è¯é£æ ¼å¹¶é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MemoryTalkerï¼Œå®ƒä»…é€šè¿‡éŸ³é¢‘è¾“å…¥åæ˜ è¯´è¯é£æ ¼ï¼Œå®ç°äº†é€¼çœŸä¸”å‡†ç¡®çš„3Dé¢éƒ¨è¿åŠ¨åˆæˆï¼Œä»¥åœ¨åº”ç”¨ç¨‹åºä¸­æœ€å¤§é™åº¦åœ°æé«˜å¯ç”¨æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç”±ä¸¤ä¸ªè®­ç»ƒé˜¶æ®µç»„æˆï¼šç¬¬ä¸€é˜¶æ®µæ˜¯å­˜å‚¨å’Œæ£€ç´¢ä¸€èˆ¬è¿åŠ¨ï¼ˆå³è®°å¿†ï¼‰ï¼Œç¬¬äºŒé˜¶æ®µæ˜¯æ‰§è¡Œä¸ªæ€§åŒ–çš„é¢éƒ¨è¿åŠ¨åˆæˆï¼ˆå³åŠ¨ç”»ï¼‰ä¸ç”±éŸ³é¢‘é©±åŠ¨çš„è¯´è¯é£æ ¼ç‰¹å¾æ‰€å½¢æˆè¿åŠ¨è®°å¿†ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¹ å¯¹äºç‰¹å®šéŸ³é¢‘åº”å¼ºè°ƒå“ªäº›é¢éƒ¨è¿åŠ¨ç±»å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„MemoryTalkerå¯ä»¥åœ¨æ²¡æœ‰é¢å¤–å…ˆéªŒä¿¡æ¯çš„æƒ…å†µä¸‹ç”Ÿæˆå¯é çš„ä¸ªæ€§åŒ–é¢éƒ¨åŠ¨ç”»ã€‚é€šè¿‡å®šé‡å’Œå®šæ€§è¯„ä¼°ä»¥åŠç”¨æˆ·ç ”ç©¶ï¼Œæˆ‘ä»¬è¯æ˜äº†æ¨¡å‹çš„æœ‰æ•ˆæ€§åŠå…¶åœ¨ä¸ªæ€§åŒ–é¢éƒ¨åŠ¨ç”»æ–¹é¢ç›¸è¾ƒäºæœ€æ–°æ–¹æ³•çš„æ€§èƒ½æå‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20562v1">PDF</a> Accepted for ICCV 2025 Project Page:   <a target="_blank" rel="noopener" href="https://cau-irislab.github.io/ICCV25-MemoryTalker/">https://cau-irislab.github.io/ICCV25-MemoryTalker/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Speech-driven 3Dé¢éƒ¨åŠ¨ç”»æŠ€æœ¯çš„æ–°è¿›å±•ã€‚é’ˆå¯¹ç°æœ‰æŠ€æœ¯éœ€è¦å…ˆéªŒä¿¡æ¯ï¼ˆå¦‚è¯´è¯è€…ç±»åˆ«æ ‡ç­¾æˆ–é¢å¤–çš„3Dé¢éƒ¨ç½‘æ ¼ï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•MemoryTalkerã€‚è¯¥æ–¹æ³•ä»…é€šè¿‡éŸ³é¢‘è¾“å…¥åæ˜ è¯´è¯é£æ ¼ï¼Œå®ç°çœŸå®ä¸”å‡†ç¡®çš„3Dé¢éƒ¨è¿åŠ¨åˆæˆï¼Œæé«˜äº†å®ç”¨æ€§ã€‚è¯¥æ–¹æ³•åŒ…æ‹¬å­˜å‚¨å’Œæ£€ç´¢é€šç”¨è¿åŠ¨çš„é˜¶æ®µï¼ˆå³è®°å¿†é˜¶æ®µï¼‰ä»¥åŠä½¿ç”¨éŸ³é¢‘é©±åŠ¨çš„è¯´è¯é£æ ¼ç‰¹å¾è¿›è¡Œä¸ªæ€§åŒ–é¢éƒ¨è¿åŠ¨åˆæˆï¼ˆå³åŠ¨ç”»é˜¶æ®µï¼‰ã€‚å®éªŒå’Œç”¨æˆ·ç ”ç©¶è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§åŠå…¶åœ¨ä¸ªæ€§åŒ–é¢éƒ¨åŠ¨ç”»æ–¹é¢çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Speech-driven 3Dé¢éƒ¨åŠ¨ç”»æ—¨åœ¨ä»ç»™å®šçš„éŸ³é¢‘ä¸­åˆæˆé€¼çœŸçš„é¢éƒ¨è¿åŠ¨åºåˆ—ï¼ŒåŒ¹é…è¯´è¯è€…çš„è¯´è¯é£æ ¼ã€‚</li>
<li>ç°æœ‰æŠ€æœ¯é€šå¸¸éœ€è¦å…ˆéªŒä¿¡æ¯ï¼Œå¦‚è¯´è¯è€…çš„ç±»åˆ«æ ‡ç­¾æˆ–é¢å¤–çš„3Dé¢éƒ¨ç½‘æ ¼ï¼Œè¿™é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚</li>
<li>MemoryTalkeræ–¹æ³•é€šè¿‡ä»…ä½¿ç”¨éŸ³é¢‘è¾“å…¥åæ˜ è¯´è¯é£æ ¼ï¼Œå®ç°äº†çœŸå®ä¸”å‡†ç¡®çš„3Dé¢éƒ¨è¿åŠ¨åˆæˆã€‚</li>
<li>MemoryTalkeræ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªè®­ç»ƒé˜¶æ®µï¼šå­˜å‚¨å’Œæ£€ç´¢é€šç”¨è¿åŠ¨çš„è®°å¿†é˜¶æ®µä»¥åŠä½¿ç”¨è¿åŠ¨è®°å¿†è¿›è¡Œä¸ªæ€§åŒ–é¢éƒ¨è¿åŠ¨åˆæˆçš„åŠ¨ç”»é˜¶æ®µã€‚</li>
<li>åœ¨åŠ¨ç”»é˜¶æ®µï¼Œæ¨¡å‹å­¦ä¹ äº†é’ˆå¯¹ç‰¹å®šéŸ³é¢‘åº”å¼ºè°ƒå“ªäº›é¢éƒ¨è¿åŠ¨ç±»å‹ã€‚</li>
<li>MemoryTalkerèƒ½å¤Ÿåœ¨æ²¡æœ‰é¢å¤–å…ˆéªŒä¿¡æ¯çš„æƒ…å†µä¸‹ç”Ÿæˆå¯é çš„ä¸ªæ€§åŒ–é¢éƒ¨åŠ¨ç”»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69b78a490d5dfef58d3eff194da1b4a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3111365737c2c38b7927092587ac1150.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2e302943403358e10f8d3735a89acc4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6753032257ed9be2357947392d82b5b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fed63bb3e440157300535d8d122b9b55.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Binaural-Speech-Enhancement-Using-Complex-Convolutional-Recurrent-Networks"><a href="#Binaural-Speech-Enhancement-Using-Complex-Convolutional-Recurrent-Networks" class="headerlink" title="Binaural Speech Enhancement Using Complex Convolutional Recurrent   Networks"></a>Binaural Speech Enhancement Using Complex Convolutional Recurrent   Networks</h2><p><strong>Authors:Vikas Tokala, Eric Grinstein, Mike Brookes, Simon Doclo, Jesper Jensen, Patrick A. Naylor</strong></p>
<p>From hearing aids to augmented and virtual reality devices, binaural speech enhancement algorithms have been established as state-of-the-art techniques to improve speech intelligibility and listening comfort. In this paper, we present an end-to-end binaural speech enhancement method using a complex recurrent convolutional network with an encoder-decoder architecture and a complex LSTM recurrent block placed between the encoder and decoder. A loss function that focuses on the preservation of spatial information in addition to speech intelligibility improvement and noise reduction is introduced. The network estimates individual complex ratio masks for the left and right-ear channels of a binaural hearing device in the time-frequency domain. We show that, compared to other baseline algorithms, the proposed method significantly improves the estimated speech intelligibility and reduces the noise while preserving the spatial information of the binaural signals in acoustic situations with a single target speaker and isotropic noise of various types. </p>
<blockquote>
<p>ä»åŠ©å¬å™¨åˆ°å¢å¼ºå’Œè™šæ‹Ÿç°å®è®¾å¤‡ï¼ŒåŒè€³è¯­éŸ³å¢å¼ºç®—æ³•å·²è¢«ç¡®ç«‹ä¸ºæœ€å…ˆè¿›çš„æŠ€æœ¯ï¼Œç”¨äºæé«˜è¯­éŸ³æ¸…æ™°åº¦å’Œå¬è§‰èˆ’é€‚åº¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„åŒè€³è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å…·æœ‰ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„å¤æ‚å¾ªç¯å·ç§¯ç½‘ç»œï¼Œå¹¶åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´æ”¾ç½®äº†ä¸€ä¸ªå¤æ‚çš„LSTMå¾ªç¯å—ã€‚é™¤äº†æé«˜è¯­éŸ³æ¸…æ™°åº¦å’Œé™ä½å™ªå£°å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°ä¸“æ³¨äºä¿ç•™ç©ºé—´ä¿¡æ¯ã€‚è¯¥ç½‘ç»œåœ¨æ—¶é¢‘åŸŸä¼°è®¡åŒè€³å¬åŠ›è®¾å¤‡çš„å·¦å³å£°é“ä¸ªåˆ«å¤æ‚æ¯”ç‡æ©æ¨¡ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä¸å…¶ä»–åŸºçº¿ç®—æ³•ç›¸æ¯”ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å•ç›®æ ‡è¯´è¯äººå’Œå„ç§ç±»å‹åŒå‘å™ªå£°çš„å£°å­¦æƒ…å†µä¸‹ï¼Œèƒ½æ˜¾è‘—æé«˜ä¼°è®¡çš„è¯­éŸ³æ¸…æ™°åº¦ï¼Œé™ä½å™ªå£°ï¼ŒåŒæ—¶ä¿ç•™åŒè€³ä¿¡å·çš„ç©ºé—´ä¿¡æ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20023v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç«¯åˆ°ç«¯çš„åŒè€³è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œé‡‡ç”¨å¤æ‚å¾ªç¯å·ç§¯ç½‘ç»œï¼Œå…·æœ‰ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå¹¶åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´è®¾ç½®äº†ä¸€ä¸ªå¤æ‚çš„LSTMå¾ªç¯å—ã€‚å¼•å…¥äº†ä¸€ç§æŸå¤±å‡½æ•°ï¼Œä¾§é‡äºä¿ç•™ç©ºé—´ä¿¡æ¯ï¼ŒåŒæ—¶æé«˜è¯­éŸ³æ¸…æ™°åº¦å¹¶å‡å°‘å™ªéŸ³ã€‚è¯¥æ–¹æ³•ä¼°è®¡åŒè€³å¬åŠ›è®¾å¤‡çš„å·¦å³å£°é“çš„å¤æ•°æ¯”ç‡æ©æ¨¡ï¼Œåœ¨å•ç›®æ ‡è¯´è¯äººå’Œå„ç§ç±»å‹åŒå‘å™ªå£°çš„å£°å­¦ç¯å¢ƒä¸­ï¼Œç›¸æ¯”å…¶ä»–åŸºçº¿ç®—æ³•ï¼Œæ˜¾è‘—æ”¹å–„è¯­éŸ³æ¸…æ™°åº¦å¹¶é™ä½å™ªéŸ³ï¼ŒåŒæ—¶ä¿ç•™åŒè€³ä¿¡å·çš„ç©ºé—´ä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç«¯åˆ°ç«¯çš„åŒè€³è¯­éŸ³å¢å¼ºæ–¹æ³•ï¼Œé‡‡ç”¨å¤æ‚å¾ªç¯å·ç§¯ç½‘ç»œå’Œç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´ä½¿ç”¨äº†å¤æ‚çš„LSTMå¾ªç¯å—ä»¥æé«˜è¯­éŸ³æ¸…æ™°åº¦ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°åœ¨ä¿ç•™ç©ºé—´ä¿¡æ¯çš„åŒæ—¶ï¼Œä¸“æ³¨äºæé«˜è¯­éŸ³æ¸…æ™°åº¦å¹¶å‡å°‘å™ªéŸ³ã€‚</li>
<li>è¯¥æ–¹æ³•å¯ä»¥ä¼°è®¡åŒè€³å¬åŠ›è®¾å¤‡çš„å·¦å³å£°é“çš„å¤æ•°æ¯”ç‡æ©æ¨¡ã€‚</li>
<li>åœ¨å•ç›®æ ‡è¯´è¯äººå’Œå„ç§ç±»å‹åŒå‘å™ªå£°çš„å£°å­¦ç¯å¢ƒä¸­ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”å…¶ä»–åŸºçº¿ç®—æ³•æ›´æœ‰æ•ˆã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æ”¹å–„è¯­éŸ³æ¸…æ™°åº¦å¹¶é™ä½å™ªéŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20023">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-137a29b624c91ba40fd1de82cf365ae9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ece4d6c4456b5779314146709bbd2199.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbc3d5cd6aba90e6200b20d0be6da79c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="HITSZâ€™s-End-To-End-Speech-Translation-Systems-Combining-Sequence-to-Sequence-Auto-Speech-Recognition-Model-and-Indic-Large-Language-Model-for-IWSLT-2025-in-Indic-Track"><a href="#HITSZâ€™s-End-To-End-Speech-Translation-Systems-Combining-Sequence-to-Sequence-Auto-Speech-Recognition-Model-and-Indic-Large-Language-Model-for-IWSLT-2025-in-Indic-Track" class="headerlink" title="HITSZâ€™s End-To-End Speech Translation Systems Combining   Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language   Model for IWSLT 2025 in Indic Track"></a>HITSZâ€™s End-To-End Speech Translation Systems Combining   Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language   Model for IWSLT 2025 in Indic Track</h2><p><strong>Authors:Xuchen Wei, Yangxin Wu, Yaoyin Zhang, Henglyu Liu, Kehai Chen, Xuefeng Bai, Min Zhang</strong></p>
<p>This paper presents HITSZâ€™s submission for the IWSLT 2025 Indic track, focusing on speech-to-text translation (ST) for English-to-Indic and Indic-to-English language pairs. To enhance translation quality in this low-resource scenario, we propose an end-to-end system integrating the pre-trained Whisper automated speech recognition (ASR) model with Krutrim, an Indic-specialized large language model (LLM). Experimental results demonstrate that our end-to-end system achieved average BLEU scores of $28.88$ for English-to-Indic directions and $27.86$ for Indic-to-English directions. Furthermore, we investigated the Chain-of-Thought (CoT) method. While this method showed potential for significant translation quality improvements on successfully parsed outputs (e.g. a $13.84$ BLEU increase for Tamil-to-English), we observed challenges in ensuring the model consistently adheres to the required CoT output format. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†HITSZä¸ºIWSLT 2025å°åœ°è¯­èµ›é“æäº¤çš„æ–¹æ¡ˆï¼Œé‡ç‚¹èšç„¦è‹±æ–‡-å°åœ°è¯­å’Œå°åœ°è¯­-è‹±æ–‡çš„è¯­éŸ³åˆ°æ–‡æœ¬ï¼ˆSTï¼‰ç¿»è¯‘ã€‚ä¸ºäº†åœ¨è¿™ç§èµ„æºç¨€ç¼ºçš„åœºæ™¯ä¸­æé«˜ç¿»è¯‘è´¨é‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé›†æˆäº†é¢„è®­ç»ƒçš„Whisperè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹å’ŒKrutrimâ€”â€”ä¸€æ¬¾å°åœ°è¯­ä¸“ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç«¯åˆ°ç«¯ç³»ç»Ÿåœ¨è‹±æ–‡åˆ°å°åœ°è¯­çš„ç¿»è¯‘æ–¹å‘å¹³å‡BLEUå¾—åˆ†ä¸º28.88ï¼Œå°åœ°è¯­åˆ°è‹±æ–‡çš„ç¿»è¯‘æ–¹å‘å¹³å‡BLEUå¾—åˆ†ä¸º27.86ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†æ€ç»´é“¾ï¼ˆCoTï¼‰æ–¹æ³•ã€‚è™½ç„¶è¯¥æ–¹æ³•åœ¨æˆåŠŸè§£æçš„è¾“å‡ºä¸Šæ˜¾ç¤ºå‡ºæé«˜ç¿»è¯‘è´¨é‡çš„æ½œåŠ›ï¼ˆä¾‹å¦‚æ³°ç±³å°”è¯­åˆ°è‹±è¯­çš„BLEUå¾—åˆ†æé«˜äº†13.84ï¼‰ï¼Œä½†æˆ‘ä»¬å‘ç°åœ¨ç¡®ä¿æ¨¡å‹å§‹ç»ˆéµå¾ªæ‰€éœ€çš„CoTè¾“å‡ºæ ¼å¼æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19616v1">PDF</a> 7 pages, 1 figure, submitted to IWSLT 2025</p>
<p><strong>Summary</strong><br>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†HITSZåœ¨IWSLT 2025 Indicè½¨é“ä¸Šçš„æäº¤å†…å®¹ï¼Œèšç„¦äºè‹±è¯­åˆ°å°åœ°è¯­å’Œå°åœ°è¯­åˆ°è‹±è¯­çš„è¯­éŸ³è¯†åˆ«ç¿»è¯‘ï¼ˆSTï¼‰ã€‚ä¸ºæå‡ä½èµ„æºåœºæ™¯ä¸‹çš„ç¿»è¯‘è´¨é‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„ç³»ç»Ÿï¼Œæ•´åˆäº†é¢„è®­ç»ƒçš„Whisperè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹å’ŒKrutrimâ€”â€”ä¸€æ¬¾é’ˆå¯¹å°åœ°è¯­çš„ä¸“ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿåœ¨è‹±è¯­åˆ°å°åœ°è¯­æ–¹å‘å¹³å‡BLEUå¾—åˆ†ä¸º28.88ï¼Œå°åœ°è¯­åˆ°è‹±è¯­æ–¹å‘ä¸º27.86ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ–¹æ³•ã€‚è™½ç„¶è¯¥æ–¹æ³•åœ¨æˆåŠŸè§£æçš„è¾“å‡ºä¸Šæ˜¾ç¤ºå‡ºæé«˜ç¿»è¯‘è´¨é‡çš„æ½œåŠ›ï¼ˆå¦‚æ³°ç±³å°”è¯­åˆ°è‹±è¯­çš„BLEUå¾—åˆ†æé«˜äº†13.84ï¼‰ï¼Œä½†æˆ‘ä»¬ä¹Ÿé¢ä¸´ç€ç¡®ä¿æ¨¡å‹å§‹ç»ˆéµå¾ªæ‰€éœ€çš„CoTè¾“å‡ºæ ¼å¼çš„éš¾é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HITSZå›¢é˜Ÿåœ¨IWSLT 2025 Indicè½¨é“ä¸Šçš„ç ”ç©¶é›†ä¸­äºè‹±è¯­åˆ°å°åœ°è¯­å’Œå°åœ°è¯­åˆ°è‹±è¯­çš„è¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„ç¿»è¯‘ç³»ç»Ÿï¼Œç»“åˆäº†é¢„è®­ç»ƒçš„Whisperè‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹å’ŒKrutrimå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨è‹±è¯­åˆ°å°åœ°è¯­å’Œå°åœ°è¯­åˆ°è‹±è¯­çš„ç¿»è¯‘ä¸­å–å¾—äº†è¾ƒé«˜çš„BLEUå¾—åˆ†ã€‚</li>
<li>è®ºæ–‡æ¢è®¨äº†é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ–¹æ³•åœ¨ç¿»è¯‘è´¨é‡æå‡æ–¹é¢çš„æ½œåŠ›ï¼Œå°¤å…¶åœ¨æˆåŠŸè§£æçš„è¾“å‡ºä¸Šè¡¨ç°æ˜¾è‘—ã€‚</li>
<li>CoTæ–¹æ³•é¢ä¸´ç¡®ä¿æ¨¡å‹éµå¾ªæ‰€éœ€è¾“å‡ºæ ¼å¼çš„éš¾é¢˜ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼Œåœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ•´åˆç°æœ‰æŠ€æœ¯å¦‚ASRå’ŒLLMèƒ½æœ‰æ•ˆæå‡ç¿»è¯‘è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19616">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3ddcbfd3f9af1b4680e9360cf9fbf787.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d7cf6c43683d2e4b5a80de022f03c6b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c4b48639577adfca107f9ef258e3ecc9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-71aac3b1a209ff4f8e7a6b35a1d45ed5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-714eaccc6467b1266fe244c954386f09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d81041a550e31c33117dd998ed7c45d9.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="SemGes-Semantics-aware-Co-Speech-Gesture-Generation-using-Semantic-Coherence-and-Relevance-Learning"><a href="#SemGes-Semantics-aware-Co-Speech-Gesture-Generation-using-Semantic-Coherence-and-Relevance-Learning" class="headerlink" title="SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic   Coherence and Relevance Learning"></a>SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic   Coherence and Relevance Learning</h2><p><strong>Authors:Lanmiao Liu, Esam Ghaleb, AslÄ± Ã–zyÃ¼rek, Zerrin Yumak</strong></p>
<p>Creating a virtual avatar with semantically coherent gestures that are aligned with speech is a challenging task. Existing gesture generation research mainly focused on generating rhythmic beat gestures, neglecting the semantic context of the gestures. In this paper, we propose a novel approach for semantic grounding in co-speech gesture generation that integrates semantic information at both fine-grained and global levels. Our approach starts with learning the motion prior through a vector-quantized variational autoencoder. Built on this model, a second-stage module is applied to automatically generate gestures from speech, text-based semantics and speaker identity that ensures consistency between the semantic relevance of generated gestures and co-occurring speech semantics through semantic coherence and relevance modules. Experimental results demonstrate that our approach enhances the realism and coherence of semantic gestures. Extensive experiments and user studies show that our method outperforms state-of-the-art approaches across two benchmarks in co-speech gesture generation in both objective and subjective metrics. The qualitative results of our model, code, dataset and pre-trained models can be viewed at <a target="_blank" rel="noopener" href="https://semgesture.github.io/">https://semgesture.github.io/</a>. </p>
<blockquote>
<p>åˆ›å»ºä¸è¯­éŸ³å¯¹é½çš„å…·æœ‰è¯­ä¹‰è¿è´¯æ‰‹åŠ¿çš„è™šæ‹ŸåŒ–èº«æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰çš„æ‰‹åŠ¿ç”Ÿæˆç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ç”Ÿæˆæœ‰èŠ‚å¥çš„èŠ‚å¥æ‰‹åŠ¿ä¸Šï¼Œå¿½è§†äº†æ‰‹åŠ¿çš„è¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ååŒè¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆä¸­çš„è¯­ä¹‰å®šä½æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ç²¾ç»†ç²’åº¦å’Œå…¨å±€å±‚é¢éƒ½æ•´åˆäº†è¯­ä¹‰ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡å­¦ä¹ è¿åŠ¨å…ˆéªŒçŸ¥è¯†ï¼Œé€šè¿‡å‘é‡é‡åŒ–å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œåº”ç”¨ç¬¬äºŒé˜¶æ®µæ¨¡å—ï¼Œä»è¯­éŸ³ã€æ–‡æœ¬è¯­ä¹‰å’Œè¯´è¯äººèº«ä»½è‡ªåŠ¨ç”Ÿæˆæ‰‹åŠ¿ï¼Œé€šè¿‡è¯­ä¹‰è¿è´¯æ€§å’Œç›¸å…³æ€§æ¨¡å—ç¡®ä¿ç”Ÿæˆæ‰‹åŠ¿çš„è¯­ä¹‰ç›¸å…³æ€§ä¸ååŒå‘ç”Ÿçš„è¯­éŸ³è¯­ä¹‰ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†è¯­ä¹‰æ‰‹åŠ¿çš„çœŸå®æ€§å’Œè¿è´¯æ€§ã€‚å¤§é‡çš„å®éªŒå’Œç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ååŒè¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆçš„ä¸¤ä¸ªæ–¹é¢éƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œå³åœ¨å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡ä¸Šå‡æœ‰æ‰€è¶…è¶Šã€‚è¯¥æ¨¡å‹çš„å®šæ€§ç»“æœã€ä»£ç ã€æ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://semgesture.github.io/">https://semgesture.github.io/</a>æŸ¥çœ‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19359v1">PDF</a> Accepted to IEEE&#x2F;CVF International Conference on Computer Vision   (ICCV) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„è¯­ä¹‰å®šä½ååŒè¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ç²¾ç»†ç²’åº¦å’Œå…¨å±€å±‚é¢éƒ½é›†æˆäº†è¯­ä¹‰ä¿¡æ¯ã€‚é€šè¿‡å‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨å­¦ä¹ è¿åŠ¨å…ˆéªŒï¼Œç„¶ååº”ç”¨ç¬¬äºŒé˜¶æ®µæ¨¡å—è‡ªåŠ¨ä»è¯­éŸ³ã€æ–‡æœ¬è¯­ä¹‰å’Œè¯´è¯äººèº«ä»½ç”Ÿæˆæ‰‹åŠ¿ï¼Œç¡®ä¿ç”Ÿæˆæ‰‹åŠ¿çš„è¯­ä¹‰ç›¸å…³æ€§ä¸ååŒå‘ç”Ÿçš„è¯­éŸ³è¯­ä¹‰ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†è¯­ä¹‰æ‰‹åŠ¿çš„çœŸå®æ€§å’Œè¿è´¯æ€§ï¼Œå¹¶åœ¨ä¸¤ä¸ªååŒè¯­éŸ³æ‰‹åŠ¿ç”ŸæˆåŸºå‡†æµ‹è¯•ä¸­å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è¯­ä¹‰å®šä½ååŒè¯­éŸ³æ‰‹åŠ¿ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•é›†æˆäº†è¯­ä¹‰ä¿¡æ¯åœ¨ç²¾ç»†ç²’åº¦å’Œå…¨å±€ä¸¤ä¸ªå±‚é¢ã€‚</li>
<li>é€šè¿‡å‘é‡é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨å­¦ä¹ è¿åŠ¨å…ˆéªŒã€‚</li>
<li>è‡ªåŠ¨ä»è¯­éŸ³ã€æ–‡æœ¬è¯­ä¹‰å’Œè¯´è¯äººèº«ä»½ç”Ÿæˆæ‰‹åŠ¿ã€‚</li>
<li>ç”Ÿæˆçš„æ‰‹åŠ¿ä¸ååŒå‘ç”Ÿçš„è¯­éŸ³è¯­ä¹‰ä¹‹é—´ä¿æŒä¸€è‡´æ€§ã€‚</li>
<li>å®éªŒå’Œç”¨æˆ·ç ”ç©¶è¯æ˜è¯¥æ–¹æ³•åœ¨å®¢è§‚å’Œä¸»è§‚æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19359">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-762692fd6213e4d15f5b6f8f055cba32.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-94360db09bd6bc44548d02fe68178c74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d26ea0d1b9c4d88fbb7505947d207fa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8f2ed3349398ebdab1166cd35c6713c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cbcba911a8e4c2acd6a3390ad976ba75.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="The-Eloquence-team-submission-for-task-1-of-MLC-SLM-challenge"><a href="#The-Eloquence-team-submission-for-task-1-of-MLC-SLM-challenge" class="headerlink" title="The Eloquence team submission for task 1 of MLC-SLM challenge"></a>The Eloquence team submission for task 1 of MLC-SLM challenge</h2><p><strong>Authors:Lorenzo Concina, Jordi Luque, Alessio Brutti, Marco Matassoni, Yuchen Zhang</strong></p>
<p>In this paper, we present our studies and experiments carried out for the task 1 of the Challenge and Workshop on Multilingual Conversational Speech Language Model (MLC-SLM), which focuses on advancing multilingual conversational speech recognition through the development of speech language models architectures. Given the increasing relevance of real-world conversational data for building robust Spoken Dialogue Systems, we explore three approaches to multilingual ASR. First, we conduct an evaluation of the official baseline to better understand its strengths and limitations, by training two projectors (linear and qformer) with different foundation models. Second we leverage the SLAM-ASR framework to train a custom multilingual linear projector. Finally we investigate the role of contrastive learning and the extended conversational context in enhancing the robustness of recognition. </p>
<blockquote>
<p>åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†é’ˆå¯¹å¤šè¯­è¨€å¯¹è¯è¯­éŸ³è¯­è¨€æ¨¡å‹æŒ‘æˆ˜ä¸ç ”è®¨ä¼šï¼ˆMLC-SLMï¼‰ä»»åŠ¡ä¸€çš„ç ”ç©¶å’Œå®éªŒã€‚è¯¥ä»»åŠ¡æ—¨åœ¨é€šè¿‡å‘å±•è¯­éŸ³è¯­è¨€æ¨¡å‹æ¶æ„ï¼Œæ¨åŠ¨å¤šè¯­è¨€å¯¹è¯è¯­éŸ³è¯†åˆ«çš„å‘å±•ã€‚é‰´äºç°å®ä¸–ç•Œå¯¹è¯æ•°æ®å¯¹äºæ„å»ºç¨³å¥çš„å£è¯­å¯¹è¯ç³»ç»Ÿè¶Šæ¥è¶Šé‡è¦ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸‰ç§å¤šè¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡è®­ç»ƒä¸¤ç§æŠ•å½±ä»ªï¼ˆçº¿æ€§æŠ•å½±ä»ªå’ŒQformeræŠ•å½±ä»ªï¼‰æ¥è¯„ä¼°å®˜æ–¹åŸºçº¿æ–¹æ¡ˆçš„ä¼˜åŠ¿å’Œä¸è¶³ï¼Œä»è€Œæ›´å¥½åœ°äº†è§£å…¶ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨SLAM-ASRæ¡†æ¶è®­ç»ƒä¸€ä¸ªè‡ªå®šä¹‰çš„å¤šè¯­è¨€çº¿æ€§æŠ•å½±ä»ªã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†å¯¹æ¯”å­¦ä¹ å’Œæ‰©å±•å¯¹è¯ä¸Šä¸‹æ–‡åœ¨æé«˜è¯†åˆ«ç¨³å¥æ€§æ–¹é¢çš„ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19308v1">PDF</a> Technical Report for MLC-SLM Challenge of Interspeech2025</p>
<p><strong>Summary</strong><br>è¯¥ç ”ç©¶è®ºæ–‡èšç„¦äºåœ¨æŒ‘æˆ˜å’Œç ”è®¨ä¼šä¸Šè¿›è¡Œçš„é’ˆå¯¹å¤šä»»åŠ¡å£è¯­è¯­è¨€æ¨¡å‹çš„ä»»åŠ¡ä¸€çš„å®éªŒå’Œç ”ç©¶ã€‚ç ”ç©¶è‡´åŠ›äºæ¨è¿›å¤šè¯­ç§ä¼šè¯è¯­éŸ³è¯†åˆ«çš„å‘å±•ï¼Œé€šè¿‡å¯¹å®˜æ–¹åŸºçº¿æ¨¡å‹è¿›è¡Œæµ‹è¯„æ¥äº†è§£å…¶ä¼˜ç‚¹å’Œå±€é™æ€§ï¼Œå¹¶æ¢è®¨äº†ä½¿ç”¨ä¸åŒåŸºç¡€æ¨¡å‹çš„æŠ•å½±å™¨è®­ç»ƒæ–¹å¼ã€‚åŒæ—¶ï¼Œç ”ç©¶è¿˜é‡‡ç”¨äº†SLAM-ASRæ¡†æ¶è®­ç»ƒäº†è‡ªå®šä¹‰çš„å¤šè¯­ç§çº¿æ€§æŠ•å½±å™¨ï¼Œå¹¶æ¢ç©¶äº†å¯¹æ¯”å­¦ä¹ å’Œæ‰©å±•ä¼šè¯è¯­å¢ƒåœ¨æé«˜è¯†åˆ«é²æ£’æ€§æ–¹é¢çš„ä½œç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å…³æ³¨äºå¤šä»»åŠ¡å£è¯­è¯­è¨€æ¨¡å‹ä»»åŠ¡ä¸€çš„å®éªŒå’Œç ”ç©¶ï¼Œè‡´åŠ›äºæ¨è¿›å¤šè¯­ç§ä¼šè¯è¯­éŸ³è¯†åˆ«æŠ€æœ¯çš„å‘å±•ã€‚</li>
<li>é€šè¿‡æµ‹è¯„å®˜æ–¹åŸºçº¿æ¨¡å‹æ¥äº†è§£å…¶ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨ä¸åŒåŸºç¡€æ¨¡å‹çš„æŠ•å½±å™¨è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬çº¿æ€§æŠ•å½±å™¨å’ŒqformeræŠ•å½±å™¨ã€‚</li>
<li>é‡‡ç”¨SLAM-ASRæ¡†æ¶è®­ç»ƒè‡ªå®šä¹‰çš„å¤šè¯­ç§çº¿æ€§æŠ•å½±å™¨ã€‚</li>
<li>ç ”ç©¶å‘ç°å¯¹æ¯”å­¦ä¹ åœ¨æé«˜è¯­éŸ³è¯†åˆ«é²æ£’æ€§æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚</li>
<li>æ‰©å±•ä¼šè¯è¯­å¢ƒå¯¹å¢å¼ºè¯†åˆ«ç³»ç»Ÿçš„é²æ£’æ€§æœ‰ä¸€å®šå¸®åŠ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19308">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d97b199cd3cbb773229aee046602713f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9fed3dcfced4eb54bac9709fdf43a7b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9050029e3cebfd6ff01bb09cc7941d47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98981d5dd38d03898c6fcb01554b1830.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="System-Report-for-CCL25-Eval-Task-10-SRAG-MAV-for-Fine-Grained-Chinese-Hate-Speech-Recognition"><a href="#System-Report-for-CCL25-Eval-Task-10-SRAG-MAV-for-Fine-Grained-Chinese-Hate-Speech-Recognition" class="headerlink" title="System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese   Hate Speech Recognition"></a>System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese   Hate Speech Recognition</h2><p><strong>Authors:Jiahao Wang, Ramen Liu, Longhui Zhang, Jing Li</strong></p>
<p>This paper presents our system for CCL25-Eval Task 10, addressing Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel SRAG-MAV framework that synergistically integrates task reformulation(TR), Self-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting (MAV). Our method reformulates the quadruplet extraction task into triplet extraction, uses dynamic retrieval from the training set to create contextual prompts, and applies multi-round inference with voting to improve output stability and performance. Our system, based on the Qwen2.5-7B model, achieves a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o (Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The code is available at <a target="_blank" rel="noopener" href="https://github.com/king-wang123/CCL25-SRAG-MAV">https://github.com/king-wang123/CCL25-SRAG-MAV</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†æˆ‘ä»¬åœ¨CCL25-Eval Task 10çš„ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿä¸“æ³¨äºç²¾ç»†ç²’åº¦çš„ä¸­æ–‡ä»‡æ¨è¨€è®ºè¯†åˆ«ï¼ˆFGCHSRï¼‰ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„SRAG-MAVæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ååŒæ•´åˆäº†ä»»åŠ¡é‡æ„ï¼ˆTRï¼‰ã€è‡ªæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆSRAGï¼‰å’Œå¤šè½®ç´¯ç§¯æŠ•ç¥¨ï¼ˆMAVï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†å››å…ƒç»„æå–ä»»åŠ¡é‡æ–°æ„å»ºä¸ºä¸‰å…ƒç»„æå–ï¼Œä½¿ç”¨è®­ç»ƒé›†çš„åŠ¨æ€æ£€ç´¢æ¥åˆ›å»ºä¸Šä¸‹æ–‡æç¤ºï¼Œå¹¶åº”ç”¨å¤šè½®æ¨ç†å’ŒæŠ•ç¥¨æ¥æé«˜è¾“å‡ºç¨³å®šæ€§å’Œæ€§èƒ½ã€‚æˆ‘ä»¬çš„ç³»ç»ŸåŸºäºQwen2.5-7Bæ¨¡å‹ï¼Œåœ¨STATE ToxiCNæ•°æ®é›†ä¸Šå®ç°äº†ç¡¬åˆ†æ•°26.66ï¼Œè½¯åˆ†æ•°48.35ï¼Œå¹³å‡åˆ†æ•°37.505ï¼Œæ˜¾è‘—è¶…è¶Šäº†GPT-4oï¼ˆå¹³å‡åˆ†æ•°15.63ï¼‰å’Œç»è¿‡å¾®è°ƒåçš„Qwen2.5-7Bæ¨¡å‹ï¼ˆå¹³å‡åˆ†æ•°ä¸º35.365ï¼‰ã€‚ä»£ç å¯è®¿é—®äº <a target="_blank" rel="noopener" href="https://github.com/king-wang123/CCL25-SRAG-MAV%E3%80%82">https://github.com/king-wang123/CCL25-SRAG-MAVã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18580v1">PDF</a> 8 pages, 3 figures, accepted as oral presentation at CCL25-Eval</p>
<p><strong>Summary</strong><br>ç³»ç»Ÿé’ˆå¯¹ç²¾ç»†ç²’åº¦ä¸­æ–‡ä»‡æ¨è¨€è®ºè¯†åˆ«ä»»åŠ¡ï¼ˆFGCHSRï¼‰æå‡ºæ–°çš„SRAG-MAVæ¡†æ¶ï¼ŒåŒ…æ‹¬ä»»åŠ¡é‡æ–°è¡¨è¿°ï¼ˆTRï¼‰ã€è‡ªæˆ‘æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆSRAGï¼‰å’Œå¤šè½®ç´¯ç§¯æŠ•ç¥¨ï¼ˆMAVï¼‰ã€‚é€šè¿‡æ”¹é©å››å…ƒç»„æå–ä»»åŠ¡ä¸ºä¸‰å…ƒç»„æå–ï¼Œä½¿ç”¨è®­ç»ƒé›†çš„åŠ¨æ€æ£€ç´¢åˆ›å»ºä¸Šä¸‹æ–‡æç¤ºï¼Œå¹¶åº”ç”¨å¤šè½®æ¨ç†å’ŒæŠ•ç¥¨æ¥æé«˜è¾“å‡ºç¨³å®šæ€§å’Œæ€§èƒ½ã€‚åœ¨STATE ToxiCNæ•°æ®é›†ä¸Šï¼ŒåŸºäºQwen2.5-7Bæ¨¡å‹çš„è¯¥ç³»ç»Ÿå–å¾—äº†æ˜¾è‘—æˆç»©ï¼Œå¹³å‡å¾—åˆ†37.505ï¼Œæ˜æ˜¾ä¼˜äºGPT-4oå’Œå¾®è°ƒåçš„Qwen2.5-7BåŸºçº¿æ¨¡å‹ã€‚ä»£ç å·²å…¬å¼€äºGitHubã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç³»ç»Ÿé’ˆå¯¹Fine-Grained Chinese Hate Speech Recognition (FGCHSR)ä»»åŠ¡è®¾è®¡ã€‚</li>
<li>æå‡ºæ–°çš„SRAG-MAVæ¡†æ¶ï¼Œæ•´åˆä»»åŠ¡é‡æ–°è¡¨è¿°ã€è‡ªæˆ‘æ£€ç´¢å¢å¼ºç”Ÿæˆå’Œå¤šè½®ç´¯ç§¯æŠ•ç¥¨æŠ€æœ¯ã€‚</li>
<li>æ”¹é©å››å…ƒç»„æå–ä»»åŠ¡ä¸ºä¸‰å…ƒç»„æå–ä»¥ç®€åŒ–å¤„ç†ã€‚</li>
<li>ä½¿ç”¨åŠ¨æ€æ£€ç´¢ä»è®­ç»ƒé›†ä¸­åˆ›å»ºä¸Šä¸‹æ–‡æç¤ºä»¥å¢å¼ºæ¨¡å‹ç†è§£ã€‚</li>
<li>å¤šè½®æ¨ç†å’ŒæŠ•ç¥¨æé«˜è¾“å‡ºç¨³å®šæ€§å’Œæ€§èƒ½ã€‚</li>
<li>åœ¨STATE ToxiCNæ•°æ®é›†ä¸Šå–å¾—æ˜¾è‘—æˆç»©ï¼Œå¹³å‡å¾—åˆ†ä¼˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18580">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bebd8432341821e03ac39d92b64be1de.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49a9d69cadd48c4406e76234f029cc27.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-029487bff595494fd626f9c4f44f4d4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3dc563291ead537b4c3962a0eb3e5d6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47d4568712a06f50b63f8845047fb07b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a2b1c634ee7d18be5ab03b710526f769.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Restoring-Rhythm-Punctuation-Restoration-Using-Transformer-Models-for-Bangla-a-Low-Resource-Language"><a href="#Restoring-Rhythm-Punctuation-Restoration-Using-Transformer-Models-for-Bangla-a-Low-Resource-Language" class="headerlink" title="Restoring Rhythm: Punctuation Restoration Using Transformer Models for   Bangla, a Low-Resource Language"></a>Restoring Rhythm: Punctuation Restoration Using Transformer Models for   Bangla, a Low-Resource Language</h2><p><strong>Authors:Md Obyedullahil Mamun, Md Adyelullahil Mamun, Arif Ahmad, Md. Imran Hossain Emu</strong></p>
<p>Punctuation restoration enhances the readability of text and is critical for post-processing tasks in Automatic Speech Recognition (ASR), especially for low-resource languages like Bangla. In this study, we explore the application of transformer-based models, specifically XLM-RoBERTa-large, to automatically restore punctuation in unpunctuated Bangla text. We focus on predicting four punctuation marks: period, comma, question mark, and exclamation mark across diverse text domains. To address the scarcity of annotated resources, we constructed a large, varied training corpus and applied data augmentation techniques. Our best-performing model, trained with an augmentation factor of alpha &#x3D; 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the Reference set, and 90.2% on the ASR set.   Results show strong generalization to reference and ASR transcripts, demonstrating the modelâ€™s effectiveness in real-world, noisy scenarios. This work establishes a strong baseline for Bangla punctuation restoration and contributes publicly available datasets and code to support future research in low-resource NLP. </p>
<blockquote>
<p>æ ‡ç‚¹ç¬¦å·çš„æ¢å¤æé«˜äº†æ–‡æœ¬çš„å¯è¯»æ€§ï¼Œå¯¹äºè¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„åå¤„ç†ä»»åŠ¡è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯å¯¹å­ŸåŠ æ‹‰è¯­ç­‰ä½èµ„æºè¯­è¨€è€Œè¨€ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†åŸºäºå˜æ¢å™¨æ¨¡å‹çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å¤§å‹çš„XLM-RoBERTaæ¨¡å‹ï¼Œä»¥è‡ªåŠ¨æ¢å¤æœªåŠ æ ‡ç‚¹ç¬¦å·çš„å­ŸåŠ æ‹‰è¯­æ–‡æœ¬ä¸­çš„æ ‡ç‚¹ç¬¦å·ã€‚æˆ‘ä»¬ä¸“æ³¨äºé¢„æµ‹å››ä¸ªæ ‡ç‚¹ç¬¦å·ï¼šå¥å·ã€é€—å·ã€é—®å·å’Œæ„Ÿå¹å·ï¼Œæ¶‰åŠä¸åŒçš„æ–‡æœ¬é¢†åŸŸã€‚ä¸ºäº†è§£å†³æ ‡æ³¨èµ„æºçš„ç¨€ç¼ºé—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§å‹ã€å¤šæ ·åŒ–çš„è®­ç»ƒè¯­æ–™åº“ï¼Œå¹¶åº”ç”¨äº†æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚æˆ‘ä»¬è¡¨ç°æœ€ä½³çš„æ¨¡å‹ä»¥alpha&#x3D;0.2%çš„æ‰©å……å› å­è¿›è¡Œè®­ç»ƒï¼Œåœ¨æ–°é—»æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†97.1%ï¼Œåœ¨å‚è€ƒé›†ä¸Šè¾¾åˆ°äº†91.2%ï¼Œåœ¨ASRé›†ä¸Šè¾¾åˆ°äº†90.2%ã€‚ç»“æœè¡¨æ˜è¯¥æ¨¡å‹åœ¨å‚è€ƒå’ŒASRè½¬å½•æ–¹é¢çš„æ³›åŒ–èƒ½åŠ›å¼ºï¼Œè¯æ˜äº†è¯¥æ¨¡å‹åœ¨ç°å®ä¸–ç•Œä¸­çš„å˜ˆæ‚åœºæ™¯ä¸­å…·æœ‰è‰¯å¥½çš„æ•ˆæœã€‚è¿™ä¸ºå­ŸåŠ æ‹‰è¯­æ ‡ç‚¹ç¬¦å·çš„æ¢å¤å»ºç«‹äº†å¼ºå¤§çš„åŸºçº¿ï¼Œå¹¶ä¸ºæœªæ¥ä½èµ„æºNLPçš„ç ”ç©¶æä¾›äº†å…¬å¼€å¯ç”¨çš„æ•°æ®é›†å’Œä»£ç æ”¯æŒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18448v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºè½¬æ¢å™¨æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯XLM-RoBERTa-largeåœ¨å­ŸåŠ æ‹‰è¯­æ— æ ‡ç‚¹æ–‡æœ¬è‡ªåŠ¨æ ‡ç‚¹æ¢å¤ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶èšç„¦äºé¢„æµ‹å››ç§æ ‡ç‚¹ï¼šå¥å·ã€é€—å·ã€é—®å·å’Œæ„Ÿå¹å·åœ¨ä¸åŒæ–‡æœ¬é¢†åŸŸçš„åº”ç”¨ã€‚ä¸ºè§£å†³æ ‡æ³¨èµ„æºåŒ®ä¹çš„é—®é¢˜ï¼Œç ”ç©¶æ„å»ºäº†ä¸€ä¸ªå¤§å‹å¤šæ ·åŒ–çš„è®­ç»ƒè¯­æ–™åº“ï¼Œå¹¶é‡‡ç”¨äº†æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚æœ€ä¼˜æ¨¡å‹åœ¨æ–°é—»æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®åº¦è¾¾åˆ°97.1%ï¼Œåœ¨å‚è€ƒé›†å’Œè¯­éŸ³è¯†åˆ«é›†ä¸Šçš„å‡†ç¡®åº¦åˆ†åˆ«ä¸º91.2%å’Œ90.2%ã€‚ç»“æœè¯æ˜æ¨¡å‹åœ¨çœŸå®ã€å˜ˆæ‚çš„åœºæ™¯ä¸­å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸ºå­ŸåŠ æ‹‰è¯­æ ‡ç‚¹æ¢å¤å»ºç«‹äº†åšå®çš„åŸºå‡†ï¼Œå¹¶ä¸ºä½èµ„æºè‡ªç„¶è¯­è¨€å¤„ç†æä¾›äº†å…¬å¼€çš„æ•°æ®é›†å’Œä»£ç æ”¯æŒæœªæ¥ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶ä½¿ç”¨è½¬æ¢å™¨æ¨¡å‹XLM-RoBERTa-largeè¿›è¡Œå­ŸåŠ æ‹‰è¯­çš„æ ‡ç‚¹æ¢å¤ã€‚</li>
<li>ç ”ç©¶å…³æ³¨å››ç§æ ‡ç‚¹çš„é¢„æµ‹ï¼šå¥å·ã€é€—å·ã€é—®å·ã€æ„Ÿå¹å·ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§å‹å¤šæ ·åŒ–çš„è®­ç»ƒè¯­æ–™åº“å¹¶é‡‡ç”¨äº†æ•°æ®å¢å¼ºæŠ€æœ¯ä»¥è§£å†³æ ‡æ³¨èµ„æºåŒ®ä¹çš„é—®é¢˜ã€‚</li>
<li>æœ€ä¼˜æ¨¡å‹åœ¨æ–°é—»æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®åº¦ä¸º97.1%ã€‚</li>
<li>æ¨¡å‹åœ¨å‚è€ƒé›†å’Œè¯­éŸ³è¯†åˆ«é›†ä¸Šçš„å‡†ç¡®åº¦åˆ†åˆ«ä¸º91.2%å’Œ90.2%ï¼Œæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç ”ç©¶ä¸ºå­ŸåŠ æ‹‰è¯­æ ‡ç‚¹æ¢å¤å»ºç«‹äº†åšå®çš„åŸºå‡†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18448">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-81a2a3f456b4b4fbd23f974e6403590c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b73936c17d8f66ec2ffe670eef11bd11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f473f53deeea858c036aa8a1ad4b3a97.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Recent-Trends-in-Distant-Conversational-Speech-Recognition-A-Review-of-CHiME-7-and-8-DASR-Challenges"><a href="#Recent-Trends-in-Distant-Conversational-Speech-Recognition-A-Review-of-CHiME-7-and-8-DASR-Challenges" class="headerlink" title="Recent Trends in Distant Conversational Speech Recognition: A Review of   CHiME-7 and 8 DASR Challenges"></a>Recent Trends in Distant Conversational Speech Recognition: A Review of   CHiME-7 and 8 DASR Challenges</h2><p><strong>Authors:Samuele Cornell, Christoph Boeddeker, Taejin Park, He Huang, Desh Raj, Matthew Wiesner, Yoshiki Masuyama, Xuankai Chang, Zhong-Qiu Wang, Stefano Squartini, Paola Garcia, Shinji Watanabe</strong></p>
<p>The CHiME-7 and 8 distant speech recognition (DASR) challenges focus on multi-channel, generalizable, joint automatic speech recognition (ASR) and diarization of conversational speech. With participation from 9 teams submitting 32 diverse systems, these challenges have contributed to state-of-the-art research in the field. This paper outlines the challengesâ€™ design, evaluation metrics, datasets, and baseline systems while analyzing key trends from participant submissions. From this analysis it emerges that: 1) Most participants use end-to-end (e2e) ASR systems, whereas hybrid systems were prevalent in previous CHiME challenges. This transition is mainly due to the availability of robust large-scale pre-trained models, which lowers the data burden for e2e-ASR. 2) Despite recent advances in neural speech separation and enhancement (SSE), all teams still heavily rely on guided source separation, suggesting that current neural SSE techniques are still unable to reliably deal with complex scenarios and different recording setups. 3) All best systems employ diarization refinement via target-speaker diarization techniques. Accurate speaker counting in the first diarization pass is thus crucial to avoid compounding errors and CHiME-8 DASR participants especially focused on this part. 4) Downstream evaluation via meeting summarization can correlate weakly with transcription quality due to the remarkable effectiveness of large-language models in handling errors. On the NOTSOFAR-1 scenario, even systems with over 50% time-constrained minimum permutation WER can perform roughly on par with the most effective ones (around 11%). 5) Despite recent progress, accurately transcribing spontaneous speech in challenging acoustic environments remains difficult, even when using computationally intensive system ensembles. </p>
<blockquote>
<p>CHiME-7å’ŒCHiME-8è¿œç¨‹è¯­éŸ³è¯†åˆ«ï¼ˆDASRï¼‰æŒ‘æˆ˜ä¸“æ³¨äºå¤šé€šé“ã€é€šç”¨åŒ–ã€è”åˆè¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå¯¹è¯è¯­éŸ³çš„èšç±»åˆ†æã€‚å…±æœ‰9ä¸ªå›¢é˜Ÿæäº¤äº†32ä¸ªä¸åŒçš„ç³»ç»Ÿå‚ä¸æŒ‘æˆ˜ï¼Œä¸ºè¿™ä¸€é¢†åŸŸçš„æœ€æ–°ç ”ç©¶åšå‡ºäº†è´¡çŒ®ã€‚æœ¬æ–‡æ¦‚è¿°äº†æŒ‘æˆ˜çš„æ–¹æ¡ˆè®¾è®¡ã€è¯„ä»·æŒ‡æ ‡ã€æ•°æ®é›†å’ŒåŸºçº¿ç³»ç»Ÿï¼ŒåŒæ—¶åˆ†æäº†å‚ä¸è€…æäº¤çš„å…³é”®è¶‹åŠ¿ã€‚ä»åˆ†æä¸­å¯ä»¥çœ‹å‡ºï¼š1ï¼‰å¤§å¤šæ•°å‚ä¸è€…ä½¿ç”¨ç«¯åˆ°ç«¯ï¼ˆe2eï¼‰è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œè€Œä¹‹å‰çš„CHiMEæŒ‘æˆ˜ä¸­æ··åˆç³»ç»Ÿæ›´ä¸ºæ™®éã€‚è¿™ç§è½¬å˜ä¸»è¦æ˜¯ç”±äºå¼ºå¤§çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„å‡ºç°ï¼Œé™ä½äº†ç«¯åˆ°ç«¯ASRçš„æ•°æ®è´Ÿæ‹…ã€‚2ï¼‰å°½ç®¡ç¥ç»è¯­éŸ³åˆ†ç¦»ä¸å¢å¼ºï¼ˆSSEï¼‰é¢†åŸŸè¿‘æœŸå–å¾—äº†è¿›å±•ï¼Œä½†æ‰€æœ‰å›¢é˜Ÿä»ä¸¥é‡ä¾èµ–å¯¼å‘æºåˆ†ç¦»ï¼Œè¿™è¡¨æ˜å½“å‰ç¥ç»SSEæŠ€æœ¯ä»æ— æ³•å¯é åœ°å¤„ç†å¤æ‚åœºæ™¯å’Œä¸åŒå½•éŸ³è®¾ç½®ã€‚3ï¼‰æ‰€æœ‰æœ€ä½³ç³»ç»Ÿéƒ½é€šè¿‡ç›®æ ‡è¯´è¯äººèšç±»åˆ†ææŠ€æœ¯è¿›è¡Œäº†èšç±»åˆ†æä¼˜åŒ–ã€‚å› æ­¤ï¼Œç¬¬ä¸€æ¬¡èšç±»åˆ†æè¿‡ç¨‹ä¸­çš„å‡†ç¡®è¯´è¯äººæ•°è®¡ç®—è‡³å…³é‡è¦ï¼Œä»¥é¿å…ç´¯ç§¯é”™è¯¯ï¼ŒCHiME-8 DASRå‚ä¸è€…å°¤å…¶å…³æ³¨è¿™éƒ¨åˆ†å†…å®¹ã€‚4ï¼‰é€šè¿‡ä¼šè®®æ‘˜è¦è¿›è¡Œçš„ä¸‹æ¸¸è¯„ä¼°ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é”™è¯¯æ–¹é¢çš„æ˜¾è‘—æœ‰æ•ˆæ€§ï¼Œå¯èƒ½ä¸è½¬å½•è´¨é‡å­˜åœ¨å¾®å¼±çš„ç›¸å…³æ€§ã€‚åœ¨NOTSOFAR-1åœºæ™¯ä¸­ï¼Œå³ä½¿ç³»ç»Ÿçš„æ—¶é—´çº¦æŸæœ€å°ç½®æ¢é”™è¯¯ç‡è¶…è¿‡50ï¼…ï¼Œå…¶è¡¨ç°ä¹Ÿå¯ä¸æœ€ä½³ç³»ç»Ÿå¤§è‡´ç›¸å½“ï¼ˆçº¦ä¸º11ï¼…ï¼‰ã€‚5ï¼‰å°½ç®¡å–å¾—äº†æœ€æ–°è¿›å±•ï¼Œä½†åœ¨å……æ»¡æŒ‘æˆ˜çš„å£°å­¦ç¯å¢ƒä¸­å‡†ç¡®è½¬å½•éè‡ªå‘è¡¨è¾¾ä»ç„¶å¾ˆå›°éš¾ï¼Œå³ä½¿ä½¿ç”¨è®¡ç®—å¯†é›†å‹çš„ç³»ç»Ÿé›†åˆä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18161v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>CHiME-7å’ŒCHiME-8çš„è¿œè·ç¦»è¯­éŸ³è¯†åˆ«ï¼ˆDASRï¼‰æŒ‘æˆ˜å…³æ³¨å¤šé€šé“ã€é€šç”¨åŒ–ã€è”åˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œå¯¹è¯è¯­éŸ³çš„è‡ªåŠ¨åˆ†æ¡£ã€‚æ­¤æ¬¡æŒ‘æˆ˜å¸å¼•äº†9ä¸ªå›¢é˜Ÿçš„å‚ä¸ï¼Œå…±æäº¤äº†32ä¸ªå¤šæ ·çš„ç³»ç»Ÿï¼Œä¸ºè¿™ä¸€é¢†åŸŸå¸¦æ¥äº†å‰æ²¿ç ”ç©¶ã€‚æœ¬æ–‡æ¦‚è¿°äº†æŒ‘æˆ˜çš„è®¾è®¡ã€è¯„ä¼°æŒ‡æ ‡ã€æ•°æ®é›†å’ŒåŸºçº¿ç³»ç»Ÿï¼ŒåŒæ—¶åˆ†æäº†å‚ä¸è€…æäº¤çš„å…³é”®è¶‹åŠ¿ã€‚åˆ†æè¡¨æ˜ï¼š1ï¼‰å¤§å¤šæ•°å‚ä¸è€…ä½¿ç”¨ç«¯åˆ°ç«¯ï¼ˆe2eï¼‰ASRç³»ç»Ÿï¼Œè€Œæ··åˆç³»ç»Ÿåœ¨è¿‡å»CHiMEæŒ‘æˆ˜ä¸­è¾ƒä¸ºæ™®éã€‚è¿™ç§è½¬å˜ä¸»è¦æ˜¯ç”±äºç¨³å¥çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„å¯è·å¾—æ€§ï¼Œé™ä½äº†e2e-ASRçš„æ•°æ®è´Ÿæ‹…ã€‚2ï¼‰å°½ç®¡ç¥ç»è¯­éŸ³åˆ†ç¦»å’Œå¢å¼ºï¼ˆSSEï¼‰æ–¹é¢å–å¾—äº†æœ€æ–°è¿›å±•ï¼Œæ‰€æœ‰å›¢é˜Ÿä»ç„¶ä¸¥é‡ä¾èµ–äºå¼•å¯¼æºåˆ†ç¦»ï¼Œè¿™è¡¨æ˜å½“å‰ç¥ç»SSEæŠ€æœ¯ä»ç„¶æ— æ³•å¯é åœ°å¤„ç†å¤æ‚åœºæ™¯å’Œä¸åŒå½•éŸ³è®¾ç½®ã€‚3ï¼‰æ‰€æœ‰æœ€ä½³ç³»ç»Ÿéƒ½é€šè¿‡ç›®æ ‡è¯´è¯äººåˆ†æ¡£æŠ€æœ¯è¿›è¡Œåˆ†æ¡£ç»†åŒ–ã€‚å› æ­¤ï¼Œç¬¬ä¸€æ¬¡åˆ†æ¡£è¿‡ç¨‹ä¸­çš„å‡†ç¡®è¯´è¯äººè®¡æ•°è‡³å…³é‡è¦ï¼Œå¯ä»¥é¿å…ç´¯ç§¯é”™è¯¯ï¼ŒCHiME-8 DASRå‚ä¸è€…å°¤å…¶å…³æ³¨è¿™éƒ¨åˆ†ã€‚4ï¼‰é€šè¿‡ä¼šè®®æ€»ç»“è¿›è¡Œä¸‹æ¸¸è¯„ä¼°å¯èƒ½ä¸è½¬å½•è´¨é‡å­˜åœ¨å¾®å¼±è”ç³»ï¼Œå› ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é”™è¯¯æ–¹é¢æ•ˆæœæ˜¾è‘—ã€‚åœ¨NOTSOFAR-1åœºæ™¯ä¸‹ï¼Œå³ä½¿ç³»ç»Ÿçš„æ—¶é—´çº¦æŸæœ€å°ç½®æ¢é”™è¯¯ç‡è¶…è¿‡50%ï¼Œå…¶è¡¨ç°ä¹Ÿå¯èƒ½ä¸æœ€å‡ºè‰²çš„ç³»ç»Ÿå¤§è‡´ç›¸åŒï¼ˆçº¦11%ï¼‰ã€‚5ï¼‰å°½ç®¡æœ‰æœ€æ–°è¿›å±•ï¼Œä½†åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦ç¯å¢ƒä¸­å‡†ç¡®è½¬å½•éè‡ªå‘çš„è¯­éŸ³ä»ç„¶æ˜¯ä¸€ä¸ªéš¾é¢˜ï¼Œå³ä½¿ä½¿ç”¨è®¡ç®—å¯†é›†å‹çš„ç³»ç»Ÿé›†åˆä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CHiME-7å’ŒCHiME-8æŒ‘æˆ˜é›†ä¸­äºå¤šé€šé“å’Œé€šç”¨åŒ–çš„è”åˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œå¯¹è¯è¯­éŸ³çš„è‡ªåŠ¨åˆ†æ¡£ã€‚</li>
<li>ç«¯åˆ°ç«¯ASRç³»ç»Ÿæˆä¸ºå¤šæ•°å‚ä¸è€…çš„é€‰æ‹©ï¼Œè¿™å¾—ç›Šäºç¨³å¥çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„æ™®åŠã€‚</li>
<li>å°½ç®¡ç¥ç»è¯­éŸ³åˆ†ç¦»å’Œå¢å¼ºæŠ€æœ¯æœ‰æ‰€è¿›å±•ï¼Œä½†ç›®å‰ä»ä¸»è¦ä¾èµ–å¼•å¯¼æºåˆ†ç¦»æŠ€æœ¯ã€‚</li>
<li>è¯´è¯äººåˆ†æ¡£çš„å‡†ç¡®æ€§åœ¨é¿å…ç´¯ç§¯é”™è¯¯æ–¹é¢è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„å½•éŸ³ç¯å¢ƒå’Œè®¾ç½®ä¸­ã€‚</li>
<li>é€šè¿‡ä¼šè®®æ€»ç»“è¿›è¡Œçš„ä¸‹æ¸¸è¯„ä¼°ä¸è½¬å½•è´¨é‡ä¹‹é—´çš„è”ç³»å¯èƒ½è¾ƒä¸ºå¾®å¼±ï¼Œå› ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹èƒ½æœ‰æ•ˆå¤„ç†é”™è¯¯ã€‚</li>
<li>åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å£°å­¦ç¯å¢ƒä¸­å‡†ç¡®è½¬å½•éè‡ªå‘çš„è¯­éŸ³ä»ç„¶æ˜¯ä¸€ä¸ªéš¾é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18161">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d822a74d5c85aef7f1c1cb80691e2332.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ebe7027fd8c518df223b5ade5f0293c4.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="AI-Telephone-Surveying-Automating-Quantitative-Data-Collection-with-an-AI-Interviewer"><a href="#AI-Telephone-Surveying-Automating-Quantitative-Data-Collection-with-an-AI-Interviewer" class="headerlink" title="AI Telephone Surveying: Automating Quantitative Data Collection with an   AI Interviewer"></a>AI Telephone Surveying: Automating Quantitative Data Collection with an   AI Interviewer</h2><p><strong>Authors:Danny D. Leybzon, Shreyas Tirumala, Nishant Jain, Summer Gillen, Michael Jackson, Cameron McPhee, Jennifer Schmidt</strong></p>
<p>With the rise of voice-enabled artificial intelligence (AI) systems, quantitative survey researchers have access to a new data-collection mode: AI telephone surveying. By using AI to conduct phone interviews, researchers can scale quantitative studies while balancing the dual goals of human-like interactivity and methodological rigor. Unlike earlier efforts that used interactive voice response (IVR) technology to automate these surveys, voice AI enables a more natural and adaptive respondent experience as it is more robust to interruptions, corrections, and other idiosyncrasies of human speech.   We built and tested an AI system to conduct quantitative surveys based on large language models (LLM), automatic speech recognition (ASR), and speech synthesis technologies. The system was specifically designed for quantitative research, and strictly adhered to research best practices like question order randomization, answer order randomization, and exact wording.   To validate the systemâ€™s effectiveness, we deployed it to conduct two pilot surveys with the SSRS Opinion Panel and followed-up with a separate human-administered survey to assess respondent experiences. We measured three key metrics: the survey completion rates, break-off rates, and respondent satisfaction scores. Our results suggest that shorter instruments and more responsive AI interviewers may contribute to improvements across all three metrics studied. </p>
<blockquote>
<p>éšç€è¯­éŸ³äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„å…´èµ·ï¼Œå®šé‡è°ƒæŸ¥ç ”ç©¶äººå‘˜è·å¾—äº†ä¸€ç§æ–°çš„æ•°æ®æ”¶é›†æ¨¡å¼ï¼šäººå·¥æ™ºèƒ½ç”µè¯è°ƒæŸ¥ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨äººå·¥æ™ºèƒ½è¿›è¡Œç”µè¯é‡‡è®¿ï¼Œå¯ä»¥åœ¨æ‰©å¤§å®šé‡ç ”ç©¶è§„æ¨¡çš„åŒæ—¶ï¼Œå¹³è¡¡äººç±»èˆ¬çš„äº’åŠ¨å’Œæ–¹æ³•çš„ä¸¥è°¨æ€§è¿™ä¸¤ä¸ªç›®æ ‡ã€‚ä¸åŒäºæ—©æœŸä½¿ç”¨äº¤äº’å¼è¯­éŸ³åº”ç­”ï¼ˆIVRï¼‰æŠ€æœ¯è‡ªåŠ¨è¿›è¡Œè¿™äº›è°ƒæŸ¥çš„åŠªåŠ›ï¼Œè¯­éŸ³äººå·¥æ™ºèƒ½åœ¨åº”å¯¹ä¸­æ–­ã€çº æ­£ä»¥åŠå…¶ä»–äººç±»è¯­è¨€çš„ç‰¹æ®Šç‰¹å¾æ–¹é¢æ›´ä¸ºå¼ºå¤§ï¼Œèƒ½å¤Ÿä¸ºå—è®¿è€…æä¾›æ›´è‡ªç„¶å’Œé€‚åº”æ€§æ›´å¼ºçš„ä½“éªŒã€‚æˆ‘ä»¬æ„å»ºå¹¶æµ‹è¯•äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³åˆæˆæŠ€æœ¯çš„AIç³»ç»Ÿæ¥è¿›è¡Œå®šé‡è°ƒæŸ¥ã€‚è¯¥ç³»ç»Ÿä¸“ä¸ºå®šé‡ç ”ç©¶è®¾è®¡ï¼Œä¸¥æ ¼éµå®ˆç ”ç©¶æœ€ä½³å®è·µï¼Œå¦‚é—®é¢˜é¡ºåºéšæœºåŒ–ã€ç­”æ¡ˆé¡ºåºéšæœºåŒ–ä»¥åŠç²¾ç¡®æªè¾ã€‚ä¸ºäº†éªŒè¯ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å°†å…¶ç”¨äºå¼€å±•ä¸¤é¡¹è¯•ç‚¹è°ƒæŸ¥ï¼Œå¹¶ä½¿ç”¨äººç±»ç®¡ç†çš„è°ƒæŸ¥æ¥è¯„ä¼°å—è®¿è€…ä½“éªŒã€‚æˆ‘ä»¬è¡¡é‡äº†ä¸‰ä¸ªå…³é”®æŒ‡æ ‡ï¼šè°ƒæŸ¥å®Œæˆç‡ã€ä¸­æ–­ç‡å’Œå—è®¿è€…æ»¡æ„åº¦å¾—åˆ†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¾ƒçŸ­çš„ä»ªå™¨å’Œååº”æ›´å¿«çš„AIé‡‡è®¿è€…å¯èƒ½åœ¨æé«˜æ‰€æœ‰ä¸‰ä¸ªç ”ç©¶æŒ‡æ ‡çš„æ–¹é¢å‘æŒ¥ä½œç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17718v1">PDF</a> </p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½ç”µè¯è°ƒæŸ¥æˆä¸ºå®šé‡è°ƒæŸ¥çš„æ–°æ¨¡å¼ã€‚åˆ©ç”¨äººå·¥æ™ºèƒ½è¿›è¡Œç”µè¯é‡‡è®¿ï¼Œç ”ç©¶è€…å¯ä»¥åœ¨æ‰©å¤§ç ”ç©¶è§„æ¨¡çš„åŒæ—¶ï¼Œå¹³è¡¡äººç±»äº’åŠ¨å’Œæ–¹æ³•è®ºä¸¥è°¨æ€§çš„åŒé‡ç›®æ ‡ã€‚æ–°ç³»ç»ŸåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³åˆæˆæŠ€æœ¯æ„å»ºï¼Œä¸¥æ ¼éµå¾ªç ”ç©¶æœ€ä½³å®è·µã€‚é€šè¿‡è¯•ç‚¹è°ƒæŸ¥å’Œåç»­äººå·¥è°ƒæŸ¥ï¼Œå‘ç°æ›´çŸ­çš„é—®å·å’Œæ›´æ™ºèƒ½çš„AIé‡‡è®¿è€…å¯èƒ½æœ‰åŠ©äºæé«˜è°ƒæŸ¥å®Œæˆç‡ã€é™ä½ä¸­æ–­ç‡å’Œæé«˜å—è®¿è€…æ»¡æ„åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”µè¯è°ƒæŸ¥æ˜¯æ–°å…´çš„æ•°æ®æ”¶é›†æ¨¡å¼ã€‚</li>
<li>AIæŠ€æœ¯èƒ½ä½¿ç ”ç©¶è€…æ‰©å¤§ç ”ç©¶è§„æ¨¡åŒæ—¶ä¿æŒä¸å—è®¿è€…çš„äº’åŠ¨æ€§å’Œæ–¹æ³•ä¸¥è°¨æ€§ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„äº’åŠ¨è¯­éŸ³åº”ç­”ç³»ç»Ÿç›¸æ¯”ï¼Œæ–°çš„è¯­éŸ³AIæŠ€æœ¯æ›´èƒ½é€‚åº”äººç±»è¯­è¨€çš„ç‹¬ç‰¹æ€§å’Œå˜åŒ–æ€§ã€‚</li>
<li>æ–°ç³»ç»ŸåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯­éŸ³åˆæˆæŠ€æœ¯æ„å»ºã€‚</li>
<li>ç³»ç»Ÿè®¾è®¡ä¸¥æ ¼éµå¾ªç ”ç©¶æœ€ä½³å®è·µï¼Œå¦‚é—®é¢˜é¡ºåºéšæœºåŒ–ã€ç­”æ¡ˆé¡ºåºéšæœºåŒ–å’Œç²¾ç¡®æªè¾ã€‚</li>
<li>é€šè¿‡è¯•ç‚¹è°ƒæŸ¥å’Œåç»­è¯„ä¼°ï¼Œå‘ç°æ›´çŸ­çš„é—®å·å’Œæ›´æ™ºèƒ½çš„AIé‡‡è®¿è€…èƒ½æé«˜è°ƒæŸ¥æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6d5943c8dcb061a4928e4cd44a733fb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a52022e4d8406778e351022551b33fce.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5c59e7176ed1f7cc718a6aa019544463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c51408fc524523349a5e934b2fd8e4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed87625628edd096e92ec9c5a95aeda8.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Seed-LiveInterpret-2-0-End-to-end-Simultaneous-Speech-to-speech-Translation-with-Your-Voice"><a href="#Seed-LiveInterpret-2-0-End-to-end-Simultaneous-Speech-to-speech-Translation-with-Your-Voice" class="headerlink" title="Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech   Translation with Your Voice"></a>Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech   Translation with Your Voice</h2><p><strong>Authors:Shanbo Cheng, Yu Bao, Zhichao Huang, Yu Lu, Ningxin Peng, Lu Xu, Runsheng Yu, Rong Cao, Yujiao Du, Ting Han, Yuxiang Hu, Zeyang Li, Sitong Liu, Shengtao Ma, Shiguang Pan, Jiongchen Xiao, Nuo Xu, Meng Yang, Rong Ye, Yiming Yu, Jun Zhang, Ruofei Zhang, Wanyi Zhang, Wenhao Zhu, Liehao Zou, Lu Lu, Yuxuan Wang, Yonghui Wu</strong></p>
<p>Simultaneous Interpretation (SI) represents one of the most daunting frontiers in the translation industry, with product-level automatic systems long plagued by intractable challenges: subpar transcription and translation quality, lack of real-time speech generation, multi-speaker confusion, and translated speech inflation, especially in long-form discourses. In this study, we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning capabilities. As a fully operational product-level solution, Seed-LiveInterpret 2.0 tackles these challenges head-on through our novel duplex speech-to-speech understanding-generating framework. Experimental results demonstrate that through large-scale pretraining and reinforcement learning, the model achieves a significantly better balance between translation accuracy and latency, validated by human interpreters to exceed 70% correctness in complex scenarios. Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by significant margins in translation quality, while slashing the average latency of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is around a near 70% reduction that drastically enhances practical usability. </p>
<blockquote>
<p>åŒå£°ä¼ è¯‘ï¼ˆSIï¼‰æ˜¯ç¿»è¯‘è¡Œä¸šä¸­æœ€å…·æŒ‘æˆ˜æ€§çš„å‰æ²¿é¢†åŸŸä¹‹ä¸€ï¼Œäº§å“çº§çš„è‡ªåŠ¨ç³»ç»Ÿé•¿æœŸä»¥æ¥ä¸€ç›´é¢ä¸´ç€éš¾ä»¥è§£å†³çš„æŒ‘æˆ˜ï¼šè½¬å½•å’Œç¿»è¯‘è´¨é‡ä¸ä½³ã€ç¼ºä¹å®æ—¶è¯­éŸ³ç”Ÿæˆã€å¤šè¯´è¯è€…æ··æ·†ï¼Œä»¥åŠåœ¨é•¿ç¯‡è®²è¯ä¸­å°¤å…¶ä¸¥é‡çš„ç¿»è¯‘åè¯­éŸ³è†¨èƒ€é—®é¢˜ã€‚æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Seed-LiveInterpret 2.0ï¼Œè¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„åŒå£°ä¼ è¯‘æ¨¡å‹ï¼Œå…·æœ‰é«˜ä¿çœŸã€è¶…ä½å»¶è¿Ÿçš„è¯­éŸ³åˆ°è¯­éŸ³ç”ŸæˆåŠŸèƒ½ï¼Œä»¥åŠè¯­éŸ³å…‹éš†èƒ½åŠ›ã€‚ä½œä¸ºå…¨æ–¹ä½çš„äº§å“çº§è§£å†³æ–¹æ¡ˆï¼ŒSeed-LiveInterpret 2.0é€šè¿‡æˆ‘ä»¬æ–°é¢–çš„åŒè¯­è¯­éŸ³åˆ°è¯­éŸ³ç†è§£ç”Ÿæˆæ¡†æ¶ï¼Œç›´æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œè¯¥æ¨¡å‹åœ¨ç¿»è¯‘å‡†ç¡®æ€§å’Œå»¶è¿Ÿä¹‹é—´å–å¾—äº†æ˜¾è‘—çš„å¹³è¡¡ï¼Œç»äººç±»ç¿»è¯‘äººå‘˜éªŒè¯ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸­çš„æ­£ç¡®ç‡è¶…è¿‡70%ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒSeed-LiveInterpret 2.0åœ¨ç¿»è¯‘è´¨é‡ä¸Šå¤§å¤§ä¼˜äºå•†ä¸šåŒå£°ä¼ è¯‘è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶å°†å…‹éš†è¯­éŸ³çš„å¹³å‡å»¶è¿Ÿä»è¿‘10ç§’å‡å°‘åˆ°æ¥è¿‘å®æ—¶çš„3ç§’ï¼Œå¤§çº¦é™ä½äº†70%ï¼Œå¤§å¤§æé«˜äº†å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17527v3">PDF</a> Seed-LiveInterpret 2.0 Technical Report</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Simultaneous Interpretationï¼ˆSIï¼‰é¢†åŸŸçš„ä¸€ä¸ªå…¨æ–°è§£å†³æ–¹æ¡ˆâ€”â€”Seed-LiveInterpret 2.0ã€‚ä½œä¸ºä¸€æ¬¾ç«¯åˆ°ç«¯çš„äº§å“çº§å³æ—¶ç¿»è¯‘æ¨¡å‹ï¼Œå®ƒå…·å¤‡é«˜ä¿çœŸã€è¶…ä½å»¶è¿Ÿçš„è¯­éŸ³åˆ°è¯­éŸ³ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶å…·å¤‡è¯­éŸ³å…‹éš†åŠŸèƒ½ã€‚é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼ŒSeed-LiveInterpret 2.0æ˜¾è‘—æé«˜äº†ç¿»è¯‘å‡†ç¡®æ€§å¹¶é™ä½äº†å»¶è¿Ÿï¼Œä¸”å¤æ‚æ€§åœºæ™¯ä¸‹çš„ç¿»è¯‘æ­£ç¡®ç‡è¶…è¿‡70%ï¼Œæ˜¾è‘—ä¼˜äºå•†ä¸šå³æ—¶ç¿»è¯‘è§£å†³æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Seed-LiveInterpret 2.0æ˜¯Simultaneous Interpretationé¢†åŸŸçš„ä¸€ä¸ªæ–°äº§å“çº§è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å®ƒå…·å¤‡ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆ°è¯­éŸ³ç”Ÿæˆèƒ½åŠ›ï¼Œå…·æœ‰é«˜ä¿çœŸå’Œè¶…ä½å»¶è¿Ÿçš„ç‰¹ç‚¹ã€‚</li>
<li>è¯¥æ¨¡å‹å…·å¤‡è¯­éŸ³å…‹éš†åŠŸèƒ½ï¼Œæé«˜äº†å®ç”¨æ€§ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼ŒSeed-LiveInterpret 2.0åœ¨ç¿»è¯‘å‡†ç¡®æ€§å’Œå»¶è¿Ÿæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚</li>
<li>åœ¨å¤æ‚æ€§åœºæ™¯ä¸‹ï¼ŒSeed-LiveInterpret 2.0çš„ç¿»è¯‘æ­£ç¡®ç‡è¶…è¿‡70%ã€‚</li>
<li>ä¸å•†ä¸šå³æ—¶ç¿»è¯‘è§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼ŒSeed-LiveInterpret 2.0åœ¨ç¿»è¯‘è´¨é‡ä¸Šè¡¨ç°æ›´ä¼˜ç§€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17527">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f617e070d83b8d49fb028c29f9b86d70.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4885915c8965569c31e65217218af2c1.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Step-Audio-2-Technical-Report"><a href="#Step-Audio-2-Technical-Report" class="headerlink" title="Step-Audio 2 Technical Report"></a>Step-Audio 2 Technical Report</h2><p><strong>Authors:Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang Zhang, Jingbei Li, Mingrui Chen, Peng Liu, Wang You, Xiangyu Tony Zhang, Xingyuan Li, Xuerui Yang, Yayue Deng, Yechang Huang, Yuxin Li, Yuxin Zhang, Zhao You, Brian Li, Changyi Wan, Hanpeng Hu, Jiangjie Zhen, Siyu Chen, Song Yuan, Xuelin Zhang, Yimin Jiang, Yu Zhou, Yuxiang Yang, Bingxin Li, Buyun Ma, Changhe Song, Dongqing Pang, Guoqiang Hu, Haiyang Sun, Kang An, Na Wang, Shuli Gao, Wei Ji, Wen Li, Wen Sun, Xuan Wen, Yong Ren, Yuankai Ma, Yufan Lu, Bin Wang, Bo Li, Changxin Miao, Che Liu, Chen Xu, Dapeng Shi, Dingyuan Hu, Donghang Wu, Enle Liu, Guanzhe Huang, Gulin Yan, Han Zhang, Hao Nie, Haonan Jia, Hongyu Zhou, Jianjian Sun, Jiaoren Wu, Jie Wu, Jie Yang, Jin Yang, Junzhe Lin, Kaixiang Li, Lei Yang, Liying Shi, Li Zhou, Longlong Gu, Ming Li, Mingliang Li, Mingxiao Li, Nan Wu, Qi Han, Qinyuan Tan, Shaoliang Pang, Shengjie Fan, Siqi Liu, Tiancheng Cao, Wanying Lu, Wenqing He, Wuxun Xie, Xu Zhao, Xueqi Li, Yanbo Yu, Yang Yang, Yi Liu, Yifan Lu, Yilei Wang, Yuanhao Ding, Yuanwei Liang, Yuanwei Lu, Yuchu Luo, Yuhe Yin, Yumeng Zhan, Yuxiang Zhang, Zidong Yang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Xiangyu Zhang, Yibo Zhu</strong></p>
<p>This paper presents Step-Audio 2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit <a target="_blank" rel="noopener" href="https://github.com/stepfun-ai/Step-Audio2">https://github.com/stepfun-ai/Step-Audio2</a> for more information. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†Step-Audio 2ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºå·¥ä¸šçº§éŸ³é¢‘ç†è§£å’Œè¯­éŸ³å¯¹è¯è€Œè®¾è®¡çš„ç«¯åˆ°ç«¯å¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡é›†æˆæ½œåœ¨éŸ³é¢‘ç¼–ç å™¨å’Œä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼ŒStep-Audio 2åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’ŒéŸ³é¢‘ç†è§£æ–¹é¢å–å¾—äº†æœ‰å‰æ™¯çš„æ€§èƒ½ã€‚ä¸ºäº†å®ç°çœŸæ­£çš„ç«¯åˆ°ç«¯è¯­éŸ³å¯¹è¯ï¼ŒStep-Audio 2å°†ç¦»æ•£éŸ³é¢‘ç¬¦å·çš„ç”Ÿæˆèå…¥è¯­è¨€å»ºæ¨¡ï¼Œæå¤§åœ°æé«˜äº†å¯¹è¯¸å¦‚è®²è¯é£æ ¼å’Œæƒ…æ„Ÿç­‰å‰¯è¯­è¨€ä¿¡æ¯çš„ååº”èƒ½åŠ›ã€‚ä¸ºäº†æœ‰æ•ˆåœ°åˆ©ç”¨ç°å®æ•°æ®ä¸­ä¸°å¯Œçš„æ–‡æœ¬å’Œå£°éŸ³çŸ¥è¯†ï¼ŒStep-Audio 2é›†æˆäº†å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œå¹¶èƒ½å¤Ÿè°ƒç”¨å¤–éƒ¨å·¥å…·ï¼Œå¦‚ç½‘ç»œæœç´¢æ¥å‡è½»è™šæ„æ€§ï¼Œä»¥åŠéŸ³é¢‘æœç´¢æ¥åˆ‡æ¢éŸ³è‰²ã€‚åœ¨æ•°ç™¾ä¸‡å°æ—¶çš„è¯­éŸ³å’ŒéŸ³é¢‘æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒStep-Audio 2åœ¨ä¸åŒå¯¹è¯åœºæ™¯ä¸­å±•ç°å‡ºæ™ºèƒ½å’Œè¡¨ç°åŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–å¼€æºå’Œå•†ä¸šè§£å†³æ–¹æ¡ˆç›¸æ¯”ï¼ŒStep-Audio 2åœ¨å„ç§éŸ³é¢‘ç†è§£å’Œå¯¹è¯åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ›´å¤šä¿¡æ¯è¯·è®¿é—® <a target="_blank" rel="noopener" href="https://github.com/stepfun-ai/Step-Audio2%E3%80%82">https://github.com/stepfun-ai/Step-Audio2ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16632v2">PDF</a> </p>
<p><strong>Summary</strong><br>     è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†Step-Audio 2ï¼Œä¸€ä¸ªé¢å‘å·¥ä¸šçº§éŸ³é¢‘ç†è§£å’Œè¯­éŸ³å¯¹è¯çš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚å®ƒé€šè¿‡ç»“åˆæ½œåœ¨éŸ³é¢‘ç¼–ç å™¨å’Œä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„å¼ºåŒ–å­¦ä¹ ï¼Œå®ç°äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’ŒéŸ³é¢‘ç†è§£çš„å‡ºè‰²æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒç”Ÿæˆç¦»æ•£éŸ³é¢‘ç¬¦å·ä»¥å¢å¼ºå¯¹å£è¯­é£æ ¼å’Œæƒ…æ„Ÿç­‰éè¯­è¨€ä¿¡æ¯çš„å“åº”æ€§ã€‚é›†æˆæ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•å’Œå¯è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚ç½‘ç»œæœç´¢å’ŒéŸ³é¢‘æœç´¢ï¼‰ä»¥é€‚åº”ç°å®ä¸–ç•Œæ•°æ®çš„ä¸°å¯Œæ–‡æœ¬å’Œå£°å­¦çŸ¥è¯†ã€‚ç»è¿‡æ•°ç™¾ä¸‡å°æ—¶è¯­éŸ³å’ŒéŸ³é¢‘æ•°æ®çš„è®­ç»ƒï¼ŒStep-Audio 2åœ¨å¤šç§å¯¹è¯åœºæ™¯ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ™ºèƒ½å’Œè¡¨ç°åŠ›ï¼Œå¹¶åœ¨å„ç§éŸ³é¢‘ç†è§£å’Œå¯¹è¯åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸å…¶ä»–å¼€æºå’Œå•†ä¸šè§£å†³æ–¹æ¡ˆç›¸æ¯”çš„ä¸šç•Œé¢†å…ˆæ°´å¹³ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·è®¿é—®â€‹â€‹ <a target="_blank" rel="noopener" href="https://github.com/stepfun-ai/Step-Audio2%E2%80%8B%E2%80%8B%E3%80%82">https://github.com/stepfun-ai/Step-Audio2â€‹â€‹ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Step-Audio 2æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€‚ç”¨äºå·¥ä¸šçº§éŸ³é¢‘ç†è§£å’Œè¯­éŸ³å¯¹è¯ã€‚</li>
<li>ç»“åˆæ½œåœ¨éŸ³é¢‘ç¼–ç å™¨å’Œä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„å¼ºåŒ–å­¦ä¹ ï¼Œæé«˜äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’ŒéŸ³é¢‘ç†è§£çš„æ€§èƒ½ã€‚</li>
<li>ç”Ÿæˆç¦»æ•£éŸ³é¢‘ç¬¦å·ä»¥å¢å¼ºå¯¹å£è¯­é£æ ¼å’Œæƒ…æ„Ÿçš„å“åº”æ€§ã€‚</li>
<li>é›†æˆæ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹æ³•ä»¥é€‚åº”ç°å®ä¸–ç•Œæ•°æ®çš„ä¸°å¯Œæ–‡æœ¬å’Œå£°å­¦çŸ¥è¯†ã€‚</li>
<li>å¯è°ƒç”¨å¤–éƒ¨å·¥å…·å¦‚ç½‘ç»œæœç´¢å’ŒéŸ³é¢‘æœç´¢æ¥å¢å¼ºåŠŸèƒ½ã€‚</li>
<li>ç»è¿‡å¤§é‡è¯­éŸ³å’ŒéŸ³é¢‘æ•°æ®è®­ç»ƒï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ™ºèƒ½å’Œè¡¨ç°åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16632">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3f7229de128cada59706590637f9ebec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8dffe6268e0b9add60849474d39a1ffc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c4118cf7f6f88eff17a7b3171c84da9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b8953103d7465d3d734dcce4fe0b178.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="TalkLess-Blending-Extractive-and-Abstractive-Speech-Summarization-for-Editing-Speech-to-Preserve-Content-and-Style"><a href="#TalkLess-Blending-Extractive-and-Abstractive-Speech-Summarization-for-Editing-Speech-to-Preserve-Content-and-Style" class="headerlink" title="TalkLess: Blending Extractive and Abstractive Speech Summarization for   Editing Speech to Preserve Content and Style"></a>TalkLess: Blending Extractive and Abstractive Speech Summarization for   Editing Speech to Preserve Content and Style</h2><p><strong>Authors:Karim Benharrak, Puyuan Peng, Amy Pavel</strong></p>
<p>Millions of people listen to podcasts, audio stories, and lectures, but editing speech remains tedious and time-consuming. Creators remove unnecessary words, cut tangential discussions, and even re-record speech to make recordings concise and engaging. Prior work automatically summarized speech by removing full sentences (extraction), but rigid extraction limits expressivity. AI tools can summarize then re-synthesize speech (abstraction), but abstraction strips the speakerâ€™s style. We present TalkLess, a system that flexibly combines extraction and abstraction to condense speech while preserving its content and style. To edit speech, TalkLess first generates possible transcript edits, selects edits to maximize compression, coverage, and audio quality, then uses a speech editing model to translate transcript edits into audio edits. TalkLessâ€™s interface provides creators control over automated edits by separating low-level wording edits (via the compression pane) from major content edits (via the outline pane). TalkLess achieves higher coverage and removes more speech errors than a state-of-the-art extractive approach. A comparison study (N&#x3D;12) showed that TalkLess significantly decreased cognitive load and editing effort in speech editing. We further demonstrate TalkLessâ€™s potential in an exploratory study (N&#x3D;3) where creators edited their own speech. </p>
<blockquote>
<p>æˆåƒä¸Šä¸‡çš„äººè†å¬æ’­å®¢ã€éŸ³é¢‘æ•…äº‹å’Œè®²åº§ï¼Œä½†ç¼–è¾‘è¯­éŸ³ä»ç„¶æ˜¯ä¸€é¡¹ä¹å‘³ä¸”è€—æ—¶çš„ä»»åŠ¡ã€‚åˆ›ä½œè€…ä¼šåˆ é™¤ä¸å¿…è¦çš„è¯æ±‡ã€å‰ªè£ç¦»é¢˜è®¨è®ºï¼Œç”šè‡³é‡æ–°å½•åˆ¶è¯­éŸ³ï¼Œä»¥ä½¿å½•éŸ³æ›´åŠ ç®€æ´ã€å¸å¼•äººã€‚ä¹‹å‰çš„å·¥ä½œé€šè¿‡è‡ªåŠ¨ç§»é™¤æ•´ä¸ªå¥å­ï¼ˆæ‘˜å½•ï¼‰æ¥è¿›è¡Œè¯­éŸ³æ‘˜è¦ï¼Œä½†åƒµåŒ–çš„æ‘˜å½•é™åˆ¶äº†è¡¨è¾¾åŠ›ã€‚AIå·¥å…·å¯ä»¥æ‘˜è¦ç„¶åé‡æ–°åˆæˆè¯­éŸ³ï¼ˆæŠ½è±¡ï¼‰ï¼Œä½†æŠ½è±¡ä¼šå‰¥å¤ºè¯´è¯è€…çš„é£æ ¼ã€‚æˆ‘ä»¬æ¨å‡ºäº†TalkLessç³»ç»Ÿï¼Œå®ƒçµæ´»åœ°ç»“åˆäº†æ‘˜å½•å’ŒæŠ½è±¡ï¼Œæµ“ç¼©è¯­éŸ³çš„åŒæ—¶ä¿ç•™å…¶å†…å®¹ä¸é£æ ¼ã€‚ä¸ºäº†ç¼–è¾‘è¯­éŸ³ï¼ŒTalkLessé¦–å…ˆç”Ÿæˆå¯èƒ½çš„è½¬å½•ç¼–è¾‘ï¼Œé€‰æ‹©ç¼–è¾‘ä»¥æœ€å¤§åŒ–å‹ç¼©ã€è¦†ç›–å’ŒéŸ³é¢‘è´¨é‡ï¼Œç„¶åä½¿ç”¨è¯­éŸ³ç¼–è¾‘æ¨¡å‹å°†è½¬å½•ç¼–è¾‘è½¬åŒ–ä¸ºéŸ³é¢‘ç¼–è¾‘ã€‚TalkLessçš„ç•Œé¢é€šè¿‡å‹ç¼©é¢æ¿æä¾›ä½çº§åˆ«çš„æªè¾ç¼–è¾‘ï¼Œé€šè¿‡å¤§çº²é¢æ¿æä¾›ä¸»è¦å†…å®¹çš„ç¼–è¾‘ï¼Œä»è€Œå®ç°å¯¹è‡ªåŠ¨åŒ–ç¼–è¾‘çš„æ§åˆ¶ã€‚TalkLessçš„è¦†ç›–é¢æ›´å¹¿ï¼Œèƒ½æ¶ˆé™¤æ¯”æœ€å…ˆè¿›çš„æ‘˜å½•æ–¹æ³•æ›´å¤šçš„è¯­éŸ³é”™è¯¯ã€‚ä¸€é¡¹æ¯”è¾ƒç ”ç©¶ï¼ˆN&#x3D;12ï¼‰æ˜¾ç¤ºï¼ŒTalkLessæ˜¾è‘—é™ä½äº†è¯­éŸ³ç¼–è¾‘çš„è®¤çŸ¥è´Ÿè·å’Œç¼–è¾‘å·¥ä½œé‡ã€‚æˆ‘ä»¬åœ¨å¦ä¸€é¡¹æ¢ç´¢æ€§ç ”ç©¶ï¼ˆN&#x3D;3ï¼‰ä¸­è¿›ä¸€æ­¥å±•ç¤ºäº†TalkLessçš„æ½œåŠ›ï¼Œåˆ›ä½œè€…åœ¨è¯¥ç ”ç©¶ä¸­ç¼–è¾‘äº†è‡ªå·±çš„è¯­éŸ³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15202v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è°ˆåˆ°ä¸€ä¸ªåä¸ºTalkLessçš„ç³»ç»Ÿï¼Œå®ƒç»“åˆäº†æå–å’ŒæŠ½è±¡æŠ€æœ¯ï¼Œèƒ½å¤Ÿå‹ç¼©æ¼”è®²å†…å®¹åŒæ—¶ä¿ç•™å†…å®¹å’Œé£æ ¼ã€‚è¯¥ç³»ç»Ÿé¦–å…ˆç”Ÿæˆå¯èƒ½çš„æ–‡æœ¬ç¼–è¾‘ï¼Œé€‰æ‹©èƒ½æœ€å¤§åŒ–å‹ç¼©ã€è¦†ç›–å’ŒéŸ³é¢‘è´¨é‡çš„ç¼–è¾‘ï¼Œç„¶åä½¿ç”¨è¯­éŸ³ç¼–è¾‘æ¨¡å‹å°†æ–‡æœ¬ç¼–è¾‘è½¬åŒ–ä¸ºéŸ³é¢‘ç¼–è¾‘ã€‚TalkLessç•Œé¢åˆ†ä¸ºå‹ç¼©é¢æ¿å’Œå†…å®¹å¤§çº²é¢æ¿ï¼Œä½¿åˆ›ä½œè€…èƒ½åˆ†åˆ«æ§åˆ¶ä½çº§åˆ«çš„æªè¾ç¼–è¾‘å’Œä¸»è¦çš„å†…å®¹ç¼–è¾‘ã€‚TalkLessç›¸æ¯”æœ€å…ˆè¿›æŠ½å–æ–¹æ³•èƒ½è¾¾åˆ°æ›´é«˜çš„è¦†ç›–ç‡å’Œå‡å°‘æ›´å¤šçš„è¯­éŸ³é”™è¯¯ã€‚ç ”ç©¶å‘ç°ï¼ˆN&#x3D;12ï¼‰ï¼ŒTalkLessèƒ½æ˜¾è‘—é™ä½è¯­éŸ³ç¼–è¾‘çš„è®¤çŸ¥è´Ÿè·å’Œç¼–è¾‘åŠªåŠ›ã€‚å¦å¤–ä¸€é¡¹æ¢ç´¢æ€§ç ”ç©¶ï¼ˆN&#x3D;3ï¼‰æ˜¾ç¤ºåˆ›ä½œè€…èƒ½å¤Ÿåˆ©ç”¨TalkLessç¼–è¾‘è‡ªå·±çš„è¯­éŸ³å†…å®¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TalkLessç³»ç»Ÿç»“åˆäº†æå–å’ŒæŠ½è±¡æŠ€æœ¯ï¼Œå®ç°äº†è¯­éŸ³å†…å®¹çš„å‹ç¼©å¹¶ä¿ç•™äº†å†…å®¹å’Œé£æ ¼ã€‚</li>
<li>TalkLessé¦–å…ˆç”Ÿæˆå¯èƒ½çš„æ–‡æœ¬ç¼–è¾‘ï¼Œç„¶åé€‰æ‹©æœ€ä½³çš„ç¼–è¾‘æ¥æé«˜å‹ç¼©æ•ˆç‡å’ŒéŸ³é¢‘è´¨é‡ã€‚</li>
<li>TalkLessçš„ç¼–è¾‘ç•Œé¢åˆ†ä¸ºå‹ç¼©é¢æ¿å’Œå†…å®¹å¤§çº²é¢æ¿ï¼Œæ–¹ä¾¿åˆ›ä½œè€…è¿›è¡Œä¸åŒå±‚æ¬¡çš„ç¼–è¾‘æ“ä½œã€‚</li>
<li>ä¸ç°æœ‰æŠ€æœ¯ç›¸æ¯”ï¼ŒTalkLessåœ¨è¦†ç›–ç‡å’Œè¯­éŸ³é”™è¯¯å¤„ç†æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚</li>
<li>ç ”ç©¶æ˜¾ç¤ºï¼ŒTalkLessèƒ½æ˜¾è‘—é™ä½è¯­éŸ³ç¼–è¾‘çš„è®¤çŸ¥è´Ÿè·å’Œç¼–è¾‘åŠªåŠ›ã€‚</li>
<li>TalkLessé€šè¿‡ä¸€é¡¹æ¢ç´¢æ€§ç ”ç©¶å±•ç¤ºäº†åœ¨åˆ›ä½œè€…è‡ªæˆ‘ç¼–è¾‘è¯­éŸ³æ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-55235e82d1e55a0bee8796bc16b5707d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6b8a277e9093b384c67d9a963f734a74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-79c876ce7b34971c441b8bc71d9e8c0b.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-eb1acddccc88c00a5d5e6ee7f516b757.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  Robust Deepfake Detection for Electronic Know Your Customer Systems   Using Registered Images
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d9b5c985a25194e12816c9a76be2b43c.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  Subtyping Breast Lesions via Generative Augmentation based Long-tailed   Recognition in Ultrasound
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28879.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
