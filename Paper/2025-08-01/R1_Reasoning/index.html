<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  SeqAffordSplat Scene-level Sequential Affordance Reasoning on 3D   Gaussian Splatting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-4cd61fb10030db1001c05898e30f80de.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-01-æ›´æ–°"><a href="#2025-08-01-æ›´æ–°" class="headerlink" title="2025-08-01 æ›´æ–°"></a>2025-08-01 æ›´æ–°</h1><h2 id="SeqAffordSplat-Scene-level-Sequential-Affordance-Reasoning-on-3D-Gaussian-Splatting"><a href="#SeqAffordSplat-Scene-level-Sequential-Affordance-Reasoning-on-3D-Gaussian-Splatting" class="headerlink" title="SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D   Gaussian Splatting"></a>SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D   Gaussian Splatting</h2><p><strong>Authors:Di Li, Jie Feng, Jiahao Chen, Weisheng Dong, Guanbin Li, Yuhui Zheng, Mingtao Feng, Guangming Shi</strong></p>
<p>3D affordance reasoning, the task of associating human instructions with the functional regions of 3D objects, is a critical capability for embodied agents. Current methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited to single-object, single-step interactions, a paradigm that falls short of addressing the long-horizon, multi-object tasks required for complex real-world applications. To bridge this gap, we introduce the novel task of Sequential 3D Gaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale benchmark featuring 1800+ scenes to support research on long-horizon affordance understanding in complex 3DGS environments. We then propose SeqSplatNet, an end-to-end framework that directly maps an instruction to a sequence of 3D affordance masks. SeqSplatNet employs a large language model that autoregressively generates text interleaved with special segmentation tokens, guiding a conditional decoder to produce the corresponding 3D mask. To handle complex scene geometry, we introduce a pre-training strategy, Conditional Geometric Reconstruction, where the model learns to reconstruct complete affordance region masks from known geometric observations, thereby building a robust geometric prior. Furthermore, to resolve semantic ambiguities, we design a feature injection mechanism that lifts rich semantic features from 2D Vision Foundation Models (VFM) and fuses them into the 3D decoder at multiple scales. Extensive experiments demonstrate that our method sets a new state-of-the-art on our challenging benchmark, effectively advancing affordance reasoning from single-step interactions to complex, sequential tasks at the scene level. </p>
<blockquote>
<p>3Då¯ç”¨æ€§æ¨ç†æ˜¯å°†äººç±»æŒ‡ä»¤ä¸3Då¯¹è±¡çš„åŠŸèƒ½åŒºåŸŸç›¸å…³è”çš„ä»»åŠ¡ï¼Œæ˜¯å®ä½“ä»£ç†çš„å…³é”®èƒ½åŠ›ã€‚å½“å‰åŸºäº3Dé«˜æ–¯å–·æ¶‚ï¼ˆ3DGSï¼‰çš„æ–¹æ³•ä»æ ¹æœ¬ä¸Šå±€é™äºå•å¯¹è±¡ã€å•æ­¥éª¤äº¤äº’çš„æ¨¡å¼ï¼Œè¿™ç§æ¨¡å¼æ— æ³•è§£å†³å¤æ‚ç°å®ä¸–ç•Œåº”ç”¨æ‰€éœ€çš„é•¿å‘¨æœŸã€å¤šå¯¹è±¡ä»»åŠ¡ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†Sequential 3D Gaussianå¯ç”¨æ€§æ¨ç†çš„æ–°ä»»åŠ¡ï¼Œå¹¶å»ºç«‹äº†SeqAffordSplatå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1800å¤šä¸ªåœºæ™¯ï¼Œæ”¯æŒåœ¨å¤æ‚3DGSç¯å¢ƒä¸­å¯¹é•¿æœŸå¯ç”¨æ€§ç†è§£çš„ç ”ç©¶ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†SeqSplatNetç«¯åˆ°ç«¯æ¡†æ¶ï¼Œå®ƒç›´æ¥å°†æŒ‡ä»¤æ˜ å°„åˆ°ä¸€ç³»åˆ—3Då¯ç”¨æ€§æ©è†œã€‚SeqSplatNeté‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡è‡ªå›å½’æ–¹å¼ç”Ÿæˆä¸ç‰¹æ®Šåˆ†å‰²ä»¤ç‰Œäº¤ç»‡çš„æ–‡æœ¬ï¼Œå¼•å¯¼æ¡ä»¶è§£ç å™¨ç”Ÿæˆç›¸åº”çš„3Dæ©è†œã€‚ä¸ºäº†å¤„ç†å¤æ‚åœºæ™¯çš„å‡ ä½•ç»“æ„ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢„è®­ç»ƒç­–ç•¥ï¼Œå³æ¡ä»¶å‡ ä½•é‡å»ºï¼Œæ¨¡å‹å­¦ä¼šä»å·²çŸ¥çš„å‡ ä½•è§‚å¯Ÿä¸­é‡å»ºå®Œæ•´çš„å¯ç”¨æ€§åŒºåŸŸæ©è†œï¼Œä»è€Œå»ºç«‹ç¨³å¥çš„å‡ ä½•å…ˆéªŒã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³è¯­ä¹‰æ­§ä¹‰ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç‰¹å¾æ³¨å…¥æœºåˆ¶ï¼Œä»2Dè§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰ä¸­æå–ä¸°å¯Œçš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶å°†å…¶èåˆåˆ°å¤šå°ºåº¦çš„3Dè§£ç å™¨ä¸­ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æˆ‘ä»¬çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šåˆ›ä¸‹äº†æœ€æ–°çºªå½•ï¼Œæœ‰æ•ˆåœ°å°†å¯ç”¨æ€§æ¨ç†ä»å•æ­¥éª¤äº¤äº’æ¨è¿›åˆ°åœºæ™¯çº§åˆ«çš„å¤æ‚ã€é¡ºåºä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23772v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å¤æ‚çš„ä¸‰ç»´ç¯å¢ƒä¸­è¿›è¡Œé•¿æœŸè§„åˆ’çš„å¤šç›®æ ‡äº¤äº’ä»»åŠ¡çš„é‡è¦æ€§ã€‚é’ˆå¯¹å½“å‰åŸºäºä¸‰ç»´é«˜æ–¯æ˜ å°„æ–¹æ³•çš„å±€é™æ€§ï¼Œæå‡ºäº†åºè´¯ä¸‰ç»´é«˜æ–¯é€‚ç”¨æ¨ç†çš„æ–°ä»»åŠ¡ï¼Œå¹¶å»ºç«‹äº†SeqAffordSplatå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•å¹³å°ï¼ŒåŒ…å«è¶…è¿‡1800ä¸ªåœºæ™¯ã€‚åŒæ—¶ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§ç«¯åˆ°ç«¯çš„æ¡†æ¶SeqSplatNetï¼Œè¯¥æ¡†æ¶å¯ç›´æ¥å°†æŒ‡ä»¤æ˜ å°„åˆ°ä¸€ç³»åˆ—ä¸‰ç»´é€‚ç”¨æ©è†œã€‚SeqSplatNeté‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡è‡ªå›å½’ç”Ÿæˆæ–‡æœ¬ä¸ç‰¹æ®Šåˆ†å‰²æ ‡è®°çš„äº¤ç»‡ï¼ŒæŒ‡å¯¼æ¡ä»¶è§£ç å™¨ç”Ÿæˆç›¸åº”çš„ä¸‰ç»´æ©è†œã€‚ä¸ºè§£å†³å¤æ‚åœºæ™¯å‡ ä½•é—®é¢˜ï¼Œå¼•å…¥äº†é¢„è®­ç»ƒç­–ç•¥â€”â€”æ¡ä»¶å‡ ä½•é‡å»ºï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ç‰¹å¾æ³¨å…¥æœºåˆ¶ï¼Œä»äºŒç»´è§†è§‰åŸºç¡€æ¨¡å‹ä¸­æå–ä¸°å¯Œçš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶å°†å…¶èåˆåˆ°ä¸‰ç»´è§£ç å™¨ä¸­ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•å¹³å°ä¸Šå–å¾—äº†æœ€æ–°æˆæœï¼Œå®ç°äº†ä»å•æ­¥äº¤äº’åˆ°å¤æ‚åœºæ™¯çº§åºè´¯ä»»åŠ¡çš„é€‚ç”¨æ€§æ¨ç†çš„è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D affordance reasoningæ˜¯å…³è”äººç±»æŒ‡ä»¤ä¸3Dç‰©ä½“åŠŸèƒ½åŒºåŸŸçš„å…³é”®èƒ½åŠ›ï¼Œå¯¹äºå®ä½“ä»£ç†è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰åŸºäº3D Gaussian Splatting (3DGS)çš„æ–¹æ³•ä»…é™äºå•ç›®æ ‡ã€å•æ­¥éª¤äº¤äº’ï¼Œæ— æ³•æ»¡è¶³å¤æ‚ç°å®ä¸–ç•Œåº”ç”¨ä¸­å¤šç›®æ ‡ã€é•¿æœŸè§„åˆ’çš„ä»»åŠ¡éœ€æ±‚ã€‚</li>
<li>ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºäº†Sequential 3D Gaussian Affordance Reasoningæ–°ä»»åŠ¡ï¼Œå¹¶å»ºç«‹SeqAffordSplatåŸºå‡†æµ‹è¯•å¹³å°ï¼Œæ¨¡æ‹ŸçœŸå®ç¯å¢ƒçš„å¤æ‚æ€§ã€‚</li>
<li>SeqSplatNetæ¡†æ¶å¯ä»¥ç›´æ¥å°†æŒ‡ä»¤è½¬æ¢ä¸ºä¸€ç³»åˆ—3Dæ©è†œï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è‡ªå›å½’ç”Ÿæˆæ–‡æœ¬ä¸åˆ†å‰²æ ‡è®°ã€‚</li>
<li>æ¡ä»¶å‡ ä½•é‡å»ºé¢„è®­ç»ƒç­–ç•¥ç”¨äºå¤„ç†å¤æ‚åœºæ™¯å‡ ä½•é—®é¢˜ï¼Œå»ºç«‹ç¨³å¥çš„å‡ ä½•å…ˆéªŒã€‚</li>
<li>é€šè¿‡ç‰¹å¾æ³¨å…¥æœºåˆ¶è§£å†³è¯­ä¹‰æ¨¡ç³Šé—®é¢˜ï¼Œå°†äºŒç»´è§†è§‰åŸºç¡€æ¨¡å‹çš„ä¸°å¯Œè¯­ä¹‰ç‰¹å¾èå…¥ä¸‰ç»´è§£ç å™¨ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-52fcd06f4e24d5923434944fdc122204.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42723816d289ea4eb2cdc248de4cff9c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2be20f2dc5a62b2156870f4d01e4ab69.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-06f335d5aec48beb87d4479869f0e46d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="CoT-Self-Instruct-Building-high-quality-synthetic-prompts-for-reasoning-and-non-reasoning-tasks"><a href="#CoT-Self-Instruct-Building-high-quality-synthetic-prompts-for-reasoning-and-non-reasoning-tasks" class="headerlink" title="CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning   and non-reasoning tasks"></a>CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning   and non-reasoning tasks</h2><p><strong>Authors:Ping Yu, Jack Lanchantin, Tianlu Wang, Weizhe Yuan, Olga Golovneva, Ilia Kulikov, Sainbayar Sukhbaatar, Jason Weston, Jing Xu</strong></p>
<p>We propose CoT-Self-Instruct, a synthetic data generation method that instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the given seed tasks, and then to generate a new synthetic prompt of similar quality and complexity for use in LLM training, followed by filtering for high-quality data with automatic metrics. In verifiable reasoning, our synthetic data significantly outperforms existing training datasets, such as s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For non-verifiable instruction-following tasks, our method surpasses the performance of human or standard self-instruct prompts on both AlpacaEval 2.0 and Arena-Hard. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†CoT-Self-Instructè¿™ä¸€åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é¦–å…ˆæ ¹æ®ç»™å®šçš„ç§å­ä»»åŠ¡é€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰è¿›è¡Œæ¨ç†å’Œè§„åˆ’ï¼Œç„¶åç”Ÿæˆè´¨é‡ç›¸ä¼¼ã€å¤æ‚æ€§ç›¸ä¼¼çš„æ–°åˆæˆæç¤ºï¼Œç”¨äºLLMè®­ç»ƒï¼Œéšåé€šè¿‡è‡ªåŠ¨åº¦é‡å¯¹é«˜è´¨é‡æ•°æ®è¿›è¡Œç­›é€‰ã€‚åœ¨å¯éªŒè¯æ¨ç†æ–¹é¢ï¼Œæˆ‘ä»¬çš„åˆæˆæ•°æ®åœ¨MATH500ã€AMC23ã€AIME24å’ŒGPQA-Diamondç­‰å¤šä¸ªæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„è®­ç»ƒæ•°æ®é›†ï¼Œå¦‚s1kå’ŒOpenMathReasoningã€‚å¯¹äºä¸å¯éªŒè¯çš„æŒ‡ä»¤è·Ÿéšä»»åŠ¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨AlpacaEval 2.0å’ŒArena-Hardä¸Šè¶…è¶Šäº†äººç±»æˆ–æ ‡å‡†è‡ªæˆ‘æŒ‡å¯¼æç¤ºçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23751v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>CoT-Self-Instructæ˜¯ä¸€ç§åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œé€šè¿‡æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ ¹æ®ç»™å®šçš„ç§å­ä»»åŠ¡è¿›è¡ŒChain-of-Thoughtï¼ˆCoTï¼‰æ¨ç†å’Œè§„åˆ’ï¼Œç”Ÿæˆé«˜è´¨é‡å’Œå¤æ‚åº¦çš„åˆæˆæç¤ºç”¨äºLLMè®­ç»ƒï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åº¦é‡å¯¹é«˜è´¨é‡æ•°æ®è¿›è¡Œç­›é€‰ã€‚åœ¨å¯éªŒè¯æ¨ç†ä»»åŠ¡ä¸­ï¼Œå…¶åˆæˆæ•°æ®æ˜¾è‘—ä¼˜äºç°æœ‰è®­ç»ƒæ•°æ®é›†å¦‚s1kå’ŒOpenMathReasoningï¼Œåœ¨MATH500ã€AMC23ã€AIME24å’ŒGPQA-Diamondç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°çªå‡ºã€‚åœ¨éå¯éªŒè¯æŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­ï¼Œè¯¥æ–¹æ³•è¶…è¶Šäººç±»æˆ–æ ‡å‡†è‡ªæˆ‘æŒ‡å¯¼æç¤ºåœ¨AlpacaEval 2.0å’ŒArena-Hardä¸Šçš„è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CoT-Self-Instructæ˜¯ä¸€ç§åˆæˆæ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œç”¨äºæŒ‡å¯¼LLMsè¿›è¡ŒåŸºäºCoTçš„æ¨ç†å’Œè§„åˆ’ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿæ ¹æ®ç»™å®šçš„ç§å­ä»»åŠ¡ç”Ÿæˆé«˜è´¨é‡å’Œå¤æ‚åº¦çš„åˆæˆæç¤ºã€‚</li>
<li>é€šè¿‡è‡ªåŠ¨åº¦é‡ï¼Œå¯ä»¥ç­›é€‰é«˜è´¨é‡æ•°æ®ç”¨äºLLMè®­ç»ƒã€‚</li>
<li>åœ¨å¯éªŒè¯æ¨ç†ä»»åŠ¡ä¸­ï¼ŒCoT-Self-Instructçš„åˆæˆæ•°æ®è¡¨ç°ä¼˜äºç°æœ‰è®­ç»ƒæ•°æ®é›†ã€‚</li>
<li>CoT-Self-Instructåœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°çªå‡ºï¼ŒåŒ…æ‹¬MATH500ã€AMC23ã€AIME24å’ŒGPQA-Diamondã€‚</li>
<li>åœ¨éå¯éªŒè¯æŒ‡ä»¤è·Ÿéšä»»åŠ¡ä¸­ï¼ŒCoT-Self-Instructè¶…è¶Šäººç±»æˆ–æ ‡å‡†è‡ªæˆ‘æŒ‡å¯¼æç¤ºçš„è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23751">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1afed3dae4971595647e96ee7194acd6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7cd28c02fa68d3baeab8cae830826c7.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="RAGNet-Large-scale-Reasoning-based-Affordance-Segmentation-Benchmark-towards-General-Grasping"><a href="#RAGNet-Large-scale-Reasoning-based-Affordance-Segmentation-Benchmark-towards-General-Grasping" class="headerlink" title="RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark   towards General Grasping"></a>RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark   towards General Grasping</h2><p><strong>Authors:Dongming Wu, Yanping Fu, Saike Huang, Yingfei Liu, Fan Jia, Nian Liu, Feng Dai, Tiancai Wang, Rao Muhammad Anwer, Fahad Shahbaz Khan, Jianbing Shen</strong></p>
<p>General robotic grasping systems require accurate object affordance perception in diverse open-world scenarios following human instructions. However, current studies suffer from the problem of lacking reasoning-based large-scale affordance prediction data, leading to considerable concern about open-world effectiveness. To address this limitation, we build a large-scale grasping-oriented affordance segmentation benchmark with human-like instructions, named RAGNet. It contains 273k images, 180 categories, and 26k reasoning instructions. The images cover diverse embodied data domains, such as wild, robot, ego-centric, and even simulation data. They are carefully annotated with an affordance map, while the difficulty of language instructions is largely increased by removing their category name and only providing functional descriptions. Furthermore, we propose a comprehensive affordance-based grasping framework, named AffordanceNet, which consists of a VLM pre-trained on our massive affordance data and a grasping network that conditions an affordance map to grasp the target. Extensive experiments on affordance segmentation benchmarks and real-robot manipulation tasks show that our model has a powerful open-world generalization ability. Our data and code is available at <a target="_blank" rel="noopener" href="https://github.com/wudongming97/AffordanceNet">https://github.com/wudongming97/AffordanceNet</a>. </p>
<blockquote>
<p>é€šç”¨æœºå™¨äººæŠ“å–ç³»ç»Ÿéœ€è¦åœ¨å¤šæ ·åŒ–å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­è·Ÿéšäººç±»æŒ‡ä»¤è¿›è¡Œå‡†ç¡®çš„å¯¹è±¡åŠŸèƒ½æ„ŸçŸ¥ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç ”ç©¶å­˜åœ¨ç€ç¼ºä¹åŸºäºæ¨ç†çš„å¤§è§„æ¨¡åŠŸèƒ½é¢„æµ‹æ•°æ®çš„é—®é¢˜ï¼Œå¼•å‘äº†äººä»¬å¯¹å¼€æ”¾ä¸–ç•Œæœ‰æ•ˆæ€§çš„æ‹…å¿§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé¢å‘æŠ“å–çš„å¤§å‹åŠŸèƒ½åˆ†å‰²åŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…å«ç±»ä¼¼äººç±»çš„æŒ‡ä»¤ï¼Œåä¸ºRAGNetã€‚å®ƒåŒ…å«273kå¼ å›¾åƒã€180ä¸ªç±»åˆ«å’Œ26kæ¡æ¨ç†æŒ‡ä»¤ã€‚å›¾åƒæ¶µç›–äº†å¤šæ ·åŒ–çš„èº«ä½“æ•°æ®åŸŸï¼Œå¦‚é‡å¤–ã€æœºå™¨äººã€ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒç”šè‡³æ˜¯æ¨¡æ‹Ÿæ•°æ®ã€‚å®ƒä»¬è¢«ç²¾å¿ƒåœ°ç”¨ä¸€ä¸ªåŠŸèƒ½å›¾è¿›è¡Œæ ‡æ³¨ï¼Œè€Œè¯­è¨€æŒ‡ä»¤çš„éš¾åº¦é€šè¿‡ç§»é™¤å…¶ç±»åˆ«åç§°å¹¶æä¾›ä»…æœ‰åŠŸèƒ½æè¿°çš„æ–¹å¼è€Œå¤§å¤§å¢åŠ ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºäºåŠŸèƒ½æ„ŸçŸ¥çš„æŠ“å–æ¡†æ¶ï¼Œåä¸ºAffordanceNetï¼Œå®ƒç”±é¢„è®­ç»ƒåœ¨æˆ‘ä»¬çš„å¤§è§„æ¨¡åŠŸèƒ½æ•°æ®ä¸Šçš„VLMå’ŒæŠ“å–ç½‘ç»œç»„æˆï¼Œè¯¥ç½‘ç»œé€šè¿‡æ¡ä»¶åŒ–åŠŸèƒ½å›¾æ¥æŠ“å–ç›®æ ‡ç‰©ä½“ã€‚åœ¨åŠŸèƒ½åˆ†å‰²åŸºå‡†æµ‹è¯•å’ŒçœŸå®æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰å¼ºå¤§çš„å¼€æ”¾ä¸–ç•Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wudongming97/AffordanceNet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wudongming97/AffordanceNetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23734v1">PDF</a> Accepted by ICCV 2025. The code is at   <a target="_blank" rel="noopener" href="https://github.com/wudongming97/AffordanceNet">https://github.com/wudongming97/AffordanceNet</a></p>
<p><strong>Summary</strong>ï¼š</p>
<p>é’ˆå¯¹é€šç”¨æœºå™¨äººæŠ“å–ç³»ç»Ÿï¼Œåœ¨å¤šæ ·å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸‹è·Ÿéšäººç±»æŒ‡ä»¤è¿›è¡Œç‰©ä½“åŠŸèƒ½æ„ŸçŸ¥çš„é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå¤§å‹æŠ“å–å¯¼å‘çš„åŠŸèƒ½åˆ†å‰²åŸºå‡†æµ‹è¯•ï¼Œåä¸ºRAGNetã€‚å®ƒåŒ…æ‹¬273kå›¾åƒã€180ç±»ã€26kæ¡æŒ‡ä»¤ã€‚å›¾åƒè¦†ç›–å¤šæ ·åŒ–çš„å®ä½“æ•°æ®åŸŸï¼Œå¦‚é‡å¤–ã€æœºå™¨äººã€è‡ªæˆ‘ä¸­å¿ƒç”šè‡³æ¨¡æ‹Ÿæ•°æ®ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºäºåŠŸèƒ½çš„æŠ“å–æ¡†æ¶AffordanceNetï¼ŒåŒ…æ‹¬åœ¨å¤§å‹åŠŸèƒ½æ•°æ®ä¸Šé¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’ŒåŸºäºåŠŸèƒ½åœ°å›¾è¿›è¡Œç›®æ ‡æŠ“å–çš„æŠ“å–ç½‘ç»œã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰å¼ºå¤§çš„å¼€æ”¾ä¸–ç•Œæ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>é€šç”¨æœºå™¨äººæŠ“å–ç³»ç»Ÿéœ€è¦å‡†ç¡®çš„å¯¹è±¡åŠŸèƒ½æ„ŸçŸ¥åœ¨å¤šæ ·å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­è·Ÿéšäººç±»æŒ‡ä»¤ã€‚</li>
<li>å½“å‰ç ”ç©¶ç¼ºä¹åŸºäºæ¨ç†çš„å¤§è§„æ¨¡åŠŸèƒ½é¢„æµ‹æ•°æ®ï¼Œå¯¹å¼€æ”¾ä¸–ç•Œæœ‰æ•ˆæ€§å­˜åœ¨æ‹…å¿§ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§å‹æŠ“å–å¯¼å‘çš„åŠŸèƒ½åˆ†å‰²åŸºå‡†æµ‹è¯•RAGNetï¼ŒåŒ…å«273kå›¾åƒã€180ç±»ã€26kæ¡æŒ‡ä»¤ã€‚</li>
<li>å›¾åƒè¦†ç›–å¤šç§å®ä½“æ•°æ®åŸŸï¼ŒåŒ…æ‹¬é‡å¤–ã€æœºå™¨äººã€è‡ªæˆ‘ä¸­å¿ƒåŠæ¨¡æ‹Ÿæ•°æ®ï¼Œå¹¶è¿›è¡Œäº†ç²¾ç»†çš„åŠŸèƒ½åœ°å›¾æ ‡æ³¨ã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„åŸºäºåŠŸèƒ½çš„æŠ“å–æ¡†æ¶AffordanceNetï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹å’ŒåŸºäºåŠŸèƒ½åœ°å›¾çš„æŠ“å–ç½‘ç»œã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒAffordanceNetæ¨¡å‹å…·æœ‰å¼ºå¤§çš„å¼€æ”¾ä¸–ç•Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23734">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-440cc2f5ac6bcbdffec64d5ec70728ea.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6eff9d831e15f4e8cffad67bef5ee48a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27da66aa6868e0583ff54ec1fe32a750.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4cd61fb10030db1001c05898e30f80de.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca940cafad7cee77cc106d6c0d71d90a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8675c60bed9a2d0563a3175494063dd6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7db301bf2465fe15e1ad84d579f59e2c.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving"><a href="#Seed-Prover-Deep-and-Broad-Reasoning-for-Automated-Theorem-Proving" class="headerlink" title="Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving"></a>Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving</h2><p><strong>Authors:Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, Cheng Ren, Jiawei Shen, Wenlei Shi, Tong Sun, He Sun, Jiahui Wang, Siran Wang, Zhihong Wang, Chenrui Wei, Shufa Wei, Yonghui Wu, Yuchen Wu, Yihang Xia, Huajian Xin, Fan Yang, Huaiyuan Ying, Hongyi Yuan, Zheng Yuan, Tianyang Zhan, Chi Zhang, Yue Zhang, Ge Zhang, Tianyun Zhao, Jianqiu Zhao, Yichi Zhou, Thomas Hanwen Zhu</strong></p>
<p>LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose \textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves $78.1%$ of formalized past IMO problems, saturates MiniF2F, and achieves over 50% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine \textbf{Seed-Geometry}, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»å±•ç°å‡ºå¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡åˆ©ç”¨å¢å¼ºå­¦ä¹ ä¸é•¿é“¾æ€ç»´ç›¸ç»“åˆçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œå½“ä»…ä½¿ç”¨è‡ªç„¶è¯­è¨€æ—¶ï¼Œç”±äºç¼ºä¹æ˜ç¡®çš„ç›‘ç£ä¿¡å·ï¼Œå®ƒä»¬åœ¨å®šç†è¯æ˜æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚ä¸“ç”¨é¢†åŸŸè¯­è¨€ï¼ˆå¦‚Leanï¼‰é€šè¿‡è¯æ˜çš„å½¢å¼éªŒè¯æä¾›æ˜ç¡®çš„ç›‘ç£ï¼Œä»è€Œèƒ½å¤Ÿé€šè¿‡å¢å¼ºå­¦ä¹ è¿›è¡Œæœ‰æ•ˆçš„è®­ç»ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åä¸ºâ€œSeed-Proverâ€çš„å¼•ç†å¼å…¨è¯æ˜æ¨ç†æ¨¡å‹ã€‚Seed-Proverå¯ä»¥åŸºäºLeanåé¦ˆã€å·²è¯æ˜çš„å¼•ç†å’Œè‡ªæˆ‘æ€»ç»“æ¥è¿­ä»£å®Œå–„å…¶è¯æ˜ã€‚ä¸ºè§£å†³IMOçº§åˆ«çš„ç«èµ›é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸‰ç§æµ‹è¯•æ—¶æ¨ç†ç­–ç•¥ï¼Œä»¥å®ç°æ·±åº¦å’Œå¹¿åº¦æ¨ç†ã€‚Seed-Proverè¯æ˜äº†78.1%çš„æ­£å¼åŒ–è¿‡å»IMOé—®é¢˜ï¼Œé¥±å’ŒMiniF2Fï¼Œå¹¶åœ¨PutnamBenchä¸Šè¾¾åˆ°50%ä»¥ä¸Šï¼Œå¤§å¤§è¶…è¿‡äº†ä¹‹å‰çš„æœ€å…ˆè¿›æ°´å¹³ã€‚ä¸ºäº†è§£å†³Leanä¸­ç¼ºä¹å‡ ä½•æ”¯æŒçš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå‡ ä½•æ¨ç†å¼•æ“â€œSeed-Geometryâ€ï¼Œå…¶æ€§èƒ½ä¼˜äºä»¥å‰çš„æ­£å¼å‡ ä½•å¼•æ“ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ä¸¤ä¸ªç³»ç»Ÿå‚åŠ äº†2025å¹´IMOï¼Œå¹¶å®Œå…¨è¯æ˜äº†6ä¸ªé—®é¢˜çš„5ä¸ªã€‚è¿™é¡¹å·¥ä½œåœ¨è‡ªåŠ¨åŒ–æ•°å­¦æ¨ç†æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œå±•ç¤ºäº†é•¿é“¾æ€ç»´æ¨ç†ä¸å½¢å¼éªŒè¯çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23726v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºSeed-Proverçš„æ¨ç†æ¨¡å‹ï¼Œèƒ½å¤ŸåŸºäºç²¾ç›Šåé¦ˆã€å·²éªŒè¯çš„å¼•ç†å’Œè‡ªæˆ‘æ€»ç»“è¿›è¡Œè¿­ä»£å¼çš„è¯æ˜ç»†åŒ–ã€‚è¯¥æ¨¡å‹è§£å†³äº†IMOçº§åˆ«çš„ç«èµ›é—®é¢˜ï¼Œå¹¶è®¾è®¡å‡ºäº†ä¸‰ç§æµ‹è¯•æ—¶æ¨ç†ç­–ç•¥ä»¥å®ç°æ·±åº¦ä¸å¹¿åº¦æ¨ç†ã€‚æ­¤å¤–ï¼Œä¸ºè§£å†³ç²¾ç›Šåœ¨å‡ ä½•æ”¯æŒæ–¹é¢çš„ä¸è¶³ï¼Œå¼•å…¥äº†Seed-Geometryå‡ ä½•æ¨ç†å¼•æ“ã€‚è¿™ä¸¤ä¸ªç³»ç»Ÿå…±åŒå‚ä¸äº†IMO 2025çš„ç«èµ›ï¼ŒæˆåŠŸè¯æ˜å…¶ä¸­äº”ä¸ªé—®é¢˜ï¼Œå±•ç°äº†å½¢å¼éªŒè¯ä¸é•¿é“¾æ€ç»´æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚è¿™æ˜¯æ•°å­¦è‡ªåŠ¨åŒ–æ¨ç†é¢†åŸŸçš„ä¸€å¤§è¿›æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsè™½èƒ½é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸é•¿é“¾æ€ç»´å±•ç°å‡ºå¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä½†åœ¨å®šç†è¯æ˜æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ä¸“ç”¨é¢†åŸŸè¯­è¨€å¦‚Leanï¼Œé€šè¿‡å½¢å¼éªŒè¯è¯æ˜ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ æä¾›äº†æ¸…æ™°çš„ç›‘ç£ä¿¡å·ã€‚</li>
<li>Seed-Proveræ¨¡å‹æ˜¯ä¸€ç§å…¨æ–°çš„æ•°å­¦æ¨ç†æ¨¡å‹ï¼Œå…·å¤‡åŸºäºç²¾ç›Šåé¦ˆã€å·²éªŒè¯çš„å¼•ç†ä»¥åŠè‡ªæˆ‘æ€»ç»“è¿›è¡Œè¿­ä»£è¯æ˜çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹è®¾è®¡äº†ä¸‰ç§æµ‹è¯•æ—¶æ¨ç†ç­–ç•¥ä»¥å¤„ç†æ·±åº¦ä¸å¹¿åº¦æ¨ç†ï¼Œå¹¶è§£å†³äº†IMOçº§åˆ«çš„ç«èµ›é—®é¢˜ã€‚</li>
<li>ä¸ºåº”å¯¹ç²¾ç›Šåœ¨å‡ ä½•æ”¯æŒæ–¹é¢çš„ä¸è¶³ï¼Œå¼•å…¥äº†Seed-Geometryå‡ ä½•æ¨ç†å¼•æ“å¹¶è¶…è¶Šäº†å…ˆå‰çš„æ­£å¼å‡ ä½•å¼•æ“æ€§èƒ½ã€‚</li>
<li>Seed-Proveræ¨¡å‹åœ¨æ­£å¼åŒ–è¿‡å»çš„IMOé—®é¢˜ã€MiniF2Få’ŒPutnamBenchä¸Šçš„è¡¨ç°å‡è¶…è¶Šäº†å…ˆå‰æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23726">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a21270483697c608e4b005b8bc9014e1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9e8463acf5157b2cd2ef3d412d356fc8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-298533a0aaf0b998c716de78d8c29509.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0199f1b029fa0eb1dc8e8aff3dcddea.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Can-LLM-Reasoning-Models-Replace-Classical-Planning-A-Benchmark-Study"><a href="#Can-LLM-Reasoning-Models-Replace-Classical-Planning-A-Benchmark-Study" class="headerlink" title="Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study"></a>Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study</h2><p><strong>Authors:Kai Goebel, Patrik Zips</strong></p>
<p>Recent advancements in Large Language Models have sparked interest in their potential for robotic task planning. While these models demonstrate strong generative capabilities, their effectiveness in producing structured and executable plans remains uncertain. This paper presents a systematic evaluation of a broad spectrum of current state of the art language models, each directly prompted using Planning Domain Definition Language domain and problem files, and compares their planning performance with the Fast Downward planner across a variety of benchmarks. In addition to measuring success rates, we assess how faithfully the generated plans translate into sequences of actions that can actually be executed, identifying both strengths and limitations of using these models in this setting. Our findings show that while the models perform well on simpler planning tasks, they continue to struggle with more complex scenarios that require precise resource management, consistent state tracking, and strict constraint compliance. These results underscore fundamental challenges in applying language models to robotic planning in real world environments. By outlining the gaps that emerge during execution, we aim to guide future research toward combined approaches that integrate language models with classical planners in order to enhance the reliability and scalability of planning in autonomous robotics. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥å¼•å‘äº†äººä»¬å¯¹æœºå™¨äººä»»åŠ¡è§„åˆ’æ½œåŠ›çš„å…´è¶£ã€‚è™½ç„¶è¿™äº›æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨åˆ¶å®šç»“æ„åŒ–ä¸”å¯æ‰§è¡Œçš„è®¡åˆ’æ–¹é¢çš„æœ‰æ•ˆæ€§ä»æœ‰å¾…éªŒè¯ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸€ç³»åˆ—æœ€æ–°çš„è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨è§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€å’Œé—®é¢˜æ–‡ä»¶ç›´æ¥æç¤ºè¿™äº›æ¨¡å‹ï¼Œå¹¶åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å°†å®ƒä»¬çš„è§„åˆ’æ€§èƒ½ä¸Fast Downwardè§„åˆ’å™¨è¿›è¡Œæ¯”è¾ƒã€‚é™¤äº†è¡¡é‡æˆåŠŸç‡ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°ç”Ÿæˆçš„è®¡åˆ’å¦‚ä½•å¿ å®è½¬åŒ–ä¸ºå¯å®é™…æ‰§è¡Œçš„è¡ŒåŠ¨åºåˆ—ï¼Œä»¥ç¡®å®šåœ¨è¿™äº›ç¯å¢ƒä¸­ä½¿ç”¨è¿™äº›æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶è¿™äº›æ¨¡å‹åœ¨ç®€å•çš„è§„åˆ’ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦ç²¾ç¡®èµ„æºç®¡ç†ã€æŒç»­çŠ¶æ€è·Ÿè¸ªå’Œä¸¥æ ¼çº¦æŸéµå®ˆçš„æ›´å¤æ‚åœºæ™¯ä¸­ï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†å°†è¯­è¨€æ¨¡å‹åº”ç”¨äºç°å®ç¯å¢ƒæœºå™¨äººè§„åˆ’çš„åŸºæœ¬æŒ‘æˆ˜ã€‚é€šè¿‡æ¦‚è¿°æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°çš„å·®è·ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›æŒ‡å¯¼ï¼Œé‡‡ç”¨ç»“åˆè¯­è¨€æ¨¡å‹å’Œç»å…¸è§„åˆ’å™¨çš„æ–¹æ³•ï¼Œä»¥æé«˜è‡ªä¸»æœºå™¨äººè§„åˆ’çš„å¯é æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23589v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººä»»åŠ¡è§„åˆ’æ–¹é¢çš„æ½œåŠ›å¼•å‘äº†å¹¿æ³›å…³æ³¨ã€‚æœ¬æ–‡å¯¹ä¸€ç³»åˆ—å½“å‰å…ˆè¿›è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œä½¿ç”¨è§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€åŸŸå’Œé—®é¢˜æ–‡ä»¶ç›´æ¥æç¤ºï¼Œä¸Fast Downwardè§„åˆ’å™¨åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œè§„åˆ’æ€§èƒ½æ¯”è¾ƒã€‚é™¤äº†æµ‹é‡æˆåŠŸç‡ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†ç”Ÿæˆè®¡åˆ’èƒ½å¦å¿ å®è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„è¡ŒåŠ¨åºåˆ—ï¼Œæ¢è®¨äº†åœ¨è¿™ç§ç¯å¢ƒä¸­ä½¿ç”¨è¿™äº›æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ã€‚ç ”ç©¶å‘ç°ï¼Œè¿™äº›æ¨¡å‹åœ¨ç®€å•çš„è§„åˆ’ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦ç²¾ç¡®èµ„æºç®¡ç†ã€æŒç»­çŠ¶æ€è·Ÿè¸ªå’Œä¸¥æ ¼çº¦æŸéµå®ˆçš„å¤æ‚åœºæ™¯ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†å°†è¯­è¨€æ¨¡å‹åº”ç”¨äºç°å®ç¯å¢ƒæœºå™¨äººè§„åˆ’çš„åŸºç¡€æŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºæœªæ¥ç ”ç©¶åº”æ•´åˆè¯­è¨€æ¨¡å‹ä¸ç»å…¸è§„åˆ’å™¨ï¼Œä»¥æé«˜è‡ªä¸»æœºå™¨äººè§„åˆ’çš„å¯é æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººä»»åŠ¡è§„åˆ’é¢†åŸŸå…·æœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ã€‚</li>
<li>æœ¬æ–‡ç³»ç»Ÿè¯„ä¼°äº†å½“å‰å…ˆè¿›è¯­è¨€æ¨¡å‹åœ¨è§„åˆ’æ€§èƒ½æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>è¯­è¨€æ¨¡å‹åœ¨ç®€å•è§„åˆ’ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚åœºæ™¯ä¸­å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å¤æ‚åœºæ™¯ä¸­çš„æŒ‘æˆ˜åŒ…æ‹¬ç²¾ç¡®èµ„æºç®¡ç†ã€æŒç»­çŠ¶æ€è·Ÿè¸ªå’Œä¸¥æ ¼çº¦æŸéµå®ˆã€‚</li>
<li>è¯­è¨€æ¨¡å‹ä¸ç»å…¸è§„åˆ’å™¨çš„ç»“åˆå¯ä»¥æé«˜æœºå™¨äººè§„åˆ’çš„å¯é æ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>ç ”ç©¶å­˜åœ¨å·®è·ï¼Œéœ€è¦è¿›ä¸€æ­¥æ¢ç´¢å’Œå‘å±•é›†æˆè¯­è¨€æ¨¡å‹å’Œç»å…¸è§„åˆ’å™¨çš„è”åˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-47ae5668cd28c1af167316672c181c2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c217ad9c3e0bbff20ff73b4ccb61665.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8bbb70b730babf0f713a554e61b67361.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-65d481cfff8f047ea059e79ec635aeeb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-064ab5da026c0e44c15c782fcbdfeaf1.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="GraphRAG-R1-Graph-Retrieval-Augmented-Generation-with-Process-Constrained-Reinforcement-Learning"><a href="#GraphRAG-R1-Graph-Retrieval-Augmented-Generation-with-Process-Constrained-Reinforcement-Learning" class="headerlink" title="GraphRAG-R1: Graph Retrieval-Augmented Generation with   Process-Constrained Reinforcement Learning"></a>GraphRAG-R1: Graph Retrieval-Augmented Generation with   Process-Constrained Reinforcement Learning</h2><p><strong>Authors:Chuanyue Yu, Kuo Zhao, Yuhan Li, Heng Chang, Mingjian Feng, Xiangzhe Jiang, Yufei Sun, Jia Li, Yuzhi Zhang, Jianxin Li, Ziwei Zhang</strong></p>
<p>Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness in enhancing the reasoning abilities of LLMs by leveraging graph structures for knowledge representation and modeling complex real-world relationships. However, existing GraphRAG methods still face significant bottlenecks when handling complex problems that require multi-hop reasoning, as their query and retrieval phases are largely based on pre-defined heuristics and do not fully utilize the reasoning potentials of LLMs. To address this problem, we propose GraphRAG-R1, an adaptive GraphRAG framework by training LLMs with process-constrained outcome-based reinforcement learning (RL) to enhance the multi-hop reasoning ability. Our method can decompose complex problems, autonomously invoke retrieval tools to acquire necessary information, and perform effective reasoning. Specifically, we utilize a modified version of Group Relative Policy Optimization (GRPO) that supports rollout-with-thinking capability. Next, we design two process-constrained reward functions. To handle the shallow retrieval problem, we design a Progressive Retrieval Attenuation (PRA) reward to encourage essential retrievals. Then, to handle the over-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the model performance with computational costs. We further design a phase-dependent training strategy, containing three training stages corresponding to cold start and these two rewards. Lastly, our method adopts a hybrid graph-textual retrieval to improve the reasoning capacity. Extensive experimental results demonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex reasoning problems compared to state-of-the-art GraphRAG methods on both in-domain and out-of-domain datasets. Furthermore, our framework can be flexibly integrated with various existing retrieval methods, consistently delivering performance improvements. </p>
<blockquote>
<p>å›¾æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆGraphRAGï¼‰é€šè¿‡åˆ©ç”¨å›¾ç»“æ„è¿›è¡ŒçŸ¥è¯†è¡¨ç¤ºå’Œå»ºæ¨¡å¤æ‚çš„ç°å®ä¸–ç•Œå…³ç³»ï¼Œåœ¨å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºå·¨å¤§çš„æœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„GraphRAGæ–¹æ³•åœ¨å¤„ç†éœ€è¦å¤šè·³æ¨ç†çš„å¤æ‚é—®é¢˜æ—¶ä»é¢ä¸´é‡å¤§ç“¶é¢ˆï¼Œå› ä¸ºå®ƒä»¬çš„æŸ¥è¯¢å’Œæ£€ç´¢é˜¶æ®µå¤§å¤šåŸºäºé¢„å®šä¹‰çš„å¯å‘å¼æ–¹æ³•ï¼Œå¹¶æ²¡æœ‰å……åˆ†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†æ½œåŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GraphRAG-R1ï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”çš„GraphRAGæ¡†æ¶ï¼Œé€šè¿‡é‡‡ç”¨è¿‡ç¨‹çº¦æŸçš„ç»“æœå¯¼å‘å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥å¢å¼ºå…¶å¤šè·³æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åˆ†è§£å¤æ‚é—®é¢˜ï¼Œè‡ªä¸»è°ƒç”¨æ£€ç´¢å·¥å…·è·å–å¿…è¦ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨ç»è¿‡ä¿®æ”¹çš„Group Relative Policy Optimizationï¼ˆGRPOï¼‰ç‰ˆæœ¬ï¼Œæ”¯æŒæ€è€ƒèƒ½åŠ›çš„æ»šåŠ¨ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸¤ä¸ªè¿‡ç¨‹çº¦æŸå¥–åŠ±å‡½æ•°ã€‚ä¸ºäº†è§£å†³æµ…å±‚æ£€ç´¢é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†Progressive Retrieval Attenuationï¼ˆPRAï¼‰å¥–åŠ±æ¥é¼“åŠ±å¿…è¦çš„æ£€ç´¢ã€‚ç„¶åï¼Œä¸ºäº†è§£å†³è¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†Cost-Aware F1ï¼ˆCAFï¼‰å¥–åŠ±æ¥å¹³è¡¡æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§é˜¶æ®µæ€§çš„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬ä¸è¿™ä¸¤ç§å¥–åŠ±ç›¸å¯¹åº”çš„å†·å¯åŠ¨åœ¨å†…çš„ä¸‰ä¸ªé˜¶æ®µã€‚æœ€åï¼Œæˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨æ··åˆå›¾æ–‡æœ¬æ£€ç´¢æ¥æé«˜æ¨ç†èƒ½åŠ›ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°çš„GraphRAGæ–¹æ³•ç›¸æ¯”ï¼ŒGraphRAG-R1åœ¨åŸŸå†…å’ŒåŸŸå¤–æ•°æ®é›†ä¸Šéƒ½èƒ½æé«˜å¤§å‹è¯­è¨€æ¨¡å‹è§£å†³å¤æ‚æ¨ç†é—®é¢˜çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥çµæ´»åœ°ä¸å„ç§ç°æœ‰çš„æ£€ç´¢æ–¹æ³•ç›¸ç»“åˆï¼ŒæŒç»­å¸¦æ¥æ€§èƒ½æ”¹è¿›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23581v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>GraphRAG-R1é€šè¿‡é‡‡ç”¨è¿‡ç¨‹çº¦æŸçš„åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒLLMsï¼Œæé«˜äº†å¤šè·³æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåˆ†è§£å¤æ‚é—®é¢˜ï¼Œè‡ªä¸»è°ƒç”¨æ£€ç´¢å·¥å…·è·å–å¿…è¦ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†ã€‚é€šè¿‡è®¾è®¡ä¸¤ç§è¿‡ç¨‹çº¦æŸå¥–åŠ±å‡½æ•°æ¥è§£å†³æµ…æ£€ç´¢å’Œè¿‡åº¦æ€è€ƒçš„é—®é¢˜ï¼Œå¹¶é‡‡ç”¨æ··åˆå›¾æ–‡æœ¬æ£€ç´¢æé«˜æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰çš„GraphRAGæ–¹æ³•ç›¸æ¯”ï¼ŒGraphRAG-R1åœ¨è§£å†³å¤æ‚æ¨ç†é—®é¢˜æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>GraphRAG-R1åˆ©ç”¨è¿‡ç¨‹çº¦æŸçš„åŸºäºç»“æœçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒLLMsï¼Œæé«˜å¤šè·³æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡è‡ªä¸»è°ƒç”¨æ£€ç´¢å·¥å…·åˆ†è§£å¤æ‚é—®é¢˜å¹¶è·å–å¿…è¦ä¿¡æ¯ã€‚</li>
<li>è®¾è®¡ä¸¤ç§è¿‡ç¨‹çº¦æŸå¥–åŠ±å‡½æ•°è§£å†³æµ…æ£€ç´¢å’Œè¿‡åº¦æ€è€ƒçš„é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨æ··åˆå›¾æ–‡æœ¬æ£€ç´¢æé«˜æ¨ç†èƒ½åŠ›ã€‚</li>
<li>GraphRAG-R1åœ¨è§£å†³å¤æ‚æ¨ç†é—®é¢˜æ–¹é¢ä¼˜äºç°æœ‰çš„GraphRAGæ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23581">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-be596d67543c6421e9adff683f8bcdea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-32ff37b31512c462af06a97d28d77716.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-278bc38d65b59918f7094a5f9ae6b573.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Med-R-3-Enhancing-Medical-Retrieval-Augmented-Reasoning-of-LLMs-via-Progressive-Reinforcement-Learning"><a href="#Med-R-3-Enhancing-Medical-Retrieval-Augmented-Reasoning-of-LLMs-via-Progressive-Reinforcement-Learning" class="headerlink" title="Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via   Progressive Reinforcement Learning"></a>Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via   Progressive Reinforcement Learning</h2><p><strong>Authors:Keer Lu, Zheng Liang, Youquan Li, Jiejun Tan, Da Pan, Shusen Zhang, Guosheng Dong, Huang Leng</strong></p>
<p>In medical scenarios, effectively retrieving external knowledge and leveraging it for rigorous logical reasoning is of significant importance. Despite their potential, existing work has predominantly focused on enhancing either retrieval or reasoning capabilities of the models in isolation, with little attention given to their joint optimization, which leads to limited coordination between the two processes. Additionally, current methods rely heavily on supervised fine-tuning (SFT), which can cause models to memorize existing problem-solving pathways, thereby restricting their generalization ability when confronted with novel problem contexts. Furthermore, while some studies have explored to improve retrieval-augmented reasoning in general domains via reinforcement learning, their reward function designs do not adequately capture the specific demands of the medical domain. To address these challenges, we introduce <strong>Med-R$^3$</strong>, a <strong>Med</strong>ical <strong>R</strong>etrieval-augmented <strong>R</strong>easoning framework driven by progressive <strong>R</strong>einforcement learning. In this framework, we first develop the modelâ€™s ability to perform logical reasoning over medical problems. Subsequently, on the basis of this foundation, we adaptively optimize the retrieval capability to better align with the characteristics of knowledge corpus and external information utilization throughout the reasoning process. Finally, we conduct joint optimization of the modelâ€™s retrieval and reasoning coordination. Extensive experiments indicate that <strong>Med-R$^3$</strong> could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by 3.93% at a comparable parameter scale, while Qwen2.5-14B augmented with Med-R$^3$ shows a more substantial gain of 13.53%. </p>
<blockquote>
<p>åœ¨åŒ»ç–—åœºæ™¯ä¸­ï¼Œæœ‰æ•ˆåœ°æ£€ç´¢å¤–éƒ¨çŸ¥è¯†å¹¶åˆ©ç”¨å…¶è¿›è¡Œä¸¥æ ¼çš„é€»è¾‘æ¨ç†å…·æœ‰éå¸¸é‡è¦çš„æ„ä¹‰ã€‚å°½ç®¡å­˜åœ¨æ½œåœ¨çš„æ”¹è¿›ç©ºé—´ï¼Œä½†ç°æœ‰å·¥ä½œä¸»è¦é›†ä¸­åœ¨å•ç‹¬å¢å¼ºæ¨¡å‹çš„æ£€ç´¢æˆ–æ¨ç†èƒ½åŠ›ä¸Šï¼Œè€Œå¯¹å®ƒä»¬çš„è”åˆä¼˜åŒ–å…³æ³¨è¾ƒå°‘ï¼Œå¯¼è‡´è¿™ä¸¤ä¸ªè¿‡ç¨‹ä¹‹é—´çš„åè°ƒæœ‰é™ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ–¹æ³•ä¸¥é‡ä¾èµ–äºæœ‰ç›‘ç£çš„å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å‹è®°å¿†ç°æœ‰çš„é—®é¢˜è§£å†³è·¯å¾„ï¼Œä»è€Œåœ¨é¢å¯¹æ–°çš„é—®é¢˜æƒ…å¢ƒæ—¶é™åˆ¶å…¶æ³›åŒ–èƒ½åŠ›ã€‚è™½ç„¶ä¸€äº›ç ”ç©¶å·²ç»æ¢ç´¢äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ æ”¹è¿›ä¸€èˆ¬é¢†åŸŸçš„æ£€ç´¢å¢å¼ºæ¨ç†ï¼Œä½†ä»–ä»¬çš„å¥–åŠ±å‡½æ•°è®¾è®¡å¹¶æ²¡æœ‰å……åˆ†æ•æ‰åˆ°åŒ»ç–—é¢†åŸŸçš„ç‰¹å®šéœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>Med-R$^3$<strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±æ¸è¿›å¼å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„</strong>åŒ»ç–—æ£€ç´¢å¢å¼ºæ¨ç†</strong>æ¡†æ¶ã€‚åœ¨æ­¤æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘æ¨¡å‹è§£å†³åŒ»ç–—é—®é¢˜çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚ç„¶ååœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è‡ªé€‚åº”åœ°ä¼˜åŒ–æ£€ç´¢èƒ½åŠ›ï¼Œä»¥æ›´å¥½åœ°ä¸çŸ¥è¯†åº“å’Œå¤–éƒ¨ä¿¡æ¯åˆ©ç”¨çš„ç‰¹å¾åœ¨æ•´ä¸ªæ¨ç†è¿‡ç¨‹ä¸­å¯¹é½ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹çš„æ£€ç´¢å’Œæ¨ç†åè°ƒè¿›è¡Œè”åˆä¼˜åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œ**Med-R$^3$**å¯ä»¥è¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å‚æ•°è§„æ¨¡ç›¸å½“çš„æƒ…å†µä¸‹ï¼ŒLLaMA3.1-8B-Instructä¸Med-R$^3$ç»“åˆè¶…è¶Šäº†é—­æºçš„GPT-4o-miniï¼Œæå‡äº†3.93%ï¼›è€ŒQwen2.5-14Bä¸Med-R$^3$ç»“åˆåˆ™å–å¾—äº†æ›´å¤§çš„æ”¶ç›Šï¼Œæå‡äº†13.53%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23541v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡å¼ºè°ƒåœ¨åŒ»ç–—åœºæ™¯ä¸­ï¼Œæœ‰æ•ˆæ£€ç´¢å¤–éƒ¨çŸ¥è¯†å¹¶ç”¨äºä¸¥è°¨çš„é€»è¾‘æ¨ç†å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç°æœ‰å·¥ä½œä¸»è¦å…³æ³¨å¢å¼ºæ¨¡å‹çš„æ£€ç´¢æˆ–æ¨ç†èƒ½åŠ›ï¼Œä½†å¾ˆå°‘å…³æ³¨ä¸¤è€…çš„è”åˆä¼˜åŒ–ï¼Œå¯¼è‡´ä¸¤è€…åè°ƒæœ‰é™ã€‚æ­¤å¤–ï¼Œå½“å‰æ–¹æ³•è¿‡äºä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œé™åˆ¶äº†æ¨¡å‹åœ¨é¢ä¸´æ–°é—®é¢˜æƒ…å¢ƒæ—¶çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºMed-R$^3$æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ¸è¿›å¼å¼ºåŒ–å­¦ä¹ ï¼Œæé«˜åŒ»ç–—é¢†åŸŸçš„æ£€ç´¢å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚å…ˆè¿›è¡Œé€»è¾‘æ¨ç†èƒ½åŠ›è®­ç»ƒï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šä¼˜åŒ–æ£€ç´¢èƒ½åŠ›ï¼Œæœ€åè¿›è¡Œæ¨¡å‹æ£€ç´¢å’Œæ¨ç†çš„è”åˆä¼˜åŒ–ã€‚å®éªŒè¡¨æ˜ï¼ŒMed-R$^3$è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—åœºæ™¯ä¸­ï¼Œå¤–éƒ¨çŸ¥è¯†çš„æœ‰æ•ˆæ£€ç´¢å’Œé€»è¾‘æ¨ç†ç›¸ç»“åˆè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰å·¥ä½œå¤šå…³æ³¨å¢å¼ºæ¨¡å‹çš„æ£€ç´¢æˆ–æ¨ç†èƒ½åŠ›ï¼Œä½†æœªå……åˆ†å…³æ³¨ä¸¤è€…çš„è”åˆä¼˜åŒ–ã€‚</li>
<li>ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Med-R$^3$æ¡†æ¶é€šè¿‡æ¸è¿›å¼å¼ºåŒ–å­¦ä¹ æé«˜åŒ»ç–—é¢†åŸŸçš„æ£€ç´¢å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚</li>
<li>Med-R$^3$å…ˆè¿›è¡Œé€»è¾‘æ¨ç†èƒ½åŠ›è®­ç»ƒï¼Œå†ä¼˜åŒ–æ£€ç´¢èƒ½åŠ›ï¼Œæœ€åè”åˆä¼˜åŒ–æ£€ç´¢å’Œæ¨ç†ã€‚</li>
<li>å®éªŒè¡¨æ˜Med-R$^3$æ¡†æ¶è¾¾åˆ°æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-37ca491a729bafd60710196e36c4790d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4c765c9c2918ef3eefd8a50e3ac67f34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80513fc864b19ef77c55b70982db027a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af52adda2f08eff4c33b4d359415cee1.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Causal-Reasoning-in-Pieces-Modular-In-Context-Learning-for-Causal-Discovery"><a href="#Causal-Reasoning-in-Pieces-Modular-In-Context-Learning-for-Causal-Discovery" class="headerlink" title="Causal Reasoning in Pieces: Modular In-Context Learning for Causal   Discovery"></a>Causal Reasoning in Pieces: Modular In-Context Learning for Causal   Discovery</h2><p><strong>Authors:Kacper Kadziolka, Saber Salehkaleybar</strong></p>
<p>Causal inference remains a fundamental challenge for large language models. Recent advances in internal reasoning with large language models have sparked interest in whether state-of-the-art reasoning models can robustly perform causal discovery-a task where conventional models often suffer from severe overfitting and near-random performance under data perturbations. We study causal discovery on the Corr2Cause benchmark using the emergent OpenAIâ€™s o-series and DeepSeek-R model families and find that these reasoning-first architectures achieve significantly greater native gains than prior approaches. To capitalize on these strengths, we introduce a modular in-context pipeline inspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding nearly three-fold improvements over conventional baselines. We further probe the pipelineâ€™s impact by analyzing reasoning chain length, complexity, and conducting qualitative and quantitative comparisons between conventional and reasoning models. Our findings suggest that while advanced reasoning models represent a substantial leap forward, carefully structured in-context frameworks are essential to maximize their capabilities and offer a generalizable blueprint for causal discovery across diverse domains. </p>
<blockquote>
<p>å› æœæ¨ç†ä»æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´çš„ä¸€é¡¹æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹å†…éƒ¨æ¨ç†çš„è¿›å±•æ¿€å‘äº†äººä»¬å¯¹è¿™æ ·ä¸€ä¸ªé—®é¢˜çš„å…´è¶£ï¼šæœ€å…ˆè¿›çš„æ¨ç†æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿç¨³å¥åœ°è¿›è¡Œå› æœå‘ç°ï¼Ÿè¿™æ˜¯ä¸€é¡¹ä»»åŠ¡ï¼Œå…¶ä¸­ä¼ ç»Ÿæ¨¡å‹ç»å¸¸å—åˆ°ä¸¥é‡çš„è¿‡æ‹Ÿåˆå’Œæ•°æ®æ‰°åŠ¨ä¸‹è¿‘ä¼¼éšæœºè¡¨ç°çš„å½±å“ã€‚æˆ‘ä»¬åœ¨Corr2CauseåŸºå‡†æµ‹è¯•é›†ä¸Šç ”ç©¶äº†OpenAIçš„oç³»åˆ—å’ŒDeepSeek-Ræ¨¡å‹å®¶æ—çš„åº”ç”¨è¡¨ç°ï¼Œå‘ç°è¿™äº›ä»¥æ¨ç†ä¸ºé‡ç‚¹çš„æ¶æ„ä¸ä¹‹å‰çš„æ–¹æ³•ç›¸æ¯”å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨è¿™äº›ä¼˜åŠ¿ï¼Œæˆ‘ä»¬å€Ÿé‰´æ ‘å½¢æ€ç»´å’Œé“¾å¼æ€ç»´æ–¹æ³•ï¼Œå¼•å…¥äº†ä¸€ç§æ¨¡å—åŒ–ä¸Šä¸‹æ–‡ç®¡é“ï¼Œä¸ä¼ ç»ŸåŸºçº¿ç›¸æ¯”ï¼Œå–å¾—äº†è¿‘ä¸‰å€çš„æå‡ã€‚é€šè¿‡è§£ææ¨ç†é“¾çš„é•¿åº¦å’Œå¤æ‚æ€§ä»¥åŠä¼ ç»Ÿæ¨¡å‹å’Œæ¨ç†æ¨¡å‹ä¹‹é—´çš„å¯¹æ¯”ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æŒ–æ˜äº†ç®¡é“çš„å½±å“ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæš—ç¤ºï¼Œè™½ç„¶å…ˆè¿›çš„æ¨ç†æ¨¡å‹å–å¾—äº†é‡å¤§è¿›æ­¥ï¼Œä½†è¦æœ€å¤§é™åº¦åœ°å‘æŒ¥å…¶èƒ½åŠ›ï¼Œç²¾å¿ƒæ„å»ºçš„ä¸Šä¸‹æ–‡æ¡†æ¶è‡³å…³é‡è¦ï¼Œè¿™ä¸ºè·¨ä¸åŒé¢†åŸŸçš„å› æœå‘ç°æä¾›äº†å¯æ¨å¹¿çš„è“å›¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23488v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹çš„å› æœæ¨æ–­ä¾ç„¶æ˜¯ä¸€ä¸ªåŸºæœ¬æŒ‘æˆ˜ã€‚æœ€æ–°å†…éƒ¨æ¨ç†æŠ€æœ¯æ¿€å‘äº†å¯¹å› æœå‘ç°ä»»åŠ¡çš„ç ”ç©¶å…´è¶£ï¼Œè¯¥ä»»åŠ¡ä¸­ä¼ ç»Ÿæ¨¡å‹å¸¸å¸¸é¢ä¸´ä¸¥é‡çš„è¿‡æ‹Ÿåˆé—®é¢˜ä»¥åŠåœ¨æ•°æ®æ‰°åŠ¨ä¸‹çš„è¿‘ä¹éšæœºæ€§èƒ½è¡¨ç°ã€‚åœ¨Corr2CauseåŸºå‡†æµ‹è¯•ä¸Šï¼Œæˆ‘ä»¬ç ”ç©¶äº†OpenAIçš„oç³»åˆ—å’ŒDeepSeek-Ræ¨¡å‹å®¶æ—çš„å› æœå‘ç°èƒ½åŠ›ï¼Œå‘ç°è¿™äº›ä»¥æ¨ç†ä¸ºä¸»çš„æ¶æ„æ¯”å…ˆå‰çš„æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨è¿™äº›ä¼˜åŠ¿ï¼Œæˆ‘ä»¬å¼•å…¥äº†å—æ€ç»´æ ‘å’Œæ€ç»´é“¾å¯å‘çš„æ¨¡å—åŒ–ä¸Šä¸‹æ–‡ç®¡é“ï¼Œå®ç°äº†è¿‘ä¹ä¸‰å€äºä¼ ç»ŸåŸºå‡†æµ‹è¯•çš„æå‡ã€‚é€šè¿‡å¯¹æ¯”ä¼ ç»Ÿæ¨¡å‹ä¸æ¨ç†æ¨¡å‹çš„æ¨ç†é“¾é•¿åº¦ã€å¤æ‚æ€§å’Œå®šé‡å®šæ€§åˆ†æï¼Œæˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œå°½ç®¡å…ˆè¿›æ¨ç†æ¨¡å‹ä»£è¡¨äº†ä¸€æ¬¡é‡å¤§é£è·ƒï¼Œä½†ç²¾å¿ƒæ„å»ºçš„ä¸Šä¸‹æ–‡æ¡†æ¶å¯¹äºæœ€å¤§åŒ–å…¶èƒ½åŠ›è‡³å…³é‡è¦ï¼Œå¹¶ä¸ºè·¨ä¸åŒé¢†åŸŸçš„å› æœå‘ç°æä¾›äº†å¯æ¨å¹¿çš„è“å›¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å› æœæ¨æ–­ä¸Šä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å…ˆè¿›æ¨ç†æ¨¡å‹å¦‚OpenAIçš„oç³»åˆ—å’ŒDeepSeek-Råœ¨å› æœå‘ç°ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>æ¨¡å—åŒ–ä¸Šä¸‹æ–‡ç®¡é“ï¼ˆå—æ€ç»´æ ‘å’Œæ€ç»´é“¾å¯å‘ï¼‰èƒ½æé«˜æ¨ç†æ¨¡å‹çš„æ€§èƒ½ï¼Œå®ç°è¿‘ä¹ä¸‰å€çš„æå‡ã€‚</li>
<li>æ¨ç†æ¨¡å‹çš„æ€§èƒ½æå‡å¾—ç›Šäºå…¶æ¨ç†é“¾çš„é•¿åº¦å’Œå¤æ‚æ€§ã€‚</li>
<li>ç²¾å¿ƒæ„å»ºçš„ä¸Šä¸‹æ–‡æ¡†æ¶å¯¹äºæœ€å¤§åŒ–æ¨ç†æ¨¡å‹çš„èƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>å…ˆè¿›æ¨ç†æ¨¡å‹ä¸ºè·¨ä¸åŒé¢†åŸŸçš„å› æœå‘ç°æä¾›äº†å¯æ¨å¹¿çš„è“å›¾ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23488">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f57c9dc4e9020f581eed86814f590d6a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b77c7f54eafda4b6971d23103a2e25ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4f32dd4e398390165102b66010dcacae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97aae62f2bfbe71fe31a03294ade7544.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="3D-R1-Enhancing-Reasoning-in-3D-VLMs-for-Unified-Scene-Understanding"><a href="#3D-R1-Enhancing-Reasoning-in-3D-VLMs-for-Unified-Scene-Understanding" class="headerlink" title="3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding"></a>3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding</h2><p><strong>Authors:Ting Huang, Zeyu Zhang, Hao Tang</strong></p>
<p>Large vision-language models (VLMs) have made significant strides in 2D visual understanding tasks, sparking interest in extending these capabilities to 3D scene understanding. However, current 3D VLMs often struggle with robust reasoning and generalization due to limitations in high-quality spatial data and the static nature of viewpoint assumptions. To address these challenges, we propose 3D-R1, a foundation model that enhances the reasoning capabilities of 3D VLMs. Specifically, we first construct a high-quality synthetic dataset with CoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine based on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1. Moreover, we leverage RLHF policy such as GRPO in the reinforcement learning training process to enhance reasoning capabilities and introduce three reward functions: a perception reward, a semantic similarity reward and a format reward to maintain detection accuracy and answer semantic precision. Furthermore, we introduce a dynamic view selection strategy that adaptively chooses the most informative perspectives for 3D scene understanding. Extensive experiments demonstrate that 3D-R1 delivers an average improvement of 10% across various 3D scene benchmarks, highlighting its effectiveness in enhancing reasoning and generalization in 3D scene understanding. Code: <a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/3D-R1">https://github.com/AIGeeksGroup/3D-R1</a>. Website: <a target="_blank" rel="noopener" href="https://aigeeksgroup.github.io/3D-R1">https://aigeeksgroup.github.io/3D-R1</a>. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨2Dè§†è§‰ç†è§£ä»»åŠ¡æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä»è€Œæ¿€å‘äº†å°†è¿™äº›èƒ½åŠ›æ‰©å±•åˆ°3Dåœºæ™¯ç†è§£çš„å…´è¶£ã€‚ç„¶è€Œï¼Œç”±äºé«˜è´¨é‡ç©ºé—´æ•°æ®çš„é™åˆ¶å’Œè§†è§’å‡è®¾çš„é™æ€æ€§è´¨ï¼Œå½“å‰çš„3D VLMsåœ¨ç¨³å¥æ¨ç†å’Œæ³›åŒ–æ–¹é¢ç»å¸¸é¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†3D-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼º3D VLMsæ¨ç†èƒ½åŠ›çš„åŸºç¡€æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ©ç”¨ç°æœ‰çš„3D-VLæ•°æ®é›†å’Œæ•°æ®å¼•æ“åŸºäºGemini 2.5 Proï¼Œæ„å»ºäº†ä¸€ä¸ªåä¸ºScene-30Kçš„é«˜è´¨é‡åˆæˆæ•°æ®é›†ï¼Œå®ƒä½œä¸º3D-R1çš„å†·å¯åŠ¨åˆå§‹åŒ–æ•°æ®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨å¦‚GRPOçš„RLHFç­–ç•¥ï¼Œå¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œå¹¶å¼•å…¥ä¸‰ç§å¥–åŠ±åŠŸèƒ½ï¼šæ„ŸçŸ¥å¥–åŠ±ã€è¯­ä¹‰ç›¸ä¼¼æ€§å¥–åŠ±å’Œæ ¼å¼å¥–åŠ±ï¼Œä»¥ç»´æŒæ£€æµ‹ç²¾åº¦å’Œç­”æ¡ˆè¯­ä¹‰ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€è§†è§’é€‰æ‹©ç­–ç•¥ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°é€‰æ‹©æœ€å…·æœ‰ä¿¡æ¯é‡çš„è§†è§’è¿›è¡Œ3Dåœºæ™¯ç†è§£ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œ3D-R1åœ¨å„ç§3Dåœºæ™¯åŸºå‡†æµ‹è¯•ä¸­å¹³å‡æé«˜äº†10%çš„æ€§èƒ½ï¼Œå‡¸æ˜¾äº†å…¶åœ¨å¢å¼º3Dåœºæ™¯ç†è§£çš„æ¨ç†å’Œæ³›åŒ–æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚ä»£ç åœ°å€ï¼š<a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/3D-R1%E3%80%82%E7%BD%91%E7%AB%99%E5%9C%B0%E5%9D%80%EF%BC%9Ahttps://aigeeksgroup.github.io/3D-R1%E3%80%82">https://github.com/AIGeeksGroup/3D-R1ã€‚ç½‘ç«™åœ°å€ï¼šhttps://aigeeksgroup.github.io/3D-R1ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23478v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸º3D-R1çš„å¢å¼ºä¸‰ç»´è§†è§‰è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚ä¸ºåº”å¯¹é«˜è´¨é‡ç©ºé—´æ•°æ®çš„å±€é™å’Œè§†è§’å‡è®¾çš„é™æ€æ€§è´¨æ‰€å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æ„å»ºäº†é«˜è´¨é‡åˆæˆæ•°æ®é›†Scene-30Kï¼Œå¹¶å¼•å…¥äº†å¼ºåŒ–å­¦ä¹ ä¸­çš„ç­–ç•¥ï¼Œå¦‚GRPOï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œé€šè¿‡å¼•å…¥ä¸‰ç§å¥–åŠ±å‡½æ•°ä»¥åŠåŠ¨æ€è§†è§’é€‰æ‹©ç­–ç•¥ï¼Œæé«˜äº†æ¨¡å‹åœ¨ä¸‰ç»´åœºæ™¯ç†è§£ä¸­çš„æ£€æµ‹ç²¾åº¦å’Œè¯­ä¹‰ç²¾åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œ3D-R1åœ¨å¤šä¸ªä¸‰ç»´åœºæ™¯åŸºå‡†æµ‹è¯•ä¸Šå¹³å‡æé«˜äº†10%çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D-R1æ¨¡å‹æ—¨åœ¨å¢å¼ºç°æœ‰ä¸‰ç»´è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„æ¨ç†å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªåä¸ºScene-30Kçš„é«˜è´¨é‡åˆæˆæ•°æ®é›†ä½œä¸º3D-R1çš„å†·å¯åŠ¨åˆå§‹åŒ–æ•°æ®ã€‚</li>
<li>åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ç­–ç•¥ï¼ˆå¦‚GRPOï¼‰å’Œä¸‰ç§å¥–åŠ±å‡½æ•°æ¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>å¥–åŠ±å‡½æ•°åŒ…æ‹¬æ„ŸçŸ¥å¥–åŠ±ã€è¯­ä¹‰ç›¸ä¼¼æ€§å¥–åŠ±å’Œæ ¼å¼å¥–åŠ±ï¼Œä»¥ç¡®ä¿æ£€æµ‹å‡†ç¡®æ€§å’Œè¯­ä¹‰ç²¾åº¦ã€‚</li>
<li>å¼•å…¥äº†åŠ¨æ€è§†è§’é€‰æ‹©ç­–ç•¥ï¼Œä¸ºä¸‰ç»´åœºæ™¯ç†è§£é€‰æ‹©æœ€å…·ä¿¡æ¯æ€§çš„è§†è§’ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œ3D-R1åœ¨å¤šä¸ªä¸‰ç»´åœºæ™¯åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†å¹³å‡10%çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23478">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-951b61c585262d6c0fbe2812a74c2530.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-410030319e4683af120bcdf409419b47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ff737ae94873c4c2b8b781b7f5bd13d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-447d41cfeaa74fa11f5017700bc88fa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-67b5e305a5a17d729c1348730e8ef446.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="FASTopoWM-Fast-Slow-Lane-Segment-Topology-Reasoning-with-Latent-World-Models"><a href="#FASTopoWM-Fast-Slow-Lane-Segment-Topology-Reasoning-with-Latent-World-Models" class="headerlink" title="FASTopoWM: Fast-Slow Lane Segment Topology Reasoning with Latent World   Models"></a>FASTopoWM: Fast-Slow Lane Segment Topology Reasoning with Latent World   Models</h2><p><strong>Authors:Yiming Yang, Hongbin Lin, Yueru Luo, Suzhong Fu, Chao Zheng, Xinrui Yan, Shuqi Mei, Kun Tang, Shuguang Cui, Zhen Li</strong></p>
<p>Lane segment topology reasoning provides comprehensive birdâ€™s-eye view (BEV) road scene understanding, which can serve as a key perception module in planning-oriented end-to-end autonomous driving systems. Existing lane topology reasoning methods often fall short in effectively leveraging temporal information to enhance detection and reasoning performance. Recently, stream-based temporal propagation method has demonstrated promising results by incorporating temporal cues at both the query and BEV levels. However, it remains limited by over-reliance on historical queries, vulnerability to pose estimation failures, and insufficient temporal propagation. To overcome these limitations, we propose FASTopoWM, a novel fast-slow lane segment topology reasoning framework augmented with latent world models. To reduce the impact of pose estimation failures, this unified framework enables parallel supervision of both historical and newly initialized queries, facilitating mutual reinforcement between the fast and slow systems. Furthermore, we introduce latent query and BEV world models conditioned on the action latent to propagate the state representations from past observations to the current timestep. This design substantially improves the performance of temporal perception within the slow pipeline. Extensive experiments on the OpenLane-V2 benchmark demonstrate that FASTopoWM outperforms state-of-the-art methods in both lane segment detection (37.4% v.s. 33.6% on mAP) and centerline perception (46.3% v.s. 41.5% on OLS). </p>
<blockquote>
<p>è½¦é“æ®µæ‹“æ‰‘æ¨ç†æä¾›äº†å…¨é¢çš„é¸Ÿç°å›¾ï¼ˆBEVï¼‰é“è·¯åœºæ™¯ç†è§£ï¼Œè¿™å¯ä»¥ä½œä¸ºé¢å‘è§„åˆ’ç«¯çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­çš„å…³é”®æ„ŸçŸ¥æ¨¡å—ã€‚ç°æœ‰çš„è½¦é“æ‹“æ‰‘æ¨ç†æ–¹æ³•é€šå¸¸ä¸èƒ½æœ‰æ•ˆåœ°åˆ©ç”¨æ—¶é—´ä¿¡æ¯æ¥æé«˜æ£€æµ‹å’Œæ¨ç†æ€§èƒ½ã€‚æœ€è¿‘ï¼ŒåŸºäºæµçš„æ—¶ç©ºä¼ æ’­æ–¹æ³•åœ¨æŸ¥è¯¢å’ŒBEVçº§åˆ«ä¸Šèå…¥æ—¶é—´çº¿ç´¢ï¼Œå–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶å—åˆ°è¿‡åº¦ä¾èµ–å†å²æŸ¥è¯¢ã€å¯¹å§¿æ€ä¼°è®¡å¤±è´¥çš„è„†å¼±æ€§ä»¥åŠæ—¶é—´ä¼ æ’­ä¸è¶³çš„å±€é™ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†FASTopoWMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¿«æ…¢è½¦é“æ®µæ‹“æ‰‘æ¨ç†æ¡†æ¶ï¼Œè¾…ä»¥æ½œåœ¨çš„ä¸–ç•Œæ¨¡å‹ã€‚ä¸ºäº†å‡å°‘å§¿æ€ä¼°è®¡å¤±è´¥çš„å½±å“ï¼Œè¿™ä¸€ç»Ÿä¸€æ¡†æ¶å®ç°å¯¹å†å²å’Œæ–°åˆå§‹åŒ–æŸ¥è¯¢çš„å¹¶è¡Œç›‘ç£ï¼Œä¿ƒè¿›å¿«æ…¢ç³»ç»Ÿä¹‹é—´çš„ç›¸äº’ä¿ƒè¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºåŠ¨ä½œæ½œå˜é‡çš„æ½œåœ¨æŸ¥è¯¢å’ŒBEVä¸–ç•Œæ¨¡å‹ï¼Œä»è¿‡å»çš„çŠ¶æ€è¡¨ç¤ºä¼ æ’­åˆ°å½“å‰æ—¶é—´æ­¥ã€‚è¿™ä¸€è®¾è®¡æ˜¾è‘—æé«˜äº†æ…¢ç®¡é“å†…çš„æ—¶é—´æ„ŸçŸ¥æ€§èƒ½ã€‚åœ¨OpenLane-V2åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFASTopoWMåœ¨è½¦é“æ®µæ£€æµ‹ï¼ˆmAPä¸Šä¸º37.4%å¯¹æ¯”33.6%ï¼‰å’Œä¸­å¿ƒçº¿æ„ŸçŸ¥ï¼ˆOLSä¸Šä¸º46.3%å¯¹æ¯”41.5%ï¼‰æ–¹é¢å‡ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23325v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šè½¦é“æ‹“æ‰‘æ¨ç†å¯¹è§„åˆ’å¯¼å‘çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿè‡³å…³é‡è¦ï¼Œæä¾›å…¨é¢çš„é¸Ÿç°å›¾ï¼ˆBEVï¼‰é“è·¯åœºæ™¯ç†è§£ã€‚ç°æœ‰æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ—¶é—´ä¿¡æ¯ï¼Œè€ŒåŸºäºæµçš„æ—¶ç©ºä¼ æ’­æ–¹æ³•è™½å·²æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ä»å—é™äºè¿‡åº¦ä¾èµ–å†å²æŸ¥è¯¢ã€å§¿æ€ä¼°è®¡å¤±è´¥å’Œæ—¶ç©ºä¼ æ’­ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œæå‡ºFASTopoWMæ¡†æ¶ï¼Œç»“åˆå¿«æ…¢ç³»ç»Ÿå’Œæ½œåœ¨ä¸–ç•Œæ¨¡å‹ï¼Œå‡å°‘å§¿æ€ä¼°è®¡å¤±è´¥çš„å½±å“ï¼Œå¹¶é€šè¿‡å¹¶è¡Œç›‘ç£å†å²å’Œæ–°åˆå§‹åŒ–æŸ¥è¯¢å®ç°ç›¸äº’å¼ºåŒ–ã€‚åœ¨OpenLane-V2åŸºå‡†æµ‹è¯•ä¸­ï¼ŒFASTopoWMåœ¨è½¦é“æ®µæ£€æµ‹å’Œä¸­å¿ƒçº¿æ„ŸçŸ¥æ–¹é¢ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è½¦é“æ‹“æ‰‘æ¨ç†æ˜¯è§„åˆ’å¯¼å‘çš„ç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„å…³é”®æ„ŸçŸ¥æ¨¡å—ã€‚</li>
<li>ç°æœ‰è½¦é“æ‹“æ‰‘æ¨ç†æ–¹æ³•æœªèƒ½æœ‰æ•ˆåˆ©ç”¨æ—¶é—´ä¿¡æ¯ï¼Œå¯¼è‡´æ£€æµ‹å’Œæ¨ç†æ€§èƒ½ä¸è¶³ã€‚</li>
<li>åŸºäºæµçš„æ—¶ç©ºä¼ æ’­æ–¹æ³•è™½ç„¶æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ä»é¢ä¸´è¿‡åº¦ä¾èµ–å†å²æŸ¥è¯¢ã€å§¿æ€ä¼°è®¡å¤±è´¥å’Œæ—¶ç©ºä¼ æ’­ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>FASTopoWMæ¡†æ¶ç»“åˆå¿«æ…¢ç³»ç»Ÿå’Œæ½œåœ¨ä¸–ç•Œæ¨¡å‹ï¼Œä»¥æé«˜è½¦é“æ‹“æ‰‘æ¨ç†çš„æ€§èƒ½ã€‚</li>
<li>FASTopoWMé€šè¿‡å¹¶è¡Œç›‘ç£å†å²å’Œæ–°åˆå§‹åŒ–æŸ¥è¯¢ï¼Œå‡å°‘å§¿æ€ä¼°è®¡å¤±è´¥çš„å½±å“ï¼Œå®ç°ç›¸äº’å¼ºåŒ–ã€‚</li>
<li>FASTopoWMåœ¨OpenLane-V2åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è½¦é“æ®µæ£€æµ‹å’Œä¸­å¿ƒçº¿æ„ŸçŸ¥æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23325">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6f3c1db2fb670a042083009f4a6823fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ef38fc48de5f5fe9c7e4874d706996f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8c1b4ea39cdfb55bb0e81d7462978c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a52bc3f0375195a602d28362ffd6b0d3.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Good-Learners-Think-Their-Thinking-Generative-PRM-Makes-Large-Reasoning-Model-More-Efficient-Math-Learner"><a href="#Good-Learners-Think-Their-Thinking-Generative-PRM-Makes-Large-Reasoning-Model-More-Efficient-Math-Learner" class="headerlink" title="Good Learners Think Their Thinking: Generative PRM Makes Large Reasoning   Model More Efficient Math Learner"></a>Good Learners Think Their Thinking: Generative PRM Makes Large Reasoning   Model More Efficient Math Learner</h2><p><strong>Authors:Tao He, Rongchuan Mu, Lizi Liao, Yixin Cao, Ming Liu, Bing Qin</strong></p>
<p>Large reasoning models (LRMs) have recently shown promise in solving complex math problems when optimized with Reinforcement Learning (RL). But conventional approaches rely on outcome-only rewards that provide sparse feedback, resulting in inefficient optimization process. In this work, we investigate the function of process reward models (PRMs) to accelerate the RL training for LRMs. We propose a novel intrinsic signal-driven generative process evaluation mechanism operating at the thought level to address major bottlenecks in RL-based training. Specifically, instead of requiring PRMs to know how to solve problems, our method uses intrinsic signals in solutions to judge stepwise correctness and aggregate contiguous correct&#x2F;incorrect steps into coherent â€˜thoughtâ€™ units. This structured, thought-level rewards enable more reliable credit assignment by reducing ambiguity in step segmentation and alleviating reward hacking. We further introduce a capability-adaptive reward mechanism that dynamically balances exploration and exploitation based on the LRMâ€™s current proficiency, guiding learning without stifling creative trial-and-error. These innovations are integrated into a new off-policy RL algorithm, TP-GRPO, which extends grouped proximal optimization with process-based rewards and improves training efficiency. Experiments on 1.5B and 7B parameter LRMs demonstrate that our method achieves higher problem-solving accuracy with significantly fewer training samples than outcome-only reward baselines. The results validate that well-structured process rewards can substantially accelerate LRM optimization in math reasoning tasks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/cs-holder/tp_grpo">https://github.com/cs-holder/tp_grpo</a>. </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰æœ€è¿‘åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œä¼˜åŒ–æ—¶ï¼Œæ˜¾ç¤ºå‡ºè§£å†³å¤æ‚æ•°å­¦é—®é¢˜çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ–¹æ³•ä»…ä¾èµ–äºç»“æœå¥–åŠ±ï¼Œæä¾›ç¨€ç–çš„åé¦ˆï¼Œå¯¼è‡´ä¼˜åŒ–è¿‡ç¨‹æ•ˆç‡ä½ä¸‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰åœ¨åŠ é€ŸLRMçš„RLè®­ç»ƒä¸­çš„åŠŸèƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„å†…åœ¨ä¿¡å·é©±åŠ¨ç”Ÿæˆè¿‡ç¨‹è¯„ä¼°æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åœ¨æ€ç»´å±‚é¢è¿ä½œï¼Œä»¥è§£å†³åŸºäºRLçš„è®­ç»ƒä¸­çš„ä¸»è¦ç“¶é¢ˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦è¿‡ç¨‹å¥–åŠ±æ¨¡å‹çŸ¥é“å¦‚ä½•è§£å†³é—®é¢˜ï¼Œè€Œæ˜¯åˆ©ç”¨è§£å†³æ–¹æ¡ˆä¸­çš„å†…åœ¨ä¿¡å·æ¥åˆ¤æ–­æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¹¶å°†è¿ç»­çš„æ­£ç¡®æˆ–é”™è¯¯æ­¥éª¤åˆå¹¶ä¸ºè¿è´¯çš„â€œæ€ç»´â€å•å…ƒã€‚è¿™ç§ç»“æ„åŒ–çš„ã€æ€ç»´å±‚é¢çš„å¥–åŠ±é€šè¿‡å‡å°‘æ­¥éª¤åˆ†å‰²çš„æ¨¡ç³Šæ€§å’Œé˜²æ­¢å¥–åŠ±æ“çºµï¼Œä½¿ä¿¡ç”¨åˆ†é…æ›´åŠ å¯é ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ç§èƒ½åŠ›è‡ªé€‚åº”å¥–åŠ±æœºåˆ¶ï¼Œæ ¹æ®LRMå½“å‰çš„èƒ½åŠ›æ°´å¹³åŠ¨æ€å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼ŒæŒ‡å¯¼å­¦ä¹ è€Œä¸ä¼šæ‰¼æ€åˆ›é€ æ€§çš„è¯•é”™ã€‚è¿™äº›åˆ›æ–°è¢«é›†æˆåˆ°ä¸€ç§æ–°çš„ç¦»çº¿ç­–ç•¥RLç®—æ³•TP-GRPOä¸­ï¼Œè¯¥ç®—æ³•åŸºäºè¿‡ç¨‹å¥–åŠ±æ‰©å±•äº†åˆ†ç»„è¿‘ç«¯ä¼˜åŒ–ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚åœ¨1.5Bå’Œ7Bå‚æ•°çš„å¤§å‹æ¨ç†æ¨¡å‹ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºä»…ä¾èµ–ç»“æœå¥–åŠ±çš„åŸºå‡†æ–¹æ³•ï¼Œè¾¾åˆ°äº†æ›´é«˜çš„è§£é¢˜å‡†ç¡®æ€§å¹¶å¤§å¹…å‡å°‘äº†è®­ç»ƒæ ·æœ¬æ•°é‡ã€‚ç»“æœè¯æ˜ï¼Œç»“æ„è‰¯å¥½çš„è¿‡ç¨‹å¥–åŠ±å¯ä»¥å¤§å¤§åŠ é€Ÿæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­LRMçš„ä¼˜åŒ–ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/cs-holder/tp_grpo%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/cs-holder/tp_grpoæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23317v1">PDF</a> 33 pages, 3 figures, 19 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨åŠ é€Ÿå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæ–¹é¢çš„ä½œç”¨ã€‚é’ˆå¯¹åŸºäºRLçš„è®­ç»ƒä¸­çš„ä¸»è¦ç“¶é¢ˆï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å†…ç”Ÿä¿¡å·é©±åŠ¨ç”Ÿæˆè¿‡ç¨‹è¯„ä¼°æœºåˆ¶ï¼Œè¯¥æœºåˆ¶åœ¨æ€æƒ³å±‚é¢è¿›è¡Œæ“ä½œï¼Œåˆ¤æ–­è§£é¢˜æ­¥éª¤çš„æ­£ç¡®æ€§ï¼Œå¹¶å°†è¿ç»­çš„æ­£ç¡®&#x2F;é”™è¯¯æ­¥éª¤åˆå¹¶ä¸ºè¿è´¯çš„â€œæ€æƒ³â€å•å…ƒã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§èƒ½åŠ›è‡ªé€‚åº”å¥–åŠ±æœºåˆ¶ï¼Œæ ¹æ®LRMå½“å‰çš„èƒ½åŠ›æ°´å¹³åŠ¨æ€å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼ŒæŒ‡å¯¼å­¦ä¹ è€Œä¸ä¼šæ‰¼æ€åˆ›é€ æ€§çš„è¯•é”™è¿‡ç¨‹ã€‚è¿™äº›åˆ›æ–°è¢«é›†æˆåˆ°ä¸€ç§æ–°çš„ç¦»çº¿RLç®—æ³•TP-GRPOä¸­ï¼Œè¯¥ç®—æ³•é€šè¿‡åŸºäºè¿‡ç¨‹å¥–åŠ±è¿›è¡Œåˆ†ç»„è¿‘ç«¯ä¼˜åŒ–ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚åœ¨å‚æ•°ä¸º1.5Bå’Œ7Bçš„LRMä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶çš„å‡†ç¡®æ€§æ›´é«˜ï¼Œæ‰€éœ€è®­ç»ƒæ ·æœ¬æ›´å°‘ã€‚è¯æ˜ç»“æ„è‰¯å¥½çš„è¿‡ç¨‹å¥–åŠ±å¯ä»¥å¤§å¤§åŠ é€Ÿæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­LRMçš„ä¼˜åŒ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰è¢«ç”¨æ¥åŠ é€Ÿå¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å†…åœ¨ä¿¡å·é©±åŠ¨ç”Ÿæˆè¿‡ç¨‹è¯„ä¼°æœºåˆ¶ï¼Œåœ¨æ€æƒ³å±‚é¢åˆ¤æ–­è§£é¢˜æ­¥éª¤çš„æ­£ç¡®æ€§ã€‚</li>
<li>é€šè¿‡å°†è¿ç»­çš„æ­£ç¡®&#x2F;é”™è¯¯æ­¥éª¤åˆå¹¶ä¸ºè¿è´¯çš„â€œæ€æƒ³â€å•å…ƒï¼Œå®ç°æ›´å¯é çš„ä¿¡ç”¨åˆ†é…ï¼Œå‡å°‘æ­¥éª¤åˆ†å‰²çš„æ¨¡ç³Šæ€§å’Œé˜²æ­¢å¥–åŠ±æ“çºµã€‚</li>
<li>å¼•å…¥èƒ½åŠ›è‡ªé€‚åº”å¥–åŠ±æœºåˆ¶ï¼Œæ ¹æ®æ¨¡å‹å½“å‰çš„èƒ½åŠ›æ°´å¹³åŠ¨æ€å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚</li>
<li>å°†è¿™äº›åˆ›æ–°é›†æˆåˆ°æ–°çš„ç¦»çº¿RLç®—æ³•TP-GRPOä¸­ï¼Œé€šè¿‡åŸºäºè¿‡ç¨‹å¥–åŠ±çš„åˆ†ç»„è¿‘ç«¯ä¼˜åŒ–ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>åœ¨1.5Bå’Œ7Bå‚æ•°çš„LRMä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæ–°æ–¹æ³•åœ¨è§£å†³æ•°å­¦é—®é¢˜æ—¶è¡¨ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§å’Œæ›´ä½çš„è®­ç»ƒæ ·æœ¬éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23317">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b58cb0c6d5516d1555f51ce85c85137e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-29e60dfaa99dd4c7e784cf117a7895fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b38844545becc8c2c1d288a7eb28b603.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="DynaSwarm-Dynamically-Graph-Structure-Selection-for-LLM-based-Multi-agent-System"><a href="#DynaSwarm-Dynamically-Graph-Structure-Selection-for-LLM-based-Multi-agent-System" class="headerlink" title="DynaSwarm: Dynamically Graph Structure Selection for LLM-based   Multi-agent System"></a>DynaSwarm: Dynamically Graph Structure Selection for LLM-based   Multi-agent System</h2><p><strong>Authors:Hui Yi Leong, Yuqing Wu</strong></p>
<p>Current multi-agent systems (MAS) frameworks often rely on manually designed and static collaboration graph structures, limiting adaptability and performance. To address these limitations, we propose DynaSwarm, a dynamic framework that enhances LLM-based MAS through two key innovations: (1) an actor-critic reinforcement learning (A2C) mechanism to optimize graph structures with improved stability over prior RL methods, and (2) a dynamic graph selector that adaptively chooses the optimal graph structure for each input sample via parameter-efficient LLM fine-tuning. DynaSwarm eliminates the need for rigid, one-fits-all graph architectures, instead leveraging sample-specific idiosyncrasies to dynamically route queries through specialized agent networks. (c) We propose to fine-tune the demonstration retriever to fully exploit the power of in-context learning (ICL). Extensive experiments on question answering, mathematical reasoning, and coding tasks demonstrate that DynaSwarm consistently outperforms state-of-the-art single-agent and MAS baselines across multiple LLM backbones. Our findings highlight the importance of sample-aware structural flexibility in LLM MAS designs. </p>
<blockquote>
<p>å½“å‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰æ¡†æ¶é€šå¸¸ä¾èµ–äºæ‰‹åŠ¨è®¾è®¡å’Œé™æ€çš„åä½œå›¾ç»“æ„ï¼Œè¿™é™åˆ¶äº†å…¶é€‚åº”æ€§å’Œæ€§èƒ½ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†DynaSwarmï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹å¢å¼ºåŸºäºLLMçš„MASï¼šï¼ˆ1ï¼‰ä¸€ç§æ¼”å‘˜è¯„è®ºå®¶å¼ºåŒ–å­¦ä¹ ï¼ˆA2Cï¼‰æœºåˆ¶ï¼Œé€šè¿‡æ”¹è¿›çš„ç¨³å®šæ€§ä¼˜åŒ–å›¾ç»“æ„ï¼Œä¼˜äºå…ˆå‰çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªåŠ¨æ€å›¾é€‰æ‹©å™¨ï¼Œé€šè¿‡å‚æ•°é«˜æ•ˆçš„LLMå¾®è°ƒè‡ªé€‚åº”åœ°ä¸ºæ¯ä¸ªè¾“å…¥æ ·æœ¬é€‰æ‹©æœ€ä½³å›¾ç»“æ„ã€‚DynaSwarmæ¶ˆé™¤äº†å¯¹ä¸¥æ ¼ã€ä¸€åˆ€åˆ‡çš„å›¾æ¶æ„çš„éœ€æ±‚ï¼Œè½¬è€Œåˆ©ç”¨æ ·æœ¬ç‰¹å®šçš„ç‰¹æ€§æ¥åŠ¨æ€åœ°é€šè¿‡ä¸“ç”¨ä»£ç†ç½‘ç»œè·¯ç”±æŸ¥è¯¢ã€‚Â©æˆ‘ä»¬æè®®å¯¹æ¼”ç¤ºæ£€ç´¢å™¨è¿›è¡Œå¾®è°ƒï¼Œä»¥å……åˆ†åˆ©ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„åŠ›é‡ã€‚åœ¨é—®ç­”ã€æ•°å­¦æ¨ç†å’Œç¼–ç ä»»åŠ¡æ–¹é¢çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDynaSwarmåœ¨å¤šä¸ªLLMä¸»å¹²ç½‘ä¸Šå§‹ç»ˆä¼˜äºæœ€æ–°çš„å•æ™ºèƒ½ä½“å’ŒMASåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨LLM MASè®¾è®¡ä¸­ï¼Œæ ·æœ¬æ„ŸçŸ¥çš„ç»“æ„çµæ´»æ€§éå¸¸é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23261v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>DynaSwarmæ˜¯ä¸€ä¸ªåŠ¨æ€æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸¤é¡¹åˆ›æ–°è§£å†³äº†å½“å‰å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰æ¡†æ¶åœ¨é€‚åº”æ€§å’Œæ€§èƒ½æ–¹é¢çš„å±€é™æ€§ã€‚é¦–å…ˆï¼Œå®ƒé‡‡ç”¨æ¼”å‘˜è¯„è®ºå®¶å¼ºåŒ–å­¦ä¹ ï¼ˆA2Cï¼‰æœºåˆ¶ä¼˜åŒ–å›¾å½¢ç»“æ„ï¼Œæé«˜äº†ç¨³å®šæ€§ã€‚å…¶æ¬¡ï¼Œå®ƒæœ‰ä¸€ä¸ªåŠ¨æ€å›¾å½¢é€‰æ‹©å™¨ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°ä¸ºæ¯ä¸ªè¾“å…¥æ ·æœ¬é€‰æ‹©æœ€ä½³å›¾å½¢ç»“æ„ã€‚DynaSwarmé€šè¿‡æ ·æœ¬ç‰¹å®šçš„ç‰¹æ€§åŠ¨æ€è·¯ç”±æŸ¥è¯¢ï¼Œå¹¶é€šè¿‡ä¸“é—¨è®¾è®¡çš„æ™ºèƒ½ä½“ç½‘ç»œå®ç°æ€§èƒ½æå‡ã€‚å®éªŒè¡¨æ˜ï¼ŒDynaSwarmåœ¨é—®ç­”ã€æ•°å­¦æ¨ç†å’Œç¼–ç ä»»åŠ¡ä¸Šå‡ä¼˜äºå•æ™ºèƒ½ä½“å’ŒMASåŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰æ¡†æ¶å—é™äºæ‰‹åŠ¨è®¾è®¡å’Œé™æ€åä½œå›¾ç»“æ„ï¼Œç¼ºä¹é€‚åº”æ€§ã€‚</li>
<li>DynaSwarmæ˜¯ä¸€ä¸ªåŠ¨æ€æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é¡¹å…³é”®åˆ›æ–°è§£å†³æ­¤é—®é¢˜ï¼šé‡‡ç”¨A2Cæœºåˆ¶ä¼˜åŒ–å›¾å½¢ç»“æ„å’Œå…·æœ‰è‡ªé€‚åº”é€‰æ‹©æœ€ä½³å›¾å½¢ç»“æ„çš„èƒ½åŠ›ã€‚</li>
<li>DynaSwarmåˆ©ç”¨æ ·æœ¬ç‰¹å®šçš„ç‰¹æ€§æ¥åŠ¨æ€è·¯ç”±æŸ¥è¯¢ï¼Œé€šè¿‡ä¸“é—¨è®¾è®¡çš„æ™ºèƒ½ä½“ç½‘ç»œæå‡æ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸éœ€è¦é€šç”¨å›¾å½¢æ¶æ„ï¼Œå¯ä»¥é’ˆå¯¹æ¯ä¸ªä»»åŠ¡è¿›è¡Œçµæ´»è°ƒæ•´ã€‚</li>
<li>DynaSwarmåœ¨é—®ç­”ã€æ•°å­¦æ¨ç†å’Œç¼–ç ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨ç°ä¼˜äºå…¶ä»–æ¡†æ¶ã€‚</li>
<li>å®éªŒç»“æœå¼ºè°ƒäº†æ ·æœ¬æ„ŸçŸ¥ç»“æ„çµæ´»æ€§åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿè®¾è®¡ä¸­é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23261">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6c21dd2840ce5432989f24774259557a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0c6ec7e8ce71bd05a3a980e4b096b5f5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6f2ae755b9f9afc07e385887b4362def.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3821c2ab93a1eed037b8809fd1ce64e7.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="P-ReMIS-Pragmatic-Reasoning-in-Mental-Health-and-a-Social-Implication"><a href="#P-ReMIS-Pragmatic-Reasoning-in-Mental-Health-and-a-Social-Implication" class="headerlink" title="P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication"></a>P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication</h2><p><strong>Authors:Sneha Oram, Pushpak Bhattacharyya</strong></p>
<p>There has been an increase in recent advancements in the explainability and development of personalized chatbots for mental health. However, the reasoning aspects for explainability and dialogue discourse have not been explored previously for mental health. Hence, we are investigating the pragmatic reasoning capability of large language models (LLMs) in this domain. We introduce P-ReMe dataset, and propose a modified definition for the pragmatic phenomena of implicature (implied meaning) and presupposition (implicit assumption) in mental health. Following the definition, we formulate two tasks in implicature and one task in presupposition. To benchmark the dataset and the presented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the experiments suggest that Mistral and Qwen show substantial reasoning capabilities in the domain. In addition, we also propose StiPRompts to study the stigma around mental health with the state-of-the-art LLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings show that Claude-3.5-haiku deals with the stigma more responsibly compared to the other two LLMs. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œåœ¨å¿ƒç†å¥åº·é¢†åŸŸï¼Œå¯è§£é‡Šæ€§å’Œä¸ªæ€§åŒ–èŠå¤©æœºå™¨äººçš„å‘å±•å–å¾—äº†è¿›å±•ã€‚ç„¶è€Œï¼Œå…³äºå¿ƒç†å¥åº·çš„å¯è§£é‡Šæ€§å’Œå¯¹è¯è¯­å¢ƒçš„æ¨ç†æ–¹é¢å°šæœªè¢«æ¢ç´¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ­£åœ¨ç ”ç©¶è¯¥é¢†åŸŸçš„å¤§å‹è¯­è¨€æ¨¡å‹çš„å®ç”¨æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä»‹ç»äº†P-ReMeæ•°æ®é›†ï¼Œå¹¶é’ˆå¯¹å¿ƒç†å¥åº·ä¸­çš„éšæ¶µï¼ˆéšå«æ„ä¹‰ï¼‰å’Œé¢„è®¾ï¼ˆéšå«å‡è®¾ï¼‰çš„è¯­ç”¨ç°è±¡æå‡ºäº†ä¿®æ”¹åçš„å®šä¹‰ã€‚æ ¹æ®å®šä¹‰ï¼Œæˆ‘ä»¬åˆ¶å®šäº†ä¸¤ä¸ªéšæ¶µä»»åŠ¡å’Œä¸¤ä¸ªé¢„è®¾ä»»åŠ¡ã€‚ä¸ºäº†å¯¹æ•°æ®é›†å’Œä»»åŠ¡è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬é€‰æ‹©äº†å››ç§æ¨¡å‹ï¼ŒåŒ…æ‹¬Llama3.1ã€Mistralã€MentaLLaMaå’ŒQwenã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMistralå’ŒQwenåœ¨è¯¥é¢†åŸŸæ˜¾ç¤ºå‡ºå¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†StiPRomptsæ¥ç ”ç©¶å›´ç»•å¿ƒç†å¥åº·çš„æ±¡ååŒ–é—®é¢˜ï¼Œå¹¶ä½¿ç”¨æœ€å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹GPT-4o miniã€Deepseek-chatå’ŒClaude-3.5-haikuè¿›è¡Œç ”ç©¶ã€‚æˆ‘ä»¬çš„è¯„ä¼°å‘ç°ï¼Œä¸å…¶ä»–ä¸¤ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒClaude-3.5-haikuæ›´èƒ½è´Ÿè´£ä»»åœ°å¤„ç†æ±¡ååŒ–é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23247v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸåœ¨ç²¾ç¥å¥åº·é¢†åŸŸï¼Œä¸ªæ€§åŒ–èŠå¤©æœºå™¨äººçš„è§£é‡Šæ€§å’Œå‘å±•æ€§æœ‰æ‰€è¿›æ­¥ï¼Œä½†å…³äºè§£é‡Šæ€§çš„æ¨ç†æ–¹é¢ä»¥åŠå¯¹è¯è¯­æ°”çš„æ¢è®¨å°šæœªæ¶‰åŠã€‚æˆ‘ä»¬æ¢ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ­¤é¢†åŸŸçš„è¯­ç”¨æ¨ç†èƒ½åŠ›ï¼Œå¹¶å¼•å…¥äº†P-ReMeæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºç²¾ç¥å¥åº·ä¸­çš„éšå«æ„ä¹‰å’Œéšå«å‡è®¾æå‡ºäº†ä¿®æ”¹åçš„å®šä¹‰ï¼Œå¹¶æ®æ­¤åˆ¶å®šäº†ä¸¤ä¸ªä»»åŠ¡å’Œé¢„è®¾ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMistralå’ŒQwenåœ¨æ­¤é¢†åŸŸçš„æ¨ç†èƒ½åŠ›æ˜¾è‘—ã€‚æˆ‘ä»¬è¿˜æå‡ºStiPRomptsæ¥ç ”ç©¶å…³äºç²¾ç¥å¥åº·çš„åè§ï¼Œç»è¿‡è¯„ä¼°å‘ç°Claude-3.5-haikuåœ¨å¤„ç†åè§æ–¹é¢è¡¨ç°æ›´ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸ªæ€§åŒ–èŠå¤©æœºå™¨äººåœ¨ç²¾ç¥å¥åº·é¢†åŸŸçš„è§£é‡Šæ€§å’Œå‘å±•æ€§æœ‰æ‰€è¿›æ­¥ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¯­ç”¨æ¨ç†èƒ½åŠ›æ­£åœ¨è¢«æ¢ç©¶ã€‚</li>
<li>P-ReMeæ•°æ®é›†è¢«å¼•å…¥ï¼Œå¹¶å¯¹éšå«æ„ä¹‰å’Œéšå«å‡è®¾æå‡ºäº†ä¿®æ”¹åçš„å®šä¹‰ã€‚</li>
<li>Mistralå’ŒQwenåœ¨ç²¾ç¥å¥åº·é¢†åŸŸçš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>StiPRomptsè¢«ç”¨æ¥ç ”ç©¶å…³äºç²¾ç¥å¥åº·çš„åè§ã€‚</li>
<li>Claude-3.5-haikuåœ¨å¤„ç†åè§æ–¹é¢è¡¨ç°æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23247">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-277b085094f62f54edc6ecce4d9eac79.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e71cba15d599630d06827f1e430ea8cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0c542b7b7cc0509afb494e4c782373a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-70749dd3b9a31769ae6f1c0d9fb342e5.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Generalized-Reinforcement-Learning-for-Retriever-Specific-Query-Rewriter-with-Unstructured-Real-World-Documents"><a href="#Generalized-Reinforcement-Learning-for-Retriever-Specific-Query-Rewriter-with-Unstructured-Real-World-Documents" class="headerlink" title="Generalized Reinforcement Learning for Retriever-Specific Query Rewriter   with Unstructured Real-World Documents"></a>Generalized Reinforcement Learning for Retriever-Specific Query Rewriter   with Unstructured Real-World Documents</h2><p><strong>Authors:Sungguk Cha, DongWook Kim, Taeseung Hahn, Mintae Kim, Youngsub Han, Byoung-Ki Jeon</strong></p>
<p>Retrieval-Augmented Generation (RAG) systems rely heavily on effective query formulation to unlock external knowledge, yet optimizing queries for diverse, unstructured real-world documents remains a challenge. We introduce \textbf{RL-QR}, a reinforcement learning framework for retriever-specific query rewriting that eliminates the need for human-annotated datasets and extends applicability to both text-only and multi-modal databases. By synthesizing scenario-question pairs and leveraging Generalized Reward Policy Optimization (GRPO), RL-QR trains query rewriters tailored to specific retrievers, enhancing retrieval performance across varied domains. Experiments on industrial in-house data demonstrate significant improvements, with $\text{RL-QR}<em>{\text{multi-modal}}$ achieving an 11% relative gain in NDCG@3 for multi-modal RAG and $\text{RL-QR}</em>{\text{lexical}}$ yielding a 9% gain for lexical retrievers. However, challenges persist with semantic and hybrid retrievers, where rewriters failed to improve performance, likely due to training misalignments. Our findings highlight RL-QRâ€™s potential to revolutionize query optimization for RAG systems, offering a scalable, annotation-free solution for real-world retrieval tasks, while identifying avenues for further refinement in semantic retrieval contexts. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿä¸¥é‡ä¾èµ–äºæœ‰æ•ˆçš„æŸ¥è¯¢æ„å»ºä»¥è®¿é—®å¤–éƒ¨çŸ¥è¯†ï¼Œç„¶è€Œï¼Œé’ˆå¯¹å¤šæ ·åŒ–ã€éç»“æ„åŒ–çš„ç°å®ä¸–ç•Œæ–‡æ¡£ä¼˜åŒ–æŸ¥è¯¢ä»æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†<strong>RL-QR</strong>è¿™ä¸€å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºç‰¹å®šäºæ£€ç´¢å™¨çš„æŸ¥è¯¢é‡å†™ï¼Œä»è€Œæ— éœ€äººå·¥æ ‡æ³¨æ•°æ®é›†ï¼Œå¹¶æ‰©å±•äº†å…¶åœ¨çº¯æ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®åº“ä¸­çš„åº”ç”¨ã€‚é€šè¿‡åˆæˆåœºæ™¯-é—®é¢˜å¯¹å¹¶åˆ©ç”¨å¹¿ä¹‰å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼ŒRL-QRè®­ç»ƒé€‚ç”¨äºç‰¹å®šæ£€ç´¢å™¨çš„æŸ¥è¯¢é‡å†™å™¨ï¼Œå¢å¼ºè·¨ä¸åŒé¢†åŸŸçš„æ£€ç´¢æ€§èƒ½ã€‚åœ¨å·¥ä¸šå†…éƒ¨æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRL-QRå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œå…¶ä¸­å¤šæ¨¡æ€RL-QR_multi-modalåœ¨NDCG@3ä¸Šå®ç°äº†ç›¸å¯¹å¢ç›Š11%ï¼Œè¯æ±‡æ£€ç´¢å™¨RL-QR_lexicalåˆ™å–å¾—äº†9%çš„å¢ç›Šã€‚ç„¶è€Œï¼Œåœ¨è¯­ä¹‰æ£€ç´¢å™¨å’Œæ··åˆæ£€ç´¢å™¨ä¸­ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œé‡å†™å™¨æœªèƒ½æ”¹å–„æ€§èƒ½ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºè®­ç»ƒä¸ä¸€è‡´é€ æˆçš„ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªå‡ºäº†RL-QRåœ¨RAGç³»ç»ŸæŸ¥è¯¢ä¼˜åŒ–æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºç°å®ä¸–ç•Œçš„æ£€ç´¢ä»»åŠ¡æä¾›äº†å¯æ‰©å±•ã€æ— éœ€æ³¨é‡Šçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶æŒ‡å‡ºäº†åœ¨è¯­ä¹‰æ£€ç´¢ä¸Šä¸‹æ–‡ä¸­è¿›ä¸€æ­¥æ”¹è¿›çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23242v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ åœ¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿçš„æŸ¥è¯¢ä¼˜åŒ–ä¸­å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚æå‡ºçš„RL-QRæ¡†æ¶èƒ½å¤Ÿå®ç°æ— éœ€äººå·¥æ ‡æ³¨æ•°æ®é›†çš„æŸ¥è¯¢é‡å†™ï¼Œå¹¶é€‚ç”¨äºæ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®åº“ã€‚é€šè¿‡åˆæˆåœºæ™¯-é—®é¢˜å¯¹å’Œå¹¿ä¹‰å¥–åŠ±ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼ŒRL-QRè®­ç»ƒå‡ºé’ˆå¯¹ç‰¹å®šæ£€ç´¢å™¨çš„æŸ¥è¯¢é‡å†™å™¨ï¼Œæé«˜äº†è·¨ä¸åŒé¢†åŸŸçš„æ£€ç´¢æ€§èƒ½ã€‚åœ¨å·¥ä¸šå†…éƒ¨æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRL-QRåœ¨å¤šæ¨¡æ€å’Œè¯æ±‡æ£€ç´¢å™¨æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œä½†åœ¨è¯­ä¹‰å’Œæ··åˆæ£€ç´¢å™¨ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚æ­¤ç ”ç©¶ä¸ºRAGç³»ç»Ÿçš„æŸ¥è¯¢ä¼˜åŒ–æä¾›äº†å¯æ‰©å±•çš„æ— æ ‡æ³¨è§£å†³æ–¹æ¡ˆï¼Œå¹¶æŒ‡å‡ºäº†è¯­ä¹‰æ£€ç´¢ä¸Šä¸‹æ–‡ä¸­è¿›ä¸€æ­¥æ”¹è¿›çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RL-QRæ¡†æ¶é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–RAGç³»ç»Ÿçš„æŸ¥è¯¢ï¼Œå®ç°æ— éœ€äººå·¥æ ‡æ³¨æ•°æ®é›†ã€‚</li>
<li>RL-QRé€‚ç”¨äºæ–‡æœ¬å’Œå¤šæ¨¡æ€æ•°æ®åº“çš„æŸ¥è¯¢é‡å†™ã€‚</li>
<li>é€šè¿‡åˆæˆåœºæ™¯-é—®é¢˜å¯¹ï¼ŒRL-QRè®­ç»ƒå‡ºé’ˆå¯¹ç‰¹å®šæ£€ç´¢å™¨çš„æŸ¥è¯¢é‡å†™å™¨ã€‚</li>
<li>RL-QRåœ¨è·¨ä¸åŒé¢†åŸŸçš„æ£€ç´¢æ€§èƒ½ä¸Šæœ‰æ‰€æé«˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å·¥ä¸šå†…éƒ¨æ•°æ®çš„å®éªŒä¸­ã€‚</li>
<li>RL-QRåœ¨å¤šæ¨¡æ€å’Œè¯æ±‡æ£€ç´¢å™¨ä¸Šå–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œç›¸å¯¹å¢ç›Šè¾¾åˆ°11%å’Œ9%ã€‚</li>
<li>åœ¨è¯­ä¹‰å’Œæ··åˆæ£€ç´¢å™¨ä¸­ï¼ŒRL-QRä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¯èƒ½æ˜¯ç”±äºè®­ç»ƒä¸åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23242">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-093e0cc6feda2831ecac918c59cd73b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1264d8d22731929d86a4a399be6bb83e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1a31599c9cc3e342656da114fe86b734.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c0bc9115a1be583515ab3df0b37876c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb955a8310db3802faef5a8765d06acb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4197e2467c2f51815c2933a09756a2de.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="FairReason-Balancing-Reasoning-and-Social-Bias-in-MLLMs"><a href="#FairReason-Balancing-Reasoning-and-Social-Bias-in-MLLMs" class="headerlink" title="FairReason: Balancing Reasoning and Social Bias in MLLMs"></a>FairReason: Balancing Reasoning and Social Bias in MLLMs</h2><p><strong>Authors:Zhenyu Pan, Yutong Zhang, Jianshu Zhang, Haoran Lu, Haozheng Luo, Yuwei Han, Philip S. Yu, Manling Li, Han Liu</strong></p>
<p>Multimodal Large Language Models (MLLMs) already achieve state-of-the-art results across a wide range of tasks and modalities. To push their reasoning ability further, recent studies explore advanced prompting schemes and post-training fine-tuning. Although these techniques improve logical accuracy, they frequently leave the modelsâ€™ outputs burdened with pronounced social biases. Clarifying how reasoning gains interact with bias mitigation-and whether the two objectives inherently trade off-therefore remains an open and pressing research problem. Our study begins by benchmarking three bias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation (KD), and rule-based reinforcement learning (RL)-under identical conditions, establishing their baseline strengths and weaknesses. Building on these results, we vary the proportion of debias-focused and reasoning-centric samples within each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement learning cuts stereotype scores by 10% while retaining 88% of the modelâ€™s original reasoning accuracy, offering concrete guidance for balancing fairness and capability in MLLMs. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å·²ç»åœ¨å„ç§ä»»åŠ¡å’Œæ¨¡æ€ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å…¶æ¨ç†èƒ½åŠ›ï¼Œæœ€è¿‘çš„ç ”ç©¶æ¢ç´¢äº†å…ˆè¿›çš„æç¤ºæ–¹æ¡ˆå’Œè®­ç»ƒåçš„å¾®è°ƒã€‚è™½ç„¶è¿™äº›æŠ€æœ¯æé«˜äº†é€»è¾‘å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬å¾€å¾€ä½¿æ¨¡å‹çš„è¾“å‡ºå¸¦æœ‰æ˜æ˜¾çš„ç¤¾ä¼šåè§ã€‚å› æ­¤ï¼Œé˜æ˜æ¨ç†æ”¶ç›Šå¦‚ä½•ä¸åè§ç¼“è§£ç›¸äº’ä½œç”¨ï¼Œä»¥åŠè¿™ä¸¤ä¸ªç›®æ ‡æ˜¯å¦æœ¬è´¨ä¸Šç›¸äº’æƒè¡¡ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾è€Œç´§è¿«çš„ç ”ç©¶é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶é¦–å…ˆåœ¨åŒç­‰æ¡ä»¶ä¸‹å¯¹ä¸‰ç§åè§ç¼“è§£ç­–ç•¥â€”â€”ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰å’ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰â€”â€”è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä»¥ç¡®å®šå®ƒä»¬çš„ä¼˜ç¼ºç‚¹ã€‚åŸºäºè¿™äº›ç»“æœï¼Œæˆ‘ä»¬åœ¨æ¯ç§èŒƒå¼ä¸­æ”¹å˜ä»¥å»åè§ä¸ºä¸­å¿ƒçš„æ ·æœ¬å’Œä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„æ ·æœ¬çš„æ¯”ä¾‹ï¼Œä»¥ç»˜åˆ¶æ¨ç†ä¸åè§ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬çš„æ‰«ææ­ç¤ºäº†ä¸€ä¸ªä¸€è‡´çš„ä¼˜åŒ–ç‚¹ï¼šä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¤§çº¦1:4æ¯”ä¾‹æ··åˆçš„è®­ç»ƒï¼Œå¯ä»¥å‡å°‘åˆ»æ¿å°è±¡å¾—åˆ†10%ï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹88%çš„åŸå§‹æ¨ç†å‡†ç¡®æ€§ï¼Œä¸ºåœ¨MLLMä¸­å¹³è¡¡å…¬å¹³å’Œèƒ½åŠ›æä¾›äº†å…·ä½“æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23067v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å¹¿æ³›çš„ä»»åŠ¡å’Œæ¨¡æ€ä¸Šå·²è¾¾æˆä¸šç•Œé¡¶å°–æˆæœã€‚ä¸ºè¿›ä¸€æ­¥æå‡å…¶æ¨ç†èƒ½åŠ›ï¼Œè¿‘æœŸç ”ç©¶å¼€å§‹æ¢ç´¢å…ˆè¿›çš„æç¤ºæ–¹æ¡ˆå’Œè®­ç»ƒåçš„å¾®è°ƒæŠ€æœ¯ã€‚å°½ç®¡è¿™äº›æŠ€æœ¯æé«˜äº†é€»è¾‘å‡†ç¡®æ€§ï¼Œä½†å®ƒä»¬å¸¸å¸¸ä½¿æ¨¡å‹è¾“å‡ºå¸¦æœ‰æ˜æ˜¾çš„ç¤¾ä¼šåè§ã€‚æœ¬ç ”ç©¶é¦–å…ˆåœ¨åŒæ¡ä»¶ä¸‹å¯¹æ¯”ä¸‰ç§å‡åç­–ç•¥ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰å’ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»¥ç¡®ç«‹å®ƒä»¬çš„åŸºç¡€ä¼˜åŠ£ã€‚æ¥ç€åŸºäºç ”ç©¶ç»“æœï¼Œé€šè¿‡è°ƒæ•´å‡åå’Œæ¨ç†ä¸­å¿ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ¨ç†ä¸åè§ä¹‹é—´çš„æƒè¡¡ã€‚å®éªŒæ­ç¤ºäº†ä¸€ä¸ªç¨³å®šçš„æœ€ä¼˜å¹³è¡¡ç‚¹ï¼šé‡‡ç”¨å¤§çº¦1:4çš„æ¯”ä¾‹ï¼Œç»“åˆå¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥å‡å°‘åˆ»æ¿å°è±¡åˆ†æ•°10%ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹åŸå§‹æ¨ç†èƒ½åŠ›çš„88%ï¼Œä¸ºå¹³è¡¡å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å…¬å¹³æ€§å’Œèƒ½åŠ›æä¾›äº†å…·ä½“æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šä¸ªä»»åŠ¡å’Œæ¨¡æ€ä¸Šå·²è¾¾åˆ°é¡¶å°–æ°´å¹³ï¼Œä½†ä»ç„¶å­˜åœ¨ç¤¾ä¼šåè§é—®é¢˜ã€‚</li>
<li>ç ”ç©¶å¯¹æ¯”äº†ä¸‰ç§å‡åç­–ç•¥ï¼šç›‘ç£å¾®è°ƒã€çŸ¥è¯†è’¸é¦å’ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>é€šè¿‡è°ƒæ•´å‡åå’Œæ¨ç†ä¸­å¿ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼Œç ”ç©¶å‘ç°æ¨ç†ä¸åè§ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚</li>
<li>é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç»“åˆä¸€å®šæ¯”ä¾‹å‡åæ ·æœ¬çš„è®­ç»ƒæ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆå‡å°‘æ¨¡å‹è¾“å‡ºçš„åˆ»æ¿å°è±¡åˆ†æ•°ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¤§çº¦1:4çš„å‡åä¸æ¨ç†ä¸­å¿ƒæ ·æœ¬æ¯”ä¾‹æ˜¯å¹³è¡¡å…¬å¹³æ€§å’Œèƒ½åŠ›çš„æœ€ä½³ç‚¹ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºæœªæ¥çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹æä¾›äº†åœ¨å…¬å¹³æ€§å’Œèƒ½åŠ›ä¹‹é—´å¯»æ‰¾å¹³è¡¡çš„å…·ä½“æŒ‡å¯¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23067">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-55d20e33566e500c2fa46f8f882eae03.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d0978a0a0f587de322562ec56daad3e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47e4ad702801c1e713acd5aff48bc96d.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Towards-Omnimodal-Expressions-and-Reasoning-in-Referring-Audio-Visual-Segmentation"><a href="#Towards-Omnimodal-Expressions-and-Reasoning-in-Referring-Audio-Visual-Segmentation" class="headerlink" title="Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual   Segmentation"></a>Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual   Segmentation</h2><p><strong>Authors:Kaining Ying, Henghui Ding, Guangquan Jie, Yu-Gang Jiang</strong></p>
<p>Referring audio-visual segmentation (RAVS) has recently seen significant advancements, yet challenges remain in integrating multimodal information and deeply understanding and reasoning about audiovisual content. To extend the boundaries of RAVS and facilitate future research in this field, we propose Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset containing 2,104 videos and 61,095 multimodal referring expressions. OmniAVS stands out with three key innovations: (1) 8 types of multimodal expressions that flexibly combine text, speech, sound, and visual cues; (2) an emphasis on understanding audio content beyond just detecting their presence; and (3) the inclusion of complex reasoning and world knowledge in expressions. Furthermore, we introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the challenges of multimodal reasoning and fine-grained understanding of audiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and perform reasoning-based segmentation. Extensive experiments show that OISA outperforms existing methods on OmniAVS and achieves competitive results on other related tasks. </p>
<blockquote>
<p>æŒ‡ä»£éŸ³é¢‘è§†è§‰åˆ†å‰²ï¼ˆRAVSï¼‰æœ€è¿‘å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨æ•´åˆå¤šæ¨¡å¼ä¿¡æ¯ä»¥åŠæ·±åº¦ç†è§£å’Œæ¨ç†è§†å¬å†…å®¹æ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºäº†æ‰©å±•RAVSçš„è¾¹ç•Œå¹¶ä¿ƒè¿›è¯¥é¢†åŸŸçš„æœªæ¥ç ”ç©¶ï¼Œæˆ‘ä»¬æå‡ºäº†OmniModalæŒ‡ä»£éŸ³é¢‘è§†è§‰åˆ†å‰²ï¼ˆOmniAVSï¼‰æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«2104ä¸ªè§†é¢‘å’Œ61095ä¸ªå¤šæ¨¡å¼æŒ‡ä»£è¡¨è¾¾ã€‚OmniAVSé€šè¿‡ä¸‰ä¸ªå…³é”®åˆ›æ–°ç‚¹è„±é¢–è€Œå‡ºï¼šï¼ˆ1ï¼‰æœ‰8ç§çµæ´»ç»“åˆæ–‡æœ¬ã€è¯­éŸ³ã€å£°éŸ³å’Œè§†è§‰çº¿ç´¢çš„å¤šæ¨¡å¼è¡¨è¾¾æ–¹å¼ï¼›ï¼ˆ2ï¼‰é™¤äº†æ£€æµ‹éŸ³é¢‘å†…å®¹æ˜¯å¦å­˜åœ¨ä¹‹å¤–ï¼Œè¿˜å¼ºè°ƒç†è§£éŸ³é¢‘å†…å®¹ï¼›ï¼ˆ3ï¼‰åœ¨è¡¨è¾¾ä¸­åŒ…å«å¤æ‚çš„æ¨ç†å’Œä¸–ç•ŒçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³OmniAVSä¸­å¤šæ¨¡å¼æ¨ç†å’Œå¯¹è§†å¬å†…å®¹çš„ç²¾ç»†ç†è§£æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†OmniModalæŒ‡ä»¤åˆ†å‰²åŠ©æ‰‹ï¼ˆOISAï¼‰ã€‚OISAä½¿ç”¨MLLMç†è§£å¤æ‚çº¿ç´¢å¹¶è¿›è¡ŒåŸºäºæ¨ç†çš„åˆ†å‰²ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒOISAåœ¨OmniAVSä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨å…¶ä»–ç›¸å…³ä»»åŠ¡ä¸Šå–å¾—äº†æœ‰ç«äº‰åŠ›çš„ç»“æœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22886v2">PDF</a> ICCV 2025, Project Page: <a target="_blank" rel="noopener" href="https://henghuiding.com/OmniAVS/">https://henghuiding.com/OmniAVS/</a></p>
<p><strong>Summary</strong></p>
<p>å¤šåª’ä½“æŒ‡ä»£éŸ³é¢‘è§†é¢‘åˆ†å‰²ï¼ˆOmniAVSï¼‰æ•°æ®é›†çš„æå‡ºï¼Œæ—¨åœ¨æ‹“å±•éŸ³é¢‘è§†è§‰åˆ†å‰²ï¼ˆRAVSï¼‰çš„è¾¹ç•Œï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„ç ”ç©¶æä¾›ä¾¿åˆ©ã€‚OmniAVSåŒ…å«2,104ä¸ªè§†é¢‘å’Œ61,095ä¸ªå¤šåª’ä½“æŒ‡ä»£è¡¨è¾¾å¼ï¼Œå…·æœ‰ä¸‰å¤§åˆ›æ–°ç‚¹ï¼šçµæ´»ç»“åˆæ–‡æœ¬ã€è¯­éŸ³ã€å£°éŸ³å’Œè§†è§‰çº¿ç´¢çš„å…«ç§å¤šåª’ä½“è¡¨è¾¾æ–¹å¼ï¼›å¼ºè°ƒå¯¹éŸ³é¢‘å†…å®¹çš„æ·±å…¥ç†è§£è€Œéä»…æ£€æµ‹å…¶å­˜åœ¨ï¼›ä»¥åŠè¡¨è¾¾ä¸­èå…¥å¤æ‚æ¨ç†å’Œä¸–ç•ŒçŸ¥è¯†ã€‚ä¸ºè§£å†³OmniAVSä¸­çš„å¤šåª’ä½“æ¨ç†å’Œç²¾ç»†éŸ³è§†é¢‘å†…å®¹ç†è§£æŒ‘æˆ˜ï¼Œå¼•å…¥äº†å¤šåª’ä½“æŒ‡ä»¤åˆ†å‰²åŠ©æ‰‹ï¼ˆOISAï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniAVSæ˜¯ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œæ‰©å±•äº†éŸ³é¢‘è§†è§‰åˆ†å‰²ï¼ˆRAVSï¼‰çš„è¾¹ç•Œï¼ŒåŒ…å«2,104ä¸ªè§†é¢‘å’Œ61,095ä¸ªå¤šåª’ä½“æŒ‡ä»£è¡¨è¾¾å¼ã€‚</li>
<li>OmniAVSå¼ºè°ƒçµæ´»ç»“åˆå¤šç§æ¨¡æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€è¯­éŸ³ã€å£°éŸ³å’Œè§†è§‰çº¿ç´¢ã€‚</li>
<li>æ•°æ®é›†æ³¨é‡å¯¹éŸ³é¢‘å†…å®¹çš„æ·±å…¥ç†è§£ï¼Œè€Œä¸ä»…ä»…æ˜¯æ£€æµ‹å…¶å­˜åœ¨ã€‚</li>
<li>OmniAVSä¸­çš„è¡¨è¾¾èå…¥äº†å¤æ‚æ¨ç†å’Œä¸–ç•ŒçŸ¥è¯†ã€‚</li>
<li>OISAè¢«å¼•å…¥ä»¥è§£å†³OmniAVSä¸­çš„å¤šåª’ä½“æ¨ç†å’Œç²¾ç»†éŸ³è§†é¢‘å†…å®¹ç†è§£æŒ‘æˆ˜ã€‚</li>
<li>OISAä½¿ç”¨MLLMç†è§£å¤æ‚çº¿ç´¢å¹¶è¿›è¡ŒåŸºäºæ¨ç†çš„åˆ†å‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bbcf28213e48b2144b6b46d91a3a7063.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8d99eba113300301d0ef94dfc40d625.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6026063e5018410149d59129f2900ef2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d881c22a8741b80f9509c7d11fefca47.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51cce565bb12853841128c9af2efa92b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5b65f881cff8c0a46f7d9e8331977a5.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Repair-R1-Better-Test-Before-Repair"><a href="#Repair-R1-Better-Test-Before-Repair" class="headerlink" title="Repair-R1: Better Test Before Repair"></a>Repair-R1: Better Test Before Repair</h2><p><strong>Authors:Haichuan Hu, Xiaochen Xie, Quanjun Zhang</strong></p>
<p>APR (Automated Program Repair) aims to automatically locate program defects, generate patches and validate the repairs. Existing techniques for APR are often combined with LLMs (Large Language Models), which leverages the code-related knowledge of LLMs to improve repair effectiveness. Current LLM-based APR methods typically utilize test cases only during the inference stage, adopting an iterative approach that performs repair first and validates it through test execution afterward. This conventional paradigm neglects two important aspects: the potential contribution of test cases in the training phase, and the possibility of leveraging testing prior to repair. To address this, we propose Repair-R1, which introduces test cases into the modelâ€™s training phase and shifts test generation to precede repair. The model is required to first generate discriminative test cases that can distinguish defective behaviors, and then perform repair based on these tests. This enables the model to better locate defects and understand the underlying causes of defects, thereby improving repair effectiveness. We implement Repair-R1 with three different backbone models, using RL (reinforcement learning) to co-optimize test generation and bug repair. Experimental results on four widely adopted benchmarks demonstrate the superiority of Repair-R1. Specially, compared to vanilla models, Repair-R1 improves repair success rate by 2.68% to 48.29%, test generation success rate by 16.38% to 53.28%, and test coverage by 0.78% to 53.96%. We publish the code and weights at <a target="_blank" rel="noopener" href="https://github.com/Tomsawyerhu/APR-RL">https://github.com/Tomsawyerhu/APR-RL</a> and <a target="_blank" rel="noopener" href="https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step">https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step</a>. </p>
<blockquote>
<p>APRï¼ˆè‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼‰æ—¨åœ¨è‡ªåŠ¨å®šä½ç¨‹åºç¼ºé™·ã€ç”Ÿæˆè¡¥ä¸å¹¶è¿›è¡ŒéªŒè¯ã€‚ç°æœ‰çš„APRæŠ€æœ¯é€šå¸¸ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸ç»“åˆï¼Œåˆ©ç”¨LLMsçš„ä»£ç ç›¸å…³çŸ¥è¯†æ¥æé«˜ä¿®å¤æ•ˆæœã€‚å½“å‰çš„åŸºäºLLMçš„APRæ–¹æ³•ä»…åœ¨æ¨ç†é˜¶æ®µä½¿ç”¨æµ‹è¯•ç”¨ä¾‹ï¼Œé‡‡ç”¨ä¸€ç§å…ˆä¿®å¤åé€šè¿‡æµ‹è¯•æ‰§è¡Œè¿›è¡ŒéªŒè¯çš„è¿­ä»£æ–¹æ³•ã€‚è¿™ç§ä¼ ç»ŸèŒƒå¼å¿½ç•¥äº†ä¸¤ä¸ªé‡è¦æ–¹é¢ï¼šæµ‹è¯•ç”¨ä¾‹åœ¨è®­ç»ƒé˜¶æ®µçš„æ½œåœ¨è´¡çŒ®ï¼Œä»¥åŠæå‰è¿›è¡Œæµ‹è¯•çš„å¯èƒ½æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Repair-R1ï¼Œå®ƒå°†æµ‹è¯•ç”¨ä¾‹å¼•å…¥åˆ°æ¨¡å‹çš„è®­ç»ƒé˜¶æ®µï¼Œå¹¶å°†æµ‹è¯•ç”Ÿæˆè½¬ç§»åˆ°ä¿®å¤ä¹‹å‰ã€‚æ¨¡å‹é¦–å…ˆè¢«è¦æ±‚ç”Ÿæˆèƒ½å¤ŸåŒºåˆ†ç¼ºé™·è¡Œä¸ºçš„è¾¨åˆ«æ€§æµ‹è¯•ç”¨ä¾‹ï¼Œç„¶ååŸºäºè¿™äº›æµ‹è¯•è¿›è¡Œä¿®å¤ã€‚è¿™ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å®šä½ç¼ºé™·å¹¶ç†è§£ç¼ºé™·çš„æ ¹æœ¬åŸå› ï¼Œä»è€Œæé«˜ä¿®å¤æ•ˆæœã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ç§ä¸åŒçš„åŸºç¡€æ¨¡å‹å®ç°äº†Repair-R1ï¼Œå¹¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å…±åŒä¼˜åŒ–æµ‹è¯•ç”Ÿæˆå’Œé”™è¯¯ä¿®å¤ã€‚åœ¨å››ä¸ªå¹¿æ³›é‡‡ç”¨çš„åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯æ˜äº†Repair-R1çš„ä¼˜è¶Šæ€§ã€‚ç‰¹åˆ«æ˜¯ä¸åŸºç¡€æ¨¡å‹ç›¸æ¯”ï¼ŒRepair-R1å°†ä¿®å¤æˆåŠŸç‡æé«˜äº†2.68%è‡³48.29%ï¼Œæµ‹è¯•ç”ŸæˆæˆåŠŸç‡æé«˜äº†16.38%è‡³53.28%ï¼Œæµ‹è¯•è¦†ç›–ç‡æé«˜äº†0.78%è‡³53.96%ã€‚æˆ‘ä»¬å·²å°†ä»£ç å’Œæƒé‡å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/Tomsawyerhu/APR-RL%E5%92%8Chttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-step%E3%80%82">https://github.com/Tomsawyerhu/APR-RLå’Œhttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-stepã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22853v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰æ—¨åœ¨è‡ªåŠ¨å®šä½ç¨‹åºç¼ºé™·ã€ç”Ÿæˆè¡¥ä¸å¹¶è¿›è¡ŒéªŒè¯ã€‚ç°æœ‰æŠ€æœ¯å¸¸ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç»“åˆï¼Œä»¥æé«˜ä¿®å¤æ•ˆæœã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°æ–¹æ³•Repair-R1ï¼Œå°†æµ‹è¯•ç”¨ä¾‹å¼•å…¥æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œå¹¶å°†æµ‹è¯•ç”Ÿæˆç½®äºä¿®å¤ä¹‹å‰ã€‚è¯¥æ–¹æ³•è¦æ±‚æ¨¡å‹é¦–å…ˆç”Ÿæˆèƒ½åŒºåˆ†ç¼ºé™·è¡Œä¸ºçš„é‰´åˆ«æ€§æµ‹è¯•ç”¨ä¾‹ï¼Œç„¶ååŸºäºè¿™äº›æµ‹è¯•è¿›è¡Œä¿®å¤ã€‚è¿™æœ‰åŠ©äºæé«˜æ¨¡å‹å®šä½ç¼ºé™·å’Œäº†è§£ç¼ºé™·æ ¹æœ¬åŸå› çš„èƒ½åŠ›ï¼Œä»è€Œæé«˜ä¿®å¤æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸æ¯”åŸºç¡€æ¨¡å‹ï¼ŒRepair-R1çš„ä¿®å¤æˆåŠŸç‡æé«˜2.68%~48.29%ï¼Œæµ‹è¯•ç”ŸæˆæˆåŠŸç‡æé«˜16.38%~53.28%ï¼Œæµ‹è¯•è¦†ç›–ç‡æé«˜0.78%~53.96%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>APRæŠ€æœ¯ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œç¨‹åºç¼ºé™·ä¿®å¤ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•å¿½ç•¥äº†æµ‹è¯•ç”¨ä¾‹åœ¨è®­ç»ƒé˜¶æ®µçš„æ½œåœ¨è´¡çŒ®å’Œæµ‹è¯•åœ¨ä¿®å¤å‰çš„åˆ©ç”¨ã€‚</li>
<li>Repair-R1å¼•å…¥è®­ç»ƒé˜¶æ®µçš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶å°†æµ‹è¯•ç”Ÿæˆç½®äºä¿®å¤ä¹‹å‰ï¼Œæé«˜ç¼ºé™·å®šä½å’Œä¿®å¤æ•ˆæœã€‚</li>
<li>Repair-R1ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æµ‹è¯•ç”Ÿæˆå’Œé”™è¯¯ä¿®å¤ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸æ¯”åŸºç¡€æ¨¡å‹ï¼ŒRepair-R1åœ¨ä¿®å¤æˆåŠŸç‡ã€æµ‹è¯•ç”ŸæˆæˆåŠŸç‡å’Œæµ‹è¯•è¦†ç›–ç‡æ–¹é¢æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>Repair-R1çš„ä»£ç å’Œæƒé‡å·²å…¬å¼€å‘å¸ƒï¼Œä¾¿äºä»–äººä½¿ç”¨å’Œç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22853">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-683bd39b6eecdf08f599b600d3f61644.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c1db51ccb30aed3d48974466d48e9f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-deaae6efbecbe7651910dd2df72a645d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e92124e9c5de94b3fc96506e4bdba9d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="RLVMR-Reinforcement-Learning-with-Verifiable-Meta-Reasoning-Rewards-for-Robust-Long-Horizon-Agents"><a href="#RLVMR-Reinforcement-Learning-with-Verifiable-Meta-Reasoning-Rewards-for-Robust-Long-Horizon-Agents" class="headerlink" title="RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for   Robust Long-Horizon Agents"></a>RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for   Robust Long-Horizon Agents</h2><p><strong>Authors:Zijing Zhang, Ziyang Chen, Mingxiao Li, Zhaopeng Tu, Xiaolong Li</strong></p>
<p>The development of autonomous agents for complex, long-horizon tasks is a central goal in AI. However, dominant training paradigms face a critical limitation: reinforcement learning (RL) methods that optimize solely for final task success often reinforce flawed or inefficient reasoning paths, a problem we term inefficient exploration. This leads to agents that are brittle and fail to generalize, as they learn to find solutions without learning how to reason coherently. To address this, we introduce RLVMR, a novel framework that integrates dense, process-level supervision into end-to-end RL by rewarding verifiable, meta-reasoning behaviors. RLVMR equips an agent to explicitly tag its cognitive steps, such as planning, exploration, and reflection, and provides programmatic, rule-based rewards for actions that contribute to effective problem-solving. These process-centric rewards are combined with the final outcome signal and optimized using a critic-free policy gradient method. On the challenging ALFWorld and ScienceWorld benchmarks, RLVMR achieves new state-of-the-art results, with our 7B model reaching an 83.6% success rate on the most difficult unseen task split. Our analysis confirms these gains stem from improved reasoning quality, including significant reductions in redundant actions and enhanced error recovery, leading to more robust, efficient, and interpretable agents. </p>
<blockquote>
<p>é’ˆå¯¹å¤æ‚ã€é•¿æœŸä»»åŠ¡å¼€å‘è‡ªä¸»æ™ºèƒ½ä½“æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒç›®æ ‡ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œä¸»æµçš„è®­ç»ƒæ¨¡å¼é¢ä¸´ä¸€ä¸ªå…³é”®å±€é™ï¼šä»…é’ˆå¯¹æœ€ç»ˆä»»åŠ¡æˆåŠŸçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•å¾€å¾€ä¼šå¼ºåŒ–æœ‰ç¼ºé™·æˆ–ä½æ•ˆçš„æ¨ç†è·¯å¾„ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºä½æ•ˆæ¢ç´¢é—®é¢˜ã€‚è¿™å¯¼è‡´æ™ºèƒ½ä½“å˜å¾—è„†å¼±ä¸”æ— æ³•æ¨å¹¿ï¼Œå› ä¸ºå®ƒä»¬å­¦ä¼šäº†å¦‚ä½•æ‰¾åˆ°è§£å†³æ–¹æ¡ˆï¼Œä½†æ²¡æœ‰å­¦ä¼šå¦‚ä½•è¿›è¡Œè¿è´¯çš„æ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†RLVMRï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œé€šè¿‡å¥–åŠ±å¯éªŒè¯çš„å…ƒæ¨ç†è¡Œä¸ºï¼Œå°†å¯†é›†çš„è¿‡ç¨‹çº§ç›‘ç£é›†æˆåˆ°ç«¯åˆ°ç«¯çš„å¼ºåŒ–å­¦ä¹ ä¸­ã€‚RLVMRä½¿æ™ºèƒ½ä½“èƒ½æ˜ç¡®æ ‡è®°å…¶è®¤çŸ¥æ­¥éª¤ï¼Œå¦‚è§„åˆ’ã€æ¢ç´¢å’Œåæ€ï¼Œå¹¶ä¸ºæœ‰åŠ©äºæœ‰æ•ˆè§£å†³é—®é¢˜çš„åŠ¨ä½œæä¾›åŸºäºç¨‹åºã€è§„åˆ™å¥–åŠ±ã€‚è¿™äº›ä»¥è¿‡ç¨‹ä¸ºä¸­å¿ƒçš„å¥–åŠ±ä¸æœ€ç»ˆç»“æœçš„ä¿¡å·ç›¸ç»“åˆï¼Œå¹¶ä½¿ç”¨æ— æ‰¹åˆ¤å®¶çš„æ”¿ç­–æ¢¯åº¦æ–¹æ³•è¿›è¡Œä¼˜åŒ–ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ALFWorldå’ŒScienceWorldåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRLVMRå–å¾—äº†æœ€æ–°çš„æœ€å…ˆè¿›çš„æˆæœï¼Œæˆ‘ä»¬çš„7Bæ¨¡å‹åœ¨æœ€å›°éš¾çš„æ— ä»»åŠ¡åˆ†å‰²ä¸­è¾¾åˆ°äº†83.6%çš„æˆåŠŸç‡ã€‚æˆ‘ä»¬çš„åˆ†æè¯å®ï¼Œè¿™äº›æ”¶ç›Šæ¥è‡ªäºæ¨ç†è´¨é‡çš„æ”¹è¿›ï¼ŒåŒ…æ‹¬åŠ¨ä½œå†—ä½™çš„æ˜¾è‘—å‡å°‘å’Œé”™è¯¯æ¢å¤çš„å¢å¼ºï¼Œä»è€Œå¸¦æ¥æ›´ç¨³å¥ã€é«˜æ•ˆå’Œå¯è§£é‡Šçš„æ™ºèƒ½ä½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22844v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æ¢è®¨äº†åœ¨AIé¢†åŸŸä¸­ï¼Œé’ˆå¯¹å¤æ‚é•¿æœŸä»»åŠ¡å¼€å‘è‡ªä¸»ä»£ç†çš„æ ¸å¿ƒç›®æ ‡ã€‚ç„¶è€Œï¼Œä¸»æµçš„è®­ç»ƒæ¨¡å¼é¢ä¸´ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šä»…é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä¼˜åŒ–æœ€ç»ˆä»»åŠ¡æˆåŠŸä¼šå¯¼è‡´ä»£ç†åœ¨é¢å¯¹é—®é¢˜è§£å†³æ–¹æ¡ˆæ—¶å¿½ç•¥åˆç†æ¨ç†è·¯å¾„çš„è®­ç»ƒã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶RLVMRï¼Œé€šè¿‡æ•´åˆå¯†é›†çš„è¿‡ç¨‹çº§ç›‘ç£ï¼Œå¥–åŠ±å¯éªŒè¯çš„å…ƒæ¨ç†è¡Œä¸ºæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚RLVMRä½¿ä»£ç†èƒ½å¤Ÿæ˜ç¡®æ ‡æ³¨å…¶è®¤çŸ¥æ­¥éª¤ï¼Œå¦‚è§„åˆ’ã€æ¢ç´¢å’Œåæ€ï¼Œå¹¶ä¸ºæœ‰åŠ©äºæœ‰æ•ˆè§£å†³é—®é¢˜çš„è¡ŒåŠ¨æä¾›ç¨‹åºåŒ–ã€åŸºäºè§„åˆ™çš„å¥–åŠ±ã€‚è¿™äº›è¿‡ç¨‹ä¸ºä¸­å¿ƒçš„å¥–åŠ±ä¸æœ€ç»ˆç»“æœçš„ä¿¡å·ç›¸ç»“åˆï¼Œä½¿ç”¨æ— æ‰¹åˆ¤å®¶çš„æ”¿ç­–æ¢¯åº¦æ–¹æ³•è¿›è¡Œä¼˜åŒ–ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ALFWorldå’ŒScienceWorldåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRLVMRå–å¾—äº†æœ€æ–°çŠ¶æ€çš„æŠ€æœ¯æˆæœï¼Œå…¶ä¸­æˆ‘ä»¬çš„7Bæ¨¡å‹åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„æœªè§ä»»åŠ¡åˆ†å‰²ä¸­è¾¾åˆ°äº†83.6%çš„æˆåŠŸç‡ã€‚åˆ†æè¯å®ï¼Œè¿™äº›æ”¶ç›Šæºäºæ¨ç†è´¨é‡çš„æé«˜ï¼ŒåŒ…æ‹¬æ˜¾è‘—å‡å°‘å†—ä½™åŠ¨ä½œå’Œå¢å¼ºé”™è¯¯æ¢å¤èƒ½åŠ›ï¼Œä»è€Œä½¿ä»£ç†æ›´åŠ ç¨³å¥ã€é«˜æ•ˆå’Œå¯è§£é‡Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è‡ªä¸»ä»£ç†åœ¨å¤æ‚é•¿æœŸä»»åŠ¡ä¸Šçš„å‘å±•æ˜¯AIé¢†åŸŸçš„é‡è¦ç›®æ ‡ã€‚</li>
<li>å½“å‰ä¸»å¯¼çš„è®­ç»ƒæ¨¡å¼é¢ä¸´å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•çš„é—®é¢˜ï¼Œå³å•çº¯è¿½æ±‚æœ€ç»ˆä»»åŠ¡æˆåŠŸå¯èƒ½ä¼šå¯¼è‡´ä»£ç†å¿½è§†åˆç†çš„æ¨ç†è·¯å¾„ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶RLVMRï¼Œé€šè¿‡æ•´åˆè¿‡ç¨‹çº§ç›‘ç£æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¥–åŠ±é‚£äº›æœ‰åŠ©äºæœ‰æ•ˆè§£å†³é—®é¢˜çš„å¯éªŒè¯çš„å…ƒæ¨ç†è¡Œä¸ºã€‚</li>
<li>RLVMRå…è®¸ä»£ç†æ˜ç¡®æ ‡æ³¨å…¶è®¤çŸ¥æ­¥éª¤ï¼Œå¦‚è§„åˆ’ã€æ¢ç´¢å’Œåæ€ã€‚</li>
<li>ç»“åˆè¿‡ç¨‹ä¸ºä¸­å¿ƒçš„å¥–åŠ±å’Œæœ€ç»ˆç»“æœçš„ä¿¡å·ï¼Œä½¿ç”¨æ— æ‰¹åˆ¤å®¶çš„æ”¿ç­–æ¢¯åº¦æ–¹æ³•è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>åœ¨ALFWorldå’ŒScienceWorldåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRLVMRå–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22844">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-801a08738bcbf86af67d0d589deeba23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d37345fd9a707282bac30190a81dbab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42053181491f547e5f1b9c2bf0e03f7a.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MoCHA-Advanced-Vision-Language-Reasoning-with-MoE-Connector-and-Hierarchical-Group-Attention"><a href="#MoCHA-Advanced-Vision-Language-Reasoning-with-MoE-Connector-and-Hierarchical-Group-Attention" class="headerlink" title="MoCHA: Advanced Vision-Language Reasoning with MoE Connector and   Hierarchical Group Attention"></a>MoCHA: Advanced Vision-Language Reasoning with MoE Connector and   Hierarchical Group Attention</h2><p><strong>Authors:Yuqi Pang, Bowen Yang, Yun Cao, Fan Rong, Xiaoyu Li, Chen He</strong></p>
<p>Vision large language models (VLLMs) are focusing primarily on handling complex and fine-grained visual information by incorporating advanced vision encoders and scaling up visual models. However, these approaches face high training and inference costs, as well as challenges in extracting visual details, effectively bridging across modalities. In this work, we propose a novel visual framework, MoCHA, to address these issues. Our framework integrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to extract complementary visual features and is equipped with a sparse Mixture of Experts Connectors (MoECs) module to dynamically select experts tailored to different visual dimensions. To mitigate redundant or insufficient use of the visual information encoded by the MoECs module, we further design a Hierarchical Group Attention (HGA) with intra- and inter-group operations and an adaptive gating strategy for encoded visual features. We train MoCHA on two mainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance across various benchmarks. Notably, MoCHA outperforms state-of-the-art open-weight models on various tasks. For example, compared to CuMo (Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate hallucination by showing improvements of 3.25% in POPE and to follow visual instructions by raising 153 points on MME. Finally, ablation studies further confirm the effectiveness and robustness of the proposed MoECs and HGA in improving the overall performance of MoCHA. </p>
<blockquote>
<p>è§†è§‰å¤§è¯­è¨€æ¨¡å‹ï¼ˆVLLMsï¼‰ä¸»è¦è‡´åŠ›äºé€šè¿‡é›†æˆå…ˆè¿›çš„è§†è§‰ç¼–ç å™¨å’Œæ‰©å¤§è§†è§‰æ¨¡å‹æ¥å¤„ç†å¤æ‚å’Œç²¾ç»†çš„è§†è§‰ä¿¡æ¯ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é¢ä¸´ç€é«˜è®­ç»ƒå’Œæ¨ç†æˆæœ¬ï¼Œä»¥åŠæå–è§†è§‰ç»†èŠ‚å’Œæœ‰æ•ˆè·¨æ¨¡æ€æ²Ÿé€šçš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è§†è§‰æ¡†æ¶MoCHAæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬çš„æ¡†æ¶é›†æˆäº†å››ç§è§†è§‰ä¸»å¹²ï¼ˆå³CLIPã€SigLIPã€DINOv2å’ŒConvNeXtï¼‰æ¥æå–äº’è¡¥çš„è§†è§‰ç‰¹å¾ï¼Œå¹¶é…å¤‡äº†ä¸€ä¸ªç¨€ç–çš„ä¸“å®¶è¿æ¥å™¨æ··åˆï¼ˆMoECsï¼‰æ¨¡å—ï¼Œä»¥åŠ¨æ€é€‰æ‹©é’ˆå¯¹ä¸åŒè§†è§‰ç»´åº¦é‡èº«å®šåˆ¶çš„ä¸“å®¶ã€‚ä¸ºäº†å‡è½»MoECsæ¨¡å—å¯¹è§†è§‰ä¿¡æ¯çš„å†—ä½™æˆ–ä¸è¶³çš„ä½¿ç”¨ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ç§åˆ†å±‚ç»„æ³¨æ„åŠ›ï¼ˆHGAï¼‰ï¼Œå®ƒå…·æœ‰ç»„å†…å’Œç»„é—´æ“ä½œä»¥åŠè‡ªé€‚åº”é—¨æ§ç­–ç•¥ï¼Œç”¨äºç¼–ç çš„è§†è§‰ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä¸»æµçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚Phi2-2.7Bå’ŒVicuna-7Bï¼‰ä¸Šè®­ç»ƒMoCHAï¼Œå¹¶åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šè¯„ä¼°å…¶æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMoCHAåœ¨å„é¡¹ä»»åŠ¡ä¸Šçš„è¡¨ç°è¶…è¿‡äº†æœ€æ–°çš„å¼€æºæ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œä¸CuMoï¼ˆMistral-7Bï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„MoCHAï¼ˆPhi-Phiå¤§æ¨¡å‹é…ç½®ã€‚åœ¨ POPE ä¸­æå‡äº†æ”¹å–„3.25%ï¼ŒåŒæ—¶åœ¨éµå¾ªè§†è§‰æŒ‡ä»¤æ–¹é¢æå‡äº†è¾¾åˆ°æœ€é«˜çš„æå‡äº†æé«˜äº†å¾ˆå¥½çš„è§†è§‰è¡¨ç°ä¸ç¨³å®šæ€§ï¼ æ”¹è¿›ç¨‹åº¦è¾ƒé«˜çš„å¤§æ¨¡å‹ä¸å®éªŒã€‚å› æ­¤è¯·é€šè¿‡æµè§ˆä¸Šä¸‹æ–‡æ›´æ·±å…¥åœ°äº†è§£å®ƒçš„è¿è¡Œæ–¹å¼å’Œä¸åŒå®ç°ç‚¹åœ¨å“ªä½¿é˜…è¯»ä¸å†é™·å…¥æ–­å¥â€œå¼è·å¾—äº†â€œæ— é™æ¬¡çš„æŒå£°â€™ï¼Œå¾—ç›Šäºæµç•…çš„ä¸Šä¸‹è¡Œçš„ä¸­é—´å¢ä»¥æ•´ä¸ªè®ºç‚¹æ„æˆçš„æ ‡é¢˜æ·»åŠ ç”¨äºå¼€å±•é€‚å½“çš„èŠ‚å¥ ã€‚ ç»è¿‡ç ”ç©¶è¡¨æ˜éœ€è¦ä¸€æ¬¡æ”¹å†™å…¶ä¸­çš„æ ¼å¼å±•å¼€å¤§è§„æ¨¡çš„å‰æœŸè¯­æ–™ä»¥åŠè¯è¯­è·å¾—æ›´å¤šåœ°å‡è®¾ä¸å…¶ä»–å¤„ç†æ–¹æ³•ä¿æŒä¸€è‡´åœ°ä½¿ç”¨ç†å¤–éƒ¨çš„è§‚å¯Ÿæ¥å¯¹å®Œæ•´çš„æ€æƒ³è·¯çº¿æ›´å¥½çš„è§„èŒƒå›å¤§åœ°åŠ©äºä»»ä½•æ¡ä»¶ä¸‹çš„ç¤¾åŒºçš„æœ€ç»ˆåˆä½œå¾—ä»¥ç¨³æ­¥ä¸Šå‡ã€‚ï¼‰ä¸åŒè¡Œçš„æ”¹è¿›æˆæœè¿›è¡Œæ¯”è¾ƒæœ€ç»ˆæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ç›¸å¯¹é‡è¦çš„æŠ€æœ¯ä»·å€¼è€Œä¸”èåˆçš„çŸ¥è¯†å¯ä»¥é€šè¿‡æ§åˆ¶ç¬¦å·ä¸ºèµ„æºçš„å·¥ä½œé¢å‘æ¨åŠ¨é€šè¿‡å»ºè®¾æ€§çš„åé¦ˆæ¥æ”¹è¿›æˆ‘ä»¬çš„æ¨¡å‹ã€‚æœ€åï¼Œæ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†æ‰€æå‡ºçš„MoECså’ŒHGAåœ¨æé«˜MoCHAæ•´ä½“æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22805v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†è§†è§‰ä¿¡æ¯æ—¶é¢ä¸´è®­ç»ƒæˆæœ¬é«˜ã€æ¨ç†æˆæœ¬é«˜ä»¥åŠæå–è§†è§‰ç»†èŠ‚ç­‰æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°é¢–çš„æ¡†æ¶MoCHAæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚MoCHAèåˆäº†å››ä¸ªä¸åŒçš„è§†è§‰éª¨æ¶æ¥æå–äº’è¡¥çš„è§†è§‰ç‰¹å¾ï¼Œå¹¶å…·æœ‰åŠ¨æ€é€‰æ‹©è§†è§‰ç»´åº¦ç›¸å…³ä¸“å®¶çš„ç¨€ç–æ··åˆä¸“å®¶è¿æ¥å™¨ï¼ˆMoECsï¼‰æ¨¡å—ã€‚ä¸ºäº†ç¼“è§£MoECsæ¨¡å—ä¸­è§†è§‰ä¿¡æ¯çš„å†—ä½™æˆ–ä¸è¶³ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†åˆ†å±‚ç»„æ³¨æ„åŠ›ï¼ˆHGAï¼‰å’Œè‡ªé€‚åº”é—¨æ§ç­–ç•¥ã€‚MoCHAåœ¨ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜è¶Šã€‚ä¾‹å¦‚ï¼Œåœ¨åº”å¯¹å¹»æƒ³é—®é¢˜æ–¹é¢ï¼Œç›¸è¾ƒäºCuMoï¼ˆMistral-7Bï¼‰ï¼Œæˆ‘ä»¬çš„MoCHAï¼ˆPhi2-2.7Bï¼‰æœ‰æ˜¾è‘—æ”¹å–„ï¼›è€Œåœ¨éµå¾ªè§†è§‰æŒ‡ä»¤æ–¹é¢ï¼Œè¯„åˆ†ä¸Šå‡äº†é«˜è¾¾153ç‚¹ã€‚å¯¹MoECså’ŒHGAçš„ç ”ç©¶è¯æ˜äº†å®ƒä»¬çš„ä¼˜åŒ–èƒ½åŠ›åŠå…¶å¯¹MoCHAæ€»ä½“æ€§èƒ½çš„ä¿ƒè¿›ä½œç”¨ã€‚æ€»ä½“æ¥è¯´ï¼Œæœ¬ç ”ç©¶æ­ç¤ºäº†ä¸€ä¸ªé«˜æ•ˆä¸”å¼ºå¤§çš„è§†è§‰å¤„ç†æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>VLLMsé¢ä¸´å¤„ç†å¤æ‚è§†è§‰ä¿¡æ¯çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é«˜è®­ç»ƒå’Œæ¨ç†æˆæœ¬ä»¥åŠæå–è§†è§‰ç»†èŠ‚çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶MoCHAæ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæ•´åˆäº†å››ä¸ªä¸åŒçš„è§†è§‰éª¨æ¶å¹¶å¼•å…¥äº†MoECsæ¨¡å—å’ŒHGAæœºåˆ¶ã€‚</li>
<li>MoCHAåœ¨ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚ç›¸è¾ƒäºå…¶ä»–æ¨¡å‹ï¼ŒMoCHAåœ¨å¤„ç†è§†è§‰ä¿¡æ¯å’Œéµå¾ªæŒ‡ä»¤æ–¹é¢æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22805">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-217a6666627e343da63be99ea3f88e40.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-04836f700c1ee951353d47b350ee81cb.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c09551fab68b5c55397581fadede21a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a646f2a0da87acc7a7e186dc5af6a54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9cf1ca857d059969b73f4b2616b4c1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e20689a4908487047b20e027705c258.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="G-Core-A-Simple-Scalable-and-Balanced-RLHF-Trainer"><a href="#G-Core-A-Simple-Scalable-and-Balanced-RLHF-Trainer" class="headerlink" title="G-Core: A Simple, Scalable and Balanced RLHF Trainer"></a>G-Core: A Simple, Scalable and Balanced RLHF Trainer</h2><p><strong>Authors:Junyu Wu, Weiming Chang, Xiaotao Liu, Guanyou He, Haoqiang Hong, Boqi Liu, Hongtao Tian, Tao Yang, Yunsheng Shi, Feng Lin, Ting Yao</strong></p>
<p>Reinforcement Learning from Human Feedback (RLHF) has become an increasingly popular paradigm for training large language models (LLMs) and diffusion models. While existing RLHF training systems have enabled significant progress, they often face challenges in scaling to multi-modal and diffusion workflows and adapting to dynamic workloads. In particular, current approaches may encounter limitations in controller scalability, flexible resource placement, and efficient orchestration when handling complex RLHF pipelines, especially in scenarios involving dynamic sampling or generative reward modeling. In this paper, we present \textbf{G-Core}, a simple, scalable, and balanced RLHF training framework designed to address these challenges. G-Core introduces a parallel controller programming model, enabling flexible and efficient orchestration of complex RLHF workflows without the bottlenecks of a single centralized controller. Furthermore, we propose a dynamic placement schema that adaptively partitions resources and schedules workloads, significantly reducing hardware idle time and improving utilization, even under highly variable training conditions. G-Core has successfully trained models that support WeChat product features serving a large-scale user base, demonstrating its effectiveness and robustness in real-world scenarios. Our results show that G-Core advances the state of the art in RLHF training, providing a solid foundation for future research and deployment of large-scale, human-aligned models. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²ç»æˆä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ‰©æ•£æ¨¡å‹è¶Šæ¥è¶Šæµè¡Œçš„èŒƒå¼ã€‚å°½ç®¡ç°æœ‰çš„RLHFè®­ç»ƒç³»ç»Ÿå·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å®ƒä»¬ç»å¸¸é¢ä¸´æ‰©å±•åˆ°å¤šæ¨¡æ€å’Œæ‰©æ•£å·¥ä½œæµç¨‹ä»¥åŠé€‚åº”åŠ¨æ€å·¥ä½œè´Ÿè½½çš„æŒ‘æˆ˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨å¤„ç†å¤æ‚çš„RLHFç®¡é“æ—¶ï¼Œå½“å‰çš„æ–¹æ³•å¯èƒ½ä¼šé‡åˆ°æ§åˆ¶å™¨å¯æ‰©å±•æ€§ã€çµæ´»èµ„æºæ”¾ç½®å’Œé«˜æ•ˆååŒæ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠåŠ¨æ€é‡‡æ ·æˆ–ç”Ÿæˆå¥–åŠ±å»ºæ¨¡çš„åœºæ™¯ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22789v2">PDF</a> I havenâ€™t received company approval yet, and I uploaded it by mistake</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ­£åœ¨æˆä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„ä¸»æµèŒƒå¼ã€‚ç°æœ‰RLHFè®­ç»ƒç³»ç»Ÿè™½ç„¶å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨å¤šæ¨¡æ€å’Œæ‰©æ•£å·¥ä½œæµç¨‹ä»¥åŠåŠ¨æ€å·¥ä½œè´Ÿè½½çš„æ‰©å±•é€‚åº”æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚G-Coreæ˜¯ä¸€ç§ç®€æ´ã€å¯æ‰©å±•å’Œå¹³è¡¡çš„RLHFè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥å¹¶è¡Œæ§åˆ¶å™¨ç¼–ç¨‹æ¨¡å‹å’ŒåŠ¨æ€èµ„æºè°ƒåº¦ç­–ç•¥æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚è¯¥æ¡†æ¶æˆåŠŸè®­ç»ƒäº†æ”¯æŒå¾®ä¿¡äº§å“ç‰¹æ€§çš„å¤§è§„æ¨¡ç”¨æˆ·æ¨¡å‹ï¼Œå±•ç¤ºäº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚æ­¤æ¡†æ¶ä¸ºæœªæ¥å¤§è§„æ¨¡ä¸äººç±»å¯¹é½æ¨¡å‹çš„éƒ¨ç½²å’Œç ”ç©¶æä¾›äº†åšå®çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>RLHFå·²æˆä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„ä¸»æµæ–¹æ³•ã€‚</li>
<li>å½“å‰RLHFè®­ç»ƒç³»ç»Ÿåœ¨å¤„ç†å¤æ‚ç®¡é“æ—¶é¢ä¸´æ§åˆ¶å™¨æ‰©å±•æ€§ã€èµ„æºçµæ´»é…ç½®å’Œå·¥ä½œæ•ˆç‡æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>G-Coreæ¡†æ¶é€šè¿‡å¼•å…¥å¹¶è¡Œæ§åˆ¶å™¨ç¼–ç¨‹æ¨¡å‹å’ŒåŠ¨æ€èµ„æºè°ƒåº¦ç­–ç•¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>G-Coreæ¡†æ¶å…·å¤‡çµæ´»æ€§å’Œæ•ˆç‡ï¼Œå¯æ”¯æŒå¤æ‚RLHFå·¥ä½œæµç¨‹ï¼Œæ— éœ€å•ä¸€é›†ä¸­æ§åˆ¶å™¨åˆ¶çº¦ã€‚</li>
<li>åŠ¨æ€èµ„æºåˆ†é…ç­–ç•¥å¯è‡ªé€‚åº”åˆ†é…èµ„æºå’Œè°ƒåº¦å·¥ä½œè´Ÿè½½ï¼Œå‡å°‘ç¡¬ä»¶ç©ºé—²æ—¶é—´ï¼Œæé«˜åˆ©ç”¨ç‡ï¼Œé€‚åº”é«˜åº¦å˜åŒ–çš„è®­ç»ƒæ¡ä»¶ã€‚</li>
<li>G-CoreæˆåŠŸè®­ç»ƒäº†æ”¯æŒå¾®ä¿¡äº§å“çš„å¤§è§„æ¨¡ç”¨æˆ·æ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨ç°å®åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9627bbbea52d809193d207a08d77ec02.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8c781287354d937c2a3e5b1150a1f1a.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-aec299c9105c1951837693714ac70ade.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  SeqAffordSplat Scene-level Sequential Affordance Reasoning on 3D   Gaussian Splatting
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e9efe258cccd9a8d695461495ee77cce.jpg" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  Who is a Better Talker Subjective and Objective Quality Assessment for   AI-Generated Talking Heads
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27927k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
