<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  IN45023 Neural Network Design Patterns in Computer Vision Seminar   Report, Summer 2025">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-a1e85749bc723dbdb5f43298501e3667.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    86 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-01-æ›´æ–°"><a href="#2025-08-01-æ›´æ–°" class="headerlink" title="2025-08-01 æ›´æ–°"></a>2025-08-01 æ›´æ–°</h1><h2 id="IN45023-Neural-Network-Design-Patterns-in-Computer-Vision-Seminar-Report-Summer-2025"><a href="#IN45023-Neural-Network-Design-Patterns-in-Computer-Vision-Seminar-Report-Summer-2025" class="headerlink" title="IN45023 Neural Network Design Patterns in Computer Vision Seminar   Report, Summer 2025"></a>IN45023 Neural Network Design Patterns in Computer Vision Seminar   Report, Summer 2025</h2><p><strong>Authors:Radu-Andrei Bourceanu, Neil De La Fuente, Jan Grimm, Andrei Jardan, Andriy Manucharyan, Cornelius Weiss, Roman Pflugfelder</strong></p>
<p>This report analyzes the evolution of key design patterns in computer vision by examining six influential papers. The analy- sis begins with foundational architectures for image recognition. We review ResNet, which introduced residual connections to overcome the vanishing gradient problem and enable effective training of significantly deeper convolutional networks. Subsequently, we examine the Vision Transformer (ViT), which established a new paradigm by applying the Transformer ar- chitecture to sequences of image patches, demonstrating the efficacy of attention-based models for large-scale image recogni- tion. Building on these visual representation backbones, we investigate generative models. Generative Adversarial Networks (GANs) are analyzed for their novel adversarial training process, which challenges a generator against a discriminator to learn complex data distributions. Then, Latent Diffusion Models (LDMs) are covered, which improve upon prior generative methods by performing a sequential denoising process in a perceptually compressed latent space. LDMs achieve high-fidelity synthesis with greater computational efficiency, representing the current state-of-the-art for image generation. Finally, we explore self-supervised learning techniques that reduce dependency on labeled data. DINO is a self-distillation framework in which a student network learns to match the output of a momentum-updated teacher, yielding features with strong k-NN classification performance. We conclude with Masked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design to reconstruct heavily masked inputs, providing a highly scalable and effective method for pre-training large-scale vision models. </p>
<blockquote>
<p>æœ¬æŠ¥å‘Šé€šè¿‡åˆ†æå…­ç¯‡æœ‰å½±å“åŠ›çš„è®ºæ–‡ï¼Œåˆ†æäº†è®¡ç®—æœºè§†è§‰ä¸­å…³é”®è®¾è®¡æ¨¡å¼çš„æ¼”å˜ã€‚åˆ†æä»å›¾åƒè¯†åˆ«çš„åŸºæœ¬æ¶æ„å¼€å§‹ã€‚æˆ‘ä»¬å›é¡¾äº†ResNetï¼Œå®ƒå¼•å…¥äº†æ®‹å·®è¿æ¥ï¼Œå…‹æœäº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå®ç°äº†å¯¹æ›´æ·±å·ç§¯ç½‘ç»œçš„æœ‰æ•ˆè®­ç»ƒã€‚ä¹‹åï¼Œæˆ‘ä»¬ç ”ç©¶äº†è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œå®ƒå°†è½¬æ¢å™¨æ¶æ„åº”ç”¨äºå›¾åƒè¡¥ä¸åºåˆ—ï¼Œå»ºç«‹äº†æ–°çš„èŒƒå¼ï¼Œè¯æ˜äº†æ³¨æ„åŠ›æ¨¡å‹åœ¨å¤§è§„æ¨¡å›¾åƒè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚åœ¨è¿™äº›è§†è§‰è¡¨ç¤ºéª¨å¹²çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”Ÿæˆæ¨¡å‹ã€‚åˆ†æäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ–°å‹å¯¹æŠ—è®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹ä½¿ç”Ÿæˆå™¨ä¸é‰´åˆ«å™¨ç›¸äº’å­¦ä¹ å¤æ‚çš„æ•°æ®åˆ†å¸ƒã€‚ç„¶åä»‹ç»äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ï¼Œé€šè¿‡æ„ŸçŸ¥å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­çš„è¿ç»­å»å™ªè¿‡ç¨‹æ”¹è¿›äº†å…ˆå‰çš„ç”Ÿæˆæ–¹æ³•ã€‚LDMså®ç°äº†é«˜ä¿çœŸåˆæˆï¼Œå…·æœ‰æ›´é«˜çš„è®¡ç®—æ•ˆç‡ï¼Œä»£è¡¨äº†å½“å‰å›¾åƒç”Ÿæˆçš„æœ€æ–°æŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢ç´¢äº†å‡å°‘å¯¹æ¯”æ ‡ç­¾æ•°æ®ä¾èµ–æ€§çš„è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ã€‚DINOæ˜¯ä¸€ç§è‡ªè’¸é¦æ¡†æ¶ï¼Œå­¦ç”Ÿç½‘ç»œå­¦ä¹ åŒ¹é…åŠ¨é‡æ›´æ–°çš„æ•™å¸ˆè¾“å‡ºï¼Œäº§ç”Ÿå…·æœ‰å¼ºå¤§k-NNåˆ†ç±»æ€§èƒ½çš„ç‰¹å¾ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å¾—å‡ºç»“è®ºï¼ŒMasked Autoencodersï¼ˆMAEï¼‰åˆ©ç”¨å¯¹ç§°çš„ç¼–ç å™¨-è§£ç å™¨è®¾è®¡æ¥é‡å»ºé«˜åº¦é®ç½©çš„è¾“å…¥ï¼Œæä¾›äº†ä¸€ç§é«˜åº¦å¯æ‰©å±•å’Œæœ‰æ•ˆçš„é¢„è®­ç»ƒå¤§è§„æ¨¡è§†è§‰æ¨¡å‹çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23357v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡åˆ†æäº†è®¡ç®—æœºè§†è§‰ä¸­å…³é”®è®¾è®¡æ¨¡å¼çš„æ¼”å˜ï¼Œé€šè¿‡è€ƒå¯Ÿå…­ç¯‡æœ‰å½±å“åŠ›çš„è®ºæ–‡ï¼Œä»å›¾åƒè¯†åˆ«çš„åŸºæœ¬æ¶æ„å¼€å§‹ï¼Œä»‹ç»äº†ResNetã€Vision Transformerï¼ˆViTï¼‰ã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ã€è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯å’ŒMAEç­‰å…³é”®æŠ€æœ¯å’Œæ–¹æ³•çš„å‘å±•å’Œåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¡ç®—æœºè§†è§‰ä¸­çš„è®¾è®¡æ¨¡å¼ä¸æ–­æ¼”å˜ï¼Œä»å›¾åƒè¯†åˆ«çš„åŸºæœ¬æ¶æ„å‘å±•åˆ°æ›´å¤æ‚çš„æ¨¡å‹å’ŒæŠ€æœ¯ã€‚</li>
<li>ResNeté€šè¿‡å¼•å…¥æ®‹å·®è¿æ¥å…‹æœäº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œæœ‰æ•ˆåœ°è®­ç»ƒäº†æ›´æ·±çš„å·ç§¯ç¥ç»ç½‘ç»œã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰å°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒè¡¥ä¸åºåˆ—ï¼Œè¯æ˜äº†æ³¨æ„åŠ›æ¨¡å‹åœ¨å¤§è§„æ¨¡å›¾åƒè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰é€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒè¿‡ç¨‹å­¦ä¹ å¤æ‚çš„æ•°æ®åˆ†å¸ƒï¼ŒæŒ‘æˆ˜ç”Ÿæˆå™¨ä¸é‰´åˆ«å™¨çš„å¯¹æŠ—ã€‚</li>
<li>æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰åœ¨æ„ŸçŸ¥å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œåºè´¯å»å™ªè¿‡ç¨‹ï¼Œå®ç°äº†é«˜ä¿çœŸåˆæˆå’Œæ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚</li>
<li>è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯å‡å°‘äº†å¯¹æ¯”å­¦ä¹ çš„ä¾èµ–æ€§ï¼Œé‡‡ç”¨è‡ªè’¸é¦ç­‰æ¡†æ¶é™ä½å¯¹æ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23357">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-f55b25dd93b83437b5e43038bd3e0a8f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb308890c31345f2aab07dd14fc1774c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43c7b7523d1996c598c4c86034ca318b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-498ae1a3c68c14971d462838ba0e1fa5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a42b24e9ff5111ddf2f9f617b925344.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models"><a href="#The-Cow-of-Rembrandt-Analyzing-Artistic-Prompt-Interpretation-in-Text-to-Image-Models" class="headerlink" title="The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in   Text-to-Image Models"></a>The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in   Text-to-Image Models</h2><p><strong>Authors:Alfio Ferrara, Sergio Picascia, Elisabetta Rocchetti</strong></p>
<p>Text-to-image diffusion models have demonstrated remarkable capabilities in generating artistic content by learning from billions of images, including popular artworks. However, the fundamental question of how these models internally represent concepts, such as content and style in paintings, remains unexplored. Traditional computer vision assumes content and style are orthogonal, but diffusion models receive no explicit guidance about this distinction during training. In this work, we investigate how transformer-based text-to-image diffusion models encode content and style concepts when generating artworks. We leverage cross-attention heatmaps to attribute pixels in generated images to specific prompt tokens, enabling us to isolate image regions influenced by content-describing versus style-describing tokens. Our findings reveal that diffusion models demonstrate varying degrees of content-style separation depending on the specific artistic prompt and style requested. In many cases, content tokens primarily influence object-related regions while style tokens affect background and texture areas, suggesting an emergent understanding of the content-style distinction. These insights contribute to our understanding of how large-scale generative models internally represent complex artistic concepts without explicit supervision. We share the code and dataset, together with an exploratory tool for visualizing attention maps at <a target="_blank" rel="noopener" href="https://github.com/umilISLab/artistic-prompt-interpretation">https://github.com/umilISLab/artistic-prompt-interpretation</a>. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹å·²ç»æ˜¾ç¤ºå‡ºä»æ•°åäº¿å¼ å›¾åƒï¼ˆåŒ…æ‹¬æµè¡Œè‰ºæœ¯ä½œå“ï¼‰ä¸­å­¦ä¹ å¹¶ç”Ÿæˆè‰ºæœ¯å†…å®¹çš„æ˜¾è‘—èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¦‚ä½•åœ¨å†…éƒ¨ä»£è¡¨ç»˜ç”»ä¸­çš„å†…å®¹å’Œé£æ ¼ç­‰æ¦‚å¿µçš„åŸºæœ¬é—®é¢˜ä»æœªè¢«æ¢ç´¢ã€‚ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰å‡è®¾å†…å®¹å’Œé£æ ¼æ˜¯æ­£äº¤çš„ï¼Œä½†æ‰©æ•£æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰å…³äºè¿™ç§åŒºåˆ«çš„æ˜ç¡®æŒ‡å¯¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†åŸºäºå˜å‹å™¨çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å¦‚ä½•åœ¨ç”Ÿæˆè‰ºæœ¯ä½œå“æ—¶ç¼–ç å†…å®¹å’Œé£æ ¼æ¦‚å¿µã€‚æˆ‘ä»¬åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›çƒ­å›¾å°†ç”Ÿæˆå›¾åƒçš„åƒç´ å½’å› äºç‰¹å®šçš„æç¤ºæ ‡è®°ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿéš”ç¦»å—å†…å®¹æè¿°æ ‡è®°ä¸é£æ ¼æè¿°æ ‡è®°å½±å“çš„å›¾åƒåŒºåŸŸã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹çš„å†…å®¹å’Œé£æ ¼åˆ†ç¦»ç¨‹åº¦å› ç‰¹å®šçš„è‰ºæœ¯æç¤ºå’Œæ‰€è¯·æ±‚çš„é£æ ¼è€Œå¼‚ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œå†…å®¹æ ‡è®°ä¸»è¦å½±å“å¯¹è±¡ç›¸å…³åŒºåŸŸï¼Œè€Œé£æ ¼æ ‡è®°å½±å“èƒŒæ™¯å’Œçº¹ç†åŒºåŸŸï¼Œè¿™è¡¨æ˜å¯¹å†…å®¹å’Œé£æ ¼åŒºåˆ«çš„ç†è§£æ­£åœ¨æ˜¾ç°ã€‚è¿™äº›è§è§£æœ‰åŠ©äºæˆ‘ä»¬äº†è§£å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹å¦‚ä½•åœ¨æ²¡æœ‰æ˜¾å¼ç›‘ç£çš„æƒ…å†µä¸‹å†…éƒ¨è¡¨ç¤ºå¤æ‚è‰ºæœ¯æ¦‚å¿µã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/umilISLab/artistic-prompt-interpretation">https://github.com/umilISLab/artistic-prompt-interpretation</a>ä¸Šåˆ†äº«äº†ä»£ç ã€æ•°æ®é›†ä»¥åŠç”¨äºå¯è§†åŒ–æ³¨æ„åŠ›å›¾çš„æ¢ç´¢å·¥å…·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23313v1">PDF</a> to be published in: Applications of AI in the Analysis of Cultural   and Artistic Heritage, organized within the 35th IEEE International Workshop   on Machine Learning for Signal Processing (MLSP) 2025</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹èƒ½ä»ä¸è®¡å…¶æ•°çš„å›¾åƒä¸­å­¦ä¹ ç”Ÿæˆè‰ºæœ¯å†…å®¹ï¼Œå±•ç°å‡ºæ˜¾è‘—çš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡ä»¿æµè¡Œè‰ºæœ¯ä½œå“æ–¹é¢ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹åœ¨å†…éƒ¨å¦‚ä½•åŒºåˆ†ç”»ä½œä¸­çš„å†…å®¹å’Œé£æ ¼ç­‰æ¦‚å¿µå°šæœªå¯çŸ¥ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºè½¬æ¢å™¨çš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆè‰ºæœ¯ä½œå“æ—¶å¦‚ä½•ç¼–ç å†…å®¹å’Œé£æ ¼æ¦‚å¿µã€‚é€šè¿‡åˆ©ç”¨äº¤å‰æ³¨æ„åŠ›çƒ­å›¾ï¼Œå°†ç”Ÿæˆå›¾åƒä¸­çš„åƒç´ å½’äºç‰¹å®šæç¤ºä»¤ç‰Œï¼Œæˆ‘ä»¬èƒ½å¤ŸåŒºåˆ†å—å†…å®¹æè¿°å’Œé£æ ¼æè¿°ä»¤ç‰Œå½±å“çš„å›¾åƒåŒºåŸŸã€‚ç ”ç©¶å‘ç°ï¼Œæ‰©æ•£æ¨¡å‹çš„çš„å†…å®¹ä¸é£æ ¼åˆ†ç¦»ç¨‹åº¦å› ç‰¹å®šçš„è‰ºæœ¯æç¤ºå’Œè¦æ±‚çš„é£æ ¼è€Œä¸åŒã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œå†…å®¹ä»¤ç‰Œä¸»è¦å½±å“å¯¹è±¡ç›¸å…³åŒºåŸŸï¼Œè€Œé£æ ¼ä»¤ç‰Œå½±å“èƒŒæ™¯å’Œçº¹ç†åŒºåŸŸï¼Œè¿™è¡¨æ˜å¯¹å†…å®¹å’Œé£æ ¼åŒºåˆ†çš„æ½œåœ¨ç†è§£ã€‚è¿™äº›è§è§£æœ‰åŠ©äºæˆ‘ä»¬äº†è§£å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹å¦‚ä½•åœ¨æ²¡æœ‰æ˜¾å¼ç›‘ç£çš„æƒ…å†µä¸‹å†…éƒ¨è¡¨ç¤ºå¤æ‚è‰ºæœ¯æ¦‚å¿µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹èƒ½ç”Ÿæˆé«˜è´¨é‡çš„è‰ºæœ¯å†…å®¹ï¼Œå…¶èƒ½åŠ›ä»¤äººç©ç›®ã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨å†…éƒ¨å¦‚ä½•åŒºåˆ†ç”»ä½œä¸­çš„å†…å®¹å’Œé£æ ¼å°šæœªè¢«ç ”ç©¶ã€‚</li>
<li>é€šè¿‡äº¤å‰æ³¨æ„åŠ›çƒ­å›¾ï¼Œå¯ä»¥åŒºåˆ†å—å†…å®¹æè¿°ä»¤ç‰Œå’Œé£æ ¼æè¿°ä»¤ç‰Œå½±å“çš„å›¾åƒåŒºåŸŸã€‚</li>
<li>æ‰©æ•£æ¨¡å‹å±•ç¤ºå‡ºçš„å†…å®¹å’Œé£æ ¼çš„åˆ†ç¦»ç¨‹åº¦æ˜¯çµæ´»çš„ï¼Œå–å†³äºç‰¹å®šçš„è‰ºæœ¯æç¤ºå’Œæ‰€éœ€é£æ ¼ã€‚</li>
<li>åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œå†…å®¹ä»¤ç‰Œä¸»è¦å½±å“å¯¹è±¡åŒºåŸŸï¼Œè€Œé£æ ¼ä»¤ç‰Œå½±å“èƒŒæ™¯å’Œçº¹ç†ã€‚</li>
<li>è¿™è¡¨æ˜æ‰©æ•£æ¨¡å‹å¯¹å†…å®¹å’Œé£æ ¼çš„åŒºåˆ«æœ‰ä¸€å®šçš„ç†è§£ï¼Œå°½ç®¡è¿™ç§ç†è§£æ˜¯éšæ€§çš„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23313">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-cbaab2d5dc74bf7f8805253b79ab041f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec4ca46180fe1eced4488586f5557e1b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c3b12316863cb073736cf4fd773429d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e4059378c7a88da83cb74610a9ce77be.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Learning-from-Heterogeneous-Structural-MRI-via-Collaborative-Domain-Adaptation-for-Late-Life-Depression-Assessment"><a href="#Learning-from-Heterogeneous-Structural-MRI-via-Collaborative-Domain-Adaptation-for-Late-Life-Depression-Assessment" class="headerlink" title="Learning from Heterogeneous Structural MRI via Collaborative Domain   Adaptation for Late-Life Depression Assessment"></a>Learning from Heterogeneous Structural MRI via Collaborative Domain   Adaptation for Late-Life Depression Assessment</h2><p><strong>Authors:Yuzhen Gao, Qianqian Wang, Yongheng Sun, Cui Wang, Yongquan Liang, Mingxia Liu</strong></p>
<p>Accurate identification of late-life depression (LLD) using structural brain MRI is essential for monitoring disease progression and facilitating timely intervention. However, existing learning-based approaches for LLD detection are often constrained by limited sample sizes (e.g., tens), which poses significant challenges for reliable model training and generalization. Although incorporating auxiliary datasets can expand the training set, substantial domain heterogeneity, such as differences in imaging protocols, scanner hardware, and population demographics, often undermines cross-domain transferability. To address this issue, we propose a Collaborative Domain Adaptation (CDA) framework for LLD detection using T1-weighted MRIs. The CDA leverages a Vision Transformer (ViT) to capture global anatomical context and a Convolutional Neural Network (CNN) to extract local structural features, with each branch comprising an encoder and a classifier. The CDA framework consists of three stages: (a) supervised training on labeled source data, (b) self-supervised target feature adaptation and (c) collaborative training on unlabeled target data. We first train ViT and CNN on source data, followed by self-supervised target feature adaptation by minimizing the discrepancy between classifier outputs from two branches to make the categorical boundary clearer. The collaborative training stage employs pseudo-labeled and augmented target-domain MRIs, enforcing prediction consistency under strong and weak augmentation to enhance domain robustness and generalization. Extensive experiments conducted on multi-site T1-weighted MRI data demonstrate that the CDA consistently outperforms state-of-the-art unsupervised domain adaptation methods. </p>
<blockquote>
<p>åˆ©ç”¨ç»“æ„æ€§è„‘MRIå‡†ç¡®è¯†åˆ«æ™šæœŸæŠ‘éƒç—‡ï¼ˆLLDï¼‰å¯¹äºç›‘æµ‹ç–¾ç—…è¿›å±•å’Œä¿ƒè¿›åŠæ—¶å¹²é¢„è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºå­¦ä¹ çš„LLDæ£€æµ‹æ–¹æ³•å¾€å¾€å—é™äºæœ‰é™çš„æ ·æœ¬è§„æ¨¡ï¼ˆä¾‹å¦‚ï¼Œä»…å‡ åä¸ªæ ·æœ¬ï¼‰ï¼Œè¿™ç»™å¯é çš„æ¨¡å‹è®­ç»ƒå’Œæ™®åŠå¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚è™½ç„¶åŠ å…¥è¾…åŠ©æ•°æ®é›†å¯ä»¥æ‰©å¤§è®­ç»ƒé›†ï¼Œä½†è¯¸å¦‚æˆåƒåè®®ã€æ‰«æä»ªç¡¬ä»¶å’Œäººå£ç»Ÿè®¡å­¦ç­‰æ–¹é¢çš„å·¨å¤§é¢†åŸŸå·®å¼‚é€šå¸¸ä¼šç ´åè·¨é¢†åŸŸçš„å¯è¿ç§»æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹LLDæ£€æµ‹çš„åä½œé¢†åŸŸé€‚åº”ï¼ˆCDAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨T1åŠ æƒMRIæ•°æ®ã€‚CDAåˆ©ç”¨è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰æ•æ‰å…¨å±€è§£å‰–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶åˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æå–å±€éƒ¨ç»“æ„ç‰¹å¾ï¼Œæ¯ä¸ªåˆ†æ”¯éƒ½åŒ…å«ç¼–ç å™¨å’Œåˆ†ç±»å™¨ã€‚CDAæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šï¼ˆaï¼‰åœ¨æ ‡è®°æºæ•°æ®ä¸Šè¿›è¡Œç›‘ç£è®­ç»ƒï¼›ï¼ˆbï¼‰è‡ªæˆ‘ç›‘ç£çš„ç›®æ ‡ç‰¹å¾é€‚åº”ï¼›ï¼ˆcï¼‰åœ¨æ ‡è®°ç›®æ ‡æ•°æ®ä¸Šè¿›è¡Œåä½œè®­ç»ƒã€‚æˆ‘ä»¬é¦–å…ˆåœ¨æºæ•°æ®ä¸Šè®­ç»ƒViTå’ŒCNNï¼Œç„¶åé€šè¿‡æœ€å°åŒ–ä¸¤ä¸ªåˆ†æ”¯åˆ†ç±»å™¨è¾“å‡ºä¹‹é—´çš„å·®å¼‚è¿›è¡Œè‡ªç›‘ç£ç›®æ ‡ç‰¹å¾é€‚åº”ï¼Œä»¥ä½¿ç±»åˆ«è¾¹ç•Œæ›´åŠ æ¸…æ™°ã€‚åä½œè®­ç»ƒé˜¶æ®µé‡‡ç”¨ä¼ªæ ‡è®°å’Œå¢å¼ºçš„ç›®æ ‡åŸŸMRIæ•°æ®ï¼Œé€šè¿‡å¼ºå¢å¼ºå’Œå¼±å¢å¼ºä¸‹é¢„æµ‹çš„ä¸€è‡´æ€§ï¼Œæé«˜é¢†åŸŸç¨³å¥æ€§å’Œæ™®åŠæ€§ã€‚åœ¨å¤šç«™ç‚¹T1åŠ æƒMRIæ•°æ®ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCDAæŒç»­ä¼˜äºæœ€æ–°çš„æ— ç›‘ç£é¢†åŸŸé€‚åº”æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22321v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ™šå¹´æŠ‘éƒç—‡ï¼ˆLLDï¼‰çš„å‡†ç¡®è¯†åˆ«ï¼Œåˆ©ç”¨ç»“æ„æ€§è„‘MRIè‡³å…³é‡è¦ã€‚ç°æœ‰å­¦ä¹ ç±»æ–¹æ³•å—é™äºæ ·æœ¬é‡ï¼Œé¢ä¸´æ¨¡å‹è®­ç»ƒå’Œæ³›åŒ–æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è·¨åŸŸä¼ è¾“é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§åŸºäºT1åŠ æƒMRIçš„ååŒåŸŸé€‚åº”ï¼ˆCDAï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ•æ‰å…¨å±€è§£å‰–èƒŒæ™¯å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æå–å±€éƒ¨ç»“æ„ç‰¹å¾ï¼Œè¿›è¡ŒLLDæ£€æµ‹ã€‚CDAæ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šæ ‡è®°æºæ•°æ®çš„ç›‘ç£è®­ç»ƒã€è‡ªæˆ‘ç›‘ç£çš„ç›®æ ‡ç‰¹å¾é€‚åº”å’Œåä½œè®­ç»ƒã€‚å®éªŒè¯æ˜ï¼ŒCDAåœ¨è·¨ç«™ç‚¹çš„T1åŠ æƒMRIæ•°æ®ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–æœ€æ–°æ— ç›‘ç£åŸŸé€‚åº”æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ™šå¹´æŠ‘éƒç—‡ï¼ˆLLDï¼‰çš„å‡†ç¡®è¯†åˆ«å¯¹ç›‘æµ‹ç–¾ç—…è¿›å±•å’ŒåŠæ—¶å¹²é¢„è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰å­¦ä¹ ç±»æ–¹æ³•å› æ ·æœ¬é‡é™åˆ¶é¢ä¸´æ¨¡å‹è®­ç»ƒå’Œæ³›åŒ–çš„æŒ‘æˆ˜ã€‚</li>
<li>ååŒåŸŸé€‚åº”ï¼ˆCDAï¼‰æ¡†æ¶ç»“åˆäº†è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œç”¨äºLLDæ£€æµ‹ã€‚</li>
<li>CDAæ¡†æ¶åŒ…æ‹¬ç›‘ç£è®­ç»ƒã€è‡ªæˆ‘ç›‘ç£çš„ç›®æ ‡ç‰¹å¾é€‚åº”å’Œåä½œè®­ç»ƒä¸‰ä¸ªé˜¶æ®µã€‚</li>
<li>CDAåˆ©ç”¨ä¼ªæ ‡è®°å’Œå¢å¼ºçš„ç›®æ ‡åŸŸMRIå›¾åƒï¼Œæé«˜åŸŸç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒCDAåœ¨è·¨ç«™ç‚¹çš„T1åŠ æƒMRIæ•°æ®ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22321">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c871985e69b0e3c969e0ae286f158aae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-578b4e5e1a2171bc3513e7a25ec7acbe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-480d024f7cbd700bc1eff10c66c47362.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-133b40efa8fe0dfe56d3ab86114a1e31.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SmartCLIP-Modular-Vision-language-Alignment-with-Identification-Guarantees"><a href="#SmartCLIP-Modular-Vision-language-Alignment-with-Identification-Guarantees" class="headerlink" title="SmartCLIP: Modular Vision-language Alignment with Identification   Guarantees"></a>SmartCLIP: Modular Vision-language Alignment with Identification   Guarantees</h2><p><strong>Authors:Shaoan Xie, Lingjing Kong, Yujia Zheng, Yu Yao, Zeyu Tang, Eric P. Xing, Guangyi Chen, Kun Zhang</strong></p>
<p>Contrastive Language-Image Pre-training (CLIP)~\citep{radford2021learning} has emerged as a pivotal model in computer vision and multimodal learning, achieving state-of-the-art performance at aligning visual and textual representations through contrastive learning. However, CLIP struggles with potential information misalignment in many image-text datasets and suffers from entangled representation. On the one hand, short captions for a single image in datasets like MSCOCO may describe disjoint regions in the image, leaving the model uncertain about which visual features to retain or disregard. On the other hand, directly aligning long captions with images can lead to the retention of entangled details, preventing the model from learning disentangled, atomic concepts â€“ ultimately limiting its generalization on certain downstream tasks involving short prompts.   In this paper, we establish theoretical conditions that enable flexible alignment between textual and visual representations across varying levels of granularity. Specifically, our framework ensures that a model can not only \emph{preserve} cross-modal semantic information in its entirety but also \emph{disentangle} visual representations to capture fine-grained textual concepts. Building on this foundation, we introduce \ours, a novel approach that identifies and aligns the most relevant visual and textual representations in a modular manner. Superior performance across various tasks demonstrates its capability to handle information misalignment and supports our identification theory. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Mid-Push/SmartCLIP">https://github.com/Mid-Push/SmartCLIP</a>. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒæ¨¡å‹ï¼ˆCLIPï¼‰åœ¨è®¡ç®—æœºè§†è§‰å’Œå¤šæ¨¡æ€å­¦ä¹ ä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ å®ç°äº†è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºçš„å¯¹é½ï¼Œå¹¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼ŒCLIPåœ¨å¤šä¸ªå›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸­å­˜åœ¨æ½œåœ¨çš„ä¿¡æ¯å¯¹é½é—®é¢˜ï¼Œå¹¶å—åˆ°çº ç¼ è¡¨ç¤ºçš„å½±å“ã€‚ä¸€æ–¹é¢ï¼Œåœ¨MSCOCOç­‰æ•°æ®é›†çš„å•å¼ å›¾ç‰‡çš„ç®€çŸ­å­—å¹•å¯èƒ½æè¿°å›¾åƒä¸­çš„ä¸åŒåŒºåŸŸï¼Œä½¿æ¨¡å‹ä¸ç¡®å®šåº”ä¿ç•™æˆ–å¿½ç•¥å“ªäº›è§†è§‰ç‰¹å¾ã€‚å¦ä¸€æ–¹é¢ï¼Œç›´æ¥å°†é•¿å­—å¹•ä¸å›¾åƒå¯¹é½å¯èƒ½å¯¼è‡´ä¿ç•™çº ç¼ çš„ç»†èŠ‚ï¼Œé˜»ç¢æ¨¡å‹å­¦ä¹ è§£è€¦çš„åŸºæœ¬æ¦‚å¿µï¼Œæœ€ç»ˆé™åˆ¶å…¶åœ¨æ¶‰åŠç®€çŸ­æç¤ºçš„æŸäº›ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†ç†è®ºæ¡ä»¶ï¼Œä½¿æ–‡æœ¬å’Œè§†è§‰è¡¨ç¤ºåœ¨ä¸åŒç²’åº¦çº§åˆ«ä¸Šå®ç°çµæ´»å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸ä»…ä¿è¯æ¨¡å‹èƒ½å¤Ÿå®Œæ•´ä¿ç•™è·¨æ¨¡æ€è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œä¸”èƒ½å¤Ÿè§£è€¦è§†è§‰è¡¨ç¤ºä»¥æ•è·ç²¾ç»†çš„æ–‡æœ¬æ¦‚å¿µã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼ˆ\oursï¼‰ï¼Œä»¥ä¸€ç§æ¨¡å—åŒ–çš„æ–¹å¼è¯†åˆ«å’Œå¯¹é½æœ€ç›¸å…³çš„è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºã€‚åœ¨å„ç§ä»»åŠ¡ä¸Šçš„å‡ºè‰²è¡¨ç°è¯æ˜äº†å…¶å¤„ç†ä¿¡æ¯ä¸å¯¹é½çš„èƒ½åŠ›ï¼Œå¹¶éªŒè¯äº†æˆ‘ä»¬çš„è¯†åˆ«ç†è®ºã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Mid-Push/SmartCLIP%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Mid-Push/SmartCLIPä¸­æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22264v1">PDF</a> CVPR2025</p>
<p><strong>Summary</strong><br>åŸºäºCLIPæ¨¡å‹çš„æ–‡æœ¬å›¾åƒé¢„è®­ç»ƒåœ¨è®¡ç®—æœºè§†è§‰å’Œå¤šæ¨¡æ€å­¦ä¹ ä¸­æ‰®æ¼”é‡è¦è§’è‰²ï¼Œä½†å­˜åœ¨ä¿¡æ¯é”™ä½é—®é¢˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒç²’åº¦ä¸Šå®ç°æ–‡æœ¬å’Œè§†è§‰è¡¨ç¤ºä¹‹é—´çš„çµæ´»å¯¹é½ï¼ŒåŒæ—¶ä¿ç•™è·¨æ¨¡æ€è¯­ä¹‰ä¿¡æ¯å¹¶è§£è€¦è§†è§‰è¡¨ç¤ºä»¥æ•æ‰ç²¾ç»†çš„æ–‡æœ¬æ¦‚å¿µã€‚å¼•å…¥çš„æ–¹æ³•ä»¥æ¨¡å—åŒ–çš„æ–¹å¼è¯†åˆ«å’Œå¯¹é½æœ€ç›¸å…³çš„è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºï¼Œå¹¶åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPæ¨¡å‹åœ¨å¤šæ¨¡æ€å­¦ä¹ å’Œè®¡ç®—æœºè§†è§‰ä¸­è¡¨ç°å“è¶Šï¼Œä½†åœ¨å›¾åƒæ–‡æœ¬æ•°æ®é›†ä¸Šå­˜åœ¨æ½œåœ¨çš„ä¿¡æ¯é”™ä½é—®é¢˜ã€‚</li>
<li>ä¿¡æ¯é”™ä½é—®é¢˜æºäºå›¾åƒå’Œæ–‡æœ¬æè¿°çš„ä¸ä¸€è‡´æ€§ï¼Œå¯¼è‡´æ¨¡å‹åœ¨å¤„ç†ä¸åŒå›¾åƒåŒºåŸŸæè¿°æ—¶é¢ä¸´ä¸ç¡®å®šæ€§ã€‚</li>
<li>ç›´æ¥å¯¹é½é•¿æè¿°å’Œå›¾åƒå¯èƒ½å¯¼è‡´çº ç¼ çš„ç»†èŠ‚è¢«ä¿ç•™ï¼Œå½±å“æ¨¡å‹å­¦ä¹ åŸå­æ¦‚å¿µçš„è§£è€¦èƒ½åŠ›ã€‚</li>
<li>æœ¬æ–‡å»ºç«‹çš„ç†è®ºæ¡ä»¶å…è®¸æ–‡æœ¬å’Œè§†è§‰è¡¨ç¤ºåœ¨ä¸åŒç²’åº¦ä¸Šçš„çµæ´»å¯¹é½ã€‚</li>
<li>ä»‹ç»äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¿ç•™è·¨æ¨¡æ€è¯­ä¹‰ä¿¡æ¯çš„åŒæ—¶è§£è€¦è§†è§‰è¡¨ç¤ºï¼Œä»¥æ•æ‰ç²¾ç»†çš„æ–‡æœ¬æ¦‚å¿µã€‚</li>
<li>æ¨¡å—åŒ–çš„æ–¹å¼æœ‰åŠ©äºè¯†åˆ«å’Œå¯¹é½æœ€ç›¸å…³çš„è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22264">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-800cebe6fc30786855ff84de174a2169.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb6457ead03e4c0d017d2377e93e098e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4430b57b92fbb6fa3ca61a65c2e53908.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c69297eb160bf4523f7ba4dab7f20a7.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Enhancing-Generalization-in-Data-free-Quantization-via-Mixup-class-Prompting"><a href="#Enhancing-Generalization-in-Data-free-Quantization-via-Mixup-class-Prompting" class="headerlink" title="Enhancing Generalization in Data-free Quantization via Mixup-class   Prompting"></a>Enhancing Generalization in Data-free Quantization via Mixup-class   Prompting</h2><p><strong>Authors:Jiwoong Park, Chaeun Lee, Yongseok Choi, Sein Park, Deokki Hong, Jungwook Choi</strong></p>
<p>Post-training quantization (PTQ) improves efficiency but struggles with limited calibration data, especially under privacy constraints. Data-free quantization (DFQ) mitigates this by generating synthetic images using generative models such as generative adversarial networks (GANs) and text-conditioned latent diffusion models (LDMs), while applying existing PTQ algorithms. However, the relationship between generated synthetic images and the generalizability of the quantized model during PTQ remains underexplored. Without investigating this relationship, synthetic images generated by previous prompt engineering methods based on single-class prompts suffer from issues such as polysemy, leading to performance degradation. We propose \textbf{mixup-class prompt}, a mixup-based text prompting strategy that fuses multiple class labels at the text prompt level to generate diverse, robust synthetic data. This approach enhances generalization, and improves optimization stability in PTQ. We provide quantitative insights through gradient norm and generalization error analysis. Experiments on convolutional neural networks (CNNs) and vision transformers (ViTs) show that our method consistently outperforms state-of-the-art DFQ methods like GenQ. Furthermore, it pushes the performance boundary in extremely low-bit scenarios, achieving new state-of-the-art accuracy in challenging 2-bit weight, 4-bit activation (W2A4) quantization. </p>
<blockquote>
<p>åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æé«˜äº†æ•ˆç‡ï¼Œä½†åœ¨æœ‰é™çš„æ ¡å‡†æ•°æ®æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨éšç§çº¦æŸä¸‹ã€‚æ— æ•°æ®é‡åŒ–ï¼ˆDFQï¼‰é€šè¿‡åˆ©ç”¨ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ–‡æœ¬æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ï¼‰ç”Ÿæˆåˆæˆå›¾åƒï¼Œå¹¶åº”ç”¨ç°æœ‰çš„PTQç®—æ³•æ¥ç¼“è§£è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œåœ¨PTQæœŸé—´ï¼Œç”Ÿæˆçš„åˆæˆå›¾åƒä¸é‡åŒ–æ¨¡å‹é€šç”¨æ€§ä¹‹é—´çš„å…³ç³»å°šæœªå¾—åˆ°è¶³å¤Ÿçš„ç ”ç©¶ã€‚ç”±äºæ²¡æœ‰ç ”ç©¶è¿™ç§å…³ç³»ï¼Œä»¥å‰åŸºäºå•ç±»æç¤ºçš„æç¤ºå·¥ç¨‹æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒå­˜åœ¨å¤šä¹‰æ€§é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬æå‡º<strong>æ··åˆç±»æç¤º</strong>ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ··åˆçš„æ–‡æœ¬æç¤ºç­–ç•¥ï¼Œå®ƒåœ¨æ–‡æœ¬æç¤ºå±‚é¢èåˆå¤šä¸ªç±»æ ‡ç­¾ï¼Œä»¥ç”Ÿæˆå¤šæ ·ã€ç¨³å¥çš„åˆæˆæ•°æ®ã€‚æ­¤æ–¹æ³•æé«˜äº†é€šç”¨æ€§ï¼Œå¹¶å¢å¼ºäº†PTQä¸­çš„ä¼˜åŒ–ç¨³å®šæ€§ã€‚æˆ‘ä»¬é€šè¿‡æ¢¯åº¦èŒƒæ•°å’Œæ³›åŒ–è¯¯å·®åˆ†ææä¾›äº†å®šé‡è§è§£ã€‚åœ¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºæœ€æ–°çš„DFQæ–¹æ³•ï¼Œå¦‚GenQã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æä½ä½çš„åœºæ™¯ä¸­æ¨åŠ¨äº†æ€§èƒ½è¾¹ç•Œï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„2ä½æƒé‡ã€4ä½æ¿€æ´»ï¼ˆW2A4ï¼‰é‡åŒ–ä¸­è¾¾åˆ°äº†æ–°çš„æœ€é«˜ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21947v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ— è®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰åœ¨æœ‰é™çš„æ ¡å‡†æ•°æ®ä¸‹æ€§èƒ½å—é™ï¼Œç‰¹åˆ«æ˜¯åœ¨éšç§çº¦æŸæ¡ä»¶ä¸‹ã€‚æ•°æ®å…è´¹é‡åŒ–ï¼ˆDFQï¼‰é€šè¿‡ç”Ÿæˆå¯¹æŠ—æ€§ç½‘ç»œï¼ˆGANsï¼‰å’Œæ–‡æœ¬æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ç­‰ç”Ÿæˆåˆæˆå›¾åƒï¼Œå¹¶åº”ç”¨ç°æœ‰PTQç®—æ³•è¿›è¡Œæ”¹è¿›ã€‚ç„¶è€Œï¼Œç”Ÿæˆçš„åˆæˆå›¾åƒä¸PTQæœŸé—´é‡åŒ–æ¨¡å‹çš„ä¸€èˆ¬åŒ–ä¹‹é—´çš„å…³ç³»å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåŸºäºæ··åˆæ–‡æœ¬æç¤ºç­–ç•¥çš„â€œmixup-class promptâ€ï¼Œè¯¥ç­–ç•¥åœ¨æ–‡æœ¬æç¤ºå±‚é¢èåˆå¤šä¸ªç±»åˆ«æ ‡ç­¾ï¼Œç”Ÿæˆå¤šæ ·ä¸”ç¨³å¥çš„åˆæˆæ•°æ®ï¼Œå¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œæé«˜PTQçš„ä¼˜åŒ–ç¨³å®šæ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨CNNå’ŒVision Transformerä¸Šå‡ä¼˜äºæœ€æ–°çš„DFQæ–¹æ³•ï¼Œä¸”åœ¨æä½æ¯”ç‰¹åœºæ™¯ä¸‹è¡¨ç°çªå‡ºï¼Œè¾¾åˆ°W2A4é‡åŒ–æ–°å¢ƒç•Œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PTQåœ¨æœ‰é™çš„æ ¡å‡†æ•°æ®ä¸‹æ€§èƒ½å—é™ï¼Œç‰¹åˆ«æ˜¯åœ¨éšç§çº¦æŸæ¡ä»¶ä¸‹ã€‚</li>
<li>DFQé€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæ”¹è¿›äº†PTQï¼Œä½†åˆæˆå›¾åƒä¸é‡åŒ–æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„å…³ç³»å°šæœªæ˜ç¡®ã€‚</li>
<li>æå‡ºåŸºäºæ··åˆæ–‡æœ¬æç¤ºç­–ç•¥çš„â€œmixup-class promptâ€ï¼Œç”Ÿæˆå¤šæ ·ä¸”ç¨³å¥çš„åˆæˆæ•°æ®ã€‚</li>
<li>è¯¥ç­–ç•¥å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æé«˜äº†PTQçš„ä¼˜åŒ–ç¨³å®šæ€§ã€‚</li>
<li>ä¸æœ€æ–°çš„DFQæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨CNNå’ŒVision Transformerä¸Šéƒ½æœ‰ä¼˜è¶Šè¡¨ç°ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æä½æ¯”ç‰¹åœºæ™¯ä¸‹è¡¨ç°çªå‡ºï¼Œè¾¾åˆ°äº†æ–°çš„é‡åŒ–å¢ƒç•Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f23a00008e793b29dc292768842dfa60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b1e987f9ff7d38ad6f948ea48bbf738.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bf7fc94ce1aa7988d1114a23d033de2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-24d9a380779cf620b7a28738ef429f82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d88922cd039adc8a16ee8b8853d1e784.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MOR-VIT-Efficient-Vision-Transformer-with-Mixture-of-Recursions"><a href="#MOR-VIT-Efficient-Vision-Transformer-with-Mixture-of-Recursions" class="headerlink" title="MOR-VIT: Efficient Vision Transformer with Mixture-of-Recursions"></a>MOR-VIT: Efficient Vision Transformer with Mixture-of-Recursions</h2><p><strong>Authors:YiZhou Li</strong></p>
<p>Vision Transformers (ViTs) have achieved remarkable success in image recognition, yet standard ViT architectures are hampered by substantial parameter redundancy and high computational cost, limiting their practical deployment. While recent efforts on efficient ViTs primarily focus on static model compression or token-level sparsification, they remain constrained by fixed computational depth for all tokens. In this work, we present MoR-ViT, a novel vision transformer framework that, for the first time, incorporates a token-level dynamic recursion mechanism inspired by the Mixture-of-Recursions (MoR) paradigm. This approach enables each token to adaptively determine its processing depth, yielding a flexible and input-dependent allocation of computational resources. Extensive experiments on ImageNet-1K and transfer benchmarks demonstrate that MoR-ViT not only achieves state-of-the-art accuracy with up to 70% parameter reduction and 2.5x inference acceleration, but also outperforms leading efficient ViT baselines such as DynamicViT and TinyViT under comparable conditions. These results establish dynamic recursion as an effective strategy for efficient vision transformers and open new avenues for scalable and deployable deep learning models in real-world scenarios. </p>
<blockquote>
<p>è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰åœ¨å›¾åƒè¯†åˆ«æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œç„¶è€Œï¼Œæ ‡å‡†çš„ViTæ¶æ„å­˜åœ¨å¤§é‡çš„å‚æ•°å†—ä½™å’Œè¾ƒé«˜çš„è®¡ç®—æˆæœ¬ï¼Œé™åˆ¶äº†å…¶å®é™…éƒ¨ç½²ã€‚å°½ç®¡æœ€è¿‘çš„å…³äºé«˜æ•ˆViTçš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨é™æ€æ¨¡å‹å‹ç¼©æˆ–ä»¤ç‰Œçº§ç¨€ç–åŒ–ä¸Šï¼Œä½†å®ƒä»¬ä»ç„¶å—åˆ°æ‰€æœ‰ä»¤ç‰Œçš„å›ºå®šè®¡ç®—æ·±åº¦çš„é™åˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MoR-ViTï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è§†è§‰è½¬æ¢å™¨æ¡†æ¶ï¼Œå®ƒé¦–æ¬¡é‡‡ç”¨äº†å—é€’å½’æ··åˆï¼ˆMoRï¼‰èŒƒå¼å¯å‘çš„ä»¤ç‰Œçº§åŠ¨æ€é€’å½’æœºåˆ¶ã€‚è¿™ç§æ–¹æ³•å…è®¸æ¯ä¸ªä»¤ç‰Œè‡ªé€‚åº”åœ°ç¡®å®šå…¶å¤„ç†æ·±åº¦ï¼Œä»è€Œå®ç°è®¡ç®—èµ„æºçš„çµæ´»å’Œè¾“å…¥ä¾èµ–åˆ†é…ã€‚åœ¨ImageNet-1Kå’Œè¿ç§»åŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMoR-ViTä¸ä»…å®ç°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶å‡å°‘äº†é«˜è¾¾70%çš„å‚æ•°å’Œ2.5å€çš„æ¨ç†åŠ é€Ÿï¼Œè€Œä¸”åœ¨åŒç­‰æ¡ä»¶ä¸‹ä¼˜äºé¢†å…ˆçš„é«˜æ•ˆViTåŸºå‡†æµ‹è¯•ï¼Œå¦‚DynamicViTå’ŒTinyViTã€‚è¿™äº›ç»“æœè¯æ˜äº†åŠ¨æ€é€’å½’æ˜¯é«˜æ•ˆè§†è§‰è½¬æ¢å™¨çš„æœ‰æ•ˆç­–ç•¥ï¼Œå¹¶ä¸ºç°å®ä¸–ç•Œä¸­å¯æ‰©å±•å’Œå¯éƒ¨ç½²çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ‰“å¼€äº†æ–°çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21761v1">PDF</a> 18 pages,9 figuers</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æå‡ºäº†MoR-ViTï¼Œä¸€ç§æ–°å‹è§†è§‰è½¬æ¢å™¨æ¡†æ¶ï¼Œé¦–æ¬¡é‡‡ç”¨åŸºäºæ··åˆé€’å½’ï¼ˆMoRï¼‰èŒƒå¼çš„ä»¤ç‰Œçº§åŠ¨æ€é€’å½’æœºåˆ¶ã€‚è¯¥æ–¹æ³•ä½¿æ¯ä¸ªä»¤ç‰Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°ç¡®å®šå…¶å¤„ç†æ·±åº¦ï¼Œå®ç°è®¡ç®—èµ„æºçš„çµæ´»å’Œè¾“å…¥ä¾èµ–åˆ†é…ã€‚åœ¨ImageNet-1Kå’Œä¼ è¾“åŸºå‡†æµ‹è¯•ä¸Šï¼ŒMoR-ViTä¸ä»…å®ç°äº†é«˜è¾¾70%çš„å‚æ•°å‡å°‘å’Œ2.5å€çš„æ¨ç†åŠ é€Ÿï¼Œè€Œä¸”è¿˜åœ¨å¯æ¯”æ¡ä»¶ä¸‹ä¼˜äºé¢†å…ˆçš„æ•ˆç‡ViTåŸºçº¿ï¼Œå¦‚DynamicViTå’ŒTinyViTã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MoR-ViTæ˜¯ä¸€ç§æ–°å‹çš„è§†è§‰è½¬æ¢å™¨æ¡†æ¶ï¼Œå¼•å…¥äº†åŸºäºæ··åˆé€’å½’èŒƒå¼çš„ä»¤ç‰Œçº§åŠ¨æ€é€’å½’æœºåˆ¶ã€‚</li>
<li>è¯¥æ–¹æ³•å…è®¸æ¯ä¸ªä»¤ç‰Œè‡ªé€‚åº”åœ°ç¡®å®šå…¶å¤„ç†æ·±åº¦ï¼Œå®ç°è®¡ç®—èµ„æºçš„çµæ´»åˆ†é…ã€‚</li>
<li>MoR-ViTå®ç°äº†é«˜è¾¾70%çš„å‚æ•°å‡å°‘å’Œ2.5å€çš„æ¨ç†åŠ é€Ÿã€‚</li>
<li>MoR-ViTåœ¨ImageNet-1Kå’Œä¼ è¾“åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†å…ˆè¿›çš„å‡†ç¡®æ€§ã€‚</li>
<li>MoR-ViTä¼˜äºå…¶ä»–é¢†å…ˆçš„æ•ˆç‡ViTåŸºçº¿ï¼Œå¦‚DynamicViTå’ŒTinyViTã€‚</li>
<li>åŠ¨æ€é€’å½’è¢«è¯æ˜æ˜¯ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥ï¼Œç”¨äºæé«˜è§†è§‰è½¬æ¢å™¨çš„æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7ed7065595b139943fe8b7b994834fc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef7822aba0262f8f4152f234be86e5fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65b37b4127f455a637753d53243a3877.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Comparative-Analysis-of-Vision-Transformers-and-Convolutional-Neural-Networks-for-Medical-Image-Classification"><a href="#Comparative-Analysis-of-Vision-Transformers-and-Convolutional-Neural-Networks-for-Medical-Image-Classification" class="headerlink" title="Comparative Analysis of Vision Transformers and Convolutional Neural   Networks for Medical Image Classification"></a>Comparative Analysis of Vision Transformers and Convolutional Neural   Networks for Medical Image Classification</h2><p><strong>Authors:Kunal Kawadkar</strong></p>
<p>The emergence of Vision Transformers (ViTs) has revolutionized computer vision, yet their effectiveness compared to traditional Convolutional Neural Networks (CNNs) in medical imaging remains under-explored. This study presents a comprehensive comparative analysis of CNN and ViT architectures across three critical medical imaging tasks: chest X-ray pneumonia detection, brain tumor classification, and skin cancer melanoma detection. We evaluated four state-of-the-art models - ResNet-50, EfficientNet-B0, ViT-Base, and DeiT-Small - across datasets totaling 8,469 medical images. Our results demonstrate task-specific model advantages: ResNet-50 achieved 98.37% accuracy on chest X-ray classification, DeiT-Small excelled at brain tumor detection with 92.16% accuracy, and EfficientNet-B0 led skin cancer classification at 81.84% accuracy. These findings provide crucial insights for practitioners selecting architectures for medical AI applications, highlighting the importance of task-specific architecture selection in clinical decision support systems. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTsï¼‰çš„å‡ºç°å·²ç»å½»åº•æ”¹å˜äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œç„¶è€Œï¼Œå…¶åœ¨åŒ»å­¦å½±åƒæ–¹é¢ä¸ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰çš„æœ‰æ•ˆæ€§å¯¹æ¯”ä»ç„¶æœ‰å¾…æ¢ç´¢ã€‚æœ¬ç ”ç©¶é’ˆå¯¹ä¸‰ç§é‡è¦çš„åŒ»å­¦å½±åƒä»»åŠ¡ï¼šèƒ¸éƒ¨Xå°„çº¿è‚ºç‚æ£€æµ‹ã€è„‘è‚¿ç˜¤åˆ†ç±»å’Œçš®è‚¤ç™Œé»‘è‰²ç´ ç˜¤æ£€æµ‹ï¼Œå¯¹CNNå’ŒViTæ¶æ„è¿›è¡Œäº†å…¨é¢çš„å¯¹æ¯”åˆ†æã€‚æˆ‘ä»¬åœ¨åŒ…å«æ€»è®¡8469å¼ åŒ»å­¦å½±åƒçš„æ•°æ®é›†ä¸Šè¯„ä¼°äº†å››ç§æœ€æ–°æ¨¡å‹â€”â€”ResNet-50ã€EfficientNet-B0ã€ViT-Baseå’ŒDeiT-Smallã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºå‡ºé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ¨¡å‹ä¼˜åŠ¿ï¼šåœ¨èƒ¸éƒ¨Xå°„çº¿åˆ†ç±»ä¸­ï¼ŒResNet-50å–å¾—äº†98.37%çš„å‡†ç¡®ç‡ï¼›åœ¨è„‘è‚¿ç˜¤æ£€æµ‹æ–¹é¢ï¼ŒDeiT-Smallä»¥92.16%çš„å‡†ç¡®ç‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼›è€Œåœ¨çš®è‚¤ç™Œåˆ†ç±»ä¸­ï¼ŒEfficientNet-B0ä»¥81.84%çš„å‡†ç¡®ç‡é¢†å…ˆã€‚è¿™äº›å‘ç°å¯¹äºä»ä¸šè€…é€‰æ‹©åŒ»å­¦äººå·¥æ™ºèƒ½åº”ç”¨çš„æ¶æ„è‡³å…³é‡è¦ï¼Œå¹¶å¼ºè°ƒåœ¨ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿä»»åŠ¡ä¸­ç‰¹å®šæ¶æ„é€‰æ‹©çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21156v1">PDF</a> 9 pages, 8 figures, 3 tables. Submitted to IEEE Access</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†Vision Transformersï¼ˆViTsï¼‰ç›¸è¾ƒäºä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰åœ¨åŒ»ç–—å½±åƒé¢†åŸŸçš„åº”ç”¨æ•ˆæœã€‚é€šè¿‡å¯¹ResNet-50ã€EfficientNet-B0ã€ViT-Baseå’ŒDeiT-Smallå››ç§å‰æ²¿æ¨¡å‹åœ¨ä¸‰é¡¹é‡è¦åŒ»ç–—å½±åƒä»»åŠ¡ä¸Šçš„æ¯”è¾ƒåˆ†æï¼Œå‘ç°ä¸åŒæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„ä¼˜åŠ¿ã€‚è¿™äº›ç»“æœå¯¹åŒ»ç–—äººå·¥æ™ºèƒ½åº”ç”¨çš„æ¶æ„é€‰æ‹©æä¾›äº†é‡è¦å‚è€ƒï¼Œå¼ºè°ƒä»»åŠ¡ç‰¹å®šæ¶æ„é€‰æ‹©åœ¨ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformersï¼ˆViTsï¼‰åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå·²å¼•èµ·é©å‘½æ€§å˜é©ï¼Œä½†åœ¨åŒ»ç–—å½±åƒé¢†åŸŸçš„ç ”ç©¶ä»ç„¶æœ‰é™ã€‚</li>
<li>å¯¹æ¯”ç ”ç©¶äº†å››ç§é¡¶å°–æ¨¡å‹åœ¨ä¸‰é¡¹åŒ»ç–—å½±åƒä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>ResNet-50åœ¨èƒ¸éƒ¨Xå…‰è‚ºç‚æ£€æµ‹æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè¾¾åˆ°98.37%å‡†ç¡®ç‡ã€‚</li>
<li>DeiT-Smallåœ¨è„‘è‚¿ç˜¤åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå‡†ç¡®ç‡ä¸º92.16%ã€‚</li>
<li>EfficientNet-B0åœ¨çš®è‚¤ç™Œé»‘è‰²ç´ ç˜¤æ£€æµ‹æ–¹é¢é¢†å…ˆï¼Œå‡†ç¡®ç‡ä¸º81.84%ã€‚</li>
<li>ä¸åŒæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°å­˜åœ¨å·®å¼‚ï¼Œé€‰æ‹©é€‚åˆçš„æ¨¡å‹æ¶æ„å¯¹åŒ»ç–—AIåº”ç”¨è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21156">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0c28b1259fe5a0c87a783d1c6b0dd038.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cbfd88139d0c71703360e8ef56db66ec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb7e67414eb5a9956745f113a73d946c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-230f28eead0721cbdc04bbe3f54629f0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a6823dd10a982e7dafa86b640369b596.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7bac96678784f7d0cd8fdb1410ba42aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dfa7a97e35d6ae6dc369a996481dbae.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="UniCT-Depth-Event-Image-Fusion-Based-Monocular-Depth-Estimation-with-Convolution-Compensated-ViT-Dual-SA-Block"><a href="#UniCT-Depth-Event-Image-Fusion-Based-Monocular-Depth-Estimation-with-Convolution-Compensated-ViT-Dual-SA-Block" class="headerlink" title="UniCT Depth: Event-Image Fusion Based Monocular Depth Estimation with   Convolution-Compensated ViT Dual SA Block"></a>UniCT Depth: Event-Image Fusion Based Monocular Depth Estimation with   Convolution-Compensated ViT Dual SA Block</h2><p><strong>Authors:Luoxi Jing, Dianxi Shi, Zhe Liu, Songchang Jin, Chunping Qiu, Ziteng Qiao, Yuxian Li, Jianqiang Xia</strong></p>
<p>Depth estimation plays a crucial role in 3D scene understanding and is extensively used in a wide range of vision tasks. Image-based methods struggle in challenging scenarios, while event cameras offer high dynamic range and temporal resolution but face difficulties with sparse data. Combining event and image data provides significant advantages, yet effective integration remains challenging. Existing CNN-based fusion methods struggle with occlusions and depth disparities due to limited receptive fields, while Transformer-based fusion methods often lack deep modality interaction. To address these issues, we propose UniCT Depth, an event-image fusion method that unifies CNNs and Transformers to model local and global features. We propose the Convolution-compensated ViT Dual SA (CcViT-DA) Block, designed for the encoder, which integrates Context Modeling Self-Attention (CMSA) to capture spatial dependencies and Modal Fusion Self-Attention (MFSA) for effective cross-modal fusion. Furthermore, we design the tailored Detail Compensation Convolution (DCC) Block to improve texture details and enhances edge representations. Experiments show that UniCT Depth outperforms existing image, event, and fusion-based monocular depth estimation methods across key metrics. </p>
<blockquote>
<p>æ·±åº¦ä¼°è®¡åœ¨3Dåœºæ™¯ç†è§£ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºå„ç§è§†è§‰ä»»åŠ¡ã€‚åŸºäºå›¾åƒçš„æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­è¡¨ç°æŒ£æ‰ï¼Œè€Œäº‹ä»¶ç›¸æœºè™½ç„¶æä¾›äº†é«˜åŠ¨æ€èŒƒå›´å’Œæ—¶åºåˆ†è¾¨ç‡ï¼Œä½†åœ¨å¤„ç†ç¨€ç–æ•°æ®æ—¶é¢ä¸´å›°éš¾ã€‚ç»“åˆäº‹ä»¶å’Œå›¾åƒæ•°æ®å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†æœ‰æ•ˆé›†æˆä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºäºCNNçš„èåˆæ–¹æ³•ç”±äºæœ‰é™çš„æ„Ÿå—é‡è€Œéš¾ä»¥å¤„ç†é®æŒ¡å’Œæ·±åº¦å·®å¼‚é—®é¢˜ï¼Œè€ŒåŸºäºTransformerçš„èåˆæ–¹æ³•é€šå¸¸ç¼ºä¹æ·±åº¦æ¨¡æ€äº¤äº’ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†UniCT Depthï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆCNNå’ŒTransformerçš„äº‹ä»¶å›¾åƒèåˆæ–¹æ³•ï¼Œç”¨äºå»ºæ¨¡å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚æˆ‘ä»¬ä¸ºç¼–ç å™¨è®¾è®¡äº†å·ç§¯è¡¥å¿ViTåŒSAï¼ˆCcViT-DAï¼‰å—ï¼Œè¯¥å—é›†æˆäº†ä¸Šä¸‹æ–‡å»ºæ¨¡è‡ªæ³¨æ„åŠ›ï¼ˆCMSAï¼‰ä»¥æ•è·ç©ºé—´ä¾èµ–å…³ç³»ï¼Œå¹¶é›†æˆäº†æ¨¡æ€èåˆè‡ªæ³¨æ„åŠ›ï¼ˆMFSAï¼‰ä»¥å®ç°æœ‰æ•ˆçš„è·¨æ¨¡æ€èåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸“é—¨çš„ç»†èŠ‚è¡¥å¿å·ç§¯ï¼ˆDCCï¼‰å—ï¼Œä»¥æé«˜çº¹ç†ç»†èŠ‚å¹¶å¢å¼ºè¾¹ç¼˜è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒUniCT Depthåœ¨å…³é”®æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰çš„å›¾åƒã€äº‹ä»¶å’ŒåŸºäºèåˆçš„å•ç›®æ·±åº¦ä¼°è®¡æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19948v1">PDF</a> Accepted by IJCAI 2025 (International Joint Conference on Artificial   Intelligence)</p>
<p><strong>Summary</strong></p>
<p>äº‹ä»¶ç›¸æœºä¸å›¾åƒæ•°æ®çš„ç»“åˆåœ¨æ·±åº¦ä¼°è®¡ä¸­å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œä½†æœ‰æ•ˆé›†æˆä»å…·æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´æœ‰é™æ„Ÿå—é‡å’Œæ¨¡æ€äº¤äº’ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼ŒUniCT Depthæ–¹æ³•èåˆCNNå’ŒTransformerï¼Œå»ºç«‹å±€éƒ¨ä¸å…¨å±€ç‰¹å¾æ¨¡å‹ã€‚æå‡ºCcViT-DAå—ä¸CMSAå’ŒMFSAï¼Œç”¨äºæ•è·ç©ºé—´ä¾èµ–æ€§å’Œæœ‰æ•ˆè·¨æ¨¡æ€èåˆã€‚è®¾è®¡DCCå—æ”¹å–„çº¹ç†ç»†èŠ‚å’Œè¾¹ç¼˜è¡¨ç¤ºã€‚å®éªŒè¡¨æ˜ï¼ŒUniCT Depthåœ¨å…³é”®æŒ‡æ ‡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº‹ä»¶ç›¸æœºä¸å›¾åƒæ•°æ®ç»“åˆå¯¹äºæ·±åº¦ä¼°è®¡è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´æœ‰æ•ˆé›†æˆæŒ‘æˆ˜ï¼Œå¦‚CNNçš„æœ‰é™æ„Ÿå—é‡å’ŒTransformerçš„æ¨¡æ€äº¤äº’ä¸è¶³ã€‚</li>
<li>UniCT Depthæ–¹æ³•èåˆCNNå’ŒTransformerï¼Œå»ºç«‹å±€éƒ¨ä¸å…¨å±€ç‰¹å¾æ¨¡å‹ã€‚</li>
<li>CcViT-DAå—ç»“åˆäº†CMSAå’ŒMFSAï¼Œåˆ†åˆ«ç”¨äºæ•è·ç©ºé—´ä¾èµ–æ€§å’Œå®ç°è·¨æ¨¡æ€èåˆã€‚</li>
<li>DCCå—è®¾è®¡ç”¨äºæ”¹å–„çº¹ç†ç»†èŠ‚å’Œè¾¹ç¼˜è¡¨ç¤ºã€‚</li>
<li>UniCT Depthåœ¨æ·±åº¦ä¼°è®¡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºç°æœ‰å›¾åƒã€äº‹ä»¶å’Œèåˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19948">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-54ff3cbfdc5944fbd343fd064b561768.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c4f417d321dd1e8b6a1eae7155292ff0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1e85749bc723dbdb5f43298501e3667.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6569611015da231e00d251e5daf8b07f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-12537b7deb477807f42cd3f388a63b44.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-568b0aff521e51cf3922466cb95e095e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Knowledge-Regularized-Negative-Feature-Tuning-of-Vision-Language-Models-for-Out-of-Distribution-Detection"><a href="#Knowledge-Regularized-Negative-Feature-Tuning-of-Vision-Language-Models-for-Out-of-Distribution-Detection" class="headerlink" title="Knowledge Regularized Negative Feature Tuning of Vision-Language Models   for Out-of-Distribution Detection"></a>Knowledge Regularized Negative Feature Tuning of Vision-Language Models   for Out-of-Distribution Detection</h2><p><strong>Authors:Wenjie Zhu, Yabin Zhang, Xin Jin, Wenjun Zeng, Lei Zhang</strong></p>
<p>Out-of-distribution (OOD) detection is crucial for building reliable machine learning models. Although negative prompt tuning has enhanced the OOD detection capabilities of vision-language models, these tuned models often suffer from reduced generalization performance on unseen classes and styles. To address this challenge, we propose a novel method called Knowledge Regularized Negative Feature Tuning (KR-NFT), which integrates an innovative adaptation architecture termed Negative Feature Tuning (NFT) and a corresponding knowledge-regularization (KR) optimization strategy. Specifically, NFT applies distribution-aware transformations to pre-trained text features, effectively separating positive and negative features into distinct spaces. This separation maximizes the distinction between in-distribution (ID) and OOD images. Additionally, we introduce image-conditional learnable factors through a lightweight meta-network, enabling dynamic adaptation to individual images and mitigating sensitivity to class and style shifts. Compared to traditional negative prompt tuning, NFT demonstrates superior efficiency and scalability. To optimize this adaptation architecture, the KR optimization strategy is designed to enhance the discrimination between ID and OOD sets while mitigating pre-trained knowledge forgetting. This enhances OOD detection performance on trained ID classes while simultaneously improving OOD detection on unseen ID datasets. Notably, when trained with few-shot samples from ImageNet dataset, KR-NFT not only improves ID classification accuracy and OOD detection but also significantly reduces the FPR95 by 5.44% under an unexplored generalization setting with unseen ID categories. Codes can be found at \href{<a target="_blank" rel="noopener" href="https://github.com/ZhuWenjie98/KRNFT%7D">https://github.com/ZhuWenjie98/KRNFT}</a>. </p>
<blockquote>
<p>åœ¨æ„å»ºå¯é çš„æœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œç¦»ç¾¤åˆ†å¸ƒï¼ˆOODï¼‰æ£€æµ‹è‡³å…³é‡è¦ã€‚å°½ç®¡è´Ÿæç¤ºè°ƒæ•´å¢å¼ºäº†è§†è§‰è¯­è¨€æ¨¡å‹çš„OODæ£€æµ‹èƒ½åŠ›ï¼Œä½†è¿™äº›è°ƒæ•´åçš„æ¨¡å‹å¾€å¾€åœ¨æœªè§ç±»åˆ«å’Œé£æ ¼ä¸Šçš„æ³›åŒ–æ€§èƒ½é™ä½ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºçŸ¥è¯†æ­£åˆ™è´Ÿç‰¹å¾è°ƒæ•´ï¼ˆKR-NFTï¼‰çš„æ–°æ–¹æ³•ï¼Œå®ƒç»“åˆäº†åä¸ºè´Ÿç‰¹å¾è°ƒæ•´ï¼ˆNFTï¼‰çš„åˆ›æ–°é€‚åº”æ¶æ„å’Œç›¸åº”çš„çŸ¥è¯†æ­£åˆ™ï¼ˆKRï¼‰ä¼˜åŒ–ç­–ç•¥ã€‚å…·ä½“è€Œè¨€ï¼ŒNFTå¯¹é¢„è®­ç»ƒçš„æ–‡æœ¬ç‰¹å¾è¿›è¡Œåˆ†å¸ƒæ„ŸçŸ¥è½¬æ¢ï¼Œæœ‰æ•ˆåœ°å°†æ­£è´Ÿç‰¹å¾åˆ†ç¦»åˆ°ä¸åŒçš„ç©ºé—´ã€‚è¿™ç§åˆ†ç¦»æœ€å¤§é™åº¦åœ°æé«˜äº†å†…åˆ†å¸ƒï¼ˆIDï¼‰å’ŒOODå›¾åƒä¹‹é—´çš„åŒºåˆ«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡è½»é‡çº§å…ƒç½‘ç»œå¼•å…¥å›¾åƒæ¡ä»¶å¯å­¦ä¹ å› å­ï¼Œå®ç°åŠ¨æ€é€‚åº”ä¸ªåˆ«å›¾åƒå¹¶å‡è½»å¯¹ç±»åˆ«å’Œé£æ ¼å˜åŒ–çš„æ•æ„Ÿæ€§ã€‚ä¸ä¼ ç»Ÿçš„è´Ÿæç¤ºè°ƒæ•´ç›¸æ¯”ï¼ŒNFTæ˜¾ç¤ºå‡ºæ›´é«˜çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚ä¸ºäº†ä¼˜åŒ–æ­¤é€‚åº”æ¶æ„ï¼ŒKRä¼˜åŒ–ç­–ç•¥æ—¨åœ¨å¢å¼ºIDå’ŒOODé›†ä¹‹é—´çš„è¾¨åˆ«åŠ›ï¼ŒåŒæ—¶å‡è½»é¢„è®­ç»ƒçŸ¥è¯†çš„é—å¿˜ã€‚è¿™æé«˜äº†è®­ç»ƒIDç±»åˆ«ä¸Šçš„OODæ£€æµ‹æ€§èƒ½ï¼ŒåŒæ—¶æ”¹è¿›äº†æœªè§IDæ•°æ®é›†ä¸Šçš„OODæ£€æµ‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“ä½¿ç”¨æ¥è‡ªImageNetæ•°æ®é›†çš„å°‘é‡æ ·æœ¬è¿›è¡Œè®­ç»ƒæ—¶ï¼ŒKR-NFTä¸ä»…æé«˜äº†IDåˆ†ç±»ç²¾åº¦å’ŒOODæ£€æµ‹èƒ½åŠ›ï¼Œè€Œä¸”åœ¨æœªè§IDç±»åˆ«çš„æœªæ¢ç´¢æ³›åŒ–è®¾ç½®ä¸‹å°†FPR95é™ä½äº†5.44%ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ZhuWenjie98/KRNFT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ZhuWenjie98/KRNFTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19847v2">PDF</a> accepted by ACMMM 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†é’ˆå¯¹æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„OODæ£€æµ‹éš¾é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œå¼•å…¥äº†çŸ¥è¯†æ­£åˆ™åŒ–è´Ÿç‰¹å¾è°ƒä¼˜ï¼ˆKR-NFTï¼‰è¿™ä¸€æ–°æ–¹æ³•ã€‚å®ƒé€šè¿‡åˆ›æ–°åœ°å¼•å…¥è´Ÿç‰¹å¾è°ƒä¼˜ï¼ˆNFTï¼‰æ¶æ„å’Œç›¸åº”çš„çŸ¥è¯†æ­£åˆ™åŒ–ï¼ˆKRï¼‰ä¼˜åŒ–ç­–ç•¥ï¼Œè§£å†³äº†ä¼ ç»Ÿè´Ÿæç¤ºè°ƒä¼˜åœ¨æœªè§ç±»åˆ«å’Œé£æ ¼ä¸Šçš„æ³›åŒ–æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚NFTé€šè¿‡å¯¹é¢„è®­ç»ƒæ–‡æœ¬ç‰¹å¾è¿›è¡Œåˆ†å¸ƒæ„ŸçŸ¥è½¬æ¢ï¼Œæœ‰æ•ˆåœ°å°†æ­£è´Ÿç‰¹å¾åˆ†ç¦»åˆ°ä¸åŒçš„ç©ºé—´ï¼Œæœ€å¤§åŒ–IDå’ŒOODå›¾åƒä¹‹é—´çš„åŒºåˆ†ã€‚åŒæ—¶ï¼Œé€šè¿‡è½»é‡çº§å…ƒç½‘ç»œå¼•å…¥å›¾åƒæ¡ä»¶å­¦ä¹ å› å­ï¼Œå®ç°åŠ¨æ€é€‚åº”ä¸ªä½“å›¾åƒå¹¶ç¼“è§£ç±»åˆ«å’Œé£æ ¼å˜åŒ–çš„å½±å“ã€‚ä¸ä¼ ç»Ÿçš„è´Ÿæç¤ºè°ƒä¼˜ç›¸æ¯”ï¼ŒNFTè¡¨ç°å‡ºæ›´é«˜çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚KRä¼˜åŒ–ç­–ç•¥æ—¨åœ¨ä¼˜åŒ–è¿™ä¸€é€‚åº”æ¶æ„ï¼Œå¢å¼ºIDå’ŒOODé›†ä¹‹é—´çš„é‰´åˆ«åŠ›ï¼ŒåŒæ—¶å‡å°‘é¢„è®­ç»ƒçŸ¥è¯†çš„é—å¿˜ï¼Œä»è€Œåœ¨ä¸æŸå¤±IDåˆ†ç±»å‡†ç¡®æ€§çš„å‰æä¸‹æé«˜OODæ£€æµ‹æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨å¯¹ImageNetæ•°æ®é›†è¿›è¡Œå°‘é‡æ ·æœ¬è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒKR-NFTä¸ä»…æé«˜äº†IDåˆ†ç±»ç²¾åº¦å’ŒOODæ£€æµ‹èƒ½åŠ›ï¼Œè€Œä¸”åœ¨æœªè§IDç±»åˆ«çš„æƒ…å†µä¸‹å°†FPR95é™ä½äº†5.44%ã€‚ä»£ç å¯åœ¨ç›¸å…³é“¾æ¥ä¸­æ‰¾åˆ°ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•KR-NFTï¼Œç”¨äºè§£å†³æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„OODæ£€æµ‹é—®é¢˜ã€‚</li>
<li>é€šè¿‡è´Ÿç‰¹å¾è°ƒä¼˜ï¼ˆNFTï¼‰æ¶æ„å°†æ­£è´Ÿç‰¹å¾åˆ†ç¦»åˆ°ä¸åŒçš„ç©ºé—´ï¼Œå¢å¼ºäº†IDå’ŒOODå›¾åƒä¹‹é—´çš„åŒºåˆ†ã€‚</li>
<li>å¼•å…¥å›¾åƒæ¡ä»¶å­¦ä¹ å› å­ï¼Œé€šè¿‡è½»é‡çº§å…ƒç½‘ç»œå®ç°åŠ¨æ€é€‚åº”ä¸ªä½“å›¾åƒã€‚</li>
<li>NFTç›¸è¾ƒäºä¼ ç»Ÿè´Ÿæç¤ºè°ƒä¼˜å±•ç°å‡ºæ›´é«˜çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>KRä¼˜åŒ–ç­–ç•¥å¢å¼ºäº†IDå’ŒOODé›†ä¹‹é—´çš„é‰´åˆ«åŠ›ï¼Œå¹¶å‡å°‘äº†é¢„è®­ç»ƒçŸ¥è¯†çš„é—å¿˜ã€‚</li>
<li>åœ¨å°‘é‡æ ·æœ¬è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒKR-NFTæ˜¾è‘—æé«˜OODæ£€æµ‹æ€§èƒ½å¹¶é™ä½FPR95ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19847">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eed7cd1d1f1404cd94072d2944bcc9e7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4eef513fd2fc62d5a82d5717faa0271a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-14d9aead208d46a34db6826b31e4bf56.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1990559244bd346ad5493df0d003d307.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f591d0bf706ed0cf76f68b162811f37b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Self-Supervised-Ultrasound-Video-Segmentation-with-Feature-Prediction-and-3D-Localised-Loss"><a href="#Self-Supervised-Ultrasound-Video-Segmentation-with-Feature-Prediction-and-3D-Localised-Loss" class="headerlink" title="Self-Supervised Ultrasound-Video Segmentation with Feature Prediction   and 3D Localised Loss"></a>Self-Supervised Ultrasound-Video Segmentation with Feature Prediction   and 3D Localised Loss</h2><p><strong>Authors:Edward Ellis, Robert Mendel, Andrew Bulpitt, Nasim Parsa, Michael F Byrne, Sharib Ali</strong></p>
<p>Acquiring and annotating large datasets in ultrasound imaging is challenging due to low contrast, high noise, and susceptibility to artefacts. This process requires significant time and clinical expertise. Self-supervised learning (SSL) offers a promising solution by leveraging unlabelled data to learn useful representations, enabling improved segmentation performance when annotated data is limited. Recent state-of-the-art developments in SSL for video data include V-JEPA, a framework solely based on feature prediction, avoiding pixel level reconstruction or negative samples. We hypothesise that V-JEPA is well-suited to ultrasound imaging, as it is less sensitive to noisy pixel-level detail while effectively leveraging temporal information. To the best of our knowledge, this is the first study to adopt V-JEPA for ultrasound video data. Similar to other patch-based masking SSL techniques such as VideoMAE, V-JEPA is well-suited to ViT-based models. However, ViTs can underperform on small medical datasets due to lack of inductive biases, limited spatial locality and absence of hierarchical feature learning. To improve locality understanding, we propose a novel 3D localisation auxiliary task to improve locality in ViT representations during V-JEPA pre-training. Our results show V-JEPA with our auxiliary task improves segmentation performance significantly across various frozen encoder configurations, with gains up to 3.4% using 100% and up to 8.35% using only 10% of the training data. </p>
<blockquote>
<p>åœ¨è¶…å£°æˆåƒä¸­ï¼Œè·å–å’Œæ ‡æ³¨å¤§å‹æ•°æ®é›†æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºå­˜åœ¨å¯¹æ¯”åº¦ä½ã€å™ªå£°é«˜ä»¥åŠå®¹æ˜“å—ä¼ªå½±å½±å“ç­‰é—®é¢˜ã€‚è¿™ä¸ªè¿‡ç¨‹éœ€è¦å¤§é‡çš„æ—¶é—´å’Œä¸´åºŠä¸“ä¸šçŸ¥è¯†ã€‚è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®å­¦ä¹ æœ‰ç”¨çš„è¡¨ç¤ºï¼Œæä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨æ ‡æ³¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ï¼Œèƒ½å¤Ÿæé«˜åˆ†å‰²æ€§èƒ½ã€‚æœ€è¿‘è§†é¢‘æ•°æ®è‡ªç›‘ç£å­¦ä¹ çš„æœ€æ–°å‘å±•åŒ…æ‹¬V-JEPAï¼Œè¿™æ˜¯ä¸€ä¸ªä»…åŸºäºç‰¹å¾é¢„æµ‹çš„æ¡†æ¶ï¼Œé¿å…äº†åƒç´ çº§é‡å»ºæˆ–è´Ÿæ ·æœ¬ã€‚æˆ‘ä»¬å‡è®¾V-JEPAéå¸¸é€‚åˆè¶…å£°æˆåƒï¼Œå› ä¸ºå®ƒå¯¹å˜ˆæ‚çš„åƒç´ çº§ç»†èŠ‚ä¸å¤ªæ•æ„Ÿï¼ŒåŒæ—¶èƒ½æœ‰æ•ˆåœ°åˆ©ç”¨æ—¶é—´ä¿¡æ¯ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°†V-JEPAç”¨äºè¶…å£°è§†é¢‘æ•°æ®çš„ç ”ç©¶ã€‚ä¸å…¶ä»–åŸºäºè¡¥ä¸çš„æ©ç SSLæŠ€æœ¯ï¼ˆå¦‚VideoMAEï¼‰ç±»ä¼¼ï¼ŒV-JEPAéå¸¸é€‚åˆViTæ¨¡å‹ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å½’çº³åè§ã€ç©ºé—´å±€éƒ¨æ€§æœ‰é™ä»¥åŠç¼ºä¹å±‚æ¬¡ç‰¹å¾å­¦ä¹ ï¼ŒViTåœ¨å°åŒ»ç–—æ•°æ®é›†ä¸Šå¯èƒ½ä¼šè¡¨ç°ä¸ä½³ã€‚ä¸ºäº†æé«˜å¯¹å±€éƒ¨æ€§çš„ç†è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„3Då®šä½è¾…åŠ©ä»»åŠ¡ï¼Œä»¥åœ¨V-JEPAé¢„è®­ç»ƒæœŸé—´æ”¹å–„ViTè¡¨ç¤ºçš„å±€éƒ¨æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„è¾…åŠ©ä»»åŠ¡çš„V-JEPAåœ¨å„ç§å›ºå®šçš„ç¼–ç å™¨é…ç½®ä¸­æ˜¾è‘—æé«˜äº†åˆ†å‰²æ€§èƒ½ï¼Œä½¿ç”¨100%çš„æ•°æ®å¢ç›Šé«˜è¾¾3.4%ï¼Œè€Œåªä½¿ç”¨10%çš„è®­ç»ƒæ•°æ®åˆ™é«˜è¾¾8.35%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18424v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è¶…å£°æˆåƒä¸­é‡‡é›†å’Œæ ‡æ³¨å¤§è§„æ¨¡æ•°æ®é›†å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå­˜åœ¨ä½å¯¹æ¯”åº¦ã€é«˜å™ªå£°å’Œæ˜“å—åˆ°å¹²æ‰°çš„é—®é¢˜ã€‚è¿™ä¸€è¿‡ç¨‹éœ€è¦è€—è´¹å¤§é‡æ—¶é—´å’Œä¸´åºŠä¸“ä¸šçŸ¥è¯†ã€‚è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰é€šè¿‡åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®å­¦ä¹ æœ‰ç”¨çš„è¡¨ç¤ºå½¢å¼ï¼Œä¸ºè§£å†³æ ‡æ³¨æ•°æ®æœ‰é™æ—¶çš„åˆ†å‰²æ€§èƒ½æå‡é—®é¢˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶é¦–æ¬¡å°†V-JEPAåº”ç”¨äºè¶…å£°è§†é¢‘æ•°æ®ï¼Œé€šè¿‡æå‡ºä¸€ä¸ªæ–°å‹çš„ä¸‰ç»´å®šä½è¾…åŠ©ä»»åŠ¡ï¼Œæ”¹å–„äº†ViTæ¨¡å‹åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„å±€éƒ¨ç†è§£é—®é¢˜ã€‚ç»“æœæ˜¾ç¤ºï¼Œç»“åˆè¾…åŠ©ä»»åŠ¡çš„V-JEPAåœ¨ä¸åŒå†»ç»“ç¼–ç å™¨é…ç½®ä¸‹å‡æ˜¾è‘—æé«˜åˆ†å‰²æ€§èƒ½ï¼Œä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®æ—¶å¢ç›Šè¾¾3.4%ï¼Œä»…ä½¿ç”¨10%è®­ç»ƒæ•°æ®æ—¶å¢ç›Šè¾¾8.35%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…å£°æˆåƒæ•°æ®é›†ä¸­è·å–å’Œæ ‡æ³¨å­˜åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ä½å¯¹æ¯”åº¦ã€é«˜å™ªå£°å’Œæ˜“å—åˆ°å¹²æ‰°ã€‚</li>
<li>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰èƒ½åˆ©ç”¨æœªæ ‡æ³¨æ•°æ®æå‡æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>V-JEPAé€‚ç”¨äºè¶…å£°è§†é¢‘æ•°æ®ï¼Œå¯¹å™ªå£°åƒç´ çº§çš„ç»†èŠ‚ä¸å¤ªæ•æ„Ÿï¼ŒåŒæ—¶èƒ½æœ‰æ•ˆåˆ©ç”¨æ—¶é—´ä¿¡æ¯ã€‚</li>
<li>V-JEPAç»“åˆViTæ¨¡å‹åœ¨è¶…å£°è§†é¢‘æ•°æ®å¤„ç†ä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚</li>
<li>ViTæ¨¡å‹åœ¨å°è§„æ¨¡åŒ»ç–—æ•°æ®é›†ä¸Šå¯èƒ½ä¼šå› ç¼ºä¹å½’çº³åç½®ã€ç©ºé—´å±€é™æ€§å’Œç¼ºä¹å±‚æ¬¡ç‰¹å¾å­¦ä¹ è€Œè¡¨ç°ä¸ä½³ã€‚</li>
<li>ä¸ºæ”¹å–„ViTæ¨¡å‹çš„å±€éƒ¨ç†è§£ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„ä¸‰ç»´å®šä½è¾…åŠ©ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8170142feacbcd0cc175811eacb88e33.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ec3c3679e68c7061a69ca5b27a0335f8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6cd5a549726d87048e1c4538e241aa1e.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="GRR-CoCa-Leveraging-LLM-Mechanisms-in-Multimodal-Model-Architectures"><a href="#GRR-CoCa-Leveraging-LLM-Mechanisms-in-Multimodal-Model-Architectures" class="headerlink" title="GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures"></a>GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures</h2><p><strong>Authors:Jake R. Patock, Nicole Catherine Lewis, Kevin McCoy, Christina Gomez, Canling Chen, Lorenzo Luzi</strong></p>
<p>State-of-the-art (SOTA) image and text generation models are multimodal models that have many similarities to large language models (LLMs). Despite achieving strong performances, leading foundational multimodal model architectures frequently lag behind the architectural sophistication of contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner (CoCa) model that incorporates Gaussian error gated linear units, root mean squared normalization, and rotary positional embedding into the textual decoders and the vision transformer (ViT) encoder. Each architectural modification has been shown to improve model performance in LLMs, but has yet to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model with the same modified textual decoders but with CoCaâ€™s original ViT encoder. We used standard pretraining and fine-tuning workflows to benchmark the models on contrastive and generative tasks. Our GRR-CoCa significantly outperformed Baseline CoCa on the pretraining dataset and three diverse fine-tuning datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were 13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We show that GRR-CoCaâ€™s modified architecture improves performance and generalization across vision-language domains. </p>
<blockquote>
<p>æœ€å…ˆè¿›çš„å›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆæ¨¡å‹æ˜¯å¤šæ¨¡æ€æ¨¡å‹ï¼Œå®ƒä»¬ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœ‰è®¸å¤šç›¸ä¼¼ä¹‹å¤„ã€‚å°½ç®¡æ€§èƒ½å¼ºåŠ²ï¼Œä½†é¢†å…ˆçš„åŸºç¡€å¤šæ¨¡æ€æ¨¡å‹æ¶æ„é€šå¸¸è½åäºå½“ä»£LLMçš„æ¶æ„å¤æ‚æ€§ã€‚æˆ‘ä»¬æå‡ºäº†GRR-CoCaï¼Œè¿™æ˜¯ä¸€ä¸ªæ”¹è¿›çš„æœ€å…ˆè¿›å¯¹æ¯”å­—å¹•æ¨¡å‹ï¼ˆCoCaï¼‰ï¼Œå®ƒç»“åˆäº†é«˜æ–¯è¯¯å·®é—¨æ§çº¿æ€§å•å…ƒã€å‡æ–¹æ ¹å½’ä¸€åŒ–å’Œæ—‹è½¬ä½ç½®åµŒå…¥åˆ°æ–‡æœ¬è§£ç å™¨å’Œè§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ç¼–ç å™¨ã€‚åœ¨LLMä¸­ï¼Œæ¯é¡¹æ¶æ„ä¿®æ”¹éƒ½è¢«è¯æ˜å¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½†å°šæœªåœ¨CoCaä¸­ä½¿ç”¨ã€‚æˆ‘ä»¬å°†GRR-CoCaä¸å¸¦æœ‰ç›¸åŒä¿®æ”¹æ–‡æœ¬è§£ç å™¨ä½†ä½¿ç”¨CoCaåŸå§‹ViTç¼–ç å™¨çš„åŸºçº¿CoCaè¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çš„é¢„è®­ç»ƒå’Œå¾®è°ƒå·¥ä½œæµç¨‹ï¼Œåœ¨å¯¹æ¯”å’Œç”Ÿæˆä»»åŠ¡ä¸Šå¯¹æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬çš„GRR-CoCaåœ¨é¢„è®­ç»ƒæ•°æ®é›†å’Œä¸‰ç»„ä¸åŒçš„å¾®è°ƒæ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºåŸºçº¿CoCaã€‚é¢„è®­ç»ƒæ”¹è¿›åŒ…æ‹¬å¯¹æ¯”æŸå¤±é™ä½27.25%ï¼Œå›°æƒ‘åº¦é™ä½3.71%ï¼ŒCoCaæŸå¤±é™ä½7.15%ã€‚å¹³å‡å¾®è°ƒæ”¹è¿›åŒ…æ‹¬å¯¹æ¯”æŸå¤±é™ä½13.66%ï¼Œå›°æƒ‘åº¦é™ä½5.18%ï¼ŒCoCaæŸå¤±é™ä½5.55%ã€‚æˆ‘ä»¬è¯æ˜GRR-CoCaçš„æ”¹è¿›æ¶æ„æé«˜äº†æ€§èƒ½å’Œè·¨è§†è§‰è¯­è¨€é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18009v1">PDF</a> 12 pages, 2 figures</p>
<p><strong>æ‘˜è¦</strong><br>GRR-CoCaæ¨¡å‹åœ¨SOTAå›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä¸­å¼•å…¥äº†å¤šé¡¹æŠ€æœ¯æ”¹è¿›ï¼Œå¦‚é«˜æ–¯è¯¯å·®é—¨æ§çº¿æ€§å•å…ƒã€å‡æ–¹æ ¹å½’ä¸€åŒ–å’Œæ—‹è½¬ä½ç½®åµŒå…¥ç­‰ï¼Œè¿™äº›æŠ€æœ¯æ”¹è¿›æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGRR-CoCaåœ¨é¢„è®­ç»ƒæ•°æ®é›†å’Œä¸‰ä¸ªä¸åŒçš„å¾®è°ƒæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºåŸºçº¿CoCaæ¨¡å‹ã€‚è¿™äº›æ”¹è¿›åœ¨å¯¹æ¯”æŸå¤±ã€å›°æƒ‘åº¦å’ŒCoCaæŸå¤±ç­‰æ–¹é¢éƒ½æœ‰æ˜¾è‘—çš„æå‡ã€‚æœ¬ç ”ç©¶è¡¨æ˜GRR-CoCaçš„æ”¹è¿›æ¶æ„æé«˜äº†æ€§èƒ½å’Œè·¨è§†è§‰è¯­è¨€é¢†åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>GRR-CoCaæ¨¡å‹æ˜¯å¯¹ç°æœ‰SOTAå›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„æ”¹è¿›ï¼Œå®ƒç»“åˆäº†å¤šé¡¹æŠ€æœ¯æå‡ï¼Œå¦‚é«˜æ–¯è¯¯å·®é—¨æ§çº¿æ€§å•å…ƒç­‰ã€‚</li>
<li>ä¸åŸºçº¿CoCaæ¨¡å‹ç›¸æ¯”ï¼ŒGRR-CoCaæ¨¡å‹åœ¨é¢„è®­ç»ƒæ•°æ®é›†ä¸Šçš„è¡¨ç°æœ‰æ˜¾è‘—æå‡ï¼Œå¯¹æ¯”æŸå¤±é™ä½äº†27.25%ï¼Œå›°æƒ‘åº¦é™ä½äº†3.71%ï¼ŒCoCaæŸå¤±é™ä½äº†7.15%ã€‚</li>
<li>åœ¨å¾®è°ƒæ•°æ®é›†ä¸Šï¼ŒGRR-CoCaçš„å¹³å‡è¡¨ç°ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå¯¹æ¯”æŸå¤±å¹³å‡æé«˜äº†13.66%ï¼Œå›°æƒ‘åº¦å¹³å‡é™ä½äº†5.18%ï¼ŒCoCaæŸå¤±å¹³å‡é™ä½äº†5.55%ã€‚</li>
<li>GRR-CoCaæ¨¡å‹åœ¨è§†è§‰å’Œè¯­è¨€é¢†åŸŸçš„æ€§èƒ½æå‡å’Œæ³›åŒ–èƒ½åŠ›å¾—åˆ°äº†éªŒè¯ã€‚è¿™è¡¨æ˜æ”¹è¿›åçš„æ¶æ„èƒ½å¤Ÿæœ‰æ•ˆåœ°æé«˜æ¨¡å‹çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>æ­¤ç ”ç©¶å±•ç¤ºäº†å¦‚ä½•å°†ä¸€äº›åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­è¡¨ç°è‰¯å¥½çš„æŠ€æœ¯æ”¹è¿›åº”ç”¨åˆ°å›¾åƒå’Œæ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œè¿™ä¸ºè¿›ä¸€æ­¥æ”¹è¿›ç›¸å…³ç ”ç©¶æä¾›äº†æ–°æ€è·¯ã€‚</li>
<li>GRR-CoCaæ¨¡å‹çš„å¼•å…¥æœ‰åŠ©äºç¼©å°å½“å‰æœ€å…ˆè¿›çš„å¤šåª’ä½“æ¨¡å‹ä¸å½“ä»£å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹é—´çš„æ¶æ„å·®è·ã€‚è¿™å¯¹äºåˆ›å»ºæ›´å¤æ‚çš„å¤šåª’ä½“åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18009">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-197b5ab89986c83ecd628fc448a57801.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f911a5f3139d1ef9edbfd4a649723d3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9224bea5a72931738344001e2444ad1e.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Dual-branch-Prompting-for-Multimodal-Machine-Translation"><a href="#Dual-branch-Prompting-for-Multimodal-Machine-Translation" class="headerlink" title="Dual-branch Prompting for Multimodal Machine Translation"></a>Dual-branch Prompting for Multimodal Machine Translation</h2><p><strong>Authors:Jie Wang, Zhendong Yang, Liansong Zong, Xiaobo Zhang, Dexian Wang, Ji Zhang</strong></p>
<p>Multimodal Machine Translation (MMT) typically enhances text-only translation by incorporating aligned visual features. Despite the remarkable progress, state-of-the-art MMT approaches often rely on paired image-text inputs at inference and are sensitive to irrelevant visual noise, which limits their robustness and practical applicability. To address these issues, we propose D2P-MMT, a diffusion-based dual-branch prompting framework for robust vision-guided translation. Specifically, D2P-MMT requires only the source text and a reconstructed image generated by a pre-trained diffusion model, which naturally filters out distracting visual details while preserving semantic cues. During training, the model jointly learns from both authentic and reconstructed images using a dual-branch prompting strategy, encouraging rich cross-modal interactions. To bridge the modality gap and mitigate training-inference discrepancies, we introduce a distributional alignment loss that enforces consistency between the output distributions of the two branches. Extensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves superior translation performance compared to existing state-of-the-art approaches. </p>
<blockquote>
<p>å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ï¼ˆMMTï¼‰é€šå¸¸é€šè¿‡èå…¥å¯¹é½çš„è§†è§‰ç‰¹å¾æ¥å¢å¼ºçº¯æ–‡æœ¬ç¿»è¯‘ã€‚å°½ç®¡å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†æœ€å…ˆè¿›çš„MMTæ–¹æ³•é€šå¸¸åœ¨æ¨ç†é˜¶æ®µä¾èµ–äºé…å¯¹çš„å›¾åƒæ–‡æœ¬è¾“å…¥ï¼Œå¹¶å¯¹ä¸ç›¸å…³çš„è§†è§‰å™ªå£°æ•æ„Ÿï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„ç¨³å¥æ€§å’Œå®é™…åº”ç”¨èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†D2P-MMTï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„åŒåˆ†æ”¯æç¤ºæ¡†æ¶ï¼Œç”¨äºå®ç°ç¨³å¥çš„è§†è§‰å¼•å¯¼ç¿»è¯‘ã€‚å…·ä½“æ¥è¯´ï¼ŒD2P-MMTä»…éœ€è¦æºæ–‡æœ¬å’Œç”±é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é‡å»ºå›¾åƒï¼Œè¿™å¯ä»¥è‡ªç„¶åœ°è¿‡æ»¤æ‰ä»¤äººåˆ†å¿ƒçš„è§†è§‰ç»†èŠ‚ï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰çº¿ç´¢ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä½¿ç”¨åŒåˆ†æ”¯æç¤ºç­–ç•¥ä»çœŸå®å’Œé‡å»ºçš„å›¾åƒä¸­å­¦ä¹ ï¼Œé¼“åŠ±ä¸°å¯Œçš„è·¨æ¨¡æ€äº¤äº’ã€‚ä¸ºäº†ç¼©å°æ¨¡æ€å·®è·å¹¶å‡è½»è®­ç»ƒä¸æ¨ç†ä¹‹é—´çš„å·®å¼‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†å¸ƒå¯¹é½æŸå¤±ï¼Œä»¥å¼ºåˆ¶ä¸¤ä¸ªåˆ†æ”¯çš„è¾“å‡ºåˆ†å¸ƒä¿æŒä¸€è‡´ã€‚åœ¨Multi30Kæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼ŒD2P-MMTå®ç°äº†å“è¶Šçš„ç¿»è¯‘æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17588v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ï¼ˆMMTï¼‰é€šè¿‡å¼•å…¥å¯¹é½çš„è§†è§‰ç‰¹å¾æ¥å¢å¼ºçº¯æ–‡æœ¬ç¿»è¯‘ã€‚ç„¶è€Œï¼Œæœ€å…ˆè¿›çš„MMTæ–¹æ³•é€šå¸¸ä¾èµ–äºæ¨ç†é˜¶æ®µçš„é…å¯¹å›¾åƒæ–‡æœ¬è¾“å…¥ï¼Œå¹¶å¯¹ä¸ç›¸å…³çš„è§†è§‰å™ªå£°æ•æ„Ÿï¼Œè¿™é™åˆ¶äº†å…¶ç¨³å¥æ€§å’Œå®é™…åº”ç”¨ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ‰©æ•£çš„åŒåˆ†æ”¯æç¤ºæ¡†æ¶D2P-MMTï¼Œç”¨äºç¨³å¥çš„è§†è§‰æŒ‡å¯¼ç¿»è¯‘ã€‚å…·ä½“è€Œè¨€ï¼ŒD2P-MMTä»…éœ€è¦æºæ–‡æœ¬å’Œç”±é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é‡æ„å›¾åƒï¼Œè¯¥å›¾åƒå¯è‡ªç„¶è¿‡æ»¤æ‰ä»¤äººåˆ†å¿ƒçš„è§†è§‰ç»†èŠ‚ï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰çº¿ç´¢ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹é‡‡ç”¨åŒåˆ†æ”¯æç¤ºç­–ç•¥ï¼Œä»çœŸå®å’Œé‡æ„çš„å›¾åƒä¸­å­¦ä¹ ï¼Œä¿ƒè¿›ä¸°å¯Œçš„è·¨æ¨¡æ€äº¤äº’ã€‚ä¸ºäº†ç¼©å°æ¨¡æ€å·®è·å¹¶å‡è½»è®­ç»ƒä¸æ¨ç†ä¹‹é—´çš„å·®å¼‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†å¸ƒå¯¹é½æŸå¤±ï¼Œä»¥å¼ºåˆ¶ä¸¤ä¸ªåˆ†æ”¯çš„è¾“å‡ºåˆ†å¸ƒä¿æŒä¸€è‡´ã€‚åœ¨Multi30Kæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒD2P-MMTç›¸è¾ƒäºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•å®ç°äº†æ›´ä¼˜è¶Šçš„ç¿»è¯‘æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ï¼ˆMMTï¼‰ç»“åˆäº†æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾ä»¥å¢å¼ºç¿»è¯‘æ€§èƒ½ã€‚</li>
<li>å½“å‰MMTæ–¹æ³•å¯¹å›¾åƒæ–‡æœ¬è¾“å…¥çš„é…å¯¹ä¾èµ–æ€§å¼ºï¼Œä¸”å¯¹è§†è§‰å™ªå£°æ•æ„Ÿã€‚</li>
<li>D2P-MMTæ¡†æ¶é€šè¿‡æ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é‡æ„å›¾åƒè¿›è¡Œç¿»è¯‘ï¼Œä»¥æé«˜ç¨³å¥æ€§ã€‚</li>
<li>D2P-MMTåˆ©ç”¨åŒåˆ†æ”¯æç¤ºç­–ç•¥è®­ç»ƒæ¨¡å‹ï¼Œä¿ƒè¿›è·¨æ¨¡æ€äº¤äº’ã€‚</li>
<li>å¼•å…¥åˆ†å¸ƒå¯¹é½æŸå¤±ä»¥ç¼©å°æ¨¡æ€å·®è·å’Œè®­ç»ƒä¸æ¨ç†å·®å¼‚ã€‚</li>
<li>D2P-MMTåœ¨Multi30Kæ•°æ®é›†ä¸Šå®ç°äº†ä¼˜è¶Šçš„ç¿»è¯‘æ€§èƒ½ã€‚</li>
<li>D2P-MMTæ–¹æ³•æœ‰åŠ©äºæ¨åŠ¨æœºå™¨ç¿»è¯‘é¢†åŸŸçš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å«å™ªå£°æˆ–å¤æ‚è§†è§‰ä¿¡æ¯çš„åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d66b5c937bd791753554dbb71b3ae77.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e3fd40b9f8326a2764be192664dc64a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8aa4d08019ca21c0fb06fa5cc13801f1.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="PIG-Nav-Key-Insights-for-Pretrained-Image-Goal-Navigation-Models"><a href="#PIG-Nav-Key-Insights-for-Pretrained-Image-Goal-Navigation-Models" class="headerlink" title="PIG-Nav: Key Insights for Pretrained Image Goal Navigation Models"></a>PIG-Nav: Key Insights for Pretrained Image Goal Navigation Models</h2><p><strong>Authors:Jiansong Wan, Chengming Zhou, Jinkua Liu, Xiangge Huang, Xiaoyu Chen, Xiaohan Yi, Qisen Yang, Baiting Zhu, Xin-Qiang Cai, Lixing Liu, Rushuai Yang, Chuheng Zhang, Sherif Abdelfattah, Hayong Shin, Pushi Zhang, Li Zhao, Jiang Bian</strong></p>
<p>Recent studies have explored pretrained (foundation) models for vision-based robotic navigation, aiming to achieve generalizable navigation and positive transfer across diverse environments while enhancing zero-shot performance in unseen settings. In this work, we introduce PIG-Nav (Pretrained Image-Goal Navigation), a new approach that further investigates pretraining strategies for vision-based navigation models and contributes in two key areas. Model-wise, we identify two critical design choices that consistently improve the performance of pretrained navigation models: (1) integrating an early-fusion network structure to combine visual observations and goal images via appropriately pretrained Vision Transformer (ViT) image encoder, and (2) introducing suitable auxiliary tasks to enhance global navigation representation learning, thus further improving navigation performance. Dataset-wise, we propose a novel data preprocessing pipeline for efficiently labeling large-scale game video datasets for navigation model training. We demonstrate that augmenting existing open navigation datasets with diverse gameplay videos improves model performance. Our model achieves an average improvement of 22.6% in zero-shot settings and a 37.5% improvement in fine-tuning settings over existing visual navigation foundation models in two complex simulated environments and one real-world environment. These results advance the state-of-the-art in pretrained image-goal navigation models. Notably, our model maintains competitive performance while requiring significantly less fine-tuning data, highlighting its potential for real-world deployment with minimal labeled supervision. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶æ¢è®¨äº†åŸºäºè§†è§‰çš„æœºå™¨äººå¯¼èˆªçš„é¢„è®­ç»ƒï¼ˆåŸºç¡€ï¼‰æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°åœ¨ä¸åŒç¯å¢ƒä¸‹çš„å¯æ¨å¹¿å¯¼èˆªå’Œæ­£å‘è¿ç§»ï¼ŒåŒæ—¶æé«˜åœ¨æœªè§åœºæ™¯ä¸­çš„é›¶æ ·æœ¬æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†PIG-Navï¼ˆé¢„è®­ç»ƒå›¾åƒç›®æ ‡å¯¼èˆªï¼‰ï¼Œè¿™æ˜¯ä¸€ç§è¿›ä¸€æ­¥æ¢ç´¢åŸºäºè§†è§‰çš„å¯¼èˆªæ¨¡å‹çš„é¢„è®­ç»ƒç­–ç•¥çš„æ–°æ–¹æ³•ï¼Œå¹¶åœ¨ä¸¤ä¸ªå…³é”®é¢†åŸŸæœ‰æ‰€è´¡çŒ®ã€‚åœ¨æ¨¡å‹æ–¹é¢ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸¤ç§å…³é”®çš„è®¾è®¡é€‰æ‹©ï¼Œè¿™äº›é€‰æ‹©ä¸€è‡´åœ°æé«˜äº†é¢„è®­ç»ƒå¯¼èˆªæ¨¡å‹çš„æ€§èƒ½ï¼šï¼ˆ1ï¼‰é‡‡ç”¨æ—©æœŸèåˆç½‘ç»œç»“æ„ï¼Œé€šè¿‡é€‚å½“é¢„è®­ç»ƒçš„Vision Transformerï¼ˆViTï¼‰å›¾åƒç¼–ç å™¨å°†è§†è§‰è§‚å¯Ÿå’Œç›®æ ‡å›¾åƒç›¸ç»“åˆï¼›ï¼ˆ2ï¼‰å¼•å…¥åˆé€‚çš„è¾…åŠ©ä»»åŠ¡ä»¥å¢å¼ºå…¨å±€å¯¼èˆªè¡¨ç¤ºå­¦ä¹ ï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜å¯¼èˆªæ€§èƒ½ã€‚åœ¨æ•°æ®é›†æ–¹é¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé«˜æ•ˆæ ‡æ³¨å¤§è§„æ¨¡æ¸¸æˆè§†é¢‘æ•°æ®é›†çš„æ–°æ•°æ®é¢„å¤„ç†ç®¡é“ï¼Œç”¨äºè®­ç»ƒå¯¼èˆªæ¨¡å‹ã€‚æˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡å¢å¼ºç°æœ‰å¼€æ”¾å¯¼èˆªæ•°æ®é›†ä¸å¤šç§æ¸¸æˆè§†é¢‘çš„èåˆï¼Œå¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸¤ä¸ªå¤æ‚çš„æ¨¡æ‹Ÿç¯å¢ƒå’Œä¸€ä¸ªçœŸå®ä¸–ç•Œç¯å¢ƒä¸­ï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸Šå¹³å‡æé«˜äº†22.6%ï¼Œåœ¨å¾®è°ƒè®¾ç½®ä¸Šæé«˜äº†37.5%ï¼Œè¶…è¶Šäº†ç°æœ‰è§†è§‰å¯¼èˆªåŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™äº›ç»“æœä»£è¡¨äº†é¢„è®­ç»ƒå›¾åƒç›®æ ‡å¯¼èˆªæ¨¡å‹çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨éœ€è¦æ›´å°‘çš„å¾®è°ƒæ•°æ®çš„åŒæ—¶ä¿æŒäº†ç«äº‰åŠ›ï¼Œè¿™å‡¸æ˜¾äº†å…¶åœ¨æœ€å°æ ‡æ³¨ç›‘ç£ä¸‹åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17220v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†PIG-Navï¼ˆé¢„è®­ç»ƒå›¾åƒç›®æ ‡å¯¼èˆªï¼‰æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è¿›ä¸€æ­¥ç ”ç©¶åŸºäºè§†è§‰çš„å¯¼èˆªæ¨¡å‹çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œå¹¶åœ¨ä¸¤ä¸ªå…³é”®é¢†åŸŸåšå‡ºè´¡çŒ®ã€‚æ¨¡å‹æ–¹é¢ï¼Œé€šè¿‡æ•´åˆæ—©æœŸèåˆç½‘ç»œç»“æ„å’Œå¼•å…¥åˆé€‚çš„è¾…åŠ©ä»»åŠ¡æ¥æé«˜é¢„è®­ç»ƒå¯¼èˆªæ¨¡å‹çš„æ€§èƒ½ã€‚æ•°æ®é›†æ–¹é¢ï¼Œæå‡ºäº†ç”¨äºå¯¼èˆªæ¨¡å‹è®­ç»ƒçš„å¤§å‹æ¸¸æˆè§†é¢‘æ•°æ®é›†çš„æ–°å‹æ•°æ®é¢„å¤„ç†ç®¡é“ï¼Œå¹¶è¯æ˜å…¶å¯¹æ¨¡å‹æ€§èƒ½çš„æ”¹è¿›ã€‚ç›¸è¾ƒäºç°æœ‰çš„è§†è§‰å¯¼èˆªé¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®ä¸–ç•Œç¯å¢ƒä¸­å‡æœ‰æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬å’Œå¾®è°ƒåœºæ™¯ä¸‹ã€‚è¿™è¡¨æ˜PIG-Navæœ‰æœ›åœ¨å®é™…åº”ç”¨ä¸­å®ç°æ›´é«˜æ•ˆå’Œç²¾ç¡®çš„å¯¼èˆªã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PIG-Navæ˜¯ä¸€ä¸ªé’ˆå¯¹å›¾åƒç›®æ ‡å¯¼èˆªçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ƒé€šè¿‡æ¢ç´¢æ–°çš„é¢„è®­ç»ƒç­–ç•¥æ¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åœ¨æ¨¡å‹æ–¹é¢ï¼Œå¼•å…¥æ—©æœŸèåˆç½‘ç»œç»“æ„ä»¥åŠåˆé€‚çš„è¾…åŠ©ä»»åŠ¡èƒ½å¤Ÿæå‡é¢„è®­ç»ƒå¯¼èˆªæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åœ¨æ•°æ®é›†æ–¹é¢ï¼Œé€šè¿‡æå‡ºæ–°çš„æ•°æ®é¢„å¤„ç†ç®¡é“æ¥é«˜æ•ˆæ ‡æ³¨å¤§å‹æ¸¸æˆè§†é¢‘æ•°æ®é›†ç”¨äºè®­ç»ƒå¯¼èˆªæ¨¡å‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–è§†è§‰å¯¼èˆªé¢„è®­ç»ƒæ¨¡å‹ç›¸æ¯”ï¼ŒPIG-Navåœ¨å¤æ‚æ¨¡æ‹Ÿç¯å¢ƒå’ŒçœŸå®ä¸–ç•Œç¯å¢ƒä¸­å‡æœ‰æ˜¾è‘—æé«˜çš„å¯¼èˆªæ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨é›¶æ ·æœ¬å’Œå¾®è°ƒåœºæ™¯ä¸‹ï¼Œå¹³å‡æå‡äº†22.6%å’Œ37.5%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17220">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fe3b4299e5d6975a93ccd3f518b973dd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11c98fcd26ad043897b4c3e219f5dd5e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8fb0b0329d2485e81aa208934b352de9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77f1f636351a1118918013b2e8e1cae7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Look-Focus-Act-Efficient-and-Robust-Robot-Learning-via-Human-Gaze-and-Foveated-Vision-Transformers"><a href="#Look-Focus-Act-Efficient-and-Robust-Robot-Learning-via-Human-Gaze-and-Foveated-Vision-Transformers" class="headerlink" title="Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and   Foveated Vision Transformers"></a>Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and   Foveated Vision Transformers</h2><p><strong>Authors:Ian Chuang, Andrew Lee, Dechen Gao, Jinyu Zou, Iman Soltani</strong></p>
<p>Human vision is a highly active process driven by gaze, which directs attention and fixation to task-relevant regions and dramatically reduces visual processing. In contrast, robot learning systems typically rely on passive, uniform processing of raw camera images. In this work, we explore how incorporating human-like active gaze into robotic policies can enhance both efficiency and performance. We build on recent advances in foveated image processing and apply them to an Active Vision robot system that emulates both human head movement and eye tracking. Extending prior work on the AV-ALOHA robot simulation platform, we introduce a framework for simultaneously collecting eye-tracking data and robot demonstrations from a human operator as well as a simulation benchmark and dataset for training robot policies that incorporate human gaze. Given the widespread use of Vision Transformers (ViTs) in robot learning, we integrate gaze information into ViTs using a foveated patch tokenization scheme inspired by recent work in image segmentation. Compared to uniform patch tokenization, this significantly reduces the number of tokens-and thus computation-without sacrificing visual fidelity near regions of interest. We also explore two approaches to gaze imitation and prediction from human data. The first is a two-stage model that predicts gaze to guide foveation and action; the second integrates gaze into the action space, allowing the policy to jointly predict gaze and actions end-to-end. Our results show that our method for foveated robot vision not only drastically reduces computational overhead, but also improves performance for high precision tasks and robustness to unseen distractors. Together, these findings suggest that human-inspired visual processing offers a useful inductive bias for robotic vision systems. <a target="_blank" rel="noopener" href="https://ian-chuang.github.io/gaze-av-aloha/">https://ian-chuang.github.io/gaze-av-aloha/</a> </p>
<blockquote>
<p>äººç±»è§†è§‰æ˜¯ä¸€ä¸ªç”±ç›®å…‰é©±åŠ¨çš„ç§¯æè¿‡ç¨‹ï¼Œå®ƒèƒ½å¤Ÿå°†æ³¨æ„åŠ›é›†ä¸­åœ¨ä¸ä»»åŠ¡ç›¸å…³çš„åŒºåŸŸï¼Œå¹¶æå¤§åœ°å‡å°‘è§†è§‰å¤„ç†ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæœºå™¨äººçš„å­¦ä¹ ç³»ç»Ÿé€šå¸¸ä¾èµ–äºå¯¹åŸå§‹ç›¸æœºå›¾åƒçš„è¢«åŠ¨å‡åŒ€å¤„ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å°†äººç±»å¼çš„ä¸»åŠ¨ç›®å…‰èå…¥æœºå™¨äººç­–ç•¥å¦‚ä½•å¢å¼ºæ•ˆç‡å’Œæ€§èƒ½ã€‚æˆ‘ä»¬åŸºäºæœ€æ–°çš„ç„¦ç‚¹å›¾åƒå¤„ç†æŠ€æœ¯çš„è¿›å±•ï¼Œå°†å…¶åº”ç”¨äºæ¨¡æ‹Ÿäººç±»å¤´éƒ¨è¿åŠ¨å’Œçœ¼çƒè¿½è¸ªçš„ä¸»åŠ¨è§†è§‰æœºå™¨äººç³»ç»Ÿã€‚åœ¨AV-ALOHAæœºå™¨äººä»¿çœŸå¹³å°çš„å‰æœŸå·¥ä½œåŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶ä»äººç±»æ“ä½œè€…é‚£é‡Œæ”¶é›†çœ¼åŠ¨æ•°æ®å’Œæœºå™¨äººæ¼”ç¤ºæ•°æ®ï¼Œä»¥åŠä¸€ä¸ªæ¨¡æ‹ŸåŸºå‡†å’Œç”¨äºè®­ç»ƒèå…¥äººç±»ç›®å…‰çš„æœºå™¨äººç­–ç•¥çš„æ•°æ®é›†ã€‚è€ƒè™‘åˆ°Vision Transformersï¼ˆViTsï¼‰åœ¨æœºå™¨äººå­¦ä¹ ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œæˆ‘ä»¬å°†ç›®å…‰ä¿¡æ¯èå…¥ViTsï¼Œé‡‡ç”¨ä¸€ç§å—æœ€æ–°å›¾åƒåˆ†å‰²å·¥ä½œå¯å‘çš„ç„¦ç‚¹è¡¥ä¸æ ‡è®°æ–¹æ¡ˆã€‚ä¸å‡åŒ€çš„è¡¥ä¸æ ‡è®°ç›¸æ¯”ï¼Œè¿™æ˜¾è‘—å‡å°‘äº†æ ‡è®°çš„æ•°é‡ï¼ˆå› æ­¤ä¹Ÿå‡å°‘äº†è®¡ç®—ï¼‰ï¼ŒåŒæ—¶ä¸ä¼šç‰ºç‰²æ„Ÿå…´è¶£åŒºåŸŸçš„è§†è§‰ä¿çœŸåº¦ã€‚æˆ‘ä»¬è¿˜æ¢ç´¢äº†ä¸¤ç§ä»äººç±»æ•°æ®ä¸­æ¨¡ä»¿å’Œé¢„æµ‹ç›®å…‰çš„æ–¹æ³•ã€‚ç¬¬ä¸€ç§æ˜¯é¢„æµ‹ç›®å…‰ä»¥æŒ‡å¯¼ç„¦ç‚¹å’Œè¡ŒåŠ¨çš„ä¸¤é˜¶æ®µæ¨¡å‹ï¼›ç¬¬äºŒç§æ˜¯å°†ç›®å…‰æ•´åˆåˆ°åŠ¨ä½œç©ºé—´ï¼Œä½¿ç­–ç•¥èƒ½å¤Ÿç«¯åˆ°ç«¯åœ°é¢„æµ‹ç›®å…‰å’ŒåŠ¨ä½œã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æœºå™¨äººè§†è§‰æ–¹æ³•ä¸ä»…æå¤§åœ°å‡å°‘äº†è®¡ç®—å¼€é”€ï¼Œè€Œä¸”æé«˜äº†é«˜ç²¾åº¦ä»»åŠ¡çš„æ€§èƒ½å’Œæœªè§å¹²æ‰°é¡¹çš„ç¨³å¥æ€§ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›å‘ç°è¡¨æ˜ï¼Œäººç±»å¯å‘çš„è§†è§‰å¤„ç†ä¸ºæœºå™¨äººè§†è§‰ç³»ç»Ÿæä¾›äº†æœ‰ç”¨çš„å…ˆéªŒçŸ¥è¯†ã€‚è¯¦æƒ…è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://ian-chuang.github.io/gaze-av-aloha/">https://ian-chuang.github.io/gaze-av-aloha/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15833v1">PDF</a> 13 pages, 10 figures</p>
<p><strong>Summary</strong><br>æœºå™¨äººè§†è§‰ç³»ç»Ÿé€šå¸¸ä¾èµ–äºè¢«åŠ¨ã€å‡åŒ€å¤„ç†åŸå§‹ç›¸æœºå›¾åƒï¼Œä¸äººç±»æ´»è·ƒã€é€‰æ‹©æ€§å…³æ³¨çš„è§†è§‰å¤„ç†æ–¹å¼å½¢æˆå¯¹æ¯”ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†å°†äººç±»å¼çš„ä¸»åŠ¨æ³¨è§†èå…¥æœºå™¨äººç­–ç•¥ï¼Œä»¥æé«˜æ•ˆç‡å’Œæ€§èƒ½ã€‚ç ”ç©¶åŸºäºæœ€è¿‘å…³äºå‡è§†å›¾åƒå¤„ç†çš„è¿›å±•ï¼Œå°†å…¶åº”ç”¨äºæ¨¡æ‹Ÿäººç±»å¤´éƒ¨è¿åŠ¨å’Œçœ¼çƒè¿½è¸ªçš„ä¸»åŠ¨è§†è§‰æœºå™¨äººç³»ç»Ÿã€‚ç ”ç©¶æ‰©å±•äº†AV-ALOHAæœºå™¨äººä»¿çœŸå¹³å°ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶æ”¶é›†æ¥è‡ªäººç±»æ“ä½œå‘˜çš„çœ¼åŠ¨æ•°æ®å’Œæœºå™¨äººæ¼”ç¤ºæ•°æ®ï¼Œä»¥åŠç”¨äºè®­ç»ƒèå…¥äººç±»æ³¨è§†çš„æœºå™¨äººç­–ç•¥ä»¿çœŸåŸºå‡†å’Œæ•°æ®é›†ã€‚ç»“åˆæœºå™¨äººå­¦ä¹ ä¸­å¹¿æ³›ä½¿ç”¨çš„Vision Transformersï¼ˆViTsï¼‰ï¼Œç ”ç©¶é€šè¿‡ä¸€ç§å—å›¾åƒåˆ†å‰²æœ€æ–°å·¥ä½œå¯å‘çš„å‡è§†æ–‘å—æ ‡è®°æ–¹æ¡ˆï¼Œå°†æ³¨è§†ä¿¡æ¯èå…¥ViTsã€‚ä¸å‡åŒ€æ–‘å—æ ‡è®°ç›¸æ¯”ï¼Œè¿™æ˜¾è‘—å‡å°‘äº†ä»¤ç‰Œæ•°é‡ï¼Œä»è€Œå‡å°‘äº†è®¡ç®—é‡ï¼ŒåŒæ—¶åœ¨ä¸ç‰ºç‰²æ„Ÿå…´è¶£åŒºåŸŸé™„è¿‘è§†è§‰ä¿çœŸåº¦çš„æƒ…å†µä¸‹å®ç°äº†è¿™ä¸€ç‚¹ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜æ¢ç´¢äº†ä¸¤ç§ä»äººç±»æ•°æ®ä¸­æ¨¡ä»¿å’Œé¢„æµ‹æ³¨è§†çš„æ–¹æ³•ã€‚ç¬¬ä¸€ç§æ˜¯é¢„æµ‹æ³¨è§†ä»¥æŒ‡å¯¼è§‚å¯Ÿå’Œè¡ŒåŠ¨çš„ä¸¤é˜¶æ®µæ¨¡å‹ï¼›ç¬¬äºŒç§æ˜¯å°†æ³¨è§†æ•´åˆåˆ°åŠ¨ä½œç©ºé—´ï¼Œä½¿ç­–ç•¥èƒ½å¤Ÿç«¯åˆ°ç«¯åœ°è”åˆé¢„æµ‹æ³¨è§†å’ŒåŠ¨ä½œã€‚ç»“æœä¸ä»…å¤§å¤§é™ä½äº†è®¡ç®—å¼€é”€ï¼Œè€Œä¸”åœ¨æ‰§è¡Œé«˜ç²¾å¯†ä»»åŠ¡å’Œåº”å¯¹æœªè§å¹²æ‰°ç‰©æ—¶æé«˜äº†æ€§èƒ½ã€‚è¿™è¡¨æ˜äººç±»å¯å‘å¼çš„è§†è§‰å¤„ç†ä¸ºæœºå™¨äººè§†è§‰ç³»ç»Ÿæä¾›äº†æœ‰ç”¨çš„å…ˆéªŒçŸ¥è¯†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººç±»è§†è§‰æ˜¯ä¸€ä¸ªæ´»è·ƒçš„è¿‡ç¨‹ï¼Œé€šè¿‡æ³¨è§†æ¥æŒ‡å¯¼æ³¨æ„åŠ›å’Œå›ºå®šä»»åŠ¡ç›¸å…³åŒºåŸŸï¼Œè¿™æ˜¾è‘—å‡å°‘äº†è§†è§‰å¤„ç†ã€‚</li>
<li>ç°æœ‰æœºå™¨äººç³»ç»Ÿå¤§å¤šä¾èµ–äºè¢«åŠ¨ã€å‡åŒ€çš„å›¾åƒå¤„ç†æ–¹å¼ï¼Œæœ¬ç ”ç©¶æ¢ç´¢äº†èå…¥äººç±»ä¸»åŠ¨æ³¨è§†çš„æ–¹å¼æ¥æé«˜æœºå™¨äººçš„æ•ˆç‡å’Œæ€§èƒ½ã€‚</li>
<li>é€šè¿‡ç»“åˆæœ€æ–°çš„å‡è§†å›¾åƒå¤„ç†æŠ€æœ¯å’ŒAV-ALOHAæœºå™¨äººä»¿çœŸå¹³å°ï¼Œç ”ç©¶å±•ç¤ºäº†å¦‚ä½•åŒæ—¶æ”¶é›†çœ¼åŠ¨æ•°æ®å’Œæœºå™¨äººæ¼”ç¤ºæ•°æ®ã€‚</li>
<li>å¼•å…¥äº†åŸºäºå‡è§†çš„æ–‘å—æ ‡è®°æ–¹æ¡ˆï¼Œå°†å…¶èå…¥Vision Transformersï¼ˆViTsï¼‰ï¼Œä»¥å‡å°‘è®¡ç®—é‡å¹¶ç»´æŒæ„Ÿå…´è¶£åŒºåŸŸçš„è§†è§‰ä¿çœŸåº¦ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯äº†èå…¥äººç±»æ³¨è§†ä¿¡æ¯çš„æœºå™¨äººè§†è§‰ç³»ç»Ÿå¯¹äºé«˜ç²¾å¯†ä»»åŠ¡å’Œåº”å¯¹æœªè§å¹²æ‰°ç‰©æ—¶çš„æ€§èƒ½æå‡ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸¤ç§ä»äººç±»æ•°æ®ä¸­æ¨¡ä»¿å’Œé¢„æµ‹æ³¨è§†çš„æ–¹æ³•ï¼šä¸€ç§æ˜¯ä¸¤é˜¶æ®µæ¨¡å‹ï¼Œå¦ä¸€ç§æ˜¯æ•´åˆæ³¨è§†åˆ°åŠ¨ä½œç©ºé—´çš„ç«¯åˆ°ç«¯è”åˆé¢„æµ‹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2068d1798060320aa12fb3d66d58fb7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-07471220baca9d5e8aac46cd41e50b16.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-56551439f1eae2f9e9c3d9f1574a6d39.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-72e03110f6130e2e6c6cb499ca6fe6df.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Smart-Eyes-for-Silent-Threats-VLMs-and-In-Context-Learning-for-THz-Imaging"><a href="#Smart-Eyes-for-Silent-Threats-VLMs-and-In-Context-Learning-for-THz-Imaging" class="headerlink" title="Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz   Imaging"></a>Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz   Imaging</h2><p><strong>Authors:Nicolas Poggi, Shashank Agnihotri, Margret Keuper</strong></p>
<p>Terahertz (THz) imaging enables non-invasive analysis for applications such as security screening and material classification, but effective image classification remains challenging due to limited annotations, low resolution, and visual ambiguity. We introduce In-Context Learning (ICL) with Vision-Language Models (VLMs) as a flexible, interpretable alternative that requires no fine-tuning. Using a modality-aligned prompting framework, we adapt two open-weight VLMs to the THz domain and evaluate them under zero-shot and one-shot settings. Our results show that ICL improves classification and interpretability in low-data regimes. This is the first application of ICL-enhanced VLMs to THz imaging, offering a promising direction for resource-constrained scientific domains. Code: \href{<a target="_blank" rel="noopener" href="https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main%7D%7BGitHub">https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub</a> repository}. </p>
<blockquote>
<p>å¤ªèµ«å…¹ï¼ˆTHzï¼‰æˆåƒèƒ½å¤Ÿå®ç°å®‰å…¨æ£€æŸ¥å’Œææ–™åˆ†ç±»ç­‰åº”ç”¨ä¸­çš„æ— åˆ›åˆ†æï¼Œä½†ç”±äºæ ‡æ³¨æœ‰é™ã€åˆ†è¾¨ç‡ä½å’Œè§†è§‰æ¨¡ç³Šï¼Œæœ‰æ•ˆçš„å›¾åƒåˆ†ç±»ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰ï¼Œä½œä¸ºä¸€ç§æ— éœ€ç²¾ç»†è°ƒæ•´çš„çµæ´»ã€å¯è§£é‡Šçš„æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡ä½¿ç”¨æ¨¡æ€å¯¹é½æç¤ºæ¡†æ¶ï¼Œæˆ‘ä»¬é€‚åº”äº†ä¸¤ä¸ªå¼€æºçš„VLMåˆ°å¤ªèµ«å…¹é¢†åŸŸï¼Œå¹¶åœ¨é›¶æ ·æœ¬å’Œå•æ ·æœ¬è®¾ç½®ä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼ŒICLåœ¨ä½æ•°æ®æƒ…å†µä¸‹æé«˜äº†åˆ†ç±»å’Œå¯è§£é‡Šæ€§ã€‚è¿™æ˜¯é¦–æ¬¡å°†å¢å¼ºå‹ICLåº”ç”¨äºå¤ªèµ«å…¹æˆåƒçš„VLMåº”ç”¨ï¼Œä¸ºèµ„æºå—é™çš„ç§‘å­¦é¢†åŸŸæä¾›äº†æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚ä»£ç ï¼š\href{<a target="_blank" rel="noopener" href="https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main%7D%7BGitHub%E4%BB%93%E5%BA%93%7D%E3%80%82">https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHubä»“åº“}ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15576v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤ªèµ«å…¹ï¼ˆTHzï¼‰æˆåƒåœ¨éä¾µå…¥æ€§åˆ†æä¸­çš„åº”ç”¨ï¼Œå¦‚å®‰æ£€å’Œç‰©æ–™åˆ†ç±»ã€‚é’ˆå¯¹THzæˆåƒä¸­ç”±äºæ ‡æ³¨æœ‰é™ã€åˆ†è¾¨ç‡ä½å’Œè§†è§‰æ¨¡ç³Šå¯¼è‡´çš„å›¾åƒåˆ†ç±»æŒ‘æˆ˜ï¼Œæœ¬æ–‡å¼•å…¥äº†åŸºäºè¯­å¢ƒå­¦ä¹ ï¼ˆICLï¼‰çš„è·¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½œä¸ºæ— éœ€ç²¾ç»†è°ƒæ•´çš„çµæ´»ã€å¯è§£é‡Šçš„æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡ä½¿ç”¨æ¨¡æ€å¯¹é½æç¤ºæ¡†æ¶ï¼Œæœ¬æ–‡é€‚åº”äº†ä¸¤ä¸ªå¼€æ”¾æƒé‡çš„VLMsåˆ°å¤ªèµ«å…¹é¢†åŸŸï¼Œå¹¶åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒICLåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹æé«˜äº†åˆ†ç±»å’Œè§£é‡Šæ€§ã€‚è¿™æ˜¯é¦–æ¬¡å°†ICLå¢å¼ºçš„VLMsåº”ç”¨äºå¤ªèµ«å…¹æˆåƒï¼Œä¸ºèµ„æºå—é™çš„ç§‘å­¦é¢†åŸŸæä¾›äº†æœ‰å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤ªèµ«å…¹ï¼ˆTHzï¼‰æˆåƒåœ¨å®‰æ£€å’Œç‰©æ–™åˆ†ç±»ç­‰éä¾µå…¥æ€§åˆ†æä¸­å…·æœ‰å¹¿æ³›åº”ç”¨ã€‚</li>
<li>ç”±äºæ ‡æ³¨æœ‰é™ã€åˆ†è¾¨ç‡ä½å’Œè§†è§‰æ¨¡ç³Šç­‰é—®é¢˜ï¼ŒTHzå›¾åƒåˆ†ç±»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥åŸºäºè¯­å¢ƒå­¦ä¹ ï¼ˆICLï¼‰çš„è·¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ä½œä¸ºè§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ¨¡æ€å¯¹é½æç¤ºæ¡†æ¶ç”¨äºé€‚åº”VLMsåˆ°å¤ªèµ«å…¹é¢†åŸŸã€‚</li>
<li>åœ¨é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬åœºæ™¯ä¸‹è¯„ä¼°äº†VLMsçš„è¡¨ç°ã€‚</li>
<li>ICLæé«˜äº†åœ¨æ•°æ®ç¨€ç¼ºæƒ…å†µä¸‹çš„åˆ†ç±»å’Œè§£é‡Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15576">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-abadf7b8b04d7d613ec9b858839919f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-93bf57e18a7b1d8c33e82b5387f1d25b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-214947f301fd8bc354340d21a7226a83.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7eb9de088484cbbeb6f55d8b81867dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f424530e689019c2f979add9ccdbff09.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Efficient-Adaptation-of-Pre-trained-Vision-Transformer-underpinned-by-Approximately-Orthogonal-Fine-Tuning-Strategy"><a href="#Efficient-Adaptation-of-Pre-trained-Vision-Transformer-underpinned-by-Approximately-Orthogonal-Fine-Tuning-Strategy" class="headerlink" title="Efficient Adaptation of Pre-trained Vision Transformer underpinned by   Approximately Orthogonal Fine-Tuning Strategy"></a>Efficient Adaptation of Pre-trained Vision Transformer underpinned by   Approximately Orthogonal Fine-Tuning Strategy</h2><p><strong>Authors:Yiting Yang, Hao Luo, Yuan Sun, Qingsen Yan, Haokui Zhang, Wei Dong, Guoqing Wang, Peng Wang, Yang Yang, Hengtao Shen</strong></p>
<p>A prevalent approach in Parameter-Efficient Fine-Tuning (PEFT) of pre-trained Vision Transformers (ViT) involves freezing the majority of the backbone parameters and solely learning low-rank adaptation weight matrices to accommodate downstream tasks. These low-rank matrices are commonly derived through the multiplication structure of down-projection and up-projection matrices, exemplified by methods such as LoRA and Adapter. In this work, we observe an approximate orthogonality among any two row or column vectors within any weight matrix of the backbone parameters; however, this property is absent in the vectors of the down&#x2F;up-projection matrices. Approximate orthogonality implies a reduction in the upper bound of the modelâ€™s generalization error, signifying that the model possesses enhanced generalization capability. If the fine-tuned down&#x2F;up-projection matrices were to exhibit this same property as the pre-trained backbone matrices, could the generalization capability of fine-tuned ViTs be further augmented? To address this question, we propose an Approximately Orthogonal Fine-Tuning (AOFT) strategy for representing the low-rank weight matrices. This strategy employs a single learnable vector to generate a set of approximately orthogonal vectors, which form the down&#x2F;up-projection matrices, thereby aligning the properties of these matrices with those of the backbone. Extensive experimental results demonstrate that our method achieves competitive performance across a range of downstream image classification tasks, confirming the efficacy of the enhanced generalization capability embedded in the down&#x2F;up-projection matrices. </p>
<blockquote>
<p>åœ¨é¢„è®­ç»ƒçš„Vision Transformerï¼ˆViTï¼‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ä¸­ï¼Œä¸€ç§æ™®éçš„æ–¹æ³•æ˜¯å†»ç»“å¤§éƒ¨åˆ†ä¸»å¹²å‚æ•°ï¼Œåªå­¦ä¹ ä½ç§©é€‚åº”æƒé‡çŸ©é˜µï¼Œä»¥é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚è¿™äº›ä½ç§©çŸ©é˜µé€šå¸¸é€šè¿‡ä¸‹æŠ•å½±å’Œä¸ŠæŠ•å½±çŸ©é˜µçš„ä¹˜æ³•ç»“æ„å¾—åˆ°ï¼Œä¾‹å¦‚LoRAå’ŒAdapterç­‰æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°é¢„è®­ç»ƒä¸»å¹²å‚æ•°çš„ä»»ä½•æƒé‡çŸ©é˜µä¸­çš„ä»»ä½•ä¸¤ä¸ªè¡Œæˆ–åˆ—å‘é‡ä¹‹é—´çš„å¤§è‡´æ­£äº¤æ€§ï¼Œç„¶è€Œè¿™ä¸€ç‰¹æ€§åœ¨ä¸‹&#x2F;ä¸ŠæŠ•å½±çŸ©é˜µçš„å‘é‡ä¸­å¹¶ä¸å­˜åœ¨ã€‚è¿‘ä¼¼æ­£äº¤æ€§æ„å‘³ç€æ¨¡å‹æ³›åŒ–è¯¯å·®çš„ä¸Šç•Œé™ä½ï¼Œè¡¨æ˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å¢å¼ºã€‚å¦‚æœå¾®è°ƒçš„ä¸‹&#x2F;ä¸ŠæŠ•å½±çŸ©é˜µèƒ½å¤Ÿå±•ç°å‡ºä¸é¢„è®­ç»ƒä¸»å¹²çŸ©é˜µç›¸åŒçš„ç‰¹æ€§ï¼Œé‚£ä¹ˆå¾®è°ƒåçš„ViTçš„æ³›åŒ–èƒ½åŠ›èƒ½å¦è¿›ä¸€æ­¥å¢å¼ºï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿‘ä¼¼æ­£äº¤å¾®è°ƒï¼ˆAOFTï¼‰ç­–ç•¥æ¥è¡¨ç¤ºä½ç§©æƒé‡çŸ©é˜µã€‚è¯¥ç­–ç•¥ä½¿ç”¨å•ä¸ªå¯å­¦ä¹ çš„å‘é‡ç”Ÿæˆä¸€ç»„è¿‘ä¼¼æ­£äº¤çš„å‘é‡ï¼Œè¿™äº›å‘é‡å½¢æˆä¸‹&#x2F;ä¸ŠæŠ•å½±çŸ©é˜µï¼Œä»è€Œä½¿è¿™äº›çŸ©é˜µçš„å±æ€§ä¸ä¸»å¹²å¯¹é½ã€‚å¤§é‡çš„å®éªŒç»“æœè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªä¸‹æ¸¸å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šå–å¾—äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œè¯å®äº†åµŒå…¥åœ¨ä¸‹&#x2F;ä¸ŠæŠ•å½±çŸ©é˜µä¸­çš„å¢å¼ºæ³›åŒ–èƒ½åŠ›çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13260v1">PDF</a> This paper is accepted by ICCV 2025</p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†é¢„è®­ç»ƒVision Transformerï¼ˆViTï¼‰çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ï¼Œä¸»è¦å†»ç»“å¤§éƒ¨åˆ†ä¸»å¹²å‚æ•°ï¼Œä»…å­¦ä¹ ä½ç§©é€‚åº”æƒé‡çŸ©é˜µä»¥é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚ç ”ç©¶ä¸­è§‚å¯Ÿåˆ°ä¸»å¹²å‚æ•°æƒé‡çŸ©é˜µä¸­çš„ä»»æ„ä¸¤ä¸ªè¡Œæˆ–åˆ—å‘é‡ä¹‹é—´å­˜åœ¨è¿‘ä¼¼æ­£äº¤æ€§ï¼Œè€Œè¿™ç§ç°è±¡åœ¨ä½ç§©çŸ©é˜µä¸­ç¼ºå¤±ã€‚è¿‘ä¼¼æ­£äº¤æ€§æœ‰åŠ©äºé™ä½æ¨¡å‹æ³›åŒ–è¯¯å·®çš„ä¸Šç•Œï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºAOFTï¼ˆè¿‘ä¼¼æ­£äº¤å¾®è°ƒï¼‰çš„ç­–ç•¥ï¼Œåˆ©ç”¨å•ä¸ªå¯å­¦ä¹ å‘é‡ç”Ÿæˆä¸€ç»„è¿‘ä¼¼æ­£äº¤çš„å‘é‡ï¼Œå½¢æˆä½ç§©æƒé‡çŸ©é˜µï¼Œä½¿è¿™äº›çŸ©é˜µçš„ç‰¹æ€§ä¸ä¸»å¹²ç›¸åŒ¹é…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªä¸‹æ¸¸å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒéªŒè¯äº†å¢å¼ºæ³›åŒ–èƒ½åŠ›åœ¨ä½ç§©çŸ©é˜µä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformeråœ¨å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ä¸­ä¸»è¦å†»ç»“ä¸»å¹²å‚æ•°ï¼Œä»…é€šè¿‡ä½ç§©é€‚åº”æƒé‡çŸ©é˜µé€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li>ä¸»å¹²å‚æ•°æƒé‡çŸ©é˜µä¸­çš„å‘é‡å­˜åœ¨è¿‘ä¼¼æ­£äº¤æ€§ã€‚</li>
<li>ä½ç§©é€‚åº”æƒé‡çŸ©é˜µï¼ˆå¦‚LoRAå’ŒAdapteræ–¹æ³•ï¼‰ä¸­çš„å‘é‡ç¼ºä¹è¿‘ä¼¼æ­£äº¤æ€§ã€‚</li>
<li>è¿‘ä¼¼æ­£äº¤æ€§æœ‰åŠ©äºé™ä½æ¨¡å‹æ³›åŒ–è¯¯å·®çš„ä¸Šç•Œï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åä¸ºAOFTçš„ç­–ç•¥ï¼Œåˆ©ç”¨å•ä¸ªå¯å­¦ä¹ å‘é‡ç”Ÿæˆè¿‘ä¼¼æ­£äº¤çš„å‘é‡ï¼Œå½¢æˆä½ç§©æƒé‡çŸ©é˜µã€‚</li>
<li>AOFTç­–ç•¥ä½¿ä½ç§©æƒé‡çŸ©é˜µçš„ç‰¹æ€§ä¸ä¸»å¹²å‚æ•°ç›¸åŒ¹é…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13260">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4bf0bf1f7f55f4cc83646f1cf03c6c37.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dfa9e4493740507ee1d8ebd69f9d319d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d7bff3e8a9ec7e189a0ff9db2af94bfa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a47b171babab883ff46240c13f54d8b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7931d9653d9ac5a5b9f9e338b80967e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d577135610fe730cd3f623abab88c64.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Leveraging-Language-Prior-for-Infrared-Small-Target-Detection"><a href="#Leveraging-Language-Prior-for-Infrared-Small-Target-Detection" class="headerlink" title="Leveraging Language Prior for Infrared Small Target Detection"></a>Leveraging Language Prior for Infrared Small Target Detection</h2><p><strong>Authors:Pranav Singh, Pravendra Singh</strong></p>
<p>IRSTD (InfraRed Small Target Detection) detects small targets in infrared blurry backgrounds and is essential for various applications. The detection task is challenging due to the small size of the targets and their sparse distribution in infrared small target datasets. Although existing IRSTD methods and datasets have led to significant advancements, they are limited by their reliance solely on the image modality. Recent advances in deep learning and large vision-language models have shown remarkable performance in various visual recognition tasks. In this work, we propose a novel multimodal IRSTD framework that incorporates language priors to guide small target detection. We leverage language-guided attention weights derived from the language prior to enhance the modelâ€™s ability for IRSTD, presenting a novel approach that combines textual information with image data to improve IRSTD capabilities. Utilizing the state-of-the-art GPT-4 vision model, we generate text descriptions that provide the locations of small targets in infrared images, employing careful prompt engineering to ensure improved accuracy. Due to the absence of multimodal IR datasets, existing IRSTD methods rely solely on image data. To address this shortcoming, we have curated a multimodal infrared dataset that includes both image and text modalities for small target detection, expanding upon the popular IRSTD-1k and NUDT-SIRST datasets. We validate the effectiveness of our approach through extensive experiments and comprehensive ablation studies. The results demonstrate significant improvements over the state-of-the-art method, with relative percentage differences of 9.74%, 13.02%, 1.25%, and 67.87% in IoU, nIoU, Pd, and Fa on the NUAA-SIRST subset, and 4.41%, 2.04%, 2.01%, and 113.43% on the IRSTD-1k subset of the LangIR dataset, respectively. </p>
<blockquote>
<p>çº¢å¤–å°ç›®æ ‡æ£€æµ‹ï¼ˆIRSTDï¼‰èƒ½å¤Ÿåœ¨çº¢å¤–æ¨¡ç³ŠèƒŒæ™¯ä¸­æ£€æµ‹åˆ°å°ç›®æ ‡ï¼Œå¯¹äºå„ç§åº”ç”¨è‡³å…³é‡è¦ã€‚ç”±äºç›®æ ‡å°ºå¯¸å°ä»¥åŠçº¢å¤–å°ç›®æ ‡æ•°æ®é›†ä¸­åˆ†å¸ƒç¨€ç–ï¼Œæ£€æµ‹ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å°½ç®¡ç°æœ‰çš„IRSTDæ–¹æ³•å’Œæ•°æ®é›†å·²ç»å¸¦æ¥äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å®ƒä»¬ä»…ä¾èµ–äºå›¾åƒæ¨¡æ€ï¼Œå­˜åœ¨ä¸€å®šçš„å±€é™æ€§ã€‚æœ€è¿‘ï¼Œæ·±åº¦å­¦ä¹ å’Œå¤§è§„æ¨¡è§†è§‰è¯­è¨€æ¨¡å‹çš„è¿›æ­¥åœ¨å„ç§è§†è§‰è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆè¯­è¨€å…ˆéªŒçŸ¥è¯†å¼•å¯¼å°ç›®æ ‡æ£€æµ‹çš„æ–°å‹å¤šæ¨¡æ€IRSTDæ¡†æ¶ã€‚æˆ‘ä»¬åˆ©ç”¨è¯­è¨€å…ˆéªŒçŸ¥è¯†è¡ç”Ÿçš„è¯­è¨€å¼•å¯¼æ³¨æ„åŠ›æƒé‡æ¥æå‡æ¨¡å‹è¿›è¡ŒIRSTDçš„èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆæ–‡æœ¬ä¿¡æ¯å’Œå›¾åƒæ•°æ®çš„æ–°å‹æ–¹æ³•æ¥æé«˜IRSTDèƒ½åŠ›ã€‚æˆ‘ä»¬åˆ©ç”¨æœ€å…ˆè¿›çš„GPT-4è§†è§‰æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æè¿°ï¼Œæä¾›çº¢å¤–å›¾åƒä¸­å°ç›®æ ‡çš„ä½ç½®ï¼Œå¹¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æç¤ºæ¥ç¡®ä¿æé«˜å‡†ç¡®æ€§ã€‚ç”±äºç¼ºå°‘å¤šæ¨¡æ€IRæ•°æ®é›†ï¼Œç°æœ‰çš„IRSTDæ–¹æ³•ä»…ä¾èµ–äºå›¾åƒæ•°æ®ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ä¸è¶³ï¼Œæˆ‘ä»¬æ•´ç†äº†ä¸€ä¸ªå¤šæ¨¡æ€çº¢å¤–æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«ç”¨äºå°ç›®æ ‡æ£€æµ‹çš„å›¾è±¡å’Œæ–‡æœ¬æ¨¡æ€ï¼Œåœ¨æµè¡Œçš„IRSTD-1kå’ŒNUDT-SIRSTæ•°æ®é›†åŸºç¡€ä¸Šè¿›è¡Œäº†æ‰©å±•ã€‚æˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„å®éªŒå’Œå…¨é¢çš„æ¶ˆèç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœè¡¨æ˜ï¼Œä¸æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨NUAA-SIRSTå­é›†ä¸Šçš„IoUã€nIoUã€Pdå’ŒFaåˆ†åˆ«æœ‰9.74%ã€13.02%ã€1.25%å’Œ67.87%çš„ç›¸å¯¹ç™¾åˆ†æ¯”å·®å¼‚ï¼Œåœ¨LangIRæ•°æ®é›†çš„IRSTD-1kå­é›†ä¸Šåˆ†åˆ«æœ‰4.41%ã€2.04%ã€2.01%å’Œ113.43%çš„ç›¸å¯¹ç™¾åˆ†æ¯”å·®å¼‚ã€‚è¿™äº›æ”¹è¿›æ˜¯æ˜¾è‘—çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13113v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†çº¢å¤–å°ç›®æ ‡æ£€æµ‹ï¼ˆIRSTDï¼‰çš„æŒ‘æˆ˜æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€IRSTDæ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†è¯­è¨€å…ˆéªŒæ¥æŒ‡å¯¼å°ç›®æ ‡æ£€æµ‹ï¼Œé€šè¿‡ä½¿ç”¨è¯­è¨€å¼•å¯¼çš„æ³¨æ„åŠ›æƒé‡å¢å¼ºæ¨¡å‹èƒ½åŠ›ã€‚åˆ©ç”¨å…ˆè¿›çš„GPT-4è§†è§‰æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æè¿°ï¼Œæä¾›çº¢å¤–å›¾åƒä¸­å°ç›®æ ‡çš„ä½ç½®ã€‚ä¸ºè§£å†³ç¼ºä¹å¤šæ¨¡æ€IRæ•°æ®é›†çš„é—®é¢˜ï¼Œä½œè€…è¿˜æ•´ç†äº†ä¸€ä¸ªåŒ…å«å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€çš„å¤šæ¨¡æ€çº¢å¤–æ•°æ®é›†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨NUAA-SIRSTå’ŒIRSTD-1kå­é›†ä¸Šç›¸è¾ƒäºç°æœ‰æ–¹æ³•æœ‰æ˜æ˜¾æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çº¢å¤–å°ç›®æ ‡æ£€æµ‹ï¼ˆIRSTDï¼‰é¢ä¸´æŒ‘æˆ˜ï¼Œå› ç›®æ ‡å°ä¸”åœ¨çº¢å¤–èƒŒæ™¯ä¸­åˆ†å¸ƒç¨€ç–ã€‚</li>
<li>ç°æœ‰IRSTDæ–¹æ³•å’Œæ•°æ®é›†å—é™äºä»…ä½¿ç”¨å›¾åƒæ¨¡æ€ã€‚</li>
<li>æå‡ºäº†æ–°çš„å¤šæ¨¡æ€IRSTDæ¡†æ¶ï¼Œç»“åˆè¯­è¨€å…ˆéªŒæŒ‡å¯¼å°ç›®æ ‡æ£€æµ‹ã€‚</li>
<li>åˆ©ç”¨GPT-4è§†è§‰æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æè¿°ï¼Œæä¾›çº¢å¤–å›¾åƒå°ç›®æ ‡ä½ç½®ã€‚</li>
<li>æ•´ç†äº†ä¸€ä¸ªåŒ…å«å›¾åƒå’Œæ–‡æœ¬æ¨¡æ€çš„å¤šæ¨¡æ€çº¢å¤–æ•°æ®é›†ï¼Œä»¥æ‰©å±•IRSTDæ•°æ®é›†ã€‚</li>
<li>é€šè¿‡å¹¿æ³›å®éªŒå’Œç»¼åˆåˆ†æéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13113">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71b07db82d16b825db0fd960b93310b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8039d00276f01e47d21e3f24d8520be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cecd8b76b07bc3a9879c16aafb4e34e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7d308b7c6db6e47ea89b7a7a765c1992.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75f623db2122e0e1add0204e30ec56f6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fc5fb6a0aa865ab9ee3b8f5c50e6fb2a.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="GLAD-Generalizable-Tuning-for-Vision-Language-Models"><a href="#GLAD-Generalizable-Tuning-for-Vision-Language-Models" class="headerlink" title="GLAD: Generalizable Tuning for Vision-Language Models"></a>GLAD: Generalizable Tuning for Vision-Language Models</h2><p><strong>Authors:Yuqi Peng, Pengfei Wang, Jianzhuang Liu, Shifeng Chen</strong></p>
<p>Pre-trained vision-language models, such as CLIP, show impressive zero-shot recognition ability and can be easily transferred to specific downstream tasks via prompt tuning, even with limited training data. However, existing prompt tuning methods face two main challenges: (1) In few-shot scenarios, data scarcity often leads to overfitting, making the model sensitive to changes in the input domain. (2) To mitigate overfitting, these methods typically rely on complex task-specific model architectures and sensitive hyperparameter tuning, severely restricting their general applicability. To address these issues, we propose a simpler and more general framework called GLAD (Generalizable LoRA tuning with RegulArized GraDient). We show that merely applying LoRA achieves performance in downstream tasks comparable to current state-of-the-art prompt-based methods. While LoRA is effective and easy to use, it remains susceptible to overfitting in few-shot learning scenarios. To mitigate this risk, we introduce a gradient-based regularization technique. This technique effectively steers the optimization trajectory, encouraging the model to find a more stable parameter region that is robust to variations in data distribution. Through extensive experiments conducted on 15 benchmark datasets, we demonstrate that GLAD outperforms previous tuning approaches in terms of base-to-novel class generalization, image domain generalization, and cross-dataset generalization. The code will be publicly available. </p>
<blockquote>
<p>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¦‚CLIPï¼Œå±•ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡æç¤ºå¾®è°ƒè½»æ¾è½¬ç§»åˆ°ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œå³ä½¿è®­ç»ƒæ•°æ®æœ‰é™ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æç¤ºå¾®è°ƒæ–¹æ³•é¢ä¸´ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š(1)åœ¨å°‘é‡æ ·æœ¬æƒ…å†µä¸‹ï¼Œæ•°æ®ç¨€ç¼ºå¾€å¾€å¯¼è‡´è¿‡æ‹Ÿåˆï¼Œä½¿æ¨¡å‹å¯¹è¾“å…¥åŸŸçš„å˜åŒ–æ•æ„Ÿã€‚(2)ä¸ºäº†ç¼“è§£è¿‡æ‹Ÿåˆï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºå¤æ‚çš„ç‰¹å®šä»»åŠ¡æ¨¡å‹æ¶æ„å’Œæ•æ„Ÿçš„è¶…å‚æ•°è°ƒæ•´ï¼Œä¸¥é‡é™åˆ¶äº†å®ƒä»¬çš„é€šç”¨é€‚ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ›´ç®€å•ã€æ›´é€šç”¨çš„æ¡†æ¶ï¼Œç§°ä¸ºGLADï¼ˆé€šè¿‡æ­£åˆ™åŒ–æ¢¯åº¦å®ç°çš„å¯æ³›åŒ–çš„LoRAè°ƒæ•´ï¼‰ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä»…ä»…åº”ç”¨LoRAå°±èƒ½åœ¨å®ç°ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œä¸å½“å‰æœ€å…ˆè¿›çš„åŸºäºæç¤ºçš„æ–¹æ³•ç›¸åª²ç¾ã€‚è™½ç„¶LoRAæœ‰æ•ˆä¸”æ˜“äºä½¿ç”¨ï¼Œä½†åœ¨å°‘é‡æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­ä»ç„¶å®¹æ˜“è¿‡æ‹Ÿåˆã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é£é™©ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºæ¢¯åº¦çš„æ­£åˆ™åŒ–æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯æœ‰æ•ˆåœ°å¼•å¯¼äº†ä¼˜åŒ–è½¨è¿¹ï¼Œé¼“åŠ±æ¨¡å‹æ‰¾åˆ°ä¸€ä¸ªæ›´ç¨³å®šçš„å‚æ•°åŒºåŸŸï¼Œè¯¥åŒºåŸŸå¯¹æ•°æ®åˆ†å¸ƒçš„å˜åŒ–å…·æœ‰é²æ£’æ€§ã€‚é€šè¿‡å¯¹15ä¸ªåŸºå‡†æ•°æ®é›†è¿›è¡Œçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†GLADåœ¨åŸºç¡€åˆ°æ–°é¢–ç±»çš„æ³›åŒ–ã€å›¾åƒåŸŸæ³›åŒ–å’Œè·¨æ•°æ®é›†æ³›åŒ–æ–¹é¢ä¼˜äºä»¥å‰çš„è°ƒæ•´æ–¹æ³•ã€‚ä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13089v1">PDF</a> ICCV 2025 workshop</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºGLADçš„é€šç”¨æ¡†æ¶ï¼Œç”¨äºè§£å†³é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å±€é™æ€§é—®é¢˜ã€‚é€šè¿‡åº”ç”¨LoRAæŠ€æœ¯ï¼Œè¯¥æ¡†æ¶åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¸å½“å‰å…ˆè¿›çš„æç¤ºæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚ä¸ºäº†ç¼“è§£åœ¨å°‘é‡æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­çš„è¿‡æ‹Ÿåˆé£é™©ï¼Œå¼•å…¥äº†åŸºäºæ¢¯åº¦çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œé¼“åŠ±æ¨¡å‹æ‰¾åˆ°æ›´ç¨³å®šçš„å‚æ•°åŒºåŸŸï¼Œä»è€Œæé«˜å¯¹ä¸åŒæ•°æ®åˆ†å¸ƒçš„é²æ£’æ€§ã€‚ç»è¿‡åœ¨15ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯ï¼ŒGLADåœ¨åŸºç¡€åˆ°æ–°é¢–ç±»åˆ«çš„æ³›åŒ–ã€å›¾åƒåŸŸæ³›åŒ–å’Œè·¨æ•°æ®é›†æ³›åŒ–æ–¹é¢å‡ä¼˜äºä¹‹å‰çš„å¾®è°ƒæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹å¦‚CLIPå…·æœ‰å¼ºå¤§çš„é›¶æ ·æœ¬è¯†åˆ«èƒ½åŠ›ï¼Œå¯é€šè¿‡æç¤ºå¾®è°ƒè½»æ¾åº”ç”¨äºç‰¹å®šä¸‹æ¸¸ä»»åŠ¡ï¼Œå³ä½¿è®­ç»ƒæ•°æ®æœ‰é™ã€‚</li>
<li>ç°æœ‰æç¤ºå¾®è°ƒæ–¹æ³•é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šå°‘é‡æ ·æœ¬åœºæ™¯ä¸­çš„æ•°æ®ç¨€ç¼ºå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œä»¥åŠéœ€è¦å¤æ‚ä»»åŠ¡ç‰¹å®šçš„æ¨¡å‹æ¶æ„å’Œæ•æ„Ÿçš„è¶…å‚æ•°è°ƒæ•´ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§ã€‚</li>
<li>GLADæ¡†æ¶é€šè¿‡åº”ç”¨LoRAæŠ€æœ¯ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œä¸å½“å‰å…ˆè¿›çš„æç¤ºæ–¹æ³•ç›¸å½“ã€‚</li>
<li>LoRAæŠ€æœ¯ç®€å•æ˜“ç”¨ï¼Œä½†åœ¨å°‘é‡æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­ä»æ˜“è¿‡æ‹Ÿåˆã€‚</li>
<li>GLADå¼•å…¥åŸºäºæ¢¯åº¦çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä»¥ç¼“è§£è¿‡æ‹Ÿåˆé£é™©ï¼Œé¼“åŠ±æ¨¡å‹æ‰¾åˆ°æ›´ç¨³å®šçš„å‚æ•°åŒºåŸŸï¼Œæé«˜å¯¹ä¸åŒæ•°æ®åˆ†å¸ƒçš„é²æ£’æ€§ã€‚</li>
<li>åœ¨15ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯æ˜¾ç¤ºï¼ŒGLADåœ¨å¤šä¸ªæ–¹é¢çš„æ³›åŒ–æ€§èƒ½ä¼˜äºå…ˆå‰çš„å¾®è°ƒæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13089">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4acbad74775ddd8e8075fd3148e76f22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b805835f00199b2c699c2a63135dee52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d7bade881f57ec8addc0310853a1a5e8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-23cd8365f5194d98a895daf55d505e65.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Fine-Grained-Image-Recognition-from-Scratch-with-Teacher-Guided-Data-Augmentation"><a href="#Fine-Grained-Image-Recognition-from-Scratch-with-Teacher-Guided-Data-Augmentation" class="headerlink" title="Fine-Grained Image Recognition from Scratch with Teacher-Guided Data   Augmentation"></a>Fine-Grained Image Recognition from Scratch with Teacher-Guided Data   Augmentation</h2><p><strong>Authors:Edwin Arkel Rios, Fernando Mikael, Oswin Gosal, Femiloye Oyerinde, Hao-Chun Liang, Bo-Cheng Lai, Min-Chun Hu</strong></p>
<p>Fine-grained image recognition (FGIR) aims to distinguish visually similar sub-categories within a broader class, such as identifying bird species. While most existing FGIR methods rely on backbones pretrained on large-scale datasets like ImageNet, this dependence limits adaptability to resource-constrained environments and hinders the development of task-specific architectures tailored to the unique challenges of FGIR.   In this work, we challenge the conventional reliance on pretrained models by demonstrating that high-performance FGIR systems can be trained entirely from scratch. We introduce a novel training framework, TGDA, that integrates data-aware augmentation with weak supervision via a fine-grained-aware teacher model, implemented through knowledge distillation. This framework unlocks the design of task-specific and hardware-aware architectures, including LRNets for low-resolution FGIR and ViTFS, a family of Vision Transformers optimized for efficient inference.   Extensive experiments across three FGIR benchmarks over diverse settings involving low-resolution and high-resolution inputs show that our method consistently matches or surpasses state-of-the-art pretrained counterparts. In particular, in the low-resolution setting, LRNets trained with TGDA improve accuracy by up to 23% over prior methods while requiring up to 20.6x less parameters, lower FLOPs, and significantly less training data. Similarly, ViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k while using 15.3x fewer trainable parameters and requiring orders of magnitudes less data. These results highlight TGDAâ€™s potential as an adaptable alternative to pretraining, paving the way for more efficient fine-grained vision systems. </p>
<blockquote>
<p>ç»†ç²’åº¦å›¾åƒè¯†åˆ«ï¼ˆFGIRï¼‰æ—¨åœ¨åŒºåˆ†å¹¿æ³›ç±»åˆ«ä¸­è§†è§‰ä¸Šç›¸ä¼¼çš„å­ç±»åˆ«ï¼Œä¾‹å¦‚è¯†åˆ«é¸Ÿç±»ç‰©ç§ã€‚è™½ç„¶å¤§å¤šæ•°ç°æœ‰çš„FGIRæ–¹æ³•ä¾èµ–äºåœ¨å¤§å‹æ•°æ®é›†ï¼ˆå¦‚ImageNetï¼‰ä¸Šé¢„è®­ç»ƒçš„backboneï¼Œä½†è¿™ç§ä¾èµ–é™åˆ¶äº†å…¶åœ¨èµ„æºå—é™ç¯å¢ƒä¸­çš„é€‚åº”æ€§ï¼Œå¹¶é˜»ç¢äº†é’ˆå¯¹FGIRçš„ç‹¬ç‰¹æŒ‘æˆ˜è€Œè®¾è®¡çš„ä»»åŠ¡ç‰¹å®šæ¶æ„çš„å‘å±•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è¯æ˜å¯ä»¥å®Œå…¨ä¸ä¾èµ–é¢„è®­ç»ƒæ¨¡å‹æ¥è®­ç»ƒé«˜æ€§èƒ½çš„FGIRç³»ç»Ÿï¼Œä»è€Œè´¨ç–‘äº†ä¼ ç»Ÿçš„ä¾èµ–é¢„è®­ç»ƒæ¨¡å‹çš„åšæ³•ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°å‹è®­ç»ƒæ¡†æ¶TGDAï¼Œå®ƒå°†æ•°æ®æ„ŸçŸ¥å¢å¼ºä¸å¼±ç›‘ç£ç›¸ç»“åˆï¼Œé€šè¿‡ç»†ç²’åº¦æ„ŸçŸ¥æ•™å¸ˆæ¨¡å‹å®ç°çŸ¥è¯†è’¸é¦ã€‚è¯¥æ¡†æ¶å¯ä»¥è§£é”ä»»åŠ¡ç‰¹å®šå’Œç¡¬ä»¶æ„ŸçŸ¥æ¶æ„çš„è®¾è®¡ï¼ŒåŒ…æ‹¬ç”¨äºä½åˆ†è¾¨ç‡FGIRçš„LRNetsä»¥åŠé’ˆå¯¹é«˜æ•ˆæ¨ç†ä¼˜åŒ–çš„Vision Transformerå®¶æ—ViTFSã€‚åœ¨æ¶‰åŠä½åˆ†è¾¨ç‡å’Œé«˜åˆ†è¾¨ç‡è¾“å…¥çš„ä¸‰ä¸ªFGIRåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆèƒ½ä¸æœ€å…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ç›¸åŒ¹é…æˆ–è¶…è¶Šã€‚ç‰¹åˆ«æ˜¯åœ¨ä½åˆ†è¾¨ç‡ç¯å¢ƒä¸‹ï¼Œä½¿ç”¨TGDAè®­ç»ƒçš„LRNetsåœ¨ç²¾åº¦ä¸Šè¾ƒå…ˆå‰æ–¹æ³•æé«˜äº†é«˜è¾¾23%ï¼ŒåŒæ—¶å‡å°‘äº†é«˜è¾¾20.6å€çš„å‚æ•°ã€FLOPså’Œå¤§é‡çš„è®­ç»ƒæ•°æ®éœ€æ±‚ã€‚åŒæ ·ï¼ŒViTFS-Tå¯ä»¥åœ¨ä½¿ç”¨æ¯”ImageNet-21kä¸Šé¢„è®­ç»ƒçš„ViT B-16å°‘15.3å€çš„å¯è®­ç»ƒå‚æ•°å’Œéœ€è¦æ›´å°‘æ•°æ®é‡çš„æƒ…å†µä¸‹è¾¾åˆ°ç›¸åŒçš„æ€§èƒ½ã€‚è¿™äº›ç»“æœçªæ˜¾äº†TGDAä½œä¸ºå¯é€‚åº”çš„é¢„è®­ç»ƒæ›¿ä»£æ–¹æ¡ˆçš„æ½œåŠ›ï¼Œä¸ºæ›´æœ‰æ•ˆçš„ç»†ç²’åº¦è§†è§‰ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12157v1">PDF</a> Main: 10 pages, 2 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¨æ–°çš„è®­ç»ƒæ¡†æ¶TGDAï¼Œç”¨äºè§£å†³ç»†ç²’åº¦å›¾åƒè¯†åˆ«ï¼ˆFGIRï¼‰é—®é¢˜ã€‚è¯¥æ¡†æ¶æ— éœ€ä¾èµ–é¢„è®­ç»ƒæ¨¡å‹ï¼Œé€šè¿‡æ•°æ®æ„ŸçŸ¥å¢å¼ºæŠ€æœ¯ä¸å¼±ç›‘ç£æ–¹å¼ï¼Œç»“åˆç²¾ç»†ç²’åº¦æ„ŸçŸ¥æ•™å¸ˆæ¨¡å‹å®ç°çŸ¥è¯†è’¸é¦ï¼Œå®ç°ä»å¤´å¼€å§‹è®­ç»ƒé«˜æ€§èƒ½FGIRç³»ç»Ÿã€‚å¼•å…¥çš„ä»»åŠ¡ç‰¹å®šå’Œç¡¬ä»¶æ„ŸçŸ¥æ¶æ„ï¼Œå¦‚ç”¨äºä½åˆ†è¾¨ç‡FGIRçš„LRNetså’Œé’ˆå¯¹é«˜æ•ˆæ¨ç†ä¼˜åŒ–çš„Vision Transformerå®¶æ—ViTFSï¼Œåœ¨ä¸‰ä¸ªFGIRåŸºå‡†æµ‹è¯•ä¸­çš„ä½åˆ†è¾¨ç‡å’Œé«˜åˆ†è¾¨ç‡è¾“å…¥ç¯å¢ƒä¸‹ï¼Œè¡¨ç°å‡è¾¾åˆ°æˆ–è¶…è¶Šäº†é¢„è®­ç»ƒæ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§å…¨æ–°çš„è®­ç»ƒæ¡†æ¶TGDAï¼Œç”¨äºè§£å†³ç»†ç²’åº¦å›¾åƒè¯†åˆ«é—®é¢˜ï¼Œä¸ä¾èµ–é¢„è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>é€šè¿‡æ•°æ®æ„ŸçŸ¥å¢å¼ºæŠ€æœ¯å’Œå¼±ç›‘ç£æ–¹å¼å®ç°çŸ¥è¯†è’¸é¦ã€‚</li>
<li>å¼•å…¥äº†ä»»åŠ¡ç‰¹å®šå’Œç¡¬ä»¶æ„ŸçŸ¥æ¶æ„ï¼Œå¦‚LRNetså’ŒViTFSã€‚</li>
<li>åœ¨ä¸‰ä¸ªFGIRåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½åˆ†è¾¨ç‡ç¯å¢ƒä¸‹LRNetså‡†ç¡®ç‡æå‡é«˜è¾¾23%ã€‚</li>
<li>ViTFS-Tåœ¨åŒ¹é…é¢„è®­ç»ƒæ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œä½¿ç”¨å‚æ•°å‡å°‘15.3å€ï¼Œæ‰€éœ€æ•°æ®é‡å¤§å¹…é™ä½ã€‚</li>
<li>TGDAä½œä¸ºä¸€ç§é€‚åº”æ€§çš„æ›¿ä»£æ–¹æ¡ˆå…·æœ‰æ½œåŠ›ï¼Œä¸ºå®ç°æ›´é«˜æ•ˆçš„ç»†ç²’åº¦è§†è§‰ç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-971b2c46a74e9db54443579f8e78da55.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f911a70cb9ca96df38e22016af152e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f02791f105a366df6d0770d695c4a87.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2042f922cda54810d1b8e7c01fef9d9d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c34f12614cc55e054f172c6055188bab.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="GS-Bias-Global-Spatial-Bias-Learner-for-Single-Image-Test-Time-Adaptation-of-Vision-Language-Models"><a href="#GS-Bias-Global-Spatial-Bias-Learner-for-Single-Image-Test-Time-Adaptation-of-Vision-Language-Models" class="headerlink" title="GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time   Adaptation of Vision-Language Models"></a>GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time   Adaptation of Vision-Language Models</h2><p><strong>Authors:Zhaohong Huang, Yuxin Zhang, Jingjing Xie, Fei Chao, Rongrong Ji</strong></p>
<p>Recent advances in test-time adaptation (TTA) for Vision-Language Models (VLMs) have garnered increasing attention, particularly through the use of multiple augmented views of a single image to boost zero-shot generalization. Unfortunately, existing methods fail to strike a satisfactory balance between performance and efficiency, either due to excessive overhead of tuning text prompts or unstable benefits from handcrafted, training-free visual feature enhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias), an efficient and effective TTA paradigm that incorporates two learnable biases during TTA, unfolded as the global bias and spatial bias. Particularly, the global bias captures the global semantic features of a test image by learning consistency across augmented views, while spatial bias learns the semantic coherence between regions in the imageâ€™s spatial visual representation. It is worth highlighting that these two sets of biases are directly added to the logits outputed by the pretrained VLMs, which circumvent the full backpropagation through VLM that hinders the efficiency of existing TTA methods. This endows GS-Bias with extremely high efficiency while achieving state-of-the-art performance on 15 benchmark datasets. For example, it achieves a 2.23% improvement over TPT in cross-dataset generalization and a 2.72% improvement in domain generalization, while requiring only 6.5% of TPTâ€™s memory usage on ImageNet. </p>
<blockquote>
<p>æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆTTAï¼‰åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ–¹é¢çš„æœ€æ–°è¿›å±•å·²ç»å¼•èµ·äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ä½¿ç”¨å•ä¸€å›¾åƒçš„å¤šé‡å¢å¼ºè§†å›¾æ¥æå‡é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æœªèƒ½åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å–å¾—ä»¤äººæ»¡æ„çš„å¹³è¡¡ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºæ–‡æœ¬æç¤ºè°ƒæ•´è¿‡åº¦æˆ–æ‰‹å·¥åˆ¶ä½œçš„ã€æ— éœ€è®­ç»ƒå³å¯æå‡è§†è§‰ç‰¹å¾çš„æ”¶ç›Šä¸ç¨³å®šæ‰€è‡´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Global-Spatial Bias Learnerï¼ˆGS-Biasï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„TTAæ–¹æ³•ï¼Œå®ƒåœ¨æµ‹è¯•æ—¶å¼•å…¥äº†ä¸¤ç§å¯å­¦ä¹ çš„åå·®ï¼Œåˆ†åˆ«ä¸ºå…¨å±€åå·®å’Œç©ºé—´åå·®ã€‚å…·ä½“æ¥è¯´ï¼Œå…¨å±€åå·®é€šè¿‡æ•æ‰å¢å¼ºè§†å›¾ä¹‹é—´çš„å…±è¯†æ¥ä¹ å¾—æµ‹è¯•å›¾åƒçš„å…¨å±€è¯­ä¹‰ç‰¹å¾ï¼Œè€Œç©ºé—´åå·®åˆ™å­¦ä¹ å›¾åƒç©ºé—´è§†è§‰è¡¨ç¤ºä¸­åŒºåŸŸä¹‹é—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œè¿™ä¸¤ç»„åå·®ç›´æ¥æ·»åŠ åˆ°é¢„è®­ç»ƒVLMè¾“å‡ºçš„logitsä¸Šï¼Œé¿å…äº†é€šè¿‡VLMè¿›è¡Œå…¨åå‘ä¼ æ’­ï¼Œä»è€Œæé«˜ç°æœ‰TTAæ–¹æ³•çš„æ•ˆç‡ã€‚è¿™ä¸ºGS-Biaså¸¦æ¥äº†æé«˜çš„æ•ˆç‡ï¼ŒåŒæ—¶åœ¨15ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå®ƒåœ¨è·¨æ•°æ®é›†æ³›åŒ–æ–¹é¢æ¯”TPTæé«˜äº†2.23%ï¼Œåœ¨åŸŸæ³›åŒ–æ–¹é¢æé«˜äº†2.72%ï¼Œè€Œåœ¨ImageNetä¸Šçš„å†…å­˜ä½¿ç”¨ç‡ä»…ä¸ºTPTçš„6.5%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11969v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è®ºæ–‡æå‡ºä¸€ç§ç§°ä¸ºGlobal-Spatial Bias Learnerï¼ˆGS-Biasï¼‰çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥å…¨å±€åå·®å’Œç©ºé—´åå·®ä¸¤ä¸ªå¯å­¦ä¹ çš„åå·®ï¼Œåœ¨TTAè¿‡ç¨‹ä¸­æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚GS-Biasèƒ½å¤Ÿé«˜æ•ˆåœ°åœ¨å¤šä¸ªå¢å¼ºè§†å›¾ä¸Šæ•è·æµ‹è¯•å›¾åƒçš„å…¨å±€è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶å­¦ä¹ å›¾åƒç©ºé—´è§†è§‰è¡¨ç¤ºä¸­åŒºåŸŸé—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚è¯¥æ–¹æ³•ç›´æ¥åœ¨é¢„è®­ç»ƒçš„VLMsè¾“å‡ºçš„logitsä¸Šæ·»åŠ åå·®ï¼Œé¿å…äº†ç°æœ‰TTAæ–¹æ³•æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œå®ç°äº†æé«˜çš„æ•ˆç‡å’Œå‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚ä¾‹å¦‚ï¼Œç›¸è¾ƒäºTPTæ–¹æ³•ï¼ŒGS-Biasåœ¨è·¨æ•°æ®é›†æ³›åŒ–å’ŒåŸŸæ³›åŒ–ä¸Šåˆ†åˆ«æå‡äº†2.23%å’Œ2.72%ï¼ŒåŒæ—¶åªéœ€è¦TPTçš„6.5%å†…å­˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Global-Spatial Bias Learner (GS-Bias)æ˜¯ä¸€ç§æ–°çš„æµ‹è¯•æ—¶è‡ªé€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•ï¼Œç”¨äºæé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>GS-Biasé€šè¿‡å¼•å…¥å…¨å±€åå·®å’Œç©ºé—´åå·®ä¸¤ä¸ªå¯å­¦ä¹ åå·®æ¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚å…¨å±€åå·®ç”¨äºæ•è·æµ‹è¯•å›¾åƒçš„å…¨å±€è¯­ä¹‰ç‰¹å¾ï¼Œç©ºé—´åå·®åˆ™å­¦ä¹ å›¾åƒç©ºé—´è§†è§‰è¡¨ç¤ºä¸­åŒºåŸŸé—´çš„è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>GS-Biasé€šè¿‡åœ¨é¢„è®­ç»ƒçš„VLMsè¾“å‡ºçš„logitsä¸Šç›´æ¥æ·»åŠ åå·®ï¼Œæé«˜äº†æ•ˆç‡å¹¶é¿å…äº†ç°æœ‰TTAæ–¹æ³•çš„ä¸è¶³ã€‚</li>
<li>GS-Biasåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œç›¸è¾ƒäºå…¶ä»–æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>GS-Biasåœ¨è·¨æ•°æ®é›†æ³›åŒ–å’ŒåŸŸæ³›åŒ–æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç›¸è¾ƒäºTPTæ–¹æ³•åˆ†åˆ«æå‡äº†2.23%å’Œ2.72%ã€‚</li>
<li>GS-Biaså…·æœ‰æé«˜çš„å†…å­˜æ•ˆç‡ï¼Œåªéœ€è¦TPTçš„6.5%å†…å­˜å³å¯å®ç°ä¼˜è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11969">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-80414386a74bb1b3b9e135e729a87ff6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-770991d47fe82942a764af0dba68995c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74ff3b06ffd953728c45226401516689.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8f3306bc01ad6d0bc945599d874fa1a2.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  3D-MOOD Lifting 2D to 3D for Monocular Open-Set Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-46c694119abd21be5efe2a3d91cd6519.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  Towards Video Thinking Test A Holistic Benchmark for Advanced Video   Reasoning and Understanding
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26384.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
