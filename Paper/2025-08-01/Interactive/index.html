<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Interactive">
    <meta name="description" content="Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  Text-to-SQL Task-oriented Dialogue Ontology Construction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Interactive | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-588fabb41b5ed47c02e1bcd973ea1dea.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Interactive</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Interactive/">
                                <span class="chip bg-color">Interactive</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                Interactive
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    39 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-02-æ›´æ–°"><a href="#2025-08-02-æ›´æ–°" class="headerlink" title="2025-08-02 æ›´æ–°"></a>2025-08-02 æ›´æ–°</h1><h2 id="Text-to-SQL-Task-oriented-Dialogue-Ontology-Construction"><a href="#Text-to-SQL-Task-oriented-Dialogue-Ontology-Construction" class="headerlink" title="Text-to-SQL Task-oriented Dialogue Ontology Construction"></a>Text-to-SQL Task-oriented Dialogue Ontology Construction</h2><p><strong>Authors:Renato Vukovic, Carel van Niekerk, Michael Heck, Benjamin Ruppik, Hsien-Chin Lin, Shutong Feng, Nurul Lubis, Milica Gasic</strong></p>
<p>Large language models (LLMs) are widely used as general-purpose knowledge sources, but they rely on parametric knowledge, limiting explainability and trustworthiness. In task-oriented dialogue (TOD) systems, this separation is explicit, using an external database structured by an explicit ontology to ensure explainability and controllability. However, building such ontologies requires manual labels or supervised training. We introduce TeQoDO: a Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM autonomously builds a TOD ontology from scratch without supervision using its inherent SQL programming capabilities combined with dialogue theory provided in the prompt. We show that TeQoDO outperforms transfer learning approaches, and its constructed ontology is competitive on a downstream dialogue state tracking task. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also scales to allow construction of much larger ontologies, which we investigate on a Wikipedia and ArXiv dataset. We view this as a step towards broader application of ontologies to increase LLM explainability. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«å¹¿æ³›åº”ç”¨äºé€šç”¨çŸ¥è¯†æºï¼Œä½†å®ƒä»¬ä¾èµ–äºå‚æ•°çŸ¥è¯†ï¼Œè¿™é™åˆ¶äº†å…¶å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦ã€‚åœ¨é¢å‘ä»»åŠ¡çš„å¯¹è¯ï¼ˆTODï¼‰ç³»ç»Ÿä¸­ï¼Œè¿™ç§åˆ†ç¦»æ˜¯æ˜ç¡®çš„ï¼Œä½¿ç”¨æ˜ç¡®çš„æœ¬ä½“æ„å»ºçš„å¤–éƒ¨æ•°æ®åº“ï¼Œä»¥ç¡®ä¿å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ã€‚ç„¶è€Œï¼Œæ„å»ºè¿™æ ·çš„æœ¬ä½“éœ€è¦æ‰‹åŠ¨æ ‡ç­¾æˆ–ç›‘ç£è®­ç»ƒã€‚æˆ‘ä»¬å¼•å…¥äº†TeQoDOï¼šä¸€ç§ä»æ–‡æœ¬åˆ°SQLçš„é¢å‘ä»»åŠ¡çš„å¯¹è¯æœ¬ä½“æ„å»ºæ–¹æ³•ã€‚åœ¨è¿™é‡Œï¼ŒLLMåˆ©ç”¨å…¶å†…åœ¨çš„SQLç¼–ç¨‹èƒ½åŠ›å’Œæç¤ºä¸­æä¾›çš„å¯¹è¯ç†è®ºï¼Œæ— éœ€ç›‘ç£å³å¯ä»é›¶å¼€å§‹è‡ªä¸»æ„å»ºTODæœ¬ä½“ã€‚æˆ‘ä»¬å±•ç¤ºäº†TeQoDOä¼˜äºè¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œå…¶æ„å»ºçš„æœ¬ä½“åœ¨ä¸‹æ¸¸å¯¹è¯çŠ¶æ€è·Ÿè¸ªä»»åŠ¡ä¸Šå…·æœ‰å¾ˆå¼ºçš„ç«äº‰åŠ›ã€‚æ¶ˆé™¤ç ”ç©¶è¡¨æ˜å¯¹è¯ç†è®ºèµ·ç€å…³é”®ä½œç”¨ã€‚TeQoDOè¿˜å¯ä»¥æ‰©å±•ï¼Œä»¥æ„å»ºæ›´å¤§çš„æœ¬ä½“ï¼Œæˆ‘ä»¬åœ¨Wikipediaå’ŒArXivæ•°æ®é›†ä¸Šå¯¹æ­¤è¿›è¡Œäº†è°ƒæŸ¥ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯å‘æ›´å¹¿æ³›çš„åº”ç”¨æœ¬ä½“ä»¥æé«˜LLMå¯è§£é‡Šæ€§è¿ˆå‡ºçš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23358v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºé€šç”¨çŸ¥è¯†æºåº”ç”¨å¹¿æ³›ï¼Œä½†ä¾èµ–å‚æ•°çŸ¥è¯†ï¼Œé™åˆ¶äº†å…¶å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦ã€‚åœ¨é¢å‘ä»»åŠ¡çš„å¯¹è¯ï¼ˆTODï¼‰ç³»ç»Ÿä¸­ï¼Œä½¿ç”¨å¤–éƒ¨æ•°æ®åº“å’Œæ˜¾æ€§æœ¬ä½“ç»“æ„æ¥ç¡®ä¿å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„é¢å‘ä»»åŠ¡çš„å¯¹è¯æœ¬ä½“æ„å»ºæ–¹æ³•TeQoDOï¼Œè¯¥æ–¹æ³•åˆ©ç”¨LLMè‡ªä¸»æ„å»ºTODæœ¬ä½“ï¼Œæ— éœ€ç›‘ç£ï¼Œç»“åˆå†…ç½®çš„SQLç¼–ç¨‹èƒ½åŠ›å’Œå¯¹è¯ç†è®ºæç¤ºã€‚ç ”ç©¶è¡¨æ˜ï¼ŒTeQoDOåœ¨ä¸‹æ¸¸å¯¹è¯çŠ¶æ€è·Ÿè¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºè¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œä¸”å…¶æ„å»ºçš„æœ¬ä½“å…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒTeQoDOè¿˜å¯ä»¥æ‰©å±•åˆ°æ›´å¤§çš„æœ¬ä½“æ„å»ºï¼Œæˆ‘ä»¬åœ¨Wikipediaå’ŒArXivæ•°æ®é›†ä¸Šè¿›è¡Œäº†è°ƒæŸ¥ã€‚è¿™è¢«è§†ä¸ºæé«˜LLMå¯è§£é‡Šæ€§çš„é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä½œä¸ºé€šç”¨çŸ¥è¯†æºå…·æœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†ç¼ºä¹å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦ã€‚</li>
<li>é¢å‘ä»»åŠ¡çš„å¯¹è¯ï¼ˆTODï¼‰ç³»ç»Ÿé€šè¿‡ä½¿ç”¨å¤–éƒ¨æ•°æ®åº“å’Œæ˜¾æ€§æœ¬ä½“ç»“æ„æ¥æé«˜å¯è§£é‡Šæ€§å’Œå¯æ§æ€§ã€‚</li>
<li>TeQoDOæ˜¯ä¸€ç§æ–°å‹çš„é¢å‘ä»»åŠ¡çš„å¯¹è¯æœ¬ä½“æ„å»ºæ–¹æ³•ï¼Œå¯ä»¥è‡ªä¸»æ„å»ºTODæœ¬ä½“ï¼Œæ— éœ€äººå·¥ç›‘ç£ã€‚</li>
<li>TeQoDOç»“åˆäº†LLMçš„SQLç¼–ç¨‹èƒ½åŠ›å’Œå¯¹è¯ç†è®ºæç¤ºã€‚</li>
<li>TeQoDOåœ¨ä¸‹æ¸¸å¯¹è¯çŠ¶æ€è·Ÿè¸ªä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºè¿ç§»å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>TeQoDOæ„å»ºçš„æœ¬ä½“å…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶ä¸”å¯ä»¥æ‰©å±•åˆ°æ›´å¤§çš„æœ¬ä½“æ„å»ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23358">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-786ee01f9c66fe111566ac62c617c98d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea557937f94e888dbebc25dcabc55e73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-222b50ce34f5f92cbc15f045e1fe5b0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cdb55f900b38f37b793665e8f71e8c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ddc3a9990acbfe212feff11cfe1d6cf.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Real-time-Generation-of-Various-Types-of-Nodding-for-Avatar-Attentive-Listening-System"><a href="#Real-time-Generation-of-Various-Types-of-Nodding-for-Avatar-Attentive-Listening-System" class="headerlink" title="Real-time Generation of Various Types of Nodding for Avatar Attentive   Listening System"></a>Real-time Generation of Various Types of Nodding for Avatar Attentive   Listening System</h2><p><strong>Authors:Kazushi Kato, Koji Inoue, Divesh Lala, Keiko Ochi, Tatsuya Kawahara</strong></p>
<p>In human dialogue, nonverbal information such as nodding and facial expressions is as crucial as verbal information, and spoken dialogue systems are also expected to express such nonverbal behaviors. We focus on nodding, which is critical in an attentive listening system, and propose a model that predicts both its timing and type in real time. The proposed model builds on the voice activity projection (VAP) model, which predicts voice activity from both listener and speaker audio. We extend it to prediction of various types of nodding in a continuous and real-time manner unlike conventional models. In addition, the proposed model incorporates multi-task learning with verbal backchannel prediction and pretraining on general dialogue data. In the timing and type prediction task, the effectiveness of multi-task learning was significantly demonstrated. We confirmed that reducing the processing rate enables real-time operation without a substantial drop in accuracy, and integrated the model into an avatar attentive listening system. Subjective evaluations showed that it outperformed the conventional method, which always does nodding in sync with verbal backchannel. The code and trained models are available at <a target="_blank" rel="noopener" href="https://github.com/MaAI-Kyoto/MaAI">https://github.com/MaAI-Kyoto/MaAI</a>. </p>
<blockquote>
<p>åœ¨äººæœºå¯¹è¯ä¸­ï¼Œéè¨€è¯­ä¿¡æ¯ï¼ˆå¦‚ç‚¹å¤´å’Œé¢éƒ¨è¡¨æƒ…ï¼‰ä¸è¨€è¯­ä¿¡æ¯åŒæ ·é‡è¦ï¼Œäººä»¬æœŸæœ›è¯­éŸ³å¯¹è¯ç³»ç»Ÿä¹Ÿèƒ½è¡¨è¾¾è¿™äº›éè¨€è¯­è¡Œä¸ºã€‚æˆ‘ä»¬ä¸“æ³¨äºç‚¹å¤´ï¼Œå®ƒåœ¨å€¾å¬ç³»ç»Ÿä¸­è‡³å…³é‡è¦ï¼Œå¹¶æå‡ºä¸€ç§æ¨¡å‹ï¼Œå¯ä»¥å®æ—¶é¢„æµ‹å…¶æ—¶æœºå’Œç±»å‹ã€‚æ‰€ææ¨¡å‹åŸºäºè¯­éŸ³æ´»åŠ¨æŠ•å½±ï¼ˆVAPï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä»å¬è€…å’Œè¯´è¯è€…çš„éŸ³é¢‘ä¸­é¢„æµ‹è¯­éŸ³æ´»åŠ¨ã€‚æˆ‘ä»¬å°†å…¶æ‰©å±•åˆ°è¿ç»­å®æ—¶é¢„æµ‹å„ç§ç‚¹å¤´ç±»å‹ï¼Œä¸åŒäºä¼ ç»Ÿæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæ‰€ææ¨¡å‹ç»“åˆäº†å¤šä»»åŠ¡å­¦ä¹ ä¸è¨€è¯­åé¦ˆé€šé“é¢„æµ‹å’Œä¸€èˆ¬å¯¹è¯æ•°æ®çš„é¢„è®­ç»ƒã€‚åœ¨æ—¶æœºå’Œç±»å‹é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œå¤šä»»åŠ¡å­¦ä¹ çš„æœ‰æ•ˆæ€§å¾—åˆ°äº†æ˜¾è‘—è¯æ˜ã€‚æˆ‘ä»¬éªŒè¯äº†é™ä½å¤„ç†é€Ÿç‡å¯ä»¥åœ¨ä¸æ˜¾è‘—é™ä½å‡†ç¡®åº¦çš„å‰æä¸‹å®ç°å®æ—¶æ“ä½œï¼Œå¹¶å°†è¯¥æ¨¡å‹é›†æˆåˆ°è™šæ‹Ÿè§’è‰²å€¾å¬ç³»ç»Ÿä¸­ã€‚ä¸»è§‚è¯„ä¼°è¡¨æ˜ï¼Œå®ƒä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œåè€…æ€»æ˜¯ä¸è¨€è¯­åé¦ˆé€šé“åŒæ­¥ç‚¹å¤´ã€‚ç›¸å…³ä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MaAI-Kyoto/MaAI%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MaAI-Kyoto/MaAIä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23298v1">PDF</a> Accepted by 27th ACM International Conference on Multimodal   Interaction (ICMI â€˜25), Long paper</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§åŸºäºè¯­éŸ³æ´»åŠ¨é¢„æµ‹æ¨¡å‹ï¼ˆVAPï¼‰çš„å®æ—¶ç‚¹å¤´é¢„æµ‹æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä¸ä»…èƒ½å¤Ÿé¢„æµ‹ç‚¹å¤´çš„æ—¶æœºï¼Œè¿˜èƒ½é¢„æµ‹ç‚¹å¤´çš„ç±»å‹ï¼Œè¿™å¯¹äºäººæœºäº¤äº’ä¸­çš„æ³¨æ„åŠ›ç³»ç»Ÿè‡³å…³é‡è¦ã€‚é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ å’Œé¢„è®­ç»ƒæŠ€æœ¯ï¼Œè¯¥æ¨¡å‹åœ¨é¢„æµ‹æ•ˆæœä¸Šæ˜¾è‘—æå‡ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œé™ä½å¤„ç†é€Ÿç‡å¯å®ç°å®æ—¶æ“ä½œè€Œä¸æŸå¤±å‡†ç¡®æ€§ã€‚é›†æˆåˆ°è™šæ‹Ÿè§’è‰²æ³¨æ„åŠ›ç³»ç»Ÿåï¼Œä¸»è§‚è¯„ä¼°æ˜¾ç¤ºå…¶ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éè¨€è¯­ä¿¡æ¯åœ¨äººæœºå¯¹è¯ä¸­è‡³å…³é‡è¦ï¼Œå¦‚ç‚¹å¤´å’Œé¢éƒ¨è¡¨æƒ…ã€‚</li>
<li>å®æ—¶ç‚¹å¤´é¢„æµ‹æ¨¡å‹åŸºäºè¯­éŸ³æ´»åŠ¨é¢„æµ‹æ¨¡å‹ï¼ˆVAPï¼‰æ„å»ºã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹ç‚¹å¤´çš„æ—¶æœºå’Œç±»å‹ã€‚</li>
<li>é€šè¿‡å¤šä»»åŠ¡å­¦ä¹ å’Œé¢„è®­ç»ƒæŠ€æœ¯ï¼Œæ¨¡å‹é¢„æµ‹æ•ˆæœæ˜¾è‘—æå‡ã€‚</li>
<li>é™ä½å¤„ç†é€Ÿç‡å¯å®ç°å®æ—¶æ“ä½œï¼Œä¸”ä¸å½±å“å‡†ç¡®æ€§ã€‚</li>
<li>é›†æˆåˆ°è™šæ‹Ÿè§’è‰²æ³¨æ„åŠ›ç³»ç»Ÿåï¼Œä¸»è§‚è¯„ä¼°æ˜¾ç¤ºè¯¥æ¨¡å‹ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23298">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-458752ced0bc4d07c912c3a9e5e9dfa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-342afd05c6526d5d2eb6ff7d8500949a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07a33c83dd5f46758771e1bebf3e417d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-823ccf16ce1a587fe983b141c93f6b52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e584b2bb129f0073f4510f1d54b42df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba079bca0e9185f43e89b465bc1af867.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f162a317e8191aa34ce82ec687608c01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b44c4576c0a10df7e3ebe7da894ceebd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="User-Feedback-in-Human-LLM-Dialogues-A-Lens-to-Understand-Users-But-Noisy-as-a-Learning-Signal"><a href="#User-Feedback-in-Human-LLM-Dialogues-A-Lens-to-Understand-Users-But-Noisy-as-a-Learning-Signal" class="headerlink" title="User Feedback in Human-LLM Dialogues: A Lens to Understand Users But   Noisy as a Learning Signal"></a>User Feedback in Human-LLM Dialogues: A Lens to Understand Users But   Noisy as a Learning Signal</h2><p><strong>Authors:Yuhan Liu, Michael J. Q. Zhang, Eunsol Choi</strong></p>
<p>Once language models (LMs) are deployed, they can interact with users long-term, ideally evolving continuously based on their feedback. Asking for direct user feedback can be disruptive; thus, we study harvesting user feedback from user-LM interaction logs. We study implicit user feedback in two user-LM interaction datasets (WildChat and LMSYS). First, we analyze user feedback in the user-LLM conversation trajectory, providing insights into when and why such feedback occurs. Second, we study harvesting learning signals from such implicit user feedback. We find that the contents of user feedback (e.g., user wanted clarification), not just the polarity (e.g., users were unhappy with the previous model response), can improve model performance in short human-designed questions (MTBench) but not on longer and more complex questions (WildBench). We also find that the usefulness of user feedback is largely tied to the quality of the userâ€™s initial prompt. Together, we provide an in-depth study of implicit user feedback, showing its potential and limitations. </p>
<blockquote>
<p>å½“è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰è¢«éƒ¨ç½²åï¼Œå®ƒä»¬å¯ä»¥é•¿æœŸä¸ç”¨æˆ·è¿›è¡Œäº¤äº’ï¼Œå¹¶åœ¨ç†æƒ³æƒ…å†µä¸‹æ ¹æ®ç”¨æˆ·çš„åé¦ˆæŒç»­è¿›åŒ–ã€‚ç›´æ¥è¯·æ±‚ç”¨æˆ·åé¦ˆå¯èƒ½ä¼šäº§ç”Ÿå¹²æ‰°ï¼Œå› æ­¤æˆ‘ä»¬ç ”ç©¶äº†ä»ç”¨æˆ·ä¸è¯­è¨€æ¨¡å‹çš„äº¤äº’æ—¥å¿—ä¸­æ”¶é›†ç”¨æˆ·åé¦ˆçš„æ–¹æ³•ã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸¤ä¸ªç”¨æˆ·ä¸è¯­è¨€æ¨¡å‹äº¤äº’æ•°æ®é›†ï¼ˆWildChatå’ŒLMSYSï¼‰ä¸­çš„éšæ€§ç”¨æˆ·åé¦ˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ†æäº†ç”¨æˆ·åœ¨ä¸è¯­è¨€æ¨¡å‹çš„å¯¹è¯è½¨è¿¹ä¸­çš„åé¦ˆï¼Œäº†è§£è¿™ç§åé¦ˆä½•æ—¶ä»¥åŠä¸ºä½•å‘ç”Ÿã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä»è¿™äº›éšæ€§ç”¨æˆ·åé¦ˆä¸­æ”¶é›†å­¦ä¹ ä¿¡å·çš„æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ï¼Œç”¨æˆ·åé¦ˆçš„å†…å®¹ï¼ˆä¾‹å¦‚ï¼Œç”¨æˆ·éœ€è¦æ¾„æ¸…ï¼‰ï¼Œè€Œä¸ä»…ä»…æ˜¯ææ€§ï¼ˆä¾‹å¦‚ï¼Œç”¨æˆ·å¯¹ä¹‹å‰çš„æ¨¡å‹å“åº”ä¸æ»¡æ„ï¼‰ï¼Œå¯ä»¥æé«˜æ¨¡å‹åœ¨ç®€çŸ­çš„äººç±»è®¾è®¡é—®é¢˜ï¼ˆMTBenchï¼‰ä¸Šçš„æ€§èƒ½ï¼Œä½†åœ¨æ›´é•¿çš„ã€æ›´å¤æ‚çš„é—®é¢˜ä¸Šï¼ˆWildBenchï¼‰åˆ™ä¸ç„¶ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œç”¨æˆ·åé¦ˆçš„æœ‰ç”¨æ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºç”¨æˆ·åˆå§‹æç¤ºçš„è´¨é‡ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¯¹éšæ€§ç”¨æˆ·åé¦ˆè¿›è¡Œäº†æ·±å…¥ç ”ç©¶ï¼Œå±•ç¤ºäº†å…¶æ½œåŠ›å’Œå±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23158v1">PDF</a> Earlier version of this paper was presented at 2nd Workshop on Models   of Human Feedback for AI Alignment (MoFA), ICML 2025</p>
<p><strong>Summary</strong></p>
<p>ç”¨æˆ·åé¦ˆçš„è¯­è¨€æ¨¡å‹åœ¨é•¿æœŸäº¤äº’ä¸­èƒ½å¤ŸæŒç»­è¿›åŒ–ã€‚ç ”ç©¶ä»ç”¨æˆ·ä¸è¯­è¨€æ¨¡å‹çš„äº’åŠ¨æ—¥å¿—ä¸­æ”¶é›†ç”¨æˆ·åé¦ˆï¼Œé€šè¿‡å¯¹WildChatå’ŒLMSYSä¸¤ä¸ªæ•°æ®é›†ä¸­çš„ç”¨æˆ·åé¦ˆè¿›è¡Œæ·±åº¦åˆ†æï¼Œå‘ç°ç”¨æˆ·åé¦ˆå†…å®¹è€Œéä»…ä»…åé¦ˆçš„æƒ…æ„Ÿææ€§å¯ä»¥æå‡æ¨¡å‹åœ¨ç®€çŸ­é—®é¢˜ä¸Šçš„è¡¨ç°ï¼Œä½†å¯¹äºæ›´å¤æ‚çš„é—®é¢˜åˆ™æ•ˆæœæœ‰é™ã€‚æ­¤å¤–ï¼Œç”¨æˆ·åé¦ˆçš„æ•ˆç”¨ä¸åˆå§‹æç¤ºçš„è´¨é‡ç´§å¯†ç›¸å…³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æ¨¡å‹åœ¨é•¿æœŸä¸ç”¨æˆ·äº¤äº’è¿‡ç¨‹ä¸­èƒ½å¤ŸæŒç»­è¿›åŒ–ã€‚</li>
<li>ç”¨æˆ·åé¦ˆå¯ä»¥é€šè¿‡åˆ†æç”¨æˆ·ä¸è¯­è¨€æ¨¡å‹çš„äº’åŠ¨æ—¥å¿—æ¥æ”¶é›†ã€‚</li>
<li>ç”¨æˆ·åé¦ˆå†…å®¹å¯¹æ¨¡å‹æ€§èƒ½çš„æå‡æœ‰é‡è¦ä½œç”¨ï¼Œå°¤å…¶æ˜¯åœ¨ç®€çŸ­é—®é¢˜ä¸Šã€‚</li>
<li>å¯¹äºå¤æ‚é—®é¢˜ï¼Œç”¨æˆ·åé¦ˆçš„æ•ˆæœæœ‰é™ã€‚</li>
<li>ç”¨æˆ·åé¦ˆçš„æ•ˆç”¨å—åˆå§‹æç¤ºè´¨é‡çš„å½±å“ã€‚</li>
<li>ç”¨æˆ·çš„åé¦ˆä¸­åŒ…å«å¯¹æ¨¡å‹çš„æ¾„æ¸…éœ€æ±‚ç­‰è¯¦ç»†ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23158">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-1982848de4e951815f201ddedec85e0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02fb48d1544f70130ce975df2130a379.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e455fdec67ed3c24b384673238f5a0af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-299b3b67f6d735a9e76ebe0a70557fbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e6cae4401a5966a9791cef7dd62d0a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-983fdc6e72ebab5f088f59df5f1ad9dd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Listening-to-the-Unspoken-Exploring-365-Aspects-of-Multimodal-Interview-Performance-Assessment"><a href="#Listening-to-the-Unspoken-Exploring-365-Aspects-of-Multimodal-Interview-Performance-Assessment" class="headerlink" title="Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview   Performance Assessment"></a>Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview   Performance Assessment</h2><p><strong>Authors:Jia Li, Yang Wang, Wenhao Qian, Zhenzhen Hu, Richang Hong, Meng Wang</strong></p>
<p>Interview performance assessment is essential for determining candidatesâ€™ suitability for professional positions. To ensure holistic and fair evaluations, we propose a novel and comprehensive framework that explores &#96;&#96;365â€™â€™ aspects of interview performance by integrating \textit{three} modalities (video, audio, and text), \textit{six} responses per candidate, and \textit{five} key evaluation dimensions. The framework employs modality-specific feature extractors to encode heterogeneous data streams and subsequently fused via a Shared Compression Multilayer Perceptron. This module compresses multimodal embeddings into a unified latent space, facilitating efficient feature interaction. To enhance prediction robustness, we incorporate a two-level ensemble learning strategy: (1) independent regression heads predict scores for each response, and (2) predictions are aggregated across responses using a mean-pooling mechanism to produce final scores for the five target dimensions. By listening to the unspoken, our approach captures both explicit and implicit cues from multimodal data, enabling comprehensive and unbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our framework secured first place in the AVI Challenge 2025, demonstrating its effectiveness and robustness in advancing automated and multimodal interview performance assessment. The full implementation is available at <a target="_blank" rel="noopener" href="https://github.com/MSA-LMC/365Aspects">https://github.com/MSA-LMC/365Aspects</a>. </p>
<blockquote>
<p>é¢è¯•ç»©æ•ˆè¯„ä¼°å¯¹äºç¡®å®šå€™é€‰äººæ˜¯å¦é€‚åˆä¸“ä¸šèŒä½è‡³å…³é‡è¦ã€‚ä¸ºäº†ç¡®ä¿å…¨é¢å’Œå…¬å¹³çš„è¯„ä»·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹ä¸”å…¨é¢çš„æ¡†æ¶ï¼Œé€šè¿‡æ•´åˆè§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬ä¸‰ç§æ¨¡å¼ï¼Œé’ˆå¯¹æ¯ä¸ªå€™é€‰äººçš„å…­ç§å›åº”ï¼Œä»¥åŠäº”ä¸ªå…³é”®è¯„ä»·ç»´åº¦ï¼Œæ¥æ¢ç´¢é¢è¯•è¡¨ç°çš„â€œ365â€æ–¹é¢ã€‚è¯¥æ¡†æ¶ä½¿ç”¨ç‰¹å®šäºæ¨¡æ€çš„ç‰¹å¾æå–å™¨æ¥ç¼–ç å¼‚è´¨æ•°æ®æµï¼Œç„¶åé€šè¿‡å…±äº«å‹ç¼©å¤šå±‚æ„ŸçŸ¥å™¨è¿›è¡Œèåˆã€‚æ­¤æ¨¡å—å°†å¤šæ¨¡æ€åµŒå…¥å‹ç¼©åˆ°ç»Ÿä¸€çš„æ½œåœ¨ç©ºé—´ï¼Œä¿ƒè¿›äº†ç‰¹å¾çš„æœ‰æ•ˆäº¤äº’ã€‚ä¸ºäº†æé«˜é¢„æµ‹çš„ç¨³å®šæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤çº§é›†æˆå­¦ä¹ ç­–ç•¥ï¼šï¼ˆ1ï¼‰ç‹¬ç«‹çš„å›å½’å¤´ç”¨äºé¢„æµ‹æ¯ä¸ªå›åº”çš„åˆ†æ•°ï¼Œï¼ˆ2ï¼‰ä½¿ç”¨å¹³å‡æ± åŒ–æœºåˆ¶å¯¹å›åº”çš„é¢„æµ‹è¿›è¡Œæ±‡æ€»ï¼Œä»¥äº§ç”Ÿäº”ä¸ªç›®æ ‡ç»´åº¦çš„æœ€ç»ˆåˆ†æ•°ã€‚é€šè¿‡å€¾å¬æœªè¨€æ˜çš„å†…å®¹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿä»å¤šæ¨¡å¼æ•°æ®ä¸­æ•è·æ˜ç¡®å’Œéšå«çš„çº¿ç´¢ï¼Œä»è€Œå®ç°å…¨é¢å’Œæ— åçš„è¯„ä»·ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨2025å¹´AVIæŒ‘æˆ˜èµ›ä¸­è·å¾—äº†ç¬¬ä¸€åï¼Œå®ç°äº†å¹³å‡å‡æ–¹è¯¯å·®0.1824ï¼Œè¯æ˜äº†å…¶åœ¨æ¨åŠ¨è‡ªåŠ¨åŒ–å’Œå¤šæ¨¡å¼é¢è¯•ç»©æ•ˆè¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚å®Œæ•´å®ç°å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MSA-LMC/365Aspects%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MSA-LMC/365Aspectsä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22676v1">PDF</a> 8 pages, 4 figures, ACM MM 2025.   github:<a target="_blank" rel="noopener" href="https://github.com/MSA-LMC/365Aspects">https://github.com/MSA-LMC/365Aspects</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„é¢è¯•è¡¨ç°è¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æ•´åˆè§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬ä¸‰ç§æ¨¡æ€ï¼Œä»å…­ä¸ªæ–¹é¢å¯¹å€™é€‰äººçš„é¢è¯•è¡¨ç°è¿›è¡Œå…¨æ–¹ä½è¯„ä»·ã€‚è¯¥æ¡†æ¶è¿ç”¨æ¨¡æ€ç‰¹å®šç‰¹å¾æå–å™¨ç¼–ç å¼‚è´¨æ•°æ®æµï¼Œå¹¶é€šè¿‡å…±äº«å‹ç¼©å¤šå±‚æ„ŸçŸ¥å™¨è¿›è¡Œèåˆã€‚åŒæ—¶é‡‡ç”¨ä¸¤çº§é›†æˆå­¦ä¹ ç­–ç•¥ï¼Œæé«˜é¢„æµ‹ç¨³å¥æ€§ã€‚è¯¥æ¡†æ¶åœ¨AVI Challenge 2025ä¸­è·å¾—ç¬¬ä¸€åï¼Œå±•ç¤ºäº†å…¶åœ¨æ¨è¿›è‡ªåŠ¨åŒ–å’Œå¤šæ¨¡æ€é¢è¯•è¡¨ç°è¯„ä¼°ä¸­çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºçš„é¢è¯•è¯„ä¼°æ¡†æ¶èåˆäº†è§†é¢‘ã€éŸ³é¢‘å’Œæ–‡æœ¬ä¸‰ç§æ¨¡æ€ï¼Œå®ç°å…¨é¢è¯„ä»·ã€‚</li>
<li>æ¡†æ¶åŒ…å«å…­ä¸ªæ–¹é¢çš„å€™é€‰äººå“åº”è¯„ä»·ã€‚</li>
<li>é€šè¿‡å…±äº«å‹ç¼©å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œä¸åŒæ¨¡æ€çš„æ•°æ®è¢«ç¼–ç å¹¶èåˆåˆ°ç»Ÿä¸€æ½œåœ¨ç©ºé—´ã€‚</li>
<li>é‡‡ç”¨ä¸¤çº§é›†æˆå­¦ä¹ ç­–ç•¥ï¼Œæé«˜é¢„æµ‹å‡†ç¡®æ€§åŠç¨³å¥æ€§ã€‚</li>
<li>æ¡†æ¶èƒ½æ•æ‰é¢è¯•ä¸­çš„æ˜ç¡®å’Œéšå«çº¿ç´¢ï¼Œè¿›è¡Œå®¢è§‚è¯„ä¼°ã€‚</li>
<li>æ¡†æ¶åœ¨AVI Challenge 2025ä¸­è·ç¬¬ä¸€åï¼Œè¯æ˜å…¶æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22676">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-23b37bf3f2b50a1a3fe067f613bb1927.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8cacd4aa62419882747cc0598993db9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00e1b63ca6ff1a48be37251b4c32bfde.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c98fda1e2ddf44e2b147360dcdc5c9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d8e8a835656456aaaeed2fe594cf64c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f7e5fd2a33ed6e2c5ff6700c84ba098.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Measurement-of-production-branching-ratio-after-muon-nuclear-capture-reaction-of-Al-and-Si-isotopes"><a href="#Measurement-of-production-branching-ratio-after-muon-nuclear-capture-reaction-of-Al-and-Si-isotopes" class="headerlink" title="Measurement of production branching ratio after muon nuclear capture   reaction of Al and Si isotopes"></a>Measurement of production branching ratio after muon nuclear capture   reaction of Al and Si isotopes</h2><p><strong>Authors:R. Mizuno, M. Niikura, T. Y. Saito, T. Matsuzaki, S. Abe, H. Fukuda, M. Hashimoto, A. Hillier, K. Ishida, N. Kawamura, S. Kawase, T. Kawata, K. Kitafuji, F. Minato, M. Oishi, A. Sato, K. Shimomura, P. Strasser, S. Takeshita, D. Tomono, Y. Watanabe</strong></p>
<p>Background: Muon nuclear capture is a reaction between a muon and a proton inside a nucleus through weak interactions. This reaction results in the formation of an excited nucleus, which subsequently de-excites by emitting several particles. Examination of the excited state allows for an investigation of the properties of nuclear excitation and particle emission in highly excited nuclei. Purpose: This study investigates muon nuclear capture of 27Al and 28,29,30Si, focusing on determining the absolute production branching ratio (BR) following muon nuclear capture and subsequent particle emissions. By measuring the absolute production BR, we can collect valuable information on the excitation energy distribution of muon nuclear capture. Methods: Measurements were conducted using the in-beam activation method at two pulsed muon facilities: RIKEN-RAL at RAL and MLF at JPARC. Absolute BRs were determined by measuring the irradiated muon number using a plastic scintillator and the \b{eta}-delayed {\gamma}-rays emitted from the produced nuclei using germanium detectors. Results: The absolute production branching ratios of muon nuclear capture on 27Al and 28,29,30Si were obtained with the highest accuracy to date. Predominant neutron emissions, even-odd atomic number dependence on particle emission probabilities, and influence of the neutron excess were observed. These results were compared with previous measurements and theoretical models and discussed regarding the excitation energy distribution, particle emission mechanism, and nuclear properties, such as resonance in the isovector transition. Conclusion: This study emphasizes the importance of considering nuclear structure effects, even-odd effects of proton and neutron numbers, neutron excess, nucleon pairing effect, and particle emission mechanisms, in the context of the muon nuclear capture reaction. </p>
<blockquote>
<p>èƒŒæ™¯ï¼šÎ¼å­æ ¸ä¿˜è·æ˜¯Î¼å­å’ŒåŸå­æ ¸å†…è´¨å­é€šè¿‡å¼±ç›¸äº’ä½œç”¨å‘ç”Ÿçš„ååº”ã€‚è¯¥ååº”å¯¼è‡´æ¿€å‘æ€æ ¸çš„å½¢æˆï¼Œéšåé€šè¿‡å‘å°„å¤šä¸ªç²’å­è€Œè¾¾åˆ°ç¨³å®šã€‚å¯¹æ¿€å‘æ€çš„ç ”ç©¶å¯ä»¥æ¢è®¨é«˜åº¦æ¿€å‘æ ¸çš„æ ¸æ¿€å‘å’Œç²’å­å‘å°„çš„æ€§è´¨ã€‚</p>
</blockquote>
<p>ç›®çš„ï¼šæœ¬ç ”ç©¶è°ƒæŸ¥äº†Î¼å­åœ¨æ ¸ä¸Šçš„ä¿˜è·æƒ…å†µï¼Œé‡ç‚¹æ¢è®¨äº†Î¼å­æ ¸ä¿˜è·åå‘ç”Ÿç²’å­å‘å°„çš„ç»å¯¹äº§ç”Ÿåˆ†æ”¯æ¯”ï¼ˆBRï¼‰ã€‚é€šè¿‡æµ‹é‡ç»å¯¹ç”Ÿäº§BRï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—æœ‰å…³Î¼å­æ ¸ä¿˜è·çš„æ¿€å‘èƒ½åˆ†å¸ƒçš„æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚</p>
<p>æ–¹æ³•ï¼šå®éªŒåœ¨ä¸¤ä¸ªè„‰å†²Î¼å­è®¾æ–½ï¼ˆRALçš„RIKEN-RALå’ŒJPARCçš„MLFï¼‰ä¸Šè¿›è¡Œäº†åŸä½æ´»åŒ–æ³•æµ‹é‡ã€‚é€šè¿‡ä½¿ç”¨å¡‘æ–™é—ªçƒä½“æµ‹é‡ç…§å°„çš„Î¼å­æ•°é‡ï¼Œå¹¶ä½¿ç”¨é”—æ¢æµ‹å™¨æµ‹é‡äº§ç”Ÿçš„æ ¸å‘å‡ºçš„å»¶è¿ŸÎ³å°„çº¿ï¼Œç¡®å®šäº†ç»å¯¹BRã€‚</p>
<p>ç»“æœï¼šè·å¾—äº†è¿„ä»Šä¸ºæ­¢å¯¹Î¼å­åœ¨æ ¸ä¸Šï¼ˆå¦‚é“å’Œå…¶ä»–ç¡…åŒä½ç´ ï¼‰å‘ç”Ÿä¿˜è·æ—¶çš„æœ€é«˜ç²¾åº¦çš„ç»å¯¹ç”Ÿäº§åˆ†æ”¯æ¯”ç»“æœã€‚è§‚å¯Ÿåˆ°ä¸»è¦çš„ä¸­å­å‘å°„ç°è±¡ã€ç²’å­å‘å°„æ¦‚ç‡ä¸åŸå­åºæ•°çš„å¥‡å¶ä¾èµ–æ€§ä»¥åŠä¸­å­è¿‡å‰©çš„å½±å“ã€‚è¿™äº›ç»“æœä¸ä¹‹å‰çš„æµ‹é‡å’Œç†è®ºæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒå’Œè®¨è®ºï¼Œæ¶‰åŠæ¿€å‘èƒ½åˆ†å¸ƒã€ç²’å­å‘å°„æœºåˆ¶å’Œè¯¸å¦‚åŒä½æ—‹è¿‡æ¸¡å…±æŒ¯çš„æ ¸æ€§è´¨ç­‰æ–¹é¢ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19753v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†Î¼å­åœ¨æ ¸å†…çš„æ•è·ååº”ï¼Œé‡ç‚¹æ¢è®¨äº†Î¼å­åœ¨æ ¸å†…çš„æ•è·åäº§ç”Ÿçš„æ¿€å‘æ€æ ¸çš„æ€§è´¨ä»¥åŠç²’å­å‘å°„è¿‡ç¨‹ã€‚é€šè¿‡å¯¹Î¼å­åœ¨æ ¸å†…çš„æ•è·ååº”çš„ç ”ç©¶ï¼Œç¡®å®šäº†åœ¨ä¸åŒåŸå­æ ¸ä¸Šå‘ç”Ÿçš„ç»å¯¹ç”Ÿäº§åˆ†æ”¯æ¯”ï¼ˆBRï¼‰ã€‚åˆ©ç”¨è„‰å†²Î¼å­è®¾æ–½çš„æµ‹é‡ï¼Œæµ‹é‡äº†è¾å°„çš„Î¼å­æ•°å’Œäº§ç”Ÿçš„æ ¸äº§ç”Ÿçš„Î·å»¶è¿ŸÎ³å°„çº¿æ¥ç¡®å®šç»å¯¹BRå€¼ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºäº†ä¸­å­å‘å°„çš„ä¸»å¯¼ä½œç”¨ï¼Œç²’å­å‘å°„æ¦‚ç‡çš„å¥‡å¶åŸå­æ•°ä¾èµ–æ€§ä»¥åŠä¸­å­è¿‡å‰©çš„å½±å“ã€‚è¿™äº›ç»“æœå¯¹äºç†è§£æ ¸æ¿€å‘æ€çš„èƒ½é‡åˆ†å¸ƒã€ç²’å­å‘å°„æœºåˆ¶å’Œæ ¸ç»“æ„æ•ˆåº”å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>Î¼å­åœ¨æ ¸å†…çš„æ•è·ååº”æ˜¯ä¸€ç§é‡è¦çš„ç ”ç©¶æ‰‹æ®µï¼Œç”¨äºç ”ç©¶é«˜åº¦æ¿€å‘æ€æ ¸çš„æ€§è´¨å’Œç²’å­å‘å°„è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡æµ‹é‡ç»å¯¹ç”Ÿäº§åˆ†æ”¯æ¯”ï¼ˆBRï¼‰ï¼Œå¯ä»¥äº†è§£Î¼å­æ ¸æ•è·çš„æ¿€å‘èƒ½é‡åˆ†å¸ƒã€‚</li>
<li>ç ”ç©¶å‘ç°ä¸­å­å‘å°„å ä¸»å¯¼åœ°ä½ï¼Œç²’å­å‘å°„æ¦‚ç‡ä¸åŸå­æ•°çš„å¥‡å¶æ€§æœ‰å…³ï¼Œå¹¶ä¸”ä¸­å­è¿‡å‰©ä¹Ÿä¼šå½±å“ååº”ç»“æœã€‚</li>
<li>è¿™äº›å‘ç°æœ‰åŠ©äºç†è§£æ ¸æ¿€å‘æ€çš„èƒ½é‡åˆ†å¸ƒã€ç²’å­å‘å°„æœºåˆ¶å’Œæ ¸ç»“æ„æ•ˆåº”ç­‰æ ¸å¿ƒé—®é¢˜ã€‚</li>
<li>å¯¹æ¯”ä»¥å¾€çš„ç ”ç©¶å’Œç†è®ºæ¨¡å‹ï¼Œè¯¥ç ”ç©¶ä¸ºæœªæ¥æ›´æ·±å…¥çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19753">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-703cb4254eecd10e29c6e5eaefcec90b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce2a690974183d4b9ce3cb453f190e52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-282041be4c58a117817eaa274d61894d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae4be94d96d35e4c77b5488c0a226b3e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DIFFA-Large-Language-Diffusion-Models-Can-Listen-and-Understand"><a href="#DIFFA-Large-Language-Diffusion-Models-Can-Listen-and-Understand" class="headerlink" title="DIFFA: Large Language Diffusion Models Can Listen and Understand"></a>DIFFA: Large Language Diffusion Models Can Listen and Understand</h2><p><strong>Authors:Jiaming Zhou, Hongjie Chen, Shiwan Zhao, Jian Kang, Jie Li, Enzhi Wang, Yujie Guo, Haoqin Sun, Hui Wang, Aobo Kong, Yong Qin, Xuelong Li</strong></p>
<p>Recent advances in Large language models (LLMs) have shown remarkable capabilities across textual and multimodal domains. In parallel, diffusion-based language models have emerged as a promising alternative to the autoregressive paradigm, offering improved controllability, bidirectional context modeling, and robust generation. However, their application to the audio modality remains underexplored. In this work, we introduce \textbf{DIFFA}, the first diffusion-based Large Audio-Language Model designed to perform spoken language understanding. DIFFA integrates a frozen diffusion language model with a lightweight dual-adapter architecture that bridges speech understanding and natural language reasoning. We employ a two-stage training pipeline: first, aligning semantic representations via an ASR objective; then, learning instruction-following abilities through synthetic audio-caption pairs automatically generated by prompting LLMs. Despite being trained on only 960 hours of ASR and 127 hours of synthetic instruction data, DIFFA demonstrates competitive performance on major benchmarks, including MMSU, MMAU, and VoiceBench, outperforming several autoregressive open-source baselines. Our results reveal the potential of diffusion-based language models for efficient and scalable audio understanding, opening a new direction for speech-driven AI. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/DIFFA.git">https://github.com/NKU-HLT/DIFFA.git</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥åœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€é¢†åŸŸè¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼ŒåŸºäºæ‰©æ•£çš„è¯­è¨€æ¨¡å‹ä½œä¸ºè‡ªå›å½’èŒƒå¼çš„æœ‰å‰é€”çš„æ›¿ä»£å“è€Œå‡ºç°ï¼Œæä¾›äº†æ›´å¥½çš„å¯æ§æ€§ã€åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œç¨³å¥çš„ç”Ÿæˆã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨éŸ³é¢‘æ¨¡æ€çš„åº”ç”¨ä»ç„¶è¢«è¾ƒå°‘æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ç¬¬ä¸€ä¸ªåŸºäºæ‰©æ•£çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹â€”â€”DIFFAï¼Œæ—¨åœ¨è¿›è¡Œå£è¯­ç†è§£ã€‚DIFFAæ•´åˆäº†ä¸€ä¸ªå†»ç»“çš„æ‰©æ•£è¯­è¨€æ¨¡å‹å’Œä¸€ä¸ªè½»é‡çº§çš„åŒé€‚é…å™¨æ¶æ„ï¼Œè¯¥æ¶æ„èƒ½å¤Ÿæ¡¥æ¥è¯­éŸ³ç†è§£å’Œè‡ªç„¶è¯­è¨€æ¨ç†ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼šé¦–å…ˆï¼Œé€šè¿‡ASRç›®æ ‡å¯¹é½è¯­ä¹‰è¡¨ç¤ºï¼›ç„¶åï¼Œé€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„åˆæˆéŸ³é¢‘å­—å¹•å¯¹æ¥å­¦ä¹ éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚å°½ç®¡ä»…åœ¨960å°æ—¶çš„ASRå’Œ127å°æ—¶çš„åˆæˆæŒ‡ä»¤æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒDIFFAåœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼ŒåŒ…æ‹¬MMSUã€MMAUå’ŒVoiceBenchï¼Œä¼˜äºå¤šä¸ªè‡ªå›å½’å¼€æºåŸºå‡†ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†æ‰©æ•£è¯­è¨€æ¨¡å‹åœ¨é«˜æ•ˆå’Œå¯æ‰©å±•éŸ³é¢‘ç†è§£æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºè¯­éŸ³é©±åŠ¨çš„AIå¼€è¾Ÿäº†æ–°çš„æ–¹å‘ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/DIFFA.git%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/NKU-HLT/DIFFA.gitä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18452v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€é¢†åŸŸå±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶å¼•å…¥é¦–ä¸ªåŸºäºæ‰©æ•£çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹DIFFAï¼Œç”¨äºè¿›è¡Œå£è¯­ç†è§£ã€‚DIFFAç»“åˆå†»ç»“çš„æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸è½»é‡çº§åŒé€‚é…å™¨æ¶æ„ï¼Œå®ç°è¯­éŸ³ç†è§£å’Œè‡ªç„¶è¯­è¨€æ¨ç†çš„æ¡¥æ¢ã€‚é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼šé¦–å…ˆé€šè¿‡ASRç›®æ ‡å¯¹é½è¯­ä¹‰è¡¨ç¤ºï¼Œç„¶åé€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„åˆæˆè¯­éŸ³æŒ‡ä»¤å¯¹è¿›è¡Œå­¦ä¹ æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚å°½ç®¡ä»…åœ¨960å°æ—¶çš„ASRå’Œ127å°æ—¶çš„åˆæˆæŒ‡ä»¤æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒDIFFAåœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼ŒåŒ…æ‹¬MMSUã€MMAUå’ŒVoiceBenchï¼Œä¼˜äºå¤šä¸ªè‡ªåŠ¨å›å½’å¼€æºåŸºå‡†ã€‚ç»“æœè¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹åœ¨éŸ³é¢‘ç†è§£æ–¹é¢å…·æœ‰é«˜æ•ˆä¸”å¯æ‰©å±•çš„æ½œåŠ›ï¼Œä¸ºè¯­éŸ³é©±åŠ¨çš„äººå·¥æ™ºèƒ½å¼€å¯æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DIFFAæ˜¯é¦–ä¸ªåŸºäºæ‰©æ•£çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå£è¯­ç†è§£ã€‚</li>
<li>DIFFAç»“åˆå†»ç»“çš„æ‰©æ•£è¯­è¨€æ¨¡å‹å’Œè½»é‡çº§åŒé€‚é…å™¨æ¶æ„ã€‚</li>
<li>DIFFAé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒï¼Œé¦–å…ˆé€šè¿‡ASRç›®æ ‡å¯¹é½è¯­ä¹‰è¡¨ç¤ºï¼Œç„¶åå­¦ä¹ æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚</li>
<li>DIFFAåœ¨åˆæˆè¯­éŸ³æŒ‡ä»¤å¯¹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„åˆæˆéŸ³é¢‘å­—å¹•ã€‚</li>
<li>DIFFAåœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬MMSUã€MMAUå’ŒVoiceBenchã€‚</li>
<li>DIFFAä¼˜äºå¤šä¸ªè‡ªåŠ¨å›å½’å¼€æºåŸºå‡†ï¼Œå±•ç°å‡ºæ‰©æ•£æ¨¡å‹åœ¨éŸ³é¢‘ç†è§£æ–¹é¢çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18452">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8afb024cec6a28de96294b2f2b1709d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7cb01fae4bd66d4dcad433b5d6928a00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e71bc595775eea249be477ee65c0d847.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-771e40bc61a1a3f2058282134a2e117a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Watch-Listen-Understand-Mislead-Tri-modal-Adversarial-Attacks-on-Short-Videos-for-Content-Appropriateness-Evaluation"><a href="#Watch-Listen-Understand-Mislead-Tri-modal-Adversarial-Attacks-on-Short-Videos-for-Content-Appropriateness-Evaluation" class="headerlink" title="Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on   Short Videos for Content Appropriateness Evaluation"></a>Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on   Short Videos for Content Appropriateness Evaluation</h2><p><strong>Authors:Sahid Hossain Mustakim, S M Jishanul Islam, Ummay Maria Muna, Montasir Chowdhury, Mohammed Jawwadul Islam, Sadia Ahmmed, Tashfia Sikder, Syed Tasdid Azam Dhrubo, Swakkhar Shatabda</strong></p>
<p>Multimodal Large Language Models (MLLMs) are increasingly used for content moderation, yet their robustness in short-form video contexts remains underexplored. Current safety evaluations often rely on unimodal attacks, failing to address combined attack vulnerabilities. In this paper, we introduce a comprehensive framework for evaluating the tri-modal safety of MLLMs. First, we present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising diverse short-form videos with human-guided synthetic adversarial attacks. Second, we propose ChimeraBreak, a novel tri-modal attack strategy that simultaneously challenges visual, auditory, and semantic reasoning pathways. Extensive experiments on state-of-the-art MLLMs reveal significant vulnerabilities with high Attack Success Rates (ASR). Our findings uncover distinct failure modes, showing model biases toward misclassifying benign or policy-violating content. We assess results using LLM-as-a-judge, demonstrating attack reasoning efficacy. Our dataset and findings provide crucial insights for developing more robust and safe MLLMs. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¶Šæ¥è¶Šå¤šåœ°ç”¨äºå†…å®¹å®¡æ ¸ï¼Œä½†å®ƒä»¬åœ¨çŸ­è§†é¢‘ä¸Šä¸‹æ–‡ä¸­çš„ç¨³å¥æ€§å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å½“å‰çš„å®‰å…¨è¯„ä¼°é€šå¸¸ä¾èµ–äºå•æ¨¡æ€æ”»å‡»ï¼Œæ— æ³•è§£å†³ç»„åˆæ”»å‡»æ¼æ´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªç”¨äºè¯„ä¼°MLLMsçš„ä¸‰æ¨¡æ€å®‰å…¨æ€§çš„ç»¼åˆæ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†çŸ­è§†é¢‘å¤šæ¨¡æ€å¯¹æŠ—ï¼ˆSVMAï¼‰æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¸¦æœ‰ç”±äººç±»å¼•å¯¼çš„åˆæˆå¯¹æŠ—æ€§æ”»å‡»çš„å¤šç§çŸ­è§†é¢‘ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¸‰æ¨¡æ€æ”»å‡»ç­–ç•¥chimeraBreakï¼Œå®ƒåŒæ—¶æŒ‘æˆ˜è§†è§‰ã€å¬è§‰å’Œè¯­ä¹‰æ¨ç†è·¯å¾„ã€‚åœ¨æœ€æ–°MLLMsä¸Šçš„å¹¿æ³›å®éªŒæ˜¾ç¤ºå­˜åœ¨æ˜¾è‘—æ¼æ´ï¼Œæ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰å¾ˆé«˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœæ­ç¤ºäº†ä¸åŒçš„å¤±è´¥æ¨¡å¼ï¼Œæ˜¾ç¤ºäº†æ¨¡å‹åå‘äºé”™è¯¯åˆ†ç±»è‰¯æ€§æˆ–è¿åç­–ç•¥çš„å†…å®¹ã€‚æˆ‘ä»¬ä½¿ç”¨LLMä½œä¸ºæ³•å®˜è¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜äº†æ”»å‡»æ¨ç†çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œç ”ç©¶ç»“æœå¯¹äºå¼€å‘æ›´ç¨³å¥å’Œå®‰å…¨çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†å…³é”®è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11968v1">PDF</a> Accepted as long paper, SVU Workshop at ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨çŸ­è§†é¢‘å†…å®¹å®¡æ ¸ä¸­çš„åº”ç”¨åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§è¯„ä¼°MLLMsä¸‰æ¨¡æ€å®‰å…¨æ€§çš„ç»¼åˆæ¡†æ¶ï¼ŒåŒ…æ‹¬Short-Video Multimodal Adversarialï¼ˆSVMAï¼‰æ•°æ®é›†å’ŒChimeraBreakæ–°å‹ä¸‰æ¨¡æ€æ”»å‡»ç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒMLLMså­˜åœ¨æ˜¾è‘—æ¼æ´ï¼Œéœ€åŠ å¼ºæ¨¡å‹å¯¹è‰¯æ€§æˆ–è¿è§„å†…å®¹çš„è¯†åˆ«èƒ½åŠ›ã€‚æ–‡ç« æä¾›çš„æ•°æ®é›†å’Œå‘ç°å¯¹äºå¼€å‘æ›´ç¨³å¥å’Œå®‰å…¨çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è‡³å…³é‡è¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMsåœ¨çŸ­è§†é¢‘å†…å®¹å®¡æ ¸ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶ç¨³å¥æ€§æœ‰å¾…æ¢ç´¢ã€‚</li>
<li>å½“å‰å®‰å…¨è¯„ä¼°ä¸»è¦ä¾èµ–å•æ¨¡æ€æ”»å‡»ï¼Œæ— æ³•åº”å¯¹ç»„åˆæ”»å‡»æ¼æ´ã€‚</li>
<li>å¼•å…¥SVMAæ•°æ®é›†ï¼ŒåŒ…å«å¸¦æœ‰åˆæˆå¯¹æŠ—æ”»å‡»çš„çŸ­è§†é¢‘ã€‚</li>
<li>æå‡ºChimeraBreakä¸‰æ¨¡æ€æ”»å‡»ç­–ç•¥ï¼ŒåŒæ—¶æŒ‘æˆ˜è§†è§‰ã€å¬è§‰å’Œè¯­ä¹‰æ¨ç†è·¯å¾„ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹å­˜åœ¨æ˜¾è‘—æ¼æ´å’Œé«˜æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ã€‚</li>
<li>æ¨¡å‹å¯¹è‰¯æ€§æˆ–è¿è§„å†…å®¹çš„è¯†åˆ«å­˜åœ¨åè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11968">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-34d4c0f5eb4b8eb8e9561989c634116a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-881ae9b9d4272710de7fcd61933ace9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-053153bcb4f20a7517c1a705c94ac0a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7eb0e8cd25d1e8641ce0d554ab57d8ad.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DoctorAgent-RL-A-Multi-Agent-Collaborative-Reinforcement-Learning-System-for-Multi-Turn-Clinical-Dialogue"><a href="#DoctorAgent-RL-A-Multi-Agent-Collaborative-Reinforcement-Learning-System-for-Multi-Turn-Clinical-Dialogue" class="headerlink" title="DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning   System for Multi-Turn Clinical Dialogue"></a>DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning   System for Multi-Turn Clinical Dialogue</h2><p><strong>Authors:Yichun Feng, Jiawei Wang, Lu Zhou, Zhen Lei, Yixue Li</strong></p>
<p>Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Single-round consultation systems require patients to describe all symptoms upfront, leading to vague diagnosis with unclear complaints. Traditional multi-turn dialogue models, constrained by static supervised learning, lack flexibility and fail to intelligently extract key clinical information. To address these limitations, we propose \Ours{}, a reinforcement learning (RL)-based multi-agent collaborative framework that models medical consultations as a dynamic decision-making process under uncertainty. The doctor agent continuously optimizes its questioning strategy within the RL framework through multi-turn interactions with the patient agent, dynamically adjusting its information-gathering path based on comprehensive rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop interaction strategies aligned with clinical reasoning logic, rather than superficially imitating patterns in existing dialogue data. Notably, we constructed MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating patient interactions. Experiments demonstrate that \Ours{} outperforms existing models in both multi-turn reasoning capability and final diagnostic performance. This approach shows immense practical value by reducing misdiagnosis risks in time-pressured settings, freeing clinicians for complex cases, and pioneering a strategy to optimize medical resource allocation and alleviate workforce shortages. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/JarvisUSTC/DoctorAgent-RL">https://github.com/JarvisUSTC/DoctorAgent-RL</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦é—®ç­”é¢†åŸŸè¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œçš„ä¸´åºŠå’¨è¯¢ä¸­çš„åº”ç”¨ä»é¢ä¸´æ ¸å¿ƒæŒ‘æˆ˜ã€‚å•è½®å’¨è¯¢ç³»ç»Ÿè¦æ±‚æ‚£è€…ä¸€æ¬¡æ€§æè¿°æ‰€æœ‰ç—‡çŠ¶ï¼Œå¯¼è‡´è¯Šæ–­æ¨¡ç³Šï¼ŒæŠ•è¯‰ä¸æ˜ç¡®ã€‚å—é™æ€ç›‘ç£å­¦ä¹ é™åˆ¶çš„ä¼ ç»Ÿå¤šè½®å¯¹è¯æ¨¡å‹ï¼Œç¼ºä¹çµæ´»æ€§ï¼Œæ— æ³•æ™ºèƒ½æå–å…³é”®ä¸´åºŠä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†\Ours{}ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼Œå®ƒå°†åŒ»ç–—å’¨è¯¢å»ºæ¨¡ä¸ºä¸ç¡®å®šç¯å¢ƒä¸‹çš„åŠ¨æ€å†³ç­–è¿‡ç¨‹ã€‚åŒ»ç”Ÿæ™ºèƒ½ä½“é€šè¿‡åœ¨RLæ¡†æ¶å†…ä¸ç—…äººæ™ºèƒ½ä½“è¿›è¡Œå¤šè½®äº’åŠ¨ï¼Œä¸æ–­ä¼˜åŒ–å…¶æé—®ç­–ç•¥ï¼Œå¹¶æ ¹æ®å’¨è¯¢è¯„ä¼°å™¨çš„ç»¼åˆå¥–åŠ±åŠ¨æ€è°ƒæ•´å…¶ä¿¡æ¯æ”¶é›†è·¯å¾„ã€‚è¿™ç§RLå¾®è°ƒæœºåˆ¶ä½¿LLMèƒ½å¤Ÿè‡ªä¸»å‘å±•ç¬¦åˆä¸´åºŠæ¨ç†é€»è¾‘çš„äº’åŠ¨ç­–ç•¥ï¼Œè€Œä¸æ˜¯ç®€å•åœ°æ¨¡ä»¿ç°æœ‰å¯¹è¯æ•°æ®ä¸­çš„æ¨¡å¼ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æ„å»ºäº†MTMedDialogï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿæ¨¡æ‹Ÿç—…äººäº’åŠ¨çš„è‹±æ–‡å¤šè½®åŒ»ç–—å’¨è¯¢æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼Œ\Ours{}åœ¨å¤šè½®æ¨ç†èƒ½åŠ›å’Œæœ€ç»ˆè¯Šæ–­æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•åœ¨å‡å°‘æ—¶é—´å‹åŠ›ä¸‹è¯¯è¯Šé£é™©ã€è§£æ”¾ä¸´åºŠåŒ»ç”Ÿå¤„ç†å¤æ‚ç—…ä¾‹ã€ä¼˜åŒ–åŒ»ç–—èµ„æºé…ç½®å’Œç¼“è§£åŠ³åŠ¨åŠ›çŸ­ç¼ºç­‰æ–¹é¢å±•ç°äº†å·¨å¤§çš„å®ç”¨ä»·å€¼ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JarvisUSTC/DoctorAgent-RL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/JarvisUSTC/DoctorAgent-RLæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19630v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦é—®ç­”é¢†åŸŸè¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†åœ¨ç°å®ä¸´åºŠå’¨è¯¢ä¸­çš„åº”ç”¨ä»é¢ä¸´æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¸ºè§£å†³å•è½®å’¨è¯¢ç³»ç»Ÿæ¨¡ç³Šè¯Šæ–­åŠä¼ ç»Ÿå¤šè½®å¯¹è¯æ¨¡å‹ç¼ºä¹çµæ´»æ€§çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶\Ours{}ï¼Œå°†åŒ»ç–—å’¨è¯¢å»ºæ¨¡ä¸ºä¸ç¡®å®šæ¡ä»¶ä¸‹çš„åŠ¨æ€å†³ç­–è¿‡ç¨‹ã€‚åŒ»ç”Ÿæ™ºèƒ½ä½“é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¸æ‚£è€…æ™ºèƒ½ä½“è¿›è¡Œå¤šè½®äº’åŠ¨ï¼Œå¹¶æ ¹æ®æ¥è‡ªå’¨è¯¢è¯„ä¼°è€…çš„ç»¼åˆå¥–åŠ±ä¸æ–­è°ƒæ•´å…¶ä¿¡æ¯æ”¶é›†è·¯å¾„ã€‚è¿™ä½¿å¾—è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»å‘å±•ç¬¦åˆä¸´åºŠæ¨ç†é€»è¾‘çš„äº¤æµç­–ç•¥ï¼Œè€Œéç®€å•åœ°æ¨¡ä»¿ç°æœ‰å¯¹è¯æ•°æ®ä¸­çš„æ¨¡å¼ã€‚å®éªŒè¯æ˜ï¼Œ\Ours{}åœ¨å¤šè½®æ¨ç†èƒ½åŠ›å’Œæœ€ç»ˆè¯Šæ–­æ€§èƒ½ä¸Šå‡ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œé™ä½äº†æ—¶é—´ç´§è¿«ç¯å¢ƒä¸‹çš„è¯¯è¯Šé£é™©ï¼Œè§£æ”¾äº†åŒ»ç”Ÿå¤„ç†å¤æ‚æ¡ˆä¾‹çš„ç²¾åŠ›ï¼Œå¹¶ä¸ºä¼˜åŒ–åŒ»ç–—èµ„æºåˆ†é…å’Œç¼“è§£åŠ³åŠ¨åŠ›çŸ­ç¼ºé—®é¢˜æä¾›äº†ç­–ç•¥ã€‚ä»£ç å’Œæ•°æ®çš„å¯ç”¨æ€§ä¿¡æ¯ä¸º<a target="_blank" rel="noopener" href="https://github.com/JarvisUSTC/DoctorAgent-RL%E3%80%82">https://github.com/JarvisUSTC/DoctorAgent-RLã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦é—®ç­”ä¸­å±•ç°å‡ºè‰²èƒ½åŠ›ï¼Œä½†åœ¨çœŸå®ä¸´åºŠå’¨è¯¢ä¸­åº”ç”¨æœ‰é™ã€‚</li>
<li>ä¼ ç»Ÿå•è½®å’¨è¯¢ç³»ç»Ÿå¯¼è‡´æ¨¡ç³Šè¯Šæ–­ï¼Œè€Œä¼ ç»Ÿå¤šè½®å¯¹è¯æ¨¡å‹å—é™äºé™æ€ç›‘ç£å­¦ä¹ ï¼Œç¼ºä¹çµæ´»æ€§ã€‚</li>
<li>æå‡ºåŸºäºå¼ºåŒ–å­¦ä¹ çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶\Ours{}ï¼Œå°†åŒ»ç–—å’¨è¯¢è§†ä¸ºä¸ç¡®å®šä¸‹çš„åŠ¨æ€å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>åŒ»ç”Ÿæ™ºèƒ½ä½“é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æé—®ç­–ç•¥ï¼Œä¸æ‚£è€…æ™ºèƒ½ä½“è¿›è¡Œå¤šè½®äº’åŠ¨ã€‚</li>
<li>\Ours{}èƒ½å¤Ÿæ ¹æ®ç»¼åˆå¥–åŠ±åŠ¨æ€è°ƒæ•´ä¿¡æ¯æœé›†è·¯å¾„ï¼Œåæ˜ ä¸´åºŠæ¨ç†é€»è¾‘ã€‚</li>
<li>å®éªŒè¯æ˜\Ours{}åœ¨å¤šè½®æ¨ç†å’Œè¯Šæ–­æ€§èƒ½ä¸Šè¶…è¶Šç°æœ‰æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19630">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-046b732eb3ba815d6eadc1107b7ab02e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-348a85480638ebb8995fd6ac95f18796.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Training-LLM-based-Tutors-to-Improve-Student-Learning-Outcomes-in-Dialogues"><a href="#Training-LLM-based-Tutors-to-Improve-Student-Learning-Outcomes-in-Dialogues" class="headerlink" title="Training LLM-based Tutors to Improve Student Learning Outcomes in   Dialogues"></a>Training LLM-based Tutors to Improve Student Learning Outcomes in   Dialogues</h2><p><strong>Authors:Alexander Scarlatos, Naiming Liu, Jaewook Lee, Richard Baraniuk, Andrew Lan</strong></p>
<p>Generative artificial intelligence (AI) has the potential to scale up personalized tutoring through large language models (LLMs). Recent AI tutors are adapted for the tutoring task by training or prompting LLMs to follow effective pedagogical principles, though they are not trained to maximize student learning throughout the course of a dialogue. Therefore, they may engage with students in a suboptimal way. We address this limitation by introducing an approach to train LLMs to generate tutor utterances that maximize the likelihood of student correctness, while still encouraging the model to follow good pedagogical practice. Specifically, we generate a set of candidate tutor utterances and score them using (1) an LLM-based student model to predict the chance of correct student responses and (2) a pedagogical rubric evaluated by GPT-4o. We then use the resulting data to train an open-source LLM, Llama 3.1 8B, using direct preference optimization. We show that tutor utterances generated by our model lead to significantly higher chances of correct student responses while maintaining the pedagogical quality of GPT-4o. We also conduct qualitative analyses and a human evaluation to demonstrate that our model generates high quality tutor utterances. </p>
<blockquote>
<p>ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å…·æœ‰é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‰©å¤§ä¸ªæ€§åŒ–è¾…å¯¼çš„æ½œåŠ›ã€‚è™½ç„¶æœ€è¿‘çš„AIè¾…å¯¼è€…é€šè¿‡è®­ç»ƒæˆ–æç¤ºLLMéµå¾ªæœ‰æ•ˆçš„æ•™å­¦åŸåˆ™è€Œé€‚åº”è¾…å¯¼ä»»åŠ¡ï¼Œä½†å®ƒä»¬å¹¶æ²¡æœ‰åœ¨å¯¹è¯è¿‡ç¨‹ä¸­æ¥å—è®­ç»ƒä»¥æœ€å¤§é™åº¦åœ°æé«˜å­¦ç”Ÿçš„å­¦ä¹ æ•ˆç‡ã€‚å› æ­¤ï¼Œå®ƒä»¬å¯èƒ½ä¸å­¦ç”Ÿçš„äº’åŠ¨æ–¹å¼ä¸ä½³ã€‚æˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸€ç§è®­ç»ƒLLMç”Ÿæˆè¾…å¯¼è¯è¯­çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥æœ€å¤§é™åº¦åœ°æé«˜å­¦ç”Ÿå›ç­”çš„æ­£ç¡®æ€§ï¼ŒåŒæ—¶é¼“åŠ±æ¨¡å‹éµå¾ªè‰¯å¥½çš„æ•™å­¦æƒ¯ä¾‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç”Ÿæˆä¸€ç»„å€™é€‰çš„è¾…å¯¼è¯è¯­ï¼Œå¹¶ä½¿ç”¨ï¼ˆ1ï¼‰åŸºäºLLMçš„å­¦ç”Ÿæ¨¡å‹é¢„æµ‹å­¦ç”Ÿæ­£ç¡®å›ç­”çš„å¯èƒ½æ€§ï¼Œï¼ˆ2ï¼‰é€šè¿‡GPT-4oè¯„ä¼°çš„æ•™å­¦è¯„åˆ†æ¥è¯„åˆ†å®ƒä»¬ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨å¾—åˆ°çš„æ•°æ®ç›´æ¥ä¼˜åŒ–å¼€æºLLMï¼ˆLlama 3.1 8Bï¼‰ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬æ¨¡å‹ç”Ÿæˆçš„è¾…å¯¼è¯è¯­èƒ½å¤Ÿæ˜¾è‘—æé«˜å­¦ç”Ÿå›ç­”çš„æ­£ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒGPT-4oçš„æ•™å­¦è´¨é‡ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å®šæ€§åˆ†æå’Œäººå·¥è¯„ä¼°ï¼Œä»¥è¯æ˜æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„è¾…å¯¼è¯è¯­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06424v2">PDF</a> Published in AIED 2025: The 26th International Conference on   Artificial Intelligence in Education</p>
<p><strong>Summary</strong><br>äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸ªæ€§åŒ–è¾…å¯¼æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„AIè¾…å¯¼å·¥å…·å¹¶æœªç»è¿‡è®­ç»ƒä»¥æœ€å¤§åŒ–å­¦ç”Ÿçš„å­¦ä¹ æ•ˆæœã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§è®­ç»ƒLLMçš„æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆèƒ½å¤Ÿæœ€å¤§åŒ–å­¦ç”Ÿæ­£ç¡®ç‡çš„è¾…å¯¼è¯è¯­ï¼ŒåŒæ—¶é¼“åŠ±æ¨¡å‹éµå¾ªè‰¯å¥½çš„æ•™å­¦åŸåˆ™ã€‚é€šè¿‡ç»“åˆå­¦ç”Ÿæ¨¡å‹å’ŒåŸºäºGPT-4oçš„æ•™å­¦è¯„åˆ†æ ‡å‡†ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†å€™é€‰è¾…å¯¼è¯è¯­å¹¶å¯¹å…¶è¿›è¡Œäº†è¯„åˆ†ã€‚æœ€ç»ˆç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ¨¡å‹ç”Ÿæˆçš„è¾…å¯¼è¯è¯­ä¸ä»…å¤§å¤§æé«˜äº†å­¦ç”Ÿçš„æ­£ç¡®ç‡ï¼Œè€Œä¸”ä¿æŒäº†GPT-4oçš„æ•™å­¦è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äººå·¥æ™ºèƒ½å…·æœ‰åœ¨ä¸ªæ€§åŒ–è¾…å¯¼é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿé€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®ç°è§„æ¨¡åŒ–åº”ç”¨ã€‚</li>
<li>å½“å‰AIè¾…å¯¼å·¥å…·å°šæœªé’ˆå¯¹æœ€å¤§åŒ–å­¦ç”Ÿå­¦ä¹ æ•ˆæœè¿›è¡Œè®­ç»ƒï¼Œå­˜åœ¨ä¸å­¦ç”Ÿäº’åŠ¨æ–¹å¼ä¸å¤Ÿä¼˜åŒ–çš„å¯èƒ½ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»“åˆå­¦ç”Ÿæ¨¡å‹å’ŒåŸºäºGPT-4oçš„æ•™å­¦è¯„åˆ†æ ‡å‡†æ¥ç”Ÿæˆå€™é€‰è¾…å¯¼è¯è¯­çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡è®­ç»ƒLLMï¼Œèƒ½å¤Ÿç”Ÿæˆæ—¢ç¬¦åˆæ•™å­¦åŸåˆ™åˆèƒ½æœ€å¤§åŒ–å­¦ç”Ÿæ­£ç¡®ç‡çš„è¾…å¯¼è¯è¯­ã€‚</li>
<li>ç›¸æ¯”ç°æœ‰AIè¾…å¯¼å·¥å…·ï¼Œæœ¬æ–‡æå‡ºçš„æ¨¡å‹åœ¨æé«˜å­¦ç”Ÿæ­£ç¡®ç‡çš„åŒæ—¶ï¼Œä¿æŒäº†è¾ƒé«˜çš„æ•™å­¦è´¨é‡ã€‚</li>
<li>è¿›è¡Œäº†å®šæ€§åˆ†æå’Œäººç±»è¯„ä¼°ï¼Œè¯æ˜äº†è¯¥æ¨¡å‹ç”Ÿæˆçš„é«˜è´¨é‡è¾…å¯¼è¯è¯­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7e192e3fd1bfc148738197b1a38d2665.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91f0bad8e2b03547202ab239f1f7c112.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-763aae0bdf92f919f1f0f0f24d17abcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44fd7c5216b0b3683df3ad25cc00d572.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Open-ended-Audio-Dialogue-Understanding-for-Large-Audio-Language-Models"><a href="#Benchmarking-Open-ended-Audio-Dialogue-Understanding-for-Large-Audio-Language-Models" class="headerlink" title="Benchmarking Open-ended Audio Dialogue Understanding for Large   Audio-Language Models"></a>Benchmarking Open-ended Audio Dialogue Understanding for Large   Audio-Language Models</h2><p><strong>Authors:Kuofeng Gao, Shu-Tao Xia, Ke Xu, Philip Torr, Jindong Gu</strong></p>
<p>Large Audio-Language Models (LALMs), such as GPT-4o, have recently unlocked audio dialogue capabilities, enabling direct spoken exchanges with humans. The potential of LALMs broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we firstly propose the evaluation of ambiguity handling in audio dialogues that expresses different intentions beyond the same literal meaning of sentences, e.g., â€œReally!?â€ with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments on 16 LALMs, our analysis reveals that existing LALMs struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones. The benchmark is available at <a target="_blank" rel="noopener" href="https://adu-bench.github.io/">https://adu-bench.github.io/</a>. </p>
<blockquote>
<p>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMsï¼‰ï¼Œå¦‚GPT-4oï¼Œæœ€è¿‘è§£é”äº†éŸ³é¢‘å¯¹è¯åŠŸèƒ½ï¼Œèƒ½å¤Ÿå®ç°ä¸äººç±»çš„ç›´æ¥å£å¤´äº¤æµã€‚LALMçš„æ½œåŠ›æ‰©å¤§äº†å…¶åœ¨éŸ³é¢‘å¯¹è¯æ”¯æŒçš„å¹¿æ³›å®é™…åœºæ™¯ä¸­çš„åº”ç”¨èŒƒå›´ã€‚ç„¶è€Œï¼Œè€ƒè™‘åˆ°è¿™äº›è¿›å±•ï¼Œç›®å‰ä»ç¼ºä¹ä¸€ä¸ªå…¨é¢è¯„ä¼°LALMåœ¨å¼€æ”¾å¼éŸ³é¢‘å¯¹è¯ç†è§£ä¸­çš„æ€§èƒ½çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†è§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†éŸ³é¢‘å¯¹è¯ç†è§£åŸºå‡†æµ‹è¯•ï¼ˆADU-Benchï¼‰ï¼Œå®ƒåŒ…æ‹¬4ä¸ªåŸºå‡†æµ‹è¯•æ•°æ®é›†ã€‚å®ƒä»¬è¯„ä¼°LALMåœ¨3ä¸ªä¸€èˆ¬åœºæ™¯ã€12é¡¹æŠ€èƒ½ã€9ç§å¤šè¯­è¨€ä»¥åŠ4ç±»æ­§ä¹‰å¤„ç†ä¸­çš„å¼€æ”¾å¼éŸ³é¢‘å¯¹è¯èƒ½åŠ›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†éŸ³é¢‘å¯¹è¯ä¸­æ­§ä¹‰å¤„ç†è¯„ä¼°ï¼Œå³è¡¨è¾¾ä¸åŒæ„å›¾çš„éŸ³é¢‘å¯¹è¯ï¼Œå³ä½¿å¥å­å…·æœ‰ç›¸åŒçš„å­—é¢æ„ä¹‰ï¼Œä¾‹å¦‚å¸¦æœ‰ä¸åŒè¯­è°ƒçš„â€œçœŸçš„å—ï¼ï¼Ÿâ€ç­‰ã€‚æ€»ä¹‹ï¼ŒADU-BenchåŒ…æ‹¬è¶…è¿‡2ä¸‡æ¡å¼€æ”¾å¼éŸ³é¢‘å¯¹è¯ï¼Œç”¨äºè¯„ä¼°LALMã€‚é€šè¿‡å¯¹16ä¸ªLALMçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬çš„åˆ†æå‘ç°ï¼Œç°æœ‰çš„LALMåœ¨å¤„ç†æ•°å­¦ç¬¦å·å’Œå…¬å¼ã€ç†è§£äººç±»è¡Œä¸ºï¼ˆå¦‚è§’è‰²æ‰®æ¼”ï¼‰ã€ç†è§£å¤šç§è¯­è¨€ä»¥åŠå¤„ç†æ¥è‡ªä¸åŒè¯­éŸ³å…ƒç´ çš„éŸ³é¢‘å¯¹è¯æ­§ä¹‰ï¼ˆå¦‚è¯­è°ƒã€åœé¡¿ä½ç½®å’ŒåŒéŸ³å­—ï¼‰æ–¹é¢å­˜åœ¨å›°éš¾ã€‚åŸºå‡†æµ‹è¯•ç½‘ç«™ä¸ºï¼š<a target="_blank" rel="noopener" href="https://adu-bench.github.io./">https://adu-bench.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05167v2">PDF</a> Accepted by ACL 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰å¦‚GPT-4oå·²è§£é”éŸ³é¢‘å¯¹è¯åŠŸèƒ½ï¼Œå®ç°ä¸äººç±»ç›´æ¥å£è¯­äº¤æµã€‚ç„¶è€Œï¼Œç›®å‰å°šæœªæœ‰å…¨é¢è¯„ä¼°LALMåœ¨å¼€æ”¾å¼éŸ³é¢‘å¯¹è¯ç†è§£æ€§èƒ½çš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºè§£å†³æ­¤ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†éŸ³é¢‘å¯¹è¯ç†è§£åŸºå‡†æµ‹è¯•ï¼ˆADU-Benchï¼‰ï¼ŒåŒ…å«4ä¸ªåŸºå‡†æµ‹è¯•é›†ï¼Œè¯„ä¼°LALMåœ¨3ä¸ªä¸€èˆ¬åœºæ™¯ã€12é¡¹æŠ€èƒ½ã€9ç§å¤šå…ƒè¯­è¨€åŠ4ç±»æ­§ä¹‰å¤„ç†ä¸­çš„å¼€æ”¾å¼éŸ³é¢‘å¯¹è¯èƒ½åŠ›ã€‚å°¤å…¶æ˜¯ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºéŸ³é¢‘å¯¹è¯ä¸­æ­§ä¹‰å¤„ç†çš„è¯„ä¼°ï¼Œå¦‚å¥å­ç›¸åŒå­—é¢æ„ä¹‰ä¸‹è¡¨è¾¾ä¸åŒæ„å›¾çš„â€œReally!?â€ã€‚æ€»ç»“æ¥è¯´ï¼ŒADU-BenchåŒ…å«è¶…è¿‡20,000ä¸ªå¼€æ”¾å¼éŸ³é¢‘å¯¹è¯ï¼Œç”¨ä»¥è¯„ä¼°LALMã€‚é€šè¿‡å¯¹16ä¸ªLALMçš„å¹¿æ³›å®éªŒï¼Œæˆ‘ä»¬å‘ç°ç°æœ‰æ¨¡å‹åœ¨æ•°å­¦ç¬¦å·å’Œå…¬å¼ã€äººç±»è¡Œä¸ºç†è§£ï¼ˆå¦‚è§’è‰²æ‰®æ¼”ï¼‰ã€å¤šè¯­è¨€ç†è§£å’ŒéŸ³é¢‘å¯¹è¯æ­§ä¹‰å¤„ç†ç­‰æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰å·²å…·å¤‡éŸ³é¢‘å¯¹è¯èƒ½åŠ›ï¼Œå®ç°ä¸äººç±»ç›´æ¥å£è¯­äº¤æµã€‚</li>
<li>ç›®å‰ç¼ºä¹è¯„ä¼°LALMåœ¨å¼€æ”¾å¼éŸ³é¢‘å¯¹è¯ç†è§£æ€§èƒ½çš„å…¨é¢åŸºå‡†æµ‹è¯•ã€‚</li>
<li>æå‡ºçš„ADU-BenchåŒ…å«4ä¸ªåŸºå‡†æµ‹è¯•é›†ï¼Œæ¶µç›–3ä¸ªåœºæ™¯ã€12é¡¹æŠ€èƒ½ã€9ç§è¯­è¨€å’Œ4ç±»æ­§ä¹‰å¤„ç†ã€‚</li>
<li>ADU-Benché¦–æ¬¡æå‡ºéŸ³é¢‘å¯¹è¯ä¸­æ­§ä¹‰å¤„ç†çš„è¯„ä¼°ã€‚</li>
<li>LALMé¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚æ•°å­¦ç¬¦å·å’Œå…¬å¼ç†è§£ã€äººç±»è¡Œä¸ºç†è§£ï¼ˆå¦‚è§’è‰²æ‰®æ¼”ï¼‰ã€å¤šè¯­è¨€ç†è§£å’Œå¤„ç†éŸ³é¢‘å¯¹è¯ä¸­çš„æ­§ä¹‰ã€‚</li>
<li>ADU-Benchä¸ºè¯„ä¼°LALMæä¾›äº†ä¸€ä¸ªé‡è¦çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05167">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c625e6bef6ea60a402922e35b1c8d995.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-442a77a2247f62d7d40401b619b141ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-070dffcb66d0d5a9be25450089640d7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-588fabb41b5ed47c02e1bcd973ea1dea.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Interactive/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Interactive/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Interactive/">
                                    <span class="chip bg-color">Interactive</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-dd6521f4c242431ec2efa8a31e43c27d.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  Multi-Modal Motion Retrieval by Learning a Fine-Grained Joint Embedding   Space
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1049d4ae6438daf03a9d1aee1979c206.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  TTS-1 Technical Report
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
