<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Interactive">
    <meta name="description" content="Interactive 方向最新论文已更新，请持续关注 Update in 2025-08-02  Text-to-SQL Task-oriented Dialogue Ontology Construction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Interactive | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-588fabb41b5ed47c02e1bcd973ea1dea.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Interactive</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Interactive/">
                                <span class="chip bg-color">Interactive</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                Interactive
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    39 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-02-更新"><a href="#2025-08-02-更新" class="headerlink" title="2025-08-02 更新"></a>2025-08-02 更新</h1><h2 id="Text-to-SQL-Task-oriented-Dialogue-Ontology-Construction"><a href="#Text-to-SQL-Task-oriented-Dialogue-Ontology-Construction" class="headerlink" title="Text-to-SQL Task-oriented Dialogue Ontology Construction"></a>Text-to-SQL Task-oriented Dialogue Ontology Construction</h2><p><strong>Authors:Renato Vukovic, Carel van Niekerk, Michael Heck, Benjamin Ruppik, Hsien-Chin Lin, Shutong Feng, Nurul Lubis, Milica Gasic</strong></p>
<p>Large language models (LLMs) are widely used as general-purpose knowledge sources, but they rely on parametric knowledge, limiting explainability and trustworthiness. In task-oriented dialogue (TOD) systems, this separation is explicit, using an external database structured by an explicit ontology to ensure explainability and controllability. However, building such ontologies requires manual labels or supervised training. We introduce TeQoDO: a Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM autonomously builds a TOD ontology from scratch without supervision using its inherent SQL programming capabilities combined with dialogue theory provided in the prompt. We show that TeQoDO outperforms transfer learning approaches, and its constructed ontology is competitive on a downstream dialogue state tracking task. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also scales to allow construction of much larger ontologies, which we investigate on a Wikipedia and ArXiv dataset. We view this as a step towards broader application of ontologies to increase LLM explainability. </p>
<blockquote>
<p>大型语言模型（LLM）被广泛应用于通用知识源，但它们依赖于参数知识，这限制了其可解释性和可信度。在面向任务的对话（TOD）系统中，这种分离是明确的，使用明确的本体构建的外部数据库，以确保可解释性和可控性。然而，构建这样的本体需要手动标签或监督训练。我们引入了TeQoDO：一种从文本到SQL的面向任务的对话本体构建方法。在这里，LLM利用其内在的SQL编程能力和提示中提供的对话理论，无需监督即可从零开始自主构建TOD本体。我们展示了TeQoDO优于迁移学习方法，其构建的本体在下游对话状态跟踪任务上具有很强的竞争力。消除研究表明对话理论起着关键作用。TeQoDO还可以扩展，以构建更大的本体，我们在Wikipedia和ArXiv数据集上对此进行了调查。我们认为这是向更广泛的应用本体以提高LLM可解释性迈出的一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23358v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）作为通用知识源应用广泛，但依赖参数知识，限制了其可解释性和可信度。在面向任务的对话（TOD）系统中，使用外部数据库和显性本体结构来确保可解释性和可控性。本文介绍了一种新型的面向任务的对话本体构建方法TeQoDO，该方法利用LLM自主构建TOD本体，无需监督，结合内置的SQL编程能力和对话理论提示。研究表明，TeQoDO在下游对话状态跟踪任务上的表现优于迁移学习方法，且其构建的本体具有竞争力。此外，TeQoDO还可以扩展到更大的本体构建，我们在Wikipedia和ArXiv数据集上进行了调查。这被视为提高LLM可解释性的重要一步。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）作为通用知识源具有广泛应用，但缺乏可解释性和可信度。</li>
<li>面向任务的对话（TOD）系统通过使用外部数据库和显性本体结构来提高可解释性和可控性。</li>
<li>TeQoDO是一种新型的面向任务的对话本体构建方法，可以自主构建TOD本体，无需人工监督。</li>
<li>TeQoDO结合了LLM的SQL编程能力和对话理论提示。</li>
<li>TeQoDO在下游对话状态跟踪任务上的表现优于迁移学习方法。</li>
<li>TeQoDO构建的本体具有竞争力，并且可以扩展到更大的本体构建。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23358">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-786ee01f9c66fe111566ac62c617c98d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ea557937f94e888dbebc25dcabc55e73.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-222b50ce34f5f92cbc15f045e1fe5b0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8cdb55f900b38f37b793665e8f71e8c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ddc3a9990acbfe212feff11cfe1d6cf.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Real-time-Generation-of-Various-Types-of-Nodding-for-Avatar-Attentive-Listening-System"><a href="#Real-time-Generation-of-Various-Types-of-Nodding-for-Avatar-Attentive-Listening-System" class="headerlink" title="Real-time Generation of Various Types of Nodding for Avatar Attentive   Listening System"></a>Real-time Generation of Various Types of Nodding for Avatar Attentive   Listening System</h2><p><strong>Authors:Kazushi Kato, Koji Inoue, Divesh Lala, Keiko Ochi, Tatsuya Kawahara</strong></p>
<p>In human dialogue, nonverbal information such as nodding and facial expressions is as crucial as verbal information, and spoken dialogue systems are also expected to express such nonverbal behaviors. We focus on nodding, which is critical in an attentive listening system, and propose a model that predicts both its timing and type in real time. The proposed model builds on the voice activity projection (VAP) model, which predicts voice activity from both listener and speaker audio. We extend it to prediction of various types of nodding in a continuous and real-time manner unlike conventional models. In addition, the proposed model incorporates multi-task learning with verbal backchannel prediction and pretraining on general dialogue data. In the timing and type prediction task, the effectiveness of multi-task learning was significantly demonstrated. We confirmed that reducing the processing rate enables real-time operation without a substantial drop in accuracy, and integrated the model into an avatar attentive listening system. Subjective evaluations showed that it outperformed the conventional method, which always does nodding in sync with verbal backchannel. The code and trained models are available at <a target="_blank" rel="noopener" href="https://github.com/MaAI-Kyoto/MaAI">https://github.com/MaAI-Kyoto/MaAI</a>. </p>
<blockquote>
<p>在人机对话中，非言语信息（如点头和面部表情）与言语信息同样重要，人们期望语音对话系统也能表达这些非言语行为。我们专注于点头，它在倾听系统中至关重要，并提出一种模型，可以实时预测其时机和类型。所提模型基于语音活动投影（VAP）模型，该模型可以从听者和说话者的音频中预测语音活动。我们将其扩展到连续实时预测各种点头类型，不同于传统模型。此外，所提模型结合了多任务学习与言语反馈通道预测和一般对话数据的预训练。在时机和类型预测任务中，多任务学习的有效性得到了显著证明。我们验证了降低处理速率可以在不显著降低准确度的前提下实现实时操作，并将该模型集成到虚拟角色倾听系统中。主观评估表明，它优于传统方法，后者总是与言语反馈通道同步点头。相关代码和训练好的模型可在<a target="_blank" rel="noopener" href="https://github.com/MaAI-Kyoto/MaAI%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MaAI-Kyoto/MaAI上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23298v1">PDF</a> Accepted by 27th ACM International Conference on Multimodal   Interaction (ICMI ‘25), Long paper</p>
<p><strong>Summary</strong></p>
<p>本文研究了一种基于语音活动预测模型（VAP）的实时点头预测模型。该模型不仅能够预测点头的时机，还能预测点头的类型，这对于人机交互中的注意力系统至关重要。通过多任务学习和预训练技术，该模型在预测效果上显著提升。研究还表明，降低处理速率可实现实时操作而不损失准确性。集成到虚拟角色注意力系统后，主观评估显示其优于传统方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>非言语信息在人机对话中至关重要，如点头和面部表情。</li>
<li>实时点头预测模型基于语音活动预测模型（VAP）构建。</li>
<li>模型能够预测点头的时机和类型。</li>
<li>通过多任务学习和预训练技术，模型预测效果显著提升。</li>
<li>降低处理速率可实现实时操作，且不影响准确性。</li>
<li>集成到虚拟角色注意力系统后，主观评估显示该模型优于传统方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23298">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-458752ced0bc4d07c912c3a9e5e9dfa6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-342afd05c6526d5d2eb6ff7d8500949a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07a33c83dd5f46758771e1bebf3e417d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-823ccf16ce1a587fe983b141c93f6b52.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6e584b2bb129f0073f4510f1d54b42df.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba079bca0e9185f43e89b465bc1af867.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f162a317e8191aa34ce82ec687608c01.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b44c4576c0a10df7e3ebe7da894ceebd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="User-Feedback-in-Human-LLM-Dialogues-A-Lens-to-Understand-Users-But-Noisy-as-a-Learning-Signal"><a href="#User-Feedback-in-Human-LLM-Dialogues-A-Lens-to-Understand-Users-But-Noisy-as-a-Learning-Signal" class="headerlink" title="User Feedback in Human-LLM Dialogues: A Lens to Understand Users But   Noisy as a Learning Signal"></a>User Feedback in Human-LLM Dialogues: A Lens to Understand Users But   Noisy as a Learning Signal</h2><p><strong>Authors:Yuhan Liu, Michael J. Q. Zhang, Eunsol Choi</strong></p>
<p>Once language models (LMs) are deployed, they can interact with users long-term, ideally evolving continuously based on their feedback. Asking for direct user feedback can be disruptive; thus, we study harvesting user feedback from user-LM interaction logs. We study implicit user feedback in two user-LM interaction datasets (WildChat and LMSYS). First, we analyze user feedback in the user-LLM conversation trajectory, providing insights into when and why such feedback occurs. Second, we study harvesting learning signals from such implicit user feedback. We find that the contents of user feedback (e.g., user wanted clarification), not just the polarity (e.g., users were unhappy with the previous model response), can improve model performance in short human-designed questions (MTBench) but not on longer and more complex questions (WildBench). We also find that the usefulness of user feedback is largely tied to the quality of the user’s initial prompt. Together, we provide an in-depth study of implicit user feedback, showing its potential and limitations. </p>
<blockquote>
<p>当语言模型（LMs）被部署后，它们可以长期与用户进行交互，并在理想情况下根据用户的反馈持续进化。直接请求用户反馈可能会产生干扰，因此我们研究了从用户与语言模型的交互日志中收集用户反馈的方法。我们研究了两个用户与语言模型交互数据集（WildChat和LMSYS）中的隐性用户反馈。首先，我们分析了用户在与语言模型的对话轨迹中的反馈，了解这种反馈何时以及为何发生。其次，我们研究了从这些隐性用户反馈中收集学习信号的方法。我们发现，用户反馈的内容（例如，用户需要澄清），而不仅仅是极性（例如，用户对之前的模型响应不满意），可以提高模型在简短的人类设计问题（MTBench）上的性能，但在更长的、更复杂的问题上（WildBench）则不然。我们还发现，用户反馈的有用性在很大程度上取决于用户初始提示的质量。总体而言，我们对隐性用户反馈进行了深入研究，展示了其潜力和局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23158v1">PDF</a> Earlier version of this paper was presented at 2nd Workshop on Models   of Human Feedback for AI Alignment (MoFA), ICML 2025</p>
<p><strong>Summary</strong></p>
<p>用户反馈的语言模型在长期交互中能够持续进化。研究从用户与语言模型的互动日志中收集用户反馈，通过对WildChat和LMSYS两个数据集中的用户反馈进行深度分析，发现用户反馈内容而非仅仅反馈的情感极性可以提升模型在简短问题上的表现，但对于更复杂的问题则效果有限。此外，用户反馈的效用与初始提示的质量紧密相关。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>语言模型在长期与用户交互过程中能够持续进化。</li>
<li>用户反馈可以通过分析用户与语言模型的互动日志来收集。</li>
<li>用户反馈内容对模型性能的提升有重要作用，尤其是在简短问题上。</li>
<li>对于复杂问题，用户反馈的效果有限。</li>
<li>用户反馈的效用受初始提示质量的影响。</li>
<li>用户的反馈中包含对模型的澄清需求等详细信息。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23158">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-1982848de4e951815f201ddedec85e0e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-02fb48d1544f70130ce975df2130a379.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e455fdec67ed3c24b384673238f5a0af.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-299b3b67f6d735a9e76ebe0a70557fbf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e6cae4401a5966a9791cef7dd62d0a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-983fdc6e72ebab5f088f59df5f1ad9dd.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Listening-to-the-Unspoken-Exploring-365-Aspects-of-Multimodal-Interview-Performance-Assessment"><a href="#Listening-to-the-Unspoken-Exploring-365-Aspects-of-Multimodal-Interview-Performance-Assessment" class="headerlink" title="Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview   Performance Assessment"></a>Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview   Performance Assessment</h2><p><strong>Authors:Jia Li, Yang Wang, Wenhao Qian, Zhenzhen Hu, Richang Hong, Meng Wang</strong></p>
<p>Interview performance assessment is essential for determining candidates’ suitability for professional positions. To ensure holistic and fair evaluations, we propose a novel and comprehensive framework that explores &#96;&#96;365’’ aspects of interview performance by integrating \textit{three} modalities (video, audio, and text), \textit{six} responses per candidate, and \textit{five} key evaluation dimensions. The framework employs modality-specific feature extractors to encode heterogeneous data streams and subsequently fused via a Shared Compression Multilayer Perceptron. This module compresses multimodal embeddings into a unified latent space, facilitating efficient feature interaction. To enhance prediction robustness, we incorporate a two-level ensemble learning strategy: (1) independent regression heads predict scores for each response, and (2) predictions are aggregated across responses using a mean-pooling mechanism to produce final scores for the five target dimensions. By listening to the unspoken, our approach captures both explicit and implicit cues from multimodal data, enabling comprehensive and unbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our framework secured first place in the AVI Challenge 2025, demonstrating its effectiveness and robustness in advancing automated and multimodal interview performance assessment. The full implementation is available at <a target="_blank" rel="noopener" href="https://github.com/MSA-LMC/365Aspects">https://github.com/MSA-LMC/365Aspects</a>. </p>
<blockquote>
<p>面试绩效评估对于确定候选人是否适合专业职位至关重要。为了确保全面和公平的评价，我们提出了一种新型且全面的框架，通过整合视频、音频和文本三种模式，针对每个候选人的六种回应，以及五个关键评价维度，来探索面试表现的“365”方面。该框架使用特定于模态的特征提取器来编码异质数据流，然后通过共享压缩多层感知器进行融合。此模块将多模态嵌入压缩到统一的潜在空间，促进了特征的有效交互。为了提高预测的稳定性，我们采用了两级集成学习策略：（1）独立的回归头用于预测每个回应的分数，（2）使用平均池化机制对回应的预测进行汇总，以产生五个目标维度的最终分数。通过倾听未言明的内容，我们的方法能够从多模式数据中捕获明确和隐含的线索，从而实现全面和无偏的评价。我们的框架在2025年AVI挑战赛中获得了第一名，实现了平均均方误差0.1824，证明了其在推动自动化和多模式面试绩效评估中的有效性和稳健性。完整实现可在<a target="_blank" rel="noopener" href="https://github.com/MSA-LMC/365Aspects%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/MSA-LMC/365Aspects上找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22676v1">PDF</a> 8 pages, 4 figures, ACM MM 2025.   github:<a target="_blank" rel="noopener" href="https://github.com/MSA-LMC/365Aspects">https://github.com/MSA-LMC/365Aspects</a></p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的面试表现评估框架，该框架通过整合视频、音频和文本三种模态，从六个方面对候选人的面试表现进行全方位评价。该框架运用模态特定特征提取器编码异质数据流，并通过共享压缩多层感知器进行融合。同时采用两级集成学习策略，提高预测稳健性。该框架在AVI Challenge 2025中获得第一名，展示了其在推进自动化和多模态面试表现评估中的有效性和稳健性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出的面试评估框架融合了视频、音频和文本三种模态，实现全面评价。</li>
<li>框架包含六个方面的候选人响应评价。</li>
<li>通过共享压缩多层感知器，不同模态的数据被编码并融合到统一潜在空间。</li>
<li>采用两级集成学习策略，提高预测准确性及稳健性。</li>
<li>框架能捕捉面试中的明确和隐含线索，进行客观评估。</li>
<li>框架在AVI Challenge 2025中获第一名，证明其有效性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22676">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-23b37bf3f2b50a1a3fe067f613bb1927.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8cacd4aa62419882747cc0598993db9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-00e1b63ca6ff1a48be37251b4c32bfde.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c98fda1e2ddf44e2b147360dcdc5c9e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d8e8a835656456aaaeed2fe594cf64c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f7e5fd2a33ed6e2c5ff6700c84ba098.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Measurement-of-production-branching-ratio-after-muon-nuclear-capture-reaction-of-Al-and-Si-isotopes"><a href="#Measurement-of-production-branching-ratio-after-muon-nuclear-capture-reaction-of-Al-and-Si-isotopes" class="headerlink" title="Measurement of production branching ratio after muon nuclear capture   reaction of Al and Si isotopes"></a>Measurement of production branching ratio after muon nuclear capture   reaction of Al and Si isotopes</h2><p><strong>Authors:R. Mizuno, M. Niikura, T. Y. Saito, T. Matsuzaki, S. Abe, H. Fukuda, M. Hashimoto, A. Hillier, K. Ishida, N. Kawamura, S. Kawase, T. Kawata, K. Kitafuji, F. Minato, M. Oishi, A. Sato, K. Shimomura, P. Strasser, S. Takeshita, D. Tomono, Y. Watanabe</strong></p>
<p>Background: Muon nuclear capture is a reaction between a muon and a proton inside a nucleus through weak interactions. This reaction results in the formation of an excited nucleus, which subsequently de-excites by emitting several particles. Examination of the excited state allows for an investigation of the properties of nuclear excitation and particle emission in highly excited nuclei. Purpose: This study investigates muon nuclear capture of 27Al and 28,29,30Si, focusing on determining the absolute production branching ratio (BR) following muon nuclear capture and subsequent particle emissions. By measuring the absolute production BR, we can collect valuable information on the excitation energy distribution of muon nuclear capture. Methods: Measurements were conducted using the in-beam activation method at two pulsed muon facilities: RIKEN-RAL at RAL and MLF at JPARC. Absolute BRs were determined by measuring the irradiated muon number using a plastic scintillator and the \b{eta}-delayed {\gamma}-rays emitted from the produced nuclei using germanium detectors. Results: The absolute production branching ratios of muon nuclear capture on 27Al and 28,29,30Si were obtained with the highest accuracy to date. Predominant neutron emissions, even-odd atomic number dependence on particle emission probabilities, and influence of the neutron excess were observed. These results were compared with previous measurements and theoretical models and discussed regarding the excitation energy distribution, particle emission mechanism, and nuclear properties, such as resonance in the isovector transition. Conclusion: This study emphasizes the importance of considering nuclear structure effects, even-odd effects of proton and neutron numbers, neutron excess, nucleon pairing effect, and particle emission mechanisms, in the context of the muon nuclear capture reaction. </p>
<blockquote>
<p>背景：μ子核俘获是μ子和原子核内质子通过弱相互作用发生的反应。该反应导致激发态核的形成，随后通过发射多个粒子而达到稳定。对激发态的研究可以探讨高度激发核的核激发和粒子发射的性质。</p>
</blockquote>
<p>目的：本研究调查了μ子在核上的俘获情况，重点探讨了μ子核俘获后发生粒子发射的绝对产生分支比（BR）。通过测量绝对生产BR，我们可以获得有关μ子核俘获的激发能分布的有价值的信息。</p>
<p>方法：实验在两个脉冲μ子设施（RAL的RIKEN-RAL和JPARC的MLF）上进行了原位活化法测量。通过使用塑料闪烁体测量照射的μ子数量，并使用锗探测器测量产生的核发出的延迟γ射线，确定了绝对BR。</p>
<p>结果：获得了迄今为止对μ子在核上（如铝和其他硅同位素）发生俘获时的最高精度的绝对生产分支比结果。观察到主要的中子发射现象、粒子发射概率与原子序数的奇偶依赖性以及中子过剩的影响。这些结果与之前的测量和理论模型进行了比较和讨论，涉及激发能分布、粒子发射机制和诸如同位旋过渡共振的核性质等方面。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19753v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文研究了μ子在核内的捕获反应，重点探讨了μ子在核内的捕获后产生的激发态核的性质以及粒子发射过程。通过对μ子在核内的捕获反应的研究，确定了在不同原子核上发生的绝对生产分支比（BR）。利用脉冲μ子设施的测量，测量了辐射的μ子数和产生的核产生的η延迟γ射线来确定绝对BR值。研究结果显示了中子发射的主导作用，粒子发射概率的奇偶原子数依赖性以及中子过剩的影响。这些结果对于理解核激发态的能量分布、粒子发射机制和核结构效应具有重要意义。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>μ子在核内的捕获反应是一种重要的研究手段，用于研究高度激发态核的性质和粒子发射过程。</li>
<li>通过测量绝对生产分支比（BR），可以了解μ子核捕获的激发能量分布。</li>
<li>研究发现中子发射占主导地位，粒子发射概率与原子数的奇偶性有关，并且中子过剩也会影响反应结果。</li>
<li>这些发现有助于理解核激发态的能量分布、粒子发射机制和核结构效应等核心问题。</li>
<li>对比以往的研究和理论模型，该研究为未来更深入的研究奠定了基础。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19753">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-703cb4254eecd10e29c6e5eaefcec90b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce2a690974183d4b9ce3cb453f190e52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-282041be4c58a117817eaa274d61894d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae4be94d96d35e4c77b5488c0a226b3e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="DIFFA-Large-Language-Diffusion-Models-Can-Listen-and-Understand"><a href="#DIFFA-Large-Language-Diffusion-Models-Can-Listen-and-Understand" class="headerlink" title="DIFFA: Large Language Diffusion Models Can Listen and Understand"></a>DIFFA: Large Language Diffusion Models Can Listen and Understand</h2><p><strong>Authors:Jiaming Zhou, Hongjie Chen, Shiwan Zhao, Jian Kang, Jie Li, Enzhi Wang, Yujie Guo, Haoqin Sun, Hui Wang, Aobo Kong, Yong Qin, Xuelong Li</strong></p>
<p>Recent advances in Large language models (LLMs) have shown remarkable capabilities across textual and multimodal domains. In parallel, diffusion-based language models have emerged as a promising alternative to the autoregressive paradigm, offering improved controllability, bidirectional context modeling, and robust generation. However, their application to the audio modality remains underexplored. In this work, we introduce \textbf{DIFFA}, the first diffusion-based Large Audio-Language Model designed to perform spoken language understanding. DIFFA integrates a frozen diffusion language model with a lightweight dual-adapter architecture that bridges speech understanding and natural language reasoning. We employ a two-stage training pipeline: first, aligning semantic representations via an ASR objective; then, learning instruction-following abilities through synthetic audio-caption pairs automatically generated by prompting LLMs. Despite being trained on only 960 hours of ASR and 127 hours of synthetic instruction data, DIFFA demonstrates competitive performance on major benchmarks, including MMSU, MMAU, and VoiceBench, outperforming several autoregressive open-source baselines. Our results reveal the potential of diffusion-based language models for efficient and scalable audio understanding, opening a new direction for speech-driven AI. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/DIFFA.git">https://github.com/NKU-HLT/DIFFA.git</a>. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进步在文本和多模态领域表现出了显著的能力。同时，基于扩散的语言模型作为自回归范式的有前途的替代品而出现，提供了更好的可控性、双向上下文建模和稳健的生成。然而，它们在音频模态的应用仍然被较少探索。在这项工作中，我们介绍了第一个基于扩散的大型音频语言模型——DIFFA，旨在进行口语理解。DIFFA整合了一个冻结的扩散语言模型和一个轻量级的双适配器架构，该架构能够桥接语音理解和自然语言推理。我们采用了两阶段训练管道：首先，通过ASR目标对齐语义表示；然后，通过自动生成的合成音频字幕对来学习遵循指令的能力。尽管仅在960小时的ASR和127小时的合成指令数据上进行训练，DIFFA在主要基准测试上表现出竞争力，包括MMSU、MMAU和VoiceBench，优于多个自回归开源基准。我们的结果揭示了扩散语言模型在高效和可扩展音频理解方面的潜力，为语音驱动的AI开辟了新的方向。我们的代码将在<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/DIFFA.git%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/NKU-HLT/DIFFA.git上提供。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18452v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>扩散模型在文本和多模态领域展现出卓越的能力。本研究引入首个基于扩散的大型音频语言模型DIFFA，用于进行口语理解。DIFFA结合冻结的扩散语言模型与轻量级双适配器架构，实现语音理解和自然语言推理的桥梁。采用两阶段训练管道：首先通过ASR目标对齐语义表示，然后通过自动生成的合成语音指令对进行学习指令遵循能力。尽管仅在960小时的ASR和127小时的合成指令数据上进行训练，DIFFA在主要基准测试上表现出竞争力，包括MMSU、MMAU和VoiceBench，优于多个自动回归开源基准。结果表明，扩散模型在音频理解方面具有高效且可扩展的潜力，为语音驱动的人工智能开启新的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DIFFA是首个基于扩散的大型音频语言模型，用于口语理解。</li>
<li>DIFFA结合冻结的扩散语言模型和轻量级双适配器架构。</li>
<li>DIFFA采用两阶段训练，首先通过ASR目标对齐语义表示，然后学习指令遵循能力。</li>
<li>DIFFA在合成语音指令对上进行训练，使用自动生成的合成音频字幕。</li>
<li>DIFFA在主要基准测试上表现优异，包括MMSU、MMAU和VoiceBench。</li>
<li>DIFFA优于多个自动回归开源基准，展现出扩散模型在音频理解方面的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18452">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8afb024cec6a28de96294b2f2b1709d9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7cb01fae4bd66d4dcad433b5d6928a00.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e71bc595775eea249be477ee65c0d847.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-771e40bc61a1a3f2058282134a2e117a.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Watch-Listen-Understand-Mislead-Tri-modal-Adversarial-Attacks-on-Short-Videos-for-Content-Appropriateness-Evaluation"><a href="#Watch-Listen-Understand-Mislead-Tri-modal-Adversarial-Attacks-on-Short-Videos-for-Content-Appropriateness-Evaluation" class="headerlink" title="Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on   Short Videos for Content Appropriateness Evaluation"></a>Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on   Short Videos for Content Appropriateness Evaluation</h2><p><strong>Authors:Sahid Hossain Mustakim, S M Jishanul Islam, Ummay Maria Muna, Montasir Chowdhury, Mohammed Jawwadul Islam, Sadia Ahmmed, Tashfia Sikder, Syed Tasdid Azam Dhrubo, Swakkhar Shatabda</strong></p>
<p>Multimodal Large Language Models (MLLMs) are increasingly used for content moderation, yet their robustness in short-form video contexts remains underexplored. Current safety evaluations often rely on unimodal attacks, failing to address combined attack vulnerabilities. In this paper, we introduce a comprehensive framework for evaluating the tri-modal safety of MLLMs. First, we present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising diverse short-form videos with human-guided synthetic adversarial attacks. Second, we propose ChimeraBreak, a novel tri-modal attack strategy that simultaneously challenges visual, auditory, and semantic reasoning pathways. Extensive experiments on state-of-the-art MLLMs reveal significant vulnerabilities with high Attack Success Rates (ASR). Our findings uncover distinct failure modes, showing model biases toward misclassifying benign or policy-violating content. We assess results using LLM-as-a-judge, demonstrating attack reasoning efficacy. Our dataset and findings provide crucial insights for developing more robust and safe MLLMs. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）越来越多地用于内容审核，但它们在短视频上下文中的稳健性尚未得到充分探索。当前的安全评估通常依赖于单模态攻击，无法解决组合攻击漏洞。在本文中，我们介绍了一个用于评估MLLMs的三模态安全性的综合框架。首先，我们提出了短视频多模态对抗（SVMA）数据集，其中包含带有由人类引导的合成对抗性攻击的多种短视频。其次，我们提出了一种新的三模态攻击策略chimeraBreak，它同时挑战视觉、听觉和语义推理路径。在最新MLLMs上的广泛实验显示存在显著漏洞，攻击成功率（ASR）很高。我们的研究结果揭示了不同的失败模式，显示了模型偏向于错误分类良性或违反策略的内容。我们使用LLM作为法官进行评估，证明了攻击推理的有效性。我们的数据集和研究结果对于开发更稳健和安全的多模态大型语言模型提供了关键见解。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11968v1">PDF</a> Accepted as long paper, SVU Workshop at ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了多模态大型语言模型（MLLMs）在短视频内容审核中的应用及其面临的挑战。文章提出了一种评估MLLMs三模态安全性的综合框架，包括Short-Video Multimodal Adversarial（SVMA）数据集和ChimeraBreak新型三模态攻击策略。实验表明，MLLMs存在显著漏洞，需加强模型对良性或违规内容的识别能力。文章提供的数据集和发现对于开发更稳健和安全的多模态大型语言模型至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMs在短视频内容审核中广泛应用，但其稳健性有待探索。</li>
<li>当前安全评估主要依赖单模态攻击，无法应对组合攻击漏洞。</li>
<li>引入SVMA数据集，包含带有合成对抗攻击的短视频。</li>
<li>提出ChimeraBreak三模态攻击策略，同时挑战视觉、听觉和语义推理路径。</li>
<li>实验表明，最先进的多模态大型语言模型存在显著漏洞和高攻击成功率（ASR）。</li>
<li>模型对良性或违规内容的识别存在偏见。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11968">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-34d4c0f5eb4b8eb8e9561989c634116a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-881ae9b9d4272710de7fcd61933ace9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-053153bcb4f20a7517c1a705c94ac0a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7eb0e8cd25d1e8641ce0d554ab57d8ad.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DoctorAgent-RL-A-Multi-Agent-Collaborative-Reinforcement-Learning-System-for-Multi-Turn-Clinical-Dialogue"><a href="#DoctorAgent-RL-A-Multi-Agent-Collaborative-Reinforcement-Learning-System-for-Multi-Turn-Clinical-Dialogue" class="headerlink" title="DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning   System for Multi-Turn Clinical Dialogue"></a>DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning   System for Multi-Turn Clinical Dialogue</h2><p><strong>Authors:Yichun Feng, Jiawei Wang, Lu Zhou, Zhen Lei, Yixue Li</strong></p>
<p>Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Single-round consultation systems require patients to describe all symptoms upfront, leading to vague diagnosis with unclear complaints. Traditional multi-turn dialogue models, constrained by static supervised learning, lack flexibility and fail to intelligently extract key clinical information. To address these limitations, we propose \Ours{}, a reinforcement learning (RL)-based multi-agent collaborative framework that models medical consultations as a dynamic decision-making process under uncertainty. The doctor agent continuously optimizes its questioning strategy within the RL framework through multi-turn interactions with the patient agent, dynamically adjusting its information-gathering path based on comprehensive rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop interaction strategies aligned with clinical reasoning logic, rather than superficially imitating patterns in existing dialogue data. Notably, we constructed MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating patient interactions. Experiments demonstrate that \Ours{} outperforms existing models in both multi-turn reasoning capability and final diagnostic performance. This approach shows immense practical value by reducing misdiagnosis risks in time-pressured settings, freeing clinicians for complex cases, and pioneering a strategy to optimize medical resource allocation and alleviate workforce shortages. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/JarvisUSTC/DoctorAgent-RL">https://github.com/JarvisUSTC/DoctorAgent-RL</a> </p>
<blockquote>
<p>大型语言模型（LLM）在生物医学问答领域表现出了卓越的能力，但它们在现实世界的临床咨询中的应用仍面临核心挑战。单轮咨询系统要求患者一次性描述所有症状，导致诊断模糊，投诉不明确。受静态监督学习限制的传统多轮对话模型，缺乏灵活性，无法智能提取关键临床信息。为了解决这些限制，我们提出了\Ours{}，这是一个基于强化学习（RL）的多智能体协作框架，它将医疗咨询建模为不确定环境下的动态决策过程。医生智能体通过在RL框架内与病人智能体进行多轮互动，不断优化其提问策略，并根据咨询评估器的综合奖励动态调整其信息收集路径。这种RL微调机制使LLM能够自主发展符合临床推理逻辑的互动策略，而不是简单地模仿现有对话数据中的模式。值得注意的是，我们构建了MTMedDialog，这是第一个能够模拟病人互动的英文多轮医疗咨询数据集。实验表明，\Ours{}在多轮推理能力和最终诊断性能上优于现有模型。这种方法在减少时间压力下误诊风险、解放临床医生处理复杂病例、优化医疗资源配置和缓解劳动力短缺等方面展现了巨大的实用价值。代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/JarvisUSTC/DoctorAgent-RL%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/JarvisUSTC/DoctorAgent-RL找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.19630v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型在生物医学问答领域表现出卓越的能力，但在现实临床咨询中的应用仍面临核心挑战。为解决单轮咨询系统模糊诊断及传统多轮对话模型缺乏灵活性的问题，我们提出了基于强化学习的多智能体协作框架\Ours{}，将医疗咨询建模为不确定条件下的动态决策过程。医生智能体通过强化学习与患者智能体进行多轮互动，并根据来自咨询评估者的综合奖励不断调整其信息收集路径。这使得语言模型能够自主发展符合临床推理逻辑的交流策略，而非简单地模仿现有对话数据中的模式。实验证明，\Ours{}在多轮推理能力和最终诊断性能上均优于现有模型，降低了时间紧迫环境下的误诊风险，解放了医生处理复杂案例的精力，并为优化医疗资源分配和缓解劳动力短缺问题提供了策略。代码和数据的可用性信息为<a target="_blank" rel="noopener" href="https://github.com/JarvisUSTC/DoctorAgent-RL%E3%80%82">https://github.com/JarvisUSTC/DoctorAgent-RL。</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在生物医学问答中展现出色能力，但在真实临床咨询中应用有限。</li>
<li>传统单轮咨询系统导致模糊诊断，而传统多轮对话模型受限于静态监督学习，缺乏灵活性。</li>
<li>提出基于强化学习的多智能体协作框架\Ours{}，将医疗咨询视为不确定下的动态决策过程。</li>
<li>医生智能体通过强化学习优化提问策略，与患者智能体进行多轮互动。</li>
<li>\Ours{}能够根据综合奖励动态调整信息搜集路径，反映临床推理逻辑。</li>
<li>实验证明\Ours{}在多轮推理和诊断性能上超越现有模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.19630">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-046b732eb3ba815d6eadc1107b7ab02e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-348a85480638ebb8995fd6ac95f18796.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Training-LLM-based-Tutors-to-Improve-Student-Learning-Outcomes-in-Dialogues"><a href="#Training-LLM-based-Tutors-to-Improve-Student-Learning-Outcomes-in-Dialogues" class="headerlink" title="Training LLM-based Tutors to Improve Student Learning Outcomes in   Dialogues"></a>Training LLM-based Tutors to Improve Student Learning Outcomes in   Dialogues</h2><p><strong>Authors:Alexander Scarlatos, Naiming Liu, Jaewook Lee, Richard Baraniuk, Andrew Lan</strong></p>
<p>Generative artificial intelligence (AI) has the potential to scale up personalized tutoring through large language models (LLMs). Recent AI tutors are adapted for the tutoring task by training or prompting LLMs to follow effective pedagogical principles, though they are not trained to maximize student learning throughout the course of a dialogue. Therefore, they may engage with students in a suboptimal way. We address this limitation by introducing an approach to train LLMs to generate tutor utterances that maximize the likelihood of student correctness, while still encouraging the model to follow good pedagogical practice. Specifically, we generate a set of candidate tutor utterances and score them using (1) an LLM-based student model to predict the chance of correct student responses and (2) a pedagogical rubric evaluated by GPT-4o. We then use the resulting data to train an open-source LLM, Llama 3.1 8B, using direct preference optimization. We show that tutor utterances generated by our model lead to significantly higher chances of correct student responses while maintaining the pedagogical quality of GPT-4o. We also conduct qualitative analyses and a human evaluation to demonstrate that our model generates high quality tutor utterances. </p>
<blockquote>
<p>生成式人工智能（AI）具有通过大型语言模型（LLM）扩大个性化辅导的潜力。虽然最近的AI辅导者通过训练或提示LLM遵循有效的教学原则而适应辅导任务，但它们并没有在对话过程中接受训练以最大限度地提高学生的学习效率。因此，它们可能与学生的互动方式不佳。我们通过引入一种训练LLM生成辅导话语的方法来解决这一局限性，这种方法可以最大限度地提高学生回答的正确性，同时鼓励模型遵循良好的教学惯例。具体来说，我们生成一组候选的辅导话语，并使用（1）基于LLM的学生模型预测学生正确回答的可能性，（2）通过GPT-4o评估的教学评分来评分它们。然后，我们使用得到的数据直接优化开源LLM（Llama 3.1 8B）。我们展示了我们模型生成的辅导话语能够显著提高学生回答的正确性，同时保持GPT-4o的教学质量。我们还进行了定性分析和人工评估，以证明我们的模型能够生成高质量的辅导话语。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.06424v2">PDF</a> Published in AIED 2025: The 26th International Conference on   Artificial Intelligence in Education</p>
<p><strong>Summary</strong><br>人工智能（AI）通过大型语言模型（LLM）在个性化辅导方面具有巨大的潜力。然而，现有的AI辅导工具并未经过训练以最大化学生的学习效果。本文提出了一种训练LLM的方法，旨在生成能够最大化学生正确率的辅导话语，同时鼓励模型遵循良好的教学原则。通过结合学生模型和基于GPT-4o的教学评分标准，我们生成了候选辅导话语并对其进行了评分。最终结果显示，我们的模型生成的辅导话语不仅大大提高了学生的正确率，而且保持了GPT-4o的教学质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>人工智能具有在个性化辅导领域的巨大潜力，能够通过大型语言模型（LLM）实现规模化应用。</li>
<li>当前AI辅导工具尚未针对最大化学生学习效果进行训练，存在与学生互动方式不够优化的可能。</li>
<li>本文提出了一种结合学生模型和基于GPT-4o的教学评分标准来生成候选辅导话语的方法。</li>
<li>通过训练LLM，能够生成既符合教学原则又能最大化学生正确率的辅导话语。</li>
<li>相比现有AI辅导工具，本文提出的模型在提高学生正确率的同时，保持了较高的教学质量。</li>
<li>进行了定性分析和人类评估，证明了该模型生成的高质量辅导话语。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.06424">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7e192e3fd1bfc148738197b1a38d2665.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-91f0bad8e2b03547202ab239f1f7c112.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-763aae0bdf92f919f1f0f0f24d17abcc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44fd7c5216b0b3683df3ad25cc00d572.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Benchmarking-Open-ended-Audio-Dialogue-Understanding-for-Large-Audio-Language-Models"><a href="#Benchmarking-Open-ended-Audio-Dialogue-Understanding-for-Large-Audio-Language-Models" class="headerlink" title="Benchmarking Open-ended Audio Dialogue Understanding for Large   Audio-Language Models"></a>Benchmarking Open-ended Audio Dialogue Understanding for Large   Audio-Language Models</h2><p><strong>Authors:Kuofeng Gao, Shu-Tao Xia, Ke Xu, Philip Torr, Jindong Gu</strong></p>
<p>Large Audio-Language Models (LALMs), such as GPT-4o, have recently unlocked audio dialogue capabilities, enabling direct spoken exchanges with humans. The potential of LALMs broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we firstly propose the evaluation of ambiguity handling in audio dialogues that expresses different intentions beyond the same literal meaning of sentences, e.g., “Really!?” with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments on 16 LALMs, our analysis reveals that existing LALMs struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones. The benchmark is available at <a target="_blank" rel="noopener" href="https://adu-bench.github.io/">https://adu-bench.github.io/</a>. </p>
<blockquote>
<p>大型音频语言模型（LALMs），如GPT-4o，最近解锁了音频对话功能，能够实现与人类的直接口头交流。LALM的潜力扩大了其在音频对话支持的广泛实际场景中的应用范围。然而，考虑到这些进展，目前仍缺乏一个全面评估LALM在开放式音频对话理解中的性能的基准测试。为了解决这一空白，我们提出了音频对话理解基准测试（ADU-Bench），它包括4个基准测试数据集。它们评估LALM在3个一般场景、12项技能、9种多语言以及4类歧义处理中的开放式音频对话能力。值得注意的是，我们首次提出了音频对话中歧义处理评估，即表达不同意图的音频对话，即使句子具有相同的字面意义，例如带有不同语调的“真的吗！？”等。总之，ADU-Bench包括超过2万条开放式音频对话，用于评估LALM。通过对16个LALM的广泛实验，我们的分析发现，现有的LALM在处理数学符号和公式、理解人类行为（如角色扮演）、理解多种语言以及处理来自不同语音元素的音频对话歧义（如语调、停顿位置和同音字）方面存在困难。基准测试网站为：<a target="_blank" rel="noopener" href="https://adu-bench.github.io./">https://adu-bench.github.io/。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.05167v2">PDF</a> Accepted by ACL 2025</p>
<p><strong>Summary</strong></p>
<p>大型音频语言模型（LALM）如GPT-4o已解锁音频对话功能，实现与人类直接口语交流。然而，目前尚未有全面评估LALM在开放式音频对话理解性能的基准测试。为解决此空白，我们提出了音频对话理解基准测试（ADU-Bench），包含4个基准测试集，评估LALM在3个一般场景、12项技能、9种多元语言及4类歧义处理中的开放式音频对话能力。尤其是，我们首次提出音频对话中歧义处理的评估，如句子相同字面意义下表达不同意图的“Really!?”。总结来说，ADU-Bench包含超过20,000个开放式音频对话，用以评估LALM。通过对16个LALM的广泛实验，我们发现现有模型在数学符号和公式、人类行为理解（如角色扮演）、多语言理解和音频对话歧义处理等方面存在挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型音频语言模型（LALM）已具备音频对话能力，实现与人类直接口语交流。</li>
<li>目前缺乏评估LALM在开放式音频对话理解性能的全面基准测试。</li>
<li>提出的ADU-Bench包含4个基准测试集，涵盖3个场景、12项技能、9种语言和4类歧义处理。</li>
<li>ADU-Bench首次提出音频对话中歧义处理的评估。</li>
<li>LALM面临挑战，如数学符号和公式理解、人类行为理解（如角色扮演）、多语言理解和处理音频对话中的歧义。</li>
<li>ADU-Bench为评估LALM提供了一个重要的基准测试平台。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.05167">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c625e6bef6ea60a402922e35b1c8d995.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-442a77a2247f62d7d40401b619b141ba.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-070dffcb66d0d5a9be25450089640d7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-588fabb41b5ed47c02e1bcd973ea1dea.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Interactive/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Interactive/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Interactive/">
                                    <span class="chip bg-color">Interactive</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-dd6521f4c242431ec2efa8a31e43c27d.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion 方向最新论文已更新，请持续关注 Update in 2025-08-02  Multi-Modal Motion Retrieval by Learning a Fine-Grained Joint Embedding   Space
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1049d4ae6438daf03a9d1aee1979c206.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-08-02  TTS-1 Technical Report
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">30191.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
