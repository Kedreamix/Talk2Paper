<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  Adaptively Distilled ControlNet Accelerated Training and Superior   Sampling for Medical Image Synthesis">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-517ce084a204f5d685320d201230dd74.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    79 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-02-æ›´æ–°"><a href="#2025-08-02-æ›´æ–°" class="headerlink" title="2025-08-02 æ›´æ–°"></a>2025-08-02 æ›´æ–°</h1><h2 id="Adaptively-Distilled-ControlNet-Accelerated-Training-and-Superior-Sampling-for-Medical-Image-Synthesis"><a href="#Adaptively-Distilled-ControlNet-Accelerated-Training-and-Superior-Sampling-for-Medical-Image-Synthesis" class="headerlink" title="Adaptively Distilled ControlNet: Accelerated Training and Superior   Sampling for Medical Image Synthesis"></a>Adaptively Distilled ControlNet: Accelerated Training and Superior   Sampling for Medical Image Synthesis</h2><p><strong>Authors:Kunpeng Qiu, Zhiying Zhou, Yongxin Guo</strong></p>
<p>Medical image annotation is constrained by privacy concerns and labor-intensive labeling, significantly limiting the performance and generalization of segmentation models. While mask-controllable diffusion models excel in synthesis, they struggle with precise lesion-mask alignment. We propose \textbf{Adaptively Distilled ControlNet}, a task-agnostic framework that accelerates training and optimization through dual-model distillation. Specifically, during training, a teacher model, conditioned on mask-image pairs, regularizes a mask-only student model via predicted noise alignment in parameter space, further enhanced by adaptive regularization based on lesion-background ratios. During sampling, only the student model is used, enabling privacy-preserving medical image generation. Comprehensive evaluations on two distinct medical datasets demonstrate state-of-the-art performance: TransUNet improves mDice&#x2F;mIoU by 2.4%&#x2F;4.2% on KiTS19, while SANet achieves 2.6%&#x2F;3.5% gains on Polyps, highlighting its effectiveness and superiority. Code is available at GitHub. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒæ ‡æ³¨å—åˆ°éšç§æ‹…å¿§å’Œæ ‡æ³¨å·¥ä½œé‡å¤§çš„é™åˆ¶ï¼Œè¿™æ˜¾è‘—é™åˆ¶äº†åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚è™½ç„¶å¯æ§åˆ¶çš„æ‰©æ•£æ¨¡å‹åœ¨åˆæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç²¾ç¡®ç—…ç¶æ©è†œå¯¹å‡†æ–¹é¢å­˜åœ¨å›°éš¾ã€‚æˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”è’¸é¦ControlNetï¼ˆAdaptively Distilled ControlNetï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä»»åŠ¡é€šç”¨çš„æ¡†æ¶ï¼Œé€šè¿‡åŒæ¨¡å‹è’¸é¦åŠ é€Ÿè®­ç»ƒå’Œä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä»¥æ•™å¸ˆæ¨¡å‹ï¼ˆåŸºäºæ©è†œå›¾åƒå¯¹ï¼‰é€šè¿‡å‚æ•°ç©ºé—´çš„é¢„æµ‹å™ªå£°å¯¹é½æ¥è§„èŒƒä»…æ©è†œçš„å­¦ç”Ÿæ¨¡å‹ï¼Œå¹¶è¿›ä¸€æ­¥å¢å¼ºåŸºäºç—…ç¶èƒŒæ™¯æ¯”çš„è‡ªé€‚åº”æ­£åˆ™åŒ–ã€‚åœ¨é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œåªä½¿ç”¨å­¦ç”Ÿæ¨¡å‹ï¼Œä»è€Œå®ç°éšç§ä¿æŠ¤çš„åŒ»å­¦å›¾åƒç”Ÿæˆã€‚åœ¨ä¸¤ä¸ªä¸åŒçš„åŒ»å­¦æ•°æ®é›†ä¸Šçš„ç»¼åˆè¯„ä¼°è¯æ˜äº†å…¶å“è¶Šæ€§èƒ½ï¼šTransUNetåœ¨KiTS19æ•°æ®é›†ä¸Šæé«˜äº†mDice&#x2F;mIoUçš„å¾—åˆ†2.4%&#x2F;4.2%ï¼Œè€ŒSANetåœ¨Polypsæ•°æ®é›†ä¸Šå®ç°äº†2.6%&#x2F;3.5%çš„å¢ç›Šï¼Œå‡¸æ˜¾äº†å…¶æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23652v1">PDF</a> Accepted by MICCAI2025</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒæ ‡æ³¨å—é™äºéšç§æ‹…å¿§å’Œç¹ççš„æ ‡æ³¨å·¥ä½œï¼Œé™åˆ¶äº†åˆ†å‰²æ¨¡å‹çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚æå‡ºè‡ªé€‚åº”è’¸é¦ControlNetï¼Œé€šè¿‡åŒæ¨¡å‹è’¸é¦åŠ é€Ÿè®­ç»ƒå’Œä¼˜åŒ–ã€‚è®­ç»ƒæœŸé—´ï¼Œä»¥æ©è†œå›¾åƒå¯¹ä¸ºæ¡ä»¶çš„æ•™å¸ˆæ¨¡å‹é€šè¿‡å‚æ•°ç©ºé—´çš„é¢„æµ‹å™ªå£°å¯¹é½å¯¹ä»…ä½¿ç”¨æ©è†œçš„å­¦ç”Ÿçš„æ¨¡å‹è¿›è¡Œæ­£åˆ™åŒ–ï¼Œå¹¶é€šè¿‡ç—…å˜èƒŒæ™¯æ¯”ç‡è¿›è¡Œè‡ªé€‚åº”å¢å¼ºã€‚é‡‡æ ·æœŸé—´ä»…ä½¿ç”¨å­¦ç”Ÿæ¨¡å‹ï¼Œå®ç°éšç§ä¿æŠ¤çš„åŒ»å­¦å›¾åƒç”Ÿæˆã€‚åœ¨ä¸¤ç§åŒ»å­¦æ•°æ®é›†ä¸Šçš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œå…¶å…·æœ‰å“è¶Šçš„æ€§èƒ½ï¼ŒTransUNetåœ¨KiTS19ä¸Šçš„mDice&#x2F;mIoUæé«˜äº†2.4%&#x2F;4.2%ï¼ŒSANetåœ¨Polypsä¸Šçš„å¢ç›Šä¸º2.6%&#x2F;3.5%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒæ ‡æ³¨é¢ä¸´éšç§å’Œç¹çæ ‡æ³¨çš„æŒ‘æˆ˜ï¼Œå½±å“æ¨¡å‹æ€§èƒ½ä¸æ³›åŒ–ã€‚</li>
<li>è‡ªé€‚åº”è’¸é¦ControlNeté€šè¿‡åŒæ¨¡å‹è’¸é¦åŠ é€Ÿè®­ç»ƒå’Œä¼˜åŒ–ã€‚</li>
<li>æ•™å¸ˆæ¨¡å‹åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨æ©è†œå›¾åƒå¯¹è¿›è¡Œæ­£åˆ™åŒ–ï¼Œå­¦ç”Ÿæ¨¡å‹ä»…ä½¿ç”¨æ©è†œã€‚</li>
<li>é‡‡æ ·æœŸé—´ä»…ä½¿ç”¨å­¦ç”Ÿæ¨¡å‹ä»¥å®ç°éšç§ä¿æŠ¤çš„åŒ»å­¦å›¾åƒç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨ä¸¤ç§åŒ»å­¦æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>TransUNetåœ¨KiTS19æ•°æ®é›†ä¸Šçš„mDice&#x2F;mIoUæœ‰æ‰€æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23652">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-27aec319c555cb84a1217756aabdbf5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fca6d88b0b96d07154dbe597e3d0a766.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ff68157b30c53504a18bac4945fe0a0c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2f75ee3bdc0bb53e7e3064e7bfa4a24a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-167f062263edb8b321cc1e920c59188a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c97563aae3e07801d61783fcd81aa12e.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Medical-Image-De-Identification-Benchmark-Challenge"><a href="#Medical-Image-De-Identification-Benchmark-Challenge" class="headerlink" title="Medical Image De-Identification Benchmark Challenge"></a>Medical Image De-Identification Benchmark Challenge</h2><p><strong>Authors:Linmin Pei, Granger Sutton, Michael Rutherford, Ulrike Wagner, Tracy Nolan, Kirk Smith, Phillip Farmer, Peter Gu, Ambar Rana, Kailing Chen, Thomas Ferleman, Brian Park, Ye Wu, Jordan Kojouharov, Gargi Singh, Jon Lemon, Tyler Willis, Milos Vukadinovic, Grant Duffy, Bryan He, David Ouyang, Marco Pereanez, Daniel Samber, Derek A. Smith, Christopher Cannistraci, Zahi Fayad, David S. Mendelson, Michele Bufano, Elmar Kotter, Hamideh Haghiri, Rajesh Baidya, Stefan Dvoretskii, Klaus H. Maier-Hein, Marco Nolden, Christopher Ablett, Silvia Siggillino, Sandeep Kaushik, Hongzhu Jiang, Sihan Xie, Zhiyu Wan, Alex Michie, Simon J Doran, Angeline Aurelia Waly, Felix A. Nathaniel Liang, Humam Arshad Mustagfirin, Michelle Grace Felicia, Kuo Po Chih, Rahul Krish, Ghulam Rasool, Nidhal Bouaynaya, Nikolas Koutsoubis, Kyle Naddeo, Kartik Pandit, Tony Oâ€™Sullivan, Raj Krish, Qinyan Pan, Scott Gustafson, Benjamin Kopchick, Laura Opsahl-Ong, Andrea Olvera-Morales, Jonathan Pinney, Kathryn Johnson, Theresa Do, Juergen Klenk, Maria Diaz, Arti Singh, Rong Chai, David A. Clunie, Fred Prior, Keyvan Farahani</strong></p>
<p>The de-identification (deID) of protected health information (PHI) and personally identifiable information (PII) is a fundamental requirement for sharing medical images, particularly through public repositories, to ensure compliance with patient privacy laws. In addition, preservation of non-PHI metadata to inform and enable downstream development of imaging artificial intelligence (AI) is an important consideration in biomedical research. The goal of MIDI-B was to provide a standardized platform for benchmarking of DICOM image deID tools based on a set of rules conformant to the HIPAA Safe Harbor regulation, the DICOM Attribute Confidentiality Profiles, and best practices in preservation of research-critical metadata, as defined by The Cancer Imaging Archive (TCIA). The challenge employed a large, diverse, multi-center, and multi-modality set of real de-identified radiology images with synthetic PHI&#x2F;PII inserted.   The MIDI-B Challenge consisted of three phases: training, validation, and test. Eighty individuals registered for the challenge. In the training phase, we encouraged participants to tune their algorithms using their in-house or public data. The validation and test phases utilized the DICOM images containing synthetic identifiers (of 216 and 322 subjects, respectively). Ten teams successfully completed the test phase of the challenge. To measure success of a rule-based approach to image deID, scores were computed as the percentage of correct actions from the total number of required actions. The scores ranged from 97.91% to 99.93%. Participants employed a variety of open-source and proprietary tools with customized configurations, large language models, and optical character recognition (OCR). In this paper we provide a comprehensive report on the MIDI-B Challengeâ€™s design, implementation, results, and lessons learned. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒä¸­å—ä¿æŠ¤çš„å¥åº·ä¿¡æ¯ï¼ˆPHIï¼‰å’Œä¸ªäººå¯è¯†åˆ«ä¿¡æ¯ï¼ˆPIIï¼‰çš„å»æ ‡è¯†åŒ–æ˜¯å…±äº«åŒ»å­¦å›¾åƒçš„åŸºæœ¬è¦æ±‚ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å…¬å…±ä»“åº“å…±äº«æ—¶ï¼Œä»¥ç¡®ä¿ç¬¦åˆæ‚£è€…éšç§æ³•å¾‹ã€‚å¦å¤–ï¼Œä¿ç•™éPHIå…ƒæ•°æ®ä»¥å‘ŠçŸ¥å¹¶ä¿ƒè¿›æˆåƒäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„ä¸‹æ¸¸å‘å±•æ˜¯ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­çš„é‡è¦è€ƒè™‘å› ç´ ã€‚MIDI-Bçš„ç›®æ ‡æ˜¯æä¾›ä¸€ä¸ªæ ‡å‡†åŒ–çš„å¹³å°ï¼Œä»¥åŸºäºä¸€ç»„ç¬¦åˆHIPAAå®‰å…¨æ¸¯æ³•è§„ã€DICOMå±æ€§ä¿å¯†é…ç½®æ–‡ä»¶ä»¥åŠç”±ç™Œç—‡æˆåƒæ¡£æ¡ˆï¼ˆTCIAï¼‰å®šä¹‰çš„ä¿ç•™ç ”ç©¶å…³é”®å…ƒæ•°æ®çš„æœ€ä½³å®è·µè§„åˆ™ï¼Œå¯¹DICOMå›¾åƒå»æ ‡è¯†åŒ–å·¥å…·è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚è¯¥æŒ‘æˆ˜é‡‡ç”¨äº†ä¸€ç»„å¤§å‹ã€å¤šæ ·ã€å¤šä¸­å¿ƒã€å¤šæ¨¡å¼çš„çœŸå®å»æ ‡è¯†åŒ–æ”¾å°„å­¦å›¾åƒï¼Œå¹¶æ’å…¥äº†åˆæˆPHI&#x2F;PIIã€‚MIDI-BæŒ‘æˆ˜åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šåŸ¹è®­ã€éªŒè¯å’Œæµ‹è¯•ã€‚å…±æœ‰80äººæ³¨å†Œå‚åŠ æ­¤æ¬¡æŒ‘æˆ˜ã€‚åœ¨åŸ¹è®­é˜¶æ®µï¼Œæˆ‘ä»¬é¼“åŠ±å‚ä¸è€…ä½¿ç”¨å…¶å†…éƒ¨æˆ–å…¬å¼€æ•°æ®è°ƒæ•´ç®—æ³•ã€‚éªŒè¯å’Œæµ‹è¯•é˜¶æ®µä½¿ç”¨äº†åŒ…å«åˆæˆæ ‡è¯†ç¬¦çš„DICOMå›¾åƒï¼ˆåˆ†åˆ«ä¸º216ä¸ªå’Œ322ä¸ªä¸»é¢˜ï¼‰ã€‚æœ‰10æ”¯é˜Ÿä¼æˆåŠŸå®Œæˆäº†æŒ‘æˆ˜çš„æµ‹è¯•é˜¶æ®µã€‚ä¸ºäº†è¡¡é‡åŸºäºè§„åˆ™çš„å›¾åƒå»æ ‡è¯†åŒ–æ–¹æ³•çš„æˆåŠŸç¨‹åº¦ï¼Œåˆ†æ•°æ˜¯æ ¹æ®æ­£ç¡®è¡ŒåŠ¨çš„æ•°é‡å æ€»è¡ŒåŠ¨æ•°é‡çš„ç™¾åˆ†æ¯”æ¥è®¡ç®—çš„ã€‚å¾—åˆ†èŒƒå›´ä»97.91%åˆ°99.93%ã€‚å‚ä¸è€…ä½¿ç”¨äº†å„ç§å¼€æºå’Œä¸“æœ‰å·¥å…·ï¼Œå…·æœ‰è‡ªå®šä¹‰é…ç½®ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†å…³äºMIDI-BæŒ‘æˆ˜çš„è®¾è®¡ã€å®æ–½ã€ç»“æœå’Œç»éªŒæ•™è®­çš„å…¨é¢æŠ¥å‘Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23608v1">PDF</a> 19 pages</p>
<p><strong>Summary</strong><br>    åŒ»å­¦å›¾åƒå…±äº«æ—¶ï¼Œéœ€å¯¹ä¿æŠ¤å¥åº·ä¿¡æ¯ï¼ˆPHIï¼‰å’Œä¸ªäººå¯è¯†åˆ«ä¿¡æ¯ï¼ˆPIIï¼‰è¿›è¡Œå»æ ‡è¯†åŒ–å¤„ç†ï¼ŒåŒæ—¶ä¿ç•™éPHIå…ƒæ•°æ®ä¿¡æ¯ä»¥æ”¯æŒä¸‹æ¸¸äººå·¥æ™ºèƒ½æˆåƒæŠ€æœ¯çš„å‘å±•ã€‚MIDI-BæŒ‘æˆ˜æ—¨åœ¨æä¾›ä¸€ä¸ªæ ‡å‡†åŒ–å¹³å°ï¼ŒåŸºäºä¸€ç»„ç¬¦åˆHIPAAå®‰å…¨æ¸¯æ³•è§„ã€DICOMå±æ€§ä¿å¯†é…ç½®æ–‡ä»¶åŠæœ€ä½³å®è·µçš„ç ”ç©¶å…³é”®æ€§å…ƒæ•°æ®å­˜å‚¨è§„å®šçš„è§„åˆ™ï¼Œå¯¹DICOMå›¾åƒå»æ ‡è¯†åŒ–å·¥å…·è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚æŒ‘æˆ˜ä½¿ç”¨äº†å¤§é‡å¤šä¸­å¿ƒã€å¤šæ¨¡å¼çš„çœŸå®å»æ ‡è¯†åŒ–æ”¾å°„å›¾åƒï¼Œå¹¶æ’å…¥äº†åˆæˆPHI&#x2F;PIIã€‚è¯¥æŒ‘æˆ˜åŒ…æ‹¬åŸ¹è®­ã€éªŒè¯å’Œæµ‹è¯•ä¸‰ä¸ªé˜¶æ®µï¼Œå…±æœ‰80äººå‚åŠ ã€‚å‚èµ›è€…é‡‡ç”¨åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼ŒæˆåŠŸç‡åœ¨97.91%è‡³99.93%ä¹‹é—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å»æ ‡è¯†åŒ–åŒ»å­¦å›¾åƒæ˜¯å…±äº«åŒ»å­¦å›¾åƒçš„åŸºæœ¬è¦æ±‚ï¼Œä»¥ç¡®ä¿ç¬¦åˆæ‚£è€…éšç§æ³•å¾‹ã€‚</li>
<li>MIDI-BæŒ‘æˆ˜æ—¨åœ¨æä¾›ä¸€ä¸ªæ ‡å‡†åŒ–å¹³å°ï¼Œç”¨äºåŸºå‡†æµ‹è¯•DICOMå›¾åƒå»æ ‡è¯†åŒ–å·¥å…·ã€‚</li>
<li>æŒ‘æˆ˜ä½¿ç”¨äº†å«æœ‰åˆæˆPHI&#x2F;PIIçš„å¤§é‡å¤šä¸­å¿ƒã€å¤šæ¨¡å¼çš„çœŸå®å»æ ‡è¯†åŒ–æ”¾å°„å›¾åƒã€‚</li>
<li>æŒ‘æˆ˜åŒ…æ‹¬åŸ¹è®­ã€éªŒè¯å’Œæµ‹è¯•ä¸‰ä¸ªé˜¶æ®µï¼Œå…±æœ‰80äººå‚åŠ ï¼Œæœ‰åä¸ªå›¢é˜ŸæˆåŠŸå®Œæˆæµ‹è¯•é˜¶æ®µã€‚</li>
<li>åŸºäºè§„åˆ™çš„å»æ ‡è¯†åŒ–æ–¹æ³•æˆåŠŸç‡åœ¨97.91%è‡³99.93%ä¹‹é—´ã€‚</li>
<li>å‚èµ›è€…é‡‡ç”¨äº†å¤šç§å¼€æºå’Œä¸“æœ‰å·¥å…·ï¼Œä»¥åŠå¤§å‹è¯­è¨€æ¨¡å‹å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4bf310fb997be0ccf56e02e30b4479a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7734d045de6e7d4bbfb284daf5f28fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad97f24be5990f9a705085d839e0a108.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3ff2088ec668db055d5fdc4940915f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c79e949f0993f019b10d5074c1b91030.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="T-Detect-Tail-Aware-Statistical-Normalization-for-Robust-Detection-of-Adversarial-Machine-Generated-Text"><a href="#T-Detect-Tail-Aware-Statistical-Normalization-for-Robust-Detection-of-Adversarial-Machine-Generated-Text" class="headerlink" title="T-Detect: Tail-Aware Statistical Normalization for Robust Detection of   Adversarial Machine-Generated Text"></a>T-Detect: Tail-Aware Statistical Normalization for Robust Detection of   Adversarial Machine-Generated Text</h2><p><strong>Authors:Alva West, Luodan Zhang, Liuliu Zhang, Minjun Zhu, Yixuan Weng, Yue Zhang</strong></p>
<p>The proliferation of sophisticated text generation models necessitates the development of robust detection methods capable of identifying machine-generated content, particularly text designed to evade detection through adversarial perturbations. Existing zero-shot detectors often rely on statistical measures that implicitly assume Gaussian distributions, a premise that falters when confronted with the heavy-tailed statistical artifacts characteristic of adversarial or non-native English texts. This paper introduces T-Detect, a novel detection method that fundamentally redesigns the statistical core of curvature-based detectors. Our primary innovation is the replacement of standard Gaussian normalization with a heavy-tailed discrepancy score derived from the Studentâ€™s t-distribution. This approach is theoretically grounded in the empirical observation that adversarial texts exhibit significant leptokurtosis, rendering traditional statistical assumptions inadequate. T-Detect computes a detection score by normalizing the log-likelihood of a passage against the expected moments of a t-distribution, providing superior resilience to statistical outliers. We validate our approach on the challenging RAID benchmark for adversarial text and the comprehensive HART dataset. Experiments show that T-Detect provides a consistent performance uplift over strong baselines, improving AUROC by up to 3.9% in targeted domains. When integrated into a two-dimensional detection framework (CT), our method achieves state-of-the-art performance, with an AUROC of 0.926 on the Books domain of RAID. Our contributions are a new, theoretically-justified statistical foundation for text detection, an ablation-validated method that demonstrates superior robustness, and a comprehensive analysis of its performance under adversarial conditions. Ours code are released at <a target="_blank" rel="noopener" href="https://github.com/ResearAI/t-detect">https://github.com/ResearAI/t-detect</a>. </p>
<blockquote>
<p>éšç€å…ˆè¿›çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„æ™®åŠï¼Œéœ€è¦å¼€å‘èƒ½å¤Ÿè¯†åˆ«æœºå™¨ç”Ÿæˆå†…å®¹ï¼ˆç‰¹åˆ«æ˜¯é€šè¿‡å¯¹æŠ—æ€§æ‰°åŠ¨è®¾è®¡çš„æ—¨åœ¨é€ƒé¿æ£€æµ‹çš„æ–‡æœ¬ï¼‰çš„ç¨³å¥æ£€æµ‹æ–¹æ³•çš„å¿…è¦æ€§æ—¥ç›Šå‡¸æ˜¾ã€‚ç°æœ‰çš„é›¶æ ·æœ¬æ£€æµ‹å™¨é€šå¸¸ä¾èµ–äºç»Ÿè®¡æŒ‡æ ‡ï¼Œè¿™äº›æŒ‡æ ‡éšå«åœ°å‡è®¾é«˜æ–¯åˆ†å¸ƒã€‚ç„¶è€Œï¼Œåœ¨é¢å¯¹å…·æœ‰åšå°¾ç»Ÿè®¡ç‰¹å¾çš„å¯¹æŠ—æ€§æˆ–éè‹±è¯­åŸç”Ÿæ–‡æœ¬æ—¶ï¼Œè¿™ä¸€å‰æä¼šå¤±æ•ˆã€‚æœ¬æ–‡ä»‹ç»äº†T-Detectï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„æ£€æµ‹æ–¹æ³•ï¼Œå®ƒä»æ ¹æœ¬ä¸Šé‡æ–°è®¾è®¡äº†åŸºäºæ›²ç‡çš„æ£€æµ‹å™¨çš„ç»Ÿè®¡æ ¸å¿ƒã€‚æˆ‘ä»¬çš„ä¸»è¦åˆ›æ–°ä¹‹å¤„åœ¨äºç”¨åŸºäºå­¦ç”Ÿtåˆ†å¸ƒçš„åšå°¾åå·®å¾—åˆ†æ›¿æ¢æ ‡å‡†çš„é«˜æ–¯å½’ä¸€åŒ–ã€‚è¿™ä¸€æ–¹æ³•ä»ç†è®ºä¸Šå»ºç«‹åœ¨å®è¯è§‚å¯Ÿçš„åŸºç¡€ä¸Šï¼Œå³å¯¹æŠ—æ€§æ–‡æœ¬è¡¨ç°å‡ºæ˜¾è‘—çš„å°–å³°åº¦ï¼Œä½¿å¾—ä¼ ç»Ÿçš„ç»Ÿè®¡å‡è®¾å˜å¾—æ— æ•ˆã€‚T-Detecté€šè¿‡è®¡ç®—ä¸€æ®µæ–‡å­—çš„å¯¹æ•°ä¼¼ç„¶ä¸tåˆ†å¸ƒçš„æœŸæœ›çŸ©çš„å½’ä¸€åŒ–æ£€æµ‹åˆ†æ•°ï¼Œå¯¹äºç»Ÿè®¡å¼‚å¸¸å€¼å…·æœ‰å‡ºè‰²çš„æŠ—å¹²æ‰°èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¯¹æŠ—æ€§æ–‡æœ¬RAIDåŸºå‡†æµ‹è¯•å’Œå…¨é¢çš„HARTæ•°æ®é›†ä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼ŒT-Detectåœ¨ç›®æ ‡é¢†åŸŸä¸Šæ¯”å¼ºå¤§çš„åŸºçº¿æä¾›äº†æŒç»­çš„æ€§èƒ½æå‡ï¼Œåœ¨ç›®æ ‡é¢†åŸŸçš„æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰æé«˜äº†é«˜è¾¾3.9%ã€‚å½“é›†æˆåˆ°äºŒç»´æ£€æµ‹æ¡†æ¶ï¼ˆCTï¼‰ä¸­æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œåœ¨RAIDçš„ä¹¦ç±é¢†åŸŸçš„æ›²çº¿ä¸‹é¢ç§¯ä¸º0.926ã€‚æˆ‘ä»¬çš„è´¡çŒ®åœ¨äºä¸ºæ–‡æœ¬æ£€æµ‹æä¾›äº†æ–°çš„ç†è®ºéªŒè¯çš„ç»Ÿè®¡åŸºç¡€ï¼Œä¸€ä¸ªç»è¿‡éªŒè¯çš„æ˜¾ç¤ºå‡ºå“è¶Šç¨³å¥æ€§çš„æ–¹æ³•ï¼Œä»¥åŠå¯¹æŠ—æ¡ä»¶ä¸‹å…¶æ€§èƒ½çš„å…¨é¢åˆ†æã€‚æˆ‘ä»¬çš„ä»£ç å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/ResearAI/t-detect%E3%80%82">https://github.com/ResearAI/t-detectã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23577v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†T-Detectè¿™ä¸€æ–°å‹çš„æ–‡æœ¬æ£€æµ‹æ–¹æ³•çš„å¼•å…¥ï¼Œå®ƒä¸“é—¨é’ˆå¯¹å¯¹æŠ—æ€§æ–‡æœ¬çš„ç‰¹ç‚¹è¿›è¡Œæ”¹è¿›ï¼Œå¯¹ä¼ ç»Ÿé«˜æ–¯æ­£æ€åˆ†å¸ƒçš„å‡è®¾è¿›è¡Œä¼˜åŒ–ã€‚ä½¿ç”¨å­¦ç”Ÿtåˆ†å¸ƒçš„é‡å°¾åå·®åˆ†æ•°æ›¿æ¢æ ‡å‡†é«˜æ–¯å½’ä¸€åŒ–æ–¹æ³•ï¼Œæå‡äº†å¯¹æŠ—æ€§æ–‡æœ¬çš„è¯†åˆ«æ•ˆæœã€‚å®éªŒéªŒè¯äº†åœ¨æŒ‘æˆ˜æ€§å’Œç»¼åˆæ€§çš„æ•°æ®é›†ä¸Šï¼ŒT-Detectæ–¹æ³•è¡¨ç°å‡ºè¶…è¶ŠåŸºçº¿æ–¹æ³•çš„ç¨³å¥æ€§èƒ½ã€‚å…¶ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>T-Detectæ˜¯ä¸€ç§æ–°å‹çš„æ–‡æœ¬æ£€æµ‹æ¨¡å‹ï¼Œç”¨äºè¯†åˆ«æœºå™¨ç”Ÿæˆçš„å¯¹æŠ—æ€§æ–‡æœ¬å†…å®¹ã€‚</li>
<li>ä¼ ç»ŸåŸºäºé«˜æ–¯åˆ†å¸ƒçš„é›¶æ ·æœ¬æ£€æµ‹å™¨åœ¨é¢å¯¹å¯¹æŠ—æ€§æˆ–éåŸç”Ÿè‹±è¯­æ–‡æœ¬æ—¶å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>T-Detectå¼•å…¥å­¦ç”Ÿtåˆ†å¸ƒçš„é‡å°¾åå·®åˆ†æ•°æ›¿æ¢é«˜æ–¯å½’ä¸€åŒ–ï¼Œåº”å¯¹æ–‡æœ¬æ•°æ®é›†ä¸­çš„é‡å°¾ç»Ÿè®¡ç‰¹æ€§é—®é¢˜ã€‚</li>
<li>å¯¹æŠ—æ€§æ–‡æœ¬è¡¨ç°å‡ºæ˜¾è‘—çš„å°–å³°åº¦ï¼Œä½¿å¾—ä¼ ç»Ÿç»Ÿè®¡å‡è®¾å¤±æ•ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23577">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-b4bb5f06a7ab4abb1c04e12a4a85419e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-08af6d53789f55d735a04e3a7f5d4e32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-237581abc8ac6143452e93490a92938e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f4a5cd9dd2b499ada10d7cccc862aaf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd8d80fbef7b02979fe1345fa8a89863.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Hyperbolic-Cycle-Alignment-for-Infrared-Visible-Image-Fusion"><a href="#Hyperbolic-Cycle-Alignment-for-Infrared-Visible-Image-Fusion" class="headerlink" title="Hyperbolic Cycle Alignment for Infrared-Visible Image Fusion"></a>Hyperbolic Cycle Alignment for Infrared-Visible Image Fusion</h2><p><strong>Authors:Timing Li, Bing Cao, Jiahe Feng, Haifang Cao, Qinghau Hu, Pengfei Zhu</strong></p>
<p>Image fusion synthesizes complementary information from multiple sources, mitigating the inherent limitations of unimodal imaging systems. Accurate image registration is essential for effective multi-source data fusion. However, existing registration methods, often based on image translation in Euclidean space, fail to handle cross-modal misalignment effectively, resulting in suboptimal alignment and fusion quality. To overcome this limitation, we explore image alignment in non-Euclidean space and propose a Hyperbolic Cycle Alignment Network (Hy-CycleAlign). To the best of our knowledge, Hy-CycleAlign is the first image registration method based on hyperbolic space. It introduces a dual-path cross-modal cyclic registration framework, in which a forward registration network aligns cross-modal inputs, while a backward registration network reconstructs the original image, forming a closed-loop registration structure with geometric consistency. Additionally, we design a Hyperbolic Hierarchy Contrastive Alignment (H$^{2}$CA) module, which maps images into hyperbolic space and imposes registration constraints, effectively reducing interference caused by modality discrepancies. We further analyze image registration in both Euclidean and hyperbolic spaces, demonstrating that hyperbolic space enables more sensitive and effective multi-modal image registration. Extensive experiments on misaligned multi-modal images demonstrate that our method significantly outperforms existing approaches in both image alignment and fusion. Our code will be publicly available. </p>
<blockquote>
<p>å›¾åƒèåˆæŠ€æœ¯é€šè¿‡èåˆå¤šä¸ªæ¥æºçš„äº’è¡¥ä¿¡æ¯ï¼Œç¼“è§£äº†å•æ¨¡æ€æˆåƒç³»ç»Ÿçš„å›ºæœ‰å±€é™æ€§ã€‚å‡†ç¡®çš„å›¾åƒé…å‡†å¯¹äºæœ‰æ•ˆçš„å¤šæºæ•°æ®èåˆè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é…å‡†æ–¹æ³•é€šå¸¸åŸºäºæ¬§å‡ é‡Œå¾—ç©ºé—´çš„å›¾åƒç¿»è¯‘ï¼Œæ— æ³•æœ‰æ•ˆåœ°å¤„ç†è·¨æ¨¡æ€çš„é”™ä½é—®é¢˜ï¼Œå¯¼è‡´é…å‡†å’Œèåˆè´¨é‡ä¸ä½³ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æ¢ç´¢äº†éæ¬§å‡ é‡Œå¾—ç©ºé—´çš„å›¾åƒé…å‡†ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŒè·¯å¾„è·¨æ¨¡æ€å¾ªç¯é…å‡†æ¡†æ¶â€”â€”Hyperbolic Cycle Alignment Networkï¼ˆHy-CycleAlignï¼‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒHy-CycleAlignæ˜¯åŸºäºåŒæ›²ç©ºé—´çš„ç¬¬ä¸€ä¸ªå›¾åƒé…å‡†æ–¹æ³•ã€‚å®ƒå¼•å…¥äº†ä¸€ç§åŒè·¯å¾„è·¨æ¨¡æ€å¾ªç¯é…å‡†æ¡†æ¶ï¼Œå…¶ä¸­å‰å‘é…å‡†ç½‘ç»œå¯¹é½è·¨æ¨¡æ€è¾“å…¥ï¼Œè€Œåå‘é…å‡†ç½‘ç»œé‡å»ºåŸå§‹å›¾åƒï¼Œå½¢æˆå…·æœ‰å‡ ä½•ä¸€è‡´æ€§çš„é—­ç¯é…å‡†ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªHyperbolic Hierarchy Contrastive Alignmentï¼ˆHÂ²CAï¼‰æ¨¡å—ï¼Œå®ƒå°†å›¾åƒæ˜ å°„åˆ°åŒæ›²ç©ºé—´å¹¶æ–½åŠ é…å‡†çº¦æŸï¼Œæœ‰æ•ˆå‡å°‘äº†ç”±æ¨¡æ€å·®å¼‚å¼•èµ·çš„å¹²æ‰°ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†æ¬§å‡ é‡Œå¾—ç©ºé—´å’ŒåŒæ›²ç©ºé—´ä¸­çš„å›¾åƒé…å‡†ï¼Œè¡¨æ˜åŒæ›²ç©ºé—´èƒ½å¤Ÿå®ç°æ›´æ•æ„Ÿå’Œæœ‰æ•ˆçš„å¤šæ¨¡æ€å›¾åƒé…å‡†ã€‚åœ¨é”™ä½å¤šæ¨¡æ€å›¾åƒä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒé…å‡†å’Œèåˆæ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23508v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å›¾åƒèåˆèƒ½å¤Ÿç»¼åˆå¤šä¸ªæ¥æºçš„äº’è¡¥ä¿¡æ¯ï¼Œå¼¥è¡¥å•æ¨¡æ€æˆåƒç³»ç»Ÿçš„å›ºæœ‰å±€é™ã€‚å‡†ç¡®çš„å›¾åƒé…å‡†æ˜¯å®ç°å¤šæºæ•°æ®æœ‰æ•ˆèåˆçš„å…³é”®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é…å‡†æ–¹æ³•å¤§å¤šåŸºäºæ¬§å‡ é‡Œå¾—ç©ºé—´çš„å›¾åƒè½¬æ¢ï¼Œéš¾ä»¥æœ‰æ•ˆå¤„ç†è·¨æ¨¡æ€çš„é”™ä½é—®é¢˜ï¼Œå¯¼è‡´é…å‡†å’Œèåˆè´¨é‡ä¸ä½³ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™ï¼Œæœ¬æ–‡æ¢ç´¢äº†éæ¬§å‡ é‡Œå¾—ç©ºé—´çš„å›¾åƒå¯¹é½ï¼Œå¹¶é¦–æ¬¡æå‡ºäº†åŸºäºåŒè·¯å¾„è·¨æ¨¡æ€å¾ªç¯é…å‡†æ¡†æ¶çš„Hyperbolic Cycle Alignment Networkï¼ˆHy-CycleAlignï¼‰ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†Hyperbolic Hierarchy Contrastive Alignmentï¼ˆHÂ²CAï¼‰æ¨¡å—ï¼Œå°†å›¾åƒæ˜ å°„åˆ°åŒæ›²ç©ºé—´å¹¶æ–½åŠ é…å‡†çº¦æŸï¼Œæœ‰æ•ˆé™ä½ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚å¼•èµ·çš„å¹²æ‰°ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºåŒæ›²ç©ºé—´è¿›è¡Œå¤šæ¨¡æ€å›¾åƒé…å‡†æ›´åŠ ç²¾ç¡®æœ‰æ•ˆã€‚å®éªŒè¯æ˜ï¼Œæœ¬æ–‡æ–¹æ³•åœ¨å›¾åƒé…å‡†å’Œèåˆæ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒèåˆèƒ½å¤Ÿç»¼åˆå¤šä¸ªæ¥æºçš„äº’è¡¥ä¿¡æ¯ï¼Œæœ‰åŠ©äºå…‹æœå•æ¨¡æ€æˆåƒçš„å±€é™æ€§ã€‚</li>
<li>å‡†ç¡®å›¾åƒé…å‡†æ˜¯å®ç°å¤šæºæ•°æ®æœ‰æ•ˆèåˆçš„å…³é”®æ­¥éª¤ã€‚</li>
<li>ç°æœ‰åŸºäºæ¬§å‡ é‡Œå¾—ç©ºé—´çš„å›¾åƒé…å‡†æ–¹æ³•éš¾ä»¥å¤„ç†è·¨æ¨¡æ€é”™ä½é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒé…å‡†æ–¹æ³•â€”â€”Hyperbolic Cycle Alignment Networkï¼ˆHy-CycleAlignï¼‰ï¼ŒåŸºäºéæ¬§å‡ é‡Œå¾—ç©ºé—´è¿›è¡Œå›¾åƒå¯¹é½ã€‚</li>
<li>Hy-CycleAlignåŒ…æ‹¬ä¸€ä¸ªå‰å‘é…å‡†ç½‘ç»œå’Œä¸€ä¸ªåå‘é…å‡†ç½‘ç»œï¼Œå½¢æˆé—­ç¯æ³¨å†Œç»“æ„ï¼Œå®ç°å‡ ä½•ä¸€è‡´æ€§ã€‚</li>
<li>è®¾è®¡çš„Hyperbolic Hierarchy Contrastive Alignmentï¼ˆHÂ²CAï¼‰æ¨¡å—å¯æœ‰æ•ˆé™ä½ä¸åŒæ¨¡æ€é—´çš„å¹²æ‰°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0c636eb8ce00e7b3def37164fc523e5d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-189c1f0f8b615011c3e9ab26a0ab693c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0281d7756a852d54685c21b5b21848cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e49dcff49153df90b6cb16a3db7b6a2.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Out-of-Distribution-Detection-in-Medical-Imaging-via-Diffusion-Trajectories"><a href="#Out-of-Distribution-Detection-in-Medical-Imaging-via-Diffusion-Trajectories" class="headerlink" title="Out-of-Distribution Detection in Medical Imaging via Diffusion   Trajectories"></a>Out-of-Distribution Detection in Medical Imaging via Diffusion   Trajectories</h2><p><strong>Authors:Lemar Abdi, Francisco Caetano, Amaan Valiuddin, Christiaan Viviers, Hamdi Joudeh, Fons van der Sommen</strong></p>
<p>In medical imaging, unsupervised out-of-distribution (OOD) detection offers an attractive approach for identifying pathological cases with extremely low incidence rates. In contrast to supervised methods, OOD-based approaches function without labels and are inherently robust to data imbalances. Current generative approaches often rely on likelihood estimation or reconstruction error, but these methods can be computationally expensive, unreliable, and require retraining if the inlier data changes. These limitations hinder their ability to distinguish nominal from anomalous inputs efficiently, consistently, and robustly. We propose a reconstruction-free OOD detection method that leverages the forward diffusion trajectories of a Stein score-based denoising diffusion model (SBDDM). By capturing trajectory curvature via the estimated Stein score, our approach enables accurate anomaly scoring with only five diffusion steps. A single SBDDM pre-trained on a large, semantically aligned medical dataset generalizes effectively across multiple Near-OOD and Far-OOD benchmarks, achieving state-of-the-art performance while drastically reducing computational cost during inference. Compared to existing methods, SBDDM achieves a relative improvement of up to 10.43% and 18.10% for Near-OOD and Far-OOD detection, making it a practical building block for real-time, reliable computer-aided diagnosis. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œæ— ç›‘ç£çš„ç¦»ç¾¤åˆ†å¸ƒï¼ˆOODï¼‰æ£€æµ‹ä¸ºè¯†åˆ«å‘ç—…ç‡æä½çš„ç—…ç†ç—…ä¾‹æä¾›äº†ä¸€ç§å¸å¼•äººçš„æ–¹æ³•ã€‚ä¸æœ‰ç›‘ç£æ–¹æ³•ä¸åŒï¼ŒåŸºäºOODçš„æ–¹æ³•æ— éœ€æ ‡ç­¾ï¼Œå¯¹æ•°æ®ä¸å¹³è¡¡å…·æœ‰å›ºæœ‰çš„é²æ£’æ€§ã€‚å½“å‰çš„ç”Ÿæˆæ–¹æ³•é€šå¸¸ä¾èµ–äºä¼¼ç„¶ä¼°è®¡æˆ–é‡å»ºè¯¯å·®ï¼Œä½†è¿™äº›æ–¹æ³•è®¡ç®—é‡å¤§ã€ä¸å¯é ï¼Œå¦‚æœå†…éƒ¨æ•°æ®å‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦é‡æ–°è®­ç»ƒã€‚è¿™äº›å±€é™æ€§é˜»ç¢äº†å®ƒä»¬æœ‰æ•ˆã€ä¸€è‡´å’Œç¨³å¥åœ°åŒºåˆ†æ­£å¸¸è¾“å…¥ä¸å¼‚å¸¸è¾“å…¥çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€é‡å»ºçš„OODæ£€æµ‹æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨åŸºäºSteinå¾—åˆ†çš„é™å™ªæ‰©æ•£æ¨¡å‹çš„æ­£å‘æ‰©æ•£è½¨è¿¹ï¼ˆSBDDMï¼‰ã€‚é€šè¿‡ä¼°è®¡çš„Steinå¾—åˆ†æ•æ‰è½¨è¿¹æ›²ç‡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»…ä½¿ç”¨äº”ä¸ªæ‰©æ•£æ­¥éª¤å³å¯å®ç°ç²¾ç¡®å¼‚å¸¸è¯„åˆ†ã€‚å•ä¸ªSBDDMé¢„è®­ç»ƒäºå¤§å‹è¯­ä¹‰å¯¹é½åŒ»å­¦æ•°æ®é›†ä¸Šï¼Œå¯æœ‰æ•ˆåº”ç”¨äºå¤šä¸ªè¿‘OODå’Œè¿œOODåŸºå‡†æµ‹è¯•ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—æˆæœ¬ã€‚ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSBDDMåœ¨è¿‘OODå’Œè¿œOODæ£€æµ‹æ–¹é¢çš„ç›¸å¯¹æ”¹è¿›ç‡åˆ†åˆ«é«˜è¾¾10.43%å’Œ18.10%ï¼Œä½¿å…¶æˆä¸ºå®æ—¶å¯é è®¡ç®—æœºè¾…åŠ©è¯Šæ–­çš„å®ç”¨æ„å»ºå—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23411v1">PDF</a> Accepted at Uncertainty for Safe Utilization of Machine Learning in   Medical Imaging, MICCAI 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ— ç›‘ç£çš„ç¦»ç¾¤åˆ†å¸ƒæ£€æµ‹ï¼Œåˆ©ç”¨Steinè¯„åˆ†å»å™ªæ‰©æ•£æ¨¡å‹çš„æ‰©æ•£è½¨è¿¹è¿›è¡Œå¼‚å¸¸è¯„åˆ†ï¼Œæœ‰æ•ˆå®ç°åŒ»å­¦å›¾åƒä¸­çš„å¼‚å¸¸æ£€æµ‹ã€‚æ­¤æ–¹æ³•ä»…éœ€äº”æ­¥æ‰©æ•£ï¼Œå¯åœ¨å¤šä¸ªè¿‘ç¦»ç¾¤åˆ†å¸ƒå’Œè¿œç¦»ç¾¤åˆ†å¸ƒæ ‡å‡†ä¸Šå®ç°è‰¯å¥½æ³›åŒ–ï¼Œæ˜¾è‘—å‡å°‘è®¡ç®—æˆæœ¬ï¼Œå…·æœ‰å®é™…åº”ç”¨äºå®æ—¶å¯é çš„è®¡ç®—æœºè¾…åŠ©è¯Šæ–­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— ç›‘ç£ç¦»ç¾¤åˆ†å¸ƒï¼ˆOODï¼‰æ£€æµ‹åœ¨åŒ»å­¦å›¾åƒä¸­ç”¨äºè¯†åˆ«ä½å‘ç—…ç‡çš„ç—…ä¾‹ã€‚</li>
<li>ä¸ç›‘ç£æ–¹æ³•ç›¸æ¯”ï¼ŒOODæ–¹æ³•æ— éœ€æ ‡ç­¾ï¼Œèƒ½æ›´ç¨³å¥åœ°åº”å¯¹æ•°æ®ä¸å¹³è¡¡ã€‚</li>
<li>å½“å‰ç”Ÿæˆå¼æ–¹æ³•å¸¸ä¾èµ–äºå¯èƒ½æ€§ä¼°è®¡æˆ–é‡å»ºè¯¯å·®ï¼Œä½†å­˜åœ¨è®¡ç®—æ˜‚è´µã€ä¸å¯é ç­‰é—®é¢˜ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºSteinè¯„åˆ†å»å™ªæ‰©æ•£æ¨¡å‹çš„é‡å»ºå¼OODæ£€æµ‹æ–¹æ³•ã€‚</li>
<li>é€šè¿‡æ•æ‰æ‰©æ•£è½¨è¿¹çš„å¼¯æ›²åº¦è¿›è¡Œå¼‚å¸¸è¯„åˆ†ï¼Œä»…äº”æ­¥æ‰©æ•£å³å¯å®ç°å‡†ç¡®å¼‚å¸¸è¯„åˆ†ã€‚</li>
<li>SBDDMæ¨¡å‹åœ¨å¤šä¸ªè¿‘ç¦»ç¾¤å’Œè¿œç¦»ç¾¤æ ‡å‡†ä¸Šå®ç°è‰¯å¥½æ³›åŒ–ï¼Œæ˜¾è‘—å‡å°‘æ¨ç†è®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23411">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3c0452124bcbd9dca9215499bc41d539.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-73ca0c27df4df270c134f4dbf2508b2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74ac3eb8886ed21642ace5edcd84a046.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Learning-Semantic-Directions-for-Feature-Augmentation-in-Domain-Generalized-Medical-Segmentation"><a href="#Learning-Semantic-Directions-for-Feature-Augmentation-in-Domain-Generalized-Medical-Segmentation" class="headerlink" title="Learning Semantic Directions for Feature Augmentation in   Domain-Generalized Medical Segmentation"></a>Learning Semantic Directions for Feature Augmentation in   Domain-Generalized Medical Segmentation</h2><p><strong>Authors:Yingkai Wang, Yaoyao Zhu, Xiuding Cai, Yuhao Xiao, Haotian Wu, Yu Yao</strong></p>
<p>Medical image segmentation plays a crucial role in clinical workflows, but domain shift often leads to performance degradation when models are applied to unseen clinical domains. This challenge arises due to variations in imaging conditions, scanner types, and acquisition protocols, limiting the practical deployment of segmentation models. Unlike natural images, medical images typically exhibit consistent anatomical structures across patients, with domain-specific variations mainly caused by imaging conditions. This unique characteristic makes medical image segmentation particularly challenging.   To address this challenge, we propose a domain generalization framework tailored for medical image segmentation. Our approach improves robustness to domain-specific variations by introducing implicit feature perturbations guided by domain statistics. Specifically, we employ a learnable semantic direction selector and a covariance-based semantic intensity sampler to modulate domain-variant features while preserving task-relevant anatomical consistency. Furthermore, we design an adaptive consistency constraint that is selectively applied only when feature adjustment leads to degraded segmentation performance. This constraint encourages the adjusted features to align with the original predictions, thereby stabilizing feature selection and improving the reliability of the segmentation.   Extensive experiments on two public multi-center benchmarks show that our framework consistently outperforms existing domain generalization approaches, achieving robust and generalizable segmentation performance across diverse clinical domains. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠå·¥ä½œæµç¨‹ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œä½†å½“æ¨¡å‹åº”ç”¨äºæœªè§è¿‡çš„ä¸´åºŠé¢†åŸŸæ—¶ï¼Œé¢†åŸŸåç§»å¾€å¾€ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¿™ä¸€æŒ‘æˆ˜æ˜¯ç”±äºæˆåƒæ¡ä»¶ã€æ‰«æä»ªç±»å‹å’Œé‡‡é›†åè®®çš„å˜åŒ–è€Œäº§ç”Ÿçš„ï¼Œé™åˆ¶äº†åˆ†å‰²æ¨¡å‹çš„å®é™…åº”ç”¨ã€‚ä¸è‡ªç„¶å›¾åƒä¸åŒï¼ŒåŒ»å­¦å›¾åƒåœ¨æ‚£è€…ä¹‹é—´é€šå¸¸è¡¨ç°å‡ºä¸€è‡´çš„è§£å‰–ç»“æ„ï¼Œé¢†åŸŸç‰¹å®šçš„å˜åŒ–ä¸»è¦ç”±æˆåƒæ¡ä»¶å¼•èµ·ã€‚è¿™ç§ç‹¬ç‰¹çš„ç‰¹ç‚¹ä½¿å¾—åŒ»å­¦å›¾åƒåˆ†å‰²ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23326v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠå·¥ä½œä¸­è‡³å…³é‡è¦ï¼Œä½†æ¨¡å‹åº”ç”¨äºæœªè§é¢†åŸŸæ—¶ï¼Œé¢†åŸŸåç§»ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¿™ä¸€æŒ‘æˆ˜æºäºæˆåƒæ¡ä»¶ã€æ‰«æä»ªç±»å‹å’Œé‡‡é›†åè®®çš„å˜åŒ–ï¼Œé™åˆ¶äº†åˆ†å‰²æ¨¡å‹çš„å®ç”¨éƒ¨ç½²ã€‚ä¸ºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„é¢†åŸŸæ³›åŒ–æ¡†æ¶ã€‚é€šè¿‡å¼•å…¥ç”±é¢†åŸŸç»Ÿè®¡å¼•å¯¼çš„éšå¼ç‰¹å¾æ‰°åŠ¨ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æé«˜äº†å¯¹é¢†åŸŸç‰¹å®šå˜åŒ–çš„ç¨³å¥æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å¤šæ ·åŒ–ä¸´åºŠé¢†åŸŸä¸Šè¡¨ç°ç¨³å¥ï¼Œä¼˜äºç°æœ‰é¢†åŸŸæ³›åŒ–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠå·¥ä½œä¸­éå¸¸é‡è¦ï¼Œä½†é¢†åŸŸåç§»ä¼šå½±å“æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é¢†åŸŸåç§»ä¸»è¦ç”±äºæˆåƒæ¡ä»¶ã€æ‰«æä»ªç±»å‹å’Œé‡‡é›†åè®®çš„å˜åŒ–å¼•èµ·ã€‚</li>
<li>åŒ»å­¦å›¾åƒå…·æœ‰ä¸€è‡´çš„è§£å‰–ç»“æ„ç‰¹æ€§ï¼Œè¿™ä½¿å¾—åŒ»å­¦å›¾åƒåˆ†å‰²æ›´å…·æŒ‘æˆ˜æ€§ã€‚</li>
<li>ä¸ºè§£å†³é¢†åŸŸåç§»é—®é¢˜ï¼Œæå‡ºäº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„é¢†åŸŸæ³›åŒ–æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥éšå¼ç‰¹å¾æ‰°åŠ¨æ¥æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿æŒä»»åŠ¡ç›¸å…³çš„è§£å‰–ç»“æ„ä¸€è‡´æ€§ã€‚</li>
<li>æ¡†æ¶ä¸­è®¾è®¡äº†è‡ªé€‚åº”ä¸€è‡´æ€§çº¦æŸï¼Œé€‰æ‹©æ€§åº”ç”¨ç‰¹å¾è°ƒæ•´ï¼Œæé«˜åˆ†å‰²çš„å¯é æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c9c6cacfa60d0b0218e0b52c02984588.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77af7ac77e5ff23a41ccff83e828c053.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d64ae584a3da4c4166ef2a7576c115ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4d442e9382879998e0a9aa68c2c1ed0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59d4f9814fc88d27292e0ba315f20c15.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-18d3bf8b0dc083ba3a0b0242b523980c.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="UniLiP-Adapting-CLIP-for-Unified-Multimodal-Understanding-Generation-and-Editing"><a href="#UniLiP-Adapting-CLIP-for-Unified-Multimodal-Understanding-Generation-and-Editing" class="headerlink" title="UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation   and Editing"></a>UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation   and Editing</h2><p><strong>Authors:Hao Tang, Chenwei Xie, Xiaoyi Bao, Tingyu Weng, Pandeng Li, Yun Zheng, Liwei Wang</strong></p>
<p>In this paper, we propose UniLIP, which extends CLIP to reconstruction, generation and editing, thereby building a unified tokenizer upon its exceptional comprehension capabilities. Previous CLIP-based unified methods often require additional diffusion decoders or quantization to support reconstruction and generation tasks, leading to inconsistent reconstruction or degradation of original comprehension performance.In contrast, we introduce a two-stage training scheme and a self-distillation strategy that progressively integrates reconstruction capabilities into CLIP, allowing it to maintain original comprehension performance while achieving effective image reconstruction. Furthermore, we propose a dual-condition architecture to connect the MLLM and diffusion transformer, using both learnable queries and the last layer multimodal hidden states as joint conditions. This method not only enables the utilization of the MLLMâ€™s strong reasoning capabilities in generation tasks, but also maximizes the exploitation of the rich information in UniLIP features during editing tasks. In text-to-image generation tasks, UniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark respectively, surpassing all previous unified models of similar scale. In image editing, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark, surpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP effectively expand the application scope of CLIP, enabling continuous CLIP features to not only serve as the optimal choice for understanding tasks but also achieve highly competitive performance in generation and editing tasks. </p>
<blockquote>
<p>æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†UniLIPï¼Œå®ƒæ‰©å±•äº†CLIPçš„åŠŸèƒ½ï¼Œä½¿å…¶èƒ½å¤Ÿè¿›è¡Œé‡å»ºã€ç”Ÿæˆå’Œç¼–è¾‘ï¼Œä»è€Œåœ¨å…¶å‡ºè‰²çš„ç†è§£èƒ½åŠ›åŸºç¡€ä¸Šæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†è¯å™¨ã€‚ä»¥å‰åŸºäºCLIPçš„ç»Ÿä¸€æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„æ‰©æ•£è§£ç å™¨æˆ–é‡åŒ–æ¥æ”¯æŒé‡å»ºå’Œç”Ÿæˆä»»åŠ¡ï¼Œè¿™å¯¼è‡´é‡å»ºä¸ä¸€è‡´æˆ–åŸå§‹ç†è§£æ€§èƒ½ä¸‹é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆå’Œè‡ªè’¸é¦ç­–ç•¥ï¼Œé€æ­¥å°†é‡å»ºèƒ½åŠ›é›†æˆåˆ°CLIPä¸­ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ä¿æŒåŸå§‹ç†è§£æ€§èƒ½çš„åŒæ—¶å®ç°æœ‰æ•ˆçš„å›¾åƒé‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒæ¡ä»¶æ¶æ„ï¼Œå°†MLLMå’Œæ‰©æ•£å˜å‹å™¨è¿æ¥èµ·æ¥ï¼Œä½¿ç”¨å¯å­¦ä¹ çš„æŸ¥è¯¢å’Œæœ€åä¸€å±‚å¤šæ¨¡æ€éšè—çŠ¶æ€ä½œä¸ºè”åˆæ¡ä»¶ã€‚è¿™ç§æ–¹æ³•ä¸ä»…èƒ½ä½¿MLLMåœ¨ç”Ÿæˆä»»åŠ¡ä¸­åˆ©ç”¨å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸”åœ¨ç¼–è¾‘ä»»åŠ¡ä¸­æœ€å¤§é™åº¦åœ°åˆ©ç”¨UniLIPç‰¹å¾çš„ä¸°å¯Œä¿¡æ¯ã€‚åœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒUniLIPåœ¨GenEvalå’ŒWISEåŸºå‡†æµ‹è¯•ä¸Šåˆ†åˆ«è·å¾—0.87å’Œ0.53çš„åˆ†æ•°ï¼Œè¶…è¿‡äº†ç›¸åŒè§„æ¨¡çš„æ‰€æœ‰å…ˆå‰ç»Ÿä¸€æ¨¡å‹ã€‚åœ¨å›¾åƒç¼–è¾‘æ–¹é¢ï¼ŒUniLIPåœ¨ImgEditåŸºå‡†æµ‹è¯•ä¸Šä¹Ÿå–å¾—äº†3.62çš„åˆ†æ•°ï¼Œè¶…è¿‡äº†æœ€è¿‘çš„å…ˆè¿›æ¨¡å‹ï¼Œå¦‚BAGELå’ŒUniWorld-V1ã€‚UniLIPæœ‰æ•ˆåœ°æ‰©å±•äº†CLIPçš„åº”ç”¨èŒƒå›´ï¼Œä½¿CLIPç‰¹å¾ä¸ä»…èƒ½å¤Ÿæˆä¸ºç†è§£ä»»åŠ¡çš„æœ€ä½³é€‰æ‹©ï¼Œè€Œä¸”åœ¨ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­ä¹Ÿå…·æœ‰é«˜åº¦çš„ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23278v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†UniLIPæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºCLIPæ¶æ„è¿›è¡Œæ‰©å±•ï¼Œæ”¯æŒé‡å»ºã€ç”Ÿæˆå’Œç¼–è¾‘åŠŸèƒ½ï¼Œæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†è¯å™¨ï¼Œå¹¶å€ŸåŠ©å…¶å‡ºè‰²çš„ç†è§£èƒ½åŠ›ã€‚é€šè¿‡å¼•å…¥ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆå’Œè‡ªè’¸é¦ç­–ç•¥ï¼ŒUniLIPèƒ½å¤Ÿåœ¨ä¿æŒåŸæœ‰ç†è§£èƒ½åŠ›çš„åŒæ—¶ï¼Œå®ç°æœ‰æ•ˆçš„å›¾åƒé‡å»ºã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§åŒæ¡ä»¶æ¶æ„ï¼Œè¿æ¥MLLMå’Œæ‰©æ•£å˜å‹å™¨ï¼Œé‡‡ç”¨å¯å­¦ä¹ çš„æŸ¥è¯¢å’Œæœ€åä¸€å±‚å¤šæ¨¡æ€éšè—çŠ¶æ€ä½œä¸ºè”åˆæ¡ä»¶ã€‚UniLIPåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆç»©ï¼Œè¶…è¶Šäº†ç±»ä¼¼è§„æ¨¡çš„ç»Ÿä¸€æ¨¡å‹ã€‚åœ¨å›¾åƒç¼–è¾‘æ–¹é¢ä¹Ÿè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚æ€»ä½“è€Œè¨€ï¼ŒUniLIPä¸ä»…æ‰©å¤§äº†CLIPçš„åº”ç”¨èŒƒå›´ï¼Œè€Œä¸”ä¸ºç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡æä¾›äº†é«˜åº¦ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniLIPæ‰©å±•äº†CLIPæ¨¡å‹ï¼Œæ”¯æŒé‡å»ºã€ç”Ÿæˆå’Œç¼–è¾‘åŠŸèƒ½ï¼Œæ„å»ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†è¯å™¨ã€‚</li>
<li>é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæ–¹æ¡ˆå’Œè‡ªè’¸é¦ç­–ç•¥ï¼ŒUniLIPåœ¨ä¿æŒåŸæœ‰ç†è§£èƒ½åŠ›çš„åŒæ—¶å®ç°äº†æœ‰æ•ˆçš„å›¾åƒé‡å»ºã€‚</li>
<li>åŒæ¡ä»¶æ¶æ„è¿æ¥äº†MLLMå’Œæ‰©æ•£å˜å‹å™¨ï¼Œç»“åˆäº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›å’Œä¸°å¯Œçš„ç‰¹å¾ä¿¡æ¯ã€‚</li>
<li>UniLIPåœ¨æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆç»©ï¼Œè¶…è¶Šäº†å…¶ä»–ç±»ä¼¼è§„æ¨¡çš„ç»Ÿä¸€æ¨¡å‹ã€‚</li>
<li>UniLIPåœ¨å›¾åƒç¼–è¾‘æ–¹é¢è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œè¶…è¿‡äº†æœ€è¿‘çš„å…ˆè¿›æ¨¡å‹ã€‚</li>
<li>UniLIPæ‰©å¤§äº†CLIPçš„åº”ç”¨èŒƒå›´ï¼Œä¸ä»…å¯ä»¥ç”¨äºç†è§£ä»»åŠ¡ï¼Œè¿˜å¯ä»¥åœ¨ç”Ÿæˆå’Œç¼–è¾‘ä»»åŠ¡ä¸­è¡¨ç°å‡ºé«˜åº¦ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23278">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f4dedfbdf71b7632a54db0637e07a927.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ad42eaa1b41deabce43f3f449c6c8b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59b19895e723abcf38b26004b96db503.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc0fb31d2c447433f93deefd92a1545a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6789c7bf7fa8771f5cfa2ff7a33d2160.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-379833647207eff983319d23c8b7e3e6.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Towards-Affordable-Tumor-Segmentation-and-Visualization-for-3D-Breast-MRI-Using-SAM2"><a href="#Towards-Affordable-Tumor-Segmentation-and-Visualization-for-3D-Breast-MRI-Using-SAM2" class="headerlink" title="Towards Affordable Tumor Segmentation and Visualization for 3D Breast   MRI Using SAM2"></a>Towards Affordable Tumor Segmentation and Visualization for 3D Breast   MRI Using SAM2</h2><p><strong>Authors:Solha Kang, Eugene Kim, Joris Vankerschaver, Utku Ozbulak</strong></p>
<p>Breast MRI provides high-resolution volumetric imaging critical for tumor assessment and treatment planning, yet manual interpretation of 3D scans remains labor-intensive and subjective. While AI-powered tools hold promise for accelerating medical image analysis, adoption of commercial medical AI products remains limited in low- and middle-income countries due to high license costs, proprietary software, and infrastructure demands. In this work, we investigate whether the Segment Anything Model 2 (SAM2) can be adapted for low-cost, minimal-input 3D tumor segmentation in breast MRI. Using a single bounding box annotation on one slice, we propagate segmentation predictions across the 3D volume using three different slice-wise tracking strategies: top-to-bottom, bottom-to-top, and center-outward. We evaluate these strategies across a large cohort of patients and find that center-outward propagation yields the most consistent and accurate segmentations. Despite being a zero-shot model not trained for volumetric medical data, SAM2 achieves strong segmentation performance under minimal supervision. We further analyze how segmentation performance relates to tumor size, location, and shape, identifying key failure modes. Our results suggest that general-purpose foundation models such as SAM2 can support 3D medical image analysis with minimal supervision, offering an accessible and affordable alternative for resource-constrained settings. </p>
<blockquote>
<p>ä¹³è…ºMRIæä¾›äº†é«˜åˆ†è¾¨ç‡çš„ä½“ç§¯æˆåƒï¼Œå¯¹è‚¿ç˜¤è¯„ä¼°å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¯¹3Dæ‰«æçš„æ‰‹åŠ¨è§£è¯»ä»ç„¶åŠ³åŠ¨å¯†é›†ä¸”ä¸»è§‚ã€‚è™½ç„¶äººå·¥æ™ºèƒ½å·¥å…·åœ¨åŠ é€ŸåŒ»å­¦å›¾åƒåˆ†ææ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç”±äºè®¸å¯è¯æˆæœ¬é«˜ã€ä¸“æœ‰è½¯ä»¶å’ŒåŸºç¡€è®¾æ–½éœ€æ±‚å¤§ï¼Œå•†ä¸šåŒ»ç–—AIäº§å“åœ¨ä½æ”¶å…¥å’Œä¸­æ”¶å…¥å›½å®¶çš„é‡‡ç”¨ä»ç„¶æœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥Segment Anything Model 2ï¼ˆSAM2ï¼‰æ˜¯å¦å¯é€‚åº”ä½æˆæœ¬ã€æœ€å°‘è¾“å…¥çš„ä¹³è…ºMRI 3Dè‚¿ç˜¤åˆ†å‰²ã€‚æˆ‘ä»¬ä»…å¯¹ä¸€ä¸ªåˆ‡ç‰‡è¿›è¡Œå•ä¸ªè¾¹ç•Œæ¡†æ³¨é‡Šï¼Œç„¶åä½¿ç”¨ä¸‰ç§ä¸åŒçš„åˆ‡ç‰‡è·Ÿè¸ªç­–ç•¥ï¼ˆä»ä¸Šåˆ°ä¸‹ã€ä»ä¸‹åˆ°ä¸Šã€å’Œä¸­å¿ƒå‘å¤–ï¼‰åœ¨3Dä½“ç§¯ä¸Šä¼ æ’­åˆ†å‰²é¢„æµ‹ã€‚æˆ‘ä»¬åœ¨å¤§æ‰¹æ‚£è€…ä¸­å¯¹è¿™äº›ç­–ç•¥è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°ä¸­å¿ƒå‘å¤–ä¼ æ’­ç­–ç•¥äº§ç”Ÿäº†æœ€ä¸€è‡´å’Œå‡†ç¡®çš„åˆ†å‰²ç»“æœã€‚å°½ç®¡SAM2æ˜¯ä¸€ä¸ªæœªé’ˆå¯¹ä½“ç§¯åŒ»å­¦æ•°æ®è¿›è¡Œè®­ç»ƒçš„é›¶æ ·æœ¬æ¨¡å‹ï¼Œä½†åœ¨æå°‘ç›‘ç£çš„æƒ…å†µä¸‹ä»å–å¾—äº†å¼ºå¤§çš„åˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†åˆ†å‰²æ€§èƒ½ä¸è‚¿ç˜¤å¤§å°ã€ä½ç½®å’Œå½¢çŠ¶çš„å…³ç³»ï¼Œç¡®å®šäº†å…³é”®çš„å¤±è´¥æ¨¡å¼ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¦‚SAM2ç­‰é€šç”¨åŸºç¡€æ¨¡å‹å¯åœ¨æå°‘ç›‘ç£çš„æƒ…å†µä¸‹æ”¯æŒ3DåŒ»å­¦å›¾åƒåˆ†æï¼Œä¸ºèµ„æºå—é™çš„ç¯å¢ƒæä¾›å¯è´Ÿæ‹…çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23272v1">PDF</a> Accepted for publication in the 28th International Conference on   Medical Image Computing and Computer Assisted Intervention (MICCAI), 2nd Deep   Breast Workshop on AI and Imaging for Diagnostic and Treatment Challenges in   Breast Care (DeepBreath), 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å°†Segment Anything Model 2ï¼ˆSAM2ï¼‰æ¨¡å‹é€‚åº”äºä½æˆæœ¬çš„ä¹³è…ºç™ŒMRIä¸‰ç»´è‚¿ç˜¤åˆ†å‰²çš„å¯èƒ½æ€§ã€‚è¯¥ç ”ç©¶åˆ©ç”¨å•ä¸€åˆ‡ç‰‡ä¸Šçš„è¾¹ç•Œæ¡†æ ‡æ³¨ï¼Œé€šè¿‡ä¸‰ç§ä¸åŒçš„åˆ‡ç‰‡è·Ÿè¸ªç­–ç•¥è¿›è¡Œä¸‰ç»´ä½“ç§¯å†…çš„åˆ†å‰²é¢„æµ‹ä¼ æ’­ï¼Œæœ€ç»ˆå‘ç°ä¸­å¿ƒå‘å¤–ä¼ æ’­ç­–ç•¥æœ€ä¸ºä¸€è‡´ä¸”å‡†ç¡®ã€‚å°½ç®¡SAM2æ˜¯ä¸€ä¸ªæœªç»ä¸‰ç»´åŒ»å­¦æ•°æ®è®­ç»ƒçš„é›¶æ ·æœ¬æ¨¡å‹ï¼Œä½†åœ¨æå°‘ç›‘ç£çš„æƒ…å†µä¸‹ä»å–å¾—äº†å¼ºå¤§çš„åˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºMRIæä¾›é«˜åˆ†è¾¨ç‡çš„ä½“ç§¯æˆåƒï¼Œå¯¹è‚¿ç˜¤è¯„ä¼°å’Œæ²»ç–—è®¡åˆ’è‡³å…³é‡è¦ï¼Œä½†æ‰‹åŠ¨è§£è¯»ä¸‰ç»´æ‰«æä»ç„¶åŠ³åŠ¨å¯†é›†ä¸”ä¸»è§‚ã€‚</li>
<li>AIå·¥å…·åœ¨åŠ é€ŸåŒ»å­¦å›¾åƒåˆ†ææ–¹é¢å…·æ½œåŠ›ï¼Œä½†åœ¨ä¸­ä½æ”¶å…¥å›½å®¶çš„é‡‡ç”¨å—åˆ°é™åˆ¶ï¼Œä¸»è¦ç”±äºé«˜æ˜‚çš„è®¸å¯æˆæœ¬ã€ä¸“æœ‰è½¯ä»¶å’ŒåŸºç¡€è®¾æ–½éœ€æ±‚ã€‚</li>
<li>ç ”ç©¶è€…è°ƒæŸ¥äº†Segment Anything Model 2ï¼ˆSAM2ï¼‰æ˜¯å¦å¯é€‚åº”ä½æˆæœ¬ã€ä½è¾“å…¥çš„ä¹³è…ºMRIä¸‰ç»´è‚¿ç˜¤åˆ†å‰²ã€‚</li>
<li>ä½¿ç”¨å•ä¸€åˆ‡ç‰‡ä¸Šçš„è¾¹ç•Œæ¡†æ ‡æ³¨ï¼Œç ”ç©¶é‡‡ç”¨äº†ä¸‰ç§ä¸åŒçš„åˆ‡ç‰‡è·Ÿè¸ªä¼ æ’­ç­–ç•¥ï¼Œå‘ç°ä¸­å¿ƒå‘å¤–ä¼ æ’­ç­–ç•¥è¡¨ç°æœ€ä½³ã€‚</li>
<li>å°½ç®¡SAM2æœªç»è¿‡ä¸‰ç»´åŒ»å­¦æ•°æ®è®­ç»ƒï¼Œä½†åœ¨æå°‘ç›‘ç£ä¸‹ä»å®ç°äº†å¼ºå¤§çš„åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶è¿›ä¸€æ­¥åˆ†æäº†åˆ†å‰²æ€§èƒ½ä¸è‚¿ç˜¤å¤§å°ã€ä½ç½®å’Œå½¢çŠ¶çš„å…³ç³»ï¼Œç¡®å®šäº†ä¸»è¦çš„å¤±è´¥æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23272">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1402f5ba6b25016cf245817f820dcae4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c49bdb8d7066ab51e541db27c06919f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-97fb2a048cfbbbbdcbc10b9466a7df96.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-469c0a2f0e48b3f97ecbb786d15ab910.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a933e0ef16ada851ddc0e8e7000ad8d2.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="EMedNeXt-An-Enhanced-Brain-Tumor-Segmentation-Framework-for-Sub-Saharan-Africa-using-MedNeXt-V2-with-Deep-Supervision"><a href="#EMedNeXt-An-Enhanced-Brain-Tumor-Segmentation-Framework-for-Sub-Saharan-Africa-using-MedNeXt-V2-with-Deep-Supervision" class="headerlink" title="EMedNeXt: An Enhanced Brain Tumor Segmentation Framework for Sub-Saharan   Africa using MedNeXt V2 with Deep Supervision"></a>EMedNeXt: An Enhanced Brain Tumor Segmentation Framework for Sub-Saharan   Africa using MedNeXt V2 with Deep Supervision</h2><p><strong>Authors:Ahmed Jaheen, Abdelrahman Elsayed, Damir Kim, Daniil Tikhonov, Matheus Scatolin, Mohor Banerjee, Qiankun Ji, Mostafa Salem, Hu Wang, Sarim Hashmi, Mohammad Yaqub</strong></p>
<p>Brain cancer affects millions worldwide, and in nearly every clinical setting, doctors rely on magnetic resonance imaging (MRI) to diagnose and monitor gliomas. However, the current standard for tumor quantification through manual segmentation of multi-parametric MRI is time-consuming, requires expert radiologists, and is often infeasible in under-resourced healthcare systems. This problem is especially pronounced in low-income regions, where MRI scanners are of lower quality and radiology expertise is scarce, leading to incorrect segmentation and quantification. In addition, the number of acquired MRI scans in Africa is typically small. To address these challenges, the BraTS-Lighthouse 2025 Challenge focuses on robust tumor segmentation in sub-Saharan Africa (SSA), where resource constraints and image quality degradation introduce significant shifts. In this study, we present EMedNeXt â€“ an enhanced brain tumor segmentation framework based on MedNeXt V2 with deep supervision and optimized post-processing pipelines tailored for SSA. EMedNeXt introduces three key contributions: a larger region of interest, an improved nnU-Net v2-based architectural skeleton, and a robust model ensembling system. Evaluated on the hidden validation set, our solution achieved an average LesionWise DSC of 0.897 with an average LesionWise NSD of 0.541 and 0.84 at a tolerance of 0.5 mm and 1.0 mm, respectively. </p>
<blockquote>
<p>è„‘ç™Œå½±å“å…¨çƒæ•°ç™¾ä¸‡äººï¼Œåœ¨ä¸´åºŠå‡ ä¹æ‰€æœ‰åœºæ™¯ä¸­ï¼ŒåŒ»ç”Ÿéƒ½ä¾èµ–ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æ¥è¯Šæ–­å’Œç›‘æµ‹èƒ¶è´¨ç˜¤ã€‚ç„¶è€Œï¼Œç›®å‰é€šè¿‡å¤šå‚æ•°MRIæ‰‹åŠ¨åˆ†å‰²å¯¹è‚¿ç˜¤è¿›è¡Œé‡åŒ–çš„æ ‡å‡†è€—æ—¶ï¼Œéœ€è¦ä¸“å®¶æ”¾å°„ç§‘åŒ»ç”Ÿï¼Œå¹¶ä¸”åœ¨èµ„æºä¸è¶³çš„å«ç”Ÿç³»ç»Ÿä¸­é€šå¸¸ä¸å¯è¡Œã€‚è¿™ä¸€é—®é¢˜åœ¨ä½æ”¶å…¥åœ°åŒºå°¤ä¸ºçªå‡ºï¼Œé‚£é‡Œçš„MRIæ‰«æä»ªè´¨é‡è¾ƒå·®ï¼Œä¸”ç¼ºä¹æ”¾å°„å­¦ä¸“å®¶ï¼Œå¯¼è‡´åˆ†å‰²å’Œé‡åŒ–ä¸å‡†ç¡®ã€‚æ­¤å¤–ï¼Œéæ´²è·å¾—çš„MRIæ‰«ææ•°é‡é€šå¸¸è¾ƒå°‘ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼ŒBraTS-Lighthouse 2025æŒ‘æˆ˜èµ›çš„é‡ç‚¹æ˜¯æ’’å“ˆæ‹‰ä»¥å—éæ´²ï¼ˆSSAï¼‰çš„ç¨³å¥è‚¿ç˜¤åˆ†å‰²ï¼Œé‚£é‡Œçš„èµ„æºçº¦æŸå’Œå›¾åƒè´¨é‡ä¸‹é™å¼•å…¥äº†é‡å¤§å˜åŒ–ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†EMedNeXtâ€”â€”ä¸€ç§åŸºäºMedNeXt V2çš„å¢å¼ºå‹è„‘è‚¿ç˜¤åˆ†å‰²æ¡†æ¶ï¼Œå…·æœ‰æ·±åº¦ç›‘ç£å’Œé’ˆå¯¹SSAä¼˜åŒ–çš„åå¤„ç†ç®¡é“ã€‚EMedNeXtæœ‰ä¸‰ä¸ªä¸»è¦è´¡çŒ®ï¼šæ›´å¤§çš„æ„Ÿå…´è¶£åŒºåŸŸã€æ”¹è¿›çš„nnU-Net v2åŸºç¡€æ¶æ„ï¼Œä»¥åŠç¨³å¥çš„æ¨¡å‹é›†æˆç³»ç»Ÿã€‚åœ¨éšè—éªŒè¯é›†ä¸Šè¯„ä¼°ï¼Œæˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆå¹³å‡LesionWise DSCè¾¾åˆ°0.897ï¼Œå¹³å‡LesionWise NSDä¸º0.541å’Œ0.84ï¼Œå®¹å¿åº¦åˆ†åˆ«ä¸º0.5æ¯«ç±³å’Œ1.0æ¯«ç±³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23256v1">PDF</a> Submitted to the BraTS-Lighthouse 2025 Challenge (MICCAI 2025)</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹éæ´²åœ°åŒºåŒ»ç–—èµ„æºåŒ®ä¹ã€MRIæ‰«æè´¨é‡ä¸ä½³ç­‰é—®é¢˜ï¼ŒEMedNeXtæ¡†æ¶é€šè¿‡æ·±åº¦å­¦ä¹ æŠ€æœ¯å®ç°äº†å¯¹è„‘èƒ¶è´¨ç˜¤çš„ç²¾å‡†åˆ†å‰²ã€‚è¯¥æ¡†æ¶åŸºäºMedNeXt V2ï¼Œå¼•å…¥æ·±åº¦ç›‘ç£æŠ€æœ¯å’Œä¼˜åŒ–åçš„åå¤„ç†æµç¨‹ï¼Œç”¨äºéæ´²åœ°åŒºçš„åŒ»å­¦å½±åƒåˆ†æã€‚å…¶æ€§èƒ½åœ¨éšè—éªŒè¯é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è„‘ç™Œè¯Šæ–­ä¸­ï¼ŒMRIæ˜¯åŒ»ç”Ÿä¾èµ–çš„é‡è¦å·¥å…·ï¼Œå°¤å…¶åœ¨ä½èµ„æºç¯å¢ƒä¸‹ã€‚</li>
<li>å½“å‰æ‰‹åŠ¨åˆ†å‰²å¤šå‚æ•°MRIå¯¹è‚¿ç˜¤è¿›è¡Œé‡åŒ–å­˜åœ¨æ—¶é—´æ¶ˆè€—å¤§ã€ä¾èµ–ä¸“å®¶åŠåœ¨èµ„æºåŒ®ä¹åœ°åŒºéš¾ä»¥å®æ–½ç­‰é—®é¢˜ã€‚</li>
<li>éæ´²åœ°åŒºç‰¹åˆ«æ˜¯æ’’å“ˆæ‹‰ä»¥å—éæ´²é¢ä¸´MRIæ‰«æè´¨é‡ä½ã€èµ„æºçº¦æŸç­‰é—®é¢˜ï¼Œè‚¿ç˜¤åˆ†å‰²æŒ‘æˆ˜æ›´å¤§ã€‚</li>
<li>BraTS-Lighthouse 2025 Challengeæ—¨åœ¨é’ˆå¯¹éæ´²åœ°åŒºçš„è‚¿ç˜¤åˆ†å‰²éš¾é¢˜è¿›è¡ŒæŒ‘æˆ˜ã€‚</li>
<li>EMedNeXtæ¡†æ¶æ˜¯åŸºäºMedNeXt V2å¼€å‘ï¼ŒåŠ å…¥äº†æ·±åº¦ç›‘ç£æŠ€æœ¯å’Œæ”¹è¿›çš„åå¤„ç†æµç¨‹ã€‚</li>
<li>EMedNeXtåœ¨éšè—éªŒè¯é›†ä¸Šçš„è¡¨ç°ä¼˜ç§€ï¼Œå¹³å‡LesionWise DSCè¾¾åˆ°0.897ï¼ŒLesionWise NSDåˆ†åˆ«è¾¾åˆ°0.541å’Œ0.84ã€‚</li>
<li>è¯¥ç ”ç©¶ä¸ºéæ´²åœ°åŒºåŠå…¶ä»–èµ„æºå—é™åœ°åŒºçš„åŒ»å­¦å½±åƒåˆ†ææä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23256">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-296fbadb775bfc9d60437a022148314f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca8811a6cc12935992263eb4eeff0a8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1bccd0bff39eea6b426a35ec28406c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f83e6adb6b88148d85baa39ffb93ecb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-517ce084a204f5d685320d201230dd74.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-106f6230796fadd3628ac7bf6a3ccc6e.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Discovery-of-a-Pair-of-Galaxies-with-Both-Hosting-X-ray-Binary-Candidates-at-z-2-544"><a href="#Discovery-of-a-Pair-of-Galaxies-with-Both-Hosting-X-ray-Binary-Candidates-at-z-2-544" class="headerlink" title="Discovery of a Pair of Galaxies with Both Hosting X-ray Binary   Candidates at $z&#x3D;2.544$"></a>Discovery of a Pair of Galaxies with Both Hosting X-ray Binary   Candidates at $z&#x3D;2.544$</h2><p><strong>Authors:Sijia Cai, Zheng Cai, Jianwei Lyu, Yunjing Wu, Xiaojing Lin, Mingyu Li, Junjie Mao, Jiayi Chen, Pengjun Lu</strong></p>
<p>Among high-redshift galaxies, aside from active galactic nuclei (AGNs), X-ray binaries (XRBs) can be significant sources of X-ray emission. XRBs play a crucial role in galaxy evolution, reflecting the stellar populations of galaxies and regulating star formation through feedback, thereby shaping galaxy structure. In this study, we report a spectroscopically confirmed X-ray emitting galaxy pair (UDF3 and UDF3-2) at $z &#x3D; 2.544$. By combining multi-wavelength observations from JWST&#x2F;NIRSpec MSA spectra, JWST&#x2F;NIRCam and MIRI imaging, Chandra, HST, VLT, ALMA, and VLA, we analyze the ionized emission lines, which are primarily driven by H II region-like processes. Additionally, we find that the mid-infrared radiation can be fully attributed to dust emission from galaxy themselves. Our results indicate that the X-ray emission from these two galaxies is dominated by high-mass XRBs, with luminosities of $L_X&#x3D; (1.43\pm0.40) \times 10^{42} , \text{erg} , \text{s}^{-1}$ for UDF3, and $(0.40\pm0.12) \times 10^{42} , \text{erg} , \text{s}^{-1}$ for UDF3-2. Furthermore, we measure the star formation rate (SFR) of $529_{-88}^{+64}$ $M_\odot$ yr$^{-1}$ for UDF3, placing it $\approx$ 0.5 dex below the $L_X$&#x2F;SFR-$z$ relation. This offset reflects the redshift-dependent enhancement of $L_X$&#x2F;SFR-$z$ relation, which is influenced by metallicity and serves as a key observable for XRB evolution. In contrast, UDF3-2, with the SFR of $34_{-6}^{+6}$ $M_\odot$ yr$^{-1}$, aligns well with the $L_X$&#x2F;SFR-$z$ relation. This galaxy pair represents the highest-redshift non-AGN-dominated galaxies with individual X-ray detections reported to date. This finding suggests that the contribution of XRBs to galaxy X-ray emission at high redshift may be underestimated. </p>
<blockquote>
<p>åœ¨é«˜çº¢ç§»æ˜Ÿç³»ä¸­ï¼Œé™¤äº†æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆAGNsï¼‰å¤–ï¼ŒXå°„çº¿åŒæ˜Ÿï¼ˆXRBsï¼‰ä¹Ÿå¯èƒ½æ˜¯Xå°„çº¿å‘å°„çš„é‡è¦æ¥æºã€‚XRBsåœ¨æ˜Ÿç³»æ¼”åŒ–ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œåæ˜ äº†æ˜Ÿç³»çš„æ’æ˜Ÿç§ç¾¤ï¼Œå¹¶é€šè¿‡åé¦ˆè°ƒèŠ‚æ’æ˜Ÿå½¢æˆï¼Œä»è€Œå¡‘é€ æ˜Ÿç³»ç»“æ„ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†ä¸€ä¸ªå…‰è°±è¯å®çš„é«˜çº¢ç§»Xå°„çº¿å‘å°„æ˜Ÿç³»å¯¹ï¼ˆUDF3å’ŒUDF3-2ï¼‰ï¼Œçº¢ç§»z&#x3D;2.544ã€‚æˆ‘ä»¬é€šè¿‡ç»“åˆJWST&#x2F;NIRSpec MSAå…‰è°±ã€JWST&#x2F;NIRCamå’ŒMIRIæˆåƒã€é’±å¾·æ‹‰ã€å“ˆå‹ƒæœ›è¿œé•œã€ç”šå¤§æœ›è¿œé•œã€ALMAå’ŒVLAçš„å¤šæ³¢é•¿è§‚æµ‹æ•°æ®ï¼Œåˆ†æäº†ç”µç¦»å‘å°„çº¿ï¼Œè¿™äº›å‘å°„çº¿ä¸»è¦ç”±H IIåŒºåŸŸè¿‡ç¨‹é©±åŠ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ä¸­çº¢å¤–è¾å°„å¯ä»¥å®Œå…¨å½’å› äºæ˜Ÿç³»è‡ªèº«çš„å°˜åŸƒå‘å°„ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤ä¸ªæ˜Ÿç³»çš„Xå°„çº¿å‘å°„ä¸»è¦ç”±é«˜è´¨é‡XRBsä¸»å¯¼ï¼ŒUDF3çš„Xå°„çº¿å…‰åº¦ä¸ºï¼ˆ1.43Â±0.40ï¼‰Ã—10^42 erg s^-1ï¼Œï¼ˆUDF3-2ä¸ºï¼ˆ0.4Â±0.1ï¼‰Ã—10^42 erg s^-1ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æµ‹é‡äº†UDF3çš„æ’æ˜Ÿå½¢æˆç‡ï¼ˆSFRï¼‰ä¸º529_{-88}^{+64} M_odotå¹´^-1ï¼Œå®ƒä½äºLX&#x2F;SFR-zå…³ç³»çš„ä¸‹æ–¹çº¦0.5 dexã€‚è¿™ç§åç§»åæ˜ äº†LX&#x2F;SFRä¸çº¢ç§»ä¹‹é—´çš„å…³ç³»çš„çº¢ç§»ä¾èµ–æ€§å¢å¼ºï¼Œè¿™å—åˆ°é‡‘å±ä¸°åº¦çš„å½±å“ï¼Œå¹¶æˆä¸ºXRBæ¼”åŒ–çš„å…³é”®è§‚æµ‹æŒ‡æ ‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒUDF3-2çš„æ’æ˜Ÿå½¢æˆç‡ä¸ºï¼ˆæ¯ç™¾å¹´æœ‰å‡ åäº¿æ’æ˜Ÿå½¢æˆç‡ï¼‰ï¼Œç¬¦åˆLX&#x2F;SFR-zå…³ç³»ã€‚è¿™å¯¹æ˜Ÿç³»ä»£è¡¨äº†è¿„ä»Šä¸ºæ­¢æŠ¥é“çš„æœ€é«˜çº¢ç§»éæ´»åŠ¨æ˜Ÿç³»æ ¸ä¸»å¯¼ä¸”ä¸ªä½“Xå°„çº¿æ£€æµ‹åˆ°çš„æ˜Ÿç³»ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œå¯¹é«˜çº¢ç§»æ—¶XRBå¯¹æ˜Ÿç³»Xå°„çº¿å‘å°„çš„è´¡çŒ®å¯èƒ½è¢«ä½ä¼°äº†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23230v1">PDF</a> 12 pages, 5 figures. Accepted for publication in ApJL</p>
<p><strong>Summary</strong></p>
<p>é«˜ç´…ç§»æ˜Ÿç³»ä¸­ï¼Œé™¤äº†æ´»åŠ¨æ˜Ÿç³»æ ¸ï¼ˆAGNsï¼‰å¤–ï¼ŒXå°„çº¿åŒæ˜Ÿï¼ˆXRBsï¼‰ä¹Ÿæ˜¯é‡è¦çš„Xå°„çº¿å‘å°„æºã€‚æœ¬ç ”ç©¶æŠ¥å‘Šäº†ä¸€å¯¹å…‰è°±è¯å®çš„Xå°„çº¿å‘å°„æ˜Ÿç³»UDF3å’ŒUDF3-2ï¼ˆçº¢ç§»z&#x3D;2.544ï¼‰ã€‚ç»“åˆå¤šæ³¢æ®µè§‚æµ‹æ•°æ®ï¼Œæˆ‘ä»¬åˆ†æäº†ç¦»å­åŒ–å‘å°„çº¿ï¼Œå¹¶å‘ç°ä¸­çº¢å¤–è¾å°„å¯å®Œå…¨å½’å› äºæ˜Ÿç³»è‡ªèº«çš„å°˜åŸƒå‘å°„ã€‚ç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤ä¸ªæ˜Ÿç³»çš„Xå°„çº¿å‘å°„ä¸»è¦ç”±é«˜è´¨é‡XRBsä¸»å¯¼ï¼Œå…¶ä¸­UDF3çš„Xå°„çº¿å…‰åº¦ä¸º(1.43Â±0.40)Ã—10^42 erg s^-1ï¼Œè€ŒUDF3-2çš„å…‰åº¦ä¸º(0.40Â±0.12)Ã—10^42 erg s^-1ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æµ‹é‡äº†UDF3çš„æ’æ˜Ÿå½¢æˆç‡ä¸º529Â±152 MâŠ™ yr^-1ï¼Œä½äºL_X&#x2F;SFR-zå…³ç³»çº¦0.5 dexã€‚è¿™ä¸€åç§»åæ˜ äº†L_X&#x2F;SFR-zå…³ç³»çš„çº¢ç§»ä¾èµ–æ€§å¢å¼ºï¼Œå—åˆ°é‡‘å±ä¸°åº¦çš„å½±å“ï¼Œæ˜¯XRBæ¼”åŒ–çš„å…³é”®è§‚æµ‹æŒ‡æ ‡ã€‚è€ŒUDF3-2ä¸L_X&#x2F;SFR-zå…³ç³»ç›¸ç¬¦ã€‚è¿™ä¸€æ˜Ÿç³»å¯¹ä»£è¡¨äº†è¿„ä»Šä¸ºæ­¢æŠ¥é“çš„æœ€é«˜çº¢ç§»éAGNä¸»å¯¼ä¸”å…·å¤‡ä¸ªä½“Xå°„çº¿æ£€æµ‹çš„æ˜Ÿç³»ï¼Œè¿™è¡¨æ˜é«˜çº¢ç§»æ—¶XRBså¯¹æ˜Ÿç³»Xå°„çº¿å‘å°„çš„è´¡çŒ®å¯èƒ½è¢«ä½ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>X-ray binaries (XRBs) are significant sources of X-ray emission in high-redshift galaxies, apart from active galactic nuclei (AGNs).</li>
<li>XRBs play a crucial role in galaxy evolution, reflecting stellar populations and regulating star formation through feedback.</li>
<li>A spectroscopically confirmed X-ray emitting galaxy pair (UDF3 and UDF3-2) at $z &#x3D; 2.544$ was reported, representing the highest-redshift non-AGN-dominated galaxies with individual X-ray detections.</li>
<li>The X-ray emission from UDF3 and UDF3-2 is primarily driven by high-mass XRBs, with specific X-ray luminosities measured.</li>
<li>UDF3çš„æ’æ˜Ÿå½¢æˆç‡ï¼ˆSFRï¼‰ä½äºL_X&#x2F;SFR-zå…³ç³»ï¼Œåæ˜ äº†L_X&#x2F;SFR-zå…³ç³»çš„çº¢ç§»ä¾èµ–æ€§å¢å¼ºï¼Œå—åˆ°é‡‘å±ä¸°åº¦çš„å½±å“ã€‚</li>
<li>UDF3-2çš„SFRä¸L_X&#x2F;SFR-zå…³ç³»ç›¸ç¬¦ï¼Œè¿™ä¸ºæˆ‘ä»¬ç†è§£XRBæ¼”åŒ–æä¾›äº†å…³é”®è§‚æµ‹æ•°æ®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23230">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-195ffd5d171a946bf36ed8702a6e72ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-961f7be358cccca6a05029560042de90.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a5a0d9102a05d09b2e9bb753bec236c2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d175df4238bdf4d11a7168f91475f8d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d66c010118769400731e095c032d9df.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="EMORe-Motion-Robust-5D-MRI-Reconstruction-via-Expectation-Maximization-Guided-Binning-Correction-and-Outlier-Rejection"><a href="#EMORe-Motion-Robust-5D-MRI-Reconstruction-via-Expectation-Maximization-Guided-Binning-Correction-and-Outlier-Rejection" class="headerlink" title="EMORe: Motion-Robust 5D MRI Reconstruction via   Expectation-Maximization-Guided Binning Correction and Outlier Rejection"></a>EMORe: Motion-Robust 5D MRI Reconstruction via   Expectation-Maximization-Guided Binning Correction and Outlier Rejection</h2><p><strong>Authors:Syed M. Arshad, Lee C. Potter, Yingmin Liu, Christopher Crabtree, Matthew S. Tong, Rizwan Ahmad</strong></p>
<p>We propose EMORe, an adaptive reconstruction method designed to enhance motion robustness in free-running, free-breathing self-gated 5D cardiac magnetic resonance imaging (MRI). Traditional self-gating-based motion binning for 5D MRI often results in residual motion artifacts due to inaccuracies in cardiac and respiratory signal extraction and sporadic bulk motion, compromising clinical utility. EMORe addresses these issues by integrating adaptive inter-bin correction and explicit outlier rejection within an expectation-maximization (EM) framework, whereby the E-step and M-step are executed alternately until convergence. In the E-step, probabilistic (soft) bin assignments are refined by correcting misassignment of valid data and rejecting motion-corrupted data to a dedicated outlier bin. In the M-step, the image estimate is improved using the refined soft bin assignments. Validation in a simulated 5D MRXCAT phantom demonstrated EMOReâ€™s superior performance compared to standard compressed sensing reconstruction, showing significant improvements in peak signal-to-noise ratio, structural similarity index, edge sharpness, and bin assignment accuracy across varying levels of simulated bulk motion. In vivo validation in 13 volunteers further confirmed EMOReâ€™s robustness, significantly enhancing blood-myocardium edge sharpness and reducing motion artifacts compared to compressed sensing, particularly in scenarios with controlled coughing-induced motion. Although EMORe incurs a modest increase in computational complexity, its adaptability and robust handling of bulk motion artifacts significantly enhance the clinical applicability and diagnostic confidence of 5D cardiac MRI. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºä¸€ç§åä¸ºEMOReçš„è‡ªé€‚åº”é‡å»ºæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è‡ªç”±å¥”è·‘ã€è‡ªç”±å‘¼å¸çš„è‡ªé—¨æ§5Då¿ƒè„ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­çš„è¿åŠ¨ç¨³å¥æ€§ã€‚åŸºäºè‡ªé—¨æ§çš„5D MRIçš„ä¼ ç»Ÿè¿åŠ¨åˆ†ç®±å¸¸å¸¸ç”±äºå¿ƒè„å’Œå‘¼å¸ä¿¡å·æå–çš„ä¸å‡†ç¡®ä»¥åŠå¶å‘çš„æ•´ä½“è¿åŠ¨è€Œå¯¼è‡´æ®‹ç•™è¿åŠ¨ä¼ªå½±ï¼Œä»è€Œå½±å“å…¶ä¸´åºŠåº”ç”¨ä»·å€¼ã€‚EMOReé€šè¿‡æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰æ¡†æ¶å†…çš„è‡ªé€‚åº”è·¨ç®±æ ¡æ­£å’Œæ˜¾å¼å¼‚å¸¸å€¼æ‹’ç»æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œå…¶ä¸­Eæ­¥å’ŒMæ­¥äº¤æ›¿æ‰§è¡Œç›´è‡³æ”¶æ•›ã€‚åœ¨Eæ­¥ä¸­ï¼Œé€šè¿‡ä¿®æ­£æœ‰æ•ˆæ•°æ®çš„è¯¯åˆ†é…å¹¶æ‹’ç»å°†è¿åŠ¨æŸåçš„æ•°æ®åˆ†é…ç»™ä¸“ç”¨çš„å¼‚å¸¸å€¼ç®±æ¥ä¼˜åŒ–æ¦‚ç‡ï¼ˆè½¯ï¼‰ç®±åˆ†é…ã€‚åœ¨Mæ­¥ä¸­ï¼Œä½¿ç”¨æ”¹è¿›åçš„è½¯ç®±åˆ†é…æ¥æ”¹å–„å›¾åƒä¼°è®¡ã€‚åœ¨æ¨¡æ‹Ÿçš„5DMRXCATå¹½çµä¸­çš„éªŒè¯è¡¨æ˜ï¼ŒEMOReç›¸å¯¹äºæ ‡å‡†å‹ç¼©æ„ŸçŸ¥é‡å»ºè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œåœ¨æ¨¡æ‹Ÿçš„ä¸åŒç¨‹åº¦çš„æ•´ä½“è¿åŠ¨ä¸‹ï¼Œå³°å€¼ä¿¡å™ªæ¯”ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ã€è¾¹ç¼˜é”åº¦å’Œç®±åˆ†é…ç²¾åº¦å‡å¾—åˆ°æ˜¾ç€æé«˜ã€‚åœ¨13åå¿—æ„¿è€…ä¸­çš„ä½“å†…éªŒè¯è¿›ä¸€æ­¥è¯å®äº†EMOReçš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯æ§çš„å’³å—½å¼•èµ·çš„è¿åŠ¨åœºæ™¯ä¸­ï¼Œä¸å‹ç¼©æ„ŸçŸ¥ç›¸æ¯”ï¼Œå®ƒæ˜¾è‘—æé«˜äº†è¡€æ¶²å¿ƒè‚Œè¾¹ç¼˜çš„æ¸…æ™°åº¦å¹¶å‡å°‘äº†è¿åŠ¨ä¼ªå½±ã€‚å°½ç®¡EMOReçš„è®¡ç®—å¤æ‚åº¦ç•¥æœ‰å¢åŠ ï¼Œä½†å…¶å¯¹æ•´ä½“è¿åŠ¨ä¼ªå½±çš„é€‚åº”æ€§å’Œç¨³å¥å¤„ç†æ˜¾ç€æé«˜äº†5Då¿ƒè„MRIçš„ä¸´åºŠé€‚ç”¨æ€§å’Œè¯Šæ–­ä¿¡å¿ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23224v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºEMOReçš„è‡ªé€‚åº”é‡å»ºæ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è‡ªç”±è¿è¡Œã€è‡ªç”±å‘¼å¸çš„è‡ªé—¨æ§5Då¿ƒè„ç£å…±æŒ¯æˆåƒä¸­çš„è¿åŠ¨ç¨³å¥æ€§ã€‚EMOReé€šè¿‡æ•´åˆè‡ªé€‚åº”çš„è·¨ç®±æ ¡æ­£å’Œæ˜ç¡®çš„å¼‚å¸¸å€¼æ‹’ç»åœ¨æœŸæœ›æœ€å¤§åŒ–æ¡†æ¶å†…è§£å†³ä¼ ç»Ÿè‡ªé—¨æ§ä¸ºåŸºç¡€çš„5D MRIä¸­çš„æ®‹ç•™è¿åŠ¨ä¼ªå½±é—®é¢˜ã€‚æ¨¡æ‹ŸéªŒè¯è¡¨æ˜ï¼Œä¸æ ‡å‡†å‹ç¼©æ„ŸçŸ¥é‡å»ºç›¸æ¯”ï¼ŒEMOReåœ¨å³°å€¼ä¿¡å™ªæ¯”ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°ã€è¾¹ç¼˜é”åº¦å’Œè£…ç®±åˆ†é…å‡†ç¡®æ€§æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚åœ¨å¿—æ„¿è€…ä¸­çš„å®éªŒè¿›ä¸€æ­¥è¯å®äº†EMOReçš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å’³å—½å¼•èµ·çš„è¿åŠ¨ä¸­ï¼Œä¸å‹ç¼©æ„ŸçŸ¥ç›¸æ¯”ï¼ŒEMOReæ˜¾è‘—æé«˜äº†è¡€æ¶²å¿ƒè‚Œè¾¹ç¼˜çš„é”åº¦å¹¶å‡å°‘äº†è¿åŠ¨ä¼ªå½±ã€‚è™½ç„¶EMOReçš„è®¡ç®—å¤æ‚åº¦ç•¥æœ‰å¢åŠ ï¼Œä½†å…¶å¯¹æ‰¹é‡è¿åŠ¨ä¼ªå½±çš„é€‚åº”æ€§å¤„ç†æ˜¾è‘—æé«˜äº†å…¶åœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—ä¿¡å¿ƒæ–¹é¢çš„é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EMOReæ˜¯ä¸€ç§æ—¨åœ¨æé«˜è‡ªç”±è¿è¡Œã€è‡ªç”±å‘¼å¸çš„è‡ªé—¨æ§5Då¿ƒè„ç£å…±æŒ¯æˆåƒä¸­è¿åŠ¨ç¨³å¥æ€§çš„è‡ªé€‚åº”é‡å»ºæ–¹æ³•ã€‚</li>
<li>ä¼ ç»Ÿè‡ªé—¨æ§æ–¹æ³•å¸¸å¸¸å› å¿ƒè„å’Œå‘¼å¸ä¿¡å·æå–çš„ä¸å‡†ç¡®ä»¥åŠçªå‘æ€§å¤§é‡è¿åŠ¨è€Œå¯¼è‡´æ®‹ç•™è¿åŠ¨ä¼ªå½±ã€‚</li>
<li>EMOReé€šè¿‡æœŸæœ›æœ€å¤§åŒ–æ¡†æ¶ä¸­çš„è‡ªé€‚åº”è·¨ç®±æ ¡æ­£å’Œå¼‚å¸¸å€¼æ‹’ç»æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>æ¨¡æ‹ŸéªŒè¯æ˜¾ç¤ºï¼Œä¸æ ‡å‡†å‹ç¼©æ„ŸçŸ¥é‡å»ºç›¸æ¯”ï¼ŒEMOReåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚</li>
<li>åœ¨å¿—æ„¿è€…ä¸­çš„å®éªŒè¿›ä¸€æ­¥è¯å®äº†EMOReåœ¨å‡å°‘è¿åŠ¨ä¼ªå½±å’Œæé«˜å›¾åƒè´¨é‡æ–¹é¢çš„ç¨³å¥æ€§ã€‚</li>
<li>EMOReå¯¹æ‰¹é‡è¿åŠ¨ä¼ªå½±çš„é€‚åº”æ€§å¤„ç†æ˜¾è‘—æé«˜äº†å…¶åœ¨ä¸´åºŠè¯Šæ–­å’Œæ²»ç–—ä¿¡å¿ƒæ–¹é¢çš„é€‚ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23224">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2cf23a3ab2c9704169323aa888e955f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29f28349ce836248f287eb834c445e72.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39fce45ef7fe8e335770c0a24a350e1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c96f5e8134d31ced72be465ef2eaf4b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa4d5801d972a3b0dc955527a390ff8.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MRpro-open-PyTorch-based-MR-reconstruction-and-processing-package"><a href="#MRpro-open-PyTorch-based-MR-reconstruction-and-processing-package" class="headerlink" title="MRpro - open PyTorch-based MR reconstruction and processing package"></a>MRpro - open PyTorch-based MR reconstruction and processing package</h2><p><strong>Authors:Felix Frederik Zimmermann, Patrick Schuenke, Christoph S. Aigner, Bill A. Bernhardt, Mara Guastini, Johannes Hammacher, Noah Jaitner, Andreas Kofler, Leonid Lunin, Stefan Martin, Catarina Redshaw Kranich, Jakob Schattenfroh, David Schote, Yanglei Wu, Christoph Kolbitsch</strong></p>
<p>We introduce MRpro, an open-source image reconstruction package built upon PyTorch and open data formats. The framework comprises three main areas. First, it provides unified data structures for the consistent manipulation of MR datasets and their associated metadata (e.g., k-space trajectories). Second, it offers a library of composable operators, proximable functionals, and optimization algorithms, including a unified Fourier operator for all common trajectories and an extended phase graph simulation for quantitative MR. These components are used to create ready-to-use implementations of key reconstruction algorithms. Third, for deep learning, MRpro includes essential building blocks such as data consistency layers, differentiable optimization layers, and state-of-the-art backbone networks and integrates public datasets to facilitate reproducibility. MRpro is developed as a collaborative project supported by automated quality control. We demonstrate the versatility of MRpro across multiple applications, including Cartesian, radial, and spiral acquisitions; motion-corrected reconstruction; cardiac MR fingerprinting; learned spatially adaptive regularization weights; model-based learned image reconstruction and quantitative parameter estimation. MRpro offers an extensible framework for MR image reconstruction. With reproducibility and maintainability at its core, it facilitates collaborative development and provides a foundation for future MR imaging research. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†MRproï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºPyTorchå’Œå¼€æ”¾æ•°æ®æ ¼å¼çš„å¼€æºå›¾åƒé‡å»ºè½¯ä»¶åŒ…ã€‚è¯¥æ¡†æ¶ä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªé¢†åŸŸã€‚é¦–å…ˆï¼Œå®ƒä¸ºMRæ•°æ®é›†åŠå…¶ç›¸å…³å…ƒæ•°æ®ï¼ˆä¾‹å¦‚kç©ºé—´è½¨è¿¹ï¼‰æä¾›ç»Ÿä¸€çš„æ•°æ®ç»“æ„ï¼Œä»¥ä¾¿è¿›è¡Œä¸€è‡´çš„æ“ä½œã€‚å…¶æ¬¡ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªå¯ç»„åˆçš„è¿ç®—ç¬¦åº“ï¼ŒåŒ…æ‹¬å¯ç”¨äºæ‰€æœ‰å¸¸è§è½¨è¿¹çš„ç»Ÿä¸€å‚…é‡Œå¶è¿ç®—ç¬¦å’Œç”¨äºå®šé‡MRçš„æ‰©å±•ç›¸ä½å›¾æ¨¡æ‹Ÿç­‰å¯æ¥è¿‘çš„åŠŸèƒ½å’Œä¼˜åŒ–ç®—æ³•ã€‚è¿™äº›ç»„ä»¶ç”¨äºåˆ›å»ºå³æ’å³ç”¨çš„å…³é”®é‡å»ºç®—æ³•çš„å®ç°ã€‚ç¬¬ä¸‰ï¼Œå¯¹äºæ·±åº¦å­¦ä¹ ï¼ŒMRproåŒ…æ‹¬å¿…è¦çš„åŸºæœ¬ç»„ä»¶ï¼Œå¦‚æ•°æ®ä¸€è‡´æ€§å±‚ã€å¯ä¼˜åŒ–å±‚ä»¥åŠæœ€å…ˆè¿›çš„éª¨å¹²ç½‘ç»œï¼Œå¹¶æ•´åˆå…¬å…±æ•°æ®é›†ä»¥ä¿ƒè¿›å¯é‡å¤æ€§ã€‚MRproæ˜¯ä¸€ä¸ªåä½œé¡¹ç›®ï¼Œå—åˆ°è‡ªåŠ¨æ§åˆ¶è´¨é‡æ”¯æŒã€‚æˆ‘ä»¬å±•ç¤ºäº†MRproåœ¨å¤šä¸ªåº”ç”¨ä¸­çš„é€šç”¨æ€§ï¼ŒåŒ…æ‹¬ç¬›å¡å°”ã€å¾„å‘å’Œèºæ—‹é‡‡é›†ï¼›è¿åŠ¨æ ¡æ­£é‡å»ºï¼›å¿ƒè„MRæŒ‡çº¹ï¼›å­¦ä¹ ç©ºé—´è‡ªé€‚åº”æ­£åˆ™åŒ–æƒé‡ï¼›åŸºäºæ¨¡å‹çš„å›¾åƒé‡å»ºå’Œå®šé‡å‚æ•°ä¼°è®¡ç­‰ã€‚MRproä¸ºMRå›¾åƒé‡å»ºæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ã€‚ä»¥å¯é‡å¤æ€§å’Œå¯ç»´æŠ¤æ€§ä¸ºæ ¸å¿ƒï¼Œå®ƒä¿ƒè¿›äº†åä½œå¼€å‘å¹¶ä¸ºæœªæ¥çš„MRæˆåƒç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23129v1">PDF</a> Submitted to Magnetic Resonance in Medicine</p>
<p><strong>Summary</strong></p>
<p>MRproæ˜¯ä¸€ä¸ªå¼€æºçš„ç£å…±æŒ¯å›¾åƒé‡å»ºè½¯ä»¶åŒ…ï¼ŒåŸºäºPyTorchå’Œå¼€æ”¾æ•°æ®æ ¼å¼æ„å»ºã€‚å®ƒä¸»è¦åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼šç»Ÿä¸€çš„æ•°æ®ç»“æ„ã€å¯ç»„åˆçš„è¿ç®—ç¬¦åº“å’Œä¼˜åŒ–ç®—æ³•ï¼Œä»¥åŠç”¨äºæ·±åº¦å­¦ä¹ çš„å…³é”®æ„å»ºæ¨¡å—ã€‚MRproåœ¨å¤šä¸ªåº”ç”¨ä¸­è¡¨ç°å‡ºå…¶é€šç”¨æ€§ï¼ŒåŒ…æ‹¬ç¬›å¡å°”ã€å¾„å‘å’Œèºæ—‹é‡‡é›†ã€è¿åŠ¨æ ¡æ­£é‡å»ºã€å¿ƒè„ç£å…±æŒ¯æŒ‡çº¹ã€å­¦ä¹ ç©ºé—´è‡ªé€‚åº”æ­£åˆ™åŒ–æƒé‡ä»¥åŠåŸºäºæ¨¡å‹çš„å›¾åƒé‡å»ºå’Œå®šé‡å‚æ•°ä¼°è®¡ã€‚MRproä¸ºç£å…±æŒ¯å›¾åƒé‡å»ºæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œæ³¨é‡å¯é‡å¤æ€§å’Œå¯ç»´æŠ¤æ€§ï¼Œä¿ƒè¿›äº†åä½œå¼€å‘å¹¶ä¸ºæœªæ¥çš„ç£å…±æŒ¯æˆåƒç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MRproæ˜¯ä¸€ä¸ªåŸºäºPyTorchå’Œå¼€æ”¾æ•°æ®æ ¼å¼çš„å¼€æºç£å…±æŒ¯å›¾åƒé‡å»ºè½¯ä»¶åŒ…ã€‚</li>
<li>å®ƒæä¾›äº†ç»Ÿä¸€çš„æ•°æ®ç»“æ„ï¼Œç”¨äºæ“ä½œMRæ•°æ®é›†åŠå…¶ç›¸å…³å…ƒæ•°æ®ã€‚</li>
<li>MRproåŒ…å«å¯ç»„åˆçš„è¿ç®—ç¬¦åº“ã€é‚»è¿‘åŠŸèƒ½å‡½æ•°å’Œä¼˜åŒ–ç®—æ³•ã€‚</li>
<li>æä¾›äº†ç”¨äºæ·±åº¦å­¦ä¹ çš„å…³é”®æ„å»ºæ¨¡å—ï¼Œå¦‚æ•°æ®ä¸€è‡´æ€§å±‚ã€å¯ä¼˜åŒ–å·®åˆ†å±‚ä»¥åŠæœ€æ–°éª¨å¹²ç½‘ç»œã€‚</li>
<li>MRproé›†æˆäº†å…¬å…±æ•°æ®é›†ï¼Œä¿ƒè¿›äº†å¯é‡å¤æ€§ã€‚</li>
<li>MRproåœ¨å¤šç§åº”ç”¨ä¸­è¡¨ç°å‡ºå…¶å¤šåŠŸèƒ½æ€§ï¼ŒåŒ…æ‹¬ä¸åŒé‡‡é›†æ–¹å¼ã€è¿åŠ¨æ ¡æ­£é‡å»ºã€å¿ƒè„ç£å…±æŒ¯æŒ‡çº¹ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23129">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-dec307d4551524b8abe85c5e382f8838.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2f51ef8c0bba29bb5fc1dd13b2e35e0b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8a4315b119ebc55abe557becb1352268.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ad1bb97fc007ab4cd4b8cbacd330aa27.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Rethink-Domain-Generalization-in-Heterogeneous-Sequence-MRI-Segmentation"><a href="#Rethink-Domain-Generalization-in-Heterogeneous-Sequence-MRI-Segmentation" class="headerlink" title="Rethink Domain Generalization in Heterogeneous Sequence MRI Segmentation"></a>Rethink Domain Generalization in Heterogeneous Sequence MRI Segmentation</h2><p><strong>Authors:Zheyuan Zhang, Linkai Peng, Wanying Dou, Cuiling Sun, Halil Ertugrul Aktas, Andrea M. Bejar, Elif Keles, Gorkem Durak, Ulas Bagci</strong></p>
<p>Clinical magnetic-resonance (MR) protocols generate many T1 and T2 sequences whose appearance differs more than the acquisition sites that produce them. Existing domain-generalization benchmarks focus almost on cross-center shifts and overlook this dominant source of variability. Pancreas segmentation remains a major challenge in abdominal imaging: the gland is small, irregularly, surrounded by organs and fat, and often suffers from low T1 contrast. State-of-the-art deep networks that already achieve &gt;90% Dice on the liver or kidneys still miss 20-30% of the pancreas. The organ is also systematically under-represented in public cross-domain benchmarks, despite its clinical importance in early cancer detection, surgery, and diabetes research. To close this gap, we present PancreasDG, a large-scale multi-center 3D MRI pancreas segmentation dataset for investigating domain generalization in medical imaging. The dataset comprises 563 MRI scans from six institutions, spanning both venous phase and out-of-phase sequences, enabling study of both cross-center and cross-sequence variations with pixel-accurate pancreas masks created by a double-blind, two-pass protocol. Through comprehensive analysis, we reveal three insights: (i) limited sampling introduces significant variance that may be mistaken for distribution shifts, (ii) cross-center performance correlates with source domain performance for identical sequences, and (iii) cross-sequence shifts require specialized solutions. We also propose a semi-supervised approach that leverages anatomical invariances, significantly outperforming state-of-the-art domain generalization techniques with 61.63% Dice score improvements and 87.00% on two test centers for cross-sequence segmentation. PancreasDG sets a new benchmark for domain generalization in medical imaging. Dataset, code, and models will be available at <a target="_blank" rel="noopener" href="https://pancreasdg.netlify.app/">https://pancreasdg.netlify.app</a>. </p>
<blockquote>
<p>ä¸´åºŠç£å…±æŒ¯ï¼ˆMRï¼‰åè®®ä¼šäº§ç”Ÿè®¸å¤šT1å’ŒT2åºåˆ—ï¼Œè¿™äº›åºåˆ—çš„è¡¨ç°åœ¨äº§ç”Ÿå®ƒä»¬çš„é‡‡é›†éƒ¨ä½ä¹‹é—´æœ‰ç€å·¨å¤§å·®å¼‚ã€‚ç°æœ‰çš„åŸŸæ³›åŒ–åŸºå‡†æµ‹è¯•å‡ ä¹é›†ä¸­åœ¨è·¨ä¸­å¿ƒè½¬ç§»ä¸Šï¼Œè€Œå¿½ç•¥äº†è¿™ä¸€ä¸»è¦çš„å˜é‡æ¥æºã€‚èƒ°è…ºåˆ†å‰²åœ¨è…¹éƒ¨æˆåƒä¸­ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼šèƒ°è…ºä½“ç§¯å°ã€å½¢æ€ä¸è§„åˆ™ã€è¢«å™¨å®˜å’Œè„‚è‚ªåŒ…å›´ï¼Œå¹¶ä¸”T1å¯¹æ¯”åº¦é€šå¸¸è¾ƒä½ã€‚æœ€å…ˆè¿›çš„æ·±åº¦ç½‘ç»œåœ¨è‚è„æˆ–è‚¾è„ä¸Šå·²ç»è¾¾åˆ°&gt;90%çš„Diceç³»æ•°ï¼Œä½†ä»ç„¶ä¼šé”™è¿‡20-30%çš„èƒ°è…ºã€‚å°½ç®¡å…¶åœ¨æ—©æœŸç™Œç—‡æ£€æµ‹ã€æ‰‹æœ¯å’Œç³–å°¿ç—…ç ”ç©¶ä¸­å…·æœ‰é‡è¦çš„ä¸´åºŠæ„ä¹‰ï¼Œä½†è¯¥å™¨å®˜åœ¨å…¬å…±è·¨åŸŸåŸºå‡†æµ‹è¯•ä¸­å´è¢«ç³»ç»Ÿæ€§åœ°ä½ä¼°äº†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†PancreasDGï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç ”ç©¶åŒ»å­¦æˆåƒé¢†åŸŸåŸŸæ³›åŒ–çš„å¤§è§„æ¨¡å¤šä¸­å¿ƒ3D MRIèƒ°è…ºåˆ†å‰²æ•°æ®é›†ã€‚æ•°æ®é›†åŒ…å«æ¥è‡ªå…­ä¸ªæœºæ„çš„563ä¸ªMRIæ‰«æï¼Œæ—¢åŒ…æ‹¬é™è„‰ç›¸ä½ä¹ŸåŒ…æ‹¬å¼‚ä½åºåˆ—ï¼Œèƒ½å¤Ÿç ”ç©¶è·¨ä¸­å¿ƒå’Œè·¨åºåˆ—å˜åŒ–ï¼Œé€šè¿‡åŒé‡ç›²æ³•çš„ä¸¤é˜¶æ®µåè®®åˆ›å»ºåƒç´ ç²¾ç¡®çš„èƒ°è…ºæ©è†œã€‚é€šè¿‡ç»¼åˆåˆ†æï¼Œæˆ‘ä»¬æ­ç¤ºäº†ä¸‰ä¸ªè§è§£ï¼šï¼ˆiï¼‰æœ‰é™é‡‡æ ·å¼•å…¥çš„é‡å¤§å˜åŒ–å¯èƒ½è¢«è¯¯è®¤ä¸ºåˆ†å¸ƒè½¬ç§»ï¼›ï¼ˆiiï¼‰è·¨ä¸­å¿ƒæ€§èƒ½ä¸ç›¸åŒåºåˆ—çš„æºåŸŸæ€§èƒ½ç›¸å…³ï¼›ï¼ˆiiiï¼‰è·¨åºåˆ—è½¬ç§»éœ€è¦ä¸“é—¨çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åˆ©ç”¨è§£å‰–ä¸å˜æ€§çš„åŠç›‘ç£æ–¹æ³•ï¼Œåœ¨è·¨åºåˆ—åˆ†å‰²æ–¹é¢æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„åŸŸæ³›åŒ–æŠ€æœ¯ï¼ŒDiceå¾—åˆ†æé«˜äº†61.63%ï¼Œåœ¨ä¸¤ä¸ªæµ‹è¯•ä¸­å¿ƒçš„å¾—åˆ†è¾¾åˆ°äº†87.00%ã€‚PancreasDGä¸ºåŒ»å­¦æˆåƒé¢†åŸŸä¸­çš„åŸŸæ³›åŒ–è®¾å®šäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ã€‚æ•°æ®é›†ã€ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://pancreasdg.netlify.appä¸Šæä¾›./">https://pancreasdg.netlify.appä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23110v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸´åºŠç£å…±æŒ¯ï¼ˆMRï¼‰åè®®äº§ç”Ÿçš„T1å’ŒT2åºåˆ—åœ¨é‡‡é›†ä½ç‚¹ä¸Šçš„å·®å¼‚è¢«å¿½ç•¥çš„é—®é¢˜ã€‚èƒ°è…ºåˆ†å‰²åœ¨è…¹éƒ¨æˆåƒä¸­ä»ç„¶æ˜¯ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œå› ä¸ºèƒ°è…ºè¾ƒå°ã€å½¢æ€ä¸è§„åˆ™ï¼Œå¹¶ä¸”ä¸å™¨å®˜å’Œè„‚è‚ªç›¸é‚»ï¼Œåœ¨T1å¯¹æ¯”ä¸‹ç»å¸¸ä¸æ˜æ˜¾ã€‚å°½ç®¡åœ¨è‚è„æˆ–è‚¾è„ä¸Šå·²ç»è¾¾åˆ°&gt;90%çš„Diceç³»æ•°ï¼Œä½†æœ€å…ˆè¿›çš„æ·±åº¦ç½‘ç»œä»ç„¶ä¼šé”™è¿‡20-30%çš„èƒ°è…ºã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæå‡ºäº†PancreasDGæ•°æ®é›†ï¼Œç”¨äºç ”ç©¶åŒ»å­¦æˆåƒä¸­çš„é¢†åŸŸæ³›åŒ–é—®é¢˜ã€‚è¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªå…­ä¸ªæœºæ„çš„563ä¸ªMRIæ‰«æï¼Œæ¶µç›–äº†é™è„‰ç›¸å’Œå¼‚ç›¸åºåˆ—ï¼Œå¯é€šè¿‡åŒé‡ç›²æµ‹ã€ä¸¤é˜¶æ®µåè®®åˆ›å»ºåƒç´ ç²¾ç¡®çš„èƒ°è…ºæ©è†œï¼Œç”¨äºç ”ç©¶è·¨ä¸­å¿ƒå’Œè·¨åºåˆ—å˜åŒ–ã€‚ç»¼åˆåˆ†ææ­ç¤ºäº†ä¸‰ä¸ªè§è§£ï¼Œå¹¶æå‡ºäº†åˆ©ç”¨åŠç›‘ç£æ–¹æ³•çš„ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è§£å‰–ä¸å˜æ€§ï¼Œæ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„é¢†åŸŸæ³›åŒ–æŠ€æœ¯ã€‚PancreasDGä¸ºåŒ»å­¦æˆåƒé¢†åŸŸæ³›åŒ–è®¾å®šäº†ä¸€ä¸ªæ–°çš„åŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸´åºŠMRåè®®äº§ç”Ÿçš„T1å’ŒT2åºåˆ—åœ¨å¤–è§‚ä¸Šçš„å·®å¼‚å¤§äºé‡‡é›†ä½ç‚¹å·®å¼‚ï¼Œè¢«å¿½è§†ä¸ºä¸»è¦çš„å˜é‡æ¥æºã€‚</li>
<li>èƒ°è…ºåˆ†å‰²æ˜¯è…¹éƒ¨æˆåƒçš„ä¸»è¦æŒ‘æˆ˜ï¼Œå› ä¸ºèƒ°è…ºè¾ƒå°ã€å½¢æ€ä¸è§„åˆ™å¹¶ä¸”å‘¨å›´æœ‰è®¸å¤šå™¨å®˜å’Œè„‚è‚ªã€‚</li>
<li>æœ€å…ˆè¿›çš„æ·±åº¦ç½‘ç»œåœ¨èƒ°è…ºåˆ†å‰²ä¸Šä»æœ‰å¾ˆå¤§æå‡ç©ºé—´ï¼Œç›¸è¾ƒäºè‚è„æˆ–è‚¾è„çš„åˆ†å‰²ï¼Œæ€§èƒ½ä¸‹é™æ˜æ˜¾ã€‚</li>
<li>PancreasDGæ•°æ®é›†æ˜¯ä¸€ä¸ªç”¨äºç ”ç©¶åŒ»å­¦æˆåƒé¢†åŸŸæ³›åŒ–çš„å¤§å‹å¤šä¸­å¿ƒ3DMRIèƒ°è…ºåˆ†å‰²æ•°æ®é›†ã€‚</li>
<li>PancreasDGåŒ…å«å¤šç§MRIæ‰«æåºåˆ—å’Œæ¥è‡ªå¤šä¸ªæœºæ„çš„æ•°æ®ï¼Œèƒ½å¤Ÿç ”ç©¶è·¨ä¸­å¿ƒå’Œè·¨åºåˆ—çš„å˜åŒ–ã€‚</li>
<li>é€šè¿‡å¯¹æ•°æ®çš„ç»¼åˆåˆ†æï¼Œæ­ç¤ºäº†é‡‡æ ·é™åˆ¶å¯¹ç ”ç©¶ç»“æœçš„å½±å“ã€è·¨ä¸­å¿ƒä¸è·¨åºåˆ—çš„æ€§èƒ½å·®å¼‚åŠç‰¹å®šè§£å†³æ–¹æ¡ˆçš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23110">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a868e76a55271d833ec77af464b6fdad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b512c3ce1d848d4cbecadd846b2a2f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cb0c3da2acb5ec43eb4440ac0c04277.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae6cad9e268d8f40dd3f359502113ed5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03e9f90eda2dff7c7b9f4f820c96d697.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce632176df686a5af0478d21fb65875d.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Viscoelastic-Profiling-of-Rare-Pediatric-Extracranial-Tumors-using-Multifrequency-MR-Elastography-A-Pilot-Study"><a href="#Viscoelastic-Profiling-of-Rare-Pediatric-Extracranial-Tumors-using-Multifrequency-MR-Elastography-A-Pilot-Study" class="headerlink" title="Viscoelastic Profiling of Rare Pediatric Extracranial Tumors using   Multifrequency MR Elastography: A Pilot Study"></a>Viscoelastic Profiling of Rare Pediatric Extracranial Tumors using   Multifrequency MR Elastography: A Pilot Study</h2><p><strong>Authors:C. Metz, S. Veldhoen, H. E. Deubzer, F. Mollica, T. Meyer, K. Hauptmann, A. H. Hagemann, A. Eggert, I. Sack, M. S. Anders</strong></p>
<p>Objectives: Magnetic resonance elastography (MRE) is a noninvasive technique for assessing the viscoelastic properties of soft biological tissues in vivo, with potential relevance for pediatric tumor evaluation. This study aimed to evaluate the feasibility of multifrequency MRE in children with solid tumors and to report initial findings on stiffness and fluidity across rare pediatric tumor entities. Additionally, the potential of viscoelastic properties as biomarkers of tumor malignancy was explored. Materials and Methods: Ten pediatric patients (mean age, 5.7 +&#x2F;- 4.8 years; four female) with extracranial solid tumors underwent multifrequency MRE. Shear waves at 30 - 70 Hz were subsequently generated and measured with a phase-sensitive single-shot spin-echo planar imaging sequence. The obtained shear wave fields were processed by wavenumber (k-)based multi-frequency inversion to reconstruct tumor stiffness and fluidity. The viscoelastic properties within the tumors were quantified and correlated with the apparent diffusion coefficient (ADC). In addition, differences in stiffness and fluidity were assessed across the histopathologically confirmed tumor entities, which were stratified into malignancy-based groups. Results: MRE was successfully performed in all patients in under five minutes. Differences in viscoelastic properties were observed among tumor entities: Stiffness, fluidity, and their spatial variability increased significantly with tumor malignancy. Furthermore, a significant inverse correlation was observed between stiffness and tumor ADC values. Conclusion: Multifrequency MRE was feasible in pediatric MRI and provided insight into tumor biomechanics. Preliminary data revealed differences in stiffness and fluidity across pediatric solid tumors correlating with malignancy. MRE holds promise for diagnosis and classification of pediatric tumor entities and their malignancy. </p>
<blockquote>
<p>ç›®çš„ï¼šç£å…±æŒ¯å¼¹æ€§æˆåƒï¼ˆMREï¼‰æ˜¯ä¸€ç§éä¾µå…¥æ€§çš„ä½“å†…è¯„ä¼°è½¯ç”Ÿç‰©ç»„ç»‡ç²˜å¼¹æ€§è´¨çš„æŠ€æœ¯ï¼Œå¯¹å„¿ç§‘è‚¿ç˜¤è¯„ä¼°å…·æœ‰æ½œåœ¨æ„ä¹‰ã€‚æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°åœ¨å„¿ç«¥å®ä½“ç˜¤æ‚£è€…ä¸­åº”ç”¨å¤šé¢‘MREçš„å¯è¡Œæ€§ï¼Œå¹¶æŠ¥å‘Šå…³äºä¸åŒç½•è§å„¿ç§‘è‚¿ç˜¤å®ä½“çš„åƒµç¡¬åº¦åŠæµåŠ¨æ€§çš„åˆæ­¥å‘ç°ã€‚æ­¤å¤–ï¼Œè¿˜æ¢è®¨äº†ç²˜å¼¹æ€§è´¨ä½œä¸ºè‚¿ç˜¤æ¶æ€§ç”Ÿç‰©æ ‡å¿—ç‰©çš„æ½œåŠ›ã€‚ææ–™ä¸æ–¹æ³•ï¼š10åï¼ˆå¹³å‡å¹´é¾„5.7Â±4.8å²ï¼Œ4åå¥³æ€§ï¼‰æ‚£æœ‰é¢…å¤–å®ä½“ç˜¤çš„å„¿ç§‘æ‚£è€…æ¥å—äº†å¤šé¢‘MREæ£€æŸ¥ã€‚éšåç”Ÿæˆå¹¶æµ‹é‡äº†30-70 Hzçš„å‰ªåˆ‡æ³¢ï¼Œé‡‡ç”¨ç›¸ä½æ•æ„Ÿçš„å•è„‰å†²è‡ªæ—‹å›æ³¢å¹³é¢æˆåƒåºåˆ—è¿›è¡Œæµ‹é‡ã€‚æ‰€è·å¾—çš„å‰ªåˆ‡æ³¢åœºé€šè¿‡åŸºäºæ³¢æ•°ï¼ˆkï¼‰çš„å¤šé¢‘åæ¼”è¿›è¡Œå¤„ç†ï¼Œä»¥é‡å»ºè‚¿ç˜¤åƒµç¡¬åº¦å’ŒæµåŠ¨æ€§ã€‚å¯¹è‚¿ç˜¤å†…éƒ¨çš„ç²˜å¼¹æ€§è´¨è¿›è¡Œå®šé‡ï¼Œå¹¶ä¸è¡¨è§‚æ‰©æ•£ç³»æ•°ï¼ˆADCï¼‰ç›¸å…³ã€‚æ­¤å¤–ï¼Œè¿˜æ ¹æ®ç—…ç†ç»„ç»‡å­¦ç¡®è®¤çš„è‚¿ç˜¤å®ä½“ï¼Œå°†å…¶åˆ†å±‚åˆ°åŸºäºæ¶æ€§çš„ç»„åˆ«ä¸­ï¼Œè¯„ä¼°åƒµç¡¬åº¦å’ŒæµåŠ¨æ€§çš„å·®å¼‚ã€‚ç»“æœï¼šæ‰€æœ‰æ‚£è€…å‡åœ¨äº”åˆ†é’Ÿå†…æˆåŠŸå®ŒæˆMREæ£€æŸ¥ã€‚åœ¨è‚¿ç˜¤å®ä½“ä¹‹é—´è§‚å¯Ÿåˆ°ç²˜å¼¹æ€§è´¨çš„ä¸åŒï¼šåƒµç¡¬åº¦ã€æµåŠ¨æ€§åŠå…¶ç©ºé—´å˜åŒ–ç‡éšè‚¿ç˜¤çš„æ¶æ€§ç¨‹åº¦å¢åŠ è€Œæ˜¾è‘—å¢åŠ ã€‚æ­¤å¤–ï¼Œåƒµç¡¬åº¦å’Œè‚¿ç˜¤ADCå€¼ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„è´Ÿç›¸å…³ã€‚ç»“è®ºï¼šå¤šé¢‘MREåœ¨å„¿ç§‘MRIä¸­æ˜¯å¯è¡Œçš„ï¼Œä¸ºè‚¿ç˜¤çš„ç”Ÿç‰©åŠ›å­¦æä¾›äº†è§è§£ã€‚åˆæ­¥æ•°æ®æ˜¾ç¤ºï¼Œå„¿ç§‘å®ä½“ç˜¤çš„åƒµç¡¬åº¦å’ŒæµåŠ¨æ€§ä¸æ¶æ€§ç¨‹åº¦ç›¸å…³ã€‚MREåœ¨è¯Šæ–­å’Œåˆ†ç±»å„¿ç§‘è‚¿ç˜¤å®ä½“åŠå…¶æ¶æ€§ç¨‹åº¦æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22657v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°åœ¨å„¿ç«¥å®ä½“ç˜¤æ‚£è€…ä¸­åº”ç”¨å¤šé¢‘ç£å…±æŒ¯å¼¹æ€§æˆåƒï¼ˆMREï¼‰çš„å¯è¡Œæ€§ï¼Œå¹¶æŠ¥å‘Šä¸åŒç½•è§å„¿ç«¥è‚¿ç˜¤å®ä½“çš„åˆæ­¥åˆšåº¦ä¸æµåŠ¨æ€§å‘ç°ã€‚æ­¤å¤–ï¼Œè¿˜æ¢è®¨äº†ç²˜å¼¹æ€§ç‰¹æ€§ä½œä¸ºè‚¿ç˜¤æ¶æ€§ç”Ÿç‰©æ ‡å¿—ç‰©çš„æ½œåŠ›ã€‚å¯¹æ‰€æœ‰æ‚£è€…è¿›è¡ŒMREæ£€æŸ¥å‡æˆåŠŸï¼Œæ£€æŸ¥æ—¶é—´ä¸åˆ°äº”åˆ†é’Ÿã€‚ä¸åŒè‚¿ç˜¤å®ä½“çš„ç²˜å¼¹æ€§ç‰¹æ€§å­˜åœ¨å·®å¼‚ï¼Œä¸”éšç€è‚¿ç˜¤æ¶æ€§ç¨‹åº¦å¢é«˜ï¼Œåˆšåº¦å’ŒæµåŠ¨æ€§åŠå…¶ç©ºé—´å˜åŒ–æ˜¾è‘—å¢åŠ ã€‚åŒæ—¶ï¼Œè‚¿ç˜¤åˆšåº¦ä¸è¡¨è§‚æ‰©æ•£ç³»æ•°ï¼ˆADCï¼‰ä¹‹é—´å‘ˆæ˜¾è‘—è´Ÿç›¸å…³ã€‚å¤šé¢‘MREåœ¨å„¿ç«¥ç£å…±æŒ¯æˆåƒä¸­æ˜¯å¯è¡Œçš„ï¼Œä¸ºè‚¿ç˜¤ç”Ÿç‰©åŠ›å­¦æä¾›äº†è§è§£ã€‚åˆæ­¥æ•°æ®æ˜¾ç¤ºï¼Œå„¿ç«¥å®ä½“ç˜¤çš„åˆšåº¦å’ŒæµåŠ¨æ€§ä¸æ¶æ€§ç¨‹åº¦ç›¸å…³ã€‚MREåœ¨è¯Šæ–­å’Œåˆ†ç±»å„¿ç«¥è‚¿ç˜¤å®ä½“åŠå…¶æ¶æ€§ç¨‹åº¦æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤šé¢‘ç£å…±æŒ¯å¼¹æ€§æˆåƒï¼ˆMREï¼‰æ˜¯ä¸€ç§éä¾µå…¥æ€§æŠ€æœ¯ï¼Œå¯ç”¨äºè¯„ä¼°ä½“å†…è½¯ç”Ÿç‰©ç»„ç»‡çš„ç²˜å¼¹æ€§ç‰¹æ€§ï¼Œå¯¹å„¿ç§‘è‚¿ç˜¤è¯„ä¼°å…·æœ‰æ½œåœ¨æ„ä¹‰ã€‚</li>
<li>MREåœ¨å„¿ç«¥å®ä½“ç˜¤æ‚£è€…ä¸­åº”ç”¨å…·æœ‰å¯è¡Œæ€§ï¼Œä¸”æ£€æŸ¥æ—¶é—´çŸ­ã€‚</li>
<li>ä¸åŒå„¿ç§‘å®ä½“è‚¿ç˜¤çš„ç²˜å¼¹æ€§ç‰¹æ€§å­˜åœ¨å·®å¼‚ã€‚</li>
<li>è‚¿ç˜¤çš„åˆšåº¦å’ŒæµåŠ¨æ€§ä¸æ¶æ€§ç¨‹åº¦å‘ˆæ­£ç›¸å…³ã€‚</li>
<li>è‚¿ç˜¤åˆšåº¦ä¸è¡¨è§‚æ‰©æ•£ç³»æ•°ï¼ˆADCï¼‰ä¹‹é—´å‘ˆè´Ÿç›¸å…³ã€‚</li>
<li>MREæŠ€æœ¯ä¸ºç†è§£è‚¿ç˜¤ç”Ÿç‰©åŠ›å­¦æä¾›äº†æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22657">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-804905dc416522ce0f13c0027cde0c62.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Bridging-the-Gap-in-Missing-Modalities-Leveraging-Knowledge-Distillation-and-Style-Matching-for-Brain-Tumor-Segmentation"><a href="#Bridging-the-Gap-in-Missing-Modalities-Leveraging-Knowledge-Distillation-and-Style-Matching-for-Brain-Tumor-Segmentation" class="headerlink" title="Bridging the Gap in Missing Modalities: Leveraging Knowledge   Distillation and Style Matching for Brain Tumor Segmentation"></a>Bridging the Gap in Missing Modalities: Leveraging Knowledge   Distillation and Style Matching for Brain Tumor Segmentation</h2><p><strong>Authors:Shenghao Zhu, Yifei Chen, Weihong Chen, Yuanhan Wang, Chang Liu, Shuo Jiang, Feiwei Qin, Changmiao Wang</strong></p>
<p>Accurate and reliable brain tumor segmentation, particularly when dealing with missing modalities, remains a critical challenge in medical image analysis. Previous studies have not fully resolved the challenges of tumor boundary segmentation insensitivity and feature transfer in the absence of key imaging modalities. In this study, we introduce MST-KDNet, aimed at addressing these critical issues. Our model features Multi-Scale Transformer Knowledge Distillation to effectively capture attention weights at various resolutions, Dual-Mode Logit Distillation to improve the transfer of knowledge, and a Global Style Matching Module that integrates feature matching with adversarial learning. Comprehensive experiments conducted on the BraTS and FeTS 2024 datasets demonstrate that MST-KDNet surpasses current leading methods in both Dice and HD95 scores, particularly in conditions with substantial modality loss. Our approach shows exceptional robustness and generalization potential, making it a promising candidate for real-world clinical applications. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/Quanato607/MST-KDNet">https://github.com/Quanato607/MST-KDNet</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­ï¼Œå‡†ç¡®å¯é çš„è„‘è‚¿ç˜¤åˆ†å‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç¼ºå¤±æ¨¡æ€æ—¶ï¼Œä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚ä¹‹å‰çš„ç ”ç©¶å°šæœªå®Œå…¨è§£å†³è‚¿ç˜¤è¾¹ç•Œåˆ†å‰²æ•æ„Ÿæ€§å’Œåœ¨ç¼ºå°‘å…³é”®æˆåƒæ¨¡æ€æ—¶çš„ç‰¹å¾è½¬ç§»çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†MST-KDNetï¼Œæ—¨åœ¨è§£å†³è¿™äº›å…³é”®é—®é¢˜ã€‚æˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰å¤šå°ºåº¦è½¬æ¢å™¨çŸ¥è¯†è’¸é¦ï¼Œå¯æœ‰æ•ˆæ•è·å„ç§åˆ†è¾¨ç‡çš„æ³¨æ„åŠ›æƒé‡ï¼›åŒæ¨¡å¼é€»è¾‘è’¸é¦ï¼Œä»¥æé«˜çŸ¥è¯†çš„è½¬ç§»ï¼›ä»¥åŠå…¨å±€é£æ ¼åŒ¹é…æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†ç‰¹å¾åŒ¹é…ä¸å¯¹æŠ—æ€§å­¦ä¹ ç›¸ç»“åˆã€‚åœ¨BraTSå’ŒFeTS 2024æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒMST-KDNetåœ¨Diceå’ŒHD95å¾—åˆ†ä¸Šå‡è¶…è¶Šäº†å½“å‰é¢†å…ˆçš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡æ€æŸå¤±è¾ƒå¤§çš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºå“è¶Šçš„é²æ£’æ€§å’Œé€šç”¨æ€§æ½œåŠ›ï¼Œæˆä¸ºçœŸå®ä¸–ç•Œä¸´åºŠåº”ç”¨çš„æœ‰å‰é€”çš„å€™é€‰è€…ã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Quanato607/MST-KDNet%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Quanato607/MST-KDNetè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22626v1">PDF</a> 11 pages, 2 figures</p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºMST-KDNetçš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œç”¨äºè§£å†³è‚¿ç˜¤è¾¹ç•Œåˆ†å‰²ä¸å‡†ç¡®å’Œç¼ºå°‘å…³é”®æˆåƒæ¨¡æ€ä¸‹çš„ç‰¹å¾è½¬ç§»é—®é¢˜ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¤šå°ºåº¦è½¬æ¢å™¨çŸ¥è¯†è’¸é¦ã€åŒæ¨¡å¼é€»è¾‘è’¸é¦å’Œå…¨å±€é£æ ¼åŒ¹é…æ¨¡å—ç­‰æŠ€æœ¯ï¼Œåœ¨BraTSå’ŒFeTS 2024æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMST-KDNetåœ¨Diceå’ŒHD95å¾—åˆ†ä¸Šè¶…è¶Šäº†å½“å‰é¢†å…ˆçš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡æ€ä¸¢å¤±è¾ƒå¤§çš„æƒ…å†µä¸‹ã€‚è¯¥æ¨¡å‹å…·æœ‰å‡ºè‰²çš„é²æ£’æ€§å’Œé€šç”¨æ€§ï¼Œæœ‰æœ›åº”ç”¨äºå®é™…çš„ä¸´åºŠåœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MST-KDNetæ¨¡å‹æ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒåˆ†æä¸­è‚¿ç˜¤è¾¹ç•Œåˆ†å‰²ä¸å‡†ç¡®å’Œç¼ºå°‘å…³é”®æˆåƒæ¨¡æ€ä¸‹çš„ç‰¹å¾è½¬ç§»é—®é¢˜ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨å¤šå°ºåº¦è½¬æ¢å™¨çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒåˆ†è¾¨ç‡ä¸‹æœ‰æ•ˆæ•æ‰æ³¨æ„åŠ›æƒé‡ã€‚</li>
<li>åŒæ¨¡å¼é€»è¾‘è’¸é¦æŠ€æœ¯æé«˜äº†çŸ¥è¯†è½¬ç§»çš„æ•ˆæœã€‚</li>
<li>å…¨å±€é£æ ¼åŒ¹é…æ¨¡å—ç»“åˆäº†ç‰¹å¾åŒ¹é…å’Œå¯¹æŠ—å­¦ä¹ ã€‚</li>
<li>åœ¨BraTSå’ŒFeTS 2024æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMST-KDNetåœ¨Diceå’ŒHD95å¾—åˆ†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå°¤å…¶æ˜¯æ¨¡æ€ä¸¢å¤±è¾ƒå¤§çš„æƒ…å†µä¸‹ã€‚</li>
<li>è¯¥æ¨¡å‹å…·æœ‰å‡ºè‰²çš„é²æ£’æ€§å’Œé€šç”¨æ€§ï¼Œæœ‰æœ›åº”ç”¨äºå®é™…çš„ä¸´åºŠåœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22626">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-45b703a5c0282bc827c3827e693bb4fd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e07c96ab3a15400dafa93c5bc10cd1fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ba29329ad508758ec9ff88ca352a555.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="CliCARE-Grounding-Large-Language-Models-in-Clinical-Guidelines-for-Decision-Support-over-Longitudinal-Cancer-Electronic-Health-Records"><a href="#CliCARE-Grounding-Large-Language-Models-in-Clinical-Guidelines-for-Decision-Support-over-Longitudinal-Cancer-Electronic-Health-Records" class="headerlink" title="CliCARE: Grounding Large Language Models in Clinical Guidelines for   Decision Support over Longitudinal Cancer Electronic Health Records"></a>CliCARE: Grounding Large Language Models in Clinical Guidelines for   Decision Support over Longitudinal Cancer Electronic Health Records</h2><p><strong>Authors:Dongchen Li, Jitao Liang, Wei Li, Xiaoyu Wang, Longbing Cao, Kun Yu</strong></p>
<p>Large Language Models (LLMs) hold significant promise for improving clinical decision support and reducing physician burnout by synthesizing complex, longitudinal cancer Electronic Health Records (EHRs). However, their implementation in this critical field faces three primary challenges: the inability to effectively process the extensive length and multilingual nature of patient records for accurate temporal analysis; a heightened risk of clinical hallucination, as conventional grounding techniques such as Retrieval-Augmented Generation (RAG) do not adequately incorporate process-oriented clinical guidelines; and unreliable evaluation metrics that hinder the validation of AI systems in oncology. To address these issues, we propose CliCARE, a framework for Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records. The framework operates by transforming unstructured, longitudinal EHRs into patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range dependencies, and then grounding the decision support process by aligning these real-world patient trajectories with a normative guideline knowledge graph. This approach provides oncologists with evidence-grounded decision support by generating a high-fidelity clinical summary and an actionable recommendation. We validated our framework using large-scale, longitudinal data from a private Chinese cancer dataset and the public English MIMIC-IV dataset. In these diverse settings, CliCARE significantly outperforms strong baselines, including leading long-context LLMs and Knowledge Graph-enhanced RAG methods. The clinical validity of our results is supported by a robust evaluation protocol, which demonstrates a high correlation with assessments made by expert oncologists. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ç»¼åˆå¤æ‚çš„çºµå‘ç™Œç—‡ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰ï¼Œåœ¨æ”¹å–„ä¸´åºŠå†³ç­–æ”¯æŒå’Œå‡è½»åŒ»ç”Ÿç–²åŠ³æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶åœ¨è¯¥å…³é”®é¢†åŸŸçš„å®æ–½é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šæ— æ³•æœ‰æ•ˆå¤„ç†æ‚£è€…è®°å½•çš„å¹¿æ³›é•¿åº¦å’Œå¤šè¯­è¨€èƒ½åŠ›ï¼Œä»¥è¿›è¡Œå‡†ç¡®çš„æ—¶é—´åˆ†æï¼›ç”±äºä¼ ç»Ÿçš„æ¥åœ°æŠ€æœ¯ï¼ˆå¦‚å¢å¼ºæ£€ç´¢ç”Ÿæˆï¼ˆRAGï¼‰ï¼‰æœªèƒ½å……åˆ†èå…¥é¢å‘è¿‡ç¨‹çš„ä¸´åºŠæŒ‡å—ï¼Œå¯¼è‡´å‡ºç°æ›´é«˜çš„ä¸´åºŠå¹»è§‰é£é™©ï¼›ä»¥åŠä¸å¯é çš„è¯„ä¼°æŒ‡æ ‡ï¼Œé˜»ç¢äº†è‚¿ç˜¤å­¦é¢†åŸŸäººå·¥æ™ºèƒ½ç³»ç»Ÿçš„éªŒè¯ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CliCAREæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨é€šè¿‡ä¸´åºŠæŒ‡å—æ”¯æŒçºµå‘ç™Œç—‡ç”µå­å¥åº·è®°å½•çš„å†³ç­–æ”¯æŒæ¥æ¥åœ°å¤§å‹è¯­è¨€æ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡å°†ä»¥éç»“æ„åŒ–å½¢å¼å­˜åœ¨çš„çºµå‘EHRè½¬æ¢ä¸ºé’ˆå¯¹æ‚£è€…çš„æ—¶åºçŸ¥è¯†å›¾è°±ï¼ˆTKGsï¼‰æ¥æ•è·é•¿æœŸä¾èµ–å…³ç³»ï¼Œç„¶åé€šè¿‡å°†è¿™äº›ç°å®ä¸–ç•Œçš„æ‚£è€…è½¨è¿¹ä¸è§„èŒƒæ€§æŒ‡å—çŸ¥è¯†å›¾è°±å¯¹é½æ¥æ”¯æŒå†³ç­–è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•ä¸ºè‚¿ç˜¤å­¦å®¶æä¾›äº†åŸºäºè¯æ®çš„æ”¯æŒå†³ç­–ä¾æ®ï¼Œç”Ÿæˆäº†é«˜ä¿çœŸåº¦çš„ä¸´åºŠæ‘˜è¦å’Œå¯è¡Œçš„å»ºè®®ã€‚æˆ‘ä»¬ä½¿ç”¨æ¥è‡ªç§äººä¸­æ–‡ç™Œç—‡æ•°æ®é›†å’Œå…¬å…±è‹±æ–‡MIMIC-IVæ•°æ®é›†çš„å¤§è§„æ¨¡çºµå‘æ•°æ®éªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶ã€‚åœ¨è¿™äº›ä¸åŒçš„ç¯å¢ƒä¸­ï¼ŒCliCAREæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼ŒåŒ…æ‹¬é¢†å…ˆçš„é•¿æ–‡ä¸Šä¸‹æ–‡LLMå’ŒçŸ¥è¯†å›¾è°±å¢å¼ºçš„RAGæ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœçš„ä¸´åºŠæœ‰æ•ˆæ€§å¾—åˆ°äº†ç¨³å¥çš„è¯„ä¼°åè®®çš„æ”¯æŒï¼Œè¯¥åè®®ä¸ä¸“å®¶è‚¿ç˜¤å­¦å®¶çš„è¯„ä¼°ç»“æœé«˜åº¦ç›¸å…³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22533v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç™Œç—‡ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰çš„ç»¼åˆå¤„ç†æ–¹é¢å…·æœ‰æ”¹å–„ä¸´åºŠå†³ç­–æ”¯æŒå’Œç¼“è§£åŒ»ç”Ÿç–²åŠ³çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå…¶å®æ–½é¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šæ— æ³•æœ‰æ•ˆå¤„ç†æ‚£è€…è®°å½•çš„æ‰©å±•é•¿åº¦å’Œå¤šè¯­è¨€èƒ½åŠ›ï¼Œå¯¼è‡´å‡†ç¡®çš„æ—¶é—´åˆ†æå›°éš¾ï¼›å­˜åœ¨ä¸´åºŠå¹»è§‰çš„é£é™©ï¼Œä¼ ç»Ÿæ¥åœ°æŠ€æœ¯å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æœªèƒ½å……åˆ†èå…¥é¢å‘è¿‡ç¨‹çš„ä¸´åºŠæŒ‡å—ï¼›ä»¥åŠç¼ºä¹å¯é çš„è¯„ä¼°æŒ‡æ ‡ï¼Œé˜»ç¢äº†è‚¿ç˜¤å­¦äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„éªŒè¯ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CliCAREæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”¨äºåœ¨çºµå‘ç™Œç—‡ç”µå­å¥åº·è®°å½•ä¸­è¿›è¡Œå†³ç­–æ”¯æŒçš„æ¥åœ°å¤§å‹è¯­è¨€æ¨¡å‹ã€‚å®ƒé€šè¿‡å°†éç»“æ„åŒ–çš„çºµå‘EHRè½¬æ¢ä¸ºæ‚£è€…ç‰¹å®šçš„æ—¶é—´çŸ¥è¯†å›¾è°±ï¼ˆTKGï¼‰æ¥æ•æ‰é•¿æœŸä¾èµ–å…³ç³»ï¼Œç„¶åé€šè¿‡å°†è¿™äº›ç°å®ä¸–ç•Œçš„æ‚£è€…è½¨è¿¹ä¸è§„èŒƒæ€§çš„æŒ‡å—çŸ¥è¯†å›¾è°±å¯¹é½æ¥æ¥åœ°å†³ç­–æ”¯æŒè¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•ä¸ºè‚¿ç˜¤å­¦å®¶æä¾›äº†ä»¥è¯æ®ä¸ºåŸºç¡€çš„å†³ç­–æ”¯æŒï¼Œç”Ÿæˆäº†é«˜ä¿çœŸåº¦çš„ä¸´åºŠæ‘˜è¦å’Œå¯è¡Œçš„å»ºè®®ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å¤§å‹çºµå‘æ•°æ®é›†ä¸Šå¾—åˆ°äº†éªŒè¯ï¼ŒåŒ…æ‹¬æ¥è‡ªä¸­å›½ç™Œç—‡æ•°æ®åº“çš„ç§æœ‰æ•°æ®é›†å’Œå…¬å…±è‹±æ–‡MIMIC-IVæ•°æ®é›†ã€‚åœ¨è¿™äº›ä¸åŒçš„ç¯å¢ƒä¸­ï¼ŒCliCAREæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„åŸºçº¿ï¼ŒåŒ…æ‹¬é¢†å…ˆçš„é•¿æ–‡ä¸Šä¸‹æ–‡LLMå’ŒçŸ¥è¯†å›¾è°±å¢å¼ºçš„RAGæ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœå¾—åˆ°äº†ç¨³å¥è¯„ä¼°åè®®çš„æ”¯æŒï¼Œè¯¥åè®®ä¸ä¸“å®¶è‚¿ç˜¤å­¦å®¶çš„è¯„ä¼°ç»“æœé«˜åº¦ç›¸å…³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç™Œç—‡ç”µå­å¥åº·è®°å½•åº”ç”¨ä¸­å…·æœ‰æ”¹å–„ä¸´åºŠå†³ç­–å’Œç¼“è§£åŒ»ç”Ÿç–²åŠ³çš„æ½œåŠ›ã€‚</li>
<li>å®æ–½å¤§å‹è¯­è¨€æ¨¡å‹äºä¸´åºŠå†³ç­–æ”¯æŒé¢ä¸´ä¸‰å¤§æŒ‘æˆ˜ï¼šå¤„ç†æ‚£è€…è®°å½•çš„æ‰©å±•é•¿åº¦å’Œå¤šè¯­è¨€èƒ½åŠ›ã€ä¸´åºŠå¹»è§‰é£é™©ã€ç¼ºä¹å¯é çš„è¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>CliCAREæ¡†æ¶é€šè¿‡åˆ›å»ºæ—¶é—´çŸ¥è¯†å›¾è°±æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œä»¥æ•æ‰é•¿æœŸä¾èµ–å…³ç³»ï¼Œå¹¶ä¸ä¸´åºŠæŒ‡å—å¯¹é½ä»¥æä¾›æ¥åœ°å†³ç­–æ”¯æŒã€‚</li>
<li>CliCAREæ¡†æ¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼ŒåŒ…æ‹¬æ¥è‡ªä¸­å›½ç™Œç—‡æ•°æ®åº“çš„ç§æœ‰æ•°æ®é›†å’Œå…¬å…±MIMIC-IVæ•°æ®é›†ã€‚</li>
<li>CliCAREæ¡†æ¶æ€§èƒ½ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒ…æ‹¬é•¿æ–‡ä¸Šä¸‹æ–‡çš„å¤§å‹è¯­è¨€æ¨¡å‹å’ŒçŸ¥è¯†å›¾è°±å¢å¼ºçš„RAGæ–¹æ³•ã€‚</li>
<li>CliCAREæ¡†æ¶çš„ä¸´åºŠæœ‰æ•ˆæ€§å¾—åˆ°äº†ä¸“å®¶è¯„ä¼°çš„æ”¯æ’‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22533">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-75b6e6d57f2292a63093766c544cbdb2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9d712d9717caeeeda407cb6ebb30ca3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7253d94f68ca03e6e2ab213014590c02.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-95d92fe8672154e69f708c70a5587c4f.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Aleatoric-Uncertainty-Medical-Image-Segmentation-Estimation-via-Flow-Matching"><a href="#Aleatoric-Uncertainty-Medical-Image-Segmentation-Estimation-via-Flow-Matching" class="headerlink" title="Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow   Matching"></a>Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow   Matching</h2><p><strong>Authors:Phi Van Nguyen, Ngoc Huynh Trinh, Duy Minh Lam Nguyen, Phu Loc Nguyen, Quoc Long Tran</strong></p>
<p>Quantifying aleatoric uncertainty in medical image segmentation is critical since it is a reflection of the natural variability observed among expert annotators. A conventional approach is to model the segmentation distribution using the generative model, but current methods limit the expression ability of generative models. While current diffusion-based approaches have demonstrated impressive performance in approximating the data distribution, their inherent stochastic sampling process and inability to model exact densities limit their effectiveness in accurately capturing uncertainty. In contrast, our proposed method leverages conditional flow matching, a simulation-free flow-based generative model that learns an exact density, to produce highly accurate segmentation results. By guiding the flow model on the input image and sampling multiple data points, our approach synthesizes segmentation samples whose pixel-wise variance reliably reflects the underlying data distribution. This sampling strategy captures uncertainties in regions with ambiguous boundaries, offering robust quantification that mirrors inter-annotator differences. Experimental results demonstrate that our method not only achieves competitive segmentation accuracy but also generates uncertainty maps that provide deeper insights into the reliability of the segmentation outcomes. The code for this paper is freely available at <a target="_blank" rel="noopener" href="https://github.com/huynhspm/Data-Uncertainty">https://github.com/huynhspm/Data-Uncertainty</a> </p>
<blockquote>
<p>é‡åŒ–åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„å¶ç„¶ä¸ç¡®å®šæ€§è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒåæ˜ äº†ä¸“å®¶æ ‡æ³¨è€…ä¹‹é—´è§‚å¯Ÿåˆ°çš„è‡ªç„¶å˜å¼‚æ€§ã€‚ä¸€ç§å¸¸è§„æ–¹æ³•æ˜¯ä½¿ç”¨ç”Ÿæˆæ¨¡å‹å¯¹åˆ†å‰²åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œä½†å½“å‰çš„æ–¹æ³•é™åˆ¶äº†ç”Ÿæˆæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚è™½ç„¶åŸºäºæ‰©æ•£çš„å½“å‰æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºåœ¨é€¼è¿‘æ•°æ®åˆ†å¸ƒæ–¹é¢çš„ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å…¶å›ºæœ‰çš„éšæœºé‡‡æ ·è¿‡ç¨‹ä»¥åŠæ— æ³•å¯¹ç¡®åˆ‡å¯†åº¦è¿›è¡Œå»ºæ¨¡çš„é™åˆ¶ï¼Œä½¿å…¶éš¾ä»¥å‡†ç¡®æ•è·ä¸ç¡®å®šæ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åˆ©ç”¨æ¡ä»¶æµåŒ¹é…ï¼Œè¿™æ˜¯ä¸€ç§æ— æ¨¡æ‹Ÿçš„åŸºäºæµçš„ç”Ÿæˆæ¨¡å‹ï¼Œå­¦ä¹ ç²¾ç¡®å¯†åº¦ï¼Œä»¥äº§ç”Ÿé«˜åº¦å‡†ç¡®çš„åˆ†å‰²ç»“æœã€‚é€šè¿‡åœ¨è¾“å…¥å›¾åƒä¸Šå¼•å¯¼æµæ¨¡å‹å¹¶é‡‡æ ·å¤šä¸ªæ•°æ®ç‚¹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åˆæˆåˆ†å‰²æ ·æœ¬ï¼Œå…¶åƒç´ çº§æ–¹å·®å¯é åœ°åæ˜ äº†åº•å±‚æ•°æ®åˆ†å¸ƒã€‚è¿™ç§é‡‡æ ·ç­–ç•¥æ•è·äº†è¾¹ç•Œæ¨¡ç³ŠåŒºåŸŸçš„ä¸ç¡®å®šæ€§ï¼Œæä¾›äº†ç¨³å¥çš„é‡åŒ–ï¼Œåæ˜ äº†æ ‡æ³¨è€…ä¹‹é—´çš„å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…å®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„åˆ†å‰²ç²¾åº¦ï¼Œè¿˜ç”Ÿæˆäº†ä¸ç¡®å®šæ€§åœ°å›¾ï¼Œä¸ºåˆ†å‰²ç»“æœçš„å¯é æ€§æä¾›äº†æ›´æ·±å…¥çš„äº†è§£ã€‚æœ¬è®ºæ–‡çš„ä»£ç å¯å…è´¹è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://github.com/huynhspm/Data-Uncertainty">https://github.com/huynhspm/Data-Uncertainty</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22418v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºæ¡ä»¶æµåŒ¹é…çš„æ— ä»¿çœŸæµç”Ÿæˆæ¨¡å‹çš„æ–¹æ³•ï¼Œç”¨äºé‡åŒ–åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„éšæœºä¸ç¡®å®šæ€§ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨åƒç´ çº§åˆ«å‡†ç¡®åæ˜ æ•°æ®åˆ†å¸ƒï¼Œé€šè¿‡æŒ‡å¯¼è¾“å…¥å›¾åƒå’Œé‡‡æ ·å¤šä¸ªæ•°æ®ç‚¹ï¼Œåˆæˆå¯é çš„åˆ†å‰²æ ·æœ¬ï¼Œä»è€Œæ•æ‰æ¨¡ç³Šè¾¹ç•ŒåŒºåŸŸçš„ä¸ç¡®å®šæ€§ï¼Œåæ˜ ä¸“å®¶æ ‡æ³¨è€…ä¹‹é—´çš„å·®å¼‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…å®ç°äº†ç«äº‰çš„åˆ†å‰²ç²¾åº¦ï¼Œè¿˜ç”Ÿæˆäº†ä¸ç¡®å®šæ€§åœ°å›¾ï¼Œä¸ºåˆ†å‰²ç»“æœçš„å¯é æ€§æä¾›äº†æ›´æ·±å…¥çš„äº†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡åŒ–åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„éšæœºä¸ç¡®å®šæ€§å¯¹äºåæ˜ ä¸“å®¶æ ‡æ³¨è€…ä¹‹é—´çš„å·®å¼‚è‡³å…³é‡è¦ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨ç”Ÿæˆæ¨¡å‹æ¨¡æ‹Ÿåˆ†å‰²åˆ†å¸ƒï¼Œä½†ç°æœ‰æ–¹æ³•é™åˆ¶äº†ç”Ÿæˆæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚</li>
<li>æ‰©æ•£æ–¹æ³•è™½ç„¶åœ¨é€¼è¿‘æ•°æ®åˆ†å¸ƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶å›ºæœ‰çš„éšæœºé‡‡æ ·è¿‡ç¨‹å’Œæ— æ³•ç²¾ç¡®å»ºæ¨¡å¯†åº¦é™åˆ¶äº†å…¶åœ¨æ•æ‰ä¸ç¡®å®šæ€§æ–¹é¢çš„å‡†ç¡®æ€§ã€‚</li>
<li>æœ¬æ–‡æå‡ºçš„åŸºäºæ¡ä»¶æµåŒ¹é…çš„æ–¹æ³•æ˜¯ä¸€ç§æ— ä»¿çœŸæµç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿå­¦ä¹ ç²¾ç¡®å¯†åº¦ï¼Œäº§ç”Ÿé«˜åº¦å‡†ç¡®çš„åˆ†å‰²ç»“æœã€‚</li>
<li>é€šè¿‡æŒ‡å¯¼æµæ¨¡å‹åœ¨è¾“å…¥å›¾åƒä¸Šå¹¶é‡‡æ ·å¤šä¸ªæ•°æ®ç‚¹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåˆæˆåæ˜ åº•å±‚æ•°æ®åˆ†å¸ƒçš„åˆ†å‰²æ ·æœ¬ï¼Œå…¶åƒç´ çº§æ–¹å·®èƒ½å¤Ÿå¯é åœ°åæ˜ ä¸ç¡®å®šæ€§ã€‚</li>
<li>è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨åŒºåŸŸè¾¹ç•Œæ¨¡ç³Šçš„æƒ…å†µä¸‹æ•æ‰ä¸ç¡®å®šæ€§ï¼Œæä¾›ç¨³å¥çš„é‡åŒ–ï¼Œåæ˜ ä¸åŒä¸“å®¶æ ‡æ³¨è€…ä¹‹é—´çš„å·®å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22418">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a659d1e71c3c30aaee63c29e1d0024b9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92d03da7a592a326803274e417b2c719.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MINR-Implicit-Neural-Representations-with-Masked-Image-Modelling"><a href="#MINR-Implicit-Neural-Representations-with-Masked-Image-Modelling" class="headerlink" title="MINR: Implicit Neural Representations with Masked Image Modelling"></a>MINR: Implicit Neural Representations with Masked Image Modelling</h2><p><strong>Authors:Sua Lee, Joonhun Lee, Myungjoo Kang</strong></p>
<p>Self-supervised learning methods like masked autoencoders (MAE) have shown significant promise in learning robust feature representations, particularly in image reconstruction-based pretraining task. However, their performance is often strongly dependent on the masking strategies used during training and can degrade when applied to out-of-distribution data. To address these limitations, we introduce the masked implicit neural representations (MINR) framework that synergizes implicit neural representations with masked image modeling. MINR learns a continuous function to represent images, enabling more robust and generalizable reconstructions irrespective of masking strategies. Our experiments demonstrate that MINR not only outperforms MAE in in-domain scenarios but also in out-of-distribution settings, while reducing model complexity. The versatility of MINR extends to various self-supervised learning applications, confirming its utility as a robust and efficient alternative to existing frameworks. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå¦‚æ©ç è‡ªåŠ¨ç¼–ç å™¨ï¼ˆMAEï¼‰åœ¨å­¦ä¹ ç¨³å¥çš„ç‰¹å¾è¡¨ç¤ºæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºäºå›¾åƒé‡å»ºçš„é¢„è®­ç»ƒä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ€§èƒ½å¾€å¾€å¼ºçƒˆä¾èµ–äºè®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ©ç ç­–ç•¥ï¼Œå½“åº”ç”¨äºç¦»ç¾¤æ•°æ®æ—¶æ€§èƒ½å¯èƒ½ä¼šä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ©ç éšå¼ç¥ç»è¡¨ç¤ºï¼ˆMINRï¼‰æ¡†æ¶ï¼Œå®ƒå°†éšå¼ç¥ç»è¡¨ç¤ºä¸æ©ç å›¾åƒå»ºæ¨¡ç›¸ç»“åˆã€‚MINRå­¦ä¹ ä¸€ç§è¿ç»­å‡½æ•°æ¥è¡¨ç¤ºå›¾åƒï¼Œä½¿å¾—æ— è®ºé‡‡ç”¨ä½•ç§æ©ç ç­–ç•¥ï¼Œéƒ½èƒ½è¿›è¡Œæ›´ç¨³å¥å’Œé€šç”¨çš„é‡å»ºã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒMINRä¸ä»…åœ¨åŸŸå†…åœºæ™¯ä¸­ä¼˜äºMAEï¼Œè€Œä¸”åœ¨ç¦»ç¾¤æ•°æ®ç¯å¢ƒä¸­ä¹Ÿè¡¨ç°æ›´å¥½ï¼ŒåŒæ—¶é™ä½äº†æ¨¡å‹å¤æ‚æ€§ã€‚MINRçš„é€šç”¨æ€§å¯åº”ç”¨äºå„ç§è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œè¯å®å…¶ä½œä¸ºç°æœ‰æ¡†æ¶çš„ç¨³å¥é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆçš„å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22404v1">PDF</a> Accepted to the ICCV 2023 workshop on Out-of-Distribution   Generalization in Computer Vision</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹æ³•åœ¨å›¾åƒé‡å»ºé¢„è®­ç»ƒä»»åŠ¡ä¸­å±•ç°å‡ºå¼ºå¤§çš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå…¶æ€§èƒ½ä¾èµ–äºè®­ç»ƒä¸­çš„æ©ç ç­–ç•¥ï¼Œå¹¶ä¸”åœ¨é¢å¯¹åˆ†å¸ƒå¤–æ•°æ®æ—¶æ€§èƒ½å¯èƒ½ä¸‹é™ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ©ç éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºï¼ˆMINRï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºä¸æ©ç å›¾åƒå»ºæ¨¡ã€‚MINRå­¦ä¹ ä¸€ç§è¿ç»­å‡½æ•°æ¥è¡¨ç¤ºå›¾åƒï¼Œæ— è®ºæ©ç ç­–ç•¥å¦‚ä½•ï¼Œéƒ½èƒ½å®ç°æ›´ç¨³å¥å’Œå¯æ³›åŒ–çš„é‡å»ºã€‚å®éªŒè¯æ˜ï¼Œæ— è®ºæ˜¯åœ¨åŸŸå†…åœºæ™¯è¿˜æ˜¯åˆ†å¸ƒå¤–è®¾ç½®ä¸­ï¼ŒMINRéƒ½ä¼˜äºMAEï¼ŒåŒæ—¶é™ä½äº†æ¨¡å‹å¤æ‚æ€§ã€‚MINRçš„é€šç”¨æ€§å¯å¹¿æ³›åº”ç”¨äºå„ç§è‡ªæˆ‘ç›‘ç£å­¦ä¹ ä»»åŠ¡ï¼Œè¯æ˜å…¶ä½œä¸ºç°æœ‰æ¡†æ¶çš„ç¨³å¥ã€é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆçš„å®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ©ç è‡ªç¼–ç å™¨ï¼ˆMAEï¼‰åœ¨å›¾åƒé‡å»ºé¢„è®­ç»ƒä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå¼ºå¤§çš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚</li>
<li>æ©ç ç­–ç•¥å¯¹MAEæ€§èƒ½å…·æœ‰é‡è¦å½±å“ã€‚</li>
<li>MINRæ¡†æ¶ç»“åˆäº†éšå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºä¸æ©ç å›¾åƒå»ºæ¨¡ã€‚</li>
<li>MINRé€šè¿‡å­¦ä¹ è¿ç»­å‡½æ•°è¡¨ç¤ºå›¾åƒï¼Œå®ç°æ›´ç¨³å¥å’Œå¯æ³›åŒ–çš„é‡å»ºã€‚</li>
<li>MINRåœ¨åŸŸå†…åœºæ™¯å’Œåˆ†å¸ƒå¤–åœºæ™¯ä¸­å‡ä¼˜äºMAEã€‚</li>
<li>MINRé™ä½äº†æ¨¡å‹å¤æ‚æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22404">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-877e547055179e7b7e88f1ff522f58b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-602ecea4e726eaedc6483138aa812185.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e333811ddd4cbcfa75cdb9d777a02f86.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1049d4ae6438daf03a9d1aee1979c206.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  TTS-1 Technical Report
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-e0aa87973b8e89f7287f66e505ae9a2f.jpg" class="responsive-img" alt="ç‰™é½¿ä¿®å¤">
                        
                        <span class="card-title">ç‰™é½¿ä¿®å¤</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ç‰™é½¿ä¿®å¤ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  PerioDet Large-Scale Panoramic Radiograph Benchmark for   Clinical-Oriented Apical Periodontitis Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/" class="post-category">
                                    ç‰™é½¿ä¿®å¤
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E7%89%99%E9%BD%BF%E4%BF%AE%E5%A4%8D/">
                        <span class="chip bg-color">ç‰™é½¿ä¿®å¤</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29058.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
