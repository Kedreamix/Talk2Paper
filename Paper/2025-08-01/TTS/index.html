<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  TTS-1 Technical Report">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-1049d4ae6438daf03a9d1aee1979c206.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    15.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    64 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-02-æ›´æ–°"><a href="#2025-08-02-æ›´æ–°" class="headerlink" title="2025-08-02 æ›´æ–°"></a>2025-08-02 æ›´æ–°</h1><h2 id="TTS-1-Technical-Report"><a href="#TTS-1-Technical-Report" class="headerlink" title="TTS-1 Technical Report"></a>TTS-1 Technical Report</h2><p><strong>Authors:Oleg Atamanenko, Anna Chalova, Joseph Coombes, Nikki Cope, Phillip Dang, Zhifeng Deng, Jimmy Du, Michael Ermolenko, Feifan Fan, Yufei Feng, Cheryl Fichter, Pavel Filimonov, Louis Fischer, Kylan Gibbs, Valeria Gusarova, Pavel Karpik, Andreas Assad Kottner, Ian Lee, Oliver Louie, Jasmine Mai, Mikhail Mamontov, Suri Mao, Nurullah Morshed, Igor Poletaev, Florin Radu, Dmytro Semernia, Evgenii Shingarev, Vikram Sivaraja, Peter Skirko, Rinat Takhautdinov, Robert Villahermosa, Jean Wang</strong></p>
<p>We introduce Inworld TTS-1, a set of two Transformer-based autoregressive text-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters and is designed for utmost quality and expressiveness in demanding applications. TTS-1 is our most efficient model, with 1.6B parameters, built for real-time speech synthesis and on-device use cases. By scaling train-time compute and applying a sequential process of pre-training, fine-tuning, and RL-alignment of the speech-language model (SpeechLM) component, both models achieve state-of-the-art performance on a variety of benchmarks, demonstrating exceptional quality relying purely on in-context learning of the speakerâ€™s voice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech with low latency, and support 11 languages with fine-grained emotional control and non-verbal vocalizations through audio markups. We additionally open-source our training and modeling code under an MIT license. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Inworld TTS-1ï¼Œè¿™æ˜¯ä¸€ç»„åŸºäºTransformerçš„è‡ªå›å½’æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ã€‚æˆ‘ä»¬çš„å¤§å‹æ¨¡å‹TTS-1-Maxæ‹¥æœ‰8.8äº¿ä¸ªå‚æ•°ï¼Œæ—¨åœ¨æ»¡è¶³é«˜è¦æ±‚å’Œé«˜è´¨é‡çš„åº”ç”¨åœºæ™¯ã€‚TTS-1åˆ™æ˜¯æˆ‘ä»¬æœ€é«˜æ•ˆçš„æ¨¡å‹ï¼Œæ‹¥æœ‰1.6äº¿ä¸ªå‚æ•°ï¼Œä¸“ä¸ºå®æ—¶è¯­éŸ³åˆæˆå’Œè®¾å¤‡ç«¯ç”¨ä¾‹è®¾è®¡ã€‚é€šè¿‡æ‰©å±•è®­ç»ƒæ—¶é—´çš„è®¡ç®—åŠ›ï¼Œå¯¹è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSpeechLMï¼‰ç»„ä»¶è¿›è¡Œé¢„è®­ç»ƒã€ç²¾ç»†è°ƒæ•´å’Œå¼ºåŒ–å­¦ä¹ å¯¹é½çš„åºåˆ—è¿‡ç¨‹ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå±•ç¤ºäº†ä»…å‡­å¯¹è¯´è¯äººå£°éŸ³çš„ä¸Šä¸‹æ–‡å­¦ä¹ å°±èƒ½å®ç°çš„å“è¶Šè´¨é‡ã€‚Inworld TTS-1å’ŒTTS-1-Maxå¯ä»¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„48kHzè¯­éŸ³ï¼Œå…·æœ‰ä½å»¶è¿Ÿï¼Œæ”¯æŒä½¿ç”¨éŸ³é¢‘æ ‡è®°è¿›è¡Œç²¾ç»†çš„æƒ…ç»ªæ§åˆ¶å’Œéè¨€è¯­å‘å£°ç­‰å¤æ‚æ“ä½œï¼Œæ¶µç›–å¤šè¾¾11ç§è¯­è¨€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»¥MITè®¸å¯è¯çš„å½¢å¼å…¬å¼€äº†æˆ‘ä»¬çš„è®­ç»ƒå’Œå»ºæ¨¡ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21138v1">PDF</a> 20 pages, 10 figures. For associated modeling and training code, see   <a target="_blank" rel="noopener" href="https://github.com/inworld-ai/tts">https://github.com/inworld-ai/tts</a></p>
<p><strong>Summary</strong></p>
<p>Inworld TTS-1æ˜¯ä¸€æ¬¾åŸºäºTransformerçš„è‡ªå›å½’æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ç³»åˆ—ï¼ŒåŒ…å«TTS-1å’ŒTTS-1-Maxä¸¤ä¸ªæ¨¡å‹ã€‚TTS-1æ³¨é‡å®æ—¶è¯­éŸ³åˆæˆå’Œè®¾å¤‡ç«¯åº”ç”¨åœºæ™¯ï¼Œæ‹¥æœ‰1.6Bå‚æ•°ï¼›è€ŒTTS-1-Maxåˆ™è¿½æ±‚é«˜è´¨é‡å’Œè¡¨è¾¾åŠ›ï¼Œé€‚ç”¨äºé«˜éœ€æ±‚åº”ç”¨ï¼Œæ‹¥æœ‰8.8Bå‚æ•°ã€‚é€šè¿‡æ‰©å¤§è®­ç»ƒæ—¶çš„è®¡ç®—è§„æ¨¡ã€å¯¹è¯­éŸ³è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€å¾®è°ƒåŠå¼ºåŒ–å­¦ä¹ å¯¹é½çš„åºåˆ—è¿‡ç¨‹ï¼Œä¸¤ä¸ªæ¨¡å‹åœ¨å¤šç§åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šæ€§èƒ½ï¼Œå¹¶å‡­å€Ÿè¯­å¢ƒå­¦ä¹ å±•ç°å‡ºè‰²çš„å£°éŸ³è´¨é‡ã€‚Inworld TTS-1å’ŒTTS-1-Maxèƒ½ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„48kHzè¯­éŸ³ï¼Œå…·æœ‰ä½å»¶è¿Ÿç‰¹æ€§ï¼Œæ”¯æŒ11ç§è¯­è¨€ï¼Œå¹¶é€šè¿‡éŸ³é¢‘æ ‡è®°å®ç°ç²¾ç»†çš„æƒ…ç»ªæ§åˆ¶å’Œéè¨€è¯­å‘å£°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Inworld TTS-1æ˜¯ä¸€æ¬¾åŸºäºTransformerçš„æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ç³»åˆ—ã€‚</li>
<li>TTS-1å’ŒTTS-1-Maxæ˜¯è¯¥ç³»åˆ—çš„ä¸¤ä¸ªæ¨¡å‹ï¼Œåˆ†åˆ«é’ˆå¯¹å®æ—¶è¯­éŸ³åˆæˆå’Œè®¾å¤‡ç«¯åº”ç”¨åœºæ™¯ä»¥åŠé«˜è´¨é‡å’Œè¡¨è¾¾åŠ›çš„é«˜éœ€æ±‚åº”ç”¨è¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>é€šè¿‡æ‰©å¤§è®­ç»ƒæ—¶çš„è®¡ç®—è§„æ¨¡å’Œå¯¹è¯­éŸ³è¯­è¨€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒã€å¾®è°ƒåŠå¼ºåŒ–å­¦ä¹ å¯¹é½ï¼Œä¸¤ä¸ªæ¨¡å‹å®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
<li>Inworld TTS-1å’ŒTTS-1-Maxèƒ½ç”Ÿæˆé«˜è´¨é‡ã€é«˜åˆ†è¾¨ç‡çš„48kHzè¯­éŸ³ï¼Œå¹¶å…·æœ‰ä½å»¶è¿Ÿç‰¹æ€§ã€‚</li>
<li>æ¨¡å‹æ”¯æŒå¤šè¾¾11ç§è¯­è¨€ï¼Œå¹¶å…·å¤‡ç²¾ç»†çš„æƒ…ç»ªæ§åˆ¶å’Œéè¨€è¯­å‘å£°èƒ½åŠ›ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨å¼€æºè®­ç»ƒå’Œå»ºæ¨¡ä»£ç ï¼Œéµå¾ªMITè®¸å¯åè®®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21138">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-571309561cecf7280112b4287d98b16c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f182f030acda279eeac235480fb262b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3a6168da13cf7311dc630be0bef26f7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-19e2d5471d37300021df094c951ebf9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-77004e0daa840bcd4e6ef709cba37361.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1ca4909f3f66c0d59e321e8ef3a40261.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3c36fcf087e7bf9a04fa29b3030ffe9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4133c82ceceba5586187f9b3b089352f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Do-Not-Mimic-My-Voice-Speaker-Identity-Unlearning-for-Zero-Shot-Text-to-Speech"><a href="#Do-Not-Mimic-My-Voice-Speaker-Identity-Unlearning-for-Zero-Shot-Text-to-Speech" class="headerlink" title="Do Not Mimic My Voice: Speaker Identity Unlearning for Zero-Shot   Text-to-Speech"></a>Do Not Mimic My Voice: Speaker Identity Unlearning for Zero-Shot   Text-to-Speech</h2><p><strong>Authors:Taesoo Kim, Jinju Kim, Dongchan Kim, Jong Hwan Ko, Gyeong-Moon Park</strong></p>
<p>The rapid advancement of Zero-Shot Text-to-Speech (ZS-TTS) technology has enabled high-fidelity voice synthesis from minimal audio cues, raising significant privacy and ethical concerns. Despite the threats to voice privacy, research to selectively remove the knowledge to replicate unwanted individual voices from pre-trained model parameters has not been explored. In this paper, we address the new challenge of speaker identity unlearning for ZS-TTS systems. To meet this goal, we propose the first machine unlearning frameworks for ZS-TTS, especially Teacher-Guided Unlearning (TGU), designed to ensure the model forgets designated speaker identities while retaining its ability to generate accurate speech for other speakers. Our proposed methods incorporate randomness to prevent consistent replication of forget speakersâ€™ voices, assuring unlearned identities remain untraceable. Additionally, we propose a new evaluation metric, speaker-Zero Retrain Forgetting (spk-ZRF). This assesses the modelâ€™s ability to disregard prompts associated with forgotten speakers, effectively neutralizing its knowledge of these voices. The experiments conducted on the state-of-the-art model demonstrate that TGU prevents the model from replicating forget speakersâ€™ voices while maintaining high quality for other speakers. The demo is available at <a target="_blank" rel="noopener" href="https://speechunlearn.github.io/">https://speechunlearn.github.io/</a> </p>
<blockquote>
<p>é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆZS-TTSï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ä½¿å¾—å¯ä»¥ä»æå°‘çš„éŸ³é¢‘çº¿ç´¢ä¸­è¿›è¡Œé«˜ä¿çœŸè¯­éŸ³åˆæˆï¼Œä»è€Œå¼•å‘äº†å…³äºéšç§å’Œä¼¦ç†çš„é‡å¤§æ‹…å¿§ã€‚å°½ç®¡è¯­éŸ³éšç§é¢ä¸´å¨èƒï¼Œä½†å…³äºä»é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ä¸­å¤åˆ¶ä¸æƒ³è¦çš„ä¸ªäººè¯­éŸ³çš„ç›¸å…³çŸ¥è¯†é€‰æ‹©æ€§åœ°æ¶ˆé™¤çš„ç ”ç©¶å°šæœªè¢«æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ç³»ç»Ÿï¼ˆZS-TTSï¼‰çš„è¯´è¯äººèº«ä»½é—å¿˜çš„æ–°æŒ‘æˆ˜ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬ä¸ºZS-TTSé¦–æ¬¡æå‡ºäº†æœºå™¨é—å¿˜æ¡†æ¶ï¼Œç‰¹åˆ«æ˜¯æ•™å¸ˆæŒ‡å¯¼çš„é—å¿˜ï¼ˆTGUï¼‰ï¼Œæ—¨åœ¨ç¡®ä¿æ¨¡å‹å¿˜è®°æŒ‡å®šçš„è¯´è¯äººèº«ä»½ï¼ŒåŒæ—¶ä¿ç•™å…¶ä¸ºå…¶ä»–è¯´è¯äººç”Ÿæˆå‡†ç¡®è¯­éŸ³çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•èå…¥äº†éšæœºæ€§ï¼Œä»¥é˜²æ­¢æŒç»­å¤åˆ¶é—å¿˜è¯´è¯è€…çš„å£°éŸ³ï¼Œç¡®ä¿å·²é—å¿˜çš„èº«ä»½æ— æ³•è¿½è¸ªã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå³è¯´è¯è€…é›¶å†è®­ç»ƒé—å¿˜ï¼ˆspk-ZRFï¼‰ã€‚è¿™å¯ä»¥è¯„ä¼°æ¨¡å‹å¿½ç•¥ä¸å·²é—å¿˜è¯´è¯è€…ç›¸å…³çš„æç¤ºçš„èƒ½åŠ›ï¼Œä»è€Œæœ‰æ•ˆåœ°ä¸­å’Œå…¶å¯¹è¿™äº›å£°éŸ³çš„äº†è§£ã€‚åœ¨å…ˆè¿›æ¨¡å‹ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼ŒTGUèƒ½å¤Ÿé˜²æ­¢æ¨¡å‹å¤åˆ¶å·²é—å¿˜è¯´è¯è€…çš„å£°éŸ³ï¼ŒåŒæ—¶ä¿æŒå¯¹å…¶ä»–è¯´è¯äººçš„é«˜è´¨é‡è¯­éŸ³ã€‚æ¼”ç¤ºç½‘ç«™ä¸ºï¼š<a target="_blank" rel="noopener" href="https://speechunlearn.github.io/">https://speechunlearn.github.io/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20140v1">PDF</a> Proceedings of the 42nd International Conference on Machine Learning   (ICML 2025), Vancouver, Canada. PMLR 267, 2025. Authors Jinju Kim and Taesoo   Kim contributed equally</p>
<p><strong>Summary</strong></p>
<p>é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆZS-TTSï¼‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¸¦æ¥äº†å¯¹éšç§å’Œä¼¦ç†çš„é‡å¤§æ‹…å¿§ã€‚ä¸ºä¿æŠ¤è¯­éŸ³éšç§ï¼Œæœ¬æ–‡é¦–æ¬¡æå‡ºäº†é’ˆå¯¹ZS-TTSç³»ç»Ÿçš„æœºå™¨é—å¿˜æ¡†æ¶ï¼ŒåŒ…æ‹¬æ•™å¸ˆå¼•å¯¼é—å¿˜ï¼ˆTGUï¼‰æŠ€æœ¯ï¼Œæ—¨åœ¨ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå¿˜è®°ç‰¹å®šè¯´è¯äººçš„èº«ä»½åŒæ—¶ä¿ç•™å¯¹å…¶ä»–è¯´è¯äººçš„å‡†ç¡®è¯­éŸ³ç”Ÿæˆèƒ½åŠ›ã€‚è¯¥æ–¹æ³•èå…¥äº†éšæœºæ€§ä»¥ç¡®ä¿æ— æ³•é‡ç°é—å¿˜è¯´è¯äººçš„å£°éŸ³ï¼Œå¹¶å¼•å…¥äº†æ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”è¯´è¯è€…é›¶è®­ç»ƒé—å¿˜ï¼ˆspk-ZRFï¼‰ï¼Œä»¥è¯„ä¼°æ¨¡å‹å¯¹é—å¿˜è¯´è¯äººçš„å¿½è§†ç¨‹åº¦ã€‚å®éªŒè¯æ˜ï¼ŒTGUèƒ½æœ‰æ•ˆé˜²æ­¢æ¨¡å‹å¤åˆ¶é—å¿˜è¯´è¯äººçš„å£°éŸ³ï¼ŒåŒæ—¶ä¿æŒå¯¹å…¶ä»–è¯´è¯äººçš„é«˜è´¨é‡è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ZS-TTSæŠ€æœ¯çš„å¿«é€Ÿå‘å±•å¼•å‘äº†å¯¹éšç§å’Œä¼¦ç†çš„æ‹…å¿§ã€‚</li>
<li>ç ”ç©¶å°šæœªæ¢ç´¢ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­ç§»é™¤ç‰¹å®šä¸ªä½“å£°éŸ³å¤åˆ¶çš„çŸ¥è¯†ã€‚</li>
<li>æœ¬æ–‡æå‡ºé¦–ä¸ªé’ˆå¯¹ZS-TTSç³»ç»Ÿçš„æœºå™¨é—å¿˜æ¡†æ¶ã€‚</li>
<li>æ•™å¸ˆå¼•å¯¼é—å¿˜ï¼ˆTGUï¼‰æŠ€æœ¯ç¡®ä¿æ¨¡å‹å¿˜è®°æŒ‡å®šè¯´è¯äººèº«ä»½ã€‚</li>
<li>TGUè®¾è®¡èƒ½ä¿ç•™æ¨¡å‹å¯¹å…¶ä»–è¯´è¯äººçš„å‡†ç¡®è¯­éŸ³ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>ä¸ºè¯„ä¼°æ¨¡å‹å¯¹é—å¿˜è¯´è¯äººçš„å¿½è§†ç¨‹åº¦ï¼Œå¼•å…¥äº†æ–°çš„è¯„ä¼°æŒ‡æ ‡â€”â€”spk-ZRFã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20140">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8dc99f390224104934f67bcf2adff44b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65518654259eb27b3e7d4b8ea6dcaebf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4ad4e116e33cfaeedec9a41b8bb3f7b2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="JWST-Spectroscopic-Confirmation-of-the-Cosmic-Gems-Arc-at-z-9-625-â€“-Insights-into-the-small-scale-structure-of-a-post-burst-system"><a href="#JWST-Spectroscopic-Confirmation-of-the-Cosmic-Gems-Arc-at-z-9-625-â€“-Insights-into-the-small-scale-structure-of-a-post-burst-system" class="headerlink" title="JWST Spectroscopic Confirmation of the Cosmic Gems Arc at z&#x3D;9.625 â€“   Insights into the small scale structure of a post-burst system"></a>JWST Spectroscopic Confirmation of the Cosmic Gems Arc at z&#x3D;9.625 â€“   Insights into the small scale structure of a post-burst system</h2><p><strong>Authors:M. Messa, E. Vanzella, F. Loiacono, A. Adamo, M. Oguri, K. Sharon, L. D. Bradley, L. Christensen, A. Claeyssens, J. Richard,  Abdurroâ€™uf, F. E. Bauer, P. Bergamini, A. Bolamperti, M. BradaÄ, F. Calura, D. Coe, J. M. Diego, C. Grillo, T. Y-Y. Hsiao, A. K. Inoue, S. Fujimoto, M. Lombardi, M. Meneghetti, T. Resseguier, M. Ricotti, P. Rosati, B. Welch, R. A. Windhorst, X. Xu, E. Zackrisson, A. Zanella, A. Zitrin</strong></p>
<p>We present JWST&#x2F;NIRSpec integral field spectroscopy of the Cosmic Gems arc, strongly magnified by the galaxy cluster SPT-CL J0615$-$5746. Six-hour integration using NIRSpec prism spectroscopy (resolution $\rm R\simeq 30-300$), covering the spectral range $0.8-5.3<del>\mu m$, reveals a pronounced $\rm Ly\alpha$-continuum break at $\lambda \simeq 1.3</del>\mu m$, and weak optical $\rm H\beta$ and $\rm [OIII]\lambda4959$ emission lines at $z&#x3D;9.625\pm0.002$, located in the reddest part of the spectrum ($\lambda &gt; 5.1<del>\mu m$). No additional ultraviolet or optical emission lines are reliably detected. A weak Balmer break is measured alongside a very blue ultraviolet slope ($\beta \leq-2.5$, $\rm F_{\lambda} \sim \lambda^{\beta}$). Spectral fitting with $\tt Bagpipes$ suggests the Cosmic Gems galaxy is in a post-starburst phase, making it the highest-redshift system currently observed in a mini-quenched state. Spatially resolved spectroscopy at tens pc scales shows relatively uniform features across subcomponents of the arc. These findings align well with physical properties previously derived from JWST&#x2F;NIRCam photometry of the stellar clusters, now corroborated by spectroscopic evidence. In particular, five observed star clusters exhibit ages of $\rm 7-30</del>Myr$. An updated lens model constrains the intrinsic sizes and masses of these clusters, confirming they are extremely compact and denser than typical star clusters in local star-forming galaxies. Additionally, four compact stellar systems consistent with star clusters ($\lesssim10$ pc) are identified along the extended tail of the arc. A sub-parsec line-emitting HII region straddling the critical line, lacking a NIRCam counterpart, is also serendipitously detected. </p>
<blockquote>
<p>æˆ‘ä»¬å¯¹å®‡å®™å®çŸ³å¼§è¿›è¡Œäº†JWST&#x2F;NIRSpecç§¯åˆ†åœºå…‰è°±è§‚æµ‹ï¼Œè¯¥å¼§å—åˆ°æ˜Ÿç³»å›¢SPT-CL J0615-5746çš„å¼ºçƒˆæ”¾å¤§ã€‚ä½¿ç”¨NIRSpecæ£±é•œå…‰è°±æ³•ï¼ˆåˆ†è¾¨ç‡Rçº¦ä¸º30-300ï¼‰è¿›è¡Œå…­å°æ—¶ç§¯åˆ†ï¼Œè¦†ç›–å…‰è°±èŒƒå›´0.8-5.3å¾®ç±³ï¼Œåœ¨æ³¢é•¿çº¦ä¸º1.3å¾®ç±³å¤„æ˜¾ç¤ºå‡ºæ˜æ˜¾çš„LyÎ±è¿ç»­ç»Ÿæ–­è£‚ï¼Œä»¥åŠåœ¨z&#x3D;9.625Â±0.002å¤„çš„å¾®å¼±å…‰å­¦HÎ²å’Œ[OIII]Î»4959å‘å°„çº¿ï¼Œä½äºå…‰è°±çš„çº¢è‰²éƒ¨åˆ†ï¼ˆÎ»&gt; 5.1å¾®ç±³ï¼‰ã€‚æ²¡æœ‰æ£€æµ‹åˆ°å…¶ä»–å¯é çš„ç´«å¤–æˆ–å…‰å­¦å‘å°„çº¿ã€‚æµ‹é‡åˆ°ä¸€ä¸ªå¾®å¼±çš„å·´å°”æœ«æ–­è£‚ä»¥åŠä¸€ä¸ªéå¸¸è“çš„ç´«å¤–æ–œç‡ï¼ˆÎ²â‰¤-2.5ï¼ŒFÎ»~Î»Î²ï¼‰ã€‚ç”¨Bagpipesè¿›è¡Œå…‰è°±æ‹Ÿåˆè¡¨æ˜ï¼Œå®‡å®™å®çŸ³æ˜Ÿç³»å¤„äºåçˆ†å‘é˜¶æ®µï¼Œä½¿å…¶æˆä¸ºç›®å‰è§‚å¯Ÿåˆ°çš„å¤„äºæœ€å°æ·¬ç­çŠ¶æ€çš„çº¢ç§»æœ€é«˜çš„ç³»ç»Ÿã€‚åœ¨æ•°åä¸ªå¤©æ–‡å•ä½è§„æ¨¡ä¸Šè¿›è¡Œç©ºé—´åˆ†è§£å…‰è°±æ˜¾ç¤ºï¼Œå¼§çš„å„ä¸ªå­æˆåˆ†çš„ç‰¹å¾ç›¸å¯¹ç»Ÿä¸€ã€‚è¿™äº›å‘ç°ä¸æ ¹æ®JWST&#x2F;NIRCamå¯¹æ˜Ÿå›¢çš„æ‘„å½±æœ¯å…ˆå‰å¾—å‡ºçš„ç‰©ç†æ€§è´¨ç›¸ç¬¦ï¼Œç°åœ¨å¾—åˆ°äº†å…‰è°±è¯æ®çš„æ”¯æŒã€‚ç‰¹åˆ«æ˜¯ï¼Œäº”ä¸ªè§‚å¯Ÿåˆ°çš„æ˜Ÿå›¢å¹´é¾„ä¸º7-30Myrã€‚æ›´æ–°çš„é€é•œæ¨¡å‹é™åˆ¶äº†è¿™äº›æ˜Ÿå›¢çš„å†…ç¦€å¤§å°å’Œè´¨é‡ï¼Œè¯å®å®ƒä»¬æ¯”æœ¬åœ°æ˜Ÿç³»ä¸­çš„å…¸å‹æ˜Ÿå›¢æ›´ç´§å‡‘ã€å¯†åº¦æ›´é«˜ã€‚æ­¤å¤–ï¼Œåœ¨å¼§çš„å»¶ä¼¸å°¾è¿¹ä¸­å‘ç°äº†å››ä¸ªä¸æ˜Ÿå›¢ä¸€è‡´çš„è‡´å¯†æ’æ˜Ÿç³»ç»Ÿï¼ˆâ‰¤10ä¸ªå¤©æ–‡å•ä½ï¼‰ã€‚è¿˜æ„å¤–æ£€æµ‹åˆ°äº†ä¸€ä¸ªè·¨è¶Šä¸´ç•Œçº¿çš„äºšç§’çº§å‘å°„HIIåŒºåŸŸï¼Œå®ƒæ²¡æœ‰NIRCamå¯¹åº”ç‰©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18705v1">PDF</a> 21 pages (15 figures, 3 tables). Submitted to A&A; see also the   companion work Vanzella et al. 2025. Comments are welcome</p>
<p><strong>Summary</strong>ï¼šåˆ©ç”¨JWST&#x2F;NIRSpecç§¯åˆ†åœºå…‰è°±æ³•å¯¹å®‡å®™å®çŸ³å¼§è¿›è¡Œäº†è§‚æµ‹ï¼Œè¯¥å¼§è¢«æ˜Ÿç³»å›¢SPT-CL J0615-5746å¼ºçƒˆæ”¾å¤§ã€‚é€šè¿‡NIRSpecæ£±é•œå…‰è°±æ³•ï¼ˆåˆ†è¾¨ç‡Rçº¦ä¸º30-300ï¼‰å¯¹è°±èŒƒå›´0.8-5.3Î¼mçš„å…­å°æ—¶ç§¯åˆ†ï¼Œå‘ç°ä½äºÎ»â‰ˆ1.3Î¼må¤„çš„LyÎ±è¿ç»­è°±æ–­è£‚ä»¥åŠä½äºÎ»&gt; 5.1Î¼mçš„å¼±å…‰å­¦HÎ²å’Œ[OIII]Î»4959å‘å°„çº¿ã€‚å…‰è°±æ‹Ÿåˆæ˜¾ç¤ºå®‡å®™å®çŸ³æ˜Ÿç³»å¤„äºçˆ†å‘åçš„é˜¶æ®µï¼Œè¿™æ˜¯ç›®å‰åœ¨è¿·æ·¬çŠ¶æ€ä¸‹è§‚å¯Ÿåˆ°çš„çº¢ç§»æœ€é«˜çš„ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>JWST&#x2F;NIRSpecç§¯åˆ†åœºå…‰è°±æ³•æ­ç¤ºäº†å®‡å®™å®çŸ³å¼§çš„è¯¦ç»†å…‰è°±ç‰¹å¾ã€‚</li>
<li>è§‚æµ‹åˆ°å¼ºçƒˆçš„LyÎ±è¿ç»­è°±æ–­è£‚ä»¥åŠå¼±å…‰å­¦HÎ²å’Œ[OIII]Î»4959å‘å°„çº¿ã€‚</li>
<li>è¯¥æ˜Ÿç³»å¤„äºçˆ†å‘åçš„é˜¶æ®µï¼Œæ˜¯ç›®å‰åœ¨è¿·æ·¬çŠ¶æ€ä¸‹è§‚å¯Ÿåˆ°çš„çº¢ç§»æœ€é«˜çš„ç³»ç»Ÿã€‚</li>
<li>æ˜Ÿç³»çš„ç©ºé—´è§£æå…‰è°±æ˜¾ç¤ºå­æˆåˆ†ç‰¹å¾ç›¸å¯¹å‡åŒ€ã€‚</li>
<li>è§‚å¯Ÿåˆ°çš„æ˜Ÿå›¢å¹´é¾„ä¸º7-30Myrï¼Œä¸JWST&#x2F;NIRCamå…‰åº¦è®¡å…ˆå‰å¯¹æ’æ˜Ÿé›†ç¾¤çš„ç‰©ç†æ€§è´¨æ¨å¯¼ä¸€è‡´ã€‚</li>
<li>æ›´æ–°åçš„é€é•œæ¨¡å‹é™åˆ¶äº†æ˜Ÿå›¢çš„å†…ç¦€å°ºå¯¸å’Œè´¨é‡ï¼Œè¯å®å®ƒä»¬æ¯”æœ¬åœ°æ’æ˜Ÿå½¢æˆæ˜Ÿç³»ä¸­çš„å…¸å‹æ˜Ÿå›¢æ›´ç´§å‡‘ã€å¯†åº¦æ›´é«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18705">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-29ba5a9f26d6b3db0bb6ac7b23acd58b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1f92f3d0e8ae68924843cf7313e22be3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0dd0752823c5d7b15847816aa4bd2028.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ac59bccdd908cff5eac92bf41b3494c9.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TTS-VAR-A-Test-Time-Scaling-Framework-for-Visual-Auto-Regressive-Generation"><a href="#TTS-VAR-A-Test-Time-Scaling-Framework-for-Visual-Auto-Regressive-Generation" class="headerlink" title="TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive   Generation"></a>TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive   Generation</h2><p><strong>Authors:Zhekai Chen, Ruihang Chu, Yukang Chen, Shiwei Zhang, Yujie Wei, Yingya Zhang, Xihui Liu</strong></p>
<p>Scaling visual generation models is essential for real-world content creation, yet requires substantial training and computational expenses. Alternatively, test-time scaling has garnered growing attention due to resource efficiency and promising performance. In this work, we present TTS-VAR, the first general test-time scaling framework for visual auto-regressive (VAR) models, modeling the generation process as a path searching problem. To dynamically balance computational efficiency with exploration capacity, we first introduce an adaptive descending batch size schedule throughout the causal generation process. Besides, inspired by VARâ€™s hierarchical coarse-to-fine multi-scale generation, our framework integrates two key components: (i) At coarse scales, we observe that generated tokens are hard for evaluation, possibly leading to erroneous acceptance of inferior samples or rejection of superior samples. Noticing that the coarse scales contain sufficient structural information, we propose clustering-based diversity search. It preserves structural variety through semantic feature clustering, enabling later selection on samples with higher potential. (ii) In fine scales, resampling-based potential selection prioritizes promising candidates using potential scores, which are defined as reward functions incorporating multi-scale generation history. Experiments on the powerful VAR model Infinity show a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights reveal that early-stage structural features effectively influence final quality, and resampling efficacy varies across generation scales. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ali-vilab/TTS-VAR">https://github.com/ali-vilab/TTS-VAR</a>. </p>
<blockquote>
<p>å°†è§†è§‰ç”Ÿæˆæ¨¡å‹è§„æ¨¡åŒ–å¯¹äºç°å®ä¸–ç•Œçš„å†…å®¹åˆ›å»ºè‡³å…³é‡è¦ï¼Œä½†è¿™éœ€è¦å¤§é‡çš„è®­ç»ƒå’Œè®¡ç®—èµ„æºã€‚ç›¸å¯¹è€Œè¨€ï¼Œæµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆtest-time scalingï¼‰å› èµ„æºæ•ˆç‡å’Œé«˜æ€§èƒ½è¡¨ç°è€Œå¤‡å—å…³æ³¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†TTS-VARï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªç”¨äºè§†è§‰è‡ªå›å½’ï¼ˆVARï¼‰æ¨¡å‹çš„é€šç”¨æµ‹è¯•æ—¶ç¼©æ”¾æ¡†æ¶ï¼Œå°†ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºè·¯å¾„æœç´¢é—®é¢˜ã€‚ä¸ºäº†åœ¨è®¡ç®—æ•ˆç‡å’Œæ¢ç´¢èƒ½åŠ›ä¹‹é—´å®ç°åŠ¨æ€å¹³è¡¡ï¼Œæˆ‘ä»¬é¦–å…ˆå¼•å…¥äº†è‡ªé€‚åº”é€’å‡æ‰¹å¤§å°è°ƒåº¦ä½œä¸ºå› æœç”Ÿæˆè¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚æ­¤å¤–ï¼Œå—VARå±‚æ¬¡åŒ–ä»ç²—åˆ°ç»†çš„å¤šå°ºåº¦ç”Ÿæˆçš„å¯å‘ï¼Œæˆ‘ä»¬çš„æ¡†æ¶é›†æˆäº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆiï¼‰åœ¨ç²—å°ºåº¦ä¸Šï¼Œæˆ‘ä»¬å‘ç°ç”Ÿæˆçš„ä»¤ç‰Œå¾ˆéš¾è¿›è¡Œè¯„ä¼°ï¼Œè¿™å¯èƒ½å¯¼è‡´åŠ£è´¨æ ·æœ¬çš„é”™è¯¯æ¥å—æˆ–ä¼˜è´¨æ ·æœ¬çš„æ‹’ç»ã€‚æ³¨æ„åˆ°ç²—å°ºåº¦åŒ…å«è¶³å¤Ÿçš„ç»“æ„ä¿¡æ¯ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºèšç±»çš„å¤šæ ·æ€§æœç´¢ã€‚å®ƒé€šè¿‡è¯­ä¹‰ç‰¹å¾èšç±»æ¥ä¿æŒç»“æ„å¤šæ ·æ€§ï¼Œä½¿åæœŸå¯¹æ½œåŠ›æ›´é«˜çš„æ ·æœ¬è¿›è¡Œç­›é€‰æˆä¸ºå¯èƒ½ã€‚ï¼ˆiiï¼‰åœ¨ç²¾ç»†å°ºåº¦ä¸Šï¼ŒåŸºäºé‡é‡‡æ ·çš„æ½œåŠ›é€‰æ‹©ä½¿ç”¨æ½œåŠ›åˆ†æ•°ä¼˜å…ˆä¼˜ç§€çš„å€™é€‰è€…ï¼Œæ½œåŠ›åˆ†æ•°è¢«å®šä¹‰ä¸ºç»“åˆå¤šå°ºåº¦ç”Ÿæˆå†å²çš„å¥–åŠ±å‡½æ•°ã€‚åœ¨å¼ºå¤§çš„VARæ¨¡å‹Infinityä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒGenEvalå¾—åˆ†ä»0.69æé«˜åˆ°0.75ï¼Œæå‡äº†æ˜¾è‘—çš„8.7%ã€‚å…³é”®çš„è§è§£è¡¨æ˜ï¼Œæ—©æœŸé˜¶æ®µçš„ç»“æ„ç‰¹å¾æœ‰æ•ˆåœ°å½±å“æœ€ç»ˆè´¨é‡ï¼Œé‡é‡‡æ ·çš„æ•ˆæœåœ¨ä¸åŒç”Ÿæˆå°ºåº¦ä¸Šæœ‰æ‰€ä¸åŒã€‚ä»£ç å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/ali-vilab/TTS-VAR%E3%80%82">https://github.com/ali-vilab/TTS-VARã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18537v1">PDF</a> 10 Tables, 9 Figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†è§‰è‡ªå›å½’ï¼ˆVARï¼‰æ¨¡å‹çš„æµ‹è¯•æ—¶ç¼©æ”¾æ¡†æ¶TTS-VARã€‚å®ƒé€šè¿‡è·¯å¾„æœç´¢é—®é¢˜å¯¹ç”Ÿæˆè¿‡ç¨‹è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”é€’å‡æ‰¹å¤§å°è°ƒåº¦æ¥å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œæ¢ç´¢èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šåœ¨ç²—å°ºåº¦ä¸Šï¼Œå®ƒè§‚å¯Ÿåˆ°ç”Ÿæˆçš„æ ‡è®°å¯¹äºè¯„ä¼°å¾ˆå›°éš¾ï¼Œå› æ­¤æå‡ºåŸºäºèšç±»çš„å¤šæ ·æ€§æœç´¢ä»¥ä¿ç•™ç»“æ„å¤šæ ·æ€§ï¼›åœ¨ç²¾ç»†å°ºåº¦ä¸Šï¼ŒåŸºäºé‡é‡‡æ ·çš„æ½œåŠ›é€‰æ‹©åˆ™ä¾§é‡äºä½¿ç”¨å¤šå°ºåº¦ç”Ÿæˆå†å²çš„å¥–åŠ±å‡½æ•°æ¥ä¼˜å…ˆé€‰æ‹©æœ‰å‰é€”çš„å€™é€‰è€…ã€‚å®éªŒæ˜¾ç¤ºï¼Œåœ¨å¼ºå¤§çš„VARæ¨¡å‹Infinityä¸Šï¼ŒTTS-VARçš„GenEvalå¾—åˆ†æé«˜äº†8.7%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTest-Time Scalingï¼‰æ˜¯ä¸€ç§é«˜æ•ˆä¸”æ€§èƒ½æœ‰ä¿è¯çš„èµ„æºä¼˜åŒ–æ–¹æ³•ã€‚å¯¹äºè§†è§‰è‡ªå›å½’ï¼ˆVARï¼‰æ¨¡å‹ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶TTS-VARç”¨äºæµ‹è¯•æ—¶ç¼©æ”¾ã€‚</li>
<li>TTS-VARæ¡†æ¶é€šè¿‡å°†ç”Ÿæˆè¿‡ç¨‹å»ºæ¨¡ä¸ºè·¯å¾„æœç´¢é—®é¢˜æ¥å¤„ç†è§†è§‰å†…å®¹ç”Ÿæˆä»»åŠ¡çš„å¤æ‚æ€§ã€‚å®ƒåŠ¨æ€å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œæ¢ç´¢èƒ½åŠ›ã€‚</li>
<li>åœ¨ç²—å°ºåº¦ä¸Šï¼ŒTTS-VARå¼•å…¥åŸºäºèšç±»çš„å¤šæ ·æ€§æœç´¢æ¥ä¿ç•™ç»“æ„å¤šæ ·æ€§ï¼Œè§£å†³ç”Ÿæˆçš„æ ‡è®°è¯„ä¼°å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>åœ¨ç²¾ç»†å°ºåº¦ä¸Šï¼ŒTTS-VARé‡‡ç”¨åŸºäºé‡é‡‡æ ·çš„æ½œåŠ›é€‰æ‹©æ–¹æ³•ï¼Œåˆ©ç”¨å¤šå°ºåº¦ç”Ÿæˆå†å²çš„å¥–åŠ±å‡½æ•°æ¥é€‰æ‹©æœ‰å‰é€”çš„å€™é€‰è€…ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å¼ºå¤§çš„VARæ¨¡å‹Infinityä¸Šåº”ç”¨TTS-VARåï¼ŒGenEvalå¾—åˆ†æ˜¾è‘—æé«˜ï¼ŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç ”ç©¶å¼ºè°ƒäº†æ—©æœŸé˜¶æ®µçš„ç»“æ„ç‰¹å¾å¯¹æœ€ç»ˆè´¨é‡çš„é‡è¦å½±å“ã€‚è¿™å¯¹äºè¿›ä¸€æ­¥æ”¹è¿›å’Œä¼˜åŒ–è§†è§‰å†…å®¹ç”Ÿæˆæ¨¡å‹å…·æœ‰é‡è¦çš„å¯ç¤ºæ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18537">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0a2af8b1db9dc3efbf0eb8c3d080246f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4a5a858df88a867c9c1e1c79602da18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd839152966a40ad14d8cdeb9abfbdba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a61fa3b3689f41d1b285f904a3bb6ce5.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Yume-An-Interactive-World-Generation-Model"><a href="#Yume-An-Interactive-World-Generation-Model" class="headerlink" title="Yume: An Interactive World Generation Model"></a>Yume: An Interactive World Generation Model</h2><p><strong>Authors:Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, Kaipeng Zhang</strong></p>
<p>Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \sekai to train \method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on <a target="_blank" rel="noopener" href="https://github.com/stdstu12/YUME">https://github.com/stdstu12/YUME</a>. Yume will update monthly to achieve its original goal. Project page: <a target="_blank" rel="noopener" href="https://stdstu12.github.io/YUME-Project/">https://stdstu12.github.io/YUME-Project/</a>. </p>
<blockquote>
<p>Yumeæ—¨åœ¨åˆ©ç”¨å›¾åƒã€æ–‡æœ¬æˆ–è§†é¢‘åˆ›å»ºäº’åŠ¨ã€é€¼çœŸã€åŠ¨æ€çš„ä¸–ç•Œï¼Œå…è®¸ä½¿ç”¨å¤–å›´è®¾å¤‡æˆ–ç¥ç»ä¿¡å·è¿›è¡Œæ¢ç´¢å’Œæ§åˆ¶ã€‚åœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†\methodçš„é¢„è§ˆç‰ˆï¼Œå®ƒå¯ä»¥ä»è¾“å…¥å›¾åƒä¸­åˆ›å»ºä¸€ä¸ªåŠ¨æ€ä¸–ç•Œï¼Œå¹¶ä½¿ç”¨é”®ç›˜æ“ä½œæ¢ç´¢è¿™ä¸ªä¸–ç•Œã€‚ä¸ºäº†å®ç°é«˜ä¿çœŸå’Œäº’åŠ¨çš„è§†é¢‘ä¸–ç•Œç”Ÿæˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç”±å››ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼ŒåŒ…æ‹¬ç›¸æœºè¿åŠ¨é‡åŒ–ã€è§†é¢‘ç”Ÿæˆæ¶æ„ã€é«˜çº§é‡‡æ ·å™¨å’Œæ¨¡å‹åŠ é€Ÿã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨é”®ç›˜è¾“å…¥å¯¹ç›¸æœºè¿åŠ¨è¿›è¡Œé‡åŒ–ï¼Œä»¥å®ç°ç¨³å®šçš„è®­ç»ƒå’Œç”¨æˆ·å‹å¥½çš„äº’åŠ¨ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥äº†å¸¦æœ‰è®°å¿†æ¨¡å—çš„Masked Video Diffusion Transformerï¼ˆMVDTï¼‰ä»¥è‡ªå›å½’æ–¹å¼è¿›è¡Œæ— é™è§†é¢‘ç”Ÿæˆã€‚ä¹‹åï¼Œæˆ‘ä»¬å‘é‡‡æ ·å™¨å¼•å…¥äº†æ— éœ€è®­ç»ƒçš„Anti-Artifact Mechanismï¼ˆAAMï¼‰å’ŒåŸºäºéšæœºå¾®åˆ†æ–¹ç¨‹çš„Time Travel Samplingï¼ˆTTS-SDEï¼‰ï¼Œä»¥æé«˜è§†è§‰è´¨é‡å’Œæ›´ç²¾ç¡®çš„æ§åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡ååŒä¼˜åŒ–å¯¹æŠ—è’¸é¦å’Œç¼“å­˜æœºåˆ¶æ¥ç ”ç©¶æ¨¡å‹åŠ é€Ÿã€‚æˆ‘ä»¬ä½¿ç”¨é«˜è´¨é‡çš„ä¸–ç•Œæ¢ç´¢æ•°æ®é›†\sekaiæ¥è®­ç»ƒ\methodï¼Œå®ƒåœ¨å„ç§åœºæ™¯å’Œåº”ç”¨ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚æ‰€æœ‰æ•°æ®ã€ä»£ç åº“å’Œæ¨¡å‹æƒé‡å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/stdstu12/YUME">https://github.com/stdstu12/YUME</a>ä¸Šæ‰¾åˆ°ã€‚Yumeå°†æ¯æœˆæ›´æ–°ä»¥è¾¾æˆå…¶åŸå§‹ç›®æ ‡ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://stdstu12.github.io/YUME-Project/">https://stdstu12.github.io/YUME-Project/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17744v1">PDF</a> </p>
<p><strong>Summary</strong><br>     ä¼˜æ¢¦é¡¹ç›®æ—¨åœ¨ä½¿ç”¨å›¾åƒã€æ–‡æœ¬æˆ–è§†é¢‘åˆ›å»ºäº’åŠ¨ã€çœŸå®ä¸”åŠ¨æ€çš„ä¸–ç•Œï¼Œå¯é€šè¿‡å¤–å›´è®¾å¤‡æˆ–ç¥ç»ä¿¡å·è¿›è¡Œæ¢ç´¢å’Œæ§åˆ¶ã€‚æœ¬æ¬¡æŠ¥å‘Šé¢„è§ˆç‰ˆçš„æ–¹æ³•é€šè¿‡è¾“å…¥å›¾åƒåˆ›å»ºåŠ¨æ€ä¸–ç•Œï¼Œå…è®¸ä½¿ç”¨é”®ç›˜æ“ä½œæ¢ç´¢ä¸–ç•Œã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ¡†æ¶ï¼ŒåŒ…å«ç›¸æœºåŠ¨ä½œé‡åŒ–ã€è§†é¢‘ç”Ÿæˆæ¶æ„ã€é«˜çº§é‡‡æ ·å™¨å’Œæ¨¡å‹åŠ é€Ÿç­‰å››ä¸ªä¸»è¦ç»„ä»¶ã€‚æ­¤å¤–ï¼Œè¿˜æ¢è®¨äº†é€šè¿‡å¯¹æŠ—è’¸é¦å’Œç¼“å­˜æœºåˆ¶çš„ååŒä¼˜åŒ–æ¥åŠ é€Ÿæ¨¡å‹ã€‚ä¼˜æ¢¦æ•°æ®é›†çš„é«˜è´¨é‡ä¸–ç•Œæ¢ç´¢æˆæœæ˜¾è‘—ï¼Œæ¨¡å‹æƒé‡åŠèµ„æ–™å‡å¯ä»å…¬å¼€é“¾æ¥è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼˜æ¢¦é¡¹ç›®çš„æ ¸å¿ƒç›®æ ‡æ˜¯ä½¿ç”¨å›¾åƒã€æ–‡æœ¬æˆ–è§†é¢‘æ¥åˆ›å»ºä¸€ä¸ªäº’åŠ¨ã€çœŸå®ä¸”åŠ¨æ€çš„ä¸–ç•Œã€‚</li>
<li>é¡¹ç›®é€šè¿‡è¾“å…¥å›¾åƒç”ŸæˆåŠ¨æ€ä¸–ç•Œï¼Œå¹¶å…è®¸ç”¨æˆ·ä½¿ç”¨é”®ç›˜æ“ä½œè¿›è¡Œæ¢ç´¢ã€‚</li>
<li>é¡¹ç›®é‡‡ç”¨åŒ…å«å››ä¸ªä¸»è¦ç»„ä»¶çš„æ¡†æ¶æ¥å®ç°è¿™ä¸€åŠŸèƒ½ï¼ŒåŒ…æ‹¬ç›¸æœºåŠ¨ä½œé‡åŒ–ã€è§†é¢‘ç”Ÿæˆæ¶æ„ã€é«˜çº§é‡‡æ ·å™¨å’Œæ¨¡å‹åŠ é€Ÿã€‚</li>
<li>é‡‡ç”¨Masked Video Diffusion Transformer (MVDT)è¿›è¡Œæ— é™è§†é¢‘çš„è‡ªå›å½’ç”Ÿæˆã€‚</li>
<li>å¼•å…¥è®­ç»ƒå¤–çš„Anti-Artifact Mechanism (AAM)å’ŒåŸºäºéšæœºå¾®åˆ†æ–¹ç¨‹çš„Time Travel Sampling (TTS-SDE)ä»¥æé«˜è§†è§‰è´¨é‡å’Œæ§åˆ¶ç²¾åº¦ã€‚</li>
<li>é¡¹ç›®ä½¿ç”¨ä¼˜æ¢¦æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å¤šç§åœºæ™¯å’Œåº”ç”¨ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17744">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3308df76dfe19ede4e7b8e81da506ff5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1fb4d9ca9cd43e726aab8c5cd061a8e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="AI-Telephone-Surveying-Automating-Quantitative-Data-Collection-with-an-AI-Interviewer"><a href="#AI-Telephone-Surveying-Automating-Quantitative-Data-Collection-with-an-AI-Interviewer" class="headerlink" title="AI Telephone Surveying: Automating Quantitative Data Collection with an   AI Interviewer"></a>AI Telephone Surveying: Automating Quantitative Data Collection with an   AI Interviewer</h2><p><strong>Authors:Danny D. Leybzon, Shreyas Tirumala, Nishant Jain, Summer Gillen, Michael Jackson, Cameron McPhee, Jennifer Schmidt</strong></p>
<p>With the rise of voice-enabled artificial intelligence (AI) systems, quantitative survey researchers have access to a new data-collection mode: AI telephone surveying. By using AI to conduct phone interviews, researchers can scale quantitative studies while balancing the dual goals of human-like interactivity and methodological rigor. Unlike earlier efforts that used interactive voice response (IVR) technology to automate these surveys, voice AI enables a more natural and adaptive respondent experience as it is more robust to interruptions, corrections, and other idiosyncrasies of human speech.   We built and tested an AI system to conduct quantitative surveys based on large language models (LLM), automatic speech recognition (ASR), and speech synthesis technologies. The system was specifically designed for quantitative research, and strictly adhered to research best practices like question order randomization, answer order randomization, and exact wording.   To validate the systemâ€™s effectiveness, we deployed it to conduct two pilot surveys with the SSRS Opinion Panel and followed-up with a separate human-administered survey to assess respondent experiences. We measured three key metrics: the survey completion rates, break-off rates, and respondent satisfaction scores. Our results suggest that shorter instruments and more responsive AI interviewers may contribute to improvements across all three metrics studied. </p>
<blockquote>
<p>éšç€è¯­éŸ³æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿçš„å…´èµ·ï¼Œå®šé‡è°ƒæŸ¥ç ”ç©¶äººå‘˜è·å¾—äº†ä¸€ç§æ–°çš„æ•°æ®æ”¶é›†æ¨¡å¼ï¼šAIç”µè¯è°ƒæŸ¥ã€‚ç ”ç©¶äººå‘˜å¯ä»¥é€šè¿‡AIè¿›è¡Œç”µè¯é‡‡è®¿ï¼Œä»è€Œåœ¨æ‰©å¤§å®šé‡ç ”ç©¶è§„æ¨¡çš„åŒæ—¶ï¼Œå¹³è¡¡äººæ€§åŒ–çš„äº’åŠ¨å’Œæ–¹æ³•çš„ä¸¥è°¨æ€§ã€‚ä¸æ—©æœŸä½¿ç”¨äº¤äº’å¼è¯­éŸ³åº”ç­”ï¼ˆIVRï¼‰æŠ€æœ¯è‡ªåŠ¨è¿›è¡Œæ­¤ç±»è°ƒæŸ¥ä¸åŒï¼Œè¯­éŸ³AIèƒ½å¤Ÿæä¾›æ›´è‡ªç„¶å’Œé€‚åº”æ€§æ›´å¼ºçš„å—è®¿è€…ä½“éªŒï¼Œå› ä¸ºå®ƒæ›´èƒ½é€‚åº”ä¸­æ–­ã€æ›´æ­£å’Œäººç±»è¯­è¨€çš„å…¶å®ƒç‰¹æ€§ã€‚æˆ‘ä»¬æ„å»ºå¹¶æµ‹è¯•äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³åˆæˆæŠ€æœ¯çš„AIç³»ç»Ÿæ¥è¿›è¡Œå®šé‡è°ƒæŸ¥ã€‚è¯¥ç³»ç»Ÿæ˜¯ä¸“é—¨ä¸ºå®šé‡ç ”ç©¶è®¾è®¡çš„ï¼Œä¸¥æ ¼éµå®ˆæœ€ä½³ç ”ç©¶å®è·µï¼Œå¦‚éšæœºé—®é¢˜é¡ºåºã€éšæœºç­”æ¡ˆé¡ºåºå’Œç²¾ç¡®æªè¾ç­‰ã€‚ä¸ºäº†éªŒè¯ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å°†å…¶éƒ¨ç½²ç”¨äºè¿›è¡Œä¸¤é¡¹è¯•ç‚¹è°ƒæŸ¥ï¼Œå¹¶éšåè¿›è¡Œä¸€æ¬¡å•ç‹¬çš„äººå·¥ç®¡ç†è°ƒæŸ¥ä»¥è¯„ä¼°å—è®¿è€…çš„ä½“éªŒã€‚æˆ‘ä»¬æµ‹é‡äº†ä¸‰ä¸ªå…³é”®æŒ‡æ ‡ï¼šè°ƒæŸ¥å®Œæˆç‡ã€ä¸­æ–­ç‡å’Œå—è®¿è€…æ»¡æ„åº¦å¾—åˆ†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ›´çŸ­çš„å·¥å…·å’Œååº”æ›´è¿…é€Ÿçš„AIé¢è¯•å®˜å¯èƒ½å¯¹è¿™ä¸‰å¤§æŒ‡æ ‡æœ‰æ‰€æ”¹å–„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17718v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>AIç”µè¯è°ƒæŸ¥â€”â€”é‡åŒ–ç ”ç©¶çš„æ–°æ¨¡å¼ã€‚åˆ©ç”¨è¯­éŸ³äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ç³»ç»Ÿï¼Œç ”ç©¶è€…å¯è§„æ¨¡åŒ–å¼€å±•å…·æœ‰äººç±»äº¤äº’æ€§çš„å®šé‡ç ”ç©¶ï¼Œå®ç°è¯­éŸ³è¯†åˆ«çš„è‡ªé€‚åº”æ€§ä¸è¿ç»­æ€§ã€‚é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åŠè¯­éŸ³åˆæˆæŠ€æœ¯æ„å»ºçš„AIè°ƒæŸ¥ç³»ç»Ÿéµå¾ªç ”ç©¶æœ€ä½³å®è·µåŸåˆ™ï¼Œç»è¿‡åˆæ­¥æµ‹è¯•æ˜¾ç¤ºï¼Œè¾ƒçŸ­çš„é—®å·å’Œæ›´çµæ´»çš„AIé‡‡è®¿è€…å¯èƒ½æœ‰åŠ©äºæé«˜è°ƒæŸ¥å®Œæˆç‡ã€é™ä½ä¸­æ–­ç‡å’Œæé«˜å—è®¿è€…æ»¡æ„åº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”µè¯è°ƒæŸ¥æˆä¸ºé‡åŒ–ç ”ç©¶çš„æ–°æ•°æ®æ”¶é›†æ¨¡å¼ã€‚</li>
<li>AIç³»ç»Ÿå¯å¹³è¡¡äººç±»äº¤äº’æ€§å’Œæ–¹æ³•è®ºä¸¥è°¨æ€§ã€‚</li>
<li>ä¸æ—©æœŸä½¿ç”¨çš„äº¤äº’å¼è¯­éŸ³åº”ç­”ï¼ˆIVRï¼‰æŠ€æœ¯ç›¸æ¯”ï¼Œè¯­éŸ³AIæä¾›æ›´è‡ªç„¶ã€æ›´é€‚åº”ä¸ªä½“å·®å¼‚çš„å—è®¿è€…ä½“éªŒã€‚</li>
<li>é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰åŠè¯­éŸ³åˆæˆæŠ€æœ¯æ„å»ºAIè°ƒæŸ¥ç³»ç»Ÿã€‚</li>
<li>AIè°ƒæŸ¥ç³»ç»Ÿéµå¾ªç ”ç©¶æœ€ä½³å®è·µåŸåˆ™ï¼Œå¦‚é—®é¢˜é¡ºåºéšæœºåŒ–ã€ç­”æ¡ˆé¡ºåºéšæœºåŒ–åŠç²¾ç¡®æªè¾ã€‚</li>
<li>é€šè¿‡åˆæ­¥æµ‹è¯•æ˜¾ç¤ºï¼Œè¾ƒçŸ­çš„é—®å·å’Œæ›´çµæ´»çš„AIé‡‡è®¿è€…æœ‰åŠ©äºæé«˜è°ƒæŸ¥å®Œæˆç‡ã€é™ä½ä¸­æ–­ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17718">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6d5943c8dcb061a4928e4cd44a733fb2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a52022e4d8406778e351022551b33fce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5c59e7176ed1f7cc718a6aa019544463.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2c51408fc524523349a5e934b2fd8e4f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ed87625628edd096e92ec9c5a95aeda8.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Synthetic-Voice-Data-for-Automatic-Speech-Recognition-in-African-Languages"><a href="#Synthetic-Voice-Data-for-Automatic-Speech-Recognition-in-African-Languages" class="headerlink" title="Synthetic Voice Data for Automatic Speech Recognition in African   Languages"></a>Synthetic Voice Data for Automatic Speech Recognition in African   Languages</h2><p><strong>Authors:Brian DeRenzi, Anna Dixon, Mohamed Aymane Farhi, Christian Resch</strong></p>
<p>Speech technology remains out of reach for most of the over 2300 languages in Africa. We present the first systematic assessment of large-scale synthetic voice corpora for African ASR. We apply a three-step process: LLM-driven text creation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages for which we create synthetic text achieved readability scores above 5 out of 7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created more than 2,500 hours of synthetic voice data at below 1% of the cost of real data. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h synthetic Hausa matched a 500h real-data-only baseline, while 579h real and 450h to 993h synthetic data created the best performance. We also present gender-disaggregated ASR performance evaluation. For very low-resource languages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2 real-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on some evaluation data, but not on others. Investigating intercoder reliability, ASR errors and evaluation datasets revealed the need for more robust reviewer protocols and more accurate evaluation data. All data and models are publicly released to invite further work to improve synthetic data for African languages. </p>
<blockquote>
<p>å¯¹äºéæ´²çš„2300å¤šç§è¯­è¨€ä¸­çš„å¤§å¤šæ•°è€Œè¨€ï¼Œè¯­éŸ³æŠ€æœ¯ä»æ˜¯éš¾ä»¥è§¦åŠçš„é¢†åŸŸã€‚æˆ‘ä»¬å¯¹éæ´²ASRçš„å¤§è§„æ¨¡åˆæˆè¯­éŸ³è¯­æ–™åº“è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿæ€§è¯„ä¼°ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸‰æ­¥è¿‡ç¨‹ï¼šåŸºäºLLMçš„æ–‡æœ¬åˆ›å»ºã€TTSè¯­éŸ³åˆæˆå’ŒASRå¾®è°ƒã€‚åœ¨æˆ‘ä»¬åˆ›å»ºçš„åˆæˆæ–‡æœ¬ä¸­ï¼Œæœ‰å…«ç§è¯­è¨€çš„å¯è¯»æ€§å¾—åˆ†é«˜äºååˆ†ä¹‹ä¸ƒã€‚æˆ‘ä»¬å¯¹ä¸‰ç§è¯­è¨€ï¼ˆè±ªè¨è¯­ã€å¾·æ´›å°”è¯­ã€å¥‘ç»´è¯­ï¼‰çš„ASRè¿›è¡Œäº†æ”¹è¿›ï¼Œå¹¶ä»¥ä½äºçœŸå®æ•°æ®æˆæœ¬çš„ç™¾åˆ†ä¹‹ä¸€åˆ›å»ºäº†è¶…è¿‡2500å°æ—¶çš„åˆæˆè¯­éŸ³æ•°æ®ã€‚ä½¿ç”¨250å°æ—¶çœŸå®æ•°æ®å’Œ25shåˆæˆè±ªè¨è¯­çš„Wav2Vec-BERT-2.0æ¨¡å‹å¾®è°ƒï¼ŒåŒ¹é…äº†ä»…ä½¿ç”¨500å°æ—¶çœŸå®æ•°æ®çš„åŸºçº¿ï¼Œè€Œä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹å’Œåˆæˆçš„æ•°æ®åˆ™å–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†æŒ‰æ€§åˆ«åˆ’åˆ†çš„ASRæ€§èƒ½è¯„ä¼°ã€‚å¯¹äºèµ„æºéå¸¸åŒ®ä¹çš„è¯­è¨€ï¼Œæ”¶ç›Šæœ‰æ‰€ä¸åŒï¼šå¥‘ç»´è¯­çš„è¯é”™è¯¯ç‡ç›¸å¯¹é™ä½äº†çº¦ç™¾åˆ†ä¹‹å…­ï¼›è±ªè¨è¯­çš„çœŸå®æ•°æ®ä¸åˆæˆæ•°æ®æ¯”ä¾‹ä¸º1:1æ—¶ï¼Œåœ¨æŸäº›è¯„ä¼°æ•°æ®ä¸Šæ˜¾ç¤ºå‡ºç›¸ä¼¼çš„æ”¹è¿›æ•ˆæœï¼Œä½†åœ¨å…¶ä»–æ–¹é¢åˆ™æ²¡æœ‰ã€‚è°ƒæŸ¥ç¼–ç å‘˜ä¹‹é—´çš„å¯é æ€§ã€ASRé”™è¯¯å’Œè¯„ä¼°æ•°æ®é›†æ­ç¤ºäº†éœ€è¦æ›´ç¨³å¥çš„å®¡æŸ¥åè®®å’Œæ›´å‡†ç¡®çš„è¯„ä¼°æ•°æ®ã€‚æ‰€æœ‰æ•°æ®æ¨¡å‹å’Œåˆæˆè¯­éŸ³éƒ½è¢«å…¬å¼€å‘å¸ƒï¼Œè¯šé‚€æ›´å¤šå·¥ä½œæ¥æé«˜éæ´²è¯­è¨€çš„åˆæˆæ•°æ®è´¨é‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17578v1">PDF</a> 29 pages incl. appendix, 8 tables, 5 figures. Authors are listed in   alphabetical order</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹éæ´²è¯­è¨€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç³»ç»Ÿçš„åˆæˆè¯­éŸ³è¯­æ–™åº“çš„ç³»ç»Ÿè¯„ä¼°ã€‚é€šè¿‡ä¸‰æ­¥æ³•åˆ›å»ºåˆæˆè¯­éŸ³ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨æ–‡æœ¬ç”Ÿæˆã€æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰è¯­éŸ³åˆæˆå’ŒASRå¾®è°ƒã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå…«ç§è¯­è¨€çš„åˆæˆæ–‡æœ¬å¯è¯»æ€§å¾—åˆ†è¶…è¿‡5åˆ†ï¼ˆæ»¡åˆ†7åˆ†ï¼‰ã€‚é€šè¿‡åˆæˆè¯­éŸ³æ•°æ®æ”¹å–„äº†ä¸‰ç§è¯­è¨€çš„ASRæ€§èƒ½ï¼Œå¹¶åˆ›å»ºäº†è¶…è¿‡2,500å°æ—¶åˆæˆè¯­éŸ³æ•°æ®ï¼Œæˆæœ¬ä»…ä¸ºçœŸå®æ•°æ®çš„1%ã€‚åŒæ—¶ï¼Œå…¬å¼€æ‰€æœ‰æ•°æ®æ¨¡å‹ä»¥é‚€è¯·æ›´å¤šç ”ç©¶è€…å’Œå¼€å‘è€…å…±åŒæ”¹è¿›éæ´²è¯­è¨€çš„åˆæˆæ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é’ˆå¯¹éæ´²è¶…è¿‡2300ç§è¯­è¨€çš„è¯­éŸ³æŠ€æœ¯å°šæœªæ™®åŠã€‚</li>
<li>å¯¹éæ´²è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è¿›è¡Œäº†å¤§è§„æ¨¡åˆæˆè¯­éŸ³è¯­æ–™åº“çš„ç³»ç»Ÿè¯„ä¼°ã€‚</li>
<li>é€šè¿‡LLMé©±åŠ¨çš„æ–‡æœ¬åˆ›å»ºã€TTSè¯­éŸ³åˆæˆå’ŒASRå¾®è°ƒçš„ä¸‰æ­¥è¿‡ç¨‹ï¼Œå…«ç§è¯­è¨€çš„åˆæˆæ–‡æœ¬å¯è¯»æ€§è¾ƒé«˜ã€‚</li>
<li>åˆæˆè¯­éŸ³æ•°æ®æ”¹å–„äº†ä¸‰ç§è¯­è¨€çš„ASRæ€§èƒ½ï¼Œä¸”æˆæœ¬è¾ƒä½ã€‚</li>
<li>ä½¿ç”¨çœŸå®ä¸åˆæˆæ•°æ®çš„ç»„åˆå¯¹ASRæ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°æœ€ä½³æ€§èƒ½æ˜¯åœ¨ä¸€å®šæ¯”ä¾‹çš„çœŸå®å’Œåˆæˆæ•°æ®ä¸‹å®ç°çš„ã€‚</li>
<li>è¿›è¡Œäº†æ€§åˆ«åˆ†ç±»çš„ASRæ€§èƒ½è¯„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17578">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3bbb476d6cdb4fc0f248e5cde0a27c2b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d9650ae5e0e5e820eef58a864bade26.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8344fa85c36838535560de114d289364.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80d51b65a775815f5ba585717efb6cf7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-48877a6f640e2452fa561649b0e27cfa.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="SplitMeanFlow-Interval-Splitting-Consistency-in-Few-Step-Generative-Modeling"><a href="#SplitMeanFlow-Interval-Splitting-Consistency-in-Few-Step-Generative-Modeling" class="headerlink" title="SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative   Modeling"></a>SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative   Modeling</h2><p><strong>Authors:Yi Guo, Wei Wang, Zhihang Yuan, Rong Cao, Kuan Chen, Zhengyang Chen, Yuanyuan Huo, Yang Zhang, Yuping Wang, Shouda Liu, Yuxuan Wang</strong></p>
<p>Generative models like Flow Matching have achieved state-of-the-art performance but are often hindered by a computationally expensive iterative sampling process. To address this, recent work has focused on few-step or one-step generation by learning the average velocity field, which directly maps noise to data. MeanFlow, a leading method in this area, learns this field by enforcing a differential identity that connects the average and instantaneous velocities. In this work, we argue that this differential formulation is a limiting special case of a more fundamental principle. We return to the first principles of average velocity and leverage the additivity property of definite integrals. This leads us to derive a novel, purely algebraic identity we term Interval Splitting Consistency. This identity establishes a self-referential relationship for the average velocity field across different time intervals without resorting to any differential operators. Based on this principle, we introduce SplitMeanFlow, a new training framework that enforces this algebraic consistency directly as a learning objective. We formally prove that the differential identity at the core of MeanFlow is recovered by taking the limit of our algebraic consistency as the interval split becomes infinitesimal. This establishes SplitMeanFlow as a direct and more general foundation for learning average velocity fields. From a practical standpoint, our algebraic approach is significantly more efficient, as it eliminates the need for JVP computations, resulting in simpler implementation, more stable training, and broader hardware compatibility. One-step and two-step SplitMeanFlow models have been successfully deployed in large-scale speech synthesis products (such as Doubao), achieving speedups of 20x. </p>
<blockquote>
<p>æµåŒ¹é…ç­‰ç”Ÿæˆæ¨¡å‹è™½ç„¶å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†å¾€å¾€å—åˆ°è®¡ç®—æˆæœ¬é«˜æ˜‚çš„è¿­ä»£é‡‡æ ·è¿‡ç¨‹çš„åˆ¶çº¦ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¿‘æœŸçš„ç ”ç©¶é›†ä¸­åœ¨é€šè¿‡å­¦ä¹ å¹³å‡é€Ÿåº¦åœºè¿›è¡Œå‡ æ­¥æˆ–ä¸€æ­¥ç”Ÿæˆï¼Œè¯¥åœºç›´æ¥å°†å™ªå£°æ˜ å°„åˆ°æ•°æ®ã€‚å…¶ä¸­ï¼ŒMeanFlowæ˜¯è¿™ä¸ªé¢†åŸŸçš„ä¸€ç§é¢†å…ˆæ–¹æ³•ï¼Œå®ƒé€šè¿‡å¼ºåˆ¶å®æ–½ä¸€ä¸ªè¿æ¥å¹³å‡é€Ÿåº¦å’Œç¬æ—¶é€Ÿåº¦çš„å·®å¼‚æ’ç­‰å¼æ¥å­¦ä¹ æ­¤åœºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™ç§å·®å¼‚å…¬å¼æ˜¯ä¸€ä¸ªæ›´æ ¹æœ¬åŸåˆ™çš„é™åˆ¶æ€§ç‰¹æ®Šæƒ…å†µã€‚æˆ‘ä»¬å›åˆ°å¹³å‡é€Ÿåº¦çš„ç¬¬ä¸€åŸç†ï¼Œå¹¶åˆ©ç”¨å®šç§¯åˆ†çš„å¯åŠ æ€§å±æ€§ã€‚è¿™å¼•å¯¼æˆ‘ä»¬æ¨å¯¼å‡ºæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œåŒºé—´åˆ†å‰²ä¸€è‡´æ€§â€çš„æ–°å‹çº¯ä»£æ•°æ’ç­‰å¼ã€‚è¯¥æ’ç­‰å¼åœ¨ä¸åŒæ—¶é—´é—´éš”ä¸Šå»ºç«‹äº†å¹³å‡é€Ÿåº¦åœºçš„è‡ªå¼•ç”¨å…³ç³»ï¼Œæ— éœ€ä½¿ç”¨ä»»ä½•å¾®åˆ†ç®—å­ã€‚åŸºäºè¿™ä¸€åŸç†ï¼Œæˆ‘ä»¬å¼•å…¥äº†SplitMeanFlowï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œç›´æ¥å¼ºåˆ¶å®æ–½è¿™ç§ä»£æ•°ä¸€è‡´æ€§ä½œä¸ºå­¦ä¹ ç›®æ ‡ã€‚æˆ‘ä»¬æ­£å¼è¯æ˜ï¼Œå½“åŒºé—´åˆ†å‰²å˜å¾—æ— ç©·å°æ—¶ï¼ŒMeanFlowæ ¸å¿ƒçš„å¾®åˆ†æ’ç­‰å¼æ˜¯é€šè¿‡æˆ‘ä»¬çš„ä»£æ•°ä¸€è‡´æ€§å¾—åˆ°çš„ã€‚è¿™ç¡®ç«‹äº†SplitMeanFlowä½œä¸ºå­¦ä¹ å¹³å‡é€Ÿåº¦åœºçš„ç›´æ¥å’Œæ›´ä¸€èˆ¬çš„åŸºçŸ³ã€‚ä»å®é™…çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬çš„ä»£æ•°æ–¹æ³•æ›´åŠ é«˜æ•ˆï¼Œå› ä¸ºå®ƒæ¶ˆé™¤äº†å¯¹JVPè®¡ç®—çš„éœ€æ±‚ï¼Œå¯¼è‡´å®ç°æ›´ç®€å•ã€è®­ç»ƒæ›´ç¨³å®šã€ç¡¬ä»¶å…¼å®¹æ€§æ›´å¹¿ã€‚ä¸€æ­¥å’Œä¸¤æ­¥SplitMeanFlowæ¨¡å‹å·²æˆåŠŸéƒ¨ç½²åœ¨å¤§å‹è¯­éŸ³åˆæˆäº§å“ï¼ˆå¦‚Doubaoï¼‰ä¸­ï¼Œå®ç°äº†20å€çš„åŠ é€Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16884v1">PDF</a> Tech Report</p>
<p><strong>Summary</strong><br>     è¿‘æœŸå·¥ä½œé€šè¿‡å¹³å‡é€Ÿåº¦åœºçš„å­¦ä¹ æ¥è§£å†³ç”Ÿæˆæ¨¡å‹å¦‚Flow Matchingçš„è®¡ç®—æ˜‚è´µé—®é¢˜ï¼Œç›´æ¥å®ç°å™ªå£°åˆ°æ•°æ®çš„æ˜ å°„ã€‚æœ¬ç ”ç©¶é‡æ–°å®¡è§†å¹³å‡é€Ÿåº¦çš„åŸºç¡€åŸç†ï¼Œåˆ©ç”¨å®šç§¯åˆ†çš„å¯åŠ æ€§å±æ€§ï¼Œæ¨å¯¼å‡ºä¸€ç§æ–°çš„ä»£æ•°æ’ç­‰å¼â€”â€”åŒºé—´åˆ†å‰²ä¸€è‡´æ€§ã€‚åŸºäºæ­¤åŸåˆ™ï¼Œæˆ‘ä»¬å¼•å…¥SplitMeanFlowè®­ç»ƒæ¡†æ¶ï¼Œç›´æ¥å¼ºåˆ¶å®æ–½è¿™ç§ä»£æ•°ä¸€è‡´æ€§ä½œä¸ºå­¦ä¹ ç›®æ ‡ã€‚SplitMeanFlowæ–¹æ³•æ›´ä¸ºé«˜æ•ˆï¼Œæ— éœ€è®¡ç®—JVPï¼Œç®€åŒ–äº†å®ç°ï¼Œæé«˜äº†è®­ç»ƒç¨³å®šæ€§å¹¶æ‰©å¤§äº†ç¡¬ä»¶å…¼å®¹æ€§ã€‚SplitMeanFlowæ¨¡å‹åœ¨å¤§å‹è¯­éŸ³åˆæˆäº§å“ï¼ˆå¦‚Doubaoï¼‰ä¸­éƒ¨ç½²ï¼Œå®ç°äº†20å€çš„åŠ é€Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆæ¨¡å‹å¦‚Flow Matchingè™½è¾¾åˆ°å…ˆè¿›æ€§èƒ½ï¼Œä½†è®¡ç®—æˆæœ¬é«˜æ˜‚ï¼Œè¿‘æœŸå·¥ä½œé€šè¿‡å¹³å‡é€Ÿåº¦åœºå­¦ä¹ æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>MeanFlowæ–¹æ³•é€šè¿‡å­¦ä¹ å¹³å‡é€Ÿåº¦åœºçš„å¾®åˆ†æ’ç­‰å¼æ¥å®ç°ï¼Œä½†æœ¬ç ”ç©¶è®¤ä¸ºè¿™ç§æ–¹æ³•æ˜¯ä¸€ç§ç‰¹æ®Šæƒ…å½¢ã€‚</li>
<li>åˆ©ç”¨å¹³å‡é€Ÿåº¦çš„åŸºç¡€åŸç†å’Œå®šç§¯åˆ†çš„å¯åŠ æ€§å±æ€§ï¼Œæ¨å¯¼å‡ºæ–°çš„ä»£æ•°æ’ç­‰å¼â€”â€”åŒºé—´åˆ†å‰²ä¸€è‡´æ€§ã€‚</li>
<li>åŸºäºæ–°çš„ä»£æ•°æ’ç­‰å¼ï¼Œæå‡ºSplitMeanFlowè®­ç»ƒæ¡†æ¶ï¼Œæ›´é«˜æ•ˆåœ°å­¦ä¹ å¹³å‡é€Ÿåº¦åœºã€‚</li>
<li>SplitMeanFlowæ–¹æ³•é¿å…äº†å¾®åˆ†è¿ç®—ï¼Œç®€åŒ–å®ç°ã€æé«˜è®­ç»ƒç¨³å®šæ€§å¹¶å¢å¼ºç¡¬ä»¶å…¼å®¹æ€§ã€‚</li>
<li>SplitMeanFlowæ¨¡å‹å·²åœ¨å¤§å‹è¯­éŸ³åˆæˆäº§å“ä¸­æˆåŠŸéƒ¨ç½²ï¼Œå¦‚Doubaoï¼Œå®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16884">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-6c66100e365e4dacf28be247234fe831.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4576424b817a184400aeab138e24403f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="EchoVoices-Preserving-Generational-Voices-and-Memories-for-Seniors-and-Children"><a href="#EchoVoices-Preserving-Generational-Voices-and-Memories-for-Seniors-and-Children" class="headerlink" title="EchoVoices: Preserving Generational Voices and Memories for Seniors and   Children"></a>EchoVoices: Preserving Generational Voices and Memories for Seniors and   Children</h2><p><strong>Authors:Haiying Xu, Haoze Liu, Mingshi Li, Siyu Cai, Guangxuan Zheng, Yuhuang Jia, Jinghua Zhao, Yong Qin</strong></p>
<p>Recent breakthroughs in intelligent speech and digital human technologies have primarily targeted mainstream adult users, often overlooking the distinct vocal patterns and interaction styles of seniors and children. These demographics possess distinct vocal characteristics, linguistic styles, and interaction patterns that challenge conventional ASR, TTS, and LLM systems. To address this, we introduce EchoVoices, an end-to-end digital human pipeline dedicated to creating persistent digital personas for seniors and children, ensuring their voices and memories are preserved for future generations. Our system integrates three core innovations: a k-NN-enhanced Whisper model for robust speech recognition of atypical speech; an age-adaptive VITS model for high-fidelity, speaker-aware speech synthesis; and an LLM-driven agent that automatically generates persona cards and leverages a RAG-based memory system for conversational consistency. Our experiments, conducted on the SeniorTalk and ChildMandarin datasets, demonstrate significant improvements in recognition accuracy, synthesis quality, and speaker similarity. EchoVoices provides a comprehensive framework for preserving generational voices, offering a new means of intergenerational connection and the creation of lasting digital legacies. </p>
<blockquote>
<p>è¿‘æœŸæ™ºèƒ½è¯­éŸ³å’Œæ•°å­—äººæŠ€æœ¯çš„çªç ´ä¸»è¦é¢å‘ä¸»æµæˆå¹´ç”¨æˆ·ï¼Œå¾€å¾€å¿½è§†äº†è€å¹´äººå’Œå„¿ç«¥ç‰¹æœ‰çš„è¯­éŸ³æ¨¡å¼å’Œäº¤äº’é£æ ¼ã€‚è¿™äº›äººç¾¤å…·æœ‰ç‹¬ç‰¹çš„è¯­éŸ³ç‰¹å¾ã€è¯­è¨€é£æ ¼å’Œäº¤äº’æ¨¡å¼ï¼Œå¯¹ä¼ ç»Ÿçš„ASRã€TTSå’ŒLLMç³»ç»Ÿæå‡ºäº†æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†EchoVoicesï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“æ³¨äºä¸ºè€å¹´äººå’Œå„¿ç«¥åˆ›å»ºæŒä¹…æ•°å­—äººæ ¼çš„ç«¯åˆ°ç«¯æ•°å­—äººç®¡é“ï¼Œç¡®ä¿ä»–ä»¬çš„å£°éŸ³å’Œè®°å¿†èƒ½å¤Ÿä¼ æ‰¿ç»™åä»£ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿé›†æˆäº†ä¸‰é¡¹æ ¸å¿ƒåˆ›æ–°ï¼šé‡‡ç”¨k-NNå¢å¼ºçš„Whisperæ¨¡å‹ï¼Œå®ç°ç¨³å¥çš„éå¸¸è§„è¯­éŸ³è¯†åˆ«ï¼›é‡‡ç”¨å¹´é¾„è‡ªé€‚åº”çš„VITSæ¨¡å‹ï¼Œå®ç°é«˜ä¿çœŸã€çŸ¥è®²è€…è¯­éŸ³åˆæˆï¼›ä»¥åŠä¸€ä¸ªç”±LLMé©±åŠ¨çš„æ™ºèƒ½ä»£ç†ï¼Œå¯è‡ªåŠ¨ç”Ÿæˆä¸ªäººåç‰‡ï¼Œå¹¶åˆ©ç”¨åŸºäºRAGçš„è®°å¿†ç³»ç»Ÿå®ç°å¯¹è¯è¿è´¯æ€§ã€‚æˆ‘ä»¬åœ¨SeniorTalkå’ŒChildMandarinæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒï¼Œè¯æ˜äº†åœ¨è¯†åˆ«å‡†ç¡®æ€§ã€åˆæˆè´¨é‡å’Œè¯´è¯äººç›¸ä¼¼æ€§æ–¹é¢çš„æ˜¾è‘—æ”¹å–„ã€‚EchoVoicesæä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ¡†æ¶æ¥ä¿å­˜ä¸–ä»£ä¹‹å£°ï¼Œä¸ºè·¨ä»£è¿æ¥å’Œåˆ›å»ºæŒä¹…çš„æ•°å­—é—äº§æä¾›äº†æ–°çš„æ‰‹æ®µã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15221v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ™ºèƒ½è¯­éŸ³ä¸æ•°å­—äººæŠ€æœ¯çš„æœ€æ–°çªç ´ä¸»è¦é›†ä¸­åœ¨ä¸»æµæˆå¹´ç”¨æˆ·ä¸Šï¼Œå¿½ç•¥äº†è€å¹´äººå’Œå„¿ç«¥ç‰¹æœ‰çš„è¯­éŸ³æ¨¡å¼ä¸äº¤äº’é£æ ¼ã€‚EchoVoicesç³»ç»Ÿè‡´åŠ›äºåˆ›å»ºé’ˆå¯¹è€å¹´äººå’Œå„¿ç«¥çš„æŒä¹…æ•°å­—äººæ ¼ï¼Œç¡®ä¿ä»–ä»¬çš„å£°éŸ³ä¸è®°å¿†å¾—ä»¥ä¼ æ‰¿ç»™æœªæ¥ä¸–ä»£ã€‚è¯¥ç³»ç»Ÿæ•´åˆä¸‰é¡¹æ ¸å¿ƒæŠ€æœ¯ï¼šé‡‡ç”¨k-NNå¢å¼ºçš„Whisperæ¨¡å‹ï¼Œå®ç°éå…¸å‹è¯­éŸ³çš„ç¨³å¥è¯†åˆ«ï¼›å¹´é¾„è‡ªé€‚åº”çš„VITSæ¨¡å‹ï¼Œå®ç°é«˜ä¿çœŸã€å…·æœ‰è¯´è¯è€…æ„è¯†çš„è¯­éŸ³åˆæˆï¼›ä»¥åŠåŸºäºLLMçš„ä»£ç†ï¼Œè‡ªåŠ¨ç”Ÿæˆä¸ªæ€§å¡ç‰‡å¹¶åˆ©ç”¨RAGä¸ºåŸºç¡€çš„å†…å­˜ç³»ç»Ÿå®ç°å¯¹è¯è¿è´¯æ€§ã€‚åœ¨SeniorTalkå’ŒChildMandarinæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEchoVoicesåœ¨è¯†åˆ«å‡†ç¡®ç‡ã€åˆæˆè´¨é‡å’Œè¯´è¯è€…ç›¸ä¼¼æ€§ä¸Šéƒ½æœ‰æ˜¾è‘—æé«˜ï¼Œä¸ºä¿ç•™ä¸–ä»£å£°éŸ³æä¾›äº†ä¸€ä¸ªç»¼åˆæ¡†æ¶ï¼Œä¸ºè·¨ä»£è¿æ¥å’Œåˆ›å»ºæŒä¹…çš„æ•°å­—é—äº§æä¾›äº†æ–°çš„æ‰‹æ®µã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EchoVoicesç³»ç»Ÿè‡´åŠ›äºåˆ›å»ºé’ˆå¯¹è€å¹´äººå’Œå„¿ç«¥çš„æŒä¹…æ•°å­—äººæ ¼ã€‚</li>
<li>è¯¥ç³»ç»Ÿæ•´åˆä¸‰é¡¹æ ¸å¿ƒæŠ€æœ¯ï¼šéå…¸å‹è¯­éŸ³çš„ç¨³å¥è¯†åˆ«ã€é«˜ä¿çœŸè¯­éŸ³åˆæˆä»¥åŠä¿è¯å¯¹è¯è¿è´¯æ€§çš„ä»£ç†æŠ€æœ¯ã€‚</li>
<li>EchoVoicesé‡‡ç”¨äº†k-NNå¢å¼ºçš„Whisperæ¨¡å‹æ¥æå‡å¯¹è€å¹´äººå’Œå„¿ç«¥ç‰¹æœ‰è¯­éŸ³æ¨¡å¼çš„è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡å¹´é¾„è‡ªé€‚åº”çš„VITSæ¨¡å‹å®ç°é«˜ä¿çœŸã€å…·æœ‰è¯´è¯è€…æ„è¯†çš„è¯­éŸ³åˆæˆã€‚</li>
<li>LLMé©±åŠ¨çš„ä»£ç†èƒ½è‡ªåŠ¨ç”Ÿæˆä¸ªæ€§å¡ç‰‡ï¼Œå¹¶åˆ©ç”¨RAGä¸ºåŸºç¡€çš„å†…å­˜ç³»ç»Ÿç¡®ä¿å¯¹è¯è¿è´¯æ€§ã€‚</li>
<li>åœ¨SeniorTalkå’ŒChildMandarinæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†EchoVoicesåœ¨è¯†åˆ«å‡†ç¡®ç‡ã€åˆæˆè´¨é‡å’Œè¯´è¯è€…ç›¸ä¼¼æ€§ä¸Šçš„æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15221">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-00b059d191ade42e9b258c54fb82dab9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b2138d09900229604697bb8e3c9191da.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-60d6d85fec9186cc3de8fdcfdf931b54.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e9d3ae1e2d85763df70c42996072524b.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DMOSpeech-2-Reinforcement-Learning-for-Duration-Prediction-in-Metric-Optimized-Speech-Synthesis"><a href="#DMOSpeech-2-Reinforcement-Learning-for-Duration-Prediction-in-Metric-Optimized-Speech-Synthesis" class="headerlink" title="DMOSpeech 2: Reinforcement Learning for Duration Prediction in   Metric-Optimized Speech Synthesis"></a>DMOSpeech 2: Reinforcement Learning for Duration Prediction in   Metric-Optimized Speech Synthesis</h2><p><strong>Authors:Yinghao Aaron Li, Xilin Jiang, Fei Tao, Cheng Niu, Kaifeng Xu, Juntong Song, Nima Mesgarani</strong></p>
<p>Diffusion-based text-to-speech (TTS) systems have made remarkable progress in zero-shot speech synthesis, yet optimizing all components for perceptual metrics remains challenging. Prior work with DMOSpeech demonstrated direct metric optimization for speech generation components, but duration prediction remained unoptimized. This paper presents DMOSpeech 2, which extends metric optimization to the duration predictor through a reinforcement learning approach. The proposed system implements a novel duration policy framework using group relative preference optimization (GRPO) with speaker similarity and word error rate as reward signals. By optimizing this previously unoptimized component, DMOSpeech 2 creates a more complete metric-optimized synthesis pipeline. Additionally, this paper introduces teacher-guided sampling, a hybrid approach leveraging a teacher model for initial denoising steps before transitioning to the student model, significantly improving output diversity while maintaining efficiency. Comprehensive evaluations demonstrate superior performance across all metrics compared to previous systems, while reducing sampling steps by half without quality degradation. These advances represent a significant step toward speech synthesis systems with metric optimization across multiple components. The audio samples, code and pre-trained models are available at <a target="_blank" rel="noopener" href="https://dmospeech2.github.io/">https://dmospeech2.github.io/</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿåœ¨é›¶æ ·æœ¬è¯­éŸ³åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ï¼Œä½†ä¼˜åŒ–æ‰€æœ‰ç»„ä»¶ä»¥ç¬¦åˆæ„ŸçŸ¥æŒ‡æ ‡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¹‹å‰ä½¿ç”¨DMOSpeechçš„å·¥ä½œå±•ç¤ºäº†é’ˆå¯¹è¯­éŸ³ç”Ÿæˆç»„ä»¶çš„ç›´æ¥æŒ‡æ ‡ä¼˜åŒ–ï¼Œä½†æŒç»­æ—¶é—´é¢„æµ‹å°šæœªä¼˜åŒ–ã€‚æœ¬æ–‡æå‡ºäº†DMOSpeech 2ï¼Œå®ƒé€šè¿‡å¼ºåŒ–å­¦ä¹ æ–¹æ³•å°†æŒ‡æ ‡ä¼˜åŒ–æ‰©å±•åˆ°æŒç»­æ—¶é—´é¢„æµ‹å™¨ã€‚æ‰€æå‡ºçš„ç³»ç»Ÿä½¿ç”¨åŸºäºç¾¤ä½“ç›¸å¯¹åå¥½ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„å¥–åŠ±ä¿¡å·æ¥å®ç°æ–°å‹æŒç»­æ—¶é—´ç­–ç•¥æ¡†æ¶ï¼Œè¯¥ç­–ç•¥è€ƒè™‘è¯´è¯äººçš„ç›¸ä¼¼æ€§å’Œè¯é”™è¯¯ç‡ä½œä¸ºå¥–åŠ±ä¿¡å·ã€‚é€šè¿‡ä¼˜åŒ–ä¹‹å‰æœªä¼˜åŒ–çš„ç»„ä»¶ï¼ŒDMOSpeech 2åˆ›å»ºäº†ä¸€ä¸ªæ›´å®Œæ•´çš„æŒ‡æ ‡ä¼˜åŒ–åˆæˆç®¡é“ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å¼•å…¥äº†æ•™å¸ˆå¼•å¯¼é‡‡æ ·ï¼Œè¿™æ˜¯ä¸€ç§æ··åˆæ–¹æ³•ï¼Œåˆ©ç”¨æ•™å¸ˆæ¨¡å‹è¿›è¡Œåˆå§‹å»å™ªæ­¥éª¤ï¼Œç„¶åè¿‡æ¸¡åˆ°å­¦ç”Ÿæ¨¡å‹ï¼Œåœ¨æé«˜è¾“å‡ºå¤šæ ·æ€§çš„åŒæ—¶ä¿æŒæ•ˆç‡ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼Œåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šçš„æ€§èƒ½å‡ä¼˜äºä¹‹å‰çš„ç³»ç»Ÿï¼ŒåŒæ—¶åœ¨å‡åŠé‡‡æ ·æ­¥éª¤çš„æƒ…å†µä¸‹ä¸é™ä½è´¨é‡ã€‚è¿™äº›è¿›å±•ä»£è¡¨äº†æœç€å…·æœ‰å¤šä¸ªç»„ä»¶æŒ‡æ ‡ä¼˜åŒ–çš„è¯­éŸ³åˆæˆç³»ç»Ÿè¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚éŸ³é¢‘æ ·æœ¬ã€ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://dmospeech2.github.io/%E6%89%BE%E5%88%B0%E3%80%82">https://dmospeech2.github.io/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14988v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†DMOSpeech 2ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡å¼ºåŒ–å­¦ä¹ æ–¹æ³•å°†åº¦é‡ä¼˜åŒ–æ‰©å±•åˆ°æ—¶é•¿é¢„æµ‹å™¨ä¸Šï¼Œå®ç°äº†é›¶æ ·æœ¬è¯­éŸ³åˆæˆçš„æ˜¾è‘—è¿›å±•ã€‚é€šè¿‡é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹åå¥½ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„æ—¶é•¿ç­–ç•¥æ¡†æ¶ï¼ŒåŒæ—¶ä½¿ç”¨è¯´è¯äººç›¸ä¼¼åº¦å’Œè¯é”™è¯¯ç‡ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œå¯¹æ—¶é•¿é¢„æµ‹è¿›è¡Œä¼˜åŒ–ï¼Œåˆ›å»ºäº†ä¸€ä¸ªæ›´å®Œæ•´çš„åº¦é‡ä¼˜åŒ–åˆæˆç®¡é“ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†æ•™å¸ˆå¼•å¯¼é‡‡æ ·æ³•ï¼Œåœ¨åˆæœŸåˆ©ç”¨æ•™å¸ˆæ¨¡å‹è¿›è¡Œé™å™ªå¤„ç†ï¼Œéšåè¿‡æ¸¡åˆ°å­¦ç”Ÿæ¨¡å‹ï¼Œä»¥æé«˜è¾“å‡ºå¤šæ ·æ€§å¹¶ç»´æŒæ•ˆç‡ã€‚ç»¼åˆè¯„ä¼°è¡¨æ˜ï¼ŒDMOSpeech 2åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šçš„æ€§èƒ½å‡ä¼˜äºä¹‹å‰çš„ç³»ç»Ÿï¼Œä¸”é‡‡æ ·æ­¥éª¤å‡å°‘ä¸€åŠè€Œè´¨é‡æ²¡æœ‰é™ä½ã€‚è¿™æ˜¯å‘å¤šç»„ä»¶åº¦é‡ä¼˜åŒ–çš„è¯­éŸ³åˆæˆç³»ç»Ÿè¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DMOSpeech 2ç³»ç»Ÿæ‰©å±•äº†åº¦é‡ä¼˜åŒ–è‡³æ—¶é•¿é¢„æµ‹å™¨ä¸Šï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ–¹æ³•å®ç°äº†é›¶æ ·æœ¬è¯­éŸ³åˆæˆçš„è¿›æ­¥ã€‚</li>
<li>é‡‡ç”¨ç¾¤ä½“ç›¸å¯¹åå¥½ä¼˜åŒ–ï¼ˆGRPOï¼‰çš„æ—¶é•¿ç­–ç•¥æ¡†æ¶ï¼Œä»¥ä¼˜åŒ–æ—¶é•¿é¢„æµ‹ã€‚</li>
<li>è¯´è¯äººç›¸ä¼¼åº¦å’Œè¯é”™è¯¯ç‡è¢«ç”¨ä½œå¥–åŠ±ä¿¡å·ï¼Œä»¥æ”¹å–„è¯­éŸ³åˆæˆçš„è´¨é‡ã€‚</li>
<li>æ•™å¸ˆå¼•å¯¼é‡‡æ ·æ³•çš„å¼•å…¥æé«˜äº†è¾“å‡ºå¤šæ ·æ€§å¹¶ç»´æŒäº†æ•ˆç‡ã€‚</li>
<li>DMOSpeech 2åœ¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ä¸Šçš„æ€§èƒ½ä¼˜äºå…ˆå‰çš„ç³»ç»Ÿã€‚</li>
<li>é‡‡æ ·æ­¥éª¤å‡å°‘ä¸€åŠï¼ŒåŒæ—¶ä¿æŒè¯­éŸ³è´¨é‡ä¸é™ä½ã€‚</li>
<li>è¿™æ˜¯æœç€å¤šç»„ä»¶åº¦é‡ä¼˜åŒ–çš„è¯­éŸ³åˆæˆç³»ç»Ÿè¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7e4c93cacbeb1e2480440eeb5fae2e1f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4e0e89d19d07c9d9fb1fd61467f75fa3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea3e03edbcc4ec6c38b188f0de374eaa.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="NonverbalTTS-A-Public-English-Corpus-of-Text-Aligned-Nonverbal-Vocalizations-with-Emotion-Annotations-for-Text-to-Speech"><a href="#NonverbalTTS-A-Public-English-Corpus-of-Text-Aligned-Nonverbal-Vocalizations-with-Emotion-Annotations-for-Text-to-Speech" class="headerlink" title="NonverbalTTS: A Public English Corpus of Text-Aligned Nonverbal   Vocalizations with Emotion Annotations for Text-to-Speech"></a>NonverbalTTS: A Public English Corpus of Text-Aligned Nonverbal   Vocalizations with Emotion Annotations for Text-to-Speech</h2><p><strong>Authors:Maksim Borisov, Egor Spirin, Daria Diatlova</strong></p>
<p>Current expressive speech synthesis models are constrained by the limited availability of open-source datasets containing diverse nonverbal vocalizations (NVs). In this work, we introduce NonverbalTTS (NVTTS), a 17-hour open-access dataset annotated with 10 types of NVs (e.g., laughter, coughs) and 8 emotional categories. The dataset is derived from popular sources, VoxCeleb and Expresso, using automated detection followed by human validation. We propose a comprehensive pipeline that integrates automatic speech recognition (ASR), NV tagging, emotion classification, and a fusion algorithm to merge transcriptions from multiple annotators. Fine-tuning open-source text-to-speech (TTS) models on the NVTTS dataset achieves parity with closed-source systems such as CosyVoice2, as measured by both human evaluation and automatic metrics, including speaker similarity and NV fidelity. By releasing NVTTS and its accompanying annotation guidelines, we address a key bottleneck in expressive TTS research. The dataset is available at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/deepvk/NonverbalTTS">https://huggingface.co/datasets/deepvk/NonverbalTTS</a>. </p>
<blockquote>
<p>å½“å‰çš„è¡¨æƒ…è¯­éŸ³åˆæˆæ¨¡å‹å—é™äºå¼€æ”¾æºæ•°æ®é›†çš„æœ‰é™å¯ç”¨æ€§ï¼Œè¿™äº›æ•°æ®é›†åŒ…å«å„ç§éè¨€è¯­å‘å£°ï¼ˆNVsï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†NonverbalTTSï¼ˆNVTTSï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«17å°æ—¶æ—¶é•¿ã€æ ‡æ³¨äº†10ç§éè¨€è¯­å‘å£°ç±»å‹å’Œ8ç§æƒ…ç»ªç±»åˆ«çš„å¼€æ”¾è®¿é—®æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ¥æºäºå—æ¬¢è¿çš„èµ„æºVoxCelebå’ŒExpressoï¼Œä½¿ç”¨è‡ªåŠ¨æ£€æµ‹åŠ äººå·¥éªŒè¯çš„æ–¹å¼ç”Ÿæˆã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»¼åˆç®¡é“ï¼Œé›†æˆäº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€NVæ ‡ç­¾ã€æƒ…ç»ªåˆ†ç±»ï¼Œä»¥åŠä¸€ä¸ªèåˆç®—æ³•æ¥åˆå¹¶å¤šä¸ªæ³¨é‡Šå™¨çš„è½¬å½•å†…å®¹ã€‚åœ¨NVTTSæ•°æ®é›†ä¸Šå¾®è°ƒå¼€æºæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ï¼Œå¯ä»¥å®ç°ä¸äººç±»è¯„ä¼°çš„å°é—­ç³»ç»Ÿï¼ˆå¦‚CosyVoice2ï¼‰ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨åº¦é‡æŒ‡æ ‡ï¼ˆåŒ…æ‹¬è¯´è¯äººç›¸ä¼¼æ€§å’ŒNVä¿çœŸåº¦ï¼‰æ¥è¡¡é‡è¿™ä¸€ç‚¹ã€‚é€šè¿‡å‘å¸ƒNVTTSåŠå…¶éšé™„çš„æ³¨é‡ŠæŒ‡å—ï¼Œæˆ‘ä»¬è§£å†³äº†è¡¨æƒ…TTSç ”ç©¶ä¸­çš„ä¸€ä¸ªå…³é”®ç“¶é¢ˆã€‚è¯¥æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/deepvk/NonverbalTTS">é“¾æ¥</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13155v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éè¨€è¯­è¯­éŸ³åˆæˆæ¨¡å‹å—é™äºå…¬å¼€æ•°æ®é›†çš„å¯è®¿é—®æ€§å’Œå¤šæ ·æ€§ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†NonverbalTTSï¼ˆNVTTSï¼‰æ•°æ®é›†ï¼ŒåŒ…å«17å°æ—¶æ—¶é•¿ï¼Œæ ‡æ³¨äº†10ç§éè¨€è¯­å£°éŸ³ç±»å‹å’Œ8ç§æƒ…æ„Ÿç±»åˆ«ã€‚è¯¥æ•°æ®é›†æ¥æºäºVoxCelebå’ŒExpressoç­‰æµè¡Œæ¥æºï¼Œé‡‡ç”¨è‡ªåŠ¨æ£€æµ‹åŠ äººå·¥éªŒè¯çš„æ–¹å¼æ„å»ºã€‚é€šè¿‡é›†æˆè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€éè¨€è¯­æ ‡ç­¾ã€æƒ…æ„Ÿåˆ†ç±»å’Œèåˆç®—æ³•ç­‰å¤šé˜¶æ®µå¤„ç†æµç¨‹ï¼Œè¯¥æ•°æ®é›†èƒ½å¤Ÿå¯¹å¤šé‡æ ‡æ³¨è¿›è¡Œèåˆè½¬å½•ã€‚åœ¨éè¨€è¯­æ•°æ®é›†ä¸Šå¾®è°ƒå¼€æ”¾æºæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹å¯å®ç°ä¸é—­æºç³»ç»Ÿç›¸è¿‘çš„æ•ˆæœã€‚NVTTSæ•°æ®é›†åŠå…¶æ ‡æ³¨æŒ‡å—çš„å‘å¸ƒè§£å†³äº†è¡¨è¾¾æ€§TTSç ”ç©¶çš„å…³é”®ç“¶é¢ˆé—®é¢˜ã€‚å¯é€šè¿‡ç›¸å…³ç½‘ç«™è®¿é—®æ­¤æ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>NVTTSæ˜¯ä¸€ä¸ªåŒ…å«å¤šç§éè¨€è¯­å£°éŸ³å’Œæƒ…æ„Ÿç±»åˆ«çš„å…¬å¼€æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†æ¥æºäºVoxCelebå’ŒExpressoç­‰æµè¡Œæ¥æºï¼Œæ ‡æ³¨äº†éè¨€è¯­å£°éŸ³å’Œæƒ…ç»ªç±»åˆ«ã€‚</li>
<li>NVTTSé‡‡ç”¨äº†è‡ªåŠ¨åŒ–å’Œäººå·¥éªŒè¯ç»“åˆçš„æ ‡æ³¨æ–¹æ³•ã€‚</li>
<li>æ•°æ®é›†æä¾›äº†ä¸€ä¸ªç»¼åˆå¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬ASRã€NVæ ‡ç­¾ã€æƒ…æ„Ÿåˆ†ç±»ç­‰ã€‚</li>
<li>åœ¨NVTTSæ•°æ®é›†ä¸Šå¾®è°ƒå¼€æ”¾æºTTSæ¨¡å‹å¯å®ç°ä¸é—­æºç³»ç»Ÿç›¸è¿‘çš„æ•ˆæœã€‚</li>
<li>NVTTSçš„å‘å¸ƒè§£å†³äº†è¡¨è¾¾æ€§TTSç ”ç©¶çš„å…³é”®ç“¶é¢ˆé—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13155">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0494ffe64da5328e10e306c6e3dd56c0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6adede6014d3c4538051304d47f73b17.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-257ce657cee0b49560e6f06c82af8b89.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2e8fae4bfe2ca4e9f55c02e287da3ea8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ebeeccb8304ade1786a2409a74da051.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc2e7338549061bf4205e0fcfbf16cef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-729960ab4f226775ece7920fa53ed94a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-226e944b043a0d93f7f391f93b5345f0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Intelligent-Virtual-Sonographer-IVS-Enhancing-Physician-Robot-Patient-Communication"><a href="#Intelligent-Virtual-Sonographer-IVS-Enhancing-Physician-Robot-Patient-Communication" class="headerlink" title="Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient   Communication"></a>Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient   Communication</h2><p><strong>Authors:Tianyu Song, Feng Li, Yuan Bi, Angelos Karlas, Amir Yousefi, Daniela Branzan, Zhongliang Jiang, Ulrich Eck, Nassir Navab</strong></p>
<p>The advancement and maturity of large language models (LLMs) and robotics have unlocked vast potential for human-computer interaction, particularly in the field of robotic ultrasound. While existing research primarily focuses on either patient-robot or physician-robot interaction, the role of an intelligent virtual sonographer (IVS) bridging physician-robot-patient communication remains underexplored. This work introduces a conversational virtual agent in Extended Reality (XR) that facilitates real-time interaction between physicians, a robotic ultrasound system(RUS), and patients. The IVS agent communicates with physicians in a professional manner while offering empathetic explanations and reassurance to patients. Furthermore, it actively controls the RUS by executing physician commands and transparently relays these actions to the patient. By integrating LLM-powered dialogue with speech-to-text, text-to-speech, and robotic control, our system enhances the efficiency, clarity, and accessibility of robotic ultrasound acquisition. This work constitutes a first step toward understanding how IVS can bridge communication gaps in physician-robot-patient interaction, providing more control and therefore trust into physician-robot interaction while improving patient experience and acceptance of robotic ultrasound. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæœºå™¨äººæŠ€æœ¯çš„ä¸æ–­è¿›æ­¥å’Œæˆç†Ÿï¼Œä¸ºäººæœºäº¤äº’ï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººè¶…å£°é¢†åŸŸï¼Œå¸¦æ¥äº†å·¨å¤§çš„æ½œåŠ›ã€‚å°½ç®¡ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æ‚£è€…ä¸æœºå™¨äººæˆ–åŒ»ç”Ÿä¸æœºå™¨äººçš„äº¤äº’ï¼Œä½†æ™ºèƒ½è™šæ‹Ÿè¶…å£°æŠ€å¸ˆï¼ˆIVSï¼‰åœ¨åŒ»ç”Ÿ-æœºå™¨äºº-æ‚£è€…æ²Ÿé€šä¸­çš„æ¡¥æ¢ä½œç”¨ä»è¢«å¿½è§†ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ‰©å±•ç°å®ï¼ˆXRï¼‰ä¸­çš„å¯¹è¯å¼è™šæ‹Ÿä»£ç†ï¼Œä¿ƒè¿›äº†åŒ»ç”Ÿã€æœºå™¨äººè¶…å£°ç³»ç»Ÿï¼ˆRUSï¼‰å’Œæ‚£è€…ä¹‹é—´çš„å®æ—¶äº¤äº’ã€‚IVSä»£ç†ä»¥ä¸“ä¸šçš„æ–¹å¼ä¸åŒ»ç”Ÿæ²Ÿé€šï¼ŒåŒæ—¶å‘æ‚£è€…æä¾›å¯Œæœ‰åŒæƒ…å¿ƒçš„è§£é‡Šå’Œå®‰æ…°ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½ä¸»åŠ¨æ§åˆ¶RUSï¼Œæ‰§è¡ŒåŒ»ç”Ÿçš„å‘½ä»¤ï¼Œå¹¶é€æ˜åœ°å‘æ‚£è€…ä¼ è¾¾è¿™äº›è¡ŒåŠ¨ã€‚é€šè¿‡æ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å¯¹è¯ä¸è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢å’Œæœºå™¨äººæ§åˆ¶ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿæé«˜äº†æœºå™¨äººè¶…å£°é‡‡é›†çš„æ•ˆç‡ã€æ¸…æ™°åº¦å’Œå¯åŠæ€§ã€‚æœ¬ç ”ç©¶æ˜¯äº†è§£IVSå¦‚ä½•å¼¥åˆåŒ»ç”Ÿ-æœºå™¨äºº-æ‚£è€…æ²Ÿé€šä¸­çš„å·®è·çš„ç¬¬ä¸€æ­¥ï¼Œä¸ºåŒ»ç”Ÿ-æœºå™¨äººäº¤äº’æä¾›æ›´å¤šçš„æ§åˆ¶å’Œä¿¡ä»»ï¼ŒåŒæ—¶æé«˜æ‚£è€…å¯¹æœºå™¨äººè¶…å£°çš„ä½“éªŒå’Œæ¥å—åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13052v1">PDF</a> Accepted at MICCAI 2025</p>
<p><strong>Summary</strong><br>æ–°ä¸€ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æœºå™¨äººæŠ€æœ¯çš„æˆç†Ÿä¸ºäººæœºäº¤äº’æ‰“å¼€äº†å·¨å¤§çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººè¶…å£°é¢†åŸŸã€‚å½“å‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨åŒ»æ‚£æœºå™¨äººæˆ–åŒ»ç”Ÿæœºå™¨äººäº¤äº’ä¸Šï¼Œè€Œæ™ºèƒ½è™šæ‹Ÿè¶…å£°å¸ˆï¼ˆIVSï¼‰åœ¨åŒ»ç”Ÿæœºå™¨äººä¸æ‚£è€…æ²Ÿé€šä¸­çš„æ¡¥æ¢ä½œç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§åŸºäºæ‰©å±•ç°å®ï¼ˆXRï¼‰çš„å¯¹è¯è™šæ‹Ÿæ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“èƒ½ä¿ƒè¿›åŒ»ç”Ÿã€æœºå™¨äººè¶…å£°ç³»ç»Ÿï¼ˆRUSï¼‰å’Œæ‚£è€…ä¹‹é—´çš„å®æ—¶äº¤äº’ã€‚IVSæ™ºèƒ½ä½“èƒ½ä»¥ä¸“ä¸šçš„æ–¹å¼ä¸åŒ»ç”Ÿæ²Ÿé€šï¼ŒåŒæ—¶å‘æ‚£è€…æä¾›å¯Œæœ‰åŒæƒ…å¿ƒçš„è§£é‡Šå’Œå®‰æ…°ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½ä¸»åŠ¨æ§åˆ¶RUSæ‰§è¡ŒåŒ»ç”Ÿçš„å‘½ä»¤ï¼Œå¹¶å‘æ‚£è€…é€æ˜åœ°ä¼ è¾¾è¿™äº›æ“ä½œã€‚é€šè¿‡æ•´åˆLLMé©±åŠ¨çš„å¯¹è¯ä¸è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬è½¬è¯­éŸ³å’Œæœºå™¨äººæ§åˆ¶ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿæé«˜äº†æœºå™¨äººè¶…å£°é‡‡é›†çš„æ•ˆç‡ã€æ¸…æ™°åº¦å’Œå¯åŠæ€§ã€‚æœ¬ç ”ç©¶æ˜¯äº†è§£IVSå¦‚ä½•å¼¥åˆåŒ»ç”Ÿæœºå™¨äººä¸æ‚£è€…æ²Ÿé€šä¸­çš„å·®è·çš„ç¬¬ä¸€æ­¥ï¼Œä¸ºæé«˜åŒ»ç”Ÿæœºå™¨äººäº¤äº’çš„æ§åˆ¶å’Œä¿¡ä»»ä»¥åŠæ”¹å–„æ‚£è€…ä½“éªŒå’Œæ¥å—åº¦æä¾›äº†é‡è¦æ€è·¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œæœºå™¨äººæŠ€æœ¯çš„æˆç†Ÿä¿ƒè¿›äº†äººæœºäº¤äº’çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººè¶…å£°é¢†åŸŸã€‚</li>
<li>å½“å‰ç ”ç©¶å¿½è§†äº†æ™ºèƒ½è™šæ‹Ÿè¶…å£°å¸ˆï¼ˆIVSï¼‰åœ¨åŒ»ç”Ÿæœºå™¨äººä¸æ‚£è€…æ²Ÿé€šä¸­çš„æ¡¥æ¢ä½œç”¨ã€‚</li>
<li>IVSèƒ½å¤Ÿä¸“ä¸šåœ°ä¸åŒ»ç”Ÿæ²Ÿé€šï¼ŒåŒæ—¶å‘æ‚£è€…æä¾›å¯Œæœ‰åŒæƒ…å¿ƒçš„è§£é‡Šå’Œå®‰æ…°ã€‚</li>
<li>IVSèƒ½ä¸»åŠ¨æ§åˆ¶æœºå™¨äººè¶…å£°ç³»ç»Ÿï¼ˆRUSï¼‰æ‰§è¡ŒåŒ»ç”Ÿçš„å‘½ä»¤ã€‚</li>
<li>ç»“åˆLLMã€è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬è½¬è¯­éŸ³å’Œæœºå™¨äººæ§åˆ¶ï¼Œæé«˜äº†æœºå™¨äººè¶…å£°é‡‡é›†çš„æ•ˆç‡ã€æ¸…æ™°åº¦å’Œå¯åŠæ€§ã€‚</li>
<li>æœ¬ç ”ç©¶æ˜¯äº†è§£IVSå¦‚ä½•ä¿ƒè¿›åŒ»ç”Ÿæœºå™¨äººä¸æ‚£è€…æ²Ÿé€šçš„ç¬¬ä¸€æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13052">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c40284c1924f1cb229bfcfc3b118a669.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e67823ec49d09948e351da6ded75bd1c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48c4fb3a82c5920dc89c04f414ce59a8.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Enkidu-Universal-Frequential-Perturbation-for-Real-Time-Audio-Privacy-Protection-against-Voice-Deepfakes"><a href="#Enkidu-Universal-Frequential-Perturbation-for-Real-Time-Audio-Privacy-Protection-against-Voice-Deepfakes" class="headerlink" title="Enkidu: Universal Frequential Perturbation for Real-Time Audio Privacy   Protection against Voice Deepfakes"></a>Enkidu: Universal Frequential Perturbation for Real-Time Audio Privacy   Protection against Voice Deepfakes</h2><p><strong>Authors:Zhou Feng, Jiahao Chen, Chunyi Zhou, Yuwen Pu, Qingming Li, Tianyu Du, Shouling Ji</strong></p>
<p>The rapid advancement of voice deepfake technologies has raised serious concerns about user audio privacy, as attackers increasingly exploit publicly available voice data to generate convincing fake audio for malicious purposes such as identity theft, financial fraud, and misinformation campaigns. While existing defense methods offer partial protection, they face critical limitations, including weak adaptability to unseen user data, poor scalability to long audio, rigid reliance on white-box knowledge, and high computational and temporal costs during the encryption process. To address these challenges and defend against personalized voice deepfake threats, we propose Enkidu, a novel user-oriented privacy-preserving framework that leverages universal frequential perturbations generated through black-box knowledge and few-shot training on a small amount of user data. These highly malleable frequency-domain noise patches enable real-time, lightweight protection with strong generalization across variable-length audio and robust resistance to voice deepfake attacks, all while preserving perceptual quality and speech intelligibility. Notably, Enkidu achieves over 50 to 200 times processing memory efficiency (as low as 0.004 gigabytes) and 3 to 7000 times runtime efficiency (real-time coefficient as low as 0.004) compared to six state-of-the-art countermeasures. Extensive experiments across six mainstream text-to-speech models and five cutting-edge automated speaker verification models demonstrate the effectiveness, transferability, and practicality of Enkidu in defending against both vanilla and adaptive voice deepfake attacks. </p>
<blockquote>
<p>éšç€è¯­éŸ³æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œç”¨æˆ·éŸ³é¢‘éšç§å¼•èµ·äº†ä¸¥é‡çš„å…³æ³¨ã€‚æ”»å‡»è€…è¶Šæ¥è¶Šåˆ©ç”¨å…¬å¼€çš„è¯­éŸ³æ•°æ®ç”Ÿæˆä»¤äººä¿¡æœçš„è™šå‡éŸ³é¢‘ï¼Œç”¨äºèº«ä»½ç›—çªƒã€é‡‘èæ¬ºè¯ˆå’Œè™šå‡ä¿¡æ¯å®£ä¼ ç­‰æ¶æ„ç›®çš„ã€‚ç°æœ‰çš„é˜²å¾¡æ–¹æ³•è™½ç„¶æä¾›äº†ä¸€å®šçš„ä¿æŠ¤ï¼Œä½†å­˜åœ¨å…³é”®å±€é™æ€§ï¼ŒåŒ…æ‹¬å¯¹æ–°ç”¨æˆ·æ•°æ®çš„é€‚åº”èƒ½åŠ›å¼±ã€å¯¹é•¿éŸ³é¢‘çš„å¯æ‰©å±•æ€§å·®ã€å¯¹ç©ºç™½ç›’çŸ¥è¯†çš„åˆšæ€§ä¾èµ–ä»¥åŠåŠ å¯†è¿‡ç¨‹ä¸­çš„é«˜è®¡ç®—å’Œæ—¶é—´æˆæœ¬ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜å¹¶é˜²èŒƒä¸ªæ€§åŒ–çš„è¯­éŸ³æ·±åº¦ä¼ªé€ å¨èƒï¼Œæˆ‘ä»¬æå‡ºäº†Enkiduï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„ç”¨æˆ·å¯¼å‘çš„éšç§ä¿æŠ¤æ¡†æ¶ã€‚å®ƒåˆ©ç”¨é€šè¿‡ç©ºç™½ç›’çŸ¥è¯†å’Œå°‘é‡ç”¨æˆ·æ•°æ®çš„å°‘é‡è®­ç»ƒç”Ÿæˆçš„é€šç”¨é¢‘ç‡æ‰°åŠ¨ã€‚è¿™äº›é«˜åº¦å¯å¡‘çš„é¢‘ç‡åŸŸå™ªå£°æ–‘å—èƒ½å¤Ÿå®ç°å®æ—¶ã€è½»é‡çº§çš„ä¿æŠ¤ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯è·¨å˜é•¿éŸ³é¢‘ä½¿ç”¨ï¼Œå¹¶å…·æœ‰æŠµæŠ—è¯­éŸ³æ·±åº¦ä¼ªé€ æ”»å‡»çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¿ç•™æ„ŸçŸ¥è´¨é‡å’Œè¯­éŸ³æ¸…æ™°åº¦ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸å…­ç§æœ€å…ˆè¿›çš„å¯¹ç­–ç›¸æ¯”ï¼ŒEnkiduçš„å¤„ç†å†…å­˜æ•ˆç‡æé«˜äº†50è‡³200å€ï¼ˆä½è‡³0.004GBï¼‰ï¼Œè¿è¡Œæ—¶æ•ˆç‡æé«˜äº†3è‡³7000å€ï¼ˆå®æ—¶ç³»æ•°ä½è‡³0.004ï¼‰ã€‚åœ¨å…­ä¸ªä¸»æµæ–‡æœ¬åˆ°è¯­éŸ³æ¨¡å‹å’Œäº”ä¸ªå…ˆè¿›çš„è‡ªåŠ¨è¯´è¯äººéªŒè¯æ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜äº†Enkiduåœ¨é˜²èŒƒæ™®é€šå’Œè‡ªé€‚åº”è¯­éŸ³æ·±åº¦ä¼ªé€ æ”»å‡»æ–¹é¢çš„æœ‰æ•ˆæ€§ã€å¯è¿ç§»æ€§å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12932v1">PDF</a> Accepted by ACM MM 2025</p>
<p><strong>Summary</strong><br>     éšç€è¯­éŸ³æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œç”¨æˆ·éŸ³é¢‘éšç§é¢ä¸´ä¸¥é‡å¨èƒã€‚æ”»å‡»è€…è¶Šæ¥è¶Šå¤šåœ°åˆ©ç”¨å…¬å¼€è¯­éŸ³æ•°æ®ç”Ÿæˆä»¤äººä¿¡æœçš„è™šå‡éŸ³é¢‘ï¼Œç”¨äºèº«ä»½ç›—çªƒã€é‡‘èæ¬ºè¯ˆå’Œè™šå‡ä¿¡æ¯å®£ä¼ ç­‰æ¶æ„ç›®çš„ã€‚é’ˆå¯¹ç°æœ‰é˜²å¾¡æ–¹æ³•é¢ä¸´çš„å…³é”®å±€é™ï¼Œå¦‚ä¸é€‚åº”æœªè§ç”¨æˆ·æ•°æ®ã€éš¾ä»¥å¤„ç†é•¿éŸ³é¢‘ã€è¿‡äºä¾èµ–ç™½ç›’çŸ¥è¯†å’ŒåŠ å¯†è¿‡ç¨‹ä¸­çš„é«˜è®¡ç®—åŠæ—¶é—´æˆæœ¬ï¼Œæˆ‘ä»¬æå‡ºäº†Enkiduè¿™ä¸€é¢å‘ç”¨æˆ·çš„æ–°å‹éšç§ä¿æŠ¤æ¡†æ¶ã€‚å®ƒé€šè¿‡é»‘ç›’çŸ¥è¯†å’Œå°‘é‡ç”¨æˆ·æ•°æ®çš„çŸ­è®­ç»ƒç”Ÿæˆé€šç”¨é¢‘åŸŸæ‰°åŠ¨ï¼Œè¿™äº›é«˜åº¦çµæ´»çš„é¢‘åŸŸå™ªå£°å—å¯å®ç°å®æ—¶ã€è½»é‡çº§çš„ä¿æŠ¤ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯æŠµå¾¡ä¸ªæ€§åŒ–è¯­éŸ³æ·±åº¦ä¼ªé€ å¨èƒã€‚ä¸å…¶ä»–å…­ç§æœ€å…ˆè¿›çš„å¯¹ç­–ç›¸æ¯”ï¼ŒEnkiduåœ¨å¤„ç†å†…å­˜æ•ˆç‡ä¸Šæé«˜äº†50è‡³200å€ï¼ˆä½è‡³0.004åƒå…†å­—èŠ‚ï¼‰ï¼Œè¿è¡Œæ—¶é—´æ•ˆç‡æé«˜äº†3è‡³7000å€ï¼ˆå®æ—¶ç³»æ•°ä½è‡³0.004ï¼‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æ·±åº¦ä¼ªé€ æŠ€æœ¯å¸¦æ¥çš„ç”¨æˆ·éŸ³é¢‘éšç§å¨èƒã€‚</li>
<li>ç°æœ‰é˜²å¾¡æ–¹æ³•å­˜åœ¨ä¸é€‚åº”æœªè§ç”¨æˆ·æ•°æ®ã€éš¾ä»¥å¤„ç†é•¿éŸ³é¢‘ã€ä¾èµ–ç™½ç›’çŸ¥è¯†å’ŒåŠ å¯†è¿‡ç¨‹æˆæœ¬é«˜æ˜‚ç­‰å±€é™ã€‚</li>
<li>Enkiduæ˜¯ä¸€ä¸ªé¢å‘ç”¨æˆ·çš„æ–°å‹éšç§ä¿æŠ¤æ¡†æ¶ï¼Œåˆ©ç”¨é»‘ç›’çŸ¥è¯†å’Œå°‘é‡ç”¨æˆ·æ•°æ®çŸ­è®­ç»ƒç”Ÿæˆé€šç”¨é¢‘åŸŸæ‰°åŠ¨ã€‚</li>
<li>Enkiduå…·æœ‰å®æ—¶ã€è½»é‡çº§ä¿æŠ¤èƒ½åŠ›ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºï¼Œå¯æŠµå¾¡ä¸ªæ€§åŒ–è¯­éŸ³æ·±åº¦ä¼ªé€ å¨èƒã€‚</li>
<li>Enkiduåœ¨å¤„ç†å†…å­˜æ•ˆç‡å’Œè¿è¡Œæ—¶é—´æ•ˆç‡ä¸Šç›¸æ¯”å…¶ä»–å¯¹ç­–æœ‰æ˜¾è‘—æé«˜ã€‚</li>
<li>Enkidué€šè¿‡å¹¿æ³›å®éªŒéªŒè¯äº†åœ¨æŠµå¾¡ä¼ ç»Ÿå’Œé€‚åº”æ€§è¯­éŸ³æ·±åº¦ä¼ªé€ æ”»å‡»æ–¹é¢çš„æœ‰æ•ˆæ€§ã€å¯è¿ç§»æ€§å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12932">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-aa8f6b9ec47269bf00453b4c0b15d608.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8c2864493724a5830409850e421800cd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-32485f8658bacc5c7c62b99c311e40c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-da705af91dc941b857aced80eb29c6cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-44788bda289b7f28be7524f64c5374a4.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Towards-Scalable-AASIST-Refining-Graph-Attention-for-Speech-Deepfake-Detection"><a href="#Towards-Scalable-AASIST-Refining-Graph-Attention-for-Speech-Deepfake-Detection" class="headerlink" title="Towards Scalable AASIST: Refining Graph Attention for Speech Deepfake   Detection"></a>Towards Scalable AASIST: Refining Graph Attention for Speech Deepfake   Detection</h2><p><strong>Authors:Ivan Viakhirev, Daniil Sirota, Aleksandr Smirnov, Kirill Borodin</strong></p>
<p>Advances in voice conversion and text-to-speech synthesis have made automatic speaker verification (ASV) systems more susceptible to spoofing attacks. This work explores modest refinements to the AASIST anti-spoofing architecture. It incorporates a frozen Wav2Vec 2.0 encoder to retain self-supervised speech representations in limited-data settings, substitutes the original graph attention block with a standardized multi-head attention module using heterogeneous query projections, and replaces heuristic frame-segment fusion with a trainable, context-aware integration layer. When evaluated on the ASVspoof 5 corpus, the proposed system reaches a 7.6% equal error rate (EER), improving on a re-implemented AASIST baseline under the same training conditions. Ablation experiments suggest that each architectural change contributes to the overall performance, indicating that targeted adjustments to established models may help strengthen speech deepfake detection in practical scenarios. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/KORALLLL/AASIST_SCALING">https://github.com/KORALLLL/AASIST_SCALING</a>. </p>
<blockquote>
<p>è¯­éŸ³è½¬æ¢å’Œæ–‡æœ¬è½¬è¯­éŸ³åˆæˆæŠ€æœ¯çš„è¿›æ­¥ä½¿å¾—è‡ªåŠ¨è¯´è¯äººéªŒè¯ï¼ˆASVï¼‰ç³»ç»Ÿæ›´å®¹æ˜“å—åˆ°æ¬ºéª—æ”»å‡»ã€‚è¿™é¡¹å·¥ä½œæ¢ç´¢äº†AASISTåæ¬ºéª—æ¶æ„çš„é€‚åº¦æ”¹è¿›ã€‚å®ƒç»“åˆäº†å†»ç»“çš„Wav2Vec 2.0ç¼–ç å™¨ï¼Œä»¥åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ä¿ç•™è‡ªæˆ‘ç›‘ç£çš„è¯­éŸ³è¡¨ç¤ºï¼Œç”¨æ ‡å‡†åŒ–çš„å¤šå¤´æ³¨æ„åŠ›æ¨¡å—æ›¿ä»£äº†åŸå§‹çš„å›¾æ³¨æ„åŠ›å—ï¼Œè¯¥æ¨¡å—ä½¿ç”¨å¼‚æ„å›¾æŸ¥è¯¢æŠ•å½±ï¼Œå¹¶ç”¨å¯è®­ç»ƒçš„ã€å…·æœ‰ä¸Šä¸‹æ–‡æ„è¯†çš„é›†æˆå±‚æ›¿ä»£äº†å¯å‘å¼å¸§æ®µèåˆã€‚åœ¨ASVspoof 5è¯­æ–™åº“ä¸Šè¯„ä¼°æ—¶ï¼Œæ‰€æå‡ºçš„ç³»ç»Ÿè¾¾åˆ°äº†7.6%çš„ç­‰é”™è¯¯ç‡ï¼ˆEERï¼‰ï¼Œåœ¨ç›¸åŒçš„è®­ç»ƒæ¡ä»¶ä¸‹æ”¹è¿›äº†é‡æ–°å®ç°çš„AASISTåŸºçº¿ã€‚æ¶ˆèå®éªŒè¡¨æ˜ï¼Œæ¯ä¸ªæ¶æ„å˜åŒ–éƒ½å¯¹æ•´ä½“æ€§èƒ½æœ‰æ‰€è´¡çŒ®ï¼Œè¿™è¡¨æ˜å¯¹å·²æœ‰æ¨¡å‹çš„æœ‰é’ˆå¯¹æ€§çš„è°ƒæ•´å¯èƒ½æœ‰åŠ©äºåŠ å¼ºå®é™…åœºæ™¯ä¸­çš„è¯­éŸ³æ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/KORALLLL/AASIST_SCALING%E3%80%82">https://github.com/KORALLLL/AASIST_SCALINGã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11777v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†é’ˆå¯¹è‡ªåŠ¨è¯´è¯äººéªŒè¯ï¼ˆASVï¼‰ç³»ç»Ÿçš„åæ¬ºéª—æ¶æ„AASISTçš„æ”¹è¿›ã€‚é€šè¿‡å¼•å…¥å†»ç»“çš„Wav2Vec 2.0ç¼–ç å™¨ä»¥åœ¨æœ‰é™æ•°æ®ç¯å¢ƒä¸­ä¿ç•™è‡ªæˆ‘ç›‘ç£çš„è¯­éŸ³è¡¨ç¤ºï¼Œä½¿ç”¨æ ‡å‡†åŒ–çš„å¤šå¤´æ³¨æ„åŠ›æ¨¡å—æ›¿ä»£åŸå§‹çš„å›¾æ³¨æ„åŠ›å—ï¼Œå¹¶ç”¨å¯è®­ç»ƒçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èåˆå±‚æ›¿ä»£å¯å‘å¼å¸§æ®µèåˆï¼Œæé«˜äº†ç³»ç»Ÿçš„æ€§èƒ½ã€‚åœ¨ASVspoof 5è¯­æ–™åº“ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿè¾¾åˆ°äº†7.6%çš„ç­‰é”™è¯¯ç‡ï¼ˆEERï¼‰ï¼Œç›¸è¾ƒäºç›¸åŒè®­ç»ƒæ¡ä»¶ä¸‹çš„é‡æ–°å®ç°çš„AASISTåŸºçº¿æœ‰æ‰€æ”¹å–„ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³è½¬æ¢å’Œæ–‡æœ¬-è¯­éŸ³åˆæˆæŠ€æœ¯çš„è¿›æ­¥ä½¿å¾—è‡ªåŠ¨è¯´è¯äººéªŒè¯ç³»ç»Ÿæ›´å®¹æ˜“å—åˆ°æ¬ºéª—æ”»å‡»ã€‚</li>
<li>å¯¹AASISTåæ¬ºéª—æ¶æ„è¿›è¡Œäº†é€‚åº¦æ”¹è¿›ä»¥æé«˜æ€§èƒ½ã€‚</li>
<li>å¼•å…¥å†»ç»“çš„Wav2Vec 2.0ç¼–ç å™¨ä»¥åœ¨æœ‰é™æ•°æ®ç¯å¢ƒä¸­ä¿ç•™è‡ªæˆ‘ç›‘ç£çš„è¯­éŸ³è¡¨ç¤ºã€‚</li>
<li>ä½¿ç”¨æ ‡å‡†åŒ–çš„å¤šå¤´æ³¨æ„åŠ›æ¨¡å—æ›¿ä»£äº†åŸå§‹çš„å›¾æ³¨æ„åŠ›å—ã€‚</li>
<li>å¯å‘å¼å¸§æ®µèåˆè¢«å¯è®­ç»ƒçš„ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„èåˆå±‚æ‰€æ›¿ä»£ã€‚</li>
<li>åœ¨ASVspoof 5è¯­æ–™åº“ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œç³»ç»Ÿè¾¾åˆ°äº†7.6%çš„ç­‰é”™è¯¯ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d07684559cc4e4d8340fc54d253190be.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8f16b5e813cc989ca81adfb8db1ae58d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-36d83a60f3f17cbcc6ce408948e4d4e0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-570e2998b2e9529ad45886bc0b66bcc1.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Supporting-SENCOTEN-Language-Documentation-Efforts-with-Automatic-Speech-Recognition"><a href="#Supporting-SENCOTEN-Language-Documentation-Efforts-with-Automatic-Speech-Recognition" class="headerlink" title="Supporting SENCOTEN Language Documentation Efforts with Automatic Speech   Recognition"></a>Supporting SENCOTEN Language Documentation Efforts with Automatic Speech   Recognition</h2><p><strong>Authors:Mengzhe Geng, Patrick Littell, Aidan Pine,  PENÃÄ†, Marc Tessier, Roland Kuhn</strong></p>
<p>The SENCOTEN language, spoken on the Saanich peninsula of southern Vancouver Island, is in the midst of vigorous language revitalization efforts to turn the tide of language loss as a result of colonial language policies. To support these on-the-ground efforts, the community is turning to digital technology. Automatic Speech Recognition (ASR) technology holds great promise for accelerating language documentation and the creation of educational resources. However, developing ASR systems for SENCOTEN is challenging due to limited data and significant vocabulary variation from its polysynthetic structure and stress-driven metathesis. To address these challenges, we propose an ASR-driven documentation pipeline that leverages augmented speech data from a text-to-speech (TTS) system and cross-lingual transfer learning with Speech Foundation Models (SFMs). An n-gram language model is also incorporated via shallow fusion or n-best restoring to maximize the use of available data. Experiments on the SENCOTEN dataset show a word error rate (WER) of 19.34% and a character error rate (CER) of 5.09% on the test set with a 57.02% out-of-vocabulary (OOV) rate. After filtering minor cedilla-related errors, WER improves to 14.32% (26.48% on unseen words) and CER to 3.45%, demonstrating the potential of our ASR-driven pipeline to support SENCOTEN language documentation. </p>
<blockquote>
<p>SENCOTENè¯­è¨€æ˜¯æ¸©å“¥åå²›å—éƒ¨SaanichåŠå²›ä¸Šçš„ä¸€ç§è¯­è¨€ï¼Œæ­£åœ¨ç§¯æè¿›è¡Œè¯­è¨€æŒ¯å…´åŠªåŠ›ï¼Œä»¥æ‰­è½¬å› æ®–æ°‘è¯­è¨€æ”¿ç­–è€Œå¯¼è‡´çš„è¯­è¨€æµå¤±è¶‹åŠ¿ã€‚ä¸ºäº†æ”¯æŒè¿™äº›å®åœ°å·¥ä½œï¼Œç¤¾åŒºæ­£è½¬å‘æ•°å­—æŠ€æœ¯ã€‚è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯åœ¨åŠ é€Ÿè¯­è¨€æ–‡æ¡£åˆ¶ä½œå’Œåˆ›å»ºæ•™è‚²èµ„æºæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºæ•°æ®æœ‰é™ä»¥åŠè¯æ±‡çš„æ˜¾è‘—å˜åŒ–ï¼ˆæ¥è‡ªå…¶ç»¼åˆç»“æ„å’Œåº”åŠ›é©±åŠ¨çš„éŸ³ä½å˜åŒ–ï¼‰ï¼Œå¼€å‘ç”¨äºSENCOTENçš„ASRç³»ç»Ÿé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ASRé©±åŠ¨çš„æ–‡æ¡£ç®¡é“ï¼Œè¯¥ç®¡é“åˆ©ç”¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„å¢å¼ºè¯­éŸ³æ•°æ®ä»¥åŠè·¨è¯­è¨€çš„è¿ç§»å­¦ä¹ ä¸è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMï¼‰ã€‚è¿˜é€šè¿‡æµ…èåˆæˆ–n-bestæ¢å¤èå…¥äº†nå…ƒè¯­è¨€æ¨¡å‹ï¼Œä»¥æœ€å¤§åŒ–åˆ©ç”¨å¯ç”¨æ•°æ®ã€‚åœ¨SENCOTENæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œæµ‹è¯•é›†çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸º19.34%ï¼Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸º5.09%ï¼Œæœªè§è¯æ±‡ï¼ˆOOVï¼‰ç‡ä¸º57.02%ã€‚è¿‡æ»¤æ‰ä¸cedillaç›¸å…³çš„å¾®å°é”™è¯¯åï¼ŒWERæé«˜åˆ°14.32%ï¼ˆæœªè§è¯æ±‡çš„é”™è¯¯ç‡ä¸º26.48%ï¼‰ï¼ŒCERä¸º3.45%ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„ASRé©±åŠ¨ç®¡é“åœ¨æ”¯æŒSENCOTENè¯­è¨€æ–‡æ¡£æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10827v2">PDF</a> Accepted by ComputEL-8</p>
<p><strong>æ‘˜è¦</strong><br>    ä½äºæ¸©å“¥åå²›å—éƒ¨è¨å°¼å¥‡åŠå²›çš„æ£®ç§‘æ»•è¯­è¨€æ­£åœ¨è¿›è¡Œæ´»åŠ›æ¢å¤å·¥ä½œï¼Œæ—¨åœ¨æ‰­è½¬å› æ®–æ°‘è¯­è¨€æ”¿ç­–å¯¼è‡´çš„è¯­è¨€æµå¤±è¶‹åŠ¿ã€‚ç¤¾åŒºæ­£å€ŸåŠ©æ•°å­—æŠ€æœ¯æ”¯æŒè¿™äº›å®åœ°å·¥ä½œï¼Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯å¯¹äºåŠ é€Ÿè¯­è¨€è®°å½•å’Œåˆ›å»ºæ•™è‚²èµ„æºå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œç”±äºæ£®ç§‘æ»•è¯­è¨€çš„æœ‰é™æ•°æ®å’Œè¯æ±‡å˜åŒ–å¤šç«¯ï¼ˆæ¥è‡ªå…¶åˆæˆç»“æ„å’Œå‹åŠ›é©±åŠ¨çš„å…ƒéŸ³äº¤æ›¿ï¼‰ï¼Œå¼€å‘ASRç³»ç»Ÿé¢ä¸´æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ASRé©±åŠ¨çš„æ–‡æ¡£å¤„ç†ç®¡é“ï¼Œè¯¥ç®¡é“åˆ©ç”¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„å¢å¼ºè¯­éŸ³æ•°æ®å’Œè·¨è¯­è¨€è¿ç§»å­¦ä¹ ä¸è¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMsï¼‰ã€‚é€šè¿‡ç»“åˆn-gramè¯­è¨€æ¨¡å‹ï¼ˆé€šè¿‡æµ…èåˆæˆ–n-bestæ¢å¤ï¼‰ï¼Œæœ€å¤§é™åº¦åœ°åˆ©ç”¨å¯ç”¨æ•°æ®ã€‚åœ¨æ£®ç§‘æ»•æ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼Œæµ‹è¯•é›†çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸º19.34%ï¼Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰ä¸º5.09%ï¼Œæœªè§è¯æ±‡è¡¨ï¼ˆOOVï¼‰çš„æ¯”ç‡ä¸º57.02%ã€‚è¿‡æ»¤æ‰ä¸cedillaç›¸å…³çš„è½»å¾®é”™è¯¯åï¼ŒWERæé«˜åˆ°14.32%ï¼ˆæœªè§è¯æ±‡çš„è¯†åˆ«ç‡ä¸º26.48%ï¼‰ï¼ŒCERé™ä½åˆ°3.45%ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„ASRé©±åŠ¨ç®¡é“åœ¨æ”¯æŒæ£®ç§‘æ»•è¯­è¨€æ–‡æ¡£æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>SENCOTENè¯­è¨€æ­£åœ¨ç»å†æ´»åŠ›æ¢å¤å·¥ä½œï¼Œä»¥åº”å¯¹æ®–æ°‘è¯­è¨€æ”¿ç­–å¯¼è‡´çš„è¯­è¨€æµå¤±ã€‚</li>
<li>ç¤¾åŒºå€ŸåŠ©æ•°å­—æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯ï¼ŒåŠ é€Ÿè¯­è¨€è®°å½•å’Œåˆ›å»ºæ•™è‚²èµ„æºã€‚</li>
<li>å¼€å‘é’ˆå¯¹SENCOTENè¯­è¨€çš„ASRç³»ç»Ÿé¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æœ‰é™çš„æ•°æ®å’Œè¯æ±‡å˜åŒ–å¤šç«¯ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ASRé©±åŠ¨çš„æ–‡æ¡£å¤„ç†ç®¡é“ï¼Œç»“åˆæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿçš„å¢å¼ºè¯­éŸ³æ•°æ®ã€è·¨è¯­è¨€è¿ç§»å­¦ä¹ å’Œè¯­éŸ³åŸºç¡€æ¨¡å‹ï¼ˆSFMsï¼‰ã€‚</li>
<li>é€šè¿‡èå…¥n-gramè¯­è¨€æ¨¡å‹ä¼˜åŒ–ASRç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼Œç³»ç»Ÿæ€§èƒ½åœ¨è¯é”™è¯¯ç‡ï¼ˆWERï¼‰å’Œå­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-ee7a301d7cd9886984e208f8f3ed4507.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1049d4ae6438daf03a9d1aee1979c206.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7faa1cda6a80087ad2fa87826b8ec3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3d6c5bc8b128213980f5d37a90da1437.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56d1035616f5dabd94ec322dc72a659a.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-588fabb41b5ed47c02e1bcd973ea1dea.jpg" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  Text-to-SQL Task-oriented Dialogue Ontology Construction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-517ce084a204f5d685320d201230dd74.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  Adaptively Distilled ControlNet Accelerated Training and Superior   Sampling for Medical Image Synthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26522.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
