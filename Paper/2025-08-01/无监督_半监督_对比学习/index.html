<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
    <meta name="description" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  Style-Aware Blending and Prototype-Based Cross-Contrast Consistency for   Semi-Supervised Medical Image Segmentation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-c571ebe75678812f54ec5c7bae6e15a0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    54 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-01-æ›´æ–°"><a href="#2025-08-01-æ›´æ–°" class="headerlink" title="2025-08-01 æ›´æ–°"></a>2025-08-01 æ›´æ–°</h1><h2 id="Style-Aware-Blending-and-Prototype-Based-Cross-Contrast-Consistency-for-Semi-Supervised-Medical-Image-Segmentation"><a href="#Style-Aware-Blending-and-Prototype-Based-Cross-Contrast-Consistency-for-Semi-Supervised-Medical-Image-Segmentation" class="headerlink" title="Style-Aware Blending and Prototype-Based Cross-Contrast Consistency for   Semi-Supervised Medical Image Segmentation"></a>Style-Aware Blending and Prototype-Based Cross-Contrast Consistency for   Semi-Supervised Medical Image Segmentation</h2><p><strong>Authors:Chaowei Chen, Xiang Zhang, Honglie Guo, Shunfang Wang</strong></p>
<p>Weak-strong consistency learning strategies are widely employed in semi-supervised medical image segmentation to train models by leveraging limited labeled data and enforcing weak-to-strong consistency. However, existing methods primarily focus on designing and combining various perturbation schemes, overlooking the inherent potential and limitations within the framework itself. In this paper, we first identify two critical deficiencies: (1) separated training data streams, which lead to confirmation bias dominated by the labeled stream; and (2) incomplete utilization of supervisory information, which limits exploration of strong-to-weak consistency. To tackle these challenges, we propose a style-aware blending and prototype-based cross-contrast consistency learning framework. Specifically, inspired by the empirical observation that the distribution mismatch between labeled and unlabeled data can be characterized by statistical moments, we design a style-guided distribution blending module to break the independent training data streams. Meanwhile, considering the potential noise in strong pseudo-labels, we introduce a prototype-based cross-contrast strategy to encourage the model to learn informative supervisory signals from both weak-to-strong and strong-to-weak predictions, while mitigating the adverse effects of noise. Experimental results demonstrate the effectiveness and superiority of our framework across multiple medical segmentation benchmarks under various semi-supervised settings. </p>
<blockquote>
<p>å¼±å¼ºä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥åœ¨åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œé€šè¿‡åˆ©ç”¨æœ‰é™çš„æ ‡è®°æ•°æ®å¹¶å¼ºåˆ¶æ‰§è¡Œå¼±åˆ°å¼ºçš„ä¸€è‡´æ€§æ¥è®­ç»ƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨è®¾è®¡å’Œç»„åˆå„ç§æ‰°åŠ¨æ–¹æ¡ˆï¼Œè€Œå¿½ç•¥äº†æ¡†æ¶æœ¬èº«çš„å†…åœ¨æ½œåŠ›å’Œå±€é™æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè¯†åˆ«å‡ºä¸¤ä¸ªå…³é”®ç¼ºé™·ï¼šï¼ˆ1ï¼‰è®­ç»ƒæ•°æ®æµåˆ†ç¦»ï¼Œå¯¼è‡´ä»¥æ ‡è®°æµä¸ºä¸»çš„ç¡®è®¤åè§ï¼›ï¼ˆ2ï¼‰ç›‘ç£ä¿¡æ¯åˆ©ç”¨ä¸å®Œæ•´ï¼Œé™åˆ¶äº†ä»å¼ºåˆ°å¼±çš„ä¸€è‡´æ€§çš„æ¢ç´¢ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé£æ ¼æ„ŸçŸ¥çš„æ··åˆå’ŒåŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ä¸€è‡´æ€§å­¦ä¹ æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œå—ç»éªŒè§‚å¯Ÿå¯å‘ï¼Œå³æ ‡è®°æ•°æ®å’Œæ— æ ‡è®°æ•°æ®ä¹‹é—´çš„åˆ†å¸ƒä¸åŒ¹é…å¯ä»¥é€šè¿‡ç»Ÿè®¡çŸ©æ¥è¡¨å¾ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé£æ ¼å¼•å¯¼çš„åˆ†å¸ƒæ··åˆæ¨¡å—æ¥æ‰“ç ´ç‹¬ç«‹çš„è®­ç»ƒæ•°æ®æµã€‚åŒæ—¶ï¼Œè€ƒè™‘åˆ°å¼ºä¼ªæ ‡ç­¾ä¸­çš„æ½œåœ¨å™ªå£°ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ç­–ç•¥ï¼Œä»¥é¼“åŠ±æ¨¡å‹ä»å¼±åˆ°å¼ºå’Œå¼ºåˆ°å¼±çš„é¢„æµ‹ä¸­å­¦ä¹ æœ‰ç”¨çš„ç›‘ç£ä¿¡å·ï¼ŒåŒæ—¶å‡è½»å™ªå£°çš„ä¸åˆ©å½±å“ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨å¤šä¸ªåŒ»å­¦åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸‹å…·æœ‰æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚åœ¨å„ç§åŠç›‘ç£è®¾ç½®ä¸‹éƒ½è¡¨ç°å‡ºäº†å‡ºè‰²çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20729v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ— ç›‘ç£åŠç›‘ç£å¯¹æ¯”å­¦ä¹ ä¸­çš„å¼±å¼ºä¸€è‡´æ€§å­¦ä¹ ç­–ç•¥å¹¿æ³›åº”ç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œé€šè¿‡ä½¿ç”¨æœ‰é™æ ‡è®°æ•°æ®æ‰§è¡Œè®­ç»ƒå’Œç»´æŒå¼±åˆ°å¼ºçš„ä¸€è‡´æ€§æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨è®¾è®¡ä¸åŒçš„æ‰°åŠ¨æ–¹æ¡ˆè¿›è¡Œç»„åˆï¼Œå¿½ç•¥äº†æ¡†æ¶æœ¬èº«çš„æ½œåœ¨èƒ½åŠ›å’Œå±€é™æ€§ã€‚é’ˆå¯¹ç°æœ‰ç ”ç©¶çš„ç¼ºé™·ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºé£æ ¼æ„ŸçŸ¥æ··åˆå’ŒåŸå‹äº¤å‰å¯¹æ¯”çš„ä¸€è‡´æ€§å­¦ä¹ æ¡†æ¶ï¼Œæ‰“ç ´ç‹¬ç«‹è®­ç»ƒæ•°æ®æµçš„é—®é¢˜å¹¶å……åˆ†åˆ©ç”¨ç›‘ç£ä¿¡æ¯å­¦ä¹ æ›´å‡†ç¡®çš„æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å¼±å¼ºä¸€è‡´æ€§å­¦ä¹ æ–¹æ³•ä¸»è¦å…³æ³¨è®¾è®¡æ‰°åŠ¨æ–¹æ¡ˆç»„åˆï¼Œå¿½ç•¥äº†æ¡†æ¶æœ¬èº«çš„æ½œåœ¨èƒ½åŠ›å’Œå±€é™æ€§ã€‚</li>
<li>æå‡ºä¸¤ç§å…³é”®ç¼ºé™·ï¼šåˆ†ç¦»çš„è®­ç»ƒæ•°æ®æµå¯¼è‡´ç¡®è®¤åè§ä¸»è¦å±€é™äºæ ‡è®°æµï¼›ç›‘ç£ä¿¡æ¯åˆ©ç”¨ä¸å®Œæ•´é™åˆ¶äº†ä»å¼ºåˆ°å¼±çš„ä¸€è‡´æ€§çš„æ¢ç´¢ã€‚</li>
<li>ä¸ºè§£å†³æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºé£æ ¼æ„ŸçŸ¥æ··åˆçš„åˆ†å¸ƒèåˆæ¨¡å—ï¼Œæ‰“ç ´ç‹¬ç«‹è®­ç»ƒæ•°æ®æµçš„é—®é¢˜ã€‚</li>
<li>è€ƒè™‘åˆ°å¼ºä¼ªæ ‡ç­¾çš„æ½œåœ¨å™ªå£°ï¼Œå¼•å…¥äº†åŸºäºåŸå‹çš„äº¤å‰å¯¹æ¯”ç­–ç•¥ï¼Œé¼“åŠ±æ¨¡å‹ä»å¼±åˆ°å¼ºå’Œå¼ºåˆ°å¼±çš„é¢„æµ‹ä¸­å­¦ä¹ æœ‰ç”¨çš„ç›‘ç£ä¿¡å·ï¼ŒåŒæ—¶å‡è½»å™ªå£°çš„ä¸åˆ©å½±å“ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šç§åŒ»å­¦åˆ†å‰²åŸºå‡†æµ‹è¯•ä¸‹ï¼Œè¯¥æ¡†æ¶åœ¨å„ç§åŠç›‘ç£è®¾ç½®ä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚</li>
<li>é€šè¿‡é£æ ¼æ„ŸçŸ¥æ··åˆå’ŒåŸå‹äº¤å‰å¯¹æ¯”ç­–ç•¥çš„ç»“åˆï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20729">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4eebe85ab8275c597819ed310962620d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c9a6f90eaf7eb8f231767407015c37b9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6c53c1fbfcd1b96d44e8a8acd6dcb224.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0fea33e009705b924bec10648494aa3.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="L-MCAT-Unpaired-Multimodal-Transformer-with-Contrastive-Attention-for-Label-Efficient-Satellite-Image-Classification"><a href="#L-MCAT-Unpaired-Multimodal-Transformer-with-Contrastive-Attention-for-Label-Efficient-Satellite-Image-Classification" class="headerlink" title="L-MCAT: Unpaired Multimodal Transformer with Contrastive Attention for   Label-Efficient Satellite Image Classification"></a>L-MCAT: Unpaired Multimodal Transformer with Contrastive Attention for   Label-Efficient Satellite Image Classification</h2><p><strong>Authors:Mitul Goswami, Mrinal Goswami</strong></p>
<p>We propose the Lightweight Multimodal Contrastive Attention Transformer (L-MCAT), a novel transformer-based framework for label-efficient remote sensing image classification using unpaired multimodal satellite data. L-MCAT introduces two core innovations: (1) Modality-Spectral Adapters (MSA) that compress high-dimensional sensor inputs into a unified embedding space, and (2) Unpaired Multimodal Attention Alignment (U-MAA), a contrastive self-supervised mechanism integrated into the attention layers to align heterogeneous modalities without pixel-level correspondence or labels. L-MCAT achieves 95.4% overall accuracy on the SEN12MS dataset using only 20 labels per class, outperforming state-of-the-art baselines while using 47x fewer parameters and 23x fewer FLOPs than MCTrans. It maintains over 92% accuracy even under 50% spatial misalignment, demonstrating robustness for real-world deployment. The model trains end-to-end in under 5 hours on a single consumer GPU. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†è½»é‡çº§å¤šæ¨¡æ€å¯¹æ¯”æ³¨æ„åŠ›è½¬æ¢å™¨ï¼ˆL-MCATï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè½¬æ¢å™¨çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºä½¿ç”¨éé…å¯¹çš„å¤šæ¨¡æ€å«æ˜Ÿæ•°æ®è¿›è¡Œæ ‡ç­¾æ•ˆç‡é«˜çš„é¥æ„Ÿå›¾åƒåˆ†ç±»ã€‚L-MCATå¼•å…¥äº†ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰æ¨¡æ€å…‰è°±é€‚é…å™¨ï¼ˆMSAï¼‰ï¼Œå®ƒå°†é«˜ç»´ä¼ æ„Ÿå™¨è¾“å…¥å‹ç¼©åˆ°ç»Ÿä¸€çš„åµŒå…¥ç©ºé—´ï¼›ï¼ˆ2ï¼‰éé…å¯¹å¤šæ¨¡æ€æ³¨æ„åŠ›å¯¹é½ï¼ˆU-MAAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é›†æˆåˆ°æ³¨æ„åŠ›å±‚ä¸­çš„å¯¹æ¯”è‡ªç›‘ç£æœºåˆ¶ï¼Œç”¨äºå¯¹é½ä¸åŒæ¨¡å¼ï¼Œæ— éœ€åƒç´ çº§çš„å¯¹åº”æˆ–æ ‡ç­¾ã€‚L-MCATåœ¨SEN12MSæ•°æ®é›†ä¸Šä»…ä½¿ç”¨æ¯ç±»20ä¸ªæ ‡ç­¾å°±å®ç°äº†95.4%çš„æ€»ä½“å‡†ç¡®ç‡ï¼Œä¼˜äºæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼ŒåŒæ—¶ä½¿ç”¨MCTransçš„47å€æ›´å°‘çš„å‚æ•°å’Œ23å€æ›´å°‘çš„FLOPsã€‚å³ä½¿åœ¨50%çš„ç©ºé—´é”™ä½æƒ…å†µä¸‹ï¼Œä¹Ÿèƒ½ä¿æŒè¶…è¿‡92%çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨ç°å®ä¸–ç•Œéƒ¨ç½²ä¸­çš„ç¨³å¥æ€§ã€‚è¯¥æ¨¡å‹åœ¨å•ä¸ªæ¶ˆè´¹çº§GPUä¸Šå¯ä»¥åœ¨ä¸åˆ°5å°æ—¶å†…è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20259v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†è½»é‡çº§å¤šæ¨¡æ€å¯¹æ¯”æ³¨æ„åŠ›è½¬æ¢å™¨ï¼ˆL-MCATï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè½¬æ¢å™¨çš„ã€åˆ©ç”¨æ— é…å¯¹å¤šæ¨¡æ€å«æ˜Ÿæ•°æ®è¿›è¡Œæ ‡ç­¾æ•ˆç‡é«˜çš„é¥æ„Ÿå›¾åƒåˆ†ç±»çš„æ–°æ¡†æ¶ã€‚L-MCATæœ‰ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°ç‚¹ï¼šä¸€æ˜¯æ¨¡æ€å…‰è°±é€‚é…å™¨ï¼ˆMSAï¼‰ï¼Œèƒ½å°†é«˜ç»´ä¼ æ„Ÿå™¨è¾“å…¥å‹ç¼©åˆ°ç»Ÿä¸€åµŒå…¥ç©ºé—´ï¼›äºŒæ˜¯æ— é…å¯¹å¤šæ¨¡æ€æ³¨æ„åŠ›å¯¹é½ï¼ˆU-MAAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¯¹æ¯”è‡ªç›‘ç£æœºåˆ¶ï¼Œé›†æˆåˆ°æ³¨æ„åŠ›å±‚ä¸­ï¼Œæ— éœ€åƒç´ çº§å¯¹åº”æˆ–æ ‡ç­¾å³å¯å¯¹é½ä¸åŒæ¨¡æ€ã€‚L-MCATåœ¨SEN12MSæ•°æ®é›†ä¸Šä»…ä½¿ç”¨æ¯ç±»20ä¸ªæ ‡ç­¾å°±è¾¾åˆ°äº†95.4%çš„æ•´ä½“å‡†ç¡®ç‡ï¼Œä¸”æ¯”MCTransä½¿ç”¨æ›´å°‘çš„å‚æ•°å’ŒFLOPsã€‚åœ¨50%çš„ç©ºé—´é”™ä½ä¸‹ï¼Œå®ƒä»èƒ½ä¿æŒè¶…è¿‡92%çš„å‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨ç°å®ä¸–ç•Œçš„éƒ¨ç½²ä¸­çš„ç¨³å¥æ€§ã€‚è¯¥æ¨¡å‹å¯åœ¨å•ä¸ªæ¶ˆè´¹è€…GPUä¸Šåœ¨ä¸åˆ°5å°æ—¶å†…è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>L-MCATæ˜¯ä¸€ä¸ªåŸºäºè½¬æ¢å™¨çš„æ¡†æ¶ï¼Œç”¨äºé¥æ„Ÿå›¾åƒåˆ†ç±»ã€‚</li>
<li>L-MCATåˆ©ç”¨æ— é…å¯¹å¤šæ¨¡æ€å«æ˜Ÿæ•°æ®ï¼Œå®ç°æ ‡ç­¾æ•ˆç‡é«˜çš„åˆ†ç±»ã€‚</li>
<li>æ¨¡æ€å…‰è°±é€‚é…å™¨ï¼ˆMSAï¼‰å°†é«˜ç»´ä¼ æ„Ÿå™¨è¾“å…¥å‹ç¼©åˆ°ç»Ÿä¸€åµŒå…¥ç©ºé—´ã€‚</li>
<li>æ— é…å¯¹å¤šæ¨¡æ€æ³¨æ„åŠ›å¯¹é½ï¼ˆU-MAAï¼‰æœºåˆ¶ç”¨äºå¯¹é½ä¸åŒæ¨¡æ€ï¼Œæ— éœ€åƒç´ çº§å¯¹åº”æˆ–æ ‡ç­¾ã€‚</li>
<li>L-MCATåœ¨SEN12MSæ•°æ®é›†ä¸Šå®ç°äº†é«˜å‡†ç¡®ç‡ï¼Œä¸”ä½¿ç”¨è¾ƒå°‘çš„å‚æ•°å’Œè®¡ç®—é‡ã€‚</li>
<li>L-MCATå…·æœ‰åœ¨50%ç©ºé—´é”™ä½ä¸‹çš„ç¨³å¥æ€§ï¼Œé€‚åˆç°å®ä¸–ç•Œçš„éƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-609515c19fbbbbe41a122561b0623c9e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c167aa67814cbfcda70f2d52cfa2291e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a0ad47e88b4fa31ca367d1f3aed4293.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86fa8e2de4e64df58ec716bc92ca2b6b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5476b605775574b1049f48b35b385f06.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc1705c886425160fb6f1119b477d8d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f7b3ecc707bbb4882baed4e561b39770.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-11eb55da9b68d93253c4ab0164534aef.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Audio-Vision-Contrastive-Learning-for-Phonological-Class-Recognition"><a href="#Audio-Vision-Contrastive-Learning-for-Phonological-Class-Recognition" class="headerlink" title="Audio-Vision Contrastive Learning for Phonological Class Recognition"></a>Audio-Vision Contrastive Learning for Phonological Class Recognition</h2><p><strong>Authors:Daiqi Liu, TomÃ¡s Arias-Vergara, Jana Hutter, Andreas Maier, Paula Andrea PÃ©rez-Toro</strong></p>
<p>Accurate classification of articulatory-phonological features plays a vital role in understanding human speech production and developing robust speech technologies, particularly in clinical contexts where targeted phonemic analysis and therapy can improve disease diagnosis accuracy and personalized rehabilitation. In this work, we propose a multimodal deep learning framework that combines real-time magnetic resonance imaging (rtMRI) and speech signals to classify three key articulatory dimensions: manner of articulation, place of articulation, and voicing. We perform classification on 15 phonological classes derived from the aforementioned articulatory dimensions and evaluate the system with four audio&#x2F;vision configurations: unimodal rtMRI, unimodal audio signals, multimodal middle fusion, and contrastive learning-based audio-vision fusion. Experimental results on the USC-TIMIT dataset show that our contrastive learning-based approach achieves state-of-the-art performance, with an average F1-score of 0.81, representing an absolute increase of 0.23 over the unimodal baseline. The results confirm the effectiveness of contrastive representation learning for multimodal articulatory analysis. Our code and processed dataset will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/DaE-plz/AC_Contrastive_Phonology">https://github.com/DaE-plz/AC_Contrastive_Phonology</a> to support future research. </p>
<blockquote>
<p>å‡†ç¡®åˆ†ç±»å‘éŸ³-éŸ³ç³»ç‰¹å¾å¯¹äºç†è§£äººç±»è¨€è¯­äº§ç”Ÿå’Œå‘å±•ç¨³å¥çš„è¯­éŸ³æŠ€æœ¯èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸´åºŠç¯å¢ƒä¸­ï¼Œæœ‰é’ˆå¯¹æ€§çš„éŸ³ç³»åˆ†æå’Œæ²»ç–—å¯ä»¥æé«˜ç–¾ç—…è¯Šæ–­çš„å‡†ç¡®æ€§å’Œä¸ªæ€§åŒ–çš„åº·å¤æ•ˆæœã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†å®æ—¶ç£å…±æŒ¯æˆåƒï¼ˆrtMRIï¼‰å’Œè¯­éŸ³ä¿¡å·ï¼Œå¯¹ä¸‰ä¸ªå…³é”®çš„å‘éŸ³ç»´åº¦è¿›è¡Œåˆ†ç±»ï¼šå‘éŸ³æ–¹å¼ã€å‘éŸ³éƒ¨ä½å’Œå‘å£°ã€‚æˆ‘ä»¬å¯¹ä»ä¸Šè¿°å‘éŸ³ç»´åº¦æ´¾ç”Ÿçš„15ä¸ªéŸ³ç³»ç±»åˆ«è¿›è¡Œåˆ†ç±»ï¼Œå¹¶ä½¿ç”¨å››ç§éŸ³é¢‘&#x2F;è§†è§‰é…ç½®å¯¹ç³»ç»Ÿè¿›è¡Œè¯„ä¼°ï¼šå•æ¨¡æ€rtMRIã€å•æ¨¡æ€éŸ³é¢‘ä¿¡å·ã€å¤šæ¨¡æ€ä¸­é—´èåˆå’ŒåŸºäºå¯¹æ¯”å­¦ä¹ çš„è§†å¬èåˆã€‚åœ¨USC-TIMITæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„åŸºäºå¯¹æ¯”å­¦ä¹ çš„æ–¹æ³•è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ï¼Œå¹³å‡F1åˆ†æ•°ä¸º0.81ï¼Œæ¯”å•æ¨¡æ€åŸºçº¿æé«˜äº†0.23ã€‚ç»“æœè¯å®äº†å¯¹æ¯”è¡¨ç¤ºå­¦ä¹ åœ¨å¤šæ¨¡æ€å‘éŸ³åˆ†æä¸­çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œå¤„ç†åçš„æ•°æ®é›†å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/DaE-plz/AC_Contrastive_Phonology%E4%B8%8A%E5%85%AC%E5%BC%80%EF%BC%8C%E4%BB%A5%E6%94%AF%E6%8C%81%E6%9C%AA%E6%9D%A5%E7%9A%84%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/DaE-plz/AC_Contrastive_Phonologyä¸Šå…¬å¼€ï¼Œä»¥æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17682v1">PDF</a> conference to TSD 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆå®æ—¶ç£å…±æŒ¯æˆåƒï¼ˆrtMRIï¼‰å’Œè¯­éŸ³ä¿¡å·ï¼Œå¯¹å‘éŸ³æ–¹å¼ã€å‘éŸ³éƒ¨ä½å’Œå‘éŸ³å£°éŸ³ä¸‰ä¸ªå…³é”®å‘éŸ³ç»´åº¦è¿›è¡Œåˆ†ç±»ã€‚åœ¨USC-TIMITæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå¯¹æ¯”å­¦ä¹ çš„å¤šæ¨¡æ€èåˆæ–¹æ³•è¾¾åˆ°äº†æœ€æ–°çš„æ€§èƒ½æ°´å¹³ï¼Œå¹³å‡F1åˆ†æ•°ä¸º0.81ï¼Œç›¸å¯¹äºå•æ¨¡æ€åŸºçº¿ç»å¯¹æé«˜äº†0.23ã€‚è¿™è¯å®äº†å¯¹æ¯”è¡¨ç¤ºå­¦ä¹ åœ¨æ¨¡æ€å‘éŸ³åˆ†æä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å‡†ç¡®åˆ†ç±»å‘éŸ³çš„ä¸‰å¤§ç»´åº¦ï¼šå‘éŸ³æ–¹å¼ã€å‘éŸ³éƒ¨ä½å’Œå‘éŸ³å£°éŸ³å¯¹äºç†è§£äººç±»è¨€è¯­äº§ç”Ÿå’Œç ”å‘ç¨³å¥çš„è¯­éŸ³æŠ€æœ¯è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸´åºŠç¯å¢ƒä¸­ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç»“åˆäº†å®æ—¶ç£å…±æŒ¯æˆåƒï¼ˆrtMRIï¼‰å’Œè¯­éŸ³ä¿¡å·è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>å¯¹æ¯”å­¦ä¹ åœ¨å‘éŸ³åˆ†æä¸­çš„åº”ç”¨å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå¹³å‡F1åˆ†æ•°è¾¾åˆ°äº†0.81ï¼Œç›¸è¾ƒäºå•æ¨¡æ€åŸºçº¿æœ‰æ˜¾è‘—çš„æå‡ã€‚</li>
<li>å¯¹æ¯”è¡¨ç¤ºå­¦ä¹ åœ¨æ¨¡æ€å‘éŸ³åˆ†æä¸­çš„æœ‰æ•ˆæ€§å¾—åˆ°äº†è¯å®ã€‚</li>
<li>è¯¥ç ”ç©¶å…¬å¼€äº†ä»£ç å’Œå¤„ç†è¿‡çš„æ•°æ®é›†ï¼Œä»¥ä¾¿æ”¯æŒæœªæ¥çš„ç ”ç©¶ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½æœ‰åŠ©äºæé«˜è¯­éŸ³ç›¸å…³ç–¾ç—…çš„è¯Šæ–­å‡†ç¡®æ€§å’Œä¸ªæ€§åŒ–åº·å¤æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17682">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3b2fe26f4c410784171c0b1aa8081b7d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c571ebe75678812f54ec5c7bae6e15a0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-face6e77ffac884bc410e61d7840f2a5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-144413f9c100a6525252f5960b477437.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-37d6e8ab840fc29e227cf2e8c861a52c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-68054eebcb06b02cb1bb15f9d89c4462.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bd856caf3c766b6ddaa94c0132dad8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb952f8979fce374de09e8e18714c8ac.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="CLAMP-Contrastive-Learning-with-Adaptive-Multi-loss-and-Progressive-Fusion-for-Multimodal-Aspect-Based-Sentiment-Analysis"><a href="#CLAMP-Contrastive-Learning-with-Adaptive-Multi-loss-and-Progressive-Fusion-for-Multimodal-Aspect-Based-Sentiment-Analysis" class="headerlink" title="CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive   Fusion for Multimodal Aspect-Based Sentiment Analysis"></a>CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive   Fusion for Multimodal Aspect-Based Sentiment Analysis</h2><p><strong>Authors:Xiaoqiang He</strong></p>
<p>Multimodal aspect-based sentiment analysis(MABSA) seeks to identify aspect terms within paired image-text data and determine their fine grained sentiment polarities, representing a fundamental task for improving the effectiveness of applications such as product review systems and public opinion monitoring. Existing methods face challenges such as cross modal alignment noise and insufficient consistency in fine-grained representations. While global modality alignment methods often overlook the connection between aspect terms and their corresponding local visual regions, bridging the representation gap between text and images remains a challenge. To address these limitations, this paper introduces an end to end Contrastive Learning framework with Adaptive Multi-loss and Progressive Attention Fusion(CLAMP). The framework is composed of three novel modules: Progressive Attention Fusion network, Multi-task Contrastive Learning, and Adaptive Multi-loss Aggregation. The Progressive Attention Fusion network enhances fine-grained alignment between textual features and image regions via hierarchical, multi-stage cross modal interactions, effectively suppressing irrelevant visual noise. Secondly, multi-task contrastive learning combines global modal contrast and local granularity alignment to enhance cross modal representation consistency. Adaptive Multi-loss Aggregation employs a dynamic uncertainty based weighting mechanism to calibrate loss contributions according to each taskâ€™s uncertainty, thereby mitigating gradient interference. Evaluation on standard public benchmarks demonstrates that CLAMP consistently outperforms the vast majority of existing state of the art methods. </p>
<blockquote>
<p>å¤šæ¨¡æ€åŸºäºæ–¹é¢çš„æƒ…æ„Ÿåˆ†æï¼ˆMABSAï¼‰æ—¨åœ¨è¯†åˆ«é…å¯¹å›¾åƒæ–‡æœ¬æ•°æ®ä¸­çš„æ–¹é¢æœ¯è¯­ï¼Œå¹¶ç¡®å®šå…¶ç»†ç²’åº¦æƒ…æ„Ÿææ€§ï¼Œæ˜¯æ”¹è¿›äº§å“è¯„è®ºç³»ç»Ÿå’Œèˆ†æƒ…ç›‘æµ‹ç­‰åº”ç”¨æ•ˆæœçš„åŸºç¡€ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´è·¨æ¨¡æ€å¯¹é½å™ªå£°å’Œç»†ç²’åº¦è¡¨ç¤ºä¸€è‡´æ€§ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚è™½ç„¶å…¨å±€æ¨¡æ€å¯¹é½æ–¹æ³•å¾€å¾€å¿½ç•¥äº†æ–¹é¢æœ¯è¯­ä¸å…¶å¯¹åº”çš„å±€éƒ¨è§†è§‰åŒºåŸŸä¹‹é—´çš„è”ç³»ï¼Œä½†å¼¥åˆæ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„è¡¨ç¤ºå·®è·ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸€ç§ç«¯åˆ°ç«¯çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…·æœ‰è‡ªé€‚åº”å¤šæŸå¤±å’Œæ¸è¿›å¼æ³¨æ„åŠ›èåˆï¼ˆCLAMPï¼‰ã€‚è¯¥æ¡†æ¶ç”±ä¸‰ä¸ªæ–°é¢–æ¨¡å—ç»„æˆï¼šæ¸è¿›å¼æ³¨æ„åŠ›èåˆç½‘ç»œã€å¤šä»»åŠ¡å¯¹æ¯”å­¦ä¹ å’Œè‡ªé€‚åº”å¤šæŸå¤±èšåˆã€‚æ¸è¿›å¼æ³¨æ„åŠ›èåˆç½‘ç»œé€šè¿‡åˆ†å±‚å¤šé˜¶æ®µè·¨æ¨¡æ€äº¤äº’ï¼Œæé«˜äº†æ–‡æœ¬ç‰¹å¾å’Œå›¾åƒåŒºåŸŸä¹‹é—´çš„ç²¾ç»†å¯¹é½ï¼Œæœ‰æ•ˆåœ°æŠ‘åˆ¶äº†æ— å…³çš„è§†è§‰å™ªå£°ã€‚å…¶æ¬¡ï¼Œå¤šä»»åŠ¡å¯¹æ¯”å­¦ä¹ ç»“åˆäº†å…¨å±€æ¨¡æ€å¯¹æ¯”å’Œå±€éƒ¨ç²’åº¦å¯¹é½ï¼Œä»¥æé«˜è·¨æ¨¡æ€è¡¨ç¤ºçš„ä¸€è‡´æ€§ã€‚è‡ªé€‚åº”å¤šæŸå¤±èšåˆé‡‡ç”¨åŸºäºåŠ¨æ€ä¸ç¡®å®šæ€§çš„åŠ æƒæœºåˆ¶ï¼Œæ ¹æ®æ¯é¡¹ä»»åŠ¡çš„ä¸ç¡®å®šæ€§æ¥æ ¡å‡†æŸå¤±çš„è´¡çŒ®ï¼Œä»è€Œå‡è½»æ¢¯åº¦å¹²æ‰°ã€‚åœ¨æ ‡å‡†å…¬å…±åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCLAMPå§‹ç»ˆåœ¨ç»å¤§å¤šæ•°ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16854v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¯¹æ¯”å­¦ä¹ çš„å¤šæ¨¡æ€é¢å‘æ–¹é¢çš„æƒ…æ„Ÿåˆ†æï¼ˆMABSAï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨è¯†åˆ«å›¾åƒæ–‡æœ¬æ•°æ®ä¸­çš„æ–¹é¢æœ¯è¯­å¹¶ç¡®å®šå…¶ç²¾ç»†ç²’åº¦çš„æƒ…æ„Ÿææ€§ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•çš„è·¨æ¨¡æ€å¯¹é½å™ªå£°å’Œç²¾ç»†ç²’åº¦è¡¨ç¤ºä¸ä¸€è‡´ç­‰æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶CLAMPï¼ŒåŒ…å«æ¸è¿›æ³¨æ„åŠ›èåˆç½‘ç»œã€å¤šä»»åŠ¡å¯¹æ¯”å­¦ä¹ å’Œè‡ªé€‚åº”å¤šæŸå¤±èšåˆä¸‰ä¸ªæ¨¡å—ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°è¿›è¡Œæ–‡æœ¬ä¸å›¾åƒä¹‹é—´çš„è·¨æ¨¡æ€å¯¹é½ï¼Œä»è€Œåœ¨äº§å“è¯„è®ºç³»ç»Ÿã€å…¬ä¼—æ„è§ç›‘æµ‹ç­‰åº”ç”¨ä¸­å®ç°æ›´å‡†ç¡®çš„æƒ…æ„Ÿåˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MABSAçš„ç›®æ ‡æ˜¯è¯†åˆ«å›¾åƒæ–‡æœ¬æ•°æ®ä¸­çš„æ–¹é¢æœ¯è¯­å¹¶ç¡®å®šå…¶ç²¾ç»†ç²’åº¦çš„æƒ…æ„Ÿææ€§ï¼Œå¯¹äºäº§å“è¯„è®ºç³»ç»Ÿå’Œå…¬ä¼—æ„è§ç›‘æµ‹ç­‰åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤šæ¨¡æ€é¢å‘æ–¹é¢çš„æƒ…æ„Ÿåˆ†æä¸Šé¢ä¸´è·¨æ¨¡æ€å¯¹é½å™ªå£°å’Œç²¾ç»†ç²’åº¦è¡¨ç¤ºä¸ä¸€è‡´ç­‰æŒ‘æˆ˜ã€‚</li>
<li>CLAMPæ¡†æ¶é€šè¿‡æ¸è¿›æ³¨æ„åŠ›èåˆç½‘ç»œå®ç°æ–‡æœ¬ä¸å›¾åƒä¹‹é—´çš„ç²¾ç»†ç²’åº¦å¯¹é½ï¼Œé€šè¿‡å¤šå±‚æ¬¡ã€å¤šé˜¶æ®µçš„è·¨æ¨¡æ€äº¤äº’æœ‰æ•ˆæŠ‘åˆ¶æ— å…³çš„è§†è§‰å™ªå£°ã€‚</li>
<li>å¤šä»»åŠ¡å¯¹æ¯”å­¦ä¹ ç»“åˆäº†å…¨å±€æ¨¡æ€å¯¹æ¯”å’Œå±€éƒ¨ç²’åº¦å¯¹é½ï¼Œå¢å¼ºäº†è·¨æ¨¡æ€è¡¨ç¤ºçš„çš„ä¸€è‡´æ€§ã€‚</li>
<li>è‡ªé€‚åº”å¤šæŸå¤±èšåˆæ ¹æ®ä»»åŠ¡çš„ä¸ç¡®å®šæ€§åŠ¨æ€è°ƒæ•´æŸå¤±è´¡çŒ®çš„æƒé‡ï¼Œå‡è½»äº†æ¢¯åº¦å¹²æ‰°çš„é—®é¢˜ã€‚</li>
<li>åœ¨å…¬å…±æ ‡å‡†åŸºå‡†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCLAMPæ¡†æ¶åœ¨æ€§èƒ½ä¸ŠæŒç»­è¶…è¶Šäº†å¤§å¤šæ•°ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16854">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-293323de57945eb92b0cd8f5cba0bb12.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5929cd042ca992ad8b8ecb8019d6373.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="CLIPTTA-Robust-Contrastive-Vision-Language-Test-Time-Adaptation"><a href="#CLIPTTA-Robust-Contrastive-Vision-Language-Test-Time-Adaptation" class="headerlink" title="CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation"></a>CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation</h2><p><strong>Authors:Marc Lafon, Gustavo Adolfo Vargas Hakim, ClÃ©ment Rambour, Christian Desrosier, Nicolas Thome</strong></p>
<p>Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities but often fail to generalize under distribution shifts. Test-time adaptation (TTA) allows models to update at inference time without labeled data, typically via entropy minimization. However, this objective is fundamentally misaligned with the contrastive image-text training of VLMs, limiting adaptation performance and introducing failure modes such as pseudo-label drift and class collapse. We propose CLIPTTA, a new gradient-based TTA method for vision-language models that leverages a soft contrastive loss aligned with CLIPâ€™s pre-training objective. We provide a theoretical analysis of CLIPTTAâ€™s gradients, showing how its batch-aware design mitigates the risk of collapse. We further extend CLIPTTA to the open-set setting, where both in-distribution (ID) and out-of-distribution (OOD) samples are encountered, using an Outlier Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75 datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms entropy-based objectives and is highly competitive with state-of-the-art TTA methods, outperforming them on a large number of datasets and exhibiting more stable performance across diverse shifts. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬èƒ½åŠ›ï¼Œä½†åœ¨åˆ†å¸ƒå˜åŒ–æ—¶å¾€å¾€æ— æ³•æ³›åŒ–ã€‚æµ‹è¯•æ—¶é€‚åº”ï¼ˆTTAï¼‰å…è®¸æ¨¡å‹åœ¨æ¨ç†æ—¶é—´æ— éœ€æ ‡è®°æ•°æ®è¿›è¡Œæ›´æ–°ï¼Œé€šå¸¸é€šè¿‡ç†µæœ€å°åŒ–å®ç°ã€‚ç„¶è€Œï¼Œè¿™ä¸€ç›®æ ‡ä¸è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹æ¯”å›¾åƒæ–‡æœ¬è®­ç»ƒå­˜åœ¨æ ¹æœ¬ä¸Šçš„ä¸ä¸€è‡´ï¼Œé™åˆ¶äº†é€‚åº”æ€§èƒ½ï¼Œå¹¶å¼•å…¥äº†å¤±è´¥æ¨¡å¼ï¼Œå¦‚ä¼ªæ ‡ç­¾æ¼‚ç§»å’Œç±»åˆ«å´©æºƒã€‚æˆ‘ä»¬æå‡ºäº†CLIPTTAï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ¢¯åº¦çš„æ–°TTAæ–¹æ³•ï¼Œç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå®ƒåˆ©ç”¨ä¸CLIPé¢„è®­ç»ƒç›®æ ‡å¯¹é½çš„è½¯å¯¹æ¯”æŸå¤±ã€‚æˆ‘ä»¬å¯¹CLIPTTAçš„æ¢¯åº¦è¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå±•ç¤ºäº†å…¶æ‰¹å¤„ç†æ„ŸçŸ¥è®¾è®¡å¦‚ä½•ç¼“è§£å´©æºƒçš„é£é™©ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†CLIPTTAæ‰©å±•åˆ°å¼€æ”¾é›†ç¯å¢ƒï¼Œåœ¨è¯¥ç¯å¢ƒä¸­é‡åˆ°æ—¢æœ‰çš„å†…éƒ¨åˆ†å¸ƒï¼ˆIDï¼‰å’ŒæœªçŸ¥çš„å¤–éƒ¨åˆ†å¸ƒï¼ˆOODï¼‰æ ·æœ¬ï¼Œä½¿ç”¨å¼‚å¸¸å€¼å¯¹æ¯”æ›å…‰ï¼ˆOCEï¼‰æŸå¤±æ¥æ”¹å–„OODæ£€æµ‹ã€‚åœ¨æ¶µç›–å¤šç§åˆ†å¸ƒå˜åŒ–çš„75ä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒCLIPTTAå§‹ç»ˆä¼˜äºåŸºäºç†µçš„ç›®æ ‡ï¼Œå¹¶ä¸æœ€å…ˆè¿›çš„TTAæ–¹æ³•ç«äº‰ï¼Œåœ¨å¤§å¤šæ•°æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºå®ƒä»¬ï¼Œå¹¶åœ¨å„ç§å˜åŒ–ä¸­è¡¨ç°å‡ºæ›´ç¨³å®šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14312v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆå¦‚CLIPï¼‰çš„æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼ˆTTAï¼‰æ–°æ–¹æ³•CLIPTTAã€‚è¯¥æ–¹æ³•åˆ©ç”¨ä¸CLIPé¢„è®­ç»ƒç›®æ ‡å¯¹é½çš„è½¯å¯¹æ¯”æŸå¤±ï¼Œé€šè¿‡æ¢¯åº¦æ›´æ–°æ¨¡å‹åœ¨æ¨ç†æ—¶çš„æ€§èƒ½ã€‚CLIPTTAå…·æœ‰ç¼“è§£ä¼ªæ ‡ç­¾æ¼‚ç§»å’Œç±»åˆ«å´©æºƒç­‰é—®é¢˜çš„åŠŸèƒ½ï¼Œå¹¶é€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒCLIPTTAè¿˜æ‰©å±•åˆ°å¼€æ”¾é›†è®¾ç½®ï¼Œä½¿ç”¨å¼‚å¸¸å€¼å¯¹æ¯”æ›å…‰ï¼ˆOCEï¼‰æŸå¤±æ”¹è¿›äº†æ£€æµ‹æœªçŸ¥æ ·æœ¬çš„èƒ½åŠ›ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCLIPTTAåœ¨å¤šç§åˆ†å¸ƒè½¬ç§»ä¸Šè¡¨ç°ç¨³å®šä¸”ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>CLIPTTAæ˜¯ä¸€ç§é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹çš„æµ‹è¯•æ—¶é—´è‡ªé€‚åº”ï¼ˆTTAï¼‰æ–¹æ³•ï¼Œç”¨äºåœ¨æ¨ç†æ—¶æ›´æ–°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>CLIPTTAä½¿ç”¨è½¯å¯¹æ¯”æŸå¤±ï¼Œä¸CLIPçš„é¢„è®­ç»ƒç›®æ ‡å¯¹é½ï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>CLIPTTAé€šè¿‡ç†è®ºåˆ†æå’Œå®éªŒéªŒè¯å…·æœ‰ç¼“è§£ä¼ªæ ‡ç­¾æ¼‚ç§»å’Œç±»åˆ«å´©æºƒç­‰é—®é¢˜çš„åŠŸèƒ½ã€‚</li>
<li>CLIPTTAæ‰©å±•åˆ°å¼€æ”¾é›†è®¾ç½®ï¼Œåˆ©ç”¨å¼‚å¸¸å€¼å¯¹æ¯”æ›å…‰ï¼ˆOCEï¼‰æŸå¤±æé«˜æ£€æµ‹æœªçŸ¥æ ·æœ¬çš„èƒ½åŠ›ã€‚</li>
<li>CLIPTTAåœ¨å¤šç§åˆ†å¸ƒè½¬ç§»ä¸Šè¡¨ç°ç¨³å®šï¼Œä¼˜äºåŸºäºç†µçš„ç›®æ ‡å’Œå…¶ä»–å…ˆè¿›çš„TTAæ–¹æ³•ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒCLIPTTAåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹éƒ½èƒ½å–å¾—è‰¯å¥½çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14312">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-83b7506cfa32994a0cb30b6e92df5c3c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81c2418f93ca2d3b5c36e9d838fd654d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f091df1ffebb52eb77a0b370ff7aaed8.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TRIQA-Image-Quality-Assessment-by-Contrastive-Pretraining-on-Ordered-Distortion-Triplets"><a href="#TRIQA-Image-Quality-Assessment-by-Contrastive-Pretraining-on-Ordered-Distortion-Triplets" class="headerlink" title="TRIQA: Image Quality Assessment by Contrastive Pretraining on Ordered   Distortion Triplets"></a>TRIQA: Image Quality Assessment by Contrastive Pretraining on Ordered   Distortion Triplets</h2><p><strong>Authors:Rajesh Sureddi, Saman Zadtootaghaj, Nabajeet Barman, Alan C. Bovik</strong></p>
<p>Image Quality Assessment (IQA) models aim to predict perceptual image quality in alignment with human judgments. No-Reference (NR) IQA remains particularly challenging due to the absence of a reference image. While deep learning has significantly advanced this field, a major hurdle in developing NR-IQA models is the limited availability of subjectively labeled data. Most existing deep learning-based NR-IQA approaches rely on pre-training on large-scale datasets before fine-tuning for IQA tasks. To further advance progress in this area, we propose a novel approach that constructs a custom dataset using a limited number of reference content images and introduces a no-reference IQA model that incorporates both content and quality features for perceptual quality prediction. Specifically, we train a quality-aware model using contrastive triplet-based learning, enabling efficient training with fewer samples while achieving strong generalization performance across publicly available datasets. Our repository is available at <a target="_blank" rel="noopener" href="https://github.com/rajeshsureddi/triqa">https://github.com/rajeshsureddi/triqa</a>. </p>
<blockquote>
<p>å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰æ¨¡å‹æ—¨åœ¨æ ¹æ®äººç±»åˆ¤æ–­é¢„æµ‹æ„ŸçŸ¥å›¾åƒè´¨é‡ã€‚æ— å‚è€ƒï¼ˆNRï¼‰IQAä»ç„¶ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºç¼ºå°‘å‚è€ƒå›¾åƒã€‚è™½ç„¶æ·±åº¦å­¦ä¹ å·²ç»å¤§å¤§æ¨åŠ¨äº†è¿™ä¸€é¢†åŸŸçš„å‘å±•ï¼Œä½†å¼€å‘NR-IQAæ¨¡å‹çš„ä¸»è¦éšœç¢æ˜¯ä¸»è§‚æ ‡ç­¾æ•°æ®çš„æœ‰é™å¯ç”¨æ€§ã€‚å¤§å¤šæ•°ç°æœ‰çš„åŸºäºæ·±åº¦å­¦ä¹ çš„NR-IQAæ–¹æ³•ä¾èµ–äºåœ¨å¤§å‹æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åå†å¯¹IQAä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†åœ¨è¿™ä¸ªé¢†åŸŸè¿›ä¸€æ­¥å–å¾—è¿›å±•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨æœ‰é™æ•°é‡çš„å‚è€ƒå†…å®¹å›¾åƒæ„å»ºè‡ªå®šä¹‰æ•°æ®é›†ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ— å‚è€ƒIQAæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å†…å®¹å’Œè´¨é‡ç‰¹å¾è¿›è¡Œæ„ŸçŸ¥è´¨é‡é¢„æµ‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºå¯¹æ¯”ä¸‰å…ƒç»„çš„å­¦ä¹ æ–¹æ³•æ¥è®­ç»ƒä¸€ä¸ªè´¨é‡æ„ŸçŸ¥æ¨¡å‹ï¼Œå¯ä»¥åœ¨è¾ƒå°‘çš„æ ·æœ¬ä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒï¼ŒåŒæ—¶åœ¨å…¬å¼€å¯ç”¨çš„æ•°æ®é›†ä¸Šå®ç°å¼ºå¤§çš„æ³›åŒ–æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»“åº“å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/rajeshsureddi/triqa%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/rajeshsureddi/triqaè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12687v1">PDF</a> 5 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆNR-IQAï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•åˆ©ç”¨æœ‰é™æ•°é‡çš„å‚è€ƒå†…å®¹å›¾åƒæ„å»ºè‡ªå®šä¹‰æ•°æ®é›†ï¼Œå¹¶å¼•å…¥ä¸€ä¸ªç»“åˆå†…å®¹å’Œè´¨é‡ç‰¹å¾çš„NR-IQAæ¨¡å‹è¿›è¡Œæ„ŸçŸ¥è´¨é‡é¢„æµ‹ã€‚é€šè¿‡é‡‡ç”¨åŸºäºå¯¹æ¯”ä¸‰å…ƒç»„å­¦ä¹ çš„æ–¹æ³•ï¼Œè¯¥æ¨¡å‹åœ¨æœ‰é™æ ·æœ¬è®­ç»ƒä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ— å‚è€ƒå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆNR-IQAï¼‰æ–°æ–¹æ³•ã€‚</li>
<li>æ„å»ºè‡ªå®šä¹‰æ•°æ®é›†ï¼Œåˆ©ç”¨æœ‰é™æ•°é‡çš„å‚è€ƒå†…å®¹å›¾åƒè¿›è¡Œè®­ç»ƒã€‚</li>
<li>å¼•å…¥äº†ç»“åˆå†…å®¹å’Œè´¨é‡ç‰¹å¾çš„NR-IQAæ¨¡å‹ã€‚</li>
<li>é‡‡ç”¨åŸºäºå¯¹æ¯”ä¸‰å…ƒç»„å­¦ä¹ çš„æ–¹æ³•ï¼Œæé«˜æ¨¡å‹æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>æ¨¡å‹åœ¨å…¬å¼€æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚</li>
<li>æä¾›äº†æ¨¡å‹ä»“åº“é“¾æ¥ï¼Œä¾¿äºå…¬ä¼—è®¿é—®å’Œè¿›ä¸€æ­¥å¼€å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12687">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b8ce4bbf532524f440b9778cd1910062.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3f19dde4975187f5278be0ebccac7fbb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-215b52cdc2ff727777f71421c2096251.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dbfb5168d4d4fa4345c5fc994de2b8b4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d97b03cc05e9672a874538109ef448c0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Language-Guided-Contrastive-Audio-Visual-Masked-Autoencoder-with-Automatically-Generated-Audio-Visual-Text-Triplets-from-Videos"><a href="#Language-Guided-Contrastive-Audio-Visual-Masked-Autoencoder-with-Automatically-Generated-Audio-Visual-Text-Triplets-from-Videos" class="headerlink" title="Language-Guided Contrastive Audio-Visual Masked Autoencoder with   Automatically Generated Audio-Visual-Text Triplets from Videos"></a>Language-Guided Contrastive Audio-Visual Masked Autoencoder with   Automatically Generated Audio-Visual-Text Triplets from Videos</h2><p><strong>Authors:Yuchi Ishikawa, Shota Nakada, Hokuto Munakata, Kazuhiro Saito, Tatsuya Komatsu, Yoshimitsu Aoki</strong></p>
<p>In this paper, we propose Language-Guided Contrastive Audio-Visual Masked Autoencoders (LG-CAV-MAE) to improve audio-visual representation learning. LG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual masked autoencoders, enabling the model to learn across audio, visual and text modalities. To train LG-CAV-MAE, we introduce an automatic method to generate audio-visual-text triplets from unlabeled videos. We first generate frame-level captions using an image captioning model and then apply CLAP-based filtering to ensure strong alignment between audio and captions. This approach yields high-quality audio-visual-text triplets without requiring manual annotations. We evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an audio-visual classification task. Our method significantly outperforms existing approaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks and a 3.2% improvement for the classification task. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†è¯­è¨€å¼•å¯¼å¯¹æ¯”è§†å¬æ©ç è‡ªç¼–ç å™¨ï¼ˆLG-CAV-MAEï¼‰ä»¥æ”¹è¿›è§†å¬è¡¨ç¤ºå­¦ä¹ ã€‚LG-CAV-MAEå°†é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨é›†æˆåˆ°å¯¹æ¯”è§†å¬æ©ç è‡ªç¼–ç å™¨ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨éŸ³é¢‘ã€è§†è§‰å’Œæ–‡æœ¬æ¨¡å¼ä¹‹é—´è¿›è¡Œå­¦ä¹ ã€‚ä¸ºäº†è®­ç»ƒLG-CAV-MAEï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆæœªæ ‡æ³¨è§†é¢‘ä¸­çš„è§†å¬æ–‡æœ¬ä¸‰å…ƒç»„çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨å›¾åƒæè¿°æ¨¡å‹ç”Ÿæˆå¸§çº§åˆ«æè¿°ï¼Œç„¶ååº”ç”¨åŸºäºCLAPçš„è¿‡æ»¤æŠ€æœ¯ä»¥ç¡®ä¿éŸ³é¢‘å’Œæè¿°ä¹‹é—´çš„å¼ºå¯¹é½ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿåœ¨æ— éœ€æ‰‹åŠ¨æ ‡æ³¨çš„æƒ…å†µä¸‹ç”Ÿæˆé«˜è´¨é‡çš„è§†å¬æ–‡æœ¬ä¸‰å…ƒç»„ã€‚æˆ‘ä»¬åœ¨è§†å¬æ£€ç´¢ä»»åŠ¡å’Œè§†å¬åˆ†ç±»ä»»åŠ¡ä¸Šè¯„ä¼°äº†LG-CAV-MAEçš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨æ£€ç´¢ä»»åŠ¡ä¸Šæé«˜äº†é«˜è¾¾5.6%çš„recall@10æŒ‡æ ‡ï¼Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸Šæé«˜äº†3.2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11967v1">PDF</a> Interspeech 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Language-Guided Contrastive Audio-Visual Masked Autoencodersï¼ˆLG-CAV-MAEï¼‰ä»¥æ”¹è¿›éŸ³é¢‘è§†è§‰è¡¨ç¤ºå­¦ä¹ ã€‚LG-CAV-MAEç»“åˆäº†é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨éŸ³é¢‘ã€è§†è§‰å’Œæ–‡æœ¬æ¨¡å¼ä¹‹é—´å­¦ä¹ ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥äº†è‡ªåŠ¨ä»éæ ‡è®°è§†é¢‘ä¸­ç”ŸæˆéŸ³é¢‘è§†è§‰æ–‡æœ¬ä¸‰å…ƒç»„çš„æ–¹æ³•ã€‚é€šè¿‡å›¾åƒæè¿°æ¨¡å‹ç”Ÿæˆå¸§çº§æè¿°ï¼Œå¹¶é‡‡ç”¨CLAPåŸºç¡€è¿‡æ»¤æŠ€æœ¯ç¡®ä¿éŸ³é¢‘ä¸æè¿°ä¹‹é—´çš„å¼ºå¯¹é½ã€‚æ­¤æ–¹æ³•æ— éœ€æ‰‹åŠ¨æ³¨é‡Šå³å¯ç”Ÿæˆé«˜è´¨é‡çš„éŸ³é¢‘è§†è§‰æ–‡æœ¬ä¸‰å…ƒç»„ã€‚åœ¨éŸ³é¢‘è§†è§‰æ£€ç´¢ä»»åŠ¡å’ŒéŸ³é¢‘è§†è§‰åˆ†ç±»ä»»åŠ¡ä¸Šè¯„ä¼°LG-CAV-MAEï¼Œè¯¥æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæ£€ç´¢ä»»åŠ¡æé«˜äº†é«˜è¾¾5.6%çš„recall@10ï¼Œåˆ†ç±»ä»»åŠ¡æé«˜äº†3.2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LG-CAV-MAEç»“åˆäº†é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨ï¼Œå®ç°è·¨éŸ³é¢‘ã€è§†è§‰å’Œæ–‡æœ¬æ¨¡å¼çš„å­¦ä¹ ã€‚</li>
<li>è®­ç»ƒLG-CAV-MAEæ—¶ï¼Œå¼•å…¥äº†ä»éæ ‡è®°è§†é¢‘ä¸­è‡ªåŠ¨ç”ŸæˆéŸ³é¢‘è§†è§‰æ–‡æœ¬ä¸‰å…ƒç»„çš„æ–¹æ³•ã€‚</li>
<li>é€šè¿‡å›¾åƒæè¿°æ¨¡å‹ç”Ÿæˆå¸§çº§æè¿°ï¼Œç¡®ä¿éŸ³é¢‘ä¸æè¿°ä¹‹é—´çš„å¼ºå¯¹é½ã€‚</li>
<li>è¯¥æ–¹æ³•æ— éœ€æ‰‹åŠ¨æ³¨é‡Šå³å¯ç”Ÿæˆé«˜è´¨é‡çš„éŸ³é¢‘è§†è§‰æ–‡æœ¬ä¸‰å…ƒç»„ã€‚</li>
<li>LG-CAV-MAEåœ¨éŸ³é¢‘è§†è§‰æ£€ç´¢ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæé«˜äº†å¬å›ç‡ã€‚</li>
<li>LG-CAV-MAEåœ¨éŸ³é¢‘è§†è§‰åˆ†ç±»ä»»åŠ¡ä¸Šä¹Ÿæœ‰æ˜¾è‘—çš„æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11967">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8f02fdc6a6d057659fde23219e2fdb09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de27d7a67306876736f827fe63403f54.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a47fc8d37fb8a00eb6f0436ff0fd57b0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-05a748017fbcb7c4f07b441f6e034cad.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="FIX-CLIP-Dual-Branch-Hierarchical-Contrastive-Learning-via-Synthetic-Captions-for-Better-Understanding-of-Long-Text"><a href="#FIX-CLIP-Dual-Branch-Hierarchical-Contrastive-Learning-via-Synthetic-Captions-for-Better-Understanding-of-Long-Text" class="headerlink" title="FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic   Captions for Better Understanding of Long Text"></a>FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic   Captions for Better Understanding of Long Text</h2><p><strong>Authors:Bingchao Wang, Zhiwei Ning, Jianyu Ding, Xuanang Gao, Yin Li, Dongsheng Jiang, Jie Yang, Wei Liu</strong></p>
<p>CLIP has shown promising performance across many short-text tasks in a zero-shot manner. However, limited by the input length of the text encoder, CLIP struggles on under-stream tasks with long-text inputs ($&gt;77$ tokens). To remedy this issue, we propose FIX-CLIP, which includes three novel modules: (1) A dual-branch training pipeline that aligns short and long texts with masked and raw images, respectively, which boosts the long-text representation while preserving the short-text ability. (2) Multiple learnable regional prompts with unidirectional masks in Transformer layers for regional information extraction. (3) A hierarchical feature alignment module in the intermediate encoder layers to promote the consistency of multi-scale features. Furthermore, we collect 30M images and utilize existing MLLMs to synthesize long-text captions for training. Extensive experiments show that FIX-CLIP achieves state-of-the-art performance on both long-text and short-text retrieval benchmarks. For downstream applications, we reveal that FIX-CLIPâ€™s text encoder delivers promising performance in a plug-and-play manner for diffusion models with long-text input. The code is available at <a target="_blank" rel="noopener" href="https://github.com/bcwang-sjtu/Fix-CLIP">https://github.com/bcwang-sjtu/Fix-CLIP</a>. </p>
<blockquote>
<p>CLIPåœ¨è®¸å¤šçŸ­æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†é›¶æ ·æœ¬æ–¹å¼ä¸‹çš„è‰¯å¥½æ€§èƒ½ã€‚ç„¶è€Œï¼Œç”±äºæ–‡æœ¬ç¼–ç å™¨çš„è¾“å…¥é•¿åº¦é™åˆ¶ï¼ŒCLIPåœ¨å¤„ç†é•¿æ–‡æœ¬è¾“å…¥ï¼ˆ&gt;77ä¸ªæ ‡è®°ï¼‰çš„æµå¼ä»»åŠ¡æ—¶é¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FIX-CLIPï¼Œå®ƒåŒ…æ‹¬ä¸‰ä¸ªæ–°é¢–æ¨¡å—ï¼šï¼ˆ1ï¼‰åŒåˆ†æ”¯è®­ç»ƒç®¡é“ï¼Œåˆ†åˆ«ç”¨æ©ç å’ŒåŸå§‹å›¾åƒå¯¹é½çŸ­æ–‡æœ¬å’Œé•¿æ–‡æœ¬ï¼Œè¿™æé«˜äº†é•¿æ–‡æœ¬è¡¨ç¤ºèƒ½åŠ›çš„åŒæ—¶ä¿ç•™äº†çŸ­æ–‡æœ¬çš„èƒ½åŠ›ã€‚ï¼ˆ2ï¼‰åœ¨Transformerå±‚ä¸­ä½¿ç”¨å¸¦æœ‰å•å‘æ©ç çš„å¯å­¦ä¹ å¤šä¸ªåŒºåŸŸæç¤ºæ¥è¿›è¡ŒåŒºåŸŸä¿¡æ¯æå–ã€‚ï¼ˆ3ï¼‰åœ¨ä¸­é—´ç¼–ç å™¨å±‚ä¸­çš„åˆ†å±‚ç‰¹å¾å¯¹é½æ¨¡å—ï¼Œä»¥ä¿ƒè¿›å¤šå°ºåº¦ç‰¹å¾çš„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¶é›†äº†3000ä¸‡å¼ å›¾åƒï¼Œå¹¶åˆ©ç”¨ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹æ¥åˆæˆé•¿æ–‡æœ¬æè¿°è¿›è¡Œè®­ç»ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFIX-CLIPåœ¨é•¿æ–‡æœ¬å’ŒçŸ­æ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸Šå‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å¯¹äºä¸‹æ¸¸åº”ç”¨ï¼Œæˆ‘ä»¬æ­ç¤ºäº†FIX-CLIPçš„æ–‡æœ¬ç¼–ç å™¨åœ¨å…·æœ‰é•¿æ–‡æœ¬è¾“å…¥çš„æ‰©æ•£æ¨¡å‹ä¸­å³æ’å³ç”¨æ–¹å¼ä¸‹å…·æœ‰è‰¯å¥½çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bcwang-sjtu/Fix-CLIP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bcwang-sjtu/Fix-CLIPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10095v2">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>CLIPæ¨¡å‹åœ¨è®¸å¤šçŸ­æ–‡æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†é•¿æ–‡æœ¬è¾“å…¥æ—¶å­˜åœ¨å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†FIX-CLIPæ¨¡å‹ï¼ŒåŒ…å«ä¸‰ä¸ªæ–°æ¨¡å—ï¼šåŒåˆ†æ”¯è®­ç»ƒç®¡é“ã€å¸¦æœ‰å•å‘æ©ç çš„å¤šåŒºåŸŸæç¤ºå’Œä¸­é—´ç¼–ç å™¨å±‚ä¸­çš„åˆ†å±‚ç‰¹å¾å¯¹é½æ¨¡å—ã€‚æˆ‘ä»¬è¿˜æ”¶é›†äº†30Må›¾åƒï¼Œå¹¶åˆ©ç”¨ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åˆæˆé•¿æ–‡æœ¬æè¿°è¿›è¡Œè®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒFIX-CLIPåœ¨é•¿çŸ­æ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°é¢†å…ˆæ°´å¹³ï¼Œå¹¶åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„å³æ’å³ç”¨æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CLIPåœ¨å¤„ç†é•¿æ–‡æœ¬è¾“å…¥æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>FIX-CLIPé€šè¿‡ä¸‰ä¸ªæ–°æ¨¡å—è§£å†³CLIPåœ¨é•¿æ–‡æœ¬å¤„ç†ä¸Šçš„é—®é¢˜ï¼šåŒåˆ†æ”¯è®­ç»ƒç®¡é“ã€å¤šåŒºåŸŸæç¤ºå’Œåˆ†å±‚ç‰¹å¾å¯¹é½ã€‚</li>
<li>FIX-CLIPæ¨¡å‹é€šè¿‡æ”¶é›†30Må›¾åƒå’Œç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹åˆæˆé•¿æ–‡æœ¬æè¿°è¿›è¡Œè®­ç»ƒã€‚</li>
<li>FIX-CLIPåœ¨é•¿çŸ­æ–‡æœ¬æ£€ç´¢åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜ç§€ã€‚</li>
<li>FIX-CLIPçš„æ–‡æœ¬ç¼–ç å™¨åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­å…·æœ‰å³æ’å³ç”¨çš„æ€§èƒ½ä¼˜åŠ¿ã€‚</li>
<li>FIX-CLIPçš„ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-91e82202fa79b4db6f86380cfb7a4578.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d39918e0848b8574fc9f3daba4d6d9bd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6b6588b4d87e75016910d47af7938a30.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3513c7fbc3fb62009e34f3ea8b397b73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c61804e6bc27fa29b6136799d7409989.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="CL-Polyp-A-Contrastive-Learning-Enhanced-Network-for-Accurate-Polyp-Segmentation"><a href="#CL-Polyp-A-Contrastive-Learning-Enhanced-Network-for-Accurate-Polyp-Segmentation" class="headerlink" title="CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp   Segmentation"></a>CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp   Segmentation</h2><p><strong>Authors:Desheng Li, Chaoliang Liu, Zhiyong Xiao</strong></p>
<p>Accurate segmentation of polyps from colonoscopy images is crucial for the early diagnosis and treatment of colorectal cancer. Most existing deep learning-based polyp segmentation methods adopt an Encoder-Decoder architecture, and some utilize multi-task frameworks that incorporate auxiliary tasks like classification to improve segmentation. However, these methods often need more labeled data and depend on task similarity, potentially limiting generalizability. To address these challenges, we propose CL-Polyp, a contrastive learning-enhanced polyp segmentation network. Our method uses contrastive learning to enhance the encoderâ€™s extraction of discriminative features by contrasting positive and negative sample pairs from polyp images. This self-supervised strategy improves visual representation without needing additional annotations. We also introduce two efficient, lightweight modules: the Modified Atrous Spatial Pyramid Pooling (MASPP) module for improved multi-scale feature fusion, and the Channel Concatenate and Element Add (CA) module to merge low-level and upsampled features for {enhanced} boundary reconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG, CVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-show that CL-Polyp consistently surpasses state-of-the-art methods. Specifically, it enhances the IoU metric by 0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets, respectively, demonstrating its effectiveness in clinical polyp segmentation. </p>
<blockquote>
<p>åœ¨ç»“è‚ é•œå›¾åƒä¸­å¯¹æ¯è‚‰è¿›è¡Œç²¾ç¡®åˆ†å‰²å¯¹äºç»“ç›´è‚ ç™Œçš„æ—©æœŸè¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚ç°æœ‰çš„å¤§å¤šæ•°åŸºäºæ·±åº¦å­¦ä¹ çš„æ¯è‚‰åˆ†å‰²æ–¹æ³•é‡‡ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œä¸€äº›æ–¹æ³•è¿˜åˆ©ç”¨å¤šä»»åŠ¡æ¡†æ¶ï¼Œç»“åˆåˆ†ç±»ç­‰è¾…åŠ©ä»»åŠ¡æ¥æé«˜åˆ†å‰²æ•ˆæœã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦æ›´å¤šçš„æ ‡è®°æ•°æ®ï¼Œå¹¶ä¾èµ–äºä»»åŠ¡ç›¸ä¼¼æ€§ï¼Œä»è€Œå¯èƒ½é™åˆ¶äº†å…¶æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†CL-Polypï¼Œä¸€ç§å¢å¼ºå¯¹æ¯”å­¦ä¹ çš„æ¯è‚‰åˆ†å‰²ç½‘ç»œã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ¥å¢å¼ºç¼–ç å™¨æå–åˆ¤åˆ«ç‰¹å¾çš„èƒ½åŠ›ï¼Œé€šè¿‡å¯¹æ¯”æ¯è‚‰å›¾åƒä¸­çš„æ­£è´Ÿæ ·æœ¬å¯¹ã€‚è¿™ç§è‡ªç›‘ç£ç­–ç•¥åœ¨ä¸éœ€è¦é¢å¤–æ³¨é‡Šçš„æƒ…å†µä¸‹æé«˜äº†è§†è§‰è¡¨ç¤ºã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ä¸ªé«˜æ•ˆã€è½»é‡çº§çš„æ¨¡å—ï¼šç”¨äºæ”¹è¿›å¤šå°ºåº¦ç‰¹å¾èåˆçš„æ”¹è¿›å‹ç©ºæ´ç©ºé—´é‡‘å­—å¡”æ± åŒ–ï¼ˆMASPPï¼‰æ¨¡å—ï¼Œä»¥åŠç”¨äºåˆå¹¶ä½çº§å’Œä¸Šé‡‡æ ·ç‰¹å¾çš„é€šé“åˆå¹¶å’Œå…ƒç´ æ·»åŠ ï¼ˆCAï¼‰æ¨¡å—ï¼Œä»¥å¢å¼ºè¾¹ç•Œé‡å»ºã€‚åœ¨Kvasir-SEGã€CVC-ClinicDBã€CVC-ColonDBã€CVC ç»“ç›´è‚ æ•°æ®åº“å’ŒETISç­‰äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒCL-Polypå§‹ç»ˆè¶…è¶Šäº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒåœ¨Kvasir-SEGå’ŒCVC-ClinicDBæ•°æ®é›†ä¸Šåˆ†åˆ«å°†IoUæŒ‡æ ‡æé«˜äº†0.011å’Œ0.020ï¼Œè¯æ˜äº†å…¶åœ¨ä¸´åºŠæ¯è‚‰åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07154v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„ç»“è‚ æ¯è‚‰åˆ†å‰²ç½‘ç»œCL-Polypï¼Œé‡‡ç”¨å¯¹æ¯”å­¦ä¹ å¢å¼ºç¼–ç å™¨æå–åˆ¤åˆ«ç‰¹å¾çš„èƒ½åŠ›ï¼Œé€šè¿‡å¯¹æ¯”ç»“è‚ æ¯è‚‰å›¾åƒä¸­çš„æ­£è´Ÿæ ·æœ¬å¯¹ï¼Œæé«˜è§†è§‰è¡¨å¾èƒ½åŠ›ï¼Œè€Œæ— éœ€é¢å¤–çš„æ ‡æ³¨ã€‚åŒæ—¶å¼•å…¥ä¸¤ä¸ªé«˜æ•ˆè½»é‡çº§æ¨¡å—â€”â€”æ”¹è¿›çš„è†¨èƒ€ç©ºé—´é‡‘å­—å¡”æ± åŒ–ï¼ˆMASPPï¼‰æ¨¡å—å’Œé€šé“åˆå¹¶ä¸å…ƒç´ æ·»åŠ ï¼ˆCAï¼‰æ¨¡å—ï¼Œåˆ†åˆ«ç”¨äºæ”¹è¿›å¤šå°ºåº¦ç‰¹å¾èåˆå’Œåˆå¹¶ä½çº§åˆ«ä¸ä¸Šé‡‡æ ·ç‰¹å¾ä»¥å¢å¼ºè¾¹ç•Œé‡å»ºã€‚åœ¨äº”ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCL-Polypçš„æ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨Kvasir-SEGå’ŒCVC-ClinicDBæ•°æ®é›†ä¸Šï¼Œäº¤å¹¶æ¯”ï¼ˆIoUï¼‰æŒ‡æ ‡åˆ†åˆ«æé«˜äº†0.011å’Œ0.020ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç»“è‚ æ¯è‚‰çš„å‡†ç¡®åˆ†å‰²å¯¹æ—©æœŸç»“è‚ ç™Œè¯Šæ–­å’Œæ²»ç–—è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰åŸºäºæ·±åº¦å­¦ä¹ çš„ç»“è‚ æ¯è‚‰åˆ†å‰²æ–¹æ³•é€šå¸¸é‡‡ç”¨Encoder-Decoderæ¶æ„ï¼Œå¹¶å€ŸåŠ©å¤šä»»åŠ¡æ¡†æ¶æé«˜åˆ†å‰²æ€§èƒ½ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„ç»“è‚ æ¯è‚‰åˆ†å‰²ç½‘ç»œCL-Polypï¼Œåˆ©ç”¨å¯¹æ¯”å­¦ä¹ æé«˜ç¼–ç å™¨æå–åˆ¤åˆ«ç‰¹å¾çš„èƒ½åŠ›ã€‚</li>
<li>CL-Polypé€šè¿‡å¯¹æ¯”ç»“è‚ æ¯è‚‰å›¾åƒä¸­çš„æ­£è´Ÿæ ·æœ¬å¯¹è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œæé«˜è§†è§‰è¡¨å¾èƒ½åŠ›ä¸”æ— éœ€é¢å¤–æ ‡æ³¨ã€‚</li>
<li>CL-Polypå¼•å…¥ä¸¤ä¸ªè½»é‡çº§æ¨¡å—ï¼Œç”¨äºæ”¹è¿›å¤šå°ºåº¦ç‰¹å¾èåˆå’Œè¾¹ç•Œé‡å»ºã€‚</li>
<li>åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCL-Polypæ€§èƒ½ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07154">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1ee75e81c4c651b3177c313831ee3134.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f50207a1cb9400e47f5dec5b2d758949.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f93f6a950ee90ac063dbf44a4661e3f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-439f92615a333ea98ab0eff110e5b654.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="WeakSupCon-Weakly-Supervised-Contrastive-Learning-for-Encoder-Pre-training"><a href="#WeakSupCon-Weakly-Supervised-Contrastive-Learning-for-Encoder-Pre-training" class="headerlink" title="WeakSupCon: Weakly Supervised Contrastive Learning for Encoder   Pre-training"></a>WeakSupCon: Weakly Supervised Contrastive Learning for Encoder   Pre-training</h2><p><strong>Authors:Bodong Zhang, Hamid Manoochehri, Xiwen Li, Beatrice S. Knudsen, Tolga Tasdizen</strong></p>
<p>Weakly supervised multiple instance learning (MIL) is a challenging task given that only bag-level labels are provided, while each bag typically contains multiple instances. This topic has been extensively studied in histopathological image analysis, where labels are usually available only at the whole slide image (WSI) level, while each WSI could be divided into thousands of small image patches for training. The dominant MIL approaches focus on feature aggregation and take fixed patch features as inputs. However, weakly supervised feature representation learning in MIL settings is always neglected. Those features used to be generated by self-supervised learning methods that do not utilize weak labels, or by foundation encoders pre-trained on other large datasets. In this paper, we propose a novel weakly supervised feature representation learning method called Weakly Supervised Contrastive Learning (WeakSupCon) that utilizes bag-level labels. In our method, we employ multi-task learning and define distinct contrastive losses for samples with different bag labels. Our experiments demonstrate that the features generated using WeakSupCon with limited computing resources significantly enhance MIL classification performance compared to self-supervised approaches across three datasets. Our WeakSupCon code is available at github.com&#x2F;BzhangURU&#x2F;Paper_WeakSupCon </p>
<blockquote>
<p>å¼±ç›‘ç£å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºåªæä¾›è¢‹çº§æ ‡ç­¾ï¼Œè€Œæ¯ä¸ªè¢‹ä¸­é€šå¸¸åŒ…å«å¤šä¸ªå®ä¾‹ã€‚è¿™ä¸€è¯é¢˜åœ¨ç—…ç†å›¾åƒåˆ†æä¸­å¾—åˆ°å¹¿æ³›ç ”ç©¶ï¼Œé€šå¸¸æ•´ä¸ªå¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰çº§åˆ«çš„æ ‡ç­¾å¯ç”¨ï¼Œè€Œæ¯ä¸ªWSIå¯ä»¥è¢«åˆ’åˆ†ä¸ºæ•°åƒä¸ªå°å›¾åƒå—ç”¨äºè®­ç»ƒã€‚ä¸»æµçš„MILæ–¹æ³•ä¾§é‡äºç‰¹å¾èšåˆï¼Œå¹¶ä»¥å›ºå®šçš„è¡¥ä¸ç‰¹å¾ä½œä¸ºè¾“å…¥ã€‚ç„¶è€Œï¼Œåœ¨MILè®¾ç½®ä¸­ï¼Œå¼±ç›‘ç£ç‰¹å¾è¡¨ç¤ºå­¦ä¹ ä¸€ç›´è¢«å¿½è§†ã€‚è¿™äº›ç‰¹å¾é€šå¸¸æ˜¯é€šè¿‡ä¸åˆ©ç”¨å¼±æ ‡ç­¾çš„è‡ªæˆ‘ç›‘ç£å­¦ä¹ æ–¹æ³•ç”Ÿæˆï¼Œæˆ–è€…ç”±åœ¨å…¶ä»–å¤§å‹æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„åŸºç¡€ç¼–ç å™¨ç”Ÿæˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¼±ç›‘ç£ç‰¹å¾è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼Œç§°ä¸ºWeakly Supervised Contrastive Learningï¼ˆWeakSupConï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è¢‹çº§æ ‡ç­¾ã€‚åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ ï¼Œå¹¶ä¸ºå…·æœ‰ä¸åŒè¢‹æ ‡ç­¾çš„æ ·æœ¬å®šä¹‰ä¸åŒçš„å¯¹æ¯”æŸå¤±ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨æœ‰é™çš„è®¡ç®—èµ„æºï¼Œé€šè¿‡WeakSupConç”Ÿæˆçš„ç‰¹å¾æ˜¾è‘—æé«˜äº†ä¸ä¸‰ä¸ªæ•°æ®é›†ä¸­çš„è‡ªæˆ‘ç›‘ç£æ–¹æ³•ç›¸æ¯”çš„MILåˆ†ç±»æ€§èƒ½ã€‚æˆ‘ä»¬çš„WeakSupConä»£ç å¯åœ¨github.com&#x2F;BzhangURU&#x2F;Paper_WeakSupConæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.04165v2">PDF</a> Medical Image Computing and Computer Assisted Intervention (MICCAI)   2025 workshop on Efficient Medical AI</p>
<p><strong>Summary</strong></p>
<p>å¼±ç›‘ç£å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ä»»åŠ¡ï¼Œä»…æä¾›è¢‹çº§æ ‡ç­¾ï¼Œè€Œæ¯ä¸ªè¢‹ä¸­é€šå¸¸åŒ…å«å¤šä¸ªå®ä¾‹ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºWeakSupConçš„å¼±ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–°æ–¹æ³•ï¼Œåˆ©ç”¨è¢‹çº§æ ‡ç­¾è¿›è¡Œç‰¹å¾è¡¨ç¤ºå­¦ä¹ ã€‚è¯¥æ–¹æ³•é‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ ï¼Œä¸ºä¸åŒè¢‹æ ‡ç­¾çš„æ ·æœ¬å®šä¹‰ä¸åŒçš„å¯¹æ¯”æŸå¤±ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨WeakSupConç”Ÿæˆçš„ç‰¹å¾åœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹ï¼Œç›¸è¾ƒäºè‡ªç›‘ç£æ–¹æ³•ï¼Œèƒ½æ˜¾è‘—æé«˜MILåˆ†ç±»æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼±ç›‘ç£å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰åœ¨ä»…æä¾›è¢‹çº§æ ‡ç­¾çš„æƒ…å†µä¸‹æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç—…ç†å›¾åƒåˆ†æé¢†åŸŸã€‚</li>
<li>ç°æœ‰çš„MILæ–¹æ³•ä¸»è¦å…³æ³¨ç‰¹å¾èšåˆï¼Œè€Œå¿½ç•¥äº†åœ¨å¼±ç›‘ç£ç¯å¢ƒä¸‹çš„ç‰¹å¾è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼±ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•WeakSupConï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è¢‹çº§æ ‡ç­¾ã€‚</li>
<li>WeakSupConé‡‡ç”¨å¤šä»»åŠ¡å­¦ä¹ ï¼Œå¹¶ä¸ºä¸åŒè¢‹æ ‡ç­¾çš„æ ·æœ¬å®šä¹‰ç‰¹å®šçš„å¯¹æ¯”æŸå¤±ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒWeakSupConåœ¨æœ‰é™çš„è®¡ç®—èµ„æºä¸‹ï¼Œèƒ½æ˜¾è‘—æé«˜MILåˆ†ç±»æ€§èƒ½ï¼Œä¼˜äºè‡ªç›‘ç£æ–¹æ³•ã€‚</li>
<li>WeakSupConä»£ç å·²å…¬å¼€ï¼Œå¯ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.04165">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-229aa89f2494b5c2c8f4d667d3a6a699.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47e9a57522a7297a49368e27c1678bb4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a8f1eded6e56286a304ea350e29840b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38b07f2b17a9544c6cd35f503ff64b4c.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Unified-3D-MRI-Representations-via-Sequence-Invariant-Contrastive-Learning"><a href="#Unified-3D-MRI-Representations-via-Sequence-Invariant-Contrastive-Learning" class="headerlink" title="Unified 3D MRI Representations via Sequence-Invariant Contrastive   Learning"></a>Unified 3D MRI Representations via Sequence-Invariant Contrastive   Learning</h2><p><strong>Authors:Liam Chalcroft, Jenny Crinion, Cathy J. Price, John Ashburner</strong></p>
<p>Self-supervised deep learning has accelerated 2D natural image analysis but remains difficult to translate into 3D MRI, where data are scarce and pre-trained 2D backbones cannot capture volumetric context. We present a \emph{sequence-invariant} self-supervised framework leveraging quantitative MRI (qMRI). By simulating multiple MRI contrasts from a single 3D qMRI scan and enforcing consistent representations across these contrasts, we learn anatomy-centric rather than sequence-specific features. The result is a single 3D encoder that excels across tasks and protocols. Experiments on healthy brain segmentation (IXI), stroke lesion segmentation (ARC), and MRI denoising show significant gains over baseline SSL approaches, especially in low-data settings (up to +8.3% Dice, +4.2 dB PSNR). It also generalises to unseen sites, supporting scalable clinical use. Code and trained models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/contrast-squared">https://github.com/liamchalcroft/contrast-squared</a> </p>
<blockquote>
<p>è‡ªç›‘ç£æ·±åº¦å­¦ä¹ å·²ç»åŠ é€Ÿäº†2Dè‡ªç„¶å›¾åƒåˆ†æï¼Œä½†å°†å…¶è½¬åŒ–ä¸º3D MRIä»ç„¶å¾ˆå›°éš¾ï¼Œå› ä¸ºMRIæ•°æ®ç¨€ç¼ºï¼Œé¢„è®­ç»ƒçš„2Dä¸»ç»“æ„æ— æ³•æ•æ‰ä½“ç§¯ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å®šé‡MRIï¼ˆqMRIï¼‰çš„åºåˆ—ä¸å˜è‡ªç›‘ç£æ¡†æ¶ã€‚é€šè¿‡æ¨¡æ‹Ÿä»å•ä¸ª3D qMRIæ‰«æä¸­è·å¾—çš„å¤šé‡MRIå¯¹æ¯”åº¦ï¼Œå¹¶å¼ºåˆ¶è¿™äº›å¯¹æ¯”åº¦ä¹‹é—´ä¿æŒä¸€è‡´çš„è¡¨ç¤ºï¼Œæˆ‘ä»¬å­¦ä¹ åˆ°çš„æ˜¯ä»¥è§£å‰–ä¸ºä¸­å¿ƒè€Œéç‰¹å®šåºåˆ—çš„ç‰¹å¾ã€‚ç»“æœæ˜¯ä¸€ä¸ªå•ä¸€çš„3Dç¼–ç å™¨ï¼Œå®ƒåœ¨å„ç§ä»»åŠ¡å’Œåè®®ä¸­éƒ½è¡¨ç°å‡ºè‰²ã€‚åœ¨å¥åº·å¤§è„‘åˆ†å‰²ï¼ˆIXIï¼‰ã€ä¸­é£ç—…ç¶åˆ†å‰²ï¼ˆARCï¼‰å’ŒMRIå»å™ªå®éªŒä¸Šï¼Œä¸ä¼ ç»Ÿçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼ˆæœ€é«˜å¯è¾¾+8.3ï¼…çš„Diceç³»æ•°ï¼Œ+4.2 dBçš„PSNRï¼‰ã€‚å®ƒè¿˜é€‚ç”¨äºæœªè§è¿‡çš„ç«™ç‚¹ï¼Œæ”¯æŒå¯æ‰©å±•çš„ä¸´åºŠä½¿ç”¨ã€‚ç›¸å…³ä»£ç å’Œè®­ç»ƒæ¨¡å‹å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/contrast-squared%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/liamchalcroft/contrast-squaredå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12057v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åˆ©ç”¨å®šé‡MRIï¼ˆqMRIï¼‰çš„åºåˆ—ä¸å˜æ€§è‡ªç›‘ç£æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿå•ä¸€3D qMRIæ‰«æçš„å¤šç§MRIå¯¹æ¯”å¹¶å¼ºåˆ¶è¿™äº›å¯¹æ¯”ä¹‹é—´ä¿æŒä¸€è‡´çš„è¡¨ç¤ºï¼Œå­¦ä¹ ä»¥è§£å‰–ä¸ºä¸­å¿ƒè€Œéç‰¹å®šåºåˆ—çš„ç‰¹å¾ã€‚è¯¥æ¡†æ¶å®ç°äº†è·¨ä»»åŠ¡å’Œåè®®çš„å•ä¸€3Dç¼–ç å™¨ï¼Œåœ¨å¥åº·å¤§è„‘åˆ†å‰²ï¼ˆIXIï¼‰ã€ä¸­é£ç—…ç¶åˆ†å‰²ï¼ˆARCï¼‰å’ŒMRIå»å™ªæ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æ•°æ®è®¾ç½®ä¸‹ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„ç«™ç‚¹ï¼Œæ”¯æŒå¯æ‰©å±•çš„ä¸´åºŠåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>åˆ©ç”¨å®šé‡MRIï¼ˆqMRIï¼‰è¿›è¡Œåºåˆ—ä¸å˜æ€§è‡ªç›‘ç£å­¦ä¹ ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿå¤šç§MRIå¯¹æ¯”æ¥å¢å¼ºè‡ªç›‘ç£å­¦ä¹ æ•ˆæœã€‚</li>
<li>æ¡†æ¶å®ç°äº†è·¨ä»»åŠ¡å’Œåè®®çš„å•ä¸€3Dç¼–ç å™¨ã€‚</li>
<li>åœ¨ä¸åŒå®éªŒï¼ˆå¦‚å¥åº·å¤§è„‘åˆ†å‰²ã€ä¸­é£ç—…ç¶åˆ†å‰²å’ŒMRIå»å™ªï¼‰ä¸­ç›¸æ¯”åŸºç¡€SSLæ–¹æ³•å–å¾—äº†æ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
<li>åœ¨ä½æ•°æ®è®¾ç½®ä¸‹è¡¨ç°ä¼˜å¼‚ï¼Œæå‡äº†è¾¾+8.3%çš„Diceç³»æ•°å’Œ+4.2 dBçš„PSNRã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿæ¨å¹¿åˆ°æœªè§è¿‡çš„ç«™ç‚¹ï¼Œæ”¯æŒå¯æ‰©å±•çš„ä¸´åºŠåº”ç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cbd68edece167c0e394d6df698ef8826.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae1048d249a80f7f2442ee94efa4eccf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-abaafaa8a84973257a8628f39371b17d.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="CABLD-Contrast-Agnostic-Brain-Landmark-Detection-with-Consistency-Based-Regularization"><a href="#CABLD-Contrast-Agnostic-Brain-Landmark-Detection-with-Consistency-Based-Regularization" class="headerlink" title="CABLD: Contrast-Agnostic Brain Landmark Detection with Consistency-Based   Regularization"></a>CABLD: Contrast-Agnostic Brain Landmark Detection with Consistency-Based   Regularization</h2><p><strong>Authors:Soorena Salari, Arash Harirpoush, Hassan Rivaz, Yiming Xiao</strong></p>
<p>Anatomical landmark detection in medical images is essential for various clinical and research applications, including disease diagnosis and surgical planning. However, manual landmark annotation is time-consuming and requires significant expertise. Existing deep learning (DL) methods often require large amounts of well-annotated data, which are costly to acquire. In this paper, we introduce CABLD, a novel self-supervised DL framework for 3D brain landmark detection in unlabeled scans with varying contrasts by using only a single reference example. To achieve this, we employed an inter-subject landmark consistency loss with an image registration loss while introducing a 3D convolution-based contrast augmentation strategy to promote model generalization to new contrasts. Additionally, we utilize an adaptive mixed loss function to schedule the contributions of different sub-tasks for optimal outcomes. We demonstrate the proposed method with the intricate task of MRI-based 3D brain landmark detection. With comprehensive experiments on four diverse clinical and public datasets, including both T1w and T2w MRI scans at different MRI field strengths, we demonstrate that CABLD outperforms the state-of-the-art methods in terms of mean radial errors (MREs) and success detection rates (SDRs). Our framework provides a robust and accurate solution for anatomical landmark detection, reducing the need for extensively annotated datasets and generalizing well across different imaging contrasts. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/HealthX-Lab/CABLD">https://github.com/HealthX-Lab/CABLD</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒä¸­çš„è§£å‰–æ ‡å¿—ç‚¹æ£€æµ‹å¯¹äºå„ç§ä¸´åºŠå’Œç ”ç©¶åº”ç”¨è‡³å…³é‡è¦ï¼ŒåŒ…æ‹¬ç–¾ç—…è¯Šæ–­å’Œæ‰‹æœ¯è§„åˆ’ã€‚ç„¶è€Œï¼Œæ‰‹åŠ¨æ ‡æ³¨æ ‡å¿—ç‚¹æ—¢è€—æ—¶åˆéœ€è¦ä¸°å¯Œçš„ä¸“ä¸šçŸ¥è¯†ã€‚ç°æœ‰çš„æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ–¹æ³•é€šå¸¸éœ€è¦å¤§é‡æ ‡æ³¨å¥½çš„æ•°æ®ï¼Œè€Œè¿™äº›æ•°æ®çš„è·å–æˆæœ¬é«˜æ˜‚ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CABLDï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„è‡ªç›‘ç£æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¯ç”¨äºåœ¨æœªæ ‡è®°çš„æ‰«æå›¾åƒä¸­å¯¹3Dè„‘æ ‡å¿—ç‚¹è¿›è¡Œæ£€æµ‹ï¼Œé€šè¿‡ä½¿ç”¨ä»…ä¸€ä¸ªå‚è€ƒæ ·æœ¬å³å¯åº”å¯¹ä¸åŒå¯¹æ¯”åº¦çš„å›¾åƒã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸»ä½“é—´æ ‡å¿—ç‚¹ä¸€è‡´æ€§æŸå¤±å’Œå›¾åƒé…å‡†æŸå¤±ï¼ŒåŒæ—¶å¼•å…¥äº†ä¸€ç§åŸºäº3Då·ç§¯çš„å¯¹æ¯”å¢å¼ºç­–ç•¥ï¼Œä»¥ä¿ƒè¿›æ¨¡å‹å¯¹æ–°å¯¹æ¯”åº¦å›¾åƒçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é‡‡ç”¨äº†ä¸€ç§è‡ªé€‚åº”æ··åˆæŸå¤±å‡½æ•°ï¼Œä»¥è°ƒåº¦ä¸åŒå­ä»»åŠ¡çš„è´¡çŒ®ä»¥è·å–æœ€ä½³ç»“æœã€‚æˆ‘ä»¬é€šè¿‡åŸºäºMRIçš„3Dè„‘æ ‡å¿—ç‚¹æ£€æµ‹è¿™ä¸€å¤æ‚ä»»åŠ¡æ¥å±•ç¤ºæ‰€æå‡ºçš„æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨å››ä¸ªä¸åŒçš„ä¸´åºŠå’Œå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼ŒåŒ…æ‹¬ä¸åŒMRIåœºå¼ºçš„T1wå’ŒT -è®ºæ–‡å·¥ä½œä¸­æœ‰å¾ˆå¤šåŒ»å­¦ä¸“æœ‰åè¯çš„è¡¨è¾¾ä¸å¤ªæ˜ç¡®æˆ–ä¸å­˜åœ¨ç›´æ¥å¯¹åº”çš„ä¸­æ–‡è¡¨è¾¾ï¼Œåœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­å¯èƒ½éœ€è¦ç»“åˆä¸“ä¸šèƒŒæ™¯è¿›è¡Œä¸€å®šçš„è§£é‡Šæˆ–å†è¡¨è¿°ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä¼šå°½åŠ›æä¾›ä¸€ä¸ªå‡†ç¡®çš„ç¿»è¯‘ã€‚å¦‚æœ‰éœ€è¦ï¼Œè¯·å’¨è¯¢åŒ»å­¦é¢†åŸŸçš„ä¸“ä¸šäººå£«ä»¥è·å–æ›´å‡†ç¡®çš„è§£é‡Šå’Œç¿»è¯‘ã€‚ä»¥ä¸‹æ˜¯åŸºäºæ‚¨æä¾›çš„æ–‡æœ¬è¿›è¡Œçš„ç¿»è¯‘ï¼š</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.17845v3">PDF</a> Accepted to ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„åŠç›‘ç£æ·±åº¦å­¦ä¹ æ¡†æ¶CABLDï¼Œç”¨äºåœ¨æ— éœ€å¤§é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œè¿›è¡Œ3Dè„‘åœ°æ ‡æ£€æµ‹ã€‚é€šè¿‡é‡‡ç”¨è·¨ä¸»ä½“åœ°æ ‡ä¸€è‡´æ€§æŸå¤±ã€å›¾åƒæ³¨å†ŒæŸå¤±å’ŒåŸºäº3Då·ç§¯çš„å¯¹æ¯”å¢å¼ºç­–ç•¥ï¼ŒCABLDèƒ½å¤Ÿåœ¨ä¸åŒçš„MRIå¯¹æ¯”åº¦æ‰«æä¸­å‡†ç¡®æ£€æµ‹åœ°æ ‡ã€‚å®éªŒè¯æ˜ï¼ŒCABLDåœ¨å¤šä¸ªä¸´åºŠå’Œå…¬å…±æ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CABLDæ˜¯ä¸€ç§æ–°å‹çš„åŠç›‘ç£æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåœ¨æ— éœ€å¤§é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œ3Dè„‘åœ°æ ‡æ£€æµ‹ã€‚</li>
<li>é€šè¿‡é‡‡ç”¨è·¨ä¸»ä½“åœ°æ ‡ä¸€è‡´æ€§æŸå¤±å’Œå›¾åƒæ³¨å†ŒæŸå¤±ï¼Œæé«˜äº†æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥åŸºäº3Då·ç§¯çš„å¯¹æ¯”å¢å¼ºç­–ç•¥ï¼Œæé«˜äº†æ¨¡å‹åœ¨ä¸åŒMRIå¯¹æ¯”åº¦ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨è‡ªé€‚åº”æ··åˆæŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–ä¸åŒå­ä»»åŠ¡çš„è´¡çŒ®ã€‚</li>
<li>åœ¨å››ä¸ªä¸åŒçš„ä¸´åºŠå’Œå…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒéªŒè¯ï¼ŒåŒ…æ‹¬ä¸åŒMRIåœºå¼ºçš„T1wå’ŒT2w MRIæ‰«æã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCABLDåœ¨å¹³å‡å¾„å‘è¯¯å·®ï¼ˆMREsï¼‰å’ŒæˆåŠŸæ£€æµ‹ç‡ï¼ˆSDRsï¼‰æ–¹é¢å‡ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.17845">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1f3b2fd64325c1328af591b9da74e80.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ba3c9d75a47aa64a089b483dc0350b72.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Positive-Augmented-Contrastive-Learning-for-Vision-and-Language-Evaluation-and-Training"><a href="#Positive-Augmented-Contrastive-Learning-for-Vision-and-Language-Evaluation-and-Training" class="headerlink" title="Positive-Augmented Contrastive Learning for Vision-and-Language   Evaluation and Training"></a>Positive-Augmented Contrastive Learning for Vision-and-Language   Evaluation and Training</h2><p><strong>Authors:Sara Sarto, Nicholas Moratelli, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara</strong></p>
<p>Despite significant advancements in caption generation, existing evaluation metrics often fail to capture the full quality or fine-grained details of captions. This is mainly due to their reliance on non-specific human-written references or noisy pre-training data. Still, finding an effective metric is crucial not only for captions evaluation but also for the generation phase. Metrics can indeed play a key role in the fine-tuning stage of captioning models, ultimately enhancing the quality of the generated captions. In this paper, we propose PAC-S++, a learnable metric that leverages the CLIP model, pre-trained on both web-collected and cleaned data and regularized through additional pairs of generated visual and textual positive samples. Exploiting this stronger and curated pre-training, we also apply PAC-S++ as a reward in the Self-Critical Sequence Training (SCST) stage typically employed to fine-tune captioning models. Extensive experiments on different image and video datasets highlight the effectiveness of PAC-S++ compared to popular metrics for the task, including its sensitivity to object hallucinations. Furthermore, we show that integrating PAC-S++ into the fine-tuning stage of a captioning model results in semantically richer captions with fewer repetitions and grammatical errors. Evaluations on out-of-domain benchmarks further demonstrate the efficacy of our fine-tuning approach in enhancing model capabilities. Source code and trained models are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/aimagelab/pacscore">https://github.com/aimagelab/pacscore</a>. </p>
<blockquote>
<p>å°½ç®¡å­—å¹•ç”Ÿæˆé¢†åŸŸå·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰çš„è¯„ä¼°æŒ‡æ ‡é€šå¸¸æ— æ³•æ•æ‰å­—å¹•çš„å®Œæ•´è´¨é‡æˆ–ç²¾ç»†ç»†èŠ‚ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬ä¾èµ–äºéç‰¹å®šçš„äººç±»ç¼–å†™å‚è€ƒæˆ–å˜ˆæ‚çš„é¢„è®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œæ‰¾åˆ°æœ‰æ•ˆçš„è¯„ä¼°æŒ‡æ ‡ä¸ä»…å¯¹å­—å¹•è¯„ä¼°è‡³å…³é‡è¦ï¼Œå¯¹ç”Ÿæˆé˜¶æ®µä¹Ÿè‡³å…³é‡è¦ã€‚å®é™…ä¸Šï¼Œè¯„ä¼°æŒ‡æ ‡åœ¨å­—å¹•æ¨¡å‹çš„å¾®è°ƒé˜¶æ®µå¯ä»¥èµ·åˆ°å…³é”®ä½œç”¨ï¼Œæœ€ç»ˆæé«˜ç”Ÿæˆå­—å¹•çš„è´¨é‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PAC-S++è¿™ä¸€å¯å­¦ä¹ çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå®ƒåˆ©ç”¨CLIPæ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨æ”¶é›†å’Œæ¸…ç†çš„ç½‘é¡µæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡ç”Ÿæˆçš„è§†è§‰å’Œæ–‡æœ¬æ­£æ ·æœ¬å¯¹è¿›è¡Œæ­£åˆ™åŒ–ã€‚åˆ©ç”¨è¿™ç§å¼ºå¤§ä¸”ç²¾é€‰çš„é¢„è®­ç»ƒï¼Œæˆ‘ä»¬è¿˜å°†åœ¨é€šå¸¸ç”¨äºå¾®è°ƒå­—å¹•æ¨¡å‹çš„è‡ªæˆ‘æ‰¹åˆ¤åºåˆ—è®­ç»ƒï¼ˆSCSTï¼‰é˜¶æ®µåº”ç”¨PAC-S++ä½œä¸ºå¥–åŠ±ã€‚åœ¨ä¸åŒå›¾åƒå’Œè§†é¢‘æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ä»»åŠ¡ä¸­æµè¡Œçš„è¯„ä¼°æŒ‡æ ‡ç›¸æ¯”ï¼ŒPAC-S++æ›´ä¸ºæœ‰æ•ˆï¼Œå…¶å¯¹å¯¹è±¡å¹»è§‰ä¹Ÿå¾ˆæ•æ„Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å±•ç¤ºå°†PAC-S++é›†æˆåˆ°å­—å¹•æ¨¡å‹çš„å¾®è°ƒé˜¶æ®µï¼Œå¯ä»¥ç”Ÿæˆè¯­ä¹‰æ›´ä¸°å¯Œã€é‡å¤æ›´å°‘ã€è¯­æ³•é”™è¯¯æ›´å°‘çš„å­—å¹•ã€‚åœ¨åŸŸå¤–åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°è¿›ä¸€æ­¥è¯æ˜äº†æˆ‘ä»¬å¾®è°ƒæ–¹æ³•åœ¨æé«˜æ¨¡å‹èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚æºä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/aimagelab/pacscore%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/aimagelab/pacscoreå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.07336v2">PDF</a> International Journal of Computer Vision (2025)</p>
<p><strong>æ‘˜è¦</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„å›¾åƒå’Œè§†é¢‘å­—å¹•ç”Ÿæˆè¯„ä¼°æ–¹æ³•PAC-S++ã€‚è¯¥æ–¹æ³•åˆ©ç”¨CLIPæ¨¡å‹ï¼Œé‡‡ç”¨ç½‘é¡µé‡‡é›†çš„æ¸…æ´æ•°æ®ä»¥åŠç”Ÿæˆçš„è§†è§‰å’Œæ–‡æœ¬æ­£æ ·æœ¬å¯¹è¿›è¡Œé¢„è®­ç»ƒï¼Œå¼ºåŒ–è¯„ä¼°å‡†ç¡®æ€§ã€‚åŒæ—¶ï¼Œå°†PAC-S++ä½œä¸ºå¥–åŠ±å‡½æ•°åº”ç”¨äºåºåˆ—è‡ªè®­ç»ƒï¼ˆSCSTï¼‰é˜¶æ®µï¼Œæé«˜å­—å¹•ç”Ÿæˆæ¨¡å‹çš„å¾®è°ƒæ•ˆæœã€‚å®éªŒè¯æ˜ï¼ŒPAC-S++ç›¸è¾ƒäºå…¶ä»–æµè¡Œè¯„ä»·æŒ‡æ ‡æ›´ä¸ºæ•æ„Ÿï¼Œèƒ½æœ‰æ•ˆè¯†åˆ«ç‰©ä½“å¹»è§‰ç°è±¡ã€‚é›†æˆåˆ°å­—å¹•æ¨¡å‹çš„å¾®è°ƒé˜¶æ®µåï¼Œèƒ½ç”Ÿæˆè¯­ä¹‰æ›´ä¸°å¯Œã€é‡å¤å’Œè¯­æ³•é”™è¯¯è¾ƒå°‘çš„å­—å¹•ã€‚è¯¥æ–¹æ³•çš„æºä»£ç å’Œè®­ç»ƒæ¨¡å‹å·²å…¬å¼€äºGitHubä¸Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å›¾åƒå’Œè§†é¢‘å­—å¹•ç”Ÿæˆè¯„ä¼°æ–¹æ³•PAC-S++ã€‚</li>
<li>åˆ©ç”¨CLIPæ¨¡å‹é¢„è®­ç»ƒï¼Œæé«˜è¯„ä¼°å‡†ç¡®æ€§ã€‚</li>
<li>PAC-S++è¢«åº”ç”¨äºåºåˆ—è‡ªè®­ç»ƒï¼ˆSCSTï¼‰é˜¶æ®µä½œä¸ºå¥–åŠ±å‡½æ•°ï¼Œå¢å¼ºå­—å¹•ç”Ÿæˆæ¨¡å‹çš„å¾®è°ƒæ•ˆæœã€‚</li>
<li>å®éªŒè¯æ˜PAC-S++ç›¸è¾ƒäºå…¶ä»–è¯„ä»·æŒ‡æ ‡æ›´ä¸ºæ•æ„Ÿï¼Œèƒ½æœ‰æ•ˆè¯†åˆ«ç‰©ä½“å¹»è§‰ç°è±¡ã€‚</li>
<li>é›†æˆåˆ°å­—å¹•æ¨¡å‹çš„å¾®è°ƒé˜¶æ®µåï¼Œèƒ½ç”Ÿæˆè¯­ä¹‰æ›´ä¸°å¯Œã€é‡å¤å’Œè¯­æ³•é”™è¯¯è¾ƒå°‘çš„å­—å¹•ã€‚</li>
<li>å…¬å¼€äº†æºä»£ç å’Œè®­ç»ƒæ¨¡å‹åœ¨GitHubä¸Šï¼Œä¾¿äºä»–äººä½¿ç”¨å’Œç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.07336">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-36d21f8367b24abcb86fd7f31c410e14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6281cefb890a12092de1506bfd03bb14.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1c7e5667f69e19f0e3617ee2c76ccfb7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="PreMix-Label-Efficient-Multiple-Instance-Learning-via-Non-Contrastive-Pre-training-and-Feature-Mixing"><a href="#PreMix-Label-Efficient-Multiple-Instance-Learning-via-Non-Contrastive-Pre-training-and-Feature-Mixing" class="headerlink" title="PreMix: Label-Efficient Multiple Instance Learning via Non-Contrastive   Pre-training and Feature Mixing"></a>PreMix: Label-Efficient Multiple Instance Learning via Non-Contrastive   Pre-training and Feature Mixing</h2><p><strong>Authors:Bryan Wong, Mun Yong Yi</strong></p>
<p>Multiple instance learning (MIL) has emerged as a powerful framework for weakly supervised whole slide image (WSI) classification, enabling slide-level predictions without requiring detailed patch-level annotations. Despite its success, a critical limitation of current MIL methods lies in the underutilization of pre-training for the MIL aggregator. Most existing approaches initialize the aggregator randomly and train it from scratch, making performance highly sensitive to the quantity of labeled WSIs and ignoring the abundance of unlabeled WSIs commonly available in clinical settings. To address this, we propose PreMix, a novel framework that leverages a non-contrastive pre-training method, Barlow Twins, augmented with the Slide Mixing approach to generate additional positive pairs and enhance feature learning, particularly under limited labeled WSI conditions. Fine-tuning with Mixup and Manifold Mixup further enhances robustness by effectively handling the diverse sizes of gigapixel WSIs. Experimental results demonstrate that integrating PreMix as a plug-in module into HIPT yields an average F1 improvement of 4.7% over the baseline HIPT across various WSI training sizes and datasets. These findings underscore its potential to advance WSI classification with limited labeled data and its applicability to real-world histopathology practices. The code is available at <a target="_blank" rel="noopener" href="https://github.com/bryanwong17/PreMix">https://github.com/bryanwong17/PreMix</a> </p>
<blockquote>
<p>å¤šå®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰å·²æˆä¸ºå¼±ç›‘ç£å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†ç±»çš„å¼ºå¤§æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€è¯¦ç»†è¡¥ä¸çº§åˆ«æ³¨é‡Šçš„æƒ…å†µä¸‹è¿›è¡Œå¹»ç¯ç‰‡çº§åˆ«çš„é¢„æµ‹ã€‚å°½ç®¡å–å¾—äº†æˆåŠŸï¼Œä½†å½“å‰MILæ–¹æ³•çš„ä¸€ä¸ªå…³é”®å±€é™æ€§åœ¨äºæœªèƒ½å……åˆ†åˆ©ç”¨MILèšåˆå™¨çš„é¢„è®­ç»ƒã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•éƒ½ä¼šéšæœºåˆå§‹åŒ–èšåˆå™¨å¹¶ä»å¤´å¼€å§‹è®­ç»ƒï¼Œè¿™ä½¿å¾—æ€§èƒ½é«˜åº¦ä¾èµ–äºæ ‡è®°çš„WSIæ•°é‡ï¼Œå¹¶å¿½ç•¥äº†ä¸´åºŠç¯å¢ƒä¸­é€šå¸¸å¤§é‡å­˜åœ¨çš„æœªæ ‡è®°çš„WSIã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†PreMixè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨éå¯¹æ¯”é¢„è®­ç»ƒæ–¹æ³•Barlow Twinsï¼Œè¾…ä»¥å¹»ç¯ç‰‡æ··åˆï¼ˆSlide Mixingï¼‰æ–¹æ³•ç”Ÿæˆé¢å¤–çš„æ­£å‘å¯¹å¹¶å¢å¼ºç‰¹å¾å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åœ¨æœ‰é™çš„æ ‡è®°WSIæ¡ä»¶ä¸‹ã€‚é€šè¿‡Mixupå’Œæµå½¢Mixupè¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥æœ‰æ•ˆå¤„ç†å‰åƒç´ WSIçš„å¤šæ ·å¤§å°ï¼Œè¿›ä¸€æ­¥æé«˜ç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°†PreMixä½œä¸ºæ’ä»¶æ¨¡å—é›†æˆåˆ°HIPTä¸­ï¼Œåœ¨å„ç§WSIè®­ç»ƒè§„æ¨¡å’Œæ•°æ®é›†ä¸Šè¾ƒåŸºçº¿HIPTå¹³å‡F1å¾—åˆ†æé«˜äº†4.7%ã€‚è¿™äº›å‘ç°çªæ˜¾äº†å…¶åœ¨æœ‰é™æ ‡è®°æ•°æ®ä¸‹æ¨è¿›WSIåˆ†ç±»çš„æ½œåŠ›ï¼Œä»¥åŠå…¶åœ¨å®é™…ç—…ç†å®è·µä¸­çš„é€‚ç”¨æ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/bryanwong17/PreMix%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/bryanwong17/PreMixæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.01162v3">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¤šé‡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰çš„å¼±ç›‘ç£å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†ç±»çš„æ–°æ¡†æ¶PreMixã€‚PreMixé€šè¿‡åˆ©ç”¨éå¯¹æ¯”é¢„è®­ç»ƒæ–¹æ³•Barlow Twinså’Œå¹»ç¯ç‰‡æ··åˆæŠ€æœ¯ï¼Œè§£å†³äº†ç°æœ‰MILæ–¹æ³•åœ¨åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹æ–¹é¢çš„ä¸è¶³ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPreMixä½œä¸ºHIPTçš„æ’ä»¶æ¨¡å—ï¼Œåœ¨æœ‰é™çš„æ ‡è®°WSIæ¡ä»¶ä¸‹ï¼Œå¹³å‡F1å¾—åˆ†æé«˜äº†4.7%ã€‚è¿™çªæ˜¾äº†PreMixåœ¨æœ‰é™æ ‡è®°æ•°æ®ä¸‹æ¨è¿›WSIåˆ†ç±»çš„æ½œåŠ›åŠå…¶åœ¨ç°å®ä¸–ç•Œç—…ç†å­¦å®è·µä¸­çš„åº”ç”¨ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šé‡å®ä¾‹å­¦ä¹ ï¼ˆMILï¼‰åœ¨å¼±ç›‘ç£å…¨å¹»ç¯ç‰‡å›¾åƒï¼ˆWSIï¼‰åˆ†ç±»ä¸­å±•ç°å‡ºå¼ºå¤§çš„æ½œåŠ›ï¼Œå¯åœ¨æ— éœ€è¯¦ç»†è¡¥ä¸çº§åˆ«æ³¨é‡Šçš„æƒ…å†µä¸‹è¿›è¡Œå¹»ç¯ç‰‡çº§åˆ«çš„é¢„æµ‹ã€‚</li>
<li>ç°æœ‰MILæ–¹æ³•çš„ä¸€ä¸ªå…³é”®å±€é™æ€§åœ¨äºæœªèƒ½å……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•éšæœºåˆå§‹åŒ–èšåˆå™¨å¹¶ä»å¤´å¼€å§‹è®­ç»ƒã€‚</li>
<li>PreMixæ¡†æ¶é€šè¿‡ç»“åˆéå¯¹æ¯”é¢„è®­ç»ƒæ–¹æ³•å’Œå¹»ç¯ç‰‡æ··åˆæŠ€æœ¯æ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç”Ÿæˆé¢å¤–çš„æ­£å¯¹å¹¶å¢å¼ºç‰¹å¾å­¦ä¹ ã€‚</li>
<li>åœ¨æœ‰é™çš„æ ‡è®°WSIæ¡ä»¶ä¸‹ï¼ŒPreMixå¢å¼ºäº†æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†åƒå…†åƒç´ å¤§å°çš„WSIæ—¶ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œå°†PreMixé›†æˆåˆ°HIPTä¸­ï¼Œåœ¨å„ç§WSIè®­ç»ƒå¤§å°å’Œæ•°æ®é›†ä¸Šå¹³å‡F1å¾—åˆ†æé«˜äº†4.7%ã€‚</li>
<li>PreMixå…·æœ‰åœ¨æœ‰é™æ ‡è®°æ•°æ®æ¡ä»¶ä¸‹æ¨è¿›WSIåˆ†ç±»çš„æ½œåŠ›ï¼Œå¹¶é€‚ç”¨äºç°å®ä¸–ç•Œçš„ç—…ç†å­¦å®è·µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.01162">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-fe95e2220ed9db9ba7f99c5f3f745882.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5de8370e394bb6264c801f0cc62a1ba7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-78757dece9ea5ccdb024873d17dc1d4a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-31683b05364f223cae05e8effc42e297.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-d9b5c985a25194e12816c9a76be2b43c.jpg" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  Subtyping Breast Lesions via Generative Augmentation based Long-tailed   Recognition in Ultrasound
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8f3306bc01ad6d0bc945599d874fa1a2.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  3D-MOOD Lifting 2D to 3D for Monocular Open-Set Object Detection
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26548.9k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
