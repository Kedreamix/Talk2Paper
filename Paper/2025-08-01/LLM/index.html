<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  SeqAffordSplat Scene-level Sequential Affordance Reasoning on 3D   Gaussian Splatting">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-aec299c9105c1951837693714ac70ade.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-01-æ›´æ–°"><a href="#2025-08-01-æ›´æ–°" class="headerlink" title="2025-08-01 æ›´æ–°"></a>2025-08-01 æ›´æ–°</h1><h2 id="SeqAffordSplat-Scene-level-Sequential-Affordance-Reasoning-on-3D-Gaussian-Splatting"><a href="#SeqAffordSplat-Scene-level-Sequential-Affordance-Reasoning-on-3D-Gaussian-Splatting" class="headerlink" title="SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D   Gaussian Splatting"></a>SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D   Gaussian Splatting</h2><p><strong>Authors:Di Li, Jie Feng, Jiahao Chen, Weisheng Dong, Guanbin Li, Yuhui Zheng, Mingtao Feng, Guangming Shi</strong></p>
<p>3D affordance reasoning, the task of associating human instructions with the functional regions of 3D objects, is a critical capability for embodied agents. Current methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited to single-object, single-step interactions, a paradigm that falls short of addressing the long-horizon, multi-object tasks required for complex real-world applications. To bridge this gap, we introduce the novel task of Sequential 3D Gaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale benchmark featuring 1800+ scenes to support research on long-horizon affordance understanding in complex 3DGS environments. We then propose SeqSplatNet, an end-to-end framework that directly maps an instruction to a sequence of 3D affordance masks. SeqSplatNet employs a large language model that autoregressively generates text interleaved with special segmentation tokens, guiding a conditional decoder to produce the corresponding 3D mask. To handle complex scene geometry, we introduce a pre-training strategy, Conditional Geometric Reconstruction, where the model learns to reconstruct complete affordance region masks from known geometric observations, thereby building a robust geometric prior. Furthermore, to resolve semantic ambiguities, we design a feature injection mechanism that lifts rich semantic features from 2D Vision Foundation Models (VFM) and fuses them into the 3D decoder at multiple scales. Extensive experiments demonstrate that our method sets a new state-of-the-art on our challenging benchmark, effectively advancing affordance reasoning from single-step interactions to complex, sequential tasks at the scene level. </p>
<blockquote>
<p>3Då¯ç”¨æ€§æ¨ç†æ˜¯å°†äººç±»æŒ‡ä»¤ä¸3Då¯¹è±¡çš„åŠŸèƒ½åŒºåŸŸç›¸å…³è”çš„ä»»åŠ¡ï¼Œè¿™æ˜¯å®ä½“ä»£ç†çš„æ ¸å¿ƒèƒ½åŠ›ã€‚ç›®å‰åŸºäº3Dé«˜æ–¯æ³¼æº…ï¼ˆ3DGSï¼‰çš„æ–¹æ³•æ ¹æœ¬ä¸Šå±€é™äºå•å¯¹è±¡ã€å•æ­¥éª¤äº¤äº’çš„æ¨¡å¼ï¼Œè¿™ç§æ¨¡å¼éš¾ä»¥æ»¡è¶³å¤æ‚ç°å®ä¸–ç•Œåº”ç”¨æ‰€éœ€çš„é•¿å‘¨æœŸã€å¤šå¯¹è±¡ä»»åŠ¡çš„è¦æ±‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†åºè´¯3Dé«˜æ–¯å¯ç”¨æ€§æ¨ç†çš„æ–°ä»»åŠ¡ï¼Œå¹¶å»ºç«‹äº†SeqAffordSplatå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«1800å¤šä¸ªåœºæ™¯ï¼Œæ”¯æŒåœ¨å¤æ‚3DGSç¯å¢ƒä¸­å¯¹é•¿æœŸå¯ç”¨æ€§ç†è§£çš„ç ”ç©¶ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†SeqSplatNetç«¯åˆ°ç«¯æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç›´æ¥å°†æŒ‡ä»¤æ˜ å°„åˆ°ä¸€ç³»åˆ—3Då¯ç”¨æ€§è’™ç‰ˆã€‚SeqSplatNeté‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é€šè¿‡è‡ªå›å½’æ–¹å¼ç”Ÿæˆä¸ç‰¹æ®Šåˆ†å‰²ä»¤ç‰Œäº¤ç»‡çš„æ–‡æœ¬ï¼Œå¼•å¯¼æ¡ä»¶è§£ç å™¨ç”Ÿæˆç›¸åº”çš„3Dè’™ç‰ˆã€‚ä¸ºäº†å¤„ç†å¤æ‚åœºæ™¯çš„å‡ ä½•ç»“æ„ï¼Œæˆ‘ä»¬å¼•å…¥äº†é¢„è®­ç»ƒç­–ç•¥ï¼Œå³æ¡ä»¶å‡ ä½•é‡å»ºï¼Œè¯¥æ¨¡å‹å­¦ä¼šä»å·²çŸ¥çš„å‡ ä½•è§‚æµ‹ä¸­é‡å»ºå®Œæ•´çš„å¯ç”¨æ€§åŒºåŸŸè’™ç‰ˆï¼Œä»è€Œå»ºç«‹ç¨³å¥çš„å‡ ä½•å…ˆéªŒã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³è¯­ä¹‰æ¨¡ç³Šæ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç‰¹å¾æ³¨å…¥æœºåˆ¶ï¼Œä»äºŒç»´è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆVFMï¼‰ä¸­æå–ä¸°å¯Œçš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶å°†å…¶èåˆåˆ°å¤šå°ºåº¦çš„ä¸‰ç»´è§£ç å™¨ä¸­ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šåˆ›ä¸‹äº†æ–°çš„æœ€ä½³çºªå½•ï¼Œæœ‰æ•ˆåœ°å°†å¯ç”¨æ€§æ¨ç†ä»å•æ­¥éª¤äº¤äº’æ¨è¿›åˆ°å¤æ‚çš„åœºæ™¯çº§åºè´¯ä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23772v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŸºäºä¸‰ç»´é«˜æ–¯æŠ•å½±æŠ€æœ¯çš„ä¸‰ç»´å¯è¾¾æ€§æ¨ç†é¢ä¸´å•ä¸€ç‰©ä½“å’Œå•ä¸€æ­¥éª¤äº¤äº’çš„é™åˆ¶ï¼Œæ— æ³•åº”å¯¹å¤æ‚ç°å®ä¸–ç•Œä¸­çš„é•¿æœŸå¤šç‰©ä½“ä»»åŠ¡ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥åºåˆ—ä¸‰ç»´é«˜æ–¯å¯è¾¾æ€§æ¨ç†ä»»åŠ¡ï¼Œå»ºç«‹SeqAffordSplatå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•åœºæ™¯ï¼Œæå‡ºSeqSplatNetæ¡†æ¶ï¼Œç›´æ¥æ˜ å°„æŒ‡ä»¤åˆ°ä¸‰ç»´å¯è¾¾æ€§æ©è†œåºåˆ—ã€‚å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒSeqSplatNetèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæ–‡æœ¬ä¸ç‰¹æ®Šåˆ†å‰²æ ‡è®°ï¼ŒæŒ‡å¯¼æ¡ä»¶è§£ç å™¨ç”Ÿæˆç›¸åº”çš„ä¸‰ç»´æ©è†œã€‚é€šè¿‡é¢„è®­ç»ƒç­–ç•¥æ„å»ºç¨³å¥å‡ ä½•å…ˆéªŒï¼Œè§£å†³å¤æ‚åœºæ™¯å‡ ä½•é—®é¢˜ï¼›è®¾è®¡ç‰¹å¾æ³¨å…¥æœºåˆ¶ï¼Œä»äºŒç»´è§†è§‰åŸºç¡€æ¨¡å‹ä¸­æå–ä¸°å¯Œè¯­ä¹‰ç‰¹å¾ï¼Œå°†å…¶èåˆåˆ°ä¸‰ç»´è§£ç å™¨ä¸­ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œå°†å¯è¾¾æ€§æ¨ç†ä»å•ä¸€æ­¥éª¤äº¤äº’æ¨å‘å¤æ‚çš„åœºæ™¯çº§åºåˆ—ä»»åŠ¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸‰ç»´é«˜æ–¯æŠ•å½±æŠ€æœ¯åœ¨å¤„ç†é•¿æœŸå¤šç‰©ä½“ä»»åŠ¡æ—¶çš„å±€é™æ€§ã€‚</li>
<li>æå‡ºåºåˆ—ä¸‰ç»´é«˜æ–¯å¯è¾¾æ€§æ¨ç†ä»»åŠ¡ä»¥åº”å¯¹å¤æ‚ç°å®ä¸–ç•Œä¸­çš„éœ€æ±‚ã€‚</li>
<li>å»ºç«‹äº†SeqAffordSplatå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•åœºæ™¯ï¼ŒåŒ…å«1800+åœºæ™¯æ”¯æŒå¯¹é•¿æœŸå¯è¾¾æ€§ç†è§£çš„ç ”ç©¶ã€‚</li>
<li>æå‡ºSeqSplatNetæ¡†æ¶ï¼Œèƒ½å¤Ÿç›´æ¥æ˜ å°„æŒ‡ä»¤åˆ°ä¸‰ç»´å¯è¾¾æ€§æ©è†œåºåˆ—ã€‚</li>
<li>é‡‡ç”¨é¢„è®­ç»ƒç­–ç•¥è§£å†³å¤æ‚åœºæ™¯å‡ ä½•é—®é¢˜ï¼Œå­¦ä¹ é‡å»ºå®Œæ•´çš„å¯è¾¾æ€§åŒºåŸŸæ©è†œã€‚</li>
<li>è®¾è®¡ç‰¹å¾æ³¨å…¥æœºåˆ¶ï¼Œä»äºŒç»´è§†è§‰åŸºç¡€æ¨¡å‹ä¸­æå–è¯­ä¹‰ç‰¹å¾å¹¶èåˆåˆ°ä¸‰ç»´è§£ç å™¨ä¸­ã€‚</li>
<li>å®éªŒè¡¨æ˜SeqSplatNetåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæœ‰æ•ˆæ¨è¿›åœºæ™¯çº§åˆ«çš„å¤æ‚åºåˆ—ä»»åŠ¡çš„å¯è¾¾æ€§æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23772">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-52fcd06f4e24d5923434944fdc122204.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42723816d289ea4eb2cdc248de4cff9c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2be20f2dc5a62b2156870f4d01e4ab69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06f335d5aec48beb87d4479869f0e46d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Rule2Text-Natural-Language-Explanation-of-Logical-Rules-in-Knowledge-Graphs"><a href="#Rule2Text-Natural-Language-Explanation-of-Logical-Rules-in-Knowledge-Graphs" class="headerlink" title="Rule2Text: Natural Language Explanation of Logical Rules in Knowledge   Graphs"></a>Rule2Text: Natural Language Explanation of Logical Rules in Knowledge   Graphs</h2><p><strong>Authors:Nasim Shirvani-Mahdavi, Devin Wingfield, Amin Ghasemi, Chengkai Li</strong></p>
<p>Knowledge graphs (KGs) often contain sufficient information to support the inference of new facts. Identifying logical rules not only improves the completeness of a knowledge graph but also enables the detection of potential errors, reveals subtle data patterns, and enhances the overall capacity for reasoning and interpretation. However, the complexity of such rules, combined with the unique labeling conventions of each KG, can make them difficult for humans to understand. In this paper, we explore the potential of large language models to generate natural language explanations for logical rules. Specifically, we extract logical rules using the AMIE 3.5.1 rule discovery algorithm from the benchmark dataset FB15k-237 and two large-scale datasets, FB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including zero- and few-shot prompting, including variable entity types, and chain-of-thought reasoning. We conduct a comprehensive human evaluation of the generated explanations based on correctness, clarity, and hallucination, and also assess the use of large language models as automatic judges. Our results demonstrate promising performance in terms of explanation correctness and clarity, although several challenges remain for future research. All scripts and data used in this study are publicly available at <a target="_blank" rel="noopener" href="https://github.com/idirlab/KGRule2NL%7D%7Bhttps://github.com/idirlab/KGRule2NL">https://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL</a>. </p>
<blockquote>
<p>çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰é€šå¸¸åŒ…å«æ”¯æŒæ–°äº‹å®æ¨æ–­çš„å……è¶³ä¿¡æ¯ã€‚è¯†åˆ«é€»è¾‘è§„åˆ™ä¸ä»…æé«˜äº†çŸ¥è¯†å›¾è°±çš„å®Œæ•´æ€§ï¼Œè¿˜èƒ½æ£€æµ‹æ½œåœ¨é”™è¯¯ï¼Œæ­ç¤ºå¾®å¦™çš„æ•°æ®æ¨¡å¼ï¼Œå¹¶å¢å¼ºäº†æ•´ä½“æ¨ç†å’Œè§£é‡Šèƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›è§„åˆ™çš„å¤æ‚æ€§ä»¥åŠæ¯ä¸ªçŸ¥è¯†å›¾è°±ç‹¬ç‰¹çš„æ ‡ç­¾è§„èŒƒä½¿å¾—äººç±»éš¾ä»¥ç†è§£ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºé€»è¾‘è§„åˆ™ç”Ÿæˆè‡ªç„¶è¯­è¨€è§£é‡Šçš„èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨AMIE 3.5.1è§„åˆ™å‘ç°ç®—æ³•ä»åŸºå‡†æ•°æ®é›†FB15k-237å’Œä¸¤ä¸ªå¤§è§„æ¨¡æ•°æ®é›†FB-CVT-REVå’ŒFB+CVT-REVä¸­æå–é€»è¾‘è§„åˆ™ã€‚æˆ‘ä»¬ç ”ç©¶äº†å„ç§æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºï¼ŒåŒ…æ‹¬å¯å˜å®ä½“ç±»å‹å’Œæ€ç»´é“¾æ¨ç†ã€‚æˆ‘ä»¬æ ¹æ®æ­£ç¡®æ€§ã€æ¸…æ™°åº¦å’Œå¹»è§‰å¯¹ç”Ÿæˆçš„è§£é‡Šè¿›è¡Œäº†å…¨é¢çš„äººå·¥è¯„ä¼°ï¼Œå¹¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè‡ªåŠ¨æ³•å®˜çš„ä½¿ç”¨æƒ…å†µã€‚æˆ‘ä»¬çš„ç»“æœè¡¨è¡¨æ˜ï¼Œåœ¨è§£é‡Šçš„æ­£ç¡®æ€§å’Œæ¸…æ™°åº¦æ–¹é¢è¡¨ç°æœ‰æ½œåŠ›ï¼Œå°½ç®¡æœªæ¥ç ”ç©¶ä»é¢ä¸´ä¸€äº›æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶ä¸­ä½¿ç”¨çš„æ‰€æœ‰è„šæœ¬å’Œæ•°æ®å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/idirlab/KGRule2NL%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/idirlab/KGRule2NLä¸Šå…¬å¼€è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23740v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šçŸ¥è¯†å›¾è°±ä¸­çš„é€»è¾‘è§„åˆ™å¯ä»¥æé«˜çŸ¥è¯†å›¾è°±çš„å®Œæ•´æ€§ï¼Œæ£€æµ‹æ½œåœ¨é”™è¯¯ï¼Œæ­ç¤ºæ•°æ®æ¨¡å¼å¹¶å¢å¼ºæ¨ç†å’Œè§£é‡Šèƒ½åŠ›ã€‚æœ¬æ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆé€»è¾‘è§„åˆ™è‡ªç„¶è¯­è¨€è§£é‡Šæ–¹é¢çš„æ½œåŠ›ã€‚é€šè¿‡ä½¿ç”¨AMIE 3.5.1è§„åˆ™å‘ç°ç®—æ³•ä»åŸºå‡†æ•°æ®é›†FB15k-237å’Œä¸¤ä¸ªå¤§è§„æ¨¡æ•°æ®é›†FB-CVT-REVå’ŒFB+CVT-REVä¸­æå–é€»è¾‘è§„åˆ™ï¼Œå¹¶æ¢è®¨äº†ä¸åŒçš„æç¤ºç­–ç•¥ã€‚å¯¹äººç±»è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œç”Ÿæˆçš„è§£é‡Šåœ¨æ­£ç¡®æ€§å’Œæ¸…æ™°åº¦æ–¹é¢è¡¨ç°å‡ºæœ‰å¸Œæœ›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>çŸ¥è¯†å›¾è°±ä¸­çš„é€»è¾‘è§„åˆ™å¯¹äºæé«˜çŸ¥è¯†å›¾è°±çš„å®Œæ•´æ€§ã€æ£€æµ‹æ½œåœ¨é”™è¯¯ã€æ­ç¤ºæ•°æ®æ¨¡å¼ä»¥åŠå¢å¼ºæ¨ç†å’Œè§£é‡Šèƒ½åŠ›è‡³å…³é‡è¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰ç”Ÿæˆé€»è¾‘è§„åˆ™è‡ªç„¶è¯­è¨€è§£é‡Šçš„æ½œåŠ›ã€‚</li>
<li>é€šè¿‡AMIE 3.5.1è§„åˆ™å‘ç°ç®—æ³•ä»åŸºå‡†æ•°æ®é›†FB15k-237ä»¥åŠä¸¤ä¸ªå¤§è§„æ¨¡æ•°æ®é›†FB-CVT-REVå’ŒFB+CVT-REVä¸­æå–é€»è¾‘è§„åˆ™ã€‚</li>
<li>æ¢è®¨äº†ä¸åŒçš„æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬é›¶æ¬¡å’Œå°‘æ¬¡æç¤ºã€åŒ…å«å¯å˜å®ä½“ç±»å‹å’Œé“¾å¼æ€ç»´æ¨ç†ã€‚</li>
<li>ç”Ÿæˆçš„è§£é‡Šç»è¿‡äººç±»è¯„ä¼°ï¼Œåœ¨æ­£ç¡®æ€§å’Œæ¸…æ™°åº¦æ–¹é¢è¡¨ç°å‡ºæœ‰å¸Œæœ›çš„æ€§èƒ½ã€‚</li>
<li>å°½ç®¡é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€è§£é‡Šæ–¹é¢ä»å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c65cf7a10aaaac71c41580b04b3a2ca6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f632b6fd2056565c2f2191dd8acb01c4.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="TextQuests-How-Good-are-LLMs-at-Text-Based-Video-Games"><a href="#TextQuests-How-Good-are-LLMs-at-Text-Based-Video-Games" class="headerlink" title="TextQuests: How Good are LLMs at Text-Based Video Games?"></a>TextQuests: How Good are LLMs at Text-Based Video Games?</h2><p><strong>Authors:Long Phan, Mantas Mazeika, Andy Zou, Dan Hendrycks</strong></p>
<p>Evaluating AI agents within complex, interactive environments that mirror real-world challenges is critical for understanding their practical capabilities. While existing agent benchmarks effectively assess skills like tool use or performance on structured tasks, they often do not fully capture an agentâ€™s ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context. To spur the development of agents capable of more robust intrinsic reasoning over long horizons, we introduce TextQuests, a benchmark based on the Infocom suite of interactive fiction games. These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on focused, stateful tasks. The benchmark is specifically designed to assess an LLM agentâ€™s capacity for self-contained problem-solving by precluding the use of external tools, thereby focusing on intrinsic long-context reasoning capabilities in an exploratory environment characterized by the need for trial-and-error learning and sustained problem-solving within a single interactive session. We release TextQuests at <a target="_blank" rel="noopener" href="https://textquests.ai/">https://textquests.ai</a>. </p>
<blockquote>
<p>è¯„ä¼°AIä»£ç†åœ¨å¤æ‚äº¤äº’ç¯å¢ƒä¸­åº”å¯¹ç°å®æŒ‘æˆ˜çš„èƒ½åŠ›ï¼Œå¯¹äºäº†è§£å…¶å®è·µèƒ½åŠ›è‡³å…³é‡è¦ã€‚å°½ç®¡ç°æœ‰çš„ä»£ç†åŸºå‡†æµ‹è¯•å¯ä»¥æœ‰æ•ˆåœ°è¯„ä¼°å·¥å…·ä½¿ç”¨æˆ–ç»“æ„åŒ–ä»»åŠ¡ä¸Šçš„è¡¨ç°ç­‰æŠ€èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸æ— æ³•å®Œå…¨æ•æ‰åˆ°ä»£ç†åœ¨æ¢ç´¢ç¯å¢ƒä¸­è‡ªä¸»è¡ŒåŠ¨çš„èƒ½åŠ›ï¼Œè¿™ç§ç¯å¢ƒè¦æ±‚åœ¨é•¿æœŸä¸”ä¸æ–­å¢é•¿çš„èƒŒæ™¯ä¸‹è¿›è¡ŒæŒç»­ã€è‡ªæˆ‘å¯¼å‘çš„æ¨ç†ã€‚ä¸ºäº†ä¿ƒè¿›èƒ½å¤Ÿåœ¨é•¿æœŸå†…å®ç°æ›´ç¨³å¥å†…åœ¨æ¨ç†èƒ½åŠ›çš„ä»£ç†çš„å‘å±•ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åŸºäºInfocomç³»åˆ—äº¤äº’å¼è™šæ„æ¸¸æˆåŸºå‡†æµ‹è¯•çš„TextQuestsã€‚è¿™äº›åŸºäºæ–‡æœ¬å†’é™©çš„æ¸¸æˆï¼Œå¯ä»¥è®©äººç±»ç©å®¶èŠ±è´¹è¶…è¿‡30å°æ—¶çš„æ—¶é—´ï¼Œéœ€è¦æ•°ç™¾ä¸ªç²¾ç¡®åŠ¨ä½œæ¥è§£å†³ï¼Œæ˜¯è¯„ä¼°äººå·¥æ™ºèƒ½ä»£ç†åœ¨å¤„ç†æœ‰çŠ¶æ€ä»»åŠ¡æ—¶çš„æœ‰æ•ˆå·¥å…·ã€‚è¯¥åŸºå‡†æµ‹è¯•æ˜¯ä¸“é—¨ä¸ºè¯„ä¼°LLMä»£ç†åœ¨ç¦æ­¢å¤–éƒ¨å·¥å…·çš„æƒ…å†µä¸‹è¿›è¡Œè‡ªæˆ‘é—®é¢˜è§£å†³çš„èƒ½åŠ›è€Œè®¾è®¡çš„ï¼Œä»è€Œä¸“æ³¨äºæ¢ç´¢ç¯å¢ƒä¸­å›ºæœ‰çš„é•¿æœŸä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ï¼Œè¯¥ç¯å¢ƒçš„ç‰¹ç‚¹æ˜¯éœ€è¦åœ¨å•æ¬¡äº¤äº’å¼ä¼šè¯ä¸­è¿›è¡Œè¯•é”™å­¦ä¹ å’ŒæŒç»­çš„é—®é¢˜è§£å†³ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://textquests.aiä¸Šå‘å¸ƒtextquests./">https://textquests.aiä¸Šå‘å¸ƒTextQuestsã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23701v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¼ºè°ƒäº†åœ¨å¤æ‚ã€äº¤äº’å¼ç¯å¢ƒä¸­è¯„ä¼°AIä»£ç†çš„é‡è¦æ€§ï¼Œè¿™äº›ç¯å¢ƒæ¨¡æ‹Ÿäº†ç°å®ä¸–ç•Œä¸­çš„æŒ‘æˆ˜ï¼Œæœ‰åŠ©äºäº†è§£AIçš„å®é™…èƒ½åŠ›ã€‚ç°æœ‰ä»£ç†åŸºå‡†æµ‹è¯•è™½ç„¶èƒ½æœ‰æ•ˆè¯„ä¼°å·¥å…·ä½¿ç”¨å’Œç»“æ„åŒ–ä»»åŠ¡ä¸Šçš„æŠ€èƒ½ï¼Œä½†æ— æ³•å…¨é¢æ•æ‰ä»£ç†åœ¨æ¢ç´¢ç¯å¢ƒä¸­è‡ªä¸»æ“ä½œçš„èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é•¿æœŸã€è‡ªæˆ‘å¯¼å‘æ¨ç†çš„æƒ…å¢ƒä¸‹ã€‚ä¸ºæ­¤ï¼Œå¼•å…¥äº†TextQuestsåŸºå‡†æµ‹è¯•ï¼Œè¯¥æµ‹è¯•åŸºäºInfocomçš„äº¤äº’å¼å°è¯´æ¸¸æˆï¼Œæ—¨åœ¨è¯„ä¼°AIä»£ç†åœ¨ç‰¹å®šã€æœ‰çŠ¶æ€ä»»åŠ¡ä¸Šçš„è‡ªæˆ‘è§£å†³é—®é¢˜çš„èƒ½åŠ›ï¼Œç‰¹åˆ«å¼ºè°ƒå†…åœ¨é•¿æœŸæ¨ç†èƒ½åŠ›ï¼Œå¹¶æ’é™¤äº†ä½¿ç”¨å¤–éƒ¨å·¥å…·çš„å¯èƒ½æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯„ä¼°AIä»£ç†åœ¨æ¨¡æ‹Ÿç°å®æŒ‘æˆ˜çš„å¤æ‚ã€äº¤äº’å¼ç¯å¢ƒä¸­çš„è¡¨ç°è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ä»£ç†åŸºå‡†æµ‹è¯•ä¸è¶³ä»¥å…¨é¢è¯„ä¼°ä»£ç†åœ¨æ¢ç´¢ç¯å¢ƒä¸­çš„è‡ªä¸»æ“ä½œèƒ½åŠ›ã€‚</li>
<li>TextQuestsåŸºå‡†æµ‹è¯•æ˜¯åŸºäºInfocomäº¤äº’å¼å°è¯´æ¸¸æˆçš„ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°è¯„ä¼°AIä»£ç†çš„èƒ½åŠ›ã€‚</li>
<li>TextQuestsç‰¹åˆ«è®¾è®¡æ¥è¯„ä¼°LLMä»£ç†çš„è‡ªæˆ‘è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æµ‹è¯•å¼ºè°ƒå†…åœ¨é•¿æœŸæ¨ç†èƒ½åŠ›ï¼Œè¦æ±‚ä»£ç†åœ¨å•æ¬¡äº’åŠ¨ä¼šè¯ä¸­è¿›è¡Œè¯•é”™å­¦ä¹ å’ŒæŒç»­è§£å†³é—®é¢˜ã€‚</li>
<li>TextQuestsåŸºå‡†æµ‹è¯•æ’é™¤äº†ä½¿ç”¨å¤–éƒ¨å·¥å…·çš„å¯èƒ½æ€§ï¼Œæ›´ä¸“æ³¨äºä»£ç†çš„å†…åœ¨èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23701">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d94a7f0d4240163335736a36f2bfb5bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce0b650df9a3eacf3b6093c3b73d3200.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b61293e3192ed611d7fa53eb42b19de6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7d63044aa512d7c8e339c6d4c8698988.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-59edce470e74f3ec282e5c59cd5d7ddb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-576566572e1cdb169117238b0365f2c4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="DepMicroDiff-Diffusion-Based-Dependency-Aware-Multimodal-Imputation-for-Microbiome-Data"><a href="#DepMicroDiff-Diffusion-Based-Dependency-Aware-Multimodal-Imputation-for-Microbiome-Data" class="headerlink" title="DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for   Microbiome Data"></a>DepMicroDiff: Diffusion-Based Dependency-Aware Multimodal Imputation for   Microbiome Data</h2><p><strong>Authors:Rabeya Tus Sadia, Qiang Cheng</strong></p>
<p>Microbiome data analysis is essential for understanding host health and disease, yet its inherent sparsity and noise pose major challenges for accurate imputation, hindering downstream tasks such as biomarker discovery. Existing imputation methods, including recent diffusion-based models, often fail to capture the complex interdependencies between microbial taxa and overlook contextual metadata that can inform imputation. We introduce DepMicroDiff, a novel framework that combines diffusion-based generative modeling with a Dependency-Aware Transformer (DAT) to explicitly capture both mutual pairwise dependencies and autoregressive relationships. DepMicroDiff is further enhanced by VAE-based pretraining across diverse cancer datasets and conditioning on patient metadata encoded via a large language model (LLM). Experiments on TCGA microbiome datasets show that DepMicroDiff substantially outperforms state-of-the-art baselines, achieving higher Pearson correlation (up to 0.712), cosine similarity (up to 0.812), and lower RMSE and MAE across multiple cancer types, demonstrating its robustness and generalizability for microbiome imputation. </p>
<blockquote>
<p>å¾®ç”Ÿç‰©ç»„æ•°æ®åˆ†æå¯¹äºç†è§£å®¿ä¸»å¥åº·å’Œç–¾ç—…è‡³å…³é‡è¦ï¼Œç„¶è€Œå…¶å†…åœ¨çš„ç¨€ç–æ€§å’Œå™ªå£°ç»™å‡†ç¡®çš„ä¼°ç®—å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œä»è€Œé˜»ç¢äº†ç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°ç­‰ä¸‹æ¸¸ä»»åŠ¡ã€‚ç°æœ‰çš„ä¼°ç®—æ–¹æ³•ï¼ŒåŒ…æ‹¬æœ€æ–°çš„åŸºäºæ‰©æ•£çš„æ¨¡å‹ï¼Œå¾€å¾€æ— æ³•æ•æ‰å¾®ç”Ÿç‰©åˆ†ç±»ç¾¤ä¹‹é—´çš„å¤æ‚ç›¸äº’ä¾èµ–æ€§ï¼Œå¹¶ä¸”å¿½ç•¥äº†å¯ä»¥æä¾›ä¼°ç®—ä¿¡æ¯çš„ä¸Šä¸‹æ–‡å…ƒæ•°æ®ã€‚æˆ‘ä»¬ä»‹ç»äº†DepMicroDiffï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆåŸºäºæ‰©æ•£çš„ç”Ÿæˆå»ºæ¨¡å’Œä¾èµ–æ„ŸçŸ¥è½¬æ¢å™¨ï¼ˆDATï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿæ˜ç¡®æ•æ‰ç›¸äº’çš„é…å¯¹ä¾èµ–å…³ç³»å’Œè‡ªå›å½’å…³ç³»ã€‚DepMicroDiffè¿˜é€šè¿‡åœ¨ä¸åŒç™Œç—‡æ•°æ®é›†ä¸Šè¿›è¡ŒåŸºäºVAEçš„é¢„è®­ç»ƒå’Œåœ¨é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç¼–ç çš„æ‚£è€…å…ƒæ•°æ®è¿›è¡Œæ¡ä»¶åŒ–ï¼Œä»è€Œå¾—åˆ°è¿›ä¸€æ­¥å¢å¼ºã€‚åœ¨TCGAå¾®ç”Ÿç‰©ç»„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDepMicroDiffæ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿æŠ€æœ¯ï¼Œåœ¨å¤šç§ç™Œç—‡ç±»å‹ä¸­å®ç°äº†æ›´é«˜çš„çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆé«˜è¾¾0.712ï¼‰ã€ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆé«˜è¾¾0.812ï¼‰ï¼Œä»¥åŠæ›´ä½çš„RMSEå’ŒMAEï¼Œè¯æ˜äº†å…¶åœ¨å¾®ç”Ÿç‰©ç»„ä¼°ç®—ä¸­çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23676v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¾®ç”Ÿç‰©ç»„æ•°æ®åˆ†æå¯¹äºç†è§£å®¿ä¸»å¥åº·å’Œç–¾ç—…è‡³å…³é‡è¦ï¼Œä½†å…¶å›ºæœ‰çš„ç¨€ç–æ€§å’Œå™ªå£°ç»™å‡†ç¡®ä¼°ç®—å¸¦æ¥é‡å¤§æŒ‘æˆ˜ï¼Œå½±å“ä¸‹æ¸¸ä»»åŠ¡å¦‚ç”Ÿç‰©æ ‡å¿—ç‰©å‘ç°ã€‚ç°æœ‰ä¼°ç®—æ–¹æ³•ï¼ŒåŒ…æ‹¬æœ€æ–°çš„åŸºäºæ‰©æ•£çš„æ¨¡å‹ï¼Œå¾€å¾€æ— æ³•æ•æ‰å¾®ç”Ÿç‰©åˆ†ç±»ç¾¤ä¹‹é—´çš„å¤æ‚ç›¸äº’ä¾èµ–æ€§ï¼Œå¹¶å¿½ç•¥äº†å¯ä»¥æä¾›ä¼°ç®—ä¿¡æ¯çš„ä¸Šä¸‹æ–‡å…ƒæ•°æ®ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹æ¡†æ¶DepMicroDiffï¼Œå®ƒç»“åˆäº†åŸºäºæ‰©æ•£çš„ç”Ÿæˆå»ºæ¨¡å’ŒåŸºäºä¾èµ–æ„ŸçŸ¥è½¬æ¢å™¨ï¼ˆDATï¼‰çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿæ˜ç¡®æ•æ‰ç›¸äº’ä¹‹é—´çš„é…å¯¹ä¾èµ–å…³ç³»å’Œè‡ªå›å½’å…³ç³»ã€‚DepMicroDiffè¿˜é€šè¿‡è·¨å¤šç§ç™Œç—‡æ•°æ®é›†è¿›è¡Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰çš„é¢„è®­ç»ƒï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹æ‚£è€…å…ƒæ•°æ®ç¼–ç ï¼Œä»¥å¢å¼ºå…¶åŠŸèƒ½ã€‚åœ¨TCGAå¾®ç”Ÿç‰©ç»„æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDepMicroDiffæ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿æ–¹æ³•ï¼Œå®ç°äº†æ›´é«˜çš„çš®å°”é€Šç›¸å…³ç³»æ•°ï¼ˆé«˜è¾¾0.712ï¼‰ã€ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆé«˜è¾¾0.812ï¼‰ï¼Œä»¥åŠæ›´ä½çš„RMSEå’ŒMAEå€¼ï¼Œè¯æ˜å…¶åœ¨å¾®ç”Ÿç‰©ä¼°ç®—ä¸­çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¾®ç”Ÿç‰©ç»„æ•°æ®åˆ†æå¯¹ç†è§£å®¿ä¸»å¥åº·å’Œç–¾ç—…è‡³å…³é‡è¦ï¼Œä½†æ•°æ®ç¨€ç–æ€§å’Œå™ªå£°å½±å“äº†å‡†ç¡®ä¼°ç®—ã€‚</li>
<li>ç°æœ‰ä¼°ç®—æ–¹æ³•éš¾ä»¥æ•æ‰å¾®ç”Ÿç‰©åˆ†ç±»ç¾¤é—´çš„å¤æ‚ç›¸äº’ä¾èµ–æ€§ã€‚</li>
<li>DepMicroDiffæ˜¯ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œç»“åˆäº†æ‰©æ•£ç”Ÿæˆå»ºæ¨¡å’Œä¾èµ–æ„ŸçŸ¥è½¬æ¢å™¨ï¼ˆDATï¼‰ã€‚</li>
<li>DepMicroDiffèƒ½å¤Ÿæ•æ‰å¾®ç”Ÿç‰©é—´çš„é…å¯¹ä¾èµ–å…³ç³»å’Œè‡ªå›å½’å…³ç³»ã€‚</li>
<li>DepMicroDiffé€šè¿‡VAEé¢„è®­ç»ƒå’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¢å¼ºåŠŸèƒ½ã€‚</li>
<li>å®éªŒè¯æ˜DepMicroDiffåœ¨å¾®ç”Ÿç‰©ä¼°ç®—ä¸Šæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23676">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-bf9788c0b9dcc2e4a5712a23803eb320.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-401c423dde214bfa1fd1fab7867d6136.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a6b42387fcf9e49db88ba76806f37f11.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e42dc4ae5d6c8f1f5e35c3d7b6243d74.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-991141254aaca51d9c8f6870bf8af75f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a69c0ea87cd7b1dff676472b74f42276.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Medical-Image-De-Identification-Benchmark-Challenge"><a href="#Medical-Image-De-Identification-Benchmark-Challenge" class="headerlink" title="Medical Image De-Identification Benchmark Challenge"></a>Medical Image De-Identification Benchmark Challenge</h2><p><strong>Authors:Linmin Pei, Granger Sutton, Michael Rutherford, Ulrike Wagner, Tracy Nolan, Kirk Smith, Phillip Farmer, Peter Gu, Ambar Rana, Kailing Chen, Thomas Ferleman, Brian Park, Ye Wu, Jordan Kojouharov, Gargi Singh, Jon Lemon, Tyler Willis, Milos Vukadinovic, Grant Duffy, Bryan He, David Ouyang, Marco Pereanez, Daniel Samber, Derek A. Smith, Christopher Cannistraci, Zahi Fayad, David S. Mendelson, Michele Bufano, Elmar Kotter, Hamideh Haghiri, Rajesh Baidya, Stefan Dvoretskii, Klaus H. Maier-Hein, Marco Nolden, Christopher Ablett, Silvia Siggillino, Sandeep Kaushik, Hongzhu Jiang, Sihan Xie, Zhiyu Wan, Alex Michie, Simon J Doran, Angeline Aurelia Waly, Felix A. Nathaniel Liang, Humam Arshad Mustagfirin, Michelle Grace Felicia, Kuo Po Chih, Rahul Krish, Ghulam Rasool, Nidhal Bouaynaya, Nikolas Koutsoubis, Kyle Naddeo, Kartik Pandit, Tony Oâ€™Sullivan, Raj Krish, Qinyan Pan, Scott Gustafson, Benjamin Kopchick, Laura Opsahl-Ong, Andrea Olvera-Morales, Jonathan Pinney, Kathryn Johnson, Theresa Do, Juergen Klenk, Maria Diaz, Arti Singh, Rong Chai, David A. Clunie, Fred Prior, Keyvan Farahani</strong></p>
<p>The de-identification (deID) of protected health information (PHI) and personally identifiable information (PII) is a fundamental requirement for sharing medical images, particularly through public repositories, to ensure compliance with patient privacy laws. In addition, preservation of non-PHI metadata to inform and enable downstream development of imaging artificial intelligence (AI) is an important consideration in biomedical research. The goal of MIDI-B was to provide a standardized platform for benchmarking of DICOM image deID tools based on a set of rules conformant to the HIPAA Safe Harbor regulation, the DICOM Attribute Confidentiality Profiles, and best practices in preservation of research-critical metadata, as defined by The Cancer Imaging Archive (TCIA). The challenge employed a large, diverse, multi-center, and multi-modality set of real de-identified radiology images with synthetic PHI&#x2F;PII inserted.   The MIDI-B Challenge consisted of three phases: training, validation, and test. Eighty individuals registered for the challenge. In the training phase, we encouraged participants to tune their algorithms using their in-house or public data. The validation and test phases utilized the DICOM images containing synthetic identifiers (of 216 and 322 subjects, respectively). Ten teams successfully completed the test phase of the challenge. To measure success of a rule-based approach to image deID, scores were computed as the percentage of correct actions from the total number of required actions. The scores ranged from 97.91% to 99.93%. Participants employed a variety of open-source and proprietary tools with customized configurations, large language models, and optical character recognition (OCR). In this paper we provide a comprehensive report on the MIDI-B Challengeâ€™s design, implementation, results, and lessons learned. </p>
<blockquote>
<p>å—ä¿æŠ¤çš„å¥åº·ä¿¡æ¯ï¼ˆPHIï¼‰å’Œä¸ªäººå¯è¯†åˆ«ä¿¡æ¯ï¼ˆPIIï¼‰çš„å»æ ‡è¯†åŒ–æ˜¯å…±äº«åŒ»ç–—å›¾åƒçš„åŸºæœ¬è¦æ±‚ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å…¬å…±å­˜å‚¨åº“å…±äº«æ—¶ï¼Œä»¥ç¡®ä¿ç¬¦åˆæ‚£è€…éšç§æ³•è§„ã€‚æ­¤å¤–ï¼Œä¿ç•™éPHIå…ƒæ•°æ®ä»¥å‘ŠçŸ¥å¹¶ä¿ƒè¿›æˆåƒäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„ä¸‹æ¸¸å‘å±•æ˜¯ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­çš„é‡è¦è€ƒè™‘å› ç´ ã€‚MIDI-Bçš„ç›®æ ‡æ˜¯æä¾›ä¸€ä¸ªæ ‡å‡†åŒ–çš„å¹³å°ï¼Œä»¥åŸºäºä¸€ç»„ç¬¦åˆHIPAAå®‰å…¨æ¸¯æ³•è§„ã€DICOMå±æ€§ä¿å¯†é…ç½®æ–‡ä»¶ä»¥åŠç™Œç—‡æˆåƒå­˜æ¡£ï¼ˆTCIAï¼‰å®šä¹‰çš„ä¿ç•™ç ”ç©¶å…³é”®å…ƒæ•°æ®çš„æœ€ä½³å®è·µæ¥è¯„ä¼°DICOMå›¾åƒå»æ ‡è¯†åŒ–å·¥å…·ã€‚è¯¥æŒ‘æˆ˜é‡‡ç”¨äº†å¤§é‡å¤šæ ·åŒ–ã€å¤šä¸­å¿ƒã€å¤šæ¨¡å¼çš„çœŸå®å»æ ‡è¯†åŒ–æ”¾å°„å­¦å›¾åƒï¼Œå¹¶æ’å…¥äº†åˆæˆPHI&#x2F;PIIã€‚MIDI-BæŒ‘æˆ˜åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šåŸ¹è®­ã€éªŒè¯å’Œæµ‹è¯•ã€‚æœ‰80äººæŠ¥åå‚åŠ æŒ‘æˆ˜ã€‚åœ¨åŸ¹è®­é˜¶æ®µï¼Œæˆ‘ä»¬é¼“åŠ±å‚ä¸è€…ä½¿ç”¨å†…éƒ¨æˆ–å…¬å¼€æ•°æ®è°ƒæ•´å…¶ç®—æ³•ã€‚éªŒè¯å’Œæµ‹è¯•é˜¶æ®µä½¿ç”¨äº†åŒ…å«åˆæˆæ ‡è¯†ç¬¦çš„DICOMå›¾åƒï¼ˆåˆ†åˆ«ä¸º216åå’Œ322åå—è¯•è€…ï¼‰ã€‚æœ‰åæ”¯é˜Ÿä¼æˆåŠŸå®Œæˆäº†æµ‹è¯•é˜¶æ®µçš„æŒ‘æˆ˜ã€‚ä¸ºäº†è¡¡é‡åŸºäºè§„åˆ™çš„å»å›¾åƒæ ‡è¯†æ–¹æ³•æˆåŠŸä¸å¦ï¼Œåˆ†æ•°æ˜¯æ ¹æ®å®Œæˆæ‰€æœ‰å¿…éœ€åŠ¨ä½œçš„æ­£ç¡®è¡ŒåŠ¨ç™¾åˆ†æ¯”æ¥è®¡ç®—çš„ã€‚åˆ†æ•°èŒƒå›´ä»97.91%åˆ°99.93%ã€‚å‚ä¸è€…ä½¿ç”¨äº†å„ç§å¼€æºå’Œä¸“æœ‰å·¥å…·ï¼Œå…·æœ‰è‡ªå®šä¹‰é…ç½®ã€å¤§å‹è¯­è¨€æ¨¡å‹å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†å…³äºMIDI-BæŒ‘æˆ˜çš„è®¾è®¡ã€å®æ–½ã€ç»“æœå’Œç»éªŒæ•™è®­çš„å…¨é¢æŠ¥å‘Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23608v1">PDF</a> 19 pages</p>
<p><strong>Summary</strong><br>    MIDI-BæŒ‘æˆ˜æ—¨åœ¨æä¾›ä¸€ä¸ªæ ‡å‡†åŒ–å¹³å°ï¼Œç”¨äºåŸºäºDICOMå›¾åƒå»æ ‡è¯†ï¼ˆdeIDï¼‰å·¥å…·çš„æ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚æŒ‘æˆ˜çš„ç›®çš„æ˜¯ç¡®ä¿åŒ»å­¦å›¾åƒå…±äº«æ—¶éµå®ˆæ‚£è€…éšç§æ³•è§„ï¼ŒåŒæ—¶ä¿ç•™éPHIå…ƒæ•°æ®ä»¥æ”¯æŒä¸‹æ¸¸çš„æˆåƒäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å‘å±•ã€‚è¯¥æŒ‘æˆ˜åŒ…æ‹¬åŸ¹è®­ã€éªŒè¯å’Œæµ‹è¯•ä¸‰ä¸ªé˜¶æ®µï¼Œæ¶‰åŠå¤§é‡çœŸå®å»æ ‡è¯†çš„æ”¾å°„å›¾åƒï¼Œå¹¶æˆåŠŸå®Œæˆæµ‹è¯•çš„å›¢é˜Ÿæœ‰åæ”¯ã€‚è§„åˆ™æ–¹æ³•çš„æˆåŠŸåº¦é‡æ˜¯åŸºäºæ­£ç¡®è¡ŒåŠ¨å æ€»è¡ŒåŠ¨ç™¾åˆ†æ¯”çš„è¯„åˆ†ï¼Œè¯„åˆ†èŒƒå›´åœ¨97.91%è‡³99.93%ä¹‹é—´ã€‚è¯¥è®ºæ–‡è¯¦ç»†æŠ¥å‘Šäº†MIDI-BæŒ‘æˆ˜çš„è®¾è®¡ã€å®æ–½ã€ç»“æœå’Œç»éªŒæ•™è®­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MIDI-BæŒ‘æˆ˜æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–å¹³å°ï¼Œç”¨äºè¯„ä¼°DICOMå›¾åƒå»æ ‡è¯†ï¼ˆdeIDï¼‰å·¥å…·çš„æ€§èƒ½ã€‚</li>
<li>æŒ‘æˆ˜çš„ç›®çš„æ˜¯ç¡®ä¿åŒ»å­¦å›¾åƒå…±äº«æ—¶éµå®ˆæ‚£è€…éšç§æ³•è§„ï¼ŒåŒæ—¶ä¿ç•™éPHIå…ƒæ•°æ®æ”¯æŒAIç ”ç©¶ã€‚</li>
<li>æŒ‘æˆ˜æ¶‰åŠå¤§é‡å¤šä¸­å¿ƒã€å¤šæ¨¡æ€çš„çœŸå®å»æ ‡è¯†æ”¾å°„å›¾åƒå’Œæ’å…¥çš„åˆæˆPHI&#x2F;PIIã€‚</li>
<li>æŒ‘æˆ˜åŒ…æ‹¬åŸ¹è®­ã€éªŒè¯å’Œæµ‹è¯•ä¸‰ä¸ªé˜¶æ®µï¼Œå…±æœ‰80äººæ³¨å†Œå‚ä¸ã€‚</li>
<li>æœ‰åæ”¯å›¢é˜ŸæˆåŠŸå®Œæˆäº†æµ‹è¯•é˜¶æ®µçš„æŒ‘æˆ˜ã€‚</li>
<li>æˆåŠŸåº¦é‡æ˜¯åŸºäºæ­£ç¡®è¡ŒåŠ¨å æ€»è¡ŒåŠ¨ç™¾åˆ†æ¯”çš„è¯„åˆ†ï¼Œæ˜¾ç¤ºè§„åˆ™æ–¹æ³•çš„é«˜åº¦å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4bf310fb997be0ccf56e02e30b4479a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b7734d045de6e7d4bbfb284daf5f28fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad97f24be5990f9a705085d839e0a108.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c3ff2088ec668db055d5fdc4940915f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c79e949f0993f019b10d5074c1b91030.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Can-LLM-Reasoning-Models-Replace-Classical-Planning-A-Benchmark-Study"><a href="#Can-LLM-Reasoning-Models-Replace-Classical-Planning-A-Benchmark-Study" class="headerlink" title="Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study"></a>Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study</h2><p><strong>Authors:Kai Goebel, Patrik Zips</strong></p>
<p>Recent advancements in Large Language Models have sparked interest in their potential for robotic task planning. While these models demonstrate strong generative capabilities, their effectiveness in producing structured and executable plans remains uncertain. This paper presents a systematic evaluation of a broad spectrum of current state of the art language models, each directly prompted using Planning Domain Definition Language domain and problem files, and compares their planning performance with the Fast Downward planner across a variety of benchmarks. In addition to measuring success rates, we assess how faithfully the generated plans translate into sequences of actions that can actually be executed, identifying both strengths and limitations of using these models in this setting. Our findings show that while the models perform well on simpler planning tasks, they continue to struggle with more complex scenarios that require precise resource management, consistent state tracking, and strict constraint compliance. These results underscore fundamental challenges in applying language models to robotic planning in real world environments. By outlining the gaps that emerge during execution, we aim to guide future research toward combined approaches that integrate language models with classical planners in order to enhance the reliability and scalability of planning in autonomous robotics. </p>
<blockquote>
<p>æœ€è¿‘å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥æ¿€å‘äº†äººä»¬å¯¹æœºå™¨ä»»åŠ¡è§„åˆ’æ½œåœ¨å¯èƒ½æ€§çš„å…´è¶£ã€‚è™½ç„¶è¿™äº›æ¨¡å‹è¡¨ç°å‡ºäº†å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä½†å®ƒä»¬å¯¹äºç»“æ„åŒ–æ‰§è¡Œè§„åˆ’çš„æ•ˆç”¨ä»ç„¶å­˜åœ¨ä¸ç¡®å®šæ€§ã€‚æœ¬æ–‡ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸€ç³»åˆ—å½“å‰å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨è§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€å’Œé—®é¢˜æ–‡ä»¶ç›´æ¥æç¤ºæ¯ä¸ªæ¨¡å‹ï¼Œå¹¶åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å°†å®ƒä»¬çš„è§„åˆ’æ€§èƒ½ä¸å¿«é€Ÿå‘ä¸‹è§„åˆ’å™¨è¿›è¡Œæ¯”è¾ƒã€‚é™¤äº†è¡¡é‡æˆåŠŸç‡ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¯„ä¼°äº†ç”Ÿæˆçš„è®¡åˆ’è½¬åŒ–ä¸ºå¯å®é™…æ‰§è¡Œçš„è¡ŒåŠ¨åºåˆ—çš„ç¨‹åº¦ï¼Œä»¥è¯†åˆ«åœ¨è¿™ç§ç¯å¢ƒä¸­ä½¿ç”¨è¿™äº›æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶è¿™äº›æ¨¡å‹åœ¨ç®€å•çš„è§„åˆ’ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦ç²¾ç¡®èµ„æºç®¡ç†ã€æŒç»­çŠ¶æ€è·Ÿè¸ªå’Œä¸¥æ ¼çº¦æŸéµå®ˆçš„å¤æ‚åœºæ™¯ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›ç»“æœçªæ˜¾äº†åœ¨ç°å®ç¯å¢ƒå°†è¯­è¨€æ¨¡å‹åº”ç”¨äºæœºå™¨äººè§„åˆ’æ—¶æ‰€é¢ä¸´çš„åŸºæœ¬æŒ‘æˆ˜ã€‚é€šè¿‡é˜è¿°æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°çš„å·®è·ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜æ–¹å‘ï¼Œé‡‡ç”¨å°†è¯­è¨€æ¨¡å‹ä¸ç»å…¸è§„åˆ’å™¨ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œä»¥æé«˜è‡ªä¸»æœºå™¨äººè§„åˆ’çš„å¯é æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23589v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººä»»åŠ¡è§„åˆ’æ–¹é¢çš„æ½œåŠ›å¼•å‘äº†å¹¿æ³›å…³æ³¨ã€‚æœ¬æ–‡å¯¹ä¸€ç³»åˆ—å…ˆè¿›è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œä½¿ç”¨è§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€åŸŸå’Œé—®é¢˜æ–‡ä»¶ç›´æ¥æç¤ºï¼Œä¸å¿«é€Ÿå‘ä¸‹è§„åˆ’å™¨åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œè§„åˆ’æ€§èƒ½æ¯”è¾ƒã€‚é™¤äº†æˆåŠŸç‡å¤–ï¼Œè¿˜è¯„ä¼°äº†ç”Ÿæˆè®¡åˆ’è½¬åŒ–ä¸ºå¯æ‰§è¡ŒåŠ¨ä½œåºåˆ—çš„å¿ å®åº¦ï¼Œæ¢è®¨äº†åœ¨è¿™äº›åœºæ™¯ä¸‹ä½¿ç”¨è¿™äº›æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ã€‚ç ”ç©¶å‘ç°åœ¨ç®€å•è§„åˆ’ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦ç²¾ç¡®èµ„æºç®¡ç†ã€æŒç»­çŠ¶æ€è·Ÿè¸ªå’Œä¸¥æ ¼çº¦æŸéµå®ˆçš„å¤æ‚åœºæ™¯ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æŒ‡å¯¼æœªæ¥ç ”ç©¶ç»“åˆè¯­è¨€æ¨¡å‹å’Œç»å…¸è§„åˆ’å™¨ï¼Œä»¥æé«˜è‡ªä¸»æœºå™¨äººè§„åˆ’çš„å¯é æ€§å’Œå¯æ‰©å±•æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æœºå™¨äººä»»åŠ¡è§„åˆ’æ–¹é¢çš„æ½œåŠ›å¼•å‘å…³æ³¨ã€‚</li>
<li>ç³»ç»Ÿè¯„ä¼°äº†ä¸€ç³»åˆ—å…ˆè¿›è¯­è¨€æ¨¡å‹åœ¨è§„åˆ’æ€§èƒ½æ–¹é¢çš„è¡¨ç°ã€‚</li>
<li>ä½¿ç”¨è§„åˆ’é¢†åŸŸå®šä¹‰è¯­è¨€è¿›è¡Œç›´æ¥æç¤ºï¼Œå¹¶ä¸å¿«é€Ÿå‘ä¸‹è§„åˆ’å™¨è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>åœ¨ç®€å•è§„åˆ’ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¤æ‚åœºæ™¯ä¸­ä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç”Ÿæˆè®¡åˆ’çš„å¿ å®åº¦è½¬åŒ–ä¸ºå¯æ‰§è¡ŒåŠ¨ä½œåºåˆ—æ˜¯è¯„ä¼°é‡ç‚¹ã€‚</li>
<li>ç ”ç©¶æŒ‡å‡ºäº†åœ¨æœºå™¨äººä»»åŠ¡è§„åˆ’ä¸­è¯­è¨€æ¨¡å‹çš„ä¼˜ç¼ºç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23589">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-47ae5668cd28c1af167316672c181c2d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3c217ad9c3e0bbff20ff73b4ccb61665.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8bbb70b730babf0f713a554e61b67361.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-65d481cfff8f047ea059e79ec635aeeb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-064ab5da026c0e44c15c782fcbdfeaf1.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Beyond-Gloss-A-Hand-Centric-Framework-for-Gloss-Free-Sign-Language-Translation"><a href="#Beyond-Gloss-A-Hand-Centric-Framework-for-Gloss-Free-Sign-Language-Translation" class="headerlink" title="Beyond Gloss: A Hand-Centric Framework for Gloss-Free Sign Language   Translation"></a>Beyond Gloss: A Hand-Centric Framework for Gloss-Free Sign Language   Translation</h2><p><strong>Authors:Sobhan Asasi, Mohamed Ilyas Lakhal, Ozge Mercanoglu Sincan, Richard Bowden</strong></p>
<p>Sign Language Translation (SLT) is a challenging task that requires bridging the modality gap between visual and linguistic information while capturing subtle variations in hand shapes and movements. To address these challenges, we introduce \textbf{BeyondGloss}, a novel gloss-free SLT framework that leverages the spatio-temporal reasoning capabilities of Video Large Language Models (VideoLLMs). Since existing VideoLLMs struggle to model long videos in detail, we propose a novel approach to generate fine-grained, temporally-aware textual descriptions of hand motion. A contrastive alignment module aligns these descriptions with video features during pre-training, encouraging the model to focus on hand-centric temporal dynamics and distinguish signs more effectively. To further enrich hand-specific representations, we distill fine-grained features from HaMeR. Additionally, we apply a contrastive loss between sign video representations and target language embeddings to reduce the modality gap in pre-training. \textbf{BeyondGloss} achieves state-of-the-art performance on the Phoenix14T and CSL-Daily benchmarks, demonstrating the effectiveness of the proposed framework. We will release the code upon acceptance of the paper. </p>
<blockquote>
<p>æ‰‹åŠ¿è¯­è¨€ç¿»è¯‘ï¼ˆSLTï¼‰æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦å¼¥åˆè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ä¹‹é—´çš„æ¨¡å¼å·®è·ï¼ŒåŒæ—¶æ•æ‰æ‰‹éƒ¨å½¢çŠ¶å’ŒåŠ¨ä½œçš„ç»†å¾®å˜åŒ–ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BeyondGlossï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„æ— å­—å¹•SLTæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideoLLMï¼‰çš„æ—¶ç©ºæ¨ç†èƒ½åŠ›ã€‚ç”±äºç°æœ‰çš„VideoLLMåœ¨è¯¦ç»†å»ºæ¨¡é•¿è§†é¢‘æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œç”Ÿæˆæ‰‹éƒ¨è¿åŠ¨çš„ç²¾ç»†æ—¶é—´æ„ŸçŸ¥æ–‡æœ¬æè¿°ã€‚å¯¹æ¯”å¯¹é½æ¨¡å—åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å°†è¿™äº›æè¿°ä¸è§†é¢‘ç‰¹å¾å¯¹é½ï¼Œé¼“åŠ±æ¨¡å‹å…³æ³¨ä»¥æ‰‹ä¸ºä¸­å¿ƒçš„ä¸´æ—¶åŠ¨æ€ï¼Œæ›´æœ‰æ•ˆåœ°åŒºåˆ†æ‰‹åŠ¿ã€‚ä¸ºäº†ä¸°å¯Œæ‰‹éƒ¨ç‰¹å®šè¡¨ç¤ºï¼Œæˆ‘ä»¬ä»HaMeRä¸­æç‚¼å‡ºç²¾ç»†ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåœ¨æ‰‹åŠ¿è§†é¢‘è¡¨ç¤ºå’Œç›®æ ‡è¯­è¨€åµŒå…¥ä¹‹é—´åº”ç”¨å¯¹æ¯”æŸå¤±ï¼Œä»¥å‡å°‘æ¨¡å¼å·®è·ã€‚BeyondGlossåœ¨Phoenix14Tå’ŒCSL-DailyåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚è®ºæ–‡è¢«æ¥å—åï¼Œæˆ‘ä»¬å°†å…¬å¼€ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23575v1">PDF</a> Accepted at BMVC 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€é¡¹æ–°çš„ç ”ç©¶ï¼Œé’ˆå¯¹æ‰‹è¯­ç¿»è¯‘ä¸­çš„è§†è§‰ä¸è¯­è¨€ä¿¡æ¯äº¤äº’éš¾é¢˜ï¼Œæå‡ºäº†BeyondGlossè¿™ä¸€å…¨æ–°æ— è¯æ³¨æ‰‹è¯­ç¿»è¯‘æ¡†æ¶ã€‚å®ƒé€šè¿‡è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ—¶é—´æ¨ç†èƒ½åŠ›ï¼Œè§£å†³ç°æœ‰æ¨¡å‹å¯¹é•¿è§†é¢‘ç»†èŠ‚å»ºæ¨¡èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œç”Ÿæˆç²¾ç»†çš„æ—¶é—´æ„ŸçŸ¥æ‰‹éƒ¨è¿åŠ¨æ–‡æœ¬æè¿°ã€‚é‡‡ç”¨å¯¹æ¯”å¯¹é½æ¨¡å—åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å°†è¿™äº›æè¿°ä¸è§†é¢‘ç‰¹å¾å¯¹é½ï¼Œä»¥æé«˜æ¨¡å‹å¯¹æ‰‹éƒ¨ä¸ºä¸­å¿ƒçš„æ—¶é—´åŠ¨æ€çš„å…³æ³¨åº¦å’Œè¾¨è¯†æ•ˆæœã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†HamReç²¾ç»†ç‰¹å¾è’¸é¦æŠ€æœ¯ï¼Œå¹¶åº”ç”¨å¯¹æ¯”æŸå¤±æ¥ç¼©å°é¢„è®­ç»ƒä¸­çš„æ¨¡æ€å·®è·ã€‚BeyondGlossåœ¨Phoenix14Tå’ŒCSL-DailyåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸šç•Œé¢†å…ˆçš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BeyondGlossæ˜¯ä¸€ä¸ªå…¨æ–°çš„æ— è¯æ³¨æ‰‹è¯­ç¿»è¯‘æ¡†æ¶ï¼Œç”¨äºè§£å†³æ‰‹è¯­ç¿»è¯‘ä¸­çš„è§†è§‰ä¸è¯­è¨€ä¿¡æ¯äº¤äº’éš¾é¢˜ã€‚</li>
<li>åˆ©ç”¨è§†é¢‘å¤§è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ—¶é—´æ¨ç†èƒ½åŠ›å¤„ç†æ‰‹è¯­ç¿»è¯‘ä»»åŠ¡ã€‚</li>
<li>æå‡ºäº†ç”Ÿæˆç²¾ç»†æ—¶é—´æ„ŸçŸ¥æ‰‹éƒ¨è¿åŠ¨æ–‡æœ¬æè¿°çš„æ–¹æ³•ï¼Œä»¥è§£å†³ç°æœ‰æ¨¡å‹å¯¹é•¿è§†é¢‘ç»†èŠ‚å»ºæ¨¡èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨å¯¹æ¯”å¯¹é½æ¨¡å—åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹é½æ–‡æœ¬æè¿°ä¸è§†é¢‘ç‰¹å¾ï¼Œæé«˜æ¨¡å‹å¯¹æ‰‹éƒ¨ä¸ºä¸­å¿ƒçš„æ—¶é—´åŠ¨æ€çš„å…³æ³¨åº¦ã€‚</li>
<li>å¼•å…¥HamReç²¾ç»†ç‰¹å¾è’¸é¦æŠ€æœ¯æ¥ä¸°å¯Œæ‰‹éƒ¨ç‰¹å®šçš„è¡¨ç°ã€‚</li>
<li>åº”ç”¨å¯¹æ¯”æŸå¤±æ¥ç¼©å°æ‰‹è¯­è§†é¢‘çš„è¡¨ç¤ºä¸ç›®æ ‡è¯­è¨€åµŒå…¥ä¹‹é—´çš„æ¨¡æ€å·®è·ã€‚</li>
<li>BeyondGlossåœ¨Phoenix14Tå’ŒCSL-DailyåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†ä¸šç•Œé¢†å…ˆçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23575">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e8abce9c69e8690d2561da3dc04b9ff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-882d9ec6db3455c72c6031cbd589d054.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-38985d5fb8556091d7c5a0ca62922b47.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="ART-Adaptive-Relation-Tuning-for-Generalized-Relation-Prediction"><a href="#ART-Adaptive-Relation-Tuning-for-Generalized-Relation-Prediction" class="headerlink" title="ART: Adaptive Relation Tuning for Generalized Relation Prediction"></a>ART: Adaptive Relation Tuning for Generalized Relation Prediction</h2><p><strong>Authors:Gopika Sudhakaran, Hikaru Shindo, Patrick Schramowski, Simone Schaub-Meyer, Kristian Kersting, Stefan Roth</strong></p>
<p>Visual relation detection (VRD) is the task of identifying the relationships between objects in a scene. VRD models trained solely on relation detection data struggle to generalize beyond the relations on which they are trained. While prompt tuning has been used to adapt vision-language models (VLMs) for VRD, it uses handcrafted prompts and struggles with novel or complex relations. We argue that instruction tuning offers a more effective solution by fine-tuning VLMs on diverse instructional data. We thus introduce ART, an Adaptive Relation Tuning framework that adapts VLMs for VRD through instruction tuning and strategic instance selection. By converting VRD datasets into an instruction tuning format and employing an adaptive sampling algorithm, ART directs the VLM to focus on informative relations while maintaining generalizability. Specifically, we focus on the relation classification, where subject-object boxes are given and the model predicts the predicate between them. We tune on a held-in set and evaluate across multiple held-out datasets of varying complexity. Our approach strongly improves over its baselines and can infer unseen relation concepts, a capability absent in mainstream VRD methods. We demonstrate ARTâ€™s practical value by using the predicted relations for segmenting complex scenes. </p>
<blockquote>
<p>è§†è§‰å…³ç³»æ£€æµ‹ï¼ˆVRDï¼‰æ˜¯è¯†åˆ«åœºæ™¯ä¸­ç‰©ä½“ä¹‹é—´å…³ç³»çš„ä»»åŠ¡ã€‚ä»…é€šè¿‡å…³ç³»æ£€æµ‹æ•°æ®è¿›è¡Œè®­ç»ƒçš„VRDæ¨¡å‹å¾ˆéš¾æ¨å¹¿åˆ°å…¶æœªè®­ç»ƒè¿‡çš„å…³ç³»ä¹‹å¤–ã€‚è™½ç„¶æç¤ºè°ƒæ•´å·²è¢«ç”¨äºé€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡ŒVRDï¼Œä½†å®ƒä½¿ç”¨æ‰‹å·¥åˆ¶ä½œçš„æç¤ºï¼Œå¹¶åœ¨é¢å¯¹æ–°é¢–æˆ–å¤æ‚å…³ç³»æ—¶é¢ä¸´å›°éš¾ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒæŒ‡ä»¤è°ƒæ•´é€šè¿‡åœ¨å¯¹å¤šæ ·åŒ–çš„æŒ‡ä»¤æ•°æ®è¿›è¡Œå¾®è°ƒæ—¶ï¼Œä¸ºVLMæä¾›æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ARTï¼Œè¿™æ˜¯ä¸€ç§è‡ªé€‚åº”å…³ç³»è°ƒæ•´æ¡†æ¶ï¼Œå®ƒé€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œç­–ç•¥æ€§å®ä¾‹é€‰æ‹©æ¥é€‚åº”VLMè¿›è¡ŒVRDã€‚é€šè¿‡å°†VRDæ•°æ®é›†è½¬æ¢ä¸ºæŒ‡ä»¤è°ƒæ•´æ ¼å¼å¹¶é‡‡ç”¨è‡ªé€‚åº”é‡‡æ ·ç®—æ³•ï¼ŒARTæŒ‡å¯¼VLMä¸“æ³¨äºä¿¡æ¯ä¸°å¯Œçš„å…³ç³»ï¼ŒåŒæ—¶ä¿æŒå…¶æ³›åŒ–èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å…³æ³¨å…³ç³»åˆ†ç±»ï¼Œå…¶ä¸­ç»™å®šä¸»ä½“-å¯¹è±¡æ¡†ï¼Œæ¨¡å‹ä¼šé¢„æµ‹å®ƒä»¬ä¹‹é—´çš„è°“è¯­ã€‚æˆ‘ä»¬åœ¨ä¿ç•™é›†ä¸Šè¿›è¡Œè°ƒæ•´ï¼Œå¹¶åœ¨å¤šä¸ªå¤æ‚çš„ä¿ç•™æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¯”å…¶åŸºçº¿æ–¹æ³•æœ‰æ˜¾è‘—æ”¹å–„ï¼Œå¹¶ä¸”å¯ä»¥æ¨æ–­å‡ºæœªè§çš„å…³ç³»æ¦‚å¿µï¼Œè¿™æ˜¯ä¸»æµVRDæ–¹æ³•æ‰€ä¸å…·å¤‡çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨é¢„æµ‹çš„å…³ç³»æ¥åˆ†å‰²å¤æ‚åœºæ™¯ï¼Œå±•ç¤ºäº†ARTçš„å®é™…ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23543v1">PDF</a> Accepted for publication in ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>è§†è§‰å…³ç³»æ£€æµ‹ï¼ˆVRDï¼‰çš„ä»»åŠ¡æ˜¯è¯†åˆ«åœºæ™¯ä¸­ç‰©ä½“ä¹‹é—´çš„å…³ç³»ã€‚ä»…é€šè¿‡å…³ç³»æ£€æµ‹æ•°æ®è®­ç»ƒçš„VRDæ¨¡å‹éš¾ä»¥æ³›åŒ–åˆ°æœªç»è¿‡è®­ç»ƒçš„å…³ç³»ä¸Šã€‚è™½ç„¶æç¤ºè°ƒæ•´å·²è¢«ç”¨äºé€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡ŒVRDï¼Œä½†å®ƒä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„æç¤ºï¼Œå¯¹äºæ–°é¢–æˆ–å¤æ‚çš„å…³ç³»å¤„ç†èµ·æ¥è¾ƒä¸ºå›°éš¾ã€‚æœ¬æ–‡æå‡ºé€šè¿‡æŒ‡ä»¤è°ƒæ•´æ¥æ›´æœ‰æ•ˆåœ°è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå³é€šè¿‡å¤šæ ·åŒ–æŒ‡ä»¤æ•°æ®å¯¹VLMè¿›è¡Œå¾®è°ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ARTï¼ˆè‡ªé€‚åº”å…³ç³»è°ƒæ•´æ¡†æ¶ï¼‰ï¼Œå®ƒé€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œç­–ç•¥æ€§å®ä¾‹é€‰æ‹©æ¥é€‚åº”VLMè¿›è¡ŒVRDã€‚é€šè¿‡å°†VRDæ•°æ®é›†è½¬æ¢ä¸ºæŒ‡ä»¤è°ƒæ•´æ ¼å¼å¹¶é‡‡ç”¨è‡ªé€‚åº”é‡‡æ ·ç®—æ³•ï¼ŒARTæŒ‡å¯¼VLMå…³æ³¨æœ‰ä¿¡æ¯é‡çš„å…³ç³»ï¼ŒåŒæ—¶ä¿æŒæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬ä¸“æ³¨äºå…³ç³»åˆ†ç±»ä»»åŠ¡ï¼Œå…¶ä¸­ç»™å®šä¸»ä½“-å¯¹è±¡æ¡†ï¼Œæ¨¡å‹é¢„æµ‹å®ƒä»¬ä¹‹é—´çš„è°“è¯­ã€‚æˆ‘ä»¬åœ¨ä¿ç•™é›†ä¸Šè¿›è¡Œè°ƒæ•´ï¼Œå¹¶åœ¨å¤šä¸ªå¤æ‚çš„ä¿ç•™å¤–æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºåŸºçº¿æ–¹æ³•æœ‰æ˜æ˜¾çš„æ”¹è¿›ï¼Œå¹¶èƒ½å¤Ÿæ¨æ–­æœªè§è¿‡çš„å…³ç³»æ¦‚å¿µï¼Œè¿™æ˜¯ä¸»æµVRDæ–¹æ³•æ‰€ä¸å…·å¤‡çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨é¢„æµ‹çš„å…³åˆ†å‰²å¤æ‚åœºæ™¯æ¥å±•ç¤ºARTçš„å®é™…ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰å…³ç³»æ£€æµ‹ï¼ˆVRDï¼‰æ˜¯è¯†åˆ«åœºæ™¯ä¸­ç‰©ä½“é—´å…³ç³»çš„ä»»åŠ¡ã€‚</li>
<li>å•çº¯ä¾èµ–å…³ç³»æ£€æµ‹æ•°æ®è®­ç»ƒçš„VRDæ¨¡å‹æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚</li>
<li>æŒ‡ä»¤è°ƒæ•´ç›¸æ¯”æ‰‹å·¥æç¤ºæ›´èƒ½æœ‰æ•ˆé€‚åº”è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡ŒVRDã€‚</li>
<li>å¼•å…¥ARTæ¡†æ¶ï¼Œé€šè¿‡æŒ‡ä»¤è°ƒæ•´å’Œç­–ç•¥æ€§å®ä¾‹é€‰æ‹©æ¥ä¼˜åŒ–VLMçš„VRDæ€§èƒ½ã€‚</li>
<li>ARTèƒ½å°†VRDæ•°æ®é›†è½¬æ¢ä¸ºæŒ‡ä»¤è°ƒæ•´æ ¼å¼ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”é‡‡æ ·ç®—æ³•æŒ‡å¯¼æ¨¡å‹å…³æ³¨æœ‰ä¿¡æ¯é‡çš„å…³ç³»ã€‚</li>
<li>ARTåœ¨å…³ç³»åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„æµ‹ä¸»ä½“ä¸å¯¹è±¡é—´è°“è¯­çš„ä»»åŠ¡ä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23543">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3168028a32e2192cbb5e59205b8c2905.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-20da3b7ffbf7fecaaa7b820394077ec2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9c0ad1f12d403e40de80c070cd1059d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5dfb59605457da71907684de728ab52f.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Med-R-3-Enhancing-Medical-Retrieval-Augmented-Reasoning-of-LLMs-via-Progressive-Reinforcement-Learning"><a href="#Med-R-3-Enhancing-Medical-Retrieval-Augmented-Reasoning-of-LLMs-via-Progressive-Reinforcement-Learning" class="headerlink" title="Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via   Progressive Reinforcement Learning"></a>Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via   Progressive Reinforcement Learning</h2><p><strong>Authors:Keer Lu, Zheng Liang, Youquan Li, Jiejun Tan, Da Pan, Shusen Zhang, Guosheng Dong, Huang Leng</strong></p>
<p>In medical scenarios, effectively retrieving external knowledge and leveraging it for rigorous logical reasoning is of significant importance. Despite their potential, existing work has predominantly focused on enhancing either retrieval or reasoning capabilities of the models in isolation, with little attention given to their joint optimization, which leads to limited coordination between the two processes. Additionally, current methods rely heavily on supervised fine-tuning (SFT), which can cause models to memorize existing problem-solving pathways, thereby restricting their generalization ability when confronted with novel problem contexts. Furthermore, while some studies have explored to improve retrieval-augmented reasoning in general domains via reinforcement learning, their reward function designs do not adequately capture the specific demands of the medical domain. To address these challenges, we introduce <strong>Med-R$^3$</strong>, a <strong>Med</strong>ical <strong>R</strong>etrieval-augmented <strong>R</strong>easoning framework driven by progressive <strong>R</strong>einforcement learning. In this framework, we first develop the modelâ€™s ability to perform logical reasoning over medical problems. Subsequently, on the basis of this foundation, we adaptively optimize the retrieval capability to better align with the characteristics of knowledge corpus and external information utilization throughout the reasoning process. Finally, we conduct joint optimization of the modelâ€™s retrieval and reasoning coordination. Extensive experiments indicate that <strong>Med-R$^3$</strong> could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by 3.93% at a comparable parameter scale, while Qwen2.5-14B augmented with Med-R$^3$ shows a more substantial gain of 13.53%. </p>
<blockquote>
<p>åœ¨åŒ»ç–—åœºæ™¯ä¸­ï¼Œæœ‰æ•ˆåœ°æ£€ç´¢å¤–éƒ¨çŸ¥è¯†å¹¶å……åˆ†åˆ©ç”¨å…¶è¿›è¡Œä¸¥æ ¼çš„é€»è¾‘æ¨ç†å…·æœ‰éå¸¸é‡è¦çš„æ„ä¹‰ã€‚å°½ç®¡å­˜åœ¨æ½œåœ¨çš„æ”¹è¿›ç©ºé—´ï¼Œä½†ç°æœ‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨å¢å¼ºæ¨¡å‹çš„æ£€ç´¢æˆ–æ¨ç†èƒ½åŠ›æ–¹é¢ï¼Œè€Œå¯¹ä¸¤è€…çš„è”åˆä¼˜åŒ–å…³æ³¨è¾ƒå°‘ï¼Œå¯¼è‡´è¿™ä¸¤ä¸ªè¿‡ç¨‹ä¹‹é—´çš„åè°ƒæœ‰é™ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ–¹æ³•ä¸¥é‡ä¾èµ–äºæœ‰ç›‘ç£çš„å¾®è°ƒï¼ˆSFTï¼‰ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¨¡å‹è®°å¿†ç°æœ‰çš„é—®é¢˜è§£å†³è·¯å¾„ï¼Œä»è€Œåœ¨é¢å¯¹æ–°çš„é—®é¢˜æƒ…å¢ƒæ—¶é™åˆ¶å…¶æ³›åŒ–èƒ½åŠ›ã€‚è™½ç„¶ä¸€äº›ç ”ç©¶å·²ç»æ¢ç´¢äº†é€šè¿‡å¼ºåŒ–å­¦ä¹ æ”¹è¿›ä¸€èˆ¬é¢†åŸŸçš„æ£€ç´¢å¢å¼ºæ¨ç†ï¼Œä½†å…¶å¥–åŠ±å‡½æ•°è®¾è®¡å¹¶æ²¡æœ‰å……åˆ†æ•æ‰åˆ°åŒ»ç–—é¢†åŸŸçš„ç‰¹å®šéœ€æ±‚ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>Med-R$^3$<strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±æ¸è¿›å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„</strong>Med</strong>ical <strong>R</strong>etrieval-augmented <strong>R</strong>easoningæ¡†æ¶ã€‚<strong>åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘æ¨¡å‹è§£å†³åŒ»ç–—é—®é¢˜çš„é€»è¾‘æ¨ç†èƒ½åŠ›ã€‚éšåï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è‡ªé€‚åº”åœ°ä¼˜åŒ–æ£€ç´¢èƒ½åŠ›ï¼Œä»¥æ›´å¥½åœ°é€‚åº”çŸ¥è¯†è¯­æ–™åº“å’Œå¤–éƒ¨ä¿¡æ¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„åˆ©ç”¨ç‰¹ç‚¹ã€‚æœ€åï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹çš„æ£€ç´¢å’Œæ¨ç†åè°ƒè¿›è¡Œè”åˆä¼˜åŒ–ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œ</strong>Med-R$^3$**èƒ½å¤Ÿè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨å‚æ•°è§„æ¨¡ç›¸å½“çš„æƒ…å†µä¸‹ï¼ŒLLaMA3.1-8B-Instruct + Med-R$^3$è¶…è¶Šäº†é—­æºçš„GPT-4o-miniï¼Œæ€§èƒ½æå‡äº†3.93%ï¼Œè€ŒQwen2.5-14Bä¸Med-R$^3$ç›¸ç»“åˆåˆ™æ˜¾ç¤ºå‡ºæ›´å¤§çš„æ€§èƒ½æå‡ï¼Œè¾¾åˆ°äº†13.53%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23541v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨åŒ»ç–—åœºæ™¯ä¸­ï¼Œå¤–éƒ¨çŸ¥è¯†çš„æ£€ç´¢å’Œé€»è¾‘æ¨ç†çš„æœ‰æ•ˆç»“åˆè‡³å…³é‡è¦ã€‚ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æ¨¡å‹çš„æ£€ç´¢æˆ–æ¨ç†èƒ½åŠ›çš„å•ä¸€æå‡ï¼Œè€Œå¿½è§†äº†äºŒè€…çš„è”åˆä¼˜åŒ–ï¼Œå¯¼è‡´ä¸¤è€…ä¹‹é—´çš„åè°ƒå—é™ã€‚æ­¤å¤–ï¼Œç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç”±æ¸è¿›å¼å¼ºåŒ–å­¦ä¹ é©±åŠ¨çš„Med-R$^3$åŒ»ç–—æ£€ç´¢å¢å¼ºæ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆåŸ¹å…»æ¨¡å‹è§£å†³åŒ»ç–—é—®é¢˜çš„èƒ½åŠ›ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè‡ªé€‚åº”ä¼˜åŒ–æ£€ç´¢èƒ½åŠ›ï¼Œä½¿å…¶ä¸çŸ¥è¯†åº“å’Œå¤–éƒ¨ä¿¡æ¯åˆ©ç”¨çš„ç‰¹æ€§ç›¸åŒ¹é…ã€‚è”åˆä¼˜åŒ–æ¨¡å‹çš„æ£€ç´¢å’Œæ¨ç†åè°ƒã€‚å®éªŒè¡¨æ˜ï¼ŒMed-R$^3$è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»ç–—åœºæ™¯ä¸­å¤–éƒ¨çŸ¥è¯†çš„æ£€ç´¢å’Œé€»è¾‘æ¨ç†ç»“åˆè‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨å•ä¸€èƒ½åŠ›çš„æå‡ï¼Œç¼ºä¹è”åˆä¼˜åŒ–ï¼Œå¯¼è‡´åè°ƒå—é™ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>Med-R$^3$æ¡†æ¶æ—¨åœ¨è§£å†³ä»¥ä¸ŠæŒ‘æˆ˜ï¼Œé€šè¿‡æ¸è¿›å¼å¼ºåŒ–å­¦ä¹ é©±åŠ¨åŒ»ç–—æ£€ç´¢å¢å¼ºæ¨ç†ã€‚</li>
<li>Med-R$^3$é¦–å…ˆåŸ¹å…»æ¨¡å‹è§£å†³åŒ»ç–—é—®é¢˜çš„èƒ½åŠ›ï¼Œå†ä¼˜åŒ–æ£€ç´¢èƒ½åŠ›ã€‚</li>
<li>Med-R$^3$å®ç°äº†æ¨¡å‹çš„æ£€ç´¢å’Œæ¨ç†çš„è”åˆä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23541">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-37ca491a729bafd60710196e36c4790d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c765c9c2918ef3eefd8a50e3ac67f34.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80513fc864b19ef77c55b70982db027a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-af52adda2f08eff4c33b4d359415cee1.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LLMs-Between-the-Nodes-Community-Discovery-Beyond-Vectors"><a href="#LLMs-Between-the-Nodes-Community-Discovery-Beyond-Vectors" class="headerlink" title="LLMs Between the Nodes: Community Discovery Beyond Vectors"></a>LLMs Between the Nodes: Community Discovery Beyond Vectors</h2><p><strong>Authors:Ekta Gujral, Apurva Sinha</strong></p>
<p>Community detection in social network graphs plays a vital role in uncovering group dynamics, influence pathways, and the spread of information. Traditional methods focus primarily on graph structural properties, but recent advancements in Large Language Models (LLMs) open up new avenues for integrating semantic and contextual information into this task. In this paper, we present a detailed investigation into how various LLM-based approaches perform in identifying communities within social graphs. We introduce a two-step framework called CommLLM, which leverages the GPT-4o model along with prompt-based reasoning to fuse language model outputs with graph structure. Evaluations are conducted on six real-world social network datasets, measuring performance using key metrics such as Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), Variation of Information (VOI), and cluster purity. Our findings reveal that LLMs, particularly when guided by graph-aware strategies, can be successfully applied to community detection tasks in small to medium-sized graphs. We observe that the integration of instruction-tuned models and carefully engineered prompts significantly improves the accuracy and coherence of detected communities. These insights not only highlight the potential of LLMs in graph-based research but also underscore the importance of tailoring model interactions to the specific structure of graph data. </p>
<blockquote>
<p>ç¤¾äº¤ç½‘ç»œå›¾ä¸­çš„ç¤¾åŒºæ£€æµ‹åœ¨æ­ç¤ºç¾¤ä½“åŠ¨æ€ã€å½±å“è·¯å¾„å’Œä¿¡æ¯ä¼ æ’­æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨å›¾çš„ç»“æ„å±æ€§ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºå°†æ­¤ä»»åŠ¡ä¸è¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯é›†æˆå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹åŸºäºLLMçš„ä¸åŒæ–¹æ³•åœ¨è¯†åˆ«ç¤¾äº¤ç½‘ç»œå›¾å†…ç¤¾åŒºæ—¶çš„è¡¨ç°è¿›è¡Œäº†è¯¦ç»†è°ƒæŸ¥ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªåˆ†ä¸¤æ­¥è¿›è¡Œçš„æ¡†æ¶ï¼Œç§°ä¸ºCommLLMï¼Œå®ƒåˆ©ç”¨GPT-4oæ¨¡å‹å’ŒåŸºäºæç¤ºçš„æ¨ç†æ¥èåˆè¯­è¨€æ¨¡å‹è¾“å‡ºå’Œå›¾ç»“æ„ã€‚æˆ‘ä»¬åœ¨å…­ä¸ªçœŸå®ä¸–ç•Œçš„ç¤¾äº¤ç½‘ç»œæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä½¿ç”¨å½’ä¸€åŒ–äº’ä¿¡æ¯ï¼ˆNMIï¼‰ã€è°ƒæ•´å…°å¾·æŒ‡æ•°ï¼ˆARIï¼‰ã€ä¿¡æ¯å˜å¼‚ï¼ˆVOIï¼‰å’Œé›†ç¾¤çº¯åº¦ç­‰å…³é”®æŒ‡æ ‡æ¥è¡¡é‡æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒLLMï¼Œå°¤å…¶æ˜¯å½“é‡‡ç”¨å›¾æ„ŸçŸ¥ç­–ç•¥è¿›è¡Œå¼•å¯¼æ—¶ï¼Œå¯ä»¥æˆåŠŸåº”ç”¨äºä¸­å°å‹å›¾çš„ç¤¾åŒºæ£€æµ‹ä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°æŒ‡ä»¤å¾®è°ƒæ¨¡å‹çš„é›†æˆå’Œç²¾å¿ƒè®¾è®¡çš„æç¤ºå¯ä»¥æ˜¾è‘—æé«˜æ£€æµ‹åˆ°çš„ç¤¾åŒºçš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ã€‚è¿™äº›è§è§£ä¸ä»…çªå‡ºäº†LLMåœ¨å›¾åŸºç ”ç©¶ä¸­çš„æ½œåŠ›ï¼Œä¹Ÿå¼ºè°ƒäº†é’ˆå¯¹å›¾å½¢æ•°æ®çš„ç‰¹å®šç»“æ„å®šåˆ¶æ¨¡å‹äº¤äº’çš„é‡è¦æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22955v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>ç¤¾åŒºæ£€æµ‹åœ¨ç¤¾ä¼šç½‘ç»œå›¾ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œæ­ç¤ºç¾¤ä½“åŠ¨æ€ã€å½±å“è·¯å¾„å’Œä¿¡æ¯ä¼ æ’­ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦å…³æ³¨å›¾çš„ç»“æ„å±æ€§ï¼Œä½†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•ä¸ºé›†æˆè¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æä¾›äº†æ–°çš„é€”å¾„ã€‚æœ¬æ–‡è¯¦ç»†è°ƒæŸ¥äº†LLMåœ¨å„ç§æ–¹æ³•ä¸­çš„è¡¨ç°ï¼Œå¹¶ä»‹ç»äº†ä¸€ä¸ªåä¸ºCommLLMçš„ä¸¤æ­¥æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨GPT-4oæ¨¡å‹å’ŒåŸºäºæç¤ºçš„æ¨ç†å°†è¯­è¨€æ¨¡å‹è¾“å‡ºä¸å›¾å½¢ç»“æ„ç›¸èåˆã€‚åœ¨å…­ä¸ªçœŸå®ä¸–ç•Œçš„ç¤¾ä¼šç½‘ç»œæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä½¿ç”¨å½’ä¸€åŒ–äº’ä¿¡æ¯ï¼ˆNMIï¼‰ã€è°ƒæ•´å…°å¾·æŒ‡æ•°ï¼ˆARIï¼‰ã€å˜å¼‚ä¿¡æ¯ï¼ˆVOIï¼‰å’Œèšç±»çº¯åº¦ç­‰å…³é”®æŒ‡æ ‡è¡¡é‡æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMåœ¨å°å‹è‡³ä¸­å‹å›¾ä¸­çš„ç¤¾åŒºæ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œç‰¹åˆ«æ˜¯é‡‡ç”¨å›¾æ„ŸçŸ¥ç­–ç•¥æŒ‡å¯¼æ—¶ã€‚é›†æˆæŒ‡ä»¤å¾®è°ƒæ¨¡å‹å’Œç²¾å¿ƒè®¾è®¡æç¤ºæ˜¾è‘—æé«˜äº†æ£€æµ‹åˆ°çš„ç¤¾åŒºçš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ã€‚è¿™äº›è§è§£ä¸ä»…çªæ˜¾äº†LLMåœ¨å›¾åŸºç ”ç©¶ä¸­çš„æ½œåŠ›ï¼Œä¹Ÿå¼ºè°ƒäº†é’ˆå¯¹å›¾å½¢æ•°æ®ç‰¹å®šç»“æ„å®šåˆ¶æ¨¡å‹äº¤äº’çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¤¾åŒºæ£€æµ‹åœ¨ç¤¾ä¼šç½‘ç»œå›¾ä¸­å…·æœ‰é‡è¦ä½œç”¨ï¼Œèƒ½æ­ç¤ºç¾¤ä½“åŠ¨æ€ã€å½±å“è·¯å¾„å’Œä¿¡æ¯ä¼ æ’­ã€‚</li>
<li>ä¼ ç»Ÿç¤¾åŒºæ£€æµ‹æ–¹æ³•ä¸»è¦å…³æ³¨å›¾çš„ç»“æ„å±æ€§ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•ä¸ºç¤¾åŒºæ£€æµ‹ä»»åŠ¡æä¾›äº†æ–°çš„é€”å¾„ï¼Œèƒ½é›†æˆè¯­ä¹‰å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåä¸ºCommLLMçš„ä¸¤æ­¥æ¡†æ¶ï¼Œåˆ©ç”¨GPT-4oæ¨¡å‹å’ŒåŸºäºæç¤ºçš„æ¨ç†æ¥èåˆè¯­è¨€æ¨¡å‹è¾“å‡ºå’Œå›¾å½¢ç»“æ„ã€‚</li>
<li>åœ¨å¤šä¸ªçœŸå®ä¸–ç•Œçš„ç¤¾ä¼šç½‘ç»œæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜LLMåœ¨å°å‹è‡³ä¸­å‹å›¾çš„ç¤¾åŒºæ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚</li>
<li>é‡‡ç”¨å›¾æ„ŸçŸ¥ç­–ç•¥æŒ‡å¯¼çš„LLMé›†æˆæ–¹æ³•èƒ½æé«˜ç¤¾åŒºæ£€æµ‹çš„å‡†ç¡®æ€§å’Œè¿è´¯æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-5f3b13a1c2100e572cd85e913aeb892b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c4bb5b5ae33416bcfcb29963833e00a1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5b223dbfc5765d0a85c751b8b8624a0d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c64b44063c688a4e02a506166d7de7a9.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Automated-Label-Placement-on-Maps-via-Large-Language-Models"><a href="#Automated-Label-Placement-on-Maps-via-Large-Language-Models" class="headerlink" title="Automated Label Placement on Maps via Large Language Models"></a>Automated Label Placement on Maps via Large Language Models</h2><p><strong>Authors:Harry Shomer, Jiejun Xu</strong></p>
<p>Label placement is a critical aspect of map design, serving as a form of spatial annotation that directly impacts clarity and interpretability. Despite its importance, label placement remains largely manual and difficult to scale, as existing automated systems struggle to integrate cartographic conventions, adapt to context, or interpret labeling instructions. In this work, we introduce a new paradigm for automatic label placement (ALP) that formulates the task as a data editing problem and leverages large language models (LLMs) for context-aware spatial annotation. To support this direction, we curate MAPLE, the first known benchmarking dataset for evaluating ALP on real-world maps, encompassing diverse landmark types and label placement annotations from open-source data. Our method retrieves labeling guidelines relevant to each landmark type leveraging retrieval-augmented generation (RAG), integrates them into prompts, and employs instruction-tuned LLMs to generate ideal label coordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall performance and generalization across different types of landmarks. This includes both zero-shot and instruction-tuned performance. Our results demonstrate that LLMs, when guided by structured prompts and domain-specific retrieval, can learn to perform accurate spatial edits, aligning the generated outputs with expert cartographic standards. Overall, our work presents a scalable framework for AI-assisted map finishing and demonstrates the potential of foundation models in structured data editing tasks. The code and data can be found at <a target="_blank" rel="noopener" href="https://github.com/HarryShomer/MAPLE">https://github.com/HarryShomer/MAPLE</a>. </p>
<blockquote>
<p>æ ‡ç­¾æ”¾ç½®æ˜¯åœ°å›¾è®¾è®¡ä¸­çš„ä¸€ä¸ªå…³é”®æ–¹é¢ï¼Œä½œä¸ºä¸€ç§ç©ºé—´æ³¨é‡Šï¼Œå®ƒç›´æ¥å½±å“åœ°å›¾çš„æ¸…æ™°åº¦å’Œå¯è§£é‡Šæ€§ã€‚å°½ç®¡å…¶é‡è¦æ€§å¾ˆé«˜ï¼Œä½†æ ‡ç­¾æ”¾ç½®ä»ç„¶ä¸»è¦æ˜¯æ‰‹åŠ¨æ“ä½œï¼Œéš¾ä»¥æ‰©å±•ï¼Œå› ä¸ºç°æœ‰çš„è‡ªåŠ¨åŒ–ç³»ç»Ÿå¾ˆéš¾èå…¥åœ°å›¾åˆ¶ä½œè§„èŒƒã€é€‚åº”ä¸Šä¸‹æ–‡æˆ–è§£é‡Šæ ‡ç­¾æŒ‡ä»¤ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è‡ªåŠ¨æ ‡ç­¾æ”¾ç½®ï¼ˆALPï¼‰èŒƒå¼ï¼Œå°†ä»»åŠ¡åˆ¶å®šä¸ºæ•°æ®ç¼–è¾‘é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥ç©ºé—´æ³¨é‡Šã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€æ–¹å‘ï¼Œæˆ‘ä»¬æ•´ç†äº†MAPLEæ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªå·²çŸ¥ç”¨äºè¯„ä¼°çœŸå®ä¸–ç•Œåœ°å›¾ä¸ŠALPæ€§èƒ½çš„åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šç§åœ°æ ‡ç±»å‹å’Œæ ‡ç­¾æ”¾ç½®æ³¨é‡Šï¼Œè¿™äº›æ•°æ®å‡æ¥è‡ªå¼€æºæ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ£€ç´¢ä¸æ¯ç§åœ°æ ‡ç±»å‹ç›¸å…³çš„æ ‡ç­¾æŒ‡å—ï¼Œå°†å…¶é›†æˆåˆ°æç¤ºä¸­ï¼Œå¹¶ä½¿ç”¨æŒ‡ä»¤å¾®è°ƒLLMç”Ÿæˆç†æƒ³çš„æ ‡ç­¾åæ ‡ã€‚æˆ‘ä»¬åœ¨MAPLEä¸Šè¯„ä¼°äº†å››ä¸ªå¼€æºLLMï¼Œåˆ†æäº†æ•´ä½“æ€§èƒ½ä»¥åŠåœ¨ä¸åŒç±»å‹åœ°æ ‡ä¹‹é—´çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™åŒ…æ‹¬é›¶æ ·æœ¬å’ŒæŒ‡ä»¤å¾®è°ƒæ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å—åˆ°ç»“æ„åŒ–æç¤ºå’Œé¢†åŸŸç‰¹å®šæ£€ç´¢çš„æŒ‡å¯¼æ—¶ï¼ŒLLMå¯ä»¥å­¦ä¹ æ‰§è¡Œç²¾ç¡®çš„ç©ºé—´ç¼–è¾‘ï¼Œä½¿ç”Ÿæˆè¾“å‡ºä¸ä¸“å®¶åœ°å›¾åˆ¶ä½œæ ‡å‡†ç›¸ç¬¦ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸ºAIè¾…åŠ©åœ°å›¾åˆ¶ä½œæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œå¹¶å±•ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨ç»“æ„åŒ–æ•°æ®ç¼–è¾‘ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HarryShomer/MAPLE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/HarryShomer/MAPLEæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22952v1">PDF</a> Workshop on AI for Data Editing (AI4DE) at KDD 2025</p>
<p><strong>Summary</strong><br>æ ‡ç­¾æ”¾ç½®åœ¨åœ°å›¾è®¾è®¡ä¸­è‡³å…³é‡è¦ï¼Œç›´æ¥å½±å“åœ°å›¾çš„æ¸…æ™°åº¦å’Œå¯è§£é‡Šæ€§ã€‚å°½ç®¡å…¶é‡è¦æ€§æ˜¾è‘—ï¼Œä½†æ ‡ç­¾æ”¾ç½®ä»ä¸»è¦ä¾èµ–æ‰‹åŠ¨æ“ä½œï¼Œéš¾ä»¥å®ç°è§„æ¨¡åŒ–ã€‚ç°æœ‰è‡ªåŠ¨ç³»ç»Ÿéš¾ä»¥èå…¥åœ°å›¾åˆ¶ä½œè§„èŒƒã€é€‚åº”ä¸Šä¸‹æ–‡æˆ–è§£é‡Šæ ‡ç­¾æŒ‡ç¤ºã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°çš„è‡ªåŠ¨æ ‡ç­¾æ”¾ç½®ï¼ˆALPï¼‰èŒƒå¼ï¼Œå°†ä»»åŠ¡å½¢å¼åŒ–ä¸ºæ•°æ®ç¼–è¾‘é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç©ºé—´æ³¨é‡Šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ•´ç†äº†MAPLEæ•°æ®é›†ï¼Œè¿™æ˜¯é¦–ä¸ªå·²çŸ¥çš„åœ¨ç°å®åœ°å›¾ä¸­è¯„ä¼°ALPçš„åŸºå‡†æ•°æ®é›†ï¼Œæ¶µç›–äº†å¤šç§åœ°æ ‡ç±»å‹å’Œæ ‡ç­¾æ”¾ç½®æ³¨é‡Šçš„å¼€æºæ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åˆ©ç”¨ä¸æ¯ç§åœ°æ ‡ç±»å‹ç›¸å…³çš„æ ‡æ³¨æŒ‡å—ï¼Œå°†å…¶æ•´åˆåˆ°æç¤ºä¸­ï¼Œå¹¶é‡‡ç”¨æŒ‡ä»¤ä¼˜åŒ–LLMç”Ÿæˆç†æƒ³æ ‡ç­¾åæ ‡ã€‚æˆ‘ä»¬åœ¨MAPLEæ•°æ®é›†ä¸Šè¯„ä¼°äº†å››ä¸ªå¼€æºLLMçš„æ•´ä½“æ€§èƒ½å’Œåœ¨ä¸åŒç±»å‹åœ°æ ‡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’ŒæŒ‡ä»¤ä¼˜åŒ–æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨ç»“æ„åŒ–æç¤ºå’Œé¢†åŸŸç‰¹å®šæ£€ç´¢çš„æŒ‡å¯¼ä¸‹ï¼ŒLLMå¯ä»¥å­¦ä¹ æ‰§è¡Œç²¾ç¡®çš„ç©ºé—´ç¼–è¾‘ï¼Œä½¿ç”Ÿæˆè¾“å‡ºä¸ä¸“å®¶åœ°å›¾åˆ¶ä½œæ ‡å‡†ç›¸ç¬¦ã€‚æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸ºAIè¾…åŠ©åœ°å›¾å®Œæˆæä¾›äº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œå¹¶å±•ç¤ºäº†åŸºç¡€æ¨¡å‹åœ¨ç»“æ„åŒ–æ•°æ®ç¼–è¾‘ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ ‡ç­¾æ”¾ç½®åœ¨åœ°å›¾è®¾è®¡ä¸­å…·æœ‰é‡è¦å½±å“ï¼Œç›´æ¥å½±å“åœ°å›¾çš„æ¸…æ™°åº¦å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>ç°æœ‰è‡ªåŠ¨æ ‡ç­¾æ”¾ç½®ç³»ç»Ÿå­˜åœ¨å›°éš¾ï¼Œéš¾ä»¥èå…¥åœ°å›¾åˆ¶ä½œè§„èŒƒã€é€‚åº”ä¸Šä¸‹æ–‡æˆ–è§£é‡Šæ ‡ç­¾æŒ‡ç¤ºã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°çš„è‡ªåŠ¨æ ‡ç­¾æ”¾ç½®ï¼ˆALPï¼‰èŒƒå¼ï¼Œå°†ä»»åŠ¡å½¢å¼åŒ–ä¸ºæ•°æ®ç¼–è¾‘é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ç©ºé—´æ³¨é‡Šã€‚</li>
<li>æ•´ç†äº†MAPLEæ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°åœ¨ç°å®åœ°å›¾ä¸­çš„ALPæ€§èƒ½ã€‚</li>
<li>é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åˆ©ç”¨ä¸åœ°æ ‡ç±»å‹ç›¸å…³çš„æ ‡æ³¨æŒ‡å—ï¼Œå¹¶é‡‡ç”¨æŒ‡ä»¤ä¼˜åŒ–LLMç”Ÿæˆç†æƒ³æ ‡ç­¾åæ ‡ã€‚</li>
<li>LLMåœ¨ç»“æ„åŒ–æç¤ºå’Œé¢†åŸŸç‰¹å®šæ£€ç´¢çš„æŒ‡å¯¼ä¸‹ï¼Œå¯ä»¥å­¦ä¹ æ‰§è¡Œç²¾ç¡®çš„ç©ºé—´ç¼–è¾‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22952">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-9261c065e823d1b7034b8420c9c8d930.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a4ef35c193bd45dd530fe5c533fe7101.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-13b18f0e35d9c9c0fb9fcfa81014381c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a483514fd0c4e2bf39d533949453c01f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a93af1e0affe9d6020fd97496d728b1d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a3c400408fd1c2c9c296001695f90fed.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Multilingual-Political-Views-of-Large-Language-Models-Identification-and-Steering"><a href="#Multilingual-Political-Views-of-Large-Language-Models-Identification-and-Steering" class="headerlink" title="Multilingual Political Views of Large Language Models: Identification   and Steering"></a>Multilingual Political Views of Large Language Models: Identification   and Steering</h2><p><strong>Authors:Daniil Gurgurov, Katharina Trinley, Ivan Vykopal, Josef van Genabith, Simon Ostermann, Roberto Zamparelli</strong></p>
<p>Large language models (LLMs) are increasingly used in everyday tools and applications, raising concerns about their potential influence on political views. While prior research has shown that LLMs often exhibit measurable political biasesâ€“frequently skewing toward liberal or progressive positionsâ€“key gaps remain. Most existing studies evaluate only a narrow set of models and languages, leaving open questions about the generalizability of political biases across architectures, scales, and multilingual settings. Moreover, few works examine whether these biases can be actively controlled.   In this work, we address these gaps through a large-scale study of political orientation in modern open-source instruction-tuned LLMs. We evaluate seven models, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using the Political Compass Test with 11 semantically equivalent paraphrases per statement to ensure robust measurement. Our results reveal that larger models consistently shift toward libertarian-left positions, with significant variations across languages and model families. To test the manipulability of political stances, we utilize a simple center-of-mass activation intervention technique and show that it reliably steers model responses toward alternative ideological positions across multiple languages. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/d-gurgurov/Political-Ideologies-LLMs">https://github.com/d-gurgurov/Political-Ideologies-LLMs</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¥å¸¸å·¥å…·å’Œåº”ç”¨ç¨‹åºä¸­çš„ä½¿ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œå¼•å‘äº†äººä»¬å¯¹å®ƒä»¬å¯¹æ”¿æ²»è§‚ç‚¹æ½œåœ¨å½±å“çš„æ‹…å¿§ã€‚è™½ç„¶å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒLLMé€šå¸¸è¡¨ç°å‡ºå¯è¡¡é‡çš„æ”¿æ²»åè§ï¼Œå¸¸å¸¸å€¾å‘äºè‡ªç”±æˆ–è¿›æ­¥ç«‹åœºï¼Œä½†ä»å­˜åœ¨å…³é”®å·®è·ã€‚å¤§å¤šæ•°ç°æœ‰ç ”ç©¶åªè¯„ä¼°äº†æœ‰é™çš„æ¨¡å‹å’Œè¯­è¨€ï¼Œå¯¹äºè·¨æ¶æ„ã€è§„æ¨¡å’Œè·¨è¯­è¨€ç¯å¢ƒä¸­æ”¿æ²»åè§çš„æ™®éæ€§ä»å­˜åœ¨å¼€æ”¾é—®é¢˜ã€‚æ­¤å¤–ï¼Œå¾ˆå°‘æœ‰ç ”ç©¶æ¢è®¨æ˜¯å¦å¯ä»¥ä¸»åŠ¨æ§åˆ¶è¿™äº›åè§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€é¡¹é’ˆå¯¹ç°ä»£å¼€æºæŒ‡ä»¤è°ƒæ•´çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ”¿æ²»å€¾å‘çš„å¤§è§„æ¨¡ç ”ç©¶æ¥å¡«è¡¥è¿™äº›ç©ºç™½ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸ƒä¸ªæ¨¡å‹ï¼ŒåŒ…æ‹¬LLaMA-3.1ã€Qwen-3å’ŒAya-Expanseç­‰ï¼Œè·¨è¶Š14ç§è¯­è¨€ä½¿ç”¨æ”¿æ²»æŒ‡å—é’ˆæµ‹è¯•ï¼Œæ¯ä¸ªé™ˆè¿°éƒ½æœ‰11ä¸ªè¯­ä¹‰ç­‰ä»·çš„åŒä¹‰æ›¿æ¢ä»¥ç¡®ä¿ç¨³å¥æµ‹é‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ›´å¤§çš„æ¨¡å‹å¾€å¾€å€¾å‘äºè‡ªç”±ä¸»ä¹‰å·¦ç¿¼ç«‹åœºï¼Œä¸åŒè¯­è¨€å’Œæ¨¡å‹å®¶æ—ä¹‹é—´å­˜åœ¨æ˜¾è‘—å˜åŒ–ã€‚ä¸ºäº†æµ‹è¯•æ”¿æ²»ç«‹åœºçš„å¯æ“æ§æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ç®€å•çš„é‡å¿ƒæ¿€æ´»å¹²é¢„æŠ€æœ¯ï¼Œå¹¶è¯æ˜è¯¥æŠ€æœ¯å¯ä»¥å¯é åœ°å¼•å¯¼æ¨¡å‹å“åº”å‘å¤šç§è¯­è¨€çš„æ›¿ä»£æ„è¯†å½¢æ€ç«‹åœºé æ‹¢ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/d-gurgurov/Political-Ideologies-LLMs%E5%85%AC%E5%BC%BA%E9%80%B%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/d-gurgurov/Political-Ideologies-LLMså…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22623v1">PDF</a> pre-print</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¥å¸¸å·¥å…·å’Œåº”ç”¨ä¸­çš„ä½¿ç”¨æ—¥ç›Šæ™®åŠï¼Œå¼•å‘äººä»¬å¯¹å…¶å¯èƒ½å¯¹æ”¿æ²»è§‚ç‚¹äº§ç”Ÿå½±å“çš„æ‹…å¿§ã€‚ç°æœ‰ç ”ç©¶è¡¨æ˜ï¼ŒLLMå¸¸è¡¨ç°å‡ºå¯è¡¡é‡çš„æ”¿æ²»åè§ï¼Œé€šå¸¸åå‘è‡ªç”±æˆ–è¿›æ­¥ç«‹åœºï¼Œä½†ä»å­˜åœ¨å…³é”®å·®è·ã€‚å¤šæ•°ç°æœ‰ç ”ç©¶ä»…è¯„ä¼°äº†æœ‰é™çš„æ¨¡å‹å’Œè¯­è¨€ï¼Œå…³äºæ”¿æ²»åè§åœ¨æ¶æ„ã€è§„æ¨¡å’Œè·¨è¯­è¨€ç¯å¢ƒä¸­çš„æ™®éæ€§é—®é¢˜å°šå­˜ç–‘é—®ã€‚æ­¤å¤–ï¼Œå¾ˆå°‘æœ‰ç ”ç©¶æ¢è®¨è¿™äº›åè§æ˜¯å¦å¯ä»¥å¾—åˆ°ä¸»åŠ¨æ§åˆ¶ã€‚æœ¬ç ”ç©¶é€šè¿‡åœ¨ç°ä»£å¼€æºæŒ‡ä»¤è®­ç»ƒLLMä¸­è¿›è¡Œå¤§è§„æ¨¡æ”¿æ²»å€¾å‘ç ”ç©¶ï¼Œå¡«è¡¥è¿™äº›ç©ºç™½ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸ƒä¸ªæ¨¡å‹ï¼ŒåŒ…æ‹¬LLaMA-3.1ã€Qwen-3å’ŒAya-Expanseç­‰ï¼Œä½¿ç”¨æ”¿æ²»é‡è¡¨æµ‹è¯•åœ¨14ç§è¯­è¨€ä¸­çš„æ”¿æ²»å€¾å‘ï¼ŒåŒæ—¶é‡‡ç”¨11ä¸ªè¯­ä¹‰ç­‰æ•ˆçš„åŒä¹‰æ›¿æ¢å¥ä»¥ç¡®ä¿ç¨³å¥æµ‹é‡ã€‚ç»“æœæ˜¾ç¤ºï¼Œå¤§å‹æ¨¡å‹æŒç»­å‘è‡ªç”±å·¦å€¾ç«‹åœºè½¬å˜ï¼Œä¸åŒè¯­è¨€å’Œæ¨¡å‹å®¶æ—é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚ä¸ºæµ‹è¯•æ”¿æ²»ç«‹åœºçš„å¯æ“æ§æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨ç®€å•çš„é‡å¿ƒæ¿€æ´»å¹²é¢„æŠ€æœ¯ï¼Œå¹¶è¯æ˜å…¶åœ¨å¤šç§è¯­è¨€ä¸­å¯é åœ°å¼•å¯¼æ¨¡å‹å“åº”èµ°å‘æ›¿ä»£æ„è¯†å½¢æ€ç«‹åœºã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/d-gurgurov/Political-Ideologies-LLMs">https://github.com/d-gurgurov/Political-Ideologies-LLMs</a> ä¸­è·å–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¥å¸¸åº”ç”¨ä¸­çš„æ™®åŠå¼•å‘å¯¹æ”¿æ²»è§‚ç‚¹æ½œåœ¨å½±å“çš„æ‹…å¿§ã€‚</li>
<li>ç°æœ‰ç ”ç©¶ä»…å¯¹æœ‰é™çš„æ¨¡å‹å’Œè¯­è¨€è¿›è¡Œäº†è¯„ä¼°ï¼Œå­˜åœ¨å…³äºæ”¿æ²»åè§æ™®éæ€§çš„é‡è¦é—®é¢˜ã€‚</li>
<li>ç ”ç©¶å‘ç°å¤§å‹æ¨¡å‹å€¾å‘äºè‡ªç”±å·¦å€¾ç«‹åœºï¼Œä¸åŒè¯­è¨€å’Œæ¨¡å‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>é‡‡ç”¨é‡å¿ƒæ¿€æ´»å¹²é¢„æŠ€æœ¯å¯æœ‰æ•ˆæ“æ§æ¨¡å‹çš„æ”¿æ²»ç«‹åœºå“åº”ã€‚</li>
<li>ç ”ç©¶æ¶µç›–äº†å¤šç§è¯­è¨€å’Œæ¨¡å‹ï¼Œä¸ºå…¨é¢ç†è§£LLMä¸­çš„æ”¿æ²»åè§æä¾›äº†é‡è¦è§è§£ã€‚</li>
<li>å…¬å…±å¯ç”¨ä»£ç ä¸ºæœªæ¥çš„ç ”ç©¶å’Œå¼€å‘æä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22623">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-34115ca52ca3a6c205d8c552fbadf0e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6070bb96499012ce2cc9c8007d2c274.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6eaba45dfb484ee54efd4b3e6c20647c.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Whoâ€™s-important-â€“-SUnSET-Synergistic-Understanding-of-Stakeholder-Events-and-Time-for-Timeline-Generation"><a href="#Whoâ€™s-important-â€“-SUnSET-Synergistic-Understanding-of-Stakeholder-Events-and-Time-for-Timeline-Generation" class="headerlink" title="Whoâ€™s important? â€“ SUnSET: Synergistic Understanding of Stakeholder,   Events and Time for Timeline Generation"></a>Whoâ€™s important? â€“ SUnSET: Synergistic Understanding of Stakeholder,   Events and Time for Timeline Generation</h2><p><strong>Authors:Tiviatis Sim, Kaiwen Yang, Shen Xin, Kenji Kawaguchi</strong></p>
<p>As news reporting becomes increasingly global and decentralized online, tracking related events across multiple sources presents significant challenges. Existing news summarization methods typically utilizes Large Language Models and Graphical methods on article-based summaries. However, this is not effective since it only considers the textual content of similarly dated articles to understand the gist of the event. To counteract the lack of analysis on the parties involved, it is essential to come up with a novel framework to gauge the importance of stakeholders and the connection of related events through the relevant entities involved. Therefore, we present SUnSET: Synergistic Understanding of Stakeholder, Events and Time for the task of Timeline Summarization (TLS). We leverage powerful Large Language Models (LLMs) to build SET triplets and introduced the use of stakeholder-based ranking to construct a $Relevancy$ metric, which can be extended into general situations. Our experimental results outperform all prior baselines and emerged as the new State-of-the-Art, highlighting the impact of stakeholder information within news article. </p>
<blockquote>
<p>éšç€æ–°é—»æŠ¥é“è¶Šæ¥è¶Šå…¨çƒåŒ–å’Œåœ¨çº¿åˆ†æ•£åŒ–ï¼Œä»å¤šä¸ªæ¥æºè¿½è¸ªç›¸å…³äº‹ä»¶å¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„æ–°é—»æ‘˜è¦æ–¹æ³•é€šå¸¸åŸºäºæ–‡ç« ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå›¾å½¢æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æœ‰æ•ˆï¼Œå› ä¸ºå®ƒåªè€ƒè™‘å…·æœ‰ç›¸åŒæ—¥æœŸçš„æ–‡ç« çš„æ–‡æœ¬å†…å®¹æ¥äº†è§£äº‹ä»¶çš„å¤§æ„ã€‚ä¸ºäº†å¼¥è¡¥å¯¹å‚ä¸æ–¹çš„åˆ†æä¸è¶³ï¼Œæå‡ºä¸€ä¸ªæ–°é¢–æ¡†æ¶æ¥è¡¡é‡åˆ©ç›Šç›¸å…³æ–¹çš„é‡è¦æ€§ä»¥åŠç›¸å…³å®ä½“æ‰€æ¶‰åŠç›¸å…³äº‹ä»¶ä¹‹é—´çš„è”ç³»è‡³å…³é‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºSUnSETï¼šååŒç†è§£åˆ©ç›Šç›¸å…³æ–¹ã€äº‹ä»¶å’Œæ—¶é—´ï¼Œç”¨äºæ—¶é—´çº¿æ‘˜è¦ï¼ˆTLSï¼‰ä»»åŠ¡ã€‚æˆ‘ä»¬åˆ©ç”¨å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„å»ºSETä¸‰å…ƒç»„ï¼Œå¹¶å¼•å…¥åŸºäºåˆ©ç›Šç›¸å…³æ–¹çš„æ’åæ¥æ„å»º$Relevancy$æŒ‡æ ‡ï¼Œè¯¥æŒ‡æ ‡å¯æ‰©å±•åˆ°ä¸€èˆ¬æƒ…å†µã€‚æˆ‘ä»¬çš„å®éªŒç»“æœä¼˜äºæ‰€æœ‰å…ˆå‰åŸºçº¿ï¼Œæˆä¸ºæ–°çš„æœ€å…ˆè¿›çš„æˆæœï¼Œçªæ˜¾æ–°é—»æ–‡ç« ä¸­åˆ©ç›Šç›¸å…³æ–¹ä¿¡æ¯çš„å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21903v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–°é—»æŠ¥é“æ—¥ç›Šå…¨çƒåŒ–å’Œåœ¨çº¿åˆ†æ•£åŒ–ï¼Œè¿½è¸ªè·¨å¤šä¸ªæ¥æºçš„ç›¸å…³äº‹ä»¶å¸¦æ¥é‡å¤§æŒ‘æˆ˜ã€‚ç°æœ‰æ–°é—»æ‘˜è¦æ–¹æ³•ä¸»è¦åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œå›¾å½¢æ–¹æ³•åŸºäºæ–‡ç« çš„æ‘˜è¦ï¼Œä½†ä»…è€ƒè™‘æ–‡æœ¬å†…å®¹æ— æ³•å…¨é¢ç†è§£äº‹ä»¶ã€‚ä¸ºåº”å¯¹å¯¹å‚ä¸æ–¹åˆ†æä¸è¶³çš„éš¾é¢˜ï¼Œæå‡ºSUnSETæ¡†æ¶ï¼Œé€šè¿‡ç›¸å…³å®ä½“è¯„ä¼°å‚ä¸æ–¹é‡è¦æ€§åŠäº‹ä»¶å…³è”ã€‚åˆ©ç”¨å¼ºå¤§çš„å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºSETä¸‰å…ƒç»„ï¼Œå¹¶å¼•å…¥åŸºäºå‚ä¸æ–¹çš„æ’åæ„å»ºç›¸å…³æ€§æŒ‡æ ‡ï¼Œå¯æ‰©å±•åˆ°ä¸€èˆ¬æƒ…å†µã€‚å®éªŒç»“æœä¼˜äºæ‰€æœ‰å…ˆå‰åŸºçº¿ï¼Œæˆä¸ºæ–°çš„æŠ€æœ¯å‰æ²¿ï¼Œçªæ˜¾å‚ä¸æ–¹ä¿¡æ¯åœ¨æ–°é—»æŠ¥é“ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°é—»æŠ¥é“çš„å…¨çƒæ€§åŒ–å’Œåœ¨çº¿åˆ†æ•£åŒ–ä½¿å¾—è¿½è¸ªè·¨å¤šä¸ªæ¥æºçš„ç›¸å…³äº‹ä»¶å…·æœ‰æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–°é—»æ‘˜è¦æ–¹æ³•ä¸»è¦å…³æ³¨æ–‡æœ¬å†…å®¹ï¼Œä½†æ— æ³•å…¨é¢ç†è§£äº‹ä»¶ã€‚</li>
<li>æå‡ºSUnSETæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡è¯„ä¼°å‚ä¸æ–¹çš„é‡è¦æ€§åŠç›¸å…³å®ä½“çš„è¿æ¥æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºSETä¸‰å…ƒç»„ä»¥å¢å¼ºç†è§£ã€‚</li>
<li>å¼•å…¥åŸºäºå‚ä¸æ–¹çš„æ’åæ¥æ„å»ºç›¸å…³æ€§æŒ‡æ ‡ï¼Œé€‚ç”¨äºå„ç§æƒ…å¢ƒã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶ä¼˜äºå…ˆå‰æŠ€æœ¯ï¼Œæˆä¸ºæ–°çš„æŠ€æœ¯å‰æ²¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eea64dfdf3e4ea781609cbaab0b335d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fd9ec6be48398320e874e4f0dc6abed9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-594efc785489378683070cbfe42030be.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1177b69f079f47c9dc74144aa5133043.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a7c756d43bfb33fed4ae3175bb118bf9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e5cf7e2b99e39f7269ac534f08191f1.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Understanding-Public-Perception-of-Crime-in-Bangladesh-A-Transformer-Based-Approach-with-Explainability"><a href="#Understanding-Public-Perception-of-Crime-in-Bangladesh-A-Transformer-Based-Approach-with-Explainability" class="headerlink" title="Understanding Public Perception of Crime in Bangladesh: A   Transformer-Based Approach with Explainability"></a>Understanding Public Perception of Crime in Bangladesh: A   Transformer-Based Approach with Explainability</h2><p><strong>Authors:Fatema Binte Hassan, Md Al Jubair, Mohammad Mehadi Hasan, Tahmid Hossain, S M Mehebubur Rahman Khan Shuvo, Mohammad Shamsul Arefin</strong></p>
<p>In recent years, social media platforms have become prominent spaces for individuals to express their opinions on ongoing events, including criminal incidents. As a result, public sentiment can shift dynamically over time. This study investigates the evolving public perception of crime-related news by classifying user-generated comments into three categories: positive, negative, and neutral. A newly curated dataset comprising 28,528 Bangla-language social media comments was developed for this purpose. We propose a transformer-based model utilizing the XLM-RoBERTa Base architecture, which achieves a classification accuracy of 97%, outperforming existing state-of-the-art methods in Bangla sentiment analysis. To enhance model interpretability, explainable AI technique is employed to identify the most influential features driving sentiment classification. The results underscore the effectiveness of transformer-based models in processing low-resource languages such as Bengali and demonstrate their potential to extract actionable insights that can support public policy formulation and crime prevention strategies. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œç¤¾äº¤åª’ä½“å¹³å°å·²æˆä¸ºä¸ªäººè¡¨è¾¾å¯¹æ­£åœ¨å‘ç”Ÿçš„äº‹ä»¶ï¼ŒåŒ…æ‹¬åˆ‘äº‹äº‹ä»¶çœ‹æ³•çš„é‡è¦ç©ºé—´ã€‚å› æ­¤ï¼Œå…¬ä¼—æƒ…ç»ªä¼šéšæ—¶é—´åŠ¨æ€å˜åŒ–ã€‚æœ¬ç ”ç©¶é€šè¿‡å°†ç”¨æˆ·ç”Ÿæˆçš„è¯„è®ºåˆ†ä¸ºç§¯æã€æ¶ˆæå’Œä¸­æ€§ä¸‰ä¸ªç±»åˆ«ï¼Œè°ƒæŸ¥äº†ä¸çŠ¯ç½ªç›¸å…³çš„æ–°é—»çš„å…¬ä¼—è®¤çŸ¥çš„æ¼”å˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ–°æ•´ç†çš„æ•°æ®é›†ï¼ŒåŒ…å«28528æ¡å­ŸåŠ æ‹‰è¯­ç¤¾äº¤åª’ä½“è¯„è®ºã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºtransformerçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨XLM-RoBERTa Baseæ¶æ„ï¼Œå®ç°äº†97%çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œä¼˜äºå­ŸåŠ æ‹‰æƒ…æ„Ÿåˆ†æä¸­ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¯è§£é‡Šçš„AIæŠ€æœ¯æ¥è¯†åˆ«é©±åŠ¨æƒ…æ„Ÿåˆ†ç±»çš„æœ€å…·å½±å“åŠ›çš„ç‰¹å¾ã€‚ç»“æœå¼ºè°ƒäº†åŸºäºtransformerçš„æ¨¡å‹åœ¨å¤„ç†ä½èµ„æºè¯­è¨€ï¼ˆå¦‚å­ŸåŠ æ‹‰è¯­ï¼‰æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶æå–å¯æ“ä½œè§è§£çš„æ½œåŠ›ï¼Œè¿™äº›è§è§£å¯ä»¥æ”¯æŒå…¬å…±æ”¿ç­–åˆ¶å®šå’ŒçŠ¯ç½ªé¢„é˜²ç­–ç•¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21234v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    ç¤¾äº¤åª’ä½“å¹³å°æˆä¸ºå…¬ä¼—è¡¨è¾¾çŠ¯ç½ªäº‹ä»¶ç›¸å…³æ„è§çš„é‡è¦åœºæ‰€ï¼Œå…¬ä¼—æƒ…ç»ªéšæ—¶é—´åŠ¨æ€å˜åŒ–ã€‚æœ¬ç ”ç©¶æ—¨åœ¨æ¢ç©¶çŠ¯ç½ªç›¸å…³æ–°é—»çš„å…¬ä¼—æ„ŸçŸ¥å˜åŒ–ï¼Œå°†ç”¨æˆ·ç”Ÿæˆçš„è¯„è®ºåˆ†ä¸ºç§¯æã€æ¶ˆæå’Œä¸­æ€§ä¸‰ç±»ã€‚ä¸ºæ­¤å¼€å‘äº†ä¸€ä¸ªåŒ…å«28,528æ¡å­ŸåŠ æ‹‰è¯­ç¤¾äº¤åª’ä½“è¯„è®ºçš„æ–°æ•°æ®é›†ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºXLM-RoBERTaåŸºç¡€çš„æ¶æ„çš„è½¬æ¢æ¨¡å‹ï¼Œå…¶åˆ†ç±»å‡†ç¡®åº¦è¾¾åˆ°äº†97%ï¼Œåœ¨å­ŸåŠ æ‹‰æƒ…æ„Ÿåˆ†æä¸­ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯æ–¹æ³•ã€‚ä¸ºæé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ï¼Œé‡‡ç”¨äº†å¯è§£é‡Šçš„AIæŠ€æœ¯æ¥ç¡®å®šå½±å“æƒ…æ„Ÿåˆ†ç±»çš„æœ€å…·å½±å“åŠ›çš„ç‰¹å¾ã€‚ç»“æœå¼ºè°ƒäº†åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹åœ¨å¤„ç†ä½èµ„æºè¯­è¨€å¦‚å­ŸåŠ æ‹‰è¯­æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å®ƒä»¬æå–æ”¯æŒå…¬å…±æ”¿ç­–åˆ¶å®šå’ŒçŠ¯ç½ªé¢„é˜²ç­–ç•¥çš„è¡ŒåŠ¨æ€§è§è§£çš„æ½œåŠ›ã€‚</p>
<p><strong>è¦ç‚¹åˆ†æ</strong></p>
<ol>
<li>ç¤¾äº¤åª’ä½“å¹³å°æˆä¸ºå…¬ä¼—è¡¨è¾¾å¯¹çŠ¯ç½ªäº‹ä»¶æ„è§çš„é‡è¦åœºæ‰€ï¼Œå…¬ä¼—æƒ…ç»ªä¼šéšæ—¶é—´åŠ¨æ€å˜åŒ–ã€‚</li>
<li>ç ”ç©¶é€šè¿‡åˆ†ç±»ç”¨æˆ·ç”Ÿæˆçš„è¯„è®ºæ¥æ¢ç©¶å…¬ä¼—å¯¹çŠ¯ç½ªç›¸å…³æ–°é—»çš„çœ‹æ³•ã€‚</li>
<li>ä¸ºæ­¤ç ”ç©¶ç›®çš„å¼€å‘äº†ä¸€ä¸ªåŒ…å«å¤§é‡å­ŸåŠ æ‹‰è¯­ç¤¾äº¤åª’ä½“è¯„è®ºçš„æ–°æ•°æ®é›†ã€‚</li>
<li>é‡‡ç”¨åŸºäºXLM-RoBERTaåŸºç¡€çš„æ¶æ„çš„è½¬æ¢æ¨¡å‹ï¼Œå®ç°äº†é«˜è¾¾97%çš„åˆ†ç±»å‡†ç¡®åº¦ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å­ŸåŠ æ‹‰æƒ…æ„Ÿåˆ†æä¸­è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>é‡‡ç”¨å¯è§£é‡Šçš„AIæŠ€æœ¯ç¡®å®šå½±å“æƒ…æ„Ÿåˆ†ç±»çš„æœ€å…³é”®ç‰¹å¾ï¼Œä»¥æé«˜æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚</li>
<li>ç»“æœå¼ºè°ƒäº†åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹åœ¨å¤„ç†ä½èµ„æºè¯­è¨€çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨å…¬å…±æ”¿ç­–åˆ¶å®šå’ŒçŠ¯ç½ªé¢„é˜²ç­–ç•¥æ–¹é¢çš„åº”ç”¨æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21234">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ff64aa5259eba218e21d1a5efdb3190b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fb0a51360e1e64e44434b5fe66f6f9b5.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e47639579aac6050789343275dd53bfa.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-023de2072f4ba2fc82ae446f521f030d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5889ccd5f2c982c0c8ff63148ea26783.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6544d584c61822d8436a6d1feef477dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b286018001700a67a2364774cee3db10.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aec299c9105c1951837693714ac70ade.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="A-ChatGPT-based-approach-for-questions-generation-in-higher-education"><a href="#A-ChatGPT-based-approach-for-questions-generation-in-higher-education" class="headerlink" title="A ChatGPT-based approach for questions generation in higher education"></a>A ChatGPT-based approach for questions generation in higher education</h2><p><strong>Authors:Sinh Trong Vu, Huong Thu Truong, Oanh Tien Do, Tu Anh Le, Tai Tan Mai</strong></p>
<p>Large language models have been widely applied in many aspects of real life, bringing significant efficiency to businesses and offering distinctive user experiences. In this paper, we focus on exploring the application of ChatGPT, a chatbot based on a large language model, to support higher educator in generating quiz questions and assessing learners. Specifically, we explore interactive prompting patterns to design an optimal AI-powered question bank creation process. The generated questions are evaluated through a â€œBlind testâ€ survey sent to various stakeholders including lecturers and learners. Initial results at the Banking Academy of Vietnam are relatively promising, suggesting a potential direction to streamline the time and effort involved in assessing learners at higher education institutes. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹å·²å¹¿æ³›åº”ç”¨äºç°å®ç”Ÿæ´»çš„å„ä¸ªæ–¹é¢ï¼Œä¸ºä¼ä¸šå¸¦æ¥äº†æ˜¾è‘—æ•ˆç‡ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›äº†ç‹¬ç‰¹ä½“éªŒã€‚æœ¬æ–‡é‡ç‚¹æ¢ç´¢åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„èŠå¤©æœºå™¨äººChatGPTåœ¨æ”¯æŒé«˜ç­‰æ•™è‚²ç”Ÿæˆé¢˜å’Œè¯„ä¼°å­¦ä¹ è€…æ–¹é¢çš„åº”ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ¢ç´¢äº¤äº’å¼æç¤ºæ¨¡å¼ï¼Œä»¥è®¾è®¡æœ€ä½³çš„AIæ”¯æŒçš„é¢˜åº“åˆ›å»ºè¿‡ç¨‹ã€‚é€šè¿‡å‘åŒ…æ‹¬è®²å¸ˆå’Œå­¦ä¹ è€…åœ¨å†…çš„å„ç§åˆ©ç›Šç›¸å…³è€…å‘é€â€œç›²æµ‹è¯•å·â€æ¥è¯„ä¼°ç”Ÿæˆçš„é—®é¢˜ã€‚åœ¨è¶Šå—é“¶è¡Œå­¦é™¢çš„åˆæ­¥ç»“æœç›¸å½“ä»¤äººé¼“èˆï¼Œä¸ºé«˜ç­‰æ•™è‚²æœºæ„åœ¨è¯„ä¼°å­¦ä¹ è€…æ–¹é¢èŠ‚çœæ—¶é—´å’ŒåŠªåŠ›æä¾›äº†æ½œåœ¨æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21174v2">PDF</a> Proceedings of the 1st ACM Workshop on AI-Powered Q&amp;A Systems for   Multimedia. 2024</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°å®ç”Ÿæ´»å¤šä¸ªé¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œä¸ºæå‡ä¸šåŠ¡æ•ˆç‡å’Œæä¾›ç‹¬ç‰¹ç”¨æˆ·ä½“éªŒèµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚æœ¬æ–‡å…³æ³¨åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„èŠå¤©æœºå™¨äººChatGPTåœ¨é«˜ç­‰æ•™è‚²ä¸­çš„åº”ç”¨ï¼Œæ”¯æŒæ•™å¸ˆç”Ÿæˆæµ‹éªŒé—®é¢˜å¹¶è¯„ä¼°å­¦ä¹ è€…ã€‚é€šè¿‡æ¢ç´¢äº¤äº’å¼æç¤ºæ¨¡å¼ï¼Œè®¾è®¡æœ€ä¼˜çš„AIåŠ©åŠ›é—®é¢˜åº“åˆ›å»ºæµç¨‹ã€‚ç”Ÿæˆçš„æµ‹éªŒé—®é¢˜é€šè¿‡å‘é€ç»™è®²å¸ˆå’Œå­¦ä¹ è€…çš„â€œç›²æµ‹â€è°ƒæŸ¥è¿›è¡Œè¯„ä¼°ã€‚è¶Šå—é“¶è¡Œå­¦é™¢çš„åˆæ­¥ç»“æœä»¤äººé¼“èˆï¼Œæ˜¾ç¤ºäº†ä¸€ä¸ªæ½œåœ¨çš„ã€å¸®åŠ©é«˜æ ¡ç®€åŒ–è¯„ä¼°å­¦ä¹ è€…è€—è´¹çš„æ—¶é—´å’Œç²¾åŠ›çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç°å®ç”Ÿæ´»ä¸­çš„åº”ç”¨å¹¿æ³›ï¼Œä¸ºä¸šåŠ¡æ•ˆç‡å’Œç”¨æˆ·ä½“éªŒå¸¦æ¥ç§¯æå½±å“ã€‚</li>
<li>ChatGPTä½œä¸ºä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„èŠå¤©æœºå™¨äººï¼Œè¢«åº”ç”¨äºæ”¯æŒé«˜ç­‰æ•™è‚²ä¸­çš„æµ‹éªŒé—®é¢˜ç”Ÿæˆå’Œè¯„ä¼°ã€‚</li>
<li>é€šè¿‡äº¤äº’å¼æç¤ºæ¨¡å¼æ¢ç´¢ï¼Œè®¾è®¡äº†ä¸€ä¸ªæœ€ä¼˜çš„AIåŠ©åŠ›é—®é¢˜åº“åˆ›å»ºæµç¨‹ã€‚</li>
<li>ç”Ÿæˆçš„æµ‹éªŒé—®é¢˜é€šè¿‡â€œç›²æµ‹â€è°ƒæŸ¥è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>åˆæ­¥åœ¨è¶Šå—é“¶è¡Œå­¦é™¢çš„åº”ç”¨ç»“æœå…·æœ‰é¼“èˆæ€§ã€‚</li>
<li>ChatGPTæœ‰æ½œåŠ›å¸®åŠ©é«˜æ ¡ç®€åŒ–è¯„ä¼°å­¦ä¹ è€…çš„è¿‡ç¨‹ï¼Œå‡å°‘æ—¶é—´å’Œç²¾åŠ›çš„æ¶ˆè€—ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8669c8d2bfaa40e57cde8dd55f79f038.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e180db5c6f7b4e4f423951a099651323.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-836a661c286d14c2339f3696111ad45f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7dcba4a9144f545d8b9f3c0d128c9439.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc85a955ce8b635999a7239aa7540837.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5787ef4f3976484fcf4b0129738e64b4.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Predicting-Cognition-from-fMRI-A-Comparative-Study-of-Graph-Transformer-and-Kernel-Models-Across-Task-and-Rest-Conditions"><a href="#Predicting-Cognition-from-fMRI-A-Comparative-Study-of-Graph-Transformer-and-Kernel-Models-Across-Task-and-Rest-Conditions" class="headerlink" title="Predicting Cognition from fMRI:A Comparative Study of Graph,   Transformer, and Kernel Models Across Task and Rest Conditions"></a>Predicting Cognition from fMRI:A Comparative Study of Graph,   Transformer, and Kernel Models Across Task and Rest Conditions</h2><p><strong>Authors:Jagruti Patel, Mikkel SchÃ¶ttner, Thomas A. W. Bolton, Patric Hagmann</strong></p>
<p>Predicting cognition from neuroimaging data in healthy individuals offers insights into the neural mechanisms underlying cognitive abilities, with potential applications in precision medicine and early detection of neurological and psychiatric conditions. This study systematically benchmarked classical machine learning (Kernel Ridge Regression (KRR)) and advanced deep learning (DL) models (Graph Neural Networks (GNN) and Transformer-GNN (TGNN)) for cognitive prediction using Resting-state (RS), Working Memory, and Language task fMRI data from the Human Connectome Project Young Adult dataset.   Our results, based on R2 scores, Pearson correlation coefficient, and mean absolute error, revealed that task-based fMRI, eliciting neural responses directly tied to cognition, outperformed RS fMRI in predicting cognitive behavior. Among the methods compared, a GNN combining structural connectivity (SC) and functional connectivity (FC) consistently achieved the highest performance across all fMRI modalities; however, its advantage over KRR using FC alone was not statistically significant. The TGNN, designed to model temporal dynamics with SC as a prior, performed competitively with FC-based approaches for task-fMRI but struggled with RS data, where its performance aligned with the lower-performing GNN that directly used fMRI time-series data as node features. These findings emphasize the importance of selecting appropriate model architectures and feature representations to fully leverage the spatial and temporal richness of neuroimaging data.   This study highlights the potential of multimodal graph-aware DL models to combine SC and FC for cognitive prediction, as well as the promise of Transformer-based approaches for capturing temporal dynamics. By providing a comprehensive comparison of models, this work serves as a guide for advancing brain-behavior modeling using fMRI, SC and DL. </p>
<blockquote>
<p>é¢„æµ‹å¥åº·äººç¾¤ä¸­çš„ç¥ç»å½±åƒæ•°æ®è®¤çŸ¥èƒ½ä¸ºè®¤çŸ¥èƒ½åŠ›çš„ç¥ç»æœºåˆ¶æä¾›è§è§£ï¼Œåœ¨ç²¾å‡†åŒ»ç–—å’Œç¥ç»ç²¾ç¥ç–¾ç—…çš„æ—©æœŸæ£€æµ‹ä¸­å…·æœ‰æ½œåœ¨åº”ç”¨ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°åŸºå‡†æµ‹è¯•äº†ç»å…¸æœºå™¨å­¦ä¹ ï¼ˆæ ¸å²­å›å½’ï¼ˆKRRï¼‰ï¼‰å’Œå…ˆè¿›çš„æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹ï¼ˆå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰å’ŒTransformer-GNNï¼ˆTGNNï¼‰ï¼‰è¿›è¡Œè®¤çŸ¥é¢„æµ‹ï¼Œä½¿ç”¨äº†äººç±»è¿æ¥ç»„é¡¹ç›®é’å¹´æ•°æ®é›†ä¸­çš„é™æ¯æ€ï¼ˆRSï¼‰ã€å·¥ä½œè®°å¿†å’Œè¯­è¨€ä»»åŠ¡fMRIæ•°æ®ã€‚æˆ‘ä»¬çš„ç»“æœåŸºäºR2åˆ†æ•°ã€çš®å°”é€Šç›¸å…³ç³»æ•°å’Œå¹³å‡ç»å¯¹è¯¯å·®ï¼Œæ˜¾ç¤ºåŸºäºä»»åŠ¡çš„fMRIï¼Œæ¿€å‘ä¸è®¤çŸ¥ç›´æ¥ç›¸å…³çš„ç¥ç»ååº”ï¼Œåœ¨é¢„æµ‹è®¤çŸ¥è¡Œä¸ºæ–¹é¢ä¼˜äºé™æ¯æ€fMRIã€‚åœ¨æ¯”è¾ƒçš„æ–¹æ³•ä¸­ï¼Œç»“åˆç»“æ„è¿é€šæ€§ï¼ˆSCï¼‰å’ŒåŠŸèƒ½è¿é€šæ€§ï¼ˆFCï¼‰çš„GNNåœ¨æ‰€æœ‰fMRIæ¨¡å¼ä¸­éƒ½å®ç°äº†æœ€ä½³æ€§èƒ½ï¼›ç„¶è€Œï¼Œå…¶ä¸ä»…ä½¿ç”¨FCçš„KRRç›¸æ¯”çš„ä¼˜åŠ¿å¹¶æœªè¾¾åˆ°ç»Ÿè®¡æ˜¾è‘—æ°´å¹³ã€‚TGNNæ—¨åœ¨ç”¨SCä½œä¸ºå…ˆéªŒæ¥å»ºæ¨¡æ—¶é—´åŠ¨æ€ï¼Œå¯¹äºä»»åŠ¡fMRIæ¥è¯´ï¼Œå…¶è¡¨ç°ä¸åŸºäºFCçš„æ–¹æ³•ç›¸å½“ï¼Œä½†åœ¨å¤„ç†RSæ•°æ®æ—¶é‡åˆ°å›°éš¾ï¼Œå…¶æ€§èƒ½ä¸ç›´æ¥ä½¿ç”¨fMRIæ—¶é—´åºåˆ—æ•°æ®ä½œä¸ºèŠ‚ç‚¹ç‰¹å¾çš„è¾ƒä½æ€§èƒ½GNNç›¸ç¬¦ã€‚è¿™äº›å‘ç°å¼ºè°ƒé€‰æ‹©é€‚å½“çš„æ¨¡å‹æ¶æ„å’Œç‰¹å¾è¡¨ç¤ºçš„é‡è¦æ€§ï¼Œä»¥å……åˆ†åˆ©ç”¨ç¥ç»æˆåƒæ•°æ®çš„ç©ºé—´å’Œæ—¶é—´ä¸°å¯Œæ€§ã€‚æœ¬ç ”ç©¶çªå‡ºäº†å¤šæ¨¡æ€å›¾æ„ŸçŸ¥æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç»“åˆSCå’ŒFCè¿›è¡Œè®¤çŸ¥é¢„æµ‹æ–¹é¢çš„æ½œåŠ›ï¼Œä»¥åŠåŸºäºTransformerçš„æ–¹æ³•åœ¨æ•æ‰æ—¶é—´åŠ¨æ€æ–¹é¢çš„å‰æ™¯ã€‚é€šè¿‡å…¨é¢æ¯”è¾ƒå„ç§æ¨¡å‹ï¼Œè¿™é¡¹å·¥ä½œä¸ºåˆ©ç”¨fMRIã€SCå’ŒDLæ¨è¿›è„‘è¡Œä¸ºå»ºæ¨¡æä¾›äº†æŒ‡å—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21016v1">PDF</a> Preliminary version; a revised version will be uploaded later</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹å¥åº·ä¸ªä½“çš„ç¥ç»æˆåƒæ•°æ®è¿›è¡Œè®¤çŸ¥é¢„æµ‹çš„ç ”ç©¶ï¼Œæ­ç¤ºäº†ç¥ç»æœºåˆ¶ä¸è®¤çŸ¥èƒ½åŠ›ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶æœ‰æœ›åº”ç”¨äºç²¾å‡†åŒ»ç–—åŠç¥ç»å’Œç²¾ç¥ç–¾ç—…çš„æ—©æœŸæ£€æµ‹ã€‚æœ¬ç ”ç©¶å¯¹æ¯”äº†ç»å…¸æœºå™¨å­¦ä¹ ï¼ˆå¦‚æ ¸å²­å›å½’ï¼‰å’Œå…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚å›¾ç¥ç»ç½‘ç»œå’ŒTransformer-GNNï¼‰åœ¨åŸºäºé™æ¯æ€ã€å·¥ä½œè®°å¿†å’Œè¯­è¨€ä»»åŠ¡çš„fMRIæ•°æ®ä¸Šçš„è®¤çŸ¥é¢„æµ‹æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ä»»åŠ¡æ€fMRIåœ¨é¢„æµ‹è®¤çŸ¥è¡Œä¸ºæ–¹é¢ä¼˜äºé™æ¯æ€fMRIã€‚æ­¤å¤–ï¼Œç»“åˆç»“æ„è¿æ¥ï¼ˆSCï¼‰å’ŒåŠŸèƒ½è¿æ¥ï¼ˆFCï¼‰çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰åœ¨æ‰€æœ‰fMRIæ¨¡æ€ä¸Šçš„è¡¨ç°æœ€ä½³ï¼Œä½†å…¶ç›¸å¯¹äºä»…ä½¿ç”¨åŠŸèƒ½è¿æ¥çš„KRRæ–¹æ³•çš„ä¼˜åŠ¿å¹¶æœªè¾¾åˆ°ç»Ÿè®¡æ˜¾è‘—æ°´å¹³ã€‚ç ”ç©¶å¼ºè°ƒäº†é€‰æ‹©é€‚å½“æ¨¡å‹æ¶æ„å’Œç‰¹å¾è¡¨ç¤ºçš„é‡è¦æ€§ï¼Œä»¥ä¾¿å……åˆ†åˆ©ç”¨ç¥ç»æˆåƒæ•°æ®çš„ç©ºé—´å’Œæ—¶é—´ä¸°å¯Œæ€§ã€‚è¿™é¡¹ç ”ç©¶çªå‡ºäº†å¤šæ¨¡æ€å›¾æ„ŸçŸ¥æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ç»“åˆç»“æ„è¿æ¥å’ŒåŠŸèƒ½è¿æ¥è¿›è¡Œè®¤çŸ¥é¢„æµ‹æ–¹é¢çš„æ½œåŠ›ï¼Œä»¥åŠåŸºäºTransformerçš„æ–¹æ³•åœ¨æ•æ‰æ—¶é—´åŠ¨æ€æ–¹é¢çš„å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶åˆ©ç”¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ä»ç¥ç»æˆåƒæ•°æ®ä¸­é¢„æµ‹è®¤çŸ¥ï¼Œä¸ºç²¾å‡†åŒ»ç–—å’Œç¥ç»ç²¾ç¥ç–¾ç—…æ—©æœŸæ£€æµ‹æä¾›çº¿ç´¢ã€‚</li>
<li>ä»»åŠ¡æ€fMRIåœ¨é¢„æµ‹è®¤çŸ¥è¡Œä¸ºæ–¹é¢ä¼˜äºé™æ¯æ€fMRIã€‚</li>
<li>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ç»“åˆç»“æ„è¿æ¥ï¼ˆSCï¼‰å’ŒåŠŸèƒ½è¿æ¥ï¼ˆFCï¼‰åœ¨æ‰€æœ‰fMRIæ¨¡æ€ä¸Šçš„è¡¨ç°æœ€ä½³ã€‚</li>
<li>Transformer-GNNåœ¨å¤„ç†åŒ…å«æ—¶é—´åŠ¨æ€ä¿¡æ¯çš„æ•°æ®æ—¶è¡¨ç°è‰¯å¥½ã€‚</li>
<li>é€‰æ‹©é€‚å½“çš„æ¨¡å‹æ¶æ„å’Œç‰¹å¾è¡¨ç¤ºå¯¹æœ‰æ•ˆåˆ©ç”¨ç¥ç»æˆåƒæ•°æ®è‡³å…³é‡è¦ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è®¤çŸ¥é¢„æµ‹æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨ç»“åˆç»“æ„è¿æ¥å’ŒåŠŸèƒ½è¿æ¥æ–¹é¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21016">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac83f59ff984305c2d036d3484c0a296.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="TADT-CSA-Temporal-Advantage-Decision-Transformer-with-Contrastive-State-Abstraction-for-Generative-Recommendation"><a href="#TADT-CSA-Temporal-Advantage-Decision-Transformer-with-Contrastive-State-Abstraction-for-Generative-Recommendation" class="headerlink" title="TADT-CSA: Temporal Advantage Decision Transformer with Contrastive State   Abstraction for Generative Recommendation"></a>TADT-CSA: Temporal Advantage Decision Transformer with Contrastive State   Abstraction for Generative Recommendation</h2><p><strong>Authors:Xiang Gao, Tianyuan Liu, Yisha Li, Jingxin Liu, Lexi Gao, Xin Li, Haiyang Lu, Liyin Hong</strong></p>
<p>With the rapid advancement of Transformer-based Large Language Models (LLMs), generative recommendation has shown great potential in enhancing both the accuracy and semantic understanding of modern recommender systems. Compared to LLMs, the Decision Transformer (DT) is a lightweight generative model applied to sequential recommendation tasks. However, DT faces challenges in trajectory stitching, often producing suboptimal trajectories. Moreover, due to the high dimensionality of user states and the vast state space inherent in recommendation scenarios, DT can incur significant computational costs and struggle to learn effective state representations. To overcome these issues, we propose a novel Temporal Advantage Decision Transformer with Contrastive State Abstraction (TADT-CSA) model. Specifically, we combine the conventional Return-To-Go (RTG) signal with a novel temporal advantage (TA) signal that encourages the model to capture both long-term returns and their sequential trend. Furthermore, we integrate a contrastive state abstraction module into the DT framework to learn more effective and expressive state representations. Within this module, we introduce a TA-conditioned State Vector Quantization (TAC-SVQ) strategy, where the TA score guides the state codebooks to incorporate contextual token information. Additionally, a reward prediction network and a contrastive transition prediction (CTP) network are employed to ensure the state codebook preserves both the reward information of the current state and the transition information between adjacent states. Empirical results on both public datasets and an online recommendation system demonstrate the effectiveness of the TADT-CSA model and its superiority over baseline methods. </p>
<blockquote>
<p>éšç€åŸºäºTransformerçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç”Ÿæˆå¼æ¨èåœ¨æé«˜ç°ä»£æ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œè¯­ä¹‰ç†è§£æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ä¸LLMç›¸æ¯”ï¼Œå†³ç­–å˜å‹å™¨ï¼ˆDTï¼‰æ˜¯ä¸€ä¸ªç”¨äºåºåˆ—æ¨èä»»åŠ¡çš„è½»é‡çº§ç”Ÿæˆæ¨¡å‹ã€‚ç„¶è€Œï¼ŒDTåœ¨è½¨è¿¹æ‹¼æ¥æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œç»å¸¸äº§ç”Ÿæ¬¡ä¼˜è½¨è¿¹ã€‚æ­¤å¤–ï¼Œç”±äºç”¨æˆ·çŠ¶æ€çš„é«˜ç»´æ€§å’Œæ¨èåœºæ™¯ä¸­çš„å›ºæœ‰å¤§è§„æ¨¡çŠ¶æ€ç©ºé—´ï¼ŒDTå¯èƒ½ä¼šäº§ç”Ÿæ˜¾è‘—çš„è®¡ç®—æˆæœ¬ï¼Œå¹¶ä¸”éš¾ä»¥å­¦ä¹ æœ‰æ•ˆçš„çŠ¶æ€è¡¨ç¤ºã€‚ä¸ºäº†å…‹æœè¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°å‹çš„æ—¶é—´ä¼˜åŠ¿å†³ç­–å˜å‹å™¨ä¸å¯¹æ¯”çŠ¶æ€æŠ½è±¡ï¼ˆTADT-CSAï¼‰æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ä¼ ç»Ÿçš„è¿”å›ç›®æ ‡ï¼ˆRTGï¼‰ä¿¡å·ä¸æ–°å‹çš„æ—¶é—´ä¼˜åŠ¿ï¼ˆTAï¼‰ä¿¡å·ç›¸ç»“åˆï¼Œé¼“åŠ±æ¨¡å‹æ•æ‰é•¿æœŸå›æŠ¥åŠå…¶åºåˆ—è¶‹åŠ¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å¯¹æ¯”çŠ¶æ€æŠ½è±¡æ¨¡å—é›†æˆåˆ°DTæ¡†æ¶ä¸­ï¼Œå­¦ä¹ æ›´æœ‰æ•ˆã€æ›´å…·è¡¨ç°åŠ›çš„çŠ¶æ€è¡¨ç¤ºã€‚åœ¨æ­¤æ¨¡å—ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ—¶é—´ä¼˜åŠ¿æ¡ä»¶çŠ¶æ€å‘é‡é‡åŒ–ï¼ˆTAC-SVQï¼‰ç­–ç•¥ï¼Œå…¶ä¸­æ—¶é—´ä¼˜åŠ¿è¯„åˆ†æŒ‡å¯¼çŠ¶æ€ç æœ¬èå…¥ä¸Šä¸‹æ–‡æ ‡è®°ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜é‡‡ç”¨äº†å¥–åŠ±é¢„æµ‹ç½‘ç»œå’Œå¯¹æ¯”è¿‡æ¸¡é¢„æµ‹ï¼ˆCTPï¼‰ç½‘ç»œï¼Œä»¥ç¡®ä¿çŠ¶æ€ç æœ¬ä¿ç•™å½“å‰çŠ¶æ€çš„å¥–åŠ±ä¿¡æ¯å’Œç›¸é‚»çŠ¶æ€ä¹‹é—´çš„è¿‡æ¸¡ä¿¡æ¯ã€‚åœ¨å…¬å…±æ•°æ®é›†å’Œåœ¨çº¿æ¨èç³»ç»Ÿä¸Šçš„ç»éªŒç»“æœè¯æ˜äº†TADT-CSAæ¨¡å‹çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠå…¶ä¼˜äºåŸºå‡†æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20327v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€TransformeråŸºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œç”Ÿæˆå¼æ¨èåœ¨æé«˜ç°ä»£æ¨èç³»ç»Ÿçš„å‡†ç¡®æ€§å’Œè¯­ä¹‰ç†è§£æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¸LLMç›¸æ¯”ï¼Œç”¨äºåºåˆ—æ¨èä»»åŠ¡çš„å†³ç­–å˜å‹å™¨ï¼ˆDTï¼‰æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ç”Ÿæˆæ¨¡å‹ï¼Œé¢ä¸´è½¨è¿¹æ‹¼æ¥çš„æŒ‘æˆ˜ï¼Œå¸¸äº§ç”Ÿæ¬¡ä¼˜è½¨è¿¹ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç»“åˆä¼ ç»ŸReturn-To-Goä¿¡å·ä¸æ–°å‹æ—¶åºä¼˜åŠ¿ä¿¡å·çš„æ–°æ¨¡å‹TADT-CSAã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†å¯¹æ¯”çŠ¶æ€æŠ½è±¡æ¨¡å—é›†æˆåˆ°DTæ¡†æ¶ä¸­ï¼Œä»¥å­¦ä¹ æ›´æœ‰æ•ˆçš„çŠ¶æ€è¡¨ç¤ºã€‚TAC-SVQç­–ç•¥ç­‰ç­–ç•¥çš„å¼•å…¥ç¡®ä¿çŠ¶æ€ç¼–ç ç°¿èå…¥ä¸Šä¸‹æ–‡æ ‡è®°ä¿¡æ¯ã€‚åœ¨å…¬å…±æ•°æ®é›†å’Œåœ¨çº¿æ¨èç³»ç»Ÿä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒTADT-CSAæ¨¡å‹çš„æœ‰æ•ˆæ€§åŠå…¶å¯¹åŸºå‡†æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TransformeråŸºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç”Ÿæˆå¼æ¨èä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>å†³ç­–å˜å‹å™¨ï¼ˆDTï¼‰æ˜¯åº”ç”¨äºåºåˆ—æ¨èä»»åŠ¡çš„è½»é‡çº§ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>DTé¢ä¸´è½¨è¿¹æ‹¼æ¥çš„æŒ‘æˆ˜ï¼Œå¸¸äº§ç”Ÿæ¬¡ä¼˜è½¨è¿¹ã€‚</li>
<li>TADT-CSAæ¨¡å‹ç»“åˆäº†ä¼ ç»ŸReturn-To-Goä¿¡å·ä¸æ–°å‹æ—¶åºä¼˜åŠ¿ä¿¡å·ã€‚</li>
<li>å¯¹æ¯”çŠ¶æ€æŠ½è±¡æ¨¡å—è¢«é›†æˆåˆ°DTæ¡†æ¶ä¸­ä»¥æé«˜çŠ¶æ€è¡¨ç¤ºçš„æœ‰æ•ˆæ€§ã€‚</li>
<li>TAC-SVQç­–ç•¥ç­‰ç­–ç•¥ç¡®ä¿çŠ¶æ€ç¼–ç ç°¿èå…¥ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7201535e9cc47f827aef553908d0bf5f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8b772d83723980e504d7b28cf17ac21b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eb299ddf65e489397d9bf8f56c05f6dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-722367e7d2a4fe99ea0e9575dd988abd.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="FAEDKV-Infinite-Window-Fourier-Transform-for-Unbiased-KV-Cache-Compression"><a href="#FAEDKV-Infinite-Window-Fourier-Transform-for-Unbiased-KV-Cache-Compression" class="headerlink" title="FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache   Compression"></a>FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache   Compression</h2><p><strong>Authors:Runchao Li, Yao Fu, Mu Sheng, Xianxuan Long, Haotian Yu, Pan Li</strong></p>
<p>The efficacy of Large Language Models (LLMs) in long-context tasks is often hampered by the substantial memory footprint and computational demands of the Key-Value (KV) cache. Current compression strategies, including token eviction and learned projections, frequently lead to biased representations â€“ either by overemphasizing recent&#x2F;high-attention tokens or by repeatedly degrading information from earlier context â€“ and may require costly model retraining. We present FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel, training-free KV cache compression framework that ensures unbiased information retention. FAEDKV operates by transforming the KV cache into the frequency domain using a proposed Infinite-Window Fourier Transform (IWDFT). This approach allows for the equalized contribution of all tokens to the compressed representation, effectively preserving both early and recent contextual information. A preliminary frequency ablation study identifies critical spectral components for layer-wise, targeted compression. Experiments on LongBench benchmark demonstrate FAEDKVâ€™s superiority over existing methods by up to 22%. In addition, our method shows superior, position-agnostic retrieval accuracy on the Needle-In-A-Haystack task compared to compression based approaches. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­çš„æ•ˆèƒ½å¸¸å¸¸å—åˆ°é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜çš„å·¨å¤§å†…å­˜å ç”¨å’Œè®¡ç®—éœ€æ±‚çš„é™åˆ¶ã€‚å½“å‰çš„å‹ç¼©ç­–ç•¥ï¼ŒåŒ…æ‹¬ä»¤ç‰Œé©±é€å’Œå­¦å¾—æŠ•å½±ï¼Œå¸¸å¸¸ä¼šå¯¼è‡´è¡¨ç¤ºåè§â€”â€”è¦ä¹ˆè¿‡åˆ†å¼ºè°ƒè¿‘æœŸ&#x2F;é«˜æ³¨æ„åŠ›ä»¤ç‰Œï¼Œè¦ä¹ˆåå¤é™ä½æ—©æœŸä¸Šä¸‹æ–‡çš„ä¿¡æ¯â€”â€”å¹¶ä¸”å¯èƒ½éœ€è¦æ˜‚è´µçš„æ¨¡å‹é‡æ–°è®­ç»ƒã€‚æˆ‘ä»¬æå‡ºäº†FAEDKVï¼ˆé”®å€¼ç¼“å­˜çš„é¢‘ç‡è‡ªé€‚åº”æ— é™çª—å£ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ã€æ— éœ€è®­ç»ƒçš„é”®å€¼ç¼“å­˜å‹ç¼©æ¡†æ¶ï¼Œå¯ç¡®ä¿ä¿ç•™æ— åè§çš„ä¿¡æ¯ã€‚FAEDKVé€šè¿‡å°†é”®å€¼ç¼“å­˜è½¬æ¢ä¸ºé¢‘ç‡åŸŸæ¥è¿è¡Œï¼Œä½¿ç”¨æ‰€æå‡ºçš„æ— é™çª—å£å‚…é‡Œå¶å˜æ¢ï¼ˆIWDFTï¼‰ã€‚è¿™ç§æ–¹æ³•å…è®¸æ‰€æœ‰ä»¤ç‰Œå¯¹å‹ç¼©è¡¨ç¤ºåšå‡ºå‡ç­‰è´¡çŒ®ï¼Œæœ‰æ•ˆåœ°ä¿ç•™æ—©æœŸå’Œè¿‘æœŸçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åˆæ­¥çš„é¢‘è°±æ¶ˆèç ”ç©¶ç¡®å®šäº†é’ˆå¯¹åˆ†å±‚å‹ç¼©çš„å…³é”®é¢‘è°±åˆ†é‡ã€‚åœ¨LongBenchåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFAEDKVç›¸è¾ƒäºç°æœ‰æ–¹æ³•æœ€å¤šé«˜å‡º22%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Haystackä»»åŠ¡ä¸­çš„needle-in-haystackæµ‹è¯•ä¸Šæ˜¾ç¤ºå‡ºä¼˜äºåŸºäºå‹ç¼©çš„æ–¹æ³•çš„ä½ç½®æ— å…³æ£€ç´¢ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20030v1">PDF</a> </p>
<p><strong>Summary</strong><br>     å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸­çš„æ•ˆç‡å› é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜çš„å·¨å¤§å†…å­˜å ç”¨å’Œè®¡ç®—éœ€æ±‚è€Œå—åˆ°é™åˆ¶ã€‚å½“å‰å‹ç¼©ç­–ç•¥ï¼Œå¦‚ä»¤ç‰Œé©±é€å’Œå­¦å¾—æŠ•å½±ï¼Œç»å¸¸å¯¼è‡´è¡¨å¾åå·®ï¼Œå¯èƒ½éœ€è€—è´¹æˆæœ¬é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬æå‡ºFAEDKVï¼ˆç”¨äºKVç¼“å­˜çš„é¢‘ç‡è‡ªé€‚åº”æ— é™çª—å£ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ã€æ— éœ€è®­ç»ƒã€ç¡®ä¿ä¿¡æ¯æ— åä¿ç•™çš„KVç¼“å­˜å‹ç¼©æ¡†æ¶ã€‚FAEDKVé€šè¿‡å°†KVç¼“å­˜è½¬æ¢ä¸ºé¢‘ç‡åŸŸæ¥æ“ä½œï¼Œåˆ©ç”¨æå‡ºçš„æ— é™çª—å£å‚…é‡Œå¶å˜æ¢ï¼ˆIWDFTï¼‰ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æ‰€æœ‰ä»¤ç‰Œå¯¹å‹ç¼©è¡¨ç¤ºçš„è´¡çŒ®å‡ç­‰ï¼Œæœ‰æ•ˆä¿ç•™æ—©æœŸå’Œè¿‘æœŸçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚åˆæ­¥é¢‘ç‡æ¶ˆèç ”ç©¶è¡¨æ˜äº†é’ˆå¯¹å±‚å†…ç›®æ ‡å‹ç¼©çš„å…³é”®é¢‘è°±æˆåˆ†ã€‚åœ¨é•¿æ¤…åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒFAEDKVç›¸è¾ƒäºç°æœ‰æ–¹æ³•çš„ä¼˜è¶Šæ€§é«˜è¾¾22%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨â€œé’ˆå°–åœ¨ç¨»è‰å †ä¸­â€ä»»åŠ¡ä¸Šçš„ä½ç½®æ— å…³æ£€ç´¢å‡†ç¡®ç‡ä¼˜äºåŸºäºå‹ç¼©çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸­é¢ä¸´KVç¼“å­˜çš„å†…å­˜å’Œè®¡ç®—æ•ˆç‡é—®é¢˜ã€‚</li>
<li>å½“å‰å‹ç¼©ç­–ç•¥å¯èƒ½å¯¼è‡´è¡¨å¾åå·®ï¼Œéœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>æå‡ºçš„FAEDKVæ¡†æ¶æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„KVç¼“å­˜å‹ç¼©æ–¹æ³•ï¼Œç¡®ä¿ä¿¡æ¯æ— åä¿ç•™ã€‚</li>
<li>FAEDKVåˆ©ç”¨æ— é™çª—å£å‚…é‡Œå¶å˜æ¢ï¼ˆIWDFTï¼‰å°†KVç¼“å­˜è½¬æ¢ä¸ºé¢‘ç‡åŸŸã€‚</li>
<li>è¯¥æ–¹æ³•ä½¿å¾—æ‰€æœ‰ä»¤ç‰Œå¯¹å‹ç¼©è¡¨ç¤ºçš„è´¡çŒ®å‡ç­‰ï¼Œä¿ç•™æ—©æœŸå’Œè¿‘æœŸçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>åˆæ­¥é¢‘ç‡æ¶ˆèç ”ç©¶ç¡®å®šäº†é’ˆå¯¹å±‚å†…ç›®æ ‡å‹ç¼©çš„å…³é”®é¢‘è°±æˆåˆ†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20030">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ee1b9439c9080062b2b1f764598e8f43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42d71b249bf8207697f134bcca3e0ffd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6dd6ed6ea3055c9a14efd19bbc3a8bd5.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Flora-Effortless-Context-Construction-to-Arbitrary-Length-and-Scale"><a href="#Flora-Effortless-Context-Construction-to-Arbitrary-Length-and-Scale" class="headerlink" title="Flora: Effortless Context Construction to Arbitrary Length and Scale"></a>Flora: Effortless Context Construction to Arbitrary Length and Scale</h2><p><strong>Authors:Tianxiang Chen, Zhentao Tan, Xiaofan Bo, Yue Wu, Tao Gong, Qi Chu, Jieping Ye, Nenghai Yu</strong></p>
<p>Effectively handling long contexts is challenging for Large Language Models (LLMs) due to the rarity of long texts, high computational demands, and substantial forgetting of short-context abilities. Recent approaches have attempted to construct long contexts for instruction tuning, but these methods often require LLMs or human interventions, which are both costly and limited in length and diversity. Also, the drop in short-context performances of present long-context LLMs remains significant. In this paper, we introduce Flora, an effortless (human&#x2F;LLM-free) long-context construction strategy. Flora can markedly enhance the long-context performance of LLMs by arbitrarily assembling short instructions based on categories and instructing LLMs to generate responses based on long-context meta-instructions. This enables Flora to produce contexts of arbitrary length and scale with rich diversity, while only slightly compromising short-context performance. Experiments on Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three long-context benchmarks while maintaining strong performances in short-context tasks. Our data-construction code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/txchen-USTC/Flora%7D%7Bhttps://github.com/txchen-USTC/Flora%7D">https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}</a>. </p>
<blockquote>
<p>æœ‰æ•ˆåœ°å¤„ç†é•¿è¯­å¢ƒå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºé•¿æ–‡æœ¬ç¨€å°‘ã€è®¡ç®—éœ€æ±‚é«˜ä»¥åŠçŸ­è¯­å¢ƒèƒ½åŠ›çš„æ˜¾è‘—é—å¿˜ã€‚è¿‘æœŸçš„æ–¹æ³•è¯•å›¾æ„å»ºé•¿è¯­å¢ƒè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸éœ€è¦å¤§å‹è¯­è¨€æ¨¡å‹æˆ–äººå·¥å¹²é¢„ï¼ŒäºŒè€…æˆæœ¬é«˜æ˜‚ï¼Œä¸”åœ¨é•¿åº¦å’Œå¤šæ ·æ€§æ–¹é¢å­˜åœ¨å±€é™ã€‚æ­¤å¤–ï¼Œå½“å‰é•¿è¯­å¢ƒå¤§å‹è¯­è¨€æ¨¡å‹çš„çŸ­è¯­å¢ƒæ€§èƒ½ä¸‹é™ä»ç„¶æ˜¾è‘—ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†Floraï¼Œè¿™æ˜¯ä¸€ç§è½»æ¾ï¼ˆæ— éœ€äººå·¥&#x2F;å¤§å‹è¯­è¨€æ¨¡å‹å‚ä¸ï¼‰çš„é•¿è¯­å¢ƒæ„å»ºç­–ç•¥ã€‚Floraå¯ä»¥é€šè¿‡åŸºäºç±»åˆ«ä»»æ„ç»„åˆçŸ­æŒ‡ä»¤ï¼Œå¹¶æŒ‡ä»¤å¤§å‹è¯­è¨€æ¨¡å‹åŸºäºé•¿è¯­å¢ƒå…ƒæŒ‡ä»¤ç”Ÿæˆå“åº”ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹çš„é•¿è¯­å¢ƒæ€§èƒ½ã€‚è¿™ä½¿å¾—Floraèƒ½å¤Ÿäº§ç”Ÿä»»æ„é•¿åº¦å’Œè§„æ¨¡ã€ä¸°å¯Œå¤šæ ·çš„è¯­å¢ƒï¼ŒåŒæ—¶ä»…ç•¥å¾®å½±å“çŸ­è¯­å¢ƒæ€§èƒ½ã€‚åœ¨Llama3-8B-Instructå’ŒQwQ-32Bä¸Šçš„å®éªŒè¡¨æ˜ï¼Œé€šè¿‡Floraå¢å¼ºçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸‰ä¸ªé•¿è¯­å¢ƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶åœ¨çŸ­è¯­å¢ƒä»»åŠ¡ä¸­ä¿æŒå¼ºåŠ²è¡¨ç°ã€‚æˆ‘ä»¬çš„æ•°æ®æ„å»ºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/txchen-USTC/Flora%E5%A4%96%E9%A2%9D%E5%8F%AF%E4%BB%A5%E5%BE%97%E5%88%B0%E3%80%82">https://github.com/txchen-USTC/Floraå¤„è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19786v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMåœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶é¢ä¸´æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬é•¿æ–‡æœ¬ç¨€ç¼ºã€è®¡ç®—éœ€æ±‚é«˜ä»¥åŠçŸ­æ–‡æœ¬èƒ½åŠ›çš„æ˜¾è‘—é—å¿˜ã€‚ç°æœ‰æ„å»ºé•¿æ–‡æœ¬çš„æ–¹æ³•å¸¸éœ€ä¾èµ–LLMæˆ–äººå·¥å¹²é¢„ï¼Œæˆæœ¬é«˜æ˜‚ä¸”é•¿åº¦å’Œå¤šæ ·æ€§å—é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Floraç­–ç•¥ï¼Œæ— éœ€äººå·¥æˆ–LLMä»‹å…¥å³å¯è½»æ¾æ„å»ºé•¿æ–‡æœ¬ã€‚Floraèƒ½é€šè¿‡åˆ†ç±»ä»»æ„ç»„åˆçŸ­æŒ‡ä»¤ï¼Œå¹¶åŸºäºé•¿æ–‡æœ¬å…ƒæŒ‡ä»¤æŒ‡å¯¼LLMç”Ÿæˆå“åº”ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºLLMçš„é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¿è¯çŸ­æ–‡æœ¬æ€§èƒ½ä¸å—å¤ªå¤§å½±å“ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨Floraå¢å¼ºçš„LLMåœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œä¸»è¦ç”±äºé•¿æ–‡æœ¬ç¨€ç¼ºã€è®¡ç®—éœ€æ±‚é«˜å’ŒçŸ­æ–‡æœ¬èƒ½åŠ›é—å¿˜ã€‚</li>
<li>ç°æœ‰æ„å»ºé•¿æ–‡æœ¬çš„æ–¹æ³•æˆæœ¬é«˜æ˜‚ä¸”å—é™ã€‚</li>
<li>Floraç­–ç•¥æ— éœ€äººå·¥æˆ–LLMä»‹å…¥å³å¯è½»æ¾æ„å»ºé•¿æ–‡æœ¬ã€‚</li>
<li>Floraé€šè¿‡ç»„åˆçŸ­æŒ‡ä»¤å’ŒåŸºäºé•¿æ–‡æœ¬å…ƒæŒ‡ä»¤æŒ‡å¯¼LLMç”Ÿæˆå“åº”ï¼Œå¢å¼ºLLMçš„é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨Floraå¢å¼ºçš„LLMåœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>Floraåœ¨ä¿è¯é•¿æ–‡æœ¬æ€§èƒ½çš„åŒæ—¶ï¼Œä¿è¯äº†çŸ­æ–‡æœ¬çš„æ€§èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19786">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-64affc774d9b9572a5f8e15853ef88bf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23a95d3f65210e5dcb2cf8355fb82711.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fcc7526047885bf32e2d600547c39ef5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ab931d5ef0b4478bcc98c1b1e799dd47.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dca80d2fb7bb6c3ed7d3b00088b56d9b.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c42f9535b5c468a5cd11a1e085e7a999.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  Hierarchical Message-Passing Policies for Multi-Agent Reinforcement   Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-4cd61fb10030db1001c05898e30f80de.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  SeqAffordSplat Scene-level Sequential Affordance Reasoning on 3D   Gaussian Splatting
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31686.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
