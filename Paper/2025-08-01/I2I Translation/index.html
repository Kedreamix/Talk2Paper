<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-08-01  Explainable Image Classification with Reduced Overconfidence for Tissue   Characterisation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4ece756ddbcbb9cb90df7645521e10d1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-08-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    80 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-08-01-更新"><a href="#2025-08-01-更新" class="headerlink" title="2025-08-01 更新"></a>2025-08-01 更新</h1><h2 id="Explainable-Image-Classification-with-Reduced-Overconfidence-for-Tissue-Characterisation"><a href="#Explainable-Image-Classification-with-Reduced-Overconfidence-for-Tissue-Characterisation" class="headerlink" title="Explainable Image Classification with Reduced Overconfidence for Tissue   Characterisation"></a>Explainable Image Classification with Reduced Overconfidence for Tissue   Characterisation</h2><p><strong>Authors:Alfie Roddan, Chi Xu, Serine Ajlouni, Irini Kakaletri, Patra Charalampaki, Stamatia Giannarou</strong></p>
<p>The deployment of Machine Learning models intraoperatively for tissue characterisation can assist decision making and guide safe tumour resections. For image classification models, pixel attribution methods are popular to infer explainability. However, overconfidence in deep learning model’s predictions translates to overconfidence in pixel attribution. In this paper, we propose the first approach which incorporates risk estimation into a pixel attribution method for improved image classification explainability. The proposed method iteratively applies a classification model with a pixel attribution method to create a volume of PA maps. This volume is used for the first time, to generate a pixel-wise distribution of PA values. We introduce a method to generate an enhanced PA map by estimating the expectation values of the pixel-wise distributions. In addition, the coefficient of variation (CV) is used to estimate pixel-wise risk of this enhanced PA map. Hence, the proposed method not only provides an improved PA map but also produces an estimation of risk on the output PA values. Performance evaluation on probe-based Confocal Laser Endomicroscopy (pCLE) data and ImageNet verifies that our improved explainability method outperforms the state-of-the-art. </p>
<blockquote>
<p>在手术过程中部署机器学习模型进行组织特征分析，可协助决策并引导安全地进行肿瘤切除手术。对于图像分类模型，像素归因方法是推断解释性的流行方法。然而，对深度学习模型预测过度自信会转化为对像素归因的过度自信。在本文中，我们首次提出了一种方法，将风险估计纳入像素归因方法，以提高图像分类的解释性。所提出的方法迭代地将分类模型与像素归因方法相结合，以创建PA映射卷。该卷被首次用于生成像素级的PA值分布。我们介绍了一种通过估计像素级分布的期望值来生成增强型PA图的方法。此外，还使用变异系数来估计增强型PA图的像素级风险。因此，所提出的方法不仅提供了改进的PA图，而且还在输出PA值上产生了风险估计。基于探针的共焦激光内窥镜（pCLE）数据和ImageNet的性能评估证实，我们改进的解释方法优于当前最新技术。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23709v1">PDF</a> </p>
<p><strong>Summary</strong><br>机器学习模型在手术中对组织进行表征可辅助决策并引导安全肿瘤切除。图像分类模型通常采用像素归因方法推断解释性。然而，对深度学习模型预测的过度自信会导致像素归因的过度自信。本文首次将风险估计融入像素归因方法，以提高图像分类的解释性。该方法通过迭代应用分类模型和像素归因方法创建像素级归因地图卷积。首次使用此卷积生成像素级归因值分布，并引入一种方法生成增强型归因地图，通过估计期望值来评估像素级风险。因此，该方法不仅提供了增强的归因地图，还输出了归因值的预估风险。在基于探针的共焦激光显微内镜数据和ImageNet上的性能评估证明，改进的解释性方法优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器学习模型在手术中用于组织表征可以提高决策效率和指导安全肿瘤切除。</li>
<li>像素归因方法是推断图像分类模型解释性的常用手段，但需避免过度自信。</li>
<li>本文首次结合风险估计与像素归因方法，提高图像分类解释性。</li>
<li>通过迭代应用分类模型和像素归因方法创建归因地图卷积，并利用此卷积生成像素级归因值分布。</li>
<li>引入一种方法生成增强型归因地图，通过估计期望值评估像素级风险。</li>
<li>所提方法不仅优化了解释性，还能对归因值的风险进行预估。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23709">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a3fdd579a42627fcec76c58e7deee1da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-730c3e932355cace58122d41a4768e84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ddeaa1ca02fbc983d3d789572012aa1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Hyperbolic-Cycle-Alignment-for-Infrared-Visible-Image-Fusion"><a href="#Hyperbolic-Cycle-Alignment-for-Infrared-Visible-Image-Fusion" class="headerlink" title="Hyperbolic Cycle Alignment for Infrared-Visible Image Fusion"></a>Hyperbolic Cycle Alignment for Infrared-Visible Image Fusion</h2><p><strong>Authors:Timing Li, Bing Cao, Jiahe Feng, Haifang Cao, Qinghau Hu, Pengfei Zhu</strong></p>
<p>Image fusion synthesizes complementary information from multiple sources, mitigating the inherent limitations of unimodal imaging systems. Accurate image registration is essential for effective multi-source data fusion. However, existing registration methods, often based on image translation in Euclidean space, fail to handle cross-modal misalignment effectively, resulting in suboptimal alignment and fusion quality. To overcome this limitation, we explore image alignment in non-Euclidean space and propose a Hyperbolic Cycle Alignment Network (Hy-CycleAlign). To the best of our knowledge, Hy-CycleAlign is the first image registration method based on hyperbolic space. It introduces a dual-path cross-modal cyclic registration framework, in which a forward registration network aligns cross-modal inputs, while a backward registration network reconstructs the original image, forming a closed-loop registration structure with geometric consistency. Additionally, we design a Hyperbolic Hierarchy Contrastive Alignment (H$^{2}$CA) module, which maps images into hyperbolic space and imposes registration constraints, effectively reducing interference caused by modality discrepancies. We further analyze image registration in both Euclidean and hyperbolic spaces, demonstrating that hyperbolic space enables more sensitive and effective multi-modal image registration. Extensive experiments on misaligned multi-modal images demonstrate that our method significantly outperforms existing approaches in both image alignment and fusion. Our code will be publicly available. </p>
<blockquote>
<p>图像融合通过综合来自多个源头的互补信息，缓解了单模态成像系统的固有局限性。准确的图像配准对于有效的多源数据融合至关重要。然而，现有的配准方法通常基于欧几里得空间的图像翻译，无法有效地处理跨模态的不对准问题，导致配准和融合质量不佳。为了克服这一局限性，我们探索了非欧几里得空间的图像配准，并提出了一种双曲循环配准网络（Hy-CycleAlign）。据我们所知，Hy-CycleAlign是基于双曲空间的第一个图像配准方法。它引入了一个双路径跨模态循环配准框架，其中前向配准网络对齐跨模态输入，而后向配准网络重建原始图像，形成一个具有几何一致性的闭环配准结构。此外，我们设计了一个双曲层次对比配准（H²CA）模块，它将图像映射到双曲空间并施加配准约束，有效减少了由模态差异引起的干扰。我们进一步分析了欧几里得空间和双曲空间中的图像配准，表明双曲空间能够实现更敏感和有效的多模态图像配准。在对错位多模态图像的大量实验中，我们的方法在图像配准和融合方面都显著优于现有方法。我们的代码将公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23508v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>图像融合通过合成来自多个源头的互补信息，减轻了单模态成像系统的固有局限性。准确图像配准是实现多源数据有效融合的关键。然而，现有的配准方法通常基于欧几里得空间的图像翻译，无法有效地处理跨模态的误对齐问题，导致配准和融合效果不佳。为克服这一局限，我们探索了在非欧几里得空间中的图像对齐，并提出了一种双路径超循环对齐网络（Hy-CycleAlign）。据我们所知，Hy-CycleAlign是基于双曲空间的第一个图像配准方法。它引入了一个双路径跨模态循环配准框架，其中正向配准网络对齐跨模态输入，反向配准网络重建原始图像，形成一个具有几何一致性的闭环配准结构。此外，我们还设计了超层次对比对齐模块（H²CA），将图像映射到双曲空间并施加配准约束，有效降低由模态差异引起的干扰。我们进一步分析了欧几里得空间和双曲空间中的图像配准，表明双曲空间能够实现更敏感和有效的多模态图像配准。在误对齐的多模态图像上的大量实验表明，我们的方法在图像配准和融合方面都显著优于现有方法。我们的代码将公开可用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>图像融合旨在结合多源信息以克服单模态成像的局限性。</li>
<li>准确配准是实现多源数据有效融合的关键步骤。</li>
<li>现有基于欧几里得空间的配准方法在处理跨模态误对齐时表现有限。</li>
<li>提出了基于非欧几里得空间的图像配准方法——Hy-CycleAlign网络。</li>
<li>Hy-CycleAlign网络采用双路径跨模态循环配准框架，包含正向和反向配准网络。</li>
<li>H²CA模块用于将图像映射到双曲空间并施加配准约束。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23508">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0c636eb8ce00e7b3def37164fc523e5d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-189c1f0f8b615011c3e9ab26a0ab693c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0281d7756a852d54685c21b5b21848cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e49dcff49153df90b6cb16a3db7b6a2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Early-Goal-Guided-Multi-Scale-Fusion-for-Real-Time-Vision-Language-Driving"><a href="#Early-Goal-Guided-Multi-Scale-Fusion-for-Real-Time-Vision-Language-Driving" class="headerlink" title="Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language   Driving"></a>Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language   Driving</h2><p><strong>Authors:Santosh Patapati, Trisanth Srinivasan</strong></p>
<p>Autonomous vehicles must react in milliseconds while reasoning about road geometry and traffic intent to navigate complex situations. We introduce NovaDrive, a single-branch vision-language architecture that processes front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a single branch. A lightweight, two-stage cross-attention block first aligns waypoint tokens with the HD map, then refines attention over fine-grained image and depth patches. Coupled with a novel smoothness loss that discourages abrupt steering and speed changes, this design eliminates the need for recurrent memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language backbone, enabling real-time inference. On the nuScenes &#x2F; Waymo subset of the MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from 2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention fusion each contribute the most to these gains. Beyond safety, NovaDrive’s shorter routes (resulting from the novel smoothness loss) translate to lower fuel or battery usage, pointing toward leaner, more easily updated driving stacks. NovaDrive can be extended to other embodied-AI domains as well. </p>
<blockquote>
<p>我们推出了NovaDrive，这是一种单分支视觉语言架构，能够在单个分支中处理前置摄像头图像、高清地图瓦片、激光雷达深度和文本坐标点。其通过一个轻量级的两阶段交叉注意力模块，首先对齐坐标点与高清地图，然后精细调整图像和深度补丁的注意力。结合一种新型平滑损失，该设计可避免急剧转向和速度变化，从而无需使用循环内存。我们微调了规模为11B的LLaMA-3.2视觉语言主干的前15层，以实现实时推理。在MD-NEX室外基准的nuScenes&#x2F;Waymo子集上，NovaDrive将成功率提升至84%（+4%），路径效率（SPL）提高到0.66（+0.11），与之前的最新技术相比，碰撞频率从2.6%降至1.2%（-1.4%）。我们的剖析研究证实，坐标点标记、部分VLM微调以及交叉注意力融合等都对这些增益贡献最大。除了提高安全性外，NovaDrive的较短路线（由新型平滑损失产生）转化为更低的燃油或电池使用率，指向更精简、更容易更新的驾驶堆栈。NovaDrive还可以扩展到其他实体AI领域。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23042v1">PDF</a> 6 pages</p>
<p><strong>Summary</strong></p>
<p>在自动驾驶过程中，处理视觉信息对于车辆应对复杂道路和交通环境至关重要。我们提出了NovaDrive系统，这是一种单一分支的视觉语言架构，可以处理前视摄像头图像、高清地图切片、激光雷达深度以及文本导航点等信息。该系统通过一种新颖的跨注意力机制实现了图像与导航点的精准对齐，并结合了一种新型的平滑损失函数，减少了车辆的急转弯和速度变化。NovaDrive在MD-NEX户外基准测试中的表现优于现有技术，成功率和路径效率均有所提高，碰撞频率降低。此外，NovaDrive还可以缩短行驶路线，降低燃油或电池消耗，并有望应用于其他智能体领域。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NovaDrive系统是一个单一的视觉语言架构，可以处理多种驾驶相关信息的输入。</li>
<li>系统通过新颖的跨注意力机制实现图像与导航点的精准对齐。</li>
<li>NovaDrive结合了新型的平滑损失函数，减少车辆的急转弯和速度变化。</li>
<li>NovaDrive在基准测试中表现优异，成功率和路径效率提高，碰撞频率降低。</li>
<li>NovaDrive可缩短行驶路线，降低燃油或电池消耗。</li>
<li>NovaDrive系统具有广泛的应用潜力，可应用于其他智能体领域。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23042">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f45317744d2a0b01d17135446177089b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47c25fd592560a81f1b14d658fe4d6ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1daad611d0cc6880e350af0df114029f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Meta-CLIP-2-A-Worldwide-Scaling-Recipe"><a href="#Meta-CLIP-2-A-Worldwide-Scaling-Recipe" class="headerlink" title="Meta CLIP 2: A Worldwide Scaling Recipe"></a>Meta CLIP 2: A Worldwide Scaling Recipe</h2><p><strong>Authors:Yung-Sung Chuang, Yang Li, Dong Wang, Ching-Feng Yeh, Kehan Lyu, Ramya Raghavendra, James Glass, Lifei Huang, Jason Weston, Luke Zettlemoyer, Xinlei Chen, Zhuang Liu, Saining Xie, Wen-tau Yih, Shang-Wen Li, Hu Xu</strong></p>
<p>Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP’s training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., “curse of multilinguality” that is common in LLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, Meta CLIP 2 ViT-H&#x2F;14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval. </p>
<blockquote>
<p>对比语言图像预训练（CLIP）是一种流行的基础模型，支持零次分类、检索和多模态大型语言模型（MLLMs）的编码器。尽管CLIP已成功地在英文世界的百亿级图像文本对上进行了训练，但进一步将CLIP的训练扩展到从全球网络数据上进行学习仍然具有挑战性：（1）没有处理方法可以用来处理非英语世界的数据点；（2）现有跨语言CLIP的英语性能表现不如其英语专属的对应模型，即常见的多语言大型模型中的“多语言诅咒”。在这里，我们提出了Meta CLIP 2，这是首次从头开始在全球网络规模的图像文本对上训练CLIP的方法。为了推广我们的发现，我们进行了严格的必要最小变化消融实验，以解决上述挑战，并提出了一种能从英语和非英语世界数据中实现互利共赢的配方。在零次ImageNet分类中，Meta CLIP 2 ViT-H&#x2F;14的精度超过了其英语专属的对应模型0.8%，并超越了mSigLIP 0.7%，令人惊讶的是在多种多语言基准测试中，如CVQA达到57.4%，Babel-ImageNet达到50.2%，XM3600的图像到文本检索达到64.3%，并且没有任何系统级别的混淆因素（如翻译、专用架构更改）。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22062v2">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Contrastive Language-Image Pretraining（CLIP）模型在全球化网络数据训练上的挑战，并推出了Meta CLIP 2模型。该模型可以在全球网页规模图像文本对上从零开始训练CLIP。研究通过严谨的消融实验，解决上述挑战并实现了英语和非英语世界数据的互利。在零样本ImageNet分类中，Meta CLIP 2超越了仅针对英语的CLIP模型和mSigLIP模型，并在跨语言基准测试中取得了最新成果。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLIP模型在全球化网络数据训练上存在挑战，缺乏处理非英语世界数据点的策略。</li>
<li>Meta CLIP 2是首个从头开始在全球网页规模图像文本对上训练的CLIP模型。</li>
<li>研究通过必要的最小变化进行了严谨的消融实验，解决上述挑战并实现英语和非英语数据的互利。</li>
<li>Meta CLIP 2在零样本ImageNet分类中性能卓越，超越了仅针对英语的CLIP模型和mSigLIP模型。</li>
<li>在跨语言基准测试中（如CVQA、Babel-ImageNet和XM3600），Meta CLIP 2实现了图像到文本的检索新水平。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22062">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-a53dce16ab723ffe2baaa4705aaa15e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8175805bc79aa52c29a3d11c3daa3a95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1b667027140ffc44f4d050d0b68d488.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f017b21fdec2b192ed3e605e24e00c86.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Semantic-Segmentation-of-iPS-Cells-Case-Study-on-Model-Complexity-in-Biomedical-Imaging"><a href="#Semantic-Segmentation-of-iPS-Cells-Case-Study-on-Model-Complexity-in-Biomedical-Imaging" class="headerlink" title="Semantic Segmentation of iPS Cells: Case Study on Model Complexity in   Biomedical Imaging"></a>Semantic Segmentation of iPS Cells: Case Study on Model Complexity in   Biomedical Imaging</h2><p><strong>Authors:Maoquan Zhang, Bisser Raytchev, Xiujuan Sun</strong></p>
<p>Medical image segmentation requires not only accuracy but also robustness under challenging imaging conditions. In this study, we show that a carefully configured DeepLabv3 model can achieve high performance in segmenting induced pluripotent stem (iPS) cell colonies, and, under our experimental conditions, outperforms large-scale foundation models such as SAM2 and its medical variant MedSAM2 without structural modifications. These results suggest that, for specialized tasks characterized by subtle, low-contrast boundaries, increased model complexity does not necessarily translate to better performance. Our work revisits the assumption that ever-larger and more generalized architectures are always preferable, and provides evidence that appropriately adapted, simpler models may offer strong accuracy and practical reliability in domain-specific biomedical applications. We also offer an open-source implementation that includes strategies for small datasets and domain-specific encoding, with the aim of supporting further advances in semantic segmentation for regenerative medicine and related fields. </p>
<blockquote>
<p>医学图像分割不仅需要准确性，还需要在具有挑战性的成像条件下具有稳健性。在这项研究中，我们展示了精心配置的DeepLabv3模型在分割诱导多能干细胞（iPS）菌落方面的高性能，并且在我们的实验条件下，未经结构修改即优于大规模基础模型（如SAM2及其医学变体MedSAM2）。这些结果表明，对于特征表现为细微、低对比度边界的特定任务，增加模型复杂性并不一定意味着更好的性能。我们的工作重新考虑了假设更大的、更通用的架构总是可取的，并提供了证据支持适当简化并适应的模型在特定领域生物医学应用中具有强大的准确性和实用性。我们还提供了一个开源实现，包括针对小数据集和特定领域编码的策略，旨在支持再生医学和相关领域的语义分割的进一步发展。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21608v1">PDF</a> 19th International Conference on Machine Vision Applications MVA2025</p>
<p><strong>摘要</strong></p>
<p>本研究展示了精心配置的DeepLabv3模型在诱导多能干细胞（iPS）菌落分割方面的高性能表现。相较于大规模的基础模型如SAM2及其医学变体MedSAM2，该模型在特定实验条件下无需结构修改便展现出更佳性能。结果提示，对于特征为细微、低对比度边界的专项任务，增加模型复杂性并不一定意味着性能提升。本研究重新审视了“越大越广的基础架构总是更可取”的假设，并提供证据表明，适当简化并适应特定领域的模型可能在生物医学应用中提供强大的准确性和实际可靠性。我们还提供了包含小数据集策略和领域特定编码策略的开源实现，旨在支持再生医学和相关领域的语义分割的进一步发展。</p>
<p><strong>要点</strong></p>
<ol>
<li>DeepLabv3模型在iPS细胞菌落分割中表现出高性能。</li>
<li>在特定实验条件下，DeepLabv3性能优于大规模基础模型SAM2及其医学变体MedSAM2。</li>
<li>对于细微、低对比度边界的专项任务，增加模型复杂性并不一定意味着性能提升。</li>
<li>研究挑战了“越大越广的基础架构更可取”的假设。</li>
<li>适当简化并适应特定领域的模型在生物医学应用中可能更可靠。</li>
<li>提供包含小数据集策略和领域特定编码策略的开源实现。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21608">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8e87f14ec6885586249392bd244b7b60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84d9642ce16b584c3532f561fa244da6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4451222ff3b4febb53467c771cdf53e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MaXsive-High-Capacity-and-Robust-Training-Free-Generative-Image-Watermarking-in-Diffusion-Models"><a href="#MaXsive-High-Capacity-and-Robust-Training-Free-Generative-Image-Watermarking-in-Diffusion-Models" class="headerlink" title="MaXsive: High-Capacity and Robust Training-Free Generative Image   Watermarking in Diffusion Models"></a>MaXsive: High-Capacity and Robust Training-Free Generative Image   Watermarking in Diffusion Models</h2><p><strong>Authors:Po-Yuan Mao, Cheng-Chang Tsai, Chun-Shien Lu</strong></p>
<p>The great success of the diffusion model in image synthesis led to the release of gigantic commercial models, raising the issue of copyright protection and inappropriate content generation. Training-free diffusion watermarking provides a low-cost solution for these issues. However, the prior works remain vulnerable to rotation, scaling, and translation (RST) attacks. Although some methods employ meticulously designed patterns to mitigate this issue, they often reduce watermark capacity, which can result in identity (ID) collusion. To address these problems, we propose MaXsive, a training-free diffusion model generative watermarking technique that has high capacity and robustness. MaXsive best utilizes the initial noise to watermark the diffusion model. Moreover, instead of using a meticulously repetitive ring pattern, we propose injecting the X-shape template to recover the RST distortions. This design significantly increases robustness without losing any capacity, making ID collusion less likely to happen. The effectiveness of MaXsive has been verified on two well-known watermarking benchmarks under the scenarios of verification and identification. </p>
<blockquote>
<p>扩散模型在图像合成方面的巨大成功催生了庞大的商业模型，进而引发了版权保护和不当内容生成的问题。无训练扩散水印技术为这些问题提供了低成本解决方案。然而，先前的工作仍然容易受到旋转、缩放和平移（RST）攻击。尽管一些方法采用精心设计的模式来缓解这一问题，但它们往往会降低水印容量，从而导致身份（ID）碰撞。为了解决这些问题，我们提出了MaXsive，这是一种无训练扩散模型生成式水印技术，具有大容量和高鲁棒性。MaXsive充分利用初始噪声来为扩散模型添加水印。此外，我们没有使用精心设计的重复性环形图案，而是提出了注入X形模板来恢复RST失真。这种设计在不损失容量的前提下显著提高了鲁棒性，使身份碰撞的可能性大大降低。MaXsive的有效性已在两个著名的水印基准测试场景下通过验证和识别得到了证实。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21195v1">PDF</a> </p>
<p><strong>Summary</strong><br>扩散模型在图像合成领域的巨大成功引发了商用大型模型的涌现，随之带来版权保护与不当内容生成的问题。训练式扩散水印法为此提供了低成本解决方案，但以往的方法容易受到旋转、缩放和平移（RST）攻击。为解决这些问题，我们提出名为MaXsive的训练式扩散模型生成水印技术，该技术具有大容量和高鲁棒性。MaXsive充分利用初始噪声进行水印处理，并提出使用X形模板来恢复RST失真，而非采用精细重复的环形图案。这一设计在不损失容量的前提下大大提高了鲁棒性，降低了身份碰撞的可能性。MaXsive在两个著名的水印基准测试上得到了验证，证明了其在验证和识别场景下的有效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>扩散模型成功推动商业大型模型的发展，但也带来了版权保护和不当内容生成的问题。</li>
<li>训练式扩散水印法为上述问题提供了低成本解决方案，但原有方法易受旋转、缩放和平移（RST）攻击。</li>
<li>MaXsive技术通过利用初始噪声进行水印处理提高了水印技术的鲁棒性和容量。</li>
<li>MaXsive使用X形模板恢复RST失真，避免了精细重复环形图案的使用。</li>
<li>MaXsive设计在不损失容量的前提下增强了鲁棒性，降低了身份碰撞的风险。</li>
<li>MaXsive在两个著名的水印基准测试上经过了验证，证明了其在验证和识别场景下的有效性。</li>
<li>MaXsive技术对于保护版权和防止不当内容生成具有重要意义。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21195">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-109ffccca7be1430bfc9a58645420e60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-372d4a60e435297af4ad42f9ce189aa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a5efe157a5a4bf7ff9e8a22dd3ebea0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de32facf89a6de131b152defd3129cec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e82f644629cc001779b7f9bc2d6bfccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca10f209d3b961080a274b9f18209f84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-347281603426d9f08a2fb04d4d5d78a2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Metabolic-Imaging-Integrated-Model-for-Prognostic-Prediction-in-Colorectal-Liver-Metastases"><a href="#A-Metabolic-Imaging-Integrated-Model-for-Prognostic-Prediction-in-Colorectal-Liver-Metastases" class="headerlink" title="A Metabolic-Imaging Integrated Model for Prognostic Prediction in   Colorectal Liver Metastases"></a>A Metabolic-Imaging Integrated Model for Prognostic Prediction in   Colorectal Liver Metastases</h2><p><strong>Authors:Qinlong Li, Pu Sun, Guanlin Zhu, Tianjiao Liang, Honggang QI</strong></p>
<p>Prognostic evaluation in patients with colorectal liver metastases (CRLM) remains challenging due to suboptimal accuracy of conventional clinical models. This study developed and validated a robust machine learning model for predicting postoperative recurrence risk. Preliminary ensemble models achieved exceptionally high performance (AUC $&gt;$ 0.98) but incorporated postoperative features, introducing data leakage risks. To enhance clinical applicability, we restricted input variables to preoperative baseline clinical parameters and radiomic features from contrast-enhanced CT imaging, specifically targeting recurrence prediction at 3, 6, and 12 months postoperatively. The 3-month recurrence prediction model demonstrated optimal performance with an AUC of 0.723 in cross-validation. Decision curve analysis revealed that across threshold probabilities of 0.55-0.95, the model consistently provided greater net benefit than “treat-all” or “treat-none” strategies, supporting its utility in postoperative surveillance and therapeutic decision-making. This study successfully developed a robust predictive model for early CRLM recurrence with confirmed clinical utility. Importantly, it highlights the critical risk of data leakage in clinical prognostic modeling and proposes a rigorous framework to mitigate this issue, enhancing model reliability and translational value in real-world settings. </p>
<blockquote>
<p>在结直肠癌肝转移（CRLM）患者的预后评估中，由于传统临床模型的准确性不高，仍面临挑战。本研究开发并验证了一个稳健的机器学习任务学习模型来预测术后复发风险。初步集合模型表现异常出色（AUC大于0.98），但包含了术后特征，存在数据泄露风险。为了提高临床实用性，我们将输入变量限制为术前基线临床参数和增强CT成像的放射学特征，特别针对术后3个月、6个月和12个月的复发预测。3个月复发预测模型在交叉验证中表现出最佳性能，AUC为0.723。决策曲线分析表明，在阈概率0.55-0.95范围内，该模型提供的净效益始终大于“全部治疗”或“不治疗”策略，这支持其在术后监测和治疗决策中的实用性。本研究成功地为早期CRLM复发构建了一个稳健的预测模型，并证实了其在临床上的实用性。重要的是，它强调了临床预后模型中数据泄露的关键风险，并提出了一个严格的框架来减轻这个问题，提高了模型在真实世界环境中的可靠性和翻译价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19734v1">PDF</a> 8 pages,4 figues</p>
<p><strong>Summary</strong>：</p>
<p>针对结直肠癌肝转移患者的预后评估仍面临挑战，因传统临床模型的准确度有待提高。本研究开发并验证了一种稳健的机器学模型以预测术后复发风险。研究初期集合模型表现出极高性能（AUC＞0.98），但纳入术后特征，存在数据泄露风险。为提高临床实用性，研究使用术前基线临床参数和增强CT成像的放射组学特征作为输入变量，专门针对术后3个月、6个月和1年的复发预测建模。其中，术后3个月复发预测模型的交叉验证表现最佳，AUC为0.723。决策曲线分析表明，在阈概率0.55至0.95范围内，该模型提供的净效益高于“一律治疗”或“一律不治疗”的策略，证实了其在术后监测和治疗决策中的实用性。本研究成功开发了一个可靠的早期CRLM复发预测模型，并强调了临床预后建模中数据泄露的风险，提出了一个严格的框架来减轻这一问题，提高了模型在现实世界的可靠性和翻译价值。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>传统临床模型在预测结直肠癌肝转移（CRLM）患者预后时的准确性有待提高。</li>
<li>集合模型虽表现出极高性能，但存在数据泄露风险。</li>
<li>研究聚焦于使用术前临床参数和放射组学特征预测术后复发风险。</li>
<li>术后三个月复发预测模型性能最佳，交叉验证AUC为0.723。</li>
<li>决策曲线分析证实该模型在特定阈概率范围内优于其他治疗策略。</li>
<li>研究成功开发了一个可靠的早期CRLM复发预测模型，有助于术后监测和治疗决策。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19734">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4c1e9c1d623599fc66e663952bc91143.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-068ba7323208a91a07ccc186a473e0ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b6c787d76455996bb300f01d4dc8f67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31623d0dc02ed087b08fc37c82caa0c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48306411e240806dbcf2f32cf6928acc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1b2303a7dc1f2b2dbe83105c0b19be5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Continual-Learning-Based-Unified-Model-for-Unpaired-Image-Restoration-Tasks"><a href="#Continual-Learning-Based-Unified-Model-for-Unpaired-Image-Restoration-Tasks" class="headerlink" title="Continual Learning-Based Unified Model for Unpaired Image Restoration   Tasks"></a>Continual Learning-Based Unified Model for Unpaired Image Restoration   Tasks</h2><p><strong>Authors:Kotha Kartheek, Lingamaneni Gnanesh Chowdary, Snehasis Mukherjee</strong></p>
<p>Restoration of images contaminated by different adverse weather conditions such as fog, snow, and rain is a challenging task due to the varying nature of the weather conditions. Most of the existing methods focus on any one particular weather conditions. However, for applications such as autonomous driving, a unified model is necessary to perform restoration of corrupted images due to different weather conditions. We propose a continual learning approach to propose a unified framework for image restoration. The proposed framework integrates three key innovations: (1) Selective Kernel Fusion layers that dynamically combine global and local features for robust adaptive feature selection; (2) Elastic Weight Consolidation (EWC) to enable continual learning and mitigate catastrophic forgetting across multiple restoration tasks; and (3) a novel Cycle-Contrastive Loss that enhances feature discrimination while preserving semantic consistency during domain translation. Further, we propose an unpaired image restoration approach to reduce the dependance of the proposed approach on the training data. Extensive experiments on standard benchmark datasets for dehazing, desnowing and deraining tasks demonstrate significant improvements in PSNR, SSIM, and perceptual quality over the state-of-the-art. </p>
<blockquote>
<p>由于天气条件的多样性，由雾、雪和雨等不同恶劣天气条件污染的图片恢复是一项具有挑战性的任务。现有的大多数方法主要关注某一种特定的天气条件。然而，对于自动驾驶等应用，由于不同的天气条件导致图像退化，因此需要一种统一模型进行恢复。我们提出了一种持续学习的方法，以建立一个统一的图像恢复框架。该框架集成了三项关键创新：1）选择性核融合层，它能动态结合全局和局部特征，实现稳健的自适应特征选择；2）弹性权重巩固（EWC），以实现持续学习，并减轻多个恢复任务中的灾难性遗忘；3）一种新的循环对比损失，它提高了特征辨别力，同时在域转换过程中保留了语义一致性。此外，我们提出了一种无需配对的图像恢复方法，以减少所提出方法对训练数据的依赖。在除雾、消雪和去雨任务的标准基准数据集上进行的广泛实验表明，在峰值信号噪声比（PSNR）、结构相似性（SSIM）和感知质量方面均有显著改进，超过了最新技术水平。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19184v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>针对恶劣天气条件（如雾、雪和雨）导致的图像污染恢复是一项具有挑战性的任务，因为天气条件的变化多样。现有方法大多专注于特定的天气条件，但在自动驾驶等应用中，需要一种统一模型来恢复因不同天气条件而损坏的图像。本文提出了一种持续学习的方法，建立一个统一的图像恢复框架，该框架集成了三大创新点：选择性核融合层可动态结合全局和局部特征进行稳健的自适应特征选择；弹性权重巩固（EWC）可实现持续学习并减轻多个恢复任务中的灾难性遗忘；以及新型的循环对比损失，可在域转换过程中增强特征鉴别力并保留语义一致性。此外，本文提出了一种无需配对的图像恢复方法，以降低该方案对训练数据的依赖性。在消雾、除雪和去雨的标准基准数据集上进行的广泛实验表明，与最新技术相比，它在峰值信噪比（PSNR）、结构相似性（SSIM）和感知质量方面都有显著提高。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>恶劣天气条件下的图像恢复是一项挑战，因为天气条件的变化多样。</li>
<li>现有方法大多专注于特定天气条件，需要一种统一模型来处理多种天气条件下的图像恢复。</li>
<li>提出的统一框架集成了三大创新点：选择性核融合层、弹性权重巩固（EWC）和循环对比损失。</li>
<li>选择性核融合层能够动态结合全局和局部特征，进行稳健的自适应特征选择。</li>
<li>弹性权重巩固（EWC）有助于实现持续学习并减轻灾难性遗忘。</li>
<li>新型的循环对比损失可增强特征鉴别力，并在域转换过程中保留语义一致性。</li>
<li>该研究还提出了一种无需配对的图像恢复方法，以降低对训练数据的依赖性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19184">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ab23022947596f80f6cf5cfb251eeb28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48a6a02a7c662ecf50e24d0e621ba5a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8722694a8ed1b7dc8983128d1cf03b12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e1d692f544d3e011f55af0a74745c99.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SIDA-Synthetic-Image-Driven-Zero-shot-Domain-Adaptation"><a href="#SIDA-Synthetic-Image-Driven-Zero-shot-Domain-Adaptation" class="headerlink" title="SIDA: Synthetic Image Driven Zero-shot Domain Adaptation"></a>SIDA: Synthetic Image Driven Zero-shot Domain Adaptation</h2><p><strong>Authors:Ye-Chan Kim, SeungJu Cha, Si-Woo Kim, Taewhan Kim, Dong-Jin Kim</strong></p>
<p>Zero-shot domain adaptation is a method for adapting a model to a target domain without utilizing target domain image data. To enable adaptation without target images, existing studies utilize CLIP’s embedding space and text description to simulate target-like style features. Despite the previous achievements in zero-shot domain adaptation, we observe that these text-driven methods struggle to capture complex real-world variations and significantly increase adaptation time due to their alignment process. Instead of relying on text descriptions, we explore solutions leveraging image data, which provides diverse and more fine-grained style cues. In this work, we propose SIDA, a novel and efficient zero-shot domain adaptation method leveraging synthetic images. To generate synthetic images, we first create detailed, source-like images and apply image translation to reflect the style of the target domain. We then utilize the style features of these synthetic images as a proxy for the target domain. Based on these features, we introduce Domain Mix and Patch Style Transfer modules, which enable effective modeling of real-world variations. In particular, Domain Mix blends multiple styles to expand the intra-domain representations, and Patch Style Transfer assigns different styles to individual patches. We demonstrate the effectiveness of our method by showing state-of-the-art performance in diverse zero-shot adaptation scenarios, particularly in challenging domains. Moreover, our approach achieves high efficiency by significantly reducing the overall adaptation time. </p>
<blockquote>
<p>零样本域自适应是一种无需使用目标域图像数据即可使模型适应目标域的方法。为了在无需目标图像的情况下实现自适应，现有研究利用CLIP的嵌入空间和文本描述来模拟目标相似的风格特征。尽管在零样本域自适应方面取得了先前的成就，我们观察到这些文本驱动的方法在捕捉现实世界复杂变化方面遇到了困难，并且由于对齐过程而增加了大量的自适应时间。我们不再依赖文本描述，而是探索利用图像数据的解决方案，图像数据提供了多样且更精细的风格线索。在这项工作中，我们提出了一种新的高效零样本域自适应方法，即利用合成图像进行自适应（SIDA）。为了生成合成图像，我们首先创建详细的源域图像，并通过图像翻译来反映目标域的样式。然后，我们利用这些合成图像的样式特征作为目标域的代理。基于这些特征，我们引入了Domain Mix和Patch Style Transfer模块，它们可以有效地模拟现实世界的变化。特别是，Domain Mix通过混合多种风格来扩展域内表示，而Patch Style Transfer则为各个补丁分配不同的风格。我们在多种零样本自适应场景中展示了我们的方法的有效性，特别是在具有挑战性的领域里。此外，我们的方法通过大大减少了总体自适应时间而实现了高效率。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18632v1">PDF</a> Accepted to ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于合成图像的高效零样本域自适应方法（SIDA）。该方法通过生成合成图像来模拟目标域的样式特征，进而实现模型对目标域的适应，而无需使用目标域的图像数据。通过混合多种风格并转移补丁风格，该方法能够更有效地模拟真实世界的变化。此外，该方法显著减少了自适应时间，提高了效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>零样本域自适应方法无需使用目标域图像数据即可使模型适应目标域。</li>
<li>本文提出了一种基于合成图像的方法来实现零样本域自适应。</li>
<li>通过生成详细的源域图像并应用图像翻译来反映目标域的样式，进而模拟目标域的样式特征。</li>
<li>引入Domain Mix和Patch Style Transfer模块，以更有效地模拟真实世界的变化。</li>
<li>Domain Mix通过混合多种风格来扩展域内表示。</li>
<li>Patch Style Transfer将不同的风格分配给单独的补丁，以提高模型的表现力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18632">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-651b585c4db1a513d190ee5438ea1453.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb24c801afbec0774ba405f533a34838.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca8d87aa79c42dc13c642d9656acbc1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c38b562a53bec8d09c68241f549ec161.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23f096b65128c1afab919719df9b3be6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2ff6bf06c8644c10a63662d75bada84.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GIIFT-Graph-guided-Inductive-Image-free-Multimodal-Machine-Translation"><a href="#GIIFT-Graph-guided-Inductive-Image-free-Multimodal-Machine-Translation" class="headerlink" title="GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation"></a>GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation</h2><p><strong>Authors:Jiafeng Xiong, Yuting Zhao</strong></p>
<p>Multimodal Machine Translation (MMT) has demonstrated the significant help of visual information in machine translation. However, existing MMT methods face challenges in leveraging the modality gap by enforcing rigid visual-linguistic alignment whilst being confined to inference within their trained multimodal domains. In this work, we construct novel multimodal scene graphs to preserve and integrate modality-specific information and introduce GIIFT, a two-stage Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph Attention Network adapter to learn multimodal knowledge in a unified fused space and inductively generalize it to broader image-free translation domains. Experimental results on the Multi30K dataset of English-to-French and English-to-German tasks demonstrate that our GIIFT surpasses existing approaches and achieves the state-of-the-art, even without images during inference. Results on the WMT benchmark show significant improvements over the image-free translation baselines, demonstrating the strength of GIIFT towards inductive image-free inference. </p>
<blockquote>
<p>多模态机器翻译（MMT）已经证明了视觉信息在机器翻译中的巨大帮助。然而，现有的MMT方法在利用模态差距方面面临挑战，它们通过强制实施严格的视觉语言对齐，同时受限于其训练的多模态域内的推理。在这项工作中，我们构建了新型的多模态场景图，以保留和整合特定模态的信息，并引入了GIIFT，这是一个两阶段的图引导归纳无图多模态机器翻译框架，它使用跨模态图注意力网络适配器，在统一融合空间学习多模态知识，并归纳推广至更广泛的无图像翻译领域。在Multi30K英语到法语和英语到德语任务的实验结果表明，我们的GIIFT超越了现有方法，达到了最先进的水平，即使在推理过程中没有使用图像。在WMT基准测试上的结果也显著优于无图像翻译基线，证明了GIIFT在无图像推理方面的优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18562v1">PDF</a> </p>
<p><strong>Summary</strong>：本研究通过构建新型的多模态场景图来融合多模态信息，提出了一种基于图注意力网络的图像自由机器翻译框架GIIFT。该框架能在统一的融合空间中学习多模态知识，并在没有图像的情况下推广到更广泛的翻译领域。在Multi30K数据集上的实验结果表明，GIIFT超越了现有方法，达到了图像自由翻译的最先进水平。在WMT基准测试上的结果也显著优于基线模型，证明了GIIFT在图像自由推理方面的优势。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>多模态机器翻译（MMT）借助视觉信息为机器翻译提供了重要帮助。</li>
<li>当前MMT方法面临模态间差距的挑战，需要更灵活的视觉语言对齐方式，并局限于已训练的多模态领域。</li>
<li>研究构建了新型的多模态场景图来保存和整合模态特定信息。</li>
<li>引入了GIIFT框架，这是一个两阶段的图引导图像自由MMT框架。</li>
<li>GIIFT使用跨模态图注意力网络适配器，在统一的融合空间中学习多模态知识，并归纳推广到更广泛的图像自由翻译领域。</li>
<li>在Multi30K数据集上的实验结果表明，GIIFT达到了最先进的图像自由翻译水平，特别是在推理过程中无需图像。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18562">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-fe4cef1813eaaa2d650cef44d76a0659.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea6305d740915593d7cfa89fb52a0482.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-722c0f6a401dc0f95aa71827455f72aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee621fc888eed6ddce4ec1175008df87.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Dual-branch-Prompting-for-Multimodal-Machine-Translation"><a href="#Dual-branch-Prompting-for-Multimodal-Machine-Translation" class="headerlink" title="Dual-branch Prompting for Multimodal Machine Translation"></a>Dual-branch Prompting for Multimodal Machine Translation</h2><p><strong>Authors:Jie Wang, Zhendong Yang, Liansong Zong, Xiaobo Zhang, Dexian Wang, Ji Zhang</strong></p>
<p>Multimodal Machine Translation (MMT) typically enhances text-only translation by incorporating aligned visual features. Despite the remarkable progress, state-of-the-art MMT approaches often rely on paired image-text inputs at inference and are sensitive to irrelevant visual noise, which limits their robustness and practical applicability. To address these issues, we propose D2P-MMT, a diffusion-based dual-branch prompting framework for robust vision-guided translation. Specifically, D2P-MMT requires only the source text and a reconstructed image generated by a pre-trained diffusion model, which naturally filters out distracting visual details while preserving semantic cues. During training, the model jointly learns from both authentic and reconstructed images using a dual-branch prompting strategy, encouraging rich cross-modal interactions. To bridge the modality gap and mitigate training-inference discrepancies, we introduce a distributional alignment loss that enforces consistency between the output distributions of the two branches. Extensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves superior translation performance compared to existing state-of-the-art approaches. </p>
<blockquote>
<p>多模态机器翻译（MMT）通常通过融入对齐的视觉特征来增强纯文本翻译。尽管取得了显著的进步，但最新的MMT方法通常在推理阶段依赖于配对图像文本输入，并对不相关的视觉噪声敏感，这限制了其稳健性和实际应用能力。为了解决这些问题，我们提出了D2P-MMT，这是一种基于扩散模型的双分支提示框架，用于实现稳健的视觉引导翻译。具体而言，D2P-MMT仅需要源文本和一个由预训练扩散模型生成的重建图像，该模型能够自然地过滤掉令人分心的视觉细节，同时保留语义线索。在训练过程中，模型采用双分支提示策略，从真实和重建的图像中学习，鼓励丰富的跨模态交互。为了缩小模态差距并减轻训练与推理之间的差异，我们引入了一种分布对齐损失，以强制两个分支输出分布的一致性。在Multi30K数据集上的大量实验表明，与现有的最新方法相比，D2P-MMT实现了卓越的翻译性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17588v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>多模态机器翻译（MMT）通过融入对齐的视觉特征增强了纯文本翻译。然而，当前先进技术的方法在推理时通常依赖于配对图像文本输入，并对无关的视觉噪声敏感，这限制了其稳健性和实际应用能力。为解决这些问题，我们提出了基于扩散模型的双分支提示框架D2P-MMT，用于稳健的视觉引导翻译。具体而言，D2P-MMT仅需要源文本和由预训练扩散模型生成的重建图像，该模型能够自然过滤掉令人分心的视觉细节，同时保留语义线索。在训练过程中，模型采用双分支提示策略，从真实和重建的图像中学习，促进跨模态的丰富交互。为缩小模态差距并减轻训练与推理之间的差异，我们引入了一种分布对齐损失，以确保两个分支输出分布的一致性。在Multi30K数据集上的广泛实验表明，D2P-MMT的翻译性能优于现有先进技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态机器翻译（MMT）通过结合视觉特征改进文本翻译。</li>
<li>当前MMT方法依赖于配对图像文本输入，对视觉噪声敏感。</li>
<li>D2P-MMT框架提出一种基于扩散模型的解决方案，仅需要源文本和重建图像。</li>
<li>重建图像自然过滤掉干扰视觉细节，保留语义线索。</li>
<li>模型采用双分支提示策略进行训练，促进跨模态交互。</li>
<li>引入分布对齐损失以缩小模态差距和减少训练与推理差异。</li>
<li>在Multi30K数据集上，D2P-MMT表现出优于现有技术的翻译性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17588">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5d66b5c937bd791753554dbb71b3ae77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e3fd40b9f8326a2764be192664dc64a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8aa4d08019ca21c0fb06fa5cc13801f1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="UniLGL-Learning-Uniform-Place-Recognition-for-FOV-limited-Panoramic-LiDAR-Global-Localization"><a href="#UniLGL-Learning-Uniform-Place-Recognition-for-FOV-limited-Panoramic-LiDAR-Global-Localization" class="headerlink" title="UniLGL: Learning Uniform Place Recognition for FOV-limited&#x2F;Panoramic   LiDAR Global Localization"></a>UniLGL: Learning Uniform Place Recognition for FOV-limited&#x2F;Panoramic   LiDAR Global Localization</h2><p><strong>Authors:Hongming Shen, Xun Chen, Yulin Hui, Zhenyu Wu, Wei Wang, Qiyang Lyu, Tianchen Deng, Danwei Wang</strong></p>
<p>Existing LGL methods typically consider only partial information (e.g., geometric features) from LiDAR observations or are designed for homogeneous LiDAR sensors, overlooking the uniformity in LGL. In this work, a uniform LGL method is proposed, termed UniLGL, which simultaneously achieves spatial and material uniformity, as well as sensor-type uniformity. The key idea of the proposed method is to encode the complete point cloud, which contains both geometric and material information, into a pair of BEV images (i.e., a spatial BEV image and an intensity BEV image). An end-to-end multi-BEV fusion network is designed to extract uniform features, equipping UniLGL with spatial and material uniformity. To ensure robust LGL across heterogeneous LiDAR sensors, a viewpoint invariance hypothesis is introduced, which replaces the conventional translation equivariance assumption commonly used in existing LPR networks and supervises UniLGL to achieve sensor-type uniformity in both global descriptors and local feature representations. Finally, based on the mapping between local features on the 2D BEV image and the point cloud, a robust global pose estimator is derived that determines the global minimum of the global pose on SE(3) without requiring additional registration. To validate the effectiveness of the proposed uniform LGL, extensive benchmarks are conducted in real-world environments, and the results show that the proposed UniLGL is demonstratively competitive compared to other State-of-the-Art LGL methods. Furthermore, UniLGL has been deployed on diverse platforms, including full-size trucks and agile Micro Aerial Vehicles (MAVs), to enable high-precision localization and mapping as well as multi-MAV collaborative exploration in port and forest environments, demonstrating the applicability of UniLGL in industrial and field scenarios. </p>
<blockquote>
<p>现有的LGL方法通常只考虑LiDAR观测的部分信息（例如几何特征），或者为均质的LiDAR传感器而设计，忽略了LGL中的均匀性。在此工作中，提出了一种统一的LGL方法，称为UniLGL，它同时实现了空间、材料以及传感器类型的均匀性。该方法的关键思想是将包含几何和材料信息的完整点云编码为一对BEV图像（即空间BEV图像和强度BEV图像）。设计了一个端到端的多BEV融合网络来提取均匀特征，使UniLGL具备空间和材料均匀性。为了确保跨不同LiDAR传感器的稳健LGL，引入了视点不变性假设，该假设取代了现有LPR网络中常用的平移等价假设，并促使UniLGL在全局描述符和局部特征表示上实现传感器类型均匀性。最后，基于二维BEV图像上的局部特征与点云之间的映射关系，导出了一个稳健的全局姿态估计器，该估计器可在SE(3)上确定全局姿态的全局最小值，无需额外的注册过程。为了验证所提统一LGL的有效性，在真实环境中进行了广泛的基准测试，结果表明与其他最新LGL方法相比，所提的UniLGL具有竞争力。此外，UniLGL已部署在多种平台上，包括全尺寸卡车和敏捷微型航空车辆（MAVs），以实现港口和森林环境中的高精度定位和映射以及多MAV协同探索，证明了UniLGL在工业和现场场景中的适用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12194v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种名为UniLGL的统一LGL方法，该方法实现了空间、材质和传感器类型的均匀性。通过将点云编码成一对BEV图像，并结合端到端的多BEV融合网络提取均匀特征，该方法具有空间和材质均匀性。为确保在不同激光雷达传感器之间的稳健性LGL，引入了视点不变假设，代替了现有LPR网络中常用的平移等价假设，并实现了传感器类型的统一。基于局部特征与点云之间的映射关系，推导出了稳健的全局姿态估计器。实验验证表明，与其他先进的LGL方法相比，UniLGL具有竞争力，并已部署在各种平台上进行实际应用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniLGL方法考虑了激光雷达观测的完整性信息，并实现了空间、材质和传感器类型的均匀性。</li>
<li>通过将点云编码成BEV图像，并结合多BEV融合网络提取均匀特征。</li>
<li>引入视点不变假设以确保在不同激光雷达传感器之间的稳健性。</li>
<li>UniLGL实现了全局和局部特征表示中的传感器类型统一。</li>
<li>基于局部特征与点云之间的映射关系，推导出了全局姿态估计器。</li>
<li>实验验证显示UniLGL与其他先进的LGL方法相比具有竞争力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12194">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c7dbd1755e3ec5c6ae0bdfe6145fbc66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81aa24cf01729421f460b89790c2d352.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4005ab3aaae796ce394830cab3261d51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eec73b948879e5156784fc2ad0fcdc9a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Time-resolved-dynamic-CBCT-reconstruction-using-prior-model-free-spatiotemporal-Gaussian-representation-PMF-STGR"><a href="#Time-resolved-dynamic-CBCT-reconstruction-using-prior-model-free-spatiotemporal-Gaussian-representation-PMF-STGR" class="headerlink" title="Time-resolved dynamic CBCT reconstruction using prior-model-free   spatiotemporal Gaussian representation (PMF-STGR)"></a>Time-resolved dynamic CBCT reconstruction using prior-model-free   spatiotemporal Gaussian representation (PMF-STGR)</h2><p><strong>Authors:Jiacheng Xie, Hua-Chieh Shao, You Zhang</strong></p>
<p>Time-resolved CBCT imaging, which reconstructs a dynamic sequence of CBCTs reflecting intra-scan motion (one CBCT per x-ray projection without phase sorting or binning), is highly desired for regular and irregular motion characterization, patient setup, and motion-adapted radiotherapy. Representing patient anatomy and associated motion fields as 3D Gaussians, we developed a Gaussian representation-based framework (PMF-STGR) for fast and accurate dynamic CBCT reconstruction. PMF-STGR comprises three major components: a dense set of 3D Gaussians to reconstruct a reference-frame CBCT for the dynamic sequence; another 3D Gaussian set to capture three-level, coarse-to-fine motion-basis-components (MBCs) to model the intra-scan motion; and a CNN-based motion encoder to solve projection-specific temporal coefficients for the MBCs. Scaled by the temporal coefficients, the learned MBCs will combine into deformation vector fields to deform the reference CBCT into projection-specific, time-resolved CBCTs to capture the dynamic motion. Due to the strong representation power of 3D Gaussians, PMF-STGR can reconstruct dynamic CBCTs in a ‘one-shot’ training fashion from a standard 3D CBCT scan, without using any prior anatomical or motion model. We evaluated PMF-STGR using XCAT phantom simulations and real patient scans. Metrics including the image relative error, structural-similarity-index-measure, tumor center-of-mass-error, and landmark localization error were used to evaluate the accuracy of solved dynamic CBCTs and motion. PMF-STGR shows clear advantages over a state-of-the-art, INR-based approach, PMF-STINR. Compared with PMF-STINR, PMF-STGR reduces reconstruction time by 50% while reconstructing less blurred images with better motion accuracy. With improved efficiency and accuracy, PMF-STGR enhances the applicability of dynamic CBCT imaging for potential clinical translation. </p>
<blockquote>
<p>时间解析CBCT成像重建了一连串反映扫描内运动的CBCT（每次X射线投影都有一个CBCT，无需相位排序或分箱），这在常规和运动不规则的运动表征、患者设置和适应性运动放疗中都非常受欢迎。我们用三维高斯表示患者解剖结构和相关运动场，开发了一个基于高斯表示的框架（PMF-STGR），用于快速准确的动态CBCT重建。PMF-STGR主要包括三个组成部分：一组密集的三维高斯用于重建动态序列的参考框架CBCT；另一组三维高斯用于捕捉三级粗到细的运动基础成分（MBCs），以模拟扫描内的运动；以及基于CNN的运动编码器，用于解决MBC的投影特定时间系数。根据时间系数，学习的MBC将组合成变形矢量场，以变形参考CBCT为投影特定的时间解析CBCT，以捕捉动态运动。由于三维高斯具有强大的表征能力，PMF-STGR可以从标准的3D CBCT扫描中进行“单次”训练方式重建动态CBCT，而无需使用任何先前的解剖或运动模型。我们使用XCAT幻影模拟和实际患者扫描对PMF-STGR进行了评估。使用图像相对误差、结构相似性指数度量、肿瘤质心误差和地标定位误差等指标来评估求解的动态CBCT和运动的准确性。与当前先进的INR方法PMF-STINR相比，PMF-STGR具有明显的优势。与PMF-STINR相比，PMF-STGR在重建时间减少50%的同时，重建的图像更清晰，运动精度更高。通过提高效率和准确性，PMF-STGR增强了动态CBCT成像的适用性，具有潜在的临床翻译价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22139v2">PDF</a> 25 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>基于三维高斯表示的框架（PMF-STGR）实现了快速准确的动态CBC成像重建。它采用一套密集的三维高斯函数来重建参考帧CBC图像，用于动态序列；另一套三维高斯函数用于捕捉不同级别的运动基础分量（MBCs）；采用CNN（卷积神经网络）基于运动编码器解决投影特定时间系数的问题。通过动态训练，PMF-STGR能够从标准的三维CBC扫描中重建动态CBC图像，无需使用任何先验解剖或运动模型。评估结果显示，与当前主流方法相比，PMF-STGR在重建时间和图像质量方面均表现出优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>时间解析CBCT成像对于常规和不规则运动表征、患者设置和运动适应放疗具有重要意义。</li>
<li>PMF-STGR框架利用三维高斯表示来快速准确地重建动态CBCT图像。</li>
<li>PMF-STGR包含三个主要组件：用于重建参考帧CBCT的密集三维高斯集、捕捉运动基础分量（MBCs）的另一三维高斯集以及基于CNN的运动编码器。</li>
<li>通过结合学习的MBC和时间系数，PMF-STGR能够重建投影特定的时间解析CBCT图像。</li>
<li>PMF-STGR具有强大的表示能力，可从标准的三维CBCT扫描中通过一次性训练重建动态CBCT图像，无需使用任何先验解剖或运动模型。</li>
<li>评估结果表明，与现有方法相比，PMF-STGR在图像质量、运动准确性和重建时间方面具有明显优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22139">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-d634c919b5907f5cbe0af67819e79853.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Versatile-Multimodal-Controls-for-Expressive-Talking-Human-Animation"><a href="#Versatile-Multimodal-Controls-for-Expressive-Talking-Human-Animation" class="headerlink" title="Versatile Multimodal Controls for Expressive Talking Human Animation"></a>Versatile Multimodal Controls for Expressive Talking Human Animation</h2><p><strong>Authors:Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Zixin Zhu, Sanping Zhou, Ming Yang, Le Wang</strong></p>
<p>In filmmaking, directors typically allow actors to perform freely based on the script before providing specific guidance on how to present key actions. AI-generated content faces similar requirements, where users not only need automatic generation of lip synchronization and basic gestures from audio input but also desire semantically accurate and expressive body movement that can be &#96;&#96;directly guided’’ through text descriptions. Therefore, we present VersaAnimator, a versatile framework that synthesizes expressive talking human videos from arbitrary portrait images. Specifically, we design a motion generator that produces basic rhythmic movements from audio input and supports text-prompt control for specific actions. The generated whole-body 3D motion tokens can animate portraits of various scales, producing talking heads, half-body gestures and even leg movements for whole-body images. Besides, we introduce a multi-modal controlled video diffusion that generates photorealistic videos, where speech signals govern lip synchronization, facial expressions, and head motions while body movements are guided by the 2D poses. Furthermore, we introduce a token2pose translator to smoothly map 3D motion tokens to 2D pose sequences. This design mitigates the stiffness resulting from direct 3D to 2D conversion and enhances the details of the generated body movements. Extensive experiments shows that VersaAnimator synthesizes lip-synced and identity-preserving videos while generating expressive and semantically meaningful whole-body motions. </p>
<blockquote>
<p>在电影制作中，导演通常会让演员根据剧本自由发挥，然后再提供关于如何呈现关键动作的具体指导。人工智能生成的内容面临着类似的要求，用户不仅需要自动生成与音频输入同步的基本动作，还希望得到语义准确、富有表现力的身体动作，这些动作可以通过文字描述来“直接指导”。因此，我们推出了VersaAnimator，这是一个通用框架，可以从任意的肖像图中合成富有表现力的说话人类视频。具体来说，我们设计了一个动作生成器，它可以从音频输入中产生基本的有节奏的动作，并支持通过文字提示来控制特定动作。生成的全身3D动作令牌可以驱动各种规模的肖像图，产生说话头部、半身手势甚至全身图像的腿部动作。此外，我们引入了一种多模式控制视频扩散技术，生成逼真的视频，其中语音信号控制唇部同步、面部表情和头部动作，而身体动作则由二维姿势引导。而且，我们还推出了token2pose转换器，它能将3D动作令牌平稳地映射到2D姿势序列。这种设计减轻了直接3D到2D转换导致的僵硬感，并增强了生成的身体动作的细节。大量实验表明，VersaAnimator能合成唇音同步、身份保留的视频，同时生成富有表现力和语义的全身动作。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08714v4">PDF</a> Accepted by ACM MM2025</p>
<p><strong>Summary</strong></p>
<p>VersaAnimator框架可以生成基于任意肖像图像的表达式说话人类视频。该框架设计了一个动作生成器，可以从音频输入产生基本节奏动作并支持通过文本描述进行特定动作的直接指导。此框架能生成全身3D动作令牌，可以动画化不同规模的肖像，产生谈话头部、半身姿势甚至是全身图像腿部动作。此外，引入多模式控制视频扩散技术，生成真实感视频，语音信号控制唇部同步、面部表情和头部运动，而身体运动则由2D姿势引导。同时，引入token2pose翻译器，将3D动作令牌平滑映射到2D姿势序列，增强生成的身体运动细节。实验表明，VersaAnimator合成的视频具有唇部同步、身份保留的特点，同时能生成表达力强、语义明确的全身动作。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VersaAnimator是一个通用框架，可以从任意肖像图像生成表达式说话人类视频。</li>
<li>该框架包含运动生成器，能根据音频输入产生基本节奏动作，并支持文本描述对特定动作的直接指导。</li>
<li>能生成全身3D动作令牌，适用于不同规模的肖像动画，包括谈话头部、半身姿势和全身图像腿部动作。</li>
<li>引入多模式控制视频扩散技术，结合语音信号和2D姿势，生成真实感视频。</li>
<li>token2pose翻译器用于将3D动作令牌映射到2D姿势序列，增强身体运动细节的真实性。</li>
<li>VersaAnimator合成的视频具有唇部同步、身份保留特点。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08714">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4ece756ddbcbb9cb90df7645521e10d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7f12638b9cf1c0f25da3f50bc91e174.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8edeef7feb631e64561559cfeaf972c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7864a8c243ae7b482377f2235a595d5f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1841831cdf57ca48d05cf99b17e27349.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-245c7d0f1c0b208f404ad96895c9a136.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Rapid-and-Reproducible-Whole-Brain-Multi-Pool-CEST-Imaging-at-3T-Using-a-Single-Shot-True-FISP-Readout"><a href="#Rapid-and-Reproducible-Whole-Brain-Multi-Pool-CEST-Imaging-at-3T-Using-a-Single-Shot-True-FISP-Readout" class="headerlink" title="Rapid and Reproducible Whole-Brain Multi-Pool CEST Imaging at 3T Using a   Single-Shot True FISP Readout"></a>Rapid and Reproducible Whole-Brain Multi-Pool CEST Imaging at 3T Using a   Single-Shot True FISP Readout</h2><p><strong>Authors:Yupeng Wu, Siyuan Fang, Siyuan Wang, Caixia Fu, Jianqi Li</strong></p>
<p>Purpose: To develop and validate a comprehensive, rapid, and reproducible solution for whole-brain, multi-pool CEST imaging at 3T, overcoming key barriers to clinical translation such as long acquisition times and inaccuracies from field inhomogeneities. Methods: This study integrated a single-shot 3D True FISP readout sequence for efficient whole-brain CEST data acquisition. A streamlined workflow was developed to acquire B0, B1, and T1 maps for correction. To overcome the time-consuming nature of traditional B1 correction, we implemented a machine learning-based method to perform rapid B1 correction using data from a single B1 power acquisition. Data were analyzed using a four-pool Lorentzian model to derive quantitative metrics, including MTRLD and the AREX. The method’s accuracy was validated in phantoms and its test-retest reproducibility was assessed in healthy volunteers across 96 brain regions. Results: The True FISP sequence acquired high-quality, whole-brain images free of major artifacts. The neural network accurately replicated the gold-standard three-point B1 correction, achieving excellent intraclass correlation (ICC &gt; 0.97) in human subjects. The AREX metric successfully corrected for T1 and MT confounders, reducing the CV from 33.6% to 6.9% in phantoms. The complete pipeline, including Z-spectrum and correction maps, took approximately 9 minutes. The method demonstrated high region-level reproducibility, with the average CV for APT_AREX under 10% for most brain regions across test-retest scans. Conclusion: This study presents a validated, end-to-end solution for whole-brain, multi-pool CEST imaging. By combining an efficient sequence with a rapid, AI-driven correction pipeline and robust quantitative analysis, our method delivers high-fidelity, reproducible, and quantitative multi-parameter maps of brain metabolism in a clinically acceptable timeframe. </p>
<blockquote>
<p>目的：旨在开发并验证一种全面、快速、可重复的针对全脑多池化学交换饱和转移成像（CEST成像）的解决方案，克服长期存在的临床翻译障碍，如采集时间过长以及由于磁场不均匀性导致的准确性问题。方法：本研究结合了单发三维真实FISP读出序列，用于高效全脑CEST数据采集。为了获取校正所需的B0、B1和T1图，开发了一个简化的工作流程。为了克服传统B1校正耗时的问题，我们实施了一种基于机器学习的方法进行快速B1校正，使用单次B1功率采集的数据。数据使用四池洛伦兹模型进行分析，以推导定量指标，包括MTRLD和AREX。该方法的准确性在模拟体模中得到了验证，并在健康志愿者的96个脑区中对测试重复再现性进行了评估。结果：True FISP序列获得的高质量全脑图像无主要伪影。神经网络准确复制了金标准三点B1校正，在人类受试者中达到了良好的组内相关系数（ICC&gt; 0.97）。AREX指标成功地校正了T1和MT混淆因素，将模拟体模中的变异系数从33.6%减少到6.9%。完整的流程包括Z谱和校正图，大约需要9分钟。该方法显示出较高的区域水平可重复性，大多数脑区的APT_AREX测试再测试扫描的平均变异系数低于10%。结论：本研究提出了一种经过验证的端到端全脑多池CEST成像解决方案。通过结合高效的序列、快速的人工智能驱动校正管道和稳健的定量分析，我们的方法在可接受的临床时间内提供了高保真、可重复和定量的脑代谢多参数图。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16986v3">PDF</a> Keywords: CEST, whole-brain, multi-pool, true fast imaging with   steady-state precession (True FISP), balanced steady state free precession   (bSSFP)</p>
<p><strong>Summary</strong></p>
<p>本文旨在开发并验证一种全面、快速、可重复的针对全脑多池 CEST 成像的综合解决方案。该研究通过整合单发射 3D True FISP 读出序列，实现了高效的全脑 CEST 数据采集。通过开发标准化工作流程，用于获取 B0、B1 和 T1 图进行校正。为了克服传统 B1 校正的时间消耗，研究实施了基于机器学习的方法，利用单次 B1 功率采集数据进行快速 B1 校正。该研究在幻影中验证了方法的准确性，并在健康志愿者中对 96 个脑区进行了测试-再测试的可重复性评估。结果显示，True FISP 序列获取的全脑图像质量高且无主要伪影。神经网络准确复制了金标准三点 B1 校正，在人类受试者中达到了极高的组内相关系数（ICC &gt; 0.97）。AREX 指标成功校正了 T1 和 MT 干扰因素，在幻影中将变异系数从 33.6% 降低到 6.9%。整个管道包括 Z 光谱和校正图，大约需要 9 分钟。该方法显示了高度区域可重复性，在测试-再测试扫描中，大多数脑区的 APT_AREX 平均变异系数低于 10%。本研究提出了一个经过验证的端到端解决方案，用于全脑多池 CEST 成像，该方法结合了高效的序列、快速的 AI 驱动校正管道和稳健的定量分析，提供高保真、可重复和量化的脑代谢多参数图，临床可接受时间范围内。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究目标是开发并验证一种适用于全脑多池 CEST 成像的综合解决方案。</li>
<li>通过整合单发射 3D True FISP 读出序列实现了高效的全脑数据获取。</li>
<li>利用机器学习技术实现快速 B1 校正以缩短成像时间。</li>
<li>使用四池洛伦兹模型进行数据分析，以生成定量指标。</li>
<li>方法在幻影中的准确性和在人类受试者中的可重复性得到了验证。</li>
<li>AREX 指标成功校正了 T1 和 MT 的干扰因素，提高了成像质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16986">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-c6e1756599463c2a1a4bffb0fc043f79.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Categorical-Schrodinger-Bridge-Matching"><a href="#Categorical-Schrodinger-Bridge-Matching" class="headerlink" title="Categorical Schrödinger Bridge Matching"></a>Categorical Schrödinger Bridge Matching</h2><p><strong>Authors:Grigoriy Ksenofontov, Alexander Korotin</strong></p>
<p>The Schr&quot;odinger Bridge (SB) is a powerful framework for solving generative modeling tasks such as unpaired domain translation. Most SB-related research focuses on continuous data space $\mathbb{R}^{D}$ and leaves open theoretical and algorithmic questions about applying SB methods to discrete data, e.g, on finite spaces $\mathbb{S}^{D}$. Notable examples of such sets $\mathbb{S}$ are codebooks of vector-quantized (VQ) representations of modern autoencoders, tokens in texts, categories of atoms in molecules, etc. In this paper, we provide a theoretical and algorithmic foundation for solving SB in discrete spaces using the recently introduced Iterative Markovian Fitting (IMF) procedure. Specifically, we theoretically justify the convergence of discrete-time IMF (D-IMF) to SB in discrete spaces. This enables us to develop a practical computational algorithm for SB, which we call Categorical Schr&quot;odinger Bridge Matching (CSBM). We show the performance of CSBM via a series of experiments with synthetic data and VQ representations of images. The code of CSBM is available at <a target="_blank" rel="noopener" href="https://github.com/gregkseno/csbm">https://github.com/gregkseno/csbm</a>. </p>
<blockquote>
<p>薛定谔桥（SB）是一个强大的框架，用于解决如非配对域翻译等生成建模任务。大多数与SB相关的研究都集中在连续数据空间$\mathbb{R}^{D}$上，而将SB方法应用于离散数据（例如有限空间$\mathbb{S}^{D}$）的理论和算法问题留待解决。集合$\mathbb{S}$的著名例子包括现代自动编码器的向量量化（VQ）表示的代码本、文本中的令牌、分子中的原子类别等。在本文中，我们利用最近提出的迭代马尔可夫拟合（IMF）程序，为解决离散空间中的SB提供了理论和算法基础。具体来说，我们从理论上证明了离散时间IMF（D-IMF）收敛到离散空间中的SB。这使我们能够为SB开发一种实用的计算算法，我们称之为分类薛定谔桥匹配（CSBM）。我们通过合成数据和VQ图像表示的一系列实验展示了CSBM的性能。CSBM的代码可在<a target="_blank" rel="noopener" href="https://github.com/gregkseno/csbm%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gregkseno/csbm找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01416v3">PDF</a> </p>
<p><strong>Summary</strong><br>     本文介绍了Schrödinger Bridge（SB）在离散空间中的应用，通过使用最近引入的Iterative Markovian Fitting（IMF）程序解决SB在离散空间中的理论和方法基础。作者提出了一种实用的计算算法，称为Categorical Schrödinger Bridge Matching（CSBM），并通过合成数据和VQ图像表示的实验验证了其性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Schrödinger Bridge (SB) 是一个强大的生成建模框架，用于解决如未配对领域翻译等任务。</li>
<li>现有研究大多关注连续数据空间中的SB，而对离散数据应用SB方法仍存在理论和算法问题。</li>
<li>文章提供了在离散空间中解决SB的理论和算法基础，使用新引入的Iterative Markovian Fitting (IMF) 程序。</li>
<li>离散时间IMF（D-IMF）收敛到SB的理论证明。</li>
<li>基于此，开发了一种实用的计算算法Categorical Schrödinger Bridge Matching (CSBM)。</li>
<li>通过合成数据和VQ图像表示的实验验证了CSBM的性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01416">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-38d216fe6692d73a4bd67f900ea1c5a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c79a7adcc688f219015178fcc4167115.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Unified-3D-MRI-Representations-via-Sequence-Invariant-Contrastive-Learning"><a href="#Unified-3D-MRI-Representations-via-Sequence-Invariant-Contrastive-Learning" class="headerlink" title="Unified 3D MRI Representations via Sequence-Invariant Contrastive   Learning"></a>Unified 3D MRI Representations via Sequence-Invariant Contrastive   Learning</h2><p><strong>Authors:Liam Chalcroft, Jenny Crinion, Cathy J. Price, John Ashburner</strong></p>
<p>Self-supervised deep learning has accelerated 2D natural image analysis but remains difficult to translate into 3D MRI, where data are scarce and pre-trained 2D backbones cannot capture volumetric context. We present a \emph{sequence-invariant} self-supervised framework leveraging quantitative MRI (qMRI). By simulating multiple MRI contrasts from a single 3D qMRI scan and enforcing consistent representations across these contrasts, we learn anatomy-centric rather than sequence-specific features. The result is a single 3D encoder that excels across tasks and protocols. Experiments on healthy brain segmentation (IXI), stroke lesion segmentation (ARC), and MRI denoising show significant gains over baseline SSL approaches, especially in low-data settings (up to +8.3% Dice, +4.2 dB PSNR). It also generalises to unseen sites, supporting scalable clinical use. Code and trained models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/contrast-squared">https://github.com/liamchalcroft/contrast-squared</a> </p>
<blockquote>
<p>自监督深度学习已加速2D自然图像分析，但转化为3D MRI仍然困难，3D MRI数据稀缺，预训练的2D骨干网无法捕捉体积上下文。我们提出了一种利用定量MRI（qMRI）的序列不变自监督框架。通过模拟单个3D qMRI扫描的多个MRI对比度，并强制这些对比度之间保持一致的表示，我们学习以解剖为中心的特征，而非特定序列的特征。结果是一个单一的3D编码器，它在各种任务和协议中都表现出色。在健康大脑分割（IXI）、中风病灶分割（ARC）和MRI降噪方面的实验显示，与传统的自监督学习方法相比，尤其是在数据较少的情况下（最高可提高+8.3％的Dice系数，+4.2分贝峰值信噪比），该方法取得了显著的优势。它还适用于未见过的站点，支持可扩展的临床使用。相关代码和训练模型已在<a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/contrast-squared%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/liamchalcroft/contrast-squared公开可用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12057v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>基于定量MRI（qMRI）的序列不变自监督框架，通过模拟单个3DqMRI扫描的多个MRI对比度并强制这些对比度之间的一致性表示，学习以解剖为中心而非特定序列的特征。该框架使3D MRI分析受益于自监督深度学习，并克服了在低数据情况下应用困难的问题。该框架在各种任务和协议中表现出卓越性能，尤其在健康大脑分割（IXI）、中风病灶分割（ARC）和MRI去噪方面取得了显著进展。代码和训练模型已公开在GitHub上提供。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>自监督深度学习在加速二维自然图像分析方面取得了进展，但难以应用于三维MRI数据。</li>
<li>三维MRI数据稀缺，且预训练的二维骨干网无法捕捉体积上下文信息，造成应用困难。</li>
<li>提出了一种利用定量MRI（qMRI）的序列不变自监督框架，模拟单个3DqMRI扫描的多个MRI对比度并强制其一致性表示。</li>
<li>学习以解剖为中心的特征而非特定序列的特征，解决MRI数据的独特挑战。</li>
<li>单一3D编码器在不同任务和协议中表现卓越，特别是在低数据设置下。</li>
<li>在健康大脑分割、中风病灶分割和MRI去噪方面取得了显著进展，相较于基线SSL方法有明显提升。</li>
<li>该框架具有在未见过的站点进行推广的能力，支持可扩展的临床应用。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12057">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-cbd68edece167c0e394d6df698ef8826.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae1048d249a80f7f2442ee94efa4eccf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-abaafaa8a84973257a8628f39371b17d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="FBSDiff-Plug-and-Play-Frequency-Band-Substitution-of-Diffusion-Features-for-Highly-Controllable-Text-Driven-Image-Translation"><a href="#FBSDiff-Plug-and-Play-Frequency-Band-Substitution-of-Diffusion-Features-for-Highly-Controllable-Text-Driven-Image-Translation" class="headerlink" title="FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features   for Highly Controllable Text-Driven Image Translation"></a>FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features   for Highly Controllable Text-Driven Image Translation</h2><p><strong>Authors:Xiang Gao, Jiaying Liu</strong></p>
<p>Large-scale text-to-image diffusion models have been a revolutionary milestone in the evolution of generative AI and multimodal technology, allowing wonderful image generation with natural-language text prompt. However, the issue of lacking controllability of such models restricts their practical applicability for real-life content creation. Thus, attention has been focused on leveraging a reference image to control text-to-image synthesis, which is also regarded as manipulating (or editing) a reference image as per a text prompt, namely, text-driven image-to-image translation. This paper contributes a novel, concise, and efficient approach that adapts pre-trained large-scale text-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality and versatile text-driven I2I translation without any model training, model fine-tuning, or online optimization process. To guide T2I generation with a reference image, we propose to decompose diverse guiding factors with different frequency bands of diffusion features in the DCT spectral space, and accordingly devise a novel frequency band substitution layer which realizes dynamic control of the reference image to the T2I generation result in a plug-and-play manner. We demonstrate that our method allows flexible control over both guiding factor and guiding intensity of the reference image simply by tuning the type and bandwidth of the substituted frequency band, respectively. Extensive qualitative and quantitative experiments verify superiority of our approach over related methods in I2I translation visual quality, versatility, and controllability. The code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/XiangGao1102/FBSDiff">https://github.com/XiangGao1102/FBSDiff</a>. </p>
<blockquote>
<p>大规模文本到图像的扩散模型在生成式人工智能和多模态技术的演变中成为了一个革命性的里程碑。它能够通过自然语言文本提示来生成精彩的图像。然而，这类模型缺乏可控性的问题限制了它们在现实生活内容创建中的实际应用。因此，人们开始关注利用参考图像来控制文本到图像的合成，这被视为根据文本提示操作（或编辑）参考图像，即文本驱动的图像到图像翻译。本文贡献了一种新颖、简洁、高效的方法，该方法将预训练的大规模文本到图像（T2I）扩散模型适应到图像到图像（I2I）范式中，以即插即用方式，无需任何模型训练、模型微调或在线优化过程，即可实现高质量和多功能文本驱动的I2I翻译。为了用参考图像引导T2I生成，我们提出在DCT谱空间中分解具有不同频率带的扩散特征的多种指导因素，并相应地设计了一种新型频率带替换层，以即插即用方式动态控制参考图像对T2I生成结果的影响。我们证明，通过简单地调整替代的频率带的类型和带宽，我们的方法能够灵活地控制参考图像的指导因素和强度。大量的定性和定量实验验证了我们方法在图像到图像翻译的视觉质量、多样性和可控性方面的优越性。代码公开在：<a target="_blank" rel="noopener" href="https://github.com/XiangGao1102/FBSDiff%E3%80%82">https://github.com/XiangGao1102/FBSDiff。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.00998v5">PDF</a> Accepted conference paper of ACM MM 2024</p>
<p><strong>Summary</strong></p>
<p>文本介绍了大型文本到图像扩散模型在生成AI和多模态技术方面的革命性里程碑，但缺乏可控性限制了其实用性。因此，研究利用参考图像来控制文本到图像的合成，提出了一种新型的、简洁高效的方法，通过预训练的大型文本到图像扩散模型，以即插即用方式实现高质量的文本驱动图像到图像翻译，无需模型训练或微调。通过分解不同频率带的扩散特征，提出了频率带替代层，实现了对参考图像的动态控制。该方法通过调整替代的频率带类型和带宽，可以灵活控制指导因素和参考图像的指导强度。实验验证了在图像到图像翻译的视觉质量、通用性和可控性方面的优越性。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型文本到图像扩散模型在生成AI和多模态技术领域具有革命性。</li>
<li>缺乏可控性是这些模型实际应用中的主要限制。</li>
<li>利用参考图像控制文本到图像的合成已成为研究焦点。</li>
<li>该论文提出了一种新型方法，通过预训练的文本到图像扩散模型实现即插即用的图像到图像翻译。</li>
<li>方法涉及在DCT谱空间中分解不同频率带的扩散特征。</li>
<li>频率带替代层可实现参考图像的动态控制。</li>
<li>该方法允许通过调整替代的频率带类型和带宽来灵活控制指导因素和参考图像的指导强度。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.00998">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-09f1391d5e3070663ca55d9d91b7a8d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b21be3ad1ab8897486ba3100c632bfcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cde09de8597f7d3593a9cc2b5c5eb2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1bbd9f8814760b1d453513e0729bdd38.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="X-ray2CTPA-Leveraging-Diffusion-Models-to-Enhance-Pulmonary-Embolism-Classification"><a href="#X-ray2CTPA-Leveraging-Diffusion-Models-to-Enhance-Pulmonary-Embolism-Classification" class="headerlink" title="X-ray2CTPA: Leveraging Diffusion Models to Enhance Pulmonary Embolism   Classification"></a>X-ray2CTPA: Leveraging Diffusion Models to Enhance Pulmonary Embolism   Classification</h2><p><strong>Authors:Noa Cahan, Eyal Klang, Galit Aviram, Yiftach Barash, Eli Konen, Raja Giryes, Hayit Greenspan</strong></p>
<p>Chest X-rays or chest radiography (CXR), commonly used for medical diagnostics, typically enables limited imaging compared to computed tomography (CT) scans, which offer more detailed and accurate three-dimensional data, particularly contrast-enhanced scans like CT Pulmonary Angiography (CTPA). However, CT scans entail higher costs, greater radiation exposure, and are less accessible than CXRs. In this work we explore cross-modal translation from a 2D low contrast-resolution X-ray input to a 3D high contrast and spatial-resolution CTPA scan. Driven by recent advances in generative AI, we introduce a novel diffusion-based approach to this task. We evaluate the models performance using both quantitative metrics and qualitative feedback from radiologists, ensuring diagnostic relevance of the generated images. Furthermore, we employ the synthesized 3D images in a classification framework and show improved AUC in a PE categorization task, using the initial CXR input. The proposed method is generalizable and capable of performing additional cross-modality translations in medical imaging. It may pave the way for more accessible and cost-effective advanced diagnostic tools. The code for this project is available: <a target="_blank" rel="noopener" href="https://github.com/NoaCahan/X-ray2CTPA">https://github.com/NoaCahan/X-ray2CTPA</a> . </p>
<blockquote>
<p>胸部X射线或胸部放射摄影（CXR）是医学诊断中常用的方法，与计算机断层扫描（CT）相比，其成像通常较为有限。CT扫描可以提供更详细和准确的三维数据，特别是像CT肺动脉造影（CTPA）这样的增强扫描。然而，CT扫描的成本较高，辐射暴露量较大，且相对于CXRs来说不太容易获得。在这项工作中，我们探索了从低对比度的2D X射线输入到高对比度和空间分辨率的3DCTPA扫描的跨模态翻译。得益于生成人工智能的最新进展，我们针对此任务引入了一种新型的基于扩散的方法。我们利用定量指标和来自放射科医师的定性反馈来评估模型性能，以确保生成图像的诊断相关性。此外，我们在分类框架中使用了合成的3D图像，并在使用初始CXR输入的PE分类任务中显示出提高了AUC。所提出的方法具有通用性，能够执行其他医学成像的跨模态翻译。它可能为更便捷和成本效益更高的先进诊断工具铺平道路。该项目的代码可在以下网址找到：<a target="_blank" rel="noopener" href="https://github.com/NoaCahan/X-ray2CTPA">https://github.com/NoaCahan/X-ray2CTPA</a> 。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16109v4">PDF</a> preprint, project code: <a target="_blank" rel="noopener" href="https://github.com/NoaCahan/X-ray2CTPA">https://github.com/NoaCahan/X-ray2CTPA</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了将二维低对比度分辨率的X光片转化为三维高对比度和高分辨率的CT肺动脉造影（CTPA）扫描的跨模态转换技术。利用生成式人工智能的最新进展，提出了一种基于扩散的方法来完成这一任务。经过定量评估和放射科医生定性反馈，生成的图像具有诊断意义。此外，合成的三维图像在分类框架中用于PE分类任务，使用初始的CXR输入提高了AUC值。该方法具有通用性，可应用于医学影像的其他跨模态转换，可能为更可访问和成本效益高的先进诊断工具铺平道路。相关代码已在GitHub上公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>介绍了从二维低对比度分辨率的X光片到三维高对比度和高分辨率CT肺动脉造影（CTPA）扫描的跨模态转换技术。</li>
<li>利用生成式人工智能的最新进展，提出了一种基于扩散的方法来完成这一任务，保证图像转化的准确性。</li>
<li>通过定量评估和放射科医生的定性反馈验证了生成的图像具有诊断意义。</li>
<li>合成的三维图像在分类框架中用于PE分类任务，提高了AUC值，证明了其在临床应用中的潜力。</li>
<li>该方法具有通用性，可应用于医学影像的其他跨模态转换任务。</li>
<li>该技术可能为更可访问和成本效益高的先进诊断工具铺平道路，有助于降低医疗成本和提高诊断效率。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.16109">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9027e75e4728efeed5ad85a3ced1c23c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e721c059ab2a7c89abe409b5dbca121.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9977c7f348965cc793d1cb3abbde5cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e6cce5da0d9f2fd47747b24125081b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a9d36f50faae489d65751564b094fc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9517987366350cad3849074b0676bdeb.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-46c694119abd21be5efe2a3d91cd6519.jpg" class="responsive-img" alt="视频理解">
                        
                        <span class="card-title">视频理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            视频理解 方向最新论文已更新，请持续关注 Update in 2025-08-01  Towards Video Thinking Test A Holistic Benchmark for Advanced Video   Reasoning and Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    视频理解
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">视频理解</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1107db2ae4fd74f81995026a3cd2516c.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-08-01  Rule2Text Natural Language Explanation of Logical Rules in Knowledge   Graphs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26024.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
