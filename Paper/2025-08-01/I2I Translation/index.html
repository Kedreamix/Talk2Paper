<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  Explainable Image Classification with Reduced Overconfidence for Tissue   Characterisation">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4ece756ddbcbb9cb90df7645521e10d1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.7k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-01-æ›´æ–°"><a href="#2025-08-01-æ›´æ–°" class="headerlink" title="2025-08-01 æ›´æ–°"></a>2025-08-01 æ›´æ–°</h1><h2 id="Explainable-Image-Classification-with-Reduced-Overconfidence-for-Tissue-Characterisation"><a href="#Explainable-Image-Classification-with-Reduced-Overconfidence-for-Tissue-Characterisation" class="headerlink" title="Explainable Image Classification with Reduced Overconfidence for Tissue   Characterisation"></a>Explainable Image Classification with Reduced Overconfidence for Tissue   Characterisation</h2><p><strong>Authors:Alfie Roddan, Chi Xu, Serine Ajlouni, Irini Kakaletri, Patra Charalampaki, Stamatia Giannarou</strong></p>
<p>The deployment of Machine Learning models intraoperatively for tissue characterisation can assist decision making and guide safe tumour resections. For image classification models, pixel attribution methods are popular to infer explainability. However, overconfidence in deep learning modelâ€™s predictions translates to overconfidence in pixel attribution. In this paper, we propose the first approach which incorporates risk estimation into a pixel attribution method for improved image classification explainability. The proposed method iteratively applies a classification model with a pixel attribution method to create a volume of PA maps. This volume is used for the first time, to generate a pixel-wise distribution of PA values. We introduce a method to generate an enhanced PA map by estimating the expectation values of the pixel-wise distributions. In addition, the coefficient of variation (CV) is used to estimate pixel-wise risk of this enhanced PA map. Hence, the proposed method not only provides an improved PA map but also produces an estimation of risk on the output PA values. Performance evaluation on probe-based Confocal Laser Endomicroscopy (pCLE) data and ImageNet verifies that our improved explainability method outperforms the state-of-the-art. </p>
<blockquote>
<p>åœ¨æ‰‹æœ¯è¿‡ç¨‹ä¸­éƒ¨ç½²æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œç»„ç»‡ç‰¹å¾åˆ†æï¼Œå¯ååŠ©å†³ç­–å¹¶å¼•å¯¼å®‰å…¨åœ°è¿›è¡Œè‚¿ç˜¤åˆ‡é™¤æ‰‹æœ¯ã€‚å¯¹äºå›¾åƒåˆ†ç±»æ¨¡å‹ï¼Œåƒç´ å½’å› æ–¹æ³•æ˜¯æ¨æ–­è§£é‡Šæ€§çš„æµè¡Œæ–¹æ³•ã€‚ç„¶è€Œï¼Œå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹è¿‡åº¦è‡ªä¿¡ä¼šè½¬åŒ–ä¸ºå¯¹åƒç´ å½’å› çš„è¿‡åº¦è‡ªä¿¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–æ¬¡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå°†é£é™©ä¼°è®¡çº³å…¥åƒç´ å½’å› æ–¹æ³•ï¼Œä»¥æé«˜å›¾åƒåˆ†ç±»çš„è§£é‡Šæ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•è¿­ä»£åœ°å°†åˆ†ç±»æ¨¡å‹ä¸åƒç´ å½’å› æ–¹æ³•ç›¸ç»“åˆï¼Œä»¥åˆ›å»ºPAæ˜ å°„å·ã€‚è¯¥å·è¢«é¦–æ¬¡ç”¨äºç”Ÿæˆåƒç´ çº§çš„PAå€¼åˆ†å¸ƒã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§é€šè¿‡ä¼°è®¡åƒç´ çº§åˆ†å¸ƒçš„æœŸæœ›å€¼æ¥ç”Ÿæˆå¢å¼ºå‹PAå›¾çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¿˜ä½¿ç”¨å˜å¼‚ç³»æ•°æ¥ä¼°è®¡å¢å¼ºå‹PAå›¾çš„åƒç´ çº§é£é™©ã€‚å› æ­¤ï¼Œæ‰€æå‡ºçš„æ–¹æ³•ä¸ä»…æä¾›äº†æ”¹è¿›çš„PAå›¾ï¼Œè€Œä¸”è¿˜åœ¨è¾“å‡ºPAå€¼ä¸Šäº§ç”Ÿäº†é£é™©ä¼°è®¡ã€‚åŸºäºæ¢é’ˆçš„å…±ç„¦æ¿€å…‰å†…çª¥é•œï¼ˆpCLEï¼‰æ•°æ®å’ŒImageNetçš„æ€§èƒ½è¯„ä¼°è¯å®ï¼Œæˆ‘ä»¬æ”¹è¿›çš„è§£é‡Šæ–¹æ³•ä¼˜äºå½“å‰æœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23709v1">PDF</a> </p>
<p><strong>Summary</strong><br>æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ‰‹æœ¯ä¸­å¯¹ç»„ç»‡è¿›è¡Œè¡¨å¾å¯è¾…åŠ©å†³ç­–å¹¶å¼•å¯¼å®‰å…¨è‚¿ç˜¤åˆ‡é™¤ã€‚å›¾åƒåˆ†ç±»æ¨¡å‹é€šå¸¸é‡‡ç”¨åƒç´ å½’å› æ–¹æ³•æ¨æ–­è§£é‡Šæ€§ã€‚ç„¶è€Œï¼Œå¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹çš„è¿‡åº¦è‡ªä¿¡ä¼šå¯¼è‡´åƒç´ å½’å› çš„è¿‡åº¦è‡ªä¿¡ã€‚æœ¬æ–‡é¦–æ¬¡å°†é£é™©ä¼°è®¡èå…¥åƒç´ å½’å› æ–¹æ³•ï¼Œä»¥æé«˜å›¾åƒåˆ†ç±»çš„è§£é‡Šæ€§ã€‚è¯¥æ–¹æ³•é€šè¿‡è¿­ä»£åº”ç”¨åˆ†ç±»æ¨¡å‹å’Œåƒç´ å½’å› æ–¹æ³•åˆ›å»ºåƒç´ çº§å½’å› åœ°å›¾å·ç§¯ã€‚é¦–æ¬¡ä½¿ç”¨æ­¤å·ç§¯ç”Ÿæˆåƒç´ çº§å½’å› å€¼åˆ†å¸ƒï¼Œå¹¶å¼•å…¥ä¸€ç§æ–¹æ³•ç”Ÿæˆå¢å¼ºå‹å½’å› åœ°å›¾ï¼Œé€šè¿‡ä¼°è®¡æœŸæœ›å€¼æ¥è¯„ä¼°åƒç´ çº§é£é™©ã€‚å› æ­¤ï¼Œè¯¥æ–¹æ³•ä¸ä»…æä¾›äº†å¢å¼ºçš„å½’å› åœ°å›¾ï¼Œè¿˜è¾“å‡ºäº†å½’å› å€¼çš„é¢„ä¼°é£é™©ã€‚åœ¨åŸºäºæ¢é’ˆçš„å…±ç„¦æ¿€å…‰æ˜¾å¾®å†…é•œæ•°æ®å’ŒImageNetä¸Šçš„æ€§èƒ½è¯„ä¼°è¯æ˜ï¼Œæ”¹è¿›çš„è§£é‡Šæ€§æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ‰‹æœ¯ä¸­ç”¨äºç»„ç»‡è¡¨å¾å¯ä»¥æé«˜å†³ç­–æ•ˆç‡å’ŒæŒ‡å¯¼å®‰å…¨è‚¿ç˜¤åˆ‡é™¤ã€‚</li>
<li>åƒç´ å½’å› æ–¹æ³•æ˜¯æ¨æ–­å›¾åƒåˆ†ç±»æ¨¡å‹è§£é‡Šæ€§çš„å¸¸ç”¨æ‰‹æ®µï¼Œä½†éœ€é¿å…è¿‡åº¦è‡ªä¿¡ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡ç»“åˆé£é™©ä¼°è®¡ä¸åƒç´ å½’å› æ–¹æ³•ï¼Œæé«˜å›¾åƒåˆ†ç±»è§£é‡Šæ€§ã€‚</li>
<li>é€šè¿‡è¿­ä»£åº”ç”¨åˆ†ç±»æ¨¡å‹å’Œåƒç´ å½’å› æ–¹æ³•åˆ›å»ºå½’å› åœ°å›¾å·ç§¯ï¼Œå¹¶åˆ©ç”¨æ­¤å·ç§¯ç”Ÿæˆåƒç´ çº§å½’å› å€¼åˆ†å¸ƒã€‚</li>
<li>å¼•å…¥ä¸€ç§æ–¹æ³•ç”Ÿæˆå¢å¼ºå‹å½’å› åœ°å›¾ï¼Œé€šè¿‡ä¼°è®¡æœŸæœ›å€¼è¯„ä¼°åƒç´ çº§é£é™©ã€‚</li>
<li>æ‰€ææ–¹æ³•ä¸ä»…ä¼˜åŒ–äº†è§£é‡Šæ€§ï¼Œè¿˜èƒ½å¯¹å½’å› å€¼çš„é£é™©è¿›è¡Œé¢„ä¼°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23709">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a3fdd579a42627fcec76c58e7deee1da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-730c3e932355cace58122d41a4768e84.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ddeaa1ca02fbc983d3d789572012aa1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Hyperbolic-Cycle-Alignment-for-Infrared-Visible-Image-Fusion"><a href="#Hyperbolic-Cycle-Alignment-for-Infrared-Visible-Image-Fusion" class="headerlink" title="Hyperbolic Cycle Alignment for Infrared-Visible Image Fusion"></a>Hyperbolic Cycle Alignment for Infrared-Visible Image Fusion</h2><p><strong>Authors:Timing Li, Bing Cao, Jiahe Feng, Haifang Cao, Qinghau Hu, Pengfei Zhu</strong></p>
<p>Image fusion synthesizes complementary information from multiple sources, mitigating the inherent limitations of unimodal imaging systems. Accurate image registration is essential for effective multi-source data fusion. However, existing registration methods, often based on image translation in Euclidean space, fail to handle cross-modal misalignment effectively, resulting in suboptimal alignment and fusion quality. To overcome this limitation, we explore image alignment in non-Euclidean space and propose a Hyperbolic Cycle Alignment Network (Hy-CycleAlign). To the best of our knowledge, Hy-CycleAlign is the first image registration method based on hyperbolic space. It introduces a dual-path cross-modal cyclic registration framework, in which a forward registration network aligns cross-modal inputs, while a backward registration network reconstructs the original image, forming a closed-loop registration structure with geometric consistency. Additionally, we design a Hyperbolic Hierarchy Contrastive Alignment (H$^{2}$CA) module, which maps images into hyperbolic space and imposes registration constraints, effectively reducing interference caused by modality discrepancies. We further analyze image registration in both Euclidean and hyperbolic spaces, demonstrating that hyperbolic space enables more sensitive and effective multi-modal image registration. Extensive experiments on misaligned multi-modal images demonstrate that our method significantly outperforms existing approaches in both image alignment and fusion. Our code will be publicly available. </p>
<blockquote>
<p>å›¾åƒèåˆé€šè¿‡ç»¼åˆæ¥è‡ªå¤šä¸ªæºå¤´çš„äº’è¡¥ä¿¡æ¯ï¼Œç¼“è§£äº†å•æ¨¡æ€æˆåƒç³»ç»Ÿçš„å›ºæœ‰å±€é™æ€§ã€‚å‡†ç¡®çš„å›¾åƒé…å‡†å¯¹äºæœ‰æ•ˆçš„å¤šæºæ•°æ®èåˆè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é…å‡†æ–¹æ³•é€šå¸¸åŸºäºæ¬§å‡ é‡Œå¾—ç©ºé—´çš„å›¾åƒç¿»è¯‘ï¼Œæ— æ³•æœ‰æ•ˆåœ°å¤„ç†è·¨æ¨¡æ€çš„ä¸å¯¹å‡†é—®é¢˜ï¼Œå¯¼è‡´é…å‡†å’Œèåˆè´¨é‡ä¸ä½³ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æ¢ç´¢äº†éæ¬§å‡ é‡Œå¾—ç©ºé—´çš„å›¾åƒé…å‡†ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŒæ›²å¾ªç¯é…å‡†ç½‘ç»œï¼ˆHy-CycleAlignï¼‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒHy-CycleAlignæ˜¯åŸºäºåŒæ›²ç©ºé—´çš„ç¬¬ä¸€ä¸ªå›¾åƒé…å‡†æ–¹æ³•ã€‚å®ƒå¼•å…¥äº†ä¸€ä¸ªåŒè·¯å¾„è·¨æ¨¡æ€å¾ªç¯é…å‡†æ¡†æ¶ï¼Œå…¶ä¸­å‰å‘é…å‡†ç½‘ç»œå¯¹é½è·¨æ¨¡æ€è¾“å…¥ï¼Œè€Œåå‘é…å‡†ç½‘ç»œé‡å»ºåŸå§‹å›¾åƒï¼Œå½¢æˆä¸€ä¸ªå…·æœ‰å‡ ä½•ä¸€è‡´æ€§çš„é—­ç¯é…å‡†ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŒæ›²å±‚æ¬¡å¯¹æ¯”é…å‡†ï¼ˆHÂ²CAï¼‰æ¨¡å—ï¼Œå®ƒå°†å›¾åƒæ˜ å°„åˆ°åŒæ›²ç©ºé—´å¹¶æ–½åŠ é…å‡†çº¦æŸï¼Œæœ‰æ•ˆå‡å°‘äº†ç”±æ¨¡æ€å·®å¼‚å¼•èµ·çš„å¹²æ‰°ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†æ¬§å‡ é‡Œå¾—ç©ºé—´å’ŒåŒæ›²ç©ºé—´ä¸­çš„å›¾åƒé…å‡†ï¼Œè¡¨æ˜åŒæ›²ç©ºé—´èƒ½å¤Ÿå®ç°æ›´æ•æ„Ÿå’Œæœ‰æ•ˆçš„å¤šæ¨¡æ€å›¾åƒé…å‡†ã€‚åœ¨å¯¹é”™ä½å¤šæ¨¡æ€å›¾åƒçš„å¤§é‡å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒé…å‡†å’Œèåˆæ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23508v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å›¾åƒèåˆé€šè¿‡åˆæˆæ¥è‡ªå¤šä¸ªæºå¤´çš„äº’è¡¥ä¿¡æ¯ï¼Œå‡è½»äº†å•æ¨¡æ€æˆåƒç³»ç»Ÿçš„å›ºæœ‰å±€é™æ€§ã€‚å‡†ç¡®å›¾åƒé…å‡†æ˜¯å®ç°å¤šæºæ•°æ®æœ‰æ•ˆèåˆçš„å…³é”®ã€‚ç„¶è€Œï¼Œç°æœ‰çš„é…å‡†æ–¹æ³•é€šå¸¸åŸºäºæ¬§å‡ é‡Œå¾—ç©ºé—´çš„å›¾åƒç¿»è¯‘ï¼Œæ— æ³•æœ‰æ•ˆåœ°å¤„ç†è·¨æ¨¡æ€çš„è¯¯å¯¹é½é—®é¢˜ï¼Œå¯¼è‡´é…å‡†å’Œèåˆæ•ˆæœä¸ä½³ã€‚ä¸ºå…‹æœè¿™ä¸€å±€é™ï¼Œæˆ‘ä»¬æ¢ç´¢äº†åœ¨éæ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­çš„å›¾åƒå¯¹é½ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŒè·¯å¾„è¶…å¾ªç¯å¯¹é½ç½‘ç»œï¼ˆHy-CycleAlignï¼‰ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒHy-CycleAlignæ˜¯åŸºäºåŒæ›²ç©ºé—´çš„ç¬¬ä¸€ä¸ªå›¾åƒé…å‡†æ–¹æ³•ã€‚å®ƒå¼•å…¥äº†ä¸€ä¸ªåŒè·¯å¾„è·¨æ¨¡æ€å¾ªç¯é…å‡†æ¡†æ¶ï¼Œå…¶ä¸­æ­£å‘é…å‡†ç½‘ç»œå¯¹é½è·¨æ¨¡æ€è¾“å…¥ï¼Œåå‘é…å‡†ç½‘ç»œé‡å»ºåŸå§‹å›¾åƒï¼Œå½¢æˆä¸€ä¸ªå…·æœ‰å‡ ä½•ä¸€è‡´æ€§çš„é—­ç¯é…å‡†ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†è¶…å±‚æ¬¡å¯¹æ¯”å¯¹é½æ¨¡å—ï¼ˆHÂ²CAï¼‰ï¼Œå°†å›¾åƒæ˜ å°„åˆ°åŒæ›²ç©ºé—´å¹¶æ–½åŠ é…å‡†çº¦æŸï¼Œæœ‰æ•ˆé™ä½ç”±æ¨¡æ€å·®å¼‚å¼•èµ·çš„å¹²æ‰°ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥åˆ†æäº†æ¬§å‡ é‡Œå¾—ç©ºé—´å’ŒåŒæ›²ç©ºé—´ä¸­çš„å›¾åƒé…å‡†ï¼Œè¡¨æ˜åŒæ›²ç©ºé—´èƒ½å¤Ÿå®ç°æ›´æ•æ„Ÿå’Œæœ‰æ•ˆçš„å¤šæ¨¡æ€å›¾åƒé…å‡†ã€‚åœ¨è¯¯å¯¹é½çš„å¤šæ¨¡æ€å›¾åƒä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å›¾åƒé…å‡†å’Œèåˆæ–¹é¢éƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒèåˆæ—¨åœ¨ç»“åˆå¤šæºä¿¡æ¯ä»¥å…‹æœå•æ¨¡æ€æˆåƒçš„å±€é™æ€§ã€‚</li>
<li>å‡†ç¡®é…å‡†æ˜¯å®ç°å¤šæºæ•°æ®æœ‰æ•ˆèåˆçš„å…³é”®æ­¥éª¤ã€‚</li>
<li>ç°æœ‰åŸºäºæ¬§å‡ é‡Œå¾—ç©ºé—´çš„é…å‡†æ–¹æ³•åœ¨å¤„ç†è·¨æ¨¡æ€è¯¯å¯¹é½æ—¶è¡¨ç°æœ‰é™ã€‚</li>
<li>æå‡ºäº†åŸºäºéæ¬§å‡ é‡Œå¾—ç©ºé—´çš„å›¾åƒé…å‡†æ–¹æ³•â€”â€”Hy-CycleAlignç½‘ç»œã€‚</li>
<li>Hy-CycleAlignç½‘ç»œé‡‡ç”¨åŒè·¯å¾„è·¨æ¨¡æ€å¾ªç¯é…å‡†æ¡†æ¶ï¼ŒåŒ…å«æ­£å‘å’Œåå‘é…å‡†ç½‘ç»œã€‚</li>
<li>HÂ²CAæ¨¡å—ç”¨äºå°†å›¾åƒæ˜ å°„åˆ°åŒæ›²ç©ºé—´å¹¶æ–½åŠ é…å‡†çº¦æŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23508">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0c636eb8ce00e7b3def37164fc523e5d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-189c1f0f8b615011c3e9ab26a0ab693c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0281d7756a852d54685c21b5b21848cf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e49dcff49153df90b6cb16a3db7b6a2.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Early-Goal-Guided-Multi-Scale-Fusion-for-Real-Time-Vision-Language-Driving"><a href="#Early-Goal-Guided-Multi-Scale-Fusion-for-Real-Time-Vision-Language-Driving" class="headerlink" title="Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language   Driving"></a>Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language   Driving</h2><p><strong>Authors:Santosh Patapati, Trisanth Srinivasan</strong></p>
<p>Autonomous vehicles must react in milliseconds while reasoning about road geometry and traffic intent to navigate complex situations. We introduce NovaDrive, a single-branch vision-language architecture that processes front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a single branch. A lightweight, two-stage cross-attention block first aligns waypoint tokens with the HD map, then refines attention over fine-grained image and depth patches. Coupled with a novel smoothness loss that discourages abrupt steering and speed changes, this design eliminates the need for recurrent memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language backbone, enabling real-time inference. On the nuScenes &#x2F; Waymo subset of the MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from 2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention fusion each contribute the most to these gains. Beyond safety, NovaDriveâ€™s shorter routes (resulting from the novel smoothness loss) translate to lower fuel or battery usage, pointing toward leaner, more easily updated driving stacks. NovaDrive can be extended to other embodied-AI domains as well. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†NovaDriveï¼Œè¿™æ˜¯ä¸€ç§å•åˆ†æ”¯è§†è§‰è¯­è¨€æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªåˆ†æ”¯ä¸­å¤„ç†å‰ç½®æ‘„åƒå¤´å›¾åƒã€é«˜æ¸…åœ°å›¾ç“¦ç‰‡ã€æ¿€å…‰é›·è¾¾æ·±åº¦å’Œæ–‡æœ¬åæ ‡ç‚¹ã€‚å…¶é€šè¿‡ä¸€ä¸ªè½»é‡çº§çš„ä¸¤é˜¶æ®µäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œé¦–å…ˆå¯¹é½åæ ‡ç‚¹ä¸é«˜æ¸…åœ°å›¾ï¼Œç„¶åç²¾ç»†è°ƒæ•´å›¾åƒå’Œæ·±åº¦è¡¥ä¸çš„æ³¨æ„åŠ›ã€‚ç»“åˆä¸€ç§æ–°å‹å¹³æ»‘æŸå¤±ï¼Œè¯¥è®¾è®¡å¯é¿å…æ€¥å‰§è½¬å‘å’Œé€Ÿåº¦å˜åŒ–ï¼Œä»è€Œæ— éœ€ä½¿ç”¨å¾ªç¯å†…å­˜ã€‚æˆ‘ä»¬å¾®è°ƒäº†è§„æ¨¡ä¸º11Bçš„LLaMA-3.2è§†è§‰è¯­è¨€ä¸»å¹²çš„å‰15å±‚ï¼Œä»¥å®ç°å®æ—¶æ¨ç†ã€‚åœ¨MD-NEXå®¤å¤–åŸºå‡†çš„nuScenes&#x2F;Waymoå­é›†ä¸Šï¼ŒNovaDriveå°†æˆåŠŸç‡æå‡è‡³84%ï¼ˆ+4%ï¼‰ï¼Œè·¯å¾„æ•ˆç‡ï¼ˆSPLï¼‰æé«˜åˆ°0.66ï¼ˆ+0.11ï¼‰ï¼Œä¸ä¹‹å‰çš„æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œç¢°æ’é¢‘ç‡ä»2.6%é™è‡³1.2%ï¼ˆ-1.4%ï¼‰ã€‚æˆ‘ä»¬çš„å‰–æç ”ç©¶è¯å®ï¼Œåæ ‡ç‚¹æ ‡è®°ã€éƒ¨åˆ†VLMå¾®è°ƒä»¥åŠäº¤å‰æ³¨æ„åŠ›èåˆç­‰éƒ½å¯¹è¿™äº›å¢ç›Šè´¡çŒ®æœ€å¤§ã€‚é™¤äº†æé«˜å®‰å…¨æ€§å¤–ï¼ŒNovaDriveçš„è¾ƒçŸ­è·¯çº¿ï¼ˆç”±æ–°å‹å¹³æ»‘æŸå¤±äº§ç”Ÿï¼‰è½¬åŒ–ä¸ºæ›´ä½çš„ç‡ƒæ²¹æˆ–ç”µæ± ä½¿ç”¨ç‡ï¼ŒæŒ‡å‘æ›´ç²¾ç®€ã€æ›´å®¹æ˜“æ›´æ–°çš„é©¾é©¶å †æ ˆã€‚NovaDriveè¿˜å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å®ä½“AIé¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23042v1">PDF</a> 6 pages</p>
<p><strong>Summary</strong></p>
<p>åœ¨è‡ªåŠ¨é©¾é©¶è¿‡ç¨‹ä¸­ï¼Œå¤„ç†è§†è§‰ä¿¡æ¯å¯¹äºè½¦è¾†åº”å¯¹å¤æ‚é“è·¯å’Œäº¤é€šç¯å¢ƒè‡³å…³é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†NovaDriveç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ç§å•ä¸€åˆ†æ”¯çš„è§†è§‰è¯­è¨€æ¶æ„ï¼Œå¯ä»¥å¤„ç†å‰è§†æ‘„åƒå¤´å›¾åƒã€é«˜æ¸…åœ°å›¾åˆ‡ç‰‡ã€æ¿€å…‰é›·è¾¾æ·±åº¦ä»¥åŠæ–‡æœ¬å¯¼èˆªç‚¹ç­‰ä¿¡æ¯ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä¸€ç§æ–°é¢–çš„è·¨æ³¨æ„åŠ›æœºåˆ¶å®ç°äº†å›¾åƒä¸å¯¼èˆªç‚¹çš„ç²¾å‡†å¯¹é½ï¼Œå¹¶ç»“åˆäº†ä¸€ç§æ–°å‹çš„å¹³æ»‘æŸå¤±å‡½æ•°ï¼Œå‡å°‘äº†è½¦è¾†çš„æ€¥è½¬å¼¯å’Œé€Ÿåº¦å˜åŒ–ã€‚NovaDriveåœ¨MD-NEXæˆ·å¤–åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ï¼ŒæˆåŠŸç‡å’Œè·¯å¾„æ•ˆç‡å‡æœ‰æ‰€æé«˜ï¼Œç¢°æ’é¢‘ç‡é™ä½ã€‚æ­¤å¤–ï¼ŒNovaDriveè¿˜å¯ä»¥ç¼©çŸ­è¡Œé©¶è·¯çº¿ï¼Œé™ä½ç‡ƒæ²¹æˆ–ç”µæ± æ¶ˆè€—ï¼Œå¹¶æœ‰æœ›åº”ç”¨äºå…¶ä»–æ™ºèƒ½ä½“é¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NovaDriveç³»ç»Ÿæ˜¯ä¸€ä¸ªå•ä¸€çš„è§†è§‰è¯­è¨€æ¶æ„ï¼Œå¯ä»¥å¤„ç†å¤šç§é©¾é©¶ç›¸å…³ä¿¡æ¯çš„è¾“å…¥ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡æ–°é¢–çš„è·¨æ³¨æ„åŠ›æœºåˆ¶å®ç°å›¾åƒä¸å¯¼èˆªç‚¹çš„ç²¾å‡†å¯¹é½ã€‚</li>
<li>NovaDriveç»“åˆäº†æ–°å‹çš„å¹³æ»‘æŸå¤±å‡½æ•°ï¼Œå‡å°‘è½¦è¾†çš„æ€¥è½¬å¼¯å’Œé€Ÿåº¦å˜åŒ–ã€‚</li>
<li>NovaDriveåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒæˆåŠŸç‡å’Œè·¯å¾„æ•ˆç‡æé«˜ï¼Œç¢°æ’é¢‘ç‡é™ä½ã€‚</li>
<li>NovaDriveå¯ç¼©çŸ­è¡Œé©¶è·¯çº¿ï¼Œé™ä½ç‡ƒæ²¹æˆ–ç”µæ± æ¶ˆè€—ã€‚</li>
<li>NovaDriveç³»ç»Ÿå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œå¯åº”ç”¨äºå…¶ä»–æ™ºèƒ½ä½“é¢†åŸŸã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23042">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f45317744d2a0b01d17135446177089b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-47c25fd592560a81f1b14d658fe4d6ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1daad611d0cc6880e350af0df114029f.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Meta-CLIP-2-A-Worldwide-Scaling-Recipe"><a href="#Meta-CLIP-2-A-Worldwide-Scaling-Recipe" class="headerlink" title="Meta CLIP 2: A Worldwide Scaling Recipe"></a>Meta CLIP 2: A Worldwide Scaling Recipe</h2><p><strong>Authors:Yung-Sung Chuang, Yang Li, Dong Wang, Ching-Feng Yeh, Kehan Lyu, Ramya Raghavendra, James Glass, Lifei Huang, Jason Weston, Luke Zettlemoyer, Xinlei Chen, Zhuang Liu, Saining Xie, Wen-tau Yih, Shang-Wen Li, Hu Xu</strong></p>
<p>Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIPâ€™s training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., â€œcurse of multilingualityâ€ that is common in LLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, Meta CLIP 2 ViT-H&#x2F;14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval. </p>
<blockquote>
<p>å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰æ˜¯ä¸€ç§æµè¡Œçš„åŸºç¡€æ¨¡å‹ï¼Œæ”¯æŒé›¶æ¬¡åˆ†ç±»ã€æ£€ç´¢å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„ç¼–ç å™¨ã€‚å°½ç®¡CLIPå·²æˆåŠŸåœ°åœ¨è‹±æ–‡ä¸–ç•Œçš„ç™¾äº¿çº§å›¾åƒæ–‡æœ¬å¯¹ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œä½†è¿›ä¸€æ­¥å°†CLIPçš„è®­ç»ƒæ‰©å±•åˆ°ä»å…¨çƒç½‘ç»œæ•°æ®ä¸Šè¿›è¡Œå­¦ä¹ ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼šï¼ˆ1ï¼‰æ²¡æœ‰å¤„ç†æ–¹æ³•å¯ä»¥ç”¨æ¥å¤„ç†éè‹±è¯­ä¸–ç•Œçš„æ•°æ®ç‚¹ï¼›ï¼ˆ2ï¼‰ç°æœ‰è·¨è¯­è¨€CLIPçš„è‹±è¯­æ€§èƒ½è¡¨ç°ä¸å¦‚å…¶è‹±è¯­ä¸“å±çš„å¯¹åº”æ¨¡å‹ï¼Œå³å¸¸è§çš„å¤šè¯­è¨€å¤§å‹æ¨¡å‹ä¸­çš„â€œå¤šè¯­è¨€è¯…å’’â€ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†Meta CLIP 2ï¼Œè¿™æ˜¯é¦–æ¬¡ä»å¤´å¼€å§‹åœ¨å…¨çƒç½‘ç»œè§„æ¨¡çš„å›¾åƒæ–‡æœ¬å¯¹ä¸Šè®­ç»ƒCLIPçš„æ–¹æ³•ã€‚ä¸ºäº†æ¨å¹¿æˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸¥æ ¼çš„å¿…è¦æœ€å°å˜åŒ–æ¶ˆèå®éªŒï¼Œä»¥è§£å†³ä¸Šè¿°æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§èƒ½ä»è‹±è¯­å’Œéè‹±è¯­ä¸–ç•Œæ•°æ®ä¸­å®ç°äº’åˆ©å…±èµ¢çš„é…æ–¹ã€‚åœ¨é›¶æ¬¡ImageNetåˆ†ç±»ä¸­ï¼ŒMeta CLIP 2 ViT-H&#x2F;14çš„ç²¾åº¦è¶…è¿‡äº†å…¶è‹±è¯­ä¸“å±çš„å¯¹åº”æ¨¡å‹0.8%ï¼Œå¹¶è¶…è¶Šäº†mSigLIP 0.7%ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯åœ¨å¤šç§å¤šè¯­è¨€åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¦‚CVQAè¾¾åˆ°57.4%ï¼ŒBabel-ImageNetè¾¾åˆ°50.2%ï¼ŒXM3600çš„å›¾åƒåˆ°æ–‡æœ¬æ£€ç´¢è¾¾åˆ°64.3%ï¼Œå¹¶ä¸”æ²¡æœ‰ä»»ä½•ç³»ç»Ÿçº§åˆ«çš„æ··æ·†å› ç´ ï¼ˆå¦‚ç¿»è¯‘ã€ä¸“ç”¨æ¶æ„æ›´æ”¹ï¼‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22062v2">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Contrastive Language-Image Pretrainingï¼ˆCLIPï¼‰æ¨¡å‹åœ¨å…¨çƒåŒ–ç½‘ç»œæ•°æ®è®­ç»ƒä¸Šçš„æŒ‘æˆ˜ï¼Œå¹¶æ¨å‡ºäº†Meta CLIP 2æ¨¡å‹ã€‚è¯¥æ¨¡å‹å¯ä»¥åœ¨å…¨çƒç½‘é¡µè§„æ¨¡å›¾åƒæ–‡æœ¬å¯¹ä¸Šä»é›¶å¼€å§‹è®­ç»ƒCLIPã€‚ç ”ç©¶é€šè¿‡ä¸¥è°¨çš„æ¶ˆèå®éªŒï¼Œè§£å†³ä¸Šè¿°æŒ‘æˆ˜å¹¶å®ç°äº†è‹±è¯­å’Œéè‹±è¯­ä¸–ç•Œæ•°æ®çš„äº’åˆ©ã€‚åœ¨é›¶æ ·æœ¬ImageNetåˆ†ç±»ä¸­ï¼ŒMeta CLIP 2è¶…è¶Šäº†ä»…é’ˆå¯¹è‹±è¯­çš„CLIPæ¨¡å‹å’ŒmSigLIPæ¨¡å‹ï¼Œå¹¶åœ¨è·¨è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>CLIPæ¨¡å‹åœ¨å…¨çƒåŒ–ç½‘ç»œæ•°æ®è®­ç»ƒä¸Šå­˜åœ¨æŒ‘æˆ˜ï¼Œç¼ºä¹å¤„ç†éè‹±è¯­ä¸–ç•Œæ•°æ®ç‚¹çš„ç­–ç•¥ã€‚</li>
<li>Meta CLIP 2æ˜¯é¦–ä¸ªä»å¤´å¼€å§‹åœ¨å…¨çƒç½‘é¡µè§„æ¨¡å›¾åƒæ–‡æœ¬å¯¹ä¸Šè®­ç»ƒçš„CLIPæ¨¡å‹ã€‚</li>
<li>ç ”ç©¶é€šè¿‡å¿…è¦çš„æœ€å°å˜åŒ–è¿›è¡Œäº†ä¸¥è°¨çš„æ¶ˆèå®éªŒï¼Œè§£å†³ä¸Šè¿°æŒ‘æˆ˜å¹¶å®ç°è‹±è¯­å’Œéè‹±è¯­æ•°æ®çš„äº’åˆ©ã€‚</li>
<li>Meta CLIP 2åœ¨é›¶æ ·æœ¬ImageNetåˆ†ç±»ä¸­æ€§èƒ½å“è¶Šï¼Œè¶…è¶Šäº†ä»…é’ˆå¯¹è‹±è¯­çš„CLIPæ¨¡å‹å’ŒmSigLIPæ¨¡å‹ã€‚</li>
<li>åœ¨è·¨è¯­è¨€åŸºå‡†æµ‹è¯•ä¸­ï¼ˆå¦‚CVQAã€Babel-ImageNetå’ŒXM3600ï¼‰ï¼ŒMeta CLIP 2å®ç°äº†å›¾åƒåˆ°æ–‡æœ¬çš„æ£€ç´¢æ–°æ°´å¹³ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22062">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a53dce16ab723ffe2baaa4705aaa15e3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8175805bc79aa52c29a3d11c3daa3a95.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1b667027140ffc44f4d050d0b68d488.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f017b21fdec2b192ed3e605e24e00c86.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Semantic-Segmentation-of-iPS-Cells-Case-Study-on-Model-Complexity-in-Biomedical-Imaging"><a href="#Semantic-Segmentation-of-iPS-Cells-Case-Study-on-Model-Complexity-in-Biomedical-Imaging" class="headerlink" title="Semantic Segmentation of iPS Cells: Case Study on Model Complexity in   Biomedical Imaging"></a>Semantic Segmentation of iPS Cells: Case Study on Model Complexity in   Biomedical Imaging</h2><p><strong>Authors:Maoquan Zhang, Bisser Raytchev, Xiujuan Sun</strong></p>
<p>Medical image segmentation requires not only accuracy but also robustness under challenging imaging conditions. In this study, we show that a carefully configured DeepLabv3 model can achieve high performance in segmenting induced pluripotent stem (iPS) cell colonies, and, under our experimental conditions, outperforms large-scale foundation models such as SAM2 and its medical variant MedSAM2 without structural modifications. These results suggest that, for specialized tasks characterized by subtle, low-contrast boundaries, increased model complexity does not necessarily translate to better performance. Our work revisits the assumption that ever-larger and more generalized architectures are always preferable, and provides evidence that appropriately adapted, simpler models may offer strong accuracy and practical reliability in domain-specific biomedical applications. We also offer an open-source implementation that includes strategies for small datasets and domain-specific encoding, with the aim of supporting further advances in semantic segmentation for regenerative medicine and related fields. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸ä»…éœ€è¦å‡†ç¡®æ€§ï¼Œè¿˜éœ€è¦åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æˆåƒæ¡ä»¶ä¸‹å…·æœ‰ç¨³å¥æ€§ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ç²¾å¿ƒé…ç½®çš„DeepLabv3æ¨¡å‹åœ¨åˆ†å‰²è¯±å¯¼å¤šèƒ½å¹²ç»†èƒï¼ˆiPSï¼‰èŒè½æ–¹é¢çš„é«˜æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨æˆ‘ä»¬çš„å®éªŒæ¡ä»¶ä¸‹ï¼Œæœªç»ç»“æ„ä¿®æ”¹å³ä¼˜äºå¤§è§„æ¨¡åŸºç¡€æ¨¡å‹ï¼ˆå¦‚SAM2åŠå…¶åŒ»å­¦å˜ä½“MedSAM2ï¼‰ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¯¹äºç‰¹å¾è¡¨ç°ä¸ºç»†å¾®ã€ä½å¯¹æ¯”åº¦è¾¹ç•Œçš„ç‰¹å®šä»»åŠ¡ï¼Œå¢åŠ æ¨¡å‹å¤æ‚æ€§å¹¶ä¸ä¸€å®šæ„å‘³ç€æ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œé‡æ–°è€ƒè™‘äº†å‡è®¾æ›´å¤§çš„ã€æ›´é€šç”¨çš„æ¶æ„æ€»æ˜¯å¯å–çš„ï¼Œå¹¶æä¾›äº†è¯æ®æ”¯æŒé€‚å½“ç®€åŒ–å¹¶é€‚åº”çš„æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­å…·æœ‰å¼ºå¤§çš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå¼€æºå®ç°ï¼ŒåŒ…æ‹¬é’ˆå¯¹å°æ•°æ®é›†å’Œç‰¹å®šé¢†åŸŸç¼–ç çš„ç­–ç•¥ï¼Œæ—¨åœ¨æ”¯æŒå†ç”ŸåŒ»å­¦å’Œç›¸å…³é¢†åŸŸçš„è¯­ä¹‰åˆ†å‰²çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21608v1">PDF</a> 19th International Conference on Machine Vision Applications MVA2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶å±•ç¤ºäº†ç²¾å¿ƒé…ç½®çš„DeepLabv3æ¨¡å‹åœ¨è¯±å¯¼å¤šèƒ½å¹²ç»†èƒï¼ˆiPSï¼‰èŒè½åˆ†å‰²æ–¹é¢çš„é«˜æ€§èƒ½è¡¨ç°ã€‚ç›¸è¾ƒäºå¤§è§„æ¨¡çš„åŸºç¡€æ¨¡å‹å¦‚SAM2åŠå…¶åŒ»å­¦å˜ä½“MedSAM2ï¼Œè¯¥æ¨¡å‹åœ¨ç‰¹å®šå®éªŒæ¡ä»¶ä¸‹æ— éœ€ç»“æ„ä¿®æ”¹ä¾¿å±•ç°å‡ºæ›´ä½³æ€§èƒ½ã€‚ç»“æœæç¤ºï¼Œå¯¹äºç‰¹å¾ä¸ºç»†å¾®ã€ä½å¯¹æ¯”åº¦è¾¹ç•Œçš„ä¸“é¡¹ä»»åŠ¡ï¼Œå¢åŠ æ¨¡å‹å¤æ‚æ€§å¹¶ä¸ä¸€å®šæ„å‘³ç€æ€§èƒ½æå‡ã€‚æœ¬ç ”ç©¶é‡æ–°å®¡è§†äº†â€œè¶Šå¤§è¶Šå¹¿çš„åŸºç¡€æ¶æ„æ€»æ˜¯æ›´å¯å–â€çš„å‡è®¾ï¼Œå¹¶æä¾›è¯æ®è¡¨æ˜ï¼Œé€‚å½“ç®€åŒ–å¹¶é€‚åº”ç‰¹å®šé¢†åŸŸçš„æ¨¡å‹å¯èƒ½åœ¨ç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­æä¾›å¼ºå¤§çš„å‡†ç¡®æ€§å’Œå®é™…å¯é æ€§ã€‚æˆ‘ä»¬è¿˜æä¾›äº†åŒ…å«å°æ•°æ®é›†ç­–ç•¥å’Œé¢†åŸŸç‰¹å®šç¼–ç ç­–ç•¥çš„å¼€æºå®ç°ï¼Œæ—¨åœ¨æ”¯æŒå†ç”ŸåŒ»å­¦å’Œç›¸å…³é¢†åŸŸçš„è¯­ä¹‰åˆ†å‰²çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>DeepLabv3æ¨¡å‹åœ¨iPSç»†èƒèŒè½åˆ†å‰²ä¸­è¡¨ç°å‡ºé«˜æ€§èƒ½ã€‚</li>
<li>åœ¨ç‰¹å®šå®éªŒæ¡ä»¶ä¸‹ï¼ŒDeepLabv3æ€§èƒ½ä¼˜äºå¤§è§„æ¨¡åŸºç¡€æ¨¡å‹SAM2åŠå…¶åŒ»å­¦å˜ä½“MedSAM2ã€‚</li>
<li>å¯¹äºç»†å¾®ã€ä½å¯¹æ¯”åº¦è¾¹ç•Œçš„ä¸“é¡¹ä»»åŠ¡ï¼Œå¢åŠ æ¨¡å‹å¤æ‚æ€§å¹¶ä¸ä¸€å®šæ„å‘³ç€æ€§èƒ½æå‡ã€‚</li>
<li>ç ”ç©¶æŒ‘æˆ˜äº†â€œè¶Šå¤§è¶Šå¹¿çš„åŸºç¡€æ¶æ„æ›´å¯å–â€çš„å‡è®¾ã€‚</li>
<li>é€‚å½“ç®€åŒ–å¹¶é€‚åº”ç‰¹å®šé¢†åŸŸçš„æ¨¡å‹åœ¨ç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­å¯èƒ½æ›´å¯é ã€‚</li>
<li>æä¾›åŒ…å«å°æ•°æ®é›†ç­–ç•¥å’Œé¢†åŸŸç‰¹å®šç¼–ç ç­–ç•¥çš„å¼€æºå®ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e87f14ec6885586249392bd244b7b60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-84d9642ce16b584c3532f561fa244da6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4451222ff3b4febb53467c771cdf53e.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MaXsive-High-Capacity-and-Robust-Training-Free-Generative-Image-Watermarking-in-Diffusion-Models"><a href="#MaXsive-High-Capacity-and-Robust-Training-Free-Generative-Image-Watermarking-in-Diffusion-Models" class="headerlink" title="MaXsive: High-Capacity and Robust Training-Free Generative Image   Watermarking in Diffusion Models"></a>MaXsive: High-Capacity and Robust Training-Free Generative Image   Watermarking in Diffusion Models</h2><p><strong>Authors:Po-Yuan Mao, Cheng-Chang Tsai, Chun-Shien Lu</strong></p>
<p>The great success of the diffusion model in image synthesis led to the release of gigantic commercial models, raising the issue of copyright protection and inappropriate content generation. Training-free diffusion watermarking provides a low-cost solution for these issues. However, the prior works remain vulnerable to rotation, scaling, and translation (RST) attacks. Although some methods employ meticulously designed patterns to mitigate this issue, they often reduce watermark capacity, which can result in identity (ID) collusion. To address these problems, we propose MaXsive, a training-free diffusion model generative watermarking technique that has high capacity and robustness. MaXsive best utilizes the initial noise to watermark the diffusion model. Moreover, instead of using a meticulously repetitive ring pattern, we propose injecting the X-shape template to recover the RST distortions. This design significantly increases robustness without losing any capacity, making ID collusion less likely to happen. The effectiveness of MaXsive has been verified on two well-known watermarking benchmarks under the scenarios of verification and identification. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆæ–¹é¢çš„å·¨å¤§æˆåŠŸå‚¬ç”Ÿäº†åºå¤§çš„å•†ä¸šæ¨¡å‹ï¼Œè¿›è€Œå¼•å‘äº†ç‰ˆæƒä¿æŠ¤å’Œä¸å½“å†…å®¹ç”Ÿæˆçš„é—®é¢˜ã€‚æ— è®­ç»ƒæ‰©æ•£æ°´å°æŠ€æœ¯ä¸ºè¿™äº›é—®é¢˜æä¾›äº†ä½æˆæœ¬è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå…ˆå‰çš„å·¥ä½œä»ç„¶å®¹æ˜“å—åˆ°æ—‹è½¬ã€ç¼©æ”¾å’Œå¹³ç§»ï¼ˆRSTï¼‰æ”»å‡»ã€‚å°½ç®¡ä¸€äº›æ–¹æ³•é‡‡ç”¨ç²¾å¿ƒè®¾è®¡çš„æ¨¡å¼æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†å®ƒä»¬å¾€å¾€ä¼šé™ä½æ°´å°å®¹é‡ï¼Œä»è€Œå¯¼è‡´èº«ä»½ï¼ˆIDï¼‰ç¢°æ’ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MaXsiveï¼Œè¿™æ˜¯ä¸€ç§æ— è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆå¼æ°´å°æŠ€æœ¯ï¼Œå…·æœ‰å¤§å®¹é‡å’Œé«˜é²æ£’æ€§ã€‚MaXsiveå……åˆ†åˆ©ç”¨åˆå§‹å™ªå£°æ¥ä¸ºæ‰©æ•£æ¨¡å‹æ·»åŠ æ°´å°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„é‡å¤æ€§ç¯å½¢å›¾æ¡ˆï¼Œè€Œæ˜¯æå‡ºäº†æ³¨å…¥Xå½¢æ¨¡æ¿æ¥æ¢å¤RSTå¤±çœŸã€‚è¿™ç§è®¾è®¡åœ¨ä¸æŸå¤±å®¹é‡çš„å‰æä¸‹æ˜¾è‘—æé«˜äº†é²æ£’æ€§ï¼Œä½¿èº«ä»½ç¢°æ’çš„å¯èƒ½æ€§å¤§å¤§é™ä½ã€‚MaXsiveçš„æœ‰æ•ˆæ€§å·²åœ¨ä¸¤ä¸ªè‘—åçš„æ°´å°åŸºå‡†æµ‹è¯•åœºæ™¯ä¸‹é€šè¿‡éªŒè¯å’Œè¯†åˆ«å¾—åˆ°äº†è¯å®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21195v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒåˆæˆé¢†åŸŸçš„å·¨å¤§æˆåŠŸå¼•å‘äº†å•†ç”¨å¤§å‹æ¨¡å‹çš„æ¶Œç°ï¼Œéšä¹‹å¸¦æ¥ç‰ˆæƒä¿æŠ¤ä¸ä¸å½“å†…å®¹ç”Ÿæˆçš„é—®é¢˜ã€‚è®­ç»ƒå¼æ‰©æ•£æ°´å°æ³•ä¸ºæ­¤æä¾›äº†ä½æˆæœ¬è§£å†³æ–¹æ¡ˆï¼Œä½†ä»¥å¾€çš„æ–¹æ³•å®¹æ˜“å—åˆ°æ—‹è½¬ã€ç¼©æ”¾å’Œå¹³ç§»ï¼ˆRSTï¼‰æ”»å‡»ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºåä¸ºMaXsiveçš„è®­ç»ƒå¼æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ°´å°æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å…·æœ‰å¤§å®¹é‡å’Œé«˜é²æ£’æ€§ã€‚MaXsiveå……åˆ†åˆ©ç”¨åˆå§‹å™ªå£°è¿›è¡Œæ°´å°å¤„ç†ï¼Œå¹¶æå‡ºä½¿ç”¨Xå½¢æ¨¡æ¿æ¥æ¢å¤RSTå¤±çœŸï¼Œè€Œéé‡‡ç”¨ç²¾ç»†é‡å¤çš„ç¯å½¢å›¾æ¡ˆã€‚è¿™ä¸€è®¾è®¡åœ¨ä¸æŸå¤±å®¹é‡çš„å‰æä¸‹å¤§å¤§æé«˜äº†é²æ£’æ€§ï¼Œé™ä½äº†èº«ä»½ç¢°æ’çš„å¯èƒ½æ€§ã€‚MaXsiveåœ¨ä¸¤ä¸ªè‘—åçš„æ°´å°åŸºå‡†æµ‹è¯•ä¸Šå¾—åˆ°äº†éªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨éªŒè¯å’Œè¯†åˆ«åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹æˆåŠŸæ¨åŠ¨å•†ä¸šå¤§å‹æ¨¡å‹çš„å‘å±•ï¼Œä½†ä¹Ÿå¸¦æ¥äº†ç‰ˆæƒä¿æŠ¤å’Œä¸å½“å†…å®¹ç”Ÿæˆçš„é—®é¢˜ã€‚</li>
<li>è®­ç»ƒå¼æ‰©æ•£æ°´å°æ³•ä¸ºä¸Šè¿°é—®é¢˜æä¾›äº†ä½æˆæœ¬è§£å†³æ–¹æ¡ˆï¼Œä½†åŸæœ‰æ–¹æ³•æ˜“å—æ—‹è½¬ã€ç¼©æ”¾å’Œå¹³ç§»ï¼ˆRSTï¼‰æ”»å‡»ã€‚</li>
<li>MaXsiveæŠ€æœ¯é€šè¿‡åˆ©ç”¨åˆå§‹å™ªå£°è¿›è¡Œæ°´å°å¤„ç†æé«˜äº†æ°´å°æŠ€æœ¯çš„é²æ£’æ€§å’Œå®¹é‡ã€‚</li>
<li>MaXsiveä½¿ç”¨Xå½¢æ¨¡æ¿æ¢å¤RSTå¤±çœŸï¼Œé¿å…äº†ç²¾ç»†é‡å¤ç¯å½¢å›¾æ¡ˆçš„ä½¿ç”¨ã€‚</li>
<li>MaXsiveè®¾è®¡åœ¨ä¸æŸå¤±å®¹é‡çš„å‰æä¸‹å¢å¼ºäº†é²æ£’æ€§ï¼Œé™ä½äº†èº«ä»½ç¢°æ’çš„é£é™©ã€‚</li>
<li>MaXsiveåœ¨ä¸¤ä¸ªè‘—åçš„æ°´å°åŸºå‡†æµ‹è¯•ä¸Šç»è¿‡äº†éªŒè¯ï¼Œè¯æ˜äº†å…¶åœ¨éªŒè¯å’Œè¯†åˆ«åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>MaXsiveæŠ€æœ¯å¯¹äºä¿æŠ¤ç‰ˆæƒå’Œé˜²æ­¢ä¸å½“å†…å®¹ç”Ÿæˆå…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21195">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-109ffccca7be1430bfc9a58645420e60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-372d4a60e435297af4ad42f9ce189aa0.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a5efe157a5a4bf7ff9e8a22dd3ebea0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de32facf89a6de131b152defd3129cec.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e82f644629cc001779b7f9bc2d6bfccf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca10f209d3b961080a274b9f18209f84.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-347281603426d9f08a2fb04d4d5d78a2.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="A-Metabolic-Imaging-Integrated-Model-for-Prognostic-Prediction-in-Colorectal-Liver-Metastases"><a href="#A-Metabolic-Imaging-Integrated-Model-for-Prognostic-Prediction-in-Colorectal-Liver-Metastases" class="headerlink" title="A Metabolic-Imaging Integrated Model for Prognostic Prediction in   Colorectal Liver Metastases"></a>A Metabolic-Imaging Integrated Model for Prognostic Prediction in   Colorectal Liver Metastases</h2><p><strong>Authors:Qinlong Li, Pu Sun, Guanlin Zhu, Tianjiao Liang, Honggang QI</strong></p>
<p>Prognostic evaluation in patients with colorectal liver metastases (CRLM) remains challenging due to suboptimal accuracy of conventional clinical models. This study developed and validated a robust machine learning model for predicting postoperative recurrence risk. Preliminary ensemble models achieved exceptionally high performance (AUC $&gt;$ 0.98) but incorporated postoperative features, introducing data leakage risks. To enhance clinical applicability, we restricted input variables to preoperative baseline clinical parameters and radiomic features from contrast-enhanced CT imaging, specifically targeting recurrence prediction at 3, 6, and 12 months postoperatively. The 3-month recurrence prediction model demonstrated optimal performance with an AUC of 0.723 in cross-validation. Decision curve analysis revealed that across threshold probabilities of 0.55-0.95, the model consistently provided greater net benefit than â€œtreat-allâ€ or â€œtreat-noneâ€ strategies, supporting its utility in postoperative surveillance and therapeutic decision-making. This study successfully developed a robust predictive model for early CRLM recurrence with confirmed clinical utility. Importantly, it highlights the critical risk of data leakage in clinical prognostic modeling and proposes a rigorous framework to mitigate this issue, enhancing model reliability and translational value in real-world settings. </p>
<blockquote>
<p>åœ¨ç»“ç›´è‚ ç™Œè‚è½¬ç§»ï¼ˆCRLMï¼‰æ‚£è€…çš„é¢„åè¯„ä¼°ä¸­ï¼Œç”±äºä¼ ç»Ÿä¸´åºŠæ¨¡å‹çš„å‡†ç¡®æ€§ä¸é«˜ï¼Œä»é¢ä¸´æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶å¼€å‘å¹¶éªŒè¯äº†ä¸€ä¸ªç¨³å¥çš„æœºå™¨å­¦ä¹ ä»»åŠ¡å­¦ä¹ æ¨¡å‹æ¥é¢„æµ‹æœ¯åå¤å‘é£é™©ã€‚åˆæ­¥é›†åˆæ¨¡å‹è¡¨ç°å¼‚å¸¸å‡ºè‰²ï¼ˆAUCå¤§äº0.98ï¼‰ï¼Œä½†åŒ…å«äº†æœ¯åç‰¹å¾ï¼Œå­˜åœ¨æ•°æ®æ³„éœ²é£é™©ã€‚ä¸ºäº†æé«˜ä¸´åºŠå®ç”¨æ€§ï¼Œæˆ‘ä»¬å°†è¾“å…¥å˜é‡é™åˆ¶ä¸ºæœ¯å‰åŸºçº¿ä¸´åºŠå‚æ•°å’Œå¢å¼ºCTæˆåƒçš„æ”¾å°„å­¦ç‰¹å¾ï¼Œç‰¹åˆ«é’ˆå¯¹æœ¯å3ä¸ªæœˆã€6ä¸ªæœˆå’Œ12ä¸ªæœˆçš„å¤å‘é¢„æµ‹ã€‚3ä¸ªæœˆå¤å‘é¢„æµ‹æ¨¡å‹åœ¨äº¤å‰éªŒè¯ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼ŒAUCä¸º0.723ã€‚å†³ç­–æ›²çº¿åˆ†æè¡¨æ˜ï¼Œåœ¨é˜ˆæ¦‚ç‡0.55-0.95èŒƒå›´å†…ï¼Œè¯¥æ¨¡å‹æä¾›çš„å‡€æ•ˆç›Šå§‹ç»ˆå¤§äºâ€œå…¨éƒ¨æ²»ç–—â€æˆ–â€œä¸æ²»ç–—â€ç­–ç•¥ï¼Œè¿™æ”¯æŒå…¶åœ¨æœ¯åç›‘æµ‹å’Œæ²»ç–—å†³ç­–ä¸­çš„å®ç”¨æ€§ã€‚æœ¬ç ”ç©¶æˆåŠŸåœ°ä¸ºæ—©æœŸCRLMå¤å‘æ„å»ºäº†ä¸€ä¸ªç¨³å¥çš„é¢„æµ‹æ¨¡å‹ï¼Œå¹¶è¯å®äº†å…¶åœ¨ä¸´åºŠä¸Šçš„å®ç”¨æ€§ã€‚é‡è¦çš„æ˜¯ï¼Œå®ƒå¼ºè°ƒäº†ä¸´åºŠé¢„åæ¨¡å‹ä¸­æ•°æ®æ³„éœ²çš„å…³é”®é£é™©ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªä¸¥æ ¼çš„æ¡†æ¶æ¥å‡è½»è¿™ä¸ªé—®é¢˜ï¼Œæé«˜äº†æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œç¯å¢ƒä¸­çš„å¯é æ€§å’Œç¿»è¯‘ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19734v1">PDF</a> 8 pages,4 figues</p>
<p><strong>Summary</strong>ï¼š</p>
<p>é’ˆå¯¹ç»“ç›´è‚ ç™Œè‚è½¬ç§»æ‚£è€…çš„é¢„åè¯„ä¼°ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¼ ç»Ÿä¸´åºŠæ¨¡å‹çš„å‡†ç¡®åº¦æœ‰å¾…æé«˜ã€‚æœ¬ç ”ç©¶å¼€å‘å¹¶éªŒè¯äº†ä¸€ç§ç¨³å¥çš„æœºå™¨å­¦æ¨¡å‹ä»¥é¢„æµ‹æœ¯åå¤å‘é£é™©ã€‚ç ”ç©¶åˆæœŸé›†åˆæ¨¡å‹è¡¨ç°å‡ºæé«˜æ€§èƒ½ï¼ˆAUCï¼0.98ï¼‰ï¼Œä½†çº³å…¥æœ¯åç‰¹å¾ï¼Œå­˜åœ¨æ•°æ®æ³„éœ²é£é™©ã€‚ä¸ºæé«˜ä¸´åºŠå®ç”¨æ€§ï¼Œç ”ç©¶ä½¿ç”¨æœ¯å‰åŸºçº¿ä¸´åºŠå‚æ•°å’Œå¢å¼ºCTæˆåƒçš„æ”¾å°„ç»„å­¦ç‰¹å¾ä½œä¸ºè¾“å…¥å˜é‡ï¼Œä¸“é—¨é’ˆå¯¹æœ¯å3ä¸ªæœˆã€6ä¸ªæœˆå’Œ1å¹´çš„å¤å‘é¢„æµ‹å»ºæ¨¡ã€‚å…¶ä¸­ï¼Œæœ¯å3ä¸ªæœˆå¤å‘é¢„æµ‹æ¨¡å‹çš„äº¤å‰éªŒè¯è¡¨ç°æœ€ä½³ï¼ŒAUCä¸º0.723ã€‚å†³ç­–æ›²çº¿åˆ†æè¡¨æ˜ï¼Œåœ¨é˜ˆæ¦‚ç‡0.55è‡³0.95èŒƒå›´å†…ï¼Œè¯¥æ¨¡å‹æä¾›çš„å‡€æ•ˆç›Šé«˜äºâ€œä¸€å¾‹æ²»ç–—â€æˆ–â€œä¸€å¾‹ä¸æ²»ç–—â€çš„ç­–ç•¥ï¼Œè¯å®äº†å…¶åœ¨æœ¯åç›‘æµ‹å’Œæ²»ç–—å†³ç­–ä¸­çš„å®ç”¨æ€§ã€‚æœ¬ç ”ç©¶æˆåŠŸå¼€å‘äº†ä¸€ä¸ªå¯é çš„æ—©æœŸCRLMå¤å‘é¢„æµ‹æ¨¡å‹ï¼Œå¹¶å¼ºè°ƒäº†ä¸´åºŠé¢„åå»ºæ¨¡ä¸­æ•°æ®æ³„éœ²çš„é£é™©ï¼Œæå‡ºäº†ä¸€ä¸ªä¸¥æ ¼çš„æ¡†æ¶æ¥å‡è½»è¿™ä¸€é—®é¢˜ï¼Œæé«˜äº†æ¨¡å‹åœ¨ç°å®ä¸–ç•Œçš„å¯é æ€§å’Œç¿»è¯‘ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ä¼ ç»Ÿä¸´åºŠæ¨¡å‹åœ¨é¢„æµ‹ç»“ç›´è‚ ç™Œè‚è½¬ç§»ï¼ˆCRLMï¼‰æ‚£è€…é¢„åæ—¶çš„å‡†ç¡®æ€§æœ‰å¾…æé«˜ã€‚</li>
<li>é›†åˆæ¨¡å‹è™½è¡¨ç°å‡ºæé«˜æ€§èƒ½ï¼Œä½†å­˜åœ¨æ•°æ®æ³„éœ²é£é™©ã€‚</li>
<li>ç ”ç©¶èšç„¦äºä½¿ç”¨æœ¯å‰ä¸´åºŠå‚æ•°å’Œæ”¾å°„ç»„å­¦ç‰¹å¾é¢„æµ‹æœ¯åå¤å‘é£é™©ã€‚</li>
<li>æœ¯åä¸‰ä¸ªæœˆå¤å‘é¢„æµ‹æ¨¡å‹æ€§èƒ½æœ€ä½³ï¼Œäº¤å‰éªŒè¯AUCä¸º0.723ã€‚</li>
<li>å†³ç­–æ›²çº¿åˆ†æè¯å®è¯¥æ¨¡å‹åœ¨ç‰¹å®šé˜ˆæ¦‚ç‡èŒƒå›´å†…ä¼˜äºå…¶ä»–æ²»ç–—ç­–ç•¥ã€‚</li>
<li>ç ”ç©¶æˆåŠŸå¼€å‘äº†ä¸€ä¸ªå¯é çš„æ—©æœŸCRLMå¤å‘é¢„æµ‹æ¨¡å‹ï¼Œæœ‰åŠ©äºæœ¯åç›‘æµ‹å’Œæ²»ç–—å†³ç­–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19734">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c1e9c1d623599fc66e663952bc91143.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-068ba7323208a91a07ccc186a473e0ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b6c787d76455996bb300f01d4dc8f67.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-31623d0dc02ed087b08fc37c82caa0c5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48306411e240806dbcf2f32cf6928acc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f1b2303a7dc1f2b2dbe83105c0b19be5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Continual-Learning-Based-Unified-Model-for-Unpaired-Image-Restoration-Tasks"><a href="#Continual-Learning-Based-Unified-Model-for-Unpaired-Image-Restoration-Tasks" class="headerlink" title="Continual Learning-Based Unified Model for Unpaired Image Restoration   Tasks"></a>Continual Learning-Based Unified Model for Unpaired Image Restoration   Tasks</h2><p><strong>Authors:Kotha Kartheek, Lingamaneni Gnanesh Chowdary, Snehasis Mukherjee</strong></p>
<p>Restoration of images contaminated by different adverse weather conditions such as fog, snow, and rain is a challenging task due to the varying nature of the weather conditions. Most of the existing methods focus on any one particular weather conditions. However, for applications such as autonomous driving, a unified model is necessary to perform restoration of corrupted images due to different weather conditions. We propose a continual learning approach to propose a unified framework for image restoration. The proposed framework integrates three key innovations: (1) Selective Kernel Fusion layers that dynamically combine global and local features for robust adaptive feature selection; (2) Elastic Weight Consolidation (EWC) to enable continual learning and mitigate catastrophic forgetting across multiple restoration tasks; and (3) a novel Cycle-Contrastive Loss that enhances feature discrimination while preserving semantic consistency during domain translation. Further, we propose an unpaired image restoration approach to reduce the dependance of the proposed approach on the training data. Extensive experiments on standard benchmark datasets for dehazing, desnowing and deraining tasks demonstrate significant improvements in PSNR, SSIM, and perceptual quality over the state-of-the-art. </p>
<blockquote>
<p>ç”±äºå¤©æ°”æ¡ä»¶çš„å¤šæ ·æ€§ï¼Œç”±é›¾ã€é›ªå’Œé›¨ç­‰ä¸åŒæ¶åŠ£å¤©æ°”æ¡ä»¶æ±¡æŸ“çš„å›¾ç‰‡æ¢å¤æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•ä¸»è¦å…³æ³¨æŸä¸€ç§ç‰¹å®šçš„å¤©æ°”æ¡ä»¶ã€‚ç„¶è€Œï¼Œå¯¹äºè‡ªåŠ¨é©¾é©¶ç­‰åº”ç”¨ï¼Œç”±äºä¸åŒçš„å¤©æ°”æ¡ä»¶å¯¼è‡´å›¾åƒé€€åŒ–ï¼Œå› æ­¤éœ€è¦ä¸€ç§ç»Ÿä¸€æ¨¡å‹è¿›è¡Œæ¢å¤ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æŒç»­å­¦ä¹ çš„æ–¹æ³•ï¼Œä»¥å»ºç«‹ä¸€ä¸ªç»Ÿä¸€çš„å›¾åƒæ¢å¤æ¡†æ¶ã€‚è¯¥æ¡†æ¶é›†æˆäº†ä¸‰é¡¹å…³é”®åˆ›æ–°ï¼š1ï¼‰é€‰æ‹©æ€§æ ¸èåˆå±‚ï¼Œå®ƒèƒ½åŠ¨æ€ç»“åˆå…¨å±€å’Œå±€éƒ¨ç‰¹å¾ï¼Œå®ç°ç¨³å¥çš„è‡ªé€‚åº”ç‰¹å¾é€‰æ‹©ï¼›2ï¼‰å¼¹æ€§æƒé‡å·©å›ºï¼ˆEWCï¼‰ï¼Œä»¥å®ç°æŒç»­å­¦ä¹ ï¼Œå¹¶å‡è½»å¤šä¸ªæ¢å¤ä»»åŠ¡ä¸­çš„ç¾éš¾æ€§é—å¿˜ï¼›3ï¼‰ä¸€ç§æ–°çš„å¾ªç¯å¯¹æ¯”æŸå¤±ï¼Œå®ƒæé«˜äº†ç‰¹å¾è¾¨åˆ«åŠ›ï¼ŒåŒæ—¶åœ¨åŸŸè½¬æ¢è¿‡ç¨‹ä¸­ä¿ç•™äº†è¯­ä¹‰ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— éœ€é…å¯¹çš„å›¾åƒæ¢å¤æ–¹æ³•ï¼Œä»¥å‡å°‘æ‰€æå‡ºæ–¹æ³•å¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–ã€‚åœ¨é™¤é›¾ã€æ¶ˆé›ªå’Œå»é›¨ä»»åŠ¡çš„æ ‡å‡†åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œåœ¨å³°å€¼ä¿¡å·å™ªå£°æ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢å‡æœ‰æ˜¾è‘—æ”¹è¿›ï¼Œè¶…è¿‡äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19184v1">PDF</a> Under Review</p>
<p><strong>Summary</strong></p>
<p>é’ˆå¯¹æ¶åŠ£å¤©æ°”æ¡ä»¶ï¼ˆå¦‚é›¾ã€é›ªå’Œé›¨ï¼‰å¯¼è‡´çš„å›¾åƒæ±¡æŸ“æ¢å¤æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå¤©æ°”æ¡ä»¶çš„å˜åŒ–å¤šæ ·ã€‚ç°æœ‰æ–¹æ³•å¤§å¤šä¸“æ³¨äºç‰¹å®šçš„å¤©æ°”æ¡ä»¶ï¼Œä½†åœ¨è‡ªåŠ¨é©¾é©¶ç­‰åº”ç”¨ä¸­ï¼Œéœ€è¦ä¸€ç§ç»Ÿä¸€æ¨¡å‹æ¥æ¢å¤å› ä¸åŒå¤©æ°”æ¡ä»¶è€ŒæŸåçš„å›¾åƒã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æŒç»­å­¦ä¹ çš„æ–¹æ³•ï¼Œå»ºç«‹ä¸€ä¸ªç»Ÿä¸€çš„å›¾åƒæ¢å¤æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†ä¸‰å¤§åˆ›æ–°ç‚¹ï¼šé€‰æ‹©æ€§æ ¸èåˆå±‚å¯åŠ¨æ€ç»“åˆå…¨å±€å’Œå±€éƒ¨ç‰¹å¾è¿›è¡Œç¨³å¥çš„è‡ªé€‚åº”ç‰¹å¾é€‰æ‹©ï¼›å¼¹æ€§æƒé‡å·©å›ºï¼ˆEWCï¼‰å¯å®ç°æŒç»­å­¦ä¹ å¹¶å‡è½»å¤šä¸ªæ¢å¤ä»»åŠ¡ä¸­çš„ç¾éš¾æ€§é—å¿˜ï¼›ä»¥åŠæ–°å‹çš„å¾ªç¯å¯¹æ¯”æŸå¤±ï¼Œå¯åœ¨åŸŸè½¬æ¢è¿‡ç¨‹ä¸­å¢å¼ºç‰¹å¾é‰´åˆ«åŠ›å¹¶ä¿ç•™è¯­ä¹‰ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ— éœ€é…å¯¹çš„å›¾åƒæ¢å¤æ–¹æ³•ï¼Œä»¥é™ä½è¯¥æ–¹æ¡ˆå¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–æ€§ã€‚åœ¨æ¶ˆé›¾ã€é™¤é›ªå’Œå»é›¨çš„æ ‡å‡†åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”ï¼Œå®ƒåœ¨å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ã€ç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢éƒ½æœ‰æ˜¾è‘—æé«˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„å›¾åƒæ¢å¤æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºå¤©æ°”æ¡ä»¶çš„å˜åŒ–å¤šæ ·ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤§å¤šä¸“æ³¨äºç‰¹å®šå¤©æ°”æ¡ä»¶ï¼Œéœ€è¦ä¸€ç§ç»Ÿä¸€æ¨¡å‹æ¥å¤„ç†å¤šç§å¤©æ°”æ¡ä»¶ä¸‹çš„å›¾åƒæ¢å¤ã€‚</li>
<li>æå‡ºçš„ç»Ÿä¸€æ¡†æ¶é›†æˆäº†ä¸‰å¤§åˆ›æ–°ç‚¹ï¼šé€‰æ‹©æ€§æ ¸èåˆå±‚ã€å¼¹æ€§æƒé‡å·©å›ºï¼ˆEWCï¼‰å’Œå¾ªç¯å¯¹æ¯”æŸå¤±ã€‚</li>
<li>é€‰æ‹©æ€§æ ¸èåˆå±‚èƒ½å¤ŸåŠ¨æ€ç»“åˆå…¨å±€å’Œå±€éƒ¨ç‰¹å¾ï¼Œè¿›è¡Œç¨³å¥çš„è‡ªé€‚åº”ç‰¹å¾é€‰æ‹©ã€‚</li>
<li>å¼¹æ€§æƒé‡å·©å›ºï¼ˆEWCï¼‰æœ‰åŠ©äºå®ç°æŒç»­å­¦ä¹ å¹¶å‡è½»ç¾éš¾æ€§é—å¿˜ã€‚</li>
<li>æ–°å‹çš„å¾ªç¯å¯¹æ¯”æŸå¤±å¯å¢å¼ºç‰¹å¾é‰´åˆ«åŠ›ï¼Œå¹¶åœ¨åŸŸè½¬æ¢è¿‡ç¨‹ä¸­ä¿ç•™è¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>è¯¥ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§æ— éœ€é…å¯¹çš„å›¾åƒæ¢å¤æ–¹æ³•ï¼Œä»¥é™ä½å¯¹è®­ç»ƒæ•°æ®çš„ä¾èµ–æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19184">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ab23022947596f80f6cf5cfb251eeb28.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48a6a02a7c662ecf50e24d0e621ba5a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8722694a8ed1b7dc8983128d1cf03b12.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3e1d692f544d3e011f55af0a74745c99.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="SIDA-Synthetic-Image-Driven-Zero-shot-Domain-Adaptation"><a href="#SIDA-Synthetic-Image-Driven-Zero-shot-Domain-Adaptation" class="headerlink" title="SIDA: Synthetic Image Driven Zero-shot Domain Adaptation"></a>SIDA: Synthetic Image Driven Zero-shot Domain Adaptation</h2><p><strong>Authors:Ye-Chan Kim, SeungJu Cha, Si-Woo Kim, Taewhan Kim, Dong-Jin Kim</strong></p>
<p>Zero-shot domain adaptation is a method for adapting a model to a target domain without utilizing target domain image data. To enable adaptation without target images, existing studies utilize CLIPâ€™s embedding space and text description to simulate target-like style features. Despite the previous achievements in zero-shot domain adaptation, we observe that these text-driven methods struggle to capture complex real-world variations and significantly increase adaptation time due to their alignment process. Instead of relying on text descriptions, we explore solutions leveraging image data, which provides diverse and more fine-grained style cues. In this work, we propose SIDA, a novel and efficient zero-shot domain adaptation method leveraging synthetic images. To generate synthetic images, we first create detailed, source-like images and apply image translation to reflect the style of the target domain. We then utilize the style features of these synthetic images as a proxy for the target domain. Based on these features, we introduce Domain Mix and Patch Style Transfer modules, which enable effective modeling of real-world variations. In particular, Domain Mix blends multiple styles to expand the intra-domain representations, and Patch Style Transfer assigns different styles to individual patches. We demonstrate the effectiveness of our method by showing state-of-the-art performance in diverse zero-shot adaptation scenarios, particularly in challenging domains. Moreover, our approach achieves high efficiency by significantly reducing the overall adaptation time. </p>
<blockquote>
<p>é›¶æ ·æœ¬åŸŸè‡ªé€‚åº”æ˜¯ä¸€ç§æ— éœ€ä½¿ç”¨ç›®æ ‡åŸŸå›¾åƒæ•°æ®å³å¯ä½¿æ¨¡å‹é€‚åº”ç›®æ ‡åŸŸçš„æ–¹æ³•ã€‚ä¸ºäº†åœ¨æ— éœ€ç›®æ ‡å›¾åƒçš„æƒ…å†µä¸‹å®ç°è‡ªé€‚åº”ï¼Œç°æœ‰ç ”ç©¶åˆ©ç”¨CLIPçš„åµŒå…¥ç©ºé—´å’Œæ–‡æœ¬æè¿°æ¥æ¨¡æ‹Ÿç›®æ ‡ç›¸ä¼¼çš„é£æ ¼ç‰¹å¾ã€‚å°½ç®¡åœ¨é›¶æ ·æœ¬åŸŸè‡ªé€‚åº”æ–¹é¢å–å¾—äº†å…ˆå‰çš„æˆå°±ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è¿™äº›æ–‡æœ¬é©±åŠ¨çš„æ–¹æ³•åœ¨æ•æ‰ç°å®ä¸–ç•Œå¤æ‚å˜åŒ–æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œå¹¶ä¸”ç”±äºå¯¹é½è¿‡ç¨‹è€Œå¢åŠ äº†å¤§é‡çš„è‡ªé€‚åº”æ—¶é—´ã€‚æˆ‘ä»¬ä¸å†ä¾èµ–æ–‡æœ¬æè¿°ï¼Œè€Œæ˜¯æ¢ç´¢åˆ©ç”¨å›¾åƒæ•°æ®çš„è§£å†³æ–¹æ¡ˆï¼Œå›¾åƒæ•°æ®æä¾›äº†å¤šæ ·ä¸”æ›´ç²¾ç»†çš„é£æ ¼çº¿ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„é«˜æ•ˆé›¶æ ·æœ¬åŸŸè‡ªé€‚åº”æ–¹æ³•ï¼Œå³åˆ©ç”¨åˆæˆå›¾åƒè¿›è¡Œè‡ªé€‚åº”ï¼ˆSIDAï¼‰ã€‚ä¸ºäº†ç”Ÿæˆåˆæˆå›¾åƒï¼Œæˆ‘ä»¬é¦–å…ˆåˆ›å»ºè¯¦ç»†çš„æºåŸŸå›¾åƒï¼Œå¹¶é€šè¿‡å›¾åƒç¿»è¯‘æ¥åæ˜ ç›®æ ‡åŸŸçš„æ ·å¼ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™äº›åˆæˆå›¾åƒçš„æ ·å¼ç‰¹å¾ä½œä¸ºç›®æ ‡åŸŸçš„ä»£ç†ã€‚åŸºäºè¿™äº›ç‰¹å¾ï¼Œæˆ‘ä»¬å¼•å…¥äº†Domain Mixå’ŒPatch Style Transferæ¨¡å—ï¼Œå®ƒä»¬å¯ä»¥æœ‰æ•ˆåœ°æ¨¡æ‹Ÿç°å®ä¸–ç•Œçš„å˜åŒ–ã€‚ç‰¹åˆ«æ˜¯ï¼ŒDomain Mixé€šè¿‡æ··åˆå¤šç§é£æ ¼æ¥æ‰©å±•åŸŸå†…è¡¨ç¤ºï¼Œè€ŒPatch Style Transferåˆ™ä¸ºå„ä¸ªè¡¥ä¸åˆ†é…ä¸åŒçš„é£æ ¼ã€‚æˆ‘ä»¬åœ¨å¤šç§é›¶æ ·æœ¬è‡ªé€‚åº”åœºæ™¯ä¸­å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„é¢†åŸŸé‡Œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¤§å¤§å‡å°‘äº†æ€»ä½“è‡ªé€‚åº”æ—¶é—´è€Œå®ç°äº†é«˜æ•ˆç‡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18632v1">PDF</a> Accepted to ACM MM 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåˆæˆå›¾åƒçš„é«˜æ•ˆé›¶æ ·æœ¬åŸŸè‡ªé€‚åº”æ–¹æ³•ï¼ˆSIDAï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒæ¥æ¨¡æ‹Ÿç›®æ ‡åŸŸçš„æ ·å¼ç‰¹å¾ï¼Œè¿›è€Œå®ç°æ¨¡å‹å¯¹ç›®æ ‡åŸŸçš„é€‚åº”ï¼Œè€Œæ— éœ€ä½¿ç”¨ç›®æ ‡åŸŸçš„å›¾åƒæ•°æ®ã€‚é€šè¿‡æ··åˆå¤šç§é£æ ¼å¹¶è½¬ç§»è¡¥ä¸é£æ ¼ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„å˜åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†è‡ªé€‚åº”æ—¶é—´ï¼Œæé«˜äº†æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é›¶æ ·æœ¬åŸŸè‡ªé€‚åº”æ–¹æ³•æ— éœ€ä½¿ç”¨ç›®æ ‡åŸŸå›¾åƒæ•°æ®å³å¯ä½¿æ¨¡å‹é€‚åº”ç›®æ ‡åŸŸã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºåˆæˆå›¾åƒçš„æ–¹æ³•æ¥å®ç°é›¶æ ·æœ¬åŸŸè‡ªé€‚åº”ã€‚</li>
<li>é€šè¿‡ç”Ÿæˆè¯¦ç»†çš„æºåŸŸå›¾åƒå¹¶åº”ç”¨å›¾åƒç¿»è¯‘æ¥åæ˜ ç›®æ ‡åŸŸçš„æ ·å¼ï¼Œè¿›è€Œæ¨¡æ‹Ÿç›®æ ‡åŸŸçš„æ ·å¼ç‰¹å¾ã€‚</li>
<li>å¼•å…¥Domain Mixå’ŒPatch Style Transferæ¨¡å—ï¼Œä»¥æ›´æœ‰æ•ˆåœ°æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„å˜åŒ–ã€‚</li>
<li>Domain Mixé€šè¿‡æ··åˆå¤šç§é£æ ¼æ¥æ‰©å±•åŸŸå†…è¡¨ç¤ºã€‚</li>
<li>Patch Style Transferå°†ä¸åŒçš„é£æ ¼åˆ†é…ç»™å•ç‹¬çš„è¡¥ä¸ï¼Œä»¥æé«˜æ¨¡å‹çš„è¡¨ç°åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18632">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-651b585c4db1a513d190ee5438ea1453.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fb24c801afbec0774ba405f533a34838.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca8d87aa79c42dc13c642d9656acbc1e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c38b562a53bec8d09c68241f549ec161.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-23f096b65128c1afab919719df9b3be6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a2ff6bf06c8644c10a63662d75bada84.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="GIIFT-Graph-guided-Inductive-Image-free-Multimodal-Machine-Translation"><a href="#GIIFT-Graph-guided-Inductive-Image-free-Multimodal-Machine-Translation" class="headerlink" title="GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation"></a>GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation</h2><p><strong>Authors:Jiafeng Xiong, Yuting Zhao</strong></p>
<p>Multimodal Machine Translation (MMT) has demonstrated the significant help of visual information in machine translation. However, existing MMT methods face challenges in leveraging the modality gap by enforcing rigid visual-linguistic alignment whilst being confined to inference within their trained multimodal domains. In this work, we construct novel multimodal scene graphs to preserve and integrate modality-specific information and introduce GIIFT, a two-stage Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph Attention Network adapter to learn multimodal knowledge in a unified fused space and inductively generalize it to broader image-free translation domains. Experimental results on the Multi30K dataset of English-to-French and English-to-German tasks demonstrate that our GIIFT surpasses existing approaches and achieves the state-of-the-art, even without images during inference. Results on the WMT benchmark show significant improvements over the image-free translation baselines, demonstrating the strength of GIIFT towards inductive image-free inference. </p>
<blockquote>
<p>å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ï¼ˆMMTï¼‰å·²ç»è¯æ˜äº†è§†è§‰ä¿¡æ¯åœ¨æœºå™¨ç¿»è¯‘ä¸­çš„å·¨å¤§å¸®åŠ©ã€‚ç„¶è€Œï¼Œç°æœ‰çš„MMTæ–¹æ³•åœ¨åˆ©ç”¨æ¨¡æ€å·®è·æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå®ƒä»¬é€šè¿‡å¼ºåˆ¶å®æ–½ä¸¥æ ¼çš„è§†è§‰è¯­è¨€å¯¹é½ï¼ŒåŒæ—¶å—é™äºå…¶è®­ç»ƒçš„å¤šæ¨¡æ€åŸŸå†…çš„æ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ„å»ºäº†æ–°å‹çš„å¤šæ¨¡æ€åœºæ™¯å›¾ï¼Œä»¥ä¿ç•™å’Œæ•´åˆç‰¹å®šæ¨¡æ€çš„ä¿¡æ¯ï¼Œå¹¶å¼•å…¥äº†GIIFTï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å›¾å¼•å¯¼å½’çº³æ— å›¾å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘æ¡†æ¶ï¼Œå®ƒä½¿ç”¨è·¨æ¨¡æ€å›¾æ³¨æ„åŠ›ç½‘ç»œé€‚é…å™¨ï¼Œåœ¨ç»Ÿä¸€èåˆç©ºé—´å­¦ä¹ å¤šæ¨¡æ€çŸ¥è¯†ï¼Œå¹¶å½’çº³æ¨å¹¿è‡³æ›´å¹¿æ³›çš„æ— å›¾åƒç¿»è¯‘é¢†åŸŸã€‚åœ¨Multi30Kè‹±è¯­åˆ°æ³•è¯­å’Œè‹±è¯­åˆ°å¾·è¯­ä»»åŠ¡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„GIIFTè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³ï¼Œå³ä½¿åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ²¡æœ‰ä½¿ç”¨å›¾åƒã€‚åœ¨WMTåŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœä¹Ÿæ˜¾è‘—ä¼˜äºæ— å›¾åƒç¿»è¯‘åŸºçº¿ï¼Œè¯æ˜äº†GIIFTåœ¨æ— å›¾åƒæ¨ç†æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18562v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶é€šè¿‡æ„å»ºæ–°å‹çš„å¤šæ¨¡æ€åœºæ™¯å›¾æ¥èåˆå¤šæ¨¡æ€ä¿¡æ¯ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå›¾æ³¨æ„åŠ›ç½‘ç»œçš„å›¾åƒè‡ªç”±æœºå™¨ç¿»è¯‘æ¡†æ¶GIIFTã€‚è¯¥æ¡†æ¶èƒ½åœ¨ç»Ÿä¸€çš„èåˆç©ºé—´ä¸­å­¦ä¹ å¤šæ¨¡æ€çŸ¥è¯†ï¼Œå¹¶åœ¨æ²¡æœ‰å›¾åƒçš„æƒ…å†µä¸‹æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„ç¿»è¯‘é¢†åŸŸã€‚åœ¨Multi30Kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGIIFTè¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œè¾¾åˆ°äº†å›¾åƒè‡ªç”±ç¿»è¯‘çš„æœ€å…ˆè¿›æ°´å¹³ã€‚åœ¨WMTåŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœä¹Ÿæ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†GIIFTåœ¨å›¾åƒè‡ªç”±æ¨ç†æ–¹é¢çš„ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ï¼ˆMMTï¼‰å€ŸåŠ©è§†è§‰ä¿¡æ¯ä¸ºæœºå™¨ç¿»è¯‘æä¾›äº†é‡è¦å¸®åŠ©ã€‚</li>
<li>å½“å‰MMTæ–¹æ³•é¢ä¸´æ¨¡æ€é—´å·®è·çš„æŒ‘æˆ˜ï¼Œéœ€è¦æ›´çµæ´»çš„è§†è§‰è¯­è¨€å¯¹é½æ–¹å¼ï¼Œå¹¶å±€é™äºå·²è®­ç»ƒçš„å¤šæ¨¡æ€é¢†åŸŸã€‚</li>
<li>ç ”ç©¶æ„å»ºäº†æ–°å‹çš„å¤šæ¨¡æ€åœºæ™¯å›¾æ¥ä¿å­˜å’Œæ•´åˆæ¨¡æ€ç‰¹å®šä¿¡æ¯ã€‚</li>
<li>å¼•å…¥äº†GIIFTæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å›¾å¼•å¯¼å›¾åƒè‡ªç”±MMTæ¡†æ¶ã€‚</li>
<li>GIIFTä½¿ç”¨è·¨æ¨¡æ€å›¾æ³¨æ„åŠ›ç½‘ç»œé€‚é…å™¨ï¼Œåœ¨ç»Ÿä¸€çš„èåˆç©ºé—´ä¸­å­¦ä¹ å¤šæ¨¡æ€çŸ¥è¯†ï¼Œå¹¶å½’çº³æ¨å¹¿åˆ°æ›´å¹¿æ³›çš„å›¾åƒè‡ªç”±ç¿»è¯‘é¢†åŸŸã€‚</li>
<li>åœ¨Multi30Kæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒGIIFTè¾¾åˆ°äº†æœ€å…ˆè¿›çš„å›¾åƒè‡ªç”±ç¿»è¯‘æ°´å¹³ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ— éœ€å›¾åƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fe4cef1813eaaa2d650cef44d76a0659.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ea6305d740915593d7cfa89fb52a0482.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-722c0f6a401dc0f95aa71827455f72aa.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ee621fc888eed6ddce4ec1175008df87.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Dual-branch-Prompting-for-Multimodal-Machine-Translation"><a href="#Dual-branch-Prompting-for-Multimodal-Machine-Translation" class="headerlink" title="Dual-branch Prompting for Multimodal Machine Translation"></a>Dual-branch Prompting for Multimodal Machine Translation</h2><p><strong>Authors:Jie Wang, Zhendong Yang, Liansong Zong, Xiaobo Zhang, Dexian Wang, Ji Zhang</strong></p>
<p>Multimodal Machine Translation (MMT) typically enhances text-only translation by incorporating aligned visual features. Despite the remarkable progress, state-of-the-art MMT approaches often rely on paired image-text inputs at inference and are sensitive to irrelevant visual noise, which limits their robustness and practical applicability. To address these issues, we propose D2P-MMT, a diffusion-based dual-branch prompting framework for robust vision-guided translation. Specifically, D2P-MMT requires only the source text and a reconstructed image generated by a pre-trained diffusion model, which naturally filters out distracting visual details while preserving semantic cues. During training, the model jointly learns from both authentic and reconstructed images using a dual-branch prompting strategy, encouraging rich cross-modal interactions. To bridge the modality gap and mitigate training-inference discrepancies, we introduce a distributional alignment loss that enforces consistency between the output distributions of the two branches. Extensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves superior translation performance compared to existing state-of-the-art approaches. </p>
<blockquote>
<p>å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ï¼ˆMMTï¼‰é€šå¸¸é€šè¿‡èå…¥å¯¹é½çš„è§†è§‰ç‰¹å¾æ¥å¢å¼ºçº¯æ–‡æœ¬ç¿»è¯‘ã€‚å°½ç®¡å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†æœ€æ–°çš„MMTæ–¹æ³•é€šå¸¸åœ¨æ¨ç†é˜¶æ®µä¾èµ–äºé…å¯¹å›¾åƒæ–‡æœ¬è¾“å…¥ï¼Œå¹¶å¯¹ä¸ç›¸å…³çš„è§†è§‰å™ªå£°æ•æ„Ÿï¼Œè¿™é™åˆ¶äº†å…¶ç¨³å¥æ€§å’Œå®é™…åº”ç”¨èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†D2P-MMTï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„åŒåˆ†æ”¯æç¤ºæ¡†æ¶ï¼Œç”¨äºå®ç°ç¨³å¥çš„è§†è§‰å¼•å¯¼ç¿»è¯‘ã€‚å…·ä½“è€Œè¨€ï¼ŒD2P-MMTä»…éœ€è¦æºæ–‡æœ¬å’Œä¸€ä¸ªç”±é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é‡å»ºå›¾åƒï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè‡ªç„¶åœ°è¿‡æ»¤æ‰ä»¤äººåˆ†å¿ƒçš„è§†è§‰ç»†èŠ‚ï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰çº¿ç´¢ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹é‡‡ç”¨åŒåˆ†æ”¯æç¤ºç­–ç•¥ï¼Œä»çœŸå®å’Œé‡å»ºçš„å›¾åƒä¸­å­¦ä¹ ï¼Œé¼“åŠ±ä¸°å¯Œçš„è·¨æ¨¡æ€äº¤äº’ã€‚ä¸ºäº†ç¼©å°æ¨¡æ€å·®è·å¹¶å‡è½»è®­ç»ƒä¸æ¨ç†ä¹‹é—´çš„å·®å¼‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†å¸ƒå¯¹é½æŸå¤±ï¼Œä»¥å¼ºåˆ¶ä¸¤ä¸ªåˆ†æ”¯è¾“å‡ºåˆ†å¸ƒçš„ä¸€è‡´æ€§ã€‚åœ¨Multi30Kæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸ç°æœ‰çš„æœ€æ–°æ–¹æ³•ç›¸æ¯”ï¼ŒD2P-MMTå®ç°äº†å“è¶Šçš„ç¿»è¯‘æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17588v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ï¼ˆMMTï¼‰é€šè¿‡èå…¥å¯¹é½çš„è§†è§‰ç‰¹å¾å¢å¼ºäº†çº¯æ–‡æœ¬ç¿»è¯‘ã€‚ç„¶è€Œï¼Œå½“å‰å…ˆè¿›æŠ€æœ¯çš„æ–¹æ³•åœ¨æ¨ç†æ—¶é€šå¸¸ä¾èµ–äºé…å¯¹å›¾åƒæ–‡æœ¬è¾“å…¥ï¼Œå¹¶å¯¹æ— å…³çš„è§†è§‰å™ªå£°æ•æ„Ÿï¼Œè¿™é™åˆ¶äº†å…¶ç¨³å¥æ€§å’Œå®é™…åº”ç”¨èƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ‰©æ•£æ¨¡å‹çš„åŒåˆ†æ”¯æç¤ºæ¡†æ¶D2P-MMTï¼Œç”¨äºç¨³å¥çš„è§†è§‰å¼•å¯¼ç¿»è¯‘ã€‚å…·ä½“è€Œè¨€ï¼ŒD2P-MMTä»…éœ€è¦æºæ–‡æœ¬å’Œç”±é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆçš„é‡å»ºå›¾åƒï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿè‡ªç„¶è¿‡æ»¤æ‰ä»¤äººåˆ†å¿ƒçš„è§†è§‰ç»†èŠ‚ï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰çº¿ç´¢ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹é‡‡ç”¨åŒåˆ†æ”¯æç¤ºç­–ç•¥ï¼Œä»çœŸå®å’Œé‡å»ºçš„å›¾åƒä¸­å­¦ä¹ ï¼Œä¿ƒè¿›è·¨æ¨¡æ€çš„ä¸°å¯Œäº¤äº’ã€‚ä¸ºç¼©å°æ¨¡æ€å·®è·å¹¶å‡è½»è®­ç»ƒä¸æ¨ç†ä¹‹é—´çš„å·®å¼‚ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åˆ†å¸ƒå¯¹é½æŸå¤±ï¼Œä»¥ç¡®ä¿ä¸¤ä¸ªåˆ†æ”¯è¾“å‡ºåˆ†å¸ƒçš„ä¸€è‡´æ€§ã€‚åœ¨Multi30Kæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒD2P-MMTçš„ç¿»è¯‘æ€§èƒ½ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æœºå™¨ç¿»è¯‘ï¼ˆMMTï¼‰é€šè¿‡ç»“åˆè§†è§‰ç‰¹å¾æ”¹è¿›æ–‡æœ¬ç¿»è¯‘ã€‚</li>
<li>å½“å‰MMTæ–¹æ³•ä¾èµ–äºé…å¯¹å›¾åƒæ–‡æœ¬è¾“å…¥ï¼Œå¯¹è§†è§‰å™ªå£°æ•æ„Ÿã€‚</li>
<li>D2P-MMTæ¡†æ¶æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„è§£å†³æ–¹æ¡ˆï¼Œä»…éœ€è¦æºæ–‡æœ¬å’Œé‡å»ºå›¾åƒã€‚</li>
<li>é‡å»ºå›¾åƒè‡ªç„¶è¿‡æ»¤æ‰å¹²æ‰°è§†è§‰ç»†èŠ‚ï¼Œä¿ç•™è¯­ä¹‰çº¿ç´¢ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨åŒåˆ†æ”¯æç¤ºç­–ç•¥è¿›è¡Œè®­ç»ƒï¼Œä¿ƒè¿›è·¨æ¨¡æ€äº¤äº’ã€‚</li>
<li>å¼•å…¥åˆ†å¸ƒå¯¹é½æŸå¤±ä»¥ç¼©å°æ¨¡æ€å·®è·å’Œå‡å°‘è®­ç»ƒä¸æ¨ç†å·®å¼‚ã€‚</li>
<li>åœ¨Multi30Kæ•°æ®é›†ä¸Šï¼ŒD2P-MMTè¡¨ç°å‡ºä¼˜äºç°æœ‰æŠ€æœ¯çš„ç¿»è¯‘æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17588">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5d66b5c937bd791753554dbb71b3ae77.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e3fd40b9f8326a2764be192664dc64a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8aa4d08019ca21c0fb06fa5cc13801f1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="UniLGL-Learning-Uniform-Place-Recognition-for-FOV-limited-Panoramic-LiDAR-Global-Localization"><a href="#UniLGL-Learning-Uniform-Place-Recognition-for-FOV-limited-Panoramic-LiDAR-Global-Localization" class="headerlink" title="UniLGL: Learning Uniform Place Recognition for FOV-limited&#x2F;Panoramic   LiDAR Global Localization"></a>UniLGL: Learning Uniform Place Recognition for FOV-limited&#x2F;Panoramic   LiDAR Global Localization</h2><p><strong>Authors:Hongming Shen, Xun Chen, Yulin Hui, Zhenyu Wu, Wei Wang, Qiyang Lyu, Tianchen Deng, Danwei Wang</strong></p>
<p>Existing LGL methods typically consider only partial information (e.g., geometric features) from LiDAR observations or are designed for homogeneous LiDAR sensors, overlooking the uniformity in LGL. In this work, a uniform LGL method is proposed, termed UniLGL, which simultaneously achieves spatial and material uniformity, as well as sensor-type uniformity. The key idea of the proposed method is to encode the complete point cloud, which contains both geometric and material information, into a pair of BEV images (i.e., a spatial BEV image and an intensity BEV image). An end-to-end multi-BEV fusion network is designed to extract uniform features, equipping UniLGL with spatial and material uniformity. To ensure robust LGL across heterogeneous LiDAR sensors, a viewpoint invariance hypothesis is introduced, which replaces the conventional translation equivariance assumption commonly used in existing LPR networks and supervises UniLGL to achieve sensor-type uniformity in both global descriptors and local feature representations. Finally, based on the mapping between local features on the 2D BEV image and the point cloud, a robust global pose estimator is derived that determines the global minimum of the global pose on SE(3) without requiring additional registration. To validate the effectiveness of the proposed uniform LGL, extensive benchmarks are conducted in real-world environments, and the results show that the proposed UniLGL is demonstratively competitive compared to other State-of-the-Art LGL methods. Furthermore, UniLGL has been deployed on diverse platforms, including full-size trucks and agile Micro Aerial Vehicles (MAVs), to enable high-precision localization and mapping as well as multi-MAV collaborative exploration in port and forest environments, demonstrating the applicability of UniLGL in industrial and field scenarios. </p>
<blockquote>
<p>ç°æœ‰çš„LGLæ–¹æ³•é€šå¸¸åªè€ƒè™‘LiDARè§‚æµ‹çš„éƒ¨åˆ†ä¿¡æ¯ï¼ˆä¾‹å¦‚å‡ ä½•ç‰¹å¾ï¼‰ï¼Œæˆ–è€…ä¸ºå‡è´¨çš„LiDARä¼ æ„Ÿå™¨è€Œè®¾è®¡ï¼Œå¿½ç•¥äº†LGLä¸­çš„å‡åŒ€æ€§ã€‚åœ¨æ­¤å·¥ä½œä¸­ï¼Œæå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„LGLæ–¹æ³•ï¼Œç§°ä¸ºUniLGLï¼Œå®ƒåŒæ—¶å®ç°äº†ç©ºé—´ã€ææ–™ä»¥åŠä¼ æ„Ÿå™¨ç±»å‹çš„å‡åŒ€æ€§ã€‚è¯¥æ–¹æ³•çš„å…³é”®æ€æƒ³æ˜¯å°†åŒ…å«å‡ ä½•å’Œææ–™ä¿¡æ¯çš„å®Œæ•´ç‚¹äº‘ç¼–ç ä¸ºä¸€å¯¹BEVå›¾åƒï¼ˆå³ç©ºé—´BEVå›¾åƒå’Œå¼ºåº¦BEVå›¾åƒï¼‰ã€‚è®¾è®¡äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¤šBEVèåˆç½‘ç»œæ¥æå–å‡åŒ€ç‰¹å¾ï¼Œä½¿UniLGLå…·å¤‡ç©ºé—´å’Œææ–™å‡åŒ€æ€§ã€‚ä¸ºäº†ç¡®ä¿è·¨ä¸åŒLiDARä¼ æ„Ÿå™¨çš„ç¨³å¥LGLï¼Œå¼•å…¥äº†è§†ç‚¹ä¸å˜æ€§å‡è®¾ï¼Œè¯¥å‡è®¾å–ä»£äº†ç°æœ‰LPRç½‘ç»œä¸­å¸¸ç”¨çš„å¹³ç§»ç­‰ä»·å‡è®¾ï¼Œå¹¶ä¿ƒä½¿UniLGLåœ¨å…¨å±€æè¿°ç¬¦å’Œå±€éƒ¨ç‰¹å¾è¡¨ç¤ºä¸Šå®ç°ä¼ æ„Ÿå™¨ç±»å‹å‡åŒ€æ€§ã€‚æœ€åï¼ŒåŸºäºäºŒç»´BEVå›¾åƒä¸Šçš„å±€éƒ¨ç‰¹å¾ä¸ç‚¹äº‘ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œå¯¼å‡ºäº†ä¸€ä¸ªç¨³å¥çš„å…¨å±€å§¿æ€ä¼°è®¡å™¨ï¼Œè¯¥ä¼°è®¡å™¨å¯åœ¨SE(3)ä¸Šç¡®å®šå…¨å±€å§¿æ€çš„å…¨å±€æœ€å°å€¼ï¼Œæ— éœ€é¢å¤–çš„æ³¨å†Œè¿‡ç¨‹ã€‚ä¸ºäº†éªŒè¯æ‰€æç»Ÿä¸€LGLçš„æœ‰æ•ˆæ€§ï¼Œåœ¨çœŸå®ç¯å¢ƒä¸­è¿›è¡Œäº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•ï¼Œç»“æœè¡¨æ˜ä¸å…¶ä»–æœ€æ–°LGLæ–¹æ³•ç›¸æ¯”ï¼Œæ‰€æçš„UniLGLå…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒUniLGLå·²éƒ¨ç½²åœ¨å¤šç§å¹³å°ä¸Šï¼ŒåŒ…æ‹¬å…¨å°ºå¯¸å¡è½¦å’Œæ•æ·å¾®å‹èˆªç©ºè½¦è¾†ï¼ˆMAVsï¼‰ï¼Œä»¥å®ç°æ¸¯å£å’Œæ£®æ—ç¯å¢ƒä¸­çš„é«˜ç²¾åº¦å®šä½å’Œæ˜ å°„ä»¥åŠå¤šMAVååŒæ¢ç´¢ï¼Œè¯æ˜äº†UniLGLåœ¨å·¥ä¸šå’Œç°åœºåœºæ™¯ä¸­çš„é€‚ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12194v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºUniLGLçš„ç»Ÿä¸€LGLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å®ç°äº†ç©ºé—´ã€æè´¨å’Œä¼ æ„Ÿå™¨ç±»å‹çš„å‡åŒ€æ€§ã€‚é€šè¿‡å°†ç‚¹äº‘ç¼–ç æˆä¸€å¯¹BEVå›¾åƒï¼Œå¹¶ç»“åˆç«¯åˆ°ç«¯çš„å¤šBEVèåˆç½‘ç»œæå–å‡åŒ€ç‰¹å¾ï¼Œè¯¥æ–¹æ³•å…·æœ‰ç©ºé—´å’Œæè´¨å‡åŒ€æ€§ã€‚ä¸ºç¡®ä¿åœ¨ä¸åŒæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨ä¹‹é—´çš„ç¨³å¥æ€§LGLï¼Œå¼•å…¥äº†è§†ç‚¹ä¸å˜å‡è®¾ï¼Œä»£æ›¿äº†ç°æœ‰LPRç½‘ç»œä¸­å¸¸ç”¨çš„å¹³ç§»ç­‰ä»·å‡è®¾ï¼Œå¹¶å®ç°äº†ä¼ æ„Ÿå™¨ç±»å‹çš„ç»Ÿä¸€ã€‚åŸºäºå±€éƒ¨ç‰¹å¾ä¸ç‚¹äº‘ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œæ¨å¯¼å‡ºäº†ç¨³å¥çš„å…¨å±€å§¿æ€ä¼°è®¡å™¨ã€‚å®éªŒéªŒè¯è¡¨æ˜ï¼Œä¸å…¶ä»–å…ˆè¿›çš„LGLæ–¹æ³•ç›¸æ¯”ï¼ŒUniLGLå…·æœ‰ç«äº‰åŠ›ï¼Œå¹¶å·²éƒ¨ç½²åœ¨å„ç§å¹³å°ä¸Šè¿›è¡Œå®é™…åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniLGLæ–¹æ³•è€ƒè™‘äº†æ¿€å…‰é›·è¾¾è§‚æµ‹çš„å®Œæ•´æ€§ä¿¡æ¯ï¼Œå¹¶å®ç°äº†ç©ºé—´ã€æè´¨å’Œä¼ æ„Ÿå™¨ç±»å‹çš„å‡åŒ€æ€§ã€‚</li>
<li>é€šè¿‡å°†ç‚¹äº‘ç¼–ç æˆBEVå›¾åƒï¼Œå¹¶ç»“åˆå¤šBEVèåˆç½‘ç»œæå–å‡åŒ€ç‰¹å¾ã€‚</li>
<li>å¼•å…¥è§†ç‚¹ä¸å˜å‡è®¾ä»¥ç¡®ä¿åœ¨ä¸åŒæ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨ä¹‹é—´çš„ç¨³å¥æ€§ã€‚</li>
<li>UniLGLå®ç°äº†å…¨å±€å’Œå±€éƒ¨ç‰¹å¾è¡¨ç¤ºä¸­çš„ä¼ æ„Ÿå™¨ç±»å‹ç»Ÿä¸€ã€‚</li>
<li>åŸºäºå±€éƒ¨ç‰¹å¾ä¸ç‚¹äº‘ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œæ¨å¯¼å‡ºäº†å…¨å±€å§¿æ€ä¼°è®¡å™¨ã€‚</li>
<li>å®éªŒéªŒè¯æ˜¾ç¤ºUniLGLä¸å…¶ä»–å…ˆè¿›çš„LGLæ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12194">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c7dbd1755e3ec5c6ae0bdfe6145fbc66.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-81aa24cf01729421f460b89790c2d352.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4005ab3aaae796ce394830cab3261d51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eec73b948879e5156784fc2ad0fcdc9a.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Time-resolved-dynamic-CBCT-reconstruction-using-prior-model-free-spatiotemporal-Gaussian-representation-PMF-STGR"><a href="#Time-resolved-dynamic-CBCT-reconstruction-using-prior-model-free-spatiotemporal-Gaussian-representation-PMF-STGR" class="headerlink" title="Time-resolved dynamic CBCT reconstruction using prior-model-free   spatiotemporal Gaussian representation (PMF-STGR)"></a>Time-resolved dynamic CBCT reconstruction using prior-model-free   spatiotemporal Gaussian representation (PMF-STGR)</h2><p><strong>Authors:Jiacheng Xie, Hua-Chieh Shao, You Zhang</strong></p>
<p>Time-resolved CBCT imaging, which reconstructs a dynamic sequence of CBCTs reflecting intra-scan motion (one CBCT per x-ray projection without phase sorting or binning), is highly desired for regular and irregular motion characterization, patient setup, and motion-adapted radiotherapy. Representing patient anatomy and associated motion fields as 3D Gaussians, we developed a Gaussian representation-based framework (PMF-STGR) for fast and accurate dynamic CBCT reconstruction. PMF-STGR comprises three major components: a dense set of 3D Gaussians to reconstruct a reference-frame CBCT for the dynamic sequence; another 3D Gaussian set to capture three-level, coarse-to-fine motion-basis-components (MBCs) to model the intra-scan motion; and a CNN-based motion encoder to solve projection-specific temporal coefficients for the MBCs. Scaled by the temporal coefficients, the learned MBCs will combine into deformation vector fields to deform the reference CBCT into projection-specific, time-resolved CBCTs to capture the dynamic motion. Due to the strong representation power of 3D Gaussians, PMF-STGR can reconstruct dynamic CBCTs in a â€˜one-shotâ€™ training fashion from a standard 3D CBCT scan, without using any prior anatomical or motion model. We evaluated PMF-STGR using XCAT phantom simulations and real patient scans. Metrics including the image relative error, structural-similarity-index-measure, tumor center-of-mass-error, and landmark localization error were used to evaluate the accuracy of solved dynamic CBCTs and motion. PMF-STGR shows clear advantages over a state-of-the-art, INR-based approach, PMF-STINR. Compared with PMF-STINR, PMF-STGR reduces reconstruction time by 50% while reconstructing less blurred images with better motion accuracy. With improved efficiency and accuracy, PMF-STGR enhances the applicability of dynamic CBCT imaging for potential clinical translation. </p>
<blockquote>
<p>æ—¶é—´è§£æCBCTæˆåƒé‡å»ºäº†ä¸€è¿ä¸²åæ˜ æ‰«æå†…è¿åŠ¨çš„CBCTï¼ˆæ¯æ¬¡Xå°„çº¿æŠ•å½±éƒ½æœ‰ä¸€ä¸ªCBCTï¼Œæ— éœ€ç›¸ä½æ’åºæˆ–åˆ†ç®±ï¼‰ï¼Œè¿™åœ¨å¸¸è§„å’Œè¿åŠ¨ä¸è§„åˆ™çš„è¿åŠ¨è¡¨å¾ã€æ‚£è€…è®¾ç½®å’Œé€‚åº”æ€§è¿åŠ¨æ”¾ç–—ä¸­éƒ½éå¸¸å—æ¬¢è¿ã€‚æˆ‘ä»¬ç”¨ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºæ‚£è€…è§£å‰–ç»“æ„å’Œç›¸å…³è¿åŠ¨åœºï¼Œå¼€å‘äº†ä¸€ä¸ªåŸºäºé«˜æ–¯è¡¨ç¤ºçš„æ¡†æ¶ï¼ˆPMF-STGRï¼‰ï¼Œç”¨äºå¿«é€Ÿå‡†ç¡®çš„åŠ¨æ€CBCTé‡å»ºã€‚PMF-STGRä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼šä¸€ç»„å¯†é›†çš„ä¸‰ç»´é«˜æ–¯ç”¨äºé‡å»ºåŠ¨æ€åºåˆ—çš„å‚è€ƒæ¡†æ¶CBCTï¼›å¦ä¸€ç»„ä¸‰ç»´é«˜æ–¯ç”¨äºæ•æ‰ä¸‰çº§ç²—åˆ°ç»†çš„è¿åŠ¨åŸºç¡€æˆåˆ†ï¼ˆMBCsï¼‰ï¼Œä»¥æ¨¡æ‹Ÿæ‰«æå†…çš„è¿åŠ¨ï¼›ä»¥åŠåŸºäºCNNçš„è¿åŠ¨ç¼–ç å™¨ï¼Œç”¨äºè§£å†³MBCçš„æŠ•å½±ç‰¹å®šæ—¶é—´ç³»æ•°ã€‚æ ¹æ®æ—¶é—´ç³»æ•°ï¼Œå­¦ä¹ çš„MBCå°†ç»„åˆæˆå˜å½¢çŸ¢é‡åœºï¼Œä»¥å˜å½¢å‚è€ƒCBCTä¸ºæŠ•å½±ç‰¹å®šçš„æ—¶é—´è§£æCBCTï¼Œä»¥æ•æ‰åŠ¨æ€è¿åŠ¨ã€‚ç”±äºä¸‰ç»´é«˜æ–¯å…·æœ‰å¼ºå¤§çš„è¡¨å¾èƒ½åŠ›ï¼ŒPMF-STGRå¯ä»¥ä»æ ‡å‡†çš„3D CBCTæ‰«æä¸­è¿›è¡Œâ€œå•æ¬¡â€è®­ç»ƒæ–¹å¼é‡å»ºåŠ¨æ€CBCTï¼Œè€Œæ— éœ€ä½¿ç”¨ä»»ä½•å…ˆå‰çš„è§£å‰–æˆ–è¿åŠ¨æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨XCATå¹»å½±æ¨¡æ‹Ÿå’Œå®é™…æ‚£è€…æ‰«æå¯¹PMF-STGRè¿›è¡Œäº†è¯„ä¼°ã€‚ä½¿ç”¨å›¾åƒç›¸å¯¹è¯¯å·®ã€ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°åº¦é‡ã€è‚¿ç˜¤è´¨å¿ƒè¯¯å·®å’Œåœ°æ ‡å®šä½è¯¯å·®ç­‰æŒ‡æ ‡æ¥è¯„ä¼°æ±‚è§£çš„åŠ¨æ€CBCTå’Œè¿åŠ¨çš„å‡†ç¡®æ€§ã€‚ä¸å½“å‰å…ˆè¿›çš„INRæ–¹æ³•PMF-STINRç›¸æ¯”ï¼ŒPMF-STGRå…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚ä¸PMF-STINRç›¸æ¯”ï¼ŒPMF-STGRåœ¨é‡å»ºæ—¶é—´å‡å°‘50%çš„åŒæ—¶ï¼Œé‡å»ºçš„å›¾åƒæ›´æ¸…æ™°ï¼Œè¿åŠ¨ç²¾åº¦æ›´é«˜ã€‚é€šè¿‡æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼ŒPMF-STGRå¢å¼ºäº†åŠ¨æ€CBCTæˆåƒçš„é€‚ç”¨æ€§ï¼Œå…·æœ‰æ½œåœ¨çš„ä¸´åºŠç¿»è¯‘ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.22139v2">PDF</a> 25 pages, 5 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¸‰ç»´é«˜æ–¯è¡¨ç¤ºçš„æ¡†æ¶ï¼ˆPMF-STGRï¼‰å®ç°äº†å¿«é€Ÿå‡†ç¡®çš„åŠ¨æ€CBCæˆåƒé‡å»ºã€‚å®ƒé‡‡ç”¨ä¸€å¥—å¯†é›†çš„ä¸‰ç»´é«˜æ–¯å‡½æ•°æ¥é‡å»ºå‚è€ƒå¸§CBCå›¾åƒï¼Œç”¨äºåŠ¨æ€åºåˆ—ï¼›å¦ä¸€å¥—ä¸‰ç»´é«˜æ–¯å‡½æ•°ç”¨äºæ•æ‰ä¸åŒçº§åˆ«çš„è¿åŠ¨åŸºç¡€åˆ†é‡ï¼ˆMBCsï¼‰ï¼›é‡‡ç”¨CNNï¼ˆå·ç§¯ç¥ç»ç½‘ç»œï¼‰åŸºäºè¿åŠ¨ç¼–ç å™¨è§£å†³æŠ•å½±ç‰¹å®šæ—¶é—´ç³»æ•°çš„é—®é¢˜ã€‚é€šè¿‡åŠ¨æ€è®­ç»ƒï¼ŒPMF-STGRèƒ½å¤Ÿä»æ ‡å‡†çš„ä¸‰ç»´CBCæ‰«æä¸­é‡å»ºåŠ¨æ€CBCå›¾åƒï¼Œæ— éœ€ä½¿ç”¨ä»»ä½•å…ˆéªŒè§£å‰–æˆ–è¿åŠ¨æ¨¡å‹ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä¸å½“å‰ä¸»æµæ–¹æ³•ç›¸æ¯”ï¼ŒPMF-STGRåœ¨é‡å»ºæ—¶é—´å’Œå›¾åƒè´¨é‡æ–¹é¢å‡è¡¨ç°å‡ºä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ—¶é—´è§£æCBCTæˆåƒå¯¹äºå¸¸è§„å’Œä¸è§„åˆ™è¿åŠ¨è¡¨å¾ã€æ‚£è€…è®¾ç½®å’Œè¿åŠ¨é€‚åº”æ”¾ç–—å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>PMF-STGRæ¡†æ¶åˆ©ç”¨ä¸‰ç»´é«˜æ–¯è¡¨ç¤ºæ¥å¿«é€Ÿå‡†ç¡®åœ°é‡å»ºåŠ¨æ€CBCTå›¾åƒã€‚</li>
<li>PMF-STGRåŒ…å«ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šç”¨äºé‡å»ºå‚è€ƒå¸§CBCTçš„å¯†é›†ä¸‰ç»´é«˜æ–¯é›†ã€æ•æ‰è¿åŠ¨åŸºç¡€åˆ†é‡ï¼ˆMBCsï¼‰çš„å¦ä¸€ä¸‰ç»´é«˜æ–¯é›†ä»¥åŠåŸºäºCNNçš„è¿åŠ¨ç¼–ç å™¨ã€‚</li>
<li>é€šè¿‡ç»“åˆå­¦ä¹ çš„MBCå’Œæ—¶é—´ç³»æ•°ï¼ŒPMF-STGRèƒ½å¤Ÿé‡å»ºæŠ•å½±ç‰¹å®šçš„æ—¶é—´è§£æCBCTå›¾åƒã€‚</li>
<li>PMF-STGRå…·æœ‰å¼ºå¤§çš„è¡¨ç¤ºèƒ½åŠ›ï¼Œå¯ä»æ ‡å‡†çš„ä¸‰ç»´CBCTæ‰«æä¸­é€šè¿‡ä¸€æ¬¡æ€§è®­ç»ƒé‡å»ºåŠ¨æ€CBCTå›¾åƒï¼Œæ— éœ€ä½¿ç”¨ä»»ä½•å…ˆéªŒè§£å‰–æˆ–è¿åŠ¨æ¨¡å‹ã€‚</li>
<li>è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒPMF-STGRåœ¨å›¾åƒè´¨é‡ã€è¿åŠ¨å‡†ç¡®æ€§å’Œé‡å»ºæ—¶é—´æ–¹é¢å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.22139">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d634c919b5907f5cbe0af67819e79853.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Versatile-Multimodal-Controls-for-Expressive-Talking-Human-Animation"><a href="#Versatile-Multimodal-Controls-for-Expressive-Talking-Human-Animation" class="headerlink" title="Versatile Multimodal Controls for Expressive Talking Human Animation"></a>Versatile Multimodal Controls for Expressive Talking Human Animation</h2><p><strong>Authors:Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Zixin Zhu, Sanping Zhou, Ming Yang, Le Wang</strong></p>
<p>In filmmaking, directors typically allow actors to perform freely based on the script before providing specific guidance on how to present key actions. AI-generated content faces similar requirements, where users not only need automatic generation of lip synchronization and basic gestures from audio input but also desire semantically accurate and expressive body movement that can be &#96;&#96;directly guidedâ€™â€™ through text descriptions. Therefore, we present VersaAnimator, a versatile framework that synthesizes expressive talking human videos from arbitrary portrait images. Specifically, we design a motion generator that produces basic rhythmic movements from audio input and supports text-prompt control for specific actions. The generated whole-body 3D motion tokens can animate portraits of various scales, producing talking heads, half-body gestures and even leg movements for whole-body images. Besides, we introduce a multi-modal controlled video diffusion that generates photorealistic videos, where speech signals govern lip synchronization, facial expressions, and head motions while body movements are guided by the 2D poses. Furthermore, we introduce a token2pose translator to smoothly map 3D motion tokens to 2D pose sequences. This design mitigates the stiffness resulting from direct 3D to 2D conversion and enhances the details of the generated body movements. Extensive experiments shows that VersaAnimator synthesizes lip-synced and identity-preserving videos while generating expressive and semantically meaningful whole-body motions. </p>
<blockquote>
<p>åœ¨ç”µå½±åˆ¶ä½œä¸­ï¼Œå¯¼æ¼”é€šå¸¸ä¼šè®©æ¼”å‘˜æ ¹æ®å‰§æœ¬è‡ªç”±å‘æŒ¥ï¼Œç„¶åå†æä¾›å…³äºå¦‚ä½•å‘ˆç°å…³é”®åŠ¨ä½œçš„å…·ä½“æŒ‡å¯¼ã€‚äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å†…å®¹é¢ä¸´ç€ç±»ä¼¼çš„è¦æ±‚ï¼Œç”¨æˆ·ä¸ä»…éœ€è¦è‡ªåŠ¨ç”Ÿæˆä¸éŸ³é¢‘è¾“å…¥åŒæ­¥çš„åŸºæœ¬åŠ¨ä½œï¼Œè¿˜å¸Œæœ›å¾—åˆ°è¯­ä¹‰å‡†ç¡®ã€å¯Œæœ‰è¡¨ç°åŠ›çš„èº«ä½“åŠ¨ä½œï¼Œè¿™äº›åŠ¨ä½œå¯ä»¥é€šè¿‡æ–‡å­—æè¿°æ¥â€œç›´æ¥æŒ‡å¯¼â€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VersaAnimatorï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå¯ä»¥ä»ä»»æ„çš„è‚–åƒå›¾ä¸­åˆæˆå¯Œæœ‰è¡¨ç°åŠ›çš„è¯´è¯äººç±»è§†é¢‘ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŠ¨ä½œç”Ÿæˆå™¨ï¼Œå®ƒå¯ä»¥ä»éŸ³é¢‘è¾“å…¥ä¸­äº§ç”ŸåŸºæœ¬çš„æœ‰èŠ‚å¥çš„åŠ¨ä½œï¼Œå¹¶æ”¯æŒé€šè¿‡æ–‡å­—æç¤ºæ¥æ§åˆ¶ç‰¹å®šåŠ¨ä½œã€‚ç”Ÿæˆçš„å…¨èº«3DåŠ¨ä½œä»¤ç‰Œå¯ä»¥é©±åŠ¨å„ç§è§„æ¨¡çš„è‚–åƒå›¾ï¼Œäº§ç”Ÿè¯´è¯å¤´éƒ¨ã€åŠèº«æ‰‹åŠ¿ç”šè‡³å…¨èº«å›¾åƒçš„è…¿éƒ¨åŠ¨ä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šæ¨¡å¼æ§åˆ¶è§†é¢‘æ‰©æ•£æŠ€æœ¯ï¼Œç”Ÿæˆé€¼çœŸçš„è§†é¢‘ï¼Œå…¶ä¸­è¯­éŸ³ä¿¡å·æ§åˆ¶å”‡éƒ¨åŒæ­¥ã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨åŠ¨ä½œï¼Œè€Œèº«ä½“åŠ¨ä½œåˆ™ç”±äºŒç»´å§¿åŠ¿å¼•å¯¼ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬è¿˜æ¨å‡ºäº†token2poseè½¬æ¢å™¨ï¼Œå®ƒèƒ½å°†3DåŠ¨ä½œä»¤ç‰Œå¹³ç¨³åœ°æ˜ å°„åˆ°2Då§¿åŠ¿åºåˆ—ã€‚è¿™ç§è®¾è®¡å‡è½»äº†ç›´æ¥3Dåˆ°2Dè½¬æ¢å¯¼è‡´çš„åƒµç¡¬æ„Ÿï¼Œå¹¶å¢å¼ºäº†ç”Ÿæˆçš„èº«ä½“åŠ¨ä½œçš„ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVersaAnimatorèƒ½åˆæˆå”‡éŸ³åŒæ­¥ã€èº«ä»½ä¿ç•™çš„è§†é¢‘ï¼ŒåŒæ—¶ç”Ÿæˆå¯Œæœ‰è¡¨ç°åŠ›å’Œè¯­ä¹‰çš„å…¨èº«åŠ¨ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08714v4">PDF</a> Accepted by ACM MM2025</p>
<p><strong>Summary</strong></p>
<p>VersaAnimatoræ¡†æ¶å¯ä»¥ç”ŸæˆåŸºäºä»»æ„è‚–åƒå›¾åƒçš„è¡¨è¾¾å¼è¯´è¯äººç±»è§†é¢‘ã€‚è¯¥æ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªåŠ¨ä½œç”Ÿæˆå™¨ï¼Œå¯ä»¥ä»éŸ³é¢‘è¾“å…¥äº§ç”ŸåŸºæœ¬èŠ‚å¥åŠ¨ä½œå¹¶æ”¯æŒé€šè¿‡æ–‡æœ¬æè¿°è¿›è¡Œç‰¹å®šåŠ¨ä½œçš„ç›´æ¥æŒ‡å¯¼ã€‚æ­¤æ¡†æ¶èƒ½ç”Ÿæˆå…¨èº«3DåŠ¨ä½œä»¤ç‰Œï¼Œå¯ä»¥åŠ¨ç”»åŒ–ä¸åŒè§„æ¨¡çš„è‚–åƒï¼Œäº§ç”Ÿè°ˆè¯å¤´éƒ¨ã€åŠèº«å§¿åŠ¿ç”šè‡³æ˜¯å…¨èº«å›¾åƒè…¿éƒ¨åŠ¨ä½œã€‚æ­¤å¤–ï¼Œå¼•å…¥å¤šæ¨¡å¼æ§åˆ¶è§†é¢‘æ‰©æ•£æŠ€æœ¯ï¼Œç”ŸæˆçœŸå®æ„Ÿè§†é¢‘ï¼Œè¯­éŸ³ä¿¡å·æ§åˆ¶å”‡éƒ¨åŒæ­¥ã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨è¿åŠ¨ï¼Œè€Œèº«ä½“è¿åŠ¨åˆ™ç”±2Då§¿åŠ¿å¼•å¯¼ã€‚åŒæ—¶ï¼Œå¼•å…¥token2poseç¿»è¯‘å™¨ï¼Œå°†3DåŠ¨ä½œä»¤ç‰Œå¹³æ»‘æ˜ å°„åˆ°2Då§¿åŠ¿åºåˆ—ï¼Œå¢å¼ºç”Ÿæˆçš„èº«ä½“è¿åŠ¨ç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼ŒVersaAnimatoråˆæˆçš„è§†é¢‘å…·æœ‰å”‡éƒ¨åŒæ­¥ã€èº«ä»½ä¿ç•™çš„ç‰¹ç‚¹ï¼ŒåŒæ—¶èƒ½ç”Ÿæˆè¡¨è¾¾åŠ›å¼ºã€è¯­ä¹‰æ˜ç¡®çš„å…¨èº«åŠ¨ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VersaAnimatoræ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå¯ä»¥ä»ä»»æ„è‚–åƒå›¾åƒç”Ÿæˆè¡¨è¾¾å¼è¯´è¯äººç±»è§†é¢‘ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…å«è¿åŠ¨ç”Ÿæˆå™¨ï¼Œèƒ½æ ¹æ®éŸ³é¢‘è¾“å…¥äº§ç”ŸåŸºæœ¬èŠ‚å¥åŠ¨ä½œï¼Œå¹¶æ”¯æŒæ–‡æœ¬æè¿°å¯¹ç‰¹å®šåŠ¨ä½œçš„ç›´æ¥æŒ‡å¯¼ã€‚</li>
<li>èƒ½ç”Ÿæˆå…¨èº«3DåŠ¨ä½œä»¤ç‰Œï¼Œé€‚ç”¨äºä¸åŒè§„æ¨¡çš„è‚–åƒåŠ¨ç”»ï¼ŒåŒ…æ‹¬è°ˆè¯å¤´éƒ¨ã€åŠèº«å§¿åŠ¿å’Œå…¨èº«å›¾åƒè…¿éƒ¨åŠ¨ä½œã€‚</li>
<li>å¼•å…¥å¤šæ¨¡å¼æ§åˆ¶è§†é¢‘æ‰©æ•£æŠ€æœ¯ï¼Œç»“åˆè¯­éŸ³ä¿¡å·å’Œ2Då§¿åŠ¿ï¼Œç”ŸæˆçœŸå®æ„Ÿè§†é¢‘ã€‚</li>
<li>token2poseç¿»è¯‘å™¨ç”¨äºå°†3DåŠ¨ä½œä»¤ç‰Œæ˜ å°„åˆ°2Då§¿åŠ¿åºåˆ—ï¼Œå¢å¼ºèº«ä½“è¿åŠ¨ç»†èŠ‚çš„çœŸå®æ€§ã€‚</li>
<li>VersaAnimatoråˆæˆçš„è§†é¢‘å…·æœ‰å”‡éƒ¨åŒæ­¥ã€èº«ä»½ä¿ç•™ç‰¹ç‚¹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4ece756ddbcbb9cb90df7645521e10d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7f12638b9cf1c0f25da3f50bc91e174.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8edeef7feb631e64561559cfeaf972c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7864a8c243ae7b482377f2235a595d5f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1841831cdf57ca48d05cf99b17e27349.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-245c7d0f1c0b208f404ad96895c9a136.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Rapid-and-Reproducible-Whole-Brain-Multi-Pool-CEST-Imaging-at-3T-Using-a-Single-Shot-True-FISP-Readout"><a href="#Rapid-and-Reproducible-Whole-Brain-Multi-Pool-CEST-Imaging-at-3T-Using-a-Single-Shot-True-FISP-Readout" class="headerlink" title="Rapid and Reproducible Whole-Brain Multi-Pool CEST Imaging at 3T Using a   Single-Shot True FISP Readout"></a>Rapid and Reproducible Whole-Brain Multi-Pool CEST Imaging at 3T Using a   Single-Shot True FISP Readout</h2><p><strong>Authors:Yupeng Wu, Siyuan Fang, Siyuan Wang, Caixia Fu, Jianqi Li</strong></p>
<p>Purpose: To develop and validate a comprehensive, rapid, and reproducible solution for whole-brain, multi-pool CEST imaging at 3T, overcoming key barriers to clinical translation such as long acquisition times and inaccuracies from field inhomogeneities. Methods: This study integrated a single-shot 3D True FISP readout sequence for efficient whole-brain CEST data acquisition. A streamlined workflow was developed to acquire B0, B1, and T1 maps for correction. To overcome the time-consuming nature of traditional B1 correction, we implemented a machine learning-based method to perform rapid B1 correction using data from a single B1 power acquisition. Data were analyzed using a four-pool Lorentzian model to derive quantitative metrics, including MTRLD and the AREX. The methodâ€™s accuracy was validated in phantoms and its test-retest reproducibility was assessed in healthy volunteers across 96 brain regions. Results: The True FISP sequence acquired high-quality, whole-brain images free of major artifacts. The neural network accurately replicated the gold-standard three-point B1 correction, achieving excellent intraclass correlation (ICC &gt; 0.97) in human subjects. The AREX metric successfully corrected for T1 and MT confounders, reducing the CV from 33.6% to 6.9% in phantoms. The complete pipeline, including Z-spectrum and correction maps, took approximately 9 minutes. The method demonstrated high region-level reproducibility, with the average CV for APT_AREX under 10% for most brain regions across test-retest scans. Conclusion: This study presents a validated, end-to-end solution for whole-brain, multi-pool CEST imaging. By combining an efficient sequence with a rapid, AI-driven correction pipeline and robust quantitative analysis, our method delivers high-fidelity, reproducible, and quantitative multi-parameter maps of brain metabolism in a clinically acceptable timeframe. </p>
<blockquote>
<p>ç›®çš„ï¼šæ—¨åœ¨å¼€å‘å¹¶éªŒè¯ä¸€ç§å…¨é¢ã€å¿«é€Ÿã€å¯é‡å¤çš„é’ˆå¯¹å…¨è„‘å¤šæ± åŒ–å­¦äº¤æ¢é¥±å’Œè½¬ç§»æˆåƒï¼ˆCESTæˆåƒï¼‰çš„è§£å†³æ–¹æ¡ˆï¼Œå…‹æœé•¿æœŸå­˜åœ¨çš„ä¸´åºŠç¿»è¯‘éšœç¢ï¼Œå¦‚é‡‡é›†æ—¶é—´è¿‡é•¿ä»¥åŠç”±äºç£åœºä¸å‡åŒ€æ€§å¯¼è‡´çš„å‡†ç¡®æ€§é—®é¢˜ã€‚æ–¹æ³•ï¼šæœ¬ç ”ç©¶ç»“åˆäº†å•å‘ä¸‰ç»´çœŸå®FISPè¯»å‡ºåºåˆ—ï¼Œç”¨äºé«˜æ•ˆå…¨è„‘CESTæ•°æ®é‡‡é›†ã€‚ä¸ºäº†è·å–æ ¡æ­£æ‰€éœ€çš„B0ã€B1å’ŒT1å›¾ï¼Œå¼€å‘äº†ä¸€ä¸ªç®€åŒ–çš„å·¥ä½œæµç¨‹ã€‚ä¸ºäº†å…‹æœä¼ ç»ŸB1æ ¡æ­£è€—æ—¶çš„é—®é¢˜ï¼Œæˆ‘ä»¬å®æ–½äº†ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•è¿›è¡Œå¿«é€ŸB1æ ¡æ­£ï¼Œä½¿ç”¨å•æ¬¡B1åŠŸç‡é‡‡é›†çš„æ•°æ®ã€‚æ•°æ®ä½¿ç”¨å››æ± æ´›ä¼¦å…¹æ¨¡å‹è¿›è¡Œåˆ†æï¼Œä»¥æ¨å¯¼å®šé‡æŒ‡æ ‡ï¼ŒåŒ…æ‹¬MTRLDå’ŒAREXã€‚è¯¥æ–¹æ³•çš„å‡†ç¡®æ€§åœ¨æ¨¡æ‹Ÿä½“æ¨¡ä¸­å¾—åˆ°äº†éªŒè¯ï¼Œå¹¶åœ¨å¥åº·å¿—æ„¿è€…çš„96ä¸ªè„‘åŒºä¸­å¯¹æµ‹è¯•é‡å¤å†ç°æ€§è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœï¼šTrue FISPåºåˆ—è·å¾—çš„é«˜è´¨é‡å…¨è„‘å›¾åƒæ— ä¸»è¦ä¼ªå½±ã€‚ç¥ç»ç½‘ç»œå‡†ç¡®å¤åˆ¶äº†é‡‘æ ‡å‡†ä¸‰ç‚¹B1æ ¡æ­£ï¼Œåœ¨äººç±»å—è¯•è€…ä¸­è¾¾åˆ°äº†è‰¯å¥½çš„ç»„å†…ç›¸å…³ç³»æ•°ï¼ˆICC&gt; 0.97ï¼‰ã€‚AREXæŒ‡æ ‡æˆåŠŸåœ°æ ¡æ­£äº†T1å’ŒMTæ··æ·†å› ç´ ï¼Œå°†æ¨¡æ‹Ÿä½“æ¨¡ä¸­çš„å˜å¼‚ç³»æ•°ä»33.6%å‡å°‘åˆ°6.9%ã€‚å®Œæ•´çš„æµç¨‹åŒ…æ‹¬Zè°±å’Œæ ¡æ­£å›¾ï¼Œå¤§çº¦éœ€è¦9åˆ†é’Ÿã€‚è¯¥æ–¹æ³•æ˜¾ç¤ºå‡ºè¾ƒé«˜çš„åŒºåŸŸæ°´å¹³å¯é‡å¤æ€§ï¼Œå¤§å¤šæ•°è„‘åŒºçš„APT_AREXæµ‹è¯•å†æµ‹è¯•æ‰«æçš„å¹³å‡å˜å¼‚ç³»æ•°ä½äº10%ã€‚ç»“è®ºï¼šæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»è¿‡éªŒè¯çš„ç«¯åˆ°ç«¯å…¨è„‘å¤šæ± CESTæˆåƒè§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ç»“åˆé«˜æ•ˆçš„åºåˆ—ã€å¿«é€Ÿçš„äººå·¥æ™ºèƒ½é©±åŠ¨æ ¡æ­£ç®¡é“å’Œç¨³å¥çš„å®šé‡åˆ†æï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯æ¥å—çš„ä¸´åºŠæ—¶é—´å†…æä¾›äº†é«˜ä¿çœŸã€å¯é‡å¤å’Œå®šé‡çš„è„‘ä»£è°¢å¤šå‚æ•°å›¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.16986v3">PDF</a> Keywords: CEST, whole-brain, multi-pool, true fast imaging with   steady-state precession (True FISP), balanced steady state free precession   (bSSFP)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ—¨åœ¨å¼€å‘å¹¶éªŒè¯ä¸€ç§å…¨é¢ã€å¿«é€Ÿã€å¯é‡å¤çš„é’ˆå¯¹å…¨è„‘å¤šæ±  CEST æˆåƒçš„ç»¼åˆè§£å†³æ–¹æ¡ˆã€‚è¯¥ç ”ç©¶é€šè¿‡æ•´åˆå•å‘å°„ 3D True FISP è¯»å‡ºåºåˆ—ï¼Œå®ç°äº†é«˜æ•ˆçš„å…¨è„‘ CEST æ•°æ®é‡‡é›†ã€‚é€šè¿‡å¼€å‘æ ‡å‡†åŒ–å·¥ä½œæµç¨‹ï¼Œç”¨äºè·å– B0ã€B1 å’Œ T1 å›¾è¿›è¡Œæ ¡æ­£ã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿ B1 æ ¡æ­£çš„æ—¶é—´æ¶ˆè€—ï¼Œç ”ç©¶å®æ–½äº†åŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œåˆ©ç”¨å•æ¬¡ B1 åŠŸç‡é‡‡é›†æ•°æ®è¿›è¡Œå¿«é€Ÿ B1 æ ¡æ­£ã€‚è¯¥ç ”ç©¶åœ¨å¹»å½±ä¸­éªŒè¯äº†æ–¹æ³•çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨å¥åº·å¿—æ„¿è€…ä¸­å¯¹ 96 ä¸ªè„‘åŒºè¿›è¡Œäº†æµ‹è¯•-å†æµ‹è¯•çš„å¯é‡å¤æ€§è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒTrue FISP åºåˆ—è·å–çš„å…¨è„‘å›¾åƒè´¨é‡é«˜ä¸”æ— ä¸»è¦ä¼ªå½±ã€‚ç¥ç»ç½‘ç»œå‡†ç¡®å¤åˆ¶äº†é‡‘æ ‡å‡†ä¸‰ç‚¹ B1 æ ¡æ­£ï¼Œåœ¨äººç±»å—è¯•è€…ä¸­è¾¾åˆ°äº†æé«˜çš„ç»„å†…ç›¸å…³ç³»æ•°ï¼ˆICC &gt; 0.97ï¼‰ã€‚AREX æŒ‡æ ‡æˆåŠŸæ ¡æ­£äº† T1 å’Œ MT å¹²æ‰°å› ç´ ï¼Œåœ¨å¹»å½±ä¸­å°†å˜å¼‚ç³»æ•°ä» 33.6% é™ä½åˆ° 6.9%ã€‚æ•´ä¸ªç®¡é“åŒ…æ‹¬ Z å…‰è°±å’Œæ ¡æ­£å›¾ï¼Œå¤§çº¦éœ€è¦ 9 åˆ†é’Ÿã€‚è¯¥æ–¹æ³•æ˜¾ç¤ºäº†é«˜åº¦åŒºåŸŸå¯é‡å¤æ€§ï¼Œåœ¨æµ‹è¯•-å†æµ‹è¯•æ‰«æä¸­ï¼Œå¤§å¤šæ•°è„‘åŒºçš„ APT_AREX å¹³å‡å˜å¼‚ç³»æ•°ä½äº 10%ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç»è¿‡éªŒè¯çš„ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆï¼Œç”¨äºå…¨è„‘å¤šæ±  CEST æˆåƒï¼Œè¯¥æ–¹æ³•ç»“åˆäº†é«˜æ•ˆçš„åºåˆ—ã€å¿«é€Ÿçš„ AI é©±åŠ¨æ ¡æ­£ç®¡é“å’Œç¨³å¥çš„å®šé‡åˆ†æï¼Œæä¾›é«˜ä¿çœŸã€å¯é‡å¤å’Œé‡åŒ–çš„è„‘ä»£è°¢å¤šå‚æ•°å›¾ï¼Œä¸´åºŠå¯æ¥å—æ—¶é—´èŒƒå›´å†…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶ç›®æ ‡æ˜¯å¼€å‘å¹¶éªŒè¯ä¸€ç§é€‚ç”¨äºå…¨è„‘å¤šæ±  CEST æˆåƒçš„ç»¼åˆè§£å†³æ–¹æ¡ˆã€‚</li>
<li>é€šè¿‡æ•´åˆå•å‘å°„ 3D True FISP è¯»å‡ºåºåˆ—å®ç°äº†é«˜æ•ˆçš„å…¨è„‘æ•°æ®è·å–ã€‚</li>
<li>åˆ©ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯å®ç°å¿«é€Ÿ B1 æ ¡æ­£ä»¥ç¼©çŸ­æˆåƒæ—¶é—´ã€‚</li>
<li>ä½¿ç”¨å››æ± æ´›ä¼¦å…¹æ¨¡å‹è¿›è¡Œæ•°æ®åˆ†æï¼Œä»¥ç”Ÿæˆå®šé‡æŒ‡æ ‡ã€‚</li>
<li>æ–¹æ³•åœ¨å¹»å½±ä¸­çš„å‡†ç¡®æ€§å’Œåœ¨äººç±»å—è¯•è€…ä¸­çš„å¯é‡å¤æ€§å¾—åˆ°äº†éªŒè¯ã€‚</li>
<li>AREX æŒ‡æ ‡æˆåŠŸæ ¡æ­£äº† T1 å’Œ MT çš„å¹²æ‰°å› ç´ ï¼Œæé«˜äº†æˆåƒè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.16986">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c6e1756599463c2a1a4bffb0fc043f79.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Categorical-Schrodinger-Bridge-Matching"><a href="#Categorical-Schrodinger-Bridge-Matching" class="headerlink" title="Categorical SchrÃ¶dinger Bridge Matching"></a>Categorical SchrÃ¶dinger Bridge Matching</h2><p><strong>Authors:Grigoriy Ksenofontov, Alexander Korotin</strong></p>
<p>The Schr&quot;odinger Bridge (SB) is a powerful framework for solving generative modeling tasks such as unpaired domain translation. Most SB-related research focuses on continuous data space $\mathbb{R}^{D}$ and leaves open theoretical and algorithmic questions about applying SB methods to discrete data, e.g, on finite spaces $\mathbb{S}^{D}$. Notable examples of such sets $\mathbb{S}$ are codebooks of vector-quantized (VQ) representations of modern autoencoders, tokens in texts, categories of atoms in molecules, etc. In this paper, we provide a theoretical and algorithmic foundation for solving SB in discrete spaces using the recently introduced Iterative Markovian Fitting (IMF) procedure. Specifically, we theoretically justify the convergence of discrete-time IMF (D-IMF) to SB in discrete spaces. This enables us to develop a practical computational algorithm for SB, which we call Categorical Schr&quot;odinger Bridge Matching (CSBM). We show the performance of CSBM via a series of experiments with synthetic data and VQ representations of images. The code of CSBM is available at <a target="_blank" rel="noopener" href="https://github.com/gregkseno/csbm">https://github.com/gregkseno/csbm</a>. </p>
<blockquote>
<p>è–›å®šè°”æ¡¥ï¼ˆSBï¼‰æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œç”¨äºè§£å†³å¦‚éé…å¯¹åŸŸç¿»è¯‘ç­‰ç”Ÿæˆå»ºæ¨¡ä»»åŠ¡ã€‚å¤§å¤šæ•°ä¸SBç›¸å…³çš„ç ”ç©¶éƒ½é›†ä¸­åœ¨è¿ç»­æ•°æ®ç©ºé—´$\mathbb{R}^{D}$ä¸Šï¼Œè€Œå°†SBæ–¹æ³•åº”ç”¨äºç¦»æ•£æ•°æ®ï¼ˆä¾‹å¦‚æœ‰é™ç©ºé—´$\mathbb{S}^{D}$ï¼‰çš„ç†è®ºå’Œç®—æ³•é—®é¢˜ç•™å¾…è§£å†³ã€‚é›†åˆ$\mathbb{S}$çš„è‘—åä¾‹å­åŒ…æ‹¬ç°ä»£è‡ªåŠ¨ç¼–ç å™¨çš„å‘é‡é‡åŒ–ï¼ˆVQï¼‰è¡¨ç¤ºçš„ä»£ç æœ¬ã€æ–‡æœ¬ä¸­çš„ä»¤ç‰Œã€åˆ†å­ä¸­çš„åŸå­ç±»åˆ«ç­‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨æœ€è¿‘æå‡ºçš„è¿­ä»£é©¬å°”å¯å¤«æ‹Ÿåˆï¼ˆIMFï¼‰ç¨‹åºï¼Œä¸ºè§£å†³ç¦»æ•£ç©ºé—´ä¸­çš„SBæä¾›äº†ç†è®ºå’Œç®—æ³•åŸºç¡€ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä»ç†è®ºä¸Šè¯æ˜äº†ç¦»æ•£æ—¶é—´IMFï¼ˆD-IMFï¼‰æ”¶æ•›åˆ°ç¦»æ•£ç©ºé—´ä¸­çš„SBã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¸ºSBå¼€å‘ä¸€ç§å®ç”¨çš„è®¡ç®—ç®—æ³•ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºåˆ†ç±»è–›å®šè°”æ¡¥åŒ¹é…ï¼ˆCSBMï¼‰ã€‚æˆ‘ä»¬é€šè¿‡åˆæˆæ•°æ®å’ŒVQå›¾åƒè¡¨ç¤ºçš„ä¸€ç³»åˆ—å®éªŒå±•ç¤ºäº†CSBMçš„æ€§èƒ½ã€‚CSBMçš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/gregkseno/csbm%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/gregkseno/csbmæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01416v3">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ä»‹ç»äº†SchrÃ¶dinger Bridgeï¼ˆSBï¼‰åœ¨ç¦»æ•£ç©ºé—´ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡ä½¿ç”¨æœ€è¿‘å¼•å…¥çš„Iterative Markovian Fittingï¼ˆIMFï¼‰ç¨‹åºè§£å†³SBåœ¨ç¦»æ•£ç©ºé—´ä¸­çš„ç†è®ºå’Œæ–¹æ³•åŸºç¡€ã€‚ä½œè€…æå‡ºäº†ä¸€ç§å®ç”¨çš„è®¡ç®—ç®—æ³•ï¼Œç§°ä¸ºCategorical SchrÃ¶dinger Bridge Matchingï¼ˆCSBMï¼‰ï¼Œå¹¶é€šè¿‡åˆæˆæ•°æ®å’ŒVQå›¾åƒè¡¨ç¤ºçš„å®éªŒéªŒè¯äº†å…¶æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SchrÃ¶dinger Bridge (SB) æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ç”Ÿæˆå»ºæ¨¡æ¡†æ¶ï¼Œç”¨äºè§£å†³å¦‚æœªé…å¯¹é¢†åŸŸç¿»è¯‘ç­‰ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¤§å¤šå…³æ³¨è¿ç»­æ•°æ®ç©ºé—´ä¸­çš„SBï¼Œè€Œå¯¹ç¦»æ•£æ•°æ®åº”ç”¨SBæ–¹æ³•ä»å­˜åœ¨ç†è®ºå’Œç®—æ³•é—®é¢˜ã€‚</li>
<li>æ–‡ç« æä¾›äº†åœ¨ç¦»æ•£ç©ºé—´ä¸­è§£å†³SBçš„ç†è®ºå’Œç®—æ³•åŸºç¡€ï¼Œä½¿ç”¨æ–°å¼•å…¥çš„Iterative Markovian Fitting (IMF) ç¨‹åºã€‚</li>
<li>ç¦»æ•£æ—¶é—´IMFï¼ˆD-IMFï¼‰æ”¶æ•›åˆ°SBçš„ç†è®ºè¯æ˜ã€‚</li>
<li>åŸºäºæ­¤ï¼Œå¼€å‘äº†ä¸€ç§å®ç”¨çš„è®¡ç®—ç®—æ³•Categorical SchrÃ¶dinger Bridge Matching (CSBM)ã€‚</li>
<li>é€šè¿‡åˆæˆæ•°æ®å’ŒVQå›¾åƒè¡¨ç¤ºçš„å®éªŒéªŒè¯äº†CSBMçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01416">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-38d216fe6692d73a4bd67f900ea1c5a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c79a7adcc688f219015178fcc4167115.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Unified-3D-MRI-Representations-via-Sequence-Invariant-Contrastive-Learning"><a href="#Unified-3D-MRI-Representations-via-Sequence-Invariant-Contrastive-Learning" class="headerlink" title="Unified 3D MRI Representations via Sequence-Invariant Contrastive   Learning"></a>Unified 3D MRI Representations via Sequence-Invariant Contrastive   Learning</h2><p><strong>Authors:Liam Chalcroft, Jenny Crinion, Cathy J. Price, John Ashburner</strong></p>
<p>Self-supervised deep learning has accelerated 2D natural image analysis but remains difficult to translate into 3D MRI, where data are scarce and pre-trained 2D backbones cannot capture volumetric context. We present a \emph{sequence-invariant} self-supervised framework leveraging quantitative MRI (qMRI). By simulating multiple MRI contrasts from a single 3D qMRI scan and enforcing consistent representations across these contrasts, we learn anatomy-centric rather than sequence-specific features. The result is a single 3D encoder that excels across tasks and protocols. Experiments on healthy brain segmentation (IXI), stroke lesion segmentation (ARC), and MRI denoising show significant gains over baseline SSL approaches, especially in low-data settings (up to +8.3% Dice, +4.2 dB PSNR). It also generalises to unseen sites, supporting scalable clinical use. Code and trained models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/contrast-squared">https://github.com/liamchalcroft/contrast-squared</a> </p>
<blockquote>
<p>è‡ªç›‘ç£æ·±åº¦å­¦ä¹ å·²åŠ é€Ÿ2Dè‡ªç„¶å›¾åƒåˆ†æï¼Œä½†è½¬åŒ–ä¸º3D MRIä»ç„¶å›°éš¾ï¼Œ3D MRIæ•°æ®ç¨€ç¼ºï¼Œé¢„è®­ç»ƒçš„2Déª¨å¹²ç½‘æ— æ³•æ•æ‰ä½“ç§¯ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å®šé‡MRIï¼ˆqMRIï¼‰çš„åºåˆ—ä¸å˜è‡ªç›‘ç£æ¡†æ¶ã€‚é€šè¿‡æ¨¡æ‹Ÿå•ä¸ª3D qMRIæ‰«æçš„å¤šä¸ªMRIå¯¹æ¯”åº¦ï¼Œå¹¶å¼ºåˆ¶è¿™äº›å¯¹æ¯”åº¦ä¹‹é—´ä¿æŒä¸€è‡´çš„è¡¨ç¤ºï¼Œæˆ‘ä»¬å­¦ä¹ ä»¥è§£å‰–ä¸ºä¸­å¿ƒçš„ç‰¹å¾ï¼Œè€Œéç‰¹å®šåºåˆ—çš„ç‰¹å¾ã€‚ç»“æœæ˜¯ä¸€ä¸ªå•ä¸€çš„3Dç¼–ç å™¨ï¼Œå®ƒåœ¨å„ç§ä»»åŠ¡å’Œåè®®ä¸­éƒ½è¡¨ç°å‡ºè‰²ã€‚åœ¨å¥åº·å¤§è„‘åˆ†å‰²ï¼ˆIXIï¼‰ã€ä¸­é£ç—…ç¶åˆ†å‰²ï¼ˆARCï¼‰å’ŒMRIé™å™ªæ–¹é¢çš„å®éªŒæ˜¾ç¤ºï¼Œä¸ä¼ ç»Ÿçš„è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼ˆæœ€é«˜å¯æé«˜+8.3ï¼…çš„Diceç³»æ•°ï¼Œ+4.2åˆ†è´å³°å€¼ä¿¡å™ªæ¯”ï¼‰ï¼Œè¯¥æ–¹æ³•å–å¾—äº†æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚å®ƒè¿˜é€‚ç”¨äºæœªè§è¿‡çš„ç«™ç‚¹ï¼Œæ”¯æŒå¯æ‰©å±•çš„ä¸´åºŠä½¿ç”¨ã€‚ç›¸å…³ä»£ç å’Œè®­ç»ƒæ¨¡å‹å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/contrast-squared%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/liamchalcroft/contrast-squaredå…¬å¼€å¯ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.12057v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºå®šé‡MRIï¼ˆqMRIï¼‰çš„åºåˆ—ä¸å˜è‡ªç›‘ç£æ¡†æ¶ï¼Œé€šè¿‡æ¨¡æ‹Ÿå•ä¸ª3DqMRIæ‰«æçš„å¤šä¸ªMRIå¯¹æ¯”åº¦å¹¶å¼ºåˆ¶è¿™äº›å¯¹æ¯”åº¦ä¹‹é—´çš„ä¸€è‡´æ€§è¡¨ç¤ºï¼Œå­¦ä¹ ä»¥è§£å‰–ä¸ºä¸­å¿ƒè€Œéç‰¹å®šåºåˆ—çš„ç‰¹å¾ã€‚è¯¥æ¡†æ¶ä½¿3D MRIåˆ†æå—ç›Šäºè‡ªç›‘ç£æ·±åº¦å­¦ä¹ ï¼Œå¹¶å…‹æœäº†åœ¨ä½æ•°æ®æƒ…å†µä¸‹åº”ç”¨å›°éš¾çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åœ¨å„ç§ä»»åŠ¡å’Œåè®®ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå°¤å…¶åœ¨å¥åº·å¤§è„‘åˆ†å‰²ï¼ˆIXIï¼‰ã€ä¸­é£ç—…ç¶åˆ†å‰²ï¼ˆARCï¼‰å’ŒMRIå»å™ªæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ä»£ç å’Œè®­ç»ƒæ¨¡å‹å·²å…¬å¼€åœ¨GitHubä¸Šæä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è‡ªç›‘ç£æ·±åº¦å­¦ä¹ åœ¨åŠ é€ŸäºŒç»´è‡ªç„¶å›¾åƒåˆ†ææ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†éš¾ä»¥åº”ç”¨äºä¸‰ç»´MRIæ•°æ®ã€‚</li>
<li>ä¸‰ç»´MRIæ•°æ®ç¨€ç¼ºï¼Œä¸”é¢„è®­ç»ƒçš„äºŒç»´éª¨å¹²ç½‘æ— æ³•æ•æ‰ä½“ç§¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé€ æˆåº”ç”¨å›°éš¾ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åˆ©ç”¨å®šé‡MRIï¼ˆqMRIï¼‰çš„åºåˆ—ä¸å˜è‡ªç›‘ç£æ¡†æ¶ï¼Œæ¨¡æ‹Ÿå•ä¸ª3DqMRIæ‰«æçš„å¤šä¸ªMRIå¯¹æ¯”åº¦å¹¶å¼ºåˆ¶å…¶ä¸€è‡´æ€§è¡¨ç¤ºã€‚</li>
<li>å­¦ä¹ ä»¥è§£å‰–ä¸ºä¸­å¿ƒçš„ç‰¹å¾è€Œéç‰¹å®šåºåˆ—çš„ç‰¹å¾ï¼Œè§£å†³MRIæ•°æ®çš„ç‹¬ç‰¹æŒ‘æˆ˜ã€‚</li>
<li>å•ä¸€3Dç¼–ç å™¨åœ¨ä¸åŒä»»åŠ¡å’Œåè®®ä¸­è¡¨ç°å“è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æ•°æ®è®¾ç½®ä¸‹ã€‚</li>
<li>åœ¨å¥åº·å¤§è„‘åˆ†å‰²ã€ä¸­é£ç—…ç¶åˆ†å‰²å’ŒMRIå»å™ªæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç›¸è¾ƒäºåŸºçº¿SSLæ–¹æ³•æœ‰æ˜æ˜¾æå‡ã€‚</li>
<li>è¯¥æ¡†æ¶å…·æœ‰åœ¨æœªè§è¿‡çš„ç«™ç‚¹è¿›è¡Œæ¨å¹¿çš„èƒ½åŠ›ï¼Œæ”¯æŒå¯æ‰©å±•çš„ä¸´åºŠåº”ç”¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.12057">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cbd68edece167c0e394d6df698ef8826.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae1048d249a80f7f2442ee94efa4eccf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-abaafaa8a84973257a8628f39371b17d.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="FBSDiff-Plug-and-Play-Frequency-Band-Substitution-of-Diffusion-Features-for-Highly-Controllable-Text-Driven-Image-Translation"><a href="#FBSDiff-Plug-and-Play-Frequency-Band-Substitution-of-Diffusion-Features-for-Highly-Controllable-Text-Driven-Image-Translation" class="headerlink" title="FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features   for Highly Controllable Text-Driven Image Translation"></a>FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features   for Highly Controllable Text-Driven Image Translation</h2><p><strong>Authors:Xiang Gao, Jiaying Liu</strong></p>
<p>Large-scale text-to-image diffusion models have been a revolutionary milestone in the evolution of generative AI and multimodal technology, allowing wonderful image generation with natural-language text prompt. However, the issue of lacking controllability of such models restricts their practical applicability for real-life content creation. Thus, attention has been focused on leveraging a reference image to control text-to-image synthesis, which is also regarded as manipulating (or editing) a reference image as per a text prompt, namely, text-driven image-to-image translation. This paper contributes a novel, concise, and efficient approach that adapts pre-trained large-scale text-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality and versatile text-driven I2I translation without any model training, model fine-tuning, or online optimization process. To guide T2I generation with a reference image, we propose to decompose diverse guiding factors with different frequency bands of diffusion features in the DCT spectral space, and accordingly devise a novel frequency band substitution layer which realizes dynamic control of the reference image to the T2I generation result in a plug-and-play manner. We demonstrate that our method allows flexible control over both guiding factor and guiding intensity of the reference image simply by tuning the type and bandwidth of the substituted frequency band, respectively. Extensive qualitative and quantitative experiments verify superiority of our approach over related methods in I2I translation visual quality, versatility, and controllability. The code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/XiangGao1102/FBSDiff">https://github.com/XiangGao1102/FBSDiff</a>. </p>
<blockquote>
<p>å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å’Œå¤šæ¨¡æ€æŠ€æœ¯çš„æ¼”å˜ä¸­æˆä¸ºäº†ä¸€ä¸ªé©å‘½æ€§çš„é‡Œç¨‹ç¢‘ã€‚å®ƒèƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€æ–‡æœ¬æç¤ºæ¥ç”Ÿæˆç²¾å½©çš„å›¾åƒã€‚ç„¶è€Œï¼Œè¿™ç±»æ¨¡å‹ç¼ºä¹å¯æ§æ€§çš„é—®é¢˜é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ç”Ÿæ´»å†…å®¹åˆ›å»ºä¸­çš„å®é™…åº”ç”¨ã€‚å› æ­¤ï¼Œäººä»¬å¼€å§‹å…³æ³¨åˆ©ç”¨å‚è€ƒå›¾åƒæ¥æ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒçš„åˆæˆï¼Œè¿™è¢«è§†ä¸ºæ ¹æ®æ–‡æœ¬æç¤ºæ“ä½œï¼ˆæˆ–ç¼–è¾‘ï¼‰å‚è€ƒå›¾åƒï¼Œå³æ–‡æœ¬é©±åŠ¨çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ã€‚æœ¬æ–‡è´¡çŒ®äº†ä¸€ç§æ–°é¢–ã€ç®€æ´ã€é«˜æ•ˆçš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†é¢„è®­ç»ƒçš„å¤§è§„æ¨¡æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ‰©æ•£æ¨¡å‹é€‚åº”åˆ°å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰èŒƒå¼ä¸­ï¼Œä»¥å³æ’å³ç”¨æ–¹å¼ï¼Œæ— éœ€ä»»ä½•æ¨¡å‹è®­ç»ƒã€æ¨¡å‹å¾®è°ƒæˆ–åœ¨çº¿ä¼˜åŒ–è¿‡ç¨‹ï¼Œå³å¯å®ç°é«˜è´¨é‡å’Œå¤šåŠŸèƒ½æ–‡æœ¬é©±åŠ¨çš„I2Iç¿»è¯‘ã€‚ä¸ºäº†ç”¨å‚è€ƒå›¾åƒå¼•å¯¼T2Iç”Ÿæˆï¼Œæˆ‘ä»¬æå‡ºåœ¨DCTè°±ç©ºé—´ä¸­åˆ†è§£å…·æœ‰ä¸åŒé¢‘ç‡å¸¦çš„æ‰©æ•£ç‰¹å¾çš„å¤šç§æŒ‡å¯¼å› ç´ ï¼Œå¹¶ç›¸åº”åœ°è®¾è®¡äº†ä¸€ç§æ–°å‹é¢‘ç‡å¸¦æ›¿æ¢å±‚ï¼Œä»¥å³æ’å³ç”¨æ–¹å¼åŠ¨æ€æ§åˆ¶å‚è€ƒå›¾åƒå¯¹T2Iç”Ÿæˆç»“æœçš„å½±å“ã€‚æˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡ç®€å•åœ°è°ƒæ•´æ›¿ä»£çš„é¢‘ç‡å¸¦çš„ç±»å‹å’Œå¸¦å®½ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿçµæ´»åœ°æ§åˆ¶å‚è€ƒå›¾åƒçš„æŒ‡å¯¼å› ç´ å’Œå¼ºåº¦ã€‚å¤§é‡çš„å®šæ€§å’Œå®šé‡å®éªŒéªŒè¯äº†æˆ‘ä»¬æ–¹æ³•åœ¨å›¾åƒåˆ°å›¾åƒç¿»è¯‘çš„è§†è§‰è´¨é‡ã€å¤šæ ·æ€§å’Œå¯æ§æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/XiangGao1102/FBSDiff%E3%80%82">https://github.com/XiangGao1102/FBSDiffã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.00998v5">PDF</a> Accepted conference paper of ACM MM 2024</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä»‹ç»äº†å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”ŸæˆAIå’Œå¤šæ¨¡æ€æŠ€æœ¯æ–¹é¢çš„é©å‘½æ€§é‡Œç¨‹ç¢‘ï¼Œä½†ç¼ºä¹å¯æ§æ€§é™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚å› æ­¤ï¼Œç ”ç©¶åˆ©ç”¨å‚è€ƒå›¾åƒæ¥æ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒçš„åˆæˆï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„ã€ç®€æ´é«˜æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œä»¥å³æ’å³ç”¨æ–¹å¼å®ç°é«˜è´¨é‡çš„æ–‡æœ¬é©±åŠ¨å›¾åƒåˆ°å›¾åƒç¿»è¯‘ï¼Œæ— éœ€æ¨¡å‹è®­ç»ƒæˆ–å¾®è°ƒã€‚é€šè¿‡åˆ†è§£ä¸åŒé¢‘ç‡å¸¦çš„æ‰©æ•£ç‰¹å¾ï¼Œæå‡ºäº†é¢‘ç‡å¸¦æ›¿ä»£å±‚ï¼Œå®ç°äº†å¯¹å‚è€ƒå›¾åƒçš„åŠ¨æ€æ§åˆ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡è°ƒæ•´æ›¿ä»£çš„é¢‘ç‡å¸¦ç±»å‹å’Œå¸¦å®½ï¼Œå¯ä»¥çµæ´»æ§åˆ¶æŒ‡å¯¼å› ç´ å’Œå‚è€ƒå›¾åƒçš„æŒ‡å¯¼å¼ºåº¦ã€‚å®éªŒéªŒè¯äº†åœ¨å›¾åƒåˆ°å›¾åƒç¿»è¯‘çš„è§†è§‰è´¨é‡ã€é€šç”¨æ€§å’Œå¯æ§æ€§æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹åœ¨ç”ŸæˆAIå’Œå¤šæ¨¡æ€æŠ€æœ¯é¢†åŸŸå…·æœ‰é©å‘½æ€§ã€‚</li>
<li>ç¼ºä¹å¯æ§æ€§æ˜¯è¿™äº›æ¨¡å‹å®é™…åº”ç”¨ä¸­çš„ä¸»è¦é™åˆ¶ã€‚</li>
<li>åˆ©ç”¨å‚è€ƒå›¾åƒæ§åˆ¶æ–‡æœ¬åˆ°å›¾åƒçš„åˆæˆå·²æˆä¸ºç ”ç©¶ç„¦ç‚¹ã€‚</li>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œé€šè¿‡é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹å®ç°å³æ’å³ç”¨çš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘ã€‚</li>
<li>æ–¹æ³•æ¶‰åŠåœ¨DCTè°±ç©ºé—´ä¸­åˆ†è§£ä¸åŒé¢‘ç‡å¸¦çš„æ‰©æ•£ç‰¹å¾ã€‚</li>
<li>é¢‘ç‡å¸¦æ›¿ä»£å±‚å¯å®ç°å‚è€ƒå›¾åƒçš„åŠ¨æ€æ§åˆ¶ã€‚</li>
<li>è¯¥æ–¹æ³•å…è®¸é€šè¿‡è°ƒæ•´æ›¿ä»£çš„é¢‘ç‡å¸¦ç±»å‹å’Œå¸¦å®½æ¥çµæ´»æ§åˆ¶æŒ‡å¯¼å› ç´ å’Œå‚è€ƒå›¾åƒçš„æŒ‡å¯¼å¼ºåº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.00998">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-09f1391d5e3070663ca55d9d91b7a8d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b21be3ad1ab8897486ba3100c632bfcd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4cde09de8597f7d3593a9cc2b5c5eb2f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1bbd9f8814760b1d453513e0729bdd38.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="X-ray2CTPA-Leveraging-Diffusion-Models-to-Enhance-Pulmonary-Embolism-Classification"><a href="#X-ray2CTPA-Leveraging-Diffusion-Models-to-Enhance-Pulmonary-Embolism-Classification" class="headerlink" title="X-ray2CTPA: Leveraging Diffusion Models to Enhance Pulmonary Embolism   Classification"></a>X-ray2CTPA: Leveraging Diffusion Models to Enhance Pulmonary Embolism   Classification</h2><p><strong>Authors:Noa Cahan, Eyal Klang, Galit Aviram, Yiftach Barash, Eli Konen, Raja Giryes, Hayit Greenspan</strong></p>
<p>Chest X-rays or chest radiography (CXR), commonly used for medical diagnostics, typically enables limited imaging compared to computed tomography (CT) scans, which offer more detailed and accurate three-dimensional data, particularly contrast-enhanced scans like CT Pulmonary Angiography (CTPA). However, CT scans entail higher costs, greater radiation exposure, and are less accessible than CXRs. In this work we explore cross-modal translation from a 2D low contrast-resolution X-ray input to a 3D high contrast and spatial-resolution CTPA scan. Driven by recent advances in generative AI, we introduce a novel diffusion-based approach to this task. We evaluate the models performance using both quantitative metrics and qualitative feedback from radiologists, ensuring diagnostic relevance of the generated images. Furthermore, we employ the synthesized 3D images in a classification framework and show improved AUC in a PE categorization task, using the initial CXR input. The proposed method is generalizable and capable of performing additional cross-modality translations in medical imaging. It may pave the way for more accessible and cost-effective advanced diagnostic tools. The code for this project is available: <a target="_blank" rel="noopener" href="https://github.com/NoaCahan/X-ray2CTPA">https://github.com/NoaCahan/X-ray2CTPA</a> . </p>
<blockquote>
<p>èƒ¸éƒ¨Xå°„çº¿æˆ–èƒ¸éƒ¨æ”¾å°„æ‘„å½±ï¼ˆCXRï¼‰æ˜¯åŒ»å­¦è¯Šæ–­ä¸­å¸¸ç”¨çš„æ–¹æ³•ï¼Œä¸è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰ç›¸æ¯”ï¼Œå…¶æˆåƒé€šå¸¸è¾ƒä¸ºæœ‰é™ã€‚CTæ‰«æå¯ä»¥æä¾›æ›´è¯¦ç»†å’Œå‡†ç¡®çš„ä¸‰ç»´æ•°æ®ï¼Œç‰¹åˆ«æ˜¯åƒCTè‚ºåŠ¨è„‰é€ å½±ï¼ˆCTPAï¼‰è¿™æ ·çš„å¢å¼ºæ‰«æã€‚ç„¶è€Œï¼ŒCTæ‰«æçš„æˆæœ¬è¾ƒé«˜ï¼Œè¾å°„æš´éœ²é‡è¾ƒå¤§ï¼Œä¸”ç›¸å¯¹äºCXRsæ¥è¯´ä¸å¤ªå®¹æ˜“è·å¾—ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä»ä½å¯¹æ¯”åº¦çš„2D Xå°„çº¿è¾“å…¥åˆ°é«˜å¯¹æ¯”åº¦å’Œç©ºé—´åˆ†è¾¨ç‡çš„3DCTPAæ‰«æçš„è·¨æ¨¡æ€ç¿»è¯‘ã€‚å¾—ç›Šäºç”Ÿæˆäººå·¥æ™ºèƒ½çš„æœ€æ–°è¿›å±•ï¼Œæˆ‘ä»¬é’ˆå¯¹æ­¤ä»»åŠ¡å¼•å…¥äº†ä¸€ç§æ–°å‹çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚æˆ‘ä»¬åˆ©ç”¨å®šé‡æŒ‡æ ‡å’Œæ¥è‡ªæ”¾å°„ç§‘åŒ»å¸ˆçš„å®šæ€§åé¦ˆæ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œä»¥ç¡®ä¿ç”Ÿæˆå›¾åƒçš„è¯Šæ–­ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨åˆ†ç±»æ¡†æ¶ä¸­ä½¿ç”¨äº†åˆæˆçš„3Då›¾åƒï¼Œå¹¶åœ¨ä½¿ç”¨åˆå§‹CXRè¾“å…¥çš„PEåˆ†ç±»ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæé«˜äº†AUCã€‚æ‰€æå‡ºçš„æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œèƒ½å¤Ÿæ‰§è¡Œå…¶ä»–åŒ»å­¦æˆåƒçš„è·¨æ¨¡æ€ç¿»è¯‘ã€‚å®ƒå¯èƒ½ä¸ºæ›´ä¾¿æ·å’Œæˆæœ¬æ•ˆç›Šæ›´é«˜çš„å…ˆè¿›è¯Šæ–­å·¥å…·é“ºå¹³é“è·¯ã€‚è¯¥é¡¹ç›®çš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/NoaCahan/X-ray2CTPA">https://github.com/NoaCahan/X-ray2CTPA</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.16109v4">PDF</a> preprint, project code: <a target="_blank" rel="noopener" href="https://github.com/NoaCahan/X-ray2CTPA">https://github.com/NoaCahan/X-ray2CTPA</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å°†äºŒç»´ä½å¯¹æ¯”åº¦åˆ†è¾¨ç‡çš„Xå…‰ç‰‡è½¬åŒ–ä¸ºä¸‰ç»´é«˜å¯¹æ¯”åº¦å’Œé«˜åˆ†è¾¨ç‡çš„CTè‚ºåŠ¨è„‰é€ å½±ï¼ˆCTPAï¼‰æ‰«æçš„è·¨æ¨¡æ€è½¬æ¢æŠ€æœ¯ã€‚åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æœ€æ–°è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ–¹æ³•æ¥å®Œæˆè¿™ä¸€ä»»åŠ¡ã€‚ç»è¿‡å®šé‡è¯„ä¼°å’Œæ”¾å°„ç§‘åŒ»ç”Ÿå®šæ€§åé¦ˆï¼Œç”Ÿæˆçš„å›¾åƒå…·æœ‰è¯Šæ–­æ„ä¹‰ã€‚æ­¤å¤–ï¼Œåˆæˆçš„ä¸‰ç»´å›¾åƒåœ¨åˆ†ç±»æ¡†æ¶ä¸­ç”¨äºPEåˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨åˆå§‹çš„CXRè¾“å…¥æé«˜äº†AUCå€¼ã€‚è¯¥æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºåŒ»å­¦å½±åƒçš„å…¶ä»–è·¨æ¨¡æ€è½¬æ¢ï¼Œå¯èƒ½ä¸ºæ›´å¯è®¿é—®å’Œæˆæœ¬æ•ˆç›Šé«˜çš„å…ˆè¿›è¯Šæ–­å·¥å…·é“ºå¹³é“è·¯ã€‚ç›¸å…³ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä»äºŒç»´ä½å¯¹æ¯”åº¦åˆ†è¾¨ç‡çš„Xå…‰ç‰‡åˆ°ä¸‰ç»´é«˜å¯¹æ¯”åº¦å’Œé«˜åˆ†è¾¨ç‡CTè‚ºåŠ¨è„‰é€ å½±ï¼ˆCTPAï¼‰æ‰«æçš„è·¨æ¨¡æ€è½¬æ¢æŠ€æœ¯ã€‚</li>
<li>åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„æœ€æ–°è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„æ–¹æ³•æ¥å®Œæˆè¿™ä¸€ä»»åŠ¡ï¼Œä¿è¯å›¾åƒè½¬åŒ–çš„å‡†ç¡®æ€§ã€‚</li>
<li>é€šè¿‡å®šé‡è¯„ä¼°å’Œæ”¾å°„ç§‘åŒ»ç”Ÿçš„å®šæ€§åé¦ˆéªŒè¯äº†ç”Ÿæˆçš„å›¾åƒå…·æœ‰è¯Šæ–­æ„ä¹‰ã€‚</li>
<li>åˆæˆçš„ä¸‰ç»´å›¾åƒåœ¨åˆ†ç±»æ¡†æ¶ä¸­ç”¨äºPEåˆ†ç±»ä»»åŠ¡ï¼Œæé«˜äº†AUCå€¼ï¼Œè¯æ˜äº†å…¶åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„æ½œåŠ›ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºåŒ»å­¦å½±åƒçš„å…¶ä»–è·¨æ¨¡æ€è½¬æ¢ä»»åŠ¡ã€‚</li>
<li>è¯¥æŠ€æœ¯å¯èƒ½ä¸ºæ›´å¯è®¿é—®å’Œæˆæœ¬æ•ˆç›Šé«˜çš„å…ˆè¿›è¯Šæ–­å·¥å…·é“ºå¹³é“è·¯ï¼Œæœ‰åŠ©äºé™ä½åŒ»ç–—æˆæœ¬å’Œæé«˜è¯Šæ–­æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2406.16109">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9027e75e4728efeed5ad85a3ced1c23c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e721c059ab2a7c89abe409b5dbca121.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9977c7f348965cc793d1cb3abbde5cd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e6cce5da0d9f2fd47747b24125081b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6a9d36f50faae489d65751564b094fc7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9517987366350cad3849074b0676bdeb.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-46c694119abd21be5efe2a3d91cd6519.jpg" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  Towards Video Thinking Test A Holistic Benchmark for Advanced Video   Reasoning and Understanding
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1107db2ae4fd74f81995026a3cd2516c.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  Rule2Text Natural Language Explanation of Logical Rules in Knowledge   Graphs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32306k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
