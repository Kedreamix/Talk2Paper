<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  Who is a Better Talker Subjective and Objective Quality Assessment for   AI-Generated Talking Heads">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-e9efe258cccd9a8d695461495ee77cce.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    16.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    67 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-01-æ›´æ–°"><a href="#2025-08-01-æ›´æ–°" class="headerlink" title="2025-08-01 æ›´æ–°"></a>2025-08-01 æ›´æ–°</h1><h2 id="Who-is-a-Better-Talker-Subjective-and-Objective-Quality-Assessment-for-AI-Generated-Talking-Heads"><a href="#Who-is-a-Better-Talker-Subjective-and-Objective-Quality-Assessment-for-AI-Generated-Talking-Heads" class="headerlink" title="Who is a Better Talker: Subjective and Objective Quality Assessment for   AI-Generated Talking Heads"></a>Who is a Better Talker: Subjective and Objective Quality Assessment for   AI-Generated Talking Heads</h2><p><strong>Authors:Yingjie Zhou, Jiezhang Cao, Zicheng Zhang, Farong Wen, Yanwei Jiang, Jun Jia, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai</strong></p>
<p>Speech-driven methods for portraits are figuratively known as â€œTalkersâ€ because of their capability to synthesize speaking mouth shapes and facial movements. Especially with the rapid development of the Text-to-Image (T2I) models, AI-Generated Talking Heads (AGTHs) have gradually become an emerging digital human media. However, challenges persist regarding the quality of these talkers and AGTHs they generate, and comprehensive studies addressing these issues remain limited. To address this gap, this paper presents the largest AGTH quality assessment dataset THQA-10K to date, which selects 12 prominent T2I models and 14 advanced talkers to generate AGTHs for 14 prompts. After excluding instances where AGTH generation is unsuccessful, the THQA-10K dataset contains 10,457 AGTHs. Then, volunteers are recruited to subjectively rate the AGTHs and give the corresponding distortion categories. In our analysis for subjective experimental results, we evaluate the performance of talkers in terms of generalizability and quality, and also expose the distortions of existing AGTHs. Finally, an objective quality assessment method based on the first frame, Y-T slice and tone-lip consistency is proposed. Experimental results show that this method can achieve state-of-the-art (SOTA) performance in AGTH quality assessment. The work is released at <a target="_blank" rel="noopener" href="https://github.com/zyj-2000/Talker">https://github.com/zyj-2000/Talker</a>. </p>
<blockquote>
<p>è¯­éŸ³é©±åŠ¨çš„è‚–åƒç”Ÿæˆæ–¹æ³•è¢«ç§°ä¸ºâ€œè¯´è¯è€…â€ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿåˆæˆè¯´è¯çš„å˜´å·´å½¢çŠ¶å’Œé¢éƒ¨åŠ¨ä½œã€‚å°¤å…¶æ˜¯éšç€æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰æ¨¡å‹çš„å¿«é€Ÿå‘å±•ï¼ŒAIç”Ÿæˆçš„è¯´è¯å¤´åƒï¼ˆAGTHï¼‰å·²é€æ¸æˆä¸ºæ–°å…´çš„æ•°å­—äººç±»åª’ä½“ã€‚ç„¶è€Œï¼Œå…³äºè¿™äº›è¯´è¯è€…å’Œä»–ä»¬ç”Ÿæˆçš„AGTHçš„è´¨é‡æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ï¼Œé’ˆå¯¹è¿™äº›é—®é¢˜çš„ç»¼åˆç ”ç©¶ä»ç„¶æœ‰é™ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬æ–‡æå‡ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„AGTHè´¨é‡è¯„ä¼°æ•°æ®é›†THQA-10Kã€‚è¯¥æ•°æ®é›†é€‰æ‹©äº†12ä¸ªçªå‡ºçš„T2Iæ¨¡å‹å’Œ14ä¸ªå…ˆè¿›çš„è¯´è¯è€…ï¼Œä¸º14ä¸ªæç¤ºç”ŸæˆAGTHã€‚åœ¨æ’é™¤AGTHç”Ÿæˆä¸æˆåŠŸçš„æƒ…å†µåï¼ŒTHQA-10Kæ•°æ®é›†åŒ…å«10ï¼Œ457ä¸ªAGTHã€‚ç„¶åï¼Œæ‹›å‹Ÿå¿—æ„¿è€…å¯¹AGTHè¿›è¡Œä¸»è§‚è¯„åˆ†å¹¶ç»™å‡ºç›¸åº”çš„å¤±çœŸç±»åˆ«ã€‚åœ¨æˆ‘ä»¬çš„ä¸»è§‚å®éªŒåˆ†æç»“æœä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†è¯´è¯è€…çš„é€šç”¨æ€§å’Œè´¨é‡æ–¹é¢çš„æ€§èƒ½ï¼Œå¹¶æš´éœ²äº†ç°æœ‰AGTHçš„å¤±çœŸæƒ…å†µã€‚æœ€åï¼Œæå‡ºäº†ä¸€ç§åŸºäºç¬¬ä¸€å¸§ã€Y-Tåˆ‡ç‰‡å’ŒéŸ³è‰²å”‡åŒæ­¥çš„å®¢è§‚è´¨é‡è¯„ä¼°æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨AGTHè´¨é‡è¯„ä¼°ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚è¯¥å·¥ä½œå‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/zyj-2000/Talker%E3%80%82">https://github.com/zyj-2000/Talkerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23343v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†AIç”Ÿæˆè¯´è¯å¤´éƒ¨ï¼ˆAGTHï¼‰çš„æŒ‘æˆ˜å’Œç°çŠ¶ã€‚ä¸ºè§£å†³å½“å‰ç ”ç©¶ç©ºç™½ï¼Œæœ¬æ–‡æ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„AGTHè´¨é‡è¯„ä¼°æ•°æ®é›†THQA-10Kã€‚æ•°æ®é›†æ¶µç›–äº†ä¸åŒæ¨¡å‹ä¸ç®—æ³•çš„å¤´éƒ¨åˆæˆæ ·æœ¬ï¼Œå¹¶ç”±å¿—æ„¿è€…ä¸»è§‚è¯„ä¼°å…¶è´¨é‡å¹¶è¿›è¡Œåˆ†ç±»ã€‚é€šè¿‡å¯¹æ•°æ®çš„åˆ†æï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºä¸»è§‚è¯„ä»·çš„æ–°å‹å®¢è§‚è´¨é‡è¯„ä¼°æ–¹æ³•ï¼Œå¹¶å–å¾—è¡Œä¸šé¢†å…ˆæ€§èƒ½ã€‚é¡¹ç›®è¯¦æƒ…å¯è®¿é—®GitHubé¡¹ç›®ç½‘ç«™äº†è§£ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AGTHå·²æˆä¸ºæ–°å…´çš„æ•°å­—åª’ä½“å½¢å¼ï¼Œä½†å­˜åœ¨è´¨é‡æŒ‘æˆ˜ã€‚</li>
<li>æ„å»ºäº†ä¸€ä¸ªå¤§å‹AGTHè´¨é‡è¯„ä¼°æ•°æ®é›†THQA-10Kã€‚</li>
<li>å¯¹å¿—æ„¿è€…è¿›è¡Œäº†å¤´éƒ¨åˆæˆæ ·æœ¬çš„ä¸»è§‚è¯„ä»·å¹¶è¿›è¡Œåˆ†ç±»åˆ†æã€‚</li>
<li>åˆ†æäº†å½“å‰å¤´éƒ¨ç”ŸæˆæŠ€æœ¯åœ¨æ™®éæ€§å’Œè´¨é‡ä¸Šçš„è¡¨ç°ä»¥åŠå­˜åœ¨çš„å¤±çœŸé—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºç¬¬ä¸€å¸§ã€Y-Tåˆ‡ç‰‡å’ŒéŸ³è°ƒå”‡åŒæ­¥çš„æ–°å‹å®¢è§‚è´¨é‡è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>æ­¤æ–¹æ³•å®ç°äº†ä¸šç•Œé¢†å…ˆçš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23343">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e9efe258cccd9a8d695461495ee77cce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2663c9db8a1d870717583907bf066401.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cedae0d7b2bbbd3fe06b992a65f347c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27815db724ace4c6e906fc871afe5627.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1ed79a54eea7a57e1432a2caab7196d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7718bac0c2f797cfe4d05b40ab1d780d.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="JWB-DH-V1-Benchmark-for-Joint-Whole-Body-Talking-Avatar-and-Speech-Generation-Version-1"><a href="#JWB-DH-V1-Benchmark-for-Joint-Whole-Body-Talking-Avatar-and-Speech-Generation-Version-1" class="headerlink" title="JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech   Generation Version 1"></a>JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech   Generation Version 1</h2><p><strong>Authors:Xinhan Di, Kristin Qi, Pengqian Yu</strong></p>
<p>Recent advances in diffusion-based video generation have enabled photo-realistic short clips, but current methods still struggle to achieve multi-modal consistency when jointly generating whole-body motion and natural speech. Current approaches lack comprehensive evaluation frameworks that assess both visual and audio quality, and there are insufficient benchmarks for region-specific performance analysis. To address these gaps, we introduce the Joint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1), comprising a large-scale multi-modal dataset with 10,000 unique identities across 2 million video samples, and an evaluation protocol for assessing joint audio-video generation of whole-body animatable avatars. Our evaluation of SOTA models reveals consistent performance disparities between face&#x2F;hand-centric and whole-body performance, which incidates essential areas for future research. The dataset and evaluation tools are publicly available at <a target="_blank" rel="noopener" href="https://github.com/deepreasonings/WholeBodyBenchmark">https://github.com/deepreasonings/WholeBodyBenchmark</a>. </p>
<blockquote>
<p>åŸºäºæ‰©æ•£çš„è§†é¢‘ç”ŸæˆæŠ€æœ¯æœ€æ–°è¿›å±•å·²èƒ½å¤Ÿå®ç°ç…§ç‰‡èˆ¬é€¼çœŸçš„çŸ­ç‰‡ï¼Œä½†å½“å‰çš„æ–¹æ³•åœ¨è”åˆç”Ÿæˆå…¨èº«è¿åŠ¨å’Œè‡ªç„¶è¯­éŸ³æ—¶ï¼Œä»éš¾ä»¥è¾¾åˆ°å¤šæ¨¡å¼ä¸€è‡´æ€§ã€‚å½“å‰çš„æ–¹æ³•ç¼ºä¹åŒæ—¶è¯„ä¼°è§†è§‰å’ŒéŸ³é¢‘è´¨é‡çš„ç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œä»¥åŠé’ˆå¯¹ç‰¹å®šåŒºåŸŸçš„æ€§èƒ½åˆ†æçš„åŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›ç©ºç™½ï¼Œæˆ‘ä»¬æ¨å‡ºäº†è”åˆå…¨èº«è°ˆè¯Avatarå’Œè¯­éŸ³ç”Ÿæˆç‰ˆæœ¬Iï¼ˆJWB-DH-V1ï¼‰ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªå¤§è§„æ¨¡å¤šæ¨¡å¼æ•°æ®é›†ï¼Œæ¶µç›–10,000ä¸ªå”¯ä¸€èº«ä»½å’Œ200ä¸‡ä¸ªè§†é¢‘æ ·æœ¬ï¼Œä»¥åŠä¸€ä¸ªç”¨äºè¯„ä¼°å…¨èº«å¯åŠ¨ç”»Avatarsè”åˆéŸ³è§†é¢‘ç”Ÿæˆçš„è¯„ä¼°åè®®ã€‚æˆ‘ä»¬å¯¹æœ€æ–°æ¨¡å‹çš„è¯„ä¼°è¡¨æ˜ï¼Œé¢éƒ¨&#x2F;æ‰‹éƒ¨ä¸ºä¸­å¿ƒå’Œå…¨èº«æ€§èƒ½ä¹‹é—´å­˜åœ¨æŒç»­çš„æ€§èƒ½å·®å¼‚ï¼Œè¿™æš—ç¤ºäº†æœªæ¥ç ”ç©¶çš„å…³é”®é¢†åŸŸã€‚æ•°æ®é›†å’Œè¯„ä¼°å·¥å…·å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/deepreasonings/WholeBodyBenchmark%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/deepreasonings/WholeBodyBenchmarkå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20987v2">PDF</a> WiCV @ ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹æ‰©æ•£è§†é¢‘ç”ŸæˆæŠ€æœ¯çš„æœ€æ–°è¿›å±•åŠå…¶åœ¨å¤šæ¨¡æ€ä¸€è‡´æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³å½“å‰æ–¹æ³•åœ¨å…¨èº«è¿åŠ¨ä¸è‡ªç„¶è¯­éŸ³è”åˆç”Ÿæˆæ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†Joint Whole-Body Talking Avatar and Speech Generation Version Iï¼ˆJWB-DH-V1ï¼‰ã€‚è¯¥æ¨¡å‹åŒ…å«å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ¶µç›–10,000ä¸ªç‹¬ç‰¹èº«ä»½ã€2ç™¾ä¸‡è§†é¢‘æ ·æœ¬ï¼Œå¹¶æä¾›äº†è¯„ä¼°å…¨èº«å¯åŠ¨ç”»è™šæ‹Ÿäººè”åˆéŸ³è§†é¢‘ç”Ÿæˆçš„è¯„ä¼°åè®®ã€‚å¯¹ç°æœ‰æ¨¡å‹çš„è¯„ä¼°æ˜¾ç¤ºï¼Œé¢éƒ¨&#x2F;æ‰‹éƒ¨ä¸­å¿ƒä¸å…¨èº«æ€§èƒ½ä¹‹é—´å­˜åœ¨æ€§èƒ½å·®å¼‚ï¼Œè¿™ä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜äº†æ–¹å‘ã€‚æ•°æ®é›†å’Œè¯„ä¼°å·¥å…·å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/deepreasonings/WholeBodyBenchmark%E5%85%AC%E5%BC%80%E5%8F%AF%E7%94%A8%E3%80%82">https://github.com/deepreasonings/WholeBodyBenchmarkå…¬å¼€å¯ç”¨ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£è§†é¢‘ç”ŸæˆæŠ€æœ¯è™½èƒ½ç”Ÿæˆé€¼çœŸçš„çŸ­ç‰‡ï¼Œä½†åœ¨å…¨èº«è¿åŠ¨ä¸è‡ªç„¶è¯­éŸ³çš„è”åˆç”Ÿæˆä¸Šä»é¢ä¸´å¤šæ¨¡æ€ä¸€è‡´æ€§æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç¼ºä¹åŒæ—¶è¯„ä¼°è§†è§‰å’ŒéŸ³é¢‘è´¨é‡çš„ç»¼åˆè¯„ä¼°æ¡†æ¶ä»¥åŠåŒºåŸŸç‰¹å®šæ€§èƒ½åˆ†æçš„åŸºå‡†ã€‚</li>
<li>å¼•å…¥çš„JWB-DH-V1æ¨¡å‹åŒ…å«å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§èº«ä»½å’Œè§†é¢‘æ ·æœ¬ï¼Œä¸ºè§£å†³ä¸Šè¿°é—®é¢˜æä¾›èµ„æºã€‚</li>
<li>è¯„ä¼°æ˜¾ç¤ºï¼Œç°æœ‰æ¨¡å‹åœ¨é¢éƒ¨&#x2F;æ‰‹éƒ¨ä¸å…¨èº«æ€§èƒ½ä¸Šå­˜åœ¨å·®è·ï¼ŒæŒ‡ç¤ºæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</li>
<li>JWB-DH-V1æä¾›äº†è¯„ä¼°åè®®ï¼Œç”¨äºè¯„ä¼°å…¨èº«å¯åŠ¨ç”»è™šæ‹Ÿäººçš„è”åˆéŸ³è§†é¢‘ç”Ÿæˆã€‚</li>
<li>æ•°æ®é›†å’Œè¯„ä¼°å·¥å…·å·²å…¬å¼€å¯ç”¨ï¼Œä¾¿äºç ”ç©¶äººå‘˜è¿›è¡Œç›¸å…³ç ”ç©¶å’Œå¼€å‘ã€‚</li>
<li>è¯¥å·¥ä½œå¡«è¡¥äº†ç°æœ‰æŠ€æœ¯åœ¨å®é™…åº”ç”¨ä¸­çš„ä¸€é¡¹ç©ºç™½ï¼Œä¸ºæœªæ¥å…¨èº«åŠ¨æ€è™šæ‹Ÿè§’è‰²çš„ç”Ÿæˆå’Œå‘å±•æ‰“ä¸‹åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20987">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a1738fdcf4dfcf467db3947c0e47550a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-696ad95e0d2fcb14f0881d251efb20f4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-11c7871f7403d58731e479868f6c99fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-53368c3d87762a87a882d497cff99cdd.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Mask-Free-Audio-driven-Talking-Face-Generation-for-Enhanced-Visual-Quality-and-Identity-Preservation"><a href="#Mask-Free-Audio-driven-Talking-Face-Generation-for-Enhanced-Visual-Quality-and-Identity-Preservation" class="headerlink" title="Mask-Free Audio-driven Talking Face Generation for Enhanced Visual   Quality and Identity Preservation"></a>Mask-Free Audio-driven Talking Face Generation for Enhanced Visual   Quality and Identity Preservation</h2><p><strong>Authors:Dogucan Yaman, Fevziye Irem Eyiokur, Leonard BÃ¤rmann, HazÄ±m Kemal Ekenel, Alexander Waibel</strong></p>
<p>Audio-Driven Talking Face Generation aims at generating realistic videos of talking faces, focusing on accurate audio-lip synchronization without deteriorating any identity-related visual details. Recent state-of-the-art methods are based on inpainting, meaning that the lower half of the input face is masked, and the model fills the masked region by generating lips aligned with the given audio. Hence, to preserve identity-related visual details from the lower half, these approaches additionally require an unmasked identity reference image randomly selected from the same video. However, this common masking strategy suffers from (1) information loss in the input faces, significantly affecting the networksâ€™ ability to preserve visual quality and identity details, (2) variation between identity reference and input image degrading reconstruction performance, and (3) the identity reference negatively impacting the model, causing unintended copying of elements unaligned with the audio. To address these issues, we propose a mask-free talking face generation approach while maintaining the 2D-based face editing task. Instead of masking the lower half, we transform the input images to have closed mouths, using a two-step landmark-based approach trained in an unpaired manner. Subsequently, we provide these edited but unmasked faces to a lip adaptation model alongside the audio to generate appropriate lip movements. Thus, our approach needs neither masked input images nor identity reference images. We conduct experiments on the benchmark LRS2 and HDTF datasets and perform various ablation studies to validate our contributions. </p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨è¯´è¯äººè„¸ç”Ÿæˆçš„ç›®æ ‡æ˜¯ä¸ºäº†ç”Ÿæˆé€¼çœŸçš„è¯´è¯äººè„¸è§†é¢‘ï¼Œé‡ç‚¹åœ¨äºå®ç°å‡†ç¡®çš„éŸ³é¢‘ä¸å˜´å”‡åŒæ­¥ï¼Œè€Œä¸æŸå®³ä»»ä½•ä¸èº«ä»½ç›¸å…³çš„è§†è§‰ç»†èŠ‚ã€‚æœ€è¿‘çš„æœ€å…ˆè¿›çš„æ–¹æ³•åŸºäºæ’å€¼æŠ€æœ¯ï¼Œæ„å‘³ç€è¾“å…¥è„¸çš„ä¸‹åŠéƒ¨åˆ†ä¼šè¢«æ©ç›–ï¼Œæ¨¡å‹é€šè¿‡åœ¨ç»™å®šçš„éŸ³é¢‘ä¸‹å¡«å……æ©ç›–åŒºåŸŸæ¥ç”Ÿæˆç›¸åº”çš„å˜´å”‡ã€‚å› æ­¤ï¼Œä¸ºäº†ä¿ç•™ä¸‹åŠéƒ¨ä¸èº«ä»½ç›¸å…³çš„è§†è§‰ç»†èŠ‚ï¼Œè¿™äº›æ–¹æ³•è¿˜éœ€è¦ä»åŒä¸€è§†é¢‘ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªæœªé®è”½çš„èº«ä»½å‚è€ƒå›¾åƒã€‚ç„¶è€Œï¼Œè¿™ç§å¸¸è§çš„é®è”½ç­–ç•¥å­˜åœ¨ä»¥ä¸‹ç¼ºç‚¹ï¼šï¼ˆ1ï¼‰è¾“å…¥äººè„¸ä¿¡æ¯ä¸¢å¤±ï¼Œä¸¥é‡å½±å“ç½‘ç»œä¿ç•™è§†è§‰è´¨é‡å’Œèº«ä»½ç»†èŠ‚çš„èƒ½åŠ›ï¼›ï¼ˆ2ï¼‰èº«ä»½å‚è€ƒä¸è¾“å…¥å›¾åƒä¹‹é—´çš„å·®å¼‚å¯¼è‡´é‡å»ºæ€§èƒ½ä¸‹é™ï¼›ï¼ˆ3ï¼‰èº«ä»½å‚è€ƒå¯¹æ¨¡å‹äº§ç”Ÿè´Ÿé¢å½±å“ï¼Œå¯¼è‡´å‡ºç°ä¸éŸ³é¢‘æœªå¯¹é½å…ƒç´ çš„æ„å¤–å¤åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— é®è”½çš„è¯´è¯äººè„¸ç”Ÿæˆæ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒåŸºäº2Dçš„é¢éƒ¨ç¼–è¾‘ä»»åŠ¡ã€‚æˆ‘ä»¬ä¸åœ¨ä¸‹åŠéƒ¨åˆ†è¿›è¡Œé®è”½ï¼Œè€Œæ˜¯ä½¿ç”¨åŸºäºæ ‡è®°çš„ä¸¤æ­¥æ–¹æ³•å°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºé—­å˜´çŠ¶æ€ï¼Œè¯¥æ–¹æ³•ä»¥ä¸æˆå¯¹çš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚éšåï¼Œæˆ‘ä»¬å°†è¿™äº›ç»è¿‡ç¼–è¾‘ä½†æœªé®è”½çš„é¢å­”ä¸éŸ³é¢‘ä¸€èµ·æä¾›ç»™å”‡éƒ¨é€‚åº”æ¨¡å‹ï¼Œä»¥ç”Ÿæˆé€‚å½“çš„å”‡éƒ¨è¿åŠ¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ—¢ä¸éœ€è¦é®è”½çš„è¾“å…¥å›¾åƒï¼Œä¹Ÿä¸éœ€è¦èº«ä»½å‚è€ƒå›¾åƒã€‚æˆ‘ä»¬åœ¨åŸºå‡†LRS2å’ŒHDTFæ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œå¹¶é€šè¿‡å„ç§æ¶ˆèç ”ç©¶éªŒè¯æˆ‘ä»¬çš„è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20953v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººè„¸ç”ŸæˆæŠ€æœ¯çš„æ–°è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€é®ç½©å’Œèº«ä»½å‚è€ƒå›¾åƒçš„æ–°çš„é¢éƒ¨ç”Ÿæˆæ–¹æ³•ã€‚è¯¥æŠ€æœ¯é‡‡ç”¨æ— é®ç½©è¾“å…¥å›¾åƒçš„æ–¹æ³•ï¼Œé€šè¿‡å˜æ¢è¾“å…¥å›¾åƒä½¿å˜´å·´é—­åˆï¼Œå†é…åˆéŸ³é¢‘è¿›è¡Œå”‡åŠ¨ä½œç”Ÿæˆï¼Œä»è€Œå®ç°äº†éŸ³é¢‘ä¸å”‡éƒ¨çš„åŒæ­¥ã€‚è¯¥æ–¹æ³•é¿å…äº†ä¼ ç»Ÿé®ç½©ç­–ç•¥å¸¦æ¥çš„ä¿¡æ¯ä¸¢å¤±å’Œèº«ä»½å‚è€ƒå›¾åƒå¸¦æ¥çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººè„¸ç”ŸæˆæŠ€æœ¯æ—¨åœ¨ç”Ÿæˆä¸ç°å®ç›¸ä¼¼çš„è§†é¢‘ï¼Œå‡†ç¡®åŒæ­¥éŸ³é¢‘ä¸å”‡éƒ¨åŠ¨ä½œï¼ŒåŒæ—¶ä¿ç•™èº«ä»½ç›¸å…³çš„è§†è§‰ç»†èŠ‚ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åŸºäºé®ç½©ç­–ç•¥ï¼Œå­˜åœ¨ä¿¡æ¯ä¸¢å¤±ã€èº«ä»½å‚è€ƒå›¾åƒä¸è¾“å…¥å›¾åƒä¹‹é—´çš„å·®å¼‚ä»¥åŠèº«ä»½å‚è€ƒå¯¹æ¨¡å‹äº§ç”Ÿçš„è´Ÿé¢å½±å“ç­‰é—®é¢˜ã€‚</li>
<li>æ–°æ–¹æ³•é‡‡ç”¨æ— é®ç½©ç­–ç•¥ï¼Œé€šè¿‡å˜æ¢è¾“å…¥å›¾åƒä½¿å˜´å·´é—­åˆï¼Œå¹¶ä½¿ç”¨å”‡é€‚åº”æ¨¡å‹é…åˆéŸ³é¢‘è¿›è¡Œé¢éƒ¨ç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•æ—¢ä¸éœ€è¦é®ç½©è¾“å…¥å›¾åƒï¼Œä¹Ÿä¸éœ€è¦èº«ä»½å‚è€ƒå›¾åƒã€‚</li>
<li>å®éªŒåœ¨LRS2å’ŒHDTFæ•°æ®é›†ä¸Šè¿›è¡Œï¼ŒéªŒè¯äº†æ–°æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>æ–°æ–¹æ³•é¿å…äº†ä¼ ç»Ÿé®ç½©ç­–ç•¥å¸¦æ¥çš„é—®é¢˜ï¼Œæé«˜äº†é¢éƒ¨ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œè´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20953">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d3e6c750754fc6cf18c1401cc4b53e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7c02748c59aa891bc849068b0034a5d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e975ab461d8570390a0c18211186ca93.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47f091417f564d5acfaefac3611dd599.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-230b99a33a34cbbb96c6175bd8fe8e53.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Learning-Phonetic-Context-Dependent-Viseme-for-Enhancing-Speech-Driven-3D-Facial-Animation"><a href="#Learning-Phonetic-Context-Dependent-Viseme-for-Enhancing-Speech-Driven-3D-Facial-Animation" class="headerlink" title="Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven   3D Facial Animation"></a>Learning Phonetic Context-Dependent Viseme for Enhancing Speech-Driven   3D Facial Animation</h2><p><strong>Authors:Hyung Kyu Kim, Hak Gu Kim</strong></p>
<p>Speech-driven 3D facial animation aims to generate realistic facial movements synchronized with audio. Traditional methods primarily minimize reconstruction loss by aligning each frame with ground-truth. However, this frame-wise approach often fails to capture the continuity of facial motion, leading to jittery and unnatural outputs due to coarticulation. To address this, we propose a novel phonetic context-aware loss, which explicitly models the influence of phonetic context on viseme transitions. By incorporating a viseme coarticulation weight, we assign adaptive importance to facial movements based on their dynamic changes over time, ensuring smoother and perceptually consistent animations. Extensive experiments demonstrate that replacing the conventional reconstruction loss with ours improves both quantitative metrics and visual quality. It highlights the importance of explicitly modeling phonetic context-dependent visemes in synthesizing natural speech-driven 3D facial animation. Project page: <a target="_blank" rel="noopener" href="https://cau-irislab.github.io/interspeech25/">https://cau-irislab.github.io/interspeech25/</a> </p>
<blockquote>
<p>è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»æ—¨åœ¨ç”Ÿæˆä¸éŸ³é¢‘åŒæ­¥çš„çœŸå®é¢éƒ¨è¿åŠ¨ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é€šè¿‡å°†æ¯ä¸€å¸§ä¸åœ°é¢çœŸå®æ•°æ®è¿›è¡Œæ¯”å¯¹æ¥æœ€å°åŒ–é‡å»ºæŸå¤±ã€‚ç„¶è€Œï¼Œè¿™ç§é€å¸§çš„æ–¹æ³•å¾€å¾€æ— æ³•æ•æ‰é¢éƒ¨è¿åŠ¨çš„è¿ç»­æ€§ï¼Œç”±äºååŒå‘éŸ³å¯¼è‡´è¾“å‡ºäº§ç”ŸæŠ–åŠ¨å’Œä¸è‡ªç„¶ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹è¯­éŸ³ä¸Šä¸‹æ–‡æ„ŸçŸ¥æŸå¤±ï¼Œè¯¥æŸå¤±èƒ½æ˜ç¡®å»ºæ¨¡è¯­éŸ³ä¸Šä¸‹æ–‡å¯¹é¢éƒ¨è¿åŠ¨çš„å½±å“ã€‚é€šè¿‡å¼•å…¥é¢éƒ¨ååŒå‘éŸ³æƒé‡ï¼Œæˆ‘ä»¬æ ¹æ®é¢éƒ¨è¿åŠ¨éšæ—¶é—´å˜åŒ–çš„åŠ¨æ€å˜åŒ–ä¸ºå…¶åˆ†é…è‡ªé€‚åº”é‡è¦æ€§ï¼Œç¡®ä¿æ›´å¹³æ»‘ã€æ„ŸçŸ¥ä¸€è‡´çš„åŠ¨ç”»ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œç”¨æˆ‘ä»¬çš„æ–¹æ³•æ›¿æ¢ä¼ ç»Ÿçš„é‡å»ºæŸå¤±å¯ä»¥æé«˜å®šé‡æŒ‡æ ‡å’Œè§†è§‰è´¨é‡ã€‚å®ƒå¼ºè°ƒäº†æ˜ç¡®å»ºæ¨¡è¯­éŸ³ä¸Šä¸‹æ–‡ç›¸å…³çš„é¢éƒ¨è¿åŠ¨åœ¨åˆæˆè‡ªç„¶è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»ä¸­çš„é‡è¦æ€§ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://cau-irislab.github.io/interspeech25/">https://cau-irislab.github.io/interspeech25/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20568v1">PDF</a> Accepted for Interspeech 2025 Project Page:   <a target="_blank" rel="noopener" href="https://cau-irislab.github.io/interspeech25/">https://cau-irislab.github.io/interspeech25/</a></p>
<p><strong>Summary</strong><br>é¢éƒ¨åŠ¨ç”»æŠ€æœ¯æ—¨åœ¨ç”Ÿæˆä¸éŸ³é¢‘åŒæ­¥çš„çœŸå®é¢éƒ¨è¿åŠ¨ã€‚ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é€šè¿‡ä¸åœ°é¢çœŸå®æ•°æ®å¸§å¯¹é½æ¥æœ€å°åŒ–é‡å»ºæŸå¤±ï¼Œä½†è¿™ç§æ–¹æ³•å¿½ç•¥äº†é¢éƒ¨è¿åŠ¨çš„è¿ç»­æ€§ï¼Œå¯¼è‡´è¾“å‡ºæ•ˆæœä¸è‡ªç„¶ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯­éŸ³è¯­å¢ƒæ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œè¯¥å‡½æ•°æ˜¾å¼åœ°æ¨¡æ‹Ÿè¯­éŸ³è¯­å¢ƒå¯¹è¯­éŸ³è¡¨æƒ…è¿‡æ¸¡çš„å½±å“ã€‚é€šè¿‡å¼•å…¥è¯­éŸ³è¡¨æƒ…ååŒå‘éŸ³æƒé‡ï¼Œæˆ‘ä»¬æ ¹æ®é¢éƒ¨è¿åŠ¨éšæ—¶é—´çš„å˜åŒ–åŠ¨æ€å˜åŒ–æ¥è°ƒæ•´å…¶é‡è¦æ€§ï¼Œä»è€Œç¡®ä¿åŠ¨ç”»æ›´ä¸ºæµç•…å’Œæ„ŸçŸ¥ä¸€è‡´ã€‚å®éªŒè¯æ˜ï¼Œä¸ä¼ ç»Ÿçš„é‡å»ºæŸå¤±å‡½æ•°ç›¸æ¯”ï¼Œè¯¥æ–°å‡½æ•°å¯ä»¥æé«˜é‡åŒ–æŒ‡æ ‡å’Œè§†è§‰æ•ˆæœï¼Œçªå‡ºäº†æ˜ç¡®æ¨¡æ‹Ÿè¯­éŸ³è¯­å¢ƒä¾èµ–çš„è¯­éŸ³è¡¨æƒ…åœ¨åˆæˆè‡ªç„¶é¢éƒ¨åŠ¨ç”»ä¸­çš„é‡è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³é©±åŠ¨çš„ä¸‰ç»´é¢éƒ¨åŠ¨ç”»æ—¨åœ¨ç”Ÿæˆä¸éŸ³é¢‘åŒæ­¥çš„çœŸå®é¢éƒ¨è¿åŠ¨ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦é€šè¿‡æœ€å°åŒ–é‡å»ºæŸå¤±æ¥åˆ›å»ºé¢éƒ¨åŠ¨ç”»ï¼Œä½†è¿™ç§æ–¹æ³•å¯èƒ½å¯¼è‡´è¾“å‡ºæ•ˆæœä¸è‡ªç„¶ï¼Œå› ä¸ºå¿½ç•¥äº†é¢éƒ¨è¿åŠ¨çš„è¿ç»­æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è¯­éŸ³è¯­å¢ƒæ„ŸçŸ¥æŸå¤±å‡½æ•°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥å‡½æ•°å¯ä»¥æ˜¾å¼åœ°æ¨¡æ‹Ÿè¯­éŸ³è¯­å¢ƒå¯¹è¯­éŸ³è¡¨æƒ…è¿‡æ¸¡çš„å½±å“ã€‚</li>
<li>é€šè¿‡å¼•å…¥è¯­éŸ³è¡¨æƒ…ååŒå‘éŸ³æƒé‡ï¼Œæ–°å‡½æ•°ç¡®ä¿äº†æ›´æµç•…å’Œæ„ŸçŸ¥ä¸€è‡´çš„åŠ¨ç”»ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæ–°çš„è¯­éŸ³è¯­å¢ƒæ„ŸçŸ¥æŸå¤±å‡½æ•°å¯ä»¥æé«˜é‡åŒ–æŒ‡æ ‡å’Œè§†è§‰æ•ˆæœã€‚</li>
<li>æ–°çš„æŸå¤±å‡½æ•°æé«˜äº†é‡å»ºé¢éƒ¨åŠ¨ç”»çš„è‡ªç„¶æ€§å’Œè¿ç»­æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20568">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3ce8d4c2f35c5ea3a21f8bc004dda592.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d834e69b32ee4cd770df812a61025d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ff5507140b2cd6e367d151d3a6b2f60.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MemoryTalker-Personalized-Speech-Driven-3D-Facial-Animation-via-Audio-Guided-Stylization"><a href="#MemoryTalker-Personalized-Speech-Driven-3D-Facial-Animation-via-Audio-Guided-Stylization" class="headerlink" title="MemoryTalker: Personalized Speech-Driven 3D Facial Animation via   Audio-Guided Stylization"></a>MemoryTalker: Personalized Speech-Driven 3D Facial Animation via   Audio-Guided Stylization</h2><p><strong>Authors:Hyung Kyu Kim, Sangmin Lee, Hak Gu Kim</strong></p>
<p>Speech-driven 3D facial animation aims to synthesize realistic facial motion sequences from given audio, matching the speakerâ€™s speaking style. However, previous works often require priors such as class labels of a speaker or additional 3D facial meshes at inference, which makes them fail to reflect the speaking style and limits their practical use. To address these issues, we propose MemoryTalker which enables realistic and accurate 3D facial motion synthesis by reflecting speaking style only with audio input to maximize usability in applications. Our framework consists of two training stages: 1-stage is storing and retrieving general motion (i.e., Memorizing), and 2-stage is to perform the personalized facial motion synthesis (i.e., Animating) with the motion memory stylized by the audio-driven speaking style feature. In this second stage, our model learns about which facial motion types should be emphasized for a particular piece of audio. As a result, our MemoryTalker can generate a reliable personalized facial animation without additional prior information. With quantitative and qualitative evaluations, as well as user study, we show the effectiveness of our model and its performance enhancement for personalized facial animation over state-of-the-art methods. </p>
<blockquote>
<p>è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»æ—¨åœ¨æ ¹æ®ç»™å®šçš„éŸ³é¢‘åˆæˆé€¼çœŸçš„é¢éƒ¨è¿åŠ¨åºåˆ—ï¼Œä»¥åŒ¹é…è¯´è¯è€…çš„è¯´è¯é£æ ¼ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„å·¥ä½œé€šå¸¸éœ€è¦å…ˆéªŒä¿¡æ¯ï¼Œå¦‚è¯´è¯è€…çš„ç±»åˆ«æ ‡ç­¾æˆ–æ¨ç†æ—¶çš„é¢å¤–3Dé¢éƒ¨ç½‘æ ¼ï¼Œè¿™ä½¿å¾—å®ƒä»¬æ— æ³•åæ˜ è¯´è¯é£æ ¼ï¼Œå¹¶é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MemoryTalkerï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡ä»…ä½¿ç”¨éŸ³é¢‘è¾“å…¥åæ˜ è¯´è¯é£æ ¼ï¼Œå®ç°é€¼çœŸä¸”å‡†ç¡®çš„3Dé¢éƒ¨è¿åŠ¨åˆæˆï¼Œä»¥æœ€å¤§åŒ–åœ¨åº”ç”¨ç¨‹åºä¸­çš„å®ç”¨æ€§ã€‚æˆ‘ä»¬çš„æ¡†æ¶ç”±ä¸¤ä¸ªè®­ç»ƒé˜¶æ®µç»„æˆï¼šç¬¬ä¸€é˜¶æ®µæ˜¯å­˜å‚¨å’Œæ£€ç´¢ä¸€èˆ¬è¿åŠ¨ï¼ˆå³è®°å¿†ï¼‰ï¼Œç¬¬äºŒé˜¶æ®µæ˜¯å€ŸåŠ©ç”±éŸ³é¢‘é©±åŠ¨çš„è¯´è¯é£æ ¼ç‰¹å¾è¿›è¡Œä¸ªæ€§åŒ–é¢éƒ¨è¿åŠ¨åˆæˆï¼ˆå³åŠ¨ç”»ï¼‰ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬çš„æ¨¡å‹å­¦ä¹ å¯¹äºç‰¹å®šéŸ³é¢‘åº”è¯¥å¼ºè°ƒå“ªç§é¢éƒ¨è¿åŠ¨ç±»å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„MemoryTalkerå¯ä»¥åœ¨æ²¡æœ‰é¢å¤–å…ˆéªŒä¿¡æ¯çš„æƒ…å†µä¸‹ç”Ÿæˆå¯é çš„ä¸ªæ€§åŒ–é¢éƒ¨åŠ¨ç”»ã€‚é€šè¿‡å®šé‡å’Œå®šæ€§è¯„ä¼°ä»¥åŠç”¨æˆ·ç ”ç©¶ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹çš„æœ‰æ•ˆæ€§åŠå…¶åœ¨ä¸ªæ€§åŒ–é¢éƒ¨åŠ¨ç”»æ–¹é¢çš„æ€§èƒ½æå‡ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20562v1">PDF</a> Accepted for ICCV 2025 Project Page:   <a target="_blank" rel="noopener" href="https://cau-irislab.github.io/ICCV25-MemoryTalker/">https://cau-irislab.github.io/ICCV25-MemoryTalker/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Speech-driven 3Dé¢éƒ¨åŠ¨ç”»æŠ€æœ¯çš„æ–°è¿›å±•ã€‚é’ˆå¯¹ç°æœ‰æŠ€æœ¯å­˜åœ¨çš„é—®é¢˜ï¼Œå¦‚éœ€è¦é¢å¤–çš„å…ˆéªŒä¿¡æ¯ï¼ˆå¦‚è¯´è¯äººçš„ç±»åˆ«æ ‡ç­¾æˆ–é¢å¤–çš„3Dé¢éƒ¨ç½‘æ ¼ï¼‰ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•MemoryTalkerã€‚è¯¥æ–¹æ³•ä»…é€šè¿‡éŸ³é¢‘è¾“å…¥å°±èƒ½åæ˜ è¯´è¯é£æ ¼ï¼Œä½¿3Dé¢éƒ¨åŠ¨ç”»çš„åˆæˆæ›´åŠ çœŸå®ã€å‡†ç¡®ï¼Œå¹¶å¯åœ¨å„ç§åº”ç”¨åœºåˆä¸­æœ€å¤§åŒ–å®ç”¨æ€§ã€‚æ–°æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªè®­ç»ƒé˜¶æ®µï¼šä¸€æ˜¯å­˜å‚¨å’Œæ£€ç´¢é€šç”¨åŠ¨ä½œï¼ˆå³è®°å¿†é˜¶æ®µï¼‰ï¼ŒäºŒæ˜¯åˆ©ç”¨éŸ³é¢‘é©±åŠ¨çš„è¯´è¯é£æ ¼ç‰¹å¾è¿›è¡Œä¸ªæ€§åŒ–é¢éƒ¨åŠ¨ç”»åˆæˆï¼ˆå³åŠ¨ç”»é˜¶æ®µï¼‰ã€‚è¯¥æ–¹æ³•èƒ½å­¦ä¹ é’ˆå¯¹ç‰¹å®šéŸ³é¢‘åº”å¼ºè°ƒå“ªäº›é¢éƒ¨åŠ¨ä½œç±»å‹ï¼Œä»è€Œå®ç°å¯é ä¸”ä¸ªæ€§åŒ–çš„é¢éƒ¨åŠ¨ç”»ç”Ÿæˆï¼Œæ— éœ€é¢å¤–çš„å…ˆéªŒä¿¡æ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Speech-driven 3Dé¢éƒ¨åŠ¨ç”»æŠ€æœ¯æ—¨åœ¨ä»ç»™å®šçš„éŸ³é¢‘ä¸­åˆæˆé€¼çœŸçš„é¢éƒ¨åŠ¨ä½œåºåˆ—ï¼ŒåŒ¹é…è¯´è¯äººçš„è¯´è¯é£æ ¼ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é€šå¸¸éœ€è¦é¢å¤–çš„å…ˆéªŒä¿¡æ¯ï¼Œå¦‚è¯´è¯äººçš„ç±»åˆ«æ ‡ç­¾æˆ–3Dé¢éƒ¨ç½‘æ ¼ï¼Œè¿™é™åˆ¶äº†å…¶å®é™…åº”ç”¨ã€‚</li>
<li>MemoryTalkeræ–¹æ³•é€šè¿‡ä»…ä½¿ç”¨éŸ³é¢‘è¾“å…¥æ¥åæ˜ è¯´è¯é£æ ¼ï¼Œä½¿3Dé¢éƒ¨åŠ¨ç”»çš„åˆæˆæ›´åŠ çœŸå®å’Œå‡†ç¡®ã€‚</li>
<li>MemoryTalkeråŒ…æ‹¬ä¸¤ä¸ªè®­ç»ƒé˜¶æ®µï¼šå­˜å‚¨å’Œæ£€ç´¢é€šç”¨åŠ¨ä½œï¼ˆè®°å¿†é˜¶æ®µï¼‰ä»¥åŠåˆ©ç”¨éŸ³é¢‘é©±åŠ¨çš„è¯´è¯é£æ ¼ç‰¹å¾è¿›è¡Œä¸ªæ€§åŒ–é¢éƒ¨åŠ¨ç”»åˆæˆï¼ˆåŠ¨ç”»é˜¶æ®µï¼‰ã€‚</li>
<li>æ¨¡å‹èƒ½å­¦ä¹ é’ˆå¯¹ç‰¹å®šéŸ³é¢‘åº”å¼ºè°ƒå“ªäº›é¢éƒ¨åŠ¨ä½œç±»å‹ï¼Œä»¥å®ç°ä¸ªæ€§åŒ–çš„é¢éƒ¨åŠ¨ç”»ç”Ÿæˆã€‚</li>
<li>MemoryTalkeræ–¹æ³•æ— éœ€é¢å¤–çš„å…ˆéªŒä¿¡æ¯ï¼Œå³å¯ç”Ÿæˆå¯é çš„ä¸ªæ€§åŒ–é¢éƒ¨åŠ¨ç”»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-69b78a490d5dfef58d3eff194da1b4a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3111365737c2c38b7927092587ac1150.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c2e302943403358e10f8d3735a89acc4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6753032257ed9be2357947392d82b5b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fed63bb3e440157300535d8d122b9b55.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Face2VoiceSync-Lightweight-Face-Voice-Consistency-for-Text-Driven-Talking-Face-Generation"><a href="#Face2VoiceSync-Lightweight-Face-Voice-Consistency-for-Text-Driven-Talking-Face-Generation" class="headerlink" title="Face2VoiceSync: Lightweight Face-Voice Consistency for Text-Driven   Talking Face Generation"></a>Face2VoiceSync: Lightweight Face-Voice Consistency for Text-Driven   Talking Face Generation</h2><p><strong>Authors:Fang Kang, Yin Cao, Haoyu Chen</strong></p>
<p>Recent studies in speech-driven talking face generation achieve promising results, but their reliance on fixed-driven speech limits further applications (e.g., face-voice mismatch). Thus, we extend the task to a more challenging setting: given a face image and text to speak, generating both talking face animation and its corresponding speeches. Accordingly, we propose a novel framework, Face2VoiceSync, with several novel contributions: 1) Voice-Face Alignment, ensuring generated voices match facial appearance; 2) Diversity &amp; Manipulation, enabling generated voice control over paralinguistic features space; 3) Efficient Training, using a lightweight VAE to bridge visual and audio large-pretrained models, with significantly fewer trainable parameters than existing methods; 4) New Evaluation Metric, fairly assessing the diversity and identity consistency. Experiments show Face2VoiceSync achieves both visual and audio state-of-the-art performances on a single 40GB GPU. </p>
<blockquote>
<p>è¿‘æœŸå…³äºè¯­éŸ³é©±åŠ¨è¯´è¯äººè„¸ç”Ÿæˆçš„ç ”ç©¶å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†å®ƒä»¬å¯¹å›ºå®šé©±åŠ¨è¯­éŸ³çš„ä¾èµ–é™åˆ¶äº†è¿›ä¸€æ­¥åº”ç”¨ï¼ˆä¾‹å¦‚ï¼Œäººè„¸ä¸è¯­éŸ³ä¸åŒ¹é…ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†ä»»åŠ¡æ‰©å±•åˆ°ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„åœºæ™¯ï¼šç»™å®šä¸€å¼ äººè„¸å›¾åƒå’Œè¦è¯´çš„æ–‡æœ¬ï¼Œç”Ÿæˆè¯´è¯çš„äººè„¸åŠ¨ç”»åŠå…¶å¯¹åº”çš„è¯­éŸ³ã€‚æ®æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶Face2VoiceSyncï¼ŒåŒ…å«å‡ ä¸ªæ–°é¢–çš„è´¡çŒ®ï¼š1ï¼‰è¯­éŸ³-äººè„¸å¯¹é½ï¼Œç¡®ä¿ç”Ÿæˆçš„è¯­éŸ³ä¸é¢éƒ¨å¤–è§‚ç›¸åŒ¹é…ï¼›2ï¼‰å¤šæ ·æ€§å’Œæ“æ§æ€§ï¼Œèƒ½å¤Ÿå¯¹è¯­è¨€ç‰¹å¾ç©ºé—´ä¸­çš„ç”Ÿæˆè¯­éŸ³è¿›è¡Œæ§åˆ¶ï¼›3ï¼‰é«˜æ•ˆè®­ç»ƒï¼Œä½¿ç”¨è½»é‡çº§çš„VAEæ¥è¿æ¥è§†è§‰å’ŒéŸ³é¢‘å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œå¯è®­ç»ƒå‚æ•°å¤§å¤§å‡å°‘ï¼›4ï¼‰æ–°çš„è¯„ä»·æŒ‡æ ‡ï¼Œå…¬æ­£åœ°è¯„ä¼°å¤šæ ·æ€§å’Œèº«ä»½ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒFace2VoiceSyncåœ¨å•ä¸ª40GB GPUä¸Šå®ç°äº†è§†è§‰å’ŒéŸ³é¢‘çš„ä¸šç•Œæœ€ä½³æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19225v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬æ‘˜è¦ï¼šè¿‘æœŸè¯­éŸ³é©±åŠ¨è¯´è¯äººè„¸ç”Ÿæˆç ”ç©¶å–å¾—æ˜¾è‘—æˆæœï¼Œä½†å—é™äºå›ºå®šé©±åŠ¨è¯­éŸ³ï¼Œåº”ç”¨åœºæ™¯å—é™ï¼ˆå¦‚äººè„¸ä¸è¯­éŸ³ä¸åŒ¹é…ï¼‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ‰©å±•ä»»åŠ¡è‡³æ›´å…·æŒ‘æˆ˜æ€§çš„åœºæ™¯ï¼šç»™å®šäººè„¸å›¾åƒå’Œæ–‡æœ¬å†…å®¹ï¼Œç”Ÿæˆå¯¹åº”çš„è¯´è¯äººè„¸åŠ¨ç”»å’Œè¯­éŸ³ã€‚æˆ‘ä»¬æå‡ºä¸€ç§æ–°å‹æ¡†æ¶Face2VoiceSyncï¼ŒåŒ…å«å¤šé¡¹åˆ›æ–°è´¡çŒ®ï¼š1ï¼‰è¯­éŸ³äººè„¸å¯¹é½ï¼Œç¡®ä¿ç”Ÿæˆè¯­éŸ³ä¸é¢éƒ¨å¤–è§‚åŒ¹é…ï¼›2ï¼‰å¤šæ ·æ€§å’Œæ“æ§æ€§ï¼Œèƒ½å¤Ÿæ§åˆ¶ç”Ÿæˆè¯­éŸ³çš„å‰¯è¯­è¨€ç‰¹å¾ç©ºé—´ï¼›3ï¼‰é«˜æ•ˆè®­ç»ƒï¼Œä½¿ç”¨è½»é‡çº§VAEæ¥è¿æ¥è§†è§‰å’ŒéŸ³é¢‘å¤§å‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•å¤§å¤§å‡å°‘å¯è®­ç»ƒå‚æ•°ï¼›4ï¼‰æ–°çš„è¯„ä»·æŒ‡æ ‡ï¼Œå…¬å¹³è¯„ä¼°å¤šæ ·æ€§å’Œèº«ä»½ä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜Face2VoiceSyncåœ¨è§†è§‰å’ŒéŸ³é¢‘æ–¹é¢éƒ½è¾¾åˆ°äº†å•å¼ 40GB GPUä¸Šçš„æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸè¯­éŸ³é©±åŠ¨è¯´è¯äººè„¸ç”Ÿæˆç ”ç©¶è™½å–å¾—è¿›å±•ï¼Œä½†ä»å­˜åœ¨å›ºå®šé©±åŠ¨è¯­éŸ³å¸¦æ¥çš„åº”ç”¨åœºæ™¯é™åˆ¶é—®é¢˜ã€‚</li>
<li>æ‰©å±•ä»»åŠ¡è‡³æ›´æŒ‘æˆ˜çš„åœºæ™¯ï¼šæ ¹æ®äººè„¸å›¾åƒå’Œæ–‡æœ¬å†…å®¹ç”Ÿæˆå¯¹åº”çš„è¯´è¯äººè„¸åŠ¨ç”»å’Œè¯­éŸ³ã€‚</li>
<li>æå‡ºæ–°å‹æ¡†æ¶Face2VoiceSyncï¼ŒåŒ…å«å¤šé¡¹åˆ›æ–°è´¡çŒ®ï¼Œå¦‚è¯­éŸ³äººè„¸å¯¹é½ã€å¤šæ ·æ€§å’Œæ“æ§æ€§ã€é«˜æ•ˆè®­ç»ƒã€æ–°çš„è¯„ä»·æŒ‡æ ‡ç­‰ã€‚</li>
<li>Face2VoiceSyncèƒ½å¤Ÿå®ç°ç”Ÿæˆè¯­éŸ³ä¸é¢éƒ¨å¤–è§‚çš„åŒ¹é…ã€‚</li>
<li>ç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼ŒFace2VoiceSyncåœ¨è®­ç»ƒå‚æ•°ä¸Šå¤§å¤§å‡å°‘ã€‚</li>
<li>å®éªŒè¡¨æ˜Face2VoiceSyncåœ¨è§†è§‰å’ŒéŸ³é¢‘æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19225">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-6095e51609f559b2b3ac17ab77003257.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8fed428f2b29436e228b9e1b4c8842c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9ccdbfc06394027a3945ec6ce0514f90.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Livatar-1-Real-Time-Talking-Heads-Generation-with-Tailored-Flow-Matching"><a href="#Livatar-1-Real-Time-Talking-Heads-Generation-with-Tailored-Flow-Matching" class="headerlink" title="Livatar-1: Real-Time Talking Heads Generation with Tailored Flow   Matching"></a>Livatar-1: Real-Time Talking Heads Generation with Tailored Flow   Matching</h2><p><strong>Authors:Haiyang Liu, Xiaolin Hong, Xuancheng Yang, Yudi Ruan, Xiang Lian, Michael Lingelbach, Hongwei Yi, Wei Li</strong></p>
<p>We present Livatar, a real-time audio-driven talking heads videos generation framework. Existing baselines suffer from limited lip-sync accuracy and long-term pose drift. We address these limitations with a flow matching based framework. Coupled with system optimizations, Livatar achieves competitive lip-sync quality with a 8.50 LipSync Confidence on the HDTF dataset, and reaches a throughput of 141 FPS with an end-to-end latency of 0.17s on a single A10 GPU. This makes high-fidelity avatars accessible to broader applications. Our project is available at <a target="_blank" rel="noopener" href="https://www.hedra.com/">https://www.hedra.com/</a> with with examples at <a target="_blank" rel="noopener" href="https://h-liu1997.github.io/Livatar-1/">https://h-liu1997.github.io/Livatar-1/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Livatarï¼Œè¿™æ˜¯ä¸€ä¸ªå®æ—¶éŸ³é¢‘é©±åŠ¨è°ˆè¯è§†é¢‘ç”Ÿæˆæ¡†æ¶ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•åœ¨å”‡åŒæ­¥å‡†ç¡®æ€§å’Œé•¿æœŸå§¿åŠ¿æ¼‚ç§»æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚æˆ‘ä»¬é€šè¿‡åŸºäºæµåŒ¹é…çš„æ¡†æ¶æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚ç»“åˆç³»ç»Ÿä¼˜åŒ–ï¼ŒLivataråœ¨HDTFæ•°æ®é›†ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„å”‡åŒæ­¥è´¨é‡ï¼Œç½®ä¿¡åº¦ä¸º8.5åˆ†ï¼›å¹¶åœ¨å•ä¸ªA10 GPUä¸Šä»¥æ¯ç§’141å¸§çš„ååé‡è¾¾åˆ°äº†ç«¯åˆ°ç«¯çš„ä½å»¶è¿Ÿä»…ä¸º0.17ç§’ã€‚è¿™ä½¿å¾—é«˜ä¿çœŸå¤´åƒèƒ½æ›´å¹¿æ³›åœ°åº”ç”¨äºå„ä¸ªé¢†åŸŸã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯é€šè¿‡é“¾æ¥ <a target="_blank" rel="noopener" href="https://www.hedra.com/">https://www.hedra.com/</a> äº†è§£è¯¦æƒ…ï¼Œå…·ä½“æ¡ˆä¾‹å±•ç¤ºé“¾æ¥ä¸º <a target="_blank" rel="noopener" href="https://h-liu1997.github.io/Livatar-1/">https://h-liu1997.github.io/Livatar-1/</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18649v1">PDF</a> Technical Report</p>
<p><strong>Summary</strong>ï¼šæˆ‘ä»¬æ¨å‡ºäº†Livatarï¼Œä¸€ä¸ªå®æ—¶éŸ³é¢‘é©±åŠ¨çš„è¯´å”±äººå¤´è§†é¢‘ç”Ÿæˆæ¡†æ¶ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•å­˜åœ¨å”‡åŒæ­¥ç²¾åº¦æœ‰é™å’Œé•¿æœŸå§¿æ€æ¼‚ç§»çš„é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡åŸºäºæµåŒ¹é…çš„æ¡†æ¶è§£å†³äº†è¿™äº›å±€é™æ€§ã€‚ç»“åˆç³»ç»Ÿä¼˜åŒ–ï¼ŒLivataråœ¨HDTFæ•°æ®é›†ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„å”‡åŒæ­¥è´¨é‡ï¼Œè¾¾åˆ°äº†8.50çš„LipSyncç½®ä¿¡åº¦ï¼Œå¹¶åœ¨å•ä¸ªA10 GPUä¸Šå®ç°äº†æ¯ç§’141å¸§çš„ååé‡å’Œä»…0.17ç§’çš„ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚è¿™ä½¿å¾—é«˜ä¿çœŸåŒ–èº«æ›´é€‚ç”¨äºå¹¿æ³›çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„é¡¹ç›®å¯åœ¨<a target="_blank" rel="noopener" href="https://www.hedra.com/%E6%89%BE%E5%88%B0%EF%BC%8C%E7%A4%BA%E4%BE%8B%E5%8F%AF%E5%9C%A8https://h-liu1997.github.io/Livatar-1/%E6%9F%A5%E7%9C%8B%E3%80%82">https://www.hedra.com/æ‰¾åˆ°ï¼Œç¤ºä¾‹å¯åœ¨https://h-liu1997.github.io/Livatar-1/æŸ¥çœ‹ã€‚</a></p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>Livataræ˜¯ä¸€ä¸ªå®æ—¶éŸ³é¢‘é©±åŠ¨çš„è¯´å”±äººå¤´è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æŠ€æœ¯çš„å±€é™æ€§ã€‚</li>
<li>ç°æœ‰æŠ€æœ¯å­˜åœ¨å”‡åŒæ­¥ç²¾åº¦æœ‰é™å’Œé•¿æœŸå§¿æ€æ¼‚ç§»çš„é—®é¢˜ï¼Œè€ŒLivataré€šè¿‡åŸºäºæµåŒ¹é…çš„æ¡†æ¶è§£å†³äº†è¿™äº›é—®é¢˜ã€‚</li>
<li>Livataråœ¨HDTFæ•°æ®é›†ä¸Šå®ç°äº†8.50çš„LipSyncç½®ä¿¡åº¦ï¼Œæ˜¾ç¤ºå‡ºå…¶å‡ºè‰²çš„å”‡åŒæ­¥è´¨é‡ã€‚</li>
<li>è¯¥ç³»ç»Ÿå…·æœ‰é«˜æ•ˆçš„æ€§èƒ½ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªA10 GPUä¸Šå®ç°æ¯ç§’141å¸§çš„ååé‡å’Œä»…0.17ç§’çš„ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚</li>
<li>é«˜ä¿çœŸåŒ–èº«æ›´é€‚ç”¨äºå¹¿æ³›çš„åº”ç”¨ï¼ŒåŒ…æ‹¬å¨±ä¹ã€æ•™è‚²ã€åŸ¹è®­ç­‰é¢†åŸŸã€‚</li>
<li>é¡¹ç›®å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://www.hedra.com/%E6%89%BE%E5%88%B0%EF%BC%8C%E5%B9%B6%E4%B8%94%E6%8F%90%E4%BE%9B%E4%BA%86%E7%A4%BA%E4%BE%8B%E9%93%BE%E6%8E%A5https://h-liu1997.github.io/Livatar-1/%E4%BB%A5%E4%BE%9B%E6%9F%A5%E7%9C%8B%E5%92%8C%E5%AD%A6%E4%B9%A0%E3%80%82">https://www.hedra.com/æ‰¾åˆ°ï¼Œå¹¶ä¸”æä¾›äº†ç¤ºä¾‹é“¾æ¥https://h-liu1997.github.io/Livatar-1/ä»¥ä¾›æŸ¥çœ‹å’Œå­¦ä¹ ã€‚</a></li>
<li>è¯¥æŠ€æœ¯å¯èƒ½ä¼šæ¨åŠ¨è™šæ‹Ÿå½¢è±¡ç”Ÿæˆçš„å‘å±•ï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›æ›´åŠ é€¼çœŸçš„äº’åŠ¨ä½“éªŒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18649">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0734db058fb17bd595d97f827ffbaad7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-15df0a5c1f353b9a54bdabf6d762f676.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="DIFFA-Large-Language-Diffusion-Models-Can-Listen-and-Understand"><a href="#DIFFA-Large-Language-Diffusion-Models-Can-Listen-and-Understand" class="headerlink" title="DIFFA: Large Language Diffusion Models Can Listen and Understand"></a>DIFFA: Large Language Diffusion Models Can Listen and Understand</h2><p><strong>Authors:Jiaming Zhou, Hongjie Chen, Shiwan Zhao, Jian Kang, Jie Li, Enzhi Wang, Yujie Guo, Haoqin Sun, Hui Wang, Aobo Kong, Yong Qin, Xuelong Li</strong></p>
<p>Recent advances in Large language models (LLMs) have shown remarkable capabilities across textual and multimodal domains. In parallel, diffusion-based language models have emerged as a promising alternative to the autoregressive paradigm, offering improved controllability, bidirectional context modeling, and robust generation. However, their application to the audio modality remains underexplored. In this work, we introduce \textbf{DIFFA}, the first diffusion-based Large Audio-Language Model designed to perform spoken language understanding. DIFFA integrates a frozen diffusion language model with a lightweight dual-adapter architecture that bridges speech understanding and natural language reasoning. We employ a two-stage training pipeline: first, aligning semantic representations via an ASR objective; then, learning instruction-following abilities through synthetic audio-caption pairs automatically generated by prompting LLMs. Despite being trained on only 960 hours of ASR and 127 hours of synthetic instruction data, DIFFA demonstrates competitive performance on major benchmarks, including MMSU, MMAU, and VoiceBench, outperforming several autoregressive open-source baselines. Our results reveal the potential of diffusion-based language models for efficient and scalable audio understanding, opening a new direction for speech-driven AI. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/DIFFA.git">https://github.com/NKU-HLT/DIFFA.git</a>. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›å±•åœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€é¢†åŸŸå±•ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚åŒæ—¶ï¼ŒåŸºäºæ‰©æ•£çš„è¯­è¨€æ¨¡å‹ä½œä¸ºä¸€ç§æœ‰å‰æ™¯çš„è‡ªå›å½’èŒƒå¼æ›¿ä»£æ–¹æ¡ˆè€Œå‡ºç°ï¼Œæä¾›äº†æ›´å¥½çš„å¯æ§æ€§ã€åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡å’Œç¨³å¥çš„ç”Ÿæˆèƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨éŸ³é¢‘æ¨¡æ€çš„åº”ç”¨ä»ç„¶è¢«è¾ƒå°‘æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†\textbf{DIFFA}ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŸºäºæ‰©æ•£çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è¿›è¡Œå£è¯­ç†è§£ã€‚DIFFAå°†å†»ç»“çš„æ‰©æ•£è¯­è¨€æ¨¡å‹ä¸è½»é‡çº§çš„åŒé€‚é…å™¨æ¶æ„ç›¸ç»“åˆï¼Œè¿™åº§æ¡¥æ¢è¿æ¥äº†è¯­éŸ³ç†è§£å’Œè‡ªç„¶è¯­è¨€æ¨ç†ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸¤é˜¶æ®µè®­ç»ƒç®¡é“ï¼šé¦–å…ˆï¼Œé€šè¿‡ASRç›®æ ‡å¯¹é½è¯­ä¹‰è¡¨ç¤ºï¼›ç„¶åï¼Œé€šè¿‡LLMsè‡ªåŠ¨ç”Ÿæˆçš„åˆæˆéŸ³é¢‘å­—å¹•å¯¹æ¥å­¦ä¹ æŒ‡ä»¤æ‰§è¡Œèƒ½åŠ›ã€‚å°½ç®¡åªåœ¨960å°æ—¶çš„ASRå’Œ127å°æ—¶çš„åˆæˆæŒ‡ä»¤æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒDIFFAåœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºç«äº‰åŠ›ï¼ŒåŒ…æ‹¬MMSUã€MMAUå’ŒVoiceBenchï¼Œè¶…è¶Šäº†å¤šä¸ªè‡ªå›å½’å¼€æºåŸºå‡†ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†åŸºäºæ‰©æ•£çš„è¯­è¨€æ¨¡å‹åœ¨é«˜æ•ˆå’Œå¯æ‰©å±•éŸ³é¢‘ç†è§£æ–¹é¢çš„æ½œåŠ›ï¼Œä¸ºè¯­éŸ³é©±åŠ¨çš„äººå·¥æ™ºèƒ½å¼€å¯äº†æ–°çš„æ–¹å‘ã€‚æˆ‘ä»¬çš„ä»£ç å°†åœ¨<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/DIFFA.git%E4%B8%8A%E6%8F%90%E4%BE%9B%E3%80%82">https://github.com/NKU-HLT/DIFFA.gitä¸Šæä¾›ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18452v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨æ–‡æœ¬å’Œå¤šæ¨¡æ€é¢†åŸŸå±•ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚æœ¬ç ”ç©¶å¼•å…¥é¦–ä¸ªåŸºäºæ‰©æ•£çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹DIFFAï¼Œæ—¨åœ¨å®ç°è¯­éŸ³ç†è§£ã€‚DIFFAç»“åˆäº†å†»ç»“çš„æ‰©æ•£è¯­è¨€æ¨¡å‹å’Œè½»é‡çº§åŒé€‚é…å™¨æ¶æ„ï¼Œå®ç°è¯­éŸ³ç†è§£å’Œè‡ªç„¶è¯­è¨€æ¨ç†ä¹‹é—´çš„æ¡¥æ¢ã€‚é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼šé¦–å…ˆé€šè¿‡å¯¹é½è¯­ä¹‰è¡¨ç¤ºè¿›è¡ŒASRç›®æ ‡ï¼›ç„¶åé€šè¿‡å­¦ä¹ åˆæˆéŸ³é¢‘å­—å¹•å¯¹æ¥è·ŸéšæŒ‡ä»¤ï¼Œè¿™äº›åˆæˆéŸ³é¢‘å­—å¹•å¯¹æ˜¯é€šè¿‡æç¤ºLLMè‡ªåŠ¨ç”Ÿæˆçš„ã€‚å°½ç®¡åªåœ¨960å°æ—¶çš„ASRå’Œ127å°æ—¶çš„åˆæˆæŒ‡ä»¤æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒDIFFAåœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºç«äº‰åŠ›ï¼ŒåŒ…æ‹¬MMSUã€MMAUå’ŒVoiceBenchï¼Œä¼˜äºå¤šä¸ªè‡ªåŠ¨å›å½’å¼€æºåŸºå‡†ã€‚è¿™è¡¨æ˜æ‰©æ•£æ¨¡å‹åœ¨éŸ³é¢‘ç†è§£æ–¹é¢å…·æœ‰é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§æ½œåŠ›ï¼Œä¸ºè¯­éŸ³é©±åŠ¨çš„AIå¼€å¯äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DIFFAæ˜¯é¦–ä¸ªåŸºäºæ‰©æ•£çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°è¯­éŸ³ç†è§£ã€‚</li>
<li>DIFFAç»“åˆäº†å†»ç»“çš„æ‰©æ•£è¯­è¨€æ¨¡å‹å’Œè½»é‡çº§åŒé€‚é…å™¨æ¶æ„ã€‚</li>
<li>DIFFAé‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒæµç¨‹ï¼Œé¦–å…ˆè¿›è¡ŒASRç›®æ ‡å¯¹é½è¯­ä¹‰è¡¨ç¤ºï¼Œç„¶åå­¦ä¹ è·ŸéšæŒ‡ä»¤çš„èƒ½åŠ›ã€‚</li>
<li>DIFFAé€šè¿‡LLMç”Ÿæˆçš„åˆæˆéŸ³é¢‘å­—å¹•å¯¹è¿›è¡Œè®­ç»ƒã€‚</li>
<li>åœ¨ä¸»è¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDIFFAè¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä¼˜äºå¤šä¸ªè‡ªåŠ¨å›å½’å¼€æºåŸºå‡†ã€‚</li>
<li>DIFFAçš„æ½œåœ¨åº”ç”¨è¡¨æ˜æ‰©æ•£æ¨¡å‹åœ¨éŸ³é¢‘ç†è§£æ–¹é¢å…·æœ‰é«˜æ•ˆæ€§å’Œå¯æ‰©å±•æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18452">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8afb024cec6a28de96294b2f2b1709d9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cb01fae4bd66d4dcad433b5d6928a00.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e71bc595775eea249be477ee65c0d847.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-771e40bc61a1a3f2058282134a2e117a.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Tiny-is-not-small-enough-High-quality-low-resource-facial-animation-models-through-hybrid-knowledge-distillation"><a href="#Tiny-is-not-small-enough-High-quality-low-resource-facial-animation-models-through-hybrid-knowledge-distillation" class="headerlink" title="Tiny is not small enough: High-quality, low-resource facial animation   models through hybrid knowledge distillation"></a>Tiny is not small enough: High-quality, low-resource facial animation   models through hybrid knowledge distillation</h2><p><strong>Authors:Zhen Han, Mattias Teye, Derek Yadgaroff, Judith BÃ¼tepage</strong></p>
<p>The training of high-quality, robust machine learning models for speech-driven 3D facial animation requires a large, diverse dataset of high-quality audio-animation pairs. To overcome the lack of such a dataset, recent work has introduced large pre-trained speech encoders that are robust to variations in the input audio and, therefore, enable the facial animation model to generalize across speakers, audio quality, and languages. However, the resulting facial animation models are prohibitively large and lend themselves only to offline inference on a dedicated machine. In this work, we explore on-device, real-time facial animation models in the context of game development. We overcome the lack of large datasets by using hybrid knowledge distillation with pseudo-labeling. Given a large audio dataset, we employ a high-performing teacher model to train very small student models. In contrast to the pre-trained speech encoders, our student models only consist of convolutional and fully-connected layers, removing the need for attention context or recurrent updates. In our experiments, we demonstrate that we can reduce the memory footprint to up to 3.4 MB and required future audio context to up to 81 ms while maintaining high-quality animations. This paves the way for on-device inference, an important step towards realistic, model-driven digital characters. </p>
<blockquote>
<p>å¯¹äºé«˜è´¨é‡ã€ç¨³å¥çš„æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œè¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»è®­ç»ƒï¼Œéœ€è¦å¤§é‡çš„é«˜è´¨é‡éŸ³é¢‘åŠ¨ç”»å¯¹æ„æˆçš„å¤šæ ·åŒ–æ•°æ®é›†ã€‚ä¸ºäº†å…‹æœç¼ºä¹è¿™æ ·çš„æ•°æ®é›†ï¼Œè¿‘æœŸçš„å·¥ä½œå¼•å…¥äº†å¤§å‹é¢„è®­ç»ƒè¯­éŸ³ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å¯¹è¾“å…¥éŸ³é¢‘çš„å˜åŒ–å…·æœ‰é²æ£’æ€§ï¼Œå› æ­¤èƒ½å¤Ÿä½¿é¢éƒ¨åŠ¨ç”»æ¨¡å‹åœ¨å‘è¨€äººã€éŸ³é¢‘è´¨é‡å’Œè¯­è¨€æ–¹é¢å®ç°æ³›åŒ–ã€‚ç„¶è€Œï¼Œç”±æ­¤äº§ç”Ÿçš„é¢éƒ¨åŠ¨ç”»æ¨¡å‹è¿‡äºåºå¤§ï¼Œåªé€‚ç”¨äºä¸“ç”¨æœºå™¨ä¸Šçš„ç¦»çº¿æ¨ç†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åœ¨æ¸¸æˆå¼€å‘çš„èƒŒæ™¯ä¸‹æ¢ç´¢äº†è®¾å¤‡ç«¯çš„å®æ—¶é¢éƒ¨åŠ¨ç”»æ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡æ··åˆçŸ¥è¯†è’¸é¦å’Œä¼ªæ ‡ç­¾æŠ€æœ¯æ¥å…‹æœç¼ºä¹å¤§å‹æ•°æ®é›†çš„é—®é¢˜ã€‚ç»™å®šä¸€ä¸ªå¤§å‹éŸ³é¢‘æ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨é«˜æ€§èƒ½çš„æ•™å¸ˆæ¨¡å‹æ¥è®­ç»ƒéå¸¸å°çš„å­¦ç”Ÿæ¨¡å‹ã€‚ä¸é¢„è®­ç»ƒçš„è¯­éŸ³ç¼–ç å™¨ä¸åŒï¼Œæˆ‘ä»¬çš„å­¦ç”Ÿæ¨¡å‹åªåŒ…å«å·ç§¯å±‚å’Œå…¨è¿æ¥å±‚ï¼Œæ— éœ€æ³¨æ„ä¸Šä¸‹æ–‡æˆ–å¾ªç¯æ›´æ–°ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯æ˜å¯ä»¥å°†å†…å­˜å ç”¨å‡å°‘åˆ°æœ€å¤š3.4MBï¼Œå¹¶å°†æœªæ¥æ‰€éœ€çš„éŸ³é¢‘ä¸Šä¸‹æ–‡æ—¶é—´å‡å°‘åˆ°æœ€å¤š81æ¯«ç§’ï¼ŒåŒæ—¶ä¿æŒé«˜è´¨é‡çš„åŠ¨ç”»ã€‚è¿™ä¸ºè®¾å¤‡ç«¯æ¨ç†é“ºå¹³äº†é“è·¯ï¼Œæ˜¯æœç€ç°å®ã€æ¨¡å‹é©±åŠ¨çš„æ•°å­—è§’è‰²è¿ˆè¿›çš„é‡è¦ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18352v1">PDF</a> Accepted to ACM Transactions on Graphics 2025 (SIGGRAPH journal   track)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»çš„é«˜è´¨é‡æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒéœ€æ±‚ï¼ŒæŒ‡å‡ºéœ€è¦å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„é«˜è´¨é‡éŸ³é¢‘åŠ¨ç”»å¯¹æ•°æ®é›†ã€‚ä¸ºå…‹æœç¼ºä¹æ­¤ç±»æ•°æ®é›†çš„é—®é¢˜ï¼Œæœ¬ç ”ç©¶é‡‡ç”¨å¤§å‹é¢„è®­ç»ƒè¯­éŸ³ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å¯¹è¾“å…¥éŸ³é¢‘çš„å˜å¼‚å…·æœ‰é²æ£’æ€§ï¼Œä½¿é¢éƒ¨åŠ¨ç”»æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒè¯´è¯è€…ã€éŸ³é¢‘è´¨é‡å’Œè¯­è¨€ä¸Šè¿›è¡Œæ³›åŒ–ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä½“ç§¯åºå¤§ï¼Œä»…é€‚ç”¨äºä¸“ç”¨æœºå™¨ä¸Šçš„ç¦»çº¿æ¨ç†ã€‚æœ¬ç ”ç©¶åœ¨æ¸¸æˆå¼€å‘èƒŒæ™¯ä¸‹æ¢ç´¢äº†å®æ—¶é¢éƒ¨åŠ¨ç”»æ¨¡å‹ï¼Œé€šè¿‡ä½¿ç”¨æ··åˆçŸ¥è¯†è’¸é¦ä¸ä¼ªæ ‡ç­¾æŠ€æœ¯æ¥å…‹æœç¼ºä¹å¤§å‹æ•°æ®é›†çš„é—®é¢˜ã€‚åˆ©ç”¨å¤§å‹éŸ³é¢‘æ•°æ®é›†å’Œé«˜æ•ˆæ•™å¸ˆæ¨¡å‹è®­ç»ƒå°å‹å­¦ç”Ÿæ¨¡å‹ï¼Œæ— éœ€æ³¨æ„åŠ›ä¸Šä¸‹æ–‡æˆ–å¾ªç¯æ›´æ–°ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä¿æŒé«˜è´¨é‡åŠ¨ç”»çš„åŒæ—¶ï¼Œå¯å‡å°å†…å­˜å ç”¨è‡³3.4MBï¼Œå¹¶å°†æœªæ¥éŸ³é¢‘ä¸Šä¸‹æ–‡éœ€æ±‚å‡å°‘è‡³81msï¼Œè¿™ä¸ºè®¾å¤‡ç«¯æ¨ç†é“ºå¹³äº†é“è·¯ï¼Œæ˜¯æœç€é€¼çœŸæ¨¡å‹é©±åŠ¨çš„æ•°å­—è§’è‰²å‘å±•çš„é‡è¦ä¸€æ­¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡è¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»éœ€è¦å¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„é«˜è´¨é‡éŸ³é¢‘åŠ¨ç”»å¯¹æ•°æ®é›†ã€‚</li>
<li>ç¼ºä¹æ­¤ç±»æ•°æ®é›†æ˜¯é¢éƒ¨åŠ¨ç”»æ¨¡å‹å‘å±•çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>é¢„è®­ç»ƒè¯­éŸ³ç¼–ç å™¨å¯¹è¾“å…¥éŸ³é¢‘å˜å¼‚å…·æœ‰é²æ£’æ€§ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ³›åŒ–ã€‚</li>
<li>ç°æœ‰æ¨¡å‹ä½“ç§¯åºå¤§ï¼Œä»…é€‚ç”¨äºç¦»çº¿æ¨ç†ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡æ··åˆçŸ¥è¯†è’¸é¦ä¸ä¼ªæ ‡ç­¾æŠ€æœ¯å…‹æœç¼ºä¹å¤§å‹æ•°æ®é›†çš„é—®é¢˜ã€‚</li>
<li>åˆ©ç”¨é«˜æ•ˆæ•™å¸ˆæ¨¡å‹è®­ç»ƒå°å‹å­¦ç”Ÿæ¨¡å‹ï¼Œé™ä½æ¨¡å‹å¤æ‚åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18352">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0edaacd5cd7060fc95c57a9219a0d5fe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d856d2da682b855b7f3e64c2ed02e815.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1b74675c41125aaaa12402b81dfa0b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9e2f52cb3fce93e4512c11c38be0eeed.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Talking-Like-a-Phisher-LLM-Based-Attacks-on-Voice-Phishing-Classifiers"><a href="#Talking-Like-a-Phisher-LLM-Based-Attacks-on-Voice-Phishing-Classifiers" class="headerlink" title="Talking Like a Phisher: LLM-Based Attacks on Voice Phishing Classifiers"></a>Talking Like a Phisher: LLM-Based Attacks on Voice Phishing Classifiers</h2><p><strong>Authors:Wenhao Li, Selvakumar Manickam, Yung-wey Chong, Shankar Karuppayah</strong></p>
<p>Voice phishing (vishing) remains a persistent threat in cybersecurity, exploiting human trust through persuasive speech. While machine learning (ML)-based classifiers have shown promise in detecting malicious call transcripts, they remain vulnerable to adversarial manipulations that preserve semantic content. In this study, we explore a novel attack vector where large language models (LLMs) are leveraged to generate adversarial vishing transcripts that evade detection while maintaining deceptive intent. We construct a systematic attack pipeline that employs prompt engineering and semantic obfuscation to transform real-world vishing scripts using four commercial LLMs. The generated transcripts are evaluated against multiple ML classifiers trained on a real-world Korean vishing dataset (KorCCViD) with statistical testing. Our experiments reveal that LLM-generated transcripts are both practically and statistically effective against ML-based classifiers. In particular, transcripts crafted by GPT-4o significantly reduce classifier accuracy (by up to 30.96%) while maintaining high semantic similarity, as measured by BERTScore. Moreover, these attacks are both time-efficient and cost-effective, with average generation times under 9 seconds and negligible financial cost per query. The results underscore the pressing need for more resilient vishing detection frameworks and highlight the imperative for LLM providers to enforce stronger safeguards against prompt misuse in adversarial social engineering contexts. </p>
<blockquote>
<p>è¯­éŸ³é’“é±¼ï¼ˆvishishï¼‰ä»ç„¶æ˜¯ç½‘ç»œå®‰å…¨ä¸­çš„æŒç»­å¨èƒï¼Œå®ƒé€šè¿‡è¯´æœæ€§è¯è¯­æ¥åˆ©ç”¨äººç±»çš„ä¿¡ä»»ã€‚è™½ç„¶åŸºäºæœºå™¨å­¦ä¹ çš„åˆ†ç±»å™¨åœ¨æ£€æµ‹æ¶æ„é€šè¯è®°å½•æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°ä¿ç•™è¯­ä¹‰å†…å®¹çš„å¯¹æŠ—æ€§æ“çºµçš„å½±å“ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§æ–°çš„æ”»å‡»å‘é‡ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆå¯¹æŠ—æ€§çš„é’“é±¼è®°å½•ï¼Œä»¥èº²é¿æ£€æµ‹åŒæ—¶ä¿æŒæ¬ºéª—æ„å›¾ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç³»ç»Ÿçš„æ”»å‡»ç®¡é“ï¼Œé‡‡ç”¨æç¤ºå·¥ç¨‹å’Œè¯­ä¹‰æ¨¡ç³ŠåŒ–æ¥è½¬æ¢ç°å®ä¸–ç•Œä¸­çš„é’“é±¼è„šæœ¬ï¼Œä½¿ç”¨å››ç§å•†ä¸šLLMã€‚ç”Ÿæˆçš„è®°å½•é’ˆå¯¹åœ¨ç°å®ä¸–ç•Œä¸­çš„éŸ©å›½é’“é±¼æ•°æ®é›†ï¼ˆKorCCViDï¼‰ä¸Šè®­ç»ƒçš„å¤šä¸ªæœºå™¨å­¦ä¹ åˆ†ç±»å™¨è¿›è¡Œè¯„ä¼°ï¼Œå¹¶è¿›è¡Œç»Ÿè®¡æµ‹è¯•ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒLLMç”Ÿæˆçš„è®°å½•åœ¨é’ˆå¯¹åŸºäºæœºå™¨å­¦ä¹ çš„åˆ†ç±»å™¨æ—¶ï¼Œå…·æœ‰å®é™…å’Œç»Ÿè®¡ä¸Šçš„æœ‰æ•ˆæ€§ã€‚ç‰¹åˆ«æ˜¯é€šè¿‡GPT-4oåˆ¶ä½œçš„è®°å½•æ˜¾è‘—é™ä½äº†åˆ†ç±»å™¨çš„å‡†ç¡®æ€§ï¼ˆæœ€é«˜è¾¾30.96%ï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†è¾ƒé«˜çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œé€šè¿‡BERTScoreè¡¡é‡ã€‚æ­¤å¤–ï¼Œè¿™äº›æ”»å‡»æ—¢é«˜æ•ˆåˆç»æµï¼Œå¹³å‡ç”Ÿæˆæ—¶é—´ä¸åˆ°9ç§’ï¼Œæ¯æ¬¡æŸ¥è¯¢çš„è´¢åŠ¡æˆæœ¬å¾®ä¹å…¶å¾®ã€‚ç»“æœçªæ˜¾äº†å¯¹æ›´å…·å¼¹æ€§çš„é’“é±¼æ£€æµ‹æ¡†æ¶çš„è¿«åˆ‡éœ€æ±‚ï¼Œå¹¶å¼ºè°ƒå¤§å‹è¯­è¨€æ¨¡å‹æä¾›å•†éœ€è¦åœ¨å¯¹æŠ—æ€§ç¤¾ä¼šå·¥ç¨‹ç¯å¢ƒä¸­åŠ å¼ºé˜²æ­¢æç¤ºè¯¯ç”¨çš„ä¿éšœæªæ–½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16291v1">PDF</a> Accepted by EAI ICDF2C 2025</p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆå¯¹æŠ—æ€§è¯­éŸ³é’“é±¼ï¼ˆvishingï¼‰è„šæœ¬çš„æ–°å‹æ”»å‡»å‘é‡ï¼Œä»¥èº²é¿æ£€æµ‹å¹¶ä¿æŒæ¬ºéª—æ„å›¾ã€‚ç ”ç©¶é€šè¿‡æç¤ºå·¥ç¨‹å’Œè¯­ä¹‰æ¨¡ç³ŠåŒ–ç­‰æŠ€æœ¯æ‰‹æ®µï¼Œä½¿ç”¨å››ç§å•†ç”¨LLMså¯¹çœŸå®ä¸–ç•Œçš„vishingè„šæœ¬è¿›è¡Œè½¬æ¢ã€‚å®éªŒè¯æ˜ï¼ŒLLMç”Ÿæˆçš„è„šæœ¬åœ¨å®é™…å’Œç»Ÿè®¡ä¸Šéƒ½èƒ½æœ‰æ•ˆåœ°å¯¹æŠ—åŸºäºæœºå™¨å­¦ä¹ çš„åˆ†ç±»å™¨ã€‚ç‰¹åˆ«æ˜¯GPT-4oç”Ÿæˆçš„è„šæœ¬åœ¨ä¿æŒé«˜è¯­ä¹‰ç›¸ä¼¼æ€§çš„åŒæ—¶ï¼Œèƒ½æ˜¾è‘—é™ä½åˆ†ç±»å™¨çš„å‡†ç¡®ç‡ï¼ˆæœ€é«˜è¾¾30.96%ï¼‰ã€‚è¿™äº›æ”»å‡»æ—¢é«˜æ•ˆåˆç»æµï¼Œå¹³å‡ç”Ÿæˆæ—¶é—´ä¸åˆ°9ç§’ï¼Œæ¯æ¬¡æŸ¥è¯¢çš„è´¢åŠ¡æˆæœ¬å‡ ä¹ä¸ºé›¶ã€‚è¿™çªæ˜¾äº†å¯¹æ›´å…·å¼¹æ€§çš„vishingæ£€æµ‹æ¡†æ¶çš„è¿«åˆ‡éœ€æ±‚ï¼Œå¹¶å¼ºè°ƒLLMæä¾›å•†éœ€è¦åœ¨å¯¹æŠ—æ€§ç¤¾ä¼šå·¥ç¨‹ç¯å¢ƒä¸­åŠ å¼ºå¯¹æç¤ºè¯¯ç”¨çš„ä¿æŠ¤ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è¯­éŸ³é’“é±¼ï¼ˆvishingï¼‰ä»æ˜¯ç½‘ç»œå®‰å…¨ä¸­çš„æŒä¹…å¨èƒï¼Œåˆ©ç”¨äººç±»ä¿¡ä»»é€šè¿‡è¯´æœæ€§è¯è¯­è¿›è¡Œæ”»å‡»ã€‚</li>
<li>æœºå™¨å­¦ä¹ çš„åˆ†ç±»å™¨åœ¨æ£€æµ‹æ¶æ„è¯­éŸ³é’“é±¼è½¬å½•æ–¹é¢å·²æœ‰åº”ç”¨ï¼Œä½†ä»æ˜“å—å¯¹æŠ—æ€§æ“ä½œçš„å¹²æ‰°ã€‚</li>
<li>ç ”ç©¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆå¯¹æŠ—æ€§vishingè„šæœ¬ï¼Œé€šè¿‡æç¤ºå·¥ç¨‹å’Œè¯­ä¹‰æ¨¡ç³ŠåŒ–ç­‰æŠ€æœ¯èº²é¿æ£€æµ‹ã€‚</li>
<li>GPT-4oç”Ÿæˆçš„è„šæœ¬èƒ½æ˜¾è‘—é™ä½åˆ†ç±»å™¨å‡†ç¡®ç‡ï¼ŒåŒæ—¶ä¿æŒé«˜è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚</li>
<li>è¿™äº›æ”»å‡»æ—¢é«˜æ•ˆåˆç»æµï¼Œå¯¹ç°æœ‰çš„MLåˆ†ç±»å™¨æ„æˆå®é™…å¨èƒã€‚</li>
<li>éœ€è¦æ›´çµæ´»çš„vishingæ£€æµ‹æ¡†æ¶æ¥åº”å¯¹æ­¤ç±»å¨èƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16291">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2f57dbed57df9e0b8aa6725e576ca7b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f09c19b2adc441657ea8a74a42f5b51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a38616ce91d7d64f43cbd2f529bdf44b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5d905393b1e03476824c59b7e70bcdd5.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="STITCH-Simultaneous-Thinking-and-Talking-with-Chunked-Reasoning-for-Spoken-Language-Models"><a href="#STITCH-Simultaneous-Thinking-and-Talking-with-Chunked-Reasoning-for-Spoken-Language-Models" class="headerlink" title="STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for   Spoken Language Models"></a>STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for   Spoken Language Models</h2><p><strong>Authors:Cheng-Han Chiang, Xiaofei Wang, Linjie Li, Chung-Ching Lin, Kevin Lin, Shujie Liu, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, Lijuan Wang</strong></p>
<p>Spoken Language Models (SLMs) are designed to take speech inputs and produce spoken responses. However, current SLMs lack the ability to perform an internal, unspoken thinking process before responding. In contrast, humans typically engage in complex mental reasoning internally, enabling them to communicate ideas clearly and concisely. Thus, integrating an unspoken thought process into SLMs is highly desirable. While naively generating a complete chain-of-thought (CoT) reasoning before starting to talk can enable thinking for SLMs, this induces additional latency for the speech response, as the CoT reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a novel generation method that alternates between the generation of unspoken reasoning chunks and spoken response chunks. Since the audio duration of a chunk of spoken response is much longer than the time to generate the tokens in a chunk of spoken response, we use the remaining free time to generate the unspoken reasoning tokens. When a chunk of audio is played to the user, the model continues to generate the next unspoken reasoning chunk, achieving simultaneous thinking and talking. Remarkably, Stitch matches the latency of baselines that cannot generate unspoken CoT by design while outperforming those baselines by 15% on math reasoning datasets; Stitch also performs equally well on non-reasoning datasets as those baseline models. Some animations and demonstrations are on the project page: <a target="_blank" rel="noopener" href="https://d223302.github.io/STITCH">https://d223302.github.io/STITCH</a>. </p>
<blockquote>
<p>å£è¯­æ¨¡å‹ï¼ˆSLMsï¼‰è¢«è®¾è®¡ç”¨äºæ¥æ”¶è¯­éŸ³è¾“å…¥å¹¶äº§ç”Ÿå£è¯­å›åº”ã€‚ç„¶è€Œï¼Œå½“å‰çš„SLMç¼ºä¹åœ¨å›åº”ä¹‹å‰è¿›è¡Œå†…éƒ¨æ— å£°æ€ç»´è¿‡ç¨‹çš„èƒ½åŠ›ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäººç±»é€šå¸¸ä¼šåœ¨å†…éƒ¨è¿›è¡Œå¤æ‚çš„å¿ƒç†æ¨ç†ï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿæ¸…æ™°ç®€æ´åœ°äº¤æµæ€æƒ³ã€‚å› æ­¤ï¼Œå°†æ— å£°çš„æ€ç»´è¿‡ç¨‹é›†æˆåˆ°SLMä¸­æ˜¯é«˜åº¦ç†æƒ³çš„ã€‚è™½ç„¶å¤©çœŸåœ°åœ¨å¼€å§‹è¯´è¯ä¹‹å‰ç”Ÿæˆå®Œæ•´çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å¯ä»¥ä¸ºSLMæä¾›æ€è€ƒèƒ½åŠ›ï¼Œä½†è¿™ä¼šå¢åŠ è¯­éŸ³å›åº”çš„å»¶è¿Ÿï¼Œå› ä¸ºæ€ç»´é“¾æ¨ç†å¯èƒ½æ˜¯ä»»æ„é•¿çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Stitchï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹ç”Ÿæˆæ–¹æ³•ï¼Œå®ƒå¯ä»¥åœ¨æ— å£°æ¨ç†ç‰‡æ®µå’Œå£è¯­å›åº”ç‰‡æ®µä¹‹é—´äº¤æ›¿ç”Ÿæˆã€‚ç”±äºå£è¯­å›åº”ç‰‡æ®µçš„éŸ³é¢‘æŒç»­æ—¶é—´æ¯”ç”Ÿæˆå£è¯­å›åº”ç‰‡æ®µçš„æ ‡è®°çš„æ—¶é—´è¦é•¿å¾—å¤šï¼Œå› æ­¤æˆ‘ä»¬åˆ©ç”¨å‰©ä½™çš„è‡ªç”±æ—¶é—´æ¥ç”Ÿæˆæ— å£°æ¨ç†æ ‡è®°ã€‚å½“ä¸€æ®µéŸ³é¢‘æ’­æ”¾ç»™ç”¨æˆ·æ—¶ï¼Œæ¨¡å‹ä¼šç»§ç»­ç”Ÿæˆä¸‹ä¸€ä¸ªæ— å£°æ¨ç†ç‰‡æ®µï¼Œå®ç°æ€è€ƒå’Œè¯´è¯åŒæ—¶è¿›è¡Œã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒStitchçš„å»¶è¿Ÿä¸é‚£äº›ä¸èƒ½ç”Ÿæˆæ— å£°CoTçš„åŸºçº¿ç›¸åŒ¹é…ï¼ŒåŒæ—¶åœ¨æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šæ¯”è¿™äº›åŸºçº¿é«˜å‡º15%ï¼›åœ¨éæ¨ç†æ•°æ®é›†ä¸Šï¼ŒStitchä¹Ÿä¸è¿™äº›åŸºçº¿æ¨¡å‹è¡¨ç°ç›¸å½“ã€‚æœ‰å…³åŠ¨ç”»å’Œæ¼”ç¤ºå†…å®¹ï¼Œè¯·è®¿é—®é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://d223302.github.io/STITCH%E3%80%82">https://d223302.github.io/STITCHã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15375v1">PDF</a> Work in progress. Project page: <a target="_blank" rel="noopener" href="https://d223302.github.io/STITCH/">https://d223302.github.io/STITCH/</a></p>
<p><strong>Summary</strong>ï¼šSpoken Language Modelsç›®å‰æ— æ³•æ‰§è¡Œå†…éƒ¨æ— å£°çš„æ€è€ƒè¿‡ç¨‹ã€‚ä½œè€…æå‡ºStitchæ–¹æ³•ï¼Œè¯¥æ³•å¯åœ¨ç”Ÿæˆå£å¤´å“åº”ä¹‹å‰äº§ç”Ÿæ— å£°çš„æ€è€ƒç‰‡æ®µï¼Œåˆ©ç”¨ç­‰å¾…è¯­éŸ³è¾“å‡ºçš„æ—¶é—´ç”Ÿæˆæ€è€ƒè¿‡ç¨‹ä¸­çš„ç¬¦å·ï¼Œå®ç°äº†æ¨¡å‹åŒæ—¶æ€è€ƒå¹¶è¾“å‡ºã€‚åœ¨é€»è¾‘æ¨ç†æ•°æ®é›†ä¸Šï¼ŒStitchæ€§èƒ½è¾ƒåŸºçº¿æ¨¡å‹é«˜å‡º15%ï¼Œå¹¶åœ¨éæ¨ç†æ•°æ®é›†ä¸Šè¡¨ç°ç›¸è¿‘ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>Spoken Language Modelsï¼ˆSLMsï¼‰èƒ½å¤Ÿæ¥æ”¶è¯­éŸ³è¾“å…¥å¹¶äº§ç”Ÿè¯­éŸ³å“åº”ï¼Œä½†ç¼ºä¹å†…éƒ¨æ— å£°æ€è€ƒè¿‡ç¨‹ã€‚</li>
<li>äººç±»å†…éƒ¨å¤æ‚çš„æ€ç»´æ¨ç†ä½¿æ²Ÿé€šæ›´æ¸…æ™°ç®€æ´ï¼Œå°†æ— å£°æ€è€ƒèå…¥SLMså…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0321781cc9567008b0b610cbc332030.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fde55a96766d06f6ad0173e13df37409.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ATL-Diff-Audio-Driven-Talking-Head-Generation-with-Early-Landmarks-Guide-Noise-Diffusion"><a href="#ATL-Diff-Audio-Driven-Talking-Head-Generation-with-Early-Landmarks-Guide-Noise-Diffusion" class="headerlink" title="ATL-Diff: Audio-Driven Talking Head Generation with Early   Landmarks-Guide Noise Diffusion"></a>ATL-Diff: Audio-Driven Talking Head Generation with Early   Landmarks-Guide Noise Diffusion</h2><p><strong>Authors:Hoang-Son Vo, Quang-Vinh Nguyen, Seungwon Kim, Hyung-Jeong Yang, Soonja Yeom, Soo-Hyung Kim</strong></p>
<p>Audio-driven talking head generation requires precise synchronization between facial animations and audio signals. This paper introduces ATL-Diff, a novel approach addressing synchronization limitations while reducing noise and computational costs. Our framework features three key components: a Landmark Generation Module converting audio to facial landmarks, a Landmarks-Guide Noise approach that decouples audio by distributing noise according to landmarks, and a 3D Identity Diffusion network preserving identity characteristics. Experiments on MEAD and CREMA-D datasets demonstrate that ATL-Diff outperforms state-of-the-art methods across all metrics. Our approach achieves near real-time processing with high-quality animations, computational efficiency, and exceptional preservation of facial nuances. This advancement offers promising applications for virtual assistants, education, medical communication, and digital platforms. The source code is available at: \href{<a target="_blank" rel="noopener" href="https://github.com/sonvth/ATL-Diff%7D%7Bhttps://github.com/sonvth/ATL-Diff%7D">https://github.com/sonvth/ATL-Diff}{https://github.com/sonvth/ATL-Diff}</a> </p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨çš„è°ˆè¯å¤´éƒ¨ç”Ÿæˆéœ€è¦é¢éƒ¨åŠ¨ç”»å’ŒéŸ³é¢‘ä¿¡å·ä¹‹é—´çš„ç²¾ç¡®åŒæ­¥ã€‚æœ¬æ–‡ä»‹ç»äº†ATL-Diffï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹æ–¹æ³•ï¼Œè§£å†³äº†åŒæ­¥é™åˆ¶é—®é¢˜ï¼ŒåŒæ—¶é™ä½äº†å™ªå£°å’Œè®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå°†éŸ³é¢‘è½¬æ¢ä¸ºé¢éƒ¨åœ°æ ‡çš„Landmark Generation Moduleã€æ ¹æ®åœ°æ ‡åˆ†å¸ƒå™ªå£°çš„Landmarks-Guide Noiseæ–¹æ³•ï¼Œä»¥åŠèƒ½å¤Ÿä¿ç•™èº«ä»½ç‰¹å¾çš„3Dèº«ä»½æ‰©æ•£ç½‘ç»œã€‚åœ¨MEADå’ŒCREMA-Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒATL-Diffåœ¨å„é¡¹æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†è¿‘å®æ—¶çš„å¤„ç†è¿‡ç¨‹ï¼Œå…·æœ‰é«˜è´¨é‡åŠ¨ç”»ã€è®¡ç®—æ•ˆç‡é«˜ã€é¢éƒ¨ç»†èŠ‚ä¿ç•™å‡ºè‰²ç­‰ç‰¹ç‚¹ã€‚è¿™ä¸€è¿›å±•ä¸ºè™šæ‹ŸåŠ©æ‰‹ã€æ•™è‚²ã€åŒ»ç–—é€šä¿¡å’Œæ•°å­—å¹³å°ç­‰é¢†åŸŸæä¾›äº†æœ‰å‰æ™¯çš„åº”ç”¨ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sonvth/ATL-Diff">https://github.com/sonvth/ATL-Diff</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12804v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºATL-Diffçš„æ–°å‹éŸ³é¢‘é©±åŠ¨è¯´è¯äººå¤´éƒ¨ç”Ÿæˆæ–¹æ³•ï¼Œè¯¥æ–¹æ³•è§£å†³äº†é¢éƒ¨åŠ¨ç”»ä¸éŸ³é¢‘ä¿¡å·ä¹‹é—´çš„åŒæ­¥é—®é¢˜ï¼Œå¹¶é™ä½äº†å™ªå£°å’Œè®¡ç®—æˆæœ¬ã€‚å…¶æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šå°†éŸ³é¢‘è½¬æ¢ä¸ºé¢éƒ¨åœ°æ ‡çš„Landmark Generation Moduleã€æ ¹æ®åœ°æ ‡åˆ†å¸ƒå™ªå£°çš„Landmarks-Guide Noiseæ–¹æ³•å’Œä¿ç•™èº«ä»½ç‰¹å¾çš„3D Identity Diffusionç½‘ç»œã€‚åœ¨MEADå’ŒCREMA-Dæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒATL-Diffåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚è¯¥æ–¹æ³•å¯å®ç°è¿‘å®æ—¶å¤„ç†ï¼Œå…·æœ‰é«˜è´¨é‡åŠ¨ç”»ã€è®¡ç®—æ•ˆç‡é«˜å’Œé¢éƒ¨ç»†èŠ‚ä¿ç•™å‡ºè‰²ç­‰ä¼˜ç‚¹ã€‚æ­¤æŠ€æœ¯å¯¹äºè™šæ‹ŸåŠ©æ‰‹ã€æ•™è‚²ã€åŒ»ç–—é€šä¿¡å’Œæ•°å­—å¹³å°ç­‰é¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ATL-Diffæ˜¯ä¸€ç§éŸ³é¢‘é©±åŠ¨çš„è¯´è¯å¤´éƒ¨ç”Ÿæˆæ–¹æ³•ï¼Œè§£å†³äº†é¢éƒ¨åŠ¨ç”»ä¸éŸ³é¢‘ä¿¡å·çš„åŒæ­¥é—®é¢˜ã€‚</li>
<li>è¯¥æ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªå…³é”®ç»„ä»¶ï¼šLandmark Generation Moduleï¼ŒLandmarks-Guide Noiseå’Œ3D Identity Diffusionç½‘ç»œã€‚</li>
<li>ATL-Diffåœ¨é™ä½å™ªå£°å’Œè®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œå®ç°äº†é«˜è´¨é‡åŠ¨ç”»å’Œå‡ºè‰²çš„é¢éƒ¨ç»†èŠ‚ä¿ç•™ã€‚</li>
<li>åœ¨MEADå’ŒCREMA-Dæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒATL-Diffåœ¨æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</li>
<li>ATL-Diffå¯å®ç°è¿‘å®æ—¶å¤„ç†ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ï¼Œç‰¹åˆ«æ˜¯åœ¨è™šæ‹ŸåŠ©æ‰‹ã€æ•™è‚²ã€åŒ»ç–—é€šä¿¡å’Œæ•°å­—å¹³å°ç­‰é¢†åŸŸã€‚</li>
<li>è¯¥æ–¹æ³•çš„æºä»£ç å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12804">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0f965e7848b0f0d120c3c2f979c4e972.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b5b433bcef9b3db1f6f14b7eef66f728.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a97220bfcbbdee2e042cf0ea6c222a7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-554b61adf4d69f65ab42b6b8d5d9e194.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc4e7518f1f9859e9cd1496a2f64a6d4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2a505312a5c65b85f2957cb8279225eb.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Think-Before-Draw-Decomposing-Emotion-Semantics-Fine-Grained-Controllable-Expressive-Talking-Head-Generation"><a href="#Think-Before-Draw-Decomposing-Emotion-Semantics-Fine-Grained-Controllable-Expressive-Talking-Head-Generation" class="headerlink" title="Think-Before-Draw: Decomposing Emotion Semantics &amp; Fine-Grained   Controllable Expressive Talking Head Generation"></a>Think-Before-Draw: Decomposing Emotion Semantics &amp; Fine-Grained   Controllable Expressive Talking Head Generation</h2><p><strong>Authors:Hanlei Shi, Leyuan Qu, Yu Liu, Di Gao, Yuhua Zheng, Taihao Li</strong></p>
<p>Emotional talking-head generation has emerged as a pivotal research area at the intersection of computer vision and multimodal artificial intelligence, with its core value lying in enhancing human-computer interaction through immersive and empathetic engagement.With the advancement of multimodal large language models, the driving signals for emotional talking-head generation has shifted from audio and video to more flexible text. However, current text-driven methods rely on predefined discrete emotion label texts, oversimplifying the dynamic complexity of real facial muscle movements and thus failing to achieve natural emotional expressiveness.This study proposes the Think-Before-Draw framework to address two key challenges: (1) In-depth semantic parsing of emotionsâ€“by innovatively introducing Chain-of-Thought (CoT), abstract emotion labels are transformed into physiologically grounded facial muscle movement descriptions, enabling the mapping from high-level semantics to actionable motion features; and (2) Fine-grained expressiveness optimizationâ€“inspired by artistsâ€™ portrait painting process, a progressive guidance denoising strategy is proposed, employing a â€œglobal emotion localizationâ€“local muscle controlâ€ mechanism to refine micro-expression dynamics in generated videos.Our experiments demonstrate that our approach achieves state-of-the-art performance on widely-used benchmarks, including MEAD and HDTF. Additionally, we collected a set of portrait images to evaluate our modelâ€™s zero-shot generation capability. </p>
<blockquote>
<p>æƒ…æ„Ÿå¯¹è¯å¤´éƒ¨ç”Ÿæˆå·²æˆä¸ºè®¡ç®—æœºè§†è§‰å’Œå¤šæ¨¡æ€äººå·¥æ™ºèƒ½äº¤å‰é¢†åŸŸçš„é‡è¦ç ”ç©¶æ–¹å‘ï¼Œå…¶æ ¸å¿ƒä»·å€¼åœ¨äºé€šè¿‡æ²‰æµ¸å¼å’Œæœ‰åŒæƒ…å¿ƒçš„äº’åŠ¨å¢å¼ºäººæœºäº¤äº’ã€‚éšç€å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„è¿›æ­¥ï¼Œæƒ…æ„Ÿå¯¹è¯å¤´éƒ¨ç”Ÿæˆçš„é©±åŠ¨ä¿¡å·å·²ä»éŸ³é¢‘å’Œè§†é¢‘è½¬å‘æ›´çµæ´»çš„æ–‡å­—ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–‡æœ¬é©±åŠ¨æ–¹æ³•ä¾èµ–äºé¢„å…ˆå®šä¹‰çš„ç¦»æ•£æƒ…ç»ªæ ‡ç­¾æ–‡æœ¬ï¼Œè¿™è¿‡äºç®€åŒ–äº†çœŸå®é¢éƒ¨è‚Œè‚‰è¿åŠ¨çš„åŠ¨æ€å¤æ‚æ€§ï¼Œå› æ­¤æ— æ³•è¾¾åˆ°é¢„æœŸçš„è‡ªç„¶æƒ…æ„Ÿè¡¨è¾¾ã€‚æœ¬ç ”ç©¶æå‡ºäº†Think-Before-Drawæ¡†æ¶ï¼Œä»¥è§£å†³ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ï¼šï¼ˆ1ï¼‰æƒ…æ„Ÿçš„æ·±å…¥è¯­ä¹‰è§£æâ€“é€šè¿‡åˆ›æ–°åœ°å¼•å…¥æ€ç»´é“¾ï¼ˆCoTï¼‰ï¼Œå°†æŠ½è±¡çš„æƒ…ç»ªæ ‡ç­¾è½¬åŒ–ä¸ºç”Ÿç†åŸºç¡€çš„é¢éƒ¨è‚Œè‚‰è¿åŠ¨æè¿°ï¼Œå®ç°ä»é«˜çº§è¯­ä¹‰åˆ°å¯æ“ä½œè¿åŠ¨ç‰¹å¾çš„æ˜ å°„ï¼›ï¼ˆ2ï¼‰ç²¾ç»†è¡¨è¾¾ä¼˜åŒ–â€“å—è‰ºæœ¯å®¶è‚–åƒç”»è¿‡ç¨‹çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§æ¸è¿›å¼æŒ‡å¯¼å»å™ªç­–ç•¥ï¼Œé‡‡ç”¨â€œå…¨å±€æƒ…æ„Ÿå®šä½â€“å±€éƒ¨è‚Œè‚‰æ§åˆ¶â€æœºåˆ¶ï¼Œå¯¹ç”Ÿæˆè§†é¢‘ä¸­çš„å¾®è¡¨æƒ…åŠ¨æ€è¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ï¼ˆåŒ…æ‹¬MEADå’ŒHDTFï¼‰ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ”¶é›†äº†ä¸€å¥—è‚–åƒå›¾åƒæ¥è¯„ä¼°æˆ‘ä»¬æ¨¡å‹çš„é›¶æ ·æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12761v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æƒ…æ„Ÿå¯¹è¯å¤´ç”Ÿæˆçš„ç ”ç©¶é¢†åŸŸï¼ŒæŒ‡å‡ºå½“å‰æ–‡æœ¬é©±åŠ¨æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶ä»‹ç»äº†Think-Before-Drawæ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼•å…¥Chain-of-Thoughtè§£å†³äº†æ·±åº¦è¯­ä¹‰è§£ææƒ…æ„Ÿå’Œç²¾ç»†è¡¨è¾¾ä¼˜åŒ–ä¸¤ä¸ªå…³é”®æŒ‘æˆ˜ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¸¸ç”¨åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æƒ…æ„Ÿå¯¹è¯å¤´ç”Ÿæˆæ˜¯è®¡ç®—æœºè§†è§‰å’Œå¤šæ¨¡æ€äººå·¥æ™ºèƒ½äº¤å‰çš„é‡è¦ç ”ç©¶é¢†åŸŸã€‚</li>
<li>è¯¥æŠ€æœ¯æ—¨åœ¨é€šè¿‡æ²‰æµ¸å¼å’Œæœ‰åŒæƒ…å¿ƒçš„äº¤äº’å¢å¼ºäººä¸è®¡ç®—æœºçš„äº¤äº’ã€‚</li>
<li>å½“å‰æ–‡æœ¬é©±åŠ¨çš„æ–¹æ³•ä¾èµ–äºé¢„å®šä¹‰çš„ç¦»æ•£æƒ…æ„Ÿæ ‡ç­¾æ–‡æœ¬ï¼Œè¿™ç®€åŒ–äº†é¢éƒ¨è‚Œè‚‰è¿åŠ¨çš„åŠ¨æ€å¤æ‚æ€§ã€‚</li>
<li>Think-Before-Drawæ¡†æ¶é€šè¿‡å¼•å…¥Chain-of-Thoughtè§£å†³äº†æ·±åº¦è¯­ä¹‰è§£ææƒ…æ„Ÿçš„é—®é¢˜ã€‚</li>
<li>è¯¥æ¡†æ¶å°†æŠ½è±¡æƒ…æ„Ÿæ ‡ç­¾è½¬åŒ–ä¸ºç”Ÿç†åŸºç¡€çš„é¢éƒ¨è‚Œè‚‰è¿åŠ¨æè¿°ï¼Œå®ç°äº†ä»é«˜çº§è¯­ä¹‰åˆ°å¯æ“ä½œè¿åŠ¨ç‰¹å¾çš„æ˜ å°„ã€‚</li>
<li>æ¡†æ¶è¿˜å€Ÿé‰´äº†è‰ºæœ¯å®¶çš„è‚–åƒç”»è¿‡ç¨‹ï¼Œæå‡ºäº†ç²¾ç»†è¡¨è¾¾ä¼˜åŒ–çš„ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12761">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0d6012c8354421dd6f4b56d085f5b9ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-362530a7c9f403873d885ce416757c82.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="AU-Blendshape-for-Fine-grained-Stylized-3D-Facial-Expression-Manipulation"><a href="#AU-Blendshape-for-Fine-grained-Stylized-3D-Facial-Expression-Manipulation" class="headerlink" title="AU-Blendshape for Fine-grained Stylized 3D Facial Expression   Manipulation"></a>AU-Blendshape for Fine-grained Stylized 3D Facial Expression   Manipulation</h2><p><strong>Authors:Hao Li, Ju Dai, Feng Zhou, Kaida Ning, Lei Li, Junjun Pan</strong></p>
<p>While 3D facial animation has made impressive progress, challenges still exist in realizing fine-grained stylized 3D facial expression manipulation due to the lack of appropriate datasets. In this paper, we introduce the AUBlendSet, a 3D facial dataset based on AU-Blendshape representation for fine-grained facial expression manipulation across identities. AUBlendSet is a blendshape data collection based on 32 standard facial action units (AUs) across 500 identities, along with an additional set of facial postures annotated with detailed AUs. Based on AUBlendSet, we propose AUBlendNet to learn AU-Blendshape basis vectors for different character styles. AUBlendNet predicts, in parallel, the AU-Blendshape basis vectors of the corresponding style for a given identity mesh, thereby achieving stylized 3D emotional facial manipulation. We comprehensively validate the effectiveness of AUBlendSet and AUBlendNet through tasks such as stylized facial expression manipulation, speech-driven emotional facial animation, and emotion recognition data augmentation. Through a series of qualitative and quantitative experiments, we demonstrate the potential and importance of AUBlendSet and AUBlendNet in 3D facial animation tasks. To the best of our knowledge, AUBlendSet is the first dataset, and AUBlendNet is the first network for continuous 3D facial expression manipulation for any identity through facial AUs. Our source code is available at <a target="_blank" rel="noopener" href="https://github.com/wslh852/AUBlendNet.git">https://github.com/wslh852/AUBlendNet.git</a>. </p>
<blockquote>
<p>è™½ç„¶3Dé¢éƒ¨åŠ¨ç”»å·²ç»å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹é€‚å½“çš„æ•°æ®é›†ï¼Œåœ¨å®ç°ç²¾ç»†ç²’åº¦çš„é£æ ¼åŒ–3Dé¢éƒ¨è¡¨æƒ…æ“çºµæ—¶ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†AUBlendSetï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºAU-Blendshapeè¡¨ç¤ºçš„3Dé¢éƒ¨æ•°æ®é›†ï¼Œç”¨äºè·¨èº«ä»½è¿›è¡Œç²¾ç»†ç²’åº¦çš„é¢éƒ¨è¡¨æƒ…æ“çºµã€‚AUBlendSetæ˜¯ä¸€ä¸ªåŸºäº32ä¸ªæ ‡å‡†é¢éƒ¨åŠ¨ä½œå•å…ƒï¼ˆAUsï¼‰çš„blendshapeæ•°æ®é›†ï¼Œæ¶µç›–äº†500ä¸ªèº«ä»½ï¼Œä»¥åŠå¦ä¸€ç»„å¸¦æœ‰è¯¦ç»†AUæ³¨é‡Šçš„é¢éƒ¨å§¿æ€ã€‚åŸºäºAUBlendSetï¼Œæˆ‘ä»¬æå‡ºäº†AUBlendNetï¼Œç”¨äºå­¦ä¹ ä¸åŒå­—ç¬¦é£æ ¼çš„AU-BlendshapeåŸºç¡€å‘é‡ã€‚AUBlendNetå¹¶è¡Œé¢„æµ‹ç»™å®šèº«ä»½ç½‘æ ¼çš„ç›¸åº”é£æ ¼çš„AU-BlendshapeåŸºç¡€å‘é‡ï¼Œä»è€Œå®ç°é£æ ¼åŒ–çš„3Dæƒ…æ„Ÿé¢éƒ¨æ“çºµã€‚æˆ‘ä»¬é€šè¿‡é£æ ¼åŒ–çš„é¢éƒ¨è¡¨æƒ…æ“çºµã€è¯­éŸ³é©±åŠ¨çš„æƒ…æ„Ÿé¢éƒ¨åŠ¨ç”»å’Œæƒ…æ„Ÿè¯†åˆ«æ•°æ®å¢å¼ºç­‰ä»»åŠ¡ï¼Œå…¨é¢éªŒè¯äº†AUBlendSetå’ŒAUBlendNetçš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡ä¸€ç³»åˆ—å®šæ€§å’Œå®šé‡å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†AUBlendSetå’ŒAUBlendNetåœ¨3Dé¢éƒ¨åŠ¨ç”»ä»»åŠ¡ä¸­çš„æ½œåŠ›å’Œé‡è¦æ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒAUBlendSetæ˜¯ç¬¬ä¸€ä¸ªæ•°æ®é›†ï¼ŒAUBlendNetæ˜¯ç¬¬ä¸€ä¸ªç½‘ç»œï¼Œå¯é€šè¿‡é¢éƒ¨AUè¿›è¡Œä»»ä½•èº«ä»½çš„è¿ç»­3Dé¢éƒ¨è¡¨æƒ…æ“çºµã€‚æˆ‘ä»¬çš„æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wslh852/AUBlendNet.git%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/wslh852/AUBlendNet.gitä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12001v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥è®ºæ–‡å¼•å…¥äº†AUBlendSetæ•°æ®é›†å’ŒAUBlendNetç½‘ç»œï¼Œç”¨äºç²¾ç»†ç²’åº¦çš„ä¸‰ç»´é¢éƒ¨è¡¨æƒ…æ“ä½œã€‚AUBlendSetåŸºäºAU-Blendshapeè¡¨ç¤ºï¼ŒåŒ…å«500ä¸ªèº«ä»½çš„32ä¸ªæ ‡å‡†é¢éƒ¨åŠ¨ä½œå•å…ƒï¼ˆAUsï¼‰ä»¥åŠè¯¦ç»†çš„AUæ³¨é‡Šçš„é¢éƒ¨å§¿æ€é›†åˆã€‚AUBlendNetå­¦ä¹ ä¸åŒè§’è‰²é£æ ¼çš„AU-BlendshapeåŸºç¡€å‘é‡ï¼Œå¹¶é¢„æµ‹ç»™å®šèº«ä»½ç½‘æ ¼çš„å¯¹åº”é£æ ¼çš„åŸºç¡€å‘é‡ï¼Œä»è€Œå®ç°é£æ ¼åŒ–çš„ä¸‰ç»´æƒ…æ„Ÿé¢éƒ¨æ“ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡å¼•å…¥äº†AUBlendSetæ•°æ®é›†ï¼Œè¿™æ˜¯åŸºäºAU-Blendshapeè¡¨ç¤ºçš„ä¸‰ç»´é¢éƒ¨è¡¨æƒ…æ•°æ®é›†ï¼ŒåŒ…å«ç²¾ç»†ç²’åº¦çš„é¢éƒ¨åŠ¨ä½œå•å…ƒä¿¡æ¯è·¨è¶Š500ä¸ªèº«ä»½ã€‚</li>
<li>æå‡ºäº†AUBlendNetç½‘ç»œï¼Œç”¨äºå­¦ä¹ ä¸åŒè§’è‰²é£æ ¼çš„AU-BlendshapeåŸºç¡€å‘é‡ã€‚</li>
<li>AUBlendNetèƒ½å¤Ÿé¢„æµ‹ç»™å®šèº«ä»½ç½‘æ ¼çš„å¯¹åº”é£æ ¼çš„AU-BlendshapeåŸºç¡€å‘é‡ï¼Œå®ç°é£æ ¼åŒ–çš„ä¸‰ç»´æƒ…æ„Ÿé¢éƒ¨æ“ä½œã€‚</li>
<li>AUBlendSetå’ŒAUBlendNeté€šè¿‡é¢éƒ¨è¡¨æƒ…æ“ä½œã€è¯­éŸ³é©±åŠ¨çš„æƒ…æ„Ÿé¢éƒ¨åŠ¨ç”»å’Œæƒ…æ„Ÿè¯†åˆ«æ•°æ®å¢å¼ºç­‰ä»»åŠ¡è¿›è¡Œäº†å…¨é¢éªŒè¯ã€‚</li>
<li>AUBlendSetæ˜¯é¦–ä¸ªç”¨äºè¿ç»­ä¸‰ç»´é¢éƒ¨è¡¨æƒ…æ“ä½œçš„æ•°æ®é›†ï¼Œè€ŒAUBlendNetæ˜¯é¦–ä¸ªé€šè¿‡é¢éƒ¨åŠ¨ä½œå•å…ƒè¿›è¡Œä»»ä½•èº«ä»½çš„ä¸‰ç»´é¢éƒ¨è¡¨æƒ…æ“ä½œçš„ç½‘ç»œã€‚</li>
<li>è®ºæ–‡é€šè¿‡å®šæ€§å’Œå®šé‡å®éªŒè¯æ˜äº†AUBlendSetå’ŒAUBlendNetåœ¨ä¸‰ç»´é¢éƒ¨åŠ¨ç”»ä»»åŠ¡ä¸­çš„æ½œåŠ›å’Œé‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12001">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a7a40d3f5d0b3703930ff4fbaa247041.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-852c4651923a5dc1fba7eb75ed03b8fd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cb49039b09b8de884733bbe906a202d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e6d3c31b3aa390b189facfe02d60d178.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2295418d5afe3470ded8c311a90141ee.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a128d5d6c1d17d73796671baee6eccab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ab1e0d378243e175ccb46462f56b15c2.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Mitigation-of-exchange-cross-talk-in-dense-quantum-dot-arrays"><a href="#Mitigation-of-exchange-cross-talk-in-dense-quantum-dot-arrays" class="headerlink" title="Mitigation of exchange cross-talk in dense quantum dot arrays"></a>Mitigation of exchange cross-talk in dense quantum dot arrays</h2><p><strong>Authors:Daniel Jirovec, Pablo Cova FariÃ±a, Stefano Reale, Stefan D. Oosterhout, Xin Zhang, Sander de Snoo, Amir Sammak, Giordano Scappucci, Menno Veldhorst, Lieven M. K. Vandersypen</strong></p>
<p>Coupled spins in semiconductor quantum dots are a versatile platform for quantum computing and simulations of complex many-body phenomena. However, on the path of scale-up, cross-talk from densely packed electrodes poses a severe challenge. While cross-talk onto the dot potentials is nowadays routinely compensated for, cross-talk on the exchange interaction is much more difficult to tackle because it is not always directly measurable. Here we propose and implement a way of characterizing and compensating cross-talk on adjacent exchange interactions by following the singlet-triplet avoided crossing in Ge. We show that we can easily identify the barrier-to-barrier cross-talk element without knowledge of the particular exchange value in a 2x4 quantum dot array. We uncover striking differences among these cross-talk elements which can be linked to the geometry of the device and the barrier gate fan-out. We validate the methodology by tuning up four-spin Heisenberg chains. The same methodology should be applicable to longer chains of spins and to other semiconductor platforms in which mixing of the singlet and the lowest-energy triplet is present or can be engineered. Additionally, this procedure is well suited for automated tuning routines as we obtain a stand-out feature that can be easily tracked and directly returns the magnitude of the cross-talk. </p>
<blockquote>
<p>åŠå¯¼ä½“é‡å­ç‚¹ä¸­çš„è€¦åˆè‡ªæ—‹æ˜¯é‡å­è®¡ç®—å’Œæ¨¡æ‹Ÿå¤æ‚å¤šä½“ç°è±¡çš„é€šç”¨å¹³å°ã€‚ç„¶è€Œï¼Œåœ¨è§„æ¨¡æ‰©å¤§çš„è¿‡ç¨‹ä¸­ï¼Œå¯†é›†ç”µæçš„ä¸²è¯æ„æˆäº†ä¸¥å³»çš„æŒ‘æˆ˜ã€‚è™½ç„¶å½“å‰å¯ä»¥å¸¸è§„è¡¥å¿å¯¹å…‰ç‚¹åŠ¿èƒ½çš„ä¸²è¯ï¼Œä½†å¤„ç†å¯¹äº¤æ¢ä½œç”¨çš„ä¸²è¯è¦å›°éš¾å¾—å¤šï¼Œå› ä¸ºåè€…å¹¶ä¸æ€»æ˜¯å¯ä»¥ç›´æ¥æµ‹é‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºå¹¶å®æ–½äº†ä¸€ç§é€šè¿‡è·Ÿè¸ªGeä¸­çš„å•é‡æ€ä¸‰é‡æ€é¿å…äº¤å‰æ¥è¡¨å¾å’Œè¡¥å¿ç›¸é‚»äº¤æ¢ä½œç”¨ä¸Šçš„ä¸²è¯çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨ä¸çŸ¥é“2x4é‡å­ç‚¹é˜µåˆ—ä¸­ç‰¹å®šäº¤æ¢å€¼çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾è¯†åˆ«å‡ºå±éšœåˆ°å±éšœä¸²è¯å…ƒç´ ã€‚æˆ‘ä»¬å‘ç°è¿™äº›ä¸²è¯å…ƒç´ ä¹‹é—´å­˜åœ¨å¼•äººæ³¨ç›®çš„å·®å¼‚ï¼Œè¿™äº›å·®å¼‚ä¸è®¾å¤‡çš„å‡ ä½•å½¢çŠ¶å’Œå±éšœé—¨æ‰‡å‡ºæœ‰å…³ã€‚æˆ‘ä»¬é€šè¿‡è°ƒæ•´å››ä¸ªè‡ªæ—‹çš„æµ·æ£®å ¡é“¾éªŒè¯äº†è¯¥æ–¹æ³•ã€‚è¯¥æ–¹æ³•åº”é€‚ç”¨äºæ›´é•¿çš„è‡ªæ—‹é“¾å’Œå­˜åœ¨æ··åˆå•é‡æ€å’Œæœ€ä½èƒ½é‡ä¸‰é‡æ€æˆ–å¯å·¥ç¨‹è®¾è®¡çš„å…¶å®ƒåŠå¯¼ä½“å¹³å°ã€‚æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬è·å¾—äº†å¯ä»¥è½»æ˜“è¿½è¸ªå¹¶ç›´æ¥è¿”å›ä¸²è¯å¹…åº¦çš„çªå‡ºç‰¹å¾ï¼Œå› æ­¤è¯¥ç¨‹åºéå¸¸é€‚åˆç”¨äºè‡ªåŠ¨è°ƒæ•´ä¾‹è¡Œç¨‹åºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.23846v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŠå¯¼ä½“é‡å­ç‚¹ä¸­çš„è€¦åˆè‡ªæ—‹æ˜¯é‡å­è®¡ç®—å’Œå¤æ‚å¤šä½“ç°è±¡æ¨¡æ‹Ÿçš„é€šç”¨å¹³å°ã€‚ç„¶è€Œï¼Œåœ¨æ‰©å¤§è§„æ¨¡çš„è¿‡ç¨‹ä¸­ï¼Œæ¥è‡ªå¯†é›†ç”µæçš„ä¸²æ‰°æ˜¯ä¸€ä¸ªä¸¥é‡çš„æŒ‘æˆ˜ã€‚å°½ç®¡ç°ä»Šå·²èƒ½å¸¸è§„è¡¥å¿ç‚¹å¯¹ç‚¹çš„æ½œåŠ¿ä¸²æ‰°ï¼Œä½†äº¤æ¢ç›¸äº’ä½œç”¨çš„ä¸²æ‰°å´æ›´åŠ éš¾ä»¥åº”å¯¹ï¼Œå› ä¸ºå®ƒå¹¶ä¸æ€»æ˜¯å¯ç›´æ¥æµ‹é‡ã€‚æœ¬æ–‡æå‡ºå¹¶å®æ–½äº†ä¸€ç§é€šè¿‡è·Ÿè¸ªGeä¸­çš„å•é‡æ€ä¸‰é‡æ€é¿å…äº¤å‰æ¥è¡¨å¾å’Œè¡¥å¿ç›¸é‚»äº¤æ¢ç›¸äº’ä½œç”¨ä¸²æ‰°çš„æ–¹æ³•ã€‚æˆ‘ä»¬å±•ç¤ºäº†åœ¨ä¸çŸ¥é“ç‰¹å®šäº¤æ¢å€¼çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥åœ¨ä¸€ä¸ª2x4é‡å­ç‚¹é˜µåˆ—ä¸­è½»æ¾è¯†åˆ«å‡ºå±éšœé—´ä¸²æ‰°å…ƒç´ ã€‚æˆ‘ä»¬å‘ç°è¿™äº›ä¸²æ‰°å…ƒç´ ä¹‹é—´æœ‰ç€æ˜¾è‘—çš„åŒºåˆ«ï¼Œè¿™äº›åŒºåˆ«ä¸è®¾å¤‡çš„å‡ ä½•å½¢çŠ¶å’Œå±éšœé—¨å‘æ•£æœ‰å…³ã€‚é€šè¿‡è°ƒæ•´å››ä¸ªè‡ªæ—‹çš„æµ·æ£®å ¡é“¾éªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚åŒæ ·çš„æ–¹æ³•åº”é€‚ç”¨äºæ›´é•¿çš„è‡ªæ—‹é“¾å’Œå…¶ä»–å­˜åœ¨æˆ–å¯è®¾è®¡æ··åˆå•é‡æ€å’Œæœ€ä½èƒ½é‡ä¸‰é‡æ€çš„åŠå¯¼ä½“å¹³å°ã€‚æ­¤å¤–ï¼Œè¯¥ç¨‹åºéå¸¸é€‚åˆè‡ªåŠ¨åŒ–è°ƒæ•´æµç¨‹ï¼Œå› ä¸ºæˆ‘ä»¬è·å¾—äº†å¯ä»¥è½»æ˜“è¿½è¸ªå¹¶ç›´æ¥è¿”å›ä¸²æ‰°å¹…åº¦çš„çªå‡ºç‰¹å¾ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠå¯¼ä½“é‡å­ç‚¹ä¸­çš„è€¦åˆè‡ªæ—‹ä¸ºé‡å­è®¡ç®—å’Œå¤æ‚ç°è±¡æ¨¡æ‹Ÿæä¾›äº†å¹³å°ã€‚</li>
<li>å¯†é›†ç”µæå¼•èµ·çš„ä¸²æ‰°æ˜¯é‡å­ç‚¹æ‰©å¤§è§„æ¨¡çš„ä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€ã€‚</li>
<li>ç‚¹å¯¹ç‚¹çš„æ½œåŠ¿ä¸²æ‰°å·²å¯å¸¸è§„è¡¥å¿ï¼Œä½†äº¤æ¢ç›¸äº’ä½œç”¨çš„ä¸²æ‰°æ›´ä¸ºæ£˜æ‰‹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§é€šè¿‡è·Ÿè¸ªGeä¸­çš„å•é‡æ€-ä¸‰é‡æ€é¿å…äº¤å‰æ¥è¡¨å¾å’Œè¡¥å¿ä¸²æ‰°çš„æ–°æ–¹æ³•ã€‚</li>
<li>å¯åœ¨ä¸äº†è§£ç‰¹å®šäº¤æ¢å€¼çš„æƒ…å†µä¸‹è¯†åˆ«å±éšœé—´ä¸²æ‰°å…ƒç´ ã€‚</li>
<li>ä¸²æ‰°å…ƒç´ ä¹‹é—´çš„å·®å¼‚ä¸è®¾å¤‡å‡ ä½•å½¢çŠ¶å’Œå±éšœé—¨å‘æ•£æœ‰å…³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.23846">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-88525e82bbd65034c406216339401240.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49eca9808e6b5f96b57c5d8e2ebcac19.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-68a64d94f2c9eaf0a407c518c85d0a5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56274dc015b5064a1117570d7245caa7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df88e4c119b7c5ec44063379d62d07b5.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="TaoAvatar-Real-Time-Lifelike-Full-Body-Talking-Avatars-for-Augmented-Reality-via-3D-Gaussian-Splatting"><a href="#TaoAvatar-Real-Time-Lifelike-Full-Body-Talking-Avatars-for-Augmented-Reality-via-3D-Gaussian-Splatting" class="headerlink" title="TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented   Reality via 3D Gaussian Splatting"></a>TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented   Reality via 3D Gaussian Splatting</h2><p><strong>Authors:Jianchuan Chen, Jingchuan Hu, Gaige Wang, Zhonghua Jiang, Tiansong Zhou, Zhiwen Chen, Chengfei Lv</strong></p>
<p>Realistic 3D full-body talking avatars hold great potential in AR, with applications ranging from e-commerce live streaming to holographic communication. Despite advances in 3D Gaussian Splatting (3DGS) for lifelike avatar creation, existing methods struggle with fine-grained control of facial expressions and body movements in full-body talking tasks. Additionally, they often lack sufficient details and cannot run in real-time on mobile devices. We present TaoAvatar, a high-fidelity, lightweight, 3DGS-based full-body talking avatar driven by various signals. Our approach starts by creating a personalized clothed human parametric template that binds Gaussians to represent appearances. We then pre-train a StyleUnet-based network to handle complex pose-dependent non-rigid deformation, which can capture high-frequency appearance details but is too resource-intensive for mobile devices. To overcome this, we â€œbakeâ€ the non-rigid deformations into a lightweight MLP-based network using a distillation technique and develop blend shapes to compensate for details. Extensive experiments show that TaoAvatar achieves state-of-the-art rendering quality while running in real-time across various devices, maintaining 90 FPS on high-definition stereo devices such as the Apple Vision Pro. </p>
<blockquote>
<p>ç°å®ä¸»ä¹‰çš„3Då…¨èº«å¯¹è¯åŒ–èº«åœ¨å¢å¼ºç°å®ï¼ˆARï¼‰ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå…¶åº”ç”¨èŒƒå›´ä»ç”µå­å•†åŠ¡ç›´æ’­åˆ°å…¨æ¯é€šä¿¡ã€‚å°½ç®¡åœ¨ç”¨äºåˆ›å»ºé€¼çœŸåŒ–èº«çš„ä¸‰ç»´é«˜æ–¯æ‹¼è´´ï¼ˆ3DGSï¼‰æ–¹é¢å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨å…¨èº«å¯¹è¯ä»»åŠ¡ä¸­çš„é¢éƒ¨è¡¨æƒ…å’Œèº¯ä½“åŠ¨ä½œçš„ç²¾ç»†æ§åˆ¶æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå®ƒä»¬é€šå¸¸ç¼ºä¹è¶³å¤Ÿçš„ç»†èŠ‚ï¼Œæ— æ³•åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šå®æ—¶è¿è¡Œã€‚æˆ‘ä»¬æå‡ºäº†TaoAvatarï¼Œä¸€ä¸ªé«˜ä¿çœŸã€è½»é‡çº§çš„åŸºäºä¸‰ç»´é«˜æ–¯æ‹¼è´´çš„å…¨èº«å¯¹è¯åŒ–èº«ï¼Œç”±å„ç§ä¿¡å·é©±åŠ¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆé€šè¿‡åˆ›å»ºä¸ªæ€§åŒ–çš„ç©¿è¡£äººç±»å‚æ•°æ¨¡æ¿æ¥ç»‘å®šé«˜æ–¯æ¥è¡¨ç¤ºå¤–è§‚ã€‚ç„¶åï¼Œæˆ‘ä»¬åŸºäºStyleUnetç½‘ç»œè¿›è¡Œé¢„è®­ç»ƒï¼Œä»¥å¤„ç†å¤æ‚çš„å§¿æ€ç›¸å…³çš„éåˆšæ€§å˜å½¢ï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿæ•æ‰é«˜é¢‘å¤–è§‚ç»†èŠ‚ï¼Œä½†å¯¹äºç§»åŠ¨è®¾å¤‡è€Œè¨€èµ„æºæ¶ˆè€—è¿‡å¤§ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨è’¸é¦æŠ€æœ¯å°†éåˆšæ€§å˜å½¢â€œçƒ˜ç„™â€åˆ°ä¸€ä¸ªåŸºäºè½»é‡çº§å¤šå±‚æ„ŸçŸ¥æœºçš„ç½‘ç»œä¸­ï¼Œå¹¶å¼€å‘æ··åˆå½¢çŠ¶æ¥è¡¥å¿ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTaoAvataråœ¨ä¿æŒå®æ—¶è¿è¡Œçš„åŒæ—¶å®ç°äº†æœ€å…ˆè¿›çš„æ¸²æŸ“è´¨é‡ï¼Œåœ¨å„ç§è®¾å¤‡ä¸Šéƒ½èƒ½ä»¥æ¯ç§’90å¸§çš„é€Ÿåº¦è¿è¡Œï¼Œå¦‚åœ¨è‹¹æœè§†è§‰ä¸“ä¸šç‰ˆç­‰é«˜åˆ†è¾¨ç‡ç«‹ä½“å£°è®¾å¤‡ä¸Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.17032v2">PDF</a> Accepted by CVPR 2025 (Highlight), project page:   <a target="_blank" rel="noopener" href="https://pixelai-team.github.io/TaoAvatar">https://PixelAI-Team.github.io/TaoAvatar</a></p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§åŸºäº3Dé«˜æ–¯æ‹¼è´´æŠ€æœ¯çš„é«˜ä¿çœŸè½»é‡çº§å…¨èº«è¯´è¯é˜¿å‡¡è¾¾ç³»ç»Ÿâ€”â€”TaoAvatarï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åˆ›å»ºä¸ªæ€§åŒ–çš„ç©¿è¡£äººç±»å‚æ•°æ¨¡æ¿å¹¶ç»‘å®šé«˜æ–¯è¡¨ç¤ºå¤–è§‚ï¼Œè§£å†³äº†ç°æœ‰æŠ€æœ¯åœ¨å…¨èº«è¯´è¯ä»»åŠ¡ä¸­å¯¹é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œæ§åˆ¶çš„ç²¾ç»†åº¦ä¸è¶³é—®é¢˜ã€‚åŒæ—¶ï¼Œè¯¥ç³»ç»Ÿå…·æœ‰å®æ—¶è¿è¡Œèƒ½åŠ›ï¼Œé€‚ç”¨äºå„ç§è®¾å¤‡ï¼Œç”šè‡³åœ¨é«˜æ¸…ç«‹ä½“å£°è®¾å¤‡ä¸Šä¹Ÿèƒ½ä¿æŒ90å¸§çš„å¸§ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TaoAvataræ˜¯ä¸€ä¸ªé«˜ä¿çœŸè½»é‡çº§çš„å…¨èº«è¯´è¯é˜¿å‡¡è¾¾ç³»ç»Ÿï¼ŒåŸºäº3Dé«˜æ–¯æ‹¼è´´æŠ€æœ¯ã€‚</li>
<li>å®ƒè§£å†³äº†ç°æœ‰æŠ€æœ¯åœ¨é¢éƒ¨è¡¨æƒ…å’ŒåŠ¨ä½œæ§åˆ¶ä¸Šçš„ç²¾ç»†åº¦é—®é¢˜ã€‚</li>
<li>é€šè¿‡åˆ›å»ºä¸ªæ€§åŒ–çš„ç©¿è¡£äººç±»å‚æ•°æ¨¡æ¿ï¼Œå°†é«˜æ–¯ç»‘å®šä»¥è¡¨ç¤ºå¤–è§‚ã€‚</li>
<li>é‡‡ç”¨é¢„è®­ç»ƒçš„StyleUnetç½‘ç»œå¤„ç†å¤æ‚çš„å§¿åŠ¿ç›¸å…³éåˆšæ€§å˜å½¢ã€‚</li>
<li>åˆ©ç”¨è’¸é¦æŠ€æœ¯å°†éåˆšæ€§å˜å½¢â€œçƒ˜ç„™â€åˆ°è½»é‡çº§MLPç½‘ç»œä¸­ï¼Œå¹¶å¼€å‘æ··åˆå½¢çŠ¶ä»¥è¡¥å¿ç»†èŠ‚ã€‚</li>
<li>TaoAvatarå®ç°äº†å“è¶Šçš„æ¸²æŸ“è´¨é‡ï¼Œå…·æœ‰å®æ—¶è¿è¡Œèƒ½åŠ›ï¼Œé€‚ç”¨äºå„ç§è®¾å¤‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.17032">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d7c6fc7f90c9ecf5776ccb3ce30576ed.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-862ccee15de60e9a0bb0b546ea2e0803.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-708c54c5743edb92f1155d68fe2c3af3.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Versatile-Multimodal-Controls-for-Expressive-Talking-Human-Animation"><a href="#Versatile-Multimodal-Controls-for-Expressive-Talking-Human-Animation" class="headerlink" title="Versatile Multimodal Controls for Expressive Talking Human Animation"></a>Versatile Multimodal Controls for Expressive Talking Human Animation</h2><p><strong>Authors:Zheng Qin, Ruobing Zheng, Yabing Wang, Tianqi Li, Zixin Zhu, Sanping Zhou, Ming Yang, Le Wang</strong></p>
<p>In filmmaking, directors typically allow actors to perform freely based on the script before providing specific guidance on how to present key actions. AI-generated content faces similar requirements, where users not only need automatic generation of lip synchronization and basic gestures from audio input but also desire semantically accurate and expressive body movement that can be &#96;&#96;directly guidedâ€™â€™ through text descriptions. Therefore, we present VersaAnimator, a versatile framework that synthesizes expressive talking human videos from arbitrary portrait images. Specifically, we design a motion generator that produces basic rhythmic movements from audio input and supports text-prompt control for specific actions. The generated whole-body 3D motion tokens can animate portraits of various scales, producing talking heads, half-body gestures and even leg movements for whole-body images. Besides, we introduce a multi-modal controlled video diffusion that generates photorealistic videos, where speech signals govern lip synchronization, facial expressions, and head motions while body movements are guided by the 2D poses. Furthermore, we introduce a token2pose translator to smoothly map 3D motion tokens to 2D pose sequences. This design mitigates the stiffness resulting from direct 3D to 2D conversion and enhances the details of the generated body movements. Extensive experiments shows that VersaAnimator synthesizes lip-synced and identity-preserving videos while generating expressive and semantically meaningful whole-body motions. </p>
<blockquote>
<p>åœ¨å½±è§†åˆ¶ä½œä¸­ï¼Œå¯¼æ¼”é€šå¸¸ä¼šæ ¹æ®å‰§æœ¬è®©æ¼”å‘˜è‡ªç”±å‘æŒ¥ï¼Œç„¶ååœ¨å‘ˆç°å…³é”®åŠ¨ä½œæ—¶æä¾›å…·ä½“æŒ‡å¯¼ã€‚äººå·¥æ™ºèƒ½ç”Ÿæˆçš„å†…å®¹ä¹Ÿé¢ä¸´ç±»ä¼¼çš„éœ€æ±‚ï¼Œç”¨æˆ·ä¸ä»…éœ€è¦è‡ªåŠ¨ç”Ÿæˆä¸éŸ³é¢‘è¾“å…¥åŒæ­¥çš„å£å‹å’ŒåŸºæœ¬åŠ¨ä½œï¼Œè¿˜å¸Œæœ›ç”Ÿæˆè¯­ä¹‰å‡†ç¡®ã€å¯Œæœ‰è¡¨ç°åŠ›çš„èº«ä½“åŠ¨ä½œï¼Œè¿™äº›åŠ¨ä½œå¯ä»¥é€šè¿‡æ–‡æœ¬æè¿°è¿›è¡Œâ€œç›´æ¥æŒ‡å¯¼â€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºäº†VersaAnimatorï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå¯ä»¥ä»ä»»æ„è‚–åƒå›¾åƒä¸­åˆæˆå¯Œæœ‰è¡¨ç°åŠ›çš„è°ˆè¯äººç±»è§†é¢‘ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŠ¨ä½œç”Ÿæˆå™¨ï¼Œå®ƒå¯ä»¥ä»éŸ³é¢‘è¾“å…¥ä¸­äº§ç”ŸåŸºæœ¬çš„èŠ‚å¥åŠ¨ä½œï¼Œå¹¶æ”¯æŒé€šè¿‡æ–‡æœ¬æç¤ºæ§åˆ¶ç‰¹å®šåŠ¨ä½œã€‚ç”Ÿæˆçš„å…¨èº«ä¸‰ç»´åŠ¨ä½œä»¤ç‰Œå¯ä»¥åŠ¨ç”»åŒ–å„ç§å°ºåº¦çš„è‚–åƒï¼Œäº§ç”Ÿè°ˆè¯å¤´ã€åŠèº«å§¿åŠ¿ç”šè‡³å…¨èº«å›¾åƒçš„åŒè…¿åŠ¨ä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šæ¨¡å¼æ§åˆ¶çš„è§†é¢‘æ‰©æ•£æ–¹æ³•ï¼Œç”Ÿæˆé€¼çœŸçš„è§†é¢‘ï¼Œå…¶ä¸­è¯­éŸ³ä¿¡å·æ§åˆ¶å£å‹åŒæ­¥ã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨åŠ¨ä½œï¼Œè€Œèº«ä½“åŠ¨ä½œåˆ™ç”±äºŒç»´å§¿åŠ¿å¼•å¯¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†token2poseç¿»è¯‘å™¨ï¼Œå°†3Dè¿åŠ¨ä»¤ç‰Œå¹³æ»‘åœ°æ˜ å°„åˆ°2Då§¿åŠ¿åºåˆ—ã€‚è¿™ç§è®¾è®¡ç¼“è§£äº†ç›´æ¥3Dåˆ°2Dè½¬æ¢å¯¼è‡´çš„åƒµç¡¬é—®é¢˜ï¼Œå¹¶å¢å¼ºäº†ç”Ÿæˆçš„èº«ä½“åŠ¨ä½œçš„ç»†èŠ‚ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVersaAnimatoråˆæˆçš„è§†é¢‘å…·æœ‰å£å‹åŒæ­¥å’Œèº«ä»½ä¿ç•™çš„ç‰¹ç‚¹ï¼ŒåŒæ—¶ç”Ÿæˆäº†å¯Œæœ‰è¡¨ç°åŠ›å’Œè¯­ä¹‰æ„ä¹‰çš„å…¨èº«åŠ¨ä½œã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.08714v4">PDF</a> Accepted by ACM MM2025</p>
<p><strong>æ‘˜è¦</strong><br>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§æ–°å‹äººå·¥æ™ºèƒ½ç”Ÿæˆå†…å®¹çš„å½±è§†åŠ¨ç”»åˆ¶ä½œæ¡†æ¶â€”â€”VersaAnimatorã€‚å®ƒèƒ½å¤ŸåŸºäºä»»æ„è‚–åƒå›¾åƒç”Ÿæˆè¯´è¯å¤´éƒ¨åŠ¨ä½œåŠå…¨èº«åŠ¨ä½œï¼Œå…·å¤‡æ–‡æœ¬æç¤ºæ§åˆ¶ç‰¹å®šåŠ¨ä½œçš„èƒ½åŠ›ã€‚é€šè¿‡è®¾è®¡è¿åŠ¨ç”Ÿæˆå™¨ï¼Œå®ç°ä»éŸ³é¢‘è¾“å…¥äº§ç”ŸåŸºæœ¬èŠ‚å¥åŠ¨ä½œå¹¶æ”¯æŒæ–‡æœ¬æç¤ºæ§åˆ¶ç‰¹å®šåŠ¨ä½œã€‚åŒæ—¶ï¼Œå¼•å…¥å¤šæ¨¡æ€æ§åˆ¶è§†é¢‘æ‰©æ•£æŠ€æœ¯ï¼Œå®ç°è¯­éŸ³ä¿¡å·æ§åˆ¶å”‡éƒ¨åŒæ­¥ã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨åŠ¨ä½œçš„åŒæ—¶ï¼Œèº«ä½“åŠ¨ä½œé€šè¿‡äºŒç»´å§¿æ€å¼•å¯¼ã€‚è¿˜å¼•å…¥token2poseç¿»è¯‘å™¨ï¼Œå°†ä¸‰ç»´è¿åŠ¨ä»¤ç‰Œå¹³æ»‘æ˜ å°„åˆ°äºŒç»´å§¿æ€åºåˆ—ï¼Œè§£å†³äº†ç›´æ¥ä¸‰ç»´åˆ°äºŒç»´è½¬æ¢çš„åƒµç¡¬é—®é¢˜ï¼Œå¢å¼ºäº†ç”Ÿæˆçš„èº«ä½“åŠ¨ä½œçš„ç»†èŠ‚ã€‚å®éªŒè¡¨æ˜ï¼ŒVersaAnimatorèƒ½å¤Ÿåˆæˆå”‡éƒ¨åŒæ­¥ã€èº«ä»½ä¿ç•™çš„è§†é¢‘ï¼ŒåŒæ—¶ç”Ÿæˆè¡¨è¾¾ä¸°å¯Œã€è¯­ä¹‰æ˜ç¡®çš„å…¨èº«åŠ¨ä½œã€‚</p>
<p><strong>è¦ç‚¹ç­æœ›</strong></p>
<p>ä»¥ä¸‹æ˜¯ä¸ƒä¸ªè¦ç‚¹å†…å®¹æ¦‚è¿°ï¼š</p>
<ul>
<li>å¯¼æ¼”åœ¨ç”µå½±åˆ¶ä½œè¿‡ç¨‹ä¸­å…è®¸æ¼”å‘˜æ ¹æ®å‰§æœ¬è‡ªç”±å‘æŒ¥ï¼ŒAIç”Ÿæˆå†…å®¹ä¹Ÿæœ‰ç±»ä¼¼éœ€æ±‚ã€‚ç”¨æˆ·ä¸ä»…éœ€è¦è‡ªåŠ¨ç”Ÿæˆä¸éŸ³é¢‘è¾“å…¥åŒ¹é…çš„å”‡éƒ¨åŒæ­¥å’ŒåŸºæœ¬æ‰‹åŠ¿ï¼Œè¿˜å¸Œæœ›å¯ä»¥é€šè¿‡æ–‡æœ¬æè¿°æ¥æŒ‡å¯¼æ›´ç²¾ç¡®å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„èº«ä½“åŠ¨ä½œã€‚</li>
<li>VersaAnimatoræ¡†æ¶è¢«æå‡ºæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ƒå¯ä»¥ä»ä»»æ„è‚–åƒå›¾åƒåˆæˆè¡¨è¾¾æ€§å¼ºçš„è°ˆè¯è§†é¢‘ã€‚</li>
<li>è®¾è®¡äº†è¿åŠ¨ç”Ÿæˆå™¨ï¼Œå¯ä»¥ä»éŸ³é¢‘è¾“å…¥äº§ç”ŸåŸºæœ¬èŠ‚å¥åŠ¨ä½œï¼Œå¹¶æ”¯æŒé€šè¿‡æ–‡æœ¬æç¤ºæ¥æ§åˆ¶ç‰¹å®šåŠ¨ä½œã€‚</li>
<li>å¼•å…¥å¤šæ¨¡æ€æ§åˆ¶è§†é¢‘æ‰©æ•£æŠ€æœ¯ï¼Œåœ¨ä¿æŒå”‡éƒ¨åŒæ­¥ã€é¢éƒ¨è¡¨æƒ…å’Œå¤´éƒ¨åŠ¨ä½œçš„åŒæ—¶ï¼Œé€šè¿‡äºŒç»´å§¿æ€å¼•å¯¼èº«ä½“åŠ¨ä½œã€‚</li>
<li>ä¸ºäº†è§£å†³ç›´æ¥ä»ä¸‰ç»´åˆ°äºŒç»´è½¬æ¢å¯èƒ½å¸¦æ¥çš„åƒµç¡¬é—®é¢˜ï¼Œå¼•å…¥äº†token2poseç¿»è¯‘å™¨ï¼Œä»¥å¹³æ»‘åœ°æ˜ å°„ä¸‰ç»´è¿åŠ¨ä»¤ç‰Œåˆ°äºŒç»´å§¿æ€åºåˆ—ã€‚</li>
<li>è¯¥æ¡†æ¶å¯ä»¥åº”ç”¨äºä¸åŒè§„æ¨¡çš„è‚–åƒåŠ¨ç”»ï¼ŒåŒ…æ‹¬è°ˆè¯å¤´éƒ¨ã€åŠèº«æ‰‹åŠ¿ç”šè‡³æ˜¯å…¨èº«å›¾åƒçš„åŠ¨ä½œç”Ÿæˆã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.08714">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-4ece756ddbcbb9cb90df7645521e10d1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7f12638b9cf1c0f25da3f50bc91e174.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8edeef7feb631e64561559cfeaf972c9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7864a8c243ae7b482377f2235a595d5f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1841831cdf57ca48d05cf99b17e27349.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-245c7d0f1c0b208f404ad96895c9a136.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-4cd61fb10030db1001c05898e30f80de.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  SeqAffordSplat Scene-level Sequential Affordance Reasoning on 3D   Gaussian Splatting
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-17/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2d0f98dca8800476a7e11dfe387d1302.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-17  SnapMoGen Human Motion Generation from Expressive Texts
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32140.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
