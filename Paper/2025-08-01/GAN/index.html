<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="GAN">
    <meta name="description" content="GAN æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  IN45023 Neural Network Design Patterns in Computer Vision Seminar   Report, Summer 2025">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>GAN | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f55b25dd93b83437b5e43038bd3e0a8f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">GAN</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/GAN/">
                                <span class="chip bg-color">GAN</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/GAN/" class="post-category">
                                GAN
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-02
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    14.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    59 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-02-æ›´æ–°"><a href="#2025-08-02-æ›´æ–°" class="headerlink" title="2025-08-02 æ›´æ–°"></a>2025-08-02 æ›´æ–°</h1><h2 id="IN45023-Neural-Network-Design-Patterns-in-Computer-Vision-Seminar-Report-Summer-2025"><a href="#IN45023-Neural-Network-Design-Patterns-in-Computer-Vision-Seminar-Report-Summer-2025" class="headerlink" title="IN45023 Neural Network Design Patterns in Computer Vision Seminar   Report, Summer 2025"></a>IN45023 Neural Network Design Patterns in Computer Vision Seminar   Report, Summer 2025</h2><p><strong>Authors:Radu-Andrei Bourceanu, Neil De La Fuente, Jan Grimm, Andrei Jardan, Andriy Manucharyan, Cornelius Weiss, Roman Pflugfelder</strong></p>
<p>This report analyzes the evolution of key design patterns in computer vision by examining six influential papers. The analy- sis begins with foundational architectures for image recognition. We review ResNet, which introduced residual connections to overcome the vanishing gradient problem and enable effective training of significantly deeper convolutional networks. Subsequently, we examine the Vision Transformer (ViT), which established a new paradigm by applying the Transformer ar- chitecture to sequences of image patches, demonstrating the efficacy of attention-based models for large-scale image recogni- tion. Building on these visual representation backbones, we investigate generative models. Generative Adversarial Networks (GANs) are analyzed for their novel adversarial training process, which challenges a generator against a discriminator to learn complex data distributions. Then, Latent Diffusion Models (LDMs) are covered, which improve upon prior generative methods by performing a sequential denoising process in a perceptually compressed latent space. LDMs achieve high-fidelity synthesis with greater computational efficiency, representing the current state-of-the-art for image generation. Finally, we explore self-supervised learning techniques that reduce dependency on labeled data. DINO is a self-distillation framework in which a student network learns to match the output of a momentum-updated teacher, yielding features with strong k-NN classification performance. We conclude with Masked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design to reconstruct heavily masked inputs, providing a highly scalable and effective method for pre-training large-scale vision models. </p>
<blockquote>
<p>æœ¬æŠ¥å‘Šé€šè¿‡åˆ†æå…­ç¯‡æœ‰å½±å“åŠ›çš„è®ºæ–‡ï¼Œåˆ†æäº†è®¡ç®—æœºè§†è§‰ä¸­å…³é”®è®¾è®¡æ¨¡å¼çš„æ¼”å˜ã€‚åˆ†æä»å›¾åƒè¯†åˆ«çš„åŸºæœ¬æ¶æ„å¼€å§‹ã€‚æˆ‘ä»¬å›é¡¾äº†ResNetï¼Œå®ƒå¼•å…¥äº†æ®‹å·®è¿æ¥ï¼Œå…‹æœäº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå®ç°äº†å¯¹æ›´æ·±å·ç§¯ç½‘ç»œçš„æœ‰æ•ˆè®­ç»ƒã€‚ä¹‹åï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒè¡¥ä¸åºåˆ—çš„Vision Transformerï¼ˆViTï¼‰ï¼Œè¿™å»ºç«‹äº†ä¸€ç§æ–°çš„æ¨¡å¼ï¼Œå¹¶è¯æ˜äº†æ³¨æ„åŠ›æ¨¡å‹åœ¨å¤§è§„æ¨¡å›¾åƒè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚åŸºäºè¿™äº›è§†è§‰è¡¨ç¤ºéª¨å¹²ç½‘ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç”Ÿæˆæ¨¡å‹ã€‚åˆ†æäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ–°å‹å¯¹æŠ—è®­ç»ƒè¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹é€šè¿‡ç”Ÿæˆå™¨ä¸é‰´åˆ«å™¨ä¹‹é—´çš„å¯¹æŠ—æ¥å­¦ä¹ å¤æ‚çš„æ•°æ®åˆ†å¸ƒã€‚ç„¶åä»‹ç»äº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ï¼Œé€šè¿‡åœ¨æ„ŸçŸ¥å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œè¿ç»­å»å™ªè¿‡ç¨‹ï¼Œæ”¹è¿›äº†å…ˆå‰çš„ç”Ÿæˆæ–¹æ³•ã€‚LDMså®ç°äº†é«˜ä¿çœŸåˆæˆï¼Œå…·æœ‰æ›´é«˜çš„è®¡ç®—æ•ˆç‡ï¼Œä»£è¡¨äº†å½“å‰å›¾åƒç”Ÿæˆçš„æœ€æ–°æŠ€æœ¯ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢ç´¢äº†å‡å°‘å¯¹æ ‡å®šæ•°æ®ä¾èµ–æ€§çš„è‡ªç›‘ç£å­¦ä¹ æŠ€æœ¯ã€‚DINOæ˜¯ä¸€ç§è‡ªè’¸é¦æ¡†æ¶ï¼Œå­¦ç”Ÿç½‘ç»œå­¦ä¹ åŒ¹é…åŠ¨é‡æ›´æ–°åçš„æ•™å¸ˆè¾“å‡ºï¼Œäº§ç”Ÿå…·æœ‰å¼ºå¤§k-NNåˆ†ç±»æ€§èƒ½çš„ç‰¹å¾ã€‚æœ€åä»¥ä½¿ç”¨ä¸å¯¹ç§°ç¼–ç å™¨è§£ç å™¨è®¾è®¡é‡å»ºé«˜åº¦é®æŒ¡è¾“å…¥çš„Masked Autoencodersï¼ˆMAEï¼‰ä¸ºä¾‹ï¼Œæä¾›äº†ä¸€ç§é«˜åº¦å¯æ‰©å±•å’Œæœ‰æ•ˆçš„é¢„è®­ç»ƒå¤§è§„æ¨¡è§†è§‰æ¨¡å‹çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23357v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡æŠ¥å‘Šåˆ†æäº†è®¡ç®—æœºè§†è§‰ä¸­å…³é”®è®¾è®¡æ¨¡å¼çš„æ¼”å˜ï¼Œé€šè¿‡è€ƒå¯Ÿå…­ç¯‡æœ‰å½±å“åŠ›çš„è®ºæ–‡è¿›è¡Œæ·±å…¥ç ”ç©¶ã€‚æŠ¥å‘Šä»å›¾åƒè¯†åˆ«çš„åŸºæœ¬æ¶æ„å¼€å§‹ï¼Œä»‹ç»äº†ResNetå’ŒVision Transformerç­‰å…³é”®æŠ€æœ¯ã€‚ç„¶åæ¢è®¨äº†ç”Ÿæˆæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ½œæ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰çš„å¯¹æŠ—æ€§è®­ç»ƒè¿‡ç¨‹å’Œåºåˆ—é™å™ªæŠ€æœ¯ã€‚æœ€åï¼ŒæŠ¥å‘Šä»‹ç»äº†è‡ªæˆ‘ç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼Œå¦‚DINOå’ŒMasked Autoencodersç­‰ï¼Œè¿™äº›æŠ€æœ¯å‡å°‘äº†å¯¹äºæ ‡æ³¨æ•°æ®çš„ä¾èµ–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŠ¥å‘Šåˆ†æäº†è®¡ç®—æœºè§†è§‰é¢†åŸŸå…³é”®è®¾è®¡æ¨¡å¼çš„æ¼”å˜ï¼Œä»å›¾åƒè¯†åˆ«çš„åŸºæœ¬æ¶æ„åˆ°ç”Ÿæˆæ¨¡å‹å’Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ æŠ€æœ¯çš„æ¼”è¿›ã€‚</li>
<li>ResNeté€šè¿‡å¼•å…¥æ®‹å·®è¿æ¥å…‹æœäº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½¿æ·±åº¦å·ç§¯ç½‘ç»œçš„æœ‰æ•ˆè®­ç»ƒæˆä¸ºå¯èƒ½ã€‚</li>
<li>Vision Transformerï¼ˆViTï¼‰å°†Transformeræ¶æ„åº”ç”¨äºå›¾åƒè¡¥ä¸åºåˆ—ï¼Œå±•ç¤ºäº†æ³¨æ„åŠ›æ¨¡å‹åœ¨å¤§è§„æ¨¡å›¾åƒè¯†åˆ«ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„å¯¹æŠ—æ€§è®­ç»ƒè¿‡ç¨‹æŒ‘æˆ˜äº†ç”Ÿæˆå™¨ä¸é‰´åˆ«å™¨çš„å¯¹æŠ—ï¼Œä»è€Œå­¦ä¹ å¤æ‚çš„æ•°æ®åˆ†å¸ƒã€‚</li>
<li>æ½œæ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰é€šè¿‡æ„ŸçŸ¥å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­çš„åºåˆ—é™å™ªè¿‡ç¨‹æ”¹è¿›äº†å…ˆå‰çš„ç”Ÿæˆæ–¹æ³•ï¼Œå®ç°äº†é«˜ä¿çœŸåˆæˆå’Œæ›´é«˜çš„è®¡ç®—æ•ˆç‡ã€‚</li>
<li>DINOæ˜¯ä¸€ç§è‡ªè’¸é¦æ¡†æ¶ï¼Œå­¦ç”Ÿç½‘ç»œå­¦ä¹ åŒ¹é…åŠ¨é‡æ›´æ–°çš„æ•™å¸ˆè¾“å‡ºï¼Œå…·æœ‰å¼ºå¤§çš„k-NNåˆ†ç±»æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23357">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f55b25dd93b83437b5e43038bd3e0a8f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cb308890c31345f2aab07dd14fc1774c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43c7b7523d1996c598c4c86034ca318b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-498ae1a3c68c14971d462838ba0e1fa5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5a42b24e9ff5111ddf2f9f617b925344.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Visual-Language-Models-as-Zero-Shot-Deepfake-Detectors"><a href="#Visual-Language-Models-as-Zero-Shot-Deepfake-Detectors" class="headerlink" title="Visual Language Models as Zero-Shot Deepfake Detectors"></a>Visual Language Models as Zero-Shot Deepfake Detectors</h2><p><strong>Authors:Viacheslav Pirogov</strong></p>
<p>The contemporary phenomenon of deepfakes, utilizing GAN or diffusion models for face swapping, presents a substantial and evolving threat in digital media, identity verification, and a multitude of other systems. The majority of existing methods for detecting deepfakes rely on training specialized classifiers to distinguish between genuine and manipulated images, focusing only on the image domain without incorporating any auxiliary tasks that could enhance robustness. In this paper, inspired by the zero-shot capabilities of Vision Language Models, we propose a novel VLM-based approach to image classification and then evaluate it for deepfake detection. Specifically, we utilize a new high-quality deepfake dataset comprising 60,000 images, on which our zero-shot models demonstrate superior performance to almost all existing methods. Subsequently, we compare the performance of the best-performing architecture, InstructBLIP, on the popular deepfake dataset DFDC-P against traditional methods in two scenarios: zero-shot and in-domain fine-tuning. Our results demonstrate the superiority of VLMs over traditional classifiers. </p>
<blockquote>
<p>åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æˆ–æ‰©æ•£æ¨¡å‹è¿›è¡Œé¢éƒ¨æ›¿æ¢çš„Deepfakesç°è±¡å·²æˆä¸ºæ•°å­—ä¼ åª’ã€èº«ä»½è®¤è¯å’Œå…¶ä»–å¤šç§ç³»ç»Ÿä¸­ä¸€ä¸ªæŒç»­å‘å±•çš„é‡å¤§å¨èƒã€‚ç›®å‰å¤§å¤šæ•°æ£€æµ‹Deepfakesçš„æ–¹æ³•éƒ½ä¾èµ–äºè®­ç»ƒä¸“é—¨çš„åˆ†ç±»å™¨æ¥åŒºåˆ†çœŸå®å’Œæ“çºµè¿‡çš„å›¾åƒï¼Œå®ƒä»¬åªä¸“æ³¨äºå›¾åƒé¢†åŸŸï¼Œæ²¡æœ‰å¼•å…¥ä»»ä½•å¯ä»¥æå‡ç¨³å¥æ€§çš„è¾…åŠ©ä»»åŠ¡ã€‚æœ¬æ–‡å—è§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ–°é¢–çš„å›¾åƒåˆ†ç±»æ–¹æ³•ï¼Œå¹¶è¯„ä¼°å…¶åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸­çš„è¡¨ç°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸€ä¸ªæ–°çš„é«˜è´¨é‡æ·±åº¦ä¼ªé€ æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«6ä¸‡å¼ å›¾åƒï¼Œæˆ‘ä»¬çš„é›¶æ ·æœ¬æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå‡ ä¹æ‰€æœ‰ç°æœ‰æ–¹æ³•ã€‚éšåï¼Œæˆ‘ä»¬åœ¨æµè¡Œçš„æ·±åº¦ä¼ªé€ æ•°æ®é›†DFDC-Pä¸Šå¯¹æ¯”äº†è¡¨ç°æœ€ä½³çš„æ¶æ„InstructBLIPä¸ä¼ ç»Ÿæ–¹æ³•åœ¨é›¶æ ·æœ¬å’Œé¢†åŸŸå†…å¾®è°ƒä¸¤ç§åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†è§†è§‰è¯­è¨€æ¨¡å‹ç›¸è¾ƒäºä¼ ç»Ÿåˆ†ç±»å™¨çš„ä¼˜è¶Šæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.22469v1">PDF</a> Accepted to the ICML 2025 Workshop on Reliable and Responsible   Foundation Models</p>
<p><strong>Summary</strong><br>    åŸºäºGANæˆ–æ‰©æ•£æ¨¡å‹çš„é¢è²Œæ¢è„¸æŠ€æœ¯å¼•å‘çš„æ·±åº¦ä¼ªé€ ç°è±¡ï¼Œå¯¹æ•°å­—åª’ä½“ã€èº«ä»½è®¤è¯ç­‰å¤šä¸ªç³»ç»Ÿé€ æˆäº†é‡å¤§ä¸”ä¸æ–­å‘å±•çš„å¨èƒã€‚ç°æœ‰çš„æ·±åº¦ä¼ªé€ æ£€æµ‹ä¸»è¦ä¾èµ–è®­ç»ƒä¸“é—¨çš„åˆ†ç±»å™¨æ¥åŒºåˆ†çœŸå®å’Œä¼ªé€ å›¾åƒï¼Œä½†è¿™ç§æ–¹æ³•ç¼ºä¹è¾…åŠ©ä»»åŠ¡çš„è¾…åŠ©ï¼Œå¯èƒ½å½±å“å…¶ç¨³å¥æ€§ã€‚æœ¬ç ”ç©¶å—è§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›å¯å‘ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŸºäºVLMçš„å›¾åƒåˆ†ç±»æ–¹æ³•ï¼Œå¹¶ç”¨äºæ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚ç ”ç©¶ä½¿ç”¨é«˜è´¨é‡çš„æ·±åº¦ä¼ªé€ æ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºé›¶æ ·æœ¬æ¨¡å‹æ€§èƒ½ä¼˜äºç°æœ‰å¤§å¤šæ•°æ–¹æ³•ã€‚æ­¤å¤–ï¼Œå¯¹æ¯”äº†æœ€ä½³æ¶æ„InstructBLIPåœ¨DFDC-Pæµè¡Œæ·±åº¦ä¼ªé€ æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬å’ŒåŸŸå†…å¾®è°ƒä¸¤ç§åœºæ™¯çš„æ€§èƒ½ï¼Œè¯æ˜äº†VLMsç›¸æ¯”ä¼ ç»Ÿåˆ†ç±»å™¨çš„ä¼˜è¶Šæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦ä¼ªé€ ç°è±¡å·²ç»æˆä¸ºæ•°å­—åª’ä½“é¢†åŸŸçš„é‡å¤§å¨èƒï¼Œç°æœ‰çš„æ£€æµ‹æ‰‹æ®µéœ€è¦æ”¹è¿›ä»¥å¢å¼ºç¨³å¥æ€§ã€‚</li>
<li>æœ¬ç ”ç©¶å—åˆ°è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºVLMçš„å›¾åƒåˆ†ç±»æ–¹æ³•ç”¨äºæ·±åº¦ä¼ªé€ æ£€æµ‹ã€‚</li>
<li>ä½¿ç”¨é«˜è´¨é‡æ·±åº¦ä¼ªé€ æ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œé›¶æ ·æœ¬æ¨¡å‹å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>å¯¹æ¯”äº†æœ€ä½³æ¶æ„InstructBLIPåœ¨æµè¡Œæ·±åº¦ä¼ªé€ æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ŒåŒ…æ‹¬é›¶æ ·æœ¬å’ŒåŸŸå†…å¾®è°ƒä¸¤ç§åœºæ™¯ã€‚</li>
<li>ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„åˆ†ç±»å™¨ç›¸æ¯”ï¼Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ·±åº¦ä¼ªé€ æ£€æµ‹æ–¹é¢æ›´å…·ä¼˜åŠ¿ã€‚</li>
<li>é€šè¿‡å¼•å…¥è¾…åŠ©ä»»åŠ¡ï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ€§èƒ½å¹¶å¢å¼ºå…¶åœ¨å®é™…åº”ç”¨ä¸­çš„ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22469">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4574ad67e7a06d38d2ff41cdc97f08b2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3a0a994545b478dbbad8fe299d7328cf.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Enhancing-Generalization-in-Data-free-Quantization-via-Mixup-class-Prompting"><a href="#Enhancing-Generalization-in-Data-free-Quantization-via-Mixup-class-Prompting" class="headerlink" title="Enhancing Generalization in Data-free Quantization via Mixup-class   Prompting"></a>Enhancing Generalization in Data-free Quantization via Mixup-class   Prompting</h2><p><strong>Authors:Jiwoong Park, Chaeun Lee, Yongseok Choi, Sein Park, Deokki Hong, Jungwook Choi</strong></p>
<p>Post-training quantization (PTQ) improves efficiency but struggles with limited calibration data, especially under privacy constraints. Data-free quantization (DFQ) mitigates this by generating synthetic images using generative models such as generative adversarial networks (GANs) and text-conditioned latent diffusion models (LDMs), while applying existing PTQ algorithms. However, the relationship between generated synthetic images and the generalizability of the quantized model during PTQ remains underexplored. Without investigating this relationship, synthetic images generated by previous prompt engineering methods based on single-class prompts suffer from issues such as polysemy, leading to performance degradation. We propose \textbf{mixup-class prompt}, a mixup-based text prompting strategy that fuses multiple class labels at the text prompt level to generate diverse, robust synthetic data. This approach enhances generalization, and improves optimization stability in PTQ. We provide quantitative insights through gradient norm and generalization error analysis. Experiments on convolutional neural networks (CNNs) and vision transformers (ViTs) show that our method consistently outperforms state-of-the-art DFQ methods like GenQ. Furthermore, it pushes the performance boundary in extremely low-bit scenarios, achieving new state-of-the-art accuracy in challenging 2-bit weight, 4-bit activation (W2A4) quantization. </p>
<blockquote>
<p>åè®­ç»ƒé‡åŒ–ï¼ˆPTQï¼‰æé«˜äº†æ•ˆç‡ï¼Œä½†åœ¨æœ‰é™çš„æ ¡å‡†æ•°æ®æ–¹é¢é‡åˆ°äº†å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨éšç§çº¦æŸä¸‹ã€‚æ— æ•°æ®é‡åŒ–ï¼ˆDFQï¼‰é€šè¿‡ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ–‡æœ¬æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰ç­‰ç”Ÿæˆæ¨¡å‹ç”Ÿæˆåˆæˆå›¾åƒï¼Œå¹¶åº”ç”¨ç°æœ‰çš„PTQç®—æ³•ï¼Œä»è€Œç¼“è§£äº†è¿™ä¸€é—®é¢˜ã€‚ç„¶è€Œï¼Œåœ¨PTQæœŸé—´ï¼Œç”Ÿæˆçš„åˆæˆå›¾åƒä¸é‡åŒ–æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¹‹é—´çš„å…³ç³»ä»æœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚å¦‚æœä¸ç ”ç©¶è¿™ç§å…³ç³»ï¼Œä»¥å‰åŸºäºå•ç±»æç¤ºçš„æç¤ºå·¥ç¨‹æ–¹æ³•ç”Ÿæˆçš„åˆæˆå›¾åƒä¼šé¢ä¸´å¤šä¹‰æ€§é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬æå‡º<strong>æ··åˆç±»æç¤º</strong>ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ··åˆçš„æ–‡æœ¬æç¤ºç­–ç•¥ï¼Œå®ƒåœ¨æ–‡æœ¬æç¤ºå±‚é¢èåˆå¤šä¸ªç±»æ ‡ç­¾ï¼Œä»¥ç”Ÿæˆå¤šæ ·ä¸”ç¨³å¥çš„åˆæˆæ•°æ®ã€‚è¿™ç§æ–¹æ³•å¢å¼ºäº†æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶æé«˜äº†PTQä¸­çš„ä¼˜åŒ–ç¨³å®šæ€§ã€‚æˆ‘ä»¬é€šè¿‡æ¢¯åº¦èŒƒæ•°å’Œæ³›åŒ–è¯¯å·®åˆ†ææä¾›äº†å®šé‡è§è§£ã€‚åœ¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºæœ€æ–°çš„DFQæ–¹æ³•ï¼Œå¦‚GenQã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æä½ä½åœºæ™¯ä¸­æ¨åŠ¨äº†æ€§èƒ½è¾¹ç•Œï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„2ä½æƒé‡ã€4ä½æ¿€æ´»ï¼ˆW2A4ï¼‰é‡åŒ–ä¸­è¾¾åˆ°äº†æ–°çš„æœ€é«˜ç²¾åº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21947v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•åœ¨éšç§çº¦æŸä¸‹åˆ©ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ–‡æœ¬æ¡ä»¶æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMsï¼‰è¿›è¡Œæ•°æ®æ— å…³çš„é‡åŒ–é—®é¢˜ã€‚æå‡ºäº†ä¸€ç§åŸºäºæ··åˆæ–‡æœ¬æç¤ºç­–ç•¥çš„åˆæˆå›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œå¢å¼ºäº†æ¨¡å‹æ³›åŒ–èƒ½åŠ›å’Œä¼˜åŒ–ç¨³å®šæ€§ï¼Œåœ¨ç‰¹å®šåœºæ™¯ä¸­æ€§èƒ½è¶…è¿‡ç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®æ— å…³çš„é‡åŒ–æ–¹æ³•é€šè¿‡ç”Ÿæˆåˆæˆå›¾åƒè§£å†³äº†æ ¡å‡†æ•°æ®æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œç­‰æŠ€æœ¯çš„åˆæˆå›¾åƒåº”ç”¨åœ¨PTQä¸­æœ‰è‰¯å¥½çš„è¡¨ç°ã€‚ä½†åˆæˆå›¾åƒä¸æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„å…³ç³»æœªè¢«å……åˆ†ç ”ç©¶ã€‚</li>
<li>ä¼ ç»ŸåŸºäºå•ä¸€ç±»åˆ«æç¤ºçš„åˆæˆå›¾åƒç”Ÿæˆæ–¹æ³•å­˜åœ¨è¯­ä¹‰æ­§ä¹‰é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºæ··åˆæ–‡æœ¬æç¤ºç­–ç•¥çš„â€œmixup-class promptâ€ï¼Œèåˆå¤šä¸ªç±»åˆ«æ ‡ç­¾ç”Ÿæˆå¤šæ ·ä¸”ç¨³å¥çš„åˆæˆæ•°æ®ã€‚</li>
<li>è¯¥æ–¹æ³•å¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæé«˜äº†ä¼˜åŒ–ç¨³å®šæ€§ï¼Œå¹¶ä¸”åœ¨å®éªŒä¸Šä¼˜äºç°æœ‰çš„DFQæŠ€æœ¯ã€‚ç‰¹åˆ«æ˜¯å®ç°äº†æä½ä½åœºæ™¯ï¼ˆå¦‚W2A4ï¼‰ä¸‹çš„æœ€æ–°å‡†ç¡®æ€§ã€‚è¿™è¡¨æ˜åœ¨æŒ‘æˆ˜åœºæ™¯ä¸‹ï¼Œè¯¥ç­–ç•¥æœ‰æ˜¾è‘—çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>é€šè¿‡æ¢¯åº¦èŒƒæ•°å’Œæ³›åŒ–è¯¯å·®åˆ†ææä¾›äº†å®šé‡è§è§£ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„ä¼˜è¶Šæ€§ã€‚è¿™äº›å®šé‡æ•°æ®ä¸ºç†è§£è¯¥ç­–ç•¥æä¾›äº†é‡è¦çš„å‚è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21947">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f23a00008e793b29dc292768842dfa60.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b1e987f9ff7d38ad6f948ea48bbf738.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bf7fc94ce1aa7988d1114a23d033de2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24d9a380779cf620b7a28738ef429f82.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d88922cd039adc8a16ee8b8853d1e784.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Bringing-Balance-to-Hand-Shape-Classification-Mitigating-Data-Imbalance-Through-Generative-Models"><a href="#Bringing-Balance-to-Hand-Shape-Classification-Mitigating-Data-Imbalance-Through-Generative-Models" class="headerlink" title="Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance   Through Generative Models"></a>Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance   Through Generative Models</h2><p><strong>Authors:Gaston Gustavo Rios, Pedro Dal Bianco, Franco Ronchetti, Facundo Quiroga, Oscar Stanchi, Santiago Ponte AhÃ³n, Waldo HasperuÃ©</strong></p>
<p>Most sign language handshape datasets are severely limited and unbalanced, posing significant challenges to effective model training. In this paper, we explore the effectiveness of augmenting the training data of a handshape classifier by generating synthetic data. We use an EfficientNet classifier trained on the RWTH German sign language handshape dataset, which is small and heavily unbalanced, applying different strategies to combine generated and real images. We compare two Generative Adversarial Networks (GAN) architectures for data generation: ReACGAN, which uses label information to condition the data generation process through an auxiliary classifier, and SPADE, which utilizes spatially-adaptive normalization to condition the generation on pose information. ReACGAN allows for the generation of realistic images that align with specific handshape labels, while SPADE focuses on generating images with accurate spatial handshape configurations. Our proposed techniques improve the current state-of-the-art accuracy on the RWTH dataset by 5%, addressing the limitations of small and unbalanced datasets. Additionally, our method demonstrates the capability to generalize across different sign language datasets by leveraging pose-based generation trained on the extensive HaGRID dataset. We achieve comparable performance to single-source trained classifiers without the need for retraining the generator. </p>
<blockquote>
<p>å¤§éƒ¨åˆ†æ‰‹è¯­æ‰‹åŠ¿æ•°æ®é›†å­˜åœ¨ä¸¥é‡çš„é™åˆ¶å’Œä¸å¹³è¡¡é—®é¢˜ï¼Œç»™æœ‰æ•ˆæ¨¡å‹è®­ç»ƒå¸¦æ¥äº†å·¨å¤§çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†é€šè¿‡ç”Ÿæˆåˆæˆæ•°æ®æ¥å¢å¼ºæ‰‹åŠ¿åˆ†ç±»å™¨è®­ç»ƒæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬ä½¿ç”¨åœ¨RWTHå¾·è¯­æ‰‹è¯­æ‰‹åŠ¿æ•°æ®é›†ä¸Šè®­ç»ƒçš„EfficientNetåˆ†ç±»å™¨ï¼Œè¯¥æ•°æ®é›†å°ä¸”æåº¦ä¸å¹³è¡¡ï¼Œåº”ç”¨ä¸åŒçš„ç­–ç•¥æ¥ç»“åˆç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ä¸¤ç§ç”¨äºæ•°æ®ç”Ÿæˆçš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¶æ„ï¼šReACGANä½¿ç”¨æ ‡ç­¾ä¿¡æ¯é€šè¿‡è¾…åŠ©åˆ†ç±»å™¨å¯¹æ•°æ®ç”Ÿæˆè¿‡ç¨‹è¿›è¡Œæ¡ä»¶æ§åˆ¶ï¼Œè€ŒSPADEåˆ™åˆ©ç”¨ç©ºé—´è‡ªé€‚åº”å½’ä¸€åŒ–å¯¹å§¿åŠ¿ä¿¡æ¯è¿›è¡Œæ¡ä»¶æ§åˆ¶ä»¥ç”Ÿæˆå›¾åƒã€‚ReACGANèƒ½å¤Ÿç”Ÿæˆä¸ç‰¹å®šæ‰‹åŠ¿æ ‡ç­¾å¯¹é½çš„çœŸå®å›¾åƒï¼Œè€ŒSPADEåˆ™ä¾§é‡äºç”Ÿæˆå…·æœ‰å‡†ç¡®ç©ºé—´æ‰‹åŠ¿é…ç½®çš„å›¾åƒã€‚æˆ‘ä»¬æå‡ºçš„æŠ€æœ¯åœ¨RWTHæ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†5%ï¼Œè§£å†³äº†æ•°æ®é›†å°ä¸”ä¸å¹³è¡¡çš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆ©ç”¨åŸºäºå§¿åŠ¿çš„ç”Ÿæˆå™¨åœ¨åºå¤§çš„HaGRIDæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå±•ç¤ºäº†åœ¨ä¸åŒæ‰‹è¯­æ•°æ®é›†ä¹‹é—´è¿›è¡Œæ¨å¹¿çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å®ç°äº†ä¸å•æºè®­ç»ƒåˆ†ç±»å™¨ç›¸å½“çš„æ€§èƒ½ï¼Œä¸”æ— éœ€é‡æ–°è®­ç»ƒç”Ÿæˆå™¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17008v1">PDF</a> 23 pages, 8 figures, to be published in Applied Soft Computing</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†é€šè¿‡ç”Ÿæˆåˆæˆæ•°æ®æ¥å¢å¼ºæ‰‹å½¢åˆ†ç±»å™¨è®­ç»ƒæ•°æ®çš„æœ‰æ•ˆæ€§ã€‚ç ”ç©¶ä½¿ç”¨äº†RWTHå¾·è¯­æ‰‹è¯­æ‰‹å½¢æ•°æ®é›†è®­ç»ƒçš„EfficientNetåˆ†ç±»å™¨ï¼Œè¯¥æ•°æ®é›†å°ä¸”ä¸å‡è¡¡ã€‚é€šè¿‡åº”ç”¨ä¸åŒçš„ç­–ç•¥æ¥ç»“åˆç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒï¼Œç ”ç©¶æ¯”è¾ƒäº†ä¸¤ç§ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¶æ„ï¼šReACGANå’ŒSPADEã€‚ReACGANé€šè¿‡ä½¿ç”¨æ ‡ç­¾ä¿¡æ¯é€šè¿‡è¾…åŠ©åˆ†ç±»å™¨å¯¹ç”Ÿæˆè¿‡ç¨‹è¿›è¡Œæ¡ä»¶æ§åˆ¶ï¼Œç”Ÿæˆä¸ç‰¹å®šæ‰‹å½¢æ ‡ç­¾å¯¹é½çš„é€¼çœŸå›¾åƒï¼Œè€ŒSPADEåˆ™ä¾§é‡äºç”Ÿæˆå…·æœ‰å‡†ç¡®ç©ºé—´æ‰‹å½¢é…ç½®çš„å›¾åƒã€‚æ‰€æå‡ºçš„æŠ€æœ¯æé«˜äº†RWTHæ•°æ®é›†ä¸Šçš„æœ€æ–°å‡†ç¡®æ€§ï¼Œè§£å†³äº†æ•°æ®é›†å°ä¸”ä¸å‡è¡¡çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨åŸºäºå§¿æ€ç”Ÿæˆçš„HaGRIDæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå±•ç¤ºäº†è·¨ä¸åŒæ‰‹è¯­æ•°æ®é›†çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ— éœ€é‡æ–°è®­ç»ƒç”Ÿæˆå™¨å³å¯å®ç°ä¸å•æºè®­ç»ƒåˆ†ç±»å™¨ç›¸å½“çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰‹è¯­æ‰‹å½¢æ•°æ®é›†æ™®éå­˜åœ¨é™åˆ¶å’Œä¸å‡è¡¡é—®é¢˜ï¼Œå¯¹æ¨¡å‹è®­ç»ƒæ„æˆæŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ç”Ÿæˆåˆæˆæ•°æ®æ¥å¢å¼ºæ‰‹å½¢åˆ†ç±»å™¨çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>é‡‡ç”¨äº†EfficientNetåˆ†ç±»å™¨ï¼Œå¹¶åœ¨RWTHå¾·è¯­æ‰‹è¯­æ‰‹å½¢æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ¯”è¾ƒäº†ReACGANå’ŒSPADEä¸¤ç§GANæ¶æ„ï¼Œç”¨äºæ•°æ®ç”Ÿæˆã€‚</li>
<li>ReACGANèƒ½å¤Ÿç”Ÿæˆä¸ç‰¹å®šæ‰‹å½¢æ ‡ç­¾å¯¹é½çš„é€¼çœŸå›¾åƒï¼Œè€ŒSPADEå…³æ³¨ç”Ÿæˆå…·æœ‰å‡†ç¡®ç©ºé—´é…ç½®çš„æ‰‹å½¢å›¾åƒã€‚</li>
<li>æ‰€æå‡ºçš„æŠ€æœ¯æé«˜äº†RWTHæ•°æ®é›†ä¸Šçš„æœ€æ–°å‡†ç¡®æ€§ï¼Œå¹¶è§£å†³äº†æ•°æ®é›†çš„é™åˆ¶é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17008">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-40efb58181899b244fb2ad1e9aa6532b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c588f118b19b6656824a5edc6df51c9d.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="GeMix-Conditional-GAN-Based-Mixup-for-Improved-Medical-Image-Augmentation"><a href="#GeMix-Conditional-GAN-Based-Mixup-for-Improved-Medical-Image-Augmentation" class="headerlink" title="GeMix: Conditional GAN-Based Mixup for Improved Medical Image   Augmentation"></a>GeMix: Conditional GAN-Based Mixup for Improved Medical Image   Augmentation</h2><p><strong>Authors:Hugo Carlesso, Maria Eliza Patulea, Moncef Garouani, Radu Tudor Ionescu, Josiane Mothe</strong></p>
<p>Mixup has become a popular augmentation strategy for image classification, yet its naive pixel-wise interpolation often produces unrealistic images that can hinder learning, particularly in high-stakes medical applications. We propose GeMix, a two-stage framework that replaces heuristic blending with a learned, label-aware interpolation powered by class-conditional GANs. First, a StyleGAN2-ADA generator is trained on the target dataset. During augmentation, we sample two label vectors from Dirichlet priors biased toward different classes and blend them via a Beta-distributed coefficient. Then, we condition the generator on this soft label to synthesize visually coherent images that lie along a continuous class manifold. We benchmark GeMix on the large-scale COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101, EfficientNet-B0). When combined with real data, our method increases macro-F1 over traditional mixup for all backbones, reducing the false negative rate for COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup, delivering stronger regularization and greater semantic fidelity, without disrupting existing training pipelines. We publicly release our code at <a target="_blank" rel="noopener" href="https://github.com/hugocarlesso/GeMix">https://github.com/hugocarlesso/GeMix</a> to foster reproducibility and further research. </p>
<blockquote>
<p>Mixupå·²æˆä¸ºå›¾åƒåˆ†ç±»ä¸­æµè¡Œçš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œä½†å…¶ç®€å•çš„åƒç´ çº§æ’å€¼å¸¸å¸¸ä¼šäº§ç”Ÿä¸çœŸå®çš„å›¾åƒï¼Œä»è€Œé˜»ç¢å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜é£é™©çš„åŒ»ç–—åº”ç”¨ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†GeMixï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„æ¡†æ¶ï¼Œå®ƒç”¨åŸºäºç±»åˆ«æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„å­¦ä¹ æ„ŸçŸ¥æ’å€¼æ›¿æ¢äº†å¯å‘å¼æ··åˆæ–¹æ³•ã€‚é¦–å…ˆï¼Œåœ¨ç›®æ ‡æ•°æ®é›†ä¸Šè®­ç»ƒStyleGAN2-ADAç”Ÿæˆå™¨ã€‚åœ¨æ•°æ®å¢å¼ºè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»åå‘ä¸åŒç±»åˆ«çš„ç‹„åˆ©å…‹é›·å…ˆéªŒä¸­é‡‡æ ·ä¸¤ä¸ªæ ‡ç­¾å‘é‡ï¼Œå¹¶é€šè¿‡Betaåˆ†å¸ƒç³»æ•°å°†å®ƒä»¬æ··åˆã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨è¿™äº›è½¯æ ‡ç­¾ä¸Šè®¾ç½®ç”Ÿæˆå™¨æ¡ä»¶ï¼Œåˆæˆæ²¿è¿ç»­ç±»åˆ«æµå½¢çš„è§†è§‰ä¸Šè¿è´¯çš„å›¾åƒã€‚æˆ‘ä»¬åœ¨å¤§è§„æ¨¡çš„COVIDx-CT-3æ•°æ®é›†ä¸Šé‡‡ç”¨ä¸‰ç§ä¸»å¹²ç½‘ç»œï¼ˆResNet-50ã€ResNet-101ã€EfficientNet-B0ï¼‰å¯¹GeMixè¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚å½“ä¸çœŸå®æ•°æ®ç»“åˆæ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰ä¸»å¹²ç½‘ç»œä¸Šç›¸å¯¹äºä¼ ç»ŸMixupæé«˜äº†å®F1åˆ†æ•°ï¼Œå¹¶é™ä½äº†COVID-19æ£€æµ‹çš„è¯¯æŠ¥ç‡ã€‚å› æ­¤ï¼ŒGeMixå¯ä»¥ä½œä¸ºåƒç´ ç©ºé—´Mixupçš„æ›¿ä»£æ–¹æ¡ˆï¼Œæä¾›æ›´å¼ºå¤§çš„æ­£åˆ™åŒ–å’Œæ›´é«˜çš„è¯­ä¹‰ä¿çœŸåº¦ï¼ŒåŒæ—¶ä¸ä¼šå¹²æ‰°ç°æœ‰çš„è®­ç»ƒç®¡é“ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/hugocarlesso/GeMix%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%BB%A5%E4%BF%83%E8%BF%9B%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E5%92%8C%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%A0%94%E7%A9%B6%E3%80%82">https://github.com/hugocarlesso/GeMixå…¬å¼€å‘å¸ƒæˆ‘ä»¬çš„ä»£ç ï¼Œä»¥ä¿ƒè¿›å¯é‡å¤æ€§å’Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.15577v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ•°æ®å¢å¼ºç­–ç•¥Mixupåœ¨å›¾åƒåˆ†ç±»ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†å…¶ç®€å•çš„åƒç´ çº§æ’å€¼åœ¨é«˜é£é™©åŒ»ç–—åº”ç”¨ä¸­ä¼šäº§ç”Ÿä¸çœŸå®å›¾åƒï¼Œé˜»ç¢å­¦ä¹ ã€‚æå‡ºGeMixï¼Œä¸€ä¸ªåŸºäºç±»æ¡ä»¶GANçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œç”¨å­¦ä¹ åˆ°çš„æ ‡ç­¾æ„ŸçŸ¥æ’å€¼æ›¿æ¢å¯å‘å¼æ··åˆã€‚GeMixåœ¨å¤§å‹COVIDx-CT-3æ•°æ®é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œèƒ½æé«˜å®F1åˆ†æ•°ï¼Œé™ä½COVID-19æ£€æµ‹çš„è¯¯æŠ¥ç‡ã€‚GeMixå¯ä½œä¸ºåƒç´ ç©ºé—´Mixupçš„æ›¿ä»£å“ï¼Œæä¾›æ›´å¼ºå¤§çš„æ­£åˆ™åŒ–å’Œæ›´é«˜çš„è¯­ä¹‰ä¿çœŸåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Mixupåœ¨å›¾åƒåˆ†ç±»ä¸­å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨é«˜é£é™©åŒ»ç–—åº”ç”¨ä¸­äº§ç”Ÿä¸çœŸå®å›¾åƒã€‚</li>
<li>GeMixæ˜¯ä¸€ä¸ªåŸºäºç±»æ¡ä»¶GANçš„ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³Mixupçš„é—®é¢˜ã€‚</li>
<li>GeMixä½¿ç”¨StyleGAN2-ADAç”Ÿæˆå™¨è¿›è¡Œç›®æ ‡æ•°æ®é›†çš„è®­ç»ƒã€‚</li>
<li>GeMixé€šè¿‡é‡‡æ ·ä¸¤ä¸ªåå‘ä¸åŒç±»åˆ«çš„æ ‡ç­¾å‘é‡ï¼Œå¹¶ä½¿ç”¨Betaåˆ†å¸ƒç³»æ•°è¿›è¡Œæ··åˆï¼Œå®ç°æ ‡ç­¾æ„ŸçŸ¥æ’å€¼ã€‚</li>
<li>GeMixåˆæˆçš„å›¾åƒåœ¨è¿ç»­çš„ç±»åˆ«æµå½¢ä¸­æ˜¯è§†è§‰è¿è´¯çš„ã€‚</li>
<li>åœ¨å¤§å‹COVIDx-CT-3æ•°æ®é›†ä¸Šï¼ŒGeMixæé«˜äº†å®F1åˆ†æ•°ï¼Œé™ä½äº†COVID-19æ£€æµ‹çš„è¯¯æŠ¥ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.15577">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3c9ca6e24e4b0c7b658685d4cfb5b12e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-51fbb2895b1fc4e719f7f1afada9d889.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9ae986152fc258fc0bd1c5463b01c4ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3fb36e1dcfe1a28d91cc76e1334fc30.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Benchmarking-GANs-Diffusion-Models-and-Flow-Matching-for-T1w-to-T2w-MRI-Translation"><a href="#Benchmarking-GANs-Diffusion-Models-and-Flow-Matching-for-T1w-to-T2w-MRI-Translation" class="headerlink" title="Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w   MRI Translation"></a>Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w   MRI Translation</h2><p><strong>Authors:Andrea Moschetto, Lemuel Puglisi, Alec Sargood, Pierluigi Dellâ€™Acqua, Francesco Guarnera, Sebastiano Battiato, Daniele RavÃ¬</strong></p>
<p>Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering distinct diagnostic insights. However, acquiring all desired modalities increases scan time and cost, motivating research into computational methods for cross-modal synthesis. To address this, recent approaches aim to synthesize missing MRI contrasts from those already acquired, reducing acquisition time while preserving diagnostic quality. Image-to-image (I2I) translation provides a promising framework for this task. In this paper, we present a comprehensive benchmark of generative models$\unicode{x2013}$specifically, Generative Adversarial Networks (GANs), diffusion models, and flow matching (FM) techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All frameworks are implemented with comparable settings and evaluated on three publicly available MRI datasets of healthy adults. Our quantitative and qualitative analyses show that the GAN-based Pix2Pix model outperforms diffusion and FM-based methods in terms of structural fidelity, image quality, and computational efficiency. Consistent with existing literature, these results suggest that flow-based models are prone to overfitting on small datasets and simpler tasks, and may require more data to match or surpass GAN performance. These findings offer practical guidance for deploying I2I translation techniques in real-world MRI workflows and highlight promising directions for future research in cross-modal medical image synthesis. Code and models are publicly available at <a target="_blank" rel="noopener" href="https://github.com/AndreaMoschetto/medical-I2I-benchmark">https://github.com/AndreaMoschetto/medical-I2I-benchmark</a>. </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰èƒ½å¤Ÿè·å–å¤šç§å›¾åƒå¯¹æ¯”ï¼Œå¦‚T1åŠ æƒï¼ˆT1wï¼‰å’ŒT2åŠ æƒï¼ˆT2wï¼‰æ‰«æï¼Œæ¯ç§å¯¹æ¯”éƒ½æä¾›ç‹¬ç‰¹çš„è¯Šæ–­è§è§£ã€‚ç„¶è€Œï¼Œè·å–æ‰€æœ‰æƒ³è¦çš„æ¨¡æ€ä¼šå¢åŠ æ‰«ææ—¶é—´å’Œæˆæœ¬ï¼Œè¿™ä¿ƒä½¿ç ”ç©¶äººå‘˜ç ”ç©¶è·¨æ¨¡æ€åˆæˆè®¡ç®—çš„æ–¹æ³•ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¿‘æœŸçš„æ–¹æ³•æ—¨åœ¨ä»å·²è·å–çš„å›¾åƒä¸­åˆæˆç¼ºå¤±çš„MRIå¯¹æ¯”ï¼Œç¼©çŸ­é‡‡é›†æ—¶é—´åŒæ—¶ä¿ç•™è¯Šæ–­è´¨é‡ã€‚å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰è½¬æ¢ä¸ºæ­¤ä»»åŠ¡æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ¡†æ¶ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹ç”Ÿæˆæ¨¡å‹â€”â€”ç‰¹åˆ«æ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€æ‰©æ•£æ¨¡å‹å’ŒæµåŒ¹é…ï¼ˆFMï¼‰æŠ€æœ¯â€”â€”è¿›è¡Œäº†å…¨é¢çš„åŸºå‡†æµ‹è¯•ï¼Œç”¨äºT1wåˆ°T2wçš„2DMRI I2Iè½¬æ¢ã€‚æ‰€æœ‰æ¡†æ¶éƒ½åœ¨ç›¸ä¼¼çš„è®¾ç½®ä¸‹å®ç°ï¼Œå¹¶åœ¨ä¸‰ä¸ªå…¬å¼€çš„æˆäººMRIæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬çš„å®šé‡å’Œå®šæ€§åˆ†æè¡¨æ˜ï¼ŒåŸºäºGANçš„Pix2Pixæ¨¡å‹åœ¨ç»“æ„ä¿çœŸåº¦ã€å›¾åƒè´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢ä¼˜äºæ‰©æ•£å’ŒFMæ–¹æ³•ã€‚ä¸ç°æœ‰æ–‡çŒ®ä¸€è‡´ï¼Œè¿™äº›ç»“æœè¡¨æ˜åŸºäºæµçš„æ¨¡å‹æ˜“äºåœ¨å°æ•°æ®é›†å’Œç®€å•ä»»åŠ¡ä¸Šè¿‡åº¦æ‹Ÿåˆï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ•°æ®æ‰èƒ½åŒ¹é…æˆ–è¶…è¶ŠGANçš„æ€§èƒ½ã€‚è¿™äº›å‘ç°æä¾›äº†åœ¨å®é™…MRIå·¥ä½œæµç¨‹ä¸­éƒ¨ç½²I2Iè½¬æ¢æŠ€æœ¯çš„å®ç”¨æŒ‡å¯¼ï¼Œå¹¶çªå‡ºäº†è·¨æ¨¡æ€åŒ»å­¦å›¾åƒåˆæˆçš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚ç›¸å…³ä»£ç å’Œæ¨¡å‹å·²å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/AndreaMoschetto/medical-I2I-benchmark">https://github.com/AndreaMoschetto/medical-I2I-benchmark</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.14575v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡ç ”ç©¶äº†ä¸€ç§åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„äºŒç»´ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰å›¾åƒè½¬æ¢æŠ€æœ¯ã€‚ç ”ç©¶å¯¹ä¸‰ç§ç”Ÿæˆæ¨¡å‹ï¼ˆåŒ…æ‹¬GANã€æ‰©æ•£æ¨¡å‹å’ŒæµåŒ¹é…æŠ€æœ¯ï¼‰è¿›è¡Œäº†åŸºå‡†æµ‹è¯•è¯„ä¼°ï¼Œç”¨äºå°†T1åŠ æƒå›¾åƒè½¬æ¢ä¸ºT2åŠ æƒå›¾åƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPix2Pixæ¨¡å‹åœ¨ç»“æ„ä¿çœŸåº¦ã€å›¾åƒè´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢è¡¨ç°ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºåœ¨å®é™…MRIå·¥ä½œæµç¨‹ä¸­éƒ¨ç½²å›¾åƒåˆ°å›¾åƒï¼ˆI2Iï¼‰è½¬æ¢æŠ€æœ¯æä¾›äº†å®ç”¨æŒ‡å¯¼ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥è·¨æ¨¡æ€åŒ»å­¦å›¾åƒåˆæˆç ”ç©¶çš„å‰æ™¯æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æœ¬æ–‡å¯¹å¤šç§ç”Ÿæˆæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•è¯„ä¼°ï¼Œä»¥ç ”ç©¶äºŒç»´ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¸­çš„å›¾åƒè½¬æ¢æŠ€æœ¯ã€‚</li>
<li>ç ”ç©¶å…³æ³¨äºå°†T1åŠ æƒå›¾åƒè½¬æ¢ä¸ºT2åŠ æƒå›¾åƒï¼Œæ—¨åœ¨å‡å°‘æ‰«ææ—¶é—´å’Œæˆæœ¬ã€‚</li>
<li>GANçš„Pix2Pixæ¨¡å‹åœ¨ç»“æ„ä¿çœŸåº¦ã€å›¾åƒè´¨é‡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>æµåŒ¹é…æ¨¡å‹åœ¨å°æ•°æ®é›†å’Œç®€å•ä»»åŠ¡ä¸Šå®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆé—®é¢˜ã€‚</li>
<li>ç ”ç©¶ç»“æœæä¾›äº†åœ¨å®é™…MRIå·¥ä½œæµç¨‹ä¸­éƒ¨ç½²I2Iè½¬æ¢æŠ€æœ¯çš„å®ç”¨æŒ‡å¯¼ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.14575">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-89d8953dcb855b279c70a4700f534fd7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Converting-T1-weighted-MRI-from-3T-to-7T-quality-using-deep-learning"><a href="#Converting-T1-weighted-MRI-from-3T-to-7T-quality-using-deep-learning" class="headerlink" title="Converting T1-weighted MRI from 3T to 7T quality using deep learning"></a>Converting T1-weighted MRI from 3T to 7T quality using deep learning</h2><p><strong>Authors:Malo Gicquel, Ruoyi Zhao, Anika Wuestefeld, Nicola Spotorno, Olof Strandberg, Kalle Ã…strÃ¶m, Yu Xiao, Laura EM Wisse, Danielle van Westen, Rik Ossenkoppele, Niklas Mattsson-Carlgren, David Berron, Oskar Hansson, Gabrielle Flood, Jacob Vogel</strong></p>
<p>Ultra-high resolution 7 tesla (7T) magnetic resonance imaging (MRI) provides detailed anatomical views, offering better signal-to-noise ratio, resolution and tissue contrast than 3T MRI, though at the cost of accessibility. We present an advanced deep learning model for synthesizing 7T brain MRI from 3T brain MRI. Paired 7T and 3T T1-weighted images were acquired from 172 participants (124 cognitively unimpaired, 48 impaired) from the Swedish BioFINDER-2 study. To synthesize 7T MRI from 3T images, we trained two models: a specialized U-Net, and a U-Net integrated with a generative adversarial network (GAN U-Net). Our models outperformed two additional state-of-the-art 3T-to-7T models in image-based evaluation metrics. Four blinded MRI professionals judged our synthetic 7T images as comparable in detail to real 7T images, and superior in subjective visual quality to 7T images, apparently due to the reduction of artifacts. Importantly, automated segmentations of the amygdalae of synthetic GAN U-Net 7T images were more similar to manually segmented amygdalae (n&#x3D;20), than automated segmentations from the 3T images that were used to synthesize the 7T images. Finally, synthetic 7T images showed similar performance to real 3T images in downstream prediction of cognitive status using MRI derivatives (n&#x3D;3,168). In all, we show that synthetic T1-weighted brain images approaching 7T quality can be generated from 3T images, which may improve image quality and segmentation, without compromising performance in downstream tasks. Future directions, possible clinical use cases, and limitations are discussed. </p>
<blockquote>
<p>è¶…é«˜åˆ†è¾¨ç‡7ç‰¹æ–¯æ‹‰ï¼ˆ7Tï¼‰ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰æä¾›äº†è¯¦ç»†çš„è§£å‰–è§†å›¾ï¼Œç›¸è¾ƒäº3T MRIï¼Œå…¶æ‹¥æœ‰æ›´å¥½çš„ä¿¡å™ªæ¯”ã€åˆ†è¾¨ç‡å’Œç»„ç»‡å¯¹æ¯”åº¦ã€‚ç„¶è€Œï¼Œè¿™ä¸€åˆ‡æ˜¯ä»¥å¯åŠæ€§ä½œä¸ºä»£ä»·çš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤Ÿä»3Tå¤§è„‘MRIåˆæˆ7Tå¤§è„‘MRIã€‚æˆ‘ä»¬ä»ç‘å…¸BioFINDER-2ç ”ç©¶ä¸­è·å¾—äº†é…å¯¹çš„7Tå’Œ3TT1åŠ æƒå›¾åƒï¼Œå…±æ¶‰åŠå‚ä¸è€…172äººï¼ˆå…¶ä¸­è®¤çŸ¥åŠŸèƒ½æ­£å¸¸è€…124äººï¼Œå—æŸè€…48äººï¼‰ã€‚ä¸ºäº†ä»3Tå›¾åƒåˆæˆ7T MRIï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸¤ç§æ¨¡å‹ï¼šä¸€ç§ä¸“ç”¨çš„U-Netæ¨¡å‹ï¼Œä»¥åŠç»“åˆäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGAN U-Netï¼‰çš„U-Netæ¨¡å‹ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨åŸºäºå›¾åƒçš„è¯„ä»·æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜äºå…¶ä»–ä¸¤ç§å…ˆè¿›çš„3Tåˆ°7Tçš„æ¨¡å‹ã€‚å››åç›²æ€MRIä¸“ä¸šäººå£«è®¤ä¸ºæˆ‘ä»¬çš„åˆæˆ7Tå›¾åƒä¸çœŸå®7Tå›¾åƒåœ¨ç»†èŠ‚ä¸Šå…·æœ‰å¯æ¯”è¾ƒæ€§ï¼Œåœ¨ä¸»è§‚è§†è§‰è´¨é‡ä¸Šä¼˜äºçœŸå®7Tå›¾åƒï¼Œæ˜¾ç„¶æ˜¯å› ä¸ºä¼ªå½±çš„å‡å°‘ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåˆæˆGAN U-Net 7Tå›¾åƒçš„æä»æ ¸è‡ªåŠ¨åŒ–åˆ†å‰²ä¸æ‰‹åŠ¨åˆ†å‰²çš„æä»æ ¸æ›´ä¸ºç›¸ä¼¼ï¼ˆn&#x3D;20ï¼‰ï¼Œç›¸è¾ƒäºç”¨äºåˆæˆ7Tå›¾åƒçš„åŸå§‹3Tå›¾åƒè‡ªåŠ¨åŒ–åˆ†å‰²ç»“æœæ›´ä¸ºå‡†ç¡®ã€‚æ­¤å¤–ï¼Œåˆæˆ7Tå›¾åƒåœ¨åˆ©ç”¨MRIè¡ç”Ÿçš„ä¸‹æ¸¸è®¤çŸ¥çŠ¶æ€é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†ä¸çœŸå®3Tå›¾åƒç›¸ä¼¼çš„æ€§èƒ½ï¼ˆn&#x3D;3,168ï¼‰ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬è¯æ˜äº†ä»3Tå›¾åƒå¯ä»¥ç”Ÿæˆæ¥è¿‘7Tè´¨é‡çš„åˆæˆT1åŠ æƒå¤§è„‘å›¾åƒï¼Œè¿™å¯ä»¥æé«˜å›¾åƒè´¨é‡å’Œåˆ†å‰²æ•ˆæœï¼ŒåŒæ—¶ä¸ä¼šæŸå®³ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†æœªæ¥æ–¹å‘ã€å¯èƒ½çš„ä¸´åºŠåº”ç”¨åœºæ™¯å’Œå±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13782v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ç»“åˆäº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„U-Netæ¨¡å‹ï¼ŒæˆåŠŸä»3T MRIåˆæˆ7T MRIå›¾åƒã€‚åˆæˆå›¾åƒåœ¨ç»†èŠ‚å’Œä¸»è§‚è§†è§‰è´¨é‡ä¸Šè¡¨ç°å‡ºä¼˜è¶Šæ€§ï¼Œä¸”åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œæœ‰æœ›æ”¹å–„å›¾åƒè´¨é‡å’Œåˆ†å‰²æ•ˆæœï¼ŒåŒæ—¶ä¸æŸå®³ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åˆ©ç”¨å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æˆåŠŸåˆæˆæ¥è¿‘7Tåˆ†è¾¨ç‡çš„è„‘MRIå›¾åƒã€‚</li>
<li>åˆæˆå›¾åƒåœ¨ç»†èŠ‚å’Œä¸»è§‚è§†è§‰è´¨é‡ä¸Šä¼˜äºçœŸå®7Tå›¾åƒã€‚</li>
<li>åˆæˆå›¾åƒå‡å°‘äº†ä¼ªå½±ï¼Œæé«˜äº†å›¾åƒè´¨é‡ã€‚</li>
<li>åˆæˆå›¾åƒçš„æä»æ ¸è‡ªåŠ¨åŒ–åˆ†å‰²ç»“æœä¸æ‰‹åŠ¨åˆ†å‰²ç»“æœæ›´ä¸ºç›¸ä¼¼ã€‚</li>
<li>åˆæˆ7Tå›¾åƒåœ¨ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚è®¤çŸ¥çŠ¶æ€é¢„æµ‹ï¼‰ä¸­çš„è¡¨ç°ä¸çœŸå®3Tå›¾åƒç›¸ä¼¼ã€‚</li>
<li>è¯¥æŠ€æœ¯å¯èƒ½åœ¨ä¸æŸå®³ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„æƒ…å†µä¸‹æ”¹å–„å›¾åƒè´¨é‡å’Œåˆ†å‰²æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13782">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-8a05b4b4797e0a9a60d488906f6d55db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-971adba9b575ebeb149a1ca15fbc3421.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Multi-population-GAN-Training-Analyzing-Co-Evolutionary-Algorithms"><a href="#Multi-population-GAN-Training-Analyzing-Co-Evolutionary-Algorithms" class="headerlink" title="Multi-population GAN Training: Analyzing Co-Evolutionary Algorithms"></a>Multi-population GAN Training: Analyzing Co-Evolutionary Algorithms</h2><p><strong>Authors:Walter P. Casas, Jamal Toutouh</strong></p>
<p>Generative adversarial networks (GANs) are powerful generative models but remain challenging to train due to pathologies suchas mode collapse and instability. Recent research has explored co-evolutionary approaches, in which populations of generators and discriminators are evolved, as a promising solution. This paper presents an empirical analysis of different coevolutionary GAN training strategies, focusing on the impact of selection and replacement mechanisms. We compare (mu,lambda), (mu+lambda) with elitism, and (mu+lambda) with tournament selection coevolutionary schemes, along with a non-evolutionary population based multi-generator multi-discriminator GAN baseline, across both synthetic low-dimensional datasets (blob and gaussian mixtures) and an image-based benchmark (MNIST). Results show that full generational replacement, i.e., (mu,lambda), consistently outperforms in terms of both sample quality and diversity, particularly when combined with larger offspring sizes. In contrast, elitist approaches tend to converge prematurely and suffer from reduced diversity. These findings highlight the importance of balancing exploration and exploitation dynamics in coevolutionary GAN training and provide guidance for designing more effective population-based generative models. </p>
<blockquote>
<p>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œä½†ç”±äºæ¨¡å¼å´©æºƒå’Œä¸ç¨³å®šç­‰ç—…ç†é—®é¢˜ï¼Œå…¶è®­ç»ƒä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ€è¿‘çš„ç ”ç©¶æ¢ç´¢äº†ååŒè¿›åŒ–æ–¹æ³•ï¼Œå…¶ä¸­ç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨çš„ç§ç¾¤è¢«å…±åŒè¿›åŒ–ï¼Œä½œä¸ºä¸€ç§å‰æ™¯å¹¿é˜”çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡å¯¹ä¸åŒååŒè¿›åŒ–GANè®­ç»ƒç­–ç•¥è¿›è¡Œäº†å®è¯åˆ†æï¼Œé‡ç‚¹ç ”ç©¶äº†é€‰æ‹©æœºåˆ¶å’Œæ›¿æ¢æœºåˆ¶çš„å½±å“ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ï¼ˆmuï¼Œlambdaï¼‰ã€ï¼ˆmu+lambdaï¼‰ç²¾è‹±ä¸»ä¹‰å’Œï¼ˆmu+lambdaï¼‰é”¦æ ‡èµ›é€‰æ‹”ååŒè¿›åŒ–æ–¹æ¡ˆï¼Œä»¥åŠåŸºäºéè¿›åŒ–ç§ç¾¤çš„å¤šç”Ÿæˆå™¨å¤šé‰´åˆ«å™¨GANåŸºçº¿ï¼Œæ—¢åŒ…æ‹¬åˆæˆä½ç»´æ•°æ®é›†ï¼ˆblobå’Œé«˜æ–¯æ··åˆï¼‰ä¹ŸåŒ…æ‹¬åŸºäºå›¾åƒçš„åŸºå‡†æµ‹è¯•ï¼ˆMNISTï¼‰ã€‚ç»“æœè¡¨æ˜ï¼Œå®Œå…¨çš„ä¸–ä»£æ›´æ›¿ï¼Œå³ï¼ˆmuï¼Œlambdaï¼‰ï¼Œåœ¨æ ·æœ¬è´¨é‡å’Œå¤šæ ·æ€§æ–¹é¢å§‹ç»ˆè¡¨ç°æ›´å¥½ï¼Œå°¤å…¶æ˜¯ä¸æ›´å¤§çš„åä»£è§„æ¨¡ç›¸ç»“åˆæ—¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç²¾è‹±ä¸»ä¹‰æ–¹æ³•å¾€å¾€è¿‡æ—©æ”¶æ•›ï¼Œä¸”å¤šæ ·æ€§é™ä½ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¹³è¡¡ååŒè¿›åŒ–GANè®­ç»ƒä¸­çš„æ¢ç´¢å’Œåˆ©ç”¨åŠ¨æ€çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºè®¾è®¡æ›´æœ‰æ•ˆçš„åŸºäºç§ç¾¤çš„ç”Ÿæˆæ¨¡å‹æä¾›äº†æŒ‡å¯¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13157v1">PDF</a> Genetic and Evolutionary Computation Conference (GECCO â€˜25   Companion), July 14â€“18, 2025, Malaga, Spain</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„è®­ç»ƒç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯å…±è¿›åŒ–æ–¹æ³•ä¸­çš„é€‰æ‹©æœºåˆ¶å’Œæ›¿ä»£æœºåˆ¶å¯¹GANè®­ç»ƒçš„å½±å“ã€‚é€šè¿‡å¯¹ä¸åŒå…±è¿›åŒ–GANè®­ç»ƒç­–ç•¥è¿›è¡Œå®è¯ç ”ç©¶ï¼ŒåŒ…æ‹¬ï¼ˆmuï¼Œlambdaï¼‰ã€ï¼ˆmu+lambdaï¼‰ç²¾è‹±ä¸»ä¹‰å’Œï¼ˆmu+lambdaï¼‰é”¦æ ‡èµ›é€‰æ‹”ç­‰æ–¹æ¡ˆï¼Œä»¥åŠåŸºäºéè¿›åŒ–ç§ç¾¤çš„å¤šç”Ÿæˆå™¨å¤šé‰´åˆ«å™¨GANåŸºçº¿ï¼Œåœ¨åˆæˆä½ç»´æ•°æ®é›†ï¼ˆblobå’Œé«˜æ–¯æ··åˆï¼‰å’Œå›¾åƒåŸºå‡†æµ‹è¯•é›†ï¼ˆMNISTï¼‰ä¸Šçš„è¡¨ç°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå®Œå…¨ä¸–ä»£æ›¿ä»£ç­–ç•¥ï¼ˆå³ï¼ˆmuï¼Œlambdaï¼‰ï¼‰åœ¨æ ·æœ¬è´¨é‡å’Œå¤šæ ·æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šï¼Œå°¤å…¶æ˜¯ç»“åˆè¾ƒå¤§çš„å­ä»£è§„æ¨¡æ—¶è¡¨ç°æ›´å¥½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç²¾è‹±ç­–ç•¥è¶‹äºè¿‡æ—©æ”¶æ•›å¹¶è¡¨ç°å‡ºè¾ƒä½çš„å¤šæ ·æ€§ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨åŠ¨åŠ›å­¦åœ¨å…±è¿›åŒ–GANè®­ç»ƒä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºè®¾è®¡æ›´æœ‰æ•ˆçš„åŸºäºç§ç¾¤çš„ç”Ÿæˆæ¨¡å‹æä¾›äº†æŒ‡å¯¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„è®­ç»ƒå­˜åœ¨æ¨¡å¼å´©æºƒå’Œä¸ç¨³å®šç­‰é—®é¢˜ã€‚</li>
<li>å…±è¿›åŒ–æ–¹æ³•æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³ç­–ç•¥ï¼Œæ¶‰åŠç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨çš„ç§ç¾¤æ¼”åŒ–ã€‚</li>
<li>å®è¯åˆ†æäº†ä¸åŒå…±è¿›åŒ–GANè®­ç»ƒç­–ç•¥ï¼Œé‡ç‚¹ç ”ç©¶äº†é€‰æ‹©æœºåˆ¶å’Œæ›¿ä»£æœºåˆ¶çš„å½±å“ã€‚</li>
<li>å®Œå…¨ä¸–ä»£æ›¿ä»£ç­–ç•¥ï¼ˆmuï¼Œlambdaï¼‰åœ¨æ ·æœ¬è´¨é‡å’Œå¤šæ ·æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>ç²¾è‹±ç­–ç•¥å¯èƒ½è¿‡æ—©æ”¶æ•›å¹¶è¡¨ç°å‡ºè¾ƒä½çš„å¤šæ ·æ€§ã€‚</li>
<li>å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨åŠ¨åŠ›å­¦åœ¨å…±è¿›åŒ–GANè®­ç»ƒä¸­è‡³å…³é‡è¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13157">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-88300f4d2982b82aaa8ebe00044058ab.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f576f22c5bf9b758a7d0faf8c9f1487.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ca4f56d0cb291a6eab15c39694c39984.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e98ff6f298d118313054ad6b29d00cb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cec988a87d356c868906450098c51cfc.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="fastWDM3D-Fast-and-Accurate-3D-Healthy-Tissue-Inpainting"><a href="#fastWDM3D-Fast-and-Accurate-3D-Healthy-Tissue-Inpainting" class="headerlink" title="fastWDM3D: Fast and Accurate 3D Healthy Tissue Inpainting"></a>fastWDM3D: Fast and Accurate 3D Healthy Tissue Inpainting</h2><p><strong>Authors:Alicia Durrer, Florentin Bieder, Paul Friedrich, Bjoern Menze, Philippe C. Cattin, Florian Kofler</strong></p>
<p>Healthy tissue inpainting has significant applications, including the generation of pseudo-healthy baselines for tumor growth models and the facilitation of image registration. In previous editions of the BraTS Local Synthesis of Healthy Brain Tissue via Inpainting Challenge, denoising diffusion probabilistic models (DDPMs) demonstrated qualitatively convincing results but suffered from low sampling speed. To mitigate this limitation, we adapted a 2D image generation approach, combining DDPMs with generative adversarial networks (GANs) and employing a variance-preserving noise schedule, for the task of 3D inpainting. Our experiments showed that the variance-preserving noise schedule and the selected reconstruction losses can be effectively utilized for high-quality 3D inpainting in a few time steps without requiring adversarial training. We applied our findings to a different architecture, a 3D wavelet diffusion model (WDM3D) that does not include a GAN component. The resulting model, denoted as fastWDM3D, obtained a SSIM of 0.8571, a MSE of 0.0079, and a PSNR of 22.26 on the BraTS inpainting test set. Remarkably, it achieved these scores using only two time steps, completing the 3D inpainting process in 1.81 s per image. When compared to other DDPMs used for healthy brain tissue inpainting, our model is up to 800 x faster while still achieving superior performance metrics. Our proposed method, fastWDM3D, represents a promising approach for fast and accurate healthy tissue inpainting. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/AliciaDurrer/fastWDM3D">https://github.com/AliciaDurrer/fastWDM3D</a>. </p>
<blockquote>
<p>å¥åº·ç»„ç»‡çš„è¡¥å…¨æŠ€æœ¯æ‹¥æœ‰ä¼—å¤šé‡è¦åº”ç”¨ï¼ŒåŒ…æ‹¬ä¸ºè‚¿ç˜¤å¢é•¿æ¨¡å‹ç”Ÿæˆä¼ªå¥åº·åŸºå‡†çº¿ä»¥åŠä¿ƒè¿›å›¾åƒé…å‡†ã€‚åœ¨ä¹‹å‰ä¸¾åŠçš„Brain Tumor Segmentation using GANï¼ˆåŸºäºGANçš„è„‘è‚¿ç˜¤åˆ†å‰²æŒ‘æˆ˜èµ›ï¼‰ä¸­ï¼Œé™å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰åœ¨å®šæ€§è¯„ä¼°ä¸Šå–å¾—äº†ä»¤äººä¿¡æœçš„ç»“æœï¼Œä½†é‡‡æ ·é€Ÿåº¦è¾ƒæ…¢ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†äºŒç»´å›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œç»“åˆäº†DDPMä¸ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼Œå¹¶ä½¿ç”¨ä¿æ–¹å·®å™ªå£°è°ƒåº¦æ–¹æ¡ˆï¼Œç”¨äºä¸‰ç»´è¡¥å…¨ä»»åŠ¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¿æ–¹å·®å™ªå£°è°ƒåº¦æ–¹æ¡ˆå’Œæ‰€é€‰çš„é‡å»ºæŸå¤±å¯ä»¥åœ¨å‡ ä¸ªæ—¶é—´æ­¥éª¤å†…æœ‰æ•ˆåœ°ç”¨äºé«˜è´¨é‡çš„ä¸‰ç»´è¡¥å…¨ï¼Œæ— éœ€å¯¹æŠ—è®­ç»ƒã€‚æˆ‘ä»¬å°†å‘ç°åº”ç”¨äºä¸åŒçš„æ¶æ„â€”â€”ä¸‰ç»´å°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDM3Dï¼‰ï¼Œä¸åŒ…æ‹¬GANç»„ä»¶ã€‚å› æ­¤å¾—åˆ°çš„æ¨¡å‹ï¼Œè¢«ç§°ä¸ºfastWDM3Dï¼Œåœ¨BraTSè¡¥å…¨æµ‹è¯•é›†ä¸Šå–å¾—äº†ç»“æ„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆSSIMï¼‰ä¸º0.8571ã€å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä¸º0.0079ä»¥åŠå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ä¸º22.26çš„æˆç»©ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒä»…ä½¿ç”¨ä¸¤ä¸ªæ—¶é—´æ­¥éª¤å°±å–å¾—äº†è¿™äº›æˆç»©ï¼Œå®Œæˆä¸‰ç»´è¡¥å…¨è¿‡ç¨‹çš„æ¯å¼ å›¾åƒä»…éœ€1.81ç§’ã€‚ä¸å…¶ä»–ç”¨äºå¥åº·è„‘ç»„ç»‡è¡¥å…¨çš„DDPMæ¨¡å‹ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é€Ÿåº¦æé«˜äº†é«˜è¾¾800å€ï¼ŒåŒæ—¶ä»å®ç°äº†å“è¶Šçš„æ€§èƒ½æŒ‡æ ‡ã€‚æˆ‘ä»¬æå‡ºçš„fastWDM3Dæ–¹æ³•ä»£è¡¨ç€ä¸€ç§å¿«é€Ÿå‡†ç¡®å¥åº·ç»„ç»‡è¡¥å…¨çš„å¾ˆæœ‰å‰é€”çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/AliciaDurrer/fastWDM3D%E3%80%82">https://github.com/AliciaDurrer/fastWDM3Dã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.13146v1">PDF</a> Philippe C. Cattin and Florian Kofler: equal contribution</p>
<p><strong>Summary</strong></p>
<p>åœ¨BraTSå¥åº·è„‘ç»„ç»‡åˆæˆæŒ‘æˆ˜ä¸­ï¼Œä¸ºäº†åŠ å¿«3Dè¡¥å…¨é€Ÿåº¦ï¼Œç ”ç©¶è€…ç»“åˆå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMsï¼‰å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ï¼Œå¹¶è¿ç”¨ä¿æ–¹å·®å™ªå£°æ—¶é—´è¡¨è¿›è¡Œæœ‰æ•ˆæ”¹è¿›ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„é«˜æ•ˆæ€§ï¼ŒåŒæ—¶æå‡ºäº†åº”ç”¨äºä¸å«GANç»„ä»¶çš„3Då°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDM3Dï¼‰çš„æ–°æ¶æ„ã€‚æ–°çš„æ¨¡å‹ç§°ä¸ºfastWDM3Dï¼Œä½¿ç”¨ä»…ä¸¤æ­¥æ—¶é—´å³å¯å®ç°é«˜è´¨é‡3Dè¡¥å…¨ï¼Œä¸”åœ¨BraTSè¡¥å…¨æµ‹è¯•é›†ä¸Šå®ç°äº†SSIM 0.8571ã€MSE 0.0079åŠPSNR 22.26çš„å¾—åˆ†ï¼Œæ¯å›¾åƒä»…éœ€1.81ç§’å®Œæˆè¡¥å…¨ã€‚ä¸å…¶ä»–ç”¨äºå¥åº·è„‘ç»„ç»‡è¡¥å…¨çš„DDPMsç›¸æ¯”ï¼Œè¯¥æ¨¡å‹é€Ÿåº¦æé«˜äº†é«˜è¾¾800å€ï¼ŒåŒæ—¶æ€§èƒ½å“è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶è€…é’ˆå¯¹å¥åº·ç»„ç»‡è¡¥å…¨ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨è‚¿ç˜¤ç”Ÿé•¿æ¨¡å‹å’Œå›¾åƒæ³¨å†Œä¸­ç”Ÿæˆä¼ªå¥åº·åŸºçº¿åº”ç”¨çš„é‡è¦æ€§è¿›è¡Œäº†æ¢è®¨ã€‚</li>
<li>é’ˆå¯¹ä¹‹å‰DDPMsåœ¨è¡¥å…¨ä»»åŠ¡ä¸­çš„ä½é‡‡æ ·é€Ÿåº¦é—®é¢˜ï¼Œç ”ç©¶è€…ç»“åˆäº†DDPMså’ŒGANsï¼Œå¹¶é‡‡ç”¨ä¿æ–¹å·®å™ªå£°æ—¶é—´è¡¨è¿›è¡Œæ”¹è¿›ã€‚</li>
<li>å®éªŒè¯æ˜ä¿æ–¹å·®å™ªå£°æ—¶é—´è¡¨åŠæ‰€é€‰é‡å»ºæŸå¤±å¯æœ‰æ•ˆç”¨äºé«˜è´¨é‡3Dè¡¥å…¨ï¼Œä¸”ä»…éœ€è¦å°‘æ•°æ—¶é—´æ­¥éª¤ã€‚</li>
<li>ç ”ç©¶è€…å°†è¯¥æ–¹æ³•åº”ç”¨äºä¸å«GANç»„ä»¶çš„3Då°æ³¢æ‰©æ•£æ¨¡å‹ï¼ˆWDM3Dï¼‰ï¼Œæå‡ºåä¸ºfastWDM3Dçš„æ–°æ¨¡å‹æ¶æ„ã€‚</li>
<li>fastWDM3Dæ¨¡å‹åœ¨BraTSè¡¥å…¨æµ‹è¯•é›†ä¸Šå®ç°äº†è¾ƒé«˜çš„SSIMã€MSEå’ŒPSNRå¾—åˆ†ï¼Œæ˜¾ç¤ºå‡ºå…¶ä¼˜è¶Šæ€§èƒ½ã€‚</li>
<li>ä¸å…¶ä»–DDPMsç›¸æ¯”ï¼ŒfastWDM3Dæ¨¡å‹é€Ÿåº¦æ˜¾è‘—æé«˜ï¼Œè¾¾åˆ°800å€çš„é€Ÿåº¦æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.13146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ee1c90fd875b4759f98c8a5cf59f063a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abc5cbdfae5672832000498581ce4ca0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1e5491b3845ab924ba393002c6083d8a.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Distributed-Generative-AI-Approach-for-Heterogeneous-Multi-Domain-Environments-under-Data-Sharing-constraints"><a href="#A-Distributed-Generative-AI-Approach-for-Heterogeneous-Multi-Domain-Environments-under-Data-Sharing-constraints" class="headerlink" title="A Distributed Generative AI Approach for Heterogeneous Multi-Domain   Environments under Data Sharing constraints"></a>A Distributed Generative AI Approach for Heterogeneous Multi-Domain   Environments under Data Sharing constraints</h2><p><strong>Authors:Youssef Tawfilis, Hossam Amer, Minar El-Aasser, Tallal Elshabrawy</strong></p>
<p>Federated Learning has gained increasing attention for its ability to enable multiple nodes to collaboratively train machine learning models without sharing their raw data. At the same time, Generative AI â€“ particularly Generative Adversarial Networks (GANs) â€“ have achieved remarkable success across a wide range of domains, such as healthcare, security, and Image Generation. However, training generative models typically requires large datasets and significant computational resources, which are often unavailable in real-world settings. Acquiring such resources can be costly and inefficient, especially when many underutilized devices â€“ such as IoT devices and edge devices â€“ with varying capabilities remain idle. Moreover, obtaining large datasets is challenging due to privacy concerns and copyright restrictions, as most devices are unwilling to share their data. To address these challenges, we propose a novel approach for decentralized GAN training that enables the utilization of distributed data and underutilized, low-capability devices while not sharing data in its raw form. Our approach is designed to tackle key challenges in decentralized environments, combining KLD-weighted Clustered Federated Learning to address the issues of data heterogeneity and multi-domain datasets, with Heterogeneous U-Shaped split learning to tackle the challenge of device heterogeneity under strict data sharing constraints â€“ ensuring that no labels or raw data, whether real or synthetic, are ever shared between nodes. Experimental results shows that our approach demonstrates consistent and significant improvements across key performance metrics, where it achieves 1.1x â€“ 2.2x higher image generation scores, an average 10% boost in classification metrics (up to 50% in multi-domain non-IID settings), in much lower latency compared to several benchmarks. Find our code at <a target="_blank" rel="noopener" href="https://github.com/youssefga28/HuSCF-GAN">https://github.com/youssefga28/HuSCF-GAN</a>. </p>
<blockquote>
<p>è”é‚¦å­¦ä¹ å› å…¶èƒ½å¤Ÿä½¿å¤šä¸ªèŠ‚ç‚¹åœ¨ä¸å…±äº«åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹ååŒè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹çš„èƒ½åŠ›è€Œå—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚ä¸æ­¤åŒæ—¶ï¼Œç”Ÿæˆå¼äººå·¥æ™ºèƒ½â€”â€”ç‰¹åˆ«æ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰åœ¨åŒ»ç–—ä¿å¥ã€å®‰å…¨ã€å›¾åƒç”Ÿæˆç­‰å¤šä¸ªé¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆå°±ã€‚ç„¶è€Œï¼Œè®­ç»ƒç”Ÿæˆæ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®é›†å’Œè®¡ç®—èµ„æºï¼Œè¿™åœ¨ç°å®ä¸–ç•Œä¸­å¾€å¾€æ— æ³•è·å¾—ã€‚è·å–æ­¤ç±»èµ„æºå¯èƒ½æˆæœ¬é«˜æ˜‚ä¸”æ•ˆç‡ä½ä¸‹ï¼Œå°¤å…¶æ˜¯å½“è®¸å¤šä½¿ç”¨ä¸è¶³çš„è®¾å¤‡ï¼ˆå¦‚ç‰©è”ç½‘è®¾å¤‡å’Œè¾¹ç¼˜è®¾å¤‡ï¼‰å…·æœ‰ä¸åŒçš„èƒ½åŠ›å¹¶ä¿æŒé—²ç½®çŠ¶æ€æ—¶ã€‚æ­¤å¤–ï¼Œç”±äºéšç§æ‹…å¿§å’Œç‰ˆæƒé™åˆ¶ï¼Œè·å¾—å¤§é‡æ•°æ®é›†å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå¤§å¤šæ•°è®¾å¤‡ä¸æ„¿æ„å…±äº«å…¶æ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åˆ†å¸ƒå¼GANè®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸å…±äº«æ•°æ®åŸå§‹å½¢å¼çš„æƒ…å†µä¸‹åˆ©ç”¨åˆ†å¸ƒå¼æ•°æ®å’Œåˆ©ç”¨ä¸è¶³çš„ä½ç«¯è®¾å¤‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ—¨åœ¨è§£å†³åˆ†å¸ƒå¼ç¯å¢ƒä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œç»“åˆKLDåŠ æƒèšç±»è”é‚¦å­¦ä¹ æ¥è§£å†³æ•°æ®å¼‚è´¨æ€§å’Œå¤šåŸŸæ•°æ®é›†çš„é—®é¢˜ï¼Œä»¥åŠå¼‚è´¨Uå½¢åˆ†å‰²å­¦ä¹ æ¥è§£å†³ä¸¥æ ¼æ•°æ®å…±äº«çº¦æŸä¸‹çš„è®¾å¤‡å¼‚è´¨æ€§çš„æŒ‘æˆ˜â€”â€”ç¡®ä¿èŠ‚ç‚¹ä¹‹é—´ä¸å…±äº«ä»»ä½•çœŸå®æˆ–åˆæˆçš„æ ‡ç­¾æˆ–åŸå§‹æ•°æ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…³é”®æ€§èƒ½æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºæŒç»­è€Œæ˜¾è‘—çš„æ”¹è¿›ï¼Œå…¶ä¸­å›¾åƒç”Ÿæˆå¾—åˆ†æé«˜äº†1.1å€è‡³2.2å€ï¼Œåˆ†ç±»æŒ‡æ ‡å¹³å‡æé«˜äº†10%ï¼ˆåœ¨å¤šåŸŸéç‹¬ç«‹åŒåˆ†å¸ƒè®¾ç½®ä¸­æœ€é«˜å¯è¾¾50%ï¼‰ï¼Œå¹¶ä¸”å»¶è¿Ÿæ—¶é—´æ›´ä½ï¼Œè¶…è¿‡äº†å¤šä¸ªåŸºå‡†æµ‹è¯•ã€‚æ‚¨å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/youssefga28/HuSCF-GAN">https://github.com/youssefga28/HuSCF-GAN</a>æ‰¾åˆ°æˆ‘ä»¬çš„ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12979v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŸºäºè”é‚¦å­¦ä¹ çš„æ–¹æ³•å®ç°äº†æ— éœ€åˆ†äº«åŸå§‹æ•°æ®çš„å¤šä¸ªèŠ‚ç‚¹ååŒè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹çš„èƒ½åŠ›ã€‚é’ˆå¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆç‰¹åˆ«æ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œGANsï¼‰è®­ç»ƒéœ€è¦å¤§é‡æ•°æ®å’Œè®¡ç®—èµ„æºçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çš„å»ä¸­å¿ƒåŒ–GANè®­ç»ƒæ–¹æ³•ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åˆ†äº«åŸå§‹æ•°æ®çš„å‰æä¸‹ï¼Œåˆ©ç”¨åˆ†æ•£çš„æ•°æ®å’Œé—²ç½®çš„ä½æ€§èƒ½è®¾å¤‡ã€‚è¯¥æ–¹æ³•é€šè¿‡KLDåŠ æƒèšç±»è”é‚¦å­¦ä¹ è§£å†³æ•°æ®å¤šæ ·æ€§å’Œå¤šåŸŸæ•°æ®é›†é—®é¢˜ï¼Œé€šè¿‡å¼‚è´¨Uå‹åˆ†å‰²å­¦ä¹ è§£å†³è®¾å¤‡å·®å¼‚æ€§çš„ä¸¥æ ¼æ•°æ®å…±äº«çº¦æŸé—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å…³é”®æ€§èƒ½æŒ‡æ ‡ä¸ŠæŒç»­ä¸”æ˜¾è‘—æé«˜ï¼Œå›¾åƒç”Ÿæˆåˆ†æ•°æé«˜1.1å€è‡³2.2å€ï¼Œåˆ†ç±»æŒ‡æ ‡å¹³å‡æå‡10%ï¼ˆåœ¨å¤šåŸŸéç‹¬ç«‹åŒåˆ†å¸ƒç¯å¢ƒä¸­æœ€é«˜æå‡50%ï¼‰ï¼Œä¸”å»¶è¿Ÿæ›´ä½ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/youssefga28/HuSCF-GAN">https://github.com/youssefga28/HuSCF-GAN</a> è·å–ã€‚</p>
<p><strong>å…³é”®æ”¶è·ç‚¹</strong></p>
<ol>
<li>è”é‚¦å­¦ä¹ å¯ä»¥å…è®¸å¤šä¸ªèŠ‚ç‚¹ååŒè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹è€Œä¸ç›´æ¥åˆ†äº«åŸå§‹æ•°æ®ã€‚</li>
<li>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰åœ¨å„ä¸ªé¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€å®‰å…¨ã€å›¾åƒç”Ÿæˆç­‰ï¼‰å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚</li>
<li>GANè®­ç»ƒéœ€è¦å¤§é‡çš„æ•°æ®é›†å’Œè®¡ç®—èµ„æºï¼Œè¿™åœ¨ç°å®ä¸–ç•Œä¸­æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚</li>
<li>åˆ©ç”¨é—²ç½®çš„ä½æ€§èƒ½è®¾å¤‡ï¼ˆå¦‚ç‰©è”ç½‘è®¾å¤‡å’Œè¾¹ç¼˜è®¾å¤‡ï¼‰å¯ä»¥æœ‰åŠ©äºè§£å†³èµ„æºä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„å»ä¸­å¿ƒåŒ–GANè®­ç»ƒæ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸åˆ†äº«åŸå§‹æ•°æ®çš„æƒ…å†µä¸‹åˆ©ç”¨åˆ†æ•£çš„æ•°æ®å’Œè¿™äº›è®¾å¤‡ã€‚</li>
<li>è¯¥æ–¹æ³•ç»“åˆäº†KLDåŠ æƒèšç±»è”é‚¦å­¦ä¹ å’Œå¼‚è´¨Uå‹åˆ†å‰²å­¦ä¹ ï¼Œä»¥å¤„ç†æ•°æ®å¤šæ ·æ€§å’Œè®¾å¤‡å·®å¼‚é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12979">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-678cbc75f9aecf93995534cde538fad3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41239569269d0463e69eb64a87b6fa09.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2b5d0bcff3283a4c4cc059fcd197773c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df22f872756cf06fb728c6cd5a0e0a48.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Pixel-Perfect-MegaMed-A-Megapixel-Scale-Vision-Language-Foundation-Model-for-Generating-High-Resolution-Medical-Images"><a href="#Pixel-Perfect-MegaMed-A-Megapixel-Scale-Vision-Language-Foundation-Model-for-Generating-High-Resolution-Medical-Images" class="headerlink" title="Pixel Perfect MegaMed: A Megapixel-Scale Vision-Language Foundation   Model for Generating High Resolution Medical Images"></a>Pixel Perfect MegaMed: A Megapixel-Scale Vision-Language Foundation   Model for Generating High Resolution Medical Images</h2><p><strong>Authors:Zahra TehraniNasab, Amar Kumar, Tal Arbel</strong></p>
<p>Medical image synthesis presents unique challenges due to the inherent complexity and high-resolution details required in clinical contexts. Traditional generative architectures such as Generative Adversarial Networks (GANs) or Variational Auto Encoder (VAEs) have shown great promise for high-resolution image generation but struggle with preserving fine-grained details that are key for accurate diagnosis. To address this issue, we introduce Pixel Perfect MegaMed, the first vision-language foundation model to synthesize images at resolutions of 1024x1024. Our method deploys a multi-scale transformer architecture designed specifically for ultra-high resolution medical image generation, enabling the preservation of both global anatomical context and local image-level details. By leveraging vision-language alignment techniques tailored to medical terminology and imaging modalities, Pixel Perfect MegaMed bridges the gap between textual descriptions and visual representations at unprecedented resolution levels. We apply our model to the CheXpert dataset and demonstrate its ability to generate clinically faithful chest X-rays from text prompts. Beyond visual quality, these high-resolution synthetic images prove valuable for downstream tasks such as classification, showing measurable performance gains when used for data augmentation, particularly in low-data regimes. Our code is accessible through the project website - <a target="_blank" rel="noopener" href="https://tehraninasab.github.io/pixelperfect-megamed">https://tehraninasab.github.io/pixelperfect-megamed</a>. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒåˆæˆå› å…¶å›ºæœ‰çš„å¤æ‚æ€§å’Œä¸´åºŠç¯å¢ƒä¸­æ‰€éœ€çš„é«˜åˆ†è¾¨ç‡ç»†èŠ‚è€Œé¢ä¸´ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ç”Ÿæˆæ¶æ„ï¼Œå¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æˆ–å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEsï¼‰ï¼Œåœ¨é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆæ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†åœ¨ä¿ç•™å¯¹äºå‡†ç¡®è¯Šæ–­è‡³å…³é‡è¦çš„ç»†å¾®ç»†èŠ‚æ–¹é¢å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Pixel Perfect MegaMedï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªèƒ½å¤Ÿåœ¨1024x1024åˆ†è¾¨ç‡ä¸‹åˆæˆå›¾åƒçš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨ä¸“é—¨ç”¨äºè¶…é«˜åˆ†è¾¨ç‡åŒ»å­¦å½±åƒç”Ÿæˆçš„å¤šå°ºåº¦å˜å‹å™¨æ¶æ„ï¼Œèƒ½å¤ŸåŒæ—¶ä¿ç•™å…¨å±€è§£å‰–ä¸Šä¸‹æ–‡å’Œå±€éƒ¨å›¾åƒçº§ç»†èŠ‚ã€‚é€šè¿‡åˆ©ç”¨é’ˆå¯¹åŒ»å­¦æœ¯è¯­å’Œæˆåƒæ¨¡å¼å®šåˆ¶çš„è§†è§‰è¯­è¨€å¯¹é½æŠ€æœ¯ï¼ŒPixel Perfect MegaMedåœ¨å‰æ‰€æœªæœ‰çš„é«˜åˆ†è¾¨ç‡æ°´å¹³ä¸Šæ­å»ºäº†æ–‡æœ¬æè¿°å’Œè§†è§‰è¡¨ç¤ºä¹‹é—´çš„æ¡¥æ¢ã€‚æˆ‘ä»¬å°†å…¶æ¨¡å‹åº”ç”¨äºCheXpertæ•°æ®é›†ï¼Œå¹¶å±•ç¤ºäº†å…¶æ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆä¸´åºŠçœŸå®æ„Ÿèƒ¸éƒ¨Xå°„çº¿çš„èƒ½åŠ›ã€‚é™¤äº†è§†è§‰è´¨é‡å¤–ï¼Œè¿™äº›é«˜åˆ†è¾¨ç‡åˆæˆå›¾åƒå¯¹äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»ï¼‰ä¹Ÿè¯æ˜æ˜¯æœ‰ä»·å€¼çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼Œç”¨äºæ•°æ®å¢å¼ºæ—¶æ˜¾ç¤ºå‡ºå¯è¡¡é‡çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„ä»£ç å¯é€šè¿‡é¡¹ç›®ç½‘ç«™è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://tehraninasab.github.io/pixelperfect-megamed">ç½‘ç«™é“¾æ¥</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.12698v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æ–°ä¸€ä»£Pixel Perfect MegaMedæ¨¡å‹é€šè¿‡ç»“åˆå¤šå°ºåº¦transformeræ¶æ„å’ŒåŒ»å­¦æœ¯è¯­çš„è§†åŠ›è¯­è¨€å¯¹é½æŠ€æœ¯ï¼Œå®ç°äº†è¶…åˆ†è¾¨ç‡çš„åŒ»å­¦å›¾åƒåˆæˆã€‚è¯¥æ¨¡å‹åœ¨CheXpertæ•°æ®é›†ä¸Šç”Ÿæˆé€¼çœŸçš„èƒ¸éƒ¨Xå…‰ç‰‡ï¼Œå¹¶è¯æ˜å…¶åœ¨æ•°æ®å¢å¼ºæ–¹é¢çš„ä»·å€¼ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½æ•°æ®åœºæ™¯ä¸‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Pixel Perfect MegaMedæ˜¯é¦–ä¸ªç”¨äºåˆæˆè¶…åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒçš„è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹é‡‡ç”¨å¤šå°ºåº¦transformeræ¶æ„ï¼Œæ—¨åœ¨ç”Ÿæˆè¶…é«˜åˆ†è¾¨ç‡åŒ»å­¦å›¾åƒã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿä¿ç•™å…¨å±€è§£å‰–ç»“æ„å’Œå±€éƒ¨å›¾åƒçº§åˆ«çš„ç»†èŠ‚ä¿¡æ¯ã€‚</li>
<li>é€šè¿‡é’ˆå¯¹åŒ»å­¦æœ¯è¯­å’Œæˆåƒæ¨¡å¼çš„è§†åŠ›è¯­è¨€å¯¹é½æŠ€æœ¯ï¼ŒPixel Perfect MegaMedå®ç°äº†æ–‡æœ¬æè¿°å’Œè§†è§‰è¡¨ç¤ºä¹‹é—´çš„æ— ç¼è¿æ¥ã€‚</li>
<li>åœ¨CheXpertæ•°æ®é›†ä¸Šåº”ç”¨æ¨¡å‹ï¼ŒæˆåŠŸç”ŸæˆåŸºäºæ–‡æœ¬æç¤ºçš„ä¸´åºŠçœŸå®èƒ¸éƒ¨Xå…‰ç‰‡ã€‚</li>
<li>é«˜åˆ†è¾¨ç‡åˆæˆå›¾åƒä¸ä»…å…·æœ‰è§†è§‰è´¨é‡ï¼Œè€Œä¸”å¯¹äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚åˆ†ç±»ï¼‰ä¹Ÿè¯æ˜å…¶ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.12698">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a567e0bb5f0d4c88d81d745518c14e8e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57e817aa0dbbcf308bea5b3261f30dff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c1a2176d3fa572acc0854816c8a72660.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d012fe2a81642ecec2f15be78edc072.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c42a57577e8cea47e099ec7775fa2046.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-330f1d2c544ed40486dfb2bd3aede002.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4a52c70c867007936d6d1befa915b616.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Expert-Operational-GANS-Towards-Real-Color-Underwater-Image-Restoration"><a href="#Expert-Operational-GANS-Towards-Real-Color-Underwater-Image-Restoration" class="headerlink" title="Expert Operational GANS: Towards Real-Color Underwater Image Restoration"></a>Expert Operational GANS: Towards Real-Color Underwater Image Restoration</h2><p><strong>Authors:Ozer Can Devecioglu, Serkan Kiranyaz, Mehmet Yamac, Moncef Gabbouj</strong></p>
<p>The wide range of deformation artifacts that arise from complex light propagation, scattering, and depth-dependent attenuation makes the underwater image restoration to remain a challenging problem. Like other single deep regressor networks, conventional GAN-based restoration methods struggle to perform well across this heterogeneous domain, since a single generator network is typically insufficient to capture the full range of visual degradations. In order to overcome this limitation, we propose xOp-GAN, a novel GAN model with several expert generator networks, each trained solely on a particular subset with a certain image quality. Thus, each generator can learn to maximize its restoration performance for a particular quality range. Once a xOp-GAN is trained, each generator can restore the input image and the best restored image can then be selected by the discriminator based on its perceptual confidence score. As a result, xOP-GAN is the first GAN model with multiple generators where the discriminator is being used during the inference of the regression task. Experimental results on benchmark Large Scale Underwater Image (LSUI) dataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB, surpassing all single-regressor models by a large margin even, with reduced complexity. </p>
<blockquote>
<p>ç”±äºå¤æ‚çš„å…‰ä¼ æ’­ã€æ•£å°„å’Œæ·±åº¦ç›¸å…³çš„è¡°å‡æ‰€äº§ç”Ÿçš„å„ç§å˜å½¢ä¼ªå½±ï¼Œæ°´ä¸‹å›¾åƒæ¢å¤ä»ç„¶æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ä¸å…¶ä»–å•ä¸€æ·±åº¦å›å½’ç½‘ç»œä¸€æ ·ï¼Œä¼ ç»Ÿçš„åŸºäºGANçš„ä¿®å¤æ–¹æ³•åœ¨è¿™ä¸ªå¼‚æ„é¢†åŸŸè¡¨ç°ä¸ä½³ï¼Œå› ä¸ºå•ä¸ªç”Ÿæˆå™¨ç½‘ç»œé€šå¸¸ä¸è¶³ä»¥æ•è·å…¨èŒƒå›´çš„è§†è§‰é€€åŒ–ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†xOp-GANï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹GANæ¨¡å‹ï¼ŒåŒ…å«å¤šä¸ªä¸“å®¶ç”Ÿæˆå™¨ç½‘ç»œï¼Œæ¯ä¸ªç½‘ç»œä»…é’ˆå¯¹ç‰¹å®šå­é›†å’Œç‰¹å®šå›¾åƒè´¨é‡è¿›è¡Œè®­ç»ƒã€‚å› æ­¤ï¼Œæ¯ä¸ªç”Ÿæˆå™¨éƒ½å¯ä»¥å­¦ä¹ åœ¨ç‰¹å®šè´¨é‡èŒƒå›´å†…æœ€å¤§åŒ–å…¶ä¿®å¤æ€§èƒ½ã€‚ä¸€æ—¦è®­ç»ƒå¥½xOp-GANæ¨¡å‹ï¼Œæ¯ä¸ªç”Ÿæˆå™¨éƒ½å¯ä»¥æ¢å¤è¾“å…¥å›¾åƒï¼Œç„¶åç”±é‰´åˆ«å™¨æ ¹æ®æ„ŸçŸ¥ç½®ä¿¡åº¦åˆ†æ•°é€‰æ‹©æœ€ä½³æ¢å¤å›¾åƒã€‚å› æ­¤ï¼ŒxOp-GANæ˜¯ç¬¬ä¸€ä¸ªé‡‡ç”¨å¤šä¸ªç”Ÿæˆå™¨çš„GANæ¨¡å‹ï¼Œåœ¨å›å½’ä»»åŠ¡çš„æ¨æ–­è¿‡ç¨‹ä¸­ä¼šä½¿ç”¨é‰´åˆ«å™¨ã€‚åœ¨å¤§è§„æ¨¡æ°´ä¸‹å›¾åƒåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒxOp-GANçš„å³°å€¼ä¿¡å™ªæ¯”å¯è¾¾25.16åˆ†è´ï¼Œåœ¨é™ä½å¤æ‚åº¦çš„åŒæ—¶å¤§å¹…è¶…è¶Šäº†æ‰€æœ‰å•ä¸€å›å½’æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.11562v1">PDF</a> 6 pages</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ°´ä¸‹å›¾åƒæ¢å¤çš„é—®é¢˜ï¼ŒxOp-GANä½œä¸ºä¸€ç§æ–°å‹çš„å¤šç”Ÿæˆå™¨GANæ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ç”±å¤æ‚çš„å…‰ä¼ æ’­ã€æ•£å°„å’Œæ·±åº¦ä¾èµ–è¡°å‡å¯¼è‡´çš„å˜å½¢é—®é¢˜ã€‚å®ƒåŒ…å«å¤šä¸ªä¸“ä¸šç”Ÿæˆå™¨ç½‘ç»œï¼Œæ¯ä¸ªç½‘ç»œé’ˆå¯¹ç‰¹å®šå›¾åƒè´¨é‡è¿›è¡Œè®­ç»ƒï¼Œå¯æœ€å¤§åŒ–å…¶æ¢å¤æ€§èƒ½ã€‚åœ¨LSUIæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒxOp-GANå®ç°äº†é«˜è¾¾25.16 dBçš„PSNRæ°´å¹³ï¼Œå¤§å¹…è¶…è¶Šäº†æ‰€æœ‰å•ä¸€å›å½’æ¨¡å‹ï¼ŒåŒæ—¶é™ä½äº†å¤æ‚æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ°´ä¸‹å›¾åƒæ¢å¤æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„éš¾é¢˜ï¼Œæ¶‰åŠåˆ°å¤æ‚çš„å…‰ä¼ æ’­ã€æ•£å°„å’Œæ·±åº¦ä¾èµ–è¡°å‡ç­‰å¤šç§å› ç´ ã€‚</li>
<li>ä¼ ç»ŸGANæ¢å¤æ–¹æ³•åœ¨é¢å¯¹å¤šç§è§†è§‰å¤±çœŸæ—¶è¡¨ç°æœ‰é™ï¼Œå•ä¸ªç”Ÿæˆå™¨ç½‘ç»œéš¾ä»¥è¦†ç›–å…¨éƒ¨é€€åŒ–èŒƒå›´ã€‚</li>
<li>xOp-GANæå‡ºäº†ä¸€ç§åŒ…å«å¤šä¸ªä¸“ä¸šç”Ÿæˆå™¨ç½‘ç»œçš„æ¨¡å‹æ¶æ„ï¼Œæ¯ä¸ªç”Ÿæˆå™¨é’ˆå¯¹ç‰¹å®šå›¾åƒè´¨é‡è¿›è¡Œè®­ç»ƒã€‚</li>
<li>æ¯ä¸ªç”Ÿæˆå™¨ç½‘ç»œèƒ½æœ€å¤§åŒ–ç‰¹å®šè´¨é‡èŒƒå›´å†…çš„æ¢å¤æ€§èƒ½ã€‚</li>
<li>åœ¨xOp-GANä¸­ï¼Œé‰´åˆ«å™¨åœ¨å›å½’ä»»åŠ¡çš„æ¨æ–­è¿‡ç¨‹ä¸­å‘æŒ¥é€‰æ‹©æœ€ä½³æ¢å¤å›¾åƒçš„ä½œç”¨ï¼ŒåŸºäºæ„ŸçŸ¥ç½®ä¿¡åº¦è¯„åˆ†è¿›è¡Œåˆ¤å®šã€‚</li>
<li>åœ¨LSUIæ•°æ®é›†ä¸Šçš„å®éªŒæ˜¾ç¤ºï¼ŒxOp-GANè¾¾åˆ°äº†é«˜PSNRæ°´å¹³ï¼ˆ25.16 dBï¼‰ï¼Œæ˜¾è‘—ä¼˜äºå•ä¸€å›å½’æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.11562">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-871aeb9414b0ec33caedccc5cb9636a0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-06ee81e3bb039491413032598f429060.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-376d7101a94b674a45f79951db317041.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b80a6db40d97dee61f91470dafbd3be2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06f4c65256f3974517d6826207c560e3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3a851757d0611e21f58080a8f6843983.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Degradation-Agnostic-Statistical-Facial-Feature-Transformation-for-Blind-Face-Restoration-in-Adverse-Weather-Conditions"><a href="#Degradation-Agnostic-Statistical-Facial-Feature-Transformation-for-Blind-Face-Restoration-in-Adverse-Weather-Conditions" class="headerlink" title="Degradation-Agnostic Statistical Facial Feature Transformation for Blind   Face Restoration in Adverse Weather Conditions"></a>Degradation-Agnostic Statistical Facial Feature Transformation for Blind   Face Restoration in Adverse Weather Conditions</h2><p><strong>Authors:Chang-Hwan Son</strong></p>
<p>With the increasing deployment of intelligent CCTV systems in outdoor environments, there is a growing demand for face recognition systems optimized for challenging weather conditions. Adverse weather significantly degrades image quality, which in turn reduces recognition accuracy. Although recent face image restoration (FIR) models based on generative adversarial networks (GANs) and diffusion models have shown progress, their performance remains limited due to the lack of dedicated modules that explicitly address weather-induced degradations. This leads to distorted facial textures and structures. To address these limitations, we propose a novel GAN-based blind FIR framework that integrates two key components: local Statistical Facial Feature Transformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The local SFFT module enhances facial structure and color fidelity by aligning the local statistical distributions of low-quality (LQ) facial regions with those of high-quality (HQ) counterparts. Complementarily, the DAFE module enables robust statistical facial feature extraction under adverse weather conditions by aligning LQ and HQ encoder representations, thereby making the restoration process adaptive to severe weather-induced degradations. Experimental results demonstrate that the proposed degradation-agnostic SFFT model outperforms existing state-of-the-art FIR methods based on GAN and diffusion models, particularly in suppressing texture distortions and accurately reconstructing facial structures. Furthermore, both the SFFT and DAFE modules are empirically validated in enhancing structural fidelity and perceptual quality in face restoration under challenging weather scenarios. </p>
<blockquote>
<p>éšç€æ™ºèƒ½CCTVç³»ç»Ÿåœ¨æˆ·å¤–ç¯å¢ƒä¸­çš„éƒ¨ç½²è¶Šæ¥è¶Šå¤šï¼Œå¯¹äºé€‚åº”æ¶åŠ£å¤©æ°”æ¡ä»¶çš„é¢éƒ¨è¯†åˆ«ç³»ç»Ÿçš„éœ€æ±‚ä¹Ÿåœ¨æ—¥ç›Šå¢é•¿ã€‚æ¶åŠ£å¤©æ°”ä¼šä¸¥é‡é™ä½å›¾åƒè´¨é‡ï¼Œè¿›è€Œå¯¼è‡´è¯†åˆ«å‡†ç¡®åº¦ä¸‹é™ã€‚å°½ç®¡åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„é¢éƒ¨å›¾åƒæ¢å¤ï¼ˆFIRï¼‰æ¨¡å‹å·²ç»å–å¾—äº†è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹ä¸“é—¨è§£å†³å¤©æ°”å¼•èµ·çš„é€€åŒ–çš„æ¨¡å—ï¼Œå…¶æ€§èƒ½ä»ç„¶å—åˆ°é™åˆ¶ã€‚è¿™ä¼šå¯¼è‡´é¢éƒ¨çº¹ç†å’Œç»“æ„å¤±çœŸã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºGANçš„ç›²FIRæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå±€éƒ¨ç»Ÿè®¡é¢éƒ¨ç‰¹å¾å˜æ¢ï¼ˆSFFTï¼‰å’Œé€€åŒ–æ— å…³ç‰¹å¾åµŒå…¥ï¼ˆDAFEï¼‰ã€‚å±€éƒ¨SFFTæ¨¡å—é€šè¿‡å¯¹é½ä½è´¨é‡ï¼ˆLQï¼‰é¢éƒ¨åŒºåŸŸçš„å±€éƒ¨ç»Ÿè®¡åˆ†å¸ƒä¸é«˜è´¨é‡ï¼ˆHQï¼‰å¯¹åº”åŒºåŸŸçš„ç»Ÿè®¡åˆ†å¸ƒï¼Œå¢å¼ºé¢éƒ¨ç»“æ„å’Œé¢œè‰²ä¿çœŸåº¦ã€‚ä½œä¸ºè¡¥å……ï¼ŒDAFEæ¨¡å—é€šè¿‡å¯¹é½LQå’ŒHQç¼–ç å™¨è¡¨ç¤ºï¼Œå®ç°åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„ç¨³å¥ç»Ÿè®¡é¢éƒ¨ç‰¹å¾æå–ï¼Œä»è€Œä½¿æ¢å¤è¿‡ç¨‹é€‚åº”ä¸¥é‡çš„å¤©æ°”å¼•èµ·çš„é€€åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„é€€åŒ–æ— å…³SFFTæ¨¡å‹ä¼˜äºåŸºäºGANå’Œæ‰©æ•£æ¨¡å‹çš„ç°æœ‰å…ˆè¿›FIRæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æŠ‘åˆ¶çº¹ç†å¤±çœŸå’Œå‡†ç¡®é‡å»ºé¢éƒ¨ç»“æ„æ–¹é¢ã€‚æ­¤å¤–ï¼ŒSFFTå’ŒDAFEæ¨¡å—åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„å¤©æ°”åœºæ™¯ä¸‹æé«˜é¢éƒ¨æ¢å¤çš„ç»“æ„ä¿çœŸåº¦å’Œæ„ŸçŸ¥è´¨é‡æ–¹é¢ä¹Ÿå¾—åˆ°äº†å®è¯éªŒè¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.07464v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹å®¤å¤–ç¯å¢ƒä¸­æ™ºèƒ½ç›‘æ§ç³»ç»Ÿçš„å¹¿æ³›åº”ç”¨ï¼Œå¯¹é¢éƒ¨è¯†åˆ«ç³»ç»Ÿæå‡ºäº†åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„ä¼˜åŒ–éœ€æ±‚ã€‚è™½ç„¶åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œæ‰©æ•£æ¨¡å‹çš„é¢éƒ¨å›¾åƒæ¢å¤ï¼ˆFIRï¼‰æ¨¡å‹å·²æœ‰æ‰€è¿›å±•ï¼Œä½†ç”±äºç¼ºä¹ä¸“é—¨åº”å¯¹å¤©æ°”å¼•èµ·çš„é™è´¨çš„æ¨¡å—ï¼Œå…¶æ€§èƒ½ä»æœ‰å±€é™ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åŸºäºGANçš„ç›²FIRæ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå±€éƒ¨ç»Ÿè®¡é¢éƒ¨ç‰¹å¾å˜æ¢ï¼ˆSFFTï¼‰å’Œé™è´¨æ— å…³ç‰¹å¾åµŒå…¥ï¼ˆDAFEï¼‰ã€‚SFFTæ¨¡å—é€šè¿‡å¯¹é½ä½è´¨é‡ï¼ˆLQï¼‰é¢éƒ¨åŒºåŸŸä¸é«˜è´¨é‡ï¼ˆHQï¼‰åŒºåŸŸçš„å±€éƒ¨ç»Ÿè®¡åˆ†å¸ƒï¼Œå¢å¼ºé¢éƒ¨ç»“æ„å’Œè‰²å½©ä¿çœŸåº¦ã€‚DAFEæ¨¡å—åˆ™é€šè¿‡å¯¹é½LQå’ŒHQç¼–ç å™¨è¡¨ç¤ºï¼Œä½¿é¢éƒ¨æ¢å¤è¿‡ç¨‹é€‚åº”æ¶åŠ£å¤©æ°”å¼•èµ·çš„é™è´¨ï¼Œå®ç°åœ¨å„ç§å¤©æ°”æ¡ä»¶ä¸‹çš„ç¨³å¥é¢éƒ¨ç‰¹å¾æå–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„é™è´¨æ— å…³SFFTæ¨¡å‹åœ¨é¢éƒ¨æ¢å¤æ–¹é¢ä¼˜äºç°æœ‰åŸºäºGANå’Œæ‰©æ•£æ¨¡å‹çš„å…ˆè¿›FIRæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨æŠ‘åˆ¶çº¹ç†å¤±çœŸå’Œå‡†ç¡®é‡å»ºé¢éƒ¨ç»“æ„æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ™ºèƒ½ç›‘æ§ç³»ç»Ÿåœ¨å®¤å¤–ç¯å¢ƒä¸­çš„å¹¿æ³›åº”ç”¨æ¨åŠ¨äº†é¢éƒ¨è¯†åˆ«ç³»ç»Ÿåœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹çš„ä¼˜åŒ–éœ€æ±‚ã€‚</li>
<li>ç°æœ‰åŸºäºGANå’Œæ‰©æ•£æ¨¡å‹çš„é¢éƒ¨å›¾åƒæ¢å¤ï¼ˆFIRï¼‰æ¨¡å‹è™½æœ‰æ‰€è¿›å±•ï¼Œä½†æ€§èƒ½å—é™äºç¼ºä¹åº”å¯¹å¤©æ°”å¼•èµ·çš„é™è´¨çš„ä¸“é—¨æ¨¡å—ã€‚</li>
<li>æå‡ºçš„ç›²FIRæ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šå±€éƒ¨ç»Ÿè®¡é¢éƒ¨ç‰¹å¾å˜æ¢ï¼ˆSFFTï¼‰å’Œé™è´¨æ— å…³ç‰¹å¾åµŒå…¥ï¼ˆDAFEï¼‰ã€‚</li>
<li>SFFTæ¨¡å—é€šè¿‡å¯¹é½LQå’ŒHQé¢éƒ¨åŒºåŸŸçš„ç»Ÿè®¡åˆ†å¸ƒï¼Œå¢å¼ºé¢éƒ¨ç»“æ„å’Œè‰²å½©ä¿çœŸåº¦ã€‚</li>
<li>DAFEæ¨¡å—å®ç°äº†åœ¨å„ç§å¤©æ°”æ¡ä»¶ä¸‹çš„ç¨³å¥é¢éƒ¨ç‰¹å¾æå–ï¼Œé€šè¿‡å¯¹é½LQå’ŒHQç¼–ç å™¨è¡¨ç¤ºï¼Œä½¿æ¢å¤è¿‡ç¨‹é€‚åº”æ¶åŠ£å¤©æ°”å¼•èµ·çš„é™è´¨ã€‚</li>
<li>æå‡ºçš„é™è´¨æ— å…³SFFTæ¨¡å‹åœ¨é¢éƒ¨æ¢å¤æ–¹é¢ä¼˜äºç°æœ‰å…ˆè¿›FIRæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.07464">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-51a498782496cc84154b3110347b68c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8354c3135b1401b1108d8334a59ae8de.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="HER2-Expression-Prediction-with-Flexible-Multi-Modal-Inputs-via-Dynamic-Bidirectional-Reconstruction"><a href="#HER2-Expression-Prediction-with-Flexible-Multi-Modal-Inputs-via-Dynamic-Bidirectional-Reconstruction" class="headerlink" title="HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic   Bidirectional Reconstruction"></a>HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic   Bidirectional Reconstruction</h2><p><strong>Authors:Jie Qin, Wei Yang, Yan Su, Yiran Zhu, Weizhen Li, Yunyue Pan, Chengchang Pan, Honggang Qi</strong></p>
<p>In breast cancer HER2 assessment, clinical evaluation relies on combined H&amp;E and IHC images, yet acquiring both modalities is often hindered by clinical constraints and cost. We propose an adaptive bimodal prediction framework that flexibly supports single- or dual-modality inputs through two core innovations: a dynamic branch selector activating modality completion or joint inference based on input availability, and a cross-modal GAN (CM-GAN) enabling feature-space reconstruction of missing modalities. This design dramatically improves H&amp;E-only accuracy from 71.44% to 94.25%, achieves 95.09% with full dual-modality inputs, and maintains 90.28% reliability under single-modality conditions. The â€œdual-modality preferred, single-modality compatibleâ€ architecture delivers near-dual-modality accuracy without mandatory synchronized acquisition, offering a cost-effective solution for resource-limited regions and significantly improving HER2 assessment accessibility. </p>
<blockquote>
<p>åœ¨ä¹³è…ºç™ŒHER2è¯„ä¼°ä¸­ï¼Œä¸´åºŠè¯„ä¼°ä¾èµ–äºH&amp;Eå’ŒIHCå›¾åƒçš„è”åˆåˆ†æï¼Œç„¶è€Œï¼Œç”±äºä¸´åºŠé™åˆ¶å’Œæˆæœ¬é—®é¢˜ï¼Œç»å¸¸éš¾ä»¥è·å¾—è¿™ä¸¤ç§æ¨¡å¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”åŒæ¨¡æ€é¢„æµ‹æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°ç‚¹çµæ´»åœ°æ”¯æŒå•æ¨¡æ€æˆ–åŒæ¨¡æ€è¾“å…¥ï¼šä¸€ä¸ªåŠ¨æ€åˆ†æ”¯é€‰æ‹©å™¨ï¼Œæ ¹æ®è¾“å…¥å¯ç”¨æ€§æ¿€æ´»æ¨¡æ€å®Œæˆæˆ–è”åˆæ¨æ–­ï¼›ä¸€ä¸ªè·¨æ¨¡æ€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆCM-GANï¼‰ï¼Œèƒ½å¤Ÿå®ç°ç¼ºå¤±æ¨¡æ€çš„ç‰¹å¾ç©ºé—´é‡å»ºã€‚è¿™ä¸€è®¾è®¡å¤§å¤§æé«˜äº†ä»…ä½¿ç”¨H&amp;Eçš„å‡†ç¡®æ€§ï¼Œä»71.44%æé«˜åˆ°94.25%ï¼Œåœ¨å…¨åŒæ¨¡æ€è¾“å…¥çš„æƒ…å†µä¸‹è¾¾åˆ°95.09%ï¼Œåœ¨å•æ¨¡æ€æ¡ä»¶ä¸‹ä¿æŒ90.28%çš„å¯é æ€§ã€‚è¿™ç§â€œä»¥åŒæ¨¡æ€ä¸ºä¸»ï¼Œå•æ¨¡æ€å…¼å®¹â€çš„æ¶æ„ï¼Œåœ¨ä¸å¼ºåˆ¶åŒæ­¥é‡‡é›†çš„æƒ…å†µä¸‹å®ç°äº†æ¥è¿‘åŒæ¨¡æ€çš„å‡†ç¡®æ€§ï¼Œä¸ºèµ„æºæœ‰é™çš„åœ°åŒºæä¾›äº†ç»æµé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶å¤§å¤§æé«˜äº†HER2è¯„ä¼°çš„å¯è¾¾æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10006v2">PDF</a> 8 pages,6 figures,3 tables,accepted by the 33rd ACM International   Conference on Multimedia(ACM MM 2025)</p>
<p><strong>Summary</strong></p>
<p>åœ¨ä¹³è…ºç™ŒHER2è¯„ä¼°ä¸­ï¼Œä¸´åºŠè¯„ä¼°ä¾èµ–äºH&amp;Eå’ŒIHCå›¾åƒçš„è”åˆä½¿ç”¨ï¼Œä½†è·å–è¿™ä¸¤ç§æ¨¡å¼å¸¸å¸¸å—åˆ°ä¸´åºŠé™åˆ¶å’Œæˆæœ¬çš„å½±å“ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªé€‚åº”åŒæ¨¡æ€é¢„æµ‹æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ä¸ªæ ¸å¿ƒåˆ›æ–°ç‚¹çµæ´»æ”¯æŒå•æ¨¡æ€æˆ–åŒæ¨¡æ€è¾“å…¥ï¼šåŠ¨æ€åˆ†æ”¯é€‰æ‹©å™¨æ ¹æ®è¾“å…¥å¯ç”¨æ€§æ¿€æ´»æ¨¡æ€å®Œæˆæˆ–è”åˆæ¨ç†ï¼Œä»¥åŠè·¨æ¨¡æ€GANï¼ˆCM-GANï¼‰å®ç°ç¼ºå¤±æ¨¡æ€çš„ç‰¹å¾ç©ºé—´é‡å»ºã€‚è¯¥è®¾è®¡æ˜¾è‘—æé«˜äº†ä»…ä½¿ç”¨H&amp;Eçš„å‡†ç¡®æ€§ï¼Œä»71.44%æé«˜åˆ°94.25%ï¼Œåœ¨å®Œæ•´çš„åŒæ¨¡æ€è¾“å…¥ä¸‹è¾¾åˆ°95.09%ï¼Œåœ¨å•æ¨¡æ€æ¡ä»¶ä¸‹ä¿æŒ90.28%çš„å¯é æ€§ã€‚è¿™ç§â€œä»¥åŒæ¨¡æ€ä¸ºä¸»ï¼Œå•æ¨¡æ€å…¼å®¹â€çš„æ¶æ„åœ¨ä¸å¼ºåˆ¶åŒæ­¥é‡‡é›†çš„æƒ…å†µä¸‹å®ç°äº†æ¥è¿‘åŒæ¨¡æ€çš„å‡†ç¡®æ€§ï¼Œä¸ºèµ„æºæœ‰é™çš„åœ°åŒºæä¾›äº†ç»æµé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶æ˜¾ç€æé«˜äº†HER2è¯„ä¼°çš„å¯è¾¾æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸´åºŠè¯„ä¼°ä¹³è…ºç™ŒHER2æ—¶ï¼Œå¸¸ä¾èµ–H&amp;Eå’ŒIHCä¸¤ç§å›¾åƒçš„è”åˆä½¿ç”¨ã€‚</li>
<li>è·å–H&amp;Eå’ŒIHCå›¾åƒæœ‰æ—¶å—ä¸´åºŠé™åˆ¶å’Œæˆæœ¬å½±å“ã€‚</li>
<li>æå‡ºäº†è‡ªé€‚åº”åŒæ¨¡æ€é¢„æµ‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½çµæ´»å¤„ç†å•æ¨¡æ€æˆ–åŒæ¨¡æ€è¾“å…¥ã€‚</li>
<li>æ¡†æ¶åŒ…æ‹¬åŠ¨æ€åˆ†æ”¯é€‰æ‹©å™¨å’Œè·¨æ¨¡æ€GANï¼ˆCM-GANï¼‰ä¸¤å¤§æ ¸å¿ƒåˆ›æ–°ã€‚</li>
<li>è¯¥è®¾è®¡æ˜¾è‘—æé«˜äº†ä»…ä½¿ç”¨H&amp;Eçš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸”èƒ½åœ¨åŒæ¨¡æ€è¾“å…¥ä¸‹è¾¾åˆ°é«˜å‡†ç¡®æ€§ã€‚</li>
<li>æ¡†æ¶èƒ½åœ¨å•æ¨¡æ€æ¡ä»¶ä¸‹ä¿æŒé«˜å¯é æ€§ï¼Œå®ç°äº†æ¥è¿‘åŒæ¨¡æ€çš„å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b1c8218789ec6c98552285019200020.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-234c26ffa3d72ec2eace9f4dcde4d2b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ee456f3abea0f5b08148a895592719ac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b1e879084839cbf4e3f7c73c2f5c89d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f2391bc2b9b1e6466a3632ec02818d05.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-066ec7dea94bb9894eac4cc6b1e51c08.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/GAN/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/GAN/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/GAN/">
                                    <span class="chip bg-color">GAN</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/%E5%85%83%E5%AE%87%E5%AE%99_%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-ffab9676fc57bdedb660e09e9f610722.jpg" class="responsive-img" alt="å…ƒå®‡å®™/è™šæ‹Ÿäºº">
                        
                        <span class="card-title">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å…ƒå®‡å®™/è™šæ‹Ÿäºº æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  MoGA 3D Generative Avatar Prior for Monocular Gaussian Avatar   Reconstruction
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/" class="post-category">
                                    å…ƒå®‡å®™/è™šæ‹Ÿäºº
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%85%83%E5%AE%87%E5%AE%99-%E8%99%9A%E6%8B%9F%E4%BA%BA/">
                        <span class="chip bg-color">å…ƒå®‡å®™/è™šæ‹Ÿäºº</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/Face%20Swapping/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-eb1acddccc88c00a5d5e6ee7f516b757.jpg" class="responsive-img" alt="Face Swapping">
                        
                        <span class="card-title">Face Swapping</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Face Swapping æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-02  Robust Deepfake Detection for Electronic Know Your Customer Systems   Using Registered Images
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Face-Swapping/" class="post-category">
                                    Face Swapping
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Face-Swapping/">
                        <span class="chip bg-color">Face Swapping</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31987.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
