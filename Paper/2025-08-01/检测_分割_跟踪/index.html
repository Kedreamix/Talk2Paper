<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
    <meta name="description" content="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  3D-MOOD Lifting 2D to 3D for Monocular Open-Set Object Detection">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-8f3306bc01ad6d0bc945599d874fa1a2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-08-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-08-01-æ›´æ–°"><a href="#2025-08-01-æ›´æ–°" class="headerlink" title="2025-08-01 æ›´æ–°"></a>2025-08-01 æ›´æ–°</h1><h2 id="3D-MOOD-Lifting-2D-to-3D-for-Monocular-Open-Set-Object-Detection"><a href="#3D-MOOD-Lifting-2D-to-3D-for-Monocular-Open-Set-Object-Detection" class="headerlink" title="3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection"></a>3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection</h2><p><strong>Authors:Yung-Hsu Yang, Luigi Piccinelli, Mattia Segu, Siyuan Li, Rui Huang, Yuqian Fu, Marc Pollefeys, Hermann Blum, Zuria Bauer</strong></p>
<p>Monocular 3D object detection is valuable for various applications such as robotics and AR&#x2F;VR. Existing methods are confined to closed-set settings, where the training and testing sets consist of the same scenes and&#x2F;or object categories. However, real-world applications often introduce new environments and novel object categories, posing a challenge to these methods. In this paper, we address monocular 3D object detection in an open-set setting and introduce the first end-to-end 3D Monocular Open-set Object Detector (3D-MOOD). We propose to lift the open-set 2D detection into 3D space through our designed 3D bounding box head, enabling end-to-end joint training for both 2D and 3D tasks to yield better overall performance. We condition the object queries with geometry prior and overcome the generalization for 3D estimation across diverse scenes. To further improve performance, we design the canonical image space for more efficient cross-dataset training. We evaluate 3D-MOOD on both closed-set settings (Omni3D) and open-set settings (Omni3D to Argoverse 2, ScanNet), and achieve new state-of-the-art results. Code and models are available at royyang0714.github.io&#x2F;3D-MOOD. </p>
<blockquote>
<p>å•ç›®3Dç›®æ ‡æ£€æµ‹åœ¨æœºå™¨äººå’ŒAR&#x2F;VRç­‰åº”ç”¨ä¸­å…·æœ‰ä»·å€¼ã€‚ç°æœ‰æ–¹æ³•ä»…é™äºå°é—­é›†åœºæ™¯ï¼Œå³è®­ç»ƒå’Œæµ‹è¯•é›†åŒ…å«ç›¸åŒçš„åœºæ™¯å’Œ&#x2F;æˆ–ç›®æ ‡ç±»åˆ«ã€‚ç„¶è€Œï¼Œç°å®ä¸–ç•Œçš„åº”ç”¨é€šå¸¸ä¼šå¼•å…¥æ–°çš„ç¯å¢ƒå’Œæ–°å‹ç›®æ ‡ç±»åˆ«ï¼Œå¯¹è¿™äº›æ–¹æ³•æ„æˆäº†æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†å•ç›®3Dç›®æ ‡æ£€æµ‹çš„å¼€æ”¾é›†è®¾ç½®é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†é¦–ä¸ªç«¯åˆ°ç«¯çš„3Då•ç›®å¼€æ”¾é›†ç›®æ ‡æ£€æµ‹å™¨ï¼ˆ3D-MOODï¼‰ã€‚æˆ‘ä»¬æå‡ºé€šè¿‡è®¾è®¡çš„3Dè¾¹ç•Œæ¡†å¤´å°†å¼€æ”¾é›†2Dæ£€æµ‹æå‡åˆ°3Dç©ºé—´ï¼Œä½¿2Då’Œ3Dä»»åŠ¡çš„ç«¯åˆ°ç«¯è”åˆè®­ç»ƒèƒ½å¤Ÿå®ç°æ›´å¥½çš„æ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬ä»¥å‡ ä½•å…ˆéªŒæ¡ä»¶å¯¹ç›®æ ‡æŸ¥è¯¢è¿›è¡Œçº¦æŸï¼Œå¹¶å…‹æœäº†åœ¨å„ç§åœºæ™¯ä¸­çš„3Dä¼°è®¡çš„æ³›åŒ–é—®é¢˜ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬è®¾è®¡äº†è§„èŒƒå›¾åƒç©ºé—´ä»¥è¿›è¡Œæ›´æœ‰æ•ˆçš„è·¨æ•°æ®é›†è®­ç»ƒã€‚æˆ‘ä»¬åœ¨å°é—­é›†ï¼ˆOmni3Dï¼‰å’Œå¼€æ”¾é›†ï¼ˆOmni3Dåˆ°Argoverse 2ï¼ŒScanNetï¼‰ä¸Šå¯¹3D-MOODè¿›è¡Œäº†è¯„ä¼°ï¼Œå–å¾—äº†æ–°çš„æœ€æ–°ç»“æœã€‚ä»£ç å’Œæ¨¡å‹å¯ä»¥åœ¨royyang0714.github.io&#x2F;3D-MOODä¸Šæ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23567v1">PDF</a> ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å•ç›®ä¸‰ç»´ç‰©ä½“æ£€æµ‹åœ¨å¼€æ”¾é›†åœºæ™¯ä¸‹çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†é¦–ä¸ªç«¯åˆ°ç«¯çš„ä¸‰ç»´å•ç›®å¼€æ”¾é›†ç‰©ä½“æ£€æµ‹å™¨ï¼ˆ3D-MOODï¼‰ã€‚è¯¥æ£€æµ‹å™¨é€šè¿‡å°†å¼€æ”¾é›†äºŒç»´æ£€æµ‹æå‡åˆ°ä¸‰ç»´ç©ºé—´ï¼Œåˆ©ç”¨è®¾è®¡çš„ä¸‰ç»´è¾¹ç•Œæ¡†å¤´å®ç°ç«¯åˆ°ç«¯çš„è”åˆè®­ç»ƒï¼Œä»¥æé«˜æ•´ä½“æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡å‡ ä½•å…ˆéªŒæ¡ä»¶åŒ–ç‰©ä½“æŸ¥è¯¢ï¼Œå¹¶è®¾è®¡è§„èŒƒåŒ–å›¾åƒç©ºé—´ä»¥è¿›è¡Œæ›´æœ‰æ•ˆçš„è·¨æ•°æ®é›†è®­ç»ƒï¼Œæé«˜äº†æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å°é—­é›†å’Œå¼€æ”¾é›†ç¯å¢ƒä¸­å¯¹3D-MOODè¿›è¡Œäº†è¯„ä¼°ï¼Œå–å¾—äº†æœ€æ–°æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯¥è®ºæ–‡ç ”ç©¶çš„æ˜¯å•ç›®ä¸‰ç»´ç‰©ä½“æ£€æµ‹çš„å¼€æ”¾é›†è®¾ç½®é—®é¢˜ï¼Œå…·æœ‰å®ç”¨ä»·å€¼åœ¨æœºå™¨äººã€AR&#x2F;VRç­‰åº”ç”¨é¢†åŸŸä¸­ã€‚</li>
<li>æå‡ºé¦–ä¸ªç«¯åˆ°ç«¯çš„å•ç›®ä¸‰ç»´å¼€æ”¾é›†ç‰©ä½“æ£€æµ‹å™¨ï¼ˆ3D-MOODï¼‰ï¼Œçªç ´ç°æœ‰æ–¹æ³•çš„å±€é™ã€‚</li>
<li>é€šè¿‡å°†å¼€æ”¾é›†äºŒç»´æ£€æµ‹æå‡åˆ°ä¸‰ç»´ç©ºé—´ï¼Œåˆ©ç”¨è®¾è®¡çš„ä¸‰ç»´è¾¹ç•Œæ¡†å¤´è¿›è¡Œç«¯åˆ°ç«¯çš„è”åˆè®­ç»ƒï¼Œå®ç°äºŒç»´å’Œä¸‰ç»´ä»»åŠ¡çš„æ•´ä½“æ€§èƒ½æå‡ã€‚</li>
<li>ç»“åˆå‡ ä½•å…ˆéªŒæ¡ä»¶åŒ–ç‰©ä½“æŸ¥è¯¢ï¼Œæ”¹å–„åœ¨å¤šç§åœºæ™¯ä¸‹çš„ä¸‰ç»´ä¼°è®¡æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è®¾è®¡è§„èŒƒåŒ–å›¾åƒç©ºé—´ä»¥è¿›è¡Œæ›´æœ‰æ•ˆçš„è·¨æ•°æ®é›†è®­ç»ƒï¼Œæé«˜æ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23567">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d0ca2717bdca3eb7617e9c42d5083883.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9f66b0cdc91066ffcdbdb323166f47f2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-871cc25f736ca81c1b6d78839363abf9.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2d2fa6bf32dcec9ed036bfd6bafcf8ec.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Learning-Semantic-Directions-for-Feature-Augmentation-in-Domain-Generalized-Medical-Segmentation"><a href="#Learning-Semantic-Directions-for-Feature-Augmentation-in-Domain-Generalized-Medical-Segmentation" class="headerlink" title="Learning Semantic Directions for Feature Augmentation in   Domain-Generalized Medical Segmentation"></a>Learning Semantic Directions for Feature Augmentation in   Domain-Generalized Medical Segmentation</h2><p><strong>Authors:Yingkai Wang, Yaoyao Zhu, Xiuding Cai, Yuhao Xiao, Haotian Wu, Yu Yao</strong></p>
<p>Medical image segmentation plays a crucial role in clinical workflows, but domain shift often leads to performance degradation when models are applied to unseen clinical domains. This challenge arises due to variations in imaging conditions, scanner types, and acquisition protocols, limiting the practical deployment of segmentation models. Unlike natural images, medical images typically exhibit consistent anatomical structures across patients, with domain-specific variations mainly caused by imaging conditions. This unique characteristic makes medical image segmentation particularly challenging.   To address this challenge, we propose a domain generalization framework tailored for medical image segmentation. Our approach improves robustness to domain-specific variations by introducing implicit feature perturbations guided by domain statistics. Specifically, we employ a learnable semantic direction selector and a covariance-based semantic intensity sampler to modulate domain-variant features while preserving task-relevant anatomical consistency. Furthermore, we design an adaptive consistency constraint that is selectively applied only when feature adjustment leads to degraded segmentation performance. This constraint encourages the adjusted features to align with the original predictions, thereby stabilizing feature selection and improving the reliability of the segmentation.   Extensive experiments on two public multi-center benchmarks show that our framework consistently outperforms existing domain generalization approaches, achieving robust and generalizable segmentation performance across diverse clinical domains. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠå·¥ä½œæµç¨‹ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†å½“æ¨¡å‹åº”ç”¨äºæœªè§è¿‡çš„ä¸´åºŠé¢†åŸŸæ—¶ï¼Œé¢†åŸŸå·®å¼‚é€šå¸¸ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¿™ä¸€æŒ‘æˆ˜æ˜¯ç”±äºæˆåƒæ¡ä»¶ã€æ‰«æä»ªç±»å‹å’Œé‡‡é›†åè®®çš„å˜åŒ–æ‰€å¯¼è‡´çš„ï¼Œé™åˆ¶äº†åˆ†å‰²æ¨¡å‹çš„å®é™…éƒ¨ç½²ã€‚ä¸è‡ªç„¶å›¾åƒä¸åŒï¼ŒåŒ»å­¦å›¾åƒåœ¨æ‚£è€…ä¹‹é—´é€šå¸¸è¡¨ç°å‡ºä¸€è‡´çš„è§£å‰–ç»“æ„ï¼Œé¢†åŸŸç‰¹å®šçš„å˜åŒ–ä¸»è¦ç”±æˆåƒæ¡ä»¶å¼•èµ·ã€‚è¿™ä¸€ç‹¬ç‰¹ç‰¹å¾ä½¿å¾—åŒ»å­¦å›¾åƒåˆ†å‰²ç‰¹åˆ«å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p>
</blockquote>
<p>ä¸ºäº†åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„é¢†åŸŸæ³›åŒ–æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¼•å…¥ç”±é¢†åŸŸç»Ÿè®¡å¼•å¯¼çš„éšå¼ç‰¹å¾æ‰°åŠ¨æ¥æé«˜å¯¹é¢†åŸŸç‰¹å®šå˜åŒ–çš„é²æ£’æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨å¯å­¦ä¹ çš„è¯­ä¹‰æ–¹å‘é€‰æ‹©å™¨å’ŒåŸºäºåæ–¹å·®çš„è¯­ä¹‰å¼ºåº¦é‡‡æ ·å™¨æ¥è°ƒèŠ‚é¢†åŸŸå˜é‡ç‰¹å¾ï¼ŒåŒæ—¶ä¿ç•™ä¸ä»»åŠ¡ç›¸å…³çš„è§£å‰–ç»“æ„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”ä¸€è‡´æ€§çº¦æŸï¼Œä»…åœ¨ç‰¹å¾è°ƒæ•´å¯¼è‡´åˆ†å‰²æ€§èƒ½ä¸‹é™æ—¶é€‰æ‹©æ€§åº”ç”¨ã€‚è¿™ç§çº¦æŸé¼“åŠ±è°ƒæ•´åçš„ç‰¹å¾ä¸åŸå§‹é¢„æµ‹å¯¹é½ï¼Œä»è€Œç¨³å®šç‰¹å¾é€‰æ‹©ï¼Œæé«˜åˆ†å‰²çš„å¯é æ€§ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23326v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠå·¥ä½œæµç¨‹ä¸­æ‰®æ¼”ç€è‡³å…³é‡è¦çš„è§’è‰²ï¼Œç„¶è€Œï¼Œå½“æ¨¡å‹åº”ç”¨äºæœªè§è¿‡çš„ä¸´åºŠé¢†åŸŸæ—¶ï¼Œé¢†åŸŸå·®å¼‚å¾€å¾€ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚è¿™ä¸€æŒ‘æˆ˜æ˜¯ç”±äºæˆåƒæ¡ä»¶ã€æ‰«æä»ªç±»å‹å’Œé‡‡é›†åè®®çš„å˜åŒ–è€Œäº§ç”Ÿçš„ï¼Œé™åˆ¶äº†åˆ†å‰²æ¨¡å‹çš„å®ç”¨éƒ¨ç½²ã€‚åŒ»å­¦å›¾åƒé€šå¸¸å±•ç°å‡ºè·¨æ‚£è€…çš„ä¸€è‡´è§£å‰–ç»“æ„ï¼Œå…¶ç‰¹æœ‰çš„é¢†åŸŸå·®å¼‚ä¸»è¦ç”±æˆåƒæ¡ä»¶å¼•èµ·ï¼Œè¿™ä½¿å¾—åŒ»å­¦å›¾åƒåˆ†å‰²å°¤ä¸ºå…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„é¢†åŸŸæ³›åŒ–æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡å¼•å…¥ç”±é¢†åŸŸç»Ÿè®¡å¼•å¯¼çš„éšå¼ç‰¹å¾æ‰°åŠ¨ï¼Œæé«˜äº†å¯¹é¢†åŸŸç‰¹å®šå˜åŒ–çš„ç¨³å¥æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨å¯å­¦ä¹ çš„è¯­ä¹‰æ–¹å‘é€‰æ‹©å™¨å’ŒåŸºäºåæ–¹å·®çš„è¯­ä¹‰å¼ºåº¦é‡‡æ ·å™¨æ¥è°ƒèŠ‚é¢†åŸŸå˜ä½“ç‰¹å¾ï¼ŒåŒæ—¶ä¿ç•™ä»»åŠ¡ç›¸å…³çš„è§£å‰–ç»“æ„ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”ä¸€è‡´æ€§çº¦æŸï¼Œä»…åœ¨ç‰¹å¾è°ƒæ•´å¯¼è‡´åˆ†å‰²æ€§èƒ½ä¸‹é™æ—¶é€‰æ‹©æ€§åº”ç”¨ã€‚è¿™ä¸€çº¦æŸä¿ƒä½¿è°ƒæ•´åçš„ç‰¹å¾ä¸åŸå§‹é¢„æµ‹ç»“æœå¯¹é½ï¼Œä»è€Œç¨³å®šç‰¹å¾é€‰æ‹©å¹¶æé«˜äº†åˆ†å‰²çš„å¯é æ€§ã€‚åœ¨å…¬å…±å¤šä¸­å¿ƒåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å§‹ç»ˆä¼˜äºç°æœ‰çš„é¢†åŸŸæ³›åŒ–æ–¹æ³•ï¼Œåœ¨å¤šæ ·åŒ–çš„ä¸´åºŠé¢†åŸŸä¸­å®ç°äº†ç¨³å¥ä¸”å¯æ¨å¹¿çš„åˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²åœ¨ä¸´åºŠåº”ç”¨ä¸­è‡³å…³é‡è¦ï¼Œä½†é¢†åŸŸå·®å¼‚å¯¼è‡´çš„æ¨¡å‹æ€§èƒ½ä¸‹é™æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚</li>
<li>åŒ»å­¦å›¾åƒé¢†åŸŸå·®å¼‚ä¸»è¦ç”±æˆåƒæ¡ä»¶å˜åŒ–å¼•èµ·ï¼Œå…·æœ‰ä¸€è‡´çš„è§£å‰–ç»“æ„ç‰¹ç‚¹ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„é¢†åŸŸæ³›åŒ–æ¡†æ¶ï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ï¼Œé€šè¿‡å¼•å…¥éšå¼ç‰¹å¾æ‰°åŠ¨æé«˜æ¨¡å‹ç¨³å¥æ€§ã€‚</li>
<li>é‡‡ç”¨å¯å­¦ä¹ çš„è¯­ä¹‰æ–¹å‘é€‰æ‹©å™¨å’ŒåŸºäºåæ–¹å·®çš„è¯­ä¹‰å¼ºåº¦é‡‡æ ·å™¨æ¥è°ƒèŠ‚é¢†åŸŸå˜ä½“ç‰¹å¾ã€‚</li>
<li>è®¾è®¡äº†è‡ªé€‚åº”ä¸€è‡´æ€§çº¦æŸï¼Œä»¥ç¨³å®šç‰¹å¾é€‰æ‹©å¹¶æ”¹å–„åˆ†å‰²ç»“æœçš„å¯é æ€§ã€‚</li>
<li>åœ¨å…¬å…±å¤šä¸­å¿ƒåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¼˜äºç°æœ‰çš„é¢†åŸŸæ³›åŒ–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c9c6cacfa60d0b0218e0b52c02984588.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-77af7ac77e5ff23a41ccff83e828c053.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d64ae584a3da4c4166ef2a7576c115ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a4d442e9382879998e0a9aa68c2c1ed0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-59d4f9814fc88d27292e0ba315f20c15.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18d3bf8b0dc083ba3a0b0242b523980c.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="EIFNet-Leveraging-Event-Image-Fusion-for-Robust-Semantic-Segmentation"><a href="#EIFNet-Leveraging-Event-Image-Fusion-for-Robust-Semantic-Segmentation" class="headerlink" title="EIFNet: Leveraging Event-Image Fusion for Robust Semantic Segmentation"></a>EIFNet: Leveraging Event-Image Fusion for Robust Semantic Segmentation</h2><p><strong>Authors:Zhijiang Li, Haoran He</strong></p>
<p>Event-based semantic segmentation explores the potential of event cameras, which offer high dynamic range and fine temporal resolution, to achieve robust scene understanding in challenging environments. Despite these advantages, the task remains difficult due to two main challenges: extracting reliable features from sparse and noisy event streams, and effectively fusing them with dense, semantically rich image data that differ in structure and representation. To address these issues, we propose EIFNet, a multi-modal fusion network that combines the strengths of both event and frame-based inputs. The network includes an Adaptive Event Feature Refinement Module (AEFRM), which improves event representations through multi-scale activity modeling and spatial attention. In addition, we introduce a Modality-Adaptive Recalibration Module (MARM) and a Multi-Head Attention Gated Fusion Module (MGFM), which align and integrate features across modalities using attention mechanisms and gated fusion strategies. Experiments on DDD17-Semantic and DSEC-Semantic datasets show that EIFNet achieves state-of-the-art performance, demonstrating its effectiveness in event-based semantic segmentation. </p>
<blockquote>
<p>åŸºäºäº‹ä»¶çš„è¯­ä¹‰åˆ†å‰²æ¢ç´¢äº†äº‹ä»¶ç›¸æœºçš„æ½œåŠ›ï¼Œäº‹ä»¶ç›¸æœºå…·æœ‰é«˜åŠ¨æ€èŒƒå›´å’Œç²¾ç»†çš„æ—¶é—´åˆ†è¾¨ç‡ï¼Œå¯åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ç¯å¢ƒä¸­å®ç°ç¨³å¥çš„åœºæ™¯ç†è§£ã€‚å°½ç®¡å…·æœ‰è¿™äº›ä¼˜åŠ¿ï¼Œä½†ç”±äºä»ç¨€ç–å’Œå˜ˆæ‚çš„äº‹ä»¶æµä¸­æå–å¯é ç‰¹å¾ä»¥åŠå°†å®ƒä»¬ä¸ç»“æ„ä¸åŒä¸”è¯­ä¹‰ä¸°å¯Œçš„å›¾åƒæ•°æ®æœ‰æ•ˆåœ°èåˆè¿™ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼Œè¯¥ä»»åŠ¡ä»ç„¶é¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†EIFNetï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆäº‹ä»¶å’Œå¸§è¾“å…¥ä¼˜åŠ¿çš„å¤šæ¨¡æ€èåˆç½‘ç»œã€‚ç½‘ç»œåŒ…æ‹¬è‡ªé€‚åº”äº‹ä»¶ç‰¹å¾ç»†åŒ–æ¨¡å—ï¼ˆAEFRMï¼‰ï¼Œå®ƒé€šè¿‡å¤šå°ºåº¦æ´»åŠ¨å»ºæ¨¡å’Œç©ºé—´æ³¨æ„åŠ›æ”¹è¿›äº‹ä»¶è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¨¡æ€è‡ªé€‚åº”æ ¡å‡†æ¨¡å—ï¼ˆMARMï¼‰å’Œå¤šå¤´æ³¨æ„åŠ›é—¨æ§èåˆæ¨¡å—ï¼ˆMGFMï¼‰ï¼Œå®ƒä»¬ä½¿ç”¨æ³¨æ„åŠ›å’Œé—¨æ§èåˆç­–ç•¥æ¥å¯¹é½å’Œæ•´åˆè·¨æ¨¡æ€çš„ç‰¹å¾ã€‚åœ¨DDD17-Semanticå’ŒDSEC-Semanticæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEIFNetè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œè¯æ˜äº†å…¶åœ¨åŸºäºäº‹ä»¶çš„è¯­ä¹‰åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21971v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>äº‹ä»¶é©±åŠ¨è¯­ä¹‰åˆ†å‰²æ¢ç´¢äº†äº‹ä»¶ç›¸æœºåœ¨åœºæ™¯ç†è§£ä¸­çš„æ½œåŠ›ã€‚è™½ç„¶å­˜åœ¨æå–å¯é ç‰¹å¾å’Œè·¨æ¨¡æ€èåˆçš„æŒ‘æˆ˜ï¼Œä½†EIFNetå¤šæ¨¡æ€èåˆç½‘ç»œé€šè¿‡è‡ªé€‚åº”äº‹ä»¶ç‰¹å¾ç»†åŒ–æ¨¡å—ã€æ¨¡æ€è‡ªé€‚åº”æ ¡å‡†æ¨¡å—å’Œå¤šå¤´æ³¨æ„åŠ›é—¨æ§èåˆæ¨¡å—ï¼Œå®ç°äº†äº‹ä»¶å’Œå¸§æ•°æ®çš„ä¼˜åŒ–ç»“åˆï¼Œåœ¨DDD17-Semanticå’ŒDSEC-Semanticæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº‹ä»¶ç›¸æœºå…·æœ‰é«˜çš„åŠ¨æ€èŒƒå›´å’Œç²¾ç»†çš„æ—¶é—´åˆ†è¾¨ç‡ï¼Œä¸ºåœºæ™¯ç†è§£å¸¦æ¥äº†æ½œåŠ›ã€‚</li>
<li>äº‹ä»¶é©±åŠ¨è¯­ä¹‰åˆ†å‰²é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä»ç¨€ç–å’Œå˜ˆæ‚çš„äº‹ä»¶æµä¸­æå–å¯é ç‰¹å¾ï¼Œä»¥åŠä¸ç»“æ„ä¸°å¯Œçš„å›¾åƒæ•°æ®æœ‰æ•ˆèåˆã€‚</li>
<li>EIFNetæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€èåˆç½‘ç»œï¼Œç»“åˆäº†äº‹ä»¶å’Œå¸§æ•°æ®çš„ä¼˜åŠ¿ã€‚</li>
<li>AEFRMæ¨¡å—é€šè¿‡å¤šå°ºåº¦æ´»åŠ¨å»ºæ¨¡å’Œç©ºé—´æ³¨æ„åŠ›æé«˜äº†äº‹ä»¶è¡¨ç¤ºã€‚</li>
<li>MARMæ¨¡å—å’ŒMGFMæ¨¡å—ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å’Œé—¨æ§èåˆç­–ç•¥æ¥è·¨æ¨¡æ€å¯¹é½å’Œé›†æˆç‰¹å¾ã€‚</li>
<li>åœ¨DDD17-Semanticå’ŒDSEC-Semanticæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒEIFNetå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21971">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ce87a7d50fb95867deafc34aa4dae779.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-46fafd8ee3ccd0cc3a3ed25b5f6d9060.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Semantic-Segmentation-of-iPS-Cells-Case-Study-on-Model-Complexity-in-Biomedical-Imaging"><a href="#Semantic-Segmentation-of-iPS-Cells-Case-Study-on-Model-Complexity-in-Biomedical-Imaging" class="headerlink" title="Semantic Segmentation of iPS Cells: Case Study on Model Complexity in   Biomedical Imaging"></a>Semantic Segmentation of iPS Cells: Case Study on Model Complexity in   Biomedical Imaging</h2><p><strong>Authors:Maoquan Zhang, Bisser Raytchev, Xiujuan Sun</strong></p>
<p>Medical image segmentation requires not only accuracy but also robustness under challenging imaging conditions. In this study, we show that a carefully configured DeepLabv3 model can achieve high performance in segmenting induced pluripotent stem (iPS) cell colonies, and, under our experimental conditions, outperforms large-scale foundation models such as SAM2 and its medical variant MedSAM2 without structural modifications. These results suggest that, for specialized tasks characterized by subtle, low-contrast boundaries, increased model complexity does not necessarily translate to better performance. Our work revisits the assumption that ever-larger and more generalized architectures are always preferable, and provides evidence that appropriately adapted, simpler models may offer strong accuracy and practical reliability in domain-specific biomedical applications. We also offer an open-source implementation that includes strategies for small datasets and domain-specific encoding, with the aim of supporting further advances in semantic segmentation for regenerative medicine and related fields. </p>
<blockquote>
<p>åŒ»ç–—å›¾åƒåˆ†å‰²ä¸ä»…éœ€è¦å‡†ç¡®æ€§ï¼Œè¿˜éœ€è¦åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æˆåƒæ¡ä»¶ä¸‹ä¿æŒç¨³å¥æ€§ã€‚æœ¬ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡ç²¾å¿ƒé…ç½®çš„DeepLabv3æ¨¡å‹åœ¨åˆ†å‰²è¯±å¯¼å¤šèƒ½å¹²ç»†èƒï¼ˆiPSï¼‰ç»†èƒèŒè½æ–¹é¢å¯ä»¥å®ç°é«˜æ€§èƒ½ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„å®éªŒæ¡ä»¶ä¸‹ï¼Œåœ¨æ— éœ€ç»“æ„ä¿®æ”¹çš„æƒ…å†µä¸‹ï¼Œä¼˜äºå¤§è§„æ¨¡åŸºç¡€æ¨¡å‹ï¼Œå¦‚SAM2åŠå…¶åŒ»å­¦å˜ä½“MedSAM2ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œå¯¹äºå…·æœ‰ç»†å¾®ã€ä½å¯¹æ¯”åº¦è¾¹ç•Œç‰¹å¾çš„ä¸“é¡¹ä»»åŠ¡ï¼Œå¢åŠ æ¨¡å‹å¤æ‚æ€§å¹¶ä¸ä¸€å®šæ„å‘³ç€æ€§èƒ½æ›´å¥½ã€‚æˆ‘ä»¬çš„å·¥ä½œé‡æ–°å®¡è§†äº†æ›´å¤§çš„ã€æ›´é€šç”¨çš„æ¶æ„å§‹ç»ˆæ›´å¯å–è¿™ä¸€å‡è®¾ï¼Œå¹¶æä¾›äº†è¯æ®ï¼Œè¯æ˜é€‚å½“ç®€åŒ–å¹¶é€‚åº”çš„æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸçš„ç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­å¯èƒ½æä¾›å¼ºå¤§çš„å‡†ç¡®æ€§å’Œå®é™…å¯é æ€§ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå¼€æºå®ç°ï¼ŒåŒ…æ‹¬é’ˆå¯¹å°æ•°æ®é›†å’Œç‰¹å®šé¢†åŸŸç¼–ç çš„ç­–ç•¥ï¼Œæ—¨åœ¨æ”¯æŒå†ç”ŸåŒ»å­¦å’Œç›¸å…³é¢†åŸŸçš„è¯­ä¹‰åˆ†å‰²çš„è¿›ä¸€æ­¥è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21608v1">PDF</a> 19th International Conference on Machine Vision Applications MVA2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬ç ”ç©¶å±•ç¤ºäº†ç²¾å¿ƒé…ç½®çš„DeepLabv3æ¨¡å‹åœ¨åˆ†å‰²è¯±å¯¼å¤šèƒ½å¹²ç»†èƒï¼ˆiPSï¼‰ç»†èƒèŒè½æ–¹é¢çš„é«˜æ€§èƒ½è¡¨ç°ã€‚ç›¸è¾ƒäºå¤§è§„æ¨¡çš„åŸºç¡€æ¨¡å‹å¦‚SAM2åŠå…¶åŒ»å­¦å˜ä½“MedSAM2ï¼Œè¯¥æ¨¡å‹åœ¨ç‰¹å®šå®éªŒæ¡ä»¶ä¸‹æ— éœ€ç»“æ„æ€§ä¿®æ”¹ä¾¿å±•ç°å‡ºä¼˜è¶Šæ€§ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¯¹äºç‰¹å¾ç»†å¾®ã€ä½å¯¹æ¯”åº¦çš„ä¸“ä¸šé¢†åŸŸä»»åŠ¡ï¼Œå¢åŠ æ¨¡å‹å¤æ‚æ€§å¹¶ä¸ä¸€å®šæ„å‘³ç€æ€§èƒ½æå‡ã€‚ç ”ç©¶é‡æ–°è€ƒé‡äº†å¤§è§„æ¨¡é€šç”¨æ¶æ„æ€»æ˜¯é¦–é€‰çš„å‡è®¾ï¼Œå¹¶æä¾›è¯æ®è¡¨æ˜é€‚å½“ç®€åŒ–çš„æ¨¡å‹åœ¨ç‰¹å®šç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­å¯èƒ½å…·æœ‰å¼ºå¤§çš„å‡†ç¡®æ€§å’Œå®ç”¨æ€§ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æä¾›åŒ…å«é’ˆå¯¹å°æ•°æ®é›†å’Œç‰¹å®šé¢†åŸŸç¼–ç ç­–ç•¥çš„å¼€æºå®ç°ï¼Œæ—¨åœ¨æ”¯æŒå†ç”ŸåŒ»å­¦å’Œç›¸å…³é¢†åŸŸçš„è¯­ä¹‰åˆ†å‰²çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepLabv3æ¨¡å‹åœ¨åˆ†å‰²è¯±å¯¼å¤šèƒ½å¹²ç»†èƒï¼ˆiPSï¼‰ç»†èƒèŒè½æ–¹é¢è¡¨ç°å‡ºé«˜æ€§èƒ½ã€‚</li>
<li>åœ¨ç‰¹å®šå®éªŒæ¡ä»¶ä¸‹ï¼ŒDeepLabv3æ¨¡å‹ä¼˜äºå¤§è§„æ¨¡çš„åŸºç¡€æ¨¡å‹ï¼Œå¦‚SAM2å’ŒMedSAM2ã€‚</li>
<li>å¯¹äºå…·æœ‰ç»†å¾®ã€ä½å¯¹æ¯”åº¦çš„ä¸“ä¸šé¢†åŸŸä»»åŠ¡ï¼Œå¢åŠ æ¨¡å‹å¤æ‚æ€§å¹¶ä¸ä¸€å®šå¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶é‡æ–°è¯„ä¼°äº†å¤§è§„æ¨¡é€šç”¨æ¶æ„çš„å¿…è¦æ€§ï¼Œå¼ºè°ƒé€‚å½“ç®€åŒ–çš„æ¨¡å‹åœ¨ç‰¹å®šç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­çš„ä¼˜åŠ¿ã€‚</li>
<li>ç ”ç©¶æä¾›äº†ä¸€ä¸ªåŒ…å«é’ˆå¯¹å°æ•°æ®é›†å’Œç‰¹å®šé¢†åŸŸç¼–ç ç­–ç•¥çš„å¼€æºå®ç°ã€‚</li>
<li>è¯¥ç ”ç©¶æ”¯æŒå†ç”ŸåŒ»å­¦å’Œç›¸å…³é¢†åŸŸçš„è¯­ä¹‰åˆ†å‰²çš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21608">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8e87f14ec6885586249392bd244b7b60.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-84d9642ce16b584c3532f561fa244da6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f4451222ff3b4febb53467c771cdf53e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Emerging-Trends-in-Pseudo-Label-Refinement-for-Weakly-Supervised-Semantic-Segmentation-with-Image-Level-Supervision"><a href="#Emerging-Trends-in-Pseudo-Label-Refinement-for-Weakly-Supervised-Semantic-Segmentation-with-Image-Level-Supervision" class="headerlink" title="Emerging Trends in Pseudo-Label Refinement for Weakly Supervised   Semantic Segmentation with Image-Level Supervision"></a>Emerging Trends in Pseudo-Label Refinement for Weakly Supervised   Semantic Segmentation with Image-Level Supervision</h2><p><strong>Authors:Zheyuan Zhang, Wang Zhang</strong></p>
<p>Unlike fully supervised semantic segmentation, weakly supervised semantic segmentation (WSSS) relies on weaker forms of supervision to perform dense prediction tasks. Among the various types of weak supervision, WSSS with image level annotations is considered both the most challenging and the most practical, attracting significant research attention. Therefore, in this review, we focus on WSSS with image level annotations. Additionally, this review concentrates on mainstream research directions, deliberately omitting less influential branches.   Given the rapid development of new methods and the limitations of existing surveys in capturing recent trends, there is a pressing need for an updated and comprehensive review. Our goal is to fill this gap by synthesizing the latest advancements and state-of-the-art techniques in WSSS with image level labels.   Basically, we provide a comprehensive review of recent advancements in WSSS with image level labels, categorizing existing methods based on the types and levels of additional supervision involved. We also examine the challenges of applying advanced methods to domain specific datasets in WSSS,a topic that remains underexplored. Finally, we discuss the current challenges, evaluate the limitations of existing approaches, and outline several promising directions for future research. This review is intended for researchers who are already familiar with the fundamental concepts of WSSS and are seeking to deepen their understanding of current advances and methodological innovations. </p>
<blockquote>
<p>ä¸å®Œå…¨ç›‘ç£è¯­ä¹‰åˆ†å‰²ä¸åŒï¼Œå¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰ä¾èµ–äºè¾ƒå¼±çš„ç›‘ç£å½¢å¼æ¥æ‰§è¡Œå¯†é›†é¢„æµ‹ä»»åŠ¡ã€‚åœ¨å¤šç§ç±»å‹çš„å¼±ç›‘ç£ä¸­ï¼Œä½¿ç”¨å›¾åƒçº§åˆ«æ³¨é‡Šçš„WSSSè¢«è®¤ä¸ºæ˜¯æœ€å…·æŒ‘æˆ˜æ€§å’Œæœ€å®ç”¨çš„ï¼Œå¼•èµ·äº†ç ”ç©¶äººå‘˜çš„å¹¿æ³›å…³æ³¨ã€‚å› æ­¤ï¼Œåœ¨æœ¬æ¬¡ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºä½¿ç”¨å›¾åƒçº§åˆ«æ³¨é‡Šçš„WSSSã€‚æ­¤å¤–ï¼Œæœ¬æ¬¡ç»¼è¿°é›†ä¸­åœ¨ä¸»è¦ç ”ç©¶æ–¹å‘ä¸Šï¼Œæ•…æ„çœç•¥äº†å½±å“è¾ƒå°çš„åˆ†æ”¯ã€‚é‰´äºæ–°æ–¹æ³•çš„å¿«é€Ÿå‘å±•ä»¥åŠç°æœ‰è°ƒæŸ¥åœ¨æ•æ‰æœ€æ–°è¶‹åŠ¿æ–¹é¢çš„å±€é™æ€§ï¼Œè¿«åˆ‡éœ€è¦è¿›è¡Œæ›´æ–°å’Œå…¨é¢çš„ç»¼è¿°ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡ç»¼åˆæœ€æ–°çš„å‘å±•æƒ…å†µå’Œæœ€å…ˆè¿›çš„å›¾åƒçº§æ ‡ç­¾WSSSæŠ€æœ¯æ¥å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬å…¨é¢å›é¡¾äº†ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾çš„WSSSçš„æœ€æ–°å‘å±•ï¼Œæ ¹æ®æ‰€æ¶‰åŠçš„å…¶ä»–ç›‘ç£å’Œçº§åˆ«å¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†å°†é«˜çº§æ–¹æ³•åº”ç”¨äºç‰¹å®šé¢†åŸŸæ•°æ®é›†çš„WSSSçš„æŒ‘æˆ˜ï¼Œè¿™æ˜¯ä¸€ä¸ªå°šæœªè¢«å……åˆ†æ¢ç´¢çš„ä¸»é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†å½“å‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œè¯„ä¼°äº†ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥ç ”ç©¶çš„å‡ ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚æœ¬ç»¼è¿°æ—¨åœ¨å¸®åŠ©å·²ç»ç†Ÿæ‚‰WSSSåŸºæœ¬æ¦‚å¿µçš„ç ”ç©¶äººå‘˜åŠ æ·±å¯¹å½“å‰è¿›å±•å’Œæ–¹æ³•åˆ›æ–°çš„ç†è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21587v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ç»¼è¿°äº†åŸºäºå›¾åƒçº§åˆ«æ ‡æ³¨çš„å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²ï¼ˆWSSSï¼‰çš„æœ€æ–°è¿›å±•å’Œå‰æ²¿æŠ€æœ¯ï¼Œé‡ç‚¹ä»‹ç»äº†ç°æœ‰æ–¹æ³•çš„åˆ†ç±»ã€é¢ä¸´çš„æŒ‘æˆ˜ä»¥åŠæœªæ¥ç ”ç©¶æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>WSSSä¾èµ–äºå¼±å½¢å¼çš„ç›‘ç£æ¥æ‰§è¡Œå¯†é›†é¢„æµ‹ä»»åŠ¡ï¼Œå…¶ä¸­åŸºäºå›¾åƒçº§åˆ«æ ‡æ³¨çš„WSSSæœ€å…·æŒ‘æˆ˜æ€§å’Œå®ç”¨æ€§ã€‚</li>
<li>ç°æœ‰æ–‡çŒ®ä¸­å¯¹WSSSçš„ç ”ç©¶å¤šé›†ä¸­åœ¨é€šç”¨æ•°æ®é›†ä¸Šï¼Œä½†å¯¹ç‰¹å®šæ•°æ®é›†çš„åº”ç”¨ä»ç¼ºä¹æ·±å…¥ç ”ç©¶ã€‚</li>
<li>å½“å‰çš„WSSSæ–¹æ³•åŸºäºé¢å¤–ç›‘ç£çš„ç±»å‹å’Œç¨‹åº¦è¿›è¡Œåˆ†ç±»ã€‚</li>
<li>åº”ç”¨å…ˆè¿›æ–¹æ³•åˆ°ç‰¹å®šé¢†åŸŸçš„WSSSæ•°æ®é›†å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰æ–¹æ³•çš„å±€é™æ€§åŒ…æ‹¬éš¾ä»¥ç²¾ç¡®æ ‡æ³¨å’Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›æœ‰é™ç­‰ã€‚</li>
<li>æœªæ¥ç ”ç©¶æ–¹å‘åŒ…æ‹¬æ”¹è¿›æ¨¡å‹æ¶æ„ä»¥æé«˜æ€§èƒ½ã€æ¢ç´¢æ›´æœ‰æ•ˆçš„å¼±ç›‘ç£ç­–ç•¥ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21587">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-edf2bc729a3813e7e7ad3a2c97320dcd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e1e7296a131b62ebc3736c761e7fd5d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fbdda4cff6727b21d44ea7e907be553c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bdd8ca7a89e6c2732a9890898f579f7.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="An-Angular-Temporal-Interaction-Network-for-Light-Field-Object-Tracking-in-Low-Light-Scenes"><a href="#An-Angular-Temporal-Interaction-Network-for-Light-Field-Object-Tracking-in-Low-Light-Scenes" class="headerlink" title="An Angular-Temporal Interaction Network for Light Field Object Tracking   in Low-Light Scenes"></a>An Angular-Temporal Interaction Network for Light Field Object Tracking   in Low-Light Scenes</h2><p><strong>Authors:Mianzhao Wang, Fan Shi, Xu Cheng, Feifei Zhang, Shengyong Chen</strong></p>
<p>High-quality 4D light field representation with efficient angular feature modeling is crucial for scene perception, as it can provide discriminative spatial-angular cues to identify moving targets. However, recent developments still struggle to deliver reliable angular modeling in the temporal domain, particularly in complex low-light scenes. In this paper, we propose a novel light field epipolar-plane structure image (ESI) representation that explicitly defines the geometric structure within the light field. By capitalizing on the abrupt changes in the angles of light rays within the epipolar plane, this representation can enhance visual expression in low-light scenes and reduce redundancy in high-dimensional light fields. We further propose an angular-temporal interaction network (ATINet) for light field object tracking that learns angular-aware representations from the geometric structural cues and angular-temporal interaction cues of light fields. Furthermore, ATINet can also be optimized in a self-supervised manner to enhance the geometric feature interaction across the temporal domain. Finally, we introduce a large-scale light field low-light dataset for object tracking. Extensive experimentation demonstrates that ATINet achieves state-of-the-art performance in single object tracking. Furthermore, we extend the proposed method to multiple object tracking, which also shows the effectiveness of high-quality light field angular-temporal modeling. </p>
<blockquote>
<p>é«˜è´¨é‡çš„å››ç»´å…‰åœºè¡¨ç¤ºä¸æœ‰æ•ˆçš„è§’åº¦ç‰¹å¾å»ºæ¨¡å¯¹äºåœºæ™¯æ„ŸçŸ¥è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒå¯ä»¥æä¾›ç”¨äºè¯†åˆ«ç§»åŠ¨ç›®æ ‡çš„ç©ºé—´è§’åº¦çº¿ç´¢ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„è¿›å±•åœ¨æ—¶é—´ä¸Šå¯é çš„è§’åº¦å»ºæ¨¡æ–¹é¢ä»ç„¶é¢ä¸´å›°éš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„ä½å…‰åœºæ™¯ä¸­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å…‰åœºæå¹³é¢ç»“æ„å›¾åƒï¼ˆESIï¼‰è¡¨ç¤ºï¼Œå®ƒæ˜ç¡®åœ°å®šä¹‰äº†å…‰åœºå†…çš„å‡ ä½•ç»“æ„ã€‚é€šè¿‡åˆ©ç”¨æå¹³é¢å†…å…‰çº¿è§’åº¦çš„æ€¥å‰§å˜åŒ–ï¼Œè¿™ç§è¡¨ç¤ºå½¢å¼å¯ä»¥æé«˜ä½å…‰åœºæ™¯ä¸­çš„è§†è§‰è¡¨è¾¾æ•ˆæœï¼Œå¹¶å‡å°‘é«˜ç»´å…‰åœºçš„å†—ä½™ä¿¡æ¯ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§ç”¨äºå…‰åœºå¯¹è±¡è·Ÿè¸ªçš„è§’åº¦æ—¶é—´äº¤äº’ç½‘ç»œï¼ˆATINetï¼‰ï¼Œè¯¥ç½‘ç»œä»å…‰åœºçš„å‡ ä½•ç»“æ„çº¿ç´¢å’Œè§’åº¦æ—¶é—´äº¤äº’çº¿ç´¢ä¸­å­¦ä¹ è§’åº¦æ„ŸçŸ¥è¡¨ç¤ºã€‚æ­¤å¤–ï¼ŒATINetè¿˜å¯ä»¥é‡‡ç”¨è‡ªç›‘ç£çš„æ–¹å¼è¿›è¡Œä¼˜åŒ–ï¼Œä»¥å¢å¼ºæ—¶é—´åŸŸä¸Šçš„å‡ ä½•ç‰¹å¾äº¤äº’ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç”¨äºå¯¹è±¡è·Ÿè¸ªçš„å¤§è§„æ¨¡å…‰åœºä½å…‰æ•°æ®é›†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒATINetåœ¨å•ç›®æ ‡è·Ÿè¸ªæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ‰€æå‡ºçš„æ–¹æ³•æ‰©å±•åˆ°å¤šç›®æ ‡è·Ÿè¸ªï¼Œè¿™ä¹Ÿè¡¨æ˜äº†é«˜è´¨é‡å…‰åœºè§’åº¦æ—¶é—´å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21460v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºé«˜æ•ˆè§’ç‰¹å¾å»ºæ¨¡çš„é«˜è´¨é‡å››ç»´å…‰åœºè¡¨ç¤ºå¯¹åœºæ™¯æ„ŸçŸ¥è‡³å…³é‡è¦ï¼Œå¯ä¸ºç§»åŠ¨ç›®æ ‡è¯†åˆ«æä¾›è¾¨åˆ«æ€§ç©ºé—´è§’çº¿ç´¢ã€‚ç„¶è€Œï¼Œè¿‘æœŸå‘å±•åœ¨å¤æ‚ä½å…‰åœºæ™¯ä¸­çš„æ—¶é—´åŸŸè§’å»ºæ¨¡ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹å…‰åœºæå¹³é¢ç»“æ„å›¾åƒï¼ˆESIï¼‰è¡¨ç¤ºæ–¹æ³•ï¼Œæ˜ç¡®å®šä¹‰äº†å…‰åœºå†…çš„å‡ ä½•ç»“æ„ã€‚é€šè¿‡åˆ©ç”¨æå¹³é¢å†…å…‰çº¿è§’åº¦çš„çªå˜ï¼Œè¯¥è¡¨ç¤ºæ–¹æ³•å¯æé«˜ä½å…‰åœºæ™¯çš„è§†è§‰è¡¨è¾¾æ•ˆæœï¼Œå¹¶å‡å°‘é«˜ç»´å…‰åœºçš„å†—ä½™ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§ç”¨äºå…‰åœºç›®æ ‡è·Ÿè¸ªçš„è§’æ—¶é—´äº¤äº’ç½‘ç»œï¼ˆATINetï¼‰ï¼Œä»å…‰åœºçš„å‡ ä½•ç»“æ„çº¿ç´¢å’Œè§’æ—¶é—´äº¤äº’çº¿ç´¢ä¸­å­¦ä¹ è§’æ„ŸçŸ¥è¡¨ç¤ºã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒATINetè¿˜å¯ä»¥é€šè¿‡è‡ªç›‘ç£æ–¹å¼è¿›è¡Œä¼˜åŒ–ï¼Œå¢å¼ºæ—¶é—´åŸŸå†…çš„å‡ ä½•ç‰¹å¾äº¤äº’ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå¤§å‹å…‰åœºä½å…‰æ•°æ®é›†ç”¨äºç›®æ ‡è·Ÿè¸ªã€‚å®éªŒè¡¨æ˜ï¼ŒATINetåœ¨å•ç›®æ ‡è·Ÿè¸ªæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œå¹¶å°†æ­¤æ–¹æ³•æ‰©å±•åˆ°å¤šç›®æ ‡è·Ÿè¸ªä¹Ÿæ˜¾ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜è´¨é‡å››ç»´å…‰åœºè¡¨ç¤ºå¯¹äºåœºæ™¯æ„ŸçŸ¥è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿæä¾›è¯†åˆ«ç§»åŠ¨ç›®æ ‡çš„è¾¨åˆ«æ€§ç©ºé—´è§’çº¿ç´¢ã€‚</li>
<li>æå¹³é¢ç»“æ„å›¾åƒï¼ˆESIï¼‰è¡¨ç¤ºæ–¹æ³•æé«˜äº†ä½å…‰åœºæ™¯çš„è§†è§‰è¡¨è¾¾æ•ˆæœå¹¶å‡å°‘äº†é«˜ç»´å…‰åœºçš„å†—ä½™ä¿¡æ¯ã€‚</li>
<li>è§’æ—¶é—´äº¤äº’ç½‘ç»œï¼ˆATINetï¼‰èƒ½ä»å…‰åœºçš„å‡ ä½•ç»“æ„å’Œè§’æ—¶é—´äº¤äº’çº¿ç´¢ä¸­å­¦ä¹ è§’æ„ŸçŸ¥è¡¨ç¤ºã€‚</li>
<li>ATINeté€šè¿‡è‡ªç›‘ç£ä¼˜åŒ–æ–¹å¼æé«˜äº†æ—¶é—´åŸŸå†…çš„å‡ ä½•ç‰¹å¾äº¤äº’èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥çš„å¤§å‹å…‰åœºä½å…‰æ•°æ®é›†ç”¨äºç›®æ ‡è·Ÿè¸ªç ”ç©¶ã€‚</li>
<li>å®éªŒæ˜¾ç¤ºATINetåœ¨å•ç›®æ ‡è·Ÿè¸ªæ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21460">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fd95e938410dac2d103c32f4120876e6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8f3306bc01ad6d0bc945599d874fa1a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7eb84cb5cf6c8e4e93842ea23c97b02c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3c2676943059a5257bddc1fc8d5ec35e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4c019fdd33c84b2a379b01f3339bb4b6.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Dual-Cross-image-Semantic-Consistency-with-Self-aware-Pseudo-Labeling-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Dual-Cross-image-Semantic-Consistency-with-Self-aware-Pseudo-Labeling-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Dual Cross-image Semantic Consistency with Self-aware Pseudo Labeling   for Semi-supervised Medical Image Segmentation"></a>Dual Cross-image Semantic Consistency with Self-aware Pseudo Labeling   for Semi-supervised Medical Image Segmentation</h2><p><strong>Authors:Han Wu, Chong Wang, Zhiming Cui</strong></p>
<p>Semi-supervised learning has proven highly effective in tackling the challenge of limited labeled training data in medical image segmentation. In general, current approaches, which rely on intra-image pixel-wise consistency training via pseudo-labeling, overlook the consistency at more comprehensive semantic levels (e.g., object region) and suffer from severe discrepancy of extracted features resulting from an imbalanced number of labeled and unlabeled data. To overcome these limitations, we present a new \underline{Du}al \underline{C}ross-\underline{i}mage \underline{S}emantic \underline{C}onsistency (DuCiSC) learning framework, for semi-supervised medical image segmentation. Concretely, beyond enforcing pixel-wise semantic consistency, DuCiSC proposes dual paradigms to encourage region-level semantic consistency across: 1) labeled and unlabeled images; and 2) labeled and fused images, by explicitly aligning their prototypes. Relying on the dual paradigms, DuCiSC can effectively establish consistent cross-image semantics via prototype representations, thereby addressing the feature discrepancy issue. Moreover, we devise a novel self-aware confidence estimation strategy to accurately select reliable pseudo labels, allowing for exploiting the training dynamics of unlabeled data. Our DuCiSC method is extensively validated on four datasets, including two popular binary benchmarks in segmenting the left atrium and pancreas, a multi-class Automatic Cardiac Diagnosis Challenge dataset, and a challenging scenario of segmenting the inferior alveolar nerve that features complicated anatomical structures, showing superior segmentation results over previous state-of-the-art approaches. Our code is publicly available at \href{<a target="_blank" rel="noopener" href="https://github.com/ShanghaiTech-IMPACT/DuCiSC%7D%7Bhttps://github.com/ShanghaiTech-IMPACT/DuCiSC%7D">https://github.com/ShanghaiTech-IMPACT/DuCiSC}{https://github.com/ShanghaiTech-IMPACT/DuCiSC}</a>. </p>
<blockquote>
<p>åœ¨åŠç›‘ç£å­¦ä¹ åœ¨å¤„ç†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æœ‰é™çš„æ ‡è®°è®­ç»ƒæ•°æ®æŒ‘æˆ˜æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆæ•ˆã€‚é€šå¸¸ï¼Œå½“å‰çš„æ–¹æ³•ä¾èµ–äºé€šè¿‡ä¼ªæ ‡ç­¾è¿›è¡Œå›¾åƒå†…åƒç´ çº§çš„ä¸€è‡´æ€§è®­ç»ƒï¼Œè€Œå¿½è§†äº†æ›´å…¨é¢çš„è¯­ä¹‰çº§åˆ«çš„ä¸€è‡´æ€§ï¼ˆä¾‹å¦‚ï¼Œå¯¹è±¡åŒºåŸŸï¼‰ï¼Œå¹¶å› æ ‡è®°å’Œæ— æ ‡è®°æ•°æ®æ•°é‡ä¸å¹³è¡¡è€Œé­å—ç‰¹å¾æå–å·®å¼‚çš„é—®é¢˜ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹æ³•â€”â€”åŒé‡äº¤å‰å›¾åƒè¯­ä¹‰ä¸€è‡´æ€§ï¼ˆDuCiSCï¼‰å­¦ä¹ æ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œé™¤äº†å¼ºåˆ¶åƒç´ çº§çš„è¯­ä¹‰ä¸€è‡´æ€§å¤–ï¼ŒDuCiSCè¿˜æå‡ºäº†åŒé‡èŒƒå¼æ¥é¼“åŠ±åŒºåŸŸçº§åˆ«çš„è¯­ä¹‰ä¸€è‡´æ€§è·¨è¶Šï¼š1ï¼‰æ ‡è®°å’Œæ— æ ‡è®°å›¾åƒï¼›ä»¥åŠ2ï¼‰æ ‡è®°å’Œèåˆå›¾åƒï¼Œé€šè¿‡æ˜ç¡®åœ°å¯¹å…¶åŸå‹è¿›è¡Œå¯¹é½ã€‚ä¾é åŒé‡èŒƒå¼ï¼ŒDuCiSCå¯ä»¥æœ‰æ•ˆåœ°å»ºç«‹ä¸€è‡´çš„è·¨å›¾åƒè¯­ä¹‰é€šè¿‡åŸå‹è¡¨ç¤ºï¼Œä»è€Œè§£å†³ç‰¹å¾å·®å¼‚é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹çš„è‡ªæ„ŸçŸ¥ç½®ä¿¡åº¦ä¼°è®¡ç­–ç•¥ï¼Œä»¥å‡†ç¡®é€‰æ‹©å¯é çš„ä¼ªæ ‡ç­¾ï¼Œä»è€Œèƒ½å¤Ÿåˆ©ç”¨æ— æ ‡ç­¾æ•°æ®çš„è®­ç»ƒåŠ¨æ€ã€‚æˆ‘ä»¬çš„DuCiSCæ–¹æ³•åœ¨å››ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›éªŒè¯ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæµè¡Œçš„å·¦å¿ƒæˆ¿å’Œèƒ°è…ºåˆ†å‰²äºŒå…ƒåŸºå‡†æµ‹è¯•ã€å¤šç±»åˆ«çš„è‡ªåŠ¨å¿ƒè„è¯Šæ–­æŒ‘æˆ˜èµ›æ•°æ®é›†ä»¥åŠä¸€ä¸ªå¤æ‚çš„ä¸‹ç‰™æ§½ç¥ç»åˆ†å‰²åœºæ™¯ï¼Œè¯¥åœºæ™¯å…·æœ‰å¤æ‚çš„è§£å‰–ç»“æ„ï¼Œæ˜¾ç¤ºå‡ºä¼˜äºä»¥å‰æœ€å…ˆè¿›çš„åˆ†å‰²æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ShanghaiTech-IMPACT/DuCiSC%E3%80%82">https://github.com/ShanghaiTech-IMPACT/DuCiSCã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21440v1">PDF</a> IEEE TMI</p>
<p><strong>Summary</strong></p>
<p>åŠç›‘ç£å­¦ä¹ åœ¨å¤„ç†åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­æœ‰é™æ ‡è®°è®­ç»ƒæ•°æ®çš„é—®é¢˜æ—¶è¡¨ç°å‡ºé«˜åº¦æœ‰æ•ˆæ€§ã€‚å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–ä¼ªæ ‡ç­¾è¿›è¡Œå›¾åƒå†…åƒç´ çº§ä¸€è‡´æ€§è®­ç»ƒï¼Œä½†å¿½ç•¥äº†æ›´å…¨é¢çš„è¯­ä¹‰çº§åˆ«çš„ä¸€è‡´æ€§ï¼Œå¹¶å—åˆ°ç‰¹å¾æå–ä¸å¹³è¡¡å¯¼è‡´çš„ç‰¹å¾å·®å¼‚é—®é¢˜çš„å›°æ‰°ã€‚é’ˆå¯¹è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åŒè·¨å›¾åƒè¯­ä¹‰ä¸€è‡´æ€§ï¼ˆDuCiSCï¼‰åŠç›‘ç£åŒ»å­¦å›¾åƒåˆ†å‰²å­¦ä¹ æ¡†æ¶ã€‚é™¤äº†å¼ºåˆ¶åƒç´ çº§è¯­ä¹‰ä¸€è‡´æ€§å¤–ï¼ŒDuCiSCè¿˜æå‡ºäº†ä¸¤ç§èŒƒå¼æ¥é¼“åŠ±è·¨æ ‡è®°å›¾åƒå’Œæ— æ ‡è®°å›¾åƒä»¥åŠæ ‡è®°å›¾åƒå’Œèåˆå›¾åƒçš„åŒºåŸŸçº§è¯­ä¹‰ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°å‹çš„è‡ªæ„ŸçŸ¥ç½®ä¿¡åº¦ä¼°è®¡ç­–ç•¥ï¼Œä»¥å‡†ç¡®é€‰æ‹©å¯é çš„ä¼ªæ ‡ç­¾ï¼Œä»è€Œåˆ©ç”¨æ— æ ‡ç­¾æ•°æ®çš„è®­ç»ƒåŠ¨æ€ã€‚DuCiSCæ–¹æ³•åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„éªŒè¯ç»“æœå‡ä¼˜äºå…ˆå‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæµè¡Œçš„å·¦å¿ƒæˆ¿å’Œèƒ°è…ºåˆ†å‰²äºŒå…ƒåŸºå‡†æµ‹è¯•ã€å¤šç±»è‡ªåŠ¨å¿ƒè„è¯Šæ–­æŒ‘æˆ˜æ•°æ®é›†ä»¥åŠå¤æ‚çš„ä¸‹é¢Œç¥ç»åˆ†å‰²åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŠç›‘ç£å­¦ä¹ åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­èƒ½æœ‰æ•ˆè§£å†³æ ‡è®°æ•°æ®æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦ä¾èµ–ä¼ªæ ‡ç­¾è¿›è¡Œåƒç´ çº§ä¸€è‡´æ€§è®­ç»ƒï¼Œä½†å¿½ç•¥äº†æ›´å…¨é¢è¯­ä¹‰çº§åˆ«çš„ä¸€è‡´æ€§ã€‚</li>
<li>DuCiSCå­¦ä¹ æ¡†æ¶é€šè¿‡åŒé‡èŒƒå¼é¼“åŠ±åŒºåŸŸçº§è¯­ä¹‰ä¸€è‡´æ€§ï¼Œæé«˜ç‰¹å¾æå–çš„å¹³è¡¡æ€§ã€‚</li>
<li>DuCiSCé‡‡ç”¨æ–°å‹è‡ªæ„ŸçŸ¥ç½®ä¿¡åº¦ä¼°è®¡ç­–ç•¥ï¼Œé€‰æ‹©å¯é çš„ä¼ªæ ‡ç­¾ä»¥åˆ©ç”¨æ— æ ‡ç­¾æ•°æ®çš„è®­ç»ƒåŠ¨æ€ã€‚</li>
<li>DuCiSCåœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºå…¶ä»–æ–¹æ³•ï¼ŒåŒ…æ‹¬å·¦å¿ƒæˆ¿å’Œèƒ°è…ºåˆ†å‰²ã€è‡ªåŠ¨å¿ƒè„è¯Šæ–­æŒ‘æˆ˜å’Œä¸‹é¢Œç¥ç»åˆ†å‰²ç­‰åœºæ™¯ã€‚</li>
<li>DuCiSCæ–¹æ³•å·²å…¬å¼€å¯ç”¨ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨å’Œæ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21440">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4e9689c44e1f8c735590b033e21f41b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e4f9813195811217391b06e01f3e5847.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0e8e57878a606add81b6bea8581d3376.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Exploring-Probabilistic-Modeling-Beyond-Domain-Generalization-for-Semantic-Segmentation"><a href="#Exploring-Probabilistic-Modeling-Beyond-Domain-Generalization-for-Semantic-Segmentation" class="headerlink" title="Exploring Probabilistic Modeling Beyond Domain Generalization for   Semantic Segmentation"></a>Exploring Probabilistic Modeling Beyond Domain Generalization for   Semantic Segmentation</h2><p><strong>Authors:I-Hsiang Chen, Hua-En Chang, Wei-Ting Chen, Jenq-Neng Hwang, Sy-Yen Kuo</strong></p>
<p>Domain Generalized Semantic Segmentation (DGSS) is a critical yet challenging task, as domain shifts in unseen environments can severely compromise model performance. While recent studies enhance feature alignment by projecting features into the source domain, they often neglect intrinsic latent domain priors, leading to suboptimal results. In this paper, we introduce PDAF, a Probabilistic Diffusion Alignment Framework that enhances the generalization of existing segmentation networks through probabilistic diffusion modeling. PDAF introduces a Latent Domain Prior (LDP) to capture domain shifts and uses this prior as a conditioning factor to align both source and unseen target domains. To achieve this, PDAF integrates into a pre-trained segmentation model and utilizes paired source and pseudo-target images to simulate latent domain shifts, enabling LDP modeling. The framework comprises three modules: the Latent Prior Extractor (LPE) predicts the LDP by supervising domain shifts; the Domain Compensation Module (DCM) adjusts feature representations to mitigate domain shifts; and the Diffusion Prior Estimator (DPE) leverages a diffusion process to estimate the LDP without requiring paired samples. This design enables PDAF to iteratively model domain shifts, progressively refining feature representations to enhance generalization under complex target conditions. Extensive experiments validate the effectiveness of PDAF across diverse and challenging urban scenes. </p>
<blockquote>
<p>é¢†åŸŸæ³›åŒ–è¯­ä¹‰åˆ†å‰²ï¼ˆDGSSï¼‰æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºæœªçŸ¥ç¯å¢ƒä¸­çš„é¢†åŸŸåç§»å¯èƒ½ä¼šä¸¥é‡æŸå®³æ¨¡å‹æ€§èƒ½ã€‚è™½ç„¶æœ€è¿‘çš„ç ”ç©¶é€šè¿‡å°†ç‰¹å¾æŠ•å½±åˆ°æºåŸŸæ¥å¢å¼ºç‰¹å¾å¯¹é½ï¼Œä½†å®ƒä»¬å¾€å¾€å¿½ç•¥äº†å†…åœ¨çš„æ½œåœ¨é¢†åŸŸå…ˆéªŒï¼Œä»è€Œå¯¼è‡´ç»“æœä¸ä½³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†PDAFï¼Œå³æ¦‚ç‡æ‰©æ•£å¯¹é½æ¡†æ¶ï¼Œå®ƒé€šè¿‡æ¦‚ç‡æ‰©æ•£å»ºæ¨¡å¢å¼ºç°æœ‰åˆ†å‰²ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›ã€‚PDAFå¼•å…¥æ½œåœ¨é¢†åŸŸå…ˆéªŒï¼ˆLDPï¼‰æ¥æ•è·é¢†åŸŸåç§»ï¼Œå¹¶å°†æ­¤å…ˆéªŒä½œä¸ºæ¡ä»¶å› ç´ æ¥å¯¹é½æºåŸŸå’Œæœªè§è¿‡çš„ç›®æ ‡åŸŸã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼ŒPDAFé›†æˆåˆ°é¢„è®­ç»ƒçš„åˆ†å‰²æ¨¡å‹ä¸­ï¼Œå¹¶åˆ©ç”¨æˆå¯¹çš„æºå’Œä¼ªç›®æ ‡å›¾åƒæ¥æ¨¡æ‹Ÿæ½œåœ¨é¢†åŸŸåç§»ï¼Œä»è€Œå®ç°LDPå»ºæ¨¡ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šæ½œåœ¨å…ˆéªŒæå–å™¨ï¼ˆLPEï¼‰é€šè¿‡ç›‘ç£é¢†åŸŸåç§»æ¥é¢„æµ‹LDPï¼›é¢†åŸŸè¡¥å¿æ¨¡å—ï¼ˆDCMï¼‰è°ƒæ•´ç‰¹å¾è¡¨ç¤ºä»¥å‡è½»é¢†åŸŸåç§»ï¼›æ‰©æ•£å…ˆéªŒä¼°è®¡å™¨ï¼ˆDPEï¼‰åˆ©ç”¨æ‰©æ•£è¿‡ç¨‹æ¥ä¼°è®¡LDPï¼Œæ— éœ€é…å¯¹æ ·æœ¬ã€‚è¿™ç§è®¾è®¡ä½¿PDAFèƒ½å¤Ÿè¿­ä»£åœ°æ¨¡æ‹Ÿé¢†åŸŸåç§»ï¼Œé€æ¸ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºï¼Œåœ¨å¤æ‚çš„ç›®æ ‡æ¡ä»¶ä¸‹æé«˜æ³›åŒ–èƒ½åŠ›ã€‚å¤§é‡å®éªŒéªŒè¯äº†PDAFåœ¨å¤šæ ·ä¸”å…·æŒ‘æˆ˜æ€§çš„åŸå¸‚åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21367v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºPDAFçš„Probabilistic Diffusion Alignment Frameworkï¼Œç”¨äºå¢å¼ºç°æœ‰åˆ†å‰²ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¦‚ç‡æ‰©æ•£å»ºæ¨¡æ•æ‰é¢†åŸŸè¿ç§»å¹¶ç”¨ä½œå¯¹é½æºåŸŸå’Œæœªè§ç›®æ ‡åŸŸçš„æ¡ä»¶å› ç´ ã€‚å®ƒåŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šLatent Prior Extractorï¼ˆLPEï¼‰ã€Domain Compensation Moduleï¼ˆDCMï¼‰å’ŒDiffusion Prior Estimatorï¼ˆDPEï¼‰ã€‚é€šè¿‡æ¨¡æ‹Ÿæ½œåœ¨é¢†åŸŸè¿ç§»å¹¶åˆ©ç”¨é…å¯¹çš„æºå’Œä¼ªç›®æ ‡å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿé€æ­¥å»ºæ¨¡é¢†åŸŸè¿ç§»å¹¶ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œæé«˜å¤æ‚ç›®æ ‡æ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¯æ˜ï¼ŒPDAFåœ¨å¤šæ ·ä¸”å¤æ‚çš„åŸå¸‚åœºæ™¯ä¸‹å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PDAFæ˜¯ä¸€ç§å¢å¼ºç°æœ‰åˆ†å‰²ç½‘ç»œæ³›åŒ–èƒ½åŠ›çš„Probabilistic Diffusion Alignment Frameworkã€‚</li>
<li>PDAFé€šè¿‡æ¦‚ç‡æ‰©æ•£å»ºæ¨¡æ•æ‰é¢†åŸŸè¿ç§»ï¼Œå¹¶å¼•å…¥Latent Domain Priorï¼ˆLDPï¼‰ä½œä¸ºå¯¹é½æºåŸŸå’Œæœªè§ç›®æ ‡åŸŸçš„æ¡ä»¶å› ç´ ã€‚</li>
<li>PDAFåŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šLatent Prior Extractorï¼ˆLPEï¼‰ç”¨äºé¢„æµ‹LDPå¹¶ç›‘æµ‹é¢†åŸŸè¿ç§»ï¼›Domain Compensation Moduleï¼ˆDCMï¼‰ç”¨äºè°ƒæ•´ç‰¹å¾è¡¨ç¤ºä»¥å‡è½»é¢†åŸŸè¿ç§»çš„å½±å“ï¼›Diffusion Prior Estimatorï¼ˆDPEï¼‰åˆ©ç”¨æ‰©æ•£è¿‡ç¨‹ä¼°è®¡LDPï¼Œæ— éœ€é…å¯¹æ ·æœ¬ã€‚</li>
<li>PDAFé€šè¿‡æ¨¡æ‹Ÿæ½œåœ¨é¢†åŸŸè¿ç§»å¹¶åˆ©ç”¨é…å¯¹çš„æºå’Œä¼ªç›®æ ‡å›¾åƒè¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿé€æ­¥å»ºæ¨¡é¢†åŸŸè¿ç§»å¹¶ä¼˜åŒ–ç‰¹å¾è¡¨ç¤ºã€‚</li>
<li>PDAFè®¾è®¡ç”¨äºå¢å¼ºåœ¨å¤æ‚ç›®æ ‡æ¡ä»¶ä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒPDAFåœ¨å¤šæ ·ä¸”å¤æ‚çš„åŸå¸‚åœºæ™¯ä¸‹å…·æœ‰æ˜¾è‘—æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21367">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3ef8cbaee87ca19789b4825a51e8d6c3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39778e8b8a4b8c89087c1225da3f0371.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-122457c027fb7f5be47d5d6fe127aff0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Wavelet-guided-Misalignment-aware-Network-for-Visible-Infrared-Object-Detection"><a href="#Wavelet-guided-Misalignment-aware-Network-for-Visible-Infrared-Object-Detection" class="headerlink" title="Wavelet-guided Misalignment-aware Network for Visible-Infrared Object   Detection"></a>Wavelet-guided Misalignment-aware Network for Visible-Infrared Object   Detection</h2><p><strong>Authors:Haote Zhang, Lipeng Gu, Wuzhou Quan, Fu Lee Wang, Honghui Fan, Jiali Tang, Dingkun Zhu, Haoran Xie, Xiaoping Zhang, Mingqiang Wei</strong></p>
<p>Visible-infrared object detection aims to enhance the detection robustness by exploiting the complementary information of visible and infrared image pairs. However, its performance is often limited by frequent misalignments caused by resolution disparities, spatial displacements, and modality inconsistencies. To address this issue, we propose the Wavelet-guided Misalignment-aware Network (WMNet), a unified framework designed to adaptively address different cross-modal misalignment patterns. WMNet incorporates wavelet-based multi-frequency analysis and modality-aware fusion mechanisms to improve the alignment and integration of cross-modal features. By jointly exploiting low and high-frequency information and introducing adaptive guidance across modalities, WMNet alleviates the adverse effects of noise, illumination variation, and spatial misalignment. Furthermore, it enhances the representation of salient target features while suppressing spurious or misleading information, thereby promoting more accurate and robust detection. Extensive evaluations on the DVTOD, DroneVehicle, and M3FD datasets demonstrate that WMNet achieves state-of-the-art performance on misaligned cross-modal object detection tasks, confirming its effectiveness and practical applicability. </p>
<blockquote>
<p>å¯è§å…‰çº¢å¤–ç›®æ ‡æ£€æµ‹æ—¨åœ¨åˆ©ç”¨å¯è§å…‰å’Œçº¢å¤–å›¾åƒå¯¹çš„äº’è¡¥ä¿¡æ¯æé«˜æ£€æµ‹ç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œå…¶æ€§èƒ½å¾€å¾€å—åˆ°ç”±åˆ†è¾¨ç‡å·®å¼‚ã€ç©ºé—´ä½ç§»å’Œæ¨¡æ€ä¸ä¸€è‡´å¼•èµ·çš„é¢‘ç¹é”™ä½çš„å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå°æ³¢å¼•å¯¼çš„é”™ä½æ„ŸçŸ¥ç½‘ç»œï¼ˆWMNetï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨è‡ªé€‚åº”åœ°è§£å†³ä¸åŒçš„è·¨æ¨¡æ€é”™ä½æ¨¡å¼ã€‚WMNetç»“åˆäº†åŸºäºå°æ³¢çš„å¤šé¢‘åˆ†æå’Œæ¨¡æ€æ„ŸçŸ¥èåˆæœºåˆ¶ï¼Œä»¥æé«˜è·¨æ¨¡æ€ç‰¹å¾çš„å¯¹é½å’Œé›†æˆã€‚é€šè¿‡è”åˆåˆ©ç”¨é«˜ä½é¢‘ä¿¡æ¯å’Œå¼•å…¥è·¨æ¨¡æ€çš„è‡ªé€‚åº”æŒ‡å¯¼ï¼ŒWMNetå‡è½»äº†å™ªå£°ã€å…‰ç…§å˜åŒ–å’Œç©ºé—´é”™ä½çš„ä¸åˆ©å½±å“ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¢å¼ºäº†æ˜¾è‘—ç›®æ ‡ç‰¹å¾çš„è¡¨ç¤ºï¼ŒåŒæ—¶æŠ‘åˆ¶äº†è™šå‡æˆ–è¯¯å¯¼ä¿¡æ¯ï¼Œä»è€Œä¿ƒè¿›äº†æ›´å‡†ç¡®å’Œç¨³å¥çš„æ£€æµ‹ã€‚åœ¨DVTODã€DroneVehicleå’ŒM3FDæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒWMNetåœ¨é”™ä½è·¨æ¨¡æ€ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè¯å®äº†å…¶æœ‰æ•ˆæ€§å’Œå®é™…åº”ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20146v1">PDF</a> </p>
<p><strong>Summary</strong>:<br>    å¯è§å…‰çº¢å¤–ç›®æ ‡æ£€æµ‹æ—¨åœ¨åˆ©ç”¨å¯è§å…‰å’Œçº¢å¤–å›¾åƒå¯¹çš„äº’è¡¥ä¿¡æ¯æé«˜æ£€æµ‹ç¨³å¥æ€§ã€‚ç„¶è€Œï¼Œæ€§èƒ½å¸¸å—é™äºè·¨æ¨¡æ€é”™ä½å¯¼è‡´çš„é¢‘ç¹è¯¯å¯¹é½é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºWavelet-guided Misalignment-awareç½‘ç»œï¼ˆWMNetï¼‰ï¼Œä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå¯è‡ªé€‚åº”è§£å†³ä¸åŒçš„è·¨æ¨¡æ€é”™ä½æ¨¡å¼ã€‚WMNetç»“åˆå°æ³¢å¤šé¢‘åˆ†æå’Œæ¨¡æ€æ„ŸçŸ¥èåˆæœºåˆ¶ï¼Œæ”¹å–„è·¨æ¨¡æ€ç‰¹å¾çš„å¯¹é½å’Œé›†æˆã€‚é€šè¿‡è”åˆåˆ©ç”¨é«˜ä½é¢‘ä¿¡æ¯å’Œå¼•å…¥è·¨æ¨¡æ€è‡ªé€‚åº”æŒ‡å¯¼ï¼ŒWMNetå‡è½»äº†å™ªå£°ã€å…‰ç…§å˜åŒ–å’Œç©ºé—´é”™ä½çš„ä¸åˆ©å½±å“ï¼Œæé«˜äº†ç›®æ ‡ç‰¹å¾çš„è¡¨ç¤ºèƒ½åŠ›ï¼ŒåŒæ—¶æŠ‘åˆ¶äº†å†—ä½™æˆ–è¯¯å¯¼ä¿¡æ¯ï¼Œä¿ƒè¿›äº†æ›´å‡†ç¡®å’Œç¨³å¥çš„æ£€æµ‹ã€‚åœ¨DVTODã€DroneVehicleå’ŒM3FDæ•°æ®é›†ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜äº†WMNetåœ¨è·¨æ¨¡æ€è¯¯å¯¹é½ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šçš„å…ˆè¿›æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å¯è§å…‰çº¢å¤–ç›®æ ‡æ£€æµ‹è¿½æ±‚åˆ©ç”¨ä¸¤ç§å›¾åƒä¿¡æ¯çš„äº’è¡¥æ€§æ¥æå‡æ£€æµ‹ç¨³å¥æ€§ã€‚</li>
<li>è·¨æ¨¡æ€é”™ä½é—®é¢˜æ˜¯å½±å“æ€§èƒ½çš„ä¸»è¦å› ç´ ä¹‹ä¸€ã€‚</li>
<li>WMNetè¢«è®¾è®¡ä¸ºä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”å¤„ç†ä¸åŒçš„è·¨æ¨¡æ€é”™ä½æ¨¡å¼ã€‚</li>
<li>WMNetç»“åˆå°æ³¢å¤šé¢‘åˆ†æå’Œæ¨¡æ€æ„ŸçŸ¥èåˆæ¥æå‡è·¨æ¨¡æ€ç‰¹å¾çš„å¯¹é½å’Œé›†æˆã€‚</li>
<li>é€šè¿‡è”åˆé«˜ä½é¢‘ä¿¡æ¯å’Œè·¨æ¨¡æ€è‡ªé€‚åº”æŒ‡å¯¼ï¼ŒWMNetæé«˜äº†ç›®æ ‡ç‰¹å¾çš„è¡¨ç¤ºèƒ½åŠ›å¹¶æŠ‘åˆ¶äº†å†—ä½™ä¿¡æ¯ã€‚</li>
<li>WMNetåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¯æ˜äº†å…¶åœ¨è·¨æ¨¡æ€è¯¯å¯¹é½ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šçš„å…ˆè¿›æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20146">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-756b64797d2bc8f00fa927f24b7bd86c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-98027c1deb90e012a566a0182088b8da.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d4cbf03ed2703131c809d65e2030ca4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d23b8a840b06f234eef2abe26076b58d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-311d7da6130d3096b4e8ffb406c97b18.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e62d0a603a81f51f98dfb0598debd4bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2a36aa2ea796d184abd7d56d1b80c9ed.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DriveIndia-An-Object-Detection-Dataset-for-Diverse-Indian-Traffic-Scenes"><a href="#DriveIndia-An-Object-Detection-Dataset-for-Diverse-Indian-Traffic-Scenes" class="headerlink" title="DriveIndia: An Object Detection Dataset for Diverse Indian Traffic   Scenes"></a>DriveIndia: An Object Detection Dataset for Diverse Indian Traffic   Scenes</h2><p><strong>Authors:Rishav Kumar, D. Santhosh Reddy, P. Rajalakshmi</strong></p>
<p>We introduce DriveIndia, a large-scale object detection dataset purpose-built to capture the complexity and unpredictability of Indian traffic environments. The dataset contains 66,986 high-resolution images annotated in YOLO format across 24 traffic-relevant object categories, encompassing diverse conditions such as varied weather (fog, rain), illumination changes, heterogeneous road infrastructure, and dense, mixed traffic patterns and collected over 120+ hours and covering 3,400+ kilometers across urban, rural, and highway routes. DriveIndia offers a comprehensive benchmark for real-world autonomous driving challenges. We provide baseline results using state-of-the-art YOLO family models, with the top-performing variant achieving a mAP50 of 78.7%. Designed to support research in robust, generalizable object detection under uncertain road conditions, DriveIndia will be publicly available via the TiHAN-IIT Hyderabad dataset repository (<a target="_blank" rel="noopener" href="https://tihan.iith.ac.in/tiand-datasets/">https://tihan.iith.ac.in/tiand-datasets/</a>). </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºDriveIndiaæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºäº†æ•æ‰å°åº¦äº¤é€šç¯å¢ƒçš„å¤æ‚æ€§å’Œä¸å¯é¢„æµ‹æ€§è€Œæ„å»ºçš„å¤§è§„æ¨¡ç‰©ä½“æ£€æµ‹æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«ä»¥YOLOæ ¼å¼æ ‡æ³¨çš„66,986å¼ é«˜åˆ†è¾¨ç‡å›¾ç‰‡ï¼Œæ¶‰åŠäº¤é€šç›¸å…³çš„24ä¸ªç±»åˆ«ç‰©ä½“ã€‚æ•°æ®é›†æ¶µç›–å„ç§æ¡ä»¶ä¸‹çš„æ•°æ®ï¼ŒåŒ…æ‹¬å¤šå˜çš„å¤©æ°”ï¼ˆå¦‚é›¾ã€é›¨ï¼‰ã€å…‰ç…§å˜åŒ–ã€ä¸åŒçš„é“è·¯åŸºç¡€è®¾æ–½ä»¥åŠå¯†é›†ä¸”æ··åˆçš„äº¤é€šæ¨¡å¼ç­‰ã€‚æ•°æ®æ”¶é›†æ—¶é—´è¶…è¿‡120å°æ—¶ï¼Œè¦†ç›–åŸå¸‚å’Œä¹¡æ‘ä»¥åŠé«˜é€Ÿå…¬è·¯è·¯çº¿å…±è¶…è¿‡ä¸‰åƒå››ç™¾å…¬é‡Œçš„è·¯æ®µã€‚å¯¹äºç°å®ä¸–ç•Œä¸­è‡ªåŠ¨é©¾é©¶æŒ‘æˆ˜æä¾›äº†å…¨é¢åŸºå‡†æŒ‡æ ‡è¯„ä¼°çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ç›®å‰å‰æ²¿çš„YOLOå®¶æ—æ¨¡å‹ç»™å‡ºåŸºçº¿æµ‹è¯•ç»“æœã€‚æ€§èƒ½æœ€ä¼˜çš„ç‰ˆæœ¬èƒ½åœ¨YOLOï¼ˆ<a href="mailto:&#77;&#x41;&#x50;&#x40;&#48;&#x2e;&#53;">&#77;&#x41;&#x50;&#x40;&#48;&#x2e;&#53;</a> IoUï¼‰åŸºå‡†æµ‹è¯•ä¸­å¾—åˆ°78.7ï¼…çš„æŒ‡æ ‡ç»“æœã€‚DriveIndiaè¢«è®¾è®¡ç”¨äºæ”¯æŒåœ¨ä¸ç¡®å®šé“è·¯æ¡ä»¶ä¸‹å®ç°ç¨³å¥å¯æ¨å¹¿çš„ç‰©ä½“æ£€æµ‹ç ”ç©¶ï¼Œå¹¶é€šè¿‡TiHAN-å°åº¦ç†å·¥å­¦é™¢æµ·å¾·æ‹‰å·´æ•°æ®é›†ä»“åº“å…¬å¼€æä¾›ï¼ˆ<a target="_blank" rel="noopener" href="https://tihan.iith.ac.in/tiand-datasets/%EF%BC%89%E3%80%82">https://tihan.iith.ac.in/tiand-datasets/ï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19912v2">PDF</a> Accepted at ITSC 2025 Conference</p>
<p><strong>Summary</strong></p>
<p>é©±åŠ¨å°åº¦æ•°æ®é›†ä»‹ç»ã€‚è¯¥æ•°æ®é›†åŒ…å«6ä¸‡6åƒå¤šå¼ é«˜åˆ†è¾¨ç‡å›¾ç‰‡ï¼Œç”¨äºæ•æ‰å°åº¦å¤æ‚çš„äº¤é€šç¯å¢ƒï¼Œå¯ç”¨äºè‡ªä¸»é©¾é©¶çœŸå®ä¸–ç•ŒæŒ‘æˆ˜è¯„ä¼°ã€‚æ•°æ®æ¶µç›–äº†å„ç§æ¡ä»¶å¦‚ä¸åŒå¤©æ°”å’Œç…§æ˜å˜åŒ–ç­‰ã€‚ä½¿ç”¨æœ€å…ˆè¿›çš„YOLOå®¶æ—æ¨¡å‹æä¾›åŸºå‡†ç»“æœï¼Œæœ€ä½³æ¨¡å‹è¾¾åˆ°78.7%çš„mAP50å‡†ç¡®ç‡ã€‚æ•°æ®å°†é€šè¿‡TiHAN-IIT Hyderabadæ•°æ®é›†ä»“åº“å…¬å¼€è®¿é—®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>DriveIndiaæ˜¯ä¸€ä¸ªé’ˆå¯¹å°åº¦äº¤é€šç¯å¢ƒæ„å»ºçš„å¤§è§„æ¨¡å¯¹è±¡æ£€æµ‹æ•°æ®é›†ã€‚</li>
<li>æ•°æ®é›†åŒ…å«è¶…è¿‡6ä¸‡å¼ é«˜åˆ†è¾¨ç‡å›¾ç‰‡ï¼Œæ¶µç›–å¤šç§äº¤é€šç›¸å…³å¯¹è±¡ç±»åˆ«ã€‚</li>
<li>æ•°æ®é›†è€ƒè™‘äº†å¤šç§æ¡ä»¶ï¼ŒåŒ…æ‹¬ä¸åŒå¤©æ°”ã€ç…§æ˜å˜åŒ–ã€é“è·¯åŸºç¡€è®¾æ–½çš„å¤šæ ·æ€§å’Œå¯†é›†æ··åˆçš„äº¤é€šæ¨¡å¼ã€‚</li>
<li>ä½¿ç”¨YOLOæ ¼å¼è¿›è¡Œæ ‡æ³¨ï¼Œå¹¶æä¾›äº†ä½¿ç”¨æœ€å…ˆè¿›YOLOå®¶æ—æ¨¡å‹çš„åŸºå‡†ç»“æœã€‚</li>
<li>æœ€ä½³æ¨¡å‹è¾¾åˆ°78.7%çš„mAP50å‡†ç¡®ç‡ã€‚</li>
<li>è¯¥æ•°æ®é›†æ—¨åœ¨æ”¯æŒä¸ç¡®å®šé“è·¯æ¡ä»¶ä¸‹çš„ç¨³å¥ã€é€šç”¨å¯¹è±¡æ£€æµ‹ç ”ç©¶ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19912">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-13c583975ed83f09056f72b93cca6d32.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6d3b3d9ec0d840cffac47352e3fb8eac.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-97d08aff5708ebaa2d2c872c2e9c236e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-71be212f15b32533d1b7d744589496d4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a87bbea9f4271389d2ff44808b81b0b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-61e7fd01abdd4000671b06f3978709e7.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="FedS2R-One-Shot-Federated-Domain-Generalization-for-Synthetic-to-Real-Semantic-Segmentation-in-Autonomous-Driving"><a href="#FedS2R-One-Shot-Federated-Domain-Generalization-for-Synthetic-to-Real-Semantic-Segmentation-in-Autonomous-Driving" class="headerlink" title="FedS2R: One-Shot Federated Domain Generalization for Synthetic-to-Real   Semantic Segmentation in Autonomous Driving"></a>FedS2R: One-Shot Federated Domain Generalization for Synthetic-to-Real   Semantic Segmentation in Autonomous Driving</h2><p><strong>Authors:Tao Lian, Jose L. GÃ³mez, Antonio M. LÃ³pez</strong></p>
<p>Federated domain generalization has shown promising progress in image classification by enabling collaborative training across multiple clients without sharing raw data. However, its potential in the semantic segmentation of autonomous driving remains underexplored. In this paper, we propose FedS2R, the first one-shot federated domain generalization framework for synthetic-to-real semantic segmentation in autonomous driving. FedS2R comprises two components: an inconsistency-driven data augmentation strategy that generates images for unstable classes, and a multi-client knowledge distillation scheme with feature fusion that distills a global model from multiple client models. Experiments on five real-world datasets, Cityscapes, BDD100K, Mapillary, IDD, and ACDC, show that the global model significantly outperforms individual client models and is only 2 mIoU points behind the model trained with simultaneous access to all client data. These results demonstrate the effectiveness of FedS2R in synthetic-to-real semantic segmentation for autonomous driving under federated learning </p>
<blockquote>
<p>è”é‚¦åŸŸæ³›åŒ–ï¼ˆFederated Domain Generalizationï¼‰åœ¨å›¾åƒåˆ†ç±»æ–¹é¢å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œé€šè¿‡è·¨å¤šä¸ªå®¢æˆ·ç«¯è¿›è¡ŒååŒè®­ç»ƒï¼Œæ— éœ€å…±äº«åŸå§‹æ•°æ®ã€‚ç„¶è€Œï¼Œå…¶åœ¨è‡ªåŠ¨é©¾é©¶è¯­ä¹‰åˆ†å‰²é¢†åŸŸçš„åº”ç”¨ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FedS2Rï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è‡ªåŠ¨é©¾é©¶åˆæˆåˆ°çœŸå®è¯­ä¹‰åˆ†å‰²çš„ä¸€ç«™å¼è”é‚¦åŸŸæ³›åŒ–æ¡†æ¶ã€‚FedS2RåŒ…å«ä¸¤ä¸ªç»„ä»¶ï¼šä¸€ç§åŸºäºä¸ä¸€è‡´æ€§çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œç”¨äºç”Ÿæˆä¸ç¨³å®šç±»çš„å›¾åƒï¼›ä¸€ç§å¤šå®¢æˆ·ç«¯çŸ¥è¯†è’¸é¦æ–¹æ¡ˆï¼Œå¸¦æœ‰ç‰¹å¾èåˆï¼Œä»å¤šä¸ªå®¢æˆ·ç«¯æ¨¡å‹ä¸­æç‚¼å‡ºå…¨å±€æ¨¡å‹ã€‚åœ¨äº”ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†Cityscapesã€BDD100Kã€Mapillaryã€IDDå’ŒACDCä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¨å±€æ¨¡å‹æ˜¾è‘—ä¼˜äºå•ä¸ªå®¢æˆ·ç«¯æ¨¡å‹ï¼Œå¹¶ä¸”ä»…æ¯”åŒæ—¶è®¿é—®æ‰€æœ‰å®¢æˆ·ç«¯æ•°æ®çš„æ¨¡å‹ä½2 mIoUç‚¹ã€‚è¿™äº›ç»“æœè¯æ˜äº†FedS2Råœ¨è”é‚¦å­¦ä¹ ä¸‹è‡ªåŠ¨é©¾é©¶åˆæˆåˆ°çœŸå®è¯­ä¹‰åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19881v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨å›¾åƒåˆ†ç±»æ–¹é¢ï¼Œè”é‚¦åŸŸæ³›åŒ–é€šè¿‡åœ¨å¤šä¸ªå®¢æˆ·ç«¯ä¹‹é—´è¿›è¡Œåä½œè®­ç»ƒä¸”ä¸å…±äº«åŸå§‹æ•°æ®çš„æ–¹å¼å±•ç°äº†è‰¯å¥½çš„å‘å±•åŠ¿å¤´ã€‚ç„¶è€Œï¼Œå®ƒåœ¨è‡ªåŠ¨é©¾é©¶è¯­ä¹‰åˆ†å‰²æ–¹é¢çš„æ½œåŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚æœ¬æ–‡æå‡ºäº†FedS2Rï¼Œè¿™æ˜¯é¦–ä¸ªé’ˆå¯¹è‡ªåŠ¨é©¾é©¶ä¸­åˆæˆåˆ°çœŸå®åœºæ™¯çš„è¯­ä¹‰åˆ†å‰²çš„ä¸€æ¬¡æ€§è”é‚¦åŸŸæ³›åŒ–æ¡†æ¶ã€‚FedS2RåŒ…å«ä¸¤ä¸ªç»„ä»¶ï¼šä¸€ç§ä»¥ä¸ä¸€è‡´æ€§é©±åŠ¨çš„æ•°æ®å¢å¼ºç­–ç•¥ï¼Œç”¨äºç”Ÿæˆä¸ç¨³å®šç±»çš„å›¾åƒï¼›ä»¥åŠä¸€ç§å…·æœ‰ç‰¹å¾èåˆçš„å¤šå®¢æˆ·ç«¯çŸ¥è¯†è’¸é¦æ–¹æ¡ˆï¼Œå®ƒä»å¤šä¸ªå®¢æˆ·ç«¯æ¨¡å‹ä¸­æç‚¼å‡ºå…¨å±€æ¨¡å‹ã€‚åœ¨äº”ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†Cityscapesã€BDD100Kã€Mapillaryã€IDDå’ŒACDCä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¨å±€æ¨¡å‹çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºå•ä¸ªå®¢æˆ·ç«¯æ¨¡å‹ï¼Œå¹¶ä¸”åªæ¯”åŒæ—¶è®¿é—®æ‰€æœ‰å®¢æˆ·ç«¯æ•°æ®çš„æ¨¡å‹è½å2 mIoUç‚¹ã€‚è¿™äº›ç»“æœè¯æ˜äº†FedS2Råœ¨è”é‚¦å­¦ä¹ ä¸‹çš„è‡ªåŠ¨é©¾é©¶åˆæˆåˆ°çœŸå®è¯­ä¹‰åˆ†å‰²ä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>è¦ç‚¹</strong></p>
<ol>
<li>è”é‚¦åŸŸæ³›åŒ–åœ¨å›¾åƒåˆ†ç±»ä¸­å±•ç°å‡ºé€šè¿‡è·¨å¤šä¸ªå®¢æˆ·ç«¯åä½œè®­ç»ƒçš„ä¼˜åŠ¿ï¼Œä½†åœ¨è‡ªåŠ¨é©¾é©¶è¯­ä¹‰åˆ†å‰²æ–¹é¢çš„åº”ç”¨å°šæœªå……åˆ†ç ”ç©¶ã€‚</li>
<li>æå‡ºäº†FedS2Ræ¡†æ¶ï¼Œå®ƒæ˜¯é’ˆå¯¹è‡ªåŠ¨é©¾é©¶åˆæˆåˆ°çœŸå®åœºæ™¯çš„è¯­ä¹‰åˆ†å‰²çš„ä¸€æ¬¡æ€§è”é‚¦åŸŸæ³›åŒ–æ¡†æ¶çš„é¦–åˆ›ã€‚</li>
<li>FedS2RåŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šä»¥ä¸ä¸€è‡´æ€§é©±åŠ¨çš„æ•°æ®å¢å¼ºç­–ç•¥å’Œå…·æœ‰ç‰¹å¾èåˆçš„å¤šå®¢æˆ·ç«¯çŸ¥è¯†è’¸é¦æ–¹æ¡ˆã€‚</li>
<li>å®éªŒåœ¨äº”ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¿›è¡Œï¼Œè¯æ˜FedS2Rèƒ½å¤Ÿæ˜¾è‘—æå‡å…¨å±€æ¨¡å‹çš„æ€§èƒ½ï¼Œä¸æœ€ä½³è¡¨ç°æ¨¡å‹ä»…ç›¸å·®2 mIoUç‚¹ã€‚</li>
<li>FedS2Ræ¡†æ¶èƒ½æœ‰æ•ˆåº”ç”¨äºè”é‚¦å­¦ä¹ ä¸‹çš„è‡ªåŠ¨é©¾é©¶è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ã€‚</li>
<li>è¯¥æ¡†æ¶å…·æœ‰æ½œåŠ›åœ¨ä¿æŠ¤æ•°æ®éšç§çš„åŒæ—¶æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä¸ºè‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å‘å±•å¼€è¾Ÿæ–°é€”å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19881">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-7b1bc1db03c9d4dc325bf54ae53f9df7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-592a35f6bdfe54aaef1fde5653a05438.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3807d8c4d4e1b966a0d161c2184ac454.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e89dd91cea559c45cbb1fbbbfe232283.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="OW-CLIP-Data-Efficient-Visual-Supervision-for-Open-World-Object-Detection-via-Human-AI-Collaboration"><a href="#OW-CLIP-Data-Efficient-Visual-Supervision-for-Open-World-Object-Detection-via-Human-AI-Collaboration" class="headerlink" title="OW-CLIP: Data-Efficient Visual Supervision for Open-World Object   Detection via Human-AI Collaboration"></a>OW-CLIP: Data-Efficient Visual Supervision for Open-World Object   Detection via Human-AI Collaboration</h2><p><strong>Authors:Junwen Duan, Wei Xue, Ziyao Kang, Shixia Liu, Jiazhi Xia</strong></p>
<p>Open-world object detection (OWOD) extends traditional object detection to identifying both known and unknown object, necessitating continuous model adaptation as new annotations emerge. Current approaches face significant limitations: 1) data-hungry training due to reliance on a large number of crowdsourced annotations, 2) susceptibility to â€œpartial feature overfitting,â€ and 3) limited flexibility due to required model architecture modifications. To tackle these issues, we present OW-CLIP, a visual analytics system that provides curated data and enables data-efficient OWOD model incremental training. OW-CLIP implements plug-and-play multimodal prompt tuning tailored for OWOD settings and introduces a novel â€œCrop-Smoothingâ€ technique to mitigate partial feature overfitting. To meet the data requirements for the training methodology, we propose dual-modal data refinement methods that leverage large language models and cross-modal similarity for data generation and filtering. Simultaneously, we develope a visualization interface that enables users to explore and deliver high-quality annotations: including class-specific visual feature phrases and fine-grained differentiated images. Quantitative evaluation demonstrates that OW-CLIP achieves competitive performance at 89% of state-of-the-art performance while requiring only 3.8% self-generated data, while outperforming SOTA approach when trained with equivalent data volumes. A case study shows the effectiveness of the developed method and the improved annotation quality of our visualization system. </p>
<blockquote>
<p>å¼€æ”¾ä¸–ç•Œç›®æ ‡æ£€æµ‹ï¼ˆOWODï¼‰å°†ä¼ ç»Ÿç›®æ ‡æ£€æµ‹æ‰©å±•åˆ°è¯†åˆ«å’ŒæœªçŸ¥ç›®æ ‡ï¼Œéšç€æ–°æ ‡æ³¨çš„å‡ºç°ï¼Œéœ€è¦æŒç»­æ¨¡å‹é€‚åº”ã€‚å½“å‰æ–¹æ³•é¢ä¸´é‡å¤§å±€é™æ€§ï¼š1ï¼‰ç”±äºä¾èµ–å¤§é‡ä¼—åŒ…æ ‡æ³¨ï¼Œå¯¼è‡´æ•°æ®é¥¥é¥¿å‹è®­ç»ƒï¼›2ï¼‰å®¹æ˜“â€œå±€éƒ¨ç‰¹å¾è¿‡åº¦æ‹Ÿåˆâ€ï¼›3ï¼‰ç”±äºéœ€è¦ä¿®æ”¹æ¨¡å‹æ¶æ„ï¼Œå¯¼è‡´çµæ´»æ€§æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†OW-CLIPï¼Œä¸€ä¸ªè§†è§‰åˆ†æç³»ç»Ÿï¼Œæä¾›å®šåˆ¶æ•°æ®å¹¶å¯ç”¨äº†æ•°æ®é«˜æ•ˆçš„OWODæ¨¡å‹å¢é‡è®­ç»ƒã€‚OW-CLIPå®ç°äº†ä¸ºOWODè®¾ç½®é‡èº«å®šåˆ¶çš„å³æ’å³ç”¨å¤šæ¨¡å¼æç¤ºè°ƒæ•´ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„â€œè£å‰ªå¹³æ»‘â€æŠ€æœ¯æ¥ç¼“è§£å±€éƒ¨ç‰¹å¾è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚ä¸ºäº†æ»¡è¶³è®­ç»ƒæ–¹æ³•çš„æ•°æ®è¦æ±‚ï¼Œæˆ‘ä»¬æå‡ºäº†åŒæ¨¡æ€æ•°æ®ç²¾ç‚¼æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè·¨æ¨¡æ€ç›¸ä¼¼æ€§è¿›è¡Œæ•°æ®ç”Ÿæˆå’Œè¿‡æ»¤ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯è§†åŒ–ç•Œé¢ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿæ¢ç´¢å’Œæä¾›é«˜è´¨é‡æ ‡æ³¨ï¼ŒåŒ…æ‹¬ç‰¹å®šç±»åˆ«çš„è§†è§‰ç‰¹å¾çŸ­è¯­å’Œç²¾ç»†å·®å¼‚å›¾åƒã€‚å®šé‡è¯„ä¼°è¡¨æ˜ï¼ŒOW-CLIPè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯çš„89%æ€§èƒ½æ°´å¹³ï¼ŒåŒæ—¶ä»…éœ€è¦3.8%çš„è‡ªæˆ‘ç”Ÿæˆæ•°æ®ï¼Œè€Œåœ¨ä½¿ç”¨ç­‰æ•ˆæ•°æ®é‡è¿›è¡Œè®­ç»ƒæ—¶è¡¨ç°ä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ã€‚æ¡ˆä¾‹ç ”ç©¶è¡¨æ˜æ‰€å¼€å‘æ–¹æ³•çš„æœ‰æ•ˆæ€§ä»¥åŠæˆ‘ä»¬å¯è§†åŒ–ç³»ç»Ÿæ”¹è¿›æ ‡æ³¨è´¨é‡çš„èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19870v1">PDF</a> 9 pages, 11 figures</p>
<p><strong>Summary</strong></p>
<p>OWODé¢ä¸´æ•°æ®ä¾èµ–æ€§å¼ºã€æ˜“å±€éƒ¨ç‰¹å¾è¿‡æ‹Ÿåˆå’Œæ¨¡å‹æ¶æ„ä¿®æ”¹çµæ´»æ€§æœ‰é™ç­‰é—®é¢˜ã€‚ä¸ºåº”å¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†OW-CLIPè§†è§‰åˆ†æç³»ç»Ÿï¼Œå®ç°æ•°æ®é«˜æ•ˆçš„OWODæ¨¡å‹å¢é‡è®­ç»ƒã€‚é‡‡ç”¨é‡èº«å®šåˆ¶çš„â€œå³æ’å³ç”¨â€å¤šæ¨¡æ€æç¤ºè°ƒæ•´ï¼Œå¼•å…¥æ–°é¢–çš„â€œCrop-Smoothingâ€æŠ€æœ¯å‡è½»å±€éƒ¨ç‰¹å¾è¿‡æ‹Ÿåˆé—®é¢˜ã€‚åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè·¨æ¨¡æ€ç›¸ä¼¼æ€§è¿›è¡ŒåŒæ¨¡æ€æ•°æ®ç²¾ç‚¼ï¼Œæ»¡è¶³è®­ç»ƒæ–¹æ³•è®ºçš„æ•°æ®è¦æ±‚ã€‚åŒæ—¶å¼€å‘å¯è§†åŒ–ç•Œé¢ï¼Œç”¨æˆ·å¯æ¢ç´¢å¹¶æä¾›é«˜è´¨é‡æ³¨é‡Šã€‚å®šé‡è¯„ä¼°æ˜¾ç¤ºï¼ŒOW-CLIPåœ¨ä»…ä½¿ç”¨3.8%è‡ªæˆ‘ç”Ÿæˆæ•°æ®çš„æƒ…å†µä¸‹è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ€§èƒ½çš„89%ï¼Œå¹¶åœ¨ç­‰æ•ˆæ•°æ®é‡è®­ç»ƒæ—¶è¡¨ç°æ›´ä¼˜ã€‚æ¡ˆä¾‹ç ”ç©¶è¯æ˜äº†å¼€å‘æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå¯è§†åŒ–ç³»ç»Ÿæé«˜çš„æ³¨é‡Šè´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¼€æ”¾ä¸–ç•Œç›®æ ‡æ£€æµ‹ï¼ˆOWODï¼‰é¢ä¸´å¤šä¸ªæŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ•°æ®ä¾èµ–æ€§å¼ºã€æ˜“å±€éƒ¨ç‰¹å¾è¿‡æ‹Ÿåˆå’Œæ¨¡å‹æ¶æ„çµæ´»æ€§æœ‰é™ã€‚</li>
<li>æå‡ºOW-CLIPè§†è§‰åˆ†æç³»ç»Ÿï¼Œå®ç°æ•°æ®é«˜æ•ˆçš„OWODæ¨¡å‹å¢é‡è®­ç»ƒï¼Œè§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>OW-CLIPé‡‡ç”¨é‡èº«å®šåˆ¶çš„å¤šæ¨¡æ€æç¤ºè°ƒæ•´ï¼Œå¼•å…¥Crop-SmoothingæŠ€æœ¯æ¥å‡è½»å±€éƒ¨ç‰¹å¾è¿‡æ‹Ÿåˆã€‚</li>
<li>åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè·¨æ¨¡æ€ç›¸ä¼¼æ€§è¿›è¡ŒåŒæ¨¡æ€æ•°æ®ç²¾ç‚¼ï¼Œæ»¡è¶³è®­ç»ƒæ–¹æ³•è®ºçš„æ•°æ®éœ€æ±‚ã€‚</li>
<li>å¼€å‘å¯è§†åŒ–ç•Œé¢ï¼Œä¾¿äºç”¨æˆ·æ¢ç´¢å¹¶æä¾›é«˜è´¨é‡æ³¨é‡Šã€‚</li>
<li>å®šé‡è¯„ä¼°æ˜¾ç¤ºï¼ŒOW-CLIPåœ¨æ•°æ®æ•ˆç‡æ–¹é¢è¡¨ç°å‡ºç«äº‰åŠ›ï¼Œä»…åœ¨3.8%çš„è‡ªæˆ‘ç”Ÿæˆæ•°æ®ä¸‹å³è¾¾åˆ°æœ€æ–°æŠ€æœ¯çš„89%æ€§èƒ½ã€‚åœ¨ç­‰æ•ˆæ•°æ®é‡è®­ç»ƒæ—¶è¡¨ç°æ›´å‡ºè‰²ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19870">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-457ca12fcb22f677688eb13f280bb7a6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-21fee5ead0c057741dd8653c84d50615.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bcac45d6df14d013849f4ce46ad9e6b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acd04a97d8ff95592adf281c88c18071.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ee21147a628492092fb1d19a72b60ce.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-992cdb5f7ce86cab809defbbaa2f9a42.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="RaGS-Unleashing-3D-Gaussian-Splatting-from-4D-Radar-and-Monocular-Cues-for-3D-Object-Detection"><a href="#RaGS-Unleashing-3D-Gaussian-Splatting-from-4D-Radar-and-Monocular-Cues-for-3D-Object-Detection" class="headerlink" title="RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues   for 3D Object Detection"></a>RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues   for 3D Object Detection</h2><p><strong>Authors:Xiaokai Bai, Chenxu Zhou, Lianqing Zheng, Si-Yuan Cao, Jianan Liu, Xiaohan Zhang, Zhengzhuang Zhang, Hui-liang Shen</strong></p>
<p>4D millimeter-wave radar has emerged as a promising sensor for autonomous driving, but effective 3D object detection from both 4D radar and monocular images remains a challenge. Existing fusion approaches typically rely on either instance-based proposals or dense BEV grids, which either lack holistic scene understanding or are limited by rigid grid structures. To address these, we propose RaGS, the first framework to leverage 3D Gaussian Splatting (GS) as representation for fusing 4D radar and monocular cues in 3D object detection. 3D GS naturally suits 3D object detection by modeling the scene as a field of Gaussians, dynamically allocating resources on foreground objects and providing a flexible, resource-efficient solution. RaGS uses a cascaded pipeline to construct and refine the Gaussian field. It starts with the Frustum-based Localization Initiation (FLI), which unprojects foreground pixels to initialize coarse 3D Gaussians positions. Then, the Iterative Multimodal Aggregation (IMA) fuses semantics and geometry, refining the limited Gaussians to the regions of interest. Finally, the Multi-level Gaussian Fusion (MGF) renders the Gaussians into multi-level BEV features for 3D object detection. By dynamically focusing on sparse objects within scenes, RaGS enable object concentrating while offering comprehensive scene perception. Extensive experiments on View-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate its state-of-the-art performance. Code will be released. </p>
<blockquote>
<p>4Dæ¯«ç±³æ³¢é›·è¾¾ä½œä¸ºè‡ªåŠ¨é©¾é©¶çš„ä¼ æ„Ÿå™¨å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä»4Dé›·è¾¾å’Œå•ç›®å›¾åƒä¸­è¿›è¡Œæœ‰æ•ˆçš„3Dç›®æ ‡æ£€æµ‹ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ç°æœ‰çš„èåˆæ–¹æ³•é€šå¸¸ä¾èµ–äºåŸºäºå®ä¾‹çš„ææ¡ˆæˆ–å¯†é›†çš„BEVç½‘æ ¼ï¼Œè¿™äº›æ–¹æ³•è¦ä¹ˆç¼ºä¹æ•´ä½“åœºæ™¯çš„ç†è§£ï¼Œè¦ä¹ˆå—åˆ°åˆšæ€§ç½‘æ ¼ç»“æ„çš„é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RaGSï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåˆ©ç”¨3Dé«˜æ–¯è´´å›¾ï¼ˆGSï¼‰ä½œä¸ºè¡¨ç¤ºæ–¹æ³•ï¼Œèåˆ4Dé›·è¾¾å’Œå•ç›®å›¾åƒæç¤ºè¿›è¡Œ3Dç›®æ ‡æ£€æµ‹ã€‚3D GSè‡ªç„¶åœ°é€‚åˆè¿›è¡Œ3Dç›®æ ‡æ£€æµ‹ï¼Œé€šè¿‡å°†åœºæ™¯å»ºæ¨¡ä¸ºé«˜æ–¯åœºï¼ŒåŠ¨æ€åˆ†é…èµ„æºäºå‰æ™¯ç›®æ ‡å¹¶æä¾›çµæ´»ä¸”èµ„æºé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚RaGSä½¿ç”¨çº§è”ç®¡é“æ¥æ„å»ºå’Œç»†åŒ–é«˜æ–¯åœºã€‚å®ƒå¼€å§‹äºåŸºäºFrustumçš„å®šä½åˆå§‹åŒ–ï¼ˆFLIï¼‰ï¼Œå°†å‰æ™¯åƒç´ åæŠ•å½±ä»¥åˆå§‹åŒ–ç²—ç•¥çš„3Dé«˜æ–¯ä½ç½®ã€‚ç„¶åï¼Œè¿­ä»£å¤šæ¨¡æ€èšåˆï¼ˆIMAï¼‰èåˆäº†è¯­ä¹‰å’Œå‡ ä½•ä¿¡æ¯ï¼Œå¯¹æœ‰é™çš„é«˜æ–¯è¿›è¡Œç²¾ç‚¼ä»¥å®šä½æ„Ÿå…´è¶£åŒºåŸŸã€‚æœ€åï¼Œå¤šå±‚æ¬¡é«˜æ–¯èåˆï¼ˆMGFï¼‰å°†é«˜æ–¯æ¸²æŸ“ä¸ºå¤šå±‚æ¬¡BEVç‰¹å¾ç”¨äº3Dç›®æ ‡æ£€æµ‹ã€‚é€šè¿‡åŠ¨æ€å…³æ³¨åœºæ™¯ä¸­çš„ç¨€ç–ç›®æ ‡ï¼ŒRaGSå®ç°äº†ç›®æ ‡é›†ä¸­å¹¶æä¾›äº†å…¨é¢çš„åœºæ™¯æ„ŸçŸ¥ã€‚åœ¨View-of-Delftã€TJ4DRadSetå’ŒOmniHD-ScenesåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¯æ˜äº†å…¶å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å°†å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19856v2">PDF</a> 9 pages, 6 figures, conference</p>
<p><strong>æ‘˜è¦</strong><br>é›·è¾¾æŠ€æœ¯ä¸å›¾åƒæŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„èåˆé¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰èåˆæ–¹æ³•ç¼ºä¹å…¨æ™¯åœºæ™¯ç†è§£æˆ–å—é™äºç½‘æ ¼ç»“æ„ã€‚æœ¬ç ”ç©¶æå‡ºRaGSæ¡†æ¶ï¼Œé¦–æ¬¡åˆ©ç”¨ä¸‰ç»´é«˜æ–¯æ˜ å°„ï¼ˆGSï¼‰æŠ€æœ¯èåˆå››ç»´é›·è¾¾ä¸å•ç›®å›¾åƒä¿¡æ¯ï¼Œå®ç°ä¸‰ç»´ç‰©ä½“æ£€æµ‹ã€‚é€šè¿‡å‰æ™¯å¯¹è±¡åŠ¨æ€åˆ†é…èµ„æºå¹¶æ„å»ºé«˜æ–¯åœºï¼Œæä¾›çµæ´»ã€é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°å“è¶Šã€‚</p>
<p><strong>è¦ç‚¹æŒæ¡</strong></p>
<ul>
<li>4Dæ¯«ç±³æ³¢é›·è¾¾åœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸå…·æœ‰æ½œåŠ›ï¼Œä½†3Dç‰©ä½“æ£€æµ‹ä»æ˜¯æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰èåˆæ–¹æ³•ç¼ºä¹å…¨æ™¯åœºæ™¯ç†è§£æˆ–å—é™äºç½‘æ ¼ç»“æ„ã€‚</li>
<li>RaGSæ¡†æ¶åˆ©ç”¨ä¸‰ç»´é«˜æ–¯æ˜ å°„ï¼ˆGSï¼‰æŠ€æœ¯èåˆå››ç»´é›·è¾¾ä¸å•ç›®å›¾åƒä¿¡æ¯ã€‚</li>
<li>é«˜æ–¯æ˜ å°„æŠ€æœ¯é€šè¿‡å‰æ™¯å¯¹è±¡åŠ¨æ€åˆ†é…èµ„æºï¼Œä¸ºä¸‰ç»´ç‰©ä½“æ£€æµ‹æä¾›çµæ´»ã€é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>RaGSé€šè¿‡çº§è”ç®¡é“æ„å»ºå’Œç»†åŒ–é«˜æ–¯åœºï¼ŒåŒ…æ‹¬åŸºäºè§†é”¥çš„å®šä½å¯åŠ¨ï¼ˆFLIï¼‰ã€è¿­ä»£å¤šæ¨¡å¼èšåˆï¼ˆIMAï¼‰å’Œå¤šçº§é«˜æ–¯èåˆï¼ˆMGFï¼‰ã€‚</li>
<li>RaGSæ–¹æ³•åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œå°†æä¾›å¼€æºä»£ç ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19856">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0edc511009429e77c152020ee011a982.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-557174558a450f85c8820260f0f9ac55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d093c386ad98b81e155e79307803ffa6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5b7884256c56c8ff35df1bf5e8575871.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-acbab909692d1066dfa6f86b11ee8862.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="DS-Det-Single-Query-Paradigm-and-Attention-Disentangled-Learning-for-Flexible-Object-Detection"><a href="#DS-Det-Single-Query-Paradigm-and-Attention-Disentangled-Learning-for-Flexible-Object-Detection" class="headerlink" title="DS-Det: Single-Query Paradigm and Attention Disentangled Learning for   Flexible Object Detection"></a>DS-Det: Single-Query Paradigm and Attention Disentangled Learning for   Flexible Object Detection</h2><p><strong>Authors:Guiping Cao, Xiangyuan Lan, Wenjian Huang, Jianguo Zhang, Dongmei Jiang, Yaowei Wang</strong></p>
<p>Popular transformer detectors have achieved promising performance through query-based learning using attention mechanisms. However, the roles of existing decoder query types (e.g., content query and positional query) are still underexplored. These queries are generally predefined with a fixed number (fixed-query), which limits their flexibility. We find that the learning of these fixed-query is impaired by Recurrent Opposing inTeractions (ROT) between two attention operations: Self-Attention (query-to-query) and Cross-Attention (query-to-encoder), thereby degrading decoder efficiency. Furthermore, â€œquery ambiguityâ€ arises when shared-weight decoder layers are processed with both one-to-one and one-to-many label assignments during training, violating DETRâ€™s one-to-one matching principle. To address these challenges, we propose DS-Det, a more efficient detector capable of detecting a flexible number of objects in images. Specifically, we reformulate and introduce a new unified Single-Query paradigm for decoder modeling, transforming the fixed-query into flexible. Furthermore, we propose a simplified decoder framework through attention disentangled learning: locating boxes with Cross-Attention (one-to-many process), deduplicating predictions with Self-Attention (one-to-one process), addressing â€œquery ambiguityâ€ and â€œROTâ€ issues directly, and enhancing decoder efficiency. We further introduce a unified PoCoo loss that leverages box size priors to prioritize query learning on hard samples such as small objects. Extensive experiments across five different backbone models on COCO2017 and WiderPerson datasets demonstrate the general effectiveness and superiority of DS-Det. The source codes are available at <a target="_blank" rel="noopener" href="https://github.com/Med-Process/DS-Det/">https://github.com/Med-Process/DS-Det/</a>. </p>
<blockquote>
<p>ç°æœ‰çš„æµè¡Œçš„è½¬æ¢å™¨æ£€æµ‹å™¨å·²é€šè¿‡åŸºäºæ³¨æ„åŠ›çš„æŸ¥è¯¢å­¦ä¹ å–å¾—äº†æœ‰å‰æ™¯çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§£ç å™¨æŸ¥è¯¢ç±»å‹ï¼ˆå¦‚å†…å®¹æŸ¥è¯¢å’Œä½ç½®æŸ¥è¯¢ï¼‰çš„è§’è‰²ä»ç„¶è¢«ä½ä¼°ã€‚è¿™äº›æŸ¥è¯¢é€šå¸¸æ˜¯é¢„å…ˆå®šä¹‰çš„å›ºå®šæ•°é‡ï¼ˆå›ºå®šæŸ¥è¯¢ï¼‰ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„çµæ´»æ€§ã€‚æˆ‘ä»¬å‘ç°è¿™äº›å›ºå®šæŸ¥è¯¢çš„å­¦ä¹ å—åˆ°ä¸¤ç§æ³¨æ„åŠ›æ“ä½œä¹‹é—´çš„å¾ªç¯ç›¸åäº¤äº’ï¼ˆROTï¼‰çš„æŸå®³ï¼šè‡ªæ³¨æ„åŠ›ï¼ˆæŸ¥è¯¢å¯¹æŸ¥è¯¢ï¼‰å’Œäº¤å‰æ³¨æ„åŠ›ï¼ˆæŸ¥è¯¢å¯¹ç¼–ç å™¨ï¼‰ï¼Œä»è€Œé™ä½äº†è§£ç å™¨çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œå½“å…±äº«æƒé‡è§£ç å™¨å±‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œä¸€å¯¹ä¸€å’Œä¸€å¯¹å¤šæ ‡ç­¾åˆ†é…æ—¶ï¼Œâ€œæŸ¥è¯¢æ­§ä¹‰â€å°±ä¼šå‡ºç°ï¼Œè¿™è¿åäº†DETRçš„ä¸€å¯¹ä¸€åŒ¹é…åŸåˆ™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†DS-Detï¼Œè¿™æ˜¯ä¸€ç§æ›´é«˜æ•ˆçš„æ£€æµ‹å™¨ï¼Œèƒ½å¤Ÿæ£€æµ‹å›¾åƒä¸­å¯å˜æ•°é‡çš„å¯¹è±¡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹è§£ç å™¨å»ºæ¨¡æå‡ºäº†æ–°çš„ç»Ÿä¸€å•æŸ¥è¯¢èŒƒå¼ï¼Œå°†å›ºå®šæŸ¥è¯¢è½¬å˜ä¸ºçµæ´»çš„æŸ¥è¯¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€åŒ–çš„è§£ç å™¨æ¡†æ¶ï¼Œé€šè¿‡æ³¨æ„åŠ›è§£è€¦å­¦ä¹ ï¼šä½¿ç”¨äº¤å‰æ³¨æ„åŠ›å®šä½ç›’å­ï¼ˆä¸€å¯¹å¤šè¿‡ç¨‹ï¼‰ï¼Œç”¨è‡ªæ³¨æ„åŠ›å»é™¤é‡å¤é¢„æµ‹ï¼ˆä¸€å¯¹ä¸€è¿‡ç¨‹ï¼‰ï¼Œç›´æ¥è§£å†³â€œæŸ¥è¯¢æ­§ä¹‰â€å’Œâ€œROTâ€é—®é¢˜ï¼Œæé«˜è§£ç å™¨æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„PoCooæŸå¤±ï¼Œå®ƒåˆ©ç”¨æ¡†å¤§å°å…ˆéªŒæ¥ä¼˜å…ˆå­¦ä¹ ç¡¬æ ·æœ¬ä¸Šçš„æŸ¥è¯¢ï¼Œå¦‚å°å¯¹è±¡ã€‚åœ¨COCO2017å’ŒWiderPersonæ•°æ®é›†ä¸Šçš„äº”ä¸ªä¸åŒä¸»å¹²æ¨¡å‹çš„å¤§é‡å®éªŒè¯æ˜äº†DS-Detçš„é€šç”¨æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Med-Process/DS-Det/">https://github.com/Med-Process/DS-Det/</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19807v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç°æœ‰åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ£€æµ‹å™¨åœ¨å¤„ç†å›¾åƒæ£€æµ‹ä»»åŠ¡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å›ºå®šæŸ¥è¯¢æ•°é‡ã€æŸ¥è¯¢å­¦ä¹ è¿‡ç¨‹ä¸­çš„äº¤äº’é—®é¢˜ä»¥åŠæŸ¥è¯¢æ­§ä¹‰ç­‰ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºDS-Detçš„æ–°æ£€æµ‹å™¨ï¼Œé€šè¿‡å¼•å…¥å•ä¸€æŸ¥è¯¢èŒƒå¼å’Œæ³¨æ„åŠ›è§£è€¦å­¦ä¹ æ¥å¢å¼ºè§£ç å™¨æ•ˆç‡ï¼Œå¹¶é€šè¿‡å¼•å…¥ä¸€ç§ä¼˜å…ˆäºç›’å­å¤§å°çš„å…ˆéªŒæŸå¤±æ¥è§£å†³â€œæŸ¥è¯¢æ­§ä¹‰â€é—®é¢˜ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒDS-Detåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ£€æµ‹å™¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç°æœ‰åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ£€æµ‹å™¨å­˜åœ¨å›ºå®šæŸ¥è¯¢æ•°é‡çš„å±€é™æ€§ã€‚è¿™äº›æŸ¥è¯¢æ•°é‡é€šå¸¸åœ¨è®­ç»ƒä¸­å·²ç»è¢«é¢„è®¾å¹¶é™åˆ¶åœ¨ä¸€å®šèŒƒå›´å†…ã€‚æ­¤å¤–è¿˜å­˜åœ¨è‡ªæ³¨æ„åŠ›ä¸è·¨æ³¨æ„åŠ›çš„ç›¸äº’ä½œç”¨é—®é¢˜å’ŒæŸ¥è¯¢æ­§ä¹‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜å¹¶æé«˜è§£ç å™¨çš„æ•ˆç‡ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†åä¸ºDS-Detçš„æ–°æ£€æµ‹å™¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19807">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-65b1c443bedea1594ed406032aa1094c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-77d9913b112d9156b2a525f463155a69.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b408fb51157bd520f7bdd182c002a7f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1b197189b4c7621b6ee09783bb03712e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-41a5731c5d17664a679ce9503152c139.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9106233358cab16a0f155c562b129c32.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="TransFlow-Motion-Knowledge-Transfer-from-Video-Diffusion-Models-to-Video-Salient-Object-Detection"><a href="#TransFlow-Motion-Knowledge-Transfer-from-Video-Diffusion-Models-to-Video-Salient-Object-Detection" class="headerlink" title="TransFlow: Motion Knowledge Transfer from Video Diffusion Models to   Video Salient Object Detection"></a>TransFlow: Motion Knowledge Transfer from Video Diffusion Models to   Video Salient Object Detection</h2><p><strong>Authors:Suhwan Cho, Minhyeok Lee, Jungho Lee, Sunghun Yang, Sangyoun Lee</strong></p>
<p>Video salient object detection (SOD) relies on motion cues to distinguish salient objects from backgrounds, but training such models is limited by scarce video datasets compared to abundant image datasets. Existing approaches that use spatial transformations to create video sequences from static images fail for motion-guided tasks, as these transformations produce unrealistic optical flows that lack semantic understanding of motion. We present TransFlow, which transfers motion knowledge from pre-trained video diffusion models to generate realistic training data for video SOD. Video diffusion models have learned rich semantic motion priors from large-scale video data, understanding how different objects naturally move in real scenes. TransFlow leverages this knowledge to generate semantically-aware optical flows from static images, where objects exhibit natural motion patterns while preserving spatial boundaries and temporal coherence. Our method achieves improved performance across multiple benchmarks, demonstrating effective motion knowledge transfer. </p>
<blockquote>
<p>è§†é¢‘æ˜¾è‘—ç›®æ ‡æ£€æµ‹ï¼ˆSODï¼‰ä¾èµ–äºè¿åŠ¨çº¿ç´¢æ¥åŒºåˆ†æ˜¾è‘—ç›®æ ‡ä¸èƒŒæ™¯ï¼Œä½†ä¸ä¸°å¯Œçš„å›¾åƒæ•°æ®é›†ç›¸æ¯”ï¼Œè®­ç»ƒæ­¤ç±»æ¨¡å‹å—åˆ°ç¨€ç¼ºè§†é¢‘æ•°æ®é›†çš„é™åˆ¶ã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨ç©ºé—´å˜æ¢ä»é™æ€å›¾åƒåˆ›å»ºè§†é¢‘åºåˆ—ï¼Œä¸é€‚ç”¨äºè¿åŠ¨å¼•å¯¼çš„ä»»åŠ¡ï¼Œå› ä¸ºè¿™äº›å˜æ¢ä¼šäº§ç”Ÿç¼ºä¹è¿åŠ¨è¯­ä¹‰ç†è§£çš„ä¸çœŸå®å…‰æµã€‚æˆ‘ä»¬æå‡ºäº†TransFlowï¼Œå®ƒå°†é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„è¿åŠ¨çŸ¥è¯†è½¬ç§»ï¼Œä»¥ç”Ÿæˆç”¨äºè§†é¢‘SODçš„ç°å®è®­ç»ƒæ•°æ®ã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹å·²ç»ä»å¤§è§„æ¨¡è§†é¢‘æ•°æ®ä¸­å­¦ä¹ äº†ä¸°å¯Œçš„è¯­ä¹‰è¿åŠ¨å…ˆéªŒçŸ¥è¯†ï¼Œç†è§£ä¸åŒå¯¹è±¡åœ¨çœŸå®åœºæ™¯ä¸­çš„è‡ªç„¶è¿åŠ¨æ–¹å¼ã€‚TransFlowåˆ©ç”¨è¿™äº›çŸ¥è¯†ä»é™æ€å›¾åƒç”Ÿæˆè¯­ä¹‰æ„ŸçŸ¥çš„å…‰æµï¼Œå…¶ä¸­å¯¹è±¡è¡¨ç°å‡ºè‡ªç„¶è¿åŠ¨æ¨¡å¼ï¼ŒåŒæ—¶ä¿ç•™ç©ºé—´è¾¹ç•Œå’Œæ—¶é—´è¿è´¯æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æœ‰æ•ˆçš„è¿åŠ¨çŸ¥è¯†è½¬ç§»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19789v1">PDF</a> ICCVW 2025</p>
<p><strong>Summary</strong></p>
<p>è§†é¢‘æ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹ï¼ˆSODï¼‰ä¾èµ–è¿åŠ¨çº¿ç´¢æ¥åŒºåˆ†æ˜¾è‘—ç›®æ ‡èƒŒæ™¯ï¼Œä½†ç”±äºè§†é¢‘æ•°æ®é›†ç›¸å¯¹äºå›¾åƒæ•°æ®é›†è¾ƒä¸ºç¨€ç¼ºï¼Œè®­ç»ƒæ­¤ç±»æ¨¡å‹å—åˆ°é™åˆ¶ã€‚ç°æœ‰æ–¹æ³•ä½¿ç”¨ç©ºé—´å˜æ¢ä»é™æ€å›¾åƒåˆ›å»ºè§†é¢‘åºåˆ—ï¼Œä½†ä¸é€‚ç”¨äºè¿åŠ¨å¼•å¯¼çš„ä»»åŠ¡ï¼Œå› ä¸ºè¿™äº›å˜æ¢ä¼šäº§ç”Ÿç¼ºä¹è¯­ä¹‰ç†è§£çš„ä¸çœŸå®çš„å…‰æµã€‚æœ¬æ–‡æå‡ºTransFlowï¼Œå®ƒå°†é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„è¿åŠ¨çŸ¥è¯†è½¬ç§»ï¼Œä»¥ç”Ÿæˆç”¨äºè§†é¢‘SODçš„ç°å®è®­ç»ƒæ•°æ®ã€‚è§†é¢‘æ‰©æ•£æ¨¡å‹ä»å¤§è§„æ¨¡è§†é¢‘æ•°æ®ä¸­å­¦ä¹ äº†ä¸°å¯Œçš„è¯­ä¹‰è¿åŠ¨å…ˆéªŒçŸ¥è¯†ï¼Œç†è§£ä¸åŒå¯¹è±¡åœ¨çœŸå®åœºæ™¯ä¸­çš„è‡ªç„¶è¿åŠ¨æ–¹å¼ã€‚TransFlowåˆ©ç”¨è¿™äº›çŸ¥è¯†ä»é™æ€å›¾åƒç”Ÿæˆè¯­ä¹‰æ„ŸçŸ¥çš„å…‰æµï¼Œå…¶ä¸­å¯¹è±¡è¡¨ç°å‡ºè‡ªç„¶è¿åŠ¨æ¨¡å¼ï¼ŒåŒæ—¶ä¿ç•™ç©ºé—´è¾¹ç•Œå’Œæ—¶é—´è¿è´¯æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ€§èƒ½æå‡ï¼Œè¯æ˜äº†æœ‰æ•ˆçš„è¿åŠ¨çŸ¥è¯†è½¬ç§»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†é¢‘æ˜¾è‘—æ€§ç›®æ ‡æ£€æµ‹ï¼ˆSODï¼‰ä¾èµ–è¿åŠ¨çº¿ç´¢è¿›è¡ŒèƒŒæ™¯åŒºåˆ†ï¼Œä½†è®­ç»ƒæ¨¡å‹å—è§†é¢‘æ•°æ®é›†ç¨€ç¼ºé™åˆ¶ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä½¿ç”¨ç©ºé—´å˜æ¢ä»é™æ€å›¾åƒåˆ›å»ºè§†é¢‘åºåˆ—ï¼Œä½†è¿™ç§æ–¹æ³•äº§ç”Ÿçš„å…‰æµä¸çœŸå®ï¼Œç¼ºä¹è¯­ä¹‰ç†è§£ã€‚</li>
<li>æœ¬æ–‡æå‡ºTransFlowï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ä¸­çš„è¿åŠ¨çŸ¥è¯†æ¥ç”Ÿæˆç°å®è®­ç»ƒæ•°æ®ã€‚</li>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹èƒ½ä»å¤§è§„æ¨¡è§†é¢‘æ•°æ®ä¸­å­¦ä¹ ä¸°å¯Œçš„è¯­ä¹‰è¿åŠ¨å…ˆéªŒçŸ¥è¯†ã€‚</li>
<li>TransFlowèƒ½ç”Ÿæˆè¯­ä¹‰æ„ŸçŸ¥çš„å…‰æµï¼Œå…¶ä¸­å¯¹è±¡è¡¨ç°å‡ºè‡ªç„¶è¿åŠ¨æ¨¡å¼ï¼ŒåŒæ—¶ä¿ç•™ç©ºé—´è¾¹ç•Œå’Œæ—¶é—´è¿è´¯æ€§ã€‚</li>
<li>TransFlowæ–¹æ³•åœ¨å¤šåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5b5d6bec1d67f295070090efadc83e3a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-74da213902d85171ee3e946b7a7d76ad.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-88fed12741f5ee32c0a6dd61a883c931.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-394361c8245d29e08484717603118671.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-04702e870d66490ab2db9d2c234a5d81.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5f00ba4df57d8e7f58e4e24e3f20c395.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Synthetic-to-Real-Camouflaged-Object-Detection"><a href="#Synthetic-to-Real-Camouflaged-Object-Detection" class="headerlink" title="Synthetic-to-Real Camouflaged Object Detection"></a>Synthetic-to-Real Camouflaged Object Detection</h2><p><strong>Authors:Zhihao Luo, Luojun Lin, Zheng Lin</strong></p>
<p>Due to the high cost of collection and labeling, there are relatively few datasets for camouflaged object detection (COD). In particular, for certain specialized categories, the available image dataset is insufficiently populated. Synthetic datasets can be utilized to alleviate the problem of limited data to some extent. However, directly training with synthetic datasets compared to real datasets can lead to a degradation in model performance. To tackle this problem, in this work, we investigate a new task, namely Syn-to-Real Camouflaged Object Detection (S2R-COD). In order to improve the model performance in real world scenarios, a set of annotated synthetic camouflaged images and a limited number of unannotated real images must be utilized. We propose the Cycling Syn-to-Real Domain Adaptation Framework (CSRDA), a method based on the student-teacher model. Specially, CSRDA propagates class information from the labeled source domain to the unlabeled target domain through pseudo labeling combined with consistency regularization. Considering that narrowing the intra-domain gap can improve the quality of pseudo labeling, CSRDA utilizes a recurrent learning framework to build an evolving real domain for bridging the source and target domain. Extensive experiments demonstrate the effectiveness of our framework, mitigating the problem of limited data and handcraft annotations in COD. Our code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Muscape/S2R-COD">https://github.com/Muscape/S2R-COD</a>. </p>
<blockquote>
<p>ç”±äºæ”¶é›†ä¸æ ‡è®°çš„æˆæœ¬è¾ƒé«˜ï¼Œé’ˆå¯¹éšè”½ç›®æ ‡æ£€æµ‹ï¼ˆCODï¼‰çš„æ•°æ®é›†ç›¸å¯¹è¾ƒå°‘ã€‚å°¤å…¶æ˜¯é’ˆå¯¹æŸäº›ç‰¹å®šç±»åˆ«ï¼Œå¯ç”¨çš„å›¾åƒæ•°æ®é›†ä¸å¤Ÿä¸°å¯Œã€‚åœ¨ä¸€å®šç¨‹åº¦ä¸Šï¼Œå¯ä»¥åˆ©ç”¨åˆæˆæ•°æ®é›†æ¥ç¼“è§£æ•°æ®æœ‰é™çš„é—®é¢˜ã€‚ç„¶è€Œï¼Œä¸çœŸå®æ•°æ®é›†ç›¸æ¯”ï¼Œç›´æ¥ä½¿ç”¨åˆæˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­ç ”ç©¶äº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼Œå³åˆæˆåˆ°çœŸå®éšè”½ç›®æ ‡æ£€æµ‹ï¼ˆS2R-CODï¼‰ã€‚ä¸ºäº†æé«˜æ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸­çš„æ€§èƒ½ï¼Œå¿…é¡»ä½¿ç”¨ä¸€ç»„å¸¦æ³¨é‡Šçš„åˆæˆéšè”½å›¾åƒå’Œå°‘é‡æœªæ³¨é‡Šçš„çœŸå®å›¾åƒã€‚æˆ‘ä»¬æå‡ºäº†å¾ªç¯åˆæˆåˆ°çœŸå®åŸŸé€‚åº”æ¡†æ¶ï¼ˆCSRDAï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå¸ˆå¾’æ¨¡å‹çš„æ–¹æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼ŒCSRDAé€šè¿‡ä¼ªæ ‡ç­¾ä¸ä¸€è‡´æ€§æ­£åˆ™åŒ–ç›¸ç»“åˆï¼Œå°†ç±»ä¿¡æ¯ä»æ ‡è®°çš„æºåŸŸä¼ æ’­åˆ°æœªæ ‡è®°çš„ç›®æ ‡åŸŸã€‚è€ƒè™‘åˆ°ç¼©å°åŸŸå†…å·®è·å¯ä»¥æé«˜ä¼ªæ ‡ç­¾çš„è´¨é‡ï¼ŒCSRDAåˆ©ç”¨é€’å½’å­¦ä¹ æ¡†æ¶æ¥æ„å»ºè¿æ¥æºåŸŸå’Œç›®æ ‡åŸŸçš„ä¸æ–­å‘å±•çš„çœŸå®åŸŸã€‚å¤§é‡å®éªŒè¯æ˜äº†æˆ‘ä»¬æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œç¼“è§£äº†CODä¸­æ•°æ®æœ‰é™å’Œæ‰‹å·¥æ ‡æ³¨çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/Muscape/S2R-COD%E3%80%82">https://github.com/Muscape/S2R-CODã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18911v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†åœ¨ä¼ªè£…ç›®æ ‡æ£€æµ‹ä¸­å› æ•°æ®æ”¶é›†ä¸æ ‡æ³¨æˆæœ¬é«˜æ˜‚å¯¼è‡´çš„æ•°æ®é›†ç¨€å°‘é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯ä¸€äº›ç‰¹å®šç±»åˆ«çš„å›¾åƒæ•°æ®é›†ä¸¥é‡ä¸è¶³ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†åˆ©ç”¨åˆæˆæ•°æ®é›†çš„æ–¹æ³•ã€‚ä½†ç›´æ¥ä½¿ç”¨åˆæˆæ•°æ®é›†è¿›è¡Œè®­ç»ƒä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ç ”ç©¶äº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼Œå³åˆæˆåˆ°ç°å®ä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼ˆS2R-CODï¼‰ã€‚ä¸ºæé«˜æ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸­çš„æ€§èƒ½ï¼Œä½¿ç”¨äº†ä¸€æ‰¹æ ‡æ³¨è¿‡çš„åˆæˆä¼ªè£…å›¾åƒå’Œå°‘é‡æœªæ ‡æ³¨çš„çœŸå®å›¾åƒã€‚æå‡ºäº†åŸºäºå­¦ç”Ÿ-æ•™å¸ˆæ¨¡å‹çš„å¾ªç¯åˆæˆåˆ°ç°å®åŸŸé€‚åº”æ¡†æ¶ï¼ˆCSRDAï¼‰ã€‚CSRDAé€šè¿‡ä¼ªæ ‡ç­¾å’Œä¸€è‡´æ€§æ­£åˆ™åŒ–å°†æºåŸŸçš„ç±»åˆ«ä¿¡æ¯ä¼ é€’ç»™ç›®æ ‡åŸŸã€‚åŒæ—¶ï¼Œç¼©å°æºåŸŸå’Œç›®æ ‡åŸŸä¹‹é—´çš„å†…éƒ¨å·®è·å¯ä»¥æé«˜ä¼ªæ ‡ç­¾çš„è´¨é‡ï¼Œå› æ­¤CSRDAé‡‡ç”¨é€’å½’å­¦ä¹ æ¡†æ¶æ¥æ„å»ºä¸æ–­æ¼”å˜çš„çœŸå®åŸŸä»¥ç¼©å°ä¸¤è€…ä¹‹é—´çš„å·®è·ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆè§£å†³äº†ä¼ªè£…ç›®æ ‡æ£€æµ‹ä¸­çš„æ•°æ®æœ‰é™å’Œæ‰‹å·¥æ ‡æ³¨é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ªè£…ç›®æ ‡æ£€æµ‹æ•°æ®é›†å› é«˜æˆæœ¬è€Œç›¸å¯¹ç¨€ç¼ºï¼Œç‰¹åˆ«æ˜¯ç‰¹å®šç±»åˆ«çš„å›¾åƒæ•°æ®é›†ã€‚</li>
<li>åˆæˆæ•°æ®é›†å¯ä»¥ç¼“è§£æ•°æ®æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>ç›´æ¥ä½¿ç”¨åˆæˆæ•°æ®é›†è®­ç»ƒä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æå‡ºæ–°çš„ä»»åŠ¡ï¼šåˆæˆåˆ°ç°å®ä¼ªè£…ç›®æ ‡æ£€æµ‹ï¼ˆS2R-CODï¼‰ã€‚</li>
<li>ä¸ºæé«˜æ¨¡å‹åœ¨ç°å®åœºæ™¯ä¸­çš„æ€§èƒ½ï¼Œç»“åˆæ ‡æ³¨çš„åˆæˆå›¾åƒå’Œå°‘é‡æœªæ ‡æ³¨çš„çœŸå®å›¾åƒã€‚</li>
<li>å¼•å…¥åŸºäºå­¦ç”Ÿ-æ•™å¸ˆæ¨¡å‹çš„å¾ªç¯åˆæˆåˆ°ç°å®åŸŸé€‚åº”æ¡†æ¶ï¼ˆCSRDAï¼‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18911">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d7ff2d8ddb9c3b01a6966ffab45507f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-75e9db74f7125e642e22aa11a6ce974f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ebcc19f64a2b2b6a338303b01330073f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-706dcf10868c96eb83d1b7b790ce4767.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-111c00b5110eac9f6dd1d5502a7113d8.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Boosting-Multi-View-Indoor-3D-Object-Detection-via-Adaptive-3D-Volume-Construction"><a href="#Boosting-Multi-View-Indoor-3D-Object-Detection-via-Adaptive-3D-Volume-Construction" class="headerlink" title="Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume   Construction"></a>Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume   Construction</h2><p><strong>Authors:Runmin Zhang, Zhu Yu, Si-Yuan Cao, Lingyu Zhu, Guangyi Zhang, Xiaokai Bai, Hui-Liang Shen</strong></p>
<p>This work presents SGCDet, a novel multi-view indoor 3D object detection framework based on adaptive 3D volume construction. Unlike previous approaches that restrict the receptive field of voxels to fixed locations on images, we introduce a geometry and context aware aggregation module to integrate geometric and contextual information within adaptive regions in each image and dynamically adjust the contributions from different views, enhancing the representation capability of voxel features. Furthermore, we propose a sparse volume construction strategy that adaptively identifies and selects voxels with high occupancy probabilities for feature refinement, minimizing redundant computation in free space. Benefiting from the above designs, our framework achieves effective and efficient volume construction in an adaptive way. Better still, our network can be supervised using only 3D bounding boxes, eliminating the dependence on ground-truth scene geometry. Experimental results demonstrate that SGCDet achieves state-of-the-art performance on the ScanNet, ScanNet200 and ARKitScenes datasets. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/RM-Zhang/SGCDet">https://github.com/RM-Zhang/SGCDet</a>. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†SGCDetï¼Œè¿™æ˜¯ä¸€ç§åŸºäºè‡ªé€‚åº”3Dä½“ç§¯æ„å»ºçš„å¤šè§†è§’å®¤å†…3Dç›®æ ‡æ£€æµ‹æ–°æ¡†æ¶ã€‚ä¸åŒäºä¹‹å‰å°†ä½“ç´ æ„Ÿå—é‡é™åˆ¶åœ¨å›¾åƒå›ºå®šä½ç½®çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªèåˆå‡ ä½•å’Œä¸Šä¸‹æ–‡ä¿¡æ¯çš„èšåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥æ•´åˆæ¯ä¸ªå›¾åƒè‡ªé€‚åº”åŒºåŸŸå†…çš„å‡ ä½•å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶åŠ¨æ€è°ƒæ•´ä¸åŒè§†è§’çš„è´¡çŒ®ï¼Œå¢å¼ºäº†ä½“ç´ ç‰¹å¾çš„è¡¨ç¤ºèƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨€ç–ä½“ç§¯æ„å»ºç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥è‡ªé€‚åº”åœ°è¯†åˆ«å’Œé€‰æ‹©é«˜å ç”¨æ¦‚ç‡çš„ä½“ç´ è¿›è¡Œç‰¹å¾ç»†åŒ–ï¼Œä»è€Œæœ€å°åŒ–ç©ºé—²ç©ºé—´çš„å†—ä½™è®¡ç®—ã€‚å¾—ç›Šäºä¸Šè¿°è®¾è®¡ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä»¥è‡ªé€‚åº”çš„æ–¹å¼å®ç°äº†æœ‰æ•ˆä¸”é«˜æ•ˆçš„ä½“ç§¯æ„å»ºã€‚æ›´å¥½çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç½‘ç»œä»…ä½¿ç”¨3Dè¾¹ç•Œæ¡†è¿›è¡Œç›‘ç®¡ï¼Œä»è€Œæ¶ˆé™¤äº†å¯¹çœŸå®åœºæ™¯å‡ ä½•çš„ä¾èµ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSGCDetåœ¨ScanNetã€ScanNet200å’ŒARKitScenesæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æºä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/RM-Zhang/SGCDet%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/RM-Zhang/SGCDetæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18331v1">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†SGCDetï¼Œä¸€ç§åŸºäºè‡ªé€‚åº”ä¸‰ç»´ä½“ç§¯æ„å»ºçš„å¤šè§†è§’å®¤å†…3Dç›®æ ‡æ£€æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å‡ ä½•å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èšåˆæ¨¡å—ï¼ŒåŠ¨æ€è°ƒæ•´ä¸åŒè§†è§’çš„è´¡çŒ®ï¼Œå¢å¼ºä½“ç§¯ç‰¹å¾çš„è¡¨ç¤ºèƒ½åŠ›ã€‚åŒæ—¶ï¼Œæå‡ºä¸€ç§ç¨€ç–ä½“ç§¯æ„å»ºç­–ç•¥ï¼Œè‡ªé€‚åº”è¯†åˆ«å¹¶é€‰æ‹©é«˜å ç”¨æ¦‚ç‡çš„ä½“ç´ è¿›è¡Œç‰¹å¾ç»†åŒ–ï¼Œå‡å°‘è‡ªç”±ç©ºé—´ä¸­çš„å†—ä½™è®¡ç®—ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSGCDetåœ¨ScanNetã€ScanNet200å’ŒARKitScenesæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SGCDetæ˜¯ä¸€ç§å¤šè§†è§’å®¤å†…3Dç›®æ ‡æ£€æµ‹æ¡†æ¶ï¼ŒåŸºäºè‡ªé€‚åº”ä¸‰ç»´ä½“ç§¯æ„å»ºã€‚</li>
<li>å¼•å…¥å‡ ä½•å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èšåˆæ¨¡å—ï¼Œæ•´åˆå›¾åƒä¸­çš„å‡ ä½•å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚</li>
<li>åŠ¨æ€è°ƒæ•´ä¸åŒè§†è§’çš„è´¡çŒ®ï¼Œå¢å¼ºä½“ç§¯ç‰¹å¾çš„è¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>æå‡ºç¨€ç–ä½“ç§¯æ„å»ºç­–ç•¥ï¼Œè‡ªé€‚åº”è¯†åˆ«å¹¶é€‰æ‹©é«˜å ç”¨æ¦‚ç‡çš„ä½“ç´ ã€‚</li>
<li>æ¡†æ¶å¯åœ¨æ— éœ€åœ°é¢çœŸå®åœºæ™¯å‡ ä½•ä¿¡æ¯çš„æƒ…å†µä¸‹è¿›è¡Œç›‘ç£è®­ç»ƒã€‚</li>
<li>å®éªŒç»“æœè¯æ˜SGCDetåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°é¢†å…ˆæ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18331">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7c48cc645fc23489b577f79f9d392434.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e93152d81ea8e89184ee47fc4810610b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-eae6152bd694c9a20874b5d96fe62b09.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4e3957f1aa79cc9a30c4048af9e3a2ea.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-48c4faf331fd575e711ee3e58af96274.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LMM-Det-Make-Large-Multimodal-Models-Excel-in-Object-Detection"><a href="#LMM-Det-Make-Large-Multimodal-Models-Excel-in-Object-Detection" class="headerlink" title="LMM-Det: Make Large Multimodal Models Excel in Object Detection"></a>LMM-Det: Make Large Multimodal Models Excel in Object Detection</h2><p><strong>Authors:Jincheng Li, Chunyu Xie, Ji Ao, Dawei Leng, Yuhui Yin</strong></p>
<p>Large multimodal models (LMMs) have garnered wide-spread attention and interest within the artificial intelligence research and industrial communities, owing to their remarkable capability in multimodal understanding, reasoning, and in-context learning, among others. While LMMs have demonstrated promising results in tackling multimodal tasks like image captioning, visual question answering, and visual grounding, the object detection capabilities of LMMs exhibit a significant gap compared to specialist detectors. To bridge the gap, we depart from the conventional methods of integrating heavy detectors with LMMs and propose LMM-Det, a simple yet effective approach that leverages a Large Multimodal Model for vanilla object Detection without relying on specialized detection modules. Specifically, we conduct a comprehensive exploratory analysis when a large multimodal model meets with object detection, revealing that the recall rate degrades significantly compared with specialist detection models. To mitigate this, we propose to increase the recall rate by introducing data distribution adjustment and inference optimization tailored for object detection. We re-organize the instruction conversations to enhance the object detection capabilities of large multimodal models. We claim that a large multimodal model possesses detection capability without any extra detection modules. Extensive experiments support our claim and show the effectiveness of the versatile LMM-Det. The datasets, models, and codes are available at <a target="_blank" rel="noopener" href="https://github.com/360CVGroup/LMM-Det">https://github.com/360CVGroup/LMM-Det</a>. </p>
<blockquote>
<p>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰å› å…¶å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›è€Œå¼•èµ·äº†äººå·¥æ™ºèƒ½ç ”ç©¶å’Œå·¥ä¸šç•Œçš„å¹¿æ³›å…³æ³¨ã€‚è™½ç„¶LMMsåœ¨å¤„ç†å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œè§†è§‰å®šä½ç­‰å¤šæ¨¡æ€ä»»åŠ¡æ–¹é¢å±•ç°å‡ºäº†è‰¯å¥½çš„æ½œåŠ›ï¼Œä½†åœ¨å¯¹è±¡æ£€æµ‹æ–¹é¢çš„èƒ½åŠ›ä¸ä¸“ä¸šæ£€æµ‹å™¨ç›¸æ¯”ä»å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ä¸ºäº†å¼¥å·®è·ï¼Œæˆ‘ä»¬æ”¾å¼ƒäº†å°†é‡å‹æ£€æµ‹å™¨ä¸LMMsé›†æˆç»“åˆçš„å¸¸è§„æ–¹æ³•ï¼Œå¹¶æå‡ºäº†LMM-Detã€‚è¿™æ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡ŒåŸºæœ¬å¯¹è±¡æ£€æµ‹ï¼Œæ— éœ€ä¾èµ–ä¸“ä¸šæ£€æµ‹æ¨¡å—ã€‚å…·ä½“æ¥è¯´ï¼Œå½“å¤§å‹å¤šæ¨¡æ€æ¨¡å‹é‡åˆ°å¯¹è±¡æ£€æµ‹æ—¶ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å…¨é¢çš„æ¢ç´¢æ€§åˆ†æï¼Œå‘ç°å…¶å¬å›ç‡ä¸ä¸“ä¸šæ£€æµ‹æ¨¡å‹ç›¸æ¯”æ˜¾è‘—é™ä½ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æè®®é€šè¿‡å¼•å…¥é’ˆå¯¹å¯¹è±¡æ£€æµ‹çš„æ•°æ®åˆ†å¸ƒè°ƒæ•´å’Œæ¨ç†ä¼˜åŒ–æ¥æé«˜å¬å›ç‡ã€‚æˆ‘ä»¬é‡æ–°ç»„ç»‡æŒ‡ä»¤å¯¹è¯ï¼Œä»¥å¢å¼ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„å¯¹è±¡æ£€æµ‹èƒ½åŠ›ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹æœ¬èº«å°±å…·å¤‡æ£€æµ‹èƒ½åŠ›ï¼Œæ— éœ€é¢å¤–çš„æ£€æµ‹æ¨¡å—ã€‚å¤§é‡å®éªŒæ”¯æŒæˆ‘ä»¬çš„è§‚ç‚¹å¹¶è¯æ˜äº†LMM-Detçš„æœ‰æ•ˆæ€§ã€‚ç›¸å…³æ•°æ®é›†ã€æ¨¡å‹å’Œä»£ç å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/360CVGroup/LMM-Det%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/360CVGroup/LMM-Detè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.18300v1">PDF</a> Accepted at ICCV 2025</p>
<p><strong>Summary</strong>ï¼šå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨äººå·¥æ™ºèƒ½ç ”ç©¶å’Œå·¥ä¸šç•Œå¼•èµ·äº†å¹¿æ³›å…³æ³¨ï¼Œå› å…¶å…·æœ‰å‡ºè‰²çš„å¤šæ¨¡æ€ç†è§£ã€æ¨ç†å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ç­‰ä¼˜ç‚¹ã€‚è™½ç„¶LMMsåœ¨å¤šæ¨¡æ€ä»»åŠ¡å¦‚å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œè§†è§‰å®šä½ç­‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢ä¸ä¸“ä¸šæ£€æµ‹å™¨ç›¸æ¯”å­˜åœ¨æ˜æ˜¾å·®è·ã€‚ä¸ºäº†ç¼©å°è¿™ä¸€å·®è·ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•LMM-Detï¼Œå®ƒåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡ŒåŸºæœ¬çš„å¯¹è±¡æ£€æµ‹ï¼Œæ— éœ€ä¾èµ–ä¸“ä¸šæ£€æµ‹æ¨¡å—ã€‚é€šè¿‡å¼•å…¥æ•°æ®åˆ†å¸ƒè°ƒæ•´å’Œé’ˆå¯¹å¯¹è±¡æ£€æµ‹çš„æ¨ç†ä¼˜åŒ–ï¼Œæé«˜äº†å¬å›ç‡ã€‚é€šè¿‡é‡ç»„æŒ‡ä»¤å¯¹è¯ï¼Œå¢å¼ºäº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„ç›®æ ‡æ£€æµ‹èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå—åˆ°å¹¿æ³›å…³æ³¨ï¼Œå…·æœ‰å‡ºè‰²çš„å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>LMMsåœ¨å¤šæ¨¡æ€ä»»åŠ¡ï¼ˆå¦‚å›¾åƒæè¿°ã€è§†è§‰é—®ç­”å’Œè§†è§‰å®šä½ï¼‰ä¸Šè¡¨ç°å‡ºè‰²ã€‚</li>
<li>LMMsåœ¨ç›®æ ‡æ£€æµ‹æ–¹é¢ä¸ä¸“ä¸šæ£€æµ‹å™¨ç›¸æ¯”å­˜åœ¨å·®è·ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•LMM-Detï¼Œåˆ©ç”¨å¤§å‹å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œå¯¹è±¡æ£€æµ‹ï¼Œæ— éœ€ä¸“ä¸šæ£€æµ‹æ¨¡å—ã€‚</li>
<li>é€šè¿‡æ•°æ®åˆ†å¸ƒè°ƒæ•´å’Œæ¨ç†ä¼˜åŒ–ï¼Œæé«˜äº†ç›®æ ‡æ£€æµ‹çš„å¬å›ç‡ã€‚</li>
<li>é€šè¿‡é‡ç»„æŒ‡ä»¤å¯¹è¯ï¼Œå¢å¼ºäº†å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„ç›®æ ‡æ£€æµ‹èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.18300">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca2569388d0cc39497566f9a290d6239.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0159604453837cf904a1779761f3448.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-138f23713ea43be60f7b7d6e4f6016b5.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0863976c1d2f8bb14832678686a3ab78.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="AFRDA-Attentive-Feature-Refinement-for-Domain-Adaptive-Semantic-Segmentation"><a href="#AFRDA-Attentive-Feature-Refinement-for-Domain-Adaptive-Semantic-Segmentation" class="headerlink" title="AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic   Segmentation"></a>AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic   Segmentation</h2><p><strong>Authors:Md. Al-Masrur Khan, Durgakant Pushp, Lantao Liu</strong></p>
<p>In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is trained on labeled source domain data (e.g., synthetic images) and adapted to an unlabeled target domain (e.g., real-world images) without access to target annotations. Existing UDA-SS methods often struggle to balance fine-grained local details with global contextual information, leading to segmentation errors in complex regions. To address this, we introduce the Adaptive Feature Refinement (AFR) module, which enhances segmentation accuracy by refining highresolution features using semantic priors from low-resolution logits. AFR also integrates high-frequency components, which capture fine-grained structures and provide crucial boundary information, improving object delineation. Additionally, AFR adaptively balances local and global information through uncertaintydriven attention, reducing misclassifications. Its lightweight design allows seamless integration into HRDA-based UDA methods, leading to state-of-the-art segmentation performance. Our approach improves existing UDA-SS methods by 1.05% mIoU on GTA V â€“&gt; Cityscapes and 1.04% mIoU on Synthiaâ€“&gt;Cityscapes. The implementation of our framework is available at: <a target="_blank" rel="noopener" href="https://github.com/Masrur02/AFRDA">https://github.com/Masrur02/AFRDA</a> </p>
<blockquote>
<p>åœ¨æ— éœ€ç›‘ç£çš„é¢†åŸŸè‡ªé€‚åº”è¯­ä¹‰åˆ†å‰²ï¼ˆUDA-SSï¼‰ä¸­ï¼Œæ¨¡å‹ä¼šåœ¨æ ‡æ³¨çš„æºåŸŸæ•°æ®ï¼ˆä¾‹å¦‚ï¼Œåˆæˆå›¾åƒï¼‰ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶é€‚åº”æœªæ ‡æ³¨çš„ç›®æ ‡åŸŸï¼ˆä¾‹å¦‚ï¼ŒçœŸå®ä¸–ç•Œå›¾åƒï¼‰ï¼Œè€Œæ— éœ€è®¿é—®ç›®æ ‡æ³¨é‡Šã€‚ç°æœ‰çš„UDA-SSæ–¹æ³•å¾€å¾€éš¾ä»¥å¹³è¡¡ç²¾ç»†çš„å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¼è‡´åœ¨å¤æ‚åŒºåŸŸå‡ºç°åˆ†å‰²é”™è¯¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†è‡ªé€‚åº”ç‰¹å¾ç»†åŒ–ï¼ˆAFRï¼‰æ¨¡å—ï¼Œå®ƒé€šè¿‡åˆ©ç”¨ä½åˆ†è¾¨ç‡é€»è¾‘ä¸­çš„è¯­ä¹‰å…ˆéªŒæ¥ç»†åŒ–é«˜åˆ†è¾¨ç‡ç‰¹å¾ï¼Œä»è€Œæé«˜åˆ†å‰²å‡†ç¡®æ€§ã€‚AFRè¿˜é›†æˆäº†é«˜é¢‘ç»„ä»¶ï¼Œè¿™äº›ç»„ä»¶å¯ä»¥æ•æ‰ç²¾ç»†ç»“æ„å¹¶æä¾›å…³é”®è¾¹ç•Œä¿¡æ¯ï¼Œä»è€Œæ”¹å–„å¯¹è±¡è½®å»“ã€‚æ­¤å¤–ï¼ŒAFRé€šè¿‡ä¸ç¡®å®šæ€§é©±åŠ¨çš„å…³æ³¨æœºåˆ¶è‡ªé€‚åº”åœ°å¹³è¡¡å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ï¼Œå‡å°‘è¯¯åˆ†ç±»ã€‚å…¶è½»é‡çº§çš„è®¾è®¡å¯ä»¥æ— ç¼é›†æˆåˆ°åŸºäºHRDAçš„UDAæ–¹æ³•ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨GTA Våˆ°Cityscapeså’ŒSynthiaåˆ°Cityscapesçš„UDA-SSä»»åŠ¡ä¸Šåˆ†åˆ«æé«˜äº†1.05%å’Œ1.04%çš„mIoUã€‚æˆ‘ä»¬æ¡†æ¶çš„å®ç°å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/Masrur02/AFRDA%E3%80%82">https://github.com/Masrur02/AFRDAã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17957v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†æ— ç›‘ç£åŸŸè‡ªé€‚åº”è¯­ä¹‰åˆ†å‰²ï¼ˆUDA-SSï¼‰ä¸­çš„ä¸€é¡¹æ–°æŠ€æœ¯â€”â€”è‡ªé€‚åº”ç‰¹å¾ç»†åŒ–ï¼ˆAFRï¼‰æ¨¡å—ã€‚è¯¥æŠ€æœ¯èƒ½å¤Ÿåœ¨æ— éœ€ç›®æ ‡åŸŸæ ‡æ³¨çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒåœ¨æºåŸŸæ•°æ®ä¸Šçš„æ¨¡å‹å¹¶é€‚åº”ç›®æ ‡åŸŸã€‚é€šè¿‡åˆ©ç”¨ä½åˆ†è¾¨ç‡é€»è¾‘è¯­ä¹‰å…ˆéªŒå¯¹é«˜åˆ†è¾¨ç‡ç‰¹å¾è¿›è¡Œç»†åŒ–ï¼Œå¹¶é›†æˆé«˜é¢‘ç‡ç»„ä»¶ä»¥æ•è·ç²¾ç»†ç»“æ„å¹¶æä¾›å…³é”®è¾¹ç•Œä¿¡æ¯ï¼Œä»è€Œæé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚æ­¤å¤–ï¼Œå®ƒé€šè¿‡ä¸ç¡®å®šæ€§é©±åŠ¨çš„æ³¨æ„åŠ›æœºåˆ¶è‡ªé€‚åº”åœ°å¹³è¡¡å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ï¼Œå‡å°‘äº†è¯¯åˆ†ç±»ã€‚è¯¥è®¾è®¡è½»å·§ï¼Œå¯æ— ç¼é›†æˆåˆ°åŸºäºHRDAçš„UDAæ–¹æ³•ä¸­ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„åˆ†å‰²æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UDA-SSæŠ€æœ¯é¢ä¸´åœ¨å¤æ‚åŒºåŸŸä¸­å¹³è¡¡å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¿¡æ¯çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´åˆ†å‰²é”™è¯¯ã€‚</li>
<li>æ–°æŠ€æœ¯AFRæ¨¡å—é€šè¿‡åˆ©ç”¨ä½åˆ†è¾¨ç‡é€»è¾‘è¯­ä¹‰å…ˆéªŒå¯¹é«˜åˆ†è¾¨ç‡ç‰¹å¾è¿›è¡Œç»†åŒ–ï¼Œæé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚</li>
<li>AFRæ¨¡å—é›†æˆé«˜é¢‘ç‡ç»„ä»¶ï¼Œæ•è·ç²¾ç»†ç»“æ„å¹¶æä¾›å…³é”®è¾¹ç•Œä¿¡æ¯ï¼Œæ”¹å–„ç‰©ä½“è½®å»“æç»˜ã€‚</li>
<li>é€šè¿‡ä¸ç¡®å®šæ€§é©±åŠ¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ŒAFRæ¨¡å—è‡ªé€‚åº”å¹³è¡¡å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯ï¼Œå‡å°‘è¯¯åˆ†ç±»ã€‚</li>
<li>AFRè®¾è®¡è½»å·§ï¼Œå¯æ— ç¼é›†æˆåˆ°åŸºäºHRDAçš„UDAæ–¹æ³•ä¸­ã€‚</li>
<li>ä¸ç°æœ‰UDA-SSæ–¹æ³•ç›¸æ¯”ï¼Œæ–°æ–¹æ³•åœ¨GTA Våˆ°Cityscapeså’ŒSynthiaåˆ°Cityscapesçš„æµ‹è¯•ä¸­åˆ†åˆ«æé«˜äº†1.05%å’Œ1.04%çš„mIoUã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2685a5b2372ce55ce9a9b2d95f59b4d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-56ffa08bf7947de63db2268cb28b39bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-66e2c8dc3c355432e4bd8928f062278c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a0f886511ba04d7ebfec76db0a9e1e13.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b8b4183cabc37e876d736dd6f417b8ac.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Dynamic-Scoring-with-Enhanced-Semantics-for-Training-Free-Human-Object-Interaction-Detection"><a href="#Dynamic-Scoring-with-Enhanced-Semantics-for-Training-Free-Human-Object-Interaction-Detection" class="headerlink" title="Dynamic Scoring with Enhanced Semantics for Training-Free Human-Object   Interaction Detection"></a>Dynamic Scoring with Enhanced Semantics for Training-Free Human-Object   Interaction Detection</h2><p><strong>Authors:Francesco Tonini, Lorenzo Vaquero, Alessandro Conti, Cigdem Beyan, Elisa Ricci</strong></p>
<p>Human-Object Interaction (HOI) detection aims to identify humans and objects within images and interpret their interactions. Existing HOI methods rely heavily on large datasets with manual annotations to learn interactions from visual cues. These annotations are labor-intensive to create, prone to inconsistency, and limit scalability to new domains and rare interactions. We argue that recent advances in Vision-Language Models (VLMs) offer untapped potential, particularly in enhancing interaction representation. While prior work has injected such potential and even proposed training-free methods, there remain key gaps. Consequently, we propose a novel training-free HOI detection framework for Dynamic Scoring with enhanced semantics (DYSCO) that effectively utilizes textual and visual interaction representations within a multimodal registry, enabling robust and nuanced interaction understanding. This registry incorporates a small set of visual cues and uses innovative interaction signatures to improve the semantic alignment of verbs, facilitating effective generalization to rare interactions. Additionally, we propose a unique multi-head attention mechanism that adaptively weights the contributions of the visual and textual features. Experimental results demonstrate that our DYSCO surpasses training-free state-of-the-art models and is competitive with training-based approaches, particularly excelling in rare interactions. Code is available at <a target="_blank" rel="noopener" href="https://github.com/francescotonini/dysco">https://github.com/francescotonini/dysco</a>. </p>
<blockquote>
<p>äººæœºäº¤äº’ï¼ˆHOIï¼‰æ£€æµ‹æ—¨åœ¨è¯†åˆ«å›¾åƒä¸­çš„äººç±»å’Œç‰©ä½“ï¼Œå¹¶è§£é‡Šå…¶äº¤äº’ã€‚ç°æœ‰çš„HOIæ–¹æ³•ä¸¥é‡ä¾èµ–äºå¸¦æœ‰æ‰‹åŠ¨æ³¨é‡Šçš„å¤§å‹æ•°æ®é›†æ¥å­¦ä¹ è§†è§‰çº¿ç´¢ä¸­çš„äº¤äº’ã€‚è¿™äº›æ³¨é‡Šçš„åˆ›å»ºéœ€è¦å¤§é‡åŠ³åŠ¨åŠ›ï¼Œå®¹æ˜“å‡ºç°ä¸ä¸€è‡´ï¼Œå¹¶ä¸”åœ¨æ–°é¢†åŸŸå’Œç½•è§äº¤äº’ä¸Šçš„å¯æ‰©å±•æ€§å—åˆ°é™åˆ¶ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æœ€æ–°è¿›å±•æä¾›äº†å°šæœªå¼€å‘çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¢å¼ºäº¤äº’è¡¨ç¤ºæ–¹é¢ã€‚å°½ç®¡å…ˆå‰çš„å·¥ä½œå·²ç»æ³¨å…¥äº†è¿™ç§æ½œåŠ›ï¼Œç”šè‡³æå‡ºäº†æ— è®­ç»ƒçš„æ–¹æ³•ï¼Œä½†ä»å­˜åœ¨å…³é”®å·®è·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— è®­ç»ƒHOIæ£€æµ‹æ¡†æ¶åŠ¨æ€è¯„åˆ†å¢å¼ºè¯­ä¹‰ï¼ˆDYSCOï¼‰ï¼Œè¯¥æ¡†æ¶æœ‰æ•ˆåˆ©ç”¨å¤šæ¨¡å¼æ³¨å†Œè¡¨ä¸­çš„æ–‡æœ¬å’Œè§†è§‰äº¤äº’è¡¨ç¤ºï¼Œå®ç°ç¨³å¥è€Œç»†å¾®çš„äº¤äº’ç†è§£ã€‚è¯¥æ³¨å†Œè¡¨ç»“åˆäº†å°‘é‡è§†è§‰çº¿ç´¢ï¼Œå¹¶ä½¿ç”¨åˆ›æ–°çš„äº¤äº’ç­¾åæ”¹è¿›åŠ¨è¯çš„è¯­ä¹‰å¯¹é½ï¼Œä¿ƒè¿›äº†å¯¹ç½•è§äº¤äº’çš„æœ‰æ•ˆæ³›åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç‹¬ç‰¹çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°åŠ æƒè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„è´¡çŒ®ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„DYSCOè¶…è¶Šäº†æ— è®­ç»ƒçš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå¹¶ä¸åŸºäºè®­ç»ƒçš„æ–¹æ³•ç›¸ç«äº‰ï¼Œç‰¹åˆ«æ˜¯åœ¨ç½•è§äº¤äº’æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/francescotonini/dysco%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/francescotonini/dyscoæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.17456v1">PDF</a> Accepted to ACM Multimedia 2025</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†Human-Object Interactionï¼ˆHOIï¼‰æ£€æµ‹çš„ä»»åŠ¡ç›®æ ‡ï¼Œå³è¯†åˆ«å›¾åƒä¸­çš„äººç±»å’Œç‰©ä½“å¹¶è§£é‡Šå…¶äº¤äº’ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–äºå¤§é‡æ‰‹å·¥æ ‡æ³¨çš„æ•°æ®é›†æ¥å­¦ä¹ è§†è§‰çº¿ç´¢ä¸­çš„äº¤äº’ï¼Œä½†è¿™äº›æ ‡æ³¨åˆ›å»ºæˆæœ¬é«˜ã€æ˜“å‡ºé”™ä¸”å¯¹æ–°é¢†åŸŸå’Œç½•è§äº¤äº’çš„å¯æ‰©å±•æ€§æœ‰é™ã€‚æ–‡ç« æŒ‡å‡ºï¼Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„è¿‘æœŸè¿›å±•å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¢å¼ºäº¤äº’è¡¨ç¤ºæ–¹é¢ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— éœ€è®­ç»ƒçš„äººæœºäº¤äº’æ£€æµ‹æ¡†æ¶DYSCOï¼Œå®ƒåˆ©ç”¨å¤šæ¨¡å¼æ³¨å†Œä¸­çš„æ–‡æœ¬å’Œè§†è§‰äº¤äº’è¡¨ç¤ºï¼Œå®ç°ç¨³å¥å’Œç»†å¾®çš„äº¤äº’ç†è§£ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ä¸€å°éƒ¨åˆ†è§†è§‰çº¿ç´¢å¹¶åˆ›æ–°ä½¿ç”¨äº¤äº’ç­¾åæ¥æ”¹è¿›åŠ¨è¯çš„è¯­ä¹‰å¯¹é½ï¼Œèƒ½å¾ˆå¥½åœ°æ³›åŒ–åˆ°ç½•è§çš„äº¤äº’ä¸Šã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æå‡ºäº†ä¸€ç§ç‹¬ç‰¹çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥è‡ªé€‚åº”åœ°åŠ æƒè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„è´¡çŒ®ã€‚å®éªŒè¡¨æ˜ï¼ŒDYSCOè¶…è¶Šäº†æ— è®­ç»ƒçŠ¶æ€ä¸‹çš„æœ€æ–°æ¨¡å‹ï¼Œå¹¶ä¸”ä¸åŸºäºè®­ç»ƒçš„æ–¹æ³•ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›ï¼Œå°¤å…¶åœ¨ç½•è§äº¤äº’æ–¹é¢è¡¨ç°çªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>Human-Object Interaction (HOI) detectionçš„ç›®æ ‡æ˜¯è¯†åˆ«å›¾åƒä¸­çš„çš„äººå’Œç‰©ä½“å¹¶è§£é‡Šå…¶äº¤äº’ã€‚</li>
<li>ç°æœ‰HOIæ£€æµ‹æ–¹æ³•ä¾èµ–äºå¤§é‡æ‰‹å·¥æ ‡æ³¨çš„æ•°æ®é›†ï¼Œå­˜åœ¨åˆ›å»ºæˆæœ¬é«˜ã€æ˜“å‡ºé”™å’Œæ³›åŒ–èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¢å¼ºäº¤äº’è¡¨ç¤ºæ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>æå‡ºçš„DYSCOæ¡†æ¶æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„äººæœºäº¤äº’æ£€æµ‹æ¡†æ¶ï¼Œåˆ©ç”¨å¤šæ¨¡å¼æ³¨å†Œä¸­çš„æ–‡æœ¬å’Œè§†è§‰äº¤äº’è¡¨ç¤ºã€‚</li>
<li>DYSCOé‡‡ç”¨åˆ›æ–°ä½¿ç”¨äº¤äº’ç­¾åå’Œæ”¹è¿›åŠ¨è¯çš„è¯­ä¹‰å¯¹é½æ¥æ³›åŒ–åˆ°ç½•è§çš„äº¤äº’ã€‚</li>
<li>DYSCOé‡‡ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œè‡ªé€‚åº”åœ°åŠ æƒè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾çš„è´¡çŒ®ã€‚</li>
<li>å®éªŒè¡¨æ˜DYSCOåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹è¡¨ç°è¶…è¶Šæœ€æ–°æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨ç½•è§äº¤äº’æ–¹é¢è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.17456">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e68b3ea87d5587dbeb92dc96d5edf1c0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-62354f4f46c83e49f5d1b77485471dbb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-485e9e338ccb82cba234aa0740afb54c.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">https://kedreamix.github.io/Talk2Paper/Paper/2025-08-01/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                                    <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-c571ebe75678812f54ec5c7bae6e15a0.jpg" class="responsive-img" alt="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
                        
                        <span class="card-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  Style-Aware Blending and Prototype-Based Cross-Contrast Consistency for   Semi-Supervised Medical Image Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-08-01/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a1e85749bc723dbdb5f43298501e3667.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-08-01  IN45023 Neural Network Design Patterns in Computer Vision Seminar   Report, Summer 2025
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-08-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">29997.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
