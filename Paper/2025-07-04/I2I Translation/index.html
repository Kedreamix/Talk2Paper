<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-07-04  How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b92c85046bac61be54c371272062d92f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    35 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-04-更新"><a href="#2025-07-04-更新" class="headerlink" title="2025-07-04 更新"></a>2025-07-04 更新</h1><h2 id="How-Well-Does-GPT-4o-Understand-Vision-Evaluating-Multimodal-Foundation-Models-on-Standard-Computer-Vision-Tasks"><a href="#How-Well-Does-GPT-4o-Understand-Vision-Evaluating-Multimodal-Foundation-Models-on-Standard-Computer-Vision-Tasks" class="headerlink" title="How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks"></a>How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks</h2><p><strong>Authors:Rahul Ramachandran, Ali Garjani, Roman Bachmann, Andrei Atanov, Oğuzhan Fatih Kar, Amir Zamir</strong></p>
<p>Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets (e.g., COCO, ImageNet and its variants, etc).   The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework.   We observe that 1) the models are not close to the state-of-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a preliminary analysis of models with native image generation, like the latest GPT-4o, shows they exhibit quirks like hallucinations and spatial misalignments. </p>
<blockquote>
<p>多模态基础模型，如GPT-4o，最近取得了显著的进步，但在视觉理解方面，这些模型的具体进展尚不清楚。在本文中，我们在标准计算机视觉任务（语义分割、目标检测、图像分类、深度和表面法线预测）上，使用既定数据集（例如COCO、ImageNet及其变体等）对流行多模态基础模型（GPT-4o、o4-mini、Gemini 1.5 Pro和Gemini 2.0 Flash、Claude 3.5 Sonnet、Qwen2-VL、Llama 3.2）的性能进行了评估。执行此任务的主要挑战在于：1）大多数模型被训练用于输出文本，无法原生表达诸如片段或3D几何等多样领域；2）许多领先的模型是专有模型，只能通过API级别访问，即无法获取其权重以进行适应。我们通过通过提示链将标准视觉任务转换为等效的文本提示和API兼容任务，创建一个标准化的基准测试框架，来解决这些挑战。我们发现：1）这些模型在任何任务上都不接近最新专业模型；然而，2）它们是相当不错的通用模型；这相当出色，因为它们可能主要是在基于图像文本的任务上进行训练的。3）它们执行语义任务明显比几何任务更好。4）虽然提示链技术会影响性能，但更好的模型对提示变化的敏感性较低。5）GPT-4o在非推理模型中表现最佳，在6个任务中的4个任务中占据首位。6）推理模型（例如o3）在几何任务上显示出改进，7）对具有原生图像生成的模型（如最新的GPT-4o）进行初步分析表明，它们具有诸如幻觉和空间不对准等特性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01955v1">PDF</a> Project page at <a target="_blank" rel="noopener" href="https://fm-vision-evals.epfl.ch/">https://fm-vision-evals.epfl.ch/</a></p>
<p><strong>Summary</strong></p>
<p>本文对比了多款流行的多模态基础模型（如GPT-4o、o4-mini、Gemini系列等）在标准计算机视觉任务上的表现。主要挑战在于这些模型主要训练输出文本，难以直接表达多样领域如分段或三维几何。通过提示链技术将标准视觉任务转化为等效的文本提示和API兼容任务，建立标准化评估框架。观察发现这些模型虽不及专业模型，但作为全能模型表现仍值得尊敬，语义任务表现优于几何任务。不同模型的提示链技术影响性能，但更好的模型对提示变化更不敏感。GPT-4o在非推理模型中表现最佳，在四项任务中排名第一。具备原生图像生成的模型如最新GPT-4o展现出一些奇特现象如幻视和空间错位。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态基础模型在计算机视觉任务上的表现被评估，主要挑战在于模型的文本输出特性与对多样领域的表达。</li>
<li>通过提示链技术将标准视觉任务转化为文本提示和API兼容任务，建立评估框架。</li>
<li>这些模型虽不及专业模型，但作为全能模型表现仍值得尊敬。</li>
<li>语义任务表现优于几何任务。</li>
<li>提示链技术对模型性能有影响，但更好的模型对提示变化更稳定。</li>
<li>GPT-4o在非推理模型中表现最佳，在四项任务中排名第一。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01955">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-25866a56a7bb23110ef12bfd70e68e3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8987b678a6043fb17c5c89c3628d8b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8665bb9a66c6900ff01051abba32ea93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86a19d55133674d93a00588735b57752.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92aaa3e7aa83ff825c7a37d03c198977.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SafePTR-Token-Level-Jailbreak-Defense-in-Multimodal-LLMs-via-Prune-then-Restore-Mechanism"><a href="#SafePTR-Token-Level-Jailbreak-Defense-in-Multimodal-LLMs-via-Prune-then-Restore-Mechanism" class="headerlink" title="SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via   Prune-then-Restore Mechanism"></a>SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via   Prune-then-Restore Mechanism</h2><p><strong>Authors:Beitao Chen, Xinyu Lyu, Lianli Gao, Jingkuan Song, Heng Tao Shen</strong></p>
<p>By incorporating visual inputs, Multimodal Large Language Models (MLLMs) extend LLMs to support visual reasoning. However, this integration also introduces new vulnerabilities, making MLLMs susceptible to multimodal jailbreak attacks and hindering their safe deployment.Existing defense methods, including Image-to-Text Translation, Safe Prompting, and Multimodal Safety Tuning, attempt to address this by aligning multimodal inputs with LLMs’ built-in safeguards.Yet, they fall short in uncovering root causes of multimodal vulnerabilities, particularly how harmful multimodal tokens trigger jailbreak in MLLMs? Consequently, they remain vulnerable to text-driven multimodal jailbreaks, often exhibiting overdefensive behaviors and imposing heavy training overhead.To bridge this gap, we present an comprehensive analysis of where, how and which harmful multimodal tokens bypass safeguards in MLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers are responsible for inducing unsafe behaviors, highlighting the potential of precisely removing a small subset of harmful tokens, without requiring safety tuning, can still effectively improve safety against jailbreaks. Motivated by this, we propose Safe Prune-then-Restore (SafePTR), an training-free defense framework that selectively prunes harmful tokens at vulnerable layers while restoring benign features at subsequent layers.Without incurring additional computational overhead, SafePTR significantly enhances the safety of MLLMs while preserving efficiency. Extensive evaluations across three MLLMs and five benchmarks demonstrate SafePTR’s state-of-the-art performance in mitigating jailbreak risks without compromising utility. </p>
<blockquote>
<p>通过融入视觉输入，多模态大型语言模型（MLLMs）将语言模型扩展到支持视觉推理。然而，这种融合也引入了新的漏洞，使得MLLMs容易受到多模态越狱攻击的影响，并阻碍了其安全部署。现有的防御方法，包括图像到文本的翻译、安全提示和多模态安全调整，试图通过对齐多模态输入与语言模型的内置安全保护措施来解决这个问题。然而，它们未能发现多模态漏洞的根本原因，尤其是有害的多模态令牌是如何触发MLLMs中的越狱的。因此，它们仍然容易受到文本驱动的多模态越狱攻击，通常表现出过度防御的行为并产生沉重的训练负担。为了填补这一空白，我们对MLLMs中有害的多模态令牌如何绕过保护进行了综合分析。令人惊讶的是，我们发现早期中间层中不到1%的令牌负责引发不安全行为，这突出了精确移除一小部分有害令牌的可能性，而无需进行安全调整，仍然可以有效地提高对抗越狱的安全性。受此启发，我们提出了无需训练的防御框架Safe Prune-then-Restore（SafePTR），它选择性地在脆弱层中删除有害令牌，同时在后续层中恢复良性特征。SafePTR无需额外的计算开销，就能显著提高MLLMs的安全性，同时保持高效性。在三个MLLMs和五个基准测试上的广泛评估表明，SafePTR在缓解越狱风险方面表现出卓越的性能，且不影响实用性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01513v1">PDF</a> </p>
<p><strong>Summary</strong><br>    多模态大型语言模型（MLLMs）通过结合视觉输入扩展了语言模型的支持范围，支持视觉推理。但这也引入了新的漏洞，使MLLMs容易受到多模态越狱攻击，阻碍了其安全部署。现有防御方法试图通过对齐多模态输入与LLMs的内置安全机制来解决问题，但仍未能发现多模态漏洞的根本原因，特别是有害的多模态令牌如何触发MLLMs中的越狱。因此，它们仍然容易受到文本驱动的多模态越狱攻击，表现出过度防御行为和沉重的训练负担。我们对MLLMs中有害多模态令牌绕过保障的情况进行了综合分析，发现早期中间层中不到1%的令牌负责引发不安全行为。基于此发现，我们提出了无需安全调整的Safe Prune-then-Restore（SafePTR）防御框架，它可以选择性地在脆弱层删除有害令牌，并在后续层恢复良性特征。SafePTR在不增加计算开销的情况下，显著提高了MLLMs的安全性，同时保持了高效性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）支持视觉推理，但引入新的漏洞，易受到多模态越狱攻击。</li>
<li>现有防御方法试图通过对齐多模态输入与LLMs的安全机制来解决问题，但存在缺陷。</li>
<li>有害的多模态令牌是引发MLLMs不安全行为的关键。</li>
<li>仅少量令牌（不到1%的早期中间层令牌）对不安全行为负责。</li>
<li>提出Safe Prune-then-Restore（SafePTR）防御框架，能选择性删除有害令牌并恢复良性特征。</li>
<li>SafePTR显著提高MLLMs的安全性，同时保持高效性，无需额外的计算开销。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01513">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ad42cbc283f5d2079fbe6f9a2f09e0aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5a1cd7a71dd6a0e7d3c107e323e4217.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cf923b7f1cf53354b483b135ad598d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3b7b5c5fdea89b57768280f3fb79036.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5eec9fb6f6d0de7138a06a6252f715a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5757d125114ee218f4c63996390a7853.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54e48ba52b25d504a4097de0cb1c82aa.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DocShaDiffusion-Diffusion-Model-in-Latent-Space-for-Document-Image-Shadow-Removal"><a href="#DocShaDiffusion-Diffusion-Model-in-Latent-Space-for-Document-Image-Shadow-Removal" class="headerlink" title="DocShaDiffusion: Diffusion Model in Latent Space for Document Image   Shadow Removal"></a>DocShaDiffusion: Diffusion Model in Latent Space for Document Image   Shadow Removal</h2><p><strong>Authors:Wenjie Liu, Bingshu Wang, Ze Wang, C. L. Philip Chen</strong></p>
<p>Document shadow removal is a crucial task in the field of document image enhancement. However, existing methods tend to remove shadows with constant color background and ignore color shadows. In this paper, we first design a diffusion model in latent space for document image shadow removal, called DocShaDiffusion. It translates shadow images from pixel space to latent space, enabling the model to more easily capture essential features. To address the issue of color shadows, we design a shadow soft-mask generation module (SSGM). It is able to produce accurate shadow mask and add noise into shadow regions specially. Guided by the shadow mask, a shadow mask-aware guided diffusion module (SMGDM) is proposed to remove shadows from document images by supervising the diffusion and denoising process. We also propose a shadow-robust perceptual feature loss to preserve details and structures in document images. Moreover, we develop a large-scale synthetic document color shadow removal dataset (SDCSRD). It simulates the distribution of realistic color shadows and provides powerful supports for the training of models. Experiments on three public datasets validate the proposed method’s superiority over state-of-the-art. Our code and dataset will be publicly available. </p>
<blockquote>
<p>文档阴影去除是文档图像增强领域中的一项关键任务。然而，现有方法往往倾向于去除具有恒定背景色的阴影，而忽视彩色阴影。在本文中，我们首先在潜在空间设计了一个扩散模型，用于文档图像阴影去除，称为DocShaDiffusion。它将阴影图像从像素空间翻译到潜在空间，使模型更容易捕获关键特征。为了解决彩色阴影的问题，我们设计了一个阴影软掩膜生成模块（SSGM）。它能够产生精确的阴影掩膜，并在阴影区域中加入噪声。在阴影掩膜的指导下，提出了一个阴影掩膜感知引导扩散模块（SMGDM），通过监督扩散和去噪过程来去除文档图像中的阴影。我们还提出了一种阴影鲁棒的感知特征损失，以保留文档图像中的细节和结构。此外，我们开发了一个大规模合成文档彩色阴影去除数据集（SDCSRD）。它模拟了现实彩色阴影的分布，为模型训练提供了有力的支持。在三个公开数据集上的实验验证了所提方法相较于最新技术的优越性。我们的代码和数据集将公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01422v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种基于潜在空间的扩散模型（DocShaDiffusion），用于文档图像阴影去除。通过像素空间到潜在空间的转换，模型能更轻松地捕捉关键特征。为解决彩色阴影问题，设计了阴影软掩膜生成模块（SSGM），能准确生成阴影掩膜并对阴影区域添加特定噪声。在监督扩散和去噪过程中，提出了阴影掩膜感知引导扩散模块（SMGDM）。同时，提出了一种阴影鲁棒感知特征损失，以保留文档图像的细节和结构。此外，开发了一个大型合成文档彩色阴影去除数据集（SDCSRD），模拟真实彩色阴影的分布，为模型训练提供有力支持。实验证明，该方法在三个公共数据集上表现优于现有技术。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>提出了基于潜在空间的扩散模型（DocShaDiffusion）进行文档图像阴影去除。</li>
<li>通过像素空间到潜在空间的转换，使模型更容易捕捉关键特征。</li>
<li>设计了阴影软掩膜生成模块（SSGM），以处理彩色阴影问题。</li>
<li>提出了阴影掩膜感知引导扩散模块（SMGDM），用于监督去除文档图像的阴影。</li>
<li>引入了一种阴影鲁棒感知特征损失，以保留文档图像的细节和结构。</li>
<li>开发了一个合成文档彩色阴影去除数据集（SDCSRD），模拟真实数据分布。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01422">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-594493b390b27ef59c3ead738d823257.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28c2626c4811173bed143809ee3ef8be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c835280ab45e764f433df60338c7882e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ce0bf599f729ce27759772d22267b44.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-018e3460bb1cfe57c06c32cd26656cc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c362a795c3738046415578e6eee9ffc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="BronchoGAN-Anatomically-consistent-and-domain-agnostic-image-to-image-translation-for-video-bronchoscopy"><a href="#BronchoGAN-Anatomically-consistent-and-domain-agnostic-image-to-image-translation-for-video-bronchoscopy" class="headerlink" title="BronchoGAN: Anatomically consistent and domain-agnostic image-to-image   translation for video bronchoscopy"></a>BronchoGAN: Anatomically consistent and domain-agnostic image-to-image   translation for video bronchoscopy</h2><p><strong>Authors:Ahmad Soliman, Ron Keuth, Marian Himstedt</strong></p>
<p>The limited availability of bronchoscopy images makes image synthesis particularly interesting for training deep learning models. Robust image translation across different domains – virtual bronchoscopy, phantom as well as in-vivo and ex-vivo image data – is pivotal for clinical applications. This paper proposes BronchoGAN introducing anatomical constraints for image-to-image translation being integrated into a conditional GAN. In particular, we force bronchial orifices to match across input and output images. We further propose to use foundation model-generated depth images as intermediate representation ensuring robustness across a variety of input domains establishing models with substantially less reliance on individual training datasets. Moreover our intermediate depth image representation allows to easily construct paired image data for training. Our experiments showed that input images from different domains (e.g. virtual bronchoscopy, phantoms) can be successfully translated to images mimicking realistic human airway appearance. We demonstrated that anatomical settings (i.e. bronchial orifices) can be robustly preserved with our approach which is shown qualitatively and quantitatively by means of improved FID, SSIM and dice coefficients scores. Our anatomical constraints enabled an improvement in the Dice coefficient of up to 0.43 for synthetic images. Through foundation models for intermediate depth representations, bronchial orifice segmentation integrated as anatomical constraints into conditional GANs we are able to robustly translate images from different bronchoscopy input domains. BronchoGAN allows to incorporate public CT scan data (virtual bronchoscopy) in order to generate large-scale bronchoscopy image datasets with realistic appearance. BronchoGAN enables to bridge the gap of missing public bronchoscopy images. </p>
<blockquote>
<p>由于支气管镜检查图像的可获取性有限，因此图像合成对于训练深度学习模型来说特别有趣。不同领域（如虚拟支气管镜、幻影以及体内和体外图像数据）之间的稳健图像翻译对临床应用至关重要。本文提出了BronchoGAN，引入了图像到图像翻译的解剖学约束，并将其集成到条件生成对抗网络（GAN）中。特别是，我们强制要求支气管开口在输入和输出图像之间匹配。我们进一步建议使用基础模型生成的深度图像作为中间表示，以确保在各种输入领域的稳健性，并建立对个别训练数据集依赖更少的模型。此外，我们的中间深度图像表示可以轻松地构建用于训练的一对图像数据。我们的实验表明，来自不同领域的输入图像（例如虚拟支气管镜、幻影）可以成功地翻译成模拟真实人类气道外观的图像。我们证明，通过我们的方法，可以稳健地保留解剖结构设置（即支气管开口），这通过改进的FID、SSIM和Dice系数得分进行定性和定量证明。我们的解剖约束使合成图像的Dice系数提高了高达0.43。通过用于中间深度表示的基础模型，将支气管开口分割作为解剖学约束集成到条件GAN中，我们能够稳健地翻译来自不同支气管镜检查输入领域的图像。BronchoGAN允许整合公共CT扫描数据（虚拟支气管镜），以生成具有逼真外观的大规模支气管镜检查图像数据集。BronchoGAN能够弥补公共支气管镜检查图像缺失的空白。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01387v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了BronchoGAN在支气管镜图像合成方面的应用。该模型通过引入解剖约束，实现了不同领域图像之间的稳健转换，如虚拟支气管镜、幻影以及体内和离体图像数据。使用基础模型生成的深度图像作为中间表示，提高了模型对不同输入领域的稳健性，并降低了对单个训练数据集的依赖。实验表明，不同领域的输入图像可以成功翻译为适应现实人类气道外观的图像。通过引入解剖约束和基于基础模型的中间深度表示，BronchoGAN能够稳健地保存解剖设置，并产生高质量的图像数据。此外，BronchoGAN还允许利用公共CT扫描数据生成大规模支气管镜图像数据集，从而弥补了公共支气管镜图像数据的缺乏。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BronchoGAN用于支气管镜图像合成，解决了图像数据有限的问题。</li>
<li>该模型实现了不同领域图像之间的稳健转换，如虚拟支气管镜、幻影以及体内和离体图像。</li>
<li>引入解剖约束，强制输入和输出图像的支气管开口匹配。</li>
<li>使用基础模型生成的深度图像作为中间表示，提高模型稳健性并降低对个别训练数据集的依赖。</li>
<li>实验证明，该模型能成功翻译不同领域的输入图像为逼真的支气管镜图像。</li>
<li>BronchoGAN能够定量和定性地保存解剖设置，并通过改进FID、SSIM和Dice系数等指标体现其效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01387">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-14e9d1f1ee5abfccc9ff87811e3adf2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8dcaafe778d3b6f80c2e5ae3a6bad9ed.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ShapeEmbed-a-self-supervised-learning-framework-for-2D-contour-quantification"><a href="#ShapeEmbed-a-self-supervised-learning-framework-for-2D-contour-quantification" class="headerlink" title="ShapeEmbed: a self-supervised learning framework for 2D contour   quantification"></a>ShapeEmbed: a self-supervised learning framework for 2D contour   quantification</h2><p><strong>Authors:Anna Foix Romero, Craig Russell, Alexander Krull, Virginie Uhlmann</strong></p>
<p>The shape of objects is an important source of visual information in a wide range of applications. One of the core challenges of shape quantification is to ensure that the extracted measurements remain invariant to transformations that preserve an object’s intrinsic geometry, such as changing its size, orientation, and position in the image. In this work, we introduce ShapeEmbed, a self-supervised representation learning framework designed to encode the contour of objects in 2D images, represented as a Euclidean distance matrix, into a shape descriptor that is invariant to translation, scaling, rotation, reflection, and point indexing. Our approach overcomes the limitations of traditional shape descriptors while improving upon existing state-of-the-art autoencoder-based approaches. We demonstrate that the descriptors learned by our framework outperform their competitors in shape classification tasks on natural and biological images. We envision our approach to be of particular relevance to biological imaging applications. </p>
<blockquote>
<p>物体的形状是众多应用领域中视觉信息的重要来源之一。形状量化的核心挑战之一是确保提取的测量值在面对保持物体内在几何特性的变换（如改变其大小、方向和图像中的位置）时保持不变。在这项工作中，我们引入了ShapeEmbed，这是一个自监督表示学习框架，旨在将二维图像中的物体轮廓编码为欧几里得距离矩阵，进而得到一个对平移、缩放、旋转、反射和点索引不变的形状描述符。我们的方法克服了传统形状描述符的局限性，同时改进了现有的基于自动编码器的最先进的方法。我们证明，我们的框架所学的描述符在自然图像和生物图像的形状分类任务中优于竞争对手。我们设想我们的方法对生物成像应用具有特别重要的意义。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01009v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>形状信息是视觉应用中的关键信息来源。本文介绍了ShapeEmbed，一种自监督表示学习框架，可将二维图像中的物体轮廓编码为形状描述符，该描述符具有对平移、缩放、旋转、反射和点索引的不变性。ShapeEmbed框架在形状分类任务上的表现优于传统和基于自动编码器的先进方法，特别是在自然和生物图像方面。该框架特别适用于生物成像应用。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>形状信息是视觉应用中的重要信息来源。</li>
<li>ShapeEmbed是一种自监督表示学习框架，用于编码物体轮廓为形状描述符。</li>
<li>该描述符具有对多种变换的不变性，包括平移、缩放、旋转、反射和点索引。</li>
<li>ShapeEmbed在形状分类任务上的表现优于传统方法和现有的先进自动编码器方法。</li>
<li>ShapeEmbed框架特别适用于自然和生物图像的形状分类任务。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01009">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-97fa0bbfd5c8d24d0ca46e3efc9e25a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c437cabb73d90dd7d999a55b34af009f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-723af7aec3f6bcf40bc87b9b01db5d6a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ADAptation-Reconstruction-based-Unsupervised-Active-Learning-for-Breast-Ultrasound-Diagnosis"><a href="#ADAptation-Reconstruction-based-Unsupervised-Active-Learning-for-Breast-Ultrasound-Diagnosis" class="headerlink" title="ADAptation: Reconstruction-based Unsupervised Active Learning for Breast   Ultrasound Diagnosis"></a>ADAptation: Reconstruction-based Unsupervised Active Learning for Breast   Ultrasound Diagnosis</h2><p><strong>Authors:Yaofei Duan, Yuhao Huang, Xin Yang, Luyi Han, Xinyu Xie, Zhiyuan Zhu, Ping He, Ka-Hou Chan, Ligang Cui, Sio-Kei Im, Dong Ni, Tao Tan</strong></p>
<p>Deep learning-based diagnostic models often suffer performance drops due to distribution shifts between training (source) and test (target) domains. Collecting and labeling sufficient target domain data for model retraining represents an optimal solution, yet is limited by time and scarce resources. Active learning (AL) offers an efficient approach to reduce annotation costs while maintaining performance, but struggles to handle the challenge posed by distribution variations across different datasets. In this study, we propose a novel unsupervised Active learning framework for Domain Adaptation, named ADAptation, which efficiently selects informative samples from multi-domain data pools under limited annotation budget. As a fundamental step, our method first utilizes the distribution homogenization capabilities of diffusion models to bridge cross-dataset gaps by translating target images into source-domain style. We then introduce two key innovations: (a) a hypersphere-constrained contrastive learning network for compact feature clustering, and (b) a dual-scoring mechanism that quantifies and balances sample uncertainty and representativeness. Extensive experiments on four breast ultrasound datasets (three public and one in-house&#x2F;multi-center) across five common deep classifiers demonstrate that our method surpasses existing strong AL-based competitors, validating its effectiveness and generalization for clinical domain adaptation. The code is available at the anonymized link: <a target="_blank" rel="noopener" href="https://github.com/miccai25-966/ADAptation">https://github.com/miccai25-966/ADAptation</a>. </p>
<blockquote>
<p>基于深度学习的诊断模型由于训练（源）和测试（目标）域之间的分布变化，常常会出现性能下降的情况。为模型重新训练收集并标注足够的目标域数据是一种最佳解决方案，然而这受到时间和资源的限制。主动学习（AL）提供了一种在保持性能的同时降低标注成本的有效方法，但难以应对不同数据集分布变化带来的挑战。在这项研究中，我们提出了一种用于域自适应的新型无监督主动学习框架，名为ADAptation，它能够在有限的标注预算下，从多域数据池中有效地选择信息样本。作为基本步骤，我们的方法首先利用扩散模型的分布均化能力，通过将目标图像翻译成源域风格来弥合跨数据集的差距。然后，我们介绍了两项关键创新：（a）用于紧凑特征聚类的超球约束对比学习网络，（b）一种量化并平衡样本不确定性和代表性的双评分机制。在四个乳房超声数据集（三个公开和一个内部&#x2F;多中心）上进行的广泛实验表明，我们的方法在五种常见的深度分类器上的表现超过了现有的强大基于AL的竞争对手，验证了其在临床域适应中的有效性和泛化能力。代码可在匿名链接中找到：<a target="_blank" rel="noopener" href="https://github.com/miccai25-966/ADAptation%E3%80%82">https://github.com/miccai25-966/ADAptation。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00474v1">PDF</a> 11 pages, 4 figures, 4 tables. Accepted by conference MICCAI2025</p>
<p><strong>Summary</strong><br>深度学习模型在诊断任务中常因训练（源）域与测试（目标）域之间的分布变化而导致性能下降。为应对此问题，收集并标注足够的目标域数据以供模型重新训练是最优解决方案，但受限于时间和资源。主动学习（AL）旨在降低标注成本同时保持性能，但难以处理不同数据集间分布变化带来的挑战。本研究提出一种名为ADAptation的新型无监督主动学习框架，该框架能在有限的标注预算下，从多域数据池中有效选择信息样本。首先，利用扩散模型的分布均化能力，通过翻译目标图像至源域风格来缩小跨数据集差距。接着引入两项创新：超球约束对比学习网络进行紧凑特征聚类，以及量化并平衡样本不确定性和代表性的双评分机制。在四个乳房超声波数据集上的实验表明，相较于其他强大的AL竞争对手，本方法表现更优，验证了其在临床域适应中的有效性和通用性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>深度学习模型在诊断任务中面临源域与目标域分布变化导致的性能下降问题。</li>
<li>收集并标注目标域数据是应对此问题的最优解决方案，但受限于时间和资源。</li>
<li>主动学习（AL）虽可降低标注成本，但难以处理不同数据集间的分布变化。</li>
<li>本研究提出名为ADAptation的新型无监督主动学习框架，能从多域数据池中有效选择信息样本。</li>
<li>利用扩散模型的分布均化能力缩小跨数据集差距。</li>
<li>引入超球约束对比学习网络和双评分机制，分别用于紧凑特征聚量和样本不确定性与代表性的量化平衡。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00474">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f0183fdae419a28b5c2150daf99bd874.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d538d9879de728bf67d71ef567184c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cdf499a44bb68c011dfaafa802e11c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e3f349f41565e18efbd340efde6c538.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SURE-VQA-Systematic-Understanding-of-Robustness-Evaluation-in-Medical-VQA-Tasks"><a href="#SURE-VQA-Systematic-Understanding-of-Robustness-Evaluation-in-Medical-VQA-Tasks" class="headerlink" title="SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical   VQA Tasks"></a>SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical   VQA Tasks</h2><p><strong>Authors:Kim-Celine Kahl, Selen Erkan, Jeremias Traub, Carsten T. Lüth, Klaus Maier-Hein, Lena Maier-Hein, Paul F. Jaeger</strong></p>
<p>Vision-Language Models (VLMs) have great potential in medical tasks, like Visual Question Answering (VQA), where they could act as interactive assistants for both patients and clinicians. Yet their robustness to distribution shifts on unseen data remains a key concern for safe deployment. Evaluating such robustness requires a controlled experimental setup that allows for systematic insights into the model’s behavior. However, we demonstrate that current setups fail to offer sufficiently thorough evaluations. To address this gap, we introduce a novel framework, called \textit{SURE-VQA}, centered around three key requirements to overcome current pitfalls and systematically analyze VLM robustness: 1) Since robustness on synthetic shifts does not necessarily translate to real-world shifts, it should be measured on real-world shifts that are inherent to the VQA data; 2) Traditional token-matching metrics often fail to capture underlying semantics, necessitating the use of large language models (LLMs) for more accurate semantic evaluation; 3) Model performance often lacks interpretability due to missing sanity baselines, thus meaningful baselines should be reported that allow assessing the multimodal impact on the VLM. To demonstrate the relevance of this framework, we conduct a study on the robustness of various Fine-Tuning (FT) methods across three medical datasets with four types of distribution shifts. Our study highlights key insights into robustness: 1) No FT method consistently outperforms others in robustness, and 2) robustness trends are more stable across FT methods than across distribution shifts. Additionally, we find that simple sanity baselines that do not use the image data can perform surprisingly well and confirm LoRA as the best-performing FT method on in-distribution data. Code is provided at <a target="_blank" rel="noopener" href="https://github.com/IML-DKFZ/sure-vqa">https://github.com/IML-DKFZ/sure-vqa</a>. </p>
<blockquote>
<p>视觉语言模型（VLMs）在医疗任务（如视觉问答（VQA））中具有巨大潜力，可以作为患者和临床医生双方的交互式助手。然而，它们在未见数据上的分布转移稳健性仍是安全部署的关键问题。评估这种稳健性需要一个受控的实验设置，允许对模型的行为进行系统性的洞察。然而，我们证明当前的设置无法提供足够全面的评估。为了解决这一差距，我们引入了一个名为“SURE-VQA”的新型框架，该框架以三个关键要求为中心，旨在克服当前漏洞并系统地分析VLM的稳健性：1）由于合成转移上的稳健性并不一定等同于现实世界中的转移，因此它应在VQA数据固有的现实世界转移中进行衡量；2）传统的令牌匹配指标通常无法捕获潜在语义，因此需要利用大型语言模型（LLM）进行更准确的语义评估；3）由于缺少基准值，模型性能往往缺乏可解释性，因此应报告有意义的基准值，以便评估对VLM的多模式影响。为了证明该框架的重要性，我们在三个医学数据集和四种分布转移类型上对各种微调（FT）方法的稳健性进行了研究。我们的研究对稳健性有了关键见解：1）没有FT方法能在稳健性方面始终优于其他方法；2）与FT方法相比，稳健性趋势在分布转移之间更为稳定。此外，我们发现不使用图像数据的简单基准线可以表现惊人，并确认LoRA是在内部分布数据上表现最佳的FT方法。代码可在<a target="_blank" rel="noopener" href="https://github.com/IML-DKFZ/sure-vqa%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/IML-DKFZ/sure-vqa找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19688v2">PDF</a> TMLR 07&#x2F;2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了视觉语言模型（VLMs）在医疗任务中的潜力与挑战。针对现有评估框架在评估模型鲁棒性方面的不足，提出了一种新的评估框架“SURE-VQA”，并围绕三个关键要求进行系统分析：1）在真实世界的数据分布变化上进行鲁棒性评估；2）使用大型语言模型（LLMs）进行更准确的语义评估；3) 报告有意义的基准以评估多模式对VLM的影响。通过对不同微调方法的研究，展示了该框架的实用性，并得出关键洞察。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMs在医疗任务如视觉问答（VQA）中有巨大潜力，可作为患者和临床医生的互动助手。</li>
<li>现有评估框架在评估模型鲁棒性方面存在不足，需要新的评估框架。</li>
<li>SURE-VQA框架旨在解决现有问题，围绕三个关键要求进行系统分析：真实世界数据分布变化的评估、使用LLMs进行语义评估、报告有意义的基准以评估多模式影响。</li>
<li>研究发现，不同微调方法在不同的医疗数据集上的鲁棒性表现不一致，没有一种方法始终优于其他方法。</li>
<li>模型鲁棒性的趋势在不同微调方法之间比在不同数据分布变化之间更稳定。</li>
<li>简单的基准测试，即不使用图像数据的测试，可能会表现得相当好。</li>
<li>LoRA是在内部分布数据上表现最佳的的微调方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19688">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-b000079ea6462b3c4d7ca381a7a28111.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e948fe18de177ef9fbc60fe3fb06c684.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2efc05dd5c40006efa36cc2cf970b9dc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Average-Calibration-Error-A-Differentiable-Loss-for-Improved-Reliability-in-Image-Segmentation"><a href="#Average-Calibration-Error-A-Differentiable-Loss-for-Improved-Reliability-in-Image-Segmentation" class="headerlink" title="Average Calibration Error: A Differentiable Loss for Improved   Reliability in Image Segmentation"></a>Average Calibration Error: A Differentiable Loss for Improved   Reliability in Image Segmentation</h2><p><strong>Authors:Theodore Barfoot, Luis Garcia-Peraza-Herrera, Ben Glocker, Tom Vercauteren</strong></p>
<p>Deep neural networks for medical image segmentation often produce overconfident results misaligned with empirical observations. Such miscalibration, challenges their clinical translation. We propose to use marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss function to improve pixel-wise calibration without compromising segmentation quality. We show that this loss, despite using hard binning, is directly differentiable, bypassing the need for approximate but differentiable surrogate or soft binning approaches. Our work also introduces the concept of dataset reliability histograms which generalises standard reliability diagrams for refined visual assessment of calibration in semantic segmentation aggregated at the dataset level. Using mL1-ACE, we reduce average and maximum calibration error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS 2021 dataset. We share our code here: <a target="_blank" rel="noopener" href="https://github.com/cai4cai/ACE-DLIRIS">https://github.com/cai4cai/ACE-DLIRIS</a> </p>
<blockquote>
<p>针对医学图像分割的深度神经网络经常产生过于自信的结果，与经验观察不符。这种误校准对其临床翻译提出了挑战。我们建议使用边际L1平均校准误差（mL1-ACE）作为一种新型辅助损失函数，以提高像素级的校准，同时不损害分割质量。我们证明，尽管使用了硬分箱，这种损失是可直接微分的，从而避免了需要使用近似但可微分的替代或软分箱方法。我们的工作还引入了数据集可靠性直方图的概念，它推广了标准可靠性图，用于在数据集级别对语义分割的校准进行精细的视觉评估。使用mL1-ACE，我们在BraTS 2021数据集上将平均和最大校准误差分别降低了45%和55%，同时保持Dice得分为87%。我们在以下链接分享我们的代码：<a target="_blank" rel="noopener" href="https://github.com/cai4cai/ACE-DLIRIS">https://github.com/cai4cai/ACE-DLIRIS</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.06759v2">PDF</a> </p>
<p><strong>Summary</strong><br>神经网络在医学图像分割中常产生过于自信的结果，与实际观测不符。提出使用边际L1平均校准误差（mL1-ACE）作为新的辅助损失函数，提高像素级校准，不影响分割质量。引入数据集可靠性直方图，用于精细的视觉评估。使用mL1-ACE在BraTS 2021数据集上平均和最大校准误差分别降低45%和55%，同时Dice分数保持87%。代码共享于<a target="_blank" rel="noopener" href="https://github.com/cai4cai/ACE-DLIRIS">链接</a>。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>神经网络在医学图像分割中存在过度自信的问题。</li>
<li>边际L1平均校准误差（mL1-ACE）作为新的辅助损失函数，可提高像素级校准。</li>
<li>引入数据集可靠性直方图进行精细的视觉评估。</li>
<li>使用mL1-ACE在BraTS 2021数据集上显著提高校准效果，同时维持较高的分割质量。</li>
<li>方法可直接应用于现有模型，无需额外的复杂操作。</li>
<li>代码已共享，便于其他研究者使用与验证。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.06759">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-e8263ab8c92ec21703b6965c65f954cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9d6ad7c0dd2036cb14bf4af38aa6af2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b92c85046bac61be54c371272062d92f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9427cce65b7878cf6ea8e534a2d40b53.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae2a53d0e4b1c20d126622245d3b15e1.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-07-04  evMLP An Efficient Event-Driven MLP Architecture for Vision
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-fbb97edd4b79d0ce4e7b21f6fe76f658.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot 方向最新论文已更新，请持续关注 Update in 2025-07-04  Activation Reward Models for Few-Shot Model Alignment
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">26254.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
