<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="I2I Translation">
    <meta name="description" content="I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-04  How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>I2I Translation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-b92c85046bac61be54c371272062d92f.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">I2I Translation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/I2I-Translation/">
                                <span class="chip bg-color">I2I Translation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                I2I Translation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    8.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    35 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-04-æ›´æ–°"><a href="#2025-07-04-æ›´æ–°" class="headerlink" title="2025-07-04 æ›´æ–°"></a>2025-07-04 æ›´æ–°</h1><h2 id="How-Well-Does-GPT-4o-Understand-Vision-Evaluating-Multimodal-Foundation-Models-on-Standard-Computer-Vision-Tasks"><a href="#How-Well-Does-GPT-4o-Understand-Vision-Evaluating-Multimodal-Foundation-Models-on-Standard-Computer-Vision-Tasks" class="headerlink" title="How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks"></a>How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks</h2><p><strong>Authors:Rahul Ramachandran, Ali Garjani, Roman Bachmann, Andrei Atanov, OÄŸuzhan Fatih Kar, Amir Zamir</strong></p>
<p>Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets (e.g., COCO, ImageNet and its variants, etc).   The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework.   We observe that 1) the models are not close to the state-of-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a preliminary analysis of models with native image generation, like the latest GPT-4o, shows they exhibit quirks like hallucinations and spatial misalignments. </p>
<blockquote>
<p>å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œå¦‚GPT-4oï¼Œæœ€è¿‘å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨è§†è§‰ç†è§£æ–¹é¢ï¼Œè¿™äº›æ¨¡å‹çš„å…·ä½“è¿›å±•å°šä¸æ¸…æ¥šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨æ ‡å‡†è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆè¯­ä¹‰åˆ†å‰²ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†ç±»ã€æ·±åº¦å’Œè¡¨é¢æ³•çº¿é¢„æµ‹ï¼‰ä¸Šï¼Œä½¿ç”¨æ—¢å®šæ•°æ®é›†ï¼ˆä¾‹å¦‚COCOã€ImageNetåŠå…¶å˜ä½“ç­‰ï¼‰å¯¹æµè¡Œå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆGPT-4oã€o4-miniã€Gemini 1.5 Proå’ŒGemini 2.0 Flashã€Claude 3.5 Sonnetã€Qwen2-VLã€Llama 3.2ï¼‰çš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ã€‚æ‰§è¡Œæ­¤ä»»åŠ¡çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºï¼š1ï¼‰å¤§å¤šæ•°æ¨¡å‹è¢«è®­ç»ƒç”¨äºè¾“å‡ºæ–‡æœ¬ï¼Œæ— æ³•åŸç”Ÿè¡¨è¾¾è¯¸å¦‚ç‰‡æ®µæˆ–3Då‡ ä½•ç­‰å¤šæ ·é¢†åŸŸï¼›2ï¼‰è®¸å¤šé¢†å…ˆçš„æ¨¡å‹æ˜¯ä¸“æœ‰æ¨¡å‹ï¼Œåªèƒ½é€šè¿‡APIçº§åˆ«è®¿é—®ï¼Œå³æ— æ³•è·å–å…¶æƒé‡ä»¥è¿›è¡Œé€‚åº”ã€‚æˆ‘ä»¬é€šè¿‡é€šè¿‡æç¤ºé“¾å°†æ ‡å‡†è§†è§‰ä»»åŠ¡è½¬æ¢ä¸ºç­‰æ•ˆçš„æ–‡æœ¬æç¤ºå’ŒAPIå…¼å®¹ä»»åŠ¡ï¼Œåˆ›å»ºä¸€ä¸ªæ ‡å‡†åŒ–çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‘ç°ï¼š1ï¼‰è¿™äº›æ¨¡å‹åœ¨ä»»ä½•ä»»åŠ¡ä¸Šéƒ½ä¸æ¥è¿‘æœ€æ–°ä¸“ä¸šæ¨¡å‹ï¼›ç„¶è€Œï¼Œ2ï¼‰å®ƒä»¬æ˜¯ç›¸å½“ä¸é”™çš„é€šç”¨æ¨¡å‹ï¼›è¿™ç›¸å½“å‡ºè‰²ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½ä¸»è¦æ˜¯åœ¨åŸºäºå›¾åƒæ–‡æœ¬çš„ä»»åŠ¡ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚3ï¼‰å®ƒä»¬æ‰§è¡Œè¯­ä¹‰ä»»åŠ¡æ˜æ˜¾æ¯”å‡ ä½•ä»»åŠ¡æ›´å¥½ã€‚4ï¼‰è™½ç„¶æç¤ºé“¾æŠ€æœ¯ä¼šå½±å“æ€§èƒ½ï¼Œä½†æ›´å¥½çš„æ¨¡å‹å¯¹æç¤ºå˜åŒ–çš„æ•æ„Ÿæ€§è¾ƒä½ã€‚5ï¼‰GPT-4oåœ¨éæ¨ç†æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œåœ¨6ä¸ªä»»åŠ¡ä¸­çš„4ä¸ªä»»åŠ¡ä¸­å æ®é¦–ä½ã€‚6ï¼‰æ¨ç†æ¨¡å‹ï¼ˆä¾‹å¦‚o3ï¼‰åœ¨å‡ ä½•ä»»åŠ¡ä¸Šæ˜¾ç¤ºå‡ºæ”¹è¿›ï¼Œ7ï¼‰å¯¹å…·æœ‰åŸç”Ÿå›¾åƒç”Ÿæˆçš„æ¨¡å‹ï¼ˆå¦‚æœ€æ–°çš„GPT-4oï¼‰è¿›è¡Œåˆæ­¥åˆ†æè¡¨æ˜ï¼Œå®ƒä»¬å…·æœ‰è¯¸å¦‚å¹»è§‰å’Œç©ºé—´ä¸å¯¹å‡†ç­‰ç‰¹æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01955v1">PDF</a> Project page at <a target="_blank" rel="noopener" href="https://fm-vision-evals.epfl.ch/">https://fm-vision-evals.epfl.ch/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å¯¹æ¯”äº†å¤šæ¬¾æµè¡Œçš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆå¦‚GPT-4oã€o4-miniã€Geminiç³»åˆ—ç­‰ï¼‰åœ¨æ ‡å‡†è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ä¸»è¦æŒ‘æˆ˜åœ¨äºè¿™äº›æ¨¡å‹ä¸»è¦è®­ç»ƒè¾“å‡ºæ–‡æœ¬ï¼Œéš¾ä»¥ç›´æ¥è¡¨è¾¾å¤šæ ·é¢†åŸŸå¦‚åˆ†æ®µæˆ–ä¸‰ç»´å‡ ä½•ã€‚é€šè¿‡æç¤ºé“¾æŠ€æœ¯å°†æ ‡å‡†è§†è§‰ä»»åŠ¡è½¬åŒ–ä¸ºç­‰æ•ˆçš„æ–‡æœ¬æç¤ºå’ŒAPIå…¼å®¹ä»»åŠ¡ï¼Œå»ºç«‹æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶ã€‚è§‚å¯Ÿå‘ç°è¿™äº›æ¨¡å‹è™½ä¸åŠä¸“ä¸šæ¨¡å‹ï¼Œä½†ä½œä¸ºå…¨èƒ½æ¨¡å‹è¡¨ç°ä»å€¼å¾—å°Šæ•¬ï¼Œè¯­ä¹‰ä»»åŠ¡è¡¨ç°ä¼˜äºå‡ ä½•ä»»åŠ¡ã€‚ä¸åŒæ¨¡å‹çš„æç¤ºé“¾æŠ€æœ¯å½±å“æ€§èƒ½ï¼Œä½†æ›´å¥½çš„æ¨¡å‹å¯¹æç¤ºå˜åŒ–æ›´ä¸æ•æ„Ÿã€‚GPT-4oåœ¨éæ¨ç†æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œåœ¨å››é¡¹ä»»åŠ¡ä¸­æ’åç¬¬ä¸€ã€‚å…·å¤‡åŸç”Ÿå›¾åƒç”Ÿæˆçš„æ¨¡å‹å¦‚æœ€æ–°GPT-4oå±•ç°å‡ºä¸€äº›å¥‡ç‰¹ç°è±¡å¦‚å¹»è§†å’Œç©ºé—´é”™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸Šçš„è¡¨ç°è¢«è¯„ä¼°ï¼Œä¸»è¦æŒ‘æˆ˜åœ¨äºæ¨¡å‹çš„æ–‡æœ¬è¾“å‡ºç‰¹æ€§ä¸å¯¹å¤šæ ·é¢†åŸŸçš„è¡¨è¾¾ã€‚</li>
<li>é€šè¿‡æç¤ºé“¾æŠ€æœ¯å°†æ ‡å‡†è§†è§‰ä»»åŠ¡è½¬åŒ–ä¸ºæ–‡æœ¬æç¤ºå’ŒAPIå…¼å®¹ä»»åŠ¡ï¼Œå»ºç«‹è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>è¿™äº›æ¨¡å‹è™½ä¸åŠä¸“ä¸šæ¨¡å‹ï¼Œä½†ä½œä¸ºå…¨èƒ½æ¨¡å‹è¡¨ç°ä»å€¼å¾—å°Šæ•¬ã€‚</li>
<li>è¯­ä¹‰ä»»åŠ¡è¡¨ç°ä¼˜äºå‡ ä½•ä»»åŠ¡ã€‚</li>
<li>æç¤ºé“¾æŠ€æœ¯å¯¹æ¨¡å‹æ€§èƒ½æœ‰å½±å“ï¼Œä½†æ›´å¥½çš„æ¨¡å‹å¯¹æç¤ºå˜åŒ–æ›´ç¨³å®šã€‚</li>
<li>GPT-4oåœ¨éæ¨ç†æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œåœ¨å››é¡¹ä»»åŠ¡ä¸­æ’åç¬¬ä¸€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-25866a56a7bb23110ef12bfd70e68e3b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d8987b678a6043fb17c5c89c3628d8b8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8665bb9a66c6900ff01051abba32ea93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86a19d55133674d93a00588735b57752.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92aaa3e7aa83ff825c7a37d03c198977.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SafePTR-Token-Level-Jailbreak-Defense-in-Multimodal-LLMs-via-Prune-then-Restore-Mechanism"><a href="#SafePTR-Token-Level-Jailbreak-Defense-in-Multimodal-LLMs-via-Prune-then-Restore-Mechanism" class="headerlink" title="SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via   Prune-then-Restore Mechanism"></a>SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via   Prune-then-Restore Mechanism</h2><p><strong>Authors:Beitao Chen, Xinyu Lyu, Lianli Gao, Jingkuan Song, Heng Tao Shen</strong></p>
<p>By incorporating visual inputs, Multimodal Large Language Models (MLLMs) extend LLMs to support visual reasoning. However, this integration also introduces new vulnerabilities, making MLLMs susceptible to multimodal jailbreak attacks and hindering their safe deployment.Existing defense methods, including Image-to-Text Translation, Safe Prompting, and Multimodal Safety Tuning, attempt to address this by aligning multimodal inputs with LLMsâ€™ built-in safeguards.Yet, they fall short in uncovering root causes of multimodal vulnerabilities, particularly how harmful multimodal tokens trigger jailbreak in MLLMs? Consequently, they remain vulnerable to text-driven multimodal jailbreaks, often exhibiting overdefensive behaviors and imposing heavy training overhead.To bridge this gap, we present an comprehensive analysis of where, how and which harmful multimodal tokens bypass safeguards in MLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers are responsible for inducing unsafe behaviors, highlighting the potential of precisely removing a small subset of harmful tokens, without requiring safety tuning, can still effectively improve safety against jailbreaks. Motivated by this, we propose Safe Prune-then-Restore (SafePTR), an training-free defense framework that selectively prunes harmful tokens at vulnerable layers while restoring benign features at subsequent layers.Without incurring additional computational overhead, SafePTR significantly enhances the safety of MLLMs while preserving efficiency. Extensive evaluations across three MLLMs and five benchmarks demonstrate SafePTRâ€™s state-of-the-art performance in mitigating jailbreak risks without compromising utility. </p>
<blockquote>
<p>é€šè¿‡èå…¥è§†è§‰è¾“å…¥ï¼Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å°†è¯­è¨€æ¨¡å‹æ‰©å±•åˆ°æ”¯æŒè§†è§‰æ¨ç†ã€‚ç„¶è€Œï¼Œè¿™ç§èåˆä¹Ÿå¼•å…¥äº†æ–°çš„æ¼æ´ï¼Œä½¿å¾—MLLMså®¹æ˜“å—åˆ°å¤šæ¨¡æ€è¶Šç‹±æ”»å‡»çš„å½±å“ï¼Œå¹¶é˜»ç¢äº†å…¶å®‰å…¨éƒ¨ç½²ã€‚ç°æœ‰çš„é˜²å¾¡æ–¹æ³•ï¼ŒåŒ…æ‹¬å›¾åƒåˆ°æ–‡æœ¬çš„ç¿»è¯‘ã€å®‰å…¨æç¤ºå’Œå¤šæ¨¡æ€å®‰å…¨è°ƒæ•´ï¼Œè¯•å›¾é€šè¿‡å¯¹é½å¤šæ¨¡æ€è¾“å…¥ä¸è¯­è¨€æ¨¡å‹çš„å†…ç½®å®‰å…¨ä¿æŠ¤æªæ–½æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œå®ƒä»¬æœªèƒ½å‘ç°å¤šæ¨¡æ€æ¼æ´çš„æ ¹æœ¬åŸå› ï¼Œå°¤å…¶æ˜¯æœ‰å®³çš„å¤šæ¨¡æ€ä»¤ç‰Œæ˜¯å¦‚ä½•è§¦å‘MLLMsä¸­çš„è¶Šç‹±çš„ã€‚å› æ­¤ï¼Œå®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°æ–‡æœ¬é©±åŠ¨çš„å¤šæ¨¡æ€è¶Šç‹±æ”»å‡»ï¼Œé€šå¸¸è¡¨ç°å‡ºè¿‡åº¦é˜²å¾¡çš„è¡Œä¸ºå¹¶äº§ç”Ÿæ²‰é‡çš„è®­ç»ƒè´Ÿæ‹…ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¯¹MLLMsä¸­æœ‰å®³çš„å¤šæ¨¡æ€ä»¤ç‰Œå¦‚ä½•ç»•è¿‡ä¿æŠ¤è¿›è¡Œäº†ç»¼åˆåˆ†æã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°æ—©æœŸä¸­é—´å±‚ä¸­ä¸åˆ°1%çš„ä»¤ç‰Œè´Ÿè´£å¼•å‘ä¸å®‰å…¨è¡Œä¸ºï¼Œè¿™çªå‡ºäº†ç²¾ç¡®ç§»é™¤ä¸€å°éƒ¨åˆ†æœ‰å®³ä»¤ç‰Œçš„å¯èƒ½æ€§ï¼Œè€Œæ— éœ€è¿›è¡Œå®‰å…¨è°ƒæ•´ï¼Œä»ç„¶å¯ä»¥æœ‰æ•ˆåœ°æé«˜å¯¹æŠ—è¶Šç‹±çš„å®‰å…¨æ€§ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€è®­ç»ƒçš„é˜²å¾¡æ¡†æ¶Safe Prune-then-Restoreï¼ˆSafePTRï¼‰ï¼Œå®ƒé€‰æ‹©æ€§åœ°åœ¨è„†å¼±å±‚ä¸­åˆ é™¤æœ‰å®³ä»¤ç‰Œï¼ŒåŒæ—¶åœ¨åç»­å±‚ä¸­æ¢å¤è‰¯æ€§ç‰¹å¾ã€‚SafePTRæ— éœ€é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œå°±èƒ½æ˜¾è‘—æé«˜MLLMsçš„å®‰å…¨æ€§ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆæ€§ã€‚åœ¨ä¸‰ä¸ªMLLMså’Œäº”ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒSafePTRåœ¨ç¼“è§£è¶Šç‹±é£é™©æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸”ä¸å½±å“å®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01513v1">PDF</a> </p>
<p><strong>Summary</strong><br>    å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šè¿‡ç»“åˆè§†è§‰è¾“å…¥æ‰©å±•äº†è¯­è¨€æ¨¡å‹çš„æ”¯æŒèŒƒå›´ï¼Œæ”¯æŒè§†è§‰æ¨ç†ã€‚ä½†è¿™ä¹Ÿå¼•å…¥äº†æ–°çš„æ¼æ´ï¼Œä½¿MLLMså®¹æ˜“å—åˆ°å¤šæ¨¡æ€è¶Šç‹±æ”»å‡»ï¼Œé˜»ç¢äº†å…¶å®‰å…¨éƒ¨ç½²ã€‚ç°æœ‰é˜²å¾¡æ–¹æ³•è¯•å›¾é€šè¿‡å¯¹é½å¤šæ¨¡æ€è¾“å…¥ä¸LLMsçš„å†…ç½®å®‰å…¨æœºåˆ¶æ¥è§£å†³é—®é¢˜ï¼Œä½†ä»æœªèƒ½å‘ç°å¤šæ¨¡æ€æ¼æ´çš„æ ¹æœ¬åŸå› ï¼Œç‰¹åˆ«æ˜¯æœ‰å®³çš„å¤šæ¨¡æ€ä»¤ç‰Œå¦‚ä½•è§¦å‘MLLMsä¸­çš„è¶Šç‹±ã€‚å› æ­¤ï¼Œå®ƒä»¬ä»ç„¶å®¹æ˜“å—åˆ°æ–‡æœ¬é©±åŠ¨çš„å¤šæ¨¡æ€è¶Šç‹±æ”»å‡»ï¼Œè¡¨ç°å‡ºè¿‡åº¦é˜²å¾¡è¡Œä¸ºå’Œæ²‰é‡çš„è®­ç»ƒè´Ÿæ‹…ã€‚æˆ‘ä»¬å¯¹MLLMsä¸­æœ‰å®³å¤šæ¨¡æ€ä»¤ç‰Œç»•è¿‡ä¿éšœçš„æƒ…å†µè¿›è¡Œäº†ç»¼åˆåˆ†æï¼Œå‘ç°æ—©æœŸä¸­é—´å±‚ä¸­ä¸åˆ°1%çš„ä»¤ç‰Œè´Ÿè´£å¼•å‘ä¸å®‰å…¨è¡Œä¸ºã€‚åŸºäºæ­¤å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†æ— éœ€å®‰å…¨è°ƒæ•´çš„Safe Prune-then-Restoreï¼ˆSafePTRï¼‰é˜²å¾¡æ¡†æ¶ï¼Œå®ƒå¯ä»¥é€‰æ‹©æ€§åœ°åœ¨è„†å¼±å±‚åˆ é™¤æœ‰å®³ä»¤ç‰Œï¼Œå¹¶åœ¨åç»­å±‚æ¢å¤è‰¯æ€§ç‰¹å¾ã€‚SafePTRåœ¨ä¸å¢åŠ è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹ï¼Œæ˜¾è‘—æé«˜äº†MLLMsçš„å®‰å…¨æ€§ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æ”¯æŒè§†è§‰æ¨ç†ï¼Œä½†å¼•å…¥æ–°çš„æ¼æ´ï¼Œæ˜“å—åˆ°å¤šæ¨¡æ€è¶Šç‹±æ”»å‡»ã€‚</li>
<li>ç°æœ‰é˜²å¾¡æ–¹æ³•è¯•å›¾é€šè¿‡å¯¹é½å¤šæ¨¡æ€è¾“å…¥ä¸LLMsçš„å®‰å…¨æœºåˆ¶æ¥è§£å†³é—®é¢˜ï¼Œä½†å­˜åœ¨ç¼ºé™·ã€‚</li>
<li>æœ‰å®³çš„å¤šæ¨¡æ€ä»¤ç‰Œæ˜¯å¼•å‘MLLMsä¸å®‰å…¨è¡Œä¸ºçš„å…³é”®ã€‚</li>
<li>ä»…å°‘é‡ä»¤ç‰Œï¼ˆä¸åˆ°1%çš„æ—©æœŸä¸­é—´å±‚ä»¤ç‰Œï¼‰å¯¹ä¸å®‰å…¨è¡Œä¸ºè´Ÿè´£ã€‚</li>
<li>æå‡ºSafe Prune-then-Restoreï¼ˆSafePTRï¼‰é˜²å¾¡æ¡†æ¶ï¼Œèƒ½é€‰æ‹©æ€§åˆ é™¤æœ‰å®³ä»¤ç‰Œå¹¶æ¢å¤è‰¯æ€§ç‰¹å¾ã€‚</li>
<li>SafePTRæ˜¾è‘—æé«˜MLLMsçš„å®‰å…¨æ€§ï¼ŒåŒæ—¶ä¿æŒé«˜æ•ˆæ€§ï¼Œæ— éœ€é¢å¤–çš„è®¡ç®—å¼€é”€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01513">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ad42cbc283f5d2079fbe6f9a2f09e0aa.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b5a1cd7a71dd6a0e7d3c107e323e4217.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6cf923b7f1cf53354b483b135ad598d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3b7b5c5fdea89b57768280f3fb79036.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5eec9fb6f6d0de7138a06a6252f715a3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5757d125114ee218f4c63996390a7853.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-54e48ba52b25d504a4097de0cb1c82aa.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DocShaDiffusion-Diffusion-Model-in-Latent-Space-for-Document-Image-Shadow-Removal"><a href="#DocShaDiffusion-Diffusion-Model-in-Latent-Space-for-Document-Image-Shadow-Removal" class="headerlink" title="DocShaDiffusion: Diffusion Model in Latent Space for Document Image   Shadow Removal"></a>DocShaDiffusion: Diffusion Model in Latent Space for Document Image   Shadow Removal</h2><p><strong>Authors:Wenjie Liu, Bingshu Wang, Ze Wang, C. L. Philip Chen</strong></p>
<p>Document shadow removal is a crucial task in the field of document image enhancement. However, existing methods tend to remove shadows with constant color background and ignore color shadows. In this paper, we first design a diffusion model in latent space for document image shadow removal, called DocShaDiffusion. It translates shadow images from pixel space to latent space, enabling the model to more easily capture essential features. To address the issue of color shadows, we design a shadow soft-mask generation module (SSGM). It is able to produce accurate shadow mask and add noise into shadow regions specially. Guided by the shadow mask, a shadow mask-aware guided diffusion module (SMGDM) is proposed to remove shadows from document images by supervising the diffusion and denoising process. We also propose a shadow-robust perceptual feature loss to preserve details and structures in document images. Moreover, we develop a large-scale synthetic document color shadow removal dataset (SDCSRD). It simulates the distribution of realistic color shadows and provides powerful supports for the training of models. Experiments on three public datasets validate the proposed methodâ€™s superiority over state-of-the-art. Our code and dataset will be publicly available. </p>
<blockquote>
<p>æ–‡æ¡£é˜´å½±å»é™¤æ˜¯æ–‡æ¡£å›¾åƒå¢å¼ºé¢†åŸŸä¸­çš„ä¸€é¡¹å…³é”®ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å€¾å‘äºå»é™¤å…·æœ‰æ’å®šèƒŒæ™¯è‰²çš„é˜´å½±ï¼Œè€Œå¿½è§†å½©è‰²é˜´å½±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆåœ¨æ½œåœ¨ç©ºé—´è®¾è®¡äº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œç”¨äºæ–‡æ¡£å›¾åƒé˜´å½±å»é™¤ï¼Œç§°ä¸ºDocShaDiffusionã€‚å®ƒå°†é˜´å½±å›¾åƒä»åƒç´ ç©ºé—´ç¿»è¯‘åˆ°æ½œåœ¨ç©ºé—´ï¼Œä½¿æ¨¡å‹æ›´å®¹æ˜“æ•è·å…³é”®ç‰¹å¾ã€‚ä¸ºäº†è§£å†³å½©è‰²é˜´å½±çš„é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé˜´å½±è½¯æ©è†œç”Ÿæˆæ¨¡å—ï¼ˆSSGMï¼‰ã€‚å®ƒèƒ½å¤Ÿäº§ç”Ÿç²¾ç¡®çš„é˜´å½±æ©è†œï¼Œå¹¶åœ¨é˜´å½±åŒºåŸŸä¸­åŠ å…¥å™ªå£°ã€‚åœ¨é˜´å½±æ©è†œçš„æŒ‡å¯¼ä¸‹ï¼Œæå‡ºäº†ä¸€ä¸ªé˜´å½±æ©è†œæ„ŸçŸ¥å¼•å¯¼æ‰©æ•£æ¨¡å—ï¼ˆSMGDMï¼‰ï¼Œé€šè¿‡ç›‘ç£æ‰©æ•£å’Œå»å™ªè¿‡ç¨‹æ¥å»é™¤æ–‡æ¡£å›¾åƒä¸­çš„é˜´å½±ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§é˜´å½±é²æ£’çš„æ„ŸçŸ¥ç‰¹å¾æŸå¤±ï¼Œä»¥ä¿ç•™æ–‡æ¡£å›¾åƒä¸­çš„ç»†èŠ‚å’Œç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤§è§„æ¨¡åˆæˆæ–‡æ¡£å½©è‰²é˜´å½±å»é™¤æ•°æ®é›†ï¼ˆSDCSRDï¼‰ã€‚å®ƒæ¨¡æ‹Ÿäº†ç°å®å½©è‰²é˜´å½±çš„åˆ†å¸ƒï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›äº†æœ‰åŠ›çš„æ”¯æŒã€‚åœ¨ä¸‰ä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•ç›¸è¾ƒäºæœ€æ–°æŠ€æœ¯çš„ä¼˜è¶Šæ€§ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01422v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ½œåœ¨ç©ºé—´çš„æ‰©æ•£æ¨¡å‹ï¼ˆDocShaDiffusionï¼‰ï¼Œç”¨äºæ–‡æ¡£å›¾åƒé˜´å½±å»é™¤ã€‚é€šè¿‡åƒç´ ç©ºé—´åˆ°æ½œåœ¨ç©ºé—´çš„è½¬æ¢ï¼Œæ¨¡å‹èƒ½æ›´è½»æ¾åœ°æ•æ‰å…³é”®ç‰¹å¾ã€‚ä¸ºè§£å†³å½©è‰²é˜´å½±é—®é¢˜ï¼Œè®¾è®¡äº†é˜´å½±è½¯æ©è†œç”Ÿæˆæ¨¡å—ï¼ˆSSGMï¼‰ï¼Œèƒ½å‡†ç¡®ç”Ÿæˆé˜´å½±æ©è†œå¹¶å¯¹é˜´å½±åŒºåŸŸæ·»åŠ ç‰¹å®šå™ªå£°ã€‚åœ¨ç›‘ç£æ‰©æ•£å’Œå»å™ªè¿‡ç¨‹ä¸­ï¼Œæå‡ºäº†é˜´å½±æ©è†œæ„ŸçŸ¥å¼•å¯¼æ‰©æ•£æ¨¡å—ï¼ˆSMGDMï¼‰ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§é˜´å½±é²æ£’æ„ŸçŸ¥ç‰¹å¾æŸå¤±ï¼Œä»¥ä¿ç•™æ–‡æ¡£å›¾åƒçš„ç»†èŠ‚å’Œç»“æ„ã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ä¸€ä¸ªå¤§å‹åˆæˆæ–‡æ¡£å½©è‰²é˜´å½±å»é™¤æ•°æ®é›†ï¼ˆSDCSRDï¼‰ï¼Œæ¨¡æ‹ŸçœŸå®å½©è‰²é˜´å½±çš„åˆ†å¸ƒï¼Œä¸ºæ¨¡å‹è®­ç»ƒæä¾›æœ‰åŠ›æ”¯æŒã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†åŸºäºæ½œåœ¨ç©ºé—´çš„æ‰©æ•£æ¨¡å‹ï¼ˆDocShaDiffusionï¼‰è¿›è¡Œæ–‡æ¡£å›¾åƒé˜´å½±å»é™¤ã€‚</li>
<li>é€šè¿‡åƒç´ ç©ºé—´åˆ°æ½œåœ¨ç©ºé—´çš„è½¬æ¢ï¼Œä½¿æ¨¡å‹æ›´å®¹æ˜“æ•æ‰å…³é”®ç‰¹å¾ã€‚</li>
<li>è®¾è®¡äº†é˜´å½±è½¯æ©è†œç”Ÿæˆæ¨¡å—ï¼ˆSSGMï¼‰ï¼Œä»¥å¤„ç†å½©è‰²é˜´å½±é—®é¢˜ã€‚</li>
<li>æå‡ºäº†é˜´å½±æ©è†œæ„ŸçŸ¥å¼•å¯¼æ‰©æ•£æ¨¡å—ï¼ˆSMGDMï¼‰ï¼Œç”¨äºç›‘ç£å»é™¤æ–‡æ¡£å›¾åƒçš„é˜´å½±ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§é˜´å½±é²æ£’æ„ŸçŸ¥ç‰¹å¾æŸå¤±ï¼Œä»¥ä¿ç•™æ–‡æ¡£å›¾åƒçš„ç»†èŠ‚å’Œç»“æ„ã€‚</li>
<li>å¼€å‘äº†ä¸€ä¸ªåˆæˆæ–‡æ¡£å½©è‰²é˜´å½±å»é™¤æ•°æ®é›†ï¼ˆSDCSRDï¼‰ï¼Œæ¨¡æ‹ŸçœŸå®æ•°æ®åˆ†å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01422">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-594493b390b27ef59c3ead738d823257.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28c2626c4811173bed143809ee3ef8be.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c835280ab45e764f433df60338c7882e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4ce0bf599f729ce27759772d22267b44.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-018e3460bb1cfe57c06c32cd26656cc6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-8c362a795c3738046415578e6eee9ffc.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="BronchoGAN-Anatomically-consistent-and-domain-agnostic-image-to-image-translation-for-video-bronchoscopy"><a href="#BronchoGAN-Anatomically-consistent-and-domain-agnostic-image-to-image-translation-for-video-bronchoscopy" class="headerlink" title="BronchoGAN: Anatomically consistent and domain-agnostic image-to-image   translation for video bronchoscopy"></a>BronchoGAN: Anatomically consistent and domain-agnostic image-to-image   translation for video bronchoscopy</h2><p><strong>Authors:Ahmad Soliman, Ron Keuth, Marian Himstedt</strong></p>
<p>The limited availability of bronchoscopy images makes image synthesis particularly interesting for training deep learning models. Robust image translation across different domains â€“ virtual bronchoscopy, phantom as well as in-vivo and ex-vivo image data â€“ is pivotal for clinical applications. This paper proposes BronchoGAN introducing anatomical constraints for image-to-image translation being integrated into a conditional GAN. In particular, we force bronchial orifices to match across input and output images. We further propose to use foundation model-generated depth images as intermediate representation ensuring robustness across a variety of input domains establishing models with substantially less reliance on individual training datasets. Moreover our intermediate depth image representation allows to easily construct paired image data for training. Our experiments showed that input images from different domains (e.g. virtual bronchoscopy, phantoms) can be successfully translated to images mimicking realistic human airway appearance. We demonstrated that anatomical settings (i.e. bronchial orifices) can be robustly preserved with our approach which is shown qualitatively and quantitatively by means of improved FID, SSIM and dice coefficients scores. Our anatomical constraints enabled an improvement in the Dice coefficient of up to 0.43 for synthetic images. Through foundation models for intermediate depth representations, bronchial orifice segmentation integrated as anatomical constraints into conditional GANs we are able to robustly translate images from different bronchoscopy input domains. BronchoGAN allows to incorporate public CT scan data (virtual bronchoscopy) in order to generate large-scale bronchoscopy image datasets with realistic appearance. BronchoGAN enables to bridge the gap of missing public bronchoscopy images. </p>
<blockquote>
<p>ç”±äºæ”¯æ°”ç®¡é•œæ£€æŸ¥å›¾åƒçš„å¯è·å–æ€§æœ‰é™ï¼Œå› æ­¤å›¾åƒåˆæˆå¯¹äºè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ¥è¯´ç‰¹åˆ«æœ‰è¶£ã€‚ä¸åŒé¢†åŸŸï¼ˆå¦‚è™šæ‹Ÿæ”¯æ°”ç®¡é•œã€å¹»å½±ä»¥åŠä½“å†…å’Œä½“å¤–å›¾åƒæ•°æ®ï¼‰ä¹‹é—´çš„ç¨³å¥å›¾åƒç¿»è¯‘å¯¹ä¸´åºŠåº”ç”¨è‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†BronchoGANï¼Œå¼•å…¥äº†å›¾åƒåˆ°å›¾åƒç¿»è¯‘çš„è§£å‰–å­¦çº¦æŸï¼Œå¹¶å°†å…¶é›†æˆåˆ°æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ä¸­ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¼ºåˆ¶è¦æ±‚æ”¯æ°”ç®¡å¼€å£åœ¨è¾“å…¥å’Œè¾“å‡ºå›¾åƒä¹‹é—´åŒ¹é…ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å»ºè®®ä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆçš„æ·±åº¦å›¾åƒä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œä»¥ç¡®ä¿åœ¨å„ç§è¾“å…¥é¢†åŸŸçš„ç¨³å¥æ€§ï¼Œå¹¶å»ºç«‹å¯¹ä¸ªåˆ«è®­ç»ƒæ•°æ®é›†ä¾èµ–æ›´å°‘çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ä¸­é—´æ·±åº¦å›¾åƒè¡¨ç¤ºå¯ä»¥è½»æ¾åœ°æ„å»ºç”¨äºè®­ç»ƒçš„ä¸€å¯¹å›¾åƒæ•°æ®ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ¥è‡ªä¸åŒé¢†åŸŸçš„è¾“å…¥å›¾åƒï¼ˆä¾‹å¦‚è™šæ‹Ÿæ”¯æ°”ç®¡é•œã€å¹»å½±ï¼‰å¯ä»¥æˆåŠŸåœ°ç¿»è¯‘æˆæ¨¡æ‹ŸçœŸå®äººç±»æ°”é“å¤–è§‚çš„å›¾åƒã€‚æˆ‘ä»¬è¯æ˜ï¼Œé€šè¿‡æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¯ä»¥ç¨³å¥åœ°ä¿ç•™è§£å‰–ç»“æ„è®¾ç½®ï¼ˆå³æ”¯æ°”ç®¡å¼€å£ï¼‰ï¼Œè¿™é€šè¿‡æ”¹è¿›çš„FIDã€SSIMå’ŒDiceç³»æ•°å¾—åˆ†è¿›è¡Œå®šæ€§å’Œå®šé‡è¯æ˜ã€‚æˆ‘ä»¬çš„è§£å‰–çº¦æŸä½¿åˆæˆå›¾åƒçš„Diceç³»æ•°æé«˜äº†é«˜è¾¾0.43ã€‚é€šè¿‡ç”¨äºä¸­é—´æ·±åº¦è¡¨ç¤ºçš„åŸºç¡€æ¨¡å‹ï¼Œå°†æ”¯æ°”ç®¡å¼€å£åˆ†å‰²ä½œä¸ºè§£å‰–å­¦çº¦æŸé›†æˆåˆ°æ¡ä»¶GANä¸­ï¼Œæˆ‘ä»¬èƒ½å¤Ÿç¨³å¥åœ°ç¿»è¯‘æ¥è‡ªä¸åŒæ”¯æ°”ç®¡é•œæ£€æŸ¥è¾“å…¥é¢†åŸŸçš„å›¾åƒã€‚BronchoGANå…è®¸æ•´åˆå…¬å…±CTæ‰«ææ•°æ®ï¼ˆè™šæ‹Ÿæ”¯æ°”ç®¡é•œï¼‰ï¼Œä»¥ç”Ÿæˆå…·æœ‰é€¼çœŸå¤–è§‚çš„å¤§è§„æ¨¡æ”¯æ°”ç®¡é•œæ£€æŸ¥å›¾åƒæ•°æ®é›†ã€‚BronchoGANèƒ½å¤Ÿå¼¥è¡¥å…¬å…±æ”¯æ°”ç®¡é•œæ£€æŸ¥å›¾åƒç¼ºå¤±çš„ç©ºç™½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01387v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†BronchoGANåœ¨æ”¯æ°”ç®¡é•œå›¾åƒåˆæˆæ–¹é¢çš„åº”ç”¨ã€‚è¯¥æ¨¡å‹é€šè¿‡å¼•å…¥è§£å‰–çº¦æŸï¼Œå®ç°äº†ä¸åŒé¢†åŸŸå›¾åƒä¹‹é—´çš„ç¨³å¥è½¬æ¢ï¼Œå¦‚è™šæ‹Ÿæ”¯æ°”ç®¡é•œã€å¹»å½±ä»¥åŠä½“å†…å’Œç¦»ä½“å›¾åƒæ•°æ®ã€‚ä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆçš„æ·±åº¦å›¾åƒä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œæé«˜äº†æ¨¡å‹å¯¹ä¸åŒè¾“å…¥é¢†åŸŸçš„ç¨³å¥æ€§ï¼Œå¹¶é™ä½äº†å¯¹å•ä¸ªè®­ç»ƒæ•°æ®é›†çš„ä¾èµ–ã€‚å®éªŒè¡¨æ˜ï¼Œä¸åŒé¢†åŸŸçš„è¾“å…¥å›¾åƒå¯ä»¥æˆåŠŸç¿»è¯‘ä¸ºé€‚åº”ç°å®äººç±»æ°”é“å¤–è§‚çš„å›¾åƒã€‚é€šè¿‡å¼•å…¥è§£å‰–çº¦æŸå’ŒåŸºäºåŸºç¡€æ¨¡å‹çš„ä¸­é—´æ·±åº¦è¡¨ç¤ºï¼ŒBronchoGANèƒ½å¤Ÿç¨³å¥åœ°ä¿å­˜è§£å‰–è®¾ç½®ï¼Œå¹¶äº§ç”Ÿé«˜è´¨é‡çš„å›¾åƒæ•°æ®ã€‚æ­¤å¤–ï¼ŒBronchoGANè¿˜å…è®¸åˆ©ç”¨å…¬å…±CTæ‰«ææ•°æ®ç”Ÿæˆå¤§è§„æ¨¡æ”¯æ°”ç®¡é•œå›¾åƒæ•°æ®é›†ï¼Œä»è€Œå¼¥è¡¥äº†å…¬å…±æ”¯æ°”ç®¡é•œå›¾åƒæ•°æ®çš„ç¼ºä¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>BronchoGANç”¨äºæ”¯æ°”ç®¡é•œå›¾åƒåˆæˆï¼Œè§£å†³äº†å›¾åƒæ•°æ®æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>è¯¥æ¨¡å‹å®ç°äº†ä¸åŒé¢†åŸŸå›¾åƒä¹‹é—´çš„ç¨³å¥è½¬æ¢ï¼Œå¦‚è™šæ‹Ÿæ”¯æ°”ç®¡é•œã€å¹»å½±ä»¥åŠä½“å†…å’Œç¦»ä½“å›¾åƒã€‚</li>
<li>å¼•å…¥è§£å‰–çº¦æŸï¼Œå¼ºåˆ¶è¾“å…¥å’Œè¾“å‡ºå›¾åƒçš„æ”¯æ°”ç®¡å¼€å£åŒ¹é…ã€‚</li>
<li>ä½¿ç”¨åŸºç¡€æ¨¡å‹ç”Ÿæˆçš„æ·±åº¦å›¾åƒä½œä¸ºä¸­é—´è¡¨ç¤ºï¼Œæé«˜æ¨¡å‹ç¨³å¥æ€§å¹¶é™ä½å¯¹ä¸ªåˆ«è®­ç»ƒæ•°æ®é›†çš„ä¾èµ–ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹èƒ½æˆåŠŸç¿»è¯‘ä¸åŒé¢†åŸŸçš„è¾“å…¥å›¾åƒä¸ºé€¼çœŸçš„æ”¯æ°”ç®¡é•œå›¾åƒã€‚</li>
<li>BronchoGANèƒ½å¤Ÿå®šé‡å’Œå®šæ€§åœ°ä¿å­˜è§£å‰–è®¾ç½®ï¼Œå¹¶é€šè¿‡æ”¹è¿›FIDã€SSIMå’ŒDiceç³»æ•°ç­‰æŒ‡æ ‡ä½“ç°å…¶æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01387">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-14e9d1f1ee5abfccc9ff87811e3adf2c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8dcaafe778d3b6f80c2e5ae3a6bad9ed.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ShapeEmbed-a-self-supervised-learning-framework-for-2D-contour-quantification"><a href="#ShapeEmbed-a-self-supervised-learning-framework-for-2D-contour-quantification" class="headerlink" title="ShapeEmbed: a self-supervised learning framework for 2D contour   quantification"></a>ShapeEmbed: a self-supervised learning framework for 2D contour   quantification</h2><p><strong>Authors:Anna Foix Romero, Craig Russell, Alexander Krull, Virginie Uhlmann</strong></p>
<p>The shape of objects is an important source of visual information in a wide range of applications. One of the core challenges of shape quantification is to ensure that the extracted measurements remain invariant to transformations that preserve an objectâ€™s intrinsic geometry, such as changing its size, orientation, and position in the image. In this work, we introduce ShapeEmbed, a self-supervised representation learning framework designed to encode the contour of objects in 2D images, represented as a Euclidean distance matrix, into a shape descriptor that is invariant to translation, scaling, rotation, reflection, and point indexing. Our approach overcomes the limitations of traditional shape descriptors while improving upon existing state-of-the-art autoencoder-based approaches. We demonstrate that the descriptors learned by our framework outperform their competitors in shape classification tasks on natural and biological images. We envision our approach to be of particular relevance to biological imaging applications. </p>
<blockquote>
<p>ç‰©ä½“çš„å½¢çŠ¶æ˜¯ä¼—å¤šåº”ç”¨é¢†åŸŸä¸­è§†è§‰ä¿¡æ¯çš„é‡è¦æ¥æºä¹‹ä¸€ã€‚å½¢çŠ¶é‡åŒ–çš„æ ¸å¿ƒæŒ‘æˆ˜ä¹‹ä¸€æ˜¯ç¡®ä¿æå–çš„æµ‹é‡å€¼åœ¨é¢å¯¹ä¿æŒç‰©ä½“å†…åœ¨å‡ ä½•ç‰¹æ€§çš„å˜æ¢ï¼ˆå¦‚æ”¹å˜å…¶å¤§å°ã€æ–¹å‘å’Œå›¾åƒä¸­çš„ä½ç½®ï¼‰æ—¶ä¿æŒä¸å˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ShapeEmbedï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å°†äºŒç»´å›¾åƒä¸­çš„ç‰©ä½“è½®å»“ç¼–ç ä¸ºæ¬§å‡ é‡Œå¾—è·ç¦»çŸ©é˜µï¼Œè¿›è€Œå¾—åˆ°ä¸€ä¸ªå¯¹å¹³ç§»ã€ç¼©æ”¾ã€æ—‹è½¬ã€åå°„å’Œç‚¹ç´¢å¼•ä¸å˜çš„å½¢çŠ¶æè¿°ç¬¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿå½¢çŠ¶æè¿°ç¬¦çš„å±€é™æ€§ï¼ŒåŒæ—¶æ”¹è¿›äº†ç°æœ‰çš„åŸºäºè‡ªåŠ¨ç¼–ç å™¨çš„æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶æ‰€å­¦çš„æè¿°ç¬¦åœ¨è‡ªç„¶å›¾åƒå’Œç”Ÿç‰©å›¾åƒçš„å½¢çŠ¶åˆ†ç±»ä»»åŠ¡ä¸­ä¼˜äºç«äº‰å¯¹æ‰‹ã€‚æˆ‘ä»¬è®¾æƒ³æˆ‘ä»¬çš„æ–¹æ³•å¯¹ç”Ÿç‰©æˆåƒåº”ç”¨å…·æœ‰ç‰¹åˆ«é‡è¦çš„æ„ä¹‰ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01009v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å½¢çŠ¶ä¿¡æ¯æ˜¯è§†è§‰åº”ç”¨ä¸­çš„å…³é”®ä¿¡æ¯æ¥æºã€‚æœ¬æ–‡ä»‹ç»äº†ShapeEmbedï¼Œä¸€ç§è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œå¯å°†äºŒç»´å›¾åƒä¸­çš„ç‰©ä½“è½®å»“ç¼–ç ä¸ºå½¢çŠ¶æè¿°ç¬¦ï¼Œè¯¥æè¿°ç¬¦å…·æœ‰å¯¹å¹³ç§»ã€ç¼©æ”¾ã€æ—‹è½¬ã€åå°„å’Œç‚¹ç´¢å¼•çš„ä¸å˜æ€§ã€‚ShapeEmbedæ¡†æ¶åœ¨å½¢çŠ¶åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿå’ŒåŸºäºè‡ªåŠ¨ç¼–ç å™¨çš„å…ˆè¿›æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶å’Œç”Ÿç‰©å›¾åƒæ–¹é¢ã€‚è¯¥æ¡†æ¶ç‰¹åˆ«é€‚ç”¨äºç”Ÿç‰©æˆåƒåº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å½¢çŠ¶ä¿¡æ¯æ˜¯è§†è§‰åº”ç”¨ä¸­çš„é‡è¦ä¿¡æ¯æ¥æºã€‚</li>
<li>ShapeEmbedæ˜¯ä¸€ç§è‡ªç›‘ç£è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºç¼–ç ç‰©ä½“è½®å»“ä¸ºå½¢çŠ¶æè¿°ç¬¦ã€‚</li>
<li>è¯¥æè¿°ç¬¦å…·æœ‰å¯¹å¤šç§å˜æ¢çš„ä¸å˜æ€§ï¼ŒåŒ…æ‹¬å¹³ç§»ã€ç¼©æ”¾ã€æ—‹è½¬ã€åå°„å’Œç‚¹ç´¢å¼•ã€‚</li>
<li>ShapeEmbedåœ¨å½¢çŠ¶åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•å’Œç°æœ‰çš„å…ˆè¿›è‡ªåŠ¨ç¼–ç å™¨æ–¹æ³•ã€‚</li>
<li>ShapeEmbedæ¡†æ¶ç‰¹åˆ«é€‚ç”¨äºè‡ªç„¶å’Œç”Ÿç‰©å›¾åƒçš„å½¢çŠ¶åˆ†ç±»ä»»åŠ¡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01009">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-97fa0bbfd5c8d24d0ca46e3efc9e25a5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c437cabb73d90dd7d999a55b34af009f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-723af7aec3f6bcf40bc87b9b01db5d6a.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="ADAptation-Reconstruction-based-Unsupervised-Active-Learning-for-Breast-Ultrasound-Diagnosis"><a href="#ADAptation-Reconstruction-based-Unsupervised-Active-Learning-for-Breast-Ultrasound-Diagnosis" class="headerlink" title="ADAptation: Reconstruction-based Unsupervised Active Learning for Breast   Ultrasound Diagnosis"></a>ADAptation: Reconstruction-based Unsupervised Active Learning for Breast   Ultrasound Diagnosis</h2><p><strong>Authors:Yaofei Duan, Yuhao Huang, Xin Yang, Luyi Han, Xinyu Xie, Zhiyuan Zhu, Ping He, Ka-Hou Chan, Ligang Cui, Sio-Kei Im, Dong Ni, Tao Tan</strong></p>
<p>Deep learning-based diagnostic models often suffer performance drops due to distribution shifts between training (source) and test (target) domains. Collecting and labeling sufficient target domain data for model retraining represents an optimal solution, yet is limited by time and scarce resources. Active learning (AL) offers an efficient approach to reduce annotation costs while maintaining performance, but struggles to handle the challenge posed by distribution variations across different datasets. In this study, we propose a novel unsupervised Active learning framework for Domain Adaptation, named ADAptation, which efficiently selects informative samples from multi-domain data pools under limited annotation budget. As a fundamental step, our method first utilizes the distribution homogenization capabilities of diffusion models to bridge cross-dataset gaps by translating target images into source-domain style. We then introduce two key innovations: (a) a hypersphere-constrained contrastive learning network for compact feature clustering, and (b) a dual-scoring mechanism that quantifies and balances sample uncertainty and representativeness. Extensive experiments on four breast ultrasound datasets (three public and one in-house&#x2F;multi-center) across five common deep classifiers demonstrate that our method surpasses existing strong AL-based competitors, validating its effectiveness and generalization for clinical domain adaptation. The code is available at the anonymized link: <a target="_blank" rel="noopener" href="https://github.com/miccai25-966/ADAptation">https://github.com/miccai25-966/ADAptation</a>. </p>
<blockquote>
<p>åŸºäºæ·±åº¦å­¦ä¹ çš„è¯Šæ–­æ¨¡å‹ç”±äºè®­ç»ƒï¼ˆæºï¼‰å’Œæµ‹è¯•ï¼ˆç›®æ ‡ï¼‰åŸŸä¹‹é—´çš„åˆ†å¸ƒå˜åŒ–ï¼Œå¸¸å¸¸ä¼šå‡ºç°æ€§èƒ½ä¸‹é™çš„æƒ…å†µã€‚ä¸ºæ¨¡å‹é‡æ–°è®­ç»ƒæ”¶é›†å¹¶æ ‡æ³¨è¶³å¤Ÿçš„ç›®æ ‡åŸŸæ•°æ®æ˜¯ä¸€ç§æœ€ä½³è§£å†³æ–¹æ¡ˆï¼Œç„¶è€Œè¿™å—åˆ°æ—¶é—´å’Œèµ„æºçš„é™åˆ¶ã€‚ä¸»åŠ¨å­¦ä¹ ï¼ˆALï¼‰æä¾›äº†ä¸€ç§åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶é™ä½æ ‡æ³¨æˆæœ¬çš„æœ‰æ•ˆæ–¹æ³•ï¼Œä½†éš¾ä»¥åº”å¯¹ä¸åŒæ•°æ®é›†åˆ†å¸ƒå˜åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºåŸŸè‡ªé€‚åº”çš„æ–°å‹æ— ç›‘ç£ä¸»åŠ¨å­¦ä¹ æ¡†æ¶ï¼Œåä¸ºADAptationï¼Œå®ƒèƒ½å¤Ÿåœ¨æœ‰é™çš„æ ‡æ³¨é¢„ç®—ä¸‹ï¼Œä»å¤šåŸŸæ•°æ®æ± ä¸­æœ‰æ•ˆåœ°é€‰æ‹©ä¿¡æ¯æ ·æœ¬ã€‚ä½œä¸ºåŸºæœ¬æ­¥éª¤ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é¦–å…ˆåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒå‡åŒ–èƒ½åŠ›ï¼Œé€šè¿‡å°†ç›®æ ‡å›¾åƒç¿»è¯‘æˆæºåŸŸé£æ ¼æ¥å¼¥åˆè·¨æ•°æ®é›†çš„å·®è·ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸¤é¡¹å…³é”®åˆ›æ–°ï¼šï¼ˆaï¼‰ç”¨äºç´§å‡‘ç‰¹å¾èšç±»çš„è¶…çƒçº¦æŸå¯¹æ¯”å­¦ä¹ ç½‘ç»œï¼Œï¼ˆbï¼‰ä¸€ç§é‡åŒ–å¹¶å¹³è¡¡æ ·æœ¬ä¸ç¡®å®šæ€§å’Œä»£è¡¨æ€§çš„åŒè¯„åˆ†æœºåˆ¶ã€‚åœ¨å››ä¸ªä¹³æˆ¿è¶…å£°æ•°æ®é›†ï¼ˆä¸‰ä¸ªå…¬å¼€å’Œä¸€ä¸ªå†…éƒ¨&#x2F;å¤šä¸­å¿ƒï¼‰ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨äº”ç§å¸¸è§çš„æ·±åº¦åˆ†ç±»å™¨ä¸Šçš„è¡¨ç°è¶…è¿‡äº†ç°æœ‰çš„å¼ºå¤§åŸºäºALçš„ç«äº‰å¯¹æ‰‹ï¼ŒéªŒè¯äº†å…¶åœ¨ä¸´åºŠåŸŸé€‚åº”ä¸­çš„æœ‰æ•ˆæ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯åœ¨åŒ¿åé“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/miccai25-966/ADAptation%E3%80%82">https://github.com/miccai25-966/ADAptationã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00474v1">PDF</a> 11 pages, 4 figures, 4 tables. Accepted by conference MICCAI2025</p>
<p><strong>Summary</strong><br>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è¯Šæ–­ä»»åŠ¡ä¸­å¸¸å› è®­ç»ƒï¼ˆæºï¼‰åŸŸä¸æµ‹è¯•ï¼ˆç›®æ ‡ï¼‰åŸŸä¹‹é—´çš„åˆ†å¸ƒå˜åŒ–è€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºåº”å¯¹æ­¤é—®é¢˜ï¼Œæ”¶é›†å¹¶æ ‡æ³¨è¶³å¤Ÿçš„ç›®æ ‡åŸŸæ•°æ®ä»¥ä¾›æ¨¡å‹é‡æ–°è®­ç»ƒæ˜¯æœ€ä¼˜è§£å†³æ–¹æ¡ˆï¼Œä½†å—é™äºæ—¶é—´å’Œèµ„æºã€‚ä¸»åŠ¨å­¦ä¹ ï¼ˆALï¼‰æ—¨åœ¨é™ä½æ ‡æ³¨æˆæœ¬åŒæ—¶ä¿æŒæ€§èƒ½ï¼Œä½†éš¾ä»¥å¤„ç†ä¸åŒæ•°æ®é›†é—´åˆ†å¸ƒå˜åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åä¸ºADAptationçš„æ–°å‹æ— ç›‘ç£ä¸»åŠ¨å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½åœ¨æœ‰é™çš„æ ‡æ³¨é¢„ç®—ä¸‹ï¼Œä»å¤šåŸŸæ•°æ®æ± ä¸­æœ‰æ•ˆé€‰æ‹©ä¿¡æ¯æ ·æœ¬ã€‚é¦–å…ˆï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒå‡åŒ–èƒ½åŠ›ï¼Œé€šè¿‡ç¿»è¯‘ç›®æ ‡å›¾åƒè‡³æºåŸŸé£æ ¼æ¥ç¼©å°è·¨æ•°æ®é›†å·®è·ã€‚æ¥ç€å¼•å…¥ä¸¤é¡¹åˆ›æ–°ï¼šè¶…çƒçº¦æŸå¯¹æ¯”å­¦ä¹ ç½‘ç»œè¿›è¡Œç´§å‡‘ç‰¹å¾èšç±»ï¼Œä»¥åŠé‡åŒ–å¹¶å¹³è¡¡æ ·æœ¬ä¸ç¡®å®šæ€§å’Œä»£è¡¨æ€§çš„åŒè¯„åˆ†æœºåˆ¶ã€‚åœ¨å››ä¸ªä¹³æˆ¿è¶…å£°æ³¢æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œç›¸è¾ƒäºå…¶ä»–å¼ºå¤§çš„ALç«äº‰å¯¹æ‰‹ï¼Œæœ¬æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼ŒéªŒè¯äº†å…¶åœ¨ä¸´åºŠåŸŸé€‚åº”ä¸­çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è¯Šæ–­ä»»åŠ¡ä¸­é¢ä¸´æºåŸŸä¸ç›®æ ‡åŸŸåˆ†å¸ƒå˜åŒ–å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ã€‚</li>
<li>æ”¶é›†å¹¶æ ‡æ³¨ç›®æ ‡åŸŸæ•°æ®æ˜¯åº”å¯¹æ­¤é—®é¢˜çš„æœ€ä¼˜è§£å†³æ–¹æ¡ˆï¼Œä½†å—é™äºæ—¶é—´å’Œèµ„æºã€‚</li>
<li>ä¸»åŠ¨å­¦ä¹ ï¼ˆALï¼‰è™½å¯é™ä½æ ‡æ³¨æˆæœ¬ï¼Œä½†éš¾ä»¥å¤„ç†ä¸åŒæ•°æ®é›†é—´çš„åˆ†å¸ƒå˜åŒ–ã€‚</li>
<li>æœ¬ç ”ç©¶æå‡ºåä¸ºADAptationçš„æ–°å‹æ— ç›‘ç£ä¸»åŠ¨å­¦ä¹ æ¡†æ¶ï¼Œèƒ½ä»å¤šåŸŸæ•°æ®æ± ä¸­æœ‰æ•ˆé€‰æ‹©ä¿¡æ¯æ ·æœ¬ã€‚</li>
<li>åˆ©ç”¨æ‰©æ•£æ¨¡å‹çš„åˆ†å¸ƒå‡åŒ–èƒ½åŠ›ç¼©å°è·¨æ•°æ®é›†å·®è·ã€‚</li>
<li>å¼•å…¥è¶…çƒçº¦æŸå¯¹æ¯”å­¦ä¹ ç½‘ç»œå’ŒåŒè¯„åˆ†æœºåˆ¶ï¼Œåˆ†åˆ«ç”¨äºç´§å‡‘ç‰¹å¾èšé‡å’Œæ ·æœ¬ä¸ç¡®å®šæ€§ä¸ä»£è¡¨æ€§çš„é‡åŒ–å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0183fdae419a28b5c2150daf99bd874.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d538d9879de728bf67d71ef567184c8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0cdf499a44bb68c011dfaafa802e11c2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5e3f349f41565e18efbd340efde6c538.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SURE-VQA-Systematic-Understanding-of-Robustness-Evaluation-in-Medical-VQA-Tasks"><a href="#SURE-VQA-Systematic-Understanding-of-Robustness-Evaluation-in-Medical-VQA-Tasks" class="headerlink" title="SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical   VQA Tasks"></a>SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical   VQA Tasks</h2><p><strong>Authors:Kim-Celine Kahl, Selen Erkan, Jeremias Traub, Carsten T. LÃ¼th, Klaus Maier-Hein, Lena Maier-Hein, Paul F. Jaeger</strong></p>
<p>Vision-Language Models (VLMs) have great potential in medical tasks, like Visual Question Answering (VQA), where they could act as interactive assistants for both patients and clinicians. Yet their robustness to distribution shifts on unseen data remains a key concern for safe deployment. Evaluating such robustness requires a controlled experimental setup that allows for systematic insights into the modelâ€™s behavior. However, we demonstrate that current setups fail to offer sufficiently thorough evaluations. To address this gap, we introduce a novel framework, called \textit{SURE-VQA}, centered around three key requirements to overcome current pitfalls and systematically analyze VLM robustness: 1) Since robustness on synthetic shifts does not necessarily translate to real-world shifts, it should be measured on real-world shifts that are inherent to the VQA data; 2) Traditional token-matching metrics often fail to capture underlying semantics, necessitating the use of large language models (LLMs) for more accurate semantic evaluation; 3) Model performance often lacks interpretability due to missing sanity baselines, thus meaningful baselines should be reported that allow assessing the multimodal impact on the VLM. To demonstrate the relevance of this framework, we conduct a study on the robustness of various Fine-Tuning (FT) methods across three medical datasets with four types of distribution shifts. Our study highlights key insights into robustness: 1) No FT method consistently outperforms others in robustness, and 2) robustness trends are more stable across FT methods than across distribution shifts. Additionally, we find that simple sanity baselines that do not use the image data can perform surprisingly well and confirm LoRA as the best-performing FT method on in-distribution data. Code is provided at <a target="_blank" rel="noopener" href="https://github.com/IML-DKFZ/sure-vqa">https://github.com/IML-DKFZ/sure-vqa</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—ä»»åŠ¡ï¼ˆå¦‚è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ï¼‰ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯ä»¥ä½œä¸ºæ‚£è€…å’Œä¸´åºŠåŒ»ç”ŸåŒæ–¹çš„äº¤äº’å¼åŠ©æ‰‹ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æœªè§æ•°æ®ä¸Šçš„åˆ†å¸ƒè½¬ç§»ç¨³å¥æ€§ä»æ˜¯å®‰å…¨éƒ¨ç½²çš„å…³é”®é—®é¢˜ã€‚è¯„ä¼°è¿™ç§ç¨³å¥æ€§éœ€è¦ä¸€ä¸ªå—æ§çš„å®éªŒè®¾ç½®ï¼Œå…è®¸å¯¹æ¨¡å‹çš„è¡Œä¸ºè¿›è¡Œç³»ç»Ÿæ€§çš„æ´å¯Ÿã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è¯æ˜å½“å‰çš„è®¾ç½®æ— æ³•æä¾›è¶³å¤Ÿå…¨é¢çš„è¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºâ€œSURE-VQAâ€çš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»¥ä¸‰ä¸ªå…³é”®è¦æ±‚ä¸ºä¸­å¿ƒï¼Œæ—¨åœ¨å…‹æœå½“å‰æ¼æ´å¹¶ç³»ç»Ÿåœ°åˆ†æVLMçš„ç¨³å¥æ€§ï¼š1ï¼‰ç”±äºåˆæˆè½¬ç§»ä¸Šçš„ç¨³å¥æ€§å¹¶ä¸ä¸€å®šç­‰åŒäºç°å®ä¸–ç•Œä¸­çš„è½¬ç§»ï¼Œå› æ­¤å®ƒåº”åœ¨VQAæ•°æ®å›ºæœ‰çš„ç°å®ä¸–ç•Œè½¬ç§»ä¸­è¿›è¡Œè¡¡é‡ï¼›2ï¼‰ä¼ ç»Ÿçš„ä»¤ç‰ŒåŒ¹é…æŒ‡æ ‡é€šå¸¸æ— æ³•æ•è·æ½œåœ¨è¯­ä¹‰ï¼Œå› æ­¤éœ€è¦åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ›´å‡†ç¡®çš„è¯­ä¹‰è¯„ä¼°ï¼›3ï¼‰ç”±äºç¼ºå°‘åŸºå‡†å€¼ï¼Œæ¨¡å‹æ€§èƒ½å¾€å¾€ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå› æ­¤åº”æŠ¥å‘Šæœ‰æ„ä¹‰çš„åŸºå‡†å€¼ï¼Œä»¥ä¾¿è¯„ä¼°å¯¹VLMçš„å¤šæ¨¡å¼å½±å“ã€‚ä¸ºäº†è¯æ˜è¯¥æ¡†æ¶çš„é‡è¦æ€§ï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªåŒ»å­¦æ•°æ®é›†å’Œå››ç§åˆ†å¸ƒè½¬ç§»ç±»å‹ä¸Šå¯¹å„ç§å¾®è°ƒï¼ˆFTï¼‰æ–¹æ³•çš„ç¨³å¥æ€§è¿›è¡Œäº†ç ”ç©¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯¹ç¨³å¥æ€§æœ‰äº†å…³é”®è§è§£ï¼š1ï¼‰æ²¡æœ‰FTæ–¹æ³•èƒ½åœ¨ç¨³å¥æ€§æ–¹é¢å§‹ç»ˆä¼˜äºå…¶ä»–æ–¹æ³•ï¼›2ï¼‰ä¸FTæ–¹æ³•ç›¸æ¯”ï¼Œç¨³å¥æ€§è¶‹åŠ¿åœ¨åˆ†å¸ƒè½¬ç§»ä¹‹é—´æ›´ä¸ºç¨³å®šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ä¸ä½¿ç”¨å›¾åƒæ•°æ®çš„ç®€å•åŸºå‡†çº¿å¯ä»¥è¡¨ç°æƒŠäººï¼Œå¹¶ç¡®è®¤LoRAæ˜¯åœ¨å†…éƒ¨åˆ†å¸ƒæ•°æ®ä¸Šè¡¨ç°æœ€ä½³çš„FTæ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/IML-DKFZ/sure-vqa%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/IML-DKFZ/sure-vqaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19688v2">PDF</a> TMLR 07&#x2F;2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—ä»»åŠ¡ä¸­çš„æ½œåŠ›ä¸æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°æ¡†æ¶åœ¨è¯„ä¼°æ¨¡å‹é²æ£’æ€§æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶â€œSURE-VQAâ€ï¼Œå¹¶å›´ç»•ä¸‰ä¸ªå…³é”®è¦æ±‚è¿›è¡Œç³»ç»Ÿåˆ†æï¼š1ï¼‰åœ¨çœŸå®ä¸–ç•Œçš„æ•°æ®åˆ†å¸ƒå˜åŒ–ä¸Šè¿›è¡Œé²æ£’æ€§è¯„ä¼°ï¼›2ï¼‰ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ›´å‡†ç¡®çš„è¯­ä¹‰è¯„ä¼°ï¼›3) æŠ¥å‘Šæœ‰æ„ä¹‰çš„åŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡å¼å¯¹VLMçš„å½±å“ã€‚é€šè¿‡å¯¹ä¸åŒå¾®è°ƒæ–¹æ³•çš„ç ”ç©¶ï¼Œå±•ç¤ºäº†è¯¥æ¡†æ¶çš„å®ç”¨æ€§ï¼Œå¹¶å¾—å‡ºå…³é”®æ´å¯Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨åŒ»ç–—ä»»åŠ¡å¦‚è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä¸­æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯ä½œä¸ºæ‚£è€…å’Œä¸´åºŠåŒ»ç”Ÿçš„äº’åŠ¨åŠ©æ‰‹ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ¡†æ¶åœ¨è¯„ä¼°æ¨¡å‹é²æ£’æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œéœ€è¦æ–°çš„è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>SURE-VQAæ¡†æ¶æ—¨åœ¨è§£å†³ç°æœ‰é—®é¢˜ï¼Œå›´ç»•ä¸‰ä¸ªå…³é”®è¦æ±‚è¿›è¡Œç³»ç»Ÿåˆ†æï¼šçœŸå®ä¸–ç•Œæ•°æ®åˆ†å¸ƒå˜åŒ–çš„è¯„ä¼°ã€ä½¿ç”¨LLMsè¿›è¡Œè¯­ä¹‰è¯„ä¼°ã€æŠ¥å‘Šæœ‰æ„ä¹‰çš„åŸºå‡†ä»¥è¯„ä¼°å¤šæ¨¡å¼å½±å“ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼Œä¸åŒå¾®è°ƒæ–¹æ³•åœ¨ä¸åŒçš„åŒ»ç–—æ•°æ®é›†ä¸Šçš„é²æ£’æ€§è¡¨ç°ä¸ä¸€è‡´ï¼Œæ²¡æœ‰ä¸€ç§æ–¹æ³•å§‹ç»ˆä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>æ¨¡å‹é²æ£’æ€§çš„è¶‹åŠ¿åœ¨ä¸åŒå¾®è°ƒæ–¹æ³•ä¹‹é—´æ¯”åœ¨ä¸åŒæ•°æ®åˆ†å¸ƒå˜åŒ–ä¹‹é—´æ›´ç¨³å®šã€‚</li>
<li>ç®€å•çš„åŸºå‡†æµ‹è¯•ï¼Œå³ä¸ä½¿ç”¨å›¾åƒæ•°æ®çš„æµ‹è¯•ï¼Œå¯èƒ½ä¼šè¡¨ç°å¾—ç›¸å½“å¥½ã€‚</li>
<li>LoRAæ˜¯åœ¨å†…éƒ¨åˆ†å¸ƒæ•°æ®ä¸Šè¡¨ç°æœ€ä½³çš„çš„å¾®è°ƒæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19688">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b000079ea6462b3c4d7ca381a7a28111.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-e948fe18de177ef9fbc60fe3fb06c684.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2efc05dd5c40006efa36cc2cf970b9dc.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Average-Calibration-Error-A-Differentiable-Loss-for-Improved-Reliability-in-Image-Segmentation"><a href="#Average-Calibration-Error-A-Differentiable-Loss-for-Improved-Reliability-in-Image-Segmentation" class="headerlink" title="Average Calibration Error: A Differentiable Loss for Improved   Reliability in Image Segmentation"></a>Average Calibration Error: A Differentiable Loss for Improved   Reliability in Image Segmentation</h2><p><strong>Authors:Theodore Barfoot, Luis Garcia-Peraza-Herrera, Ben Glocker, Tom Vercauteren</strong></p>
<p>Deep neural networks for medical image segmentation often produce overconfident results misaligned with empirical observations. Such miscalibration, challenges their clinical translation. We propose to use marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss function to improve pixel-wise calibration without compromising segmentation quality. We show that this loss, despite using hard binning, is directly differentiable, bypassing the need for approximate but differentiable surrogate or soft binning approaches. Our work also introduces the concept of dataset reliability histograms which generalises standard reliability diagrams for refined visual assessment of calibration in semantic segmentation aggregated at the dataset level. Using mL1-ACE, we reduce average and maximum calibration error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS 2021 dataset. We share our code here: <a target="_blank" rel="noopener" href="https://github.com/cai4cai/ACE-DLIRIS">https://github.com/cai4cai/ACE-DLIRIS</a> </p>
<blockquote>
<p>é’ˆå¯¹åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ·±åº¦ç¥ç»ç½‘ç»œç»å¸¸äº§ç”Ÿè¿‡äºè‡ªä¿¡çš„ç»“æœï¼Œä¸ç»éªŒè§‚å¯Ÿä¸ç¬¦ã€‚è¿™ç§è¯¯æ ¡å‡†å¯¹å…¶ä¸´åºŠç¿»è¯‘æå‡ºäº†æŒ‘æˆ˜ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨è¾¹é™…L1å¹³å‡æ ¡å‡†è¯¯å·®ï¼ˆmL1-ACEï¼‰ä½œä¸ºä¸€ç§æ–°å‹è¾…åŠ©æŸå¤±å‡½æ•°ï¼Œä»¥æé«˜åƒç´ çº§çš„æ ¡å‡†ï¼ŒåŒæ—¶ä¸æŸå®³åˆ†å‰²è´¨é‡ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå°½ç®¡ä½¿ç”¨äº†ç¡¬åˆ†ç®±ï¼Œè¿™ç§æŸå¤±æ˜¯å¯ç›´æ¥å¾®åˆ†çš„ï¼Œä»è€Œé¿å…äº†éœ€è¦ä½¿ç”¨è¿‘ä¼¼ä½†å¯å¾®åˆ†çš„æ›¿ä»£æˆ–è½¯åˆ†ç®±æ–¹æ³•ã€‚æˆ‘ä»¬çš„å·¥ä½œè¿˜å¼•å…¥äº†æ•°æ®é›†å¯é æ€§ç›´æ–¹å›¾çš„æ¦‚å¿µï¼Œå®ƒæ¨å¹¿äº†æ ‡å‡†å¯é æ€§å›¾ï¼Œç”¨äºåœ¨æ•°æ®é›†çº§åˆ«å¯¹è¯­ä¹‰åˆ†å‰²çš„æ ¡å‡†è¿›è¡Œç²¾ç»†çš„è§†è§‰è¯„ä¼°ã€‚ä½¿ç”¨mL1-ACEï¼Œæˆ‘ä»¬åœ¨BraTS 2021æ•°æ®é›†ä¸Šå°†å¹³å‡å’Œæœ€å¤§æ ¡å‡†è¯¯å·®åˆ†åˆ«é™ä½äº†45%å’Œ55%ï¼ŒåŒæ—¶ä¿æŒDiceå¾—åˆ†ä¸º87%ã€‚æˆ‘ä»¬åœ¨ä»¥ä¸‹é“¾æ¥åˆ†äº«æˆ‘ä»¬çš„ä»£ç ï¼š<a target="_blank" rel="noopener" href="https://github.com/cai4cai/ACE-DLIRIS">https://github.com/cai4cai/ACE-DLIRIS</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2403.06759v2">PDF</a> </p>
<p><strong>Summary</strong><br>ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¸¸äº§ç”Ÿè¿‡äºè‡ªä¿¡çš„ç»“æœï¼Œä¸å®é™…è§‚æµ‹ä¸ç¬¦ã€‚æå‡ºä½¿ç”¨è¾¹é™…L1å¹³å‡æ ¡å‡†è¯¯å·®ï¼ˆmL1-ACEï¼‰ä½œä¸ºæ–°çš„è¾…åŠ©æŸå¤±å‡½æ•°ï¼Œæé«˜åƒç´ çº§æ ¡å‡†ï¼Œä¸å½±å“åˆ†å‰²è´¨é‡ã€‚å¼•å…¥æ•°æ®é›†å¯é æ€§ç›´æ–¹å›¾ï¼Œç”¨äºç²¾ç»†çš„è§†è§‰è¯„ä¼°ã€‚ä½¿ç”¨mL1-ACEåœ¨BraTS 2021æ•°æ®é›†ä¸Šå¹³å‡å’Œæœ€å¤§æ ¡å‡†è¯¯å·®åˆ†åˆ«é™ä½45%å’Œ55%ï¼ŒåŒæ—¶Diceåˆ†æ•°ä¿æŒ87%ã€‚ä»£ç å…±äº«äº<a target="_blank" rel="noopener" href="https://github.com/cai4cai/ACE-DLIRIS">é“¾æ¥</a>ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç¥ç»ç½‘ç»œåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å­˜åœ¨è¿‡åº¦è‡ªä¿¡çš„é—®é¢˜ã€‚</li>
<li>è¾¹é™…L1å¹³å‡æ ¡å‡†è¯¯å·®ï¼ˆmL1-ACEï¼‰ä½œä¸ºæ–°çš„è¾…åŠ©æŸå¤±å‡½æ•°ï¼Œå¯æé«˜åƒç´ çº§æ ¡å‡†ã€‚</li>
<li>å¼•å…¥æ•°æ®é›†å¯é æ€§ç›´æ–¹å›¾è¿›è¡Œç²¾ç»†çš„è§†è§‰è¯„ä¼°ã€‚</li>
<li>ä½¿ç”¨mL1-ACEåœ¨BraTS 2021æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜æ ¡å‡†æ•ˆæœï¼ŒåŒæ—¶ç»´æŒè¾ƒé«˜çš„åˆ†å‰²è´¨é‡ã€‚</li>
<li>æ–¹æ³•å¯ç›´æ¥åº”ç”¨äºç°æœ‰æ¨¡å‹ï¼Œæ— éœ€é¢å¤–çš„å¤æ‚æ“ä½œã€‚</li>
<li>ä»£ç å·²å…±äº«ï¼Œä¾¿äºå…¶ä»–ç ”ç©¶è€…ä½¿ç”¨ä¸éªŒè¯ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2403.06759">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-e8263ab8c92ec21703b6965c65f954cd.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9d6ad7c0dd2036cb14bf4af38aa6af2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b92c85046bac61be54c371272062d92f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9427cce65b7878cf6ea8e534a2d40b53.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/I2I%20Translation/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/I2I%20Translation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/I2I-Translation/">
                                    <span class="chip bg-color">I2I Translation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-ae2a53d0e4b1c20d126622245d3b15e1.jpg" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-04  evMLP An Efficient Event-Driven MLP Architecture for Vision
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/Few-Shot/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-fbb97edd4b79d0ce4e7b21f6fe76f658.jpg" class="responsive-img" alt="Few-Shot">
                        
                        <span class="card-title">Few-Shot</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-04  Activation Reward Models for Few-Shot Model Alignment
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                    Few-Shot
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Few-Shot/">
                        <span class="chip bg-color">Few-Shot</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">26254.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
