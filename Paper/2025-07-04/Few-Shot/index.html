<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Few-Shot">
    <meta name="description" content="Few-Shot æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-04  Activation Reward Models for Few-Shot Model Alignment">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Few-Shot | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-fbb97edd4b79d0ce4e7b21f6fe76f658.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Few-Shot</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Few-Shot/">
                                <span class="chip bg-color">Few-Shot</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Few-Shot/" class="post-category">
                                Few-Shot
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    12.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    50 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-04-æ›´æ–°"><a href="#2025-07-04-æ›´æ–°" class="headerlink" title="2025-07-04 æ›´æ–°"></a>2025-07-04 æ›´æ–°</h1><h2 id="Activation-Reward-Models-for-Few-Shot-Model-Alignment"><a href="#Activation-Reward-Models-for-Few-Shot-Model-Alignment" class="headerlink" title="Activation Reward Models for Few-Shot Model Alignment"></a>Activation Reward Models for Few-Shot Model Alignment</h2><p><strong>Authors:Tianning Chai, Chancharik Mitra, Brandon Huang, Gautam Rajendrakumar Gare, Zhiqiu Lin, Assaf Arbelle, Leonid Karlinsky, Rogerio Feris, Trevor Darrell, Deva Ramanan, Roei Herzig</strong></p>
<p>Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to human preferences is a central challenge in improving the quality of the modelsâ€™ generative outputs for real-world applications. A common approach is to use reward modeling to encode preferences, enabling alignment via post-training using reinforcement learning. However, traditional reward modeling is not easily adaptable to new preferences because it requires a separate reward model, commonly trained on large preference datasets. To address this, we introduce Activation Reward Models (Activation RMs) â€“ a novel few-shot reward modeling method that leverages activation steering to construct well-aligned reward signals using minimal supervision and no additional model finetuning. Activation RMs outperform existing few-shot reward modeling approaches such as LLM-as-a-judge with in-context learning, voting-based scoring, and token probability scoring on standard reward modeling benchmarks. Furthermore, we demonstrate the effectiveness of Activation RMs in mitigating reward hacking behaviors, highlighting their utility for safety-critical applications. Toward this end, we propose PreferenceHack, a novel few-shot setting benchmark, the first to test reward models on reward hacking in a paired preference format. Finally, we show that Activation RM achieves state-of-the-art performance on this benchmark, surpassing even GPT-4o. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ä¸äººç±»åå¥½å¯¹é½ï¼Œæ˜¯æå‡æ¨¡å‹åœ¨ç°å®åº”ç”¨ä¸­çš„ç”Ÿæˆè¾“å‡ºè´¨é‡çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯ä½¿ç”¨å¥–åŠ±å»ºæ¨¡æ¥ç¼–ç åå¥½ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒåå¯¹é½ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„å¥–åŠ±å»ºæ¨¡éš¾ä»¥é€‚åº”æ–°çš„åå¥½ï¼Œå› ä¸ºå®ƒéœ€è¦å•ç‹¬çš„å¥–åŠ±æ¨¡å‹ï¼Œé€šå¸¸æ˜¯åœ¨å¤§å‹åå¥½æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¿€æ´»å¥–åŠ±æ¨¡å‹ï¼ˆActivation RMsï¼‰â€”â€”ä¸€ç§æ–°å‹çš„åŸºäºå°‘é‡æ ·æœ¬çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨æ¿€æ´»è½¬å‘æŠ€æœ¯ï¼Œä½¿ç”¨æœ€å°‘çš„ç›‘ç£ä¿¡æ¯æ„å»ºè‰¯å¥½å¯¹é½çš„å¥–åŠ±ä¿¡å·ï¼Œæ— éœ€é¢å¤–çš„æ¨¡å‹å¾®è°ƒã€‚åœ¨æ ‡å‡†å¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ¿€æ´»å¥–åŠ±æ¨¡å‹åœ¨ç°æœ‰çš„åŸºäºå°‘é‡æ ·æœ¬çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ä¸Šè¡¨ç°æ›´ä¼˜ï¼Œå¦‚ä½œä¸ºè¯„å§”çš„LLMä¸ä¸Šä¸‹æ–‡å­¦ä¹ ã€åŸºäºæŠ•ç¥¨çš„è¯„åˆ†å’ŒåŸºäºæ ‡è®°æ¦‚ç‡çš„è¯„åˆ†ç­‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¯æ˜äº†æ¿€æ´»å¥–åŠ±æ¨¡å‹åœ¨ç¼“è§£å¥–åŠ±é»‘å®¢è¡Œä¸ºæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾å…¶åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†PreferenceHackâ€”â€”ä¸€ä¸ªåŸºäºå°‘é‡æ ·æœ¬è®¾ç½®çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œå®ƒæ˜¯é¦–ä¸ªæµ‹è¯•å¥–åŠ±æ¨¡å‹åœ¨é…å¯¹åå¥½æ ¼å¼ä¸‹çš„å¥–åŠ±é»‘å®¢è¡Œä¸ºçš„æµ‹è¯•ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†æ¿€æ´»RMåœ¨è¿™ä¸€åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½è¡¨ç°ï¼Œç”šè‡³è¶…è¶Šäº†GPT-4oã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01368v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¸äººç±»åå¥½å¯¹é½æ˜¯æå‡æ¨¡å‹ç”Ÿæˆè´¨é‡ã€æ»¡è¶³çœŸå®ä¸–ç•Œåº”ç”¨éœ€æ±‚çš„ä¸­å¿ƒæŒ‘æˆ˜ã€‚ä¼ ç»Ÿå¥–åŠ±å»ºæ¨¡éš¾ä»¥é€‚åº”æ–°åå¥½ï¼Œéœ€è¦å•ç‹¬å¥–åŠ±æ¨¡å‹ï¼Œå¸¸è§äºå¤§å‹åå¥½æ•°æ®é›†è®­ç»ƒã€‚æœ¬æ–‡å¼•å…¥Activation Reward Modelsï¼ˆARMsï¼‰ï¼Œä¸€ç§åˆ©ç”¨æ¿€æ´»æ§åˆ¶æ„å»ºè‰¯å¥½å¯¹é½å¥–åŠ±ä¿¡å·çš„æ–°é¢–few-shotå¥–åŠ±å»ºæ¨¡æ–¹æ³•ï¼Œåªéœ€æœ€å°ç›‘ç£ï¼Œæ— éœ€é¢å¤–æ¨¡å‹å¾®è°ƒã€‚ARMsåœ¨æ ‡å‡†å¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºLLM-as-a-judgeç­‰ç°æœ‰few-shotå¥–åŠ±å»ºæ¨¡æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡å±•ç¤ºäº†ARMsåœ¨ç¼“è§£å¥–åŠ±é»‘å®¢è¡Œä¸ºæ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œçªæ˜¾å…¶åœ¨å®‰å…¨å…³é”®åº”ç”¨ä¸­çš„å®ç”¨æ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºPreferenceHackï¼Œä¸€ç§æ–°é¢–çš„few-shotè®¾ç½®åŸºå‡†æµ‹è¯•ï¼Œé¦–æ¬¡ä»¥é…å¯¹åå¥½æ ¼å¼æµ‹è¯•å¥–åŠ±æ¨¡å‹åœ¨å¥–åŠ±é»‘å®¢æ–¹é¢çš„è¡¨ç°ã€‚æœ€ç»ˆï¼ŒARMåœ¨åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼Œç”šè‡³è¶…è¶ŠGPT-4oã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤šæ¨¡æ€æ¨¡å‹ä¸äººç±»åå¥½å¯¹é½æ˜¯æå‡æ¨¡å‹è´¨é‡çš„å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>ä¼ ç»Ÿå¥–åŠ±å»ºæ¨¡æ–¹æ³•éš¾ä»¥é€‚åº”æ–°åå¥½ï¼Œéœ€è¦å¤§å‹åå¥½æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚</li>
<li>å¼•å…¥Activation Reward Modelsï¼ˆARMsï¼‰ï¼Œä¸€ç§åˆ©ç”¨æ¿€æ´»æ§åˆ¶æ„å»ºå¥–åŠ±ä¿¡å·çš„few-shotå¥–åŠ±å»ºæ¨¡æ–°æ–¹æ³•ã€‚</li>
<li>ARMæ–¹æ³•åœ¨æ ‡å‡†å¥–åŠ±å»ºæ¨¡åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>ARMèƒ½æœ‰æ•ˆç¼“è§£å¥–åŠ±é»‘å®¢è¡Œä¸ºï¼Œé€‚ç”¨äºå®‰å…¨å…³é”®åº”ç”¨ã€‚</li>
<li>æå‡ºPreferenceHackåŸºå‡†æµ‹è¯•ï¼Œç”¨äºæµ‹è¯•å¥–åŠ±æ¨¡å‹åœ¨é…å¯¹åå¥½æ ¼å¼ä¸‹çš„å¥–åŠ±é»‘å®¢è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01368">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-41c6a173b7edef8635a09c179c5dc177.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-55619b153d480b829f762342762afac7.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-38a4017ab7e96a28b30fdbf70df5f077.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Symbolic-or-Numerical-Understanding-Physics-Problem-Solving-in-Reasoning-LLMs"><a href="#Symbolic-or-Numerical-Understanding-Physics-Problem-Solving-in-Reasoning-LLMs" class="headerlink" title="Symbolic or Numerical? Understanding Physics Problem Solving in   Reasoning LLMs"></a>Symbolic or Numerical? Understanding Physics Problem Solving in   Reasoning LLMs</h2><p><strong>Authors:Nifu Dan, Yujun Cai, Yiwei Wang</strong></p>
<p>Navigating the complexities of physics reasoning has long been a difficult task for Large Language Models (LLMs), requiring a synthesis of profound conceptual understanding and adept problem-solving techniques. In this study, we investigate the application of advanced instruction-tuned reasoning models, such as Deepseek-R1, to address a diverse spectrum of physics problems curated from the challenging SciBench benchmark. Our comprehensive experimental evaluation reveals the remarkable capabilities of reasoning models. Not only do they achieve state-of-the-art accuracy in answering intricate physics questions, but they also generate distinctive reasoning patterns that emphasize on symbolic derivation. Furthermore, our findings indicate that even for these highly sophisticated reasoning models, the strategic incorporation of few-shot prompting can still yield measurable improvements in overall accuracy, highlighting the potential for continued performance gains. </p>
<blockquote>
<p>é©¾é©­ç‰©ç†æ¨ç†çš„å¤æ‚æ€§å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ä¸€ç›´æ˜¯ä¸€é¡¹å›°éš¾çš„ä»»åŠ¡ï¼Œéœ€è¦æ·±åˆ»çš„æ¦‚å¿µç†è§£å’Œç†Ÿç»ƒçš„é—®é¢˜è§£å†³æŠ€æœ¯ç›¸ç»“åˆã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å…ˆè¿›æŒ‡ä»¤è°ƒä¼˜æ¨ç†æ¨¡å‹ï¼ˆå¦‚Deepseek-R1ï¼‰çš„åº”ç”¨ï¼Œä»¥è§£å†³ä»å…·æœ‰æŒ‘æˆ˜æ€§çš„SciBenchåŸºå‡†æµ‹è¯•ä¸­ç²¾å¿ƒæŒ‘é€‰çš„å„ç§ç‰©ç†é—®é¢˜ã€‚æˆ‘ä»¬çš„ç»¼åˆå®éªŒè¯„ä¼°æ˜¾ç¤ºäº†æ¨ç†æ¨¡å‹çš„å“è¶Šèƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹ä¸ä»…è¾¾åˆ°äº†è§£å†³å¤æ‚ç‰©ç†é—®é¢˜çš„æœ€æ–°å‡†ç¡®æ€§æ°´å¹³ï¼Œè€Œä¸”è¿˜äº§ç”Ÿäº†ç‹¬ç‰¹çš„æ¨ç†æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼ä¾§é‡äºç¬¦å·æ¨å¯¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯å¯¹äºè¿™äº›é«˜åº¦å¤æ‚çš„æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡æˆ˜ç•¥æ€§åœ°èå…¥å°‘é‡æç¤ºï¼ˆfew-shot promptingï¼‰ï¼Œä»ç„¶å¯ä»¥æé«˜æ€»ä½“å‡†ç¡®æ€§ï¼Œè¿™çªæ˜¾äº†æŒç»­æé«˜æ€§èƒ½çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01334v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æ¢è®¨äº†å…ˆè¿›æŒ‡ä»¤è°ƒæ•´å‹æ¨ç†æ¨¡å‹ï¼Œå¦‚Deepseek-R1åœ¨è§£å†³å¤šæ ·åŒ–ç‰©ç†é—®é¢˜æ–¹é¢çš„åº”ç”¨ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¿™äº›æ¨¡å‹ä¸ä»…åœ¨å›ç­”å¤æ‚ç‰©ç†é—®é¢˜æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè¿˜å±•ç°äº†ç‹¬ç‰¹çš„æ¨ç†æ¨¡å¼ï¼Œä¾§é‡äºç¬¦å·æ¨å¯¼ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ï¼Œå³ä½¿æ˜¯å¯¹äºé«˜åº¦å¤æ‚çš„æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡å°‘æ•°æ ·æœ¬æç¤ºç­–ç•¥çš„èå…¥ï¼Œä»å¯å®ç°æ€»ä½“å‡†ç¡®æ€§çš„æ˜¾è‘—æå‡ï¼Œæ˜¾ç¤ºå‡ºæ½œåœ¨çš„æŒç»­æ€§èƒ½æå‡å¯èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å…ˆè¿›çš„æŒ‡ä»¤è°ƒæ•´å‹æ¨ç†æ¨¡å‹ï¼Œå¦‚Deepseek-R1ï¼Œè¢«åº”ç”¨äºè§£å†³ä¸€ç³»åˆ—æ¥è‡ªSciBenchåŸºå‡†æµ‹è¯•çš„ç‰©ç†é—®é¢˜ã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨å›ç­”å¤æ‚ç‰©ç†é—®é¢˜æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>æ¨ç†æ¨¡å‹å±•ç°å‡ºç‹¬ç‰¹çš„æ¨ç†æ¨¡å¼ï¼Œä¾§é‡äºç¬¦å·æ¨å¯¼ã€‚</li>
<li>èå…¥å°‘æ•°æ ·æœ¬æç¤ºç­–ç•¥å¯æ˜¾è‘—æå‡æ¨¡å‹çš„æ€»ä½“å‡†ç¡®æ€§ã€‚</li>
<li>è¿™ç§ç­–ç•¥å¯¹äºé«˜åº¦å¤æ‚çš„æ¨ç†æ¨¡å‹åŒæ ·æœ‰æ•ˆã€‚</li>
<li>è¯¥ç ”ç©¶æ­ç¤ºäº†æŒç»­æ€§èƒ½æå‡çš„å¯èƒ½æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01334">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-f75168278177cd46de96df312c7a4822.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43ee0b3af7d989529f628567f903df25.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efed7ee1e128c14e937744ff362c89d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bce1c83d5c44a0ac7c57117747866cef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39a106965c2a43254c57b5354cd250ca.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Evaluating-LLMs-and-Prompting-Strategies-for-Automated-Hardware-Diagnosis-from-Textual-User-Reports"><a href="#Evaluating-LLMs-and-Prompting-Strategies-for-Automated-Hardware-Diagnosis-from-Textual-User-Reports" class="headerlink" title="Evaluating LLMs and Prompting Strategies for Automated Hardware   Diagnosis from Textual User-Reports"></a>Evaluating LLMs and Prompting Strategies for Automated Hardware   Diagnosis from Textual User-Reports</h2><p><strong>Authors:Carlos Caminha, Maria de Lourdes M. Silva, Iago C. Chaves, Felipe T. Brito, Victor A. E. Farias, Javam C. Machado</strong></p>
<p>Computer manufacturers offer platforms for users to describe device faults using textual reports such as â€œMy screen is flickeringâ€. Identifying the faulty component from the report is essential for automating tests and improving user experience. However, such reports are often ambiguous and lack detail, making this task challenging. Large Language Models (LLMs) have shown promise in addressing such issues. This study evaluates 27 open-source models (1B-72B parameters) and 2 proprietary LLMs using four prompting strategies: Zero-Shot, Few-Shot, Chain-of-Thought (CoT), and CoT+Few-Shot (CoT+FS). We conducted 98,948 inferences, processing over 51 million input tokens and generating 13 million output tokens. We achieve f1-score up to 0.76. Results show that three models offer the best balance between size and performance: mistral-small-24b-instruct and two smaller models, llama-3.2-1b-instruct and gemma-2-2b-it, that offer competitive performance with lower VRAM usage, enabling efficient inference on end-user devices as modern laptops or smartphones with NPUs. </p>
<blockquote>
<p>è®¡ç®—æœºåˆ¶é€ å•†ä¸ºç”¨æˆ·æä¾›å¹³å°ï¼Œè®©ä»–ä»¬é€šè¿‡æ–‡æœ¬æŠ¥å‘Šæè¿°è®¾å¤‡æ•…éšœï¼Œä¾‹å¦‚â€œæˆ‘çš„å±å¹•åœ¨é—ªçƒâ€ã€‚ä»æŠ¥å‘Šä¸­è¯†åˆ«å‡ºæ•…éšœç»„ä»¶å¯¹äºè‡ªåŠ¨åŒ–æµ‹è¯•å’Œæé«˜ç”¨æˆ·ä½“éªŒè‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ¥å‘Šå¾€å¾€æ¨¡ç³Šä¸æ¸…ï¼Œç¼ºä¹ç»†èŠ‚ï¼Œä½¿å¾—è¿™ä¸€ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è§£å†³è¿™äº›é—®é¢˜æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä½¿ç”¨å››ç§æç¤ºç­–ç•¥çš„27ä¸ªå¼€æºæ¨¡å‹ï¼ˆå‚æ•°èŒƒå›´ä»1Båˆ°72Bï¼‰å’Œ2ä¸ªä¸“æœ‰LLMï¼šé›¶æ ·æœ¬ã€å°æ ·æœ¬ã€æ€ç»´é“¾ï¼ˆCoTï¼‰å’Œæ€ç»´é“¾+å°æ ·æœ¬ï¼ˆCoT+FSï¼‰ã€‚æˆ‘ä»¬è¿›è¡Œäº†98948æ¬¡æ¨æ–­ï¼Œå¤„ç†è¶…è¿‡5100ä¸‡ä¸ªè¾“å…¥ä»¤ç‰Œå¹¶ç”Ÿæˆäº†è¶…è¿‡1.3äº¿ä¸ªè¾“å‡ºä»¤ç‰Œã€‚æˆ‘ä»¬è¾¾åˆ°äº†é«˜è¾¾0.76çš„f1åˆ†æ•°ã€‚ç»“æœè¡¨æ˜ï¼Œä¸‰ä¸ªæ¨¡å‹åœ¨è§„æ¨¡å’Œæ€§èƒ½ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ï¼šmistral-small-24b-instructå’Œä¸¤ä¸ªè¾ƒå°çš„æ¨¡å‹llama-3.2-1b-instructå’Œgemma-2-2b-itï¼Œå®ƒä»¬åœ¨è¾ƒä½VRAMä½¿ç”¨æƒ…å†µä¸‹æä¾›äº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½è¡¨ç°ï¼Œèƒ½å¤Ÿåœ¨ç°ä»£ç¬”è®°æœ¬ç”µè„‘æˆ–å¸¦æœ‰ç¥ç»å¤„ç†å•å…ƒï¼ˆNPUï¼‰çš„æ™ºèƒ½æ‰‹æœºä¸Šè¿›è¡Œé«˜æ•ˆæ¨æ–­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00742v1">PDF</a> To be published in the Proceedings of the Brazilian Integrated   Software and Hardware Seminar 2025 (SEMISH 2025)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥è§£å†³ç”¨æˆ·æè¿°è®¾å¤‡æ•…éšœæ—¶å­˜åœ¨çš„é—®é¢˜ã€‚é€šè¿‡å¯¹27ä¸ªå¼€æºæ¨¡å‹ï¼ˆå‚æ•°èŒƒå›´ä»1Båˆ°72Bï¼‰å’Œ2ä¸ªä¸“æœ‰LLMsè¿›è¡Œè¯„ä»·ï¼Œé‡‡ç”¨å››ç§æç¤ºç­–ç•¥ï¼šZero-Shotã€Few-Shotã€Chain-of-Thoughtï¼ˆCoTï¼‰å’ŒCoT+Few-Shotï¼ˆCoT+FSï¼‰ã€‚ç ”ç©¶å®ç°äº†é«˜è¾¾0.76çš„f1åˆ†æ•°ï¼Œå¹¶å‘ç°ä¸‰æ¬¾æ¨¡å‹åœ¨è§„æ¨¡å’Œæ€§èƒ½ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ï¼šmistral-small-24b-instructå’Œä¸¤ä¸ªè¾ƒå°çš„æ¨¡å‹llama-3.2-1b-instructå’Œgemma-2-2b-itã€‚è¿™äº›æ¨¡å‹åœ¨ç»ˆç«¯ç”¨æˆ·è®¾å¤‡ï¼ˆå¦‚å¸¦æœ‰ç¥ç»å¤„ç†å•å…ƒçš„ç°ä»£ç¬”è®°æœ¬ç”µè„‘æˆ–æ™ºèƒ½æ‰‹æœºï¼‰ä¸Šè¿›è¡Œé«˜æ•ˆæ¨ç†æ—¶ï¼Œå…·æœ‰è¾ƒä½çš„è§†é¢‘éšæœºè®¿é—®å†…å­˜ï¼ˆVRAMï¼‰ä½¿ç”¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«ç”¨äºè§£å†³ç”¨æˆ·æè¿°è®¾å¤‡æ•…éšœæ—¶çš„æ¨¡ç³Šå’Œç¼ºä¹ç»†èŠ‚çš„é—®é¢˜ã€‚</li>
<li>ç ”ç©¶è¯„ä¼°äº†å¤šç§LLMsæ¨¡å‹ï¼ŒåŒ…æ‹¬å¼€æºå’Œä¸“æœ‰æ¨¡å‹ï¼Œé‡‡ç”¨å››ç§ä¸åŒçš„æç¤ºç­–ç•¥ã€‚</li>
<li>é€šè¿‡è¿™äº›ç­–ç•¥ï¼Œç ”ç©¶å®ç°äº†è¾ƒé«˜çš„f1åˆ†æ•°ï¼Œè¡¨æ˜æ¨¡å‹åœ¨è¯†åˆ«æ•…éšœç»„ä»¶æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>å‘ç°ä¸‰æ¬¾æ¨¡å‹åœ¨è§„æ¨¡ï¼ˆå‚æ•°æ•°é‡ï¼‰å’Œæ€§èƒ½ä¹‹é—´è¾¾åˆ°äº†æœ€ä½³å¹³è¡¡ã€‚</li>
<li>è¿™äº›æœ€ä½³æ¨¡å‹åœ¨æ¨ç†æ—¶å…·æœ‰è¾ƒä½çš„è§†é¢‘éšæœºè®¿é—®å†…å­˜ï¼ˆVRAMï¼‰ä½¿ç”¨é‡ï¼Œé€‚åˆåœ¨ç»ˆç«¯ç”¨æˆ·è®¾å¤‡ä¸Šè¿›è¡Œé«˜æ•ˆæ¨ç†ã€‚</li>
<li>é“¾å¼æ€ç»´ï¼ˆChain-of-Thought, CoTï¼‰å’Œå°‘é‡æ ·æœ¬æç¤ºï¼ˆFew-Shotï¼‰ç­‰ç­–ç•¥åœ¨è¯„ä»·ä¸­è¡¨ç°å‡ºæ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00742">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0db8b07588adff399f2389ea45c76fef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7a765a0826039b8dee0cc6beece9532e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d37f3d17a3add8ce974c2162fb75cc2e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c03c9f2d953d1dfb4b3af0d80624d45e.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Few-shot-Classification-as-Multi-instance-Verification-Effective-Backbone-agnostic-Transfer-across-Domains"><a href="#Few-shot-Classification-as-Multi-instance-Verification-Effective-Backbone-agnostic-Transfer-across-Domains" class="headerlink" title="Few-shot Classification as Multi-instance Verification: Effective   Backbone-agnostic Transfer across Domains"></a>Few-shot Classification as Multi-instance Verification: Effective   Backbone-agnostic Transfer across Domains</h2><p><strong>Authors:Xin Xu, Eibe Frank, Geoffrey Holmes</strong></p>
<p>We investigate cross-domain few-shot learning under the constraint that fine-tuning of backbones (i.e., feature extractors) is impossible or infeasible â€“ a scenario that is increasingly common in practical use cases. Handling the low-quality and static embeddings produced by frozen, â€œblack-boxâ€ backbones leads to a problem representation of few-shot classification as a series of multiple instance verification (MIV) tasks. Inspired by this representation, we introduce a novel approach to few-shot domain adaptation, named the â€œMIV-headâ€, akin to a classification head that is agnostic to any pretrained backbone and computationally efficient. The core components designed for the MIV-head, when trained on few-shot data from a target domain, collectively yield strong performance on test data from that domain. Importantly, it does so without fine-tuning the backbone, and within the â€œmeta-testingâ€ phase. Experimenting under various settings and on an extension of the Meta-dataset benchmark for cross-domain few-shot image classification, using representative off-the-shelf convolutional neural network and vision transformer backbones pretrained on ImageNet1K, we show that the MIV-head achieves highly competitive accuracy when compared to state-of-the-art â€œadapterâ€ (or partially fine-tuning) methods applied to the same backbones, while incurring substantially lower adaptation cost. We also find well-known â€œclassification headâ€ approaches lag far behind in terms of accuracy. Ablation study empirically justifies the core components of our approach. We share our code at <a target="_blank" rel="noopener" href="https://github.com/xxweka/MIV-head">https://github.com/xxweka/MIV-head</a>. </p>
<blockquote>
<p>æˆ‘ä»¬ç ”ç©¶äº†è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ï¼Œå‰ææ˜¯æ— æ³•æˆ–ä¸å¯è¡Œå¯¹ä¸»å¹²ç½‘ç»œï¼ˆå³ç‰¹å¾æå–å™¨ï¼‰è¿›è¡Œå¾®è°ƒâ€”â€”è¿™åœ¨ç°å®åº”ç”¨åœºæ™¯ä¸­è¶Šæ¥è¶Šå¸¸è§ã€‚å¤„ç†ç”±å†»ç»“çš„â€œé»‘ç®±â€ä¸»å¹²äº§ç”Ÿçš„ä½è´¨é‡å’Œé™æ€åµŒå…¥ï¼Œå¯¼è‡´å°æ ·æœ¬åˆ†ç±»çš„é—®é¢˜è¢«è¡¨è¿°ä¸ºä¸€ç³»åˆ—åŸºäºå¤šé‡å®ä¾‹éªŒè¯ï¼ˆMIVï¼‰çš„ä»»åŠ¡ã€‚å—è¿™ç§è¡¨è¿°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå°æ ·æœ¬åŸŸé€‚åº”çš„æ–°æ–¹æ³•ï¼Œåä¸ºâ€œMIV-headâ€ï¼Œç±»ä¼¼äºå¯¹ä»»ä½•é¢„è®­ç»ƒä¸»å¹²éƒ½æ— å…³çš„åˆ†ç±»å¤´ï¼Œå¹¶ä¸”è®¡ç®—æ•ˆç‡é«˜ã€‚MIV-headçš„æ ¸å¿ƒç»„ä»¶ç»è¿‡ç›®æ ‡åŸŸçš„å°æ ·æœ¬æ•°æ®è®­ç»ƒåï¼Œåœ¨æ¥è‡ªè¯¥åŸŸçš„æµ‹è¯•æ•°æ®ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™ä¸€åˆ‡éƒ½æ˜¯åœ¨ä¸å¾®è°ƒä¸»å¹²çš„æƒ…å†µä¸‹ï¼Œä¸”åœ¨â€œå…ƒæµ‹è¯•â€é˜¶æ®µå®Œæˆçš„ã€‚åœ¨å„ç§è®¾ç½®ä¸‹ä»¥åŠè·¨åŸŸå°æ ·æœ¬å›¾åƒåˆ†ç±»çš„Meta-datasetåŸºå‡†æµ‹è¯•çš„æ‰©å±•ä¸Šè¿›è¡Œå®éªŒï¼Œä½¿ç”¨ä»£è¡¨æ€§çš„å³ç”¨å‹å·ç§¯ç¥ç»ç½‘ç»œå’Œè§†è§‰è½¬æ¢å™¨ä¸»å¹²åœ¨ImageNet1Kä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸åº”ç”¨äºç›¸åŒä¸»å¹²çš„æœ€æ–°â€œé€‚é…å™¨â€ï¼ˆæˆ–éƒ¨åˆ†å¾®è°ƒï¼‰æ–¹æ³•ç›¸æ¯”ï¼ŒMIV-headåœ¨å‡†ç¡®æ€§æ–¹é¢è¾¾åˆ°äº†é«˜åº¦ç«äº‰æ°´å¹³ï¼ŒåŒæ—¶äº§ç”Ÿçš„é€‚åº”æˆæœ¬å¤§å¤§é™ä½ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œä¼—æ‰€å‘¨çŸ¥çš„æ–¹æ³•ä¸­çš„â€œåˆ†ç±»å¤´â€åœ¨å‡†ç¡®æ€§æ–¹é¢è¿œè¿œè½åã€‚æ¶ˆèç ”ç©¶ç»éªŒæ€§åœ°è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/xxweka/MIV-head%E5%88%86%E4%BA%AB%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82">https://github.com/xxweka/MIV-headåˆ†äº«äº†æˆ‘ä»¬çš„ä»£ç ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00401v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬æ–‡æ¢ç´¢äº†æ— æ³•åœ¨ç‰¹å®šæƒ…å¢ƒä¸‹å¾®è°ƒéª¨å¹²ç½‘ï¼ˆç‰¹å¾æå–å™¨ï¼‰çš„è·¨åŸŸå°æ ·æœ¬å­¦ä¹ ã€‚é€šè¿‡å°†é—®é¢˜è¡¨è¿°ä¸ºä¸€ç³»åˆ—å¤šé‡å®ä¾‹éªŒè¯ï¼ˆMIVï¼‰ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åä¸ºâ€œMIV-headâ€çš„æ–°å‹å°æ ·æœ¬åŸŸè‡ªé€‚åº”æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¸ä¾èµ–äºé¢„è®­ç»ƒéª¨å¹²ç½‘ï¼Œè®¡ç®—æ•ˆç‡é«˜ï¼Œåœ¨ç›®æ ‡åŸŸçš„å°‘é‡æ•°æ®ä¸Šè®­ç»ƒæ—¶è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚åœ¨è·¨åŸŸå°å›¾åƒåˆ†ç±»çš„Meta-datasetåŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œä¸åº”ç”¨äºç›¸åŒéª¨å¹²ç½‘çš„æœ€æ–°é€‚é…å™¨æ–¹æ³•ç›¸æ¯”ï¼ŒMIV-headè¾¾åˆ°äº†é«˜åº¦ç«äº‰æ€§çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶é™ä½äº†é€‚åº”æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†è·¨åŸŸå°æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸‹çš„éª¨å¹²ç½‘å¾®è°ƒé™åˆ¶ï¼Œæå‡ºäº†åˆ©ç”¨å†»ç»“çš„â€œé»‘ç®±â€éª¨å¹²ç½‘äº§ç”Ÿçš„ä½è´¨é‡å’Œé™æ€åµŒå…¥çš„é—®é¢˜ã€‚</li>
<li>å°†å°æ ·æœ¬åˆ†ç±»é—®é¢˜è¡¨è¿°ä¸ºå¤šé‡å®ä¾‹éªŒè¯ï¼ˆMIVï¼‰ä»»åŠ¡ï¼Œå¯å‘äº†ä¸€ç§æ–°çš„å°‘æ ·æœ¬åŸŸè‡ªé€‚åº”æ–¹æ³•â€”â€”MIV-headã€‚</li>
<li>MIV-headè®¾è®¡äº†ä¸€ç§ä¸é¢„è®­ç»ƒéª¨å¹²ç½‘æ— å…³çš„åˆ†ç±»å¤´ï¼Œå…·æœ‰è®¡ç®—æ•ˆç‡é«˜çš„ä¼˜ç‚¹ã€‚</li>
<li>åœ¨ç›®æ ‡åŸŸçš„å°‘é‡æ•°æ®ä¸Šè®­ç»ƒæ—¶ï¼ŒMIV-headè¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¸éœ€è¦å¾®è°ƒéª¨å¹²ç½‘ï¼Œåœ¨å…ƒæµ‹è¯•é˜¶æ®µä¹Ÿæ˜¯å¦‚æ­¤ã€‚</li>
<li>åœ¨è·¨åŸŸå°å›¾åƒåˆ†ç±»çš„Meta-datasetåŸºå‡†æµ‹è¯•é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMIV-headä¸æœ€æ–°é€‚é…å™¨æ–¹æ³•ç›¸æ¯”å…·æœ‰é«˜åº¦ç«äº‰æ€§çš„å‡†ç¡®æ€§ã€‚</li>
<li>MIV-headé™ä½äº†é€‚åº”æˆæœ¬ï¼Œä¸å·²çŸ¥çš„åˆ†ç±»å¤´æ–¹æ³•ç›¸æ¯”ï¼Œå…¶å‡†ç¡®æ€§è¿œè¿œè¶…å‡ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00401">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ca9bb6d60bc4a43e5f3932ae71022215.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-baa96afc0f1143e5ebbf9910e457db9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ae9439fe5b35fe8fcd739383a7c376a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-387ca6dca6636f1df5acc3a70a65e292.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="PlantSegNeRF-A-few-shot-cross-dataset-method-for-plant-3D-instance-point-cloud-reconstruction-via-joint-channel-NeRF-with-multi-view-image-instance-matching"><a href="#PlantSegNeRF-A-few-shot-cross-dataset-method-for-plant-3D-instance-point-cloud-reconstruction-via-joint-channel-NeRF-with-multi-view-image-instance-matching" class="headerlink" title="PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance   point cloud reconstruction via joint-channel NeRF with multi-view image   instance matching"></a>PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance   point cloud reconstruction via joint-channel NeRF with multi-view image   instance matching</h2><p><strong>Authors:Xin Yang, Ruiming Du, Hanyang Huang, Jiayang Xie, Pengyao Xie, Leisen Fang, Ziyue Guo, Nanjun Jiang, Yu Jiang, Haiyan Cen</strong></p>
<p>Organ segmentation of plant point clouds is a prerequisite for the high-resolution and accurate extraction of organ-level phenotypic traits. Although the fast development of deep learning has boosted much research on segmentation of plant point clouds, the existing techniques for organ segmentation still face limitations in resolution, segmentation accuracy, and generalizability across various plant species. In this study, we proposed a novel approach called plant segmentation neural radiance fields (PlantSegNeRF), aiming to directly generate high-precision instance point clouds from multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF performed 2D instance segmentation on the multi-view images to generate instance masks for each organ with a corresponding ID. The multi-view instance IDs corresponding to the same plant organ were then matched and refined using a specially designed instance matching module. The instance NeRF was developed to render an implicit scene, containing color, density, semantic and instance information. The implicit scene was ultimately converted into high-precision plant instance point clouds based on the volume density. The results proved that in semantic segmentation of point clouds, PlantSegNeRF outperformed the commonly used methods, demonstrating an average improvement of 16.1%, 18.3%, 17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the second-best results on structurally complex datasets. More importantly, PlantSegNeRF exhibited significant advantages in plant point cloud instance segmentation tasks. Across all plant datasets, it achieved average improvements of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively. This study extends the organ-level plant phenotyping and provides a high-throughput way to supply high-quality 3D data for the development of large-scale models in plant science. </p>
<blockquote>
<p>æ¤ç‰©ç‚¹äº‘å™¨å®˜åˆ†å‰²æ˜¯å®ç°å™¨å®˜æ°´å¹³è¡¨å‹ç‰¹å¾é«˜ç²¾åº¦ã€é«˜åˆ†è¾¨ç‡æå–çš„å‰æã€‚å°½ç®¡æ·±åº¦å­¦ä¹ å¿«é€Ÿå‘å±•ï¼Œæ¨åŠ¨äº†æ¤ç‰©ç‚¹äº‘åˆ†å‰²é¢†åŸŸçš„å¤§é‡ç ”ç©¶ï¼Œä½†ç°æœ‰çš„å™¨å®˜åˆ†å‰²æŠ€æœ¯ä»é¢ä¸´ç€åˆ†è¾¨ç‡ã€åˆ†å‰²ç²¾åº¦ä»¥åŠè·¨ç‰©ç§æ³›åŒ–èƒ½åŠ›ç­‰æ–¹é¢çš„å±€é™ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºPlantSegNeRFçš„æ–°å‹æ–¹æ³•ï¼Œæ—¨åœ¨ä»å¤šè§†è§’RGBå›¾åƒåºåˆ—ç›´æ¥ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„å®ä¾‹ç‚¹äº‘ï¼Œé€‚ç”¨äºå¹¿æ³›çš„æ¤ç‰©ç‰©ç§ã€‚PlantSegNeRFå¯¹å¤šè§†è§’å›¾åƒè¿›è¡ŒäºŒç»´å®ä¾‹åˆ†å‰²ï¼Œä¸ºæ¯ä¸ªå™¨å®˜ç”Ÿæˆå…·æœ‰ç›¸åº”IDçš„å®ä¾‹æ©è†œã€‚ç„¶åï¼Œä½¿ç”¨ä¸“é—¨è®¾è®¡çš„å®ä¾‹åŒ¹é…æ¨¡å—å¯¹å¯¹åº”äºåŒä¸€æ¤ç‰©å™¨å®˜çš„è·¨è§†è§’å®ä¾‹IDè¿›è¡ŒåŒ¹é…å’Œç»†åŒ–ã€‚å¼€å‘äº†å®ä¾‹NeRFï¼Œä»¥å‘ˆç°åŒ…å«é¢œè‰²ã€å¯†åº¦ã€è¯­ä¹‰å’Œå®ä¾‹ä¿¡æ¯çš„éšå¼åœºæ™¯ã€‚æœ€ç»ˆï¼ŒåŸºäºä½“ç§¯å¯†åº¦å°†éšå¼åœºæ™¯è½¬æ¢ä¸ºé«˜ç²¾åº¦æ¤ç‰©å®ä¾‹ç‚¹äº‘ã€‚ç»“æœè¯æ˜ï¼Œåœ¨ç‚¹äº‘è¯­ä¹‰åˆ†å‰²æ–¹é¢ï¼ŒPlantSegNeRFä¼˜äºå¸¸ç”¨æ–¹æ³•ï¼Œåœ¨ç»“æ„å¤æ‚çš„æ•°æ®é›†ä¸Šï¼Œä¸ç¬¬äºŒå¥½çš„ç»“æœç›¸æ¯”ï¼Œç²¾åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°å’ŒIoUå¹³å‡æé«˜äº†16.1%ã€18.3%ã€17.8%å’Œ24.2%ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒPlantSegNeRFåœ¨æ¤ç‰©ç‚¹äº‘å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚åœ¨æ‰€æœ‰æ¤ç‰©æ•°æ®é›†ä¸Šï¼ŒmPrecã€mRecã€mCovå’ŒmWCovå¹³å‡åˆ†åˆ«æé«˜äº†11.7%ã€38.2%ã€32.2%å’Œ25.3%ã€‚è¯¥ç ”ç©¶æ‰©å±•äº†æ¤ç‰©å™¨å®˜æ°´å¹³çš„è¡¨å‹åˆ†æï¼Œå¹¶ä¸ºæ¤ç‰©ç§‘å­¦ä¸­å¤§è§„æ¨¡æ¨¡å‹çš„å¼€å‘æä¾›äº†ä¸€ç§é«˜è´¨é‡3Dæ•°æ®çš„é«˜é€šé‡ä¾›åº”æ–¹å¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00371v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºPlantSegNeRFçš„æ–°æ–¹æ³•ï¼Œç”¨äºä»å¤šè§†è§’RGBå›¾åƒåºåˆ—ç›´æ¥ç”Ÿæˆé«˜ç²¾åº¦æ¤ç‰©å™¨å®˜ç‚¹äº‘ã€‚è¯¥æ–¹æ³•é€šè¿‡å¤šè§†è§’å›¾åƒè¿›è¡ŒäºŒç»´å®ä¾‹åˆ†å‰²ï¼Œç”Ÿæˆæ¯ä¸ªå™¨å®˜çš„å®ä¾‹æ©è†œå’Œç›¸åº”IDã€‚é€šè¿‡è®¾è®¡çš„å®ä¾‹åŒ¹é…æ¨¡å—å¯¹åŒä¸€æ¤ç‰©å™¨å®˜çš„è·¨è§†è§’å®ä¾‹IDè¿›è¡ŒåŒ¹é…å’Œç»†åŒ–ã€‚åˆ©ç”¨éšå¼åœºæ™¯æ¸²æŸ“æŠ€æœ¯ç”ŸæˆåŒ…å«é¢œè‰²ã€å¯†åº¦ã€è¯­ä¹‰å’Œå®ä¾‹ä¿¡æ¯çš„NeRFæ¨¡å‹ï¼Œæœ€ç»ˆæ ¹æ®ä½“ç§¯å¯†åº¦è½¬æ¢ä¸ºé«˜ç²¾åº¦æ¤ç‰©å®ä¾‹ç‚¹äº‘ã€‚åœ¨ç‚¹äº‘è¯­ä¹‰åˆ†å‰²æ–¹é¢ï¼ŒPlantSegNeRFç›¸è¾ƒäºå¸¸ç”¨æ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹³å‡æé«˜äº†ç²¾åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°å’ŒIoUç­‰æŒ‡æ ‡ã€‚åœ¨æ¤ç‰©ç‚¹äº‘å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­ï¼ŒPlantSegNeRFä¹Ÿå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œä¸ºæ¤ç‰©ç§‘å­¦çš„å¤§è§„æ¨¡æ¨¡å‹å¼€å‘æä¾›äº†é«˜è´¨é‡çš„ä¸‰ç»´æ•°æ®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PlantSegNeRFæ˜¯ä¸€ç§ç”¨äºæ¤ç‰©ç‚¹äº‘åˆ†å‰²çš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿä»å¤šè§†è§’RGBå›¾åƒåºåˆ—ç”Ÿæˆé«˜ç²¾åº¦æ¤ç‰©å™¨å®˜ç‚¹äº‘ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡äºŒç»´å®ä¾‹åˆ†å‰²ç”Ÿæˆå®ä¾‹æ©è†œå’ŒIDï¼Œå¹¶å¯¹åŒä¸€å™¨å®˜çš„è·¨è§†è§’å®ä¾‹è¿›è¡ŒåŒ¹é…å’Œç»†åŒ–ã€‚</li>
<li>PlantSegNeRFåˆ©ç”¨NeRFæ¨¡å‹æ¸²æŸ“åŒ…å«å¤šç§ä¿¡æ¯çš„éšå¼åœºæ™¯ã€‚</li>
<li>åœ¨è¯­ä¹‰åˆ†å‰²æ–¹é¢ï¼ŒPlantSegNeRFè¾ƒå¸¸ç”¨æ–¹æ³•è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œæé«˜äº†å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚</li>
<li>PlantSegNeRFåœ¨æ¤ç‰©ç‚¹äº‘å®ä¾‹åˆ†å‰²ä»»åŠ¡ä¸­ä¹Ÿæœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œé€‚ç”¨äºæ¤ç‰©ç§‘å­¦çš„å¤§è§„æ¨¡æ¨¡å‹å¼€å‘ã€‚</li>
<li>è¯¥æ–¹æ³•ä¸ºé«˜é€šé‡æä¾›é«˜è´¨é‡ä¸‰ç»´æ•°æ®ï¼Œä¸ºæ¤ç‰©ç§‘å­¦ç ”ç©¶æä¾›æ–°çš„è§†è§’å’Œæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00371">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-46480644acaf08f9253a398767849bec.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Modeling-Data-Diversity-for-Joint-Instance-and-Verbalizer-Selection-in-Cold-Start-Scenarios"><a href="#Modeling-Data-Diversity-for-Joint-Instance-and-Verbalizer-Selection-in-Cold-Start-Scenarios" class="headerlink" title="Modeling Data Diversity for Joint Instance and Verbalizer Selection in   Cold-Start Scenarios"></a>Modeling Data Diversity for Joint Instance and Verbalizer Selection in   Cold-Start Scenarios</h2><p><strong>Authors:Mohna Chakraborty, Adithya Kulkarni, Qi Li</strong></p>
<p>Prompt-based methods leverage the knowledge of pre-trained language models (PLMs) trained with a masked language modeling (MLM) objective; however, these methods are sensitive to template, verbalizer, and few-shot instance selection, particularly in cold-start settings with no labeled data. Existing studies overlook the dependency between instances and verbalizers, where instance-label probabilities depend on verbalizer token proximity in the embedding space. To address this, we propose COLDSELECT, a joint verbalizer and instance selection approach that models data diversity. COLDSELECT maps PLM vocabulary and $h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction and clustering to ensure efficient and diverse selection. By optimizing for minimal uncertainty and maximal diversity, COLDSELECT captures data relationships effectively. Experiments on eight benchmarks demonstrate COLDSELECTâ€™s superiority in reducing uncertainty and enhancing generalization, outperforming baselines in verbalizer and few-shot instance selection for cold-start scenarios. </p>
<blockquote>
<p>åŸºäºæç¤ºçš„æ–¹æ³•åˆ©ç”¨äº†åˆ©ç”¨æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ç›®æ ‡è¿›è¡Œè®­ç»ƒçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰çš„çŸ¥è¯†ï¼›ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¯¹æ¨¡æ¿ã€è¡¨è¿°å™¨å’Œå°‘é‡å®ä¾‹é€‰æ‹©éå¸¸æ•æ„Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰æ ‡è®°æ•°æ®çš„å†·å¯åŠ¨ç¯å¢ƒä¸­ã€‚ç°æœ‰ç ”ç©¶å¿½è§†äº†å®ä¾‹å’Œè¡¨è¿°å™¨ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå…¶ä¸­å®ä¾‹æ ‡ç­¾æ¦‚ç‡å–å†³äºåµŒå…¥ç©ºé—´ä¸­è¡¨è¿°å™¨ä»¤ç‰Œçš„æ¥è¿‘ç¨‹åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†COLDSELECTï¼Œè¿™æ˜¯ä¸€ç§è”åˆè¡¨è¿°å™¨å’Œå®ä¾‹é€‰æ‹©æ–¹æ³•ï¼Œå¯ä»¥æ¨¡æ‹Ÿæ•°æ®å¤šæ ·æ€§ã€‚COLDSELECTå°†PLMè¯æ±‡è¡¨å’Œh_[MASK]åµŒå…¥æ˜ å°„åˆ°å…±äº«ç©ºé—´ï¼Œåº”ç”¨é™ç»´å’Œèšç±»æŠ€æœ¯ï¼Œä»¥ç¡®ä¿é«˜æ•ˆå’Œå¤šæ ·åŒ–çš„é€‰æ‹©ã€‚é€šè¿‡ä¼˜åŒ–æœ€å°ä¸ç¡®å®šæ€§å’Œæœ€å¤§å¤šæ ·æ€§ï¼ŒCOLDSELECTå¯ä»¥æœ‰æ•ˆåœ°æ•è·æ•°æ®å…³ç³»ã€‚åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCOLDSELECTåœ¨å‡å°‘ä¸ç¡®å®šæ€§å’Œæé«˜æ³›åŒ–èƒ½åŠ›æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œåœ¨å†·å¯åŠ¨åœºæ™¯çš„è¡¨è¿°å™¨å’Œå°‘é‡å®ä¾‹é€‰æ‹©æ–¹é¢ä¼˜äºåŸºçº¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00330v1">PDF</a> </p>
<p><strong>Summary</strong>:<br>é¢„å…ˆè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰ä½¿ç”¨åŸºäºæç¤ºçš„æ–¹æ³•ç»“åˆæ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼Œä½†è¿™ç§æ–¹æ³•å¯¹æ¨¡æ¿ã€æè¿°è¯å’Œå°‘é‡å®ä¾‹çš„é€‰æ‹©éå¸¸æ•æ„Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰æ ‡è®°æ•°æ®çš„å†·å¯åŠ¨ç¯å¢ƒä¸­å°¤ä¸ºå¦‚æ­¤ã€‚ç°æœ‰ç ”ç©¶å¿½ç•¥äº†å®ä¾‹ä¸æè¿°è¯ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºCOLDSELECTçš„æ–¹æ³•ï¼Œç”¨äºå¯¹å®ä¾‹è¿›è¡Œé€‰æ‹©å’Œè”åˆé€‰æ‹©æè¿°è¯ä»¥å»ºæ¨¡æ•°æ®å¤šæ ·æ€§ã€‚é€šè¿‡å°†PLMè¯æ±‡è¡¨å’Œæ©ç åµŒå…¥æ˜ å°„åˆ°å…±äº«ç©ºé—´ï¼Œåº”ç”¨é™ç»´å’Œèšç±»æŠ€æœ¯æ¥ç¡®ä¿é«˜æ•ˆä¸”å¤šæ ·åŒ–çš„é€‰æ‹©ã€‚é€šè¿‡æœ€å°åŒ–ä¸ç¡®å®šæ€§å¹¶æœ€å¤§åŒ–å¤šæ ·æ€§è¿›è¡Œä¼˜åŒ–ï¼ŒCOLDSELECTå¯ä»¥æœ‰æ•ˆåœ°æ•è·æ•°æ®å…³ç³»ã€‚åœ¨å…«ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCOLDSELECTåœ¨å‡å°‘ä¸ç¡®å®šæ€§å’Œæé«˜æ³›åŒ–èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨å†·å¯åŠ¨åœºæ™¯ä¸­æ”¹è¿›äº†æè¿°è¯å’Œå°‘é‡å®ä¾‹çš„é€‰æ‹©ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>åŸºäºæç¤ºçš„æ–¹æ³•ç»“åˆäº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œæ©ç è¯­è¨€å»ºæ¨¡ç›®æ ‡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•è¿‡äºä¾èµ–æ¨¡æ¿ã€æè¿°è¯å’Œå®ä¾‹é€‰æ‹©ï¼Œåœ¨å†·å¯åŠ¨åœºæ™¯ä¸­å°¤ä¸ºé‡è¦ã€‚</li>
<li>COLDSELECTè€ƒè™‘äº†å®ä¾‹ä¸æè¿°è¯ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œé€šè¿‡å»ºæ¨¡æ•°æ®å¤šæ ·æ€§æ¥ä¼˜åŒ–é€‰æ‹©è¿‡ç¨‹ã€‚</li>
<li>COLDSELECTä½¿ç”¨é™ç»´å’Œèšç±»æŠ€æœ¯å°†PLMè¯æ±‡è¡¨å’Œæ©ç åµŒå…¥æ˜ å°„åˆ°å…±äº«ç©ºé—´ã€‚</li>
<li>COLDSELECTæ—¨åœ¨æœ€å°åŒ–ä¸ç¡®å®šæ€§å¹¶æœ€å¤§åŒ–å¤šæ ·æ€§ï¼Œä»¥æ•è·æ•°æ®å…³ç³»ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00330">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b773264ba10e681b8315ba7e298111c4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-120334064ddf30ddb32fd76b35949bbd.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Beyond-Low-Rank-Tuning-Model-Prior-Guided-Rank-Allocation-for-Effective-Transfer-in-Low-Data-and-Large-Gap-Regimes"><a href="#Beyond-Low-Rank-Tuning-Model-Prior-Guided-Rank-Allocation-for-Effective-Transfer-in-Low-Data-and-Large-Gap-Regimes" class="headerlink" title="Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective   Transfer in Low-Data and Large-Gap Regimes"></a>Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective   Transfer in Low-Data and Large-Gap Regimes</h2><p><strong>Authors:Chuyan Zhang, Kefan Wang, Yun Gu</strong></p>
<p>Low-Rank Adaptation (LoRA) has proven effective in reducing computational costs while maintaining performance comparable to fully fine-tuned foundation models across various tasks. However, its fixed low-rank structure restricts its adaptability in scenarios with substantial domain gaps, where higher ranks are often required to capture domain-specific complexities. Current adaptive LoRA methods attempt to overcome this limitation by dynamically expanding or selectively allocating ranks, but these approaches frequently depend on computationally intensive techniques such as iterative pruning, rank searches, or additional regularization. To address these challenges, we introduce Stable Rank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the stable rank of pre-trained weight matrices as a natural prior for layer-wise rank allocation. By leveraging the stable rank, which reflects the intrinsic dimensionality of the weights, SR-LoRA enables a principled and efficient redistribution of ranks across layers, enhancing adaptability without incurring additional search costs. Empirical evaluations on few-shot tasks with significant domain gaps show that SR-LoRA consistently outperforms recent adaptive LoRA variants, achieving a superior trade-off between performance and efficiency. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA">https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA</a>. </p>
<blockquote>
<p>ä½ç§©é€‚åº”ï¼ˆLoRAï¼‰å·²è¯æ˜åœ¨å‡å°‘è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œèƒ½å¤Ÿåœ¨å„ç§ä»»åŠ¡ä¸Šä¿æŒä¸å®Œå…¨å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…¶å›ºå®šçš„ä½ç§©ç»“æ„é™åˆ¶äº†å…¶åœ¨å­˜åœ¨æ˜¾è‘—åŸŸå·®å¼‚åœºæ™¯ä¸­çš„é€‚åº”æ€§ï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œé€šå¸¸éœ€è¦æ›´é«˜çš„ç§©æ¥æ•æ‰ç‰¹å®šäºåŸŸçš„å¤æ‚æ€§ã€‚å½“å‰çš„è‡ªé€‚åº”LoRAæ–¹æ³•è¯•å›¾é€šè¿‡åŠ¨æ€æ‰©å±•æˆ–é€‰æ‹©æ€§åˆ†é…ç§©æ¥å…‹æœè¿™ä¸€é™åˆ¶ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºè®¡ç®—å¯†é›†å‹çš„æŠ€æœ¯ï¼Œå¦‚è¿­ä»£ä¿®å‰ªã€ç§©æœç´¢æˆ–é¢å¤–çš„æ­£åˆ™åŒ–ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¨³å®šç§©å¼•å¯¼ä½ç§©é€‚åº”ï¼ˆSR-LoRAï¼‰è¿™ä¸€æ–°æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨é¢„è®­ç»ƒæƒé‡çŸ©é˜µçš„ç¨³å®šç§©ä½œä¸ºé€å±‚åˆ†é…ç§©çš„è‡ªç„¶å…ˆéªŒã€‚é€šè¿‡åˆ©ç”¨ç¨³å®šç§©ï¼ˆåæ˜ æƒé‡çš„å›ºæœ‰ç»´åº¦ï¼‰ï¼ŒSR-LoRAèƒ½å¤Ÿå®ç°å„å±‚ä¹‹é—´æœ‰åºä¸”é«˜æ•ˆçš„ç§©é‡æ–°åˆ†é…ï¼Œæé«˜é€‚åº”æ€§ï¼Œä¸”æ— éœ€é¢å¤–æœç´¢æˆæœ¬ã€‚åœ¨å…·æœ‰æ˜¾è‘—åŸŸå·®å¼‚çš„å°‘é‡ä»»åŠ¡ä¸Šçš„å®è¯è¯„ä¼°è¡¨æ˜ï¼ŒSR-LoRAå§‹ç»ˆä¼˜äºæœ€è¿‘çš„è‡ªé€‚åº”LoRAå˜ä½“ï¼Œåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´å®ç°äº†å‡ºè‰²çš„æƒè¡¡ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ <a target="_blank" rel="noopener" href="https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA">https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA</a> è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00327v1">PDF</a> Accepted by ICCV 2025</p>
<p><strong>Summary</strong></p>
<p>LoRAæ–¹æ³•é€šè¿‡ä½ç§©é€‚åº”å‡å°‘è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒä¸å®Œå…¨å¾®è°ƒçš„åŸºç¡€æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…¶å›ºå®šçš„ä½ç§©ç»“æ„åœ¨å­˜åœ¨æ˜¾è‘—åŸŸå·®è·çš„åœºæ™¯ä¸­é™åˆ¶äº†å…¶é€‚åº”æ€§ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºSR-LoRAæ¡†æ¶ï¼Œåˆ©ç”¨é¢„è®­ç»ƒæƒé‡çŸ©é˜µçš„ç¨³å®šç§©ä½œä¸ºé€å±‚åˆ†é…ç§©çš„è‡ªç„¶å…ˆéªŒã€‚SR-LoRAåœ¨æå‡é€‚åº”æ€§çš„åŒæ—¶ä¸å¢åŠ æœç´¢æˆæœ¬ã€‚åœ¨å…·æœ‰æ˜¾è‘—åŸŸå·®è·çš„å°‘é‡ä»»åŠ¡ä¸Šçš„å®è¯è¯„ä¼°æ˜¾ç¤ºï¼ŒSR-LoRAæŒç»­ä¼˜äºæœ€æ–°çš„è‡ªé€‚åº”LoRAå˜ä½“ï¼Œåœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°ä¼˜è¶Šçš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoRAæ–¹æ³•å‡å°‘äº†è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ç»´æŒäº†é«˜æ€§èƒ½ã€‚</li>
<li>å›ºå®šä½ç§©ç»“æ„åœ¨åŸŸå·®è·å¤§çš„åœºæ™¯ä¸­é™åˆ¶äº†é€‚åº”æ€§ã€‚</li>
<li>å½“å‰è‡ªé€‚åº”LoRAæ–¹æ³•è¯•å›¾é€šè¿‡åŠ¨æ€æ‰©å±•æˆ–é€‰æ‹©æ€§åˆ†é…ç§©æ¥å…‹æœæ­¤é™åˆ¶ï¼Œä½†å¸¸ä¾èµ–è®¡ç®—å¯†é›†çš„æŠ€æœ¯ã€‚</li>
<li>SR-LoRAæ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒæƒé‡çŸ©é˜µçš„ç¨³å®šç§©ä½œä¸ºè‡ªç„¶å…ˆéªŒï¼Œè¿›è¡Œå±‚é—´ç§©çš„åˆ†é…ã€‚</li>
<li>SR-LoRAæå‡äº†é€‚åº”æ€§ä¸”ä¸ä¼šå¢åŠ é¢å¤–çš„æœç´¢æˆæœ¬ã€‚</li>
<li>åœ¨å…·æœ‰æ˜¾è‘—åŸŸå·®è·çš„å°‘é‡ä»»åŠ¡ä¸Šï¼ŒSR-LoRAæ€§èƒ½ä¼˜è¶Šï¼Œè¶…è¿‡å…¶ä»–è‡ªé€‚åº”LoRAå˜ä½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-af1096ad5982420efc77f536d77e873c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-160f11d0b707d5357402e1deb7a61f03.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-db1a3eadec2e2a6c13222631b285724a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9f1920cae8e3c4d2cc94e8c0ba9b610.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f3c2ea30adc7a196f70c288b65fe0419.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1c06900d7a8226b1e4022e379298328.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Explainable-Sentiment-Analysis-with-DeepSeek-R1-Performance-Efficiency-and-Few-Shot-Learning"><a href="#Explainable-Sentiment-Analysis-with-DeepSeek-R1-Performance-Efficiency-and-Few-Shot-Learning" class="headerlink" title="Explainable Sentiment Analysis with DeepSeek-R1: Performance,   Efficiency, and Few-Shot Learning"></a>Explainable Sentiment Analysis with DeepSeek-R1: Performance,   Efficiency, and Few-Shot Learning</h2><p><strong>Authors:Donghao Huang, Zhaoxia Wang</strong></p>
<p>Large language models (LLMs) have transformed sentiment analysis, yet balancing accuracy, efficiency, and explainability remains a critical challenge. This study presents the first comprehensive evaluation of DeepSeek-R1â€“an open-source reasoning modelâ€“against OpenAIâ€™s GPT-4o and GPT-4o-mini. We test the full 671B model and its distilled variants, systematically documenting few-shot learning curves. Our experiments show DeepSeek-R1 achieves a 91.39% F1 score on 5-class sentiment and 99.31% accuracy on binary tasks with just 5 shots, an eightfold improvement in few-shot efficiency over GPT-4o. Architecture-specific distillation effects emerge, where a 32B Qwen2.5-based model outperforms the 70B Llama-based variant by 6.69 percentage points. While its reasoning process reduces throughput, DeepSeek-R1 offers superior explainability via transparent, step-by-step traces, establishing it as a powerful, interpretable open-source alternative. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æ”¹å˜äº†æƒ…æ„Ÿåˆ†æé¢†åŸŸï¼Œä½†åœ¨å¹³è¡¡å‡†ç¡®æ€§ã€æ•ˆç‡å’Œå¯è§£é‡Šæ€§æ–¹é¢ä»ç„¶æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶é¦–æ¬¡å¯¹DeepSeek-R1è¿™ä¸€å¼€æºæ¨ç†æ¨¡å‹è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œå¹¶ä¸OpenAIçš„GPT-4oå’ŒGPT-4o-miniè¿›è¡Œå¯¹æ¯”ã€‚æˆ‘ä»¬æµ‹è¯•äº†å®Œæ•´çš„671Bæ¨¡å‹åŠå…¶è’¸é¦å˜ä½“ï¼Œç³»ç»Ÿåœ°è®°å½•äº†å°æ ·æœ¬å­¦ä¹ æ›²çº¿ã€‚å®éªŒè¡¨æ˜ï¼ŒDeepSeek-R1åœ¨5ç±»æƒ…æ„Ÿåˆ†æä¸Šè¾¾åˆ°91.39%çš„F1åˆ†æ•°ï¼Œåœ¨äºŒå…ƒä»»åŠ¡ä¸Šè¾¾åˆ°99.31%çš„å‡†ç¡®ç‡ï¼Œä»…éœ€è¦5ä¸ªå°æ ·æœ¬ï¼Œç›¸è¾ƒäºGPT-4oï¼Œåœ¨å°æ ·æœ¬æ•ˆç‡ä¸Šæé«˜äº†å…«å€ã€‚å‡ºç°äº†ä¸æ¶æ„ç‰¹å®šçš„è’¸é¦æ•ˆåº”ï¼Œå…¶ä¸­åŸºäº32B Qwen2.5çš„æ¨¡å‹ä¼˜äºåŸºäº70B Llamaçš„å˜ä½“ï¼Œé«˜å‡º6.69ä¸ªç™¾åˆ†ç‚¹ã€‚è™½ç„¶å…¶æ¨ç†è¿‡ç¨‹é™ä½äº†ååé‡ï¼Œä½†DeepSeek-R1é€šè¿‡é€æ˜ã€é€æ­¥çš„è·Ÿè¸ªæä¾›äº†å“è¶Šçš„å¯è§£é‡Šæ€§ï¼Œä½¿å…¶æˆä¸ºå¼ºå¤§ã€å¯è§£é‡Šçš„å¼€æºæ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.11655v2">PDF</a> 10 pages, 2 figures, 6 tables, revised and re-submitted to an IEEE   journal</p>
<p><strong>Summary</strong></p>
<p>DeepSeek-R1æ¨¡å‹åœ¨æƒ…æ„Ÿåˆ†ææ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä¸OpenAIçš„GPT-4oç³»åˆ—æ¨¡å‹ç›¸æ¯”ï¼Œå…¶åœ¨å°‘é‡æ ·æœ¬å­¦ä¹ æ–¹é¢å±•ç°å‡ºæ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚DeepSeek-R1æ¨¡å‹å…·æœ‰ä¼˜ç§€çš„å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”å…¶æ¶æ„ç‰¹å®šçš„è’¸é¦æ•ˆæœåœ¨ä¸åŒæ¨¡å‹ä¸­è¡¨ç°çªå‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1æ¨¡å‹åœ¨æƒ…æ„Ÿåˆ†ææ–¹é¢è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œä¸GPT-4oç³»åˆ—æ¨¡å‹å¯¹æ¯”ï¼Œå±•ç°äº†æ›´é«˜çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>DeepSeek-R1åœ¨å°‘é‡æ ·æœ¬å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œè¾¾åˆ°äº†91.39%çš„F1åˆ†æ•°ï¼ˆäº”ç±»æƒ…æ„Ÿåˆ†æï¼‰å’Œ99.31%çš„å‡†ç¡®ç‡ï¼ˆäºŒå…ƒä»»åŠ¡ï¼‰ã€‚</li>
<li>ä¸GPT-4oç›¸æ¯”ï¼ŒDeepSeek-R1åœ¨å°‘æ•°é•œå¤´æ•ˆç‡ä¸Šå®ç°äº†å…«å€çš„æå‡ã€‚</li>
<li>DeepSeek-R1æ¨¡å‹çš„æ¶æ„ç‰¹å®šè’¸é¦æ•ˆæœçªå‡ºï¼Œå…¶ä¸­åŸºäº32B Qwen2.5çš„æ¨¡å‹åœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†åŸºäº70B Llamaçš„æ¨¡å‹ã€‚</li>
<li>è™½ç„¶DeepSeek-R1çš„æ¨ç†è¿‡ç¨‹å¯èƒ½ä¼šå½±å“ååé‡ï¼Œä½†å…¶é€æ˜çš„é€æ­¥è·Ÿè¸ªä¸ºå…¶æä¾›äº†å“è¶Šçš„å¯è§£é‡Šæ€§ã€‚</li>
<li>DeepSeek-R1æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ã€å¯è§£é‡Šçš„å¼€æºæ›¿ä»£æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.11655">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-32292bb4eecfa90632f854b425622501.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-613c3a4b60e871bbd0ac61434c4b3b82.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-898d268a6ecc8dce6235e14cabeae891.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c2d36b18dcfcb2419b65c77b8f04e3ce.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d0ac305642d0911de9a8518f5b44cc9.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Exploring-Intrinsic-Normal-Prototypes-within-a-Single-Image-for-Universal-Anomaly-Detection"><a href="#Exploring-Intrinsic-Normal-Prototypes-within-a-Single-Image-for-Universal-Anomaly-Detection" class="headerlink" title="Exploring Intrinsic Normal Prototypes within a Single Image for   Universal Anomaly Detection"></a>Exploring Intrinsic Normal Prototypes within a Single Image for   Universal Anomaly Detection</h2><p><strong>Authors:Wei Luo, Yunkang Cao, Haiming Yao, Xiaotian Zhang, Jianan Lou, Yuqi Cheng, Weiming Shen, Wenyong Yu</strong></p>
<p>Anomaly detection (AD) is essential for industrial inspection, yet existing methods typically rely on &#96;&#96;comparingâ€™â€™ test images to normal references from a training set. However, variations in appearance and positioning often complicate the alignment of these references with the test image, limiting detection accuracy. We observe that most anomalies manifest as local variations, meaning that even within anomalous images, valuable normal information remains. We argue that this information is useful and may be more aligned with the anomalies since both the anomalies and the normal information originate from the same image. Therefore, rather than relying on external normality from the training set, we propose INP-Former, a novel method that extracts Intrinsic Normal Prototypes (INPs) directly from the test image. Specifically, we introduce the INP Extractor, which linearly combines normal tokens to represent INPs. We further propose an INP Coherence Loss to ensure INPs can faithfully represent normality for the testing image. These INPs then guide the INP-Guided Decoder to reconstruct only normal tokens, with reconstruction errors serving as anomaly scores. Additionally, we propose a Soft Mining Loss to prioritize hard-to-optimize samples during training. INP-Former achieves state-of-the-art performance in single-class, multi-class, and few-shot AD tasks across MVTec-AD, VisA, and Real-IAD, positioning it as a versatile and universal solution for AD. Remarkably, INP-Former also demonstrates some zero-shot AD capability. Code is available at:<a target="_blank" rel="noopener" href="https://github.com/luow23/INP-Former">https://github.com/luow23/INP-Former</a>. </p>
<blockquote>
<p>å¼‚å¸¸æ£€æµ‹ï¼ˆADï¼‰åœ¨å·¥ä¸šæ£€æµ‹ä¸­è‡³å…³é‡è¦ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸ä¾èµ–äºå°†æµ‹è¯•å›¾åƒä¸è®­ç»ƒé›†ä¸­çš„æ­£å¸¸å‚è€ƒå›¾åƒè¿›è¡Œâ€œæ¯”è¾ƒâ€ã€‚ç„¶è€Œï¼Œå¤–è§‚å’Œä½ç½®çš„å˜åŒ–ç»å¸¸ä½¿è¿™äº›å‚è€ƒå›¾åƒä¸æµ‹è¯•å›¾åƒçš„å¯¹é½å˜å¾—å¤æ‚ï¼Œä»è€Œé™åˆ¶äº†æ£€æµ‹ç²¾åº¦ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå¤§å¤šæ•°å¼‚å¸¸è¡¨ç°ä¸ºå±€éƒ¨å˜åŒ–ï¼Œè¿™æ„å‘³ç€å³ä½¿åœ¨å¼‚å¸¸å›¾åƒå†…éƒ¨ï¼Œä»ç„¶æœ‰å®è´µçš„æ­£å¸¸ä¿¡æ¯ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™äº›ä¿¡æ¯æ˜¯æœ‰ç”¨çš„ï¼Œå¹¶ä¸”å¯èƒ½ä¸å¼‚å¸¸æ›´å¯¹é½ï¼Œå› ä¸ºå¼‚å¸¸å’Œæ­£å¸¸ä¿¡æ¯éƒ½æ¥è‡ªåŒä¸€å›¾åƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•INP-Formerï¼Œå®ƒç›´æ¥ä»æµ‹è¯•å›¾åƒä¸­æå–å†…åœ¨æ­£å¸¸åŸå‹ï¼ˆINPsï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†INPæå–å™¨ï¼Œå®ƒçº¿æ€§ç»„åˆæ­£å¸¸ä»¤ç‰Œæ¥è¡¨ç¤ºINPsã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§INPä¸€è‡´æ€§æŸå¤±ï¼Œä»¥ç¡®ä¿INPsèƒ½å¤Ÿå¿ å®åœ°ä»£è¡¨æµ‹è¯•å›¾åƒçš„æ­£å¸¸æ€§ã€‚è¿™äº›INPsç„¶åå¼•å¯¼INPå¼•å¯¼è§£ç å™¨ä»…é‡å»ºæ­£å¸¸ä»¤ç‰Œï¼Œé‡å»ºè¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è½¯æŒ–æ˜æŸå¤±ï¼Œä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜å…ˆå¤„ç†éš¾ä»¥ä¼˜åŒ–çš„æ ·æœ¬ã€‚INP-Formeråœ¨MVTec-ADã€VisAå’ŒReal-IADä¸Šçš„å•ç±»ã€å¤šç±»å’Œå°‘æ ·æœ¬ADä»»åŠ¡ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½¿å…¶æˆä¸ºADçš„é€šç”¨è§£å†³æ–¹æ¡ˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒINP-Formerè¿˜è¡¨ç°å‡ºä¸€äº›é›¶æ ·æœ¬ADèƒ½åŠ›ã€‚ä»£ç å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/luow23/INP-Former%E3%80%82">https://github.com/luow23/INP-Formerã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.02424v2">PDF</a> Accepted by CVPR2025</p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºINP-Formerçš„æ–°å‹å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œå®ƒç›´æ¥ä»æµ‹è¯•å›¾åƒä¸­æå–å†…åœ¨æ­£å¸¸åŸå‹ï¼ˆINPsï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥INPæå–å™¨å’ŒINPå¼•å¯¼è§£ç å™¨ï¼Œåˆ©ç”¨æ­£å¸¸ä»¤ç‰Œçš„é‡æ„è¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°ï¼Œå®ç°æ— éœ€ä¾èµ–å¤–éƒ¨æ­£å¸¸å‚è€ƒçš„å¼‚å¸¸æ£€æµ‹ã€‚æ­¤å¤–ï¼Œè¿˜å¼•å…¥äº†INPä¸€è‡´æ€§æŸå¤±å’Œè½¯æŒ–æ˜æŸå¤±ï¼Œä»¥æé«˜æ£€æµ‹æ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ã€‚è¯¥æ–¹æ³•åœ¨MVTec-ADã€VisAå’ŒReal-IADç­‰å¤šä¸ªæ•°æ®é›†ä¸Šçš„å•ç±»ã€å¤šç±»å’Œå°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œå¹¶å…·æœ‰é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¼ ç»Ÿå¼‚å¸¸æ£€æµ‹æ–¹æ³•é€šå¸¸ä¾èµ–äºä¸è®­ç»ƒé›†ä¸­çš„æ­£å¸¸å‚è€ƒå›¾åƒè¿›è¡Œæ¯”è¾ƒï¼Œä½†è¿™ç§æ–¹æ³•å—åˆ°å¤–è§‚å’Œä½ç½®å˜åŒ–çš„å½±å“ï¼Œé™åˆ¶äº†æ£€æµ‹ç²¾åº¦ã€‚</li>
<li>å¤§å¤šæ•°å¼‚å¸¸è¡¨ç°ä¸ºå±€éƒ¨å˜åŒ–ï¼Œæµ‹è¯•å›¾åƒä¸­çš„å¼‚å¸¸åŒºåŸŸå†…ä»åŒ…å«æœ‰ä»·å€¼çš„æ­£å¸¸ä¿¡æ¯ã€‚</li>
<li>INP-Formeræ–¹æ³•ç›´æ¥ä»æµ‹è¯•å›¾åƒä¸­æå–å†…åœ¨æ­£å¸¸åŸå‹ï¼ˆINPsï¼‰ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨æ­£å¸¸å‚è€ƒã€‚</li>
<li>é€šè¿‡å¼•å…¥INPæå–å™¨å’ŒINPå¼•å¯¼è§£ç å™¨ï¼Œåˆ©ç”¨æ­£å¸¸ä»¤ç‰Œçš„é‡æ„è¯¯å·®ä½œä¸ºå¼‚å¸¸åˆ†æ•°è¿›è¡Œå¼‚å¸¸æ£€æµ‹ã€‚</li>
<li>INPä¸€è‡´æ€§æŸå¤±ç¡®ä¿INPsèƒ½å¿ å®ä»£è¡¨æµ‹è¯•å›¾åƒçš„æ­£å¸¸æ€§ã€‚</li>
<li>è½¯æŒ–æ˜æŸå¤±åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜å…ˆå¤„ç†éš¾ä»¥ä¼˜åŒ–çš„æ ·æœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.02424">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-871544559dfb1b1f122308ecbbc33bff.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c3dce207a988575dd0c6f8bd553cb24.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b0d66cd38e7b96ad71cad75a6e0ac73f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6dc312d90adb98e1fde2460620e20cae.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c6ed3a369041d8e5a7163cbbf796bd97.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d9d95502fc508f32db594f44692b261.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Mamba-FSCIL-Dynamic-Adaptation-with-Selective-State-Space-Model-for-Few-Shot-Class-Incremental-Learning"><a href="#Mamba-FSCIL-Dynamic-Adaptation-with-Selective-State-Space-Model-for-Few-Shot-Class-Incremental-Learning" class="headerlink" title="Mamba-FSCIL: Dynamic Adaptation with Selective State Space Model for   Few-Shot Class-Incremental Learning"></a>Mamba-FSCIL: Dynamic Adaptation with Selective State Space Model for   Few-Shot Class-Incremental Learning</h2><p><strong>Authors:Xiaojie Li, Yibo Yang, Jianlong Wu, Yue Yu, Ming-Hsuan Yang, Liqiang Nie, Min Zhang</strong></p>
<p>Few-shot class-incremental learning (FSCIL) aims to incrementally learn novel classes from limited examples while preserving knowledge of previously learned classes. Existing methods face a critical dilemma: static architectures rely on a fixed parameter space to learn from data that arrive sequentially, prone to overfitting to the current session, while dynamic architectures require the expansion of the parameter space continually, leading to increased complexity. In this study, we explore the potential of Selective State Space Models (SSMs) for FSCIL. Mamba leverages its input-dependent parameters to dynamically adjust its processing patterns and generate content-aware scan patterns within a fixed architecture. This enables it to configure distinct processing for base and novel classes, effectively preserving existing knowledge while adapting to new ones. To leverage Mambaâ€™s potential for FSCIL, we design two key modules: First, we propose a dual selective SSM projector that dynamically adjusts the projection parameters based on the intermediate features for dynamic adaptation. The dual-design structurally decouples base and novel class processing with a frozen base branch, employing a frozen base branch to maintain robust base-class features and a dynamic incremental branch that adaptively learns distinctive feature shifts for novel classes. Second, we develop a class-sensitive selective scan mechanism to guide dynamic adaptation of the incremental branch. It minimizes the disruption to base-class representations caused by training on novel data, and meanwhile, forces the selective scan to perform in distinct patterns between base and novel classes. Extensive experiments on miniImageNet, CUB-200, and CIFAR-100 demonstrate that Mamba-FSCIL achieves state-of-the-art performance. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xiaojieli0903/Mamba-FSCIL">https://github.com/xiaojieli0903/Mamba-FSCIL</a>. </p>
<blockquote>
<p>å°‘é‡ç±»åˆ«å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰æ—¨åœ¨ä»æœ‰é™çš„æ ·æœ¬ä¸­é€æ­¥å­¦ä¹ æ–°çš„ç±»åˆ«ï¼ŒåŒæ—¶ä¿ç•™å¯¹å…ˆå‰å­¦ä¹ ç±»åˆ«çš„çŸ¥è¯†ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´ä¸€ä¸ªå…³é”®å›°å¢ƒï¼šé™æ€æ¶æ„ä¾èµ–äºå›ºå®šçš„å‚æ•°ç©ºé—´æ¥å­¦ä¹ æŒ‰é¡ºåºåˆ°è¾¾çš„æ•°æ®ï¼Œå®¹æ˜“å¯¹å½“å‰ä¼šè¯å‡ºç°è¿‡æ‹Ÿåˆï¼Œè€ŒåŠ¨æ€æ¶æ„åˆ™éœ€è¦ä¸æ–­åœ°æ‰©å±•å‚æ•°ç©ºé—´ï¼Œå¯¼è‡´å¤æ‚æ€§å¢åŠ ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨FSCILä¸­çš„æ½œåŠ›ã€‚Mambaåˆ©ç”¨å…¶è¾“å…¥ç›¸å…³çš„å‚æ•°æ¥åŠ¨æ€è°ƒæ•´å…¶å¤„ç†æ¨¡å¼ï¼Œå¹¶åœ¨å›ºå®šæ¶æ„å†…ç”Ÿæˆå†…å®¹æ„ŸçŸ¥çš„æ‰«ææ¨¡å¼ã€‚è¿™ä½¿å…¶èƒ½å¤Ÿä¸ºåŸºæœ¬ç±»åˆ«å’Œæ–°ç±»åˆ«é…ç½®ä¸åŒçš„å¤„ç†è¿‡ç¨‹ï¼Œæœ‰æ•ˆåœ°ä¿ç•™ç°æœ‰çŸ¥è¯†å¹¶é€‚åº”æ–°ç±»åˆ«ã€‚ä¸ºäº†åˆ©ç”¨Mambaåœ¨FSCILä¸­çš„æ½œåŠ›ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒé€‰æ‹©æ€§SSMæŠ•å½±ä»ªï¼Œå®ƒæ ¹æ®ä¸­é—´ç‰¹å¾åŠ¨æ€è°ƒæ•´æŠ•å½±å‚æ•°ï¼Œä»¥å®ç°åŠ¨æ€é€‚åº”ã€‚è¯¥åŒè®¾è®¡åœ¨ç»“æ„ä¸Šå°†åŸºæœ¬ç±»åˆ«å’Œæ–°ç±»åˆ«çš„å¤„ç†è¿‡ç¨‹è§£è€¦ï¼Œé‡‡ç”¨å†»ç»“çš„åŸºæœ¬åˆ†æ”¯æ¥ä¿æŒç¨³å¥çš„åŸºæœ¬ç±»åˆ«ç‰¹å¾ï¼Œä»¥åŠä¸€ä¸ªåŠ¨æ€å¢é‡åˆ†æ”¯ï¼Œè¯¥åˆ†æ”¯è‡ªé€‚åº”åœ°å­¦ä¹ æ–°ç±»åˆ«çš„ç‰¹å¾å˜åŒ–ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§ç±»åˆ«æ•æ„Ÿçš„é€‰æ‹©æ€§æ‰«ææœºåˆ¶ï¼Œä»¥æŒ‡å¯¼å¢é‡åˆ†æ”¯çš„åŠ¨æ€é€‚åº”ã€‚å®ƒå°½é‡å‡å°‘å¯¹åŸºç¡€ç±»åˆ«è¡¨ç¤ºè¿›è¡Œæ–°æ•°æ®è®­ç»ƒæ—¶çš„å¹²æ‰°ï¼ŒåŒæ—¶è¿«ä½¿é€‰æ‹©æ€§æ‰«æåœ¨åŸºç¡€ç±»åˆ«å’Œæ–°ç±»åˆ«ä¹‹é—´ä»¥ä¸åŒçš„æ¨¡å¼è¿›è¡Œã€‚åœ¨miniImageNetã€CUB-200å’ŒCIFAR-100ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒMamba-FSCILè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xiaojieli0903/Mamba-FSCIL">https://github.com/xiaojieli0903/Mamba-FSCIL</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.06136v3">PDF</a> Code: <a target="_blank" rel="noopener" href="https://github.com/xiaojieli0903/Mamba-FSCIL">https://github.com/xiaojieli0903/Mamba-FSCIL</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†Few-Shotç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰é¢†åŸŸçš„ä¸€ä¸ªæ–°é—®é¢˜ã€‚ç°æœ‰æ–¹æ³•é¢ä¸´é™æ€æ¶æ„å’ŒåŠ¨æ€æ¶æ„ä¹‹é—´çš„ä¸¤éš¾é€‰æ‹©ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨FSCILä¸­çš„æ½œåŠ›ã€‚Mambaåœ¨å…¶å›ºå®šæ¶æ„å†…åˆ©ç”¨è¾“å…¥ç›¸å…³å‚æ•°åŠ¨æ€è°ƒæ•´å¤„ç†æ¨¡å¼ï¼Œç”Ÿæˆå†…å®¹æ„ŸçŸ¥æ‰«ææ¨¡å¼ï¼Œæœ‰æ•ˆä¿ç•™ç°æœ‰çŸ¥è¯†å¹¶é€‚åº”æ–°ç±»åˆ«ã€‚ä¸ºå®ç°Mambaåœ¨FSCILä¸­çš„æ½œåŠ›ï¼Œè®¾è®¡äº†ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šåŒé€‰æ‹©æ€§SSMæŠ•å½±ä»ªå’Œç±»æ•æ„Ÿé€‰æ‹©æ€§æ‰«ææœºåˆ¶ã€‚è¿™ä¸¤ä¸ªæ¨¡å—ä½¿Mambaèƒ½å¤Ÿåœ¨é™æ€æ¶æ„å†…å®ç°åŠ¨æ€é€‚åº”ï¼Œä»è€Œåœ¨æœ‰é™çš„æ ·æœ¬ä¸­å®ç°ç±»çš„å¢é‡å­¦ä¹ ï¼Œå¹¶è¾¾åˆ°å…ˆè¿›æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Few-Shotç±»å¢é‡å­¦ä¹ ï¼ˆFSCILï¼‰çš„ç›®æ ‡æ˜¯å®ç°åœ¨æœ‰é™çš„ç¤ºä¾‹ä¸‹å¯¹æ–°ç±»åˆ«çš„å¢é‡å­¦ä¹ ï¼ŒåŒæ—¶ä¿ç•™å¯¹å…ˆå‰å­¦ä¹ ç±»åˆ«çš„çŸ¥è¯†ã€‚</li>
<li>ç°æœ‰æ–¹æ³•é¢ä¸´é™æ€å’ŒåŠ¨æ€æ¶æ„ä¹‹é—´çš„æƒè¡¡ï¼Œå‰è€…æ˜“è¿‡æ‹Ÿåˆå½“å‰æ•°æ®ï¼Œåè€…å‚æ•°ç©ºé—´æŒç»­æ‰©å±•å¯¼è‡´å¤æ‚æ€§å¢åŠ ã€‚</li>
<li>Mambaåˆ©ç”¨é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆSSMsï¼‰åœ¨å›ºå®šæ¶æ„å†…å®ç°åŠ¨æ€å¤„ç†æ¨¡å¼è°ƒæ•´ï¼Œç”Ÿæˆå†…å®¹æ„ŸçŸ¥æ‰«ææ¨¡å¼ã€‚</li>
<li>Mambaé€šè¿‡åŒé€‰æ‹©æ€§SSMæŠ•å½±ä»ªå®ç°åŸºç±»å’Œæ–°é¢–ç±»çš„å¤„ç†è§£è€¦ï¼Œé€šè¿‡å†»ç»“åŸºåˆ†æ”¯æ¥ä¿æŒç¨³å¥çš„åŸºç±»ç‰¹å¾ï¼Œå¹¶é€šè¿‡åŠ¨æ€å¢é‡åˆ†æ”¯è‡ªé€‚åº”å­¦ä¹ æ–°é¢–ç±»çš„ç‰¹å¾å˜åŒ–ã€‚</li>
<li>ç±»æ•æ„Ÿé€‰æ‹©æ€§æ‰«ææœºåˆ¶å¼•å¯¼å¢é‡åˆ†æ”¯çš„åŠ¨æ€é€‚åº”ï¼Œæœ€å°åŒ–å¯¹åŸºç±»è¡¨ç¤ºçš„æ–°æ•°æ®è®­ç»ƒçš„å¹²æ‰°ï¼Œå¹¶å¼ºåˆ¶é€‰æ‹©æ€§æ‰«æåœ¨åŸºç±»å’Œæ–°é¢–ç±»ä¹‹é—´æ‰§è¡Œä¸åŒçš„æ¨¡å¼ã€‚</li>
<li>Mamba-FSCILåœ¨miniImageNetã€CUB-200å’ŒCIFAR-100ä¸Šçš„å®éªŒè¡¨æ˜å…¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.06136">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-d550b8c7fa153750ede20d7f027b509d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d61f03d549c9e77187a8e3fc95f3439.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc3116290bcff7bc3600550ca12773e6.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="CPT-Competence-progressive-Training-Strategy-for-Few-shot-Node-Classification"><a href="#CPT-Competence-progressive-Training-Strategy-for-Few-shot-Node-Classification" class="headerlink" title="CPT: Competence-progressive Training Strategy for Few-shot Node   Classification"></a>CPT: Competence-progressive Training Strategy for Few-shot Node   Classification</h2><p><strong>Authors:Qilong Yan, Yufeng Zhang, Jinghao Zhang, Jingpu Duan, Jian Yin</strong></p>
<p>Graph Neural Networks (GNNs) have made significant advancements in node classification, but their success relies on sufficient labeled nodes per class in the training data. Real-world graph data often exhibits a long-tail distribution with sparse labels, emphasizing the importance of GNNsâ€™ ability in few-shot node classification, which entails categorizing nodes with limited data. Traditional episodic meta-learning approaches have shown promise in this domain, but they face an inherent limitation: it might lead the model to converge to suboptimal solutions because of random and uniform task assignment, ignoring task difficulty levels. This could lead the meta-learner to face complex tasks too soon, hindering proper learning. Ideally, the meta-learner should start with simple concepts and advance to more complex ones, like human learning. So, we introduce CPT, a novel two-stage curriculum learning method that aligns task difficulty with the meta-learnerâ€™s progressive competence, enhancing overall performance. Specifically, in CPTâ€™s initial stage, the focus is on simpler tasks, fostering foundational skills for engaging with complex tasks later. Importantly, the second stage dynamically adjusts task difficulty based on the meta-learnerâ€™s growing competence, aiming for optimal knowledge acquisition. Extensive experiments on popular node classification datasets demonstrate significant improvements of our strategy over existing methods. </p>
<blockquote>
<p>å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰åœ¨èŠ‚ç‚¹åˆ†ç±»æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†å…¶æˆåŠŸä¾èµ–äºè®­ç»ƒæ•°æ®ä¸­æ¯ç±»çš„è¶³å¤Ÿæ ‡ç­¾èŠ‚ç‚¹ã€‚ç°å®ä¸–ç•Œçš„å›¾æ•°æ®ç»å¸¸è¡¨ç°å‡ºé•¿å°¾åˆ†å¸ƒå’Œç¨€ç–æ ‡ç­¾ï¼Œè¿™å¼ºè°ƒäº†åœ¨æœ‰é™æ•°æ®ä¸‹è¿›è¡ŒèŠ‚ç‚¹åˆ†ç±»çš„GNNèƒ½åŠ›çš„é‡è¦æ€§ã€‚ä¼ ç»Ÿçš„å‘¨æœŸå¼å…ƒå­¦ä¹ æ–¹æ³•åœ¨è¿™ä¸ªé¢†åŸŸæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬é¢ä¸´ä¸€ä¸ªå†…åœ¨å±€é™ï¼šç”±äºéšæœºå’Œç»Ÿä¸€çš„ä»»åŠ¡åˆ†é…ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹æ”¶æ•›åˆ°æ¬¡ä¼˜è§£ï¼Œå¿½ç•¥ä»»åŠ¡éš¾åº¦çº§åˆ«ã€‚è¿™å¯èƒ½å¯¼è‡´å…ƒå­¦ä¹ è€…è¿‡æ—©åœ°é¢å¯¹å¤æ‚ä»»åŠ¡ï¼Œé˜»ç¢é€‚å½“çš„å­¦ä¹ ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œå…ƒå­¦ä¹ è€…åº”è¯¥ä»ç®€å•çš„æ¦‚å¿µå¼€å§‹ï¼Œé€æ­¥å­¦ä¹ æ›´å¤æ‚çš„æ¦‚å¿µï¼Œå°±åƒäººç±»å­¦ä¹ ä¸€æ ·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†CPTï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„ä¸¤é˜¶æ®µè¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œå®ƒæ ¹æ®å…ƒå­¦ä¹ è€…çš„è¿›æ­¥èƒ½åŠ›æ¥åŒ¹é…ä»»åŠ¡éš¾åº¦ï¼Œä»è€Œæé«˜æ•´ä½“æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨CPTçš„åˆå§‹é˜¶æ®µï¼Œé‡ç‚¹æ˜¯æ›´ç®€å•çš„ä»»åŠ¡ï¼Œä¸ºæ—¥åå¤„ç†å¤æ‚ä»»åŠ¡åŸ¹å…»åŸºç¡€æŠ€èƒ½ã€‚é‡è¦çš„æ˜¯ï¼Œç¬¬äºŒé˜¶æ®µæ ¹æ®å…ƒå­¦ä¹ è€…ä¸æ–­å¢é•¿çš„èƒ½åŠ›åŠ¨æ€è°ƒæ•´ä»»åŠ¡éš¾åº¦ï¼Œä»¥å®ç°æœ€ä½³çŸ¥è¯†è·å–ã€‚åœ¨æµè¡Œçš„èŠ‚ç‚¹åˆ†ç±»æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç­–ç•¥å¤§å¤§ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2402.00450v5">PDF</a> APWEB-WAIM 2025</p>
<p><strong>Summary</strong></p>
<p>GNNåœ¨èŠ‚ç‚¹åˆ†ç±»ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨è®­ç»ƒæ•°æ®ä¸­å¯¹æ¯ä¸ªç±»åˆ«æ ‡æ³¨èŠ‚ç‚¹çš„æ•°é‡æœ‰è¾ƒé«˜è¦æ±‚ã€‚ç°å®ä¸–ç•Œçš„å›¾æ•°æ®å¸¸å‘ˆç°é•¿å°¾åˆ†å¸ƒï¼Œæ ‡ç­¾ç¨€ç–ï¼Œå¼ºè°ƒäº†åœ¨æœ‰é™æ•°æ®ä¸‹GNNè¿›è¡Œå°‘æ ·æœ¬èŠ‚ç‚¹åˆ†ç±»çš„é‡è¦æ€§ã€‚ä¼ ç»Ÿçš„æ—¶é—´ç»å†å…ƒå­¦ä¹ æ–¹æ³•åœ¨æ­¤é¢†åŸŸæ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å­˜åœ¨å›ºæœ‰å±€é™ï¼šéšæœºå’Œå‡åŒ€çš„ä»»åŠ¡åˆ†é…å¯èƒ½å¯¼è‡´æ¨¡å‹æ”¶æ•›åˆ°æ¬¡ä¼˜è§£ï¼Œå¿½è§†ä»»åŠ¡éš¾åº¦çº§åˆ«ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥CPTï¼Œä¸€ç§æ–°å‹ä¸¤é˜¶æ®µè¯¾ç¨‹å­¦ä¹ æ–¹æ³•ï¼Œæ ¹æ®å…ƒå­¦ä¹ è€…çš„è¿›æ­¥èƒ½åŠ›è°ƒæ•´ä»»åŠ¡éš¾åº¦ï¼Œæé«˜æ€»ä½“æ€§èƒ½ã€‚åˆå§‹é˜¶æ®µä¸“æ³¨äºç®€å•ä»»åŠ¡ï¼ŒåŸ¹å…»åŸºç¡€æŠ€èƒ½ï¼›ç¬¬äºŒé˜¶æ®µæ ¹æ®å…ƒå­¦ä¹ è€…çš„æˆé•¿èƒ½åŠ›åŠ¨æ€è°ƒæ•´ä»»åŠ¡éš¾åº¦ï¼Œå®ç°æœ€ä½³çŸ¥è¯†è·å–ã€‚åœ¨æµè¡Œçš„èŠ‚ç‚¹åˆ†ç±»æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç­–ç•¥ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GNNåœ¨èŠ‚ç‚¹åˆ†ç±»ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä½†éœ€è¦å……è¶³æ ‡æ³¨èŠ‚ç‚¹æ•°æ®ã€‚</li>
<li>ç°å®ä¸–ç•Œçš„å›¾æ•°æ®å¸¸å‘ˆç°æ ‡ç­¾ç¨€ç–å’Œé•¿å°¾åˆ†å¸ƒçš„ç‰¹ç‚¹ã€‚</li>
<li>ä¼ ç»Ÿå…ƒå­¦ä¹ æ–¹æ³•åœ¨ä»»åŠ¡åˆ†é…ä¸Šå¯èƒ½å­˜åœ¨ç¼ºé™·ï¼Œå¿½è§†ä»»åŠ¡éš¾åº¦ã€‚</li>
<li>å¼•å…¥CPTï¼Œä¸€ç§ç»“åˆè¯¾ç¨‹å­¦ä¹ å’Œå…ƒå­¦ä¹ çš„ä¸¤é˜¶æ®µæ–¹æ³•ã€‚</li>
<li>åˆå§‹é˜¶æ®µä¸“æ³¨äºç®€å•ä»»åŠ¡ï¼ŒåŸ¹å…»åŸºç¡€æŠ€èƒ½ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µæ ¹æ®å…ƒå­¦ä¹ è€…çš„æˆé•¿èƒ½åŠ›åŠ¨æ€è°ƒæ•´ä»»åŠ¡éš¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2402.00450">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6a7c978f333288a7b1436180312f1728.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-00b034bf14afa137aca64e990c113f78.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3f7b6786f3f41416f7e3496f20dfb5bc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f01d633fabd66337b207bb8ccf0061a2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b7d191d099102529e28972293021ba6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="D-2-ST-Adapter-Disentangled-and-Deformable-Spatio-Temporal-Adapter-for-Few-shot-Action-Recognition"><a href="#D-2-ST-Adapter-Disentangled-and-Deformable-Spatio-Temporal-Adapter-for-Few-shot-Action-Recognition" class="headerlink" title="D$^2$ST-Adapter: Disentangled-and-Deformable Spatio-Temporal Adapter for   Few-shot Action Recognition"></a>D$^2$ST-Adapter: Disentangled-and-Deformable Spatio-Temporal Adapter for   Few-shot Action Recognition</h2><p><strong>Authors:Wenjie Pei, Qizhong Tan, Guangming Lu, Jiandong Tian, Jun Yu</strong></p>
<p>Adapting pre-trained image models to video modality has proven to be an effective strategy for robust few-shot action recognition. In this work, we explore the potential of adapter tuning in image-to-video model adaptation and propose a novel video adapter tuning framework, called Disentangled-and-Deformable Spatio-Temporal Adapter (D$^2$ST-Adapter). It features a lightweight design, low adaptation overhead and powerful spatio-temporal feature adaptation capabilities. D$^2$ST-Adapter is structured with an internal dual-pathway architecture that enables built-in disentangled encoding of spatial and temporal features within the adapter, seamlessly integrating into the single-stream feature learning framework of pre-trained image models. In particular, we develop an efficient yet effective implementation of the D$^2$ST-Adapter, incorporating the specially devised anisotropic Deformable Spatio-Temporal Attention as its pivotal operation. This mechanism can be individually tailored for two pathways with anisotropic sampling densities along the spatial and temporal domains in 3D spatio-temporal space, enabling disentangled encoding of spatial and temporal features while maintaining a lightweight design. Extensive experiments by instantiating our method on both pre-trained ResNet and ViT demonstrate the superiority of our method over state-of-the-art methods. Our method is particularly well-suited to challenging scenarios where temporal dynamics are critical for action recognition. Code is available at <a target="_blank" rel="noopener" href="https://github.com/qizhongtan/D2ST-Adapter">https://github.com/qizhongtan/D2ST-Adapter</a>. </p>
<blockquote>
<p>å°†é¢„è®­ç»ƒå›¾åƒæ¨¡å‹é€‚é…åˆ°è§†é¢‘æ¨¡æ€å·²ç»è¢«è¯æ˜æ˜¯å®ç°åœ¨å°‘æ•°æ ·æœ¬ä¸Šè¿›è¡Œç¨³å¥åŠ¨ä½œè¯†åˆ«çš„ä¸€ä¸ªæœ‰æ•ˆç­–ç•¥ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†é€‚é…å™¨è°ƒæ•´åœ¨å›¾åƒåˆ°è§†é¢‘æ¨¡å‹é€‚é…ä¸­çš„æ½œåŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ç§åä¸ºâ€œè§£çº ç¼ å’Œå¯å˜æ—¶ç©ºé€‚é…å™¨ï¼ˆD^2ST-Adapterï¼‰â€çš„æ–°å‹è§†é¢‘é€‚é…å™¨è°ƒæ•´æ¡†æ¶ã€‚å®ƒå…·æœ‰è½»é‡çº§è®¾è®¡ã€ä½é€‚é…å¼€é”€å’Œå¼ºå¤§çš„æ—¶ç©ºç‰¹å¾é€‚é…èƒ½åŠ›ã€‚D^2ST-Adapteré‡‡ç”¨å†…éƒ¨åŒè·¯å¾„æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨é€‚é…å™¨å†…éƒ¨å¯¹ç©ºé—´å’Œæ—¶é—´ç‰¹å¾è¿›è¡Œå†…ç½®è§£çº ç¼ ç¼–ç ï¼Œæ— ç¼é›†æˆåˆ°é¢„è®­ç»ƒå›¾åƒæ¨¡å‹çš„å•ä¸€æµç‰¹å¾å­¦ä¹ æ¡†æ¶ä¸­ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬å¯¹D^2ST-Adapterè¿›è¡Œäº†é«˜æ•ˆè€Œæœ‰æ•ˆçš„å®ç°ï¼Œå°†ä¸“é—¨è®¾è®¡çš„å„å‘å¼‚æ€§å¯å˜æ—¶ç©ºæ³¨æ„åŠ›ä½œä¸ºå…¶å…³é”®æ“ä½œã€‚è¿™ç§æœºåˆ¶å¯ä»¥é’ˆå¯¹ä¸¤æ¡è·¯å¾„è¿›è¡Œä¸ªæ€§åŒ–è®¾ç½®ï¼Œåœ¨3Dæ—¶ç©ºç©ºé—´çš„æ—¶ç©ºåŸŸä¸Šå…·æœ‰å„å‘å¼‚æ€§çš„é‡‡æ ·å¯†åº¦ï¼Œä»è€Œåœ¨ä¿æŒè½»é‡çº§è®¾è®¡çš„åŒæ—¶å®ç°ç©ºé—´å’Œæ—¶é—´ç‰¹å¾çš„è§£çº ç¼ ç¼–ç ã€‚åœ¨é¢„è®­ç»ƒçš„ResNetå’ŒViTä¸Šè¿›è¡Œå®ä¾‹åŒ–å®éªŒçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºé‚£äº›æ—¶é—´åŠ¨æ€å¯¹åŠ¨ä½œè¯†åˆ«è‡³å…³é‡è¦çš„å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ã€‚ä»£ç å¯åœ¨[<a target="_blank" rel="noopener" href="https://github.com/qizhongtan/D2ST-Adapter%E6%89%BE%E5%88%B0%E3%80%82]">https://github.com/qizhongtan/D2ST-Adapteræ‰¾åˆ°ã€‚]</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.01431v4">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†é€‚é…å™¨è°ƒæ•´åœ¨å›¾åƒåˆ°è§†é¢‘æ¨¡å‹é€‚åº”ä¸­çš„æ½œåŠ›ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘é€‚é…å™¨è°ƒæ•´æ¡†æ¶D$^2$ST-Adapterã€‚å®ƒå…·æœ‰è½»é‡çº§è®¾è®¡ã€ä½é€‚åº”å¼€é”€å’Œå¼ºå¤§çš„æ—¶ç©ºç‰¹å¾é€‚åº”åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡å†…éƒ¨åŒè·¯å¾„æ¶æ„å®ç°ç©ºé—´å’Œæ—¶é—´ç‰¹å¾çš„å†…åœ¨åˆ†ç¦»ç¼–ç ï¼Œæ— ç¼é›†æˆåˆ°é¢„è®­ç»ƒå›¾åƒæ¨¡å‹çš„å•æµç‰¹å¾å­¦ä¹ æ¡†æ¶ä¸­ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é«˜æ•ˆä¸”æœ‰æ•ˆçš„D$^2$ST-Adapterå®ç°ï¼Œå…¶ä¸­å¼•å…¥äº†ç‰¹æ®Šçš„å¯å˜å½¢æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶ä½œä¸ºå…¶å…³é”®æ“ä½œã€‚è¿™ç§æœºåˆ¶å¯ä»¥åœ¨ä¸¤ä¸ªè·¯å¾„ä¸Šç‹¬ç«‹å®šåˆ¶ï¼Œåœ¨æ—¶ç©ºåŸŸçš„é‡‡æ ·å¯†åº¦ä¸Šè¡¨ç°å‡ºå¼‚å‘æ€§ï¼Œä»è€Œåœ¨ä¿æŒè½»é‡çº§è®¾è®¡çš„åŒæ—¶å®ç°æ—¶ç©ºç‰¹å¾çš„åˆ†ç¦»ç¼–ç ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢„è®­ç»ƒçš„ResNetå’ŒViTä¸Šéƒ½ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºæ—¶é—´åŠ¨æ€å¯¹åŠ¨ä½œè¯†åˆ«è‡³å…³é‡è¦çš„æŒ‘æˆ˜åœºæ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é€‚é…å™¨è°ƒæ•´åœ¨å›¾åƒåˆ°è§†é¢‘æ¨¡å‹é€‚åº”ä¸­çš„æ½œåŠ›è¢«æ¢ç´¢ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„è§†é¢‘é€‚é…å™¨è°ƒæ•´æ¡†æ¶D$^2$ST-Adapterï¼Œå…·æœ‰è½»é‡çº§è®¾è®¡å’Œå¼ºå¤§çš„æ—¶ç©ºç‰¹å¾é€‚åº”åŠ›ã€‚</li>
<li>D$^2$ST-Adapteré€šè¿‡å†…éƒ¨åŒè·¯å¾„æ¶æ„å®ç°ç©ºé—´å’Œæ—¶é—´ç‰¹å¾çš„åˆ†ç¦»ç¼–ç ã€‚</li>
<li>å¼•å…¥äº†å¯å˜å½¢æ—¶ç©ºæ³¨æ„åŠ›æœºåˆ¶ä½œä¸ºD$^2$ST-Adapterçš„å…³é”®æ“ä½œã€‚</li>
<li>è¯¥æœºåˆ¶å¯ä»¥åœ¨ä¸¤ä¸ªè·¯å¾„ä¸Šç‹¬ç«‹å®šåˆ¶ï¼Œè¡¨ç°å‡ºå¼‚å‘æ€§é‡‡æ ·å¯†åº¦ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é¢„è®­ç»ƒæ¨¡å‹ResNetå’ŒViTä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºæ—¶é—´åŠ¨æ€å¯¹åŠ¨ä½œè¯†åˆ«è‡³å…³é‡è¦çš„æŒ‘æˆ˜åœºæ™¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2312.01431">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-aa4aa4937d374ddb7658be316021a135.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-90db395f7333885f68e3dfbd473668ec.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-fbb97edd4b79d0ce4e7b21f6fe76f658.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-159781a1ee6dab39f8ada19180af2e29.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/Few-Shot/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/Few-Shot/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Few-Shot/">
                                    <span class="chip bg-color">Few-Shot</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b92c85046bac61be54c371272062d92f.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-04  How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c67ee15f9e529e3599ade5099ad2f61b.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-04  TD-MPC-Opt Distilling Model-Based Multi-Task Reinforcement Learning   Agents
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">25219.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
