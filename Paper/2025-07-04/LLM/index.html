<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-04  How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-a98cd8abec040d0225b50d9cad037d3c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    82 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-04-æ›´æ–°"><a href="#2025-07-04-æ›´æ–°" class="headerlink" title="2025-07-04 æ›´æ–°"></a>2025-07-04 æ›´æ–°</h1><h2 id="How-Well-Does-GPT-4o-Understand-Vision-Evaluating-Multimodal-Foundation-Models-on-Standard-Computer-Vision-Tasks"><a href="#How-Well-Does-GPT-4o-Understand-Vision-Evaluating-Multimodal-Foundation-Models-on-Standard-Computer-Vision-Tasks" class="headerlink" title="How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks"></a>How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks</h2><p><strong>Authors:Rahul Ramachandran, Ali Garjani, Roman Bachmann, Andrei Atanov, OÄŸuzhan Fatih Kar, Amir Zamir</strong></p>
<p>Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets (e.g., COCO, ImageNet and its variants, etc).   The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework.   We observe that 1) the models are not close to the state-of-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a preliminary analysis of models with native image generation, like the latest GPT-4o, shows they exhibit quirks like hallucinations and spatial misalignments. </p>
<blockquote>
<p>å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œå¦‚GPT-4oï¼Œæœ€è¿‘å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†å°šä¸æ¸…æ¥šè¿™äº›æ¨¡å‹åœ¨ç†è§£è§†è§‰æ–¹é¢å…·ä½“å¤„äºä»€ä¹ˆæ°´å¹³ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åœ¨æ ‡å‡†è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆè¯­ä¹‰åˆ†å‰²ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†ç±»ã€æ·±åº¦å’Œè¡¨é¢æ³•çº¿é¢„æµ‹ï¼‰ä¸Šè¯„ä¼°äº†æµè¡Œå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆGPT-4oã€o4-miniã€Gemini 1.5 Proå’ŒGemini 2.0 Flashã€Claude 3.5 Sonnetã€Qwen2-VLã€Llama 3.2ï¼‰çš„æ€§èƒ½ï¼Œä½¿ç”¨äº†å·²å»ºç«‹çš„æ•°æ®é›†ï¼ˆä¾‹å¦‚COCOã€ImageNetåŠå…¶å˜ä½“ç­‰ï¼‰ã€‚æ‰§è¡Œæ­¤æ“ä½œçš„ä¸»è¦æŒ‘æˆ˜æ˜¯ï¼š1ï¼‰å¤§å¤šæ•°æ¨¡å‹è¢«è®­ç»ƒç”¨äºè¾“å‡ºæ–‡æœ¬ï¼Œæ— æ³•åŸç”Ÿè¡¨è¾¾è¯¸å¦‚ç‰‡æ®µæˆ–3Då‡ ä½•ç­‰å¤šæ ·é¢†åŸŸï¼›2ï¼‰è®¸å¤šé¢†å…ˆçš„æ¨¡å‹æ˜¯ä¸“æœ‰æ¨¡å‹ï¼Œä»…å¯é€šè¿‡APIçº§åˆ«è®¿é—®ï¼Œå³æ— æ³•è·å–å…¶æƒé‡æ¥è¿›è¡Œé€‚åº”ã€‚æˆ‘ä»¬é€šè¿‡é€šè¿‡æç¤ºé“¾å°†æ ‡å‡†è§†è§‰ä»»åŠ¡è½¬æ¢ä¸ºç­‰æ•ˆçš„æ–‡æœ¬æç¤ºå’ŒAPIå…¼å®¹ä»»åŠ¡ï¼Œåˆ›å»ºæ ‡å‡†åŒ–çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬å‘ç°ï¼š1ï¼‰è¿™äº›æ¨¡å‹åœ¨ä»»ä½•ä»»åŠ¡ä¸Šéƒ½ä¸æ¥è¿‘æœ€æ–°ä¸“ä¸šæ¨¡å‹ï¼›ç„¶è€Œ2ï¼‰å®ƒä»¬æ˜¯ç›¸å½“ä¸é”™çš„é€šç”¨æ¨¡å‹ï¼›è¿™ç›¸å½“ä»¤äººç©ç›®ï¼Œå› ä¸ºå®ƒä»¬ä¸»è¦åŸºäºå›¾åƒæ–‡æœ¬ä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚3ï¼‰å®ƒä»¬æ‰§è¡Œè¯­ä¹‰ä»»åŠ¡æ˜æ˜¾æ¯”å‡ ä½•ä»»åŠ¡æ›´å¥½ã€‚4ï¼‰è™½ç„¶æç¤ºé“¾æŠ€æœ¯ä¼šå½±å“æ€§èƒ½ï¼Œä½†æ›´å¥½çš„æ¨¡å‹å¯¹æç¤ºå˜åŒ–è¡¨ç°å‡ºè¾ƒä½çš„æ•æ„Ÿæ€§ã€‚5ï¼‰GPT-4oåœ¨éæ¨ç†æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œåœ¨6ä¸ªä»»åŠ¡ä¸­çš„4ä¸ªä¸­ä½å±…æ¦œé¦–ã€‚6ï¼‰æ¨ç†æ¨¡å‹ï¼ˆä¾‹å¦‚o3ï¼‰åœ¨å‡ ä½•ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ”¹è¿›ï¼Œ7ï¼‰å¯¹å…·æœ‰åŸç”Ÿå›¾åƒç”Ÿæˆçš„æ¨¡å‹ï¼ˆå¦‚æœ€æ–°çš„GPT-4oï¼‰è¿›è¡Œåˆæ­¥åˆ†æè¡¨æ˜ï¼Œå®ƒä»¬å…·æœ‰è¯¸å¦‚å¹»è§‰å’Œç©ºé—´ä¸åŒ¹é…ç­‰ç‰¹æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01955v1">PDF</a> Project page at <a target="_blank" rel="noopener" href="https://fm-vision-evals.epfl.ch/">https://fm-vision-evals.epfl.ch/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ˆå¦‚GPT-4oç­‰ï¼‰åœ¨æ ‡å‡†è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¡¨ç°ã€‚æ–‡ç« é€šè¿‡å¯¹æ¯”ä¸åŒæ¨¡å‹åœ¨è¯­ä¹‰åˆ†å‰²ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒåˆ†ç±»ã€æ·±åº¦é¢„æµ‹å’Œè¡¨é¢æ³•çº¿é¢„æµ‹ç­‰ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå‘ç°è¿™äº›æ¨¡å‹è™½ç„¶è¿œæœªè¾¾åˆ°ä¸“ä¸šæ¨¡å‹çš„æ°´å¹³ï¼Œä½†åœ¨å¤šä»»åŠ¡ä¸Šçš„è¡¨ç°å±•ç°å‡ºå…¶è‰¯å¥½çš„é€šç”¨æ€§ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†ä½¿ç”¨æç¤ºä¸²è”æŠ€æœ¯æ¥è§£å†³å¤šæ¨¡æ€æ¨¡å‹çš„å±€é™æ€§é—®é¢˜ã€‚ä¸åŒæ¨¡å‹çš„ç‰¹æ€§å­˜åœ¨å·®å¼‚ï¼ŒGPT-4oåœ¨éæ¨ç†æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œè€Œåœ¨å‡ ä½•ä»»åŠ¡ä¸Šå¸¦æœ‰æ¨ç†èƒ½åŠ›çš„æ¨¡å‹å¦‚o3æœ‰æ‰€æ”¹å–„ã€‚åˆæ­¥åˆ†æè¡¨æ˜ï¼Œå…·æœ‰åŸç”Ÿå›¾åƒç”ŸæˆåŠŸèƒ½çš„æ¨¡å‹å¦‚æœ€æ–°çš„GPT-4oä¼šå‡ºç°å¹»è§†å’Œç©ºé—´ä¸å¯¹é½ç­‰é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½å‚å·®ä¸é½ï¼Œä½†ä»å±•ç°å‡ºè‰¯å¥½çš„é€šç”¨æ€§ã€‚</li>
<li>æ¨¡å‹é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šæ— æ³•åŸç”Ÿè¡¨è¾¾å¤šé¢†åŸŸå†…å®¹ä»¥åŠä¸“æœ‰æ¨¡å‹çš„æƒé‡ä¸å¯è®¿é—®æ€§ã€‚</li>
<li>æç¤ºä¸²è”æŠ€æœ¯ç”¨äºè§£å†³å¤šæ¨¡æ€æ¨¡å‹çš„å±€é™æ€§é—®é¢˜ã€‚</li>
<li>GPT-4oåœ¨éæ¨ç†æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œåœ¨å¤šæ•°ä»»åŠ¡ä¸­ä½åˆ—ç¬¬ä¸€ã€‚</li>
<li>å¸¦æœ‰æ¨ç†èƒ½åŠ›çš„æ¨¡å‹å¦‚o3åœ¨å‡ ä½•ä»»åŠ¡ä¸Šæœ‰æ‰€æ”¹å–„ã€‚</li>
<li>å…·æœ‰åŸç”Ÿå›¾åƒç”ŸæˆåŠŸèƒ½çš„æ¨¡å‹å­˜åœ¨å¹»è§†å’Œç©ºé—´ä¸å¯¹é½ç­‰é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01955">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-25866a56a7bb23110ef12bfd70e68e3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8987b678a6043fb17c5c89c3628d8b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8665bb9a66c6900ff01051abba32ea93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86a19d55133674d93a00588735b57752.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92aaa3e7aa83ff825c7a37d03c198977.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Kwai-Keye-VL-Technical-Report"><a href="#Kwai-Keye-VL-Technical-Report" class="headerlink" title="Kwai Keye-VL Technical Report"></a>Kwai Keye-VL Technical Report</h2><p><strong>Authors: Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Hao Peng, Haojie Ding, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Jin Ouyang, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yang Zhou, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zhenhua Wu, Zhenyu Li, Zhixin Ling, Ziming Li, Dehua Ma, Di Xu, Haixuan Gao, Hang Li, Jiawei Guo, Jing Wang, Lejian Ren, Muhao Wei, Qianqian Wang, Qigen Hu, Shiyao Wang, Tao Yu, Xinchen Luo, Yan Li, Yiming Liang, Yuhang Hu, Zeyi Lu, Zhuoran Yang, Zixing Zhang</strong></p>
<p>While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in todayâ€™s digital landscape. To bridge this gap, we introduce \textbf{Kwai Keye-VL}, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a four-stage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode <code>cold-start&#39;&#39; data mixture, which includes </code>thinkingâ€™â€™, <code>non-thinking&#39;&#39;, </code>auto-thinkâ€™â€™, &#96;&#96;think with imageâ€™â€™, and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the \textbf{KC-MMBench}, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage. </p>
<blockquote>
<p>è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é™æ€å›¾åƒä¸Šè¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬å¸¸å¸¸åœ¨ç†è§£åŠ¨æ€ã€ä¿¡æ¯å¯†é›†åº¦é«˜çš„çŸ­è§†é¢‘ä¸Šè¡¨ç°ä¸è¶³ï¼Œè€ŒçŸ­è§†é¢‘å´æ˜¯å½“ä»Šæ•°å­—æ—¶ä»£çš„ä¸»å¯¼åª’ä½“ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†<strong>Kwai Keye-VL</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‹¥æœ‰8äº¿å‚æ•°çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œä¸“ä¸ºé¢†å…ˆçš„çŸ­è§†é¢‘ç†è§£èƒ½åŠ›è€Œè®¾è®¡ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„é€šç”¨è§†è§‰è¯­è¨€åŠŸèƒ½ã€‚Keye-VLçš„å‘å±•åŸºäºä¸¤å¤§æ ¸å¿ƒæ”¯æŸ±ï¼šä¸€æ˜¯è¶…è¿‡600äº¿æ ‡è®°çš„å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼Œå…¶ä¸­å¼ºçƒˆä¾§é‡äºè§†é¢‘ï¼›äºŒæ˜¯åˆ›æ–°çš„è®­ç»ƒé…æ–¹ã€‚è¯¥é…æ–¹åŒ…æ‹¬ä¸€ä¸ªç”¨äºå®ç°åšå®çš„è§†è§‰è¯­è¨€å¯¹é½çš„å››é˜¶æ®µé¢„è®­ç»ƒè¿‡ç¨‹ï¼Œä»¥åŠä¸€ä¸ªç»†è‡´çš„ä¸¤é˜¶æ®µåè®­ç»ƒè¿‡ç¨‹ã€‚ç¬¬ä¸€é˜¶æ®µåè®­ç»ƒå¢å¼ºäº†åŸºç¡€èƒ½åŠ›ï¼Œå¦‚æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼›ç¬¬äºŒé˜¶æ®µåˆ™ä¸“æ³¨äºåˆºæ¿€é«˜çº§æ¨ç†èƒ½åŠ›ã€‚åœ¨è¿™ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬çš„å…³é”®åˆ›æ–°æ˜¯äº”æ¨¡å¼â€œå†·å¯åŠ¨â€æ•°æ®æ··åˆï¼ŒåŒ…æ‹¬â€œæ€è€ƒâ€ã€â€œéæ€è€ƒâ€ã€â€œè‡ªåŠ¨æ€è€ƒâ€ã€â€œä¸å›¾åƒä¸€èµ·æ€è€ƒâ€ä»¥åŠé«˜è´¨é‡è§†é¢‘æ•°æ®ã€‚è¿™ç§æ··åˆæ•™ä¼šæ¨¡å‹å†³å®šä½•æ—¶ä»¥åŠå¦‚ä½•æ¨ç†ã€‚éšåçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¯¹é½æ­¥éª¤è¿›ä¸€æ­¥å¢å¼ºäº†è¿™äº›æ¨ç†èƒ½åŠ›ï¼Œå¹¶çº æ­£äº†å¼‚å¸¸æ¨¡å‹è¡Œä¸ºï¼Œå¦‚é‡å¤è¾“å‡ºã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œç»“æœè¡¨æ˜Keye-VLåœ¨å…¬å…±è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå¹¶åœ¨åŸºäºå›¾åƒçš„ä¸€èˆ¬ä»»åŠ¡ä¸­ä¿æŒäº†é«˜åº¦ç«äº‰åŠ›ï¼ˆå›¾1ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘å¹¶å‘å¸ƒäº†é’ˆå¯¹ç°å®çŸ­è§†é¢‘åœºæ™¯çš„å…¨æ–°åŸºå‡†æµ‹è¯•<strong>KC-MMBench</strong>ï¼ŒKeye-VLåœ¨æ­¤å±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01949v1">PDF</a> Technical Report: <a target="_blank" rel="noopener" href="https://github.com/Kwai-Keye/Keye">https://github.com/Kwai-Keye/Keye</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é™æ€å›¾åƒä¸Šçš„å“è¶Šè¡¨ç°ï¼Œé’ˆå¯¹å½“ä»Šæ•°å­—æ™¯è§‚ä¸­å ä¸»å¯¼åœ°ä½çš„ä¿¡æ¯å¯†é›†å‹çŸ­è§†é¢‘ï¼Œæ¨å‡ºäº†Kwai Keye-VLæ¨¡å‹ã€‚è¯¥æ¨¡å‹æ‹¥æœ‰è¶…è¿‡8äº¿å‚æ•°ï¼Œæ—¨åœ¨å®ç°çŸ­è§†é¢‘ç†è§£çš„å“è¶Šæ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„é€šç”¨è§†è§‰è¯­è¨€åŠŸèƒ½ã€‚Keye-VLçš„å‘å±•å»ºç«‹åœ¨ä¸¤ä¸ªæ ¸å¿ƒæ”¯æŸ±ä¸Šï¼šè¶…è¿‡600äº¿æ ‡è®°çš„å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†å’Œåˆ›æ–°çš„è®­ç»ƒé…æ–¹ã€‚é€šè¿‡é¢„è®­ç»ƒå’Œç‰¹æ®Šè®¾è®¡çš„åè®­ç»ƒè¿‡ç¨‹å¢å¼ºè§†è§‰è¯­è¨€å¯¹é½ï¼ŒKeye-VLå±•ç°äº†å…ˆè¿›çš„è§†é¢‘ç†è§£æ€§èƒ½ã€‚åœ¨æ¨å‡ºæ–°çš„åŸºå‡†æµ‹è¯•KC-MMBenchä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é™æ€å›¾åƒä¸Šçš„è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ä¿¡æ¯å¯†é›†å‹çŸ­è§†é¢‘ä¸Šçš„ç†è§£ä»æœ‰å¾…æé«˜ã€‚</li>
<li>Kwai Keye-VLæ˜¯ä¸€æ¬¾æ—¨åœ¨ç†è§£çŸ­è§†é¢‘çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå…·å¤‡è¶…è¿‡8äº¿çš„å‚æ•°ã€‚</li>
<li>Keye-VLçš„å»ºç«‹ä¾èµ–äºä¸¤ä¸ªæ ¸å¿ƒæ”¯æŸ±ï¼šå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†å’Œåˆ›æ–°è®­ç»ƒé…æ–¹ã€‚</li>
<li>è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨å››é˜¶æ®µé¢„è®­ç»ƒè¿‡ç¨‹ç¡®ä¿è§†è§‰è¯­è¨€å¯¹é½ã€‚</li>
<li>åè®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€é˜¶æ®µå¢å¼ºåŸºç¡€èƒ½åŠ›å¦‚æŒ‡ä»¤éµå¾ªï¼Œç¬¬äºŒé˜¶æ®µä¸“æ³¨äºé«˜çº§æ¨ç†èƒ½åŠ›è®­ç»ƒã€‚å…¶ä¸­å…³é”®åˆ›æ–°åœ¨äºå¼•å…¥äº”æ¨¡å¼çš„å†·å¯åŠ¨æ•°æ®æ··åˆç­–ç•¥ï¼Œä»¥åŠéšåçš„å¼ºåŒ–å­¦ä¹ å’Œå¯¹é½æ­¥éª¤æ¥å¢å¼ºæ¨ç†èƒ½åŠ›å¹¶çº æ­£å¼‚å¸¸æ¨¡å‹è¡Œä¸ºã€‚</li>
<li>Keye-VLåœ¨å…¬å…±è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœï¼Œå¹¶åœ¨ä¸€èˆ¬å›¾åƒä»»åŠ¡ä¸Šä¿æŒäº†é«˜åº¦ç«äº‰åŠ›ã€‚æ­¤å¤–è¿˜å¼€å‘äº†é’ˆå¯¹ç°å®çŸ­è§†é¢‘åœºæ™¯çš„KC-MMBenchåŸºå‡†æµ‹è¯•ã€‚Keye-VLåœ¨æ­¤è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71d2b8efe72733d01ee1b92778680729.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a24ae13c309662119cc8e1efce624ab.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Driven-Closed-Loop-UAV-Operation-with-Semantic-Observations"><a href="#Large-Language-Model-Driven-Closed-Loop-UAV-Operation-with-Semantic-Observations" class="headerlink" title="Large Language Model-Driven Closed-Loop UAV Operation with Semantic   Observations"></a>Large Language Model-Driven Closed-Loop UAV Operation with Semantic   Observations</h2><p><strong>Authors:Wenhao Wang, Yanyan Li, Long Jiao, Jiawei Yuan</strong></p>
<p>Large Language Models (LLMs) have revolutionized robotic autonomy, including Unmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential of LLMs for translating human instructions into executable control code for UAV operations. However, LLMs still face challenges from logical reasoning and complex decision-making, leading to concerns about the reliability of LLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop control framework that enables reliable UAV operations powered by effective feedback and refinement using two LLM modules, i.e., a Code Generator and an Evaluator. Our framework transforms numerical state observations from UAV operations into natural language trajectory descriptions to enhance the evaluator LLMâ€™s understanding of UAV dynamics for precise feedback generation. Our framework also enables a simulation-based refinement process, and hence eliminates the risks to physical UAVs caused by incorrect code execution during the refinement. Extensive experiments on UAV control tasks with different complexities are conducted. The experimental results show that our framework can achieve reliable UAV operations using LLMs, which significantly outperforms baseline approaches in terms of success rate and completeness with the increase of task complexity. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æ”¹å˜äº†åŒ…æ‹¬æ— äººé£è¡Œå™¨ï¼ˆUAVï¼‰åœ¨å†…çš„æœºå™¨äººè‡ªä¸»æ€§ã€‚æœ€è¿‘çš„ç ”ç©¶å·²ç»è¯æ˜äº†LLMåœ¨å°†äººç±»æŒ‡ä»¤ç¿»è¯‘æˆå¯æ‰§è¡Œçš„æ§åˆ¶ä»£ç ä»¥è¿›è¡Œæ— äººæœºæ“ä½œæ–¹é¢çš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒLLMåœ¨é€»è¾‘æ¨ç†å’Œå¤æ‚å†³ç­–åˆ¶å®šæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹LLMé©±åŠ¨çš„æ— äººæœºæ“ä½œå¯é æ€§çš„æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§LLMé©±åŠ¨çš„é—­ç¯æ§åˆ¶æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡æœ‰æ•ˆçš„åé¦ˆå’Œæ”¹è¿›ï¼Œä½¿ç”¨ä¸¤ä¸ªLLMæ¨¡å—ï¼ˆå³ä»£ç ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ï¼‰å®ç°äº†å¯é çš„æ— äººæœºæ“ä½œã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†æ— äººæœºæ“ä½œçš„æ•°å€¼çŠ¶æ€è§‚å¯Ÿç»“æœè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€è½¨è¿¹æè¿°ï¼Œä»¥æé«˜è¯„ä¼°LLMå¯¹æ— äººæœºåŠ¨æ€çš„ç†è§£ï¼Œä»è€Œç”Ÿæˆç²¾ç¡®çš„åé¦ˆã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿˜æ”¯æŒåŸºäºæ¨¡æ‹Ÿçš„æ”¹è¿›è¿‡ç¨‹ï¼Œæ¶ˆé™¤äº†æ”¹è¿›è¿‡ç¨‹ä¸­å› ä»£ç æ‰§è¡Œé”™è¯¯è€Œå¯¹å®é™…æ— äººæœºé€ æˆçš„é£é™©ã€‚å¯¹å…·æœ‰ä¸åŒå¤æ‚æ€§çš„æ— äººæœºæ§åˆ¶ä»»åŠ¡è¿›è¡Œäº†å¤§é‡å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿå®ç°å¯é çš„LLMæ— äººæœºæ“ä½œï¼Œåœ¨ä»»åŠ¡å¤æ‚åº¦å¢åŠ çš„æƒ…å†µä¸‹ï¼Œç›¸è¾ƒäºåŸºå‡†æ–¹æ³•ï¼Œå…¶åœ¨æˆåŠŸç‡å’Œå®Œæ•´æ€§æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01930v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ— äººæœºè‡ªä¸»é£è¡Œé¢†åŸŸå…·æœ‰é©å‘½æ€§æ½œåŠ›ï¼Œä½†é¢ä¸´é€»è¾‘ç†è§£å’Œå¤æ‚å†³ç­–çš„æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§LLMé©±åŠ¨çš„é—­ç¯æ§åˆ¶æ¡†æ¶ï¼Œé€šè¿‡æœ‰æ•ˆçš„åé¦ˆå’Œæ”¹è¿›ï¼Œä½¿ç”¨ä»£ç ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ä¸¤ä¸ªLLMæ¨¡å—ï¼Œå®ç°å¯é çš„æ— äººæœºæ“ä½œã€‚è¯¥æ¡†æ¶å°†æ— äººæœºçš„æ•°å€¼çŠ¶æ€è§‚æµ‹è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€è½¨è¿¹æè¿°ï¼Œæé«˜è¯„ä¼°å™¨å¯¹æ— äººæœºåŠ¨æ€çš„ç†è§£ï¼Œä»¥ç”Ÿæˆç²¾ç¡®çš„åé¦ˆã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä»»åŠ¡å¤æ‚åº¦å¢åŠ çš„æƒ…å†µä¸‹ä»èƒ½å®ç°å¯é çš„æ— äººæœºæ“ä½œï¼Œæ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMåœ¨æ— äººæœºè‡ªä¸»é£è¡Œé¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿå°†äººç±»æŒ‡ä»¤è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„æ§åˆ¶ä»£ç ã€‚</li>
<li>å½“å‰LLMåœ¨é€»è¾‘ç†è§£å’Œå¤æ‚å†³ç­–æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå½±å“æ— äººæœºæ“ä½œçš„å¯é æ€§ã€‚</li>
<li>æå‡ºä¸€ç§LLMé©±åŠ¨çš„é—­ç¯æ§åˆ¶æ¡†æ¶ï¼ŒåŒ…å«ä»£ç ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ä¸¤ä¸ªæ¨¡å—ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡å°†æ— äººæœºçš„æ•°å€¼çŠ¶æ€è§‚æµ‹è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€è½¨è¿¹æè¿°ï¼Œå¢å¼ºå¯¹æ— äººæœºåŠ¨æ€çš„ç†è§£ã€‚</li>
<li>æ¡†æ¶æ”¯æŒåŸºäºæ¨¡æ‹Ÿçš„æ”¹è¿›è¿‡ç¨‹ï¼Œå‡å°‘å®é™…æ— äººæœºå› ä»£ç æ‰§è¡Œé”™è¯¯å¯¼è‡´çš„é£é™©ã€‚</li>
<li>å®éªŒè¯æ˜è¯¥æ¡†æ¶åœ¨ä»»åŠ¡å¤æ‚åº¦å¢åŠ æ—¶ä»èƒ½ä¿æŒé«˜æˆåŠŸç‡å’Œå®Œæ•´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8b60c2d2eb8cf32ed5c1842971859e76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b4fab06cc3db47da849f4fd44147abe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b06dfa91ab452f7067797ee3365e2f5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a691b73db0c36312c4a697f6aec9130.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f682c04b456367edb962cd4817e6762.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6747a0fa04a0d4a381a966325df8558b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Gradient-Adaptive-Policy-Optimization-Towards-Multi-Objective-Alignment-of-Large-Language-Models"><a href="#Gradient-Adaptive-Policy-Optimization-Towards-Multi-Objective-Alignment-of-Large-Language-Models" class="headerlink" title="Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment   of Large Language Models"></a>Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment   of Large Language Models</h2><p><strong>Authors:Chengao Li, Hanyu Zhang, Yunkun Xu, Hongyan Xue, Xiang Ao, Qing He</strong></p>
<p>Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful technique for aligning large language models (LLMs) with human preferences. However, effectively aligning LLMs with diverse human preferences remains a significant challenge, particularly when they are conflict. To address this issue, we frame human value alignment as a multi-objective optimization problem, aiming to maximize a set of potentially conflicting objectives. We introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning paradigm that employs multiple-gradient descent to align LLMs with diverse preference distributions. GAPO adaptively rescales the gradients for each objective to determine an update direction that optimally balances the trade-offs between objectives. Additionally, we introduce P-GAPO, which incorporates user preferences across different objectives and achieves Pareto solutions that better align with the userâ€™s specific needs. Our theoretical analysis demonstrates that GAPO converges towards a Pareto optimal solution for multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms current state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²ç»æˆä¸ºä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œç”¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆåœ°å°†LLMä¸å¤šæ ·çš„äººç±»åå¥½å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨å­˜åœ¨å†²çªçš„æ—¶å€™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†äººç±»ä»·å€¼å¯¹é½ä½œä¸ºä¸€ä¸ªå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ä¸€ç»„å¯èƒ½ç›¸äº’å†²çªçš„ç›®æ ‡ã€‚æˆ‘ä»¬å¼•å…¥äº†æ¢¯åº¦è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼ˆGAPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¾®è°ƒèŒƒå¼ï¼Œé‡‡ç”¨å¤šæ¢¯åº¦ä¸‹é™æ³•å°†LLMä¸å„ç§åå¥½åˆ†å¸ƒå¯¹é½ã€‚GAPOè‡ªé€‚åº”åœ°é‡æ–°è°ƒæ•´æ¯ä¸ªç›®æ ‡çš„æ¢¯åº¦ï¼Œä»¥ç¡®å®šä¸€ä¸ªæ›´æ–°æ–¹å‘ï¼Œè¯¥æ–¹å‘èƒ½å¤Ÿæœ€ä½³åœ°å¹³è¡¡ç›®æ ‡ä¹‹é—´çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†P-GAPOï¼Œå®ƒç»“åˆäº†ä¸åŒç›®æ ‡ä¸Šçš„ç”¨æˆ·åå¥½ï¼Œå®ç°äº†å¸•ç´¯æ‰˜è§£å†³æ–¹æ¡ˆï¼Œæ›´å¥½åœ°ç¬¦åˆç”¨æˆ·çš„ç‰¹å®šéœ€æ±‚ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æè¡¨æ˜ï¼ŒGAPOæœå‘å¤šä¸ªç›®æ ‡çš„å¸•ç´¯æ‰˜æœ€ä¼˜è§£æ”¶æ•›ã€‚åœ¨Mistral-7Bä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒGAPOä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨æœ‰ç”¨æ€§å’Œæ— å®³æ€§æ–¹é¢éƒ½å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01915v1">PDF</a> 19 pages, 3 figures. Accepted by ACL 2025 (main)</p>
<p><strong>æ‘˜è¦</strong><br>    åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æŠ€æœ¯å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½ã€‚é’ˆå¯¹LLMå¯¹é½å¤šæ ·çš„äººç±»åå¥½æ—¶å­˜åœ¨çš„å†²çªé—®é¢˜ï¼Œæå‡ºå°†äººç±»ä»·å€¼å¯¹é½è§†ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ã€‚å¼•å…¥æ¢¯åº¦è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼ˆGAPOï¼‰è¿™ä¸€æ–°å‹å¾®è°ƒèŒƒå¼ï¼Œé‡‡ç”¨å¤šæ¢¯åº¦ä¸‹é™æ³•å¯¹é½LLMä¸å¤šæ ·åå¥½åˆ†å¸ƒã€‚GAPOè‡ªé€‚åº”è°ƒæ•´æ¯ä¸ªç›®æ ‡çš„æ¢¯åº¦ï¼Œç¡®å®šæœ€ä¼˜å¹³è¡¡å„ç›®æ ‡é—´æƒè¡¡çš„æ›´æ–°æ–¹å‘ã€‚æ­¤å¤–ï¼Œæ¨å‡ºP-GAPOï¼Œèå…¥ç”¨æˆ·åœ¨ä¸åŒç›®æ ‡ä¸Šçš„åå¥½ï¼Œå®ç°æ›´ç¬¦åˆç”¨æˆ·ç‰¹å®šéœ€æ±‚çš„å¸•ç´¯æ‰˜è§£å†³æ–¹æ¡ˆã€‚ç†è®ºåˆ†æå’ŒMistral-7Bä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒGAPOä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨æœ‰ç”¨æ€§å’Œæ— å®³æ€§æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>RLHFæŠ€æœ¯ç”¨äºå¯¹é½LLMä¸äººç±»åå¥½ï¼Œå±•ç°å¼ºå¤§æ½œåŠ›ã€‚</li>
<li>å¯¹é½LLMä¸å¤šæ ·äººç±»åå¥½å­˜åœ¨å†²çªé—®é¢˜ã€‚</li>
<li>å°†äººç±»ä»·å€¼å¯¹é½è§†ä¸ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>å¼•å…¥GAPOè¿™ä¸€æ–°å‹å¾®è°ƒèŒƒå¼ï¼Œé‡‡ç”¨å¤šæ¢¯åº¦ä¸‹é™æ³•å¤„ç†å¤šæ ·åå¥½åˆ†å¸ƒã€‚</li>
<li>GAPOè‡ªé€‚åº”è°ƒæ•´æ¢¯åº¦ï¼Œç¡®å®šæ›´æ–°æ–¹å‘ä»¥å¹³è¡¡å„ç›®æ ‡é—´æƒè¡¡ã€‚</li>
<li>P-GAPOèå…¥ç”¨æˆ·åœ¨ä¸åŒç›®æ ‡ä¸Šçš„åå¥½ï¼Œå®ç°æ›´ç¬¦åˆç”¨æˆ·éœ€æ±‚çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>GAPOåœ¨ç†è®ºåˆ†æå’Œå®è¯ç ”ç©¶ä¸Šå‡è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œä¼˜äºå½“å‰æœ€å…ˆè¿›æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e0174499ff4ad12e2fb869fcbd503e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7eaa75b3f4051e393b712eeedf35e816.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a2b97eafe9c83f666885761f922911c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Reasoning-to-Edit-Hypothetical-Instruction-Based-Image-Editing-with-Visual-Reasoning"><a href="#Reasoning-to-Edit-Hypothetical-Instruction-Based-Image-Editing-with-Visual-Reasoning" class="headerlink" title="Reasoning to Edit: Hypothetical Instruction-Based Image Editing with   Visual Reasoning"></a>Reasoning to Edit: Hypothetical Instruction-Based Image Editing with   Visual Reasoning</h2><p><strong>Authors:Qingdong He, Xueqin Chen, Chaoyi Wang, Yanjie Pan, Xiaobin Hu, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xiangtai Li, Jiangning Zhang</strong></p>
<p>Instruction-based image editing (IIE) has advanced rapidly with the success of diffusion models. However, existing efforts primarily focus on simple and explicit instructions to execute editing operations such as adding, deleting, moving, or swapping objects. They struggle to handle more complex implicit hypothetical instructions that require deeper reasoning to infer plausible visual changes and user intent. Additionally, current datasets provide limited support for training and evaluating reasoning-aware editing capabilities. Architecturally, these methods also lack mechanisms for fine-grained detail extraction that support such reasoning. To address these limitations, we propose Reason50K, a large-scale dataset specifically curated for training and evaluating hypothetical instruction reasoning image editing, along with ReasonBrain, a novel framework designed to reason over and execute implicit hypothetical instructions across diverse scenarios. Reason50K includes over 50K samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs) for editing guidance generation and a diffusion model for image synthesis, incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture detailed visual and textual semantics essential for supporting instruction reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal Enhancer (CME) that enables rich interactions between the fine-grained cues and MLLM-derived features. Extensive experiments demonstrate that ReasonBrain consistently outperforms state-of-the-art baselines on reasoning scenarios while exhibiting strong zero-shot generalization to conventional IIE tasks. Our dataset and code will be released publicly. </p>
<blockquote>
<p>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ï¼ˆIIEï¼‰éšç€æ‰©æ•£æ¨¡å‹çš„æˆåŠŸè€Œè¿…é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨æ‰§è¡Œç®€å•çš„æ˜ç¡®æŒ‡ä»¤ï¼Œå¦‚æ·»åŠ ã€åˆ é™¤ã€ç§»åŠ¨æˆ–äº¤æ¢å¯¹è±¡ç­‰ç¼–è¾‘æ“ä½œã€‚ä»–ä»¬å¾ˆéš¾å¤„ç†æ›´å¤æ‚çš„éšå«å‡è®¾æŒ‡ä»¤ï¼Œè¿™äº›æŒ‡ä»¤éœ€è¦æ›´æ·±çš„æ¨ç†æ¥æ¨æ–­åˆç†çš„è§†è§‰å˜åŒ–å’Œç”¨æˆ·æ„å›¾ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ•°æ®é›†åœ¨æ”¯æŒè®­ç»ƒå’Œè¯„ä¼°æ¨ç†æ„ŸçŸ¥ç¼–è¾‘èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚åœ¨ç»“æ„ä¸Šï¼Œè¿™äº›æ–¹æ³•ä¹Ÿç¼ºä¹æ”¯æŒæ­¤ç±»æ¨ç†çš„ç²¾ç»†ç»†èŠ‚æå–æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Reason50Kï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè®­ç»ƒå’Œè¯„ä¼°å‡è®¾æŒ‡ä»¤æ¨ç†å›¾åƒç¼–è¾‘çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»¥åŠReasonBrainï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨å¤šç§åœºæ™¯ä¸‹æ‰§è¡Œéšå«å‡è®¾æŒ‡ä»¤è¿›è¡Œæ¨ç†ã€‚Reason50KåŒ…æ‹¬è¶…è¿‡50Kä¸ªæ ·æœ¬ï¼Œæ¶µç›–å››ç§å…³é”®æ¨ç†åœºæ™¯ï¼šç‰©ç†æ¨ç†ã€æ—¶é—´æ¨ç†ã€å› æœæ¨ç†å’Œæ•…äº‹æ¨ç†ã€‚ReasonBrainåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”Ÿæˆç¼–è¾‘æŒ‡å¯¼ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒåˆæˆï¼ŒåŒæ—¶èå…¥ç²¾ç»†æ¨ç†çº¿ç´¢æå–ï¼ˆFRCEï¼‰æ¨¡å—ï¼Œä»¥æ•æ‰æ”¯æŒæŒ‡ä»¤æ¨ç†çš„è¯¦ç»†è§†è§‰å’Œæ–‡æœ¬è¯­ä¹‰ã€‚ä¸ºäº†å‡å°‘è¯­ä¹‰æŸå¤±ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†è·¨æ¨¡æ€å¢å¼ºå™¨ï¼ˆCMEï¼‰ï¼Œå®ƒä½¿ç²¾ç»†çº¿ç´¢å’ŒMLLMè¡ç”Ÿç‰¹å¾ä¹‹é—´èƒ½å¤Ÿè¿›è¡Œä¸°å¯Œçš„äº¤äº’ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒReasonBrainåœ¨æ¨ç†åœºæ™¯ä¸ŠæŒç»­è¶…è¶Šæœ€æ–°åŸºçº¿ï¼ŒåŒæ—¶åœ¨å¸¸è§„IIEä»»åŠ¡ä¸Šå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01908v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ï¼ˆIIEï¼‰çš„ç°æœ‰æŒ‘æˆ˜å’Œé™åˆ¶ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨ç®€å•ç›´æ¥çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œéš¾ä»¥å¤„ç†å¤æ‚çš„éšå«å‡è®¾æŒ‡ä»¤ï¼Œä¸”ç¼ºä¹ç›¸åº”çš„è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†ä»¥åŠæ”¯æŒæ·±åº¦æ¨ç†çš„æ¶æ„ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†Reason50Kæ•°æ®é›†å’ŒReasonBrainæ¡†æ¶ã€‚Reason50KåŒ…å«è¶…è¿‡50Kä¸ªæ ·æœ¬ï¼Œæ¶µç›–å››ç§å…³é”®æ¨ç†åœºæ™¯ï¼šç‰©ç†ã€æ—¶é—´ã€å› æœå’Œæ•…äº‹æ¨ç†ã€‚ReasonBrainåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›è¡Œç¼–è¾‘æŒ‡å¯¼ç”Ÿæˆå’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒåˆæˆï¼Œå¹¶å¼•å…¥ç²¾ç»†æ¨ç†çº¿ç´¢æå–ï¼ˆFRCEï¼‰æ¨¡å—æ•æ‰è¯¦ç»†çš„è§†è§‰å’Œæ–‡æœ¬è¯­ä¹‰ä»¥æ”¯æŒæŒ‡ä»¤æ¨ç†ã€‚åŒæ—¶ï¼Œå¼•å…¥è·¨æ¨¡æ€å¢å¼ºå™¨ï¼ˆCMEï¼‰å‡å°‘è¯­ä¹‰æŸå¤±ã€‚å®éªŒè¡¨æ˜ï¼ŒReasonBrainåœ¨æ¨ç†åœºæ™¯ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ï¼Œå¹¶åœ¨å¸¸è§„IIEä»»åŠ¡ä¸Šå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å›¾åƒç¼–è¾‘æ–¹æ³•ä¸»è¦å¤„ç†ç®€å•ç›´æ¥çš„ç¼–è¾‘æŒ‡ä»¤ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚çš„éšå«å‡è®¾æŒ‡ä»¤ã€‚</li>
<li>ç¼ºä¹é’ˆå¯¹éšå«å‡è®¾æŒ‡ä»¤çš„å›¾åƒç¼–è¾‘è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†ã€‚</li>
<li>æå‡ºReason50Kæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡50Kä¸ªæ ·æœ¬ï¼Œæ¶µç›–å¤šç§å…³é”®æ¨ç†åœºæ™¯ã€‚</li>
<li>ä»‹ç»ReasonBrainæ¡†æ¶ï¼Œç»“åˆMLLMsã€æ‰©æ•£æ¨¡å‹å’ŒFRCEæ¨¡å—å¤„ç†éšå«å‡è®¾æŒ‡ä»¤ã€‚</li>
<li>FRCEæ¨¡å—ç”¨äºæ•æ‰è¯¦ç»†çš„è§†è§‰å’Œæ–‡æœ¬è¯­ä¹‰ï¼Œæ”¯æŒæŒ‡ä»¤æ¨ç†ã€‚</li>
<li>å¼•å…¥è·¨æ¨¡æ€å¢å¼ºå™¨ï¼ˆCMEï¼‰å‡å°‘è¯­ä¹‰æŸå¤±ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-02ecc7f203b5dcd1da51e1f4896ead5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03c5afdb1b2171a1d4793ff58685c324.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a174114b9bab6f90864463b72687a5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae1aba70778f478c908b0454c5407f2c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="High-Layer-Attention-Pruning-with-Rescaling"><a href="#High-Layer-Attention-Pruning-with-Rescaling" class="headerlink" title="High-Layer Attention Pruning with Rescaling"></a>High-Layer Attention Pruning with Rescaling</h2><p><strong>Authors:Songtao Liu, Peng Liu</strong></p>
<p>Pruning is a highly effective approach for compressing large language models (LLMs), significantly reducing inference latency. However, conventional training-free structured pruning methods often employ a heuristic metric that indiscriminately removes some attention heads across all pruning layers, without considering their positions within the network architecture. In this work, we propose a novel pruning algorithm that strategically prunes attention heads in the modelâ€™s higher layers. Since the removal of attention heads can alter the magnitude of token representations, we introduce an adaptive rescaling parameter that calibrates the representation scale post-pruning to counteract this effect. We conduct comprehensive experiments on a wide range of LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our evaluation includes both generation and discriminative tasks across 27 datasets. The results consistently demonstrate that our method outperforms existing structured pruning methods. This improvement is particularly notable in generation tasks, where our approach significantly outperforms existing baselines. </p>
<blockquote>
<p>å‰ªææ˜¯å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ç§é«˜æ•ˆæ–¹æ³•ï¼Œèƒ½æ˜¾è‘—å‡å°‘æ¨ç†å»¶è¿Ÿã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ— è®­ç»ƒç»“æ„åŒ–å‰ªææ–¹æ³•é€šå¸¸é‡‡ç”¨å¯å‘å¼åº¦é‡ï¼Œä¸åŠ åŒºåˆ«åœ°ç§»é™¤æ‰€æœ‰å‰ªæå±‚ä¸­çš„ä¸€äº›æ³¨æ„åŠ›å¤´ï¼Œè€Œæ²¡æœ‰è€ƒè™‘åˆ°å®ƒä»¬åœ¨ç½‘ç»œæ¶æ„ä¸­çš„ä½ç½®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å‰ªæç®—æ³•ï¼Œè¯¥ç®—æ³•ä¼šç­–ç•¥æ€§åœ°å‰ªé™¤æ¨¡å‹ä¸­è¾ƒé«˜å±‚çš„æ³¨æ„åŠ›å¤´ã€‚ç”±äºç§»é™¤æ³¨æ„åŠ›å¤´å¯èƒ½ä¼šæ”¹å˜ä»¤ç‰Œè¡¨ç¤ºçš„å¹…åº¦ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè‡ªé€‚åº”ç¼©æ”¾å‚æ•°ï¼Œåœ¨å‰ªæåæ ¡å‡†è¡¨ç¤ºè§„æ¨¡ï¼Œä»¥æŠµæ¶ˆè¿™ä¸€å½±å“ã€‚æˆ‘ä»¬åœ¨å¤šç§LLMä¸Šè¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼ŒåŒ…æ‹¬LLaMA3.1-8Bã€Mistral-7B-v0.3ã€Qwen2-7Bå’ŒGemma2-9Bã€‚æˆ‘ä»¬çš„è¯„ä¼°åŒ…æ‹¬27ä¸ªæ•°æ®é›†çš„ç”Ÿæˆå’Œåˆ¤åˆ«ä»»åŠ¡ã€‚ç»“æœä¸€è‡´è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰çš„ç»“æ„åŒ–å‰ªææ–¹æ³•ã€‚åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œè¿™ä¸€æ”¹è¿›å°¤ä¸ºçªå‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01900v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°å‹ä¿®å‰ªç®—æ³•ï¼Œè¯¥ç®—æ³•ç­–ç•¥æ€§åœ°ä¿®å‰ªæ¨¡å‹é«˜å±‚ä¸­çš„æ³¨æ„åŠ›å¤´ï¼Œå¹¶å¼•å…¥è‡ªé€‚åº”ç¼©æ”¾å‚æ•°æ¥æ ¡å‡†ä¿®å‰ªåçš„è¡¨ç¤ºè§„æ¨¡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§LLMä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¿®å‰ªæ˜¯å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹çš„æœ‰æ•ˆæ–¹æ³•ï¼Œèƒ½æ˜¾è‘—é™ä½æ¨ç†å»¶è¿Ÿã€‚</li>
<li>ä¼ ç»Ÿçš„æ— è®­ç»ƒç»“æ„åŒ–ä¿®å‰ªæ–¹æ³•å¸¸å¸¸ä½¿ç”¨å¯å‘å¼åº¦é‡æ ‡å‡†ï¼Œä¸è€ƒè™‘ç½‘ç»œæ¶æ„ä¸­çš„æ³¨æ„åŠ›å¤´ä½ç½®ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹ä¿®å‰ªç®—æ³•ï¼Œç­–ç•¥æ€§åœ°ä¿®å‰ªæ¨¡å‹é«˜å±‚ä¸­çš„æ³¨æ„åŠ›å¤´ã€‚</li>
<li>å¼•å…¥è‡ªé€‚åº”ç¼©æ”¾å‚æ•°æ¥æ ¡å‡†ä¿®å‰ªåçš„è¡¨ç¤ºè§„æ¨¡ï¼Œä»¥æŠµæ¶ˆä¿®å‰ªæ³¨æ„åŠ›å¤´å¯¹ä»¤ç‰Œè¡¨ç¤ºå¹…åº¦çš„å½±å“ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šç§LLMä¸Šè¿›è¡Œäº†å¹¿æ³›å®éªŒï¼ŒåŒ…æ‹¬LLaMA3.1-8Bã€Mistral-7B-v0.3ã€Qwen2-7Bå’ŒGemma2-9Bã€‚</li>
<li>è¯„ä»·åŒ…æ‹¬ç”Ÿæˆå’Œåˆ¤åˆ«ä»»åŠ¡ï¼Œæ¶‰åŠ27ä¸ªæ•°æ®é›†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01900">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a9f49498624cfd01df56c0eba3e3b299.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5404fc6bb64136aec1a97440fb0c9196.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c4bc0f6b9c870b6c556dea95c1ff12b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9589d194cfac57d3ec106b7635deecab.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MiCoTA-Bridging-the-Learnability-Gap-with-Intermediate-CoT-and-Teacher-Assistants"><a href="#MiCoTA-Bridging-the-Learnability-Gap-with-Intermediate-CoT-and-Teacher-Assistants" class="headerlink" title="MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher   Assistants"></a>MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher   Assistants</h2><p><strong>Authors:Dongyi Ding, Tiannan Wang, Chenghao Zhu, Meiling Tao, Yuchen Eleanor Jiang, Wangchunshu Zhou</strong></p>
<p>Large language models (LLMs) excel at reasoning tasks requiring long thought sequences for planning, reflection, and refinement. However, their substantial model size and high computational demands are impractical for widespread deployment. Yet, small language models (SLMs) often struggle to learn long-form CoT reasoning due to their limited capacity, a phenomenon we refer to as the â€œSLMs Learnability Gapâ€. To address this, we introduce \textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation (MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA employs intermediate-sized models as teacher assistants and utilizes intermediate-length CoT sequences to bridge both the capacity and reasoning length gaps. Our experiments on downstream tasks demonstrate that although SLMs distilled from large teachers can perform poorly, by applying MiCoTA, they achieve significant improvements in reasoning performance. Specifically, Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and 3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform a quantitative experiment demonstrating that our method produces data more closely aligned with base SLM distributions. Our insights pave the way for future research into long-CoT data distillation for SLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿éœ€è¦é•¿æœŸæ€è€ƒåºåˆ—è¿›è¡Œè§„åˆ’ã€åæ€å’Œå®Œå–„çš„æ¨ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå®ƒä»¬åºå¤§çš„æ¨¡å‹å°ºå¯¸å’Œé«˜è®¡ç®—éœ€æ±‚ä½¿å¾—éš¾ä»¥å¹¿æ³›éƒ¨ç½²ã€‚ç„¶è€Œï¼Œå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰å¾€å¾€å› å®¹é‡æœ‰é™è€Œéš¾ä»¥å­¦ä¹ é•¿å½¢å¼çš„è¿é”æ¨ç†ï¼ˆCoTï¼‰ï¼Œè¿™ç§ç°è±¡æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œSLMsçš„å¯å­¦ä¹ æ€§å·®è·â€ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†<strong>MiCoTAl</strong>ï¼ˆ<strong>Mi</strong>d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillationï¼Œä¸­é—´å¤§å°çš„æ¨ç†åŠ©æ‰‹è’¸é¦æ¡†æ¶ï¼‰ï¼Œæ—¨åœ¨æ”¹å–„SLMçš„é•¿CoTè’¸é¦ã€‚MiCoTAé‡‡ç”¨ä¸­é—´å¤§å°çš„æ¨¡å‹ä½œä¸ºæ•™å¸ˆåŠ©æ‰‹ï¼Œå¹¶åˆ©ç”¨ä¸­é—´é•¿åº¦çš„CoTåºåˆ—æ¥å¼¥è¡¥å®¹é‡å’Œæ¨ç†é•¿åº¦çš„å·®è·ã€‚æˆ‘ä»¬åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè™½ç„¶ä»å¤§å‹æ•™å¸ˆè’¸é¦çš„SLMè¡¨ç°ä¸ä½³ï¼Œä½†é€šè¿‡åº”ç”¨MiCoTAï¼Œå®ƒä»¬åœ¨æ¨ç†æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚å…·ä½“æ¥è¯´ï¼ŒQwen2.5-7B-Instructå’ŒQwen2.5-3B-Instructåœ¨AIME2024ã€AMCã€Olympiadã€MATH-500å’ŒGSM8KåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡åˆ†æ•°åˆ†åˆ«æé«˜äº†3.47å’Œ3.93ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£MiCoTAçš„æœºåˆ¶ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å®šé‡å®éªŒï¼Œè¯æ˜æˆ‘ä»¬çš„æ–¹æ³•äº§ç”Ÿçš„æ•°æ®ä¸åŸºç¡€SLMåˆ†å¸ƒæ›´åŠ å»åˆã€‚æˆ‘ä»¬çš„è§è§£ä¸ºå°å‹è¯­è¨€æ¨¡å‹çš„é•¿CoTæ•°æ®è’¸é¦çš„æœªæ¥ç ”ç©¶é“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01887v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰åœ¨å­¦ä¹ é•¿å½¢å¼é“¾æ¨ç†ï¼ˆCoTï¼‰æ—¶å­˜åœ¨çš„â€œå­¦ä¹ å·®è·â€é—®é¢˜ï¼Œæå‡ºäº†MiCoTAlæ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä¸­ç­‰è§„æ¨¡æ¨¡å‹ä½œä¸ºæ•™å¸ˆåŠ©æ‰‹ï¼Œå¹¶é€šè¿‡ä¸­é—´é•¿åº¦çš„CoTåºåˆ—æ¥å¼¥è¡¥å®¹é‡å’Œæ¨ç†é•¿åº¦çš„å·®è·ã€‚å®éªŒè¡¨æ˜ï¼Œé€šè¿‡åº”ç”¨MiCoTAï¼ŒSLMåœ¨æ¨ç†æ€§èƒ½ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿éœ€è¦é•¿æ€è€ƒåºåˆ—çš„æ¨ç†ä»»åŠ¡ï¼Œä½†æ¨¡å‹ä½“ç§¯åºå¤§ã€è®¡ç®—éœ€æ±‚é«˜ï¼Œä¸åˆ©äºå¹¿æ³›éƒ¨ç½²ã€‚</li>
<li>å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMï¼‰ç”±äºå®¹é‡æœ‰é™ï¼Œåœ¨å­¦ä¹ é•¿å½¢å¼é“¾æ¨ç†ï¼ˆCoTï¼‰æ—¶å­˜åœ¨å›°éš¾ï¼Œå³â€œSLMsçš„å­¦ä¹ å·®è·â€ã€‚</li>
<li>MiCoTAlæ¡†æ¶æ—¨åœ¨æ”¹è¿›SLMçš„é•¿CoTè’¸é¦ã€‚å®ƒåˆ©ç”¨ä¸­ç­‰è§„æ¨¡æ¨¡å‹ä½œä¸ºæ•™å¸ˆåŠ©æ‰‹ï¼Œå¹¶é‡‡ç”¨ä¸­é—´é•¿åº¦çš„CoTåºåˆ—æ¥æ¡¥æ¥å®¹é‡å’Œæ¨ç†é•¿åº¦çš„å·®è·ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œåº”ç”¨MiCoTAçš„SLMåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„æ¨ç†æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚</li>
<li>Qwen2.5-7B-Instructå’ŒQwen2.5-3B-Instructåœ¨AIME2024ã€AMCã€Olympiadã€MATH-500å’ŒGSM8KåŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡åˆ†æ•°åˆ†åˆ«æé«˜äº†3.47å’Œ3.93ã€‚</li>
<li>é€šè¿‡å®šé‡å®éªŒï¼Œè¯æ˜äº†MiCoTAäº§ç”Ÿçš„æ•°æ®ä¸åŸºç¡€SLMåˆ†å¸ƒæ›´ä¸ºä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01887">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c8636f675acecb66b399a6cbcfd58a85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5d645219bf36f3681da3d41606b0ece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cfc1015e577969c2e2d0c8fe2d424ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bce7d862c0ff37a2a3f1948c8cfea2e5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Eka-Eval-A-Comprehensive-Evaluation-Framework-for-Large-Language-Models-in-Indian-Languages"><a href="#Eka-Eval-A-Comprehensive-Evaluation-Framework-for-Large-Language-Models-in-Indian-Languages" class="headerlink" title="Eka-Eval : A Comprehensive Evaluation Framework for Large Language   Models in Indian Languages"></a>Eka-Eval : A Comprehensive Evaluation Framework for Large Language   Models in Indian Languages</h2><p><strong>Authors:Samridhi Raj Sinha, Rajvee Sheth, Abhishek Upperwal, Mayank Singh</strong></p>
<p>The rapid advancement of Large Language Models (LLMs) has intensified the need for evaluation frameworks that go beyond English centric benchmarks and address the requirements of linguistically diverse regions such as India. We present EKA-EVAL, a unified and production-ready evaluation framework that integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning categories like reasoning, mathematics, tool use, long-context understanding, and reading comprehension. Compared to existing Indian language evaluation tools, EKA-EVAL offers broader benchmark coverage, with built-in support for distributed inference, quantization, and multi-GPU usage. Our systematic comparison positions EKA-EVAL as the first end-to-end, extensible evaluation suite tailored for both global and Indic LLMs, significantly lowering the barrier to multilingual benchmarking. The framework is open-source and publicly available at <a target="_blank" rel="noopener" href="https://github.com/lingo-iitgn/">https://github.com/lingo-iitgn/</a> eka-eval and a part of ongoing EKA initiative (<a target="_blank" rel="noopener" href="https://eka.soket.ai/">https://eka.soket.ai</a>), which aims to scale up to over 100 benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•åŠ å‰§äº†å¯¹è¯„ä¼°æ¡†æ¶çš„éœ€æ±‚ï¼Œè¿™äº›è¯„ä¼°æ¡†æ¶éœ€è¦è¶…è¶Šè‹±è¯­ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶æ»¡è¶³è¯­è¨€å¤šæ ·åœ°åŒºï¼ˆå¦‚å°åº¦ï¼‰çš„éœ€æ±‚ã€‚æˆ‘ä»¬æ¨å‡ºäº†EKA-EVALï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€ä¸”é€‚ç”¨äºç”Ÿäº§çš„è¯„ä¼°æ¡†æ¶ï¼Œé›†æˆäº†è¶…è¿‡35ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬10ä¸ªå°åº¦è¯­ç‰¹å®šæ•°æ®é›†ï¼Œæ¶µç›–æ¨ç†ã€æ•°å­¦ã€å·¥å…·ä½¿ç”¨ã€é•¿æ–‡æœ¬ç†è§£å’Œé˜…è¯»ç†è§£ç­‰ç±»åˆ«ã€‚ä¸ç°æœ‰çš„å°åº¦è¯­è¨€è¯„ä¼°å·¥å…·ç›¸æ¯”ï¼ŒEKA-EVALæä¾›äº†æ›´å¹¿æ³›çš„åŸºå‡†æµ‹è¯•è¦†ç›–ï¼Œå†…ç½®æ”¯æŒåˆ†å¸ƒå¼æ¨ç†ã€é‡åŒ–å’Œå¤šGPUä½¿ç”¨ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿæ¯”è¾ƒæ˜¾ç¤ºï¼ŒEKA-EVALæ˜¯ä¸ºå…¨çƒå’Œå°åº¦è¯­LLMé‡èº«å®šåˆ¶çš„ç¬¬ä¸€ä¸ªç«¯åˆ°ç«¯å¯æ‰©å±•è¯„ä¼°å¥—ä»¶ï¼Œå¤§å¤§é™ä½äº†å¤šè¯­è¨€åŸºå‡†æµ‹è¯•çš„é—¨æ§›ã€‚è¯¥æ¡†æ¶æ˜¯å¼€æºçš„ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lingo-iitgn/eka-eval%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%EF%BC%8C%E5%B9%B6%E4%B8%94%E6%98%AF%E6%AD%A3%E5%9C%A8%E8%BF%9B%E8%A1%8C%E4%B8%AD%E7%9A%84EKA%E5%80%A1%E8%AE%AE%EF%BC%88https://eka.soket.ai%EF%BC%89%E7%9A%84%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%8C%E8%AF%A5%E5%80%A1%E8%AE%AE%E6%97%A8%E5%9C%A8%E6%89%A9%E5%B1%95%E5%88%B0%E8%B6%85%E8%BF%87100%E4%B8%AA%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%EF%BC%8C%E4%B8%BALLM%E5%BB%BA%E7%AB%8B%E7%A8%B3%E5%81%A5%E7%9A%84%E5%A4%9A%E8%AF%AD%E8%A8%80%E8%AF%84%E4%BC%B0%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E3%80%82">https://github.com/lingo-iitgn/eka-evalä¸Šå…¬å¼€è®¿é—®ï¼Œå¹¶ä¸”æ˜¯æ­£åœ¨è¿›è¡Œä¸­çš„EKAå€¡è®®ï¼ˆhttps://eka.soket.aiï¼‰çš„ä¸€éƒ¨åˆ†ï¼Œè¯¥å€¡è®®æ—¨åœ¨æ‰©å±•åˆ°è¶…è¿‡100ä¸ªåŸºå‡†æµ‹è¯•ï¼Œä¸ºLLMå»ºç«‹ç¨³å¥çš„å¤šè¯­è¨€è¯„ä¼°ç”Ÿæ€ç³»ç»Ÿã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01853v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå¯¹è¯„ä¼°æ¡†æ¶çš„éœ€æ±‚æ„ˆå‘è¿«åˆ‡ï¼Œéœ€è¦è¶…è¶Šè‹±è¯­ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶æ»¡è¶³è¯­è¨€å¤šæ ·åœ°åŒºçš„éœ€è¦ï¼Œå¦‚å°åº¦ã€‚æˆ‘ä»¬æ¨å‡ºEKA-EVALè¯„ä¼°æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä¸€ä½“åŒ–å¹¶æ”¯æŒç”Ÿäº§ç¯å¢ƒä½¿ç”¨ã€‚å®ƒé›†æˆäº†è¶…è¿‡35ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬é¢å‘å°åœ°è¯­çš„ç‰¹å®šæ•°æ®é›†10ä¸ªï¼Œæ¶µç›–æ¨ç†ã€æ•°å­¦ã€å·¥å…·ä½¿ç”¨ã€é•¿è¯­å¢ƒç†è§£å’Œé˜…è¯»ç†è§£ç­‰ç±»åˆ«ã€‚ä¸ç°æœ‰çš„å°åº¦è¯­è¨€è¯„ä¼°å·¥å…·ç›¸æ¯”ï¼ŒEKA-EVALæä¾›æ›´å¹¿æ³›çš„åŸºå‡†æµ‹è¯•è¦†ç›–ï¼Œæ”¯æŒåˆ†å¸ƒå¼æ¨ç†ã€é‡åŒ–å’Œå¤šGPUä½¿ç”¨ã€‚é€šè¿‡ç³»ç»Ÿæ€§æ¯”è¾ƒï¼ŒEKA-EVALå®šä½ä¸ºå…¨çƒå’Œå°åœ°è¯­å¤§å‹è¯­è¨€æ¨¡å‹çš„é¦–ä¸ªç«¯åˆ°ç«¯å¯æ‰©å±•è¯„ä¼°å¥—ä»¶ï¼Œæ˜¾è‘—é™ä½å¤šè¯­è¨€åŸºå‡†æµ‹è¯•çš„é—¨æ§›ã€‚è¯¥æ¡†æ¶å¼€æºå¹¶å¯åœ¨å…¬å¼€å¹³å°ä¸Šè·å–ã€‚å®ƒæ˜¯EKAå€¡è®®çš„ä¸€éƒ¨åˆ†ï¼Œæ—¨åœ¨å»ºç«‹å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€è¯„ä¼°ç”Ÿæ€ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EKA-EVALæ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒå¤šç§è¯­è¨€å¹¶ç‰¹åˆ«å…³æ³¨å°åº¦åœ°åŒºçš„éœ€æ±‚ã€‚</li>
<li>è¯¥æ¡†æ¶é›†æˆäº†è¶…è¿‡35ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬é’ˆå¯¹å°åœ°è¯­çš„ç‰¹å®šæ•°æ®é›†ã€‚</li>
<li>EKA-EVALæä¾›äº†å¹¿æ³›çš„åŸºå‡†æµ‹è¯•è¦†ç›–ï¼Œæ”¯æŒåˆ†å¸ƒå¼æ¨ç†ã€é‡åŒ–å’Œå¤šGPUä½¿ç”¨ã€‚</li>
<li>ä¸ç°æœ‰è¯„ä¼°å·¥å…·ç›¸æ¯”ï¼ŒEKA-EVALå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿è¯­å¢ƒç†è§£å’Œé˜…è¯»ç†è§£çš„ç±»åˆ«ä¸Šã€‚</li>
<li>EKA-EVALæ˜¯å¼€æºçš„å¹¶ä¸”å¯ä»¥åœ¨å…¬å¼€å¹³å°ä¸Šè·å–ï¼Œä½œä¸ºEKAå€¡è®®çš„ä¸€éƒ¨åˆ†æ—¨åœ¨æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šè¯­è¨€è¯„ä¼°ç”Ÿæ€ç³»ç»Ÿçš„å‘å±•ã€‚</li>
<li>è¯¥æ¡†æ¶å…·æœ‰å¯æ‰©å±•æ€§ï¼Œæ—¨åœ¨æ”¯æŒè¶…è¿‡ä¸€ç™¾ä¸ªåŸºå‡†æµ‹è¯•çš„æœªæ¥æ‰©å±•éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01853">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3784f5b1902f4072b4971966a995518f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-489befd98e4fcaffbf297e8be45d1586.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-febcdffe3891c682ce65068345b49d93.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="APRMCTS-Improving-LLM-based-Automated-Program-Repair-with-Iterative-Tree-Search"><a href="#APRMCTS-Improving-LLM-based-Automated-Program-Repair-with-Iterative-Tree-Search" class="headerlink" title="APRMCTS: Improving LLM-based Automated Program Repair with Iterative   Tree Search"></a>APRMCTS: Improving LLM-based Automated Program Repair with Iterative   Tree Search</h2><p><strong>Authors:Haichuan Hu, Congqing He, Hao Zhang, Xiaochen Xie, Quanjun Zhang</strong></p>
<p>Automated Program Repair (APR) attempts to fix software bugs without human intervention, which plays a crucial role in software development and maintenance. Recently, with the advances in Large Language Models (LLMs), a rapidly increasing number of APR techniques have been proposed with remarkable performance. However, existing LLM-based APR techniques typically adopt trial-and-error strategies, which suffer from two major drawbacks: (1) inherently limited patch effectiveness due to local exploration, and (2) low search efficiency due to redundant exploration. In this paper, we propose APRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS incorporates Monte Carlo Tree Search (MCTS) into patch searching by performing a global evaluation of the explored patches and selecting the most promising one for subsequent refinement and generation. APRMCTS effectively resolves the problems of falling into local optima and thus helps improve the efficiency of patch searching. Our experiments on 835 bugs from Defects4J demonstrate that, when integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which outperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini, GPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs, respectively. More importantly, APRMCTS boasts a significant performance advantage while employing small patch size (16 and 32), notably fewer than the 500 and 10,000 patches adopted in previous studies. In terms of cost, compared to existing state-of-the-art LLM-based APR methods, APRMCTS has time and monetary costs of less than 20% and 50%, respectively. Our extensive study demonstrates that APRMCTS exhibits good effectiveness and efficiency, with particular advantages in addressing complex bugs. </p>
<blockquote>
<p>è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰æ—¨åœ¨æ— éœ€äººå·¥å¹²é¢„å³å¯ä¿®å¤è½¯ä»¶é”™è¯¯ï¼Œåœ¨è½¯ä»¶å¼€å‘å’Œç»´æŠ¤ä¸­æ‰®æ¼”å…³é”®è§’è‰²ã€‚æœ€è¿‘ï¼Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ï¼Œå·²ç»æå‡ºäº†è¶Šæ¥è¶Šå¤šçš„APRæŠ€æœ¯ï¼Œå¹¶è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŸºäºLLMçš„APRæŠ€æœ¯é€šå¸¸é‡‡ç”¨è¯•é”™ç­–ç•¥ï¼Œè¿™å­˜åœ¨ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹ï¼šï¼ˆ1ï¼‰ç”±äºå±€éƒ¨æ¢ç´¢ï¼Œè¡¥ä¸æœ‰æ•ˆæ€§å›ºæœ‰åœ°æœ‰é™ï¼›ï¼ˆ2ï¼‰ç”±äºå†—ä½™æ¢ç´¢ï¼Œæœç´¢æ•ˆç‡ä½ä¸‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†APRMCTSï¼Œå®ƒä½¿ç”¨è¿­ä»£æ ‘æœç´¢æ¥æ”¹å–„åŸºäºLLMçš„APRã€‚APRMCTSå°†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰èå…¥è¡¥ä¸æœç´¢ï¼Œé€šè¿‡å¯¹æ¢ç´¢çš„è¡¥ä¸è¿›è¡Œå…¨å±€è¯„ä¼°ï¼Œé€‰æ‹©æœ€æœ‰å‰é€”çš„è¡¥ä¸è¿›è¡Œåç»­ç²¾ç‚¼å’Œç”Ÿæˆã€‚APRMCTSæœ‰æ•ˆåœ°è§£å†³äº†é™·å…¥å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜ï¼Œä»è€Œæœ‰åŠ©äºæé«˜è¡¥ä¸æœç´¢çš„æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨Defects4Jçš„835ä¸ªé”™è¯¯ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå½“ä¸GPT-3.5é›†æˆæ—¶ï¼ŒAPRMCTSå¯ä»¥ä¿®å¤æ€»å…±201ä¸ªé”™è¯¯ï¼Œä¼˜äºæ‰€æœ‰æœ€å…ˆè¿›çš„åŸºçº¿ã€‚æ­¤å¤–ï¼ŒAPRMCTSå¸®åŠ©GPT-4o-miniã€GPT-3.5ã€Yi-Coder-9Bå’ŒQwen2.5-Coder-7Båˆ†åˆ«ä¿®å¤äº†30ã€27ã€37å’Œ28ä¸ªé¢å¤–çš„é”™è¯¯ã€‚æ›´é‡è¦çš„æ˜¯ï¼ŒAPRMCTSåœ¨é‡‡ç”¨è¾ƒå°çš„è¡¥ä¸å¤§å°ï¼ˆ16å’Œ32ï¼‰æ—¶ï¼Œå…·æœ‰æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œè¿œè¿œå°‘äºä»¥å‰ç ”ç©¶ä¸­é‡‡ç”¨çš„500å’Œ10000ä¸ªè¡¥ä¸ã€‚å°±æˆæœ¬è€Œè¨€ï¼Œä¸ç°æœ‰çš„æœ€å…ˆè¿›çš„åŸºäºLLMçš„APRæ–¹æ³•ç›¸æ¯”ï¼ŒAPRMCTSçš„æ—¶é—´å’Œé‡‘é’±æˆæœ¬åˆ†åˆ«ä½äº20%å’Œ50%ã€‚æˆ‘ä»¬çš„å¹¿æ³›ç ”ç©¶è¡¨æ˜ï¼ŒAPRMCTSå…·æœ‰è‰¯å¥½çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³å¤æ‚çš„é”™è¯¯æ–¹é¢å…·æœ‰ç‰¹åˆ«çš„ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01827v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰æŠ€æœ¯æ—¨åœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹ä¿®å¤è½¯ä»¶ä¸­çš„é”™è¯¯ï¼Œè¿™åœ¨è½¯ä»¶çš„å¼€å‘å’Œç»´æŠ¤ä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œå·²ç»æå‡ºäº†è®¸å¤šè¡¨ç°ä¼˜å¼‚çš„APRæŠ€æœ¯ã€‚ç„¶è€Œï¼Œç°æœ‰çš„LLM-based APRæŠ€æœ¯é€šå¸¸é‡‡ç”¨è¯•é”™ç­–ç•¥ï¼Œå­˜åœ¨ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹ï¼šä¸€æ˜¯ç”±äºå±€éƒ¨æœç´¢å¯¼è‡´çš„è¡¥ä¸æœ‰æ•ˆæ€§å—é™ï¼ŒäºŒæ˜¯ç”±äºå†—ä½™æœç´¢å¯¼è‡´çš„æœç´¢æ•ˆç‡ä½ä¸‹ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†APRMCTSæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è¿­ä»£æ ‘æœç´¢æ”¹è¿›äº†åŸºäºLLMçš„APRã€‚APRMCTSé€šè¿‡å°†è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰èå…¥è¡¥ä¸æœç´¢ï¼Œé€šè¿‡å¯¹æ¢ç´¢çš„è¡¥ä¸è¿›è¡Œå…¨å±€è¯„ä¼°ï¼Œé€‰æ‹©æœ€æœ‰å‰é€”çš„è¡¥ä¸è¿›è¡Œåç»­å®Œå–„å’Œç”Ÿæˆã€‚APRMCTSæœ‰æ•ˆåœ°è§£å†³äº†é™·å…¥å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜ï¼Œä»è€Œæé«˜äº†è¡¥ä¸æœç´¢çš„æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œä¸GPT-3.5é›†æˆåï¼ŒAPRMCTSå¯ä»¥ä¿®å¤æ›´å¤šçš„é”™è¯¯ï¼Œå¹¶æ˜¾è‘—ä¼˜äºå…¶ä»–æœ€æ–°åŸºçº¿ã€‚æ­¤å¤–ï¼ŒAPRMCTSåœ¨é‡‡ç”¨è¾ƒå°çš„è¡¥ä¸å¤§å°ï¼ˆ16å’Œ32ï¼‰æ—¶ï¼Œæ€§èƒ½ä¼˜åŠ¿æ˜¾è‘—ï¼Œæ˜¾è‘—å°‘äºä»¥å‰ç ”ç©¶ä¸­é‡‡ç”¨çš„500å’Œ10,000ä¸ªè¡¥ä¸ã€‚åœ¨æˆæœ¬å’Œæ•ˆç‡æ–¹é¢ï¼ŒAPRMCTSçš„æ—¶é—´å’Œé‡‘é’±æˆæœ¬åˆ†åˆ«ä½äºç°æœ‰æœ€æ–°LLM-based APRæ–¹æ³•çš„20%å’Œ50%ã€‚æ€»çš„æ¥è¯´ï¼ŒAPRMCTSåœ¨è§£å†³å¤æ‚é”™è¯¯æ–¹é¢è¡¨ç°å‡ºè‰¯å¥½çš„æœ‰æ•ˆæ€§å’Œæ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è‡ªåŠ¨åŒ–ç¨‹åºä¿®å¤ï¼ˆAPRï¼‰æ˜¯è½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸä¸­é‡è¦çš„ç¯èŠ‚ï¼Œå°¤å…¶æ˜¯åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–¹æ³•è¿‘å¹´æ¥å¾—åˆ°äº†å¹¿æ³›å…³æ³¨ã€‚</li>
<li>ç°æœ‰LLM-based APRæŠ€æœ¯ä¸»è¦é‡‡ç”¨è¯•é”™ç­–ç•¥ï¼Œå­˜åœ¨è¡¥ä¸æœ‰æ•ˆæ€§å—é™å’Œæœç´¢æ•ˆç‡ä½ä¸‹çš„é—®é¢˜ã€‚</li>
<li>APRMCTSæ–¹æ³•é€šè¿‡ç»“åˆè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰å’ŒLLMï¼Œæé«˜äº†APRçš„æ•ˆç‡ã€‚</li>
<li>APRMCTSå¯ä»¥è§£å†³å±€éƒ¨æœ€ä¼˜é—®é¢˜ï¼Œæ˜¾è‘—æé«˜è¡¥ä¸æœç´¢æ•ˆæœã€‚</li>
<li>ä¸GPT-3.5é›†æˆåï¼ŒAPRMCTSå¯ä»¥ä¿®å¤æ›´å¤šé”™è¯¯ï¼Œä¼˜äºå…¶ä»–æœ€æ–°åŸºçº¿ã€‚</li>
<li>APRMCTSåœ¨é‡‡ç”¨è¾ƒå°çš„è¡¥ä¸å¤§å°æ—¶è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e172fe93f836db10efd36a38600294f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e61b1e0659e5440a9a010ea98d0b446e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-298cf36603826b6e278419284b78f967.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4814fd2cb91f626f3c86277fa2e8fc4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LoRA-Fine-Tuning-Without-GPUs-A-CPU-Efficient-Meta-Generation-Framework-for-LLMs"><a href="#LoRA-Fine-Tuning-Without-GPUs-A-CPU-Efficient-Meta-Generation-Framework-for-LLMs" class="headerlink" title="LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework   for LLMs"></a>LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework   for LLMs</h2><p><strong>Authors:Reza Arabpour, Haitz SÃ¡ez de OcÃ¡riz Borde, Anastasis Kratsios</strong></p>
<p>Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language Models (LLMs) by enabling parameter-efficient updates. However, their widespread adoption remains limited by the reliance on GPU-based training. In this work, we propose a theoretically grounded approach to LoRA fine-tuning designed specifically for users with limited computational resources, particularly those restricted to standard laptop CPUs. Our method learns a meta-operator that maps any input dataset, represented as a probability distribution, to a set of LoRA weights by leveraging a large bank of pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of performing new gradient-based updates, our pipeline constructs adapters via lightweight combinations of existing LoRAs directly on CPU. While the resulting adapters do not match the performance of GPU-trained counterparts, they consistently outperform the base Mistral model on downstream tasks, offering a practical and accessible alternative to traditional GPU-based fine-tuning. </p>
<blockquote>
<p>ä½ç§©é€‚é…å™¨ï¼ˆLoRAsï¼‰é€šè¿‡å®ç°å‚æ•°é«˜æ•ˆçš„æ›´æ–°ï¼Œå·²ç»æ”¹å˜äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¾®è°ƒæ–¹å¼ã€‚ç„¶è€Œï¼Œå…¶å¹¿æ³›åº”ç”¨ä»å—é™äºå¯¹åŸºäºGPUçš„è®­ç»ƒçš„ä¾èµ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é’ˆå¯¹è®¡ç®—èµ„æºæœ‰é™çš„ç”¨æˆ·ï¼Œç‰¹åˆ«æ˜¯é‚£äº›ä»…é™äºæ ‡å‡†ç¬”è®°æœ¬ç”µè„‘CPUçš„ç”¨æˆ·è®¾è®¡çš„LoRAå¾®è°ƒç†è®ºæ‰å®çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å­¦ä¹ äº†ä¸€ä¸ªå…ƒæ“ä½œç¬¦ï¼Œè¯¥æ“ä½œç¬¦èƒ½å¤Ÿå°†ä»»ä½•è¾“å…¥æ•°æ®é›†æ˜ å°„ä¸ºä¸€ç³»åˆ—LoRAæƒé‡ï¼Œè¿™äº›æƒé‡ä»¥æ¦‚ç‡åˆ†å¸ƒçš„å½¢å¼è¡¨ç¤ºï¼Œå¹¶åˆ©ç”¨å¤§é‡é¢„è®­ç»ƒçš„é€‚é…å™¨æ¥ä¸ºMistral-7B-Instruct-v0.2æ¨¡å‹æœåŠ¡ã€‚æˆ‘ä»¬çš„ç®¡é“ä¸æ˜¯åœ¨CPUä¸Šæ‰§è¡Œæ–°çš„åŸºäºæ¢¯åº¦çš„æ›´æ–°ï¼Œè€Œæ˜¯é€šè¿‡è½»é‡çº§ç»„åˆç°æœ‰çš„LoRAsç›´æ¥æ„å»ºé€‚é…å™¨ã€‚è™½ç„¶ç”Ÿæˆçš„é€‚é…å™¨æ€§èƒ½ä¸å¦‚åœ¨GPUä¸Šè®­ç»ƒçš„å¯¹åº”ç‰©ï¼Œä½†å®ƒä»¬åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºåŸºç¡€Mistralæ¨¡å‹ï¼Œä¸ºä¼ ç»Ÿçš„åŸºäºGPUçš„å¾®è°ƒæä¾›äº†ä¸€ä¸ªå®ç”¨ä¸”å¯è®¿é—®çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01806v1">PDF</a> 5-page main paper (excluding references) + 11-page appendix, 3   tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for   Foundation Models</p>
<p><strong>Summary</strong></p>
<p>ä½ç§©é€‚é…å™¨ï¼ˆLoRAsï¼‰å®ç°äº†å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç²¾ç»†è°ƒæ•´ï¼Œå®ç°äº†å‚æ•°é«˜æ•ˆçš„æ›´æ–°ã€‚ç„¶è€Œï¼Œå…¶å¹¿æ³›åº”ç”¨ä»ç„¶å—é™äºå¯¹åŸºäºGPUçš„è®­ç»ƒçš„ä¾èµ–ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹è®¡ç®—èµ„æºæœ‰é™çš„ç”¨æˆ·ï¼Œç‰¹åˆ«æ˜¯ä»…é™äºæ ‡å‡†ç¬”è®°æœ¬ç”µè„‘CPUçš„ç”¨æˆ·è¿›è¡ŒLoRAç²¾ç»†è°ƒæ•´çš„ç†è®ºä¾æ®æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å­¦ä¹ ä¸€ä¸ªå…ƒæ“ä½œç¬¦ï¼Œå®ƒå°†ä»»ä½•è¾“å…¥æ•°æ®é›†æ˜ å°„ä¸ºä¸€ç³»åˆ—LoRAæƒé‡ï¼Œé€šè¿‡åˆ©ç”¨å¤§é‡é¢„è®­ç»ƒé€‚é…å™¨æ¥å®ç°å¯¹Mistral-7B-Instruct-v0.2æ¨¡å‹çš„æ˜ å°„ã€‚æˆ‘ä»¬çš„ç®¡é“é€šè¿‡ç›´æ¥åœ¨CPUä¸Šç»“åˆç°æœ‰çš„è½»é‡çº§LoRAsæ¥æ„å»ºé€‚é…å™¨ï¼Œè€Œä¸æ˜¯æ‰§è¡Œæ–°çš„åŸºäºæ¢¯åº¦çš„æ›´æ–°ï¼Œè™½ç„¶ç”Ÿæˆçš„é€‚é…å™¨æ€§èƒ½ä¸å¦‚GPUè®­ç»ƒçš„å¯¹åº”ç‰©ï¼Œä½†å®ƒä»¬åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šå§‹ç»ˆä¼˜äºåŸºæœ¬Mistralæ¨¡å‹ï¼Œä¸ºä¼ ç»Ÿçš„åŸºäºGPUçš„å¾®è°ƒæä¾›äº†ä¸€ç§å®ç”¨ä¸”å¯è®¿é—®çš„æ›¿ä»£æ–¹æ¡ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoRAså®ç°äº†LLMsçš„ç²¾ç»†è°ƒæ•´ï¼Œå¹¶æ¨åŠ¨äº†å‚æ•°é«˜æ•ˆçš„æ›´æ–°ã€‚</li>
<li>LoRAsçš„å¹¿æ³›åº”ç”¨å—é™äºå¯¹GPUè®­ç»ƒçš„ä¾èµ–ã€‚</li>
<li>é’ˆå¯¹è®¡ç®—èµ„æºæœ‰é™çš„ç”¨æˆ·ï¼ˆç‰¹åˆ«æ˜¯æ ‡å‡†ç¬”è®°æœ¬ç”µè„‘CPUç”¨æˆ·ï¼‰ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„LoRAç²¾ç»†è°ƒæ•´æ–¹æ³•ã€‚</li>
<li>æ–°æ–¹æ³•é€šè¿‡å­¦ä¹ ä¸€ä¸ªå…ƒæ“ä½œç¬¦æ¥æ˜ å°„è¾“å…¥æ•°æ®é›†åˆ°LoRAæƒé‡ï¼Œåˆ©ç”¨é¢„è®­ç»ƒé€‚é…å™¨å®ç°ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç»“åˆç°æœ‰è½»é‡çº§LoRAsæ„å»ºé€‚é…å™¨ï¼Œé¿å…äº†æ–°çš„åŸºäºæ¢¯åº¦çš„æ›´æ–°ã€‚</li>
<li>è™½ç„¶æ–°ç”Ÿæˆçš„é€‚é…å™¨æ€§èƒ½ä¸å¦‚GPUè®­ç»ƒçš„é€‚é…å™¨ï¼Œä½†å®ƒä»¬åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºåŸºç¡€æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01806">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8586e1bf45b3f8d7d28df4de62b36681.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a046cef9960fc7e9f84f93d2db198862.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="HCNQA-Enhancing-3D-VQA-with-Hierarchical-Concentration-Narrowing-Supervision"><a href="#HCNQA-Enhancing-3D-VQA-with-Hierarchical-Concentration-Narrowing-Supervision" class="headerlink" title="HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing   Supervision"></a>HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing   Supervision</h2><p><strong>Authors:Shengli Zhou, Jianuo Zhu, Qilin Huang, Fangjing Wang, Yanfu Zhang, Feng Zheng</strong></p>
<p>3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the physical world and perform spatial reasoning. Answer-centric supervision is a commonly used training method for 3D VQA models. Many models that utilize this strategy have achieved promising results in 3D VQA tasks. However, the answer-centric approach only supervises the final output of models and allows models to develop reasoning pathways freely. The absence of supervision on the reasoning pathway enables the potential for developing superficial shortcuts through common patterns in question-answer pairs. Moreover, although slow-thinking methods advance large language models, they suffer from underthinking. To address these issues, we propose \textbf{HCNQA}, a 3D VQA model leveraging a hierarchical concentration narrowing supervision method. By mimicking the human process of gradually focusing from a broad area to specific objects while searching for answers, our method guides the model to perform three phases of concentration narrowing through hierarchical supervision. By supervising key checkpoints on a general reasoning pathway, our method can ensure the development of a rational and effective reasoning pathway. Extensive experimental results demonstrate that our method can effectively ensure that the model develops a rational reasoning pathway and performs better. The code is available at <a target="_blank" rel="noopener" href="https://github.com/JianuoZhu/HCNQA">https://github.com/JianuoZhu/HCNQA</a>. </p>
<blockquote>
<p>3Dè§†è§‰é—®ç­”ï¼ˆ3D VQAï¼‰å¯¹äºæ¨¡å‹æ„ŸçŸ¥ç‰©ç†ä¸–ç•Œå¹¶è¿›è¡Œç©ºé—´æ¨ç†è‡³å…³é‡è¦ã€‚ä»¥ç­”æ¡ˆä¸ºä¸­å¿ƒçš„ç›‘ç£æ˜¯3D VQAæ¨¡å‹å¸¸ç”¨çš„è®­ç»ƒæ–¹æ³•ã€‚è®¸å¤šé‡‡ç”¨è¿™ç§ç­–ç•¥çš„æ¨¡å‹åœ¨3D VQAä»»åŠ¡ä¸­å–å¾—äº†ä»¤äººé¼“èˆçš„ç»“æœã€‚ç„¶è€Œï¼Œä»¥ç­”æ¡ˆä¸ºä¸­å¿ƒçš„æ–¹æ³•åªç›‘ç£æ¨¡å‹çš„æœ€ç»ˆè¾“å‡ºï¼Œå…è®¸æ¨¡å‹è‡ªç”±å‘å±•æ¨ç†è·¯å¾„ã€‚æ¨ç†è·¯å¾„ä¸Šç¼ºä¹ç›‘ç£ä½¿å¾—æ¨¡å‹å¯èƒ½é€šè¿‡é—®ç­”å¯¹ä¸­çš„å¸¸è§æ¨¡å¼å½¢æˆæµ…å±‚æ¬¡çš„æ·å¾„ã€‚æ­¤å¤–ï¼Œå°½ç®¡æ…¢æ€è€ƒæ–¹æ³•ä¿ƒè¿›äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œä½†å®ƒä»¬å´å­˜åœ¨æ€è€ƒä¸è¶³çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†<strong>HCNQA</strong>ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨åˆ†å±‚é›†ä¸­ç¼©å°ç›‘ç£æ–¹æ³•çš„3D VQAæ¨¡å‹ã€‚é€šè¿‡æ¨¡ä»¿äººç±»åœ¨å¯»æ‰¾ç­”æ¡ˆæ—¶ä»å¹¿é˜”åŒºåŸŸé€æ¸èšç„¦åˆ°ç‰¹å®šå¯¹è±¡çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡åˆ†å±‚ç›‘ç£å¼•å¯¼æ¨¡å‹è¿›è¡Œä¸‰ä¸ªé˜¶æ®µé›†ä¸­ç¼©å°ã€‚é€šè¿‡å¯¹ä¸€èˆ¬æ¨ç†è·¯å¾„ä¸Šçš„å…³é”®æ£€æŸ¥ç‚¹è¿›è¡Œç›‘ç£ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç¡®ä¿å½¢æˆåˆç†æœ‰æ•ˆçš„æ¨ç†è·¯å¾„ã€‚å¤§é‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°ç¡®ä¿æ¨¡å‹å½¢æˆåˆç†çš„æ¨ç†è·¯å¾„å¹¶è¡¨ç°æ›´å¥½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JianuoZhu/HCNQA">https://github.com/JianuoZhu/HCNQA</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01800v1">PDF</a> ICANN 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä¸‰ç»´è§†è§‰é—®ç­”ï¼ˆ3D VQAï¼‰ä¸­ç­”æ¡ˆä¸­å¿ƒç›‘ç£æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¨¡å‹HCNQAã€‚è¯¥æ¨¡å‹é‡‡ç”¨å±‚æ¬¡åŒ–é›†ä¸­ç¼©å°ç›‘ç£æ–¹æ³•ï¼Œé€šè¿‡æ¨¡ä»¿äººç±»å¯»æ‰¾ç­”æ¡ˆæ—¶çš„èšç„¦è¿‡ç¨‹ï¼Œå¼•å¯¼æ¨¡å‹è¿›è¡Œä¸‰ä¸ªé˜¶æ®µé›†ä¸­ç¼©å°ã€‚é€šè¿‡ç›‘ç£ä¸€èˆ¬æ¨ç†è·¯å¾„çš„å…³é”®æ£€æŸ¥ç‚¹ï¼Œç¡®ä¿æ¨¡å‹å‘å±•å‡ºåˆç†æœ‰æ•ˆçš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæé«˜æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D VQAæ˜¯æ¨¡å‹æ„ŸçŸ¥ç°å®ä¸–ç•Œå¹¶è¿›è¡Œç©ºé—´æ¨ç†çš„å…³é”®ã€‚</li>
<li>ç­”æ¡ˆä¸­å¿ƒç›‘ç£æ˜¯3D VQAæ¨¡å‹çš„å¸¸ç”¨è®­ç»ƒæ–¹æ³•ï¼Œè™½ç„¶å–å¾—äº†ä¸€å®šçš„æˆæœï¼Œä½†å­˜åœ¨æ½œåœ¨é—®é¢˜ã€‚</li>
<li>ç­”æ¡ˆä¸­å¿ƒç›‘ç£å…è®¸æ¨¡å‹è‡ªç”±å‘å±•æ¨ç†è·¯å¾„ï¼Œå¯èƒ½å¯¼è‡´è¡¨é¢æ·å¾„å’Œæ— æ•ˆç­”æ¡ˆã€‚</li>
<li>HCNQAæ˜¯ä¸€ä¸ªæ–°çš„ä¸‰ç»´è§†è§‰é—®ç­”æ¨¡å‹ï¼Œé‡‡ç”¨å±‚æ¬¡åŒ–é›†ä¸­ç¼©å°ç›‘ç£æ–¹æ³•ã€‚</li>
<li>HCNQAé€šè¿‡æ¨¡ä»¿äººç±»å¯»æ‰¾ç­”æ¡ˆæ—¶çš„èšç„¦è¿‡ç¨‹ï¼Œå¼•å¯¼æ¨¡å‹è¿›è¡Œä¸‰ä¸ªé˜¶æ®µé›†ä¸­ç¼©å°ã€‚</li>
<li>HCNQAé€šè¿‡ç›‘ç£ä¸€èˆ¬æ¨ç†è·¯å¾„çš„å…³é”®æ£€æŸ¥ç‚¹ï¼Œç¡®ä¿æ¨¡å‹å‘å±•å‡ºåˆç†æœ‰æ•ˆçš„æ¨ç†è·¯å¾„ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01800">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a1f17cdb5fb7d613fc01d3ac92c9f75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d08569ad7898f6d27b83eca57d7cfeb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8aeeda61eb63c1cd4687eb5d6a3f05a6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Rethinking-Discrete-Tokens-Treating-Them-as-Conditions-for-Continuous-Autoregressive-Image-Synthesis"><a href="#Rethinking-Discrete-Tokens-Treating-Them-as-Conditions-for-Continuous-Autoregressive-Image-Synthesis" class="headerlink" title="Rethinking Discrete Tokens: Treating Them as Conditions for Continuous   Autoregressive Image Synthesis"></a>Rethinking Discrete Tokens: Treating Them as Conditions for Continuous   Autoregressive Image Synthesis</h2><p><strong>Authors:Peng Zheng, Junke Wang, Yi Chang, Yizhou Yu, Rui Ma, Zuxuan Wu</strong></p>
<p>Recent advances in large language models (LLMs) have spurred interests in encoding images as discrete tokens and leveraging autoregressive (AR) frameworks for visual generation. However, the quantization process in AR-based visual generation models inherently introduces information loss that degrades image fidelity. To mitigate this limitation, recent studies have explored to autoregressively predict continuous tokens. Unlike discrete tokens that reside in a structured and bounded space, continuous representations exist in an unbounded, high-dimensional space, making density estimation more challenging and increasing the risk of generating out-of-distribution artifacts. Based on the above findings, this work introduces DisCon (Discrete-Conditioned Continuous Autoregressive Model), a novel framework that reinterprets discrete tokens as conditional signals rather than generation targets. By modeling the conditional probability of continuous representations conditioned on discrete tokens, DisCon circumvents the optimization challenges of continuous token modeling while avoiding the information loss caused by quantization. DisCon achieves a gFID score of 1.38 on ImageNet 256$\times$256 generation, outperforming state-of-the-art autoregressive approaches by a clear margin. </p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æ¿€å‘äº†å°†å›¾åƒç¼–ç ä¸ºç¦»æ•£æ ‡è®°å¹¶åˆ©ç”¨è‡ªå›å½’ï¼ˆARï¼‰æ¡†æ¶è¿›è¡Œè§†è§‰ç”Ÿæˆçš„å…´è¶£ã€‚ç„¶è€Œï¼ŒARå‹è§†è§‰ç”Ÿæˆæ¨¡å‹ä¸­çš„é‡åŒ–è¿‡ç¨‹ä¼šå›ºæœ‰åœ°å¼•å…¥ä¿¡æ¯æŸå¤±ï¼Œé™ä½å›¾åƒä¿çœŸåº¦ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€å±€é™æ€§ï¼Œè¿‘æœŸç ”ç©¶å·²ç»æ¢ç´¢äº†è‡ªå›å½’é¢„æµ‹è¿ç»­æ ‡è®°çš„æ–¹æ³•ã€‚ä¸å±…ä½åœ¨ç»“æ„åŒ–æœ‰ç•Œç©ºé—´ä¸­çš„ç¦»æ•£æ ‡è®°ä¸åŒï¼Œè¿ç»­è¡¨ç¤ºå­˜åœ¨äºæ— ç•Œçš„é«˜ç»´ç©ºé—´ä¸­ï¼Œè¿™ä½¿å¾—å¯†åº¦ä¼°è®¡æ›´å…·æŒ‘æˆ˜æ€§ï¼Œå¹¶å¢åŠ äº†ç”Ÿæˆç¦»ç¾¤åˆ†å¸ƒä¼ªå½±çš„é£é™©ã€‚åŸºäºä¸Šè¿°å‘ç°ï¼Œè¿™é¡¹å·¥ä½œå¼•å…¥äº†DisConï¼ˆç¦»æ•£æ¡ä»¶è¿ç»­è‡ªå›å½’æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–æ¡†æ¶ï¼Œå®ƒå°†ç¦»æ•£æ ‡è®°é‡æ–°è§£é‡Šä¸ºæ¡ä»¶ä¿¡å·è€Œä¸æ˜¯ç”Ÿæˆç›®æ ‡ã€‚é€šè¿‡å¯¹ç¦»æ•£æ ‡è®°æ¡ä»¶ä¸‹çš„è¿ç»­è¡¨ç¤ºçš„æœ‰æ¡ä»¶æ¦‚ç‡å»ºæ¨¡ï¼ŒDisConè§„é¿äº†è¿ç»­æ ‡è®°å»ºæ¨¡çš„ä¼˜åŒ–æŒ‘æˆ˜ï¼ŒåŒæ—¶é¿å…äº†é‡åŒ–å¼•èµ·çš„ä¿¡æ¯æŸå¤±ã€‚DisConåœ¨ImageNet 256x256ç”Ÿæˆä»»åŠ¡ä¸Šå®ç°äº†1.38çš„gFIDåˆ†æ•°ï¼Œç›¸è¾ƒäºæœ€å…ˆè¿›çš„å…¶ä»–è‡ªå›å½’æ–¹æ³•æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01756v1">PDF</a> accepted by iccv 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•æ¿€å‘äº†å°†å›¾åƒç¼–ç ä¸ºç¦»æ•£æ ‡è®°å¹¶åˆ©ç”¨è‡ªå›å½’ï¼ˆARï¼‰æ¡†æ¶è¿›è¡Œè§†è§‰ç”Ÿæˆçš„å…´è¶£ã€‚ç„¶è€Œï¼ŒAR-basedè§†è§‰ç”Ÿæˆæ¨¡å‹çš„é‡åŒ–è¿‡ç¨‹ä¼šå›ºæœ‰åœ°é€ æˆä¿¡æ¯æŸå¤±ï¼Œé™ä½å›¾åƒä¿çœŸåº¦ã€‚ä¸ºè§£å†³æ­¤é™åˆ¶ï¼Œè¿‘æœŸç ”ç©¶å¼€å§‹æ¢ç´¢è‡ªå›å½’é¢„æµ‹è¿ç»­æ ‡è®°ã€‚ä¸åŒäºå­˜åœ¨äºç»“æ„åŒ–æœ‰ç•Œç©ºé—´ä¸­çš„ç¦»æ•£æ ‡è®°ï¼Œè¿ç»­è¡¨ç¤ºå­˜åœ¨äºæ— ç•Œçš„é«˜ç»´ç©ºé—´ä¸­ï¼Œä½¿å¾—å¯†åº¦ä¼°è®¡æ›´å…·æŒ‘æˆ˜æ€§ï¼Œå¹¶å¢åŠ äº†ç”Ÿæˆç¦»ç¾¤åˆ†å¸ƒä¼ªå½±çš„é£é™©ã€‚åŸºäºä¸Šè¿°å‘ç°ï¼Œæœ¬æ–‡ä»‹ç»äº†DisConï¼ˆç¦»æ•£æ¡ä»¶è¿ç»­è‡ªå›å½’æ¨¡å‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒå°†ç¦»æ•£æ ‡è®°é‡æ–°è§£é‡Šä¸ºæ¡ä»¶ä¿¡å·è€Œä¸æ˜¯ç”Ÿæˆç›®æ ‡ã€‚é€šè¿‡å»ºæ¨¡ç¦»æ•£æ ‡è®°æ¡ä»¶ä¸‹çš„è¿ç»­è¡¨ç¤ºçš„æ¡ä»¶æ¦‚ç‡ï¼ŒDisConè§„é¿äº†è¿ç»­æ ‡è®°å»ºæ¨¡çš„ä¼˜åŒ–æŒ‘æˆ˜ï¼ŒåŒæ—¶é¿å…äº†é‡åŒ–é€ æˆçš„ä¿¡æ¯æŸå¤±ã€‚DisConåœ¨ImageNet 256Ã—256ç”Ÿæˆä¸Šå®ç°äº†1.38çš„gFIDåˆ†æ•°ï¼Œè¾ƒå…ˆè¿›çš„è‡ªå›å½’æ–¹æ³•æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å›¾åƒç¼–ç å’Œè§†è§‰ç”Ÿæˆé¢†åŸŸå¼•å‘å…³æ³¨ã€‚</li>
<li>è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹åœ¨è§†è§‰ç”Ÿæˆä¸­çš„é‡åŒ–è¿‡ç¨‹ä¼šå¼•å…¥ä¿¡æ¯æŸå¤±ã€‚</li>
<li>è¿ç»­è¡¨ç¤ºè¾ƒç¦»æ•£è¡¨ç¤ºæ›´åŠ å¤æ‚ï¼Œå¯†åº¦ä¼°è®¡æ›´å…·æŒ‘æˆ˜æ€§ï¼Œå­˜åœ¨ç”Ÿæˆç¦»ç¾¤åˆ†å¸ƒä¼ªå½±çš„é£é™©ã€‚</li>
<li>DisConæ¡†æ¶å°†ç¦»æ•£æ ‡è®°é‡æ–°è§£é‡Šä¸ºæ¡ä»¶ä¿¡å·ï¼Œè§„é¿äº†è¿ç»­æ ‡è®°å»ºæ¨¡çš„æŒ‘æˆ˜ã€‚</li>
<li>DisConå®ç°äº†åœ¨ImageNetä¸Šçš„é«˜ä¿çœŸå›¾åƒç”Ÿæˆï¼ŒgFIDåˆ†æ•°è¾ƒä½ï¼Œæ€§èƒ½ä¼˜è¶Šã€‚</li>
<li>DisConæ¡†æ¶åœ¨é¿å…ä¿¡æ¯æŸå¤±çš„åŒæ—¶ï¼Œå®ç°äº†ä¼˜åŒ–çš„è§†è§‰æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01756">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-300693c6d62a3f67fe18cf9b4350305c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1af5d28ff1b027bd1205b7c43d0849d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e59062e0d0aea0528e16e8f092cbe383.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ecc1a7727398dadec47a17f7ed05e7d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b710246ce0a3158d27024ba3e38c97d0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LLMs-for-Legal-Subsumption-in-German-Employment-Contracts"><a href="#LLMs-for-Legal-Subsumption-in-German-Employment-Contracts" class="headerlink" title="LLMs for Legal Subsumption in German Employment Contracts"></a>LLMs for Legal Subsumption in German Employment Contracts</h2><p><strong>Authors:Oliver Wardas, Florian Matthes</strong></p>
<p>Legal work, characterized by its text-heavy and resource-intensive nature, presents unique challenges and opportunities for NLP research. While data-driven approaches have advanced the field, their lack of interpretability and trustworthiness limits their applicability in dynamic legal environments. To address these issues, we collaborated with legal experts to extend an existing dataset and explored the use of Large Language Models (LLMs) and in-context learning to evaluate the legality of clauses in German employment contracts. Our work evaluates the ability of different LLMs to classify clauses as â€œvalid,â€ â€œunfair,â€ or â€œvoidâ€ under three legal context variants: no legal context, full-text sources of laws and court rulings, and distilled versions of these (referred to as examination guidelines). Results show that full-text sources moderately improve performance, while examination guidelines significantly enhance recall for void clauses and weighted F1-Score, reaching 80%. Despite these advancements, LLMsâ€™ performance when using full-text sources remains substantially below that of human lawyers. We contribute an extended dataset, including examination guidelines, referenced legal sources, and corresponding annotations, alongside our code and all log files. Our findings highlight the potential of LLMs to assist lawyers in contract legality review while also underscoring the limitations of the methods presented. </p>
<blockquote>
<p>æ³•å¾‹å·¥ä½œä»¥å…¶æ–‡æœ¬å¯†é›†å’Œèµ„æºå¯†é›†çš„ç‰¹ç‚¹å‘ˆç°å‡ºç‹¬ç‰¹çš„æŒ‘æˆ˜å’Œæœºä¼šï¼Œå¯¹è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶äº¦æ˜¯å¦‚æ­¤ã€‚å°½ç®¡æ•°æ®é©±åŠ¨çš„æ–¹æ³•æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„å‘å±•ï¼Œä½†ç”±äºå…¶ç¼ºä¹å¯è§£é‡Šæ€§å’Œå¯ä¿¡åº¦ï¼Œåœ¨åŠ¨æ€çš„æ³•å¾‹ç¯å¢ƒä¸­å…¶é€‚ç”¨æ€§æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¸æ³•å¾‹ä¸“å®¶åˆä½œæ‰©å±•äº†ç°æœ‰æ•°æ®é›†ï¼Œå¹¶æ¢ç´¢äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ æ¥è¯„ä¼°å¾·å›½å°±ä¸šåˆåŒæ¡æ¬¾çš„åˆæ³•æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œè¯„ä¼°äº†ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸‰ç§æ³•å¾‹ä¸Šä¸‹æ–‡å˜ä½“ä¸‹å°†æ¡æ¬¾åˆ†ç±»ä¸ºâ€œæœ‰æ•ˆâ€ï¼Œâ€œä¸å…¬å¹³â€æˆ–â€œæ— æ•ˆâ€çš„èƒ½åŠ›ï¼šæ— æ³•å¾‹ä¸Šä¸‹æ–‡ã€æ³•å¾‹ä¸æ³•é™¢åˆ¤å†³çš„å…¨æ–‡æ¥æºä»¥åŠè¿™äº›å†…å®¹çš„æç‚¼ç‰ˆæœ¬ï¼ˆç§°ä¸ºæ£€æŸ¥æŒ‡å—ï¼‰ã€‚ç»“æœè¡¨æ˜ï¼Œå…¨æ–‡æ¥æºå¯ä»¥é€‚åº¦æé«˜æ€§èƒ½ï¼Œè€Œæ£€æŸ¥æŒ‡å—å¯ä»¥æ˜¾è‘—æé«˜æ— æ•ˆæ¡æ¬¾çš„å¬å›ç‡å’ŒåŠ æƒF1åˆ†æ•°ï¼Œè¾¾åˆ°80%ã€‚å°½ç®¡æœ‰æ‰€è¿›å±•ï¼Œä½†åœ¨ä½¿ç”¨å…¨æ–‡æ¥æºæ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ä»ç„¶è¿œè¿œä½äºäººç±»å¾‹å¸ˆã€‚æˆ‘ä»¬è´¡çŒ®äº†ä¸€ä¸ªæ‰©å±•çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ£€æŸ¥æŒ‡å—ã€å¼•ç”¨çš„æ³•å¾‹æ¥æºå’Œç›¸åº”æ³¨é‡Šï¼Œä»¥åŠæˆ‘ä»¬çš„ä»£ç å’Œæ‰€æœ‰æ—¥å¿—æ–‡ä»¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ååŠ©å¾‹å¸ˆå®¡æŸ¥åˆåŒåˆæ³•æ€§æ–¹é¢çš„æ½œåŠ›ï¼ŒåŒæ—¶ä¹Ÿå¼ºè°ƒäº†æ‰€æå‡ºæ–¹æ³•çš„å±€é™æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01734v1">PDF</a> PrePrint - ICAIL25, Chicago</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†æ³•å¾‹å·¥ä½œå¯¹è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ç ”ç©¶çš„ç‹¬ç‰¹æŒ‘æˆ˜å’Œæœºé‡ã€‚æ•°æ®é©±åŠ¨çš„æ–¹æ³•è™½ç„¶æ¨åŠ¨äº†è¯¥é¢†åŸŸçš„å‘å±•ï¼Œä½†å…¶ç¼ºä¹è§£é‡Šæ€§å’Œå¯ä¿¡åº¦ï¼Œé™åˆ¶äº†å…¶åœ¨åŠ¨æ€æ³•å¾‹ç¯å¢ƒä¸­çš„é€‚ç”¨æ€§ã€‚ä¸ºç ”ç©¶è§£å†³è¿™äº›é—®é¢˜ï¼Œç ”ç©¶è€…ä¸æ³•å¾‹ä¸“å®¶åˆä½œæ‰©å±•ç°æœ‰æ•°æ®é›†ï¼Œæ¢ç´¢ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œä¸Šä¸‹æ–‡å­¦ä¹ æ¥è¯„ä¼°å¾·å›½å°±ä¸šåˆåŒæ¡æ¬¾çš„åˆæ³•æ€§ã€‚ç ”ç©¶è¯„ä¼°äº†ä¸åŒLLMåœ¨ä¸‰ç§æ³•å¾‹ä¸Šä¸‹æ–‡ä¸‹å¯¹æ¡æ¬¾è¿›è¡Œåˆ†ç±»çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ— æ³•å¾‹ä¸Šä¸‹æ–‡ã€å…¨æ–‡æ³•å¾‹æ¥æºå’Œæ³•é™¢åˆ¤å†³ä»¥åŠç²¾ç®€ç‰ˆï¼ˆç§°ä¸ºå®¡æŸ¥æŒ‡å—ï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œå…¨æ–‡æ¥æºé€‚åº¦æé«˜äº†æ€§èƒ½ï¼Œè€Œå®¡æŸ¥æŒ‡å—æ˜¾è‘—æé«˜äº†æ— æ•ˆæ¡æ¬¾çš„å¬å›ç‡å’ŒåŠ æƒF1åˆ†æ•°ï¼Œè¾¾åˆ°80%ã€‚å°½ç®¡å–å¾—äº†è¿™äº›è¿›å±•ï¼Œä½†ä½¿ç”¨å…¨æ–‡æ¥æºçš„LLMæ€§èƒ½ä»ç„¶è¿œä½äºäººç±»å¾‹å¸ˆçš„è¡¨ç°ã€‚ç ”ç©¶è€…è´¡çŒ®äº†ä¸€ä¸ªæ‰©å±•çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬å®¡æŸ¥æŒ‡å—ã€å‚è€ƒæ³•å¾‹æ¥æºå’Œç›¸åº”æ³¨é‡Šï¼Œä»¥åŠä»£ç å’Œæ‰€æœ‰æ—¥å¿—æ–‡ä»¶ã€‚ç ”ç©¶å‘ç°çªæ˜¾äº†LLMåœ¨ååŠ©å¾‹å¸ˆå®¡æŸ¥åˆåŒåˆæ³•æ€§æ–¹é¢çš„æ½œåŠ›ï¼ŒåŒæ—¶ä¹Ÿå¼ºè°ƒäº†æ‰€æå‡ºæ–¹æ³•çš„å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æ³•å¾‹å·¥ä½œå¯¹NLPç ”ç©¶å¸¦æ¥ç‹¬ç‰¹æŒ‘æˆ˜å’Œæœºé‡ï¼Œæ•°æ®é©±åŠ¨æ–¹æ³•éœ€ç»“åˆæ³•å¾‹ä¸“ä¸šçŸ¥è¯†ã€‚</li>
<li>LLMså’Œä¸Šä¸‹æ–‡å­¦ä¹ è¢«ç”¨äºè¯„ä¼°å¾·å›½å°±ä¸šåˆåŒæ¡æ¬¾çš„åˆæ³•æ€§ã€‚</li>
<li>ä¸‰ç§æ³•å¾‹ä¸Šä¸‹æ–‡ä¸‹è¯„ä¼°äº†LLMsçš„æ€§èƒ½ï¼šæ— ä¸Šä¸‹æ–‡ã€å…¨æ–‡æ³•å¾‹æ¥æºå’Œå®¡æŸ¥æŒ‡å—ã€‚</li>
<li>å®¡æŸ¥æŒ‡å—èƒ½æ˜¾è‘—æé«˜LLMsçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯†åˆ«æ— æ•ˆæ¡æ¬¾æ–¹é¢ã€‚</li>
<li>LLMsåœ¨åˆåŒåˆæ³•æ€§å®¡æŸ¥æ–¹é¢çš„æ€§èƒ½ä»è¿œä½äºäººç±»å¾‹å¸ˆã€‚</li>
<li>è´¡çŒ®äº†ä¸€ä¸ªæ‰©å±•æ•°æ®é›†ï¼ŒåŒ…å«å®¡æŸ¥æŒ‡å—ã€æ³•å¾‹æ¥æºå’Œæ³¨é‡Šç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01734">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-0bb7f2f3e10798a0f069f5620a481416.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-73d9500d3359c9a296bd5a70a7885ac8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d105f7a3e7482f34a16a49c0382f96bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9dcbce2572606654652bf36f143b37d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dd2644cdf79c9b729fbc899a398b783.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Token-Communication-in-the-Era-of-Large-Models-An-Information-Bottleneck-Based-Approach"><a href="#Token-Communication-in-the-Era-of-Large-Models-An-Information-Bottleneck-Based-Approach" class="headerlink" title="Token Communication in the Era of Large Models: An Information   Bottleneck-Based Approach"></a>Token Communication in the Era of Large Models: An Information   Bottleneck-Based Approach</h2><p><strong>Authors:Hao Wei, Wanli Ni, Wen Wang, Wenjun Xu, Dusit Niyato, Ping Zhang</strong></p>
<p>This letter proposes UniToCom, a unified token communication paradigm that treats tokens as the fundamental units for both processing and wireless transmission. Specifically, to enable efficient token representations, we propose a generative information bottleneck (GenIB) principle, which facilitates the learning of tokens that preserve essential information while supporting reliable generation across multiple modalities. By doing this, GenIB-based tokenization is conducive to improving the communication efficiency and reducing computational complexity. Additionally, we develop $\sigma$-GenIB to address the challenges of variance collapse in autoregressive modeling, maintaining representational diversity and stability. Moreover, we employ a causal Transformer-based multimodal large language model (MLLM) at the receiver to unify the processing of both discrete and continuous tokens under the next-token prediction paradigm. Simulation results validate the effectiveness and superiority of the proposed UniToCom compared to baselines under dynamic channel conditions. By integrating token processing with MLLMs, UniToCom enables scalable and generalizable communication in favor of multimodal understanding and generation, providing a potential solution for next-generation intelligent communications. </p>
<blockquote>
<p>è¿™å°ä¿¡æå‡ºäº†UniToComï¼Œä¸€ç§ç»Ÿä¸€çš„ä»¤ç‰Œé€šä¿¡èŒƒå¼ï¼Œå®ƒå°†ä»¤ç‰Œè§†ä¸ºå¤„ç†å’Œæ— çº¿ä¼ è¾“çš„åŸºæœ¬å•ä½ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å®ç°é«˜æ•ˆçš„ä»¤ç‰Œè¡¨ç¤ºï¼Œæˆ‘ä»¬æå‡ºäº†ç”Ÿæˆä¿¡æ¯ç“¶é¢ˆï¼ˆGenIBï¼‰åŸåˆ™ï¼Œè¯¥åŸåˆ™æœ‰åŠ©äºå­¦ä¹ ä¿ç•™é‡è¦ä¿¡æ¯çš„ä»¤ç‰Œï¼ŒåŒæ—¶æ”¯æŒè·¨å¤šç§æ¨¡æ€çš„å¯é ç”Ÿæˆã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒåŸºäºGenIBçš„ä»¤ç‰ŒåŒ–æœ‰åˆ©äºæé«˜é€šä¿¡æ•ˆç‡å¹¶é™ä½è®¡ç®—å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œä¸ºäº†è§£å†³è‡ªå›å½’å»ºæ¨¡ä¸­çš„æ–¹å·®å´©æºƒæŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼€å‘äº†Ïƒ-GenIBï¼Œä»¥ä¿æŒè¡¨ç¤ºçš„å¤šæ ·æ€§å’Œç¨³å®šæ€§ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬åœ¨æ¥æ”¶å™¨ç«¯é‡‡ç”¨åŸºäºå› æœTransformerçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œåœ¨ä¸‹ä¸€ä»¤ç‰Œé¢„æµ‹èŒƒå¼ä¸‹ç»Ÿä¸€ç¦»æ•£å’Œè¿ç»­ä»¤ç‰Œçš„å¤„ç†ã€‚ä»¿çœŸç»“æœéªŒè¯äº†æ‰€æå‡ºçš„UniToComåœ¨åŠ¨æ€ä¿¡é“æ¡ä»¶ä¸‹çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚é€šè¿‡å°†ä»¤ç‰Œå¤„ç†ä¸MLLMsé›†æˆï¼ŒUniToComä½¿é€šä¿¡å…·æœ‰å¯æ‰©å±•æ€§å’Œæ³›åŒ–æ€§ï¼Œæœ‰åˆ©äºå¤šæ¨¡æ€çš„ç†è§£å’Œç”Ÿæˆï¼Œä¸ºä¸‹ä¸€ä»£æ™ºèƒ½é€šä¿¡æä¾›äº†æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01728v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†UniToComï¼Œä¸€ç§ç»Ÿä¸€çš„ä»¤ç‰Œé€šä¿¡èŒƒå¼ï¼Œå°†ä»¤ç‰Œä½œä¸ºå¤„ç†å’Œæ— çº¿ä¼ è¾“çš„åŸºæœ¬å•ä½ã€‚ä¸ºå®ç°é«˜æ•ˆçš„ä»¤ç‰Œè¡¨ç¤ºï¼Œæå‡ºäº†åŸºäºç”Ÿæˆä¿¡æ¯ç“¶é¢ˆï¼ˆGenIBï¼‰çš„åŸåˆ™ï¼Œæœ‰åŠ©äºå­¦ä¹ èƒ½å¤Ÿä¿ç•™å…³é”®ä¿¡æ¯å¹¶æ”¯æŒè·¨å¤šæ¨¡æ€å¯é ç”Ÿæˆçš„ä»¤ç‰Œã€‚æ­¤å¤–ï¼Œä¸ºè§£å†³è‡ªå›å½’å»ºæ¨¡ä¸­çš„æ–¹å·®å´©æºƒé—®é¢˜ï¼Œå¼€å‘äº†Ïƒ-GenIBï¼Œä¿æŒè¡¨ç¤ºçš„å¤šæ ·æ€§å’Œç¨³å®šæ€§ã€‚æ¥æ”¶ç«¯é‡‡ç”¨åŸºäºå› æœTransformerçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œç»Ÿä¸€å¤„ç†ç¦»æ•£å’Œè¿ç»­ä»¤ç‰Œï¼Œæ¨¡æ‹Ÿç»“æœè¡¨æ˜UniToComåœ¨åŠ¨æ€ä¿¡é“æ¡ä»¶ä¸‹ç›¸æ¯”åŸºçº¿æ–¹æ³•æ›´æœ‰æ•ˆå’Œä¼˜è¶Šã€‚UniToComå°†ä»¤ç‰Œå¤„ç†ä¸MLLMsé›†æˆï¼Œä¸ºä¸‹ä¸€ä»£æ™ºèƒ½é€šä¿¡æä¾›äº†å¯æ‰©å±•å’Œé€šç”¨çš„é€šä¿¡æ–¹æ¡ˆï¼Œæœ‰åˆ©äºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniToComæ˜¯ä¸€ç§æ–°çš„ç»Ÿä¸€ä»¤ç‰Œé€šä¿¡èŒƒå¼ï¼Œå°†ä»¤ç‰Œä½œä¸ºå¤„ç†å’Œæ— çº¿ä¼ è¾“çš„åŸºæœ¬å•ä½ã€‚</li>
<li>GenIBåŸåˆ™ç”¨äºå®ç°é«˜æ•ˆçš„ä»¤ç‰Œè¡¨ç¤ºï¼Œä¿ç•™å…³é”®ä¿¡æ¯å¹¶æ”¯æŒè·¨å¤šæ¨¡æ€çš„å¯é ç”Ÿæˆã€‚</li>
<li>Ïƒ-GenIBè§£å†³äº†è‡ªå›å½’å»ºæ¨¡ä¸­çš„æ–¹å·®å´©æºƒé—®é¢˜ï¼Œç»´æŒè¡¨ç¤ºçš„å¤šæ ·æ€§å’Œç¨³å®šæ€§ã€‚</li>
<li>æ¥æ”¶ç«¯é‡‡ç”¨åŸºäºå› æœTransformerçš„MLLMï¼Œèƒ½ç»Ÿä¸€å¤„ç†ç¦»æ•£å’Œè¿ç»­ä»¤ç‰Œã€‚</li>
<li>UniToComåœ¨åŠ¨æ€ä¿¡é“æ¡ä»¶ä¸‹ç›¸æ¯”åŸºçº¿æ–¹æ³•æ›´æœ‰æ•ˆå’Œä¼˜è¶Šã€‚</li>
<li>UniToComæœ‰åˆ©äºå¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆï¼Œä¸ºä¸‹ä¸€ä»£æ™ºèƒ½é€šä¿¡æä¾›äº†å¯æ‰©å±•å’Œé€šç”¨çš„é€šä¿¡æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01728">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-64709211ba911e32df7650e23af7550e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d5b7b27c41a285ff8dabe3cb1ff0047.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a98cd8abec040d0225b50d9cad037d3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e2e224613c6f16ded77a8e0e84d45c0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Agent-Ideate-A-Framework-for-Product-Idea-Generation-from-Patents-Using-Agentic-AI"><a href="#Agent-Ideate-A-Framework-for-Product-Idea-Generation-from-Patents-Using-Agentic-AI" class="headerlink" title="Agent Ideate: A Framework for Product Idea Generation from Patents Using   Agentic AI"></a>Agent Ideate: A Framework for Product Idea Generation from Patents Using   Agentic AI</h2><p><strong>Authors:Gopichand Kanumolu, Ashok Urlana, Charaka Vinayak Kumar, Bala Mallikarjunarao Garlapati</strong></p>
<p>Patents contain rich technical knowledge that can inspire innovative product ideas, yet accessing and interpreting this information remains a challenge. This work explores the use of Large Language Models (LLMs) and autonomous agents to mine and generate product concepts from a given patent. In this work, we design Agent Ideate, a framework for automatically generating product-based business ideas from patents. We experimented with open-source LLMs and agent-based architectures across three domains: Computer Science, Natural Language Processing, and Material Chemistry. Evaluation results show that the agentic approach consistently outperformed standalone LLMs in terms of idea quality, relevance, and novelty. These findings suggest that combining LLMs with agentic workflows can significantly enhance the innovation pipeline by unlocking the untapped potential of business idea generation from patent data. </p>
<blockquote>
<p>ä¸“åˆ©åŒ…å«ä¸°å¯Œçš„æŠ€æœ¯çŸ¥è¯†ï¼Œèƒ½å¤Ÿæ¿€å‘åˆ›æ–°çš„äº§å“ç†å¿µï¼Œç„¶è€Œè®¿é—®å’Œè§£é‡Šè¿™äº›ä¿¡æ¯ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè‡ªä¸»ä»£ç†ä»ç»™å®šä¸“åˆ©ä¸­æå–å’Œç”Ÿæˆäº§å“æ¦‚å¿µçš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†Agent Ideateï¼Œè¿™æ˜¯ä¸€ä¸ªä»ä¸“åˆ©ä¸­è‡ªåŠ¨ç”ŸæˆåŸºäºäº§å“çš„å•†ä¸šæƒ³æ³•çš„æ¡†æ¶ã€‚æˆ‘ä»¬åœ¨è®¡ç®—æœºç§‘å­¦ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œææ–™åŒ–å­¦ä¸‰ä¸ªé¢†åŸŸå°è¯•äº†å¼€æºLLMå’ŒåŸºäºä»£ç†çš„æ¶æ„ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œåœ¨æ€æƒ³è´¨é‡ã€ç›¸å…³æ€§å’Œæ–°é¢–æ€§æ–¹é¢ï¼Œä»£ç†æ–¹æ³•å§‹ç»ˆä¼˜äºå•ç‹¬çš„LLMã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œå°†LLMä¸ä»£ç†å·¥ä½œæµç¨‹ç›¸ç»“åˆï¼Œå¯ä»¥é€šè¿‡è§£é”ä¸“åˆ©æ•°æ®ä¸­å°šæœªå¼€å‘çš„å•†ä¸šç†å¿µç”Ÿæˆæ½œåŠ›ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºåˆ›æ–°ç®¡é“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01717v1">PDF</a> AgentScen Workshop, IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>ä¸“åˆ©è•´å«ä¸°å¯Œçš„æŠ€æœ¯çŸ¥è¯†ï¼Œå¯æ¿€å‘åˆ›æ–°äº§å“çµæ„Ÿï¼Œä½†è®¿é—®å’Œè§£è¯»è¿™äº›ä¿¡æ¯ä»å…·æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æ¢ç´¢ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè‡ªä¸»ä»£ç†æ¥ä»ç»™å®šä¸“åˆ©ä¸­æŒ–æ˜å’Œç”Ÿæˆäº§å“æ¦‚å¿µã€‚æˆ‘ä»¬è®¾è®¡äº†Agent Ideateæ¡†æ¶ï¼Œå¯è‡ªåŠ¨ä»ä¸“åˆ©ä¸­äº§ç”ŸåŸºäºäº§å“çš„å•†ä¸šæƒ³æ³•ã€‚æˆ‘ä»¬åœ¨è®¡ç®—æœºç§‘å­¦ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œææ–™åŒ–å­¦ä¸‰ä¸ªé¢†åŸŸè¿›è¡Œäº†å¼€æºLLMå’ŒåŸºäºä»£ç†çš„æ¶æ„çš„å®éªŒã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œåœ¨æƒ³æ³•è´¨é‡ã€ç›¸å…³æ€§å’Œæ–°é¢–æ€§æ–¹é¢ï¼Œä»£ç†æ–¹æ³•å§‹ç»ˆä¼˜äºå•ç‹¬çš„LLMã€‚è¿™è¡¨æ˜å°†LLMä¸ä»£ç†å·¥ä½œæµç¨‹ç›¸ç»“åˆï¼Œå¯ä»¥é€šè¿‡è§£é”ä¸“åˆ©æ•°æ®ä¸­æœªè¢«å‘æ˜çš„æ½œåŠ›ï¼Œæ˜¾è‘—å¢å¼ºåˆ›æ–°ç®¡é“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¸“åˆ©è•´å«ä¸°å¯ŒæŠ€æœ¯çŸ¥è¯†ï¼Œæ˜¯æ¿€å‘åˆ›æ–°äº§å“çµæ„Ÿçš„å®è´µèµ„æºã€‚</li>
<li>è®¿é—®å’Œè§£è¯»ä¸“åˆ©ä¿¡æ¯å…·æŒ‘æˆ˜ï¼Œéœ€å€ŸåŠ©å…ˆè¿›æŠ€æœ¯å¦‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè‡ªä¸»ä»£ç†ã€‚</li>
<li>ä»‹ç»äº†Agent Ideateæ¡†æ¶ï¼Œå¯è‡ªåŠ¨ä»ä¸“åˆ©ä¸­äº§ç”ŸåŸºäºäº§å“çš„å•†ä¸šæƒ³æ³•ã€‚</li>
<li>åœ¨è®¡ç®—æœºç§‘å­¦ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œææ–™åŒ–å­¦ä¸‰ä¸ªé¢†åŸŸè¿›è¡Œäº†å®éªŒã€‚</li>
<li>ä»£ç†æ–¹æ³•åœ¨æƒ³æ³•è´¨é‡ã€ç›¸å…³æ€§å’Œæ–°é¢–æ€§æ–¹é¢ä¼˜äºå•ç‹¬çš„LLMã€‚</li>
<li>ç»“åˆLLMå’Œä»£ç†å·¥ä½œæµç¨‹èƒ½æ˜¾è‘—å¢å¼ºåˆ›æ–°ç®¡é“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01717">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-abffc47810a00427372af6548bf0a5ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43259abb6481e4bf285b24bc0f0ae5b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3ddc3b021b10261e994defeec5eae4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8acaf2251d90a173adc9a2316e94f3e3.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="GPT-But-Backwards-Exactly-Inverting-Language-Model-Outputs"><a href="#GPT-But-Backwards-Exactly-Inverting-Language-Model-Outputs" class="headerlink" title="GPT, But Backwards: Exactly Inverting Language Model Outputs"></a>GPT, But Backwards: Exactly Inverting Language Model Outputs</h2><p><strong>Authors:Adrians Skapars, Edoardo Manino, Youcheng Sun, Lucas C. Cordeiro</strong></p>
<p>While existing auditing techniques attempt to identify potential unwanted behaviours in large language models (LLMs), we address the complementary forensic problem of reconstructing the exact input that led to an existing LLM output - enabling post-incident analysis and potentially the detection of fake output reports. We formalize exact input reconstruction as a discrete optimisation problem with a unique global minimum and introduce SODA, an efficient gradient-based algorithm that operates on a continuous relaxation of the input search space with periodic restarts and parameter decay. Through comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we demonstrate that SODA significantly outperforms existing approaches. We succeed in fully recovering 79.5% of shorter out-of-distribution inputs from next-token logits, without a single false positive, but struggle to extract private information from the outputs of longer (15+ token) input sequences. This suggests that standard deployment practices may currently provide adequate protection against malicious use of our method. Our code is available at <a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.15539879">https://doi.org/10.5281/zenodo.15539879</a>. </p>
<blockquote>
<p>ç°æœ‰çš„å®¡è®¡æŠ€æœ¯æ—¨åœ¨è¯†åˆ«å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å¯èƒ½å­˜åœ¨çš„æ½œåœ¨ä¸é€‚å½“è¡Œä¸ºï¼Œè€Œæˆ‘ä»¬å¯¹ä¸ä¹‹äº’è¡¥çš„é—®é¢˜è¿›è¡Œé‡æ„â€”â€”å‡†ç¡®é‡å»ºè¾“å…¥ä»¥å¾—åˆ°ç°æœ‰LLMè¾“å‡ºï¼Œä»è€Œä½¿äº‹ååˆ†ææˆä¸ºå¯èƒ½å¹¶å¯èƒ½å‘ç°è™šå‡è¾“å‡ºæŠ¥å‘Šã€‚æˆ‘ä»¬å°†ç²¾ç¡®è¾“å…¥é‡å»ºå½¢å¼åŒ–ä¸ºå…·æœ‰å”¯ä¸€å…¨å±€æœ€å°å€¼çš„ç¦»æ•£ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶å¼•å…¥äº†SODAç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ•ˆçš„åŸºäºæ¢¯åº¦çš„ç®—æ³•ï¼Œå¯¹è¾“å…¥æœç´¢ç©ºé—´è¿›è¡Œè¿ç»­æ¾å¼›å¤„ç†å¹¶è¾…ä»¥å‘¨æœŸæ€§çš„é‡å¯å’Œå‚æ•°è¡°å‡ã€‚æˆ‘ä»¬å¯¹ä»æ‹¥æœ‰æ•°ç™¾ä¸‡è‡³æ•°åäº¿å‚æ•°è§„æ¨¡çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†å…¨é¢çš„å®éªŒéªŒè¯ï¼Œè¡¨æ˜SODAç®—æ³•åœ¨æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•çš„åŒæ—¶èƒ½å¤ŸæˆåŠŸå®Œå…¨æ¢å¤ä¸‹ä¸€æ ‡è®°æ—¥å¿—è¾“å‡ºä¸­çš„é«˜è¾¾ç™¾åˆ†ä¹‹ä¸ƒåä¹ç‚¹äº”ï¼Œæ— ä»»ä½•è¯¯æŠ¥ç°è±¡å‡ºç°ï¼Œä½†å¯¹æ›´é•¿è¾“å…¥åºåˆ—çš„è¾“å‡ºä¿¡æ¯ä¸­æå–éšç§ä¿¡æ¯ä»å­˜åœ¨é—®é¢˜ã€‚è¿™è¡¨æ˜ç°æœ‰çš„éƒ¨ç½²å®è·µåœ¨å½“å‰å¯èƒ½ä¸ºå¯¹æŠ—æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†å……åˆ†çš„ä¿æŠ¤æ‰‹æ®µã€‚æˆ‘ä»¬çš„ä»£ç ä½äºå…¬å¼€ç½‘å€<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.15539879%E4%B8%AD%E4%BE%9B%E5%A4%A7%E5%AE%B6%E4%B8%8B%E8%BD%BD%E5%AD%A6%E4%B9%A0%E3%80%82">https://doi.org/10.5281/zenodo.15539879ä¸­ä¾›å¤§å®¶ä¸‹è½½å­¦ä¹ ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01693v1">PDF</a> 9 pages, ICML 2025 Workshop on Reliable and Responsible Foundation   Models</p>
<p><strong>Summary</strong><br>æœ¬æ–‡å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å…¥é‡å»ºé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å®¡è®¡æŠ€æœ¯SODAï¼Œæ—¨åœ¨é€šè¿‡æœ€å°åŒ–è¾“å‡ºä¸çœŸå®è¾“å‡ºä¹‹é—´çš„å·®å¼‚æ¥é‡æ„ç²¾ç¡®çš„è¾“å…¥ï¼Œä½¿äº‹ååˆ†æå’Œæ£€æµ‹å‡è¾“å‡ºæŠ¥å‘Šæˆä¸ºå¯èƒ½ã€‚é€šè¿‡åœ¨ä¸åŒè§„æ¨¡çš„LLMä¸Šçš„ç»¼åˆå®éªŒéªŒè¯ï¼ŒSODAåœ¨é‡å»ºçŸ­è¾“å‡ºåˆ†å¸ƒå¤–è¾“å…¥æ–¹é¢å…·æœ‰å‡ºè‰²çš„è¡¨ç°ï¼Œèƒ½å¤Ÿå®Œå…¨æ¢å¤é«˜è¾¾79.5%çš„è¾“å…¥ä¿¡æ¯ä¸”æ— å‡é˜³æ€§ç»“æœï¼Œä½†å¯¹è¾ƒé•¿è¾“å…¥åºåˆ—çš„ç§äººä¿¡æ¯æå–èƒ½åŠ›å—é™ã€‚å»ºè®®é‡‡å–é€‚å½“çš„éƒ¨ç½²æªæ–½æ¥ä¿æŠ¤æ¨¡å‹çš„æ»¥ç”¨é£é™©ã€‚ç›¸å…³ä»£ç å¯é€šè¿‡æä¾›çš„é“¾æ¥è·å–ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶äººå‘˜ä¸“æ³¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å…¥é‡å»ºé—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„å®¡è®¡é¢†åŸŸã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å®¡è®¡æŠ€æœ¯SODAï¼Œæ—¨åœ¨é€šè¿‡æœ€å°åŒ–è¾“å‡ºå·®å¼‚æ¥é‡æ„ç²¾ç¡®è¾“å…¥ã€‚</li>
<li>SODAæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œèƒ½å¤Ÿå®Œå…¨æ¢å¤çŸ­è¾“å‡ºåˆ†å¸ƒå¤–çš„è¾“å…¥ä¿¡æ¯é«˜è¾¾79.5%ã€‚</li>
<li>SODAåœ¨æ— å‡é˜³æ€§ç»“æœçš„æƒ…å†µä¸‹æˆåŠŸé‡å»ºè¾“å…¥ä¿¡æ¯ã€‚</li>
<li>å¯¹äºè¾ƒé•¿è¾“å…¥åºåˆ—çš„ç§äººä¿¡æ¯æå–èƒ½åŠ›å—é™ï¼Œæç¤ºå½“å‰éƒ¨ç½²å®è·µå¯èƒ½è¶³ä»¥é˜²æ­¢æ¶æ„ä½¿ç”¨ã€‚</li>
<li>ä»£ç å¯é€šè¿‡æä¾›çš„é“¾æ¥è·å–ï¼Œä¾¿äºåç»­ç ”ç©¶æˆ–åº”ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01693">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-18e5088f91836faa88549a207109bc4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc5522373bad9f96548e6788bb40225d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b931b8407a842e9e9e2d0cdf6bb3a2ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a679f53a51084a2f63aaa8f520706e1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-122b65256120359157ca744908180377.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Confidence-and-Stability-of-Global-and-Pairwise-Scores-in-NLP-Evaluation"><a href="#Confidence-and-Stability-of-Global-and-Pairwise-Scores-in-NLP-Evaluation" class="headerlink" title="Confidence and Stability of Global and Pairwise Scores in NLP Evaluation"></a>Confidence and Stability of Global and Pairwise Scores in NLP Evaluation</h2><p><strong>Authors:Georgii Levtsov, Dmitry Ustalov</strong></p>
<p>With the advent of highly capable instruction-tuned neural language models, benchmarking in natural language processing (NLP) is increasingly shifting towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper empirically investigates the strengths and weaknesses of both global scores and pairwise comparisons to aid decision-making in selecting appropriate model evaluation strategies. Through computational experiments on synthetic and real-world datasets using standard global metrics and the popular Bradley-Terry model for pairwise comparisons, we found that while global scores provide more reliable overall rankings, they can underestimate strong models with rare, significant errors or low confidence. Conversely, pairwise comparisons are particularly effective for identifying strong contenders among models with lower global scores, especially where quality metrics are hard to define (e.g., text generation), though they require more comparisons to converge if ties are frequent. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/HSPyroblast/srw-ranking">https://github.com/HSPyroblast/srw-ranking</a> under a permissive license. </p>
<blockquote>
<p>éšç€èƒ½åŠ›è¶…å¼ºçš„æŒ‡ä»¤è°ƒä¼˜ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹çš„å‡ºç°ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„åŸºå‡†æµ‹è¯•æ­£é€æ¸ä»ä¼ ç»Ÿçš„å…¨å±€ç‚¹çŠ¶å¾—åˆ†ï¼ˆä¾‹å¦‚GLUEã€BIG-benchã€SWE-benchï¼‰è½¬å‘è¯¸å¦‚LMSYS Arenaä¹‹ç±»çš„æˆå¯¹æ¯”è¾ƒæ’è¡Œæ¦œã€‚æœ¬æ–‡å®è¯ç ”ç©¶äº†å…¨å±€å¾—åˆ†å’Œæˆå¯¹æ¯”è¾ƒçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ï¼Œä»¥å¸®åŠ©é€‰æ‹©é€‚å½“çš„æ¨¡å‹è¯„ä¼°ç­–ç•¥è¿›è¡Œå†³ç­–ã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ ‡å‡†å…¨å±€æŒ‡æ ‡å’Œæµè¡Œçš„Bradley-Terryæ¨¡å‹å¯¹åˆæˆæ•°æ®é›†å’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†è¿›è¡Œè®¡ç®—å®éªŒï¼Œå‘ç°è™½ç„¶å…¨å±€å¾—åˆ†èƒ½æä¾›æ›´å¯é çš„æ•´ä½“æ’åï¼Œä½†å®ƒä»¬å¯èƒ½ä¼šä½ä¼°å…·æœ‰ç½•è§é‡å¤§é”™è¯¯æˆ–ä½ç½®ä¿¡åº¦çš„å¼ºå¤§æ¨¡å‹ã€‚ç›¸åï¼Œæˆå¯¹æ¯”è¾ƒåœ¨è¯†åˆ«å…·æœ‰è¾ƒä½å…¨å±€å¾—åˆ†çš„æ¨¡å‹ä¸­çš„å¼ºåŠ²ç«äº‰è€…æ–¹é¢ç‰¹åˆ«æœ‰æ•ˆï¼Œå°¤å…¶æ˜¯åœ¨è´¨é‡æŒ‡æ ‡éš¾ä»¥å®šä¹‰çš„æƒ…å†µä¸‹ï¼ˆä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆï¼‰ï¼Œä¸è¿‡ï¼Œå¦‚æœå¹³å±€å¾ˆå¸¸è§ï¼Œåˆ™éœ€è¦æ›´å¤šçš„æ¯”è¾ƒæ¥æ”¶æ•›ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/HSPyroblast/srw-ranking%EF%BC%88%E9%87%87%E7%94%A8%E8%AE%B8%E5%8F%AF%E8%AE%B8%E5%8F%AF%EF%BC%89%E4%B8%8B%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/HSPyroblast/srw-rankingï¼ˆé‡‡ç”¨è®¸å¯è®¸å¯ï¼‰ä¸‹è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01633v1">PDF</a> 8 pages, accepted at ACL SRW 2025</p>
<p><strong>Summary</strong></p>
<p>éšç€æŒ‡ä»¤ä¼˜åŒ–å‹ç¥ç»è¯­è¨€æ¨¡å‹çš„å‘å±•ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„åŸºå‡†æµ‹è¯•è¶Šæ¥è¶Šå€¾å‘äºä½¿ç”¨æˆå¯¹æ¯”è¾ƒæ’è¡Œæ¦œï¼ˆå¦‚LMSYS Arenaï¼‰ï¼Œè€Œéä¼ ç»Ÿçš„å…¨å±€ç‚¹å€¼æ’è¡Œæ¦œï¼ˆå¦‚GLUEã€BIG-benchã€SWE-benchï¼‰ã€‚æœ¬æ–‡å®è¯ç ”ç©¶äº†å…¨å±€å¾—åˆ†å’Œæˆå¯¹æ¯”è¾ƒçš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿ï¼Œä»¥è¾…åŠ©é€‰æ‹©åˆé€‚çš„æ¨¡å‹è¯„ä¼°ç­–ç•¥ã€‚ç ”ç©¶å‘ç°ï¼Œå…¨å±€å¾—åˆ†è™½ç„¶èƒ½æä¾›æ›´å¯é çš„æ•´ä½“æ’åï¼Œä½†å¯èƒ½ä½ä¼°å…·æœ‰ç½•è§é‡å¤§é”™è¯¯æˆ–ä½ç½®ä¿¡åº¦çš„å¼ºæ¨¡å‹ã€‚è€Œæˆå¯¹æ¯”è¾ƒåœ¨è¯†åˆ«ä½å…¨çƒå¾—åˆ†ä¸­çš„å¼ºæ¨¡å‹æ–¹é¢ç‰¹åˆ«æœ‰æ•ˆï¼Œå°¤å…¶æ˜¯åœ¨è´¨é‡æŒ‡æ ‡éš¾ä»¥å®šä¹‰çš„æƒ…å†µä¸‹ï¼ˆå¦‚æ–‡æœ¬ç”Ÿæˆï¼‰ã€‚ä½†å¦‚å­˜åœ¨é¢‘ç¹å¹³å±€ï¼Œæˆå¯¹æ¯”è¾ƒçš„æ”¶æ•›éœ€è¦æ›´å¤šçš„æ¯”è¾ƒæ¬¡æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹çš„è¿›æ­¥æ¨åŠ¨äº†NLPåŸºå‡†æµ‹è¯•çš„è½¬å˜ï¼Œä»ä¼ ç»Ÿçš„å…¨å±€ç‚¹å€¼æ’è¡Œæ¦œè½¬å‘æˆå¯¹æ¯”è¾ƒæ’è¡Œæ¦œã€‚</li>
<li>å…¨å±€å¾—åˆ†æä¾›å¯é çš„æ•´ä½“æ’åï¼Œä½†å¯èƒ½ä½ä¼°æŸäº›å…·æœ‰æ˜¾è‘—é”™è¯¯æˆ–ä½ç½®ä¿¡åº¦çš„å¼ºæ¨¡å‹ã€‚</li>
<li>æˆå¯¹æ¯”è¾ƒèƒ½æ›´æœ‰æ•ˆåœ°è¯†åˆ«ä½å…¨çƒå¾—åˆ†ä¸­çš„å¼ºæ¨¡å‹ï¼Œå°¤å…¶åœ¨è´¨é‡æŒ‡æ ‡éš¾ä»¥å®šä¹‰çš„æƒ…å†µä¸‹ã€‚</li>
<li>æˆå¯¹æ¯”è¾ƒåœ¨æ¨¡å‹è¯„ä¼°ä¸­æä¾›äº†æ–°çš„è§†è§’ï¼Œå°¤å…¶æ˜¯åœ¨éš¾ä»¥ç›´æ¥æ¯”è¾ƒæ¨¡å‹æ€§èƒ½æ—¶ã€‚</li>
<li>å½“å­˜åœ¨é¢‘ç¹å¹³å±€æ—¶ï¼Œæˆå¯¹æ¯”è¾ƒçš„æ”¶æ•›éœ€è¦æ›´å¤šçš„æ¯”è¾ƒæ¬¡æ•°ã€‚</li>
<li>å®è¯ç ”ç©¶æ˜¯é€šè¿‡åˆæˆå’ŒçœŸå®æ•°æ®é›†çš„è®¡ç®—å®éªŒè¿›è¡Œçš„ï¼Œé‡‡ç”¨äº†æ ‡å‡†å…¨å±€æŒ‡æ ‡å’Œæµè¡Œçš„Bradley-Terryæ¨¡å‹è¿›è¡Œæˆå¯¹æ¯”è¾ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01633">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-2bf6a645b44a9223471ce23cbbe2d852.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b048bd1d3286261d953c1a7677da977.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82a8ad3420933e2ebb2bc8abbe819df3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d10ad247e92425ddd3500556925fab6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-750e7dccfe33cefc26ad576825ffeed5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aefe94b522be48250c0afd2f05f617af.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Symbolic-or-Numerical-Understanding-Physics-Problem-Solving-in-Reasoning-LLMs"><a href="#Symbolic-or-Numerical-Understanding-Physics-Problem-Solving-in-Reasoning-LLMs" class="headerlink" title="Symbolic or Numerical? Understanding Physics Problem Solving in   Reasoning LLMs"></a>Symbolic or Numerical? Understanding Physics Problem Solving in   Reasoning LLMs</h2><p><strong>Authors:Nifu Dan, Yujun Cai, Yiwei Wang</strong></p>
<p>Navigating the complexities of physics reasoning has long been a difficult task for Large Language Models (LLMs), requiring a synthesis of profound conceptual understanding and adept problem-solving techniques. In this study, we investigate the application of advanced instruction-tuned reasoning models, such as Deepseek-R1, to address a diverse spectrum of physics problems curated from the challenging SciBench benchmark. Our comprehensive experimental evaluation reveals the remarkable capabilities of reasoning models. Not only do they achieve state-of-the-art accuracy in answering intricate physics questions, but they also generate distinctive reasoning patterns that emphasize on symbolic derivation. Furthermore, our findings indicate that even for these highly sophisticated reasoning models, the strategic incorporation of few-shot prompting can still yield measurable improvements in overall accuracy, highlighting the potential for continued performance gains. </p>
<blockquote>
<p>é•¿æœŸä»¥æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åº”å¯¹ç‰©ç†å­¦æ¨ç†çš„å¤æ‚æ€§æ–¹é¢ä¸€ç›´æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ï¼Œè¿™è¦æ±‚æ·±åšçš„æ¦‚å¿µç†è§£å’Œç²¾æ¹›çš„é—®é¢˜è§£å†³æŠ€å·§ç›¸ç»“åˆã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†é«˜çº§æŒ‡ä»¤ä¼˜åŒ–æ¨ç†æ¨¡å‹ï¼ˆå¦‚Deepseek-R1ï¼‰åœ¨åº”å¯¹æ¥è‡ªå…·æœ‰æŒ‘æˆ˜æ€§çš„SciBenchåŸºå‡†æµ‹è¯•çš„ä¸€ç³»åˆ—ç‰©ç†é—®é¢˜æ—¶çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„ç»¼åˆå®éªŒè¯„ä¼°æ­ç¤ºäº†æ¨ç†æ¨¡å‹çš„å“è¶Šèƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹ä¸ä»…åœ¨å›ç­”å¤æ‚ç‰©ç†é—®é¢˜æ—¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œè€Œä¸”è¿˜äº§ç”Ÿäº†å¼ºè°ƒç¬¦å·æ¨å¯¼çš„ç‹¬ç‰¹æ¨ç†æ¨¡å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯å¯¹äºè¿™äº›é«˜åº¦å¤æ‚çš„æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡ç­–ç•¥æ€§åœ°èå…¥å°‘é‡æç¤ºï¼ˆfew-shot promptingï¼‰ï¼Œä»ç„¶å¯ä»¥åœ¨æ•´ä½“å‡†ç¡®æ€§ä¸Šå–å¾—å¯è¡¡é‡çš„è¿›æ­¥ï¼Œè¿™å‡¸æ˜¾äº†æ€§èƒ½è¿›ä¸€æ­¥æå‡çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01334v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>æœ¬ç ”ç©¶æ¢è®¨äº†å…ˆè¿›æŒ‡ä»¤è°ƒä¼˜æ¨ç†æ¨¡å‹ï¼Œå¦‚Deepseek-R1ï¼Œåœ¨è§£å†³æ¥è‡ªSciBenchåŸºå‡†æŒ‘æˆ˜çš„å¤šæ ·åŒ–ç‰©ç†é—®é¢˜æ—¶çš„åº”ç”¨ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæ¨ç†æ¨¡å‹å…·æœ‰å‡ºè‰²èƒ½åŠ›ï¼Œä¸ä»…è¾¾åˆ°äº†å›ç­”å¤æ‚ç‰©ç†é—®é¢˜çš„æœ€æ–°å‡†ç¡®æ€§æ ‡å‡†ï¼Œè€Œä¸”äº§ç”Ÿäº†å¼ºè°ƒç¬¦å·æ¨å¯¼çš„ç‹¬ç‰¹æ¨ç†æ¨¡å¼ã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°å³ä½¿å¯¹äºè¿™äº›é«˜åº¦å¤æ‚çš„æ¨ç†æ¨¡å‹ï¼Œæˆ˜ç•¥æ€§åœ°èå…¥å°‘é‡æç¤ºä»å¯æ˜¾è‘—æé«˜æ•´ä½“å‡†ç¡®æ€§ï¼Œå±•ç¤ºäº†æŒç»­æé«˜æ€§èƒ½çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å…ˆè¿›æŒ‡ä»¤è°ƒä¼˜æ¨ç†æ¨¡å‹å¦‚Deepseek-R1åœ¨è§£å†³å¤šæ ·åŒ–ç‰©ç†é—®é¢˜æ—¶è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ã€‚</li>
<li>è¿™äº›æ¨¡å‹åœ¨å›ç­”å¤æ‚ç‰©ç†é—®é¢˜æ—¶è¾¾åˆ°äº†æœ€æ–°å‡†ç¡®æ€§æ ‡å‡†ã€‚</li>
<li>æ¨ç†æ¨¡å‹äº§ç”Ÿäº†å¼ºè°ƒç¬¦å·æ¨å¯¼çš„ç‹¬ç‰¹æ¨ç†æ¨¡å¼ã€‚</li>
<li>å³ä½¿å¯¹äºé«˜åº¦å¤æ‚çš„æ¨ç†æ¨¡å‹ï¼Œèå…¥å°‘é‡æç¤ºä»å¯æé«˜æ•´ä½“å‡†ç¡®æ€§ã€‚</li>
<li>æˆ˜ç•¥æ€§åœ°ä½¿ç”¨æç¤ºå¯èƒ½æœ‰åŠ©äºæ¨ç†æ¨¡å‹çš„æŒç»­æ€§èƒ½æå‡ã€‚</li>
<li>æ­¤ç ”ç©¶æ­ç¤ºäº†ç‰©ç†æ¨ç†å¯¹äºLLMçš„å¤æ‚æ€§ï¼Œå¹¶å±•ç¤ºäº†é«˜çº§æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01334">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f75168278177cd46de96df312c7a4822.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43ee0b3af7d989529f628567f903df25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efed7ee1e128c14e937744ff362c89d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bce1c83d5c44a0ac7c57117747866cef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39a106965c2a43254c57b5354cd250ca.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Towards-Efficient-Educational-Chatbots-Benchmarking-RAG-Frameworks"><a href="#Towards-Efficient-Educational-Chatbots-Benchmarking-RAG-Frameworks" class="headerlink" title="Towards Efficient Educational Chatbots: Benchmarking RAG Frameworks"></a>Towards Efficient Educational Chatbots: Benchmarking RAG Frameworks</h2><p><strong>Authors:Umar Ali Khan, Ekram Khan, Fiza Khan, Athar Ali Moinuddin</strong></p>
<p>Large Language Models (LLMs) have proven immensely beneficial in education by capturing vast amounts of literature-based information, allowing them to generate context without relying on external sources. In this paper, we propose a generative AI-powered GATE question-answering framework (GATE stands for Graduate Aptitude Test in Engineering) that leverages LLMs to explain GATE solutions and support students in their exam preparation. We conducted extensive benchmarking to select the optimal embedding model and LLM, evaluating our framework based on criteria such as latency, faithfulness, and relevance, with additional validation through human evaluation. Our chatbot integrates state-of-the-art embedding models and LLMs to deliver accurate, context-aware responses. Through rigorous experimentation, we identified configurations that balance performance and computational efficiency, ensuring a reliable chatbot to serve studentsâ€™ needs. Additionally, we discuss the challenges faced in data processing and modeling and implemented solutions. Our work explores the application of Retrieval-Augmented Generation (RAG) for GATE Q&#x2F;A explanation tasks, and our findings demonstrate significant improvements in retrieval accuracy and response quality. This research offers practical insights for developing effective AI-driven educational tools while highlighting areas for future enhancement in usability and scalability. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡æ•è·å¤§é‡æ–‡çŒ®ä¿¡æ¯ï¼Œç”Ÿæˆä¸Šä¸‹æ–‡è€Œä¸ä¾èµ–å¤–éƒ¨èµ„æºï¼Œåœ¨æ•™è‚²é¢†åŸŸè¯æ˜å…·æœ‰å·¨å¤§çš„ä¼˜åŠ¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„GATEé—®ç­”æ¡†æ¶ï¼ˆGATEä»£è¡¨å·¥ç¨‹ç ”ç©¶ç”Ÿå…¥å­¦è€ƒè¯•ï¼‰ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨LLMæ¥è§£é‡ŠGATEè§£å†³æ–¹æ¡ˆå¹¶æ”¯æŒå­¦ç”Ÿå¤‡è€ƒã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„æ ‡å‡†æµ‹è¯•ï¼Œä»¥é€‰æ‹©æœ€ä½³çš„åµŒå…¥æ¨¡å‹å’ŒLLMï¼Œå¹¶æ ¹æ®å»¶è¿Ÿã€å¿ è¯šåº¦å’Œç›¸å…³æ€§ç­‰æ ‡å‡†è¯„ä¼°æˆ‘ä»¬çš„æ¡†æ¶ï¼Œå¹¶é€šè¿‡äººå·¥è¯„ä¼°è¿›è¡Œé¢å¤–éªŒè¯ã€‚æˆ‘ä»¬çš„èŠå¤©æœºå™¨äººé›†æˆäº†æœ€å…ˆè¿›çš„åµŒå…¥æ¨¡å‹å’ŒLLMï¼Œä»¥æä¾›å‡†ç¡®ã€è¯­å¢ƒæ„ŸçŸ¥çš„å“åº”ã€‚é€šè¿‡ä¸¥æ ¼çš„å®éªŒï¼Œæˆ‘ä»¬ç¡®å®šäº†å¹³è¡¡æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡çš„é…ç½®ï¼Œç¡®ä¿å¯é çš„èŠå¤©æœºå™¨äººå¯ä»¥æ»¡è¶³å­¦ç”Ÿçš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†æ•°æ®å¤„ç†å’Œå»ºæ¨¡æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå¹¶å®æ–½äº†è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„å·¥ä½œæ¢ç´¢äº†ç”¨äºGATEé—®ç­”è§£é‡Šä»»åŠ¡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„åº”ç”¨ï¼Œç ”ç©¶ç»“æœè¡¨æ˜åœ¨æ£€ç´¢å‡†ç¡®æ€§å’Œå“åº”è´¨é‡æ–¹é¢å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚è¯¥ç ”ç©¶ä¸ºå¼€å‘æœ‰æ•ˆçš„AIé©±åŠ¨æ•™è‚²å·¥å…·æä¾›äº†å®é™…è§è§£ï¼ŒåŒæ—¶å¼ºè°ƒäº†æœªæ¥åœ¨å¯ç”¨æ€§å’Œå¯æ‰©å±•æ€§æ–¹é¢çš„æ”¹è¿›æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00781v2">PDF</a> One of the co-authors is having conflict in the submission to arXiv   due to many edits (we have to make changes in evaluation strategies, i.e.   section 5); in the paper there are still formatting issues</p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæ•æ‰å¤§é‡æ–‡çŒ®ä¿¡æ¯å¹¶ç”Ÿæˆä¸Šä¸‹æ–‡å†…å®¹ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨èµ„æºã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„GATEé—®ç­”æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è§£é‡ŠGATEè§£é¢˜æ–¹æ¡ˆï¼Œæ”¯æŒå­¦ç”Ÿå¤‡è€ƒã€‚ç»è¿‡å¹¿æ³›è¯„ä¼°ï¼Œæˆ‘ä»¬é€‰æ‹©äº†æœ€ä½³åµŒå…¥æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¹¶åŸºäºå»¶è¿Ÿã€å¿ å®åº¦å’Œç›¸å…³æ€§ç­‰æ ‡å‡†å¯¹æ¡†æ¶è¿›è¡Œäº†éªŒè¯ã€‚æˆ‘ä»¬çš„èŠå¤©æœºå™¨äººé›†æˆäº†æœ€å…ˆè¿›çš„åµŒå…¥æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæä¾›å‡†ç¡®ã€è¯­å¢ƒåŒ–çš„å›åº”ã€‚ç ”ç©¶è¿˜æ¢è®¨äº†æ•°æ®å¤„ç†å’Œå»ºæ¨¡çš„æŒ‘æˆ˜åŠè§£å†³æ–¹æ¡ˆï¼Œå¹¶æ¢ç´¢äº†ç”¨äºGATEé—®ç­”è§£é‡Šçš„æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯çš„å®é™…åº”ç”¨ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—æé«˜çš„æ£€ç´¢å‡†ç¡®æ€§å’Œå“åº”è´¨é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæ•æ‰å¤§é‡æ–‡çŒ®ä¿¡æ¯å¹¶ç”Ÿæˆä¸Šä¸‹æ–‡å†…å®¹ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„GATEé—®ç­”æ¡†æ¶ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ”¯æŒå­¦ç”Ÿå¤‡è€ƒã€‚</li>
<li>å¹¿æ³›è¯„ä¼°åé€‰æ‹©äº†æœ€ä½³åµŒå…¥æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>æ¡†æ¶éªŒè¯åŸºäºå»¶è¿Ÿã€å¿ å®åº¦å’Œç›¸å…³æ€§ç­‰æ ‡å‡†ã€‚</li>
<li>èŠå¤©æœºå™¨äººé›†æˆäº†æœ€å…ˆè¿›çš„åµŒå…¥æ¨¡å‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶æ¢è®¨äº†æ•°æ®å¤„ç†å’Œå»ºæ¨¡çš„æŒ‘æˆ˜åŠè§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00781">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3cc79a1e620b4c389e4b53bc27b56bd8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-107fad89b982181529f54217ff6d4634.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1c05ee1cca4a52e60a04cb0328a6a48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ef397859e8b3d8056dc72437e82f4df.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="SURE-VQA-Systematic-Understanding-of-Robustness-Evaluation-in-Medical-VQA-Tasks"><a href="#SURE-VQA-Systematic-Understanding-of-Robustness-Evaluation-in-Medical-VQA-Tasks" class="headerlink" title="SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical   VQA Tasks"></a>SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical   VQA Tasks</h2><p><strong>Authors:Kim-Celine Kahl, Selen Erkan, Jeremias Traub, Carsten T. LÃ¼th, Klaus Maier-Hein, Lena Maier-Hein, Paul F. Jaeger</strong></p>
<p>Vision-Language Models (VLMs) have great potential in medical tasks, like Visual Question Answering (VQA), where they could act as interactive assistants for both patients and clinicians. Yet their robustness to distribution shifts on unseen data remains a key concern for safe deployment. Evaluating such robustness requires a controlled experimental setup that allows for systematic insights into the modelâ€™s behavior. However, we demonstrate that current setups fail to offer sufficiently thorough evaluations. To address this gap, we introduce a novel framework, called \textit{SURE-VQA}, centered around three key requirements to overcome current pitfalls and systematically analyze VLM robustness: 1) Since robustness on synthetic shifts does not necessarily translate to real-world shifts, it should be measured on real-world shifts that are inherent to the VQA data; 2) Traditional token-matching metrics often fail to capture underlying semantics, necessitating the use of large language models (LLMs) for more accurate semantic evaluation; 3) Model performance often lacks interpretability due to missing sanity baselines, thus meaningful baselines should be reported that allow assessing the multimodal impact on the VLM. To demonstrate the relevance of this framework, we conduct a study on the robustness of various Fine-Tuning (FT) methods across three medical datasets with four types of distribution shifts. Our study highlights key insights into robustness: 1) No FT method consistently outperforms others in robustness, and 2) robustness trends are more stable across FT methods than across distribution shifts. Additionally, we find that simple sanity baselines that do not use the image data can perform surprisingly well and confirm LoRA as the best-performing FT method on in-distribution data. Code is provided at <a target="_blank" rel="noopener" href="https://github.com/IML-DKFZ/sure-vqa">https://github.com/IML-DKFZ/sure-vqa</a>. </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—ä»»åŠ¡ï¼ˆå¦‚è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ï¼‰ä¸­å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯ä»¥ä½œä¸ºæ‚£è€…å’Œä¸´åºŠåŒ»ç”Ÿä¹‹é—´çš„äº¤äº’å¼åŠ©æ‰‹ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æœªè§æ•°æ®ä¸Šçš„åˆ†å¸ƒè½¬ç§»é²æ£’æ€§ä»æ˜¯å®‰å…¨éƒ¨ç½²çš„å…³é”®é—®é¢˜ã€‚è¯„ä¼°è¿™ç§é²æ£’æ€§éœ€è¦ä¸€ä¸ªå—æ§çš„å®éªŒè®¾ç½®ï¼Œå…è®¸å¯¹æ¨¡å‹çš„è¡Œä¸ºè¿›è¡Œç³»ç»Ÿæ€§çš„æ´å¯Ÿã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è¯æ˜å½“å‰çš„è®¾ç½®æ— æ³•æä¾›è¶³å¤Ÿå…¨é¢çš„è¯„ä¼°ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåä¸ºâ€œSURE-VQAâ€çš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å›´ç»•ä¸‰ä¸ªå…³é”®è¦æ±‚æ¥å…‹æœå½“å‰çš„é—®é¢˜å¹¶ç³»ç»Ÿåœ°åˆ†æVLMçš„é²æ£’æ€§ï¼š1ï¼‰ç”±äºåˆæˆè½¬ç§»ä¸Šçš„é²æ£’æ€§å¹¶ä¸ä¸€å®šè½¬åŒ–ä¸ºç°å®ä¸–ç•Œçš„è½¬ç§»ï¼Œå› æ­¤å®ƒåº”è¯¥åœ¨VQAæ•°æ®æ‰€å›ºæœ‰çš„ç°å®ä¸–ç•Œçš„è½¬ç§»ä¸Šè¿›è¡Œè¡¡é‡ï¼›2ï¼‰ä¼ ç»Ÿçš„ä»¤ç‰ŒåŒ¹é…æŒ‡æ ‡å¾€å¾€æ— æ³•æ•æ‰æ½œåœ¨çš„è¯­ä¹‰ï¼Œå› æ­¤éœ€è¦åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ›´å‡†ç¡®çš„è¯­ä¹‰è¯„ä¼°ï¼›3ï¼‰ç”±äºç¼ºå°‘åŸºå‡†çº¿ï¼Œæ¨¡å‹æ€§èƒ½å¾€å¾€ç¼ºä¹å¯è§£é‡Šæ€§ï¼Œå› æ­¤åº”æŠ¥å‘Šæœ‰æ„ä¹‰çš„åŸºå‡†çº¿ï¼Œä»¥ä¾¿è¯„ä¼°å¯¹VLMçš„å¤šæ¨¡å¼å½±å“ã€‚ä¸ºäº†è¯æ˜è¯¥æ¡†æ¶çš„é‡è¦æ€§ï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šå¯¹å„ç§å¾®è°ƒï¼ˆFTï¼‰æ–¹æ³•çš„é²æ£’æ€§è¿›è¡Œäº†ç ”ç©¶ï¼Œè¿™äº›æ•°æ®é›†åŒ…å«å››ç§ç±»å‹çš„åˆ†å¸ƒè½¬ç§»ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯¹é²æ£’æ€§æä¾›äº†å…³é”®çš„è§è§£ï¼š1ï¼‰æ²¡æœ‰ä¸€ç§FTæ–¹æ³•åœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½åœ¨é²æ£’æ€§æ–¹é¢è¡¨ç°æœ€å¥½ï¼›å¹¶ä¸”2ï¼‰ä¸åˆ†å¸ƒè½¬ç§»ç›¸æ¯”ï¼ŒFTæ–¹æ³•åœ¨é²æ£’æ€§è¶‹åŠ¿æ–¹é¢æ›´åŠ ç¨³å®šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ä¸ä½¿ç”¨å›¾åƒæ•°æ®çš„ç®€å•åŸºå‡†çº¿å¯èƒ½ä¼šè¡¨ç°å¾—ä»¤äººæƒŠè®¶åœ°å¥½ï¼Œå¹¶ç¡®è®¤LoRAæ˜¯åœ¨å†…éƒ¨åˆ†å¸ƒæ•°æ®ä¸Šè¡¨ç°æœ€ä½³çš„FTæ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/IML-DKFZ/sure-vqa%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/IML-DKFZ/sure-vqaæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19688v2">PDF</a> TMLR 07&#x2F;2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—ä»»åŠ¡ä¸­çš„æ½œåŠ›ä¸æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°æ¡†æ¶åœ¨è¯„ä¼°æ¨¡å‹å¯¹æœªè§æ•°æ®åˆ†å¸ƒè½¬ç§»é²æ£’æ€§æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶â€œSURE-VQAâ€ï¼Œå¹¶å›´ç»•ä¸‰ä¸ªå…³é”®è¦æ±‚è¿›è¡Œæ„å»ºï¼šæµ‹é‡çœŸå®ä¸–ç•Œçš„æ•°æ®åˆ†å¸ƒè½¬ç§»ã€ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œæ›´å‡†ç¡®çš„è¯­ä¹‰è¯„ä¼°ä»¥åŠæŠ¥å‘Šæœ‰æ„ä¹‰çš„åŸºå‡†ç‚¹ä»¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡å¯¹ä¸åŒå¾®è°ƒæ–¹æ³•åœ¨ä¸‰ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šå››ç§åˆ†å¸ƒè½¬ç§»çš„ç ”ç©¶ï¼Œå‘ç°æ²¡æœ‰ä¸€ç§æ–¹æ³•åœ¨æ‰€æœ‰æƒ…å†µä¸‹å‡è¡¨ç°å‡ºæœ€ä½³çš„é²æ£’æ€§ï¼Œå¹¶ä¸”ç¨³å®šæ€§åœ¨ä¸åŒæ–¹æ³•ä¹‹é—´æ¯”åœ¨åˆ†å¸ƒè½¬ç§»ä¹‹é—´æ›´ä¸ºä¸€è‡´ã€‚åŒæ—¶å‘ç°ç®€å•çš„åŸºå‡†æ¨¡å‹åœ¨æ²¡æœ‰ä½¿ç”¨å›¾åƒæ•°æ®çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¡¨ç°è‰¯å¥½ï¼Œå¹¶ä¸”ç¡®è®¤LoRAåœ¨å¸¸è§„æ•°æ®ä¸Šè¡¨ç°æœ€ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—ä»»åŠ¡ä¸­å…·æœ‰æ½œåŠ›ï¼Œå¦‚è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ï¼Œå¯ä½œä¸ºæ‚£è€…å’Œä¸´åºŠåŒ»ç”Ÿçš„äº¤äº’å¼åŠ©æ‰‹ã€‚</li>
<li>å½“å‰è¯„ä¼°æ¡†æ¶åœ¨è¯„ä¼°æ¨¡å‹å¯¹æœªè§æ•°æ®åˆ†å¸ƒè½¬ç§»çš„é²æ£’æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>å¼•å…¥æ–°çš„è¯„ä¼°æ¡†æ¶â€œSURE-VQAâ€ï¼Œå›´ç»•ä¸‰ä¸ªå…³é”®è¦æ±‚ï¼šæµ‹é‡çœŸå®ä¸–ç•Œçš„æ•°æ®åˆ†å¸ƒè½¬ç§»ã€ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¯­ä¹‰è¯„ä¼°ä»¥åŠæŠ¥å‘Šæœ‰æ„ä¹‰çš„åŸºå‡†ç‚¹ã€‚</li>
<li>åœ¨ä¸‰ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šè¿›è¡Œäº†ä¸åŒå¾®è°ƒæ–¹æ³•çš„ç ”ç©¶ï¼Œå‘ç°æ²¡æœ‰ä¸€ç§æ–¹æ³•åœ¨æ‰€æœ‰æƒ…å†µä¸‹å‡è¡¨ç°å‡ºæœ€ä½³çš„é²æ£’æ€§ã€‚</li>
<li>ä¸åŒå¾®è°ƒæ–¹æ³•çš„é²æ£’æ€§ç¨³å®šæ€§æ¯”åˆ†å¸ƒè½¬ç§»çš„ç¨³å®šæ€§æ›´é«˜ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ç®€å•çš„åŸºå‡†æ¨¡å‹å³ä½¿æ²¡æœ‰ä½¿ç”¨å›¾åƒæ•°æ®ä¹Ÿèƒ½è¡¨ç°å¾—å¾ˆå¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19688">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b000079ea6462b3c4d7ca381a7a28111.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e948fe18de177ef9fbc60fe3fb06c684.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2efc05dd5c40006efa36cc2cf970b9dc.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c67ee15f9e529e3599ade5099ad2f61b.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-04  TD-MPC-Opt Distilling Model-Based Multi-Task Reinforcement Learning   Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-92e8021beacd89a3722a62a26f80a163.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-04  Kwai Keye-VL Technical Report
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
