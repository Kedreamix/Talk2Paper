<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM 方向最新论文已更新，请持续关注 Update in 2025-07-04  How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-a98cd8abec040d0225b50d9cad037d3c.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    20.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    82 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-04-更新"><a href="#2025-07-04-更新" class="headerlink" title="2025-07-04 更新"></a>2025-07-04 更新</h1><h2 id="How-Well-Does-GPT-4o-Understand-Vision-Evaluating-Multimodal-Foundation-Models-on-Standard-Computer-Vision-Tasks"><a href="#How-Well-Does-GPT-4o-Understand-Vision-Evaluating-Multimodal-Foundation-Models-on-Standard-Computer-Vision-Tasks" class="headerlink" title="How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks"></a>How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks</h2><p><strong>Authors:Rahul Ramachandran, Ali Garjani, Roman Bachmann, Andrei Atanov, Oğuzhan Fatih Kar, Amir Zamir</strong></p>
<p>Multimodal foundation models, such as GPT-4o, have recently made remarkable progress, but it is not clear where exactly these models stand in terms of understanding vision. In this paper, we benchmark the performance of popular multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0 Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision tasks (semantic segmentation, object detection, image classification, depth and surface normal prediction) using established datasets (e.g., COCO, ImageNet and its variants, etc).   The main challenges to performing this are: 1) most models are trained to output text and cannot natively express versatile domains, such as segments or 3D geometry, and 2) many leading models are proprietary and accessible only at an API level, i.e., there is no weight access to adapt them. We address these challenges by translating standard vision tasks into equivalent text-promptable and API-compatible tasks via prompt chaining to create a standardized benchmarking framework.   We observe that 1) the models are not close to the state-of-the-art specialist models at any task. However, 2) they are respectable generalists; this is remarkable as they are presumably trained on primarily image-text-based tasks. 3) They perform semantic tasks notably better than geometric ones. 4) While the prompt-chaining techniques affect performance, better models exhibit less sensitivity to prompt variations. 5) GPT-4o performs the best among non-reasoning models, securing the top position in 4 out of 6 tasks, 6) reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a preliminary analysis of models with native image generation, like the latest GPT-4o, shows they exhibit quirks like hallucinations and spatial misalignments. </p>
<blockquote>
<p>多模态基础模型，如GPT-4o，最近取得了显著的进步，但尚不清楚这些模型在理解视觉方面具体处于什么水平。在本文中，我们在标准计算机视觉任务（语义分割、目标检测、图像分类、深度和表面法线预测）上评估了流行多模态基础模型（GPT-4o、o4-mini、Gemini 1.5 Pro和Gemini 2.0 Flash、Claude 3.5 Sonnet、Qwen2-VL、Llama 3.2）的性能，使用了已建立的数据集（例如COCO、ImageNet及其变体等）。执行此操作的主要挑战是：1）大多数模型被训练用于输出文本，无法原生表达诸如片段或3D几何等多样领域；2）许多领先的模型是专有模型，仅可通过API级别访问，即无法获取其权重来进行适应。我们通过通过提示链将标准视觉任务转换为等效的文本提示和API兼容任务，创建标准化的基准测试框架，来解决这些挑战。我们发现：1）这些模型在任何任务上都不接近最新专业模型；然而2）它们是相当不错的通用模型；这相当令人瞩目，因为它们主要基于图像文本任务进行训练。3）它们执行语义任务明显比几何任务更好。4）虽然提示链技术会影响性能，但更好的模型对提示变化表现出较低的敏感性。5）GPT-4o在非推理模型中表现最佳，在6个任务中的4个中位居榜首。6）推理模型（例如o3）在几何任务中显示出改进，7）对具有原生图像生成的模型（如最新的GPT-4o）进行初步分析表明，它们具有诸如幻觉和空间不匹配等特性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01955v1">PDF</a> Project page at <a target="_blank" rel="noopener" href="https://fm-vision-evals.epfl.ch/">https://fm-vision-evals.epfl.ch/</a></p>
<p><strong>Summary</strong></p>
<p>本文探讨了多模态基础模型（如GPT-4o等）在标准计算机视觉任务上的性能表现。文章通过对比不同模型在语义分割、目标检测、图像分类、深度预测和表面法线预测等任务上的表现，发现这些模型虽然远未达到专业模型的水平，但在多任务上的表现展现出其良好的通用性。此外，文章还介绍了使用提示串联技术来解决多模态模型的局限性问题。不同模型的特性存在差异，GPT-4o在非推理模型中表现最佳，而在几何任务上带有推理能力的模型如o3有所改善。初步分析表明，具有原生图像生成功能的模型如最新的GPT-4o会出现幻视和空间不对齐等问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态基础模型在计算机视觉任务上的性能参差不齐，但仍展现出良好的通用性。</li>
<li>模型面临两大挑战：无法原生表达多领域内容以及专有模型的权重不可访问性。</li>
<li>提示串联技术用于解决多模态模型的局限性问题。</li>
<li>GPT-4o在非推理模型中表现最佳，在多数任务中位列第一。</li>
<li>带有推理能力的模型如o3在几何任务上有所改善。</li>
<li>具有原生图像生成功能的模型存在幻视和空间不对齐等问题。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01955">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-25866a56a7bb23110ef12bfd70e68e3b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d8987b678a6043fb17c5c89c3628d8b8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8665bb9a66c6900ff01051abba32ea93.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-86a19d55133674d93a00588735b57752.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-92aaa3e7aa83ff825c7a37d03c198977.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Kwai-Keye-VL-Technical-Report"><a href="#Kwai-Keye-VL-Technical-Report" class="headerlink" title="Kwai Keye-VL Technical Report"></a>Kwai Keye-VL Technical Report</h2><p><strong>Authors: Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Hao Peng, Haojie Ding, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Jin Ouyang, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yang Zhou, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zhenhua Wu, Zhenyu Li, Zhixin Ling, Ziming Li, Dehua Ma, Di Xu, Haixuan Gao, Hang Li, Jiawei Guo, Jing Wang, Lejian Ren, Muhao Wei, Qianqian Wang, Qigen Hu, Shiyao Wang, Tao Yu, Xinchen Luo, Yan Li, Yiming Liang, Yuhang Hu, Zeyi Lu, Zhuoran Yang, Zixing Zhang</strong></p>
<p>While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today’s digital landscape. To bridge this gap, we introduce \textbf{Kwai Keye-VL}, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a four-stage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode <code>cold-start&#39;&#39; data mixture, which includes </code>thinking’’, <code>non-thinking&#39;&#39;, </code>auto-think’’, &#96;&#96;think with image’’, and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the \textbf{KC-MMBench}, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage. </p>
<blockquote>
<p>虽然多模态大型语言模型（MLLMs）在静态图像上表现出卓越的能力，但它们常常在理解动态、信息密集度高的短视频上表现不足，而短视频却是当今数字时代的主导媒体。为了弥补这一差距，我们推出了<strong>Kwai Keye-VL</strong>，这是一个拥有8亿参数的多模态基础模型，专为领先的短视频理解能力而设计，同时保持了强大的通用视觉语言功能。Keye-VL的发展基于两大核心支柱：一是超过600亿标记的大规模高质量数据集，其中强烈侧重于视频；二是创新的训练配方。该配方包括一个用于实现坚实的视觉语言对齐的四阶段预训练过程，以及一个细致的两阶段后训练过程。第一阶段后训练增强了基础能力，如指令遵循能力；第二阶段则专注于刺激高级推理能力。在这一阶段，我们的关键创新是五模式“冷启动”数据混合，包括“思考”、“非思考”、“自动思考”、“与图像一起思考”以及高质量视频数据。这种混合教会模型决定何时以及如何推理。随后的强化学习（RL）和对齐步骤进一步增强了这些推理能力，并纠正了异常模型行为，如重复输出。为了验证我们的方法，我们进行了广泛的评估，结果表明Keye-VL在公共视频基准测试中达到了最新水平，并在基于图像的一般任务中保持了高度竞争力（图1）。此外，我们开发并发布了针对现实短视频场景的全新基准测试<strong>KC-MMBench</strong>，Keye-VL在此展现出显著优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01949v1">PDF</a> Technical Report: <a target="_blank" rel="noopener" href="https://github.com/Kwai-Keye/Keye">https://github.com/Kwai-Keye/Keye</a></p>
<p><strong>Summary</strong></p>
<p>基于多模态大型语言模型（MLLMs）在静态图像上的卓越表现，针对当今数字景观中占主导地位的信息密集型短视频，推出了Kwai Keye-VL模型。该模型拥有超过8亿参数，旨在实现短视频理解的卓越性能，同时保持强大的通用视觉语言功能。Keye-VL的发展建立在两个核心支柱上：超过600亿标记的大规模高质量数据集和创新的训练配方。通过预训练和特殊设计的后训练过程增强视觉语言对齐，Keye-VL展现了先进的视频理解性能。在推出新的基准测试KC-MMBench上表现出显著优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>多模态大型语言模型（MLLMs）在静态图像上的表现良好，但在信息密集型短视频上的理解仍有待提高。</li>
<li>Kwai Keye-VL是一款旨在理解短视频的多模态模型，具备超过8亿的参数。</li>
<li>Keye-VL的建立依赖于两个核心支柱：大规模高质量数据集和创新训练配方。</li>
<li>训练过程中采用四阶段预训练过程确保视觉语言对齐。</li>
<li>后训练过程分为两个阶段，第一阶段增强基础能力如指令遵循，第二阶段专注于高级推理能力训练。其中关键创新在于引入五模式的冷启动数据混合策略，以及随后的强化学习和对齐步骤来增强推理能力并纠正异常模型行为。</li>
<li>Keye-VL在公共视频基准测试中取得了最先进的成果，并在一般图像任务上保持了高度竞争力。此外还开发了针对现实短视频场景的KC-MMBench基准测试。Keye-VL在此表现出显著优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01949">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-71d2b8efe72733d01ee1b92778680729.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9a24ae13c309662119cc8e1efce624ab.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Driven-Closed-Loop-UAV-Operation-with-Semantic-Observations"><a href="#Large-Language-Model-Driven-Closed-Loop-UAV-Operation-with-Semantic-Observations" class="headerlink" title="Large Language Model-Driven Closed-Loop UAV Operation with Semantic   Observations"></a>Large Language Model-Driven Closed-Loop UAV Operation with Semantic   Observations</h2><p><strong>Authors:Wenhao Wang, Yanyan Li, Long Jiao, Jiawei Yuan</strong></p>
<p>Large Language Models (LLMs) have revolutionized robotic autonomy, including Unmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential of LLMs for translating human instructions into executable control code for UAV operations. However, LLMs still face challenges from logical reasoning and complex decision-making, leading to concerns about the reliability of LLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop control framework that enables reliable UAV operations powered by effective feedback and refinement using two LLM modules, i.e., a Code Generator and an Evaluator. Our framework transforms numerical state observations from UAV operations into natural language trajectory descriptions to enhance the evaluator LLM’s understanding of UAV dynamics for precise feedback generation. Our framework also enables a simulation-based refinement process, and hence eliminates the risks to physical UAVs caused by incorrect code execution during the refinement. Extensive experiments on UAV control tasks with different complexities are conducted. The experimental results show that our framework can achieve reliable UAV operations using LLMs, which significantly outperforms baseline approaches in terms of success rate and completeness with the increase of task complexity. </p>
<blockquote>
<p>大型语言模型（LLM）已经改变了包括无人飞行器（UAV）在内的机器人自主性。最近的研究已经证明了LLM在将人类指令翻译成可执行的控制代码以进行无人机操作方面的潜力。然而，LLM在逻辑推理和复杂决策制定方面仍面临挑战，这引发了人们对LLM驱动的无人机操作可靠性的担忧。在本文中，我们提出了一种LLM驱动的闭环控制框架，该框架通过有效的反馈和改进，使用两个LLM模块（即代码生成器和评估器）实现了可靠的无人机操作。我们的框架将无人机操作的数值状态观察结果转化为自然语言轨迹描述，以提高评估LLM对无人机动态的理解，从而生成精确的反馈。我们的框架还支持基于模拟的改进过程，消除了改进过程中因代码执行错误而对实际无人机造成的风险。对具有不同复杂性的无人机控制任务进行了大量实验。实验结果表明，我们的框架能够实现可靠的LLM无人机操作，在任务复杂度增加的情况下，相较于基准方法，其在成功率和完整性方面表现出显著优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01930v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）在无人机自主飞行领域具有革命性潜力，但面临逻辑理解和复杂决策的挑战。本文提出一种LLM驱动的闭环控制框架，通过有效的反馈和改进，使用代码生成器和评估器两个LLM模块，实现可靠的无人机操作。该框架将无人机的数值状态观测转化为自然语言轨迹描述，提高评估器对无人机动态的理解，以生成精确的反馈。实验证明，该框架在任务复杂度增加的情况下仍能实现可靠的无人机操作，显著优于基准方法。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM在无人机自主飞行领域具有巨大潜力，能够将人类指令转化为可执行的控制代码。</li>
<li>当前LLM在逻辑理解和复杂决策方面存在挑战，影响无人机操作的可靠性。</li>
<li>提出一种LLM驱动的闭环控制框架，包含代码生成器和评估器两个模块。</li>
<li>该框架通过将无人机的数值状态观测转化为自然语言轨迹描述，增强对无人机动态的理解。</li>
<li>框架支持基于模拟的改进过程，减少实际无人机因代码执行错误导致的风险。</li>
<li>实验证明该框架在任务复杂度增加时仍能保持高成功率和完整性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01930">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8b60c2d2eb8cf32ed5c1842971859e76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b4fab06cc3db47da849f4fd44147abe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b06dfa91ab452f7067797ee3365e2f5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a691b73db0c36312c4a697f6aec9130.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7f682c04b456367edb962cd4817e6762.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6747a0fa04a0d4a381a966325df8558b.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Gradient-Adaptive-Policy-Optimization-Towards-Multi-Objective-Alignment-of-Large-Language-Models"><a href="#Gradient-Adaptive-Policy-Optimization-Towards-Multi-Objective-Alignment-of-Large-Language-Models" class="headerlink" title="Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment   of Large Language Models"></a>Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment   of Large Language Models</h2><p><strong>Authors:Chengao Li, Hanyu Zhang, Yunkun Xu, Hongyan Xue, Xiang Ao, Qing He</strong></p>
<p>Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful technique for aligning large language models (LLMs) with human preferences. However, effectively aligning LLMs with diverse human preferences remains a significant challenge, particularly when they are conflict. To address this issue, we frame human value alignment as a multi-objective optimization problem, aiming to maximize a set of potentially conflicting objectives. We introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning paradigm that employs multiple-gradient descent to align LLMs with diverse preference distributions. GAPO adaptively rescales the gradients for each objective to determine an update direction that optimally balances the trade-offs between objectives. Additionally, we introduce P-GAPO, which incorporates user preferences across different objectives and achieves Pareto solutions that better align with the user’s specific needs. Our theoretical analysis demonstrates that GAPO converges towards a Pareto optimal solution for multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms current state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness. </p>
<blockquote>
<p>强化学习从人类反馈（RLHF）已经成为一种强大的技术，用于将大型语言模型（LLM）与人类偏好对齐。然而，有效地将LLM与多样的人类偏好对齐仍然是一个巨大的挑战，尤其是在存在冲突的时候。为了解决这个问题，我们将人类价值对齐作为一个多目标优化问题，旨在最大化一组可能相互冲突的目标。我们引入了梯度自适应策略优化（GAPO），这是一种新型微调范式，采用多梯度下降法将LLM与各种偏好分布对齐。GAPO自适应地重新调整每个目标的梯度，以确定一个更新方向，该方向能够最佳地平衡目标之间的权衡。此外，我们引入了P-GAPO，它结合了不同目标上的用户偏好，实现了帕累托解决方案，更好地符合用户的特定需求。我们的理论分析表明，GAPO朝向多个目标的帕累托最优解收敛。在Mistral-7B上的实证结果表明，GAPO优于当前最先进的方法，在有用性和无害性方面都实现了卓越的性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01915v1">PDF</a> 19 pages, 3 figures. Accepted by ACL 2025 (main)</p>
<p><strong>摘要</strong><br>    利用强化学习从人类反馈（RLHF）技术对齐大型语言模型（LLM）与人类偏好。针对LLM对齐多样的人类偏好时存在的冲突问题，提出将人类价值对齐视为多目标优化问题。引入梯度自适应策略优化（GAPO）这一新型微调范式，采用多梯度下降法对齐LLM与多样偏好分布。GAPO自适应调整每个目标的梯度，确定最优平衡各目标间权衡的更新方向。此外，推出P-GAPO，融入用户在不同目标上的偏好，实现更符合用户特定需求的帕累托解决方案。理论分析和Mistral-7B上的实证结果表明，GAPO优于当前最先进的方法，在有用性和无害性方面表现优越。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>RLHF技术用于对齐LLM与人类偏好，展现强大潜力。</li>
<li>对齐LLM与多样人类偏好存在冲突问题。</li>
<li>将人类价值对齐视为多目标优化问题。</li>
<li>引入GAPO这一新型微调范式，采用多梯度下降法处理多样偏好分布。</li>
<li>GAPO自适应调整梯度，确定更新方向以平衡各目标间权衡。</li>
<li>P-GAPO融入用户在不同目标上的偏好，实现更符合用户需求的解决方案。</li>
<li>GAPO在理论分析和实证研究上均表现出优异性能，优于当前最先进方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01915">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8e0174499ff4ad12e2fb869fcbd503e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7eaa75b3f4051e393b712eeedf35e816.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a2b97eafe9c83f666885761f922911c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Reasoning-to-Edit-Hypothetical-Instruction-Based-Image-Editing-with-Visual-Reasoning"><a href="#Reasoning-to-Edit-Hypothetical-Instruction-Based-Image-Editing-with-Visual-Reasoning" class="headerlink" title="Reasoning to Edit: Hypothetical Instruction-Based Image Editing with   Visual Reasoning"></a>Reasoning to Edit: Hypothetical Instruction-Based Image Editing with   Visual Reasoning</h2><p><strong>Authors:Qingdong He, Xueqin Chen, Chaoyi Wang, Yanjie Pan, Xiaobin Hu, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xiangtai Li, Jiangning Zhang</strong></p>
<p>Instruction-based image editing (IIE) has advanced rapidly with the success of diffusion models. However, existing efforts primarily focus on simple and explicit instructions to execute editing operations such as adding, deleting, moving, or swapping objects. They struggle to handle more complex implicit hypothetical instructions that require deeper reasoning to infer plausible visual changes and user intent. Additionally, current datasets provide limited support for training and evaluating reasoning-aware editing capabilities. Architecturally, these methods also lack mechanisms for fine-grained detail extraction that support such reasoning. To address these limitations, we propose Reason50K, a large-scale dataset specifically curated for training and evaluating hypothetical instruction reasoning image editing, along with ReasonBrain, a novel framework designed to reason over and execute implicit hypothetical instructions across diverse scenarios. Reason50K includes over 50K samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs) for editing guidance generation and a diffusion model for image synthesis, incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture detailed visual and textual semantics essential for supporting instruction reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal Enhancer (CME) that enables rich interactions between the fine-grained cues and MLLM-derived features. Extensive experiments demonstrate that ReasonBrain consistently outperforms state-of-the-art baselines on reasoning scenarios while exhibiting strong zero-shot generalization to conventional IIE tasks. Our dataset and code will be released publicly. </p>
<blockquote>
<p>基于指令的图像编辑（IIE）随着扩散模型的成功而迅速发展。然而，现有的努力主要集中在执行简单的明确指令，如添加、删除、移动或交换对象等编辑操作。他们很难处理更复杂的隐含假设指令，这些指令需要更深的推理来推断合理的视觉变化和用户意图。此外，当前的数据集在支持训练和评估推理感知编辑能力方面存在局限性。在结构上，这些方法也缺乏支持此类推理的精细细节提取机制。为了解决这个问题，我们提出了Reason50K，这是一个专门用于训练和评估假设指令推理图像编辑的大规模数据集，以及ReasonBrain，这是一个新颖框架，旨在在多种场景下执行隐含假设指令进行推理。Reason50K包括超过50K个样本，涵盖四种关键推理场景：物理推理、时间推理、因果推理和故事推理。ReasonBrain利用多模态大型语言模型（MLLMs）生成编辑指导，并使用扩散模型进行图像合成，同时融入精细推理线索提取（FRCE）模块，以捕捉支持指令推理的详细视觉和文本语义。为了减少语义损失，我们进一步引入了跨模态增强器（CME），它使精细线索和MLLM衍生特征之间能够进行丰富的交互。大量实验表明，ReasonBrain在推理场景上持续超越最新基线，同时在常规IIE任务上展现出强大的零样本泛化能力。我们的数据集和代码将公开发布。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01908v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了基于指令的图像编辑（IIE）的现有挑战和限制。现有方法主要关注简单直接的编辑指令，难以处理复杂的隐含假设指令，且缺乏相应的训练和评估数据集以及支持深度推理的架构。为此，本文提出了Reason50K数据集和ReasonBrain框架。Reason50K包含超过50K个样本，涵盖四种关键推理场景：物理、时间、因果和故事推理。ReasonBrain利用多模态大型语言模型（MLLMs）进行编辑指导生成和扩散模型进行图像合成，并引入精细推理线索提取（FRCE）模块捕捉详细的视觉和文本语义以支持指令推理。同时，引入跨模态增强器（CME）减少语义损失。实验表明，ReasonBrain在推理场景上表现优于现有技术基线，并在常规IIE任务上展现出强大的零样本泛化能力。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>现有图像编辑方法主要处理简单直接的编辑指令，难以应对复杂的隐含假设指令。</li>
<li>缺乏针对隐含假设指令的图像编辑训练和评估数据集。</li>
<li>提出Reason50K数据集，包含超过50K个样本，涵盖多种关键推理场景。</li>
<li>介绍ReasonBrain框架，结合MLLMs、扩散模型和FRCE模块处理隐含假设指令。</li>
<li>FRCE模块用于捕捉详细的视觉和文本语义，支持指令推理。</li>
<li>引入跨模态增强器（CME）减少语义损失。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01908">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-02ecc7f203b5dcd1da51e1f4896ead5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03c5afdb1b2171a1d4793ff58685c324.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0a174114b9bab6f90864463b72687a5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ae1aba70778f478c908b0454c5407f2c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="High-Layer-Attention-Pruning-with-Rescaling"><a href="#High-Layer-Attention-Pruning-with-Rescaling" class="headerlink" title="High-Layer Attention Pruning with Rescaling"></a>High-Layer Attention Pruning with Rescaling</h2><p><strong>Authors:Songtao Liu, Peng Liu</strong></p>
<p>Pruning is a highly effective approach for compressing large language models (LLMs), significantly reducing inference latency. However, conventional training-free structured pruning methods often employ a heuristic metric that indiscriminately removes some attention heads across all pruning layers, without considering their positions within the network architecture. In this work, we propose a novel pruning algorithm that strategically prunes attention heads in the model’s higher layers. Since the removal of attention heads can alter the magnitude of token representations, we introduce an adaptive rescaling parameter that calibrates the representation scale post-pruning to counteract this effect. We conduct comprehensive experiments on a wide range of LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our evaluation includes both generation and discriminative tasks across 27 datasets. The results consistently demonstrate that our method outperforms existing structured pruning methods. This improvement is particularly notable in generation tasks, where our approach significantly outperforms existing baselines. </p>
<blockquote>
<p>剪枝是压缩大型语言模型（LLM）的一种高效方法，能显著减少推理延迟。然而，传统的无训练结构化剪枝方法通常采用启发式度量，不加区别地移除所有剪枝层中的一些注意力头，而没有考虑到它们在网络架构中的位置。在这项工作中，我们提出了一种新的剪枝算法，该算法会策略性地剪除模型中较高层的注意力头。由于移除注意力头可能会改变令牌表示的幅度，我们引入了一个自适应缩放参数，在剪枝后校准表示规模，以抵消这一影响。我们在多种LLM上进行了全面的实验，包括LLaMA3.1-8B、Mistral-7B-v0.3、Qwen2-7B和Gemma2-9B。我们的评估包括27个数据集的生成和判别任务。结果一致表明，我们的方法优于现有的结构化剪枝方法。在生成任务中，我们的方法显著优于现有基线，这一改进尤为突出。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01900v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了一种针对大型语言模型（LLM）的新型修剪算法，该算法策略性地修剪模型高层中的注意力头，并引入自适应缩放参数来校准修剪后的表示规模。实验证明，该方法在多种LLM上表现优异，特别是在生成任务中显著优于现有基线。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>修剪是压缩大型语言模型的有效方法，能显著降低推理延迟。</li>
<li>传统的无训练结构化修剪方法常常使用启发式度量标准，不考虑网络架构中的注意力头位置。</li>
<li>本文提出了一种新型修剪算法，策略性地修剪模型高层中的注意力头。</li>
<li>引入自适应缩放参数来校准修剪后的表示规模，以抵消修剪注意力头对令牌表示幅度的影响。</li>
<li>该方法在多种LLM上进行了广泛实验，包括LLaMA3.1-8B、Mistral-7B-v0.3、Qwen2-7B和Gemma2-9B。</li>
<li>评价包括生成和判别任务，涉及27个数据集。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01900">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-a9f49498624cfd01df56c0eba3e3b299.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5404fc6bb64136aec1a97440fb0c9196.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c4bc0f6b9c870b6c556dea95c1ff12b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9589d194cfac57d3ec106b7635deecab.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="MiCoTA-Bridging-the-Learnability-Gap-with-Intermediate-CoT-and-Teacher-Assistants"><a href="#MiCoTA-Bridging-the-Learnability-Gap-with-Intermediate-CoT-and-Teacher-Assistants" class="headerlink" title="MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher   Assistants"></a>MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher   Assistants</h2><p><strong>Authors:Dongyi Ding, Tiannan Wang, Chenghao Zhu, Meiling Tao, Yuchen Eleanor Jiang, Wangchunshu Zhou</strong></p>
<p>Large language models (LLMs) excel at reasoning tasks requiring long thought sequences for planning, reflection, and refinement. However, their substantial model size and high computational demands are impractical for widespread deployment. Yet, small language models (SLMs) often struggle to learn long-form CoT reasoning due to their limited capacity, a phenomenon we refer to as the “SLMs Learnability Gap”. To address this, we introduce \textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation (MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA employs intermediate-sized models as teacher assistants and utilizes intermediate-length CoT sequences to bridge both the capacity and reasoning length gaps. Our experiments on downstream tasks demonstrate that although SLMs distilled from large teachers can perform poorly, by applying MiCoTA, they achieve significant improvements in reasoning performance. Specifically, Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and 3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform a quantitative experiment demonstrating that our method produces data more closely aligned with base SLM distributions. Our insights pave the way for future research into long-CoT data distillation for SLMs. </p>
<blockquote>
<p>大型语言模型（LLM）擅长需要长期思考序列进行规划、反思和完善的推理任务。然而，它们庞大的模型尺寸和高计算需求使得难以广泛部署。然而，小型语言模型（SLM）往往因容量有限而难以学习长形式的连锁推理（CoT），这种现象我们称之为“SLMs的可学习性差距”。为了解决这一问题，我们引入了<strong>MiCoTAl</strong>（<strong>Mi</strong>d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation，中间大小的推理助手蒸馏框架），旨在改善SLM的长CoT蒸馏。MiCoTA采用中间大小的模型作为教师助手，并利用中间长度的CoT序列来弥补容量和推理长度的差距。我们在下游任务上的实验表明，虽然从大型教师蒸馏的SLM表现不佳，但通过应用MiCoTA，它们在推理性能上取得了显著改进。具体来说，Qwen2.5-7B-Instruct和Qwen2.5-3B-Instruct在AIME2024、AMC、Olympiad、MATH-500和GSM8K基准测试上的平均分数分别提高了3.47和3.93。为了更好地理解MiCoTA的机制，我们进行了一项定量实验，证明我们的方法产生的数据与基础SLM分布更加吻合。我们的见解为小型语言模型的长CoT数据蒸馏的未来研究铺平了道路。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01887v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong>：针对小型语言模型（SLM）在学习长形式链推理（CoT）时存在的“学习差距”问题，提出了MiCoTAl框架。该框架利用中等规模模型作为教师助手，并通过中间长度的CoT序列来弥补容量和推理长度的差距。实验表明，通过应用MiCoTA，SLM在推理性能上取得了显著改进。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型（LLM）擅长需要长思考序列的推理任务，但模型体积庞大、计算需求高，不利于广泛部署。</li>
<li>小型语言模型（SLM）由于容量有限，在学习长形式链推理（CoT）时存在困难，即“SLMs的学习差距”。</li>
<li>MiCoTAl框架旨在改进SLM的长CoT蒸馏。它利用中等规模模型作为教师助手，并采用中间长度的CoT序列来桥接容量和推理长度的差距。</li>
<li>实验表明，应用MiCoTA的SLM在下游任务上的推理性能得到了显著提高。</li>
<li>Qwen2.5-7B-Instruct和Qwen2.5-3B-Instruct在AIME2024、AMC、Olympiad、MATH-500和GSM8K基准测试上的平均分数分别提高了3.47和3.93。</li>
<li>通过定量实验，证明了MiCoTA产生的数据与基础SLM分布更为一致。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01887">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-c8636f675acecb66b399a6cbcfd58a85.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d5d645219bf36f3681da3d41606b0ece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7cfc1015e577969c2e2d0c8fe2d424ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bce7d862c0ff37a2a3f1948c8cfea2e5.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Eka-Eval-A-Comprehensive-Evaluation-Framework-for-Large-Language-Models-in-Indian-Languages"><a href="#Eka-Eval-A-Comprehensive-Evaluation-Framework-for-Large-Language-Models-in-Indian-Languages" class="headerlink" title="Eka-Eval : A Comprehensive Evaluation Framework for Large Language   Models in Indian Languages"></a>Eka-Eval : A Comprehensive Evaluation Framework for Large Language   Models in Indian Languages</h2><p><strong>Authors:Samridhi Raj Sinha, Rajvee Sheth, Abhishek Upperwal, Mayank Singh</strong></p>
<p>The rapid advancement of Large Language Models (LLMs) has intensified the need for evaluation frameworks that go beyond English centric benchmarks and address the requirements of linguistically diverse regions such as India. We present EKA-EVAL, a unified and production-ready evaluation framework that integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning categories like reasoning, mathematics, tool use, long-context understanding, and reading comprehension. Compared to existing Indian language evaluation tools, EKA-EVAL offers broader benchmark coverage, with built-in support for distributed inference, quantization, and multi-GPU usage. Our systematic comparison positions EKA-EVAL as the first end-to-end, extensible evaluation suite tailored for both global and Indic LLMs, significantly lowering the barrier to multilingual benchmarking. The framework is open-source and publicly available at <a target="_blank" rel="noopener" href="https://github.com/lingo-iitgn/">https://github.com/lingo-iitgn/</a> eka-eval and a part of ongoing EKA initiative (<a target="_blank" rel="noopener" href="https://eka.soket.ai/">https://eka.soket.ai</a>), which aims to scale up to over 100 benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs. </p>
<blockquote>
<p>大型语言模型（LLM）的快速发展加剧了对评估框架的需求，这些评估框架需要超越英语为中心的基准测试，并满足语言多样地区（如印度）的需求。我们推出了EKA-EVAL，这是一个统一且适用于生产的评估框架，集成了超过35个基准测试，包括10个印度语特定数据集，涵盖推理、数学、工具使用、长文本理解和阅读理解等类别。与现有的印度语言评估工具相比，EKA-EVAL提供了更广泛的基准测试覆盖，内置支持分布式推理、量化和多GPU使用。我们的系统比较显示，EKA-EVAL是为全球和印度语LLM量身定制的第一个端到端可扩展评估套件，大大降低了多语言基准测试的门槛。该框架是开源的，可在<a target="_blank" rel="noopener" href="https://github.com/lingo-iitgn/eka-eval%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%EF%BC%8C%E5%B9%B6%E4%B8%94%E6%98%AF%E6%AD%A3%E5%9C%A8%E8%BF%9B%E8%A1%8C%E4%B8%AD%E7%9A%84EKA%E5%80%A1%E8%AE%AE%EF%BC%88https://eka.soket.ai%EF%BC%89%E7%9A%84%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%8C%E8%AF%A5%E5%80%A1%E8%AE%AE%E6%97%A8%E5%9C%A8%E6%89%A9%E5%B1%95%E5%88%B0%E8%B6%85%E8%BF%87100%E4%B8%AA%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%EF%BC%8C%E4%B8%BALLM%E5%BB%BA%E7%AB%8B%E7%A8%B3%E5%81%A5%E7%9A%84%E5%A4%9A%E8%AF%AD%E8%A8%80%E8%AF%84%E4%BC%B0%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E3%80%82">https://github.com/lingo-iitgn/eka-eval上公开访问，并且是正在进行中的EKA倡议（https://eka.soket.ai）的一部分，该倡议旨在扩展到超过100个基准测试，为LLM建立稳健的多语言评估生态系统。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01853v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>随着大型语言模型（LLM）的快速发展，对评估框架的需求愈发迫切，需要超越英语为中心的基准测试，并满足语言多样地区的需要，如印度。我们推出EKA-EVAL评估框架，该框架一体化并支持生产环境使用。它集成了超过35个基准测试，包括面向印地语的特定数据集10个，涵盖推理、数学、工具使用、长语境理解和阅读理解等类别。与现有的印度语言评估工具相比，EKA-EVAL提供更广泛的基准测试覆盖，支持分布式推理、量化和多GPU使用。通过系统性比较，EKA-EVAL定位为全球和印地语大型语言模型的首个端到端可扩展评估套件，显著降低多语言基准测试的门槛。该框架开源并可在公开平台上获取。它是EKA倡议的一部分，旨在建立大型语言模型的多语言评估生态系统。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EKA-EVAL是一个针对大型语言模型的统一评估框架，支持多种语言并特别关注印度地区的需求。</li>
<li>该框架集成了超过35个基准测试，包括针对印地语的特定数据集。</li>
<li>EKA-EVAL提供了广泛的基准测试覆盖，支持分布式推理、量化和多GPU使用。</li>
<li>与现有评估工具相比，EKA-EVAL具有显著优势，特别是在长语境理解和阅读理解的类别上。</li>
<li>EKA-EVAL是开源的并且可以在公开平台上获取，作为EKA倡议的一部分旨在推动大型语言模型的多语言评估生态系统的发展。</li>
<li>该框架具有可扩展性，旨在支持超过一百个基准测试的未来扩展需求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01853">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-3784f5b1902f4072b4971966a995518f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-489befd98e4fcaffbf297e8be45d1586.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-febcdffe3891c682ce65068345b49d93.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="APRMCTS-Improving-LLM-based-Automated-Program-Repair-with-Iterative-Tree-Search"><a href="#APRMCTS-Improving-LLM-based-Automated-Program-Repair-with-Iterative-Tree-Search" class="headerlink" title="APRMCTS: Improving LLM-based Automated Program Repair with Iterative   Tree Search"></a>APRMCTS: Improving LLM-based Automated Program Repair with Iterative   Tree Search</h2><p><strong>Authors:Haichuan Hu, Congqing He, Hao Zhang, Xiaochen Xie, Quanjun Zhang</strong></p>
<p>Automated Program Repair (APR) attempts to fix software bugs without human intervention, which plays a crucial role in software development and maintenance. Recently, with the advances in Large Language Models (LLMs), a rapidly increasing number of APR techniques have been proposed with remarkable performance. However, existing LLM-based APR techniques typically adopt trial-and-error strategies, which suffer from two major drawbacks: (1) inherently limited patch effectiveness due to local exploration, and (2) low search efficiency due to redundant exploration. In this paper, we propose APRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS incorporates Monte Carlo Tree Search (MCTS) into patch searching by performing a global evaluation of the explored patches and selecting the most promising one for subsequent refinement and generation. APRMCTS effectively resolves the problems of falling into local optima and thus helps improve the efficiency of patch searching. Our experiments on 835 bugs from Defects4J demonstrate that, when integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which outperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini, GPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs, respectively. More importantly, APRMCTS boasts a significant performance advantage while employing small patch size (16 and 32), notably fewer than the 500 and 10,000 patches adopted in previous studies. In terms of cost, compared to existing state-of-the-art LLM-based APR methods, APRMCTS has time and monetary costs of less than 20% and 50%, respectively. Our extensive study demonstrates that APRMCTS exhibits good effectiveness and efficiency, with particular advantages in addressing complex bugs. </p>
<blockquote>
<p>自动化程序修复（APR）旨在无需人工干预即可修复软件错误，在软件开发和维护中扮演关键角色。最近，随着大型语言模型（LLM）的进步，已经提出了越来越多的APR技术，并表现出了卓越的性能。然而，现有的基于LLM的APR技术通常采用试错策略，这存在两个主要缺点：（1）由于局部探索，补丁有效性固有地有限；（2）由于冗余探索，搜索效率低下。在本文中，我们提出了APRMCTS，它使用迭代树搜索来改善基于LLM的APR。APRMCTS将蒙特卡洛树搜索（MCTS）融入补丁搜索，通过对探索的补丁进行全局评估，选择最有前途的补丁进行后续精炼和生成。APRMCTS有效地解决了陷入局部最优的问题，从而有助于提高补丁搜索的效率。我们在Defects4J的835个错误上的实验表明，当与GPT-3.5集成时，APRMCTS可以修复总共201个错误，优于所有最先进的基线。此外，APRMCTS帮助GPT-4o-mini、GPT-3.5、Yi-Coder-9B和Qwen2.5-Coder-7B分别修复了30、27、37和28个额外的错误。更重要的是，APRMCTS在采用较小的补丁大小（16和32）时，具有显著的性能优势，远远少于以前研究中采用的500和10000个补丁。就成本而言，与现有的最先进的基于LLM的APR方法相比，APRMCTS的时间和金钱成本分别低于20%和50%。我们的广泛研究表明，APRMCTS具有良好的有效性和效率，特别是在解决复杂的错误方面具有特别的优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01827v1">PDF</a> </p>
<p><strong>Summary</strong>：</p>
<p>自动化程序修复（APR）技术旨在无需人工干预的情况下修复软件中的错误，这在软件的开发和维护中起到了关键作用。随着大型语言模型（LLM）的快速发展，已经提出了许多表现优异的APR技术。然而，现有的LLM-based APR技术通常采用试错策略，存在两个主要缺点：一是由于局部搜索导致的补丁有效性受限，二是由于冗余搜索导致的搜索效率低下。针对这些问题，本文提出了APRMCTS方法，该方法利用迭代树搜索改进了基于LLM的APR。APRMCTS通过将蒙特卡洛树搜索（MCTS）融入补丁搜索，通过对探索的补丁进行全局评估，选择最有前途的补丁进行后续完善和生成。APRMCTS有效地解决了陷入局部最优的问题，从而提高了补丁搜索的效率。实验表明，与GPT-3.5集成后，APRMCTS可以修复更多的错误，并显著优于其他最新基线。此外，APRMCTS在采用较小的补丁大小（16和32）时，性能优势显著，显著少于以前研究中采用的500和10,000个补丁。在成本和效率方面，APRMCTS的时间和金钱成本分别低于现有最新LLM-based APR方法的20%和50%。总的来说，APRMCTS在解决复杂错误方面表现出良好的有效性和效率。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>自动化程序修复（APR）是软件开发生命周期中重要的环节，尤其是基于大型语言模型（LLM）的方法近年来得到了广泛关注。</li>
<li>现有LLM-based APR技术主要采用试错策略，存在补丁有效性受限和搜索效率低下的问题。</li>
<li>APRMCTS方法通过结合蒙特卡洛树搜索（MCTS）和LLM，提高了APR的效率。</li>
<li>APRMCTS可以解决局部最优问题，显著提高补丁搜索效果。</li>
<li>与GPT-3.5集成后，APRMCTS可以修复更多错误，优于其他最新基线。</li>
<li>APRMCTS在采用较小的补丁大小时表现出显著的性能优势。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01827">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8e172fe93f836db10efd36a38600294f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e61b1e0659e5440a9a010ea98d0b446e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-298cf36603826b6e278419284b78f967.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d4814fd2cb91f626f3c86277fa2e8fc4.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="LoRA-Fine-Tuning-Without-GPUs-A-CPU-Efficient-Meta-Generation-Framework-for-LLMs"><a href="#LoRA-Fine-Tuning-Without-GPUs-A-CPU-Efficient-Meta-Generation-Framework-for-LLMs" class="headerlink" title="LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework   for LLMs"></a>LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework   for LLMs</h2><p><strong>Authors:Reza Arabpour, Haitz Sáez de Ocáriz Borde, Anastasis Kratsios</strong></p>
<p>Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language Models (LLMs) by enabling parameter-efficient updates. However, their widespread adoption remains limited by the reliance on GPU-based training. In this work, we propose a theoretically grounded approach to LoRA fine-tuning designed specifically for users with limited computational resources, particularly those restricted to standard laptop CPUs. Our method learns a meta-operator that maps any input dataset, represented as a probability distribution, to a set of LoRA weights by leveraging a large bank of pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of performing new gradient-based updates, our pipeline constructs adapters via lightweight combinations of existing LoRAs directly on CPU. While the resulting adapters do not match the performance of GPU-trained counterparts, they consistently outperform the base Mistral model on downstream tasks, offering a practical and accessible alternative to traditional GPU-based fine-tuning. </p>
<blockquote>
<p>低秩适配器（LoRAs）通过实现参数高效的更新，已经改变了大型语言模型（LLMs）的微调方式。然而，其广泛应用仍受限于对基于GPU的训练的依赖。在这项工作中，我们提出了一种针对计算资源有限的用户，特别是那些仅限于标准笔记本电脑CPU的用户设计的LoRA微调理论扎实的方法。我们的方法学习了一个元操作符，该操作符能够将任何输入数据集映射为一系列LoRA权重，这些权重以概率分布的形式表示，并利用大量预训练的适配器来为Mistral-7B-Instruct-v0.2模型服务。我们的管道不是在CPU上执行新的基于梯度的更新，而是通过轻量级组合现有的LoRAs直接构建适配器。虽然生成的适配器性能不如在GPU上训练的对应物，但它们在下游任务上始终优于基础Mistral模型，为传统的基于GPU的微调提供了一个实用且可访问的替代方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01806v1">PDF</a> 5-page main paper (excluding references) + 11-page appendix, 3   tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for   Foundation Models</p>
<p><strong>Summary</strong></p>
<p>低秩适配器（LoRAs）实现了对大型语言模型（LLMs）的精细调整，实现了参数高效的更新。然而，其广泛应用仍然受限于对基于GPU的训练的依赖。本研究提出了一种针对计算资源有限的用户，特别是仅限于标准笔记本电脑CPU的用户进行LoRA精细调整的理论依据方法。我们的方法学习一个元操作符，它将任何输入数据集映射为一系列LoRA权重，通过利用大量预训练适配器来实现对Mistral-7B-Instruct-v0.2模型的映射。我们的管道通过直接在CPU上结合现有的轻量级LoRAs来构建适配器，而不是执行新的基于梯度的更新，虽然生成的适配器性能不如GPU训练的对应物，但它们在下游任务上始终优于基本Mistral模型，为传统的基于GPU的微调提供了一种实用且可访问的替代方案。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LoRAs实现了LLMs的精细调整，并推动了参数高效的更新。</li>
<li>LoRAs的广泛应用受限于对GPU训练的依赖。</li>
<li>针对计算资源有限的用户（特别是标准笔记本电脑CPU用户），提出了一种新的LoRA精细调整方法。</li>
<li>新方法通过学习一个元操作符来映射输入数据集到LoRA权重，利用预训练适配器实现。</li>
<li>该方法通过结合现有轻量级LoRAs构建适配器，避免了新的基于梯度的更新。</li>
<li>虽然新生成的适配器性能不如GPU训练的适配器，但它们在下游任务上的表现优于基础模型。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01806">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8586e1bf45b3f8d7d28df4de62b36681.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a046cef9960fc7e9f84f93d2db198862.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="HCNQA-Enhancing-3D-VQA-with-Hierarchical-Concentration-Narrowing-Supervision"><a href="#HCNQA-Enhancing-3D-VQA-with-Hierarchical-Concentration-Narrowing-Supervision" class="headerlink" title="HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing   Supervision"></a>HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing   Supervision</h2><p><strong>Authors:Shengli Zhou, Jianuo Zhu, Qilin Huang, Fangjing Wang, Yanfu Zhang, Feng Zheng</strong></p>
<p>3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the physical world and perform spatial reasoning. Answer-centric supervision is a commonly used training method for 3D VQA models. Many models that utilize this strategy have achieved promising results in 3D VQA tasks. However, the answer-centric approach only supervises the final output of models and allows models to develop reasoning pathways freely. The absence of supervision on the reasoning pathway enables the potential for developing superficial shortcuts through common patterns in question-answer pairs. Moreover, although slow-thinking methods advance large language models, they suffer from underthinking. To address these issues, we propose \textbf{HCNQA}, a 3D VQA model leveraging a hierarchical concentration narrowing supervision method. By mimicking the human process of gradually focusing from a broad area to specific objects while searching for answers, our method guides the model to perform three phases of concentration narrowing through hierarchical supervision. By supervising key checkpoints on a general reasoning pathway, our method can ensure the development of a rational and effective reasoning pathway. Extensive experimental results demonstrate that our method can effectively ensure that the model develops a rational reasoning pathway and performs better. The code is available at <a target="_blank" rel="noopener" href="https://github.com/JianuoZhu/HCNQA">https://github.com/JianuoZhu/HCNQA</a>. </p>
<blockquote>
<p>3D视觉问答（3D VQA）对于模型感知物理世界并进行空间推理至关重要。以答案为中心的监督是3D VQA模型常用的训练方法。许多采用这种策略的模型在3D VQA任务中取得了令人鼓舞的结果。然而，以答案为中心的方法只监督模型的最终输出，允许模型自由发展推理路径。推理路径上缺乏监督使得模型可能通过问答对中的常见模式形成浅层次的捷径。此外，尽管慢思考方法促进了大型语言模型的发展，但它们却存在思考不足的问题。为了解决这些问题，我们提出了<strong>HCNQA</strong>，这是一种利用分层集中缩小监督方法的3D VQA模型。通过模仿人类在寻找答案时从广阔区域逐渐聚焦到特定对象的过程，我们的方法通过分层监督引导模型进行三个阶段集中缩小。通过对一般推理路径上的关键检查点进行监督，我们的方法可以确保形成合理有效的推理路径。大量的实验结果表明，我们的方法可以有效地确保模型形成合理的推理路径并表现更好。代码可在<a target="_blank" rel="noopener" href="https://github.com/JianuoZhu/HCNQA">https://github.com/JianuoZhu/HCNQA</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01800v1">PDF</a> ICANN 2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了三维视觉问答（3D VQA）中答案中心监督方法的局限性，并提出了一个新的模型HCNQA。该模型采用层次化集中缩小监督方法，通过模仿人类寻找答案时的聚焦过程，引导模型进行三个阶段集中缩小。通过监督一般推理路径的关键检查点，确保模型发展出合理有效的推理路径。实验结果表明，该方法能有效提高模型性能。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>3D VQA是模型感知现实世界并进行空间推理的关键。</li>
<li>答案中心监督是3D VQA模型的常用训练方法，虽然取得了一定的成果，但存在潜在问题。</li>
<li>答案中心监督允许模型自由发展推理路径，可能导致表面捷径和无效答案。</li>
<li>HCNQA是一个新的三维视觉问答模型，采用层次化集中缩小监督方法。</li>
<li>HCNQA通过模仿人类寻找答案时的聚焦过程，引导模型进行三个阶段集中缩小。</li>
<li>HCNQA通过监督一般推理路径的关键检查点，确保模型发展出合理有效的推理路径。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01800">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-9a1f17cdb5fb7d613fc01d3ac92c9f75.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3d08569ad7898f6d27b83eca57d7cfeb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8aeeda61eb63c1cd4687eb5d6a3f05a6.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Rethinking-Discrete-Tokens-Treating-Them-as-Conditions-for-Continuous-Autoregressive-Image-Synthesis"><a href="#Rethinking-Discrete-Tokens-Treating-Them-as-Conditions-for-Continuous-Autoregressive-Image-Synthesis" class="headerlink" title="Rethinking Discrete Tokens: Treating Them as Conditions for Continuous   Autoregressive Image Synthesis"></a>Rethinking Discrete Tokens: Treating Them as Conditions for Continuous   Autoregressive Image Synthesis</h2><p><strong>Authors:Peng Zheng, Junke Wang, Yi Chang, Yizhou Yu, Rui Ma, Zuxuan Wu</strong></p>
<p>Recent advances in large language models (LLMs) have spurred interests in encoding images as discrete tokens and leveraging autoregressive (AR) frameworks for visual generation. However, the quantization process in AR-based visual generation models inherently introduces information loss that degrades image fidelity. To mitigate this limitation, recent studies have explored to autoregressively predict continuous tokens. Unlike discrete tokens that reside in a structured and bounded space, continuous representations exist in an unbounded, high-dimensional space, making density estimation more challenging and increasing the risk of generating out-of-distribution artifacts. Based on the above findings, this work introduces DisCon (Discrete-Conditioned Continuous Autoregressive Model), a novel framework that reinterprets discrete tokens as conditional signals rather than generation targets. By modeling the conditional probability of continuous representations conditioned on discrete tokens, DisCon circumvents the optimization challenges of continuous token modeling while avoiding the information loss caused by quantization. DisCon achieves a gFID score of 1.38 on ImageNet 256$\times$256 generation, outperforming state-of-the-art autoregressive approaches by a clear margin. </p>
<blockquote>
<p>近期大型语言模型（LLM）的进步激发了将图像编码为离散标记并利用自回归（AR）框架进行视觉生成的兴趣。然而，AR型视觉生成模型中的量化过程会固有地引入信息损失，降低图像保真度。为了缓解这一局限性，近期研究已经探索了自回归预测连续标记的方法。与居住在结构化有界空间中的离散标记不同，连续表示存在于无界的高维空间中，这使得密度估计更具挑战性，并增加了生成离群分布伪影的风险。基于上述发现，这项工作引入了DisCon（离散条件连续自回归模型），这是一种新颖框架，它将离散标记重新解释为条件信号而不是生成目标。通过对离散标记条件下的连续表示的有条件概率建模，DisCon规避了连续标记建模的优化挑战，同时避免了量化引起的信息损失。DisCon在ImageNet 256x256生成任务上实现了1.38的gFID分数，相较于最先进的其他自回归方法有明显优势。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01756v1">PDF</a> accepted by iccv 2025</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）的最新进展激发了将图像编码为离散标记并利用自回归（AR）框架进行视觉生成的兴趣。然而，AR-based视觉生成模型的量化过程会固有地造成信息损失，降低图像保真度。为解决此限制，近期研究开始探索自回归预测连续标记。不同于存在于结构化有界空间中的离散标记，连续表示存在于无界的高维空间中，使得密度估计更具挑战性，并增加了生成离群分布伪影的风险。基于上述发现，本文介绍了DisCon（离散条件连续自回归模型），这是一个新型框架，它将离散标记重新解释为条件信号而不是生成目标。通过建模离散标记条件下的连续表示的条件概率，DisCon规避了连续标记建模的优化挑战，同时避免了量化造成的信息损失。DisCon在ImageNet 256×256生成上实现了1.38的gFID分数，较先进的自回归方法有明显优势。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）在图像编码和视觉生成领域引发关注。</li>
<li>自回归（AR）模型在视觉生成中的量化过程会引入信息损失。</li>
<li>连续表示较离散表示更加复杂，密度估计更具挑战性，存在生成离群分布伪影的风险。</li>
<li>DisCon框架将离散标记重新解释为条件信号，规避了连续标记建模的挑战。</li>
<li>DisCon实现了在ImageNet上的高保真图像生成，gFID分数较低，性能优越。</li>
<li>DisCon框架在避免信息损失的同时，实现了优化的视觉效果。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01756">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-300693c6d62a3f67fe18cf9b4350305c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a1af5d28ff1b027bd1205b7c43d0849d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e59062e0d0aea0528e16e8f092cbe383.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4ecc1a7727398dadec47a17f7ed05e7d.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b710246ce0a3158d27024ba3e38c97d0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="LLMs-for-Legal-Subsumption-in-German-Employment-Contracts"><a href="#LLMs-for-Legal-Subsumption-in-German-Employment-Contracts" class="headerlink" title="LLMs for Legal Subsumption in German Employment Contracts"></a>LLMs for Legal Subsumption in German Employment Contracts</h2><p><strong>Authors:Oliver Wardas, Florian Matthes</strong></p>
<p>Legal work, characterized by its text-heavy and resource-intensive nature, presents unique challenges and opportunities for NLP research. While data-driven approaches have advanced the field, their lack of interpretability and trustworthiness limits their applicability in dynamic legal environments. To address these issues, we collaborated with legal experts to extend an existing dataset and explored the use of Large Language Models (LLMs) and in-context learning to evaluate the legality of clauses in German employment contracts. Our work evaluates the ability of different LLMs to classify clauses as “valid,” “unfair,” or “void” under three legal context variants: no legal context, full-text sources of laws and court rulings, and distilled versions of these (referred to as examination guidelines). Results show that full-text sources moderately improve performance, while examination guidelines significantly enhance recall for void clauses and weighted F1-Score, reaching 80%. Despite these advancements, LLMs’ performance when using full-text sources remains substantially below that of human lawyers. We contribute an extended dataset, including examination guidelines, referenced legal sources, and corresponding annotations, alongside our code and all log files. Our findings highlight the potential of LLMs to assist lawyers in contract legality review while also underscoring the limitations of the methods presented. </p>
<blockquote>
<p>法律工作以其文本密集和资源密集的特点呈现出独特的挑战和机会，对自然语言处理研究亦是如此。尽管数据驱动的方法推动了该领域的发展，但由于其缺乏可解释性和可信度，在动态的法律环境中其适用性有限。为了解决这个问题，我们与法律专家合作扩展了现有数据集，并探索了使用大型语言模型（LLM）和上下文学习来评估德国就业合同条款的合法性。我们的工作评估了不同的大型语言模型在三种法律上下文变体下将条款分类为“有效”，“不公平”或“无效”的能力：无法律上下文、法律与法院判决的全文来源以及这些内容的提炼版本（称为检查指南）。结果表明，全文来源可以适度提高性能，而检查指南可以显著提高无效条款的召回率和加权F1分数，达到80%。尽管有所进展，但在使用全文来源时，大型语言模型的性能仍然远远低于人类律师。我们贡献了一个扩展的数据集，包括检查指南、引用的法律来源和相应注释，以及我们的代码和所有日志文件。我们的研究结果表明大型语言模型在协助律师审查合同合法性方面的潜力，同时也强调了所提出方法的局限性。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01734v1">PDF</a> PrePrint - ICAIL25, Chicago</p>
<p><strong>Summary</strong>：</p>
<p>本文探讨了法律工作对自然语言处理（NLP）研究的独特挑战和机遇。数据驱动的方法虽然推动了该领域的发展，但其缺乏解释性和可信度，限制了其在动态法律环境中的适用性。为研究解决这些问题，研究者与法律专家合作扩展现有数据集，探索使用大型语言模型（LLM）和上下文学习来评估德国就业合同条款的合法性。研究评估了不同LLM在三种法律上下文下对条款进行分类的能力，包括无法律上下文、全文法律来源和法院判决以及精简版（称为审查指南）。结果显示，全文来源适度提高了性能，而审查指南显著提高了无效条款的召回率和加权F1分数，达到80%。尽管取得了这些进展，但使用全文来源的LLM性能仍然远低于人类律师的表现。研究者贡献了一个扩展的数据集，包括审查指南、参考法律来源和相应注释，以及代码和所有日志文件。研究发现突显了LLM在协助律师审查合同合法性方面的潜力，同时也强调了所提出方法的局限性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>法律工作对NLP研究带来独特挑战和机遇，数据驱动方法需结合法律专业知识。</li>
<li>LLMs和上下文学习被用于评估德国就业合同条款的合法性。</li>
<li>三种法律上下文下评估了LLMs的性能：无上下文、全文法律来源和审查指南。</li>
<li>审查指南能显著提高LLMs的性能，特别是在识别无效条款方面。</li>
<li>LLMs在合同合法性审查方面的性能仍远低于人类律师。</li>
<li>贡献了一个扩展数据集，包含审查指南、法律来源和注释等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01734">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-0bb7f2f3e10798a0f069f5620a481416.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-73d9500d3359c9a296bd5a70a7885ac8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d105f7a3e7482f34a16a49c0382f96bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9dcbce2572606654652bf36f143b37d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4dd2644cdf79c9b729fbc899a398b783.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Token-Communication-in-the-Era-of-Large-Models-An-Information-Bottleneck-Based-Approach"><a href="#Token-Communication-in-the-Era-of-Large-Models-An-Information-Bottleneck-Based-Approach" class="headerlink" title="Token Communication in the Era of Large Models: An Information   Bottleneck-Based Approach"></a>Token Communication in the Era of Large Models: An Information   Bottleneck-Based Approach</h2><p><strong>Authors:Hao Wei, Wanli Ni, Wen Wang, Wenjun Xu, Dusit Niyato, Ping Zhang</strong></p>
<p>This letter proposes UniToCom, a unified token communication paradigm that treats tokens as the fundamental units for both processing and wireless transmission. Specifically, to enable efficient token representations, we propose a generative information bottleneck (GenIB) principle, which facilitates the learning of tokens that preserve essential information while supporting reliable generation across multiple modalities. By doing this, GenIB-based tokenization is conducive to improving the communication efficiency and reducing computational complexity. Additionally, we develop $\sigma$-GenIB to address the challenges of variance collapse in autoregressive modeling, maintaining representational diversity and stability. Moreover, we employ a causal Transformer-based multimodal large language model (MLLM) at the receiver to unify the processing of both discrete and continuous tokens under the next-token prediction paradigm. Simulation results validate the effectiveness and superiority of the proposed UniToCom compared to baselines under dynamic channel conditions. By integrating token processing with MLLMs, UniToCom enables scalable and generalizable communication in favor of multimodal understanding and generation, providing a potential solution for next-generation intelligent communications. </p>
<blockquote>
<p>这封信提出了UniToCom，一种统一的令牌通信范式，它将令牌视为处理和无线传输的基本单位。具体来说，为了实现高效的令牌表示，我们提出了生成信息瓶颈（GenIB）原则，该原则有助于学习保留重要信息的令牌，同时支持跨多种模态的可靠生成。通过这种方式，基于GenIB的令牌化有利于提高通信效率并降低计算复杂性。此外，为了解决自回归建模中的方差崩溃挑战，我们开发了σ-GenIB，以保持表示的多样性和稳定性。而且，我们在接收器端采用基于因果Transformer的多模态大型语言模型（MLLM），在下一令牌预测范式下统一离散和连续令牌的处理。仿真结果验证了所提出的UniToCom在动态信道条件下的有效性和优越性。通过将令牌处理与MLLMs集成，UniToCom使通信具有可扩展性和泛化性，有利于多模态的理解和生成，为下一代智能通信提供了潜在的解决方案。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01728v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文提出了UniToCom，一种统一的令牌通信范式，将令牌作为处理和无线传输的基本单位。为实现高效的令牌表示，提出了基于生成信息瓶颈（GenIB）的原则，有助于学习能够保留关键信息并支持跨多模态可靠生成的令牌。此外，为解决自回归建模中的方差崩溃问题，开发了σ-GenIB，保持表示的多样性和稳定性。接收端采用基于因果Transformer的多模态大型语言模型（MLLM），统一处理离散和连续令牌，模拟结果表明UniToCom在动态信道条件下相比基线方法更有效和优越。UniToCom将令牌处理与MLLMs集成，为下一代智能通信提供了可扩展和通用的通信方案，有利于多模态理解和生成。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UniToCom是一种新的统一令牌通信范式，将令牌作为处理和无线传输的基本单位。</li>
<li>GenIB原则用于实现高效的令牌表示，保留关键信息并支持跨多模态的可靠生成。</li>
<li>σ-GenIB解决了自回归建模中的方差崩溃问题，维持表示的多样性和稳定性。</li>
<li>接收端采用基于因果Transformer的MLLM，能统一处理离散和连续令牌。</li>
<li>UniToCom在动态信道条件下相比基线方法更有效和优越。</li>
<li>UniToCom有利于多模态理解和生成，为下一代智能通信提供了可扩展和通用的通信方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01728">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-64709211ba911e32df7650e23af7550e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d5b7b27c41a285ff8dabe3cb1ff0047.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a98cd8abec040d0225b50d9cad037d3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e2e224613c6f16ded77a8e0e84d45c0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Agent-Ideate-A-Framework-for-Product-Idea-Generation-from-Patents-Using-Agentic-AI"><a href="#Agent-Ideate-A-Framework-for-Product-Idea-Generation-from-Patents-Using-Agentic-AI" class="headerlink" title="Agent Ideate: A Framework for Product Idea Generation from Patents Using   Agentic AI"></a>Agent Ideate: A Framework for Product Idea Generation from Patents Using   Agentic AI</h2><p><strong>Authors:Gopichand Kanumolu, Ashok Urlana, Charaka Vinayak Kumar, Bala Mallikarjunarao Garlapati</strong></p>
<p>Patents contain rich technical knowledge that can inspire innovative product ideas, yet accessing and interpreting this information remains a challenge. This work explores the use of Large Language Models (LLMs) and autonomous agents to mine and generate product concepts from a given patent. In this work, we design Agent Ideate, a framework for automatically generating product-based business ideas from patents. We experimented with open-source LLMs and agent-based architectures across three domains: Computer Science, Natural Language Processing, and Material Chemistry. Evaluation results show that the agentic approach consistently outperformed standalone LLMs in terms of idea quality, relevance, and novelty. These findings suggest that combining LLMs with agentic workflows can significantly enhance the innovation pipeline by unlocking the untapped potential of business idea generation from patent data. </p>
<blockquote>
<p>专利包含丰富的技术知识，能够激发创新的产品理念，然而访问和解释这些信息仍然是一个挑战。本研究探索了使用大型语言模型（LLM）和自主代理从给定专利中提取和生成产品概念的方法。在这项工作中，我们设计了Agent Ideate，这是一个从专利中自动生成基于产品的商业想法的框架。我们在计算机科学、自然语言处理和材料化学三个领域尝试了开源LLM和基于代理的架构。评估结果表明，在思想质量、相关性和新颖性方面，代理方法始终优于单独的LLM。这些发现表明，将LLM与代理工作流程相结合，可以通过解锁专利数据中尚未开发的商业理念生成潜力，从而显著增强创新管道。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01717v1">PDF</a> AgentScen Workshop, IJCAI 2025</p>
<p><strong>Summary</strong></p>
<p>专利蕴含丰富的技术知识，可激发创新产品灵感，但访问和解读这些信息仍具挑战。本研究探索使用大型语言模型（LLM）和自主代理来从给定专利中挖掘和生成产品概念。我们设计了Agent Ideate框架，可自动从专利中产生基于产品的商业想法。我们在计算机科学、自然语言处理和材料化学三个领域进行了开源LLM和基于代理的架构的实验。评估结果表明，在想法质量、相关性和新颖性方面，代理方法始终优于单独的LLM。这表明将LLM与代理工作流程相结合，可以通过解锁专利数据中未被发掘的潜力，显著增强创新管道。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>专利蕴含丰富技术知识，是激发创新产品灵感的宝贵资源。</li>
<li>访问和解读专利信息具挑战，需借助先进技术如大型语言模型（LLM）和自主代理。</li>
<li>介绍了Agent Ideate框架，可自动从专利中产生基于产品的商业想法。</li>
<li>在计算机科学、自然语言处理和材料化学三个领域进行了实验。</li>
<li>代理方法在想法质量、相关性和新颖性方面优于单独的LLM。</li>
<li>结合LLM和代理工作流程能显著增强创新管道。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01717">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-abffc47810a00427372af6548bf0a5ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43259abb6481e4bf285b24bc0f0ae5b7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d3ddc3b021b10261e994defeec5eae4e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8acaf2251d90a173adc9a2316e94f3e3.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="GPT-But-Backwards-Exactly-Inverting-Language-Model-Outputs"><a href="#GPT-But-Backwards-Exactly-Inverting-Language-Model-Outputs" class="headerlink" title="GPT, But Backwards: Exactly Inverting Language Model Outputs"></a>GPT, But Backwards: Exactly Inverting Language Model Outputs</h2><p><strong>Authors:Adrians Skapars, Edoardo Manino, Youcheng Sun, Lucas C. Cordeiro</strong></p>
<p>While existing auditing techniques attempt to identify potential unwanted behaviours in large language models (LLMs), we address the complementary forensic problem of reconstructing the exact input that led to an existing LLM output - enabling post-incident analysis and potentially the detection of fake output reports. We formalize exact input reconstruction as a discrete optimisation problem with a unique global minimum and introduce SODA, an efficient gradient-based algorithm that operates on a continuous relaxation of the input search space with periodic restarts and parameter decay. Through comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we demonstrate that SODA significantly outperforms existing approaches. We succeed in fully recovering 79.5% of shorter out-of-distribution inputs from next-token logits, without a single false positive, but struggle to extract private information from the outputs of longer (15+ token) input sequences. This suggests that standard deployment practices may currently provide adequate protection against malicious use of our method. Our code is available at <a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.15539879">https://doi.org/10.5281/zenodo.15539879</a>. </p>
<blockquote>
<p>现有的审计技术旨在识别大型语言模型（LLM）中可能存在的潜在不适当行为，而我们对与之互补的问题进行重构——准确重建输入以得到现有LLM输出，从而使事后分析成为可能并可能发现虚假输出报告。我们将精确输入重建形式化为具有唯一全局最小值的离散优化问题，并引入了SODA算法，这是一种高效的基于梯度的算法，对输入搜索空间进行连续松弛处理并辅以周期性的重启和参数衰减。我们对从拥有数百万至数十亿参数规模的大型语言模型进行了全面的实验验证，表明SODA算法在显著优于现有方法的同时能够成功完全恢复下一标记日志输出中的高达百分之七十九点五，无任何误报现象出现，但对更长输入序列的输出信息中提取隐私信息仍存在问题。这表明现有的部署实践在当前可能为对抗我们的方法提供了充分的保护手段。我们的代码位于公开网址<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.15539879%E4%B8%AD%E4%BE%9B%E5%A4%A7%E5%AE%B6%E4%B8%8B%E8%BD%BD%E5%AD%A6%E4%B9%A0%E3%80%82">https://doi.org/10.5281/zenodo.15539879中供大家下载学习。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01693v1">PDF</a> 9 pages, ICML 2025 Workshop on Reliable and Responsible Foundation   Models</p>
<p><strong>Summary</strong><br>本文关注大型语言模型（LLM）的输入重建问题，提出了一种新的审计技术SODA，旨在通过最小化输出与真实输出之间的差异来重构精确的输入，使事后分析和检测假输出报告成为可能。通过在不同规模的LLM上的综合实验验证，SODA在重建短输出分布外输入方面具有出色的表现，能够完全恢复高达79.5%的输入信息且无假阳性结果，但对较长输入序列的私人信息提取能力受限。建议采取适当的部署措施来保护模型的滥用风险。相关代码可通过提供的链接获取。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究人员专注于大型语言模型（LLM）的输入重建问题，这是一个重要的审计领域。</li>
<li>提出了一种新的审计技术SODA，旨在通过最小化输出差异来重构精确输入。</li>
<li>SODA显著优于现有方法，能够完全恢复短输出分布外的输入信息高达79.5%。</li>
<li>SODA在无假阳性结果的情况下成功重建输入信息。</li>
<li>对于较长输入序列的私人信息提取能力受限，提示当前部署实践可能足以防止恶意使用。</li>
<li>代码可通过提供的链接获取，便于后续研究或应用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01693">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-18e5088f91836faa88549a207109bc4f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bc5522373bad9f96548e6788bb40225d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b931b8407a842e9e9e2d0cdf6bb3a2ca.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a679f53a51084a2f63aaa8f520706e1f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-122b65256120359157ca744908180377.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Confidence-and-Stability-of-Global-and-Pairwise-Scores-in-NLP-Evaluation"><a href="#Confidence-and-Stability-of-Global-and-Pairwise-Scores-in-NLP-Evaluation" class="headerlink" title="Confidence and Stability of Global and Pairwise Scores in NLP Evaluation"></a>Confidence and Stability of Global and Pairwise Scores in NLP Evaluation</h2><p><strong>Authors:Georgii Levtsov, Dmitry Ustalov</strong></p>
<p>With the advent of highly capable instruction-tuned neural language models, benchmarking in natural language processing (NLP) is increasingly shifting towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper empirically investigates the strengths and weaknesses of both global scores and pairwise comparisons to aid decision-making in selecting appropriate model evaluation strategies. Through computational experiments on synthetic and real-world datasets using standard global metrics and the popular Bradley-Terry model for pairwise comparisons, we found that while global scores provide more reliable overall rankings, they can underestimate strong models with rare, significant errors or low confidence. Conversely, pairwise comparisons are particularly effective for identifying strong contenders among models with lower global scores, especially where quality metrics are hard to define (e.g., text generation), though they require more comparisons to converge if ties are frequent. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/HSPyroblast/srw-ranking">https://github.com/HSPyroblast/srw-ranking</a> under a permissive license. </p>
<blockquote>
<p>随着能力超强的指令调优神经网络语言模型的出现，自然语言处理（NLP）的基准测试正逐渐从传统的全局点状得分（例如GLUE、BIG-bench、SWE-bench）转向诸如LMSYS Arena之类的成对比较排行榜。本文实证研究了全局得分和成对比较的优势和劣势，以帮助选择适当的模型评估策略进行决策。我们通过使用标准全局指标和流行的Bradley-Terry模型对合成数据集和真实世界数据集进行计算实验，发现虽然全局得分能提供更可靠的整体排名，但它们可能会低估具有罕见重大错误或低置信度的强大模型。相反，成对比较在识别具有较低全局得分的模型中的强劲竞争者方面特别有效，尤其是在质量指标难以定义的情况下（例如文本生成），不过，如果平局很常见，则需要更多的比较来收敛。我们的代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/HSPyroblast/srw-ranking%EF%BC%88%E9%87%87%E7%94%A8%E8%AE%B8%E5%8F%AF%E8%AE%B8%E5%8F%AF%EF%BC%89%E4%B8%8B%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/HSPyroblast/srw-ranking（采用许可许可）下获得。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01633v1">PDF</a> 8 pages, accepted at ACL SRW 2025</p>
<p><strong>Summary</strong></p>
<p>随着指令优化型神经语言模型的发展，自然语言处理（NLP）中的基准测试越来越倾向于使用成对比较排行榜（如LMSYS Arena），而非传统的全局点值排行榜（如GLUE、BIG-bench、SWE-bench）。本文实证研究了全局得分和成对比较的优势和劣势，以辅助选择合适的模型评估策略。研究发现，全局得分虽然能提供更可靠的整体排名，但可能低估具有罕见重大错误或低置信度的强模型。而成对比较在识别低全球得分中的强模型方面特别有效，尤其是在质量指标难以定义的情况下（如文本生成）。但如存在频繁平局，成对比较的收敛需要更多的比较次数。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>神经网络语言模型的进步推动了NLP基准测试的转变，从传统的全局点值排行榜转向成对比较排行榜。</li>
<li>全局得分提供可靠的整体排名，但可能低估某些具有显著错误或低置信度的强模型。</li>
<li>成对比较能更有效地识别低全球得分中的强模型，尤其在质量指标难以定义的情况下。</li>
<li>成对比较在模型评估中提供了新的视角，尤其是在难以直接比较模型性能时。</li>
<li>当存在频繁平局时，成对比较的收敛需要更多的比较次数。</li>
<li>实证研究是通过合成和真实数据集的计算实验进行的，采用了标准全局指标和流行的Bradley-Terry模型进行成对比较。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01633">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-2bf6a645b44a9223471ce23cbbe2d852.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4b048bd1d3286261d953c1a7677da977.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-82a8ad3420933e2ebb2bc8abbe819df3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d10ad247e92425ddd3500556925fab6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-750e7dccfe33cefc26ad576825ffeed5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aefe94b522be48250c0afd2f05f617af.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Symbolic-or-Numerical-Understanding-Physics-Problem-Solving-in-Reasoning-LLMs"><a href="#Symbolic-or-Numerical-Understanding-Physics-Problem-Solving-in-Reasoning-LLMs" class="headerlink" title="Symbolic or Numerical? Understanding Physics Problem Solving in   Reasoning LLMs"></a>Symbolic or Numerical? Understanding Physics Problem Solving in   Reasoning LLMs</h2><p><strong>Authors:Nifu Dan, Yujun Cai, Yiwei Wang</strong></p>
<p>Navigating the complexities of physics reasoning has long been a difficult task for Large Language Models (LLMs), requiring a synthesis of profound conceptual understanding and adept problem-solving techniques. In this study, we investigate the application of advanced instruction-tuned reasoning models, such as Deepseek-R1, to address a diverse spectrum of physics problems curated from the challenging SciBench benchmark. Our comprehensive experimental evaluation reveals the remarkable capabilities of reasoning models. Not only do they achieve state-of-the-art accuracy in answering intricate physics questions, but they also generate distinctive reasoning patterns that emphasize on symbolic derivation. Furthermore, our findings indicate that even for these highly sophisticated reasoning models, the strategic incorporation of few-shot prompting can still yield measurable improvements in overall accuracy, highlighting the potential for continued performance gains. </p>
<blockquote>
<p>长期以来，大型语言模型（LLM）在应对物理学推理的复杂性方面一直是一项艰巨的任务，这要求深厚的概念理解和精湛的问题解决技巧相结合。在这项研究中，我们探讨了高级指令优化推理模型（如Deepseek-R1）在应对来自具有挑战性的SciBench基准测试的一系列物理问题时的应用。我们的综合实验评估揭示了推理模型的卓越能力。这些模型不仅在回答复杂物理问题时达到了最先进的准确性，而且还产生了强调符号推导的独特推理模式。此外，我们的研究结果表明，即使是对于这些高度复杂的推理模型，通过策略性地融入少量提示（few-shot prompting），仍然可以在整体准确性上取得可衡量的进步，这凸显了性能进一步提升的潜力。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01334v1">PDF</a> </p>
<p><strong>Summary</strong>：<br>本研究探讨了先进指令调优推理模型，如Deepseek-R1，在解决来自SciBench基准挑战的多样化物理问题时的应用。实验评估表明，推理模型具有出色能力，不仅达到了回答复杂物理问题的最新准确性标准，而且产生了强调符号推导的独特推理模式。此外，研究发现即使对于这些高度复杂的推理模型，战略性地融入少量提示仍可显著提高整体准确性，展示了持续提高性能的潜力。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>先进指令调优推理模型如Deepseek-R1在解决多样化物理问题时表现出卓越的能力。</li>
<li>这些模型在回答复杂物理问题时达到了最新准确性标准。</li>
<li>推理模型产生了强调符号推导的独特推理模式。</li>
<li>即使对于高度复杂的推理模型，融入少量提示仍可提高整体准确性。</li>
<li>战略性地使用提示可能有助于推理模型的持续性能提升。</li>
<li>此研究揭示了物理推理对于LLM的复杂性，并展示了高级指令调优模型的潜力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01334">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-f75168278177cd46de96df312c7a4822.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43ee0b3af7d989529f628567f903df25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efed7ee1e128c14e937744ff362c89d8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bce1c83d5c44a0ac7c57117747866cef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-39a106965c2a43254c57b5354cd250ca.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Towards-Efficient-Educational-Chatbots-Benchmarking-RAG-Frameworks"><a href="#Towards-Efficient-Educational-Chatbots-Benchmarking-RAG-Frameworks" class="headerlink" title="Towards Efficient Educational Chatbots: Benchmarking RAG Frameworks"></a>Towards Efficient Educational Chatbots: Benchmarking RAG Frameworks</h2><p><strong>Authors:Umar Ali Khan, Ekram Khan, Fiza Khan, Athar Ali Moinuddin</strong></p>
<p>Large Language Models (LLMs) have proven immensely beneficial in education by capturing vast amounts of literature-based information, allowing them to generate context without relying on external sources. In this paper, we propose a generative AI-powered GATE question-answering framework (GATE stands for Graduate Aptitude Test in Engineering) that leverages LLMs to explain GATE solutions and support students in their exam preparation. We conducted extensive benchmarking to select the optimal embedding model and LLM, evaluating our framework based on criteria such as latency, faithfulness, and relevance, with additional validation through human evaluation. Our chatbot integrates state-of-the-art embedding models and LLMs to deliver accurate, context-aware responses. Through rigorous experimentation, we identified configurations that balance performance and computational efficiency, ensuring a reliable chatbot to serve students’ needs. Additionally, we discuss the challenges faced in data processing and modeling and implemented solutions. Our work explores the application of Retrieval-Augmented Generation (RAG) for GATE Q&#x2F;A explanation tasks, and our findings demonstrate significant improvements in retrieval accuracy and response quality. This research offers practical insights for developing effective AI-driven educational tools while highlighting areas for future enhancement in usability and scalability. </p>
<blockquote>
<p>大型语言模型（LLM）通过捕获大量文献信息，生成上下文而不依赖外部资源，在教育领域证明具有巨大的优势。在本文中，我们提出了一种基于生成式人工智能的GATE问答框架（GATE代表工程研究生入学考试），该框架利用LLM来解释GATE解决方案并支持学生备考。我们进行了广泛的标准测试，以选择最佳的嵌入模型和LLM，并根据延迟、忠诚度和相关性等标准评估我们的框架，并通过人工评估进行额外验证。我们的聊天机器人集成了最先进的嵌入模型和LLM，以提供准确、语境感知的响应。通过严格的实验，我们确定了平衡性能和计算效率的配置，确保可靠的聊天机器人可以满足学生的需求。此外，我们还讨论了数据处理和建模所面临的挑战，并实施了解决方案。我们的工作探索了用于GATE问答解释任务的检索增强生成（RAG）的应用，研究结果表明在检索准确性和响应质量方面取得了显著改进。该研究为开发有效的AI驱动教育工具提供了实际见解，同时强调了未来在可用性和可扩展性方面的改进方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.00781v2">PDF</a> One of the co-authors is having conflict in the submission to arXiv   due to many edits (we have to make changes in evaluation strategies, i.e.   section 5); in the paper there are still formatting issues</p>
<p><strong>Summary</strong><br>大型语言模型在教育领域的应用具有显著优势，能够捕捉大量文献信息并生成上下文内容，无需依赖外部资源。本研究提出了一种基于生成式人工智能的GATE问答框架，利用大型语言模型解释GATE解题方案，支持学生备考。经过广泛评估，我们选择了最佳嵌入模型和大型语言模型，并基于延迟、忠实度和相关性等标准对框架进行了验证。我们的聊天机器人集成了最先进的嵌入模型和大型语言模型，提供准确、语境化的回应。研究还探讨了数据处理和建模的挑战及解决方案，并探索了用于GATE问答解释的检索增强生成技术的实际应用，显示出显著提高的检索准确性和响应质量。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型在教育领域的应用具有显著优势，能够捕捉大量文献信息并生成上下文内容。</li>
<li>研究提出了一种基于生成式人工智能的GATE问答框架，利用大型语言模型支持学生备考。</li>
<li>广泛评估后选择了最佳嵌入模型和大型语言模型。</li>
<li>框架验证基于延迟、忠实度和相关性等标准。</li>
<li>聊天机器人集成了最先进的嵌入模型和大型语言模型。</li>
<li>研究探讨了数据处理和建模的挑战及解决方案。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.00781">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-3cc79a1e620b4c389e4b53bc27b56bd8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-107fad89b982181529f54217ff6d4634.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c1c05ee1cca4a52e60a04cb0328a6a48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ef397859e8b3d8056dc72437e82f4df.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="SURE-VQA-Systematic-Understanding-of-Robustness-Evaluation-in-Medical-VQA-Tasks"><a href="#SURE-VQA-Systematic-Understanding-of-Robustness-Evaluation-in-Medical-VQA-Tasks" class="headerlink" title="SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical   VQA Tasks"></a>SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical   VQA Tasks</h2><p><strong>Authors:Kim-Celine Kahl, Selen Erkan, Jeremias Traub, Carsten T. Lüth, Klaus Maier-Hein, Lena Maier-Hein, Paul F. Jaeger</strong></p>
<p>Vision-Language Models (VLMs) have great potential in medical tasks, like Visual Question Answering (VQA), where they could act as interactive assistants for both patients and clinicians. Yet their robustness to distribution shifts on unseen data remains a key concern for safe deployment. Evaluating such robustness requires a controlled experimental setup that allows for systematic insights into the model’s behavior. However, we demonstrate that current setups fail to offer sufficiently thorough evaluations. To address this gap, we introduce a novel framework, called \textit{SURE-VQA}, centered around three key requirements to overcome current pitfalls and systematically analyze VLM robustness: 1) Since robustness on synthetic shifts does not necessarily translate to real-world shifts, it should be measured on real-world shifts that are inherent to the VQA data; 2) Traditional token-matching metrics often fail to capture underlying semantics, necessitating the use of large language models (LLMs) for more accurate semantic evaluation; 3) Model performance often lacks interpretability due to missing sanity baselines, thus meaningful baselines should be reported that allow assessing the multimodal impact on the VLM. To demonstrate the relevance of this framework, we conduct a study on the robustness of various Fine-Tuning (FT) methods across three medical datasets with four types of distribution shifts. Our study highlights key insights into robustness: 1) No FT method consistently outperforms others in robustness, and 2) robustness trends are more stable across FT methods than across distribution shifts. Additionally, we find that simple sanity baselines that do not use the image data can perform surprisingly well and confirm LoRA as the best-performing FT method on in-distribution data. Code is provided at <a target="_blank" rel="noopener" href="https://github.com/IML-DKFZ/sure-vqa">https://github.com/IML-DKFZ/sure-vqa</a>. </p>
<blockquote>
<p>视觉语言模型（VLMs）在医疗任务（如视觉问答（VQA））中具有巨大潜力，可以作为患者和临床医生之间的交互式助手。然而，它们在未见数据上的分布转移鲁棒性仍是安全部署的关键问题。评估这种鲁棒性需要一个受控的实验设置，允许对模型的行为进行系统性的洞察。然而，我们证明当前的设置无法提供足够全面的评估。为了弥补这一空白，我们引入了一个名为“SURE-VQA”的新型框架，该框架围绕三个关键要求来克服当前的问题并系统地分析VLM的鲁棒性：1）由于合成转移上的鲁棒性并不一定转化为现实世界的转移，因此它应该在VQA数据所固有的现实世界的转移上进行衡量；2）传统的令牌匹配指标往往无法捕捉潜在的语义，因此需要利用大型语言模型（LLM）进行更准确的语义评估；3）由于缺少基准线，模型性能往往缺乏可解释性，因此应报告有意义的基准线，以便评估对VLM的多模式影响。为了证明该框架的重要性，我们在三个医疗数据集上对各种微调（FT）方法的鲁棒性进行了研究，这些数据集包含四种类型的分布转移。我们的研究对鲁棒性提供了关键的见解：1）没有一种FT方法在所有情况下都在鲁棒性方面表现最好；并且2）与分布转移相比，FT方法在鲁棒性趋势方面更加稳定。此外，我们发现不使用图像数据的简单基准线可能会表现得令人惊讶地好，并确认LoRA是在内部分布数据上表现最佳的FT方法。代码可在<a target="_blank" rel="noopener" href="https://github.com/IML-DKFZ/sure-vqa%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/IML-DKFZ/sure-vqa找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19688v2">PDF</a> TMLR 07&#x2F;2025</p>
<p><strong>Summary</strong></p>
<p>本文探讨了视觉语言模型（VLMs）在医疗任务中的潜力与挑战。针对现有评估框架在评估模型对未见数据分布转移鲁棒性方面的不足，提出了一种新的评估框架“SURE-VQA”，并围绕三个关键要求进行构建：测量真实世界的数据分布转移、使用大型语言模型（LLMs）进行更准确的语义评估以及报告有意义的基准点以评估模型的性能。通过对不同微调方法在三个医疗数据集上四种分布转移的研究，发现没有一种方法在所有情况下均表现出最佳的鲁棒性，并且稳定性在不同方法之间比在分布转移之间更为一致。同时发现简单的基准模型在没有使用图像数据的情况下也能表现良好，并且确认LoRA在常规数据上表现最佳。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉语言模型（VLMs）在医疗任务中具有潜力，如视觉问答（VQA），可作为患者和临床医生的交互式助手。</li>
<li>当前评估框架在评估模型对未见数据分布转移的鲁棒性方面存在不足。</li>
<li>引入新的评估框架“SURE-VQA”，围绕三个关键要求：测量真实世界的数据分布转移、使用大型语言模型进行语义评估以及报告有意义的基准点。</li>
<li>在三个医疗数据集上进行了不同微调方法的研究，发现没有一种方法在所有情况下均表现出最佳的鲁棒性。</li>
<li>不同微调方法的鲁棒性稳定性比分布转移的稳定性更高。</li>
<li>研究表明简单的基准模型即使没有使用图像数据也能表现得很好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19688">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-b000079ea6462b3c4d7ca381a7a28111.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e948fe18de177ef9fbc60fe3fb06c684.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2efc05dd5c40006efa36cc2cf970b9dc.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-c67ee15f9e529e3599ade5099ad2f61b.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent 方向最新论文已更新，请持续关注 Update in 2025-07-04  TD-MPC-Opt Distilling Model-Based Multi-Task Reinforcement Learning   Agents
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-92e8021beacd89a3722a62a26f80a163.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-07-04  Kwai Keye-VL Technical Report
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28172.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
