<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer 方向最新论文已更新，请持续关注 Update in 2025-07-04  evMLP An Efficient Event-Driven MLP Architecture for Vision">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ae2a53d0e4b1c20d126622245d3b15e1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-07-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    10.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    41 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-07-04-更新"><a href="#2025-07-04-更新" class="headerlink" title="2025-07-04 更新"></a>2025-07-04 更新</h1><h2 id="evMLP-An-Efficient-Event-Driven-MLP-Architecture-for-Vision"><a href="#evMLP-An-Efficient-Event-Driven-MLP-Architecture-for-Vision" class="headerlink" title="evMLP: An Efficient Event-Driven MLP Architecture for Vision"></a>evMLP: An Efficient Event-Driven MLP Architecture for Vision</h2><p><strong>Authors:Zhentan Zheng</strong></p>
<p>Deep neural networks have achieved remarkable results in computer vision tasks. In the early days, Convolutional Neural Networks (CNNs) were the mainstream architecture. In recent years, Vision Transformers (ViTs) have become increasingly popular. In addition, exploring applications of multi-layer perceptrons (MLPs) has provided new perspectives for research into vision model architectures. In this paper, we present evMLP accompanied by a simple event-driven local update mechanism. The proposed evMLP can independently process patches on images or feature maps via MLPs. We define changes between consecutive frames as “events”. Under the event-driven local update mechanism, evMLP selectively processes patches where events occur. For sequential image data (e.g., video processing), this approach improves computational performance by avoiding redundant computations. Through ImageNet image classification experiments, evMLP attains accuracy competitive with state-of-the-art models. More significantly, experimental results on multiple video datasets demonstrate that evMLP reduces computational cost via its event-driven local update mechanism while maintaining output consistency with its non-event-driven baseline. The code and trained models are available at <a target="_blank" rel="noopener" href="https://github.com/i-evi/evMLP">https://github.com/i-evi/evMLP</a>. </p>
<blockquote>
<p>深度神经网络在计算机视觉任务中取得了显著成果。在早期，卷积神经网络（CNNs）是主流架构。近年来，视觉转换器（ViTs）越来越受欢迎。此外，多层感知器（MLPs）的应用研究为视觉模型架构的研究提供了新的视角。在本文中，我们提出了带有简单事件驱动局部更新机制的evMLP。所提出的evMLP可以通过MLP独立处理图像或特征图上的补丁。我们将连续帧之间的变化定义为“事件”。在事件驱动的局部更新机制下，evMLP会选择性地处理发生事件的地方。对于序列图像数据（例如视频处理），这种方法通过避免冗余计算来提高计算性能。通过ImageNet图像分类实验，evMLP的准确率达到了前沿模型的竞争水平。更重要的是，多个视频数据集的实验结果表明，evMLP通过其事件驱动的局部更新机制降低了计算成本，同时保持了与非事件驱动基准模型的输出一致性。代码和训练好的模型可在<a target="_blank" rel="noopener" href="https://github.com/i-evi/evMLP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/i-evi/evMLP找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01927v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了evMLP模型，该模型结合了多层感知器（MLPs）和事件驱动局部更新机制。evMLP能独立处理图像或特征图上的块，对连续帧之间的变化定义为“事件”，并仅选择事件发生的块进行处理。这种方法提高了计算性能，尤其是在处理序列图像数据（如视频处理）时。实验结果表明，evMLP在维持输出一致性的同时，通过事件驱动局部更新机制降低了计算成本。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>evMLP结合了多层感知器（MLPs）和事件驱动局部更新机制，提供了一种新的视觉模型架构。</li>
<li>evMLP能独立处理图像或特征图上的块。</li>
<li>事件被定义为连续帧之间的变化，evMLP仅选择事件发生的块进行处理。</li>
<li>这种处理方法提高了计算性能，尤其在处理序列图像数据时。</li>
<li>实验结果显示，evMLP在ImageNet图像分类任务中具有与最新模型竞争的准确性。</li>
<li>在多个视频数据集上的实验结果表明，evMLP通过事件驱动局部更新机制降低了计算成本。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01927">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-63ec1845597d903fc0553d8e4473c2c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd7ce53c751232af299156e0e48afa56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1cde5b42e63a4bd1e1a2cddf3933435.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7f3990ba5c67e3a272d2e58a2eabf41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3905a8e39d86efabbfd963cc6d37371.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81dafd93d6573ec28e0053bd66c9fca4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62fc75f10eabf354858cea6fc62bc366.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Are-Vision-Transformer-Representations-Semantically-Meaningful-A-Case-Study-in-Medical-Imaging"><a href="#Are-Vision-Transformer-Representations-Semantically-Meaningful-A-Case-Study-in-Medical-Imaging" class="headerlink" title="Are Vision Transformer Representations Semantically Meaningful? A Case   Study in Medical Imaging"></a>Are Vision Transformer Representations Semantically Meaningful? A Case   Study in Medical Imaging</h2><p><strong>Authors:Montasir Shams, Chashi Mahiul Islam, Shaeke Salman, Phat Tran, Xiuwen Liu</strong></p>
<p>Vision transformers (ViTs) have rapidly gained prominence in medical imaging tasks such as disease classification, segmentation, and detection due to their superior accuracy compared to conventional deep learning models. However, due to their size and complex interactions via the self-attention mechanism, they are not well understood. In particular, it is unclear whether the representations produced by such models are semantically meaningful. In this paper, using a projected gradient-based algorithm, we show that their representations are not semantically meaningful and they are inherently vulnerable to small changes. Images with imperceptible differences can have very different representations; on the other hand, images that should belong to different semantic classes can have nearly identical representations. Such vulnerability can lead to unreliable classification results; for example, unnoticeable changes cause the classification accuracy to be reduced by over 60%. %. To the best of our knowledge, this is the first work to systematically demonstrate this fundamental lack of semantic meaningfulness in ViT representations for medical image classification, revealing a critical challenge for their deployment in safety-critical systems. </p>
<blockquote>
<p>视觉转换器（ViTs）在医学成像任务（如疾病分类、分割和检测）中迅速崭露头角，与传统深度学习模型相比，它们具有更高的准确性。然而，由于其规模和通过自注意力机制产生的复杂交互，人们对其理解尚不充分。尤其不清楚的是，此类模型产生的表示是否具有语义意义。在本文中，我们使用基于投影的梯度算法，证明它们的表示并不具有语义意义，并且本质上是脆弱的，容易受微小变化的影响。具有几乎无法察觉差异的图像可能会有截然不同的表示；另一方面，本应属于不同语义类别的图像可能会有几乎相同的表示。这种脆弱性可能导致分类结果不可靠；例如，难以察觉的变化导致分类准确率降低超过60%。据我们所知，这是首次系统地证明在医学图像分类中ViT表示缺乏基本语义意义的工作，揭示了其在安全关键系统中部署的关键挑战。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01788v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>本文指出，尽管Vision Transformers（ViTs）在医疗图像分类、分割和检测等任务中表现出较高的准确性，但其表示缺乏语义意义，容易受到微小变化的影响。使用基于梯度下降算法的研究表明，微小差异的图像可以有截然不同的表示，而本应属于不同语义类的图像却可能有几乎相同的表示。这种脆弱性导致分类结果不可靠，微小变化可能导致分类准确率下降超过60%。这一发现揭示了ViT在医疗图像分类中的关键挑战，对于其在安全关键系统的部署构成重大挑战。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers（ViTs）在医疗成像任务中表现优异，但存在缺乏语义意义的问题。</li>
<li>ViTs容易受到微小变化的影响，微小差异可能导致截然不同的表示。</li>
<li>本应属于不同语义类的图像可能具有几乎相同的表示。</li>
<li>这种缺乏语义意义的表示会导致分类结果不可靠。</li>
<li>微小变化可能导致ViTs的分类准确率下降超过60%。</li>
<li>这是首次系统地证明ViT在医疗图像分类中表示缺乏语义意义的工作。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01788">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-5ac1d07105f37e318f85b15abe6dba51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09acbc79a6bd2bc9414b76584bf89fa6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-206b0f82c0cb822ebfdaa8a10e431a29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae2a53d0e4b1c20d126622245d3b15e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5da34302d17d91f6375031b4a3691437.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c694f96cd523005fa085639a6ae81b7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb644755d46dd562349f35fe2c753e7d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SPoT-Subpixel-Placement-of-Tokens-in-Vision-Transformers"><a href="#SPoT-Subpixel-Placement-of-Tokens-in-Vision-Transformers" class="headerlink" title="SPoT: Subpixel Placement of Tokens in Vision Transformers"></a>SPoT: Subpixel Placement of Tokens in Vision Transformers</h2><p><strong>Authors:Martine Hjelkrem-Tan, Marius Aasan, Gabriel Y. Arteaga, Adín Ramírez Rivera</strong></p>
<p>Vision Transformers naturally accommodate sparsity, yet standard tokenization methods confine features to discrete patch grids. This constraint prevents models from fully exploiting sparse regimes, forcing awkward compromises. We propose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that positions tokens continuously within images, effectively sidestepping grid-based limitations. With our proposed oracle-guided search, we uncover substantial performance gains achievable with ideal subpixel token positioning, drastically reducing the number of tokens necessary for accurate predictions during inference. SPoT provides a new direction for flexible, efficient, and interpretable ViT architectures, redefining sparsity as a strategic advantage rather than an imposed limitation. </p>
<blockquote>
<p>视觉Transformer自然地适应了稀疏性，但标准的令牌化方法将特征限制在离散补丁网格中。这一约束阻止了模型充分利用稀疏状态，迫使模型做出妥协。我们提出了Subpixel Token放置（SPoT）这种新型令牌化策略，能够在图像中连续放置令牌，有效地避免了基于网格的限制。通过我们提出的以oracle引导搜索，我们发现理想的子像素令牌放置可以实现显著的性能提升，极大地减少了推理过程中进行准确预测所需的令牌数量。SPoT为灵活、高效和可解释的ViT架构提供了新的方向，将稀疏性重新定义为战略优势而不是强制限制。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01654v1">PDF</a> To appear in Workshop on Efficient Computing under Limited Resources:   Visual Computing (ICCV 2025). Code available at   <a target="_blank" rel="noopener" href="https://github.com/dsb-ifi/SPoT">https://github.com/dsb-ifi/SPoT</a></p>
<p><strong>Summary</strong><br>     视觉转换器自然地适应稀疏性，但标准的令牌化方法将特征限制在离散补丁网格中。这种约束阻止了模型充分利用稀疏状态，迫使模型做出妥协。我们提出了Subpixel Placement of Tokens（SPoT）这一新的令牌化策略，该策略能够在图像中连续放置令牌，有效地避免了基于网格的限制。通过提出的oracle引导搜索，我们发现理想的子像素令牌位置可实现显著的性能提升，在推理过程中大大降低了准确预测所需的令牌数量。SPoT为灵活、高效、可解释的ViT架构提供了新的方向，重新定义稀疏性为战略优势而非强制限制。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>视觉转换器具有适应稀疏性的自然特性。</li>
<li>传统的令牌化方法限制特征在离散补丁网格中，阻碍了模型性能的提升。</li>
<li>SPoT策略能够在图像中连续放置令牌，突破基于网格的限制。</li>
<li>通过oracle引导搜索，发现理想子像素令牌位置能显著提高模型性能。</li>
<li>SPoT策略降低了推理过程中准确预测所需的令牌数量，提升了模型的效率。</li>
<li>SPoT为视觉变压器架构的发展提供了新的方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01654">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-3a4625fa3e0b1d64ac4ac62a9b192935.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccd8b5dad37637cd9259909a8bd39e74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0395ed01702e779f815ff82854c7e4dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6d293a689a37d55dba0c23c9a472a2c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-832386f1bf0e4713aca02d1fb94fa9ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-574e7c8386eba3e442979c869a9719a4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SAILViT-Towards-Robust-and-Generalizable-Visual-Backbones-for-MLLMs-via-Gradual-Feature-Refinement"><a href="#SAILViT-Towards-Robust-and-Generalizable-Visual-Backbones-for-MLLMs-via-Gradual-Feature-Refinement" class="headerlink" title="SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via   Gradual Feature Refinement"></a>SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via   Gradual Feature Refinement</h2><p><strong>Authors:Weijie Yin, Dingkang Yang, Hongyuan Dong, Zijian Kang, Jiacong Wang, Xiao Liang, Chao Feng, Jiao Ran</strong></p>
<p>Vision Transformers (ViTs) are essential as foundation backbones in establishing the visual comprehension capabilities of Multimodal Large Language Models (MLLMs). Although most ViTs achieve impressive performance through image-text pair-based contrastive learning or self-supervised mechanisms, they struggle to engage in connector-based co-training directly with LLMs due to potential parameter initialization conflicts and modality semantic gaps. To address the above challenges, this paper proposes SAILViT, a gradual feature learning-enhanced ViT for facilitating MLLMs to break through performance bottlenecks in complex multimodal interactions. SAILViT achieves coarse-to-fine-grained feature alignment and world knowledge infusion with gradual feature refinement, which better serves target training demands. We perform thorough empirical analyses to confirm the powerful robustness and generalizability of SAILViT across different dimensions, including parameter sizes, model architectures, training strategies, and data scales. Equipped with SAILViT, existing MLLMs show significant and consistent performance improvements on the OpenCompass benchmark across extensive downstream tasks. SAILViT series models are released at <a target="_blank" rel="noopener" href="https://huggingface.co/BytedanceDouyinContent">https://huggingface.co/BytedanceDouyinContent</a>. </p>
<blockquote>
<p>视觉Transformer（ViT）作为基础骨干在多模态大型语言模型（MLLM）的视觉理解能力构建中起到了关键作用。虽然大多数ViT通过基于图像文本对比的对比学习或自监督机制取得了令人印象深刻的性能，但由于潜在的参数初始化冲突和模态语义鸿沟，它们在直接与LLM进行基于连接器的协同训练方面遇到了困难。针对上述挑战，本文提出了SAILViT，这是一种用于促进MLLM在复杂多模态交互中突破性能瓶颈的逐步特征学习增强型ViT。SAILViT通过逐步特征细化实现粗到细粒度的特征对齐和世界知识注入，更好地服务于目标训练需求。我们进行了彻底的实证分析，以验证SAILViT在不同维度，包括参数大小、模型架构、训练策略和数据规模上的强大稳健性和通用性。配备SAILViT的现有MLLM在OpenCompass基准测试上表现出显著且一致的性能改进。SAILViT系列模型已发布在<a target="_blank" rel="noopener" href="https://huggingface.co/BytedanceDouyinContent%E4%B8%8A%E3%80%82">https://huggingface.co/BytedanceDouyinContent上。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01643v1">PDF</a> We release SAILViT, a series of versatile vision foundation models</p>
<p><strong>Summary</strong></p>
<p>ViTs作为多模态大语言模型（MLLMs）的视觉理解基础骨架至关重要。尽管大多数ViTs通过图像文本对对比学习或自监督机制取得了令人印象深刻的性能，但它们与LLMs的直接连接器共训练面临着潜在的参数初始化冲突和模态语义鸿沟的挑战。为解决上述问题，本文提出了SAILViT，这是一种具有渐进特征学习能力的ViT，有助于MLLMs在复杂的多模态交互中突破性能瓶颈。SAILViT通过渐进特征细化实现粗到细粒度的特征对齐和世界知识注入，更好地服务于目标训练需求。通过彻底的实证分析，我们证实了SAILViT在不同维度上的强大稳健性和泛化能力，包括参数大小、模型架构、训练策略和数据规模。配备SAILViT的现有MLLMs在OpenCompass基准测试上的下游任务中显示出显著且一致的性能改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) 扮演多模态大语言模型（MLLMs）视觉理解基础骨架的关键角色。</li>
<li>ViTs虽然能通过图像文本对比学习或自监督机制取得良好性能，但在与LLMs直接连接器共训练时面临参数初始化冲突和模态语义鸿沟的挑战。</li>
<li>SAILViT是一种改进的ViT模型，通过渐进特征学习能力，有助于MLLMs在复杂多模态交互中突破性能瓶颈。</li>
<li>SAILViT实现粗到细粒度的特征对齐和世界知识注入。</li>
<li>实证分析表明，SAILViT在参数大小、模型架构、训练策略和数据规模等多个维度上具有强大的稳健性和泛化能力。</li>
<li>配备SAILViT的MLLMs在OpenCompass基准测试上的性能得到显著改善。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01643">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-161b716d91fce0bf90dbb2e992eb9b23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df5e1b88db280b6e446f278284512d37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8da1a94ddc69da7dec268935e713ff9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecb25e319c3431347d005a842d36e0f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cff7d167d9f3a76daca933377f0e516b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TrackingMiM-Efficient-Mamba-in-Mamba-Serialization-for-Real-time-UAV-Object-Tracking"><a href="#TrackingMiM-Efficient-Mamba-in-Mamba-Serialization-for-Real-time-UAV-Object-Tracking" class="headerlink" title="TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV   Object Tracking"></a>TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV   Object Tracking</h2><p><strong>Authors:Bingxi Liu, Calvin Chen, Junhao Li, Guyang Yu, Haoqian Song, Xuchen Liu, Jinqiang Cui, Hong Zhang</strong></p>
<p>The Vision Transformer (ViT) model has long struggled with the challenge of quadratic complexity, a limitation that becomes especially critical in unmanned aerial vehicle (UAV) tracking systems, where data must be processed in real time. In this study, we explore the recently proposed State-Space Model, Mamba, leveraging its computational efficiency and capability for long-sequence modeling to effectively process dense image sequences in tracking tasks. First, we highlight the issue of temporal inconsistency in existing Mamba-based methods, specifically the failure to account for temporal continuity in the Mamba scanning mechanism. Secondly, building upon this insight,we propose TrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model for handling image sequence of tracking problem. In our framework, the mamba scan is performed in a nested way while independently process temporal and spatial coherent patch tokens. While the template frame is encoded as query token and utilized for tracking in every scan. Extensive experiments conducted on five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves state-of-the-art precision while offering noticeable higher speed in UAV tracking. </p>
<blockquote>
<p>Vision Transformer（ViT）模型长期以来一直面临着二次复杂度的挑战，这一限制在无人飞行器（UAV）跟踪系统中尤为关键，因为数据必须实时处理。在这项研究中，我们探索了最近提出的State-Space Model，即Mamba模型，利用其计算效率和长序列建模能力，有效处理跟踪任务中的密集图像序列。首先，我们强调了现有Mamba方法中存在的时间不一致性问题，特别是Mamba扫描机制未能考虑到时间的连续性。其次，基于这一见解，我们提出了TrackingMiM，这是一个Mamba-in-Mamba架构，是一个用于处理跟踪问题图像序列的最小计算负担模型。在我们的框架中，mamba扫描以嵌套的方式进行，同时独立处理时间和空间连贯的补丁令牌。同时，模板帧被编码为查询令牌，并在每次扫描中用于跟踪。在五个UAV跟踪基准测试上进行的广泛实验证实，所提出的TrackingMiM达到了最先进的精度，同时在UAV跟踪中提供了明显的更高速度。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01535v1">PDF</a> 12 pages</p>
<p><strong>Summary</strong></p>
<p>该文章介绍了针对Vision Transformer模型在处理无人机追踪系统中的二次复杂度问题进行研究的结果。文章探讨了最近提出的State-Space模型Mamba在解决该问题方面的潜力，并指出了现有Mamba方法存在的时序不一致性问题。在此基础上，文章提出了一种基于Mamba的TrackingMiM架构，该架构以最小的计算负担处理跟踪问题的图像序列。实验证明，TrackingMiM在五个无人机追踪基准测试中达到了最先进的精度，同时在无人机追踪中提供了更高的速度。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer模型在无人机追踪系统中面临二次复杂度挑战。</li>
<li>State-Space模型Mamba在处理实时数据处理方面具有计算效率和长期序列建模能力。</li>
<li>现有Mamba方法存在时序不一致性问题，未能充分考虑时序连续性。</li>
<li>TrackingMiM架构被提出，它是一种基于Mamba的嵌套扫描方式处理跟踪问题的图像序列的模型。</li>
<li>TrackingMiM能同时处理时序和空间连贯的补丁令牌，并将模板帧编码为查询令牌，用于每次扫描中的跟踪。</li>
<li>TrackingMiM在五个无人机追踪基准测试中实现了最先进的精度。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01535">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-82de78257fa2fb913a03c5199a1290ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd30b6d037260d00604c6b308e77b738.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UPRE-Zero-Shot-Domain-Adaptation-for-Object-Detection-via-Unified-Prompt-and-Representation-Enhancement"><a href="#UPRE-Zero-Shot-Domain-Adaptation-for-Object-Detection-via-Unified-Prompt-and-Representation-Enhancement" class="headerlink" title="UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified   Prompt and Representation Enhancement"></a>UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified   Prompt and Representation Enhancement</h2><p><strong>Authors:Xiao Zhang, Fei Wei, Yong Wang, Wenda Zhao, Feiyi Li, Xiangxiang Chu</strong></p>
<p>Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the lack of images in the target domain. Previous approaches leverage Vision-Language Models (VLMs) to tackle this challenge, exploiting their zero-shot learning capabilities. However, these methods primarily address domain distribution shifts and overlook the misalignment between the detection task and VLMs, which rely on manually crafted prompts. To overcome these limitations, we propose the unified prompt and representation enhancement (UPRE) framework, which jointly optimizes both textual prompts and visual representations. Specifically, our approach introduces a multi-view domain prompt that combines linguistic domain priors with detection-specific knowledge, and a visual representation enhancement module that produces domain style variations. Furthermore, we introduce multi-level enhancement strategies, including relative domain distance and positive-negative separation, which align multi-modal representations at the image level and capture diverse visual representations at the instance level, respectively. Extensive experiments conducted on nine benchmark datasets demonstrate the superior performance of our framework in ZSDA detection scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/UPRE">https://github.com/AMAP-ML/UPRE</a>. </p>
<blockquote>
<p>零样本域自适应（ZSDA）由于目标域缺乏图像而面临巨大挑战。之前的方法利用视觉语言模型（VLM）来应对这一挑战，发挥它们的零样本学习能力。然而，这些方法主要解决域分布转移问题，而忽略了检测任务与VLM之间的不匹配，VLM依赖于手工制作的提示。为了克服这些限制，我们提出了统一提示和表示增强（UPRE）框架，该框架联合优化了文本提示和视觉表示。具体而言，我们的方法引入了一种多视图域提示，将语言域先验知识与检测特定知识相结合，以及一个视觉表示增强模块，用于生成域风格变化。此外，我们引入了多层次增强策略，包括相对域距离和正负分离，分别在图像级别对齐多模式表示，在实例级别捕获多样化的视觉表示。在九个基准数据集上进行的广泛实验表明，我们的框架在ZSDA检测场景中具有卓越的性能。代码可在<a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/UPRE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AMAP-ML/UPRE找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00721v1">PDF</a> ICCV2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为UPRE的统一提示和表示增强框架，用于解决零镜头域适应（ZSDA）检测中的挑战。该框架联合优化文本提示和视觉表示，通过引入多视角域提示和视觉表示增强模块，并结合相对域距离和多级增强策略，提高了ZSDA检测场景的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UPRE框架解决了零镜头域适应（ZSDA）检测中的挑战，通过联合优化文本提示和视觉表示。</li>
<li>引入多视角域提示，结合语言域先验和检测特定知识。</li>
<li>视觉表示增强模块用于生成具有域风格变化的表示。</li>
<li>采用相对域距离和正负分离的多级增强策略，分别对齐图像级别的多模式表示和捕获实例级别的多样视觉表示。</li>
<li>在九个基准数据集上进行的广泛实验证明了UPRE框架在ZSDA检测场景中的优越性。</li>
<li>代码已公开，便于研究者和开发者使用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00721">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-273b5981ed51e8d18776b2fde2a8c49b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3bbdd914ef07f98a2ab2538d3c00b878.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7870f451037050b4697fea48b540040f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9c032ab5a02805d35639010b0cad6e5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LLaVA-SP-Enhancing-Visual-Representation-with-Visual-Spatial-Tokens-for-MLLMs"><a href="#LLaVA-SP-Enhancing-Visual-Representation-with-Visual-Spatial-Tokens-for-MLLMs" class="headerlink" title="LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for   MLLMs"></a>LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for   MLLMs</h2><p><strong>Authors:Haoran Lou, Chunxiao Fan, Ziyan Liu, Yuexin Wu, Xinxiang Wang</strong></p>
<p>The architecture of multimodal large language models (MLLMs) commonly connects a vision encoder, often based on CLIP-ViT, to a large language model. While CLIP-ViT works well for capturing global image features, it struggles to model local relationships between adjacent patches, leading to weaker visual representation, which in turn affects the detailed understanding ability of MLLMs. To solve this, we propose LLaVA-SP, which \textbf{ only adds six spatial visual tokens} to the original visual tokens to enhance the visual representation. Our approach offers three key advantages: 1)We propose a novel Projector, which uses convolutional kernels to derive visual spatial tokens from ViT patch features, simulating two visual spatial ordering approaches: <code>from central region to global&quot; and </code>from abstract to specific”. Then, a cross-attention mechanism is applied to fuse fine-grained visual information, enriching the overall visual representation. 2) We present two model variants: LLaVA-SP-Cropping, which focuses on detail features through progressive cropping, and LLaVA-SP-Pooling, which captures global semantics through adaptive pooling, enabling the model to handle diverse visual understanding tasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA, achieves significant performance improvements across various multimodal benchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple tasks with nearly identical inference latency. The code and models are available at \href{<a target="_blank" rel="noopener" href="https://github.com/CnFaker/LLaVA-SP%7D%7B/texttt%7Bhttps://github.com/CnFaker/LLaVA-SP%7D%7D">https://github.com/CnFaker/LLaVA-SP}{\texttt{https://github.com/CnFaker/LLaVA-SP}}</a>. </p>
<blockquote>
<p>多模态大型语言模型（MLLMs）的架构通常将基于CLIP-ViT的视觉编码器连接到大型语言模型。虽然CLIP-ViT在捕捉全局图像特征方面表现良好，但在对相邻补丁之间的局部关系进行建模时遇到困难，导致视觉表示较弱，进而影响了MLLM的详细理解能力。为了解决这个问题，我们提出了LLaVA-SP，它通过在原始视觉标记中添加六个空间视觉标记来增强视觉表示。我们的方法提供了三个关键优势：</p>
</blockquote>
<p>1）我们提出了一种新型投影仪，该投影仪使用卷积核从ViT补丁特征中推导出视觉空间标记，模拟两种视觉空间排序方法：“从中心区域到全局”和“从抽象到具体”。然后应用交叉注意力机制融合精细的视觉信息，丰富整体的视觉表示。</p>
<p>2）我们推出了两款模型变体：LLaVA-SP-Cropping，通过渐进裁剪关注细节特征；以及LLaVA-SP-Pooling，通过自适应池化捕捉全局语义，使模型能够处理各种视觉理解任务。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00505v1">PDF</a> ICCV</p>
<p><strong>Summary</strong></p>
<p>该文本介绍了一种名为LLaVA-SP的多模态大型语言模型架构改进方案。针对CLIP-ViT在捕捉局部图像关系方面的不足，LLaVA-SP仅添加六个空间视觉令牌来增强视觉表示。该方法通过卷积核生成空间视觉令牌，模拟两种视觉空间排序方法，并通过跨注意力机制融合精细视觉信息，丰富了整体视觉表示。该方法还提供了两种模型变体，并在多种多模态基准测试中实现了显著的性能提升。LLaVA-SP与LoRA微调后，在多任务中优于当前先进的LLaVA-1.5模型，且推理延迟几乎相同。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaVA-SP针对CLIP-ViT在捕捉局部图像关系方面的不足提出了解决方案。</li>
<li>LLaVA-SP通过添加六个空间视觉令牌增强视觉表示。</li>
<li>使用卷积核生成空间视觉令牌，模拟两种视觉空间排序方法。</li>
<li>跨注意力机制用于融合精细视觉信息，丰富整体视觉表示。</li>
<li>LLaVA-SP提供两种模型变体，以适应不同的视觉理解任务。</li>
<li>广泛实验表明，LLaVA-SP在多模态基准测试中实现了显著性能提升。</li>
<li>LLaVA-SP与LoRA微调后，在多任务中优于LLaVA-1.5模型，且推理延迟低。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00505">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-2f2653282561c83ae62076b7a005b7b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa2a46ddb2dbd8cabad494288767ebf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5d407bdd1f8f10609fd8915146f2733.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f55cbef21c35cb975f495f56ca2d935f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d52eb47551eef98db42a77d24551c0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cfdb8c6fef107017a5f302bb4515b1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa62791c3b052a3b73e1e5b761d537a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-912cc9e4ec1be86150becbb0139f1604.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Time-Series-Representations-for-Classification-Lie-Hidden-in-Pretrained-Vision-Transformers"><a href="#Time-Series-Representations-for-Classification-Lie-Hidden-in-Pretrained-Vision-Transformers" class="headerlink" title="Time Series Representations for Classification Lie Hidden in Pretrained   Vision Transformers"></a>Time Series Representations for Classification Lie Hidden in Pretrained   Vision Transformers</h2><p><strong>Authors:Simon Roschmann, Quentin Bouniot, Vasilii Feofanov, Ievgen Redko, Zeynep Akata</strong></p>
<p>Time series classification is a fundamental task in healthcare and industry, yet the development of time series foundation models (TSFMs) remains limited by the scarcity of publicly available time series datasets. In this work, we propose Time Vision Transformer (TiViT), a framework that converts time series into images to leverage the representational power of frozen Vision Transformers (ViTs) pretrained on large-scale image datasets. First, we theoretically motivate our approach by analyzing the 2D patching of ViTs for time series, showing that it can increase the number of label-relevant tokens and reduce the sample complexity. Second, we empirically demonstrate that TiViT achieves state-of-the-art performance on standard time series classification benchmarks by utilizing the hidden representations of large OpenCLIP models. We explore the structure of TiViT representations and find that intermediate layers with high intrinsic dimension are the most effective for time series classification. Finally, we assess the alignment between TiViT and TSFM representation spaces and identify a strong complementarity, with further performance gains achieved by combining their features. Our findings reveal a new direction for reusing vision representations in a non-visual domain. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/TiViT">https://github.com/ExplainableML/TiViT</a>. </p>
<blockquote>
<p>时间序列分类是医疗和工业领域的一项基础任务，然而，由于公开可用时间序列数据集的稀缺，时间序列基础模型（TSFMs）的发展受到限制。在这项工作中，我们提出了时间视觉转换器（TiViT），这是一个将时间序列转换为图像以利用在大型图像数据集上预训练的冻结视觉转换器（ViT）的表示能力的框架。首先，我们通过分析用于时间序列的ViT的2D补丁从理论上证明了我们的方法，表明它可以增加标签相关令牌的数量并降低样本复杂性。其次，我们通过实证证明，TiViT利用大型OpenCLIP模型的隐藏表示，在标准时间序列分类基准测试上实现了最先进的性能。我们探索了TiViT表示的结构，发现具有高内在维度的中间层对时间序列分类最为有效。最后，我们评估了TiViT和TSFM表示空间之间的对齐情况，并发现了强烈的互补性，通过结合它们的特征实现了进一步的性能提升。我们的研究为在非视觉领域重新使用视觉表示提供了新的方向。代码可在<a target="_blank" rel="noopener" href="https://github.com/ExplainableML/TiViT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ExplainableML/TiViT找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08641v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>本文提出了Time Vision Transformer（TiViT）框架，将时间序列转化为图像，利用在大型图像数据集上预训练的冻结Vision Transformer（ViT）的表示能力。理论分析表明，ViT的2D贴片处理可以提高标签相关令牌的数目并降低样本复杂性。在标准时间序列分类基准测试上，TiViT利用大型OpenCLIP模型的隐藏表示取得了最先进的性能。研究发现，具有高中维数的中间层对于时间序列分类最为有效。最后，评估了TiViT与TSFMs的代表性空间对齐情况，发现它们具有很强的互补性，通过结合其特征可以实现进一步的性能提升。这为在非视觉领域重新利用视觉表示提供了新的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Time Vision Transformer (TiViT) 框架能将时间序列转化为图像，利用Vision Transformer（ViT）在大型图像数据集上的预训练表示能力。</li>
<li>理论分析显示ViT的2D贴片处理能提高标签相关令牌的数目并降低样本复杂性。</li>
<li>在标准时间序列分类基准测试中，TiViT取得了最先进的性能。</li>
<li>研究发现，TiViT的中间层对于时间序列分类最为有效，这些层具有高中维数。</li>
<li>TiViT与TSFMs的代表性空间具有强互补性，结合其特征可进一步提高性能。</li>
<li>此研究为在非视觉领域重新利用视觉表示提供了新的方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08641">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-4cd6d8a6e40d67360b1b2d8db4ad89b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-339e8028477fcbc7a85fc2a04775fdfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d5db1d05b0f73808e26833de76f865f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f0fc2851b59daa19fb025977333b987.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d611405351f06e3c06b686cddb70717d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8e820857fde1ba60e882ba2b8843b15.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Semantic-Alignment-and-Reinforcement-for-Data-Free-Quantization-of-Vision-Transformers"><a href="#Semantic-Alignment-and-Reinforcement-for-Data-Free-Quantization-of-Vision-Transformers" class="headerlink" title="Semantic Alignment and Reinforcement for Data-Free Quantization of   Vision Transformers"></a>Semantic Alignment and Reinforcement for Data-Free Quantization of   Vision Transformers</h2><p><strong>Authors:Yunshan Zhong, Yuyao Zhou, Yuxin Zhang, Wanchen Sui, Shen Li, Yong Li, Fei Chao, Rongrong Ji</strong></p>
<p>Data-free quantization (DFQ) enables model quantization without accessing real data, addressing concerns regarding data security and privacy. With the growing adoption of Vision Transformers (ViTs), DFQ for ViTs has garnered significant attention. However, existing DFQ methods exhibit two limitations: (1) semantic distortion, where the semantics of synthetic images deviate substantially from those of real images, and (2) semantic inadequacy, where synthetic images contain extensive regions with limited content and oversimplified textures, leading to suboptimal quantization performance. To address these limitations, we propose SARDFQ, a novel Semantics Alignment and Reinforcement Data-Free Quantization method for ViTs. To address semantic distortion, SARDFQ incorporates Attention Priors Alignment (APA), which optimizes synthetic images to follow randomly generated structure attention priors. To mitigate semantic inadequacy, SARDFQ introduces Multi-Semantic Reinforcement (MSR), leveraging localized patch optimization to enhance semantic richness across synthetic images. Furthermore, SARDFQ employs Soft-Label Learning (SL), wherein multiple semantic targets are adapted to facilitate the learning of multi-semantic images augmented by MSR. Extensive experiments demonstrate the effectiveness of SARDFQ, significantly surpassing existing methods. For example, SARDFQ improves top-1 accuracy on ImageNet by 15.52% for W4A4 ViT-B. The code is at <a target="_blank" rel="noopener" href="https://github.com/zysxmu/SARDFQ">https://github.com/zysxmu/SARDFQ</a>. </p>
<blockquote>
<p>无数据量化（DFQ）能够在无需访问真实数据的情况下实现模型量化，解决了数据安全和隐私的担忧。随着视觉转换器（ViTs）的广泛应用，ViTs的无数据量化（DFQ）已引起广泛关注。然而，现有的DFQ方法存在两个局限性：（1）语义失真，即合成图像的语义与真实图像的语义存在较大偏差；（2）语义不足，合成图像包含大量内容有限、纹理过于简单的区域，导致量化性能不佳。为了解决这些局限性，我们提出了SARDFQ，一种用于视觉转换器的新型语义对齐和强化无数据量化方法。为了解决语义失真问题，SARDFQ结合了注意力先验对齐（APA），优化合成图像以遵循随机生成的结构注意力先验。为了缓解语义不足的问题，SARDFQ引入了多语义强化（MSR），利用局部补丁优化来增强合成图像中的语义丰富性。此外，SARDFQ采用软标签学习（SL），适应多个语义目标，以促进由MSR增强的多语义图像的学习。大量实验证明了SARDFQ的有效性，显著超越了现有方法。例如，SARDFQ在ImageNet上的top-1准确率提高了15.52%，适用于W4A4 ViT-B。代码地址为：<a target="_blank" rel="noopener" href="https://github.com/zysxmu/SARDFQ%E3%80%82">https://github.com/zysxmu/SARDFQ。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16553v3">PDF</a> ICCV2025</p>
<p><strong>Summary</strong><br>数据无关量化（DFQ）可在不接触真实数据的情况下实现模型量化，关注数据安全和隐私保护。针对视觉变压器（ViTs）的DFQ存在语义失真和语义不足的问题。为此，提出一种新型的语义对齐和强化数据无关量化方法SARDFQ。通过优化合成图像遵循随机生成的结构注意力先验来解决语义失真问题。此外，SARDFQ引入了多语义强化（MSR）来减轻语义不足的问题，通过局部补丁优化增强合成图像的语义丰富性。采用软标签学习（SL），适应多个语义目标，促进由MSR增强的多语义图像的学习。实验证明SARDFQ效果显著，显著优于现有方法，例如W4A4 ViT-B在ImageNet上的top-1准确率提高15.52%。详情可访问相关代码库。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>数据无关量化（DFQ）允许在不接触真实数据的情况下进行模型量化，注重数据安全和隐私。</li>
<li>针对ViTs的现有DFQ方法存在语义失真和语义不足的问题。</li>
<li>SARDFQ是一种新型的针对ViTs的DFQ方法，通过Attention Priors Alignment (APA)解决语义失真问题。</li>
<li>SARDFQ引入Multi-Semantic Reinforcement (MSR)来增强合成图像的语义丰富性，解决语义不足的问题。</li>
<li>SARDFQ采用Soft-Label Learning (SL)，适应多个语义目标，促进多语义图像的学习。</li>
<li>实验证明SARDFQ在性能上显著优于现有方法。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16553">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-424cca03769ca8455ab8b48ac3d7556f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd05a03ab7e79073e1962828e14e023a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18fa203703979bf85a9848cdeea83d76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27d74ebe7e661fa8002e28903c1a4c75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c766735de2aff444b0351c2a19c5c39.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Advancing-Textual-Prompt-Learning-with-Anchored-Attributes"><a href="#Advancing-Textual-Prompt-Learning-with-Anchored-Attributes" class="headerlink" title="Advancing Textual Prompt Learning with Anchored Attributes"></a>Advancing Textual Prompt Learning with Anchored Attributes</h2><p><strong>Authors:Zheng Li, Yibing Song, Ming-Ming Cheng, Xiang Li, Jian Yang</strong></p>
<p>Textual-based prompt learning methods primarily employ multiple learnable soft prompts and hard class tokens in a cascading manner as text inputs, aiming to align image and text (category) spaces for downstream tasks. However, current training is restricted to aligning images with predefined known categories and cannot be associated with unknown categories. In this work, we propose utilizing universal attributes as a bridge to enhance the alignment between images and unknown categories. Specifically, we introduce an Attribute-anchored Textual Prompt learning method for vision-language models, named ATPrompt. This approach expands the learning space of soft prompts from the original one-dimensional category level into the multi-dimensional attribute level by incorporating multiple attribute tokens into the learnable soft prompts. Through this modification, we transform the text prompt from a category-centric form to an attribute-category hybrid form. Additionally, we introduce a straightforward differentiable attribute search method to identify representative and suitable attributes for downstream tasks. As an easy-to-use plug-in technique, ATPrompt can seamlessly replace the existing basic prompt format in textual-based methods, providing general improvements at a negligible computational cost. Extensive experiments across 11 datasets validate the effectiveness of our method. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/zhengli97/ATPrompt">https://github.com/zhengli97/ATPrompt</a>. </p>
<blockquote>
<p>基于文本的提示学习方法主要使用多个可学习的软提示和硬类别令牌以级联方式作为文本输入，旨在对齐图像和文本（类别）空间以进行下游任务。然而，当前训练仅限于对齐图像与预定义的已知类别，无法与未知类别相关联。在本文中，我们提出利用通用属性作为桥梁，增强图像与未知类别之间的对齐。具体来说，我们为视觉语言模型引入了一种名为ATPrompt的属性锚定文本提示学习方法。该方法通过将多个属性令牌融入可学习的软提示中，将软提示的学习空间从原始的一维类别层面扩展到多维属性层面。通过这一改进，我们将文本提示从以类别为中心的形式转变为属性-类别混合形式。此外，我们还引入了一种简单的可区分属性搜索方法，用于识别下游任务的代表性和合适属性。作为一种易于使用的插件技术，ATPrompt可以无缝替换基于文本方法中的基本提示格式，以微小的计算成本提供一般的改进。在11个数据集上的大量实验验证了我们的方法的有效性。代码公开在<a target="_blank" rel="noopener" href="https://github.com/zhengli97/ATPrompt%E3%80%82">https://github.com/zhengli97/ATPrompt。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09442v3">PDF</a> ICCV 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/zhengli97/ATPrompt">https://github.com/zhengli97/ATPrompt</a>. Project Page:   <a target="_blank" rel="noopener" href="https://zhengli97.github.io/ATPrompt/">https://zhengli97.github.io/ATPrompt/</a></p>
<p><strong>Summary</strong><br>在文本基于提示的学习方法基础上，提出一种利用通用属性作为桥梁，增强图像与未知类别之间对齐的方法。引入属性锚定的文本提示学习方法，通过结合多个属性标记，将软提示的学习空间从原始的一维类别层面扩展到多维属性层面。此外，还介绍了一种简单的可微属性搜索方法，用于确定下游任务的代表性属性。作为一种易于使用的插件技术，ATPrompt可以无缝地替换现有文本提示方法中的基本格式，以微小的计算成本提供一般性的改进。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>文本基于提示的学习方法主要通过级联多个可学习的软提示和硬类别标记作为文本输入，旨在实现对下游任务中的图像和文本（类别）空间的对齐。</li>
<li>当前训练限制在于只能对齐已知类别的图像，无法关联未知类别。</li>
<li>提出利用通用属性作为桥梁，增强图像与未知类别之间的对齐。</li>
<li>引入属性锚定的文本提示学习方法（ATPrompt），将软提示的学习空间扩展到多维属性层面。</li>
<li>ATPrompt将文本提示从以类别为中心的形式转变为属性-类别混合形式。</li>
<li>ATPrompt提供了一种简单的可微属性搜索方法，用于确定适合下游任务的代表性属性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09442">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-e763c344dbf3dadca4ec14afa7ee853d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06aa6e2fefe8dde4dba31dba7bfd79b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd090d32be6751498824bcaaf161199e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-832074a8042eb9ad649239ab4da874f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad93f77b5a9881b0d5e71313c056e0b4.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Semantic-Equitable-Clustering-A-Simple-and-Effective-Strategy-for-Clustering-Vision-Tokens"><a href="#Semantic-Equitable-Clustering-A-Simple-and-Effective-Strategy-for-Clustering-Vision-Tokens" class="headerlink" title="Semantic Equitable Clustering: A Simple and Effective Strategy for   Clustering Vision Tokens"></a>Semantic Equitable Clustering: A Simple and Effective Strategy for   Clustering Vision Tokens</h2><p><strong>Authors:Qihang Fan, Huaibo Huang, Mingrui Chen, Ran He</strong></p>
<p>The Vision Transformer (ViT) has gained prominence for its superior relational modeling prowess. However, its global attention mechanism’s quadratic complexity poses substantial computational burdens. A common remedy spatially groups tokens for self-attention, reducing computational requirements. Nonetheless, this strategy neglects semantic information in tokens, possibly scattering semantically-linked tokens across distinct groups, thus compromising the efficacy of self-attention intended for modeling inter-token dependencies. Motivated by these insights, we introduce a fast and balanced clustering method, named Semantic Equitable Clustering (SEC). SEC clusters tokens based on their global semantic relevance in an efficient, straightforward manner. In contrast to traditional clustering methods requiring multiple iterations, our method achieves token clustering in a single pass. Additionally, SEC regulates the number of tokens per cluster, ensuring a balanced distribution for effective parallel processing on current computational platforms without necessitating further optimization. Capitalizing on SEC, we propose a versatile vision backbone, SECViT. Comprehensive experiments in image classification, object detection, instance segmentation, and semantic segmentation validate the effectiveness of SECViT. Moreover, SEC can be conveniently and swiftly applied to multimodal large language models (MLLM), such as LLaVA, to serve as a vision language connector, effectively accelerating the model’s efficiency while maintaining unchanged or better performance. </p>
<blockquote>
<p>Vision Transformer（ViT）因其出色的关系建模能力而备受瞩目。然而，其全局注意力机制的二次复杂性带来了巨大的计算负担。一种常见的补救方法是空间分组令牌进行自注意力，以降低计算要求。然而，这种策略忽略了令牌中的语义信息，可能将语义上相关联的令牌分散到不同的组，从而损害自注意力在建模令牌间依赖关系时的有效性。</p>
</blockquote>
<p>基于这些见解，我们引入了一种快速且均衡的聚类方法，称为语义均衡聚类（SEC）。SEC以高效、简单的方式，基于令牌的全局语义相关性对令牌进行聚类。与传统的需要多次迭代的聚类方法不同，我们的方法可以在单次传递中实现令牌聚类。此外，SEC还调节每个集群中的令牌数量，确保在当前计算平台上进行有效的并行处理，而无需进一步优化。</p>
<p>利用SEC，我们提出了一种通用的视觉主干网SECViT。在图像分类、目标检测、实例分割和语义分割方面的综合实验验证了SECViT的有效性。此外，SEC还可以轻松快速地应用于多模态大型语言模型（MLLM），如LLaVA，作为视觉语言连接器，在保持性能不变或更好的情况下有效提高模型的效率。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.13337v3">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>本文介绍了Vision Transformer（ViT）在关系建模方面的优势，但其全局注意力机制的二次复杂性带来了较大的计算负担。为解决这一问题，提出了一种快速均衡聚类方法——Semantic Equitable Clustering（SEC）。SEC基于全局语义相关性对令牌进行聚类，实现了高效直接的聚类方式。相较于传统聚类方法的多迭代过程，SEC实现了单遍聚类。此外，SEC调控每个集群的令牌数量，确保了在现有计算平台上的有效并行处理。基于SEC，提出了一种通用的视觉主干SECViT。在图像分类、目标检测、实例分割和语义分割等方面的实验验证了SECViT的有效性。此外，SEC可轻松快速地应用于多模态大型语言模型（MLLM），如LLaVA，作为视觉语言连接器，在保持性能不变或更好的情况下，有效提高模型的效率。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer (ViT) 展现出卓越的关系建模能力，但其全局注意力机制的二次复杂性带来计算负担。</li>
<li>现有的空间分组令牌策略忽略了令牌间的语义信息，可能影响建模效果。</li>
<li>引入了一种快速均衡聚类方法——Semantic Equitable Clustering (SEC)，基于全局语义相关性进行高效直接的聚类。</li>
<li>SEC实现了单遍聚类过程，并调控每个集群的令牌数量，便于在现有计算平台上的并行处理。</li>
<li>基于SEC提出了SECViT模型，用于提高图像分类、目标检测等任务的性能。</li>
<li>SEC可以方便地应用于多模态大型语言模型（MLLM），作为视觉语言连接器以提高模型效率并保持性能。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.13337">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-4ba5727f96ec81c5f89be589e85b75f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aea9e48ff10c0b2c0b406aa031344485.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cad6b7473473e9d3c1a1472de9918d9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a71709a38c55bb7ecb49c55d2ca0606f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b396250839dfccd8e0743ef4e54f5eb.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-dd30b6d037260d00604c6b308e77b738.jpg" class="responsive-img" alt="检测/分割/跟踪">
                        
                        <span class="card-title">检测/分割/跟踪</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            检测/分割/跟踪 方向最新论文已更新，请持续关注 Update in 2025-07-04  A Gift from the Integration of Discriminative and Diffusion-based   Generative Learning Boundary Refinement Remote Sensing Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    检测/分割/跟踪
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">检测/分割/跟踪</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b92c85046bac61be54c371272062d92f.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation 方向最新论文已更新，请持续关注 Update in 2025-07-04  How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25879.3k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
