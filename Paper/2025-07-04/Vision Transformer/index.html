<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-04  evMLP An Efficient Event-Driven MLP Architecture for Vision">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-ae2a53d0e4b1c20d126622245d3b15e1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    41 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-04-æ›´æ–°"><a href="#2025-07-04-æ›´æ–°" class="headerlink" title="2025-07-04 æ›´æ–°"></a>2025-07-04 æ›´æ–°</h1><h2 id="evMLP-An-Efficient-Event-Driven-MLP-Architecture-for-Vision"><a href="#evMLP-An-Efficient-Event-Driven-MLP-Architecture-for-Vision" class="headerlink" title="evMLP: An Efficient Event-Driven MLP Architecture for Vision"></a>evMLP: An Efficient Event-Driven MLP Architecture for Vision</h2><p><strong>Authors:Zhentan Zheng</strong></p>
<p>Deep neural networks have achieved remarkable results in computer vision tasks. In the early days, Convolutional Neural Networks (CNNs) were the mainstream architecture. In recent years, Vision Transformers (ViTs) have become increasingly popular. In addition, exploring applications of multi-layer perceptrons (MLPs) has provided new perspectives for research into vision model architectures. In this paper, we present evMLP accompanied by a simple event-driven local update mechanism. The proposed evMLP can independently process patches on images or feature maps via MLPs. We define changes between consecutive frames as â€œeventsâ€. Under the event-driven local update mechanism, evMLP selectively processes patches where events occur. For sequential image data (e.g., video processing), this approach improves computational performance by avoiding redundant computations. Through ImageNet image classification experiments, evMLP attains accuracy competitive with state-of-the-art models. More significantly, experimental results on multiple video datasets demonstrate that evMLP reduces computational cost via its event-driven local update mechanism while maintaining output consistency with its non-event-driven baseline. The code and trained models are available at <a target="_blank" rel="noopener" href="https://github.com/i-evi/evMLP">https://github.com/i-evi/evMLP</a>. </p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚åœ¨æ—©æœŸï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰æ˜¯ä¸»æµæ¶æ„ã€‚è¿‘å¹´æ¥ï¼Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚æ­¤å¤–ï¼Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰çš„åº”ç”¨ç ”ç©¶ä¸ºè§†è§‰æ¨¡å‹æ¶æ„çš„ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¸¦æœ‰ç®€å•äº‹ä»¶é©±åŠ¨å±€éƒ¨æ›´æ–°æœºåˆ¶çš„evMLPã€‚æ‰€æå‡ºçš„evMLPå¯ä»¥é€šè¿‡MLPç‹¬ç«‹å¤„ç†å›¾åƒæˆ–ç‰¹å¾å›¾ä¸Šçš„è¡¥ä¸ã€‚æˆ‘ä»¬å°†è¿ç»­å¸§ä¹‹é—´çš„å˜åŒ–å®šä¹‰ä¸ºâ€œäº‹ä»¶â€ã€‚åœ¨äº‹ä»¶é©±åŠ¨çš„å±€éƒ¨æ›´æ–°æœºåˆ¶ä¸‹ï¼ŒevMLPä¼šé€‰æ‹©æ€§åœ°å¤„ç†å‘ç”Ÿäº‹ä»¶çš„åœ°æ–¹ã€‚å¯¹äºåºåˆ—å›¾åƒæ•°æ®ï¼ˆä¾‹å¦‚è§†é¢‘å¤„ç†ï¼‰ï¼Œè¿™ç§æ–¹æ³•é€šè¿‡é¿å…å†—ä½™è®¡ç®—æ¥æé«˜è®¡ç®—æ€§èƒ½ã€‚é€šè¿‡ImageNetå›¾åƒåˆ†ç±»å®éªŒï¼ŒevMLPçš„å‡†ç¡®ç‡è¾¾åˆ°äº†å‰æ²¿æ¨¡å‹çš„ç«äº‰æ°´å¹³ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå¤šä¸ªè§†é¢‘æ•°æ®é›†çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒevMLPé€šè¿‡å…¶äº‹ä»¶é©±åŠ¨çš„å±€éƒ¨æ›´æ–°æœºåˆ¶é™ä½äº†è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†ä¸éäº‹ä»¶é©±åŠ¨åŸºå‡†æ¨¡å‹çš„è¾“å‡ºä¸€è‡´æ€§ã€‚ä»£ç å’Œè®­ç»ƒå¥½çš„æ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/i-evi/evMLP%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/i-evi/evMLPæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01927v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†evMLPæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰å’Œäº‹ä»¶é©±åŠ¨å±€éƒ¨æ›´æ–°æœºåˆ¶ã€‚evMLPèƒ½ç‹¬ç«‹å¤„ç†å›¾åƒæˆ–ç‰¹å¾å›¾ä¸Šçš„å—ï¼Œå¯¹è¿ç»­å¸§ä¹‹é—´çš„å˜åŒ–å®šä¹‰ä¸ºâ€œäº‹ä»¶â€ï¼Œå¹¶ä»…é€‰æ‹©äº‹ä»¶å‘ç”Ÿçš„å—è¿›è¡Œå¤„ç†ã€‚è¿™ç§æ–¹æ³•æé«˜äº†è®¡ç®—æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†åºåˆ—å›¾åƒæ•°æ®ï¼ˆå¦‚è§†é¢‘å¤„ç†ï¼‰æ—¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒevMLPåœ¨ç»´æŒè¾“å‡ºä¸€è‡´æ€§çš„åŒæ—¶ï¼Œé€šè¿‡äº‹ä»¶é©±åŠ¨å±€éƒ¨æ›´æ–°æœºåˆ¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>evMLPç»“åˆäº†å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPsï¼‰å’Œäº‹ä»¶é©±åŠ¨å±€éƒ¨æ›´æ–°æœºåˆ¶ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è§†è§‰æ¨¡å‹æ¶æ„ã€‚</li>
<li>evMLPèƒ½ç‹¬ç«‹å¤„ç†å›¾åƒæˆ–ç‰¹å¾å›¾ä¸Šçš„å—ã€‚</li>
<li>äº‹ä»¶è¢«å®šä¹‰ä¸ºè¿ç»­å¸§ä¹‹é—´çš„å˜åŒ–ï¼ŒevMLPä»…é€‰æ‹©äº‹ä»¶å‘ç”Ÿçš„å—è¿›è¡Œå¤„ç†ã€‚</li>
<li>è¿™ç§å¤„ç†æ–¹æ³•æé«˜äº†è®¡ç®—æ€§èƒ½ï¼Œå°¤å…¶åœ¨å¤„ç†åºåˆ—å›¾åƒæ•°æ®æ—¶ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒevMLPåœ¨ImageNetå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­å…·æœ‰ä¸æœ€æ–°æ¨¡å‹ç«äº‰çš„å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨å¤šä¸ªè§†é¢‘æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒevMLPé€šè¿‡äº‹ä»¶é©±åŠ¨å±€éƒ¨æ›´æ–°æœºåˆ¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01927">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-63ec1845597d903fc0553d8e4473c2c1.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bd7ce53c751232af299156e0e48afa56.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d1cde5b42e63a4bd1e1a2cddf3933435.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a7f3990ba5c67e3a272d2e58a2eabf41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3905a8e39d86efabbfd963cc6d37371.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-81dafd93d6573ec28e0053bd66c9fca4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-62fc75f10eabf354858cea6fc62bc366.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Are-Vision-Transformer-Representations-Semantically-Meaningful-A-Case-Study-in-Medical-Imaging"><a href="#Are-Vision-Transformer-Representations-Semantically-Meaningful-A-Case-Study-in-Medical-Imaging" class="headerlink" title="Are Vision Transformer Representations Semantically Meaningful? A Case   Study in Medical Imaging"></a>Are Vision Transformer Representations Semantically Meaningful? A Case   Study in Medical Imaging</h2><p><strong>Authors:Montasir Shams, Chashi Mahiul Islam, Shaeke Salman, Phat Tran, Xiuwen Liu</strong></p>
<p>Vision transformers (ViTs) have rapidly gained prominence in medical imaging tasks such as disease classification, segmentation, and detection due to their superior accuracy compared to conventional deep learning models. However, due to their size and complex interactions via the self-attention mechanism, they are not well understood. In particular, it is unclear whether the representations produced by such models are semantically meaningful. In this paper, using a projected gradient-based algorithm, we show that their representations are not semantically meaningful and they are inherently vulnerable to small changes. Images with imperceptible differences can have very different representations; on the other hand, images that should belong to different semantic classes can have nearly identical representations. Such vulnerability can lead to unreliable classification results; for example, unnoticeable changes cause the classification accuracy to be reduced by over 60%. %. To the best of our knowledge, this is the first work to systematically demonstrate this fundamental lack of semantic meaningfulness in ViT representations for medical image classification, revealing a critical challenge for their deployment in safety-critical systems. </p>
<blockquote>
<p>è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰åœ¨åŒ»å­¦æˆåƒä»»åŠ¡ï¼ˆå¦‚ç–¾ç—…åˆ†ç±»ã€åˆ†å‰²å’Œæ£€æµ‹ï¼‰ä¸­è¿…é€Ÿå´­éœ²å¤´è§’ï¼Œä¸ä¼ ç»Ÿæ·±åº¦å­¦ä¹ æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒä»¬å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç”±äºå…¶è§„æ¨¡å’Œé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶äº§ç”Ÿçš„å¤æ‚äº¤äº’ï¼Œäººä»¬å¯¹å…¶ç†è§£å°šä¸å……åˆ†ã€‚å°¤å…¶ä¸æ¸…æ¥šçš„æ˜¯ï¼Œæ­¤ç±»æ¨¡å‹äº§ç”Ÿçš„è¡¨ç¤ºæ˜¯å¦å…·æœ‰è¯­ä¹‰æ„ä¹‰ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºæŠ•å½±çš„æ¢¯åº¦ç®—æ³•ï¼Œè¯æ˜å®ƒä»¬çš„è¡¨ç¤ºå¹¶ä¸å…·æœ‰è¯­ä¹‰æ„ä¹‰ï¼Œå¹¶ä¸”æœ¬è´¨ä¸Šæ˜¯è„†å¼±çš„ï¼Œå®¹æ˜“å—å¾®å°å˜åŒ–çš„å½±å“ã€‚å…·æœ‰å‡ ä¹æ— æ³•å¯Ÿè§‰å·®å¼‚çš„å›¾åƒå¯èƒ½ä¼šæœ‰æˆªç„¶ä¸åŒçš„è¡¨ç¤ºï¼›å¦ä¸€æ–¹é¢ï¼Œæœ¬åº”å±äºä¸åŒè¯­ä¹‰ç±»åˆ«çš„å›¾åƒå¯èƒ½ä¼šæœ‰å‡ ä¹ç›¸åŒçš„è¡¨ç¤ºã€‚è¿™ç§è„†å¼±æ€§å¯èƒ½å¯¼è‡´åˆ†ç±»ç»“æœä¸å¯é ï¼›ä¾‹å¦‚ï¼Œéš¾ä»¥å¯Ÿè§‰çš„å˜åŒ–å¯¼è‡´åˆ†ç±»å‡†ç¡®ç‡é™ä½è¶…è¿‡60%ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡ç³»ç»Ÿåœ°è¯æ˜åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­ViTè¡¨ç¤ºç¼ºä¹åŸºæœ¬è¯­ä¹‰æ„ä¹‰çš„å·¥ä½œï¼Œæ­ç¤ºäº†å…¶åœ¨å®‰å…¨å…³é”®ç³»ç»Ÿä¸­éƒ¨ç½²çš„å…³é”®æŒ‘æˆ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01788v1">PDF</a> 9 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æŒ‡å‡ºï¼Œå°½ç®¡Vision Transformersï¼ˆViTsï¼‰åœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ã€åˆ†å‰²å’Œæ£€æµ‹ç­‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œä½†å…¶è¡¨ç¤ºç¼ºä¹è¯­ä¹‰æ„ä¹‰ï¼Œå®¹æ˜“å—åˆ°å¾®å°å˜åŒ–çš„å½±å“ã€‚ä½¿ç”¨åŸºäºæ¢¯åº¦ä¸‹é™ç®—æ³•çš„ç ”ç©¶è¡¨æ˜ï¼Œå¾®å°å·®å¼‚çš„å›¾åƒå¯ä»¥æœ‰æˆªç„¶ä¸åŒçš„è¡¨ç¤ºï¼Œè€Œæœ¬åº”å±äºä¸åŒè¯­ä¹‰ç±»çš„å›¾åƒå´å¯èƒ½æœ‰å‡ ä¹ç›¸åŒçš„è¡¨ç¤ºã€‚è¿™ç§è„†å¼±æ€§å¯¼è‡´åˆ†ç±»ç»“æœä¸å¯é ï¼Œå¾®å°å˜åŒ–å¯èƒ½å¯¼è‡´åˆ†ç±»å‡†ç¡®ç‡ä¸‹é™è¶…è¿‡60%ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†ViTåœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œå¯¹äºå…¶åœ¨å®‰å…¨å…³é”®ç³»ç»Ÿçš„éƒ¨ç½²æ„æˆé‡å¤§æŒ‘æˆ˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformersï¼ˆViTsï¼‰åœ¨åŒ»ç–—æˆåƒä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†å­˜åœ¨ç¼ºä¹è¯­ä¹‰æ„ä¹‰çš„é—®é¢˜ã€‚</li>
<li>ViTså®¹æ˜“å—åˆ°å¾®å°å˜åŒ–çš„å½±å“ï¼Œå¾®å°å·®å¼‚å¯èƒ½å¯¼è‡´æˆªç„¶ä¸åŒçš„è¡¨ç¤ºã€‚</li>
<li>æœ¬åº”å±äºä¸åŒè¯­ä¹‰ç±»çš„å›¾åƒå¯èƒ½å…·æœ‰å‡ ä¹ç›¸åŒçš„è¡¨ç¤ºã€‚</li>
<li>è¿™ç§ç¼ºä¹è¯­ä¹‰æ„ä¹‰çš„è¡¨ç¤ºä¼šå¯¼è‡´åˆ†ç±»ç»“æœä¸å¯é ã€‚</li>
<li>å¾®å°å˜åŒ–å¯èƒ½å¯¼è‡´ViTsçš„åˆ†ç±»å‡†ç¡®ç‡ä¸‹é™è¶…è¿‡60%ã€‚</li>
<li>è¿™æ˜¯é¦–æ¬¡ç³»ç»Ÿåœ°è¯æ˜ViTåœ¨åŒ»ç–—å›¾åƒåˆ†ç±»ä¸­è¡¨ç¤ºç¼ºä¹è¯­ä¹‰æ„ä¹‰çš„å·¥ä½œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01788">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5ac1d07105f37e318f85b15abe6dba51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-09acbc79a6bd2bc9414b76584bf89fa6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-206b0f82c0cb822ebfdaa8a10e431a29.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae2a53d0e4b1c20d126622245d3b15e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5da34302d17d91f6375031b4a3691437.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c694f96cd523005fa085639a6ae81b7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cb644755d46dd562349f35fe2c753e7d.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="SPoT-Subpixel-Placement-of-Tokens-in-Vision-Transformers"><a href="#SPoT-Subpixel-Placement-of-Tokens-in-Vision-Transformers" class="headerlink" title="SPoT: Subpixel Placement of Tokens in Vision Transformers"></a>SPoT: Subpixel Placement of Tokens in Vision Transformers</h2><p><strong>Authors:Martine Hjelkrem-Tan, Marius Aasan, Gabriel Y. Arteaga, AdÃ­n RamÃ­rez Rivera</strong></p>
<p>Vision Transformers naturally accommodate sparsity, yet standard tokenization methods confine features to discrete patch grids. This constraint prevents models from fully exploiting sparse regimes, forcing awkward compromises. We propose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that positions tokens continuously within images, effectively sidestepping grid-based limitations. With our proposed oracle-guided search, we uncover substantial performance gains achievable with ideal subpixel token positioning, drastically reducing the number of tokens necessary for accurate predictions during inference. SPoT provides a new direction for flexible, efficient, and interpretable ViT architectures, redefining sparsity as a strategic advantage rather than an imposed limitation. </p>
<blockquote>
<p>è§†è§‰Transformerè‡ªç„¶åœ°é€‚åº”äº†ç¨€ç–æ€§ï¼Œä½†æ ‡å‡†çš„ä»¤ç‰ŒåŒ–æ–¹æ³•å°†ç‰¹å¾é™åˆ¶åœ¨ç¦»æ•£è¡¥ä¸ç½‘æ ¼ä¸­ã€‚è¿™ä¸€çº¦æŸé˜»æ­¢äº†æ¨¡å‹å……åˆ†åˆ©ç”¨ç¨€ç–çŠ¶æ€ï¼Œè¿«ä½¿æ¨¡å‹åšå‡ºå¦¥åã€‚æˆ‘ä»¬æå‡ºäº†Subpixel Tokenæ”¾ç½®ï¼ˆSPoTï¼‰è¿™ç§æ–°å‹ä»¤ç‰ŒåŒ–ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨å›¾åƒä¸­è¿ç»­æ”¾ç½®ä»¤ç‰Œï¼Œæœ‰æ•ˆåœ°é¿å…äº†åŸºäºç½‘æ ¼çš„é™åˆ¶ã€‚é€šè¿‡æˆ‘ä»¬æå‡ºçš„ä»¥oracleå¼•å¯¼æœç´¢ï¼Œæˆ‘ä»¬å‘ç°ç†æƒ³çš„å­åƒç´ ä»¤ç‰Œæ”¾ç½®å¯ä»¥å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œæå¤§åœ°å‡å°‘äº†æ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œå‡†ç¡®é¢„æµ‹æ‰€éœ€çš„ä»¤ç‰Œæ•°é‡ã€‚SPoTä¸ºçµæ´»ã€é«˜æ•ˆå’Œå¯è§£é‡Šçš„ViTæ¶æ„æä¾›äº†æ–°çš„æ–¹å‘ï¼Œå°†ç¨€ç–æ€§é‡æ–°å®šä¹‰ä¸ºæˆ˜ç•¥ä¼˜åŠ¿è€Œä¸æ˜¯å¼ºåˆ¶é™åˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01654v1">PDF</a> To appear in Workshop on Efficient Computing under Limited Resources:   Visual Computing (ICCV 2025). Code available at   <a target="_blank" rel="noopener" href="https://github.com/dsb-ifi/SPoT">https://github.com/dsb-ifi/SPoT</a></p>
<p><strong>Summary</strong><br>     è§†è§‰è½¬æ¢å™¨è‡ªç„¶åœ°é€‚åº”ç¨€ç–æ€§ï¼Œä½†æ ‡å‡†çš„ä»¤ç‰ŒåŒ–æ–¹æ³•å°†ç‰¹å¾é™åˆ¶åœ¨ç¦»æ•£è¡¥ä¸ç½‘æ ¼ä¸­ã€‚è¿™ç§çº¦æŸé˜»æ­¢äº†æ¨¡å‹å……åˆ†åˆ©ç”¨ç¨€ç–çŠ¶æ€ï¼Œè¿«ä½¿æ¨¡å‹åšå‡ºå¦¥åã€‚æˆ‘ä»¬æå‡ºäº†Subpixel Placement of Tokensï¼ˆSPoTï¼‰è¿™ä¸€æ–°çš„ä»¤ç‰ŒåŒ–ç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿåœ¨å›¾åƒä¸­è¿ç»­æ”¾ç½®ä»¤ç‰Œï¼Œæœ‰æ•ˆåœ°é¿å…äº†åŸºäºç½‘æ ¼çš„é™åˆ¶ã€‚é€šè¿‡æå‡ºçš„oracleå¼•å¯¼æœç´¢ï¼Œæˆ‘ä»¬å‘ç°ç†æƒ³çš„å­åƒç´ ä»¤ç‰Œä½ç½®å¯å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­å¤§å¤§é™ä½äº†å‡†ç¡®é¢„æµ‹æ‰€éœ€çš„ä»¤ç‰Œæ•°é‡ã€‚SPoTä¸ºçµæ´»ã€é«˜æ•ˆã€å¯è§£é‡Šçš„ViTæ¶æ„æä¾›äº†æ–°çš„æ–¹å‘ï¼Œé‡æ–°å®šä¹‰ç¨€ç–æ€§ä¸ºæˆ˜ç•¥ä¼˜åŠ¿è€Œéå¼ºåˆ¶é™åˆ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰è½¬æ¢å™¨å…·æœ‰é€‚åº”ç¨€ç–æ€§çš„è‡ªç„¶ç‰¹æ€§ã€‚</li>
<li>ä¼ ç»Ÿçš„ä»¤ç‰ŒåŒ–æ–¹æ³•é™åˆ¶ç‰¹å¾åœ¨ç¦»æ•£è¡¥ä¸ç½‘æ ¼ä¸­ï¼Œé˜»ç¢äº†æ¨¡å‹æ€§èƒ½çš„æå‡ã€‚</li>
<li>SPoTç­–ç•¥èƒ½å¤Ÿåœ¨å›¾åƒä¸­è¿ç»­æ”¾ç½®ä»¤ç‰Œï¼Œçªç ´åŸºäºç½‘æ ¼çš„é™åˆ¶ã€‚</li>
<li>é€šè¿‡oracleå¼•å¯¼æœç´¢ï¼Œå‘ç°ç†æƒ³å­åƒç´ ä»¤ç‰Œä½ç½®èƒ½æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>SPoTç­–ç•¥é™ä½äº†æ¨ç†è¿‡ç¨‹ä¸­å‡†ç¡®é¢„æµ‹æ‰€éœ€çš„ä»¤ç‰Œæ•°é‡ï¼Œæå‡äº†æ¨¡å‹çš„æ•ˆç‡ã€‚</li>
<li>SPoTä¸ºè§†è§‰å˜å‹å™¨æ¶æ„çš„å‘å±•æä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01654">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3a4625fa3e0b1d64ac4ac62a9b192935.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ccd8b5dad37637cd9259909a8bd39e74.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0395ed01702e779f815ff82854c7e4dc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6d293a689a37d55dba0c23c9a472a2c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-832386f1bf0e4713aca02d1fb94fa9ee.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-574e7c8386eba3e442979c869a9719a4.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="SAILViT-Towards-Robust-and-Generalizable-Visual-Backbones-for-MLLMs-via-Gradual-Feature-Refinement"><a href="#SAILViT-Towards-Robust-and-Generalizable-Visual-Backbones-for-MLLMs-via-Gradual-Feature-Refinement" class="headerlink" title="SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via   Gradual Feature Refinement"></a>SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via   Gradual Feature Refinement</h2><p><strong>Authors:Weijie Yin, Dingkang Yang, Hongyuan Dong, Zijian Kang, Jiacong Wang, Xiao Liang, Chao Feng, Jiao Ran</strong></p>
<p>Vision Transformers (ViTs) are essential as foundation backbones in establishing the visual comprehension capabilities of Multimodal Large Language Models (MLLMs). Although most ViTs achieve impressive performance through image-text pair-based contrastive learning or self-supervised mechanisms, they struggle to engage in connector-based co-training directly with LLMs due to potential parameter initialization conflicts and modality semantic gaps. To address the above challenges, this paper proposes SAILViT, a gradual feature learning-enhanced ViT for facilitating MLLMs to break through performance bottlenecks in complex multimodal interactions. SAILViT achieves coarse-to-fine-grained feature alignment and world knowledge infusion with gradual feature refinement, which better serves target training demands. We perform thorough empirical analyses to confirm the powerful robustness and generalizability of SAILViT across different dimensions, including parameter sizes, model architectures, training strategies, and data scales. Equipped with SAILViT, existing MLLMs show significant and consistent performance improvements on the OpenCompass benchmark across extensive downstream tasks. SAILViT series models are released at <a target="_blank" rel="noopener" href="https://huggingface.co/BytedanceDouyinContent">https://huggingface.co/BytedanceDouyinContent</a>. </p>
<blockquote>
<p>è§†è§‰Transformerï¼ˆViTï¼‰ä½œä¸ºåŸºç¡€éª¨å¹²åœ¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è§†è§‰ç†è§£èƒ½åŠ›æ„å»ºä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚è™½ç„¶å¤§å¤šæ•°ViTé€šè¿‡åŸºäºå›¾åƒæ–‡æœ¬å¯¹æ¯”çš„å¯¹æ¯”å­¦ä¹ æˆ–è‡ªç›‘ç£æœºåˆ¶å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†ç”±äºæ½œåœ¨çš„å‚æ•°åˆå§‹åŒ–å†²çªå’Œæ¨¡æ€è¯­ä¹‰é¸¿æ²Ÿï¼Œå®ƒä»¬åœ¨ç›´æ¥ä¸LLMè¿›è¡ŒåŸºäºè¿æ¥å™¨çš„ååŒè®­ç»ƒæ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚é’ˆå¯¹ä¸Šè¿°æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†SAILViTï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºä¿ƒè¿›MLLMåœ¨å¤æ‚å¤šæ¨¡æ€äº¤äº’ä¸­çªç ´æ€§èƒ½ç“¶é¢ˆçš„é€æ­¥ç‰¹å¾å­¦ä¹ å¢å¼ºå‹ViTã€‚SAILViTé€šè¿‡é€æ­¥ç‰¹å¾ç»†åŒ–å®ç°ç²—åˆ°ç»†ç²’åº¦çš„ç‰¹å¾å¯¹é½å’Œä¸–ç•ŒçŸ¥è¯†æ³¨å…¥ï¼Œæ›´å¥½åœ°æœåŠ¡äºç›®æ ‡è®­ç»ƒéœ€æ±‚ã€‚æˆ‘ä»¬è¿›è¡Œäº†å½»åº•çš„å®è¯åˆ†æï¼Œä»¥éªŒè¯SAILViTåœ¨ä¸åŒç»´åº¦ï¼ŒåŒ…æ‹¬å‚æ•°å¤§å°ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒç­–ç•¥å’Œæ•°æ®è§„æ¨¡ä¸Šçš„å¼ºå¤§ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚é…å¤‡SAILViTçš„ç°æœ‰MLLMåœ¨OpenCompassåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°å‡ºæ˜¾è‘—ä¸”ä¸€è‡´çš„æ€§èƒ½æ”¹è¿›ã€‚SAILViTç³»åˆ—æ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/BytedanceDouyinContent%E4%B8%8A%E3%80%82">https://huggingface.co/BytedanceDouyinContentä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01643v1">PDF</a> We release SAILViT, a series of versatile vision foundation models</p>
<p><strong>Summary</strong></p>
<p>ViTsä½œä¸ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„è§†è§‰ç†è§£åŸºç¡€éª¨æ¶è‡³å…³é‡è¦ã€‚å°½ç®¡å¤§å¤šæ•°ViTsé€šè¿‡å›¾åƒæ–‡æœ¬å¯¹å¯¹æ¯”å­¦ä¹ æˆ–è‡ªç›‘ç£æœºåˆ¶å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ä¸LLMsçš„ç›´æ¥è¿æ¥å™¨å…±è®­ç»ƒé¢ä¸´ç€æ½œåœ¨çš„å‚æ•°åˆå§‹åŒ–å†²çªå’Œæ¨¡æ€è¯­ä¹‰é¸¿æ²Ÿçš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†SAILViTï¼Œè¿™æ˜¯ä¸€ç§å…·æœ‰æ¸è¿›ç‰¹å¾å­¦ä¹ èƒ½åŠ›çš„ViTï¼Œæœ‰åŠ©äºMLLMsåœ¨å¤æ‚çš„å¤šæ¨¡æ€äº¤äº’ä¸­çªç ´æ€§èƒ½ç“¶é¢ˆã€‚SAILViTé€šè¿‡æ¸è¿›ç‰¹å¾ç»†åŒ–å®ç°ç²—åˆ°ç»†ç²’åº¦çš„ç‰¹å¾å¯¹é½å’Œä¸–ç•ŒçŸ¥è¯†æ³¨å…¥ï¼Œæ›´å¥½åœ°æœåŠ¡äºç›®æ ‡è®­ç»ƒéœ€æ±‚ã€‚é€šè¿‡å½»åº•çš„å®è¯åˆ†æï¼Œæˆ‘ä»¬è¯å®äº†SAILViTåœ¨ä¸åŒç»´åº¦ä¸Šçš„å¼ºå¤§ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬å‚æ•°å¤§å°ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒç­–ç•¥å’Œæ•°æ®è§„æ¨¡ã€‚é…å¤‡SAILViTçš„ç°æœ‰MLLMsåœ¨OpenCompassåŸºå‡†æµ‹è¯•ä¸Šçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ˜¾è‘—ä¸”ä¸€è‡´çš„æ€§èƒ½æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) æ‰®æ¼”å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è§†è§‰ç†è§£åŸºç¡€éª¨æ¶çš„å…³é”®è§’è‰²ã€‚</li>
<li>ViTsè™½ç„¶èƒ½é€šè¿‡å›¾åƒæ–‡æœ¬å¯¹æ¯”å­¦ä¹ æˆ–è‡ªç›‘ç£æœºåˆ¶å–å¾—è‰¯å¥½æ€§èƒ½ï¼Œä½†åœ¨ä¸LLMsç›´æ¥è¿æ¥å™¨å…±è®­ç»ƒæ—¶é¢ä¸´å‚æ•°åˆå§‹åŒ–å†²çªå’Œæ¨¡æ€è¯­ä¹‰é¸¿æ²Ÿçš„æŒ‘æˆ˜ã€‚</li>
<li>SAILViTæ˜¯ä¸€ç§æ”¹è¿›çš„ViTæ¨¡å‹ï¼Œé€šè¿‡æ¸è¿›ç‰¹å¾å­¦ä¹ èƒ½åŠ›ï¼Œæœ‰åŠ©äºMLLMsåœ¨å¤æ‚å¤šæ¨¡æ€äº¤äº’ä¸­çªç ´æ€§èƒ½ç“¶é¢ˆã€‚</li>
<li>SAILViTå®ç°ç²—åˆ°ç»†ç²’åº¦çš„ç‰¹å¾å¯¹é½å’Œä¸–ç•ŒçŸ¥è¯†æ³¨å…¥ã€‚</li>
<li>å®è¯åˆ†æè¡¨æ˜ï¼ŒSAILViTåœ¨å‚æ•°å¤§å°ã€æ¨¡å‹æ¶æ„ã€è®­ç»ƒç­–ç•¥å’Œæ•°æ®è§„æ¨¡ç­‰å¤šä¸ªç»´åº¦ä¸Šå…·æœ‰å¼ºå¤§çš„ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>é…å¤‡SAILViTçš„MLLMsåœ¨OpenCompassåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½å¾—åˆ°æ˜¾è‘—æ”¹å–„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01643">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-161b716d91fce0bf90dbb2e992eb9b23.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df5e1b88db280b6e446f278284512d37.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b8da1a94ddc69da7dec268935e713ff9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ecb25e319c3431347d005a842d36e0f1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cff7d167d9f3a76daca933377f0e516b.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TrackingMiM-Efficient-Mamba-in-Mamba-Serialization-for-Real-time-UAV-Object-Tracking"><a href="#TrackingMiM-Efficient-Mamba-in-Mamba-Serialization-for-Real-time-UAV-Object-Tracking" class="headerlink" title="TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV   Object Tracking"></a>TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV   Object Tracking</h2><p><strong>Authors:Bingxi Liu, Calvin Chen, Junhao Li, Guyang Yu, Haoqian Song, Xuchen Liu, Jinqiang Cui, Hong Zhang</strong></p>
<p>The Vision Transformer (ViT) model has long struggled with the challenge of quadratic complexity, a limitation that becomes especially critical in unmanned aerial vehicle (UAV) tracking systems, where data must be processed in real time. In this study, we explore the recently proposed State-Space Model, Mamba, leveraging its computational efficiency and capability for long-sequence modeling to effectively process dense image sequences in tracking tasks. First, we highlight the issue of temporal inconsistency in existing Mamba-based methods, specifically the failure to account for temporal continuity in the Mamba scanning mechanism. Secondly, building upon this insight,we propose TrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model for handling image sequence of tracking problem. In our framework, the mamba scan is performed in a nested way while independently process temporal and spatial coherent patch tokens. While the template frame is encoded as query token and utilized for tracking in every scan. Extensive experiments conducted on five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves state-of-the-art precision while offering noticeable higher speed in UAV tracking. </p>
<blockquote>
<p>Vision Transformerï¼ˆViTï¼‰æ¨¡å‹é•¿æœŸä»¥æ¥ä¸€ç›´é¢ä¸´ç€äºŒæ¬¡å¤æ‚åº¦çš„æŒ‘æˆ˜ï¼Œè¿™ä¸€é™åˆ¶åœ¨æ— äººé£è¡Œå™¨ï¼ˆUAVï¼‰è·Ÿè¸ªç³»ç»Ÿä¸­å°¤ä¸ºå…³é”®ï¼Œå› ä¸ºæ•°æ®å¿…é¡»å®æ—¶å¤„ç†ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æœ€è¿‘æå‡ºçš„State-Space Modelï¼Œå³Mambaæ¨¡å‹ï¼Œåˆ©ç”¨å…¶è®¡ç®—æ•ˆç‡å’Œé•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›ï¼Œæœ‰æ•ˆå¤„ç†è·Ÿè¸ªä»»åŠ¡ä¸­çš„å¯†é›†å›¾åƒåºåˆ—ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼ºè°ƒäº†ç°æœ‰Mambaæ–¹æ³•ä¸­å­˜åœ¨çš„æ—¶é—´ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯Mambaæ‰«ææœºåˆ¶æœªèƒ½è€ƒè™‘åˆ°æ—¶é—´çš„è¿ç»­æ€§ã€‚å…¶æ¬¡ï¼ŒåŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†TrackingMiMï¼Œè¿™æ˜¯ä¸€ä¸ªMamba-in-Mambaæ¶æ„ï¼Œæ˜¯ä¸€ä¸ªç”¨äºå¤„ç†è·Ÿè¸ªé—®é¢˜å›¾åƒåºåˆ—çš„æœ€å°è®¡ç®—è´Ÿæ‹…æ¨¡å‹ã€‚åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­ï¼Œmambaæ‰«æä»¥åµŒå¥—çš„æ–¹å¼è¿›è¡Œï¼ŒåŒæ—¶ç‹¬ç«‹å¤„ç†æ—¶é—´å’Œç©ºé—´è¿è´¯çš„è¡¥ä¸ä»¤ç‰Œã€‚åŒæ—¶ï¼Œæ¨¡æ¿å¸§è¢«ç¼–ç ä¸ºæŸ¥è¯¢ä»¤ç‰Œï¼Œå¹¶åœ¨æ¯æ¬¡æ‰«æä¸­ç”¨äºè·Ÿè¸ªã€‚åœ¨äº”ä¸ªUAVè·Ÿè¸ªåŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯å®ï¼Œæ‰€æå‡ºçš„TrackingMiMè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç²¾åº¦ï¼ŒåŒæ—¶åœ¨UAVè·Ÿè¸ªä¸­æä¾›äº†æ˜æ˜¾çš„æ›´é«˜é€Ÿåº¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01535v1">PDF</a> 12 pages</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç« ä»‹ç»äº†é’ˆå¯¹Vision Transformeræ¨¡å‹åœ¨å¤„ç†æ— äººæœºè¿½è¸ªç³»ç»Ÿä¸­çš„äºŒæ¬¡å¤æ‚åº¦é—®é¢˜è¿›è¡Œç ”ç©¶çš„ç»“æœã€‚æ–‡ç« æ¢è®¨äº†æœ€è¿‘æå‡ºçš„State-Spaceæ¨¡å‹Mambaåœ¨è§£å†³è¯¥é—®é¢˜æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶æŒ‡å‡ºäº†ç°æœ‰Mambaæ–¹æ³•å­˜åœ¨çš„æ—¶åºä¸ä¸€è‡´æ€§é—®é¢˜ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§åŸºäºMambaçš„TrackingMiMæ¶æ„ï¼Œè¯¥æ¶æ„ä»¥æœ€å°çš„è®¡ç®—è´Ÿæ‹…å¤„ç†è·Ÿè¸ªé—®é¢˜çš„å›¾åƒåºåˆ—ã€‚å®éªŒè¯æ˜ï¼ŒTrackingMiMåœ¨äº”ä¸ªæ— äººæœºè¿½è¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç²¾åº¦ï¼ŒåŒæ—¶åœ¨æ— äººæœºè¿½è¸ªä¸­æä¾›äº†æ›´é«˜çš„é€Ÿåº¦ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformeræ¨¡å‹åœ¨æ— äººæœºè¿½è¸ªç³»ç»Ÿä¸­é¢ä¸´äºŒæ¬¡å¤æ‚åº¦æŒ‘æˆ˜ã€‚</li>
<li>State-Spaceæ¨¡å‹Mambaåœ¨å¤„ç†å®æ—¶æ•°æ®å¤„ç†æ–¹é¢å…·æœ‰è®¡ç®—æ•ˆç‡å’Œé•¿æœŸåºåˆ—å»ºæ¨¡èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰Mambaæ–¹æ³•å­˜åœ¨æ—¶åºä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œæœªèƒ½å……åˆ†è€ƒè™‘æ—¶åºè¿ç»­æ€§ã€‚</li>
<li>TrackingMiMæ¶æ„è¢«æå‡ºï¼Œå®ƒæ˜¯ä¸€ç§åŸºäºMambaçš„åµŒå¥—æ‰«ææ–¹å¼å¤„ç†è·Ÿè¸ªé—®é¢˜çš„å›¾åƒåºåˆ—çš„æ¨¡å‹ã€‚</li>
<li>TrackingMiMèƒ½åŒæ—¶å¤„ç†æ—¶åºå’Œç©ºé—´è¿è´¯çš„è¡¥ä¸ä»¤ç‰Œï¼Œå¹¶å°†æ¨¡æ¿å¸§ç¼–ç ä¸ºæŸ¥è¯¢ä»¤ç‰Œï¼Œç”¨äºæ¯æ¬¡æ‰«æä¸­çš„è·Ÿè¸ªã€‚</li>
<li>TrackingMiMåœ¨äº”ä¸ªæ— äººæœºè¿½è¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01535">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-82de78257fa2fb913a03c5199a1290ad.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-dd30b6d037260d00604c6b308e77b738.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UPRE-Zero-Shot-Domain-Adaptation-for-Object-Detection-via-Unified-Prompt-and-Representation-Enhancement"><a href="#UPRE-Zero-Shot-Domain-Adaptation-for-Object-Detection-via-Unified-Prompt-and-Representation-Enhancement" class="headerlink" title="UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified   Prompt and Representation Enhancement"></a>UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified   Prompt and Representation Enhancement</h2><p><strong>Authors:Xiao Zhang, Fei Wei, Yong Wang, Wenda Zhao, Feiyi Li, Xiangxiang Chu</strong></p>
<p>Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the lack of images in the target domain. Previous approaches leverage Vision-Language Models (VLMs) to tackle this challenge, exploiting their zero-shot learning capabilities. However, these methods primarily address domain distribution shifts and overlook the misalignment between the detection task and VLMs, which rely on manually crafted prompts. To overcome these limitations, we propose the unified prompt and representation enhancement (UPRE) framework, which jointly optimizes both textual prompts and visual representations. Specifically, our approach introduces a multi-view domain prompt that combines linguistic domain priors with detection-specific knowledge, and a visual representation enhancement module that produces domain style variations. Furthermore, we introduce multi-level enhancement strategies, including relative domain distance and positive-negative separation, which align multi-modal representations at the image level and capture diverse visual representations at the instance level, respectively. Extensive experiments conducted on nine benchmark datasets demonstrate the superior performance of our framework in ZSDA detection scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/UPRE">https://github.com/AMAP-ML/UPRE</a>. </p>
<blockquote>
<p>é›¶æ ·æœ¬åŸŸè‡ªé€‚åº”ï¼ˆZSDAï¼‰ç”±äºç›®æ ‡åŸŸç¼ºä¹å›¾åƒè€Œé¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚ä¹‹å‰çš„æ–¹æ³•åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå‘æŒ¥å®ƒä»¬çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä¸»è¦è§£å†³åŸŸåˆ†å¸ƒè½¬ç§»é—®é¢˜ï¼Œè€Œå¿½ç•¥äº†æ£€æµ‹ä»»åŠ¡ä¸VLMä¹‹é—´çš„ä¸åŒ¹é…ï¼ŒVLMä¾èµ–äºæ‰‹å·¥åˆ¶ä½œçš„æç¤ºã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€æç¤ºå’Œè¡¨ç¤ºå¢å¼ºï¼ˆUPREï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶è”åˆä¼˜åŒ–äº†æ–‡æœ¬æç¤ºå’Œè§†è§‰è¡¨ç¤ºã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸€ç§å¤šè§†å›¾åŸŸæç¤ºï¼Œå°†è¯­è¨€åŸŸå…ˆéªŒçŸ¥è¯†ä¸æ£€æµ‹ç‰¹å®šçŸ¥è¯†ç›¸ç»“åˆï¼Œä»¥åŠä¸€ä¸ªè§†è§‰è¡¨ç¤ºå¢å¼ºæ¨¡å—ï¼Œç”¨äºç”ŸæˆåŸŸé£æ ¼å˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šå±‚æ¬¡å¢å¼ºç­–ç•¥ï¼ŒåŒ…æ‹¬ç›¸å¯¹åŸŸè·ç¦»å’Œæ­£è´Ÿåˆ†ç¦»ï¼Œåˆ†åˆ«åœ¨å›¾åƒçº§åˆ«å¯¹é½å¤šæ¨¡å¼è¡¨ç¤ºï¼Œåœ¨å®ä¾‹çº§åˆ«æ•è·å¤šæ ·åŒ–çš„è§†è§‰è¡¨ç¤ºã€‚åœ¨ä¹ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨ZSDAæ£€æµ‹åœºæ™¯ä¸­å…·æœ‰å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/AMAP-ML/UPRE%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/AMAP-ML/UPREæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00721v1">PDF</a> ICCV2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºUPREçš„ç»Ÿä¸€æç¤ºå’Œè¡¨ç¤ºå¢å¼ºæ¡†æ¶ï¼Œç”¨äºè§£å†³é›¶é•œå¤´åŸŸé€‚åº”ï¼ˆZSDAï¼‰æ£€æµ‹ä¸­çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶è”åˆä¼˜åŒ–æ–‡æœ¬æç¤ºå’Œè§†è§‰è¡¨ç¤ºï¼Œé€šè¿‡å¼•å…¥å¤šè§†è§’åŸŸæç¤ºå’Œè§†è§‰è¡¨ç¤ºå¢å¼ºæ¨¡å—ï¼Œå¹¶ç»“åˆç›¸å¯¹åŸŸè·ç¦»å’Œå¤šçº§å¢å¼ºç­–ç•¥ï¼Œæé«˜äº†ZSDAæ£€æµ‹åœºæ™¯çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>UPREæ¡†æ¶è§£å†³äº†é›¶é•œå¤´åŸŸé€‚åº”ï¼ˆZSDAï¼‰æ£€æµ‹ä¸­çš„æŒ‘æˆ˜ï¼Œé€šè¿‡è”åˆä¼˜åŒ–æ–‡æœ¬æç¤ºå’Œè§†è§‰è¡¨ç¤ºã€‚</li>
<li>å¼•å…¥å¤šè§†è§’åŸŸæç¤ºï¼Œç»“åˆè¯­è¨€åŸŸå…ˆéªŒå’Œæ£€æµ‹ç‰¹å®šçŸ¥è¯†ã€‚</li>
<li>è§†è§‰è¡¨ç¤ºå¢å¼ºæ¨¡å—ç”¨äºç”Ÿæˆå…·æœ‰åŸŸé£æ ¼å˜åŒ–çš„è¡¨ç¤ºã€‚</li>
<li>é‡‡ç”¨ç›¸å¯¹åŸŸè·ç¦»å’Œæ­£è´Ÿåˆ†ç¦»çš„å¤šçº§å¢å¼ºç­–ç•¥ï¼Œåˆ†åˆ«å¯¹é½å›¾åƒçº§åˆ«çš„å¤šæ¨¡å¼è¡¨ç¤ºå’Œæ•è·å®ä¾‹çº§åˆ«çš„å¤šæ ·è§†è§‰è¡¨ç¤ºã€‚</li>
<li>åœ¨ä¹ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¯æ˜äº†UPREæ¡†æ¶åœ¨ZSDAæ£€æµ‹åœºæ™¯ä¸­çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>ä»£ç å·²å…¬å¼€ï¼Œä¾¿äºç ”ç©¶è€…å’Œå¼€å‘è€…ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00721">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-273b5981ed51e8d18776b2fde2a8c49b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-3bbdd914ef07f98a2ab2538d3c00b878.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7870f451037050b4697fea48b540040f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b9c032ab5a02805d35639010b0cad6e5.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="LLaVA-SP-Enhancing-Visual-Representation-with-Visual-Spatial-Tokens-for-MLLMs"><a href="#LLaVA-SP-Enhancing-Visual-Representation-with-Visual-Spatial-Tokens-for-MLLMs" class="headerlink" title="LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for   MLLMs"></a>LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for   MLLMs</h2><p><strong>Authors:Haoran Lou, Chunxiao Fan, Ziyan Liu, Yuexin Wu, Xinxiang Wang</strong></p>
<p>The architecture of multimodal large language models (MLLMs) commonly connects a vision encoder, often based on CLIP-ViT, to a large language model. While CLIP-ViT works well for capturing global image features, it struggles to model local relationships between adjacent patches, leading to weaker visual representation, which in turn affects the detailed understanding ability of MLLMs. To solve this, we propose LLaVA-SP, which \textbf{ only adds six spatial visual tokens} to the original visual tokens to enhance the visual representation. Our approach offers three key advantages: 1)We propose a novel Projector, which uses convolutional kernels to derive visual spatial tokens from ViT patch features, simulating two visual spatial ordering approaches: <code>from central region to global&quot; and </code>from abstract to specificâ€. Then, a cross-attention mechanism is applied to fuse fine-grained visual information, enriching the overall visual representation. 2) We present two model variants: LLaVA-SP-Cropping, which focuses on detail features through progressive cropping, and LLaVA-SP-Pooling, which captures global semantics through adaptive pooling, enabling the model to handle diverse visual understanding tasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA, achieves significant performance improvements across various multimodal benchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple tasks with nearly identical inference latency. The code and models are available at \href{<a target="_blank" rel="noopener" href="https://github.com/CnFaker/LLaVA-SP%7D%7B/texttt%7Bhttps://github.com/CnFaker/LLaVA-SP%7D%7D">https://github.com/CnFaker/LLaVA-SP}{\texttt{https://github.com/CnFaker/LLaVA-SP}}</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ¶æ„é€šå¸¸å°†åŸºäºCLIP-ViTçš„è§†è§‰ç¼–ç å™¨è¿æ¥åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ã€‚è™½ç„¶CLIP-ViTåœ¨æ•æ‰å…¨å±€å›¾åƒç‰¹å¾æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å¯¹ç›¸é‚»è¡¥ä¸ä¹‹é—´çš„å±€éƒ¨å…³ç³»è¿›è¡Œå»ºæ¨¡æ—¶é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´è§†è§‰è¡¨ç¤ºè¾ƒå¼±ï¼Œè¿›è€Œå½±å“äº†MLLMçš„è¯¦ç»†ç†è§£èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†LLaVA-SPï¼Œå®ƒé€šè¿‡åœ¨åŸå§‹è§†è§‰æ ‡è®°ä¸­æ·»åŠ å…­ä¸ªç©ºé—´è§†è§‰æ ‡è®°æ¥å¢å¼ºè§†è§‰è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•æä¾›äº†ä¸‰ä¸ªå…³é”®ä¼˜åŠ¿ï¼š</p>
</blockquote>
<p>1ï¼‰æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æŠ•å½±ä»ªï¼Œè¯¥æŠ•å½±ä»ªä½¿ç”¨å·ç§¯æ ¸ä»ViTè¡¥ä¸ç‰¹å¾ä¸­æ¨å¯¼å‡ºè§†è§‰ç©ºé—´æ ‡è®°ï¼Œæ¨¡æ‹Ÿä¸¤ç§è§†è§‰ç©ºé—´æ’åºæ–¹æ³•ï¼šâ€œä»ä¸­å¿ƒåŒºåŸŸåˆ°å…¨å±€â€å’Œâ€œä»æŠ½è±¡åˆ°å…·ä½“â€ã€‚ç„¶ååº”ç”¨äº¤å‰æ³¨æ„åŠ›æœºåˆ¶èåˆç²¾ç»†çš„è§†è§‰ä¿¡æ¯ï¼Œä¸°å¯Œæ•´ä½“çš„è§†è§‰è¡¨ç¤ºã€‚</p>
<p>2ï¼‰æˆ‘ä»¬æ¨å‡ºäº†ä¸¤æ¬¾æ¨¡å‹å˜ä½“ï¼šLLaVA-SP-Croppingï¼Œé€šè¿‡æ¸è¿›è£å‰ªå…³æ³¨ç»†èŠ‚ç‰¹å¾ï¼›ä»¥åŠLLaVA-SP-Poolingï¼Œé€šè¿‡è‡ªé€‚åº”æ± åŒ–æ•æ‰å…¨å±€è¯­ä¹‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå¤„ç†å„ç§è§†è§‰ç†è§£ä»»åŠ¡ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00505v1">PDF</a> ICCV</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºLLaVA-SPçš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹æ¶æ„æ”¹è¿›æ–¹æ¡ˆã€‚é’ˆå¯¹CLIP-ViTåœ¨æ•æ‰å±€éƒ¨å›¾åƒå…³ç³»æ–¹é¢çš„ä¸è¶³ï¼ŒLLaVA-SPä»…æ·»åŠ å…­ä¸ªç©ºé—´è§†è§‰ä»¤ç‰Œæ¥å¢å¼ºè§†è§‰è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é€šè¿‡å·ç§¯æ ¸ç”Ÿæˆç©ºé—´è§†è§‰ä»¤ç‰Œï¼Œæ¨¡æ‹Ÿä¸¤ç§è§†è§‰ç©ºé—´æ’åºæ–¹æ³•ï¼Œå¹¶é€šè¿‡è·¨æ³¨æ„åŠ›æœºåˆ¶èåˆç²¾ç»†è§†è§‰ä¿¡æ¯ï¼Œä¸°å¯Œäº†æ•´ä½“è§†è§‰è¡¨ç¤ºã€‚è¯¥æ–¹æ³•è¿˜æä¾›äº†ä¸¤ç§æ¨¡å‹å˜ä½“ï¼Œå¹¶åœ¨å¤šç§å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚LLaVA-SPä¸LoRAå¾®è°ƒåï¼Œåœ¨å¤šä»»åŠ¡ä¸­ä¼˜äºå½“å‰å…ˆè¿›çš„LLaVA-1.5æ¨¡å‹ï¼Œä¸”æ¨ç†å»¶è¿Ÿå‡ ä¹ç›¸åŒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLaVA-SPé’ˆå¯¹CLIP-ViTåœ¨æ•æ‰å±€éƒ¨å›¾åƒå…³ç³»æ–¹é¢çš„ä¸è¶³æå‡ºäº†è§£å†³æ–¹æ¡ˆã€‚</li>
<li>LLaVA-SPé€šè¿‡æ·»åŠ å…­ä¸ªç©ºé—´è§†è§‰ä»¤ç‰Œå¢å¼ºè§†è§‰è¡¨ç¤ºã€‚</li>
<li>ä½¿ç”¨å·ç§¯æ ¸ç”Ÿæˆç©ºé—´è§†è§‰ä»¤ç‰Œï¼Œæ¨¡æ‹Ÿä¸¤ç§è§†è§‰ç©ºé—´æ’åºæ–¹æ³•ã€‚</li>
<li>è·¨æ³¨æ„åŠ›æœºåˆ¶ç”¨äºèåˆç²¾ç»†è§†è§‰ä¿¡æ¯ï¼Œä¸°å¯Œæ•´ä½“è§†è§‰è¡¨ç¤ºã€‚</li>
<li>LLaVA-SPæä¾›ä¸¤ç§æ¨¡å‹å˜ä½“ï¼Œä»¥é€‚åº”ä¸åŒçš„è§†è§‰ç†è§£ä»»åŠ¡ã€‚</li>
<li>å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒLLaVA-SPåœ¨å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚</li>
<li>LLaVA-SPä¸LoRAå¾®è°ƒåï¼Œåœ¨å¤šä»»åŠ¡ä¸­ä¼˜äºLLaVA-1.5æ¨¡å‹ï¼Œä¸”æ¨ç†å»¶è¿Ÿä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00505">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2f2653282561c83ae62076b7a005b7b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dfa2a46ddb2dbd8cabad494288767ebf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-c5d407bdd1f8f10609fd8915146f2733.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-f55cbef21c35cb975f495f56ca2d935f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8d52eb47551eef98db42a77d24551c0b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6cfdb8c6fef107017a5f302bb4515b1a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-aa62791c3b052a3b73e1e5b761d537a2.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-912cc9e4ec1be86150becbb0139f1604.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Time-Series-Representations-for-Classification-Lie-Hidden-in-Pretrained-Vision-Transformers"><a href="#Time-Series-Representations-for-Classification-Lie-Hidden-in-Pretrained-Vision-Transformers" class="headerlink" title="Time Series Representations for Classification Lie Hidden in Pretrained   Vision Transformers"></a>Time Series Representations for Classification Lie Hidden in Pretrained   Vision Transformers</h2><p><strong>Authors:Simon Roschmann, Quentin Bouniot, Vasilii Feofanov, Ievgen Redko, Zeynep Akata</strong></p>
<p>Time series classification is a fundamental task in healthcare and industry, yet the development of time series foundation models (TSFMs) remains limited by the scarcity of publicly available time series datasets. In this work, we propose Time Vision Transformer (TiViT), a framework that converts time series into images to leverage the representational power of frozen Vision Transformers (ViTs) pretrained on large-scale image datasets. First, we theoretically motivate our approach by analyzing the 2D patching of ViTs for time series, showing that it can increase the number of label-relevant tokens and reduce the sample complexity. Second, we empirically demonstrate that TiViT achieves state-of-the-art performance on standard time series classification benchmarks by utilizing the hidden representations of large OpenCLIP models. We explore the structure of TiViT representations and find that intermediate layers with high intrinsic dimension are the most effective for time series classification. Finally, we assess the alignment between TiViT and TSFM representation spaces and identify a strong complementarity, with further performance gains achieved by combining their features. Our findings reveal a new direction for reusing vision representations in a non-visual domain. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/TiViT">https://github.com/ExplainableML/TiViT</a>. </p>
<blockquote>
<p>æ—¶é—´åºåˆ—åˆ†ç±»æ˜¯åŒ»ç–—å’Œå·¥ä¸šé¢†åŸŸçš„ä¸€é¡¹åŸºç¡€ä»»åŠ¡ï¼Œç„¶è€Œï¼Œç”±äºå…¬å¼€å¯ç”¨æ—¶é—´åºåˆ—æ•°æ®é›†çš„ç¨€ç¼ºï¼Œæ—¶é—´åºåˆ—åŸºç¡€æ¨¡å‹ï¼ˆTSFMsï¼‰çš„å‘å±•å—åˆ°é™åˆ¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶é—´è§†è§‰è½¬æ¢å™¨ï¼ˆTiViTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†æ—¶é—´åºåˆ—è½¬æ¢ä¸ºå›¾åƒä»¥åˆ©ç”¨åœ¨å¤§å‹å›¾åƒæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„å†»ç»“è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„è¡¨ç¤ºèƒ½åŠ›çš„æ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡åˆ†æç”¨äºæ—¶é—´åºåˆ—çš„ViTçš„2Dè¡¥ä¸ä»ç†è®ºä¸Šè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¡¨æ˜å®ƒå¯ä»¥å¢åŠ æ ‡ç­¾ç›¸å…³ä»¤ç‰Œçš„æ•°é‡å¹¶é™ä½æ ·æœ¬å¤æ‚æ€§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬é€šè¿‡å®è¯è¯æ˜ï¼ŒTiViTåˆ©ç”¨å¤§å‹OpenCLIPæ¨¡å‹çš„éšè—è¡¨ç¤ºï¼Œåœ¨æ ‡å‡†æ—¶é—´åºåˆ—åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬æ¢ç´¢äº†TiViTè¡¨ç¤ºçš„ç»“æ„ï¼Œå‘ç°å…·æœ‰é«˜å†…åœ¨ç»´åº¦çš„ä¸­é—´å±‚å¯¹æ—¶é—´åºåˆ—åˆ†ç±»æœ€ä¸ºæœ‰æ•ˆã€‚æœ€åï¼Œæˆ‘ä»¬è¯„ä¼°äº†TiViTå’ŒTSFMè¡¨ç¤ºç©ºé—´ä¹‹é—´çš„å¯¹é½æƒ…å†µï¼Œå¹¶å‘ç°äº†å¼ºçƒˆçš„äº’è¡¥æ€§ï¼Œé€šè¿‡ç»“åˆå®ƒä»¬çš„ç‰¹å¾å®ç°äº†è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºåœ¨éè§†è§‰é¢†åŸŸé‡æ–°ä½¿ç”¨è§†è§‰è¡¨ç¤ºæä¾›äº†æ–°çš„æ–¹å‘ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ExplainableML/TiViT%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ExplainableML/TiViTæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08641v2">PDF</a> Preprint</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†Time Vision Transformerï¼ˆTiViTï¼‰æ¡†æ¶ï¼Œå°†æ—¶é—´åºåˆ—è½¬åŒ–ä¸ºå›¾åƒï¼Œåˆ©ç”¨åœ¨å¤§å‹å›¾åƒæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„å†»ç»“Vision Transformerï¼ˆViTï¼‰çš„è¡¨ç¤ºèƒ½åŠ›ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼ŒViTçš„2Dè´´ç‰‡å¤„ç†å¯ä»¥æé«˜æ ‡ç­¾ç›¸å…³ä»¤ç‰Œçš„æ•°ç›®å¹¶é™ä½æ ·æœ¬å¤æ‚æ€§ã€‚åœ¨æ ‡å‡†æ—¶é—´åºåˆ—åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸Šï¼ŒTiViTåˆ©ç”¨å¤§å‹OpenCLIPæ¨¡å‹çš„éšè—è¡¨ç¤ºå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç ”ç©¶å‘ç°ï¼Œå…·æœ‰é«˜ä¸­ç»´æ•°çš„ä¸­é—´å±‚å¯¹äºæ—¶é—´åºåˆ—åˆ†ç±»æœ€ä¸ºæœ‰æ•ˆã€‚æœ€åï¼Œè¯„ä¼°äº†TiViTä¸TSFMsçš„ä»£è¡¨æ€§ç©ºé—´å¯¹é½æƒ…å†µï¼Œå‘ç°å®ƒä»¬å…·æœ‰å¾ˆå¼ºçš„äº’è¡¥æ€§ï¼Œé€šè¿‡ç»“åˆå…¶ç‰¹å¾å¯ä»¥å®ç°è¿›ä¸€æ­¥çš„æ€§èƒ½æå‡ã€‚è¿™ä¸ºåœ¨éè§†è§‰é¢†åŸŸé‡æ–°åˆ©ç”¨è§†è§‰è¡¨ç¤ºæä¾›äº†æ–°çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Time Vision Transformer (TiViT) æ¡†æ¶èƒ½å°†æ—¶é—´åºåˆ—è½¬åŒ–ä¸ºå›¾åƒï¼Œåˆ©ç”¨Vision Transformerï¼ˆViTï¼‰åœ¨å¤§å‹å›¾åƒæ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒè¡¨ç¤ºèƒ½åŠ›ã€‚</li>
<li>ç†è®ºåˆ†ææ˜¾ç¤ºViTçš„2Dè´´ç‰‡å¤„ç†èƒ½æé«˜æ ‡ç­¾ç›¸å…³ä»¤ç‰Œçš„æ•°ç›®å¹¶é™ä½æ ·æœ¬å¤æ‚æ€§ã€‚</li>
<li>åœ¨æ ‡å‡†æ—¶é—´åºåˆ—åˆ†ç±»åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTiViTå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</li>
<li>ç ”ç©¶å‘ç°ï¼ŒTiViTçš„ä¸­é—´å±‚å¯¹äºæ—¶é—´åºåˆ—åˆ†ç±»æœ€ä¸ºæœ‰æ•ˆï¼Œè¿™äº›å±‚å…·æœ‰é«˜ä¸­ç»´æ•°ã€‚</li>
<li>TiViTä¸TSFMsçš„ä»£è¡¨æ€§ç©ºé—´å…·æœ‰å¼ºäº’è¡¥æ€§ï¼Œç»“åˆå…¶ç‰¹å¾å¯è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚</li>
<li>æ­¤ç ”ç©¶ä¸ºåœ¨éè§†è§‰é¢†åŸŸé‡æ–°åˆ©ç”¨è§†è§‰è¡¨ç¤ºæä¾›äº†æ–°çš„æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08641">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4cd6d8a6e40d67360b1b2d8db4ad89b1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-339e8028477fcbc7a85fc2a04775fdfb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9d5db1d05b0f73808e26833de76f865f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5f0fc2851b59daa19fb025977333b987.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d611405351f06e3c06b686cddb70717d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c8e820857fde1ba60e882ba2b8843b15.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Semantic-Alignment-and-Reinforcement-for-Data-Free-Quantization-of-Vision-Transformers"><a href="#Semantic-Alignment-and-Reinforcement-for-Data-Free-Quantization-of-Vision-Transformers" class="headerlink" title="Semantic Alignment and Reinforcement for Data-Free Quantization of   Vision Transformers"></a>Semantic Alignment and Reinforcement for Data-Free Quantization of   Vision Transformers</h2><p><strong>Authors:Yunshan Zhong, Yuyao Zhou, Yuxin Zhang, Wanchen Sui, Shen Li, Yong Li, Fei Chao, Rongrong Ji</strong></p>
<p>Data-free quantization (DFQ) enables model quantization without accessing real data, addressing concerns regarding data security and privacy. With the growing adoption of Vision Transformers (ViTs), DFQ for ViTs has garnered significant attention. However, existing DFQ methods exhibit two limitations: (1) semantic distortion, where the semantics of synthetic images deviate substantially from those of real images, and (2) semantic inadequacy, where synthetic images contain extensive regions with limited content and oversimplified textures, leading to suboptimal quantization performance. To address these limitations, we propose SARDFQ, a novel Semantics Alignment and Reinforcement Data-Free Quantization method for ViTs. To address semantic distortion, SARDFQ incorporates Attention Priors Alignment (APA), which optimizes synthetic images to follow randomly generated structure attention priors. To mitigate semantic inadequacy, SARDFQ introduces Multi-Semantic Reinforcement (MSR), leveraging localized patch optimization to enhance semantic richness across synthetic images. Furthermore, SARDFQ employs Soft-Label Learning (SL), wherein multiple semantic targets are adapted to facilitate the learning of multi-semantic images augmented by MSR. Extensive experiments demonstrate the effectiveness of SARDFQ, significantly surpassing existing methods. For example, SARDFQ improves top-1 accuracy on ImageNet by 15.52% for W4A4 ViT-B. The code is at <a target="_blank" rel="noopener" href="https://github.com/zysxmu/SARDFQ">https://github.com/zysxmu/SARDFQ</a>. </p>
<blockquote>
<p>æ— æ•°æ®é‡åŒ–ï¼ˆDFQï¼‰èƒ½å¤Ÿåœ¨æ— éœ€è®¿é—®çœŸå®æ•°æ®çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹é‡åŒ–ï¼Œè§£å†³äº†æ•°æ®å®‰å…¨å’Œéšç§çš„æ‹…å¿§ã€‚éšç€è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰çš„å¹¿æ³›åº”ç”¨ï¼ŒViTsçš„æ— æ•°æ®é‡åŒ–ï¼ˆDFQï¼‰å·²å¼•èµ·å¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„DFQæ–¹æ³•å­˜åœ¨ä¸¤ä¸ªå±€é™æ€§ï¼šï¼ˆ1ï¼‰è¯­ä¹‰å¤±çœŸï¼Œå³åˆæˆå›¾åƒçš„è¯­ä¹‰ä¸çœŸå®å›¾åƒçš„è¯­ä¹‰å­˜åœ¨è¾ƒå¤§åå·®ï¼›ï¼ˆ2ï¼‰è¯­ä¹‰ä¸è¶³ï¼Œåˆæˆå›¾åƒåŒ…å«å¤§é‡å†…å®¹æœ‰é™ã€çº¹ç†è¿‡äºç®€å•çš„åŒºåŸŸï¼Œå¯¼è‡´é‡åŒ–æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†SARDFQï¼Œä¸€ç§ç”¨äºè§†è§‰è½¬æ¢å™¨çš„æ–°å‹è¯­ä¹‰å¯¹é½å’Œå¼ºåŒ–æ— æ•°æ®é‡åŒ–æ–¹æ³•ã€‚ä¸ºäº†è§£å†³è¯­ä¹‰å¤±çœŸé—®é¢˜ï¼ŒSARDFQç»“åˆäº†æ³¨æ„åŠ›å…ˆéªŒå¯¹é½ï¼ˆAPAï¼‰ï¼Œä¼˜åŒ–åˆæˆå›¾åƒä»¥éµå¾ªéšæœºç”Ÿæˆçš„ç»“æ„æ³¨æ„åŠ›å…ˆéªŒã€‚ä¸ºäº†ç¼“è§£è¯­ä¹‰ä¸è¶³çš„é—®é¢˜ï¼ŒSARDFQå¼•å…¥äº†å¤šè¯­ä¹‰å¼ºåŒ–ï¼ˆMSRï¼‰ï¼Œåˆ©ç”¨å±€éƒ¨è¡¥ä¸ä¼˜åŒ–æ¥å¢å¼ºåˆæˆå›¾åƒä¸­çš„è¯­ä¹‰ä¸°å¯Œæ€§ã€‚æ­¤å¤–ï¼ŒSARDFQé‡‡ç”¨è½¯æ ‡ç­¾å­¦ä¹ ï¼ˆSLï¼‰ï¼Œé€‚åº”å¤šä¸ªè¯­ä¹‰ç›®æ ‡ï¼Œä»¥ä¿ƒè¿›ç”±MSRå¢å¼ºçš„å¤šè¯­ä¹‰å›¾åƒçš„å­¦ä¹ ã€‚å¤§é‡å®éªŒè¯æ˜äº†SARDFQçš„æœ‰æ•ˆæ€§ï¼Œæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚ä¾‹å¦‚ï¼ŒSARDFQåœ¨ImageNetä¸Šçš„top-1å‡†ç¡®ç‡æé«˜äº†15.52%ï¼Œé€‚ç”¨äºW4A4 ViT-Bã€‚ä»£ç åœ°å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://github.com/zysxmu/SARDFQ%E3%80%82">https://github.com/zysxmu/SARDFQã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.16553v3">PDF</a> ICCV2025</p>
<p><strong>Summary</strong><br>æ•°æ®æ— å…³é‡åŒ–ï¼ˆDFQï¼‰å¯åœ¨ä¸æ¥è§¦çœŸå®æ•°æ®çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹é‡åŒ–ï¼Œå…³æ³¨æ•°æ®å®‰å…¨å’Œéšç§ä¿æŠ¤ã€‚é’ˆå¯¹è§†è§‰å˜å‹å™¨ï¼ˆViTsï¼‰çš„DFQå­˜åœ¨è¯­ä¹‰å¤±çœŸå’Œè¯­ä¹‰ä¸è¶³çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºä¸€ç§æ–°å‹çš„è¯­ä¹‰å¯¹é½å’Œå¼ºåŒ–æ•°æ®æ— å…³é‡åŒ–æ–¹æ³•SARDFQã€‚é€šè¿‡ä¼˜åŒ–åˆæˆå›¾åƒéµå¾ªéšæœºç”Ÿæˆçš„ç»“æ„æ³¨æ„åŠ›å…ˆéªŒæ¥è§£å†³è¯­ä¹‰å¤±çœŸé—®é¢˜ã€‚æ­¤å¤–ï¼ŒSARDFQå¼•å…¥äº†å¤šè¯­ä¹‰å¼ºåŒ–ï¼ˆMSRï¼‰æ¥å‡è½»è¯­ä¹‰ä¸è¶³çš„é—®é¢˜ï¼Œé€šè¿‡å±€éƒ¨è¡¥ä¸ä¼˜åŒ–å¢å¼ºåˆæˆå›¾åƒçš„è¯­ä¹‰ä¸°å¯Œæ€§ã€‚é‡‡ç”¨è½¯æ ‡ç­¾å­¦ä¹ ï¼ˆSLï¼‰ï¼Œé€‚åº”å¤šä¸ªè¯­ä¹‰ç›®æ ‡ï¼Œä¿ƒè¿›ç”±MSRå¢å¼ºçš„å¤šè¯­ä¹‰å›¾åƒçš„å­¦ä¹ ã€‚å®éªŒè¯æ˜SARDFQæ•ˆæœæ˜¾è‘—ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚W4A4 ViT-Båœ¨ImageNetä¸Šçš„top-1å‡†ç¡®ç‡æé«˜15.52%ã€‚è¯¦æƒ…å¯è®¿é—®ç›¸å…³ä»£ç åº“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°æ®æ— å…³é‡åŒ–ï¼ˆDFQï¼‰å…è®¸åœ¨ä¸æ¥è§¦çœŸå®æ•°æ®çš„æƒ…å†µä¸‹è¿›è¡Œæ¨¡å‹é‡åŒ–ï¼Œæ³¨é‡æ•°æ®å®‰å…¨å’Œéšç§ã€‚</li>
<li>é’ˆå¯¹ViTsçš„ç°æœ‰DFQæ–¹æ³•å­˜åœ¨è¯­ä¹‰å¤±çœŸå’Œè¯­ä¹‰ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>SARDFQæ˜¯ä¸€ç§æ–°å‹çš„é’ˆå¯¹ViTsçš„DFQæ–¹æ³•ï¼Œé€šè¿‡Attention Priors Alignment (APA)è§£å†³è¯­ä¹‰å¤±çœŸé—®é¢˜ã€‚</li>
<li>SARDFQå¼•å…¥Multi-Semantic Reinforcement (MSR)æ¥å¢å¼ºåˆæˆå›¾åƒçš„è¯­ä¹‰ä¸°å¯Œæ€§ï¼Œè§£å†³è¯­ä¹‰ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>SARDFQé‡‡ç”¨Soft-Label Learning (SL)ï¼Œé€‚åº”å¤šä¸ªè¯­ä¹‰ç›®æ ‡ï¼Œä¿ƒè¿›å¤šè¯­ä¹‰å›¾åƒçš„å­¦ä¹ ã€‚</li>
<li>å®éªŒè¯æ˜SARDFQåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.16553">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-424cca03769ca8455ab8b48ac3d7556f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd05a03ab7e79073e1962828e14e023a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-18fa203703979bf85a9848cdeea83d76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-27d74ebe7e661fa8002e28903c1a4c75.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2c766735de2aff444b0351c2a19c5c39.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Advancing-Textual-Prompt-Learning-with-Anchored-Attributes"><a href="#Advancing-Textual-Prompt-Learning-with-Anchored-Attributes" class="headerlink" title="Advancing Textual Prompt Learning with Anchored Attributes"></a>Advancing Textual Prompt Learning with Anchored Attributes</h2><p><strong>Authors:Zheng Li, Yibing Song, Ming-Ming Cheng, Xiang Li, Jian Yang</strong></p>
<p>Textual-based prompt learning methods primarily employ multiple learnable soft prompts and hard class tokens in a cascading manner as text inputs, aiming to align image and text (category) spaces for downstream tasks. However, current training is restricted to aligning images with predefined known categories and cannot be associated with unknown categories. In this work, we propose utilizing universal attributes as a bridge to enhance the alignment between images and unknown categories. Specifically, we introduce an Attribute-anchored Textual Prompt learning method for vision-language models, named ATPrompt. This approach expands the learning space of soft prompts from the original one-dimensional category level into the multi-dimensional attribute level by incorporating multiple attribute tokens into the learnable soft prompts. Through this modification, we transform the text prompt from a category-centric form to an attribute-category hybrid form. Additionally, we introduce a straightforward differentiable attribute search method to identify representative and suitable attributes for downstream tasks. As an easy-to-use plug-in technique, ATPrompt can seamlessly replace the existing basic prompt format in textual-based methods, providing general improvements at a negligible computational cost. Extensive experiments across 11 datasets validate the effectiveness of our method. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/zhengli97/ATPrompt">https://github.com/zhengli97/ATPrompt</a>. </p>
<blockquote>
<p>åŸºäºæ–‡æœ¬çš„æç¤ºå­¦ä¹ æ–¹æ³•ä¸»è¦ä½¿ç”¨å¤šä¸ªå¯å­¦ä¹ çš„è½¯æç¤ºå’Œç¡¬ç±»åˆ«ä»¤ç‰Œä»¥çº§è”æ–¹å¼ä½œä¸ºæ–‡æœ¬è¾“å…¥ï¼Œæ—¨åœ¨å¯¹é½å›¾åƒå’Œæ–‡æœ¬ï¼ˆç±»åˆ«ï¼‰ç©ºé—´ä»¥è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå½“å‰è®­ç»ƒä»…é™äºå¯¹é½å›¾åƒä¸é¢„å®šä¹‰çš„å·²çŸ¥ç±»åˆ«ï¼Œæ— æ³•ä¸æœªçŸ¥ç±»åˆ«ç›¸å…³è”ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨é€šç”¨å±æ€§ä½œä¸ºæ¡¥æ¢ï¼Œå¢å¼ºå›¾åƒä¸æœªçŸ¥ç±»åˆ«ä¹‹é—´çš„å¯¹é½ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹å¼•å…¥äº†ä¸€ç§åä¸ºATPromptçš„å±æ€§é”šå®šæ–‡æœ¬æç¤ºå­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†å¤šä¸ªå±æ€§ä»¤ç‰Œèå…¥å¯å­¦ä¹ çš„è½¯æç¤ºä¸­ï¼Œå°†è½¯æç¤ºçš„å­¦ä¹ ç©ºé—´ä»åŸå§‹çš„ä¸€ç»´ç±»åˆ«å±‚é¢æ‰©å±•åˆ°å¤šç»´å±æ€§å±‚é¢ã€‚é€šè¿‡è¿™ä¸€æ”¹è¿›ï¼Œæˆ‘ä»¬å°†æ–‡æœ¬æç¤ºä»ä»¥ç±»åˆ«ä¸ºä¸­å¿ƒçš„å½¢å¼è½¬å˜ä¸ºå±æ€§-ç±»åˆ«æ··åˆå½¢å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§ç®€å•çš„å¯åŒºåˆ†å±æ€§æœç´¢æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«ä¸‹æ¸¸ä»»åŠ¡çš„ä»£è¡¨æ€§å’Œåˆé€‚å±æ€§ã€‚ä½œä¸ºä¸€ç§æ˜“äºä½¿ç”¨çš„æ’ä»¶æŠ€æœ¯ï¼ŒATPromptå¯ä»¥æ— ç¼æ›¿æ¢åŸºäºæ–‡æœ¬æ–¹æ³•ä¸­çš„åŸºæœ¬æç¤ºæ ¼å¼ï¼Œä»¥å¾®å°çš„è®¡ç®—æˆæœ¬æä¾›ä¸€èˆ¬çš„æ”¹è¿›ã€‚åœ¨11ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ä»£ç å…¬å¼€åœ¨<a target="_blank" rel="noopener" href="https://github.com/zhengli97/ATPrompt%E3%80%82">https://github.com/zhengli97/ATPromptã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.09442v3">PDF</a> ICCV 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/zhengli97/ATPrompt">https://github.com/zhengli97/ATPrompt</a>. Project Page:   <a target="_blank" rel="noopener" href="https://zhengli97.github.io/ATPrompt/">https://zhengli97.github.io/ATPrompt/</a></p>
<p><strong>Summary</strong><br>åœ¨æ–‡æœ¬åŸºäºæç¤ºçš„å­¦ä¹ æ–¹æ³•åŸºç¡€ä¸Šï¼Œæå‡ºä¸€ç§åˆ©ç”¨é€šç”¨å±æ€§ä½œä¸ºæ¡¥æ¢ï¼Œå¢å¼ºå›¾åƒä¸æœªçŸ¥ç±»åˆ«ä¹‹é—´å¯¹é½çš„æ–¹æ³•ã€‚å¼•å…¥å±æ€§é”šå®šçš„æ–‡æœ¬æç¤ºå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆå¤šä¸ªå±æ€§æ ‡è®°ï¼Œå°†è½¯æç¤ºçš„å­¦ä¹ ç©ºé—´ä»åŸå§‹çš„ä¸€ç»´ç±»åˆ«å±‚é¢æ‰©å±•åˆ°å¤šç»´å±æ€§å±‚é¢ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†ä¸€ç§ç®€å•çš„å¯å¾®å±æ€§æœç´¢æ–¹æ³•ï¼Œç”¨äºç¡®å®šä¸‹æ¸¸ä»»åŠ¡çš„ä»£è¡¨æ€§å±æ€§ã€‚ä½œä¸ºä¸€ç§æ˜“äºä½¿ç”¨çš„æ’ä»¶æŠ€æœ¯ï¼ŒATPromptå¯ä»¥æ— ç¼åœ°æ›¿æ¢ç°æœ‰æ–‡æœ¬æç¤ºæ–¹æ³•ä¸­çš„åŸºæœ¬æ ¼å¼ï¼Œä»¥å¾®å°çš„è®¡ç®—æˆæœ¬æä¾›ä¸€èˆ¬æ€§çš„æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬åŸºäºæç¤ºçš„å­¦ä¹ æ–¹æ³•ä¸»è¦é€šè¿‡çº§è”å¤šä¸ªå¯å­¦ä¹ çš„è½¯æç¤ºå’Œç¡¬ç±»åˆ«æ ‡è®°ä½œä¸ºæ–‡æœ¬è¾“å…¥ï¼Œæ—¨åœ¨å®ç°å¯¹ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„å›¾åƒå’Œæ–‡æœ¬ï¼ˆç±»åˆ«ï¼‰ç©ºé—´çš„å¯¹é½ã€‚</li>
<li>å½“å‰è®­ç»ƒé™åˆ¶åœ¨äºåªèƒ½å¯¹é½å·²çŸ¥ç±»åˆ«çš„å›¾åƒï¼Œæ— æ³•å…³è”æœªçŸ¥ç±»åˆ«ã€‚</li>
<li>æå‡ºåˆ©ç”¨é€šç”¨å±æ€§ä½œä¸ºæ¡¥æ¢ï¼Œå¢å¼ºå›¾åƒä¸æœªçŸ¥ç±»åˆ«ä¹‹é—´çš„å¯¹é½ã€‚</li>
<li>å¼•å…¥å±æ€§é”šå®šçš„æ–‡æœ¬æç¤ºå­¦ä¹ æ–¹æ³•ï¼ˆATPromptï¼‰ï¼Œå°†è½¯æç¤ºçš„å­¦ä¹ ç©ºé—´æ‰©å±•åˆ°å¤šç»´å±æ€§å±‚é¢ã€‚</li>
<li>ATPromptå°†æ–‡æœ¬æç¤ºä»ä»¥ç±»åˆ«ä¸ºä¸­å¿ƒçš„å½¢å¼è½¬å˜ä¸ºå±æ€§-ç±»åˆ«æ··åˆå½¢å¼ã€‚</li>
<li>ATPromptæä¾›äº†ä¸€ç§ç®€å•çš„å¯å¾®å±æ€§æœç´¢æ–¹æ³•ï¼Œç”¨äºç¡®å®šé€‚åˆä¸‹æ¸¸ä»»åŠ¡çš„ä»£è¡¨æ€§å±æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2412.09442">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e763c344dbf3dadca4ec14afa7ee853d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-06aa6e2fefe8dde4dba31dba7bfd79b6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fd090d32be6751498824bcaaf161199e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-832074a8042eb9ad649239ab4da874f2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ad93f77b5a9881b0d5e71313c056e0b4.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Semantic-Equitable-Clustering-A-Simple-and-Effective-Strategy-for-Clustering-Vision-Tokens"><a href="#Semantic-Equitable-Clustering-A-Simple-and-Effective-Strategy-for-Clustering-Vision-Tokens" class="headerlink" title="Semantic Equitable Clustering: A Simple and Effective Strategy for   Clustering Vision Tokens"></a>Semantic Equitable Clustering: A Simple and Effective Strategy for   Clustering Vision Tokens</h2><p><strong>Authors:Qihang Fan, Huaibo Huang, Mingrui Chen, Ran He</strong></p>
<p>The Vision Transformer (ViT) has gained prominence for its superior relational modeling prowess. However, its global attention mechanismâ€™s quadratic complexity poses substantial computational burdens. A common remedy spatially groups tokens for self-attention, reducing computational requirements. Nonetheless, this strategy neglects semantic information in tokens, possibly scattering semantically-linked tokens across distinct groups, thus compromising the efficacy of self-attention intended for modeling inter-token dependencies. Motivated by these insights, we introduce a fast and balanced clustering method, named Semantic Equitable Clustering (SEC). SEC clusters tokens based on their global semantic relevance in an efficient, straightforward manner. In contrast to traditional clustering methods requiring multiple iterations, our method achieves token clustering in a single pass. Additionally, SEC regulates the number of tokens per cluster, ensuring a balanced distribution for effective parallel processing on current computational platforms without necessitating further optimization. Capitalizing on SEC, we propose a versatile vision backbone, SECViT. Comprehensive experiments in image classification, object detection, instance segmentation, and semantic segmentation validate the effectiveness of SECViT. Moreover, SEC can be conveniently and swiftly applied to multimodal large language models (MLLM), such as LLaVA, to serve as a vision language connector, effectively accelerating the modelâ€™s efficiency while maintaining unchanged or better performance. </p>
<blockquote>
<p>Vision Transformerï¼ˆViTï¼‰å› å…¶å‡ºè‰²çš„å…³ç³»å»ºæ¨¡èƒ½åŠ›è€Œå¤‡å—ç©ç›®ã€‚ç„¶è€Œï¼Œå…¶å…¨å±€æ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚æ€§å¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—è´Ÿæ‹…ã€‚ä¸€ç§å¸¸è§çš„è¡¥æ•‘æ–¹æ³•æ˜¯ç©ºé—´åˆ†ç»„ä»¤ç‰Œè¿›è¡Œè‡ªæ³¨æ„åŠ›ï¼Œä»¥é™ä½è®¡ç®—è¦æ±‚ã€‚ç„¶è€Œï¼Œè¿™ç§ç­–ç•¥å¿½ç•¥äº†ä»¤ç‰Œä¸­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯èƒ½å°†è¯­ä¹‰ä¸Šç›¸å…³è”çš„ä»¤ç‰Œåˆ†æ•£åˆ°ä¸åŒçš„ç»„ï¼Œä»è€ŒæŸå®³è‡ªæ³¨æ„åŠ›åœ¨å»ºæ¨¡ä»¤ç‰Œé—´ä¾èµ–å…³ç³»æ—¶çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p>åŸºäºè¿™äº›è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¿«é€Ÿä¸”å‡è¡¡çš„èšç±»æ–¹æ³•ï¼Œç§°ä¸ºè¯­ä¹‰å‡è¡¡èšç±»ï¼ˆSECï¼‰ã€‚SECä»¥é«˜æ•ˆã€ç®€å•çš„æ–¹å¼ï¼ŒåŸºäºä»¤ç‰Œçš„å…¨å±€è¯­ä¹‰ç›¸å…³æ€§å¯¹ä»¤ç‰Œè¿›è¡Œèšç±»ã€‚ä¸ä¼ ç»Ÿçš„éœ€è¦å¤šæ¬¡è¿­ä»£çš„èšç±»æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨å•æ¬¡ä¼ é€’ä¸­å®ç°ä»¤ç‰Œèšç±»ã€‚æ­¤å¤–ï¼ŒSECè¿˜è°ƒèŠ‚æ¯ä¸ªé›†ç¾¤ä¸­çš„ä»¤ç‰Œæ•°é‡ï¼Œç¡®ä¿åœ¨å½“å‰è®¡ç®—å¹³å°ä¸Šè¿›è¡Œæœ‰æ•ˆçš„å¹¶è¡Œå¤„ç†ï¼Œè€Œæ— éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚</p>
<p>åˆ©ç”¨SECï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨çš„è§†è§‰ä¸»å¹²ç½‘SECViTã€‚åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œè¯­ä¹‰åˆ†å‰²æ–¹é¢çš„ç»¼åˆå®éªŒéªŒè¯äº†SECViTçš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒSECè¿˜å¯ä»¥è½»æ¾å¿«é€Ÿåœ°åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå¦‚LLaVAï¼Œä½œä¸ºè§†è§‰è¯­è¨€è¿æ¥å™¨ï¼Œåœ¨ä¿æŒæ€§èƒ½ä¸å˜æˆ–æ›´å¥½çš„æƒ…å†µä¸‹æœ‰æ•ˆæé«˜æ¨¡å‹çš„æ•ˆç‡ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.13337v3">PDF</a> Accepted by ICCV2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Vision Transformerï¼ˆViTï¼‰åœ¨å…³ç³»å»ºæ¨¡æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä½†å…¶å…¨å±€æ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚æ€§å¸¦æ¥äº†è¾ƒå¤§çš„è®¡ç®—è´Ÿæ‹…ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å¿«é€Ÿå‡è¡¡èšç±»æ–¹æ³•â€”â€”Semantic Equitable Clusteringï¼ˆSECï¼‰ã€‚SECåŸºäºå…¨å±€è¯­ä¹‰ç›¸å…³æ€§å¯¹ä»¤ç‰Œè¿›è¡Œèšç±»ï¼Œå®ç°äº†é«˜æ•ˆç›´æ¥çš„èšç±»æ–¹å¼ã€‚ç›¸è¾ƒäºä¼ ç»Ÿèšç±»æ–¹æ³•çš„å¤šè¿­ä»£è¿‡ç¨‹ï¼ŒSECå®ç°äº†å•éèšç±»ã€‚æ­¤å¤–ï¼ŒSECè°ƒæ§æ¯ä¸ªé›†ç¾¤çš„ä»¤ç‰Œæ•°é‡ï¼Œç¡®ä¿äº†åœ¨ç°æœ‰è®¡ç®—å¹³å°ä¸Šçš„æœ‰æ•ˆå¹¶è¡Œå¤„ç†ã€‚åŸºäºSECï¼Œæå‡ºäº†ä¸€ç§é€šç”¨çš„è§†è§‰ä¸»å¹²SECViTã€‚åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œè¯­ä¹‰åˆ†å‰²ç­‰æ–¹é¢çš„å®éªŒéªŒè¯äº†SECViTçš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼ŒSECå¯è½»æ¾å¿«é€Ÿåœ°åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œå¦‚LLaVAï¼Œä½œä¸ºè§†è§‰è¯­è¨€è¿æ¥å™¨ï¼Œåœ¨ä¿æŒæ€§èƒ½ä¸å˜æˆ–æ›´å¥½çš„æƒ…å†µä¸‹ï¼Œæœ‰æ•ˆæé«˜æ¨¡å‹çš„æ•ˆç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformer (ViT) å±•ç°å‡ºå“è¶Šçš„å…³ç³»å»ºæ¨¡èƒ½åŠ›ï¼Œä½†å…¶å…¨å±€æ³¨æ„åŠ›æœºåˆ¶çš„äºŒæ¬¡å¤æ‚æ€§å¸¦æ¥è®¡ç®—è´Ÿæ‹…ã€‚</li>
<li>ç°æœ‰çš„ç©ºé—´åˆ†ç»„ä»¤ç‰Œç­–ç•¥å¿½ç•¥äº†ä»¤ç‰Œé—´çš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯èƒ½å½±å“å»ºæ¨¡æ•ˆæœã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§å¿«é€Ÿå‡è¡¡èšç±»æ–¹æ³•â€”â€”Semantic Equitable Clustering (SEC)ï¼ŒåŸºäºå…¨å±€è¯­ä¹‰ç›¸å…³æ€§è¿›è¡Œé«˜æ•ˆç›´æ¥çš„èšç±»ã€‚</li>
<li>SECå®ç°äº†å•éèšç±»è¿‡ç¨‹ï¼Œå¹¶è°ƒæ§æ¯ä¸ªé›†ç¾¤çš„ä»¤ç‰Œæ•°é‡ï¼Œä¾¿äºåœ¨ç°æœ‰è®¡ç®—å¹³å°ä¸Šçš„å¹¶è¡Œå¤„ç†ã€‚</li>
<li>åŸºäºSECæå‡ºäº†SECViTæ¨¡å‹ï¼Œç”¨äºæé«˜å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>SECå¯ä»¥æ–¹ä¾¿åœ°åº”ç”¨äºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œä½œä¸ºè§†è§‰è¯­è¨€è¿æ¥å™¨ä»¥æé«˜æ¨¡å‹æ•ˆç‡å¹¶ä¿æŒæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.13337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4ba5727f96ec81c5f89be589e85b75f1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aea9e48ff10c0b2c0b406aa031344485.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cad6b7473473e9d3c1a1472de9918d9f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a71709a38c55bb7ecb49c55d2ca0606f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b396250839dfccd8e0743ef4e54f5eb.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-dd30b6d037260d00604c6b308e77b738.jpg" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-04  A Gift from the Integration of Discriminative and Diffusion-based   Generative Learning Boundary Refinement Remote Sensing Semantic Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/I2I%20Translation/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-b92c85046bac61be54c371272062d92f.jpg" class="responsive-img" alt="I2I Translation">
                        
                        <span class="card-title">I2I Translation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            I2I Translation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-04  How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/I2I-Translation/" class="post-category">
                                    I2I Translation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/I2I-Translation/">
                        <span class="chip bg-color">I2I Translation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
