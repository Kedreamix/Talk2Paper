<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-04  Kwai Keye-VL Technical Report">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pica.zhimg.com/v2-92e8021beacd89a3722a62a26f80a163.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-04
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-09
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    83 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-07-04-æ›´æ–°"><a href="#2025-07-04-æ›´æ–°" class="headerlink" title="2025-07-04 æ›´æ–°"></a>2025-07-04 æ›´æ–°</h1><h2 id="Kwai-Keye-VL-Technical-Report"><a href="#Kwai-Keye-VL-Technical-Report" class="headerlink" title="Kwai Keye-VL Technical Report"></a>Kwai Keye-VL Technical Report</h2><p><strong>Authors: Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Hao Peng, Haojie Ding, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Jin Ouyang, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yang Zhou, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zhenhua Wu, Zhenyu Li, Zhixin Ling, Ziming Li, Dehua Ma, Di Xu, Haixuan Gao, Hang Li, Jiawei Guo, Jing Wang, Lejian Ren, Muhao Wei, Qianqian Wang, Qigen Hu, Shiyao Wang, Tao Yu, Xinchen Luo, Yan Li, Yiming Liang, Yuhang Hu, Zeyi Lu, Zhuoran Yang, Zixing Zhang</strong></p>
<p>While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in todayâ€™s digital landscape. To bridge this gap, we introduce \textbf{Kwai Keye-VL}, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a four-stage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode <code>cold-start&#39;&#39; data mixture, which includes </code>thinkingâ€™â€™, <code>non-thinking&#39;&#39;, </code>auto-thinkâ€™â€™, &#96;&#96;think with imageâ€™â€™, and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the \textbf{KC-MMBench}, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage. </p>
<blockquote>
<p>è™½ç„¶å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨é™æ€å›¾åƒä¸Šè¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œä½†å®ƒä»¬é€šå¸¸éš¾ä»¥ç†è§£åŠ¨æ€ã€ä¿¡æ¯å¯†é›†çš„çŸ­è§†é¢‘ï¼Œè€ŒçŸ­è§†é¢‘å´æ˜¯å½“ä»Šæ•°å­—æ—¶ä»£çš„ä¸»å¯¼åª’ä½“ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†<strong>Kwai Keye-VL</strong>ï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘çŸ­è§†é¢‘ç†è§£çš„å…ˆè¿›å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼Œæ‹¥æœ‰8äº¿å‚æ•°ï¼ŒåŒæ—¶ä¿æŒå¼ºå¤§çš„é€šç”¨è§†è§‰è¯­è¨€åŠŸèƒ½ã€‚Keye-VLçš„å‘å±•åŸºäºä¸¤ä¸ªæ ¸å¿ƒï¼šä¸€ä¸ªè§„æ¨¡åºå¤§ã€é«˜è´¨é‡çš„æ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡600äº¿ä¸ªæ ‡è®°ï¼Œå¹¶å¼ºçƒˆä¾§é‡äºè§†é¢‘ï¼›ä»¥åŠä¸€ç§åˆ›æ–°çš„è®­ç»ƒé…æ–¹ã€‚è¯¥é…æ–¹å…·æœ‰å››ä¸ªé˜¶æ®µçš„é¢„è®­ç»ƒè¿‡ç¨‹ï¼Œä»¥å®ç°åšå®çš„è§†è§‰è¯­è¨€å¯¹é½ï¼Œéšåæ˜¯ç»†è‡´çš„ä¸¤é˜¶æ®µåè®­ç»ƒè¿‡ç¨‹ã€‚ç¬¬ä¸€é˜¶æ®µå¢å¼ºåŸºç¡€èƒ½åŠ›ï¼Œå¦‚æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼›ç¬¬äºŒé˜¶æ®µåˆ™ä¸“æ³¨äºåˆºæ¿€é«˜çº§æ¨ç†èƒ½åŠ›ã€‚åœ¨ç¬¬äºŒé˜¶æ®µä¸­ï¼Œæˆ‘ä»¬çš„å…³é”®åˆ›æ–°æ˜¯äº”æ¨¡å¼â€œå†·å¯åŠ¨â€æ•°æ®æ··åˆï¼ŒåŒ…æ‹¬â€œæ€è€ƒâ€ã€â€œéæ€è€ƒâ€ã€â€œè‡ªåŠ¨æ€è€ƒâ€ã€â€œä¸å›¾åƒä¸€èµ·æ€è€ƒâ€ä»¥åŠé«˜è´¨é‡çš„è§†é¢‘æ•°æ®ã€‚è¿™ç§æ··åˆæ•™ä¼šæ¨¡å‹å†³å®šä½•æ—¶ä»¥åŠå¦‚ä½•æ¨ç†ã€‚éšåçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œå¯¹é½æ­¥éª¤è¿›ä¸€æ­¥å¢å¼ºäº†è¿™äº›æ¨ç†èƒ½åŠ›ï¼Œå¹¶çº æ­£äº†å¼‚å¸¸æ¨¡å‹è¡Œä¸ºï¼Œä¾‹å¦‚é‡å¤è¾“å‡ºã€‚ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºKeye-VLåœ¨å…¬å…±è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œå¹¶åœ¨åŸºäºå›¾åƒçš„ä¸€èˆ¬ä»»åŠ¡ä¸­ä¿æŒé«˜åº¦ç«äº‰åŠ›ï¼ˆå›¾1ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘å¹¶å‘å¸ƒäº†é’ˆå¯¹ç°å®çŸ­è§†é¢‘åœºæ™¯çš„å…¨æ–°åŸºå‡†æµ‹è¯•<strong>KC-MMBench</strong>ï¼ŒKeye-VLåœ¨æ­¤åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01949v1">PDF</a> Technical Report: <a target="_blank" rel="noopener" href="https://github.com/Kwai-Keye/Keye">https://github.com/Kwai-Keye/Keye</a></p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†é™æ€å›¾åƒæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç†è§£å’Œå¤„ç†åŠ¨æ€ã€ä¿¡æ¯å¯†é›†å‹çš„çŸ­è§†é¢‘æ–¹é¢å­˜åœ¨ç¼ºé™·ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†â€œKwai Keye-VLâ€æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹çŸ­è§†é¢‘ç†è§£çš„å‰æ²¿æ€§èƒ½çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†å¼ºå¤§çš„é€šç”¨è§†è§‰è¯­è¨€åŠŸèƒ½ã€‚å…¶æ ¸å¿ƒå‘å±•åŸºäºä¸¤ä¸ªæ”¯æŸ±ï¼šè¶…è¿‡600äº¿ä»¤ç‰Œçš„å¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼Œé‡ç‚¹å¼ºè°ƒè§†é¢‘å†…å®¹ï¼›ä»¥åŠåˆ›æ–°çš„è®­ç»ƒé…æ–¹ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€åè®­ç»ƒä¸¤ä¸ªé˜¶æ®µã€‚Keye-VLåœ¨å…¬å…±è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°ç»“æœï¼Œå¹¶åœ¨é€šç”¨å›¾åƒä»»åŠ¡ä¸­ä¿æŒé«˜åº¦ç«äº‰åŠ›ã€‚åŒæ—¶æ¨å‡ºé’ˆå¯¹çœŸå®çŸ­è§†é¢‘åœºæ™¯çš„å…¨æ–°åŸºå‡†æµ‹è¯•KC-MMBenchï¼ŒKeye-VLå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨çŸ­è§†é¢‘ç†è§£æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>Kwai Keye-VLæ¨¡å‹æ—¨åœ¨è§£å†³è¿™ä¸€ç¼ºé™·ï¼Œå…·æœ‰å¼ºå¤§çš„çŸ­è§†é¢‘ç†è§£èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒé€šç”¨è§†è§‰è¯­è¨€åŠŸèƒ½ã€‚</li>
<li>Keye-VLçš„å‘å±•åŸºäºå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†å’Œåˆ›æ–°çš„è®­ç»ƒé…æ–¹ã€‚</li>
<li>è®­ç»ƒé…æ–¹åŒ…æ‹¬é¢„è®­ç»ƒã€åè®­ç»ƒä¸¤ä¸ªé˜¶æ®µï¼Œæ³¨é‡è§†è§‰è¯­è¨€å¯¹é½å’Œé«˜çº§æ¨ç†èƒ½åŠ›çš„åŸ¹å…»ã€‚</li>
<li>Keye-VLåœ¨å…¬å…±è§†é¢‘åŸºå‡†æµ‹è¯•ä¸­å–å¾—æœ€æ–°æˆæœï¼Œå¹¶åœ¨é€šç”¨å›¾åƒä»»åŠ¡ä¸­ä¿æŒç«äº‰åŠ›ã€‚</li>
<li>æ¨å‡ºæ–°çš„åŸºå‡†æµ‹è¯•KC-MMBenchï¼Œé’ˆå¯¹çœŸå®çŸ­è§†é¢‘åœºæ™¯ï¼ŒKeye-VLå±•ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-71d2b8efe72733d01ee1b92778680729.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9a24ae13c309662119cc8e1efce624ab.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Driven-Closed-Loop-UAV-Operation-with-Semantic-Observations"><a href="#Large-Language-Model-Driven-Closed-Loop-UAV-Operation-with-Semantic-Observations" class="headerlink" title="Large Language Model-Driven Closed-Loop UAV Operation with Semantic   Observations"></a>Large Language Model-Driven Closed-Loop UAV Operation with Semantic   Observations</h2><p><strong>Authors:Wenhao Wang, Yanyan Li, Long Jiao, Jiawei Yuan</strong></p>
<p>Large Language Models (LLMs) have revolutionized robotic autonomy, including Unmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential of LLMs for translating human instructions into executable control code for UAV operations. However, LLMs still face challenges from logical reasoning and complex decision-making, leading to concerns about the reliability of LLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop control framework that enables reliable UAV operations powered by effective feedback and refinement using two LLM modules, i.e., a Code Generator and an Evaluator. Our framework transforms numerical state observations from UAV operations into natural language trajectory descriptions to enhance the evaluator LLMâ€™s understanding of UAV dynamics for precise feedback generation. Our framework also enables a simulation-based refinement process, and hence eliminates the risks to physical UAVs caused by incorrect code execution during the refinement. Extensive experiments on UAV control tasks with different complexities are conducted. The experimental results show that our framework can achieve reliable UAV operations using LLMs, which significantly outperforms baseline approaches in terms of success rate and completeness with the increase of task complexity. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»å½»åº•æ”¹å˜äº†æœºå™¨äººè‡ªä¸»æ€§ï¼ŒåŒ…æ‹¬æ— äººæœºï¼ˆUAVsï¼‰ã€‚æœ€è¿‘çš„ç ”ç©¶å·²ç»è¯æ˜äº†LLMå°†äººç±»æŒ‡ä»¤ç¿»è¯‘æˆå¯æ‰§è¡Œæ§åˆ¶ä»£ç ä»¥æ”¯æŒæ— äººæœºæ“ä½œçš„æ½œåŠ›ã€‚ç„¶è€Œï¼ŒLLMåœ¨é€»è¾‘æ¨ç†å’Œå¤æ‚å†³ç­–åˆ¶å®šæ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œè¿™å¼•å‘äº†äººä»¬å¯¹LLMé©±åŠ¨çš„æ— äººæœºæ“ä½œå¯é æ€§çš„æ‹…å¿§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§LLMé©±åŠ¨çš„é—­ç¯æ§åˆ¶æ¡†æ¶ï¼Œé€šè¿‡æœ‰æ•ˆçš„åé¦ˆå’Œæ”¹è¿›ï¼Œä½¿ç”¨ä¸¤ä¸ªLLMæ¨¡å—å³ä»£ç ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ï¼Œå®ç°äº†å¯é çš„æ— äººæœºæ“ä½œã€‚æˆ‘ä»¬çš„æ¡†æ¶å°†æ— äººæœºçš„æ•°å€¼çŠ¶æ€è§‚å¯Ÿè½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€è½¨è¿¹æè¿°ï¼Œå¢å¼ºäº†è¯„ä¼°LLMå¯¹æ— äººæœºåŠ¨åŠ›å­¦çš„ç†è§£ï¼Œä»¥å®ç°ç²¾ç¡®åé¦ˆç”Ÿæˆã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿˜å¯ç”¨äº†åŸºäºæ¨¡æ‹Ÿçš„æ”¹è¿›è¿‡ç¨‹ï¼Œå› æ­¤æ¶ˆé™¤äº†æ”¹è¿›è¿‡ç¨‹ä¸­ç”±äºä»£ç æ‰§è¡Œä¸æ­£ç¡®è€Œå¯¹å®é™…æ— äººæœºé€ æˆçš„é£é™©ã€‚åœ¨å…·æœ‰ä¸åŒå¤æ‚æ€§çš„æ— äººæœºæ§åˆ¶ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶èƒ½å¤Ÿåœ¨LLMæ”¯æŒä¸‹å®ç°å¯é çš„æ— äººæœºæ“ä½œï¼Œéšç€ä»»åŠ¡å¤æ‚æ€§çš„å¢åŠ ï¼Œåœ¨æˆåŠŸç‡å’Œå®Œæ•´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01930v1">PDF</a> 10 pages</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ— äººæœºè‡ªä¸»é£è¡Œé¢†åŸŸå¼•å‘äº†é©å‘½æ€§çš„å˜é©ï¼Œä½†å…¶åœ¨é€»è¾‘ç†è§£å’Œå¤æ‚å†³ç­–åˆ¶å®šæ–¹é¢ä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§LLMé©±åŠ¨çš„é—­ç¯æ§åˆ¶æ¡†æ¶ï¼Œåˆ©ç”¨ä¸¤ä¸ªLLMæ¨¡å—ï¼ˆä»£ç ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ï¼‰æä¾›æœ‰æ•ˆåé¦ˆå’Œæ”¹è¿›ï¼Œä»¥å®ç°å¯é çš„æ— äººæœºæ“ä½œã€‚è¯¥æ¡†æ¶é€šè¿‡å°†æ— äººæœºçš„æ•°å€¼çŠ¶æ€è§‚æµ‹è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€è½¨è¿¹æè¿°ï¼Œå¢å¼ºè¯„ä¼°å™¨å¯¹æ— äººæœºåŠ¨åŠ›å­¦çš„ç†è§£ï¼Œä»è€Œç”Ÿæˆç²¾ç¡®åé¦ˆã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜æ”¯æŒåŸºäºæ¨¡æ‹Ÿçš„æ”¹è¿›æµç¨‹ï¼Œå‡å°‘äº†æ”¹è¿›è¿‡ç¨‹ä¸­ç”±äºä»£ç æ‰§è¡Œé”™è¯¯å¯¹å®é™…æ— äººæœºé€ æˆçš„é£é™©ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°å¯é çš„LLMé©±åŠ¨çš„æ— äººæœºæ“ä½œï¼Œå¹¶ä¸”åœ¨ä»»åŠ¡å¤æ‚æ€§å¢åŠ çš„æƒ…å†µä¸‹ï¼Œå…¶åœ¨æˆåŠŸç‡å’Œå®Œæ•´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ— äººæœºè‡ªä¸»é£è¡Œé¢†åŸŸå…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>LLMsåœ¨é€»è¾‘ç†è§£å’Œå¤æ‚å†³ç­–åˆ¶å®šæ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œå½±å“æ— äººæœºæ“ä½œçš„å¯é æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§LLMé©±åŠ¨çš„é—­ç¯æ§åˆ¶æ¡†æ¶ï¼Œåˆ©ç”¨ä»£ç ç”Ÿæˆå™¨å’Œè¯„ä¼°å™¨ä¸¤ä¸ªæ¨¡å—æä¾›æœ‰æ•ˆåé¦ˆå’Œæ”¹è¿›ã€‚</li>
<li>æ¡†æ¶èƒ½å°†æ— äººæœºçš„æ•°å€¼çŠ¶æ€è§‚æµ‹è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€è½¨è¿¹æè¿°ï¼Œå¢å¼ºè¯„ä¼°å™¨å¯¹æ— äººæœºåŠ¨åŠ›å­¦çš„ç†è§£ã€‚</li>
<li>æ¡†æ¶æ”¯æŒåŸºäºæ¨¡æ‹Ÿçš„æ”¹è¿›æµç¨‹ï¼Œå‡å°‘å®é™…æ— äººæœºçš„é£é™©ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æˆåŠŸç‡å’Œå®Œæ•´æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»»åŠ¡å¤æ‚æ€§å¢åŠ çš„æƒ…å†µä¸‹ã€‚</li>
<li>è¯¥æ¡†æ¶ä¸ºå®ç°æ›´é«˜çº§åˆ«çš„æ— äººæœºè‡ªä¸»æ“ä½œæä¾›äº†å¯èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-8b60c2d2eb8cf32ed5c1842971859e76.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1b4fab06cc3db47da849f4fd44147abe.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-b06dfa91ab452f7067797ee3365e2f5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a691b73db0c36312c4a697f6aec9130.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7f682c04b456367edb962cd4817e6762.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6747a0fa04a0d4a381a966325df8558b.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="NaturalThoughts-Selecting-and-Distilling-Reasoning-Traces-for-General-Reasoning-Tasks"><a href="#NaturalThoughts-Selecting-and-Distilling-Reasoning-Traces-for-General-Reasoning-Tasks" class="headerlink" title="NaturalThoughts: Selecting and Distilling Reasoning Traces for General   Reasoning Tasks"></a>NaturalThoughts: Selecting and Distilling Reasoning Traces for General   Reasoning Tasks</h2><p><strong>Authors:Yang Li, Youssef Emad, Karthik Padthe, Jack Lanchantin, Weizhe Yuan, Thao Nguyen, Jason Weston, Shang-Wen Li, Dong Wang, Ilia Kulikov, Xian Li</strong></p>
<p>Recent work has shown that distilling reasoning traces from a larger teacher model via supervised finetuning outperforms reinforcement learning with the smaller student model alone (Guo et al. 2025). However, there has not been a systematic study of what kind of reasoning demonstrations from the teacher are most effective in improving the student modelâ€™s reasoning capabilities. In this work we curate high-quality â€œNaturalThoughtsâ€ by selecting reasoning traces from a strong teacher model based on a large pool of questions from NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of factors that affect distilling reasoning capabilities, in terms of sample efficiency and scalability for general reasoning tasks. We observe that simply scaling up data size with random sampling is a strong baseline with steady performance gains. Further, we find that selecting difficult examples that require more diverse reasoning strategies is more sample-efficient to transfer the teacher modelâ€™s reasoning skills. Evaluated on both Llama and Qwen models, training with NaturalThoughts outperforms existing reasoning datasets such as OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including GPQA-Diamond, MMLU-Pro and SuperGPQA. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æœ‰ç›‘ç£å¾®è°ƒä»è¾ƒå¤§çš„æ•™å¸ˆæ¨¡å‹ä¸­è’¸é¦æ¨ç†è½¨è¿¹ï¼Œä¼˜äºä»…ä½¿ç”¨è¾ƒå°çš„å­¦ç”Ÿæ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼ˆGuoç­‰ï¼Œ2025å¹´ï¼‰ã€‚ç„¶è€Œï¼Œå°šæœªæœ‰ç³»ç»Ÿç ”ç©¶æ•™å¸ˆå±•ç¤ºä½•ç§æ¨ç†æœ€æœ‰æ•ˆï¼Œä»¥æé«˜å­¦ç”Ÿæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»å¤§é‡é—®é¢˜ä¸­ç­›é€‰æ¨ç†è½¨è¿¹ï¼Œé€šè¿‡å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ç²¾å¿ƒåˆ¶ä½œé«˜è´¨é‡çš„â€œNaturalThoughtsâ€ï¼ˆYuanç­‰ï¼Œ2025å¹´ï¼‰ã€‚æˆ‘ä»¬é¦–å…ˆç³»ç»Ÿåœ°åˆ†æäº†å½±å“è’¸é¦æ¨ç†èƒ½åŠ›çš„å› ç´ ï¼ŒåŒ…æ‹¬æ ·æœ¬æ•ˆç‡å’Œä¸€èˆ¬æ¨ç†ä»»åŠ¡çš„æ‰©å±•æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œä»…é€šè¿‡éšæœºæŠ½æ ·æ‰©å¤§æ•°æ®é‡æ˜¯ä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿ï¼Œå…·æœ‰ç¨³å®šçš„æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°é€‰æ‹©éœ€è¦æ›´å¤šä¸åŒæ¨ç†ç­–ç•¥çš„å›°éš¾ä¾‹å­åœ¨è½¬ç§»æ•™å¸ˆæ¨¡å‹çš„æ¨ç†æŠ€èƒ½æ–¹é¢æ›´ä¸ºæ ·æœ¬é«˜æ•ˆã€‚åœ¨Llamaå’ŒQwenæ¨¡å‹ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œä½¿ç”¨NaturalThoughtsè¿›è¡Œè®­ç»ƒä¼˜äºç°æœ‰çš„æ¨ç†æ•°æ®é›†ï¼Œå¦‚OpenThoughtsã€LIMOç­‰ï¼Œåœ¨åŒ…æ‹¬GPQA-Diamondã€MMLU-Proå’ŒSuperGPQAåœ¨å†…çš„é€šç”¨STEMæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æ›´ä½³ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01921v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†é€šè¿‡æ•™å¸ˆæ¨¡å‹çš„æ¨ç†ç—•è¿¹æ¥æ”¹è¿›å­¦ç”Ÿæ¨¡å‹çš„æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚ç ”ç©¶è¡¨æ˜ï¼Œä»å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ä¸­ç­›é€‰é«˜è´¨é‡æ¨ç†ç—•è¿¹ï¼Œå¹¶åŸºäºè¿™äº›å› ç´ è¿›è¡Œç³»ç»Ÿæ€§åˆ†æï¼Œå¯ä»¥æé«˜æ ·æœ¬æ•ˆç‡å’Œå¯æ¨å¹¿æ€§ã€‚é€šè¿‡å¯¹æ¯”å®éªŒï¼Œå‘ç°é€‰æ‹©éœ€è¦æ›´å¤šæ ·åŒ–æ¨ç†ç­–ç•¥çš„ä¾‹å­æ›´ä¸ºæœ‰æ•ˆã€‚åœ¨å¤šä¸ªæ¨¡å‹ä¸Šè¯„ä¼°ï¼Œä½¿ç”¨NaturalThoughtsè®­ç»ƒçš„æ•ˆæœä¼˜äºç°æœ‰æ¨ç†æ•°æ®é›†ï¼Œå¦‚OpenThoughtsã€LIMOç­‰ï¼Œåœ¨STEMæ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•™å¸ˆæ¨¡å‹çš„æ¨ç†ç—•è¿¹å¯æ”¹è¿›å­¦ç”Ÿæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é€šè¿‡ç³»ç»Ÿæ€§åˆ†æï¼Œå‘ç°å½±å“è’¸é¦æ¨ç†èƒ½åŠ›çš„å…³é”®å› ç´ åŒ…æ‹¬æ ·æœ¬æ•ˆç‡å’Œå¯æ¨å¹¿æ€§ã€‚</li>
<li>å•çº¯æ‰©å¤§æ•°æ®è§„æ¨¡å¹¶éšæœºé‡‡æ ·æ˜¯æ€§èƒ½æå‡çš„å¼ºåŸºçº¿ã€‚</li>
<li>é€‰æ‹©éœ€è¦æ›´å¤šæ ·åŒ–æ¨ç†ç­–ç•¥çš„ä¾‹å­æ›´ä¸ºæ ·æœ¬é«˜æ•ˆã€‚</li>
<li>ä½¿ç”¨NaturalThoughtsè®­ç»ƒæ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºç°æœ‰æ¨ç†æ•°æ®é›†ã€‚</li>
<li>åœ¨STEMæ¨ç†æµ‹è¯•ä¸­ï¼Œä½¿ç”¨NaturalThoughtsè®­ç»ƒçš„æ¨¡å‹å¦‚Llamaå’ŒQwenè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01921">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-95b90ad6360e6116491ab2b98c513d50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b028801b78d531ee032b3e86ec8644a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bfd0e4b9d011be669f7e9a95d44f7a7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e921adc8a09f20fef9add02968aed45.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Gradient-Adaptive-Policy-Optimization-Towards-Multi-Objective-Alignment-of-Large-Language-Models"><a href="#Gradient-Adaptive-Policy-Optimization-Towards-Multi-Objective-Alignment-of-Large-Language-Models" class="headerlink" title="Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment   of Large Language Models"></a>Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment   of Large Language Models</h2><p><strong>Authors:Chengao Li, Hanyu Zhang, Yunkun Xu, Hongyan Xue, Xiang Ao, Qing He</strong></p>
<p>Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful technique for aligning large language models (LLMs) with human preferences. However, effectively aligning LLMs with diverse human preferences remains a significant challenge, particularly when they are conflict. To address this issue, we frame human value alignment as a multi-objective optimization problem, aiming to maximize a set of potentially conflicting objectives. We introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning paradigm that employs multiple-gradient descent to align LLMs with diverse preference distributions. GAPO adaptively rescales the gradients for each objective to determine an update direction that optimally balances the trade-offs between objectives. Additionally, we introduce P-GAPO, which incorporates user preferences across different objectives and achieves Pareto solutions that better align with the userâ€™s specific needs. Our theoretical analysis demonstrates that GAPO converges towards a Pareto optimal solution for multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms current state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰å·²ç»æˆä¸ºä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œç”¨äºå°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½å¯¹é½ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆåœ°å°†LLMä¸å¤šæ ·åŒ–çš„äººç±»åå¥½å¯¹é½ä»ç„¶æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å½“è¿™äº›åå¥½å†²çªæ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å°†äººç±»ä»·å€¼å¯¹é½ä½œä¸ºä¸€ä¸ªå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ä¸€ç»„å¯èƒ½ç›¸äº’å†²çªçš„ç›®æ ‡ã€‚æˆ‘ä»¬å¼•å…¥äº†æ¢¯åº¦è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼ˆGAPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¾®è°ƒèŒƒå¼ï¼Œé‡‡ç”¨å¤šæ¢¯åº¦ä¸‹é™æ³•å°†LLMä¸å¤šæ ·åŒ–çš„åå¥½åˆ†å¸ƒå¯¹é½ã€‚GAPOè‡ªé€‚åº”åœ°é‡æ–°è°ƒæ•´æ¯ä¸ªç›®æ ‡çš„æ¢¯åº¦ï¼Œä»¥ç¡®å®šä¸€ä¸ªæ›´æ–°æ–¹å‘ï¼Œè¯¥æ–¹å‘èƒ½æœ€ä¼˜åœ°å¹³è¡¡ç›®æ ‡ä¹‹é—´çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†P-GAPOï¼Œå®ƒç»“åˆäº†ä¸åŒç›®æ ‡ä¸Šçš„ç”¨æˆ·åå¥½ï¼Œå®ç°äº†å¸•ç´¯æ‰˜è§£å†³æ–¹æ¡ˆï¼Œæ›´å¥½åœ°ç¬¦åˆç”¨æˆ·çš„ç‰¹å®šéœ€æ±‚ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æè¡¨æ˜ï¼ŒGAPOæœç€å¤šä¸ªç›®æ ‡çš„å¸•ç´¯æ‰˜æœ€ä¼˜è§£æ”¶æ•›ã€‚åœ¨Mistral-7Bä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒGAPOä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨æœ‰ç”¨æ€§å’Œæ— å®³æ€§æ–¹é¢éƒ½å®ç°äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01915v1">PDF</a> 19 pages, 3 figures. Accepted by ACL 2025 (main)</p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æŠ€æœ¯å·²åœ¨å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»åå¥½æ–¹é¢å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¯¹é½LLMä¸å¤šæ ·ä¸”å†²çªçš„äººç±»åå¥½ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å°†å…¶æ¡†æ¶åŒ–ä¸ºä¸€ä¸ªå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ä¸€ç³»åˆ—å¯èƒ½å†²çªçš„ç›®æ ‡ã€‚æˆ‘ä»¬å¼•å…¥äº†æ¢¯åº¦è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼ˆGAPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¾®è°ƒæ¨¡å¼ï¼Œé‡‡ç”¨å¤šæ¢¯åº¦ä¸‹é™æ³•å¯¹é½LLMä¸å¤šæ ·çš„åå¥½åˆ†å¸ƒã€‚GAPOè‡ªé€‚åº”åœ°è°ƒæ•´æ¯ä¸ªç›®æ ‡çš„æ¢¯åº¦ï¼Œä»¥ç¡®å®šæœ€ä¼˜çš„æ›´æ–°æ–¹å‘ï¼Œå¹³è¡¡ç›®æ ‡ä¹‹é—´çš„æƒè¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†P-GAPOï¼Œå®ƒç»“åˆäº†ç”¨æˆ·åœ¨ä¸åŒç›®æ ‡ä¸Šçš„åå¥½ï¼Œå®ç°äº†æ›´ç¬¦åˆç”¨æˆ·ç‰¹å®šéœ€æ±‚çš„å¸•ç´¯æ‰˜è§£å†³æ–¹æ¡ˆã€‚ç†è®ºåˆ†æå’Œåœ¨Mistral-7Bä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒGAPOä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œåœ¨æœ‰ç”¨æ€§å’Œæ— å®³æ€§æ–¹é¢éƒ½å–å¾—äº†å“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLHFæŠ€æœ¯å·²æ˜¾ç¤ºå‡ºåœ¨å¯¹é½LLMä¸äººç±»åå¥½æ–¹é¢çš„å®åŠ›ï¼Œä½†å¤„ç†å¤šæ ·ä¸”å†²çªçš„äººç±»åå¥½ä»å…·æŒ‘æˆ˜ã€‚</li>
<li>å¼•å…¥å¤šç›®æ ‡ä¼˜åŒ–æ¡†æ¶æ¥è§£å†³äººç±»ä»·å€¼å¯¹é½é—®é¢˜ï¼Œæ—¨åœ¨æœ€å¤§åŒ–ä¸€ç³»åˆ—å¯èƒ½å†²çªçš„ç›®æ ‡ã€‚</li>
<li>GAPOæ˜¯ä¸€ç§æ–°çš„å¾®è°ƒæ¨¡å¼ï¼Œé‡‡ç”¨å¤šæ¢¯åº¦ä¸‹é™æ³•æ¥å¯¹é½LLMä¸å¤šæ ·çš„åå¥½åˆ†å¸ƒï¼Œè‡ªé€‚åº”åœ°è°ƒæ•´æ¢¯åº¦ä»¥å¹³è¡¡ç›®æ ‡ä¹‹é—´çš„æƒè¡¡ã€‚</li>
<li>P-GAPOç»“åˆç”¨æˆ·åœ¨ä¸åŒç›®æ ‡ä¸Šçš„åå¥½ï¼Œå®ç°æ›´ç¬¦åˆç”¨æˆ·ç‰¹å®šéœ€æ±‚çš„å¸•ç´¯æ‰˜è§£å†³æ–¹æ¡ˆã€‚</li>
<li>GAPOçš„ç†è®ºåˆ†æè¯æ˜äº†å…¶åœ¨å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ä¸­çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>åœ¨Mistral-7Bä¸Šçš„å®è¯ç»“æœè¡¨æ˜ï¼ŒGAPOåœ¨æœ‰ç”¨æ€§å’Œæ— å®³æ€§æ–¹é¢éƒ½ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>GAPOæŠ€æœ¯æœ‰æœ›æ”¹è¿›LLMçš„æ€§èƒ½ï¼Œä½¿å…¶æ›´å¥½åœ°æ»¡è¶³äººç±»çš„éœ€æ±‚å’Œåå¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01915">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8e0174499ff4ad12e2fb869fcbd503e1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7eaa75b3f4051e393b712eeedf35e816.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0a2b97eafe9c83f666885761f922911c.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Reasoning-to-Edit-Hypothetical-Instruction-Based-Image-Editing-with-Visual-Reasoning"><a href="#Reasoning-to-Edit-Hypothetical-Instruction-Based-Image-Editing-with-Visual-Reasoning" class="headerlink" title="Reasoning to Edit: Hypothetical Instruction-Based Image Editing with   Visual Reasoning"></a>Reasoning to Edit: Hypothetical Instruction-Based Image Editing with   Visual Reasoning</h2><p><strong>Authors:Qingdong He, Xueqin Chen, Chaoyi Wang, Yanjie Pan, Xiaobin Hu, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xiangtai Li, Jiangning Zhang</strong></p>
<p>Instruction-based image editing (IIE) has advanced rapidly with the success of diffusion models. However, existing efforts primarily focus on simple and explicit instructions to execute editing operations such as adding, deleting, moving, or swapping objects. They struggle to handle more complex implicit hypothetical instructions that require deeper reasoning to infer plausible visual changes and user intent. Additionally, current datasets provide limited support for training and evaluating reasoning-aware editing capabilities. Architecturally, these methods also lack mechanisms for fine-grained detail extraction that support such reasoning. To address these limitations, we propose Reason50K, a large-scale dataset specifically curated for training and evaluating hypothetical instruction reasoning image editing, along with ReasonBrain, a novel framework designed to reason over and execute implicit hypothetical instructions across diverse scenarios. Reason50K includes over 50K samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs) for editing guidance generation and a diffusion model for image synthesis, incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture detailed visual and textual semantics essential for supporting instruction reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal Enhancer (CME) that enables rich interactions between the fine-grained cues and MLLM-derived features. Extensive experiments demonstrate that ReasonBrain consistently outperforms state-of-the-art baselines on reasoning scenarios while exhibiting strong zero-shot generalization to conventional IIE tasks. Our dataset and code will be released publicly. </p>
<blockquote>
<p>åŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ï¼ˆIIEï¼‰éšç€æ‰©æ•£æ¨¡å‹çš„æˆåŠŸè€Œè¿…é€Ÿå‘å±•ã€‚ç„¶è€Œï¼Œç°æœ‰çš„åŠªåŠ›ä¸»è¦é›†ä¸­åœ¨æ‰§è¡Œç®€å•çš„æ˜ç¡®æŒ‡ä»¤æ¥è¿›è¡Œç¼–è¾‘æ“ä½œï¼Œå¦‚æ·»åŠ ã€åˆ é™¤ã€ç§»åŠ¨æˆ–äº¤æ¢å¯¹è±¡ã€‚ä»–ä»¬éš¾ä»¥å¤„ç†æ›´å¤æ‚çš„éšå«å‡è®¾æŒ‡ä»¤ï¼Œè¿™äº›æŒ‡ä»¤éœ€è¦æ›´æ·±å…¥çš„æ¨ç†æ¥æ¨æ–­åˆç†çš„è§†è§‰å˜åŒ–å’Œç”¨æˆ·æ„å›¾ã€‚æ­¤å¤–ï¼Œå½“å‰çš„æ•°æ®é›†åœ¨æ”¯æŒè®­ç»ƒå’Œè¯„ä¼°æ¨ç†æ„ŸçŸ¥ç¼–è¾‘èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä»æ¶æ„ä¸Šçœ‹ï¼Œè¿™äº›æ–¹æ³•ä¹Ÿç¼ºä¹æ”¯æŒæ­¤ç±»æ¨ç†çš„ç²¾ç»†ç»†èŠ‚æå–æœºåˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Reason50Kï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè®­ç»ƒå’Œè¯„ä¼°å‡è®¾æŒ‡ä»¤æ¨ç†å›¾åƒç¼–è¾‘è€Œç­–åˆ’çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œä»¥åŠReasonBrainï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨åœ¨å¤šç§åœºæ™¯ä¸­æ‰§è¡Œéšå«å‡è®¾æŒ‡ä»¤è¿›è¡Œæ¨ç†ã€‚Reason50KåŒ…å«è¶…è¿‡50Kä¸ªæ ·æœ¬ï¼Œæ¶µç›–å››ç§å…³é”®æ¨ç†åœºæ™¯ï¼šç‰©ç†æ¨ç†ã€æ—¶é—´æ¨ç†ã€å› æœæ¨ç†å’Œæ•…äº‹æ¨ç†ã€‚ReasonBrainåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç”Ÿæˆç¼–è¾‘æŒ‡å—å’Œæ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒåˆæˆï¼Œå¹¶ç»“åˆç²¾ç»†æ¨ç†çº¿ç´¢æå–ï¼ˆFRCEï¼‰æ¨¡å—æ¥æ•è·æ”¯æŒæŒ‡ä»¤æ¨ç†çš„è¯¦ç»†è§†è§‰å’Œæ–‡æœ¬è¯­ä¹‰ã€‚ä¸ºäº†å‡å°‘è¯­ä¹‰æŸå¤±ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†è·¨æ¨¡æ€å¢å¼ºå™¨ï¼ˆCMEï¼‰ï¼Œå®ƒå®ç°äº†ç²¾ç»†çº¿ç´¢å’ŒMLLMæ´¾ç”Ÿç‰¹å¾ä¹‹é—´çš„ä¸°å¯Œäº¤äº’ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒReasonBrainåœ¨æ¨ç†åœºæ™¯ä¸Šå§‹ç»ˆä¼˜äºæœ€æ–°åŸºçº¿ï¼ŒåŒæ—¶åœ¨å¸¸è§„IIEä»»åŠ¡ä¸Šå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01908v1">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨æ‰©æ•£æ¨¡å‹çš„æ¨åŠ¨ä¸‹ï¼ŒåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘æŠ€æœ¯è·å¾—äº†é£é€Ÿè¿›æ­¥ï¼Œä½†ç›®å‰ä¸»è¦é›†ä¸­åœ¨å¯¹æ·»åŠ ã€åˆ é™¤ã€ç§»åŠ¨æˆ–æ›¿æ¢ç‰©ä½“ç­‰ç®€å•æ˜ç¡®çš„æ“ä½œæŒ‡ä»¤çš„å¤„ç†ä¸Šã€‚å¯¹äºéœ€è¦æ·±å…¥æ¨ç†æ¥æ¨æ–­å¯èƒ½è§†è§‰å˜åŒ–å’Œç”¨æˆ·æ„å›¾çš„æ›´å¤æ‚çš„éšå«å‡è®¾æŒ‡ä»¤ï¼Œç°æœ‰æŠ€æœ¯é¢ä¸´æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå½“å‰æ•°æ®é›†å¯¹è®­ç»ƒè¯„ä¼°æ¨ç†æ„ŸçŸ¥ç¼–è¾‘èƒ½åŠ›æ”¯æŒæœ‰é™ã€‚é’ˆå¯¹è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Reason50Kæ•°æ®é›†ï¼Œä¸“ä¸ºè®­ç»ƒå’Œè¯„ä¼°å‡è®¾æŒ‡ä»¤æ¨ç†å›¾åƒç¼–è¾‘è€Œè®¾è®¡ï¼ŒåŒ…å«è¶…è¿‡5ä¸‡ä¸ªæ ·æœ¬ï¼Œæ¶µç›–å››ç§å…³é”®æ¨ç†åœºæ™¯ï¼šç‰©ç†æ¨ç†ã€æ—¶é—´æ¨ç†ã€å› æœæ¨ç†å’Œæ•…äº‹æ¨ç†ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºäº†ReasonBrainæ¡†æ¶ï¼Œç”¨äºæ‰§è¡Œå„ç§åœºæ™¯ä¸­çš„éšå«å‡è®¾æŒ‡ä»¤ã€‚ReasonBrainåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç¼–è¾‘æŒ‡å¯¼ï¼Œå¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒåˆæˆã€‚æ­¤å¤–ï¼Œå®ƒè¿˜åŒ…æ‹¬ä¸€ä¸ªç²¾ç»†æ¨ç†çº¿ç´¢æå–æ¨¡å—ï¼Œç”¨äºæ•æ‰è¯¦ç»†çš„è§†è§‰å’Œæ–‡æœ¬è¯­ä¹‰ï¼Œä»¥æ”¯æŒæŒ‡ä»¤æ¨ç†ã€‚ä¸ºäº†å‡è½»è¯­ä¹‰æŸå¤±ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†è·¨æ¨¡æ€å¢å¼ºå™¨ï¼Œä½¿ç²¾ç»†çº¿ç´¢ä¸MLLMæ´¾ç”Ÿç‰¹å¾ä¹‹é—´èƒ½å¤Ÿè¿›è¡Œä¸°å¯Œçš„äº¤äº’ã€‚å®éªŒè¡¨æ˜ï¼ŒReasonBrainåœ¨æ¨ç†åœºæ™¯ä¸Šä¸€è‡´ä¼˜äºæœ€æ–°æŠ€æœ¯åŸºçº¿ï¼Œå¹¶åœ¨å¸¸è§„IIEä»»åŠ¡ä¸Šå±•ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å›¾åƒç¼–è¾‘æŠ€æœ¯ä¸»è¦å¤„ç†ç®€å•æ˜ç¡®çš„æ“ä½œæŒ‡ä»¤ï¼Œéš¾ä»¥åº”å¯¹å¤æ‚çš„éšå«å‡è®¾æŒ‡ä»¤ã€‚</li>
<li>ç°æœ‰æ•°æ®é›†å¯¹è®­ç»ƒè¯„ä¼°æ¨ç†æ„ŸçŸ¥ç¼–è¾‘èƒ½åŠ›çš„æ”¯æŒæœ‰é™ã€‚</li>
<li>æ¨å‡ºReason50Kæ•°æ®é›†ï¼Œä¸“ä¸ºè®­ç»ƒå’Œè¯„ä¼°å‡è®¾æŒ‡ä»¤æ¨ç†å›¾åƒç¼–è¾‘ã€‚</li>
<li>ä»‹ç»ReasonBrainæ¡†æ¶ï¼Œç”¨äºæ‰§è¡Œå„ç§åœºæ™¯ä¸­çš„éšå«å‡è®¾æŒ‡ä»¤ã€‚</li>
<li>ReasonBrainåˆ©ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç¼–è¾‘æŒ‡å¯¼å¹¶ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œå›¾åƒåˆæˆã€‚</li>
<li>ReasonBrainåŒ…æ‹¬ç²¾ç»†æ¨ç†çº¿ç´¢æå–æ¨¡å—ä»¥æ•æ‰è¯¦ç»†çš„è§†è§‰å’Œæ–‡æœ¬è¯­ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01908">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02ecc7f203b5dcd1da51e1f4896ead5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03c5afdb1b2171a1d4793ff58685c324.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0a174114b9bab6f90864463b72687a5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ae1aba70778f478c908b0454c5407f2c.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Eka-Eval-A-Comprehensive-Evaluation-Framework-for-Large-Language-Models-in-Indian-Languages"><a href="#Eka-Eval-A-Comprehensive-Evaluation-Framework-for-Large-Language-Models-in-Indian-Languages" class="headerlink" title="Eka-Eval : A Comprehensive Evaluation Framework for Large Language   Models in Indian Languages"></a>Eka-Eval : A Comprehensive Evaluation Framework for Large Language   Models in Indian Languages</h2><p><strong>Authors:Samridhi Raj Sinha, Rajvee Sheth, Abhishek Upperwal, Mayank Singh</strong></p>
<p>The rapid advancement of Large Language Models (LLMs) has intensified the need for evaluation frameworks that go beyond English centric benchmarks and address the requirements of linguistically diverse regions such as India. We present EKA-EVAL, a unified and production-ready evaluation framework that integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning categories like reasoning, mathematics, tool use, long-context understanding, and reading comprehension. Compared to existing Indian language evaluation tools, EKA-EVAL offers broader benchmark coverage, with built-in support for distributed inference, quantization, and multi-GPU usage. Our systematic comparison positions EKA-EVAL as the first end-to-end, extensible evaluation suite tailored for both global and Indic LLMs, significantly lowering the barrier to multilingual benchmarking. The framework is open-source and publicly available at <a target="_blank" rel="noopener" href="https://github.com/lingo-iitgn/">https://github.com/lingo-iitgn/</a> eka-eval and a part of ongoing EKA initiative (<a target="_blank" rel="noopener" href="https://eka.soket.ai/">https://eka.soket.ai</a>), which aims to scale up to over 100 benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•åŠ å‰§äº†å¯¹è¯„ä¼°æ¡†æ¶çš„éœ€æ±‚ï¼Œè¿™äº›è¯„ä¼°æ¡†æ¶éœ€è¦è¶…è¶Šè‹±è¯­ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶æ»¡è¶³è¯­è¨€å¤šæ ·åœ°åŒºï¼ˆå¦‚å°åº¦ï¼‰çš„è¦æ±‚ã€‚æˆ‘ä»¬æ¨å‡ºäº†EKA-EVALï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€ä¸”é€‚ç”¨äºç”Ÿäº§çš„è¯„ä¼°æ¡†æ¶ï¼Œé›†æˆäº†è¶…è¿‡35ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬10ä¸ªå°åº¦è¯­ç‰¹å®šæ•°æ®é›†ï¼Œæ¶µç›–æ¨ç†ã€æ•°å­¦ã€å·¥å…·ä½¿ç”¨ã€é•¿æ–‡æœ¬ç†è§£å’Œé˜…è¯»ç†è§£ç­‰ç±»åˆ«ã€‚ä¸ç°æœ‰çš„å°åº¦è¯­è¨€è¯„ä¼°å·¥å…·ç›¸æ¯”ï¼ŒEKA-EVALæä¾›äº†æ›´å¹¿æ³›çš„åŸºå‡†æµ‹è¯•è¦†ç›–ï¼Œå†…ç½®æ”¯æŒåˆ†å¸ƒå¼æ¨ç†ã€é‡åŒ–å’Œå¤šGPUä½¿ç”¨ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿæ¯”è¾ƒæ˜¾ç¤ºï¼ŒEKA-EVALæ˜¯ç¬¬ä¸€ä¸ªä¸ºå…¨çƒå’Œå°åº¦è¯­å¤§å‹è¯­è¨€æ¨¡å‹é‡èº«å®šåˆ¶çš„ç«¯åˆ°ç«¯å¯æ‰©å±•è¯„ä¼°å¥—ä»¶ï¼Œæå¤§åœ°é™ä½äº†å¤šè¯­è¨€åŸºå‡†æµ‹è¯•çš„éš¾åº¦ã€‚è¯¥æ¡†æ¶æ˜¯å¼€æºçš„ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lingo-iitgn/eka-eval%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%EF%BC%8C%E5%B9%B6%E4%BD%9C%E4%B8%BA%E6%AD%A3%E5%9C%A8%E8%BF%9B%E8%A1%8C%E7%9A%84EKA%E5%80%A1%E8%AE%AE%EF%BC%88https://eka.soket.ai%EF%BC%89%E7%9A%84%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%8C%E8%AF%A5%E5%80%A1%E8%AE%AE%E6%97%A8%E5%9C%A8%E6%89%A9%E5%B1%95%E5%88%B0%E8%B6%85%E8%BF%87100%E4%B8%AA%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%EF%BC%8C%E4%B8%BA%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B%E7%A8%B3%E5%81%A5%E7%9A%84%E5%A4%9A%E8%AF%AD%E8%A8%80%E8%AF%84%E4%BC%B0%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E3%80%82">https://github.com/lingo-iitgn/eka-evalå…¬å¼€è®¿é—®ï¼Œå¹¶ä½œä¸ºæ­£åœ¨è¿›è¡Œçš„EKAå€¡è®®ï¼ˆhttps://eka.soket.aiï¼‰çš„ä¸€éƒ¨åˆ†ï¼Œè¯¥å€¡è®®æ—¨åœ¨æ‰©å±•åˆ°è¶…è¿‡100ä¸ªåŸºå‡†æµ‹è¯•ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹å»ºç«‹ç¨³å¥çš„å¤šè¯­è¨€è¯„ä¼°ç”Ÿæ€ç³»ç»Ÿã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01853v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¿«é€Ÿå‘å±•åŠ å‰§äº†å¯¹è¯„ä¼°æ¡†æ¶çš„éœ€æ±‚ï¼Œè¿™äº›è¯„ä¼°æ¡†æ¶éœ€è¦è¶…è¶Šè‹±è¯­ä¸ºä¸­å¿ƒçš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶æ»¡è¶³è¯­è¨€å¤šæ ·åœ°åŒºçš„éœ€è¦ï¼Œå¦‚å°åº¦ã€‚æœ¬æ–‡ä»‹ç»äº†EKA-EVALï¼Œä¸€ä¸ªç»Ÿä¸€ä¸”é€‚ç”¨äºç”Ÿäº§çš„è¯„ä¼°æ¡†æ¶ï¼Œé›†æˆäº†è¶…è¿‡35ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬10ä¸ªå°åº¦è¯­ç‰¹å®šæ•°æ®é›†ï¼Œæ¶µç›–æ¨ç†ã€æ•°å­¦ã€å·¥å…·ä½¿ç”¨ã€é•¿æ–‡æœ¬ç†è§£å’Œé˜…è¯»ç†è§£èƒ½åŠ›ç­‰ç±»åˆ«ã€‚ä¸ç°æœ‰çš„å°åº¦è¯­è¨€è¯„ä¼°å·¥å…·ç›¸æ¯”ï¼ŒEKA-EVALå…·æœ‰æ›´å¹¿æ³›çš„åŸºå‡†æµ‹è¯•è¦†ç›–åº¦ï¼Œå¹¶å†…ç½®æ”¯æŒåˆ†å¸ƒå¼æ¨ç†ã€é‡åŒ–å’Œå¤šGPUä½¿ç”¨ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿæ¯”è¾ƒè¡¨æ˜ï¼ŒEKA-EVALæ˜¯ç¬¬ä¸€ä¸ªä¸ºå…¨çƒå’Œå°åº¦è¯­è¨€LLMé‡èº«å®šåˆ¶çš„ç«¯åˆ°ç«¯å¯æ‰©å±•è¯„ä¼°å¥—ä»¶ï¼Œæ˜¾è‘—é™ä½å¤šè¯­è¨€åŸºå‡†æµ‹è¯•çš„é—¨æ§›ã€‚è¯¥æ¡†æ¶æ˜¯å¼€æºçš„ï¼Œå¯åœ¨å…¬å¼€è®¿é—®çš„<a target="_blank" rel="noopener" href="https://github.com/lingo-iitgn/eka-eval%E6%89%BE%E5%88%B0%EF%BC%8C%E4%B9%9F%E6%98%AF%E6%AD%A3%E5%9C%A8%E8%BF%9B%E8%A1%8C%E4%B8%AD%E7%9A%84EKA%E8%AE%A1%E5%88%92%E7%9A%84%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%88https://eka.soket.ai%EF%BC%89%EF%BC%8C%E6%97%A8%E5%9C%A8%E6%89%A9%E5%B1%95%E5%88%B0%E8%B6%85%E8%BF%87100%E4%B8%AA%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%EF%BC%8C%E4%B8%BALLMs%E5%BB%BA%E7%AB%8B%E4%B8%80%E4%B8%AA%E7%A8%B3%E5%81%A5%E7%9A%84%E5%A4%9A%E8%AF%AD%E8%A8%80%E8%AF%84%E4%BC%B0%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E3%80%82">https://github.com/lingo-iitgn/eka-evalæ‰¾åˆ°ï¼Œä¹Ÿæ˜¯æ­£åœ¨è¿›è¡Œä¸­çš„EKAè®¡åˆ’çš„ä¸€éƒ¨åˆ†ï¼ˆhttps://eka.soket.aiï¼‰ï¼Œæ—¨åœ¨æ‰©å±•åˆ°è¶…è¿‡100ä¸ªåŸºå‡†æµ‹è¯•ï¼Œä¸ºLLMså»ºç«‹ä¸€ä¸ªç¨³å¥çš„å¤šè¯­è¨€è¯„ä¼°ç”Ÿæ€ç³»ç»Ÿã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EKA-EVALæ˜¯ä¸€ä¸ªç»Ÿä¸€ä¸”é€‚ç”¨äºç”Ÿäº§çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½ã€‚</li>
<li>å®ƒé›†æˆäº†è¶…è¿‡35ä¸ªåŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬é’ˆå¯¹å°åº¦è¯­è¨€çš„ç‰¹å®šæ•°æ®é›†ã€‚</li>
<li>EKA-EVALæ”¯æŒåˆ†å¸ƒå¼æ¨ç†ã€é‡åŒ–å’Œå¤šGPUä½¿ç”¨ã€‚</li>
<li>ä¸å…¶ä»–å°åº¦è¯­è¨€è¯„ä¼°å·¥å…·ç›¸æ¯”ï¼ŒEKA-EVALå…·æœ‰æ›´å¹¿æ³›çš„åŸºå‡†æµ‹è¯•è¦†ç›–åº¦ã€‚</li>
<li>EKA-EVALæ˜¯é¦–ä¸ªä¸ºå…¨ä¸­å›½å’Œå°åº¦è¯­è¨€LLMé‡èº«å®šåˆ¶çš„ç«¯åˆ°ç«¯å¯æ‰©å±•è¯„ä¼°å¥—ä»¶ã€‚</li>
<li>è¯¥æ¡†æ¶é™ä½äº†å¤šè¯­è¨€åŸºå‡†æµ‹è¯•çš„é—¨æ§›ï¼Œä¸ºLLMsçš„å¤šè¯­è¨€è¯„ä¼°æä¾›äº†ä¾¿åˆ©ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01853">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-3784f5b1902f4072b4971966a995518f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-489befd98e4fcaffbf297e8be45d1586.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-febcdffe3891c682ce65068345b49d93.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="AdamMeme-Adaptively-Probe-the-Reasoning-Capacity-of-Multimodal-Large-Language-Models-on-Harmfulness"><a href="#AdamMeme-Adaptively-Probe-the-Reasoning-Capacity-of-Multimodal-Large-Language-Models-on-Harmfulness" class="headerlink" title="AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large   Language Models on Harmfulness"></a>AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large   Language Models on Harmfulness</h2><p><strong>Authors:Zixin Chen, Hongzhan Lin, Kaixin Li, Ziyang Luo, Zhen Ye, Guang Chen, Zhiyong Huang, Jing Ma</strong></p>
<p>The proliferation of multimodal memes in the social media era demands that multimodal Large Language Models (mLLMs) effectively understand meme harmfulness. Existing benchmarks for assessing mLLMs on harmful meme understanding rely on accuracy-based, model-agnostic evaluations using static datasets. These benchmarks are limited in their ability to provide up-to-date and thorough assessments, as online memes evolve dynamically. To address this, we propose AdamMeme, a flexible, agent-based evaluation framework that adaptively probes the reasoning capabilities of mLLMs in deciphering meme harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive evaluations by iteratively updating the meme data with challenging samples, thereby exposing specific limitations in how mLLMs interpret harmfulness. Extensive experiments show that our framework systematically reveals the varying performance of different target mLLMs, offering in-depth, fine-grained analyses of model-specific weaknesses. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Lbotirx/AdamMeme">https://github.com/Lbotirx/AdamMeme</a>. </p>
<blockquote>
<p>åœ¨ç¤¾äº¤åª’ä½“æ—¶ä»£ï¼Œå¤šæ¨¡å¼ç½‘ç»œè¿·å› çš„æ¿€å¢è¦æ±‚å¤šæ¨¡å¼è¯­è¨€æ¨¡å‹ï¼ˆmLLMsï¼‰æœ‰æ•ˆåœ°ç†è§£è¿·å› çš„æœ‰å®³æ€§ã€‚è¯„ä¼°mLLMå¯¹æœ‰å®³è¿·å› ç†è§£èƒ½åŠ›çš„ç°æœ‰åŸºå‡†ä¾èµ–äºåŸºäºå‡†ç¡®æ€§çš„ã€æ¨¡å‹ä¸å¯çŸ¥çš„é™æ€æ•°æ®é›†è¯„ä¼°ã€‚è¿™äº›åŸºå‡†å—é™äºæ— æ³•æä¾›æœ€æ–°å’Œå…¨é¢çš„è¯„ä¼°ï¼Œå› ä¸ºåœ¨çº¿è¿·å› ä¼šåŠ¨æ€å‘å±•ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AdamMemeï¼Œè¿™æ˜¯ä¸€ä¸ªçµæ´»çš„åŸºäºä»£ç†çš„è¯„ä¼°æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°æ£€æµ‹mLLMåœ¨è§£é‡Šè¿·å› æœ‰å®³æ€§æ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¤šä»£ç†åä½œï¼ŒAdamMemeé€šè¿‡ä¸æ–­è¿­ä»£æ›´æ–°è¿·å› æ•°æ®ï¼Œå¹¶æä¾›å…·æœ‰æŒ‘æˆ˜æ€§çš„æ ·æœ¬æ¥è¿›è¡Œå…¨é¢è¯„ä¼°ï¼Œä»è€Œæ­ç¤ºmLLMåœ¨è§£é‡Šæœ‰å®³æ€§æ–¹é¢çš„ç‰¹å®šå±€é™æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ç³»ç»Ÿåœ°æ­ç¤ºäº†ä¸åŒç›®æ ‡mLLMçš„ä¸åŒè¡¨ç°ï¼Œå¯¹æ¨¡å‹ç‰¹å®šçš„å¼±ç‚¹è¿›è¡Œäº†æ·±å…¥ç»†è‡´çš„åˆ†æã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/Lbotirx/AdamMeme%E3%80%82">https://github.com/Lbotirx/AdamMemeã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01702v1">PDF</a> ACL 2025</p>
<p><strong>Summary</strong></p>
<p>å¤šåª’ä½“æ¨¡å› åœ¨ç¤¾ä¼šåª’ä½“æ—¶ä»£çš„æ™®åŠï¼Œè¦æ±‚å¤šåª’ä½“è¯­è¨€æ¨¡å‹ï¼ˆmLLMsï¼‰æœ‰æ•ˆåœ°ç†è§£æ¨¡å› çš„å±å®³æ€§ã€‚ç°æœ‰çš„è¯„ä¼°mLLMåœ¨ç†è§£æœ‰å®³æ¨¡å› æ–¹é¢çš„èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ä¸»è¦ä¾èµ–äºåŸºäºå‡†ç¡®åº¦çš„é™æ€æ•°æ®é›†æ¨¡å‹æ— å…³è¯„ä¼°ã€‚ç„¶è€Œï¼Œç”±äºåœ¨çº¿æ¨¡å› çš„åŠ¨æ€æ¼”å˜ï¼Œè¿™äº›åŸºå‡†æµ‹è¯•åœ¨æä¾›æœ€æ–°å’Œå…¨é¢çš„è¯„ä¼°æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†AdamMemeï¼Œè¿™æ˜¯ä¸€ä¸ªçµæ´»çš„åŸºäºä»£ç†çš„è¯„ä»·æ¡†æ¶ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°æ£€æµ‹mLLMåœ¨è§£é‡Šæ¨¡å› å±å®³æ€§æ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚é€šè¿‡å¤šæ™ºèƒ½ä½“åä½œï¼ŒAdamMemeé€šè¿‡ä¸æ–­è¿­ä»£æ›´æ–°æ¨¡å› æ•°æ®æ¥æä¾›å…¨é¢çš„è¯„ä¼°ï¼Œä»è€Œæ­ç¤ºmLLMåœ¨è§£é‡Šå±å®³æ€§çš„å…·ä½“å±€é™æ€§ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Lbotirx/AdamMeme%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Lbotirx/AdamMemeè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šåª’ä½“æ¨¡å› åœ¨ç¤¾ä¼šåª’ä½“æ—¶ä»£å¹¿æ³›ä¼ æ’­ï¼Œå¯¹mLLMç†è§£æ¨¡å› å±å®³æ€§çš„èƒ½åŠ›æå‡ºè¦æ±‚ã€‚</li>
<li>ç°è¡Œçš„åŸºå‡†æµ‹è¯•æ— æ³•å…¨é¢è¯„ä¼°mLLMåœ¨ç†è§£æœ‰å®³æ¨¡å› æ–¹é¢çš„èƒ½åŠ›ï¼Œå› ä¸ºå®ƒä»¬ä¸»è¦ä¾èµ–é™æ€æ•°æ®é›†å’Œå‡†ç¡®åº¦è¯„ä¼°ã€‚</li>
<li>AdamMemeæ˜¯ä¸€ä¸ªçµæ´»çš„åŸºäºä»£ç†çš„è¯„ä»·æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†æµ‹è¯•çš„å±€é™æ€§ã€‚</li>
<li>AdamMemeé€šè¿‡å¤šæ™ºèƒ½ä½“åä½œï¼Œèƒ½å¤Ÿè‡ªé€‚åº”åœ°æ£€æµ‹mLLMåœ¨è§£é‡Šæ¨¡å› å±å®³æ€§æ–¹é¢çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>AdamMemeé€šè¿‡ä¸æ–­è¿­ä»£æ›´æ–°æ¨¡å› æ•°æ®ï¼Œä»¥æä¾›æ›´å…¨é¢çš„è¯„ä¼°ç»“æœã€‚</li>
<li>è¯¥æ¡†æ¶æ­ç¤ºäº†mLLMåœ¨è§£é‡Šæ¨¡å› å±å®³æ€§çš„å…·ä½“å±€é™æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01702">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7ebe459dc6c4a558a7412b2aeeb6ed67.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e074d9cc16e21ecc99ba7c400ad95eaf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ce04f4d77bd3aa51d6c2e26128e6159a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6bdec0ed7104dc247c1dadc5d20e40c4.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Blending-Supervised-and-Reinforcement-Fine-Tuning-with-Prefix-Sampling"><a href="#Blending-Supervised-and-Reinforcement-Fine-Tuning-with-Prefix-Sampling" class="headerlink" title="Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling"></a>Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling</h2><p><strong>Authors:Zeyu Huang, Tianhao Cheng, Zihan Qiu, Zili Wang, Yinghui Xu, Edoardo M. Ponti, Ivan Titov</strong></p>
<p>Existing post-training techniques for large language models are broadly categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking demonstration data but can lead to problematic generalization as a form of behavior cloning. Conversely, RFT can significantly enhance a modelâ€™s performance but is prone to learn unexpected behaviors, and its performance is highly sensitive to the initial policy. In this paper, we propose a unified view of these methods and introduce Prefix-RFT, a hybrid approach that synergizes learning from both demonstration and exploration. Using mathematical reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is both simple and effective. It not only surpasses the performance of standalone SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key advantage is its seamless integration into existing open-source frameworks, requiring only minimal modifications to the standard RFT pipeline. Our analysis highlights the complementary nature of SFT and RFT, and validates that Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore, ablation studies confirm the methodâ€™s robustness to variations in the quality and quantity of demonstration data. We hope this work offers a new perspective on LLM post-training, suggesting that a unified paradigm that judiciously integrates demonstration and exploration could be a promising direction for future research. </p>
<blockquote>
<p>ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒåæŠ€æœ¯ä¸»è¦åˆ†ä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ä¸¤å¤§ç±»ã€‚æ¯ç§èŒƒå¼éƒ½æœ‰å…¶ç‹¬ç‰¹çš„æƒè¡¡ï¼šSFTæ“…é•¿æ¨¡ä»¿æ¼”ç¤ºæ•°æ®ï¼Œä½†å¯èƒ½ä¼šå¯¼è‡´è¡Œä¸ºå…‹éš†çš„ä¸€èˆ¬åŒ–é—®é¢˜ã€‚ç›¸åï¼ŒRFTå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†å®¹æ˜“å­¦ä¹ æ„å¤–è¡Œä¸ºï¼Œå¹¶ä¸”å…¶æ€§èƒ½å¯¹åˆå§‹ç­–ç•¥é«˜åº¦æ•æ„Ÿã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹è¿™äº›æ–¹æ³•æå‡ºäº†ç»Ÿä¸€è§‚ç‚¹ï¼Œå¹¶ä»‹ç»äº†Prefix-RFTï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†æ¼”ç¤ºå’Œæ¢ç©¶å­¦ä¹ çš„æ··åˆæ–¹æ³•ã€‚æˆ‘ä»¬ä»¥æ•°å­¦æ¨ç†é—®é¢˜ä¸ºæµ‹è¯•å¹³å°ï¼Œå®è¯è¡¨æ˜Prefix-RFTç®€å•æœ‰æ•ˆã€‚å®ƒä¸ä»…è¶…è¶Šäº†ç‹¬ç«‹çš„SFTå’ŒRFTçš„æ€§èƒ½ï¼Œè€Œä¸”ä¼˜äºå¹¶è¡Œæ··åˆç­–ç•¥çš„RFTæ–¹æ³•ã€‚ä¸€ä¸ªå…³é”®ä¼˜åŠ¿æ˜¯å®ƒèƒ½æ— ç¼é›†æˆåˆ°ç°æœ‰çš„å¼€æºæ¡†æ¶ä¸­ï¼Œåªéœ€å¯¹æ ‡å‡†RFTç®¡é“è¿›è¡Œæœ€å°ä¿®æ”¹ã€‚æˆ‘ä»¬çš„åˆ†æå¼ºè°ƒäº†SFTå’ŒRFTçš„äº’è¡¥æ€§ï¼Œå¹¶éªŒè¯äº†Prefix-RFTæœ‰æ•ˆåœ°åè°ƒäº†è¿™ä¸¤ç§å­¦ä¹ èŒƒå¼ã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶è¯å®äº†è¯¥æ–¹æ³•å¯¹æ¼”ç¤ºæ•°æ®çš„è´¨é‡å’Œæ•°é‡çš„å˜åŒ–çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œä¸ºLLMçš„åæœŸè®­ç»ƒæä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶è¡¨æ˜ç»Ÿä¸€èŒƒå¼ï¼Œå³å®¡æ…åœ°æ•´åˆæ¼”ç¤ºå’Œæ¢ç©¶å¯èƒ½æ˜¯æœªæ¥ç ”ç©¶çš„æœ‰å‰é€”çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01679v1">PDF</a> Work in progress</p>
<p><strong>Summary</strong><br>å¤§è¯­è¨€æ¨¡å‹çš„ç°æœ‰åè®­ç»ƒæŠ€æœ¯ä¸»è¦åˆ†ä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ä¸¤ç§ã€‚ä¸¤è€…å„æœ‰ä¼˜ç¼ºç‚¹ï¼šSFTæ“…é•¿æ¨¡ä»¿æ¼”ç¤ºæ•°æ®ä½†å¯èƒ½å¯¼è‡´æ³›åŒ–é—®é¢˜ï¼Œè€ŒRFTèƒ½æé«˜æ¨¡å‹æ€§èƒ½ä½†æ˜“å­¦åˆ°æ„å¤–è¡Œä¸ºï¼Œä¸”æ€§èƒ½é«˜åº¦ä¾èµ–åˆå§‹ç­–ç•¥ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§ç»Ÿä¸€è§†è§’ï¼Œå¹¶ä»‹ç»äº†Prefix-RFTè¿™ä¸€æ··åˆæ–¹æ³•ï¼Œå®ƒç»“åˆäº†æ¼”ç¤ºå­¦ä¹ å’Œæ¢ç´¢å­¦ä¹ çš„ä¼˜ç‚¹ã€‚å®éªŒè¯æ˜ï¼ŒPrefix-RFTä¸ä»…è¶…è¶Šäº†å•ç‹¬çš„SFTå’ŒRFTçš„æ€§èƒ½ï¼Œè€Œä¸”ä¼˜äºå¹¶è¡Œæ··åˆç­–ç•¥RFTæ–¹æ³•ã€‚å…¶å…³é”®ä¼˜åŠ¿æ˜¯è½»æ¾é›†æˆåˆ°ç°æœ‰å¼€æºæ¡†æ¶ä¸­ï¼Œåªéœ€å¯¹æ ‡å‡†RFTç®¡é“è¿›è¡Œæœ€å°ä¿®æ”¹ã€‚æœ¬æ–‡åˆ†æäº†SFTå’ŒRFTçš„äº’è¡¥æ€§ï¼ŒéªŒè¯äº†Prefix-RFTèƒ½æœ‰æ•ˆåè°ƒè¿™ä¸¤ç§å­¦ä¹ èŒƒå¼ã€‚æ­¤å¤–ï¼Œæ¶ˆèç ”ç©¶è¯å®äº†è¯¥æ–¹æ³•å¯¹æ¼”ç¤ºæ•°æ®è´¨é‡å’Œæ•°é‡çš„å˜åŒ–å…·æœ‰ç¨³å¥æ€§ã€‚æœ¬ç ”ç©¶ä¸ºLLMåè®­ç»ƒæä¾›äº†æ–°çš„è§†è§’ï¼Œå»ºè®®ç»Ÿä¸€èŒƒå¼ï¼Œé€‚å½“é›†æˆæ¼”ç¤ºå’Œæ¢ç´¢ï¼Œå¯èƒ½æˆä¸ºæœªæ¥ç ”ç©¶çš„æœ‰å¸Œæœ›çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒæŠ€æœ¯åˆ†ä¸ºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œå¼ºåŒ–å¾®è°ƒï¼ˆRFTï¼‰ã€‚</li>
<li>SFTæ“…é•¿æ¨¡ä»¿æ¼”ç¤ºæ•°æ®ä½†å¯èƒ½æ³›åŒ–é—®é¢˜ï¼ŒRFTèƒ½æé«˜æ€§èƒ½ä½†æ˜“å­¦åˆ°æ„å¤–è¡Œä¸ºã€‚</li>
<li>Prefix-RFTæ˜¯ä¸€ç§æ··åˆæ–¹æ³•ï¼Œç»“åˆäº†æ¼”ç¤ºå­¦ä¹ å’Œæ¢ç´¢å­¦ä¹ çš„ä¼˜ç‚¹ã€‚</li>
<li>Prefix-RFTè¶…è¶Šäº†SFTå’ŒRFTçš„æ€§èƒ½ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰å¼€æºæ¡†æ¶ä¸­ã€‚</li>
<li>Prefix-RFTèƒ½æœ‰æ•ˆåè°ƒSFTå’ŒRFTçš„äº’è¡¥æ€§ã€‚</li>
<li>Prefix-RFTå¯¹æ¼”ç¤ºæ•°æ®çš„è´¨é‡å’Œæ•°é‡å˜åŒ–å…·æœ‰ç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7f572f83f2de028a3c9bfee24769792c.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a1ec6b4e1bdbbafc25750d398c4ab55e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-fdefb9f6fb221ff46751482f8cb962f7.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="AsyncFlow-An-Asynchronous-Streaming-RL-Framework-for-Efficient-LLM-Post-Training"><a href="#AsyncFlow-An-Asynchronous-Streaming-RL-Framework-for-Efficient-LLM-Post-Training" class="headerlink" title="AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM   Post-Training"></a>AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM   Post-Training</h2><p><strong>Authors:Zhenyu Han, Ansheng You, Haibo Wang, Kui Luo, Guang Yang, Wenqi Shi, Menglong Chen, Sicheng Zhang, Zeshun Lan, Chunshi Deng, Huazhong Ji, Wenjie Liu, Yu Huang, Yixiang Zhang, Chenyi Pan, Jing Wang, Xin Huang, Chunsheng Li, Jianping Wu</strong></p>
<p>Reinforcement learning (RL) has become a pivotal technology in the post-training phase of large language models (LLMs). Traditional task-colocated RL frameworks suffer from significant scalability bottlenecks, while task-separated RL frameworks face challenges in complex dataflows and the corresponding resource idling and workload imbalance. Moreover, most existing frameworks are tightly coupled with LLM training or inference engines, making it difficult to support custom-designed engines. To address these challenges, we propose AsyncFlow, an asynchronous streaming RL framework for efficient post-training. Specifically, we introduce a distributed data storage and transfer module that provides a unified data management and fine-grained scheduling capability in a fully streamed manner. This architecture inherently facilitates automated pipeline overlapping among RL tasks and dynamic load balancing. Moreover, we propose a producer-consumer-based asynchronous workflow engineered to minimize computational idleness by strategically deferring parameter update process within staleness thresholds. Finally, the core capability of AsynFlow is architecturally decoupled from underlying training and inference engines and encapsulated by service-oriented user interfaces, offering a modular and customizable user experience. Extensive experiments demonstrate an average of 1.59 throughput improvement compared with state-of-the-art baseline. The presented architecture in this work provides actionable insights for next-generation RL training system designs. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒåé˜¶æ®µçš„æ ¸å¿ƒæŠ€æœ¯ã€‚ä¼ ç»Ÿçš„ä»»åŠ¡å…±ç½®RLæ¡†æ¶å­˜åœ¨ä¸¥é‡çš„å¯æ‰©å±•æ€§ç“¶é¢ˆï¼Œè€Œä»»åŠ¡åˆ†ç¦»çš„RLæ¡†æ¶åœ¨å¤æ‚æ•°æ®æµå’Œç›¸åº”çš„èµ„æºç©ºé—²å’Œå·¥ä½œè´Ÿè½½ä¸å¹³è¡¡æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ç°æœ‰æ¡†æ¶ä¸LLMè®­ç»ƒæˆ–æ¨ç†å¼•æ“ç´§å¯†è€¦åˆï¼Œéš¾ä»¥æ”¯æŒè‡ªå®šä¹‰å¼•æ“ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†AsyncFlowï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¼‚æ­¥æµå¼RLæ¡†æ¶ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåˆ†å¸ƒå¼æ•°æ®å­˜å‚¨å’Œä¼ è¾“æ¨¡å—ï¼Œä»¥å®Œå…¨æµå¼çš„æ–¹å¼æä¾›ç»Ÿä¸€çš„æ•°æ®ç®¡ç†å’Œç»†ç²’åº¦è°ƒåº¦èƒ½åŠ›ã€‚è¯¥æ¶æ„å¤©ç„¶åœ°ä¿ƒè¿›äº†RLä»»åŠ¡ä¹‹é—´çš„è‡ªåŠ¨åŒ–ç®¡é“é‡å å’ŒåŠ¨æ€è´Ÿè½½å‡è¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç”Ÿäº§è€…-æ¶ˆè´¹è€…çš„å¼‚æ­¥å·¥ä½œæµç¨‹ï¼Œé€šè¿‡æˆ˜ç•¥æ€§æ¨è¿Ÿå‚æ•°æ›´æ–°è¿‡ç¨‹å¹¶åœ¨é™ˆæ—§åº¦é˜ˆå€¼å†…æ¥æœ€å°åŒ–è®¡ç®—ç©ºé—²æ—¶é—´ã€‚æœ€åï¼ŒAsynFlowçš„æ ¸å¿ƒèƒ½åŠ›ä¸åº•å±‚è®­ç»ƒå’Œæ¨ç†å¼•æ“æ¶æ„ä¸Šè§£è€¦ï¼Œå¹¶é€šè¿‡é¢å‘æœåŠ¡çš„ç”¨æˆ·ç•Œé¢è¿›è¡Œå°è£…ï¼Œæä¾›æ¨¡å—åŒ–å’Œå¯å®šåˆ¶çš„ç”¨æˆ·ä½“éªŒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸æœ€æ–°åŸºçº¿ç›¸æ¯”ï¼ŒAsynFlowçš„ååé‡å¹³å‡æé«˜äº†1.59ã€‚æœ¬æ–‡æå‡ºçš„æ¶æ„ä¸ºä¸‹ä¸€ä»£RLè®­ç»ƒç³»ç»Ÿè®¾è®¡æä¾›äº†å¯æ“ä½œçš„è§è§£ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01663v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºAsyncFlowçš„å¼‚æ­¥æµå¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚é€šè¿‡å¼•å…¥åˆ†å¸ƒå¼æ•°æ®å­˜å‚¨å’Œè½¬ç§»æ¨¡å—ä»¥åŠåŸºäºç”Ÿäº§è€…æ¶ˆè´¹è€…çš„å¼‚æ­¥å·¥ä½œæµç¨‹ï¼Œå®ç°äº†ç²¾ç»†åŒ–çš„ä»»åŠ¡è°ƒåº¦å’Œè´Ÿè½½å‡è¡¡ï¼Œå‡å°‘äº†è®¡ç®—ç©ºé—²æ—¶é—´ï¼Œæé«˜äº†è®­ç»ƒé€Ÿåº¦ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶çš„æ ¸å¿ƒèƒ½åŠ›ä¸åº•å±‚è®­ç»ƒå’Œæ¨ç†å¼•æ“è§£è€¦ï¼Œæä¾›æ¨¡å—åŒ–ã€å¯å®šåˆ¶çš„ç”¨æˆ·ä½“éªŒã€‚å®éªŒè¡¨æ˜ï¼Œç›¸æ¯”ç°æœ‰æŠ€æœ¯ï¼Œè¯¥æ¡†æ¶çš„ååé‡å¹³å‡æé«˜äº†1.59å€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AsyncFlowæ˜¯ä¸€ä¸ªå¼‚æ­¥æµå¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>å¼•å…¥åˆ†å¸ƒå¼æ•°æ®å­˜å‚¨å’Œè½¬ç§»æ¨¡å—ï¼Œå®ç°ç»Ÿä¸€æ•°æ®ç®¡ç†å’Œç²¾ç»†åŒ–çš„ä»»åŠ¡è°ƒåº¦ã€‚</li>
<li>åŸºäºç”Ÿäº§è€…æ¶ˆè´¹è€…çš„å¼‚æ­¥å·¥ä½œæµç¨‹å‡å°‘äº†è®¡ç®—ç©ºé—²æ—¶é—´ã€‚</li>
<li>æ¡†æ¶å®ç°äº†åŠ¨æ€è´Ÿè½½å‡è¡¡å’Œè‡ªåŠ¨åŒ–ç®¡é“é‡å ï¼Œæé«˜äº†è®­ç»ƒé€Ÿåº¦ã€‚</li>
<li>AsyncFlowçš„æ ¸å¿ƒèƒ½åŠ›ä¸åº•å±‚è®­ç»ƒå’Œæ¨ç†å¼•æ“è§£è€¦ï¼Œæä¾›æ¨¡å—åŒ–ã€å¯å®šåˆ¶çš„ç”¨æˆ·ä½“éªŒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸æ¯”ç°æœ‰æŠ€æœ¯ï¼ŒAsyncFlowçš„ååé‡æœ‰æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01663">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2d9398b244a2415f23e607f0f132c7e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-13671cf0a9a3e5e98ecdd7fef40c4f61.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c1c011e82c75e28e67cd34ce3217598.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2d37ec457a5218b478054344f885c441.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d9b6399ce4b732f327a05dcefe6ad60a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abf694ac2773f4bff38901eb578b0df5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7b6816704a0bc6a9f664d5ea956d00ff.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="T3DM-Test-Time-Training-Guided-Distribution-Shift-Modelling-for-Temporal-Knowledge-Graph-Reasoning"><a href="#T3DM-Test-Time-Training-Guided-Distribution-Shift-Modelling-for-Temporal-Knowledge-Graph-Reasoning" class="headerlink" title="T3DM: Test-Time Training-Guided Distribution Shift Modelling for   Temporal Knowledge Graph Reasoning"></a>T3DM: Test-Time Training-Guided Distribution Shift Modelling for   Temporal Knowledge Graph Reasoning</h2><p><strong>Authors:Yuehang Si, Zefan Zeng, Jincai Huang, Qing Cheng</strong></p>
<p>Temporal Knowledge Graph (TKG) is an efficient method for describing the dynamic development of facts along a timeline. Most research on TKG reasoning (TKGR) focuses on modelling the repetition of global facts and designing patterns of local historical facts. However, they face two significant challenges: inadequate modeling of the event distribution shift between training and test samples, and reliance on random entity substitution for generating negative samples, which often results in low-quality sampling. To this end, we propose a novel distributional feature modeling approach for training TKGR models, Test-Time Training-guided Distribution shift Modelling (T3DM), to adjust the model based on distribution shift and ensure the global consistency of model reasoning. In addition, we design a negative-sampling strategy to generate higher-quality negative quadruples based on adversarial training. Extensive experiments show that T3DM provides better and more robust results than the state-of-the-art baselines in most cases. </p>
<blockquote>
<p>æ—¶åºçŸ¥è¯†å›¾è°±ï¼ˆTKGï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ï¼Œç”¨äºæè¿°äº‹å®æ²¿æ—¶é—´çº¿çš„åŠ¨æ€å‘å±•ã€‚å…³äºTKGæ¨ç†ï¼ˆTKGRï¼‰çš„ç ”ç©¶å¤§å¤šé›†ä¸­åœ¨å…¨çƒäº‹å®é‡å¤å»ºæ¨¡å’Œå±€éƒ¨å†å²äº‹å®è®¾è®¡æ¨¡å¼ä¸Šã€‚ç„¶è€Œï¼Œä»–ä»¬é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šè®­ç»ƒæ ·æœ¬å’Œæµ‹è¯•æ ·æœ¬ä¹‹é—´äº‹ä»¶åˆ†å¸ƒå˜åŒ–çš„å»ºæ¨¡ä¸è¶³ï¼Œä»¥åŠä¾èµ–éšæœºå®ä½“æ›¿æ¢ç”Ÿæˆè´Ÿæ ·æœ¬ï¼Œè¿™å¾€å¾€å¯¼è‡´é‡‡æ ·è´¨é‡ä½ä¸‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„åˆ†å¸ƒç‰¹å¾å»ºæ¨¡æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒTKGRæ¨¡å‹ï¼Œå³æµ‹è¯•æ—¶è®­ç»ƒå¼•å¯¼çš„åˆ†å¸ƒè½¬ç§»å»ºæ¨¡ï¼ˆT3DMï¼‰ï¼Œä»¥æ ¹æ®åˆ†å¸ƒå˜åŒ–è°ƒæ•´æ¨¡å‹ï¼Œå¹¶ç¡®ä¿æ¨¡å‹æ¨ç†çš„å…¨å±€ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºå¯¹æŠ—è®­ç»ƒçš„è´Ÿé‡‡æ ·ç­–ç•¥ï¼Œç”Ÿæˆæ›´é«˜è´¨é‡çš„è´Ÿå››å…ƒç»„ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒT3DMæä¾›çš„ç»“æœæ¯”ä»¥å¾€æœ€å…ˆè¿›çš„åŸºçº¿æ›´å¥½ã€æ›´ç¨³å¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01597v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æ—¶é—´çŸ¥è¯†å›¾è°±ï¼ˆTKGï¼‰æ˜¯ç”¨äºæè¿°äº‹å®æ²¿æ—¶é—´çº¿çš„åŠ¨æ€å‘å±•çš„æœ‰æ•ˆæ–¹æ³•ã€‚æœ¬æ–‡æå‡ºäº†æ–°å‹çš„åˆ†å‘ç‰¹å¾å»ºæ¨¡æ–¹æ³•ä»¥è®­ç»ƒTKGRæ¨¡å‹ï¼Œæµ‹è¯•æ—¶è®­ç»ƒå¼•å¯¼çš„åˆ†å¸ƒå¼è½¬æ¢æ¨¡å‹ï¼ˆT3DMï¼‰ï¼Œå¯æ ¹æ®åˆ†å¸ƒå˜åŒ–è°ƒæ•´æ¨¡å‹ï¼Œç¡®ä¿æ¨¡å‹æ¨ç†çš„å…¨å±€ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºå¯¹æŠ—è®­ç»ƒçš„è´Ÿé‡‡æ ·ç­–ç•¥ï¼Œç”Ÿæˆæ›´é«˜è´¨é‡çš„è´Ÿå››å…ƒç»„ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒT3DMç›¸è¾ƒäºç°æœ‰åŸºçº¿å…·æœ‰æ›´å¥½çš„æ€§èƒ½å’Œé²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æ—¶é—´çŸ¥è¯†å›¾è°±ï¼ˆTKGï¼‰æ˜¯ç”¨äºæè¿°äº‹å®æ²¿æ—¶é—´çº¿çš„åŠ¨æ€å‘å±•çš„æ–¹æ³•ã€‚</li>
<li>å½“å‰TKGRç ”ç©¶é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šè®­ç»ƒä¸æµ‹è¯•æ ·æœ¬é—´äº‹ä»¶åˆ†å¸ƒå˜åŒ–çš„å»ºæ¨¡ä¸è¶³ä»¥åŠå¯¹éšæœºå®ä½“æ›¿æ¢ç”Ÿæˆè´Ÿæ ·æœ¬çš„ä¾èµ–ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„åˆ†å¸ƒç‰¹å¾å»ºæ¨¡æ–¹æ³•â€”â€”æµ‹è¯•æ—¶è®­ç»ƒå¼•å¯¼çš„åˆ†å¸ƒå¼è½¬æ¢æ¨¡å‹ï¼ˆT3DMï¼‰ï¼Œç”¨äºè°ƒæ•´æ¨¡å‹ä»¥é€‚åº”åˆ†å¸ƒå˜åŒ–å¹¶ä¿éšœæ¨¡å‹æ¨ç†çš„å…¨å±€ä¸€è‡´æ€§ã€‚</li>
<li>è®¾è®¡äº†ä¸€ç§åŸºäºå¯¹æŠ—è®­ç»ƒçš„è´Ÿé‡‡æ ·ç­–ç•¥ï¼Œç”Ÿæˆæ›´é«˜è´¨é‡çš„è´Ÿå››å…ƒç»„ã€‚</li>
<li>T3DMåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æä¾›äº†ç›¸è¾ƒäºç°æœ‰åŸºçº¿çš„æ›´å¥½å’Œæ›´ç¨³å¥çš„ç»“æœã€‚</li>
<li>é€šè¿‡å¹¿æ³›å®éªŒéªŒè¯äº†T3DMçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01597">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-7332cf7453d3892c50493cc06a99e7c6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fc65fbb23fdcac6824156c142fe6473f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b4828223ee754bb6c642e1b79c23aa34.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-604cd9c457c8afe54490926e69b15686.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Self-Guided-Process-Reward-Optimization-with-Masked-Step-Advantage-for-Process-Reinforcement-Learning"><a href="#Self-Guided-Process-Reward-Optimization-with-Masked-Step-Advantage-for-Process-Reinforcement-Learning" class="headerlink" title="Self-Guided Process Reward Optimization with Masked Step Advantage for   Process Reinforcement Learning"></a>Self-Guided Process Reward Optimization with Masked Step Advantage for   Process Reinforcement Learning</h2><p><strong>Authors:Wu Fei, Hao Kong, Shuxian Liang, Yang Lin, Yibo Yang, Jing Tang, Lei Chen, Xiansheng Hua</strong></p>
<p>Process Reinforcement Learning<del>(PRL) has demonstrated considerable potential in enhancing the reasoning capabilities of Large Language Models</del>(LLMs). However, introducing additional process reward models incurs substantial computational overhead, and there is no unified theoretical framework for process-level advantage estimation. To bridge this gap, we propose \textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward \textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables process-aware RL through two key innovations: (1) we first theoretically demonstrate that process rewards can be derived intrinsically from the policy model itself, and (2) we introduce well-defined cumulative process rewards and \textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which facilitates rigorous step-wise action advantage estimation within shared-prompt sampling groups. Our experimental results demonstrate that SPRO outperforms vaniila GRPO with 3.4x higher training efficiency and a 17.5% test accuracy improvement. Furthermore, SPRO maintains a stable and elevated policy entropy throughout training while reducing the average response length by approximately $1&#x2F;3$, evidencing sufficient exploration and prevention of reward hacking. Notably, SPRO incurs no additional computational overhead compared to outcome-supervised RL methods such as GRPO, which benefit industrial implementation. </p>
<blockquote>
<p>è¿‡ç¨‹å¼ºåŒ–å­¦ä¹ ï¼ˆPRLï¼‰åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œå¼•å…¥é¢å¤–çš„æµç¨‹å¥–åŠ±æ¨¡å‹ä¼šäº§ç”Ÿå¤§é‡çš„è®¡ç®—å¼€é”€ï¼Œå¹¶ä¸”æ²¡æœ‰ç»Ÿä¸€çš„ç†è®ºæ¡†æ¶æ¥è¿›è¡Œæµç¨‹çº§åˆ«çš„ä¼˜åŠ¿ä¼°è®¡ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªå¼•å¯¼æµç¨‹å¥–åŠ±ä¼˜åŒ–ï¼ˆSPROï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹å®ç°äº†æµç¨‹æ„ŸçŸ¥çš„RLï¼šï¼ˆ1ï¼‰æˆ‘ä»¬é¦–å…ˆä»ç†è®ºä¸Šè¯æ˜ï¼Œæµç¨‹å¥–åŠ±å¯ä»¥ä»ç­–ç•¥æ¨¡å‹æœ¬èº«å†…åœ¨åœ°æ´¾ç”Ÿå‡ºæ¥ï¼›ï¼ˆ2ï¼‰æˆ‘ä»¬å¼•å…¥äº†å®šä¹‰è‰¯å¥½çš„ç´¯ç§¯æµç¨‹å¥–åŠ±å’Œé®ç½©æ­¥éª¤ä¼˜åŠ¿ï¼ˆMSAï¼‰ï¼Œè¿™æœ‰åŠ©äºåœ¨å…±äº«æç¤ºé‡‡æ ·ç»„å†…è¿›è¡Œä¸¥æ ¼çš„é€æ­¥è¡ŒåŠ¨ä¼˜åŠ¿ä¼°è®¡ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSPROä¼˜äºåŸºçº¿GRPOï¼Œè®­ç»ƒæ•ˆç‡æé«˜äº†3.4å€ï¼Œæµ‹è¯•ç²¾åº¦æé«˜äº†17.5%ã€‚æ­¤å¤–ï¼ŒSPROåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒäº†ç¨³å®šå’Œè¾ƒé«˜çš„ç­–ç•¥ç†µï¼ŒåŒæ—¶å¹³å‡å“åº”é•¿åº¦å‡å°‘äº†çº¦ä¸‰åˆ†ä¹‹ä¸€ï¼Œè¯æ˜äº†å…¶è¶³å¤Ÿçš„æ¢ç´¢èƒ½åŠ›å’Œé˜²æ­¢å¥–åŠ±ä½œå¼Šçš„æ•ˆæœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸ç»“æœç›‘ç£çš„RLæ–¹æ³•ï¼ˆå¦‚GRPOï¼‰ç›¸æ¯”ï¼ŒSPROæ²¡æœ‰é¢å¤–çš„è®¡ç®—å¼€é”€ï¼Œè¿™å¯¹å…¶åœ¨å·¥ä¸šå®æ–½ä¸­çš„åº”ç”¨éå¸¸æœ‰åˆ©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01551v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¼ºåŒ–å­¦ä¹ åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚é’ˆå¯¹è¿‡ç¨‹å¥–åŠ±æ¨¡å‹å¼•å…¥çš„è®¡ç®—å¼€é”€è¾ƒå¤§åŠç¼ºä¹ç»Ÿä¸€ç†è®ºæ¡†æ¶çš„é—®é¢˜ï¼Œæå‡ºäº†è‡ªæŒ‡å¯¼è¿‡ç¨‹å¥–åŠ±ä¼˜åŒ–ï¼ˆSPROï¼‰æ¡†æ¶ï¼Œé€šè¿‡ä»æ”¿ç­–æ¨¡å‹æœ¬èº«å†…åœ¨æ¨å¯¼è¿‡ç¨‹å¥–åŠ±å’Œå¼•å…¥ç´¯ç§¯è¿‡ç¨‹å¥–åŠ±å’Œé®ç½©æ­¥éª¤ä¼˜åŠ¿ï¼Œå®ç°äº†è¿‡ç¨‹æ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSPROåœ¨è®­ç»ƒæ•ˆç‡ã€æµ‹è¯•ç²¾åº¦ã€ç­–ç•¥ç†µä»¥åŠå“åº”é•¿åº¦æ§åˆ¶ç­‰æ–¹é¢å‡æœ‰æ˜¾è‘—æå‡ï¼Œä¸”æœªå¢åŠ é¢å¤–è®¡ç®—å¼€é”€ï¼Œé€‚åˆå·¥ä¸šåŒ–å®æ–½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰èƒ½æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>SPROæ¡†æ¶è§£å†³äº†è¿‡ç¨‹å¥–åŠ±æ¨¡å‹å¼•å…¥çš„è®¡ç®—å¼€é”€é—®é¢˜ã€‚</li>
<li>SPROé€šè¿‡ä»æ”¿ç­–æ¨¡å‹æœ¬èº«æ¨å¯¼è¿‡ç¨‹å¥–åŠ±ï¼Œå®ç°äº†è¿‡ç¨‹æ„ŸçŸ¥çš„å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>å¼•å…¥ç´¯ç§¯è¿‡ç¨‹å¥–åŠ±å’Œé®ç½©æ­¥éª¤ä¼˜åŠ¿ï¼Œå®ç°äº†ä¸¥æ ¼é€æ­¥è¡ŒåŠ¨ä¼˜åŠ¿ä¼°è®¡ã€‚</li>
<li>SPROåœ¨è®­ç»ƒæ•ˆç‡ã€æµ‹è¯•ç²¾åº¦ä¸Šæœ‰æ˜¾è‘—æå‡ï¼Œå¹¶ä¸”ç»´æŒç¨³å®šçš„ç­–ç•¥ç†µã€‚</li>
<li>SPROèƒ½å¤Ÿå‡å°‘å¹³å‡å“åº”é•¿åº¦ï¼Œå®ç°æ›´é«˜æ•ˆçš„å“åº”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01551">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87a03fd4e68ac40058357048309d7e73.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cd8c3fb44c843698fe7bd68e8b09d1ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-03798eb008e2027ea7d2323506321381.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="MARVIS-Modality-Adaptive-Reasoning-over-VISualizations"><a href="#MARVIS-Modality-Adaptive-Reasoning-over-VISualizations" class="headerlink" title="MARVIS: Modality Adaptive Reasoning over VISualizations"></a>MARVIS: Modality Adaptive Reasoning over VISualizations</h2><p><strong>Authors:Benjamin Feuer, Lennart Purucker, Oussama Elachqar, Chinmay Hegde</strong></p>
<p>Scientific applications of machine learning often rely on small, specialized models tuned to particular domains. Such models often achieve excellent performance, but lack flexibility. Foundation models offer versatility, but typically underperform specialized approaches, especially on non-traditional modalities and long-tail domains. We propose MARVIS (Modality Adaptive Reasoning over VISualizations), a training-free method that enables even small vision-language models to predict any data modality with high accuracy. MARVIS transforms latent embedding spaces into visual representations and then leverages the spatial and fine-grained reasoning skills of VLMs to successfully interpret and utilize them. MARVIS achieves competitive performance on vision, audio, biological, and tabular domains using a single 3B parameter model, achieving results that beat Gemini by 16% on average and approach specialized methods, without exposing personally identifiable information (P.I.I.) or requiring any domain-specific training. We open source our code and datasets at <a target="_blank" rel="noopener" href="https://github.com/penfever/marvis">https://github.com/penfever/marvis</a> </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ åœ¨ç§‘å­¦åº”ç”¨ä¸Šé€šå¸¸ä¾èµ–äºé’ˆå¯¹ç‰¹å®šé¢†åŸŸè¿›è¡Œå¾®è°ƒçš„å°å‹ä¸“ä¸šæ¨¡å‹ã€‚è¿™äº›æ¨¡å‹é€šå¸¸æ€§èƒ½å‡ºè‰²ï¼Œä½†ç¼ºä¹çµæ´»æ€§ã€‚åŸºç¡€æ¨¡å‹æä¾›äº†å¤šåŠŸèƒ½æ€§ï¼Œä½†åœ¨éä¼ ç»Ÿæ¨¡å¼å’Œé•¿å°¾é¢†åŸŸä¸Šé€šå¸¸è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬æå‡ºäº†MARVISï¼ˆå¯è§†åŒ–ä¸Šçš„æ¨¡æ€è‡ªé€‚åº”æ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå³ä½¿å¯¹äºå°å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä¹Ÿèƒ½ä»¥é«˜å‡†ç¡®æ€§é¢„æµ‹ä»»ä½•æ•°æ®æ¨¡æ€ã€‚MARVISå°†æ½œåœ¨åµŒå…¥ç©ºé—´è½¬æ¢ä¸ºå¯è§†åŒ–è¡¨ç¤ºå½¢å¼ï¼Œç„¶ååˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´åŒ–å’Œç²¾ç»†æ¨ç†æŠ€èƒ½æ¥æˆåŠŸè§£é‡Šå’Œåˆ©ç”¨å®ƒä»¬ã€‚ä½¿ç”¨å•ä¸ª3Bå‚æ•°æ¨¡å‹ï¼ŒMARVISåœ¨è§†è§‰ã€éŸ³é¢‘ã€ç”Ÿç‰©å’Œè¡¨æ ¼é¢†åŸŸå®ç°äº†æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼Œå¹³å‡å‡»è´¥åŒå­åº§æ¨¡å‹è¾¾1.å€ç»“æœï¼Œå¹¶æ¥è¿‘ä¸“ä¸šæ–¹æ³•çš„åº”ç”¨æ•ˆæœï¼ŒåŒæ—¶ä¸æ¶‰åŠä¸ªäººèº«ä»½ä¿¡æ¯çš„æš´éœ²ï¼ˆPIIï¼‰æˆ–ä»»ä½•ç‰¹å®šé¢†åŸŸçš„è®­ç»ƒéœ€æ±‚ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/penfever/marvis%E4%B8%8A%E5%85%AC%E5%BC%80%E4%BA%86%E6%88%91%E4%BB%AC%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82">https://github.com/penfever/marvisä¸Šå…¬å¼€äº†æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01544v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœºå™¨å­¦ä¹ çš„ç§‘å­¦åº”ç”¨é€šå¸¸éœ€è¦é’ˆå¯¹ç‰¹å®šé¢†åŸŸè°ƒæ•´å°å‹ä¸“ç”¨æ¨¡å‹ï¼Œè™½ç„¶è¿™äº›æ¨¡å‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œä½†ç¼ºä¹çµæ´»æ€§ã€‚è€ŒåŸºç¡€æ¨¡å‹è™½ç„¶å…·æœ‰é€šç”¨æ€§ï¼Œä½†åœ¨éä¼ ç»Ÿæ¨¡æ€å’Œé•¿å°¾é¢†åŸŸä¸Šçš„è¡¨ç°é€šå¸¸è¾ƒå·®ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ— éœ€è®­ç»ƒçš„MARVISæ–¹æ³•ï¼Œå®ƒå¯ä»¥è®©å³ä½¿æ˜¯å°å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¹Ÿèƒ½ä»¥é«˜å‡†ç¡®ç‡é¢„æµ‹ä»»ä½•æ•°æ®æ¨¡æ€ã€‚MARVISé€šè¿‡å°†æ½œåœ¨åµŒå…¥ç©ºé—´è½¬æ¢ä¸ºå¯è§†åŒ–è¡¨ç¤ºï¼Œç„¶ååˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´å’Œç²¾ç»†æ¨ç†æŠ€èƒ½æ¥æˆåŠŸè§£é‡Šå’Œåˆ©ç”¨å®ƒä»¬ã€‚ä½¿ç”¨å•ä¸€3Bå‚æ•°æ¨¡å‹ï¼ŒMARVISåœ¨è§†è§‰ã€éŸ³é¢‘ã€ç”Ÿç‰©å’Œè¡¨æ ¼é¢†åŸŸå®ç°äº†å‡ºè‰²çš„æ€§èƒ½è¡¨ç°ï¼Œå¹³å‡æ¯”Geminié«˜å‡º16%ï¼Œä¸”æ— éœ€ç‰¹å®šé¢†åŸŸçš„è®­ç»ƒæˆ–æš´éœ²ä¸ªäººä¿¡æ¯ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/penfever/marvis%E5%BC%80%E6%BA%90%E4%BA%86%E4%BB%A3%E7%A0%81%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82">https://github.com/penfever/marviså¼€æºäº†ä»£ç å’Œæ•°æ®é›†ã€‚</a></p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æœºå™¨å­¦ä¹ çš„ç§‘å­¦åº”ç”¨å—é™äºç‰¹å®šé¢†åŸŸçš„ä¸“ç”¨æ¨¡å‹ä¸é€šç”¨æ¨¡å‹é—´çš„æƒè¡¡ã€‚</li>
<li>ä¸“ç”¨æ¨¡å‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ä½†ç¼ºä¹çµæ´»æ€§ï¼Œè€ŒåŸºç¡€æ¨¡å‹å…·å¤‡é€šç”¨æ€§ä½†åœ¨éä¼ ç»Ÿå’Œé•¿å°¾é¢†åŸŸè¡¨ç°ä¸ä½³ã€‚</li>
<li>MARVISæ–¹æ³•æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œèƒ½åœ¨å°å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸Šå®ç°é«˜å‡†ç¡®ç‡çš„å¤šç§æ•°æ®æ¨¡æ€é¢„æµ‹ã€‚</li>
<li>MARVISé€šè¿‡å°†æ½œåœ¨åµŒå…¥ç©ºé—´è½¬æ¢ä¸ºå¯è§†åŒ–è¡¨ç¤ºæ¥æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MARVISåœ¨å¤šç§é¢†åŸŸï¼ˆåŒ…æ‹¬è§†è§‰ã€éŸ³é¢‘ã€ç”Ÿç‰©å’Œè¡¨æ ¼ï¼‰å®ç°äº†å‡ºè‰²çš„æ€§èƒ½è¡¨ç°ã€‚</li>
<li>MARVISç›¸è¾ƒäºGeminiå¹³å‡æé«˜äº†16%çš„æ€§èƒ½è¡¨ç°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01544">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2d89b5364def8ac52c2595fafb3ffda9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-522aa1d30e6ee519a1f9f1c3baa74f7f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-218fb612f9278412a168e7c1257eaf5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b5bd1dab3f66b72df12ca460f78db4b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39726f344fc47cba4c55a386b57fd9d5.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Coherent-Online-Road-Topology-Estimation-and-Reasoning-with-Standard-Definition-Maps"><a href="#Coherent-Online-Road-Topology-Estimation-and-Reasoning-with-Standard-Definition-Maps" class="headerlink" title="Coherent Online Road Topology Estimation and Reasoning with   Standard-Definition Maps"></a>Coherent Online Road Topology Estimation and Reasoning with   Standard-Definition Maps</h2><p><strong>Authors:Khanh Son Pham, Christian Witte, Jens Behley, Johannes Betz, Cyrill Stachniss</strong></p>
<p>Most autonomous cars rely on the availability of high-definition (HD) maps. Current research aims to address this constraint by directly predicting HD map elements from onboard sensors and reasoning about the relationships between the predicted map and traffic elements. Despite recent advancements, the coherent online construction of HD maps remains a challenging endeavor, as it necessitates modeling the high complexity of road topologies in a unified and consistent manner. To address this challenge, we propose a coherent approach to predict lane segments and their corresponding topology, as well as road boundaries, all by leveraging prior map information represented by commonly available standard-definition (SD) maps. We propose a network architecture, which leverages hybrid lane segment encodings comprising prior information and denoising techniques to enhance training stability and performance. Furthermore, we facilitate past frames for temporal consistency. Our experimental evaluation demonstrates that our approach outperforms previous methods by a large margin, highlighting the benefits of our modeling scheme. </p>
<blockquote>
<p>å¤§éƒ¨åˆ†è‡ªåŠ¨é©¾é©¶æ±½è½¦éƒ½ä¾èµ–äºé«˜æ¸…ï¼ˆHDï¼‰åœ°å›¾çš„å¯ç”¨æ€§ã€‚ç›®å‰çš„ç ”ç©¶æ—¨åœ¨é€šè¿‡ç›´æ¥ä»è½¦è½½ä¼ æ„Ÿå™¨é¢„æµ‹é«˜æ¸…åœ°å›¾å…ƒç´ å¹¶æ¨ç†é¢„æµ‹åœ°å›¾å’Œäº¤é€šå…ƒç´ ä¹‹é—´çš„å…³ç³»æ¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚å°½ç®¡æœ€è¿‘æœ‰è¿›å±•ï¼Œä½†åœ¨çº¿æ„å»ºè¿è´¯çš„é«˜æ¸…åœ°å›¾ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„å·¥ä½œï¼Œå› ä¸ºå®ƒéœ€è¦ä»¥ç»Ÿä¸€å’Œè¿è´¯çš„æ–¹å¼å¯¹é“è·¯æ‹“æ‰‘çš„é«˜å¤æ‚æ€§è¿›è¡Œå»ºæ¨¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿è´¯çš„æ–¹æ³•ï¼Œå¯ä»¥é¢„æµ‹è½¦é“æ®µåŠå…¶ç›¸åº”çš„æ‹“æ‰‘ç»“æ„ä»¥åŠé“è·¯è¾¹ç•Œï¼Œè¿™å……åˆ†åˆ©ç”¨äº†å¸¸è§çš„æ ‡å‡†å®šä¹‰ï¼ˆSDï¼‰åœ°å›¾æ‰€è¡¨ç¤ºçš„å…ˆéªŒåœ°å›¾ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç½‘ç»œæ¶æ„ï¼Œè¯¥æ¶æ„åˆ©ç”¨åŒ…å«å…ˆéªŒä¿¡æ¯å’Œå»å™ªæŠ€æœ¯çš„æ··åˆè½¦é“æ®µç¼–ç ï¼Œä»¥æé«˜è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨è¿‡å»çš„å¸§æ¥å®ç°æ—¶é—´ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¤§å¤§ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œçªå‡ºäº†æˆ‘ä»¬å»ºæ¨¡æ–¹æ¡ˆçš„ä¼˜ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01397v1">PDF</a> Accepted at IROS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ©ç”¨æ ‡å‡†å®šä¹‰åœ°å›¾çš„å…ˆéªŒä¿¡æ¯ï¼Œé¢„æµ‹é«˜æ¸…æ™°åº¦åœ°å›¾çš„è½¦é“çº¿æ®µåŠå…¶æ‹“æ‰‘ç»“æ„å’Œé“è·¯è¾¹ç•Œçš„è¿è´¯æ–¹æ³•ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ç½‘ç»œæ¶æ„ï¼Œèåˆäº†å…ˆéªŒä¿¡æ¯çš„æ··åˆè½¦é“æ®µç¼–ç å’Œé™å™ªæŠ€æœ¯ï¼Œä»¥æé«˜è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚åŒæ—¶ï¼Œåˆ©ç”¨è¿‡å»å¸§å®ç°æ—¶é—´ä¸€è‡´æ€§ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå¤§å¤§ä¼˜äºä»¥å‰çš„æ–¹æ³•ï¼Œçªå‡ºäº†å»ºæ¨¡æ–¹æ¡ˆçš„ä¼˜ç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å¤šæ•°è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¾èµ–äºé«˜æ¸…æ™°åº¦ï¼ˆHDï¼‰åœ°å›¾çš„å¯ç”¨æ€§ã€‚</li>
<li>å½“å‰ç ”ç©¶æ—¨åœ¨é€šè¿‡ç›´æ¥é¢„æµ‹HDåœ°å›¾å…ƒç´ å’Œæ¨ç†é¢„æµ‹åœ°å›¾ä¸äº¤é€šå…ƒç´ ä¹‹é—´çš„å…³ç³»æ¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚</li>
<li>åœ¨çº¿æ„å»ºHDåœ°å›¾æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦ç»Ÿä¸€å’Œä¸€è‡´åœ°å»ºæ¨¡é“è·¯æ‹“æ‰‘çš„é«˜å¤æ‚æ€§ã€‚</li>
<li>æå‡ºäº†ä¸€ç§è¿è´¯çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ ‡å‡†å®šä¹‰ï¼ˆSDï¼‰åœ°å›¾çš„å…ˆéªŒä¿¡æ¯æ¥é¢„æµ‹è½¦é“çº¿æ®µåŠå…¶æ‹“æ‰‘ç»“æ„å’Œé“è·¯è¾¹ç•Œã€‚</li>
<li>é‡‡ç”¨ç½‘ç»œæ¶æ„ï¼Œèåˆå…ˆéªŒä¿¡æ¯çš„æ··åˆè½¦é“æ®µç¼–ç å’Œé™å™ªæŠ€æœ¯ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§å’Œæ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨è¿‡å»å¸§å®ç°æ—¶é—´ä¸€è‡´æ€§ï¼Œæé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01397">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-675f437030543920d8fddd8b8a0cf4d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40d6bfa94b8e59970278616ccb8832ae.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-915fc09f6799b582da206730d7b89358.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5bc338f21e2500bd8e9322b21513a393.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="RALLY-Role-Adaptive-LLM-Driven-Yoked-Navigation-for-Agentic-UAV-Swarms"><a href="#RALLY-Role-Adaptive-LLM-Driven-Yoked-Navigation-for-Agentic-UAV-Swarms" class="headerlink" title="RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms"></a>RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms</h2><p><strong>Authors:Ziyao Wang, Rongpeng Li, Sizhao Li, Yuming Xiang, Haiping Wang, Zhifeng Zhao, Honggang Zhang</strong></p>
<p>Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as a critical research focus, and it typically requires the swarm to navigate effectively while avoiding obstacles and achieving continuous coverage over multiple mission targets. Although traditional Multi-Agent Reinforcement Learning (MARL) approaches offer dynamic adaptability, they are hindered by the semantic gap in numerical communication and the rigidity of homogeneous role structures, resulting in poor generalization and limited task scalability. Recent advances in Large Language Model (LLM)-based control frameworks demonstrate strong semantic reasoning capabilities by leveraging extensive prior knowledge. However, due to the lack of online learning and over-reliance on static priors, these works often struggle with effective exploration, leading to reduced individual potential and overall system performance. To address these limitations, we propose a Role-Adaptive LLM-Driven Yoked navigation algorithm RALLY. Specifically, we first develop an LLM-driven semantic decision framework that uses structured natural language for efficient semantic communication and collaborative reasoning. Afterward, we introduce a dynamic role-heterogeneity mechanism for adaptive role switching and personalized decision-making. Furthermore, we propose a Role-value Mixing Network (RMIX)-based assignment strategy that integrates LLM offline priors with MARL online policies to enable semi-offline training of role selection strategies. Experiments in the Multi-Agent Particle Environment (MPE) environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY outperforms conventional approaches in terms of task coverage, convergence speed, and generalization, highlighting its strong potential for collaborative navigation in agentic multi-UAV systems. </p>
<blockquote>
<p>æ— äººæœºç¾¤æ™ºèƒ½æ§åˆ¶å·²æˆä¸ºé‡è¦çš„ç ”ç©¶ç„¦ç‚¹ï¼Œé€šå¸¸éœ€è¦å®ƒä»¬åœ¨æœ‰æ•ˆå¯¼èˆªçš„åŒæ—¶é¿å…éšœç¢ç‰©ï¼Œå¹¶å¯¹å¤šä¸ªä»»åŠ¡ç›®æ ‡å®ç°è¿ç»­è¦†ç›–ã€‚å°½ç®¡ä¼ ç»Ÿçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ–¹æ³•æä¾›äº†åŠ¨æ€é€‚åº”æ€§ï¼Œä½†å®ƒä»¬å—åˆ°æ•°å€¼é€šä¿¡ä¸­çš„è¯­ä¹‰å·®è·å’ŒåŒè´¨è§’è‰²ç»“æ„çš„é™åˆ¶ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›è¾ƒå·®å’Œä»»åŠ¡å¯æ‰©å±•æ€§æœ‰é™ã€‚æœ€è¿‘åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ§åˆ¶æ¡†æ¶çš„è¿›å±•ï¼Œé€šè¿‡åˆ©ç”¨ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†å±•ç¤ºäº†å¼ºå¤§çš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹åœ¨çº¿å­¦ä¹ å’Œè¿‡åº¦ä¾èµ–é™æ€å…ˆéªŒçŸ¥è¯†ï¼Œè¿™äº›å·¥ä½œå¾€å¾€éš¾ä»¥è¿›è¡Œæœ‰æ•ˆçš„æ¢ç´¢ï¼Œå¯¼è‡´ä¸ªä½“æ½œåŠ›ä¸‹é™å’Œæ•´ä½“ç³»ç»Ÿæ€§èƒ½é™ä½ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè‡ªé€‚åº”è§’è‰²çš„LLMé©±åŠ¨å¯¼èˆªç®—æ³•RALLYã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ä¸ªåŸºäºLLMçš„è¯­ä¹‰å†³ç­–æ¡†æ¶ï¼Œä½¿ç”¨ç»“æ„åŒ–è‡ªç„¶è¯­è¨€è¿›è¡Œé«˜æ•ˆçš„è¯­ä¹‰é€šä¿¡å’Œåä½œæ¨ç†ã€‚éšåï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŠ¨æ€è§’è‰²å¼‚è´¨æ€§æœºåˆ¶ï¼Œç”¨äºè‡ªé€‚åº”è§’è‰²åˆ‡æ¢å’Œä¸ªæ€§åŒ–å†³ç­–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè§’è‰²ä»·å€¼æ··åˆç½‘ç»œï¼ˆRMIXï¼‰çš„åˆ†é…ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ç»“åˆäº†LLMçš„ç¦»çº¿å…ˆéªŒçŸ¥è¯†å’ŒMARLçš„åœ¨çº¿ç­–ç•¥ï¼Œå®ç°äº†è§’è‰²é€‰æ‹©ç­–ç•¥çš„åŠç¦»çº¿è®­ç»ƒã€‚åœ¨Multi-Agentç²’å­ç¯å¢ƒï¼ˆMPEï¼‰ç¯å¢ƒå’Œè½¯ä»¶åœ¨ç¯ï¼ˆSITLï¼‰å¹³å°ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒRALLYåœ¨ä»»åŠ¡è¦†ç›–ã€æ”¶æ•›é€Ÿåº¦å’Œæ³›åŒ–æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œçªæ˜¾äº†å…¶åœ¨æ™ºèƒ½å¤šæ— äººæœºç³»ç»ŸååŒå¯¼èˆªä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01378v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ— äººæœºç¾¤ï¼ˆUAVsï¼‰çš„æ™ºèƒ½æ§åˆ¶æ˜¯å½“ä¸‹é‡è¦çš„ç ”ç©¶ç„¦ç‚¹ã€‚è¿™éœ€è¦ç¾¤ä½“è¿›è¡Œé«˜æ•ˆçš„å¯¼èˆªï¼ŒåŒæ—¶é¿å…éšœç¢å¹¶å®ç°å¯¹å¤šä¸ªä»»åŠ¡ç›®æ ‡çš„è¿ç»­è¦†ç›–ã€‚ä¼ ç»Ÿçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰æ–¹æ³•è™½ç„¶å…·æœ‰åŠ¨æ€é€‚åº”æ€§ï¼Œä½†å®ƒä»¬å—åˆ°æ•°å€¼é€šä¿¡ä¸­çš„è¯­ä¹‰å·®è·å’ŒåŒè´¨è§’è‰²ç»“æ„åƒµåŒ–ç­‰é™åˆ¶ï¼Œå¯¼è‡´æ³›åŒ–æ€§èƒ½ä¸ä½³å’Œä»»åŠ¡å¯æ‰©å±•æ€§æœ‰é™ã€‚åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ§åˆ¶æ¡†æ¶å…·æœ‰å¼ºå¤§çš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡åˆ©ç”¨ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†å±•ç°äº†å‡ºè‰²çš„è¡¨ç°ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹åœ¨çº¿å­¦ä¹ å’Œå¯¹é™æ€å…ˆéªŒçš„è¿‡åº¦ä¾èµ–ï¼Œè¿™äº›ç ”ç©¶åœ¨æœ‰æ•ˆæ¢ç´¢æ–¹é¢é¢ä¸´æŒ‘æˆ˜ï¼Œå¯¼è‡´ä¸ªä½“æ½œèƒ½å’Œç³»ç»Ÿæ•´ä½“æ€§èƒ½é™ä½ã€‚ä¸ºè§£å†³è¿™äº›å±€é™ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§è§’è‰²è‡ªé€‚åº”çš„LLMé©±åŠ¨ååŒå¯¼èˆªç®—æ³•RALLYã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬æ„å»ºäº†LLMé©±åŠ¨çš„è¯­ä¹‰å†³ç­–æ¡†æ¶ï¼Œåˆ©ç”¨ç»“æ„åŒ–è‡ªç„¶è¯­è¨€è¿›è¡Œé«˜æ•ˆçš„è¯­ä¹‰é€šä¿¡å’Œåä½œæ¨ç†ã€‚éšåå¼•å…¥åŠ¨æ€è§’è‰²å¼‚è´¨æ€§æœºåˆ¶ï¼Œå®ç°è‡ªé€‚åº”è§’è‰²åˆ‡æ¢å’Œä¸ªæ€§åŒ–å†³ç­–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºåŸºäºè§’è‰²ä»·å€¼æ··åˆç½‘ç»œï¼ˆRMIXï¼‰çš„åˆ†é…ç­–ç•¥ï¼ŒèåˆLLMçš„ç¦»çº¿å…ˆéªŒçŸ¥è¯†ä¸MARLçš„åœ¨çº¿ç­–ç•¥ï¼Œå®ç°è§’è‰²é€‰æ‹©ç­–ç•¥çš„åŠç¦»çº¿è®­ç»ƒã€‚åœ¨Multi-Agentç²’å­ç¯å¢ƒï¼ˆMPEï¼‰å’ŒSoftware-In-The-Loopï¼ˆSITLï¼‰å¹³å°çš„å®éªŒè¡¨æ˜ï¼ŒRALLYåœ¨ä»»åŠ¡è¦†ç›–ã€æ”¶æ•›é€Ÿåº¦å’Œæ³›åŒ–ç­‰æ–¹é¢ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œçªæ˜¾å…¶åœ¨æ™ºèƒ½å¤šæ— äººæœºç³»ç»Ÿåä½œå¯¼èˆªä¸­çš„å¼ºå¤§æ½œåŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ— äººæœºç¾¤æ™ºèƒ½æ§åˆ¶æ˜¯å½“å‰å…³é”®ç ”ç©¶ç„¦ç‚¹ï¼Œéœ€è¦å®ç°é«˜æ•ˆå¯¼èˆªã€éšœç¢é¿å…åŠå¤šä»»åŠ¡ç›®æ ‡è¦†ç›–ã€‚</li>
<li>ä¼ ç»ŸMARLæ–¹æ³•è™½å…·æœ‰åŠ¨æ€é€‚åº”æ€§ï¼Œä½†å—è¯­ä¹‰å·®è·å’Œè§’è‰²ç»“æ„é™åˆ¶ï¼Œæ³›åŒ–æ€§èƒ½æœ‰é™ã€‚</li>
<li>åŸºäºLLMçš„æ§åˆ¶æ¡†æ¶å±•ç°å‡ºå¼ºå¤§çš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ï¼Œåˆ©ç”¨å…ˆéªŒçŸ¥è¯†è§£å†³å¤æ‚ä»»åŠ¡ã€‚</li>
<li>LLMæ–¹æ³•ç¼ºä¹åœ¨çº¿å­¦ä¹ å’Œè¿‡åº¦ä¾èµ–é™æ€å…ˆéªŒçŸ¥è¯†ï¼Œå½±å“æœ‰æ•ˆæ¢ç´¢å’Œç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>RALLYç®—æ³•ç»“åˆLLMå’ŒMARLçš„ä¼˜åŠ¿ï¼Œé€šè¿‡è¯­ä¹‰å†³ç­–æ¡†æ¶å’ŒåŠ¨æ€è§’è‰²æœºåˆ¶æå‡ç³»ç»Ÿæ€§èƒ½ã€‚</li>
<li>RALLYé‡‡ç”¨è§’è‰²ä»·å€¼æ··åˆç½‘ç»œï¼ˆRMIXï¼‰å®ç°ç¦»çº¿ä¸åœ¨çº¿ç­–ç•¥èåˆï¼Œä¼˜åŒ–è§’è‰²é€‰æ‹©ç­–ç•¥è®­ç»ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01378">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b66d6c16e8805ac9a92f9a9792a1224f.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6a19d6fb1853da846cae5c77374284ed.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5b24b162fc4bc450cced77056da8e74b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c51306db988c5eebf31ea7ee5aa86e51.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c67ee15f9e529e3599ade5099ad2f61b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bd44940ac7a49dee5dfcf4fcb2ed9614.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Symbolic-or-Numerical-Understanding-Physics-Problem-Solving-in-Reasoning-LLMs"><a href="#Symbolic-or-Numerical-Understanding-Physics-Problem-Solving-in-Reasoning-LLMs" class="headerlink" title="Symbolic or Numerical? Understanding Physics Problem Solving in   Reasoning LLMs"></a>Symbolic or Numerical? Understanding Physics Problem Solving in   Reasoning LLMs</h2><p><strong>Authors:Nifu Dan, Yujun Cai, Yiwei Wang</strong></p>
<p>Navigating the complexities of physics reasoning has long been a difficult task for Large Language Models (LLMs), requiring a synthesis of profound conceptual understanding and adept problem-solving techniques. In this study, we investigate the application of advanced instruction-tuned reasoning models, such as Deepseek-R1, to address a diverse spectrum of physics problems curated from the challenging SciBench benchmark. Our comprehensive experimental evaluation reveals the remarkable capabilities of reasoning models. Not only do they achieve state-of-the-art accuracy in answering intricate physics questions, but they also generate distinctive reasoning patterns that emphasize on symbolic derivation. Furthermore, our findings indicate that even for these highly sophisticated reasoning models, the strategic incorporation of few-shot prompting can still yield measurable improvements in overall accuracy, highlighting the potential for continued performance gains. </p>
<blockquote>
<p>é•¿æœŸä»¥æ¥ï¼Œåœ¨ç‰©ç†æ¨ç†çš„å¤æ‚æ€§ä¸­å¯¼èˆªå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ï¼Œéœ€è¦æ·±åˆ»çš„æ¦‚å¿µç†è§£å’Œç†Ÿç»ƒçš„é—®é¢˜è§£å†³æŠ€æœ¯çš„ç»“åˆã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†é«˜çº§æŒ‡ä»¤è°ƒæ•´æ¨ç†æ¨¡å‹ï¼ˆå¦‚Deepseek-R1ï¼‰åœ¨è§£å†³ä»å…·æœ‰æŒ‘æˆ˜æ€§çš„SciBenchåŸºå‡†æµ‹è¯•ä¸­ç²¾é€‰çš„å„ç§ç‰©ç†é—®é¢˜æ–¹é¢çš„åº”ç”¨ã€‚æˆ‘ä»¬çš„ç»¼åˆå®éªŒè¯„ä¼°æ˜¾ç¤ºäº†æ¨ç†æ¨¡å‹çš„å“è¶Šèƒ½åŠ›ã€‚å®ƒä»¬ä¸ä»…åœ¨å›ç­”å¤æ‚ç‰©ç†é—®é¢˜æ—¶è¾¾åˆ°äº†æœ€å…ˆè¿›çš„å‡†ç¡®æ€§ï¼Œè€Œä¸”è¿˜äº§ç”Ÿäº†å¼ºè°ƒç¬¦å·æ¨å¯¼çš„ç‹¬ç‰¹æ¨ç†æ¨¡å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯å¯¹äºè¿™äº›é«˜åº¦å¤æ‚çš„æ¨ç†æ¨¡å‹ï¼Œæˆ˜ç•¥æ€§åœ°é‡‡ç”¨å°‘é‡æç¤ºä»ç„¶å¯ä»¥æé«˜æ•´ä½“å‡†ç¡®æ€§ï¼Œè¿™å‡¸æ˜¾äº†æŒç»­æé«˜æ€§èƒ½çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01334v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬ç ”ç©¶æ¢è®¨äº†å…ˆè¿›çš„æŒ‡ä»¤ä¼˜åŒ–æ¨ç†æ¨¡å‹ï¼ˆå¦‚Deepseek-R1ï¼‰åœ¨åº”å¯¹æ¥è‡ªSciBenchåŸºå‡†æŒ‘æˆ˜çš„å¤šæ ·åŒ–ç‰©ç†é—®é¢˜æ—¶çš„åº”ç”¨ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œè¿™äº›æ¨ç†æ¨¡å‹ä¸ä»…èƒ½åœ¨å›ç­”å¤æ‚ç‰©ç†é—®é¢˜æ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œè¿˜å±•ç°å‡ºç‹¬ç‰¹çš„å¼ºè°ƒç¬¦å·æ¨å¯¼çš„æ¨ç†æ¨¡å¼ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°ï¼Œå³ä½¿å¯¹è¿™äº›é«˜åº¦å…ˆè¿›çš„æ¨ç†æ¨¡å‹ï¼Œé€šè¿‡å°‘æ ·æœ¬æç¤ºçš„æˆ˜ç•¥æ•´åˆä»ç„¶èƒ½æé«˜æ€»ä½“ç²¾åº¦ï¼Œå±•ç¤ºäº†æ½œåœ¨çš„è¿›ä¸€æ­¥æ€§èƒ½æå‡ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>å…ˆè¿›çš„æŒ‡ä»¤ä¼˜åŒ–æ¨ç†æ¨¡å‹åœ¨è§£å†³ç‰©ç†é—®é¢˜æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å›ç­”å¤æ‚ç‰©ç†é—®é¢˜æ–¹é¢è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
<li>è¿™äº›æ¨ç†æ¨¡å‹å¼ºè°ƒç¬¦å·æ¨å¯¼çš„é‡è¦æ€§ï¼Œå±•ç°å‡ºç‹¬ç‰¹çš„æ¨ç†æ¨¡å¼ã€‚</li>
<li>é€šè¿‡ç»“åˆå°‘æ ·æœ¬æç¤ºï¼Œè¿™äº›æ¨ç†æ¨¡å‹çš„æ€§èƒ½å¯ä»¥è¿›ä¸€æ­¥æé«˜ï¼Œæ˜¾ç¤ºå‡ºæ½œåœ¨çš„è¿›ä¸€æ­¥æ€§èƒ½æå‡ç©ºé—´ã€‚</li>
<li>è¿™ç§ç»“åˆæ–¹æ³•å¯¹äºé«˜åº¦å…ˆè¿›çš„æ¨ç†æ¨¡å‹ä»ç„¶æœ‰æ•ˆã€‚</li>
<li>è¿™ç§ç ”ç©¶å¯¹äºæé«˜å¤§è¯­è¨€æ¨¡å‹åœ¨ç‰©ç†é¢†åŸŸçš„æ¨ç†èƒ½åŠ›å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
<li>Deepseek-R1ç­‰æ¨¡å‹çš„åº”ç”¨æ‰©å±•äº†å®ƒä»¬åœ¨è§£å†³ç‰¹å®šé¢†åŸŸé—®é¢˜æ—¶çš„èƒ½åŠ›è¾¹ç•Œã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01334">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f75168278177cd46de96df312c7a4822.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-43ee0b3af7d989529f628567f903df25.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efed7ee1e128c14e937744ff362c89d8.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-bce1c83d5c44a0ac7c57117747866cef.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-39a106965c2a43254c57b5354cd250ca.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Reasoner-for-Real-World-Event-Detection-Scaling-Reinforcement-Learning-via-Adaptive-Perplexity-Aware-Sampling-Strategy"><a href="#Reasoner-for-Real-World-Event-Detection-Scaling-Reinforcement-Learning-via-Adaptive-Perplexity-Aware-Sampling-Strategy" class="headerlink" title="Reasoner for Real-World Event Detection: Scaling Reinforcement Learning   via Adaptive Perplexity-Aware Sampling Strategy"></a>Reasoner for Real-World Event Detection: Scaling Reinforcement Learning   via Adaptive Perplexity-Aware Sampling Strategy</h2><p><strong>Authors:Xiaoyun Zhang, Jingqing Ruan, Xing Ma, Yawen Zhu, Jiansong Chen, Ke Zeng, Xunliang Cai</strong></p>
<p>Detecting abnormal events in real-world customer service dialogues is highly challenging due to the complexity of business data and the dynamic nature of customer interactions. Moreover, models must demonstrate strong out-of-domain (OOD) generalization to enable rapid adaptation across different business scenarios and maximize commercial value. In this work, we propose a novel Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that leverages the advanced reasoning capabilities of large language models for abnormal event detection. APARL introduces a dual-loop dynamic curriculum learning architecture, enabling the model to progressively focus on more challenging samples as its proficiency increases. This design effectively addresses performance bottlenecks and significantly enhances OOD transferability. Extensive evaluations on food delivery dialogue tasks show that our model achieves significantly enhanced adaptability and robustness, attaining the highest F1 score with an average improvement of 17.19%, and an average improvement of 9.59% in OOD transfer tests. This method provides a superior solution for industrial deployment of anomaly detection models, contributing to improved operational efficiency and commercial benefits. </p>
<blockquote>
<p>åœ¨ç°å®ä¸–ç•Œä¸­çš„å®¢æˆ·æœåŠ¡å¯¹è¯ä¸­æ£€æµ‹å¼‚å¸¸äº‹ä»¶æ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºä¸šåŠ¡æ•°æ®çš„å¤æ‚æ€§å’Œå®¢æˆ·äº’åŠ¨çš„åŠ¨æ€æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹å¿…é¡»å±•ç°å‡ºå¼ºå¤§çš„åŸŸå¤–ï¼ˆOODï¼‰æ³›åŒ–èƒ½åŠ›ï¼Œä»¥ä¾¿åœ¨ä¸åŒçš„ä¸šåŠ¡åœºæ™¯ä¸­å¿«é€Ÿé€‚åº”å¹¶æœ€å¤§åŒ–å•†ä¸šä»·å€¼ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„è‡ªé€‚åº”å›°æƒ‘åº¦æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆAPARLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å…ˆè¿›æ¨ç†èƒ½åŠ›è¿›è¡Œå¼‚å¸¸äº‹ä»¶æ£€æµ‹ã€‚APARLå¼•å…¥äº†ä¸€ç§åŒå¾ªç¯åŠ¨æ€è¯¾ç¨‹å­¦ä¹ æ¶æ„ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿéšç€ç†Ÿç»ƒç¨‹åº¦çš„æé«˜è€Œé€æ­¥å…³æ³¨æ›´å…·æŒ‘æˆ˜æ€§çš„æ ·æœ¬ã€‚è¿™ç§è®¾è®¡æœ‰æ•ˆåœ°è§£å†³äº†æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶å¤§å¤§æé«˜äº†OODçš„è¿ç§»èƒ½åŠ›ã€‚åœ¨é£Ÿå“é…é€å¯¹è¯ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨é€‚åº”æ€§å’Œç¨³å¥æ€§æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œè·å¾—äº†æœ€é«˜çš„F1åˆ†æ•°ï¼Œå¹³å‡æé«˜äº†17.19%ï¼Œåœ¨OODè¿ç§»æµ‹è¯•ä¸­å¹³å‡æé«˜äº†9.59%ã€‚è¯¥æ–¹æ³•ä¸ºå¼‚å¸¸æ£€æµ‹æ¨¡å‹çš„å·¥ä¸šéƒ¨ç½²æä¾›äº†å“è¶Šçš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºæé«˜è¿è¥æ•ˆç‡å’Œå•†ä¸šæ•ˆç›Šåšå‡ºäº†è´¡çŒ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01327v1">PDF</a> 15 pages, 6 figures, submitted to EMNLP</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºAPARLçš„æ–°å‹è‡ªé€‚åº”å›°æƒ‘åº¦æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºåœ¨ç°å®ä¸–ç•Œå®¢æˆ·æœåŠ¡å¯¹è¯ä¸­æ£€æµ‹å¼‚å¸¸äº‹ä»¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡åŒå¾ªç¯åŠ¨æ€è¯¾ç¨‹å­¦ä¹ æ¶æ„ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿé€æ¸å…³æ³¨æ›´å…·æŒ‘æˆ˜æ€§çš„æ ·æœ¬ï¼Œå¹¶æé«˜å…¶æ€§èƒ½ã€‚åœ¨é£Ÿå“é…é€å¯¹è¯ä»»åŠ¡ä¸Šçš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼Œè¯¥æ¨¡å‹å…·æœ‰å‡ºè‰²çš„é€‚åº”æ€§å’Œç¨³å¥æ€§ï¼ŒF1å¾—åˆ†æ˜¾è‘—æé«˜ï¼Œå¹³å‡æ”¹è¿›ç‡ä¸º17.19%ï¼Œåœ¨OODè½¬ç§»æµ‹è¯•ä¸­çš„å¹³å‡æ”¹è¿›ç‡ä¸º9.59%ã€‚è¯¥æ–¹æ³•ä¸ºå¼‚å¸¸æ£€æµ‹æ¨¡å‹åœ¨å·¥ä¸šéƒ¨ç½²ä¸­æä¾›äº†å“è¶Šè§£å†³æ–¹æ¡ˆï¼Œæœ‰åŠ©äºæé«˜æ“ä½œæ•ˆç‡å’Œå•†ä¸šæ•ˆç›Šã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>APARLæ¡†æ¶ç”¨äºåœ¨ç°å®ä¸–ç•Œå®¢æˆ·æœåŠ¡å¯¹è¯ä¸­æ£€æµ‹å¼‚å¸¸äº‹ä»¶ã€‚</li>
<li>APARLå¼•å…¥åŒå¾ªç¯åŠ¨æ€è¯¾ç¨‹å­¦ä¹ æ¶æ„ã€‚</li>
<li>æ¨¡å‹èƒ½å¤Ÿé€æ¸å…³æ³¨æ›´å…·æŒ‘æˆ˜æ€§çš„æ ·æœ¬ï¼Œæé«˜å…¶æ€§èƒ½ã€‚</li>
<li>åœ¨é£Ÿå“é…é€å¯¹è¯ä»»åŠ¡ä¸Šï¼ŒAPARLæ¨¡å‹è¡¨ç°å‡ºå‡ºè‰²çš„é€‚åº”æ€§å’Œç¨³å¥æ€§ã€‚</li>
<li>APARLæ¨¡å‹F1å¾—åˆ†æ˜¾è‘—æé«˜ï¼Œå¹³å‡æ”¹è¿›ç‡ä¸º17.19%ã€‚</li>
<li>APARLæ¨¡å‹åœ¨OODè½¬ç§»æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰¯å¥½çš„é€‚åº”æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01327">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-a33f99482ba769fe00268dc2830bcc36.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-27e7d8598d8cb37688bd46e3eeaae23b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-d58c0556d65c978ad20cc5010e04e8eb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bce64ceaa21aa58a919bc1f5fceab71a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-786024ad645a3167c129b0c9e97309ad.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-adc067e2d5aca9e3ffd151ebd7cd44eb.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Frustratingly-Simple-Retrieval-Improves-Challenging-Reasoning-Intensive-Benchmarks"><a href="#Frustratingly-Simple-Retrieval-Improves-Challenging-Reasoning-Intensive-Benchmarks" class="headerlink" title="Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive   Benchmarks"></a>Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive   Benchmarks</h2><p><strong>Authors:Xinxi Lyu, Michael Duan, Rulin Shao, Pang Wei Koh, Sewon Min</strong></p>
<p>Retrieval-augmented Generation (RAG) has primarily been studied in limited settings, such as factoid question answering; more challenging, reasoning-intensive benchmarks have seen limited success from minimal RAG. In this work, we challenge this prevailing view on established, reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We identify a key missing component in prior work: a usable, web-scale datastore aligned with the breadth of pretraining data. To this end, we introduce CompactDS: a diverse, high-quality, web-scale datastore that achieves high retrieval accuracy and subsecond latency on a single-node. The key insights are (1) most web content can be filtered out without sacrificing coverage, and a compact, high-quality subset is sufficient; and (2) combining in-memory approximate nearest neighbor (ANN) retrieval and on-disk exact search balances speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves consistent accuracy improvements across all benchmarks and model sizes (8Bâ€“70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA, and 19% on MATH. No single data source suffices alone, highlighting the importance of diversity of sources (web crawls, curated math, academic papers, textbooks). Finally, we show that our carefully designed in-house datastore matches or outperforms web search engines such as Google Search, as well as recently proposed, complex agent-based RAG systemsâ€“all while maintaining simplicity, reproducibility, and self-containment. We release CompactDS and our retrieval pipeline, supporting future research exploring retrieval-based AI systems. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ä¸»è¦åœ¨æœ‰é™çš„ç¯å¢ƒä¸­è¿›è¡Œäº†ç ”ç©¶ï¼Œå¦‚åŸºäºäº‹å®çš„é—®é¢˜å›ç­”ï¼›åœ¨æ›´å…·æŒ‘æˆ˜æ€§çš„ã€æ³¨é‡æ¨ç†çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒRAGçš„æ•ˆæœæœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é’ˆå¯¹ç°æœ‰çš„æ³¨é‡æ¨ç†çš„åŸºå‡†æµ‹è¯•ï¼šMMLUã€MMLU Proã€AGI Evalã€GPQAå’ŒMATHï¼Œæå‡ºè´¨ç–‘ã€‚æˆ‘ä»¬å‘ç°å…ˆå‰å·¥ä½œä¸­ç¼ºå°‘ä¸€ä¸ªå…³é”®ç»„ä»¶ï¼šä¸€ä¸ªä¸é¢„è®­ç»ƒæ•°æ®å¹¿åº¦ç›¸åŒ¹é…çš„å¯ç”¨çš„å¤§è§„æ¨¡ç½‘ç»œæ•°æ®å­˜å‚¨åº“ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†CompactDSï¼šä¸€ä¸ªå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„å¤§è§„æ¨¡ç½‘ç»œæ•°æ®å­˜å‚¨åº“ï¼Œåœ¨å•ä¸ªèŠ‚ç‚¹ä¸Šå®ç°äº†é«˜æ£€ç´¢å‡†ç¡®ç‡å’Œäºšç§’çº§çš„å»¶è¿Ÿã€‚å…³é”®è§è§£æ˜¯ï¼ˆ1ï¼‰å¤§éƒ¨åˆ†ç½‘é¡µå†…å®¹å¯ä»¥è¢«è¿‡æ»¤æ‰è€Œä¸ä¼šç‰ºç‰²è¦†ç›–èŒƒå›´ï¼Œä¸€ä¸ªç´§å‡‘çš„é«˜è´¨é‡å­é›†å°±è¶³å¤Ÿäº†ï¼›ï¼ˆ2ï¼‰ç»“åˆå†…å­˜ä¸­çš„è¿‘ä¼¼æœ€è¿‘é‚»ï¼ˆANNï¼‰æ£€ç´¢å’Œç£ç›˜ä¸Šçš„ç²¾ç¡®æœç´¢å¯ä»¥å¹³è¡¡é€Ÿåº¦å’Œå¬å›ç‡ã€‚ä½¿ç”¨CompactDSï¼Œæˆ‘ä»¬è¯æ˜äº†ç®€å•çš„RAGç®¡é“åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹å¤§å°ï¼ˆ8B-70Bï¼‰ä¸Šéƒ½èƒ½å®ç°ä¸€è‡´çš„å‡†ç¡®æ€§æé«˜ï¼Œåœ¨MMLUä¸Šçš„ç›¸å¯¹å¢ç›Šä¸º10%ï¼Œåœ¨MMLU Proä¸Šä¸º33%ï¼Œåœ¨GPQAä¸Šä¸º14%ï¼Œåœ¨MATHä¸Šä¸º19%ã€‚æ²¡æœ‰å•ä¸€çš„æ•°æ®æºå¯ä»¥å•ç‹¬ä½¿ç”¨ï¼Œè¿™çªæ˜¾äº†æ•°æ®æºå¤šæ ·æ€§çš„é‡è¦æ€§ï¼ˆç½‘ç»œçˆ¬è™«ã€ç²¾é€‰çš„æ•°å­¦èµ„æ–™ã€å­¦æœ¯è®ºæ–‡ã€æ•™ç§‘ä¹¦ç­‰ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„ç²¾å¿ƒè®¾è®¡å†…éƒ¨æ•°æ®å­˜å‚¨åº“å¯ä»¥ä¸è°·æ­Œæœç´¢ç­‰ç½‘ç»œæœç´¢å¼•æ“ä»¥åŠæœ€è¿‘æå‡ºçš„å¤æ‚åŸºäºä»£ç†çš„RAGç³»ç»Ÿç›¸åª²ç¾ç”šè‡³æ›´èƒœä¸€ç­¹â€”â€”åŒæ—¶ä¿æŒäº†ç®€å•æ€§ã€å¯å¤åˆ¶æ€§å’Œè‡ªä¸»æ€§ã€‚æˆ‘ä»¬å‘å¸ƒCompactDSå’Œæˆ‘ä»¬çš„æ£€ç´¢ç®¡é“ï¼Œæ”¯æŒæœªæ¥å¯¹åŸºäºæ£€ç´¢çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„æ¢ç´¢ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01297v1">PDF</a> 33 pages, 2 figures, 27 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å¤§å‹é¢„è®­ç»ƒæ•°æ®èƒŒæ™¯ä¸‹ï¼ŒåŸºäºæ£€ç´¢çš„å¢å¼ºç”Ÿæˆæ¨¡å‹ï¼ˆRAGï¼‰åœ¨æ¨ç†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°ã€‚ä½œè€…æå‡ºäº†ä¸€é¡¹å…³é”®ç¼ºå¤±çš„ç»„ä»¶ï¼šä¸€ä¸ªå¯ç”¨çš„å¤§å‹ç½‘ç»œæ•°æ®å­˜å‚¨ï¼Œå®ƒä¸é¢„è®­ç»ƒæ•°æ®çš„å¹¿åº¦ç›¸åŒ¹é…ã€‚ä¸ºæ­¤ï¼Œä»–ä»¬å¼•å…¥äº†CompactDSï¼Œè¿™æ˜¯ä¸€ä¸ªå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„å¤§å‹ç½‘ç»œæ•°æ®å­˜å‚¨ï¼Œèƒ½å¤Ÿåœ¨å•ä¸ªèŠ‚ç‚¹ä¸Šå®ç°é«˜æ£€ç´¢å‡†ç¡®æ€§å’Œäºšç§’çº§çš„å»¶è¿Ÿã€‚ä½¿ç”¨CompactDSï¼Œä½œè€…å±•ç¤ºäº†ç®€å•çš„RAGç®¡é“åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹å¤§å°ä¸Šéƒ½èƒ½å®ç°ä¸€è‡´çš„å‡†ç¡®æ€§æ”¹è¿›ã€‚æœ€åï¼Œä½œè€…è®¤ä¸ºæ²¡æœ‰å•ä¸€çš„æ•°æ®æ¥æºå¯ä»¥å•ç‹¬æ»¡è¶³éœ€æ±‚ï¼Œå¼ºè°ƒäº†æ•°æ®æ¥æºå¤šæ ·æ€§çš„é‡è¦æ€§ã€‚ä»–ä»¬é‡Šæ”¾äº†CompactDSå’Œæ£€ç´¢ç®¡é“ï¼Œä»¥æ”¯æŒæœªæ¥å¯¹åŸºäºæ£€ç´¢çš„AIç³»ç»Ÿçš„æ¢ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAGåœ¨æ¨ç†å¯†é›†å‹åŸºå‡†æµ‹è¯•ä¸Šçš„è¡¨ç°æœ‰é™ï¼Œä¸»è¦ç”±äºç¼ºå°‘ä¸€ä¸ªä¸é¢„è®­ç»ƒæ•°æ®å¹¿åº¦ç›¸åŒ¹é…çš„å¤§å‹ç½‘ç»œæ•°æ®å­˜å‚¨ç»„ä»¶ã€‚</li>
<li>å¼•å…¥çš„CompactDSæ˜¯ä¸€ä¸ªå¤šæ ·åŒ–ã€é«˜è´¨é‡çš„å¤§å‹ç½‘ç»œæ•°æ®å­˜å‚¨ï¼Œå®ç°äº†é«˜æ£€ç´¢å‡†ç¡®æ€§å’Œäºšç§’çº§å»¶è¿Ÿã€‚</li>
<li>ä½¿ç”¨CompactDSçš„RAGç®¡é“åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•å’Œä¸åŒæ¨¡å‹å¤§å°ä¸Šéƒ½å®ç°äº†å‡†ç¡®æ€§æ”¹è¿›ã€‚</li>
<li>æ•°æ®æ¥æºçš„å¤šæ ·æ€§å¯¹RAGçš„æ€§èƒ½è‡³å…³é‡è¦ï¼Œæ²¡æœ‰å•ä¸€æ•°æ®æ¥æºå¯ä»¥å•ç‹¬æ»¡è¶³éœ€æ±‚ã€‚</li>
<li>CompactDSåŠå…¶æ£€ç´¢ç®¡é“å·²è¢«å‘å¸ƒï¼Œä»¥æ”¯æŒå¯¹åŸºäºæ£€ç´¢çš„AIç³»ç»Ÿçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</li>
<li>CompactDSçš„è¡¨ç°åœ¨å¤šä¸ªæ–¹é¢è¶…è¿‡æˆ–åŒ¹é…äº†Google Searchç­‰ç½‘ç»œæœç´¢å¼•æ“ä»¥åŠå¤æ‚çš„åŸºäºä»£ç†çš„RAGç³»ç»Ÿï¼ŒåŒæ—¶ä¿æŒäº†ç®€å•æ€§ã€å¯é‡å¤æ€§å’Œè‡ªç»™è‡ªè¶³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01297">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a7b651549a7464a606ffe53c4f5c9537.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29a8ac73327d97d483a80f8a5dda3023.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="PULSE-Practical-Evaluation-Scenarios-for-Large-Multimodal-Model-Unlearning"><a href="#PULSE-Practical-Evaluation-Scenarios-for-Large-Multimodal-Model-Unlearning" class="headerlink" title="PULSE: Practical Evaluation Scenarios for Large Multimodal Model   Unlearning"></a>PULSE: Practical Evaluation Scenarios for Large Multimodal Model   Unlearning</h2><p><strong>Authors:Tatsuki Kawakami, Kazuki Egashira, Atsuyuki Miyai, Go Irie, Kiyoharu Aizawa</strong></p>
<p>In recent years, unlearning techniques, which are methods for inducing a model to â€œforgetâ€ previously learned information, have attracted attention as a way to address privacy and copyright concerns in large language models (LLMs) and large multimodal models (LMMs). While several unlearning benchmarks have been established for LLMs, a practical evaluation framework for unlearning in LMMs has been less explored. Specifically, existing unlearning benchmark for LMMs considers only scenarios in which the model is required to unlearn fine-tuned knowledge through a single unlearning operation. In this study, we introduce PULSE protocol for realistic unlearning scenarios for LMMs by introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for analyzing the effect across different knowledge acquisition phases and (ii) Long-term Sustainability Evaluation to address sequential requests. We then evaluate existing unlearning methods along these dimensions. Our results reveal that, although some techniques can successfully unlearn knowledge acquired through fine-tuning, they struggle to eliminate information learned during pre-training. Moreover, methods that effectively unlearn a batch of target data in a single operation exhibit substantial performance degradation when the same data are split and unlearned sequentially. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œé—å¿˜æŠ€æœ¯ï¼ˆå³è®©æ¨¡å‹â€œå¿˜è®°â€å…ˆå‰å­¦ä¹ ä¿¡æ¯çš„æ–¹æ³•ï¼‰å¼•èµ·äº†äººä»¬çš„å…³æ³¨ï¼Œä½œä¸ºä¸€ç§è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMï¼‰ä¸­çš„éšç§å’Œç‰ˆæƒé—®é¢˜çš„é€”å¾„ã€‚è™½ç„¶ä¸ºLLMå»ºç«‹äº†å¤šä¸ªé—å¿˜åŸºå‡†æµ‹è¯•ï¼Œä½†å…³äºLMMä¸­é—å¿˜çš„å®ç”¨è¯„ä¼°æ¡†æ¶çš„ç ”ç©¶è¾ƒå°‘ã€‚å…·ä½“æ¥è¯´ï¼Œç°æœ‰çš„LMMé—å¿˜åŸºå‡†æµ‹è¯•åªè€ƒè™‘æ¨¡å‹éœ€è¦é€šè¿‡ä¸€æ¬¡é—å¿˜æ“ä½œæ¥é—å¿˜å¾®è°ƒçŸ¥è¯†çš„åœºæ™¯ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥ä¸¤ä¸ªå…³é”®è§†è§’ï¼Œä¸ºLMMçš„ç°å®é—å¿˜åœºæ™¯å¼•å…¥äº†PULSEåè®®ï¼šï¼ˆiï¼‰é¢„è®­ç»ƒçŸ¥è¯†é—å¿˜ï¼Œä»¥åˆ†æä¸åŒçŸ¥è¯†è·å–é˜¶æ®µçš„å½±å“ï¼›ï¼ˆiiï¼‰é•¿æœŸå¯æŒç»­æ€§è¯„ä¼°ï¼Œä»¥è§£å†³é¡ºåºè¯·æ±‚é—®é¢˜ã€‚ç„¶åï¼Œæˆ‘ä»¬æ²¿ç€è¿™äº›ç»´åº¦è¯„ä¼°äº†ç°æœ‰çš„é—å¿˜æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°½ç®¡ä¸€äº›æŠ€æœ¯å¯ä»¥æˆåŠŸé—å¿˜é€šè¿‡å¾®è°ƒè·å¾—çš„çŸ¥è¯†ï¼Œä½†å®ƒä»¬å¾ˆéš¾æ¶ˆé™¤åœ¨é¢„è®­ç»ƒæœŸé—´å­¦ä¹ çš„ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œé‚£äº›åœ¨ä¸€æ¬¡æ“ä½œä¸­æœ‰æ•ˆé—å¿˜ä¸€æ‰¹ç›®æ ‡æ•°æ®çš„æ–¹æ³•ï¼Œå½“ç›¸åŒçš„æ•°æ®è¢«åˆ†å‰²å¹¶é¡ºåºé—å¿˜æ—¶ï¼Œä¼šè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01271v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¸­çš„éšç§å’Œç‰ˆæƒé—®é¢˜ï¼Œæå‡ºåˆ©ç”¨æ¨¡å‹â€œé—å¿˜â€å…ˆå‰å­¦ä¹ ä¿¡æ¯çš„å»å­¦ä¹ æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†é’ˆå¯¹å¤§å‹å¤šæ¨¡æ€æ¨¡å‹çš„PULSEåè®®æ¥åº”å¯¹ç°å®æƒ…å†µä¸‹çš„é—å¿˜éœ€æ±‚ï¼Œå¼•å…¥äº†ä¸¤ç§ä¸åŒçš„è§‚ç‚¹æ¥è¯„ä»·ç°æœ‰çš„é—å¿˜æ–¹æ³•ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ä¸€äº›æŠ€æœ¯å¯ä»¥åœ¨å¾®è°ƒçŸ¥è¯†ä¸ŠæˆåŠŸé—å¿˜ï¼Œä½†åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¿¡æ¯é—å¿˜å´å­˜åœ¨é—®é¢˜ã€‚åŒæ—¶ï¼Œåœ¨åˆ†å‰²ç›¸åŒæ•°æ®å¹¶è¿›è¡Œè¿ç»­é—å¿˜æ“ä½œæ—¶ï¼Œæ•ˆæœå¯èƒ½ä¼šå¤§å¹…é™ä½ã€‚æ•´ä½“è€Œè¨€ï¼Œä¸ºLMMsè®¾è®¡å‡ºçµæ´»é«˜æ•ˆçš„é—å¿˜åè®®ä¾ç„¶æ˜¯æŒ‘æˆ˜æ€§å·¥ä½œæ–¹å‘ã€‚æœŸå¾…æ­¤å·¥ä½œå¯ä»¥ä¸ºå»ºç«‹é«˜æ•ˆç°å®çš„æ¨¡å‹å»å­¦ä¹ æ–¹æ³•æä¾›æ›´å¤šçš„è§è§£ä¸æŒ‡å¼•ã€‚å¯¹æ¨è¿›å®é™…åº”ç”¨ä»¥åŠæœªæ¥æŠ€æœ¯è¿­ä»£å…·æœ‰ç§¯ææ„ä¹‰ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä¸­çš„éšç§å’Œç‰ˆæƒé—®é¢˜å—åˆ°å…³æ³¨ã€‚å¼•å…¥å»å­¦ä¹ æ–¹æ³•æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ã€‚è¯¥æ–¹æ³•å…³æ³¨å¦‚ä½•ä½¿æ¨¡å‹â€œå¿˜è®°â€å…ˆå‰å­¦ä¹ çš„ä¿¡æ¯ã€‚ </li>
<li>æ–‡ç« æå‡ºäº†PULSEåè®®ï¼Œè¯¥åè®®ä¸ºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹æä¾›äº†åº”å¯¹ç°å®é—å¿˜éœ€æ±‚çš„è§£å†³æ–¹æ¡ˆã€‚å®ƒå¼•å…¥äº†ä¸¤ç§ä¸åŒçš„è§‚ç‚¹æ¥è¯„ä»·ç°æœ‰çš„é—å¿˜æ–¹æ³•ï¼šé¢„è®­ç»ƒçŸ¥è¯†çš„é—å¿˜å’Œé•¿æœŸå¯æŒç»­æ€§è¯„ä¼°ä»¥åº”å¯¹è¿ç»­è¯·æ±‚ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01271">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-129108e51b50172505d929a95180e9a4.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-92e8021beacd89a3722a62a26f80a163.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcae827d0a40bd837c91da96df1931bf.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1c57a6e37da8b164a7219a4b6a2e084c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-6ba0b240d6ac299d875c75b51f927ec1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7211a3c2b5f0a5ad08a533a73ab74f91.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="AIGVE-MACS-Unified-Multi-Aspect-Commenting-and-Scoring-Model-for-AI-Generated-Video-Evaluation"><a href="#AIGVE-MACS-Unified-Multi-Aspect-Commenting-and-Scoring-Model-for-AI-Generated-Video-Evaluation" class="headerlink" title="AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for   AI-Generated Video Evaluation"></a>AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for   AI-Generated Video Evaluation</h2><p><strong>Authors:Xiao Liu, Jiawei Zhang</strong></p>
<p>The rapid advancement of AI-generated video models has created a pressing need for robust and interpretable evaluation frameworks. Existing metrics are limited to producing numerical scores without explanatory comments, resulting in low interpretability and human evaluation alignment. To address those challenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video Evaluation(AIGVE), which can provide not only numerical scores but also multi-aspect language comment feedback in evaluating these generated videos. Central to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising 2,500 AI-generated videos and 22,500 human-annotated detailed comments and numerical scores across nine critical evaluation aspects. Leveraging AIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a novel token-wise weighted loss and a dynamic frame sampling strategy to better align with human evaluators. Comprehensive experiments across supervised and zero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art performance in both scoring correlation and comment quality, significantly outperforming prior baselines including GPT-4o and VideoScore. In addition, we further showcase a multi-agent refinement framework where feedback from AIGVE-MACS drives iterative improvements in video generation, leading to 53.5% quality enhancement. This work establishes a new paradigm for comprehensive, human-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2 and AIGVE-MACS at <a target="_blank" rel="noopener" href="https://huggingface.co/xiaoliux/AIGVE-MACS">https://huggingface.co/xiaoliux/AIGVE-MACS</a>. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½ç”Ÿæˆè§†é¢‘æ¨¡å‹çš„å¿«é€Ÿå‘å±•å¯¹ç¨³å¥ã€å¯è§£é‡Šçš„è¯„ä»·æ¡†æ¶äº§ç”Ÿäº†è¿«åˆ‡çš„éœ€æ±‚ã€‚ç°æœ‰æŒ‡æ ‡ä»…é™äºäº§ç”Ÿæ²¡æœ‰è§£é‡Šæ€§è¯„è®ºçš„æ•°å€¼åˆ†æ•°ï¼Œå¯¼è‡´è§£é‡Šæ€§å’Œäººç±»è¯„ä»·ä¸€è‡´æ€§è¾ƒä½ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†AIGVE-MACSï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„AIç”Ÿæˆè§†é¢‘è¯„ä»·æ¨¡å‹ï¼ˆAIGVEï¼‰ï¼Œå®ƒä¸ä»…å¯ä»¥æä¾›æ•°å€¼åˆ†æ•°ï¼Œè¿˜å¯ä»¥åœ¨è¯„ä»·è¿™äº›ç”Ÿæˆçš„è§†é¢‘æ—¶æä¾›å¤šæ–¹é¢çš„è¯­è¨€è¯„è®ºåé¦ˆã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„æ ¸å¿ƒæ˜¯AIGVE-BENCH 2ï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«2500ä¸ªAIç”Ÿæˆçš„è§†é¢‘å’Œ22500ä¸ªäººç±»æ³¨é‡Šçš„è¯¦ç»†è¯„è®ºå’Œæ•°å€¼åˆ†æ•°ï¼Œæ¶µç›–ä¹ä¸ªå…³é”®è¯„ä»·æ–¹é¢ã€‚å€ŸåŠ©AIGVE-BENCH 2ï¼ŒAIGVE-MACSç»“åˆäº†æœ€æ–°çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨æ–°é¢–çš„tokenåŠ æƒæŸå¤±å’ŒåŠ¨æ€å¸§é‡‡æ ·ç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°ä¸äººç±»è¯„ä¼°è€…å¯¹é½ã€‚åœ¨ç›‘ç£å’Œé›¶åŸºå‡†åŸºå‡†æµ‹è¯•ä¸­çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒAIGVE-MACSåœ¨è¯„åˆ†å…³è”å’Œè¯„è®ºè´¨é‡æ–¹é¢éƒ½è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæ˜¾è‘—ä¼˜äºåŒ…æ‹¬GPT-4oå’ŒVideoScoreåœ¨å†…çš„å…ˆå‰åŸºå‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥å±•ç¤ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç»†åŒ–æ¡†æ¶ï¼Œå…¶ä¸­AIGVE-MACSçš„åé¦ˆé©±åŠ¨è§†é¢‘ç”Ÿæˆçš„è¿­ä»£æ”¹è¿›ï¼Œæé«˜äº†53.5%çš„è´¨é‡ã€‚è¿™é¡¹å·¥ä½œä¸ºAIç”Ÿæˆè§†é¢‘çš„å…¨é¢ã€äººç±»å¯¹é½çš„è¯„ä»·å»ºç«‹äº†æ–°çš„èŒƒå¼ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/xiaoliux/AIGVE-MACS%E4%B8%8A%E5%8F%91%E5%B8%83%E4%BA%86AIGVE-BENCH">https://huggingface.co/xiaoliux/AIGVE-MACSä¸Šå‘å¸ƒäº†AIGVE-BENCH</a> 2å’ŒAIGVE-MACSã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01255v1">PDF</a> Working in Progress</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†AIç”Ÿæˆè§†é¢‘æ¨¡å‹è¯„ä¼°çš„æŒ‘æˆ˜åŠè§£å†³æ–¹æ¡ˆã€‚é’ˆå¯¹ç°æœ‰è¯„ä¼°æ¡†æ¶è§£é‡Šæ€§ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ç»Ÿä¸€çš„AIç”Ÿæˆè§†é¢‘è¯„ä¼°æ¨¡å‹AIGVE-MACSã€‚è¯¥æ¨¡å‹ä¸ä»…èƒ½æä¾›æ•°å€¼è¯„åˆ†ï¼Œè¿˜èƒ½åœ¨è¯„ä»·ç”Ÿæˆè§†é¢‘æ—¶ç»™å‡ºå¤šæ–¹é¢çš„è¯­è¨€è¯„è®ºåé¦ˆã€‚AIGVE-BENCH 2çš„å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•æ˜¾ç¤ºï¼ŒAIGVE-MACSåœ¨è¯„åˆ†ç›¸å…³æ€§å’Œè¯„è®ºè´¨é‡æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œæ˜¾è‘—ä¼˜äºGPT-4oå’ŒVideoScoreç­‰åŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè¿˜å±•ç¤ºäº†å¤šæ™ºèƒ½ä½“ç»†åŒ–æ¡†æ¶ï¼ŒAIGVE-MACSçš„åé¦ˆé©±åŠ¨è§†é¢‘ç”Ÿæˆçš„è¿­ä»£æ”¹è¿›ï¼Œæé«˜äº†è§†é¢‘è´¨é‡ã€‚æœ¬æ–‡å»ºç«‹äº†AIç”Ÿæˆè§†é¢‘å…¨é¢ã€ç¬¦åˆäººç±»è¯„ä¼°çš„æ–°èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIç”Ÿæˆè§†é¢‘æ¨¡å‹çš„å¿«é€Ÿå‘å±•éœ€è¦æ›´å¼ºå¤§å’Œå¯è§£é‡Šçš„è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ¡†æ¶ä¸»è¦æä¾›æ•°å€¼è¯„åˆ†ï¼Œç¼ºä¹è§£é‡Šæ€§ï¼Œä¸äººç±»è¯„ä»·å¯¹é½å›°éš¾ã€‚</li>
<li>å¼•å…¥AIGVE-MACSæ¨¡å‹ï¼Œä¸ä»…æä¾›æ•°å€¼è¯„åˆ†ï¼Œè¿˜ç»™å‡ºå¤šæ–¹é¢çš„è¯­è¨€è¯„è®ºåé¦ˆã€‚</li>
<li>AIGVE-BENCH 2å¤§è§„æ¨¡åŸºå‡†æµ‹è¯•éªŒè¯äº†AIGVE-MACSæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>AIGVE-MACSåœ¨è¯„åˆ†ç›¸å…³æ€§å’Œè¯„è®ºè´¨é‡æ–¹é¢è¾¾åˆ°æœ€æ–°æ°´å¹³ï¼Œä¼˜äºå…¶ä»–åŸºçº¿æ¨¡å‹ã€‚</li>
<li>å¤šæ™ºèƒ½ä½“ç»†åŒ–æ¡†æ¶å±•ç¤ºäº†AIGVE-MACSé©±åŠ¨çš„è§†é¢‘ç”Ÿæˆè¿­ä»£æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01255">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9889688bce49fceac94ad34f7c8d9809.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0795d488a644b7e05189e572fc3e8607.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-aafb2997f77c6a3bbbd9955506f7fccc.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="GLM-4-1V-Thinking-Towards-Versatile-Multimodal-Reasoning-with-Scalable-Reinforcement-Learning"><a href="#GLM-4-1V-Thinking-Towards-Versatile-Multimodal-Reasoning-with-Scalable-Reinforcement-Learning" class="headerlink" title="GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable   Reinforcement Learning"></a>GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable   Reinforcement Learning</h2><p><strong>Authors:GLM-V Team,  :, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Wenkai Li, Wei Jia, Xin Lyu, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuxuan Zhang, Zhanxiao Du, Zhenyu Hou, Zhao Xue, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, Jie Tang</strong></p>
<p>We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to advance general-purpose multimodal understanding and reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. We then propose Reinforcement Learning with Curriculum Sampling (RLCS) to unlock the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document understanding. We open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art performance among models of comparable size. In a comprehensive evaluation across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all tasks and achieves comparable or even superior performance on 18 benchmarks relative to the significantly larger Qwen2.5-VL-72B. Notably, GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance compared to closed-source models such as GPT-4o on challenging tasks including long document understanding and STEM reasoning, further underscoring its strong capabilities. Code, models and more information are released at <a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4.1V-Thinking">https://github.com/THUDM/GLM-4.1V-Thinking</a>. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†GLM-4.1V-Thinkingï¼Œè¿™æ˜¯ä¸€æ¬¾æ—¨åœ¨æ¨åŠ¨é€šç”¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†çš„è§†è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚åœ¨è¿™ä»½æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬åˆ†äº«äº†æˆ‘ä»¬åœ¨å¼€å‘ä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„è®­ç»ƒæ¡†æ¶è¿‡ç¨‹ä¸­çš„ä¸»è¦å‘ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œå¼€å‘äº†ä¸€ä¸ªå…·æœ‰å·¨å¤§æ½œåŠ›çš„è§†è§‰åŸºç¡€æ¨¡å‹ï¼Œå¯ä»¥è¯´æ˜¯ä¸ºæœ€ç»ˆæ€§èƒ½è®¾å®šäº†ä¸Šé™ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†é‡‡ç”¨è¯¾ç¨‹é‡‡æ ·å¼ºåŒ–å­¦ä¹ ï¼ˆRLCSï¼‰çš„æ–¹æ³•ï¼Œä»¥å……åˆ†å‘æŒ¥æ¨¡å‹æ½œåŠ›ï¼Œå…¨é¢æé«˜è·¨å„ç§ä»»åŠ¡çš„å…¨é¢èƒ½åŠ›ï¼ŒåŒ…æ‹¬STEMé—®é¢˜è§£å†³ã€è§†é¢‘ç†è§£ã€å†…å®¹è¯†åˆ«ã€ç¼–ç ã€æ¥åœ°ã€åŸºäºGUIçš„ä»£ç†å’Œé•¿æ–‡æ¡£ç†è§£ã€‚æˆ‘ä»¬å¼€æºäº†GLM-4.1V-9B-Thinkingï¼Œåœ¨åŒç±»æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨æ¶µç›–çš„å…¬å…±åŸºå‡†æµ‹è¯•ä¸­å…¨é¢è¯„ä¼°è¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹æ€§èƒ½å‡ ä¹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šè¶…è¿‡äº†Qwen2.5-VL-7Bæ¨¡å‹çš„è¡¨ç°æ°´å¹³ï¼›ç›¸å¯¹æ›´å¤§è§„æ¨¡è®­ç»ƒå‡ºçš„Qwen2.5-VL-72Bæ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨å…¶ä¸­åå…«æ¡åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†ä¸ä¹‹ç›¸å½“ç”šè‡³æ›´ä¼˜çš„è¡¨ç°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒGLM-4.1V-9B-Thinkingåœ¨åŒ…æ‹¬é•¿æ–‡æ¡£ç†è§£å’ŒSTEMæ¨ç†ç­‰æŒ‘æˆ˜æ€§ä»»åŠ¡ä¸Šï¼Œç›¸è¾ƒäºé—­æºæ¨¡å‹å¦‚GPT-4oä¹Ÿè¡¨ç°å‡ºäº†ç«äº‰åŠ›æˆ–æ›´å‡ºè‰²çš„æ€§èƒ½è¡¨ç°ï¼Œè¿™è¿›ä¸€æ­¥å‡¸æ˜¾äº†å…¶å¼ºå¤§çš„èƒ½åŠ›ã€‚ç›¸å…³ä»£ç ã€æ¨¡å‹å’Œæ›´å¤šä¿¡æ¯å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4.1V-Thinking%E4%B8%8A%E3%80%82">https://github.com/THUDM/GLM-4.1V-Thinkingä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01006v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>GLM-4.1V-Thinkingæ˜¯ä¸€æ¬¾è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œæ—¨åœ¨æ¨è¿›é€šç”¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†ã€‚æœ¬ç ”ç©¶æŠ¥å‘Šåˆ†äº«äº†è¯¥æ¨ç†ä¸ºä¸­å¿ƒçš„è®­ç»ƒæ¡†æ¶å¼€å‘çš„å…³é”®å‘ç°ã€‚é¦–å…ˆï¼Œé€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œå¼€å‘äº†ä¸€ä¸ªå…·æœ‰å·¨å¤§æ½œåŠ›çš„è§†è§‰åŸºç¡€æ¨¡å‹ã€‚ç„¶åï¼Œæå‡ºä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¸è¯¾ç¨‹é‡‡æ ·ï¼ˆRLCSï¼‰æ¥é‡Šæ”¾æ¨¡å‹çš„å…¨æ½œåŠ›ï¼Œåœ¨å¤šç§ä»»åŠ¡ä¸Šå®ç°äº†å…¨é¢çš„èƒ½åŠ›å¢å¼ºï¼ŒåŒ…æ‹¬STEMé—®é¢˜è§£å†³ã€è§†é¢‘ç†è§£ã€å†…å®¹è¯†åˆ«ã€ç¼–ç ã€æ¥åœ°ã€åŸºäºGUIçš„ä»£ç†å’Œé•¿æ–‡æ¡£ç†è§£ã€‚å¼€æºçš„GLM-4.1V-9B-Thinkingåœ¨åŒç±»æ¨¡å‹ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨28ä¸ªå…¬å…±åŸºå‡†æµ‹è¯•çš„å…¨é¢è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨å‡ ä¹æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½ä¼˜äºQwen2.5-VL-7Bï¼Œåœ¨18ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½ä¸æ›´å¤§çš„Qwen2.5-VL-72Bç›¸å½“ç”šè‡³æ›´å¥½ã€‚GLM-4.1V-9B-Thinkingåœ¨åŒ…æ‹¬é•¿æ–‡æ¡£ç†è§£å’ŒSTEMæ¨ç†ç­‰å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸Šï¼Œä¸é—­æºæ¨¡å‹å¦‚GPT-4oç›¸æ¯”ä¹Ÿè¡¨ç°å‡ºç«äº‰åŠ›æˆ–æ›´å‡ºè‰²çš„æ€§èƒ½ï¼Œè¿›ä¸€æ­¥è¯æ˜äº†å…¶å¼ºå¤§çš„èƒ½åŠ›ã€‚ç›¸å…³ä¿¡æ¯å’Œæ¨¡å‹å·²å‘å¸ƒåœ¨<a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4.1V-Thinking%E3%80%82">https://github.com/THUDM/GLM-4.1V-Thinkingã€‚</a></p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>GLM-4.1V-Thinkingæ˜¯ä¸€ä¸ªæ—¨åœ¨æ¨è¿›é€šç”¨å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚</li>
<li>é€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œå»ºç«‹äº†å…·æœ‰å·¨å¤§æ½œåŠ›çš„è§†è§‰åŸºç¡€æ¨¡å‹ã€‚</li>
<li>æå‡ºå¼ºåŒ–å­¦ä¹ ä¸è¯¾ç¨‹é‡‡æ ·ï¼ˆRLCSï¼‰æ¥é‡Šæ”¾æ¨¡å‹çš„å…¨æ½œåŠ›ï¼Œå¢å¼ºå¤šç§ä»»åŠ¡èƒ½åŠ›ã€‚</li>
<li>GLM-4.1V-9B-Thinkingåœ¨å¤šé¡¹ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€ï¼ŒåŒ…æ‹¬STEMé—®é¢˜è§£å†³ã€è§†é¢‘ç†è§£ç­‰ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå¦‚Qwen2.5-VL-7Bã€‚</li>
<li>GLM-4.1V-9B-Thinkingä¸å¤§å‹é—­æºæ¨¡å‹å¦‚GPT-4oç›¸æ¯”ï¼Œå…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01006">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa119661de9cddc96473acd4ed893b50.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-122fb43127c3e9d53d03139f88b70056.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-af7bf2aa1c529e8a225055973e723919.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-864cdaa1c375cd530e8939a93fa7d977.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-07-04/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-04/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pica.zhimg.com/v2-a98cd8abec040d0225b50d9cad037d3c.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-04  How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation   Models on Standard Computer Vision Tasks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-07-04
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-07-03/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-61ad4e9b3bd8a3c0c0d14c4da4c96e16.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-07-03  How to Move Your Dragon Text-to-Motion Synthesis for Large-Vocabulary   Objects
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-07-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">27197.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
