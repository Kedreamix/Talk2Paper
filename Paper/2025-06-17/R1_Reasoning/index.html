<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-17  Schema-R1 A reasoning training approach for schema linking in   Text-to-SQL Task">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-0106ae2049aba135f37814fd5207b9e6.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-17-æ›´æ–°"><a href="#2025-06-17-æ›´æ–°" class="headerlink" title="2025-06-17 æ›´æ–°"></a>2025-06-17 æ›´æ–°</h1><h2 id="Schema-R1-A-reasoning-training-approach-for-schema-linking-in-Text-to-SQL-Task"><a href="#Schema-R1-A-reasoning-training-approach-for-schema-linking-in-Text-to-SQL-Task" class="headerlink" title="Schema-R1: A reasoning training approach for schema linking in   Text-to-SQL Task"></a>Schema-R1: A reasoning training approach for schema linking in   Text-to-SQL Task</h2><p><strong>Authors:Wuzhenghong Wen, Su Pan, yuwei Sun</strong></p>
<p>Schema linking is a critical step in Text-to-SQL task, aiming to accurately predict the table names and column names required for the SQL query based on the given question. However, current fine-tuning approaches for schema linking models employ a rote-learning paradigm, excessively optimizing for ground truth schema linking outcomes while compromising reasoning ability. This limitation arises because of the difficulty in acquiring a high-quality reasoning sample for downstream tasks. To address this, we propose Schema-R1, a reasoning schema linking model trained using reinforcement learning. Specifically, Schema-R1 consists of three key steps: constructing small batches of high-quality reasoning samples, supervised fine-tuning for cold-start initialization, and rule-based reinforcement learning training. The final results demonstrate that our method effectively enhances the reasoning ability of the schema linking model, achieving a 10% improvement in filter accuracy compared to the existing method. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/hongWin/Schema-R1/">https://github.com/hongWin/Schema-R1/</a>. </p>
<blockquote>
<p>åœ¨æ–‡æœ¬åˆ°SQLçš„ä»»åŠ¡ä¸­ï¼Œæ¨¡å¼é“¾æ¥æ˜¯å…³é”®æ­¥éª¤ï¼Œæ—¨åœ¨æ ¹æ®ç»™å®šé—®é¢˜å‡†ç¡®é¢„æµ‹SQLæŸ¥è¯¢æ‰€éœ€çš„è¡¨åå’Œåˆ—åã€‚ç„¶è€Œï¼Œå½“å‰çš„æ¨¡å¼é“¾æ¥æ¨¡å‹å¾®è°ƒæ–¹æ³•é‡‡ç”¨æœºæ¢°å­¦ä¹ æ¨¡å¼ï¼Œè¿‡åº¦ä¼˜åŒ–é’ˆå¯¹çœŸå®åœºæ™¯çš„æ¨¡å¼é“¾æ¥ç»“æœï¼Œä»è€Œç‰ºç‰²äº†æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸€å±€é™æ€§çš„äº§ç”Ÿæ˜¯ç”±äºä¸‹æ¸¸ä»»åŠ¡éš¾ä»¥è·å–é«˜è´¨é‡çš„æ¨ç†æ ·æœ¬ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æ¨¡å¼æ¨ç†é“¾æ¥æ¨¡å‹Schema-R1ã€‚å…·ä½“æ¥è¯´ï¼ŒSchema-R1åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šæ„å»ºé«˜è´¨é‡æ¨ç†æ ·æœ¬çš„å°æ‰¹æ¬¡ã€ç›‘ç£å¾®è°ƒè¿›è¡Œå†·å¯åŠ¨åˆå§‹åŒ–ã€åŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚æœ€ç»ˆç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æœ‰æ•ˆåœ°æé«˜äº†æ¨¡å¼é“¾æ¥æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¿‡æ»¤ç²¾åº¦æé«˜äº†10%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/hongWin/Schema-R1/%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/hongWin/Schema-R1/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11986v1">PDF</a> 11 pages, 3 figures, conference</p>
<p><strong>Summary</strong></p>
<p>æ–‡æœ¬ä¸»è¦ä»‹ç»äº†åœ¨æ–‡æœ¬åˆ°SQLä»»åŠ¡ä¸­ï¼Œæ¨¡å¼é“¾æ¥çš„é‡è¦æ€§åŠå…¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚å½“å‰çš„æ–¹æ³•è¿‡äºä¼˜åŒ–å¯¹æ ‡å‡†æ¨¡å¼é“¾æ¥ç»“æœçš„å­¦ä¹ ï¼Œè€Œå¿½ç•¥äº†æ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†Schema-R1æ¨¡å‹ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬æ„å»ºé«˜è´¨é‡æ¨ç†æ ·æœ¬çš„å°æ‰¹æ¬¡ã€ç›‘ç£å¾®è°ƒå†·å¯åŠ¨åˆå§‹åŒ–å’ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚æœ€ç»ˆç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•æé«˜äº†æ¨¡å¼é“¾æ¥æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œè¿‡æ»¤ç²¾åº¦æé«˜äº†10%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨¡å¼é“¾æ¥åœ¨æ–‡æœ¬åˆ°SQLä»»åŠ¡ä¸­èµ·å…³é”®ä½œç”¨ï¼Œæ—¨åœ¨é¢„æµ‹SQLæŸ¥è¯¢æ‰€éœ€çš„è¡¨åå’Œåˆ—åã€‚</li>
<li>å½“å‰æ–¹æ³•è¿‡åº¦ä¼˜åŒ–å¯¹æ ‡å‡†æ¨¡å¼é“¾æ¥ç»“æœçš„å­¦ä¹ ï¼Œå½±å“æ¨ç†èƒ½åŠ›ã€‚</li>
<li>Schema-R1æ¨¡å‹è¢«æå‡ºä»¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œé‡‡ç”¨å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒã€‚</li>
<li>Schema-R1åŒ…æ‹¬ä¸‰ä¸ªå…³é”®æ­¥éª¤ï¼šæ„å»ºé«˜è´¨é‡æ¨ç†æ ·æœ¬çš„å°æ‰¹æ¬¡ã€ç›‘ç£å¾®è°ƒå†·å¯åŠ¨åˆå§‹åŒ–å’ŒåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚</li>
<li>Schema-R1æ¨¡å‹æé«˜äº†æ¨¡å¼é“¾æ¥æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒSchema-R1æ¨¡å‹çš„è¿‡æ»¤ç²¾åº¦æé«˜äº†10%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11986">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8af659ff5ab4fbe9fff405d29e2bbe78.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-754bb15989de630ae080f3545a8f18a7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49efc63c2ce4a729efc2f95c4e069af5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-24303eb434b4d0d5dc0ab155099cb998.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TreeRL-LLM-Reinforcement-Learning-with-On-Policy-Tree-Search"><a href="#TreeRL-LLM-Reinforcement-Learning-with-On-Policy-Tree-Search" class="headerlink" title="TreeRL: LLM Reinforcement Learning with On-Policy Tree Search"></a>TreeRL: LLM Reinforcement Learning with On-Policy Tree Search</h2><p><strong>Authors:Zhenyu Hou, Ziniu Hu, Yujiang Li, Rui Lu, Jie Tang, Yuxiao Dong</strong></p>
<p>Reinforcement learning (RL) with tree search has demonstrated superior performance in traditional reasoning tasks. Compared to conventional independent chain sampling strategies with outcome supervision, tree search enables better exploration of the reasoning space and provides dense, on-policy process rewards during RL training but remains under-explored in On-Policy LLM RL. We propose TreeRL, a reinforcement learning framework that directly incorporates on-policy tree search for RL training. Our approach includes intermediate supervision and eliminates the need for a separate reward model training. Existing approaches typically train a separate process reward model, which can suffer from distribution mismatch and reward hacking. We also introduce a cost-effective tree search approach that achieves higher search efficiency under the same generation token budget by strategically branching from high-uncertainty intermediate steps rather than using random branching. Experiments on challenging math and code reasoning benchmarks demonstrate that TreeRL achieves superior performance compared to traditional ChainRL, highlighting the potential of tree search for LLM. TreeRL is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/THUDM/TreeRL">https://github.com/THUDM/TreeRL</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»“åˆæ ‘æœç´¢åœ¨ä¼ ç»Ÿæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ä¸å…·æœ‰ç»“æœç›‘ç£çš„å¸¸è§„ç‹¬ç«‹é“¾é‡‡æ ·ç­–ç•¥ç›¸æ¯”ï¼Œæ ‘æœç´¢èƒ½å¤Ÿæ›´å¥½åœ°æ¢ç´¢æ¨ç†ç©ºé—´ï¼Œå¹¶åœ¨RLè®­ç»ƒè¿‡ç¨‹ä¸­æä¾›å¯†é›†çš„åœ¨ç­–ç•¥è¿‡ç¨‹å¥–åŠ±ï¼Œä½†åœ¨On-Policy LLM RLä¸­ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†TreeRLï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç›´æ¥ç»“åˆåœ¨ç­–ç•¥æ ‘æœç´¢ç”¨äºRLè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸­é—´ç›‘ç£ï¼Œæ¶ˆé™¤äº†å¯¹å•ç‹¬å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„éœ€è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¼šè®­ç»ƒä¸€ä¸ªå•ç‹¬çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œè¿™å¯èƒ½ä¼šé­å—åˆ†å¸ƒä¸åŒ¹é…å’Œå¥–åŠ±é»‘å®¢æ”»å‡»çš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§å…·æœ‰æˆæœ¬æ•ˆç›Šçš„æ ‘æœç´¢æ–¹æ³•ï¼Œé€šè¿‡ä»é«˜ä¸ç¡®å®šæ€§çš„ä¸­é—´æ­¥éª¤è¿›è¡Œç­–ç•¥åˆ†æ”¯ï¼Œåœ¨ç›¸åŒçš„ç”Ÿæˆä»¤ç‰Œé¢„ç®—ä¸‹å®ç°æ›´é«˜çš„æœç´¢æ•ˆç‡ï¼Œè€Œä¸æ˜¯ä½¿ç”¨éšæœºåˆ†æ”¯ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTreeRLç›¸è¾ƒäºä¼ ç»Ÿçš„ChainRLå®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œçªå‡ºäº†æ ‘æœç´¢åœ¨LLMä¸­çš„æ½œåŠ›ã€‚TreeRLå·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/THUDM/TreeRL%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/THUDM/TreeRLå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11902v1">PDF</a> Accepted to ACL 2025 main conference</p>
<p><strong>Summary</strong></p>
<p>æ ‘æœç´¢å¼ºåŒ–å­¦ä¹ ï¼ˆTreeRLï¼‰åœ¨ä¼ ç»Ÿæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚ä¸ä¼ ç»Ÿç‹¬ç«‹çš„é“¾é‡‡æ ·ç­–ç•¥ç›¸æ¯”ï¼Œæ ‘æœç´¢èƒ½å¤Ÿæ›´å¥½åœ°æ¢ç´¢æ¨ç†ç©ºé—´ï¼Œå¹¶åœ¨RLè®­ç»ƒè¿‡ç¨‹ä¸­æä¾›å¯†é›†çš„ã€åœ¨çº¿ç­–ç•¥è¿‡ç¨‹å¥–åŠ±ã€‚æˆ‘ä»¬æå‡ºTreeRLï¼Œä¸€ä¸ªç›´æ¥æ•´åˆåœ¨çº¿ç­–ç•¥æ ‘æœç´¢çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºRLè®­ç»ƒã€‚æ­¤æ–¹æ³•åŒ…å«ä¸­é—´ç›‘ç£ï¼Œæ— éœ€å•ç‹¬è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œè§£å†³äº†åˆ†å¸ƒä¸åŒ¹é…å’Œå¥–åŠ±ä½œå¼Šé—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§é«˜æ•ˆçš„æ ‘æœç´¢æ–¹æ³•ï¼Œåœ¨ç›¸åŒçš„ç”Ÿæˆä»¤ç‰Œé¢„ç®—ä¸‹ï¼Œé€šè¿‡ä»é«˜ä¸ç¡®å®šæ€§çš„ä¸­é—´æ­¥éª¤è¿›è¡Œç­–ç•¥æ€§åˆ†æ”¯ï¼Œæé«˜æœç´¢æ•ˆç‡ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šï¼ŒTreeRLç›¸è¾ƒäºä¼ ç»Ÿçš„ChainRLå®ç°äº†ä¼˜è¶Šçš„æ€§èƒ½ï¼Œçªæ˜¾äº†æ ‘æœç´¢åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TreeRLç»“åˆæ ‘æœç´¢ä¸å¼ºåŒ–å­¦ä¹ ï¼Œæå‡ä¼ ç»Ÿæ¨ç†ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>ä¸ç‹¬ç«‹é“¾é‡‡æ ·ç­–ç•¥ç›¸æ¯”ï¼Œæ ‘æœç´¢èƒ½æ›´æœ‰æ•ˆåœ°æ¢ç´¢æ¨ç†ç©ºé—´ã€‚</li>
<li>TreeRLæä¾›å¯†é›†çš„åœ¨çº¿ç­–ç•¥è¿‡ç¨‹å¥–åŠ±ï¼Œæ”¹å–„RLè®­ç»ƒã€‚</li>
<li>TreeRLç»“åˆä¸­é—´ç›‘ç£ï¼Œæ— éœ€å•ç‹¬è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œè§£å†³åˆ†å¸ƒä¸åŒ¹é…å’Œå¥–åŠ±ä½œå¼Šé—®é¢˜ã€‚</li>
<li>å¼•å…¥é«˜æ•ˆæ ‘æœç´¢æ–¹æ³•ï¼Œæé«˜æœç´¢æ•ˆç‡ã€‚</li>
<li>TreeRLåœ¨æŒ‘æˆ˜æ€§çš„æ•°å­¦å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4a6aaa3f7cc4fed9931200e40748b8d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1f962b459bb680704267bc237ab367f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-efb1993a5f107c60629943dc7fb46b6c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-baa2e655819ec7fa6d760dc4b98764df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28acc9ffaaddddbf56757f955b9b91f3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bb6a4564e8be32ee51c449c933f0dcac.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MTabVQA-Evaluating-Multi-Tabular-Reasoning-of-Language-Models-in-Visual-Space"><a href="#MTabVQA-Evaluating-Multi-Tabular-Reasoning-of-Language-Models-in-Visual-Space" class="headerlink" title="MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual   Space"></a>MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual   Space</h2><p><strong>Authors:Anshul Singh, Chris Biemann, Jan Strich</strong></p>
<p>Vision-Language Models (VLMs) have demonstrated remarkable capabilities in interpreting visual layouts and text. However, a significant challenge remains in their ability to interpret robustly and reason over multi-tabular data presented as images, a common occurrence in real-world scenarios like web pages and digital documents. Existing benchmarks typically address single tables or non-visual data (text&#x2F;structured). This leaves a critical gap: they donâ€™t assess the ability to parse diverse table images, correlate information across them, and perform multi-hop reasoning on the combined visual data. We introduce MTabVQA, a novel benchmark specifically designed for multi-tabular visual question answering to bridge that gap. MTabVQA comprises 3,745 complex question-answer pairs that necessitate multi-hop reasoning across several visually rendered table images. We provide extensive benchmark results for state-of-the-art VLMs on MTabVQA, revealing significant performance limitations. We further investigate post-training techniques to enhance these reasoning abilities and release MTabVQA-Instruct, a large-scale instruction-tuning dataset. Our experiments show that fine-tuning VLMs with MTabVQA-Instruct substantially improves their performance on visual multi-tabular reasoning. Code and dataset (<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval">https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval</a>) are available online (<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/MTabVQA-EMNLP-B16E">https://anonymous.4open.science/r/MTabVQA-EMNLP-B16E</a>). </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§£è¯»è§†è§‰å¸ƒå±€å’Œæ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨è§£è¯»å¹¶æ¨ç†ä»¥å›¾åƒå½¢å¼å‘ˆç°çš„å¤šè¡¨æ ¼æ•°æ®æ–¹é¢ï¼Œä»å­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚è¿™åœ¨ç½‘é¡µå’Œæ•°å­—æ–‡æ¡£ç­‰ç°å®åœºæ™¯ä¸­æ˜¯å¾ˆå¸¸è§çš„æƒ…å†µã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸é’ˆå¯¹å•è¡¨æˆ–éè§†è§‰æ•°æ®ï¼ˆæ–‡æœ¬&#x2F;ç»“æ„åŒ–æ•°æ®ï¼‰ã€‚è¿™å°±å‡ºç°äº†ä¸€ä¸ªå…³é”®çš„ç©ºç™½ï¼šå®ƒä»¬æ— æ³•è¯„ä¼°è§£æå¤šç§è¡¨æ ¼å›¾åƒã€è·¨è¡¨å…³è”ä¿¡æ¯ä»¥åŠåœ¨åˆå¹¶çš„è§†è§‰æ•°æ®ä¸Šè¿›è¡Œå¤šæ­¥æ¨ç†çš„èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†MTabVQAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºå¤šè¡¨æ ¼è§†è§‰é—®ç­”çš„æ–°å‹åŸºå‡†æµ‹è¯•ã€‚MTabVQAåŒ…å«3745ä¸ªå¤æ‚çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œè¿™äº›é—®é¢˜éœ€è¦åœ¨å¤šä¸ªå¯è§†åŒ–çš„è¡¨æ ¼å›¾åƒä¸Šè¿›è¡Œå¤šæ­¥æ¨ç†ã€‚æˆ‘ä»¬æä¾›äº†å…³äºMTabVQAçš„æœ€å…ˆè¿›çš„VLMsçš„åŸºå‡†æµ‹è¯•ç»“æœï¼Œæ­ç¤ºäº†æ˜¾è‘—çš„æ€§èƒ½å±€é™ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†åè®­ç»ƒæŠ€æœ¯æ¥æé«˜è¿™äº›æ¨ç†èƒ½åŠ›ï¼Œå¹¶å‘å¸ƒäº†MTabVQA-Instructå¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨MTabVQA-Instructå¾®è°ƒVLMså¯ä»¥æ˜¾è‘—æé«˜å…¶åœ¨è§†è§‰å¤šè¡¨æ ¼æ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚ä»£ç å’Œæ•°æ®é›†ï¼ˆ<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval%EF%BC%89%E5%8F%AF%E5%9C%A8%E7%BD%91%E4%B8%8A%E6%89%BE%E5%88%B0%EF%BC%88https://anonymous.4open.science/r/MTabVQA-EMNLP-B16E%EF%BC%89%E3%80%82">https://huggingface.co/datasets/mtabvqa/MTabVQA-Evalï¼‰å¯åœ¨ç½‘ä¸Šæ‰¾åˆ°ï¼ˆhttps://anonymous.4open.science/r/MTabVQA-EMNLP-B16Eï¼‰ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11684v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†å¤šè¡¨æ ¼å›¾åƒæ•°æ®æ—¶çš„æŒ‘æˆ˜ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸åªé’ˆå¯¹å•è¡¨æ ¼æˆ–éè§†è§‰æ•°æ®ï¼Œæ— æ³•è¯„ä¼°æ¨¡å‹è§£æå¤šæ ·åŒ–è¡¨æ ¼å›¾åƒã€è·¨è¡¨æ ¼å…³è”ä¿¡æ¯ä»¥åŠåœ¨åˆå¹¶çš„è§†è§‰æ•°æ®ä¸Šè¿›è¡Œå¤šè·³æ¨ç†çš„èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†MTabVQAåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºå¤šè¡¨æ ¼è§†è§‰é—®ç­”ï¼ŒåŒ…å«3745ä¸ªéœ€è¦è¿›è¡Œå¤šè·³æ¨ç†çš„å¤æ‚é—®ç­”å¯¹ã€‚æ–‡ç« è¿˜æä¾›äº†å…ˆè¿›VLMsåœ¨MTabVQAä¸Šçš„åŸºå‡†æµ‹è¯•ç»“æœï¼Œå¹¶æ¢è®¨äº†å¢å¼ºæ¨ç†èƒ½åŠ›çš„åè®­ç»ƒæŠ€æœ¯ï¼ŒåŒæ—¶å‘å¸ƒäº†å¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†MTabVQA-Instructã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨MTabVQA-Instructå¾®è°ƒVLMsèƒ½æ˜¾è‘—æé«˜å…¶åœ¨è§†è§‰å¤šè¡¨æ ¼æ¨ç†ä¸Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨å¤„ç†å¤šè¡¨æ ¼å›¾åƒæ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•æ— æ³•è¯„ä¼°æ¨¡å‹åœ¨è§£æå¤šæ ·åŒ–è¡¨æ ¼å›¾åƒã€è·¨è¡¨æ ¼å…³è”ä¿¡æ¯ä»¥åŠå¤šè·³æ¨ç†æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥MTabVQAåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºå¤šè¡¨æ ¼è§†è§‰é—®ç­”ï¼ŒåŒ…å«å¤æ‚é—®ç­”å¯¹ï¼Œéœ€è¦å¤šè·³æ¨ç†ã€‚</li>
<li>å…ˆè¿›VLMsåœ¨MTabVQAä¸Šçš„åŸºå‡†æµ‹è¯•ç»“æœæ˜¾ç¤ºå­˜åœ¨æ€§èƒ½å±€é™ã€‚</li>
<li>æ¢è®¨äº†å¢å¼ºVLMsæ¨ç†èƒ½åŠ›çš„åè®­ç»ƒæŠ€æœ¯ã€‚</li>
<li>å‘å¸ƒäº†å¤§è§„æ¨¡æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†MTabVQA-Instructï¼Œç”¨äºå¾®è°ƒVLMsã€‚</li>
<li>ä½¿ç”¨MTabVQA-Instructå¾®è°ƒèƒ½æ˜¾è‘—æé«˜VLMsåœ¨è§†è§‰å¤šè¡¨æ ¼æ¨ç†ä¸Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6220ab38495ed535e18fd0d1c1d7fcd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d0bc6e05cdaf899748d95934ae7b0ba.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-9b641d0a7197e15a38deeb22e1cee230.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b4b788b78258a276e597e07e6f7a870.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-49e268b2275a2a70596186aade6a8b86.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6885bb79b9811fd3990e85a7cd11e070.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="EasyARC-Evaluating-Vision-Language-Models-on-True-Visual-Reasoning"><a href="#EasyARC-Evaluating-Vision-Language-Models-on-True-Visual-Reasoning" class="headerlink" title="EasyARC: Evaluating Vision Language Models on True Visual Reasoning"></a>EasyARC: Evaluating Vision Language Models on True Visual Reasoning</h2><p><strong>Authors:Mert Unsal, Aylin Akkus</strong></p>
<p>Building on recent advances in language-based reasoning models, we explore multimodal reasoning that integrates vision and text. Existing multimodal benchmarks primarily test visual extraction combined with text-based reasoning, lacking true visual reasoning with more complex interactions between vision and language. Inspired by the ARC challenge, we introduce EasyARC, a vision-language benchmark requiring multi-image, multi-step reasoning, and self-correction. EasyARC is procedurally generated, fully verifiable, and scalable, making it ideal for reinforcement learning (RL) pipelines. The generators incorporate progressive difficulty levels, enabling structured evaluation across task types and complexities. We benchmark state-of-the-art vision-language models and analyze their failure modes. We argue that EasyARC sets a new standard for evaluating true reasoning and test-time scaling capabilities in vision-language models. We open-source our benchmark dataset and evaluation code. </p>
<blockquote>
<p>åŸºäºè¯­è¨€æ¨ç†æ¨¡å‹çš„æœ€æ–°è¿›å±•ï¼Œæˆ‘ä»¬æ¢ç´¢äº†èåˆè§†è§‰å’Œæ–‡æœ¬çš„å¤šæ¨¡æ€æ¨ç†ã€‚ç°æœ‰çš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸»è¦æµ‹è¯•äº†ä¸æ–‡æœ¬æ¨ç†ç›¸ç»“åˆçš„è§†è§‰æå–ï¼Œç¼ºä¹åœ¨è§†è§‰å’Œè¯­è¨€ä¹‹é—´æ›´å¤æ‚äº¤äº’çš„çœŸæ­£è§†è§‰æ¨ç†ã€‚å—ARCæŒ‘æˆ˜çš„å¯å‘ï¼Œæˆ‘ä»¬å¼•å…¥äº†EasyARCï¼Œè¿™æ˜¯ä¸€ä¸ªè¦æ±‚å¤šå›¾åƒã€å¤šæ­¥éª¤æ¨ç†å’Œè‡ªè¡Œæ ¡æ­£çš„è§†å¬è¯­è¨€åŸºå‡†æµ‹è¯•ã€‚EasyARCæ˜¯ç¨‹åºç”Ÿæˆçš„ï¼Œå¯å®Œå…¨éªŒè¯å’Œæ‰©å±•ï¼Œå› æ­¤éå¸¸é€‚åˆç”¨äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®¡é“ã€‚ç”Ÿæˆå™¨èå…¥æ¸è¿›çš„éš¾åº¦çº§åˆ«ï¼Œèƒ½å¤Ÿé’ˆå¯¹ä»»åŠ¡ç±»å‹å’Œå¤æ‚æ€§è¿›è¡Œç»“æ„åŒ–è¯„ä¼°ã€‚æˆ‘ä»¬è¯„ä¼°äº†æœ€å…ˆè¿›çš„å£°éŸ³å›¾åƒè¯­è¨€æ¨¡å‹ï¼Œåˆ†æäº†å®ƒä»¬çš„æ•…éšœæ¨¡å¼ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒEasyARCä¸ºè¯„ä¼°è§†å¬è¯­è¨€æ¨¡å‹ä¸­çš„çœŸæ­£æ¨ç†èƒ½åŠ›å’Œæµ‹è¯•æ—¶æ‰©å±•èƒ½åŠ›è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚æˆ‘ä»¬å…¬å¼€äº†åŸºå‡†æ•°æ®é›†å’Œè¯„ä¼°ä»£ç çš„å¼€æºç‰ˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11595v1">PDF</a> CVPR2025 Workshop on Test-time Scaling for Computer Vision</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†åŸºäºè¯­è¨€çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬ã€‚é’ˆå¯¹å½“å‰å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬åŸºç¡€ä¸Šçš„è§†è§‰æå–æ¨ç†ä¸Šï¼Œç¼ºä¹çœŸæ­£çš„è§†è§‰æ¨ç†ä»¥åŠè§†è§‰å’Œè¯­è¨€ä¹‹é—´æ›´å¤æ‚äº¤äº’çš„é—®é¢˜ï¼Œæˆ‘ä»¬å—åˆ°ARCæŒ‘æˆ˜çš„å¯å‘ï¼Œå¼•å…¥äº†EasyARCï¼Œè¿™æ˜¯ä¸€ä¸ªéœ€è¦å¤šå›¾åƒã€å¤šæ­¥éª¤æ¨ç†å’Œè‡ªæˆ‘ä¿®æ­£çš„è§†è¯­è¨€åŸºå‡†æµ‹è¯•ã€‚EasyARCæ˜¯ç¨‹åºç”Ÿæˆçš„ï¼Œå…·æœ‰å®Œå…¨å¯éªŒè¯æ€§å’Œå¯æ‰©å±•æ€§ï¼Œéå¸¸é€‚åˆå¼ºåŒ–å­¦ä¹ ç®¡é“ã€‚æˆ‘ä»¬çš„ç”Ÿæˆå™¨èå…¥æ¸è¿›çš„éš¾åº¦çº§åˆ«ï¼Œèƒ½å¤Ÿå¯¹ä»»åŠ¡ç±»å‹å’Œå¤æ‚æ€§è¿›è¡Œç»“æ„åŒ–è¯„ä¼°ã€‚æˆ‘ä»¬å¯¹æœ€å…ˆè¿›çš„è§†è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œåˆ†æäº†å®ƒä»¬çš„å¤±è´¥æ¨¡å¼ã€‚æˆ‘ä»¬è®¤ä¸ºï¼ŒEasyARCä¸ºè¯„ä¼°è§†è¯­è¨€æ¨¡å‹ä¸­çš„çœŸæ­£æ¨ç†å’Œæµ‹è¯•æ—¶æ ‡åº¦èƒ½åŠ›è®¾å®šäº†æ–°çš„æ ‡å‡†ã€‚æˆ‘ä»¬å…¬å¼€äº†åŸºå‡†æµ‹è¯•æ•°æ®é›†å’Œè¯„ä¼°ä»£ç ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡ç« ä»‹ç»äº†åŸºäºè¯­è¨€çš„å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†è§†è§‰å’Œæ–‡æœ¬ã€‚</li>
<li>å½“å‰å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨è§†è§‰æå–ç»“åˆæ–‡æœ¬åŸºç¡€çš„æ¨ç†ï¼Œç¼ºä¹çœŸæ­£çš„è§†è§‰æ¨ç†å’Œè§†è§‰ä¸è¯­è¨€ä¹‹é—´çš„å¤æ‚äº¤äº’ã€‚</li>
<li>æå‡ºäº†EasyARCåŸºå‡†æµ‹è¯•ï¼Œè¦æ±‚å¤šå›¾åƒã€å¤šæ­¥éª¤æ¨ç†å’Œè‡ªæˆ‘ä¿®æ­£ï¼Œä»¥è¯„ä¼°è§†è¯­è¨€æ¨¡å‹çš„çœŸæ­£æ¨ç†èƒ½åŠ›ã€‚</li>
<li>EasyARCæ˜¯ç¨‹åºç”Ÿæˆçš„ã€å®Œå…¨å¯éªŒè¯å’Œå¯æ‰©å±•çš„ï¼Œé€‚åˆå¼ºåŒ–å­¦ä¹ ç®¡é“ã€‚</li>
<li>ç”Ÿæˆå™¨å…·æœ‰æ¸è¿›çš„éš¾åº¦çº§åˆ«ï¼Œå¯è¿›è¡Œç»“æ„åŒ–è¯„ä¼°ï¼Œæ¶µç›–ä»»åŠ¡ç±»å‹å’Œå¤æ‚æ€§ã€‚</li>
<li>æ–‡ç« å¯¹æœ€å…ˆè¿›çš„è§†è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶åˆ†æäº†å…¶å¤±è´¥æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11595">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-39dc2ea950c823578638a5f46fedd2a1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7e4b3c3861acca4f904b204bac7c8f36.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1f6079c35adc94e9225b60727a154267.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9b343d75c982b5d17741d5db7d72533b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-71525e240832421e2e1b19382e3dfe1e.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="RAG-Enhancing-Retrieval-Augmented-Generation-with-Application-Aware-Reasoning"><a href="#RAG-Enhancing-Retrieval-Augmented-Generation-with-Application-Aware-Reasoning" class="headerlink" title="RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware   Reasoning"></a>RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware   Reasoning</h2><p><strong>Authors:Yu Wang, Shiwan Zhao, Ming Fan, Zhihu Wang, Yubo Zhang, Xicheng Zhang, Zhengfan Wang, Heyuan Huang, Ting Liu</strong></p>
<p>The integration of external knowledge through Retrieval-Augmented Generation (RAG) has become foundational in enhancing large language models (LLMs) for knowledge-intensive tasks. However, existing RAG paradigms often overlook the cognitive step of applying knowledge, leaving a gap between retrieved facts and task-specific reasoning. In this work, we introduce RAG+, a principled and modular extension that explicitly incorporates application-aware reasoning into the RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and aligned application examples, created either manually or automatically, and retrieves both jointly during inference. This design enables LLMs not only to access relevant information but also to apply it within structured, goal-oriented reasoning processes. Experiments across mathematical, legal, and medical domains, conducted on multiple models, demonstrate that RAG+ consistently outperforms standard RAG variants, achieving average improvements of 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval with actionable application, RAG+ advances a more cognitively grounded framework for knowledge integration, representing a step toward more interpretable and capable LLMs. </p>
<blockquote>
<p>é€šè¿‡æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ•´åˆå¤–éƒ¨çŸ¥è¯†ï¼Œå·²æˆä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šçš„è¡¨ç°çš„åŸºç¡€ã€‚ç„¶è€Œï¼Œç°æœ‰çš„RAGèŒƒå¼å¾€å¾€å¿½è§†äº†åº”ç”¨çŸ¥è¯†çš„è®¤çŸ¥æ­¥éª¤ï¼Œå¯¼è‡´æ£€ç´¢åˆ°çš„äº‹å®ä¸ç‰¹å®šä»»åŠ¡çš„æ¨ç†ä¹‹é—´å­˜åœ¨å·®è·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†RAG+ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰åŸåˆ™çš„å’Œæ¨¡å—åŒ–çš„æ‰©å±•ï¼Œå®ƒæ˜¾å¼åœ°å°†åº”ç”¨æ„ŸçŸ¥æ¨ç†çº³å…¥RAGç®¡é“ã€‚RAG+æ„å»ºäº†ä¸€ä¸ªç”±çŸ¥è¯†å’Œå¯¹é½çš„åº”ç”¨ç¤ºä¾‹ç»„æˆçš„åŒè¯­æ–™åº“ï¼Œè¿™äº›ç¤ºä¾‹å¯ä»¥æ˜¯æ‰‹åŠ¨æˆ–è‡ªåŠ¨åˆ›å»ºçš„ï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­å…±åŒæ£€ç´¢ã€‚è¿™ç§è®¾è®¡ä½¿LLMä¸ä»…èƒ½å¤Ÿè®¿é—®ç›¸å…³ä¿¡æ¯ï¼Œè€Œä¸”è¿˜èƒ½å¤Ÿåœ¨ç»“æ„åŒ–ã€ç›®æ ‡å¯¼å‘çš„æ¨ç†è¿‡ç¨‹ä¸­åº”ç”¨å®ƒã€‚åœ¨å¤šæ¨¡å‹è¿›è¡Œçš„æ•°å­¦ã€æ³•å¾‹å’ŒåŒ»å­¦é¢†åŸŸçš„å®éªŒè¡¨æ˜ï¼ŒRAG+æŒç»­ä¼˜äºæ ‡å‡†RAGå˜ä½“ï¼Œå¹³å‡æ”¹è¿›3-5%ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸­çš„å³°å€¼å¢ç›Šé«˜è¾¾7.5%ã€‚é€šè¿‡æ¡¥æ¥æ£€ç´¢å’Œå¯æ“ä½œçš„åº”ç”¨ï¼ŒRAG+ä¸ºçŸ¥è¯†æ•´åˆæä¾›äº†ä¸€ä¸ªæ›´è®¤çŸ¥åŸºç¡€çš„æ¡†æ¶ï¼Œæœç€æ›´å…·è§£é‡Šæ€§å’Œèƒ½åŠ›çš„LLMè¿ˆå‡ºäº†ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11555v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºæ£€ç´¢å¢å¼ºçš„ç”Ÿæˆï¼ˆRAGï¼‰çš„æ”¹è¿›ç‰ˆæœ¬â€”â€”RAG+ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæ˜ç¡®åœ°èå…¥åº”ç”¨æ„ŸçŸ¥æ¨ç†ã€‚é€šè¿‡æ„å»ºåŒ…å«çŸ¥è¯†å’Œå¯¹é½åº”ç”¨ç¤ºä¾‹çš„åŒé‡è¯­æ–™åº“ï¼Œå¹¶åœ¨æ¨ç†æ—¶è”åˆæ£€ç´¢ï¼ŒRAG+ä¸ä»…ä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿè®¿é—®ç›¸å…³ä¿¡æ¯ï¼Œè¿˜èƒ½åœ¨ç»“æ„åŒ–çš„ç›®æ ‡å¯¼å‘æ¨ç†è¿‡ç¨‹ä¸­åº”ç”¨è¿™äº›çŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼ŒRAG+åœ¨å¤šä¸ªæ¨¡å‹ä¸Šè¡¨ç°ä¼˜äºæ ‡å‡†RAGå˜ä½“ï¼Œå¹³å‡æé«˜äº†3-5%çš„æ€§èƒ½ï¼Œåœ¨å¤æ‚åœºæ™¯ä¸‹æœ€é«˜æå‡äº†7.5%ã€‚è¿™ä¸ºçŸ¥è¯†æ•´åˆæä¾›äº†ä¸€ä¸ªæ›´è®¤çŸ¥åŒ–çš„æ¡†æ¶ï¼Œä½¿è¯­è¨€æ¨¡å‹æ›´å…·è§£é‡Šæ€§å’Œèƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RAG+ æ˜¯åŸºäº Retrieval-Augmented Generation (RAG) çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œæ—¨åœ¨æ˜ç¡®èå…¥åº”ç”¨æ„ŸçŸ¥æ¨ç†ã€‚</li>
<li>RAG+ é€šè¿‡æ„å»ºåŒ…å«çŸ¥è¯†å’Œå¯¹é½åº”ç”¨ç¤ºä¾‹çš„åŒé‡è¯­æ–™åº“æ¥å¼ºåŒ–æ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥åŒé‡è¯­æ–™åº“å¯ä»¥é€šè¿‡æ‰‹åŠ¨æˆ–è‡ªåŠ¨çš„æ–¹å¼åˆ›å»ºã€‚</li>
<li>åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒRAG+ èƒ½å¤Ÿè”åˆæ£€ç´¢çŸ¥è¯†å’Œåº”ç”¨ç¤ºä¾‹ã€‚</li>
<li>RAG+ ä½¿è¯­è¨€æ¨¡å‹èƒ½å¤Ÿä¸ä»…è·å–ç›¸å…³ä¿¡æ¯ï¼Œè¿˜èƒ½åœ¨ç»“æ„åŒ–çš„ã€ç›®æ ‡å¯¼å‘çš„æ¨ç†è¿‡ç¨‹ä¸­åº”ç”¨çŸ¥è¯†ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒRAG+ åœ¨å¤šä¸ªæ¨¡å‹å’Œå¤šä¸ªé¢†åŸŸï¼ˆå¦‚æ•°å­¦ã€æ³•å¾‹å’ŒåŒ»å­¦ï¼‰ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå¹³å‡æé«˜äº†3-5%çš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11555">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-38ca4a04703426474357083ff30aa2f0.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-45612a8a2820b0ba029531d3ed3f0357.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0f5206b91a7281b0bb143c786ec75696.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-38219e7e7396d2c4e7fc37f2aa64731e.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86fc29f0babb329ee0015251e381c73f.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Reviving-DSP-for-Advanced-Theorem-Proving-in-the-Era-of-Reasoning-Models"><a href="#Reviving-DSP-for-Advanced-Theorem-Proving-in-the-Era-of-Reasoning-Models" class="headerlink" title="Reviving DSP for Advanced Theorem Proving in the Era of Reasoning Models"></a>Reviving DSP for Advanced Theorem Proving in the Era of Reasoning Models</h2><p><strong>Authors:Chenrui Cao, Liangcheng Song, Zenan Li, Xinyi Le, Xian Zhang, Hui Xue, Fan Yang</strong></p>
<p>Recent advancements, such as DeepSeek-Prover-V2-671B and Kimina-Prover-Preview-72B, demonstrate a prevailing trend in leveraging reinforcement learning (RL)-based large-scale training for automated theorem proving. Surprisingly, we discover that even without any training, careful neuro-symbolic coordination of existing off-the-shelf reasoning models and tactic step provers can achieve comparable performance. This paper introduces \textbf{DSP+}, an improved version of the Draft, Sketch, and Prove framework, featuring a \emph{fine-grained and integrated} neuro-symbolic enhancement for each phase: (1) In the draft phase, we prompt reasoning models to generate concise natural-language subgoals to benefit the sketch phase, removing thinking tokens and references to human-written proofs; (2) In the sketch phase, subgoals are autoformalized with hypotheses to benefit the proving phase, and sketch lines containing syntactic errors are masked according to predefined rules; (3) In the proving phase, we tightly integrate symbolic search methods like Aesop with step provers to establish proofs for the sketch subgoals. Experimental results show that, without any additional model training or fine-tuning, DSP+ solves 80.7%, 32.8%, and 24 out of 644 problems from miniF2F, ProofNet, and PutnamBench, respectively, while requiring fewer budgets compared to state-of-the-arts. DSP+ proves \texttt{imo_2019_p1}, an IMO problem in miniF2F that is not solved by any prior work. Additionally, DSP+ generates proof patterns comprehensible by human experts, facilitating the identification of formalization errors; For example, eight wrongly formalized statements in miniF2F are discovered. Our results highlight the potential of classical reasoning patterns besides the RL-based training. All components will be open-sourced. </p>
<blockquote>
<p>æœ€è¿‘çš„è¿›å±•ï¼Œå¦‚DeepSeek-Prover-V2-671Bå’ŒKimina-Prover-Preview-72Bï¼Œæ˜¾ç¤ºå‡ºåˆ©ç”¨åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„å¤§è§„æ¨¡è®­ç»ƒè¿›è¡Œè‡ªåŠ¨å®šç†è¯æ˜çš„ä¸€ç§æµè¡Œè¶‹åŠ¿ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ²¡æœ‰ä»»ä½•è®­ç»ƒï¼Œé€šè¿‡ç°æœ‰ç°æˆçš„æ¨ç†æ¨¡å‹å’Œæˆ˜æœ¯æ­¥éª¤è¯æ˜å™¨çš„ç»†è‡´ç¥ç»ç¬¦å·åè°ƒï¼Œä¹Ÿå¯ä»¥å®ç°ç›¸å½“çš„æ€§èƒ½ã€‚æœ¬æ–‡ä»‹ç»äº†<strong>DSP+<strong>ï¼Œè¿™æ˜¯Draftã€Sketchå’ŒProveæ¡†æ¶çš„æ”¹è¿›ç‰ˆï¼Œä¸ºæ¯ä¸€é˜¶æ®µæä¾›äº†</strong>ç²¾ç»†ç²’åº¦å’Œé›†æˆçš„ç¥ç»ç¬¦å·å¢å¼º</strong>ï¼šï¼ˆ1ï¼‰åœ¨è‰ç¨¿é˜¶æ®µï¼Œæˆ‘ä»¬é¼“åŠ±æ¨ç†æ¨¡å‹ç”Ÿæˆç®€æ´çš„è‡ªç„¶è¯­è¨€å­ç›®æ ‡ï¼Œä»¥æœ‰åˆ©äºè‰å›¾é˜¶æ®µçš„è¿›è¡Œï¼ŒåŒæ—¶ç§»é™¤æ€è€ƒæ ‡è®°å’Œå‚è€ƒäººä¸ºç¼–å†™çš„è¯æ˜ï¼›ï¼ˆ2ï¼‰åœ¨è‰å›¾é˜¶æ®µï¼Œå­ç›®æ ‡ä¼šè‡ªåŠ¨å½¢å¼åŒ–ï¼Œä»¥æœ‰åˆ©äºè¯æ˜é˜¶æ®µçš„è¿›è¡Œï¼Œå¹¶ä¸”æ ¹æ®é¢„å®šçš„è§„åˆ™æ©ç›–å«æœ‰è¯­æ³•é”™è¯¯çš„è‰å›¾çº¿ï¼›ï¼ˆ3ï¼‰åœ¨è¯æ˜é˜¶æ®µï¼Œæˆ‘ä»¬ç´§å¯†é›†æˆè¯¸å¦‚Aesopä¹‹ç±»çš„ç¬¦å·æœç´¢æ–¹æ³•ä¸æ­¥éª¤è¯æ˜å™¨ï¼Œä»¥å»ºç«‹è‰å›¾å­ç›®æ ‡çš„è¯æ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— éœ€ä»»ä½•é¢å¤–çš„æ¨¡å‹è®­ç»ƒæˆ–å¾®è°ƒï¼ŒDSP+è§£å†³äº†miniF2Fä¸­çš„80.7%ã€ProofNetä¸­çš„32.8%ä»¥åŠPutnamBenchä¸­çš„24ä¸ªé—®é¢˜ã€‚ç›¸è¾ƒäºç°æœ‰æŠ€æœ¯ï¼Œå…¶æˆæœ¬æ›´ä½ã€‚DSP+è¯æ˜äº†miniF2Fä¸­çš„&#96;imo_2019_p1â€™è¿™ä¸€IMOé—®é¢˜ï¼Œæ­¤å‰æ²¡æœ‰ä»»ä½•å·¥ä½œèƒ½å¤Ÿè§£å†³è¯¥é—®é¢˜ã€‚æ­¤å¤–ï¼ŒDSP+ç”Ÿæˆäº†äººç±»ä¸“å®¶å¯ç†è§£çš„è¯æ˜æ¨¡å¼ï¼Œæœ‰åŠ©äºè¯†åˆ«å½¢å¼åŒ–é”™è¯¯ï¼›ä¾‹å¦‚ï¼Œåœ¨miniF2Fä¸­å‘ç°äº†å…«æ¡é”™è¯¯çš„å½¢å¼åŒ–é™ˆè¿°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœçªæ˜¾äº†é™¤RLè®­ç»ƒå¤–çš„ç»å…¸æ¨ç†æ¨¡å¼çš„æ½œåŠ›ã€‚æ‰€æœ‰ç»„ä»¶éƒ½å°†å¼€æºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11487v1">PDF</a> 31 pages. Associated code and results are available at   <a target="_blank" rel="noopener" href="https://github.com/microsoft/DSP-Plus">https://github.com/microsoft/DSP-Plus</a></p>
<p><strong>Summary</strong></p>
<p>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†DSP+çš„è¯ç”ŸèƒŒæ™¯ã€æ ¸å¿ƒæ–¹æ³•å’Œå®éªŒç»“æœã€‚åœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ç²¾ç»†çš„ç¥ç»ç¬¦å·åè°ƒï¼ŒDSP+å®ç°äº†ä¸å…ˆè¿›è‡ªåŠ¨åŒ–å®šç†è¯æ˜å™¨ç›¸å½“çš„æ€§èƒ½ã€‚DSP+ä¼˜åŒ–äº†Draftã€Sketchå’ŒProveä¸‰ä¸ªé˜¶æ®µï¼Œå¹¶æˆåŠŸè§£å†³äº†å¤šä¸ªé—®é¢˜ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ç”Ÿæˆäº†äººç±»ä¸“å®¶å¯ç†è§£çš„è¯æ˜æ¨¡å¼ï¼Œå¹¶å‘ç°äº†å½¢å¼åŒ–é”™è¯¯ã€‚è¯¥è®ºæ–‡å¼ºè°ƒäº†ç»å…¸æ¨ç†æ¨¡å¼çš„é‡è¦æ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶æ½œåŠ›ã€‚æ‰€æœ‰ç»„ä»¶éƒ½å°†å¼€æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ€æ–°ç ”ç©¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒï¼Œå®ç°äº†è‡ªåŠ¨åŒ–å®šç†è¯æ˜çš„è¿›æ­¥ã€‚</li>
<li>DSP+æ˜¯Draftã€Sketchå’ŒProveæ¡†æ¶çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰ç²¾ç»†çš„ç¥ç»ç¬¦å·å¢å¼ºã€‚</li>
<li>åœ¨æ— éœ€ä»»ä½•é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹ï¼ŒDSP+å®ç°äº†ä¸å…ˆè¿›è‡ªåŠ¨åŒ–å®šç†è¯æ˜å™¨ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>DSP+é€šè¿‡è‡ªåŠ¨ç”Ÿæˆç®€æ´çš„è‡ªç„¶è¯­è¨€å­ç›®æ ‡ã€è‡ªåŠ¨å½¢å¼åŒ–å­ç›®æ ‡ä»¥åŠç´§å¯†é›†æˆç¬¦å·æœç´¢æ–¹æ³•æ¥å»ºç«‹è¯æ˜ã€‚</li>
<li>DSP+æˆåŠŸè§£å†³äº†å¤šä¸ªé—®é¢˜é›†çš„é—®é¢˜ï¼Œå¹¶ç”Ÿæˆäº†äººç±»ä¸“å®¶å¯ç†è§£çš„è¯æ˜æ¨¡å¼ã€‚</li>
<li>DSP+å‘ç°äº†å½¢å¼åŒ–é”™è¯¯ï¼Œå¹¶å¼ºè°ƒäº†ç»å…¸æ¨ç†æ¨¡å¼çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11487">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-58caf10f84359310ffcae1c6fa329bc6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9051e60a551f4cbe5786845ad7fe3f59.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Med-PRM-Medical-Reasoning-Models-with-Stepwise-Guideline-verified-Process-Rewards"><a href="#Med-PRM-Medical-Reasoning-Models-with-Stepwise-Guideline-verified-Process-Rewards" class="headerlink" title="Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified   Process Rewards"></a>Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified   Process Rewards</h2><p><strong>Authors:Jaehoon Yun, Jiwoong Sohn, Jungwoo Park, Hyunjae Kim, Xiangru Tang, Yanjun Shao, Yonghoe Koo, Minhyeok Ko, Qingyu Chen, Mark Gerstein, Michael Moor, Jaewoo Kang</strong></p>
<p>Large language models have shown promise in clinical decision making, but current approaches struggle to localize and correct errors at specific steps of the reasoning process. This limitation is critical in medicine, where identifying and addressing reasoning errors is essential for accurate diagnosis and effective patient care. We introduce Med-PRM, a process reward modeling framework that leverages retrieval-augmented generation to verify each reasoning step against established medical knowledge bases. By verifying intermediate reasoning steps with evidence retrieved from clinical guidelines and literature, our model can precisely assess the reasoning quality in a fine-grained manner. Evaluations on five medical QA benchmarks and two open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art performance, with improving the performance of base models by up to 13.50% using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by integrating it in a plug-and-play fashion with strong policy models such as Meerkat, achieving over 80% accuracy on MedQA for the first time using small-scale models of 8 billion parameters. Our code and data are available at: <a target="_blank" rel="noopener" href="https://med-prm.github.io/">https://med-prm.github.io/</a> </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠå†³ç­–ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œä½†å½“å‰æ–¹æ³•éš¾ä»¥åœ¨æ¨ç†è¿‡ç¨‹çš„ç‰¹å®šæ­¥éª¤ä¸­å®šä½å’Œçº æ­£é”™è¯¯ã€‚åœ¨åŒ»å­¦é¢†åŸŸï¼Œè¿™ä¸€å±€é™è‡³å…³é‡è¦ï¼Œè¯†åˆ«å’Œè§£å†³æ¨ç†é”™è¯¯å¯¹äºå‡†ç¡®è¯Šæ–­å’Œæ²»ç–—æ‚£è€…è‡³å…³é‡è¦ã€‚æˆ‘ä»¬å¼•å…¥äº†Med-PRMï¼Œè¿™æ˜¯ä¸€ç§è¿‡ç¨‹å¥–åŠ±å»ºæ¨¡æ¡†æ¶ï¼Œåˆ©ç”¨å¢å¼ºæ£€ç´¢ç”Ÿæˆçš„æ–¹æ³•ï¼Œæ ¹æ®å»ºç«‹åŒ»å­¦çŸ¥è¯†åº“éªŒè¯æ¯ä¸€æ­¥æ¨ç†ã€‚é€šè¿‡ä»ä¸´åºŠæŒ‡å—å’Œæ–‡çŒ®ä¸­æ£€ç´¢çš„è¯æ®éªŒè¯ä¸­é—´æ¨ç†æ­¥éª¤ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ç²¾ç»†åœ°è¯„ä¼°æ¨ç†è´¨é‡ã€‚åœ¨äº”ä¸ªåŒ»ç–—é—®ç­”åŸºå‡†æµ‹è¯•å’Œä¸¤ä¸ªå¼€æ”¾å¼è¯Šæ–­ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMed-PRMå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½¿ç”¨Med-PRMæé«˜åŸºç¡€æ¨¡å‹çš„æ€§èƒ½é«˜è¾¾13.5%ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†Med-PRMä»¥å³æ’å³ç”¨æ–¹å¼ä¸å¼ºå¤§çš„æ”¿ç­–æ¨¡å‹ï¼ˆå¦‚Meerkatï¼‰é›†æˆï¼Œæˆ‘ä»¬åœ¨å°è§„æ¨¡æ¨¡å‹ï¼ˆ8äº¿å‚æ•°ï¼‰çš„MedQAä¸Šé¦–æ¬¡å®ç°äº†è¶…è¿‡80%çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://med-prm.github.io/%E6%9F%A5%E7%9C%8B%E3%80%82">https://med-prm.github.io/æŸ¥çœ‹ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11474v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠå†³ç­–ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨æ¨ç†è¿‡ç¨‹çš„ç‰¹å®šæ­¥éª¤ä¸­å®šä½å’Œçº æ­£é”™è¯¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Med-PRMæ¡†æ¶ï¼Œå®ƒåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ–¹å¼ï¼Œé€šè¿‡å¯¹ç…§åŒ»å­¦çŸ¥è¯†åº“éªŒè¯æ¯ä¸€æ­¥æ¨ç†ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿç²¾ç»†åœ°è¯„ä¼°æ¨ç†è´¨é‡ï¼Œé€šè¿‡ä»ä¸´åºŠæŒ‡å—å’Œæ–‡çŒ®ä¸­æ£€ç´¢è¯æ®æ¥éªŒè¯ä¸­é—´æ¨ç†æ­¥éª¤ã€‚åœ¨äº”ä¸ªåŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•å’Œä¸¤é¡¹å¼€æ”¾å¼è¯Šæ–­ä»»åŠ¡ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMed-PRMå®ç°äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼Œè¾ƒåŸºç¡€æ¨¡å‹æ€§èƒ½æå‡æœ€å¤šè¾¾13.5%ã€‚æ­¤å¤–ï¼Œé€šè¿‡å°†Med-PRMä¸å¼ºå¤§çš„ç­–ç•¥æ¨¡å‹ï¼ˆå¦‚Meerkatï¼‰ä»¥æ’ä»¶æ–¹å¼è¿›è¡Œé›†æˆï¼Œæˆ‘ä»¬åœ¨å°å‹æ¨¡å‹ï¼ˆ8äº¿å‚æ•°ï¼‰ä¸Šé¦–æ¬¡å®ç°äº†è¶…è¿‡80%çš„MedQAå‡†ç¡®ç‡ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å¯åœ¨<a target="_blank" rel="noopener" href="https://med-prm.github.io/">é“¾æ¥</a>æ‰¾åˆ°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸´åºŠå†³ç­–ä¸­å±•ç°æ½œåŠ›ï¼Œä½†å­˜åœ¨å®šä½å’Œçº æ­£ç‰¹å®šæ­¥éª¤é”™è¯¯çš„é—®é¢˜ã€‚</li>
<li>Med-PRMæ¡†æ¶åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆæ–¹å¼ï¼ŒéªŒè¯æ¯ä¸€æ­¥æ¨ç†ä¸åŒ»å­¦çŸ¥è¯†åº“çš„å¯¹ç…§ã€‚</li>
<li>Med-PRMèƒ½å¤Ÿç²¾ç»†è¯„ä¼°æ¨ç†è´¨é‡ï¼Œé€šè¿‡ä»ä¸´åºŠæŒ‡å—å’Œæ–‡çŒ®ä¸­æ£€ç´¢è¯æ®éªŒè¯ä¸­é—´æ­¥éª¤ã€‚</li>
<li>Med-PRMåœ¨äº”ä¸ªåŒ»å­¦é—®ç­”åŸºå‡†æµ‹è¯•å’Œä¸¤é¡¹å¼€æ”¾å¼è¯Šæ–­ä»»åŠ¡ä¸Šå®ç°æœ€å…ˆè¿›è¡¨ç°ã€‚</li>
<li>Med-PRMè¾ƒåŸºç¡€æ¨¡å‹æ€§èƒ½æå‡æœ€å¤šè¾¾13.5%ï¼Œæ˜¾ç¤ºå‡ºæ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>Med-PRMå…·æœ‰é€šç”¨æ€§ï¼Œå¯ä¸å…¶ä»–å¼ºå¤§ç­–ç•¥æ¨¡å‹ï¼ˆå¦‚Meerkatï¼‰ä»¥æ’ä»¶æ–¹å¼é›†æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11474">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-572f839991f9fc1602737e1841933399.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bc38d4a4c4c04224072103f05cfad2e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d24ce0d232b97b8cd6098fb134313e4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b0335dc522e5b1836655b99a56f55613.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Dynamic-Double-Space-Tower"><a href="#Dynamic-Double-Space-Tower" class="headerlink" title="Dynamic Double Space Tower"></a>Dynamic Double Space Tower</h2><p><strong>Authors:Weikai Sun, Shijie Song, Han Wang</strong></p>
<p>The Visual Question Answering (VQA) task requires the simultaneous understanding of image content and question semantics. However, existing methods often have difficulty handling complex reasoning scenarios due to insufficient cross-modal interaction and capturing the entity spatial relationships in the image.\cite{huang2023adaptive}\cite{liu2021comparing}\cite{guibas2021adaptive}\cite{zhang2022vsa}We studied a brand-new approach to replace the attention mechanism in order to enhance the reasoning ability of the model and its understanding of spatial relationships.Specifically, we propose a dynamic bidirectional spatial tower, which is divided into four layers to observe the image according to the principle of human gestalt vision. This naturally provides a powerful structural prior for the spatial organization between entities, enabling the model to no longer blindly search for relationships between pixels but make judgments based on more meaningful perceptual units. Change from â€œseeing imagesâ€ to â€œperceiving and organizing image contentâ€.A large number of experiments have shown that our module can be used in any other multimodal model and achieve advanced results, demonstrating its potential in spatial relationship processing.Meanwhile, the multimodal visual question-answering model July trained by our method has achieved state-of-the-art results with only 3B parameters, especially on the question-answering dataset of spatial relations. </p>
<blockquote>
<p>è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡éœ€è¦åŒæ—¶ç†è§£å›¾åƒå†…å®¹å’Œé—®é¢˜è¯­ä¹‰ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ç”±äºè·¨æ¨¡æ€äº¤äº’ä¸è¶³ä»¥åŠéš¾ä»¥æ•æ‰å›¾åƒä¸­çš„å®ä½“ç©ºé—´å…³ç³»ï¼Œå¾€å¾€éš¾ä»¥å¤„ç†å¤æ‚çš„æ¨ç†åœºæ™¯ã€‚\cite{huang2023adaptive}\cite{liu2021comparing}\cite{guibas2021adaptive}\cite{zhang2022vsa}æˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§å…¨æ–°çš„æ–¹æ³•æ¥æ›¿ä»£æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå…¶å¯¹ç©ºé—´å…³ç³»çš„ç†è§£ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€åŒå‘ç©ºé—´å¡”ï¼Œå®ƒåˆ†ä¸ºå››å±‚ï¼Œæ ¹æ®äººç±»æ ¼å¼å¡”è§†è§‰åŸç†æ¥è§‚å¯Ÿå›¾åƒã€‚è¿™ä¸ºå®ä½“ä¹‹é—´çš„ç©ºé—´ç»„ç»‡æä¾›äº†å¼ºå¤§çš„ç»“æ„å…ˆéªŒï¼Œä½¿æ¨¡å‹ä¸å†ç›²ç›®åœ°æœç´¢åƒç´ ä¹‹é—´çš„å…³ç³»ï¼Œè€Œæ˜¯åŸºäºæ›´æœ‰æ„ä¹‰çš„æ„ŸçŸ¥å•å…ƒè¿›è¡Œåˆ¤æ–­ã€‚ä»â€œçœ‹å›¾åƒâ€è½¬å˜ä¸ºâ€œæ„ŸçŸ¥å’Œç»„ç»‡å›¾åƒå†…å®¹â€ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å—å¯ä»¥åº”ç”¨äºä»»ä½•å…¶ä»–å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¹¶å–å¾—å…ˆè¿›ç»“æœï¼Œè¯æ˜äº†å…¶åœ¨ç©ºé—´å…³ç³»å¤„ç†ä¸­çš„æ½œåŠ›ã€‚åŒæ—¶ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•è®­ç»ƒçš„å¤šæ¨¡æ€è§†è§‰é—®ç­”æ¨¡å‹Julyï¼Œä»…ä½¿ç”¨3Bå‚æ•°å°±å®ç°äº†æœ€æ–°ç»“æœï¼Œå°¤å…¶åœ¨ç©ºé—´å…³ç³»çš„é—®ç­”æ•°æ®é›†ä¸Šè¡¨ç°å°¤å…¶å‡ºè‰²ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11394v1">PDF</a> </p>
<p><strong>Summary</strong><br>     è§†è§‰é—®ç­”ä»»åŠ¡éœ€åŒæ—¶ç†è§£å›¾åƒå†…å®¹ä¸é—®é¢˜è¯­ä¹‰ã€‚ç„¶è€Œç°æœ‰æ–¹æ³•å¤„ç†å¤æ‚æ¨ç†åœºæ™¯æ—¶ï¼Œå› è·¨æ¨¡æ€äº¤äº’ä¸è¶³åŠå›¾åƒå†…å®ä½“ç©ºé—´å…³ç³»æ•æ‰èƒ½åŠ›æœ‰é™è€Œè¡¨ç°å›°éš¾ã€‚ä¸ºæå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›åŠå¯¹ç©ºé—´å…³ç³»çš„ç†è§£ï¼Œç ”ç©¶äº†ä¸€ç§æ–°å‹æ–¹æ³•æ›¿ä»£æ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†åŠ¨æ€åŒå‘ç©ºé—´å¡”ï¼Œå…¶åˆ†å››å±‚ä¾æ®äººç±»æ•´ä½“è§‚åŸåˆ™è§‚å¯Ÿå›¾åƒã€‚è¿™ä¸ºå®ä½“é—´çš„ç©ºé—´ç»„ç»‡æä¾›äº†å¼ºå¤§çš„ç»“æ„å…ˆéªŒï¼Œä½¿æ¨¡å‹ä¸å†ç›²ç›®æœç´¢åƒç´ é—´çš„å…³ç³»ï¼Œè€ŒåŸºäºæ›´æœ‰æ„ä¹‰çš„æ„ŸçŸ¥å•å…ƒè¿›è¡Œåˆ¤æ–­ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å—å¯åº”ç”¨äºä»»ä½•å…¶ä»–å¤šæ¨¡æ€æ¨¡å‹å¹¶å–å¾—å…ˆè¿›ç»“æœï¼Œå°¤å…¶åœ¨ç©ºé—´å…³ç³»é—®ç­”æ•°æ®é›†ä¸Šï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•è®­ç»ƒçš„Julyå¤šæ¨¡æ€è§†è§‰é—®ç­”æ¨¡å‹ä»…3Bå‚æ•°å³è¾¾åˆ°æœ€æ–°æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰é—®ç­”ä»»åŠ¡åŒæ—¶æ¶‰åŠå›¾åƒå†…å®¹å’Œé—®é¢˜è¯­ä¹‰çš„ç†è§£ã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨å¤æ‚æ¨ç†åœºæ™¯ä¸­å­˜åœ¨è·¨æ¨¡æ€äº¤äº’ä¸è¶³å’Œå®ä½“ç©ºé—´å…³ç³»æ•æ‰èƒ½åŠ›æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€åŒå‘ç©ºé—´å¡”æ›¿ä»£æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œå¯¹ç©ºé—´å…³ç³»çš„ç†è§£ã€‚</li>
<li>åŠ¨æ€åŒå‘ç©ºé—´å¡”ä¾æ®äººç±»æ•´ä½“è§‚åŸåˆ™è®¾è®¡ï¼Œåˆ†ä¸ºå››å±‚è§‚å¯Ÿå›¾åƒã€‚</li>
<li>è¯¥æ¨¡å—å¯ä¸ºå®ä½“é—´çš„ç©ºé—´ç»„ç»‡æä¾›å¼ºå¤§çš„ç»“æ„å…ˆéªŒã€‚</li>
<li>æ¨¡å‹å¯åŸºäºæœ‰æ„ä¹‰çš„æ„ŸçŸ¥å•å…ƒè¿›è¡Œåˆ¤æ–­ï¼Œä¸å†ç›²ç›®æœç´¢åƒç´ é—´çš„å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11394">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cc094ff7de64589ddcdd939b694e53cc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f8e6d41d01a385535c9c17e8a6f6535a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e521badf2c00678c9420bce6fc8e0c0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-562e636971aa186f569138fac6aa47f7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f46f9ee4c207fcb200586fe1c1f66f41.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3f880946fa5fbecc88fd69ed9293b46e.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Shapley-Machine-A-Game-Theoretic-Framework-for-N-Agent-Ad-Hoc-Teamwork"><a href="#Shapley-Machine-A-Game-Theoretic-Framework-for-N-Agent-Ad-Hoc-Teamwork" class="headerlink" title="Shapley Machine: A Game-Theoretic Framework for N-Agent Ad Hoc Teamwork"></a>Shapley Machine: A Game-Theoretic Framework for N-Agent Ad Hoc Teamwork</h2><p><strong>Authors:Jianhong Wang, Yang Li, Samuel Kaski, Jonathan Lawry</strong></p>
<p>Open multi-agent systems are increasingly important in modeling real-world applications, such as smart grids, swarm robotics, etc. In this paper, we aim to investigate a recently proposed problem for open multi-agent systems, referred to as n-agent ad hoc teamwork (NAHT), where only a number of agents are controlled. Existing methods tend to be based on heuristic design and consequently lack theoretical rigor and ambiguous credit assignment among agents. To address these limitations, we model and solve NAHT through the lens of cooperative game theory. More specifically, we first model an open multi-agent system, characterized by its value, as an instance situated in a space of cooperative games, generated by a set of basis games. We then extend this space, along with the state space, to accommodate dynamic scenarios, thereby characterizing NAHT. Exploiting the justifiable assumption that basis game values correspond to a sequence of n-step returns with different horizons, we represent the state values for NAHT in a form similar to $\lambda$-returns. Furthermore, we derive Shapley values to allocate state values to the controlled agents, as credits for their contributions to the ad hoc team. Different from the conventional approach to shaping Shapley values in an explicit form, we shape Shapley values by fulfilling the three axioms uniquely describing them, well defined on the extended game space describing NAHT. To estimate Shapley values in dynamic scenarios, we propose a TD($\lambda$)-like algorithm. The resulting reinforcement learning (RL) algorithm is referred to as Shapley Machine. To our best knowledge, this is the first time that the concepts from cooperative game theory are directly related to RL concepts. In experiments, we demonstrate the effectiveness of Shapley Machine and verify reasonableness of our theory. </p>
<blockquote>
<p>å¼€æ”¾å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œåº”ç”¨ï¼ˆå¦‚æ™ºèƒ½ç”µç½‘ã€ç¾¤ä½“æœºå™¨äººç­‰ï¼‰ä¸­çš„é‡è¦æ€§æ—¥ç›Šå¢åŠ ã€‚æœ¬æ–‡é’ˆå¯¹å¼€æ”¾å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæå‡ºçš„ä¸€ä¸ªæ–°é—®é¢˜è¿›è¡Œç ”ç©¶ï¼Œå³næ™ºèƒ½ä½“ä¸´æ—¶ååŒä½œä¸šï¼ˆNAHTï¼‰ï¼Œå…¶ä¸­åªæœ‰éƒ¨åˆ†æ™ºèƒ½ä½“å—æ§ã€‚ç°æœ‰æ–¹æ³•å¾€å¾€åŸºäºå¯å‘å¼è®¾è®¡ï¼Œå› æ­¤ç¼ºä¹ç†è®ºä¸¥è°¨æ€§ï¼Œå¹¶ä¸”åœ¨æ™ºèƒ½ä½“ä¹‹é—´åˆ†é…ä¿¡ç”¨ä¸æ˜ç¡®ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬é€šè¿‡åˆä½œåšå¼ˆè®ºçš„è§†è§’æ¥æ„å»ºå’Œè§£å†³NAHTã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆå°†ä¸€ä¸ªå¼€æ”¾å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå»ºæ¨¡ä¸ºä¸€ä¸ªç‰¹å®šå®ä¾‹ï¼Œè¯¥å®ä¾‹ä½äºåˆä½œåšå¼ˆç©ºé—´å†…ï¼Œç”±ä¸€ç»„åŸºç¡€åšå¼ˆç”Ÿæˆã€‚ç„¶åï¼Œæˆ‘ä»¬æ‰©å±•è¿™ä¸ªç©ºé—´ä»¥åŠçŠ¶æ€ç©ºé—´ä»¥é€‚åº”åŠ¨æ€åœºæ™¯ï¼Œä»è€Œæè¿°NAHTã€‚é€šè¿‡åˆç†åˆ©ç”¨åŸºç¡€åšå¼ˆå€¼å¯¹åº”äºä¸åŒè§†é‡çš„næ­¥å›æŠ¥åºåˆ—çš„åˆç†è®ºå‡è®¾ï¼Œæˆ‘ä»¬ä»¥ç±»ä¼¼äºÎ»å›æŠ¥çš„å½¢å¼è¡¨ç¤ºNAHTçš„çŠ¶æ€å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡åˆ©ç”¨åˆä½œåšå¼ˆä¸­å”¯ä¸€æè¿°å®ƒä»¬çš„ä¸‰ä¸ªå…¬ç†æ¥å¡‘é€ Shapleyå€¼ï¼Œå°†çŠ¶æ€å€¼åˆ†é…ç»™å—æ§æ™ºèƒ½ä½“ï¼Œä½œä¸ºä»–ä»¬å¯¹ä¸´æ—¶å›¢é˜Ÿçš„è´¡çŒ®ä¿¡ç”¨ã€‚ä¸åŒäºä¼ ç»Ÿä»¥æ˜ç¡®å½¢å¼å¡‘é€ Shapleyå€¼çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨æè¿°NAHTçš„æ‰©å±•åšå¼ˆç©ºé—´ä¸­å®šä¹‰äº†å®ƒä»¬ã€‚ä¸ºäº†ä¼°è®¡åŠ¨æ€åœºæ™¯ä¸­çš„Shapleyå€¼ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç±»ä¼¼äºTD(Î»)çš„ç®—æ³•ã€‚æ‰€å¾—çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•è¢«ç§°ä¸ºShapleyæœºå™¨ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°†åˆä½œåšå¼ˆçš„æ¦‚å¿µç›´æ¥ä¸RLæ¦‚å¿µè”ç³»èµ·æ¥ã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬éªŒè¯äº†Shapleyæœºå™¨çš„æœ‰æ•ˆæ€§å¹¶éªŒè¯äº†æˆ‘ä»¬çš„ç†è®ºçš„åˆç†æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11285v1">PDF</a> 25 pages</p>
<p><strong>æ‘˜è¦</strong><br>åœ¨æ™ºèƒ½ç”µç½‘ã€ç¾¤æ™ºèƒ½æœºå™¨äººç­‰åº”ç”¨åœºæ™¯ä¸­ï¼Œå¼€æ”¾å¼å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå»ºæ¨¡å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬æ–‡æ—¨åœ¨ç ”ç©¶å¼€æ”¾å¼å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„ä¸€ä¸ªæ–°é—®é¢˜ï¼Œå³næ™ºèƒ½ä½“å³æ—¶å›¢é˜Ÿåä½œï¼ˆNAHTï¼‰ã€‚ç°æœ‰æ–¹æ³•å€¾å‘äºåŸºäºå¯å‘å¼è®¾è®¡ï¼Œç¼ºä¹ç†è®ºä¸¥è°¨æ€§å’Œæ˜ç¡®çš„æ™ºèƒ½ä½“é—´ä¿¡ç”¨åˆ†é…æœºåˆ¶ã€‚æœ¬æ–‡å°è¯•é€šè¿‡åˆä½œåšå¼ˆè®ºå¯¹NAHTè¿›è¡Œå»ºæ¨¡å’Œæ±‚è§£ã€‚é¦–å…ˆï¼Œå°†å¼€æ”¾å¼å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå»ºæ¨¡ä¸ºä¸€ä¸ªåˆä½œåšå¼ˆç©ºé—´ä¸­çš„å®ä¾‹ï¼Œè¯¥ç³»ç»Ÿç”±ä¸€ç»„åŸºç¡€æ¸¸æˆç”Ÿæˆã€‚é€šè¿‡æ‰©å±•è¿™ä¸ªç©ºé—´å’ŒçŠ¶æ€ç©ºé—´ä»¥é€‚åº”åŠ¨æ€åœºæ™¯ï¼Œæœ¬æ–‡å¯¹NAHTè¿›è¡Œäº†è¡¨å¾ã€‚åˆ©ç”¨åŸºç¡€æ¸¸æˆå€¼å¯¹åº”äºä¸åŒè§†é‡çš„næ­¥å›æŠ¥åºåˆ—çš„åˆç†å‡è®¾ï¼Œæˆ‘ä»¬ä»¥ç±»ä¼¼äºÎ»å›æŠ¥çš„å½¢å¼è¡¨ç¤ºNAHTçš„çŠ¶æ€å€¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æ»¡è¶³ä¸‰ä¸ªç‹¬ç‰¹æè¿°å®ƒä»¬çš„å…¬ç†æ¥å¡‘é€ Shapleyå€¼ï¼Œä»¥å°†çŠ¶æ€å€¼åˆ†é…ç»™å—æ§çš„æ™ºèƒ½ä½“ï¼Œä½œä¸ºä»–ä»¬å¯¹å³æ—¶å›¢é˜Ÿçš„è´¡çŒ®ä¿¡ç”¨ã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•åœ¨æ˜ç¡®å½¢å¼ä¸Šå¡‘é€ Shapleyå€¼çš„æ–¹å¼ï¼Œæˆ‘ä»¬åœ¨æè¿°NAHTçš„æ‰©å±•åšå¼ˆç©ºé—´ä¸Šå®šä¹‰äº†å®ƒä»¬ã€‚ä¸ºäº†ä¼°è®¡åŠ¨æ€åœºæ™¯ä¸­çš„Shapleyå€¼ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç±»ä¼¼äºTD(Î»)çš„ç®—æ³•ã€‚å¾—åˆ°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•è¢«ç§°ä¸ºShapley Machineã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°†åˆä½œåšå¼ˆè®ºçš„æ¦‚å¿µä¸å¼ºåŒ–å­¦ä¹ æ¦‚å¿µç›´æ¥å…³è”èµ·æ¥ã€‚å®éªŒéªŒè¯äº†Shapley Machineçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„ç†è®ºåˆç†æ€§ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼€æ”¾å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„é‡è¦æ€§ä¸æ–­æå‡ï¼Œå¦‚æ™ºèƒ½ç”µç½‘å’Œç¾¤æœºå™¨äººæŠ€æœ¯ã€‚</li>
<li>næ™ºèƒ½ä½“å³æ—¶å›¢é˜Ÿåä½œï¼ˆNAHTï¼‰é—®é¢˜æ˜¯ä¸€ä¸ªæ–°å…´çš„ç ”ç©¶æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ç¼ºä¹ç†è®ºä¸¥è°¨æ€§ï¼Œå¹¶ä¸”åœ¨æ™ºèƒ½ä½“ä¹‹é—´çš„ä¿¡ç”¨åˆ†é…ä¸Šè¡¨ç°æ¨¡ç³Šã€‚</li>
<li>æœ¬æ–‡é€šè¿‡åˆä½œåšå¼ˆè®ºå¯¹NAHTè¿›è¡Œå»ºæ¨¡ï¼Œå°†æ™ºèƒ½ä½“ç³»ç»Ÿçš„ä»·å€¼è§†ä¸ºåˆä½œåšå¼ˆçš„ä¸€ä¸ªå®ä¾‹ã€‚</li>
<li>é€šè¿‡æ‰©å±•åˆä½œåšå¼ˆç©ºé—´å’ŒçŠ¶æ€ç©ºé—´æ¥é€‚åº”åŠ¨æ€åœºæ™¯ï¼Œå¯¹NAHTçš„ç‰¹æ€§è¿›è¡Œäº†åˆ»ç”»ã€‚</li>
<li>åˆ©ç”¨åŸºç¡€æ¸¸æˆå€¼ä¸ä¸åŒè§†é‡çš„næ­¥å›æŠ¥åºåˆ—ç›¸å¯¹åº”çš„ç†å¿µï¼Œå®šä¹‰äº†ç±»ä¼¼Î»å›æŠ¥çš„çŠ¶æ€å€¼è¡¨ç¤ºã€‚</li>
<li>é‡‡ç”¨å¡‘é€ Shapleyå€¼çš„æ–¹æ³•ï¼Œé€šè¿‡æ»¡è¶³æè¿°Shapleyå€¼çš„ä¸‰ä¸ªç‹¬ç‰¹å…¬ç†ï¼Œä¸ºå—æ§æ™ºèƒ½ä½“åˆ†é…çŠ¶æ€å€¼ï¼Œä½œä¸ºå¯¹å³æ—¶å›¢é˜Ÿè´¡çŒ®çš„ä¿¡ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11285">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3583865af09b0850e5a8a85379f1759c.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MMMG-A-Massive-Multidisciplinary-Multi-Tier-Generation-Benchmark-for-Text-to-Image-Reasoning"><a href="#MMMG-A-Massive-Multidisciplinary-Multi-Tier-Generation-Benchmark-for-Text-to-Image-Reasoning" class="headerlink" title="MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for   Text-to-Image Reasoning"></a>MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for   Text-to-Image Reasoning</h2><p><strong>Authors:Yuxuan Luo, Yuhui Yuan, Junwen Chen, Haonan Cai, Ziyi Yue, Yuwei Yang, Fatima Zohra Daha, Ji Li, Zhouhui Lian</strong></p>
<p>In this paper, we introduce knowledge image generation as a new task, alongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation Benchmark (MMMG) to probe the reasoning capability of image generation models. Knowledge images have been central to human civilization and to the mechanisms of human learning â€“ a fact underscored by dual-coding theory and the picture-superiority effect. Generating such images is challenging, demanding multimodal reasoning that fuses world knowledge with pixel-level grounding into clear explanatory visuals. To enable comprehensive evaluation, MMMG offers 4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines, 6 educational levels, and diverse knowledge formats such as charts, diagrams, and mind maps. To eliminate confounding complexity during evaluation, we adopt a unified Knowledge Graph (KG) representation. Each KG explicitly delineates a target imageâ€™s core entities and their dependencies. We further introduce MMMG-Score to evaluate generated knowledge images. This metric combines factual fidelity, measured by graph-edit distance between KGs, with visual clarity assessment. Comprehensive evaluations of 16 state-of-the-art text-to-image generation models expose serious reasoning deficits â€“ low entity fidelity, weak relations, and clutter â€“ with GPT-4o achieving an MMMG-Score of only 50.20, underscoring the benchmarkâ€™s difficulty. To spur further progress, we release FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines a reasoning LLM with diffusion models and is trained on 16,000 curated knowledge image-prompt pairs. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°çš„ä»»åŠ¡â€”â€”çŸ¥è¯†å›¾åƒç”Ÿæˆï¼Œä»¥åŠå¤§è§„æ¨¡å¤šå­¦ç§‘å¤šçº§çŸ¥è¯†å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆMMMGï¼‰ï¼Œä»¥æ¢æµ‹å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚çŸ¥è¯†å›¾åƒåœ¨äººç±»æ–‡æ˜å’Œäººç±»å­¦ä¹ æœºåˆ¶ä¸­å æ®æ ¸å¿ƒåœ°ä½ï¼Œè¿™ä¸€äº‹å®ç”±åŒé‡ç¼–ç ç†è®ºå’Œå›¾åƒä¼˜åŠ¿æ•ˆåº”æ‰€å¼ºè°ƒã€‚ç”Ÿæˆè¿™æ ·çš„å›¾åƒå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå®ƒè¦æ±‚å¤šæ¨¡æ€æ¨ç†ï¼Œå°†ä¸–ç•ŒçŸ¥è¯†ä¸åƒç´ çº§åœ°é¢èåˆæˆæ¸…æ™°çš„è§£é‡Šæ€§è§†è§‰ã€‚ä¸ºäº†è¿›è¡Œç»¼åˆè¯„ä¼°ï¼ŒMMMGæä¾›äº†4456ä¸ªä¸“å®¶éªŒè¯çš„çŸ¥è¯†å›¾åƒæç¤ºå¯¹ï¼Œæ¶µç›–10ä¸ªå­¦ç§‘ï¼Œ6ä¸ªæ•™è‚²æ°´å¹³ï¼Œä»¥åŠå¤šæ ·åŒ–çš„çŸ¥è¯†å½¢å¼ï¼Œå¦‚å›¾è¡¨ã€å›¾è¡¨å’Œæ€ç»´å¯¼å›¾ã€‚ä¸ºäº†æ¶ˆé™¤è¯„ä¼°è¿‡ç¨‹ä¸­çš„æ··æ·†å¤æ‚æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨ç»Ÿä¸€çš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰è¡¨ç¤ºã€‚æ¯ä¸ªçŸ¥è¯†å›¾è°±éƒ½æ˜ç¡®æè¿°äº†ç›®æ ‡å›¾åƒçš„æ ¸å¿ƒå®ä½“åŠå…¶ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†MMMGè¯„åˆ†æ¥è¯„ä¼°ç”Ÿæˆçš„çŸ¥è¯†å›¾åƒã€‚è¯¥æŒ‡æ ‡ç»“åˆäº†äº‹å®å‡†ç¡®æ€§ï¼Œé€šè¿‡çŸ¥è¯†å›¾è°±ä¹‹é—´çš„å›¾ç¼–è¾‘è·ç¦»æ¥è¡¡é‡ï¼Œä»¥åŠè§†è§‰æ¸…æ™°åº¦çš„è¯„ä¼°ã€‚å¯¹16æ¬¾æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å…¨é¢è¯„ä¼°æ­ç¤ºäº†ä¸¥é‡çš„æ¨ç†ç¼ºé™·ï¼ŒåŒ…æ‹¬å®ä½“å¿ å®åº¦ä½ã€å…³ç³»è–„å¼±å’Œæ‚ä¹±æ— ç« çš„ç°è±¡ã€‚GPT-4oçš„MMMGè¯„åˆ†ä»…ä¸º50.20ï¼Œå‡¸æ˜¾äº†åŸºå‡†æµ‹è¯•çš„å›°éš¾ç¨‹åº¦ã€‚ä¸ºäº†æ¨åŠ¨è¿›ä¸€æ­¥çš„è¿›æ­¥ï¼Œæˆ‘ä»¬å‘å¸ƒäº†Flux-Reasonï¼ˆMMMGè¯„åˆ†ä¸º34.45ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆä¸”å¼€æ”¾çš„åŸºçº¿ï¼Œå®ƒç»“åˆäº†æ¨ç†LLMä¸æ‰©æ•£æ¨¡å‹ï¼Œå¹¶åœ¨16000ä¸ªç²¾é€‰çš„çŸ¥è¯†å›¾åƒæç¤ºå¯¹ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10963v2">PDF</a> 85 pages, 70 figures, code: <a target="_blank" rel="noopener" href="https://github.com/MMMGBench/MMMG">https://github.com/MMMGBench/MMMG</a>,   project page: <a target="_blank" rel="noopener" href="https://mmmgbench.github.io/">https://mmmgbench.github.io/</a></p>
<p><strong>Summary</strong><br>åœ¨è¿™ä¸ªæ–‡æœ¬ä¸­ï¼Œä»‹ç»äº†çŸ¥è¯†å›¾åƒç”Ÿæˆä½œä¸ºä¸€ä¸ªæ–°ä»»åŠ¡ï¼Œå¹¶ä¼´éšç€å¤§è§„æ¨¡å¤šå­¦ç§‘å¤šçº§çŸ¥è¯†å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆMMMGï¼‰æ¥æ£€æµ‹å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚çŸ¥è¯†å›¾åƒåœ¨äººç±»æ–‡æ˜å’Œäººç±»å­¦ä¹ æœºåˆ¶ä¸­å æ®é‡è¦åœ°ä½ï¼Œè¦æ±‚å›¾åƒç”Ÿæˆæ¨¡å‹éœ€è¦å…·å¤‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œèåˆä¸–ç•ŒçŸ¥è¯†ä¸åƒç´ çº§è§†è§‰å†…å®¹ç”Ÿæˆæ¸…æ™°çš„å¯è§†åŒ–è§£é‡Šã€‚MMMGæä¾›äº†åŒ…å«åä¸ªå­¦ç§‘ã€å…­ä¸ªæ•™è‚²çº§åˆ«å’Œä¸åŒçŸ¥è¯†æ ¼å¼ï¼ˆå¦‚å›¾è¡¨ã€å›¾è¡¨å’Œæ€ç»´å¯¼å›¾ç­‰ï¼‰çš„4456ä¸ªä¸“å®¶éªŒè¯çš„çŸ¥è¯†å›¾åƒæç¤ºå¯¹ï¼Œä»¥è¿›è¡Œå…¨é¢è¯„ä¼°ã€‚ä¸ºäº†ç®€åŒ–è¯„ä¼°å¤æ‚æ€§ï¼Œé‡‡ç”¨ç»Ÿä¸€çš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰è¡¨ç¤ºæ³•ã€‚æ¯ä¸ªKGæ˜ç¡®æè¿°äº†ç›®æ ‡å›¾åƒçš„æ ¸å¿ƒå®ä½“åŠå…¶ä¾èµ–å…³ç³»ã€‚æ­¤å¤–ï¼Œè¿˜ä»‹ç»äº†MMMGè¯„åˆ†æ¥è¯„ä¼°ç”Ÿæˆçš„çŸ¥è¯†å›¾åƒï¼Œè¯¥è¯„åˆ†ç»“åˆäº†åŸºäºçŸ¥è¯†å›¾è°±ç¼–è¾‘è·ç¦»çš„äº‹å®å‡†ç¡®æ€§å’Œè§†è§‰æ¸…æ™°åº¦è¯„ä¼°ã€‚å¯¹16ç§æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å…¨é¢è¯„ä¼°è¡¨æ˜ï¼Œå­˜åœ¨ä¸¥é‡çš„æ¨ç†ç¼ºé™·ï¼ŒåŒ…æ‹¬å®ä½“å¿ å®åº¦ä½ã€å…³ç³»è–„å¼±å’Œæ··ä¹±ç­‰ã€‚ä¸ºäº†æ¨åŠ¨è¿›ä¸€æ­¥çš„å‘å±•ï¼Œæ¨å‡ºäº†FLUX-Reasonï¼ˆMMMGè¯„åˆ†ä¸º34.45ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å¼€æ”¾åŸºçº¿ï¼Œç»“åˆäº†æ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹ä¸æ‰©æ•£æ¨¡å‹ï¼Œå¹¶åœ¨16000ä¸ªç²¾é€‰çš„çŸ¥è¯†å›¾åƒæç¤ºå¯¹ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å›¾åƒç”Ÿæˆè¢«å¼•å…¥ä¸ºä¸€ä¸ªæ–°ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>çŸ¥è¯†å›¾åƒåœ¨äººç±»å­¦ä¹ å’Œæ–‡æ˜ä¸­èµ·é‡è¦ä½œç”¨ï¼Œéœ€è¦æ¨¡å‹å…·å¤‡å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MMMGåŸºå‡†æµ‹è¯•æä¾›äº†åŒ…å«å¤šä¸ªå­¦ç§‘ã€æ•™è‚²çº§åˆ«å’ŒçŸ¥è¯†æ ¼å¼çš„çŸ¥è¯†å›¾åƒæç¤ºå¯¹ï¼Œä»¥å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é‡‡ç”¨çŸ¥è¯†å›¾è°±è¡¨ç¤ºæ³•æ¥ç®€åŒ–è¯„ä¼°å¤æ‚æ€§ï¼Œå¼ºè°ƒç›®æ ‡å›¾åƒçš„æ ¸å¿ƒå®ä½“å’Œä¾èµ–å…³ç³»ã€‚</li>
<li>MMMGè¯„åˆ†ç»“åˆäº†äº‹å®å‡†ç¡®æ€§å’Œè§†è§‰æ¸…æ™°åº¦è¯„ä¼°æ¥è¯„ä»·ç”Ÿæˆçš„çŸ¥è¯†å›¾åƒã€‚</li>
<li>å½“å‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹å­˜åœ¨æ¨ç†ç¼ºé™·ï¼Œå¦‚å®ä½“å¿ å®åº¦ä½ã€å…³ç³»è–„å¼±å’Œæ··ä¹±ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10963">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cd1ef773dd5440e1e028ebd89edf7970.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-759b43668dcc73b54c3573d07f8cd979.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-61f5cc35af2995dc9d941893ca19df72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45a377a146a57f5f4f3673b420023513.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6417f68225408b87a977c020f398d7f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f2970b49a2b8102ee26dc5923c90338.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Scientistsâ€™-First-Exam-Probing-Cognitive-Abilities-of-MLLM-via-Perception-Understanding-and-Reasoning"><a href="#Scientistsâ€™-First-Exam-Probing-Cognitive-Abilities-of-MLLM-via-Perception-Understanding-and-Reasoning" class="headerlink" title="Scientistsâ€™ First Exam: Probing Cognitive Abilities of MLLM via   Perception, Understanding, and Reasoning"></a>Scientistsâ€™ First Exam: Probing Cognitive Abilities of MLLM via   Perception, Understanding, and Reasoning</h2><p><strong>Authors:Yuhao Zhou, Yiheng Wang, Xuming He, Ruoyao Xiao, Zhiwei Li, Qiantai Feng, Zijie Guo, Yuejin Yang, Hao Wu, Wenxuan Huang, Jiaqi Wei, Dan Si, Xiuqi Yao, Jia Bu, Haiwen Huang, Tianfan Fu, Shixiang Tang, Ben Fei, Dongzhan Zhou, Fenghua Ling, Yan Lu, Siqi Sun, Chenhui Li, Guanjie Zheng, Jiancheng Lv, Wenlong Zhang, Lei Bai</strong></p>
<p>Scientific discoveries increasingly rely on complex multimodal reasoning based on information-intensive scientific data and domain-specific expertise. Empowered by expert-level scientific benchmarks, scientific Multimodal Large Language Models (MLLMs) hold the potential to significantly enhance this discovery process in realistic workflows. However, current scientific benchmarks mostly focus on evaluating the knowledge understanding capabilities of MLLMs, leading to an inadequate assessment of their perception and reasoning abilities. To address this gap, we present the Scientistsâ€™ First Exam (SFE) benchmark, designed to evaluate the scientific cognitive capacities of MLLMs through three interconnected levels: scientific signal perception, scientific attribute understanding, scientific comparative reasoning. Specifically, SFE comprises 830 expert-verified VQA pairs across three question types, spanning 66 multimodal tasks across five high-value disciplines. Extensive experiments reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08% and 26.52% on SFE, highlighting significant room for MLLMs to improve in scientific realms. We hope the insights obtained in SFE will facilitate further developments in AI-enhanced scientific discoveries. </p>
<blockquote>
<p>ç§‘å­¦ç ”ç©¶æ—¥ç›Šä¾èµ–äºåŸºäºä¿¡æ¯å¯†é›†çš„ç§‘å­¦æ•°æ®å’Œç‰¹å®šé¢†åŸŸä¸“ä¸šçŸ¥è¯†çš„å¤æ‚å¤šæ¨¡æ€æ¨ç†ã€‚åœ¨ä¸“å®¶çº§ç§‘å­¦åŸºå‡†æµ‹è¯•çš„æ”¯æŒä¸‹ï¼Œç§‘å­¦å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å®é™…å·¥ä½œæµç¨‹ä¸­å…·æœ‰å¢å¼ºè¿™ä¸€å‘ç°è¿‡ç¨‹çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç§‘å­¦åŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºè¯„ä¼°MLLMsçš„çŸ¥è¯†ç†è§£èƒ½åŠ›ï¼Œå¯¼è‡´å¯¹å…¶æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›çš„è¯„ä¼°ä¸è¶³ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ç§‘å­¦å®¶ç¬¬ä¸€æ¬¡è€ƒè¯•ï¼ˆSFEï¼‰åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨é€šè¿‡ä¸‰ä¸ªç›¸äº’å…³è”çš„æ°´å¹³æ¥è¯„ä¼°MLLMsçš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ï¼šç§‘å­¦ä¿¡å·æ„ŸçŸ¥ã€ç§‘å­¦å±æ€§ç†è§£ã€ç§‘å­¦æ¯”è¾ƒæ¨ç†ã€‚å…·ä½“æ¥è¯´ï¼ŒSFEåŒ…å«830ä¸ªä¸“å®¶éªŒè¯çš„é—®ç­”å¯¹ï¼Œæ¶‰åŠä¸‰ç§é—®é¢˜ç±»å‹ï¼Œæ¶µç›–66ä¸ªå¤šæ¨¡æ€ä»»åŠ¡ï¼Œæ¶‰åŠäº”ä¸ªé«˜ä»·å€¼å­¦ç§‘ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œç›®å‰æœ€å…ˆè¿›çš„GPT-o3å’ŒInternVL-3åœ¨SFEä¸Šçš„è¡¨ç°ä»…ä¸º34.08%å’Œ26.52%ï¼Œè¿™è¡¨æ˜MLLMsåœ¨ç§‘å­¦ç ”ç©¶é¢†åŸŸä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚æˆ‘ä»¬å¸Œæœ›SFEä¸­è·å¾—çš„è§è§£å°†æœ‰åŠ©äºäººå·¥æ™ºèƒ½å¢å¼ºç§‘å­¦å‘ç°çš„è¿›ä¸€æ­¥å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10521v2">PDF</a> 82 pages</p>
<p><strong>Summary</strong></p>
<p>åŸºäºä¿¡æ¯å¯†é›†å‹ç§‘å­¦æ•°æ®å’Œé¢†åŸŸç‰¹å®šä¸“ä¸šçŸ¥è¯†ï¼Œç§‘å­¦å‘ç°è¶Šæ¥è¶Šä¾èµ–äºå¤æ‚çš„è·¨æ¨¡æ€æ¨ç†ã€‚ç§‘å­¦è·¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å€ŸåŠ©ä¸“å®¶çº§ç§‘å­¦åŸºå‡†ï¼Œå…·æœ‰å¢å¼ºç°å®å·¥ä½œæµç¨‹ä¸­çš„å‘ç°è¿‡ç¨‹çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç§‘å­¦åŸºå‡†ä¸»è¦ä¾§é‡äºè¯„ä¼°MLLMsçš„çŸ¥è¯†ç†è§£èƒ½åŠ›ï¼Œå¯¼è‡´å¯¹å…¶æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›çš„è¯„ä¼°ä¸è¶³ã€‚ä¸ºè§£å†³è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ç§‘å­¦å®¶ç¬¬ä¸€æ¬¡è€ƒè¯•ï¼ˆSFEï¼‰åŸºå‡†ï¼Œæ—¨åœ¨é€šè¿‡ä¸‰ä¸ªç›¸äº’è”ç³»çš„æ°´å¹³è¯„ä¼°MLLMsçš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ï¼šç§‘å­¦ä¿¡å·æ„ŸçŸ¥ã€ç§‘å­¦å±æ€§ç†è§£ã€ç§‘å­¦æ¯”è¾ƒæ¨ç†ã€‚SFEåŒ…å«830ä¸ªä¸“å®¶éªŒè¯çš„é—®ç­”å¯¹ï¼Œè·¨è¶Šä¸‰ç§é—®é¢˜ç±»å‹å’Œ66ä¸ªè·¨æ¨¡æ€ä»»åŠ¡ï¼Œæ¶‰åŠäº”ä¸ªé«˜ä»·å€¼å­¦ç§‘ã€‚å®éªŒè¡¨æ˜ï¼Œå½“å‰æœ€å…ˆè¿›çš„GPT-o3å’ŒInternVL-3åœ¨SFEä¸Šçš„è¡¨ç°ä»…ä¸º34.08%å’Œ26.52%ï¼Œè¿™è¡¨æ˜MLLMsåœ¨ç§‘å­¦é¢†åŸŸè¿˜æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç§‘å­¦å‘ç°è¶Šæ¥è¶Šä¾èµ–å¤æ‚çš„å¤šæ¨¡æ€æ¨ç†å’Œä¿¡æ¯å¯†é›†å‹ç§‘å­¦æ•°æ®ã€‚</li>
<li>ç§‘å­¦è·¨æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å…·æœ‰æå‡ç§‘å­¦å‘ç°è¿‡ç¨‹çš„æ½œåŠ›ã€‚</li>
<li>å½“å‰çš„ç§‘å­¦åŸºå‡†ä¸»è¦è¯„ä¼°MLLMsçš„çŸ¥è¯†ç†è§£èƒ½åŠ›ï¼Œå¿½ç•¥äº†æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›çš„è¯„ä¼°ã€‚</li>
<li>ç§‘å­¦å®¶ç¬¬ä¸€æ¬¡è€ƒè¯•ï¼ˆSFEï¼‰åŸºå‡†æ—¨åœ¨å…¨é¢è¯„ä¼°MLLMsçš„ç§‘å­¦è®¤çŸ¥èƒ½åŠ›ã€‚</li>
<li>SFEåŸºå‡†åŒ…å«ä¸“å®¶éªŒè¯çš„é—®ç­”å¯¹ï¼Œè·¨è¶Šå¤šç§é—®é¢˜ç±»å‹å’Œå­¦ç§‘ã€‚</li>
<li>æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨SFEä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œè¡¨æ˜MLLMsåœ¨ç§‘å­¦é¢†åŸŸä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10521">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-42ad72cea6097d06b28b57c4b9daa101.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a970e2bbb9edf925c2b94b260c5e7e8d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-122fcd0cf5bc197d0dedc0a1a73382e6.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-40e346a7ed5d56d9af01b6b00ac708e9.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e579c00f906d25d004c5ecb1e3ff9026.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Motion-R1-Chain-of-Thought-Reasoning-and-Reinforcement-Learning-for-Human-Motion-Generation"><a href="#Motion-R1-Chain-of-Thought-Reasoning-and-Reinforcement-Learning-for-Human-Motion-Generation" class="headerlink" title="Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for   Human Motion Generation"></a>Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for   Human Motion Generation</h2><p><strong>Authors:Runqi Ouyang, Haoyun Li, Zhenyuan Zhang, Xiaofeng Wang, Zheng Zhu, Guan Huang, Xingang Wang</strong></p>
<p>Recent advances in large language models, especially in natural language understanding and reasoning, have opened new possibilities for text-to-motion generation. Although existing approaches have made notable progress in semantic alignment and motion synthesis, they often rely on end-to-end mapping strategies that fail to capture deep linguistic structures and logical reasoning. Consequently, generated motions tend to lack controllability, consistency, and diversity. To address these limitations, we propose Motion-R1, a unified motion-language modeling framework that integrates a Chain-of-Thought mechanism. By explicitly decomposing complex textual instructions into logically structured action paths, Motion-R1 provides high-level semantic guidance for motion generation, significantly enhancing the modelâ€™s ability to interpret and execute multi-step, long-horizon, and compositionally rich commands. To train our model, we adopt Group Relative Policy Optimization, a reinforcement learning algorithm designed for large models, which leverages motion quality feedback to optimize reasoning chains and motion synthesis jointly. Extensive experiments across multiple benchmark datasets demonstrate that Motion-R1 achieves competitive or superior performance compared to state-of-the-art methods, particularly in scenarios requiring nuanced semantic understanding and long-term temporal coherence. The code, model and data will be publicly available. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œæ¨ç†æ–¹é¢çš„è¿›å±•ä¸ºæ–‡æœ¬åˆ°è¿åŠ¨çš„ç”Ÿæˆæä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚å°½ç®¡ç°æœ‰æ–¹æ³•åœ¨è¯­ä¹‰å¯¹é½å’Œè¿åŠ¨åˆæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºç«¯åˆ°ç«¯çš„æ˜ å°„ç­–ç•¥ï¼Œæ— æ³•æ•æ‰æ·±å±‚è¯­è¨€ç»“æ„å’Œé€»è¾‘æ¨ç†ã€‚å› æ­¤ï¼Œç”Ÿæˆçš„è¿åŠ¨å¾€å¾€ç¼ºä¹å¯æ§æ€§ã€ä¸€è‡´æ€§å’Œå¤šæ ·æ€§ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Motion-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è¿åŠ¨è¯­è¨€å»ºæ¨¡æ¡†æ¶ï¼Œå®ƒé›†æˆäº†æ€ç»´é“¾æœºåˆ¶ã€‚é€šè¿‡å°†å¤æ‚çš„æ–‡æœ¬æŒ‡ä»¤æ˜ç¡®åœ°åˆ†è§£ä¸ºé€»è¾‘ç»“æ„åŒ–çš„è¡ŒåŠ¨è·¯å¾„ï¼ŒMotion-R1ä¸ºè¿åŠ¨ç”Ÿæˆæä¾›é«˜çº§è¯­ä¹‰æŒ‡å¯¼ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹è§£é‡Šå’Œæ‰§è¡Œå¤šæ­¥éª¤ã€é•¿æœŸå’Œç»„åˆä¸°å¯ŒæŒ‡ä»¤çš„èƒ½åŠ›ã€‚ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†é’ˆå¯¹å¤§å‹æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•â€”â€”ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œè¯¥ç®—æ³•åˆ©ç”¨è¿åŠ¨è´¨é‡åé¦ˆæ¥è”åˆä¼˜åŒ–æ¨ç†é“¾å’Œè¿åŠ¨åˆæˆã€‚åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒMotion-R1ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨éœ€è¦å¾®å¦™è¯­ä¹‰ç†è§£å’Œé•¿æœŸæ—¶é—´è¿è´¯æ€§çš„åœºæ™¯ä¸­ï¼Œå–å¾—äº†ç«äº‰æˆ–å“è¶Šçš„æ€§èƒ½ã€‚ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å°†å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10353v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆé¢†åŸŸçš„æ–°è¿›å±•ï¼Œæå‡ºäº†ä¸€ç§åä¸ºMotion-R1çš„ç»Ÿä¸€è¿åŠ¨è¯­è¨€å»ºæ¨¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»“åˆChain-of-Thoughtæœºåˆ¶ï¼Œå°†å¤æ‚çš„æ–‡æœ¬æŒ‡ä»¤åˆ†è§£ä¸ºé€»è¾‘ç»“æ„åŒ–çš„è¡ŒåŠ¨è·¯å¾„ï¼Œä¸ºè¿åŠ¨ç”Ÿæˆæä¾›é«˜çº§è¯­ä¹‰æŒ‡å¯¼ã€‚Motion-R1èƒ½å¤Ÿè§£é‡Šå¹¶æ‰§è¡Œå¤šæ­¥éª¤ã€é•¿æœŸè§†é‡å’Œç»„åˆä¸°å¯Œçš„æŒ‡ä»¤ï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ ç®—æ³•Group Relative Policy Optimizationè¿›è¡Œè®­ç»ƒï¼Œä¼˜åŒ–æ¨ç†é“¾å’Œè¿åŠ¨åˆæˆçš„è”åˆä¼˜åŒ–ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMotion-R1åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šå®ç°äº†ä¸æœ€æ–°æŠ€æœ¯ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¾®å¦™è¯­ä¹‰ç†è§£å’Œé•¿æœŸæ—¶é—´è¿è´¯æ€§çš„åœºæ™¯ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ–‡æœ¬åˆ°è¿åŠ¨ç”Ÿæˆé¢†åŸŸå–å¾—æ–°è¿›å±•ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–ç«¯å¯¹ç«¯æ˜ å°„ç­–ç•¥ï¼Œéš¾ä»¥æ•æ‰æ·±å±‚è¯­è¨€ç»“æ„å’Œé€»è¾‘æ¨ç†ã€‚</li>
<li>Motion-R1æ¡†æ¶ç»“åˆChain-of-Thoughtæœºåˆ¶ï¼Œåˆ†è§£å¤æ‚æ–‡æœ¬æŒ‡ä»¤ä¸ºé€»è¾‘ç»“æ„åŒ–çš„è¡ŒåŠ¨è·¯å¾„ã€‚</li>
<li>Motion-R1æä¾›é«˜çº§è¯­ä¹‰æŒ‡å¯¼ï¼Œå¢å¼ºæ¨¡å‹å¯¹å¤šæ­¥éª¤ã€é•¿æœŸè§†é‡å’Œç»„åˆä¸°å¯ŒæŒ‡ä»¤çš„è§£è¯»å’Œæ‰§è¡Œèƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•Group Relative Policy Optimizationè¿›è¡Œè®­ç»ƒï¼Œä¼˜åŒ–æ¨ç†é“¾å’Œè¿åŠ¨åˆæˆçš„è”åˆä¼˜åŒ–ã€‚</li>
<li>Motion-R1åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦å¾®å¦™è¯­ä¹‰ç†è§£å’Œé•¿æœŸæ—¶é—´è¿è´¯æ€§çš„åœºæ™¯ä¸­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10353">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-b5497613cd00c34e385a9366419f5a82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc821b3a22a5c622efa4c17a2912ae49.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0106ae2049aba135f37814fd5207b9e6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e23504e7be4d3153c36085bb8c475e94.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Intra-Trajectory-Consistency-for-Reward-Modeling"><a href="#Intra-Trajectory-Consistency-for-Reward-Modeling" class="headerlink" title="Intra-Trajectory Consistency for Reward Modeling"></a>Intra-Trajectory Consistency for Reward Modeling</h2><p><strong>Authors:Chaoyang Zhou, Shunyu Liu, Zengmao Wang, Di Wang, Rong-Cheng Tu, Bo Du, Dacheng Tao</strong></p>
<p>Reward models are critical for improving large language models (LLMs), particularly in reinforcement learning from human feedback (RLHF) or inference-time verification. Current reward modeling typically relies on scores of overall responses to learn the outcome rewards for the responses. However, since the response-level scores are coarse-grained supervision signals, the reward model struggles to identify the specific components within a response trajectory that truly correlate with the scores, leading to poor generalization on unseen responses. In this paper, we propose to leverage generation probabilities to establish reward consistency between processes in the response trajectory, which allows the response-level supervisory signal to propagate across processes, thereby providing additional fine-grained signals for reward learning. Building on analysis under the Bayesian framework, we develop an intra-trajectory consistency regularization to enforce that adjacent processes with higher next-token generation probability maintain more consistent rewards. We apply the proposed regularization to the advanced outcome reward model, improving its performance on RewardBench. Besides, we show that the reward model trained with the proposed regularization induces better DPO-aligned policies and achieves better best-of-N (BON) inference-time verification results. Our code is provided in <a target="_blank" rel="noopener" href="https://github.com/chaoyang101/ICRM">https://github.com/chaoyang101/ICRM</a>. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹å¯¹äºæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æˆ–æ¨ç†æ—¶é—´éªŒè¯ä¸­ã€‚å½“å‰çš„å¥–åŠ±å»ºæ¨¡é€šå¸¸ä¾èµ–äºæ•´ä½“å“åº”çš„åˆ†æ•°æ¥å­¦ä¹ å“åº”çš„ç»“æœå¥–åŠ±ã€‚ç„¶è€Œï¼Œç”±äºå“åº”çº§åˆ«çš„åˆ†æ•°æ˜¯ç²—ç²’åº¦çš„ç›‘ç£ä¿¡å·ï¼Œå¥–åŠ±æ¨¡å‹éš¾ä»¥è¯†åˆ«å“åº”è½¨è¿¹ä¸­çœŸæ­£ä¸åˆ†æ•°ç›¸å…³çš„ç‰¹å®šç»„ä»¶ï¼Œå¯¼è‡´åœ¨æœªè§è¿‡çš„å“åº”ä¸Šçš„æ³›åŒ–æ€§èƒ½è¾ƒå·®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨ç”Ÿæˆæ¦‚ç‡æ¥å»ºç«‹å“åº”è½¨è¿¹ä¸­è¿‡ç¨‹ä¹‹é—´çš„å¥–åŠ±ä¸€è‡´æ€§ï¼Œè¿™å…è®¸å“åº”çº§åˆ«çš„ç›‘ç£ä¿¡å·åœ¨è¿‡ç¨‹ä¹‹é—´ä¼ æ’­ï¼Œä»è€Œä¸ºå¥–åŠ±å­¦ä¹ æä¾›é¢å¤–çš„ç»†ç²’åº¦ä¿¡å·ã€‚æˆ‘ä»¬åœ¨è´å¶æ–¯æ¡†æ¶çš„åˆ†æåŸºç¡€ä¸Šï¼Œå¼€å‘äº†ä¸€ç§è½¨è¿¹å†…ä¸€è‡´æ€§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»¥å¼ºåˆ¶ç›¸é‚»è¿‡ç¨‹å…·æœ‰æ›´é«˜çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œç”Ÿæˆæ¦‚ç‡ï¼Œä»è€Œä¿æŒæ›´ä¸€è‡´çš„å¥–åŠ±ã€‚æˆ‘ä»¬å°†æ‰€æå‡ºçš„æ­£åˆ™åŒ–åº”ç”¨äºé«˜çº§ç»“æœå¥–åŠ±æ¨¡å‹ï¼Œåœ¨RewardBenchä¸Šæé«˜äº†å…¶æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ï¼Œä½¿ç”¨æ‰€æå‡ºæ­£åˆ™åŒ–è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ä¼šè¯±å¯¼æ›´å¥½çš„DPOå¯¹é½ç­–ç•¥ï¼Œå¹¶åœ¨æœ€ä½³Nï¼ˆBONï¼‰æ¨ç†æ—¶é—´éªŒè¯ä¸­å®ç°æ›´å¥½çš„ç»“æœã€‚æˆ‘ä»¬çš„ä»£ç ä½äº<a target="_blank" rel="noopener" href="https://github.com/chaoyang101/ICRM%E3%80%82">https://github.com/chaoyang101/ICRMã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09096v2">PDF</a> Under review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºåˆ©ç”¨ç”Ÿæˆæ¦‚ç‡å»ºç«‹å“åº”è½¨è¿¹ä¸­å¥–åŠ±æ¨¡å‹çš„ä¸€è‡´æ€§ï¼Œä»¥è§£å†³ç°æœ‰å¥–åŠ±æ¨¡å‹åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ”¹è¿›ä¸­çš„ä¸è¶³ã€‚ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹ä¾èµ–äºæ•´ä½“å“åº”çš„è¯„åˆ†æ¥å­¦ä¹ å¥–åŠ±ç»“æœï¼Œä½†è¿™ç§æ–¹å¼éš¾ä»¥è¯†åˆ«å“åº”è½¨è¿¹ä¸­çœŸæ­£ä¸è¯„åˆ†ç›¸å…³çš„ç‰¹å®šç»„ä»¶ã€‚æ–°æ–¹æ³•é€šè¿‡å¼•å…¥ç”Ÿæˆæ¦‚ç‡ï¼Œä½¿å¾—å“åº”çº§åˆ«çš„ç›‘ç£ä¿¡å·å¯ä»¥åœ¨è½¨è¿¹è¿‡ç¨‹ä¸­ä¼ æ’­ï¼Œä¸ºå¥–åŠ±å­¦ä¹ æä¾›é¢å¤–çš„ç²¾ç»†ä¿¡å·ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸€ç§åŸºäºè´å¶æ–¯æ¡†æ¶çš„è½¨è¿¹å†…ä¸€è‡´æ€§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»¥å¼ºåŒ–å…·æœ‰è¾ƒé«˜ç”Ÿæˆæ¦‚ç‡çš„ç›¸é‚»è¿‡ç¨‹ä¹‹é—´çš„å¥–åŠ±ä¸€è‡´æ€§ã€‚æ–°æ–¹æ³•æé«˜äº†å¥–åŠ±æ¨¡å‹åœ¨RewardBenchä¸Šçš„æ€§èƒ½ï¼Œå¹¶èƒ½æ›´å¥½åœ°ç”Ÿæˆä¸äººç±»åå¥½ä¸€è‡´çš„å†³ç­–ç­–ç•¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¥–åŠ±æ¨¡å‹å¯¹æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰å’Œæ¨ç†æ—¶é—´éªŒè¯æ–¹é¢ã€‚</li>
<li>å½“å‰å¥–åŠ±å»ºæ¨¡é€šå¸¸ä¾èµ–äºæ•´ä½“å“åº”çš„è¯„åˆ†æ¥å­¦ä¹ å¥–åŠ±ç»“æœï¼Œä½†è¿™ç§æ–¹å¼å­˜åœ¨å±€é™æ€§ï¼Œéš¾ä»¥è¯†åˆ«å“åº”è½¨è¿¹ä¸­çœŸæ­£ä¸è¯„åˆ†ç›¸å…³çš„éƒ¨åˆ†ã€‚</li>
<li>æœ¬æ–‡å»ºè®®åˆ©ç”¨ç”Ÿæˆæ¦‚ç‡å»ºç«‹å“åº”è½¨è¿¹ä¸­å¥–åŠ±æ¨¡å‹çš„ä¸€è‡´æ€§ã€‚</li>
<li>æ–°æ–¹æ³•å…è®¸å“åº”çº§åˆ«çš„ç›‘ç£ä¿¡å·åœ¨è½¨è¿¹è¿‡ç¨‹ä¸­ä¼ æ’­ï¼Œä¸ºå¥–åŠ±å­¦ä¹ æä¾›é¢å¤–çš„ç²¾ç»†ä¿¡å·ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§åŸºäºè´å¶æ–¯æ¡†æ¶çš„è½¨è¿¹å†…ä¸€è‡´æ€§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»¥å¼ºåŒ–ç›¸é‚»è¿‡ç¨‹ä¹‹é—´çš„å¥–åŠ±ä¸€è‡´æ€§ã€‚</li>
<li>æ–°æ–¹æ³•æé«˜äº†å¥–åŠ±æ¨¡å‹åœ¨RewardBenchä¸Šçš„æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09096">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-902ae0864cf48a194c1df3dd52ac87ef.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-df0ef3f5818cd7c76d078ffb5107a134.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-149b92e7dae17f505b8c5a2d5b406af7.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="e3-Learning-to-Explore-Enables-Extrapolation-of-Test-Time-Compute-for-LLMs"><a href="#e3-Learning-to-Explore-Enables-Extrapolation-of-Test-Time-Compute-for-LLMs" class="headerlink" title="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for   LLMs"></a>e3: Learning to Explore Enables Extrapolation of Test-Time Compute for   LLMs</h2><p><strong>Authors:Amrith Setlur, Matthew Y. R. Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, Aviral Kumar</strong></p>
<p>Test-time scaling offers a promising path to improve LLM reasoning by utilizing more compute at inference time; however, the true promise of this paradigm lies in extrapolation (i.e., improvement in performance on hard problems as LLMs keep â€œthinkingâ€ for longer, beyond the maximum token budget they were trained on). Surprisingly, we find that most existing reasoning models do not extrapolate well. We show that one way to enable extrapolation is by training the LLM to perform in-context exploration: training the LLM to effectively spend its test time budget by chaining operations (such as generation, verification, refinement, etc.), or testing multiple hypotheses before it commits to an answer. To enable in-context exploration, we identify three key ingredients as part of our recipe e3: (1) chaining skills that the base LLM has asymmetric competence in, e.g., chaining verification (easy) with generation (hard), as a way to implement in-context search; (2) leveraging â€œnegativeâ€ gradients from incorrect traces to amplify exploration during RL, resulting in longer search traces that chains additional asymmetries; and (3) coupling task difficulty with training token budget during training via a specifically-designed curriculum to structure in-context exploration. Our recipe e3 produces the best known 1.7B model according to AIMEâ€™25 and HMMTâ€™25 scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not only attains high pass@1 scores, but also improves pass@k over the base model. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾æŠ€æœ¯æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„è·¯å¾„ï¼Œé€šè¿‡åˆ©ç”¨æ›´å¤šçš„è®¡ç®—èµ„æºæ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ä¸€æ–¹æ³•çš„çœŸæ­£æ½œåŠ›åœ¨äºå¤–æ¨ï¼ˆå³ï¼Œåœ¨LLMâ€œæ€è€ƒâ€çš„æ—¶é—´è¶…è¿‡å…¶è®­ç»ƒæ—¶çš„æœ€å¤§ä»¤ç‰Œé¢„ç®—æ—¶ï¼Œåœ¨éš¾é¢˜ä¸Šçš„æ€§èƒ½æå‡ï¼‰ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å¤§å¤šæ•°ç°æœ‰çš„æ¨ç†æ¨¡å‹çš„å¤–æ¨èƒ½åŠ›å¹¶ä¸å¼ºã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§å®ç°å¤–æ¨çš„æ–¹æ³•æ˜¯è®­ç»ƒLLMè¿›è¡Œä¸Šä¸‹æ–‡æ¢ç´¢ï¼šè®­ç»ƒLLMé€šè¿‡é“¾æ¥æ“ä½œï¼ˆå¦‚ç”Ÿæˆã€éªŒè¯ã€ç²¾ç‚¼ç­‰ï¼‰æœ‰æ•ˆåœ°åˆ©ç”¨å…¶æµ‹è¯•æ—¶é—´é¢„ç®—ï¼Œæˆ–åœ¨æäº¤ç­”æ¡ˆä¹‹å‰æµ‹è¯•å¤šä¸ªå‡è®¾ã€‚ä¸ºäº†è¿›è¡Œä¸Šä¸‹æ–‡æ¢ç´¢ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸‰ä¸ªå…³é”®è¦ç´ ä½œä¸ºæˆ‘ä»¬é…æ–¹e3çš„ä¸€éƒ¨åˆ†ï¼šï¼ˆ1ï¼‰é“¾æ¥åŸºç¡€LLMå…·æœ‰ä¸å¯¹ç§°èƒ½åŠ›æŠ€èƒ½ï¼Œä¾‹å¦‚å°†éªŒè¯ï¼ˆå®¹æ˜“ï¼‰ä¸ç”Ÿæˆï¼ˆå›°éš¾ï¼‰é“¾æ¥èµ·æ¥ï¼Œä½œä¸ºå®ç°ä¸Šä¸‹æ–‡æœç´¢çš„ä¸€ç§æ–¹å¼ï¼›ï¼ˆ2ï¼‰åˆ©ç”¨é”™è¯¯è½¨è¿¹çš„â€œè´Ÿé¢â€æ¢¯åº¦æ¥å¼ºåŒ–å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ¢ç´¢ï¼Œä»è€Œäº§ç”Ÿæ›´é•¿çš„æœç´¢è½¨è¿¹å¹¶é“¾æ¥æ›´å¤šçš„ä¸å¯¹ç§°æ€§ï¼›ï¼ˆ3ï¼‰é€šè¿‡åœ¨è®­ç»ƒæœŸé—´ä¸“é—¨è®¾è®¡è¯¾ç¨‹å°†ä»»åŠ¡éš¾åº¦ä¸è®­ç»ƒä»¤ç‰Œé¢„ç®—ç›¸ç»“åˆï¼Œä»¥æ„å»ºä¸Šä¸‹æ–‡æ¢ç´¢çš„ç»“æ„ã€‚æˆ‘ä»¬çš„e3é…æ–¹ç”Ÿäº§å‡ºäº†æ ¹æ®AIMEâ€™25å’ŒHMMTâ€™25åˆ†æ•°è¡¨ç°æœ€å¥½çš„å·²çŸ¥1.7Bæ¨¡å‹ï¼Œå¹¶å¤–æ¨åˆ°è®­ç»ƒä»¤ç‰Œé¢„ç®—çš„2å€ã€‚æˆ‘ä»¬çš„e3-1.7Bæ¨¡å‹ä¸ä»…è¾¾åˆ°äº†å¾ˆé«˜çš„pass@1åˆ†æ•°ï¼Œè€Œä¸”ç›¸å¯¹äºåŸºç¡€æ¨¡å‹è¿˜æé«˜äº†pass@kåˆ†æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09026v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬æ¢è®¨äº†æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆtest-time scalingï¼‰åœ¨æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚ä½œè€…æŒ‡å‡ºï¼ŒçœŸæ­£çš„æ½œåŠ›åœ¨äºå¤–æ¨ï¼ˆextrapolationï¼‰â€”â€”å³åœ¨è¶…å‡ºè®­ç»ƒæ—¶çš„æœ€å¤§ä»¤ç‰Œé¢„ç®—çš„æƒ…å†µä¸‹ï¼ŒLLMæ€§èƒ½çš„æå‡ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ¨ç†æ¨¡å‹çš„å¤–æ¨èƒ½åŠ›ä¸ä½³ã€‚ä¸ºäº†æ”¹å–„è¿™ä¸€ç‚¹ï¼Œä½œè€…æå‡ºäº†ä¸€ç§è®­ç»ƒLLMè¿›è¡Œä¸Šä¸‹æ–‡å†…æ¢ç´¢çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æŠ€èƒ½é“¾ã€åˆ©ç”¨è´Ÿæ¢¯åº¦æ”¾å¤§å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢è¿‡ç¨‹ï¼Œä»¥åŠåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†ä»»åŠ¡éš¾åº¦ä¸è®­ç»ƒä»¤ç‰Œé¢„ç®—ç›¸ç»“åˆã€‚è¿™äº›æŠ€æœ¯å…±åŒæ„æˆäº†ä½œè€…çš„e3é…æ–¹ï¼ŒæˆåŠŸæé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶ç¼©æ”¾æ˜¯ä¸€ç§åˆ©ç”¨æ›´å¤šè®¡ç®—èµ„æºåœ¨æ¨ç†é˜¶æ®µæå‡å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æ–¹æ³•ã€‚</li>
<li>å¤–æ¨æ˜¯æµ‹è¯•æ—¶ç¼©æ”¾çš„ä¸€ä¸ªé‡è¦æ–¹é¢ï¼ŒæŒ‡åœ¨è¶…å‡ºè®­ç»ƒæ—¶çš„æœ€å¤§ä»¤ç‰Œé¢„ç®—æ—¶LLMçš„æ€§èƒ½æå‡ã€‚</li>
<li>ç°æœ‰æ¨ç†æ¨¡å‹åœ¨å¤–æ¨æ–¹é¢è¡¨ç°ä¸ä½³ã€‚</li>
<li>ä¸€ç§æ”¹å–„å¤–æ¨èƒ½åŠ›çš„æ–¹æ³•æ˜¯è®­ç»ƒLLMè¿›è¡Œä¸Šä¸‹æ–‡å†…æ¢ç´¢ï¼ŒåŒ…æ‹¬æŠ€èƒ½é“¾å’Œå¼ºåŒ–å­¦ä¹ ä¸­çš„è´Ÿæ¢¯åº¦åˆ©ç”¨ã€‚</li>
<li>æŠ€èƒ½é“¾æ˜¯å°†LLMåœ¨ä¸åŒä»»åŠ¡ä¸Šçš„ä¸å¯¹ç§°èƒ½åŠ›ç»“åˆï¼Œä¾‹å¦‚å°†ç®€å•çš„éªŒè¯ä¸å›°éš¾çš„ç”Ÿæˆç»“åˆèµ·æ¥ï¼Œå®ç°ä¸Šä¸‹æ–‡æœç´¢ã€‚</li>
<li>é€šè¿‡è®¾è®¡ä¸“é—¨çš„è¯¾ç¨‹æ¥ç»“åˆä»»åŠ¡éš¾åº¦å’Œè®­ç»ƒæ—¶çš„ä»¤ç‰Œé¢„ç®—ï¼Œä»è€Œç»“æ„åŒ–ä¸Šä¸‹æ–‡æ¢ç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf058bb1abf948d6f19bf151f597e052.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40fc0dbe113ec3791fb56f3b892fa654.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e92c894c318ce797534c1976e6e1b1fc.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ba1a7daded12a6b6c15d2d36209431d3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Lingshu-A-Generalist-Foundation-Model-for-Unified-Multimodal-Medical-Understanding-and-Reasoning"><a href="#Lingshu-A-Generalist-Foundation-Model-for-Unified-Multimodal-Medical-Understanding-and-Reasoning" class="headerlink" title="Lingshu: A Generalist Foundation Model for Unified Multimodal Medical   Understanding and Reasoning"></a>Lingshu: A Generalist Foundation Model for Unified Multimodal Medical   Understanding and Reasoning</h2><p><strong>Authors: LASA Team, Weiwen Xu, Hou Pong Chan, Long Li, Mahani Aljunied, Ruifeng Yuan, Jianyu Wang, Chenghao Xiao, Guizhen Chen, Chaoqun Liu, Zhaodonghui Li, Yu Sun, Junao Shen, Chaojun Wang, Jie Tan, Deli Zhao, Tingyang Xu, Hao Zhang, Yu Rong</strong></p>
<p>Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in understanding common visual elements, largely due to their large-scale datasets and advanced training strategies. However, their effectiveness in medical applications remains limited due to the inherent discrepancies between data and tasks in medical scenarios and those in the general domain. Concretely, existing medical MLLMs face the following critical limitations: (1) limited coverage of medical knowledge beyond imaging, (2) heightened susceptibility to hallucinations due to suboptimal data curation processes, (3) lack of reasoning capabilities tailored for complex medical scenarios. To address these challenges, we first propose a comprehensive data curation procedure that (1) efficiently acquires rich medical knowledge data not only from medical imaging but also from extensive medical texts and general-domain data; and (2) synthesizes accurate medical captions, visual question answering (VQA), and reasoning samples. As a result, we build a multimodal dataset enriched with extensive medical knowledge. Building on the curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu undergoes multi-stage training to embed medical expertise and enhance its task-solving capabilities progressively. Besides, we preliminarily explore the potential of applying reinforcement learning with verifiable rewards paradigm to enhance Lingshuâ€™s medical reasoning ability. Additionally, we develop MedEvalKit, a unified evaluation framework that consolidates leading multimodal and textual medical benchmarks for standardized, fair, and efficient model assessment. We evaluate the performance of Lingshu on three fundamental medical tasks, multimodal QA, text-based QA, and medical report generation. The results show that Lingshu consistently outperforms the existing open-source multimodal models on most tasks â€¦ </p>
<blockquote>
<p>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç†è§£å¸¸è§è§†è§‰å…ƒç´ æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œè¿™ä¸»è¦å½’åŠŸäºå…¶å¤§è§„æ¨¡æ•°æ®é›†å’Œå…ˆè¿›çš„è®­ç»ƒç­–ç•¥ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨åŒ»ç–—åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ç”±äºåŒ»ç–—åœºæ™¯ä¸­çš„æ•°æ®ä¸ä»»åŠ¡ä¸é€šç”¨é¢†åŸŸä¹‹é—´çš„å·®å¼‚è€Œå—åˆ°é™åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œç°æœ‰çš„åŒ»ç–—MLLMsé¢ä¸´ä»¥ä¸‹å…³é”®å±€é™ï¼šï¼ˆ1ï¼‰åŒ»å­¦çŸ¥è¯†è¦†ç›–èŒƒå›´å¹¿ï¼Œä½†ä¸ä»…é™äºæˆåƒï¼Œï¼ˆ2ï¼‰ç”±äºæ•°æ®æ•´ç†è¿‡ç¨‹çš„ä¸è¶³ï¼Œæ›´å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œï¼ˆ3ï¼‰ç¼ºä¹é’ˆå¯¹å¤æ‚åŒ»ç–—åœºæ™¯çš„æ¨ç†èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€é¡¹å…¨é¢çš„æ•°æ®æ•´ç†ç¨‹åºï¼Œè¯¥ç¨‹åºï¼ˆ1ï¼‰èƒ½å¤Ÿæœ‰æ•ˆåœ°è·å–ä¸°å¯Œçš„åŒ»å­¦çŸ¥è¯†æ•°æ®ï¼Œä¸ä»…æ¥è‡ªåŒ»å­¦å½±åƒï¼Œè¿˜æ¥è‡ªå¹¿æ³›çš„åŒ»å­¦æ–‡æœ¬å’Œé€šç”¨é¢†åŸŸæ•°æ®ï¼›ï¼ˆ2ï¼‰åˆæˆå‡†ç¡®çš„åŒ»å­¦æ ‡é¢˜ã€è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å’Œæ¨ç†æ ·æœ¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåŒ…å«ä¸°å¯ŒåŒ»å­¦çŸ¥è¯†çš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚åŸºäºæ•´ç†åçš„æ•°æ®ï¼Œæˆ‘ä»¬å¼•å…¥äº†æˆ‘ä»¬çš„åŒ»ç–—ä¸“ç”¨MLLMï¼šçµæ¢ã€‚çµæ¢ç»å†å¤šé˜¶æ®µè®­ç»ƒï¼ŒåµŒå…¥åŒ»å­¦ä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶é€æ­¥æé«˜ä»»åŠ¡è§£å†³èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆæ­¥æ¢ç´¢äº†åº”ç”¨å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±èŒƒå¼æ¥æé«˜çµæ¢çš„åŒ»å­¦æ¨ç†èƒ½åŠ›ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼€å‘äº†MedEvalKitï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ¡†æ¶ï¼Œå®ƒæ•´åˆäº†é¢†å…ˆçš„å¤šæ¨¡æ€å’Œæ–‡æœ¬åŒ»ç–—åŸºå‡†ï¼Œç”¨äºæ ‡å‡†åŒ–ã€å…¬å¹³å’Œé«˜æ•ˆçš„æ¨¡å‹è¯„ä¼°ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºæœ¬åŒ»ç–—ä»»åŠ¡ä¸Šè¯„ä¼°äº†çµæ¢çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€é—®ç­”ã€åŸºäºæ–‡æœ¬çš„é—®ç­”å’ŒåŒ»ç–—æŠ¥å‘Šç”Ÿæˆã€‚ç»“æœè¡¨æ˜ï¼ŒLingshuåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šçš„æ€§èƒ½éƒ½è¶…è¿‡äº†ç°æœ‰çš„å¼€æºå¤šæ¨¡æ€æ¨¡å‹â€¦â€¦</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07044v4">PDF</a> Technical Report, 53 pages, 25 tables, and 16 figures. Our webpage is   <a target="_blank" rel="noopener" href="https://alibaba-damo-academy.github.io/lingshu/">https://alibaba-damo-academy.github.io/lingshu/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»å­¦åº”ç”¨ä¸­çš„æŒ‘æˆ˜åŠå…¶é’ˆå¯¹è¿™äº›æŒ‘æˆ˜æ‰€é‡‡å–çš„åˆ›æ–°æ€§è§£å†³æ–¹æ¡ˆã€‚é’ˆå¯¹åŒ»å­¦é¢†åŸŸæ•°æ®ç‰¹æ®Šæ€§ï¼Œæå‡ºäº†å…¨é¢çš„æ•°æ®æ•´ç†ç¨‹åºï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªå¯Œå«åŒ»å­¦çŸ¥è¯†çš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚åŸºäºè¯¥æ•°æ®é›†ï¼Œè®­ç»ƒäº†å…·å¤‡åŒ»å­¦ä¸“é•¿çš„MLLMâ€”â€”Lingshuï¼Œè¯¥æ¨¡å‹é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒåµŒå…¥åŒ»å­¦ä¸“ä¸šçŸ¥è¯†å¹¶å¢å¼ºä»»åŠ¡è§£å†³èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿˜åˆæ­¥æ¢ç´¢äº†åº”ç”¨å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ æå‡LingshuåŒ»å­¦æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ï¼Œå¹¶å¼€å‘äº†ç»Ÿä¸€è¯„ä¼°æ¡†æ¶MedEvalKitï¼Œç”¨äºæ ‡å‡†åŒ–ã€å…¬å¹³ã€é«˜æ•ˆåœ°è¯„ä¼°æ¨¡å‹ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒLingshuåœ¨å¤šé¡¹åŸºç¡€åŒ»å­¦ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰å¼€æºå¤šæ¨¡æ€æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åŒ»å­¦åº”ç”¨ä¸­é¢ä¸´æ•°æ®å·®å¼‚æ€§çš„æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åŒ»å­¦MLLMsåœ¨åŒ»ç–—çŸ¥è¯†è¦†ç›–ã€æ•°æ®ä¼˜åŒ–å’Œæ¨ç†èƒ½åŠ›æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>æå‡ºäº†å…¨é¢çš„æ•°æ®æ•´ç†ç¨‹åºï¼ŒèåˆåŒ»ç–—å½±åƒã€åŒ»ç–—æ–‡æœ¬å’Œé€šç”¨é¢†åŸŸæ•°æ®ï¼Œæ„å»ºå¯Œå«åŒ»å­¦çŸ¥è¯†çš„å¤šæ¨¡æ€æ•°æ®é›†ã€‚</li>
<li>ä»‹ç»äº†åŸºäºè¯¥æ•°æ®é›†çš„åŒ»å­¦ä¸“ä¸šåŒ–MLLMâ€”â€”Lingshuï¼Œé€šè¿‡å¤šé˜¶æ®µè®­ç»ƒå¢å¼ºä»»åŠ¡è§£å†³èƒ½åŠ›ã€‚</li>
<li>åˆæ­¥æ¢ç´¢äº†åº”ç”¨å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ æå‡LingshuåŒ»å­¦æ¨ç†èƒ½åŠ›çš„æ½œåŠ›ã€‚</li>
<li>å¼€å‘äº†ç»Ÿä¸€è¯„ä¼°æ¡†æ¶MedEvalKitï¼Œç”¨äºæ ‡å‡†åŒ–ã€å…¬å¹³ã€é«˜æ•ˆè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-67ba85b7b2b12f1ab3169608809ce637.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f02dac1167914448cae29244099f10ab.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5448d30d0d634b046291230c47a2cf6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-7590e781b721d46f2b22ce92130a57c1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-01941c2409268ce14bcbeda96f46cd64.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Does-Thinking-More-always-Help-Understanding-Test-Time-Scaling-in-Reasoning-Models"><a href="#Does-Thinking-More-always-Help-Understanding-Test-Time-Scaling-in-Reasoning-Models" class="headerlink" title="Does Thinking More always Help? Understanding Test-Time Scaling in   Reasoning Models"></a>Does Thinking More always Help? Understanding Test-Time Scaling in   Reasoning Models</h2><p><strong>Authors:Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Yifu Lu, Mengdi Wang, Dinesh Manocha, Furong Huang, Mohammad Ghavamzadeh, Amrit Singh Bedi</strong></p>
<p>Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek R1) have led to a popular belief that extending thinking traces using prompts like â€œWaitâ€ or â€œLet me rethinkâ€ can improve performance. This raises a natural question: Does thinking more at test-time truly lead to better reasoning? To answer this question, we perform a detailed empirical study across models and benchmarks, which reveals a consistent pattern of initial performance improvements from additional thinking followed by a decline, due to â€œoverthinkingâ€. To understand this non-monotonic trend, we consider a simple probabilistic model, which reveals that additional thinking increases output variance-creating an illusion of improved reasoning while ultimately undermining precision. Thus, observed gains from â€œmore thinkingâ€ are not true indicators of improved reasoning, but artifacts stemming from the connection between model uncertainty and evaluation metric. This suggests that test-time scaling through extended thinking is not an effective way to utilize the inference thinking budget. Recognizing these limitations, we introduce an alternative test-time scaling approach, parallel thinking, inspired by Best-of-N sampling. Our method generates multiple independent reasoning paths within the same inference budget and selects the most consistent response via majority vote, achieving up to 20% higher accuracy compared to extended thinking. This provides a simple yet effective mechanism for test-time scaling of reasoning models. </p>
<blockquote>
<p>å…³äºæ¨ç†æ¨¡å‹çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾ï¼ˆä¾‹å¦‚OpenAI o1ã€DeepSeek R1ï¼‰çš„æœ€æ–°è¶‹åŠ¿å¯¼è‡´äº†ä¸€ç§æ™®éçš„è§‚ç‚¹ï¼Œå³é€šè¿‡ç±»ä¼¼â€œç­‰ä¸€ä¸‹â€æˆ–â€œè®©æˆ‘å†æ€è€ƒä¸€ä¸‹â€çš„æç¤ºæ¥æ‰©å±•æ€è€ƒè½¨è¿¹å¯ä»¥æé«˜æ€§èƒ½ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªè‡ªç„¶çš„é—®é¢˜ï¼šåœ¨æµ‹è¯•æ—¶æ€è€ƒæ›´å¤šæ˜¯å¦çœŸçš„ä¼šå¯¼è‡´æ›´å¥½çš„æ¨ç†ï¼Ÿä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹å’ŒåŸºå‡†è¿›è¡Œäº†è¯¦ç»†çš„å®è¯ç ”ç©¶ï¼Œç»“æœæ˜¾ç¤ºåˆå§‹æ€§èƒ½åœ¨é¢å¤–çš„æ€è€ƒåæœ‰æ”¹å–„ï¼Œä½†éšåç”±äºâ€œè¿‡åº¦æ€è€ƒâ€è€Œä¸‹é™ã€‚ä¸ºäº†ç†è§£è¿™ç§éå•è°ƒè¶‹åŠ¿ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸€ä¸ªç®€å•çš„æ¦‚ç‡æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è¡¨æ˜é¢å¤–çš„æ€è€ƒå¢åŠ äº†è¾“å‡ºæ–¹å·®ï¼Œä»è€Œäº§ç”Ÿäº†ä¸€ç§æ”¹å–„æ¨ç†çš„é”™è§‰ï¼Œä½†å®é™…ä¸Šä¼šç ´åç²¾åº¦ã€‚å› æ­¤ï¼Œâ€œæ›´å¤šæ€è€ƒâ€æ‰€å¸¦æ¥çš„è§‚å¯Ÿæ”¶ç›Šå¹¶ä¸æ˜¯æ¨ç†èƒ½åŠ›çœŸæ­£æé«˜çš„æŒ‡ç¤ºå™¨ï¼Œè€Œæ˜¯æºäºæ¨¡å‹ä¸ç¡®å®šæ€§å’Œè¯„ä»·æŒ‡æ ‡ä¹‹é—´è”ç³»çš„äº§ç‰©ã€‚è¿™è¡¨æ˜é€šè¿‡å»¶é•¿æ€è€ƒæ¥è¿›è¡Œæµ‹è¯•æ—¶é—´ç¼©æ”¾å¹¶ä¸æ˜¯æœ‰æ•ˆåˆ©ç”¨æ¨ç†é¢„ç®—çš„æœ‰æ•ˆæ–¹æ³•ã€‚è®¤è¯†åˆ°è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å—Best-of-Né‡‡æ ·å¯å‘çš„æ›¿ä»£æµ‹è¯•æ—¶é—´ç¼©æ”¾æ–¹æ³•â€”â€”å¹¶è¡Œæ€è€ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç›¸åŒçš„æ¨ç†é¢„ç®—å†…ç”Ÿæˆå¤šä¸ªç‹¬ç«‹çš„æ¨ç†è·¯å¾„ï¼Œå¹¶é€šè¿‡å¤šæ•°æŠ•ç¥¨é€‰æ‹©æœ€ä¸€è‡´çš„å“åº”ï¼Œä¸å»¶é•¿æ€è€ƒç›¸æ¯”ï¼Œå®ç°äº†é«˜è¾¾20%çš„å‡†ç¡®æ€§æé«˜ã€‚è¿™ä¸ºæ¨ç†æ¨¡å‹çš„æµ‹è¯•æ—¶é—´ç¼©æ”¾æä¾›äº†ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æœºåˆ¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.04210v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†æµ‹è¯•æ—¶æ‰©å±•æ€è€ƒæ˜¯å¦çœŸæ­£æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›çš„é—®é¢˜ã€‚ç ”ç©¶å‘ç°åœ¨åˆå§‹é˜¶æ®µå¢åŠ æ€è€ƒæ—¶é—´ç¡®å®èƒ½å¤Ÿæé«˜æ€§èƒ½ï¼Œä½†éšåç”±äºâ€œè¿‡åº¦æ€è€ƒâ€ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚é‡‡ç”¨æ¦‚ç‡æ¨¡å‹åˆ†æè¿™ä¸€éå•è°ƒè¶‹åŠ¿è¡¨æ˜ï¼Œå¢åŠ æ€è€ƒæ—¶é—´ä¼šå¢åŠ è¾“å‡ºæ–¹å·®ï¼Œä»è€Œäº§ç”Ÿæ”¹è¿›æ¨ç†çš„é”™è§‰ï¼Œå®é™…ä¸Šä¼šæŸå®³ç²¾åº¦ã€‚å› æ­¤ï¼Œè§‚å¯Ÿåˆ°çš„é€šè¿‡â€œæ›´å¤šæ€è€ƒâ€å¸¦æ¥çš„æ”¶ç›Šå¹¶éçœŸæ­£åæ˜ æ¨ç†èƒ½åŠ›çš„æé«˜ï¼Œè€Œæ˜¯æºäºæ¨¡å‹ä¸ç¡®å®šæ€§ä¸è¯„ä»·æŒ‡æ ‡ä¹‹é—´çš„å…³è”ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶æ‰©å±•æ€è€ƒçš„æ–¹æ³•â€”â€”å¹¶è¡Œæ€è€ƒï¼Œè¯¥æ–¹æ³•å—åˆ°Best-of-Né‡‡æ ·çš„å¯å‘ï¼Œèƒ½å¤Ÿåœ¨åŒä¸€æ¨ç†é¢„ç®—å†…ç”Ÿæˆå¤šä¸ªç‹¬ç«‹æ¨ç†è·¯å¾„ï¼Œå¹¶é€šè¿‡å¤šæ•°æŠ•ç¥¨é€‰æ‹©æœ€ä¸€è‡´çš„å›åº”ï¼Œä¸æ‰©å±•æ€è€ƒç›¸æ¯”ï¼Œæé«˜äº†é«˜è¾¾20%çš„å‡†ç¡®ç‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶æ‰©å±•æ€è€ƒæ˜¯å¦çœŸæ­£æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›çš„é—®é¢˜è¢«æå‡ºå¹¶ç ”ç©¶ã€‚</li>
<li>å¢åŠ æ€è€ƒæ—¶é—´åˆå§‹èƒ½æé«˜æ€§èƒ½ï¼Œä½†éšåç”±äºâ€œè¿‡åº¦æ€è€ƒâ€ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚</li>
<li>æ¦‚ç‡æ¨¡å‹åˆ†ææ˜¾ç¤ºå¢åŠ æ€è€ƒæ—¶é—´ä¼šå¢åŠ è¾“å‡ºæ–¹å·®ï¼Œäº§ç”Ÿæ”¹è¿›æ¨ç†çš„é”™è§‰ï¼Œå®é™…ä¸ŠæŸå®³ç²¾åº¦ã€‚</li>
<li>â€œæ›´å¤šæ€è€ƒâ€å¸¦æ¥çš„æ”¶ç›Šå¹¶éçœŸæ­£åæ˜ æ¨ç†èƒ½åŠ›çš„æé«˜ï¼Œè€Œæ˜¯æºäºæ¨¡å‹ä¸ç¡®å®šæ€§ä¸è¯„ä»·æŒ‡æ ‡çš„å…³è”ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æµ‹è¯•æ—¶æ‰©å±•æ€è€ƒæ–¹æ³•â€”â€”å¹¶è¡Œæ€è€ƒã€‚</li>
<li>å¹¶è¡Œæ€è€ƒèƒ½åœ¨åŒä¸€æ¨ç†é¢„ç®—å†…ç”Ÿæˆå¤šä¸ªç‹¬ç«‹æ¨ç†è·¯å¾„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.04210">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-0883fcc328b1d742bc8e9a3e2c346bd1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b068b61b24e9b295fbe2183fa8dfc044.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-371c6c836c767dcc4bf7df499fb8722d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3e1647cdb21ab7460ccbe682cc1e7ea0.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="CheXGenBench-A-Unified-Benchmark-For-Fidelity-Privacy-and-Utility-of-Synthetic-Chest-Radiographs"><a href="#CheXGenBench-A-Unified-Benchmark-For-Fidelity-Privacy-and-Utility-of-Synthetic-Chest-Radiographs" class="headerlink" title="CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of   Synthetic Chest Radiographs"></a>CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of   Synthetic Chest Radiographs</h2><p><strong>Authors:Raman Dutt, Pedro Sanchez, Yongchen Yao, Steven McDonagh, Sotirios A. Tsaftaris, Timothy Hospedales</strong></p>
<p>We introduce CheXGenBench, a rigorous and multifaceted evaluation framework for synthetic chest radiograph generation that simultaneously assesses fidelity, privacy risks, and clinical utility across state-of-the-art text-to-image generative models. Despite rapid advancements in generative AI for real-world imagery, medical domain evaluations have been hindered by methodological inconsistencies, outdated architectural comparisons, and disconnected assessment criteria that rarely address the practical clinical value of synthetic samples. CheXGenBench overcomes these limitations through standardised data partitioning and a unified evaluation protocol comprising over 20 quantitative metrics that systematically analyse generation quality, potential privacy vulnerabilities, and downstream clinical applicability across 11 leading text-to-image architectures. Our results reveal critical inefficiencies in the existing evaluation protocols, particularly in assessing generative fidelity, leading to inconsistent and uninformative comparisons. Our framework establishes a standardised benchmark for the medical AI community, enabling objective and reproducible comparisons while facilitating seamless integration of both existing and future generative models. Additionally, we release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K radiographs generated by the top-performing model (Sana 0.6B) in our benchmark to support further research in this critical domain. Through CheXGenBench, we establish a new state-of-the-art and release our framework, models, and SynthCheX-75K dataset at <a target="_blank" rel="noopener" href="https://raman1121.github.io/CheXGenBench/">https://raman1121.github.io/CheXGenBench/</a> </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºCheXGenBenchï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹åˆæˆèƒ¸éƒ¨Xå…‰ç‰‡ç”ŸæˆæŠ€æœ¯çš„ä¸¥æ ¼å¤šå…ƒè¯„ä¼°æ¡†æ¶ï¼Œå¯ä»¥åŒæ—¶è¯„ä¼°æœ€å‰æ²¿çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„ä¿çœŸåº¦ã€éšç§é£é™©ä»¥åŠä¸´åºŠå®ç”¨æ€§ã€‚å°½ç®¡ç°å®ä¸–ç•Œå›¾åƒç”Ÿæˆäººå·¥æ™ºèƒ½å–å¾—äº†å¿«é€Ÿå‘å±•ï¼Œä½†åŒ»ç–—é¢†åŸŸè¯„ä¼°ä»å—åˆ°æ–¹æ³•ä¸ä¸€è‡´ã€æ¶æ„å¯¹æ¯”è¿‡æ—¶ä»¥åŠè¯„ä¼°æ ‡å‡†è„±èŠ‚ç­‰é—®é¢˜çš„é˜»ç¢ï¼Œè¿™äº›é—®é¢˜å¾ˆå°‘å…³æ³¨åˆæˆæ ·æœ¬çš„å®é™…ä¸´åºŠä»·å€¼ã€‚CheXGenBenché€šè¿‡æ ‡å‡†åŒ–æ•°æ®åˆ†åŒºä»¥åŠåŒ…å«è¶…è¿‡20ä¸ªé‡åŒ–æŒ‡æ ‡çš„ç»Ÿä¸€è¯„ä¼°åè®®æ¥å…‹æœè¿™äº›é™åˆ¶ï¼Œè¯¥åè®®ç³»ç»Ÿåœ°åˆ†æäº†ç”Ÿæˆè´¨é‡ã€æ½œåœ¨çš„éšç§æ¼æ´ä»¥åŠåœ¨11ä¸ªé¢†å…ˆçš„æ–‡æœ¬åˆ°å›¾åƒæ¶æ„ä¸­çš„ä¸‹æ¸¸ä¸´åºŠé€‚ç”¨æ€§ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†ç°æœ‰è¯„ä¼°åè®®ä¸­çš„å…³é”®ä½æ•ˆä¹‹å¤„ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯„ä¼°ç”Ÿæˆä¿çœŸåº¦æ–¹é¢ï¼Œå¯¼è‡´æ¯”è¾ƒç»“æœä¸ä¸€è‡´ä¸”ç¼ºä¹ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºåŒ»ç–—äººå·¥æ™ºèƒ½ç¤¾åŒºå»ºç«‹äº†æ ‡å‡†åŒ–åŸºå‡†ï¼Œèƒ½å¤Ÿè¿›è¡Œå®¢è§‚å’Œå¯é‡å¤çš„æ¯”è¾ƒï¼ŒåŒæ—¶ä¿ƒè¿›äº†ç°æœ‰å’Œæœªæ¥ç”Ÿæˆæ¨¡å‹çš„æ— ç¼é›†æˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªé«˜è´¨é‡åˆæˆæ•°æ®é›†SynthCheX-75Kï¼Œè¯¥æ•°æ®é›†åŒ…å«ç”±æˆ‘ä»¬åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼ˆSana 0.6Bï¼‰ç”Ÿæˆçš„75Kå¼ æ”¾å°„å½±åƒå›¾ï¼Œä»¥æ”¯æŒè¿™ä¸€å…³é”®é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚é€šè¿‡CheXGenBenchï¼Œæˆ‘ä»¬æ ‘ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡æ†ï¼Œå¹¶å°†æˆ‘ä»¬çš„æ¡†æ¶ã€æ¨¡å‹å’ŒSynthCheX-75Kæ•°æ®é›†å‘å¸ƒåœ¨[<a target="_blank" rel="noopener" href="https://raman1121.github.io/CheXGenBench/]%E4%B8%8A%E3%80%82">https://raman1121.github.io/CheXGenBench/]ä¸Šã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.10496v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æˆ‘ä»¬æ¨å‡ºäº†CheXGenBenchï¼Œè¿™æ˜¯ä¸€ä¸ªå¯¹åˆæˆèƒ¸ç‰‡çš„ç”Ÿæˆè¿›è¡Œç»¼åˆè¯„ä»·çš„ä¸¥è°¨ä¸”å¤šå…ƒçš„è¯„ä»·æ¡†æ¶ã€‚è¯¥æ¡†æ¶å¯ä»¥åŒæ—¶è¯„ä¼°å¤šä¸ªæ–‡æœ¬è‡³å›¾åƒç”Ÿæˆæ¨¡å‹çš„çœŸå®æ€§ã€éšç§é£é™©ä»¥åŠä¸´åºŠåº”ç”¨ä»·å€¼ã€‚è™½ç„¶ç”Ÿæˆå¼AIåœ¨ç°å®å›¾åƒé¢†åŸŸå‘å±•è¿…é€Ÿï¼Œä½†åŒ»ç–—é¢†åŸŸçš„è¯„ä»·å—åˆ°æ–¹æ³•ä¸ä¸€è‡´ã€æ¶æ„æ¯”è¾ƒè¿‡æ—¶ä»¥åŠè¯„ä¼°æ ‡å‡†è„±èŠ‚ç­‰é—®é¢˜çš„é˜»ç¢ï¼Œå¾ˆå°‘å…³æ³¨åˆæˆæ ·æœ¬çš„å®é™…ä¸´åºŠä»·å€¼ã€‚CheXGenBenché€šè¿‡æ ‡å‡†åŒ–æ•°æ®åˆ’åˆ†å’Œç»Ÿä¸€çš„è¯„ä¼°åè®®ï¼ŒåŒ…æ‹¬è¶…è¿‡20ä¸ªå®šé‡æŒ‡æ ‡ï¼Œç³»ç»Ÿåœ°åˆ†æç”Ÿæˆè´¨é‡ã€æ½œåœ¨çš„éšç§æ¼æ´ä»¥åŠä¸‹æ¸¸åœ¨ä¸´åºŠåº”ç”¨çš„é€‚ç”¨æ€§ï¼Œæ¶µç›–äº†11ä¸ªä¸»æµçš„æ–‡æœ¬è‡³å›¾åƒæ¶æ„ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†ç°æœ‰è¯„ä¼°åè®®çš„å…³é”®æ•ˆç‡é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è¯„ä¼°ç”ŸæˆçœŸå®æ€§æ–¹é¢ï¼Œå¯¼è‡´æ¯”è¾ƒç»“æœä¸ä¸€è‡´ä¸”ç¼ºä¹ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸ºåŒ»ç–—AIç¤¾åŒºå»ºç«‹äº†æ ‡å‡†åŒ–çš„åŸºå‡†æµ‹è¯•ï¼Œä½¿å®¢è§‚å’Œå¯é‡å¤çš„æ¯”è¾ƒæˆä¸ºå¯èƒ½ï¼ŒåŒæ—¶ä¿ƒè¿›äº†ç°æœ‰å’Œæœªæ¥ç”Ÿæˆæ¨¡å‹çš„æ— ç¼é›†æˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªé«˜è´¨é‡çš„äººå·¥æ•°æ®é›†SynthCheX-75Kï¼ŒåŒ…å«ç”±æˆ‘ä»¬åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³çš„æ¨¡å‹ï¼ˆSana 0.6Bï¼‰ç”Ÿæˆçš„75Kå¼ æ”¾å°„ç…§ç‰‡ï¼Œä»¥æ”¯æŒè¿™ä¸€å…³é”®é¢†åŸŸçš„ç ”ç©¶ã€‚é€šè¿‡CheXGenBenchï¼Œæˆ‘ä»¬å»ºç«‹äº†æ–°çš„æ ‡å‡†å¹¶å‘å¸ƒäº†æˆ‘ä»¬çš„æ¡†æ¶ã€æ¨¡å‹å’ŒSynthCheX-75Kæ•°æ®é›†ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>CheXGenBenchæ˜¯ä¸€ä¸ªé’ˆå¯¹åˆæˆèƒ¸ç‰‡çš„ç”Ÿæˆèƒ½åŠ›çš„å¤šå…ƒåŒ–è¯„ä»·æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤ŸåŒæ—¶è¯„ä¼°å¤šä¸ªæ–‡æœ¬è‡³å›¾åƒç”Ÿæˆæ¨¡å‹çš„å¤šä¸ªæ–¹é¢ï¼šçœŸå®æ€§ã€éšç§é£é™©ä»¥åŠä¸´åºŠåº”ç”¨ä»·å€¼ç­‰ã€‚</li>
<li>è™½ç„¶AIæŠ€æœ¯åœ¨ç°å®å›¾åƒé¢†åŸŸçš„ç”ŸæˆæŠ€æœ¯å–å¾—äº†è¿›æ­¥ï¼Œä½†åœ¨åŒ»ç–—é¢†åŸŸçš„è¯„ä¼°ä»å­˜åœ¨è¯¸å¤šæŒ‘æˆ˜ã€‚</li>
<li>CheXGenBenché€šè¿‡æ ‡å‡†åŒ–æ•°æ®åˆ’åˆ†å’Œç»Ÿä¸€çš„è¯„ä¼°åè®®è§£å†³äº†ç°æœ‰è¯„ä¼°åè®®çš„ä¸è¶³ã€‚</li>
<li>è¯¥æ¡†æ¶åŒ…å«è¶…è¿‡20ä¸ªå®šé‡æŒ‡æ ‡ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°åˆ†æç”Ÿæˆè´¨é‡ã€æ½œåœ¨çš„éšç§æ¼æ´ä»¥åŠä¸‹æ¸¸åœ¨ä¸´åºŠåº”ç”¨çš„é€‚ç”¨æ€§ç­‰å¤šä¸ªæ–¹é¢ã€‚</li>
<li>CheXGenBenchæ¶µç›–äº†å¤šä¸ªä¸»æµçš„æ–‡æœ¬è‡³å›¾åƒæ¶æ„ï¼Œå¹¶æ­ç¤ºäº†ç°æœ‰è¯„ä¼°åè®®çš„å…³é”®é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.10496">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-502453e730a2527f4ac64553ea1bf6d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-de8106921042fda00b9227b3cf3b6e7d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80949a9259eaf7d342952832f3422d78.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Quantitative-Analysis-of-Performance-Drop-in-DeepSeek-Model-Quantization"><a href="#Quantitative-Analysis-of-Performance-Drop-in-DeepSeek-Model-Quantization" class="headerlink" title="Quantitative Analysis of Performance Drop in DeepSeek Model Quantization"></a>Quantitative Analysis of Performance Drop in DeepSeek Model Quantization</h2><p><strong>Authors:Enbo Zhao, Yi Shen, Shuming Shi, Jieyun Huang, Zhihao Chen, Ning Wang, Siqi Xiao, Jian Zhang, Kai Wang, Shiguo Lian</strong></p>
<p>Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally, possibly because the official service often suffers from being busy and some organizations have data privacy concerns. While single-machine deployment offers infrastructure simplicity, the modelsâ€™ 671B FP8 parameter configuration exceeds the practical memory limits of a standard 8-GPU machine. Quantization is a widely used technique that helps reduce model memory consumption. However, it is unclear what the performance of DeepSeek-R1 and V3 will be after being quantized. This technical report presents the first quantitative evaluation of multi-bitwidth quantization across the complete DeepSeek model spectrum. Key findings reveal that 4-bit quantization maintains little performance degradation versus FP8 while enabling single-machine deployment on standard NVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization method that significantly outperforms traditional Q3_K_M variant on various benchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach in most tasks. Moreover, DQ3_K_M supports single-machine deployment configurations for both NVIDIA H100&#x2F;A100 and Huawei 910B. Our implementation of DQ3_K_M is released at <a target="_blank" rel="noopener" href="https://github.com/UnicomAI/DeepSeek-Eval">https://github.com/UnicomAI/DeepSeek-Eval</a>, containing optimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3. </p>
<blockquote>
<p>æœ€è¿‘ï¼Œå¯¹æœ¬åœ°éƒ¨ç½²DeepSeek-R1å’ŒV3çš„éœ€æ±‚å¾ˆé«˜ï¼Œå¯èƒ½æ˜¯å› ä¸ºå®˜æ–¹æœåŠ¡ç»å¸¸ç¹å¿™ï¼Œè€Œä¸”ä¸€äº›ç»„ç»‡å¯¹æ•°æ®éšç§å­˜åœ¨æ‹…å¿§ã€‚è™½ç„¶å•æœºéƒ¨ç½²å¯ä»¥æä¾›åŸºç¡€è®¾æ–½çš„ç®€å•æ€§ï¼Œä½†æ¨¡å‹çš„671B FP8å‚æ•°é…ç½®è¶…å‡ºäº†æ ‡å‡†8 GPUæœºå™¨çš„å®é™…å†…å­˜é™åˆ¶ã€‚é‡åŒ–æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æŠ€æœ¯ï¼Œæœ‰åŠ©äºå‡å°‘æ¨¡å‹å†…å­˜æ¶ˆè€—ã€‚ç„¶è€Œï¼Œé‡åŒ–åDeepSeek-R1å’ŒV3çš„æ€§èƒ½å°šä¸æ¸…æ¥šã€‚æœ¬æŠ€æœ¯æŠ¥å‘Šé¦–æ¬¡å¯¹DeepSeekæ¨¡å‹è°±è¿›è¡Œå…¨é¢å¤šä½å®½é‡åŒ–è¯„ä¼°ã€‚ç ”ç©¶å‘ç°ï¼Œé‡‡ç”¨å››æ¯”ç‰¹é‡åŒ–çš„æ€§èƒ½å‡ ä¹æ²¡æœ‰ä¸‹é™ç›¸æ¯”äºFP8æ¨¡å¼åŒæ—¶å¯ä»¥åœ¨æ ‡å‡†NVIDIA GPUè®¾å¤‡ä¸Šå®ç°å•æœºéƒ¨ç½²ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†DQ3_K_Mçš„åŠ¨æ€ä¸‰æ¯”ç‰¹é‡åŒ–æ–¹æ³•åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å¤§å¤§ä¼˜äºä¼ ç»Ÿçš„Q3_K_Må˜ä½“åŒæ—¶DQ3_K_Måœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸­ä¸å››æ¯”ç‰¹é‡åŒ–ï¼ˆQ4_K_Mï¼‰æ–¹æ³•ç›¸å½“ã€‚æ­¤å¤–ï¼ŒDQ3_K_Mæ”¯æŒNVIDIA H100&#x2F;A100å’Œåä¸º910Bçš„å•æœºéƒ¨ç½²é…ç½®ã€‚æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://github.com/UnicomAI/DeepSeek-Eval">https://github.com/UnicomAI/DeepSeek-Eval</a>ä¸Šå‘å¸ƒäº†DQ3_K_Mçš„å®ç°åŒ…å«DeepSeek-R1å’ŒDeepSeek-V3çš„ä¼˜åŒ–ä¸‰æ¯”ç‰¹é‡åŒ–å˜ä½“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02390v2">PDF</a> This version added the results of DeepSeek-V3-0324</p>
<p><strong>Summary</strong></p>
<p>æ·±åº¦å­¦ä¹ æ¨¡å‹DeepSeek-R1å’ŒDeepSeek-V3åœ¨æœ¬åœ°éƒ¨ç½²éœ€æ±‚ä¸Šå‡ï¼Œç”±äºå®˜æ–¹æœåŠ¡ç¹å¿™åŠæ•°æ®éšç§é¡¾è™‘ï¼Œå¼•å‘å¹¿æ³›å…³æ³¨ã€‚è™½ç„¶å•æœºéƒ¨ç½²èƒ½å¤Ÿç®€åŒ–åŸºç¡€è®¾æ–½ï¼Œä½†å…¶å†…å­˜é™åˆ¶å¯¹éƒ¨ç½²éœ€æ±‚å­˜åœ¨æŒ‘æˆ˜ã€‚è¿™ç¯‡æŠ¥å‘Šæä¾›äº†åœ¨DeepSeekæ¨¡å‹ç³»åˆ—ä¸Šè¿›è¡Œé‡åŒ–åˆ†æçš„å®šé‡è¯„ä»·ã€‚ä¸»è¦ç ”ç©¶å‘ç°4ä½é‡åŒ–èƒ½åœ¨å†…å­˜æ¶ˆè€—å‡å°‘çš„åŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½ã€‚åŒæ—¶æå‡ºä¸€ç§åŠ¨æ€3ä½é‡åŒ–æ–¹æ³•DQ3_K_Mï¼Œå¯¹Q3_K_Mè¿›è¡Œä¼˜åŒ–å¹¶åœ¨å¤šæ•°ä»»åŠ¡ä¸­ä¸å½“å‰æŠ€æœ¯æœ€ä½³è¡¨ç°çš„4ä½é‡åŒ–åª²ç¾ã€‚è¯¥æŠ€æœ¯å®ç°åœ¨<a target="_blank" rel="noopener" href="https://github.com/UnicomAI/DeepSeek-Eval">https://github.com/UnicomAI/DeepSeek-Eval</a>å¤„å¼€æºå‘å¸ƒï¼Œä»¥æ”¯æŒä¸åŒé…ç½®ä¸éœ€æ±‚çš„ç”¨æˆ·ä½¿ç”¨ã€‚æ­¤æ¬¡å®ç°å°†ä¿ƒè¿›æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å•æœºéƒ¨ç½²çš„æ•ˆç‡å’Œæ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeepSeek-R1å’ŒDeepSeek-V3å› æœåŠ¡ç¹å¿™å’Œæ•°æ®éšç§æ‹…å¿§å—åˆ°å¹¿æ³›å…³æ³¨ã€‚éšç€å…¶åœ¨æœ¬åœ°éƒ¨ç½²éœ€æ±‚çš„ä¸Šå‡ï¼Œé’ˆå¯¹æ¨¡å‹è§„æ¨¡çš„ä¼˜åŒ–æˆä¸ºäº†å¿…è¦è®®é¢˜ã€‚æœ¬æŠ¥å‘Šåˆ™æä¾›äº†ä¸€ç§ä¼˜åŒ–æ‰‹æ®µä»¥è§£å†³è¯¥é—®é¢˜ã€‚æŠ¥å‘Šçš„æˆæœå…è®¸æœ¬åœ°è®¡ç®—æœºå¿«é€Ÿåº”ç”¨è¯¥ç®—æ³•å¯¹ä¸ªä½“åšå‡ºæ™ºèƒ½åŒ–ååº”å’Œåˆ†æå·¥ä½œä»¥åŠ å¿«å¤„ç†é€Ÿåº¦å¹¶é™ä½ç®—åŠ›æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02390">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0abed4d2752f9d39614051381091c934.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-426b1776ec98f0e3196d26bd9475066a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-86aa5d7607c6a2b453339b12f7fbea8b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-87ab7dbd3687898b848eff7d4caf411e.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="New-Dataset-and-Methods-for-Fine-Grained-Compositional-Referring-Expression-Comprehension-via-Specialist-MLLM-Collaboration"><a href="#New-Dataset-and-Methods-for-Fine-Grained-Compositional-Referring-Expression-Comprehension-via-Specialist-MLLM-Collaboration" class="headerlink" title="New Dataset and Methods for Fine-Grained Compositional Referring   Expression Comprehension via Specialist-MLLM Collaboration"></a>New Dataset and Methods for Fine-Grained Compositional Referring   Expression Comprehension via Specialist-MLLM Collaboration</h2><p><strong>Authors:Xuzheng Yang, Junzhuo Liu, Peng Wang, Guoqing Wang, Yang Yang, Heng Tao Shen</strong></p>
<p>Referring Expression Comprehension (REC) is a foundational cross-modal task that evaluates the interplay of language understanding, image comprehension, and language-to-image grounding. It serves as an essential testing ground for Multimodal Large Language Models (MLLMs). To advance this field, we introduced a new REC dataset in our previous conference paper, characterized by two key features. First, it is designed with controllable difficulty levels, requiring multi-level fine-grained reasoning across object categories, attributes, and multi-hop relationships. Second, it incorporates negative text and images generated through fine-grained editing and augmentation, explicitly testing a modelâ€™s ability to reject scenarios where the target object is absent, an often overlooked yet critical challenge in existing datasets. In this extended work, we propose two new methods to tackle the challenges of fine-grained REC by combining the strengths of Specialist Models and MLLMs. The first method adaptively assigns simple cases to faster, lightweight models and reserves complex ones for powerful MLLMs, balancing accuracy and efficiency. The second method lets a specialist generate a set of possible object regions, and the MLLM selects the most plausible one using its reasoning ability. These collaborative strategies lead to significant improvements on our dataset and other challenging benchmarks. Our results show that combining specialized and general-purpose models offers a practical path toward solving complex real-world vision-language tasks. Our dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/sleepyshep/FineCops-Ref">https://github.com/sleepyshep/FineCops-Ref</a>. </p>
<blockquote>
<p>æŒ‡ä»£è¡¨è¾¾ç†è§£ï¼ˆRECï¼‰æ˜¯ä¸€é¡¹åŸºç¡€çš„å¤šæ¨¡å¼ä»»åŠ¡ï¼Œå®ƒè¯„ä¼°è¯­è¨€ç†è§£ã€å›¾åƒç†è§£å’Œè¯­è¨€åˆ°å›¾åƒå®šä½ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚å®ƒæ˜¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„é‡è¦æµ‹è¯•å¹³å°ã€‚ä¸ºäº†æ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„å‘å±•ï¼Œæˆ‘ä»¬åœ¨ä¹‹å‰çš„ä¼šè®®è®ºæ–‡ä¸­å¼•å…¥äº†ä¸€ä¸ªæ–°çš„RECæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å…·æœ‰ä¸¤ä¸ªå…³é”®ç‰¹å¾ã€‚é¦–å…ˆï¼Œå®ƒæŒ‰ç…§å¯æ§çš„éš¾åº¦çº§åˆ«è®¾è®¡ï¼Œéœ€è¦å¯¹å¯¹è±¡ç±»åˆ«ã€å±æ€§å’Œå¤šè·³å…³ç³»è¿›è¡Œå¤šå±‚æ¬¡ç²¾ç»†æ¨ç†ã€‚å…¶æ¬¡ï¼Œå®ƒç»“åˆäº†é€šè¿‡ç²¾ç»†ç¼–è¾‘å’Œå¢å¼ºç”Ÿæˆçš„è´Ÿé¢æ–‡æœ¬å’Œå›¾åƒï¼Œæ˜ç¡®æµ‹è¯•äº†æ¨¡å‹åœ¨ç›®æ ‡å¯¹è±¡ç¼ºå¤±çš„åœºæ™¯ä¸­çš„æ‹’ç»èƒ½åŠ›ï¼Œè¿™æ˜¯ç°æœ‰æ•°æ®é›†ä¸­ç»å¸¸è¢«å¿½è§†ä½†è‡³å…³é‡è¦çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹æ‰©å±•å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆä¸“ä¸šæ¨¡å‹å’Œå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¼˜ç‚¹æ¥è§£å†³ç²¾ç»†RECçš„æŒ‘æˆ˜ã€‚ç¬¬ä¸€ç§æ–¹æ³•è‡ªé€‚åº”åœ°å°†ç®€å•æ¡ˆä¾‹åˆ†é…ç»™æ›´å¿«ã€æ›´è½»å‹çš„æ¨¡å‹ï¼Œå¹¶å°†å¤æ‚æ¡ˆä¾‹ç•™ç»™å¼ºå¤§çš„MLLMsï¼Œä»è€Œå¹³è¡¡å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ç¬¬äºŒç§æ–¹æ³•è®©ä¸“å®¶ç”Ÿæˆä¸€ç»„å¯èƒ½çš„å¯¹è±¡åŒºåŸŸï¼Œç„¶åä½¿ç”¨MLLMçš„æ¨ç†èƒ½åŠ›é€‰æ‹©æœ€åˆç†çš„åŒºåŸŸã€‚è¿™äº›åä½œç­–ç•¥åœ¨æˆ‘ä»¬çš„æ•°æ®é›†å’Œå…¶ä»–å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç»“åˆä¸“ä¸šæ¨¡å‹å’Œé€šç”¨æ¨¡å‹ä¸ºè§£å†³å¤æ‚çš„ç°å®ä¸–ç•Œè§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†å®é™…é€”å¾„ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sleepyshep/FineCops-Ref%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sleepyshep/FineCops-Refä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20104v3">PDF</a> Accepted by TPAMI 2025</p>
<p><strong>Summary</strong>ï¼š<br>æœ¬æ–‡ä»‹ç»äº†é¢å‘è§†è§‰è¯­è¨€ä»»åŠ¡çš„è·¨æ¨¡æ€åŸºç¡€ä»»åŠ¡â€”â€”æŒ‡ä»£è¡¨è¾¾å¼ç†è§£ï¼ˆRECï¼‰ã€‚æ–‡ç« é‡ç‚¹é˜è¿°äº†æ–°æ¨å‡ºçš„RECæ•°æ®é›†çš„ç‰¹ç‚¹åŠåœ¨Multimodalå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰æµ‹è¯•ä¸­çš„é‡è¦æ€§ã€‚æ–‡ç« è¿˜æå‡ºäº†ä¸¤ç§æ–°æ–¹æ³•æ¥åº”å¯¹ç²¾ç»†åŒ–çš„RECæŒ‘æˆ˜ï¼Œé€šè¿‡ç»“åˆä¸“ä¸šæ¨¡å‹å’ŒMLLMsçš„ä¼˜åŠ¿ï¼Œåœ¨æ•°æ®é›†å’Œå…¶ä»–æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚ç»“æœè¡¨æ˜ï¼Œç»“åˆä¸“ä¸šæ¨¡å‹å’Œé€šç”¨æ¨¡å‹ä¸ºè§£å†³å¤æ‚çš„ç°å®ä¸–ç•Œè§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†å®ç”¨é€”å¾„ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>æŒ‡ä»£è¡¨è¾¾å¼ç†è§£ï¼ˆRECï¼‰æ˜¯è·¨æ¨¡æ€ä»»åŠ¡çš„åŸºç¡€ï¼Œæ¶‰åŠè¯­è¨€ç†è§£ã€å›¾åƒç†è§£å’Œè¯­è¨€åˆ°å›¾åƒçš„æ˜ å°„ã€‚</li>
<li>æ–°æ¨å‡ºçš„RECæ•°æ®é›†å…·æœ‰ä¸¤ä¸ªå…³é”®ç‰¹ç‚¹ï¼šå¯æ§çš„éš¾åº¦çº§åˆ«å’ŒåŒ…å«è´Ÿé¢æ–‡æœ¬å’Œå›¾åƒçš„ç²¾ç»†ç¼–è¾‘å’Œå¢å¼ºã€‚</li>
<li>æå‡ºäº†ä¸¤ç§æ–°æ–¹æ³•æ¥åº”å¯¹ç²¾ç»†åŒ–çš„RECæŒ‘æˆ˜ï¼šè‡ªé€‚åº”åˆ†é…ç®€å•æ¡ˆä¾‹ç»™è½»é‡çº§æ¨¡å‹ï¼Œå¤æ‚æ¡ˆä¾‹ç•™ç»™å¼ºå¤§çš„MLLMsï¼›ä¸“å®¶ç”Ÿæˆå¯èƒ½çš„å¯¹è±¡åŒºåŸŸï¼Œç”±MLLMåˆ©ç”¨æ¨ç†èƒ½åŠ›é€‰æ‹©æœ€åˆç†çš„åŒºåŸŸã€‚</li>
<li>ç»“åˆä¸“ä¸šæ¨¡å‹å’Œé€šç”¨æ¨¡å‹åœ¨æ•°æ®é›†å’Œå…¶ä»–åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>æ–‡ç« å¼ºè°ƒäº†è§£å†³å¤æ‚ç°å®ä¸–ç•Œè§†è§‰è¯­è¨€ä»»åŠ¡éœ€è¦ç»“åˆå®é™…éœ€æ±‚å’Œæ¨¡å‹ç‰¹ç‚¹ï¼Œé‡‡ç”¨é€‚å½“çš„ç­–ç•¥ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-afdde2754c13d2ef9ebefcfba4da727f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-470bb5e80162673fc79373746e792a4c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3ebc4a5130c9fb9f4d34b9a364daf4d3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c3dd7c40c427aefcc3be53b0c3aad9b7.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Learning-Strategic-Language-Agents-in-the-Werewolf-Game-with-Iterative-Latent-Space-Policy-Optimization"><a href="#Learning-Strategic-Language-Agents-in-the-Werewolf-Game-with-Iterative-Latent-Space-Policy-Optimization" class="headerlink" title="Learning Strategic Language Agents in the Werewolf Game with Iterative   Latent Space Policy Optimization"></a>Learning Strategic Language Agents in the Werewolf Game with Iterative   Latent Space Policy Optimization</h2><p><strong>Authors:Zelai Xu, Wanjun Gu, Chao Yu, Yi Wu, Yu Wang</strong></p>
<p>Large language model (LLM) agents have recently demonstrated impressive capabilities in various domains like open-ended conversation and multi-step decision-making. However, it remains challenging for these agents to solve strategic language games, such as Werewolf, which demand both strategic decision-making and free-form language interactions. Existing LLM agents often suffer from intrinsic bias in their action distributions and limited exploration of the unbounded text action space, resulting in suboptimal performance. To address these challenges, we propose Latent Space Policy Optimization (LSPO), an iterative framework that combines game-theoretic methods with LLM fine-tuning to build strategic language agents. LSPO leverages the observation that while the language space is combinatorially large, the underlying strategy space is relatively compact. We first map free-form utterances into a finite latent strategy space, yielding an abstracted extensive-form game. Then we apply game-theoretic methods like Counterfactual Regret Minimization (CFR) to optimize the policy in the latent space. Finally, we fine-tune the LLM via Direct Preference Optimization (DPO) to align with the learned policy. By iteratively alternating between these steps, our LSPO agents progressively enhance both strategic reasoning and language communication. Experiment on the Werewolf game shows that our agents iteratively expand the strategy space with improving performance and outperform existing Werewolf agents, underscoring their effectiveness in free-form language games with strategic interactions. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†æœ€è¿‘åœ¨å¼€æ”¾å¯¹è¯å’Œå¤šæ­¥å†³ç­–ç­‰å„ä¸ªé¢†åŸŸè¡¨ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå¯¹äºæˆ˜ç•¥è¯­è¨€æ¸¸æˆï¼ˆå¦‚Werewolfæ¸¸æˆï¼‰ï¼Œè¿™äº›ä»£ç†ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›æ¸¸æˆéœ€è¦æˆ˜ç•¥å†³ç­–å’Œè‡ªç”±å½¢å¼çš„è¯­è¨€äº¤äº’ï¼Œè€Œç°æœ‰çš„LLMä»£ç†åœ¨è¡ŒåŠ¨åˆ†å¸ƒä¸Šå­˜åœ¨å†…åœ¨åè§ï¼Œå¹¶ä¸”å¯¹æ— é™æ–‡æœ¬è¡ŒåŠ¨ç©ºé—´çš„æ¢ç´¢æœ‰é™ï¼Œå¯¼è‡´è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ½œåœ¨ç©ºé—´ç­–ç•¥ä¼˜åŒ–ï¼ˆLSPOï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç»“åˆåšå¼ˆè®ºæ–¹æ³•å’ŒLLMå¾®è°ƒæ¥æ„å»ºæˆ˜ç•¥è¯­è¨€ä»£ç†çš„è¿­ä»£æ¡†æ¶ã€‚LSPOåˆ©ç”¨äº†ä¸€ä¸ªè§‚å¯Ÿç»“æœï¼Œå³è™½ç„¶è¯­è¨€ç©ºé—´æ˜¯ç»„åˆæ€§çš„å·¨å¤§ï¼Œä½†æ½œåœ¨çš„ç­–ç•¥ç©ºé—´æ˜¯ç›¸å¯¹ç´§å‡‘çš„ã€‚æˆ‘ä»¬é¦–å…ˆå°†è‡ªç”±å½¢å¼çš„å‘è¨€æ˜ å°„åˆ°æœ‰é™çš„æ½œåœ¨ç­–ç•¥ç©ºé—´ï¼Œå½¢æˆä¸€ä¸ªæŠ½è±¡çš„æ‰©å±•å½¢å¼çš„æ¸¸æˆã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨åšå¼ˆè®ºæ–¹æ³•ï¼Œå¦‚åäº‹å®é—æ†¾æœ€å°åŒ–ï¼ˆCFRï¼‰ï¼Œä»¥ä¼˜åŒ–æ½œåœ¨ç©ºé—´ä¸­çš„ç­–ç•¥ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œä»¥ä¸å­¦åˆ°çš„ç­–ç•¥ä¿æŒä¸€è‡´ã€‚é€šè¿‡åœ¨è¿™å‡ æ­¥ä¹‹é—´è¿­ä»£äº¤æ›¿ï¼Œæˆ‘ä»¬çš„LSPOä»£ç†åœ¨æˆ˜ç•¥æ¨ç†å’Œè¯­è¨€æ²Ÿé€šæ–¹é¢é€æ­¥å¢å¼ºã€‚åœ¨Werewolfæ¸¸æˆä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä»£ç†é€šè¿‡è¿­ä»£æ‰©å±•ç­–ç•¥ç©ºé—´ï¼Œæ€§èƒ½å¾—åˆ°æå‡ï¼Œå¹¶è¶…è¶Šäº†ç°æœ‰çš„Werewolfä»£ç†ï¼Œè¿™è¯æ˜äº†å®ƒä»¬åœ¨å…·æœ‰æˆ˜ç•¥äº¤äº’çš„è‡ªç”±å½¢å¼è¯­è¨€æ¸¸æˆä¸­çš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.04686v2">PDF</a> Published in ICML 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨å¼€æ”¾å¯¹è¯å’Œå¤šæ­¥å†³ç­–ç­‰é¢†åŸŸå±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨è§£å†³æˆ˜ç•¥è¯­è¨€æ¸¸æˆï¼ˆå¦‚Werewolfï¼‰æ—¶ä»é¢ä¸´æŒ‘æˆ˜ã€‚ç°æœ‰LLMä»£ç†å­˜åœ¨è¡ŒåŠ¨åˆ†å¸ƒå†…åœ¨åè§å’Œæ–‡æœ¬è¡ŒåŠ¨ç©ºé—´æ¢ç´¢æœ‰é™çš„é—®é¢˜ï¼Œå¯¼è‡´æ€§èƒ½ä¸ä½³ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºæ½œåœ¨ç©ºé—´ç­–ç•¥ä¼˜åŒ–ï¼ˆLSPOï¼‰æ–¹æ³•ï¼Œç»“åˆåšå¼ˆè®ºæ–¹æ³•å’ŒLLMå¾®è°ƒï¼Œæ„å»ºæˆ˜ç•¥è¯­è¨€ä»£ç†ã€‚LSPOåˆ©ç”¨è¯­è¨€ç©ºé—´ç»„åˆå¤§ä½†æ½œåœ¨ç­–ç•¥ç©ºé—´ç›¸å¯¹ç´§å‡‘çš„è§‚å¯Ÿï¼Œå°†è‡ªç”±å½¢å¼çš„è¨€è®ºæ˜ å°„åˆ°æœ‰é™çš„æ½œåœ¨ç­–ç•¥ç©ºé—´ï¼Œå½¢æˆä¸€ä¸ªæŠ½è±¡åŒ–çš„æ‰©å±•å½¢å¼æ¸¸æˆã€‚ç„¶ååº”ç”¨åšå¼ˆè®ºæ–¹æ³•ï¼Œå¦‚åäº‹å®åæ‚”æœ€å°åŒ–ï¼ˆCFRï¼‰ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­ä¼˜åŒ–ç­–ç•¥ã€‚æœ€åï¼Œé€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¾®è°ƒLLMï¼Œä»¥ä¸å­¦åˆ°çš„ç­–ç•¥ä¸€è‡´ã€‚é€šè¿‡è¿­ä»£äº¤æ›¿è¿›è¡Œè¿™äº›æ­¥éª¤ï¼ŒLSPOä»£ç†é€æ­¥å¢å¼ºæˆ˜ç•¥æ¨ç†å’Œè¯­è¨€æ²Ÿé€šã€‚åœ¨Werewolfæ¸¸æˆçš„å®éªŒè¡¨æ˜ï¼ŒLSPOä»£ç†é€šè¿‡è¿­ä»£æ‰©å±•ç­–ç•¥ç©ºé—´å¹¶æå‡æ€§èƒ½ï¼Œè¶…è¶Šäº†ç°æœ‰Werewolfä»£ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä»£ç†åœ¨æˆ˜ç•¥è¯­è¨€æ¸¸æˆï¼ˆå¦‚Werewolfï¼‰ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œéœ€æé«˜æˆ˜ç•¥å†³ç­–å’Œè‡ªç”±å½¢å¼è¯­è¨€äº¤äº’çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰LLMä»£ç†å­˜åœ¨è¡ŒåŠ¨åˆ†å¸ƒåè§å’Œæ–‡æœ¬è¡ŒåŠ¨ç©ºé—´æ¢ç´¢æœ‰é™çš„é—®é¢˜ã€‚</li>
<li>LSPOæ–¹æ³•ç»“åˆåšå¼ˆè®ºå’ŒLLMå¾®è°ƒæ¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>LSPOå°†è‡ªç”±å½¢å¼çš„è¨€è®ºæ˜ å°„åˆ°æ½œåœ¨ç­–ç•¥ç©ºé—´ï¼Œå½¢æˆæŠ½è±¡åŒ–æ¸¸æˆã€‚</li>
<li>åº”ç”¨åšå¼ˆè®ºæ–¹æ³•ï¼ˆå¦‚CFRï¼‰åœ¨æ½œåœ¨ç©ºé—´ä¸­ä¼˜åŒ–ç­–ç•¥ã€‚</li>
<li>é€šè¿‡ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰å¾®è°ƒLLMï¼Œä¸å­¦åˆ°çš„ç­–ç•¥ä¸€è‡´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.04686">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f0cffb04d47f2147a57a8c322d763ff3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f84f67500f43b5cc421bd9fdb39ed597.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-abaa7bad61a4ba35fdd37c804a86d3fb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-802e68226f4d2e5ca47ac91db4d3452e.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-17/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-17/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-17/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-4a6aaa3f7cc4fed9931200e40748b8d7.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-17  Improving Large Language Model Safety with Contrastive Representation   Learning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-16/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-5c48b8e8212f227dbb8a8da4815a0dfc.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-16  DISCO Balances the Scales Adaptive Domain- and Difficulty-Aware   Reinforcement Learning on Imbalanced Data
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23901.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
