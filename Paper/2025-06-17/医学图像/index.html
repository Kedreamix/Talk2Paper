<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-17  Simple Radiology VLLM Test-time Scaling with Thought Graph Traversal">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-9cefe4fc38396dae12cc5162b9e929ca.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-17-æ›´æ–°"><a href="#2025-06-17-æ›´æ–°" class="headerlink" title="2025-06-17 æ›´æ–°"></a>2025-06-17 æ›´æ–°</h1><h2 id="Simple-Radiology-VLLM-Test-time-Scaling-with-Thought-Graph-Traversal"><a href="#Simple-Radiology-VLLM-Test-time-Scaling-with-Thought-Graph-Traversal" class="headerlink" title="Simple Radiology VLLM Test-time Scaling with Thought Graph Traversal"></a>Simple Radiology VLLM Test-time Scaling with Thought Graph Traversal</h2><p><strong>Authors:Yue Yao, Zelin Wen, Yan Tong, Xinyu Tian, Xuqing Li, Xiao Ma, Dongliang Xu, Tom Gedeon</strong></p>
<p>Test-time scaling offers a promising way to improve the reasoning performance of vision-language large models (VLLMs) without additional training. In this paper, we explore a simple but effective approach for applying test-time scaling to radiology report generation. Specifically, we introduce a lightweight Thought Graph Traversal (TGT) framework that guides the model to reason through organ-specific findings in a medically coherent order. This framework integrates structured medical priors into the prompt, enabling deeper and more logical analysis with no changes to the underlying model. To further enhance reasoning depth, we apply a reasoning budget forcing strategy that adjusts the modelâ€™s inference depth at test time by dynamically extending its generation process. This simple yet powerful combination allows a frozen radiology VLLM to self-correct and generate more accurate, consistent chest X-ray reports. Our method outperforms baseline prompting approaches on standard benchmarks, and also reveals dataset biases through traceable reasoning paths. Code and prompts are open-sourced for reproducibility at <a target="_blank" rel="noopener" href="https://github.com/glerium/Thought-Graph-Traversal">https://github.com/glerium/Thought-Graph-Traversal</a>. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆTest-time scalingï¼‰æä¾›äº†ä¸€ç§æ— éœ€é¢å¤–è®­ç»ƒå³å¯æé«˜è§†è§‰è¯­è¨€å¤§å‹æ¨¡å‹ï¼ˆVLLMï¼‰æ¨ç†æ€§èƒ½çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å°†æµ‹è¯•æ—¶ç¼©æ”¾åº”ç”¨äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè½»é‡çº§çš„æ€ç»´å›¾éå†ï¼ˆTGTï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä»¥åŒ»å­¦è¿è´¯çš„é¡ºåºå¼•å¯¼æ¨¡å‹é€šè¿‡ç‰¹å®šå™¨å®˜çš„æ£€æŸ¥ç»“æœè¿›è¡Œæ¨ç†ã€‚è¯¥æ¡†æ¶å°†ç»“æ„åŒ–çš„åŒ»å­¦å…ˆéªŒçŸ¥è¯†é›†æˆåˆ°æç¤ºä¸­ï¼Œæ— éœ€æ›´æ”¹åº•å±‚æ¨¡å‹å³å¯è¿›è¡Œæ›´æ·±å…¥ã€æ›´é€»è¾‘åŒ–çš„åˆ†æã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¨ç†æ·±åº¦ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ¨ç†é¢„ç®—å¼ºåˆ¶ç­–ç•¥ï¼Œé€šè¿‡åœ¨æµ‹è¯•æ—¶åŠ¨æ€æ‰©å±•æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹æ¥è°ƒæ•´æ¨¡å‹çš„æ¨ç†æ·±åº¦ã€‚è¿™ç§ç®€å•è€Œå¼ºå¤§çš„ç»„åˆå…è®¸å›ºå®šçš„æ”¾å°„å­¦VLLMè¿›è¡Œè‡ªæˆ‘ä¿®æ­£ï¼Œå¹¶ç”Ÿæˆæ›´å‡†ç¡®ã€ä¸€è‡´çš„èƒ¸éƒ¨Xå…‰æŠ¥å‘Šã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šä¼˜äºåŸºçº¿æç¤ºæ–¹æ³•ï¼Œå¹¶é€šè¿‡å¯è¿½æº¯çš„æ¨ç†è·¯å¾„æ­ç¤ºäº†æ•°æ®é›†åè§ã€‚ä»£ç å’Œæç¤ºå·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/glerium/Thought-Graph-Traversal%E4%B8%8A%E5%BC%80%E6%BA%90%EF%BC%8C%E4%BB%A5%E4%BE%BF%E8%BF%9B%E8%A1%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E9%AA%8C%E8%AF%81%E3%80%82">https://github.com/glerium/Thought-Graph-Traversalä¸Šå¼€æºï¼Œä»¥ä¾¿è¿›è¡Œå¯é‡å¤æ€§éªŒè¯ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11989v1">PDF</a> arXiv admin note: text overlap with arXiv:2404.11209 by other authors</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢ç´¢äº†ä¸€ç§å°†æµ‹è¯•æ—¶ç¼©æ”¾åº”ç”¨äºæ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„æœ‰æ•ˆæ–¹æ³•ã€‚é€šè¿‡å¼•å…¥è½»é‡çº§çš„æ€ç»´å›¾éå†ï¼ˆTGTï¼‰æ¡†æ¶ï¼ŒæŒ‡å¯¼æ¨¡å‹ä»¥åŒ»å­¦é€»è¾‘é¡ºåºè¿›è¡Œå™¨å®˜ç‰¹å¼‚æ€§å‘ç°çš„æ¨ç†ã€‚è¯¥æ¡†æ¶å°†ç»“æ„åŒ–åŒ»å­¦å…ˆéªŒçŸ¥è¯†èå…¥æç¤ºï¼Œæ— éœ€æ›´æ”¹åº•å±‚æ¨¡å‹å³å¯å®ç°æ›´æ·±å…¥ã€æ›´é€»è¾‘åŒ–çš„åˆ†æã€‚åº”ç”¨æ¨ç†é¢„ç®—å¼ºåˆ¶ç­–ç•¥è¿›ä¸€æ­¥å¢å¼ºæ¨ç†æ·±åº¦ï¼Œé€šè¿‡åŠ¨æ€æ‰©å±•æ¨¡å‹çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œåœ¨æµ‹è¯•æ—¶è°ƒæ•´æ¨¡å‹çš„æ¨ç†æ·±åº¦ã€‚è¯¥ç»„åˆæ–¹æ³•å…è®¸å†»ç»“çš„æ”¾å°„å­¦VLLMè¿›è¡Œè‡ªæˆ‘ä¿®æ­£ï¼Œç”Ÿæˆæ›´å‡†ç¡®ã€ä¸€è‡´çš„èƒ¸éƒ¨Xå…‰æŠ¥å‘Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶ç¼©æ”¾å¯æ”¹å–„è§†è§‰è¯­è¨€å¤§æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>å¼•å…¥è½»é‡çº§çš„æ€ç»´å›¾éå†ï¼ˆTGTï¼‰æ¡†æ¶ï¼Œä»¥åŒ»å­¦é€»è¾‘é¡ºåºæŒ‡å¯¼æ¨¡å‹æ¨ç†ã€‚</li>
<li>TGTæ¡†æ¶å°†ç»“æ„åŒ–åŒ»å­¦å…ˆéªŒçŸ¥è¯†èå…¥æç¤ºï¼Œå®ç°æ›´æ·±å…¥çš„åˆ†æã€‚</li>
<li>æ¨ç†é¢„ç®—å¼ºåˆ¶ç­–ç•¥å¢å¼ºæ¨ç†æ·±åº¦ï¼Œè°ƒæ•´æ¨¡å‹åœ¨æµ‹è¯•æ—¶çš„æ¨ç†æ·±åº¦ã€‚</li>
<li>è¯¥æ–¹æ³•å…è®¸å†»ç»“çš„æ”¾å°„å­¦VLLMè‡ªæˆ‘ä¿®æ­£ï¼Œç”Ÿæˆæ›´å‡†ç¡®ã€ä¸€è‡´çš„æŠ¥å‘Šã€‚</li>
<li>åœ¨æ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ–¹æ³•ä¼˜äºåŸºå‡†æç¤ºæ–¹æ³•ã€‚</li>
<li>é€šè¿‡å¯è¿½æº¯çš„æ¨ç†è·¯å¾„æ­ç¤ºæ•°æ®é›†åå·®ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11989">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ed8a1d98318095ce8e8b39c725b05903.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12945ec3bda99eef0349842d2dd41188.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe6be2ce48fdf0488e95c4f9634e1f7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57afcb555a7ff0a24c78db0eda0789c1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Automated-Treatment-Planning-for-Interstitial-HDR-Brachytherapy-for-Locally-Advanced-Cervical-Cancer-using-Deep-Reinforcement-Learning"><a href="#Automated-Treatment-Planning-for-Interstitial-HDR-Brachytherapy-for-Locally-Advanced-Cervical-Cancer-using-Deep-Reinforcement-Learning" class="headerlink" title="Automated Treatment Planning for Interstitial HDR Brachytherapy for   Locally Advanced Cervical Cancer using Deep Reinforcement Learning"></a>Automated Treatment Planning for Interstitial HDR Brachytherapy for   Locally Advanced Cervical Cancer using Deep Reinforcement Learning</h2><p><strong>Authors:Mohammadamin Moradi, Runyu Jiang, Yingzi Liu, Malvern Madondo, Tianming Wu, James J. Sohn, Xiaofeng Yang, Yasmin Hasan, Zhen Tian</strong></p>
<p>High-dose-rate (HDR) brachytherapy plays a critical role in the treatment of locally advanced cervical cancer but remains highly dependent on manual treatment planning expertise. The objective of this study is to develop a fully automated HDR brachytherapy planning framework that integrates reinforcement learning (RL) and dose-based optimization to generate clinically acceptable treatment plans with improved consistency and efficiency. We propose a hierarchical two-stage autoplanning framework. In the first stage, a deep Q-network (DQN)-based RL agent iteratively selects treatment planning parameters (TPPs), which control the trade-offs between target coverage and organ-at-risk (OAR) sparing. The agentâ€™s state representation includes both dose-volume histogram (DVH) metrics and current TPP values, while its reward function incorporates clinical dose objectives and safety constraints, including D90, V150, V200 for targets, and D2cc for all relevant OARs (bladder, rectum, sigmoid, small bowel, and large bowel). In the second stage, a customized Adam-based optimizer computes the corresponding dwell time distribution for the selected TPPs using a clinically informed loss function. The framework was evaluated on a cohort of patients with complex applicator geometries. The proposed framework successfully learned clinically meaningful TPP adjustments across diverse patient anatomies. For the unseen test patients, the RL-based automated planning method achieved an average score of 93.89%, outperforming the clinical plans which averaged 91.86%. These findings are notable given that score improvements were achieved while maintaining full target coverage and reducing CTV hot spots in most cases. </p>
<blockquote>
<p>é«˜å‰‚é‡ç‡ï¼ˆHDRï¼‰è¿‘è·ç¦»æ”¾å°„æ²»ç–—åœ¨æ²»ç–—å±€éƒ¨æ™šæœŸå®«é¢ˆç™Œæ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œä½†ä»ç„¶é«˜åº¦ä¾èµ–äºæ‰‹åŠ¨æ²»ç–—è®¡åˆ’çš„ä¸“ä¸šçŸ¥è¯†ã€‚æœ¬ç ”ç©¶çš„ç›®æ ‡æ˜¯ä¸ºHDRè¿‘è·ç¦»æ”¾å°„æ²»ç–—å»ºç«‹ä¸€ä¸ªå…¨è‡ªåŠ¨åŒ–çš„æ²»ç–—è®¡åˆ’æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’ŒåŸºäºå‰‚é‡çš„ä¼˜åŒ–ï¼Œä»¥ç”Ÿæˆä¸´åºŠå¯æ¥å—çš„ã€å…·æœ‰æ”¹è¿›çš„ä¸€è‡´æ€§å’Œæ•ˆç‡çš„æ²»ç–—è®¡åˆ’ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†å±‚çš„ä¸¤é˜¶æ®µè‡ªåŠ¨è§„åˆ’æ¡†æ¶ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼ŒåŸºäºæ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰çš„RLä»£ç†é€šè¿‡è¿­ä»£é€‰æ‹©æ²»ç–—è®¡åˆ’å‚æ•°ï¼ˆTPPsï¼‰ï¼Œè¿™äº›å‚æ•°æ§åˆ¶ç›®æ ‡è¦†ç›–å’Œå±é™©å™¨å®˜ï¼ˆOARï¼‰èŠ‚çœä¹‹é—´çš„æƒè¡¡ã€‚ä»£ç†çš„çŠ¶æ€è¡¨ç¤ºåŒ…æ‹¬å‰‚é‡ä½“ç§¯ç›´æ–¹å›¾ï¼ˆDVHï¼‰æŒ‡æ ‡å’Œå½“å‰TPPå€¼ï¼Œå…¶å¥–åŠ±å‡½æ•°ç»“åˆäº†ä¸´åºŠå‰‚é‡ç›®æ ‡å’Œå®‰å…¨çº¦æŸï¼ŒåŒ…æ‹¬ç›®æ ‡åŒºåŸŸçš„D90ã€V150ã€V200ä»¥åŠæ‰€æœ‰ç›¸å…³å±é™©å™¨å®˜çš„D2ccï¼ˆè†€èƒ±ã€ç›´è‚ ã€ä¹™çŠ¶ç»“è‚ ã€å°è‚ å’Œå¤§è‚ ï¼‰ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œä¸€ä¸ªå®šåˆ¶çš„åŸºäºAdamçš„ä¼˜åŒ–å™¨ä½¿ç”¨ä¸´åºŠä¿¡æ¯çš„æŸå¤±å‡½æ•°è®¡ç®—æ‰€é€‰TPPsçš„ç›¸åº”åœç•™æ—¶é—´åˆ†å¸ƒã€‚è¯¥æ¡†æ¶åœ¨ä¸€ç»„å…·æœ‰å¤æ‚åº”ç”¨å™¨å‡ ä½•ç»“æ„çš„æ‚£è€…ä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚æ‰€æå‡ºæ¡†æ¶æˆåŠŸåœ°åœ¨ä¸åŒçš„æ‚£è€…è§£å‰–ç»“æ„ä¸­å­¦åˆ°äº†æœ‰ä¸´åºŠæ„ä¹‰çš„TPPè°ƒæ•´ã€‚å¯¹äºæœªè§è¿‡çš„æµ‹è¯•æ‚£è€…ï¼ŒåŸºäºRLçš„è‡ªåŠ¨è§„åˆ’æ–¹æ³•å–å¾—äº†å¹³å‡93.89%çš„è¯„åˆ†ï¼Œä¼˜äºå¹³å‡91.86%çš„ä¸´åºŠè®¡åˆ’ã€‚è¿™äº›å‘ç°å€¼å¾—æ³¨æ„ï¼Œå› ä¸ºåœ¨æé«˜è¯„åˆ†çš„åŒæ—¶ï¼Œå¤§å¤šæ•°ç—…ä¾‹éƒ½å®ç°äº†å…¨ç›®æ ‡è¦†ç›–å¹¶å‡å°‘äº†CTVçƒ­ç‚¹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11957v1">PDF</a> 12 pages, 2 figures, 3 tables</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ä¸ªé›†å¼ºåŒ–å­¦ä¹ ä¸å‰‚é‡ä¼˜åŒ–äºä¸€ä½“çš„å…¨è‡ªåŠ¨é«˜å‰‚é‡ç‡è¿‘è·ç¦»æ²»ç–—è®¡åˆ’æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå…·æœ‰æ›´å¥½ä¸€è‡´æ€§å’Œæ•ˆç‡çš„ä¸´åºŠå¯æ¥å—æ²»ç–—è®¡åˆ’ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åˆ†å±‚ä¸¤é˜¶æ®µè‡ªåŠ¨è§„åˆ’æ–¹æ³•ï¼Œé€šè¿‡æ·±åº¦Qç½‘ç»œå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“é€‰æ‹©æ²»ç–—è®¡åˆ’å‚æ•°ï¼Œå¹¶åˆ©ç”¨åŸºäºäºšå½“çš„ä¼˜åŒ–å™¨è®¡ç®—ç›¸åº”çš„åœç•™æ—¶é—´åˆ†å¸ƒã€‚åœ¨å¤æ‚åº”ç”¨å™¨å‡ ä½•ç»“æ„çš„ç—…äººç¾¤ä½“ä¸­è¯„ä¼°äº†è¯¥æ¡†æ¶ï¼Œç»“æœæ˜¾ç¤ºå…¶åœ¨ä¸åŒç—…äººè§£å‰–ç»“æ„ä¸­æˆåŠŸå­¦ä¹ äº†å…·æœ‰ä¸´åºŠæ„ä¹‰çš„æ²»ç–—è®¡åˆ’å‚æ•°è°ƒæ•´ï¼Œå¹¶åœ¨æœªè§æµ‹è¯•ç—…äººä¸­å–å¾—äº†ä¼˜äºä¸´åºŠè®¡åˆ’çš„å¹³å‡æˆç»©ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬ç ”ç©¶ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å’Œå‰‚é‡ä¼˜åŒ–æŠ€æœ¯æ¥å¼€å‘å…¨è‡ªåŠ¨HDRè¿‘è·ç¦»æ²»ç–—è®¡åˆ’æ¡†æ¶ã€‚</li>
<li>é‡‡ç”¨äº†å±‚æ¬¡ä¸¤é˜¶æ®µçš„è‡ªåŠ¨è§„åˆ’æ–¹æ³•ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨æ·±åº¦Qç½‘ç»œå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“é€‰æ‹©æ²»ç–—è®¡åˆ’å‚æ•°ï¼ŒåŒ…æ‹¬å‰‚é‡ä½“ç§¯ç›´æ–¹å›¾æŒ‡æ ‡å’Œç›®æ ‡åŒºåŸŸä¸é£é™©å™¨å®˜çš„æƒè¡¡å…³ç³»ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µåˆ©ç”¨åŸºäºäºšå½“çš„ä¼˜åŒ–å™¨è®¡ç®—ç›¸åº”çš„åœç•™æ—¶é—´åˆ†å¸ƒã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å…·æœ‰å¤æ‚åº”ç”¨å™¨å‡ ä½•ç»“æ„çš„ç—…äººç¾¤ä½“ä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>ç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶æˆåŠŸå­¦ä¹ äº†åœ¨ä¸åŒç—…äººè§£å‰–ç»“æ„ä¸­çš„ä¸´åºŠæ„ä¹‰æ²»ç–—è®¡åˆ’å‚æ•°è°ƒæ•´ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11957">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-38df4fd62cf8163d6fdb16e7a9660ef7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a63b0d539aa12d45b3f7fdf08541f5fa.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MindGrab-for-BrainChop-Fast-and-Accurate-Skull-Stripping-for-Command-Line-and-Browser"><a href="#MindGrab-for-BrainChop-Fast-and-Accurate-Skull-Stripping-for-Command-Line-and-Browser" class="headerlink" title="MindGrab for BrainChop: Fast and Accurate Skull Stripping for Command   Line and Browser"></a>MindGrab for BrainChop: Fast and Accurate Skull Stripping for Command   Line and Browser</h2><p><strong>Authors:Armina Fani, Mike Doan, Isabelle Le, Alex Fedorov, Malte Hoffmann, Chris Rorden, Sergey Plis</strong></p>
<p>We developed MindGrab, a parameter- and memory-efficient deep fully-convolutional model for volumetric skull-stripping in head images of any modality. Its architecture, informed by a spectral interpretation of dilated convolutions, was trained exclusively on modality-agnostic synthetic data. MindGrab was evaluated on a retrospective dataset of 606 multimodal adult-brain scans (T1, T2, DWI, MRA, PDw MRI, EPI, CT, PET) sourced from the SynthStrip dataset. Performance was benchmarked against SynthStrip, ROBEX, and BET using Dice scores, with Wilcoxon signed-rank significance tests. MindGrab achieved a mean Dice score of 95.9 with standard deviation (SD) 1.6 across modalities, significantly outperforming classical methods (ROBEX: 89.1 SD 7.7, P &lt; 0.05; BET: 85.2 SD 14.4, P &lt; 0.05). Compared to SynthStrip (96.5 SD 1.1, P&#x3D;0.0352), MindGrab delivered equivalent or superior performance in nearly half of the tested scenarios, with minor differences (&lt;3% Dice) in the others. MindGrab utilized 95% fewer parameters (146,237 vs. 2,566,561) than SynthStrip. This efficiency yielded at least 2x faster inference, 50% lower memory usage on GPUs, and enabled exceptional performance (e.g., 10-30x speedup, and up to 30x memory reduction) and accessibility on a wider range of hardware, including systems without high-end GPUs. MindGrab delivers state-of-the-art accuracy with dramatically lower resource demands, supported in brainchop-cli (<a target="_blank" rel="noopener" href="https://pypi.org/project/brainchop/">https://pypi.org/project/brainchop/</a>) and at brainchop.org. </p>
<blockquote>
<p>æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åä¸ºMindGrabçš„é«˜æ•ˆå‚æ•°å’Œå†…å­˜ä½¿ç”¨çš„æ·±åº¦å…¨å·ç§¯æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ç”¨äºå¤„ç†å„ç§æ¨¡æ€çš„å¤´éƒ¨å›¾åƒçš„ä½“ç§¯é¢…éª¨å‰¥ç¦»ã€‚å…¶ç»“æ„åŸºäºå¯¹è†¨èƒ€å·ç§¯çš„é¢‘è°±è§£é‡Šè€Œè®¾è®¡ï¼Œå¹¶ä»…åœ¨æ¨¡æ€æ— å…³çš„åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚MindGrabåœ¨æ¥è‡ªSynthStripæ•°æ®é›†çš„606ä¾‹å¤šæ¨¡æ€æˆäººè„‘æ‰«æï¼ˆåŒ…æ‹¬T1ã€T2ã€DWIã€MRAã€PDw MRIã€EPIã€CTã€PETï¼‰çš„å›é¡¾æ€§æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å…¶æ€§èƒ½ä»¥Diceå¾—åˆ†ä¸SynthStripã€ROBEXå’ŒBETè¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶ä½¿ç”¨Wilcoxonç¬¦å·ç§©æ£€éªŒæ³•è¿›è¡Œæ˜¾è‘—æ€§æ£€éªŒã€‚MindGrabåœ¨è·¨æ¨¡æ€ä¸‹çš„å¹³å‡Diceå¾—åˆ†ä¸º95.9ï¼ˆæ ‡å‡†å·®ä¸º1.6ï¼‰ï¼Œæ˜¾è‘—ä¼˜äºç»å…¸æ–¹æ³•ï¼ˆROBEXï¼š89.1ï¼Œæ ‡å‡†å·®ä¸º7.7ï¼›BETï¼š85.2ï¼Œæ ‡å‡†å·®ä¸º14.4ï¼ŒP &lt; 0.05ï¼‰ã€‚ä¸SynthStripï¼ˆDiceå¾—åˆ†ä¸º96.5ï¼Œæ ‡å‡†å·®ä¸º1.1ï¼ŒP&#x3D;0.0352ï¼‰ç›¸æ¯”ï¼ŒMindGrabåœ¨è¿‘ä¹ä¸€åŠçš„æµ‹è¯•åœºæ™¯ä¸­è¡¨ç°ç›¸å½“æˆ–æ›´å¥½ï¼Œåœ¨å…¶ä»–åœºæ™¯ä¸­å·®å¼‚è¾ƒå°ï¼ˆ&lt;3% Diceï¼‰ã€‚MindGrabä½¿ç”¨çš„å‚æ•°æ¯”SynthStripå°‘95%ï¼ˆ146ï¼Œ237 vs 2ï¼Œ566ï¼Œ561ï¼‰ã€‚è¿™ç§æ•ˆç‡å¸¦æ¥äº†è‡³å°‘ä¸¤å€æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼ŒGPUå†…å­˜ä½¿ç”¨ç‡é™ä½äº†50%ï¼Œå¹¶ä¸”åœ¨æ›´å¹¿æ³›çš„ç¡¬ä»¶ä¸Šå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ˆä¾‹å¦‚ï¼ŒåŠ é€Ÿé«˜è¾¾åå€è‡³ä¸‰åå€ï¼Œå†…å­˜å‡å°‘é«˜è¾¾ä¸‰åå€ï¼‰ã€‚MindGrabä»¥è¶…é«˜çš„å‡†ç¡®æ€§è¾¾åˆ°äº†ä¸šç•Œé¢†å…ˆæ°´å¹³ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†èµ„æºéœ€æ±‚ï¼Œå¯åœ¨brainchop-cliï¼ˆ<a target="_blank" rel="noopener" href="https://pypi.org/project/brainchop/%EF%BC%89%E5%92%8Cbrainchop.org%E4%B8%8A%E6%94%AF%E6%8C%81%E4%BD%BF%E7%94%A8%E3%80%82">https://pypi.org/project/brainchop/ï¼‰å’Œbrainchop.orgä¸Šæ”¯æŒä½¿ç”¨ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11860v1">PDF</a> 12 pages, 1 table, 4 figures. 2 supplementary tables, 1 supplementary   figure. Brainchop-cli: <a target="_blank" rel="noopener" href="https://pypi.org/project/brainchop/">https://pypi.org/project/brainchop/</a> . Brainchop web:   <a target="_blank" rel="noopener" href="https://brainchop.org/">https://brainchop.org/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†MindGrabçš„å¼€å‘ä¸åº”ç”¨ã€‚MindGrabæ˜¯ä¸€ç§å‚æ•°å’Œå†…å­˜æ•ˆç‡é«˜çš„æ·±åº¦å…¨å·ç§¯æ¨¡å‹ï¼Œç”¨äºå„ç§æ¨¡æ€çš„å¤´éƒ¨å›¾åƒä¸­çš„ä½“ç§¯é¢…éª¨å‰¥ç¦»ã€‚å…¶æ¶æ„åŸºäºè†¨èƒ€å·ç§¯çš„è°±è§£é‡Šï¼Œä»…åœ¨æ¨¡æ€æ— å…³çš„åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚åœ¨å¤šç§æ¨¡æ€çš„æˆäººè„‘æ‰«ææ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒMindGrabåœ¨é¢…éª¨å‰¥ç¦»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–ä¼ ç»Ÿæ–¹æ³•ï¼Œè¾¾åˆ°äº†è¾ƒé«˜çš„Diceåˆ†æ•°ï¼Œå¹¶ä¸”åœ¨å‚æ•°å’Œå†…å­˜ä½¿ç”¨æ–¹é¢æ›´åŠ é«˜æ•ˆï¼Œå¯ä»¥åœ¨æ›´å¹¿æ³›çš„ç¡¬ä»¶ä¸Šå®ç°å¿«é€Ÿæ¨ç†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MindGrabæ˜¯ä¸€ç§ç”¨äºå¤´éƒ¨å›¾åƒä½“ç§¯é¢…éª¨å‰¥ç¦»çš„æ·±åº¦å…¨å·ç§¯æ¨¡å‹ã€‚</li>
<li>MindGrabçš„æ¶æ„åŸºäºè†¨èƒ€å·ç§¯çš„è°±è§£é‡Šè®¾è®¡ã€‚</li>
<li>MindGrabåœ¨æ¨¡æ€æ— å…³çš„åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚</li>
<li>MindGrabåœ¨å¤šç§æ¨¡æ€çš„æˆäººè„‘æ‰«ææ•°æ®é›†ä¸Šçš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>MindGrabåœ¨é¢…éª¨å‰¥ç¦»ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼˜äºå…¶ä»–ä¼ ç»Ÿæ–¹æ³•ï¼Œè¾¾åˆ°äº†è¾ƒé«˜çš„Diceåˆ†æ•°ã€‚</li>
<li>MindGrabå…·æœ‰é«˜æ•ˆçš„å‚æ•°å’Œå†…å­˜ä½¿ç”¨ï¼Œå¯åœ¨å¹¿æ³›çš„ç¡¬ä»¶ä¸Šå®ç°å¿«é€Ÿæ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11860">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-443cb98782a1d9ae8740701db076608d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef00dd6231ffcc9cc999124ab7bea139.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80e346fb6dc62c22ce9a7b05c81ddbe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a62471e5b6f5a17524ac608ee28392b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a85b2548cb4906ef51c5dd2d6d1c1184.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deb75ada6bde003d388836757802b2ea.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Self-supervised-Learning-of-Echocardiographic-Video-Representations-via-Online-Cluster-Distillation"><a href="#Self-supervised-Learning-of-Echocardiographic-Video-Representations-via-Online-Cluster-Distillation" class="headerlink" title="Self-supervised Learning of Echocardiographic Video Representations via   Online Cluster Distillation"></a>Self-supervised Learning of Echocardiographic Video Representations via   Online Cluster Distillation</h2><p><strong>Authors:Divyanshu Mishra, Mohammadreza Salehi, Pramit Saha, Olga Patey, Aris T. Papageorghiou, Yuki M. Asano, J. Alison Noble</strong></p>
<p>Self-supervised learning (SSL) has achieved major advances in natural images and video understanding, but challenges remain in domains like echocardiography (heart ultrasound) due to subtle anatomical structures, complex temporal dynamics, and the current lack of domain-specific pre-trained models. Existing SSL approaches such as contrastive, masked modeling, and clustering-based methods struggle with high intersample similarity, sensitivity to low PSNR inputs common in ultrasound, or aggressive augmentations that distort clinically relevant features. We present DISCOVR (Distilled Image Supervision for Cross Modal Video Representation), a self-supervised dual branch framework for cardiac ultrasound video representation learning. DISCOVR combines a clustering-based video encoder that models temporal dynamics with an online image encoder that extracts fine-grained spatial semantics. These branches are connected through a semantic cluster distillation loss that transfers anatomical knowledge from the evolving image encoder to the video encoder, enabling temporally coherent representations enriched with fine-grained semantic understanding. Evaluated on six echocardiography datasets spanning fetal, pediatric, and adult populations, DISCOVR outperforms both specialized video anomaly detection methods and state-of-the-art video-SSL baselines in zero-shot and linear probing setups, and achieves superior segmentation transfer. </p>
<blockquote>
<p>è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰åœ¨è‡ªç„¶å›¾åƒå’Œè§†é¢‘ç†è§£æ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†åœ¨è¶…å£°å¿ƒåŠ¨å›¾ï¼ˆå¿ƒè„è¶…å£°ï¼‰ç­‰é¢†åŸŸä»é¢ä¸´æŒ‘æˆ˜ï¼ŒåŸå› åœ¨äºå…¶å¾®å¦™çš„è§£å‰–ç»“æ„ã€å¤æ‚çš„æ—¶åºåŠ¨æ€ä»¥åŠç¼ºä¹ç‰¹å®šçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚ç°æœ‰çš„SSLæ–¹æ³•ï¼Œå¦‚å¯¹æ¯”å­¦ä¹ ã€æ©è†œå»ºæ¨¡å’ŒåŸºäºèšç±»çš„æ–¹æ³•ï¼Œé¢ä¸´é«˜æ ·æœ¬é—´ç›¸ä¼¼æ€§ã€å¯¹å¸¸è§è¶…å£°çš„ä½PSNRè¾“å…¥çš„æ•æ„Ÿæ€§ï¼Œæˆ–è¿‡äºæ¿€çƒˆçš„å¢å¼ºæ‰‹æ®µä¼šæ‰­æ›²ä¸´åºŠä¸Šç›¸å…³çš„ç‰¹å¾ã€‚æˆ‘ä»¬æå‡ºäº†DISCOVRï¼ˆç”¨äºè·¨æ¨¡æ€è§†é¢‘è¡¨ç¤ºçš„è’¸é¦å›¾åƒç›‘ç£ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¿ƒè„è¶…å£°è§†é¢‘è¡¨ç¤ºå­¦ä¹ çš„è‡ªç›‘ç£åŒåˆ†æ”¯æ¡†æ¶ã€‚DISCOVRç»“åˆäº†ä¸€ä¸ªåŸºäºèšç±»çš„è§†é¢‘ç¼–ç å™¨ï¼Œè¯¥ç¼–ç å™¨å¯¹æ—¶åºåŠ¨æ€è¿›è¡Œå»ºæ¨¡ï¼Œä»¥åŠä¸€ä¸ªåœ¨çº¿å›¾åƒç¼–ç å™¨ï¼Œç”¨äºæå–ç²¾ç»†çš„ç©ºé—´è¯­ä¹‰ã€‚è¿™ä¸¤ä¸ªåˆ†æ”¯é€šè¿‡è¯­ä¹‰èšç±»è’¸é¦æŸå¤±ç›¸è¿æ¥ï¼Œå°†è§£å‰–çŸ¥è¯†ä»ä¸æ–­è¿›åŒ–çš„å›¾åƒç¼–ç å™¨è½¬ç§»åˆ°è§†é¢‘ç¼–ç å™¨ï¼Œä»è€Œå®ç°ä¸°å¯Œçš„æ—¶åºè¿è´¯è¡¨ç¤ºå’Œç²¾ç»†çš„è¯­ä¹‰ç†è§£ã€‚åœ¨æ¶µç›–èƒå„¿ã€å„¿ç«¥å’Œæˆäººç¾¤ä½“çš„å…­ä¸ªè¶…å£°å¿ƒåŠ¨å›¾æ•°æ®é›†ä¸Šè¯„ä¼°ï¼ŒDISCOVRåœ¨é›¶æ ·æœ¬å’Œçº¿æ€§æ¢æµ‹è®¾ç½®ä¸­è¡¨ç°å‡ºè¶…è¶Šä¸“ä¸šè§†é¢‘å¼‚å¸¸æ£€æµ‹æ–¹æ³•å’Œæœ€æ–°è§†é¢‘SSLåŸºå‡†çš„æ€§èƒ½ï¼Œå¹¶å®ç°ä¼˜è¶Šçš„åˆ†å‰²è¿ç§»æ•ˆæœã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11777v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨å¿ƒè„è¶…å£°è§†é¢‘è¡¨ç¤ºå­¦ä¹ ä¸­åº”ç”¨çš„è‡ªç›‘ç£åŒåˆ†æ”¯æ¡†æ¶DISCOVRã€‚è¯¥æ¡†æ¶ç»“åˆäº†åŸºäºèšç±»çš„è§†é¢‘ç¼–ç å™¨å’Œåœ¨çº¿å›¾åƒç¼–ç å™¨ï¼Œé€šè¿‡è¯­ä¹‰é›†ç¾¤è’¸é¦æŸå¤±è¿æ¥è¿™ä¸¤ä¸ªåˆ†æ”¯ï¼Œä»è€Œè½¬ç§»è§£å‰–çŸ¥è¯†ï¼Œå®ç°æ—¶ç©ºä¸€è‡´çš„ä¸°å¯Œç²¾ç»†è¯­ä¹‰ç†è§£ã€‚åœ¨å¤šä¸ªèƒå„¿ã€å„¿ç«¥å’Œæˆäººç¾¤ä½“å›å£°æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒDISCOVRåœ¨é›¶æ ·æœ¬å’Œçº¿æ€§æ¢æµ‹è®¾ç½®ä¸­è¡¨ç°å‡ºè‰²ï¼Œå®ç°äº†ä¼˜è¶Šçš„åˆ†å‰²è¿ç§»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SSLåœ¨è‡ªç„¶å›¾åƒå’Œè§†é¢‘ç†è§£æ–¹é¢å–å¾—é‡å¤§è¿›å±•ï¼Œä½†åœ¨å¿ƒè„è¶…å£°é¢†åŸŸä»é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰SSLæ–¹æ³•å¦‚å¯¹æ¯”å­¦ä¹ ã€æ©æ¨¡å»ºæ¨¡å’Œèšç±»æ–¹æ³•åœ¨é¢å¯¹å¿ƒè„è¶…å£°æ•°æ®æ—¶å­˜åœ¨å›°éš¾ã€‚</li>
<li>DISCOVRæ˜¯ä¸€ä¸ªè‡ªç›‘ç£åŒåˆ†æ”¯æ¡†æ¶ï¼Œç”¨äºå¿ƒè„è¶…å£°è§†é¢‘è¡¨ç¤ºå­¦ä¹ ã€‚</li>
<li>DISCOVRç»“åˆäº†åŸºäºèšç±»çš„è§†é¢‘ç¼–ç å™¨å’Œåœ¨çº¿å›¾åƒç¼–ç å™¨ã€‚</li>
<li>é€šè¿‡è¯­ä¹‰é›†ç¾¤è’¸é¦æŸå¤±ï¼ŒDISCOVRèƒ½å¤Ÿè½¬ç§»è§£å‰–çŸ¥è¯†ï¼Œå®ç°æ—¶ç©ºä¸€è‡´çš„ç†è§£ã€‚</li>
<li>DISCOVRåœ¨å¤šä¸ªå›å£°æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœä¼˜äºä¸“ä¸šè§†é¢‘å¼‚å¸¸æ£€æµ‹æ–¹æ³•å’Œæœ€æ–°è§†é¢‘SSLåŸºçº¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11777">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-555e64204be73a2df22900d943f5cd9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4de4b9ee59ed8cf023803076969b7013.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-689e9bc21abfac967c32de7879073e26.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fcbf9935ed7a7dbbbbb225d8bc9da02.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DMAF-Net-An-Effective-Modality-Rebalancing-Framework-for-Incomplete-Multi-Modal-Medical-Image-Segmentation"><a href="#DMAF-Net-An-Effective-Modality-Rebalancing-Framework-for-Incomplete-Multi-Modal-Medical-Image-Segmentation" class="headerlink" title="DMAF-Net: An Effective Modality Rebalancing Framework for Incomplete   Multi-Modal Medical Image Segmentation"></a>DMAF-Net: An Effective Modality Rebalancing Framework for Incomplete   Multi-Modal Medical Image Segmentation</h2><p><strong>Authors:Libin Lan, Hongxing Li, Zunhui Xia, Yudong Zhang</strong></p>
<p>Incomplete multi-modal medical image segmentation faces critical challenges from modality imbalance, including imbalanced modality missing rates and heterogeneous modality contributions. Due to their reliance on idealized assumptions of complete modality availability, existing methods fail to dynamically balance contributions and neglect the structural relationships between modalities, resulting in suboptimal performance in real-world clinical scenarios. To address these limitations, we propose a novel model, named Dynamic Modality-Aware Fusion Network (DMAF-Net). The DMAF-Net adopts three key ideas. First, it introduces a Dynamic Modality-Aware Fusion (DMAF) module to suppress missing-modality interference by combining transformer attention with adaptive masking and weight modality contributions dynamically through attention maps. Second, it designs a synergistic Relation Distillation and Prototype Distillation framework to enforce global-local feature alignment via covariance consistency and masked graph attention, while ensuring semantic consistency through cross-modal class-specific prototype alignment. Third, it presents a Dynamic Training Monitoring (DTM) strategy to stabilize optimization under imbalanced missing rates by tracking distillation gaps in real-time, and to balance convergence speeds across modalities by adaptively reweighting losses and scaling gradients. Extensive experiments on BraTS2020 and MyoPS2020 demonstrate that DMAF-Net outperforms existing methods for incomplete multi-modal medical image segmentation. Extensive experiments on BraTS2020 and MyoPS2020 demonstrate that DMAF-Net outperforms existing methods for incomplete multi-modal medical image segmentation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/violet-42/DMAF-Net">https://github.com/violet-42/DMAF-Net</a>. </p>
<blockquote>
<p>ä¸å®Œæ•´å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´æ¥è‡ªæ¨¡æ€ä¸å¹³è¡¡çš„å…³é”®æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ¨¡æ€ç¼ºå¤±ç‡çš„ä¸å¹³è¡¡å’Œæ¨¡æ€è´¡çŒ®çš„å¼‚è´¨æ€§ã€‚ç”±äºç°æœ‰æ–¹æ³•ä¾èµ–äºå®Œæ•´æ¨¡æ€å¯ç”¨æ€§çš„ç†æƒ³åŒ–å‡è®¾ï¼Œå®ƒä»¬æ— æ³•åŠ¨æ€å¹³è¡¡è´¡çŒ®å¹¶å¿½ç•¥äº†æ¨¡æ€ä¹‹é—´çš„ç»“æ„å…³ç³»ï¼Œå¯¼è‡´åœ¨çœŸå®ä¸–ç•Œä¸´åºŠåœºæ™¯ä¸­çš„æ€§èƒ½ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºåŠ¨æ€æ¨¡æ€æ„ŸçŸ¥èåˆç½‘ç»œï¼ˆDMAF-Netï¼‰çš„æ–°æ¨¡å‹ã€‚DMAF-Neté‡‡ç”¨äº†ä¸‰ä¸ªå…³é”®æ€æƒ³ã€‚é¦–å…ˆï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªåŠ¨æ€æ¨¡æ€æ„ŸçŸ¥èåˆï¼ˆDMAFï¼‰æ¨¡å—ï¼Œé€šè¿‡ç»“åˆå˜å‹å™¨æ³¨æ„åŠ›ä¸è‡ªé€‚åº”æ©æ¨¡ï¼ŒåŠ¨æ€åœ°åŠ æƒæ¨¡æ€è´¡çŒ®ï¼Œä»è€ŒæŠ‘åˆ¶ç¼ºå¤±æ¨¡æ€çš„å¹²æ‰°ã€‚å…¶æ¬¡ï¼Œå®ƒè®¾è®¡äº†ä¸€ä¸ªååŒå…³ç³»è’¸é¦å’ŒåŸå‹è’¸é¦æ¡†æ¶ï¼Œé€šè¿‡åæ–¹å·®ä¸€è‡´æ€§ã€æ©è†œå›¾æ³¨æ„åŠ›æ¥æ‰§è¡Œå…¨å±€å±€éƒ¨ç‰¹å¾å¯¹é½ï¼ŒåŒæ—¶é€šè¿‡è·¨æ¨¡æ€ç±»ç‰¹å®šåŸå‹å¯¹é½ç¡®ä¿è¯­ä¹‰ä¸€è‡´æ€§ã€‚ç¬¬ä¸‰ï¼Œå®ƒæå‡ºäº†ä¸€ç§åŠ¨æ€è®­ç»ƒç›‘æ§ï¼ˆDTMï¼‰ç­–ç•¥ï¼Œé€šè¿‡å®æ—¶è·Ÿè¸ªè’¸é¦é—´éš™æ¥ç¨³å®šä¸å¹³è¡¡ç¼ºå¤±ç‡ä¸‹çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”åœ°é‡æ–°åŠ æƒæŸå¤±å’Œç¼©æ”¾æ¢¯åº¦æ¥å¹³è¡¡è·¨æ¨¡æ€çš„æ”¶æ•›é€Ÿåº¦ã€‚åœ¨BraTS2020å’ŒMyoPS2020ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒDMAF-Netåœ¨ä¸å®Œæ•´å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/violet-42/DMAF-Net">https://github.com/violet-42/DMAF-Net</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11691v1">PDF</a> 12 pages, 4 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºDMAF-Netçš„åŠ¨æ€æ¨¡æ€æ„ŸçŸ¥èåˆç½‘ç»œï¼Œç”¨äºè§£å†³å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­çš„æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ã€‚DMAF-Netå¼•å…¥åŠ¨æ€æ¨¡æ€æ„ŸçŸ¥èåˆæ¨¡å—ï¼Œé€šè¿‡ç»“åˆå˜å‹å™¨æ³¨æ„åŠ›æœºåˆ¶å’Œè‡ªé€‚åº”æ©æ¨¡æ¥æŠ‘åˆ¶ç¼ºå¤±æ¨¡æ€çš„å¹²æ‰°ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›å›¾åŠ¨æ€è°ƒæ•´æƒé‡ã€‚åŒæ—¶è®¾è®¡ååŒå…³ç³»è’¸é¦å’ŒåŸå‹è’¸é¦æ¡†æ¶ï¼Œä»¥é€šè¿‡åæ–¹å·®ä¸€è‡´æ€§ã€æ©è†œå›¾æ³¨æ„åŠ›å®ç°å…¨å±€å±€éƒ¨ç‰¹å¾å¯¹é½ï¼Œå¹¶é€šè¿‡è·¨æ¨¡æ€ç±»ç‰¹å®šåŸå‹å¯¹é½ç¡®ä¿è¯­ä¹‰ä¸€è‡´æ€§ã€‚æ­¤å¤–ï¼ŒDMAF-Netè¿˜é‡‡ç”¨åŠ¨æ€è®­ç»ƒç›‘æ§ç­–ç•¥ï¼Œä»¥é€‚åº”ä¸å¹³è¡¡çš„ç¼ºå¤±ç‡å¹¶ä¼˜åŒ–å¹³è¡¡æ”¶æ•›é€Ÿåº¦ã€‚å®éªŒè¡¨æ˜DMAF-Netåœ¨å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤šæ¨¡æ€åŒ»å­¦å›¾åƒåˆ†å‰²é¢ä¸´æ¨¡æ€ä¸å¹³è¡¡çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç¼ºå¤±æ¨¡æ€å’Œå¼‚è´¨æ¨¡æ€è´¡çŒ®ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–å®Œæ•´æ¨¡æ€å¯ç”¨æ€§å‡è®¾ï¼Œæ— æ³•åŠ¨æ€å¹³è¡¡è´¡çŒ®å¹¶å¿½ç•¥æ¨¡æ€é—´çš„ç»“æ„å…³ç³»ã€‚</li>
<li>DMAF-Neté€šè¿‡åŠ¨æ€æ¨¡æ€æ„ŸçŸ¥èåˆæ¨¡å—æŠ‘åˆ¶ç¼ºå¤±æ¨¡æ€å¹²æ‰°ï¼Œå¹¶ç»“åˆæ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€è°ƒæ•´æƒé‡ã€‚</li>
<li>DMAF-Netè®¾è®¡ååŒå…³ç³»è’¸é¦å’ŒåŸå‹è’¸é¦æ¡†æ¶ï¼Œå®ç°å…¨å±€å±€éƒ¨ç‰¹å¾å¯¹é½å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
<li>DMAF-Neté‡‡ç”¨åŠ¨æ€è®­ç»ƒç›‘æ§ç­–ç•¥æ¥é€‚åº”ä¸å¹³è¡¡çš„ç¼ºå¤±ç‡å’Œä¼˜åŒ–æ”¶æ•›é€Ÿåº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11691">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-7a43166a3fc0f5013a97a2c6809097bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c59f6fbe15e54b8ccf2ff0f146d9659.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f0145b218fb5d7297988d52d7c8de99.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Comparing-the-Space-Densities-of-Millisecond-Spin-Magnetars-and-Fast-X-Ray-Transients"><a href="#Comparing-the-Space-Densities-of-Millisecond-Spin-Magnetars-and-Fast-X-Ray-Transients" class="headerlink" title="Comparing the Space Densities of Millisecond-Spin Magnetars and Fast   X-Ray Transients"></a>Comparing the Space Densities of Millisecond-Spin Magnetars and Fast   X-Ray Transients</h2><p><strong>Authors:Sumedha Biswas, Peter G. Jonker, M. Coleman Miller, Andrew Levan, Jonathan Quirola-VÃ¡squez</strong></p>
<p>Fast X-ray transients (FXTs) are bright X-ray flashes with durations of minutes to hours, peak isotropic luminosities of L_X,peak ~ 10^42-10^47 erg&#x2F;s, and total isotropic energies of E ~ 10^47-10^50 erg. They have been detected in the soft X-ray band by Chandra, XMM-Newton, Swift-XRT, and, most recently, by Einstein Probe, which has reported more than 50 FXTs in its first year of operation. While several models have been proposed, the nature of many FXTs remains unknown. One model suggests FXTs are powered by the spin-down of newly formed millisecond magnetars, typically produced by binary neutron star (BNS) mergers. However, the BNS volumetric rate, ~10^2 Gpc^-3 yr^-1, barely overlaps with the estimated FXT rate of 10^3-10^4 Gpc^-3 yr^-1. Even within that overlap, BNS mergers would need to produce FXTs at nearly 100% efficiency. We explore whether other millisecond magnetar formation channels could account for this discrepancy. We compile rate densities for several proposed progenitors: accretion-induced collapse of white dwarfs, binary white dwarf mergers, neutron star-white dwarf mergers, and the collapse of massive stars, and convert Galactic event rates into volumetric rates using either the star formation rate or the stellar mass density distributions as a function of redshift. We find that the highest potential formation rates arise from binary white dwarf mergers and massive star collapses. However, both channels face theoretical and observational challenges: the spin and magnetic field properties of the resulting neutron stars are uncertain, and few are expected to satisfy both conditions required for FXT production. Across all scenarios, the fraction of suitable millisecond magnetars is low or poorly constrained. We conclude that they are unlikely to be the dominant progenitors of FXTs and can contribute to at most 10% of the observed FXT population. </p>
<blockquote>
<p>å¿«é€ŸXå°„çº¿ç¬å˜ï¼ˆFXTsï¼‰æ˜¯æ˜äº®çš„Xå°„çº¿é—ªå…‰ï¼ŒæŒç»­æ—¶é—´ä»å‡ åˆ†é’Ÿåˆ°å‡ å°æ—¶ä¸ç­‰ï¼Œå³°å€¼ç­‰è·å…‰åº¦çº¦ä¸ºLx,peak ~ 10^42-10^47 erg&#x2F;sï¼Œæ€»ç­‰è·èƒ½é‡çº¦ä¸ºE ~ 10^47-10^50 ergã€‚å®ƒä»¬å·²åœ¨è½¯Xå°„çº¿æ³¢æ®µè¢«é’±å¾·æ‹‰ã€XMM-ç‰›é¡¿ã€Swift-XRTç­‰æ¢æµ‹å™¨æ¢æµ‹åˆ°ï¼Œæœ€è¿‘è¿˜è¢«çˆ±å› æ–¯å¦æ¢æµ‹å™¨æ¢æµ‹åˆ°ï¼Œè¯¥æ¢æµ‹å™¨åœ¨å…¶è¿è¥çš„ç¬¬ä¸€å¹´å†…å°±æŠ¥å‘Šäº†è¶…è¿‡50ä¸ªFXTsã€‚å°½ç®¡å·²ç»æå‡ºäº†å‡ ç§æ¨¡å‹ï¼Œä½†è®¸å¤šFXTsçš„æœ¬è´¨ä»ä¸å¾—è€ŒçŸ¥ã€‚æœ‰ä¸€ç§æ¨¡å‹è®¤ä¸ºFXTsæ˜¯ç”±æ–°å½¢æˆçš„æ¯«ç§’ç£æ˜Ÿçš„è‡ªè½¬å‡é€Ÿè€Œäº§ç”Ÿçš„ï¼Œé€šå¸¸æ˜¯ç”±åŒä¸­å­æ˜Ÿï¼ˆBNSï¼‰åˆå¹¶äº§ç”Ÿçš„ã€‚ç„¶è€Œï¼ŒBNSçš„ä½“ç§¯ç‡çº¦ä¸º10^2 Gpc^-3 yr^-1ï¼Œå‹‰å¼ºä¸ä¼°è®¡çš„FXTç‡ï¼ˆæ¯å•ä½ä½“ç§¯åœ¨æ¯ç«‹æ–¹ç«‹æ–¹æ ¼ç‘å¸Œåº¦çš„ä¸€å¹´æœ‰æ•°ä¸‡ç«‹æ–¹ç±³å°ºåº¦è§‚æµ‹åº¦è§ä¼½ç›å°„çº¿çš„æ—¶ç©ºå³°å€¼é—ªå…‰ç‚¹æ•°ç›®ï¼‰çš„æ¯ç«‹æ–¹å‰ç§’ç§’çš„æ•°é‡çº§è¾¾åˆ°ä¸‡è‡³åä¸‡äº¿æ¯ä¸‰å¹´å†…çš„æ¯”ä¾‹ç›¸å½“ï¼Œç”šè‡³åœ¨è¿™ä¸ªé‡å èŒƒå›´å†…ï¼ŒBNSåˆå¹¶ä¹Ÿéœ€è¦ä»¥æ¥è¿‘ç™¾åˆ†ä¹‹ç™¾çš„æ•ˆç‡äº§ç”ŸFXTsã€‚æˆ‘ä»¬æ¢è®¨äº†å…¶ä»–æ¯«ç§’ç£æ˜Ÿå½¢æˆé€šé“æ˜¯å¦èƒ½è§£é‡Šè¿™ä¸€å·®å¼‚ã€‚æˆ‘ä»¬æ±‡æ€»äº†å‡ ç§æå‡ºçš„å€™é€‰å½¢æˆæºçš„é€Ÿç‡å¯†åº¦ï¼šç™½çŸ®æ˜Ÿå¼•å‘çš„åç¼©ã€ç™½çŸ®æ˜ŸåŒæ˜Ÿåˆå¹¶ã€ä¸­å­æ˜Ÿä¸ç™½çŸ®æ˜Ÿåˆå¹¶ä»¥åŠå¤§è´¨é‡æ’æ˜Ÿåç¼©ï¼Œå¹¶ä½¿ç”¨æ’æ˜Ÿå½¢æˆç‡æˆ–æ’æ˜Ÿè´¨é‡å¯†åº¦åˆ†å¸ƒå‡½æ•°ä¸çº¢ç§»å…³ç³»å°†é“¶æ²³ç³»äº‹ä»¶é€Ÿç‡è½¬æ¢ä¸ºä½“ç§¯é€Ÿç‡ã€‚æˆ‘ä»¬å‘ç°æœ€é«˜æ½œåœ¨å½¢æˆç‡æ¥è‡ªäºç™½çŸ®æ˜ŸåŒæ˜Ÿåˆå¹¶å’Œå¤§è´¨é‡æ’æ˜Ÿåç¼©ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ä¸ªé€šé“éƒ½é¢ä¸´ç€ç†è®ºå’Œè§‚æµ‹ä¸Šçš„æŒ‘æˆ˜ï¼šæ‰€å½¢æˆä¸­å­æ˜Ÿçš„è‡ªè½¬å’Œç£åœºç‰¹æ€§å°šä¸ç¡®å®šï¼Œè€Œä¸”é¢„è®¡å¾ˆå°‘æœ‰æ»¡è¶³äº§ç”ŸFXTæ‰€éœ€æ¡ä»¶çš„å®ä¾‹ã€‚åœ¨æ‰€æœ‰åœºæ™¯ä¸­ï¼Œé€‚åˆäº§ç”ŸFXTçš„æ¯«ç§’ç£æ˜Ÿæ¯”ä¾‹è¾ƒä½æˆ–éš¾ä»¥ç¡®å®šã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œå®ƒä»¬ä¸å¤ªå¯èƒ½æˆä¸ºFXTsçš„ä¸»è¦æ¥æºï¼Œæœ€å¤šåªèƒ½è´¡çŒ®è§‚å¯Ÿåˆ°çš„FXTäººå£çš„ç™¾åˆ†ä¹‹åã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11676v1">PDF</a> Accepted for publication in A&A;</p>
<p><strong>æ‘˜è¦</strong></p>
<p>å¿«Xå°„çº¿ç¬æ€ï¼ˆFXTsï¼‰ä¸ºæ˜äº®Xå°„çº¿é—ªå…‰ï¼ŒæŒç»­æ—¶é—´ä¸ºåˆ†é’Ÿè‡³æ•°å°æ—¶ï¼Œå³°å€¼å…‰åº¦çº¦ä¸º10^42-10^47erg&#x2F;sï¼Œæ€»èƒ½é‡çº¦ä¸ºE ~ 10^47-10^50ergã€‚å®ƒä»¬å·²åœ¨è½¯Xå°„çº¿æ³¢æ®µè¢«é’±å¾·æ‹‰ã€XMM-ç‰›é¡¿ã€Swift-XRTç­‰æœ›è¿œé•œæ¢æµ‹åˆ°ï¼Œæœ€è¿‘è¿˜è¢«çˆ±å› æ–¯å¦æ¢æµ‹å™¨æ¢æµ‹åˆ°è¶…è¿‡50ä¸ªFXTsçš„æ¡ˆä¾‹ã€‚è™½ç„¶æå‡ºäº†å‡ ç§æ¨¡å‹ï¼Œä½†è®¸å¤šFXTsçš„æ€§è´¨ä»ç„¶æœªçŸ¥ã€‚æœ¬æ–‡æ¢è®¨äº†é™¤æ¯«ç§’ç£æ˜Ÿè‡ªè½¬å‡é€Ÿä»¥å¤–çš„å…¶ä»–æ¯«ç§’ç£æ˜Ÿå½¢æˆé€šé“æ˜¯å¦èƒ½ä¸ºè¿™ä¸€å·®å¼‚æä¾›è§£é‡Šã€‚æˆ‘ä»¬ç¼–è¯‘äº†å‡ ç§æè®®ä¸­çš„ç¥–ç»†èƒçš„é€Ÿç‡å¯†åº¦ï¼ŒåŒ…æ‹¬ç™½çŸ®æ˜Ÿåå¡Œã€åŒç™½çŸ®æ˜Ÿåˆå¹¶ã€ä¸­å­æ˜Ÿä¸ç™½çŸ®æ˜Ÿåˆå¹¶ä»¥åŠå¤§è´¨é‡æ’æ˜Ÿåå¡Œç­‰ã€‚æˆ‘ä»¬å‘ç°æœ€é«˜çš„æ½œåœ¨å½¢æˆç‡å¯èƒ½æ¥è‡ªåŒç™½çŸ®æ˜Ÿåˆå¹¶å’Œå¤§è´¨é‡æ’æ˜Ÿåå¡Œï¼Œä½†è¿™ä¸¤æ¡é€”å¾„åœ¨ç†è®ºå’Œè§‚æµ‹ä¸Šé¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬äº§ç”Ÿçš„ä¸­å­æ˜Ÿçš„è‡ªè½¬å’Œç£åœºç‰¹æ€§ä¸ç¡®å®šï¼Œå¹¶ä¸”åªæœ‰å¾ˆå°‘èƒ½æ»¡è¶³ç”Ÿäº§FXTæ‰€éœ€çš„ä¸¤é¡¹æ¡ä»¶ã€‚æ€»çš„æ¥è¯´ï¼Œåˆé€‚çš„æ¯«ç§’ç£æ˜Ÿæ¯”ä¾‹è¾ƒä½æˆ–éš¾ä»¥ç¡®å®šï¼Œå› æ­¤å®ƒä»¬ä¸å¤ªå¯èƒ½æˆä¸ºFXTsçš„ä¸»è¦ç¥–ç»†èƒï¼Œæœ€å¤šåªèƒ½è´¡çŒ®è§‚å¯Ÿåˆ°FXTäººç¾¤çš„ç™¾åˆ†ä¹‹åã€‚æ–‡ç« é‡ç‚¹ä»‹ç»äº†ç›®å‰å…³äºå¿«é€ŸXå°„çº¿ç¬æ€æˆå› çš„æ¢ç©¶ä»¥åŠä¸åŒæ¨¡å‹çš„ç†è®ºé¢„æµ‹å’Œå­˜åœ¨çš„æŒ‘æˆ˜ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>FXTæ˜¯æ˜äº®çš„Xå°„çº¿é—ªå…‰ï¼ŒæŒç»­æ—¶é—´ä»å‡ åˆ†é’Ÿåˆ°å‡ å°æ—¶ä¸ç­‰ï¼Œå…·æœ‰è¾ƒé«˜çš„å³°å€¼å…‰åº¦å’Œæ€»èƒ½é‡ã€‚</li>
<li>FXTså·²ç»è¢«å¤šä¸ªXå°„çº¿æœ›è¿œé•œæ¢æµ‹åˆ°ï¼ŒåŒ…æ‹¬çˆ±å› æ–¯å¦æ¢æµ‹å™¨åœ¨å…¶è¿è¥çš„ç¬¬ä¸€å¹´å°±æŠ¥å‘Šäº†è¶…è¿‡äº”åæ¬¡è§‚æµ‹ã€‚</li>
<li>è™½ç„¶å­˜åœ¨å¤šç§æ¨¡å‹æ¥è§£é‡ŠFXTsçš„èµ·æºï¼Œä½†å…¶ç¡®åˆ‡æ€§è´¨ä»ç„¶æœªçŸ¥ã€‚</li>
<li>æ–‡ç« æ¢è®¨äº†æ¯«ç§’ç£æ˜Ÿè‡ªè½¬å‡é€Ÿæ¨¡å‹ä»¥å¤–çš„å…¶ä»–å¯èƒ½çš„FXTæˆå› æ¨¡å‹ã€‚</li>
<li>åˆ†æäº†ä¸åŒæ¨¡å‹çš„æ½œåœ¨å½¢æˆç‡ï¼Œå‘ç°åŒç™½çŸ®æ˜Ÿåˆå¹¶å’Œå¤§è´¨é‡æ’æ˜Ÿåå¡Œæ˜¯æ½œåœ¨çš„é«˜å½¢æˆç‡æ¨¡å‹ã€‚ä½†è¿™äº›æ¨¡å‹é¢ä¸´ç†è®ºæŒ‘æˆ˜å’Œè§‚æµ‹å›°éš¾ã€‚è¿™äº›æ¨¡å‹äº§ç”Ÿçš„ä¸­å­æ˜Ÿçš„ç‰¹æ€§ï¼ˆè‡ªè½¬å’Œç£åœºï¼‰å¹¶ä¸ç¡®å®šï¼Œå¹¶ä¸”ä¸ä¸€å®šæ»¡è¶³äº§ç”ŸFXTçš„æ¡ä»¶ã€‚ </li>
<li>æ–‡ç« å¾—å‡ºçš„ç»“è®ºæ˜¯æ¯«ç§’ç£æ˜Ÿä¸å¤ªå¯èƒ½æˆä¸ºFXTsçš„ä¸»è¦æˆå› ï¼Œå…¶å¯¹è§‚å¯Ÿåˆ°çš„FXTäººå£çš„è´¡çŒ®æœ€å¤šä¸ºç™¾åˆ†ä¹‹åã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11676">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-395633029a7d55a752c3538bd0f4295d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca8f815f735f8605ef7db98e2920c684.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d207be4b02739d7204af9598423c8300.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-449bd7dea761d2f92b213789ad9952f7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Cross-Modal-Clustering-Guided-Negative-Sampling-for-Self-Supervised-Joint-Learning-from-Medical-Images-and-Reports"><a href="#Cross-Modal-Clustering-Guided-Negative-Sampling-for-Self-Supervised-Joint-Learning-from-Medical-Images-and-Reports" class="headerlink" title="Cross-Modal Clustering-Guided Negative Sampling for Self-Supervised   Joint Learning from Medical Images and Reports"></a>Cross-Modal Clustering-Guided Negative Sampling for Self-Supervised   Joint Learning from Medical Images and Reports</h2><p><strong>Authors:Libin Lan, Hongxing Li, Zunhui Xia, Juan Zhou, Xiaofei Zhu, Yongmei Li, Yudong Zhang, Xin Luo</strong></p>
<p>Learning medical visual representations directly from paired images and reports through multimodal self-supervised learning has emerged as a novel and efficient approach to digital diagnosis in recent years. However, existing models suffer from several severe limitations. 1) neglecting the selection of negative samples, resulting in the scarcity of hard negatives and the inclusion of false negatives; 2) focusing on global feature extraction, but overlooking the fine-grained local details that are crucial for medical image recognition tasks; and 3) contrastive learning primarily targets high-level features but ignoring low-level details which are essential for accurate medical analysis. Motivated by these critical issues, this paper presents a Cross-Modal Cluster-Guided Negative Sampling (CM-CGNS) method with two-fold ideas. First, it extends the k-means clustering used for local text features in the single-modal domain to the multimodal domain through cross-modal attention. This improvement increases the number of negative samples and boosts the model representation capability. Second, it introduces a Cross-Modal Masked Image Reconstruction (CM-MIR) module that leverages local text-to-image features obtained via cross-modal attention to reconstruct masked local image regions. This module significantly strengthens the modelâ€™s cross-modal information interaction capabilities and retains low-level image features essential for downstream tasks. By well handling the aforementioned limitations, the proposed CM-CGNS can learn effective and robust medical visual representations suitable for various recognition tasks. Extensive experimental results on classification, detection, and segmentation tasks across five downstream datasets show that our method outperforms state-of-the-art approaches on multiple metrics, verifying its superior performance. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œé€šè¿‡å¤šæ¨¡æ€è‡ªç›‘ç£å­¦ä¹ ç›´æ¥ä»é…å¯¹å›¾åƒå’ŒæŠ¥å‘Šä¸­å­¦ä¹ åŒ»å­¦è§†è§‰è¡¨ç¤ºï¼Œå·²æˆä¸ºæ•°å­—è¯Šæ–­çš„ä¸€ç§æ–°é¢–ä¸”é«˜æ•ˆçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç°æœ‰æ¨¡å‹å­˜åœ¨å‡ ä¸ªä¸¥é‡çš„å±€é™æ€§ã€‚1ï¼‰å¿½è§†äº†è´Ÿæ ·æœ¬çš„é€‰æ‹©ï¼Œå¯¼è‡´ç¡¬è´Ÿæ ·æœ¬ç¨€ç¼ºï¼Œä»¥åŠé”™è¯¯è´Ÿæ ·æœ¬çš„åŒ…å«ï¼›2ï¼‰è™½ç„¶å…³æ³¨å…¨å±€ç‰¹å¾æå–ï¼Œä½†å´å¿½ç•¥äº†å¯¹äºåŒ»å­¦å›¾åƒè¯†åˆ«ä»»åŠ¡è‡³å…³é‡è¦çš„ç»†å¾®å±€éƒ¨ç»†èŠ‚ï¼›3ï¼‰å¯¹æ¯”å­¦ä¹ ä¸»è¦é’ˆå¯¹é«˜çº§ç‰¹å¾ï¼Œä½†å¿½ç•¥äº†å¯¹äºå‡†ç¡®åŒ»å­¦åˆ†æè‡³å…³é‡è¦çš„ä½çº§ç»†èŠ‚ã€‚é’ˆå¯¹è¿™äº›å…³é”®é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è·¨æ¨¡æ€èšç±»å¼•å¯¼è´Ÿé‡‡æ ·ï¼ˆCM-CGNSï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å…·æœ‰ä¸¤æ–¹é¢çš„æ€æƒ³ã€‚é¦–å…ˆï¼Œå®ƒå°†ç”¨äºå±€éƒ¨æ–‡æœ¬ç‰¹å¾çš„k-meansèšç±»æ‰©å±•åˆ°å¤šæ¨¡æ€é¢†åŸŸï¼Œé€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›æé«˜è´Ÿæ ·æœ¬çš„æ•°é‡å¹¶å¢å¼ºæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªè·¨æ¨¡æ€æ©è†œå›¾åƒé‡å»ºï¼ˆCM-MIRï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åˆ©ç”¨é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›è·å¾—çš„å±€éƒ¨æ–‡æœ¬åˆ°å›¾åƒçš„ç‰¹å¾æ¥é‡å»ºæ©è†œå±€éƒ¨å›¾åƒåŒºåŸŸã€‚è¿™æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„è·¨æ¨¡æ€ä¿¡æ¯äº¤äº’èƒ½åŠ›ï¼Œå¹¶ä¿ç•™äº†ä¸‹æ¸¸ä»»åŠ¡æ‰€éœ€çš„ä½çº§å›¾åƒç‰¹å¾ã€‚é€šè¿‡å¦¥å–„å¤„ç†ä¸Šè¿°å±€é™æ€§ï¼Œæ‰€æå‡ºçš„CM-CGNSå¯ä»¥å­¦ä¹ é€‚ç”¨äºå„ç§è¯†åˆ«ä»»åŠ¡çš„æœ‰æ•ˆå’Œé²æ£’çš„åŒ»å­¦è§†è§‰è¡¨ç¤ºã€‚åœ¨äº”ä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šè¿›è¡Œçš„åˆ†ç±»ã€æ£€æµ‹å’Œåˆ†å‰²ä»»åŠ¡çš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºæœ€æ–°æŠ€æœ¯æ–¹æ³•ï¼ŒéªŒè¯äº†å…¶å“è¶Šæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11674v1">PDF</a> This work has been submitted to the IEEE TMI for possible   publication. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/violet-42/CM-CGNS">https://github.com/violet-42/CM-CGNS</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè·¨æ¨¡æ€èšç±»å¼•å¯¼çš„è´Ÿé‡‡æ ·ï¼ˆCM-CGNSï¼‰æ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰åŒ»å­¦å›¾åƒè¡¨ç¤ºå­¦ä¹ æ¨¡å‹ä¸­çš„ä¸‰å¤§é—®é¢˜ã€‚é¦–å…ˆï¼Œå®ƒé€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›å°†Kå‡å€¼èšç±»ä»å•æ¨¡æ€æ‰©å±•åˆ°å¤šæ¨¡æ€é¢†åŸŸï¼Œæé«˜äº†è´Ÿæ ·æœ¬çš„æ•°é‡å¹¶å¢å¼ºäº†æ¨¡å‹è¡¨å¾èƒ½åŠ›ã€‚å…¶æ¬¡ï¼Œå¼•å…¥è·¨æ¨¡æ€æ©ç›–å›¾åƒé‡å»ºï¼ˆCM-MIRï¼‰æ¨¡å—ï¼Œåˆ©ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›è·å¾—çš„å±€éƒ¨æ–‡æœ¬åˆ°å›¾åƒçš„ç‰¹å¾æ¥é‡å»ºæ©ç›–çš„å±€éƒ¨å›¾åƒåŒºåŸŸï¼Œå¢å¼ºäº†æ¨¡å‹çš„è·¨æ¨¡æ€ä¿¡æ¯äº¤äº’èƒ½åŠ›å¹¶ä¿ç•™äº†é‡è¦çš„ä½çº§å›¾åƒç‰¹å¾ã€‚é€šè¿‡è§£å†³ä¸Šè¿°é—®é¢˜ï¼ŒCM-CGNSèƒ½å¤Ÿå­¦ä¹ é€‚ç”¨äºå„ç§è¯†åˆ«ä»»åŠ¡çš„åŒ»ç–—è§†è§‰è¡¨ç¤ºã€‚åœ¨äº”ä¸ªä¸‹æ¸¸æ•°æ®é›†ä¸Šçš„åˆ†ç±»ã€æ£€æµ‹å’Œåˆ†å‰²ä»»åŠ¡çš„å¹¿æ³›å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºæœ€æ–°æŠ€æœ¯ï¼ŒéªŒè¯äº†å…¶å“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>ç›´æ¥ä»é…å¯¹å›¾åƒå’ŒæŠ¥å‘Šä¸­å­¦ä¹ åŒ»å­¦è§†è§‰è¡¨ç¤ºå·²æˆä¸ºè¿‘å¹´æ¥çš„æ–°å‹é«˜æ•ˆæ•°å­—è¯Šæ–­æ–¹æ³•ã€‚</li>
<li>ç°æœ‰æ¨¡å‹å­˜åœ¨ä¸‰å¤§å±€é™æ€§ï¼šå¿½è§†è´Ÿæ ·æœ¬é€‰æ‹©ã€å…³æ³¨å…¨å±€ç‰¹å¾è€Œå¿½è§†å±€éƒ¨ç»†èŠ‚ä»¥åŠå¯¹æ¯”å­¦ä¹ ä¸»è¦å…³æ³¨é«˜çº§ç‰¹å¾è€Œå¿½è§†ä½çº§ç‰¹å¾ã€‚</li>
<li>CM-CGNSæ–¹æ³•é€šè¿‡è·¨æ¨¡æ€æ³¨æ„åŠ›å°†å•æ¨¡æ€é¢†åŸŸçš„Kå‡å€¼èšç±»æ‰©å±•åˆ°å¤šæ¨¡æ€é¢†åŸŸï¼Œæé«˜äº†è´Ÿæ ·æœ¬æ•°é‡å’Œæ¨¡å‹è¡¨å¾èƒ½åŠ›ã€‚</li>
<li>CM-MIRæ¨¡å—çš„å¼•å…¥åˆ©ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›è·å¾—çš„å±€éƒ¨æ–‡æœ¬åˆ°å›¾åƒçš„ç‰¹å¾æ¥é‡å»ºæ©ç›–çš„å±€éƒ¨å›¾åƒåŒºåŸŸï¼Œå¢å¼ºäº†æ¨¡å‹çš„è·¨æ¨¡æ€ä¿¡æ¯äº¤äº’èƒ½åŠ›ã€‚</li>
<li>CM-CGNSèƒ½å¤Ÿå­¦ä¹ é€‚ç”¨äºå„ç§åŒ»ç–—è¯†åˆ«ä»»åŠ¡çš„è§†è§‰è¡¨ç¤ºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤šä¸ªæ•°æ®é›†å’Œå¤šä¸ªä»»åŠ¡ä¸Šï¼ŒCM-CGNSæ–¹æ³•çš„è¡¨ç°ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11674">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-919c6a820049132bc6f9f888efd5955f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f9a2585a55b9ff1d43ad69626eb8504.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1e5d6341c07e5886d74936fb538b37b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Prohibited-Items-Segmentation-via-Occlusion-aware-Bilayer-Modeling"><a href="#Prohibited-Items-Segmentation-via-Occlusion-aware-Bilayer-Modeling" class="headerlink" title="Prohibited Items Segmentation via Occlusion-aware Bilayer Modeling"></a>Prohibited Items Segmentation via Occlusion-aware Bilayer Modeling</h2><p><strong>Authors:Yunhan Ren, Ruihuang Li, Lingbo Liu, Changwen Chen</strong></p>
<p>Instance segmentation of prohibited items in security X-ray images is a critical yet challenging task. This is mainly caused by the significant appearance gap between prohibited items in X-ray images and natural objects, as well as the severe overlapping among objects in X-ray images. To address these issues, we propose an occlusion-aware instance segmentation pipeline designed to identify prohibited items in X-ray images. Specifically, to bridge the representation gap, we integrate the Segment Anything Model (SAM) into our pipeline, taking advantage of its rich priors and zero-shot generalization capabilities. To address the overlap between prohibited items, we design an occlusion-aware bilayer mask decoder module that explicitly models the occlusion relationships. To supervise occlusion estimation, we manually annotated occlusion areas of prohibited items in two large-scale X-ray image segmentation datasets, PIDray and PIXray. We then reorganized these additional annotations together with the original information as two occlusion-annotated datasets, PIDray-A and PIXray-A. Extensive experimental results on these occlusion-annotated datasets demonstrate the effectiveness of our proposed method. The datasets and codes are available at: <a target="_blank" rel="noopener" href="https://github.com/Ryh1218/Occ">https://github.com/Ryh1218/Occ</a> </p>
<blockquote>
<p>åœ¨å®‰æ£€Xå…‰å›¾åƒä¸­å¯¹è¿ç¦ç‰©å“è¿›è¡Œå®ä¾‹åˆ†å‰²æ˜¯ä¸€é¡¹è‡³å…³é‡è¦ä¸”å¯Œæœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºè¿ç¦ç‰©å“åœ¨Xå…‰å›¾åƒä¸­çš„å¤–è§‚ä¸è‡ªç„¶ç‰©ä½“æœ‰å¾ˆå¤§çš„å·®è·ï¼Œä»¥åŠXå…‰å›¾åƒä¸­ç‰©ä½“ä¹‹é—´çš„ä¸¥é‡é‡å ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é®æŒ¡æ„ŸçŸ¥çš„å®ä¾‹åˆ†å‰²ç®¡é“ï¼Œç”¨äºè¯†åˆ«Xå…‰å›¾åƒä¸­çš„è¿ç¦ç‰©å“ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†å¼¥åˆè¡¨ç¤ºä¸Šçš„å·®è·ï¼Œæˆ‘ä»¬å°†â€œä¸‡ç‰©å¯åˆ†å‰²â€æ¨¡å‹ï¼ˆSegment Anything Modelï¼ŒSAMï¼‰é›†æˆåˆ°æˆ‘ä»¬çš„ç®¡é“ä¸­ï¼Œåˆ©ç”¨å…¶ä¸°å¯Œçš„å…ˆéªŒçŸ¥è¯†å’Œé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿ç¦ç‰©å“ä¹‹é—´çš„é‡å é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé®æŒ¡æ„ŸçŸ¥çš„åŒå±‚æ©è†œè§£ç å™¨æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥æ˜¾å¼åœ°å»ºæ¨¡é®æŒ¡å…³ç³»ã€‚ä¸ºäº†ç›‘ç£é®æŒ¡ä¼°è®¡ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªå¤§è§„æ¨¡çš„Xå…‰å›¾åƒåˆ†å‰²æ•°æ®é›†PIDrayå’ŒPIXrayä¸Šæ‰‹åŠ¨æ ‡æ³¨äº†è¿ç¦ç‰©å“çš„é®æŒ¡åŒºåŸŸï¼Œç„¶åå°†è¿™äº›é¢å¤–çš„æ ‡æ³¨ä¸åŸå§‹ä¿¡æ¯ä¸€èµ·é‡æ–°ç»„ç»‡æˆä¸¤ä¸ªå¸¦é®æŒ¡æ ‡æ³¨çš„æ•°æ®é›†PIDray-Aå’ŒPIXray-Aã€‚åœ¨è¿™äº›å¸¦é®æŒ¡æ ‡æ³¨çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„å¤§é‡å®éªŒç»“æœè¡¨æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Ryh1218/Occ%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Ryh1218/Occæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11661v1">PDF</a> Accepted by ICME 2025</p>
<p><strong>Summary</strong><br>ç¦æ­¢ç‰©å“åœ¨å®‰æ£€Xå…‰å›¾åƒä¸­çš„å®ä¾‹åˆ†å‰²æ˜¯ä¸€é¡¹é‡è¦è€Œå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é®æŒ¡æ„ŸçŸ¥çš„å®ä¾‹åˆ†å‰²ç®¡é“è®¾è®¡ï¼Œæ—¨åœ¨è¯†åˆ«Xå…‰å›¾åƒä¸­çš„ç¦æ­¢ç‰©å“ã€‚æˆ‘ä»¬æ•´åˆäº†åˆ†æ®µä»»ä½•äº‹æƒ…æ¨¡å‹ï¼ˆSAMï¼‰ï¼Œä»¥ç¼©å°è¡¨ç¤ºå·®è·å¹¶å¤„ç†ç‰©å“ä¹‹é—´çš„é‡å é—®é¢˜ã€‚ä¸ºè§£å†³ç¦æ­¢ç‰©å“çš„é®æŒ¡é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé®æŒ¡æ„ŸçŸ¥çš„åŒå±‚æ©è†œè§£ç æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥æ˜ç¡®å»ºæ¨¡é®æŒ¡å…³ç³»ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå¤§è§„æ¨¡çš„Xå…‰å›¾åƒåˆ†å‰²æ•°æ®é›†PIDrayå’ŒPIXrayä¸Šæ‰‹åŠ¨æ ‡æ³¨äº†ç¦æ­¢ç‰©å“çš„é®æŒ¡åŒºåŸŸï¼Œå¹¶ä»¥æ­¤åˆ›å»ºäº†ä¸¤ä¸ªå¸¦é®æŒ¡æ³¨é‡Šçš„æ•°æ®é›†PIDray-Aå’ŒPIXray-Aã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨è¿™äº›å¸¦é®æŒ¡æ³¨é‡Šçš„æ•°æ®é›†ä¸Šæ•ˆæœæ˜¾è‘—ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å®ä¾‹åˆ†å‰²åœ¨å®‰æ£€Xå…‰å›¾åƒä¸­è¯†åˆ«ç¦æ­¢ç‰©å“æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå› ä¸ºç‰©å“é—´å­˜åœ¨å¤–è§‚å·®å¼‚å’Œä¸¥é‡é‡å ã€‚</li>
<li>æˆ‘ä»¬æå‡ºäº†ä¸€ç§é®æŒ¡æ„ŸçŸ¥çš„å®ä¾‹åˆ†å‰²ç®¡é“è®¾è®¡æ¥è¯†åˆ«è¿™äº›ç‰©å“ã€‚</li>
<li>åˆ©ç”¨Segment Anything Modelï¼ˆSAMï¼‰ç¼©å°è¡¨ç¤ºå·®è·ã€‚</li>
<li>è®¾è®¡äº†ä¸€ä¸ªåŒå±‚æ©è†œè§£ç æ¨¡å—æ¥å¤„ç†ç‰©å“é—´çš„é®æŒ¡å…³ç³»ã€‚</li>
<li>åœ¨ä¸¤ä¸ªå¤§å‹Xå…‰å›¾åƒåˆ†å‰²æ•°æ®é›†ä¸Šæ‰‹åŠ¨æ ‡æ³¨äº†ç¦æ­¢ç‰©å“çš„é®æŒ¡åŒºåŸŸï¼Œå¹¶åˆ›å»ºäº†å¸¦é®æŒ¡æ³¨é‡Šçš„æ•°æ®é›†ã€‚</li>
<li>å®éªŒè¯æ˜åœ¨å¸¦é®æŒ¡æ³¨é‡Šçš„æ•°æ®é›†ä¸Šçš„æ–¹æ³•æ•ˆæœæ˜¾è‘—ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8fd3b2d8bbe6ca8478d1d8efe0fcb63b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07eb805eddd1a9758b179cdab9106746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-426ce04d30db68618608d72aaf893ef4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-423e6be170cd2c2edb074a78e4c46407.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6ce8e315b597f0191351be7544f5907.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cefe4fc38396dae12cc5162b9e929ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8db2c52feb295dab0ead5675a8708904.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Taming-Stable-Diffusion-for-Computed-Tomography-Blind-Super-Resolution"><a href="#Taming-Stable-Diffusion-for-Computed-Tomography-Blind-Super-Resolution" class="headerlink" title="Taming Stable Diffusion for Computed Tomography Blind Super-Resolution"></a>Taming Stable Diffusion for Computed Tomography Blind Super-Resolution</h2><p><strong>Authors:Chunlei Li, Yilei Shi, Haoxi Hu, Jingliang Hu, Xiao Xiang Zhu, Lichao Mou</strong></p>
<p>High-resolution computed tomography (CT) imaging is essential for medical diagnosis but requires increased radiation exposure, creating a critical trade-off between image quality and patient safety. While deep learning methods have shown promise in CT super-resolution, they face challenges with complex degradations and limited medical training data. Meanwhile, large-scale pre-trained diffusion models, particularly Stable Diffusion, have demonstrated remarkable capabilities in synthesizing fine details across various vision tasks. Motivated by this, we propose a novel framework that adapts Stable Diffusion for CT blind super-resolution. We employ a practical degradation model to synthesize realistic low-quality images and leverage a pre-trained vision-language model to generate corresponding descriptions. Subsequently, we perform super-resolution using Stable Diffusion with a specialized controlling strategy, conditioned on both low-resolution inputs and the generated text descriptions. Extensive experiments show that our method outperforms existing approaches, demonstrating its potential for achieving high-quality CT imaging at reduced radiation doses. Our code will be made publicly available. </p>
<blockquote>
<p>é«˜åˆ†è¾¨ç‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æˆåƒå¯¹äºåŒ»å­¦è¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†ä¼šå¢åŠ è¾å°„æš´éœ²ï¼Œä»è€Œåœ¨å›¾åƒè´¨é‡å’Œæ‚£è€…å®‰å…¨ä¹‹é—´å½¢æˆäº†å…³é”®çš„æƒè¡¡ã€‚æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨CTè¶…åˆ†è¾¨ç‡é¢†åŸŸå·²å±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ï¼Œä½†ä»é¢ä¸´å¤æ‚é€€åŒ–é—®é¢˜å’Œæœ‰é™çš„åŒ»å­¦è®­ç»ƒæ•°æ®æŒ‘æˆ˜ã€‚ä¸æ­¤åŒæ—¶ï¼Œå¤§è§„æ¨¡é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯Stable Diffusionï¼Œåœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­åˆæˆç²¾ç»†ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€‚åº”Stable Diffusionçš„CTç›²è¶…åˆ†è¾¨ç‡æ–°æ¡†æ¶ã€‚æˆ‘ä»¬é‡‡ç”¨å®ç”¨çš„é€€åŒ–æ¨¡å‹åˆæˆé€¼çœŸçš„ä½è´¨é‡å›¾åƒï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆç›¸åº”çš„æè¿°ã€‚éšåï¼Œæˆ‘ä»¬ä½¿ç”¨Stable Diffusionè¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ï¼Œé‡‡ç”¨ä¸“é—¨çš„æ§åˆ¶ç­–ç•¥ï¼Œä»¥ä½åˆ†è¾¨ç‡è¾“å…¥å’Œç”Ÿæˆçš„æ–‡æœ¬æè¿°ä¸ºæ¡ä»¶ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°äº†åœ¨é™ä½è¾å°„å‰‚é‡ä¸‹å®ç°é«˜è´¨é‡CTæˆåƒçš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€æä¾›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11496v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨åŒ»å­¦è¯Šæ–­ä¸­ï¼Œé«˜åˆ†è¾¨ç‡çš„è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æˆåƒè‡³å…³é‡è¦ï¼Œä½†åŒæ—¶ä¹Ÿå¢åŠ äº†æ‚£è€…çš„è¾å°„æš´éœ²é£é™©ã€‚æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨CTè¶…åˆ†è¾¨ç‡å¤„ç†æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ä»é¢ä¸´å¤æ‚é€€åŒ–å’Œæœ‰é™åŒ»å­¦è®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜ã€‚å—å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚Stable Diffusionï¼‰åœ¨åˆæˆå„ç§è§†è§‰ä»»åŠ¡ç²¾ç»†ç»†èŠ‚æ–¹é¢çš„å‡ºè‰²è¡¨ç°çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºStable Diffusionçš„CTç›²è¶…åˆ†è¾¨ç‡å¤„ç†çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å®ç”¨çš„é€€åŒ–æ¨¡å‹åˆæˆé€¼çœŸçš„ä½è´¨é‡å›¾åƒï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆç›¸åº”çš„æè¿°ã€‚ç„¶åï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸€ç§ç‰¹æ®Šçš„æ§åˆ¶ç­–ç•¥è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ï¼Œè¯¥ç­–ç•¥æ ¹æ®ä½åˆ†è¾¨ç‡è¾“å…¥å’Œç”Ÿæˆçš„æ–‡æœ¬æè¿°è¿›è¡Œæ¡ä»¶å¤„ç†ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œæœ‰æœ›åœ¨é™ä½è¾å°„å‰‚é‡çš„åŒæ—¶å®ç°é«˜è´¨é‡çš„CTæˆåƒã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€æä¾›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>é«˜åˆ†è¾¨ç‡CTæˆåƒå¯¹äºåŒ»å­¦è¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†éœ€è¦å¹³è¡¡å›¾åƒè´¨é‡å’Œæ‚£è€…è¾å°„æš´éœ²ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨CTè¶…åˆ†è¾¨ç‡å¤„ç†ä¸­æœ‰æ½œåŠ›ï¼Œä½†é¢ä¸´å¤æ‚é€€åŒ–å’Œæœ‰é™åŒ»å­¦æ•°æ®çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹å¦‚Stable Diffusionåœ¨åˆæˆç²¾ç»†ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºStable Diffusionçš„CTç›²è¶…åˆ†è¾¨ç‡å¤„ç†çš„æ–°æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶åˆ©ç”¨é€€åŒ–æ¨¡å‹åˆæˆä½è´¨é‡å›¾åƒå¹¶åˆ©ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆæè¿°ã€‚</li>
<li>åˆ©ç”¨ä½åˆ†è¾¨ç‡è¾“å…¥å’Œæ–‡æœ¬æè¿°è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ã€‚</li>
<li>å®éªŒè¡¨æ˜è¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œå¯å®ç°é™ä½è¾å°„å‰‚é‡ä¸‹çš„é«˜è´¨é‡CTæˆåƒã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11496">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-d397898207aa858353b577a64be996e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-757c01d51b11bd154ab8e931081dd179.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b706d9f8458750592023c83c66fc06f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Awareness-Enables-Efficient-Labeling-for-Cancer-Subtyping-in-Digital-Pathology"><a href="#Uncertainty-Awareness-Enables-Efficient-Labeling-for-Cancer-Subtyping-in-Digital-Pathology" class="headerlink" title="Uncertainty Awareness Enables Efficient Labeling for Cancer Subtyping in   Digital Pathology"></a>Uncertainty Awareness Enables Efficient Labeling for Cancer Subtyping in   Digital Pathology</h2><p><strong>Authors:Nirhoshan Sivaroopan, Chamuditha Jayanga Galappaththige, Chalani Ekanayake, Hasindri Watawana, Ranga Rodrigo, Chamira U. S. Edussooriya, Dushan N. Wadduwage</strong></p>
<p>Machine-learning-assisted cancer subtyping is a promising avenue in digital pathology. Cancer subtyping models, however, require careful training using expert annotations so that they can be inferred with a degree of known certainty (or uncertainty). To this end, we introduce the concept of uncertainty awareness into a self-supervised contrastive learning model. This is achieved by computing an evidence vector at every epoch, which assesses the modelâ€™s confidence in its predictions. The derived uncertainty score is then utilized as a metric to selectively label the most crucial images that require further annotation, thus iteratively refining the training process. With just 1-10% of strategically selected annotations, we attain state-of-the-art performance in cancer subtyping on benchmark datasets. Our method not only strategically guides the annotation process to minimize the need for extensive labeled datasets, but also improves the precision and efficiency of classifications. This development is particularly beneficial in settings where the availability of labeled data is limited, offering a promising direction for future research and application in digital pathology. </p>
<blockquote>
<p>æœºå™¨å­¦ä¹ è¾…åŠ©ç™Œç—‡åˆ†å‹æ˜¯æ•°å­—ç—…ç†å­¦é¢†åŸŸçš„ä¸€ä¸ªå‰æ™¯å¹¿é˜”çš„ç ”ç©¶æ–¹å‘ã€‚ç„¶è€Œï¼Œç™Œç—‡åˆ†å‹æ¨¡å‹éœ€è¦ä½¿ç”¨ä¸“å®¶æ³¨é‡Šè¿›è¡Œä»”ç»†è®­ç»ƒï¼Œä»¥ä¾¿èƒ½å¤Ÿä»¥å·²çŸ¥çš„ç¡®å®šæ€§ï¼ˆæˆ–ä¸ç¡®å®šæ€§ï¼‰è¿›è¡Œæ¨æ–­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä¸ç¡®å®šæ€§çš„æ¦‚å¿µå¼•å…¥äº†ä¸€ç§è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ¨¡å‹ã€‚è¿™æ˜¯é€šè¿‡æ¯ä¸ªæ—¶ä»£è®¡ç®—è¯æ®å‘é‡æ¥å®ç°çš„ï¼Œè¯¥è¯æ®å‘é‡è¯„ä¼°æ¨¡å‹å¯¹å…¶é¢„æµ‹çš„ç½®ä¿¡åº¦ã€‚ç„¶ååˆ©ç”¨æ´¾ç”Ÿå‡ºçš„ä¸ç¡®å®šæ€§è¯„åˆ†ä½œä¸ºæŒ‡æ ‡ï¼Œæœ‰é€‰æ‹©åœ°æ ‡è®°æœ€éœ€è¦è¿›ä¸€æ­¥æ³¨é‡Šçš„å›¾åƒï¼Œä»è€Œè¿­ä»£åœ°æ”¹è¿›è®­ç»ƒè¿‡ç¨‹ã€‚ä»…ä½¿ç”¨1-10%çš„æˆ˜ç•¥é€‰æ‹©æ³¨é‡Šï¼Œæˆ‘ä»¬åœ¨åŸºå‡†æ•°æ®é›†ä¸Šçš„ç™Œç—‡åˆ†å‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æˆ˜ç•¥æ€§åœ°å¼•å¯¼äº†æ³¨é‡Šè¿‡ç¨‹ï¼Œä»¥æœ€å¤§é™åº¦åœ°å‡å°‘éœ€è¦å¤§é‡æ ‡è®°æ•°æ®é›†çš„éœ€æ±‚ï¼Œè€Œä¸”è¿˜æé«˜äº†åˆ†ç±»çš„ç²¾ç¡®æ€§å’Œæ•ˆç‡ã€‚åœ¨æ ‡è®°æ•°æ®å¯ç”¨æ€§æœ‰é™çš„æƒ…å†µä¸‹ï¼Œè¿™ä¸€å‘å±•ç‰¹åˆ«æœ‰ç›Šï¼Œä¸ºæ•°å­—ç—…ç†å­¦æœªæ¥çš„ç ”ç©¶ä¸åº”ç”¨æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11439v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœºå™¨å­¦ä¹ è¾…åŠ©ç™Œç—‡åˆ†å‹æ˜¯æ•°å­—ç—…ç†å­¦ä¸­çš„ä¸€æ¡æœ‰å‰é€”çš„é“è·¯ã€‚æˆ‘ä»¬å¼•å…¥ä¸ç¡®å®šæ€§æ„è¯†æ¦‚å¿µåˆ°è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ¨¡å‹ä¸­ï¼Œé€šè¿‡è®¡ç®—æ¯ä¸ªæ—¶æœŸçš„è¯æ®å‘é‡è¯„ä¼°æ¨¡å‹çš„é¢„æµ‹ç½®ä¿¡åº¦ï¼Œå®ç°ä¸ç¡®å®šæ€§è¯„åˆ†ä½œä¸ºæŒ‡æ ‡æ¥é€‰æ‹©æ€§æ ‡æ³¨éœ€è¦æ›´å¤šæ³¨é‡Šçš„å…³é”®å›¾åƒï¼Œä»è€Œè¿­ä»£ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚ä»…ä½¿ç”¨1-10%çš„ç­–ç•¥é€‰æ‹©æ³¨é‡Šï¼Œå°±èƒ½åœ¨åŸºå‡†æ•°æ®é›†ä¸Šå®ç°ç™Œç—‡åˆ†å‹çš„æœ€æ–°æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æˆ˜ç•¥æ€§åœ°å¼•å¯¼æ³¨é‡Šè¿‡ç¨‹ï¼Œå‡å°‘éœ€è¦å¤§é‡æ ‡è®°æ•°æ®é›†çš„éœ€æ±‚ï¼Œè¿˜æé«˜äº†åˆ†ç±»çš„ç²¾ç¡®åº¦å’Œæ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ ‡è®°æ•°æ®æœ‰é™çš„ç¯å¢ƒä¸­ï¼Œä¸ºæ•°å­—ç—…ç†å­¦æœªæ¥çš„ç ”ç©¶ä¸åº”ç”¨æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ è¾…åŠ©ç™Œç—‡åˆ†å‹æ˜¯æ•°å­—ç—…ç†å­¦ä¸­çš„æœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚</li>
<li>å¼•å…¥ä¸ç¡®å®šæ€§æ„è¯†æ¦‚å¿µåˆ°è‡ªç›‘ç£å¯¹æ¯”å­¦ä¹ æ¨¡å‹ä¸­ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>é€šè¿‡è®¡ç®—è¯æ®å‘é‡è¯„ä¼°æ¨¡å‹çš„é¢„æµ‹ç½®ä¿¡åº¦ï¼Œå®ç°ä¸ç¡®å®šæ€§è¯„åˆ†ã€‚</li>
<li>åˆ©ç”¨ä¸ç¡®å®šæ€§è¯„åˆ†é€‰æ‹©æ€§æ ‡æ³¨éœ€è¦æ›´å¤šæ³¨é‡Šçš„å…³é”®å›¾åƒï¼Œè¿­ä»£ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>ä»…éœ€å°‘é‡ç­–ç•¥æ€§é€‰æ‹©çš„æ³¨é‡Šå°±èƒ½è¾¾åˆ°ç™Œç—‡åˆ†å‹çš„æœ€æ–°æ€§èƒ½ã€‚</li>
<li>æ–¹æ³•èƒ½æˆ˜ç•¥æ€§åœ°å¼•å¯¼æ³¨é‡Šè¿‡ç¨‹ï¼Œå‡å°‘éœ€è¦å¤§é‡æ ‡è®°æ•°æ®é›†çš„éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11439">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-e48c72b0f965b2239f27007bdf7506c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9be0f471d5e53b4fd32b45db83c394f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-902fa276b02203ccd55361e8a27974c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f923bcdbdd4ad4a3f1581b0d46bab601.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="3D-RAD-A-Comprehensive-3D-Radiology-Med-VQA-Dataset-with-Multi-Temporal-Analysis-and-Diverse-Diagnostic-Tasks"><a href="#3D-RAD-A-Comprehensive-3D-Radiology-Med-VQA-Dataset-with-Multi-Temporal-Analysis-and-Diverse-Diagnostic-Tasks" class="headerlink" title="3D-RAD: A Comprehensive 3D Radiology Med-VQA Dataset with Multi-Temporal   Analysis and Diverse Diagnostic Tasks"></a>3D-RAD: A Comprehensive 3D Radiology Med-VQA Dataset with Multi-Temporal   Analysis and Diverse Diagnostic Tasks</h2><p><strong>Authors:Xiaotang Gai, Jiaxiang Liu, Yichen Li, Zijie Meng, Jian Wu, Zuozhu Liu</strong></p>
<p>Medical Visual Question Answering (Med-VQA) holds significant potential for clinical decision support, yet existing efforts primarily focus on 2D imaging with limited task diversity. This paper presents 3D-RAD, a large-scale dataset designed to advance 3D Med-VQA using radiology CT scans. The 3D-RAD dataset encompasses six diverse VQA tasks: anomaly detection, image observation, medical computation, existence detection, static temporal diagnosis, and longitudinal temporal diagnosis. It supports both open- and closed-ended questions while introducing complex reasoning challenges, including computational tasks and multi-stage temporal analysis, to enable comprehensive benchmarking. Extensive evaluations demonstrate that existing vision-language models (VLMs), especially medical VLMs exhibit limited generalization, particularly in multi-temporal tasks, underscoring the challenges of real-world 3D diagnostic reasoning. To drive future advancements, we release a high-quality training set 3D-RAD-T of 136,195 expert-aligned samples, showing that fine-tuning on this dataset could significantly enhance model performance. Our dataset and code, aiming to catalyze multimodal medical AI research and establish a robust foundation for 3D medical visual understanding, are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Tang-xiaoxiao/M3D-RAD">https://github.com/Tang-xiaoxiao/M3D-RAD</a>. </p>
<blockquote>
<p>åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰åœ¨ä¸´åºŠå†³ç­–æ”¯æŒæ–¹é¢æ‹¥æœ‰å·¨å¤§æ½œåŠ›ï¼Œä½†ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨äºŒç»´æˆåƒä¸Šï¼Œä»»åŠ¡å¤šæ ·æ€§æœ‰é™ã€‚æœ¬æ–‡ä»‹ç»äº†å¤§è§„æ¨¡æ•°æ®é›†3D-RADï¼Œæ—¨åœ¨åˆ©ç”¨æ”¾å°„å­¦CTæ‰«ææ¨è¿›ä¸‰ç»´Med-VQAçš„å‘å±•ã€‚3D-RADæ•°æ®é›†åŒ…å«å…­ä¸ªå¤šæ ·åŒ–çš„VQAä»»åŠ¡ï¼šå¼‚å¸¸æ£€æµ‹ã€å›¾åƒè§‚å¯Ÿã€åŒ»å­¦è®¡ç®—ã€å­˜åœ¨æ£€æµ‹ã€é™æ€æ—¶é—´è¯Šæ–­å’Œçºµå‘æ—¶é—´è¯Šæ–­ã€‚å®ƒæ”¯æŒå¼€æ”¾å’Œå°é—­æ€§é—®é¢˜ï¼ŒåŒæ—¶å¼•å…¥å¤æ‚çš„æ¨ç†æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®¡ç®—ä»»åŠ¡å’Œå¤šé˜¶æ®µæ—¶é—´åˆ†æï¼Œä»¥å®ç°å…¨é¢çš„åŸºå‡†æµ‹è¯•ã€‚å¤§é‡è¯„ä¼°è¡¨æ˜ï¼Œç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰ç‰¹åˆ«æ˜¯åŒ»å­¦VLMsçš„æ³›åŒ–èƒ½åŠ›æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ—¶é—´ä»»åŠ¡ä¸­ï¼Œè¿™çªå‡ºäº†ç°å®ä¸–ç•Œä¸‰ç»´è¯Šæ–­æ¨ç†çš„æŒ‘æˆ˜ã€‚ä¸ºäº†æ¨åŠ¨æœªæ¥çš„è¿›æ­¥ï¼Œæˆ‘ä»¬å‘å¸ƒäº†é«˜è´¨é‡çš„è®­ç»ƒé›†3D-RAD-Tï¼ŒåŒ…å«136,195ä¸ªä¸“å®¶å¯¹é½æ ·æœ¬ï¼Œè¡¨æ˜åœ¨æ­¤æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç æ—¨åœ¨æ¨åŠ¨å¤šæ¨¡æ€åŒ»ç–—äººå·¥æ™ºèƒ½ç ”ç©¶å¹¶ä¸ºä¸‰ç»´åŒ»å­¦è§†è§‰ç†è§£å»ºç«‹ç¨³å¥çš„åŸºç¡€ï¼Œå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Tang-xiaoxiao/M3D-RAD%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Tang-xiaoxiao/M3D-RADä¸Šå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11147v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†3D-RADï¼Œç”¨äºæ¨è¿›åŸºäºä¸‰ç»´åŒ»å­¦å½±åƒï¼ˆå¦‚CTæ‰«æï¼‰çš„åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰ã€‚è¯¥æ•°æ®é›†æ¶µç›–å…­ç§å¤šæ ·çš„é—®ç­”ä»»åŠ¡ï¼Œæ”¯æŒå¼€æ”¾å’Œå°é—­æ€§é—®é¢˜ï¼Œå¹¶å¼•å…¥å¤æ‚çš„æ¨ç†æŒ‘æˆ˜ï¼Œå¦‚è®¡ç®—ä»»åŠ¡å’Œå¤šæ—¶ç›¸åˆ†æã€‚ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è¯¥æ•°æ®é›†ä¸Šçš„è¡¨ç°æœ‰é™ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ—¶ç›¸ä»»åŠ¡ä¸­ã€‚ä¸ºæå‡æ¨¡å‹æ€§èƒ½ï¼Œå‘å¸ƒäº†ä¸€ä¸ªé«˜è´¨é‡çš„è®­ç»ƒé›†3D-RAD-Tã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D-RADæ•°æ®é›†ç”¨äºæ¨è¿›åŸºäºä¸‰ç»´åŒ»å­¦å½±åƒçš„åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰ã€‚</li>
<li>è¯¥æ•°æ®é›†åŒ…å«å…­ç§å¤šæ ·çš„é—®ç­”ä»»åŠ¡ï¼Œæ¶µç›–å¼‚å¸¸æ£€æµ‹ã€å›¾åƒè§‚å¯Ÿã€åŒ»å­¦è®¡ç®—ç­‰ã€‚</li>
<li>å¼•å…¥å¤æ‚æ¨ç†æŒ‘æˆ˜ï¼Œå¦‚è®¡ç®—ä»»åŠ¡å’Œå¤šæ—¶ç›¸åˆ†æã€‚</li>
<li>ç°æœ‰è§†è§‰è¯­è¨€æ¨¡å‹åœ¨è¯¥æ•°æ®é›†ä¸Šçš„è¡¨ç°æœ‰é™ï¼Œå°¤å…¶åœ¨å¤šæ—¶ç›¸ä»»åŠ¡ä¸­ã€‚</li>
<li>å…¬å¼€äº†ä¸€ä¸ªé«˜è´¨é‡çš„è®­ç»ƒé›†3D-RAD-Tï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ•°æ®é›†å’Œä»£ç å…¬å¼€å¯ç”¨ï¼Œæ—¨åœ¨æ¨åŠ¨å¤šå­¦ç§‘åŒ»ç–—äººå·¥æ™ºèƒ½ç ”ç©¶ï¼Œå¹¶ä¸ºä¸‰ç»´åŒ»å­¦è§†è§‰ç†è§£å»ºç«‹ç¨³å¥åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11147">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-63385aa89d52cd7a190991f769c9dda2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17e48f3f52c339470d7f89f29b8fa2db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-907aeef9c824d1ca496481109b73e3bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-005c841a0e7cbb0a4410f7dac515eb0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e5030f5bcea2c40accd0552e31c48d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42297fc5baf83e41d5e119bf55a6589a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a1ef17c4ce2c9f3b0c4a6156b509146.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ContextLoss-Context-Information-for-Topology-Preserving-Segmentation"><a href="#ContextLoss-Context-Information-for-Topology-Preserving-Segmentation" class="headerlink" title="ContextLoss: Context Information for Topology-Preserving Segmentation"></a>ContextLoss: Context Information for Topology-Preserving Segmentation</h2><p><strong>Authors:Benedict Schacht, Imke Greving, Simone Frintrop, Berit Zeller-Plumhoff, Christian Wilms</strong></p>
<p>In image segmentation, preserving the topology of segmented structures like vessels, membranes, or roads is crucial. For instance, topological errors on road networks can significantly impact navigation. Recently proposed solutions are loss functions based on critical pixel masks that consider the whole skeleton of the segmented structures in the critical pixel mask. We propose the novel loss function ContextLoss (CLoss) that improves topological correctness by considering topological errors with their whole context in the critical pixel mask. The additional context improves the network focus on the topological errors. Further, we propose two intuitive metrics to verify improved connectivity due to a closing of missed connections. We benchmark our proposed CLoss on three public datasets (2D &amp; 3D) and our own 3D nano-imaging dataset of bone cement lines. Training with our proposed CLoss increases performance on topology-aware metrics and repairs up to 44% more missed connections than other state-of-the-art methods. We make the code publicly available. </p>
<blockquote>
<p>åœ¨å›¾åƒåˆ†å‰²ä¸­ï¼Œä¿æŒåˆ†å‰²ç»“æ„ï¼ˆå¦‚è¡€ç®¡ã€è†œæˆ–é“è·¯ï¼‰çš„æ‹“æ‰‘ç»“æ„è‡³å…³é‡è¦ã€‚ä¾‹å¦‚ï¼Œé“è·¯ç½‘ç»œä¸Šçš„æ‹“æ‰‘é”™è¯¯ä¼šå¯¹å¯¼èˆªäº§ç”Ÿé‡å¤§å½±å“ã€‚æœ€è¿‘æå‡ºçš„è§£å†³æ–¹æ¡ˆæ˜¯åŸºäºå…³é”®åƒç´ æ©è†œçš„æŸå¤±å‡½æ•°ï¼Œå®ƒè€ƒè™‘äº†å…³é”®åƒç´ æ©è†œä¸­åˆ†å‰²ç»“æ„çš„æ•´ä½“éª¨æ¶ã€‚æˆ‘ä»¬æå‡ºäº†æ–°å‹çš„æŸå¤±å‡½æ•°ContextLossï¼ˆCLossï¼‰ï¼Œé€šè¿‡è€ƒè™‘å…³é”®åƒç´ æ©è†œä¸­æ‹“æ‰‘é”™è¯¯åŠå…¶æ•´ä½“ä¸Šä¸‹æ–‡ï¼Œæé«˜äº†æ‹“æ‰‘çš„æ­£ç¡®æ€§ã€‚é¢å¤–çš„ä¸Šä¸‹æ–‡æé«˜äº†ç½‘ç»œå¯¹æ‹“æ‰‘é”™è¯¯çš„å…³æ³¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªç›´è§‚çš„åº¦é‡æŒ‡æ ‡ï¼Œä»¥éªŒè¯ç”±äºé—­åˆçš„é—æ¼è¿æ¥è€Œæé«˜äº†è¿é€šæ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ï¼ˆ2Då’Œ3Dï¼‰ä»¥åŠæˆ‘ä»¬è‡ªå·±çš„3Dçº³ç±³æˆåƒéª¨æ°´æ³¥çº¿æ•°æ®é›†ä¸Šå¯¹æˆ‘ä»¬çš„CLossè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ã€‚ä½¿ç”¨æˆ‘ä»¬æå‡ºçš„CLossè¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æé«˜æ‹“æ‰‘æ„ŸçŸ¥æŒ‡æ ‡çš„æ€§èƒ½ï¼Œå¹¶ä¸”å¯ä»¥ä¿®å¤æ¯”å…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•å¤šè¾¾44%çš„é—æ¼è¿æ¥ã€‚æˆ‘ä»¬å…¬å¼€æä¾›ä»£ç ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11134v1">PDF</a> 13 pages, 7 figures, accepted to ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹çš„æŸå¤±å‡½æ•°ContextLossï¼ˆCLossï¼‰ï¼Œç”¨äºæ”¹è¿›å›¾åƒåˆ†å‰²ä¸­çš„æ‹“æ‰‘æ­£ç¡®æ€§ã€‚è¯¥å‡½æ•°é€šè¿‡è€ƒè™‘å…³é”®åƒç´ æ©è†œä¸­çš„æ‹“æ‰‘é”™è¯¯åŠå…¶æ•´ä½“ä¸Šä¸‹æ–‡ï¼Œæé«˜äº†ç½‘ç»œå¯¹æ‹“æ‰‘é”™è¯¯çš„å…³æ³¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸¤ç§ç›´è§‚çš„åº¦é‡æŒ‡æ ‡ï¼Œä»¥éªŒè¯å› é—­åˆé—æ¼è¿æ¥è€Œæé«˜çš„è¿é€šæ€§ã€‚åœ¨ä¸‰ä¸ªå…¬å…±æ•°æ®é›†ï¼ˆåŒ…æ‹¬2Då’Œ3Dï¼‰ä»¥åŠè‡ªå·±çš„3Dçº³ç±³æˆåƒæ•°æ®é›†ä¸Šè¿›è¡Œçš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œä½¿ç”¨CLossè¿›è¡Œè®­ç»ƒå¯æé«˜æ‹“æ‰‘æ„ŸçŸ¥æŒ‡æ ‡çš„æ€§èƒ½ï¼Œå¹¶ä¿®å¤äº†æ¯”å…¶ä»–æœ€å…ˆè¿›æ–¹æ³•å¤šå‡º44%çš„é—æ¼è¿æ¥ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ContextLossï¼ˆCLossï¼‰æ˜¯ä¸€ç§æ–°å‹çš„æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨æ”¹è¿›å›¾åƒåˆ†å‰²ä¸­çš„æ‹“æ‰‘æ­£ç¡®æ€§ã€‚</li>
<li>CLossé€šè¿‡è€ƒè™‘å…³é”®åƒç´ æ©è†œä¸­çš„æ‹“æ‰‘é”™è¯¯åŠå…¶æ•´ä½“ä¸Šä¸‹æ–‡ï¼Œæé«˜ç½‘ç»œå¯¹æ‹“æ‰‘é”™è¯¯çš„å…³æ³¨ã€‚</li>
<li>æå‡ºä¸¤ç§ç›´è§‚åº¦é‡æŒ‡æ ‡ï¼Œç”¨äºéªŒè¯å› ä½¿ç”¨CLossè€Œæé«˜çš„è¿é€šæ€§ã€‚</li>
<li>åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œä½¿ç”¨CLossè¿›è¡Œè®­ç»ƒå¯æé«˜æ‹“æ‰‘æ„ŸçŸ¥æŒ‡æ ‡çš„æ€§èƒ½ã€‚</li>
<li>CLossèƒ½ä¿®å¤æ¯”å…¶ä»–æœ€å…ˆè¿›æ–¹æ³•å¤šå‡º44%çš„é—æ¼è¿æ¥ã€‚</li>
<li>ä»£ç å·²å…¬å¼€ä¾›å…¬ä¼—ä½¿ç”¨ã€‚</li>
<li>CLossçš„åº”ç”¨èŒƒå›´å¯èƒ½æ¶µç›–éœ€è¦é«˜åº¦å…³æ³¨æ‹“æ‰‘æ­£ç¡®æ€§çš„é¢†åŸŸï¼Œå¦‚é“è·¯ç½‘ç»œã€è¡€ç®¡åˆ†å‰²ç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11134">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-965216df85f471e4b92ece45b6fd5d8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5023c524f36bfe4eaa5e593e779f8bdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0c596bd2b6857f863cbdaaaf4b8e3a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e2445e0db62bae762ce6934d82575ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6dd10936f4a0b6e4fad357c831f08134.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c8c9c0c9fd1e0616bd9e6073184fdaa.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="PiPViT-Patch-based-Visual-Interpretable-Prototypes-for-Retinal-Image-Analysis"><a href="#PiPViT-Patch-based-Visual-Interpretable-Prototypes-for-Retinal-Image-Analysis" class="headerlink" title="PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image   Analysis"></a>PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image   Analysis</h2><p><strong>Authors:Marzieh Oghbaie, Teresa AraÃºjo, Hrvoje BogunoviÄ‡</strong></p>
<p>Background and Objective: Prototype-based methods improve interpretability by learning fine-grained part-prototypes; however, their visualization in the input pixel space is not always consistent with human-understandable biomarkers. In addition, well-known prototype-based approaches typically learn extremely granular prototypes that are less interpretable in medical imaging, where both the presence and extent of biomarkers and lesions are critical.   Methods: To address these challenges, we propose PiPViT (Patch-based Visual Interpretable Prototypes), an inherently interpretable prototypical model for image recognition. Leveraging a vision transformer (ViT), PiPViT captures long-range dependencies among patches to learn robust, human-interpretable prototypes that approximate lesion extent only using image-level labels. Additionally, PiPViT benefits from contrastive learning and multi-resolution input processing, which enables effective localization of biomarkers across scales.   Results: We evaluated PiPViT on retinal OCT image classification across four datasets, where it achieved competitive quantitative performance compared to state-of-the-art methods while delivering more meaningful explanations. Moreover, quantitative evaluation on a hold-out test set confirms that the learned prototypes are semantically and clinically relevant. We believe PiPViT can transparently explain its decisions and assist clinicians in understanding diagnostic outcomes. Github page: <a target="_blank" rel="noopener" href="https://github.com/marziehoghbaie/PiPViT">https://github.com/marziehoghbaie/PiPViT</a> </p>
<blockquote>
<p>èƒŒæ™¯ä¸ç›®æ ‡ï¼šåŸºäºåŸå‹çš„æ–¹æ³•é€šè¿‡å­¦ä¹ ç²¾ç»†çš„å±€éƒ¨åŸå‹æ¥æé«˜è§£é‡Šæ€§ï¼Œä½†å®ƒä»¬åœ¨è¾“å…¥åƒç´ ç©ºé—´ä¸­çš„å¯è§†åŒ–å¹¶ä¸æ€»æ˜¯ä¸äººä»¬å¯ç†è§£çš„ç”Ÿç‰©æ ‡å¿—ç‰©ç›¸ä¸€è‡´ã€‚æ­¤å¤–ï¼Œä¼—æ‰€å‘¨çŸ¥çš„åŸºäºåŸå‹çš„æ–¹æ³•é€šå¸¸å­¦ä¹ æå…¶ç²¾ç»†çš„åŸå‹ï¼Œåœ¨åŒ»å­¦æˆåƒä¸­ä¸å¤ªå®¹æ˜“è§£é‡Šï¼Œå…¶ä¸­ç”Ÿç‰©æ ‡å¿—ç‰©å’Œç—…å˜çš„å­˜åœ¨å’Œç¨‹åº¦éƒ½è‡³å…³é‡è¦ã€‚æ–¹æ³•ï¼šé’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†PiPViTï¼ˆåŸºäºè¡¥ä¸çš„è§†è§‰å¯è§£é‡ŠåŸå‹ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå›¾åƒè¯†åˆ«çš„å›ºæœ‰å¯è§£é‡ŠåŸå‹æ¨¡å‹ã€‚å€ŸåŠ©è§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰ï¼ŒPiPViTæ•è·è¡¥ä¸ä¹‹é—´çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œä»…ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾å­¦ä¹ ç¨³å¥ä¸”äººç±»å¯è§£é‡Šçš„åŸå‹ï¼Œä»¥è¿‘ä¼¼ç—…å˜ç¨‹åº¦ã€‚æ­¤å¤–ï¼ŒPiPViTå—ç›Šäºå¯¹æ¯”å­¦ä¹ å’Œå¤šåˆ†è¾¨ç‡è¾“å…¥å¤„ç†ï¼Œè¿™å®ç°äº†è·¨å°ºåº¦çš„ç”Ÿç‰©æ ‡å¿—ç‰©æœ‰æ•ˆå®šä½ã€‚ç»“æœï¼šæˆ‘ä»¬åœ¨å››ä¸ªæ•°æ®é›†ä¸Šå¯¹è§†ç½‘è†œOCTå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¯„ä¼°äº†PiPViTï¼Œå…¶åœ¨å®šé‡æ€§èƒ½ä¸Šè¾¾åˆ°äº†ä¸æœ€å…ˆè¿›çš„æ–¹æ³•ç›¸å½“çš„ç«äº‰æ°´å¹³ï¼ŒåŒæ—¶æä¾›äº†æ›´æœ‰æ„ä¹‰çš„è§£é‡Šã€‚æ­¤å¤–ï¼Œåœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šçš„å®šé‡è¯„ä¼°è¯å®ï¼Œæ‰€å­¦ä¹ çš„åŸå‹åœ¨è¯­ä¹‰å’Œä¸´åºŠä¸Šéƒ½å…·æœ‰ç›¸å…³æ€§ã€‚æˆ‘ä»¬ç›¸ä¿¡PiPViTèƒ½å¤Ÿé€æ˜åœ°è§£é‡Šå…¶å†³ç­–ï¼Œå¹¶å¸®åŠ©ä¸´åºŠåŒ»ç”Ÿç†è§£è¯Šæ–­ç»“æœã€‚GitHubé¡µé¢ï¼š<a target="_blank" rel="noopener" href="https://github.com/marziehoghbaie/PiPViT">https://github.com/marziehoghbaie/PiPViT</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10669v2">PDF</a> </p>
<p><strong>Summary</strong><br>     PiPViTæ˜¯ä¸€ä¸ªåŸºäºè§†è§‰çš„è§£è¯»åŸå‹æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³åŒ»å­¦å›¾åƒä¸­ç°æœ‰åŸå‹æ–¹æ³•çš„å¯è§†åŒ–ä¸è§£è¯»é—®é¢˜ã€‚é€šè¿‡æ•æ‰å›¾åƒå—ä¹‹é—´çš„é•¿æœŸä¾èµ–å…³ç³»ï¼Œè¯¥æ¨¡å‹èƒ½å­¦ä¹ é²æ£’ä¸”å¯è§£è¯»çš„åŸå‹ï¼Œå¹¶è¿‘ä¼¼ç—…ç¶èŒƒå›´ã€‚é€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œå¤šåˆ†è¾¨ç‡è¾“å…¥å¤„ç†ï¼Œæ¨¡å‹åœ¨è§†ç½‘è†œOCTå›¾åƒåˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒæ—¶æä¾›æœ‰æ„ä¹‰çš„è§£é‡Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PiPViTæ˜¯ä¸€ä¸ªé’ˆå¯¹å›¾åƒè¯†åˆ«çš„åŸºäºè§†è§‰çš„å¯è§£è¯»åŸå‹æ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹åˆ©ç”¨è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ•æ‰å›¾åƒå—ä¹‹é—´çš„é•¿æœŸä¾èµ–å…³ç³»ã€‚</li>
<li>PiPViTèƒ½å­¦ä¹ é²æ£’ä¸”å¯è§£è¯»çš„åŸå‹ï¼Œè¿™äº›åŸå‹å¯ä»¥è¿‘ä¼¼è¡¨ç¤ºç—…ç¶èŒƒå›´ã€‚</li>
<li>PiPViTé€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œå¤šåˆ†è¾¨ç‡è¾“å…¥å¤„ç†ï¼Œå®ç°äº†æœ‰æ•ˆå®šä½ä¸åŒå°ºåº¦çš„ç”Ÿç‰©æ ‡å¿—ç‰©ã€‚</li>
<li>åœ¨å››ä¸ªè§†ç½‘è†œOCTå›¾åƒåˆ†ç±»æ•°æ®é›†ä¸Šï¼ŒPiPViTè¡¨ç°å‡ºä¸æœ€æ–°æŠ€æœ¯ç›¸å½“çš„å®šé‡æ€§èƒ½ã€‚</li>
<li>å®šæ€§è¯„ä»·è¡¨æ˜ï¼Œå­¦ä¹ åˆ°çš„åŸå‹å…·æœ‰è¯­ä¹‰å’Œä¸´åºŠç›¸å…³æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10669">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0969f7adfbbed0f62ac0ac02d0c509fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22c307ff2862f361125a11c7c90838b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e77e404dc185bcbe1d351602dcb3826d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0b64cacedf2e77b764f7d8df895575b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="We-Care-Each-Pixel-Calibrating-on-Medical-Segmentation-Model"><a href="#We-Care-Each-Pixel-Calibrating-on-Medical-Segmentation-Model" class="headerlink" title="We Care Each Pixel: Calibrating on Medical Segmentation Model"></a>We Care Each Pixel: Calibrating on Medical Segmentation Model</h2><p><strong>Authors:Wenhao Liang, Wei Zhang, Lin Yue, Miao Xu, Olaf Maennel, Weitong Chen</strong></p>
<p>Medical image segmentation is fundamental for computer-aided diagnostics, providing accurate delineation of anatomical structures and pathological regions. While common metrics such as Accuracy, DSC, IoU, and HD primarily quantify spatial agreement between predictions and ground-truth labels, they do not assess the calibration quality of segmentation models, which is crucial for clinical reliability. To address this limitation, we propose pixel-wise Expected Calibration Error (pECE), a novel metric that explicitly measures miscalibration at the pixel level, thereby ensuring both spatial precision and confidence reliability. We further introduce a morphological adaptation strategy that applies morphological operations to ground-truth masks before computing calibration losses, particularly benefiting margin-based losses such as Margin SVLS and NACL. Additionally, we present the Signed Distance Calibration Loss (SDC), which aligns boundary geometry with calibration objectives by penalizing discrepancies between predicted and ground-truth signed distance functions (SDFs). Extensive experiments demonstrate that our method not only enhances segmentation performance but also improves calibration quality, yielding more trustworthy confidence estimates. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/EagleAdelaide/SDC-Loss">https://github.com/EagleAdelaide/SDC-Loss</a>. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­è‡³å…³é‡è¦ï¼Œå®ƒæä¾›äº†å‡†ç¡®çš„è§£å‰–ç»“æ„å’Œç—…ç†åŒºåŸŸçš„æè¿°ã€‚è™½ç„¶å‡†ç¡®ç‡ã€DSCã€IoUå’ŒHDç­‰å¸¸ç”¨æŒ‡æ ‡ä¸»è¦é‡åŒ–é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾ä¹‹é—´çš„ç©ºé—´ä¸€è‡´æ€§ï¼Œä½†å®ƒä»¬å¹¶æ²¡æœ‰è¯„ä¼°åˆ†å‰²æ¨¡å‹çš„æ ¡å‡†è´¨é‡ï¼Œè¿™å¯¹äºä¸´åºŠå¯é æ€§è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†åƒç´ çº§æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆpECEï¼‰è¿™ä¸€æ–°å‹æŒ‡æ ‡ï¼Œå®ƒæ˜ç¡®åœ°æµ‹é‡åƒç´ çº§çš„æ ¡å‡†è¯¯å·®ï¼Œä»è€Œç¡®ä¿ç©ºé—´ç²¾åº¦å’Œç½®ä¿¡åº¦å¯é æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å½¢æ€å­¦é€‚åº”ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨å¯¹çœŸå®æ ‡ç­¾æ©è†œè¿›è¡Œå½¢æ€å­¦æ“ä½œåæ‰è®¡ç®—æ ¡å‡†æŸå¤±ï¼Œè¿™å¯¹åŸºäºè¾¹è·çš„æŸå¤±ï¼ˆå¦‚Margin SVLSå’ŒNACLï¼‰ç‰¹åˆ«æœ‰ç›Šã€‚å¦å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¦å·è·ç¦»æ ¡å‡†æŸå¤±ï¼ˆSDCï¼‰ï¼Œå®ƒé€šè¿‡æƒ©ç½šé¢„æµ‹å’ŒçœŸå®æ ‡ç­¾ä¹‹é—´ç¬¦å·è·ç¦»å‡½æ•°ï¼ˆSDFï¼‰çš„å·®å¼‚ï¼Œä½¿è¾¹ç•Œå‡ ä½•ä¸æ ¡å‡†ç›®æ ‡å¯¹é½ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æé«˜äº†åˆ†å‰²æ€§èƒ½ï¼Œè¿˜æé«˜äº†æ ¡å‡†è´¨é‡ï¼Œäº§ç”Ÿäº†æ›´å¯é çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚ä»£ç å¯åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/EagleAdelaide/SDC-Loss%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/EagleAdelaide/SDC-Lossè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05107v2">PDF</a> Under Reviewing</p>
<p><strong>Summary</strong></p>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹äºè®¡ç®—æœºè¾…åŠ©è¯Šæ–­è‡³å…³é‡è¦ï¼Œèƒ½å¤Ÿå‡†ç¡®æç»˜è§£å‰–ç»“æ„å’Œç—…ç†åŒºåŸŸã€‚ä¸ºæé«˜åˆ†å‰²æ¨¡å‹çš„æ ¡å‡†è´¨é‡ï¼Œæå‡ºåƒç´ çº§æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆpECEï¼‰è¿™ä¸€æ–°æŒ‡æ ‡ï¼Œä»¥è¡¡é‡åƒç´ çº§åˆ«çš„è¯¯æ ¡å‡†æƒ…å†µï¼Œç¡®ä¿ç©ºé—´ç²¾åº¦å’Œç½®ä¿¡åº¦å¯é æ€§ã€‚åŒæ—¶ï¼Œå¼•å…¥å½¢æ€å­¦é€‚åº”ç­–ç•¥ï¼Œå¯¹çœŸå®æ ‡ç­¾æ©è†œè¿›è¡Œå½¢æ€å­¦æ“ä½œæ¥è®¡ç®—æ ¡å‡†æŸå¤±ï¼Œå°¤å…¶æœ‰åˆ©äºåŸºäºè¾¹è·çš„æŸå¤±ã€‚æ­¤å¤–ï¼Œæ¨å‡ºç¬¦å·è·ç¦»æ ¡å‡†æŸå¤±ï¼ˆSDCï¼‰ï¼Œé€šè¿‡æƒ©ç½šé¢„æµ‹ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„ç¬¦å·è·ç¦»å‡½æ•°å·®å¼‚ï¼Œå¯¹é½è¾¹ç•Œå‡ ä½•ä¸æ ¡å‡†ç›®æ ‡ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•ä¸ä»…æå‡åˆ†å‰²æ€§èƒ½ï¼Œæ›´æé«˜æ ¡å‡†è´¨é‡ï¼Œäº§ç”Ÿæ›´å¯é çš„ç½®ä¿¡åº¦ä¼°è®¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²å¯¹è®¡ç®—æœºè¾…åŠ©è¯Šæ–­è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰è¯„ä¼°æŒ‡æ ‡ä¸»è¦å…³æ³¨ç©ºé—´ä¸€è‡´æ€§ï¼Œç¼ºä¹æ¨¡å‹æ ¡å‡†è´¨é‡çš„è¯„ä¼°ã€‚</li>
<li>æå‡ºåƒç´ çº§æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆpECEï¼‰æŒ‡æ ‡ï¼Œè¡¡é‡åƒç´ çº§åˆ«è¯¯æ ¡å‡†æƒ…å†µã€‚</li>
<li>å¼•å…¥å½¢æ€å­¦é€‚åº”ç­–ç•¥ï¼Œé€šè¿‡å½¢æ€å­¦æ“ä½œè®¡ç®—æ ¡å‡†æŸå¤±ã€‚</li>
<li>ä»‹ç»ç¬¦å·è·ç¦»æ ¡å‡†æŸå¤±ï¼ˆSDCï¼‰ï¼Œå¯¹é½è¾¹ç•Œå‡ ä½•ä¸æ ¡å‡†ç›®æ ‡ã€‚</li>
<li>æ–¹æ³•æå‡åˆ†å‰²æ€§èƒ½åŠæ ¡å‡†è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05107">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6571beca283858a65736daeb9649e2b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbc77f38220450278a050143f7268ad4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0db425d3f06a5702e1e7d48ccf207c04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aef861b8218c480a2e21485991422d7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Censor-Aware-Semi-Supervised-Survival-Time-Prediction-in-Lung-Cancer-Using-Clinical-and-Radiomics-Features"><a href="#Censor-Aware-Semi-Supervised-Survival-Time-Prediction-in-Lung-Cancer-Using-Clinical-and-Radiomics-Features" class="headerlink" title="Censor-Aware Semi-Supervised Survival Time Prediction in Lung Cancer   Using Clinical and Radiomics Features"></a>Censor-Aware Semi-Supervised Survival Time Prediction in Lung Cancer   Using Clinical and Radiomics Features</h2><p><strong>Authors:Arman Gorji, Ali Fathi Jouzdani, Nima Sanati, Ren Yuan, Arman Rahmim, Mohammad R. Salmanpour</strong></p>
<p>Objectives: Lung cancer poses a significant global health challenge, necessitating improved prognostic methods for personalized treatment. This study introduces a censor-aware semi-supervised learning (SSL) framework that integrates clinical and imaging data, addressing biases in traditional models handling censored data. Methods: We analyzed clinical, PET and CT data from 199 lung cancer patients from public and local data respositories, focusing on overall survival (OS) time as the primary outcome. Handcrafted (HRF) and Deep Radiomics features (DRF) were extracted after preprocessing using ViSERA software and were combined with clinical features (CF). Feature dimensions were optimized using Principal Component Analysis (PCA), followed by the application of supervised learning (SL) and SSL. SSL incorporated pseudo-labeling of censored data to improve performance. Seven regressors and three hazard ratio survival analysis (HRSA) algorithms were optimized using five-fold cross-validation, grid search and external test bootstrapping. Results: For PET HRFs, SSL reduced the mean absolute error (MAE) by 26.5%, achieving 1.55 years with PCA+decision tree regression, compared to SLâ€™s 2.11 years with PCA+KNNR (p&lt;0.05). Combining HRFs (CT_HRF) and DRFs from CT images using SSL+PCA+KNNR achieved an MAE of 2.08 years, outperforming SLâ€™s 2.26 years by 7.96% (p&lt;0.05). In HRSA, CT_HRF applied to PCA+Component Wise Gradient Boosting Survival Analysis achieved an external c-index of 0.65, effectively differentiating high- and low-risk groups. Conclusions: We demonstrated that the SSL strategy significantly outperforms SL across PET, CT, and CF. As such, censor-aware SSL applied to HRFs from PET images significantly improved survival prediction performance by 26.5% compared to the SL approach. </p>
<blockquote>
<p>ç›®æ ‡ï¼šè‚ºç™Œæ„æˆä¸€é¡¹é‡å¤§çš„å…¨çƒå¥åº·æŒ‘æˆ˜ï¼Œéœ€è¦æ”¹è¿›é¢„æµ‹æ–¹æ³•ä»¥å®ç°ä¸ªæ€§åŒ–æ²»ç–—ã€‚æœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æœ‰å®¡æŸ¥æ„è¯†çš„åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èåˆäº†ä¸´åºŠå’Œæˆåƒæ•°æ®ï¼Œè§£å†³äº†ä¼ ç»Ÿæ¨¡å‹åœ¨å¤„ç†å—å®¡æŸ¥æ•°æ®æ—¶çš„åè§é—®é¢˜ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šæˆ‘ä»¬åˆ†æäº†æ¥è‡ªå…¬å…±å’Œæœ¬åœ°æ•°æ®ä»“åº“çš„199åè‚ºç™Œæ‚£è€…çš„ä¸´åºŠã€PETå’ŒCTæ•°æ®ï¼Œä»¥æ€»ä½“å­˜æ´»æ—¶é—´ä½œä¸ºä¸»è¦ç»“æœã€‚ä½¿ç”¨ViSERAè½¯ä»¶é¢„å¤„ç†åæå–æ‰‹å·¥ç‰¹å¾ï¼ˆHRFï¼‰å’Œæ·±åº¦æ”¾å°„å­¦ç‰¹å¾ï¼ˆDRFï¼‰ï¼Œå¹¶ä¸ä¸´åºŠç‰¹å¾ï¼ˆCFï¼‰ç›¸ç»“åˆã€‚ä½¿ç”¨ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ä¼˜åŒ–ç‰¹å¾ç»´åº¦ï¼Œç„¶ååº”ç”¨ç›‘ç£å­¦ä¹ ï¼ˆSLï¼‰å’ŒSSLã€‚SSLç»“åˆäº†ä¼ªæ ‡ç­¾æ³•å¤„ç†å—å®¡æŸ¥æ•°æ®ä»¥æé«˜æ€§èƒ½ã€‚é€šè¿‡äº”æŠ˜äº¤å‰éªŒè¯ã€ç½‘æ ¼æœç´¢å’Œå¤–éƒ¨æµ‹è¯•Bootstrapä¼˜åŒ–ä¸ƒä¸ªå›å½’å™¨å’Œä¸‰ä¸ªå±é™©æ¯”ç‡ç”Ÿå­˜åˆ†æï¼ˆHRSAï¼‰ç®—æ³•ã€‚</p>
<p>ç»“æœï¼šå¯¹äºPETçš„HRFsï¼ŒSSLå°†å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰é™ä½äº†26.5%ï¼Œä½¿ç”¨PCA+å†³ç­–æ ‘å›å½’è¾¾åˆ°1.55å¹´ï¼Œè€ŒSLä¸PCA+KNNRçš„ç»„åˆä¸º2.11å¹´ï¼ˆp&lt;0.05ï¼‰ã€‚ç»“åˆæ¥è‡ªCTå›¾åƒçš„HRFsï¼ˆCT_HRFï¼‰å’ŒDRFsä½¿ç”¨SSL+PCA+KNNRçš„MAEä¸º2.08å¹´ï¼Œä¼˜äºSLçš„2.26å¹´ï¼Œæé«˜äº†7.96%ï¼ˆp&lt;0.05ï¼‰ã€‚åœ¨HRSAä¸­ï¼Œå°†CT_HRFåº”ç”¨äºPCA+Component Wise Gradient Boosting Survival Analysisï¼Œå¤–éƒ¨cæŒ‡æ•°ä¸º0.65ï¼Œæœ‰æ•ˆåœ°åŒºåˆ†äº†é«˜ã€ä½é£é™©ç»„ã€‚</p>
<p>ç»“è®ºï¼šæˆ‘ä»¬è¯æ˜SSLç­–ç•¥åœ¨PETã€CTå’ŒCFæ–¹é¢æ˜¾è‘—ä¼˜äºSLã€‚å› æ­¤ï¼Œä¸SLæ–¹æ³•ç›¸æ¯”ï¼Œåº”ç”¨äºPETå›¾åƒHRFsçš„æœ‰å®¡æŸ¥æ„è¯†çš„SSLå°†ç”Ÿå­˜é¢„æµ‹æ€§èƒ½æé«˜äº†26.5%ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01661v4">PDF</a> 11 pages, 4 Figures and 4 Tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç»“åˆä¸´åºŠå’Œæˆåƒæ•°æ®çš„å®¡æŸ¥æ„ŸçŸ¥åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¡†æ¶ï¼Œä»¥è§£å†³ä¼ ç»Ÿæ¨¡å‹å¤„ç†å®¡æŸ¥æ•°æ®æ—¶çš„åè§é—®é¢˜ã€‚é€šè¿‡å¯¹199åè‚ºç™Œæ‚£è€…çš„ä¸´åºŠã€PETå’ŒCTæ•°æ®è¿›è¡Œåˆ†æï¼Œè¯¥ç ”ç©¶å‘ç°SSLç­–ç•¥åœ¨PETã€CTå’Œä¸´åºŠç‰¹å¾æ–¹é¢çš„è¡¨ç°å‡ä¼˜äºç›‘ç£å­¦ä¹ ï¼ˆSLï¼‰ï¼Œå°¤å…¶æ˜¯åœ¨PETçš„æ‰‹å·¥ç‰¹å¾ï¼ˆHRFï¼‰ä¸Šï¼ŒSSLå°†å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰é™ä½äº†26.5%ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿå­˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¼•å…¥äº†å®¡æŸ¥æ„ŸçŸ¥åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¡†æ¶ï¼Œèåˆäº†ä¸´åºŠå’Œæˆåƒæ•°æ®ï¼Œä»¥æ”¹è¿›å¯¹è‚ºç™Œæ‚£è€…çš„ä¸ªæ€§åŒ–æ²»ç–—é¢„åæ–¹æ³•ã€‚</li>
<li>é€šè¿‡åˆ†æ199åè‚ºç™Œæ‚£è€…çš„ä¸´åºŠã€PETå’ŒCTæ•°æ®ï¼Œé‡ç‚¹ç ”ç©¶æ‚£è€…çš„æ€»ä½“ç”Ÿå­˜æ—¶é—´ã€‚</li>
<li>SSLæ¡†æ¶é€šè¿‡ä¼ªæ ‡è®°å®¡æŸ¥æ•°æ®æé«˜äº†æ€§èƒ½ï¼Œåœ¨PETçš„æ‰‹å·¥ç‰¹å¾ï¼ˆHRFï¼‰ä¸Šï¼Œä¸ç›‘ç£å­¦ä¹ ï¼ˆSLï¼‰ç›¸æ¯”ï¼ŒSSLå°†å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰é™ä½äº†26.5%ã€‚</li>
<li>ç»“åˆCTå›¾åƒçš„HRFå’ŒDeep Radiomicsç‰¹å¾ï¼ˆDRFï¼‰ï¼Œä½¿ç”¨SSL+PCA+KNNRçš„MAEä¸º2.08å¹´ï¼Œä¼˜äºSLçš„2.26å¹´ï¼Œæå‡äº†7.96%ã€‚</li>
<li>åœ¨å±å®³æ¯”ç‡ç”Ÿå­˜åˆ†æï¼ˆHRSAï¼‰ä¸­ï¼Œä½¿ç”¨PCA+Component Wise Gradient Boosting Survival Analysisç»“åˆCT_HRFè¾¾åˆ°äº†å¤–éƒ¨c-æŒ‡æ•°ä¸º0.65ï¼Œèƒ½æœ‰æ•ˆåŒºåˆ†é«˜ã€ä½é£é™©ç»„ã€‚</li>
<li>SSLç­–ç•¥åœ¨PETã€CTå’Œä¸´åºŠç‰¹å¾æ–¹é¢çš„è¡¨ç°å‡ä¼˜äºSLã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01661">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15e5b8a279b71da307a9dd96057b84fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fe5b84e40ec50dbc908acb343a2fe9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ce0b335a3849bf071cf95ecf8c0f945.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e76e9db3ea3bdec2bc01897f32fcf55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-566ff697d53c43a375df15a3584670d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b80046fb395a21d30dec944b8d92423.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce95558ca118212ccf216e1634dcb47c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc39cf04b1dc12ec7070f435c1a4a1db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc42ecacf76027484344ee2fa5e695dd.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="UKAN-EP-Enhancing-U-KAN-with-Efficient-Attention-and-Pyramid-Aggregation-for-3D-Multi-Modal-MRI-Brain-Tumor-Segmentation"><a href="#UKAN-EP-Enhancing-U-KAN-with-Efficient-Attention-and-Pyramid-Aggregation-for-3D-Multi-Modal-MRI-Brain-Tumor-Segmentation" class="headerlink" title="UKAN-EP: Enhancing U-KAN with Efficient Attention and Pyramid   Aggregation for 3D Multi-Modal MRI Brain Tumor Segmentation"></a>UKAN-EP: Enhancing U-KAN with Efficient Attention and Pyramid   Aggregation for 3D Multi-Modal MRI Brain Tumor Segmentation</h2><p><strong>Authors:Yanbing Chen, Tianze Tang, Taehyo Kim, Hai Shu</strong></p>
<p>Gliomas are among the most common malignant brain tumors and are characterized by considerable heterogeneity, which complicates accurate detection and segmentation. Multi-modal MRI is the clinical standard for glioma imaging, but variability across modalities and high computational complexity hinder effective automated segmentation. In this paper, we propose UKAN-EP, a novel 3D extension of the original 2D U-KAN model for multi-modal MRI brain tumor segmentation. While U-KAN integrates Kolmogorov-Arnold Network (KAN) layers into a U-Net backbone, UKAN-EP further incorporates Efficient Channel Attention (ECA) and Pyramid Feature Aggregation (PFA) modules to enhance inter-modality feature fusion and multi-scale feature representation. We also introduce a dynamic loss weighting strategy that adaptively balances the Cross-Entropy and Dice losses during training. We evaluate UKAN-EP on the 2024 BraTS-GLI dataset and compare it against strong baselines including U-Net, Attention U-Net, and Swin UNETR. Results show that UKAN-EP achieves superior segmentation performance while requiring substantially fewer computational resources. An extensive ablation study further demonstrates the effectiveness of ECA and PFA, as well as the limited utility of self-attention and spatial attention alternatives. Code is available at <a target="_blank" rel="noopener" href="https://github.com/TianzeTang0504/UKAN-EP">https://github.com/TianzeTang0504/UKAN-EP</a>. </p>
<blockquote>
<p>èƒ¶è´¨ç˜¤æ˜¯æœ€å¸¸è§çš„æ¶æ€§è„‘è‚¿ç˜¤ä¹‹ä¸€ï¼Œå…¶ç‰¹ç‚¹æ˜¯å…·æœ‰ç›¸å½“å¤§çš„å¼‚è´¨æ€§ï¼Œè¿™ä½¿å¾—å‡†ç¡®çš„æ£€æµ‹å’Œåˆ†å‰²å˜å¾—å¤æ‚ã€‚å¤šæ¨¡æ€MRIæ˜¯èƒ¶è´¨ç˜¤æˆåƒçš„ä¸´åºŠæ ‡å‡†ï¼Œä½†ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚å’Œè¾ƒé«˜çš„è®¡ç®—å¤æ‚æ€§é˜»ç¢äº†æœ‰æ•ˆçš„è‡ªåŠ¨åˆ†å‰²ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†UKAN-EPï¼Œè¿™æ˜¯åŸå§‹äºŒç»´U-KANæ¨¡å‹çš„å¤šæ¨¡æ€MRIè„‘è‚¿ç˜¤åˆ†å‰²çš„æ–°å‹ä¸‰ç»´æ‰©å±•ã€‚U-KANå°†Kolmogorov-Arnoldç½‘ç»œï¼ˆKANï¼‰å±‚é›†æˆåˆ°U-Netä¸»å¹²ä¸­ï¼Œè€ŒUKAN-EPè¿›ä¸€æ­¥ç»“åˆäº†é«˜æ•ˆé€šé“æ³¨æ„åŠ›ï¼ˆECAï¼‰å’Œé‡‘å­—å¡”ç‰¹å¾èšåˆï¼ˆPFAï¼‰æ¨¡å—ï¼Œä»¥å¢å¼ºè·¨æ¨¡æ€ç‰¹å¾èåˆå’Œå¤šå°ºåº¦ç‰¹å¾è¡¨ç¤ºã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§åŠ¨æ€æŸå¤±æƒé‡ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥è‡ªé€‚åº”åœ°å¹³è¡¡è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ä¸­çš„äº¤å‰ç†µå’ŒDiceæŸå¤±ã€‚æˆ‘ä»¬åœ¨åŒ…å«U-Netã€Attention U-Netå’ŒSwin UNETRç­‰å¼ºå¤§åŸºå‡†æ¨¡å‹çš„BraTS-GLIæ•°æ®é›†ä¸Šè¯„ä¼°äº†UKAN-EPçš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒUKAN-EPåœ¨è¾¾åˆ°æ›´é«˜çš„åˆ†å‰²æ€§èƒ½çš„åŒæ—¶ï¼Œæ‰€éœ€çš„è®¡ç®—èµ„æºå¤§å¤§å‡å°‘ã€‚å¤§é‡çš„æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯æ˜äº†ECAå’ŒPFAçš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠè‡ªæ³¨æ„åŠ›å’Œç©ºé—´æ³¨æ„åŠ›æ›¿ä»£æ–¹æ¡ˆçš„å±€é™æ€§ã€‚ç›¸å…³ä»£ç å¯è®¿é—®äºï¼š<a target="_blank" rel="noopener" href="https://github.com/TianzeTang0504/UKAN-EP%E3%80%82">https://github.com/TianzeTang0504/UKAN-EPã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.00273v2">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹èƒ¶è´¨è„‘ç˜¤çš„æ¶æ€§è„‘è‚¿ç˜¤åœ¨åŒ»ç–—å›¾åƒåˆ†æä¸­æä¸ºå¸¸è§ï¼Œä½†å› å…¶æ˜¾è‘—çš„å¼‚è´¨æ€§å¯¼è‡´å‡†ç¡®æ£€æµ‹å’Œåˆ†å‰²å˜å¾—å¤æ‚ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„åŸºäºå¤šæ¨¡æ€MRIçš„èƒ¶è´¨è„‘ç˜¤åˆ†å‰²æ¨¡å‹â€”â€”UKAN-EPã€‚è¯¥æ¨¡å‹ç»“åˆäº†Kolmogorov-Arnoldç½‘ç»œå±‚ã€é«˜æ•ˆé€šé“æ³¨æ„åŠ›æ¨¡å—å’Œå¤šå°ºåº¦ç‰¹å¾é‡‘å­—å¡”æ¨¡å—ï¼Œå¼ºåŒ–äº†æ¨¡æ€é—´çš„ç‰¹å¾èåˆä¸å¤šå°ºåº¦ç‰¹å¾è¡¨è¾¾ã€‚é€šè¿‡åŠ¨æ€æŸå¤±æƒé‡ç­–ç•¥ï¼Œè‡ªé€‚åº”å¹³è¡¡äº¤å‰ç†µå’ŒDiceæŸå¤±è¿›è¡Œè®­ç»ƒã€‚åœ¨BraTS-GLIæ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒUKAN-EPä¸ä»…å®ç°äº†ä¼˜è¶Šçš„åˆ†å‰²æ€§èƒ½ï¼Œè€Œä¸”è®¡ç®—èµ„æºæ¶ˆè€—æ›´å°‘ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥ç ”ç©¶å…³æ³¨èƒ¶è´¨è„‘ç˜¤è¿™ä¸€å¸¸è§æ¶æ€§è„‘è‚¿ç˜¤çš„å›¾åƒåˆ†ææŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯å…¶å¼‚è´¨æ€§é—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå¤šæ¨¡æ€MRIçš„èƒ¶è´¨è„‘ç˜¤åˆ†å‰²æ¨¡å‹UKAN-EPï¼Œæ‰©å±•äº†åŸæœ‰çš„äºŒç»´æ¨¡å‹å¹¶åŠ å…¥äº†æ›´å¤šä¼˜åŒ–æ¨¡å—ã€‚</li>
<li>UKAN-EPé›†æˆäº†Kolmogorov-Arnoldç½‘ç»œå±‚ï¼Œä»¥å¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>Efficient Channel Attentionï¼ˆECAï¼‰å’ŒPyramid Feature Aggregationï¼ˆPFAï¼‰æ¨¡å—çš„åŠ å…¥æå‡äº†æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚ECAæé«˜äº†é€šé“é—´æ³¨æ„åŠ›çš„åˆ©ç”¨æ•ˆç‡ï¼Œè€ŒPFAå¢å¼ºäº†å¤šå°ºåº¦ç‰¹å¾çš„èåˆèƒ½åŠ›ã€‚</li>
<li>åŠ¨æ€æŸå¤±æƒé‡ç­–ç•¥æœ‰åŠ©äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªé€‚åº”å¹³è¡¡ä¸åŒæŸå¤±å‡½æ•°çš„å½±å“ã€‚</li>
<li>åœ¨BraTS-GLIæ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†UKAN-EPæ¨¡å‹åœ¨èƒ¶è´¨è„‘ç˜¤åˆ†å‰²ä¸Šçš„ä¼˜è¶Šæ€§ï¼Œå¹¶ä¸”è®¡ç®—èµ„æºæ¶ˆè€—è¾ƒä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.00273">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-730bb5bc5cd9afef9d1a5341004aeecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc22fda98a2aa5050a91eaaf25892f14.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Direct3Î³-A-Pipeline-for-Direct-Three-gamma-PET-Image-Reconstruction"><a href="#Direct3Î³-A-Pipeline-for-Direct-Three-gamma-PET-Image-Reconstruction" class="headerlink" title="Direct3Î³: A Pipeline for Direct Three-gamma PET Image   Reconstruction"></a>Direct3Î³: A Pipeline for Direct Three-gamma PET Image   Reconstruction</h2><p><strong>Authors:Youness Mellak, Alexandre Bousse, Thibaut Merlin, Debora Giovagnoli, Dimitris Visvikis</strong></p>
<p>This paper presents a novel image reconstruction pipeline for three-gamma (3-{\gamma}) positron emission tomography (PET) aimed at improving spatial resolution and reducing noise in nuclear medicine. The proposed Direct3{\gamma} pipeline addresses the inherent challenges in 3-{\gamma} PET systems, such as detector imperfections and uncertainty in photon interaction points. A key feature of the pipeline is its ability to determine the order of interactions through a model trained on Monte Carlo (MC) simulations using the Geant4 Application for Tomography Emission (GATE) toolkit, thus providing the necessary information to construct Compton cones which intersect with the line of response (LOR) to provide an estimate of the emission point. The pipeline processes 3-{\gamma} PET raw data, reconstructs histoimages by propagating energy and spatial uncertainties along the LOR, and applies a 3-D convolutional neural network (CNN) to refine these intermediate images into high-quality reconstructions. To further enhance image quality, the pipeline leverages both supervised learning and adversarial losses, the latter preserving fine structural details. Experimental results show that Direct3{\gamma} consistently outperforms conventional 200-ps time-of-flight (TOF) PET in terms of SSIM and PSNR. </p>
<blockquote>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸‰ä¼½é©¬ï¼ˆ3-Î³ï¼‰æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒé‡å»ºçš„æ–°æµç¨‹ï¼Œæ—¨åœ¨æé«˜æ ¸åŒ»å­¦ä¸­çš„ç©ºé—´åˆ†è¾¨ç‡å¹¶é™ä½å™ªå£°ã€‚æ‰€æå‡ºçš„Direct3Î³æµç¨‹è§£å†³äº†3-Î³ PETç³»ç»Ÿå›ºæœ‰çš„æŒ‘æˆ˜ï¼Œå¦‚æ¢æµ‹å™¨ç¼ºé™·å’Œå…‰å­äº¤äº’ç‚¹çš„ä¸ç¡®å®šæ€§ã€‚è¯¥æµç¨‹çš„ä¸€ä¸ªå…³é”®åŠŸèƒ½æ˜¯ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡ä½¿ç”¨Geant4å‘å°„æ–­å±‚æ‰«æåº”ç”¨ç¨‹åºï¼ˆGATEï¼‰å·¥å…·åŒ…è¿›è¡Œçš„è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰æ¨¡æ‹Ÿè®­ç»ƒæ¨¡å‹æ¥ç¡®å®šäº¤äº’çš„é¡ºåºï¼Œä»è€Œæä¾›æ„å»ºäº¤äºå“åº”çº¿ï¼ˆLORï¼‰çš„åº·æ™®é¡¿é”¥çš„å¿…è¦ä¿¡æ¯ï¼Œä»¥ä¼°è®¡å‘å°„ç‚¹ã€‚è¯¥æµç¨‹å¤„ç†3-Î³ PETåŸå§‹æ•°æ®ï¼Œé€šè¿‡ä¼ æ’­èƒ½é‡å’Œç©ºé—´ä¸ç¡®å®šæ€§æ²¿LORé‡å»ºç›´æ–¹å›¾åƒï¼Œå¹¶åº”ç”¨ä¸‰ç»´å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å°†è¿™äº›ä¸­é—´å›¾åƒç²¾ç»†åŒ–ä¸ºé«˜è´¨é‡é‡å»ºã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜å›¾åƒè´¨é‡ï¼Œè¯¥æµç¨‹ç»“åˆäº†ç›‘ç£å­¦ä¹ å’Œå¯¹æŠ—æ€§æŸå¤±ï¼Œåè€…èƒ½å¤Ÿä¿ç•™ç²¾ç»†çš„ç»“æ„ç»†èŠ‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDirect3Î³åœ¨ç»“æ„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆSSIMï¼‰å’Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ–¹é¢å§‹ç»ˆä¼˜äºä¼ ç»Ÿçš„200çš®ç§’é£è¡Œæ—¶é—´ï¼ˆTOFï¼‰PETã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18337v6">PDF</a> 11 pages, 11 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹ä¸‰ä¼½é©¬ï¼ˆ3-Î³ï¼‰æ­£ç”µå­å‘å°„æ–­å±‚æ‰«æï¼ˆPETï¼‰å›¾åƒé‡å»ºçš„æ–°æµç¨‹ï¼Œæ—¨åœ¨æé«˜æ ¸åŒ»å­¦ä¸­çš„ç©ºé—´åˆ†è¾¨ç‡å¹¶é™ä½å™ªå£°ã€‚è¯¥æµç¨‹é€šè¿‡ä¸€ç³»åˆ—æŠ€æœ¯å’Œç®—æ³•ï¼Œå¦‚åˆ©ç”¨è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿè®­ç»ƒæ¨¡å‹ã€ç¡®å®šäº¤äº’é¡ºåºã€æ„å»ºåº·æ™®é¡¿é”¥ä¸å“åº”çº¿äº¤ç‚¹ç­‰ï¼Œè§£å†³3-Î³ PETç³»ç»Ÿçš„å›ºæœ‰æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æµç¨‹åœ¨ç»“æ„ç›¸ä¼¼åº¦æŒ‡æ•°ï¼ˆSSIMï¼‰å’Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„200çš®ç§’é£è¡Œæ—¶é—´ï¼ˆTOFï¼‰PETã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é’ˆå¯¹ä¸‰ä¼½é©¬ï¼ˆ3-Î³ï¼‰PETçš„æ–°å‹å›¾åƒé‡å»ºæµç¨‹ï¼Œæ—¨åœ¨æé«˜ç©ºé—´åˆ†è¾¨ç‡å¹¶é™ä½æ ¸åŒ»å­¦ä¸­çš„å™ªå£°ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨Geant4 Application for Tomography Emissionï¼ˆGATEï¼‰å·¥å…·åŒ…çš„è’™ç‰¹å¡æ´›ï¼ˆMCï¼‰æ¨¡æ‹Ÿè¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œç¡®å®šå…‰å­äº¤äº’é¡ºåºã€‚</li>
<li>è¯¥æµç¨‹é€šè¿‡æ„å»ºåº·æ™®é¡¿é”¥ä¸å“åº”çº¿äº¤ç‚¹æ¥ä¼°è®¡å‘å°„ç‚¹ï¼Œä»è€Œå¤„ç†3-Î³ PETåŸå§‹æ•°æ®ã€‚</li>
<li>é€šè¿‡æ²¿å“åº”çº¿ä¼ æ’­èƒ½é‡å’Œç©ºé—´ä¸ç¡®å®šæ€§æ¥é‡å»ºç›´æ–¹å›¾åƒï¼Œå¹¶åˆ©ç”¨ä¸‰ç»´å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å¯¹ä¸­é—´å›¾åƒè¿›è¡Œç²¾ç»†åŒ–å¤„ç†ï¼Œç”Ÿæˆé«˜è´¨é‡é‡å»ºå›¾åƒã€‚</li>
<li>è¯¥æµç¨‹é‡‡ç”¨ç›‘ç£å­¦ä¹ å’Œå¯¹æŠ—æ€§æŸå¤±æ¥è¿›ä¸€æ­¥æé«˜å›¾åƒè´¨é‡ï¼Œå…¶ä¸­å¯¹æŠ—æ€§æŸå¤±æœ‰åŠ©äºä¿ç•™ç²¾ç»†ç»“æ„ç»†èŠ‚ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æµç¨‹åœ¨ç»“æ„ç›¸ä¼¼åº¦æŒ‡æ•°ï¼ˆSSIMï¼‰å’Œå³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œä¼˜äºä¼ ç»Ÿçš„200çš®ç§’é£è¡Œæ—¶é—´ï¼ˆTOFï¼‰PETã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18337">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8676efb046b0547fcbeb31f8e17e6897.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c237686cd3535fc11daea7695616bd21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d686919113adf714ed5302f80c19123.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf6181951dfb16932106658d26fa1577.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d3193083c913e5a2bbde118dd4d3bc2.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MedVersa-A-Generalist-Foundation-Model-for-Medical-Image-Interpretation"><a href="#MedVersa-A-Generalist-Foundation-Model-for-Medical-Image-Interpretation" class="headerlink" title="MedVersa: A Generalist Foundation Model for Medical Image Interpretation"></a>MedVersa: A Generalist Foundation Model for Medical Image Interpretation</h2><p><strong>Authors:Hong-Yu Zhou, JuliÃ¡n NicolÃ¡s Acosta, Subathra Adithan, Suvrankar Datta, Eric J. Topol, Pranav Rajpurkar</strong></p>
<p>Current medical AI systems are often limited to narrow applications, hindering widespread adoption. We present MedVersa, a generalist foundation model trained on tens of millions of compiled medical instances. MedVersa unlocks generalist learning from multimodal inputs and outputs, representing the first example of a generalist model reaching competitive performance with leading specialized solutions across a variety of medical imaging scenarios. MedVersa achieves state-of-the-art performance in nine tasks, sometimes outperforming counterparts by over 10%. Radiologist evaluation shows MedVersa-generated reports get superior performance in 95% of normal studies, while matching or exceeding human reports in 71% of cases overall. User studies showed notable reductions in report writing time and discrepancies with the use of MedVersa. Our findings underscore the value of flexible, multimodal AI systems in advancing medical image interpretation and supporting clinical expertise. </p>
<blockquote>
<p>å½“å‰åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»Ÿå¾€å¾€å±€é™äºç‰¹å®šåº”ç”¨ï¼Œé˜»ç¢äº†å…¶å¹¿æ³›åº”ç”¨ã€‚æˆ‘ä»¬æ¨å‡ºäº†MedVersaï¼Œè¿™æ˜¯ä¸€æ¬¾åœ¨æ•°ç™¾ä¸‡åŒ»ç–—å®ä¾‹ä¸Šè®­ç»ƒçš„é€šç”¨åŸºç¡€æ¨¡å‹ã€‚MedVersaè§£é”äº†ä»å¤šæ¨¡å¼è¾“å…¥å’Œè¾“å‡ºä¸­å­¦ä¹ é€šç”¨çŸ¥è¯†çš„èƒ½åŠ›ï¼Œæˆä¸ºç¬¬ä¸€ä¸ªåœ¨å¤šç§åŒ»å­¦æˆåƒåœºæ™¯ä¸­è¾¾åˆ°é¢†å…ˆä¸“ä¸šè§£å†³æ–¹æ¡ˆç«äº‰åŠ›çš„é€šç”¨æ¨¡å‹ã€‚MedVersaåœ¨ä¹ä¸ªä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œæœ‰æ—¶è¾ƒåŒç±»äº§å“çš„æ€§èƒ½é«˜å‡ºè¶…è¿‡10%ã€‚æ”¾å°„ç§‘åŒ»ç”Ÿè¯„ä»·æ˜¾ç¤ºï¼ŒMedVersaç”Ÿæˆçš„æŠ¥å‘Šåœ¨æ­£å¸¸ç ”ç©¶çš„95%ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå¹¶åœ¨æ€»ä½“æƒ…å†µä¸‹æœ‰71%ä¸äººç±»æŠ¥å‘Šç›¸åŒ¹é…æˆ–è¶…è¶Šäººç±»æŠ¥å‘Šã€‚ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨MedVersaæ˜¾è‘—å‡å°‘äº†æŠ¥å‘Šç¼–å†™æ—¶é—´å’Œå·®å¼‚ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†çµæ´»ã€å¤šæ¨¡å¼äººå·¥æ™ºèƒ½ç³»ç»Ÿåœ¨æ¨è¿›åŒ»å­¦å›¾åƒè§£è¯»å’Œæ”¯æŒä¸´åºŠä¸“ä¸šçŸ¥è¯†æ–¹é¢çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.07988v2">PDF</a> Technical study</p>
<p><strong>Summary</strong><br>åŒ»å­¦AIç³»ç»Ÿé€šå¸¸å±€é™äºç‰¹å®šåº”ç”¨ï¼Œé˜»ç¢å…¶å¹¿æ³›åº”ç”¨ã€‚æˆ‘ä»¬æ¨å‡ºMedVersaï¼Œè¿™æ˜¯ä¸€ç§ç»è¿‡æ•°ç™¾ä¸‡åŒ»ç–—å®ä¾‹è®­ç»ƒçš„åŸºç¡€é€šç”¨æ¨¡å‹ã€‚MedVersaæ”¯æŒä»å¤šæ¨¡å¼è¾“å…¥å’Œè¾“å‡ºä¸­å­¦ä¹ é€šç”¨çŸ¥è¯†ï¼Œæˆä¸ºé¦–ä¸ªåœ¨å¤šç§åŒ»å­¦æˆåƒåœºæ™¯ä¸­ä¸é¢†å…ˆçš„ä¸“ä¸šè§£å†³æ–¹æ¡ˆç›¸ç«äº‰çš„é€šç”¨æ¨¡å‹ã€‚åœ¨ä¹é¡¹ä»»åŠ¡ä¸­ï¼ŒMedVersaå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œæœ‰æ—¶è¾ƒåŒç±»äº§å“çš„æ€§èƒ½é«˜å‡ºè¶…è¿‡10%ã€‚æ”¾å°„ç§‘åŒ»ç”Ÿè¯„ä¼°æ˜¾ç¤ºï¼ŒMedVersaç”Ÿæˆçš„æŠ¥å‘Šåœ¨æ­£å¸¸ç ”ç©¶çš„95%ä¸­è¡¨ç°ä¼˜è¶Šï¼Œå¹¶åœ¨æ€»ä½“ä¸Šä»¥åŒ¹é…æˆ–è¶…è¿‡äººç±»æŠ¥å‘Šçš„é€Ÿåº¦åœ¨71%çš„æ¡ˆä¾‹ä¸­è¡¨ç°è‰¯å¥½ã€‚ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨MedVersaæ˜¾è‘—å‡å°‘äº†æŠ¥å‘Šç¼–å†™æ—¶é—´å’Œå·®å¼‚ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†çµæ´»çš„å¤šæ¨¡å¼AIç³»ç»Ÿåœ¨æ¨è¿›åŒ»å­¦å›¾åƒè§£è¯»å’Œæ”¯æŒä¸´åºŠä¸“ä¸šçŸ¥è¯†æ–¹é¢çš„ä»·å€¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦AIç³»ç»Ÿçš„å±€é™æ€§ï¼šå½“å‰åŒ»å­¦AIç³»ç»Ÿé€šå¸¸ä»…é™äºç‰¹å®šåº”ç”¨ï¼Œé™åˆ¶äº†å…¶å¹¿æ³›åº”ç”¨ã€‚</li>
<li>MedVersaçš„å¼•å…¥ï¼šæå‡ºäº†ä¸€ç§åä¸ºMedVersaçš„é€šç”¨åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»è¿‡æ•°ç™¾ä¸‡åŒ»ç–—å®ä¾‹è®­ç»ƒã€‚</li>
<li>å¤šæ¨¡æ€å­¦ä¹ ä¸è¾“å‡ºï¼šMedVersaæ”¯æŒä»å¤šæ¨¡å¼è¾“å…¥å’Œè¾“å‡ºä¸­å­¦ä¹ ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ›æ–°ç‰¹ç‚¹ã€‚</li>
<li>ç«äº‰æ€§èƒ½ï¼šMedVersaåœ¨å¤šç§åŒ»å­¦æˆåƒåœºæ™¯ä¸­è¡¨ç°å‡ºä¸é¢†å…ˆçš„ä¸“ä¸šè§£å†³æ–¹æ¡ˆç›¸å½“çš„ç«äº‰åŠ›ã€‚</li>
<li>å…ˆè¿›æ€§èƒ½è¡¨ç°ï¼šåœ¨ä¹é¡¹ä»»åŠ¡ä¸­ï¼ŒMedVersaè¾¾åˆ°æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹æ˜¾è‘—ä¼˜äºå…¶ä»–ç³»ç»Ÿã€‚</li>
<li>æ”¾å°„ç§‘åŒ»ç”Ÿè¯„ä¼°ç»“æœï¼šåœ¨æ”¾å°„ç§‘åŒ»ç”Ÿè¯„ä¼°ä¸­ï¼ŒMedVersaç”Ÿæˆçš„æŠ¥å‘Šåœ¨å¤§éƒ¨åˆ†æƒ…å†µä¸‹è¡¨ç°è‰¯å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.07988">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-aa43c762a6880e3eaa3313bd717bb59f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a76721bc49c3afc27db47c8cf3c9a76a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-456f55ec5be92ac1255dab795dcdf128.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c7622b9dc8034b5612f7fcba18cb1a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fbb2ca44e38db36b157dfb6ce26606b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-17/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-52c133cbb52da685471dd033058441b1.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-17  S2ST-Omni An Efficient and Scalable Multilingual Speech-to-Speech   Translation Framework via Seamlessly Speech-Text Alignment and Streaming   Speech Decoder
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-17/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-99808382c44ef4bc9f8c2ce88090b33a.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-17  Taming Stable Diffusion for Computed Tomography Blind Super-Resolution
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">31686.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
