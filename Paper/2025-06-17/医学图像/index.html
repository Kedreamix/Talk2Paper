<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="医学图像">
    <meta name="description" content="医学图像 方向最新论文已更新，请持续关注 Update in 2025-06-17  Simple Radiology VLLM Test-time Scaling with Thought Graph Traversal">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>医学图像 | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic1.zhimg.com/v2-9cefe4fc38396dae12cc5162b9e929ca.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">医学图像</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">医学图像</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                医学图像
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-06-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    18.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    76 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-06-17-更新"><a href="#2025-06-17-更新" class="headerlink" title="2025-06-17 更新"></a>2025-06-17 更新</h1><h2 id="Simple-Radiology-VLLM-Test-time-Scaling-with-Thought-Graph-Traversal"><a href="#Simple-Radiology-VLLM-Test-time-Scaling-with-Thought-Graph-Traversal" class="headerlink" title="Simple Radiology VLLM Test-time Scaling with Thought Graph Traversal"></a>Simple Radiology VLLM Test-time Scaling with Thought Graph Traversal</h2><p><strong>Authors:Yue Yao, Zelin Wen, Yan Tong, Xinyu Tian, Xuqing Li, Xiao Ma, Dongliang Xu, Tom Gedeon</strong></p>
<p>Test-time scaling offers a promising way to improve the reasoning performance of vision-language large models (VLLMs) without additional training. In this paper, we explore a simple but effective approach for applying test-time scaling to radiology report generation. Specifically, we introduce a lightweight Thought Graph Traversal (TGT) framework that guides the model to reason through organ-specific findings in a medically coherent order. This framework integrates structured medical priors into the prompt, enabling deeper and more logical analysis with no changes to the underlying model. To further enhance reasoning depth, we apply a reasoning budget forcing strategy that adjusts the model’s inference depth at test time by dynamically extending its generation process. This simple yet powerful combination allows a frozen radiology VLLM to self-correct and generate more accurate, consistent chest X-ray reports. Our method outperforms baseline prompting approaches on standard benchmarks, and also reveals dataset biases through traceable reasoning paths. Code and prompts are open-sourced for reproducibility at <a target="_blank" rel="noopener" href="https://github.com/glerium/Thought-Graph-Traversal">https://github.com/glerium/Thought-Graph-Traversal</a>. </p>
<blockquote>
<p>测试时缩放（Test-time scaling）提供了一种无需额外训练即可提高视觉语言大型模型（VLLM）推理性能的有前途的方法。在本文中，我们探讨了将测试时缩放应用于放射学报告生成的一种简单而有效的方法。具体来说，我们引入了一个轻量级的思维图遍历（TGT）框架，该框架以医学连贯的顺序引导模型通过特定器官的检查结果进行推理。该框架将结构化的医学先验知识集成到提示中，无需更改底层模型即可进行更深入、更逻辑化的分析。为了进一步提高推理深度，我们采用了推理预算强制策略，通过在测试时动态扩展模型的生成过程来调整模型的推理深度。这种简单而强大的组合允许固定的放射学VLLM进行自我修正，并生成更准确、一致的胸部X光报告。我们的方法在标准基准测试上优于基线提示方法，并通过可追溯的推理路径揭示了数据集偏见。代码和提示已在<a target="_blank" rel="noopener" href="https://github.com/glerium/Thought-Graph-Traversal%E4%B8%8A%E5%BC%80%E6%BA%90%EF%BC%8C%E4%BB%A5%E4%BE%BF%E8%BF%9B%E8%A1%8C%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E9%AA%8C%E8%AF%81%E3%80%82">https://github.com/glerium/Thought-Graph-Traversal上开源，以便进行可重复性验证。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11989v1">PDF</a> arXiv admin note: text overlap with arXiv:2404.11209 by other authors</p>
<p><strong>Summary</strong></p>
<p>本文探索了一种将测试时缩放应用于放射学报告生成的有效方法。通过引入轻量级的思维图遍历（TGT）框架，指导模型以医学逻辑顺序进行器官特异性发现的推理。该框架将结构化医学先验知识融入提示，无需更改底层模型即可实现更深入、更逻辑化的分析。应用推理预算强制策略进一步增强推理深度，通过动态扩展模型的生成过程，在测试时调整模型的推理深度。该组合方法允许冻结的放射学VLLM进行自我修正，生成更准确、一致的胸部X光报告。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>测试时缩放可改善视觉语言大模型的推理性能。</li>
<li>引入轻量级的思维图遍历（TGT）框架，以医学逻辑顺序指导模型推理。</li>
<li>TGT框架将结构化医学先验知识融入提示，实现更深入的分析。</li>
<li>推理预算强制策略增强推理深度，调整模型在测试时的推理深度。</li>
<li>该方法允许冻结的放射学VLLM自我修正，生成更准确、一致的报告。</li>
<li>在标准基准测试上，该方法优于基准提示方法。</li>
<li>通过可追溯的推理路径揭示数据集偏差。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11989">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-ed8a1d98318095ce8e8b39c725b05903.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-12945ec3bda99eef0349842d2dd41188.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fe6be2ce48fdf0488e95c4f9634e1f7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-57afcb555a7ff0a24c78db0eda0789c1.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Automated-Treatment-Planning-for-Interstitial-HDR-Brachytherapy-for-Locally-Advanced-Cervical-Cancer-using-Deep-Reinforcement-Learning"><a href="#Automated-Treatment-Planning-for-Interstitial-HDR-Brachytherapy-for-Locally-Advanced-Cervical-Cancer-using-Deep-Reinforcement-Learning" class="headerlink" title="Automated Treatment Planning for Interstitial HDR Brachytherapy for   Locally Advanced Cervical Cancer using Deep Reinforcement Learning"></a>Automated Treatment Planning for Interstitial HDR Brachytherapy for   Locally Advanced Cervical Cancer using Deep Reinforcement Learning</h2><p><strong>Authors:Mohammadamin Moradi, Runyu Jiang, Yingzi Liu, Malvern Madondo, Tianming Wu, James J. Sohn, Xiaofeng Yang, Yasmin Hasan, Zhen Tian</strong></p>
<p>High-dose-rate (HDR) brachytherapy plays a critical role in the treatment of locally advanced cervical cancer but remains highly dependent on manual treatment planning expertise. The objective of this study is to develop a fully automated HDR brachytherapy planning framework that integrates reinforcement learning (RL) and dose-based optimization to generate clinically acceptable treatment plans with improved consistency and efficiency. We propose a hierarchical two-stage autoplanning framework. In the first stage, a deep Q-network (DQN)-based RL agent iteratively selects treatment planning parameters (TPPs), which control the trade-offs between target coverage and organ-at-risk (OAR) sparing. The agent’s state representation includes both dose-volume histogram (DVH) metrics and current TPP values, while its reward function incorporates clinical dose objectives and safety constraints, including D90, V150, V200 for targets, and D2cc for all relevant OARs (bladder, rectum, sigmoid, small bowel, and large bowel). In the second stage, a customized Adam-based optimizer computes the corresponding dwell time distribution for the selected TPPs using a clinically informed loss function. The framework was evaluated on a cohort of patients with complex applicator geometries. The proposed framework successfully learned clinically meaningful TPP adjustments across diverse patient anatomies. For the unseen test patients, the RL-based automated planning method achieved an average score of 93.89%, outperforming the clinical plans which averaged 91.86%. These findings are notable given that score improvements were achieved while maintaining full target coverage and reducing CTV hot spots in most cases. </p>
<blockquote>
<p>高剂量率（HDR）近距离放射治疗在治疗局部晚期宫颈癌方面起着至关重要的作用，但仍然高度依赖于手动治疗计划的专业知识。本研究的目标是为HDR近距离放射治疗建立一个全自动化的治疗计划框架，该框架结合了强化学习（RL）和基于剂量的优化，以生成临床可接受的、具有改进的一致性和效率的治疗计划。我们提出了一个分层的两阶段自动规划框架。在第一阶段，基于深度Q网络（DQN）的RL代理通过迭代选择治疗计划参数（TPPs），这些参数控制目标覆盖和危险器官（OAR）节省之间的权衡。代理的状态表示包括剂量体积直方图（DVH）指标和当前TPP值，其奖励函数结合了临床剂量目标和安全约束，包括目标区域的D90、V150、V200以及所有相关危险器官的D2cc（膀胱、直肠、乙状结肠、小肠和大肠）。在第二阶段，一个定制的基于Adam的优化器使用临床信息的损失函数计算所选TPPs的相应停留时间分布。该框架在一组具有复杂应用器几何结构的患者中进行了评估。所提出框架成功地在不同的患者解剖结构中学到了有临床意义的TPP调整。对于未见过的测试患者，基于RL的自动规划方法取得了平均93.89%的评分，优于平均91.86%的临床计划。这些发现值得注意，因为在提高评分的同时，大多数病例都实现了全目标覆盖并减少了CTV热点。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11957v1">PDF</a> 12 pages, 2 figures, 3 tables</p>
<p><strong>Summary</strong><br>     本研究旨在开发一个集强化学习与剂量优化于一体的全自动高剂量率近距离治疗计划框架，用于生成具有更好一致性和效率的临床可接受治疗计划。该框架采用分层两阶段自动规划方法，通过深度Q网络强化学习智能体选择治疗计划参数，并利用基于亚当的优化器计算相应的停留时间分布。在复杂应用器几何结构的病人群体中评估了该框架，结果显示其在不同病人解剖结构中成功学习了具有临床意义的治疗计划参数调整，并在未见测试病人中取得了优于临床计划的平均成绩。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>本研究使用强化学习和剂量优化技术来开发全自动HDR近距离治疗计划框架。</li>
<li>采用了层次两阶段的自动规划方法。</li>
<li>第一阶段利用深度Q网络强化学习智能体选择治疗计划参数，包括剂量体积直方图指标和目标区域与风险器官的权衡关系。</li>
<li>第二阶段利用基于亚当的优化器计算相应的停留时间分布。</li>
<li>该框架在具有复杂应用器几何结构的病人群体中进行了评估。</li>
<li>结果显示该框架成功学习了在不同病人解剖结构中的临床意义治疗计划参数调整。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11957">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-38df4fd62cf8163d6fdb16e7a9660ef7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a63b0d539aa12d45b3f7fdf08541f5fa.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="MindGrab-for-BrainChop-Fast-and-Accurate-Skull-Stripping-for-Command-Line-and-Browser"><a href="#MindGrab-for-BrainChop-Fast-and-Accurate-Skull-Stripping-for-Command-Line-and-Browser" class="headerlink" title="MindGrab for BrainChop: Fast and Accurate Skull Stripping for Command   Line and Browser"></a>MindGrab for BrainChop: Fast and Accurate Skull Stripping for Command   Line and Browser</h2><p><strong>Authors:Armina Fani, Mike Doan, Isabelle Le, Alex Fedorov, Malte Hoffmann, Chris Rorden, Sergey Plis</strong></p>
<p>We developed MindGrab, a parameter- and memory-efficient deep fully-convolutional model for volumetric skull-stripping in head images of any modality. Its architecture, informed by a spectral interpretation of dilated convolutions, was trained exclusively on modality-agnostic synthetic data. MindGrab was evaluated on a retrospective dataset of 606 multimodal adult-brain scans (T1, T2, DWI, MRA, PDw MRI, EPI, CT, PET) sourced from the SynthStrip dataset. Performance was benchmarked against SynthStrip, ROBEX, and BET using Dice scores, with Wilcoxon signed-rank significance tests. MindGrab achieved a mean Dice score of 95.9 with standard deviation (SD) 1.6 across modalities, significantly outperforming classical methods (ROBEX: 89.1 SD 7.7, P &lt; 0.05; BET: 85.2 SD 14.4, P &lt; 0.05). Compared to SynthStrip (96.5 SD 1.1, P&#x3D;0.0352), MindGrab delivered equivalent or superior performance in nearly half of the tested scenarios, with minor differences (&lt;3% Dice) in the others. MindGrab utilized 95% fewer parameters (146,237 vs. 2,566,561) than SynthStrip. This efficiency yielded at least 2x faster inference, 50% lower memory usage on GPUs, and enabled exceptional performance (e.g., 10-30x speedup, and up to 30x memory reduction) and accessibility on a wider range of hardware, including systems without high-end GPUs. MindGrab delivers state-of-the-art accuracy with dramatically lower resource demands, supported in brainchop-cli (<a target="_blank" rel="noopener" href="https://pypi.org/project/brainchop/">https://pypi.org/project/brainchop/</a>) and at brainchop.org. </p>
<blockquote>
<p>我们开发了一种名为MindGrab的高效参数和内存使用的深度全卷积模型，该模型可用于处理各种模态的头部图像的体积颅骨剥离。其结构基于对膨胀卷积的频谱解释而设计，并仅在模态无关的合成数据上进行训练。MindGrab在来自SynthStrip数据集的606例多模态成人脑扫描（包括T1、T2、DWI、MRA、PDw MRI、EPI、CT、PET）的回顾性数据集上进行了评估。其性能以Dice得分与SynthStrip、ROBEX和BET进行了比较，并使用Wilcoxon符号秩检验法进行显著性检验。MindGrab在跨模态下的平均Dice得分为95.9（标准差为1.6），显著优于经典方法（ROBEX：89.1，标准差为7.7；BET：85.2，标准差为14.4，P &lt; 0.05）。与SynthStrip（Dice得分为96.5，标准差为1.1，P&#x3D;0.0352）相比，MindGrab在近乎一半的测试场景中表现相当或更好，在其他场景中差异较小（&lt;3% Dice）。MindGrab使用的参数比SynthStrip少95%（146，237 vs 2，566，561）。这种效率带来了至少两倍更快的推理速度，GPU内存使用率降低了50%，并且在更广泛的硬件上实现了卓越的性能（例如，加速高达十倍至三十倍，内存减少高达三十倍）。MindGrab以超高的准确性达到了业界领先水平，同时大大降低了资源需求，可在brainchop-cli（<a target="_blank" rel="noopener" href="https://pypi.org/project/brainchop/%EF%BC%89%E5%92%8Cbrainchop.org%E4%B8%8A%E6%94%AF%E6%8C%81%E4%BD%BF%E7%94%A8%E3%80%82">https://pypi.org/project/brainchop/）和brainchop.org上支持使用。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11860v1">PDF</a> 12 pages, 1 table, 4 figures. 2 supplementary tables, 1 supplementary   figure. Brainchop-cli: <a target="_blank" rel="noopener" href="https://pypi.org/project/brainchop/">https://pypi.org/project/brainchop/</a> . Brainchop web:   <a target="_blank" rel="noopener" href="https://brainchop.org/">https://brainchop.org/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了MindGrab的开发与应用。MindGrab是一种参数和内存效率高的深度全卷积模型，用于各种模态的头部图像中的体积颅骨剥离。其架构基于膨胀卷积的谱解释，仅在模态无关的合成数据上进行训练。在多种模态的成人脑扫描数据集上的评估表明，MindGrab在颅骨剥离任务上的性能优于其他传统方法，达到了较高的Dice分数，并且在参数和内存使用方面更加高效，可以在更广泛的硬件上实现快速推理。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MindGrab是一种用于头部图像体积颅骨剥离的深度全卷积模型。</li>
<li>MindGrab的架构基于膨胀卷积的谱解释设计。</li>
<li>MindGrab在模态无关的合成数据上进行训练。</li>
<li>MindGrab在多种模态的成人脑扫描数据集上的性能进行了评估。</li>
<li>MindGrab在颅骨剥离任务上的性能优于其他传统方法，达到了较高的Dice分数。</li>
<li>MindGrab具有高效的参数和内存使用，可在广泛的硬件上实现快速推理。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11860">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-443cb98782a1d9ae8740701db076608d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ef00dd6231ffcc9cc999124ab7bea139.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-80e346fb6dc62c22ce9a7b05c81ddbe5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5a62471e5b6f5a17524ac608ee28392b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a85b2548cb4906ef51c5dd2d6d1c1184.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-deb75ada6bde003d388836757802b2ea.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Self-supervised-Learning-of-Echocardiographic-Video-Representations-via-Online-Cluster-Distillation"><a href="#Self-supervised-Learning-of-Echocardiographic-Video-Representations-via-Online-Cluster-Distillation" class="headerlink" title="Self-supervised Learning of Echocardiographic Video Representations via   Online Cluster Distillation"></a>Self-supervised Learning of Echocardiographic Video Representations via   Online Cluster Distillation</h2><p><strong>Authors:Divyanshu Mishra, Mohammadreza Salehi, Pramit Saha, Olga Patey, Aris T. Papageorghiou, Yuki M. Asano, J. Alison Noble</strong></p>
<p>Self-supervised learning (SSL) has achieved major advances in natural images and video understanding, but challenges remain in domains like echocardiography (heart ultrasound) due to subtle anatomical structures, complex temporal dynamics, and the current lack of domain-specific pre-trained models. Existing SSL approaches such as contrastive, masked modeling, and clustering-based methods struggle with high intersample similarity, sensitivity to low PSNR inputs common in ultrasound, or aggressive augmentations that distort clinically relevant features. We present DISCOVR (Distilled Image Supervision for Cross Modal Video Representation), a self-supervised dual branch framework for cardiac ultrasound video representation learning. DISCOVR combines a clustering-based video encoder that models temporal dynamics with an online image encoder that extracts fine-grained spatial semantics. These branches are connected through a semantic cluster distillation loss that transfers anatomical knowledge from the evolving image encoder to the video encoder, enabling temporally coherent representations enriched with fine-grained semantic understanding. Evaluated on six echocardiography datasets spanning fetal, pediatric, and adult populations, DISCOVR outperforms both specialized video anomaly detection methods and state-of-the-art video-SSL baselines in zero-shot and linear probing setups, and achieves superior segmentation transfer. </p>
<blockquote>
<p>自监督学习（SSL）在自然图像和视频理解方面取得了重大进展，但在超声心动图（心脏超声）等领域仍面临挑战，原因在于其微妙的解剖结构、复杂的时序动态以及缺乏特定的预训练模型。现有的SSL方法，如对比学习、掩膜建模和基于聚类的方法，面临高样本间相似性、对常见超声的低PSNR输入的敏感性，或过于激烈的增强手段会扭曲临床上相关的特征。我们提出了DISCOVR（用于跨模态视频表示的蒸馏图像监督），这是一个用于心脏超声视频表示学习的自监督双分支框架。DISCOVR结合了一个基于聚类的视频编码器，该编码器对时序动态进行建模，以及一个在线图像编码器，用于提取精细的空间语义。这两个分支通过语义聚类蒸馏损失相连接，将解剖知识从不断进化的图像编码器转移到视频编码器，从而实现丰富的时序连贯表示和精细的语义理解。在涵盖胎儿、儿童和成人群体的六个超声心动图数据集上评估，DISCOVR在零样本和线性探测设置中表现出超越专业视频异常检测方法和最新视频SSL基准的性能，并实现优越的分割迁移效果。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11777v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了在心脏超声视频表示学习中应用的自监督双分支框架DISCOVR。该框架结合了基于聚类的视频编码器和在线图像编码器，通过语义集群蒸馏损失连接这两个分支，从而转移解剖知识，实现时空一致的丰富精细语义理解。在多个胎儿、儿童和成人群体回声数据集上的评估表明，DISCOVR在零样本和线性探测设置中表现出色，实现了优越的分割迁移。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SSL在自然图像和视频理解方面取得重大进展，但在心脏超声领域仍面临挑战。</li>
<li>现有SSL方法如对比学习、掩模建模和聚类方法在面对心脏超声数据时存在困难。</li>
<li>DISCOVR是一个自监督双分支框架，用于心脏超声视频表示学习。</li>
<li>DISCOVR结合了基于聚类的视频编码器和在线图像编码器。</li>
<li>通过语义集群蒸馏损失，DISCOVR能够转移解剖知识，实现时空一致的理解。</li>
<li>DISCOVR在多个回声数据集上的评估结果优于专业视频异常检测方法和最新视频SSL基线。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11777">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-555e64204be73a2df22900d943f5cd9c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4de4b9ee59ed8cf023803076969b7013.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-689e9bc21abfac967c32de7879073e26.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1fcbf9935ed7a7dbbbbb225d8bc9da02.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DMAF-Net-An-Effective-Modality-Rebalancing-Framework-for-Incomplete-Multi-Modal-Medical-Image-Segmentation"><a href="#DMAF-Net-An-Effective-Modality-Rebalancing-Framework-for-Incomplete-Multi-Modal-Medical-Image-Segmentation" class="headerlink" title="DMAF-Net: An Effective Modality Rebalancing Framework for Incomplete   Multi-Modal Medical Image Segmentation"></a>DMAF-Net: An Effective Modality Rebalancing Framework for Incomplete   Multi-Modal Medical Image Segmentation</h2><p><strong>Authors:Libin Lan, Hongxing Li, Zunhui Xia, Yudong Zhang</strong></p>
<p>Incomplete multi-modal medical image segmentation faces critical challenges from modality imbalance, including imbalanced modality missing rates and heterogeneous modality contributions. Due to their reliance on idealized assumptions of complete modality availability, existing methods fail to dynamically balance contributions and neglect the structural relationships between modalities, resulting in suboptimal performance in real-world clinical scenarios. To address these limitations, we propose a novel model, named Dynamic Modality-Aware Fusion Network (DMAF-Net). The DMAF-Net adopts three key ideas. First, it introduces a Dynamic Modality-Aware Fusion (DMAF) module to suppress missing-modality interference by combining transformer attention with adaptive masking and weight modality contributions dynamically through attention maps. Second, it designs a synergistic Relation Distillation and Prototype Distillation framework to enforce global-local feature alignment via covariance consistency and masked graph attention, while ensuring semantic consistency through cross-modal class-specific prototype alignment. Third, it presents a Dynamic Training Monitoring (DTM) strategy to stabilize optimization under imbalanced missing rates by tracking distillation gaps in real-time, and to balance convergence speeds across modalities by adaptively reweighting losses and scaling gradients. Extensive experiments on BraTS2020 and MyoPS2020 demonstrate that DMAF-Net outperforms existing methods for incomplete multi-modal medical image segmentation. Extensive experiments on BraTS2020 and MyoPS2020 demonstrate that DMAF-Net outperforms existing methods for incomplete multi-modal medical image segmentation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/violet-42/DMAF-Net">https://github.com/violet-42/DMAF-Net</a>. </p>
<blockquote>
<p>不完整多模态医学图像分割面临来自模态不平衡的关键挑战，包括模态缺失率的不平衡和模态贡献的异质性。由于现有方法依赖于完整模态可用性的理想化假设，它们无法动态平衡贡献并忽略了模态之间的结构关系，导致在真实世界临床场景中的性能不佳。为了解决这些局限性，我们提出了一种名为动态模态感知融合网络（DMAF-Net）的新模型。DMAF-Net采用了三个关键思想。首先，它引入了一个动态模态感知融合（DMAF）模块，通过结合变压器注意力与自适应掩模，动态地加权模态贡献，从而抑制缺失模态的干扰。其次，它设计了一个协同关系蒸馏和原型蒸馏框架，通过协方差一致性、掩膜图注意力来执行全局局部特征对齐，同时通过跨模态类特定原型对齐确保语义一致性。第三，它提出了一种动态训练监控（DTM）策略，通过实时跟踪蒸馏间隙来稳定不平衡缺失率下的优化过程，并通过自适应地重新加权损失和缩放梯度来平衡跨模态的收敛速度。在BraTS2020和MyoPS2020上的大量实验表明，DMAF-Net在不完整多模态医学图像分割方面优于现有方法。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/violet-42/DMAF-Net">https://github.com/violet-42/DMAF-Net</a>获取。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11691v1">PDF</a> 12 pages, 4 figures, 3 tables</p>
<p><strong>Summary</strong></p>
<p>本文提出一种名为DMAF-Net的动态模态感知融合网络，用于解决多模态医学图像分割中的模态不平衡问题。DMAF-Net引入动态模态感知融合模块，通过结合变压器注意力机制和自适应掩模来抑制缺失模态的干扰，并通过注意力图动态调整权重。同时设计协同关系蒸馏和原型蒸馏框架，以通过协方差一致性、掩膜图注意力实现全局局部特征对齐，并通过跨模态类特定原型对齐确保语义一致性。此外，DMAF-Net还采用动态训练监控策略，以适应不平衡的缺失率并优化平衡收敛速度。实验表明DMAF-Net在多模态医学图像分割上的表现优于现有方法。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>多模态医学图像分割面临模态不平衡的挑战，包括缺失模态和异质模态贡献。</li>
<li>现有方法依赖完整模态可用性假设，无法动态平衡贡献并忽略模态间的结构关系。</li>
<li>DMAF-Net通过动态模态感知融合模块抑制缺失模态干扰，并结合注意力机制动态调整权重。</li>
<li>DMAF-Net设计协同关系蒸馏和原型蒸馏框架，实现全局局部特征对齐和语义一致性。</li>
<li>DMAF-Net采用动态训练监控策略来适应不平衡的缺失率和优化收敛速度。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11691">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-7a43166a3fc0f5013a97a2c6809097bc.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-2c59f6fbe15e54b8ccf2ff0f146d9659.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-4f0145b218fb5d7297988d52d7c8de99.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Comparing-the-Space-Densities-of-Millisecond-Spin-Magnetars-and-Fast-X-Ray-Transients"><a href="#Comparing-the-Space-Densities-of-Millisecond-Spin-Magnetars-and-Fast-X-Ray-Transients" class="headerlink" title="Comparing the Space Densities of Millisecond-Spin Magnetars and Fast   X-Ray Transients"></a>Comparing the Space Densities of Millisecond-Spin Magnetars and Fast   X-Ray Transients</h2><p><strong>Authors:Sumedha Biswas, Peter G. Jonker, M. Coleman Miller, Andrew Levan, Jonathan Quirola-Vásquez</strong></p>
<p>Fast X-ray transients (FXTs) are bright X-ray flashes with durations of minutes to hours, peak isotropic luminosities of L_X,peak ~ 10^42-10^47 erg&#x2F;s, and total isotropic energies of E ~ 10^47-10^50 erg. They have been detected in the soft X-ray band by Chandra, XMM-Newton, Swift-XRT, and, most recently, by Einstein Probe, which has reported more than 50 FXTs in its first year of operation. While several models have been proposed, the nature of many FXTs remains unknown. One model suggests FXTs are powered by the spin-down of newly formed millisecond magnetars, typically produced by binary neutron star (BNS) mergers. However, the BNS volumetric rate, ~10^2 Gpc^-3 yr^-1, barely overlaps with the estimated FXT rate of 10^3-10^4 Gpc^-3 yr^-1. Even within that overlap, BNS mergers would need to produce FXTs at nearly 100% efficiency. We explore whether other millisecond magnetar formation channels could account for this discrepancy. We compile rate densities for several proposed progenitors: accretion-induced collapse of white dwarfs, binary white dwarf mergers, neutron star-white dwarf mergers, and the collapse of massive stars, and convert Galactic event rates into volumetric rates using either the star formation rate or the stellar mass density distributions as a function of redshift. We find that the highest potential formation rates arise from binary white dwarf mergers and massive star collapses. However, both channels face theoretical and observational challenges: the spin and magnetic field properties of the resulting neutron stars are uncertain, and few are expected to satisfy both conditions required for FXT production. Across all scenarios, the fraction of suitable millisecond magnetars is low or poorly constrained. We conclude that they are unlikely to be the dominant progenitors of FXTs and can contribute to at most 10% of the observed FXT population. </p>
<blockquote>
<p>快速X射线瞬变（FXTs）是明亮的X射线闪光，持续时间从几分钟到几小时不等，峰值等距光度约为Lx,peak ~ 10^42-10^47 erg&#x2F;s，总等距能量约为E ~ 10^47-10^50 erg。它们已在软X射线波段被钱德拉、XMM-牛顿、Swift-XRT等探测器探测到，最近还被爱因斯坦探测器探测到，该探测器在其运营的第一年内就报告了超过50个FXTs。尽管已经提出了几种模型，但许多FXTs的本质仍不得而知。有一种模型认为FXTs是由新形成的毫秒磁星的自转减速而产生的，通常是由双中子星（BNS）合并产生的。然而，BNS的体积率约为10^2 Gpc^-3 yr^-1，勉强与估计的FXT率（每单位体积在每立方立方格瑞希度的一年有数万立方米尺度观测度见伽玛射线的时空峰值闪光点数目）的每立方吉秒秒的数量级达到万至十万亿每三年内的比例相当，甚至在这个重叠范围内，BNS合并也需要以接近百分之百的效率产生FXTs。我们探讨了其他毫秒磁星形成通道是否能解释这一差异。我们汇总了几种提出的候选形成源的速率密度：白矮星引发的坍缩、白矮星双星合并、中子星与白矮星合并以及大质量恒星坍缩，并使用恒星形成率或恒星质量密度分布函数与红移关系将银河系事件速率转换为体积速率。我们发现最高潜在形成率来自于白矮星双星合并和大质量恒星坍缩。然而，这两个通道都面临着理论和观测上的挑战：所形成中子星的自转和磁场特性尚不确定，而且预计很少有满足产生FXT所需条件的实例。在所有场景中，适合产生FXT的毫秒磁星比例较低或难以确定。我们得出结论，它们不太可能成为FXTs的主要来源，最多只能贡献观察到的FXT人口的百分之十。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11676v1">PDF</a> Accepted for publication in A&A;</p>
<p><strong>摘要</strong></p>
<p>快X射线瞬态（FXTs）为明亮X射线闪光，持续时间为分钟至数小时，峰值光度约为10^42-10^47erg&#x2F;s，总能量约为E ~ 10^47-10^50erg。它们已在软X射线波段被钱德拉、XMM-牛顿、Swift-XRT等望远镜探测到，最近还被爱因斯坦探测器探测到超过50个FXTs的案例。虽然提出了几种模型，但许多FXTs的性质仍然未知。本文探讨了除毫秒磁星自转减速以外的其他毫秒磁星形成通道是否能为这一差异提供解释。我们编译了几种提议中的祖细胞的速率密度，包括白矮星坍塌、双白矮星合并、中子星与白矮星合并以及大质量恒星坍塌等。我们发现最高的潜在形成率可能来自双白矮星合并和大质量恒星坍塌，但这两条途径在理论和观测上面临挑战，因为它们产生的中子星的自转和磁场特性不确定，并且只有很少能满足生产FXT所需的两项条件。总的来说，合适的毫秒磁星比例较低或难以确定，因此它们不太可能成为FXTs的主要祖细胞，最多只能贡献观察到FXT人群的百分之十。文章重点介绍了目前关于快速X射线瞬态成因的探究以及不同模型的理论预测和存在的挑战。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>FXT是明亮的X射线闪光，持续时间从几分钟到几小时不等，具有较高的峰值光度和总能量。</li>
<li>FXTs已经被多个X射线望远镜探测到，包括爱因斯坦探测器在其运营的第一年就报告了超过五十次观测。</li>
<li>虽然存在多种模型来解释FXTs的起源，但其确切性质仍然未知。</li>
<li>文章探讨了毫秒磁星自转减速模型以外的其他可能的FXT成因模型。</li>
<li>分析了不同模型的潜在形成率，发现双白矮星合并和大质量恒星坍塌是潜在的高形成率模型。但这些模型面临理论挑战和观测困难。这些模型产生的中子星的特性（自转和磁场）并不确定，并且不一定满足产生FXT的条件。 </li>
<li>文章得出的结论是毫秒磁星不太可能成为FXTs的主要成因，其对观察到的FXT人口的贡献最多为百分之十。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11676">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-395633029a7d55a752c3538bd0f4295d.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ca8f815f735f8605ef7db98e2920c684.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-d207be4b02739d7204af9598423c8300.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-449bd7dea761d2f92b213789ad9952f7.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Cross-Modal-Clustering-Guided-Negative-Sampling-for-Self-Supervised-Joint-Learning-from-Medical-Images-and-Reports"><a href="#Cross-Modal-Clustering-Guided-Negative-Sampling-for-Self-Supervised-Joint-Learning-from-Medical-Images-and-Reports" class="headerlink" title="Cross-Modal Clustering-Guided Negative Sampling for Self-Supervised   Joint Learning from Medical Images and Reports"></a>Cross-Modal Clustering-Guided Negative Sampling for Self-Supervised   Joint Learning from Medical Images and Reports</h2><p><strong>Authors:Libin Lan, Hongxing Li, Zunhui Xia, Juan Zhou, Xiaofei Zhu, Yongmei Li, Yudong Zhang, Xin Luo</strong></p>
<p>Learning medical visual representations directly from paired images and reports through multimodal self-supervised learning has emerged as a novel and efficient approach to digital diagnosis in recent years. However, existing models suffer from several severe limitations. 1) neglecting the selection of negative samples, resulting in the scarcity of hard negatives and the inclusion of false negatives; 2) focusing on global feature extraction, but overlooking the fine-grained local details that are crucial for medical image recognition tasks; and 3) contrastive learning primarily targets high-level features but ignoring low-level details which are essential for accurate medical analysis. Motivated by these critical issues, this paper presents a Cross-Modal Cluster-Guided Negative Sampling (CM-CGNS) method with two-fold ideas. First, it extends the k-means clustering used for local text features in the single-modal domain to the multimodal domain through cross-modal attention. This improvement increases the number of negative samples and boosts the model representation capability. Second, it introduces a Cross-Modal Masked Image Reconstruction (CM-MIR) module that leverages local text-to-image features obtained via cross-modal attention to reconstruct masked local image regions. This module significantly strengthens the model’s cross-modal information interaction capabilities and retains low-level image features essential for downstream tasks. By well handling the aforementioned limitations, the proposed CM-CGNS can learn effective and robust medical visual representations suitable for various recognition tasks. Extensive experimental results on classification, detection, and segmentation tasks across five downstream datasets show that our method outperforms state-of-the-art approaches on multiple metrics, verifying its superior performance. </p>
<blockquote>
<p>近年来，通过多模态自监督学习直接从配对图像和报告中学习医学视觉表示，已成为数字诊断的一种新颖且高效的方法。然而，现有模型存在几个严重的局限性。1）忽视了负样本的选择，导致硬负样本稀缺，以及错误负样本的包含；2）虽然关注全局特征提取，但却忽略了对于医学图像识别任务至关重要的细微局部细节；3）对比学习主要针对高级特征，但忽略了对于准确医学分析至关重要的低级细节。针对这些关键问题，本文提出了一种跨模态聚类引导负采样（CM-CGNS）方法，该方法具有两方面的思想。首先，它将用于局部文本特征的k-means聚类扩展到多模态领域，通过跨模态注意力提高负样本的数量并增强模型的表示能力。其次，它引入了一个跨模态掩膜图像重建（CM-MIR）模块，该模块利用通过跨模态注意力获得的局部文本到图像的特征来重建掩膜局部图像区域。这显著增强了模型的跨模态信息交互能力，并保留了下游任务所需的低级图像特征。通过妥善处理上述局限性，所提出的CM-CGNS可以学习适用于各种识别任务的有效和鲁棒的医学视觉表示。在五个下游数据集上进行的分类、检测和分割任务的广泛实验结果表明，我们的方法在多个指标上优于最新技术方法，验证了其卓越性能。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11674v1">PDF</a> This work has been submitted to the IEEE TMI for possible   publication. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/violet-42/CM-CGNS">https://github.com/violet-42/CM-CGNS</a></p>
<p><strong>摘要</strong></p>
<p>本文提出了一种基于跨模态聚类引导的负采样（CM-CGNS）方法，解决了现有医学图像表示学习模型中的三大问题。首先，它通过跨模态注意力将K均值聚类从单模态扩展到多模态领域，提高了负样本的数量并增强了模型表征能力。其次，引入跨模态掩盖图像重建（CM-MIR）模块，利用跨模态注意力获得的局部文本到图像的特征来重建掩盖的局部图像区域，增强了模型的跨模态信息交互能力并保留了重要的低级图像特征。通过解决上述问题，CM-CGNS能够学习适用于各种识别任务的医疗视觉表示。在五个下游数据集上的分类、检测和分割任务的广泛实验结果表明，该方法在多个指标上优于最新技术，验证了其卓越性能。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>直接从配对图像和报告中学习医学视觉表示已成为近年来的新型高效数字诊断方法。</li>
<li>现有模型存在三大局限性：忽视负样本选择、关注全局特征而忽视局部细节以及对比学习主要关注高级特征而忽视低级特征。</li>
<li>CM-CGNS方法通过跨模态注意力将单模态领域的K均值聚类扩展到多模态领域，提高了负样本数量和模型表征能力。</li>
<li>CM-MIR模块的引入利用跨模态注意力获得的局部文本到图像的特征来重建掩盖的局部图像区域，增强了模型的跨模态信息交互能力。</li>
<li>CM-CGNS能够学习适用于各种医疗识别任务的视觉表示。</li>
<li>实验结果表明，在多个数据集和多个任务上，CM-CGNS方法的表现优于现有先进技术。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11674">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-919c6a820049132bc6f9f888efd5955f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4f9a2585a55b9ff1d43ad69626eb8504.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e1e5d6341c07e5886d74936fb538b37b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Prohibited-Items-Segmentation-via-Occlusion-aware-Bilayer-Modeling"><a href="#Prohibited-Items-Segmentation-via-Occlusion-aware-Bilayer-Modeling" class="headerlink" title="Prohibited Items Segmentation via Occlusion-aware Bilayer Modeling"></a>Prohibited Items Segmentation via Occlusion-aware Bilayer Modeling</h2><p><strong>Authors:Yunhan Ren, Ruihuang Li, Lingbo Liu, Changwen Chen</strong></p>
<p>Instance segmentation of prohibited items in security X-ray images is a critical yet challenging task. This is mainly caused by the significant appearance gap between prohibited items in X-ray images and natural objects, as well as the severe overlapping among objects in X-ray images. To address these issues, we propose an occlusion-aware instance segmentation pipeline designed to identify prohibited items in X-ray images. Specifically, to bridge the representation gap, we integrate the Segment Anything Model (SAM) into our pipeline, taking advantage of its rich priors and zero-shot generalization capabilities. To address the overlap between prohibited items, we design an occlusion-aware bilayer mask decoder module that explicitly models the occlusion relationships. To supervise occlusion estimation, we manually annotated occlusion areas of prohibited items in two large-scale X-ray image segmentation datasets, PIDray and PIXray. We then reorganized these additional annotations together with the original information as two occlusion-annotated datasets, PIDray-A and PIXray-A. Extensive experimental results on these occlusion-annotated datasets demonstrate the effectiveness of our proposed method. The datasets and codes are available at: <a target="_blank" rel="noopener" href="https://github.com/Ryh1218/Occ">https://github.com/Ryh1218/Occ</a> </p>
<blockquote>
<p>在安检X光图像中对违禁物品进行实例分割是一项至关重要且富有挑战性的任务。这主要是因为违禁物品在X光图像中的外观与自然物体有很大的差距，以及X光图像中物体之间的严重重叠。为了解决这些问题，我们提出了一种遮挡感知的实例分割管道，用于识别X光图像中的违禁物品。具体来说，为了弥合表示上的差距，我们将“万物可分割”模型（Segment Anything Model，SAM）集成到我们的管道中，利用其丰富的先验知识和零样本泛化能力。为了解决违禁物品之间的重叠问题，我们设计了一个遮挡感知的双层掩膜解码器模块，该模块可以显式地建模遮挡关系。为了监督遮挡估计，我们在两个大规模的X光图像分割数据集PIDray和PIXray上手动标注了违禁物品的遮挡区域，然后将这些额外的标注与原始信息一起重新组织成两个带遮挡标注的数据集PIDray-A和PIXray-A。在这些带遮挡标注的数据集上进行的大量实验结果表明了我们提出的方法的有效性。数据集和代码可在<a target="_blank" rel="noopener" href="https://github.com/Ryh1218/Occ%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/Ryh1218/Occ找到。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11661v1">PDF</a> Accepted by ICME 2025</p>
<p><strong>Summary</strong><br>禁止物品在安检X光图像中的实例分割是一项重要而具有挑战性的任务。针对这一问题，我们提出了一种遮挡感知的实例分割管道设计，旨在识别X光图像中的禁止物品。我们整合了分段任何事情模型（SAM），以缩小表示差距并处理物品之间的重叠问题。为解决禁止物品的遮挡问题，我们设计了一个遮挡感知的双层掩膜解码模块，该模块可以明确建模遮挡关系。我们在两个大规模的X光图像分割数据集PIDray和PIXray上手动标注了禁止物品的遮挡区域，并以此创建了两个带遮挡注释的数据集PIDray-A和PIXray-A。实验证明，我们提出的方法在这些带遮挡注释的数据集上效果显著。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>实例分割在安检X光图像中识别禁止物品是一项挑战，因为物品间存在外观差异和严重重叠。</li>
<li>我们提出了一种遮挡感知的实例分割管道设计来识别这些物品。</li>
<li>利用Segment Anything Model（SAM）缩小表示差距。</li>
<li>设计了一个双层掩膜解码模块来处理物品间的遮挡关系。</li>
<li>在两个大型X光图像分割数据集上手动标注了禁止物品的遮挡区域，并创建了带遮挡注释的数据集。</li>
<li>实验证明在带遮挡注释的数据集上的方法效果显著。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11661">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8fd3b2d8bbe6ca8478d1d8efe0fcb63b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-07eb805eddd1a9758b179cdab9106746.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-426ce04d30db68618608d72aaf893ef4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-423e6be170cd2c2edb074a78e4c46407.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6ce8e315b597f0191351be7544f5907.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9cefe4fc38396dae12cc5162b9e929ca.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8db2c52feb295dab0ead5675a8708904.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Taming-Stable-Diffusion-for-Computed-Tomography-Blind-Super-Resolution"><a href="#Taming-Stable-Diffusion-for-Computed-Tomography-Blind-Super-Resolution" class="headerlink" title="Taming Stable Diffusion for Computed Tomography Blind Super-Resolution"></a>Taming Stable Diffusion for Computed Tomography Blind Super-Resolution</h2><p><strong>Authors:Chunlei Li, Yilei Shi, Haoxi Hu, Jingliang Hu, Xiao Xiang Zhu, Lichao Mou</strong></p>
<p>High-resolution computed tomography (CT) imaging is essential for medical diagnosis but requires increased radiation exposure, creating a critical trade-off between image quality and patient safety. While deep learning methods have shown promise in CT super-resolution, they face challenges with complex degradations and limited medical training data. Meanwhile, large-scale pre-trained diffusion models, particularly Stable Diffusion, have demonstrated remarkable capabilities in synthesizing fine details across various vision tasks. Motivated by this, we propose a novel framework that adapts Stable Diffusion for CT blind super-resolution. We employ a practical degradation model to synthesize realistic low-quality images and leverage a pre-trained vision-language model to generate corresponding descriptions. Subsequently, we perform super-resolution using Stable Diffusion with a specialized controlling strategy, conditioned on both low-resolution inputs and the generated text descriptions. Extensive experiments show that our method outperforms existing approaches, demonstrating its potential for achieving high-quality CT imaging at reduced radiation doses. Our code will be made publicly available. </p>
<blockquote>
<p>高分辨率计算机断层扫描（CT）成像对于医学诊断至关重要，但会增加辐射暴露，从而在图像质量和患者安全之间形成了关键的权衡。深度学习方法在CT超分辨率领域已展现出巨大的潜力，但仍面临复杂退化问题和有限的医学训练数据挑战。与此同时，大规模预训练的扩散模型，特别是Stable Diffusion，在各种视觉任务中合成精细细节方面表现出了显著的能力。受此启发，我们提出了一种适应Stable Diffusion的CT盲超分辨率新框架。我们采用实用的退化模型合成逼真的低质量图像，并利用预训练的视觉语言模型生成相应的描述。随后，我们使用Stable Diffusion进行超分辨率处理，采用专门的控制策略，以低分辨率输入和生成的文本描述为条件。大量实验表明，我们的方法优于现有方法，展现了在降低辐射剂量下实现高质量CT成像的潜力。我们的代码将公开提供。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11496v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在医学诊断中，高分辨率的计算机断层扫描（CT）成像至关重要，但同时也增加了患者的辐射暴露风险。深度学习方法在CT超分辨率处理方面显示出潜力，但仍面临复杂退化和有限医学训练数据的挑战。受大型预训练扩散模型（如Stable Diffusion）在合成各种视觉任务精细细节方面的出色表现的启发，我们提出了一种基于Stable Diffusion的CT盲超分辨率处理的新框架。该框架利用实用的退化模型合成逼真的低质量图像，并利用预训练的视觉语言模型生成相应的描述。然后，我们采用一种特殊的控制策略进行超分辨率处理，该策略根据低分辨率输入和生成的文本描述进行条件处理。实验表明，我们的方法优于现有方法，有望在降低辐射剂量的同时实现高质量的CT成像。我们的代码将公开提供。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>高分辨率CT成像对于医学诊断至关重要，但需要平衡图像质量和患者辐射暴露。</li>
<li>深度学习方法在CT超分辨率处理中有潜力，但面临复杂退化和有限医学数据的挑战。</li>
<li>大型预训练扩散模型如Stable Diffusion在合成精细细节方面表现出色。</li>
<li>提出了一种基于Stable Diffusion的CT盲超分辨率处理的新框架。</li>
<li>该框架利用退化模型合成低质量图像并利用视觉语言模型生成描述。</li>
<li>利用低分辨率输入和文本描述进行超分辨率处理。</li>
<li>实验表明该方法优于现有技术，可实现降低辐射剂量下的高质量CT成像。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11496">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-d397898207aa858353b577a64be996e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-757c01d51b11bd154ab8e931081dd179.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b706d9f8458750592023c83c66fc06f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Uncertainty-Awareness-Enables-Efficient-Labeling-for-Cancer-Subtyping-in-Digital-Pathology"><a href="#Uncertainty-Awareness-Enables-Efficient-Labeling-for-Cancer-Subtyping-in-Digital-Pathology" class="headerlink" title="Uncertainty Awareness Enables Efficient Labeling for Cancer Subtyping in   Digital Pathology"></a>Uncertainty Awareness Enables Efficient Labeling for Cancer Subtyping in   Digital Pathology</h2><p><strong>Authors:Nirhoshan Sivaroopan, Chamuditha Jayanga Galappaththige, Chalani Ekanayake, Hasindri Watawana, Ranga Rodrigo, Chamira U. S. Edussooriya, Dushan N. Wadduwage</strong></p>
<p>Machine-learning-assisted cancer subtyping is a promising avenue in digital pathology. Cancer subtyping models, however, require careful training using expert annotations so that they can be inferred with a degree of known certainty (or uncertainty). To this end, we introduce the concept of uncertainty awareness into a self-supervised contrastive learning model. This is achieved by computing an evidence vector at every epoch, which assesses the model’s confidence in its predictions. The derived uncertainty score is then utilized as a metric to selectively label the most crucial images that require further annotation, thus iteratively refining the training process. With just 1-10% of strategically selected annotations, we attain state-of-the-art performance in cancer subtyping on benchmark datasets. Our method not only strategically guides the annotation process to minimize the need for extensive labeled datasets, but also improves the precision and efficiency of classifications. This development is particularly beneficial in settings where the availability of labeled data is limited, offering a promising direction for future research and application in digital pathology. </p>
<blockquote>
<p>机器学习辅助癌症分型是数字病理学领域的一个前景广阔的研究方向。然而，癌症分型模型需要使用专家注释进行仔细训练，以便能够以已知的确定性（或不确定性）进行推断。为此，我们将不确定性的概念引入了一种自监督对比学习模型。这是通过每个时代计算证据向量来实现的，该证据向量评估模型对其预测的置信度。然后利用派生出的不确定性评分作为指标，有选择地标记最需要进一步注释的图像，从而迭代地改进训练过程。仅使用1-10%的战略选择注释，我们在基准数据集上的癌症分型达到了最先进的性能。我们的方法不仅战略性地引导了注释过程，以最大限度地减少需要大量标记数据集的需求，而且还提高了分类的精确性和效率。在标记数据可用性有限的情况下，这一发展特别有益，为数字病理学未来的研究与应用提供了有前景的方向。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11439v1">PDF</a> </p>
<p><strong>Summary</strong><br>     机器学习辅助癌症分型是数字病理学中的一条有前途的道路。我们引入不确定性意识概念到自监督对比学习模型中，通过计算每个时期的证据向量评估模型的预测置信度，实现不确定性评分作为指标来选择性标注需要更多注释的关键图像，从而迭代优化训练过程。仅使用1-10%的策略选择注释，就能在基准数据集上实现癌症分型的最新性能。我们的方法不仅战略性地引导注释过程，减少需要大量标记数据集的需求，还提高了分类的精确度和效率，特别是在标记数据有限的环境中，为数字病理学未来的研究与应用提供了有前景的方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>机器学习辅助癌症分型是数字病理学中的有前途的研究方向。</li>
<li>引入不确定性意识概念到自监督对比学习模型中以提高模型性能。</li>
<li>通过计算证据向量评估模型的预测置信度，实现不确定性评分。</li>
<li>利用不确定性评分选择性标注需要更多注释的关键图像，迭代优化训练过程。</li>
<li>仅需少量策略性选择的注释就能达到癌症分型的最新性能。</li>
<li>方法能战略性地引导注释过程，减少需要大量标记数据集的需求。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11439">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-e48c72b0f965b2239f27007bdf7506c7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c9be0f471d5e53b4fd32b45db83c394f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-902fa276b02203ccd55361e8a27974c3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-f923bcdbdd4ad4a3f1581b0d46bab601.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="3D-RAD-A-Comprehensive-3D-Radiology-Med-VQA-Dataset-with-Multi-Temporal-Analysis-and-Diverse-Diagnostic-Tasks"><a href="#3D-RAD-A-Comprehensive-3D-Radiology-Med-VQA-Dataset-with-Multi-Temporal-Analysis-and-Diverse-Diagnostic-Tasks" class="headerlink" title="3D-RAD: A Comprehensive 3D Radiology Med-VQA Dataset with Multi-Temporal   Analysis and Diverse Diagnostic Tasks"></a>3D-RAD: A Comprehensive 3D Radiology Med-VQA Dataset with Multi-Temporal   Analysis and Diverse Diagnostic Tasks</h2><p><strong>Authors:Xiaotang Gai, Jiaxiang Liu, Yichen Li, Zijie Meng, Jian Wu, Zuozhu Liu</strong></p>
<p>Medical Visual Question Answering (Med-VQA) holds significant potential for clinical decision support, yet existing efforts primarily focus on 2D imaging with limited task diversity. This paper presents 3D-RAD, a large-scale dataset designed to advance 3D Med-VQA using radiology CT scans. The 3D-RAD dataset encompasses six diverse VQA tasks: anomaly detection, image observation, medical computation, existence detection, static temporal diagnosis, and longitudinal temporal diagnosis. It supports both open- and closed-ended questions while introducing complex reasoning challenges, including computational tasks and multi-stage temporal analysis, to enable comprehensive benchmarking. Extensive evaluations demonstrate that existing vision-language models (VLMs), especially medical VLMs exhibit limited generalization, particularly in multi-temporal tasks, underscoring the challenges of real-world 3D diagnostic reasoning. To drive future advancements, we release a high-quality training set 3D-RAD-T of 136,195 expert-aligned samples, showing that fine-tuning on this dataset could significantly enhance model performance. Our dataset and code, aiming to catalyze multimodal medical AI research and establish a robust foundation for 3D medical visual understanding, are publicly available at <a target="_blank" rel="noopener" href="https://github.com/Tang-xiaoxiao/M3D-RAD">https://github.com/Tang-xiaoxiao/M3D-RAD</a>. </p>
<blockquote>
<p>医学视觉问答（Med-VQA）在临床决策支持方面拥有巨大潜力，但现有研究主要集中在二维成像上，任务多样性有限。本文介绍了大规模数据集3D-RAD，旨在利用放射学CT扫描推进三维Med-VQA的发展。3D-RAD数据集包含六个多样化的VQA任务：异常检测、图像观察、医学计算、存在检测、静态时间诊断和纵向时间诊断。它支持开放和封闭性问题，同时引入复杂的推理挑战，包括计算任务和多阶段时间分析，以实现全面的基准测试。大量评估表明，现有的视觉语言模型（VLMs）特别是医学VLMs的泛化能力有限，特别是在多时间任务中，这突出了现实世界三维诊断推理的挑战。为了推动未来的进步，我们发布了高质量的训练集3D-RAD-T，包含136,195个专家对齐样本，表明在此数据集上进行微调可以显著提高模型性能。我们的数据集和代码旨在推动多模态医疗人工智能研究并为三维医学视觉理解建立稳健的基础，可在<a target="_blank" rel="noopener" href="https://github.com/Tang-xiaoxiao/M3D-RAD%E4%B8%8A%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Tang-xiaoxiao/M3D-RAD上公开访问。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11147v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>本文介绍了一个大型数据集3D-RAD，用于推进基于三维医学影像（如CT扫描）的医疗视觉问答（Med-VQA）。该数据集涵盖六种多样的问答任务，支持开放和封闭性问题，并引入复杂的推理挑战，如计算任务和多时相分析。现有视觉语言模型（VLMs）在该数据集上的表现有限，特别是在多时相任务中。为提升模型性能，发布了一个高质量的训练集3D-RAD-T。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>3D-RAD数据集用于推进基于三维医学影像的医疗视觉问答（Med-VQA）。</li>
<li>该数据集包含六种多样的问答任务，涵盖异常检测、图像观察、医学计算等。</li>
<li>引入复杂推理挑战，如计算任务和多时相分析。</li>
<li>现有视觉语言模型在该数据集上的表现有限，尤其在多时相任务中。</li>
<li>公开了一个高质量的训练集3D-RAD-T，以提高模型性能。</li>
<li>数据集和代码公开可用，旨在推动多学科医疗人工智能研究，并为三维医学视觉理解建立稳健基础。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11147">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-63385aa89d52cd7a190991f769c9dda2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-17e48f3f52c339470d7f89f29b8fa2db.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-907aeef9c824d1ca496481109b73e3bd.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-005c841a0e7cbb0a4410f7dac515eb0c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5e5030f5bcea2c40accd0552e31c48d6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-42297fc5baf83e41d5e119bf55a6589a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8a1ef17c4ce2c9f3b0c4a6156b509146.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ContextLoss-Context-Information-for-Topology-Preserving-Segmentation"><a href="#ContextLoss-Context-Information-for-Topology-Preserving-Segmentation" class="headerlink" title="ContextLoss: Context Information for Topology-Preserving Segmentation"></a>ContextLoss: Context Information for Topology-Preserving Segmentation</h2><p><strong>Authors:Benedict Schacht, Imke Greving, Simone Frintrop, Berit Zeller-Plumhoff, Christian Wilms</strong></p>
<p>In image segmentation, preserving the topology of segmented structures like vessels, membranes, or roads is crucial. For instance, topological errors on road networks can significantly impact navigation. Recently proposed solutions are loss functions based on critical pixel masks that consider the whole skeleton of the segmented structures in the critical pixel mask. We propose the novel loss function ContextLoss (CLoss) that improves topological correctness by considering topological errors with their whole context in the critical pixel mask. The additional context improves the network focus on the topological errors. Further, we propose two intuitive metrics to verify improved connectivity due to a closing of missed connections. We benchmark our proposed CLoss on three public datasets (2D &amp; 3D) and our own 3D nano-imaging dataset of bone cement lines. Training with our proposed CLoss increases performance on topology-aware metrics and repairs up to 44% more missed connections than other state-of-the-art methods. We make the code publicly available. </p>
<blockquote>
<p>在图像分割中，保持分割结构（如血管、膜或道路）的拓扑结构至关重要。例如，道路网络上的拓扑错误会对导航产生重大影响。最近提出的解决方案是基于关键像素掩膜的损失函数，它考虑了关键像素掩膜中分割结构的整体骨架。我们提出了新型的损失函数ContextLoss（CLoss），通过考虑关键像素掩膜中拓扑错误及其整体上下文，提高了拓扑的正确性。额外的上下文提高了网络对拓扑错误的关注。此外，我们提出了两个直观的度量指标，以验证由于闭合的遗漏连接而提高了连通性。我们在三个公共数据集（2D和3D）以及我们自己的3D纳米成像骨水泥线数据集上对我们的CLoss进行了基准测试。使用我们提出的CLoss进行训练，可以提高拓扑感知指标的性能，并且可以修复比其他最先进的方法多达44%的遗漏连接。我们公开提供代码。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11134v1">PDF</a> 13 pages, 7 figures, accepted to ICIP 2025</p>
<p><strong>Summary</strong></p>
<p>本文提出一种新型的损失函数ContextLoss（CLoss），用于改进图像分割中的拓扑正确性。该函数通过考虑关键像素掩膜中的拓扑错误及其整体上下文，提高了网络对拓扑错误的关注。此外，本文还提出了两种直观的度量指标，以验证因闭合遗漏连接而提高的连通性。在三个公共数据集（包括2D和3D）以及自己的3D纳米成像数据集上进行的基准测试表明，使用CLoss进行训练可提高拓扑感知指标的性能，并修复了比其他最先进方法多出44%的遗漏连接。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ContextLoss（CLoss）是一种新型的损失函数，旨在改进图像分割中的拓扑正确性。</li>
<li>CLoss通过考虑关键像素掩膜中的拓扑错误及其整体上下文，提高网络对拓扑错误的关注。</li>
<li>提出两种直观度量指标，用于验证因使用CLoss而提高的连通性。</li>
<li>在多个公共数据集上的基准测试表明，使用CLoss进行训练可提高拓扑感知指标的性能。</li>
<li>CLoss能修复比其他最先进方法多出44%的遗漏连接。</li>
<li>代码已公开供公众使用。</li>
<li>CLoss的应用范围可能涵盖需要高度关注拓扑正确性的领域，如道路网络、血管分割等。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11134">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-965216df85f471e4b92ece45b6fd5d8e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-5023c524f36bfe4eaa5e593e779f8bdd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b0c596bd2b6857f863cbdaaaf4b8e3a4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e2445e0db62bae762ce6934d82575ae.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6dd10936f4a0b6e4fad357c831f08134.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6c8c9c0c9fd1e0616bd9e6073184fdaa.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="PiPViT-Patch-based-Visual-Interpretable-Prototypes-for-Retinal-Image-Analysis"><a href="#PiPViT-Patch-based-Visual-Interpretable-Prototypes-for-Retinal-Image-Analysis" class="headerlink" title="PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image   Analysis"></a>PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image   Analysis</h2><p><strong>Authors:Marzieh Oghbaie, Teresa Araújo, Hrvoje Bogunović</strong></p>
<p>Background and Objective: Prototype-based methods improve interpretability by learning fine-grained part-prototypes; however, their visualization in the input pixel space is not always consistent with human-understandable biomarkers. In addition, well-known prototype-based approaches typically learn extremely granular prototypes that are less interpretable in medical imaging, where both the presence and extent of biomarkers and lesions are critical.   Methods: To address these challenges, we propose PiPViT (Patch-based Visual Interpretable Prototypes), an inherently interpretable prototypical model for image recognition. Leveraging a vision transformer (ViT), PiPViT captures long-range dependencies among patches to learn robust, human-interpretable prototypes that approximate lesion extent only using image-level labels. Additionally, PiPViT benefits from contrastive learning and multi-resolution input processing, which enables effective localization of biomarkers across scales.   Results: We evaluated PiPViT on retinal OCT image classification across four datasets, where it achieved competitive quantitative performance compared to state-of-the-art methods while delivering more meaningful explanations. Moreover, quantitative evaluation on a hold-out test set confirms that the learned prototypes are semantically and clinically relevant. We believe PiPViT can transparently explain its decisions and assist clinicians in understanding diagnostic outcomes. Github page: <a target="_blank" rel="noopener" href="https://github.com/marziehoghbaie/PiPViT">https://github.com/marziehoghbaie/PiPViT</a> </p>
<blockquote>
<p>背景与目标：基于原型的方法通过学习精细的局部原型来提高解释性，但它们在输入像素空间中的可视化并不总是与人们可理解的生物标志物相一致。此外，众所周知的基于原型的方法通常学习极其精细的原型，在医学成像中不太容易解释，其中生物标志物和病变的存在和程度都至关重要。方法：针对这些挑战，我们提出了PiPViT（基于补丁的视觉可解释原型），这是一种用于图像识别的固有可解释原型模型。借助视觉变压器（ViT），PiPViT捕获补丁之间的长距离依赖关系，仅使用图像级标签学习稳健且人类可解释的原型，以近似病变程度。此外，PiPViT受益于对比学习和多分辨率输入处理，这实现了跨尺度的生物标志物有效定位。结果：我们在四个数据集上对视网膜OCT图像分类任务上评估了PiPViT，其在定量性能上达到了与最先进的方法相当的竞争水平，同时提供了更有意义的解释。此外，在独立测试集上的定量评估证实，所学习的原型在语义和临床上都具有相关性。我们相信PiPViT能够透明地解释其决策，并帮助临床医生理解诊断结果。GitHub页面：<a target="_blank" rel="noopener" href="https://github.com/marziehoghbaie/PiPViT">https://github.com/marziehoghbaie/PiPViT</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10669v2">PDF</a> </p>
<p><strong>Summary</strong><br>     PiPViT是一个基于视觉的解读原型模型，旨在解决医学图像中现有原型方法的可视化与解读问题。通过捕捉图像块之间的长期依赖关系，该模型能学习鲁棒且可解读的原型，并近似病灶范围。通过对比学习和多分辨率输入处理，模型在视网膜OCT图像分类任务上表现优异，同时提供有意义的解释。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PiPViT是一个针对图像识别的基于视觉的可解读原型模型。</li>
<li>该模型利用视觉转换器（ViT）捕捉图像块之间的长期依赖关系。</li>
<li>PiPViT能学习鲁棒且可解读的原型，这些原型可以近似表示病灶范围。</li>
<li>PiPViT通过对比学习和多分辨率输入处理，实现了有效定位不同尺度的生物标志物。</li>
<li>在四个视网膜OCT图像分类数据集上，PiPViT表现出与最新技术相当的定量性能。</li>
<li>定性评价表明，学习到的原型具有语义和临床相关性。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10669">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-0969f7adfbbed0f62ac0ac02d0c509fe.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-22c307ff2862f361125a11c7c90838b0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e77e404dc185bcbe1d351602dcb3826d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e0b64cacedf2e77b764f7d8df895575b.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="We-Care-Each-Pixel-Calibrating-on-Medical-Segmentation-Model"><a href="#We-Care-Each-Pixel-Calibrating-on-Medical-Segmentation-Model" class="headerlink" title="We Care Each Pixel: Calibrating on Medical Segmentation Model"></a>We Care Each Pixel: Calibrating on Medical Segmentation Model</h2><p><strong>Authors:Wenhao Liang, Wei Zhang, Lin Yue, Miao Xu, Olaf Maennel, Weitong Chen</strong></p>
<p>Medical image segmentation is fundamental for computer-aided diagnostics, providing accurate delineation of anatomical structures and pathological regions. While common metrics such as Accuracy, DSC, IoU, and HD primarily quantify spatial agreement between predictions and ground-truth labels, they do not assess the calibration quality of segmentation models, which is crucial for clinical reliability. To address this limitation, we propose pixel-wise Expected Calibration Error (pECE), a novel metric that explicitly measures miscalibration at the pixel level, thereby ensuring both spatial precision and confidence reliability. We further introduce a morphological adaptation strategy that applies morphological operations to ground-truth masks before computing calibration losses, particularly benefiting margin-based losses such as Margin SVLS and NACL. Additionally, we present the Signed Distance Calibration Loss (SDC), which aligns boundary geometry with calibration objectives by penalizing discrepancies between predicted and ground-truth signed distance functions (SDFs). Extensive experiments demonstrate that our method not only enhances segmentation performance but also improves calibration quality, yielding more trustworthy confidence estimates. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/EagleAdelaide/SDC-Loss">https://github.com/EagleAdelaide/SDC-Loss</a>. </p>
<blockquote>
<p>医学图像分割对于计算机辅助诊断至关重要，它提供了准确的解剖结构和病理区域的描述。虽然准确率、DSC、IoU和HD等常用指标主要量化预测和真实标签之间的空间一致性，但它们并没有评估分割模型的校准质量，这对于临床可靠性至关重要。为了解决这一局限性，我们提出了像素级期望校准误差（pECE）这一新型指标，它明确地测量像素级的校准误差，从而确保空间精度和置信度可靠性。此外，我们还引入了一种形态学适应策略，该策略在对真实标签掩膜进行形态学操作后才计算校准损失，这对基于边距的损失（如Margin SVLS和NACL）特别有益。另外，我们提出了符号距离校准损失（SDC），它通过惩罚预测和真实标签之间符号距离函数（SDF）的差异，使边界几何与校准目标对齐。大量实验表明，我们的方法不仅提高了分割性能，还提高了校准质量，产生了更可靠的置信度估计。代码可在：<a target="_blank" rel="noopener" href="https://github.com/EagleAdelaide/SDC-Loss%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/EagleAdelaide/SDC-Loss获取。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.05107v2">PDF</a> Under Reviewing</p>
<p><strong>Summary</strong></p>
<p>医学图像分割对于计算机辅助诊断至关重要，能够准确描绘解剖结构和病理区域。为提高分割模型的校准质量，提出像素级期望校准误差（pECE）这一新指标，以衡量像素级别的误校准情况，确保空间精度和置信度可靠性。同时，引入形态学适应策略，对真实标签掩膜进行形态学操作来计算校准损失，尤其有利于基于边距的损失。此外，推出符号距离校准损失（SDC），通过惩罚预测与真实标签之间的符号距离函数差异，对齐边界几何与校准目标。实验证明，该方法不仅提升分割性能，更提高校准质量，产生更可靠的置信度估计。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学图像分割对计算机辅助诊断至关重要。</li>
<li>现有评估指标主要关注空间一致性，缺乏模型校准质量的评估。</li>
<li>提出像素级期望校准误差（pECE）指标，衡量像素级别误校准情况。</li>
<li>引入形态学适应策略，通过形态学操作计算校准损失。</li>
<li>介绍符号距离校准损失（SDC），对齐边界几何与校准目标。</li>
<li>方法提升分割性能及校准质量。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.05107">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-6571beca283858a65736daeb9649e2b3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bbc77f38220450278a050143f7268ad4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0db425d3f06a5702e1e7d48ccf207c04.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3aef861b8218c480a2e21485991422d7.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Censor-Aware-Semi-Supervised-Survival-Time-Prediction-in-Lung-Cancer-Using-Clinical-and-Radiomics-Features"><a href="#Censor-Aware-Semi-Supervised-Survival-Time-Prediction-in-Lung-Cancer-Using-Clinical-and-Radiomics-Features" class="headerlink" title="Censor-Aware Semi-Supervised Survival Time Prediction in Lung Cancer   Using Clinical and Radiomics Features"></a>Censor-Aware Semi-Supervised Survival Time Prediction in Lung Cancer   Using Clinical and Radiomics Features</h2><p><strong>Authors:Arman Gorji, Ali Fathi Jouzdani, Nima Sanati, Ren Yuan, Arman Rahmim, Mohammad R. Salmanpour</strong></p>
<p>Objectives: Lung cancer poses a significant global health challenge, necessitating improved prognostic methods for personalized treatment. This study introduces a censor-aware semi-supervised learning (SSL) framework that integrates clinical and imaging data, addressing biases in traditional models handling censored data. Methods: We analyzed clinical, PET and CT data from 199 lung cancer patients from public and local data respositories, focusing on overall survival (OS) time as the primary outcome. Handcrafted (HRF) and Deep Radiomics features (DRF) were extracted after preprocessing using ViSERA software and were combined with clinical features (CF). Feature dimensions were optimized using Principal Component Analysis (PCA), followed by the application of supervised learning (SL) and SSL. SSL incorporated pseudo-labeling of censored data to improve performance. Seven regressors and three hazard ratio survival analysis (HRSA) algorithms were optimized using five-fold cross-validation, grid search and external test bootstrapping. Results: For PET HRFs, SSL reduced the mean absolute error (MAE) by 26.5%, achieving 1.55 years with PCA+decision tree regression, compared to SL’s 2.11 years with PCA+KNNR (p&lt;0.05). Combining HRFs (CT_HRF) and DRFs from CT images using SSL+PCA+KNNR achieved an MAE of 2.08 years, outperforming SL’s 2.26 years by 7.96% (p&lt;0.05). In HRSA, CT_HRF applied to PCA+Component Wise Gradient Boosting Survival Analysis achieved an external c-index of 0.65, effectively differentiating high- and low-risk groups. Conclusions: We demonstrated that the SSL strategy significantly outperforms SL across PET, CT, and CF. As such, censor-aware SSL applied to HRFs from PET images significantly improved survival prediction performance by 26.5% compared to the SL approach. </p>
<blockquote>
<p>目标：肺癌构成一项重大的全球健康挑战，需要改进预测方法以实现个性化治疗。本研究引入了一种有审查意识的半监督学习（SSL）框架，该框架融合了临床和成像数据，解决了传统模型在处理受审查数据时的偏见问题。</p>
</blockquote>
<p>方法：我们分析了来自公共和本地数据仓库的199名肺癌患者的临床、PET和CT数据，以总体存活时间作为主要结果。使用ViSERA软件预处理后提取手工特征（HRF）和深度放射学特征（DRF），并与临床特征（CF）相结合。使用主成分分析（PCA）优化特征维度，然后应用监督学习（SL）和SSL。SSL结合了伪标签法处理受审查数据以提高性能。通过五折交叉验证、网格搜索和外部测试Bootstrap优化七个回归器和三个危险比率生存分析（HRSA）算法。</p>
<p>结果：对于PET的HRFs，SSL将平均绝对误差（MAE）降低了26.5%，使用PCA+决策树回归达到1.55年，而SL与PCA+KNNR的组合为2.11年（p&lt;0.05）。结合来自CT图像的HRFs（CT_HRF）和DRFs使用SSL+PCA+KNNR的MAE为2.08年，优于SL的2.26年，提高了7.96%（p&lt;0.05）。在HRSA中，将CT_HRF应用于PCA+Component Wise Gradient Boosting Survival Analysis，外部c指数为0.65，有效地区分了高、低风险组。</p>
<p>结论：我们证明SSL策略在PET、CT和CF方面显著优于SL。因此，与SL方法相比，应用于PET图像HRFs的有审查意识的SSL将生存预测性能提高了26.5%。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.01661v4">PDF</a> 11 pages, 4 Figures and 4 Tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种结合临床和成像数据的审查感知半监督学习（SSL）框架，以解决传统模型处理审查数据时的偏见问题。通过对199名肺癌患者的临床、PET和CT数据进行分析，该研究发现SSL策略在PET、CT和临床特征方面的表现均优于监督学习（SL），尤其是在PET的手工特征（HRF）上，SSL将平均绝对误差（MAE）降低了26.5%，显著提高了生存预测的准确性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>研究引入了审查感知半监督学习（SSL）框架，融合了临床和成像数据，以改进对肺癌患者的个性化治疗预后方法。</li>
<li>通过分析199名肺癌患者的临床、PET和CT数据，重点研究患者的总体生存时间。</li>
<li>SSL框架通过伪标记审查数据提高了性能，在PET的手工特征（HRF）上，与监督学习（SL）相比，SSL将平均绝对误差（MAE）降低了26.5%。</li>
<li>结合CT图像的HRF和Deep Radiomics特征（DRF），使用SSL+PCA+KNNR的MAE为2.08年，优于SL的2.26年，提升了7.96%。</li>
<li>在危害比率生存分析（HRSA）中，使用PCA+Component Wise Gradient Boosting Survival Analysis结合CT_HRF达到了外部c-指数为0.65，能有效区分高、低风险组。</li>
<li>SSL策略在PET、CT和临床特征方面的表现均优于SL。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.01661">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-15e5b8a279b71da307a9dd96057b84fb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6fe5b84e40ec50dbc908acb343a2fe9f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2ce0b335a3849bf071cf95ecf8c0f945.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e76e9db3ea3bdec2bc01897f32fcf55.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-566ff697d53c43a375df15a3584670d8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b80046fb395a21d30dec944b8d92423.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-ce95558ca118212ccf216e1634dcb47c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-cc39cf04b1dc12ec7070f435c1a4a1db.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-cc42ecacf76027484344ee2fa5e695dd.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="UKAN-EP-Enhancing-U-KAN-with-Efficient-Attention-and-Pyramid-Aggregation-for-3D-Multi-Modal-MRI-Brain-Tumor-Segmentation"><a href="#UKAN-EP-Enhancing-U-KAN-with-Efficient-Attention-and-Pyramid-Aggregation-for-3D-Multi-Modal-MRI-Brain-Tumor-Segmentation" class="headerlink" title="UKAN-EP: Enhancing U-KAN with Efficient Attention and Pyramid   Aggregation for 3D Multi-Modal MRI Brain Tumor Segmentation"></a>UKAN-EP: Enhancing U-KAN with Efficient Attention and Pyramid   Aggregation for 3D Multi-Modal MRI Brain Tumor Segmentation</h2><p><strong>Authors:Yanbing Chen, Tianze Tang, Taehyo Kim, Hai Shu</strong></p>
<p>Gliomas are among the most common malignant brain tumors and are characterized by considerable heterogeneity, which complicates accurate detection and segmentation. Multi-modal MRI is the clinical standard for glioma imaging, but variability across modalities and high computational complexity hinder effective automated segmentation. In this paper, we propose UKAN-EP, a novel 3D extension of the original 2D U-KAN model for multi-modal MRI brain tumor segmentation. While U-KAN integrates Kolmogorov-Arnold Network (KAN) layers into a U-Net backbone, UKAN-EP further incorporates Efficient Channel Attention (ECA) and Pyramid Feature Aggregation (PFA) modules to enhance inter-modality feature fusion and multi-scale feature representation. We also introduce a dynamic loss weighting strategy that adaptively balances the Cross-Entropy and Dice losses during training. We evaluate UKAN-EP on the 2024 BraTS-GLI dataset and compare it against strong baselines including U-Net, Attention U-Net, and Swin UNETR. Results show that UKAN-EP achieves superior segmentation performance while requiring substantially fewer computational resources. An extensive ablation study further demonstrates the effectiveness of ECA and PFA, as well as the limited utility of self-attention and spatial attention alternatives. Code is available at <a target="_blank" rel="noopener" href="https://github.com/TianzeTang0504/UKAN-EP">https://github.com/TianzeTang0504/UKAN-EP</a>. </p>
<blockquote>
<p>胶质瘤是最常见的恶性脑肿瘤之一，其特点是具有相当大的异质性，这使得准确的检测和分割变得复杂。多模态MRI是胶质瘤成像的临床标准，但不同模态之间的差异和较高的计算复杂性阻碍了有效的自动分割。在本文中，我们提出了UKAN-EP，这是原始二维U-KAN模型的多模态MRI脑肿瘤分割的新型三维扩展。U-KAN将Kolmogorov-Arnold网络（KAN）层集成到U-Net主干中，而UKAN-EP进一步结合了高效通道注意力（ECA）和金字塔特征聚合（PFA）模块，以增强跨模态特征融合和多尺度特征表示。我们还引入了一种动态损失权重策略，该策略可以自适应地平衡训练和测试过程中的交叉熵和Dice损失。我们在包含U-Net、Attention U-Net和Swin UNETR等强大基准模型的BraTS-GLI数据集上评估了UKAN-EP的性能。结果表明，UKAN-EP在达到更高的分割性能的同时，所需的计算资源大大减少。大量的消融研究进一步证明了ECA和PFA的有效性，以及自注意力和空间注意力替代方案的局限性。相关代码可访问于：<a target="_blank" rel="noopener" href="https://github.com/TianzeTang0504/UKAN-EP%E3%80%82">https://github.com/TianzeTang0504/UKAN-EP。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2408.00273v2">PDF</a> </p>
<p><strong>Summary</strong><br>     针对胶质脑瘤的恶性脑肿瘤在医疗图像分析中极为常见，但因其显著的异质性导致准确检测和分割变得复杂。本研究提出了一种新型的基于多模态MRI的胶质脑瘤分割模型——UKAN-EP。该模型结合了Kolmogorov-Arnold网络层、高效通道注意力模块和多尺度特征金字塔模块，强化了模态间的特征融合与多尺度特征表达。通过动态损失权重策略，自适应平衡交叉熵和Dice损失进行训练。在BraTS-GLI数据集上的评估显示，UKAN-EP不仅实现了优越的分割性能，而且计算资源消耗更少。代码已公开。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该研究关注胶质脑瘤这一常见恶性脑肿瘤的图像分析挑战，尤其是其异质性问题。</li>
<li>提出了一种新的基于多模态MRI的胶质脑瘤分割模型UKAN-EP，扩展了原有的二维模型并加入了更多优化模块。</li>
<li>UKAN-EP集成了Kolmogorov-Arnold网络层，以增强模型的性能。</li>
<li>Efficient Channel Attention（ECA）和Pyramid Feature Aggregation（PFA）模块的加入提升了模型的性能表现。ECA提高了通道间注意力的利用效率，而PFA增强了多尺度特征的融合能力。</li>
<li>动态损失权重策略有助于在训练过程中自适应平衡不同损失函数的影响。</li>
<li>在BraTS-GLI数据集上的实验验证了UKAN-EP模型在胶质脑瘤分割上的优越性，并且计算资源消耗较低。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2408.00273">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pic1.zhimg.com/v2-730bb5bc5cd9afef9d1a5341004aeecb.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dc22fda98a2aa5050a91eaaf25892f14.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Direct3γ-A-Pipeline-for-Direct-Three-gamma-PET-Image-Reconstruction"><a href="#Direct3γ-A-Pipeline-for-Direct-Three-gamma-PET-Image-Reconstruction" class="headerlink" title="Direct3γ: A Pipeline for Direct Three-gamma PET Image   Reconstruction"></a>Direct3γ: A Pipeline for Direct Three-gamma PET Image   Reconstruction</h2><p><strong>Authors:Youness Mellak, Alexandre Bousse, Thibaut Merlin, Debora Giovagnoli, Dimitris Visvikis</strong></p>
<p>This paper presents a novel image reconstruction pipeline for three-gamma (3-{\gamma}) positron emission tomography (PET) aimed at improving spatial resolution and reducing noise in nuclear medicine. The proposed Direct3{\gamma} pipeline addresses the inherent challenges in 3-{\gamma} PET systems, such as detector imperfections and uncertainty in photon interaction points. A key feature of the pipeline is its ability to determine the order of interactions through a model trained on Monte Carlo (MC) simulations using the Geant4 Application for Tomography Emission (GATE) toolkit, thus providing the necessary information to construct Compton cones which intersect with the line of response (LOR) to provide an estimate of the emission point. The pipeline processes 3-{\gamma} PET raw data, reconstructs histoimages by propagating energy and spatial uncertainties along the LOR, and applies a 3-D convolutional neural network (CNN) to refine these intermediate images into high-quality reconstructions. To further enhance image quality, the pipeline leverages both supervised learning and adversarial losses, the latter preserving fine structural details. Experimental results show that Direct3{\gamma} consistently outperforms conventional 200-ps time-of-flight (TOF) PET in terms of SSIM and PSNR. </p>
<blockquote>
<p>本文提出了一种针对三伽马（3-γ）正电子发射断层扫描（PET）图像重建的新流程，旨在提高核医学中的空间分辨率并降低噪声。所提出的Direct3γ流程解决了3-γ PET系统固有的挑战，如探测器缺陷和光子交互点的不确定性。该流程的一个关键功能是，它能够通过使用Geant4发射断层扫描应用程序（GATE）工具包进行的蒙特卡洛（MC）模拟训练模型来确定交互的顺序，从而提供构建交于响应线（LOR）的康普顿锥的必要信息，以估计发射点。该流程处理3-γ PET原始数据，通过传播能量和空间不确定性沿LOR重建直方图像，并应用三维卷积神经网络（CNN）将这些中间图像精细化为高质量重建。为了进一步提高图像质量，该流程结合了监督学习和对抗性损失，后者能够保留精细的结构细节。实验结果表明，Direct3γ在结构相似性度量（SSIM）和峰值信噪比（PSNR）方面始终优于传统的200皮秒飞行时间（TOF）PET。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.18337v6">PDF</a> 11 pages, 11 figures, 2 tables</p>
<p><strong>Summary</strong></p>
<p>本文介绍了一种针对三伽马（3-γ）正电子发射断层扫描（PET）图像重建的新流程，旨在提高核医学中的空间分辨率并降低噪声。该流程通过一系列技术和算法，如利用蒙特卡洛模拟训练模型、确定交互顺序、构建康普顿锥与响应线交点等，解决3-γ PET系统的固有挑战。实验结果表明，该流程在结构相似度指数（SSIM）和峰值信噪比（PSNR）方面优于传统的200皮秒飞行时间（TOF）PET。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>该论文提出了一种针对三伽马（3-γ）PET的新型图像重建流程，旨在提高空间分辨率并降低核医学中的噪声。</li>
<li>通过利用Geant4 Application for Tomography Emission（GATE）工具包的蒙特卡洛（MC）模拟进行模型训练，确定光子交互顺序。</li>
<li>该流程通过构建康普顿锥与响应线交点来估计发射点，从而处理3-γ PET原始数据。</li>
<li>通过沿响应线传播能量和空间不确定性来重建直方图像，并利用三维卷积神经网络（CNN）对中间图像进行精细化处理，生成高质量重建图像。</li>
<li>该流程采用监督学习和对抗性损失来进一步提高图像质量，其中对抗性损失有助于保留精细结构细节。</li>
<li>实验结果表明，该流程在结构相似度指数（SSIM）和峰值信噪比（PSNR）方面表现优异，优于传统的200皮秒飞行时间（TOF）PET。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.18337">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://picx.zhimg.com/v2-8676efb046b0547fcbeb31f8e17e6897.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c237686cd3535fc11daea7695616bd21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d686919113adf714ed5302f80c19123.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bf6181951dfb16932106658d26fa1577.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0d3193083c913e5a2bbde118dd4d3bc2.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="MedVersa-A-Generalist-Foundation-Model-for-Medical-Image-Interpretation"><a href="#MedVersa-A-Generalist-Foundation-Model-for-Medical-Image-Interpretation" class="headerlink" title="MedVersa: A Generalist Foundation Model for Medical Image Interpretation"></a>MedVersa: A Generalist Foundation Model for Medical Image Interpretation</h2><p><strong>Authors:Hong-Yu Zhou, Julián Nicolás Acosta, Subathra Adithan, Suvrankar Datta, Eric J. Topol, Pranav Rajpurkar</strong></p>
<p>Current medical AI systems are often limited to narrow applications, hindering widespread adoption. We present MedVersa, a generalist foundation model trained on tens of millions of compiled medical instances. MedVersa unlocks generalist learning from multimodal inputs and outputs, representing the first example of a generalist model reaching competitive performance with leading specialized solutions across a variety of medical imaging scenarios. MedVersa achieves state-of-the-art performance in nine tasks, sometimes outperforming counterparts by over 10%. Radiologist evaluation shows MedVersa-generated reports get superior performance in 95% of normal studies, while matching or exceeding human reports in 71% of cases overall. User studies showed notable reductions in report writing time and discrepancies with the use of MedVersa. Our findings underscore the value of flexible, multimodal AI systems in advancing medical image interpretation and supporting clinical expertise. </p>
<blockquote>
<p>当前医疗人工智能系统往往局限于特定应用，阻碍了其广泛应用。我们推出了MedVersa，这是一款在数百万医疗实例上训练的通用基础模型。MedVersa解锁了从多模式输入和输出中学习通用知识的能力，成为第一个在多种医学成像场景中达到领先专业解决方案竞争力的通用模型。MedVersa在九个任务中达到了最新技术水平，有时较同类产品的性能高出超过10%。放射科医生评价显示，MedVersa生成的报告在正常研究的95%中表现优越，并在总体情况下有71%与人类报告相匹配或超越人类报告。用户研究表明，使用MedVersa显著减少了报告编写时间和差异。我们的研究强调了灵活、多模式人工智能系统在推进医学图像解读和支持临床专业知识方面的价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2405.07988v2">PDF</a> Technical study</p>
<p><strong>Summary</strong><br>医学AI系统通常局限于特定应用，阻碍其广泛应用。我们推出MedVersa，这是一种经过数百万医疗实例训练的基础通用模型。MedVersa支持从多模式输入和输出中学习通用知识，成为首个在多种医学成像场景中与领先的专业解决方案相竞争的通用模型。在九项任务中，MedVersa取得了最先进的性能，有时较同类产品的性能高出超过10%。放射科医生评估显示，MedVersa生成的报告在正常研究的95%中表现优越，并在总体上以匹配或超过人类报告的速度在71%的案例中表现良好。用户研究表明，使用MedVersa显著减少了报告编写时间和差异。我们的研究强调了灵活的多模式AI系统在推进医学图像解读和支持临床专业知识方面的价值。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>医学AI系统的局限性：当前医学AI系统通常仅限于特定应用，限制了其广泛应用。</li>
<li>MedVersa的引入：提出了一种名为MedVersa的通用基础模型，该模型经过数百万医疗实例训练。</li>
<li>多模态学习与输出：MedVersa支持从多模式输入和输出中学习，这是一个创新特点。</li>
<li>竞争性能：MedVersa在多种医学成像场景中表现出与领先的专业解决方案相当的竞争力。</li>
<li>先进性能表现：在九项任务中，MedVersa达到最先进的性能水平，并在某些情况下显著优于其他系统。</li>
<li>放射科医生评估结果：在放射科医生评估中，MedVersa生成的报告在大部分情况下表现良好。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2405.07988">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="https://pica.zhimg.com/v2-aa43c762a6880e3eaa3313bd717bb59f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a76721bc49c3afc27db47c8cf3c9a76a.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-456f55ec5be92ac1255dab795dcdf128.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-7c7622b9dc8034b5612f7fcba18cb1a4.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-5fbb2ca44e38db36b157dfb6ce26606b.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">医学图像</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-17/TTS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-52c133cbb52da685471dd033058441b1.jpg" class="responsive-img" alt="TTS">
                        
                        <span class="card-title">TTS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            TTS 方向最新论文已更新，请持续关注 Update in 2025-06-17  S2ST-Omni An Efficient and Scalable Multilingual Speech-to-Speech   Translation Framework via Seamlessly Speech-Text Alignment and Streaming   Speech Decoder
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                    TTS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/TTS/">
                        <span class="chip bg-color">TTS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-17/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-99808382c44ef4bc9f8c2ce88090b33a.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2025-06-17  Taming Stable Diffusion for Computed Tomography Blind Super-Resolution
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">25156.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
