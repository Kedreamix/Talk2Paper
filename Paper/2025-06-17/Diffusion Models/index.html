<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Diffusion Models">
    <meta name="description" content="Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-17  Taming Stable Diffusion for Computed Tomography Blind Super-Resolution">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Diffusion Models | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-99808382c44ef4bc9f8c2ce88090b33a.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Diffusion Models</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Diffusion-Models/">
                                <span class="chip bg-color">Diffusion Models</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                Diffusion Models
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    10.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    42 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-17-æ›´æ–°"><a href="#2025-06-17-æ›´æ–°" class="headerlink" title="2025-06-17 æ›´æ–°"></a>2025-06-17 æ›´æ–°</h1><h2 id="Taming-Stable-Diffusion-for-Computed-Tomography-Blind-Super-Resolution"><a href="#Taming-Stable-Diffusion-for-Computed-Tomography-Blind-Super-Resolution" class="headerlink" title="Taming Stable Diffusion for Computed Tomography Blind Super-Resolution"></a>Taming Stable Diffusion for Computed Tomography Blind Super-Resolution</h2><p><strong>Authors:Chunlei Li, Yilei Shi, Haoxi Hu, Jingliang Hu, Xiao Xiang Zhu, Lichao Mou</strong></p>
<p>High-resolution computed tomography (CT) imaging is essential for medical diagnosis but requires increased radiation exposure, creating a critical trade-off between image quality and patient safety. While deep learning methods have shown promise in CT super-resolution, they face challenges with complex degradations and limited medical training data. Meanwhile, large-scale pre-trained diffusion models, particularly Stable Diffusion, have demonstrated remarkable capabilities in synthesizing fine details across various vision tasks. Motivated by this, we propose a novel framework that adapts Stable Diffusion for CT blind super-resolution. We employ a practical degradation model to synthesize realistic low-quality images and leverage a pre-trained vision-language model to generate corresponding descriptions. Subsequently, we perform super-resolution using Stable Diffusion with a specialized controlling strategy, conditioned on both low-resolution inputs and the generated text descriptions. Extensive experiments show that our method outperforms existing approaches, demonstrating its potential for achieving high-quality CT imaging at reduced radiation doses. Our code will be made publicly available. </p>
<blockquote>
<p>é«˜åˆ†è¾¨ç‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æˆåƒå¯¹äºåŒ»å­¦è¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†éœ€è¦è¿›è¡Œè¾ƒé«˜çš„è¾å°„æš´éœ²ï¼Œå› æ­¤éœ€è¦åœ¨å›¾åƒè´¨é‡å’Œæ‚£è€…å®‰å…¨ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚è™½ç„¶æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨CTè¶…åˆ†è¾¨ç‡æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬é¢ä¸´ç€å¤æ‚çš„é€€åŒ–å’Œæœ‰é™çš„åŒ»å­¦è®­ç»ƒæ•°æ®çš„æŒ‘æˆ˜ã€‚ä¸æ­¤åŒæ—¶ï¼Œå¤§è§„æ¨¡é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯Stable Diffusionï¼Œåœ¨å„ç§è§†è§‰ä»»åŠ¡ä¸­åˆæˆç²¾ç»†ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºäº†æƒŠäººçš„èƒ½åŠ›ã€‚å—æ­¤å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†Stable Diffusioné€‚åº”äºCTç›²è¶…åˆ†è¾¨ç‡ã€‚æˆ‘ä»¬é‡‡ç”¨å®ç”¨çš„é€€åŒ–æ¨¡å‹æ¥åˆæˆé€¼çœŸçš„ä½è´¨é‡å›¾åƒï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆç›¸åº”çš„æè¿°ã€‚éšåï¼Œæˆ‘ä»¬ä½¿ç”¨ç¨³å®šæ‰©æ•£è¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ï¼Œå¹¶é‡‡ç”¨ç‰¹æ®Šçš„æ§åˆ¶ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åŸºäºä½åˆ†è¾¨ç‡è¾“å…¥å’Œç”Ÿæˆçš„æ–‡æœ¬æè¿°è¿›è¡Œæ¡ä»¶å¤„ç†ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå±•ç°å‡ºåœ¨é™ä½è¾å°„å‰‚é‡ä¸‹å®ç°é«˜è´¨é‡CTæˆåƒçš„æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11496v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨åŒ»å­¦è¯Šæ–­ä¸­é«˜åˆ†è¾¨ç‡è®¡ç®—æœºæ–­å±‚æ‰«æï¼ˆCTï¼‰æˆåƒçš„é‡è¦æ€§ä¸å…¶å¯¹æ‚£è€…è¾å°„æš´éœ²çš„å¢åŠ ä¹‹é—´çš„çŸ›ç›¾ã€‚æ·±åº¦å­¦ä¹ è™½ç„¶åœ¨CTè¶…åˆ†è¾¨ç‡æŠ€æœ¯ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œä½†åœ¨å¤„ç†å¤æ‚é€€åŒ–å’Œæœ‰é™åŒ»ç–—è®­ç»ƒæ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚å—å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯Stable Diffusionåœ¨å„é¡¹è§†è§‰ä»»åŠ¡ä¸­åˆæˆç²¾ç»†ç»†èŠ‚æ–¹é¢çš„å‡ºè‰²è¡¨ç°çš„å¯å‘ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€‚åº”äºCTç›²è¶…åˆ†è¾¨ç‡çš„æ–°å‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å®ç”¨é€€åŒ–æ¨¡å‹åˆæˆé€¼çœŸçš„ä½è´¨é‡å›¾åƒï¼Œå¹¶åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰è¯­è¨€æ¨¡å‹ç”Ÿæˆç›¸åº”æè¿°ã€‚éšåï¼Œä½¿ç”¨Stable Diffusionè¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ï¼Œé‡‡ç”¨ä¸“é—¨çš„æ§åˆ¶ç­–ç•¥ï¼Œä»¥ä½åˆ†è¾¨ç‡è¾“å…¥å’Œç”Ÿæˆçš„æ–‡æœ¬æè¿°ä¸ºæ¡ä»¶ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæœ‰æœ›åœ¨é™ä½è¾å°„å‰‚é‡çš„åŒæ—¶å®ç°é«˜è´¨é‡çš„CTæˆåƒã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>é«˜åˆ†è¾¨ç‡CTæˆåƒåœ¨åŒ»å­¦è¯Šæ–­ä¸­è‡³å…³é‡è¦ï¼Œä½†ä¼šå¢åŠ æ‚£è€…è¾å°„æš´éœ²ï¼Œéœ€è¦åœ¨å›¾åƒè´¨é‡å’Œæ‚£è€…å®‰å…¨ä¹‹é—´å–å¾—å¹³è¡¡ã€‚</li>
<li>æ·±åº¦å­¦ä¹ åœ¨CTè¶…åˆ†è¾¨ç‡æŠ€æœ¯ä¸­çš„åº”ç”¨è™½å…·æ½œåŠ›ï¼Œä½†å¤„ç†å¤æ‚é€€åŒ–å’Œæœ‰é™åŒ»ç–—è®­ç»ƒæ•°æ®æ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼Œå¦‚Stable Diffusionï¼Œåœ¨åˆæˆç²¾ç»†è§†è§‰ç»†èŠ‚æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹æ¡†æ¶ï¼Œé€‚åº”äºCTç›²è¶…åˆ†è¾¨ç‡ï¼Œç»“åˆå®ç”¨é€€åŒ–æ¨¡å‹ã€é¢„è®­ç»ƒè§†è§‰è¯­è¨€æ¨¡å‹å’ŒStable Diffusionã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡åˆæˆä½è´¨é‡å›¾åƒå’Œç”Ÿæˆç›¸åº”æè¿°ï¼Œåˆ©ç”¨Stable Diffusionè¿›è¡Œè¶…åˆ†è¾¨ç‡å¤„ç†ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œæœ‰æœ›æé«˜CTæˆåƒè´¨é‡å¹¶é™ä½è¾å°„å‰‚é‡ã€‚</li>
<li>è¯¥ç ”ç©¶çš„ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11496">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d397898207aa858353b577a64be996e5.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-757c01d51b11bd154ab8e931081dd179.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-3b706d9f8458750592023c83c66fc06f.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="GaussMarker-Robust-Dual-Domain-Watermark-for-Diffusion-Models"><a href="#GaussMarker-Robust-Dual-Domain-Watermark-for-Diffusion-Models" class="headerlink" title="GaussMarker: Robust Dual-Domain Watermark for Diffusion Models"></a>GaussMarker: Robust Dual-Domain Watermark for Diffusion Models</h2><p><strong>Authors:Kecen Li, Zhicong Huang, Xinwen Hou, Cheng Hong</strong></p>
<p>As Diffusion Models (DM) generate increasingly realistic images, related issues such as copyright and misuse have become a growing concern. Watermarking is one of the promising solutions. Existing methods inject the watermark into the single-domain of initial Gaussian noise for generation, which suffers from unsatisfactory robustness. This paper presents the first dual-domain DM watermarking approach using a pipelined injector to consistently embed watermarks in both the spatial and frequency domains. To further boost robustness against certain image manipulations and advanced attacks, we introduce a model-independent learnable Gaussian Noise Restorer (GNR) to refine Gaussian noise extracted from manipulated images and enhance detection robustness by integrating the detection scores of both watermarks. GaussMarker efficiently achieves state-of-the-art performance under eight image distortions and four advanced attacks across three versions of Stable Diffusion with better recall and lower false positive rates, as preferred in real applications. </p>
<blockquote>
<p>éšç€æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰ç”Ÿæˆçš„å›¾åƒè¶Šæ¥è¶ŠçœŸå®ï¼Œç‰ˆæƒå’Œè¯¯ç”¨ç­‰ç›¸å…³é—®é¢˜å·²æˆä¸ºè¶Šæ¥è¶Šå¤§çš„å…³æ³¨ç‚¹ã€‚æ°´å°æ˜¯ä¸€ç§æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚ç°æœ‰æ–¹æ³•å°†æ°´å°æ³¨å…¥åˆå§‹é«˜æ–¯å™ªå£°çš„å•ä¸€é¢†åŸŸä¸­è¿›è¡Œç”Ÿæˆï¼Œè¿™å¯¼è‡´é²æ£’æ€§ä¸ä½³ã€‚æœ¬æ–‡æå‡ºäº†ç¬¬ä¸€ç§åŒåŸŸDMæ°´å°æ–¹æ³•ï¼Œä½¿ç”¨æµæ°´çº¿æ³¨å…¥å™¨åœ¨ç©ºé—´å’Œé¢‘ç‡ä¸¤ä¸ªé¢†åŸŸä¸­æŒç»­åµŒå…¥æ°´å°ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºå¯¹æŸäº›å›¾åƒæ“ä½œå’Œé«˜çº§æ”»å‡»çš„é²æ£’æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¨¡å‹ç‹¬ç«‹çš„å¯å­¦ä¹ é«˜æ–¯å™ªå£°ä¿®å¤å™¨ï¼ˆGNRï¼‰ï¼Œä»¥æ”¹è¿›ä»æ“ä½œè¿‡çš„å›¾åƒä¸­æå–çš„é«˜æ–¯å™ªå£°ï¼Œå¹¶é€šè¿‡æ•´åˆä¸¤ä¸ªæ°´å°çš„æ£€æµ‹åˆ†æ•°æ¥æé«˜æ£€æµ‹ç¨³å¥æ€§ã€‚GaussMarkeråœ¨ä¸‰ç§ç‰ˆæœ¬çš„Stable Diffusionä¸‹ï¼Œåœ¨å…«ç§å›¾åƒå¤±çœŸå’Œå››ç§é«˜çº§æ”»å‡»ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œå…·æœ‰æ›´å¥½çš„å¬å›ç‡å’Œæ›´ä½çš„è¯¯æŠ¥ç‡ï¼Œé€‚åˆåœ¨çœŸå®åº”ç”¨ä¸­ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11444v1">PDF</a> Accepted at ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰ç”Ÿæˆçš„å›¾åƒè¶Šæ¥è¶Šé€¼çœŸï¼Œç›¸å…³çš„ç‰ˆæƒå’Œæ»¥ç”¨é—®é¢˜é€æ¸æˆä¸ºå…³æ³¨çš„ç„¦ç‚¹ã€‚æ°´å°æ˜¯ä¸€ç§æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚ç°æœ‰æ–¹æ³•å°†æ°´å°æ³¨å…¥åˆå§‹é«˜æ–¯å™ªå£°çš„å•åŸŸç”Ÿæˆï¼Œå…¶ç¨³å¥æ€§å¹¶ä¸ç†æƒ³ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºåŒåŸŸDMæ°´å°æ–¹æ³•ï¼Œé‡‡ç”¨ç®¡é“æ³¨å…¥å™¨åœ¨ç©ºé—´å’Œé¢‘ç‡åŸŸä¸­æŒç»­åµŒå…¥æ°´å°ã€‚ä¸ºæé«˜å¯¹ç‰¹å®šå›¾åƒæ“ä½œå’Œé«˜çº§æ”»å‡»çš„ç¨³å¥æ€§ï¼Œå¼•å…¥æ¨¡å‹æ— å…³çš„å¯å­¦ä¹ é«˜æ–¯å™ªå£°ä¿®å¤å™¨ï¼ˆGNRï¼‰ï¼Œä¿®å¤ä»æ“ä½œå›¾åƒä¸­æå–çš„é«˜æ–¯å™ªå£°ï¼Œå¹¶é€šè¿‡æ•´åˆä¸¤ä¸ªæ°´å°çš„æ£€æµ‹åˆ†æ•°æé«˜æ£€æµ‹ç¨³å¥æ€§ã€‚GaussMarkeråœ¨ä¸‰ç§ç‰ˆæœ¬çš„Stable Diffusionä¸‹ï¼Œé¢å¯¹å…«ç§å›¾åƒå¤±çœŸå’Œå››ç§é«˜çº§æ”»å‡»ï¼Œå®ç°äº†é«˜æ•ˆæ€§èƒ½ï¼Œå¬å›ç‡æ›´é«˜ï¼Œè¯¯æŠ¥ç‡æ›´ä½ï¼Œé€‚ç”¨äºå®é™…åº”ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹ï¼ˆDMï¼‰ç”Ÿæˆçš„å›¾åƒæ—¥ç›Šé€¼çœŸï¼Œå¼•å‘ç‰ˆæƒå’Œæ»¥ç”¨é—®é¢˜çš„å…³æ³¨ã€‚</li>
<li>æ°´å°æ˜¯è§£å†³è¿™äº›é—®é¢˜çš„æœ‰å‰æ™¯çš„æ–¹æ¡ˆã€‚</li>
<li>ç°æœ‰æ–¹æ³•åœ¨æ°´å°åµŒå…¥å•åŸŸçš„é«˜æ–¯å™ªå£°ç”Ÿæˆä¸­å­˜åœ¨ç¨³å¥æ€§é—®é¢˜ã€‚</li>
<li>æœ¬æ–‡é¦–æ¬¡æå‡ºåŒåŸŸDMæ°´å°æ–¹æ³•ï¼ŒåŒæ—¶åµŒå…¥ç©ºé—´å’Œé¢‘ç‡åŸŸã€‚</li>
<li>ä¸ºæé«˜ç¨³å¥æ€§ï¼Œå¼•å…¥æ¨¡å‹æ— å…³çš„å¯å­¦ä¹ é«˜æ–¯å™ªå£°ä¿®å¤å™¨ï¼ˆGNRï¼‰ã€‚</li>
<li>GaussMarkeråœ¨å¤šç§å›¾åƒå¤±çœŸå’Œé«˜çº§æ”»å‡»ä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11444">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2623f96b53084a9bd54880f96b448e7e.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-577b9ab7259f96729f30157cad94565b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-99808382c44ef4bc9f8c2ce88090b33a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a051ed1c1bb4f3ee3770fc35c8a44e80.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-f31c87477ede225e20cb0d7736487b98.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Auditing-Data-Provenance-in-Real-world-Text-to-Image-Diffusion-Models-for-Privacy-and-Copyright-Protection"><a href="#Auditing-Data-Provenance-in-Real-world-Text-to-Image-Diffusion-Models-for-Privacy-and-Copyright-Protection" class="headerlink" title="Auditing Data Provenance in Real-world Text-to-Image Diffusion Models   for Privacy and Copyright Protection"></a>Auditing Data Provenance in Real-world Text-to-Image Diffusion Models   for Privacy and Copyright Protection</h2><p><strong>Authors:Jie Zhu, Leye Wang</strong></p>
<p>Text-to-image diffusion model since its propose has significantly influenced the content creation due to its impressive generation capability. However, this capability depends on large-scale text-image datasets gathered from web platforms like social media, posing substantial challenges in copyright compliance and personal privacy leakage. Though there are some efforts devoted to explore approaches for auditing data provenance in text-to-image diffusion models, existing work has unrealistic assumptions that can obtain model internal knowledge, e.g., intermediate results, or the evaluation is not reliable. To fill this gap, we propose a completely black-box auditing framework called Feature Semantic Consistency-based Auditing (FSCA). It utilizes two types of semantic connections within the text-to-image diffusion model for auditing, eliminating the need for access to internal knowledge. To demonstrate the effectiveness of our FSCA framework, we perform extensive experiments on LAION-mi dataset and COCO dataset, and compare with eight state-of-the-art baseline approaches. The results show that FSCA surpasses previous baseline approaches across various metrics and different data distributions, showcasing the superiority of our FSCA. Moreover, we introduce a recall balance strategy and a threshold adjustment strategy, which collectively allows FSCA to reach up a user-level accuracy of 90% in a real-world auditing scenario with only 10 samples&#x2F;user, highlighting its strong auditing potential in real-world applications. Our code is made available at <a target="_blank" rel="noopener" href="https://github.com/JiePKU/FSCA">https://github.com/JiePKU/FSCA</a>. </p>
<blockquote>
<p>è‡ªä»æ–‡æœ¬åˆ°å›¾åƒçš„æ‰©æ•£æ¨¡å‹æå‡ºä»¥æ¥ï¼Œç”±äºå…¶ä»¤äººå°è±¡æ·±åˆ»çš„ç”Ÿæˆèƒ½åŠ›ï¼Œå®ƒæ˜¾è‘—å½±å“äº†å†…å®¹åˆ›ä½œã€‚ç„¶è€Œï¼Œè¿™ç§èƒ½åŠ›ä¾èµ–äºä»ç¤¾äº¤å¹³å°å¦‚ç¤¾äº¤åª’ä½“ç­‰ç½‘ç»œä¸Šæ”¶é›†çš„å¤§è§„æ¨¡æ–‡æœ¬å›¾åƒæ•°æ®é›†ï¼Œè¿™å¸¦æ¥äº†ç‰ˆæƒåˆè§„å’Œä¸ªäººéšç§æ³„éœ²çš„å®è´¨æŒ‘æˆ˜ã€‚å°½ç®¡æœ‰äº†ä¸€äº›åŠªåŠ›æ¥æ¢ç´¢å®¡è®¡æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­æ•°æ®æ¥æºçš„æ–¹æ³•ï¼Œä½†ç°æœ‰å·¥ä½œåœ¨å‡è®¾èƒ½å¤Ÿè·å¾—æ¨¡å‹å†…éƒ¨çŸ¥è¯†ï¼ˆå¦‚ä¸­é—´ç»“æœï¼‰æ–¹é¢å¹¶ä¸ç°å®ï¼Œæˆ–è€…è¯„ä¼°ç»“æœä¸å¯é ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºåŸºäºç‰¹å¾è¯­ä¹‰ä¸€è‡´æ€§çš„å®¡è®¡ï¼ˆFSCAï¼‰çš„å®Œå…¨é»‘ç®±å®¡è®¡æ¡†æ¶ã€‚å®ƒåˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ä¸­çš„ä¸¤ç§è¯­ä¹‰è¿æ¥æ¥è¿›è¡Œå®¡è®¡ï¼Œæ— éœ€è®¿é—®å†…éƒ¨çŸ¥è¯†ã€‚ä¸ºäº†è¯æ˜æˆ‘ä»¬çš„FSCAæ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨LAION-miæ•°æ®é›†å’ŒCOCOæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤§é‡å®éªŒï¼Œå¹¶ä¸å…«ç§æœ€å…ˆè¿›çš„åŸºçº¿æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒFSCAåœ¨å„é¡¹æŒ‡æ ‡å’Œä¸åŒæ•°æ®åˆ†å¸ƒæ–¹é¢éƒ½è¶…è¶Šäº†ä¹‹å‰çš„åŸºçº¿æ–¹æ³•ï¼Œå±•ç¤ºäº†æˆ‘ä»¬çš„FSCAçš„ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¬å›å¹³è¡¡ç­–ç•¥å’Œé˜ˆå€¼è°ƒæ•´ç­–ç•¥ï¼Œå…±åŒä½¿FSCAåœ¨åªæœ‰10ä¸ªæ ·æœ¬&#x2F;ç”¨æˆ·çš„æƒ…å†µä¸‹ï¼ŒçœŸå®ä¸–ç•Œçš„å®¡è®¡åœºæ™¯ä¸­è¾¾åˆ°äº†90%çš„ç”¨æˆ·çº§å‡†ç¡®ç‡ï¼Œå‡¸æ˜¾äº†å…¶åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„å¼ºå¤§å®¡è®¡æ½œåŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/JiePKU/FSCA%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/JiePKU/FSCAä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11434v1">PDF</a> Under Review; A user-level accuracy of 90% in a real-world auditing   scenario</p>
<p><strong>Summary</strong><br>     æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹çš„å½±å“æ˜¾è‘—ï¼Œä½†å…¶ç”Ÿæˆèƒ½åŠ›ä¾èµ–äºä»ç¤¾äº¤å¹³å°å¦‚ç¤¾äº¤åª’ä½“ç­‰æœé›†çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå­˜åœ¨ç‰ˆæƒåˆè§„å’Œä¸ªäººéšç§æ³„éœ²çš„æŒ‘æˆ˜ã€‚é’ˆå¯¹ç°æœ‰å®¡è®¡æ–¹æ³•çš„ä¸åˆ‡å®é™…å‡è®¾å’Œè¯„ä¼°ä¸å¯é é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç‰¹å¾è¯­ä¹‰ä¸€è‡´æ€§çš„å®¡è®¡æ¡†æ¶FSCAã€‚å®ƒåœ¨æ— éœ€è®¿é—®æ¨¡å‹å†…éƒ¨çŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œåˆ©ç”¨ä¸¤ç§è¯­ä¹‰è¿æ¥è¿›è¡Œå®¡è®¡ã€‚å®éªŒè¯æ˜ï¼ŒFSCAåœ¨å¤šç§æŒ‡æ ‡å’Œä¸åŒæ•°æ®åˆ†å¸ƒä¸Šè¶…è¶Šäº†å…«ç§æœ€æ–°åŸºçº¿æ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†å¬å›å¹³è¡¡ç­–ç•¥å’Œé˜ˆå€¼è°ƒæ•´ç­–ç•¥ï¼Œåœ¨çœŸå®ä¸–ç•Œå®¡è®¡åœºæ™¯ä¸­å®ç°äº†ç”¨æˆ·çº§90%çš„å‡†ç¡®ç‡ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨çœŸå®ä¸–ç•Œåº”ç”¨ä¸­çš„å¼ºå¤§å®¡è®¡æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–‡æœ¬è½¬å›¾åƒæ‰©æ•£æ¨¡å‹ç”Ÿæˆèƒ½åŠ›ä¾èµ–äºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œå¼•å‘ç‰ˆæƒå’Œä¸ªäººéšç§æ³„éœ²é—®é¢˜ã€‚</li>
<li>ç›®å‰å®¡è®¡æ•°æ®æº¯æºçš„æ–¹æ³•å­˜åœ¨ä¸ç°å®çš„å‡è®¾æˆ–è¯„ä¼°ä¸å¯é çš„é—®é¢˜ã€‚</li>
<li>æå‡ºçš„FSCAæ¡†æ¶æ— éœ€è®¿é—®æ¨¡å‹å†…éƒ¨çŸ¥è¯†ï¼Œåˆ©ç”¨ä¸¤ç§è¯­ä¹‰è¿æ¥è¿›è¡Œå®¡è®¡ã€‚</li>
<li>å®éªŒè¯æ˜FSCAåœ¨å„ç§æŒ‡æ ‡å’Œæ•°æ®åˆ†å¸ƒä¸Šä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
<li>FSCAæ¡†æ¶å¼•å…¥äº†å¬å›å¹³è¡¡ç­–ç•¥å’Œé˜ˆå€¼è°ƒæ•´ç­–ç•¥ä»¥æé«˜å®¡è®¡å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨çœŸå®ä¸–ç•Œå®¡è®¡åœºæ™¯ä¸­ï¼ŒFSCAè¾¾åˆ°äº†ç”¨æˆ·çº§90%çš„å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11434">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6401d4467a2afbb7f07e91616743500e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-0982d999582f8d55d84056ca5ce356dc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-afeeb14f29c679bfab13b10a76c6fc81.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Score-based-Generative-Diffusion-Models-to-Synthesize-Full-dose-FDG-Brain-PET-from-MRI-in-Epilepsy-Patients"><a href="#Score-based-Generative-Diffusion-Models-to-Synthesize-Full-dose-FDG-Brain-PET-from-MRI-in-Epilepsy-Patients" class="headerlink" title="Score-based Generative Diffusion Models to Synthesize Full-dose FDG   Brain PET from MRI in Epilepsy Patients"></a>Score-based Generative Diffusion Models to Synthesize Full-dose FDG   Brain PET from MRI in Epilepsy Patients</h2><p><strong>Authors:Jiaqi Wu, Jiahong Ouyang, Farshad Moradi, Mohammad Mehdi Khalighi, Greg Zaharchuk</strong></p>
<p>Fluorodeoxyglucose (FDG) PET to evaluate patients with epilepsy is one of the most common applications for simultaneous PET&#x2F;MRI, given the need to image both brain structure and metabolism, but is suboptimal due to the radiation dose in this young population. Little work has been done synthesizing diagnostic quality PET images from MRI data or MRI data with ultralow-dose PET using advanced generative AI methods, such as diffusion models, with attention to clinical evaluations tailored for the epilepsy population. Here we compared the performance of diffusion- and non-diffusion-based deep learning models for the MRI-to-PET image translation task for epilepsy imaging using simultaneous PET&#x2F;MRI in 52 subjects (40 train&#x2F;2 validate&#x2F;10 hold-out test). We tested three different models: 2 score-based generative diffusion models (SGM-Karras Diffusion [SGM-KD] and SGM-variance preserving [SGM-VP]) and a Transformer-Unet. We report results on standard image processing metrics as well as clinically relevant metrics, including congruency measures (Congruence Index and Congruency Mean Absolute Error) that assess hemispheric metabolic asymmetry, which is a key part of the clinical analysis of these images. The SGM-KD produced the best qualitative and quantitative results when synthesizing PET purely from T1w and T2 FLAIR images with the least mean absolute error in whole-brain specific uptake value ratio (SUVR) and highest intraclass correlation coefficient. When 1% low-dose PET images are included in the inputs, all models improve significantly and are interchangeable for quantitative performance and visual quality. In summary, SGMs hold great potential for pure MRI-to-PET translation, while all 3 model types can synthesize full-dose FDG-PET accurately using MRI and ultralow-dose PET. </p>
<blockquote>
<p>æ°Ÿè„±æ°§è‘¡è„ç³–ï¼ˆFDGï¼‰PETåœ¨è¯„ä¼°ç™«ç—«æ‚£è€…æ–¹é¢çš„åº”ç”¨æ˜¯æœ€å¸¸è§çš„PET&#x2F;MRIåŒæ­¥åº”ç”¨ä¹‹ä¸€ï¼Œç”±äºéœ€è¦åŒæ—¶æˆåƒè„‘ç»“æ„å’Œä»£è°¢ã€‚ä½†æ˜¯ï¼Œç”±äºå¹´è½»äººç¾¤ä¸­çš„è¾å°„å‰‚é‡é—®é¢˜ï¼Œå…¶æ•ˆæœå¹¶ä¸ç†æƒ³ã€‚ç›®å‰é²œæœ‰ç ”ç©¶åˆ©ç”¨å…ˆè¿›çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ–¹æ³•ï¼ˆå¦‚æ‰©æ•£æ¨¡å‹ï¼‰åˆæˆè¯Šæ–­è´¨é‡çš„PETå›¾åƒæˆ–ç»“åˆMRIæ•°æ®å’Œè¶…ä½å‰‚é‡PETæ•°æ®çš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶é’ˆå¯¹ç™«ç—«äººç¾¤è¿›è¡Œä¸´åºŠè¯„ä¼°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†åŸºäºæ‰©æ•£å’Œéæ‰©æ•£çš„æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨MRIåˆ°PETå›¾åƒè½¬æ¢ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¯¥ä»»åŠ¡ä½¿ç”¨åŒæ­¥PET&#x2F;MRIå¯¹52åç™«ç—«æ‚£è€…ï¼ˆè®­ç»ƒé›†40äººï¼ŒéªŒè¯é›†2äººï¼Œä¿ç•™æµ‹è¯•é›†10äººï¼‰è¿›è¡Œæˆåƒã€‚æˆ‘ä»¬æµ‹è¯•äº†ä¸‰ç§ä¸åŒçš„æ¨¡å‹ï¼šä¸¤ç§åŸºäºåˆ†æ•°çš„ç”Ÿæˆæ‰©æ•£æ¨¡å‹ï¼ˆSGM-Karrasæ‰©æ•£[SGM-KD]å’ŒSGMæ–¹å·®ä¿æŒ[SGM-VP]ï¼‰ä»¥åŠä¸€ä¸ªTransformer-Unetæ¨¡å‹ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†æ ‡å‡†å›¾åƒå¤„ç†æŒ‡æ ‡çš„ç»“æœï¼Œä»¥åŠå…·æœ‰ä¸´åºŠæ„ä¹‰çš„æŒ‡æ ‡ï¼ŒåŒ…æ‹¬è¯„ä¼°åŠçƒä»£è°¢ä¸å¯¹ç§°æ€§çš„ä¸€è‡´æ€§åº¦é‡ï¼ˆä¸€è‡´æŒ‡æ•°å’Œä¸€è‡´æ€§å¹³å‡ç»å¯¹è¯¯å·®ï¼‰ï¼Œè¿™æ˜¯è¿™äº›å›¾åƒä¸´åºŠåˆ†æçš„å…³é”®éƒ¨åˆ†ã€‚å½“ä»…ä½¿ç”¨T1wå’ŒT2 FLAIRå›¾åƒåˆæˆPETæ—¶ï¼ŒSGM-KDäº§ç”Ÿäº†æœ€ä½³çš„ä¸»è§‚å’Œå®¢è§‚ç»“æœï¼Œå…¶åœ¨å…¨è„‘ç‰¹å®šæ‘„å–å€¼æ¯”ç‡ï¼ˆSUVRï¼‰ä¸­çš„å¹³å‡ç»å¯¹è¯¯å·®æœ€å°ï¼Œå¹¶ä¸”åŒç±»ç›¸å…³ç³»æ•°æœ€é«˜ã€‚å½“åŒ…å«1%çš„ä½å‰‚é‡PETå›¾åƒæ—¶ï¼Œæ‰€æœ‰æ¨¡å‹çš„å®šé‡æ€§èƒ½å’Œè§†è§‰è´¨é‡å‡å¾—åˆ°æ˜¾è‘—æé«˜å¹¶å¯äº’æ¢ä½¿ç”¨ã€‚æ€»ä¹‹ï¼ŒSGMsåœ¨çº¯MRIåˆ°PETè½¬æ¢æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ï¼Œè€Œä¸‰ç§æ¨¡å‹ç±»å‹éƒ½å¯ä»¥åˆ©ç”¨MRIå’Œè¶…ä½å‰‚é‡PETå‡†ç¡®åˆæˆå…¨å‰‚é‡FDG-PETå›¾åƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11297v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬ç ”ç©¶ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬æ‰©æ•£æ¨¡å‹å’Œéæ‰©æ•£æ¨¡å‹ï¼Œå¯¹MRI-to-PETå›¾åƒè½¬æ¢ä»»åŠ¡è¿›è¡Œäº†å¯¹æ¯”ç ”ç©¶ã€‚å®éªŒé’ˆå¯¹ç™«ç—«æ‚£è€…ç¾¤ä½“ï¼Œä½¿ç”¨äº†åŒ…å«MRIæ•°æ®å’Œè¶…ä½å‰‚é‡PETæ•°æ®çš„æ ·æœ¬æ•°æ®é›†ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºSGM-KDæ¨¡å‹çš„å›¾åƒåˆæˆæ•ˆæœæœ€ä½³ï¼Œå…·æœ‰æœ€å°çš„å¹³å‡ç»å¯¹è¯¯å·®å’Œæœ€é«˜çš„è¯„åˆ†ä¸€è‡´æ€§ã€‚å½“åŒ…å«è¶…ä½å‰‚é‡PETæ•°æ®æ—¶ï¼Œæ‰€æœ‰æ¨¡å‹çš„å®šé‡æ€§èƒ½å’Œè§†è§‰è´¨é‡å‡æœ‰æ˜¾è‘—æé«˜ã€‚æ‰©æ•£æ¨¡å‹åœ¨çº¯MRIåˆ°PETçš„è½¬æ¢æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿåˆæˆé«˜è´¨é‡çš„PETå›¾åƒã€‚è¿™å¯¹äºå‡å°‘ç™«ç—«æ‚£è€…çš„è¾å°„å‰‚é‡å¹¶æ”¹å–„è¯Šæ–­å›¾åƒè´¨é‡å…·æœ‰é‡è¦æ„ä¹‰ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹å’Œéæ‰©æ•£æ¨¡å‹åœ¨MRIåˆ°PETå›¾åƒè½¬æ¢ä»»åŠ¡ä¸­è¿›è¡Œäº†æ¯”è¾ƒç ”ç©¶ã€‚</li>
<li>å®éªŒé’ˆå¯¹ç™«ç—«æ‚£è€…ç¾¤ä½“è¿›è¡Œï¼Œæ¶‰åŠå¤šç§æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½è¯„ä¼°ã€‚</li>
<li>SGM-KDæ¨¡å‹åœ¨åˆæˆPETå›¾åƒæ–¹é¢è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œå…·æœ‰æœ€å°çš„å¹³å‡ç»å¯¹è¯¯å·®å’Œæœ€é«˜çš„ä¸€è‡´æ€§è¯„åˆ†ã€‚</li>
<li>å½“ç»“åˆè¶…ä½å‰‚é‡PETæ•°æ®æ—¶ï¼Œæ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½å‡æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11297">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c5560987d736e8bc840711c4766ffc46.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-51ee651eb549eac4c3c582e076dc06ab.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-eebeee3db3599f267c3694a019075049.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="DiffPR-Diffusion-Based-Phase-Reconstruction-via-Frequency-Decoupled-Learning"><a href="#DiffPR-Diffusion-Based-Phase-Reconstruction-via-Frequency-Decoupled-Learning" class="headerlink" title="DiffPR: Diffusion-Based Phase Reconstruction via Frequency-Decoupled   Learning"></a>DiffPR: Diffusion-Based Phase Reconstruction via Frequency-Decoupled   Learning</h2><p><strong>Authors:Yi Zhang</strong></p>
<p>Oversmoothing remains a persistent problem when applying deep learning to off-axis quantitative phase imaging (QPI). End-to-end U-Nets favour low-frequency content and under-represent fine, diagnostic detail. We trace this issue to spectral bias and show that the bias is reinforced by high-level skip connections that feed high-frequency features directly into the decoder. Removing those deepest skips thus supervising the network only at a low resolution significantly improves generalisation and fidelity. Building on this insight, we introduce DiffPR, a two-stage frequency-decoupled framework. Stage 1: an asymmetric U-Net with cancelled high-frequency skips predicts a quarter-scale phase map from the interferogram, capturing reliable low-frequency structure while avoiding spectral bias. Stage 2: the upsampled prediction, lightly perturbed with Gaussian noise, is refined by an unconditional diffusion model that iteratively recovers the missing high-frequency residuals through reverse denoising. Experiments on four QPI datasets (B-Cell, WBC, HeLa, 3T3) show that DiffPR outperforms strong U-Net baselines, boosting PSNR by up to 1.1 dB and reducing MAE by 11 percent, while delivering markedly sharper membrane ridges and speckle patterns. The results demonstrate that cancelling high-level skips and delegating detail synthesis to a diffusion prior is an effective remedy for the spectral bias that limits conventional phase-retrieval networks. </p>
<blockquote>
<p>åœ¨æ·±å…‰å­¦åè½´å®šé‡ç›¸ä½æˆåƒï¼ˆQPIï¼‰ä¸­åº”ç”¨æ·±åº¦å­¦ä¹ æ—¶ï¼Œè¿‡å¹³æ»‘ä»ç„¶æ˜¯ä¸€ä¸ªæŒç»­å­˜åœ¨çš„é—®é¢˜ã€‚ç«¯åˆ°ç«¯U-Netå€¾å‘äºä½é¢‘å†…å®¹ï¼Œå¹¶ä¸”æœªèƒ½å……åˆ†è¡¨ç¤ºç²¾ç»†çš„è¯Šæ–­ç»†èŠ‚ã€‚æˆ‘ä»¬è¿½æŸ¥è¿™ä¸ªé—®é¢˜åˆ°å…‰è°±åå·®ï¼Œå¹¶è¡¨æ˜è¿™ç§åå·®æ˜¯ç”±é«˜çº§è·³è·ƒè¿æ¥æ‰€åŠ å¼ºçš„ï¼Œè¿™äº›è¿æ¥ç›´æ¥å°†é«˜é¢‘ç‰¹å¾è¾“å…¥è§£ç å™¨ã€‚å› æ­¤ï¼Œç§»é™¤æœ€æ·±çš„è·³è·ƒè¿æ¥ï¼Œåªåœ¨ä½åˆ†è¾¨ç‡ä¸‹ç›‘ç£ç½‘ç»œï¼Œèƒ½æ˜¾è‘—æ”¹è¿›é€šç”¨æ€§å’Œä¿çœŸåº¦ã€‚åŸºäºè¿™ä¸€è§è§£ï¼Œæˆ‘ä»¬å¼•å…¥äº†DiffPRï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„é¢‘ç‡è§£è€¦æ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µï¼šä¸€ä¸ªå…·æœ‰å–æ¶ˆçš„é«˜é¢‘è·³è·ƒçš„ä¸å¯¹ç§°U-Netä»å¹²æ¶‰å›¾ä¸­é¢„æµ‹å››åˆ†ä¹‹ä¸€è§„æ¨¡çš„ç›¸ä½å›¾ï¼Œæ•æ‰å¯é çš„ä½é¢‘ç»“æ„ï¼ŒåŒæ—¶é¿å…å…‰è°±åå·®ã€‚ç¬¬äºŒé˜¶æ®µï¼šé€šè¿‡é«˜æ–¯å™ªå£°è½»å¾®æ‰°åŠ¨çš„ä¸Šé‡‡æ ·é¢„æµ‹ç»“æœï¼Œç”±æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹è¿›è¡Œç»†åŒ–ï¼Œè¯¥æ¨¡å‹é€šè¿‡åå‘å»å™ªè¿­ä»£æ¢å¤ä¸¢å¤±çš„é«˜é¢‘æ®‹å·®ã€‚åœ¨å››ä¸ªQPIæ•°æ®é›†ï¼ˆBç»†èƒã€WBCã€HeLaã€3T3ï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDiffPRä¼˜äºå¼ºå¤§çš„U-NetåŸºå‡†æµ‹è¯•ï¼Œæé«˜äº†å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰é«˜è¾¾1.1åˆ†è´ï¼Œé™ä½äº†å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰11%ï¼ŒåŒæ—¶æ˜¾è‘—æé«˜äº†è†œè„Šå’Œæ–‘ç‚¹æ¨¡å¼çš„æ¸…æ™°åº¦ã€‚ç»“æœè¡¨æ˜ï¼Œå–æ¶ˆé«˜çº§è·³è·ƒå¹¶å°†ç»†èŠ‚åˆæˆå§”æ‰˜ç»™æ‰©æ•£å…ˆéªŒï¼Œæ˜¯å…‹æœé™åˆ¶ä¼ ç»Ÿç›¸ä½æ£€ç´¢ç½‘ç»œçš„å…‰è°±åå·®çš„æœ‰æ•ˆæ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11183v1">PDF</a> </p>
<p><strong>Summary</strong><br>     é’ˆå¯¹æ·±åº¦å­¦ä¹ åœ¨è½´å¤–å®šé‡ç›¸ä½æˆåƒï¼ˆQPIï¼‰ä¸­çš„åº”ç”¨ï¼Œå­˜åœ¨è¿‡å¹³æ»‘é—®é¢˜ã€‚æˆ‘ä»¬è¿½è¸ªåˆ°é¢‘è°±åè§ï¼Œå¹¶å‘ç°é«˜çº§è·³è·ƒè¿æ¥ä¼šå¼ºåŒ–è¿™ä¸€åè§ã€‚ç§»é™¤æœ€æ·±å±‚æ¬¡çš„è·³è·ƒè¿æ¥ï¼Œä»…åœ¨ä½åˆ†è¾¨ç‡ä¸‹å¯¹ç½‘ç»œè¿›è¡Œç›‘ç£ï¼Œèƒ½æ˜¾è‘—æé«˜æ³›åŒ–èƒ½åŠ›å’Œä¿çœŸåº¦ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†DiffPRï¼Œä¸€ä¸ªä¸¤é˜¶æ®µçš„é¢‘ç‡è§£è€¦æ¡†æ¶ã€‚ç¬¬ä¸€é˜¶æ®µï¼šä¸€ä¸ªå…·æœ‰å–æ¶ˆé«˜é¢‘è·³è·ƒçš„ä¸å¯¹ç§°U-Netï¼Œä»å¹²æ¶‰å›¾é¢„æµ‹å››åˆ†ä¹‹ä¸€å°ºåº¦çš„ç›¸ä½å›¾ï¼Œæ•æ‰å¯é çš„ä½é¢‘ç»“æ„ï¼Œé¿å…é¢‘è°±åè§ã€‚ç¬¬äºŒé˜¶æ®µï¼šé€šè¿‡é«˜æ–¯å™ªå£°è½»å¾®æ‰°åŠ¨ä¸Šé‡‡æ ·é¢„æµ‹å€¼ï¼Œç”±æ— æ¡ä»¶æ‰©æ•£æ¨¡å‹è¿›è¡Œç»†åŒ–ï¼Œé€šè¿‡åå‘å»å™ªè¿­ä»£æ¢å¤ç¼ºå¤±çš„é«˜é¢‘æ®‹å·®ã€‚åœ¨å››ä¸ªQPIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDiffPRä¼˜äºå¼ºå¤§çš„U-NetåŸºå‡†æµ‹è¯•ï¼Œæé«˜äº†å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰è¾¾1.1åˆ†è´ï¼Œå¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰é™ä½11%ï¼ŒåŒæ—¶å‘ˆç°å‡ºæ›´æ¸…æ™°çš„è†œè„Šå’Œæ–‘ç‚¹æ¨¡å¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±åº¦å­¦ä¹ åœ¨è½´å¤–å®šé‡ç›¸ä½æˆåƒä¸­é¢ä¸´è¿‡å¹³æ»‘é—®é¢˜ã€‚</li>
<li>é—®é¢˜æ ¹æºåœ¨äºé¢‘è°±åè§ï¼Œé«˜çº§è·³è·ƒè¿æ¥å¼ºåŒ–äº†è¿™ä¸€åè§ã€‚</li>
<li>ç§»é™¤é«˜çº§è·³è·ƒè¿æ¥å¹¶åªåœ¨ä½åˆ†è¾¨ç‡ä¸‹å¯¹ç½‘ç»œè¿›è¡Œç›‘ç£ï¼Œèƒ½æ”¹å–„ç½‘ç»œæ³›åŒ–èƒ½åŠ›å’Œä¿çœŸåº¦ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªä¸¤é˜¶æ®µçš„é¢‘ç‡è§£è€¦æ¡†æ¶DiffPRæ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ç¬¬ä¸€é˜¶æ®µé¢„æµ‹ä½é¢‘ç‡ç»“æ„ï¼Œé¿å…é¢‘è°±åè§ã€‚</li>
<li>ç¬¬äºŒé˜¶æ®µé€šè¿‡æ‰©æ•£æ¨¡å‹ç»†åŒ–é¢„æµ‹ç»“æœï¼Œæ¢å¤é«˜é¢‘ç»†èŠ‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11183">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-a7c445cbde6a37e5a9a40fc2443f2f18.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a158a0cec59e91ff30ac87e47c250de6.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-189d33de6b4d1b59e2973aa8c9b8b2d2.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dd765282cf2bb4e399fe4eb63027cc1b.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="MMMG-A-Massive-Multidisciplinary-Multi-Tier-Generation-Benchmark-for-Text-to-Image-Reasoning"><a href="#MMMG-A-Massive-Multidisciplinary-Multi-Tier-Generation-Benchmark-for-Text-to-Image-Reasoning" class="headerlink" title="MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for   Text-to-Image Reasoning"></a>MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for   Text-to-Image Reasoning</h2><p><strong>Authors:Yuxuan Luo, Yuhui Yuan, Junwen Chen, Haonan Cai, Ziyi Yue, Yuwei Yang, Fatima Zohra Daha, Ji Li, Zhouhui Lian</strong></p>
<p>In this paper, we introduce knowledge image generation as a new task, alongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation Benchmark (MMMG) to probe the reasoning capability of image generation models. Knowledge images have been central to human civilization and to the mechanisms of human learning â€“ a fact underscored by dual-coding theory and the picture-superiority effect. Generating such images is challenging, demanding multimodal reasoning that fuses world knowledge with pixel-level grounding into clear explanatory visuals. To enable comprehensive evaluation, MMMG offers 4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines, 6 educational levels, and diverse knowledge formats such as charts, diagrams, and mind maps. To eliminate confounding complexity during evaluation, we adopt a unified Knowledge Graph (KG) representation. Each KG explicitly delineates a target imageâ€™s core entities and their dependencies. We further introduce MMMG-Score to evaluate generated knowledge images. This metric combines factual fidelity, measured by graph-edit distance between KGs, with visual clarity assessment. Comprehensive evaluations of 16 state-of-the-art text-to-image generation models expose serious reasoning deficits â€“ low entity fidelity, weak relations, and clutter â€“ with GPT-4o achieving an MMMG-Score of only 50.20, underscoring the benchmarkâ€™s difficulty. To spur further progress, we release FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines a reasoning LLM with diffusion models and is trained on 16,000 curated knowledge image-prompt pairs. </p>
<blockquote>
<p>æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹æ–°çŸ¥è¯†å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œå¹¶ä¼´éšæ¨å‡ºäº†å¤§è§„æ¨¡å¤šå­¦ç§‘å¤šå±‚æ¬¡çŸ¥è¯†å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆMMMGï¼‰ï¼Œä»¥æ¢æµ‹å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚çŸ¥è¯†å›¾åƒåœ¨äººç±»æ–‡æ˜å’Œäººç±»å­¦ä¹ æœºåˆ¶ä¸­å æ®æ ¸å¿ƒåœ°ä½ï¼Œè¿™ä¸€äº‹å®åœ¨åŒé‡ç¼–ç ç†è®ºå’Œå›¾åƒä¼˜åŠ¿æ•ˆåº”ä¸­å¾—åˆ°äº†å¼ºè°ƒã€‚ç”Ÿæˆè¿™æ ·çš„å›¾åƒå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå®ƒè¦æ±‚å¤šæ¨¡æ€æ¨ç†ï¼Œå°†ä¸–ç•ŒçŸ¥è¯†ä¸åƒç´ çº§åœ°é¢èåˆæˆæ¸…æ™°çš„è§£é‡Šæ€§è§†è§‰ã€‚ä¸ºäº†è¿›è¡Œå…¨é¢çš„è¯„ä¼°ï¼ŒMMMGæä¾›äº†4456ä¸ªä¸“å®¶éªŒè¯è¿‡çš„ï¼ˆçŸ¥è¯†ï¼‰å›¾åƒæç¤ºå¯¹ï¼Œæ¶µç›–10ä¸ªå­¦ç§‘ï¼Œ6ä¸ªæ•™è‚²æ°´å¹³ï¼Œä»¥åŠå¤šæ ·åŒ–çš„çŸ¥è¯†å½¢å¼ï¼Œå¦‚å›¾è¡¨ã€å›¾è§£å’Œæ€ç»´å¯¼å›¾ã€‚ä¸ºäº†æ¶ˆé™¤è¯„ä¼°è¿‡ç¨‹ä¸­çš„æ··æ·†å¤æ‚æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨ç»Ÿä¸€çš„çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰è¡¨ç¤ºæ³•ã€‚æ¯ä¸ªçŸ¥è¯†å›¾è°±éƒ½æ˜ç¡®åœ°å‹¾å‹’å‡ºç›®æ ‡å›¾åƒçš„æ ¸å¿ƒå®ä½“åŠå…¶ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†MMMGè¯„åˆ†ï¼Œä»¥è¯„ä¼°ç”Ÿæˆçš„çŸ¥è¯†å›¾åƒã€‚è¯¥æŒ‡æ ‡ç»“åˆäº†äº‹å®å‡†ç¡®æ€§ï¼ˆé€šè¿‡çŸ¥è¯†å›¾è°±ä¹‹é—´çš„å›¾ç¼–è¾‘è·ç¦»æ¥è¡¡é‡ï¼‰å’Œè§†è§‰æ¸…æ™°åº¦è¯„ä¼°ã€‚å¯¹16æ¬¾æœ€å…ˆè¿›çš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å…¨é¢è¯„ä¼°æ­ç¤ºäº†ä¸¥é‡çš„æ¨ç†ç¼ºé™·â€”â€”å®ä½“ä¿çœŸåº¦ä½ã€å…³ç³»å¼±ã€æ‚ä¹±æ— ç« â€”â€”GPT-4oçš„MMMGè¯„åˆ†ä»…ä¸º50.20ï¼Œçªæ˜¾äº†åŸºå‡†æµ‹è¯•çš„å›°éš¾ã€‚ä¸ºäº†ä¿ƒè¿›è¿›ä¸€æ­¥çš„è¿›æ­¥ï¼Œæˆ‘ä»¬å‘å¸ƒäº†FLUX-Reasonï¼ˆMMMGè¯„åˆ†ä¸º34.45ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å¼€æ”¾åŸºçº¿ï¼Œå®ƒç»“åˆäº†æ¨ç†LLMå’Œæ‰©æ•£æ¨¡å‹ï¼Œå¹¶åœ¨16000ä¸ªç²¾é€‰çš„çŸ¥è¯†å›¾åƒæç¤ºå¯¹ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.10963v2">PDF</a> 85 pages, 70 figures, code: <a target="_blank" rel="noopener" href="https://github.com/MMMGBench/MMMG">https://github.com/MMMGBench/MMMG</a>,   project page: <a target="_blank" rel="noopener" href="https://mmmgbench.github.io/">https://mmmgbench.github.io/</a></p>
<p><strong>Summary</strong><br>    æœ¬æ–‡å¼•å…¥çŸ¥è¯†å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œå¹¶æ¨å‡ºå¤§è§„æ¨¡å¤šå­¦ç§‘å¤šçº§çŸ¥è¯†å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆMMMGï¼‰ï¼Œä»¥æ£€æµ‹å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚çŸ¥è¯†å›¾åƒåœ¨äººç±»æ–‡æ˜å’Œæœºåˆ¶å­¦ä¹ ä¸­å æ®æ ¸å¿ƒåœ°ä½ï¼Œç”Ÿæˆæ­¤ç±»å›¾åƒéœ€è¦èåˆå¤šå­¦ç§‘çŸ¥è¯†ï¼Œå¹¶è¿›è¡Œåƒç´ çº§æ¨ç†ã€‚MMMGæä¾›åŒ…å«åä¸ªå­¦ç§‘ã€å…­ä¸ªæ•™è‚²æ°´å¹³çš„ä¸“ä¸šéªŒè¯çŸ¥è¯†å›¾åƒæç¤ºå¯¹ï¼Œå¹¶é‡‡ç”¨ç»Ÿä¸€çš„çŸ¥è¯†å›¾è°±è¡¨ç¤ºè¿›è¡Œç»¼åˆè¯„ä¼°ã€‚åŒæ—¶å¼•å…¥MMMGè¯„åˆ†æ¥è¯„ä¼°ç”Ÿæˆçš„çŸ¥è¯†å›¾åƒè´¨é‡ï¼ŒåŒ…æ‹¬äº‹å®å‡†ç¡®æ€§å’Œè§†è§‰æ¸…æ™°åº¦ã€‚å¯¹ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºå­˜åœ¨æ¨ç†ç¼ºé™·ï¼ŒGPT-4oå¾—åˆ†ä»…ä¸º50.20ï¼Œå‡¸æ˜¾è¯¥åŸºå‡†çš„éš¾åº¦ã€‚ä¸ºæ¨è¿›è¿›æ­¥ï¼Œæ¨å‡ºæœ‰æ•ˆä¸”å¼€æ”¾çš„åŸºçº¿FLUX-Reasonï¼Œç»“åˆæ¨ç†å¤§å‹è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹ï¼Œåœ¨çŸ¥è¯†å›¾åƒæç¤ºå¯¹ä¸Šè¿›è¡Œè®­ç»ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çŸ¥è¯†å›¾åƒç”Ÿæˆè¢«å¼•å…¥ä½œä¸ºæ–°ä»»åŠ¡ï¼Œä¼´éšå¤§è§„æ¨¡å¤šå­¦ç§‘å¤šçº§çŸ¥è¯†å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆMMMGï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°å›¾åƒç”Ÿæˆæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>çŸ¥è¯†å›¾åƒåœ¨äººç±»å­¦ä¹ å’Œæ–‡æ˜ä¸­å æ®æ ¸å¿ƒåœ°ä½ï¼Œç”Ÿæˆè¿™ç±»å›¾åƒéœ€è¦èåˆä¸–ç•ŒçŸ¥è¯†ä¸åƒç´ çº§æ¨ç†ã€‚</li>
<li>MMMGæä¾›æ¶µç›–10ä¸ªå­¦ç§‘ã€6ä¸ªæ•™è‚²æ°´å¹³çš„ä¸“å®¶éªŒè¯çŸ¥è¯†å›¾åƒæç¤ºå¯¹ï¼Œå¹¶é‡‡ç”¨ç»Ÿä¸€çš„çŸ¥è¯†å›¾è°±è¡¨ç¤ºè¿›è¡Œè¯„ä»·ã€‚</li>
<li>MMMGè¯„åˆ†ç»“åˆäº†äº‹å®å‡†ç¡®æ€§ä¸è§†è§‰æ¸…æ™°åº¦çš„è¯„ä¼°ï¼Œæ¥è¯„ä»·ç”Ÿæˆçš„çŸ¥è¯†å›¾åƒè´¨é‡ã€‚</li>
<li>å¯¹ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹çš„å…¨é¢è¯„ä¼°æ˜¾ç¤ºå­˜åœ¨æ¨ç†ç¼ºé™·ï¼Œå¼ºè°ƒè¯¥åŸºå‡†æµ‹è¯•çš„éš¾åº¦ã€‚</li>
<li>GPT-4oåœ¨MMMGè¯„åˆ†ä¸­çš„è¡¨ç°ä¸ä½³ï¼Œå¾—åˆ†ä¸º50.20ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.10963">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-cd1ef773dd5440e1e028ebd89edf7970.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-759b43668dcc73b54c3573d07f8cd979.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-61f5cc35af2995dc9d941893ca19df72.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-45a377a146a57f5f4f3673b420023513.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6417f68225408b87a977c020f398d7f8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-9f2970b49a2b8102ee26dc5923c90338.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Diffuse-Everything-Multimodal-Diffusion-Models-on-Arbitrary-State-Spaces"><a href="#Diffuse-Everything-Multimodal-Diffusion-Models-on-Arbitrary-State-Spaces" class="headerlink" title="Diffuse Everything: Multimodal Diffusion Models on Arbitrary State   Spaces"></a>Diffuse Everything: Multimodal Diffusion Models on Arbitrary State   Spaces</h2><p><strong>Authors:Kevin Rojas, Yuchen Zhu, Sichen Zhu, Felix X. -F. Ye, Molei Tao</strong></p>
<p>Diffusion models have demonstrated remarkable performance in generating unimodal data across various tasks, including image, video, and text generation. On the contrary, the joint generation of multimodal data through diffusion models is still in the early stages of exploration. Existing approaches heavily rely on external preprocessing protocols, such as tokenizers and variational autoencoders, to harmonize varied data representations into a unified, unimodal format. This process heavily demands the high accuracy of encoders and decoders, which can be problematic for applications with limited data. To lift this restriction, we propose a novel framework for building multimodal diffusion models on arbitrary state spaces, enabling native generation of coupled data across different modalities. By introducing an innovative decoupled noise schedule for each modality, we enable both unconditional and modality-conditioned generation within a single model simultaneously. We empirically validate our approach for text-image generation and mixed-type tabular data synthesis, demonstrating that it achieves competitive performance. </p>
<blockquote>
<p>æ‰©æ•£æ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šç”Ÿæˆå•æ¨¡æ€æ•°æ®æ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬ç”Ÿæˆã€‚ç›¸åï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€æ•°æ®çš„è”åˆç”Ÿæˆä»åœ¨æ—©æœŸæ¢ç´¢é˜¶æ®µã€‚ç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºå¤–éƒ¨é¢„å¤„ç†åè®®ï¼Œå¦‚æ ‡è®°å™¨å’Œå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼Œä»¥å°†å„ç§æ•°æ®è¡¨ç¤ºåè°ƒç»Ÿä¸€æˆä¸€ç§å•ä¸€çš„å•æ¨¡æ€æ ¼å¼ã€‚è¿™ä¸€è¿‡ç¨‹å¯¹ç¼–ç å™¨å’Œè§£ç å™¨çš„é«˜å‡†ç¡®æ€§æå‡ºäº†è‹›åˆ»çš„è¦æ±‚ï¼Œå¯¹äºæ•°æ®æœ‰é™çš„åº”ç”¨å¯èƒ½ä¼šå¸¦æ¥é—®é¢˜ã€‚ä¸ºäº†æ¶ˆé™¤è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨ä»»æ„çŠ¶æ€ç©ºé—´ä¸Šæ„å»ºå¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹çš„æ–°å‹æ¡†æ¶ï¼Œèƒ½å¤Ÿå®ç°ä¸åŒæ¨¡æ€çš„è€¦åˆæ•°æ®æœ¬åœ°ç”Ÿæˆã€‚é€šè¿‡å¼•å…¥é’ˆå¯¹æ¯ä¸ªæ¨¡æ€çš„åˆ›æ–°æ€§è§£è€¦å™ªå£°æ—¶é—´è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å•ä¸ªæ¨¡å‹ä¸­åŒæ—¶å®ç°æ— æ¡ä»¶å’Œæ¨¡æ€æ¡ä»¶ç”Ÿæˆã€‚æˆ‘ä»¬é€šè¿‡æ–‡æœ¬å›¾åƒç”Ÿæˆå’Œæ··åˆç±»å‹è¡¨æ ¼æ•°æ®åˆæˆçš„å®è¯ç ”ç©¶éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¯æ˜å…¶æ€§èƒ½å…·æœ‰ç«äº‰åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07903v2">PDF</a> Accepted to ICML 2025. Code available at   <a target="_blank" rel="noopener" href="https://github.com/KevinRojas1499/Diffuse-Everything">https://github.com/KevinRojas1499/Diffuse-Everything</a></p>
<p><strong>æ‘˜è¦</strong></p>
<p>æ‰©æ•£æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºç”Ÿæˆå•æ¨¡æ€æ•°æ®çš„æ˜¾è‘—æ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬ç”Ÿæˆã€‚ç›¸åï¼Œé€šè¿‡æ‰©æ•£æ¨¡å‹è¿›è¡Œå¤šæ¨¡æ€æ•°æ®çš„è”åˆç”Ÿæˆä»å¤„äºæ¢ç´¢çš„æ—©æœŸé˜¶æ®µã€‚ç°æœ‰æ–¹æ³•ä¸¥é‡ä¾èµ–äºå¤–éƒ¨é¢„å¤„ç†åè®®ï¼Œå¦‚æ ‡è®°å™¨å’Œå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼Œä»¥å°†ä¸åŒçš„æ•°æ®è¡¨ç¤ºåè°ƒæˆç»Ÿä¸€ã€å•æ¨¡æ€çš„æ ¼å¼ã€‚è¿™ä¸€è¿‡ç¨‹å¯¹ç¼–ç å™¨å’Œè§£ç å™¨çš„é«˜å‡†ç¡®æ€§æå‡ºäº†å¾ˆé«˜çš„è¦æ±‚ï¼Œå¯¹äºæ•°æ®æœ‰é™çš„åº”ç”¨æ¥è¯´å¯èƒ½æ˜¯ä¸ªé—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åœ¨ä»»æ„çŠ¶æ€ç©ºé—´ä¸Šæ„å»ºå¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹çš„æ–°æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒæ¨¡æ€ä¹‹é—´å®ç°æœ¬åœ°ç”Ÿæˆè€¦åˆæ•°æ®ã€‚é€šè¿‡å¼•å…¥é’ˆå¯¹æ¯ç§æ¨¡æ€çš„åˆ›æ–°è§£è€¦å™ªå£°è°ƒåº¦ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨å•ä¸ªæ¨¡å‹ä¸­åŒæ—¶å®ç°æ— æ¡ä»¶ç”Ÿæˆå’Œæ¨¡æ€æ¡ä»¶ç”Ÿæˆã€‚æˆ‘ä»¬å¯¹æ–‡æœ¬å›¾åƒç”Ÿæˆå’Œæ··åˆç±»å‹è¡¨æ•°æ®çš„åˆæˆè¿›è¡Œäº†å®è¯éªŒè¯ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æ‰©æ•£æ¨¡å‹åœ¨ç”Ÿæˆå•æ¨¡æ€æ•°æ®æ–¹é¢è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼ŒåŒ…æ‹¬å›¾åƒã€è§†é¢‘å’Œæ–‡æœ¬ç”Ÿæˆã€‚</li>
<li>å¤šæ¨¡æ€æ•°æ®çš„è”åˆç”Ÿæˆé€šè¿‡æ‰©æ•£æ¨¡å‹ä»å¤„äºæ—©æœŸæ¢ç´¢é˜¶æ®µã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¾èµ–å¤–éƒ¨é¢„å¤„ç†åè®®æ¥åè°ƒä¸åŒæ•°æ®è¡¨ç¤ºï¼Œè¿™å¯èƒ½å¯¹ç¼ºä¹æ•°æ®çš„åº”ç”¨é€ æˆé—®é¢˜ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ¨¡æ€æ‰©æ•£æ¨¡å‹æ¡†æ¶ï¼Œé€‚ç”¨äºä»»æ„çŠ¶æ€ç©ºé—´ï¼Œèƒ½å¤Ÿåœ¨ä¸åŒæ¨¡æ€ä¹‹é—´å®ç°æœ¬åœ°ç”Ÿæˆè€¦åˆæ•°æ®ã€‚</li>
<li>é€šè¿‡å¼•å…¥é’ˆå¯¹æ¯ç§æ¨¡æ€çš„è§£è€¦å™ªå£°è°ƒåº¦ï¼Œå®ç°äº†æ— æ¡ä»¶ç”Ÿæˆå’Œæ¨¡æ€æ¡ä»¶ç”Ÿæˆã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨æ–‡æœ¬å›¾åƒç”Ÿæˆå’Œæ··åˆç±»å‹è¡¨æ•°æ®åˆæˆæ–¹é¢è¿›è¡Œäº†å®è¯éªŒè¯ã€‚</li>
<li>è¯¥æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ï¼Œèƒ½å¤Ÿåº”å¯¹å„ç§ç”Ÿæˆä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07903">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-4288f59a8a4ad61ab6fea1a02c7eb567.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2d7f223f834e0dd4c843a00d662753e0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-7db5478279a8dfb90036db155b33bf6b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-677c122ea7f02e7e3367e5ddd8f85e3e.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-56535978981b33db7ff4ba4523181a1b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-433c8a3164a187fa11bcd8b5c5723d2b.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Consistent-Video-Editing-as-Flow-Driven-Image-to-Video-Generation"><a href="#Consistent-Video-Editing-as-Flow-Driven-Image-to-Video-Generation" class="headerlink" title="Consistent Video Editing as Flow-Driven Image-to-Video Generation"></a>Consistent Video Editing as Flow-Driven Image-to-Video Generation</h2><p><strong>Authors:Ge Wang, Songlin Fan, Hangxu Liu, Quanjian Song, Hewei Wang, Jinfeng Xu</strong></p>
<p>With the prosper of video diffusion models, down-stream applications like video editing have been significantly promoted without consuming much computational cost. One particular challenge in this task lies at the motion transfer process from the source video to the edited one, where it requires the consideration of the shape deformation in between, meanwhile maintaining the temporal consistency in the generated video sequence. However, existing methods fail to model complicated motion patterns for video editing, and are fundamentally limited to object replacement, where tasks with non-rigid object motions like multi-object and portrait editing are largely neglected. In this paper, we observe that optical flows offer a promising alternative in complex motion modeling, and present FlowV2V to re-investigate video editing as a task of flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V decomposes the entire pipeline into first-frame editing and conditional I2V generation, and simulates pseudo flow sequence that aligns with the deformed shape, thus ensuring the consistency during editing. Experimental results on DAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error illustrate the superior temporal consistency and sample quality of FlowV2V compared to existing state-of-the-art ones. Furthermore, we conduct comprehensive ablation studies to analyze the internal functionalities of the first-frame paradigm and flow alignment in the proposed method. </p>
<blockquote>
<p>éšç€è§†é¢‘æ‰©æ•£æ¨¡å‹çš„ç¹è£ï¼Œä¸‹æ¸¸åº”ç”¨å¦‚è§†é¢‘ç¼–è¾‘å¾—åˆ°äº†æå¤§çš„æ¨åŠ¨ï¼Œè€Œä¸”ä¸éœ€è¦æ¶ˆè€—å¤§é‡çš„è®¡ç®—æˆæœ¬ã€‚åœ¨è¯¥ä»»åŠ¡ä¸­ï¼Œä¸€ä¸ªç‰¹åˆ«çš„æŒ‘æˆ˜åœ¨äºä»æºè§†é¢‘åˆ°ç¼–è¾‘è§†é¢‘çš„åŠ¨æ€è½¬ç§»è¿‡ç¨‹ï¼Œè¿™éœ€è¦è€ƒè™‘ä¸¤è€…ä¹‹é—´çš„å½¢çŠ¶å˜å½¢ï¼ŒåŒæ—¶ä¿æŒç”Ÿæˆè§†é¢‘åºåˆ—çš„æ—¶é—´ä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•æ— æ³•å¯¹è§†é¢‘ç¼–è¾‘ä¸­çš„å¤æ‚è¿åŠ¨æ¨¡å¼è¿›è¡Œå»ºæ¨¡ï¼Œå¹¶ä¸”åŸºæœ¬ä¸Šä»…é™äºå¯¹è±¡æ›¿æ¢ï¼Œå¯¹äºéåˆšæ€§å¯¹è±¡è¿åŠ¨çš„ä»»åŠ¡ï¼Œå¦‚å¤šå¯¹è±¡å’Œäººåƒç¼–è¾‘ï¼Œå´è¢«å¤§å¤§å¿½è§†äº†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å…‰å­¦æµåœ¨å¤æ‚è¿åŠ¨å»ºæ¨¡ä¸­æä¾›äº†æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¹¶æå‡ºFlowV2Væ¥é‡æ–°ç ”ç©¶è§†é¢‘ç¼–è¾‘ä½œä¸ºæµé©±åŠ¨å›¾åƒåˆ°è§†é¢‘ï¼ˆI2Vï¼‰ç”Ÿæˆçš„ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼ŒFlowV2Vå°†æ•´ä¸ªç®¡é“åˆ†è§£ä¸ºç¬¬ä¸€å¸§ç¼–è¾‘å’Œæ¡ä»¶I2Vç”Ÿæˆï¼Œå¹¶æ¨¡æ‹Ÿä¸å˜å½¢å½¢çŠ¶å¯¹é½çš„ä¼ªæµåºåˆ—ï¼Œä»è€Œç¡®ä¿ç¼–è¾‘è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ã€‚åœ¨DAVIS-EDITä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸ç°æœ‰æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”ï¼ŒFlowV2Våœ¨DOVERå’Œwarping errorä¸Šåˆ†åˆ«æé«˜äº†13.67%å’Œ50.66%ï¼Œè¯æ˜äº†å…¶åœ¨æ—¶é—´ä¸€è‡´æ€§å’Œæ ·æœ¬è´¨é‡æ–¹é¢çš„ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œåˆ†æäº†ç¬¬ä¸€å¸§èŒƒå¼å’Œæµå¯¹é½åœ¨æ‰€æå‡ºçš„æ–¹æ³•ä¸­çš„å†…éƒ¨åŠŸèƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07713v2">PDF</a> 16 pages, 12 figures</p>
<p><strong>Summary</strong><br>è§†é¢‘æ‰©æ•£æ¨¡å‹çš„å‘å±•æå¤§åœ°æ¨åŠ¨äº†è§†é¢‘ç¼–è¾‘ç­‰ä¸‹æ¸¸åº”ç”¨çš„å‘å±•ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚è¯¥æ–‡é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å¤æ‚è¿åŠ¨æ¨¡å¼å»ºæ¨¡æ–¹é¢çš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå…‰å­¦æµçš„FlowV2Væ¨¡å‹ï¼Œç”¨äºå¤„ç†å½¢çŠ¶å˜å½¢å’Œæ—¶åºä¸€è‡´æ€§ç­‰æŒ‘æˆ˜ã€‚FlowV2Vå°†è§†é¢‘ç¼–è¾‘ä»»åŠ¡åˆ†è§£ä¸ºå¸§ç¼–è¾‘å’Œæ¡ä»¶I2Vç”Ÿæˆä¸¤ä¸ªæ­¥éª¤ï¼Œæ¨¡æ‹Ÿä¸å˜å½¢å½¢çŠ¶å¯¹é½çš„ä¼ªæµåºåˆ—ã€‚åœ¨DAVIS-EDITä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒFlowV2Våœ¨æ—¶åºä¸€è‡´æ€§å’Œæ ·æœ¬è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜è¿›è¡Œäº†å…¨é¢çš„æ¶ˆèç ”ç©¶ï¼Œåˆ†æäº†è¯¥æ–¹æ³•çš„å†…éƒ¨åŠŸèƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è§†é¢‘æ‰©æ•£æ¨¡å‹ä¿ƒè¿›äº†è§†é¢‘ç¼–è¾‘é¢†åŸŸçš„å‘å±•ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
<li>è§†é¢‘ç¼–è¾‘ä»»åŠ¡é¢ä¸´ä»æºè§†é¢‘åˆ°ç›®æ ‡è§†é¢‘çš„å¤æ‚è¿åŠ¨è½¬ç§»æŒ‘æˆ˜ã€‚éœ€è¦å…¼é¡¾å½¢çŠ¶å˜å½¢å’Œæ—¶é—´åºåˆ—ä¸€è‡´æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥å¤„ç†å¤æ‚è¿åŠ¨æ¨¡å¼å»ºæ¨¡ï¼Œå°¤å…¶é’ˆå¯¹éåˆšä½“å¯¹è±¡è¿åŠ¨çš„ç¼–è¾‘ä»»åŠ¡å—é™ã€‚</li>
<li>å…‰å­¦æµåœ¨å¤æ‚è¿åŠ¨å»ºæ¨¡ä¸­å±•ç°å‡ºæ½œåŠ›ï¼Œè¢«ç”¨äºFlowV2Væ¨¡å‹ã€‚</li>
<li>FlowV2Væ¨¡å‹å°†è§†é¢‘ç¼–è¾‘åˆ†è§£ä¸ºå¸§ç¼–è¾‘å’Œæ¡ä»¶I2Vç”Ÿæˆä¸¤ä¸ªæ­¥éª¤ã€‚</li>
<li>é€šè¿‡æ¨¡æ‹Ÿä¸å˜å½¢å½¢çŠ¶å¯¹é½çš„ä¼ªæµåºåˆ—ï¼ŒFlowV2Vç¡®ä¿ç¼–è¾‘è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨DAVIS-EDITä¸Šçš„å®éªŒéªŒè¯äº†FlowV2Væ¨¡å‹çš„ä¼˜åŠ¿ï¼Œåœ¨æ—¶åºä¸€è‡´æ€§å’Œæ ·æœ¬è´¨é‡æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3e2f0bf75b9818b3b4e984be03088b3c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-394d391dfd4939fe5003853a37493ea8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9cb97e470f32a8ec5055535fc713607d.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Ming-Lite-Uni-Advancements-in-Unified-Architecture-for-Natural-Multimodal-Interaction"><a href="#Ming-Lite-Uni-Advancements-in-Unified-Architecture-for-Natural-Multimodal-Interaction" class="headerlink" title="Ming-Lite-Uni: Advancements in Unified Architecture for Natural   Multimodal Interaction"></a>Ming-Lite-Uni: Advancements in Unified Architecture for Natural   Multimodal Interaction</h2><p><strong>Authors:Inclusion AI, Biao Gong, Cheng Zou, Dandan Zheng, Hu Yu, Jingdong Chen, Jianxin Sun, Junbo Zhao, Jun Zhou, Kaixiang Ji, Lixiang Ru, Libin Wang, Qingpei Guo, Rui Liu, Weilong Chai, Xinyu Xiao, Ziyuan Huang</strong></p>
<p>We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†Ming-Lite-Uniï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ¨¡å¼æ¡†æ¶ï¼Œå…·æœ‰æ–°è®¾è®¡çš„ç»Ÿä¸€è§†è§‰ç”Ÿæˆå™¨å’Œé’ˆå¯¹è§†è§‰å’Œè¯­è¨€çš„ç»Ÿä¸€æœ¬åœ°å›¾å±‚è‡ªé€‚åº”æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼Œæ­¤é¡¹ç›®æä¾›äº†é›†æˆMetaQuerieså’ŒM2-omniæ¡†æ¶çš„å¼€æºå®ç°ï¼ŒåŒæ—¶å¼•å…¥äº†æ–°å‹çš„å¤šå°ºåº¦å¯å­¦ä¹ ä»¤ç‰Œå’Œå¤šå°ºåº¦è¡¨ç¤ºå¯¹é½ç­–ç•¥ã€‚é€šè¿‡åˆ©ç”¨å›ºå®šçš„MLLMå’Œå¯å­¦ä¹ çš„æ‰©æ•£æ¨¡å‹ï¼ŒMing-Lite-Uniä½¿æœ¬åœ°å¤šæ¨¡å¼ARæ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’ŒåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œæ‰©å±•äº†å…¶è¶…è¶Šçº¯è§†è§‰ç†è§£çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¯æ˜äº†Ming-Lite-Uniçš„å¼ºå¤§æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†å…¶äº¤äº’è¿‡ç¨‹çš„æµç•…æ€§ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹æƒé‡å‡å·²å¼€æºï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºå†…çš„è¿›ä¸€æ­¥æ¢ç´¢ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™é¡¹å·¥ä½œä¸å½“å‰çš„å¤šåª’ä½“äººå·¥æ™ºèƒ½é‡Œç¨‹ç¢‘ä¿æŒä¸€è‡´ï¼Œå¦‚ChatGPT-4åœ¨2025å¹´3æœˆ25æ—¥æ›´æ–°çš„æœ¬åœ°å›¾åƒç”ŸæˆåŠŸèƒ½ï¼Œå¼ºè°ƒäº†åƒMing-Lite-Uniè¿™æ ·çš„ç»Ÿä¸€æ¨¡å‹åœ¨é€šå¾€äººå·¥æ™ºèƒ½é€šç”¨åŒ–é“è·¯ä¸Šçš„é‡è¦æ€§ã€‚Ming-Lite-Uniç›®å‰å¤„äºAlphaé˜¶æ®µï¼Œæœªæ¥å°†ä¼šè¿›ä¸€æ­¥å®Œå–„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.02471v3">PDF</a> <a target="_blank" rel="noopener" href="https://github.com/inclusionAI/Ming/tree/Ming-Lite-Omni-Preview/Ming-unify">https://github.com/inclusionAI/Ming/tree/Ming-Lite-Omni-Preview/Ming-unify</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Ming-Lite-Uniè¿™ä¸€å¼€æºå¤šæ¨¡æ€æ¡†æ¶ï¼Œå…¶ç‰¹ç‚¹ä¸ºå…¨æ–°è®¾è®¡çš„ç»Ÿä¸€è§†è§‰ç”Ÿæˆå™¨ä»¥åŠé’ˆå¯¹è§†è§‰å’Œè¯­è¨€ç»Ÿä¸€åŒ–çš„æœ¬åœ°å¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹ã€‚è¯¥é¡¹ç›®å®ç°äº†é›†æˆMetaQuerieså’ŒM2-omniæ¡†æ¶çš„å¼€æºå®ç°ï¼Œå¹¶å¼•å…¥æ–°å‹å¤šå°ºåº¦å¯å­¦ä¹ ä»¤ç‰Œå’Œå¤šå°ºåº¦è¡¨ç¤ºå¯¹é½ç­–ç•¥ã€‚Ming-Lite-Uniåˆ©ç”¨å›ºå®šçš„MLLMå’Œå¯å­¦ä¹ çš„æ‰©æ•£æ¨¡å‹ï¼Œä½¿æœ¬åœ°å¤šæ¨¡æ€ARæ¨¡å‹èƒ½å¤Ÿæ‰§è¡Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’ŒåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ï¼Œè¶…è¶Šäº†çº¯è§†è§‰ç†è§£çš„èƒ½åŠ›ã€‚å®éªŒç»“æœè¯æ˜äº†Ming-Lite-Uniçš„å¼ºå¤§æ€§èƒ½ï¼Œå…¶äº¤äº’è¿‡ç¨‹è¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æµç•…æ€§ã€‚æ‰€æœ‰ä»£ç å’Œæ¨¡å‹æƒé‡å‡å·²å¼€æºï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºå†…çš„è¿›ä¸€æ­¥æ¢ç´¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ming-Lite-Uniæ˜¯ä¸€ä¸ªå¼€æºå¤šæ¨¡æ€æ¡†æ¶ï¼Œé›†è§†è§‰å’Œè¯­è¨€äºä¸€ä½“ã€‚</li>
<li>æ¡†æ¶åŒ…å«æ–°è®¾è®¡çš„ç»Ÿä¸€è§†è§‰ç”Ÿæˆå™¨å’Œæœ¬åœ°å¤šæ¨¡æ€è‡ªå›å½’æ¨¡å‹ã€‚</li>
<li>å®ç°äº†MetaQuerieså’ŒM2-omniæ¡†æ¶çš„é›†æˆã€‚</li>
<li>å¼•å…¥å¤šå°ºåº¦å¯å­¦ä¹ ä»¤ç‰Œå’Œå¤šå°ºåº¦è¡¨ç¤ºå¯¹é½ç­–ç•¥ã€‚</li>
<li>åˆ©ç”¨å›ºå®šçš„MLLMå’Œå¯å­¦ä¹ çš„æ‰©æ•£æ¨¡å‹ï¼Œæ”¯æŒæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå’ŒåŸºäºæŒ‡ä»¤çš„å›¾åƒç¼–è¾‘ä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœå±•ç¤ºäº†Ming-Lite-Uniçš„å¼ºå¤§æ€§èƒ½ï¼Œå…¶äº¤äº’è¿‡ç¨‹æµç•…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.02471">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-ad78cbf614f113a89e1d1d4e16554511.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-161e89071d69ff9ea19ef921d96c9afe.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-4250c4cfea42b43946ce25636b1e44a1.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-78bfbdb8ba9e13c48bf6b219ee0631ff.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-29c7288bc0de2e28b2c88cd12d144e21.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d019031236945e165e3541d7f4f6b1a7.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-42a1f3b214677632ab4fdea492bbbf71.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="3D-WAG-Hierarchical-Wavelet-Guided-Autoregressive-Generation-for-High-Fidelity-3D-Shapes"><a href="#3D-WAG-Hierarchical-Wavelet-Guided-Autoregressive-Generation-for-High-Fidelity-3D-Shapes" class="headerlink" title="3D-WAG: Hierarchical Wavelet-Guided Autoregressive Generation for   High-Fidelity 3D Shapes"></a>3D-WAG: Hierarchical Wavelet-Guided Autoregressive Generation for   High-Fidelity 3D Shapes</h2><p><strong>Authors:Tejaswini Medi, Arianna Rampini, Pradyumna Reddy, Pradeep Kumar Jayaraman, Margret Keuper</strong></p>
<p>Autoregressive (AR) models have achieved remarkable success in natural language and image generation, but their application to 3D shape modeling remains largely unexplored. Unlike diffusion models, AR models enable more efficient and controllable generation with faster inference times, making them especially suitable for data-intensive domains. Traditional 3D generative models using AR approaches often rely on <code>next-token&quot; predictions at the voxel or point level. While effective for certain applications, these methods can be restrictive and computationally expensive when dealing with large-scale 3D data. To tackle these challenges, we introduce 3D-WAG, an AR model for 3D implicit distance fields that can perform unconditional shape generation, class-conditioned and also text-conditioned shape generation. Our key idea is to encode shapes as multi-scale wavelet token maps and use a Transformer to predict the </code>next higher-resolution token mapâ€ in an autoregressive manner. By redefining 3D AR generation task as <code>next-scale&quot; prediction, we reduce the computational cost of generation compared to traditional </code>next-tokenâ€ prediction models, while preserving essential geometric details of 3D shapes in a more structured and hierarchical manner. We evaluate 3D-WAG to showcase its benefit by quantitative and qualitative comparisons with state-of-the-art methods on widely used benchmarks. Our results show 3D-WAG achieves superior performance in key metrics like Coverage and MMD, generating high-fidelity 3D shapes that closely match the real data distribution. </p>
<blockquote>
<p>è‡ªå›å½’ï¼ˆARï¼‰æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å’Œå›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆå°±ï¼Œä½†åœ¨3Då½¢çŠ¶å»ºæ¨¡æ–¹é¢çš„åº”ç”¨ä»å¾…æ¢ç´¢ã€‚ä¸åŒäºæ‰©æ•£æ¨¡å‹ï¼ŒARæ¨¡å‹èƒ½å¤Ÿå®ç°æ›´é«˜æ•ˆå¯æ§çš„ç”Ÿæˆï¼Œå…·æœ‰æ›´å¿«çš„æ¨ç†æ—¶é—´ï¼Œä½¿å…¶ç‰¹åˆ«é€‚åˆæ•°æ®å¯†é›†å‹é¢†åŸŸã€‚ä¼ ç»Ÿçš„é‡‡ç”¨ARæ–¹æ³•çš„3Dç”Ÿæˆæ¨¡å‹é€šå¸¸ä¾èµ–äºä½“ç´ æˆ–ç‚¹çº§çš„â€œä¸‹ä¸€ä¸ªä»¤ç‰Œâ€é¢„æµ‹ã€‚è™½ç„¶è¿™åœ¨æŸäº›åº”ç”¨ä¸­å¾ˆæœ‰æ•ˆï¼Œä½†åœ¨å¤„ç†å¤§è§„æ¨¡3Dæ•°æ®æ—¶ï¼Œè¿™äº›æ–¹æ³•å¯èƒ½ä¼šå—åˆ°é™åˆ¶å¹¶ä¸”è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†3D-WAGï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº3Déšè·ç¦»åœºçš„ARæ¨¡å‹ï¼Œå¯ä»¥æ‰§è¡Œæ— æ¡ä»¶å½¢çŠ¶ç”Ÿæˆã€ç±»åˆ«æ¡ä»¶å½¢çŠ¶ç”Ÿæˆå’Œæ–‡æœ¬æ¡ä»¶å½¢çŠ¶ç”Ÿæˆã€‚æˆ‘ä»¬çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å½¢çŠ¶ç¼–ç ä¸ºå¤šå°ºåº¦å°æ³¢ä»¤ç‰Œå›¾ï¼Œå¹¶ä½¿ç”¨Transformerä»¥è‡ªå›å½’çš„æ–¹å¼é¢„æµ‹â€œä¸‹ä¸€ä¸ªæ›´é«˜åˆ†è¾¨ç‡çš„ä»¤ç‰Œå›¾â€ã€‚é€šè¿‡é‡æ–°å°†3DARç”Ÿæˆä»»åŠ¡å®šä¹‰ä¸ºâ€œä¸‹ä¸€ä¸ªå°ºåº¦â€é¢„æµ‹ï¼Œæˆ‘ä»¬ä¸ä¼ ç»Ÿçš„â€œä¸‹ä¸€ä¸ªä»¤ç‰Œâ€é¢„æµ‹æ¨¡å‹ç›¸æ¯”ï¼Œé™ä½äº†ç”Ÿæˆçš„è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä»¥æ›´ç»“æ„å’Œåˆ†å±‚çš„æ–¹å¼ä¿ç•™äº†3Då½¢çŠ¶çš„åŸºæœ¬å‡ ä½•ç»†èŠ‚ã€‚æˆ‘ä»¬é€šè¿‡å®šé‡å’Œå®šæ€§çš„æ–¹æ³•è¯„ä¼°äº†3D-WAGä¸æœ€å…ˆè¿›çš„æ–¹æ³•åœ¨å¹¿æ³›ä½¿ç”¨åŸºå‡†æµ‹è¯•ä¸Šçš„æ¯”è¾ƒï¼Œä»¥å±•ç¤ºå…¶ä¼˜åŠ¿ã€‚ç»“æœè¡¨æ˜ï¼Œ3D-WAGåœ¨è¦†ç›–ç‡å’ŒMMDç­‰å…³é”®æŒ‡æ ‡ä¸Šè¡¨ç°ä¼˜è¶Šï¼Œç”Ÿæˆçš„é«˜ä¿çœŸ3Då½¢çŠ¶ä¸çœŸå®æ•°æ®åˆ†å¸ƒç´§å¯†åŒ¹é…ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2411.19037v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹ä¸‰ç»´éšè·ç¦»åœºçš„è‡ªå›å½’æ¨¡å‹3D-WAGï¼Œè¯¥æ¨¡å‹å¯è¿›è¡Œæ— æ¡ä»¶å½¢çŠ¶ç”Ÿæˆã€ç±»åˆ«æ¡ä»¶å½¢çŠ¶ç”Ÿæˆå’Œæ–‡æœ¬æ¡ä»¶å½¢çŠ¶ç”Ÿæˆã€‚é€šè¿‡ç¼–ç å½¢çŠ¶ä¸ºå¤šçº§å°æ³¢æ ‡è®°å›¾ï¼Œå¹¶åˆ©ç”¨Transformerä»¥è‡ªå›å½’æ–¹å¼é¢„æµ‹â€œä¸‹ä¸€ä¸ªæ›´é«˜åˆ†è¾¨ç‡çš„æ ‡è®°å›¾â€ï¼Œè¯¥æ¨¡å‹é™ä½äº†ç”Ÿæˆçš„è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿ç•™äº†ä¸‰ç»´å½¢çŠ¶çš„å‡ ä½•ç»†èŠ‚ã€‚åœ¨å¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ä¸­ä¸æœ€å…ˆè¿›çš„æ–¹æ³•è¿›è¡Œäº†å®šé‡å’Œå®šæ€§çš„æ¯”è¾ƒï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨å…³é”®æŒ‡æ ‡å¦‚è¦†ç›–ç‡å’ŒMMDä¸Šï¼Œ3D-WAGå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œç”Ÿæˆçš„ä¸‰ç»´å½¢çŠ¶ä¸çœŸå®æ•°æ®åˆ†å¸ƒé«˜åº¦åŒ¹é…ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ARæ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å’Œå›¾åƒç”Ÿæˆæ–¹é¢å–å¾—äº†æ˜¾è‘—æˆåŠŸï¼Œä½†åœ¨3Då½¢çŠ¶å»ºæ¨¡æ–¹é¢çš„åº”ç”¨å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚</li>
<li>ä¼ ç»Ÿçš„åŸºäºARæ–¹æ³•çš„3Dç”Ÿæˆæ¨¡å‹é€šå¸¸ä¾èµ–äºâ€œä¸‹ä¸€ä¸ªæ ‡è®°â€é¢„æµ‹ï¼Œåœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶å¯èƒ½å—åˆ°é™åˆ¶ã€‚</li>
<li>3D-WAGæ¨¡å‹æ˜¯ä¸€ç§é’ˆå¯¹ä¸‰ç»´éšè·ç¦»åœºçš„è‡ªå›å½’æ¨¡å‹ï¼Œé€šè¿‡ç¼–ç å½¢çŠ¶ä¸ºå¤šçº§å°æ³¢æ ‡è®°å›¾è¿›è¡Œé¢„æµ‹ï¼Œé™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
<li>3D-WAGæ¨¡å‹å¯å®ç°æ— æ¡ä»¶ã€ç±»åˆ«æ¡ä»¶å’Œæ–‡æœ¬æ¡ä»¶çš„å½¢çŠ¶ç”Ÿæˆã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œ3D-WAGåœ¨è¦†ç›–ç‡å’ŒMMDç­‰å…³é”®æŒ‡æ ‡ä¸Šå®ç°äº†å“è¶Šæ€§èƒ½ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.19037">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-c15f9a708ace923c3d8db5657bc94df8.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e430574b53c071e76c381ec71aeaeebf.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b87c6f348cb4686b1965867da757f4e8.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-ec9d9d5579277c0a9f165fe81ab7cab1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-17/Diffusion%20Models/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-17/Diffusion%20Models/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Diffusion-Models/">
                                    <span class="chip bg-color">Diffusion Models</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-17/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-9cefe4fc38396dae12cc5162b9e929ca.jpg" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-17  Simple Radiology VLLM Test-time Scaling with Thought Graph Traversal
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-17/NeRF/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-75474039f5f3863b21ed57c708f2f1c2.jpg" class="responsive-img" alt="NeRF">
                        
                        <span class="card-title">NeRF</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            NeRF æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-17  On Apparent Absence of Green Gap in InGaN/GaN Quantum Disks and Wells   Grown by Plasma-Assisted Molecular Beam Epitaxy
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/NeRF/" class="post-category">
                                    NeRF
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/NeRF/">
                        <span class="chip bg-color">NeRF</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">23523.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
