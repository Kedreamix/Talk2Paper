<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-17  Improving Large Language Model Safety with Contrastive Representation   Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4a6aaa3f7cc4fed9931200e40748b8d7.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-06-17
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-07-06
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    84 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-06-17-æ›´æ–°"><a href="#2025-06-17-æ›´æ–°" class="headerlink" title="2025-06-17 æ›´æ–°"></a>2025-06-17 æ›´æ–°</h1><h2 id="Improving-Large-Language-Model-Safety-with-Contrastive-Representation-Learning"><a href="#Improving-Large-Language-Model-Safety-with-Contrastive-Representation-Learning" class="headerlink" title="Improving Large Language Model Safety with Contrastive Representation   Learning"></a>Improving Large Language Model Safety with Contrastive Representation   Learning</h2><p><strong>Authors:Samuel Simko, Mrinmaya Sachan, Bernhard SchÃ¶lkopf, Zhijing Jin</strong></p>
<p>Large Language Models (LLMs) are powerful tools with profound societal impacts, yet their ability to generate responses to diverse and uncontrolled inputs leaves them vulnerable to adversarial attacks. While existing defenses often struggle to generalize across varying attack types, recent advancements in representation engineering offer promising alternatives. In this work, we propose a defense framework that formulates model defense as a contrastive representation learning (CRL) problem. Our method finetunes a model using a triplet-based loss combined with adversarial hard negative mining to encourage separation between benign and harmful representations. Our experimental results across multiple models demonstrate that our approach outperforms prior representation engineering-based defenses, improving robustness against both input-level and embedding-space attacks without compromising standard performance. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/samuelsimko/crl-llm-defense">https://github.com/samuelsimko/crl-llm-defense</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å…·æœ‰æ·±è¿œç¤¾ä¼šå½±å“çš„å¼ºå¤§å·¥å…·ï¼Œä½†å®ƒä»¬å¯¹å¤šæ ·åŒ–å’Œéæ§åˆ¶è¾“å…¥çš„å“åº”èƒ½åŠ›ä½¿å…¶å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»ã€‚è™½ç„¶ç°æœ‰çš„é˜²å¾¡æ‰‹æ®µå¾€å¾€éš¾ä»¥åœ¨å¤šç§æ”»å‡»ç±»å‹ä¸­æ¨å¹¿ï¼Œä½†è¡¨ç¤ºå·¥ç¨‹æ–¹é¢çš„æœ€æ–°è¿›å±•æä¾›äº†æœ‰å¸Œæœ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé˜²å¾¡æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å°†æ¨¡å‹é˜²å¾¡åˆ¶å®šä¸ºä¸€ä¸ªå¯¹æ¯”è¡¨ç¤ºå­¦ä¹ ï¼ˆCRLï¼‰é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä½¿ç”¨åŸºäºä¸‰å…ƒç»„çš„æŸå¤±ä¸å¯¹æŠ—æ€§ç¡¬è´ŸæŒ–æ˜ç›¸ç»“åˆæ¥å¾®è°ƒæ¨¡å‹ï¼Œä»¥é¼“åŠ±è‰¯æ€§è¡¨ç¤ºå’Œæœ‰å®³è¡¨ç¤ºä¹‹é—´çš„åˆ†ç¦»ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ¨¡å‹ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¼˜äºåŸºäºè¡¨ç¤ºå·¥ç¨‹çš„å…ˆå‰é˜²å¾¡æ‰‹æ®µï¼Œæé«˜äº†å¯¹è¾“å…¥çº§åˆ«å’ŒåµŒå…¥ç©ºé—´æ”»å‡»çš„ç¨³å¥æ€§ï¼ŒåŒæ—¶ä¸æŸå®³æ ‡å‡†æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/samuelsimko/crl-llm-defense%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/samuelsimko/crl-llm-defenseæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11938v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>LLMæ¨¡å‹å¯¹ç¤¾ä¼šæœ‰æ·±è¿œå½±å“ï¼Œä½†å…¶ç”Ÿæˆå¤šæ ·åŒ–ä¸”ä¸å¯æ§è¾“å…¥çš„å“åº”ä½¿å…¶æ˜“å—å¯¹æŠ—æ€§æ”»å‡»ã€‚ç°æœ‰é˜²å¾¡ç­–ç•¥å¾€å¾€éš¾ä»¥åœ¨ä¸åŒæ”»å‡»ç±»å‹ä¹‹é—´å®ç°æ³›åŒ–ï¼Œè€Œæœ€è¿‘çš„è¡¨ç¤ºå·¥ç¨‹è¿›å±•æä¾›äº†æœ‰å¸Œæœ›çš„æ›¿ä»£æ–¹æ¡ˆã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é˜²å¾¡æ¡†æ¶ï¼Œå°†æ¨¡å‹é˜²å¾¡è¡¨è¿°ä¸ºå¯¹æ¯”è¡¨ç¤ºå­¦ä¹ ï¼ˆCRLï¼‰é—®é¢˜ã€‚è¯¥æ–¹æ³•ä½¿ç”¨åŸºäºä¸‰å…ƒç»„çš„æŸå¤±ç»“åˆå¯¹æŠ—æ€§ç¡¬è´ŸæŒ–æ˜æ¥å¾®è°ƒæ¨¡å‹ï¼Œé¼“åŠ±è‰¯æ€§è¡¨ç¤ºä¸æœ‰å®³è¡¨ç¤ºä¹‹é—´çš„åˆ†ç¦»ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºåŸºäºè¡¨ç¤ºå·¥ç¨‹çš„å…ˆå‰é˜²å¾¡ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ä¸æŸå®³æ ‡å‡†æ€§èƒ½çš„æƒ…å†µä¸‹æé«˜é’ˆå¯¹è¾“å…¥çº§åˆ«å’ŒåµŒå…¥ç©ºé—´æ”»å‡»çš„ç¨³å¥æ€§ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/samuelsimko/crl-llm-defense%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/samuelsimko/crl-llm-defenseæ‰¾åˆ°ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMæ¨¡å‹å› å…¶ç”Ÿæˆå¤šæ ·åŒ–å“åº”çš„èƒ½åŠ›è€Œå…·æœ‰ç¤¾ä¼šå½±å“åŠ›ï¼Œä½†ä¹Ÿå®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»ã€‚</li>
<li>ç°æœ‰é˜²å¾¡ç­–ç•¥åœ¨æ³›åŒ–æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦æ–°çš„è§£å†³æ–¹æ¡ˆã€‚</li>
<li>å¯¹æ¯”è¡¨ç¤ºå­¦ä¹ ï¼ˆCRLï¼‰è¢«æå‡ºä½œä¸ºä¸€ç§æ–°çš„é˜²å¾¡æ¡†æ¶ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡å¾®è°ƒæ¨¡å‹å¹¶ä½¿ç”¨åŸºäºä¸‰å…ƒç»„çš„æŸå¤±ç»“åˆå¯¹æŠ—æ€§ç¡¬è´ŸæŒ–æ˜ï¼Œæ¥åˆ†ç¦»è‰¯æ€§è¡¨ç¤ºå’Œæœ‰å®³è¡¨ç¤ºã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æé«˜äº†æ¨¡å‹é’ˆå¯¹è¾“å…¥çº§åˆ«å’ŒåµŒå…¥ç©ºé—´æ”»å‡»çš„ç¨³å¥æ€§ã€‚</li>
<li>è¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜äºä¹‹å‰çš„è¡¨ç¤ºå·¥ç¨‹é˜²å¾¡ç­–ç•¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11938">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-3cc09966903a7d04156ea9f4c819cc43.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e8b1c078cc02cdf6b5760fe2bc3777b6.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-5ead4d9d156c9edf2c292dc3198e80d5.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Feedback-Friction-LLMs-Struggle-to-Fully-Incorporate-External-Feedback"><a href="#Feedback-Friction-LLMs-Struggle-to-Fully-Incorporate-External-Feedback" class="headerlink" title="Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback"></a>Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback</h2><p><strong>Authors:Dongwei Jiang, Alvin Zhang, Andrew Wang, Nicholas Andrews, Daniel Khashabi</strong></p>
<p>Recent studies have shown LLMs possess some ability to improve their responses when given external feedback. However, it remains unclear how effectively and thoroughly these models can incorporate extrinsic feedback. In an ideal scenario, if LLMs receive near-perfect and complete feedback, we would expect them to fully integrate the feedback and change their incorrect answers to correct ones. In this paper, we systematically investigate LLMsâ€™ ability to incorporate feedback by designing a controlled experimental environment. For each problem, a solver model attempts a solution, then a feedback generator with access to near-complete ground-truth answers produces targeted feedback, after which the solver tries again. We evaluate this pipeline across a diverse range of tasks, including math reasoning, knowledge reasoning, scientific reasoning, and general multi-domain evaluations with state-of-the-art language models including Claude 3.7 (with and without extended thinking). Surprisingly, even under these near-ideal conditions, solver models consistently show resistance to feedback, a limitation that we term FEEDBACK FRICTION. To mitigate this limitation, we experiment with sampling-based strategies like progressive temperature increases and explicit rejection of previously attempted incorrect answers, which yield improvements but still fail to help models achieve target performance. We also perform a rigorous exploration of potential causes of FEEDBACK FRICTION, ruling out factors such as model overconfidence and data familiarity. We hope that highlighting this issue in LLMs and ruling out several apparent causes will help future research in self-improvement. </p>
<blockquote>
<p>æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¥æ”¶åˆ°å¤–éƒ¨åé¦ˆåï¼Œèƒ½å¤Ÿæ”¹å–„å…¶å›åº”èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¦‚ä½•æœ‰æ•ˆå¹¶å…¨é¢åœ°èå…¥å¤–åœ¨åé¦ˆä»ç„¶ä¸æ˜ç¡®ã€‚åœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œå¦‚æœå¤§å‹è¯­è¨€æ¨¡å‹æ”¶åˆ°è¿‘ä¹å®Œç¾ä¸”å®Œæ•´çš„åé¦ˆï¼Œæˆ‘ä»¬æœŸæœ›å®ƒä»¬èƒ½å¤Ÿå®Œå…¨æ•´åˆåé¦ˆå¹¶æ›´æ­£é”™è¯¯çš„ç­”æ¡ˆã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è®¾è®¡ä¸€ä¸ªå—æ§çš„å®éªŒç¯å¢ƒæ¥ç³»ç»Ÿç ”ç©¶å¤§å‹è¯­è¨€æ¨¡å‹èå…¥åé¦ˆçš„èƒ½åŠ›ã€‚é’ˆå¯¹æ¯ä¸ªé—®é¢˜ï¼Œæ±‚è§£æ¨¡å‹å…ˆå°è¯•è§£ç­”ï¼Œç„¶åä¸€ä¸ªèƒ½å¤Ÿè®¿é—®è¿‘ä¹å®Œæ•´çš„çœŸå®ç­”æ¡ˆçš„åé¦ˆç”Ÿæˆå™¨äº§ç”Ÿæœ‰é’ˆå¯¹æ€§çš„åé¦ˆï¼Œä¹‹åæ±‚è§£æ¨¡å‹å†æ¬¡å°è¯•ã€‚æˆ‘ä»¬åœ¨å„ç§ä»»åŠ¡ä¸­è¯„ä¼°äº†è¿™ä¸ªæµç¨‹ï¼ŒåŒ…æ‹¬æ•°å­¦æ¨ç†ã€çŸ¥è¯†æ¨ç†ã€ç§‘å­¦æ¨ç†å’ŒåŒ…å«Claude 3.7ï¼ˆå¸¦å’Œä¸å¸¦æ‰©å±•æ€è€ƒï¼‰çš„æœ€æ–°è¯­è¨€æ¨¡å‹çš„ä¸€èˆ¬å¤šåŸŸè¯„ä¼°ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå³ä½¿åœ¨è¿‘ä¹ç†æƒ³çš„æ¡ä»¶ä¸‹ï¼Œæ±‚è§£æ¨¡å‹å§‹ç»ˆæ˜¾ç¤ºå‡ºå¯¹åé¦ˆçš„æŠµæŠ—ï¼Œè¿™æ˜¯æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œåé¦ˆæ‘©æ“¦åŠ›â€çš„é™åˆ¶ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é™åˆ¶ï¼Œæˆ‘ä»¬å°è¯•é‡‡ç”¨åŸºäºé‡‡æ ·çš„ç­–ç•¥ï¼Œå¦‚é€æ­¥å¢åŠ æ¸©åº¦å€¼å’Œæ˜¾å¼æ‹’ç»ä¹‹å‰å°è¯•è¿‡çš„é”™è¯¯ç­”æ¡ˆï¼Œè™½ç„¶è¿™æœ‰æ‰€æ”¹å–„ï¼Œä½†ä»æœªèƒ½å¸®åŠ©æ¨¡å‹è¾¾åˆ°ç›®æ ‡æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å¯¹â€œåé¦ˆæ‘©æ“¦åŠ›â€çš„æ½œåœ¨åŸå› è¿›è¡Œäº†ä¸¥æ ¼çš„æ¢ç´¢ï¼Œæ’é™¤äº†æ¨¡å‹è¿‡åº¦è‡ªä¿¡å’Œæ•°æ®ç†Ÿæ‚‰åº¦ç­‰å› ç´ ã€‚æˆ‘ä»¬å¸Œæœ›çªå‡ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¿™ä¸€é—®é¢˜ï¼Œå¹¶æ’é™¤å‡ ä¸ªæ˜æ˜¾çš„åŸå› ï¼Œä»¥å¸®åŠ©æœªæ¥çš„è‡ªæˆ‘æ”¹è¿›ç ”ç©¶ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11930v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒ½å¤Ÿåœ¨ç»™å®šå¤–éƒ¨åé¦ˆæ—¶æ”¹è¿›å…¶å“åº”ï¼Œä½†å¦‚ä½•æœ‰æ•ˆã€å…¨é¢åœ°èå…¥å¤–åœ¨åé¦ˆå°šä¸æ¸…æ¥šã€‚æœ¬ç ”ç©¶é€šè¿‡è®¾è®¡å—æ§å®éªŒç¯å¢ƒï¼Œç³»ç»Ÿæ¢è®¨äº†LLMèå…¥åé¦ˆçš„èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œå³ä½¿åœ¨è¿‘ä¹å®Œç¾çš„åé¦ˆæ¡ä»¶ä¸‹ï¼Œæ±‚è§£æ¨¡å‹ä»å¯¹åé¦ˆè¡¨ç°å‡ºä¸€å®šçš„æŠµæŠ—æ€§ï¼Œç§°ä¹‹ä¸ºâ€œåé¦ˆæ‘©æ“¦â€ã€‚æœ¬ç ”ç©¶å°è¯•é‡‡ç”¨åŸºäºé‡‡æ ·çš„ç­–ç•¥æ¥ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œä½†ä»æœªèƒ½ä½¿æ¨¡å‹è¾¾åˆ°ç›®æ ‡æ€§èƒ½ã€‚ç ”ç©¶è¿˜å¯¹â€œåé¦ˆæ‘©æ“¦â€çš„æ½œåœ¨åŸå› è¿›è¡Œäº†æ·±å…¥æ¢è®¨ï¼Œæ’é™¤äº†æ¨¡å‹è¿‡åº¦è‡ªä¿¡å’Œæ•°æ®ç†Ÿæ‚‰åº¦ç­‰å› ç´ ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>LLMsèƒ½å¤Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šåˆ©ç”¨å¤–éƒ¨åé¦ˆæ”¹è¿›å…¶å›åº”ã€‚</li>
<li>åœ¨è¿‘ä¹å®Œç¾çš„åé¦ˆæ¡ä»¶ä¸‹ï¼ŒLLMsä»è¡¨ç°å‡ºå¯¹åé¦ˆçš„æŠµæŠ—æ€§ï¼Œç§°ä¸ºâ€œåé¦ˆæ‘©æ“¦â€ã€‚</li>
<li>åŸºäºé‡‡æ ·çš„ç­–ç•¥ï¼Œå¦‚é€æ­¥æé«˜æ¸©åº¦ã€æ˜ç¡®æ‹’ç»ä¹‹å‰é”™è¯¯çš„ç­”æ¡ˆï¼Œæœ‰åŠ©äºç¼“è§£â€œåé¦ˆæ‘©æ“¦â€ï¼Œä½†ä»æœªè¾¾åˆ°ç†æƒ³æ•ˆæœã€‚</li>
<li>ç ”ç©¶æ’é™¤äº†æ¨¡å‹è¿‡åº¦è‡ªä¿¡å’Œæ•°æ®ç†Ÿæ‚‰åº¦ç­‰å› ç´ ï¼Œå¯¹â€œåé¦ˆæ‘©æ“¦â€çš„æ½œåœ¨åŸå› è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11930">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a47e5390016592bb9c05d586319c0fa9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-b17f033e6e3132523f17870a86a44b9a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-586c9d03cb865406a1406840234356b3.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="LiveCodeBench-Pro-How-Do-Olympiad-Medalists-Judge-LLMs-in-Competitive-Programming"><a href="#LiveCodeBench-Pro-How-Do-Olympiad-Medalists-Judge-LLMs-in-Competitive-Programming" class="headerlink" title="LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive   Programming?"></a>LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive   Programming?</h2><p><strong>Authors:Zihan Zheng, Zerui Cheng, Zeyu Shen, Shang Zhou, Kaiyuan Liu, Hansen He, Dongruixuan Li, Stanley Wei, Hangyi Hao, Jianzhu Yao, Peiyao Sheng, Zixuan Wang, Wenhao Chai, Aleksandra Korolova, Peter Henderson, Sanjeev Arora, Pramod Viswanath, Jingbo Shang, Saining Xie</strong></p>
<p>Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programming. Drawing on knowledge from a group of medalists in international algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and where limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem for algorithmic categories and conducts a line-by-line analysis of failed model-generated submissions. Using this new data and benchmark, we find that frontier models still have significant limitations: without external tools, the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems, domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy problems but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. High performance appears largely driven by implementation precision and tool augmentation, not superior reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster levels, while offering fine-grained diagnostics to steer future improvements in code-centric LLM reasoning. </p>
<blockquote>
<p>æœ€è¿‘æŠ¥å‘Šæ˜¾ç¤ºï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¼–ç¨‹ç«èµ›ä¸­è¡¨ç°è¶…è¿‡ç²¾è‹±äººç±»ã€‚æˆ‘ä»¬å€ŸåŠ©å›½é™…ç®—æ³•ç«èµ›è·å¥–è€…çš„çŸ¥è¯†ï¼Œé‡æ–°å®¡è§†è¿™ä¸€è¯´æ³•ï¼Œæ¢è®¨LLMä¸äººç±»ä¸“å®¶ä¹‹é—´çš„å·®å¼‚ä»¥åŠä»ç„¶å­˜åœ¨çš„å±€é™æ€§ã€‚æˆ‘ä»¬æ¨å‡ºäº†LiveCodeBench Proï¼Œè¿™æ˜¯ä¸€ä¸ªç”±Codeforcesã€ICPCå’ŒIOIçš„é—®é¢˜ç»„æˆçš„åŸºå‡†æµ‹è¯•ï¼ŒæŒç»­æ›´æ–°ä»¥å‡å°‘æ•°æ®æ±¡æŸ“çš„å¯èƒ½æ€§ã€‚ä¸€æ”¯å¥¥æ—åŒ¹å…‹å¥–ç‰Œå¾—ä¸»å›¢é˜Ÿä¸ºæ¯ä¸€ä¸ªé—®é¢˜è¿›è¡Œç®—æ³•ç±»åˆ«æ³¨é‡Šï¼Œå¹¶å¯¹å¤±è´¥çš„æ¨¡å‹æäº¤è¿›è¡Œé€è¡Œåˆ†æã€‚é€šè¿‡ä½¿ç”¨è¿™äº›æ–°æ•°æ®å’ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°å‰æ²¿æ¨¡å‹ä»ç„¶å­˜åœ¨æ˜¾è‘—å±€é™æ€§ï¼šæ²¡æœ‰å¤–éƒ¨å·¥å…·çš„æƒ…å†µä¸‹ï¼Œæœ€ä½³æ¨¡å‹åœ¨ä¸­éš¾åº¦é—®é¢˜ä¸Šåªæœ‰53%çš„é€šè¿‡ç‡ï¼Œåœ¨éš¾é¢˜ä¸Šä¸ºé›¶ï¼Œè¿™äº›éƒ½æ˜¯ä¸“å®¶äººç±»ä»ç„¶æ“…é•¿çš„é¢†åŸŸã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼ŒLLMåœ¨éœ€è¦å¤§é‡å®ç°çš„é—®é¢˜ä¸Šå–å¾—æˆåŠŸï¼Œä½†åœ¨å¾®å¦™çš„ç®—æ³•æ¨ç†å’Œå¤æ‚çš„æ¡ˆä¾‹åˆ†æä¸Šé‡åˆ°å›°éš¾ï¼Œç»å¸¸ç”Ÿæˆé”™è¯¯çš„ç†ç”±ã€‚é«˜æ€§èƒ½ä¼¼ä¹ä¸»è¦æºäºå®ç°ç²¾åº¦å’Œå·¥å…·è¾…åŠ©ï¼Œè€Œéå‡ºè‰²çš„æ¨ç†èƒ½åŠ›ã€‚å› æ­¤ï¼ŒLiveCodeBench Proå‡¸æ˜¾äº†ä¸äººç±»å¤§å¸ˆæ°´å¹³çš„å·¨å¤§å·®è·ï¼ŒåŒæ—¶ä¸ºæœªæ¥æ”¹è¿›ä»£ç ä¸­å¿ƒçš„LLMæ¨ç†æä¾›äº†ç²¾ç»†çš„è¯Šæ–­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11928v1">PDF</a> Project Page at <a target="_blank" rel="noopener" href="https://livecodebenchpro.com/">https://livecodebenchpro.com/</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç«äº‰æ€§ç¼–ç¨‹ä¸­æ˜¯å¦è¶…è¶Šäººç±»ç²¾è‹±çš„äº‰è®ºæŒç»­ä¸æ–­ã€‚æœ¬æ–‡é€šè¿‡å›½é™…ç®—æ³•ç«èµ›è·å¥–è€…çš„è§‚ç‚¹ï¼Œé‡æ–°å®¡è§†è¿™ä¸€è¯´æ³•ï¼Œå¹¶æ¢è®¨LLMä¸äººç±»ä¸“å®¶çš„å·®å¼‚åŠå­˜åœ¨çš„å±€é™æ€§ã€‚é€šè¿‡å¼•å…¥LiveCodeBench Proè¿™ä¸€åŸºå‡†æµ‹è¯•å¹³å°ï¼Œæˆ‘ä»¬å‘ç°å‰æ²¿æ¨¡å‹ä»å­˜åœ¨æ˜¾è‘—ç¼ºé™·ï¼šåœ¨ä¸å€ŸåŠ©å¤–éƒ¨å·¥å…·çš„æƒ…å†µä¸‹ï¼Œæœ€ä½³æ¨¡å‹ä»…èƒ½åœ¨ä¸­ç­‰éš¾åº¦é—®é¢˜ä¸Šè¾¾åˆ°53%çš„é€šè¿‡ç‡ï¼Œè€Œåœ¨éš¾åº¦è¾ƒå¤§çš„é—®é¢˜ä¸Šåˆ™æ— æ³•çªç ´ã€‚æ­¤å¤–ï¼ŒLLMåœ¨æ³¨é‡å®ç°çš„é¢˜ç›®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦å¾®å¦™ç®—æ³•æ¨ç†å’Œå¤æ‚æ¡ˆä¾‹åˆ†ææ–¹é¢å´è¡¨ç°æ¬ ä½³ï¼Œæœ‰æ—¶ä¼šç»™å‡ºé”™è¯¯çš„è§£é‡Šã€‚å…¶é«˜è¡¨ç°ä¸»è¦å¾—ç›Šäºå®æ–½ç²¾å‡†æ€§å’Œå·¥å…·è¾…åŠ©ï¼Œè€Œéå“è¶Šæ¨ç†èƒ½åŠ›ã€‚LiveCodeBench Proä¸ä»…çªæ˜¾äº†ä¸äººç±»é¡¶å°–æ°´å¹³çš„å·¨å¤§å·®è·ï¼Œè¿˜ä¸ºæœªæ¥ä»£ç å¯¼å‘å‹LLMæ¨ç†çš„æ”¹è¿›æä¾›äº†ç²¾ç»†çš„è¯Šæ–­æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç«äº‰æ€§ç¼–ç¨‹æ–¹é¢çš„è¡¨ç°å—åˆ°å…³æ³¨ï¼Œå­˜åœ¨å…³äºå…¶æ˜¯å¦è¶…è¶Šäººç±»ç²¾è‹±çš„äº‰è®ºã€‚</li>
<li>é€šè¿‡å¼•å…¥LiveCodeBench ProåŸºå‡†æµ‹è¯•å¹³å°ï¼Œå‘ç°å‰æ²¿æ¨¡å‹åœ¨è§£å†³ç‰¹å®šéš¾åº¦é—®é¢˜æ—¶å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>LLMåœ¨æ³¨é‡å®ç°çš„é¢˜ç›®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨éœ€è¦å¾®å¦™ç®—æ³•æ¨ç†å’Œå¤æ‚æ¡ˆä¾‹åˆ†ææ–¹é¢å­˜åœ¨å›°éš¾ã€‚</li>
<li>LLMçš„é«˜è¡¨ç°ä¸»è¦å¾—ç›Šäºå®æ–½ç²¾å‡†æ€§å’Œå·¥å…·è¾…åŠ©ï¼Œè€Œéå“è¶Šæ¨ç†èƒ½åŠ›ã€‚</li>
<li>LiveCodeBench Proä¸ºè¯„ä¼°LLMä¸äººç±»çš„å·®è·æä¾›äº†å¹³å°ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥æ”¹è¿›çš„æ–¹å‘ã€‚</li>
<li>ç›®å‰LLMä»æ— æ³•å®Œå…¨æ›¿ä»£äººç±»ä¸“å®¶ï¼Œå°¤å…¶åœ¨è§£å†³é«˜éš¾åº¦é—®é¢˜æ—¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11928">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-5d357a7db4931c7d905f85dd8f468e82.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c6a7054c7cedcc71176ea47e3f0a0c5b.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0ad0b6d0ec8bad0ff0906bcb77c3c10a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-730c52bbf2440e56f17a656b8ce0af2a.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TreeRL-LLM-Reinforcement-Learning-with-On-Policy-Tree-Search"><a href="#TreeRL-LLM-Reinforcement-Learning-with-On-Policy-Tree-Search" class="headerlink" title="TreeRL: LLM Reinforcement Learning with On-Policy Tree Search"></a>TreeRL: LLM Reinforcement Learning with On-Policy Tree Search</h2><p><strong>Authors:Zhenyu Hou, Ziniu Hu, Yujiang Li, Rui Lu, Jie Tang, Yuxiao Dong</strong></p>
<p>Reinforcement learning (RL) with tree search has demonstrated superior performance in traditional reasoning tasks. Compared to conventional independent chain sampling strategies with outcome supervision, tree search enables better exploration of the reasoning space and provides dense, on-policy process rewards during RL training but remains under-explored in On-Policy LLM RL. We propose TreeRL, a reinforcement learning framework that directly incorporates on-policy tree search for RL training. Our approach includes intermediate supervision and eliminates the need for a separate reward model training. Existing approaches typically train a separate process reward model, which can suffer from distribution mismatch and reward hacking. We also introduce a cost-effective tree search approach that achieves higher search efficiency under the same generation token budget by strategically branching from high-uncertainty intermediate steps rather than using random branching. Experiments on challenging math and code reasoning benchmarks demonstrate that TreeRL achieves superior performance compared to traditional ChainRL, highlighting the potential of tree search for LLM. TreeRL is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/THUDM/TreeRL">https://github.com/THUDM/TreeRL</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç»“åˆæ ‘æœç´¢åœ¨ä¼ ç»Ÿæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿç‹¬ç«‹çš„é“¾é‡‡æ ·ç­–ç•¥ç›¸æ¯”ï¼Œæ ‘æœç´¢èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¢ç´¢æ¨ç†ç©ºé—´ï¼Œå¹¶åœ¨RLè®­ç»ƒè¿‡ç¨‹ä¸­æä¾›å¯†é›†çš„ç­–ç•¥å†…è¿‡ç¨‹å¥–åŠ±ï¼Œä½†åœ¨åŸºäºç­–ç•¥çš„LLM RLä¸­ä»è¢«è¾ƒå°‘æ¢ç´¢ã€‚æˆ‘ä»¬æå‡ºäº†TreeRLï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç›´æ¥ç»“åˆç­–ç•¥å†…æ ‘æœç´¢ç”¨äºRLè®­ç»ƒã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸­é—´ç›‘ç£ï¼Œæ¶ˆé™¤äº†å¯¹å•ç‹¬å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„éœ€è¦ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸è®­ç»ƒä¸€ä¸ªå•ç‹¬çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼Œè¿™å¯èƒ½ä¼šé­å—åˆ†å¸ƒä¸åŒ¹é…å’Œå¥–åŠ±ä½œå¼Šçš„é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å…·æœ‰æˆæœ¬æ•ˆç›Šçš„æ ‘æœç´¢æ–¹æ³•ï¼Œé€šè¿‡ä»é«˜ä¸ç¡®å®šæ€§çš„ä¸­é—´æ­¥éª¤è¿›è¡Œç­–ç•¥æ€§åˆ†æ”¯ï¼Œè€Œä¸æ˜¯ä½¿ç”¨éšæœºåˆ†æ”¯ï¼Œåœ¨ç›¸åŒçš„ç”Ÿæˆä»¤ç‰Œé¢„ç®—ä¸‹å®ç°æ›´é«˜çš„æœç´¢æ•ˆç‡ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTreeRLç›¸è¾ƒäºä¼ ç»Ÿçš„ChainRLå®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼Œçªæ˜¾äº†æ ‘æœç´¢åœ¨LLMä¸­çš„æ½œåŠ›ã€‚TreeRLå·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/THUDM/TreeRL%E4%B8%8A%E5%BC%80%E6%BA%90%E3%80%82">https://github.com/THUDM/TreeRLä¸Šå¼€æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11902v1">PDF</a> Accepted to ACL 2025 main conference</p>
<p><strong>Summary</strong></p>
<p>æ ‘æœç´¢å¼ºåŒ–å­¦ä¹ ï¼ˆTreeRLï¼‰æ¡†æ¶ç›´æ¥æ•´åˆäº†åœ¨çº¿ç­–ç•¥æ ‘æœç´¢ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œè¡¨ç°å‡ºä¼˜è¶Šçš„æ€§èƒ½ã€‚ä¸ä¼ ç»Ÿç‹¬ç«‹çš„é“¾é‡‡æ ·ç­–ç•¥ç›¸æ¯”ï¼Œæ ‘æœç´¢èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¢ç´¢æ¨ç†ç©ºé—´ï¼Œå¹¶æä¾›å¯†é›†çš„åœ¨çº¿ç­–ç•¥è¿‡ç¨‹å¥–åŠ±ã€‚TreeRLè¿˜åŒ…æ‹¬ä¸­é—´ç›‘ç£ï¼Œæ¶ˆé™¤äº†å¯¹å•ç‹¬å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„éœ€è¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…·æœ‰æˆæœ¬æ•ˆç›Šçš„æ ‘æœç´¢æ–¹æ³•ï¼Œé€šè¿‡ä»é«˜ä¸ç¡®å®šæ€§çš„ä¸­é—´æ­¥éª¤è¿›è¡Œæˆ˜ç•¥åˆ†æ”¯ï¼Œå®ç°åœ¨ç›¸åŒçš„ç”Ÿæˆä»¤ç‰Œé¢„ç®—ä¸‹æ›´é«˜çš„æœç´¢æ•ˆç‡ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸­ï¼ŒTreeRLç›¸æ¯”ä¼ ç»Ÿçš„ChainRLå®ç°äº†æ›´å¥½çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TreeRLç»“åˆäº†åœ¨çº¿ç­–ç•¥æ ‘æœç´¢ç”¨äºå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œæé«˜äº†æ€§èƒ½ã€‚</li>
<li>æ ‘æœç´¢èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°æ¢ç´¢æ¨ç†ç©ºé—´ã€‚</li>
<li>TreeRLæä¾›äº†å¯†é›†çš„åœ¨çº¿ç­–ç•¥è¿‡ç¨‹å¥–åŠ±ã€‚</li>
<li>TreeRLåŒ…æ‹¬ä¸­é—´ç›‘ç£ï¼Œæ¶ˆé™¤äº†å¯¹å•ç‹¬å¥–åŠ±æ¨¡å‹è®­ç»ƒçš„éœ€æ±‚ã€‚</li>
<li>æå‡ºçš„æ ‘æœç´¢æ–¹æ³•å…·æœ‰æˆæœ¬æ•ˆç›Šï¼Œé€šè¿‡æˆ˜ç•¥åˆ†æ”¯æé«˜æœç´¢æ•ˆç‡ã€‚</li>
<li>TreeRLåœ¨æŒ‘æˆ˜æ€§çš„æ•°å­¦å’Œä»£ç æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜è¶Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4a6aaa3f7cc4fed9931200e40748b8d7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d1f962b459bb680704267bc237ab367f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-efb1993a5f107c60629943dc7fb46b6c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-baa2e655819ec7fa6d760dc4b98764df.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-28acc9ffaaddddbf56757f955b9b91f3.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-bb6a4564e8be32ee51c449c933f0dcac.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Towards-a-Cascaded-LLM-Framework-for-Cost-effective-Human-AI-Decision-Making"><a href="#Towards-a-Cascaded-LLM-Framework-for-Cost-effective-Human-AI-Decision-Making" class="headerlink" title="Towards a Cascaded LLM Framework for Cost-effective Human-AI   Decision-Making"></a>Towards a Cascaded LLM Framework for Cost-effective Human-AI   Decision-Making</h2><p><strong>Authors:Claudio Fanconi, Mihaela van der Schaar</strong></p>
<p>Effective human-AI decision-making balances three key factors: the \textit{correctness} of predictions, the \textit{cost} of knowledge and reasoning complexity, and the confidence about whether to \textit{abstain} automated answers or involve human experts. In this work, we present a cascaded LLM decision framework that adaptively delegates tasks across multiple tiers of expertise â€“ a base model for initial candidate answers, a more capable and knowledgeable (but costlier) large model, and a human expert for when the model cascade abstains. Our method proceeds in two stages. First, a deferral policy determines whether to accept the base modelâ€™s answer or regenerate it with the large model based on the confidence score. Second, an abstention policy decides whether the cascade model response is sufficiently certain or requires human intervention. Moreover, we incorporate an online learning mechanism in the framework that can leverage human feedback to improve decision quality over time. We demonstrate this approach to general question-answering (ARC-Easy and ARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results show that our cascaded strategy outperforms in most cases single-model baselines in accuracy while reducing cost and providing a principled way to handle abstentions. </p>
<blockquote>
<p>æœ‰æ•ˆçš„äººæœºå†³ç­–å¹³è¡¡äº†ä¸‰ä¸ªå…³é”®å› ç´ ï¼šé¢„æµ‹çš„â€œæ­£ç¡®æ€§â€ã€çŸ¥è¯†çš„â€œæˆæœ¬â€å’Œæ¨ç†çš„å¤æ‚æ€§ï¼Œä»¥åŠå¯¹äºæ˜¯å¦æ”¾å¼ƒè‡ªåŠ¨åŒ–ç­”æ¡ˆæˆ–å¯»æ±‚äººç±»ä¸“å®¶å‚ä¸çš„ä¿¡å¿ƒã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªçº§è”çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å†³ç­–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè‡ªé€‚åº”åœ°åœ¨å¤šä¸ªä¸“ä¸šå±‚çº§ä¹‹é—´åˆ†é…ä»»åŠ¡â€”â€”ä¸€ä¸ªç”¨äºåˆå§‹å€™é€‰ç­”æ¡ˆçš„åŸºç¡€æ¨¡å‹ï¼Œä¸€ä¸ªèƒ½åŠ›æ›´å¼ºã€çŸ¥è¯†æ›´ä¸°å¯Œä½†æˆæœ¬æ›´é«˜çš„å¤§å‹æ¨¡å‹ï¼Œä»¥åŠåœ¨æ¨¡å‹çº§è”æ”¾å¼ƒæ—¶ä»‹å…¥çš„äººç±»ä¸“å®¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚é¦–å…ˆï¼Œæ‹’ç»ç­–ç•¥ä¼šå†³å®šæ˜¯å¦æ¥å—åŸºç¡€æ¨¡å‹çš„ç­”æ¡ˆï¼Œæˆ–è€…åŸºäºä¿¡å¿ƒåˆ†æ•°ä½¿ç”¨å¤§å‹æ¨¡å‹é‡æ–°ç”Ÿæˆç­”æ¡ˆã€‚å…¶æ¬¡ï¼Œå¼ƒæƒç­–ç•¥ä¼šå†³å®šçº§è”æ¨¡å‹å“åº”æ˜¯å¦è¶³å¤Ÿç¡®å®šï¼Œæˆ–è€…æ˜¯å¦éœ€è¦äººç±»å¹²é¢„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨æ¡†æ¶ä¸­çº³å…¥äº†ä¸€ç§åœ¨çº¿å­¦ä¹ æœºåˆ¶ï¼Œå¯ä»¥åˆ©ç”¨äººç±»åé¦ˆæ¥éšç€æ—¶é—´çš„æ¨ç§»æé«˜å†³ç­–è´¨é‡ã€‚æˆ‘ä»¬åœ¨é€šç”¨é—®ç­”ï¼ˆARC-Easyå’ŒARC-Challengeï¼‰å’ŒåŒ»ç–—é—®ç­”ï¼ˆMedQAå’ŒMedMCQAï¼‰ä¸­å±•ç¤ºäº†è¿™ç§æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„çº§è”ç­–ç•¥åœ¨å‡†ç¡®æ€§æ–¹é¢ä¼˜äºå•æ¨¡å‹åŸºå‡†çº¿ï¼ŒåŒæ—¶é™ä½æˆæœ¬ï¼Œå¹¶æä¾›äº†ä¸€ç§æœ‰åŸåˆ™çš„å¤„ç†å¼ƒæƒçš„æ–¹å¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11887v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼šæœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§çº§è”LLMå†³ç­–æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨ä¸åŒä¸“ä¸šå±‚æ¬¡ä¸Šè‡ªé€‚åº”åœ°å§”æ´¾ä»»åŠ¡ã€‚æ¡†æ¶åŒ…æ‹¬åˆå§‹ç­”æ¡ˆçš„åŸºç¡€æ¨¡å‹ã€èƒ½åŠ›æ›´å¼ºä½†æˆæœ¬æ›´é«˜çš„å¤§å‹æ¨¡å‹ï¼Œä»¥åŠåœ¨æ¨¡å‹çº§è”æ‹’ç»ç­”æ¡ˆæ—¶ä»‹å…¥çš„äººç±»ä¸“å®¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ‹’ç»ç­–ç•¥å’Œå›é¿ç­–ç•¥ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œå†³ç­–ï¼Œå¹¶åœ¨æ¡†æ¶ä¸­èå…¥äº†åœ¨çº¿å­¦ä¹ æœºåˆ¶ï¼Œå¯ä»¥åˆ©ç”¨äººç±»åé¦ˆæ¥æé«˜å†³ç­–è´¨é‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œçº§è”ç­–ç•¥åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¼˜äºå•æ¨¡å‹åŸºå‡†çº¿çš„å‡†ç¡®æ€§ï¼ŒåŒæ—¶é™ä½æˆæœ¬ï¼Œå¹¶æä¾›äº†ä¸€ç§å¤„ç†å›é¿çš„è§„èŒƒåŒ–æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªçº§è”LLMå†³ç­–æ¡†æ¶ï¼Œæ—¨åœ¨å®ç°äºº-AIååŒå†³ç­–ã€‚</li>
<li>æ¡†æ¶åŒ…å«åŸºç¡€æ¨¡å‹ã€å¤§å‹æ¨¡å‹å’Œäººç±»ä¸“å®¶ä¸‰ä¸ªå±‚æ¬¡ï¼Œèƒ½å¤Ÿè‡ªé€‚åº”å§”æ´¾ä»»åŠ¡ã€‚</li>
<li>é€šè¿‡æ‹’ç»ç­–ç•¥å’Œå›é¿ç­–ç•¥ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œå†³ç­–ã€‚</li>
<li>æ¡†æ¶èå…¥äº†åœ¨çº¿å­¦ä¹ æœºåˆ¶ï¼Œå¯ä»¥åˆ©ç”¨äººç±»åé¦ˆæé«˜å†³ç­–è´¨é‡ã€‚</li>
<li>çº§è”ç­–ç•¥åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¼˜äºå•æ¨¡å‹åŸºå‡†çº¿çš„å‡†ç¡®æ€§ã€‚</li>
<li>çº§è”ç­–ç•¥èƒ½å¤Ÿé™ä½æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11887">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-5631d1ad71be1d946aed2287589aa3eb.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-0cde2e81a6aadde62a62975052d6bddd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6ce939ab47eef7f89473b8ddefe7d109.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e3e68bc81bacabe7696f3046614aaf44.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e8354ef6a462676d3567a8f98f8292be.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Beyond-Homogeneous-Attention-Memory-Efficient-LLMs-via-Fourier-Approximated-KV-Cache"><a href="#Beyond-Homogeneous-Attention-Memory-Efficient-LLMs-via-Fourier-Approximated-KV-Cache" class="headerlink" title="Beyond Homogeneous Attention: Memory-Efficient LLMs via   Fourier-Approximated KV Cache"></a>Beyond Homogeneous Attention: Memory-Efficient LLMs via   Fourier-Approximated KV Cache</h2><p><strong>Authors:Xiaoran Liu, Siyang He, Qiqi Wang, Ruixiao Li, Yuerong Song, Zhigeng Liu, Linlin Li, Qun Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu</strong></p>
<p>Large Language Models struggle with memory demands from the growing Key-Value (KV) cache as context lengths increase. Existing compression methods homogenize head dimensions or rely on attention-guided token pruning, often sacrificing accuracy or introducing computational overhead. We propose FourierAttention, a training-free framework that exploits the heterogeneous roles of transformer head dimensions: lower dimensions prioritize local context, while upper ones capture long-range dependencies. By projecting the long-context-insensitive dimensions onto orthogonal Fourier bases, FourierAttention approximates their temporal evolution with fixed-length spectral coefficients. Evaluations on LLaMA models show that FourierAttention achieves the best long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel, FlashFourierAttention, is designed to optimize memory via streamlined read-write operations, enabling efficient deployment without performance compromise. </p>
<blockquote>
<p>éšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢ä¸´æ¥è‡ªä¸æ–­å¢é•¿çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜çš„å†…å­˜éœ€æ±‚æŒ‘æˆ˜ã€‚ç°æœ‰çš„å‹ç¼©æ–¹æ³•è¦ä¹ˆä½¿å¤´ç»´åº¦åŒè´¨åŒ–ï¼Œè¦ä¹ˆä¾èµ–äºæ³¨æ„åŠ›å¼•å¯¼ä»¤ç‰Œä¿®å‰ªï¼Œé€šå¸¸ä¼šç‰ºç‰²å‡†ç¡®æ€§æˆ–å¼•å…¥è®¡ç®—å¼€é”€ã€‚æˆ‘ä»¬æå‡ºäº†FourierAttentionï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æ¡†æ¶ï¼Œå®ƒåˆ©ç”¨å˜å‹å™¨å¤´ç»´åº¦çš„å¼‚æ„è§’è‰²ï¼šè¾ƒä½çš„ç»´åº¦ä¼˜å…ˆå¤„ç†å±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œè€Œè¾ƒé«˜çš„ç»´åº¦åˆ™æ•æ‰è¿œç¨‹ä¾èµ–å…³ç³»ã€‚é€šè¿‡å°†è¿œç¨‹ä¸Šä¸‹æ–‡æ— å…³ç»´åº¦æŠ•å½±åˆ°æ­£äº¤å‚…é‡Œå¶åŸºä¸Šï¼ŒFourierAttentionä½¿ç”¨å›ºå®šé•¿åº¦çš„è°±ç³»æ•°æ¥è¿‘ä¼¼å®ƒä»¬çš„æ—¶åºæ¼”åŒ–ã€‚åœ¨LLaMAæ¨¡å‹ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒFourierAttentionåœ¨é•¿ä¸Šä¸‹æ–‡å‡†ç¡®æ€§æ–¹é¢è¾¾åˆ°äº†LongBenchå’ŒHaystackä¸­çš„å°–é’ˆï¼ˆNIAHï¼‰çš„æœ€ä½³è¡¨ç°ã€‚æ­¤å¤–ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„Tritonå†…æ ¸FlashFourierAttentionï¼Œé€šè¿‡ç®€åŒ–çš„è¯»å†™æ“ä½œä¼˜åŒ–å†…å­˜ï¼Œä»¥å®ç°é«˜æ•ˆçš„éƒ¨ç½²è€Œä¸å½±å“æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11886v1">PDF</a> 10 pages, 7 figures, work in progress</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¢é•¿çš„å…³é”®å€¼ç¼“å­˜æ—¶é¢ä¸´è®°å¿†éœ€æ±‚é—®é¢˜ï¼Œéšç€ä¸Šä¸‹æ–‡é•¿åº¦çš„å¢åŠ ï¼Œç°æœ‰å‹ç¼©æ–¹æ³•å¾€å¾€ç‰ºç‰²å‡†ç¡®æ€§æˆ–å¢åŠ è®¡ç®—å¼€é”€ã€‚æœ¬æ–‡æå‡ºä¸€ç§åä¸ºFourierAttentionçš„è®­ç»ƒæ— å…³æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨å˜æ¢å™¨å¤´ç»´åº¦çš„ä¸åŒè§’è‰²ï¼šè¾ƒä½ç»´åº¦å…³æ³¨å±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œè€Œè¾ƒé«˜ç»´åº¦æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚é€šè¿‡å°†é•¿ä¸Šä¸‹æ–‡ä¸æ•æ„Ÿç»´åº¦æ˜ å°„åˆ°æ­£äº¤å‚…é‡Œå¶åŸºä¸Šï¼ŒFourierAttentionç”¨å›ºå®šé•¿åº¦çš„è°±ç³»æ•°è¿‘ä¼¼å…¶æ—¶é—´æ¼”å˜ã€‚åœ¨LLaMAæ¨¡å‹ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒFourierAttentionåœ¨é•¿åŸºå‡†æµ‹è¯•ï¼ˆLongBenchï¼‰å’Œé’ˆåœ¨ç¨»è‰å †ä¸­çš„å¯»æ‰¾ï¼ˆNeedle-In-A-Haystackï¼ŒNIAHï¼‰ä»»åŠ¡ä¸Šå®ç°äº†æœ€ä½³çš„é•¿ä¸Šä¸‹æ–‡å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œè®¾è®¡äº†ä¸€ç§è‡ªå®šä¹‰çš„Tritonå†…æ ¸FlashFourierAttentionï¼Œé€šè¿‡ä¼˜åŒ–è¯»å†™æ“ä½œæ¥èŠ‚çœå†…å­˜ï¼Œå®ç°é«˜æ•ˆéƒ¨ç½²è€Œä¸å½±å“æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´éšç€ä¸Šä¸‹æ–‡é•¿åº¦å¢åŠ è€Œå¢åŠ çš„è®°å¿†éœ€æ±‚é—®é¢˜ã€‚</li>
<li>ç°æœ‰å‹ç¼©æ–¹æ³•å¯èƒ½åœ¨ç‰ºç‰²å‡†ç¡®æ€§æˆ–å¢åŠ è®¡ç®—å¼€é”€çš„æƒ…å†µä¸‹æ•ˆæœä¸ä½³ã€‚</li>
<li>FourierAttentionæ¡†æ¶åˆ©ç”¨å˜æ¢å™¨å¤´ç»´åº¦çš„ä¸åŒè§’è‰²ï¼Œä»¥åº”å¯¹é•¿ä¸Šä¸‹æ–‡å¤„ç†ã€‚</li>
<li>FourierAttentioné€šè¿‡å°†é•¿ä¸Šä¸‹æ–‡ä¸æ•æ„Ÿç»´åº¦æ˜ å°„åˆ°å‚…é‡Œå¶åŸºä¸Šï¼Œæé«˜äº†é•¿ä¸Šä¸‹æ–‡å‡†ç¡®æ€§ã€‚</li>
<li>åœ¨LLaMAæ¨¡å‹ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼ŒFourierAttentionåœ¨ç‰¹å®šä»»åŠ¡ä¸Šå®ç°äº†æœ€ä½³æ€§èƒ½ã€‚</li>
<li>è®¾è®¡äº†FlashFourierAttentionï¼Œä¸€ç§è‡ªå®šä¹‰çš„Tritonå†…æ ¸ï¼Œä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨å¹¶æé«˜æ•ˆç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11886">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-18dd830d3ee0bf918145b6333a10a61c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-dae036c25f39c03051c93f6cc120c8e2.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-423f536394ceaece896e265de83f790a.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-a72138eebf68ef450c15374075016a52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-1a98960f0098b7bf7904d943e4c60f22.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4d1333a64f8704fdd6cf112e182d2c5c.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-0e7cc8650b50dae49ae4796402845e9b.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Addressing-Bias-in-LLMs-Strategies-and-Application-to-Fair-AI-based-Recruitment"><a href="#Addressing-Bias-in-LLMs-Strategies-and-Application-to-Fair-AI-based-Recruitment" class="headerlink" title="Addressing Bias in LLMs: Strategies and Application to Fair AI-based   Recruitment"></a>Addressing Bias in LLMs: Strategies and Application to Fair AI-based   Recruitment</h2><p><strong>Authors:Alejandro PeÃ±a, Julian Fierrez, Aythami Morales, Gonzalo Mancera, Miguel Lopez, Ruben Tolosana</strong></p>
<p>The use of language technologies in high-stake settings is increasing in recent years, mostly motivated by the success of Large Language Models (LLMs). However, despite the great performance of LLMs, they are are susceptible to ethical concerns, such as demographic biases, accountability, or privacy. This work seeks to analyze the capacity of Transformers-based systems to learn demographic biases present in the data, using a case study on AI-based automated recruitment. We propose a privacy-enhancing framework to reduce gender information from the learning pipeline as a way to mitigate biased behaviors in the final tools. Our experiments analyze the influence of data biases on systems built on two different LLMs, and how the proposed framework effectively prevents trained systems from reproducing the bias in the data. </p>
<blockquote>
<p>è¿‘å¹´æ¥ï¼Œåœ¨é«˜é£é™©ç¯å¢ƒä¸­ä½¿ç”¨è¯­è¨€æŠ€æœ¯çš„è¶‹åŠ¿æ­£åœ¨å¢åŠ ï¼Œè¿™ä¸»è¦å¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå°½ç®¡LLMçš„æ€§èƒ½å‡ºè‰²ï¼Œä½†å®ƒä»¬ä¹Ÿé¢ä¸´ç€é“å¾·ä¸Šçš„æ‹…å¿§ï¼Œä¾‹å¦‚äººå£ç»Ÿè®¡åè§ã€è´£ä»»æˆ–éšç§ç­‰é—®é¢˜ã€‚æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡åˆ†æåŸºäºTransformerçš„ç³»ç»Ÿå­¦ä¹ æ•°æ®ä¸­ç°æœ‰çš„äººå£ç»Ÿè®¡åè§çš„èƒ½åŠ›ï¼Œä»¥äººå·¥æ™ºèƒ½è‡ªåŠ¨åŒ–æ‹›è˜çš„æ¡ˆä¾‹åˆ†æä¸ºä¾‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¢å¼ºéšç§çš„æ¡†æ¶ï¼Œä»¥å‡å°‘å­¦ä¹ ç®¡é“ä¸­çš„æ€§åˆ«ä¿¡æ¯ï¼Œä½œä¸ºå‡è½»æœ€ç»ˆå·¥å…·ä¸­åè§è¡Œä¸ºçš„ä¸€ç§æ–¹å¼ã€‚æˆ‘ä»¬çš„å®éªŒåˆ†æäº†å»ºç«‹åœ¨ä¸¤ä¸ªä¸åŒLLMä¸Šçš„ç³»ç»Ÿæ•°æ®åè§çš„å½±å“ï¼Œä»¥åŠæ‰€æå‡ºçš„æ¡†æ¶å¦‚ä½•æœ‰æ•ˆåœ°é˜²æ­¢è®­ç»ƒç³»ç»Ÿåœ¨æ•°æ®ä¸­å¤åˆ¶åè§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11880v1">PDF</a> Submitted to AIES 2025 (Under Review)</p>
<p><strong>Summary</strong><br>     éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æˆåŠŸåº”ç”¨ï¼Œè¯­è¨€æŠ€æœ¯åœ¨é«˜é£é™©åœºæ™¯çš„ä½¿ç”¨æ—¥ç›Šå¢åŠ ã€‚ç„¶è€Œï¼ŒLLMså­˜åœ¨ä¼¦ç†é—®é¢˜ï¼Œå¦‚äººå£ç»Ÿè®¡åè§ã€é—®è´£åˆ¶å’Œéšç§ç­‰ã€‚æœ¬ç ”ç©¶æ—¨åœ¨åˆ†æåŸºäºTransformerçš„ç³»ç»Ÿå­¦ä¹ æ•°æ®ä¸­äººå£ç»Ÿè®¡åè§çš„èƒ½åŠ›ï¼Œä»¥äººå·¥æ™ºèƒ½è‡ªåŠ¨åŒ–æ‹›è˜ä¸ºä¾‹è¿›è¡Œæ¡ˆä¾‹ç ”ç©¶ã€‚ä¸ºäº†å‡è½»æœ€ç»ˆå·¥å…·ä¸­çš„åè§è¡Œä¸ºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¢å¼ºéšç§çš„æ¡†æ¶ï¼Œä»¥å‡å°‘å­¦ä¹ ç®¡é“ä¸­çš„æ€§åˆ«ä¿¡æ¯ã€‚å®éªŒåˆ†æäº†å»ºç«‹åœ¨ä¸¤ç§ä¸åŒLLMsä¸Šçš„ç³»ç»Ÿæ•°æ®åè§çš„å½±å“ï¼Œä»¥åŠè¯¥æ¡†æ¶å¦‚ä½•æœ‰æ•ˆé˜²æ­¢è®­ç»ƒç³»ç»Ÿåœ¨æ•°æ®ä¸­é‡å¤åè§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­è¨€æŠ€æœ¯åœ¨é«˜é£é™©åœºæ™¯çš„ä½¿ç”¨é€å¹´å¢åŠ ï¼Œä¸»è¦å¾—ç›Šäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æˆåŠŸåº”ç”¨ã€‚</li>
<li>LLMså­˜åœ¨ä¼¦ç†é—®é¢˜ï¼Œå¦‚äººå£ç»Ÿè®¡åè§ã€é—®è´£åˆ¶å’Œéšç§ç­‰ã€‚</li>
<li>æœ¬ç ”ç©¶é€šè¿‡äººå·¥æ™ºèƒ½è‡ªåŠ¨åŒ–æ‹›è˜çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œæ¢è®¨äº†åŸºäºTransformerçš„ç³»ç»Ÿå¦‚ä½•å­¦ä¹ æ•°æ®ä¸­çš„åè§ã€‚</li>
<li>ä¸ºäº†å‡è½»æœ€ç»ˆå·¥å…·ä¸­çš„åè§è¡Œä¸ºï¼Œæå‡ºäº†ä¸€ä¸ªå¢å¼ºéšç§çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥å‡å°‘å­¦ä¹ è¿‡ç¨‹ä¸­çš„æ€§åˆ«ä¿¡æ¯ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæ•°æ®åè§ä¼šå¯¹åŸºäºä¸åŒLLMsçš„ç³»ç»Ÿäº§ç”Ÿå½±å“ã€‚</li>
<li>æ‰€æå‡ºçš„æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢è®­ç»ƒç³»ç»Ÿåœ¨æ•°æ®ä¸­é‡å¤åè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11880">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-247bd3096e20f3b3c81cbe1a021ae152.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-e5ffd3c7a100d40089ab682ecb7f3538.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c21ee9e3d667146c19d31c68983c294a.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-Short-Survey-on-Formalising-Software-Requirements-using-Large-Language-Models"><a href="#A-Short-Survey-on-Formalising-Software-Requirements-using-Large-Language-Models" class="headerlink" title="A Short Survey on Formalising Software Requirements using Large Language   Models"></a>A Short Survey on Formalising Software Requirements using Large Language   Models</h2><p><strong>Authors:Arshad Beg, Diarmuid Oâ€™Donoghue, Rosemary Monahan</strong></p>
<p>This paper presents a focused literature survey on the use of large language models (LLM) to assist in writing formal specifications for software. A summary of thirty-five key papers is presented, including examples for specifying programs written in Dafny, C and Java. This paper arose from the project VERIFAI - Traceability and verification of natural language requirements that addresses the challenges in writing formal specifications from requirements that are expressed in natural language. Our methodology employed multiple academic databases to identify relevant research. The AI-assisted tool Elicit facilitated the initial paper selection, which were manually screened for final selection. The survey provides valuable insights and future directions for utilising LLMs while formalising software requirements. </p>
<blockquote>
<p>æœ¬æ–‡é‡ç‚¹è°ƒæŸ¥äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¾…åŠ©æ’°å†™è½¯ä»¶æ­£å¼è§„æ ¼æ–¹é¢çš„åº”ç”¨ã€‚æœ¬æ–‡åˆ—å‡ºäº†35ç¯‡é‡è¦è®ºæ–‡çš„æ‘˜è¦ï¼Œå…¶ä¸­åŒ…æ‹¬ç”¨Dafnyã€Cå’ŒJavaç¼–å†™çš„ç¨‹åºçš„ç¤ºä¾‹ã€‚æœ¬æ–‡æºäºVERIFAIé¡¹ç›®â€”â€”è‡ªç„¶è¯­è¨€è¦æ±‚çš„éœ€æ±‚è¿½è¸ªä¸éªŒè¯ï¼Œæ—¨åœ¨è§£å†³ä»è‡ªç„¶è¯­è¨€è¡¨è¿°çš„è¦æ±‚ä¸­ç¼–å†™æ­£å¼è§„æ ¼æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨å¤šä¸ªå­¦æœ¯æ•°æ®åº“æ¥ç¡®å®šç›¸å…³ç ”ç©¶ã€‚AIè¾…åŠ©å·¥å…·Elicitæœ‰åŠ©äºåˆæ­¥ç­›é€‰è®ºæ–‡ï¼Œæœ€ç»ˆç»è¿‡äººå·¥ç­›é€‰ç¡®å®šå…¥é€‰è®ºæ–‡ã€‚æœ¬æ¬¡è°ƒæŸ¥æä¾›äº†æœ‰ä»·å€¼çš„è§è§£å’Œæœªæ¥åˆ©ç”¨LLMè¿›è¡Œè½¯ä»¶è¦æ±‚å½¢å¼åŒ–éªŒè¯çš„æ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11874v1">PDF</a> Submitted to SAIV 2025 as extended abstract and received valuable   comments improving our draft. This version is the improved one after   addressing suggestions from reviewers for improving the draft</p>
<p><strong>Summary</strong><br>LLMåœ¨è½¯ä»¶å½¢å¼åŒ–è§„èŒƒå†™ä½œä¸­çš„åº”ç”¨ç»¼è¿°ã€‚æ–‡ç« æ±‡æ€»äº†35ç¯‡å…³é”®è®ºæ–‡ï¼Œæ¶µç›–ç”¨Dafnyã€Cå’ŒJavaç¼–å†™çš„ç¨‹åºè§„èŒƒç¤ºä¾‹ã€‚è¯¥æ–‡ç« æºäºVERIFAIé¡¹ç›®ï¼Œè§£å†³è‡ªç„¶è¯­è¨€è¦æ±‚ä¸‹çš„å½¢å¼åŒ–è§„èŒƒå†™ä½œæŒ‘æˆ˜ã€‚ç»¼è¿°æä¾›äº†åˆ©ç”¨AIè¾…åŠ©å·¥å…·è¿›è¡Œåˆå§‹è®ºæ–‡ç­›é€‰çš„æ–¹æ³•å’Œæœ‰ä»·å€¼çš„è§è§£åŠæœªæ¥çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è¯¥ç»¼è¿°èšç„¦äºç ”ç©¶LLMåœ¨ååŠ©ç¼–å†™è½¯ä»¶å½¢å¼åŒ–è§„èŒƒæ–¹é¢çš„åº”ç”¨ã€‚</li>
<li>æ–‡ç« æ±‡æ€»äº†å…³äºè¯¥ä¸»é¢˜çš„ç ”ç©¶è®ºæ–‡ï¼Œå¹¶æä¾›äº†åŒ…æ‹¬Dafnyã€Cå’ŒJavaåœ¨å†…çš„ç¨‹åºè§„èŒƒç¤ºä¾‹ã€‚</li>
<li>VERIFAIé¡¹ç›®æ—¨åœ¨è§£å†³ä»è‡ªç„¶è¯­è¨€è¦æ±‚ç¼–å†™å½¢å¼åŒ–è§„èŒƒæ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>é€šè¿‡å¤šç§å­¦æœ¯æ•°æ®åº“ç¡®å®šç›¸å…³ç ”ç©¶æˆæœå¹¶è¿›è¡Œæ–‡çŒ®è°ƒç ”ã€‚</li>
<li>é‡‡ç”¨AIè¾…åŠ©å·¥å…·Elicitè¿›è¡Œåˆæ­¥è®ºæ–‡ç­›é€‰ï¼Œå†æ‰‹åŠ¨ç­›é€‰æœ€ç»ˆè®ºæ–‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11874">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-45b9d54e8cf9da9794eff74dc004c1e4.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-fae718bf3641b1eae1b31466fea7d67d.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-27616c809d494127c0dcf928897ad02b.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Post-Persona-Alignment-for-Multi-Session-Dialogue-Generation"><a href="#Post-Persona-Alignment-for-Multi-Session-Dialogue-Generation" class="headerlink" title="Post Persona Alignment for Multi-Session Dialogue Generation"></a>Post Persona Alignment for Multi-Session Dialogue Generation</h2><p><strong>Authors:Yi-Pei Chen, Noriki Nishida, Hideki Nakayama, Yuji Matsumoto</strong></p>
<p>Multi-session persona-based dialogue generation presents challenges in maintaining long-term consistency and generating diverse, personalized responses. While large language models (LLMs) excel in single-session dialogues, they struggle to preserve persona fidelity and conversational coherence across extended interactions. Existing methods typically retrieve persona information before response generation, which can constrain diversity and result in generic outputs. We propose Post Persona Alignment (PPA), a novel two-stage framework that reverses this process. PPA first generates a general response based solely on dialogue context, then retrieves relevant persona memories using the response as a query, and finally refines the response to align with the speakerâ€™s persona. This post-hoc alignment strategy promotes naturalness and diversity while preserving consistency and personalization. Experiments on multi-session LLM-generated dialogue data demonstrate that PPA significantly outperforms prior approaches in consistency, diversity, and persona relevance, offering a more flexible and effective paradigm for long-term personalized dialogue generation. </p>
<blockquote>
<p>åŸºäºå¤šä¼šè¯äººæ ¼çš„å¯¹è¯ç”Ÿæˆåœ¨ä¿æŒé•¿æœŸä¸€è‡´æ€§ä»¥åŠç”Ÿæˆå¤šæ ·åŒ–å’Œä¸ªæ€§åŒ–çš„å“åº”æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å•ä¼šè¯å¯¹è¯ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨æ‰©å±•äº¤äº’ä¸­ä¿æŒäººæ ¼ä¿çœŸå’Œå¯¹è¯è¿è´¯æ€§æ–¹é¢å´é‡åˆ°å›°éš¾ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨ç”Ÿæˆå“åº”ä¹‹å‰æ£€ç´¢äººæ ¼ä¿¡æ¯ï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶å¤šæ ·æ€§å¹¶å¯¼è‡´é€šç”¨è¾“å‡ºã€‚æˆ‘ä»¬æå‡ºä¸€ç§åä¸ºâ€œåäººæ ¼å¯¹é½â€ï¼ˆPPAï¼‰çš„æ–°å‹ä¸¤é˜¶æ®µæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åè½¬äº†è¿™ä¸€è¿‡ç¨‹ã€‚PPAé¦–å…ˆä»…åŸºäºå¯¹è¯ä¸Šä¸‹æ–‡ç”Ÿæˆä¸€èˆ¬å“åº”ï¼Œç„¶åä½¿ç”¨å“åº”ä½œä¸ºæŸ¥è¯¢æ£€ç´¢ç›¸å…³çš„äººæ ¼è®°å¿†ï¼Œæœ€åå¯¹å“åº”è¿›è¡Œå¾®è°ƒä»¥ä¸è¯´è¯è€…çš„äººæ ¼å¯¹é½ã€‚è¿™ç§äº‹åå¯¹é½ç­–ç•¥æ—¢ä¿ƒè¿›äº†è‡ªç„¶æ€§å’Œå¤šæ ·æ€§ï¼Œåˆä¿æŒäº†è¿è´¯æ€§å’Œä¸ªæ€§åŒ–ã€‚åœ¨å¤šä¼šè¯LLMç”Ÿæˆçš„å¯¹è¯æ•°æ®ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸€è‡´æ€§ã€å¤šæ ·æ€§å’Œäººæ ¼ç›¸å…³æ€§æ–¹é¢ï¼ŒPPAæ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œä¸ºé•¿æœŸä¸ªæ€§åŒ–å¯¹è¯ç”Ÿæˆæä¾›äº†æ›´çµæ´»æœ‰æ•ˆçš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11857v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šä¼šè¯åŸºäºäººç‰©ä¸ªæ€§çš„å¯¹è¯ç”Ÿæˆåœ¨ç»´æŒé•¿æœŸä¸€è‡´æ€§ã€ç”Ÿæˆå¤šæ ·åŒ–å’Œä¸ªæ€§åŒ–å“åº”æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å•ä¼šè¯å¯¹è¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è·¨æ‰©å±•äº¤äº’ä¸­éš¾ä»¥ä¿æŒäººç‰©ä¸€è‡´æ€§ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨ç”Ÿæˆå“åº”ä¹‹å‰æ£€ç´¢äººç‰©ä¿¡æ¯ï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶å¤šæ ·æ€§å¹¶å¯¼è‡´é€šç”¨è¾“å‡ºã€‚æˆ‘ä»¬æå‡ºä¸€ç§æ–°çš„ä¸¤æ­¥æ¡†æ¶Post Persona Alignment (PPA)ï¼Œå®ƒé€šè¿‡ç”ŸæˆåŸºäºå¯¹è¯ä¸Šä¸‹æ–‡çš„ä¸€èˆ¬å“åº”ï¼Œç„¶åä½¿ç”¨å“åº”æŸ¥è¯¢ç›¸å…³çš„ä¸ªæ€§è®°å¿†è¿›è¡Œç²¾ç»†åŒ–å¯¹é½ï¼Œä»è€Œåœ¨ä¿ƒè¿›è‡ªç„¶æ€§å’Œå¤šæ ·æ€§çš„åŒæ—¶ä¿æŒä¸€è‡´æ€§å’Œä¸ªæ€§åŒ–ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨å¤šä¼šè¯LLMç”Ÿæˆçš„å¯¹è¯æ•°æ®ä¸­ï¼ŒPPAåœ¨ä¸€è‡´æ€§ã€å¤šæ ·æ€§å’Œä¸ªæ€§ç›¸å…³æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œä¸ºé•¿æœŸä¸ªæ€§åŒ–å¯¹è¯ç”Ÿæˆæä¾›äº†æ›´çµæ´»æœ‰æ•ˆçš„èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šä¼šè¯åŸºäºäººç‰©ä¸ªæ€§çš„å¯¹è¯ç”Ÿæˆé¢ä¸´é•¿æœŸä¸€è‡´æ€§ã€å“åº”å¤šæ ·åŒ–å’Œä¸ªæ€§åŒ–æ–¹é¢çš„æŒ‘æˆ˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å•ä¼šè¯å¯¹è¯ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è·¨æ‰©å±•äº¤äº’ä¸­éš¾ä»¥ä¿æŒäººç‰©ä¸€è‡´æ€§ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•é€šå¸¸åœ¨ç”Ÿæˆå“åº”å‰æ£€ç´¢äººç‰©ä¿¡æ¯ï¼Œé™åˆ¶äº†å“åº”çš„å¤šæ ·æ€§å¹¶å¯èƒ½å¯¼è‡´é€šç”¨è¾“å‡ºã€‚</li>
<li>æå‡ºçš„Post Persona Alignment (PPA)æ¡†æ¶é‡‡ç”¨ä¸¤æ­¥ç­–ç•¥ï¼Œé¦–å…ˆåŸºäºå¯¹è¯ä¸Šä¸‹æ–‡ç”Ÿæˆä¸€èˆ¬å“åº”ï¼Œç„¶åæ£€ç´¢ç›¸å…³çš„ä¸ªæ€§è®°å¿†è¿›è¡Œç²¾ç»†åŒ–å¯¹é½ã€‚</li>
<li>PPAç­–ç•¥åœ¨ä¿æŒè‡ªç„¶æ€§å’Œå¤šæ ·æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿä¿æŒä¸€è‡´æ€§å¹¶ä¿ƒè¿›ä¸ªæ€§åŒ–ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼ŒPPAåœ¨ä¸€è‡´æ€§ã€å¤šæ ·æ€§å’Œä¸ªæ€§ç›¸å…³æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11857">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-13caf67f43b01ef9d5aa9f25fe4a59d0.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-00bf3e906608ca050ceffa051d66f83f.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MTabVQA-Evaluating-Multi-Tabular-Reasoning-of-Language-Models-in-Visual-Space"><a href="#MTabVQA-Evaluating-Multi-Tabular-Reasoning-of-Language-Models-in-Visual-Space" class="headerlink" title="MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual   Space"></a>MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual   Space</h2><p><strong>Authors:Anshul Singh, Chris Biemann, Jan Strich</strong></p>
<p>Vision-Language Models (VLMs) have demonstrated remarkable capabilities in interpreting visual layouts and text. However, a significant challenge remains in their ability to interpret robustly and reason over multi-tabular data presented as images, a common occurrence in real-world scenarios like web pages and digital documents. Existing benchmarks typically address single tables or non-visual data (text&#x2F;structured). This leaves a critical gap: they donâ€™t assess the ability to parse diverse table images, correlate information across them, and perform multi-hop reasoning on the combined visual data. We introduce MTabVQA, a novel benchmark specifically designed for multi-tabular visual question answering to bridge that gap. MTabVQA comprises 3,745 complex question-answer pairs that necessitate multi-hop reasoning across several visually rendered table images. We provide extensive benchmark results for state-of-the-art VLMs on MTabVQA, revealing significant performance limitations. We further investigate post-training techniques to enhance these reasoning abilities and release MTabVQA-Instruct, a large-scale instruction-tuning dataset. Our experiments show that fine-tuning VLMs with MTabVQA-Instruct substantially improves their performance on visual multi-tabular reasoning. Code and dataset (<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval">https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval</a>) are available online (<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/MTabVQA-EMNLP-B16E">https://anonymous.4open.science/r/MTabVQA-EMNLP-B16E</a>). </p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨è§£è¯»è§†è§‰å¸ƒå±€å’Œæ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨é¢å¯¹ä»¥å›¾åƒå½¢å¼å‘ˆç°çš„å¤šè¡¨æ ¼æ•°æ®æ—¶ï¼Œå®ƒä»¬ä»ç„¶é¢ä¸´ç€ç¨³å¥è§£è¯»å’Œæ¨ç†çš„é‡å¤§æŒ‘æˆ˜ï¼Œè¿™åœ¨ç½‘é¡µå’Œæ•°å­—æ–‡æ¡£ç­‰ç°å®åœºæ™¯ä¸­æ˜¯å¾ˆå¸¸è§çš„ç°è±¡ã€‚ç°æœ‰çš„åŸºå‡†æµ‹è¯•é€šå¸¸é’ˆå¯¹å•ä¸ªè¡¨æ ¼æˆ–éè§†è§‰æ•°æ®ï¼ˆæ–‡æœ¬&#x2F;ç»“æ„åŒ–æ•°æ®ï¼‰ï¼Œè¿™ç•™ä¸‹äº†ä¸€ä¸ªå…³é”®çš„ç©ºç™½ï¼šå®ƒä»¬æ— æ³•è¯„ä¼°è§£æå¤šç§è¡¨æ ¼å›¾åƒã€è·¨å›¾åƒå…³è”ä¿¡æ¯ä»¥åŠå¯¹ç»„åˆè§†è§‰æ•°æ®è¿›è¡Œå¤šè·³æ¨ç†çš„èƒ½åŠ›ã€‚æˆ‘ä»¬æ¨å‡ºäº†MTabVQAï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå¤šè¡¨æ ¼è§†è§‰é—®ç­”è®¾è®¡çš„å…¨æ–°åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨å¡«è¡¥è¿™ä¸€ç©ºç™½ã€‚MTabVQAåŒ…å«3745ä¸ªå¤æ‚çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œè¿™äº›é—®é¢˜ç­”æ¡ˆå¯¹éœ€è¦åœ¨å¤šä¸ªè§†è§‰å‘ˆç°çš„è¡¨æ ¼å›¾åƒä¹‹é—´è¿›è¡Œå¤šè·³æ¨ç†ã€‚æˆ‘ä»¬ä¸ºæœ€å…ˆè¿›çš„VLMsåœ¨MTabVQAä¸Šçš„åŸºå‡†æµ‹è¯•ç»“æœæä¾›äº†è¯¦å°½çš„ç»“æœï¼Œæ­ç¤ºäº†å…¶æ€§èƒ½ä¸Šçš„é‡å¤§å±€é™æ€§ã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥ç ”ç©¶äº†ç”¨äºå¢å¼ºè¿™äº›æ¨ç†èƒ½åŠ›çš„åè®­ç»ƒæŠ€æœ¯ï¼Œå¹¶å‘å¸ƒäº†MTabVQA-Instructå¤§è§„æ¨¡æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨MTabVQA-Instructå¾®è°ƒVLMså¯ä»¥æ˜¾è‘—æé«˜å…¶åœ¨è§†è§‰å¤šè¡¨æ ¼æ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡ä»¥ä¸‹é“¾æ¥è·å–ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval">https://huggingface.co/datasets/mtabvqa/MTabVQA-Eval</a> å’Œ <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/MTabVQA-EMNLP-B16E">https://anonymous.4open.science/r/MTabVQA-EMNLP-B16E</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11684v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨å¤„ç†è§†è§‰å¸ƒå±€å’Œæ–‡æœ¬æ—¶çš„å‡ºè‰²è¡¨ç°ï¼Œä½†åœ¨è§£æå¤šè¡¨æ ¼å›¾åƒæ•°æ®å¹¶è¿›è¡Œå¤šè·³æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œä½œè€…å¼•å…¥äº†MTabVQAåŸºå‡†æµ‹è¯•ï¼Œè¯¥åŸºå‡†æµ‹è¯•ä¸“é—¨ç”¨äºå¤šè¡¨æ ¼è§†è§‰é—®ç­”ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†æµ‹è¯•çš„ç©ºç™½ã€‚MTabVQAåŒ…å«3745ä¸ªå¤æ‚çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œè¿™äº›é—®é¢˜éœ€è¦åœ¨å¤šä¸ªè§†è§‰å‘ˆç°è¡¨æ ¼å›¾åƒä¹‹é—´è¿›è¡Œå¤šè·³æ¨ç†ã€‚ä½œè€…å¯¹æœ€æ–°VLMsåœ¨MTabVQAä¸Šçš„è¡¨ç°è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œå¹¶å‘ç°å…¶æ€§èƒ½å­˜åœ¨æ˜¾è‘—å±€é™æ€§ã€‚ä½œè€…è¿˜æ¢è®¨äº†å¢å¼ºè¿™äº›æ¨ç†èƒ½åŠ›çš„åè®­ç»ƒæŠ€æœ¯ï¼Œå¹¶å‘å¸ƒäº†å¤§è§„æ¨¡çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†MTabVQA-Instructã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨MTabVQA-Instructå¾®è°ƒVLMså¯æ˜¾è‘—æé«˜å…¶åœ¨è§†è§‰å¤šè¡¨æ ¼æ¨ç†æ–¹é¢çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLMsåœ¨å¤„ç†è§†è§‰å¸ƒå±€å’Œæ–‡æœ¬æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è§£æå¤šè¡¨æ ¼å›¾åƒæ•°æ®å’Œè¿›è¡Œå¤šè·³æ¨ç†æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦å…³æ³¨å•è¡¨æˆ–éè§†è§‰æ•°æ®ï¼Œæ— æ³•è¯„ä¼°å¯¹å¤šæ ·è¡¨æ ¼å›¾åƒçš„è§£æèƒ½åŠ›ã€è·¨è¡¨æ ¼çš„ä¿¡æ¯å…³è”ä»¥åŠå¤šè·³è§†è§‰æ•°æ®æ¨ç†ã€‚</li>
<li>å¼•å…¥MTabVQAåŸºå‡†æµ‹è¯•ï¼Œä¸“é—¨ç”¨äºå¤šè¡¨æ ¼è§†è§‰é—®ç­”ï¼ŒåŒ…å«å¤æ‚çš„é—®é¢˜ç­”æ¡ˆå¯¹ï¼Œéœ€è¦è·¨å¤šä¸ªè§†è§‰è¡¨æ ¼è¿›è¡Œå¤šè·³æ¨ç†ã€‚</li>
<li>æœ€æ–°VLMsåœ¨MTabVQAä¸Šçš„æ€§èƒ½å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>æ¢è®¨äº†å¢å¼ºVLMsåœ¨å¤šè¡¨æ ¼è§†è§‰æ¨ç†æ–¹é¢çš„æ€§èƒ½çš„åè®­ç»ƒæŠ€æœ¯ã€‚</li>
<li>å‘å¸ƒäº†å¤§è§„æ¨¡çš„æŒ‡ä»¤è°ƒæ•´æ•°æ®é›†MTabVQA-Instructï¼Œç”¨äºå¾®è°ƒVLMsã€‚</li>
<li>ä½¿ç”¨MTabVQA-Instructå¾®è°ƒçš„VLMsåœ¨è§†è§‰å¤šè¡¨æ ¼æ¨ç†æ–¹é¢çš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11684">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6220ab38495ed535e18fd0d1c1d7fcd9.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-1d0bc6e05cdaf899748d95934ae7b0ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9b641d0a7197e15a38deeb22e1cee230.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-2b4b788b78258a276e597e07e6f7a870.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-49e268b2275a2a70596186aade6a8b86.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-6885bb79b9811fd3990e85a7cd11e070.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Dynamic-Mixture-of-Curriculum-LoRA-Experts-for-Continual-Multimodal-Instruction-Tuning"><a href="#Dynamic-Mixture-of-Curriculum-LoRA-Experts-for-Continual-Multimodal-Instruction-Tuning" class="headerlink" title="Dynamic Mixture of Curriculum LoRA Experts for Continual Multimodal   Instruction Tuning"></a>Dynamic Mixture of Curriculum LoRA Experts for Continual Multimodal   Instruction Tuning</h2><p><strong>Authors:Chendi Ge, Xin Wang, Zeyang Zhang, Hong Chen, Jiapei Fan, Longtao Huang, Hui Xue, Wenwu Zhu</strong></p>
<p>Continual multimodal instruction tuning is crucial for adapting Multimodal Large Language Models (MLLMs) to evolving tasks. However, most existing methods adopt a fixed architecture, struggling with adapting to new tasks due to static model capacity. We propose to evolve the architecture under parameter budgets for dynamic task adaptation, which remains unexplored and imposes two challenges: 1) task architecture conflict, where different tasks require varying layer-wise adaptations, and 2) modality imbalance, where different tasks rely unevenly on modalities, leading to unbalanced updates. To address these challenges, we propose a novel Dynamic Mixture of Curriculum LoRA Experts (D-MoLE) method, which automatically evolves MLLMâ€™s architecture with controlled parameter budgets to continually adapt to new tasks while retaining previously learned knowledge. Specifically, we propose a dynamic layer-wise expert allocator, which automatically allocates LoRA experts across layers to resolve architecture conflicts, and routes instructions layer-wisely to facilitate knowledge sharing among experts. Then, we propose a gradient-based inter-modal continual curriculum, which adjusts the update ratio of each module in MLLM based on the difficulty of each modality within the task to alleviate the modality imbalance problem. Extensive experiments show that D-MoLE significantly outperforms state-of-the-art baselines, achieving a 15% average improvement over the best baseline. To the best of our knowledge, this is the first study of continual learning for MLLMs from an architectural perspective. </p>
<blockquote>
<p>æŒç»­å¤šæ¨¡å¼æŒ‡ä»¤è°ƒæ•´å¯¹äºé€‚åº”ä¸æ–­æ¼”å˜ä»»åŠ¡çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰æ–¹æ³•é‡‡ç”¨å›ºå®šæ¶æ„ï¼Œç”±äºé™æ€æ¨¡å‹å®¹é‡è€Œæ— æ³•é€‚åº”æ–°ä»»åŠ¡ã€‚æˆ‘ä»¬å»ºè®®åœ¨å‚æ•°é¢„ç®—ä¸‹å‘å±•åŠ¨æ€ä»»åŠ¡é€‚åº”çš„æ¶æ„ï¼Œè¿™ä¸€é¢†åŸŸå°šæœªè¢«æ¢ç´¢å¹¶å¸¦æ¥äº†ä¸¤ä¸ªæŒ‘æˆ˜ï¼š1ï¼‰ä»»åŠ¡æ¶æ„å†²çªï¼Œä¸åŒçš„ä»»åŠ¡éœ€è¦ä¸åŒçš„åˆ†å±‚é€‚åº”ï¼›2ï¼‰æ¨¡æ€ä¸å¹³è¡¡ï¼Œä¸åŒçš„ä»»åŠ¡å¯¹æ¨¡æ€çš„ä¾èµ–ç¨‹åº¦ä¸åŒï¼Œå¯¼è‡´æ›´æ–°ä¸å‡è¡¡ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŠ¨æ€è¯¾ç¨‹LoRAä¸“å®¶æ··åˆï¼ˆD-MoLEï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿæ§åˆ¶å‚æ•°é¢„ç®—è‡ªåŠ¨è¿›åŒ–MLLMæ¶æ„ï¼Œä¸æ–­é€‚åº”æ–°ä»»åŠ¡å¹¶ä¿ç•™å…ˆå‰å­¦åˆ°çš„çŸ¥è¯†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠ¨æ€åˆ†å±‚ä¸“å®¶åˆ†é…å™¨ï¼Œå®ƒä¼šè‡ªåŠ¨åœ¨å±‚ä¹‹é—´åˆ†é…LoRAä¸“å®¶ä»¥è§£å†³æ¶æ„å†²çªï¼Œå¹¶æœ‰é’ˆå¯¹æ€§åœ°è·¯ç”±æŒ‡ä»¤ä»¥ä¿ƒè¿›ä¸“å®¶ä¹‹é—´çš„çŸ¥è¯†å…±äº«ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºæ¢¯åº¦çš„è·¨æ¨¡æ€è¿ç»­è¯¾ç¨‹å­¦ä¹ ï¼Œæ ¹æ®ä»»åŠ¡å†…å„æ¨¡æ€çš„éš¾åº¦è°ƒæ•´MLLMä¸­æ¯ä¸ªæ¨¡å—çš„æ›´æ–°æ¯”ä¾‹ï¼Œä»¥ç¼“è§£æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒD-MoLEæ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿æ–¹æ³•ï¼Œå¹³å‡æé«˜äº†çº¦15%ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡ä»æ¶æ„è§’åº¦ç ”ç©¶MLLMçš„æŒç»­å­¦ä¹ é—®é¢˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11672v1">PDF</a> Accepted by ICML 2025</p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æŒç»­å¤šä»»åŠ¡è°ƒæ•´å¾ˆé‡è¦ã€‚ç°æœ‰æ–¹æ³•å¤§å¤šé‡‡ç”¨å›ºå®šæ¶æ„ï¼Œéš¾ä»¥é€‚åº”æ–°ä»»åŠ¡ã€‚æˆ‘ä»¬æè®®åœ¨å‚æ•°é¢„ç®—ä¸‹è¿›åŒ–æ¶æ„ä»¥é€‚åº”åŠ¨æ€ä»»åŠ¡ï¼Œé¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä»»åŠ¡æ¶æ„å†²çªå’Œæ¨¡æ€ä¸å¹³è¡¡ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŠ¨æ€æ··åˆè¯¾ç¨‹LoRAä¸“å®¶ï¼ˆD-MoLEï¼‰æ–¹æ³•ï¼Œå¯è‡ªåŠ¨è°ƒæ•´MLLMæ¶æ„ä»¥æŒç»­é€‚åº”æ–°ä»»åŠ¡å¹¶ä¿ç•™å…ˆå‰çŸ¥è¯†ã€‚é€šè¿‡åŠ¨æ€å±‚çº§ä¸“å®¶åˆ†é…å™¨å’ŒåŸºäºæ¢¯åº¦çš„è·¨æ¨¡æ€æŒç»­è¯¾ç¨‹æ¥è§£å†³è¿™äº›é—®é¢˜ã€‚å®éªŒæ˜¾ç¤ºï¼ŒD-MoLEæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ï¼Œå¹³å‡æå‡è¾¾15%ã€‚è¿™æ˜¯é¦–ä¸ªä»æ¶æ„è§’åº¦ç ”ç©¶MLLMçš„è¿ç»­å­¦ä¹ ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰éœ€è¦æŒç»­å¤šä»»åŠ¡è°ƒæ•´ä»¥é€‚åº”ä¸æ–­å˜åŒ–çš„åœºæ™¯ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤§å¤šé‡‡ç”¨å›ºå®šæ¶æ„ï¼Œéš¾ä»¥é€‚åº”æ–°ä»»åŠ¡éœ€æ±‚çš„å˜åŒ–ã€‚</li>
<li>åŠ¨æ€è°ƒæ•´æ¶æ„é¢ä¸´ä¸¤å¤§æŒ‘æˆ˜ï¼šä»»åŠ¡æ¶æ„å†²çªå’Œæ¨¡æ€ä¸å¹³è¡¡ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•D-MoLEï¼Œå¯ä»¥è‡ªåŠ¨è°ƒæ•´MLLMçš„æ¶æ„ä»¥é€‚åº”åŠ¨æ€ä»»åŠ¡ã€‚</li>
<li>D-MoLEé€šè¿‡åŠ¨æ€å±‚çº§ä¸“å®¶åˆ†é…å™¨è§£å†³æ¶æ„å†²çªï¼Œå¹¶é€šè¿‡åŸºäºæ¢¯åº¦çš„è·¨æ¨¡æ€æŒç»­è¯¾ç¨‹è§£å†³æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒD-MoLEæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œå¹³å‡æå‡æ•ˆæœè¾¾åˆ°15%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11672">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-3d8c3b351490f22164957f1bb59a707f.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-b9f7c35676808c033aae9812ddc92671.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-03c5ae2f725af5e7251b3773b82b4fdf.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-d38e4ea1cbc7f35dbbcc1d785d24ede1.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Gondola-Grounded-Vision-Language-Planning-for-Generalizable-Robotic-Manipulation"><a href="#Gondola-Grounded-Vision-Language-Planning-for-Generalizable-Robotic-Manipulation" class="headerlink" title="Gondola: Grounded Vision Language Planning for Generalizable Robotic   Manipulation"></a>Gondola: Grounded Vision Language Planning for Generalizable Robotic   Manipulation</h2><p><strong>Authors:Shizhe Chen, Ricardo Garcia, Paul Pacaud, Cordelia Schmid</strong></p>
<p>Robotic manipulation faces a significant challenge in generalizing across unseen objects, environments and tasks specified by diverse language instructions. To improve generalization capabilities, recent research has incorporated large language models (LLMs) for planning and action execution. While promising, these methods often fall short in generating grounded plans in visual environments. Although efforts have been made to perform visual instructional tuning on LLMs for robotic manipulation, existing methods are typically constrained by single-view image input and struggle with precise object grounding. In this work, we introduce Gondola, a novel grounded vision-language planning model based on LLMs for generalizable robotic manipulation. Gondola takes multi-view images and history plans to produce the next action plan with interleaved texts and segmentation masks of target objects and locations. To support the training of Gondola, we construct three types of datasets using the RLBench simulator, namely robot grounded planning, multi-view referring expression and pseudo long-horizon task datasets. Gondola outperforms the state-of-the-art LLM-based method across all four generalization levels of the GemBench dataset, including novel placements, rigid objects, articulated objects and long-horizon tasks. </p>
<blockquote>
<p>æœºå™¨äººæ“ä½œé¢ä¸´åœ¨æœªè§è¿‡çš„ç‰©ä½“ã€ç¯å¢ƒå’Œé€šè¿‡å¤šæ ·åŒ–è¯­è¨€æŒ‡ä»¤æ‰€æŒ‡å®šçš„ä»»åŠ¡ä¸­è¿›è¡Œæ³›åŒ–çš„é‡å¤§æŒ‘æˆ˜ã€‚ä¸ºäº†æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œæœ€è¿‘çš„ç ”ç©¶å·²å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çº³å…¥è§„åˆ’å’Œè¡ŒåŠ¨æ‰§è¡Œä¸­ã€‚è™½ç„¶è¿™äº›æ–¹æ³•å‰æ™¯å¹¿é˜”ï¼Œä½†åœ¨è§†è§‰ç¯å¢ƒä¸­ç”ŸæˆåŸºäºå®é™…æƒ…å¢ƒçš„è®¡åˆ’æ—¶å¾€å¾€æœ‰æ‰€ä¸è¶³ã€‚å°½ç®¡äººä»¬å·²ç»åŠªåŠ›åœ¨LLMä¸Šå¯¹æœºå™¨äººæ“ä½œè¿›è¡Œè§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸å—åˆ°å•ä¸€è§†å›¾å›¾åƒè¾“å…¥çš„åˆ¶çº¦ï¼Œå¹¶ä¸”åœ¨ç²¾ç¡®å¯¹è±¡æ¥åœ°æ–¹é¢é‡åˆ°äº†å›°éš¾ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åŸºäºLLMçš„é€šç”¨æœºå™¨äººæ“ä½œçš„æ–°å‹åŸºäºå®é™…æƒ…å¢ƒçš„è§†è§‰è¯­è¨€è§„åˆ’æ¨¡å‹Gondolaã€‚Gondolaé‡‡ç”¨å¤šè§†å›¾å›¾åƒå’Œå†å²è®¡åˆ’æ¥ç”Ÿæˆä¸‹ä¸€ä¸ªå¸¦æœ‰ç›®æ ‡å¯¹è±¡å’Œä½ç½®çš„æ–‡æœ¬å’Œåˆ†å‰²æ©ç çš„äº¤äº’è®¡åˆ’ã€‚ä¸ºäº†æ”¯æŒGondolaçš„è®­ç»ƒï¼Œæˆ‘ä»¬ä½¿ç”¨RLBenchæ¨¡æ‹Ÿå™¨æ„å»ºäº†ä¸‰ç§ç±»å‹çš„æ•°æ®é›†ï¼Œå³æœºå™¨äººæ¥åœ°è§„åˆ’ã€å¤šè§†å›¾å¼•ç”¨è¡¨è¾¾å¼å’Œä¼ªé•¿æœŸä»»åŠ¡æ•°æ®é›†ã€‚Gondolaåœ¨GemBenchæ•°æ®é›†çš„æ‰€æœ‰å››ä¸ªæ³›åŒ–å±‚æ¬¡ä¸Šå‡ä¼˜äºæœ€æ–°çš„åŸºäºLLMçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æ–°å‹æ”¾ç½®ã€åˆšä½“ã€å…³èŠ‚å¼ç‰©ä½“å’Œé•¿æœŸä»»åŠ¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.11261v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœºå™¨äººæ“ä½œé¢ä¸´åœ¨æœªçŸ¥ç‰©ä½“ã€ç¯å¢ƒå’Œä»»åŠ¡ä¸­æ³›åŒ–èƒ½åŠ›çš„é—®é¢˜ï¼Œæœ€è¿‘çš„ç ”ç©¶ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œè§„åˆ’å’Œè¡ŒåŠ¨æ‰§è¡Œã€‚å°½ç®¡æœ‰å‰æ™¯ï¼Œä½†è¿™äº›æ–¹æ³•å¾€å¾€éš¾ä»¥åœ¨è§†è§‰ç¯å¢ƒä¸­ç”ŸæˆåŸºäºæƒ…å¢ƒçš„è®¡åˆ’ã€‚å°½ç®¡å·²æœ‰å¯¹LLMè¿›è¡Œè§†è§‰æŒ‡ä»¤è°ƒæ•´çš„åŠªåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸å—é™äºå•è§†å›¾å›¾åƒè¾“å…¥ï¼Œå¹¶ä¸”åœ¨ç²¾ç¡®å¯¹è±¡æ¥åœ°æ–¹é¢é‡åˆ°å›°éš¾ã€‚æœ¬ç ”ç©¶ä»‹ç»äº†åŸºäºLLMçš„é€šç”¨æœºå™¨äººæ“ä½œçš„æ–°å‹åŸºäºè§†è§‰çš„è¯­è¨€è§„åˆ’æ¨¡å‹Gondolaã€‚Gondolaé‡‡ç”¨å¤šè§†å›¾å›¾åƒå’Œå†å²è®¡åˆ’æ¥ç”Ÿæˆå¸¦æœ‰ç›®æ ‡å¯¹è±¡å’Œä½ç½®æ–‡æœ¬æè¿°å’Œåˆ†å‰²æ©ç çš„ä¸‹ä¸€ä¸ªè¡ŒåŠ¨è®¡åˆ’ã€‚ä¸ºäº†æ”¯æŒGondolaçš„è®­ç»ƒï¼Œæˆ‘ä»¬ä½¿ç”¨RLBenchæ¨¡æ‹Ÿå™¨æ„å»ºäº†ä¸‰ç§ç±»å‹çš„æ•°æ®é›†ï¼Œåˆ†åˆ«æ˜¯æœºå™¨äººæ¥åœ°è§„åˆ’ã€å¤šè§†å›¾å¼•ç”¨è¡¨è¾¾å¼å’Œä¼ªé•¿å‘¨æœŸä»»åŠ¡æ•°æ®é›†ã€‚åœ¨GemBenchæ•°æ®é›†çš„æ‰€æœ‰å››ä¸ªæ³›åŒ–å±‚æ¬¡ä¸Šï¼ŒGondolaçš„è¡¨ç°å‡ä¼˜äºæœ€æ–°çš„åŸºäºLLMçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æ–°æ”¾ç½®ã€åˆšä½“ã€å…³èŠ‚å¯¹è±¡å’Œé•¿å‘¨æœŸä»»åŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨äººæ“ä½œé¢ä¸´æ³›åŒ–æŒ‘æˆ˜ï¼Œéœ€è¦å¤„ç†ä¸åŒçš„ç‰©ä½“ã€ç¯å¢ƒå’Œä»»åŠ¡ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²è¢«å¼•å…¥æé«˜æœºå™¨äººçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•éš¾ä»¥åœ¨è§†è§‰ç¯å¢ƒä¸­ç”ŸæˆåŸºäºæƒ…å¢ƒçš„è®¡åˆ’ã€‚</li>
<li>Gondolaæ¨¡å‹é‡‡ç”¨å¤šè§†å›¾å›¾åƒå’Œå†å²è®¡åˆ’ç”Ÿæˆè¡ŒåŠ¨è®¡åˆ’ï¼ŒåŒ…æ‹¬ç›®æ ‡å¯¹è±¡å’Œä½ç½®çš„æ–‡æœ¬æè¿°å’Œåˆ†å‰²æ©ç ã€‚</li>
<li>ä¸ºäº†è®­ç»ƒGondolaæ¨¡å‹ï¼Œä½¿ç”¨äº†ä¸‰ç§ç±»å‹çš„æ•°æ®é›†ï¼šæœºå™¨äººæ¥åœ°è§„åˆ’ã€å¤šè§†å›¾å¼•ç”¨è¡¨è¾¾å¼å’Œä¼ªé•¿å‘¨æœŸä»»åŠ¡æ•°æ®é›†ã€‚</li>
<li>Gondolaåœ¨GemBenchæ•°æ®é›†çš„å››ä¸ªæ³›åŒ–å±‚æ¬¡ä¸Šè¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.11261">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pica.zhimg.com/v2-706e97afdc2f6e6cefa5813c73f7ed9b.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a8efd7a4753c6dcbc0f967b9cf706e5b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-444b744b1beac1bbed86dec2d4e7f8a3.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-bcc6547d6a713c98248dfd68785256cd.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-for-Toxic-Language-Detection-in-Low-Resource-Balkan-Languages"><a href="#Large-Language-Models-for-Toxic-Language-Detection-in-Low-Resource-Balkan-Languages" class="headerlink" title="Large Language Models for Toxic Language Detection in Low-Resource   Balkan Languages"></a>Large Language Models for Toxic Language Detection in Low-Resource   Balkan Languages</h2><p><strong>Authors:Amel Muminovic, Amela Kadric Muminovic</strong></p>
<p>Online toxic language causes real harm, especially in regions with limited moderation tools. In this study, we evaluate how large language models handle toxic comments in Serbian, Croatian, and Bosnian, languages with limited labeled data. We built and manually labeled a dataset of 4,500 YouTube and TikTok comments drawn from videos across diverse categories, including music, politics, sports, modeling, influencer content, discussions of sexism, and general topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus) were tested in two modes: zero-shot and context-augmented. We measured precision, recall, F1 score, accuracy and false positive rates. Including a short context snippet raised recall by about 0.12 on average and improved F1 score by up to 0.10, though it sometimes increased false positives. The best balance came from Gemini in context-augmented mode, reaching an F1 score of 0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the lowest false alarms. We show how adding minimal context can improve toxic language detection in low-resource settings and suggest practical strategies such as improved prompt design and threshold calibration. These results show that prompt design alone can yield meaningful gains in toxicity detection for underserved Balkan language communities. </p>
<blockquote>
<p>ç½‘ç»œæœ‰æ¯’è¯­è¨€ä¼šå¯¼è‡´çœŸå®ä¼¤å®³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹ç®¡ç†å·¥å…·çš„åœ°åŒºã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•å¤„ç†å¡å°”ç»´äºšè¯­ã€å…‹ç½—åœ°äºšè¯­å’Œæ³¢æ–¯å°¼äºšè¯­çš„æ¶æ¯’è¯„è®ºï¼Œè¿™äº›è¯­è¨€çš„æ ‡è®°æ•°æ®æœ‰é™ã€‚æˆ‘ä»¬æ„å»ºå¹¶æ‰‹åŠ¨æ ‡è®°äº†ä¸€ä¸ªåŒ…å«4500æ¡YouTubeå’ŒTikTokè¯„è®ºçš„æ•°æ®é›†ï¼Œè¿™äº›è¯„è®ºæ¥è‡ªéŸ³ä¹ã€æ”¿æ²»ã€ä½“è‚²ã€æ¨¡ç‰¹ã€ç½‘çº¢å†…å®¹ã€æ€§åˆ«ä¸»ä¹‰è®¨è®ºå’Œä¸€èˆ¬è¯é¢˜ç­‰ä¸åŒç±»åˆ«çš„è§†é¢‘ã€‚å››ç§æ¨¡å‹ï¼ˆGPT-3.5 Turboã€GPT-4.1ã€Gemini 1.5 Proå’ŒClaude 3 Opusï¼‰åœ¨ä¸¤ç§æ¨¡å¼ä¸‹è¿›è¡Œäº†æµ‹è¯•ï¼šé›¶æ ·æœ¬æ¨¡å¼å’Œä¸Šä¸‹æ–‡å¢å¼ºæ¨¡å¼ã€‚æˆ‘ä»¬æµ‹é‡äº†ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°ã€å‡†ç¡®ç‡å’Œè¯¯æŠ¥ç‡ã€‚åŒ…å«ç®€çŸ­çš„ä¸Šä¸‹æ–‡ç‰‡æ®µå¹³å‡æé«˜äº†çº¦0.12çš„å¬å›ç‡ï¼Œå¹¶ä½¿F1åˆ†æ•°æé«˜äº†é«˜è¾¾0.10ï¼Œå°½ç®¡æœ‰æ—¶ä¼šå¢åŠ è¯¯æŠ¥ã€‚æœ€ä½³å¹³è¡¡æ¥è‡ªä¸Šä¸‹æ–‡å¢å¼ºæ¨¡å¼ä¸‹çš„Geminiï¼ŒF1åˆ†æ•°å’Œå‡†ç¡®ç‡å‡è¾¾åˆ°0.82ï¼Œè€Œé›¶æ ·æœ¬æ¨¡å¼ä¸‹çš„GPT-4.1åœ¨ç²¾ç¡®åº¦ä¸Šé¢†å…ˆï¼Œå¹¶ä¸”è¯¯æŠ¥ç‡æœ€ä½ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­é€šè¿‡æ·»åŠ æœ€å°ä¸Šä¸‹æ–‡æ¥æ”¹è¿›æœ‰æ¯’è¯­è¨€æ£€æµ‹ï¼Œå¹¶æå‡ºå®ç”¨ç­–ç•¥ï¼Œå¦‚æ”¹è¿›æç¤ºè®¾è®¡å’Œé˜ˆå€¼æ ¡å‡†ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œä»…é€šè¿‡æç¤ºè®¾è®¡å°±å¯ä»¥åœ¨æœåŠ¡äºå·´å°”å¹²è¯­ç¤¾åŒºçš„æ¯’å®³æ£€æµ‹æ–¹é¢å–å¾—æœ‰æ„ä¹‰çš„æ”¶è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09992v2">PDF</a> 8 pages</p>
<p><strong>æ‘˜è¦</strong></p>
<p>åœ¨çº¿æœ‰æ¯’è¯­è¨€ä¼šé€ æˆå®é™…ä¼¤å®³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹é€‚åº¦å·¥å…·çš„åœ°åŒºã€‚æœ¬ç ”ç©¶è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¦‚ä½•å¤„ç†å¡å°”ç»´äºšè¯­ã€å…‹ç½—åœ°äºšè¯­å’Œæ³¢æ–¯å°¼äºšè¯­çš„æ¯’æ€§è¯„è®ºï¼Œè¿™äº›è¯­è¨€çš„æ•°æ®æ ‡ç­¾æœ‰é™ã€‚æˆ‘ä»¬æ„å»ºå¹¶æ‰‹åŠ¨æ ‡æ³¨äº†ä¸€ä¸ªåŒ…å«4500æ¡YouTubeå’ŒTikTokè¯„è®ºçš„æ•°æ®é›†ï¼Œè¿™äº›è¯„è®ºæ¥è‡ªéŸ³ä¹ã€æ”¿æ²»ã€ä½“è‚²ã€æ¨¡ç‰¹ã€å½±å“è€…å†…å®¹ã€æ€§åˆ«ä¸»ä¹‰è®¨è®ºå’Œä¸€èˆ¬è¯é¢˜ç­‰å„ç±»è§†é¢‘ã€‚å››é¡¹æ¨¡å‹ï¼ˆGPT-3.5 Turboã€GPT-4.1ã€åŒå­åº§1.5 Proå’ŒClaude 3 Opusï¼‰åœ¨ä¸¤ç§æ¨¡å¼ä¸‹è¿›è¡Œäº†æµ‹è¯•ï¼šé›¶æ ·æœ¬å’Œä¸Šä¸‹æ–‡å¢å¼ºã€‚æˆ‘ä»¬æµ‹é‡äº†ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°ã€å‡†ç¡®ç‡å’Œè¯¯æŠ¥ç‡ã€‚åŒ…å«ç®€çŸ­ä¸Šä¸‹æ–‡ç‰‡æ®µå¹³å‡æé«˜äº†çº¦0.12çš„å¬å›ç‡ï¼Œå¹¶æé«˜äº†é«˜è¾¾0.1çš„F1åˆ†æ•°ï¼Œä½†æœ‰æ—¶ä¼šå¢åŠ è¯¯æŠ¥ã€‚æœ€ä½³çš„å¹³è¡¡æ¥è‡ªä¸Šä¸‹æ–‡å¢å¼ºæ¨¡å¼ä¸‹çš„åŒå­åº§ï¼Œè¾¾åˆ°F1åˆ†æ•°0.82å’Œå‡†ç¡®ç‡0.82ï¼Œè€Œé›¶æ ·æœ¬GPT-4.1åœ¨ç²¾åº¦ä¸Šé¢†å…ˆï¼Œè¯¯æŠ¥ç‡æœ€ä½ã€‚æˆ‘ä»¬å±•ç¤ºäº†åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ï¼Œæ·»åŠ æœ€å°ä¸Šä¸‹æ–‡å¦‚ä½•æ”¹è¿›æœ‰æ¯’è¯­è¨€æ£€æµ‹ï¼Œå¹¶æå‡ºå®é™…ç­–ç•¥ï¼Œå¦‚æ”¹è¿›æç¤ºè®¾è®¡å’Œé˜ˆå€¼æ ¡å‡†ã€‚ç»“æœè¡¨æ˜ï¼Œä»…é€šè¿‡æç¤ºè®¾è®¡å°±å¯ä»¥ä¸ºæœåŠ¡ä¸è¶³çš„å·´å°”å¹²è¯­ç¤¾åŒºåœ¨æ¯’æ€§æ£€æµ‹æ–¹é¢å¸¦æ¥å®é™…æ„ä¹‰ä¸Šçš„æ”¶ç›Šã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åœ¨çº¿æœ‰æ¯’è¯­è¨€å¯å¯¼è‡´å®é™…ä¼¤å®³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç¼ºä¹é€‚å½“å·¥å…·çš„åœ°åŒºã€‚</li>
<li>æœ¬ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†å¡å°”ç»´äºšè¯­ã€å…‹ç½—åœ°äºšè¯­å’Œæ³¢æ–¯å°¼äºšè¯­çš„æ¯’æ€§è¯„è®ºæ–¹é¢çš„æ€§èƒ½ã€‚</li>
<li>æ„å»ºå¹¶æ‰‹åŠ¨æ ‡æ³¨äº†ä¸€ä¸ªåŒ…å«å„ç±»è§†é¢‘è¯„è®ºçš„æ•°æ®é›†ã€‚</li>
<li>åœ¨é›¶æ ·æœ¬å’Œä¸Šä¸‹æ–‡å¢å¼ºä¸¤ç§æ¨¡å¼ä¸‹æµ‹è¯•äº†å››ç§æ¨¡å‹ã€‚</li>
<li>æ·»åŠ ç®€çŸ­ä¸Šä¸‹æ–‡ç‰‡æ®µå¯æé«˜æ¨¡å‹æ€§èƒ½ï¼Œä½†å¯èƒ½å¢åŠ è¯¯æŠ¥ã€‚</li>
<li>Geminiæ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å¢å¼ºæ¨¡å¼ä¸‹è¡¨ç°æœ€ä½³ï¼Œè€ŒGPT-4.1åœ¨é›¶æ ·æœ¬æ¨¡å¼ä¸‹åœ¨ç²¾åº¦ä¸Šé¢†å…ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e276d9823aa4dc81d2a838922828831b.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-4bb526417475bb3dba62bb98d2113745.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8445a292bb194832b94f4171cfe09f52.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-43cc52ab9ebc18f57063204cd4d15b2f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9e7e2179cdeaf4774f5bc6e8fac562bc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-2082bf53d3abf3dce45b8cd9ed10f22f.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="e3-Learning-to-Explore-Enables-Extrapolation-of-Test-Time-Compute-for-LLMs"><a href="#e3-Learning-to-Explore-Enables-Extrapolation-of-Test-Time-Compute-for-LLMs" class="headerlink" title="e3: Learning to Explore Enables Extrapolation of Test-Time Compute for   LLMs"></a>e3: Learning to Explore Enables Extrapolation of Test-Time Compute for   LLMs</h2><p><strong>Authors:Amrith Setlur, Matthew Y. R. Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, Aviral Kumar</strong></p>
<p>Test-time scaling offers a promising path to improve LLM reasoning by utilizing more compute at inference time; however, the true promise of this paradigm lies in extrapolation (i.e., improvement in performance on hard problems as LLMs keep â€œthinkingâ€ for longer, beyond the maximum token budget they were trained on). Surprisingly, we find that most existing reasoning models do not extrapolate well. We show that one way to enable extrapolation is by training the LLM to perform in-context exploration: training the LLM to effectively spend its test time budget by chaining operations (such as generation, verification, refinement, etc.), or testing multiple hypotheses before it commits to an answer. To enable in-context exploration, we identify three key ingredients as part of our recipe e3: (1) chaining skills that the base LLM has asymmetric competence in, e.g., chaining verification (easy) with generation (hard), as a way to implement in-context search; (2) leveraging â€œnegativeâ€ gradients from incorrect traces to amplify exploration during RL, resulting in longer search traces that chains additional asymmetries; and (3) coupling task difficulty with training token budget during training via a specifically-designed curriculum to structure in-context exploration. Our recipe e3 produces the best known 1.7B model according to AIMEâ€™25 and HMMTâ€™25 scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not only attains high pass@1 scores, but also improves pass@k over the base model. </p>
<blockquote>
<p>æµ‹è¯•æ—¶ç¼©æ”¾æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„è·¯å¾„ï¼Œé€šè¿‡åœ¨æ¨ç†æ—¶é—´åˆ©ç”¨æ›´å¤šçš„è®¡ç®—æ¥æ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼›ç„¶è€Œï¼Œè¯¥æ¨¡å¼çš„çœŸæ­£æ½œåŠ›åœ¨äºå¤–æ¨ï¼ˆå³ï¼Œéšç€å¤§å‹è¯­è¨€æ¨¡å‹â€œæ€è€ƒâ€çš„æ—¶é—´å»¶é•¿ï¼Œè¶…å‡ºå…¶è®­ç»ƒæ—¶çš„æœ€å¤§ä»¤ç‰Œé¢„ç®—ï¼Œæ€§èƒ½çš„æå‡ï¼‰ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å¤§å¤šæ•°ç°æœ‰çš„æ¨ç†æ¨¡å‹çš„å¤–æ¨æ•ˆæœä¸ä½³ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå®ç°å¤–æ¨çš„ä¸€ç§æ–¹æ³•æ˜¯è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¸Šä¸‹æ–‡æ¢ç´¢ï¼šè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹æœ‰æ•ˆåœ°åˆ©ç”¨å…¶æµ‹è¯•æ—¶é—´é¢„ç®—ï¼Œé€šè¿‡æ“ä½œé“¾ï¼ˆå¦‚ç”Ÿæˆã€éªŒè¯ã€ç²¾ç‚¼ç­‰ï¼‰ï¼Œæˆ–åœ¨æäº¤ç­”æ¡ˆä¹‹å‰æµ‹è¯•å¤šä¸ªå‡è®¾ã€‚ä¸ºäº†å®ç°ä¸Šä¸‹æ–‡æ¢ç´¢ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä½œä¸ºæˆ‘ä»¬é…æ–¹e3çš„ä¸‰ä¸ªå…³é”®è¦ç´ ï¼šï¼ˆ1ï¼‰é“¾æ¥åŸºç¡€å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰ä¸å¯¹ç§°èƒ½åŠ›æŠ€èƒ½ï¼Œä¾‹å¦‚å°†éªŒè¯ï¼ˆç®€å•ï¼‰ä¸ç”Ÿæˆï¼ˆå›°éš¾ï¼‰è¿›è¡Œé“¾æ¥ï¼Œä½œä¸ºä¸€ç§å®ç°ä¸Šä¸‹æ–‡æœç´¢çš„æ–¹å¼ï¼›ï¼ˆ2ï¼‰åˆ©ç”¨æ¥è‡ªä¸æ­£ç¡®è½¨è¿¹çš„â€œè´Ÿé¢â€æ¢¯åº¦æ¥æ”¾å¤§å¼ºåŒ–å­¦ä¹ æœŸé—´çš„æ¢ç´¢ï¼Œä»è€Œäº§ç”Ÿæ›´é•¿çš„æœç´¢è½¨è¿¹ï¼Œé“¾æ¥é¢å¤–çš„ä¸å¯¹ç§°æ€§ï¼›ï¼ˆ3ï¼‰é€šè¿‡ä¸“é—¨è®¾è®¡çš„è¯¾ç¨‹åœ¨è®­ç»ƒæœŸé—´å°†ä»»åŠ¡éš¾åº¦ä¸è®­ç»ƒä»¤ç‰Œé¢„ç®—ç›¸ç»“åˆï¼Œä»¥æ„å»ºä¸Šä¸‹æ–‡æ¢ç´¢çš„ç»“æ„ã€‚æˆ‘ä»¬çš„é…æ–¹e3æ ¹æ®AIMEâ€™25å’ŒHMMTâ€™25çš„åˆ†æ•°äº§ç”Ÿäº†æœ€å¥½çš„å·²çŸ¥1.7Bæ¨¡å‹ï¼Œå¹¶å¤–æ¨åˆ°è®­ç»ƒä»¤ç‰Œé¢„ç®—çš„2å€ã€‚æˆ‘ä»¬çš„e3-1.7Bæ¨¡å‹ä¸ä»…è¾¾åˆ°äº†å¾ˆé«˜çš„pass@1åˆ†æ•°ï¼Œè€Œä¸”ç›¸å¯¹äºåŸºç¡€æ¨¡å‹è¿˜æé«˜äº†pass@kåˆ†æ•°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09026v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<pre><code>æµ‹è¯•æ—¶ç¼©æ”¾æä¾›äº†ä¸€ç§åˆ©ç”¨æ›´å¤šè®¡ç®—èµ„æºæé«˜LLMæ¨ç†èƒ½åŠ›çš„æœ‰å‰é€”çš„è·¯å¾„ã€‚çœŸæ­£çš„æ½œåŠ›åœ¨äºå¤–æ¨ï¼Œå³åœ¨æ›´é•¿æ—¶é—´çš„æ€è€ƒä¸­ï¼Œè¶…å‡ºè®­ç»ƒçš„æœ€å¤§æ ‡è®°é¢„ç®—çš„æƒ…å†µä¸‹æé«˜æ€§èƒ½ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æ„å¤–åœ°å‘ç°ï¼Œå¤§å¤šæ•°ç°æœ‰çš„æ¨ç†æ¨¡å‹çš„å¤–æ¨èƒ½åŠ›ä¸å¼ºã€‚æœ¬æ–‡é€šè¿‡è®­ç»ƒLLMè¿›è¡Œä¸Šä¸‹æ–‡æ¢ç´¢æ¥å®ç°å¤–æ¨èƒ½åŠ›çš„æ–¹æ³•ä¹‹ä¸€ï¼šè®­ç»ƒLLMæœ‰æ•ˆåœ°åˆ©ç”¨å…¶æµ‹è¯•æ—¶é—´é¢„ç®—ï¼Œé€šè¿‡æ“ä½œé“¾æ¥ï¼ˆå¦‚ç”Ÿæˆã€éªŒè¯ã€ç²¾ç‚¼ç­‰ï¼‰æˆ–åœ¨æäº¤ç­”æ¡ˆä¹‹å‰æµ‹è¯•å¤šä¸ªå‡è®¾æ¥è¿›è¡Œä¸Šä¸‹æ–‡æ¢ç´¢ã€‚æœ¬æ–‡ä»‹ç»äº†å®ç°ä¸Šä¸‹æ–‡æ¢ç´¢çš„ä¸‰ä¸ªå…³é”®è¦ç´ ï¼šé“¾æ¥ä¸å¯¹ç§°æŠ€èƒ½çš„è®­ç»ƒï¼Œåˆ©ç”¨é”™è¯¯è½¨è¿¹çš„â€œè´Ÿé¢â€æ¢¯åº¦æ”¾å¤§å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢ï¼Œä»¥åŠé€šè¿‡ä¸“é—¨è®¾è®¡çš„è¯¾ç¨‹å°†ä»»åŠ¡éš¾åº¦ä¸è®­ç»ƒæ ‡è®°é¢„ç®—è¿›è¡Œè€¦åˆä»¥ç»“æ„åŒ–ä¸Šä¸‹æ–‡æ¢ç´¢ã€‚æ ¹æ®AIME&#39;25å’ŒHMMT&#39;25çš„è¯„åˆ†ï¼Œæœ¬æ–‡å¼€å‘çš„e3é…æ–¹ç”Ÿäº§çš„æ¨¡å‹è¡¨ç°æœ€ä½³ï¼Œå¹¶åœ¨è®­ç»ƒæ ‡è®°é¢„ç®—çš„åŸºç¡€ä¸Šå¤–æ¨åˆ°ä¸¤å€ã€‚æˆ‘ä»¬çš„e3-1.7Bæ¨¡å‹ä¸ä»…è·å¾—äº†é«˜é€šè¿‡ç‡åˆ†æ•°ï¼Œè€Œä¸”ç›¸å¯¹äºåŸºç¡€æ¨¡å‹ä¹Ÿæé«˜äº†pass@kå¾—åˆ†ã€‚
</code></pre>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æµ‹è¯•æ—¶ç¼©æ”¾åˆ©ç”¨æ›´å¤šè®¡ç®—èµ„æºå¯ä»¥æé«˜LLMçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>å¤–æ¨èƒ½åŠ›åœ¨LLMä¸­å…·æœ‰é‡è¦æ„ä¹‰ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶ã€‚</li>
<li>ç°æœ‰æ¨ç†æ¨¡å‹åœ¨å¤–æ¨æ–¹é¢è¡¨ç°ä¸ä½³ã€‚</li>
<li>é€šè¿‡è®­ç»ƒLLMè¿›è¡Œä¸Šä¸‹æ–‡æ¢ç´¢ï¼Œå¯ä»¥æé«˜å…¶å¤–æ¨èƒ½åŠ›ã€‚</li>
<li>å®ç°ä¸Šä¸‹æ–‡æ¢ç´¢çš„å…³é”®è¦ç´ åŒ…æ‹¬ï¼šé“¾æ¥ä¸å¯¹ç§°æŠ€èƒ½ã€åˆ©ç”¨è´Ÿé¢æ¢¯åº¦æ”¾å¤§æ¢ç´¢ã€ä»»åŠ¡éš¾åº¦ä¸è®­ç»ƒæ ‡è®°é¢„ç®—çš„è€¦åˆã€‚</li>
<li>e3é…æ–¹ç”Ÿäº§çš„æ¨¡å‹åœ¨AIMEâ€™25å’ŒHMMTâ€™25çš„è¯„åˆ†ä¸­è¡¨ç°æœ€ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bf058bb1abf948d6f19bf151f597e052.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-40fc0dbe113ec3791fb56f3b892fa654.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-e92c894c318ce797534c1976e6e1b1fc.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-ba1a7daded12a6b6c15d2d36209431d3.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="LLaVA-c-Continual-Improved-Visual-Instruction-Tuning"><a href="#LLaVA-c-Continual-Improved-Visual-Instruction-Tuning" class="headerlink" title="LLaVA-c: Continual Improved Visual Instruction Tuning"></a>LLaVA-c: Continual Improved Visual Instruction Tuning</h2><p><strong>Authors:Wenzhuo Liu, Fei Zhu, Haiyang Guo, Longhui Wei, Cheng-Lin Liu</strong></p>
<p>Multimodal models like LLaVA-1.5 achieve state-of-the-art visual understanding through visual instruction tuning on multitask datasets, enabling strong instruction-following and multimodal performance. However, multitask learning faces challenges such as task balancing, requiring careful adjustment of data proportions, and expansion costs, where new tasks risk catastrophic forgetting and need costly retraining. Continual learning provides a promising alternative to acquiring new knowledge incrementally while preserving existing capabilities. However, current methods prioritize task-specific performance, neglecting base model degradation from overfitting to specific instructions, which undermines general capabilities. In this work, we propose a simple but effective method with two modifications on LLaVA-1.5: spectral-aware consolidation for improved task balance and unsupervised inquiry regularization to prevent base model degradation. We evaluate both general and task-specific performance across continual pretraining and fine-tuning. Experiments demonstrate that LLaVA-c consistently enhances standard benchmark performance and preserves general capabilities. For the first time, we show that task-by-task continual learning can achieve results that match or surpass multitask joint learning. The code will be publicly released. </p>
<blockquote>
<p>LLaVA-1.5ç­‰å¤šæ¨¡æ€æ¨¡å‹é€šè¿‡åœ¨å¤šä»»åŠ¡æ•°æ®é›†ä¸Šè¿›è¡Œè§†è§‰æŒ‡ä»¤è°ƒæ•´ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„è§†è§‰ç†è§£ï¼Œä»è€Œå®ç°äº†å¼ºå¤§çš„æŒ‡ä»¤éµå¾ªå’Œå¤šæ¨¡æ€æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤šä»»åŠ¡å­¦ä¹ é¢ä¸´ä»»åŠ¡å¹³è¡¡ç­‰æŒ‘æˆ˜ï¼Œéœ€è¦ä»”ç»†è°ƒæ•´æ•°æ®æ¯”ä¾‹å’Œæ‰©å±•æˆæœ¬ï¼Œæ–°å¢ä»»åŠ¡å­˜åœ¨ç¾éš¾æ€§é—å¿˜çš„é£é™©å¹¶éœ€è¦æ˜‚è´µçš„é‡æ–°è®­ç»ƒã€‚æŒç»­å­¦ä¹ æä¾›äº†ä¸€ç§æœ‰å‰æ™¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå¯ä»¥åœ¨ä¿ç•™ç°æœ‰èƒ½åŠ›çš„åŒæ—¶é€æ­¥è·å–æ–°çŸ¥è¯†ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ä¾§é‡äºç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ï¼Œå¿½è§†äº†ç”±äºè¿‡åº¦æ‹Ÿåˆç‰¹å®šæŒ‡ä»¤å¯¼è‡´çš„åŸºå‡†æ¨¡å‹é€€åŒ–é—®é¢˜ï¼Œä»è€ŒæŸå®³äº†é€šç”¨èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¯¹LLaVA-1.5è¿›è¡Œäº†ä¸¤é¡¹ä¿®æ”¹ï¼Œæå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•ï¼šå…‰è°±æ„ŸçŸ¥å·©å›ºä»¥æé«˜ä»»åŠ¡å¹³è¡¡ï¼Œä»¥åŠæ— ç›‘ç£æŸ¥è¯¢æ­£åˆ™åŒ–ä»¥é˜²æ­¢åŸºç¡€æ¨¡å‹é€€åŒ–ã€‚æˆ‘ä»¬è¯„ä¼°äº†æŒç»­é¢„è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ä¸­çš„é€šç”¨æ€§èƒ½å’Œç‰¹å®šä»»åŠ¡æ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒLLaVA-cæŒç»­æé«˜äº†æ ‡å‡†åŸºå‡†æ€§èƒ½å¹¶ä¿ç•™äº†é€šç”¨èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–æ¬¡è¯æ˜ï¼ŒæŒ‰ä»»åŠ¡é¡ºåºçš„æŒç»­å­¦ä¹ å¯ä»¥è¾¾åˆ°æˆ–å¤šäºå¤šä»»åŠ¡è”åˆå­¦ä¹ çš„ç»“æœã€‚ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.08666v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºLLaVA-1.5çš„å¤šæ¨¡æ€æ¨¡å‹é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´åœ¨å¤šä»»åŠ¡æ•°æ®é›†ä¸Šå®ç°äº†å…ˆè¿›çš„è§†è§‰ç†è§£ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æŒ‡ä»¤éµå¾ªå’Œå¤šæ¨¡æ€æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¤šä»»åŠ¡å­¦ä¹ é¢ä¸´ä»»åŠ¡å¹³è¡¡å’Œæ•°æ®æ¯”ä¾‹è°ƒæ•´çš„æŒ‘æˆ˜ï¼Œä»¥åŠæ–°ä»»åŠ¡çš„æ‰©å±•æˆæœ¬é—®é¢˜ï¼ŒåŒ…æ‹¬ç¾éš¾æ€§é—å¿˜å’Œéœ€è¦æ˜‚è´µé‡æ–°è®­ç»ƒçš„é£é™©ã€‚æŒç»­å­¦ä¹ ä¸ºé€æ­¥è·å–æ–°çŸ¥è¯†æä¾›äº†å‰æ™¯ï¼ŒåŒæ—¶ä¿ç•™ç°æœ‰èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“å‰æ–¹æ³•ä¾§é‡äºç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ï¼Œå¿½ç•¥äº†åŸºç¡€æ¨¡å‹è¿‡åº¦æ‹Ÿåˆç‰¹å®šæŒ‡ä»¤å¯¼è‡´çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼Œä»è€ŒæŸå®³äº†å…¶é€šç”¨èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§ç®€å•æœ‰æ•ˆçš„æ–¹æ³•å¯¹LLaVA-1.5è¿›è¡Œä¸¤é¡¹æ”¹è¿›ï¼šå…‰è°±æ„ŸçŸ¥æ•´åˆä»¥æ”¹è¿›ä»»åŠ¡å¹³è¡¡å’Œæ— ç›‘ç£æŸ¥è¯¢æ­£åˆ™åŒ–ä»¥é˜²æ­¢åŸºç¡€æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚æˆ‘ä»¬è¯„ä¼°æŒç»­é¢„è®­ç»ƒå’Œç²¾ç»†è°ƒæ•´è¿‡ç¨‹ä¸­çš„é€šç”¨å’Œä»»åŠ¡ç‰¹å®šæ€§èƒ½ã€‚å®éªŒè¡¨æ˜ï¼ŒLLaVA-cæé«˜äº†æ ‡å‡†åŸºå‡†æµ‹è¯•çš„æ€§èƒ½å¹¶ä¿æŒå…¶é€šç”¨èƒ½åŠ›ã€‚é¦–æ¬¡å±•ç¤ºæŒ‰ä»»åŠ¡é¡ºåºçš„è¿ç»­å­¦ä¹ å¯ä»¥è¾¾åˆ°ç”šè‡³è¶…è¿‡å¤šä»»åŠ¡è”åˆå­¦ä¹ çš„æ•ˆæœã€‚ä»£ç å°†å…¬å¼€å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€æ¨¡å‹å¦‚LLaVA-1.5é€šè¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´å®ç°å…ˆè¿›è§†è§‰ç†è§£ï¼Œæ”¯æŒå¤šä»»åŠ¡æ•°æ®é›†ä¸Šçš„å¼ºæŒ‡ä»¤éµå¾ªå’Œå¤šæ¨¡æ€æ€§èƒ½ã€‚</li>
<li>å¤šä»»åŠ¡å­¦ä¹ é¢ä¸´ä»»åŠ¡å¹³è¡¡å’Œæ‰©å±•æˆæœ¬æŒ‘æˆ˜ï¼Œéœ€è°ƒæ•´æ•°æ®æ¯”ä¾‹å¹¶åº”å¯¹ç¾éš¾æ€§é—å¿˜å’Œé‡æ–°è®­ç»ƒé£é™©ã€‚</li>
<li>æŒç»­å­¦ä¹ èƒ½é€æ­¥è·å–æ–°çŸ¥è¯†åŒæ—¶ä¿ç•™ç°æœ‰èƒ½åŠ›ï¼Œæ˜¯åº”å¯¹å¤šä»»åŠ¡å­¦ä¹ æŒ‘æˆ˜çš„æœ‰å‰é€”çš„æ–¹æ³•ã€‚</li>
<li>å½“å‰æ–¹æ³•é‡è§†ç‰¹å®šä»»åŠ¡æ€§èƒ½è€Œå¿½è§†åŸºç¡€æ¨¡å‹è¿‡åº¦æ‹Ÿåˆå¯¼è‡´çš„é€šç”¨èƒ½åŠ›ä¸‹é™é—®é¢˜ã€‚</li>
<li>æå‡ºçš„LLaVA-cæ¨¡å‹é€šè¿‡å…‰è°±æ„ŸçŸ¥æ•´åˆå’Œæ— ç›‘ç£æŸ¥è¯¢æ­£åˆ™åŒ–æ”¹è¿›ä»»åŠ¡å¹³è¡¡å¹¶é˜²æ­¢åŸºç¡€æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚</li>
<li>LLaVA-cåœ¨é¢„è®­ç»ƒå’Œç²¾ç»†è°ƒæ•´è¿‡ç¨‹ä¸­ä¿æŒäº†è‰¯å¥½çš„é€šç”¨å’Œä»»åŠ¡ç‰¹å®šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.08666">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-865e8bda7481f40f028572afb9b8dece.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-66f125ec5a9bd761b4a6960f8179e867.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-aa2e0d1034cfbdc9489c707222e1cdc7.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8c2a78e6382a441a72eb41128b924703.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-915cb02b371f411b4c79c1da79fd1ab3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-1893772bcaeff96415435098df0e75dd.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Improving-Large-Language-Models-with-Concept-Aware-Fine-Tuning"><a href="#Improving-Large-Language-Models-with-Concept-Aware-Fine-Tuning" class="headerlink" title="Improving Large Language Models with Concept-Aware Fine-Tuning"></a>Improving Large Language Models with Concept-Aware Fine-Tuning</h2><p><strong>Authors:Michael K. Chen, Xikun Zhang, Jiaxing Huang, Dacheng Tao</strong></p>
<p>Large language models (LLMs) have become the cornerstone of modern AI. However, the existing paradigm of next-token prediction fundamentally limits their ability to form coherent, high-level concepts, making it a critical barrier to human-like understanding and reasoning. Take the phrase â€œribonucleic acidâ€ as an example: an LLM will first decompose it into tokens, i.e., artificial text fragments (â€œribâ€, â€œonâ€, â€¦), then learn each token sequentially, rather than grasping the phrase as a unified, coherent semantic entity. This fragmented representation hinders deeper conceptual understanding and, ultimately, the development of truly intelligent systems. In response, we introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned. By enabling the learning of sequences that span multiple tokens, this method fosters stronger concept-aware learning. Our experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including traditional applications like text summarization and domain-specific ones like de novo protein design. Multi-token prediction was previously only possible in the prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first to bring the multi-token setting to the post-training phase, thus effectively democratizing its benefits for the broader community of practitioners and researchers. Finally, the unexpected effectiveness of our proposed method suggests wider implications for the machine learning research community. All code and data are available at <a target="_blank" rel="noopener" href="https://github.com/michaelchen-lab/caft-llm">https://github.com/michaelchen-lab/caft-llm</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²æˆä¸ºç°ä»£äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œé¢„æµ‹èŒƒå¼ä»æ ¹æœ¬ä¸Šé™åˆ¶äº†å®ƒä»¬å½¢æˆè¿è´¯ã€é«˜çº§æ¦‚å¿µçš„èƒ½åŠ›ï¼Œè¿™æˆä¸ºå®ç°äººç±»ç†è§£å’Œæ¨ç†çš„è‡³å…³é‡è¦çš„éšœç¢ã€‚ä»¥çŸ­è¯­â€œæ ¸ç³–æ ¸é…¸â€ä¸ºä¾‹ï¼šLLMä¼šé¦–å…ˆå°†å…¶åˆ†è§£æˆä»¤ç‰Œï¼Œå³äººå·¥æ–‡æœ¬ç‰‡æ®µï¼ˆâ€œribâ€ã€â€œonâ€â€¦â€¦ï¼‰ï¼Œç„¶åæŒ‰é¡ºåºå­¦ä¹ æ¯ä¸ªä»¤ç‰Œï¼Œè€Œä¸æ˜¯å°†æ•´ä¸ªçŸ­è¯­ä½œä¸ºä¸€ä¸ªç»Ÿä¸€ã€è¿è´¯çš„è¯­ä¹‰å®ä½“æ¥æŠŠæ¡ã€‚è¿™ç§ç¢ç‰‡åŒ–çš„è¡¨ç¤ºå½¢å¼é˜»ç¢äº†æ›´æ·±çš„æ¦‚å¿µç†è§£ï¼Œå¹¶æœ€ç»ˆé˜»ç¢äº†çœŸæ­£æ™ºèƒ½ç³»ç»Ÿçš„å¼€å‘ã€‚ä½œä¸ºå›åº”ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ¦‚å¿µæ„ŸçŸ¥å¾®è°ƒï¼ˆCAFTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šä»¤ç‰Œè®­ç»ƒæ–¹æ³•ï¼Œé‡æ–°å®šä¹‰äº†å¦‚ä½•å¾®è°ƒLLMã€‚é€šè¿‡æ”¯æŒå­¦ä¹ è·¨è¶Šå¤šä¸ªä»¤ç‰Œçš„åºåˆ—ï¼Œè¿™ç§æ–¹æ³•ä¿ƒè¿›äº†æ›´å¼ºçš„æ¦‚å¿µæ„ŸçŸ¥å­¦ä¹ ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä¸ä¼ ç»Ÿçš„ä¸‹ä¸€ä¸ªä»¤ç‰Œå¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒCAFTåœ¨å¤šç§ä»»åŠ¡ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿçš„æ–‡æœ¬æ‘˜è¦å’Œç‰¹å®šçš„åŸŸå†…ä»»åŠ¡å¦‚å…¨æ–°è›‹ç™½è´¨è®¾è®¡ã€‚ä»¥å‰ï¼Œå¤šä»¤ç‰Œé¢„æµ‹ä»…åœ¨æ˜‚è´µçš„é¢„è®­ç»ƒé˜¶æ®µæ‰å¯èƒ½å®ç°ï¼›æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒCAFTé¦–æ¬¡å°†å¤šä»¤ç‰Œè®¾ç½®å¼•å…¥åˆ°è®­ç»ƒåé˜¶æ®µï¼Œä»è€Œæœ‰æ•ˆåœ°æ™®åŠäº†å…¶å¯¹äºå¹¿å¤§ä»ä¸šè€…å’Œç ”ç©¶äººå‘˜çš„ç›Šå¤„ã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„å‡ºäººæ„æ–™çš„æœ‰æ•ˆæ€§è¡¨æ˜å¯¹æœºå™¨å­¦ä¹ ç ”ç©¶ç•Œæœ‰æ›´å¹¿æ³›çš„å½±å“ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®å‡å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/michaelchen-lab/caft-llm%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/michaelchen-lab/caft-llmæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07833v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯ç°ä»£äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒï¼Œä½†å…¶åŸºäºä¸‹ä¸€ä¸ªè¯é¢„æµ‹çš„ç°æœ‰èŒƒå¼é™åˆ¶äº†å…¶å½¢æˆè¿è´¯ã€é«˜çº§æ¦‚å¿µçš„èƒ½åŠ›ï¼Œæˆä¸ºå®ç°äººç±»ç†è§£å’Œæ¨ç†çš„ç“¶é¢ˆã€‚ä»¥â€œæ ¸ç³–æ ¸é…¸â€è¿™ä¸ªçŸ­è¯­ä¸ºä¾‹ï¼ŒLLMä¼šå°†å…¶åˆ†è§£æˆå¤šä¸ªå•ç‹¬çš„æ–‡æœ¬ç‰‡æ®µè¿›è¡Œå­¦ä¹ ï¼Œè€Œéä½œä¸ºä¸€ä¸ªç»Ÿä¸€ã€è¿è´¯çš„è¯­ä¹‰å®ä½“æ¥ç†è§£ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¦‚å¿µæ„ŸçŸ¥å¾®è°ƒï¼ˆCAFTï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„å¤šä»¤ç‰Œè®­ç»ƒæ–¹æ³•ï¼Œé‡æ–°å®šä¹‰äº†LLMçš„å¾®è°ƒæ–¹å¼ã€‚é€šè¿‡å…è®¸å­¦ä¹ è·¨è¶Šå¤šä¸ªä»¤ç‰Œçš„åºåˆ—ï¼Œè¯¥æ–¹æ³•ä¿ƒè¿›äº†æ›´å¼ºå¤§çš„æ¦‚å¿µæ„ŸçŸ¥å­¦ä¹ ã€‚å®éªŒè¯æ˜ï¼Œä¸ä¼ ç»Ÿçš„ä¸‹ä¸€ä¸ªè¯å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒCAFTåœ¨å„ç§ä»»åŠ¡ä¸Šéƒ½æœ‰æ˜¾è‘—æ”¹å–„ï¼ŒåŒ…æ‹¬æ–‡æœ¬æ‘˜è¦å’Œå»åˆ›è›‹ç™½è´¨è®¾è®¡ç­‰é¢†åŸŸã€‚CAFTå°†å¤šä»¤ç‰Œé¢„æµ‹ä»æ˜‚è´µçš„é¢„è®­ç»ƒé˜¶æ®µå¸¦åˆ°äº†åè®­ç»ƒé˜¶æ®µï¼Œä½¿å¹¿å¤§å®è·µè€…å’Œç ”ç©¶äººå‘˜éƒ½èƒ½å—ç›Šã€‚è¿™è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰æ›´å¹¿æ³›çš„æ„ä¹‰å’Œå½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMä½œä¸ºç°ä»£AIçš„æ ¸å¿ƒï¼Œåœ¨å½¢æˆè¿è´¯ã€é«˜çº§æ¦‚å¿µæ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>ç°æœ‰LLMå¤„ç†è¯­è¨€çš„æ–¹å¼æ˜¯å°†æ–‡æœ¬åˆ†è§£æˆå•ç‹¬æ–‡æœ¬ç‰‡æ®µï¼Œè¿™é™åˆ¶äº†å…¶æ·±åº¦ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>CAFTæ˜¯ä¸€ç§æ–°çš„å¤šä»¤ç‰Œè®­ç»ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿè§£å†³ä¸Šè¿°é—®é¢˜ï¼Œä¿ƒè¿›æ›´å¼ºå¤§çš„æ¦‚å¿µæ„ŸçŸ¥å­¦ä¹ ã€‚</li>
<li>CAFTé€šè¿‡å…è®¸å­¦ä¹ è·¨è¶Šå¤šä¸ªä»¤ç‰Œçš„åºåˆ—ï¼Œæ”¹å–„äº†LLMåœ¨å„ç§ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</li>
<li>ä¸ä¼ ç»Ÿçš„ä¸‹ä¸€ä¸ªè¯å¾®è°ƒæ–¹æ³•ç›¸æ¯”ï¼ŒCAFTå…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚</li>
<li>CAFTå°†å¤šä»¤ç‰Œé¢„æµ‹å¸¦åˆ°äº†åè®­ç»ƒé˜¶æ®µï¼Œä½¿å¹¿å¤§å®è·µè€…å’Œç ”ç©¶äººå‘˜éƒ½èƒ½å—ç›Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07833">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-27876032a9f09fb539db2a122d838988.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-8e046955ece797180c726565347dcf48.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-47834ac5db4caba404a6497cdcf2e8c1.jpg" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SAP-Bench-Benchmarking-Multimodal-Large-Language-Models-in-Surgical-Action-Planning"><a href="#SAP-Bench-Benchmarking-Multimodal-Large-Language-Models-in-Surgical-Action-Planning" class="headerlink" title="SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical   Action Planning"></a>SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical   Action Planning</h2><p><strong>Authors:Mengya Xu, Zhongzhen Huang, Dillan Imans, Yiru Ye, Xiaofan Zhang, Qi Dou</strong></p>
<p>Effective evaluation is critical for driving advancements in MLLM research. The surgical action planning (SAP) task, which aims to generate future action sequences from visual inputs, demands precise and sophisticated analytical capabilities. Unlike mathematical reasoning, surgical decision-making operates in life-critical domains and requires meticulous, verifiable processes to ensure reliability and patient safety. This task demands the ability to distinguish between atomic visual actions and coordinate complex, long-horizon procedures, capabilities that are inadequately evaluated by current benchmarks. To address this gap, we introduce SAP-Bench, a large-scale, high-quality dataset designed to enable multimodal large language models (MLLMs) to perform interpretable surgical action planning. Our SAP-Bench benchmark, derived from the cholecystectomy procedures context with the mean duration of 1137.5s, and introduces temporally-grounded surgical action annotations, comprising the 1,226 clinically validated action clips (mean duration: 68.7s) capturing five fundamental surgical actions across 74 procedures. The dataset provides 1,152 strategically sampled current frames, each paired with the corresponding next action as multimodal analysis anchors. We propose the MLLM-SAP framework that leverages MLLMs to generate next action recommendations from the current surgical scene and natural language instructions, enhanced with injected surgical domain knowledge. To assess our datasetâ€™s effectiveness and the broader capabilities of current models, we evaluate seven state-of-the-art MLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5, Step-1o, and GLM-4v) and reveal critical gaps in next action prediction performance. </p>
<blockquote>
<p>æœ‰æ•ˆçš„è¯„ä¼°æ˜¯æ¨åŠ¨MLLMç ”ç©¶è¿›æ­¥çš„å…³é”®ã€‚æ‰‹æœ¯è¡ŒåŠ¨è§„åˆ’ï¼ˆSAPï¼‰ä»»åŠ¡æ—¨åœ¨ä»è§†è§‰è¾“å…¥ç”Ÿæˆæœªæ¥è¡ŒåŠ¨åºåˆ—ï¼Œå®ƒè¦æ±‚ç²¾ç¡®è€Œå¤æ‚çš„åˆ†æèƒ½åŠ›ã€‚ä¸åŒäºæ•°å­¦æ¨ç†ï¼Œæ‰‹æœ¯å†³ç­–æ˜¯åœ¨å…³é”®çš„ç”Ÿå‘½é¢†åŸŸä¸­è¿›è¡Œï¼Œéœ€è¦ä¸¥è°¨ã€å¯éªŒè¯çš„è¿‡ç¨‹æ¥ä¿è¯å¯é æ€§å’Œç—…äººå®‰å…¨ã€‚è¿™é¡¹ä»»åŠ¡éœ€è¦åŒºåˆ†åŸå­è§†è§‰è¡ŒåŠ¨ï¼Œå¹¶åè°ƒå¤æ‚ã€é•¿æœŸè§„åˆ’çš„ç¨‹åºï¼Œè€Œå½“å‰çš„æ ‡å‡†è¯„ä¼°å¹¶ä¸èƒ½å……åˆ†è¯„ä¼°è¿™äº›èƒ½åŠ›ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†SAP-Benchï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œæ—¨åœ¨ä½¿å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤Ÿæ‰§è¡Œå¯è§£é‡Šçš„æ‰‹æœ¯è¡ŒåŠ¨è§„åˆ’ã€‚æˆ‘ä»¬çš„SAP-BenchåŸºå‡†æµ‹è¯•æºäºèƒ†å›Šåˆ‡é™¤æœ¯çš„ä¸Šä¸‹æ–‡ç¯å¢ƒï¼Œå¹³å‡æŒç»­æ—¶é—´ä¸º1137.5ç§’ï¼Œå¼•å…¥åŸºäºæ—¶é—´çš„æ‰‹æœ¯è¡ŒåŠ¨æ³¨é‡Šï¼ŒåŒ…å«1226ä¸ªç»è¿‡ä¸´åºŠéªŒè¯çš„è¡ŒåŠ¨ç‰‡æ®µï¼ˆå¹³å‡æŒç»­æ—¶é—´ï¼š68.7ç§’ï¼‰ï¼Œæ¶µç›–äº†74ä¸ªç¨‹åºä¸­çš„äº”ä¸ªåŸºæœ¬æ‰‹æœ¯è¡ŒåŠ¨ã€‚æ•°æ®é›†æä¾›äº†1152ä¸ªæˆ˜ç•¥é‡‡æ ·çš„å½“å‰å¸§ï¼Œæ¯ä¸ªå¸§éƒ½ä¸ç›¸åº”çš„ä¸‹ä¸€ä¸ªè¡ŒåŠ¨ä½œä¸ºå¤šæ¨¡æ€åˆ†æé”šç‚¹é…å¯¹ã€‚æˆ‘ä»¬æå‡ºäº†MLLM-SAPæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨MLLMsæ ¹æ®å½“å‰æ‰‹æœ¯åœºæ™¯å’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆä¸‹ä¸€ä¸ªè¡ŒåŠ¨å»ºè®®ï¼Œå¹¶æ³¨å…¥äº†æ‰‹æœ¯é¢†åŸŸçš„çŸ¥è¯†è¿›è¡Œå¢å¼ºã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬æ•°æ®é›†çš„æœ‰æ•ˆæ€§å’Œå½“å‰æ¨¡å‹çš„ç»¼åˆèƒ½åŠ›ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸ƒä¸ªæœ€æ–°MLLMsï¼ˆä¾‹å¦‚OpenAI-o1ã€GPT-4oã€QwenVL2.5-72Bã€Claude-3.5-Sonnetã€GeminiPro2.5ã€Step-1oå’ŒGLM-4vï¼‰ï¼Œå¹¶æ­ç¤ºäº†ä¸‹ä¸€æ­¥è¡ŒåŠ¨é¢„æµ‹æ€§èƒ½ä¸­çš„å…³é”®å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.07196v2">PDF</a> The authors could not reach a consensus on the final version of this   paper, necessitating its withdrawal</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ‰æ•ˆè¯„ä¼°æ˜¯æ¨åŠ¨MLLMç ”ç©¶è¿›æ­¥çš„å…³é”®ã€‚æ‰‹æœ¯è¡ŒåŠ¨è§„åˆ’ï¼ˆSAPï¼‰ä»»åŠ¡æ—¨åœ¨ä»è§†è§‰è¾“å…¥ç”Ÿæˆæœªæ¥è¡ŒåŠ¨åºåˆ—ï¼Œéœ€è¦ç²¾ç¡®è€Œé«˜çº§çš„åˆ†æèƒ½åŠ›ã€‚ä¸åŒäºæ•°å­¦æ¨ç†ï¼Œæ‰‹æœ¯å†³ç­–æ˜¯åœ¨ç”Ÿå‘½æ”¸å…³é¢†åŸŸè¿›è¡Œçš„ï¼Œéœ€è¦ä¸¥æ ¼çš„å¯éªŒè¯è¿‡ç¨‹ä»¥ç¡®ä¿å¯é æ€§å’Œæ‚£è€…å®‰å…¨ã€‚æ­¤ä»»åŠ¡è¦æ±‚åŒºåˆ†åŸå­è§†è§‰è¡ŒåŠ¨å¹¶åè°ƒå¤æ‚ã€é•¿æœŸè§„åˆ’çš„ç¨‹åºï¼Œè€Œå½“å‰åŸºå‡†æµ‹è¯•æ— æ³•å……åˆ†è¯„ä¼°è¿™äº›èƒ½åŠ›ã€‚ä¸ºè§£å†³æ­¤å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºSAP-Benchï¼Œä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œæ—¨åœ¨ä½¿å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰èƒ½å¤Ÿæ‰§è¡Œå¯è§£é‡Šçš„æ‰‹æœ¯è¡ŒåŠ¨è§„åˆ’ã€‚SAP-BenchåŸºå‡†æµ‹è¯•æºäºèƒ†å›Šåˆ‡é™¤æœ¯çš„ä¸Šä¸‹æ–‡ç¯å¢ƒï¼Œå¹³å‡æŒç»­æ—¶é—´ä¸º1137.5ç§’ï¼Œå¹¶å¼•å…¥äº†æ—¶é—´åŸºç¡€çš„æ‰‹æœ¯è¡ŒåŠ¨æ³¨é‡Šï¼ŒåŒ…å«1226ä¸ªç»è¿‡ä¸´åºŠéªŒè¯çš„è¡ŒåŠ¨ç‰‡æ®µï¼ˆå¹³å‡æŒç»­æ—¶é—´ï¼š68.7ç§’ï¼‰ï¼Œæ¶µç›–74ä¸ªç¨‹åºä¸­çš„äº”ä¸ªåŸºæœ¬æ‰‹æœ¯è¡ŒåŠ¨ã€‚æ•°æ®é›†æä¾›äº†æˆ˜ç•¥æŠ½æ ·çš„å½“å‰å¸§ï¼Œæ¯ä¸€å¸§éƒ½é…å¯¹æœ‰ç›¸åº”çš„ä¸‹ä¸€ä¸ªè¡ŒåŠ¨ä½œä¸ºå¤šæ¨¡æ€åˆ†æé”šç‚¹ã€‚æˆ‘ä»¬æå‡ºäº†MLLM-SAPæ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨MLLMsä»å½“å‰æ‰‹æœ¯åœºæ™¯å’Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆä¸‹ä¸€ä¸ªè¡ŒåŠ¨å»ºè®®ï¼Œå¹¶è¾…ä»¥æ³¨å…¥çš„æ‰‹æœ¯é¢†åŸŸçŸ¥è¯†ã€‚ä¸ºäº†è¯„ä¼°æ•°æ®é›†çš„æ•ˆç”¨ä»¥åŠå½“å‰æ¨¡å‹çš„æ•´ä½“èƒ½åŠ›ï¼Œæˆ‘ä»¬å¯¹ä¸ƒé¡¹æœ€å…ˆè¿›çš„MLLMè¿›è¡Œäº†è¯„ä¼°ï¼ˆä¾‹å¦‚OpenAI-o1ã€GPT-4oç­‰ï¼‰ï¼Œå¹¶æ­ç¤ºäº†ä¸‹ä¸€æ­¥è¡ŒåŠ¨é¢„æµ‹æ€§èƒ½çš„å…³é”®å·®è·ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>æœ‰æ•ˆè¯„ä¼°å¯¹äºæ¨åŠ¨MLLMåœ¨æ‰‹æœ¯è¡ŒåŠ¨è§„åˆ’ç­‰é¢†åŸŸçš„è¿›æ­¥è‡³å…³é‡è¦ã€‚</li>
<li>å½“å‰åŸºå‡†æµ‹è¯•æ— æ³•å……åˆ†è¯„ä¼°åŒºåˆ†åŸå­è§†è§‰è¡ŒåŠ¨å¹¶åè°ƒå¤æ‚ç¨‹åºçš„èƒ½åŠ›ã€‚</li>
<li>SAP-Benchæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œæ—¨åœ¨æ”¯æŒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ‰‹æœ¯è¡ŒåŠ¨è§„åˆ’ä»»åŠ¡ã€‚</li>
<li>SAP-Benchæ•°æ®é›†åŒ…å«ç»è¿‡ä¸´åºŠéªŒè¯çš„æ‰‹æœ¯è¡ŒåŠ¨ç‰‡æ®µå’Œå½“å‰å¸§çš„ç²¾ç¡®æ•°æ®ã€‚</li>
<li>MLLM-SAPæ¡†æ¶åˆ©ç”¨å¤šæ¨¡æ€åˆ†æå¹¶ç»“åˆæ‰‹æœ¯é¢†åŸŸçŸ¥è¯†ç”Ÿæˆä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®ã€‚</li>
<li>å¯¹å¤šä¸ªå…ˆè¿›æ¨¡å‹çš„è¯„ä¼°è¡¨æ˜åœ¨é¢„æµ‹æ‰‹æœ¯ä¸‹ä¸€æ­¥è¡ŒåŠ¨æ–¹é¢å­˜åœ¨æ€§èƒ½å·®è·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07196">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bcbb5218d3389e2280a57ebee88b6c73.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-a0fe387651cf6874e91f7a3ef6e5ff1a.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-21c2cd3c30c80e6da7711cc3da037de1.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a8c5500f1b534cb4188e769b52cfaffd.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-c54ea29de62ec49189a27f209a26308b.jpg" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Long-context-Non-factoid-Question-Answering-in-Indic-Languages"><a href="#Long-context-Non-factoid-Question-Answering-in-Indic-Languages" class="headerlink" title="Long-context Non-factoid Question Answering in Indic Languages"></a>Long-context Non-factoid Question Answering in Indic Languages</h2><p><strong>Authors:Ritwik Mishra, Rajiv Ratn Shah, Ponnurangam Kumaraguru</strong></p>
<p>Question Answering (QA) tasks, which involve extracting answers from a given context, are relatively straightforward for modern Large Language Models (LLMs) when the context is short. However, long contexts pose challenges due to the quadratic complexity of the self-attention mechanism. This challenge is compounded in Indic languages, which are often low-resource. This study explores context-shortening techniques, including Open Information Extraction (OIE), coreference resolution, Answer Paragraph Selection (APS), and their combinations, to improve QA performance. Compared to the baseline of unshortened (long) contexts, our experiments on four Indic languages (Hindi, Tamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield an average improvement of 4% in semantic scores and 47% in token-level scores when evaluated on three popular LLMs without fine-tuning. Furthermore, with fine-tuning, we achieve an average increase of 2% in both semantic and token-level scores. Additionally, context-shortening reduces computational overhead. Explainability techniques like LIME and SHAP reveal that when the APS model confidently identifies the paragraph containing the answer, nearly all tokens within the selected text receive high relevance scores. However, the study also highlights the limitations of LLM-based QA systems in addressing non-factoid questions, particularly those requiring reasoning or debate. Moreover, verbalizing OIE-generated triples does not enhance system performance. These findings emphasize the potential of context-shortening techniques to improve the efficiency and effectiveness of LLM-based QA systems, especially for low-resource languages. The source code and resources are available at <a target="_blank" rel="noopener" href="https://github.com/ritwikmishra/IndicGenQA">https://github.com/ritwikmishra/IndicGenQA</a>. </p>
<blockquote>
<p>é—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡å¯¹äºç°ä»£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è¯´ï¼Œå½“ä¸Šä¸‹æ–‡ç®€çŸ­æ—¶ï¼Œç›´æ¥ä»ä¸Šä¸‹æ–‡ä¸­æå–ç­”æ¡ˆç›¸å¯¹ç®€å•ã€‚ç„¶è€Œï¼Œé•¿ä¸Šä¸‹æ–‡ç”±äºè‡ªæ³¨æ„æœºåˆ¶çš„äºŒæ¬¡å¤æ‚æ€§è€Œå¸¦æ¥æŒ‘æˆ˜ã€‚åœ¨å°åº¦è¯­è¨€ï¼ˆé€šå¸¸èµ„æºè¾ƒå°‘ï¼‰ä¸­ï¼Œè¿™ä¸€æŒ‘æˆ˜æ›´ä¸ºä¸¥é‡ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä¸Šä¸‹æ–‡ç¼©çŸ­æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¼€æ”¾ä¿¡æ¯æå–ï¼ˆOIEï¼‰ã€æ ¸å¿ƒå¼•ç”¨è§£æã€ç­”æ¡ˆæ®µè½é€‰æ‹©ï¼ˆAPSï¼‰åŠå…¶ç»„åˆï¼Œä»¥æé«˜é—®ç­”æ€§èƒ½ã€‚ä¸æœªç¼©çŸ­ï¼ˆé•¿ï¼‰ä¸Šä¸‹æ–‡çš„åŸºçº¿ç›¸æ¯”ï¼Œæˆ‘ä»¬åœ¨å››ç§å°åº¦è¯­è¨€ï¼ˆå°åœ°è¯­ã€æ³°ç±³å°”è¯­ã€æ³°å¢å›ºè¯­å’Œä¹Œå°”éƒ½è¯­ï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸‰ç§æµè¡Œçš„LLMä¸Šè¯„ä¼°æ—¶ï¼Œä¸Šä¸‹æ–‡ç¼©çŸ­æŠ€æœ¯åœ¨è¯­ä¹‰å¾—åˆ†ä¸Šå¹³å‡æé«˜äº†4%ï¼Œåœ¨ä»¤ç‰Œçº§åˆ«å¾—åˆ†ä¸Šæé«˜äº†47%ï¼Œæ— éœ€å¾®è°ƒã€‚æ­¤å¤–ï¼Œé€šè¿‡å¾®è°ƒï¼Œæˆ‘ä»¬åœ¨è¯­ä¹‰å’Œä»¤ç‰Œçº§åˆ«å¾—åˆ†ä¸Šéƒ½å¹³å‡æé«˜äº†2%ã€‚æ­¤å¤–ï¼Œä¸Šä¸‹æ–‡ç¼©çŸ­è¿˜å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚LIMEå’ŒSHAPç­‰è§£é‡Šæ€§æŠ€æœ¯è¡¨æ˜ï¼Œå½“APSæ¨¡å‹è‡ªä¿¡åœ°è¯†åˆ«å‡ºåŒ…å«ç­”æ¡ˆçš„æ®µè½æ—¶ï¼Œæ‰€é€‰æ–‡æœ¬ä¸­çš„å‡ ä¹æ‰€æœ‰ä»¤ç‰Œéƒ½ä¼šè·å¾—é«˜ç›¸å…³æ€§åˆ†æ•°ã€‚ç„¶è€Œï¼Œè¯¥ç ”ç©¶è¿˜å¼ºè°ƒäº†åŸºäºLLMçš„é—®ç­”ç³»ç»Ÿåœ¨è§£å†³éäº‹å®é—®é¢˜ä¸Šçš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯é‚£äº›éœ€è¦æ¨ç†æˆ–è¾©è®ºçš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œç”¨è¨€è¯­è¡¨è¾¾OIEç”Ÿæˆçš„ä¸‰å…ƒç»„å¹¶ä¸èƒ½æé«˜ç³»ç»Ÿæ€§èƒ½ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†ä¸Šä¸‹æ–‡ç¼©çŸ­æŠ€æœ¯åœ¨æé«˜åŸºäºLLMçš„é—®ç­”ç³»ç»Ÿçš„æ•ˆç‡å’Œæœ‰æ•ˆæ€§æ–¹é¢çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹äºä½èµ„æºè¯­è¨€ã€‚æºä»£ç å’Œèµ„æºå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ritwikmishra/IndicGenQA%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ritwikmishra/IndicGenQAæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.13615v2">PDF</a> Short version of this manuscript accepted at   <a target="_blank" rel="noopener" href="https://bda2025.iiitb.net/">https://bda2025.iiitb.net/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé—®ç­”ä»»åŠ¡æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é•¿æ–‡æœ¬è¯­å¢ƒä¸‹çš„æŒ‘æˆ˜ã€‚ç ”ç©¶é’ˆå¯¹å°åº¦è¯­è¨€ï¼ˆå¦‚å°åœ°è¯­ã€æ³°ç±³å°”è¯­ã€æ³°å¢å›ºè¯­å’Œä¹Œå°”éƒ½è¯­ï¼‰æå‡ºäº†ä¸Šä¸‹æ–‡ç¼©çŸ­æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¼€æ”¾ä¿¡æ¯æå–ï¼ˆOIEï¼‰ã€æ ¸å¿ƒè§£å†³ï¼ˆcoreference resolutionï¼‰ã€ç­”æ¡ˆæ®µè½é€‰æ‹©ï¼ˆAPSï¼‰åŠå…¶ç»„åˆã€‚å®éªŒè¡¨æ˜ï¼Œä¸Šä¸‹æ–‡ç¼©çŸ­æŠ€æœ¯åœ¨ä¸å¾®è°ƒçš„æƒ…å†µä¸‹å¹³å‡æé«˜äº†è¯­ä¹‰å¾—åˆ†4%ï¼Œæé«˜äº†ä»¤ç‰Œçº§åˆ«çš„å¾—åˆ†47%ã€‚æ­¤å¤–ï¼Œè¿™äº›æŠ€æœ¯è¿˜å¯ä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚ç„¶è€Œï¼Œè¯¥ç ”ç©¶ä¹ŸæŒ‡å‡ºäº†LLMåœ¨è§£å†³éäº‹å®æ€§é—®é¢˜æ–¹é¢çš„å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯éœ€è¦æ¨ç†æˆ–è¾©è®ºçš„é—®é¢˜ã€‚ç ”ç©¶å¼ºè°ƒäº†ä¸Šä¸‹æ–‡ç¼©çŸ­æŠ€æœ¯åœ¨æé«˜LLMé—®ç­”ç³»ç»Ÿçš„æ•ˆç‡å’Œæœ‰æ•ˆæ€§æ–¹é¢çš„æ½œåŠ›ã€‚ç›¸å…³èµ„æºå·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é—®ç­”ä»»åŠ¡ä¸­å¤„ç†é•¿æ–‡æœ¬æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ä½èµ„æºçš„å°åº¦è¯­è¨€ã€‚</li>
<li>ä¸Šä¸‹æ–‡ç¼©çŸ­æŠ€æœ¯å¦‚å¼€æ”¾ä¿¡æ¯æå–ï¼ˆOIEï¼‰ã€æ ¸å¿ƒè§£å†³å’Œç­”æ¡ˆæ®µè½é€‰æ‹©ï¼ˆAPSï¼‰èƒ½æœ‰æ•ˆæé«˜é—®ç­”æ€§èƒ½ã€‚</li>
<li>åœ¨ä¸å¾®è°ƒçš„æƒ…å†µä¸‹ï¼Œä¸Šä¸‹æ–‡ç¼©çŸ­æŠ€æœ¯å¹³å‡æé«˜äº†è¯­ä¹‰å¾—åˆ†å’Œä»¤ç‰Œçº§åˆ«å¾—åˆ†ã€‚</li>
<li>ä¸Šä¸‹æ–‡ç¼©çŸ­æŠ€æœ¯å¯ä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚</li>
<li>LLMåœ¨è§£å†³éäº‹å®æ€§é—®é¢˜æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨éœ€è¦æ¨ç†æˆ–è¾©è®ºçš„æƒ…å†µä¸‹ã€‚</li>
<li>è§£é‡Šæ€§æŠ€æœ¯å¦‚LIMEå’ŒSHAPå¯ä»¥å¸®åŠ©ç†è§£æ¨¡å‹å†³ç­–è¿‡ç¨‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.13615">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic1.zhimg.com/v2-2d5c5506b0cd8d8b2d96cde09a27a902.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-9d221a177f0ac4d126de2305439d15ba.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-8e742315353e1fc5a62b6daf13b82b00.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-6e15b154aa5f1169f87429a9b1095b98.jpg" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Safer-or-Luckier-LLMs-as-Safety-Evaluators-Are-Not-Robust-to-Artifacts"><a href="#Safer-or-Luckier-LLMs-as-Safety-Evaluators-Are-Not-Robust-to-Artifacts" class="headerlink" title="Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts"></a>Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts</h2><p><strong>Authors:Hongyu Chen, Seraphina Goldfarb-Tarrant</strong></p>
<p>Large Language Models (LLMs) are increasingly employed as automated evaluators to assess the safety of generated content, yet their reliability in this role remains uncertain. This study evaluates a diverse set of 11 LLM judge models across critical safety domains, examining three key aspects: self-consistency in repeated judging tasks, alignment with human judgments, and susceptibility to input artifacts such as apologetic or verbose phrasing. Our findings reveal that biases in LLM judges can significantly distort the final verdict on which content source is safer, undermining the validity of comparative evaluations. Notably, apologetic language artifacts alone can skew evaluator preferences by up to 98%. Contrary to expectations, larger models do not consistently exhibit greater robustness, while smaller models sometimes show higher resistance to specific artifacts. To mitigate LLM evaluator robustness issues, we investigate jury-based evaluations aggregating decisions from multiple models. Although this approach both improves robustness and enhances alignment to human judgements, artifact sensitivity persists even with the best jury configurations. These results highlight the urgent need for diversified, artifact-resistant methodologies to ensure reliable safety assessments. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¶Šæ¥è¶Šå¤šåœ°è¢«ç”¨ä½œè‡ªåŠ¨åŒ–è¯„ä¼°å™¨æ¥è¯„ä¼°ç”Ÿæˆå†…å®¹çš„å®‰å…¨æ€§ï¼Œä½†å®ƒä»¬åœ¨è¿™ä¸€è§’è‰²ä¸­çš„å¯é æ€§ä»ç„¶ä¸ç¡®å®šã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†11ä¸ªLLMåˆ¤æ–­æ¨¡å‹åœ¨ä¸åŒå…³é”®å®‰å…¨é¢†åŸŸçš„è¡¨ç°ï¼Œå¹¶è€ƒå¯Ÿäº†ä¸‰ä¸ªæ–¹é¢ï¼šé‡å¤åˆ¤æ–­ä»»åŠ¡ä¸­çš„è‡ªæˆ‘ä¸€è‡´æ€§ã€ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§ä»¥åŠå—è¯¸å¦‚é“æ­‰æˆ–å†—é•¿æªè¾ç­‰è¾“å…¥å› ç´ çš„å½±å“ç¨‹åº¦ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°ï¼ŒLLMåˆ¤æ–­ä¸­çš„åè§ä¼šä¸¥é‡æ‰­æ›²å¯¹å“ªä¸ªå†…å®¹æºæ›´å®‰å…¨çš„æœ€ç»ˆåˆ¤æ–­ï¼ŒæŸå®³æ¯”è¾ƒè¯„ä¼°çš„æœ‰æ•ˆæ€§ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»…ä»…é“æ­‰çš„è¯­è¨€ç‰¹å¾æœ¬èº«å°±å¯ä»¥ä½¿è¯„ä¼°è€…çš„åå¥½äº§ç”Ÿé«˜è¾¾98%çš„åå·®ã€‚ä¸é¢„æœŸç›¸åï¼Œæ›´å¤§çš„æ¨¡å‹å¹¶ä¸æ€»æ˜¯è¡¨ç°å‡ºæ›´é«˜çš„ç¨³å¥æ€§ï¼Œè€Œè¾ƒå°çš„æ¨¡å‹æœ‰æ—¶ä¼šå¯¹æŸäº›ç‰¹å¾è¡¨ç°å‡ºæ›´é«˜çš„æŠµæŠ—åŠ›ã€‚ä¸ºäº†è§£å†³LLMè¯„ä¼°å™¨çš„ç¨³å¥æ€§é—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŸºäºé™ªå®¡å›¢çš„è¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•èšåˆäº†å¤šä¸ªæ¨¡å‹çš„å†³ç­–ã€‚å°½ç®¡è¿™ç§æ–¹æ³•æé«˜äº†ç¨³å¥æ€§å¹¶å¢å¼ºäº†ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§ï¼Œä½†å³ä½¿åœ¨æœ€ä½³çš„é™ªå®¡å›¢é…ç½®ä¸‹ï¼Œå¯¹ç‰¹å¾çš„æ•æ„Ÿæ€§ä»ç„¶å­˜åœ¨ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†é‡‡ç”¨å¤šæ ·åŒ–ã€æŠ—ç‰¹å¾å¹²æ‰°æ–¹æ³•çš„è¿«åˆ‡éœ€æ±‚ï¼Œä»¥ç¡®ä¿å¯é çš„å®‰å…¨è¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09347v2">PDF</a> 9 pages, ACL 2025</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«è¶Šæ¥è¶Šå¤šåœ°ç”¨ä½œè‡ªåŠ¨ç”Ÿæˆå†…å®¹çš„è¯„ä¼°å™¨ï¼Œä½†å…¶å¯é æ€§å°šä¸ç¡®å®šã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸åŒå®‰å…¨é¢†åŸŸä¸­çš„11ä¸ªLLMåˆ¤æ–­æ¨¡å‹ï¼Œå¹¶è€ƒå¯Ÿäº†å…¶åœ¨é‡å¤åˆ¤æ–­ä»»åŠ¡ä¸­çš„è‡ªæˆ‘ä¸€è‡´æ€§ã€ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§ä»¥åŠæ˜¯å¦å®¹æ˜“å—åˆ°å¦‚é“æ­‰æˆ–å†—é•¿æªè¾ç­‰è¾“å…¥ä¼ªå½±çš„å½±å“ã€‚ç ”ç©¶å‘ç°ï¼ŒLLMåˆ¤æ–­ä¸­çš„åè§ä¼šä¸¥é‡æ‰­æ›²å…³äºå“ªä¸ªå†…å®¹æºæ›´å®‰å…¨çš„æœ€ç»ˆåˆ¤æ–­ï¼Œä»è€Œå½±å“æ¯”è¾ƒè¯„ä¼°çš„æœ‰æ•ˆæ€§ã€‚ä»…é“æ­‰æ€§è¯­è¨€ä¼ªå½±å°±èƒ½ä½¿è¯„ä¼°è€…çš„åå¥½åç¦»è¾¾98%ã€‚å‡ºäººæ„æ–™çš„æ˜¯ï¼Œè¾ƒå¤§çš„æ¨¡å‹å¹¶ä¸æ€»èƒ½è¡¨ç°å‡ºæ›´é«˜çš„ç¨³å¥æ€§ï¼Œè€Œè¾ƒå°çš„æ¨¡å‹åœ¨æŸäº›æƒ…å†µä¸‹åˆ™æ˜¾ç¤ºå‡ºå¯¹ç‰¹å®šä¼ªå½±çš„æ›´é«˜æŠµæŠ—åŠ›ã€‚ä¸ºäº†è§£å†³LLMè¯„ä¼°å™¨çš„ç¨³å¥æ€§é—®é¢˜ï¼Œæœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºé™ªå®¡å›¢çš„è¯„ä¼°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†å¤šä¸ªæ¨¡å‹ã€‚å°½ç®¡è¿™ç§æ–¹æ³•æ—¢æé«˜äº†ç¨³å¥æ€§åˆå¢å¼ºäº†ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§ï¼Œä½†æœ€ä½³é™ªå®¡å›¢é…ç½®ä»å­˜åœ¨ä¼ªå½±æ•æ„Ÿæ€§ã€‚è¿™äº›ç»“æœå¼ºè°ƒäº†é‡‡ç”¨å¤šæ ·åŒ–ã€æŠ—ä¼ªå½±æ–¹æ³•çš„ç´§è¿«æ€§ï¼Œä»¥ç¡®ä¿å¯é çš„å®‰å…¨è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¢«ç”¨ä½œå†…å®¹å®‰å…¨è¯„ä¼°å™¨çš„å¯é æ€§å°šä¸ç¡®å®šã€‚</li>
<li>LLMåˆ¤æ–­æ¨¡å‹åœ¨è‡ªæˆ‘ä¸€è‡´æ€§ã€ä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§å’Œè¾“å…¥ä¼ªå½±çš„æ•æ„Ÿæ€§æ–¹é¢å­˜åœ¨å…³é”®ç¼ºé™·ã€‚</li>
<li>åè§å¯èƒ½å½±å“LLMå¯¹å†…å®¹å®‰å…¨æ€§çš„æœ€ç»ˆåˆ¤æ–­ã€‚</li>
<li>é“æ­‰æ€§è¯­è¨€ä¼ªå½±å¯æ˜¾è‘—å½±å“è¯„ä¼°ç»“æœã€‚</li>
<li>è¾ƒå¤§çš„æ¨¡å‹å¹¶ä¸ä¸€å®šæ¯”å°å‹æ¨¡å‹æ›´ç¨³å¥ã€‚</li>
<li>åŸºäºé™ªå®¡å›¢çš„è¯„ä¼°æ–¹æ³•å¯æé«˜ç¨³å¥æ€§å’Œä¸äººç±»åˆ¤æ–­çš„ä¸€è‡´æ€§ï¼Œä½†ä»å­˜åœ¨ä¼ªå½±æ•æ„Ÿæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09347">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f74df1619d529ccda97aaaa945f26a92.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-a746d5885d914e04fd4e24683ab6a92c.jpg" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="New-Dataset-and-Methods-for-Fine-Grained-Compositional-Referring-Expression-Comprehension-via-Specialist-MLLM-Collaboration"><a href="#New-Dataset-and-Methods-for-Fine-Grained-Compositional-Referring-Expression-Comprehension-via-Specialist-MLLM-Collaboration" class="headerlink" title="New Dataset and Methods for Fine-Grained Compositional Referring   Expression Comprehension via Specialist-MLLM Collaboration"></a>New Dataset and Methods for Fine-Grained Compositional Referring   Expression Comprehension via Specialist-MLLM Collaboration</h2><p><strong>Authors:Xuzheng Yang, Junzhuo Liu, Peng Wang, Guoqing Wang, Yang Yang, Heng Tao Shen</strong></p>
<p>Referring Expression Comprehension (REC) is a foundational cross-modal task that evaluates the interplay of language understanding, image comprehension, and language-to-image grounding. It serves as an essential testing ground for Multimodal Large Language Models (MLLMs). To advance this field, we introduced a new REC dataset in our previous conference paper, characterized by two key features. First, it is designed with controllable difficulty levels, requiring multi-level fine-grained reasoning across object categories, attributes, and multi-hop relationships. Second, it incorporates negative text and images generated through fine-grained editing and augmentation, explicitly testing a modelâ€™s ability to reject scenarios where the target object is absent, an often overlooked yet critical challenge in existing datasets. In this extended work, we propose two new methods to tackle the challenges of fine-grained REC by combining the strengths of Specialist Models and MLLMs. The first method adaptively assigns simple cases to faster, lightweight models and reserves complex ones for powerful MLLMs, balancing accuracy and efficiency. The second method lets a specialist generate a set of possible object regions, and the MLLM selects the most plausible one using its reasoning ability. These collaborative strategies lead to significant improvements on our dataset and other challenging benchmarks. Our results show that combining specialized and general-purpose models offers a practical path toward solving complex real-world vision-language tasks. Our dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/sleepyshep/FineCops-Ref">https://github.com/sleepyshep/FineCops-Ref</a>. </p>
<blockquote>
<p>æŒ‡ä»£è¡¨è¾¾å¼ç†è§£ï¼ˆRECï¼‰æ˜¯ä¸€é¡¹åŸºç¡€çš„å¤šæ¨¡å¼ä»»åŠ¡ï¼Œå®ƒè¯„ä¼°è¯­è¨€ç†è§£ã€å›¾åƒç†è§£å’Œè¯­è¨€åˆ°å›¾åƒå®šä½ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚å®ƒæ˜¯æµ‹è¯•å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„é‡è¦åœºæ‰€ã€‚ä¸ºäº†æ¨åŠ¨è¿™ä¸€é¢†åŸŸçš„å‘å±•ï¼Œæˆ‘ä»¬åœ¨ä¹‹å‰çš„ä¼šè®®è®ºæ–‡ä¸­å¼•å…¥äº†ä¸€ä¸ªæ–°çš„RECæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å…·æœ‰ä¸¤ä¸ªå…³é”®ç‰¹å¾ã€‚é¦–å…ˆï¼Œå®ƒæŒ‰å¯æ§çš„éš¾åº¦çº§åˆ«è®¾è®¡ï¼Œéœ€è¦è¿›è¡Œè·¨å¯¹è±¡ç±»åˆ«ã€å±æ€§å’Œå¤šè·³å…³ç³»çš„å¤šçº§ç²¾ç»†æ¨ç†ã€‚å…¶æ¬¡ï¼Œå®ƒç»“åˆäº†é€šè¿‡ç²¾ç»†ç¼–è¾‘å’Œæ‰©å……ç”Ÿæˆçš„è´Ÿæ–‡æœ¬å’Œå›¾åƒï¼Œæ˜ç¡®æµ‹è¯•äº†æ¨¡å‹åœ¨æ‹’ç»ç›®æ ‡å¯¹è±¡ç¼ºå¤±çš„åœºæ™¯æ—¶çš„èƒ½åŠ›ï¼Œè¿™æ˜¯ç°æœ‰æ•°æ®é›†ä¸­ç»å¸¸è¢«å¿½è§†ä½†è‡³å…³é‡è¦çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹æ‰©å±•å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°çš„æ–¹æ³•æ¥è§£å†³ç²¾ç»†RECçš„æŒ‘æˆ˜ï¼Œé€šè¿‡å°†ä¸“ä¸šæ¨¡å‹å’ŒMLLMçš„ä¼˜åŠ¿ç»“åˆèµ·æ¥ã€‚ç¬¬ä¸€ç§æ–¹æ³•è‡ªé€‚åº”åœ°å°†ç®€å•æ¡ˆä¾‹åˆ†é…ç»™æ›´å¿«ã€æ›´è½»å‹çš„æ¨¡å‹ï¼Œå¹¶ä¿ç•™å¤æ‚æ¡ˆä¾‹ä¾›å¼ºå¤§çš„MLLMå¤„ç†ï¼Œä»¥å¹³è¡¡å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ç¬¬äºŒç§æ–¹æ³•è®©ä¸“å®¶ç”Ÿæˆä¸€ç»„å¯èƒ½çš„å¯¹è±¡åŒºåŸŸï¼Œç„¶åä½¿ç”¨MLLMçš„æ¨ç†èƒ½åŠ›é€‰æ‹©æœ€åˆç†çš„åŒºåŸŸã€‚è¿™äº›åä½œç­–ç•¥åœ¨æˆ‘ä»¬çš„æ•°æ®é›†å’Œå…¶ä»–å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç»“åˆä¸“ç”¨å’Œé€šç”¨æ¨¡å‹ä¸ºè§£å†³å¤æ‚çš„ç°å®ä¸–ç•Œè§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†å®ç”¨é€”å¾„ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sleepyshep/FineCops-Ref%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/sleepyshep/FineCops-Refä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.20104v3">PDF</a> Accepted by TPAMI 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è¡¨è¾¾ç†è§£ï¼ˆRECï¼‰æ˜¯ä¸€ä¸ªè·¨æ¨¡æ€åŸºç¡€ä»»åŠ¡ï¼Œè¯„ä¼°è¯­è¨€ç†è§£ã€å›¾åƒç†è§£å’Œè¯­è¨€åˆ°å›¾åƒå®šä½ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚å®ƒæ˜¯æµ‹è¯•å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„é‡è¦åœºæ‰€ã€‚ä¸ºæ¨è¿›æ­¤é¢†åŸŸå‘å±•ï¼Œæˆ‘ä»¬åœ¨ä¹‹å‰çš„ä¼šè®®è®ºæ–‡ä¸­å¼•å…¥äº†ä¸€ä¸ªæ–°çš„RECæ•°æ®é›†ï¼Œå…·æœ‰ä¸¤ä¸ªå…³é”®ç‰¹å¾ï¼šä¸€æ˜¯è®¾è®¡å¯æ§éš¾åº¦çº§åˆ«ï¼Œéœ€è¦è·¨å¯¹è±¡ç±»åˆ«ã€å±æ€§å’Œå¤šè·³å…³ç³»è¿›è¡Œå¤šå±‚æ¬¡ç²¾ç»†æ¨ç†ï¼›äºŒæ˜¯é€šè¿‡ç²¾ç»†ç¼–è¾‘å’Œæ‰©å……ç”Ÿæˆè´Ÿæ–‡æœ¬å’Œå›¾åƒï¼Œæ˜ç¡®æµ‹è¯•æ¨¡å‹åœ¨ç›®æ ‡å¯¹è±¡ç¼ºå¤±åœºæ™¯ä¸‹çš„æ‹’ç»èƒ½åŠ›ï¼Œè¿™æ˜¯ç°æœ‰æ•°æ®é›†ä¸­å¸¸è¢«å¿½è§†ä½†è‡³å…³é‡è¦çš„æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹æ‰©å±•å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸¤ç§æ–°æ–¹æ³•æ¥è§£å†³ç²¾ç»†RECçš„æŒ‘æˆ˜ï¼Œç»“åˆä¸“ä¸šæ¨¡å‹å’ŒMLLMsçš„ä¼˜åŠ¿ã€‚ç¬¬ä¸€ç§æ–¹æ³•è‡ªé€‚åº”åœ°ä¸ºç®€å•æ¡ˆä¾‹åˆ†é…å¿«é€Ÿã€è½»å‹çš„æ¨¡å‹ï¼Œä¸ºå¤æ‚æ¡ˆä¾‹ä¿ç•™å¼ºå¤§çš„MLLMsï¼Œå¹³è¡¡å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ç¬¬äºŒç§æ–¹æ³•è®©ä¸“å®¶ç”Ÿæˆä¸€ç»„å¯èƒ½çš„å¯¹è±¡åŒºåŸŸï¼ŒMLLMåˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›é€‰æ‹©æœ€åˆç†çš„åŒºåŸŸã€‚è¿™äº›åä½œç­–ç•¥åœ¨æˆ‘ä»¬çš„æ•°æ®é›†å’Œå…¶ä»–å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾ç€æ”¹è¿›ã€‚ç»“æœè¡¨æ˜ï¼Œç»“åˆä¸“ä¸šæ¨¡å‹å’Œé€šç”¨æ¨¡å‹ä¸ºè§£å†³å¤æ‚çš„ç°å®ä¸–ç•Œè§†è§‰è¯­è¨€ä»»åŠ¡æä¾›äº†å®ç”¨é€”å¾„ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/sleepyshep/FineCops-Ref">https://github.com/sleepyshep/FineCops-Ref</a>è·å–ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è¡¨è¾¾ç†è§£ï¼ˆRECï¼‰ä»»åŠ¡è¯„ä¼°è¯­è¨€ç†è§£ã€å›¾åƒç†è§£å’Œè¯­è¨€åˆ°å›¾åƒå®šä½çš„å¤šæ¨¡æ€äº¤äº’ã€‚</li>
<li>æ–°RECæ•°æ®é›†å…·æœ‰å¯æ§éš¾åº¦çº§åˆ«å’Œè´Ÿæ–‡æœ¬&#x2F;å›¾åƒç”Ÿæˆï¼Œç”¨äºæµ‹è¯•æ¨¡å‹åœ¨ç›®æ ‡ç¼ºå¤±åœºæ™¯çš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºä¸¤ç§æ–°æ–¹æ³•æ¥å¤„ç†ç²¾ç»†RECæŒ‘æˆ˜ï¼Œç»“åˆä¸“ä¸šæ¨¡å‹å’ŒMLLMsçš„ä¼˜åŠ¿ã€‚</li>
<li>ç¬¬ä¸€ç§æ–¹æ³•è‡ªé€‚åº”åˆ†é…ç®€å•å’Œå¤æ‚æ¡ˆä¾‹ç»™ä¸åŒæ¨¡å‹ï¼Œä»¥æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>ç¬¬äºŒç§æ–¹æ³•åˆ©ç”¨ä¸“å®¶ç”Ÿæˆå¯¹è±¡åŒºåŸŸï¼Œç”±MLLMé€‰æ‹©æœ€åˆç†çš„åŒºåŸŸè¿›è¡Œæ¨ç†ã€‚</li>
<li>åä½œç­–ç•¥åœ¨æ•°æ®é›†å’Œå…¶ä»–åŸºå‡†æµ‹è¯•ä¸Šå–å¾—æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.20104">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-afdde2754c13d2ef9ebefcfba4da727f.jpg" align="middle">
<img src="https://picx.zhimg.com/v2-470bb5e80162673fc79373746e792a4c.jpg" align="middle">
<img src="https://pic1.zhimg.com/v2-3ebc4a5130c9fb9f4d34b9a364daf4d3.jpg" align="middle">
<img src="https://pica.zhimg.com/v2-c3dd7c40c427aefcc3be53b0c3aad9b7.jpg" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-06-17/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-06-17/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-17/Agent/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-61e68f30588cf7df4ba5e28838773d0a.jpg" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-17  SEC-bench Automated Benchmarking of LLM Agents on Real-World Software   Security Tasks
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-06-17
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-06-17/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://pic1.zhimg.com/v2-0106ae2049aba135f37814fd5207b9e6.jpg" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-06-17  Schema-R1 A reasoning training approach for schema linking in   Text-to-SQL Task
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-06-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30166.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
