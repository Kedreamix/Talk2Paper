<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-19  RoboGPT-R1 Enhancing Robot Planning with Reinforcement Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-a67bbd7b6fa932a2fa5e550137d2933a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810410&auth_key=1760810410-0-0-b578d88a4d8a46c82cd688c74962f8f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-21
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    22.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    92 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-19-æ›´æ–°"><a href="#2025-10-19-æ›´æ–°" class="headerlink" title="2025-10-19 æ›´æ–°"></a>2025-10-19 æ›´æ–°</h1><h2 id="RoboGPT-R1-Enhancing-Robot-Planning-with-Reinforcement-Learning"><a href="#RoboGPT-R1-Enhancing-Robot-Planning-with-Reinforcement-Learning" class="headerlink" title="RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning"></a>RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning</h2><p><strong>Authors:Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li</strong></p>
<p>Improving the reasoning capabilities of embodied agents is crucial for robots to complete complex human instructions in long-view manipulation tasks successfully. Despite the success of large language models and vision language models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue facing challenges in performing long-horizon manipulation tasks in complex real-world environments, owing to their restricted common sense and reasoning capabilities. Considering that aligning general-purpose vision language models to robotic planning tasks via supervised fine-tuning suffers from poor generalization and insufficient physical understanding, we propose RoboGPT-R1, a two-stage fine-tuning framework for embodied planning. In this framework, supervised training acquires foundational knowledge through expert sequences, followed by RL to address the modelâ€™s shortcomings in visual-spatial understanding and reasoning. To achieve physical understanding and action sequence consistency in multi-step reasoning tasks, we design a rule-based reward function that simultaneously considers long-horizon performance and action constraint in the environment. The reasoning model, trained on Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini, by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the EmbodiedBench benchmark. </p>
<blockquote>
<p>æé«˜å®ä½“ä»£ç†çš„æ¨ç†èƒ½åŠ›å¯¹äºæœºå™¨äººåœ¨é•¿æœŸæ“æ§ä»»åŠ¡ä¸­æˆåŠŸå®Œæˆå¤æ‚çš„äººç±»æŒ‡ä»¤è‡³å…³é‡è¦ã€‚å°½ç®¡åŸºäºç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§„åˆ’ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬ä»ç„¶é¢ä¸´ç€åœ¨å¤æ‚çš„çœŸå®ç¯å¢ƒä¸­æ‰§è¡Œé•¿æœŸæ“æ§ä»»åŠ¡çš„æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºå®ƒä»¬æœ‰é™çš„å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚è€ƒè™‘åˆ°é€šè¿‡ç›‘ç£å¾®è°ƒå°†é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸æœºå™¨äººè§„åˆ’ä»»åŠ¡å¯¹é½å­˜åœ¨æ³›åŒ–æ€§å·®å’Œç‰©ç†ç†è§£ä¸è¶³çš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†RoboGPT-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå®ä½“è§„åˆ’çš„ä¸¤é˜¶æ®µå¾®è°ƒæ¡†æ¶ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œç›‘ç£è®­ç»ƒé€šè¿‡ä¸“å®¶åºåˆ—è·å¾—åŸºç¡€çŸ¥è¯†ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥è§£å†³æ¨¡å‹åœ¨è§†è§‰ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚ä¸ºäº†å®ç°å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­çš„ç‰©ç†ç†è§£å’ŒåŠ¨ä½œåºåˆ—ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°åŒæ—¶è€ƒè™‘äº†é•¿æœŸæ€§èƒ½å’Œç¯å¢ƒä¸­çš„åŠ¨ä½œçº¦æŸã€‚åœ¨EmbodiedBenchåŸºå‡†æµ‹è¯•ä¸Šï¼Œè¯¥æ¨ç†æ¨¡å‹çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºæ›´å¤§è§„æ¨¡çš„GPT-4o-miniæ¨¡å‹ï¼Œé«˜å‡º21.33%ï¼Œå¹¶ä¸”è¶…è¿‡äº†å…¶ä»–åœ¨Qwen2.5-VL-7Bä¸Šè®­ç»ƒçš„å·¥ä½œï¼Œé«˜å‡º20.33%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14828v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœºå™¨äººå®Œæˆå¤æ‚äººç±»æŒ‡ä»¤çš„é•¿æœŸæ“ä½œä»»åŠ¡ä¸­ï¼Œæé«˜å…¶æ¨ç†èƒ½åŠ›è‡³å…³é‡è¦ã€‚è™½ç„¶åŸºäºç›‘ç£å¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨è§„åˆ’ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬ä»é¢ä¸´åœ¨å¤æ‚ç°å®ç¯å¢ƒä¸­æ‰§è¡Œé•¿æœŸæ“ä½œä»»åŠ¡çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹é€šç”¨å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†RoboGPT-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µçš„å¾®è°ƒæ¡†æ¶ï¼Œç”¨äºåµŒå…¥å¼è§„åˆ’ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡ä¸“å®¶åºåˆ—è¿›è¡Œç›‘ç£è®­ç»ƒè·å–åŸºç¡€çŸ¥è¯†ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥è§£å†³æ¨¡å‹åœ¨è§†è§‰ç©ºé—´ç†è§£å’Œæ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚ä¸ºå®ç°å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­çš„ç‰©ç†ç†è§£å’ŒåŠ¨ä½œåºåˆ—ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°ï¼ŒåŒæ—¶è€ƒè™‘é•¿æœŸæ€§èƒ½å’Œç¯å¢ƒä¸­åŠ¨ä½œçº¦æŸã€‚åœ¨EmbodiedBenchåŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨ç†æ¨¡å‹åœ¨Qwen2.5-VL-3Bä¸Šæ˜¾è‘—ä¼˜äºè¾ƒå¤§è§„æ¨¡çš„GPT-4o-miniæ¨¡å‹ï¼Œå‡†ç¡®ç‡æé«˜21.33%ï¼Œå¹¶ä¸”è¶…è¶Šäº†å…¶ä»–åœ¨Qwen2.5-VL-7Bä¸Šè®­ç»ƒçš„å·¥ä½œï¼Œå‡†ç¡®ç‡æé«˜20.33%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æé«˜æœºå™¨äººçš„æ¨ç†èƒ½åŠ›å¯¹äºå®Œæˆå¤æ‚çš„é•¿æœŸæ“ä½œä»»åŠ¡è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰è¯­è¨€æ¨¡å‹åœ¨çœŸå®ç¯å¢ƒçš„é•¿æœŸæ“ä½œä»»åŠ¡ä¸­é¢ä¸´æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹é€šç”¨å¸¸è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†RoboGPT-R1æ¡†æ¶ï¼Œç»“åˆç›‘ç£è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜åµŒå…¥å¼è§„åˆ’ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>RoboGPT-R1é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒï¼šç¬¬ä¸€é˜¶æ®µé€šè¿‡ä¸“å®¶åºåˆ—è¿›è¡Œç›‘ç£è®­ç»ƒï¼Œç¬¬äºŒé˜¶æ®µè§£å†³è§†è§‰ç©ºé—´ç†è§£å’Œæ¨ç†çš„ä¸è¶³ã€‚</li>
<li>è®¾è®¡äº†åŸºäºè§„åˆ™çš„å¥–åŠ±å‡½æ•°ï¼Œä»¥å®ç°å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸­çš„ç‰©ç†ç†è§£å’ŒåŠ¨ä½œåºåˆ—ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨EmbodiedBenchåŸºå‡†æµ‹è¯•ä¸­ï¼ŒRoboGPT-R1æ˜¾è‘—ä¼˜äºå…¶ä»–æ¨¡å‹ï¼Œå‡†ç¡®ç‡è¾¾åˆ°è¾ƒé«˜æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14828">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a099ba6074f47e3e889319f629068932~resize:0:q75.jpg?source=1f5c5e47&expiration=1760809947&auth_key=1760809947-0-0-16a6dc40eeea7d1af91bf21d281e3979&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e69d26203f68eba4ccdf3dc77eb805db~resize:0:q75.jpg?source=1f5c5e47&expiration=1760809955&auth_key=1760809955-0-0-717021cabc28a7266b5db57b837ef612&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7422f8a710d4ef94e3e57bf037190e52~resize:0:q75.jpg?source=1f5c5e47&expiration=1760809962&auth_key=1760809962-0-0-3281b55f29fd1ae7b6b4fe4be22bb10b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="ATGen-Adversarial-Reinforcement-Learning-for-Test-Case-Generation"><a href="#ATGen-Adversarial-Reinforcement-Learning-for-Test-Case-Generation" class="headerlink" title="ATGen: Adversarial Reinforcement Learning for Test Case Generation"></a>ATGen: Adversarial Reinforcement Learning for Test Case Generation</h2><p><strong>Authors:Qingyao Li, Xinyi Dai, Weiwen Liu, Xiangyang Li, Yasheng Wang, Ruiming Tang, Yong Yu, Weinan Zhang</strong></p>
<p>Large Language Models (LLMs) excel at code generation, yet their outputs often contain subtle bugs, for which effective test cases are a critical bottleneck. Existing test generation methods, whether based on prompting or supervised fine-tuning, rely on static datasets. This imposes a <code>fixed-difficulty ceiling&#39;&#39;, fundamentally limiting their ability to uncover novel or more complex bugs beyond their training scope. To overcome this, we introduce ATGen, a framework that trains a test case generator via adversarial reinforcement learning. ATGen pits a test generator against an adversarial code generator that continuously crafts harder bugs to evade the current policy. This dynamic loop creates a curriculum of increasing difficulty challenging current policy. The test generator is optimized via Reinforcement Learning (RL) to jointly maximize </code>Output Accuracyâ€™â€™ and &#96;&#96;Attack Successâ€™â€™, enabling it to learn a progressively stronger policy that breaks the fixed-difficulty ceiling of static training. Extensive experiments demonstrate that ATGen significantly outperforms state-of-the-art baselines. We further validate its practical utility, showing it serves as both a more effective filter for Best-of-N inference and a higher-quality reward source for training code generation models. Our work establishes a new, dynamic paradigm for improving the reliability of LLM-generated code. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬çš„è¾“å‡ºå¸¸å¸¸å«æœ‰å¾®å¦™çš„é”™è¯¯ï¼Œæœ‰æ•ˆçš„æµ‹è¯•ç”¨ä¾‹æ˜¯å…³é”®çš„ç“¶é¢ˆã€‚ç°æœ‰çš„æµ‹è¯•ç”Ÿæˆæ–¹æ³•ï¼Œæ— è®ºåŸºäºæç¤ºè¿˜æ˜¯ç›‘ç£å¾®è°ƒï¼Œéƒ½ä¾èµ–äºé™æ€æ•°æ®é›†ã€‚è¿™è®¾å®šäº†ä¸€ä¸ªâ€œå›ºå®šéš¾åº¦ä¸Šé™â€ï¼Œä»æ ¹æœ¬ä¸Šé™åˆ¶äº†å®ƒä»¬å‘ç°è®­ç»ƒèŒƒå›´ä¹‹å¤–çš„æ–°é¢–æˆ–æ›´å¤æ‚é”™è¯¯çš„èƒ½åŠ›ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ATGenï¼Œä¸€ä¸ªé€šè¿‡å¯¹æŠ—æ€§å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨çš„æ¡†æ¶ã€‚ATGenå°†æµ‹è¯•ç”Ÿæˆå™¨ä¸å¯¹æŠ—æ€§ä»£ç ç”Ÿæˆå™¨ç›¸å¯¹æŠ—ï¼Œåè€…ä¸æ–­åˆ¶é€ æ›´éš¾ä»¥è§£å†³çš„é”™è¯¯æ¥é€ƒé¿å½“å‰ç­–ç•¥ã€‚è¿™ç§åŠ¨æ€å¾ªç¯åˆ›å»ºäº†ä¸€ä¸ªéš¾åº¦ä¸æ–­å¢åŠ çš„è¯¾ç¨‹ï¼Œä»¥æŒ‘æˆ˜å½“å‰ç­–ç•¥ã€‚æµ‹è¯•ç”Ÿæˆå™¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œä»¥æœ€å¤§åŒ–â€œè¾“å‡ºå‡†ç¡®æ€§â€å’Œâ€œæ”»å‡»æˆåŠŸæ€§â€ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ ä¸€ä¸ªé€æ¸å¼ºå¤§çš„ç­–ç•¥ï¼Œæ‰“ç ´é™æ€è®­ç»ƒçš„å›ºå®šéš¾åº¦ä¸Šé™ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒATGenæ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨å®è·µä¸­çš„å®ç”¨æ€§ï¼Œè¡¨æ˜å®ƒæ—¢æ˜¯æ›´æœ‰æ•ˆçš„Best-of-Næ¨ç†è¿‡æ»¤å™¨ï¼Œä¹Ÿæ˜¯æ›´é«˜è´¨é‡çš„ä»£ç ç”Ÿæˆæ¨¡å‹è®­ç»ƒå¥–åŠ±æ¥æºã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºæé«˜LLMç”Ÿæˆä»£ç çš„å¯é æ€§å»ºç«‹äº†æ–°çš„åŠ¨æ€èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14635v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ“…é•¿ä»£ç ç”Ÿæˆï¼Œä½†å…¶è¾“å‡ºå¸¸å«æœ‰å¾®å¦™é”™è¯¯ï¼Œæœ‰æ•ˆçš„æµ‹è¯•ç”¨ä¾‹æ˜¯å…³é”®çš„ç“¶é¢ˆã€‚ç°æœ‰çš„æµ‹è¯•ç”Ÿæˆæ–¹æ³•ï¼Œæ— è®ºæ˜¯åŸºäºæç¤ºè¿˜æ˜¯ç›‘ç£å¾®è°ƒï¼Œéƒ½ä¾èµ–äºé™æ€æ•°æ®é›†ï¼Œè¿™å¯¼è‡´å®ƒä»¬å­˜åœ¨â€œå›ºå®šéš¾åº¦ä¸Šé™â€ï¼Œæ— æ³•å‘ç°è®­ç»ƒèŒƒå›´ä¹‹å¤–çš„æ–°é¢–æˆ–å¤æ‚é”™è¯¯ã€‚ä¸ºçªç ´æ­¤é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºATGenæ¡†æ¶ï¼Œé€šè¿‡å¯¹æŠ—æ€§å¼ºåŒ–å­¦ä¹ è®­ç»ƒæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå™¨ã€‚ATGenå°†æµ‹è¯•ç”Ÿæˆå™¨ä¸å¯¹æŠ—æ€§ä»£ç ç”Ÿæˆå™¨ç›¸å¯¹æŠ—ï¼Œåè€…ä¸æ–­åˆ¶é€ æ›´éš¾ä»¥è¢«å‘ç°çš„é”™è¯¯ä»¥é€ƒé¿å½“å‰ç­–ç•¥ã€‚è¿™ç§åŠ¨æ€å¾ªç¯åˆ›å»ºäº†ä¸€ä¸ªä¸æ–­å¢é•¿çš„éš¾åº¦è¯¾ç¨‹ï¼ŒæŒ‘æˆ˜å½“å‰ç­–ç•¥ã€‚æµ‹è¯•ç”Ÿæˆå™¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œä»¥æœ€å¤§åŒ–â€œè¾“å‡ºå‡†ç¡®æ€§â€å’Œâ€œæ”»å‡»æˆåŠŸç‡â€ï¼Œä½¿å…¶å­¦ä¼šé€æ­¥å¼ºå¤§çš„ç­–ç•¥ï¼Œæ‰“ç ´é™æ€è®­ç»ƒçš„å›ºå®šéš¾åº¦ä¸Šé™ã€‚å®éªŒè¡¨æ˜ï¼ŒATGenæ˜¾è‘—ä¼˜äºæœ€æ–°åŸºçº¿ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥éªŒè¯äº†å…¶å®ç”¨æ€§ï¼Œè¡¨æ˜å®ƒæ—¢æ˜¯Best-of-Næ¨ç†çš„æ›´æœ‰æ•ˆè¿‡æ»¤å™¨ï¼Œä¹Ÿæ˜¯è®­ç»ƒä»£ç ç”Ÿæˆæ¨¡å‹çš„é«˜è´¨é‡å¥–åŠ±æ¥æºã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºæå‡LLMç”Ÿæˆä»£ç çš„å¯é æ€§å»ºç«‹äº†æ–°çš„åŠ¨æ€èŒƒå¼ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsæ“…é•¿ä»£ç ç”Ÿæˆï¼Œä½†è¾“å‡ºå¸¸å«é”™è¯¯ï¼Œéœ€è¦æœ‰æ•ˆçš„æµ‹è¯•æ–¹æ³•æ¥æ£€æŸ¥å…¶æ­£ç¡®æ€§ã€‚</li>
<li>å½“å‰æµ‹è¯•ç”Ÿæˆæ–¹æ³•å­˜åœ¨â€œå›ºå®šéš¾åº¦ä¸Šé™â€ï¼Œéš¾ä»¥å‘ç°æ–°é¢–æˆ–å¤æ‚é”™è¯¯ã€‚</li>
<li>ATGenæ¡†æ¶é€šè¿‡å¯¹æŠ—æ€§å¼ºåŒ–å­¦ä¹ è®­ç»ƒæµ‹è¯•ç”Ÿæˆå™¨æ¥å…‹æœæ­¤é—®é¢˜ã€‚</li>
<li>ATGenåˆ›å»ºä¸€ä¸ªéš¾åº¦é€’å¢çš„è¯¾ç¨‹æ¥æŒ‘æˆ˜æµ‹è¯•ç”Ÿæˆå™¨çš„ç­–ç•¥ã€‚</li>
<li>ATGenä¼˜åŒ–çš„æµ‹è¯•ç”Ÿæˆå™¨èƒ½æœ€å¤§åŒ–è¾“å‡ºå‡†ç¡®æ€§å’Œæ”»å‡»æˆåŠŸç‡ï¼Œæ‰“ç ´é™æ€è®­ç»ƒçš„éš¾åº¦ä¸Šé™ã€‚</li>
<li>å®éªŒè¯æ˜ATGenæ˜¾è‘—ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14635">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-faff5652a7ede264a9eef2417555fbef~resize:0:q75.jpg?source=1f5c5e47&expiration=1760809970&auth_key=1760809970-0-0-223b808be750d987b92ab4d5b389d770&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-556886f781d9f95e0583e12d60b77cf8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760809977&auth_key=1760809977-0-0-164156df3daa85754dbc9e37cb1390e3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Synthesizing-Agentic-Data-for-Web-Agents-with-Progressive-Difficulty-Enhancement-Mechanisms"><a href="#Synthesizing-Agentic-Data-for-Web-Agents-with-Progressive-Difficulty-Enhancement-Mechanisms" class="headerlink" title="Synthesizing Agentic Data for Web Agents with Progressive Difficulty   Enhancement Mechanisms"></a>Synthesizing Agentic Data for Web Agents with Progressive Difficulty   Enhancement Mechanisms</h2><p><strong>Authors:Shrey Pandit, Xuan-Phi Nguyen, Yifei Ming, Austin Xu, Jiayu Wang, Caiming Xiong, Shafiq Joty</strong></p>
<p>Web-based â€˜deep researchâ€™ agents aim to solve complex question - answering tasks through long-horizon interactions with online tools. These tasks remain challenging, as the underlying language models are often not optimized for long-horizon reasoning and exploration. Prior work has proposed workflows for constructing instruction-tuning datasets, often leveraging knowledge graphs. However, such methods typically lack fine-grained control over difficulty and quality, yielding synthetic data that falls short of capturing the complexity required for long-horizon reasoning. Furthermore, many studies conflate data and training effects by comparing models trained under different optimization recipes, making it difficult to isolate and evaluate the effectiveness of the data itself. We introduce a two-pronged data synthesis pipeline that generates question - answer pairs by progressively increasing task complexity until a frontier baseline web agent fails. The baseline agent plays multiple roles in this process: attempting the questions, validating factuality, checking for alternative answers, and enforcing filtering. To evaluate the effectiveness of our synthesis methods, we adopt a controlled training setup based on distillation from strong web agents. Experiments across multiple web-based benchmarks show that our dataset - despite being smaller - enables the training of more effective web agents than existing datasets. In particular, our data exhibits twice the diversity in tool-use actions, allowing models trained on it to achieve stronger performance while avoiding repetitive tool-calling behaviors. </p>
<blockquote>
<p>åŸºäºç½‘ç»œçš„â€æ·±åº¦ç ”ç©¶â€ä»£ç†æ—¨åœ¨é€šè¿‡ä¸åœ¨çº¿å·¥å…·è¿›è¡Œé•¿æœŸäº¤äº’æ¥è§£å†³å¤æ‚çš„é—®ç­”ä»»åŠ¡ã€‚è¿™äº›ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºåŸºç¡€è¯­è¨€æ¨¡å‹é€šå¸¸æ²¡æœ‰é’ˆå¯¹é•¿æœŸæ¨ç†å’Œæ¢ç´¢è¿›è¡Œä¼˜åŒ–ã€‚å…ˆå‰çš„å·¥ä½œå·²ç»æå‡ºäº†æ„å»ºæŒ‡ä»¤è°ƒæ•´æ•°æ®é›†çš„å·¥ä½œæµç¨‹ï¼Œé€šå¸¸åˆ©ç”¨çŸ¥è¯†å›¾è°±ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ç¼ºä¹å¯¹éš¾åº¦å’Œè´¨é‡çš„ç²¾ç»†æ§åˆ¶ï¼Œäº§ç”Ÿçš„åˆæˆæ•°æ®æ— æ³•æ•æ‰é•¿æœŸæ¨ç†æ‰€éœ€çš„å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œè®¸å¤šç ”ç©¶é€šè¿‡æ¯”è¾ƒåœ¨ä¸åŒä¼˜åŒ–é…æ–¹ä¸‹è®­ç»ƒçš„æ¨¡å‹æ¥æ··æ·†æ•°æ®å’Œè®­ç»ƒæ•ˆæœï¼Œå¾ˆéš¾å­¤ç«‹åœ°è¯„ä¼°å’Œè¯„ä¼°æ•°æ®æœ¬èº«çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§åŒç®¡é½ä¸‹çš„æ•°æ®åˆæˆç®¡é“ï¼Œé€šè¿‡é€æ­¥å¢åŠ ä»»åŠ¡å¤æ‚æ€§æ¥ç”Ÿæˆé—®ç­”å¯¹ï¼Œç›´åˆ°å‰æ²¿åŸºçº¿ç½‘ç»œä»£ç†å¤±è´¥ã€‚åŸºçº¿ä»£ç†åœ¨æ­¤è¿‡ç¨‹ä¸­æ‰®æ¼”å¤šä¸ªè§’è‰²ï¼šå°è¯•é—®é¢˜ã€éªŒè¯äº‹å®æ€§ã€æ£€æŸ¥æ›¿ä»£ç­”æ¡ˆå’Œæ‰§è¡Œè¿‡æ»¤ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„åˆæˆæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†åŸºäºä»å¼ºå¤§ç½‘ç»œä»£ç†ä¸­æç‚¼å‡ºæ¥çš„å—æ§è®­ç»ƒè®¾ç½®ã€‚åœ¨å¤šä¸ªç½‘ç»œåŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå°½ç®¡æˆ‘ä»¬çš„æ•°æ®é›†è¾ƒå°ï¼Œä½†å®ƒèƒ½è®­ç»ƒå‡ºæ¯”ç°æœ‰æ•°æ®é›†æ›´æœ‰æ•ˆçš„ç½‘ç»œä»£ç†ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ•°æ®åœ¨å·¥å…·ä½¿ç”¨è¡Œä¸ºæ–¹é¢è¡¨ç°å‡ºä¸¤å€çš„å¤šæ ·æ€§ï¼Œä½¿å¾—åœ¨æ­¤ä¹‹ä¸Šè®­ç»ƒçš„æ¨¡å‹èƒ½å¤Ÿå–å¾—æ›´å¼ºçš„æ€§èƒ½ï¼ŒåŒæ—¶é¿å…é‡å¤çš„å·¥å…·è°ƒç”¨è¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13913v1">PDF</a> Preprint. ICLR 26 submission</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†åŸºäºç½‘ç»œçš„â€œæ·±åº¦ç ”ç©¶â€ä»£ç†ï¼Œæ—¨åœ¨é€šè¿‡åœ¨çº¿å·¥å…·è¿›è¡Œé•¿æœŸäº¤äº’æ¥è§£å†³å¤æ‚çš„é—®ç­”ä»»åŠ¡ã€‚ç„¶è€Œï¼Œç”±äºåº•å±‚è¯­è¨€æ¨¡å‹é€šå¸¸æœªé’ˆå¯¹é•¿æœŸæ¨ç†å’Œæ¢ç´¢è¿›è¡Œä¼˜åŒ–ï¼Œè¿™äº›ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŒç®¡é½ä¸‹çš„æ•°æ®åˆæˆç®¡é“ï¼Œé€šè¿‡é€æ­¥å¢åŠ ä»»åŠ¡å¤æ‚æ€§æ¥ç”Ÿæˆé—®ç­”å¯¹ï¼Œç›´åˆ°å‰æ²¿åŸºçº¿ç½‘ç»œä»£ç†å¤±è´¥ã€‚ä¸ºäº†è¯„ä¼°åˆæˆæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä½œè€…é‡‡ç”¨äº†ä¸€ç§åŸºäºä»å¼ºå¤§ç½‘ç»œä»£ç†ä¸­æå–çŸ¥è¯†çš„å—æ§è®­ç»ƒè®¾ç½®ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡æ•°æ®é‡è¾ƒå°ï¼Œä½†ä½œè€…åœ¨å¤šä¸ªç½‘ç»œåŸºå‡†æµ‹è¯•ä¸Šçš„æ•°æ®é›†èƒ½å¤Ÿè®­ç»ƒå‡ºæ›´æœ‰æ•ˆçš„ç½‘ç»œä»£ç†ï¼Œç‰¹åˆ«æ˜¯è¯¥æ•°æ®åœ¨å·¥å…·ä½¿ç”¨è¡Œä¸ºä¸Šè¡¨ç°å‡ºæ›´å¼ºçš„å¤šæ ·æ€§ï¼Œèƒ½å¤Ÿé¿å…é‡å¤çš„å·¥å…·è°ƒç”¨è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŸºäºç½‘ç»œçš„â€œæ·±åº¦ç ”ç©¶â€ä»£ç†è‡´åŠ›äºé€šè¿‡åœ¨çº¿å·¥å…·è¿›è¡Œé•¿æœŸäº¤äº’å®Œæˆå¤æ‚é—®ç­”ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰è¯­è¨€æ¨¡å‹é’ˆå¯¹é•¿æœŸæ¨ç†å’Œæ¢ç´¢çš„ä¼˜åŒ–ä¸è¶³ï¼Œä½¿å¾—è¿™äº›ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</li>
<li>ä½œè€…æå‡ºäº†ä¸€ç§æ•°æ®åˆæˆç®¡é“ï¼Œé€šè¿‡é€æ­¥å¢åŠ ä»»åŠ¡å¤æ‚æ€§æ¥ç”Ÿæˆé—®ç­”å¯¹ï¼Œç›´è‡³åŸºçº¿ç½‘ç»œä»£ç†æ— æ³•åº”å¯¹ã€‚</li>
<li>åŸºçº¿ä»£ç†åœ¨æ­¤è¿‡ç¨‹ä¸­æ‰®æ¼”å¤šé‡è§’è‰²ï¼ŒåŒ…æ‹¬å°è¯•é—®é¢˜ã€éªŒè¯äº‹å®ã€æ£€æŸ¥æ›¿ä»£ç­”æ¡ˆå’Œæ‰§è¡Œè¿‡æ»¤ã€‚</li>
<li>ä½œè€…é‡‡ç”¨äº†ä¸€ç§åŸºäºä»å¼ºå¤§ç½‘ç»œä»£ç†ä¸­æå–çŸ¥è¯†çš„å—æ§è®­ç»ƒè®¾ç½®æ¥è¯„ä¼°æ•°æ®åˆæˆæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä¸ç°æœ‰æ•°æ®é›†ç›¸æ¯”ï¼Œä½œè€…åœ¨å¤šä¸ªç½‘ç»œåŸºå‡†æµ‹è¯•ä¸Šçš„æ•°æ®é›†è¡¨ç°æ›´ä¼˜ç§€ï¼Œè®­ç»ƒå‡ºæ›´æœ‰æ•ˆçš„ç½‘ç»œä»£ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13913">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5068e536e9c0603cd35b51a3be1d3b41~resize:0:q75.jpg?source=1f5c5e47&expiration=1760809985&auth_key=1760809985-0-0-d7aa0d024957bc75cad9b3e8f5cd6797&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7fcab1798ac445b2695e171d66068ddb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810055&auth_key=1760810055-0-0-289d83e47fd295676388d4389951df88&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-57bcf3e7be95900bece3f2587ca53c15~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810062&auth_key=1760810062-0-0-02be5cce4b121e646ee104e45a17c20f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-14c04f5fd7f5cf2847c8f299046e0e74~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810069&auth_key=1760810069-0-0-2ae7453f83e18e956964a734c9b98c27&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="The-Art-of-Scaling-Reinforcement-Learning-Compute-for-LLMs"><a href="#The-Art-of-Scaling-Reinforcement-Learning-Compute-for-LLMs" class="headerlink" title="The Art of Scaling Reinforcement Learning Compute for LLMs"></a>The Art of Scaling Reinforcement Learning Compute for LLMs</h2><p><strong>Authors:Devvrit Khatri, Lovish Madaan, Rishabh Tiwari, Rachit Bansal, Sai Surya Duvvuri, Manzil Zaheer, Inderjit S. Dhillon, David Brandfonbrener, Rishabh Agarwal</strong></p>
<p>Reinforcement learning (RL) has become central to training large language models (LLMs), yet the field lacks predictive scaling methodologies comparable to those established for pre-training. Despite rapidly rising compute budgets, there is no principled understanding of how to evaluate algorithmic improvements for scaling RL compute. We present the first large-scale systematic study, amounting to more than 400,000 GPU-hours, that defines a principled framework for analyzing and predicting RL scaling in LLMs. We fit sigmoidal compute-performance curves for RL training and ablate a wide range of common design choices to analyze their effects on asymptotic performance and compute efficiency. We observe: (1) Not all recipes yield similar asymptotic performance, (2) Details such as loss aggregation, normalization, curriculum, and off-policy algorithm primarily modulate compute efficiency without materially shifting the asymptote, and (3) Stable, scalable recipes follow predictable scaling trajectories, enabling extrapolation from smaller-scale runs. Combining these insights, we propose a best-practice recipe, ScaleRL, and demonstrate its effectiveness by successfully scaling and predicting validation performance on a single RL run scaled up to 100,000 GPU-hours. Our work provides both a scientific framework for analyzing scaling in RL and a practical recipe that brings RL training closer to the predictability long achieved in pre-training. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²æˆä¸ºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ ¸å¿ƒï¼Œç„¶è€Œï¼Œè¯¥é¢†åŸŸç¼ºä¹å¯ä¸é¢„è®­ç»ƒå»ºç«‹çš„é¢„æµ‹æ‰©å±•æ–¹æ³•ç›¸æ¯”è¾ƒçš„æ–¹æ³•ã€‚å°½ç®¡è®¡ç®—é¢„ç®—è¿…é€Ÿå¢åŠ ï¼Œä½†æ²¡æœ‰åŸåˆ™æ€§çš„ç†è§£å¦‚ä½•è¯„ä¼°ç®—æ³•æ”¹è¿›å¯¹æ‰©å±•RLè®¡ç®—çš„å½±å“ã€‚æˆ‘ä»¬è¿›è¡Œäº†è¶…è¿‡40ä¸‡GPUå°æ—¶çš„å¤§è§„æ¨¡ç³»ç»Ÿç ”ç©¶ï¼Œå®šä¹‰äº†ä¸€ä¸ªåˆ†æé¢„æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¼ºåŒ–å­¦ä¹ æ‰©å±•çš„åŸåˆ™æ€§æ¡†æ¶ã€‚æˆ‘ä»¬å¯¹å¼ºåŒ–å­¦ä¹ çš„è®¡ç®—æ€§èƒ½æ‹Ÿåˆäº†Sigmoidæ›²çº¿ï¼Œå¹¶åˆ†æäº†ä¸€ç³»åˆ—é€šç”¨è®¾è®¡é€‰æ‹©å¯¹å…¶æ¸è¿‘æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡çš„å½±å“ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼šï¼ˆ1ï¼‰å¹¶éæ‰€æœ‰æ–¹æ³•éƒ½èƒ½äº§ç”Ÿç›¸ä¼¼çš„æ¸è¿‘æ€§èƒ½ï¼›ï¼ˆ2ï¼‰æŸå¤±èšåˆã€å½’ä¸€åŒ–ã€è¯¾ç¨‹åŠç¦»çº¿ç­–ç•¥ç®—æ³•ç­‰ç»†èŠ‚ä¸»è¦è°ƒèŠ‚è®¡ç®—æ•ˆç‡ï¼Œè€Œä¸ä¼šå®è´¨æ€§æ”¹å˜æ¸è¿‘æ€§èƒ½ï¼›ï¼ˆ3ï¼‰ç¨³å®šã€å¯æ‰©å±•çš„æ–¹æ³•éµå¾ªå¯é¢„æµ‹çš„æ‰©å±•è½¨è¿¹ï¼Œå¯ä»è¾ƒå°çš„è¿è¡Œä¸­æ¨æ–­å‡ºæ›´å¤§çš„è§„æ¨¡ã€‚ç»“åˆè¿™äº›è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæœ€ä½³å®è·µæ–¹æ³•ScaleRLï¼Œå¹¶é€šè¿‡ä¸€æ¬¡å¼ºåŒ–å­¦ä¹ è¿è¡ŒæˆåŠŸæ‰©å±•åˆ°10ä¸‡GPUå°æ—¶æ¥éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œæ—¢ä¸ºåˆ†æå¼ºåŒ–å­¦ä¹ ä¸­çš„æ‰©å±•æä¾›äº†ä¸€ä¸ªç§‘å­¦æ¡†æ¶ï¼Œä¹Ÿä¸ºä½¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ›´æ¥è¿‘é•¿æœŸå®ç°çš„é¢„è®­ç»ƒé¢„æµ‹æ€§æä¾›äº†ä¸€ä¸ªå®ç”¨æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13786v1">PDF</a> 28 pages, 20 figures</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ç ”ç©¶äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒä¸­çš„åº”ç”¨ï¼Œå¹¶è¿›è¡Œäº†å¤§è§„æ¨¡çš„ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œå®šä¹‰äº†ä¸€ä¸ªåŸåˆ™æ€§çš„æ¡†æ¶æ¥åˆ†æé¢„æµ‹RLåœ¨LLMä¸­çš„æ‰©å±•æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œä¸åŒçš„è®­ç»ƒç­–ç•¥å¯¹æœ€ç»ˆæ€§èƒ½å’Œè®¡ç®—æ•ˆç‡æœ‰ä¸åŒçš„å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€ç§å¯é çš„è®­ç»ƒç­–ç•¥ï¼Œæ—¨åœ¨å®ç°è®¡ç®—æ•ˆç‡çš„çªç ´å¹¶æ¨è¿›å¼ºåŒ–å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒè¿›ç¨‹ã€‚ç ”ç©¶å»ºç«‹äº†ç†è§£å¹¶è¯„ä¼°RLè®¡ç®—æ•ˆç‡æ”¹å–„åŸåˆ™ï¼Œæœ‰åˆ©äºæ›´æœ‰æ•ˆåœ°è®­ç»ƒå’Œé¢„æµ‹å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚è¿™é¡¹å·¥ä½œæ—¢æœ‰ç†è®ºåŸºç¡€ä¹Ÿè´´åˆå®è·µéœ€æ±‚ï¼Œæ˜¯æ¨åŠ¨æœºå™¨å­¦ä¹ ç ”ç©¶å–å¾—é•¿è¶³å‘å±•çš„ä¸€é¡¹é‡è¦å·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯æœ¬æ–‡çš„ä¸»è¦è§è§£ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒä¸­æ‰®æ¼”äº†æ ¸å¿ƒè§’è‰²ï¼Œä½†ç›®å‰ç¼ºä¹é¢„æµ‹å…¶æ‰©å±•æ€§çš„æ–¹æ³•ã€‚</li>
<li>å¯¹å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿›è¡Œäº†å¤§è§„æ¨¡çš„ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œå¹¶å®šä¹‰äº†åŸåˆ™æ€§çš„æ¡†æ¶æ¥åˆ†æé¢„æµ‹å…¶åœ¨LLMä¸­çš„æ‰©å±•æ€§ã€‚ </li>
<li>é€šè¿‡å¤§é‡çš„è®¡ç®—å®è·µæ•°æ®ç ”ç©¶å‘ç°å¹¶éæ‰€æœ‰çš„è®­ç»ƒç­–ç•¥éƒ½ä¼šå¸¦æ¥ç›¸ä¼¼çš„æœ€ç»ˆç»“æœã€‚æ­¤å¤–æŸå¤±èšé›†ï¼Œæ­£è§„åŒ–ç­‰æ–¹æ³•å¹¶ä¸èƒ½æ ¹æœ¬å½±å“ç»“æœæé™ã€‚è€Œç¨³å®šçš„å¯æ‰©å±•è®­ç»ƒç­–ç•¥éµå¾ªå¯é¢„æµ‹çš„è·¯å¾„ã€‚ </li>
<li>ç ”ç©¶é€šè¿‡å¤§é‡å®éªŒè§‚å¯Ÿå‘ç°ä¸åŒè®¾è®¡é€‰æ‹©å¯¹æ¸è¿›æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡çš„å½±å“ä¸åŒã€‚ </li>
<li>ç»“åˆè¿™äº›å‘ç°ï¼Œæå‡ºäº†ä¸€ç§æœ€ä½³å®è·µçš„è®­ç»ƒç­–ç•¥ScaleRLï¼Œå¹¶é€šè¿‡å°è§„æ¨¡è¿è¡Œçš„æ‰©å±•éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚ </li>
<li>è¯¥ç ”ç©¶ä¸ä»…æä¾›äº†ä¸€ä¸ªç§‘å­¦çš„æ¡†æ¶æ¥åˆ†æå¼ºåŒ–å­¦ä¹ ä¸­çš„æ‰©å±•æ€§ï¼Œè¿˜æå‡ºäº†ä¸€ç§å®ç”¨çš„è®­ç»ƒç­–ç•¥ï¼Œä½¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ›´æ¥è¿‘é•¿æœŸå®ç°çš„é¢„è®­ç»ƒé¢„æµ‹æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13786">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ea5f0e0c428acff5ca2783f4fe80f642~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810076&auth_key=1760810076-0-0-a225fb92544074bb9ab95689202dbd70&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-06a5b036f3efb1faf2708f5f255103c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810083&auth_key=1760810083-0-0-35bec6439c53088e110f16f2b1ab9f30&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0e9418cc38c366a6bea7f50b0c6afcf9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810090&auth_key=1760810090-0-0-b707e5a8fe5ba5800a422bcad9c5799b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2abbb13e70b287f8252bfdc8183d5a9d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810097&auth_key=1760810097-0-0-e61617465fd982efdc6225c0aaad1275&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Beyond-Static-LLM-Policies-Imitation-Enhanced-Reinforcement-Learning-for-Recommendation"><a href="#Beyond-Static-LLM-Policies-Imitation-Enhanced-Reinforcement-Learning-for-Recommendation" class="headerlink" title="Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning   for Recommendation"></a>Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning   for Recommendation</h2><p><strong>Authors:Yi Zhang, Lili Xie, Ruihong Qiu, Jiajun Liu, Sen Wang</strong></p>
<p>Recommender systems (RecSys) have become critical tools for enhancing user engagement by delivering personalized content across diverse digital platforms. Recent advancements in large language models (LLMs) demonstrate significant potential for improving RecSys, primarily due to their exceptional generalization capabilities and sophisticated contextual understanding, which facilitate the generation of flexible and interpretable recommendations. However, the direct deployment of LLMs as primary recommendation policies presents notable challenges, including persistent latency issues stemming from frequent API calls and inherent model limitations such as hallucinations and biases. To address these issues, this paper proposes a novel offline reinforcement learning (RL) framework that leverages imitation learning from LLM-generated trajectories. Specifically, inverse reinforcement learning is employed to extract robust reward models from LLM demonstrations. This approach negates the need for LLM fine-tuning, thereby substantially reducing computational overhead. Simultaneously, the RL policy is guided by the cumulative rewards derived from these demonstrations, effectively transferring the semantic insights captured by the LLM. Comprehensive experiments conducted on two benchmark datasets validate the effectiveness of the proposed method, demonstrating superior performance when compared against state-of-the-art RL-based and in-context learning baselines. The code can be found at <a target="_blank" rel="noopener" href="https://github.com/ArronDZhang/IL-Rec">https://github.com/ArronDZhang/IL-Rec</a>. </p>
<blockquote>
<p>æ¨èç³»ç»Ÿï¼ˆRecSysï¼‰å·²æˆä¸ºé€šè¿‡ä¸åŒæ•°å­—å¹³å°æä¾›ä¸ªæ€§åŒ–å†…å®¹æ¥æé«˜ç”¨æˆ·å‚ä¸åº¦çš„é‡è¦å·¥å…·ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ€æ–°è¿›å±•æ˜¾ç¤ºå‡ºåœ¨æé«˜æ¨èç³»ç»Ÿæ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬å‡ºè‰²çš„æ³›åŒ–èƒ½åŠ›å’Œå¤æ‚ä¸Šä¸‹æ–‡ç†è§£ï¼Œæœ‰åŠ©äºç”Ÿæˆçµæ´»ä¸”å¯è§£é‡Šçš„å»ºè®®ã€‚ç„¶è€Œï¼Œç›´æ¥å°†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºä¸»è¦çš„æ¨èç­–ç•¥éƒ¨ç½²ä¼šé‡åˆ°ä¸€äº›æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æºäºé¢‘ç¹APIè°ƒç”¨çš„æŒä¹…å»¶è¿Ÿé—®é¢˜ï¼Œä»¥åŠæ¨¡å‹æœ¬èº«çš„å›ºæœ‰å±€é™æ€§ï¼Œå¦‚å¹»æƒ³å’Œåè§ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨ä»å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è½¨è¿¹è¿›è¡Œæ¨¡ä»¿å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œé‡‡ç”¨é€†å‘å¼ºåŒ–å­¦ä¹ ä»å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¼”ç¤ºä¸­æå–ç¨³å¥çš„å¥–åŠ±æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•é¿å…äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ç²¾ç»†è°ƒæ•´éœ€æ±‚ï¼Œä»è€Œå¤§å¤§å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚åŒæ—¶ï¼Œå¼ºåŒ–å­¦ä¹ æ”¿ç­–ç”±è¿™äº›æ¼”ç¤ºä¸­å¾—å‡ºçš„ç´¯ç§¯å¥–åŠ±æ¥æŒ‡å¯¼ï¼Œæœ‰æ•ˆåœ°è½¬ç§»äº†å¤§å‹è¯­è¨€æ¨¡å‹æ•è·çš„è¯­ä¹‰æ´å¯ŸåŠ›ã€‚åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¿›è¡Œçš„ç»¼åˆå®éªŒéªŒè¯äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸æœ€æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å’Œä¸Šä¸‹æ–‡å­¦ä¹ åŸºçº¿ç›¸æ¯”ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/ArronDZhang/IL-Rec%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/ArronDZhang/IL-Recæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13229v1">PDF</a> ICDM 2025 Accepted Paper</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨èç³»ç»Ÿå› å…¶å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œä¸°å¯Œçš„ä¸Šä¸‹æ–‡ç†è§£è€Œå±•ç°å‡ºå·¨å¤§çš„æ½œåŠ›ã€‚ç„¶è€Œï¼Œç›´æ¥éƒ¨ç½²LLMä½œä¸ºä¸»è¦çš„æ¨èç­–ç•¥é¢ä¸´æŒ‘æˆ˜ï¼Œå¦‚å»¶è¿Ÿé—®é¢˜å’Œæ¨¡å‹å›ºæœ‰å±€é™ã€‚æœ¬æ–‡æå‡ºä¸€ç§åˆ©ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡æ¨¡ä»¿å­¦ä¹ ä»LLMç”Ÿæˆçš„è½¨è¿¹ä¸­å­¦ä¹ ã€‚å…·ä½“è€Œè¨€ï¼Œåˆ©ç”¨é€†å‘å¼ºåŒ–å­¦ä¹ ä»LLMæ¼”ç¤ºä¸­æå–ç¨³å¥çš„å¥–åŠ±æ¨¡å‹ï¼Œæ— éœ€å¯¹LLMè¿›è¡Œå¾®è°ƒï¼Œé™ä½äº†è®¡ç®—å¼€é”€ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¨èç³»ç»Ÿï¼ˆRecSysï¼‰é€šè¿‡æä¾›ä¸ªæ€§åŒ–å†…å®¹å¢å¼ºäº†ç”¨æˆ·å‚ä¸åº¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºè‰²æ³›åŒ–èƒ½åŠ›å’Œä¸Šä¸‹æ–‡ç†è§£æ½œåŠ›æå‡äº†æ¨èç³»ç»Ÿçš„æ€§èƒ½ã€‚</li>
<li>ç›´æ¥ä½¿ç”¨LLMä½œä¸ºæ¨èç­–ç•¥ä¸»è¦é¢ä¸´å»¶è¿Ÿå’Œæ¨¡å‹å±€é™ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æ–°æ–¹æ³•åˆ©ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œé€šè¿‡æ¨¡ä»¿å­¦ä¹ ä»LLMç”Ÿæˆçš„è½¨è¿¹ä¸­æå–å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>é€†å‘å¼ºåŒ–å­¦ä¹ ç”¨äºæå–ç¨³å¥çš„å¥–åŠ±æ¨¡å‹ï¼Œæ— éœ€å¯¹LLMè¿›è¡Œå¾®è°ƒã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç´¯è®¡å¥–åŠ±æŒ‡å¯¼RLç­–ç•¥ï¼Œæœ‰æ•ˆè½¬ç§»LLMæ•è·çš„è¯­ä¹‰è§è§£ã€‚</li>
<li>å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŸºå‡†æ•°æ®é›†ä¸Šè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œç›¸è¾ƒäºå…¶ä»–å…ˆè¿›çš„RLå’Œä¸Šä¸‹æ–‡å­¦ä¹ åŸºçº¿æœ‰ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13229">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-0c526484e1967111cfc6dc856d686825~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810104&auth_key=1760810104-0-0-9274ae9a5c117323e277f36d95eb27eb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-10c73d68278b0a3f1776119af97671d4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810111&auth_key=1760810111-0-0-078492cbcfd0acbac68ea8d186e5e238&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2081d02c9496e9a0088972b265809922~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810152&auth_key=1760810152-0-0-f145a6886e16063fe2ef829f934c74f8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="CoT-Evo-Evolutionary-Distillation-of-Chain-of-Thought-for-Scientific-Reasoning"><a href="#CoT-Evo-Evolutionary-Distillation-of-Chain-of-Thought-for-Scientific-Reasoning" class="headerlink" title="CoT-Evo: Evolutionary Distillation of Chain-of-Thought for Scientific   Reasoning"></a>CoT-Evo: Evolutionary Distillation of Chain-of-Thought for Scientific   Reasoning</h2><p><strong>Authors:Kehua Feng, Keyan Ding, Zhihui Zhu, Lei Liang, Qiang Zhang, Huajun Chen</strong></p>
<p>While chain-of-thought (CoT) distillation from advanced large language models (LLMs) has proven effective in general reasoning tasks, it struggles in scientific domains where even advanced models often produce incorrect or superficial reasoning due to high complexity and specialized knowledge requirements. Directly distilling from such flawed outputs results in low-quality training data and limits the performance of smaller student models. To overcome this, we propose CoT-Evo, an evolutionary CoT distillation framework. It begins by constructing a diverse pool of reasoning trajectories from multiple LLM thinkers, enriches them with automatically retrieved domain knowledge, and iteratively refines the trajectories using novelty-driven selection, reflective recombination and mutation. The refinement is guided by a fitness function that evaluates answer correctness, coherence, and effective knowledge utilization. This results in a high-quality CoT dataset tailored for scientific reasoning. We employ this evolved dataset to fine-tune a compact model, which achieves state-of-the-art performance on scientific reasoning benchmarks. Our work establishes a scalable approach to synthesizing high-fidelity scientific reasoning data from diverse and fallible LLMs. </p>
<blockquote>
<p>è™½ç„¶åŸºäºå…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€ç»´é“¾ï¼ˆCoTï¼‰è’¸é¦æŠ€æœ¯åœ¨ä¸€èˆ¬æ¨ç†ä»»åŠ¡ä¸­å·²è¢«è¯æ˜æ˜¯æœ‰æ•ˆçš„ï¼Œä½†åœ¨ç§‘å­¦é¢†åŸŸï¼Œç”±äºé«˜åº¦å¤æ‚æ€§å’Œä¸“ä¸šçŸ¥è¯†è¦æ±‚ï¼Œå³ä½¿æ˜¯å…ˆè¿›æ¨¡å‹ä¹Ÿå¾€å¾€ä¼šäº§ç”Ÿé”™è¯¯æˆ–è‚¤æµ…çš„æ¨ç†ã€‚ç›´æ¥ä»è¿™ç§æœ‰ç¼ºé™·çš„è¾“å‡ºä¸­è¿›è¡Œè’¸é¦ä¼šå¯¼è‡´è®­ç»ƒæ•°æ®è´¨é‡ä½ä¸‹ï¼Œå¹¶é™åˆ¶å°å‹å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†CoT-Evoï¼Œè¿™æ˜¯ä¸€ä¸ªè¿›åŒ–çš„CoTè’¸é¦æ¡†æ¶ã€‚å®ƒé¦–å…ˆä»ä¸åŒLLMæ€è€ƒè€…æ„å»ºå¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹æ± ï¼Œé€šè¿‡è‡ªåŠ¨æ£€ç´¢é¢†åŸŸçŸ¥è¯†æ¥ä¸°å¯Œå®ƒä»¬ï¼Œå¹¶é€šè¿‡æ–°å¥‡æ€§é©±åŠ¨çš„é€‰æ‹©ã€åæ€æ€§é‡ç»„å’Œçªå˜æ¥è¿­ä»£ä¼˜åŒ–è¿™äº›è½¨è¿¹ã€‚ä¼˜åŒ–ç”±é€‚åº”åº¦å‡½æ•°å¼•å¯¼ï¼Œè¯¥å‡½æ•°è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€è¿è´¯æ€§å’Œæœ‰æ•ˆçš„çŸ¥è¯†åˆ©ç”¨ã€‚è¿™äº§ç”Ÿäº†ä¸€ä¸ªé’ˆå¯¹ç§‘å­¦æ¨ç†é‡èº«å®šåˆ¶çš„é«˜è´¨é‡CoTæ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªè¿›åŒ–æ•°æ®é›†å¯¹ç´§å‡‘æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè¯¥æ¨¡å‹åœ¨ç§‘å­¦ç´ å…»è¯„ä¼°ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œå»ºç«‹äº†ä¸€ç§ä»å¤šæ ·åŒ–å’Œæ˜“å‡ºé”™çš„LLMä¸­åˆæˆé«˜ä¿çœŸç§‘å­¦æ¨ç†æ•°æ®çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13166v2">PDF</a> 28 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>å…ˆè¿›çš„é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è’¸é¦æŠ€æœ¯åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„é€šç”¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç§‘å­¦é¢†åŸŸå­˜åœ¨æŒ‘æˆ˜ã€‚ç”±äºé«˜åº¦å¤æ‚æ€§å’Œä¸“ä¸šçŸ¥è¯†éœ€æ±‚ï¼Œå³ä½¿é«˜çº§æ¨¡å‹ä¹Ÿä¼šäº§ç”Ÿé”™è¯¯æˆ–è‚¤æµ…çš„æ¨ç†ã€‚ç›´æ¥è’¸é¦è¿™äº›æœ‰ç¼ºé™·çš„è¾“å‡ºä¼šå¯¼è‡´ä½è´¨é‡çš„è®­ç»ƒæ•°æ®ï¼Œå¹¶é™åˆ¶å°å‹å­¦ç”Ÿæ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†CoT-Evoï¼Œä¸€ç§è¿›åŒ–å¼CoTè’¸é¦æ¡†æ¶ã€‚å®ƒé¦–å…ˆæ„å»ºå¤šç§æ¨ç†è½¨è¿¹çš„æ± ï¼Œè¿™äº›è½¨è¿¹æ¥æºäºå¤šä¸ªLLMçš„æ€è€ƒè€…ï¼Œå¹¶è‡ªåŠ¨ä¸°å¯Œå…¶é¢†åŸŸçŸ¥è¯†ï¼Œé€šè¿‡æ–°é¢–æ€§é©±åŠ¨çš„é€‰æ‹©ã€åæ€æ€§é‡ç»„å’Œçªå˜è¿›è¡Œè¿­ä»£ä¼˜åŒ–è½¨è¿¹ã€‚ä¼˜åŒ–ç”±é€‚åº”åº¦å‡½æ•°å¼•å¯¼ï¼Œè¯¥å‡½æ•°è¯„ä¼°ç­”æ¡ˆçš„æ­£ç¡®æ€§ã€è¿è´¯æ€§å’Œæœ‰æ•ˆçš„çŸ¥è¯†åˆ©ç”¨ã€‚è¿™äº§ç”Ÿäº†ä¸€ä¸ªé€‚ç”¨äºç§‘å­¦æ¨ç†çš„é«˜è´¨é‡CoTæ•°æ®é›†ã€‚æˆ‘ä»¬åˆ©ç”¨è¿™ä¸ªè¿›åŒ–çš„æ•°æ®é›†å¯¹ç´§å‡‘æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œåœ¨ç§‘æŠ€æ¨ç†åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„å·¥ä½œå»ºç«‹äº†ä¸€ç§ä»å¤šæ ·åŒ–å’Œæ˜“å‡ºé”™çš„LLMä¸­åˆæˆé«˜ä¿çœŸç§‘å­¦æ¨ç†æ•°æ®çš„å¯æ‰©å±•æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰è’¸é¦åœ¨é€šç”¨æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç§‘å­¦é¢†åŸŸé¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>ç›´æ¥ä»æœ‰ç¼ºé™·çš„æ¨¡å‹è’¸é¦ä¼šå¯¼è‡´ä½è´¨é‡è®­ç»ƒæ•°æ®ã€‚</li>
<li>CoT-Evoæ¡†æ¶é€šè¿‡æ„å»ºå¤šæ ·åŒ–çš„æ¨ç†è½¨è¿¹æ± æ¥è§£å†³é—®é¢˜ï¼Œè¿™äº›è½¨è¿¹æ¥æºäºå¤šä¸ªLLMçš„æ€è€ƒè€…ã€‚</li>
<li>è‡ªåŠ¨æ£€ç´¢çš„é¢†åŸŸçŸ¥è¯†ä¸°å¯Œäº†è¿™äº›è½¨è¿¹ã€‚</li>
<li>é€šè¿‡æ–°é¢–æ€§é©±åŠ¨çš„é€‰æ‹©ã€åæ€æ€§é‡ç»„å’Œçªå˜è¿›è¡Œè¿­ä»£ä¼˜åŒ–è½¨è¿¹ã€‚</li>
<li>ä¸€ä¸ªé€‚åº”åº¦å‡½æ•°ç”¨äºè¯„ä¼°ç­”æ¡ˆçš„è´¨é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2c90d873e7697dced8745a0652daadbd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810160&auth_key=1760810160-0-0-b0473ecc62b90303cd3f4d1d9d4ad2b9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5617822cba49cbcc95334cb5f7a0de12~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810167&auth_key=1760810167-0-0-0eb9c02ce1c4363bbc50bf3112162b6b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Retrieval-in-the-Chain-Bootstrapping-Large-Language-Models-for-Generative-Retrieval"><a href="#Retrieval-in-the-Chain-Bootstrapping-Large-Language-Models-for-Generative-Retrieval" class="headerlink" title="Retrieval-in-the-Chain: Bootstrapping Large Language Models for   Generative Retrieval"></a>Retrieval-in-the-Chain: Bootstrapping Large Language Models for   Generative Retrieval</h2><p><strong>Authors:Yingchen zhang, Ruqing zhang, Jiafeng Guo, Wenjun Peng, Sen Li, Fuyu Lv</strong></p>
<p>Generative retrieval (GR) is an emerging paradigm that leverages large language models (LLMs) to autoregressively generate document identifiers (docids) relevant to a given query. Prior works have focused on leveraging the generative capabilities of LLMs to improve GR, while overlooking that their reasoning capabilities could likewise help. This raises a key question: Can explicit reasoning benefit GR? To investigate, we first conduct a preliminary study where an LLM is prompted to generate free-form chain-of-thought (CoT) reasoning before performing constrained docid decoding. Although this method outperforms standard GR, the generated reasoning tends to be verbose and poorly aligned with the docid space. These limitations motivate the development of a reasoning mechanism better tailored to GR.   Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented framework for GR that converts free-form CoT reasoning into a compact, structured format, and iteratively refines the reasoning during the retrieval process. R4R augments an existing GR method by leveraging a reasoning-capable LLM that has been instruction-tuned for GR. At inference time, R4R first uses the LLM to generate an initial structured reasoning; then the same LLM alternates between (i) constrained decoding with the chosen GR method to produce candidate docids and (ii) updating the reasoning based on retrieval results to improve the next round. R4R does not require additional models or training, and instead a single LLM serves as both the reasoning generator and the retriever. Extensive experiments on Natural Questions, MS MARCO, and a real-world item-search benchmark validate the effectiveness of R4R. </p>
<blockquote>
<p>ç”Ÿæˆå¼æ£€ç´¢ï¼ˆGRï¼‰æ˜¯ä¸€ç§æ–°å…´èŒƒå¼ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è‡ªåŠ¨ç”Ÿæˆä¸ç»™å®šæŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£æ ‡è¯†ç¬¦ï¼ˆdocidsï¼‰ã€‚æ—©æœŸçš„ç ”ç©¶å·¥ä½œä¸»è¦é›†ä¸­åœ¨åˆ©ç”¨LLMçš„ç”Ÿæˆèƒ½åŠ›æ¥æé«˜GRçš„æ€§èƒ½ï¼Œå´å¿½è§†äº†å®ƒä»¬çš„æ¨ç†èƒ½åŠ›åŒæ ·å¯ä»¥å¸®åŠ©è¿™ä¸€ç‚¹ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šæ˜ç¡®çš„æ¨ç†èƒ½å¦æœ‰ç›ŠäºGRï¼Ÿä¸ºäº†æ¢ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†ä¸€é¡¹åˆæ­¥ç ”ç©¶ï¼Œæç¤ºLLMåœ¨æ‰§è¡Œçº¦æŸdocidè§£ç ä¹‹å‰ç”Ÿæˆè‡ªç”±å½¢å¼çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ã€‚å°½ç®¡è¿™ç§æ–¹æ³•ä¼˜äºæ ‡å‡†çš„GRï¼Œä½†ç”Ÿæˆçš„æ¨ç†å¾€å¾€è¿‡äºå†—é•¿ï¼Œä¸”ä¸docidç©ºé—´ä¸å¤ªå¯¹é½ã€‚è¿™äº›å±€é™æ€§ä¿ƒä½¿æˆ‘ä»¬å¼€å‘ä¸€ç§æ›´é€‚åˆGRçš„æ¨ç†æœºåˆ¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Reason-for-Retrievalï¼ˆR4Rï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºGRçš„æ¨ç†å¢å¼ºæ¡†æ¶ï¼Œå®ƒå°†è‡ªç”±å½¢å¼çš„CoTæ¨ç†è½¬æ¢ä¸ºç´§å‡‘çš„ç»“æ„åŒ–æ ¼å¼ï¼Œå¹¶åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­è¿­ä»£åœ°ä¼˜åŒ–æ¨ç†ã€‚R4Ré€šè¿‡åˆ©ç”¨å…·å¤‡GRæŒ‡ä»¤è°ƒä¼˜çš„å…·æœ‰æ¨ç†èƒ½åŠ›çš„LLMæ¥å¢å¼ºç°æœ‰çš„GRæ–¹æ³•ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒR4Ré¦–å…ˆä½¿ç”¨LLMç”Ÿæˆåˆå§‹çš„ç»“æ„åŒ–æ¨ç†ï¼›ç„¶åï¼Œç›¸åŒçš„LLMäº¤æ›¿æ‰§è¡Œï¼ˆiï¼‰ä½¿ç”¨æ‰€é€‰çš„GRæ–¹æ³•è¿›è¡Œçº¦æŸè§£ç ä»¥äº§ç”Ÿå€™é€‰docidsï¼Œï¼ˆiiï¼‰åŸºäºæ£€ç´¢ç»“æœæ›´æ–°æ¨ç†ä»¥æ”¹è¿›ä¸‹ä¸€è½®æ¬¡çš„æ£€ç´¢ã€‚R4Rä¸éœ€è¦é¢å¤–çš„æ¨¡å‹æˆ–è®­ç»ƒï¼Œè€Œæ˜¯ä½¿ç”¨å•ä¸ªLLMåŒæ—¶ä½œä¸ºæ¨ç†ç”Ÿæˆå™¨å’Œæ£€ç´¢å™¨ã€‚åœ¨è‡ªç„¶é—®é¢˜ã€MS MARCOå’Œç°å®ä¸–ç•Œä¸­çš„å•†å“æœç´¢åŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†R4Rçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13095v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç”Ÿæˆå¼æ£€ç´¢ï¼ˆGRï¼‰çš„æ–°èŒƒå¼ï¼Œè¯¥èŒƒå¼åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆèƒ½åŠ›æ¥ç”Ÿæˆä¸ç»™å®šæŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£æ ‡è¯†ç¬¦ï¼ˆdocidï¼‰ã€‚å°½ç®¡ä¹‹å‰çš„ç ”ç©¶å·²ç»å‘ç°LLMçš„ç”Ÿæˆèƒ½åŠ›å¯ä»¥æ”¹å–„GRï¼Œä½†å®ƒä»¬å¾€å¾€å¿½ç•¥äº†LLMçš„æ¨ç†èƒ½åŠ›ä¹Ÿå¯èƒ½å¯¹æ­¤æœ‰æ‰€å¸®åŠ©ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ—¨åœ¨æ¢è®¨æ˜¾å¼æ¨ç†æ˜¯å¦æœ‰åŠ©äºGRã€‚ä¸ºäº†ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…é¦–å…ˆè¿›è¡Œäº†ä¸€é¡¹åˆæ­¥ç ”ç©¶ï¼Œå‘ç°è™½ç„¶é€šè¿‡æç¤ºLLMè¿›è¡Œè‡ªç”±å½¢å¼çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å†è¿›è¡Œçº¦æŸdocidè§£ç çš„æ–¹æ³•ä¼˜äºæ ‡å‡†GRï¼Œä½†ç”Ÿæˆçš„æ¨ç†å¾€å¾€è¿‡äºå†—é•¿ä¸”éš¾ä»¥ä¸docidç©ºé—´å¯¹é½ã€‚å› æ­¤ï¼Œä½œè€…æå‡ºäº†é’ˆå¯¹GRçš„é‡èº«å®šåˆ¶çš„æ¨ç†æœºåˆ¶â€”â€”Reason-for-Retrievalï¼ˆR4Rï¼‰ã€‚R4Rå°†è‡ªç”±å½¢å¼çš„CoTæ¨ç†è½¬æ¢ä¸ºç´§å‡‘çš„ç»“æ„åŒ–æ ¼å¼ï¼Œå¹¶åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­è¿­ä»£åœ°ä¼˜åŒ–æ¨ç†ã€‚R4Ré€šè¿‡åœ¨ç°æœ‰çš„GRæ–¹æ³•ä¸Šå¢åŠ å…·å¤‡GRæŒ‡ä»¤è°ƒä¼˜èƒ½åŠ›çš„æ¨ç†å‹LLMæ¥å®ç°å¢å¼ºæ•ˆæœã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒR4Ré¦–å…ˆåˆ©ç”¨LLMç”Ÿæˆåˆå§‹ç»“æ„åŒ–æ¨ç†ï¼Œç„¶ååœ¨é€‰æ‹©çš„GRæ–¹æ³•çš„çº¦æŸè§£ç ä¸åŸºäºæ£€ç´¢ç»“æœçš„æ¨ç†æ›´æ–°ä¹‹é—´äº¤æ›¿è¿›è¡Œï¼Œä»¥äº§ç”Ÿå€™é€‰docidsã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒR4Råœ¨Natural Questionsã€MS MARCOä»¥åŠçœŸå®ä¸–ç•Œçš„ç‰©å“æœç´¢åŸºå‡†æµ‹è¯•ä¸Šå‡æœ‰æ•ˆã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç”Ÿæˆå¼æ£€ç´¢ï¼ˆGRï¼‰åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£æ ‡è¯†ç¬¦ï¼ˆdocidï¼‰ã€‚</li>
<li>æ­¤å‰çš„ç ”ç©¶ä¸»è¦å…³æ³¨LLMçš„ç”Ÿæˆèƒ½åŠ›å¯¹GRçš„æ”¹è¿›ï¼Œä½†å¿½è§†äº†å…¶æ¨ç†èƒ½åŠ›å¯èƒ½å¸¦æ¥çš„ç›Šå¤„ã€‚</li>
<li>åˆæ­¥ç ”ç©¶è¡¨æ˜ï¼Œç»“åˆLLMçš„è‡ªç”±å½¢å¼æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†å’Œçº¦æŸdocidè§£ç çš„æ–¹æ³•è™½ä¼˜äºæ ‡å‡†GRï¼Œä½†å­˜åœ¨æ¨ç†è¿‡äºå†—é•¿å’Œä¸docidç©ºé—´ä¸å¯¹é½çš„é—®é¢˜ã€‚</li>
<li>æå‡ºReason-for-Retrievalï¼ˆR4Rï¼‰æ¡†æ¶ï¼Œå°†è‡ªç”±å½¢å¼çš„CoTæ¨ç†è½¬åŒ–ä¸ºç´§å‡‘çš„ç»“æ„åŒ–æ ¼å¼ï¼Œå¹¶åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­è¿­ä»£ä¼˜åŒ–æ¨ç†ã€‚</li>
<li>R4Ré€šè¿‡å¢åŠ å…·å¤‡GRæŒ‡ä»¤è°ƒä¼˜èƒ½åŠ›çš„æ¨ç†å‹LLMæ¥å¢å¼ºç°æœ‰GRæ–¹æ³•ã€‚</li>
<li>R4Råœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºæœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬Natural Questionsã€MS MARCOä»¥åŠçœŸå®ä¸–ç•Œçš„ç‰©å“æœç´¢ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13095">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-9f65b0defe59ec3f4474392d16b1409a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810175&auth_key=1760810175-0-0-6ee3c3e691a72b3eb1c213bf7424bc3d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8f7d3a2d2e9b59fa6c5809598e151579~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810182&auth_key=1760810182-0-0-56824d2b8c1b6ef9f05edc457abc82fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-54909d62486394003d7ba95199ba7376~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810189&auth_key=1760810189-0-0-461d6855d0769f74fdc807efb1183f4f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-338aaccc421e2acb56f970c6db9620dd~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810195&auth_key=1760810195-0-0-79e6ad72fa7a16d89926c53f94d5ca8f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-33162bd3fe6dd5fa8c9a9eea6f9c80a6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810203&auth_key=1760810203-0-0-5f9397b2c3f6c46b5e89a744aee0a53b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="A-2-FM-An-Adaptive-Agent-Foundation-Model-for-Tool-Aware-Hybrid-Reasoning"><a href="#A-2-FM-An-Adaptive-Agent-Foundation-Model-for-Tool-Aware-Hybrid-Reasoning" class="headerlink" title="A$^2$FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid   Reasoning"></a>A$^2$FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid   Reasoning</h2><p><strong>Authors:Qianben Chen, Jingyi Cao, Jiayu Zhang, Tianrui Qin, Xiaowan Li, King Zhu, Dingfeng Shi, He Zhu, Minghao Liu, Xiaobo Liang, Xin Gui, Ge Zhang, Jian Yang, Yuchen Eleanor Jiang, Wangchunshu Zhou</strong></p>
<p>Large language models split into two families: reasoning-centric LLMs, which strengthen internal chain-of-thought reasoning but cannot invoke external tools, and agentic LLMs, which learn to interact with environments and leverage tools but often lag in deep reasoning. This divide arises from fundamentally different training objectives, leading to mismatched strengths and inefficiency on simple queries, where both families tend to overthink or over-call tools. In this work, we present Adaptive Agent Foundation Model (A$^2$FM), a unified framework that follows a route-then-align principle: the model first learns task-aware routing and then aligns mode-specific trajectories under a shared backbone. To address the inefficiency gap, we introduce a third mode-instant-that handles simple queries directly, preventing unnecessary reasoning or tool calls while complementing the agentic and reasoning modes. To jointly enhance accuracy and efficiency, we propose Adaptive Policy Optimization (APO), which enforces adaptive sampling across modes and applies a cost-regularized reward. On the 32B scale, A$^2$FM achieves 13.4% on BrowseComp, 70.4% on AIME25, and 16.7% on HLE, setting new SOTA among comparable models and performing competitively with frontier LLMs across agentic, reasoning, and general benchmarks. Notably, the adaptive execution achieves a cost of pass of only $0.00487 per correct answer-cutting cost by 45.2% relative to reasoning and 33.5% relative to agentic, thus delivering substantially higher cost efficiency while maintaining comparable accuracy. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ä¸»è¦åˆ†ä¸ºä¸¤ä¸ªå®¶æ—ï¼šä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå®ƒä»¬å¢å¼ºäº†å†…éƒ¨æ€ç»´é“¾çš„æ¨ç†èƒ½åŠ›ï¼Œä½†ä¸èƒ½è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼›ä»¥åŠèƒ½å¤Ÿå­¦ä¹ ä¸ç¯å¢ƒäº’åŠ¨å¹¶åˆ©ç”¨å·¥å…·çš„è¯­è¨€æ¨¡å‹ï¼Œä½†å®ƒä»¬å¾€å¾€åœ¨æ·±åº¦æ¨ç†ä¸Šè¡¨ç°æ»åã€‚è¿™ç§åˆ†æ­§æºäºæ ¹æœ¬ä¸åŒçš„è®­ç»ƒç›®æ ‡ï¼Œå¯¼è‡´åœ¨ç®€å•æŸ¥è¯¢ä¸Šçš„ä¼˜åŠ¿ä¸åŒ¹é…ï¼Œä¸¤ä¸ªå®¶æ—éƒ½å€¾å‘äºè¿‡åº¦æ€è€ƒæˆ–è¿‡åº¦è°ƒç”¨å·¥å…·ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”ä»£ç†åŸºç¡€æ¨¡å‹ï¼ˆA$^2$FMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªéµå¾ªâ€œå…ˆè·¯ç”±åå¯¹é½â€åŸåˆ™çš„ç»Ÿæ¡†æ¶ï¼šæ¨¡å‹é¦–å…ˆå­¦ä¹ ä»»åŠ¡æ„ŸçŸ¥è·¯ç”±ï¼Œç„¶ååœ¨å…±äº«ä¸»å¹²ä¸‹å¯¹é½ç‰¹å®šæ¨¡å¼çš„è½¨è¿¹ã€‚ä¸ºäº†è§£å†³æ•ˆç‡å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç¬¬ä¸‰ç§æ¨¡å¼å³æ—¶æ¨¡å¼ï¼Œè¯¥æ¨¡å¼ç›´æ¥å¤„ç†ç®€å•æŸ¥è¯¢ï¼Œé˜²æ­¢ä¸å¿…è¦çš„æ¨ç†æˆ–å·¥å…·è°ƒç”¨ï¼ŒåŒæ—¶è¡¥å……ä»£ç†æ¨¡å¼å’Œæ¨ç†æ¨¡å¼ã€‚ä¸ºäº†åŒæ—¶æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼ˆAPOï¼‰ï¼Œå®ƒå¼ºåˆ¶æ‰§è¡Œè·¨æ¨¡å¼çš„è‡ªé€‚åº”é‡‡æ ·ï¼Œå¹¶åº”ç”¨æˆæœ¬æ­£åˆ™åŒ–å¥–åŠ±ã€‚åœ¨32Bè§„æ¨¡ä¸Šï¼ŒA$^2$FMåœ¨BrowseCompä¸Šè¾¾åˆ°äº†13.4%ï¼Œåœ¨AIME25ä¸Šè¾¾åˆ°äº†70.4%ï¼Œåœ¨HLEä¸Šè¾¾åˆ°äº†16.7%ï¼Œåœ¨åŒç±»æ¨¡å‹ä¸­è¾¾åˆ°äº†æ–°çš„SOTAæ°´å¹³ï¼Œå¹¶åœ¨ä»£ç†ã€æ¨ç†å’Œé€šç”¨åŸºå‡†æµ‹è¯•ä¸­ä¸å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ç«äº‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè‡ªé€‚åº”æ‰§è¡Œä»…å®ç°äº†æ¯æ­£ç¡®ç­”æ¡ˆ0.00487çš„é€šè¿‡æˆæœ¬ï¼Œç›¸å¯¹äºæ¨ç†å‡å°‘äº†45.2%ï¼Œç›¸å¯¹äºä»£ç†å‡å°‘äº†33.5%ï¼Œä»è€Œåœ¨ä¿æŒç›¸å½“å‡†ç¡®æ€§çš„åŒæ—¶å®ç°äº†æ›´é«˜çš„æˆæœ¬æ•ˆç›Šã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12838v2">PDF</a> 9 pages, 5 figures, submitted to ICLR 2026</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹åˆ†ä¸ºä¸¤ä¸ªå®¶æ—ï¼šæ³¨é‡å†…éƒ¨æ¨ç†è¿‡ç¨‹çš„æ¨ç†ä¸­å¿ƒå‹LLMå’Œèƒ½å¤Ÿä¸ç¯å¢ƒäº’åŠ¨å¹¶åˆ©ç”¨å·¥å…·ä½†æ·±åº¦æ¨ç†èƒ½åŠ›è¾ƒå¼±çš„ä»£ç†å‹LLMã€‚ä¸¤è€…æºäºä¸åŒçš„è®­ç»ƒç›®æ ‡ï¼Œåœ¨å¤„ç†ç®€å•æŸ¥è¯¢æ—¶å¾€å¾€å­˜åœ¨èƒ½åŠ›ä¸åŒ¹é…ã€æ•ˆç‡ä¸é«˜çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†è‡ªé€‚åº”ä»£ç†åŸºé‡‘ä¼šæ¨¡å‹ï¼ˆAÂ²FMï¼‰ï¼Œé‡‡ç”¨â€œå…ˆè·¯ç”±åå¯¹é½â€çš„åŸåˆ™ï¼Œæ¨¡å‹é¦–å…ˆå­¦ä¹ ä»»åŠ¡æ„ŸçŸ¥è·¯ç”±ï¼Œç„¶ååœ¨å…±äº«ä¸»å¹²ä¸‹å¯¹é½ç‰¹å®šæ¨¡å¼è½¨è¿¹ã€‚ä¸ºè§£å†³æ•ˆç‡å·®è·é—®é¢˜ï¼Œå¼•å…¥ç¬¬ä¸‰ç§å³æ—¶æ¨¡å¼ï¼Œç›´æ¥å¤„ç†ç®€å•æŸ¥è¯¢ï¼Œé¿å…ä¸å¿…è¦çš„æ¨ç†æˆ–å·¥å…·è°ƒç”¨ï¼ŒåŒæ—¶è¡¥å……ä»£ç†å’Œæ¨ç†æ¨¡å¼ã€‚é€šè¿‡è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼ˆAPOï¼‰ï¼Œå¯¹ä¸‰ç§æ¨¡å¼è¿›è¡Œè‡ªé€‚åº”é‡‡æ ·å¹¶åº”ç”¨æˆæœ¬æ­£åˆ™åŒ–å¥–åŠ±ï¼Œä»¥æé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚åœ¨32Bè§„æ¨¡ä¸Šï¼ŒAÂ²FMåœ¨BrowseCompä¸Šè¾¾åˆ°13.4%ï¼Œåœ¨AIME25ä¸Šè¾¾åˆ°70.4%ï¼Œåœ¨HLEä¸Šè¾¾åˆ°16.7%ï¼Œåœ¨åŒç±»æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œå¹¶åœ¨å‰æ²¿LLMçš„åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°è‰¯å¥½ã€‚ç‰¹åˆ«æ˜¯è‡ªé€‚åº”æ‰§è¡Œåªéœ€æ¯æ­£ç¡®å›ç­”ä¸€ä¸ªé—®é¢˜èŠ±è´¹0.00487çš„æˆæœ¬ï¼Œç›¸è¾ƒäºæ¨ç†æ¨¡å¼å’Œä»£ç†æ¨¡å¼åˆ†åˆ«é™ä½äº†45.2%å’Œ33.5%çš„æˆæœ¬ï¼Œå®ç°äº†é«˜æˆæœ¬æ•ˆç›Šä¸è‰¯å¥½å‡†ç¡®æ€§çš„å¹³è¡¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åˆ†ä¸ºæ¨ç†ä¸­å¿ƒå‹LLMå’Œä»£ç†å‹LLMä¸¤ä¸ªå®¶æ—ï¼Œå„æœ‰ä¼˜ç¼ºç‚¹ã€‚</li>
<li>ä¸¤è€…å¤„ç†ç®€å•æŸ¥è¯¢æ—¶å­˜åœ¨èƒ½åŠ›ä¸åŒ¹é…å’Œæ•ˆç‡é—®é¢˜ã€‚</li>
<li>æå‡ºè‡ªé€‚åº”ä»£ç†åŸºé‡‘ä¼šæ¨¡å‹ï¼ˆAÂ²FMï¼‰ï¼Œé‡‡ç”¨â€œå…ˆè·¯ç”±åå¯¹é½â€åŸåˆ™ï¼Œä»¥æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
<li>å¼•å…¥ç¬¬ä¸‰ç§å³æ—¶æ¨¡å¼æ¥å¤„ç†ç®€å•æŸ¥è¯¢ï¼Œé¿å…ä¸å¿…è¦çš„æ¨ç†æˆ–å·¥å…·è°ƒç”¨ã€‚</li>
<li>é€šè¿‡è‡ªé€‚åº”ç­–ç•¥ä¼˜åŒ–ï¼ˆAPOï¼‰å®ç°ä¸‰ç§æ¨¡å¼çš„è‡ªé€‚åº”é‡‡æ ·å’Œå¥–åŠ±ã€‚</li>
<li>AÂ²FMåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬BrowseCompã€AIME25å’ŒHLEç­‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12838">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1a42057f466b9f05bb05e7668c40c50d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810210&auth_key=1760810210-0-0-537419f764d73c8fbd3c235909e2c761&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-25c0a9af954cfde953b1739a47986795~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810217&auth_key=1760810217-0-0-2f8387d5b0cc688fb154dea9bdd2d4c2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d352b381573d0add335a4dc8dc6f44a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810239&auth_key=1760810239-0-0-678c59d0d812ab6d15cf2a2bc7720192&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="BanglaMATH-A-Bangla-benchmark-dataset-for-testing-LLM-mathematical-reasoning-at-grades-6-7-and-8"><a href="#BanglaMATH-A-Bangla-benchmark-dataset-for-testing-LLM-mathematical-reasoning-at-grades-6-7-and-8" class="headerlink" title="BanglaMATH : A Bangla benchmark dataset for testing LLM mathematical   reasoning at grades 6, 7, and 8"></a>BanglaMATH : A Bangla benchmark dataset for testing LLM mathematical   reasoning at grades 6, 7, and 8</h2><p><strong>Authors:Tabia Tanzin Prama, Christopher M. Danforth, Peter Sheridan Dodds</strong></p>
<p>Large Language Models (LLMs) have tremendous potential to play a key role in supporting mathematical reasoning, with growing use in education and AI research. However, most existing benchmarks are limited to English, creating a significant gap for low-resource languages. For example, Bangla is spoken by nearly 250 million people who would collectively benefit from LLMs capable of native fluency. To address this, we present BanglaMATH, a dataset of 1.7k Bangla math word problems across topics such as Arithmetic, Algebra, Geometry, and Logical Reasoning, sourced from Bangla elementary school workbooks and annotated with details like grade level and number of reasoning steps. We have designed BanglaMATH to evaluate the mathematical capabilities of both commercial and open-source LLMs in Bangla, and we find that Gemini 2.5 Flash and DeepSeek V3 are the only models to achieve strong performance, with $\ge$ 80% accuracy across three elementary school grades. Furthermore, we assess the robustness and language bias of these top-performing LLMs by augmenting the original problems with distracting information, and translating the problems into English. We show that both LLMs fail to maintain robustness and exhibit significant performance bias in Bangla. Our study underlines current limitations of LLMs in handling arithmetic and mathematical reasoning in low-resource languages, and highlights the need for further research on multilingual and equitable mathematical understanding. Dataset link: \href{<a target="_blank" rel="noopener" href="https://github.com/TabiaTanzin/BanglaMATH-A-Bangla-benchmark-dataset-for-testing-LLM-mathematical-reasoning-at-grades-6-7-and-8.git%7D%7Bhttps://github.com/BanglaMATH%7D">https://github.com/TabiaTanzin/BanglaMATH-A-Bangla-benchmark-dataset-for-testing-LLM-mathematical-reasoning-at-grades-6-7-and-8.git}{https://github.com/BanglaMATH}</a> </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ”¯æŒæ•°å­¦æ¨ç†æ–¹é¢å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œå¹¶åœ¨æ•™è‚²å’Œäººå·¥æ™ºèƒ½ç ”ç©¶æ–¹é¢å¾—åˆ°è¶Šæ¥è¶Šå¹¿æ³›çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰åŸºå‡†æµ‹è¯•ä»…é™äºè‹±è¯­ï¼Œè¿™ä¸ºä½èµ„æºè¯­è¨€åˆ›é€ äº†é‡å¤§å·®è·ã€‚ä¾‹å¦‚ï¼Œå­ŸåŠ æ‹‰è¯­æœ‰å°†è¿‘2.5äº¿äººä½¿ç”¨ï¼Œè¿™äº›äººç¾¤å°†å…±åŒå—ç›Šäºèƒ½å¤Ÿç†Ÿç»ƒä½¿ç”¨å­ŸåŠ æ‹‰è¯­çš„LLMã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†BanglaMATHæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«1700ä¸ªæ¶µç›–ç®—æœ¯ã€ä»£æ•°ã€å‡ ä½•å’Œé€»è¾‘æ¨ç†ç­‰ä¸»é¢˜çš„å­ŸåŠ æ‹‰æ•°å­¦æ–‡å­—é—®é¢˜ï¼Œè¿™äº›é—®é¢˜æ¥æºäºå­ŸåŠ æ‹‰å°å­¦è¯¾æœ¬ï¼Œå¹¶è¯¦ç»†æ ‡æ³¨äº†å¹´çº§å’Œæ¨ç†æ­¥éª¤ç­‰ä¿¡æ¯ã€‚æˆ‘ä»¬è®¾è®¡BanglaMATHæ˜¯ä¸ºäº†è¯„ä¼°å•†ä¸šå’Œå¼€æºLLMåœ¨å­ŸåŠ æ‹‰è¯­çš„æ•°å­¦èƒ½åŠ›ï¼Œæˆ‘ä»¬å‘ç°Gemini 2.5 Flashå’ŒDeepSeek V3æ˜¯è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œåœ¨ä¸‰ä¸ªå°å­¦å¹´çº§ä¸­å‡†ç¡®ç‡â‰¥80%ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€šè¿‡æ·»åŠ å¹²æ‰°ä¿¡æ¯å’Œå°†é—®é¢˜ç¿»è¯‘æˆè‹±è¯­æ¥è¯„ä¼°è¿™äº›è¡¨ç°æœ€ä½³çš„LLMçš„é²æ£’æ€§å’Œè¯­è¨€åè§ã€‚æˆ‘ä»¬å‘ç°è¿™ä¸¤ä¸ªLLMéƒ½æ— æ³•ç»´æŒå…¶é²æ£’æ€§ï¼Œåœ¨å­ŸåŠ æ‹‰è¯­è¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½åè§ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†LLMåœ¨å¤„ç†ä½èµ„æºè¯­è¨€çš„ç®—æœ¯å’Œæ•°å­¦æ¨ç†æ–¹é¢çš„å½“å‰å±€é™æ€§ï¼Œå¹¶çªæ˜¾äº†å¯¹å¤šå…ƒå’Œå¹³ç­‰çš„æ•°å­¦ç†è§£è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶çš„å¿…è¦æ€§ã€‚æ•°æ®é›†é“¾æ¥ï¼š[ç‚¹å‡»æ­¤å¤„æŸ¥çœ‹æ•°æ®é›†](<a target="_blank" rel="noopener" href="https://github.com/TabiaTanzin/BanglaMATH-A-Bangla-benchmark-dataset-for-testing-LLM-mathematical-reasoning-at-grades-6-7-and-8.git%EF%BC%89">https://github.com/TabiaTanzin/BanglaMATH-A-Bangla-benchmark-dataset-for-testing-LLM-mathematical-reasoning-at-grades-6-7-and-8.gitï¼‰</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12836v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ•°å­¦æ¨ç†æ–¹é¢çš„æ½œåŠ›åŠå…¶åœ¨æ•™è‚²å’ŒAIç ”ç©¶ä¸­çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä»…é™äºè‹±è¯­ï¼Œå¯¹äºä½èµ„æºè¯­è¨€å­˜åœ¨æ˜¾è‘—å·®è·ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†BanglaMATHæ•°æ®é›†ï¼ŒåŒ…å«170ä¸‡é“å­ŸåŠ æ‹‰æ•°å­¦åº”ç”¨é¢˜ï¼Œæ¶µç›–ç®—æœ¯ã€ä»£æ•°ã€å‡ ä½•å’Œé€»è¾‘æ¨ç†ç­‰å¤šä¸ªä¸»é¢˜ã€‚æ•°æ®é›†æ—¨åœ¨è¯„ä¼°å•†ä¸šå’Œå¼€æºLLMåœ¨å­ŸåŠ æ‹‰è¯­ä¸­çš„æ•°å­¦èƒ½åŠ›ï¼Œå‘ç°ä»…æœ‰Gemini 2.5 Flashå’ŒDeepSeek V3è¡¨ç°ä¼˜å¼‚ï¼Œä½†å­˜åœ¨é²æ£’æ€§å’Œè¯­è¨€åè§é—®é¢˜ã€‚ç ”ç©¶å¼ºè°ƒäº†LLMåœ¨å¤„ç†ä½èµ„æºè¯­è¨€ä¸­çš„ç®—æœ¯å’Œæ•°å­¦æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œå¹¶å¼ºè°ƒäº†è¿›è¡Œå¤šè¯­ç§å’Œå…¬å¹³æ•°å­¦ç†è§£ç ”ç©¶çš„å¿…è¦æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦æ¨ç†æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯¹æ•™è‚²é¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨ä»·å€¼ã€‚</li>
<li>å½“å‰åŸºå‡†æµ‹è¯•ä¸»è¦å±€é™äºè‹±è¯­ï¼Œç¼ºä¹é’ˆå¯¹ä½èµ„æºè¯­è¨€çš„æµ‹è¯•æ ‡å‡†ã€‚</li>
<li>BanglaMATHæ•°æ®é›†ä¸ºå­ŸåŠ æ‹‰è¯­ç¯å¢ƒä¸‹çš„æ•°å­¦èƒ½åŠ›è¯„ä¼°æä¾›äº†é‡è¦èµ„æºã€‚</li>
<li>Gemini 2.5 Flashå’ŒDeepSeek V3åœ¨BanglaMATHä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†å­˜åœ¨é²æ£’æ€§å’Œè¯­è¨€åè§é—®é¢˜ã€‚</li>
<li>LLMåœ¨å¤„ç†ä½èµ„æºè¯­è¨€çš„ç®—æœ¯å’Œæ•°å­¦æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>éœ€è¦è¿›ä¸€æ­¥å¼€å±•å¤šè¯­ç§ç ”ç©¶ï¼Œæé«˜LLMçš„æ•°å­¦ç†è§£èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12836">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4015718106946bc3608eb892f0a6a3ad~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810246&auth_key=1760810246-0-0-3571998a627210800812ed2aaee694b4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-458f2ff0fcb15ea8c66924bc79af2592~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810254&auth_key=1760810254-0-0-7315ba6beeda8e4c3d92b54116e1bc76&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-04b05b890c1e8e8065adb7613da8be44~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810261&auth_key=1760810261-0-0-ea1471d2bc44ecfd51ce3b47d3291e27&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0d7851de6031edd1e091d145b698d9d9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810267&auth_key=1760810267-0-0-c61e98cc7762ba62b0eebed6644b50f0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-54666f1c25015cba32dda3b6fcbeaf5b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810274&auth_key=1760810274-0-0-e90c99df16ac3b8d4c51f69b79969612&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-31a55904b90a117d232dde51821f0944~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810281&auth_key=1760810281-0-0-27c77fd4b937102be46bd664126ffb94&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6b84f428ff4b8178d61ba9eb46e26dc0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810287&auth_key=1760810287-0-0-b21f5d95934c1b27b05f0777878a8a29&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bb780d1e622be540c961e9a4efbd1c08~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810294&auth_key=1760810294-0-0-27c29b953035b6908fc777b47858d949&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e287ae1752661d6f075c6dacecb10120~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810301&auth_key=1760810301-0-0-666ccdc2e4c8c92ad2b14bedef2d59fe&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Ax-Prover-A-Deep-Reasoning-Agentic-Framework-for-Theorem-Proving-in-Mathematics-and-Quantum-Physics"><a href="#Ax-Prover-A-Deep-Reasoning-Agentic-Framework-for-Theorem-Proving-in-Mathematics-and-Quantum-Physics" class="headerlink" title="Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in   Mathematics and Quantum Physics"></a>Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in   Mathematics and Quantum Physics</h2><p><strong>Authors:Marco Del Tredici, Jacob McCarran, Benjamin Breen, Javier Aspuru Mijares, Weichen Winston Yin, Jacob M. Taylor, Frank H. L. Koppens, Dirk Englund</strong></p>
<p>We present Ax-Prover, a multi-agent system for automated theorem proving in Lean that can solve problems across diverse scientific domains and operate either autonomously or collaboratively with human experts. To achieve this, Ax-Prover approaches scientific problem solving through formal proof generation, a process that demands both creative reasoning and strict syntactic rigor. Ax-Prover meets this challenge by equipping Large Language Models (LLMs), which provide knowledge and reasoning, with Lean tools via the Model Context Protocol (MCP), which ensure formal correctness. To evaluate its performance as an autonomous prover, we benchmark our approach against frontier LLMs and specialized prover models on two public math benchmarks and on two Lean benchmarks we introduce in the fields of abstract algebra and quantum theory. On public datasets, Ax-Prover is competitive with state-of-the-art provers, while it largely outperforms them on the new benchmarks. This shows that, unlike specialized systems that struggle to generalize, our tool-based agentic theorem prover approach offers a generalizable methodology for formal verification across diverse scientific domains. Furthermore, we demonstrate Ax-Proverâ€™s assistant capabilities in a practical use case, showing how it enabled an expert mathematician to formalize the proof of a complex cryptography theorem. </p>
<blockquote>
<p>æˆ‘ä»¬æ¨å‡ºäº†Ax-Proverï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºLeanè‡ªåŠ¨å®šç†è¯æ˜çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå¯ä»¥è§£å†³ä¸åŒç§‘å­¦é¢†åŸŸçš„å„ç§é—®é¢˜ï¼Œå¹¶å¯ä»¥è‡ªä¸»è¿è¡Œæˆ–ä¸äººç±»ä¸“å®¶åä½œã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼ŒAx-Proveré€šè¿‡å½¢å¼åŒ–è¯æ˜ç”Ÿæˆæ¥è§£å†³ç§‘å­¦é—®é¢˜ï¼Œè¿™ä¸€è¿‡ç¨‹æ—¢éœ€è¦åˆ›é€ æ€§çš„æ¨ç†åˆéœ€è¦ä¸¥æ ¼çš„å¥æ³•ä¸¥è°¨æ€§ã€‚Ax-Proveré€šè¿‡ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›æ¥åº”å¯¹è¿™ä¸€æŒ‘æˆ˜ï¼Œå¹¶é€šè¿‡æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰ä¸Leanå·¥å…·ç›¸ç»“åˆï¼Œç¡®ä¿å½¢å¼æ­£ç¡®æ€§ã€‚ä¸ºäº†è¯„ä¼°å…¶ä½œä¸ºè‡ªä¸»è¯æ˜å™¨çš„æ€§èƒ½ï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¬å…±æ•°å­¦åŸºå‡†æµ‹è¯•ä»¥åŠæˆ‘ä»¬åœ¨æŠ½è±¡ä»£æ•°å’Œé‡å­ç†è®ºé¢†åŸŸå¼•å…¥çš„ä¸¤ä¸ªLeanåŸºå‡†æµ‹è¯•ä¸Šï¼Œå°†æˆ‘ä»¬çš„æ–¹æ³•ä¸å‰æ²¿çš„LLMå’Œä¸“ç”¨è¯æ˜å™¨æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šï¼ŒAx-Proverä¸æœ€å…ˆè¿›çš„è¯æ˜å™¨å…·æœ‰ç«äº‰åŠ›ï¼Œè€Œåœ¨æ–°åŸºå‡†æµ‹è¯•ä¸Šåˆ™å¤§å¤§ä¼˜äºå®ƒä»¬ã€‚è¿™è¡¨æ˜ï¼Œä¸é‚£äº›éš¾ä»¥æ¨å¹¿çš„ä¸“ç”¨ç³»ç»Ÿä¸åŒï¼Œæˆ‘ä»¬åŸºäºå·¥å…·çš„æ™ºèƒ½å®šç†è¯æ˜å™¨æ–¹æ³•ä¸ºè·¨ä¸åŒç§‘å­¦é¢†åŸŸçš„æ­£å¼éªŒè¯æä¾›äº†ä¸€ç§å¯æ¨å¹¿çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†Ax-Proveråœ¨å®é™…ç”¨ä¾‹ä¸­çš„åŠ©ç†èƒ½åŠ›ï¼Œå±•ç¤ºäº†å®ƒå¦‚ä½•å¸®åŠ©ä¸€ä½ä¸“å®¶æ•°å­¦å®¶å½¢å¼åŒ–ä¸€ä¸ªå¤æ‚çš„å¯†ç å­¦å®šç†çš„è¯æ˜ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12787v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>Ax-Proveræ˜¯ä¸€ä¸ªç”¨äºLeanè‡ªåŠ¨åŒ–å®šç†è¯æ˜çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå¯è§£å†³ä¸åŒç§‘å­¦é¢†åŸŸçš„å„ç§é—®é¢˜ï¼Œå¹¶å¯ä¸äººç±»ä¸“å®¶è‡ªä¸»æˆ–åä½œå·¥ä½œã€‚å®ƒé€šè¿‡å½¢å¼åŒ–è¯æ˜ç”Ÿæˆæ¥åº”å¯¹ç§‘å­¦é—®é¢˜çš„æ±‚è§£æŒ‘æˆ˜ï¼Œè¿™ä¸€æµç¨‹è¦æ±‚å…·æœ‰åˆ›é€ æ€§å’Œä¸¥æ ¼ç²¾ç¡®çš„å¥æ³•ä¸¥è°¨æ€§ã€‚Ax-Proveré…å¤‡äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå€ŸåŠ©æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰ä¸Leanå·¥å…·å®ç°äº†å½¢å¼åŒ–æ­£ç¡®æ€§çš„ä¿éšœã€‚å¯¹è‡ªä¸»è¯æ˜è€…æ€§èƒ½çš„è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨å…¬å¼€æ•°æ®é›†ä¸ŠAx-Proverä¸æœ€æ–°å‰æ²¿çš„è¯æ˜å™¨æ€§èƒ½ç›¸å½“ï¼Œä½†åœ¨æˆ‘ä»¬å¼•å…¥çš„æŠ½è±¡ä»£æ•°å’Œé‡å­ç†è®ºçš„æ–°åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼ŒAx-Proverè¿˜å±•ç¤ºäº†å…¶ä½œä¸ºåŠ©æ‰‹çš„å®ç”¨èƒ½åŠ›ï¼Œè¯æ˜äº†å®ƒå¦‚ä½•å¸®åŠ©æ•°å­¦å®¶å½¢å¼åŒ–å¤æ‚å¯†ç å®šç†çš„è¯æ˜ã€‚ç®€è€Œè¨€ä¹‹ï¼ŒAx-Proverä¸ä»…æä¾›äº†é€šç”¨çš„å®šç†è¯æ˜æ–¹æ³•ï¼Œä¹Ÿé€šè¿‡ä¸äººç±»åˆä½œå±•ç°å‡ºå¼ºå¤§çš„åŠ©åŠ›å’Œæ•ˆç‡æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Ax-Proveræ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºè‡ªåŠ¨åŒ–å®šç†è¯æ˜ï¼Œé€‚ç”¨äºå¤šä¸ªç§‘å­¦é¢†åŸŸã€‚</li>
<li>å®ƒç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’ŒLeanå·¥å…·ï¼Œé€šè¿‡å½¢å¼åŒ–è¯æ˜ç”Ÿæˆæ¥è§£å†³é—®é¢˜ã€‚</li>
<li>Ax-Proverèƒ½å¤Ÿè‡ªä¸»å·¥ä½œï¼Œä¹Ÿèƒ½ä¸äººç±»ä¸“å®¶åä½œã€‚</li>
<li>åœ¨å…¬å…±æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒAx-Proverè¡¨ç°è‰¯å¥½ï¼Œå¹¶ä¸”åœ¨ç‰¹å®šé¢†åŸŸçš„æ–°åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºå…¶ä»–è¯æ˜å™¨ã€‚</li>
<li>Ax-Proverå±•ç¤ºäº†å…¶åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ï¼Œå¦‚å¸®åŠ©æ•°å­¦å®¶å½¢å¼åŒ–å¤æ‚å¯†ç å®šç†çš„è¯æ˜ã€‚</li>
<li>Ax-Proveræä¾›äº†ä¸€ä¸ªé€šç”¨çš„æ–¹æ³•æ¥è§£å†³å½¢å¼åŒ–éªŒè¯é—®é¢˜ï¼Œè¿™å¯¹äºè·¨ä¸åŒç§‘å­¦é¢†åŸŸçš„æ¨å¹¿å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12787">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-b043282a506ba2f1f9e4eeea6b89c00c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810308&auth_key=1760810308-0-0-85a0884ed81f4b126b512a504d4549c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-a75a352cf9fe96c6c406ebbadd70b966~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810316&auth_key=1760810316-0-0-c63cd9323e50bb87f137da7dcd008293&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c0cb00bbe66731e6151e51a8209e975a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810322&auth_key=1760810322-0-0-fb881f1bffb108680cce2867e974fb00&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-55a8da84885322d6f29bf9d2f0f4e521~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810329&auth_key=1760810329-0-0-23e573f04279189d9d48f791f360d627&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Hierarchical-Alignment-Surgical-Fine-Tuning-via-Functional-Layer-Specialization-in-Large-Language-Models"><a href="#Hierarchical-Alignment-Surgical-Fine-Tuning-via-Functional-Layer-Specialization-in-Large-Language-Models" class="headerlink" title="Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer   Specialization in Large Language Models"></a>Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer   Specialization in Large Language Models</h2><p><strong>Authors:Yukun Zhang, Qi Dong</strong></p>
<p>Existing alignment techniques for Large Language Models (LLMs), such as Direct Preference Optimization (DPO), typically treat the model as a monolithic entity, applying uniform optimization pressure across all layers. This approach overlooks the functional specialization within the Transformer architecture, where different layers are known to handle distinct tasks from syntax to abstract reasoning. In this paper, we challenge this one-size-fits-all paradigm by introducing Hierarchical Alignment, a novel method that applies targeted DPO to distinct functional blocks of a modelâ€™s layers: local (syntax), intermediate (logic), and global (factuality). Through a series of controlled experiments on state-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for surgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge, demonstrate significant and predictable improvements. Specifically, aligning the local layers (Local-Align) enhances grammatical fluency. More importantly, aligning the global layers (Global-Align) not only improves factual consistency as hypothesized but also proves to be the most effective strategy for enhancing logical coherence, outperforming all baselines. Critically, all hierarchical strategies successfully avoid the â€œalignment taxâ€ observed in standard DPO, where gains in fluency come at the cost of degraded logical reasoning. These findings establish a more resource-efficient, controllable, and interpretable path for model alignment, highlighting the immense potential of shifting from monolithic optimization to structure-aware surgical fine-tuning to build more advanced and reliable LLMs. </p>
<blockquote>
<p>ç°æœ‰çš„é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¹é½æŠ€æœ¯ï¼Œå¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ï¼Œé€šå¸¸å°†æ¨¡å‹è§†ä¸ºä¸€ä¸ªå•ä¸€å®ä½“ï¼Œåœ¨æ‰€æœ‰å±‚ä¸Šåº”ç”¨ç»Ÿä¸€çš„ä¼˜åŒ–å‹åŠ›ã€‚è¿™ç§æ–¹æ³•å¿½ç•¥äº†Transformeræ¶æ„å†…çš„åŠŸèƒ½ä¸“ä¸šåŒ–ï¼Œä¸åŒå±‚è´Ÿè´£ä»è¯­æ³•åˆ°æŠ½è±¡æ¨ç†çš„ä¸åŒä»»åŠ¡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å¼•å…¥åˆ†å±‚å¯¹é½ï¼ŒæŒ‘æˆ˜äº†è¿™ç§ä¸€åˆ€åˆ‡çš„æ¨¡å¼ã€‚è¿™æ˜¯ä¸€ç§æ–°æ–¹æ³•ï¼Œå¯¹æ¨¡å‹çš„å±‚çš„ä¸åŒåŠŸèƒ½å—è¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„DPOï¼šæœ¬åœ°ï¼ˆè¯­æ³•ï¼‰ã€ä¸­é—´ï¼ˆé€»è¾‘ï¼‰å’Œå…¨å±€ï¼ˆçœŸå®æ€§ï¼‰ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä¸€ç³»åˆ—å…ˆè¿›æ¨¡å‹ä¸Šè¿›è¡Œæ§åˆ¶å®éªŒï¼Œå¦‚ä½¿ç”¨LoRAè¿›è¡Œç²¾ç»†è°ƒæ•´çš„Llama-3.1-8Bå’ŒQwen1.5-7Bï¼Œç”±å¼ºå¤§çš„LLM-as-Judgeè¿›è¡Œè¯„ä¼°ï¼Œç»“æœè¯æ˜äº†æ˜¾è‘—ä¸”å¯é¢„æµ‹çš„è¿›æ­¥ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹é½æœ¬åœ°å±‚ï¼ˆLocal-Alignï¼‰æé«˜äº†è¯­æ³•æµç•…æ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå¯¹é½å…¨å±€å±‚ï¼ˆGlobal-Alignï¼‰ä¸ä»…æ”¹å–„äº†äº‹å®ä¸€è‡´æ€§ï¼Œè€Œä¸”è¢«è¯æ˜æ˜¯æé«˜é€»è¾‘è¿è´¯æ€§çš„æœ€æœ‰æ•ˆç­–ç•¥ï¼Œè¶…è¿‡äº†æ‰€æœ‰åŸºçº¿ã€‚å…³é”®çš„æ˜¯ï¼Œæ‰€æœ‰åˆ†å±‚ç­–ç•¥éƒ½æˆåŠŸé¿å…äº†æ ‡å‡†DPOä¸­è§‚å¯Ÿåˆ°çš„â€œå¯¹é½ç¨â€ï¼Œå³åœ¨æµç•…æ€§æé«˜çš„åŒæ—¶ä»˜å‡ºäº†é€»è¾‘æ¨ç†èƒ½åŠ›ä¸‹é™çš„ä»£ä»·ã€‚è¿™äº›å‘ç°ä¸ºæ¨¡å‹å¯¹é½å»ºç«‹äº†ä¸€æ¡æ›´é«˜æ•ˆã€å¯æ§ã€å¯è§£é‡Šçš„é“è·¯ï¼Œçªå‡ºäº†ä»å•ä¸€ä¼˜åŒ–è½¬å‘ç»“æ„æ„ŸçŸ¥ç²¾ç»†è°ƒæ•´çš„å·¨å¤§æ½œåŠ›ï¼Œä»¥æ„å»ºæ›´å…ˆè¿›ã€æ›´å¯é çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12044v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆ†å±‚å¯¹é½æ–¹æ³•ï¼Œé€šè¿‡å®šå‘ä¼˜åŒ–ä¸åŒåŠŸèƒ½å—ï¼ˆå¦‚è¯­æ³•ã€é€»è¾‘å’Œäº‹å®æ€§ï¼‰æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ”¹å–„è¯­æ³•æµç•…æ€§ã€æé«˜äº‹å®ä¸€è‡´æ€§å’Œé€»è¾‘è¿è´¯æ€§æ–¹é¢æ•ˆæœæ˜¾è‘—ï¼ŒåŒæ—¶é¿å…äº†ä¼ ç»ŸDPOæ–¹æ³•çš„â€œå¯¹é½ç¨â€é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç°æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯¹é½æŠ€æœ¯å¦‚ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰é€šå¸¸å°†æ¨¡å‹è§†ä¸ºä¸€ä¸ªæ•´ä½“è¿›è¡Œä¼˜åŒ–ï¼Œå¿½ç•¥äº†Transformeræ¶æ„ä¸­çš„åŠŸèƒ½ä¸“ä¸šåŒ–ã€‚</li>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å±‚å¯¹é½æ–¹æ³•ï¼Œé’ˆå¯¹æ¨¡å‹çš„ä¸åŒå±‚ï¼ˆå±€éƒ¨ã€ä¸­é—´å’Œå…¨å±€ï¼‰è¿›è¡Œå®šå‘ä¼˜åŒ–ã€‚</li>
<li>åˆ†å±‚å¯¹é½æ–¹æ³•æ˜¾è‘—æé«˜äº†æ¨¡å‹çš„è¯­æ³•æµç•…æ€§ã€äº‹å®ä¸€è‡´æ€§å’Œé€»è¾‘è¿è´¯æ€§ã€‚</li>
<li>åˆ†å±‚å¯¹é½æ–¹æ³•é¿å…äº†ä¼ ç»ŸDPOæ–¹æ³•çš„â€œå¯¹é½ç¨â€é—®é¢˜ï¼Œå³æé«˜æµç•…åº¦æ—¶å¯èƒ½å¯¼è‡´çš„é€»è¾‘æ¨ç†èƒ½åŠ›ä¸‹é™ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ†å±‚å¯¹é½æ–¹æ³•åœ¨æ”¹è¿›æ¨¡å‹æ€§èƒ½æ–¹é¢æ•ˆæœæ˜¾è‘—ï¼Œå°¤å…¶æ˜¯åœ¨æé«˜å…¨å±€å±‚å¯¹é½æ–¹é¢ï¼Œè¿™ä¸ºæ›´å…ˆè¿›ã€æ›´å¯é çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„æ„å»ºæä¾›äº†æ›´å¯æ§å’Œå¯è§£é‡Šçš„è·¯å¾„ã€‚</li>
<li>é€šè¿‡ä½¿ç”¨LoRAè¿›è¡Œæœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒï¼Œå®éªŒéªŒè¯äº†åˆ†å±‚å¯¹é½æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12044">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d3367e80d1731b29110a0a626c46c6a5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810336&auth_key=1760810336-0-0-b13f1dbff38da679287e1258ea93416d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b940482261e59757d5372fcbc74e9586~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810344&auth_key=1760810344-0-0-efb5716be037073cbff73b3f493f6424&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f25f2ef3eac5f7fd7c0691a573881c26~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810352&auth_key=1760810352-0-0-11b51e26ecacf61474a9fa2cc0a7519d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Are-Large-Reasoning-Models-Interruptible"><a href="#Are-Large-Reasoning-Models-Interruptible" class="headerlink" title="Are Large Reasoning Models Interruptible?"></a>Are Large Reasoning Models Interruptible?</h2><p><strong>Authors:Tsung-Han Wu, Mihran Miroyan, David M. Chan, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez</strong></p>
<p>Large Reasoning Models (LRMs) excel at complex reasoning but are traditionally evaluated in static, â€œfrozen worldâ€ settings: model responses are assumed to be instantaneous, and the context of a request is presumed to be immutable over the duration of the response. While generally true for short-term tasks, the â€œfrozen worldâ€ assumption breaks down in modern reasoning tasks such as assistive programming, where models may take hours to think through problems and code may change dramatically from the time the model starts thinking to the modelâ€™s final output. In this work, we challenge the frozen world assumption and evaluate LRM robustness under two realistic dynamic scenarios: interruptions, which test the quality of the modelâ€™s partial outputs on a limited budget, and dynamic context, which tests model adaptation to in-flight changes. Across mathematics and programming benchmarks that require long-form reasoning, static evaluations consistently overestimate robustness: even state-of-the-art LRMs, which achieve high accuracy in static settings, can fail unpredictably when interrupted or exposed to changing context, with performance dropping by up to 60% when updates are introduced late in the reasoning process. Our analysis further reveals several novel failure modes, including reasoning leakage, where models fold the reasoning into their final answer when interrupted; panic, where under time pressure models abandon reasoning entirely and return incorrect answers; and self-doubt, where performance degrades while incorporating updated information. Project Page: <a target="_blank" rel="noopener" href="http://dynamic-lm.github.io/">http://dynamic-lm.github.io/</a> </p>
<blockquote>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¼ ç»Ÿä¸Šæ˜¯åœ¨é™æ€ã€â€œå†»ç»“ä¸–ç•Œâ€ç¯å¢ƒä¸­è¿›è¡Œè¯„ä¼°çš„ï¼šæ¨¡å‹å“åº”è¢«å‡å®šä¸ºç¬æ—¶çš„ï¼Œå¹¶ä¸”è¯·æ±‚çš„èƒŒæ™¯åœ¨å“åº”æœŸé—´å†…å‡å®šä¸ºä¸å¯å˜çš„ã€‚è™½ç„¶åœ¨çŸ­æœŸä»»åŠ¡ä¸­é€šå¸¸æ˜¯æ­£ç¡®çš„ï¼Œâ€œå†»ç»“ä¸–ç•Œâ€å‡è®¾åœ¨ç°ä»£æ¨ç†ä»»åŠ¡ä¸­å´©æºƒäº†ï¼Œä¾‹å¦‚è¾…åŠ©ç¼–ç¨‹ï¼Œå…¶ä¸­æ¨¡å‹å¯èƒ½éœ€è¦æ•°å°æ—¶çš„æ—¶é—´æ¥è€ƒè™‘é—®é¢˜ï¼Œå¹¶ä¸”ä»£ç å¯èƒ½ä¼šä»æ¨¡å‹å¼€å§‹æ€è€ƒåˆ°æ¨¡å‹çš„æœ€ç»ˆè¾“å‡ºä¹‹é—´å‘ç”Ÿå·¨å¤§çš„å˜åŒ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æŒ‘æˆ˜â€œå†»ç»“ä¸–ç•Œâ€å‡è®¾ï¼Œå¹¶åœ¨ä¸¤ä¸ªç°å®åŠ¨æ€åœºæ™¯ä¸‹è¯„ä¼°LRMçš„ç¨³å¥æ€§ï¼šä¸­æ–­ï¼Œæµ‹è¯•æ¨¡å‹åœ¨æœ‰é™é¢„ç®—ä¸‹éƒ¨åˆ†è¾“å‡ºçš„è´¨é‡ï¼›åŠ¨æ€ä¸Šä¸‹æ–‡ï¼Œæµ‹è¯•æ¨¡å‹å¯¹å®æ—¶å˜åŒ–çš„é€‚åº”èƒ½åŠ›ã€‚åœ¨æ•°å­¦å’Œç¼–ç¨‹åŸºå‡†æµ‹è¯•ä¸­ï¼Œéœ€è¦é•¿ç¯‡æ¨ç†çš„é™æ€è¯„ä¼°å¾€å¾€ä¼šé«˜ä¼°ç¨³å¥æ€§ï¼šå³ä½¿åœ¨é™æ€ç¯å¢ƒä¸­è¾¾åˆ°é«˜å‡†ç¡®æ€§çš„æœ€æ–°LRMï¼Œåœ¨ä¸­æ–­æˆ–æš´éœ²äºå˜åŒ–çš„ä¸Šä¸‹æ–‡æ—¶ä¹Ÿä¼šå‘ç”Ÿä¸å¯é¢„æµ‹çš„å¤±è´¥ï¼Œå½“åœ¨æ¨ç†è¿‡ç¨‹ä¸­åæœŸå¼•å…¥æ›´æ–°æ—¶ï¼Œæ€§èƒ½å¯èƒ½ä¼šä¸‹é™é«˜è¾¾60%ã€‚æˆ‘ä»¬çš„åˆ†æè¿˜æ­ç¤ºäº†å¤šç§æ–°å‹å¤±è´¥æ¨¡å¼ï¼ŒåŒ…æ‹¬æ¨ç†æ³„æ¼ï¼Œå…¶ä¸­æ¨¡å‹åœ¨ä¸­æ–­æ—¶å°†æ¨ç†æŠ˜å åˆ°å…¶æœ€ç»ˆç­”æ¡ˆä¸­ï¼›ææ…Œï¼Œåœ¨æ—¶é—´å‹åŠ›ä¸‹æ¨¡å‹å®Œå…¨æ”¾å¼ƒæ¨ç†å¹¶è¿”å›é”™è¯¯çš„ç­”æ¡ˆï¼›ä»¥åŠè‡ªæˆ‘æ€€ç–‘ï¼Œåœ¨èå…¥æ›´æ–°ä¿¡æ¯æ—¶æ€§èƒ½ä¸‹é™ã€‚é¡¹ç›®é¡µé¢ï¼š<a target="_blank" rel="noopener" href="http://dynamic-lm.github.io/">http://dynamic-lm.github.io/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11713v3">PDF</a> Project Page: <a target="_blank" rel="noopener" href="http://dynamic-lm.github.io/">http://dynamic-lm.github.io</a></p>
<p><strong>Summary</strong></p>
<p>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰æ“…é•¿å¤„ç†å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼Œä½†ä¼ ç»Ÿçš„è¯„ä¼°æ–¹å¼å¸¸å¸¸æ˜¯åœ¨é™æ€çš„â€œå†»ç»“ä¸–ç•Œâ€è®¾å®šä¸‹è¿›è¡Œã€‚æ¨¡å‹å“åº”è¢«å‡è®¾ä¸ºç¬æ—¶å®Œæˆï¼Œè¯·æ±‚çš„èƒŒæ™¯åœ¨å“åº”è¿‡ç¨‹ä¸­ä¿æŒä¸å˜ã€‚è™½ç„¶è¿™ä¸€å‡è®¾åœ¨çŸ­æœŸä»»åŠ¡ä¸­å¤§è‡´é€‚ç”¨ï¼Œä½†åœ¨ç°ä»£æ¨ç†ä»»åŠ¡å¦‚è¾…åŠ©ç¼–ç¨‹ä¸­ï¼Œæ¨¡å‹å¯èƒ½éœ€è¦æ•°å°æ—¶æ¥æ€è€ƒé—®é¢˜ï¼Œä»£ç ä»æ¨¡å‹å¼€å§‹æ€è€ƒåˆ°æœ€ç»ˆè¾“å‡ºçš„è¿‡ç¨‹ä¸­å¯èƒ½ä¼šå‘ç”Ÿæ˜¾è‘—å˜åŒ–ã€‚æœ¬æ–‡æŒ‘æˆ˜äº†â€œå†»ç»“ä¸–ç•Œâ€å‡è®¾ï¼Œå¹¶åœ¨ä¸¤ä¸ªç°å®åŠ¨æ€åœºæ™¯ä¸‹è¯„ä¼°äº†LRMçš„ç¨³å¥æ€§ï¼šä¸­æ–­å’ŒåŠ¨æ€ä¸Šä¸‹æ–‡ã€‚åœ¨æ•°å­¦å’Œç¼–ç¨‹åŸºå‡†æµ‹è¯•ä¸­çš„é•¿å½¢å¼æ¨ç†ä»»åŠ¡æ˜¾ç¤ºï¼Œé™æ€è¯„ä¼°å¾€å¾€ä¼šé«˜ä¼°æ¨¡å‹çš„ç¨³å¥æ€§ï¼šå³ä½¿åœ¨é™æ€ç¯å¢ƒä¸­è¡¨ç°é«˜è¶…çš„LRMsï¼Œåœ¨é¢å¯¹ä¸­æ–­æˆ–å˜åŒ–ä¸Šä¸‹æ–‡æ—¶ï¼Œæ€§èƒ½å¯èƒ½ä¼šæ„å¤–ä¸‹é™ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­åæœŸå¼•å…¥æ›´æ–°æ—¶ï¼Œæ€§èƒ½å¯èƒ½ä¼šä¸‹é™é«˜è¾¾60%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†ä¼ ç»Ÿè¯„ä¼°æ–¹æ³•ä¸»è¦åŸºäºé™æ€ç¯å¢ƒå‡è®¾ã€‚</li>
<li>â€œå†»ç»“ä¸–ç•Œâ€å‡è®¾åœ¨ç°ä»£æ¨ç†ä»»åŠ¡å¦‚è¾…åŠ©ç¼–ç¨‹ä¸­å¯èƒ½ä¸æˆç«‹ã€‚</li>
<li>æ¨¡å‹åœ¨æ€è€ƒè¿‡ç¨‹ä¸­å¯èƒ½éœ€è¦é•¿æ—¶é—´ï¼Œä¸”ä»£ç å¯èƒ½åœ¨æ¨¡å‹è¾“å‡ºå‰å‘ç”Ÿæ˜¾è‘—å˜åŒ–ã€‚</li>
<li>åœ¨ä¸­æ–­å’ŒåŠ¨æ€ä¸Šä¸‹æ–‡ä¸¤ä¸ªç°å®åœºæ™¯ä¸‹è¯„ä¼°äº†LRMçš„ç¨³å¥æ€§ã€‚</li>
<li>é™æ€è¯„ä¼°å¯èƒ½é«˜ä¼°æ¨¡å‹çš„ç¨³å¥æ€§ï¼ŒLRMsåœ¨é¢ä¸´ä¸­æ–­æˆ–å˜åŒ–çš„ä¸Šä¸‹æ–‡æ—¶æ€§èƒ½å¯èƒ½ä¼šå¤§å¹…ä¸‹é™ã€‚</li>
<li>æ¨¡å‹åœ¨é¢å¯¹ä¸­æ–­æ—¶å¯èƒ½å‡ºç°æ¨ç†æ³„éœ²ã€ææ…Œå’Œè‡ªæˆ‘æ€€ç–‘ç­‰æ–°å‹å¤±è´¥æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11713">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2e37bf66ab51811417924afd39711aa5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810360&auth_key=1760810360-0-0-0bb9b6b7863fdc771d688877f0a583fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ed2c6afc4cea1e449e0b41e353d952f3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810367&auth_key=1760810367-0-0-b203b15226c1e2075a4fb253d00718bd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-829f250dc5c28633d03e81ada396ec58~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810374&auth_key=1760810374-0-0-eac8f95f2abba40ecc4b3c19e8f037bc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Demystifying-Reinforcement-Learning-in-Agentic-Reasoning"><a href="#Demystifying-Reinforcement-Learning-in-Agentic-Reasoning" class="headerlink" title="Demystifying Reinforcement Learning in Agentic Reasoning"></a>Demystifying Reinforcement Learning in Agentic Reasoning</h2><p><strong>Authors:Zhaochen Yu, Ling Yang, Jiaru Zou, Shuicheng Yan, Mengdi Wang</strong></p>
<p>Recently, the emergence of agentic RL has showcased that RL could also effectively improve the agentic reasoning ability of LLMs, yet the key design principles and optimal practices remain unclear. In this work, we conduct a comprehensive and systematic investigation to demystify reinforcement learning in agentic reasoning from three key perspectives: data, algorithm, and reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic trajectories with real end-to-end tool-use trajectories yields a far stronger SFT initialization; high-diversity, model-aware datasets sustain exploration and markedly improve RL performance. (ii) Exploration-friendly techniques are crucial for agentic RL, such as clip higher, overlong reward shaping, and maintaining adequate policy entropy could improve the training efficiency. (iii) A deliberative strategy with fewer tool calls outperforms frequent tool calls or verbose self-reasoning, improving tool efficiency and final accuracy. Together, these simple practices consistently enhance agentic reasoning and training efficiency, achieving strong results on challenging benchmarks with smaller models, and establishing a practical baseline for future agentic RL research. Beyond these empirical insights, we further contribute a high-quality, real end-to-end agentic SFT dataset along with a high-quality RL dataset, and demonstrate the effectiveness of our insights in boosting the agentic reasoning ability of LLMs across four challenging benchmarks, including AIME2024&#x2F;AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes, 4B-sized models could also achieve superior agentic reasoning performance compared to 32B-sized models. Code and models: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/Open-AgentRL">https://github.com/Gen-Verse/Open-AgentRL</a> </p>
<blockquote>
<p>æœ€è¿‘ï¼Œä»£ç†å¼ºåŒ–å­¦ä¹ ï¼ˆagentic RLï¼‰çš„å‡ºç°å±•ç¤ºäº†ä¸€ä¸ªäº‹å®ï¼Œå³å¼ºåŒ–å­¦ä¹ å¯ä»¥æœ‰æ•ˆåœ°æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ä»£ç†æ¨ç†èƒ½åŠ›ï¼Œç„¶è€Œå…³é”®çš„è®¾è®¡åŸåˆ™å’Œæœ€ä½³å®è·µä»ç„¶ä¸æ˜ç¡®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»æ•°æ®ã€ç®—æ³•å’Œæ¨ç†æ¨¡å¼è¿™ä¸‰ä¸ªå…³é”®è§’åº¦å¯¹å¼ºåŒ–å­¦ä¹ åœ¨ä»£ç†æ¨ç†ä¸­çš„å¥¥ç§˜è¿›è¡Œäº†å…¨é¢ç³»ç»Ÿçš„ç ”ç©¶ã€‚æˆ‘ä»¬å¼ºè°ƒäº†æˆ‘ä»¬çš„å…³é”®è§è§£ï¼šé¦–å…ˆï¼Œï¼ˆiï¼‰ç”¨çœŸå®çš„ç«¯åˆ°ç«¯å·¥å…·ä½¿ç”¨è½¨è¿¹æ›¿æ¢æ‹¼æ¥çš„åˆæˆè½¨è¿¹ï¼Œä¼šäº§ç”Ÿæ›´å¼ºçš„SFTåˆå§‹åŒ–æ•ˆæœï¼›é«˜å¤šæ ·æ€§ã€æ¨¡å‹æ„ŸçŸ¥çš„æ•°æ®é›†ç»´æŒæ¢ç´¢å¹¶æ˜¾è‘—æé«˜äº†å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ã€‚ï¼ˆiiï¼‰æ¢ç´¢å‹å¥½çš„æŠ€æœ¯å¯¹äºä»£ç†å¼ºåŒ–å­¦ä¹ è‡³å…³é‡è¦ï¼Œå¦‚clip higherã€è¿‡åº¦å¥–åŠ±å¡‘é€ ä»¥åŠä¿æŒé€‚å½“çš„ç­–ç•¥ç†µå¯ä»¥æé«˜è®­ç»ƒæ•ˆç‡ã€‚ï¼ˆiiiï¼‰é‡‡ç”¨å·¥å…·è°ƒç”¨è¾ƒå°‘çš„å®¡æ…ç­–ç•¥ä¼˜äºé¢‘ç¹çš„å·¥å…·è°ƒç”¨æˆ–å†—é•¿çš„è‡ªæˆ‘æ¨ç†ï¼Œèƒ½å¤Ÿæé«˜å·¥å…·æ•ˆç‡å’Œæœ€ç»ˆå‡†ç¡®æ€§ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™äº›ç®€å•çš„å®è·µå§‹ç»ˆæé«˜äº†ä»£ç†æ¨ç†å’Œè®­ç»ƒæ•ˆç‡ï¼Œåœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœï¼Œå³ä½¿æ˜¯å°æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ï¼Œä¸ºæœªæ¥ä»£ç†å¼ºåŒ–å­¦ä¹ ç ”ç©¶å»ºç«‹äº†å®ç”¨çš„åŸºå‡†çº¿ã€‚é™¤äº†è¿™äº›ç»éªŒè§è§£ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›ä¸€æ­¥è´¡çŒ®äº†ä¸€ä¸ªé«˜è´¨é‡çš„ç«¯åˆ°ç«¯ä»£ç†SFTæ•°æ®é›†å’Œä¸€ä¸ªé«˜è´¨é‡çš„RLæ•°æ®é›†ï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„è§è§£åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç†æ¨ç†èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œè·¨è¶Šå››ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…æ‹¬AIME2024&#x2F;AIME2025ã€GPQA-Diamondå’ŒLiveCodeBench-v6ã€‚å€ŸåŠ©æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå³ä½¿è§„æ¨¡ä¸º4Bçš„æ¨¡å‹ä¹Ÿèƒ½åœ¨ä»£ç†æ¨ç†æ€§èƒ½ä¸Šè¶…è¶Šè§„æ¨¡ä¸º32Bçš„æ¨¡å‹ã€‚ä»£ç å’Œæ¨¡å‹å¯é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/Open-AgentRL%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/Gen-Verse/Open-AgentRLè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11701v1">PDF</a> Code and models: <a target="_blank" rel="noopener" href="https://github.com/Gen-Verse/Open-AgentRL">https://github.com/Gen-Verse/Open-AgentRL</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¼ºåŒ–å­¦ä¹ åœ¨æ™ºèƒ½ä½“æ¨ç†ä¸­çš„åº”ç”¨ï¼Œä»æ•°æ®ã€ç®—æ³•å’Œæ¨ç†æ¨¡å¼ä¸‰ä¸ªè§’åº¦è¿›è¡Œäº†å…¨é¢ç³»ç»Ÿçš„ç ”ç©¶ã€‚ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨çœŸå®ç«¯åˆ°ç«¯çš„å·¥å…·ä½¿ç”¨è½¨è¿¹æ›¿æ¢åˆæˆè½¨è¿¹ï¼Œèƒ½æé«˜æ¨¡å‹çš„åˆå§‹æ€§èƒ½ï¼›æ¢ç´¢å‹å¥½çš„æŠ€æœ¯å’Œè¾ƒå°‘çš„å·¥å…·è°ƒç”¨ç­–ç•¥èƒ½æé«˜è®­ç»ƒæ•ˆç‡å’Œæœ€ç»ˆç²¾åº¦ã€‚è¿™äº›å®è·µæ–¹æ³•èƒ½å¢å¼ºæ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›ï¼Œå¹¶åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸­å–å¾—è‰¯å¥½æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä½¿ç”¨çœŸå®ç«¯åˆ°ç«¯çš„å·¥å…·ä½¿ç”¨è½¨è¿¹æ›¿æ¢åˆæˆè½¨è¿¹ï¼Œèƒ½æœ‰æ•ˆæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>æ¢ç´¢å‹å¥½çš„æŠ€æœ¯ï¼Œå¦‚clip higherã€overlong reward shapingå’Œç»´æŒé€‚å½“çš„æ”¿ç­–ç†µï¼Œèƒ½æé«˜è®­ç»ƒæ•ˆç‡ã€‚</li>
<li>é‡‡ç”¨è¾ƒå°‘çš„å·¥å…·è°ƒç”¨ç­–ç•¥ï¼Œèƒ½æé«˜å·¥å…·æ•ˆç‡å’Œæœ€ç»ˆç²¾åº¦ã€‚</li>
<li>æœ¬æ–‡æä¾›äº†ä¸€ä»½é«˜è´¨é‡çš„ã€çœŸå®çš„ç«¯åˆ°ç«¯æ™ºèƒ½ä½“SFTæ•°æ®é›†å’Œä¸€ä»½é«˜è´¨é‡çš„RLæ•°æ®é›†ã€‚</li>
<li>è¿™äº›å®è·µæ–¹æ³•èƒ½åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åŸºå‡†æµ‹è¯•ä¸Šæé«˜æ™ºèƒ½ä½“æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬AIME2024&#x2F;AIME2025ã€GPQA-Diamondå’ŒLiveCodeBench-v6ç­‰ã€‚</li>
<li>å³ä½¿æ˜¯è¾ƒå°çš„æ¨¡å‹ï¼Œä¹Ÿèƒ½é€šè¿‡éµå¾ªè¿™äº›å®è·µæ–¹æ³•å®ç°å‡ºè‰²çš„æ™ºèƒ½ä½“æ¨ç†æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11701">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-96e73c924903ee855bb15c5c7c3eb65c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810382&auth_key=1760810382-0-0-265c533835ed363d021784ff42abcd1c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3e46d449cec0d521aee331e5e1210653~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810389&auth_key=1760810389-0-0-9045465bf2d4546a5b477205674914c3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-30193e57f28f3a94dae9a8fde155a1e3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810396&auth_key=1760810396-0-0-fb9bab532e21eb42d9de3f14eff9b27d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-497079fd4f5be22f60263d98df2cc4a8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810403&auth_key=1760810403-0-0-e06fe7e7497020aff0551dcc0f7462f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Enhancing-Long-Chain-of-Thought-Reasoning-through-Multi-Path-Plan-Aggregation"><a href="#Enhancing-Long-Chain-of-Thought-Reasoning-through-Multi-Path-Plan-Aggregation" class="headerlink" title="Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan   Aggregation"></a>Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan   Aggregation</h2><p><strong>Authors:Siheng Xiong, Ali Payani, Faramarz Fekri</strong></p>
<p>Inference-time scaling enhances the reasoning ability of a language model (LM) by extending its chain-of-thought (CoT). However, existing approaches typically generate the entire reasoning chain in a single forward pass, which often leads to CoT derailment, i.e., the reasoning trajectory drifting off course due to compounding errors. This problem is particularly severe for smaller LMs with long CoTs due to their limited capacity. To address this, we analyze raw long CoTs and uncover a reasoning hierarchy consisting of planning and execution steps. Our analysis reveals that most reasoning errors stem from incorrect planning. Motivated by this observation, we propose Multi-Path Plan Aggregation (MPPA), a framework that augments single-pass reasoning with plan exploration and aggregation. Following a variable interval schedule based on the token position, MPPA generates multiple candidate plans and aggregates them into a refined planning step. To maintain efficiency, we adopt a minimal design in which the base LM serves as the primary policy, while a lightweight LoRA module implements the plan aggregation policy. We further observe that outcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K tokens). To overcome this, we introduce online Step-DPO, a process-level preference optimization scheme that leverages Twisted Sequential Monte Carlo (TSMC) to provide scalable stepwise supervision using small LMs. This yields more efficient training, improved stability, and higher accuracy. Extensive experiments on challenging math, science, and logical reasoning benchmarks demonstrate that, with only 10% SFT data and 5% of preference pairs, our method outperforms both the DeepSeek-R1 distillation baseline and the outcome-reward RL baseline across multiple base models and tasks. </p>
<blockquote>
<p>æ¨ç†æ—¶é—´ç¼©æ”¾é€šè¿‡æ‰©å±•è¯­è¨€æ¨¡å‹çš„æ€ç»´é“¾ï¼ˆCoTï¼‰æå‡å…¶æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸åœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­ç”Ÿæˆæ•´ä¸ªæ¨ç†é“¾ï¼Œè¿™å¸¸å¸¸å¯¼è‡´æ€ç»´é“¾è„±è½¨ï¼Œå³ç”±äºç´¯ç§¯é”™è¯¯å¯¼è‡´æ¨ç†è½¨è¿¹åç¦»æ­£ç¡®æ–¹å‘ã€‚å¯¹äºå…·æœ‰è¾ƒé•¿æ€ç»´é“¾çš„å°å‹è¯­è¨€æ¨¡å‹ï¼Œè¿™ä¸ªé—®é¢˜å°¤ä¸ºä¸¥é‡ï¼Œå› ä¸ºå®ƒä»¬çš„èƒ½åŠ›æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ†æåŸå§‹çš„é•¿æ€ç»´é“¾å¹¶æ­ç¤ºäº†ä¸€ä¸ªç”±è§„åˆ’å’Œæ‰§è¡Œæ­¥éª¤ç»„æˆçš„æ¨ç†å±‚æ¬¡ç»“æ„ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå¤§å¤šæ•°æ¨ç†é”™è¯¯æºäºè§„åˆ’ä¸æ­£ç¡®ã€‚å—æ­¤è§‚å¯Ÿçš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šè·¯å¾„è®¡åˆ’èšåˆï¼ˆMPPAï¼‰æ¡†æ¶ï¼Œå®ƒé€šè¿‡è®¡åˆ’æ¢ç´¢å’Œèšåˆæ¥å¢å¼ºå•è·¯å¾„æ¨ç†ã€‚éµå¾ªåŸºäºä»¤ç‰Œä½ç½®çš„å¯å˜é—´éš”è°ƒåº¦ï¼ŒMPPAç”Ÿæˆå¤šä¸ªå€™é€‰è®¡åˆ’å¹¶å°†å…¶èšåˆåˆ°ç²¾ç»†çš„è§„åˆ’æ­¥éª¤ä¸­ã€‚ä¸ºäº†ä¿æŒæ•ˆç‡ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æœ€å°è®¾è®¡ï¼Œå…¶ä¸­åŸºç¡€è¯­è¨€æ¨¡å‹å……å½“ä¸»è¦ç­–ç•¥ï¼Œè€Œè½»é‡çº§çš„LoRAæ¨¡å—å®ç°è®¡åˆ’èšåˆç­–ç•¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°ï¼Œå¯¹äºè¶…è¿‡4Kä»¤ç‰Œçš„é•¿è½¨è¿¹ï¼Œç»“æœå¥–åŠ±å¼ºåŒ–å­¦ä¹ æ˜¯ä½æ•ˆçš„ã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†åœ¨çº¿Step-DPOï¼Œè¿™æ˜¯ä¸€ç§è¿‡ç¨‹çº§åå¥½ä¼˜åŒ–æ–¹æ¡ˆï¼Œå®ƒåˆ©ç”¨æ‰­æ›²åºè´¯è’™ç‰¹å¡æ´›ï¼ˆTSMCï¼‰æä¾›å¯ä¼¸ç¼©çš„é€æ­¥ç›‘ç£ï¼Œå¹¶ä½¿ç”¨å°å‹è¯­è¨€æ¨¡å‹ã€‚è¿™å¸¦æ¥äº†æ›´æœ‰æ•ˆçš„è®­ç»ƒã€æ›´é«˜çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ã€‚åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦ã€ç§‘å­¦å’Œé€»è¾‘æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œä»…ä½¿ç”¨10%çš„SFTæ•°æ®å’Œ5%çš„åå¥½å¯¹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªåŸºç¡€æ¨¡å‹å’Œä»»åŠ¡ä¸Šè¶…è¶Šäº†DeepSeek-R1è’¸é¦åŸºå‡†æµ‹è¯•å’Œç»“æœå¥–åŠ±å¼ºåŒ–å­¦ä¹ åŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11620v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMulti-Path Plan Aggregationï¼ˆMPPAï¼‰çš„æ¡†æ¶ï¼Œç”¨äºå¢å¼ºè¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡è®¡åˆ’æ¢ç´¢å’Œèšåˆæ¥æ”¹è¿›ä¼ ç»Ÿçš„å•ä¸€æ¨ç†è¿‡ç¨‹ï¼Œè§£å†³äº†é•¿æ¨ç†é“¾ä¸­å¸¸è§çš„è½¨è¿¹åç¦»é—®é¢˜ã€‚é€šè¿‡å¼•å…¥åœ¨çº¿Step-DPOå’ŒTwisted Sequential Monte Carloï¼ˆTSMCï¼‰ï¼Œå®ç°äº†é«˜æ•ˆã€ç¨³å®šçš„è®­ç»ƒï¼Œæé«˜äº†æ¨ç†å‡†ç¡®æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªåŸºå‡†æ¨¡å‹å’Œä»»åŠ¡ä¸Šå‡ä¼˜äºDeepSeek-R1è’¸é¦åŸºçº¿å’Œæ–¹æ³•ç»“æœå¼ºåŒ–å­¦ä¹ åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>æ¨ç†æ—¶é—´ç¼©æ”¾å¯é€šè¿‡æ‰©å±•è¯­è¨€æ¨¡å‹çš„æ€ç»´é“¾å¢å¼ºæ¨ç†èƒ½åŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•é€šå¸¸åœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­ç”Ÿæˆæ•´ä¸ªæ¨ç†é“¾ï¼Œå¯¼è‡´æ¨ç†è½¨è¿¹åç¦»ã€‚</li>
<li>æå‡ºäº†Multi-Path Plan Aggregationï¼ˆMPPAï¼‰æ¡†æ¶ï¼Œé€šè¿‡è®¡åˆ’æ¢ç´¢å’Œèšåˆæ”¹è¿›äº†ä¼ ç»Ÿçš„å•ä¸€æ¨ç†è¿‡ç¨‹ã€‚</li>
<li>MPPAæ¡†æ¶é‡‡ç”¨åŸºäºä»¤ç‰Œä½ç½®çš„å˜é‡é—´éš”è°ƒåº¦ç”Ÿæˆå¤šä¸ªå€™é€‰è®¡åˆ’ï¼Œå¹¶èšé›†å®ƒä»¬ä»¥å½¢æˆç²¾ç‚¼çš„è§„åˆ’æ­¥éª¤ã€‚</li>
<li>å¼•å…¥åœ¨çº¿Step-DPOå’ŒTwisted Sequential Monte Carloï¼ˆTSMCï¼‰æ¥å®ç°é«˜æ•ˆã€ç¨³å®šçš„è®­ç»ƒå’Œæé«˜æ¨ç†å‡†ç¡®æ€§ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11620">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a67bbd7b6fa932a2fa5e550137d2933a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810410&auth_key=1760810410-0-0-b578d88a4d8a46c82cd688c74962f8f2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5188e93d18eb007d3a0d6fa32e5142f9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810418&auth_key=1760810418-0-0-cf4dfedb984423e4677bb6604d232903&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e28ce9f76f0e382b65f56f024d523866~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810424&auth_key=1760810424-0-0-b0ae83dace8b16791ed46c77a1768547&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-9c3b2cfbe6ff6959689e80c74713b4c0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810431&auth_key=1760810431-0-0-5991dde7cacc1647c2a7ab396b7e8197&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-958e6854c8bf4ee580cda4b20d8661ca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810440&auth_key=1760810440-0-0-31301d169fa0b3b158416c9bc29539e0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6ac0ce9b5f81a740819ad9c2b43fd5b9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810447&auth_key=1760810447-0-0-56c4f07854491dd0247bdf5ec4ddf582&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Vision-LLMs-for-Spatiotemporal-Traffic-Forecasting"><a href="#Vision-LLMs-for-Spatiotemporal-Traffic-Forecasting" class="headerlink" title="Vision-LLMs for Spatiotemporal Traffic Forecasting"></a>Vision-LLMs for Spatiotemporal Traffic Forecasting</h2><p><strong>Authors:Ning Yang, Hengyu Zhong, Haijun Zhang, Randall Berry</strong></p>
<p>Accurate spatiotemporal traffic forecasting is a critical prerequisite for proactive resource management in dense urban mobile networks. While Large Language Models (LLMs) have shown promise in time series analysis, they inherently struggle to model the complex spatial dependencies of grid-based traffic data. Effectively extending LLMs to this domain is challenging, as representing the vast amount of information from dense geographical grids can be inefficient and overwhelm the modelâ€™s context. To address these challenges, we propose ST-Vision-LLM, a novel framework that reframes spatiotemporal forecasting as a vision-language fusion problem. Our approach leverages a Vision-LLM visual encoder to process historical global traffic matrices as image sequences, providing the model with a comprehensive global view to inform cell-level predictions. To overcome the inefficiency of LLMs in handling numerical data, we introduce an efficient encoding scheme that represents floating-point values as single tokens via a specialized vocabulary, coupled with a two-stage numerical alignment fine-tuning process. The model is first trained with Supervised Fine-Tuning (SFT) and then further optimized for predictive accuracy using Group Relative Policy Optimization (GRPO), a memory-efficient reinforcement learning method. Evaluations on real-world mobile traffic datasets demonstrate that ST-Vision-LLM outperforms existing methods by 15.6% in long-term prediction accuracy and exceeds the second-best baseline by over 30.04% in cross-domain few-shot scenarios. Our extensive experiments validate the modelâ€™s strong generalization capabilities across various data-scarce environments. </p>
<blockquote>
<p>ç²¾ç¡®çš„æ—¶ç©ºäº¤é€šé¢„æµ‹æ˜¯å¯†é›†åŸå¸‚ç§»åŠ¨ç½‘ç»œä¸­è¿›è¡Œä¸»åŠ¨èµ„æºç®¡ç†çš„å…³é”®å‰æã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ—¶é—´åºåˆ—åˆ†ææ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬æœ¬è´¨ä¸Šå¾ˆéš¾å¯¹åŸºäºç½‘æ ¼çš„äº¤é€šæ•°æ®çš„å¤æ‚ç©ºé—´ä¾èµ–æ€§è¿›è¡Œå»ºæ¨¡ã€‚å°†LLMæœ‰æ•ˆæ‰©å±•åˆ°è¿™ä¸€é¢†åŸŸå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºè¡¨ç¤ºæ¥è‡ªå¯†é›†åœ°ç†ç½‘æ ¼çš„å¤§é‡ä¿¡æ¯å¯èƒ½æ•ˆç‡ä½ä¸‹ï¼Œä¼šä½¿æ¨¡å‹çš„ä¸Šä¸‹æ–‡æ„Ÿåˆ°å›°æƒ‘ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ST-Vision-LLMè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒå°†æ—¶ç©ºé¢„æµ‹é‡æ–°æ„æƒ³ä¸ºä¸€ä¸ªè§†è§‰è¯­è¨€èåˆé—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨Vision-LLMè§†è§‰ç¼–ç å™¨æ¥å¤„ç†å†å²å…¨çƒäº¤é€šçŸ©é˜µä½œä¸ºå›¾åƒåºåˆ—ï¼Œä¸ºæ¨¡å‹æä¾›å…¨é¢çš„å…¨å±€è§†å›¾ï¼Œä»¥æ”¯æŒè¿›è¡Œå•å…ƒçº§åˆ«çš„é¢„æµ‹ã€‚ä¸ºäº†è§£å†³LLMåœ¨å¤„ç†æ•°å€¼æ•°æ®æ–¹é¢çš„æ•ˆç‡ä½ä¸‹é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æœ‰æ•ˆçš„ç¼–ç æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆé€šè¿‡ä¸“ç”¨è¯æ±‡å°†æµ®ç‚¹æ•°å€¼è¡¨ç¤ºä¸ºå•ä¸ªä»¤ç‰Œï¼Œå†åŠ ä¸Šä¸€ä¸ªä¸¤é˜¶æ®µçš„æ•°å€¼å¯¹é½å¾®è°ƒè¿‡ç¨‹ã€‚æ¨¡å‹é¦–å…ˆé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œè®­ç»ƒï¼Œç„¶åé‡‡ç”¨å†…å­˜é«˜æ•ˆå¼ºåŒ–å­¦ä¹ æ–¹æ³•â€”â€”é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œè¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œä»¥æé«˜é¢„æµ‹ç²¾åº¦ã€‚åœ¨ç°å®ä¸–ç•Œç§»åŠ¨äº¤é€šæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒST-Vision-LLMçš„é•¿æœŸé¢„æµ‹ç²¾åº¦æ¯”ç°æœ‰æ–¹æ³•é«˜å‡º15.6%ï¼Œåœ¨è·¨åŸŸå°æ ·æœ¬åœºæ™¯ä¸­è¶…è¿‡ç¬¬äºŒååŸºçº¿30.04%ä»¥ä¸Šã€‚æˆ‘ä»¬çš„å¹¿æ³›å®éªŒéªŒè¯äº†è¯¥æ¨¡å‹åœ¨å„ç§æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸­çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11282v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç²¾å‡†çš„æ—¶ç©ºäº¤é€šé¢„æµ‹åœ¨å¯†é›†åŸå¸‚ç§»åŠ¨ç½‘ç»œä¸­è¿›è¡Œèµ„æºä¸»åŠ¨ç®¡ç†çš„å¿…è¦æ€§ï¼Œæœ¬ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å»ºæ¨¡ç½‘æ ¼äº¤é€šæ•°æ®å¤æ‚ç©ºé—´ä¾èµ–å…³ç³»æ–¹é¢çš„å›ºæœ‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ST-Vision-LLMè¿™ä¸€æ–°å‹æ¡†æ¶ã€‚è¯¥ç ”ç©¶å°†æ—¶ç©ºé¢„æµ‹é‡æ–°å®šä½ä¸ºè§†è§‰è¯­è¨€èåˆé—®é¢˜ï¼Œå¹¶åˆ©ç”¨Vision-LLMè§†è§‰ç¼–ç å™¨å¤„ç†å†å²å…¨çƒäº¤é€šçŸ©é˜µä½œä¸ºå›¾åƒåºåˆ—ï¼Œä¸ºæ¨¡å‹æä¾›å…¨çƒè§†è§’ä»¥æ”¯æŒå•å…ƒæ ¼çº§åˆ«çš„é¢„æµ‹ã€‚ä¸ºè§£å†³LLMåœ¨å¤„ç†æ•°å€¼æ•°æ®æ—¶çš„ä½æ•ˆé—®é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†é«˜æ•ˆçš„ç¼–ç æ–¹æ¡ˆï¼Œé€šè¿‡ä¸“é—¨çš„è¯æ±‡è¡¨å°†æµ®ç‚¹æ•°å€¼è¡¨ç¤ºä¸ºå•ä¸ªä»¤ç‰Œï¼Œå¹¶ç»“åˆä¸¤é˜¶æ®µæ•°å€¼å¯¹é½å¾®è°ƒè¿‡ç¨‹ã€‚æ¨¡å‹é¦–å…ˆé€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è¿›è¡Œè®­ç»ƒï¼Œç„¶åé‡‡ç”¨å†…å­˜é«˜æ•ˆå¼ºåŒ–å­¦ä¹ æ–¹æ³•â€”â€”é›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›ä¸€æ­¥ä¼˜åŒ–é¢„æµ‹ç²¾åº¦ã€‚åœ¨çœŸå®ç§»åŠ¨äº¤é€šæ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼ŒST-Vision-LLMçš„é•¿æœŸé¢„æµ‹ç²¾åº¦é«˜å‡ºç°æœ‰æ–¹æ³•15.6%ï¼Œåœ¨è·¨åŸŸå°‘æ ·æœ¬åœºæ™¯ä¸­è¶…å‡ºç¬¬äºŒååŸºçº¿30.04%ä»¥ä¸Šã€‚å¹¿æ³›å®éªŒéªŒè¯äº†è¯¥æ¨¡å‹åœ¨ä¸åŒæ•°æ®ç¨€ç¼ºç¯å¢ƒä¸‹çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯†é›†åŸå¸‚ç§»åŠ¨ç½‘ç»œçš„æ—¶ç©ºäº¤é€šé¢„æµ‹å¯¹äºèµ„æºä¸»åŠ¨ç®¡ç†è‡³å…³é‡è¦ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å»ºæ¨¡ç½‘æ ¼äº¤é€šæ•°æ®çš„å¤æ‚ç©ºé—´ä¾èµ–å…³ç³»æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>ST-Vision-LLMæ¡†æ¶å°†æ—¶ç©ºé¢„æµ‹é‡æ–°å®šä½ä¸ºè§†è§‰è¯­è¨€èåˆé—®é¢˜ã€‚</li>
<li>åˆ©ç”¨Vision-LLMè§†è§‰ç¼–ç å™¨å¤„ç†å†å²å…¨çƒäº¤é€šçŸ©é˜µä½œä¸ºå›¾åƒåºåˆ—ã€‚</li>
<li>å¼•å…¥é«˜æ•ˆç¼–ç æ–¹æ¡ˆï¼Œå°†æµ®ç‚¹æ•°è¡¨ç¤ºä¸ºå•ä¸ªä»¤ç‰Œï¼Œè§£å†³LLMå¤„ç†æ•°å€¼æ•°æ®çš„ä½æ•ˆé—®é¢˜ã€‚</li>
<li>æ¨¡å‹é€šè¿‡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œé›†å›¢ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œè®­ç»ƒå’Œä¼˜åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1911d8f8fdd5cd290c346f77c06e04d4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810455&auth_key=1760810455-0-0-b3589da6c2eddc332c266d146cba750b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b4db7ec236541c959f93c11c68706f73~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810462&auth_key=1760810462-0-0-5713cae8faec906f993ed56e7ab8ad38&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Towards-Real-Time-Fake-News-Detection-under-Evidence-Scarcity"><a href="#Towards-Real-Time-Fake-News-Detection-under-Evidence-Scarcity" class="headerlink" title="Towards Real-Time Fake News Detection under Evidence Scarcity"></a>Towards Real-Time Fake News Detection under Evidence Scarcity</h2><p><strong>Authors:Guangyu Wei, Ke Han, Yueming Lyu, Yu Luo, Yue Jiang, Caifeng Shan, Nicu Sebe</strong></p>
<p>Fake news detection becomes particularly challenging in real-time scenarios, where emerging events often lack sufficient supporting evidence. Existing approaches often rely heavily on external evidence and therefore struggle to generalize under evidence scarcity. To address this issue, we propose Evaluation-Aware Selection of Experts (EASE), a novel framework for real-time fake news detection that dynamically adapts its decision-making process according to the assessed sufficiency of available evidence. EASE introduces a sequential evaluation mechanism comprising three independent perspectives: (1) Evidence-based evaluation, which assesses evidence and incorporates it into decision-making only when the evidence is sufficiently supportive; (2) Reasoning-based evaluation, which leverages the world knowledge of large language models (LLMs) and applies them only when their reliability is adequately established; and (3) Sentiment-based fallback, which integrates sentiment cues when neither evidence nor reasoning is reliable. To enhance the accuracy of evaluation processes, EASE employs instruction tuning with pseudo labels to guide each evaluator in justifying its perspective-specific knowledge through interpretable reasoning. Furthermore, the expert modules integrate the evaluatorsâ€™ justified assessments with the news content to enable evaluation-aware decision-making, thereby enhancing overall detection accuracy. Moreover, we introduce RealTimeNews-25, a new benchmark comprising recent news for evaluating model generalization on emerging news with limited evidence. Extensive experiments demonstrate that EASE not only achieves state-of-the-art performance across multiple benchmarks, but also significantly improves generalization to real-time news. The code and dataset are available: <a target="_blank" rel="noopener" href="https://github.com/wgyhhhh/EASE">https://github.com/wgyhhhh/EASE</a>. </p>
<blockquote>
<p>å®æ—¶åœºæ™¯ä¸­çš„å‡æ–°é—»æ£€æµ‹å˜å¾—å°¤ä¸ºå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ–°å…´äº‹ä»¶å¾€å¾€ç¼ºä¹è¶³å¤Ÿçš„æ”¯æŒè¯æ®ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä¸¥é‡ä¾èµ–äºå¤–éƒ¨è¯æ®ï¼Œå› æ­¤åœ¨è¯æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹å¾ˆéš¾æ¨å¹¿ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œåŸºäºè¯„ä¼°çš„ä¸“å®¶é€‰æ‹©â€ï¼ˆEASEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå®æ—¶å‡æ–°é—»æ£€æµ‹çš„æ–°å‹æ¡†æ¶ï¼Œå®ƒå¯ä»¥æ ¹æ®ç°æœ‰è¯æ®çš„å……è¶³æ€§æ¥åŠ¨æ€è°ƒæ•´å…¶å†³ç­–è¿‡ç¨‹ã€‚EASEå¼•å…¥äº†ä¸€ç§é¡ºåºè¯„ä¼°æœºåˆ¶ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªç‹¬ç«‹çš„è§’åº¦ï¼šï¼ˆ1ï¼‰åŸºäºè¯æ®çš„è¯„ä»·ï¼Œå®ƒåªåœ¨è¯æ®å……è¶³æ”¯æŒçš„æƒ…å†µä¸‹è¯„ä¼°è¯æ®å¹¶å°†å…¶çº³å…¥å†³ç­–ï¼›ï¼ˆ2ï¼‰åŸºäºæ¨ç†çš„è¯„ä»·ï¼Œå®ƒåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸–ç•ŒçŸ¥è¯†ï¼Œåªåœ¨å¯é æ€§å¾—åˆ°å……åˆ†è¯æ˜çš„æƒ…å†µä¸‹åº”ç”¨ï¼›ï¼ˆ3ï¼‰åŸºäºæƒ…æ„Ÿçš„åå¤‡ï¼Œå½“è¯æ®å’Œæ¨ç†éƒ½ä¸å¯é æ—¶ï¼Œå®ƒæ•´åˆæƒ…æ„Ÿçº¿ç´¢ã€‚ä¸ºäº†æé«˜è¯„ä¼°è¿‡ç¨‹çš„å‡†ç¡®æ€§ï¼ŒEASEé‡‡ç”¨æŒ‡ä»¤å¾®è°ƒä¸ä¼ªæ ‡ç­¾ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œå¼•å¯¼æ¯ä¸ªè¯„ä¼°è€…é€šè¿‡å¯è§£é‡Šçš„ç†ç”±æ¥è¯æ˜å…¶ç‰¹å®šè§†è§’çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œä¸“å®¶æ¨¡å—å°†è¯„ä¼°è€…çš„åˆç†è¯„ä¼°ä¸æ–°é—»å†…å®¹ç›¸ç»“åˆï¼Œä»¥å®ç°åŸºäºè¯„ä¼°çš„å†³ç­–åˆ¶å®šï¼Œä»è€Œæé«˜æ€»ä½“æ£€æµ‹å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†RealTimeNews-25ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«æœ€è¿‘çš„æ–°é—»ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨è¯æ®æœ‰é™çš„æœ€æ–°æ–°é—»ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒEASEä¸ä»…åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½ï¼Œè€Œä¸”åœ¨å®æ—¶æ–°é—»ä¸­çš„æ³›åŒ–èƒ½åŠ›ä¹Ÿå¾—åˆ°äº†æ˜¾ç€æé«˜ã€‚ç›¸å…³ä»£ç å’Œæ•°æ®é›†å¯é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/wgyhhhh/EASE">https://github.com/wgyhhhh/EASE</a> ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11277v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>å®æ—¶å‡æ–°é—»æ£€æµ‹é¢ä¸´è¯æ®ä¸è¶³çš„æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•ä¾èµ–å¤–éƒ¨è¯æ®ï¼Œéš¾ä»¥åœ¨è¯æ®ä¸è¶³çš„æƒ…å†µä¸‹è¿›è¡Œæ³›åŒ–ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œåŸºäºè¯„ä¼°çš„ä¸“å®¶é€‰æ‹©â€ï¼ˆEASEï¼‰çš„æ–°æ¡†æ¶ï¼Œå®ƒèƒ½æ ¹æ®ç°æœ‰è¯æ®çš„å……è¶³æ€§åŠ¨æ€è°ƒæ•´å†³ç­–è¿‡ç¨‹ã€‚EASEå¼•å…¥äº†ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªç‹¬ç«‹è§’åº¦çš„åºåˆ—è¯„ä¼°æœºåˆ¶ï¼šï¼ˆ1ï¼‰åŸºäºè¯æ®çš„è¯„ä»·åªåœ¨è¯æ®å……è¶³æ—¶è¿›è¡Œè¯„ä¼°å’Œå†³ç­–ï¼›ï¼ˆ2ï¼‰åŸºäºæ¨ç†çš„è¯„ä»·åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„å…¨çƒçŸ¥è¯†ï¼Œåªåœ¨å¯é æ€§å¾—åˆ°è¯å®æ—¶åº”ç”¨ï¼›ï¼ˆ3ï¼‰å½“è¯æ®å’Œæ¨ç†éƒ½ä¸å¯é æ—¶ï¼ŒåŸºäºæƒ…æ„Ÿçš„å¤‡é€‰æ–¹æ¡ˆä¼šæ•´åˆæƒ…æ„Ÿçº¿ç´¢ã€‚ä¸ºæé«˜è¯„ä¼°è¿‡ç¨‹çš„å‡†ç¡®æ€§ï¼ŒEASEé‡‡ç”¨æŒ‡ä»¤å¾®è°ƒä¸ä¼ªæ ‡ç­¾æ¥å¼•å¯¼æ¯ä¸ªè¯„ä¼°è€…é€šè¿‡å¯è§£é‡Šçš„ç†ç”±æ¥è¯æ˜å…¶ç‰¹å®šçŸ¥è¯†çš„åˆç†æ€§ã€‚æ­¤å¤–ï¼Œä¸“å®¶æ¨¡å—æ•´åˆè¯„ä¼°è€…çš„åˆç†è¯„ä¼°ä¸æ–°é—»å†…å®¹ï¼Œå®ç°è¯„ä¼°æ„ŸçŸ¥çš„å†³ç­–åˆ¶å®šï¼Œä»è€Œæé«˜æ•´ä½“æ£€æµ‹å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†RealTimeNews-25åŸºå‡†æµ‹è¯•ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨è¯æ®æœ‰é™çš„å®æ—¶æ–°é—»ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒEASEä¸ä»…åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€ä½³æ€§èƒ½ï¼Œè€Œä¸”åœ¨å®æ—¶æ–°é—»ä¸­æ˜¾è‘—æé«˜æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å®æ—¶å‡æ–°é—»æ£€æµ‹é¢ä¸´è¯æ®ä¸è¶³çš„æŒ‘æˆ˜ã€‚</li>
<li>EASEæ¡†æ¶èƒ½åŠ¨æ€é€‚åº”è¯æ®å……è¶³æ€§è¿›è¡Œå†³ç­–ã€‚</li>
<li>EASEé‡‡ç”¨åŸºäºä¸‰ä¸ªç‹¬ç«‹è§’åº¦çš„åºåˆ—è¯„ä¼°æœºåˆ¶ï¼šåŸºäºè¯æ®ã€åŸºäºæ¨ç†ã€åŸºäºæƒ…æ„Ÿçš„å¤‡é€‰æ–¹æ¡ˆã€‚</li>
<li>EASEé€šè¿‡æŒ‡ä»¤å¾®è°ƒä¸ä¼ªæ ‡ç­¾æé«˜è¯„ä¼°è¿‡ç¨‹çš„å‡†ç¡®æ€§ã€‚</li>
<li>ä¸“å®¶æ¨¡å—æ•´åˆè¯„ä¼°ç»“æœä¸æ–°é—»å†…å®¹ï¼Œå®ç°è¯„ä¼°æ„ŸçŸ¥çš„å†³ç­–ã€‚</li>
<li>RealTimeNews-25åŸºå‡†æµ‹è¯•ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨å®æ—¶æ–°é—»ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>EASEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°æœ€ä½³ï¼Œå¹¶åœ¨å®æ—¶æ–°é—»ä¸­æ˜¾è‘—æé«˜æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11277">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f4611a551b59bd6265bf9046b71670b4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810469&auth_key=1760810469-0-0-e7b72a7d31d337721432fb32f7d800d8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6142d4972f6e619dea755119b6cd020f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810477&auth_key=1760810477-0-0-cae4ee8bacb41f2034f0e5ed57ee3f2d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-adf5d4ea7e5223b89fb5980c3c758d91~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810484&auth_key=1760810484-0-0-32fff57cf0134cb9b59fb45b7451e727&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-39e1462b0500bb69559fa526630f44f6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810491&auth_key=1760810491-0-0-446ffd54a4ff316b7388cff8965bc9f7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="DyKnow-RAG-Dynamic-Knowledge-Utilization-Reinforcement-Framework-for-Noisy-Retrieval-Augmented-Generation-in-E-commerce-Search-Relevance"><a href="#DyKnow-RAG-Dynamic-Knowledge-Utilization-Reinforcement-Framework-for-Noisy-Retrieval-Augmented-Generation-in-E-commerce-Search-Relevance" class="headerlink" title="DyKnow-RAG: Dynamic Knowledge Utilization Reinforcement Framework for   Noisy Retrieval-Augmented Generation in E-commerce Search Relevance"></a>DyKnow-RAG: Dynamic Knowledge Utilization Reinforcement Framework for   Noisy Retrieval-Augmented Generation in E-commerce Search Relevance</h2><p><strong>Authors:Tingqiao Xu, Shaowei Yao, Chenhe Dong, Yiming Jin, Zerui Huang, Dan Ou, Haihong Tang</strong></p>
<p>Accurately modeling query-item relevance drives e-commerce ranking, yet long-tail, knowledge-heavy, and fast-evolving queries exceed parametric LLM coverage. External context (reviews, attribute encyclopedias, UGC) can help but is noisy, and single-pass latency and cost forbid any clean-then-summarize step. The model must, per query, judge relevance and decide whether to use, partially use, or ignore the context. DyKnow-RAG is a dynamic noisy-RAG framework built on Group Relative Policy Optimization. It trains two rollout groups (no external context vs a single retrieved chunk) and applies posterior-driven inter-group advantage scaling that adaptively reweights their contributions by the per-query correctness gap. This teaches when to trust retrieval versus fall back to parametric knowledge, without process labels, value networks, or extra inference passes, preserving single-pass, single-chunk deployment under production latency. Training combines: (1) supervised initialization with a structured rationale that explicitly records the context-usage decision; (2) an RL pool prioritized by SFT uncertainty to focus where context choice is most consequential; and (3) an optional lightweight DPO warm start to stabilize with-context calibration. Under a unified retrieval&#x2F;index and fixed latency budget, DyKnow-RAG outperforms SFT, DPO, and vanilla GRPO in offline tests, and delivers consistent lifts on GSB, Query Goodrate, and Item Goodrate in Taobao A&#x2F;B testing. It is deployed in Taobaoâ€™s production relevance system, serving live traffic. To our knowledge, it is among the first single-pass RAG solutions for e-commerce relevance, turning noisy external signals into reliable gains without added online complexity. </p>
<blockquote>
<p>å‡†ç¡®åœ°å¯¹æŸ¥è¯¢é¡¹ç›®ç›¸å…³æ€§è¿›è¡Œå»ºæ¨¡æ˜¯æ¨åŠ¨ç”µå­å•†åŠ¡æ’åçš„é‡è¦å› ç´ ã€‚ç„¶è€Œï¼Œé•¿å°¾ã€çŸ¥è¯†å¯†é›†å‹å’Œå¿«é€Ÿæ¼”å˜çš„æŸ¥è¯¢è¶…å‡ºäº†å‚æ•°åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„è¦†ç›–èŒƒå›´ã€‚å¤–éƒ¨ä¸Šä¸‹æ–‡ï¼ˆè¯„è®ºã€å±æ€§ç™¾ç§‘å…¨ä¹¦ã€ç”¨æˆ·ç”Ÿæˆå†…å®¹ï¼‰å¯èƒ½æœ‰æ‰€å¸®åŠ©ï¼Œä½†å­˜åœ¨å™ªå£°å¹²æ‰°ï¼Œå•æ¬¡ä¼ é€’çš„å»¶è¿Ÿå’Œæˆæœ¬ä¸å…è®¸è¿›è¡Œä»»ä½•æ¸…ç†ç„¶åè¿›è¡Œæ‘˜è¦çš„æ­¥éª¤ã€‚è¯¥æ¨¡å‹å¿…é¡»é’ˆå¯¹æ¯ä¸ªæŸ¥è¯¢ï¼Œåˆ¤æ–­ç›¸å…³æ€§å¹¶å†³å®šæ˜¯å¦ä½¿ç”¨ã€éƒ¨åˆ†ä½¿ç”¨æˆ–å¿½ç•¥ä¸Šä¸‹æ–‡ã€‚DyKnow-RAGæ˜¯ä¸€ä¸ªåŸºäºç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–çš„åŠ¨æ€å™ªå£°RAGæ¡†æ¶ã€‚å®ƒè®­ç»ƒäº†ä¸¤ä¸ªæ¼”ç»ƒç»„ï¼ˆæ— å¤–éƒ¨ä¸Šä¸‹æ–‡ä¸å•ä¸ªæ£€ç´¢å—ï¼‰ï¼Œå¹¶åº”ç”¨äº†åéªŒé©±åŠ¨ç»„é—´ä¼˜åŠ¿ç¼©æ”¾ï¼Œè¿™å¯ä»¥æ ¹æ®æ¯ä¸ªæŸ¥è¯¢çš„æ­£ç¡®æ€§å·®è·è‡ªé€‚åº”åœ°é‡æ–°åŠ æƒå…¶è´¡çŒ®ã€‚è¿™æ•™ä¼šäº†æ¨¡å‹åœ¨ä½•æ—¶ä¿¡ä»»æ£€ç´¢ä¸ä½•æ—¶å›é€€åˆ°å‚æ•°åŒ–çŸ¥è¯†ä¹‹é—´åšå‡ºé€‰æ‹©ï¼Œè€Œæ— éœ€å¤„ç†æ ‡ç­¾ã€ä»·å€¼ç½‘ç»œæˆ–é¢å¤–çš„æ¨ç†ä¼ é€’ï¼Œä»è€Œä¿æŒäº†ç”Ÿäº§å»¶è¿Ÿä¸‹çš„å•æ¬¡ä¼ é€’ã€å•ä¸ªå—éƒ¨ç½²ã€‚è®­ç»ƒç»“åˆäº†ï¼šï¼ˆ1ï¼‰ä½¿ç”¨ç»“æ„åŒ–ç†ç”±çš„ç›‘ç£åˆå§‹åŒ–ï¼Œæ˜ç¡®è®°å½•ä¸Šä¸‹æ–‡ä½¿ç”¨å†³ç­–ï¼›ï¼ˆ2ï¼‰ç”±SFTä¸ç¡®å®šæ€§ä¼˜å…ˆçš„å¼ºåŒ–å­¦ä¹ æ± ï¼Œä»¥é›†ä¸­å…³æ³¨ä¸Šä¸‹æ–‡é€‰æ‹©æœ€å…³é”®çš„æ–¹é¢ï¼›ï¼ˆ3ï¼‰ä¸€ä¸ªå¯é€‰çš„è½»é‡çº§DPOé¢„çƒ­æ¥ç¨³å®šä¸Šä¸‹æ–‡æ ¡å‡†ã€‚åœ¨ç»Ÿä¸€çš„æ£€ç´¢&#x2F;ç´¢å¼•å’Œå›ºå®šçš„å»¶è¿Ÿé¢„ç®—ä¸‹ï¼ŒDyKnow-RAGåœ¨ç¦»çº¿æµ‹è¯•ä¸­ä¼˜äºSFTã€DPOå’Œç®€å•çš„GRPOï¼Œåœ¨æ·˜å®çš„A&#x2F;Bæµ‹è¯•ä¸­ï¼Œå¯¹GSBã€æŸ¥è¯¢å¥½ç‡å’Œé¡¹ç›®å¥½ç‡å®ç°äº†æŒç»­çš„æå‡ã€‚å®ƒå·²éƒ¨ç½²åœ¨æ·˜å®çš„ç”Ÿäº§ç›¸å…³æ€§ç³»ç»Ÿä¸­ï¼Œä¸ºå®æ—¶æµé‡æä¾›æœåŠ¡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒæ˜¯ç”µå­å•†åŠ¡ç›¸å…³æ€§ä¸­é¦–ä¸ªå•é€šé“RAGè§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå°†å˜ˆæ‚çš„å¤–éƒ¨ä¿¡å·è½¬åŒ–ä¸ºå¯é çš„æ”¶ç›Šï¼Œè€Œæ— éœ€å¢åŠ åœ¨çº¿å¤æ‚æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11122v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åœ¨ç”µå­å•†åŠ¡æ’åä¸­å‡†ç¡®å»ºæ¨¡æŸ¥è¯¢é¡¹ç›¸å…³æ€§çš„é‡è¦æ€§ã€‚é’ˆå¯¹é•¿å°¾ã€çŸ¥è¯†å¯†é›†ã€å¿«é€Ÿå˜åŒ–çš„æŸ¥è¯¢ï¼Œæå‡ºäº†DynKnow-RAGåŠ¨æ€å™ªå£°RAGæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿåœ¨æ— é¢å¤–åœ¨çº¿å¤æ‚æ€§çš„æƒ…å†µä¸‹ï¼Œå°†å™ªå£°å¤–éƒ¨ä¿¡å·è½¬åŒ–ä¸ºå¯é çš„ä¼˜åŠ¿ã€‚DynKnow-RAGé‡‡ç”¨åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œè®­ç»ƒäº†ä¸¤ä¸ªæ»šåŠ¨ç»„ï¼Œå¹¶è‡ªé€‚åº”åœ°é‡æ–°è¯„ä¼°å…¶è´¡çŒ®ï¼Œå†³å®šä½•æ—¶ä¿¡ä»»æ£€ç´¢ç»“æœï¼Œä½•æ—¶å›é€€åˆ°å‚æ•°çŸ¥è¯†ã€‚åœ¨æ·˜å®çš„A&#x2F;Bæµ‹è¯•ä¸­ï¼ŒDynKnow-RAGåœ¨ç¦»çº¿æµ‹è¯•ã€GSBã€æŸ¥è¯¢å¥½è¯„ç‡å’Œå•†å“å¥½è¯„ç‡æ–¹é¢éƒ½è¡¨ç°å‡ºäº†ä¸€è‡´çš„æå‡ï¼Œå¹¶å·²éƒ¨ç½²åœ¨æ·˜å®çš„ç”Ÿäº§ç›¸å…³æ€§ç³»ç»Ÿä¸­ï¼Œä¸ºå®æ—¶æµé‡æä¾›æœåŠ¡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŸ¥è¯¢é¡¹ç›¸å…³æ€§çš„ç²¾ç¡®å»ºæ¨¡å¯¹ç”µå­å•†åŠ¡æ’åè‡³å…³é‡è¦ã€‚</li>
<li>é’ˆå¯¹é•¿å°¾ã€çŸ¥è¯†å¯†é›†å’Œå¿«é€Ÿå˜åŒ–çš„æŸ¥è¯¢ï¼Œç°æœ‰çš„å‚æ•°åŒ–LLMè¦†ç›–ä¸è¶³ã€‚</li>
<li>å¤–éƒ¨ä¸Šä¸‹æ–‡ï¼ˆå¦‚è¯„è®ºã€å±æ€§ç™¾ç§‘å…¨ä¹¦ã€ç”¨æˆ·ç”Ÿæˆå†…å®¹ï¼‰æœ‰åŠ©äºå¼¥è¡¥è¿™ä¸€ä¸è¶³ï¼Œä½†å­˜åœ¨å™ªå£°é—®é¢˜ã€‚</li>
<li>DynKnow-RAGæ˜¯ä¸€ä¸ªåŠ¨æ€å™ªå£°RAGæ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®æŸ¥è¯¢çš„æ­£ç¡®æ€§å·®è·è‡ªé€‚åº”åœ°é‡æ–°è¯„ä¼°æ— ä¸Šä¸‹æ–‡å’Œæœ‰å•ä¸€æ£€ç´¢ç»“æœçš„è´¡çŒ®ã€‚</li>
<li>DynKnow-RAGç»“åˆç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ å’Œå¯é€‰çš„DPOé¢„çƒ­ç­–ç•¥è¿›è¡Œè®­ç»ƒã€‚</li>
<li>DynKnow-RAGåœ¨ç¦»çº¿æµ‹è¯•ã€GSBã€æŸ¥è¯¢å¥½è¯„ç‡å’Œå•†å“å¥½è¯„ç‡æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶åœ¨æ·˜å®ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11122">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a97ab1423d1588d5def5f75e7ebc36c3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810498&auth_key=1760810498-0-0-3b3bfd23d34a009a02ea49015474c24f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-847fea46262fa7d3a47645d708c1642e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810506&auth_key=1760810506-0-0-004da2f0105bf8bd5e8fc25a30410145&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2b890a0f16f41132d7d411d6ce6b2cb9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810513&auth_key=1760810513-0-0-3068509df7812075cefc05f45ce0f1cb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Video-STR-Reinforcing-MLLMs-in-Video-Spatio-Temporal-Reasoning-with-Relation-Graph"><a href="#Video-STR-Reinforcing-MLLMs-in-Video-Spatio-Temporal-Reasoning-with-Relation-Graph" class="headerlink" title="Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with   Relation Graph"></a>Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with   Relation Graph</h2><p><strong>Authors:Wentao Wang, Heqing Zou, Tianze Luo, Rui Huang, Yutian Zhao, Zhuochen Wang, Hansheng Zhang, Chengwei Qin, Yan Wang, Lin Zhao, Huaijian Zhang</strong></p>
<p>Recent progress in Multimodal Large Language Models (MLLMs) has demonstrated strong semantic understanding capabilities, but struggles to perform precise spatio-temporal understanding. Existing spatio-temporal methods primarily focus on the video itself, while overlooking the physical information within the video, such as multi-object layouts and motion. Such limitations restrict the use of MLLMs in downstream applications that demand high precision, including embodied intelligence and VR. To address this issue, we present Video-STR, a novel graph-based reinforcement method for precise Video Spatio-Temporal Reasoning. Building upon the capacity of Reinforcement Learning with Verifiable Reward (RLVR) to improve model abilities, we introduce a reasoning mechanism using graph-based Group Relative Policy Optimization (GRPO) method to guide the model in inferring the underlying spatio-temporal topology of scenarios during the thinking process. To resolve the lack of spatio-temporal training data, we construct the STV-205k dataset with 205k question-answering pairs, covering dynamic multi-object scenes in both indoor and outdoor environments, to support the model training. Experiments show that Video-STR achieves state-of-the-art results on various benchmarks, outperforming the base model by 13% on STI-Bench, and demonstrating the effectiveness of our approach and dataset. Code, model, and data will be released. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰è¿›å±•è¡¨æ˜å…¶å…·æœ‰è¾ƒå¼ºçš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œä½†åœ¨æ‰§è¡Œç²¾ç¡®æ—¶ç©ºç†è§£æ–¹é¢é‡åˆ°å›°éš¾ã€‚ç°æœ‰çš„æ—¶ç©ºæ–¹æ³•ä¸»è¦å…³æ³¨è§†é¢‘æœ¬èº«ï¼Œè€Œå¿½ç•¥äº†è§†é¢‘å†…çš„ç‰©ç†ä¿¡æ¯ï¼Œå¦‚å¤šå¯¹è±¡å¸ƒå±€å’Œè¿åŠ¨ã€‚è¿™äº›é™åˆ¶å½±å“äº†MLLMsåœ¨éœ€è¦é«˜ç²¾åº¦çš„ä¸‹æ¸¸åº”ç”¨ä¸­çš„ä½¿ç”¨ï¼ŒåŒ…æ‹¬æ™ºèƒ½èº«ä½“å’Œè™šæ‹Ÿç°å®ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Video-STRï¼Œè¿™æ˜¯ä¸€ç§åŸºäºå›¾çš„æ–°å‹å¼ºåŒ–æ–¹æ³•ï¼Œç”¨äºç²¾ç¡®çš„è§†é¢‘æ—¶ç©ºæ¨ç†ã€‚æˆ‘ä»¬å€ŸåŠ©å¼ºåŒ–å­¦ä¹ ä¸å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰çš„èƒ½åŠ›æ¥æé«˜æ¨¡å‹èƒ½åŠ›ï¼Œå¹¶å¼•å…¥ä¸€ç§åŸºäºå›¾çš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ–¹æ³•çš„æ¨ç†æœºåˆ¶ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹åœ¨æ€è€ƒè¿‡ç¨‹ä¸­æ¨æ–­åœºæ™¯ä¸‹çš„æ—¶ç©ºæ‹“æ‰‘ç»“æ„ã€‚ä¸ºäº†è§£å†³æ—¶ç©ºè®­ç»ƒæ•°æ®çš„ç¼ºä¹ï¼Œæˆ‘ä»¬æ„å»ºäº†STV-205kæ•°æ®é›†ï¼ŒåŒ…å«205ä¸‡é—®ç­”å¯¹ï¼Œè¦†ç›–å®¤å†…å¤–åŠ¨æ€å¤šå¯¹è±¡åœºæ™¯ï¼Œä»¥æ”¯æŒæ¨¡å‹è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒVideo-STRåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼Œåœ¨STI-Benchä¸Šçš„è¡¨ç°æ¯”åŸºç¡€æ¨¡å‹é«˜å‡º13%ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å’Œæ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚ä»£ç ã€æ¨¡å‹å’Œæ•°æ®å°†å‘å¸ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10976v1">PDF</a> </p>
<p><strong>Summary</strong><br>å¤šåª’ä½“æ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨è¯­ä¹‰ç†è§£æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†åœ¨ç²¾ç¡®æ—¶ç©ºç†è§£æ–¹é¢ä»æœ‰å›°éš¾ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå›¾çš„å¼ºåŒ–æ–¹æ³•Video-STRï¼Œç”¨äºç²¾ç¡®è§†é¢‘æ—¶ç©ºæ¨ç†ã€‚æˆ‘ä»¬å¼•å…¥äº†åŸºäºå›¾çš„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰æ–¹æ³•ï¼Œä»¥è§£å†³æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„æ—¶ç©ºæ‹“æ‰‘æ¨æ–­é—®é¢˜ã€‚ä¸ºè§£å†³æ—¶ç©ºè®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†STV-205kæ•°æ®é›†ï¼Œå¹¶æ”¯æŒæ¨¡å‹è®­ç»ƒã€‚å®éªŒè¡¨æ˜ï¼ŒVideo-STRåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº†æœ€æ–°ç»“æœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MLLMså±•ç°å‡ºå¼ºå¤§çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œä½†åœ¨ç²¾ç¡®æ—¶ç©ºç†è§£æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>Video-STRæ˜¯ä¸€ç§åŸºäºå›¾çš„å¼ºåŒ–æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³MLLMsåœ¨ç²¾ç¡®æ—¶ç©ºæ¨ç†æ–¹é¢çš„ä¸è¶³ã€‚</li>
<li>Video-STRå¼•å…¥GRPOæ–¹æ³•ï¼ŒæŒ‡å¯¼æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ¨æ–­åœºæ™¯çš„æ—¶ç©ºæ‹“æ‰‘ã€‚</li>
<li>ä¸ºæ”¯æŒæ¨¡å‹è®­ç»ƒï¼Œæ„å»ºäº†STV-205kæ•°æ®é›†ï¼ŒåŒ…å«205ké—®ç­”å¯¹ï¼Œè¦†ç›–å®¤å†…å’Œå®¤å¤–åŠ¨æ€å¤šå¯¹è±¡åœºæ™¯ã€‚</li>
<li>Video-STRåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸Šå–å¾—æœ€æ–°ç»“æœï¼Œè¾ƒåŸºç¡€æ¨¡å‹åœ¨STI-Benchä¸Šæé«˜äº†13%ã€‚</li>
<li>Video-STRæ–¹æ³•ã€æ¨¡å‹å’Œæ•°æ®å°†å…¬å¼€å‘å¸ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10976">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-33e0fc80234c8565ca9486ee9ea07b36~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810520&auth_key=1760810520-0-0-1f6f4ba9e8521bddb4535a82758aa0ca&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c83b628d088851f2d23440a09ac64b28~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810527&auth_key=1760810527-0-0-dd9a526bbfd934bfc254b689fbea00bc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-078727b187618716b0ea49519639342b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810534&auth_key=1760810534-0-0-c7bb2b5ddc90cc2c7f92f4c3ee079e61&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6887493a02ca66800977208a13b9abbf~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810541&auth_key=1760810541-0-0-b916cb7d864f977b42c1c049be40b0e6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Chart-RVR-Reinforcement-Learning-with-Verifiable-Rewards-for-Explainable-Chart-Reasoning"><a href="#Chart-RVR-Reinforcement-Learning-with-Verifiable-Rewards-for-Explainable-Chart-Reasoning" class="headerlink" title="Chart-RVR: Reinforcement Learning with Verifiable Rewards for   Explainable Chart Reasoning"></a>Chart-RVR: Reinforcement Learning with Verifiable Rewards for   Explainable Chart Reasoning</h2><p><strong>Authors:Sanchit Sinha, Oana Frunza, Kashif Rasul, Yuriy Nevmyvaka, Aidong Zhang</strong></p>
<p>The capabilities of Large Vision-Language Models (LVLMs) have reached state-of-the-art on many visual reasoning tasks, including chart reasoning, yet they still falter on out-of-distribution (OOD) data, and degrade further when asked to produce their chain-of-thought (CoT) rationales, limiting explainability. We present Chart-RVR, a general framework that fine-tunes LVLMs to be more robust and explainable for chart reasoning by coupling Group Relative Policy Optimization (GRPO) with automatically verifiable rewards. Our framework comprises of three rewards that maximize: (i) correct chart-type classification, (ii) faithful chart table reconstruction, and (iii) process conformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently outperforms standard supervised fine-tuning (SFT) on both in-distribution and out-of-distribution datasets, closing the OOD performance gap while improving rationale fidelity. The resulting models, the Chart-RVR-3B series, achieve state-of-the-art results on six chart-reasoning benchmarks spanning in-domain and OOD settings, surpassing all existing models of comparable size. Beyond accuracy, Chart-RVR yields more interpretable CoT rationales, strengthening trust and reliability - showcasing the power of verifiable rewards with GRPO for training reliable, interpretable chart-reasoning models. </p>
<blockquote>
<p>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰çš„èƒ½åŠ›åœ¨è®¸å¤šè§†è§‰æ¨ç†ä»»åŠ¡ä¸Šå·²ç»è¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å‡†ï¼ŒåŒ…æ‹¬å›¾è¡¨æ¨ç†ï¼Œç„¶è€Œå®ƒä»¬åœ¨å¤„ç†ç¦»ç¾¤åˆ†å¸ƒï¼ˆOODï¼‰æ•°æ®æ—¶ä»ç„¶ä¼šå‡ºç°å›°éš¾ï¼Œå½“è¦æ±‚äº§ç”Ÿæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†æ—¶ï¼Œæ€§èƒ½ä¼šè¿›ä¸€æ­¥ä¸‹é™ï¼Œè¿™é™åˆ¶äº†å¯è§£é‡Šæ€§ã€‚æˆ‘ä»¬æå‡ºäº†Chart-RVRï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨æ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œå¯è‡ªåŠ¨éªŒè¯çš„å¥–åŠ±ï¼Œå¯¹LVLMsè¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶åœ¨é¢å¯¹å›¾è¡¨æ¨ç†æ—¶æ›´åŠ ç¨³å¥å’Œå¯è§£é‡Šã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå¥–åŠ±ï¼Œä»¥æœ€å¤§åŒ–ï¼šï¼ˆiï¼‰æ­£ç¡®çš„å›¾è¡¨ç±»å‹åˆ†ç±»ï¼Œï¼ˆiiï¼‰å¿ è¯šçš„å›¾è¡¨è¡¨æ ¼é‡å»ºï¼Œï¼ˆiiiï¼‰è¿‡ç¨‹ä¸€è‡´æ€§ã€‚åº”ç”¨äº3äº¿å‚æ•°LVLMsï¼ŒChart-RVRåœ¨å†…éƒ¨å’Œå¤–éƒ¨æ•°æ®é›†ä¸Šå‡æŒç»­ä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œåœ¨ç¼©å°ç¦»ç¾¤æ€§èƒ½å·®è·çš„åŒæ—¶æé«˜äº†ç†ç”±å¿ å®åº¦ã€‚ç”±æ­¤äº§ç”Ÿçš„æ¨¡å‹â€”â€”Chart-RVR-3Bç³»åˆ—ï¼Œåœ¨æ¶µç›–å†…éƒ¨å’Œç¦»ç¾¤è®¾ç½®çš„å…­ä¸ªå›¾è¡¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æˆæœï¼Œè¶…è¶Šäº†æ‰€æœ‰ç°æœ‰è§„æ¨¡ç›¸å½“çš„æ¨¡å‹ã€‚é™¤äº†å‡†ç¡®æ€§ä¹‹å¤–ï¼ŒChart-RVRäº§ç”Ÿäº†æ›´å…·è§£é‡Šæ€§çš„CoTç†ç”±ï¼Œå¢å¼ºäº†ä¿¡ä»»å’Œå¯é æ€§â€”â€”å±•ç¤ºäº†GRPOä¸å¯éªŒè¯å¥–åŠ±ç»“åˆè®­ç»ƒå¯é ã€å¯è§£é‡Šçš„å›¾è¡¨æ¨ç†æ¨¡å‹çš„å¨åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10973v1">PDF</a> 23 pages</p>
<p><strong>Summary</strong><br>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è§†è§‰æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ï¼Œä½†åœ¨åˆ†å¸ƒå¤–æ•°æ®ä¸Šä»ç„¶å­˜åœ¨é—®é¢˜ï¼Œå¹¶ä¸”åœ¨äº§ç”Ÿæ€ç»´é“¾ï¼ˆCoTï¼‰è§£é‡Šæ—¶æ€§èƒ½ä¼šè¿›ä¸€æ­¥ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Chart-RVRæ¡†æ¶ï¼Œå®ƒé€šè¿‡ç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œå¯è‡ªåŠ¨éªŒè¯çš„å¥–åŠ±æ¥å¾®è°ƒLVLMsï¼Œä½¿å…¶å¯¹å›¾è¡¨æ¨ç†æ›´åŠ ç¨³å¥å’Œå¯è§£é‡Šã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªå¥–åŠ±ï¼Œä»¥æœ€å¤§åŒ–ï¼šï¼ˆiï¼‰æ­£ç¡®çš„å›¾è¡¨ç±»å‹åˆ†ç±»ï¼Œï¼ˆiiï¼‰å¿ å®çš„å›¾è¡¨é‡å»ºï¼Œï¼ˆiiiï¼‰æµç¨‹ä¸€è‡´æ€§ã€‚åœ¨3äº¿å‚æ•°LVLMsä¸Šçš„åº”ç”¨è¡¨æ˜ï¼ŒChart-RVRåœ¨å†…éƒ¨å’Œå¤–éƒ¨æ•°æ®é›†ä¸Šçš„æ€§èƒ½å‡ä¼˜äºæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œç¼©å°äº†OODæ€§èƒ½å·®è·ï¼ŒåŒæ—¶æé«˜äº†è§£é‡Šæ€§ã€‚Chart-RVRç³»åˆ—æ¨¡å‹åœ¨æ¶µç›–å†…éƒ¨å’Œå¤–éƒ¨è®¾ç½®çš„å…­ä¸ªå›¾è¡¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šè¾¾åˆ°äº†æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚é™¤äº†å‡†ç¡®æ€§ä¹‹å¤–ï¼ŒChart-RVRè¿˜äº§ç”Ÿäº†æ›´å…·è§£é‡Šæ€§çš„æ€ç»´é“¾è§£é‡Šï¼Œå¢å¼ºäº†ä¿¡ä»»å’Œå¯é æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMsï¼‰åœ¨è®¸å¤šè§†è§‰æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œä½†åœ¨å¤„ç†åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®æ—¶ä»æœ‰å±€é™ã€‚</li>
<li>Chart-RVRæ¡†æ¶æ—¨åœ¨é€šè¿‡ç»“åˆç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰å’Œè‡ªåŠ¨éªŒè¯å¥–åŠ±æ¥æå‡LVLMsçš„ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>Chart-RVRæ¡†æ¶åŒ…å«ä¸‰ä¸ªå¥–åŠ±ï¼Œåˆ†åˆ«å…³æ³¨æ­£ç¡®çš„å›¾è¡¨ç±»å‹åˆ†ç±»ã€å¿ å®çš„å›¾è¡¨é‡å»ºå’Œæµç¨‹ä¸€è‡´æ€§ã€‚</li>
<li>åœ¨3äº¿å‚æ•°LVLMsä¸Šåº”ç”¨Chart-RVRæ¡†æ¶ï¼Œæ— è®ºåœ¨å†…éƒ¨è¿˜æ˜¯å¤–éƒ¨æ•°æ®é›†ä¸Šï¼Œå…¶æ€§èƒ½å‡è¶…è¶Šæ ‡å‡†ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚</li>
<li>Chart-RVRæ¡†æ¶ç¼©å°äº†OODæ€§èƒ½å·®è·ï¼ŒåŒæ—¶æé«˜äº†æ¨¡å‹è§£é‡Šæ€§ã€‚</li>
<li>Chart-RVRç³»åˆ—æ¨¡å‹åœ¨å¤šä¸ªå›¾è¡¨æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°æœ€æ–°æŠ€æœ¯æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10973">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-4597b594afbda80b546904deec13f9d2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810549&auth_key=1760810549-0-0-34cb6ec4bb7f521c6d4ae40448d7fddb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c99c0c49582ed2cf66f53174852f9d66~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810556&auth_key=1760810556-0-0-42eb63b352a79d3418c6ad7bd49d6805&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Rediscovering-Entropy-Regularization-Adaptive-Coefficient-Unlocks-Its-Potential-for-LLM-Reinforcement-Learning"><a href="#Rediscovering-Entropy-Regularization-Adaptive-Coefficient-Unlocks-Its-Potential-for-LLM-Reinforcement-Learning" class="headerlink" title="Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its   Potential for LLM Reinforcement Learning"></a>Rediscovering Entropy Regularization: Adaptive Coefficient Unlocks Its   Potential for LLM Reinforcement Learning</h2><p><strong>Authors:Xiaoyun Zhang, Xiaojian Yuan, Di Huang, Wang You, Chen Hu, Jingqing Ruan, Kejiang Chen, Xing Hu</strong></p>
<p>Reasoning ability has become a defining capability of Large Language Models (LLMs), with Reinforcement Learning with Verifiable Rewards (RLVR) emerging as a key paradigm to enhance it. However, RLVR training often suffers from policy entropy collapse, where the policy becomes overly deterministic, hindering exploration and limiting reasoning performance. While entropy regularization is a common remedy, its effectiveness is highly sensitive to the fixed coefficient, making it unstable across tasks and models. In this work, we revisit entropy regularization in RLVR and argue that its potential has been largely underestimated. Our analysis shows that (i) tasks of varying difficulty demand distinct exploration intensities, and (ii) balanced exploration may require the policy entropy to be maintained within a moderate range below its initial level. Therefore, we propose Adaptive Entropy Regularization (AER)â€“a framework that dynamically balances exploration and exploitation via three components: difficulty-aware coefficient allocation, initial-anchored target entropy, and dynamic global coefficient adjustment. Experiments on multiple mathematical reasoning benchmarks show that AER consistently outperforms baselines, improving both reasoning accuracy and exploration capability. </p>
<blockquote>
<p>æ¨ç†èƒ½åŠ›å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸€ç§æ ¸å¿ƒåŠŸèƒ½ï¼Œè€Œå¼ºåŒ–å­¦ä¹ é€šè¿‡å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰å·²æˆä¸ºå¢å¼ºè¿™ä¸€åŠŸèƒ½çš„å…³é”®èŒƒå¼ã€‚ç„¶è€Œï¼ŒRLVRè®­ç»ƒç»å¸¸å—åˆ°ç­–ç•¥ç†µå´©æºƒçš„å½±å“ï¼Œç­–ç•¥å˜å¾—è¿‡äºç¡®å®šæ€§ï¼Œé˜»ç¢äº†æ¢ç´¢å¹¶é™åˆ¶äº†æ¨ç†æ€§èƒ½ã€‚è™½ç„¶ç†µæ­£åˆ™åŒ–æ˜¯ä¸€ç§å¸¸è§çš„è¡¥æ•‘æ–¹æ³•ï¼Œä½†å…¶æœ‰æ•ˆæ€§å¯¹å›ºå®šç³»æ•°é«˜åº¦æ•æ„Ÿï¼Œå¯¼è‡´å…¶åœ¨ä¸åŒä»»åŠ¡å’Œæ¨¡å‹ä¸­çš„ç¨³å®šæ€§è¾ƒå·®ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†RLVRä¸­çš„ç†µæ­£åˆ™åŒ–ï¼Œå¹¶è®¤ä¸ºå…¶æ½œåŠ›åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¢«ä½ä¼°äº†ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼ˆiï¼‰ä¸åŒéš¾åº¦çš„ä»»åŠ¡éœ€è¦ä¸åŒçš„æ¢ç´¢å¼ºåº¦ï¼›ï¼ˆiiï¼‰å¹³è¡¡æ¢ç´¢å¯èƒ½éœ€è¦å°†ç­–ç•¥ç†µä¿æŒåœ¨åˆå§‹æ°´å¹³ä»¥ä¸‹çš„é€‚åº¦èŒƒå›´å†…ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”ç†µæ­£åˆ™åŒ–ï¼ˆAERï¼‰â€”â€”ä¸€ä¸ªæ¡†æ¶ï¼Œé€šè¿‡éš¾åº¦æ„ŸçŸ¥ç³»æ•°åˆ†é…ã€åˆå§‹é”šå®šç›®æ ‡ç†µå’ŒåŠ¨æ€å…¨å±€ç³»æ•°è°ƒæ•´ä¸‰ä¸ªç»„ä»¶æ¥åŠ¨æ€å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ã€‚åœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAERå§‹ç»ˆä¼˜äºåŸºçº¿ï¼Œæé«˜äº†æ¨ç†å‡†ç¡®æ€§å’Œæ¢ç´¢èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10959v2">PDF</a> 16 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å·²ç»æˆä¸ºä¸€ç§é‡è¦çš„ç‰¹æ€§ï¼Œå¼ºåŒ–å­¦ä¹ åŠ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯æé«˜è¿™ç§èƒ½åŠ›çš„ä¸€ç§å…³é”®æ–¹æ³•ã€‚ç„¶è€Œï¼ŒRLVRè®­ç»ƒå¸¸é‡åˆ°ç­–ç•¥ç†µå´©æºƒçš„é—®é¢˜ï¼Œé™åˆ¶äº†æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å’Œæ¨ç†æ€§èƒ½ã€‚æœ¬æ–‡é‡æ–°å®¡è§†äº†RLVRä¸­çš„ç†µæ­£åˆ™åŒ–æ–¹æ³•ï¼Œå¹¶æå‡ºäº†è‡ªé€‚åº”ç†µæ­£åˆ™åŒ–ï¼ˆAERï¼‰æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨ï¼ŒåŒ…æ‹¬éš¾åº¦æ„ŸçŸ¥ç³»æ•°åˆ†é…ã€åˆå§‹é”šå®šç›®æ ‡ç†µå’ŒåŠ¨æ€å…¨å±€ç³»æ•°è°ƒæ•´ä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚å®éªŒè¯æ˜ï¼ŒAERåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæé«˜äº†æ¨ç†å‡†ç¡®æ€§å’Œæ¢ç´¢èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åŠ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ˜¯æé«˜å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›çš„é‡è¦æ–¹æ³•ã€‚</li>
<li>RLVRè®­ç»ƒä¸­çš„ç­–ç•¥ç†µå´©æºƒé—®é¢˜é™åˆ¶äº†æ¨¡å‹çš„æ¢ç´¢èƒ½åŠ›å’Œæ¨ç†æ€§èƒ½ã€‚</li>
<li>ç†µæ­£åˆ™åŒ–æ˜¯ä¸€ç§è§£å†³ç­–ç•¥ç†µå´©æºƒé—®é¢˜çš„å¸¸è§æ–¹æ³•ï¼Œä½†å…¶æœ‰æ•ˆæ€§å¯¹å›ºå®šç³»æ•°æ•æ„Ÿï¼Œä»»åŠ¡ä¸æ¨¡å‹é—´çš„ç¨³å®šæ€§è¾ƒå·®ã€‚</li>
<li>æœ¬æ–‡æå‡ºè‡ªé€‚åº”ç†µæ­£åˆ™åŒ–ï¼ˆAERï¼‰æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>AERåŒ…æ‹¬éš¾åº¦æ„ŸçŸ¥ç³»æ•°åˆ†é…ã€åˆå§‹é”šå®šç›®æ ‡ç†µå’ŒåŠ¨æ€å…¨å±€ç³»æ•°è°ƒæ•´ä¸‰ä¸ªå…³é”®ç»„ä»¶ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒAERåœ¨å¤šä¸ªæ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæé«˜äº†æ¨ç†å‡†ç¡®æ€§å’Œæ¢ç´¢èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10959">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5886e3cf9e69d171a62ef03d09ed652e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810563&auth_key=1760810563-0-0-2f395579df27f8b53c16298fc48398d7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-805f89baad2fa26c3775b063376b7724~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810570&auth_key=1760810570-0-0-d84c092ea072c3b4c99a2de98221ca33&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d298dd579ef104d9f5b8a892037b5afa~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810577&auth_key=1760810577-0-0-ba2e38b25b4b59912e7a221c96e4e96e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-68bdc6bbf39052271492b2c3b979137c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760810583&auth_key=1760810583-0-0-b48504a6afa8a5694979cf3045877d1c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-19/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-19/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-19/LLM/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-e6ae81fb6af72e956905e01558e482dc~resize:0:q75.jpg?source=1f5c5e47&expiration=1760812539&auth_key=1760812539-0-0-59beb2fc73107975a8ede6dc09e6a3fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-19  Prompt-Guided Spatial Understanding with RGB-D Transformers for   Fine-Grained Object Relation Reasoning
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-18/Talking%20Head%20Generation/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-63ea9726baebdbc2f571e940f0ef5894~resize:0:q75.jpg?source=1f5c5e47&expiration=1760756965&auth_key=1760756965-0-0-1a5e6d6ffd688eeee265c5a69bf2bd01&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Talking Head Generation">
                        
                        <span class="card-title">Talking Head Generation</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Talking Head Generation æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-18  Talking Points Describing and Localizing Pixels
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-18
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                    Talking Head Generation
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                        <span class="chip bg-color">Talking Head Generation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">30930.2k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
