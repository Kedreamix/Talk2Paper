<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Speech">
    <meta name="description" content="Speech æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-19  End-to-end Automatic Speech Recognition and Speech Translation   Integration of Speech Foundational Models and LLMs">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Speech | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-daa56626c15f43c655239f8b4ce78f7d')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Speech</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Speech/">
                                <span class="chip bg-color">Speech</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Speech/" class="post-category">
                                Speech
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    55 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-19-æ›´æ–°"><a href="#2025-10-19-æ›´æ–°" class="headerlink" title="2025-10-19 æ›´æ–°"></a>2025-10-19 æ›´æ–°</h1><h2 id="End-to-end-Automatic-Speech-Recognition-and-Speech-Translation-Integration-of-Speech-Foundational-Models-and-LLMs"><a href="#End-to-end-Automatic-Speech-Recognition-and-Speech-Translation-Integration-of-Speech-Foundational-Models-and-LLMs" class="headerlink" title="End-to-end Automatic Speech Recognition and Speech Translation:   Integration of Speech Foundational Models and LLMs"></a>End-to-end Automatic Speech Recognition and Speech Translation:   Integration of Speech Foundational Models and LLMs</h2><p><strong>Authors:Nam Luu, OndÅ™ej Bojar</strong></p>
<p>Speech Translation (ST) is a machine translation task that involves converting speech signals from one language to the corresponding text in another language; this task has two different approaches, namely the traditional cascade and the more recent end-to-end. This paper explores a combined end-to-end architecture of pre-trained speech encoders and Large Language Models (LLMs) for performing both Automatic Speech Recognition (ASR) and ST simultaneously. Experiments with the English-to-German language pair show that our best model not only can achieve better translation results than SeamlessM4T, a large foundational end-to-end, multi-modal translation model, but can also match the performance of a cascaded system with Whisper and NLLB, with up to a score gain of 8% in $\text{COMET}^{\text{DA}}_{22}$ metric. </p>
<blockquote>
<p>è¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰æ˜¯ä¸€é¡¹æœºå™¨ç¿»è¯‘ä»»åŠ¡ï¼Œæ¶‰åŠå°†ä¸€ç§è¯­è¨€çš„è¯­éŸ³ä¿¡å·è½¬æ¢ä¸ºå¦ä¸€ç§è¯­è¨€ä¸­çš„ç›¸åº”æ–‡æœ¬ï¼›è¿™é¡¹ä»»åŠ¡æœ‰ä¸¤ç§ä¸åŒçš„æ–¹æ³•ï¼Œå³ä¼ ç»Ÿçš„çº§è”æ–¹æ³•å’Œæ›´æ–°çš„ç«¯åˆ°ç«¯æ–¹æ³•ã€‚æœ¬æ–‡æ¢ç´¢äº†ä¸€ç§ç»“åˆé¢„è®­ç»ƒè¯­éŸ³ç¼–ç å™¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç«¯åˆ°ç«¯æ¶æ„ï¼Œå¯åŒæ—¶æ‰§è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’ŒSTã€‚ä½¿ç”¨è‹±å¾·è¯­è¨€å¯¹è¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æœ€å¥½çš„æ¨¡å‹ä¸ä»…å¯ä»¥å®ç°æ¯”æ— ç¼M4Tï¼ˆä¸€ç§å¤§å‹ç«¯åˆ°ç«¯å¤šæ¨¡æ€ç¿»è¯‘æ¨¡å‹ï¼‰æ›´å¥½çš„ç¿»è¯‘ç»“æœï¼Œè¿˜å¯ä»¥ä¸ç»“åˆäº†whisperå’ŒNLLBçš„çº§è”ç³»ç»Ÿç›¸åŒ¹æ•Œï¼Œåœ¨COMET DA 22æŒ‡æ ‡ä¸Šå¾—åˆ†æé«˜äº†é«˜è¾¾8%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10329v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ç»“åˆé¢„è®­ç»ƒè¯­éŸ³ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç«¯åˆ°ç«¯æ¶æ„ï¼Œç”¨äºåŒæ—¶æ‰§è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è‹±å¾·è¯­è¨€å¯¹ä¸Šä¸ä»…ä¼˜äºå¤§å‹åŸºç¡€ç«¯åˆ°ç«¯å¤šæ¨¡æ€ç¿»è¯‘æ¨¡å‹SeamlessM4Tï¼Œè¿˜èƒ½ä¸é‡‡ç”¨Whisperå’ŒNLLBçš„çº§è”ç³»ç»Ÿç›¸åŒ¹é…ï¼Œåœ¨COMET^DA_22æŒ‡æ ‡ä¸Šå¾—åˆ†æé«˜äº†8%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³ç¿»è¯‘ï¼ˆSTï¼‰æ˜¯æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¹‹ä¸€ï¼Œæ¶‰åŠå°†è¯­éŸ³ä¿¡å·ä»ä¸€ç§è¯­è¨€è½¬æ¢ä¸ºå¦ä¸€ç§è¯­è¨€çš„ç›¸åº”æ–‡æœ¬ã€‚</li>
<li>è¯­éŸ³ç¿»è¯‘ä»»åŠ¡æœ‰ä¸¤ç§æ–¹æ³•ï¼šä¼ ç»Ÿçš„çº§è”å’Œè¾ƒæ–°çš„ç«¯åˆ°ç«¯æ–¹æ³•ã€‚</li>
<li>æœ¬æ–‡æ¢ç´¢äº†ç»“åˆé¢„è®­ç»ƒè¯­éŸ³ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç«¯åˆ°ç«¯æ¶æ„ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½åŒæ—¶æ‰§è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³ç¿»è¯‘ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è‹±å¾·è¯­è¨€å¯¹ä¸Šçš„ç¿»è¯‘ç»“æœä¼˜äºSeamlessM4Tæ¨¡å‹ã€‚</li>
<li>è¯¥æ¨¡å‹çš„æ€§èƒ½ä¸çº§è”ç³»ç»Ÿç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æŒ‡æ ‡ä¸Šè¡¨ç°æ›´å¥½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10329">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d59c2f6209aca85e5158c427d9db2298" align="middle">
<img src="https://picx.zhimg.com/v2-c02d4d902312ea6b1cf5aa9219a78811" align="middle">
<img src="https://picx.zhimg.com/v2-07d2def8b36ba236a95789f86eae5185" align="middle">
<img src="https://picx.zhimg.com/v2-3c87797c54dc6893e173480bdbf5f40e" align="middle">
<img src="https://picx.zhimg.com/v2-a95dd97089037c6a7a1ccec2e208c106" align="middle">
<img src="https://picx.zhimg.com/v2-be5682dd6310483dd6d59eba0f5f5bd4" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SyncLipMAE-Contrastive-Masked-Pretraining-for-Audio-Visual-Talking-Face-Representation"><a href="#SyncLipMAE-Contrastive-Masked-Pretraining-for-Audio-Visual-Talking-Face-Representation" class="headerlink" title="SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face   Representation"></a>SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face   Representation</h2><p><strong>Authors:Zeyu Ling, Xiaodong Gu, Jiangnan Tang, Changqing Zou</strong></p>
<p>We introduce SyncLipMAE, a self-supervised pretraining framework for talking-face video that learns synchronization-aware and transferable facial dynamics from unlabeled audio-visual streams. Our approach couples masked visual modeling with cross-modal contrastive alignment and employs three per-frame prompt tokens that explicitly encode the essential factors of a talking-face frame - identity, vocal motion (speech-synchronized facial dynamics), and ambient motion (audio-agnostic movements such as blinks and head pose). The contrastive objective uses time-aligned vocal-motion and audio tokens as positives and misaligned pairs as negatives, driving both modalities into a shared embedding space and yielding token-level audio-visual stream synchronization. After pretraining, the aligned audio tokens together with the visual prompt tokens (identity, vocal motion, ambient motion) form a unified interface for four disparate downstream settings: (i) audio-visual stream synchronization; (ii) facial emotion and head&#x2F;face action recognition; (iii) visual speech recognition; and (iv) visual dubbing, for which we enable indistinguishable audio- or video-driven control within a single model. Across four task families that require distinct capabilities, SyncLipMAE achieves state-of-the-art results, underscoring the effectiveness of synchronization-aware, factorized self-supervised pretraining. </p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†SyncLipMAEï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè¯´è¯äººè„¸è§†é¢‘çš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œå®ƒå¯ä»¥ä»æ— æ ‡ç­¾çš„è§†å¬æµä¸­å­¦ä¹ åŒæ­¥æ„ŸçŸ¥å’Œå¯è½¬ç§»çš„é¢åŠ¨åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•å°†æ©æ¨¡è§†è§‰å»ºæ¨¡ä¸è·¨æ¨¡æ€å¯¹æ¯”å¯¹é½ç›¸ç»“åˆï¼Œå¹¶é‡‡ç”¨ä¸‰ä¸ªæ¯å¸§æç¤ºä»¤ç‰Œï¼Œæ˜ç¡®ç¼–ç è¯´è¯äººè„¸å¸§çš„å…³é”®å› ç´ â€”â€”èº«ä»½ã€è¯­éŸ³è¿åŠ¨ï¼ˆä¸è¯­éŸ³åŒæ­¥çš„é¢åŠ¨åŠ›ï¼‰å’Œç¯å¢ƒè¿åŠ¨ï¼ˆä¸éŸ³é¢‘æ— å…³çš„è¿åŠ¨ï¼Œå¦‚çœ¨çœ¼å’Œå¤´éƒ¨å§¿åŠ¿ï¼‰ã€‚å¯¹æ¯”ç›®æ ‡ä½¿ç”¨æ—¶é—´å¯¹é½çš„è¯­éŸ³è¿åŠ¨å’Œå£°å­¦ä»¤ç‰Œä½œä¸ºæ­£æ ·æœ¬ï¼Œé”™ä½å¯¹ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œå°†è¿™ä¸¤ç§æ¨¡å¼é©±åŠ¨åˆ°å…±äº«åµŒå…¥ç©ºé—´ï¼Œå¹¶äº§ç”Ÿä»¤ç‰Œçº§çš„è§†å¬æµåŒæ­¥ã€‚é¢„è®­ç»ƒåï¼Œå¯¹é½çš„å£°å­¦ä»¤ç‰Œä¸è§†è§‰æç¤ºä»¤ç‰Œï¼ˆèº«ä»½ã€è¯­éŸ³è¿åŠ¨ã€ç¯å¢ƒè¿åŠ¨ï¼‰ä¸ºå››ä¸ªç‹¬ç«‹çš„ä¸‹æ¸¸è®¾ç½®å½¢æˆäº†ä¸€ä¸ªç»Ÿä¸€ç•Œé¢ï¼šï¼ˆiï¼‰è§†å¬æµåŒæ­¥ï¼›ï¼ˆiiï¼‰é¢éƒ¨æƒ…æ„Ÿä¸å¤´éƒ¨&#x2F;é¢éƒ¨åŠ¨ä½œè¯†åˆ«ï¼›ï¼ˆiiiï¼‰è§†è§‰è¯­éŸ³è¯†åˆ«ï¼›ï¼ˆivï¼‰è§†é¢‘é…éŸ³ï¼Œæˆ‘ä»¬åœ¨å•ä¸ªæ¨¡å‹ä¸­å®ç°äº†æ— æ³•åŒºåˆ†çš„éŸ³é¢‘é©±åŠ¨æˆ–è§†é¢‘é©±åŠ¨æ§åˆ¶ã€‚åœ¨éœ€è¦ä¸åŒèƒ½åŠ›çš„å››ä¸ªä»»åŠ¡å®¶æ—ä¸­ï¼ŒSyncLipMAEå–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœï¼Œè¿™çªå‡ºäº†åŒæ­¥æ„ŸçŸ¥ã€å› å­åŒ–çš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10069v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>SyncLipMAEæ˜¯ä¸€ä¸ªç”¨äºè¯´è¯äººè„¸è§†é¢‘çš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œèƒ½å¤Ÿä»æ— æ ‡ç­¾çš„è§†å¬æµä¸­å­¦ä¹ åŒæ­¥æ„ŸçŸ¥å’Œå¯è¿ç§»çš„é¢éƒ¨åŠ¨æ€ã€‚å®ƒé€šè¿‡ç»“åˆæ©ç è§†è§‰å»ºæ¨¡å’Œè·¨æ¨¡æ€å¯¹æ¯”å¯¹é½ï¼Œé‡‡ç”¨æ¯å¸§ä¸‰ä¸ªæç¤ºä»¤ç‰Œæ˜¾å¼ç¼–ç è¯´è¯äººè„¸å¸§çš„å…³é”®å› ç´ ï¼ŒåŒ…æ‹¬èº«ä»½ã€è¯­éŸ³åŠ¨ä½œå’Œå‘¨å›´åŠ¨ä½œã€‚è¯¥æ¡†æ¶é€šè¿‡æ—¶é—´å¯¹é½çš„è¯­éŸ³åŠ¨ä½œå’ŒéŸ³é¢‘ä»¤ç‰Œä½œä¸ºæ­£å‘å’Œé”™ä½çš„å¯¹ä½œä¸ºè´Ÿå‘ï¼Œæ¨åŠ¨ä¸¤ç§æ¨¡å¼è¿›å…¥å…±äº«åµŒå…¥ç©ºé—´ï¼Œå®ç°ä»¤ç‰Œçº§è§†å¬æµåŒæ­¥ã€‚é¢„è®­ç»ƒåï¼Œå¯¹é½çš„éŸ³é¢‘ä»¤ç‰Œä¸è§†è§‰æç¤ºä»¤ç‰Œï¼ˆèº«ä»½ã€è¯­éŸ³åŠ¨ä½œã€å‘¨å›´åŠ¨ä½œï¼‰å½¢æˆä¸€ä¸ªç»Ÿä¸€çš„æ¥å£ï¼Œä¸ºå››ç§ä¸åŒçš„ä¸‹æ¸¸è®¾ç½®æä¾›å¼ºå¤§çš„æ”¯æŒï¼ŒåŒ…æ‹¬è§†å¬æµåŒæ­¥ã€é¢éƒ¨æƒ…æ„Ÿä¸å¤´éƒ¨&#x2F;é¢éƒ¨åŠ¨ä½œè¯†åˆ«ã€è§†è§‰è¯­éŸ³è¯†åˆ«å’Œè§†è§‰é…éŸ³ã€‚SyncLipMAEåœ¨å››ä¸ªéœ€è¦ä¸åŒèƒ½åŠ›çš„ä»»åŠ¡å®¶æ—ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œå‡¸æ˜¾äº†åŒæ­¥æ„ŸçŸ¥ã€å› å­åŒ–çš„è‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒçš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>SyncLipMAEæ˜¯ä¸€ä¸ªè‡ªæˆ‘ç›‘ç£é¢„è®­ç»ƒæ¡†æ¶ï¼Œé€‚ç”¨äºè¯´è¯äººè„¸è§†é¢‘ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½ä»æ— æ ‡ç­¾çš„è§†å¬æµä¸­å­¦ä¹ åŒæ­¥æ„ŸçŸ¥å’Œå¯è¿ç§»çš„é¢éƒ¨åŠ¨æ€ã€‚</li>
<li>é€šè¿‡ç»“åˆæ©ç è§†è§‰å»ºæ¨¡å’Œè·¨æ¨¡æ€å¯¹æ¯”å¯¹é½ï¼Œå®ç°éŸ³é¢‘ä¸è§†é¢‘çš„åŒ¹é…ä¸èåˆã€‚</li>
<li>æ¯å¸§ä¸‰ä¸ªæç¤ºä»¤ç‰Œæ˜¾å¼ç¼–ç èº«ä»½ã€è¯­éŸ³åŠ¨ä½œå’Œå‘¨å›´åŠ¨ä½œç­‰å…³é”®å› ç´ ã€‚</li>
<li>å¯¹æ¯”ç›®æ ‡ä½¿ç”¨æ—¶é—´å¯¹é½çš„è¯­éŸ³åŠ¨ä½œå’ŒéŸ³é¢‘ä»¤ç‰Œä½œä¸ºæ­£å‘æ ·æœ¬ï¼Œé”™ä½çš„å¯¹ä½œä¸ºè´Ÿå‘æ ·æœ¬ã€‚</li>
<li>é¢„è®­ç»ƒåçš„æ¨¡å‹å¯ä»¥æ”¯æŒå››ç§ä¸‹æ¸¸ä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†å¬æµåŒæ­¥ã€é¢éƒ¨æƒ…æ„Ÿä¸åŠ¨ä½œè¯†åˆ«ã€è§†è§‰è¯­éŸ³è¯†åˆ«å’Œè§†è§‰é…éŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10069">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9c6c231fed765423258fe234ae412690" align="middle">
<img src="https://picx.zhimg.com/v2-391de2510e38295189764810bb249260" align="middle">
<img src="https://picx.zhimg.com/v2-190877a8d3a02c18ee8f78ee7cbb4971" align="middle">
<img src="https://picx.zhimg.com/v2-e99abe911990569078aa7a1f866286f6" align="middle">
<img src="https://picx.zhimg.com/v2-a15551b727f9e713ce75cbbe3e045ac1" align="middle">
<img src="https://picx.zhimg.com/v2-6d00000819b45008474ef976b1a274a9" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Mind-Paced-Speaking-A-Dual-Brain-Approach-to-Real-Time-Reasoning-in-Spoken-Language-Models"><a href="#Mind-Paced-Speaking-A-Dual-Brain-Approach-to-Real-Time-Reasoning-in-Spoken-Language-Models" class="headerlink" title="Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in   Spoken Language Models"></a>Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in   Spoken Language Models</h2><p><strong>Authors:Donghang Wu, Haoyang Zhang, Jun Chen,  Xiangyu,  Zhang, Hexin Liu, Eng Siong Chng, Fei Tian, Xuerui Yang, Xiangyu Zhang, Daxin Jiang, Gang Yu</strong></p>
<p>Real-time Spoken Language Models (SLMs) struggle to leverage Chain-of-Thought (CoT) reasoning due to the prohibitive latency of generating the entire thought process sequentially. Enabling SLMs to think while speaking, similar to humans, is attracting increasing attention. We present, for the first time, Mind-Paced Speaking (MPS), a brain-inspired framework that enables high-fidelity, real-time reasoning. Similar to how humans utilize distinct brain regions for thinking and responding, we propose a novel dual-brain approach, employing a â€œFormulation Brainâ€ for high-level reasoning to pace and guide a separate â€œArticulation Brainâ€ for fluent speech generation. This division of labor eliminates mode-switching, preserving the integrity of the reasoning process. Experiments show that MPS significantly outperforms existing think-while-speaking methods and achieves reasoning performance comparable to models that pre-compute the full CoT before speaking, while drastically reducing latency. Under a zero-latency configuration, the proposed method achieves an accuracy of 92.8% on the mathematical reasoning task Spoken-MQA and attains a score of 82.5 on the speech conversation task URO-Bench. Our work effectively bridges the gap between high-quality reasoning and real-time interaction. </p>
<blockquote>
<p>å®æ—¶å£è¯­æ¨¡å‹ï¼ˆSLMï¼‰ç”±äºç”Ÿæˆæ•´ä¸ªæ€ç»´è¿‡ç¨‹çš„å»¶è¿Ÿæ€§è€Œæ— æ³•å……åˆ†åˆ©ç”¨æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ã€‚ä½¿SLMèƒ½å¤Ÿåœ¨è¯´è¯æ—¶è¿›è¡Œæ€è€ƒï¼Œç±»ä¼¼äºäººç±»ï¼Œæ­£å¼•èµ·è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚æˆ‘ä»¬é¦–æ¬¡æå‡ºâ€œæ€ç»´é©±åŠ¨è¯´è¯â€ï¼ˆMPSï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå—å¤§è„‘å¯å‘çš„æ¡†æ¶ï¼Œå¯å®ç°é«˜ä¿çœŸã€å®æ—¶æ¨ç†ã€‚ä¸äººç±»åˆ©ç”¨ä¸åŒçš„å¤§è„‘åŒºåŸŸè¿›è¡Œæ€ç»´å’Œå›åº”çš„æ–¹å¼ç±»ä¼¼ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹åŒè„‘æ–¹æ³•ï¼Œé‡‡ç”¨ä¸€ä¸ªâ€œåˆ¶å®šå†³ç­–çš„å¤§è„‘â€è¿›è¡Œé«˜çº§æ¨ç†ï¼Œä»¥æ§åˆ¶å’ŒæŒ‡å¯¼å¦ä¸€ä¸ªâ€œå‘å£°å¤§è„‘â€è¿›è¡Œæµç•…çš„è¯­éŸ³ç”Ÿæˆã€‚è¿™ç§åˆ†å·¥æ¶ˆé™¤äº†æ¨¡å¼åˆ‡æ¢ï¼Œä¿æŒäº†æ¨ç†è¿‡ç¨‹çš„å®Œæ•´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒMPSæ˜¾è‘—ä¼˜äºç°æœ‰çš„è¾¹æ€è€ƒè¾¹è¯´è¯çš„æ–¹æ³•ï¼Œå…¶æ¨ç†æ€§èƒ½ä¸é¢„å…ˆè®¡ç®—å®Œæ•´çš„CoTå†è¯´è¯æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶å¤§å¤§é™ä½äº†å»¶è¿Ÿã€‚åœ¨é›¶å»¶è¿Ÿé…ç½®ä¸‹ï¼Œæ‰€ææ–¹æ³•åœ¨å£è¯­æ•°å­¦æ¨ç†ä»»åŠ¡Spoken-MQAä¸Šè¾¾åˆ°äº†92.8%çš„å‡†ç¡®ç‡ï¼Œåœ¨è¯­éŸ³å¯¹è¯ä»»åŠ¡URO-Benchä¸Šè¾¾åˆ°äº†82.5åˆ†ã€‚æˆ‘ä»¬çš„å·¥ä½œæœ‰æ•ˆåœ°å¼¥åˆäº†é«˜è´¨é‡æ¨ç†å’Œå®æ—¶äº¤äº’ä¹‹é—´çš„å·®è·ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09592v1">PDF</a> 13 pages, 3 figures</p>
<p><strong>Summary</strong></p>
<p>å®æ—¶å£è¯­æ¨¡å‹ï¼ˆSLMï¼‰åœ¨åˆ©ç”¨é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†æ—¶é¢ä¸´å›°éš¾ï¼Œå› ä¸ºç”Ÿæˆæ•´ä¸ªæ€ç»´è¿‡ç¨‹çš„æ—¶é—´å»¶è¿Ÿè¿‡é•¿ã€‚æœ¬æ–‡é¦–æ¬¡æå‡ºâ€œæ€ç»´é©±åŠ¨å£è¯­â€ï¼ˆMPSï¼‰æ¡†æ¶ï¼Œå€Ÿé‰´äººè„‘ç»“æ„å®ç°é«˜ä¿çœŸå®æ—¶æ¨ç†ã€‚äººç±»åˆ©ç”¨ä¸åŒè„‘åŒºè¿›è¡Œæ€ç»´å’Œååº”ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ç±»ä¼¼çš„äººè„‘é©±åŠ¨çš„åŒè„‘æ–¹æ³•ï¼Œé€šè¿‡â€œé…æ–¹è„‘â€è¿›è¡Œé«˜çº§æ¨ç†æ¥æ§åˆ¶å’Œå¼•å¯¼â€œå‘éŸ³è„‘â€è¿›è¡Œæµç•…çš„è¯­è¨€ç”Ÿæˆã€‚è¿™ç§åˆ†å·¥æ¶ˆé™¤äº†æ¨¡å¼åˆ‡æ¢ï¼Œä¿æŒäº†æ¨ç†è¿‡ç¨‹çš„å®Œæ•´æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒMPSæ˜¾è‘—ä¼˜äºç°æœ‰çš„è¾¹æ€è€ƒè¾¹è¯´è¯çš„æ–¹æ³•ï¼Œå…¶æ¨ç†æ€§èƒ½ä¸é¢„å…ˆè®¡ç®—å®Œæ•´CoTå†è¯´è¯çš„æ¨¡å‹ç›¸å½“ï¼ŒåŒæ—¶å¤§å¹…é™ä½äº†å»¶è¿Ÿã€‚åœ¨é›¶å»¶è¿Ÿé…ç½®ä¸‹ï¼Œè¯¥æ–¹æ³•åœ¨å£è¯­æ•°å­¦æ¨ç†ä»»åŠ¡Spoken-MQAä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°92.8%ï¼Œåœ¨è¯­éŸ³å¯¹è¯ä»»åŠ¡URO-Benchä¸Šçš„å¾—åˆ†ä¸º82.5ã€‚æœ¬ç ”ç©¶æœ‰æ•ˆåœ°ç¼©å°äº†é«˜è´¨é‡æ¨ç†å’Œå®æ—¶äº¤äº’ä¹‹é—´çš„å·®è·ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å®æ—¶å£è¯­æ¨¡å‹é¢ä¸´åˆ©ç”¨é“¾å¼æ€ç»´è¿›è¡Œæ¨ç†çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦é¡ºåºç”Ÿæˆæ•´ä¸ªæ€ç»´è¿‡ç¨‹ï¼Œå¯¼è‡´å»¶è¿Ÿè¿‡é•¿ã€‚</li>
<li>é¦–æ¬¡æå‡ºâ€œæ€ç»´é©±åŠ¨å£è¯­â€ï¼ˆMPSï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å€Ÿé‰´äººè„‘çš„è¿ä½œæ–¹å¼ä»¥å®ç°é«˜ä¿çœŸå®æ—¶æ¨ç†ã€‚</li>
<li>MPSæå‡ºäº†åŒè„‘æ–¹æ³•ï¼Œä½¿ç”¨â€œé…æ–¹è„‘â€è¿›è¡Œé«˜çº§æ¨ç†å’Œæ§åˆ¶ï¼ŒåŒæ—¶é€šè¿‡â€œå‘éŸ³è„‘â€è¿›è¡Œæµç•…çš„è¯­è¨€ç”Ÿæˆã€‚</li>
<li>è¿™ç§åˆ†å·¥èƒ½å¤Ÿæ¶ˆé™¤æ¨¡å¼åˆ‡æ¢ï¼Œä¿æŒæ¨ç†è¿‡ç¨‹çš„è¿ç»­æ€§ã€‚</li>
<li>MPSæ˜¾è‘—ä¼˜äºç°æœ‰çš„è¾¹æ€è€ƒè¾¹è¯´è¯çš„æ–¹æ³•ï¼Œå®ç°äº†ä¸é¢„å…ˆè®¡ç®—å®Œæ•´æ€ç»´é“¾çš„æ¨¡å‹ç›¸å½“çš„æ¨ç†æ€§èƒ½ã€‚</li>
<li>MPSé™ä½äº†å»¶è¿Ÿï¼Œæé«˜äº†å®æ—¶äº¤äº’çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-95c9447844baa6b05010ed335127c14d" align="middle">
<img src="https://picx.zhimg.com/v2-87d7e58618cccbded886639f34c14b66" align="middle">
<img src="https://picx.zhimg.com/v2-2980ba30e73f44115c784fe27f90728b" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Accent-Invariant-Automatic-Speech-Recognition-via-Saliency-Driven-Spectrogram-Masking"><a href="#Accent-Invariant-Automatic-Speech-Recognition-via-Saliency-Driven-Spectrogram-Masking" class="headerlink" title="Accent-Invariant Automatic Speech Recognition via Saliency-Driven   Spectrogram Masking"></a>Accent-Invariant Automatic Speech Recognition via Saliency-Driven   Spectrogram Masking</h2><p><strong>Authors:Mohammad Hossein Sameti, Sepehr Harfi Moridani, Ali Zarean, Hossein Sameti</strong></p>
<p>Pre-trained transformer-based models have significantly advanced automatic speech recognition (ASR), yet they remain sensitive to accent and dialectal variations, resulting in elevated word error rates (WER) in linguistically diverse languages such as English and Persian. To address this challenge, we propose an accent-invariant ASR framework that integrates accent and dialect classification into the recognition pipeline. Our approach involves training a spectrogram-based classifier to capture accent-specific cues, masking the regions most influential to its predictions, and using the masked spectrograms for data augmentation. This enhances the robustness of ASR models against accent variability. We evaluate the method using both English and Persian speech. For Persian, we introduce a newly collected dataset spanning multiple regional accents, establishing the first systematic benchmark for accent variation in Persian ASR that fills a critical gap in multilingual speech research and provides a foundation for future studies on low-resource, linguistically diverse languages. Experimental results with the Whisper model demonstrate that our masking and augmentation strategy yields substantial WER reductions in both English and Persian settings, confirming the effectiveness of the approach. This research advances the development of multilingual ASR systems that are resilient to accent and dialect diversity. Code and dataset are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/MH-Sameti/Accent_invariant_ASR">https://github.com/MH-Sameti/Accent_invariant_ASR</a> </p>
<blockquote>
<p>é¢„è®­ç»ƒåŸºäºtransformerçš„æ¨¡å‹å·²ç»æå¤§åœ°æ¨åŠ¨äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„è¿›æ­¥ï¼Œä½†å®ƒä»¬ä»ç„¶å¯¹å£éŸ³å’Œæ–¹è¨€å˜åŒ–æ•æ„Ÿï¼Œå¯¼è‡´åœ¨è‹±è¯­å’Œæ³¢æ–¯è¯­ç­‰è¯­è¨€å¤šæ ·åŒ–çš„è¯­è¨€ä¸­è¯é”™è¯¯ç‡ï¼ˆWERï¼‰ä¸Šå‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å£éŸ³ä¸å˜çš„ASRæ¡†æ¶ï¼Œå°†å£éŸ³å’Œæ–¹è¨€åˆ†ç±»æ•´åˆåˆ°è¯†åˆ«æµç¨‹ä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬è®­ç»ƒä¸€ä¸ªåŸºäºé¢‘è°±å›¾çš„åˆ†ç±»å™¨æ¥æ•æ‰å£éŸ³ç‰¹å®šçš„çº¿ç´¢ï¼Œæ©ç›–å¯¹å…¶é¢„æµ‹å½±å“æœ€å¤§çš„åŒºåŸŸï¼Œå¹¶ä½¿ç”¨æ©è”½çš„é¢‘è°±å›¾è¿›è¡Œæ•°æ®å¢å¼ºã€‚è¿™å¢å¼ºäº†ASRæ¨¡å‹å¯¹å£éŸ³å˜åŒ–çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬åŒæ—¶ä½¿ç”¨è‹±è¯­å’Œæ³¢æ–¯è¯­è¯­éŸ³å¯¹æ–¹æ³•è¿›è¡Œäº†è¯„ä¼°ã€‚å¯¹äºæ³¢æ–¯è¯­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°æ”¶é›†çš„æ•°æ®é›†ï¼Œæ¶µç›–å¤šç§åœ°åŒºå£éŸ³ï¼Œå»ºç«‹äº†æ³¢æ–¯è¯­ASRä¸­å£éŸ³å˜åŒ–çš„ç¬¬ä¸€å¥—ç³»ç»ŸåŸºå‡†æµ‹è¯•ï¼Œå¡«è¡¥äº†å¤šè¯­ç§è¯­éŸ³ç ”ç©¶ä¸­çš„ä¸€ä¸ªå…³é”®ç©ºç™½ï¼Œä¸ºæœªæ¥çš„ä½èµ„æºã€è¯­è¨€å¤šæ ·åŒ–è¯­è¨€çš„ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚ä½¿ç”¨Whisperæ¨¡å‹çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ©è”½å’Œå¢å¼ºç­–ç•¥åœ¨è‹±è¯­å’Œæ³¢æ–¯è¯­ç¯å¢ƒä¸­éƒ½å®ç°äº†æ˜¾è‘—çš„WERé™ä½ï¼Œè¯å®äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶æ¨åŠ¨äº†å¯¹å£éŸ³å’Œæ–¹è¨€å¤šæ ·æ€§æœ‰æŠµæŠ—åŠ›çš„å¤šè¯­ç§ASRç³»ç»Ÿçš„å¼€å‘ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è·å¾—ï¼š<a target="_blank" rel="noopener" href="https://github.com/MH-Sameti/Accent_invariant_ASR">https://github.com/MH-Sameti/Accent_invariant_ASR</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09528v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§é’ˆå¯¹å£éŸ³ä¸å˜é‡çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å°†å£éŸ³å’Œæ–¹è¨€åˆ†ç±»çº³å…¥è¯†åˆ«æµç¨‹æ¥è§£å†³å£éŸ³å’Œæ–¹è¨€å¤šæ ·æ€§å¯¹ASRçš„å½±å“ã€‚è¯¥ç ”ç©¶é€šè¿‡è®­ç»ƒåŸºäºé¢‘è°±å›¾çš„åˆ†ç±»å™¨æ¥æ•æ‰å£éŸ³ç‰¹å®šçº¿ç´¢ï¼Œå¹¶å¯¹é¢„æµ‹ä¸­æœ€é‡è¦çš„åŒºåŸŸè¿›è¡Œæ©ç ï¼Œä½¿ç”¨æ©ç é¢‘è°±å›¾è¿›è¡Œæ•°æ®å¢å¼ºï¼Œä»è€Œæé«˜ASRæ¨¡å‹å¯¹å£éŸ³å˜å¼‚çš„ç¨³å¥æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‹±è¯­å’Œæ³¢æ–¯è¯­è®¾ç½®ä¸­å‡å®ç°äº†æ˜¾è‘—çš„è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é¢„è®­ç»ƒåŸºäºè½¬æ¢å™¨çš„æ¨¡å‹åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­æœ‰æ˜¾è‘—è¿›æ­¥ï¼Œä½†ä»é¢ä¸´å£éŸ³å’Œæ–¹è¨€å˜åŒ–å¸¦æ¥çš„æŒ‘æˆ˜ã€‚</li>
<li>æå‡ºçš„å£éŸ³ä¸å˜ASRæ¡†æ¶é€šè¿‡å°†å£éŸ³å’Œæ–¹è¨€åˆ†ç±»çº³å…¥è¯†åˆ«æµç¨‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨åŸºäºé¢‘è°±å›¾çš„åˆ†ç±»å™¨æ•æ‰å£éŸ³ç‰¹å®šçº¿ç´¢ï¼Œå¹¶å¯¹å…³é”®åŒºåŸŸè¿›è¡Œæ©ç ä»¥è¿›è¡Œæ•°æ®å¢å¼ºã€‚</li>
<li>ç ”ç©¶æé«˜äº†ASRæ¨¡å‹å¯¹å£éŸ³å˜å¼‚çš„ç¨³å¥æ€§ï¼Œå¹¶åœ¨è‹±è¯­å’Œæ³¢æ–¯è¯­è¯„ä¼°ä¸­å–å¾—å®è´¨æ€§è¯é”™è¯¯ç‡ï¼ˆWERï¼‰é™ä½ã€‚</li>
<li>å¼•å…¥äº†ä¸€ä¸ªæ–°æ”¶é›†çš„æ¶µç›–å¤šç§åŒºåŸŸå£éŸ³çš„æ³¢æ–¯è¯­æ•°æ®é›†ï¼Œä¸ºä»¥åçš„ä½èµ„æºã€è¯­è¨€å¤šæ ·æ€§ç ”ç©¶æä¾›äº†åŸºç¡€ã€‚</li>
<li>å…¬å¼€å¯ç”¨çš„ä»£ç å’Œæ•°æ®é›†æœ‰åŠ©äºæ¨åŠ¨å¤šè¯­è¨€ASRç³»ç»Ÿçš„è¿›ä¸€æ­¥å‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09528">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-03b6fc1399d06bb9f16db02ab1187331" align="middle">
<img src="https://picx.zhimg.com/v2-daa56626c15f43c655239f8b4ce78f7d" align="middle">
<img src="https://picx.zhimg.com/v2-f89ec4ebcd58214c54a724ab175b14e0" align="middle">
<img src="https://picx.zhimg.com/v2-a71a14c2c0d2d9fca171a4670d865c8c" align="middle">
<img src="https://picx.zhimg.com/v2-87420e7170b0529d84808af59e8c94dd" align="middle">
<img src="https://picx.zhimg.com/v2-fa9ba71a3db0eef23640e96d639465d6" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="A-Study-of-the-Removability-of-Speaker-Adversarial-Perturbations"><a href="#A-Study-of-the-Removability-of-Speaker-Adversarial-Perturbations" class="headerlink" title="A Study of the Removability of Speaker-Adversarial Perturbations"></a>A Study of the Removability of Speaker-Adversarial Perturbations</h2><p><strong>Authors:Liping Chen, Chenyang Guo, Kong Aik Lee, Zhen-Hua Ling, Wu Guo</strong></p>
<p>Recent advancements in adversarial attacks have demonstrated their effectiveness in misleading speaker recognition models, making wrong predictions about speaker identities. On the other hand, defense techniques against speaker-adversarial attacks focus on reducing the effects of speaker-adversarial perturbations on speaker attribute extraction. These techniques do not seek to fully remove the perturbations and restore the original speech. To this end, this paper studies the removability of speaker-adversarial perturbations. Specifically, the investigation is conducted assuming various degrees of awareness of the perturbation generator across three scenarios: ignorant, semi-informed, and well-informed. Besides, we consider both the optimization-based and feedforward perturbation generation methods. Experiments conducted on the LibriSpeech dataset demonstrated that: 1) in the ignorant scenario, speaker-adversarial perturbations cannot be eliminated, although their impact on speaker attribute extraction is reduced, 2) in the semi-informed scenario, the speaker-adversarial perturbations cannot be fully removed, while those generated by the feedforward model can be considerably reduced, and 3) in the well-informed scenario, speaker-adversarial perturbations are nearly eliminated, allowing for the restoration of the original speech. Audio samples can be found in <a target="_blank" rel="noopener" href="https://voiceprivacy.github.io/Perturbation-Generation-Removal/">https://voiceprivacy.github.io/Perturbation-Generation-Removal/</a>. </p>
<blockquote>
<p>æœ€è¿‘å¯¹æŠ—æ€§æ”»å‡»çš„æœ€æ–°è¿›å±•è¡¨æ˜ï¼Œå®ƒä»¬åœ¨è¯¯å¯¼è¯´è¯äººè¯†åˆ«æ¨¡å‹ã€å¯¹è¯´è¯äººèº«ä»½åšå‡ºé”™è¯¯é¢„æµ‹æ–¹é¢éå¸¸æœ‰æ•ˆã€‚å¦ä¸€æ–¹é¢ï¼Œé’ˆå¯¹è¯´è¯äººå¯¹æŠ—æ€§æ”»å‡»çš„é˜²å¾¡æŠ€æœ¯ä¾§é‡äºå‡å°‘è¯´è¯äººå¯¹æŠ—æ€§æ‰°åŠ¨å¯¹è¯´è¯äººå±æ€§æå–çš„å½±å“ã€‚è¿™äº›æŠ€æœ¯å¹¶ä¸å¯»æ±‚å®Œå…¨æ¶ˆé™¤æ‰°åŠ¨å¹¶æ¢å¤åŸå§‹è¯­éŸ³ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡ç ”ç©¶äº†è¯´è¯äººå¯¹æŠ—æ€§æ‰°åŠ¨çš„å¯æ¶ˆé™¤æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œç ”ç©¶æ˜¯åœ¨å‡è®¾æ‰°åŠ¨ç”Ÿæˆå™¨åœ¨ä¸‰ç§åœºæ™¯ä¸‹çš„ä¸åŒæ„è¯†ç¨‹åº¦è¿›è¡Œçš„ï¼šæ— çŸ¥ã€åŠçŸ¥å’Œæ˜çŸ¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è€ƒè™‘äº†åŸºäºä¼˜åŒ–çš„å’Œå‰é¦ˆçš„æ‰°åŠ¨ç”Ÿæˆæ–¹æ³•ã€‚åœ¨LibriSpeechæ•°æ®é›†ä¸Šè¿›è¡Œçš„å®éªŒè¡¨æ˜ï¼š1ï¼‰åœ¨æ— çŸ¥çš„åœºæ™¯ä¸­ï¼Œæ— æ³•æ¶ˆé™¤è¯´è¯äººå¯¹æŠ—æ€§æ‰°åŠ¨ï¼Œè™½ç„¶å®ƒä»¬å¯¹è¯´è¯äººå±æ€§æå–çš„å½±å“æœ‰æ‰€å‡å°‘ï¼›2ï¼‰åœ¨åŠçŸ¥çš„åœºæ™¯ä¸­ï¼Œæ— æ³•å®Œå…¨æ¶ˆé™¤è¯´è¯äººå¯¹æŠ—æ€§æ‰°åŠ¨ï¼Œè€Œç”±å‰é¦ˆæ¨¡å‹ç”Ÿæˆçš„æ‰°åŠ¨å¯ä»¥å¤§å¤§å‡å°‘ï¼›3ï¼‰åœ¨æ˜çŸ¥çš„åœºæ™¯ä¸­ï¼Œè¯´è¯äººå¯¹æŠ—æ€§æ‰°åŠ¨å‡ ä¹è¢«æ¶ˆé™¤ï¼Œå¯ä»¥æ¢å¤åŸå§‹è¯­éŸ³ã€‚éŸ³é¢‘æ ·æœ¬å¯åœ¨<a target="_blank" rel="noopener" href="https://voiceprivacy.github.io/Perturbation-Generation-Removal/%E6%89%BE%E5%88%B0%E3%80%82">https://voiceprivacy.github.io/Perturbation-Generation-Removal/æ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09504v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå¯¹æŠ—æ€§æ”»å‡»å¯æœ‰æ•ˆè¯¯å¯¼è¯´è¯äººè¯†åˆ«æ¨¡å‹ï¼Œå¯¹å…¶åšå‡ºé”™è¯¯é¢„æµ‹ã€‚ä¸ºåº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡ä¸“æ³¨äºç ”ç©¶æ¶ˆé™¤è¯´è¯äººå¯¹æŠ—æ€§æ‰°åŠ¨çš„èƒ½åŠ›ã€‚ç ”ç©¶å‘ç°ä¸åŒåœºæ™¯ï¼ˆä¸çŸ¥æƒ…ã€åŠçŸ¥æƒ…å’ŒçŸ¥æƒ…ï¼‰ä»¥åŠä¸åŒçš„æ‰°åŠ¨ç”Ÿæˆæ–¹æ³•ï¼ˆä¼˜åŒ–æ³•å’Œå‰é¦ˆæ³•ï¼‰ä¸‹æ¶ˆé™¤å¯¹æŠ—æ€§æ‰°åŠ¨çš„éš¾æ˜“ç¨‹åº¦ä¸åŒã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ä¸çŸ¥æƒ…æƒ…å†µä¸‹æ— æ³•æ¶ˆé™¤æ‰°åŠ¨ï¼ŒåŠçŸ¥æƒ…æƒ…å†µä¸‹å¯ä»¥éƒ¨åˆ†å‡å°‘æ‰°åŠ¨ï¼Œå°¤å…¶æ˜¯å‰é¦ˆæ¨¡å‹ç”Ÿæˆçš„æ‰°åŠ¨ï¼Œè€Œåœ¨çŸ¥æƒ…æƒ…å†µä¸‹å‡ ä¹å¯ä»¥å®Œå…¨æ¶ˆé™¤æ‰°åŠ¨å¹¶æ¢å¤åŸå§‹è¯­éŸ³ã€‚å…·ä½“éŸ³é¢‘æ ·æœ¬å¯å‚è§ç›¸å…³ç½‘ç«™ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æŠ—æ€§æ”»å‡»å·²æ˜¾ç¤ºèƒ½å¤Ÿè¯¯å¯¼è¯´è¯äººè¯†åˆ«æ¨¡å‹ï¼Œå¼•å‘è¯¯åˆ¤ã€‚</li>
<li>ç ”ç©¶èšç„¦äºæ¶ˆé™¤è¯´è¯äººå¯¹æŠ—æ€§æ‰°åŠ¨çš„èƒ½åŠ›ã€‚</li>
<li>åœ¨ä¸åŒåœºæ™¯ï¼ˆä¸çŸ¥æƒ…ã€åŠçŸ¥æƒ…å’ŒçŸ¥æƒ…ï¼‰ä¸‹ï¼Œæ¶ˆé™¤å¯¹æŠ—æ€§æ‰°åŠ¨çš„éš¾åº¦ä¸åŒã€‚</li>
<li>ä¼˜åŒ–æ³•å’Œå‰é¦ˆæ³•æ˜¯ä¸¤ç§ä¸»è¦çš„æ‰°åŠ¨ç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>åœ¨ä¸çŸ¥æƒ…æƒ…å†µä¸‹ï¼Œæ— æ³•æ¶ˆé™¤è¯´è¯äººå¯¹æŠ—æ€§æ‰°åŠ¨ï¼Œä½†å…¶å¯¹è¯´è¯äººå±æ€§æå–çš„å½±å“å¯å‡è½»ã€‚</li>
<li>åœ¨åŠçŸ¥æƒ…æƒ…å†µä¸‹ï¼Œå¯ä»¥éƒ¨åˆ†å‡å°‘å¯¹æŠ—æ€§æ‰°åŠ¨ï¼Œç‰¹åˆ«æ˜¯å‰é¦ˆæ¨¡å‹ç”Ÿæˆçš„æ‰°åŠ¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09504">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e2e529d6331fceefc2ab20bce4a8425" align="middle">
<img src="https://picx.zhimg.com/v2-b0e1b2d422277f5bb5fd74e31f868866" align="middle">
<img src="https://picx.zhimg.com/v2-8f1851012be79de259d983a705b3e3ad" align="middle">
<img src="https://picx.zhimg.com/v2-f3e3fb315b779f4498e4b1e9d95eb99b" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="WildElder-A-Chinese-Elderly-Speech-Dataset-from-the-Wild-with-Fine-Grained-Manual-Annotations"><a href="#WildElder-A-Chinese-Elderly-Speech-Dataset-from-the-Wild-with-Fine-Grained-Manual-Annotations" class="headerlink" title="WildElder: A Chinese Elderly Speech Dataset from the Wild with   Fine-Grained Manual Annotations"></a>WildElder: A Chinese Elderly Speech Dataset from the Wild with   Fine-Grained Manual Annotations</h2><p><strong>Authors:Hui Wang, Jiaming Zhou, Jiabei He, Haoqin Sun, Yong Qin</strong></p>
<p>Elderly speech poses unique challenges for automatic processing due to age-related changes such as slower articulation and vocal tremors. Existing Chinese datasets are mostly recorded in controlled environments, limiting their diversity and real-world applicability. To address this gap, we present WildElder, a Mandarin elderly speech corpus collected from online videos and enriched with fine-grained manual annotations, including transcription, speaker age, gender, and accent strength. Combining the realism of in-the-wild data with expert curation, WildElder enables robust research on automatic speech recognition and speaker profiling. Experimental results reveal both the difficulties of elderly speech recognition and the potential of WildElder as a challenging new benchmark. The dataset and code are available at <a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/WildElder">https://github.com/NKU-HLT/WildElder</a>. </p>
<blockquote>
<p>è€å¹´è¯­éŸ³ç”±äºä¸å¹´é¾„ç›¸å…³çš„å˜åŒ–ï¼ˆå¦‚å‘éŸ³è¾ƒæ…¢å’Œå—“éŸ³é¢¤æŠ–ï¼‰è€Œå¯¹è‡ªåŠ¨å¤„ç†æå‡ºäº†ç‹¬ç‰¹æŒ‘æˆ˜ã€‚ç°æœ‰çš„ä¸­æ–‡æ•°æ®é›†å¤§å¤šæ˜¯åœ¨å—æ§ç¯å¢ƒä¸­è®°å½•çš„ï¼Œè¿™é™åˆ¶äº†å…¶å¤šæ ·æ€§å’Œåœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†WildElderï¼Œè¿™æ˜¯ä¸€ä¸ªä»åœ¨çº¿è§†é¢‘æ”¶é›†çš„æ™®é€šè¯è€å¹´è¯­éŸ³è¯­æ–™åº“ï¼Œé€šè¿‡ç²¾ç»†çš„æ‰‹åŠ¨æ³¨é‡Šè¿›è¡Œäº†ä¸°å¯Œï¼ŒåŒ…æ‹¬è½¬å½•ã€è¯´è¯äººçš„å¹´é¾„ã€æ€§åˆ«å’Œå£éŸ³å¼ºåº¦ã€‚ç»“åˆäº†é‡ç”Ÿæ•°æ®çš„çœŸå®æ€§å’Œä¸“å®¶æ•´ç†ï¼ŒWildElderä¸ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯´è¯äººåˆ†ææä¾›äº†ç¨³å¥çš„ç ”ç©¶æ”¯æŒã€‚å®éªŒç»“æœè¡¨æ˜è€å¹´è¯­éŸ³è¯†åˆ«çš„éš¾åº¦ä»¥åŠWildElderä½œä¸ºä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ–°åŸºå‡†çš„æ½œåŠ›ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/WildElder%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/NKU-HLT/WildElderæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09344v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†è€å¹´è¯­éŸ³å¯¹äºè‡ªåŠ¨å¤„ç†æ‰€é¢ä¸´çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å‘éŸ³è¾ƒæ…¢å’Œå—“éŸ³é¢¤æŠ–ç­‰å¹´é¾„ç›¸å…³å˜åŒ–ã€‚ç°æœ‰çš„ä¸­æ–‡æ•°æ®é›†å¤§å¤šåœ¨å—æ§ç¯å¢ƒä¸­å½•åˆ¶ï¼Œé™åˆ¶äº†å…¶å¤šæ ·æ€§å’Œç°å®ä¸–ç•Œçš„é€‚ç”¨æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†WildElderï¼Œä¸€ä¸ªä»åœ¨çº¿è§†é¢‘æ”¶é›†å¹¶ç”¨ç²¾ç»†æ‰‹åŠ¨æ³¨é‡Šä¸°å¯Œçš„æ™®é€šè¯è€å¹´è¯­éŸ³è¯­æ–™åº“ï¼ŒåŒ…æ‹¬è½¬å½•ã€è¯´è¯äººå¹´é¾„ã€æ€§åˆ«å’Œå£éŸ³å¼ºåº¦ç­‰ã€‚ç»“åˆäº†é‡å¤–æ•°æ®çš„çœŸå®æ€§å’Œä¸“å®¶ç®¡ç†ï¼ŒWildElderä¸ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯´è¯äººåˆ†ææä¾›äº†ç¨³å¥çš„ç ”ç©¶æ”¯æŒã€‚å®éªŒç»“æœè¡¨æ˜è€å¹´è¯­éŸ³è¯†åˆ«çš„éš¾åº¦ä»¥åŠWildElderä½œä¸ºå…·æœ‰æŒ‘æˆ˜æ€§çš„æ–°åŸºå‡†çš„æ½œåŠ›ã€‚æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/NKU-HLT/WildElder%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/NKU-HLT/WildElderè·å–ã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è€å¹´è¯­éŸ³ç”±äºå¹´é¾„ç›¸å…³çš„å˜åŒ–ï¼ˆå¦‚å‘éŸ³è¾ƒæ…¢å’Œå—“éŸ³é¢¤æŠ–ï¼‰ç»™è‡ªåŠ¨å¤„ç†å¸¦æ¥ç‹¬ç‰¹æŒ‘æˆ˜ã€‚</li>
<li>ç°æœ‰ä¸­æ–‡æ•°æ®é›†ä¸»è¦æ¥æºäºå—æ§ç¯å¢ƒï¼Œé™åˆ¶äº†å…¶åœ¨çœŸå®ä¸–ç•Œçš„å¤šæ ·æ€§å’Œé€‚ç”¨æ€§ã€‚</li>
<li>WildElderæ˜¯é¦–ä¸ªä»åœ¨çº¿è§†é¢‘æ”¶é›†çš„æ™®é€šè¯è€å¹´è¯­éŸ³è¯­æ–™åº“ï¼ŒåŒ…å«è¯¦ç»†çš„æ³¨é‡Šï¼Œå¦‚è½¬å½•ã€è¯´è¯äººå¹´é¾„ã€æ€§åˆ«å’Œå£éŸ³å¼ºåº¦ç­‰ã€‚</li>
<li>WildElderç»“åˆäº†é‡å¤–æ•°æ®çš„çœŸå®æ€§å’Œä¸“å®¶ç®¡ç†ï¼Œä¸ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«å’Œè¯´è¯äººåˆ†ææä¾›äº†ç¨³å¥çš„ç ”ç©¶æ”¯æŒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜è€å¹´è¯­éŸ³è¯†åˆ«çš„éš¾åº¦è¾ƒé«˜ã€‚</li>
<li>WildElderæ•°æ®é›†æä¾›äº†ä¸€ä¸ªæŒ‘æˆ˜æ€§çš„æ–°åŸºå‡†ï¼Œæœ‰åŠ©äºæ¨åŠ¨è€å¹´è¯­éŸ³è¯†åˆ«çš„ç ”ç©¶è¿›æ­¥ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09344">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e934a7434ca5f4e2ff77def973264e9c" align="middle">
<img src="https://picx.zhimg.com/v2-5c771d380c76e46df57d5595f762a38e" align="middle">
<img src="https://picx.zhimg.com/v2-3375c54cf6dea775e0392ee8c99d57b3" align="middle">
<img src="https://picx.zhimg.com/v2-6268f693f1fa8b76f4edc709c8c22c27" align="middle">
<img src="https://picx.zhimg.com/v2-e1a79692fa1891f8383e47868c11bc13" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="O-O-VC-Synthetic-Data-Driven-One-to-One-Alignment-for-Any-to-Any-Voice-Conversion"><a href="#O-O-VC-Synthetic-Data-Driven-One-to-One-Alignment-for-Any-to-Any-Voice-Conversion" class="headerlink" title="O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice   Conversion"></a>O_O-VC: Synthetic Data-Driven One-to-One Alignment for Any-to-Any Voice   Conversion</h2><p><strong>Authors:Huu Tuong Tu, Huan Vu, cuong tien nguyen, Dien Hy Ngo, Nguyen Thi Thu Trang</strong></p>
<p>Traditional voice conversion (VC) methods typically attempt to separate speaker identity and linguistic information into distinct representations, which are then combined to reconstruct the audio. However, effectively disentangling these factors remains challenging, often leading to information loss during training. In this paper, we propose a new approach that leverages synthetic speech data generated by a high-quality, pretrained multispeaker text-to-speech (TTS) model. Specifically, synthetic data pairs that share the same linguistic content but differ in speaker identity are used as input-output pairs to train the voice conversion model. This enables the model to learn a direct mapping between source and target voices, effectively capturing speaker-specific characteristics while preserving linguistic content. Additionally, we introduce a flexible training strategy for any-to-any voice conversion that generalizes well to unseen speakers and new languages, enhancing adaptability and performance in zero-shot scenarios. Our experiments show that our proposed method achieves a 16.35% relative reduction in word error rate and a 5.91% improvement in speaker cosine similarity, outperforming several state-of-the-art methods. Voice conversion samples can be accessed at: <a target="_blank" rel="noopener" href="https://oovc-emnlp-2025.github.io/">https://oovc-emnlp-2025.github.io/</a> </p>
<blockquote>
<p>ä¼ ç»Ÿè¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰æ–¹æ³•é€šå¸¸å°è¯•å°†è¯´è¯äººèº«ä»½å’Œè¯­è¨€ä¿¡æ¯åˆ†ç¦»æˆä¸åŒçš„è¡¨ç¤ºï¼Œç„¶åå°†å®ƒä»¬ç»“åˆèµ·æ¥é‡å»ºéŸ³é¢‘ã€‚ç„¶è€Œï¼Œæœ‰æ•ˆåœ°è§£å¼€è¿™äº›å› ç´ ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œé€šå¸¸ä¼šå¯¼è‡´è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¿¡æ¯ä¸¢å¤±ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ç”±é«˜è´¨é‡ã€é¢„è®­ç»ƒçš„å¤šè¯´è¯äººæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ç”Ÿæˆçš„äººé€ è¯­éŸ³æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œäººé€ æ•°æ®å¯¹å…±äº«ç›¸åŒè¯­è¨€å†…å®¹ä½†åœ¨è¯´è¯äººèº«ä»½ä¸Šæœ‰æ‰€ä¸åŒï¼Œè¢«ç”¨ä½œè¾“å…¥å’Œè¾“å‡ºæ¥è®­ç»ƒè¯­éŸ³è½¬æ¢æ¨¡å‹ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æºè¯­éŸ³å’Œç›®æ ‡è¯­éŸ³ä¹‹é—´çš„ç›´æ¥æ˜ å°„ï¼Œæœ‰æ•ˆåœ°æ•æ‰è¯´è¯äººçš„ç‰¹å®šç‰¹å¾ï¼ŒåŒæ—¶ä¿ç•™è¯­è¨€å†…å®¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§çµæ´»çš„ä»»ä½•åˆ°ä»»ä½•è¯­éŸ³è½¬æ¢è®­ç»ƒç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯ä»¥å¾ˆå¥½åœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„è¯´è¯äººå’Œæ–°è¯­è¨€ï¼Œæé«˜äº†é›¶æ ·æœ¬åœºæ™¯ä¸­çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•å®ç°äº†å•è¯é”™è¯¯ç‡é™ä½16.35%ï¼Œè¯´è¯äººä½™å¼¦ç›¸ä¼¼æ€§æé«˜5.91%ï¼Œä¼˜äºå‡ ç§æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚è¯­éŸ³è½¬æ¢æ ·æœ¬å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://oovc-emnlp-2025.github.io/">https://oovc-emnlp-2025.github.io/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09061v1">PDF</a> EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯­éŸ³è½¬æ¢æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨é«˜è´¨é‡é¢„è®­ç»ƒå¤šè¯´è¯äººæ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹ç”Ÿæˆåˆæˆè¯­éŸ³æ•°æ®ã€‚é€šè¿‡åˆ©ç”¨å…·æœ‰ç›¸åŒè¯­è¨€å†…å®¹ä½†è¯´è¯äººèº«ä»½ä¸åŒçš„åˆæˆæ•°æ®å¯¹ä½œä¸ºè¾“å…¥-è¾“å‡ºå¯¹ï¼Œè®­ç»ƒè¯­éŸ³è½¬æ¢æ¨¡å‹ï¼Œå®ç°äº†å¯¹æºè¯­éŸ³å’Œç›®æ ‡è¯­éŸ³çš„ç›´æ¥æ˜ å°„ï¼Œæœ‰æ•ˆæ•æ‰è¯´è¯äººç‰¹å¾çš„åŒæ—¶ä¿ç•™äº†è¯­è¨€å†…å®¹ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§çµæ´»çš„ä»»ä½•åˆ°ä»»ä½•ï¼ˆany-to-anyï¼‰è¯­éŸ³è½¬æ¢è®­ç»ƒç­–ç•¥ï¼Œå¯å¾ˆå¥½åœ°æ¨å¹¿åˆ°æœªè§è¿‡çš„è¯´è¯äººå’Œæ–°è¯­è¨€ï¼Œæé«˜äº†é›¶æ ·æœ¬åœºæ™¯ä¸‹çš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è¯é”™è¯¯ç‡ä¸Šç›¸å¯¹é™ä½äº†16.35%ï¼Œåœ¨è¯´è¯äººä½™å¼¦ç›¸ä¼¼æ€§ä¸Šæé«˜äº†5.91%ï¼Œä¼˜äºå¤šç§æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ–°æ–¹æ³•åˆ©ç”¨é¢„è®­ç»ƒçš„å¤šè¯´è¯äººTTSæ¨¡å‹ç”Ÿæˆåˆæˆè¯­éŸ³æ•°æ®ï¼Œç”¨äºè®­ç»ƒè¯­éŸ³è½¬æ¢æ¨¡å‹ã€‚</li>
<li>æ–¹æ³•å®ç°äº†æºè¯­éŸ³å’Œç›®æ ‡è¯­éŸ³çš„ç›´æ¥æ˜ å°„ï¼Œæœ‰æ•ˆåˆ†ç¦»è¯´è¯äººèº«ä»½å’Œè¯­è¨€ä¿¡æ¯ã€‚</li>
<li>çµæ´»çš„ä»»ä½•åˆ°ä»»ä½•ï¼ˆany-to-anyï¼‰è¯­éŸ³è½¬æ¢è®­ç»ƒç­–ç•¥ï¼Œé€‚åº”äºæœªè§è¿‡çš„è¯´è¯äººå’Œæ–°è¯­è¨€ã€‚</li>
<li>æ–¹æ³•åœ¨è¯é”™è¯¯ç‡å’Œè¯´è¯äººä½™å¼¦ç›¸ä¼¼æ€§ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ï¼Œä¼˜äºç°æœ‰å…ˆè¿›æ–¹æ³•ã€‚</li>
<li>è¯¥æ–¹æ³•æé«˜äº†è¯­éŸ³è½¬æ¢æ¨¡å‹çš„é€‚åº”æ€§å’Œæ€§èƒ½ï¼Œå°¤å…¶åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹ã€‚</li>
<li>åˆæˆè¯­éŸ³æ•°æ®å¯¹ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09061">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a0bc629c568b5fbfe0fbf155a09bd4f0" align="middle">
<img src="https://picx.zhimg.com/v2-df7811b536b3398043b4121e16997782" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Model-Based-Ranking-of-Source-Languages-for-Zero-Shot-Cross-Lingual-Transfer"><a href="#Model-Based-Ranking-of-Source-Languages-for-Zero-Shot-Cross-Lingual-Transfer" class="headerlink" title="Model-Based Ranking of Source Languages for Zero-Shot Cross-Lingual   Transfer"></a>Model-Based Ranking of Source Languages for Zero-Shot Cross-Lingual   Transfer</h2><p><strong>Authors:Abteen Ebrahimi, Adam Wiemerslage, Katharina von der Wense</strong></p>
<p>We present NN-Rank, an algorithm for ranking source languages for cross-lingual transfer, which leverages hidden representations from multilingual models and unlabeled target-language data. We experiment with two pretrained multilingual models and two tasks: part-of-speech tagging (POS) and named entity recognition (NER). We consider 51 source languages and evaluate on 56 and 72 target languages for POS and NER, respectively. When using in-domain data, NN-Rank beats state-of-the-art baselines that leverage lexical and linguistic features, with average improvements of up to 35.56 NDCG for POS and 18.14 NDCG for NER. As prior approaches can fall back to language-level features if target language data is not available, we show that NN-Rank remains competitive using only the Bible, an out-of-domain corpus available for a large number of languages. Ablations on the amount of unlabeled target data show that, for subsets consisting of as few as 25 examples, NN-Rank produces high-quality rankings which achieve 92.8% of the NDCG achieved using all available target data for ranking. </p>
<blockquote>
<p>æˆ‘ä»¬æå‡ºäº†NN-Rankç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºè·¨è¯­è¨€è¿ç§»çš„æºè¯­è¨€æ’åç®—æ³•ï¼Œå®ƒåˆ©ç”¨å¤šè¯­è¨€æ¨¡å‹çš„éšè—è¡¨ç¤ºå’Œæœªæ ‡è®°çš„ç›®æ ‡è¯­è¨€æ•°æ®ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªé¢„è®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹å’Œä¸¤ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œå®éªŒï¼šè¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰å’Œå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€‚æˆ‘ä»¬è€ƒè™‘äº†51ç§æºè¯­è¨€ï¼Œå¹¶åœ¨POSå’ŒNERä¸Šåˆ†åˆ«å¯¹56ç§å’Œ72ç§ç›®æ ‡è¯­è¨€è¿›è¡Œè¯„ä¼°ã€‚åœ¨ä½¿ç”¨é¢†åŸŸæ•°æ®çš„æƒ…å†µä¸‹ï¼ŒNN-Rankå‡»è´¥äº†åˆ©ç”¨è¯æ±‡å’Œè¯­è¨€ç‰¹å¾çš„æœ€æ–°åŸºçº¿ï¼Œå¹³å‡æ”¹è¿›äº†é«˜è¾¾35.56 NDCGï¼ˆå¯¹äºPOSï¼‰å’Œ18.14 NDCGï¼ˆå¯¹äºNERï¼‰ã€‚ç”±äºå…ˆå‰çš„æ–¹æ³•åœ¨ç›®æ ‡è¯­è¨€æ•°æ®ä¸å¯ç”¨æ—¶ä¼šå›è½åˆ°è¯­è¨€çº§ç‰¹å¾ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œä»…ä½¿ç”¨åœ£ç»ï¼ˆä¸€ä¸ªå¯ç”¨äºå¤šç§è¯­è¨€çš„åŸŸå¤–è¯­æ–™åº“ï¼‰æ—¶ï¼ŒNN-Rankä»å…·æœ‰ç«äº‰åŠ›ã€‚å¯¹æœªæ ‡è®°ç›®æ ‡æ•°æ®é‡çš„æ¶ˆèå®éªŒè¡¨æ˜ï¼Œå¯¹äºä»…åŒ…å«25ä¸ªä¾‹å­çš„å­é›†ï¼ŒNN-Rankèƒ½å¤Ÿäº§ç”Ÿé«˜è´¨é‡æ’åï¼Œå®ç°ä½¿ç”¨æ‰€æœ‰å¯ç”¨ç›®æ ‡æ•°æ®è¿›è¡Œæ’åæ—¶çš„92.8% NDCGã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03202v2">PDF</a> Accepted to EMNLP 2025 (Main)</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†NN-Rankç®—æ³•ï¼Œè¯¥ç®—æ³•ç”¨äºè·¨è¯­è¨€è½¬ç§»å­¦ä¹ ä¸­æºè¯­è¨€çš„æ’åã€‚å®ƒåˆ©ç”¨å¤šè¯­è¨€æ¨¡å‹çš„éšè—è¡¨ç¤ºå’Œæœªæ ‡è®°çš„ç›®æ ‡è¯­è¨€æ•°æ®è¿›è¡Œæ’åã€‚å®éªŒé‡‡ç”¨ä¸¤ç§é¢„è®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹å’Œä¸¤ç§ä»»åŠ¡ï¼šè¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰å’Œå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€‚è€ƒè™‘51ç§æºè¯­è¨€ï¼Œåœ¨POSå’ŒNERä¸Šåˆ†åˆ«å¯¹56ç§å’Œ72ç§ç›®æ ‡è¯­è¨€è¿›è¡Œè¯„ä¼°ã€‚ä½¿ç”¨é¢†åŸŸæ•°æ®æ—¶ï¼ŒNN-Rankä¼˜äºåˆ©ç”¨è¯æ±‡å’Œè¯­è¨€ç‰¹å¾çš„æœ€æ–°åŸºçº¿ï¼ŒPOSå’ŒNERçš„å¹³å‡æ”¹è¿›åˆ†åˆ«é«˜è¾¾35.56 NDCGå’Œ18.14 NDCGã€‚å³ä½¿åœ¨ç›®æ ‡è¯­è¨€æ•°æ®ä¸å¯ç”¨çš„æƒ…å†µä¸‹ï¼ŒNN-Rankä»å…·æœ‰ç«äº‰åŠ›ï¼Œä»…ä½¿ç”¨å¤§é‡è¯­è¨€å…±æœ‰çš„ã€Šåœ£ç»ã€‹è¯­æ–™åº“ã€‚å…³äºç›®æ ‡è¯­è¨€æ— æ ‡ç­¾æ•°æ®é‡çš„æ¶ˆèç ”ç©¶è¡¨æ˜ï¼Œå³ä½¿åªæœ‰25ä¸ªç¤ºä¾‹ï¼ŒNN-Rankä¹Ÿèƒ½äº§ç”Ÿé«˜è´¨é‡æ’åï¼Œå®ç°ä½¿ç”¨æ‰€æœ‰ç›®æ ‡æ•°æ®æ’åçš„92.8%çš„NDCGã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NN-Rankæ˜¯ä¸€ç§ç”¨äºè·¨è¯­è¨€è½¬ç§»å­¦ä¹ çš„æºè¯­è¨€æ’åç®—æ³•ã€‚</li>
<li>è¯¥ç®—æ³•åˆ©ç”¨å¤šè¯­è¨€æ¨¡å‹çš„éšè—è¡¨ç¤ºå’Œæœªæ ‡è®°çš„ç›®æ ‡è¯­è¨€æ•°æ®ã€‚</li>
<li>å®éªŒæ¶‰åŠä¸¤ç§é¢„è®­ç»ƒçš„å¤šè¯­è¨€æ¨¡å‹å’Œä¸¤ç§ä»»åŠ¡ï¼šè¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰å’Œå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€‚</li>
<li>NN-Rankåœ¨å¤šç§æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€ä¸Šçš„è¡¨ç°è¶…è¶Šäº†ç°æœ‰åŸºçº¿ã€‚</li>
<li>ä½¿ç”¨é¢†åŸŸæ•°æ®æ—¶ï¼ŒNN-Rankçš„è¡¨ç°å°¤å…¶å‡ºè‰²ï¼Œå¹³å‡æ”¹è¿›æ˜¾è‘—ã€‚</li>
<li>å³ä½¿åœ¨æ²¡æœ‰ç›®æ ‡è¯­è¨€æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒNN-Rankä»å…·æœ‰ç«äº‰åŠ›ï¼Œè¿™å½’åŠŸäºå…¶åˆ©ç”¨æ™®éå¯ç”¨çš„è¯­æ–™åº“ï¼ˆå¦‚ã€Šåœ£ç»ã€‹ï¼‰çš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03202">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20a499b8ac54ed8f1acd428618799a35" align="middle">
<img src="https://picx.zhimg.com/v2-9b7529cf0cae32d6908f152fb6a1cf14" align="middle">
<img src="https://picx.zhimg.com/v2-31511db777cb9ac7cb8a38c0b40fba6d" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="From-Coarse-to-Fine-Recursive-Audio-Visual-Semantic-Enhancement-for-Speech-Separation"><a href="#From-Coarse-to-Fine-Recursive-Audio-Visual-Semantic-Enhancement-for-Speech-Separation" class="headerlink" title="From Coarse to Fine: Recursive Audio-Visual Semantic Enhancement for   Speech Separation"></a>From Coarse to Fine: Recursive Audio-Visual Semantic Enhancement for   Speech Separation</h2><p><strong>Authors:Ke Xue, Rongfei Fan,  Lixin, Dawei Zhao, Chao Zhu, Han Hu</strong></p>
<p>Audio-visual speech separation aims to isolate each speakerâ€™s clean voice from mixtures by leveraging visual cues such as lip movements and facial features. While visual information provides complementary semantic guidance, existing methods often underexploit its potential by relying on static visual representations. In this paper, we propose CSFNet, a Coarse-to-Separate-Fine Network that introduces a recursive semantic enhancement paradigm for more effective separation. CSFNet operates in two stages: (1) Coarse Separation, where a first-pass estimation reconstructs a coarse audio waveform from the mixture and visual input; and (2) Fine Separation, where the coarse audio is fed back into an audio-visual speech recognition (AVSR) model together with the visual stream. This recursive process produces more discriminative semantic representations, which are then used to extract refined audio. To further exploit these semantics, we design a speaker-aware perceptual fusion block to encode speaker identity across modalities, and a multi-range spectro-temporal separation network to capture both local and global time-frequency patterns. Extensive experiments on three benchmark datasets and two noisy datasets show that CSFNet achieves state-of-the-art (SOTA) performance, with substantial coarse-to-fine improvements, validating the necessity and effectiveness of our recursive semantic enhancement framework. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³åˆ†ç¦»æ—¨åœ¨åˆ©ç”¨è§†è§‰çº¿ç´¢ï¼ˆå¦‚å˜´å”‡åŠ¨ä½œå’Œé¢éƒ¨ç‰¹å¾ï¼‰æ¥ä»æ··åˆå£°éŸ³ä¸­éš”ç¦»æ¯ä¸ªè¯´è¯äººçš„æ¸…æ™°å£°éŸ³ã€‚è™½ç„¶è§†è§‰ä¿¡æ¯æä¾›äº†è¡¥å……è¯­ä¹‰æŒ‡å¯¼ï¼Œä½†ç°æœ‰æ–¹æ³•å¸¸å¸¸é€šè¿‡ä¾èµ–é™æ€è§†è§‰è¡¨ç¤ºè€Œæœªèƒ½å……åˆ†åˆ©ç”¨å…¶æ½œåŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CSFNetï¼Œè¿™æ˜¯ä¸€ç§ä»ç²—ç•¥åˆ°åˆ†ç¦»çš„ç²¾ç»†ç½‘ç»œï¼Œå®ƒå¼•å…¥äº†ä¸€ç§é€’å½’è¯­ä¹‰å¢å¼ºèŒƒå¼ï¼Œä»¥å®ç°æ›´æœ‰æ•ˆçš„åˆ†ç¦»ã€‚CSFNetåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š1ï¼‰ç²—ç•¥åˆ†ç¦»ï¼Œç¬¬ä¸€é˜¶æ®µä¼°è®¡ä»æ··åˆå£°éŸ³å’Œè§†è§‰è¾“å…¥é‡å»ºå‡ºç²—ç•¥çš„éŸ³é¢‘æ³¢å½¢ï¼›2ï¼‰ç²¾ç»†åˆ†ç¦»ï¼Œå°†ç²—ç•¥çš„éŸ³é¢‘åé¦ˆåˆ°è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰æ¨¡å‹ä¸­ä¸è§†è§‰æµä¸€èµ·ã€‚è¿™ç§é€’å½’è¿‡ç¨‹äº§ç”Ÿäº†æ›´å…·åŒºåˆ†æ€§çš„è¯­ä¹‰è¡¨ç¤ºï¼Œç„¶åç”¨äºæå–ç²¾ç‚¼çš„éŸ³é¢‘ã€‚ä¸ºäº†è¿›ä¸€æ­¥åˆ©ç”¨è¿™äº›è¯­ä¹‰ä¿¡æ¯ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªè¯´è¯äººæ„ŸçŸ¥èåˆå—ï¼Œä»¥ç¼–ç è·¨æ¨¡æ€çš„è¯´è¯äººèº«ä»½ï¼Œä»¥åŠä¸€ä¸ªå¤šèŒƒå›´å…‰è°±æ—¶é—´åˆ†ç¦»ç½‘ç»œï¼Œä»¥æ•è·å±€éƒ¨å’Œå…¨å±€æ—¶é—´é¢‘ç‡æ¨¡å¼ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†å’Œä¸¤ä¸ªå™ªå£°æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCSFNetè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå®ç°äº†ä»ç²—ç•¥åˆ°ç²¾ç»†çš„æ˜¾è‘—æ”¹è¿›ï¼ŒéªŒè¯äº†æˆ‘ä»¬çš„é€’å½’è¯­ä¹‰å¢å¼ºæ¡†æ¶çš„å¿…è¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.22425v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡ä»‹ç»äº†éŸ³é¢‘è§†è§‰è¯­éŸ³åˆ†ç¦»æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åˆ©ç”¨è§†è§‰çº¿ç´¢ï¼ˆå¦‚å˜´å”‡åŠ¨ä½œå’Œé¢éƒ¨ç‰¹å¾ï¼‰æ¥åˆ†ç¦»æ··åˆå£°éŸ³ä¸­çš„æ¯ä¸ªè¯´è¯äººçš„æ¸…æ™°å£°éŸ³ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•æœªå……åˆ†åˆ©ç”¨è§†è§‰ä¿¡æ¯çš„æ½œåŠ›ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºCSFNetçš„ç²—ç»†åˆ†ç¦»ç½‘ç»œï¼Œé€šè¿‡é€’å½’è¯­ä¹‰å¢å¼ºèŒƒå¼å®ç°æ›´æœ‰æ•ˆçš„åˆ†ç¦»ã€‚CSFNetåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç²—åˆ†ç¦»å’Œç²¾ç»†åˆ†ç¦»ã€‚åœ¨ç²—åˆ†ç¦»é˜¶æ®µï¼Œé€šè¿‡åˆæ­¥ä¼°è®¡ä»æ··åˆå£°éŸ³å’Œè§†è§‰è¾“å…¥é‡å»ºä¸€ä¸ªç²—ç•¥çš„éŸ³é¢‘æ³¢å½¢ï¼›åœ¨ç²¾ç»†åˆ†ç¦»é˜¶æ®µï¼Œå°†ç²—ç•¥çš„éŸ³é¢‘åé¦ˆåˆ°è§†å¬è¯­éŸ³è¯†åˆ«ï¼ˆAVSRï¼‰æ¨¡å‹ï¼Œä¸è§†è§‰æµä¸€èµ·å¤„ç†ã€‚è¿™ä¸ªè¿‡ç¨‹äº§ç”Ÿäº†æ›´å…·è¾¨è¯†åº¦çš„è¯­ä¹‰è¡¨ç¤ºï¼Œç„¶åç”¨äºæå–ç²¾ç‚¼çš„éŸ³é¢‘ã€‚ä¸ºå……åˆ†åˆ©ç”¨è¿™äº›è¯­ä¹‰ä¿¡æ¯ï¼Œè®¾è®¡äº†ä¸€ä¸ªæ„ŸçŸ¥èåˆæ¨¡å—æ¥ç¼–ç è·¨æ¨¡æ€çš„è¯´è¯äººèº«ä»½ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªå¤šèŒƒå›´çš„æ—¶é—´å’Œé¢‘ç‡åˆ†ç¦»ç½‘ç»œæ¥æ•æ‰å±€éƒ¨å’Œå…¨å±€çš„æ—¶é—´é¢‘ç‡æ¨¡å¼ã€‚å®éªŒè¡¨æ˜ï¼ŒCSFNetåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒéªŒè¯äº†é€’å½’è¯­ä¹‰å¢å¼ºæ¡†æ¶çš„å¿…è¦æ€§å’Œæœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>éŸ³é¢‘è§†è§‰è¯­éŸ³åˆ†ç¦»æŠ€æœ¯åˆ©ç”¨è§†è§‰çº¿ç´¢æ¥åˆ†ç¦»æ··åˆå£°éŸ³ä¸­çš„æ¯ä¸ªè¯´è¯äººçš„å£°éŸ³ã€‚</li>
<li>CSFNetç½‘ç»œåˆ†ä¸ºç²—åˆ†ç¦»å’Œç²¾ç»†åˆ†ç¦»ä¸¤ä¸ªé˜¶æ®µï¼Œé€šè¿‡é€’å½’è¯­ä¹‰å¢å¼ºå®ç°æœ‰æ•ˆåˆ†ç¦»ã€‚</li>
<li>ç²—åˆ†ç¦»é˜¶æ®µé€šè¿‡åˆæ­¥ä¼°è®¡ä»æ··åˆå£°éŸ³å’Œè§†è§‰è¾“å…¥é‡å»ºç²—ç•¥éŸ³é¢‘æ³¢å½¢ã€‚</li>
<li>ç²¾ç»†åˆ†ç¦»é˜¶æ®µå°†ç²—ç•¥éŸ³é¢‘ä¸è§†è§‰æµä¸€èµ·åé¦ˆåˆ°AVSRæ¨¡å‹ï¼Œäº§ç”Ÿæ›´å…·è¾¨è¯†åº¦çš„è¯­ä¹‰è¡¨ç¤ºã€‚</li>
<li>æå‡ºäº†ä¸€ä¸ªæ„ŸçŸ¥èåˆæ¨¡å—æ¥ç¼–ç è·¨æ¨¡æ€çš„è¯´è¯äººèº«ä»½ã€‚</li>
<li>å¤šèŒƒå›´çš„æ—¶é—´å’Œé¢‘ç‡åˆ†ç¦»ç½‘ç»œèƒ½å¤Ÿæ•æ‰å±€éƒ¨å’Œå…¨å±€çš„æ—¶é—´é¢‘ç‡æ¨¡å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6ced7f2d48d16e2466bfe9fcf827a3f5" align="middle">
<img src="https://picx.zhimg.com/v2-2e711a0b96171eb89eca86e97f4291fa" align="middle">
<img src="https://picx.zhimg.com/v2-5a65ba37d5e8fa0f1389032d61315cfb" align="middle">
<img src="https://picx.zhimg.com/v2-ed0c9632450b28ae59c5a037e7fb1f1e" align="middle">
<img src="https://picx.zhimg.com/v2-204152c3a64f7feeae757977181b6668" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="TISDiSS-A-Training-Time-and-Inference-Time-Scalable-Framework-for-Discriminative-Source-Separation"><a href="#TISDiSS-A-Training-Time-and-Inference-Time-Scalable-Framework-for-Discriminative-Source-Separation" class="headerlink" title="TISDiSS: A Training-Time and Inference-Time Scalable Framework for   Discriminative Source Separation"></a>TISDiSS: A Training-Time and Inference-Time Scalable Framework for   Discriminative Source Separation</h2><p><strong>Authors:Yongsheng Feng, Yuetonghui Xu, Jiehui Luo, Hongjia Liu, Xiaobing Li, Feng Yu, Wei Li</strong></p>
<p>Source separation is a fundamental task in speech, music, and audio processing, and it also provides cleaner and larger data for training generative models. However, improving separation performance in practice often depends on increasingly large networks, inflating training and deployment costs. Motivated by recent advances in inference-time scaling for generative modeling, we propose Training-Time and Inference-Time Scalable Discriminative Source Separation (TISDiSS), a unified framework that integrates early-split multi-loss supervision, shared-parameter design, and dynamic inference repetitions. TISDiSS enables flexible speed-performance trade-offs by adjusting inference depth without retraining additional models. We further provide systematic analyses of architectural and training choices and show that training with more inference repetitions improves shallow-inference performance, benefiting low-latency applications. Experiments on standard speech separation benchmarks demonstrate state-of-the-art performance with a reduced parameter count, establishing TISDiSS as a scalable and practical framework for adaptive source separation. Code is available at <a target="_blank" rel="noopener" href="https://github.com/WingSingFung/TISDiSS">https://github.com/WingSingFung/TISDiSS</a>. </p>
<blockquote>
<p>æºåˆ†ç¦»æ˜¯è¯­éŸ³ã€éŸ³ä¹ã€éŸ³é¢‘å¤„ç†ä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œå®ƒä¸ºè®­ç»ƒç”Ÿæˆæ¨¡å‹æä¾›äº†æ›´å¹²å‡€ã€æ›´å¤§çš„æ•°æ®ã€‚ç„¶è€Œï¼Œåœ¨å®è·µä¸­æé«˜åˆ†ç¦»æ€§èƒ½å¾€å¾€ä¾èµ–äºè§„æ¨¡æ—¥ç›Šæ‰©å¤§çš„ç½‘ç»œï¼Œä»è€Œå¢åŠ äº†è®­ç»ƒå’Œéƒ¨ç½²æˆæœ¬ã€‚å—ç”Ÿæˆæ¨¡å‹æ¨ç†æ—¶é—´ç¼©æ”¾æ–¹é¢è¿‘æœŸè¿›å±•çš„æ¨åŠ¨ï¼Œæˆ‘ä»¬æå‡ºäº†è®­ç»ƒæ—¶é—´å’Œæ¨ç†æ—¶é—´å¯ä¼¸ç¼©çš„åˆ¤åˆ«æºåˆ†ç¦»ï¼ˆTISDiSSï¼‰è¿™ä¸€ç»Ÿä¸€æ¡†æ¶ï¼Œå®ƒé›†æˆäº†æ—©æœŸåˆ†å‰²å¤šæŸå¤±ç›‘ç£ã€å…±äº«å‚æ•°è®¾è®¡å’ŒåŠ¨æ€æ¨ç†é‡å¤ã€‚TISDiSSé€šè¿‡è°ƒæ•´æ¨ç†æ·±åº¦è€Œæ— éœ€é‡æ–°è®­ç»ƒå…¶ä»–æ¨¡å‹ï¼Œå®ç°äº†çµæ´»çš„é€Ÿåº¦æ€§èƒ½æƒè¡¡ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å¯¹æ¶æ„å’Œè®­ç»ƒé€‰æ‹©è¿›è¡Œäº†ç³»ç»Ÿåˆ†æï¼Œå¹¶è¡¨æ˜åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨æ›´å¤šçš„æ¨ç†é‡å¤å¯ä»¥æé«˜æµ…å±‚æ¨ç†æ€§èƒ½ï¼Œæœ‰åˆ©äºä½å»¶è¿Ÿåº”ç”¨ç¨‹åºã€‚åœ¨æ ‡å‡†è¯­éŸ³åˆ†ç¦»åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¯æ˜äº†å…¶å“è¶Šçš„æ€§èƒ½å’Œå‡å°‘çš„å‚æ•°æ•°é‡ï¼Œç¡®ç«‹äº†TISDiSSä½œä¸ºè‡ªé€‚åº”æºåˆ†ç¦»çš„å®ç”¨ä¸”å¯æ‰©å±•æ¡†æ¶çš„åœ°ä½ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/WingSingFung/TISDiSS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/WingSingFung/TISDiSSæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.15666v3">PDF</a> Submitted to ICASSP 2026.(C) 2025 IEEE. Personal use of this material   is permitted. Permission from IEEE must be obtained for all other uses, in   any current or future media, including reprinting&#x2F;republishing this material   for advertising or promotional purposes, creating new collective works, for   resale or redistribution to servers or lists, or reuse of any copyrighted   component of this work</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºTISDiSSçš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒæ—¶é—´å’Œæ¨ç†æ—¶é—´å¯ä¼¸ç¼©çš„åˆ¤åˆ«æºåˆ†ç¦»ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ—©æœŸåˆ†å‰²å¤šæŸå¤±ç›‘ç£ã€å…±äº«å‚æ•°è®¾è®¡å’ŒåŠ¨æ€æ¨ç†é‡å¤ï¼Œå¯çµæ´»è°ƒæ•´æ¨ç†æ·±åº¦ä»¥å–å¾—é€Ÿåº¦ä¸æ€§èƒ½çš„æƒè¡¡ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒTISDiSSåœ¨æ ‡å‡†è¯­éŸ³åˆ†ç¦»åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å“è¶Šçš„æ€§èƒ½ï¼ŒåŒæ—¶å‡å°‘äº†å‚æ•°æ•°é‡ï¼Œæˆä¸ºè‡ªé€‚åº”æºåˆ†ç¦»çš„å¯ä¼¸ç¼©å®ç”¨æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>TISDiSSæ˜¯ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºæºåˆ†ç¦»ä»»åŠ¡ï¼Œé›†æˆæ—©æœŸåˆ†å‰²å¤šæŸå¤±ç›‘ç£ã€å…±äº«å‚æ•°è®¾è®¡å’ŒåŠ¨æ€æ¨ç†é‡å¤ã€‚</li>
<li>æ¡†æ¶èƒ½å¤Ÿå®ç°çµæ´»çš„é€Ÿåº¦ä¸æ€§èƒ½æƒè¡¡ï¼Œé€šè¿‡è°ƒæ•´æ¨ç†æ·±åº¦ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚</li>
<li>TISDiSSèƒ½å¤Ÿæé«˜æºåˆ†ç¦»æ€§èƒ½ï¼ŒåŒæ—¶é™ä½ç½‘ç»œå’Œè®­ç»ƒæˆæœ¬ã€‚</li>
<li>é€šè¿‡å®éªŒéªŒè¯ï¼ŒTISDiSSåœ¨æ ‡å‡†è¯­éŸ³åˆ†ç¦»åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>è¯¥æ¡†æ¶å‡å°‘äº†å‚æ•°æ•°é‡ï¼Œä½¿å…¶æ›´é€‚ç”¨äºå®é™…åº”ç”¨ã€‚</li>
<li>TISDiSSæ¡†æ¶å¯¹äºè‡ªé€‚åº”æºåˆ†ç¦»ä»»åŠ¡å…·æœ‰å¯ä¼¸ç¼©æ€§å’Œå®ç”¨æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15666">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a8a56ec18979a3e7e80a7b5ea4a78a6e" align="middle">
<img src="https://picx.zhimg.com/v2-16843ca755599b236d6398969862cf12" align="middle">
<img src="https://picx.zhimg.com/v2-42a7659b4d18219eb30301814a275a7e" align="middle">
<img src="https://picx.zhimg.com/v2-403ec772d9b76cbdbf6311413c482a58" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="NSPDI-SNN-An-efficient-lightweight-SNN-based-on-nonlinear-synaptic-pruning-and-dendritic-integration"><a href="#NSPDI-SNN-An-efficient-lightweight-SNN-based-on-nonlinear-synaptic-pruning-and-dendritic-integration" class="headerlink" title="NSPDI-SNN: An efficient lightweight SNN based on nonlinear synaptic   pruning and dendritic integration"></a>NSPDI-SNN: An efficient lightweight SNN based on nonlinear synaptic   pruning and dendritic integration</h2><p><strong>Authors:Wuque Cai, Hongze Sun, Jiayi He, Qianqian Liao, Yunliang Zang, Duo Chen, Dezhong Yao, Daqing Guo</strong></p>
<p>Spiking neural networks (SNNs) are artificial neural networks based on simulated biological neurons and have attracted much attention in recent artificial intelligence technology studies. The dendrites in biological neurons have efficient information processing ability and computational power; however, the neurons of SNNs rarely match the complex structure of the dendrites. Inspired by the nonlinear structure and highly sparse properties of neuronal dendrites, in this study, we propose an efficient, lightweight SNN method with nonlinear pruning and dendritic integration (NSPDI-SNN). In this method, we introduce nonlinear dendritic integration (NDI) to improve the representation of the spatiotemporal information of neurons. We implement heterogeneous state transition ratios of dendritic spines and construct a new and flexible nonlinear synaptic pruning (NSP) method to achieve the high sparsity of SNN. We conducted systematic experiments on three benchmark datasets (DVS128 Gesture, CIFAR10-DVS, and CIFAR10) and extended the evaluation to two complex tasks (speech recognition and reinforcement learning-based maze navigation task). Across all tasks, NSPDI-SNN consistently achieved high sparsity with minimal performance degradation. In particular, our method achieved the best experimental results on all three event stream datasets. Further analysis showed that NSPDI significantly improved the efficiency of synaptic information transfer as sparsity increased. In conclusion, our results indicate that the complex structure and nonlinear computation of neuronal dendrites provide a promising approach for developing efficient SNN methods. </p>
<blockquote>
<p>è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSpiking Neural Networksï¼Œç®€ç§°SNNsï¼‰æ˜¯åŸºäºæ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»å…ƒçš„äººå·¥ç¥ç»ç½‘ç»œï¼Œåœ¨æœ€è¿‘çš„äººå·¥æ™ºèƒ½æŠ€æœ¯ç ”ç©¶é¢†åŸŸå¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚ç”Ÿç‰©ç¥ç»å…ƒä¸­çš„æ ‘çªå…·æœ‰é«˜æ•ˆçš„ä¿¡æ¯å¤„ç†èƒ½åŠ›å’Œè®¡ç®—åŠ›ï¼Œç„¶è€Œï¼ŒSNNsçš„ç¥ç»å…ƒå¾ˆå°‘åŒ¹é…æ ‘çªçš„å¤æ‚ç»“æ„ã€‚æœ¬ç ”ç©¶å—ç¥ç»å…ƒæ ‘çªçš„éçº¿æ€§ç»“æ„å’Œé«˜åº¦ç¨€ç–ç‰¹æ€§çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§å…·æœ‰éçº¿æ€§ä¿®å‰ªå’Œæ ‘çªæ•´åˆï¼ˆNSPDI-SNNï¼‰çš„é«˜æ•ˆã€è½»é‡çº§SNNæ–¹æ³•ã€‚åœ¨è¯¥æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†éçº¿æ€§æ ‘çªæ•´åˆï¼ˆNDIï¼‰æŠ€æœ¯ï¼Œä»¥æé«˜ç¥ç»å…ƒæ—¶ç©ºä¿¡æ¯çš„è¡¨ç¤ºèƒ½åŠ›ã€‚æˆ‘ä»¬å®ç°äº†æ ‘çªè„Šçš„å¼‚è´¨çŠ¶æ€è½¬æ¢æ¯”ç‡ï¼Œå¹¶æ„å»ºäº†ä¸€ç§æ–°çš„çµæ´»éçº¿æ€§çªè§¦ä¿®å‰ªï¼ˆNSPï¼‰æ–¹æ³•ï¼Œä»¥å®ç°SNNçš„é«˜åº¦ç¨€ç–æ€§ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ï¼ˆDVS128æ‰‹åŠ¿ã€CIFAR10-DVSå’ŒCIFAR10ï¼‰ä¸Šè¿›è¡Œäº†ç³»ç»Ÿå®éªŒï¼Œå¹¶å°†è¯„ä¼°æ‰©å±•åˆ°äº†ä¸¤ä¸ªå¤æ‚ä»»åŠ¡ï¼ˆè¯­éŸ³è¯†åˆ«å’ŒåŸºäºå¼ºåŒ–å­¦ä¹ çš„è¿·å®«å¯¼èˆªä»»åŠ¡ï¼‰ã€‚åœ¨æ‰€æœ‰ä»»åŠ¡ä¸­ï¼ŒNSPDI-SNNå§‹ç»ˆå®ç°äº†é«˜ç¨€ç–æ€§ï¼Œå¹¶ä¸”æ€§èƒ½ä¸‹é™å¹…åº¦æœ€å°ã€‚å°¤å…¶å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸‰ä¸ªäº‹ä»¶æµæ•°æ®é›†ä¸Šå‡å–å¾—äº†æœ€ä½³å®éªŒç»“æœã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼Œéšç€ç¨€ç–æ€§çš„å¢åŠ ï¼ŒNSPDIæ˜¾è‘—æé«˜äº†çªè§¦ä¿¡æ¯ä¼ è¾“çš„æ•ˆç‡ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç¥ç»å…ƒæ ‘çªçš„å¤æ‚ç»“æ„å’Œéçº¿æ€§è®¡ç®—ä¸ºå®ç°é«˜æ•ˆSNNæ–¹æ³•æä¾›äº†æœ‰å‰é€”çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.21566v2">PDF</a> 16 pages, 9 figures, 7 tables; This manuscript has been submitted for   possible pulication</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åŸºäºéçº¿æ€§ä¿®å‰ªå’Œæ ‘çªæ•´åˆï¼ˆNSPDI-SNNï¼‰çš„é«˜æ•ˆè½»é‡çº§è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¼•å…¥éçº¿æ€§æ ‘çªæ•´åˆï¼ˆNDIï¼‰ä»¥æ”¹å–„ç¥ç»å…ƒæ—¶ç©ºä¿¡æ¯çš„è¡¨ç¤ºï¼Œå¹¶å®ç°æ ‘çªè„Šçš„å¼‚è´¨çŠ¶æ€è½¬æ¢æ¯”ç‡ã€‚é€šè¿‡æ„å»ºæ–°çš„çµæ´»éçº¿æ€§ä¿®å‰ªï¼ˆNSPï¼‰æ–¹æ³•ï¼Œå®ç°SNNçš„é«˜ç¨€ç–æ€§ã€‚ç³»ç»Ÿå®éªŒè¡¨æ˜ï¼ŒNSPDI-SNNåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½å®ç°äº†é«˜ç¨€ç–æ€§ï¼Œä¸”æ€§èƒ½ä¸‹é™æœ€å°ï¼Œç‰¹åˆ«æ˜¯åœ¨äº‹ä»¶æµæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæœ€ä½³ã€‚è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼Œéšç€ç¨€ç–æ€§çš„å¢åŠ ï¼ŒNSPDIæ˜¾ç€æé«˜äº†çªè§¦ä¿¡æ¯ä¼ é€’çš„æ•ˆç‡ã€‚å› æ­¤ï¼Œç¥ç»å…ƒæ ‘çªçš„å¤æ‚ç»“æ„å’Œéçº¿æ€§è®¡ç®—å¯¹äºå¼€å‘é«˜æ•ˆSNNæ–¹æ³•å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SNNsæ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»å…ƒï¼Œåœ¨è¿‘æœŸäººå·¥æ™ºèƒ½æŠ€æœ¯ç ”ç©¶ä¸­è·å¾—å…³æ³¨ã€‚</li>
<li>ç”Ÿç‰©ç¥ç»å…ƒçš„æ ‘çªå…·æœ‰é«˜æ•ˆçš„ä¿¡æ¯å¤„ç†èƒ½åŠ›å’Œè®¡ç®—åŠŸç‡ï¼Œä½†ä¼ ç»ŸSNNséš¾ä»¥åŒ¹é…å…¶å¤æ‚ç»“æ„ã€‚</li>
<li>NSPDI-SNNæ–¹æ³•ç»“åˆäº†éçº¿æ€§ä¿®å‰ªå’Œæ ‘çªæ•´åˆï¼Œæ—¨åœ¨æé«˜SNNçš„æ•ˆç‡ã€‚</li>
<li>NDIç”¨äºæ”¹å–„ç¥ç»å…ƒæ—¶ç©ºä¿¡æ¯çš„è¡¨ç¤ºï¼Œè€Œå¼‚è´¨çŠ¶æ€è½¬æ¢æ¯”ç‡çš„æ ‘çªè„Šå¢å¼ºäº†æ–¹æ³•çš„çµæ´»æ€§ã€‚</li>
<li>NSPDI-SNNå®ç°äº†é«˜ç¨€ç–æ€§ï¼Œä¸”åœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†å’Œå¤æ‚ä»»åŠ¡ä¸Šçš„æ€§èƒ½æ˜¾è‘—ã€‚</li>
<li>éšç€ç¨€ç–æ€§çš„å¢åŠ ï¼ŒNSPDIåœ¨çªè§¦ä¿¡æ¯ä¼ é€’æ–¹é¢çš„æ•ˆç‡æé«˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.21566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-aa03f271b1352c299f33998ceb0fc948" align="middle">
<img src="https://picx.zhimg.com/v2-c0855b97d2b375c9413bf58b021d9f6e" align="middle">
<img src="https://picx.zhimg.com/v2-a9a2e1f41fbae4e0a59fecb06e9bef8f" align="middle">
<img src="https://picx.zhimg.com/v2-b6d80719bb9c499fb8e2ebfcf10a4c8f" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Towards-Inclusive-Communication-A-Unified-Framework-for-Generating-Spoken-Language-from-Sign-Lip-and-Audio"><a href="#Towards-Inclusive-Communication-A-Unified-Framework-for-Generating-Spoken-Language-from-Sign-Lip-and-Audio" class="headerlink" title="Towards Inclusive Communication: A Unified Framework for Generating   Spoken Language from Sign, Lip, and Audio"></a>Towards Inclusive Communication: A Unified Framework for Generating   Spoken Language from Sign, Lip, and Audio</h2><p><strong>Authors:Jeong Hun Yeo, Hyeongseop Rha, Sungjune Park, Junil Won, Yong Man Ro</strong></p>
<p>Audio is the primary modality for human communication and has driven the success of Automatic Speech Recognition (ASR) technologies. However, such audio-centric systems inherently exclude individuals who are deaf or hard of hearing. Visual alternatives such as sign language and lip reading offer effective substitutes, and recent advances in Sign Language Translation (SLT) and Visual Speech Recognition (VSR) have improved audio-less communication. Yet, these modalities have largely been studied in isolation, and their integration within a unified framework remains underexplored. In this paper, we propose the first unified framework capable of handling diverse combinations of sign language, lip movements, and audio for spoken-language text generation. We focus on three main objectives: (i) designing a unified, modality-agnostic architecture capable of effectively processing heterogeneous inputs; (ii) exploring the underexamined synergy among modalities, particularly the role of lip movements as non-manual cues in sign language comprehension; and (iii) achieving performance on par with or superior to state-of-the-art models specialized for individual tasks. Building on this framework, we achieve performance on par with or better than task-specific state-of-the-art models across SLT, VSR, ASR, and Audio-Visual Speech Recognition. Furthermore, our analysis reveals a key linguistic insight: explicitly modeling lip movements as a distinct modality significantly improves SLT performance by capturing critical non-manual cues. </p>
<blockquote>
<p>éŸ³é¢‘æ˜¯äººç±»äº¤æµçš„ä¸»è¦å½¢å¼ï¼Œå¹¶æ¨åŠ¨äº†è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯çš„æˆåŠŸã€‚ç„¶è€Œï¼Œè¿™ç§ä»¥éŸ³é¢‘ä¸ºä¸­å¿ƒçš„ç³»ç»Ÿæœ¬è´¨ä¸Šæ’é™¤äº†è‹å“‘äººçš„ä½¿ç”¨ã€‚å¦‚æ‰‹è¯­å’Œå”‡è¯»è¿™æ ·çš„è§†è§‰æ›¿ä»£æ–¹æ¡ˆæä¾›äº†æœ‰æ•ˆçš„æ›¿ä»£æ–¹å¼ï¼Œæœ€è¿‘åœ¨æ‰‹è¯­ç¿»è¯‘ï¼ˆSLTï¼‰å’Œè§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰æ–¹é¢çš„è¿›å±•æé«˜äº†æ— å£°é€šä¿¡çš„æ•ˆæœã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å¼å¤§å¤šè¢«å­¤ç«‹ç ”ç©¶ï¼Œå®ƒä»¬åœ¨ç»Ÿä¸€æ¡†æ¶ä¸­çš„æ•´åˆå°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†æ‰‹è¯­ã€å”‡åŠ¨å’ŒéŸ³é¢‘ç­‰å¤šç§ç»„åˆçš„ç»Ÿæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå£è¯­æ–‡æœ¬ã€‚æˆ‘ä»¬é‡ç‚¹å…³æ³¨ä¸‰ä¸ªä¸»è¦ç›®æ ‡ï¼šï¼ˆiï¼‰è®¾è®¡ä¸€ç§ç»Ÿä¸€ã€ä¸æ¨¡å¼æ— å…³çš„æ¶æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å„ç§å¼‚æ„è¾“å…¥ï¼›ï¼ˆiiï¼‰æ¢ç´¢å„æ¨¡å¼ä¹‹é—´å°šæœªç ”ç©¶çš„ååŒä½œç”¨ï¼Œç‰¹åˆ«æ˜¯å”‡åŠ¨ä½œä¸ºéæ‰‹åŠ¨çº¿ç´¢åœ¨æ‰‹è¯­ç†è§£ä¸­çš„ä½œç”¨ï¼›ï¼ˆiiiï¼‰è¾¾åˆ°æˆ–è¶…è¿‡é’ˆå¯¹å•ä¸ªä»»åŠ¡ä¸“ä¸šåŒ–çš„æœ€æ–°æ¨¡å‹çš„æ€§èƒ½ã€‚åŸºäºè¯¥æ¡†æ¶ï¼Œæˆ‘ä»¬åœ¨SLTã€VSRã€ASRå’Œè§†å¬è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¾¾åˆ°äº†æˆ–ä¼˜äºç‰¹å®šä»»åŠ¡çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸€ä¸ªå…³é”®çš„è¯­è¨€å­¦è§è§£ï¼šæ˜ç¡®åœ°å°†å”‡åŠ¨å»ºæ¨¡ä¸ºä¸€ç§ç‹¬ç‰¹çš„æ¨¡å¼ï¼Œé€šè¿‡æ•æ‰å…³é”®çš„éæ‰‹åŠ¨çº¿ç´¢ï¼Œå¯ä»¥æ˜¾è‘—æé«˜SLTæ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20476v2">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>è¯¥æ–‡æœ¬ä»‹ç»äº†éŸ³é¢‘ä½œä¸ºäººç±»æ²Ÿé€šçš„ä¸»è¦æ–¹å¼åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯ä¸­çš„é‡è¦ä½œç”¨ï¼Œä½†åŒæ—¶ä¹ŸæŒ‡å‡ºè¿™ç§éŸ³é¢‘ä¸ºä¸­å¿ƒçš„ç³»ç»Ÿæ’æ–¥äº†è‹å“‘äººå£«ã€‚è§†è§‰æ›¿ä»£æ–¹æ¡ˆå¦‚æ‰‹è¯­å’Œå”‡è¯»æä¾›äº†æœ‰æ•ˆçš„æ›¿ä»£æ–¹å¼ï¼Œæœ€è¿‘çš„ç¬¦å·è¯­è¨€ç¿»è¯‘ï¼ˆSLTï¼‰å’Œè§†è§‰è¯­éŸ³è¯†åˆ«ï¼ˆVSRï¼‰æŠ€æœ¯çš„è¿›æ­¥æ”¹å–„äº†æ— å£°æ²Ÿé€šã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å¼å¤§å¤šå­¤ç«‹ç ”ç©¶ï¼Œå®ƒä»¬åœ¨ç»Ÿä¸€æ¡†æ¶ä¸­çš„æ•´åˆä»ç„¶è¢«å¿½è§†ã€‚æœ¬æ–‡æå‡ºäº†ç¬¬ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†æ‰‹è¯­ã€å”‡åŠ¨å’ŒéŸ³é¢‘ç­‰å¤šç§ç»„åˆçš„ç»Ÿä¸€æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå£è¯­æ–‡æœ¬ã€‚æœ¬æ–‡çš„é‡ç‚¹æ˜¯è®¾è®¡ä¸€ç§ç»Ÿä¸€ã€æ¨¡å¼æ— å…³çš„æ¶æ„ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å¼‚è´¨è¾“å…¥ï¼›æ¢ç´¢å„æ¨¡å¼ä¹‹é—´æœªè¢«å‘ç°ååŒä½œç”¨ï¼Œç‰¹åˆ«æ˜¯å”‡åŠ¨ä½œä¸ºéæ‰‹åŠ¨çº¿ç´¢åœ¨æ‰‹è¯­ç†è§£ä¸­çš„ä½œç”¨ï¼›ä»¥åŠå®ç°ä¸æˆ–ä¼˜äºé’ˆå¯¹ä¸ªåˆ«ä»»åŠ¡çš„æœ€æ–°æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡å¯¹è¿™ä¸€æ¡†æ¶çš„æ„å»ºï¼Œæˆ‘ä»¬åœ¨SLTã€VSRã€ASRå’Œè§†å¬è¯­éŸ³è¯†åˆ«æ–¹é¢çš„æ€§èƒ½è¾¾åˆ°æˆ–ä¼˜äºä»»åŠ¡ç‰¹å®šæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„åˆ†ææ­ç¤ºäº†ä¸€ä¸ªå…³é”®çš„è¯­è¨€å­¦è§è§£ï¼šå°†å”‡åŠ¨æ˜ç¡®å»ºæ¨¡ä¸ºä¸€ç§ç‹¬ç‰¹çš„æ¨¡å¼ï¼Œé€šè¿‡æ•æ‰é‡è¦çš„éæ‰‹åŠ¨çº¿ç´¢ï¼Œå¯ä»¥æ˜¾è‘—æé«˜SLTæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>éŸ³é¢‘è™½ç„¶æ˜¯äººç±»æ²Ÿé€šçš„ä¸»è¦æ–¹å¼ï¼Œä½†åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æŠ€æœ¯ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œä½†è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶å¿½ç•¥äº†è‹å“‘äººå£«çš„éœ€æ±‚ã€‚</li>
<li>è§†è§‰æ›¿ä»£æ–¹æ¡ˆå¦‚æ‰‹è¯­å’Œå”‡è¯»æ˜¯æœ‰æ•ˆçš„æ›¿ä»£æ–¹å¼ï¼Œè¿‘æœŸçš„æ‰‹è¯­ç¿»è¯‘å’Œè§†è§‰è¯­éŸ³è¯†åˆ«æŠ€æœ¯è¿›æ­¥å¢å¼ºäº†æ— å£°æ²Ÿé€šçš„èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œå¯ä»¥å¤„ç†åŒ…æ‹¬æ‰‹è¯­ã€å”‡åŠ¨å’ŒéŸ³é¢‘ç­‰å¤šç§æ¨¡å¼çš„è¾“å…¥ä¿¡æ¯ã€‚æ­¤æ¡†æ¶å¯ä»¥å®ç°å¯¹å„ç§è¾“å…¥æ¨¡å¼çš„æœ‰æ•ˆå¤„ç†å¹¶ç”Ÿæˆå£è¯­æ–‡æœ¬ã€‚</li>
<li>æ¢ç´¢äº†ä¸åŒæ¨¡å¼ä¹‹é—´çš„ååŒä½œç”¨ï¼Œç‰¹åˆ«æ˜¯å”‡åŠ¨ä½œä¸ºéæ‰‹åŠ¨çº¿ç´¢åœ¨æ‰‹è¯­ç†è§£ä¸­çš„é‡è¦æ€§ã€‚è¿™ç§ååŒä½œç”¨æœ‰åŠ©äºå¢å¼ºç³»ç»Ÿçš„æ€§èƒ½å’Œç†è§£èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¡†æ¶åœ¨å¤šç§ä»»åŠ¡ä¸Šçš„æ€§èƒ½è¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰ä»»åŠ¡ç‰¹å®šæ¨¡å‹çš„è¡¨ç°ã€‚è¿™è¯æ˜äº†ç»Ÿä¸€æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªé¢†åŸŸéƒ½å…·æœ‰æ½œåœ¨çš„åº”ç”¨ä»·å€¼ã€‚</li>
<li>é€šè¿‡å»ºæ¨¡å”‡åŠ¨ä½œä¸ºä¸€ç§ç‹¬ç‰¹çš„æ¨¡å¼ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ‰‹è¯­ç¿»è¯‘çš„æ€§èƒ½ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†éæ‰‹åŠ¨çº¿ç´¢åœ¨æ²Ÿé€šä¸­çš„é‡è¦æ€§ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’å’Œæ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fd71d29d768922b361e476a7830a6690" align="middle">
<img src="https://picx.zhimg.com/v2-73afebb1f8f438a2a4036de7c226d7e2" align="middle">
<img src="https://picx.zhimg.com/v2-d9c870bab0fef466eeb5e637dcacd453" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="A-Fast-and-Lightweight-Model-for-Causal-Audio-Visual-Speech-Separation"><a href="#A-Fast-and-Lightweight-Model-for-Causal-Audio-Visual-Speech-Separation" class="headerlink" title="A Fast and Lightweight Model for Causal Audio-Visual Speech Separation"></a>A Fast and Lightweight Model for Causal Audio-Visual Speech Separation</h2><p><strong>Authors:Wendi Sang, Kai Li, Runxuan Yang, Jianqiang Huang, Xiaolin Hu</strong></p>
<p>Audio-visual speech separation (AVSS) aims to extract a target speech signal from a mixed signal by leveraging both auditory and visual (lip movement) cues. However, most existing AVSS methods exhibit complex architectures and rely on future context, operating offline, which renders them unsuitable for real-time applications. Inspired by the pipeline of RTFSNet, we propose a novel streaming AVSS model, named Swift-Net, which enhances the causal processing capabilities required for real-time applications. Swift-Net adopts a lightweight visual feature extraction module and an efficient fusion module for audio-visual integration. Additionally, Swift-Net employs Grouped SRUs to integrate historical information across different feature spaces, thereby improving the utilization efficiency of historical information. We further propose a causal transformation template to facilitate the conversion of non-causal AVSS models into causal counterparts. Experiments on three standard benchmark datasets (LRS2, LRS3, and VoxCeleb2) demonstrated that under causal conditions, our proposed Swift-Net exhibited outstanding performance, highlighting the potential of this method for processing speech in complex environments. </p>
<blockquote>
<p>è§†å¬è¯­éŸ³åˆ†ç¦»ï¼ˆAVSSï¼‰æ—¨åœ¨åˆ©ç”¨å¬è§‰å’Œè§†è§‰ï¼ˆå”‡éƒ¨åŠ¨ä½œï¼‰çº¿ç´¢ä»æ··åˆä¿¡å·ä¸­æå–ç›®æ ‡è¯­éŸ³ä¿¡å·ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„AVSSæ–¹æ³•å…·æœ‰å¤æ‚çš„æ¶æ„ï¼Œå¹¶ä¸”ä¾èµ–äºæœªæ¥ä¸Šä¸‹æ–‡è¿›è¡Œç¦»çº¿æ“ä½œï¼Œè¿™ä½¿å¾—å®ƒä»¬ä¸é€‚åˆå®æ—¶åº”ç”¨ã€‚å—RTFSNetæµç¨‹çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„æµå¼AVSSæ¨¡å‹ï¼Œåä¸ºSwift-Netï¼Œå®ƒå¢å¼ºäº†å®æ—¶åº”ç”¨æ‰€éœ€çš„å› æœå¤„ç†èƒ½åŠ›ã€‚Swift-Neté‡‡ç”¨è½»é‡çº§çš„è§†è§‰ç‰¹å¾æå–æ¨¡å—å’Œé«˜æ•ˆçš„è§†å¬èåˆæ¨¡å—ã€‚æ­¤å¤–ï¼ŒSwift-Neté‡‡ç”¨åˆ†ç»„ SRUæ¥æ•´åˆä¸åŒç‰¹å¾ç©ºé—´çš„å†å²ä¿¡æ¯ï¼Œä»è€Œæé«˜å†å²ä¿¡æ¯çš„åˆ©ç”¨æ•ˆç‡ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å› æœè½¬æ¢æ¨¡æ¿ï¼Œä»¥ä¿ƒè¿›éå› æœAVSSæ¨¡å‹å‘å› æœæ¨¡å‹çš„è½¬åŒ–ã€‚åœ¨ä¸‰ä¸ªæ ‡å‡†åŸºå‡†æ•°æ®é›†ï¼ˆLRS2ã€LRS3å’ŒVoxCeleb2ï¼‰ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨å› æœæ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬æå‡ºçš„Swift-Netè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œçªæ˜¾äº†è¯¥æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­å¤„ç†è¯­éŸ³çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.06689v2">PDF</a> Accepted by ECAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„å®æ—¶éŸ³é¢‘è§†è§‰è¯­éŸ³åˆ†ç¦»æ¨¡å‹Swift-Netï¼Œæ—¨åœ¨ä»æ··åˆä¿¡å·ä¸­æå–ç›®æ ‡è¯­éŸ³ä¿¡å·ã€‚å®ƒåˆ©ç”¨è§†å¬çº¿ç´¢ï¼Œé‡‡ç”¨è½»é‡çº§çš„è§†è§‰ç‰¹å¾æå–æ¨¡å—å’Œé«˜æ•ˆçš„èåˆæ¨¡å—è¿›è¡ŒéŸ³é¢‘è§†è§‰æ•´åˆï¼Œæé«˜å› æœå¤„ç†èƒ½åŠ›ä»¥æ»¡è¶³å®æ—¶åº”ç”¨çš„éœ€æ±‚ã€‚æ­¤å¤–ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å› æœæ¡ä»¶ä¸‹ï¼ŒSwift-Netåœ¨å¤æ‚ç¯å¢ƒä¸­å¤„ç†è¯­éŸ³çš„æ½œåŠ›å·¨å¤§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Swift-Netæ˜¯ä¸€ç§åŸºäºè§†å¬ä¿¡æ¯çš„å®æ—¶è¯­éŸ³åˆ†ç¦»æ¨¡å‹ã€‚</li>
<li>å®ƒåˆ©ç”¨è½»é‡çº§è§†è§‰ç‰¹å¾æå–æ¨¡å—å’Œé«˜æ•ˆèåˆæ¨¡å—è¿›è¡ŒéŸ³é¢‘è§†è§‰æ•´åˆã€‚</li>
<li>Swift-Neté‡‡ç”¨Grouped SRUæŠ€æœ¯ï¼Œæé«˜å†å²ä¿¡æ¯çš„åˆ©ç”¨æ•ˆç‡å’Œè·¨ç‰¹å¾ç©ºé—´çš„æ•´åˆèƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å› æœè½¬æ¢æ¨¡æ¿ï¼Œå¯å°†éå› æœAVSSæ¨¡å‹è½¬æ¢ä¸ºå› æœæ¨¡å‹ã€‚</li>
<li>åœ¨ä¸‰ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSwift-Netåœ¨å› æœæ¡ä»¶ä¸‹è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
<li>Swift-Neté€‚ç”¨äºå¤„ç†å¤æ‚ç¯å¢ƒä¸­çš„è¯­éŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06689">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cba3436994d4aeef2e10a9c544bbf5e6" align="middle">
<img src="https://picx.zhimg.com/v2-5eb473b9aab1da043d07d8ef69e2b019" align="middle">
<img src="https://picx.zhimg.com/v2-78252c16fab7b5529f1f7ec2ac7a6722" align="middle">
<img src="https://picx.zhimg.com/v2-e3c95d1a2550a7a8ddc109502ab6f206" align="middle">
<img src="https://picx.zhimg.com/v2-8f73b57b30a8c4f0fe9b63c0616de9ab" align="middle">
<img src="https://picx.zhimg.com/v2-e614cfde30eff5fc224c27b65033d969" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Stimulus-Modality-Matters-Impact-of-Perceptual-Evaluations-from-Different-Modalities-on-Speech-Emotion-Recognition-System-Performance"><a href="#Stimulus-Modality-Matters-Impact-of-Perceptual-Evaluations-from-Different-Modalities-on-Speech-Emotion-Recognition-System-Performance" class="headerlink" title="Stimulus Modality Matters: Impact of Perceptual Evaluations from   Different Modalities on Speech Emotion Recognition System Performance"></a>Stimulus Modality Matters: Impact of Perceptual Evaluations from   Different Modalities on Speech Emotion Recognition System Performance</h2><p><strong>Authors:Huang-Cheng Chou, Haibin Wu, Hung-yi Lee, Chi-Chun Lee</strong></p>
<p>Speech Emotion Recognition (SER) systems rely on speech input and emotional labels annotated by humans. However, various emotion databases collect perceptional evaluations in different ways. For instance, the IEMOCAP dataset uses video clips with sounds for annotators to provide their emotional perceptions. However, the most significant English emotion dataset, the MSP-PODCAST, only provides speech for raters to choose the emotional ratings. Nevertheless, using speech as input is the standard approach to training SER systems. Therefore, the open question is the emotional labels elicited by which scenarios are the most effective for training SER systems. We comprehensively compare the effectiveness of SER systems trained with labels elicited by different modality stimuli and evaluate the SER systems on various testing conditions. Also, we introduce an all-inclusive label that combines all labels elicited by various modalities. We show that using labels elicited by voice-only stimuli for training yields better performance on the test set, whereas labels elicited by voice-only stimuli. </p>
<blockquote>
<p>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ç³»ç»Ÿä¾èµ–äºè¯­éŸ³è¾“å…¥å’Œäººç±»æ ‡æ³¨çš„æƒ…æ„Ÿæ ‡ç­¾ã€‚ç„¶è€Œï¼Œå„ç§æƒ…æ„Ÿæ•°æ®åº“ä»¥ä¸åŒçš„æ–¹å¼æ”¶é›†æ„ŸçŸ¥è¯„ä¼°ã€‚ä¾‹å¦‚ï¼ŒIEMOCAPæ•°æ®é›†ä½¿ç”¨å¸¦æœ‰å£°éŸ³çš„è§†é¢‘ç‰‡æ®µä¾›æ³¨é‡Šè€…æä¾›ä»–ä»¬çš„æƒ…æ„Ÿæ„ŸçŸ¥ã€‚ç„¶è€Œï¼Œæœ€å¤§çš„è‹±è¯­æƒ…æ„Ÿæ•°æ®é›†MSP-PODCASTåªæä¾›è¯­éŸ³ä¾›è¯„åˆ†è€…é€‰æ‹©æƒ…æ„Ÿè¯„åˆ†ã€‚å°½ç®¡å¦‚æ­¤ï¼Œä½¿ç”¨è¯­éŸ³ä½œä¸ºè¾“å…¥æ˜¯è®­ç»ƒSERç³»ç»Ÿçš„æ ‡å‡†æ–¹æ³•ã€‚å› æ­¤ï¼Œå…¬å¼€çš„é—®é¢˜æ˜¯å“ªç§åœºæ™¯å¼•å‘çš„æƒ…æ„Ÿæ ‡ç­¾å¯¹äºè®­ç»ƒSERç³»ç»Ÿæœ€ä¸ºæœ‰æ•ˆã€‚æˆ‘ä»¬å…¨é¢æ¯”è¾ƒäº†ä½¿ç”¨ä¸åŒæ¨¡æ€åˆºæ¿€å¼•å‘çš„æ ‡ç­¾è®­ç»ƒçš„SERç³»ç»Ÿçš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨å„ç§æµ‹è¯•æ¡ä»¶ä¸‹å¯¹SERç³»ç»Ÿè¿›è¡Œäº†è¯„ä¼°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªåŒ…å®¹æ€§æ ‡ç­¾ï¼Œè¯¥æ ‡ç­¾ç»“åˆäº†ç”±å„ç§æ¨¡æ€å¼•å‘çš„æ‰€æœ‰æ ‡ç­¾ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä½¿ç”¨ä»…ç”±å£°éŸ³åˆºæ¿€å¼•å‘çš„æ ‡ç­¾è¿›è¡Œè®­ç»ƒå¯ä»¥åœ¨æµ‹è¯•é›†ä¸Šè·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œè€Œç”±å¤šç§æ¨¡æ€ç»„åˆäº§ç”Ÿçš„æ ‡ç­¾çš„æ€§èƒ½åˆ™è¾ƒå·®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2409.10762v4">PDF</a> 5 pages, 2 figures, 4 tables, acceptance for ICASSP 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ç³»ç»Ÿçš„è®­ç»ƒæ•°æ®æ ‡æ³¨æ–¹å¼å¯¹å…¶æ€§èƒ½çš„å½±å“ã€‚æ–‡ç« å¯¹æ¯”äº†ä¸åŒæ¨¡æ€åˆºæ¿€ä¸‹äº§ç”Ÿçš„æƒ…æ„Ÿæ ‡ç­¾çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å‘ç°ä½¿ç”¨ä»…ç”±è¯­éŸ³åˆºæ¿€äº§ç”Ÿçš„æ ‡ç­¾åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½è¡¨ç°æ›´ä½³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯­éŸ³æƒ…æ„Ÿè¯†åˆ«ï¼ˆSERï¼‰ç³»ç»Ÿä¾èµ–äºäººç±»æ ‡æ³¨çš„è¯­éŸ³è¾“å…¥å’Œæƒ…æ„Ÿæ ‡ç­¾ã€‚</li>
<li>ä¸åŒæƒ…æ„Ÿæ•°æ®åº“æ”¶é›†æ„ŸçŸ¥è¯„ä»·çš„æ–¹å¼ä¸åŒï¼Œå¦‚IEMOCAPæ•°æ®é›†ä½¿ç”¨è§†é¢‘å‰ªè¾‘å’Œå£°éŸ³ä¾›æ³¨é‡Šè€…æä¾›æƒ…æ„Ÿæ„ŸçŸ¥ï¼Œè€ŒMSP-PODCASTåˆ™ä»…æä¾›è¯­éŸ³ä¾›è¯„ä¼°è€…é€‰æ‹©æƒ…æ„Ÿè¯„çº§ã€‚</li>
<li>ä½¿ç”¨è¯­éŸ³ä½œä¸ºè¾“å…¥æ˜¯è®­ç»ƒSERç³»ç»Ÿçš„æ ‡å‡†æ–¹æ³•ã€‚</li>
<li>æƒ…æ„Ÿæ ‡ç­¾åœ¨ä¸åŒçš„åœºæ™¯ä¸‹å¯¹SERç³»ç»Ÿçš„è®­ç»ƒæ•ˆæœä¸åŒã€‚</li>
<li>å¯¹æ¯”äº†ä¸åŒæ¨¡æ€åˆºæ¿€ä¸‹äº§ç”Ÿçš„æƒ…æ„Ÿæ ‡ç­¾çš„æœ‰æ•ˆæ€§ã€‚</li>
<li>ä½¿ç”¨ä»…ç”±è¯­éŸ³åˆºæ¿€äº§ç”Ÿçš„æ ‡ç­¾åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½è¡¨ç°æ›´ä½³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2409.10762">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac48fa85e2d328cd8af375fcac851d61" align="middle">
<img src="https://picx.zhimg.com/v2-27748eec64e92ff99dc95eb0c5f2c29e" align="middle">
<img src="https://picx.zhimg.com/v2-a4cbc69ca8829a94cdee4a0f4a49fa21" align="middle">
<img src="https://picx.zhimg.com/v2-8cd81282c9a9216723c67bcce787a3ff" align="middle">
<img src="https://picx.zhimg.com/v2-a300624bf4fea92ea46e5c74e874ffb3" align="middle">
<img src="https://picx.zhimg.com/v2-1ddd6f569f52bf2038439c19c64d9eb9" align="middle">
<img src="https://picx.zhimg.com/v2-53f1b1906a0e219400ebc0b282d6a6f6" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-19/Speech/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-19/Speech/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Speech/">
                                    <span class="chip bg-color">Speech</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-19/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2570b5f9434bec72bd32b7dedc655704" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-19  EGSTalker Real-Time Audio-Driven Talking Head Generation with Efficient   Gaussian Deformation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-19/Vision%20Transformer/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-653e5f648e69b22339c81c099618c84a" class="responsive-img" alt="Vision Transformer">
                        
                        <span class="card-title">Vision Transformer</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-19  UniRGB-IR A Unified Framework for Visible-Infrared Semantic Tasks via   Adapter Tuning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                    Vision Transformer
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Vision-Transformer/">
                        <span class="chip bg-color">Vision Transformer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
