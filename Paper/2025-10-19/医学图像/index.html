<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="åŒ»å­¦å›¾åƒ">
    <meta name="description" content="åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-19  JEDA Query-Free Clinical Order Search from Ambient Dialogues">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>åŒ»å­¦å›¾åƒ | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://pic-private.zhihu.com/v2-4a09a249e5de3de82e8096ed77062a4c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824424&auth_key=1760824424-0-0-4373e4b72161bd5f1117fddf1e3ffb60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">åŒ»å­¦å›¾åƒ</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                åŒ»å­¦å›¾åƒ
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-10-19
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    19.6k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    80 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-10-19-æ›´æ–°"><a href="#2025-10-19-æ›´æ–°" class="headerlink" title="2025-10-19 æ›´æ–°"></a>2025-10-19 æ›´æ–°</h1><h2 id="JEDA-Query-Free-Clinical-Order-Search-from-Ambient-Dialogues"><a href="#JEDA-Query-Free-Clinical-Order-Search-from-Ambient-Dialogues" class="headerlink" title="JEDA: Query-Free Clinical Order Search from Ambient Dialogues"></a>JEDA: Query-Free Clinical Order Search from Ambient Dialogues</h2><p><strong>Authors:Praphul Singh, Corey Barrett, Sumana Srivasta, Amitabh Saikia, Irfan Bulu, Sri Gadde, Krishnaram Kenthapadi</strong></p>
<p>Clinical conversations mix explicit directives (order a chest X-ray) with implicit reasoning (the cough worsened overnight, we should check for pneumonia). Many systems rely on LLM rewriting, adding latency, instability, and opacity that hinder real-time ordering. We present JEDA (Joint Embedding for Direct and Ambient clinical orders), a domain-initialized bi-encoder that retrieves canonical orders directly and, in a query-free mode, encodes a short rolling window of ambient dialogue to trigger retrieval. Initialized from PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA aligns heterogeneous expressions of intent to shared order concepts. Training uses constrained LLM guidance to tie each signed order to complementary formulations (command only, context only, command+context, context+reasoning), producing clearer inter-order separation, tighter query extendash order coupling, and stronger generalization. The query-free mode is noise-resilient, reducing sensitivity to disfluencies and ASR errors by conditioning on a short window rather than a single utterance. Deployed in practice, JEDA yields large gains and substantially outperforms its base encoder and recent open embedders (Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The result is a fast, interpretable, LLM-free retrieval layer that links ambient context to actionable clinical orders in real time. </p>
<blockquote>
<p>ä¸´åºŠå¯¹è¯ä¸­èåˆäº†æ˜ç¡®çš„æŒ‡ç¤ºï¼ˆå¦‚è¿›è¡Œèƒ¸éƒ¨Xå…‰æ£€æŸ¥ï¼‰ä¸éšå«çš„æ¨ç†ï¼ˆå¤œé—´å’³å—½åŠ é‡ï¼Œæˆ‘ä»¬åº”è¯¥æ£€æŸ¥æ˜¯å¦æ‚£è‚ºç‚ï¼‰ã€‚è®¸å¤šç³»ç»Ÿä¾èµ–äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé‡å†™ï¼Œå¢åŠ äº†å»¶è¿Ÿã€ä¸ç¨³å®šå’Œé€æ˜åº¦é™ä½ï¼Œä»è€Œé˜»ç¢äº†å®æ—¶æ’åºã€‚æˆ‘ä»¬æå‡ºäº†JEDAï¼ˆç”¨äºç›´æ¥å’Œå‘¨å›´ç¯å¢ƒä¸´åºŠè®¢å•çš„è”åˆåµŒå…¥ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸŸåˆå§‹åŒ–åŒç¼–ç å™¨ï¼Œå®ƒå¯ä»¥ç›´æ¥æ£€ç´¢è§„èŒƒè®¢å•ï¼Œå¹¶åœ¨æ— æŸ¥è¯¢æ¨¡å¼ä¸‹ï¼Œå°†å‘¨å›´å¯¹è¯çš„ç®€çŸ­æ»šåŠ¨çª—å£è¿›è¡Œç¼–ç ï¼Œä»¥è§¦å‘æ£€ç´¢ã€‚JEDAé€šè¿‡PubMedBERTè¿›è¡Œåˆå§‹åŒ–ï¼Œå¹¶ä½¿ç”¨å…·æœ‰é˜²é‡å¤å¯¹æ¯”ç›®æ ‡çš„æ–¹æ³•è¿›è¡Œå¾®è°ƒï¼Œå°†æ„å›¾çš„å¼‚è´¨è¡¨è¾¾ä¸å…±äº«è®¢å•æ¦‚å¿µå¯¹é½ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨å—é™åˆ¶çš„å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡å¯¼ï¼Œå°†æ¯ä¸ªå·²ç­¾ç½²çš„è®¢å•ä¸è¡¥å……é…æ–¹ï¼ˆä»…å‘½ä»¤ã€ä»…ä¸Šä¸‹æ–‡ã€å‘½ä»¤+ä¸Šä¸‹æ–‡ã€ä¸Šä¸‹æ–‡+æ¨ç†ï¼‰ç›¸å…³è”ï¼Œä»è€Œäº§ç”Ÿæ›´æ¸…æ™°çš„è®¢å•é—´åˆ†ç¦»ã€æ›´ç´§å¯†çš„æŸ¥è¯¢æ‰©å±•è®¢å•è€¦åˆä»¥åŠæ›´å¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ— æŸ¥è¯¢æ¨¡å¼æ˜¯å™ªå£°è€ç”¨çš„ï¼Œé€šè¿‡åŸºäºç®€çŸ­çª—å£è€Œä¸æ˜¯å•ä¸ªè¯è¯­è¿›è¡Œæ¡ä»¶è®¾ç½®ï¼Œå‡å°‘äº†å¯¹å£è¯¯å’Œè¯­éŸ³è¯†åˆ«é”™è¯¯çš„æ•æ„Ÿæ€§ã€‚åœ¨å®è·µä¸­éƒ¨ç½²JEDAå–å¾—äº†å·¨å¤§çš„æ”¶ç›Šï¼Œå¹¶æ˜¾è‘—ä¼˜äºå…¶åŸºç¡€ç¼–ç å™¨ä»¥åŠæœ€è¿‘çš„å¼€æ”¾åµŒå…¥å™¨ï¼ˆLinq Embed Mistralã€SFR Embeddingã€GTE Qwenã€BGE largeã€Embedding Gemmaï¼‰ã€‚ç»“æœæ˜¯ä¸€ä¸ªå¿«é€Ÿã€å¯è§£é‡Šã€æ— éœ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ£€ç´¢å±‚ï¼Œèƒ½å¤Ÿå®æ—¶å°†å‘¨å›´ç¯å¢ƒä¸Šä¸‹æ–‡ä¸å¯æ“ä½œçš„ä¸´åºŠè®¢å•è”ç³»èµ·æ¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14169v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸´åºŠå¯¹è¯ä¸­æŒ‡ä»¤ä¸æ¨ç†çš„æ··åˆç‰¹ç‚¹ï¼Œä»¥åŠç°æœ‰ç³»ç»Ÿä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¦æ¥çš„å»¶è¿Ÿã€ä¸ç¨³å®šå’Œé€æ˜åº¦é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åä¸ºJEDAçš„è”åˆåµŒå…¥æ–¹æ³•ï¼Œç”¨äºç›´æ¥å’Œé—´æ¥ä¸´åºŠè®¢å•ã€‚JEDAé€šè¿‡PubMedBERTåˆå§‹åŒ–å¹¶å¾®è°ƒï¼Œä½¿ç”¨å®‰å…¨çš„å¯¹æ¯”ç›®æ ‡å¯¹é½ä¸åŒæ„å›¾è¡¨è¾¾ï¼Œè®­ç»ƒä½¿ç”¨å—çº¦æŸçš„LLMæŒ‡å¯¼æ¥ç»‘å®šæ¯ç§å·²ç­¾è®¢å•å’Œè¡¥å……åˆ¶å‰‚ï¼Œä»¥æé«˜æ¸…æ™°åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚åœ¨å®è·µä¸­éƒ¨ç½²JEDAå–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œæ˜æ˜¾ä¼˜äºåŸºç¡€ç¼–ç å™¨å’Œæœ€æ–°å¼€æ”¾åµŒå…¥å™¨ã€‚ç»“æœæ˜¯ä¸€ä¸ªå¿«é€Ÿã€å¯è§£é‡Šã€æ— éœ€å¤§å‹è¯­è¨€æ¨¡å‹çš„æ£€ç´¢å±‚ï¼Œå®æ—¶åœ°å°†ä¸Šä¸‹æ–‡èƒŒæ™¯ä¸å¯æ“ä½œçš„ä¸´åºŠè®¢å•ç›¸å…³è”ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ä¸´åºŠå¯¹è¯åŒ…å«æ˜ç¡®æŒ‡ä»¤å’Œéšå«æ¨ç†ã€‚</li>
<li>ä¼ ç»Ÿç³»ç»Ÿä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¯¼è‡´å»¶è¿Ÿã€ä¸ç¨³å®šå’Œé€æ˜åº¦é—®é¢˜ã€‚</li>
<li>JEDAæ˜¯ä¸€ç§åŒç¼–ç å™¨æ–¹æ³•ï¼Œç”¨äºç›´æ¥å’Œé—´æ¥ä¸´åºŠè®¢å•æ£€ç´¢ã€‚</li>
<li>JEDAä½¿ç”¨PubMedBERTåˆå§‹åŒ–ï¼Œé€šè¿‡å¾®è°ƒä¸å¯¹æ¯”ç›®æ ‡å¯¹é½ä¸åŒæ„å›¾è¡¨è¾¾ã€‚</li>
<li>JEDAè®­ç»ƒä¸­ä½¿ç”¨å—çº¦æŸçš„LLMæŒ‡å¯¼ï¼Œä»¥æé«˜è®¢å•æ¸…æ™°åº¦å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>JEDAå…·æœ‰æŸ¥è¯¢è‡ªç”±æ¨¡å¼ï¼Œå¯¹å™ªéŸ³å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é”™è¯¯å…·æœ‰æŠ—æ€§ã€‚</li>
<li>JEDAåœ¨å®è·µä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜æ˜¾ä¼˜äºå…¶ä»–ç¼–ç å™¨å’Œå¼€æ”¾åµŒå…¥å™¨ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14169">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-2bd1781316735ae89bd66b1d9c08c2c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823713&auth_key=1760823713-0-0-33213496a36eafa673ebcd401cd97e9d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-74d3ce67685e24d91ccc6a0615108703~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823720&auth_key=1760823720-0-0-f27a6b6d4e1ee48b9c2fe47fc6a54a26&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-54884ac305e3e846ad2813895d716af2~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823726&auth_key=1760823726-0-0-74e5ca76f625207e3fbc2194ec394733&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b44ce3e0859ccf1cf24f00dad92006ec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823732&auth_key=1760823732-0-0-94a2ce8be51075354e83e15f44bcab10&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aba096fd5433bf1090efd9ef01a411ce~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823739&auth_key=1760823739-0-0-bada665283f5de6d23fef6b5bb29ca2e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-413862c2333ac4ca6b2351e030069dce~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823745&auth_key=1760823745-0-0-b06b5f1a4c23c6d880018ed83894a96a&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="DiffLoc-Diffusion-Model-Based-High-Precision-Positioning-for-6G-Networks"><a href="#DiffLoc-Diffusion-Model-Based-High-Precision-Positioning-for-6G-Networks" class="headerlink" title="DiffLoc: Diffusion Model-Based High-Precision Positioning for 6G   Networks"></a>DiffLoc: Diffusion Model-Based High-Precision Positioning for 6G   Networks</h2><p><strong>Authors:Taekyun Lee, Tommaso Balercia, Heasung Kim, Hyeji Kim, Jeffrey G. Andrews</strong></p>
<p>This paper introduces a novel framework for high-accuracy outdoor user equipment (UE) positioning that applies a conditional generative diffusion model directly to high-dimensional massive MIMO channel state information (CSI). Traditional fingerprinting methods struggle to scale to large, dynamic outdoor environments and require dense, impractical data surveys. To overcome these limitations, our approach learns a direct mapping from raw uplink Sounding Reference Signal (SRS) fingerprints to continuous geographic coordinates. We demonstrate that our DiffLoc framework achieves unprecedented sub-centimeter precision, with our best model (DiffLoc-CT) delivering 0.5 cm fusion accuracy and 1-2 cm single base station (BS) accuracy in a realistic, ray-traced Tokyo urban macro-cell environment. This represents an order-of-magnitude improvement over existing methods, including supervised regression approaches (over 10 m error) and grid-based fusion (3 m error). Our consistency training approach reduces inference time from 200 steps to just 2 steps while maintaining exceptional accuracy even for high-speed users (15-25 m&#x2F;s) and unseen user trajectories, demonstrating the practical feasibility of our framework for real-time 6G applications. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºé«˜ç²¾åº¦æˆ·å¤–ç”¨æˆ·è®¾å¤‡ï¼ˆUEï¼‰å®šä½çš„æ–°å‹æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç›´æ¥å°†æ¡ä»¶ç”Ÿæˆæ‰©æ•£æ¨¡å‹åº”ç”¨äºé«˜ç»´å¤§è§„æ¨¡MIMOä¿¡é“çŠ¶æ€ä¿¡æ¯ï¼ˆCSIï¼‰ã€‚ä¼ ç»Ÿçš„æŒ‡çº¹æ–¹æ³•éš¾ä»¥æ‰©å±•åˆ°å¤§å‹ã€åŠ¨æ€çš„å®¤å¤–ç¯å¢ƒï¼Œå¹¶ä¸”éœ€è¦å¯†é›†ã€ä¸åˆ‡å®é™…çš„æ•°æ®è°ƒæŸ¥ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å­¦ä¹ ä»åŸå§‹ä¸Šè¡Œé“¾è·¯æ¢æµ‹å‚è€ƒä¿¡å·ï¼ˆSRSï¼‰æŒ‡çº¹åˆ°è¿ç»­åœ°ç†åæ ‡çš„ç›´æ¥æ˜ å°„ã€‚æˆ‘ä»¬è¯æ˜ï¼Œæˆ‘ä»¬çš„DiffLocæ¡†æ¶å®ç°äº†å‰æ‰€æœªæœ‰çš„äºšå˜ç±³çº§ç²¾åº¦ï¼Œå…¶ä¸­æœ€ä½³æ¨¡å‹ï¼ˆDiffLoc-CTï¼‰åœ¨ç°å®çš„ã€é€šè¿‡å°„çº¿è¿½è¸ªçš„ä¸œäº¬åŸå¸‚å®å°åŒºç¯å¢ƒä¸­å®ç°äº†0.5å˜ç±³çš„èåˆç²¾åº¦å’Œ1-2å˜ç±³çš„å•åŸºç«™ï¼ˆBSï¼‰ç²¾åº¦ã€‚è¿™ä»£è¡¨äº†ç›¸å¯¹äºç°æœ‰æ–¹æ³•çš„æ•°é‡çº§æ”¹è¿›ï¼ŒåŒ…æ‹¬ç›‘ç£å›å½’æ–¹æ³•ï¼ˆè¯¯å·®è¶…è¿‡10ç±³ï¼‰å’Œç½‘æ ¼èåˆæ–¹æ³•ï¼ˆè¯¯å·®ä¸º3ç±³ï¼‰ã€‚æˆ‘ä»¬çš„ä¸€è‡´æ€§è®­ç»ƒæ–¹æ³•å°†æ¨ç†æ—¶é—´ä»200æ­¥å‡å°‘åˆ°ä»…ä¸¤æ­¥ï¼ŒåŒæ—¶å³ä½¿åœ¨é«˜é€Ÿç”¨æˆ·ï¼ˆé€Ÿåº¦ä¸ºæ¯ç§’é€Ÿåº¦15~25ç±³ï¼‰å’Œæœªè§è¿‡çš„ç”¨æˆ·è½¨è¿¹æƒ…å†µä¸‹ä¹Ÿä¿æŒäº†å‡ºè‰²çš„å‡†ç¡®æ€§ï¼Œè¿™è¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶åœ¨å®é™…å®æ—¶åº”ç”¨ä¸­çš„å¯è¡Œæ€§ã€‚è¿™å¯¹äºæœªæ¥çš„å®æ—¶åº”ç”¨éå¸¸æœ‰åˆ©ï¼Œç‰¹åˆ«æ˜¯åœ¨æœªæ¥çš„æ— çº¿é€šä¿¡é¢†åŸŸï¼Œå¦‚ç‰©è”ç½‘å’Œè‡ªåŠ¨é©¾é©¶æ±½è½¦ç­‰é¢†åŸŸã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.14111v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºæ¡ä»¶ç”Ÿæˆæ‰©æ•£æ¨¡å‹çš„æ–°å‹é«˜ç²¾åº¦æˆ·å¤–ç”¨æˆ·è®¾å¤‡ï¼ˆUEï¼‰å®šä½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç›´æ¥åº”ç”¨äºé«˜ç»´å¤§è§„æ¨¡MIMOä¿¡é“çŠ¶æ€ä¿¡æ¯ï¼ˆCSIï¼‰ã€‚ä¼ ç»Ÿçš„æŒ‡çº¹æ–¹æ³•éš¾ä»¥æ‰©å±•åˆ°å¤§å‹åŠ¨æ€æˆ·å¤–ç¯å¢ƒï¼Œä¸”éœ€è¦å¯†é›†ã€ä¸åˆ‡å®é™…çš„æ•°æ®è°ƒæŸ¥ã€‚è€Œæœ¬æ–‡æå‡ºçš„DiffLocæ¡†æ¶å¯ä»¥å®ç°ä»åŸå§‹ä¸Šè¡Œé“¾è·¯æ¢æµ‹å‚è€ƒä¿¡å·ï¼ˆSRSï¼‰æŒ‡çº¹åˆ°è¿ç»­åœ°ç†åæ ‡çš„ç›´æ¥æ˜ å°„ï¼Œå®ç°å‰æ‰€æœªæœ‰çš„äºšå˜ç±³çº§ç²¾åº¦ã€‚åœ¨ç°å®çš„ä¸œäº¬åŸå¸‚å®è§‚ç¯å¢ƒä¸­ï¼Œæœ€ä½³æ¨¡å‹DiffLoc-CTå®ç°äº†0.5å˜ç±³çš„èåˆç²¾åº¦å’Œ1-2å˜ç±³çš„å•åŸºç«™ç²¾åº¦ï¼Œæ¯”ç°æœ‰æ–¹æ³•ï¼ˆåŒ…æ‹¬ç›‘ç£å›å½’æ–¹æ³•å’Œç½‘æ ¼èåˆæ–¹æ³•ï¼‰çš„è¯¯å·®æé«˜äº†æ•°ä¸ªæ•°é‡çº§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡çš„ä¸€è‡´æ€§è®­ç»ƒæ–¹æ³•å°†æ¨ç†æ—¶é—´ä»200æ­¥å‡å°‘åˆ°ä»…2æ­¥ï¼Œå³ä½¿åœ¨ç”¨æˆ·é«˜é€Ÿç§»åŠ¨ï¼ˆ15-25 m&#x2F;sï¼‰å’Œæœªè§ç”¨æˆ·è½¨è¿¹çš„æƒ…å†µä¸‹ä¹Ÿèƒ½ä¿æŒå‡ºè‰²çš„å‡†ç¡®æ€§ï¼Œè¯æ˜äº†è¯¥æ¡†æ¶åœ¨å®æ—¶6Gåº”ç”¨ä¸­çš„å®é™…å¯è¡Œæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„é«˜ç²¾åº¦æˆ·å¤–ç”¨æˆ·è®¾å¤‡å®šä½æ¡†æ¶DiffLocï¼ŒåŸºäºæ¡ä»¶ç”Ÿæˆæ‰©æ•£æ¨¡å‹å¤„ç†é«˜ç»´å¤§è§„æ¨¡MIMOä¿¡é“çŠ¶æ€ä¿¡æ¯ã€‚</li>
<li>ä¼ ç»ŸæŒ‡çº¹æ–¹æ³•é¢ä¸´å¤§å‹åŠ¨æ€ç¯å¢ƒæŒ‘æˆ˜ï¼Œè€ŒDiffLocèƒ½å®ç°ä»SRSæŒ‡çº¹åˆ°åœ°ç†åæ ‡çš„ç›´æ¥æ˜ å°„ã€‚</li>
<li>DiffLocæ¡†æ¶å®ç°äº†å‰æ‰€æœªæœ‰çš„äºšå˜ç±³çº§å®šä½ç²¾åº¦ï¼Œåœ¨ä¸œäº¬åŸå¸‚å®è§‚ç¯å¢ƒä¸‹è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼ŒDiffLocçš„ç²¾åº¦æ˜¾è‘—æé«˜ï¼ŒåŒ…æ‹¬ç›‘ç£å›å½’å’Œç½‘æ ¼èåˆæ–¹æ³•ã€‚</li>
<li>ä¸€è‡´æ€§è®­ç»ƒæ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ¨ç†æ—¶é—´ï¼Œæé«˜äº†æ¡†æ¶çš„å®é™…åº”ç”¨æ•ˆç‡ã€‚</li>
<li>DiffLocæ¡†æ¶é€‚ç”¨äºé«˜é€Ÿç§»åŠ¨ç”¨æˆ·å’Œæœªè§è½¨è¿¹çš„æƒ…å†µã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.14111">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-ad7fe6a687f03d509999aa3f08be1830~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823752&auth_key=1760823752-0-0-12d05e9bcb0cc5e53affe913bc44195b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d2c21bedd1c0d9bd55262bfcb17b3fb8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823760&auth_key=1760823760-0-0-054574c5c51b655b3f9189acee203a1b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e7df2e35873d343dcbdf1afb649fe9fe~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823766&auth_key=1760823766-0-0-fddc8f2f28d055f403bf3b809f94f877&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2068d4c8a5a81627bb900b0a909a9c6b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823774&auth_key=1760823774-0-0-ce6ca472a8b83d094d5799dec4cbe688&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-87509c5b6f247df6b341cb45f8a018a0~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823780&auth_key=1760823780-0-0-5032833b76725d338a61c2c107a9d6a5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Large-Language-Model-Agents-Enable-Autonomous-Design-and-Image-Analysis-of-Microwell-Microfluidics"><a href="#Large-Language-Model-Agents-Enable-Autonomous-Design-and-Image-Analysis-of-Microwell-Microfluidics" class="headerlink" title="Large Language Model Agents Enable Autonomous Design and Image Analysis   of Microwell Microfluidics"></a>Large Language Model Agents Enable Autonomous Design and Image Analysis   of Microwell Microfluidics</h2><p><strong>Authors:Dinh-Nguyen Nguyen, Sadia Shakil, Raymond Kai-Yu Tong, Ngoc-Duy Dinh</strong></p>
<p>Microwell microfluidics has been utilized for single-cell analysis to reveal heterogeneity in gene expression, signaling pathways, and phenotypic responses for identifying rare cell types, understanding disease progression, and developing more precise therapeutic strategies. However, designing microwell microfluidics is a considerably complex task, requiring knowledge, experience, and CAD software, as well as manual intervention, which often fails initial designs, demanding multiple costly and time-consuming iterations. In this study, we establish an autonomous large language model (LLM)-driven microwell design framework to generate code-based computer-aided design (CAD) scripts, that enables the rapid and reproducible creation of microwells with diverse geometries and imaging-based analysis. We propose a multimodal large language model (MLLM)-logistic regression framework based on integrating high-level semantic descriptions generated by MLLMs with image embeddings for image classification tasks, aiming to identify microwell occupancy and microwell shape. The fused multimodal representation is input to a logistic regression model, which is both interpretable and computationally efficient. We achieved significant improvements, exceeding 0.92 for occupancy classification and 0.99 for shape classification, across all evaluated MLLMs, compared with 0.50 and 0.55, respectively, when relying solely on direct classification. The MLLM-logistic regression framework is a scalable, efficient solution for high-throughput microwell image analysis. Our study demonstrates an autonomous design microwell platform by translating natural language prompts into optimized device geometries, CAD scripts and image analysis, facilitating the development of next-generation digital discovery by integration of literature mining, autonomous design and experimental data analysis. </p>
<blockquote>
<p>å¾®å­”å¾®æµä½“æŠ€æœ¯å·²è¢«åº”ç”¨äºå•ç»†èƒåˆ†æï¼Œä»¥æ­ç¤ºåŸºå› è¡¨è¾¾ã€ä¿¡å·é€šè·¯å’Œè¡¨å‹ååº”ä¸­çš„å¼‚è´¨æ€§ï¼Œä»è€Œè¯†åˆ«ç¨€æœ‰ç»†èƒç±»å‹ã€äº†è§£ç–¾ç—…è¿›å±•ï¼Œå¹¶å¼€å‘æ›´ç²¾ç¡®çš„æ²»ç–—ç­–ç•¥ã€‚ç„¶è€Œï¼Œè®¾è®¡å¾®å­”å¾®æµä½“æ˜¯ä¸€é¡¹ç›¸å½“å¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦çŸ¥è¯†ã€ç»éªŒå’Œè®¡ç®—æœºè¾…åŠ©è®¾è®¡è½¯ä»¶ï¼Œä»¥åŠæ‰‹åŠ¨å¹²é¢„ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´åˆå§‹è®¾è®¡å¤±è´¥ï¼Œéœ€è¦è¿›è¡Œå¤šæ¬¡æ˜‚è´µå’Œè€—æ—¶çš„è¿­ä»£ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªè‡ªä¸»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é©±åŠ¨çš„å¾®å­”è®¾è®¡æ¡†æ¶ï¼Œç”ŸæˆåŸºäºä»£ç çš„è®¡ç®—è¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰è„šæœ¬ï¼Œèƒ½å¤Ÿè¿…é€Ÿã€å¯é‡å¤åœ°åˆ›å»ºå…·æœ‰ä¸åŒå‡ ä½•å½¢çŠ¶çš„å¾®å­”ï¼Œå¹¶è¿›è¡ŒåŸºäºæˆåƒçš„åˆ†æã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„é€»è¾‘å›å½’æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ—¨åœ¨å°†MLLMç”Ÿæˆçš„é«˜çº§è¯­ä¹‰æè¿°ä¸å›¾åƒåµŒå…¥ç›¸ç»“åˆï¼Œç”¨äºå›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œä»¥è¯†åˆ«å¾®å­”å ç”¨æƒ…å†µå’Œå¾®å­”å½¢çŠ¶ã€‚èåˆçš„å¤šæ¨¡æ€è¡¨ç¤ºè¢«è¾“å…¥é€»è¾‘å›å½’æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ—¢å…·æœ‰å¯è§£é‡Šæ€§åˆè®¡ç®—é«˜æ•ˆã€‚æˆ‘ä»¬å–å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼Œåœ¨æ‰€æœ‰è¯„ä¼°çš„MLLMä¸­ï¼Œå ç”¨åˆ†ç±»è¶…è¿‡0.92ï¼Œå½¢çŠ¶åˆ†ç±»è¶…è¿‡0.99ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œä»…ä¾èµ–ç›´æ¥åˆ†ç±»æ—¶åˆ†åˆ«ä¸º0.50å’Œ0.55ã€‚MLLMé€»è¾‘å›å½’æ¡†æ¶æ˜¯ä¸€ä¸ªå¯æ‰©å±•ã€é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºé«˜é€šé‡å¾®å­”å›¾åƒåˆ†æã€‚æˆ‘ä»¬çš„ç ”ç©¶å±•ç¤ºäº†ä¸€ä¸ªè‡ªä¸»è®¾è®¡çš„å¾®å­”å¹³å°ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æç¤ºå°†å…¶è½¬åŒ–ä¸ºä¼˜åŒ–çš„è®¾å¤‡å‡ ä½•å½¢çŠ¶ã€CADè„šæœ¬å’Œå›¾åƒåˆ†æï¼Œä¿ƒè¿›äº†é€šè¿‡æ–‡çŒ®æŒ–æ˜ã€è‡ªä¸»è®¾è®¡å’Œå®éªŒæ•°æ®åˆ†æçš„ä¸‹ä¸€ä»£æ•°å­—å‘ç°çš„å¼€å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.13883v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åˆ©ç”¨å¾®äº•å¾®æµä½“æŠ€æœ¯è¿›è¡Œå•ç»†èƒåˆ†æçš„é‡è¦æ€§ï¼ŒåŒ…æ‹¬æ­ç¤ºåŸºå› è¡¨è¾¾ã€ä¿¡å·é€šè·¯å’Œè¡¨å‹ååº”çš„å¼‚è´¨æ€§ï¼Œä»¥åŠå…¶åœ¨ç–¾ç—…è¿›å±•å’Œæ²»ç–—ç­–ç•¥ä¸­çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œè®¾è®¡å¾®äº•å¾®æµä½“æ˜¯ä¸€é¡¹å¤æ‚ä¸”éœ€è¦ä¸“ä¸šæŠ€èƒ½çš„ä»»åŠ¡ï¼Œå¾€å¾€éœ€è¦è®¡ç®—æœºè¾…åŠ©è®¾è®¡è½¯ä»¶ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–å¾®äº•è®¾è®¡æ¡†æ¶ï¼Œé€šè¿‡ç”ŸæˆåŸºäºä»£ç çš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡è„šæœ¬å®ç°å¿«é€Ÿå¯é‡å¤æ€§çš„å¾®äº•åˆ›å»ºã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†ä¸€ä¸ªç»“åˆæ–‡æœ¬è¯­ä¹‰æè¿°å’Œå›¾åƒåµŒå…¥çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é€»è¾‘å›å½’æ¡†æ¶ï¼Œç”¨äºè§£å†³å›¾åƒåˆ†ç±»é—®é¢˜ï¼Œä¾‹å¦‚å¾®äº•å ç”¨ç‡å’Œå¾®äº•å½¢çŠ¶çš„è¯†åˆ«ã€‚è¿™ä¸€è‡ªåŠ¨åŒ–å¹³å°ä¸ºæ•°å­—å‘ç°çš„ä¸‹ä¸€ä»£å‘å±•æä¾›äº†ä¾¿åˆ©ï¼Œé€šè¿‡æ•´åˆæ–‡çŒ®æŒ–æ˜ã€è‡ªä¸»è®¾è®¡å’Œå®éªŒæ•°æ®åˆ†æï¼Œå°†è‡ªç„¶è¯­è¨€æç¤ºè½¬åŒ–ä¸ºä¼˜åŒ–åçš„è®¾å¤‡å‡ ä½•å½¢çŠ¶å’Œå›¾åƒåˆ†æã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¾®äº•å¾®æµä½“æŠ€æœ¯ç”¨äºå•ç»†èƒåˆ†æåœ¨åŸºå› è¡¨è¾¾ã€ä¿¡å·é€šè·¯å’Œè¡¨å‹ååº”å¼‚è´¨æ€§æ­ç¤ºä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚</li>
<li>è®¾è®¡å¾®äº•å¾®æµä½“æ˜¯ä¸€é¡¹å¤æ‚ä»»åŠ¡ï¼Œéœ€è¦ä¸“ä¸šçŸ¥è¯†ã€ç»éªŒå’Œè®¡ç®—æœºè¾…åŠ©è®¾è®¡è½¯ä»¶ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªåŠ¨åŒ–å¾®äº•è®¾è®¡æ¡†æ¶ï¼Œé€šè¿‡ç”ŸæˆåŸºäºä»£ç çš„è®¡ç®—æœºè¾…åŠ©è®¾è®¡è„šæœ¬å®ç°å¿«é€Ÿå¯é‡å¤æ€§çš„å¾®äº•åˆ›å»ºã€‚</li>
<li>æå‡ºä¸€ç§å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹é€»è¾‘å›å½’æ¡†æ¶ï¼Œç»“åˆæ–‡æœ¬è¯­ä¹‰æè¿°å’Œå›¾åƒåµŒå…¥ï¼Œè§£å†³äº†å¦‚å¾®äº•å ç”¨å’Œå½¢çŠ¶è¯†åˆ«çš„å›¾åƒåˆ†ç±»é—®é¢˜ã€‚</li>
<li>å¤§å‹è¯­è¨€æ¨¡å‹é€»è¾‘å›å½’æ¡†æ¶æ˜¾è‘—æé«˜äº†åˆ†ç±»æ€§èƒ½ï¼Œå ç”¨åˆ†ç±»è¶…è¿‡92%ï¼Œå½¢çŠ¶åˆ†ç±»è¶…è¿‡99%ã€‚</li>
<li>è¯¥æ¡†æ¶å…·æœ‰å¯æ‰©å±•æ€§å’Œé«˜æ•ˆæ€§ï¼Œä¸ºè§£å†³é«˜é€šé‡å¾®äº•å›¾åƒåˆ†ææä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.13883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-174301e4e3e447670c7bc5115e9bfa7d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823787&auth_key=1760823787-0-0-f67b3cf2c3a6cb2d2eb574c20fe804bd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d3f69a8b793abdb328069e79d1996977~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823794&auth_key=1760823794-0-0-207ad9c9a33d222cd21c94ebf9a4b28d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-bed6cc53f61a7eec62211a6f3422bc47~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823801&auth_key=1760823801-0-0-8f4c32ac4fbeeda5e3fdbd1ddf9a7066&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Evaluating-the-Explainability-of-Vision-Transformers-in-Medical-Imaging"><a href="#Evaluating-the-Explainability-of-Vision-Transformers-in-Medical-Imaging" class="headerlink" title="Evaluating the Explainability of Vision Transformers in Medical Imaging"></a>Evaluating the Explainability of Vision Transformers in Medical Imaging</h2><p><strong>Authors:Leili Barekatain, Ben Glocker</strong></p>
<p>Understanding model decisions is crucial in medical imaging, where interpretability directly impacts clinical trust and adoption. Vision Transformers (ViTs) have demonstrated state-of-the-art performance in diagnostic imaging; however, their complex attention mechanisms pose challenges to explainability. This study evaluates the explainability of different Vision Transformer architectures and pre-training strategies - ViT, DeiT, DINO, and Swin Transformer - using Gradient Attention Rollout and Grad-CAM. We conduct both quantitative and qualitative analyses on two medical imaging tasks: peripheral blood cell classification and breast ultrasound image classification. Our findings indicate that DINO combined with Grad-CAM offers the most faithful and localized explanations across datasets. Grad-CAM consistently produces class-discriminative and spatially precise heatmaps, while Gradient Attention Rollout yields more scattered activations. Even in misclassification cases, DINO with Grad-CAM highlights clinically relevant morphological features that appear to have misled the model. By improving model transparency, this research supports the reliable and explainable integration of ViTs into critical medical diagnostic workflows. </p>
<blockquote>
<p>åœ¨åŒ»å­¦æˆåƒé¢†åŸŸï¼Œç†è§£æ¨¡å‹å†³ç­–è‡³å…³é‡è¦ï¼Œå› ä¸ºå¯è§£é‡Šæ€§ç›´æ¥å½±å“ä¸´åºŠä¿¡ä»»å’Œé‡‡ç”¨ã€‚è§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰åœ¨è¯Šæ–­æˆåƒæ–¹é¢è¡¨ç°å‡ºäº†å“è¶Šçš„æ€§èƒ½ï¼›ç„¶è€Œï¼Œå…¶å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶å¯¹è§£é‡Šæ€§æ„æˆæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶ä½¿ç”¨æ¢¯åº¦æ³¨æ„åŠ›å±•å¼€å’ŒGrad-CAMè¯„ä¼°äº†ä¸åŒçš„è§†è§‰è½¬æ¢å™¨æ¶æ„å’Œé¢„è®­ç»ƒç­–ç•¥â€”â€”ViTã€DeiTã€DINOå’ŒSwin Transformerçš„è§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨ä¸¤é¡¹åŒ»å­¦æˆåƒä»»åŠ¡ä¸Šè¿›è¡Œäº†å®šé‡å’Œå®šæ€§åˆ†æï¼šå¤–å‘¨è¡€ç»†èƒåˆ†ç±»å’Œä¹³è…ºè¶…å£°å›¾åƒåˆ†ç±»ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒDINOä¸Grad-CAMç›¸ç»“åˆæä¾›äº†è·¨æ•°æ®é›†æœ€å¿ å®å’Œå±€éƒ¨åŒ–çš„è§£é‡Šã€‚Grad-CAMå§‹ç»ˆäº§ç”Ÿç±»åˆ«åŒºåˆ†åº¦é«˜ä¸”ç©ºé—´ç²¾ç¡®çš„çƒ­å›¾ï¼Œè€Œæ¢¯åº¦æ³¨æ„åŠ›å±•å¼€äº§ç”Ÿçš„æ¿€æ´»æ›´åˆ†æ•£ã€‚å³ä½¿åœ¨è¯¯åˆ†ç±»çš„æƒ…å†µä¸‹ï¼ŒDINOä¸Grad-CAMä¹Ÿèƒ½çªå‡ºä¸´åºŠä¸Šä¸å½¢æ€ç›¸å…³çš„ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾ä¼¼ä¹å¯¼è‡´äº†æ¨¡å‹çš„è¯¯å¯¼ã€‚é€šè¿‡æé«˜æ¨¡å‹çš„é€æ˜åº¦ï¼Œè¯¥ç ”ç©¶æ”¯æŒå°†ViTså¯é ä¸”å¯è§£é‡Šåœ°é›†æˆåˆ°å…³é”®çš„åŒ»å­¦è¯Šæ–­å·¥ä½œæµç¨‹ä¸­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.12021v1">PDF</a> Accepted at Workshop on Interpretability of Machine Intelligence in   Medical Image Computing at MICCAI 2025</p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶è¯„ä¼°äº†ä¸åŒVision Transformeræ¶æ„å’Œé¢„è®­ç»ƒç­–ç•¥ï¼ˆåŒ…æ‹¬ViTã€DeiTã€DINOå’ŒSwin Transformerï¼‰åœ¨åŒ»å­¦æˆåƒä»»åŠ¡ä¸­çš„è§£é‡Šæ€§ï¼Œé‡‡ç”¨Gradient Attention Rolloutå’ŒGrad-CAMæ–¹æ³•è¿›è¡Œå®šé‡å’Œå®šæ€§åˆ†æã€‚ç ”ç©¶å‘ç°ï¼ŒDINOç»“åˆGrad-CAMæä¾›è·¨æ•°æ®é›†çš„æœ€å¿ å®å’Œå±€éƒ¨åŒ–è§£é‡Šã€‚è¿™é¡¹ç ”ç©¶é€šè¿‡æé«˜æ¨¡å‹çš„é€æ˜åº¦ï¼Œæ”¯æŒå°†ViTså¯é åœ°èå…¥å…³é”®çš„åŒ»å­¦è¯Šæ–­æµç¨‹ä¸­ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) åœ¨åŒ»å­¦æˆåƒä¸­å±•ç°å‡ºè‰²çš„æ€§èƒ½ï¼Œä½†å…¶å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶å¯¹è§£é‡Šæ€§æ„æˆæŒ‘æˆ˜ã€‚</li>
<li>ç ”ç©¶é€šè¿‡Gradient Attention Rolloutå’ŒGrad-CAMè¯„ä¼°äº†ä¸åŒViTæ¶æ„å’Œé¢„è®­ç»ƒç­–ç•¥çš„è§£é‡Šæ€§ã€‚</li>
<li>DINOç»“åˆGrad-CAMæä¾›è·¨æ•°æ®é›†çš„æœ€å¿ å®å’Œå±€éƒ¨åŒ–è§£é‡Šï¼Œæ”¯æŒæ¨¡å‹çš„å¯é å’Œé€æ˜ã€‚</li>
<li>Grad-CAMèƒ½äº§ç”Ÿç±»åˆ«åŒºåˆ†åº¦é«˜ã€ç©ºé—´ç²¾ç¡®çš„çƒ­å›¾ï¼Œè€ŒGradient Attention Rolloutäº§ç”Ÿçš„æ¿€æ´»æ›´ä¸ºåˆ†æ•£ã€‚</li>
<li>å³ä½¿åœ¨è¯¯åˆ†ç±»çš„æƒ…å†µä¸‹ï¼ŒDINOä¸Grad-CAMä¹Ÿèƒ½çªå‡ºä¸´åºŠä¸Šé‡è¦çš„å½¢æ€ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾ä¼¼ä¹è¯¯å¯¼äº†æ¨¡å‹ã€‚</li>
<li>ç ”ç©¶ç»“æœå¯¹äºå°†ViTsèå…¥åŒ»å­¦è¯Šæ–­æµç¨‹å…·æœ‰é‡è¦æ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.12021">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3aa5fcc104290e1312af22316a44d5a6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823808&auth_key=1760823808-0-0-e080a186253f22c709204aff915456fa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3c589ebe5dd5c052e835dfd3c267c0c6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823814&auth_key=1760823814-0-0-8bdcbde5fe5ba3c102fdae5ac095ed60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-16ecba0e33851f66911af0af511cd3ef~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823821&auth_key=1760823821-0-0-ea113f1cb1a2e570781bc6b7cda149f6&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7af5e3021c1a455a2f91f444bd570b6c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823827&auth_key=1760823827-0-0-c0eae6199d9ae142cc8f574026800c4e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f75dd62d54b02d9870f17e65a62b20df~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823834&auth_key=1760823834-0-0-0491ec44fa9f4e1bd4f479a0cbf3f77f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="EvoCAD-Evolutionary-CAD-Code-Generation-with-Vision-Language-Models"><a href="#EvoCAD-Evolutionary-CAD-Code-Generation-with-Vision-Language-Models" class="headerlink" title="EvoCAD: Evolutionary CAD Code Generation with Vision Language Models"></a>EvoCAD: Evolutionary CAD Code Generation with Vision Language Models</h2><p><strong>Authors:Tobias Preintner, Weixuan Yuan, Adrian KÃ¶nig, Thomas BÃ¤ck, Elena Raponi, Niki van Stein</strong></p>
<p>Combining large language models with evolutionary computation algorithms represents a promising research direction leveraging the remarkable generative and in-context learning capabilities of LLMs with the strengths of evolutionary algorithms. In this work, we present EvoCAD, a method for generating computer-aided design (CAD) objects through their symbolic representations using vision language models and evolutionary optimization. Our method samples multiple CAD objects, which are then optimized using an evolutionary approach with vision language and reasoning language models. We assess our method using GPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and comparing it to prior methods. Additionally, we introduce two new metrics based on topological properties defined by the Euler characteristic, which capture a form of semantic similarity between 3D objects. Our results demonstrate that EvoCAD outperforms previous approaches on multiple metrics, particularly in generating topologically correct objects, which can be efficiently evaluated using our two novel metrics that complement existing spatial metrics. </p>
<blockquote>
<p>å°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è¿›åŒ–è®¡ç®—ç®—æ³•ç›¸ç»“åˆï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ä»¥åŠè¿›åŒ–ç®—æ³•çš„ä¼˜åŠ¿ï¼Œæ˜¯ä¸€ä¸ªå……æ»¡å‰æ™¯çš„ç ”ç©¶æ–¹å‘ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†EvoCADæ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹å’Œè¿›åŒ–ä¼˜åŒ–ç”Ÿæˆè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰å¯¹è±¡çš„ç¬¦å·è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡æ ·å¤šä¸ªCADå¯¹è±¡ï¼Œç„¶åä½¿ç”¨å¸¦æœ‰è§†è§‰è¯­è¨€å’Œæ¨ç†è¯­è¨€æ¨¡å‹çš„è¿›åŒ–æ–¹æ³•è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬ä½¿ç”¨GPT-4Vå’ŒGPT-4oè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨CADPromptåŸºå‡†æ•°æ®é›†ä¸Šå¯¹å…¶è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å°†å…¶ä¸å…ˆå‰çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†åŸºäºæ¬§æ‹‰ç‰¹å¾å®šä¹‰çš„ä¸¤ä¸ªæ–°æŒ‡æ ‡ï¼Œå®ƒä»¬èƒ½å¤Ÿæ•æ‰ä¸‰ç»´å¯¹è±¡ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šï¼ŒEvoCADä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆæ‹“æ‰‘æ­£ç¡®çš„å¯¹è±¡æ–¹é¢ï¼Œè¿™å¯ä»¥é€šè¿‡æˆ‘ä»¬è¡¥å……ç°æœ‰ç©ºé—´æŒ‡æ ‡çš„ä¸¤ä¸ªæ–°æŒ‡æ ‡è¿›è¡Œæœ‰æ•ˆè¯„ä¼°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11631v1">PDF</a> Accepted to IEEE ICTAI 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬å·¥ä½œæå‡ºä¸€ç§åä¸ºEvoCADçš„æ–¹æ³•ï¼Œç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œè¿›åŒ–è®¡ç®—ç®—æ³•ï¼Œé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹å’Œè¿›åŒ–ä¼˜åŒ–ç”Ÿæˆè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰å¯¹è±¡çš„ç¬¦å·è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é‡‡ç”¨è¿›åŒ–ç®—æ³•ä¼˜åŒ–é‡‡æ ·å¾—åˆ°çš„CADå¯¹è±¡ï¼Œå¹¶åœ¨CADPromptåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°å…¶æ€§èƒ½ï¼Œå¼•å…¥åŸºäºEulerç‰¹å¾æ‹“æ‰‘å±æ€§çš„ä¸¤ä¸ªæ–°æŒ‡æ ‡æ¥æ•æ‰ä¸‰ç»´å¯¹è±¡ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒEvoCADåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆæ‹“æ‰‘æ­£ç¡®çš„å¯¹è±¡æ–¹é¢ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>EvoCADç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œè¿›åŒ–è®¡ç®—ç®—æ³•ï¼Œç”Ÿæˆè®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰å¯¹è±¡çš„ç¬¦å·è¡¨ç¤ºã€‚</li>
<li>ä½¿ç”¨è¿›åŒ–ç®—æ³•ä¼˜åŒ–CADå¯¹è±¡çš„é‡‡æ ·ç»“æœã€‚</li>
<li>åœ¨CADPromptåŸºå‡†æ•°æ®é›†ä¸Šè¯„ä¼°EvoCADæ€§èƒ½ã€‚</li>
<li>å¼•å…¥ä¸¤ä¸ªåŸºäºEulerç‰¹å¾çš„æ‹“æ‰‘å±æ€§æ–°æŒ‡æ ‡ï¼Œç”¨äºæ•æ‰ä¸‰ç»´å¯¹è±¡ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚</li>
<li>EvoCADåœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šä¼˜äºå…ˆå‰æ–¹æ³•ï¼Œå°¤å…¶åœ¨ç”Ÿæˆæ‹“æ‰‘æ­£ç¡®çš„å¯¹è±¡æ–¹é¢è¡¨ç°çªå‡ºã€‚</li>
<li>æ–°æå‡ºçš„ä¸¤ä¸ªæŒ‡æ ‡èƒ½å¤Ÿè¡¥å……ç°æœ‰çš„ç©ºé—´æŒ‡æ ‡ï¼Œå¯¹å¯¹è±¡è¿›è¡Œæ›´æœ‰æ•ˆçš„è¯„ä¼°ã€‚</li>
<li>è¯¥æ–¹æ³•å±•ç°äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è¿›åŒ–è®¡ç®—ç®—æ³•ç»“åˆçš„æ½œåŠ›ï¼Œä¸ºè®¡ç®—æœºè¾…åŠ©è®¾è®¡é¢†åŸŸå¸¦æ¥æ–°çš„ç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11631">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-da7e9e6cb06085578f34b47043a48ec4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823841&auth_key=1760823841-0-0-99138c42b5775e335f4ef45253ed2e9e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b590ba8e380f37c36d4d27dc9ce5e00a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823848&auth_key=1760823848-0-0-fa0fc701cb42d7922d35ed855b5a0a73&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-90fe4b9f51d64bf5e44a582b92a8fe68~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823855&auth_key=1760823855-0-0-2227b316bd8dc8872df6eff3f045f13b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-3827dc742ae1a7771e729a8ba73940ea~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823862&auth_key=1760823862-0-0-f5867d542996a47a0883b10a5f854bf3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e26285ea52d9c5ef11c07998f5eacbee~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823868&auth_key=1760823868-0-0-a7aa33cc67c73cbbf171316d38977b07&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="TDADL-IE-A-Deep-Learning-Driven-Cryptographic-Architecture-for-Medical-Image-Security"><a href="#TDADL-IE-A-Deep-Learning-Driven-Cryptographic-Architecture-for-Medical-Image-Security" class="headerlink" title="TDADL-IE: A Deep Learning-Driven Cryptographic Architecture for Medical   Image Security"></a>TDADL-IE: A Deep Learning-Driven Cryptographic Architecture for Medical   Image Security</h2><p><strong>Authors:Junhua Zhou, Quanjun Li, Weixuan Li, Guang Yu, Yihua Shao, Yihang Dong, Mengqian Wang, Zimeng Li, Changwei Gong, Xuhang Chen</strong></p>
<p>The rise of digital medical imaging, like MRI and CT, demands strong encryption to protect patient data in telemedicine and cloud storage. Chaotic systems are popular for image encryption due to their sensitivity and unique characteristics, but existing methods often lack sufficient security. This paper presents the Three-dimensional Diffusion Algorithm and Deep Learning Image Encryption system (TDADL-IE), built on three key elements. First, we propose an enhanced chaotic generator using an LSTM network with a 1D-Sine Quadratic Chaotic Map (1D-SQCM) for better pseudorandom sequence generation. Next, a new three-dimensional diffusion algorithm (TDA) is applied to encrypt permuted images. TDADL-IE is versatile for images of any size. Experiments confirm its effectiveness against various security threats. The code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/QuincyQAQ/TDADL-IE%7D%7Bhttps://github.com/QuincyQAQ/TDADL-IE%7D">https://github.com/QuincyQAQ/TDADL-IE}{https://github.com/QuincyQAQ/TDADL-IE}</a>. </p>
<blockquote>
<p>æ•°å­—åŒ»å­¦æˆåƒï¼ˆå¦‚MRIå’ŒCTï¼‰çš„å…´èµ·ï¼Œè¦æ±‚åœ¨è¿œç¨‹åŒ»ç–—å’Œäº‘å­˜å‚¨ä¸­ä¿æŠ¤æ‚£è€…æ•°æ®éœ€è¦å¼ºå¤§çš„åŠ å¯†æŠ€æœ¯ã€‚ç”±äºæ··æ²Œç³»ç»Ÿçš„æ•æ„Ÿæ€§å’Œç‹¬ç‰¹ç‰¹æ€§ï¼Œå®ƒä»¬åœ¨å›¾è±¡åŠ å¯†ä¸­å¾ˆå—æ¬¢è¿ï¼Œä½†ç°æœ‰æ–¹æ³•å¾€å¾€ç¼ºä¹è¶³å¤Ÿçš„å®‰å…¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºä¸‰ä¸ªå…³é”®å…ƒç´ çš„ä¸‰ç»´æ‰©æ•£ç®—æ³•å’Œæ·±åº¦å­¦ä¹ å›¾åƒåŠ å¯†ç³»ç»Ÿï¼ˆTDADL-IEï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºçš„æ··æ²Œç”Ÿæˆå™¨ï¼Œè¯¥ç”Ÿæˆå™¨ä½¿ç”¨å¸¦æœ‰1D-Sine Quadratic Chaotic Mapï¼ˆ1D-SQCMï¼‰çš„LSTMç½‘ç»œï¼Œä»¥ç”Ÿæˆæ›´å¥½çš„ä¼ªéšæœºåºåˆ—ã€‚æ¥ä¸‹æ¥ï¼Œå°†æ–°çš„ä¸‰ç»´æ‰©æ•£ç®—æ³•ï¼ˆTDAï¼‰åº”ç”¨äºåŠ å¯†ç½®æ¢å›¾åƒã€‚TDADL-IEé€‚ç”¨äºä»»ä½•å¤§å°çš„å›¾åƒã€‚å®éªŒè¯å®äº†å…¶å¯¹æŠ—å„ç§å®‰å…¨å¨èƒçš„æœ‰æ•ˆæ€§ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/QuincyQAQ/TDADL-IE">https://github.com/QuincyQAQ/TDADL-IE</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11301v1">PDF</a> Accepted By BIBM 2025</p>
<p><strong>Summary</strong></p>
<p>æ•°å­—åŒ»å­¦å½±åƒæŠ€æœ¯å¦‚MRIå’ŒCTçš„å‘å±•ï¼Œè¦æ±‚è¿œç¨‹åŒ»ç–—å’Œäº‘å­˜å‚¨ä¸­çš„æ‚£è€…æ•°æ®å¿…é¡»æœ‰å¼ºå¤§çš„åŠ å¯†ä¿æŠ¤ã€‚æœ¬æ–‡æå‡ºåŸºäºä¸‰ä¸ªå…³é”®å…ƒç´ çš„å…¨æ–°ä¸‰ç»´æ‰©æ•£ç®—æ³•ä¸æ·±åº¦å­¦ä¹ å›¾åƒåŠ å¯†ç³»ç»Ÿï¼ˆTDADL-IEï¼‰ã€‚é¦–å…ˆï¼Œåˆ©ç”¨LSTMç½‘ç»œä¸ä¸€ç»´æ­£å¼¦äºŒæ¬¡æ··æ²Œæ˜ å°„ï¼ˆ1D-SQCMï¼‰æ„å»ºå¢å¼ºå‹æ··æ²Œç”Ÿæˆå™¨ï¼Œä»¥ç”Ÿæˆæ›´ä½³çš„ä¼ªéšæœºåºåˆ—ã€‚å…¶æ¬¡ï¼Œåº”ç”¨æ–°å‹ä¸‰ç»´æ‰©æ•£ç®—æ³•ï¼ˆTDAï¼‰å¯¹ç½®æ¢å›¾åƒè¿›è¡ŒåŠ å¯†ã€‚TDADL-IEé€‚ç”¨äºä»»ä½•å¤§å°çš„å›¾åƒï¼Œå®éªŒè¯å®å…¶èƒ½æœ‰æ•ˆæŠµå¾¡å„ç§å®‰å…¨å¨èƒã€‚ä»£ç å…¬å¼€äºï¼š<a target="_blank" rel="noopener" href="https://github.com/QuincyQAQ/TDADL-IE%E3%80%82">https://github.com/QuincyQAQ/TDADL-IEã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ•°å­—åŒ»å­¦å½±åƒæŠ€æœ¯çš„æ™®åŠéœ€è¦æ›´å¼ºçš„åŠ å¯†æŠ€æœ¯æ¥ä¿æŠ¤æ‚£è€…æ•°æ®åœ¨è¿œç¨‹åŒ»ç–—å’Œäº‘å­˜å‚¨ä¸­çš„å®‰å…¨ã€‚</li>
<li>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç»“åˆäº†æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„ä¸‰ç»´æ‰©æ•£ç®—æ³•å’Œå›¾åƒåŠ å¯†ç³»ç»Ÿï¼ˆTDADL-IEï¼‰ã€‚</li>
<li>é‡‡ç”¨å¢å¼ºå‹æ··æ²Œç”Ÿæˆå™¨ç”Ÿæˆä¼ªéšæœºåºåˆ—ï¼Œåˆ©ç”¨ä¸€ç»´æ­£å¼¦äºŒæ¬¡æ··æ²Œæ˜ å°„ï¼ˆ1D-SQCMï¼‰å’ŒLSTMç½‘ç»œè¿›è¡Œæ„å»ºã€‚</li>
<li>TDADL-IEä½¿ç”¨æ–°å‹ä¸‰ç»´æ‰©æ•£ç®—æ³•ï¼ˆTDAï¼‰å¯¹å›¾åƒè¿›è¡ŒåŠ å¯†ã€‚</li>
<li>è¯¥ç³»ç»Ÿé€‚ç”¨äºä»»ä½•å¤§å°çš„å›¾åƒåŠ å¯†ï¼Œå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚</li>
<li>å®éªŒè¯æ˜TDADL-IEèƒ½æœ‰æ•ˆæŠµå¾¡å¤šç§å®‰å…¨å¨èƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11301">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-1ffc763ab157cdb69b302d134c32ad7b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823876&auth_key=1760823876-0-0-afea929fb382dd1f15cd4f56eb203000&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fe12b59ccb3c3317f66f9700a7ddaa2d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823883&auth_key=1760823883-0-0-201ce2266024a3d5d8ffb5a4b3855fac&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-89043a4bed372af291c782a22a48a138~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823890&auth_key=1760823890-0-0-ddf34af32f2f30850e76de8cbc4ba09e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1fb531a48ceeb865c9c298a39edc49b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823897&auth_key=1760823897-0-0-b1f24645b6601866df729f788a0baa08&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dc013746770398627ab74cedbae0202c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823903&auth_key=1760823903-0-0-fe41eb10dfd215704522977b7b94b202&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-adecc5d717ec4d32df87cd1105118fec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823910&auth_key=1760823910-0-0-397d35e4ed896d3edde959f8f7d4d88b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="DTEA-Dynamic-Topology-Weaving-and-Instability-Driven-Entropic-Attenuation-for-Medical-Image-Segmentation"><a href="#DTEA-Dynamic-Topology-Weaving-and-Instability-Driven-Entropic-Attenuation-for-Medical-Image-Segmentation" class="headerlink" title="DTEA: Dynamic Topology Weaving and Instability-Driven Entropic   Attenuation for Medical Image Segmentation"></a>DTEA: Dynamic Topology Weaving and Instability-Driven Entropic   Attenuation for Medical Image Segmentation</h2><p><strong>Authors:Weixuan Li, Quanjun Li, Guang Yu, Song Yang, Zimeng Li, Chi-Man Pun, Yupeng Liu, Xuhang Chen</strong></p>
<p>In medical image segmentation, skip connections are used to merge global context and reduce the semantic gap between encoder and decoder. Current methods often struggle with limited structural representation and insufficient contextual modeling, affecting generalization in complex clinical scenarios. We propose the DTEA model, featuring a new skip connection framework with the Semantic Topology Reconfiguration (STR) and Entropic Perturbation Gating (EPG) modules. STR reorganizes multi-scale semantic features into a dynamic hypergraph to better model cross-resolution anatomical dependencies, enhancing structural and semantic representation. EPG assesses channel stability after perturbation and filters high-entropy channels to emphasize clinically important regions and improve spatial attention. Extensive experiments on three benchmark datasets show our framework achieves superior segmentation accuracy and better generalization across various clinical settings. The code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/LWX-Research/DTEA%7D%7Bhttps://github.com/LWX-Research/DTEA%7D">https://github.com/LWX-Research/DTEA}{https://github.com/LWX-Research/DTEA}</a>. </p>
<blockquote>
<p>åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œè·³è¿‡è¿æ¥ç”¨äºåˆå¹¶å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå¹¶å‡å°‘ç¼–ç å™¨ä¸è§£ç å™¨ä¹‹é—´çš„è¯­ä¹‰å·®è·ã€‚å½“å‰çš„æ–¹æ³•å¾€å¾€é¢ä¸´ç»“æ„è¡¨ç¤ºæœ‰é™å’Œä¸Šä¸‹æ–‡å»ºæ¨¡ä¸è¶³çš„é—®é¢˜ï¼Œå½±å“äº†åœ¨å¤æ‚ä¸´åºŠåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬æå‡ºäº†DTEAæ¨¡å‹ï¼Œå®ƒé‡‡ç”¨å…·æœ‰è¯­ä¹‰æ‹“æ‰‘é‡æ„ï¼ˆSTRï¼‰å’Œç†µæ‰°åŠ¨é—¨æ§ï¼ˆEPGï¼‰æ¨¡å—çš„æ–°å‹è·³è¿‡è¿æ¥æ¡†æ¶ã€‚STRå°†å¤šå°ºåº¦è¯­ä¹‰ç‰¹å¾é‡ç»„ä¸ºåŠ¨æ€è¶…å›¾ï¼Œä»¥æ›´å¥½åœ°å»ºæ¨¡è·¨åˆ†è¾¨ç‡çš„è§£å‰–ä¾èµ–æ€§ï¼Œå¢å¼ºç»“æ„å’Œè¯­ä¹‰è¡¨ç¤ºã€‚EPGè¯„ä¼°æ‰°åŠ¨åçš„é€šé“ç¨³å®šæ€§ï¼Œå¹¶è¿‡æ»¤é«˜ç†µé€šé“ï¼Œä»¥å¼ºè°ƒä¸´åºŠä¸Šé‡è¦çš„åŒºåŸŸå¹¶æé«˜ç©ºé—´æ³¨æ„åŠ›ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å®ç°äº†è¾ƒé«˜çš„åˆ†å‰²ç²¾åº¦ï¼Œå¹¶åœ¨å„ç§ä¸´åºŠç¯å¢ƒä¸­å…·æœ‰è¾ƒå¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/LWX-Research/DTEA">https://github.com/LWX-Research/DTEA</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11259v1">PDF</a> Accepted by BIBM 2025</p>
<p><strong>Summary</strong><br>     åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­ï¼Œä½¿ç”¨è·³è¿æ¥åˆå¹¶å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç¼©å°ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„è¯­ä¹‰é¸¿æ²Ÿã€‚å½“å‰æ–¹æ³•å­˜åœ¨ç»“æ„è¡¨å¾æœ‰é™å’Œä¸Šä¸‹æ–‡å»ºæ¨¡ä¸è¶³çš„é—®é¢˜ï¼Œå½±å“åœ¨å¤æ‚ä¸´åºŠåœºæ™¯ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚æå‡ºDTEAæ¨¡å‹ï¼Œé‡‡ç”¨å¸¦æœ‰è¯­ä¹‰æ‹“æ‰‘é‡æ„ï¼ˆSTRï¼‰å’Œç†µæ‰°åŠ¨é—¨æ§ï¼ˆEPGï¼‰æ¨¡å—çš„æ–°è·³è¿æ¥æ¡†æ¶ã€‚STRå°†å¤šå°ºåº¦è¯­ä¹‰ç‰¹å¾é‡æ„ä¸ºåŠ¨æ€è¶…å›¾ï¼Œä»¥æ›´å¥½åœ°å»ºæ¨¡è·¨åˆ†è¾¨ç‡è§£å‰–ä¾èµ–æ€§ï¼Œå¢å¼ºç»“æ„å’Œè¯­ä¹‰è¡¨å¾ã€‚EPGè¯„ä¼°æ‰°åŠ¨åçš„é€šé“ç¨³å®šæ€§ï¼Œè¿‡æ»¤é«˜ç†µé€šé“ä»¥çªå‡ºä¸´åºŠé‡è¦åŒºåŸŸï¼Œæé«˜ç©ºé—´æ³¨æ„åŠ›ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å®ç°äº†æ›´é«˜çš„åˆ†å‰²ç²¾åº¦ï¼Œå¹¶åœ¨å„ç§ä¸´åºŠç¯å¢ƒä¸­å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è·³è¿æ¥ç”¨äºåˆå¹¶å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç¼©å°ç¼–ç å™¨å’Œè§£ç å™¨é—´çš„è¯­ä¹‰é¸¿æ²Ÿã€‚</li>
<li>å½“å‰æ–¹æ³•å­˜åœ¨ç»“æ„è¡¨å¾æœ‰é™å’Œä¸Šä¸‹æ–‡å»ºæ¨¡ä¸è¶³çš„é—®é¢˜ã€‚</li>
<li>DTEAæ¨¡å‹é‡‡ç”¨æ–°çš„è·³è¿æ¥æ¡†æ¶ï¼ŒåŒ…æ‹¬è¯­ä¹‰æ‹“æ‰‘é‡æ„ï¼ˆSTRï¼‰å’Œç†µæ‰°åŠ¨é—¨æ§ï¼ˆEPGï¼‰æ¨¡å—ã€‚</li>
<li>STRé€šè¿‡åŠ¨æ€è¶…å›¾é‡æ„å¤šå°ºåº¦è¯­ä¹‰ç‰¹å¾ï¼Œå¢å¼ºç»“æ„å’Œè¯­ä¹‰è¡¨å¾ã€‚</li>
<li>EPGè¯„ä¼°é€šé“ç¨³å®šæ€§ï¼Œè¿‡æ»¤é«˜ç†µé€šé“ä»¥æé«˜ç©ºé—´æ³¨æ„åŠ›ï¼Œçªå‡ºä¸´åºŠé‡è¦åŒºåŸŸã€‚</li>
<li>åœ¨ä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDTEAæ¨¡å‹å®ç°äº†è¾ƒé«˜çš„åˆ†å‰²ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-cacffc6b1ae09f0c3079ebf307426e8a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823918&auth_key=1760823918-0-0-5d313f78e7e14641186ba492e4e67c18&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-32f7918e20c2dcc6c49840d365312a3e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823925&auth_key=1760823925-0-0-5879187914a627e9228106ee63dffca0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5ad1d12ddeb5f1b1709b16e6db886175~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823932&auth_key=1760823932-0-0-bd2d9bcce8832549496ed1b3c86e6bec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-67875eedc54a639a49f4475d56e91b19~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823939&auth_key=1760823939-0-0-788ab76a3c1b78838f0d031d80fdd3ff&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d0b73ec2b4fdd159cbda1f93d3e62045~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823979&auth_key=1760823979-0-0-d369a1ff2f509814074d025a41dca6ec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-476e92ce2b4f28619f50aa120b0e3e66~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823986&auth_key=1760823986-0-0-4b1a6f1351b93d2be2f8beaf24c2d0ec&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1aaed12ce3c09cc98e7cee66250c6ec9~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823993&auth_key=1760823993-0-0-b3f87aac698cf7c4c645f06d35f6b052&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4f720620c5c0a78294ef26685f5e3681~resize:0:q75.jpg?source=1f5c5e47&expiration=1760823999&auth_key=1760823999-0-0-e1fa543ba47451ebb29661f30286d8a0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-2d5bd67101234a7af372315082a0568e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824006&auth_key=1760824006-0-0-fabdf4fef19b1102bd6045a16e7dd12b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="G2L-From-Giga-Scale-to-Cancer-Specific-Large-Scale-Pathology-Foundation-Models-via-Knowledge-Distillation"><a href="#G2L-From-Giga-Scale-to-Cancer-Specific-Large-Scale-Pathology-Foundation-Models-via-Knowledge-Distillation" class="headerlink" title="G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation   Models via Knowledge Distillation"></a>G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation   Models via Knowledge Distillation</h2><p><strong>Authors:Yesung Cho, Sungmin Lee, Geongyu Lee, Minkyung Lee, Jongbae Park, Dongmyung Shin</strong></p>
<p>Recent studies in pathology foundation models have shown that scaling training data, diversifying cancer types, and increasing model size consistently improve their performance. However, giga-scale foundation models, which are trained on hundreds of thousands of slides covering tens of cancer types and contain billions of parameters, pose significant challenges for practical use due to their tremendous computational costs in both development and deployment. In this work, we present a novel strategy, named the G2L framework, to increase the performance of large-scale foundation models, which consist of only $15%$ of the parameters of giga-scale models, to a comparable performance level of giga-scale models in cancer-specific tasks. Our approach applies knowledge distillation, transferring the capabilities of a giga-scale model to a large-scale model, using just 1K pathology slides of a target cancer (e.g., breast, prostate, etc.). The resulting distilled model not only outperformed state-of-the-art models of the same size (i.e., large-scale) across several benchmarks but also, interestingly, surpassed the giga-scale teacher and huge-scale models in some benchmarks. In addition, the distilled model exhibited a higher robustness index, indicating improved resilience to image variations originating from multiple institutions. These findings suggest that the proposed distillation approach for a large-scale model is a data- and parameter-efficient way to achieve giga-scale-level performance for cancer-specific applications without prohibitive computational burden. </p>
<blockquote>
<p>è¿‘æœŸçš„ç—…ç†å­¦åŸºç¡€æ¨¡å‹ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æ‰©å¤§è®­ç»ƒæ•°æ®è§„æ¨¡ã€å¤šæ ·åŒ–ç™Œç—‡ç±»å‹ä»¥åŠå¢åŠ æ¨¡å‹å¤§å°ï¼Œå¯ä»¥æŒç»­æé«˜å…¶æ€§èƒ½ã€‚ç„¶è€Œï¼Œè®­ç»ƒåœ¨æ•°ä»¥åä¸‡è®¡çš„åˆ‡ç‰‡ä¸Šã€è¦†ç›–æ•°åç§ç™Œç—‡ç±»å‹å¹¶ä¸”åŒ…å«æ•°åäº¿å‚æ•°çš„åƒå…†è§„æ¨¡åŸºç¡€æ¨¡å‹ï¼Œåœ¨å¼€å‘å’Œéƒ¨ç½²æ–¹é¢éƒ½éœ€è¦å·¨å¤§çš„è®¡ç®—æˆæœ¬ï¼Œå› æ­¤åœ¨å®é™…åº”ç”¨ä¸­å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºG2Læ¡†æ¶çš„æ–°å‹ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™äº›æ¨¡å‹ä»…åŒ…å«åƒå…†æ¨¡å‹çš„15ï¼…å‚æ•°ï¼Œå°±èƒ½è¾¾åˆ°ç‰¹å®šç™Œç—‡ä»»åŠ¡çš„åƒå…†æ¨¡å‹æ€§èƒ½æ°´å¹³ã€‚æˆ‘ä»¬çš„æ–¹æ³•åº”ç”¨çŸ¥è¯†è’¸é¦ï¼Œå°†åƒå…†æ¨¡å‹çš„èƒ½åŠ›è½¬ç§»åˆ°å¤§è§„æ¨¡æ¨¡å‹ä¸Šï¼Œä»…ä½¿ç”¨ç›®æ ‡ç™Œç—‡ï¼ˆä¾‹å¦‚ä¹³è…ºç™Œã€å‰åˆ—è…ºç™Œç­‰ï¼‰çš„1000ä¸ªç—…ç†åˆ‡ç‰‡ã€‚å¾—åˆ°çš„è’¸é¦æ¨¡å‹ä¸ä»…åœ¨åŒä¸€è§„æ¨¡ï¼ˆå³å¤§è§„æ¨¡ï¼‰çš„æ¨¡å‹ä¸­è¡¨ç°å‡ºæœ€ä½³æ€§èƒ½ï¼Œè€Œä¸”åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†åƒå…†æ•™å¸ˆæ¨¡å‹å’Œå¤§è§„æ¨¡æ¨¡å‹ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹ç”šè‡³è¶…è¿‡äº†åƒå…†æ•™å¸ˆæ¨¡å‹å’Œè¶…å¤§è§„æ¨¡æ¨¡å‹ã€‚æ­¤å¤–ï¼Œè’¸é¦æ¨¡å‹å±•ç°å‡ºæ›´é«˜çš„ç¨³å¥æ€§æŒ‡æ•°ï¼Œè¡¨æ˜å…¶å¯¹äºæ¥è‡ªå¤šä¸ªæœºæ„çš„å›¾åƒå˜åŒ–çš„é€‚åº”æ€§æ›´å¼ºã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œé’ˆå¯¹å¤§è§„æ¨¡æ¨¡å‹æå‡ºçš„è’¸é¦æ–¹æ³•æ˜¯ä¸€ç§æ•°æ®é«˜æ•ˆä¸”å‚æ•°é«˜æ•ˆçš„æ–¹å¼ï¼Œå¯åœ¨ä¸æ‰¿å—å·¨å¤§è®¡ç®—è´Ÿæ‹…çš„æƒ…å†µä¸‹å®ç°åƒå…†è§„æ¨¡çš„ç™Œç—‡ç‰¹å¼‚æ€§åº”ç”¨æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11176v1">PDF</a> </p>
<p><strong>Summary</strong><br>    æœ¬æ–‡æå‡ºä¸€ç§åä¸ºG2Læ¡†æ¶çš„æ–°å‹ç­–ç•¥ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œå°†å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹çš„æ€§èƒ½æå‡åˆ°ä¸åƒå…†è§„æ¨¡æ¨¡å‹ç›¸å½“çš„æ°´å¹³ï¼Œä¸”ä»…ä½¿ç”¨ç›®æ ‡ç™Œç—‡ï¼ˆå¦‚ä¹³è…ºç™Œã€å‰åˆ—è…ºç™Œç­‰ï¼‰çš„1Kç—…ç†åˆ‡ç‰‡ã€‚è¿™ç§è’¸é¦æ¨¡å‹ä¸ä»…åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰å¤§è§„æ¨¡æ¨¡å‹ï¼Œè¿˜åœ¨æŸäº›åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†åŸå§‹åƒå…†è§„æ¨¡æ¨¡å‹ï¼Œå¹¶å±•ç°å‡ºæ›´é«˜çš„ç¨³å¥æ€§æŒ‡æ•°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶è¡¨æ˜ï¼Œæ‰©å¤§è®­ç»ƒæ•°æ®ã€å¤šæ ·åŒ–ç™Œç—‡ç±»å‹å’Œå¢åŠ æ¨¡å‹è§„æ¨¡èƒ½æŒç»­æå‡ç—…ç†åŸºç¡€æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åƒäº¿è§„æ¨¡çš„åŸºç¡€æ¨¡å‹è™½ç„¶æ€§èƒ½ä¼˜è¶Šï¼Œä½†åœ¨å¼€å‘å’Œéƒ¨ç½²è¿‡ç¨‹ä¸­çš„è®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æå‡ºG2Læ¡†æ¶ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œä»…ä½¿ç”¨1Kç›®æ ‡ç™Œç—‡ç—…ç†åˆ‡ç‰‡ï¼Œæé«˜å¤§è§„æ¨¡æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>è’¸é¦æ¨¡å‹åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ç°æœ‰å¤§è§„æ¨¡æ¨¡å‹ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹è¶…è¶Šäº†åŸå§‹åƒå…†è§„æ¨¡æ¨¡å‹ã€‚</li>
<li>è’¸é¦æ¨¡å‹å±•ç°å‡ºæ›´é«˜çš„ç¨³å¥æ€§æŒ‡æ•°ï¼Œå¯¹æ¥è‡ªå¤šä¸ªæœºæ„çš„å›¾åƒå˜åŒ–æ›´å…·æŠ—æ€§ã€‚</li>
<li>è’¸é¦ç­–ç•¥æ˜¯ä¸€ç§æ•°æ®é«˜æ•ˆã€å‚æ•°æœ‰æ•ˆçš„æ–¹æ³•ï¼Œæ— éœ€å·¨å¤§çš„è®¡ç®—è´Ÿæ‹…å³å¯å®ç°é’ˆå¯¹ç™Œç—‡ç‰¹å®šåº”ç”¨çš„åƒå…†è§„æ¨¡æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11176">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-8013a3fade2881759343785605c7937d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824048&auth_key=1760824048-0-0-2e789e220031664b2ce0f5168800a592&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-55d477e087c44f618680bf06ba55e2e5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824055&auth_key=1760824055-0-0-acfb2009e6eff250631e8f9b221a3341&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ba685e9568078eb2b86dba8519a6142f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824062&auth_key=1760824062-0-0-cf985940887e9aebe4ec2a1694f6ce25&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-8f30cd29af5458b7250f1e11eaed56c4~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824068&auth_key=1760824068-0-0-070058ebc3728c46bae1136872d00b3c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-0c96406adc0e38c6889b2c7b94a16e82~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824074&auth_key=1760824074-0-0-4e004610222401f95a188b58937a33f8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Text-Enhanced-Panoptic-Symbol-Spotting-in-CAD-Drawings"><a href="#Text-Enhanced-Panoptic-Symbol-Spotting-in-CAD-Drawings" class="headerlink" title="Text-Enhanced Panoptic Symbol Spotting in CAD Drawings"></a>Text-Enhanced Panoptic Symbol Spotting in CAD Drawings</h2><p><strong>Authors:Xianlin Liu, Yan Gong, Bohao Li, Jiajing Huang, Bowen Du, Junchen Ye, Liyan Xu</strong></p>
<p>With the widespread adoption of Computer-Aided Design(CAD) drawings in engineering, architecture, and industrial design, the ability to accurately interpret and analyze these drawings has become increasingly critical. Among various subtasks, panoptic symbol spotting plays a vital role in enabling downstream applications such as CAD automation and design retrieval. Existing methods primarily focus on geometric primitives within the CAD drawings to address this task, but they face following major problems: they usually overlook the rich textual annotations present in CAD drawings and they lack explicit modeling of relationships among primitives, resulting in incomprehensive understanding of the holistic drawings. To fill this gap, we propose a panoptic symbol spotting framework that incorporates textual annotations. The framework constructs unified representations by jointly modeling geometric and textual primitives. Then, using visual features extract by pretrained CNN as the initial representations, a Transformer-based backbone is employed, enhanced with a type-aware attention mechanism to explicitly model the different types of spatial dependencies between various primitives. Extensive experiments on the real-world dataset demonstrate that the proposed method outperforms existing approaches on symbol spotting tasks involving textual annotations, and exhibits superior robustness when applied to complex CAD drawings. </p>
<blockquote>
<p>éšç€è®¡ç®—æœºè¾…åŠ©è®¾è®¡ï¼ˆCADï¼‰ç»˜å›¾åœ¨å·¥ç¨‹ã€å»ºç­‘å’Œå·¥ä¸šè®¾è®¡ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå‡†ç¡®è§£è¯»å’Œåˆ†æè¿™äº›ç»˜å›¾çš„èƒ½åŠ›å˜å¾—è‡³å…³é‡è¦ã€‚åœ¨ä¼—å¤šå­ä»»åŠ¡ä¸­ï¼Œå…¨æ™¯ç¬¦å·è¯†åˆ«å¯¹äºä¸‹æ¸¸åº”ç”¨å¦‚CADè‡ªåŠ¨åŒ–å’Œè®¾è®¡æ£€ç´¢ç­‰å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨CADç»˜å›¾ä¸­çš„å‡ ä½•å…ƒç´ æ¥è§£å†³è¿™ä¸€ä»»åŠ¡ï¼Œä½†å®ƒä»¬é¢ä¸´ä»¥ä¸‹é—®é¢˜ï¼šå®ƒä»¬é€šå¸¸å¿½ç•¥äº†CADç»˜å›¾ä¸­çš„ä¸°å¯Œæ–‡æœ¬æ³¨é‡Šï¼Œç¼ºä¹å¯¹å…ƒç´ é—´å…³ç³»çš„æ˜¾å¼å»ºæ¨¡ï¼Œå¯¼è‡´å¯¹æ•´ä½“ç»˜å›¾çš„ç»¼åˆç†è§£ä¸è¶³ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç»“åˆæ–‡æœ¬æ³¨é‡Šçš„å…¨æ™¯ç¬¦å·è¯†åˆ«æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡è”åˆå»ºæ¨¡å‡ ä½•å’Œæ–‡æœ¬å…ƒç´ æ¥æ„å»ºç»Ÿä¸€è¡¨ç¤ºã€‚ç„¶åï¼Œä½¿ç”¨ç”±é¢„è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œæå–çš„è§†è§‰ç‰¹å¾ä½œä¸ºåˆå§‹è¡¨ç¤ºï¼Œé‡‡ç”¨åŸºäºTransformerçš„éª¨å¹²ç½‘ï¼Œå¹¶è¾…ä»¥ç±»å‹æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»¥æ˜¾å¼åœ°æ¨¡æ‹Ÿå„ç§å…ƒç´ ä¹‹é—´çš„ä¸åŒç±»å‹ç©ºé—´ä¾èµ–æ€§ã€‚åœ¨çœŸå®æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æ¶‰åŠæ–‡æœ¬æ³¨é‡Šçš„ç¬¦å·è¯†åˆ«ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨åº”ç”¨äºå¤æ‚çš„CADç»˜å›¾æ—¶è¡¨ç°å‡ºå“è¶Šçš„ç¨³å¥æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.11091v1">PDF</a> 7 pages, 3figures. This version is the original submitted manuscript   of the paper accepted by The 12th International Conference on Behavioural and   Social Computing</p>
<p><strong>Summary</strong></p>
<p>CADç»˜å›¾è§£è¯»ä¸­çš„å…¨æ™¯ç¬¦å·è¯†åˆ«æŠ€æœ¯è‡³å…³é‡è¦ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨CADç»˜å›¾ä¸­çš„å‡ ä½•åŸºæœ¬å…ƒç´ ï¼Œä½†å¿½ç•¥äº†æ–‡æœ¬æ ‡æ³¨å’Œå…ƒç´ é—´å…³ç³»çš„å»ºæ¨¡ã€‚æœ¬æ–‡æå‡ºä¸€ä¸ªç»“åˆæ–‡æœ¬æ ‡æ³¨çš„å…¨æ™¯ç¬¦å·è¯†åˆ«æ¡†æ¶ï¼Œé€šè¿‡è”åˆå»ºæ¨¡å‡ ä½•å’Œæ–‡æœ¬åŸºæœ¬å…ƒç´ æ„å»ºç»Ÿä¸€è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨åŸºäºè§†è§‰ç‰¹å¾çš„é¢„è®­ç»ƒCNNæå–åˆå§‹è¡¨ç¤ºï¼Œåˆ©ç”¨å¸¦ç±»å‹æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶çš„Transformeréª¨å¹²ç½‘ç»œå»ºæ¨¡å„ç§å…ƒç´ é—´çš„ç©ºé—´ä¾èµ–å…³ç³»ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨åŒ…å«æ–‡æœ¬æ ‡æ³¨çš„ç¬¦å·è¯†åˆ«ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¯¹å¤æ‚CADç»˜å›¾è¡¨ç°å‡ºè¾ƒå¼ºçš„é²æ£’æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CADç»˜å›¾è§£è¯»ä¸­çš„å…¨æ™¯ç¬¦å·è¯†åˆ«æŠ€æœ¯å¾ˆé‡è¦ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å‡ ä½•åŸºæœ¬å…ƒç´ ï¼Œå¿½ç•¥æ–‡æœ¬æ ‡æ³¨å’Œå…ƒç´ é—´å…³ç³»çš„å»ºæ¨¡ã€‚</li>
<li>æœ¬æ–‡æå‡ºç»“åˆæ–‡æœ¬æ ‡æ³¨çš„å…¨æ™¯ç¬¦å·è¯†åˆ«æ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶é€šè¿‡è”åˆå»ºæ¨¡å‡ ä½•å’Œæ–‡æœ¬åŸºæœ¬å…ƒç´ æ„å»ºç»Ÿä¸€è¡¨ç¤ºã€‚</li>
<li>é‡‡ç”¨é¢„è®­ç»ƒCNNæå–è§†è§‰ç‰¹å¾ä½œä¸ºåˆå§‹è¡¨ç¤ºã€‚</li>
<li>ä½¿ç”¨å¸¦ç±»å‹æ„ŸçŸ¥æ³¨æ„åŠ›æœºåˆ¶çš„Transformeréª¨å¹²ç½‘ç»œå»ºæ¨¡å…ƒç´ é—´çš„ç©ºé—´ä¾èµ–å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.11091">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e0f59bda69fb1ba3103a949c39ed2218~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824082&auth_key=1760824082-0-0-0ffbe5689b078960c219bbe6974c9ee3&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-efa86bcd38ad3e8947bc2f5e0bc2a44b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824089&auth_key=1760824089-0-0-d1ac9d57ba8d7f28dd933518681230fc&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5943ab28068710e95feb5a1883f9e7be~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824096&auth_key=1760824096-0-0-59875d91d0a5268d959846f43b9418ef&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f3fda353b7f236f72f8f0b30cbe74110~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824103&auth_key=1760824103-0-0-ae0896eae39ea2eafac137dde27f867d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MSM-Seg-A-Modality-and-Slice-Memory-Framework-with-Category-Agnostic-Prompting-for-Multi-Modal-Brain-Tumor-Segmentation"><a href="#MSM-Seg-A-Modality-and-Slice-Memory-Framework-with-Category-Agnostic-Prompting-for-Multi-Modal-Brain-Tumor-Segmentation" class="headerlink" title="MSM-Seg: A Modality-and-Slice Memory Framework with Category-Agnostic   Prompting for Multi-Modal Brain Tumor Segmentation"></a>MSM-Seg: A Modality-and-Slice Memory Framework with Category-Agnostic   Prompting for Multi-Modal Brain Tumor Segmentation</h2><p><strong>Authors:Yuxiang Luo, Qing Xu, Hai Huang, Yuqi Ouyang, Zhen Chen, Wenting Duan</strong></p>
<p>Multi-modal brain tumor segmentation is critical for clinical diagnosis, and it requires accurate identification of distinct internal anatomical subregions. While the recent prompt-based segmentation paradigms enable interactive experiences for clinicians, existing methods ignore cross-modal correlations and rely on labor-intensive category-specific prompts, limiting their applicability in real-world scenarios. To address these issues, we propose a MSM-Seg framework for multi-modal brain tumor segmentation. The MSM-Seg introduces a novel dual-memory segmentation paradigm that synergistically integrates multi-modal and inter-slice information with the efficient category-agnostic prompt for brain tumor understanding. To this end, we first devise a modality-and-slice memory attention (MSMA) to exploit the cross-modal and inter-slice relationships among the input scans. Then, we propose a multi-scale category-agnostic prompt encoder (MCP-Encoder) to provide tumor region guidance for decoding. Moreover, we devise a modality-adaptive fusion decoder (MF-Decoder) that leverages the complementary decoding information across different modalities to improve segmentation accuracy. Extensive experiments on different MRI datasets demonstrate that our MSM-Seg framework outperforms state-of-the-art methods in multi-modal metastases and glioma tumor segmentation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xq141839/MSM-Seg">https://github.com/xq141839/MSM-Seg</a>. </p>
<blockquote>
<p>å¤šæ¨¡æ€è„‘è‚¿ç˜¤åˆ†å‰²å¯¹äºä¸´åºŠè¯Šæ–­è‡³å…³é‡è¦ï¼Œå®ƒè¦æ±‚å‡†ç¡®è¯†åˆ«ä¸åŒçš„å†…éƒ¨è§£å‰–äºšåŒºã€‚è™½ç„¶åŸºäºæç¤ºçš„åˆ†å‰²èŒƒå¼æœ€è¿‘ä½¿ä¸´åºŠåŒ»ç”Ÿèƒ½å¤Ÿæ‹¥æœ‰äº¤äº’å¼ä½“éªŒï¼Œä½†ç°æœ‰æ–¹æ³•å¿½ç•¥äº†è·¨æ¨¡æ€çš„å…³è”ï¼Œå¹¶ä¾èµ–äºåŠ³åŠ¨å¯†é›†å‹çš„ç‰¹å®šç±»åˆ«æç¤ºï¼Œé™åˆ¶äº†å®ƒä»¬åœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šæ¨¡æ€è„‘è‚¿ç˜¤åˆ†å‰²çš„MSM-Segæ¡†æ¶ã€‚MSM-Segå¼•å…¥äº†ä¸€ç§æ–°å‹åŒå†…å­˜åˆ†å‰²èŒƒå¼ï¼Œè¯¥èŒƒå¼ååŒæ•´åˆå¤šæ¨¡æ€å’Œè·¨åˆ‡ç‰‡ä¿¡æ¯ï¼Œä»¥åŠæœ‰æ•ˆçš„ç±»åˆ«æ— å…³æç¤ºï¼Œä»¥ä¿ƒè¿›å¯¹è„‘è‚¿ç˜¤çš„ç†è§£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ç§æ¨¡æ€å’Œåˆ‡ç‰‡å†…å­˜æ³¨æ„åŠ›ï¼ˆMSMAï¼‰ï¼Œä»¥åˆ©ç”¨è¾“å…¥æ‰«æä¹‹é—´çš„è·¨æ¨¡æ€å’Œè·¨åˆ‡ç‰‡å…³ç³»ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šå°ºåº¦ç±»åˆ«æ— å…³æç¤ºç¼–ç å™¨ï¼ˆMCP-Encoderï¼‰ï¼Œä»¥ä¸ºè§£ç æä¾›è‚¿ç˜¤åŒºåŸŸæŒ‡å¯¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ¨¡æ€è‡ªé€‚åº”èåˆè§£ç å™¨ï¼ˆMF-Decoderï¼‰ï¼Œè¯¥è§£ç å™¨åˆ©ç”¨ä¸åŒæ¨¡æ€ä¹‹é—´çš„äº’è¡¥è§£ç ä¿¡æ¯æ¥æé«˜åˆ†å‰²ç²¾åº¦ã€‚åœ¨ä¸åŒMRIæ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MSM-Segæ¡†æ¶åœ¨å¤šæ¨¡æ€è½¬ç§»å’Œèƒ¶è´¨ç˜¤è‚¿ç˜¤åˆ†å‰²æ–¹é¢çš„æ€§èƒ½ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xq141839/MSM-Seg%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/xq141839/MSM-Segä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10679v1">PDF</a> Under Review</p>
<p><strong>Summary</strong><br>å¤šæ¨¡æ€è„‘è‚¿ç˜¤åˆ†å‰²å¯¹ä¸´åºŠè¯Šæ–­è‡³å…³é‡è¦ï¼Œéœ€è¦å‡†ç¡®è¯†åˆ«ä¸åŒçš„å†…éƒ¨è§£å‰–äºšåŒºã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å¿½è§†è·¨æ¨¡æ€å…³è”å¹¶ä¾èµ–åŠ³åŠ¨å¯†é›†å‹çš„ç‰¹å®šç±»åˆ«æç¤ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†MSM-Segæ¡†æ¶è¿›è¡Œå¤šæ¨¡æ€è„‘è‚¿ç˜¤åˆ†å‰²ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸€ç§æ–°å‹åŒè®°å¿†åˆ†å‰²èŒƒå¼ï¼ŒååŒæ•´åˆå¤šæ¨¡æ€å’Œè·¨åˆ‡ç‰‡ä¿¡æ¯ï¼Œå¹¶å€ŸåŠ©é«˜æ•ˆçš„ç±»åˆ«æ— å…³æç¤ºè¿›è¡Œè„‘è‚¿ç˜¤ç†è§£ã€‚é€šè¿‡æ¨¡æ€å’Œåˆ‡ç‰‡è®°å¿†æ³¨æ„åŠ›ï¼ˆMSMAï¼‰æœºåˆ¶æŒ–æ˜è¾“å…¥æ‰«æä¸­çš„è·¨æ¨¡æ€å’Œè·¨åˆ‡ç‰‡å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šå°ºåº¦ç±»åˆ«æ— å…³æç¤ºç¼–ç å™¨ï¼ˆMCP-Encoderï¼‰ä»¥æä¾›è‚¿ç˜¤åŒºåŸŸæŒ‡å¯¼è¿›è¡Œè§£ç ï¼Œå¹¶è®¾è®¡äº†æ¨¡æ€è‡ªé€‚åº”èåˆè§£ç å™¨ï¼ˆMF-Decoderï¼‰ï¼Œåˆ©ç”¨ä¸åŒæ¨¡æ€çš„äº’è¡¥è§£ç ä¿¡æ¯æé«˜åˆ†å‰²ç²¾åº¦ã€‚åœ¨å¤šä¸ªMRIæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„MSM-Segæ¡†æ¶åœ¨å¤šæ¨¡æ€è½¬ç§»å’Œèƒ¶è´¨ç˜¤è‚¿ç˜¤åˆ†å‰²æ–¹é¢çš„æ€§èƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤šæ¨¡æ€è„‘è‚¿ç˜¤åˆ†å‰²å¯¹ä¸´åºŠè¯Šæ–­éå¸¸é‡è¦ï¼Œéœ€è¦ç²¾å‡†è¯†åˆ«å†…éƒ¨è§£å‰–äºšåŒºã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨å¿½è§†è·¨æ¨¡æ€å…³è”å’Œä¾èµ–åŠ³åŠ¨å¯†é›†å‹ç‰¹å®šç±»åˆ«æç¤ºçš„é—®é¢˜ã€‚</li>
<li>æå‡ºäº†MSM-Segæ¡†æ¶ï¼Œé‡‡ç”¨æ–°å‹åŒè®°å¿†åˆ†å‰²èŒƒå¼è¿›è¡Œå¤šæ¨¡æ€è„‘è‚¿ç˜¤åˆ†å‰²ã€‚</li>
<li>è¯¥æ¡†æ¶æ•´åˆäº†å¤šæ¨¡æ€å’Œè·¨åˆ‡ç‰‡ä¿¡æ¯ï¼Œå¹¶å€ŸåŠ©é«˜æ•ˆçš„ç±»åˆ«æ— å…³æç¤ºè¿›è¡Œè„‘è‚¿ç˜¤ç†è§£ã€‚</li>
<li>MSM-Segæ¡†æ¶é€šè¿‡æ¨¡æ€å’Œåˆ‡ç‰‡è®°å¿†æ³¨æ„åŠ›æœºåˆ¶æŒ–æ˜è¾“å…¥æ‰«æä¸­çš„è·¨æ¨¡æ€å’Œè·¨åˆ‡ç‰‡å…³ç³»ã€‚</li>
<li>å¤šå°ºåº¦ç±»åˆ«æ— å…³æç¤ºç¼–ç å™¨å’Œæ¨¡æ€è‡ªé€‚åº”èåˆè§£ç å™¨æé«˜äº†è‚¿ç˜¤åˆ†å‰²ç²¾åº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10679">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-c599349cd399ac3ec285af2a850ca1fb~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824111&auth_key=1760824111-0-0-fb550548ca4052dc1662ced31c79aea2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-10cea39b2e292f6fb0f87e88f2565568~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824119&auth_key=1760824119-0-0-7fb5f396984d55938786b1bc5239de69&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-6eb0726461d7fc8a4d936f76a3064fd3~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824126&auth_key=1760824126-0-0-4ffdf66ba6a79541b661e2d2f4400003&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b08df17afe889204679e7577a59eed9d~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824132&auth_key=1760824132-0-0-7a10a7686f8d262304f3cb8f8b91a62c&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5b429d2237d61848b9b80afc7bc5a75a~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824139&auth_key=1760824139-0-0-4d4cdfbffb59cdd3397e5c72e8b91637&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Learning-from-Disagreement-A-Group-Decision-Simulation-Framework-for-Robust-Medical-Image-Segmentation"><a href="#Learning-from-Disagreement-A-Group-Decision-Simulation-Framework-for-Robust-Medical-Image-Segmentation" class="headerlink" title="Learning from Disagreement: A Group Decision Simulation Framework for   Robust Medical Image Segmentation"></a>Learning from Disagreement: A Group Decision Simulation Framework for   Robust Medical Image Segmentation</h2><p><strong>Authors:Chen Zhong, Yuxuan Yang, Xinyue Zhang, Ruohan Ma, Yong Guo, Gang Li, Jupeng Li</strong></p>
<p>Medical image segmentation annotation suffers from inter-rater variability (IRV) due to differences in annotatorsâ€™ expertise and the inherent blurriness of medical images. Standard approaches that simply average expert labels are flawed, as they discard the valuable clinical uncertainty revealed in disagreements. We introduce a fundamentally new approach with our group decision simulation framework, which works by mimicking the collaborative decision-making process of a clinical panel. Under this framework, an Expert Signature Generator (ESG) learns to represent individual annotator styles in a unique latent space. A Simulated Consultation Module (SCM) then intelligently generates the final segmentation by sampling from this space. This method achieved state-of-the-art results on challenging CBCT and MRI datasets (92.11% and 90.72% Dice scores). By treating expert disagreement as a useful signal instead of noise, our work provides a clear path toward more robust and trustworthy AI systems for healthcare. </p>
<blockquote>
<p>åŒ»å­¦å›¾åƒåˆ†å‰²æ ‡æ³¨å­˜åœ¨è¯„ä¼°è€…é—´å˜å¼‚ï¼ˆIRVï¼‰çš„é—®é¢˜ï¼Œè¿™æ˜¯ç”±äºæ ‡æ³¨äººå‘˜çš„ä¸“ä¸šå·®å¼‚å’ŒåŒ»å­¦å›¾åƒæœ¬èº«çš„æ¨¡ç³Šæ€§æ‰€å¯¼è‡´çš„ã€‚ä¸€äº›ç®€å•åœ°å¹³å‡ä¸“å®¶æ ‡ç­¾çš„æ ‡å‡†æ–¹æ³•å­˜åœ¨ç¼ºé™·ï¼Œå› ä¸ºå®ƒä»¬å¿½ç•¥äº†åˆ†æ­§ä¸­ä½“ç°å‡ºçš„æœ‰ä»·å€¼çš„ä¸´åºŠä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å…¨æ–°çš„æ–¹æ³•ï¼Œå³æˆ‘ä»¬çš„ç¾¤ä½“å†³ç­–æ¨¡æ‹Ÿæ¡†æ¶ï¼Œå®ƒé€šè¿‡æ¨¡æ‹Ÿä¸´åºŠå°ç»„çš„åˆä½œå†³ç­–è¿‡ç¨‹æ¥å‘æŒ¥ä½œç”¨ã€‚åœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œä¸“å®¶ç­¾åç”Ÿæˆå™¨ï¼ˆESGï¼‰å­¦ä¹ åœ¨ç‹¬ç‰¹çš„æ½œåœ¨ç©ºé—´ä¸­è¡¨ç¤ºå•ä¸ªæ³¨é‡Šè€…çš„é£æ ¼ã€‚ç„¶åï¼Œæ¨¡æ‹Ÿå’¨è¯¢æ¨¡å—ï¼ˆSCMï¼‰é€šè¿‡ä»æ­¤ç©ºé—´ä¸­é‡‡æ ·æ™ºèƒ½ç”Ÿæˆæœ€ç»ˆçš„åˆ†å‰²ã€‚è¯¥æ–¹æ³•åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„CBCå’ŒMRIæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼ˆDiceå¾—åˆ†åˆ†åˆ«ä¸º92.11%å’Œ90.72%ï¼‰ã€‚é€šè¿‡å°†ä¸“å®¶åˆ†æ­§è§†ä¸ºæœ‰ç”¨çš„ä¿¡å·è€Œéå™ªéŸ³ï¼Œæˆ‘ä»¬çš„å·¥ä½œä¸ºåŒ»ç–—ä¿å¥é¢†åŸŸæä¾›æ›´ç¨³å¥å’Œå¯ä¿¡èµ–çš„AIç³»ç»Ÿæä¾›äº†æ˜ç¡®çš„é€”å¾„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10462v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²æ ‡æ³¨å­˜åœ¨è¯„æ³¨äººå˜å¼‚æ€§é—®é¢˜ï¼Œå› ä¸ºæ ‡æ³¨äººçš„ä¸“ä¸šå·®å¼‚å’ŒåŒ»å­¦å›¾åƒæœ¬èº«çš„æ¨¡ç³Šæ€§ã€‚æ–°çš„å†³ç­–æ¨¡æ‹Ÿæ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿä¸´åºŠå°ç»„çš„åä½œå†³ç­–è¿‡ç¨‹æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚è¯¥æ–¹æ³•å­¦ä¹ ä»£è¡¨ä¸ªä½“æ³¨é‡Šå™¨é£æ ¼çš„ä¸“å®¶ç­¾åç”Ÿæˆå™¨ï¼ˆESGï¼‰ï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿåå•†æ¨¡å—ï¼ˆSCMï¼‰æ™ºèƒ½ç”Ÿæˆæœ€ç»ˆåˆ†å‰²ç»“æœã€‚æ­¤æ–¹æ³•åœ¨å¤æ‚çš„CBCCTå’ŒMRIæ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°ç»“æœï¼Œè¯æ˜äº†å¯¹å¾…ä¸“å®¶åˆ†æ­§ä½œä¸ºä¸€ç§æœ‰ç”¨ä¿¡å·è€Œéå™ªéŸ³çš„æœ‰æ•ˆæ€§ã€‚æ­¤å·¥ä½œä¸ºå®ç°æ›´ä¸ºç¨³å¥å¯é çš„åŒ»ç–—äººå·¥æ™ºèƒ½ç³»ç»ŸæŒ‡æ˜äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²æ ‡æ³¨å­˜åœ¨è¯„æ³¨äººå˜å¼‚æ€§é—®é¢˜ã€‚</li>
<li>ä¼ ç»Ÿæ–¹æ³•ç®€å•å¹³å‡ä¸“å®¶æ ‡ç­¾å­˜åœ¨é—®é¢˜ï¼Œæ— æ³•ä½“ç°ä¸´åºŠä¸ç¡®å®šæ€§ã€‚</li>
<li>æå‡ºå…¨æ–°çš„å†³ç­–æ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ¨¡æ‹Ÿä¸´åºŠå°ç»„çš„åä½œå†³ç­–è¿‡ç¨‹ã€‚</li>
<li>é€šè¿‡ä¸“å®¶ç­¾åç”Ÿæˆå™¨ï¼ˆESGï¼‰å­¦ä¹ ä¸ªä½“æ³¨é‡Šå™¨é£æ ¼ã€‚</li>
<li>æ¨¡æ‹Ÿåå•†æ¨¡å—ï¼ˆSCMï¼‰æ™ºèƒ½ç”Ÿæˆæœ€ç»ˆåˆ†å‰²ç»“æœã€‚</li>
<li>åœ¨CBCCTå’ŒMRIæ•°æ®é›†ä¸Šå–å¾—æœ€æ–°ç»“æœï¼Œè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10462">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-a001df14f5c8ecb548f1d90be2dc31e7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824146&auth_key=1760824146-0-0-cccf526d7d9eeec62b5dadfbbbaa987e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-069c158fbe40820c3b94d3159bdcfb62~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824154&auth_key=1760824154-0-0-17dd2b1d34b3dab0c4eac5d447a684ea&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-cca0ce3f153df4a2898f10574342f23c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824160&auth_key=1760824160-0-0-949edee04d3b086e3121e1f1b95d50c8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f9563853ff5dc6bcfb0e64c12850e071~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824166&auth_key=1760824166-0-0-444afb1d0d4e9de9721d592147ee7ecd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-61d8539c75b022b1be9f6ba450d4a859~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824173&auth_key=1760824173-0-0-fb2590f77038c77916ad9839af7e8767&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-640036f47e6d48d29b706142da08e7b6~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824180&auth_key=1760824180-0-0-aa98ae91bfb5f550f207299a26b64f00&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Traj-CoA-Patient-Trajectory-Modeling-via-Chain-of-Agents-for-Lung-Cancer-Risk-Prediction"><a href="#Traj-CoA-Patient-Trajectory-Modeling-via-Chain-of-Agents-for-Lung-Cancer-Risk-Prediction" class="headerlink" title="Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung   Cancer Risk Prediction"></a>Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung   Cancer Risk Prediction</h2><p><strong>Authors:Sihang Zeng, Yujuan Fu, Sitong Zhou, Zixuan Yu, Lucas Jing Liu, Jun Wen, Matthew Thompson, Ruth Etzioni, Meliha Yetisgen</strong></p>
<p>Large language models (LLMs) offer a generalizable approach for modeling patient trajectories, but suffer from the long and noisy nature of electronic health records (EHR) data in temporal reasoning. To address these challenges, we introduce Traj-CoA, a multi-agent system involving chain-of-agents for patient trajectory modeling. Traj-CoA employs a chain of worker agents to process EHR data in manageable chunks sequentially, distilling critical events into a shared long-term memory module, EHRMem, to reduce noise and preserve a comprehensive timeline. A final manager agent synthesizes the worker agentsâ€™ summary and the extracted timeline in EHRMem to make predictions. In a zero-shot one-year lung cancer risk prediction task based on five-year EHR data, Traj-CoA outperforms baselines of four categories. Analysis reveals that Traj-CoA exhibits clinically aligned temporal reasoning, establishing it as a promisingly robust and generalizable approach for modeling complex patient trajectories. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºç—…äººè½¨è¿¹å»ºæ¨¡æä¾›äº†ä¸€ç§é€šç”¨æ–¹æ³•ï¼Œä½†å…¶åœ¨å¤„ç†æ—¶é—´åºåˆ—æ¨ç†æ—¶é¢ä¸´ç€ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ•°æ®å­˜åœ¨çš„æ—¶é—´é•¿å’Œå˜ˆæ‚é—®é¢˜ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Traj-CoAç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶‰åŠä»£ç†é“¾çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºç—…äººè½¨è¿¹å»ºæ¨¡ã€‚Traj-CoAé‡‡ç”¨ä¸€ç³»åˆ—æ™ºèƒ½ä½“æ¥é¡ºåºå¤„ç†å¯ç®¡ç†çš„ç”µå­å¥åº·è®°å½•æ•°æ®å—ï¼Œå°†æ•°æ®ä¸­çš„å…³é”®äº‹ä»¶è’¸é¦åˆ°ä¸€ä¸ªå…±äº«çš„é•¿æœŸè®°å¿†æ¨¡å—EHRMemä¸­ï¼Œä»¥å‡å°‘å™ªéŸ³å¹¶ä¿ç•™å…¨é¢çš„æ—¶é—´çº¿ã€‚æœ€åçš„ç®¡ç†æ™ºèƒ½ä½“ä¼šæ ¹æ®å·¥äººæ™ºèƒ½ä½“çš„æ€»ç»“å’ŒEHRMemä¸­æå–çš„æ—¶é—´çº¿è¿›è¡Œé¢„æµ‹ã€‚åœ¨ä¸€ä¸ªåŸºäºäº”å¹´ç”µå­å¥åº·è®°å½•æ•°æ®çš„é›¶æ ·æœ¬è‚ºç™Œé£é™©é¢„æµ‹ä»»åŠ¡ä¸­ï¼ŒTraj-CoAçš„è¡¨ç°ä¼˜äºå››ä¸ªç±»åˆ«çš„åŸºçº¿æ¨¡å‹ã€‚åˆ†æè¡¨æ˜ï¼ŒTraj-CoAå±•ç°äº†ä¸ä¸´åºŠç›¸ç¬¦çš„æ—¶é—´åºåˆ—æ¨ç†èƒ½åŠ›ï¼Œè¯æ˜å®ƒæ˜¯ä¸€ç§ç¨³å¥ä¸”é€šç”¨çš„å»ºæ¨¡å¤æ‚ç—…äººè½¨è¿¹çš„å¯é æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10454v1">PDF</a> Accepted by NeurIPS 2025 GenAI4Health Workshop</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ‚£è€…è½¨è¿¹å»ºæ¨¡ä¸­çš„é€šç”¨æ€§å’Œç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ•°æ®åœ¨æ—¶åºæ¨ç†ä¸­çš„é•¿å™ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†Traj-CoAå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿé€šè¿‡ä¸€ç³»åˆ—å·¥ä½œæ™ºèƒ½ä½“å¤„ç†EHRæ•°æ®ï¼Œå°†å…³é”®äº‹ä»¶è’¸é¦åˆ°å…±äº«çš„é•¿æœŸå†…å­˜æ¨¡å—EHRMemä¸­ï¼Œä»¥å‡å°å™ªå£°å¹¶ä¿ç•™å…¨é¢çš„æ—¶é—´çº¿ã€‚æœ€ç»ˆç®¡ç†è€…æ™ºèƒ½ä½“ç»“åˆäº†å·¥ä½œæ™ºèƒ½ä½“çš„æ‘˜è¦å’ŒEHRMemä¸­æå–çš„æ—¶é—´çº¿è¿›è¡Œé¢„æµ‹ã€‚åœ¨åŸºäºäº”å¹´EHRæ•°æ®çš„é›¶æ ·æœ¬ä¸€å¹´è‚ºç™Œé£é™©é¢„æµ‹ä»»åŠ¡ä¸­ï¼ŒTraj-CoAä¼˜äºå››ç±»åŸºçº¿æ–¹æ³•ï¼Œå±•ç°å‡ºè‰¯å¥½çš„ä¸´åºŠä¸€è‡´æ€§æ—¶åºæ¨ç†èƒ½åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Traj-CoAæ˜¯ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œç”¨äºå¤„ç†ç”µå­å¥åº·è®°å½•ï¼ˆEHRï¼‰æ•°æ®ï¼Œä»¥è¿›è¡Œæ‚£è€…è½¨è¿¹å»ºæ¨¡ã€‚</li>
<li>Traj-CoAé€šè¿‡å·¥ä½œæ™ºèƒ½ä½“å¤„ç†æ•°æ®ï¼Œå°†æ•°æ®åˆ†è§£ä¸ºå¯ç®¡ç†çš„éƒ¨åˆ†ï¼Œå¹¶é€šè¿‡å…±äº«é•¿æœŸå†…å­˜æ¨¡å—EHRMemè¿›è¡Œå…³é”®äº‹ä»¶çš„è’¸é¦å’Œå™ªå£°å‡å°‘ã€‚</li>
<li>EHRMemæ¨¡å—ä¿å­˜äº†å…¨é¢çš„æ—¶é—´çº¿ä¿¡æ¯ã€‚</li>
<li>æœ€ç»ˆç®¡ç†è€…æ™ºèƒ½ä½“ç»“åˆå·¥ä½œæ™ºèƒ½ä½“çš„æ‘˜è¦å’ŒEHRMemä¸­çš„æ—¶é—´çº¿è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>Traj-CoAåœ¨é›¶æ ·æœ¬è‚ºç™Œé£é™©é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>Traj-CoAå…·æœ‰ä¸´åºŠä¸€è‡´æ€§çš„æ—¶åºæ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10454">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5bbe8b8852a1ad17466694cdd8684288~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824187&auth_key=1760824187-0-0-ce2cfbd2384f7f476cb2c5ada8f30954&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-773e7fb631aba2b3a745009bea110eba~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824194&auth_key=1760824194-0-0-33dad0a690a3a967305a8a3de0382e5f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f48668793bd35eba335676bb719f77ec~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824201&auth_key=1760824201-0-0-356646f3b4b3ed57f42535eb2d44aa34&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Are-Video-Models-Emerging-as-Zero-Shot-Learners-and-Reasoners-in-Medical-Imaging"><a href="#Are-Video-Models-Emerging-as-Zero-Shot-Learners-and-Reasoners-in-Medical-Imaging" class="headerlink" title="Are Video Models Emerging as Zero-Shot Learners and Reasoners in Medical   Imaging?"></a>Are Video Models Emerging as Zero-Shot Learners and Reasoners in Medical   Imaging?</h2><p><strong>Authors:Yuxiang Lai, Jike Zhong, Ming Li, Yuheng Li, Xiaofeng Yang</strong></p>
<p>Recent advances in large generative models have shown that simple autoregressive formulations, when scaled appropriately, can exhibit strong zero-shot generalization across domains. Motivated by this trend, we investigate whether autoregressive video modeling principles can be directly applied to medical imaging tasks, despite the model never being trained on medical data. Specifically, we evaluate a large vision model (LVM) in a zero-shot setting across four representative tasks: organ segmentation, denoising, super-resolution, and motion prediction. Remarkably, even without domain-specific fine-tuning, the LVM can delineate anatomical structures in CT scans and achieve competitive performance on segmentation, denoising, and super-resolution. Most notably, in radiotherapy motion prediction, the model forecasts future 3D CT phases directly from prior phases of a 4D CT scan, producing anatomically consistent predictions that capture patient-specific respiratory dynamics with realistic temporal coherence. We evaluate the LVM on 4D CT data from 122 patients, totaling over 1,820 3D CT volumes. Despite no prior exposure to medical data, the model achieves strong performance across all tasks and surpasses specialized DVF-based and generative baselines in motion prediction, achieving state-of-the-art spatial accuracy. These findings reveal the emergence of zero-shot capabilities in medical video modeling and highlight the potential of general-purpose video models to serve as unified learners and reasoners laying the groundwork for future medical foundation models built on video models. </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹ç”Ÿæˆæ¨¡å‹çš„è¿›å±•è¡¨æ˜ï¼Œç®€å•çš„è‡ªå›å½’å…¬å¼ï¼Œåœ¨é€‚å½“è§„æ¨¡ä¸Šï¼Œå¯ä»¥å±•ç°å‡ºè·¨é¢†åŸŸçš„é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚å—æ­¤è¶‹åŠ¿çš„æ¨åŠ¨ï¼Œæˆ‘ä»¬ç ”ç©¶æ˜¯å¦å¯ä»¥å°†è‡ªå›å½’è§†é¢‘å»ºæ¨¡åŸåˆ™ç›´æ¥åº”ç”¨äºåŒ»å­¦æˆåƒä»»åŠ¡ï¼Œå°½ç®¡è¯¥æ¨¡å‹ä»æœªåœ¨åŒ»ç–—æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬åœ¨å››ä¸ªä»£è¡¨æ€§ä»»åŠ¡ä¸­è¯„ä¼°äº†ä¸€ä¸ªå¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMï¼‰çš„é›¶æ ·æœ¬è®¾ç½®ï¼šå™¨å®˜åˆ†å‰²ã€å»å™ªã€è¶…åˆ†è¾¨ç‡å’Œè¿åŠ¨é¢„æµ‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿æ²¡æœ‰é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„å¾®è°ƒï¼ŒLVMä¹Ÿèƒ½åœ¨CTæ‰«æä¸­æç»˜å‡ºè§£å‰–ç»“æ„ï¼Œå¹¶åœ¨åˆ†å‰²ã€å»å™ªå’Œè¶…åˆ†è¾¨ç‡æ–¹é¢è¾¾åˆ°é¢‡å…·ç«äº‰åŠ›çš„æ€§èƒ½ã€‚æœ€å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ”¾ç–—è¿åŠ¨é¢„æµ‹ä¸­ï¼Œè¯¥æ¨¡å‹ç›´æ¥ä»4D CTæ‰«æçš„å…ˆå‰é˜¶æ®µé¢„æµ‹æœªæ¥çš„3D CTé˜¶æ®µï¼Œäº§ç”Ÿè§£å‰–ç»“æ„ä¸€è‡´çš„é¢„æµ‹ï¼Œæ•æ‰æ‚£è€…ç‰¹å®šçš„å‘¼å¸åŠ¨åŠ›å­¦ï¼Œå…·æœ‰ç°å®çš„æ—¶é—´è¿è´¯æ€§ã€‚æˆ‘ä»¬åœ¨æ¥è‡ª122åæ‚£è€…çš„4D CTæ•°æ®ä¸Šè¯„ä¼°äº†LVMï¼Œæ€»è®¡è¶…è¿‡1820ä¸ª3D CTä½“ç§¯ã€‚å°½ç®¡è¯¥æ¨¡å‹ä¹‹å‰æ²¡æœ‰æ¥è§¦è¿‡åŒ»ç–—æ•°æ®ï¼Œä½†å®ƒåœ¨æ‰€æœ‰ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼Œåœ¨è¿åŠ¨é¢„æµ‹æ–¹é¢è¶…è¶Šäº†åŸºäºDVFå’Œç”ŸæˆåŸºçº¿ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç©ºé—´ç²¾åº¦ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†åŒ»ç–—è§†é¢‘å»ºæ¨¡ä¸­é›¶æ ·æœ¬èƒ½åŠ›çš„å‡ºç°ï¼Œå¹¶çªå‡ºäº†é€šç”¨è§†é¢‘æ¨¡å‹ä½œä¸ºæœªæ¥å»ºç«‹åœ¨è§†é¢‘æ¨¡å‹ä¸Šçš„åŒ»ç–—åŸºç¡€æ¨¡å‹çš„ç»Ÿä¸€å­¦ä¹ è€…å’Œæ¨ç†è€…çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10254v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†å¤§å‹ç”Ÿæˆæ¨¡å‹åœ¨åŒ»å­¦è§†é¢‘å»ºæ¨¡ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶å‘ç°åœ¨é›¶æ ·æœ¬åœºæ™¯ä¸‹ï¼Œå³ä½¿æœªç»è¿‡åŒ»å­¦æ•°æ®è®­ç»ƒï¼ŒåŸºäºè‡ªåŠ¨å›å½’åŸç†çš„å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMï¼‰ä¹Ÿèƒ½åœ¨åŒ»å­¦æˆåƒä»»åŠ¡ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨å™¨å®˜åˆ†å‰²ã€å»å™ªã€è¶…åˆ†è¾¨ç‡å’ŒåŠ¨æ€é¢„æµ‹ç­‰å››ä¸ªä»»åŠ¡ä¸­ï¼ŒLVMå±•ç°äº†ç«äº‰æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¾ç–—åŠ¨æ€é¢„æµ‹ä¸­ï¼Œè¯¥æ¨¡å‹å¯ç›´æ¥ä»4D CTæ‰«æçš„å…ˆå‰é˜¶æ®µé¢„æµ‹æœªæ¥çš„3D CTé˜¶æ®µï¼Œäº§ç”Ÿè§£å‰–ç»“æ„ä¸€è‡´ã€æ•æ‰æ‚£è€…ç‰¹å®šå‘¼å¸åŠ¨æ€çš„é¢„æµ‹ç»“æœã€‚è¿™äº›å‘ç°æ­ç¤ºäº†é›¶æ ·æœ¬èƒ½åŠ›åœ¨åŒ»å­¦è§†é¢‘å»ºæ¨¡ä¸­çš„æ¶Œç°ï¼Œçªæ˜¾äº†é€šç”¨è§†é¢‘æ¨¡å‹ä½œä¸ºç»Ÿä¸€å­¦ä¹ è€…å’Œæ¨ç†è€…çš„æ½œåŠ›ï¼Œä¸ºæœªæ¥çš„åŒ»å­¦åŸºç¡€æ¨¡å‹æä¾›äº†å‚è€ƒã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹ç”Ÿæˆæ¨¡å‹åœ¨åŒ»å­¦è§†é¢‘å»ºæ¨¡ä¸­å±•ç°å‡ºé›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>è‡ªåŠ¨å›å½’åŸç†çš„å¤§å‹è§†è§‰æ¨¡å‹ï¼ˆLVMï¼‰åœ¨åŒ»å­¦æˆåƒä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚</li>
<li>LVMåœ¨å™¨å®˜åˆ†å‰²ã€å»å™ªå’Œè¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­å…·æœ‰ç«äº‰åŠ›ã€‚</li>
<li>åœ¨æ”¾ç–—åŠ¨æ€é¢„æµ‹ä¸­ï¼ŒLVMèƒ½é¢„æµ‹æœªæ¥çš„3D CTé˜¶æ®µï¼Œäº§ç”Ÿè§£å‰–ç»“æ„ä¸€è‡´çš„é¢„æµ‹ç»“æœã€‚</li>
<li>LVMæ¨¡å‹èƒ½æ•æ‰æ‚£è€…ç‰¹å®šçš„å‘¼å¸åŠ¨æ€ï¼Œå…·æœ‰ç°å®çš„æ—¶é—´è¿è´¯æ€§ã€‚</li>
<li>é›¶æ ·æœ¬èƒ½åŠ›åœ¨åŒ»å­¦è§†é¢‘å»ºæ¨¡ä¸­çš„æ¶Œç°è¡¨æ˜é€šç”¨è§†é¢‘æ¨¡å‹çš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10254">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-334c9f552d6c400f9896c1e93de9fe3e~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824208&auth_key=1760824208-0-0-512f97cc9fe6f2fa606e2e46d51d71af&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-b17c0dd0ea173613bef741e060750f65~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824216&auth_key=1760824216-0-0-cfc5382cb7dc7cbf512ef2045bbee4f9&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-318766030863c3a2a4e47806f8d9de89~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824223&auth_key=1760824223-0-0-b11239284909ca23b62a6ed3e73d8a52&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-849fd25ce44e953216dcc6942cbad869~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824230&auth_key=1760824230-0-0-01ef33d98b54fb58a92c1246d5c6f974&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ViConEx-Med-Visual-Concept-Explainability-via-Multi-Concept-Token-Transformer-for-Medical-Image-Analysis"><a href="#ViConEx-Med-Visual-Concept-Explainability-via-Multi-Concept-Token-Transformer-for-Medical-Image-Analysis" class="headerlink" title="ViConEx-Med: Visual Concept Explainability via Multi-Concept Token   Transformer for Medical Image Analysis"></a>ViConEx-Med: Visual Concept Explainability via Multi-Concept Token   Transformer for Medical Image Analysis</h2><p><strong>Authors:Cristiano PatrÃ­cio, LuÃ­s F. Teixeira, JoÃ£o C. Neves</strong></p>
<p>Concept-based models aim to explain model decisions with human-understandable concepts. However, most existing approaches treat concepts as numerical attributes, without providing complementary visual explanations that could localize the predicted concepts. This limits their utility in real-world applications and particularly in high-stakes scenarios, such as medical use-cases. This paper proposes ViConEx-Med, a novel transformer-based framework for visual concept explainability, which introduces multi-concept learnable tokens to jointly predict and localize visual concepts. By leveraging specialized attention layers for processing visual and text-based concept tokens, our method produces concept-level localization maps while maintaining high predictive accuracy. Experiments on both synthetic and real-world medical datasets demonstrate that ViConEx-Med outperforms prior concept-based models and achieves competitive performance with black-box models in terms of both concept detection and localization precision. Our results suggest a promising direction for building inherently interpretable models grounded in visual concepts. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/CristianoPatricio/viconex-med">https://github.com/CristianoPatricio/viconex-med</a>. </p>
<blockquote>
<p>åŸºäºæ¦‚å¿µæ¨¡å‹çš„ç›®çš„æ˜¯ç”¨äººç±»å¯ç†è§£çš„æ¦‚å¿µæ¥è§£é‡Šæ¨¡å‹çš„å†³ç­–ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¤§å¤šæ•°æ–¹æ³•éƒ½å°†æ¦‚å¿µè§†ä¸ºæ•°å€¼å±æ€§ï¼Œè€Œæ²¡æœ‰æä¾›å¯ä»¥å®šä½é¢„æµ‹æ¦‚å¿µçš„è¡¥å……è§†è§‰è§£é‡Šã€‚è¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°å®ä¸–ç•Œåº”ç”¨å’Œé«˜é£é™©åœºæ™¯ï¼ˆå¦‚åŒ»ç–—ç”¨ä¾‹ï¼‰ä¸­çš„å®ç”¨æ€§ã€‚æœ¬æ–‡æå‡ºäº†ViConEx-Medï¼Œè¿™æ˜¯ä¸€ç§åŸºäºtransformerçš„è§†è§‰æ¦‚å¿µå¯è§£é‡Šæ€§æ–°æ¡†æ¶ï¼Œå®ƒå¼•å…¥äº†å¤šæ¦‚å¿µå¯å­¦ä¹ ä»¤ç‰Œæ¥å…±åŒé¢„æµ‹å’Œå®šä½è§†è§‰æ¦‚å¿µã€‚é€šè¿‡åˆ©ç”¨ç”¨äºå¤„ç†è§†è§‰å’ŒåŸºäºæ–‡æœ¬çš„æ¦‚å¿µä»¤ç‰Œçš„ç‰¹æ®Šæ³¨æ„åŠ›å±‚ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¿æŒé«˜é¢„æµ‹å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œç”Ÿæˆäº†æ¦‚å¿µå±‚é¢çš„å®šä½å›¾ã€‚åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•ŒåŒ»ç–—æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒViConEx-Medä¼˜äºå…ˆå‰çš„åŸºäºæ¦‚å¿µæ¨¡å‹ï¼Œåœ¨æ¦‚å¿µæ£€æµ‹å’Œå®šä½ç²¾åº¦æ–¹é¢å®ç°äº†ä¸é»‘åŒ£æ¨¡å‹ç›¸ç«äº‰çš„è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå»ºç«‹åŸºäºè§†è§‰æ¦‚å¿µçš„å†…åœ¨å¯è§£é‡Šæ¨¡å‹æä¾›äº†ä¸€ä¸ªæœ‰å‰é€”çš„æ–¹å‘ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/CristianoPatricio/viconex-med%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/CristianoPatricio/viconex-medä¸Šå…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.10174v1">PDF</a> This work has been submitted to the IEEE for possible publication</p>
<p><strong>Summary</strong><br>åŒ»å­¦å›¾åƒæ¦‚å¿µæ¨¡å‹æ—¨åœ¨ç”¨äººç±»å¯ç†è§£çš„æ¦‚å¿µæ¥è§£é‡Šæ¨¡å‹å†³ç­–ã€‚ä½†ç°æœ‰æ–¹æ³•å¤šå°†æ¦‚å¿µè§†ä¸ºæ•°å€¼å±æ€§ï¼Œç¼ºä¹å¯è§†åŒ–è§£é‡Šæ¥å®šä½é¢„æµ‹æ¦‚å¿µï¼Œè¿™åœ¨çœŸå®ä¸–ç•Œåº”ç”¨å’ŒåŒ»ç–—ç­‰é«˜é£é™©åœºæ™¯ä¸­é™åˆ¶äº†å…¶æ•ˆç”¨ã€‚æœ¬æ–‡æå‡ºViConEx-Medæ¡†æ¶ï¼Œé‡‡ç”¨åŸºäºè§†è§‰æ¦‚å¿µè§£é‡Šæ€§çš„æ–°å‹è½¬æ¢å™¨æŠ€æœ¯ï¼Œå¼•å…¥å¤šæ¦‚å¿µå­¦ä¹ ä»¤ç‰Œè¿›è¡Œè§†è§‰æ¦‚å¿µçš„è”åˆé¢„æµ‹å’Œå®šä½ã€‚é€šè¿‡åˆ©ç”¨ä¸“é—¨çš„æ³¨æ„åŠ›å±‚å¤„ç†è§†è§‰å’Œæ–‡æœ¬æ¦‚å¿µä»¤ç‰Œï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜é¢„æµ‹å‡†ç¡®æ€§çš„åŒæ—¶ï¼Œç”Ÿæˆæ¦‚å¿µçº§åˆ«çš„å®šä½å›¾ã€‚åœ¨åˆæˆå’ŒçœŸå®åŒ»ç–—æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒViConEx-Medä¼˜äºç°æœ‰æ¦‚å¿µæ¨¡å‹ï¼Œä¸é»‘ç›’æ¨¡å‹åœ¨æ¦‚å¿µæ£€æµ‹å’Œå®šä½ç²¾åº¦æ–¹é¢è¡¨ç°ç›¸å½“ã€‚è¿™ä¸ºæ„å»ºåŸºäºè§†è§‰æ¦‚å¿µçš„å›ºæœ‰å¯è§£é‡Šæ¨¡å‹æä¾›äº†æœ‰å‰æ™¯çš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ¦‚å¿µæ¨¡å‹ç”¨äºè§£é‡ŠåŒ»å­¦å›¾åƒæ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å°†æ¦‚å¿µè§†ä¸ºæ•°å€¼å±æ€§ï¼Œç¼ºä¹å¯è§†åŒ–è§£é‡Šã€‚</li>
<li>ViConEx-Medæ¡†æ¶å¼•å…¥å¤šæ¦‚å¿µå­¦ä¹ ä»¤ç‰Œè¿›è¡Œè§†è§‰æ¦‚å¿µçš„è”åˆé¢„æµ‹å’Œå®šä½ã€‚</li>
<li>é€šè¿‡ä¸“é—¨çš„æ³¨æ„åŠ›å±‚å¤„ç†è§†è§‰å’Œæ–‡æœ¬æ¦‚å¿µä»¤ç‰Œï¼Œç”Ÿæˆæ¦‚å¿µçº§åˆ«å®šä½å›¾ã€‚</li>
<li>å®éªŒè¯æ˜ViConEx-Medåœ¨æ¦‚å¿µæ£€æµ‹å’Œå®šä½ç²¾åº¦æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚</li>
<li>è¯¥æ¡†æ¶ä¼˜äºç°æœ‰æ¦‚å¿µæ¨¡å‹ï¼Œä¸é»‘ç›’æ¨¡å‹è¡¨ç°ç›¸å½“ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.10174">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-e640fb285216172ba520fcf34f17cee7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824237&auth_key=1760824237-0-0-4ff8ef36e6746f68cdeb5ba78b67fba5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-86d903efbd4050871176b52fa1e69954~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824245&auth_key=1760824245-0-0-c2c4dbd38037e5ad4666cddd91acabcb&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-38b9572c43cc13cfbecc29b1b170c011~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824252&auth_key=1760824252-0-0-f14cec0ec87973a13517c6128c26d366&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="A-Biophysically-Conditioned-Generative-Framework-for-3D-Brain-Tumor-MRI-Synthesis"><a href="#A-Biophysically-Conditioned-Generative-Framework-for-3D-Brain-Tumor-MRI-Synthesis" class="headerlink" title="A Biophysically-Conditioned Generative Framework for 3D Brain Tumor MRI   Synthesis"></a>A Biophysically-Conditioned Generative Framework for 3D Brain Tumor MRI   Synthesis</h2><p><strong>Authors:Valentin Biller, Lucas Zimmer, Can Erdur, Sandeep Nagar, Daniel RÃ¼ckert, Niklas Bubeck, Jonas Weidner</strong></p>
<p>Magnetic resonance imaging (MRI) inpainting supports numerous clinical and research applications. We introduce the first generative model that conditions on voxel-level, continuous tumor concentrations to synthesize high-fidelity brain tumor MRIs. For the BraTS 2025 Inpainting Challenge, we adapt this architecture to the complementary task of healthy tissue restoration by setting the tumor concentrations to zero. Our latent diffusion model conditioned on both tissue segmentations and the tumor concentrations generates 3D spatially coherent and anatomically consistent images for both tumor synthesis and healthy tissue inpainting. For healthy inpainting, we achieve a PSNR of 18.5, and for tumor inpainting, we achieve 17.4. Our code is available at: <a target="_blank" rel="noopener" href="https://github.com/valentin-biller/ldm.git">https://github.com/valentin-biller/ldm.git</a> </p>
<blockquote>
<p>ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰åœ¨å¤šä¸ªä¸´åºŠå’Œç ”ç©¶åº”ç”¨ä¸­éƒ½æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚æˆ‘ä»¬å¼•å…¥é¦–ä¸ªåŸºäºä½“ç´ çº§åˆ«ã€è¿ç»­çš„è‚¿ç˜¤æµ“åº¦çš„ç”Ÿæˆæ¨¡å‹ï¼Œä»¥åˆæˆé«˜ä¿çœŸè„‘è‚¿ç˜¤MRIã€‚åœ¨BraTS 2025è¡¥å…¨æŒ‘æˆ˜ä¸­ï¼Œæˆ‘ä»¬å°†è‚¿ç˜¤æµ“åº¦è®¾ç½®ä¸ºé›¶ï¼Œä»¥é€‚åº”å¥åº·ç»„ç»‡ä¿®å¤çš„è¾…åŠ©ä»»åŠ¡ã€‚æˆ‘ä»¬çš„æ½œåœ¨æ‰©æ•£æ¨¡å‹æ—¢æ ¹æ®ç»„ç»‡åˆ†å‰²åˆæ ¹æ®è‚¿ç˜¤æµ“åº¦è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œå¯ç”Ÿæˆç”¨äºè‚¿ç˜¤åˆæˆå’Œå¥åº·ç»„ç»‡è¡¥å…¨çš„3Dç©ºé—´è¿è´¯æ€§å’Œè§£å‰–ä¸€è‡´æ€§å›¾åƒã€‚å¯¹äºå¥åº·ç»„ç»‡çš„è¡¥å…¨ï¼Œæˆ‘ä»¬å®ç°çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰ä¸º18.5ï¼Œå¯¹äºè‚¿ç˜¤ç»„ç»‡çš„è¡¥å…¨ï¼Œæˆ‘ä»¬å®ç°çš„PSNRä¸º17.4ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://github.com/valentin-biller/ldm.git">https://github.com/valentin-biller/ldm.git</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09365v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åŸºäºç£æ€§å…±æŒ¯æˆåƒï¼ˆMRIï¼‰çš„ç”Ÿæˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯æ ¹æ®ä½“ç´ çº§è¿ç»­çš„è‚¿ç˜¤æµ“åº¦ä¿¡æ¯åˆæˆé«˜ä¿çœŸåº¦çš„è„‘è‚¿ç˜¤MRIã€‚è¯¥æ¨¡å‹å¯åº”ç”¨äºBraTS 2025 Inpainting Challengeä¸­çš„è‚¿ç˜¤åˆæˆä¸å¥åº·ç»„ç»‡ä¿®å¤ä»»åŠ¡ã€‚å¯¹äºå¥åº·ç»„ç»‡çš„ä¿®å¤ï¼Œè¯¥æ¨¡å‹çš„å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰è¾¾åˆ°18.5ï¼›å¯¹äºè‚¿ç˜¤çš„ä¿®å¤ï¼ŒPSNRä¸º17.4ã€‚æ¨¡å‹ä»£ç å·²å…¬å¼€äº<a target="_blank" rel="noopener" href="https://github.com/valentin-biller/ldm.git%E3%80%82">https://github.com/valentin-biller/ldm.gitã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»‹ç»äº†ä¸€ç§åŸºäºMRIçš„ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½åˆ©ç”¨ä½“ç´ çº§è¿ç»­çš„è‚¿ç˜¤æµ“åº¦ä¿¡æ¯ã€‚</li>
<li>æ¨¡å‹å¯ç”¨äºåˆæˆé«˜ä¿çœŸåº¦çš„è„‘è‚¿ç˜¤MRIï¼Œé€‚ç”¨äºä¸´åºŠå’Œç ”ç©¶åº”ç”¨ã€‚</li>
<li>æ¨¡å‹å¯åº”ç”¨äºBraTS 2025 Inpainting Challengeä¸­çš„è‚¿ç˜¤åˆæˆä¸å¥åº·ç»„ç»‡ä¿®å¤ä»»åŠ¡ã€‚</li>
<li>å¥åº·ç»„ç»‡ä¿®å¤ä»»åŠ¡çš„PSNRè¾¾åˆ°18.5ï¼Œè‚¿ç˜¤ä¿®å¤ä»»åŠ¡çš„PSNRä¸º17.4ã€‚</li>
<li>æ¨¡å‹é‡‡ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ŒåŒæ—¶åŸºäºç»„ç»‡åˆ†å‰²å’Œè‚¿ç˜¤æµ“åº¦ä¿¡æ¯è¿›è¡Œç”Ÿæˆã€‚</li>
<li>æ¨¡å‹èƒ½ç”Ÿæˆ3Dç©ºé—´è¿è´¯ã€è§£å‰–ç»“æ„ä¸€è‡´çš„å›¾ç‰‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09365">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-5243bcbb0c89b10d2c60237ccb4a2a91~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824259&auth_key=1760824259-0-0-9d823d8ceb14722301752ca4ec007f70&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-ff32ded784435777abb04a7d4cd142b7~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824266&auth_key=1760824266-0-0-a59d6751ede51a6167308d408bc92f05&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-dfca3795b12afc03a43941c37c1f7e26~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824273&auth_key=1760824273-0-0-c3b86e580b30183ea34d599a2bd99630&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="MIP-Based-Tumor-Segmentation-A-Radiologist-Inspired-Approach"><a href="#MIP-Based-Tumor-Segmentation-A-Radiologist-Inspired-Approach" class="headerlink" title="MIP-Based Tumor Segmentation: A Radiologist-Inspired Approach"></a>MIP-Based Tumor Segmentation: A Radiologist-Inspired Approach</h2><p><strong>Authors:Romario Zarik, Nahum Kiryati, Michael Green, Liran Domachevsky, Arnaldo Mayer</strong></p>
<p>PET&#x2F;CT imaging is the gold standard for tumor detection, offering high accuracy in identifying local and metastatic lesions. Radiologists often begin assessment with rotational Multi-Angle Maximum Intensity Projections (MIPs) from PET, confirming findings with volumetric slices. This workflow is time-consuming, especially in metastatic cases. Despite their clinical utility, MIPs are underutilized in automated tumor segmentation, where 3D volumetric data remains the norm. We propose an alternative approach that trains segmentation models directly on MIPs, bypassing the need to segment 3D volumes and then project. This better aligns the model with its target domain and yields substantial gains in computational efficiency and training time. We also introduce a novel occlusion correction method that restores MIP annotations occluded by high-intensity structures, improving segmentation. Using the autoPET 2022 Grand Challenge dataset, we evaluate our method against standard 3D pipelines in terms of performance and training&#x2F;computation efficiency for segmentation and classification, and analyze how MIP count affects segmentation. Our MIP-based approach achieves segmentation performance on par with 3D (&lt;&#x3D;1% Dice difference, 26.7% better Hausdorff Distance), while reducing training time (convergence time) by 55.8-75.8%, energy per epoch by 71.7-76%, and TFLOPs by two orders of magnitude, highlighting its scalability for clinical use. For classification, using 16 MIPs only as input, we surpass 3D performance while reducing training time by over 10x and energy consumption per epoch by 93.35%. Our analysis of the impact of MIP count on segmentation identified 48 views as optimal, offering the best trade-off between performance and efficiency. </p>
<blockquote>
<p>PET&#x2F;CTæˆåƒåœ¨è‚¿ç˜¤æ£€æµ‹æ–¹é¢æ˜¯é‡‘æ ‡å‡†ï¼Œå…¶èƒ½å‡†ç¡®è¯†åˆ«å±€éƒ¨å’Œè½¬ç§»æ€§ç—…ç¶ã€‚æ”¾å°„ç§‘åŒ»ç”Ÿé€šå¸¸ä»PETçš„æ—‹è½¬å¤šè§’åº¦æœ€å¤§å¼ºåº¦æŠ•å½±ï¼ˆMIPsï¼‰å¼€å§‹è¯„ä¼°ï¼Œå¹¶é€šè¿‡ä½“ç§¯åˆ‡ç‰‡è¿›è¡ŒéªŒè¯ã€‚ç‰¹åˆ«æ˜¯åœ¨è½¬ç§»æ€§ç—…ä¾‹ä¸­ï¼Œè¿™ç§å·¥ä½œæµç¨‹éå¸¸è€—æ—¶ã€‚å°½ç®¡MIPåœ¨ä¸´åºŠä¸Šæœ‰å®ç”¨ä»·å€¼ï¼Œä½†åœ¨è‡ªåŠ¨è‚¿ç˜¤åˆ†å‰²ä¸­å´æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ï¼Œè€Œ3Dä½“ç§¯æ•°æ®ä»æ˜¯å¸¸æ€ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ›¿ä»£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç›´æ¥åœ¨MIPä¸Šè®­ç»ƒåˆ†å‰²æ¨¡å‹ï¼Œä»è€Œæ— éœ€åˆ†å‰²3Dä½“ç§¯ç„¶åå†æŠ•å½±ï¼Œè¿™ä½¿æ¨¡å‹ä¸å…¶ç›®æ ‡é¢†åŸŸæ›´åŠ åŒ¹é…ï¼Œå¹¶åœ¨è®¡ç®—æ•ˆç‡å’Œè®­ç»ƒæ—¶é—´ä¸Šå®ç°äº†å¯è§‚çš„æ”¶ç›Šã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„é®æŒ¡æ ¡æ­£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥æ¢å¤è¢«é«˜å¼ºåº¦ç»“æ„é®æŒ¡çš„MIPæ³¨é‡Šï¼Œä»è€Œæé«˜äº†åˆ†å‰²æ•ˆæœã€‚æˆ‘ä»¬ä½¿ç”¨autoPET 2022æŒ‘æˆ˜èµ›æ•°æ®é›†è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¼ ç»Ÿ3Dç®¡é“åœ¨åˆ†å‰²å’Œåˆ†ç±»æ–¹é¢çš„æ€§èƒ½ä»¥åŠè®­ç»ƒå’Œè®¡ç®—æ•ˆç‡ï¼Œå¹¶åˆ†æäº†MIPè®¡æ•°å¦‚ä½•å½±å“åˆ†å‰²ã€‚æˆ‘ä»¬çš„åŸºäºMIPçš„æ–¹æ³•åœ¨åˆ†å‰²æ€§èƒ½ä¸Šè¾¾åˆ°äº†ä¸3Dç›¸å½“çš„æ°´å¹³ï¼ˆDiceå·®å¼‚â‰¤1%ï¼ŒHausdorffè·ç¦»æé«˜äº†26.7%ï¼‰ï¼ŒåŒæ—¶å‡å°‘äº†è®­ç»ƒæ—¶é—´ï¼ˆæ”¶æ•›æ—¶é—´ï¼‰çš„55.8%-75.8%ï¼Œæ¯ä¸ªå‘¨æœŸèƒ½è€—å‡å°‘äº†71.7%-76%ï¼ŒTFLOPså‡å°‘äº†ä¸¤ä¸ªæ•°é‡çº§ï¼Œçªæ˜¾äº†å…¶ä¸´åºŠåº”ç”¨çš„å¯æ‰©å±•æ€§ã€‚å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œä»…ä½¿ç”¨16ä¸ªMIPä½œä¸ºè¾“å…¥ï¼Œæˆ‘ä»¬åœ¨æ€§èƒ½ä¸Šè¶…è¿‡äº†3Dï¼ŒåŒæ—¶å‡å°‘äº†è¶…è¿‡10å€çš„è®­ç»ƒæ—¶é—´å’Œæ¯ä¸ªå‘¨æœŸçš„èƒ½è€—è¾¾93.35%ã€‚æˆ‘ä»¬å¯¹MIPè®¡æ•°å¯¹åˆ†å‰²å½±å“çš„åˆ†æç¡®å®šäº†48ä¸ªè§†å›¾æ˜¯æœ€ä¼˜çš„ï¼Œå¯åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´æä¾›æœ€ä½³æƒè¡¡ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.09326v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>    PET&#x2F;CTæˆåƒåœ¨è‚¿ç˜¤æ£€æµ‹æ–¹é¢æ˜¯é‡‘æ ‡å‡†ï¼Œèƒ½å‡†ç¡®è¯†åˆ«å±€éƒ¨å’Œè½¬ç§»æ€§ç—…ç¶ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºMIPï¼ˆå¤šè§’åº¦æœ€å¤§å¼ºåº¦æŠ•å½±ï¼‰çš„è‚¿ç˜¤åˆ†å‰²æ–¹æ³•ï¼Œç›´æ¥å¯¹MIPsè¿›è¡Œè®­ç»ƒï¼Œé¿å…äº†ä¸‰ç»´ä½“ç§¯çš„åˆ†å‰²å’ŒæŠ•å½±éœ€æ±‚ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡å’Œè®­ç»ƒæ—¶é—´ã€‚åŒæ—¶ï¼Œå¼•å…¥äº†ä¸€ç§æ–°é¢–çš„é®æŒ¡æ ¡æ­£æ–¹æ³•ï¼Œç”¨äºæ¢å¤è¢«é«˜å¼ºåº¦ç»“æ„é®æŒ¡çš„MIPæ³¨é‡Šï¼Œæé«˜äº†åˆ†å‰²æ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMIPæ–¹æ³•è¾¾åˆ°ä¸ä¸‰ç»´æŠ€æœ¯ç›¸å½“çš„åˆ†å‰²æ€§èƒ½ï¼ŒåŒæ—¶é™ä½äº†è®­ç»ƒæ—¶é—´ã€èƒ½è€—å’Œè®¡ç®—é‡ã€‚å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œä»…ä½¿ç”¨16ä¸ªMIPä½œä¸ºè¾“å…¥ï¼Œå³å¯è¶…è¶Šä¸‰ç»´æŠ€æœ¯çš„æ€§èƒ½ã€‚å¯¹MIPæ•°é‡å¯¹åˆ†å‰²å½±å“çš„åˆ†æè¡¨æ˜ï¼Œ48ä¸ªè§†è§’æ˜¯æœ€ä½³çš„ï¼Œèƒ½åœ¨æ€§èƒ½å’Œæ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>PET&#x2F;CTæˆåƒåœ¨è‚¿ç˜¤æ£€æµ‹ä¸­å…·æœ‰é«˜å‡†ç¡®æ€§ï¼Œæ˜¯å½“å‰çš„é‡‘æ ‡å‡†ã€‚</li>
<li>ä¼ ç»Ÿçš„è‚¿ç˜¤åˆ†å‰²æ–¹æ³•ä¸»è¦åŸºäºä¸‰ç»´ä½“ç§¯æ•°æ®ï¼Œä½†å¤„ç†è½¬ç§»æ€§ç—…ä¾‹æ—¶è€—æ—¶è¾ƒé•¿ã€‚</li>
<li>æå‡ºä¸€ç§åŸºäºMIPçš„è‚¿ç˜¤åˆ†å‰²æ–¹æ³•ï¼Œç›´æ¥è®­ç»ƒåˆ†å‰²æ¨¡å‹ï¼Œæé«˜è®¡ç®—æ•ˆç‡å’Œè®­ç»ƒæ—¶é—´ã€‚</li>
<li>å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„é®æŒ¡æ ¡æ­£æ–¹æ³•ï¼Œæé«˜äº†åˆ†å‰²æ•ˆæœçš„å‡†ç¡®æ€§ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMIPæ–¹æ³•åœ¨åˆ†å‰²æ€§èƒ½ä¸Šæ¥è¿‘ä¸‰ç»´æŠ€æœ¯ï¼Œä½†è®­ç»ƒæ—¶é—´ã€èƒ½è€—å’Œè®¡ç®—é‡æœ‰æ‰€é™ä½ã€‚</li>
<li>å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨è¾ƒå°‘çš„MIPè¾“å…¥å³å¯è¾¾åˆ°ç”šè‡³è¶…è¶Šä¸‰ç»´æŠ€æœ¯çš„æ€§èƒ½ã€‚</li>
<li>åˆ†æå‘ç°ï¼Œ48ä¸ªè§†è§’çš„MIPæ•°é‡åœ¨åˆ†å‰²æ€§èƒ½ä¸æ•ˆç‡ä¹‹é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.09326">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-dfbded7bf88c35c45f1ef9c4987d9535~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824280&auth_key=1760824280-0-0-68b6cd138bd129d0c1e0154dcbe80800&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-c40382bfc1bd1079623c2283cefb6d2b~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824286&auth_key=1760824286-0-0-70d30df312a0d328ab5f4a486082bef8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-aed576de72a943a0d34878341fc59555~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824293&auth_key=1760824293-0-0-d33c514115f3fa2452fedcf1ace3ab93&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="SAM2-3dMed-Empowering-SAM2-for-3D-Medical-Image-Segmentation"><a href="#SAM2-3dMed-Empowering-SAM2-for-3D-Medical-Image-Segmentation" class="headerlink" title="SAM2-3dMed: Empowering SAM2 for 3D Medical Image Segmentation"></a>SAM2-3dMed: Empowering SAM2 for 3D Medical Image Segmentation</h2><p><strong>Authors:Yeqing Yang, Le Xu, Lixia Tian</strong></p>
<p>Accurate segmentation of 3D medical images is critical for clinical applications like disease assessment and treatment planning. While the Segment Anything Model 2 (SAM2) has shown remarkable success in video object segmentation by leveraging temporal cues, its direct application to 3D medical images faces two fundamental domain gaps: 1) the bidirectional anatomical continuity between slices contrasts sharply with the unidirectional temporal flow in videos, and 2) precise boundary delineation, crucial for morphological analysis, is often underexplored in video tasks. To bridge these gaps, we propose SAM2-3dMed, an adaptation of SAM2 for 3D medical imaging. Our framework introduces two key innovations: 1) a Slice Relative Position Prediction (SRPP) module explicitly models bidirectional inter-slice dependencies by guiding SAM2 to predict the relative positions of different slices in a self-supervised manner; 2) a Boundary Detection (BD) module enhances segmentation accuracy along critical organ and tissue boundaries. Extensive experiments on three diverse medical datasets (the Lung, Spleen, and Pancreas in the Medical Segmentation Decathlon (MSD) dataset) demonstrate that SAM2-3dMed significantly outperforms state-of-the-art methods, achieving superior performance in segmentation overlap and boundary precision. Our approach not only advances 3D medical image segmentation performance but also offers a general paradigm for adapting video-centric foundation models to spatial volumetric data. </p>
<blockquote>
<p>åœ¨ç–¾ç—…è¯„ä¼°å’Œåˆ¶å®šæ²»ç–—æ–¹æ¡ˆç­‰ä¸´åºŠåº”ç”¨ä¸­ï¼Œå¯¹ä¸‰ç»´åŒ»å­¦å›¾åƒè¿›è¡Œç²¾ç¡®åˆ†å‰²è‡³å…³é‡è¦ã€‚è™½ç„¶Segment Anything Model 2ï¼ˆSAM2ï¼‰é€šè¿‡åˆ©ç”¨æ—¶åºçº¿ç´¢åœ¨è§†é¢‘å¯¹è±¡åˆ†å‰²æ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œä½†å°†å…¶ç›´æ¥åº”ç”¨äºä¸‰ç»´åŒ»å­¦å›¾åƒé¢ä¸´ä¸¤ä¸ªåŸºæœ¬çš„é¢†åŸŸå·®è·ï¼šä¸€æ˜¯åˆ‡ç‰‡ä¹‹é—´çš„åŒå‘è§£å‰–è¿ç»­æ€§ä¸è§†é¢‘ä¸­çš„å•å‘æ—¶é—´æµå­˜åœ¨æ˜æ˜¾å¯¹æ¯”ï¼›äºŒæ˜¯ç²¾ç¡®çš„è¾¹ç¼˜ç•Œå®šå¯¹äºå½¢æ€åˆ†æè‡³å…³é‡è¦ï¼Œåœ¨è§†é¢‘ä»»åŠ¡ä¸­ç»å¸¸è¢«å¿½è§†ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†SAM2-3dMedï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸‰ç»´åŒ»å­¦æˆåƒçš„SAM2çš„æ”¹è¿›æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¼•å…¥äº†ä¸¤ä¸ªå…³é”®çš„åˆ›æ–°ç‚¹ï¼šä¸€æ˜¯åˆ‡ç‰‡ç›¸å¯¹ä½ç½®é¢„æµ‹ï¼ˆSRPPï¼‰æ¨¡å—é€šè¿‡æŒ‡å¯¼SAM2ä»¥è‡ªæˆ‘ç›‘ç£çš„æ–¹å¼é¢„æµ‹ä¸åŒåˆ‡ç‰‡çš„ç›¸å¯¹ä½ç½®ï¼Œæ˜¾å¼åœ°å»ºç«‹åˆ‡ç‰‡é—´çš„åŒå‘ä¾èµ–å…³ç³»ï¼›äºŒæ˜¯è¾¹ç•Œæ£€æµ‹ï¼ˆBDï¼‰æ¨¡å—å¯æé«˜é‡è¦å™¨å®˜å’Œç»„ç»‡è¾¹ç•Œå¤„çš„åˆ†å‰²ç²¾åº¦ã€‚åœ¨åŒ»å­¦åˆ†å‰²å…¨èƒ½æŒ‘æˆ˜èµ›ï¼ˆMSDï¼‰æ•°æ®é›†çš„ä¸‰éƒ¨åˆ†æ•°æ®é›†ï¼ˆè‚ºéƒ¨ã€è„¾è„å’Œèƒ°è…ºï¼‰ä¸Šè¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒSAM2-3dMedåœ¨åˆ†å‰²é‡å å’Œè¾¹ç•Œç²¾åº¦æ–¹é¢å‡æ˜¾è‘—ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æé«˜äº†ä¸‰ç»´åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ€§èƒ½ï¼Œè¿˜ä¸ºå°†è§†é¢‘ä¸­å¿ƒåŸºç¡€æ¨¡å‹é€‚åº”äºç©ºé—´ä½“ç§¯æ•°æ®æä¾›äº†ä¸€ä¸ªé€šç”¨çš„èŒƒå¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08967v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>SAM2æ¨¡å‹åœ¨è§†é¢‘å¯¹è±¡åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä½†ç›´æ¥åº”ç”¨äº3DåŒ»å­¦å›¾åƒå­˜åœ¨ä¸¤ä¸ªé¢†åŸŸå·®è·ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†SAM2-3dMedæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥Slice Relative Position Predictionï¼ˆSRPPï¼‰æ¨¡å—å’ŒBoundary Detectionï¼ˆBDï¼‰æ¨¡å—æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œæ˜¾è‘—æé«˜äº†åœ¨åŒ»å­¦åˆ†å‰²æŒ‘æˆ˜èµ›ï¼ˆMSDï¼‰æ•°æ®é›†ï¼ˆåŒ…æ‹¬è‚ºã€è„¾å’Œèƒ°è…ºï¼‰ä¸Šçš„åˆ†å‰²é‡å å’Œè¾¹ç•Œç²¾åº¦ï¼Œä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>SAM2æ¨¡å‹åœ¨è§†é¢‘å¯¹è±¡åˆ†å‰²ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å¤„ç†3DåŒ»å­¦å›¾åƒæ—¶é¢ä¸´ä¸¤ä¸ªä¸»è¦é—®é¢˜ã€‚</li>
<li>é’ˆå¯¹è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†SAM2-3dMedæ¡†æ¶ï¼Œå®ƒæ˜¯ä¸“ä¸ºå¤„ç†3DåŒ»å­¦å›¾åƒè®¾è®¡çš„ã€‚</li>
<li>SAM2-3dMedå¼•å…¥äº†ä¸¤ä¸ªå…³é”®æ¨¡å—ï¼šSlice Relative Position Predictionï¼ˆSRPPï¼‰æ¨¡å—å’ŒBoundary Detectionï¼ˆBDï¼‰æ¨¡å—ã€‚SRPPæ¨¡å—é€šè¿‡é¢„æµ‹ä¸åŒåˆ‡ç‰‡ä¹‹é—´çš„ç›¸å¯¹ä½ç½®æ¥æ¨¡æ‹ŸåŒå‘åˆ‡ç‰‡é—´ä¾èµ–æ€§ï¼›BDæ¨¡å—æé«˜äº†å…³é”®å™¨å®˜å’Œç»„ç»‡çš„è¾¹ç•Œåˆ†å‰²ç²¾åº¦ã€‚</li>
<li>åœ¨ä¸‰ä¸ªä¸åŒçš„åŒ»å­¦æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒSAM2-3dMedæ˜¾è‘—æé«˜äº†åˆ†å‰²é‡å å’Œè¾¹ç•Œç²¾åº¦ï¼Œä¼˜äºå…¶ä»–æœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08967">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-3f163ea3a59c6c5fb67f1dfe29a397af~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824300&auth_key=1760824300-0-0-22f0de6e3fb48a7fc0625fb8c28cdbd7&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-70e4ba749565a5784c24980f4279d436~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824307&auth_key=1760824307-0-0-eac2f3c23866c7d271271834f6516869&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-466f441f97ae52b4fb3ea776e0611ab1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824314&auth_key=1760824314-0-0-4108a71a4d8f377777ed086deb6aecaa&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="Progressive-Uncertainty-Guided-Evidential-U-KAN-for-Trustworthy-Medical-Image-Segmentation"><a href="#Progressive-Uncertainty-Guided-Evidential-U-KAN-for-Trustworthy-Medical-Image-Segmentation" class="headerlink" title="Progressive Uncertainty-Guided Evidential U-KAN for Trustworthy Medical   Image Segmentation"></a>Progressive Uncertainty-Guided Evidential U-KAN for Trustworthy Medical   Image Segmentation</h2><p><strong>Authors:Zhen Yang, Yansong Ma, Lei Chen</strong></p>
<p>Trustworthy medical image segmentation aims at deliver accurate and reliable results for clinical decision-making. Most existing methods adopt the evidence deep learning (EDL) paradigm due to its computational efficiency and theoretical robustness. However, the EDL-based methods often neglect leveraging uncertainty maps rich in attention cues to refine ambiguous boundary segmentation. To address this, we propose a progressive evidence uncertainty guided attention (PEUA) mechanism to guide the model to focus on the feature representation learning of hard regions. Unlike conventional approaches, PEUA progressively refines attention using uncertainty maps while employing low-rank learning to denoise attention weights, enhancing feature learning for challenging regions. Concurrently, standard EDL methods suppress evidence of incorrect class indiscriminately via Kullback-Leibler (KL) regularization, impairing the uncertainty assessment in ambiguous areas and consequently distorts the corresponding attention guidance. We thus introduce a semantic-preserving evidence learning (SAEL) strategy, integrating a semantic-smooth evidence generator and a fidelity-enhancing regularization term to retain critical semantics. Finally, by embedding PEUA and SAEL with the state-of-the-art U-KAN, we proposes Evidential U-KAN, a novel solution for trustworthy medical image segmentation. Extensive experiments on 4 datasets demonstrate superior accuracy and reliability over the competing methods. The code is available at \href{<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Evidence-U-KAN-BBE8%7D%7Bgithub%7D">https://anonymous.4open.science/r/Evidence-U-KAN-BBE8}{github}</a>. </p>
<blockquote>
<p>å¯ä¿¡åŒ»å­¦å›¾åƒåˆ†å‰²æ—¨åœ¨æä¾›å‡†ç¡®å¯é çš„ç»“æœï¼Œä¸ºä¸´åºŠå†³ç­–æä¾›æ”¯æŒã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•é‡‡ç”¨è¯æ®æ·±åº¦å­¦ä¹ ï¼ˆEDLï¼‰èŒƒå¼ï¼Œå› å…¶è®¡ç®—æ•ˆç‡å’Œç†è®ºç¨³å¥æ€§ã€‚ç„¶è€Œï¼ŒåŸºäºEDLçš„æ–¹æ³•å¾€å¾€å¿½ç•¥äº†åˆ©ç”¨å¯Œå«æ³¨æ„åŠ›çº¿ç´¢çš„ä¸ç¡®å®šæ€§æ˜ å°„æ¥ç»†åŒ–æ¨¡ç³Šçš„è¾¹ç•Œåˆ†å‰²ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¸è¿›è¯æ®ä¸ç¡®å®šæ€§å¼•å¯¼æ³¨æ„åŠ›ï¼ˆPEUAï¼‰æœºåˆ¶ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨å›°éš¾åŒºåŸŸç‰¹å¾è¡¨ç¤ºå­¦ä¹ ã€‚ä¸å¸¸è§„æ–¹æ³•ä¸åŒï¼ŒPEUAåˆ©ç”¨ä¸ç¡®å®šæ€§æ˜ å°„é€æ­¥ç»†åŒ–æ³¨æ„åŠ›ï¼ŒåŒæ—¶é‡‡ç”¨ä½ç§©å­¦ä¹ å¯¹æ³¨æ„åŠ›æƒé‡è¿›è¡Œå»å™ªï¼Œå¢å¼ºå¯¹æŒ‘æˆ˜åŒºåŸŸçš„ç‰¹å¾å­¦ä¹ ã€‚åŒæ—¶ï¼Œæ ‡å‡†EDLæ–¹æ³•é€šè¿‡Kullback-Leiblerï¼ˆKLï¼‰æ­£åˆ™åŒ–æ— å·®åˆ«åœ°æŠ‘åˆ¶é”™è¯¯ç±»åˆ«çš„è¯æ®ï¼ŒæŸå®³æ¨¡ç³ŠåŒºåŸŸçš„ä¸ç¡®å®šæ€§è¯„ä¼°ï¼Œå¹¶å› æ­¤æ‰­æ›²ç›¸åº”çš„æ³¨æ„åŠ›å¼•å¯¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è¯­ä¹‰ä¿ç•™è¯æ®å­¦ä¹ ï¼ˆSAELï¼‰ç­–ç•¥ï¼Œé›†æˆè¯­ä¹‰å¹³æ»‘è¯æ®ç”Ÿæˆå™¨å’Œä¿çœŸåº¦å¢å¼ºæ­£åˆ™åŒ–é¡¹ä»¥ä¿ç•™å…³é”®è¯­ä¹‰ã€‚æœ€åï¼Œé€šè¿‡å°†PEUAå’ŒSAELåµŒå…¥åˆ°æœ€å…ˆè¿›çš„U-KANä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Evidential U-KANï¼Œè¿™æ˜¯ä¸€ç§å¯ä¿¡åŒ»å­¦å›¾åƒåˆ†å‰²çš„æ–°è§£å†³æ–¹æ¡ˆã€‚åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œå…¶åœ¨å‡†ç¡®æ€§å’Œå¯é æ€§æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Evidence-U-KAN-BBE8">https://anonymous.4open.science/r/Evidence-U-KAN-BBE8</a>çš„githubä¸Šè·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08949v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>åŒ»å­¦å›¾åƒåˆ†å‰²éœ€å‡†ç¡®å¯é ï¼Œä¸ºä¸´åºŠå†³ç­–æä¾›æ”¯æŒã€‚ç°æœ‰æ–¹æ³•å¤šé‡‡ç”¨è¯æ®æ·±åº¦å­¦ä¹ ï¼ˆEDLï¼‰èŒƒå¼ï¼Œæ³¨é‡è®¡ç®—æ•ˆç‡å’Œç†è®ºç¨³å¥æ€§ã€‚ç„¶è€Œï¼ŒEDLæ–¹æ³•å¾€å¾€å¿½è§†åˆ©ç”¨å¯Œå«æ³¨æ„åŠ›çº¿ç´¢çš„ä¸ç¡®å®šæ€§å›¾æ¥ä¼˜åŒ–æ¨¡ç³Šè¾¹ç•Œåˆ†å‰²ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†æ¸è¿›è¯æ®ä¸ç¡®å®šæ€§å¼•å¯¼æ³¨æ„åŠ›ï¼ˆPEUAï¼‰æœºåˆ¶ï¼Œå¼•å¯¼æ¨¡å‹å…³æ³¨ç¡¬åŒºåŸŸç‰¹å¾è¡¨ç¤ºå­¦ä¹ ã€‚PEUAä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼Œå®ƒåˆ©ç”¨ä¸ç¡®å®šæ€§å›¾æ¸è¿›ä¼˜åŒ–æ³¨æ„åŠ›ï¼Œå¹¶é‡‡ç”¨ä½ç§©å­¦ä¹ å»å™ªæ³¨æ„åŠ›æƒé‡ï¼Œå¢å¼ºå¯¹æŒ‘æˆ˜åŒºåŸŸçš„ç‰¹å¾å­¦ä¹ ã€‚åŒæ—¶ï¼Œæ ‡å‡†EDLæ–¹æ³•ä¼šæ— å·®åˆ«åœ°æŠ‘åˆ¶é”™è¯¯ç±»åˆ«çš„è¯æ®ï¼Œé€šè¿‡Kullback-Leiblerï¼ˆKLï¼‰æ­£åˆ™åŒ–æŸå®³æ¨¡ç³ŠåŒºåŸŸçš„ä¸ç¡®å®šæ€§è¯„ä¼°ï¼Œå¹¶å¯¼è‡´æ³¨æ„åŠ›æŒ‡å¯¼å¤±çœŸã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥è¯­ä¹‰ä¿ç•™è¯æ®å­¦ä¹ ï¼ˆSAELï¼‰ç­–ç•¥ï¼Œé›†æˆè¯­ä¹‰å¹³æ»‘è¯æ®ç”Ÿæˆå™¨å’Œä¿çœŸå¢å¼ºæ­£åˆ™åŒ–é¡¹ï¼Œä»¥ä¿ç•™å…³é”®è¯­ä¹‰ã€‚é€šè¿‡å°†PEUAå’ŒSAELåµŒå…¥åˆ°æœ€å…ˆè¿›çš„U-KANä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†è¯æ®U-KANï¼Œè¿™æ˜¯ä¸€ç§å¯é çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ–°è§£å†³æ–¹æ¡ˆã€‚åœ¨å››ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œå…¶åœ¨å‡†ç¡®æ€§å’Œå¯é æ€§æ–¹é¢ä¼˜äºå…¶ä»–æ–¹æ³•ã€‚ä»£ç å¯é€šè¿‡é“¾æ¥è®¿é—®ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŒ»å­¦å›¾åƒåˆ†å‰²éœ€è¦å‡†ç¡®å¯é çš„ç»“æœä»¥æ”¯æŒä¸´åºŠå†³ç­–ã€‚</li>
<li>ç°æœ‰è¯æ®æ·±åº¦å­¦ä¹ ï¼ˆEDLï¼‰æ–¹æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­å¿½ç•¥äº†ä¸ç¡®å®šæ€§å›¾çš„ä½œç”¨ã€‚</li>
<li>æå‡ºçš„PEUAæœºåˆ¶åˆ©ç”¨ä¸ç¡®å®šæ€§å›¾æ¸è¿›ä¼˜åŒ–æ³¨æ„åŠ›ï¼Œå¢å¼ºç‰¹å¾å­¦ä¹ ã€‚</li>
<li>æ ‡å‡†EDLæ–¹æ³•åœ¨å¤„ç†æ¨¡ç³ŠåŒºåŸŸæ—¶å­˜åœ¨ç¼ºé™·ï¼Œå¯èƒ½å½±å“ä¸ç¡®å®šæ€§è¯„ä¼°å’Œæ³¨æ„åŠ›æŒ‡å¯¼ã€‚</li>
<li>å¼•å…¥çš„SAELç­–ç•¥é€šè¿‡ä¿ç•™å…³é”®è¯­ä¹‰æ¥æ”¹å–„è¯æ®å­¦ä¹ ã€‚</li>
<li>å°†PEUAå’ŒSAELä¸U-KANç»“åˆï¼Œå½¢æˆæ–°çš„åŒ»å­¦å›¾åƒåˆ†å‰²è§£å†³æ–¹æ¡ˆâ€”â€”è¯æ®U-KANã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08949">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-92339c1f4b4e8f0d3edb5f9e500fe708~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824321&auth_key=1760824321-0-0-bda59b7b835e2d48d17a00e80d8c4716&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-f0e30870b821b3f61c2034f77976ad92~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824328&auth_key=1760824328-0-0-5996721bcdca0170a8ec5be773037a73&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-fdb4c2f44479c66a7e04c3513f2c3e80~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824335&auth_key=1760824335-0-0-53636ce2eb909958c7986b82101a67d5&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-132ac4275765094be08f5e01731bf3b8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824342&auth_key=1760824342-0-0-869f52a4822f9fed5de0e37c1d1891b0&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-1274e940beb92163d355281a4636ebda~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824348&auth_key=1760824348-0-0-fa0d30ce3b25b30ec8c567ea355cf2c4&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-7d15f30f3d5d10516f73df14ef0aee4f~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824355&auth_key=1760824355-0-0-1bd8ad33283ac057c1c23cfe9211f8fd&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="The-Boundaries-of-Fair-AI-in-Medical-Image-Prognosis-A-Causal-Perspective"><a href="#The-Boundaries-of-Fair-AI-in-Medical-Image-Prognosis-A-Causal-Perspective" class="headerlink" title="The Boundaries of Fair AI in Medical Image Prognosis: A Causal   Perspective"></a>The Boundaries of Fair AI in Medical Image Prognosis: A Causal   Perspective</h2><p><strong>Authors:Thai-Hoang Pham, Jiayuan Chen, Seungyeon Lee, Yuanlong Wang, Sayoko Moroi, Xueru Zhang, Ping Zhang</strong></p>
<p>As machine learning (ML) algorithms are increasingly used in medical image analysis, concerns have emerged about their potential biases against certain social groups. Although many approaches have been proposed to ensure the fairness of ML models, most existing works focus only on medical image diagnosis tasks, such as image classification and segmentation, and overlooked prognosis scenarios, which involve predicting the likely outcome or progression of a medical condition over time. To address this gap, we introduce FairTTE, the first comprehensive framework for assessing fairness in time-to-event (TTE) prediction in medical imaging. FairTTE encompasses a diverse range of imaging modalities and TTE outcomes, integrating cutting-edge TTE prediction and fairness algorithms to enable systematic and fine-grained analysis of fairness in medical image prognosis. Leveraging causal analysis techniques, FairTTE uncovers and quantifies distinct sources of bias embedded within medical imaging datasets. Our large-scale evaluation reveals that bias is pervasive across different imaging modalities and that current fairness methods offer limited mitigation. We further demonstrate a strong association between underlying bias sources and model disparities, emphasizing the need for holistic approaches that target all forms of bias. Notably, we find that fairness becomes increasingly difficult to maintain under distribution shifts, underscoring the limitations of existing solutions and the pressing need for more robust, equitable prognostic models. </p>
<blockquote>
<p>éšç€æœºå™¨å­¦ä¹ ç®—æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œäººä»¬è¶Šæ¥è¶Šå…³æ³¨å®ƒä»¬å¯¹ç‰¹å®šç¤¾ä¼šç¾¤ä½“çš„æ½œåœ¨åè§ã€‚å°½ç®¡å·²ç»æå‡ºäº†è®¸å¤šæ–¹æ³•æ¥ç¡®ä¿æœºå™¨å­¦ä¹ æ¨¡å‹çš„å…¬å¹³æ€§ï¼Œä½†å¤§å¤šæ•°ç°æœ‰å·¥ä½œåªå…³æ³¨åŒ»å­¦å›¾åƒè¯Šæ–­ä»»åŠ¡ï¼Œå¦‚å›¾åƒåˆ†ç±»å’Œåˆ†å‰²ï¼Œè€Œå¿½ç•¥äº†é¢„åæƒ…æ™¯ï¼Œè¿™æ¶‰åŠåˆ°é¢„æµ‹åŒ»ç–—çŠ¶å†µéšæ—¶é—´å¯èƒ½çš„ç»“æœæˆ–è¿›å±•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å¼•å…¥äº†FairTTEï¼Œè¿™æ˜¯åŒ»å­¦æˆåƒä¸­è¯„ä¼°æ—¶é—´åˆ°äº‹ä»¶ï¼ˆTTEï¼‰é¢„æµ‹å…¬å¹³æ€§çš„é¦–ä¸ªç»¼åˆæ¡†æ¶ã€‚FairTTEæ¶µç›–äº†å¹¿æ³›çš„æˆåƒæ–¹å¼å’ŒTTEç»“æœï¼Œé›†æˆäº†å‰æ²¿çš„TTEé¢„æµ‹å’Œå…¬å¹³æ€§ç®—æ³•ï¼Œèƒ½å¤Ÿå¯¹åŒ»å­¦å›¾åƒé¢„åçš„å…¬å¹³æ€§è¿›è¡Œç³»ç»Ÿå’Œç²¾ç»†çš„åˆ†æã€‚å€ŸåŠ©å› æœåˆ†ææŠ€æœ¯ï¼ŒFairTTEèƒ½å¤Ÿå‘ç°å’Œé‡åŒ–åŒ»å­¦æˆåƒæ•°æ®é›†ä¸­åµŒå…¥çš„ä¸åŒåè§æ¥æºã€‚æˆ‘ä»¬çš„å¤§è§„æ¨¡è¯„ä¼°æ˜¾ç¤ºï¼Œä¸åŒæˆåƒæ–¹å¼ä¸­éƒ½æ™®éå­˜åœ¨åè§ï¼Œç›®å‰çš„å…¬å¹³æ–¹æ³•æä¾›çš„ç¼“è§£æªæ–½æœ‰é™ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜äº†åŸºç¡€åè§æ¥æºä¸æ¨¡å‹å·®å¼‚ä¹‹é—´çš„å¼ºçƒˆå…³è”ï¼Œå¼ºè°ƒéœ€è¦å…¨é¢å…³æ³¨æ‰€æœ‰å½¢å¼çš„åè§çš„æ–¹æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å…¬å¹³åœ¨åˆ†å¸ƒå˜åŒ–çš„æƒ…å†µä¸‹è¶Šæ¥è¶Šéš¾ä»¥ç»´æŒï¼Œè¿™çªæ˜¾äº†ç°æœ‰è§£å†³æ–¹æ¡ˆçš„å±€é™æ€§ä»¥åŠå¯¹æ›´ç¨³å¥ã€å…¬å¹³çš„é¢„åæ¨¡å‹çš„è¿«åˆ‡éœ€æ±‚ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08840v1">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä¸»è¦å…³æ³¨æœºå™¨å­¦ä¹ ç®—æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­çš„æ½œåœ¨åè§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„æµ‹ç–¾ç—…å‘ç”Ÿæˆ–è¿›å±•çš„é¢„åæƒ…æ™¯ä¸­çš„å…¬å¹³æ€§è¯„ä¼°ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†FairTTEæ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç³»ç»Ÿåœ°ã€ç²¾ç»†åœ°åˆ†æåŒ»å­¦å›¾åƒé¢„åä¸­çš„å…¬å¹³æ€§ï¼Œæ¶µç›–å¤šç§æˆåƒæ–¹å¼å’Œæ—¶é—´è‡³äº‹ä»¶ï¼ˆTTEï¼‰ç»“æœï¼Œå¹¶é›†æˆäº†å…ˆè¿›çš„TTEé¢„æµ‹å’Œå…¬å¹³æ€§ç®—æ³•ã€‚é€šè¿‡å› æœåˆ†ææŠ€æœ¯ï¼ŒFairTTEèƒ½å¤Ÿå‘ç°å’Œé‡åŒ–åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸­çš„ä¸åŒåè§æ¥æºã€‚æ–‡ç« è¿˜æŒ‡å‡ºå½“å‰å…¬å¹³æ€§æ–¹æ³•æä¾›çš„ç¼“è§£æªæ–½æœ‰é™ï¼Œåè§åœ¨å¤šç§æˆåƒæ¨¡æ€ä¸­æ™®éå­˜åœ¨ï¼Œä¸”å­˜åœ¨æ¨¡å‹å·®å¼‚æ€§ä¸åº•å±‚åè§æ¥æºä¹‹é—´çš„å¼ºçƒˆå…³è”ã€‚æ­¤å¤–ï¼Œæ–‡ç« å¼ºè°ƒäº†å…¬å¹³æ€§çš„ç»´æŠ¤åœ¨åˆ†å¸ƒè½¬ç§»ä¸‹å˜å¾—æ›´åŠ å›°éš¾ï¼Œçªæ˜¾äº†ç°æœ‰è§£å†³æ–¹æ¡ˆçš„å±€é™æ€§ä»¥åŠå¯¹æ›´ç¨³å¥ã€å…¬å¹³çš„é¢„åæ¨¡å‹çš„è¿«åˆ‡éœ€æ±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœºå™¨å­¦ä¹ ç®—æ³•åœ¨åŒ»å­¦å›¾åƒåˆ†æä¸­å­˜åœ¨æ½œåœ¨åè§é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢„åæƒ…æ™¯ä¸­çš„å…¬å¹³æ€§è¯„ä¼°ã€‚</li>
<li>FairTTEæ¡†æ¶è¢«æå‡ºç”¨äºç³»ç»Ÿåœ°ã€ç²¾ç»†åœ°åˆ†æåŒ»å­¦å›¾åƒé¢„åä¸­çš„å…¬å¹³æ€§ã€‚</li>
<li>FairTTEæ¶µç›–å¤šç§æˆåƒæ–¹å¼å’Œæ—¶é—´è‡³äº‹ä»¶ï¼ˆTTEï¼‰ç»“æœï¼Œé›†æˆå…ˆè¿›çš„TTEé¢„æµ‹å’Œå…¬å¹³æ€§ç®—æ³•ã€‚</li>
<li>é€šè¿‡å› æœåˆ†ææŠ€æœ¯ï¼ŒFairTTEèƒ½å¤Ÿå‘ç°å’Œé‡åŒ–åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸­çš„åè§æ¥æºã€‚</li>
<li>å½“å‰å…¬å¹³æ€§æ–¹æ³•æä¾›çš„ç¼“è§£æªæ–½æœ‰é™ï¼Œåè§åœ¨å¤šç§æˆåƒæ¨¡æ€ä¸­æ™®éå­˜åœ¨ã€‚</li>
<li>æ¨¡å‹å·®å¼‚æ€§ä¸åº•å±‚åè§æ¥æºä¹‹é—´å­˜åœ¨å¼ºçƒˆå…³è”ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.08840">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-d823eede807a7920865c63e3aca99aa1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824362&auth_key=1760824362-0-0-5b878180f67e374f5e3961830c271e21&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d06a4ecdcb8eed35946ecb94e87d29a5~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824370&auth_key=1760824370-0-0-6feca4989e80d9fbb6e9e0990deb5af8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-121b564434d3ded2b680a8aa00555e76~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824377&auth_key=1760824377-0-0-ae01f476c62d17a78cd4f89df966067f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5466798bf23f3864887dee6c445ef6c8~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824383&auth_key=1760824383-0-0-dffcb87f53b07ef14c128d49bd2e716b&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="A-Denoising-Framework-for-Real-World-Ultra-Low-Dose-Lung-CT-Images-Based-on-an-Image-Purification-Strategy"><a href="#A-Denoising-Framework-for-Real-World-Ultra-Low-Dose-Lung-CT-Images-Based-on-an-Image-Purification-Strategy" class="headerlink" title="A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based   on an Image Purification Strategy"></a>A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based   on an Image Purification Strategy</h2><p><strong>Authors:Guoliang Gong, Man Yu</strong></p>
<p>Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but introduces severe noise and artifacts. It also leads to substantial spatial misalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses challenges for directly applying existing denoising networks trained on synthetic noise or aligned data. To address this core challenge in uLDCT denoising, this paper proposes an innovative denoising framework based on an Image Purification (IP) strategy. First, we construct a real clinical uLDCT lung dataset. Then, we propose an Image Purification strategy that generates structurally aligned uLDCT-NDCT image pairs, providing a high-quality data foundation for network training. Building upon this, we propose a Frequency-domain Flow Matching (FFM) model, which works synergistically with the IP strategy to excellently preserve the anatomical structure integrity of denoised images. Experiments on the real clinical dataset demonstrate that our IP strategy significantly enhances the performance of multiple mainstream denoising models on the uLDCT task. Notably, our proposed FFM model combined with the IP strategy achieves state-of-the-art (SOTA) results in anatomical structure preservation. This study provides an effective solution to the data mismatch problem in real-world uLDCT denoising. Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/MonkeyDadLufy/flow-matching">https://github.com/MonkeyDadLufy/flow-matching</a>. </p>
<blockquote>
<p>è¶…ä½å‰‚é‡CTï¼ˆuLDCTï¼‰æ˜¾è‘—å‡å°‘äº†è¾å°„æš´éœ²ï¼Œä½†å¼•å…¥äº†ä¸¥é‡çš„å™ªå£°å’Œä¼ªå½±ã€‚å®ƒè¿˜å¯¼è‡´uLDCTä¸æ­£å¸¸å‰‚é‡CTï¼ˆNDCTï¼‰å›¾åƒå¯¹ä¹‹é—´å‡ºç°è¾ƒå¤§çš„ç©ºé—´é”™ä½ã€‚è¿™ä¸ºç›´æ¥åº”ç”¨ç°æœ‰åˆæˆå™ªå£°æˆ–å¯¹é½æ•°æ®è®­ç»ƒçš„é™å™ªç½‘ç»œå¸¦æ¥äº†æŒ‘æˆ˜ã€‚é’ˆå¯¹uLDCTé™å™ªä¸­çš„è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒå‡€åŒ–ï¼ˆIPï¼‰ç­–ç•¥çš„é™å™ªæ¡†æ¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ„å»ºäº†çœŸå®çš„ä¸´åºŠuLDCTè‚ºéƒ¨æ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å›¾åƒå‡€åŒ–ç­–ç•¥ï¼Œç”Ÿæˆç»“æ„å¯¹é½çš„uLDCT-NDCTå›¾åƒå¯¹ï¼Œä¸ºç½‘ç»œè®­ç»ƒæä¾›é«˜è´¨é‡çš„æ•°æ®åŸºç¡€ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æå‡ºäº†é¢‘åŸŸæµåŒ¹é…ï¼ˆFFMï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¸IPç­–ç•¥ååŒå·¥ä½œï¼Œå‡ºè‰²åœ°ä¿ç•™äº†å»å™ªå›¾åƒçš„è§£å‰–ç»“æ„å®Œæ•´æ€§ã€‚åœ¨çœŸå®ä¸´åºŠæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„IPç­–ç•¥æ˜¾è‘—æé«˜äº†å¤šä¸ªä¸»æµé™å™ªæ¨¡å‹åœ¨uLDCTä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æå‡ºçš„FFMæ¨¡å‹ä¸IPç­–ç•¥ç›¸ç»“åˆï¼Œåœ¨è§£å‰–ç»“æ„ä¿ç•™æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚æœ¬ç ”ç©¶ä¸ºè§£å†³çœŸå®ä¸–ç•ŒuLDCTé™å™ªä¸­çš„æ•°æ®ä¸åŒ¹é…é—®é¢˜æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/MonkeyDadLufy/flow-matching%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MonkeyDadLufy/flow-matchingè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.07492v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬è®ºæ–‡é’ˆå¯¹è¶…ä½å‰‚é‡CTï¼ˆuLDCTï¼‰å›¾åƒä¸­çš„å™ªå£°å’Œä¼ªå½±é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå›¾åƒå‡€åŒ–ï¼ˆIPï¼‰ç­–ç•¥çš„é™å™ªæ¡†æ¶ã€‚é€šè¿‡æ„å»ºçœŸå®ä¸´åºŠuLDCTè‚ºéƒ¨æ•°æ®é›†ï¼Œæå‡ºä¸€ç§ç”Ÿæˆç»“æ„å¯¹é½çš„uLDCT-NDCTå›¾åƒå¯¹çš„æ–¹æ³•ï¼Œä¸ºç½‘ç»œè®­ç»ƒæä¾›é«˜è´¨é‡çš„æ•°æ®åŸºç¡€ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œç»“åˆé¢‘åŸŸæµåŒ¹é…ï¼ˆFFMï¼‰æ¨¡å‹ï¼ŒååŒIPç­–ç•¥ï¼Œå‡ºè‰²åœ°ä¿ç•™äº†å»å™ªå›¾åƒçš„è§£å‰–ç»“æ„å®Œæ•´æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥ç­–ç•¥æ˜¾è‘—æå‡ä¸»æµå»å™ªæ¨¡å‹åœ¨uLDCTä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å‰–ç»“æ„ä¿ç•™æ–¹é¢è¾¾åˆ°å…ˆè¿›æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¶…ä½å‰‚é‡CTï¼ˆuLDCTï¼‰é™ä½äº†è¾å°„æš´éœ²ï¼Œä½†å¼•å…¥äº†ä¸¥é‡çš„å™ªå£°å’Œä¼ªå½±ã€‚</li>
<li>uLDCTä¸æ­£å¸¸å‰‚é‡CTï¼ˆNDCTï¼‰å›¾åƒå¯¹ä¹‹é—´å­˜åœ¨ç©ºé—´ä¸å¯¹é½é—®é¢˜ï¼Œç»™ç°æœ‰é™å™ªç½‘ç»œçš„ç›´æ¥åº”ç”¨å¸¦æ¥æŒ‘æˆ˜ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾åƒå‡€åŒ–ï¼ˆIPï¼‰ç­–ç•¥çš„é™å™ªæ¡†æ¶ï¼Œæ„å»ºçœŸå®ä¸´åºŠuLDCTè‚ºéƒ¨æ•°æ®é›†ã€‚</li>
<li>è®ºæ–‡å¼•å…¥äº†é¢‘åŸŸæµåŒ¹é…ï¼ˆFFMï¼‰æ¨¡å‹ï¼Œä¸IPç­–ç•¥ç»“åˆï¼Œæœ‰æ•ˆä¿ç•™å»å™ªå›¾åƒçš„è§£å‰–ç»“æ„å®Œæ•´æ€§ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒIPç­–ç•¥æ˜¾è‘—æå‡ä¸»æµå»å™ªæ¨¡å‹åœ¨uLDCTä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>ç»“åˆIPç­–ç•¥å’ŒFFMæ¨¡å‹çš„æ–¹æ¡ˆåœ¨è§£å‰–ç»“æ„ä¿ç•™æ–¹é¢è¾¾åˆ°å…ˆè¿›æ°´å¹³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.07492">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://pic-private.zhihu.com/v2-f4a434f71cc2bc9424c7a73b6630adef~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824391&auth_key=1760824391-0-0-7fa7fd81549da91ca1e00103737aa50e&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-031afa7e22350fb7e098885b936f8a14~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824398&auth_key=1760824398-0-0-f6e6b68b5ffb261e8594dc15085db38f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-35cb7e17076619c8f510270b7c757211~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824405&auth_key=1760824405-0-0-4fc64a4dc6d1adc708f6a5191ea4fa9f&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-e682f9b498a8dc4418e83f5c2b2aa71c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824411&auth_key=1760824411-0-0-661bb5a13809b00f7d66b425cda38db8&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-5f9257e51d861c95c777b8ea5d1405c1~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824417&auth_key=1760824417-0-0-4d76c23797a94f8cea1b49b4722ad82d&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-4a09a249e5de3de82e8096ed77062a4c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824424&auth_key=1760824424-0-0-4373e4b72161bd5f1117fddf1e3ffb60&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
<img src="https://pic-private.zhihu.com/v2-d5a580609045785a9bedfae3fb5edf5c~resize:0:q75.jpg?source=1f5c5e47&expiration=1760824430&auth_key=1760824430-0-0-8b9c0c5d49ac4be55f1597c31e6dd871&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-10-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">https://kedreamix.github.io/Talk2Paper/Paper/2025-10-19/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                                    <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-19/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-6ca024a8b174edad67272a0321794bca~resize:0:q75.jpg?source=1f5c5e47&expiration=1760825438&auth_key=1760825438-0-0-add3fbed820b33d68225fa53d3e78d14&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-19  LaMoGen Laban Movement-Guided Diffusion for Text-to-Motion Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-10-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-10-19/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://pic-private.zhihu.com/v2-48d7a0de5dcce92de9e8e1aff5191110~resize:0:q75.jpg?source=1f5c5e47&expiration=1760821529&auth_key=1760821529-0-0-1661c5edbe86f02a7ccbc7fa110f05d2&protocol=v2&sampling=False&animatedImagePlayCount=1&overTime=60&incremental=False&sceneCode=article_draft_web&animatedImageAutoPlay=False&retryCount=3&precoder=False" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-10-19  Synthetic History Evaluating Visual Representations of the Past in   Diffusion Models
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-10-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32140.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
