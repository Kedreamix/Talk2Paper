<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-21  A Survey of Reinforcement Learning for Large Reasoning Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-86e6b9669dd266acde172d119f6c3aad')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-09-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    70 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-09-21-æ›´æ–°"><a href="#2025-09-21-æ›´æ–°" class="headerlink" title="2025-09-21 æ›´æ–°"></a>2025-09-21 æ›´æ–°</h1><h2 id="A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models"><a href="#A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models" class="headerlink" title="A Survey of Reinforcement Learning for Large Reasoning Models"></a>A Survey of Reinforcement Learning for Large Reasoning Models</h2><p><strong>Authors:Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, Bowen Zhou</strong></p>
<p>In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: <a target="_blank" rel="noopener" href="https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs">https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs</a> </p>
<blockquote>
<p>æœ¬æ–‡ç»¼è¿°äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚å¼ºåŒ–å­¦ä¹ åœ¨æ¨è¿›LLMèƒ½åŠ›è¾¹ç•Œæ–¹é¢å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ•°å­¦å’Œç¼–ç ç­‰å¤æ‚é€»è¾‘ä»»åŠ¡æ–¹é¢ã€‚å› æ­¤ï¼Œå¼ºåŒ–å­¦ä¹ å·²ç»æˆä¸ºå°†LLMè½¬åŒ–ä¸ºLRMçš„åŸºç¡€æ–¹æ³•ã€‚éšç€è¯¥é¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼ŒRLåœ¨LRMä¸Šçš„è¿›ä¸€æ­¥æ‰©å±•ç°åœ¨ä¸ä»…é¢ä¸´ç€è®¡ç®—èµ„æºçš„æŒ‘æˆ˜ï¼Œè¿˜é¢ä¸´ç€ç®—æ³•è®¾è®¡ã€è®­ç»ƒæ•°æ®å’ŒåŸºç¡€è®¾æ–½æ–¹é¢çš„æŒ‘æˆ˜ã€‚å› æ­¤ï¼Œé‡æ–°å®¡è§†è¯¥é¢†åŸŸçš„å‘å±•ï¼Œé‡æ–°è¯„ä¼°å…¶è½¨è¿¹ï¼Œæ¢ç´¢æé«˜RLåœ¨äººå·¥æ™ºèƒ½è¶…çº§æ™ºèƒ½ï¼ˆASIï¼‰æ–¹é¢çš„å¯æ‰©å±•æ€§çš„ç­–ç•¥å…·æœ‰é‡è¦æ„ä¹‰ã€‚å°¤å…¶æ˜¯ï¼Œæˆ‘ä»¬ç ”ç©¶è‡ªDeepSeek-R1å‘å¸ƒä»¥æ¥ï¼Œå°†RLåº”ç”¨äºLLMå’ŒLRMçš„æ¨ç†èƒ½åŠ›çš„ç ”ç©¶ï¼ŒåŒ…æ‹¬åŸºç¡€ç»„ä»¶ã€æ ¸å¿ƒé—®é¢˜ã€è®­ç»ƒèµ„æºå’Œä¸‹æ¸¸åº”ç”¨ï¼Œä»¥ç¡®å®šè¿™ä¸€å¿«é€Ÿæ¼”å˜é¢†åŸŸçš„æœªæ¥æœºé‡å’Œæ–¹å‘ã€‚æˆ‘ä»¬å¸Œæœ›è¿™æ¬¡ç»¼è¿°èƒ½å¤Ÿä¿ƒè¿›RLåœ¨æ›´å¹¿æ³›çš„æ¨ç†æ¨¡å‹ä¸Šçš„æœªæ¥ç ”ç©¶ã€‚Githubï¼š<a target="_blank" rel="noopener" href="https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs">https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08827v2">PDF</a> Fixed typos; added missing and recent citations (114 -&gt; 117 pages)</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ åœ¨æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†æ•°å­¦å’Œç¼–ç ç­‰å¤æ‚é€»è¾‘ä»»åŠ¡æ–¹é¢ã€‚æœ¬è®ºæ–‡å›é¡¾äº†å¼ºåŒ–å­¦ä¹ åœ¨LLMå’ŒLRMï¼ˆè¯­è¨€æ¨ç†æ¨¡å‹ï¼‰ä¸­çš„åº”ç”¨è¿›å±•ï¼Œå¹¶æ¢è®¨äº†å…¶é¢ä¸´çš„åŸºç¡€æŒ‘æˆ˜å’Œæœªæ¥çš„å‘å±•æ–¹å‘ã€‚é€šè¿‡æ·±å…¥äº†è§£å…¶å…³é”®ç»„ä»¶ã€æ ¸å¿ƒé—®é¢˜ã€è®­ç»ƒèµ„æºå’Œä¸‹æ¸¸åº”ç”¨ï¼Œæˆ‘ä»¬ä¸ºè¿™ä¸€å¿«é€Ÿæ¼”å˜çš„é¢†åŸŸæä¾›äº†æœªæ¥çš„æœºé‡å’Œæ–¹å‘ã€‚å¸Œæœ›æœ¬æ¬¡ç»¼è¿°èƒ½æ¨åŠ¨åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ›´å¹¿æ³›æ¨ç†æ¨¡å‹çš„ç ”ç©¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨æ¨åŠ¨å¤§å‹è¯­è¨€æ¨¡å‹èƒ½åŠ›æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚é€»è¾‘ä»»åŠ¡æ–¹é¢ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ å·²æˆä¸ºå°†å¤§å‹è¯­è¨€æ¨¡å‹è½¬åŒ–ä¸ºè¯­è¨€æ¨ç†æ¨¡å‹çš„åŸºç¡€æ–¹æ³•ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ åœ¨LLMå’ŒLRMé¢†åŸŸçš„åº”ç”¨é¢ä¸´åŸºç¡€æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®¡ç®—èµ„æºã€ç®—æ³•è®¾è®¡ã€è®­ç»ƒæ•°æ®å’ŒåŸºç¡€è®¾æ–½ç­‰æ–¹é¢ã€‚</li>
<li>è®ºæ–‡è¯¦ç»†ç ”ç©¶äº†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºLLMå’ŒLRMçš„æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯è‡ªDeepSeek-R1å‘å¸ƒä»¥æ¥çš„ç ”ç©¶ã€‚</li>
<li>è®ºæ–‡å¼ºè°ƒäº†ç†è§£è¯¥é¢†åŸŸçš„æ ¸å¿ƒé—®é¢˜ã€è®­ç»ƒèµ„æºå’Œä¸‹æ¸¸åº”ç”¨çš„é‡è¦æ€§ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</li>
<li>å½“å‰é¢†åŸŸå‘å±•è¿…é€Ÿï¼Œå…·æœ‰å¹¿é˜”çš„æœªæ¥æœºé‡å’Œç ”ç©¶æ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08827">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e5d484980b7924334b07bd1dd014ea01" align="middle">
<img src="https://picx.zhimg.com/v2-0b2af3342df4c2c785a69a48ab4320e2" align="middle">
<img src="https://picx.zhimg.com/v2-f3650cbea26b4cd0a3ae0a166192527a" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MovieCORE-COgnitive-REasoning-in-Movies"><a href="#MovieCORE-COgnitive-REasoning-in-Movies" class="headerlink" title="MovieCORE: COgnitive REasoning in Movies"></a>MovieCORE: COgnitive REasoning in Movies</h2><p><strong>Authors:Gueter Josmy Faure, Min-Hung Chen, Jia-Fong Yeh, Ying Cheng, Hung-Ting Su, Yung-Hao Tang, Shang-Hong Lai, Winston H. Hsu</strong></p>
<p>This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at <a target="_blank" rel="noopener" href="https://joslefaure.github.io/assets/html/moviecore.html">https://joslefaure.github.io/assets/html/moviecore.html</a>. </p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†MovieCOREï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„è§†é¢‘é—®ç­”ï¼ˆVQAï¼‰æ•°æ®é›†ï¼Œæ—¨åœ¨æ¢ç©¶å¯¹ç”µå½±å†…å®¹çš„æ›´æ·±å±‚æ¬¡è®¤çŸ¥ç†è§£ã€‚ä¸ç°æœ‰ä¸»è¦é›†ä¸­åœ¨è¡¨é¢å±‚æ¬¡ç†è§£çš„æ•°æ®é›†ä¸åŒï¼ŒMovieCOREä¾§é‡äºæå‡ºéœ€è¦ç³»ç»Ÿ2å‹æ€ç»´çš„é—®é¢˜ï¼ŒåŒæ—¶é’ˆå¯¹è§†é¢‘ææ–™è¿›è¡Œæ·±å…¥æ¢è®¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„ä»£ç†å¤´è„‘é£æš´æ–¹æ³•ï¼Œåˆ©ç”¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ€è€ƒä»£ç†ï¼Œç”Ÿæˆå¹¶ä¼˜åŒ–é«˜è´¨é‡çš„é—®é¢˜-ç­”æ¡ˆå¯¹ã€‚ä¸ºäº†è¯„ä¼°æ•°æ®é›†çš„è´¨é‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç³»åˆ—è®¤çŸ¥æµ‹è¯•ï¼Œè¯„ä¼°æ·±åº¦ã€æ€ç»´æ¿€å‘æ½œåŠ›å’Œå¥æ³•å¤æ‚æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°æ–¹æ¡ˆï¼Œä»¥è¯„ä¼°VQAæ¨¡å‹åœ¨æ›´æ·±å±‚æ¬¡è®¤çŸ¥ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³ç°æœ‰è§†é¢‘è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å±€é™æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»£ç†å¢å¼ºæ¨¡å—ï¼ˆACEï¼‰ï¼Œå³åœ¨è®­ç»ƒåé€šè¿‡é«˜è¾¾25%çš„æ”¹è¿›æé«˜æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºæ¨è¿›AIç³»ç»Ÿä¸­çš„ç”µå½±ç†è§£æä¾›äº†è´¡çŒ®ï¼Œå¹¶ä¸ºå½“å‰VQAæ¨¡å‹åœ¨é¢å¯¹å…³äºç”µå½±å†…å®¹çš„æ›´å…·æŒ‘æˆ˜æ€§å’Œç»†å¾®çš„é—®é¢˜æ—¶æä¾›äº†æœ‰ä»·å€¼çš„èƒ½åŠ›ä¸å±€é™æ€§çš„è§è§£ã€‚æˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ã€æ•°æ®é›†å’Œä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://joslefaure.github.io/assets/html/moviecore.html">https://joslefaure.github.io/assets/html/moviecore.html</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19026v3">PDF</a> Accepted for EMNLPâ€™2025 Main Conference (Oral Presentation). Project   Page: <a target="_blank" rel="noopener" href="https://joslefaure.github.io/assets/html/moviecore.html">https://joslefaure.github.io/assets/html/moviecore.html</a></p>
<p><strong>Summary</strong><br>æœ¬æ–‡ä»‹ç»äº†MovieCOREæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨æ·±å…¥äº†è§£ç”µå½±å†…å®¹çš„æ–°å‹è§†é¢‘é—®ç­”ï¼ˆVQAï¼‰æ•°æ®é›†ã€‚ä¸åŒäºç°æœ‰çš„å…³æ³¨è¡¨å±‚ç†è§£çš„æ•°æ®é›†ï¼ŒMovieCOREä¾§é‡äºæ¿€å‘ç³»ç»Ÿæ€è€ƒäºŒçº§çš„é—®é¢˜ï¼Œå¹¶é’ˆå¯¹è§†é¢‘ææ–™ç‰¹å®šè®¾è®¡ã€‚æœ¬æ–‡æå‡ºäº†åˆ©ç”¨å¤šä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºæ€ç»´ä»£ç†çš„æ™ºèƒ½å¤´è„‘é£æš´æ–¹æ³•ï¼Œç”Ÿæˆå¹¶ä¼˜åŒ–é«˜è´¨é‡çš„é—®é¢˜ç­”æ¡ˆå¯¹ã€‚é€šè¿‡å¼€å‘ä¸€ç³»åˆ—è®¤çŸ¥æµ‹è¯•æ¥è¯„ä¼°æ•°æ®é›†è´¨é‡ï¼ŒåŒ…æ‹¬æ·±åº¦ã€æ€è€ƒå¯å‘æ½œåŠ›å’Œå¥æ³•å¤æ‚æ€§ã€‚åŒæ—¶ï¼Œä¸ºè¯„ä¼°VQAæ¨¡å‹åœ¨æ·±å±‚è®¤çŸ¥ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œæå‡ºäº†å…¨é¢çš„è¯„ä¼°æ–¹æ¡ˆã€‚ä¸ºè§£å†³ç°æœ‰è§†é¢‘è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ï¼Œå¼•å…¥äº†å¢å¼ºä»£ç†é€‰æ‹©å¢å¼ºï¼ˆACEï¼‰æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨è®­ç»ƒåæé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚æœ¬æ–‡å·¥ä½œä¸ºæ¨è¿›AIç³»ç»Ÿä¸­çš„ç”µå½±ç†è§£æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼Œå¹¶å±•ç¤ºäº†å½“å‰VQAæ¨¡å‹åœ¨é¢å¯¹æ›´å…·æŒ‘æˆ˜æ€§å’Œç»†å¾®æ€§çš„é—®é¢˜æ—¶çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MovieCOREæ˜¯ä¸€ä¸ªæ—¨åœ¨ä¿ƒè¿›å¯¹ç”µå½±å†…å®¹çš„æ·±å±‚è®¤çŸ¥ç†è§£çš„æ–°å‹è§†é¢‘é—®ç­”ï¼ˆVQAï¼‰æ•°æ®é›†ã€‚</li>
<li>ä¸å…¶ä»–æ•°æ®é›†ç›¸æ¯”ï¼ŒMovieCOREä¾§é‡äºæ¿€å‘ç³»ç»ŸäºŒçº§æ€è€ƒçš„é—®é¢˜ã€‚</li>
<li>é‡‡ç”¨æ™ºèƒ½å¤´è„‘é£æš´æ–¹æ³•ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå’Œä¼˜åŒ–é—®é¢˜ç­”æ¡ˆå¯¹ã€‚</li>
<li>é€šè¿‡è®¤çŸ¥æµ‹è¯•è¯„ä¼°æ•°æ®é›†è´¨é‡ï¼ŒåŒ…æ‹¬æ·±åº¦ã€å¯å‘æ½œåŠ›å’Œå¥æ³•å¤æ‚æ€§ã€‚</li>
<li>æå‡ºäº†å…¨é¢çš„è¯„ä¼°æ–¹æ¡ˆï¼Œç”¨äºè¯„ä¼°VQAæ¨¡å‹åœ¨æ·±å±‚è®¤çŸ¥ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>å¼•å…¥Agentic Choice Enhancement (ACE)æ¨¡å—ï¼Œä»¥æé«˜è§†é¢‘è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19026">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-28f9f8dbc3d715039803a0e153b94041" align="middle">
<img src="https://picx.zhimg.com/v2-8c3ef313c12bbbdc7fc36817bc018844" align="middle">
<img src="https://picx.zhimg.com/v2-55a0a58baac0cc6f47e33d2a2d8ad963" align="middle">
<img src="https://picx.zhimg.com/v2-8753f1f17c590e40b628c48e2b3d0a3d" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="InMind-Evaluating-LLMs-in-Capturing-and-Applying-Individual-Human-Reasoning-Styles"><a href="#InMind-Evaluating-LLMs-in-Capturing-and-Applying-Individual-Human-Reasoning-Styles" class="headerlink" title="InMind: Evaluating LLMs in Capturing and Applying Individual Human   Reasoning Styles"></a>InMind: Evaluating LLMs in Capturing and Applying Individual Human   Reasoning Styles</h2><p><strong>Authors:Zizhen Li, Chuanhao Li, Yibin Wang, Qi Chen, Diping Song, Yukang Feng, Jianwen Sun, Jiaxin Ai, Fanrui Zhang, Mingzhu Sun, Kaipeng Zhang</strong></p>
<p>LLMs have shown strong performance on human-centric reasoning tasks. While previous evaluations have explored whether LLMs can infer intentions or detect deception, they often overlook the individualized reasoning styles that influence how people interpret and act in social contexts. Social deduction games (SDGs) provide a natural testbed for evaluating individualized reasoning styles, where different players may adopt diverse but contextually valid reasoning strategies under identical conditions. To address this, we introduce InMind, a cognitively grounded evaluation framework designed to assess whether LLMs can capture and apply personalized reasoning styles in SDGs. InMind enhances structured gameplay data with round-level strategy traces and post-game reflections, collected under both Observer and Participant modes. It supports four cognitively motivated tasks that jointly evaluate both static alignment and dynamic adaptation. As a case study, we apply InMind to the game Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o frequently rely on lexical cues, struggling to anchor reflections in temporal gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These findings reveal key limitations in current LLMsâ€™ capacity for individualized, adaptive reasoning, and position InMind as a step toward cognitively aligned human-AI interaction. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨äººç±»ä¸ºä¸­å¿ƒçš„æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚è™½ç„¶ä¹‹å‰çš„è¯„ä¼°å·²ç»æ¢è®¨äº†LLMæ˜¯å¦èƒ½å¤Ÿæ¨æ–­æ„å›¾æˆ–æ£€æµ‹æ¬ºéª—ï¼Œä½†å®ƒä»¬ç»å¸¸å¿½ç•¥å½±å“äººä»¬åœ¨ç¤¾ä¼šç¯å¢ƒä¸­è§£é‡Šå’Œè¡Œä¸ºçš„ä¸ªæ€§åŒ–æ¨ç†é£æ ¼ã€‚ç¤¾ä¼šæ¨ç†æ¸¸æˆï¼ˆSDGï¼‰ä¸ºè¯„ä¼°ä¸ªæ€§åŒ–æ¨ç†é£æ ¼æä¾›äº†å¤©ç„¶çš„æµ‹è¯•åºŠï¼Œåœ¨ä¸åŒæ¡ä»¶ä¸‹ï¼Œä¸åŒçš„ç©å®¶å¯èƒ½ä¼šé‡‡ç”¨å¤šæ ·ä½†è¯­å¢ƒæœ‰æ•ˆçš„æ¨ç†ç­–ç•¥ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†InMindï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè®¤çŸ¥çš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°LLMæ˜¯å¦èƒ½æ•æ‰å’Œåº”ç”¨ä¸ªæ€§åŒ–æ¨ç†é£æ ¼äºSDGä¸­ã€‚InMindé€šè¿‡è§‚å¯Ÿè€…æ¨¡å¼å’Œå‚ä¸è€…æ¨¡å¼ä¸‹çš„å›åˆç­–ç•¥è½¨è¿¹å’Œæ¸¸æˆååæ€ï¼Œå¢å¼ºäº†ç»“æ„åŒ–æ¸¸æˆç©æ³•æ•°æ®ã€‚å®ƒæ”¯æŒå››é¡¹è®¤çŸ¥é©±åŠ¨çš„ä»»åŠ¡ï¼Œå…±åŒè¯„ä¼°é™æ€å¯¹é½å’ŒåŠ¨æ€é€‚åº”ã€‚ä½œä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬å°†InMindåº”ç”¨äºAvalonæ¸¸æˆï¼Œè¯„ä¼°äº†1. è¯„ä¼°äº†æœ€æ–°é¢–çš„11ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚é€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”šè‡³GPT-4ä¹Ÿç»å¸¸ä¾èµ–äºè¯æ±‡çº¿ç´¢ï¼Œéš¾ä»¥å°†åæ€é”šå®šåœ¨æš‚æ—¶çš„æ¸¸æˆä¸­æˆ–é€‚åº”ä¸æ–­å˜åŒ–çš„ç­–ç•¥ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåƒDeepSeek-R1è¿™æ ·çš„å¢å¼ºæ¨ç†çš„å¤§å‹è¯­è¨€æ¨¡å‹åˆ™æ˜¾ç¤ºå‡ºæ—©æœŸé£æ ¼æ•æ„Ÿæ¨ç†çš„è¿¹è±¡ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸ªæ€§åŒ–é€‚åº”æ€§æ¨ç†æ–¹é¢çš„å…³é”®å±€é™æ€§ï¼Œå¹¶å°†InMindå®šä½ä¸ºæœç€è®¤çŸ¥å¯¹é½çš„äººæœºäº¤äº’è¿ˆå‡ºçš„ä¸€æ­¥ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16072v2">PDF</a> EMNLP 2025 MainConference</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººç±»ä¸­å¿ƒæ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†åœ¨å¤„ç†ç¤¾ä¼šæ¨ç†ä¸­çš„ä¸ªæ€§åŒ–æ¨ç†é£æ ¼æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚ä¸ºäº†è¯„ä¼°LLMsæ˜¯å¦èƒ½å¤Ÿåœ¨ç¤¾ä¼šæ¨ç†æ¸¸æˆä¸­æ•æ‰å’Œåº”ç”¨ä¸ªæ€§åŒ–æ¨ç†é£æ ¼ï¼Œæˆ‘ä»¬å¼•å…¥äº†InMindè¯„ä¼°æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç»“åˆäº†ç»“æ„åŒ–æ¸¸æˆæ•°æ®ã€å›åˆçº§ç­–ç•¥ç—•è¿¹å’Œæ¸¸æˆååæ€ï¼Œæ”¯æŒå››ç§è®¤çŸ¥é©±åŠ¨çš„ä»»åŠ¡ï¼Œä»¥è¯„ä¼°é™æ€å¯¹é½å’ŒåŠ¨æ€é€‚åº”ã€‚ä»¥Avalonæ¸¸æˆä¸ºä¾‹ï¼Œå‘ç°é€šç”¨LLMsï¼ˆå¦‚GPT-4oï¼‰å¸¸ä¾èµ–è¯æ±‡çº¿ç´¢ï¼Œéš¾ä»¥åœ¨åŠ¨æ€æ¸¸æˆä¸­è¿›è¡Œåæ€å’Œé€‚åº”å˜åŒ–ç­–ç•¥ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…·å¤‡æ¨ç†èƒ½åŠ›çš„LLMsï¼ˆå¦‚DeepSeek-R1ï¼‰å±•ç°å‡ºæ—©æœŸä¸ªæ€§åŒ–æ¨ç†èƒ½åŠ›ã€‚è¿™æ­ç¤ºäº†å½“å‰LLMsåœ¨ä¸ªæ€§åŒ–é€‚åº”æ€§æ¨ç†æ–¹é¢çš„å…³é”®å±€é™ï¼Œè€ŒInMindä¸ºè®¤çŸ¥å¯¹é½çš„äººæœºäº¤äº’æä¾›äº†æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMsåœ¨äººç±»ä¸­å¿ƒæ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†ä¸ªæ€§åŒ–æ¨ç†é£æ ¼æ–¹é¢å­˜åœ¨å±€é™ã€‚</li>
<li>InMindè¯„ä¼°æ¡†æ¶ç”¨äºè¯„ä¼°LLMsåœ¨ç¤¾äº¤æ¨ç†æ¸¸æˆä¸­æ•æ‰å’Œåº”ç”¨ä¸ªæ€§åŒ–æ¨ç†é£æ ¼çš„èƒ½åŠ›ã€‚</li>
<li>InMindç»“åˆäº†ç»“æ„åŒ–æ¸¸æˆæ•°æ®ã€å›åˆçº§ç­–ç•¥ç—•è¿¹å’Œæ¸¸æˆååæ€ã€‚</li>
<li>InMindæ”¯æŒå››ç§è®¤çŸ¥é©±åŠ¨çš„ä»»åŠ¡ï¼Œä»¥è¯„ä¼°LLMsçš„é™æ€å¯¹é½å’ŒåŠ¨æ€é€‚åº”èƒ½åŠ›ã€‚</li>
<li>é€šç”¨LLMsï¼ˆå¦‚GPT-4oï¼‰ä¾èµ–è¯æ±‡çº¿ç´¢ï¼Œéš¾ä»¥åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œåæ€å’Œé€‚åº”å˜åŒ–ç­–ç•¥ã€‚</li>
<li>å…·å¤‡æ¨ç†èƒ½åŠ›çš„LLMsï¼ˆå¦‚DeepSeek-R1ï¼‰å±•ç°å‡ºä¸ªæ€§åŒ–æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16072">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-20a45d60dfb452e91ca747558bb87765" align="middle">
<img src="https://picx.zhimg.com/v2-3bfb79ae1257ecca0d70f017eaeef2ba" align="middle">
<img src="https://picx.zhimg.com/v2-edd427dbc884f46d88431295af2b5844" align="middle">
<img src="https://picx.zhimg.com/v2-b70a4abb5a598a2a27a233ac658fe3cf" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Select-to-Know-An-Internal-External-Knowledge-Self-Selection-Framework-for-Domain-Specific-Question-Answering"><a href="#Select-to-Know-An-Internal-External-Knowledge-Self-Selection-Framework-for-Domain-Specific-Question-Answering" class="headerlink" title="Select to Know: An Internal-External Knowledge Self-Selection Framework   for Domain-Specific Question Answering"></a>Select to Know: An Internal-External Knowledge Self-Selection Framework   for Domain-Specific Question Answering</h2><p><strong>Authors:Bolei He, Xinran He, Run Shao, Shanfu Shu, Xianwei Xue, Mingquan Cheng, Haifeng Li, Zhenhua Ling</strong></p>
<p>Large Language Models (LLMs) perform well in general QA but often struggle in domain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces external knowledge but suffers from hallucinations and latency due to noisy retrievals. Continued pretraining internalizes domain knowledge but is costly and lacks cross-domain flexibility. We attribute this challenge to the long-tail distribution of domain knowledge, which leaves partial yet useful internal knowledge underutilized. We further argue that knowledge acquisition should be progressive, mirroring human learning: first understanding concepts, then applying them to complex reasoning. To address this, we propose Selct2Know (S2K), a cost-effective framework that internalizes domain knowledge through an internal-external knowledge self-selection strategy and selective supervised fine-tuning. We also introduce a structured reasoning data generation pipeline and integrate GRPO to enhance reasoning ability. Experiments on medical, legal, and financial QA benchmarks show that S2K consistently outperforms existing methods and matches domain-pretrained LLMs with significantly lower cost. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸€èˆ¬é—®ç­”ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸçš„åœºæ™¯ä¸­å¸¸å¸¸é‡åˆ°å›°éš¾ã€‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å¼•å…¥äº†å¤–éƒ¨çŸ¥è¯†ï¼Œä½†ç”±äºæ£€ç´¢ç»“æœä¸­çš„å™ªå£°è€Œäº§ç”Ÿäº†è™šæ„å’Œå»¶è¿Ÿé—®é¢˜ã€‚æŒç»­é¢„è®­ç»ƒå¯ä»¥å†…åŒ–é¢†åŸŸçŸ¥è¯†ï¼Œä½†æˆæœ¬é«˜æ˜‚ä¸”ç¼ºä¹è·¨é¢†åŸŸçµæ´»æ€§ã€‚æˆ‘ä»¬å°†è¿™ä¸€æŒ‘æˆ˜å½’å› äºé¢†åŸŸçŸ¥è¯†å­˜åœ¨çš„é•¿å°¾åˆ†å¸ƒç°è±¡ï¼Œå¯¼è‡´éƒ¨åˆ†æœ‰ç”¨çš„å†…éƒ¨çŸ¥è¯†æœªè¢«å……åˆ†åˆ©ç”¨ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è®¤ä¸ºï¼ŒçŸ¥è¯†è·å–åº”è¯¥æ˜¯ä¸€ä¸ªå¾ªåºæ¸è¿›çš„è¿‡ç¨‹ï¼Œæ¨¡ä»¿äººç±»å­¦ä¹ è¿‡ç¨‹ï¼šé¦–å…ˆç†è§£æ¦‚å¿µï¼Œç„¶åå°†å…¶åº”ç”¨äºå¤æ‚æ¨ç†ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†Selct2Knowï¼ˆS2Kï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å†…å¤–çŸ¥è¯†è‡ªé€‰ç­–ç•¥å’Œé€‰æ‹©æ€§ç›‘ç£å¾®è°ƒæ¥å†…åŒ–é¢†åŸŸçŸ¥è¯†çš„æˆæœ¬æ•ˆç›Šé«˜çš„æ¡†æ¶ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªç»“æ„åŒ–æ¨ç†æ•°æ®ç”Ÿæˆç®¡é“ï¼Œå¹¶æ•´åˆäº†GRPOä»¥å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚åœ¨åŒ»ç–—ã€æ³•å¾‹å’Œè´¢åŠ¡é—®ç­”åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒS2Kå§‹ç»ˆä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶ä»¥æ›´ä½çš„æˆæœ¬åŒ¹é…é¢†åŸŸé¢„è®­ç»ƒçš„LLMã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15213v2">PDF</a> EMNLP2025 Findings</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é€šç”¨é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨ç‰¹å®šé¢†åŸŸåœºæ™¯ä¸­å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†Selct2Knowï¼ˆS2Kï¼‰æ¡†æ¶ï¼Œé€šè¿‡å†…å¤–çŸ¥è¯†è‡ªæˆ‘é€‰æ‹©ç­–ç•¥å’Œé€‰æ‹©æ€§ç›‘ç£å¾®è°ƒæ¥å†…åŒ–é¢†åŸŸçŸ¥è¯†ï¼Œå¹¶å¼•å…¥ç»“æ„åŒ–æ¨ç†æ•°æ®ç”Ÿæˆç®¡é“å’ŒGRPOå¢å¼ºæ¨ç†èƒ½åŠ›ã€‚å®éªŒè¡¨æ˜ï¼ŒS2Kåœ¨åŒ»ç–—ã€æ³•å¾‹å’Œè´¢åŠ¡é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰æˆæœ¬æ•ˆç›Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç‰¹å®šé¢†åŸŸé—®ç­”ä»»åŠ¡ä¸­å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>Retrieval-Augmented Generationï¼ˆRAGï¼‰æ–¹æ³•å¼•å…¥å¤–éƒ¨çŸ¥è¯†ï¼Œä½†å­˜åœ¨å¹»è§‰å’Œå»¶è¿Ÿé—®é¢˜ã€‚</li>
<li>å•çº¯ç»§ç»­é¢„è®­ç»ƒå†…åŒ–é¢†åŸŸçŸ¥è¯†æˆæœ¬é«˜æ˜‚ï¼Œä¸”ç¼ºä¹è·¨é¢†åŸŸçµæ´»æ€§ã€‚</li>
<li>çŸ¥è¯†è·å–åº”è¯¥åƒäººç±»å­¦ä¹ ä¸€æ ·ï¼Œå…ˆç†è§£æ¦‚å¿µï¼Œå†åº”ç”¨äºå¤æ‚æ¨ç†ã€‚</li>
<li>Selct2Knowï¼ˆS2Kï¼‰æ¡†æ¶é€šè¿‡å†…å¤–çŸ¥è¯†è‡ªæˆ‘é€‰æ‹©ç­–ç•¥å’Œé€‰æ‹©æ€§ç›‘ç£å¾®è°ƒæ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚</li>
<li>S2Kæ¡†æ¶å¼•å…¥ç»“æ„åŒ–æ¨ç†æ•°æ®ç”Ÿæˆç®¡é“å’ŒGRPOå¢å¼ºæŠ€æœ¯æ¥æå‡æ¨ç†èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15213">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-31454a8946b40eca9385c80afe3424fc" align="middle">
<img src="https://picx.zhimg.com/v2-6cb895834f25af6d89f3227364dc23bc" align="middle">
<img src="https://picx.zhimg.com/v2-cf001ab9574483800525736c4f852029" align="middle">
<img src="https://picx.zhimg.com/v2-b6a1cfb3590d916715b828fc3b0c0687" align="middle">
<img src="https://picx.zhimg.com/v2-39b7f01bc57295ca1dc60650869e77a5" align="middle">
<img src="https://picx.zhimg.com/v2-a5172d7a86d8c59b26a102efdc4fb6d0" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space"><a href="#MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space" class="headerlink" title="MolmoAct: Action Reasoning Models that can Reason in Space"></a>MolmoAct: Action Reasoning Models that can Reason in Space</h2><p><strong>Authors:Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna</strong></p>
<p>Reasoning is central to purposeful action, yet most robotic foundation models map perception and instructions directly to control, which limits adaptability, generalization, and semantic grounding. We introduce Action Reasoning Models (ARMs), a class of robotic foundation models that integrate perception, planning, and control through a structured three-stage pipeline. Our model, MolmoAct, encodes observations and instructions into depth-aware perception tokens, generates mid-level spatial plans as editable trajectory traces, and predicts precise low-level actions, enabling explainable and steerable behavior. MolmoAct-7B-D achieves strong performance across simulation and real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching tasks, surpassing closed-source Pi-0 and GR00T N1.5; 86.6% average success on LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks; and in real-world fine-tuning, an additional 10% (single-arm) and an additional 22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines by an additional 23.3% on out-of-distribution generalization and achieves top human-preference scores for open-ended instruction following and trajectory steering. Furthermore, we release, for the first time, the MolmoAct Dataset â€“ a mid-training robot dataset comprising over 10,000 high quality robot trajectories across diverse scenarios and tasks. Training with this dataset yields an average 5.5% improvement in general performance over the base model. We release all model weights, training code, our collected dataset, and our action reasoning dataset, establishing MolmoAct as both a state-of-the-art robotics foundation model and an open blueprint for building ARMs that transform perception into purposeful action through structured reasoning. Blogpost: <a target="_blank" rel="noopener" href="https://allenai.org/blog/molmoact">https://allenai.org/blog/molmoact</a> </p>
<blockquote>
<p>æ¨ç†æ˜¯ç›®æ ‡è¡ŒåŠ¨çš„æ ¸å¿ƒï¼Œç„¶è€Œå¤§å¤šæ•°æœºå™¨äººåŸºç¡€æ¨¡å‹ç›´æ¥å°†æ„ŸçŸ¥å’ŒæŒ‡ä»¤æ˜ å°„åˆ°æ§åˆ¶ä¸Šï¼Œè¿™é™åˆ¶äº†å…¶é€‚åº”æ€§ã€é€šç”¨æ€§å’Œè¯­ä¹‰åŸºç¡€ã€‚æˆ‘ä»¬å¼•å…¥äº†è¡ŒåŠ¨æ¨ç†æ¨¡å‹ï¼ˆARMsï¼‰ï¼Œè¿™æ˜¯ä¸€ç±»æœºå™¨äººåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ç»“æ„åŒ–çš„ä¸‰é˜¶æ®µç®¡é“å°†æ„ŸçŸ¥ã€è§„åˆ’å’Œæ§åˆ¶ç»“åˆåœ¨ä¸€èµ·ã€‚æˆ‘ä»¬çš„æ¨¡å‹MolmoActå°†è§‚å¯Ÿå’ŒæŒ‡ä»¤ç¼–ç ä¸ºæ·±åº¦æ„ŸçŸ¥æ ‡è®°ï¼Œç”Ÿæˆå¯ç¼–è¾‘çš„è½¨è¿¹è½¨è¿¹ä½œä¸ºä¸­çº§ç©ºé—´è®¡åˆ’ï¼Œå¹¶é¢„æµ‹ç²¾ç¡®çš„ä½çº§è¡ŒåŠ¨ï¼Œä»è€Œå®ç°å¯è§£é‡Šå’Œå¯å¼•å¯¼çš„è¡Œä¸ºã€‚MolmoAct-7B-Dåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒè®¾ç½®ä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ï¼šåœ¨SimplerEnvè§†è§‰åŒ¹é…ä»»åŠ¡ä¸Šè¾¾åˆ°70.5%çš„é›¶å°„å‡»å‡†ç¡®ç‡ï¼Œè¶…è¶Šå°é—­å¼Pi-0å’ŒGR00TN1.5ï¼›åœ¨LIBEROä¸Šçš„å¹³å‡æˆåŠŸç‡è¾¾åˆ°86.6%ï¼Œå…¶ä¸­åŒ…æ‹¬åœ¨é•¿è¿œä»»åŠ¡ä¸Šç›¸å¯¹äºThinkActçš„é¢å¤–6.3%çš„æ”¶ç›Šï¼›åœ¨çœŸå®ä¸–ç•Œçš„å¾®è°ƒä¸­ï¼Œç›¸å¯¹äºPi-0-FASTï¼Œå•è‡‚ä»»åŠ¡æœ‰é¢å¤–çš„10%æ”¶ç›Šï¼ŒåŒæ‰‹ååŒä»»åŠ¡æœ‰é¢å¤–çš„22.7%æ”¶ç›Šã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ä»¥é¢å¤–çš„23.3%çš„ä¼˜åŠ¿è¶…è¶ŠåŸºçº¿æ¨¡å‹åœ¨è¶…å‡ºåˆ†å¸ƒæ³›åŒ–ä¸Šçš„è¡¨ç°ï¼Œå¹¶åœ¨å¼€æ”¾å¼æŒ‡ä»¤éµå¾ªå’Œè½¨è¿¹å¼•å¯¼æ–¹é¢è¾¾åˆ°äººç±»åå¥½åº¦æœ€é«˜åˆ†æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é¦–æ¬¡å‘å¸ƒMolmoActæ•°æ®é›†â€”â€”è¿™æ˜¯ä¸€ä¸ªä¸­æœŸè®­ç»ƒæœºå™¨äººæ•°æ®é›†ï¼ŒåŒ…å«å„ç§åœºæ™¯å’Œä»»åŠ¡ä¸­çš„è¶…è¿‡1ä¸‡æ¡é«˜è´¨é‡æœºå™¨äººè½¨è¿¹ã€‚ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè®­ç»ƒç›¸å¯¹äºåŸºç¡€æ¨¡å‹åœ¨æ€»ä½“ä¸Šæé«˜äº†å¹³å‡5.5%çš„æ€§èƒ½ã€‚æˆ‘ä»¬å‘å¸ƒæ‰€æœ‰æ¨¡å‹æƒé‡ã€è®­ç»ƒä»£ç ã€æ”¶é›†çš„æ•°æ®é›†ä»¥åŠæˆ‘ä»¬çš„è¡ŒåŠ¨æ¨ç†æ•°æ®é›†ï¼Œç¡®ç«‹äº†MolmoActä½œä¸ºæœ€å…ˆè¿›çš„æœºå™¨äººåŸºç¡€æ¨¡å‹ï¼Œå¹¶ä¸ºæ„å»ºé€šè¿‡ç»“æ„åŒ–æ¨ç†å°†æ„ŸçŸ¥è½¬åŒ–ä¸ºæœ‰ç›®çš„è¡ŒåŠ¨çš„ARMsæä¾›äº†å¼€æ”¾è“å›¾ã€‚åšå®¢æ–‡ç« ï¼š<a target="_blank" rel="noopener" href="https://allenai.org/blog/molmoact">https://allenai.org/blog/molmoact</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07917v4">PDF</a> Updated GR00T result to N1.5</p>
<p><strong>Summary</strong></p>
<p>åŸºäºæ–‡ä¸­æè¿°ï¼Œæœºå™¨äººé¢†åŸŸå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼šå¤§éƒ¨åˆ†æœºå™¨äººåŸºç¡€æ¨¡å‹å°†æ„ŸçŸ¥å’ŒæŒ‡ä»¤ç›´æ¥æ˜ å°„åˆ°æ§åˆ¶ä¸Šï¼Œé™åˆ¶äº†é€‚åº”æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œè¯­ä¹‰æ¥åœ°èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æœºå™¨äººåŸºç¡€æ¨¡å‹â€”â€”è¡ŒåŠ¨æ¨ç†æ¨¡å‹ï¼ˆARMsï¼‰ï¼Œå…¶ä¸­MolmoActæ¨¡å‹å…·æœ‰æ·±åº¦æ„ŸçŸ¥æ ‡è®°ã€å¯ç¼–è¾‘è½¨è¿¹è·Ÿè¸ªä»¥åŠç²¾ç¡®ä½çº§åŠ¨ä½œç”Ÿæˆç­‰ç‰¹æ€§ã€‚MolmoActæ¨¡å‹èƒ½å¤Ÿåœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­å®ç°å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶ä¸”å…·æœ‰å¯è§£é‡Šæ€§å’Œå¯å¼•å¯¼æ€§è¡Œä¸ºã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜é¦–æ¬¡å‘å¸ƒäº†MolmoActæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡ä¸€ä¸‡æ¡é«˜è´¨é‡çš„æœºå™¨äººè½¨è¿¹æ•°æ®ï¼Œç”¨äºè®­ç»ƒæœºå™¨äººæ¨¡å‹ã€‚æ€»ä½“æ¥è¯´ï¼ŒMolmoActæ˜¯ä¸€ä¸ªå…ˆè¿›çš„æœºå™¨äººåŸºç¡€æ¨¡å‹ï¼Œé€šè¿‡ç»“æ„åŒ–æ¨ç†å°†æ„ŸçŸ¥è½¬åŒ–ä¸ºæœ‰ç›®çš„æ€§çš„è¡ŒåŠ¨ã€‚æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…<a target="_blank" rel="noopener" href="https://allenai.org/blog/molmoact%E3%80%82">https://allenai.org/blog/molmoactã€‚</a></p>
<p><strong>Key Takeaways</strong></p>
<p>ä»¥ä¸‹æ˜¯å…³é”®è§è§£ï¼š</p>
<p>ä¸€ã€è¡ŒåŠ¨æ¨ç†æ¨¡å‹ï¼ˆARMsï¼‰å¼•å…¥æœºå™¨äººåŸºç¡€æ¨¡å‹ä¸­ä»¥è§£å†³ä¼ ç»Ÿæ¨¡å‹çš„é€‚åº”æ€§ã€æ³›åŒ–åŠè¯­ä¹‰æ¥åœ°é—®é¢˜ã€‚<br>äºŒã€MolmoActæ¨¡å‹é€šè¿‡æ·±åº¦æ„ŸçŸ¥æ ‡è®°ã€å¯ç¼–è¾‘è½¨è¿¹è·Ÿè¸ªå’Œç²¾ç¡®ä½çº§åŠ¨ä½œç”Ÿæˆç­‰æŠ€æœ¯å®ç°ä¼˜ç§€çš„æ€§èƒ½ã€‚<br>ä¸‰ã€MolmoActæ¨¡å‹åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½ï¼Œå…·æœ‰å¯è§£é‡Šæ€§å’Œå¯å¼•å¯¼æ€§è¡Œä¸ºã€‚<br>å››ã€é¦–æ¬¡å‘å¸ƒMolmoActæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡ä¸€ä¸‡æ¡é«˜è´¨é‡çš„æœºå™¨äººè½¨è¿¹æ•°æ®ï¼Œç”¨äºè®­ç»ƒæœºå™¨äººæ¨¡å‹ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚<br>äº”ã€MolmoActæ•°æ®é›†ä¸ºæ„å»ºæ›´å…ˆè¿›çš„ARMsæä¾›äº†å¼€æ”¾è“å›¾ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿæ›´å¥½åœ°é€šè¿‡ç»“æ„åŒ–æ¨ç†å°†æ„ŸçŸ¥è½¬åŒ–ä¸ºæœ‰ç›®çš„æ€§çš„è¡ŒåŠ¨ã€‚<br>å…­ã€MolmoActåœ¨å¤šä¸ªä»»åŠ¡ä¸­å®ç°äº†çªç ´æ€§çš„æ€§èƒ½ï¼Œå¦‚SimplerEnvè§†è§‰åŒ¹é…ä»»åŠ¡ã€LIBEROä»»åŠ¡ç­‰ã€‚ç›¸è¾ƒäºå…¶ä»–åŸºå‡†æµ‹è¯•æ¨¡å‹ï¼Œå…¶è¡¨ç°å°¤ä¸ºå‡ºè‰²ã€‚</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07917">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ac2c4e6f665e2a2daa1dedf3f2b512be" align="middle">
<img src="https://picx.zhimg.com/v2-4b817ddc0262dd48b7d93834a9892318" align="middle">
<img src="https://picx.zhimg.com/v2-992deca53e1e8e5e03a275abe6428fc3" align="middle">
<img src="https://picx.zhimg.com/v2-ada5bd869f6f384902e76b0deefe5c5c" align="middle">
<img src="https://picx.zhimg.com/v2-a1c1ce82f7dc1411df8fcfe9b6d7e22b" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UR-2-Unify-RAG-and-Reasoning-through-Reinforcement-Learning"><a href="#UR-2-Unify-RAG-and-Reasoning-through-Reinforcement-Learning" class="headerlink" title="UR$^2$: Unify RAG and Reasoning through Reinforcement Learning"></a>UR$^2$: Unify RAG and Reasoning through Reinforcement Learning</h2><p><strong>Authors:Weitao Li, Boran Xiang, Xiaolong Wang, Zhinan Gou, Weizhi Ma, Yang Liu</strong></p>
<p>Large Language Models (LLMs) have shown remarkable capabilities through two complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR), which optimizes complex reasoning abilities. However, these two capabilities are often developed in isolation, and existing efforts to unify them remain narrow in scope â€“ typically limited to open-domain QA with fixed retrieval settings and task-specific constraints. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a general framework that unifies retrieval and reasoning through reinforcement learning. UR2 introduces two key contributions: a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries. These components are designed to enable dynamic coordination between retrieval and reasoning, improving adaptability across a diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical, and mathematical reasoning tasks demonstrate that UR$^2$ (built on Qwen-2.5-3&#x2F;7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks. We have released all code, models, and data at <a target="_blank" rel="noopener" href="https://github.com/Tsinghua-dhy/UR2">https://github.com/Tsinghua-dhy/UR2</a>. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä¸¤ç§äº’è¡¥çš„æ¨¡å¼å±•ç°äº†æ˜¾è‘—çš„èƒ½åŠ›ï¼šå¢å¼ºçŸ¥è¯†åŸºç¡€çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¼˜åŒ–å¤æ‚æ¨ç†èƒ½åŠ›çš„æ¥è‡ªå¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§èƒ½åŠ›å¾€å¾€ç‹¬ç«‹å‘å±•ï¼Œç°æœ‰çš„ç»Ÿä¸€å®ƒä»¬çš„å°è¯•ä»ç„¶å±€é™äºå¼€æ”¾åŸŸé—®ç­”çš„å›ºå®šæ£€ç´¢è®¾ç½®å’Œä»»åŠ¡ç‰¹å®šçº¦æŸã€‚è¿™ç§ç¼ºä¹æ•´åˆé™åˆ¶äº†RAG-RLæ–¹æ³•åœ¨æ›´å¹¿æ³›é¢†åŸŸçš„åº”ç”¨ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†URÂ²ï¼ˆç»Ÿä¸€RAGå’Œæ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ç»Ÿä¸€æ£€ç´¢å’Œæ¨ç†çš„ä¸€èˆ¬æ¡†æ¶ã€‚URÂ²æœ‰ä¸¤ä¸ªå…³é”®è´¡çŒ®ï¼šä¸€ä¸ªéš¾åº¦æ„ŸçŸ¥çš„è¯¾ç¨‹è®­ç»ƒï¼Œæœ‰é€‰æ‹©åœ°ä¸ºå…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜è°ƒç”¨æ£€ç´¢åŠŸèƒ½ï¼›ä»¥åŠä¸€ç§æ··åˆçŸ¥è¯†è®¿é—®ç­–ç•¥ï¼Œç»“åˆé¢†åŸŸç‰¹å®šçš„ç¦»çº¿è¯­æ–™åº“å’ŒLLMç”Ÿæˆçš„æ‘˜è¦ã€‚è¿™äº›ç»„ä»¶çš„è®¾è®¡æ—¨åœ¨å®ç°æ£€ç´¢å’Œæ¨ç†ä¹‹é—´çš„åŠ¨æ€åè°ƒï¼Œæé«˜åœ¨å„ç§ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚åœ¨å¼€æ”¾åŸŸé—®ç­”ã€MMLU-Proã€åŒ»ç–—å’Œæ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒURÂ²ï¼ˆåŸºäºQwen-2.5-3&#x2F;7Bå’ŒLLaMA-3.1-8Bï¼‰æ˜¾è‘—ä¼˜äºç°æœ‰çš„RAGå’ŒRLæ–¹æ³•ï¼Œå¹¶åœ¨ä¸€äº›åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†ä¸GPT-4o-miniå’ŒGPT-4.1-miniç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬å·²åœ¨<a target="_blank" rel="noopener" href="https://github.com/Tsinghua-dhy/UR2%E5%8F%91%E5%B8%83%E6%89%80%E6%9C%89%E4%BB%A3%E7%A0%81%E3%80%81%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E7%9A%84%E3%80%82">https://github.com/Tsinghua-dhy/UR2å‘å¸ƒæ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®çš„ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06165v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡ä¸¤ç§äº’è¡¥èŒƒå¼å±•ç°äº†æ˜¾è‘—çš„èƒ½åŠ›ï¼šå¢å¼ºçŸ¥è¯†å®šä½çš„å›æº¯å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œä¼˜åŒ–å¤æ‚æ¨ç†èƒ½åŠ›çš„å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§èƒ½åŠ›é€šå¸¸å­¤ç«‹å‘å±•ï¼Œç°æœ‰çš„ç»Ÿä¸€å®ƒä»¬çš„å·¥ä½œèŒƒå›´ç‹­çª„ï¼Œä»…é™äºå¼€æ”¾åŸŸé—®ç­”å’Œå›ºå®šæ£€ç´¢è®¾ç½®çš„ç‰¹å®šä»»åŠ¡çº¦æŸã€‚ä¸ºå¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºURÂ²ï¼ˆç»Ÿä¸€RAGå’Œæ¨ç†ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ç»Ÿä¸€æ£€ç´¢å’Œæ¨ç†çš„é€šç”¨æ¡†æ¶ã€‚URÂ²å¼•å…¥äº†ä¸¤ä¸ªå…³é”®è´¡çŒ®ï¼šéš¾åº¦æ„ŸçŸ¥è¯¾ç¨‹è®­ç»ƒï¼Œé€‰æ‹©æ€§åœ°å¯¹æŒ‘æˆ˜æ€§é—®é¢˜ä»…ä½¿ç”¨æ£€ç´¢åŠŸèƒ½ï¼›æ··åˆçŸ¥è¯†è®¿é—®ç­–ç•¥ï¼Œç»“åˆé¢†åŸŸç‰¹å®šçš„ç¦»çº¿è¯­æ–™åº“ä¸LLMç”Ÿæˆçš„æ‘˜è¦ã€‚è¿™äº›ç»„ä»¶æ—¨åœ¨å®ç°æ£€ç´¢å’Œæ¨ç†ä¹‹é—´çš„åŠ¨æ€åè°ƒï¼Œæé«˜åœ¨å„ç§ä»»åŠ¡ä¸­çš„é€‚åº”æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒURÂ²ï¼ˆåŸºäºQwen-2.5-3&#x2F;7Bå’ŒLLaMA-3.1-8Bï¼‰æ˜¾è‘—ä¼˜äºç°æœ‰çš„RAGå’ŒRLæ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ä¸GPT-4o-miniå’ŒGPT-4.1-miniç›¸å½“çš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šè¿‡å›æº¯å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰å±•ç°å‡ºå¼ºå¤§çš„èƒ½åŠ›ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¾€å¾€å­¤ç«‹å‘å±•è¿™ä¸¤ç§èƒ½åŠ›ï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§å’Œåº”ç”¨èŒƒå›´ã€‚</li>
<li>URÂ²æ¡†æ¶æ—¨åœ¨é€šè¿‡å¼ºåŒ–å­¦ä¹ ç»Ÿä¸€æ£€ç´¢å’Œæ¨ç†ï¼Œæé«˜æ¨¡å‹çš„é€‚åº”æ€§ã€‚</li>
<li>URÂ²å¼•å…¥éš¾åº¦æ„ŸçŸ¥è¯¾ç¨‹è®­ç»ƒå’Œæ··åˆçŸ¥è¯†è®¿é—®ç­–ç•¥ï¼Œå®ç°æ£€ç´¢å’Œæ¨ç†ä¹‹é—´çš„åŠ¨æ€åè°ƒã€‚</li>
<li>URÂ²æ˜¾è‘—ä¼˜äºç°æœ‰çš„RAGå’ŒRLæ–¹æ³•ï¼Œå¹¶åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå®ç°äº†ä¸é«˜çº§æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚</li>
<li>URÂ²æ¡†æ¶å·²å‘å¸ƒæ‰€æœ‰ä»£ç ã€æ¨¡å‹å’Œæ•°æ®çš„å¼€æºåœ°å€ã€‚</li>
<li>URÂ²æ¡†æ¶æœ‰æœ›ä¿ƒè¿›å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å„ä¸ªé¢†åŸŸçš„åº”ç”¨å’Œå‘å±•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06165">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e6f66eba6bcabf227cfce6d966de9265" align="middle">
<img src="https://picx.zhimg.com/v2-a9c56ada69304e305c80e7c4a987fa1c" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Uni-cot-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision"><a href="#Uni-cot-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision" class="headerlink" title="Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and   Vision"></a>Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and   Vision</h2><p><strong>Authors:Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, Hao Li</strong></p>
<p>Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large Language Models (LLMs) by decomposing complex tasks into simpler, sequential subtasks. However, extending CoT to vision-language reasoning tasks remains challenging, as it often requires interpreting transitions of visual states to support reasoning. Existing methods often struggle with this due to limited capacity of modeling visual state transitions or incoherent visual trajectories caused by fragmented architectures.   To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought framework that enables coherent and grounded multimodal reasoning within a single unified model. The key idea is to leverage a model capable of both image understanding and generation to reason over visual content and model evolving visual states. However, empowering a unified model to achieve that is non-trivial, given the high computational cost and the burden of training. To address this, Uni-CoT introduces a novel two-level reasoning paradigm: A Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask execution. This design significantly reduces the computational overhead. Furthermore, we introduce a structured training paradigm that combines interleaved image-text supervision for macro-level CoT with multi-task objectives for micro-level CoT. Together, these innovations allow Uni-CoT to perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our design, all experiments can be efficiently completed using only 8 A100 GPUs with 80GB VRAM each. Experimental results on reasoning-driven image generation benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT demonstrates SOTA performance and strong generalization, establishing Uni-CoT as a promising solution for multi-modal reasoning. Project Page and Code: <a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/">https://sais-fuxi.github.io/projects/uni-cot/</a> </p>
<blockquote>
<p>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†è¢«å¹¿æ³›åº”ç”¨äºé€šè¿‡åˆ†è§£å¤æ‚ä»»åŠ¡ä¸ºæ›´ç®€å•ã€è¿ç»­çš„å­ä»»åŠ¡æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°†CoTæ‰©å±•åˆ°è§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå®ƒé€šå¸¸éœ€è¦è§£é‡Šè§†è§‰çŠ¶æ€çš„è¿‡æ¸¡ä»¥æ”¯æŒæ¨ç†ã€‚ç°æœ‰æ–¹æ³•å¸¸å¸¸åœ¨è¿™æ–¹é¢é‡åˆ°å›°éš¾ï¼Œå› ä¸ºå®ƒä»¬å»ºæ¨¡è§†è§‰çŠ¶æ€è¿‡æ¸¡çš„èƒ½åŠ›æœ‰é™ï¼Œæˆ–è€…ç”±äºæ¶æ„ç¢ç‰‡åŒ–è€Œå¯¼è‡´è§†è§‰è½¨è¿¹ä¸ä¸€è‡´ã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†Uni-CoTï¼Œä¸€ä¸ªç»Ÿä¸€çš„é“¾å¼æ€ç»´æ¡†æ¶ï¼Œå®ƒèƒ½åœ¨å•ä¸ªç»Ÿä¸€æ¨¡å‹å†…è¿›è¡Œè¿è´¯ä¸”åŸºäºåœ°é¢çš„å¤šæ¨¡æ€æ¨ç†ã€‚ä¸»è¦æ€æƒ³æ˜¯åˆ©ç”¨èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆå›¾åƒçš„æ¨¡å‹æ¥æ¨ç†è§†è§‰å†…å®¹å¹¶æ¨¡æ‹Ÿä¸æ–­æ¼”å˜çš„è§†è§‰çŠ¶æ€ã€‚ç„¶è€Œï¼Œåœ¨ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ä¸­å®ç°è¿™ä¸€ç‚¹å¹¶ä¸ç®€å•ï¼Œå› ä¸ºè¿™ä¼šå¸¦æ¥å¾ˆé«˜çš„è®¡ç®—æˆæœ¬ä»¥åŠè®­ç»ƒè´Ÿæ‹…ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒUni-CoTå¼•å…¥äº†ä¸€ç§æ–°å‹çš„ä¸¤çº§æ¨ç†æ¨¡å¼ï¼šç”¨äºé«˜çº§ä»»åŠ¡è§„åˆ’çš„å®è§‚å±‚é¢CoTå’Œç”¨äºå­ä»»åŠ¡æ‰§è¡Œçš„å¾®è§‚å±‚é¢CoTã€‚è¿™ç§è®¾è®¡æ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç»“æ„åŒ–çš„è®­ç»ƒæ¨¡å¼ï¼Œè¯¥æ¨¡å¼ç»“åˆäº†å®è§‚å±‚é¢CoTçš„äº¤é”™å›¾åƒæ–‡æœ¬ç›‘ç£ä»¥åŠå¾®è§‚å±‚é¢CoTçš„å¤šä»»åŠ¡ç›®æ ‡ã€‚è¿™äº›åˆ›æ–°ä½¿Uni-CoTèƒ½å¤Ÿè¿›è¡Œå¯æ‰©å±•ä¸”è¿è´¯çš„å¤šæ¨¡æ€æ¨ç†ã€‚æ­¤å¤–ï¼Œå¾—ç›Šäºæˆ‘ä»¬çš„è®¾è®¡ï¼Œæ‰€æœ‰å®éªŒéƒ½åªéœ€ä½¿ç”¨8ä¸ªå¸¦æœ‰80GB VRAMçš„A100 GPUå³å¯é«˜æ•ˆå®Œæˆã€‚åœ¨åŸºäºæ¨ç†çš„å›¾åƒç”ŸæˆåŸºå‡†æµ‹è¯•ï¼ˆWISEï¼‰å’Œç¼–è¾‘åŸºå‡†æµ‹è¯•ï¼ˆRISEå’ŒKRISï¼‰ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒUni-CoTå±•ç°å‡ºå“è¶Šçš„æ€§èƒ½å’Œå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆä¸ºå¤šæ¨¡æ€æ¨ç†é¢†åŸŸçš„ä¸€ä¸ªæœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆã€‚é¡¹ç›®é¡µé¢å’Œä»£ç ï¼š<a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/">https://sais-fuxi.github.io/projects/uni-cot/</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05606v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/">https://sais-fuxi.github.io/projects/uni-cot/</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Chain-of-Thoughtï¼ˆCoTï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„å¹¿æ³›åº”ç”¨ï¼Œå¹¶æŒ‡å‡ºå…¶åœ¨è§†è§‰è¯­è¨€æ¨ç†ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ã€‚ä¸ºå…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºUni-CoTçš„ç»Ÿä¸€æ€ç»´é“¾æ¡†æ¶ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€æ¨¡å‹ä¸­å®ç°è¿è´¯çš„å¤šæ¨¡æ€æ¨ç†ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨æ—¢èƒ½ç†è§£å›¾åƒåˆèƒ½ç”Ÿæˆå›¾åƒçš„æ¨¡å‹ï¼Œå¯¹è§†è§‰å†…å®¹å’Œä¸æ–­å˜åŒ–çš„è§†è§‰çŠ¶æ€è¿›è¡Œæ¨ç†ã€‚ä¸ºé™ä½è®¡ç®—æˆæœ¬å’Œè®­ç»ƒè´Ÿæ‹…ï¼ŒUni-CoTå¼•å…¥äº†ä¸¤çº§æ¨ç†æ¨¡å¼ï¼Œå³ç”¨äºé«˜çº§ä»»åŠ¡è§„åˆ’çš„å®è§‚å±‚æ¬¡CoTå’Œç”¨äºå­ä»»åŠ¡æ‰§è¡Œçš„å¾®è§‚å±‚æ¬¡CoTã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§ç»“æ„åŒ–çš„è®­ç»ƒæ¨¡å¼ï¼Œå°†å®è§‚å±‚æ¬¡çš„CoTä¸å¾®è§‚å±‚æ¬¡çš„CoTå¤šä»»åŠ¡ç›®æ ‡ç›¸ç»“åˆã€‚è¿™äº›åˆ›æ–°ä½¿Uni-CoTèƒ½å¤Ÿè¿›è¡Œå¯æ‰©å±•å’Œè¿è´¯çš„å¤šæ¨¡æ€æ¨ç†ï¼Œå¹¶åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Uni-CoTæ¡†æ¶æ˜¯ä¸€ç§å°†è§†è§‰å’Œè¯­è¨€ç»“åˆçš„å¤šæ¨¡æ€æ¨ç†æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€æ¨¡å‹ä¸­å®ç°è¿è´¯çš„æ¨ç†ã€‚</li>
<li>å®ƒåˆ©ç”¨æ—¢èƒ½ç†è§£å›¾åƒåˆèƒ½ç”Ÿæˆå›¾åƒçš„æ¨¡å‹è¿›è¡Œè§†è§‰çŠ¶æ€æ¨ç†ã€‚</li>
<li>Uni-CoTå¼•å…¥äº†ä¸¤çº§æ¨ç†æ¨¡å¼ï¼ŒåŒ…æ‹¬å®è§‚å±‚æ¬¡çš„CoTç”¨äºä»»åŠ¡è§„åˆ’ï¼Œå¾®è§‚å±‚æ¬¡çš„CoTç”¨äºå­ä»»åŠ¡æ‰§è¡Œã€‚</li>
<li>ç»“æ„åŒ–çš„è®­ç»ƒæ¨¡å¼ç»“åˆäº†å®è§‚å’Œå¾®è§‚å±‚æ¬¡çš„CoTç›®æ ‡ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>Uni-CoTå…·æœ‰å¯æ‰©å±•æ€§ï¼Œå¯ä»¥åœ¨æœ‰é™çš„è®¡ç®—èµ„æºï¼ˆä»…ä½¿ç”¨8ä¸ªA100 GPUï¼‰ä¸Šè¿›è¡Œå¤§è§„æ¨¡è¿ç®—ã€‚</li>
<li>åœ¨å›¾åƒç”Ÿæˆå’Œç¼–è¾‘åŸºå‡†æµ‹è¯•ä¸­ï¼ŒUni-CoTè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05606">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-052b6c26cb8503e0ae8dcb5ed3ad2f79" align="middle">
<img src="https://picx.zhimg.com/v2-cb99a2a2fbb8cea80c6e2e1ca239ebd4" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Posterior-GRPO-Rewarding-Reasoning-Processes-in-Code-Generation"><a href="#Posterior-GRPO-Rewarding-Reasoning-Processes-in-Code-Generation" class="headerlink" title="Posterior-GRPO: Rewarding Reasoning Processes in Code Generation"></a>Posterior-GRPO: Rewarding Reasoning Processes in Code Generation</h2><p><strong>Authors:Lishui Fan, Yu Zhang, Mouxiang Chen, Zhongxin Liu</strong></p>
<p>Reinforcement learning (RL) has significantly advanced code generation for large language models (LLMs). However, current paradigms rely on outcome-based rewards from test cases, neglecting the quality of the intermediate reasoning process. While supervising the reasoning process directly is a promising direction, it is highly susceptible to reward hacking, where the policy model learns to exploit the reasoning reward signal without improving final outcomes. To address this, we introduce a unified framework that can effectively incorporate the quality of the reasoning process during RL. First, to enable reasoning evaluation, we develop LCB-RB, a benchmark comprising preference pairs of superior and inferior reasoning processes. Second, to accurately score reasoning quality, we introduce an Optimized-Degraded based (OD-based) method for reward model training. This method generates high-quality preference pairs by systematically optimizing and degrading initial reasoning paths along curated dimensions of reasoning quality, such as factual accuracy, logical rigor, and coherence. A 7B parameter reward model with this method achieves state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success. By selectively applying rewards to the reasoning processes of only successful outcomes, P-GRPO effectively mitigates reward hacking and aligns the modelâ€™s internal reasoning with final code correctness. A 7B parameter model with P-GRPO achieves superior performance across diverse code generation tasks, outperforming outcome-only baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further demonstrate the generalizability of our approach by extending it to mathematical tasks. Our models, dataset, and code are publicly available. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç ç”Ÿæˆæ–¹é¢å–å¾—äº†é‡å¤§è¿›å±•ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ¨¡å¼ä¾èµ–äºæµ‹è¯•ç”¨ä¾‹çš„ç»“æœå¯¼å‘å¥–åŠ±ï¼Œå¿½è§†äº†ä¸­é—´æ¨ç†è¿‡ç¨‹çš„è´¨é‡ã€‚è™½ç„¶ç›´æ¥ç›‘ç£æ¨ç†è¿‡ç¨‹æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ï¼Œä½†å®ƒå¾ˆå®¹æ˜“å—åˆ°å¥–åŠ±ä½œå¼Šçš„å½±å“ï¼Œå³æ”¿ç­–æ¨¡å‹å­¦ä¼šåˆ©ç”¨æ¨ç†å¥–åŠ±ä¿¡å·è€Œä¸æ”¹å–„æœ€ç»ˆç»“æœã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå¯ä»¥æœ‰æ•ˆåœ°åœ¨RLä¸­èå…¥æ¨ç†è¿‡ç¨‹çš„è´¨é‡ã€‚é¦–å…ˆï¼Œä¸ºäº†è¿›è¡Œæ¨ç†è¯„ä¼°ï¼Œæˆ‘ä»¬å¼€å‘äº†LCB-RBåŸºå‡†æµ‹è¯•ï¼Œå®ƒåŒ…æ‹¬ä¼˜è´¨å’ŒåŠ£è´¨æ¨ç†è¿‡ç¨‹çš„åå¥½å¯¹ã€‚å…¶æ¬¡ï¼Œä¸ºäº†å‡†ç¡®è¯„ä¼°æ¨ç†è´¨é‡ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŸºäºä¼˜åŒ–å’Œé™çº§ï¼ˆOD-basedï¼‰çš„æ–¹æ³•æ¥è¿›è¡Œå¥–åŠ±æ¨¡å‹è®­ç»ƒã€‚è¯¥æ–¹æ³•é€šè¿‡ç³»ç»Ÿåœ°ä¼˜åŒ–å’Œé™çº§åˆå§‹æ¨ç†è·¯å¾„ï¼Œæ²¿æ¨ç†è´¨é‡çš„ç²¾é€‰ç»´åº¦ç”Ÿæˆé«˜è´¨é‡åå¥½å¯¹ï¼Œå¦‚äº‹å®å‡†ç¡®æ€§ã€é€»è¾‘ä¸¥è°¨æ€§å’Œè¿è´¯æ€§ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œä¸€ä¸ªæ‹¥æœ‰7Bå‚æ•°çš„å¥–åŠ±æ¨¡å‹åœ¨LCB-RBä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å…¶ä»–åŸºå‡†æµ‹è¯•ä¸­å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†åç½®GRPOï¼ˆP-GRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„RLæ–¹æ³•ï¼ŒåŸºäºä»»åŠ¡æˆåŠŸæ¥è®¾å®šè¿‡ç¨‹å¥–åŠ±ã€‚é€šè¿‡æœ‰é€‰æ‹©åœ°å¯¹æˆåŠŸç»“æœçš„æ¨ç†è¿‡ç¨‹æ–½åŠ å¥–åŠ±ï¼ŒP-GRPOæœ‰æ•ˆåœ°å‡è½»äº†å¥–åŠ±ä½œå¼Šé—®é¢˜ï¼Œå¹¶ä½¿æ¨¡å‹çš„å†…éƒ¨æ¨ç†ä¸æœ€ç»ˆçš„ä»£ç æ­£ç¡®æ€§ç›¸ç¬¦ã€‚ä½¿ç”¨P-GRPOçš„7Bå‚æ•°æ¨¡å‹åœ¨å¤šç§ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºåªå…³æ³¨ç»“æœçš„åŸºçº¿æé«˜äº†4.5%ï¼Œå¹¶å®ç°äº†ä¸GPT-4-Turboç›¸å½“çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜é€šè¿‡å°†å…¶æ‰©å±•åˆ°æ•°å­¦ä»»åŠ¡æ¥å±•ç¤ºäº†æˆ‘ä»¬æ–¹æ³•çš„æ³›åŒ–æ€§ã€‚æˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05170v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­ï¼Œè¿‡ç¨‹å¥–åŠ±å¯¹äºæå‡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä»£ç ç”Ÿæˆèƒ½åŠ›è‡³å…³é‡è¦ã€‚å½“å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºæµ‹è¯•æ¡ˆä¾‹çš„ç»“æœå¥–åŠ±ï¼Œå¿½è§†äº†ä¸­é—´æ¨ç†è¿‡ç¨‹çš„è´¨é‡ã€‚ç›´æ¥ç›‘ç£æ¨ç†è¿‡ç¨‹æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„æ–¹å‘ï¼Œä½†å®ƒå®¹æ˜“å—å¥–åŠ±æœºåˆ¶æ»¥ç”¨çš„å½±å“ï¼Œä½¿å¾—ç­–ç•¥æ¨¡å‹å­¦ä¹ å¦‚ä½•åˆ©ç”¨æ¨ç†å¥–åŠ±ä¿¡å·è€Œä¸æé«˜æœ€ç»ˆç»“æœã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œæ—¨åœ¨æœ‰æ•ˆåœ°ç»“åˆæ¨ç†è¿‡ç¨‹çš„è´¨é‡è¿›è¡ŒRLã€‚é¦–å…ˆï¼Œä¸ºäº†è¿›è¡Œæ¨ç†è¯„ä¼°ï¼Œæˆ‘ä»¬å¼€å‘äº†LCB-RBåŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«ä¼˜è´¨å’ŒåŠ£è´¨æ¨ç†è¿‡ç¨‹çš„åå¥½å¯¹ã€‚å…¶æ¬¡ï¼Œä¸ºäº†å‡†ç¡®è¯„ä¼°æ¨ç†è´¨é‡ï¼Œæˆ‘ä»¬å¼•å…¥åŸºäºä¼˜åŒ–ä¸é™çº§ï¼ˆOD-basedï¼‰çš„æ–¹æ³•è®­ç»ƒå¥–åŠ±æ¨¡å‹ã€‚è¿™ç§æ–¹æ³•é€šè¿‡ç³»ç»Ÿåœ°ä¼˜åŒ–å’Œé™çº§åˆå§‹æ¨ç†è·¯å¾„çš„ç‰¹å®šç»´åº¦ï¼Œå¦‚äº‹å®å‡†ç¡®æ€§ã€é€»è¾‘ä¸¥è°¨æ€§å’Œè¿è´¯æ€§æ¥ç”Ÿæˆé«˜è´¨é‡åå¥½å¯¹ã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥åŸºäºä»»åŠ¡æˆåŠŸç»“æœçš„æ¡ä»¶å¥–åŠ±æ–¹æ³•â€”â€”åç½®GRPOï¼ˆP-GRPOï¼‰ã€‚P-GRPOæœ‰é€‰æ‹©åœ°å°†å¥–åŠ±åº”ç”¨äºæˆåŠŸç»“æœçš„æ¨ç†è¿‡ç¨‹ï¼Œæœ‰æ•ˆå‡è½»äº†å¥–åŠ±æ»¥ç”¨é—®é¢˜å¹¶ä½¿æ¨¡å‹çš„å†…éƒ¨æ¨ç†ä¸æœ€ç»ˆä»£ç çš„æ­£ç¡®æ€§ç›¸ç¬¦ã€‚ä½¿ç”¨P-GRPOçš„7Bå‚æ•°æ¨¡å‹åœ¨å¤šç§ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸è¾ƒäºä»…ä¾èµ–ç»“æœçš„åŸºçº¿æé«˜äº†4.5%ï¼Œæ€§èƒ½ä¸GPT-4-Turboç›¸å½“ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜é€‚ç”¨äºæ•°å­¦ä»»åŠ¡ï¼Œç›¸å…³æ¨¡å‹å’Œä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ åœ¨ä»£ç ç”Ÿæˆä¸­å¼•å…¥äº†è¿‡ç¨‹å¥–åŠ±æ¥æå‡å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>å½“å‰æ–¹æ³•ä¸»è¦å…³æ³¨ç»“æœå¥–åŠ±ï¼Œå¿½è§†äº†æ¨ç†è¿‡ç¨‹çš„è´¨é‡ã€‚</li>
<li>æå‡ºä¸€ä¸ªç»Ÿä¸€æ¡†æ¶æ¥ç»“åˆæ¨ç†è¿‡ç¨‹çš„è´¨é‡ï¼Œå¼€å‘LCB-RBåŸºå‡†æµ‹è¯•å’ŒåŸºäºä¼˜åŒ–ä¸é™çº§çš„å¥–åŠ±æ¨¡å‹è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>å¼•å…¥Posterior-GRPOï¼ˆP-GRPOï¼‰æ–¹æ³•ï¼Œå°†å¥–åŠ±åº”ç”¨äºæˆåŠŸç»“æœçš„æ¨ç†è¿‡ç¨‹ï¼Œå‡è½»å¥–åŠ±æ»¥ç”¨é—®é¢˜ã€‚</li>
<li>ä½¿ç”¨P-GRPOçš„æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ€§èƒ½ä¸GPT-4-Turboç›¸å½“ã€‚</li>
<li>æ–¹æ³•å…·æœ‰é€šç”¨æ€§ï¼Œå¯åº”ç”¨äºæ•°å­¦ä»»åŠ¡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05170">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a6ee3f11cb498ef3ebe097fe377a8b91" align="middle">
<img src="https://picx.zhimg.com/v2-1f19ae98b9b361b5afe8c1bcbb368860" align="middle">
<img src="https://picx.zhimg.com/v2-9fdd2d24804af87ba40f376c96c0e265" align="middle">
<img src="https://picx.zhimg.com/v2-c9d614703286f0875f524cd3dbbdf890" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Mitigating-Attention-Hacking-in-Preference-Based-Reward-Modeling-via-Interaction-Distillation"><a href="#Mitigating-Attention-Hacking-in-Preference-Based-Reward-Modeling-via-Interaction-Distillation" class="headerlink" title="Mitigating Attention Hacking in Preference-Based Reward Modeling via   Interaction Distillation"></a>Mitigating Attention Hacking in Preference-Based Reward Modeling via   Interaction Distillation</h2><p><strong>Authors:Jianxiang Zang, Meiling Ning, Shihan Dou, Jiazheng Zhang, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>The reward model (RM), as the core component of reinforcement learning from human feedback (RLHF) for large language models (LLMs), responsible for providing reward signals to generated responses. However, mainstream preference modeling in RM is inadequate in terms of token-level interaction, making its judgment signals vulnerable to being hacked by misallocated attention to context. This stems from two fundamental limitations: (1) Current preference modeling employs decoder-only architectures, where the unidirectional causal attention mechanism leads to forward-decaying intra-sequence attention within the prompt-response sequence. (2) The independent Siamese-encoding paradigm induces the absence of token-level inter-sequence attention between chosen and rejected sequences. To address this â€œattention hackingâ€, we propose â€œInteraction Distillationâ€, a novel training framework for more adequate preference modeling through attention-level optimization. The method introduces an interaction-based natural language understanding model as the teacher to provide sophisticated token interaction patterns via comprehensive attention, and guides the preference modeling to simulate teacher modelâ€™s interaction pattern through an attentional alignment objective. Through extensive experiments, interaction distillation has demonstrated its ability to provide more stable and generalizable reward signals compared to state-of-the-art RM optimization methods that target data noise, highlighting the attention hacking constitute a more fundamental limitation in RM. </p>
<blockquote>
<p>å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ä¸ºç”Ÿæˆçš„å“åº”æä¾›å¥–åŠ±ä¿¡å·ã€‚ç„¶è€Œï¼ŒRMä¸­çš„ä¸»æµåå¥½å»ºæ¨¡åœ¨ä»¤ç‰Œçº§äº¤äº’æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä½¿å¾—å…¶åˆ¤æ–­ä¿¡å·å®¹æ˜“å—åˆ°å¯¹ä¸Šä¸‹æ–‡é”™é…æ³¨æ„åŠ›çš„æ”»å‡»ã€‚è¿™æºäºä¸¤ä¸ªåŸºæœ¬å±€é™ï¼šï¼ˆ1ï¼‰å½“å‰çš„åå¥½å»ºæ¨¡ä»…é‡‡ç”¨è§£ç å™¨æ¶æ„ï¼Œå…¶ä¸­å•å‘å› æœæ³¨æ„åŠ›æœºåˆ¶å¯¼è‡´æç¤º-å“åº”åºåˆ—å†…çš„åºåˆ—å†…æ³¨æ„åŠ›å‘ˆç°å‰å‘è¡°å‡ã€‚ï¼ˆ2ï¼‰ç‹¬ç«‹çš„Siameseç¼–ç èŒƒå¼å¯¼è‡´æ‰€é€‰åºåˆ—å’Œæ‹’ç»åºåˆ—ä¹‹é—´ç¼ºä¹ä»¤ç‰Œçº§åºåˆ—é—´æ³¨æ„åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ç§â€œæ³¨æ„åŠ›æ”»å‡»â€ï¼Œæˆ‘ä»¬æå‡ºäº†â€œäº¤äº’è’¸é¦â€ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æ³¨æ„åŠ›å±‚é¢ä¼˜åŒ–æ¥è¿›è¡Œæ›´å……åˆ†åå¥½å»ºæ¨¡çš„æ–°å‹è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ–¹æ³•å¼•å…¥äº†ä¸€ç§åŸºäºäº¤äº’çš„è‡ªç„¶è¯­è¨€ç†è§£æ¨¡å‹ä½œä¸ºæ•™å¸ˆï¼Œé€šè¿‡å…¨é¢çš„æ³¨æ„åŠ›æä¾›ç²¾ç»†çš„ä»¤ç‰Œäº¤äº’æ¨¡å¼ï¼Œå¹¶é€šè¿‡æ³¨æ„åŠ›å¯¹é½ç›®æ ‡æŒ‡å¯¼åå¥½å»ºæ¨¡æ¥æ¨¡æ‹Ÿæ•™å¸ˆæ¨¡å‹çš„äº¤äº’æ¨¡å¼ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œäº¤äº’è’¸é¦æ˜¾ç¤ºå‡ºå…¶æä¾›æ¯”é’ˆå¯¹æ•°æ®å™ªå£°çš„æœ€å…ˆè¿›RMä¼˜åŒ–æ–¹æ³•æ›´ç¨³å®šå’Œå¯æ¨å¹¿çš„å¥–åŠ±ä¿¡å·çš„èƒ½åŠ›ï¼Œå¼ºè°ƒäº†æ³¨æ„åŠ›æ”»å‡»åœ¨RMä¸­æ„æˆæ›´åŸºæœ¬çš„å±€é™ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02618v2">PDF</a> This paper is not suitable for this topic, we need to adjust the   context</p>
<p><strong>Summary</strong>ï¼š<br>å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ˜¯æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç”Ÿæˆå“åº”æä¾›å¥–åŠ±ä¿¡å·ã€‚ç„¶è€Œï¼Œä¸»æµçš„åå¥½å»ºæ¨¡åœ¨RMä¸­å­˜åœ¨å¯¹tokençº§åˆ«äº¤äº’çš„ä¸è¶³ï¼Œå¯¼è‡´åˆ¤æ–­ä¿¡å·å®¹æ˜“å—åˆ°é”™è¯¯åˆ†é…çš„æ³¨æ„åŠ›å¯¹ä¸Šä¸‹æ–‡çš„å½±å“ã€‚è¿™æºäºä¸¤ä¸ªåŸºæœ¬å±€é™ï¼šä¸€æ˜¯å½“å‰åå¥½å»ºæ¨¡é‡‡ç”¨å•å‘å› æœæ³¨æ„åŠ›æœºåˆ¶çš„è§£ç å™¨æ¶æ„ï¼Œå¯¼è‡´æç¤ºå“åº”åºåˆ—å†…çš„åºåˆ—å†…æ³¨æ„åŠ›è¡°å‡ï¼›äºŒæ˜¯ç‹¬ç«‹Siameseç¼–ç èŒƒå¼å¯¼è‡´æ‰€é€‰åºåˆ—å’Œæ‹’ç»åºåˆ—ä¹‹é—´çš„tokençº§åˆ«åºåˆ—é—´æ³¨æ„åŠ›ç¼ºå¤±ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†â€œäº¤äº’è’¸é¦â€è¿™ä¸€æ–°å‹è®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡æ³¨æ„åŠ›å±‚é¢çš„ä¼˜åŒ–è¿›è¡Œæ›´å‡†ç¡®çš„åå¥½å»ºæ¨¡ã€‚â€œäº¤äº’è’¸é¦â€å¼•å…¥åŸºäºäº¤äº’çš„è‡ªç„¶è¯­è¨€ç†è§£æ¨¡å‹ä½œä¸ºæ•™å¸ˆï¼Œé€šè¿‡å…¨é¢çš„æ³¨æ„åŠ›æä¾›å¤æ‚çš„tokenäº¤äº’æ¨¡å¼ï¼Œå¹¶å¼•å¯¼åå¥½å»ºæ¨¡æ¨¡æ‹Ÿæ•™å¸ˆæ¨¡å‹çš„äº¤äº’æ¨¡å¼é€šè¿‡æ³¨æ„åŠ›å¯¹é½ç›®æ ‡å®ç°ã€‚é€šè¿‡å®éªŒè¯æ˜ï¼Œç›¸è¾ƒäºé’ˆå¯¹æ•°æ®å™ªå£°ä¼˜åŒ–çš„å…¶ä»–RMæ–¹æ³•ï¼Œâ€œäº¤äº’è’¸é¦â€èƒ½å¤Ÿæä¾›æ›´ç¨³å®šå’Œå¯æ³›åŒ–çš„å¥–åŠ±ä¿¡å·ã€‚è¿™è¡¨æ˜æ³¨æ„åŠ›é»‘å®¢æ”»å‡»åœ¨RMä¸­æ˜¯ä¸€ä¸ªæ›´åŸºæœ¬çš„å±€é™æ€§ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ ä¸­çš„å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ˜¯æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›å¥–åŠ±ä¿¡å·ã€‚</li>
<li>å½“å‰ä¸»æµRMåœ¨tokençº§åˆ«äº¤äº’æ–¹é¢çš„ä¸è¶³å¯¼è‡´å…¶å®¹æ˜“å—åˆ°é”™è¯¯çš„æ³¨æ„åŠ›å½±å“ï¼Œè¿›è€Œå¯¼è‡´å¯¹ä¸Šä¸‹æ–‡çš„åˆ¤æ–­å­˜åœ¨è¯¯å·®ã€‚</li>
<li>åå¥½å»ºæ¨¡å­˜åœ¨ä¸¤ä¸ªåŸºæœ¬å±€é™ï¼šé‡‡ç”¨å•å‘å› æœæ³¨æ„åŠ›æœºåˆ¶çš„è§£ç å™¨æ¶æ„å’Œç‹¬ç«‹Siameseç¼–ç èŒƒå¼å¯¼è‡´tokençº§åˆ«äº¤äº’ä¸è¶³ã€‚</li>
<li>ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæå‡ºäº†â€œäº¤äº’è’¸é¦â€è¿™ä¸€æ–°å‹è®­ç»ƒæ¡†æ¶ã€‚è¿™ä¸€æ–¹æ³•å¼•å…¥äº†åŸºäºäº¤äº’çš„è‡ªç„¶è¯­è¨€ç†è§£æ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¥æŒ‡å¯¼åå¥½å»ºæ¨¡ã€‚</li>
<li>â€œäº¤äº’è’¸é¦â€é€šè¿‡å…¨é¢çš„æ³¨æ„åŠ›æä¾›å¤æ‚çš„tokenäº¤äº’æ¨¡å¼ï¼Œå¹¶å¼•å¯¼åå¥½å»ºæ¨¡æ¨¡æ‹Ÿæ•™å¸ˆæ¨¡å‹çš„äº¤äº’æ¨¡å¼é€šè¿‡æ³¨æ„åŠ›å¯¹é½ç›®æ ‡å®ç°ã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºæä¾›æ›´ç¨³å®šå’Œå¯æ³›åŒ–çš„å¥–åŠ±ä¿¡å·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02618">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-0aa758030e87e3261ee82f31b003c935" align="middle">
<img src="https://picx.zhimg.com/v2-f6893c163fc7f47bae88b58b02984b63" align="middle">
<img src="https://picx.zhimg.com/v2-9aa88decb8b2322586720b54f82c4e94" align="middle">
<img src="https://picx.zhimg.com/v2-df1bfefaa73abf3ab37ed7b367788baf" align="middle">
<img src="https://picx.zhimg.com/v2-79c624f9f637e5a058a01bf9c913aecf" align="middle">
<img src="https://picx.zhimg.com/v2-ffebb094f31a77d29f0db6cd31f80a29" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DualSG-A-Dual-Stream-Explicit-Semantic-Guided-Multivariate-Time-Series-Forecasting-Framework"><a href="#DualSG-A-Dual-Stream-Explicit-Semantic-Guided-Multivariate-Time-Series-Forecasting-Framework" class="headerlink" title="DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series   Forecasting Framework"></a>DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series   Forecasting Framework</h2><p><strong>Authors:Kuiye Ding, Fanda Fan, Yao Wang, Ruijie jian, Xiaorui Wang, Luqi Gong, Yishan Jiang, Chunjie Luo, Jianfeng Zhan</strong></p>
<p>Multivariate Time Series Forecasting plays a key role in many applications. Recent works have explored using Large Language Models for MTSF to take advantage of their reasoning abilities. However, many methods treat LLMs as end-to-end forecasters, which often leads to a loss of numerical precision and forces LLMs to handle patterns beyond their intended design. Alternatively, methods that attempt to align textual and time series modalities within latent space frequently encounter alignment difficulty. In this paper, we propose to treat LLMs not as standalone forecasters, but as semantic guidance modules within a dual-stream framework. We propose DualSG, a dual-stream framework that provides explicit semantic guidance, where LLMs act as Semantic Guides to refine rather than replace traditional predictions. As part of DualSG, we introduce Time Series Caption, an explicit prompt format that summarizes trend patterns in natural language and provides interpretable context for LLMs, rather than relying on implicit alignment between text and time series in the latent space. We also design a caption-guided fusion module that explicitly models inter-variable relationships while reducing noise and computation. Experiments on real-world datasets from diverse domains show that DualSG consistently outperforms 15 state-of-the-art baselines, demonstrating the value of explicitly combining numerical forecasting with semantic guidance. </p>
<blockquote>
<p>å¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹åœ¨ä¼—å¤šåº”ç”¨ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚è¿‘æœŸçš„ç ”ç©¶å·¥ä½œæ¢ç´¢äº†ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡ŒMTSFï¼Œä»¥åˆ©ç”¨å…¶æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè®¸å¤šæ–¹æ³•å°†å¤§å‹è¯­è¨€æ¨¡å‹è§†ä¸ºç«¯åˆ°ç«¯çš„é¢„æµ‹å™¨ï¼Œè¿™å¾€å¾€ä¼šå¯¼è‡´æ•°å€¼ç²¾åº¦çš„æŸå¤±ï¼Œå¹¶è¿«ä½¿å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†è¶…å‡ºå…¶é¢„æœŸè®¾è®¡çš„æ¨¡å¼ã€‚å¦ä¸€æ–¹é¢ï¼Œå°è¯•åœ¨æ½œåœ¨ç©ºé—´å†…å¯¹é½æ–‡æœ¬å’Œæ—¶é—´åºåˆ—æ¨¡æ€çš„æ–¹æ³•ç»å¸¸é‡åˆ°å¯¹é½å›°éš¾çš„é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸åº”å°†å¤§å‹è¯­è¨€æ¨¡å‹è§†ä¸ºç‹¬ç«‹çš„é¢„æµ‹å™¨ï¼Œè€Œæ˜¯åº”ä½œä¸ºåŒæµæ¡†æ¶å†…çš„è¯­ä¹‰æŒ‡å¯¼æ¨¡å—ã€‚æˆ‘ä»¬æå‡ºäº†DualSGï¼Œè¿™æ˜¯ä¸€ä¸ªåŒæµæ¡†æ¶ï¼Œæä¾›æ˜ç¡®çš„è¯­ä¹‰æŒ‡å¯¼ï¼Œå…¶ä¸­å¤§å‹è¯­è¨€æ¨¡å‹å……å½“è¯­ä¹‰æŒ‡å—ï¼Œä»¥æ”¹è¿›è€Œä¸æ˜¯æ›¿ä»£ä¼ ç»Ÿé¢„æµ‹ã€‚ä½œä¸ºDualSGçš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ—¶é—´åºåˆ—å­—å¹•ï¼Œè¿™æ˜¯ä¸€ç§æ˜ç¡®çš„æç¤ºæ ¼å¼ï¼Œå¯ä»¥æ€»ç»“è‡ªç„¶è¯­è¨€ä¸­çš„è¶‹åŠ¿æ¨¡å¼ï¼Œå¹¶ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›å¯è§£é‡Šçš„èƒŒæ™¯ï¼Œè€Œä¸æ˜¯ä¾èµ–æ½œåœ¨ç©ºé—´ä¸­æ–‡æœ¬å’Œåºåˆ—ä¹‹é—´çš„éšå«å¯¹é½ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªå­—å¹•å¼•å¯¼èåˆæ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥æ˜ç¡®åœ°å»ºæ¨¡å˜é‡ä¹‹é—´çš„å…³ç³»ï¼ŒåŒæ—¶å‡å°‘å™ªå£°å’Œè®¡ç®—é‡ã€‚åœ¨æ¥è‡ªä¸åŒé¢†åŸŸçš„çœŸå®æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDualSGæŒç»­ä¼˜äº15ä¸ªæœ€æ–°åŸºçº¿ï¼Œè¯æ˜äº†æ˜¾å¼ç»“åˆæ•°å€¼é¢„æµ‹å’Œè¯­ä¹‰æŒ‡å¯¼çš„ä»·å€¼ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21830v4">PDF</a> This paper has been accepted by ACM Multimedia 2025 (ACM MM 2025)</p>
<p><strong>Summary</strong>ï¼šåˆ©ç”¨å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œæ—¶åºæ•°æ®é¢„æµ‹ä¸­çš„é‡è¦æ€§æ—¥ç›Šæ˜¾è‘—ã€‚æœ¬ç ”ç©¶æå‡ºä¸€ç§åŒæµæ¡†æ¶DualSGï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¯­ä¹‰æŒ‡å¯¼æ¨¡å—ï¼Œç”¨äºæé«˜é¢„æµ‹çš„ç²¾åº¦å’Œå¯è§£é‡Šæ€§ã€‚é€šè¿‡å¼•å…¥æ—¶é—´åºåˆ—å­—å¹•ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ˜ç¡®æè¿°è¶‹åŠ¿æ¨¡å¼å¹¶æä¾›å¯ç†è§£çš„ä¸Šä¸‹æ–‡ç¯å¢ƒã€‚å®éªŒè¯æ˜ï¼ŒDualSGåœ¨çœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šå˜é‡æ—¶é—´åºåˆ—é¢„æµ‹ä¸­æ‰®æ¼”é‡è¦è§’è‰²ã€‚</li>
<li>å½“å‰æ–¹æ³•å¸¸å¸¸å°†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºç‹¬ç«‹é¢„æµ‹å™¨ï¼Œå¯èƒ½å¯¼è‡´æ•°å€¼ç²¾åº¦æŸå¤±å’Œå¤„ç†è¶…å‡ºè®¾è®¡èŒƒå›´çš„å›¾æ¡ˆã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§åŒæµæ¡†æ¶DualSGï¼Œå°†å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºè¯­ä¹‰æŒ‡å¯¼æ¨¡å—ï¼Œè€Œéç‹¬ç«‹é¢„æµ‹å™¨ã€‚</li>
<li>å¼•å…¥æ—¶é—´åºåˆ—å­—å¹•ï¼Œæ˜ç¡®æè¿°è¶‹åŠ¿æ¨¡å¼å¹¶æä¾›å¯ç†è§£çš„ä¸Šä¸‹æ–‡ç¯å¢ƒç»™å¤§å‹è¯­è¨€æ¨¡å‹ã€‚</li>
<li>è®¾è®¡äº†å­—å¹•å¼•å¯¼èåˆæ¨¡å—ï¼Œæ˜ç¡®å»ºæ¨¡å˜é‡é—´å…³ç³»ï¼Œå‡å°‘å™ªå£°å’Œè®¡ç®—ã€‚</li>
<li>å®éªŒè¯æ˜ï¼ŒDualSGåœ¨çœŸå®æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–å…ˆè¿›æ–¹æ³•ï¼Œè¡¨æ˜ç»“åˆæ•°å€¼é¢„æµ‹å’Œè¯­ä¹‰æŒ‡å¯¼çš„ä»·å€¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21830">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-66d1749ecb3cace6d64b6b8c1aa9964a" align="middle">
<img src="https://picx.zhimg.com/v2-acaa919d9a318d57e13b6d374a9901a1" align="middle">
<img src="https://picx.zhimg.com/v2-8b244bb573dbd62108f04fc1cc6130f5" align="middle">
<img src="https://picx.zhimg.com/v2-73fa3d827ee28cb958e01f27ca38c14b" align="middle">
<img src="https://picx.zhimg.com/v2-61592458b84aa9603a91bb09c562a711" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LoRA-PAR-A-Flexible-Dual-System-LoRA-Partitioning-Approach-to-Efficient-LLM-Fine-Tuning"><a href="#LoRA-PAR-A-Flexible-Dual-System-LoRA-Partitioning-Approach-to-Efficient-LLM-Fine-Tuning" class="headerlink" title="LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient   LLM Fine-Tuning"></a>LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient   LLM Fine-Tuning</h2><p><strong>Authors:Yining Huang, Bin Li, Keke Tang, Meilian Chen</strong></p>
<p>Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit substantially from chain-of-thought (CoT) reasoning, yet pushing their performance typically requires vast data, large model sizes, and full-parameter fine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost, most existing approaches primarily address domain adaptation or layer-wise allocation rather than explicitly tailoring data and parameters to different response demands. Inspired by â€œThinking, Fast and Slow,â€ which characterizes two distinct modes of thought-System 1 (fast, intuitive, often automatic) and System 2 (slower, more deliberative and analytic)-we draw an analogy that different â€œsubregionsâ€ of an LLMâ€™s parameters might similarly specialize for tasks that demand quick, intuitive responses versus those requiring multi-step logical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework that partitions both data and parameters by System 1 or System 2 demands, using fewer yet more focused parameters for each task. Specifically, we classify task data via multi-model role-playing and voting, and partition parameters based on importance scoring, then adopt a two-stage fine-tuning strategy of training System 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and intuition and refine System 2 tasks with reinforcement learning (RL) to reinforce deeper logical deliberation next. Extensive experiments show that the two-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while matching or surpassing SOTA PEFT baselines. </p>
<blockquote>
<p>å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹å¦‚DeepSeek-R1å’ŒOpenAI-O1åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå—ç›Šäºæ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†ï¼Œç„¶è€Œæå‡å®ƒä»¬çš„æ€§èƒ½é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®ã€åºå¤§çš„æ¨¡å‹è§„æ¨¡å’Œå…¨å‚æ•°å¾®è°ƒã€‚è™½ç„¶å‚æ•°æ•ˆç‡é«˜çš„å¾®è°ƒï¼ˆPEFTï¼‰æœ‰åŠ©äºé™ä½æˆæœ¬ï¼Œä½†å¤§å¤šæ•°ç°æœ‰æ–¹æ³•ä¸»è¦è§£å†³åŸŸé€‚åº”æˆ–é€å±‚åˆ†é…é—®é¢˜ï¼Œè€Œä¸æ˜¯æ˜¾å¼åœ°é’ˆå¯¹æ•°æ®å’Œå‚æ•°è¿›è¡Œä¸åŒçš„å“åº”éœ€æ±‚è°ƒæ•´ã€‚å—ã€Šæ€è€ƒï¼Œå¿«ä¸æ…¢ã€‹çš„å¯å‘ï¼Œè¯¥ä¹¦æè¿°äº†ä¸¤ç§æˆªç„¶ä¸åŒçš„æ€ç»´æ¨¡å¼â€”â€”ç³»ç»Ÿ1ï¼ˆå¿«é€Ÿã€ç›´è§‚ã€é€šå¸¸è‡ªåŠ¨ï¼‰å’Œç³»ç»Ÿ2ï¼ˆè¾ƒæ…¢ã€æ›´å®¡æ…å’Œåˆ†ææ€§ï¼‰â€”â€”æˆ‘ä»¬è¿›è¡Œç±»æ¯”ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„â€œä¸åŒå­åŒºåŸŸâ€å‚æ•°å¯èƒ½ç±»ä¼¼äºé’ˆå¯¹éœ€è¦å¿«é€Ÿç›´è§‚ååº”çš„ä»»åŠ¡ä¸éœ€è¦å¤šæ­¥éª¤é€»è¾‘æ¨ç†çš„ä»»åŠ¡è¿›è¡Œä¸“ä¸šåŒ–å¤„ç†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LoRA-PARåŒç³»ç»ŸLoRAæ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ ¹æ®ç³»ç»Ÿ1æˆ–ç³»ç»Ÿ2çš„éœ€æ±‚å¯¹æ•°æ®å’Œå‚æ•°è¿›è¡Œåˆ†åŒºï¼Œä¸ºæ¯ä¸ªä»»åŠ¡ä½¿ç”¨æ›´å°‘ä½†æ›´ä¸“æ³¨çš„å‚æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡å¤šæ¨¡å‹è§’è‰²æ‰®æ¼”å’ŒæŠ•ç¥¨å¯¹ä»»åŠ¡æ•°æ®è¿›è¡Œåˆ†ç±»ï¼Œæ ¹æ®é‡è¦æ€§è¯„åˆ†å¯¹å‚æ•°è¿›è¡Œåˆ†åŒºï¼Œç„¶åé‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒç³»ç»Ÿ1ä»»åŠ¡ä»¥å¢å¼ºçŸ¥è¯†å’Œç›´è§‰ï¼Œç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹ç³»ç»Ÿ2ä»»åŠ¡è¿›è¡Œå¾®è°ƒä»¥åŠ å¼ºæ›´æ·±å…¥çš„é€»è¾‘æ€è€ƒã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥â€”â€”SFTå’ŒRLï¼Œé™ä½äº†æ´»åŠ¨å‚æ•°çš„ä½¿ç”¨é‡ï¼ŒåŒæ—¶è¾¾åˆ°æˆ–è¶…è¶Šäº†æœ€æ–°çš„PEFTåŸºå‡†æµ‹è¯•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20999v3">PDF</a> 12 pages</p>
<p><strong>Summary</strong><br>     å¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹å¦‚DeepSeek-R1å’ŒOpenAI-O1æ˜¾è‘—å—ç›Šäºé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ï¼Œä½†è¦æå‡å…¶æ€§èƒ½é€šå¸¸éœ€ä¾èµ–å¤§é‡æ•°æ®ã€åºå¤§çš„æ¨¡å‹è§„æ¨¡å’Œå…¨é¢çš„å‚æ•°å¾®è°ƒã€‚å°½ç®¡å‚æ•°æ•ˆç‡å¾®è°ƒï¼ˆPEFTï¼‰æœ‰åŠ©äºé™ä½æˆæœ¬ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨é¢†åŸŸé€‚åº”æˆ–åˆ†å±‚åˆ†é…ä¸Šï¼Œå¹¶æœªæ˜ç¡®é’ˆå¯¹æ•°æ®å’Œå‚æ•°é€‚åº”ä¸åŒå“åº”éœ€æ±‚ã€‚å—â€œå¿«é€Ÿæ€è€ƒï¼Œæ…¢é€Ÿå†³ç­–â€çš„å¯å‘ï¼Œæˆ‘ä»¬å°†LLMå‚æ•°çš„ä¸åŒâ€œå­åŒºåŸŸâ€ç±»æ¯”ä¸ºä¸“é—¨æ‰§è¡Œå¿«é€Ÿç›´è§‰ååº”ä¸éœ€è¦å¤šæ­¥éª¤é€»è¾‘æ¨ç†çš„ä»»åŠ¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†LoRA-PARåŒç³»ç»Ÿæ¡†æ¶ï¼Œæ ¹æ®ç³»ç»Ÿ1å’Œç³»ç»Ÿ2çš„éœ€æ±‚å¯¹æ•°æ®å’Œå‚æ•°è¿›è¡Œåˆ†åŒºï¼Œä¸ºæ¯ä¸ªä»»åŠ¡ä½¿ç”¨æ›´å°‘ä½†æ›´é›†ä¸­çš„å‚æ•°ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é€šè¿‡å¤šæ¨¡å‹è§’è‰²æ‰®æ¼”å’ŒæŠ•ç¥¨å¯¹ä»»åŠ¡æ•°æ®è¿›è¡Œåˆ†ç±»ï¼Œæ ¹æ®é‡è¦æ€§è¯„åˆ†å¯¹å‚æ•°è¿›è¡Œåˆ†åŒºï¼Œç„¶åé‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œå…ˆç”¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒç³»ç»Ÿ1ä»»åŠ¡ä»¥å¢å¼ºçŸ¥è¯†å’Œç›´è§‰ï¼Œå†ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å®Œå–„ç³»ç»Ÿ2ä»»åŠ¡ä»¥å¼ºåŒ–æ·±åº¦é€»è¾‘æ€è€ƒã€‚å®éªŒè¡¨æ˜ï¼Œä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥SFTå’ŒRLåœ¨é™ä½æ´»åŠ¨å‚æ•°ä½¿ç”¨çš„åŒæ—¶ï¼Œè¾¾åˆ°äº†æˆ–è¶…è¶Šäº†æœ€æ–°PEFTåŸºå‡†ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹ç”Ÿæˆæ¨¡å‹å—ç›Šäºé“¾å¼æ€ç»´ï¼ˆCoTï¼‰æ¨ç†ã€‚</li>
<li>å‚æ•°æ•ˆç‡å¾®è°ƒï¼ˆPEFTï¼‰å¯¹äºé™ä½æ¨¡å‹æˆæœ¬è‡³å…³é‡è¦ã€‚</li>
<li>ç°æœ‰PEFTæ–¹æ³•ä¸»è¦å…³æ³¨é¢†åŸŸé€‚åº”å’Œåˆ†å±‚åˆ†é…ï¼Œç¼ºä¹é’ˆå¯¹æ•°æ®å’Œå‚æ•°çš„çµæ´»é€‚åº”ã€‚</li>
<li>å—â€œå¿«é€Ÿæ€è€ƒï¼Œæ…¢é€Ÿå†³ç­–â€å¯å‘ï¼Œæå‡ºLLMå‚æ•°çš„ä¸åŒâ€œå­åŒºåŸŸâ€å¯ä¸“é—¨æ‰§è¡Œä¸åŒä»»åŠ¡ã€‚</li>
<li>LoRA-PARæ¡†æ¶æ ¹æ®ç³»ç»Ÿ1å’Œç³»ç»Ÿ2çš„éœ€æ±‚å¯¹æ•°æ®å’Œå‚æ•°è¿›è¡Œåˆ†åŒºã€‚</li>
<li>é‡‡ç”¨ä¸¤é˜¶æ®µå¾®è°ƒç­–ç•¥ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å¼ºåŒ–çŸ¥è¯†å’Œç›´è§‰ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¼ºåŒ–æ·±åº¦é€»è¾‘æ€è€ƒã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20999">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d4a8bc19819bf592ed35ba652dfa9bd7" align="middle">
<img src="https://picx.zhimg.com/v2-7d676e9a31db71c9a6bc1d02ce233f15" align="middle">
<img src="https://picx.zhimg.com/v2-5b766dbf46c3c4a112909737ce944510" align="middle">
<img src="https://picx.zhimg.com/v2-497e7a952f8102e1236877e9348a3865" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="NIRS-An-Ontology-for-Non-Invasive-Respiratory-Support-in-Acute-Care"><a href="#NIRS-An-Ontology-for-Non-Invasive-Respiratory-Support-in-Acute-Care" class="headerlink" title="NIRS: An Ontology for Non-Invasive Respiratory Support in Acute Care"></a>NIRS: An Ontology for Non-Invasive Respiratory Support in Acute Care</h2><p><strong>Authors:Md Fantacher Islam, Jarrod Mosier, Vignesh Subbian</strong></p>
<p>Objective: Managing patients with respiratory failure increasingly involves non-invasive respiratory support (NIRS) strategies as alternatives to traditional ventilation methods. However, despite the rapidly expanding use of NIRS, there is a significant challenge to its best use under all medical circumstances. It lacks a unified ontological structure, complicating guidance on NIRS modalities across healthcare systems. Our goal is to develop NIRS ontology to support knowledge representation in acute care settings by providing a unified framework that enhances data clarity, interoperability, and clinical decision-making.   Methods: We developed the NIRS ontology using Web Ontology Language (OWL) semantics and Protege to organize clinical concepts and relationships. To enable rule-based clinical reasoning beyond hierarchical structures, we added Semantic Web Rule Language (SWRL) rules. We evaluated logical reasoning by adding 17 hypothetical clinical scenarios. We used SPARQL queries to retrieve and test targeted inferences.   Results: The ontology has 129 classes, 11 object properties, and 17 data properties across 886 axioms that establish concept relationships. To standardize clinical concepts, we added 361 annotations, including descriptive definitions based on controlled vocabularies. SPARQL queries successfully validated all test cases (rules) by retrieving appropriate patient outcomes: for instance, a patient treated with HFNC (high-flow nasal cannula) for 2 hours due to acute respiratory failure may avoid endotracheal intubation.   Conclusion: We developed an ontology that captures NIRS modalities in a unified framework and demonstrated its applicability through the evaluation of hypothetical patient scenarios and alignment with standardized vocabularies, which may need to be expanded to encompass a broader scope. </p>
<blockquote>
<p>ç›®æ ‡ï¼šéšç€æ— åˆ›å‘¼å¸æ”¯æŒï¼ˆNIRSï¼‰ç­–ç•¥çš„å‘å±•ï¼Œå…¶åœ¨å‘¼å¸è¡°ç«­æ‚£è€…ç®¡ç†ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œæˆä¸ºä¼ ç»Ÿé€šæ°”æ–¹æ³•çš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå°½ç®¡æ— åˆ›å‘¼å¸æ”¯æŒçš„ä½¿ç”¨æ­£åœ¨è¿…é€Ÿæ‰©å¤§ï¼Œä½†åœ¨æ‰€æœ‰åŒ»ç–—æƒ…å†µä¸‹ï¼Œå…¶æœ€ä½³ä½¿ç”¨ä»å­˜åœ¨é‡å¤§æŒ‘æˆ˜ã€‚ç¼ºä¹ç»Ÿä¸€çš„æœ¬ä½“ç»“æ„ï¼Œå¯¼è‡´è·¨åŒ»ç–—ä¿å¥ç³»ç»Ÿçš„æ— åˆ›å‘¼å¸æ”¯æŒæ¨¡å¼æŒ‡å¯¼å˜å¾—å¤æ‚ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¼€å‘æ— åˆ›å‘¼å¸æ”¯æŒæœ¬ä½“ï¼Œä»¥æ”¯æŒæ€¥æ€§æŠ¤ç†ç¯å¢ƒä¸­çš„çŸ¥è¯†è¡¨ç¤ºã€‚é€šè¿‡æä¾›ç»Ÿä¸€çš„æ¡†æ¶ï¼Œå¢å¼ºæ•°æ®çš„æ¸…æ™°åº¦ã€äº’æ“ä½œæ€§å’Œä¸´åºŠå†³ç­–èƒ½åŠ›ã€‚</p>
</blockquote>
<p>æ–¹æ³•ï¼šæˆ‘ä»¬ä½¿ç”¨Webæœ¬ä½“è¯­è¨€ï¼ˆOWLï¼‰è¯­ä¹‰å’ŒProtegeæ¥ç»„ç»‡ä¸´åºŠæ¦‚å¿µå’Œå…³ç³»ï¼Œä»¥å¼€å‘æ— åˆ›å‘¼å¸æ”¯æŒæœ¬ä½“ã€‚ä¸ºäº†è¶…è¶Šå±‚æ¬¡ç»“æ„å®ç°åŸºäºè§„åˆ™çš„ä¸´åºŠæ¨ç†ï¼Œæˆ‘ä»¬æ·»åŠ äº†è¯­ä¹‰Webè§„åˆ™è¯­è¨€ï¼ˆSWRLï¼‰è§„åˆ™ã€‚æˆ‘ä»¬é€šè¿‡æ·»åŠ 17ä¸ªå‡è®¾çš„ä¸´åºŠåœºæ™¯æ¥è¯„ä¼°é€»è¾‘æ¨ç†ã€‚æˆ‘ä»¬ä½¿ç”¨SPARQLæŸ¥è¯¢æ¥æ£€ç´¢å’Œæµ‹è¯•ç›®æ ‡æ¨æ–­ã€‚</p>
<p>ç»“æœï¼šè¯¥æœ¬ä½“åŒ…å«129ä¸ªç±»ã€11ä¸ªå¯¹è±¡å±æ€§å’Œ17ä¸ªæ•°æ®å±æ€§ï¼Œè·¨è¶Š886æ¡å…¬ç†å»ºç«‹æ¦‚å¿µå…³ç³»ã€‚ä¸ºäº†æ ‡å‡†åŒ–ä¸´åºŠæ¦‚å¿µï¼Œæˆ‘ä»¬æ·»åŠ äº†361ä¸ªæ³¨é‡Šï¼ŒåŒ…æ‹¬åŸºäºå—æ§è¯æ±‡çš„æè¿°æ€§å®šä¹‰ã€‚SPARQLæŸ¥è¯¢æˆåŠŸéªŒè¯äº†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ï¼ˆè§„åˆ™ï¼‰ï¼Œé€šè¿‡æ£€ç´¢é€‚å½“çš„ç—…äººç»“æœï¼Œä¾‹å¦‚ï¼Œå› æ€¥æ€§å‘¼å¸è¡°ç«­æ¥å—é«˜æµé‡é¼»å¯¼ç®¡æ²»ç–—2å°æ—¶çš„æ‚£è€…å¯èƒ½é¿å…æ°”ç®¡æ’ç®¡ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19992v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>NIRSæœ¬ä½“æ„å»º<br>å½“å‰æ‚£è€…ç®¡ç†çš„ç ”ç©¶é€æ¸é›†ä¸­åœ¨æ— åˆ›å‘¼å¸æ”¯æŒï¼ˆNIRSï¼‰ç­–ç•¥ä¸Šï¼Œå®ƒæ˜¯ä¼ ç»Ÿé€šæ°”æ–¹æ³•çš„æ›¿ä»£æ–¹æ³•ã€‚å°½ç®¡NIRSçš„åº”ç”¨æ­£åœ¨è¿…é€Ÿæ‰©å¤§ï¼Œä½†åœ¨æ‰€æœ‰åŒ»ç–—æƒ…å†µä¸‹å…¶æœ€ä½³ä½¿ç”¨ä»å­˜åœ¨æŒ‘æˆ˜ã€‚ç”±äºç¼ºä¹ç»Ÿä¸€çš„æœ¬ä½“ç»“æ„ï¼Œå¯¼è‡´åœ¨åŒ»ç–—ä¿å¥ç³»ç»Ÿä¸­å¯¹NIRSæ¨¡å¼çš„æŒ‡å¯¼å˜å¾—å¤æ‚ã€‚æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ä¸ªNIRSæœ¬ä½“ï¼Œä»¥æ”¯æŒæ€¥æ€§æŠ¤ç†ç¯å¢ƒä¸‹çš„çŸ¥è¯†è¡¨ç¤ºï¼Œé€šè¿‡æä¾›ç»Ÿä¸€çš„æ¡†æ¶æ¥å¢å¼ºæ•°æ®çš„æ¸…æ™°åº¦ã€äº’æ“ä½œæ€§å’Œä¸´åºŠå†³ç­–èƒ½åŠ›ã€‚æœ¬ç ”ç©¶ä½¿ç”¨Webæœ¬ä½“è¯­è¨€ï¼ˆOWLï¼‰è¯­ä¹‰å’ŒProtegeæ¥ç»„ç»‡ä¸´åºŠæ¦‚å¿µå’Œå…³ç³»ã€‚ä¸ºäº†å»ºç«‹æ¦‚å¿µä¹‹é—´çš„å…³è”å’Œä¸´åºŠè§„åˆ™æ¨ç†çš„éœ€è¦ï¼Œæˆ‘ä»¬åœ¨è¯­ä¹‰ç½‘ç»œä¸Šå¼•å…¥äº†SWRLè§„åˆ™ï¼Œå¹¶ä¸”æ„å»ºäº†è¯„ä¼°é€»è¾‘çš„æ¨ç†å‡è®¾ã€‚ä½¿ç”¨SPARQLæŸ¥è¯¢è¿›è¡Œæ¨ç†æµ‹è¯•ï¼ŒéªŒè¯å…¶å‡†ç¡®æ€§ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬æ„å»ºäº†æ¶µç›–å„ç§æ¦‚å¿µå’Œå…³ç³»çš„ç»Ÿä¸€æ¡†æ¶çš„NIRSæœ¬ä½“ã€‚åŸºäºæ ‡å‡†è¯æ±‡å¯¹ä¸´åºŠæ¦‚å¿µè¿›è¡Œäº†æ ‡å‡†åŒ–æè¿°ï¼Œä¸”åˆæ­¥æµ‹è¯•æˆåŠŸéªŒè¯äº†æ‰€æœ‰å‡è®¾æ¡ˆä¾‹çš„æ¨ç†å‡†ç¡®æ€§ã€‚ä¾‹å¦‚ï¼Œå¯¹äºå› æ€¥æ€§å‘¼å¸è¡°ç«­æ¥å—é«˜æµé‡é¼»å¯¼ç®¡æ²»ç–—ä¸¤å°æ—¶çš„æ‚£è€…ï¼Œå¯ä»¥é¿å…è¿›è¡Œæ°”ç®¡æ’ç®¡ã€‚ç»“è®ºï¼šæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ä¸‹çš„NIRSæ¨¡å¼æœ¬ä½“ï¼Œå¹¶é€šè¿‡å‡è®¾æ‚£è€…æƒ…æ™¯çš„è¯„ä¼°å’Œæ ‡å‡†åŒ–è¯æ±‡çš„åŒ¹é…æ¥éªŒè¯å…¶é€‚ç”¨æ€§ï¼Œæœªæ¥å¯èƒ½éœ€è¦æ‰©å¤§å…¶èŒƒå›´ä»¥æ¶µç›–æ›´å¹¿æ³›çš„é¢†åŸŸã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>NIRSç­–ç•¥ä½œä¸ºä¼ ç»Ÿé€šæ°”æ–¹æ³•çš„æ›¿ä»£æ–¹æ¡ˆåœ¨æ€¥æ€§æŠ¤ç†ç¯å¢ƒä¸­å—åˆ°è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚</li>
<li>å¼€å‘ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶è‡³å…³é‡è¦ï¼Œä»¥æé«˜NIRSæ•°æ®æ¸…æ™°åº¦ã€äº’æ“ä½œæ€§å’Œä¸´åºŠå†³ç­–èƒ½åŠ›ã€‚</li>
<li>ä½¿ç”¨OWLè¯­ä¹‰å’ŒProtegeå·¥å…·æ„å»ºNIRSæœ¬ä½“ä»¥ç»„ç»‡ä¸´åºŠæ¦‚å¿µå’Œå…³ç³»ã€‚é€šè¿‡å¼•å…¥SWRLè§„åˆ™å®ç°äº†åŸºäºè§„åˆ™çš„æ¨ç†ï¼Œä¿ƒè¿›äº†æ¦‚å¿µé—´å…³ç³»çš„å»ºç«‹ã€‚</li>
<li>æœ¬ä½“åŒ…å«æ¦‚å¿µä¸æœ¯è¯­çš„ç»Ÿä¸€æ¡†æ¶ã€æ ‡å‡†åŒ–çš„ä¸´åºŠæ¦‚å¿µæè¿°å’Œå‡è®¾æ¡ˆä¾‹éªŒè¯çš„æ¨ç†å‡†ç¡®æ€§ç­‰å…³é”®å…ƒç´ ã€‚æ­¤å¤–é€šè¿‡SPARQLæŸ¥è¯¢æµ‹è¯•è¯æ˜äº†å…¶å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ã€‚è¿™å¯èƒ½æœ‰åŠ©äºæé«˜å†³ç­–æ•ˆç‡å’Œä¿ƒè¿›ä¸´åºŠçŸ¥è¯†çš„å…±äº«ä¸ä½¿ç”¨ã€‚</li>
<li>æœ¬ä½“æ„å»ºæœ‰åŠ©äºæ ‡å‡†åŒ–ä¸´åºŠå®è·µä¸­çš„æœ¯è¯­å’Œæ¦‚å¿µï¼Œæé«˜åŒ»ç–—å†³ç­–çš„è´¨é‡å’Œæ•ˆç‡ã€‚æœªæ¥å¯èƒ½éœ€è¦è¿›ä¸€æ­¥æ‰©å¤§åº”ç”¨èŒƒå›´å¹¶èå…¥æ›´å¤šå†…å®¹ä»¥æé«˜é€‚ç”¨æ€§ã€‚å¯¹äºæ›´å¤æ‚çš„ä¸´åºŠæƒ…æ™¯å’Œéœ€æ±‚è¿›ä¸€æ­¥ç ”ç©¶å’Œæ‰©å±•æœ¬ä½“æ˜¯å¿…è¦çš„æ­¥éª¤å’Œæ–¹å‘ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19992">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1629d59b55a795379ee8118175aa2a37" align="middle">
<img src="https://picx.zhimg.com/v2-5ae07725d375dcdc92c56b69901e8298" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ThinkAct-Vision-Language-Action-Reasoning-via-Reinforced-Visual-Latent-Planning"><a href="#ThinkAct-Vision-Language-Action-Reasoning-via-Reinforced-Visual-Latent-Planning" class="headerlink" title="ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent   Planning"></a>ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent   Planning</h2><p><strong>Authors:Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, Fu-En Yang</strong></p>
<p>Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks. </p>
<blockquote>
<p>è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨ç†ä»»åŠ¡è¦æ±‚æ™ºèƒ½ä¸»ä½“è§£é‡Šå¤šæ¨¡å¼æŒ‡ä»¤ï¼Œæ‰§è¡Œé•¿æœŸè§„åˆ’ï¼Œå¹¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œè‡ªé€‚åº”åŠ¨ä½œã€‚ç°æœ‰æ–¹æ³•é€šå¸¸ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è®­ç»ƒVLAæ¨¡å‹ï¼Œç›´æ¥å°†è¾“å…¥æ˜ å°„åˆ°åŠ¨ä½œï¼Œæ²¡æœ‰æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œè¿™é˜»ç¢äº†å®ƒä»¬è¿›è¡Œå¤šæ­¥éª¤è§„åˆ’æˆ–é€‚åº”å¤æ‚ä»»åŠ¡å˜åŒ–çš„èƒ½åŠ›ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ThinkActï¼Œè¿™æ˜¯ä¸€ä¸ªåŒç³»ç»Ÿæ¡†æ¶ï¼Œå®ƒé€šè¿‡å¼ºåŒ–è§†è§‰æ½œåœ¨è§„åˆ’æ¥æ¡¥æ¥é«˜çº§æ¨ç†å’Œä½çº§åŠ¨ä½œæ‰§è¡Œã€‚ThinkActè®­ç»ƒä¸€ä¸ªå¤šæ¨¡å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œä»¥ç”Ÿæˆç”±ä¸è¡ŒåŠ¨ä¸€è‡´çš„è§†è§‰å¥–åŠ±å¼•å¯¼çš„å®ä½“æ¨ç†è®¡åˆ’ï¼Œè¿™äº›å¥–åŠ±åŸºäºç›®æ ‡å®Œæˆå’Œè½¨è¿¹ä¸€è‡´æ€§ã€‚è¿™äº›æ¨ç†è®¡åˆ’è¢«å‹ç¼©æˆè§†è§‰è®¡åˆ’æ½œåœ¨é‡ï¼Œä»¥åœ¨ä¸‹æ¸¸åŠ¨ä½œæ¨¡å‹ä¸­æ‰§è¡Œé’ˆå¯¹ç›®æ ‡ç¯å¢ƒçš„ç¨³å¥åŠ¨ä½œã€‚å…³äºå®ä½“æ¨ç†å’Œæœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•çš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒThinkActèƒ½å¤Ÿåœ¨å¤æ‚çš„å®ä½“AIä»»åŠ¡ä¸­å®ç°å°‘é‡é€‚åº”ã€é•¿æœŸè§„åˆ’å’Œè‡ªæˆ‘ä¿®æ­£è¡Œä¸ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16815v2">PDF</a> NeurIPS 2025. Project page:   <a target="_blank" rel="noopener" href="https://jasper0314-huang.github.io/thinkact-vla/">https://jasper0314-huang.github.io/thinkact-vla/</a></p>
<p><strong>Summary</strong><br>è§†è§‰è¯­è¨€åŠ¨ä½œï¼ˆVLAï¼‰æ¨ç†ä»»åŠ¡éœ€è¦æ™ºèƒ½ä½“è§£é‡Šå¤šæ¨¡å¼æŒ‡ä»¤ï¼Œåœ¨åŠ¨æ€ç¯å¢ƒä¸­è¿›è¡Œé•¿æœŸè§„åˆ’å¹¶è‡ªé€‚åº”è¡ŒåŠ¨ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨ç«¯åˆ°ç«¯çš„è®­ç»ƒæ–¹å¼ï¼Œç›´æ¥å°†è¾“å…¥æ˜ å°„åˆ°è¡ŒåŠ¨ï¼Œç¼ºä¹æ˜ç¡®çš„æ¨ç†è¿‡ç¨‹ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨å¤šæ­¥éª¤è§„åˆ’æˆ–å¤æ‚ä»»åŠ¡å˜åŒ–ä¸­çš„é€‚åº”èƒ½åŠ›ã€‚æœ¬æ–‡æå‡ºThinkActï¼Œä¸€ä¸ªåŒç³»ç»Ÿæ¡†æ¶ï¼Œé€šè¿‡å¼ºåŒ–è§†è§‰æ½œåœ¨è§„åˆ’ï¼Œå°†é«˜çº§æ¨ç†ä¸ä½çº§è¡ŒåŠ¨æ‰§è¡Œç›¸ç»“åˆã€‚ThinkActè®­ç»ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”ŸæˆåŸºäºç›®æ ‡å®Œæˆå’Œè½¨è¿¹ä¸€è‡´æ€§çš„å¥–åŠ±å¼•å¯¼çš„èº«ä½“æ¨ç†è®¡åˆ’ã€‚è¿™äº›æ¨ç†è®¡åˆ’è¢«å‹ç¼©æˆè§†è§‰è®¡åˆ’æ½œåœ¨çŠ¶æ€ï¼Œä»¥åœ¨ç›®æ ‡ç¯å¢ƒä¸­å¯¹ä¸‹æ¸¸è¡ŒåŠ¨æ¨¡å‹è¿›è¡Œç¨³å¥çš„è¡ŒåŠ¨æ‰§è¡Œã€‚åœ¨èº«ä½“åŒ–æ¨ç†å’Œæœºå™¨äººæ“ä½œåŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒThinkActèƒ½å¤Ÿåœ¨å¤æ‚çš„åµŒå…¥å¼AIä»»åŠ¡ä¸­å®ç°å°‘æ ·æœ¬é€‚åº”ã€é•¿æœŸè§„åˆ’å’Œè‡ªæˆ‘ä¿®æ­£è¡Œä¸ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLAæ¨ç†ä»»åŠ¡è¦æ±‚æ™ºèƒ½ä½“è§£é‡Šå¤šæ¨¡å¼æŒ‡ä»¤ã€è¿›è¡Œé•¿æœŸè§„åˆ’å¹¶åœ¨åŠ¨æ€ç¯å¢ƒä¸­è‡ªé€‚åº”è¡ŒåŠ¨ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ç›´æ¥æ˜ å°„è¾“å…¥åˆ°è¡ŒåŠ¨ï¼Œç¼ºä¹æ˜ç¡®æ¨ç†ï¼Œé™åˆ¶äº†å¤šæ­¥éª¤è§„åˆ’å’Œå¤æ‚ä»»åŠ¡å˜åŒ–çš„é€‚åº”èƒ½åŠ›ã€‚</li>
<li>ThinkActæ˜¯ä¸€ä¸ªåŒç³»ç»Ÿæ¡†æ¶ï¼Œç»“åˆé«˜çº§æ¨ç†å’Œä½çº§è¡ŒåŠ¨æ‰§è¡Œã€‚</li>
<li>ThinkActé€šè¿‡å¼ºåŒ–è§†è§‰æ½œåœ¨è§„åˆ’æ¥è®­ç»ƒå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆèº«ä½“æ¨ç†è®¡åˆ’ã€‚</li>
<li>æ¨ç†è®¡åˆ’è¢«å‹ç¼©æˆè§†è§‰è®¡åˆ’æ½œåœ¨çŠ¶æ€ï¼Œä»¥åœ¨ç›®æ ‡ç¯å¢ƒä¸­è¿›è¡Œç¨³å¥çš„è¡ŒåŠ¨æ‰§è¡Œã€‚</li>
<li>ThinkActå®ç°äº†å°‘æ ·æœ¬é€‚åº”ã€é•¿æœŸè§„åˆ’å’Œè‡ªæˆ‘ä¿®æ­£è¡Œä¸ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16815">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e856f7068800be5947fc00d6c082cf2d" align="middle">
<img src="https://picx.zhimg.com/v2-26165280aef05380758055dc6996ac68" align="middle">
<img src="https://picx.zhimg.com/v2-6087562152b4780163d95b6009a36d66" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Temporal-Aware-GPU-Resource-Allocation-for-Distributed-LLM-Inference-via-Reinforcement-Learning"><a href="#Temporal-Aware-GPU-Resource-Allocation-for-Distributed-LLM-Inference-via-Reinforcement-Learning" class="headerlink" title="Temporal-Aware GPU Resource Allocation for Distributed LLM Inference via   Reinforcement Learning"></a>Temporal-Aware GPU Resource Allocation for Distributed LLM Inference via   Reinforcement Learning</h2><p><strong>Authors:Chengze Du, Zhiwei Yu, Heng Xu, Haojie Wang, Bo liu, Jialong Li</strong></p>
<p>The rapid growth of large language model (LLM) services imposes increasing demands on distributed GPU inference infrastructure. Most existing scheduling systems follow a reactive paradigm, relying solely on the current system state to make decisions, without considering how task demand and resource availability evolve over time. This lack of temporal awareness in reactive approaches leads to inefficient GPU utilization, high task migration overhead, and poor system responsiveness under dynamic workloads. In this work, we identify the fundamental limitations of these instantaneous-state-only scheduling approaches and propose Temporal Optimal Resource scheduling via Two-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling framework that captures both long-term workload patterns and short-term execution constraints. It adopts a two-layer design: a macro-level scheduler leverages reinforcement learning and optimal transport to coordinate inter-region task distribution, while a micro-level allocator refines task-to-server assignments within each region to reduce latency and switching costs. Experimental results across multiple network topologies show that TORTA reduces average inference response time by up to 15%, improves load balance by approximately 4-5%, and cuts total operational cost by 10-20% compared to state-of-the-art baseline methods. </p>
<blockquote>
<p>å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡çš„å¿«é€Ÿå¢é•¿å¯¹åˆ†å¸ƒå¼GPUæ¨ç†åŸºç¡€è®¾æ–½æå‡ºäº†æ›´é«˜çš„è¦æ±‚ã€‚ç°æœ‰çš„å¤§å¤šæ•°è°ƒåº¦ç³»ç»Ÿéƒ½é‡‡ç”¨ååº”å¼èŒƒå¼ï¼Œä»…æ ¹æ®å½“å‰ç³»ç»ŸçŠ¶æ€åšå‡ºå†³ç­–ï¼Œè€Œæ²¡æœ‰è€ƒè™‘åˆ°ä»»åŠ¡éœ€æ±‚å’Œèµ„æºå¯ç”¨æ€§çš„æ—¶é—´æ¼”å˜ã€‚è¿™ç§ååº”å¼æ–¹æ³•ä¸­ç¼ºä¹æ—¶é—´æ„ŸçŸ¥å¯¼è‡´äº†GPUåˆ©ç”¨ç‡ä½ä¸‹ã€ä»»åŠ¡è¿ç§»å¼€é”€é«˜ä»¥åŠåœ¨åŠ¨æ€å·¥ä½œè´Ÿè½½ä¸‹ç³»ç»Ÿå“åº”æ€§å·®çš„é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºäº†è¿™äº›ä»…åŸºäºå³æ—¶çŠ¶æ€çš„è°ƒåº¦æ–¹æ³•çš„åŸºæœ¬å±€é™æ€§ï¼Œå¹¶æå‡ºäº†é€šè¿‡ä¸¤å±‚æ¶æ„è¿›è¡Œæ—¶é—´æœ€ä¼˜èµ„æºè°ƒåº¦ï¼ˆTORTAï¼‰ã€‚TORTAå¼•å…¥äº†ä¸€ä¸ªæ—¶ç©ºè°ƒåº¦æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ•æ‰é•¿æœŸå·¥ä½œè´Ÿè½½æ¨¡å¼å’ŒçŸ­æœŸæ‰§è¡Œçº¦æŸã€‚å®ƒé‡‡ç”¨äº†ä¸¤å±‚è®¾è®¡ï¼šå®è§‚è°ƒåº¦å™¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å’Œæœ€ä¼˜ä¼ è¾“æ¥åè°ƒåŒºåŸŸé—´çš„ä»»åŠ¡åˆ†é…ï¼Œè€Œå¾®è§‚åˆ†é…å™¨åˆ™ç»†åŒ–æ¯ä¸ªåŒºåŸŸå†…çš„ä»»åŠ¡åˆ°æœåŠ¡å™¨çš„åˆ†é…ï¼Œä»¥å‡å°‘å»¶è¿Ÿå’Œåˆ‡æ¢æˆæœ¬ã€‚åœ¨å¤šç§ç½‘ç»œæ‹“æ‰‘ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€å…ˆè¿›çš„åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼ŒTORTAå°†å¹³å‡æ¨ç†å“åº”æ—¶é—´å‡å°‘äº†é«˜è¾¾15%ï¼Œæé«˜äº†å¤§çº¦4-5%çš„è´Ÿè½½å¹³è¡¡ï¼Œå¹¶é™ä½äº†10-20%çš„æ€»è¿è¥æˆæœ¬ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10259v2">PDF</a> 17 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡çš„å¿«é€Ÿå¢é•¿å¯¹åˆ†å¸ƒå¼GPUæ¨ç†åŸºç¡€è®¾æ–½æå‡ºäº†è¶Šæ¥è¶Šé«˜çš„è¦æ±‚ã€‚ç°æœ‰è°ƒåº¦ç³»ç»Ÿä¸»è¦éµå¾ªååº”èŒƒå¼ï¼Œä»…æ ¹æ®å½“å‰ç³»ç»ŸçŠ¶æ€è¿›è¡Œå†³ç­–ï¼Œä¸è€ƒè™‘ä»»åŠ¡éœ€æ±‚å’Œèµ„æºå¯ç”¨æ€§çš„æ—¶é—´æ¼”å˜ã€‚è¿™å¯¼è‡´GPUåˆ©ç”¨ç‡ä½ä¸‹ã€ä»»åŠ¡è¿ç§»å¼€é”€é«˜ä»¥åŠåœ¨åŠ¨æ€å·¥ä½œè´Ÿè½½ä¸‹çš„ç³»ç»Ÿå“åº”æ€§å·®ã€‚æœ¬ç ”ç©¶æå‡ºäº†æ—¶ç©ºæœ€ä¼˜èµ„æºè°ƒåº¦ä¸¤å±‚æ¶æ„ï¼ˆTORTAï¼‰ï¼Œè§£å†³äº†å³æ—¶çŠ¶æ€è°ƒåº¦æ–¹æ³•çš„æ ¹æœ¬å±€é™æ€§ã€‚TORTAé‡‡ç”¨æ—¶ç©ºè°ƒåº¦æ¡†æ¶ï¼Œæ—¢æ•æ‰é•¿æœŸå·¥ä½œè´Ÿè½½æ¨¡å¼åˆè€ƒè™‘çŸ­æœŸæ‰§è¡Œçº¦æŸã€‚å®ƒé‡‡ç”¨ä¸¤å±‚è®¾è®¡ï¼šå®è§‚è°ƒåº¦å™¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å’Œæœ€ä¼˜ä¼ è¾“åè°ƒè·¨åœ°åŒºä»»åŠ¡åˆ†å¸ƒï¼Œå¾®è§‚åˆ†é…å™¨åˆ™ç»†åŒ–åŒºåŸŸå†…ä»»åŠ¡åˆ°æœåŠ¡å™¨çš„åˆ†é…ä»¥é™ä½å»¶è¿Ÿå’Œåˆ‡æ¢æˆæœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸æœ€æ–°åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼ŒTORTAå¹³å‡æ¨ç†å“åº”æ—¶é—´å‡å°‘15%ï¼Œè´Ÿè½½å¹³è¡¡æå‡çº¦4-5%ï¼Œæ€»è¿è¥æˆæœ¬é™ä½10-20%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¢é•¿å¯¹åˆ†å¸ƒå¼GPUæ¨ç†åŸºç¡€è®¾æ–½çš„éœ€æ±‚å¢åŠ ã€‚</li>
<li>ç°æœ‰è°ƒåº¦ç³»ç»Ÿä¸»è¦åŸºäºååº”èŒƒå¼ï¼Œä»…è€ƒè™‘å³æ—¶ç³»ç»ŸçŠ¶æ€ï¼Œå­˜åœ¨æ—¶ç©ºæ„ŸçŸ¥ç¼ºå¤±ã€‚</li>
<li>æ—¶ç©ºç¼ºå¤±å¯¼è‡´GPUåˆ©ç”¨ç‡ä½ã€ä»»åŠ¡è¿ç§»å¼€é”€å¤§ä»¥åŠåŠ¨æ€å·¥ä½œè´Ÿè½½ä¸‹çš„ç³»ç»Ÿå“åº”æ€§å·®ã€‚</li>
<li>TORTAæ¡†æ¶æå‡ºä¸€ä¸ªæ—¶ç©ºè°ƒåº¦è§£å†³æ–¹æ¡ˆï¼Œç»“åˆé•¿æœŸå·¥ä½œè´Ÿè½½æ¨¡å¼å’ŒçŸ­æœŸæ‰§è¡Œçº¦æŸã€‚</li>
<li>TORTAé‡‡ç”¨ä¸¤å±‚è®¾è®¡ï¼Œå®è§‚è°ƒåº¦å™¨åˆ©ç”¨å¼ºåŒ–å­¦ä¹ å’Œæœ€ä¼˜ä¼ è¾“è¿›è¡ŒåŒºåŸŸé—´ä»»åŠ¡åè°ƒï¼Œå¾®è§‚åˆ†é…å™¨ä¼˜åŒ–åŒºåŸŸå†…ä»»åŠ¡åˆ†é…ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTORTAåœ¨å¹³å‡æ¨ç†å“åº”æ—¶é—´ã€è´Ÿè½½å¹³è¡¡å’Œè¿è¥æˆæœ¬æ–¹é¢ç›¸æ¯”ç°æœ‰æ–¹æ³•æœ‰æ˜æ˜¾æ”¹è¿›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10259">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d92e06602ab725f0df8eb2bc74a6654a" align="middle">
<img src="https://picx.zhimg.com/v2-bf16abfbbe3f8d7ac865d7dc4b2bd9a0" align="middle">
<img src="https://picx.zhimg.com/v2-9c964ee32b86726499da418ff331fb61" align="middle">
<img src="https://picx.zhimg.com/v2-d79f98f6572e4f0e58db7235008b9bc9" align="middle">
<img src="https://picx.zhimg.com/v2-86e6b9669dd266acde172d119f6c3aad" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Prompting-Obscures-Hallucination-Cues-in-Large-Language-Models-An-Empirical-Evaluation"><a href="#Chain-of-Thought-Prompting-Obscures-Hallucination-Cues-in-Large-Language-Models-An-Empirical-Evaluation" class="headerlink" title="Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models: An Empirical Evaluation"></a>Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models: An Empirical Evaluation</h2><p><strong>Authors:Jiahao Cheng, Tiancheng Su, Jia Yuan, Guoxiu He, Jiawei Liu, Xinqi Tao, Jingwen Xie, Huaxia Li</strong></p>
<p>Large Language Models (LLMs) often exhibit \textit{hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step reasoning, but its impact on hallucination detection remains underexplored. To bridge this gap, we conduct a systematic empirical evaluation. We begin with a pilot experiment, revealing that CoT reasoning significantly affects the LLMâ€™s internal states and token probability distributions. Building on this, we evaluate the impact of various CoT prompting methods on mainstream hallucination detection methods across both instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three key dimensions: changes in hallucination score distributions, variations in detection accuracy, and shifts in detection confidence. Our findings show that while CoT prompting helps reduce hallucination frequency, it also tends to obscure critical signals used for detection, impairing the effectiveness of various detection methods. Our study highlights an overlooked trade-off in the use of reasoning. Code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ECNU-Text-Computing/cot-hallu-detect">https://github.com/ECNU-Text-Computing/cot-hallu-detect</a> . </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¸¸å¸¸å‡ºç°â€œå¹»è§‰â€ï¼Œå³ç”Ÿæˆä¸æç¤ºå†…å®¹äº‹å®ä¸Šä¸æ­£ç¡®æˆ–è¯­ä¹‰ä¸Šä¸ç›¸å…³çš„å†…å®¹ã€‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯ä»¥é€šè¿‡é¼“åŠ±é€æ­¥æ¨ç†æ¥å‡è½»å¹»è§‰ï¼Œä½†å…¶å¯¹å¹»è§‰æ£€æµ‹çš„å½±å“ä»è¢«å¿½è§†ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†è¯•ç‚¹å®éªŒï¼Œæ­ç¤ºäº†CoTæ¨ç†å¯¹LLMçš„å†…éƒ¨çŠ¶æ€å’Œç¬¦å·æ¦‚ç‡åˆ†å¸ƒäº§ç”Ÿäº†æ˜¾è‘—å½±å“ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¯„ä¼°äº†å„ç§CoTæç¤ºæ–¹æ³•å¯¹ä¸»æµå¹»è§‰æ£€æµ‹æ–¹æ³•çš„ç›´æ¥å½±å“ï¼Œæ¶‰åŠæŒ‡ä»¤å¾®è°ƒè¯­è¨€å’Œé¢å‘æ¨ç†çš„LLMã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸‰ä¸ªå…³é”®ç»´åº¦ï¼šå¹»è§‰è¯„åˆ†åˆ†å¸ƒçš„å˜åŒ–ã€æ£€æµ‹å‡†ç¡®åº¦çš„å˜åŒ–ä»¥åŠæ£€æµ‹ä¿¡å¿ƒçš„è½¬å˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶CoTæç¤ºæœ‰åŠ©äºå‡å°‘å¹»è§‰é¢‘ç‡ï¼Œä½†å®ƒä¹Ÿæœ‰æ©ç›–ç”¨äºæ£€æµ‹çš„å…³é”®ä¿¡å·çš„è¶‹åŠ¿ï¼Œä»è€ŒæŸå®³äº†å„ç§æ£€æµ‹æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶çªå‡ºäº†åœ¨ä½¿ç”¨æ¨ç†æ—¶å¿½ç•¥çš„æƒè¡¡ã€‚ä»£ç å…¬å¼€åœ¨ï¼š<a target="_blank" rel="noopener" href="https://github.com/ECNU-Text-Computing/cot-hallu-detect%E3%80%82">https://github.com/ECNU-Text-Computing/cot-hallu-detectã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17088v3">PDF</a> Accepted at EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å®¹æ˜“å‡ºç°å¹»è§‰ï¼Œå³ç”Ÿæˆä¸æç¤ºå†…å®¹äº‹å®ä¸Šä¸æ­£ç¡®æˆ–è¯­ä¹‰ä¸Šä¸ç›¸å…³çš„å›åº”ã€‚é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯ä»¥é€šè¿‡é¼“åŠ±é€æ­¥æ¨ç†æ¥å‡è½»å¹»è§‰é—®é¢˜ï¼Œä½†å…¶å¯¹å¹»è§‰æ£€æµ‹çš„å½±å“å°šæœªå¾—åˆ°å……åˆ†ç ”ç©¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ç³»ç»Ÿçš„å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬é¦–å…ˆè¿›è¡Œäº†ä¸€é¡¹è¯•ç‚¹å®éªŒï¼Œå‘ç°CoTæ¨ç†æ˜¾è‘—å½±å“äº†LLMçš„å†…éƒ¨çŠ¶æ€å’Œä»¤ç‰Œæ¦‚ç‡åˆ†å¸ƒã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸åŒçš„CoTæç¤ºæ–¹æ³•å¯¹ä¸»æµå¹»è§‰æ£€æµ‹æ–¹æ³•çš„å½±å“ï¼Œè¿™äº›å½±å“æ¶‰åŠæŒ‡ä»¤å¾®è°ƒåŠé¢å‘æ¨ç†çš„LLMsã€‚æˆ‘ä»¬ç ”ç©¶äº†ä¸‰ä¸ªå…³é”®æ–¹é¢ï¼šå¹»è§‰åˆ†æ•°åˆ†å¸ƒçš„å˜åŒ–ã€æ£€æµ‹å‡†ç¡®åº¦çš„å˜åŒ–ä»¥åŠæ£€æµ‹ä¿¡å¿ƒçš„å˜åŒ–ã€‚ç ”ç©¶å‘ç°ï¼Œè™½ç„¶CoTæç¤ºæœ‰åŠ©äºå‡å°‘å¹»è§‰é¢‘ç‡ï¼Œä½†å®ƒä¹Ÿå€¾å‘äºæ©ç›–ç”¨äºæ£€æµ‹çš„å…³é”®ä¿¡å·ï¼Œä»è€ŒæŸå®³äº†å„ç§æ£€æµ‹æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ­ç¤ºäº†åœ¨ä½¿ç”¨æ¨ç†è¿‡ç¨‹ä¸­ä¸€ä¸ªè¢«å¿½è§†çš„æƒè¡¡é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¼šç”Ÿæˆäº‹å®ä¸æ­£ç¡®æˆ–è¯­ä¹‰ä¸ç›¸å…³çš„å†…å®¹ï¼Œå³å‡ºç°å¹»è§‰ã€‚</li>
<li>é“¾å¼æ€ç»´ï¼ˆCoTï¼‰æç¤ºå¯ä»¥é€šè¿‡é¼“åŠ±é€æ­¥æ¨ç†æ¥å‡è½»LLMsçš„å¹»è§‰é—®é¢˜ã€‚</li>
<li>CoTæ¨ç†å¯¹LLMçš„å†…éƒ¨çŠ¶æ€å’Œä»¤ç‰Œæ¦‚ç‡åˆ†å¸ƒæœ‰æ˜¾è‘—å½±å“ã€‚</li>
<li>CoTæç¤ºæ–¹æ³•å¯¹ä¸»æµå¹»è§‰æ£€æµ‹æ–¹æ³•çš„å½±å“æ¶‰åŠæŒ‡ä»¤å¾®è°ƒåŠé¢å‘æ¨ç†çš„LLMsã€‚</li>
<li>CoTæç¤ºæœ‰åŠ©äºå‡å°‘å¹»è§‰é¢‘ç‡ï¼Œä½†å¯èƒ½æ©ç›–ç”¨äºæ£€æµ‹çš„å…³é”®ä¿¡å·ï¼Œå½±å“æ£€æµ‹æ–¹æ³•å’Œå‡†ç¡®åº¦ã€‚</li>
<li>åœ¨ä½¿ç”¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨ä¸€ä¸ªè¢«å¿½è§†çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>ç ”ç©¶çš„ä»£ç å·²å…¬å¼€å¯ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17088">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2ddc8291f006dd7a6ddc406e3a25dd9" align="middle">
<img src="https://picx.zhimg.com/v2-22ebbabe3b8c025503fd860095031fd5" align="middle">
<img src="https://picx.zhimg.com/v2-0eeaacc7473f2aae59b377e7a9b14aca" align="middle">
<img src="https://picx.zhimg.com/v2-d9984b0e1f46bb600d49d3b4304c0e70" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-21/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-21/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-21/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-80bbbfebb11fb1925143ed37144a6297" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-21  TDRM Smooth Reward Models with Temporal Difference for LLM RL and   Inference
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-87c76983ba346ae6c3bd67373c445edb" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-09-20  Morph A Motion-free Physics Optimization Framework for Human Motion   Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
