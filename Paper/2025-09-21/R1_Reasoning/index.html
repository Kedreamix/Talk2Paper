<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning 方向最新论文已更新，请持续关注 Update in 2025-09-21  A Survey of Reinforcement Learning for Large Reasoning Models">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loader页面消失采用渐隐的方式*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo出现动画 */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//使用渐隐的方法淡出loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//强制显示loading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">嘘~  正在从服务器偷取页面 . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.10259v2/page_5_0.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-09-21
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-09-22
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    70 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 大语言模型的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2025-09-21-更新"><a href="#2025-09-21-更新" class="headerlink" title="2025-09-21 更新"></a>2025-09-21 更新</h1><h2 id="A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models"><a href="#A-Survey-of-Reinforcement-Learning-for-Large-Reasoning-Models" class="headerlink" title="A Survey of Reinforcement Learning for Large Reasoning Models"></a>A Survey of Reinforcement Learning for Large Reasoning Models</h2><p><strong>Authors:Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, Bowen Zhou</strong></p>
<p>In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: <a target="_blank" rel="noopener" href="https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs">https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs</a> </p>
<blockquote>
<p>本文综述了强化学习（RL）在大型语言模型（LLM）推理方面的最新进展。强化学习在推进LLM能力边界方面取得了显著的成功，特别是在处理数学和编码等复杂逻辑任务方面。因此，强化学习已经成为将LLM转化为LRM的基础方法。随着该领域的快速发展，RL在LRM上的进一步扩展现在不仅面临着计算资源的挑战，还面临着算法设计、训练数据和基础设施方面的挑战。因此，重新审视该领域的发展，重新评估其轨迹，探索提高RL在人工智能超级智能（ASI）方面的可扩展性的策略具有重要意义。尤其是，我们研究自DeepSeek-R1发布以来，将RL应用于LLM和LRM的推理能力的研究，包括基础组件、核心问题、训练资源和下游应用，以确定这一快速演变领域的未来机遇和方向。我们希望这次综述能够促进RL在更广泛的推理模型上的未来研究。Github：<a target="_blank" rel="noopener" href="https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs">https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.08827v2">PDF</a> Fixed typos; added missing and recent citations (114 -&gt; 117 pages)</p>
<p><strong>Summary</strong><br>强化学习在推动大型语言模型（LLM）的能力方面取得了显著进展，特别是在处理数学和编码等复杂逻辑任务方面。本论文回顾了强化学习在LLM和LRM（语言推理模型）中的应用进展，并探讨了其面临的基础挑战和未来的发展方向。通过深入了解其关键组件、核心问题、训练资源和下游应用，我们为这一快速演变的领域提供了未来的机遇和方向。希望本次综述能推动基于强化学习的更广泛推理模型的研究。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习在推动大型语言模型能力方面取得了显著进展，特别是在处理复杂逻辑任务方面。</li>
<li>强化学习已成为将大型语言模型转化为语言推理模型的基础方法。</li>
<li>强化学习在LLM和LRM领域的应用面临基础挑战，包括计算资源、算法设计、训练数据和基础设施等方面。</li>
<li>论文详细研究了强化学习应用于LLM和LRM的推理能力，特别是自DeepSeek-R1发布以来的研究。</li>
<li>论文强调了理解该领域的核心问题、训练资源和下游应用的重要性，为未来研究提供了方向。</li>
<li>当前领域发展迅速，具有广阔的未来机遇和研究方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.08827">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2509.08827v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2509.08827v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2509.08827v2/page_5_0.jpg" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="MovieCORE-COgnitive-REasoning-in-Movies"><a href="#MovieCORE-COgnitive-REasoning-in-Movies" class="headerlink" title="MovieCORE: COgnitive REasoning in Movies"></a>MovieCORE: COgnitive REasoning in Movies</h2><p><strong>Authors:Gueter Josmy Faure, Min-Hung Chen, Jia-Fong Yeh, Ying Cheng, Hung-Ting Su, Yung-Hao Tang, Shang-Hong Lai, Winston H. Hsu</strong></p>
<p>This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at <a target="_blank" rel="noopener" href="https://joslefaure.github.io/assets/html/moviecore.html">https://joslefaure.github.io/assets/html/moviecore.html</a>. </p>
<blockquote>
<p>本文介绍了MovieCORE，这是一个新型的视频问答（VQA）数据集，旨在探究对电影内容的更深层次认知理解。与现有主要集中在表面层次理解的数据集不同，MovieCORE侧重于提出需要系统2型思维的问题，同时针对视频材料进行深入探讨。我们提出了一种创新的代理头脑风暴方法，利用多个大型语言模型（LLM）作为思考代理，生成并优化高质量的问题-答案对。为了评估数据集的质量，我们开发了一系列认知测试，评估深度、思维激发潜力和句法复杂性。我们还提出了一个全面的评估方案，以评估VQA模型在更深层次认知任务上的性能。为了解决现有视频语言模型（VLM）的局限性，我们引入了代理增强模块（ACE），即在训练后通过高达25%的改进提高模型的推理能力。我们的研究为推进AI系统中的电影理解提供了贡献，并为当前VQA模型在面对关于电影内容的更具挑战性和细微的问题时提供了有价值的能力与局限性的见解。我们的项目页面、数据集和代码可在<a target="_blank" rel="noopener" href="https://joslefaure.github.io/assets/html/moviecore.html">https://joslefaure.github.io/assets/html/moviecore.html</a>找到。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.19026v3">PDF</a> Accepted for EMNLP’2025 Main Conference (Oral Presentation). Project   Page: <a target="_blank" rel="noopener" href="https://joslefaure.github.io/assets/html/moviecore.html">https://joslefaure.github.io/assets/html/moviecore.html</a></p>
<p><strong>Summary</strong><br>本文介绍了MovieCORE数据集，这是一个旨在深入了解电影内容的新型视频问答（VQA）数据集。不同于现有的关注表层理解的数据集，MovieCORE侧重于激发系统思考二级的问题，并针对视频材料特定设计。本文提出了利用多个大型语言模型作为思维代理的智能头脑风暴方法，生成并优化高质量的问题答案对。通过开发一系列认知测试来评估数据集质量，包括深度、思考启发潜力和句法复杂性。同时，为评估VQA模型在深层认知任务上的性能，提出了全面的评估方案。为解决现有视频语言模型的局限性，引入了增强代理选择增强（ACE）模块，该模块在训练后提高了模型的推理能力。本文工作为推进AI系统中的电影理解提供了有价值的见解，并展示了当前VQA模型在面对更具挑战性和细微性的问题时的能力和局限性。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>MovieCORE是一个旨在促进对电影内容的深层认知理解的新型视频问答（VQA）数据集。</li>
<li>与其他数据集相比，MovieCORE侧重于激发系统二级思考的问题。</li>
<li>采用智能头脑风暴方法，利用大型语言模型生成和优化问题答案对。</li>
<li>通过认知测试评估数据集质量，包括深度、启发潜力和句法复杂性。</li>
<li>提出了全面的评估方案，用于评估VQA模型在深层认知任务上的性能。</li>
<li>引入Agentic Choice Enhancement (ACE)模块，以提高视频语言模型的推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.19026">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.19026v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.19026v3/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.19026v3/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.19026v3/page_4_1.jpg" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="InMind-Evaluating-LLMs-in-Capturing-and-Applying-Individual-Human-Reasoning-Styles"><a href="#InMind-Evaluating-LLMs-in-Capturing-and-Applying-Individual-Human-Reasoning-Styles" class="headerlink" title="InMind: Evaluating LLMs in Capturing and Applying Individual Human   Reasoning Styles"></a>InMind: Evaluating LLMs in Capturing and Applying Individual Human   Reasoning Styles</h2><p><strong>Authors:Zizhen Li, Chuanhao Li, Yibin Wang, Qi Chen, Diping Song, Yukang Feng, Jianwen Sun, Jiaxin Ai, Fanrui Zhang, Mingzhu Sun, Kaipeng Zhang</strong></p>
<p>LLMs have shown strong performance on human-centric reasoning tasks. While previous evaluations have explored whether LLMs can infer intentions or detect deception, they often overlook the individualized reasoning styles that influence how people interpret and act in social contexts. Social deduction games (SDGs) provide a natural testbed for evaluating individualized reasoning styles, where different players may adopt diverse but contextually valid reasoning strategies under identical conditions. To address this, we introduce InMind, a cognitively grounded evaluation framework designed to assess whether LLMs can capture and apply personalized reasoning styles in SDGs. InMind enhances structured gameplay data with round-level strategy traces and post-game reflections, collected under both Observer and Participant modes. It supports four cognitively motivated tasks that jointly evaluate both static alignment and dynamic adaptation. As a case study, we apply InMind to the game Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o frequently rely on lexical cues, struggling to anchor reflections in temporal gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These findings reveal key limitations in current LLMs’ capacity for individualized, adaptive reasoning, and position InMind as a step toward cognitively aligned human-AI interaction. </p>
<blockquote>
<p>大型语言模型（LLM）在人类为中心的推理任务中表现出了强大的性能。虽然之前的评估已经探讨了LLM是否能够推断意图或检测欺骗，但它们经常忽略影响人们在社会环境中解释和行为的个性化推理风格。社会推理游戏（SDG）为评估个性化推理风格提供了天然的测试床，在不同条件下，不同的玩家可能会采用多样但语境有效的推理策略。为了解决这一问题，我们引入了InMind，这是一个基于认知的评估框架，旨在评估LLM是否能捕捉和应用个性化推理风格于SDG中。InMind通过观察者模式和参与者模式下的回合策略轨迹和游戏后反思，增强了结构化游戏玩法数据。它支持四项认知驱动的任务，共同评估静态对齐和动态适应。作为案例研究，我们将InMind应用于Avalon游戏，评估了1. 评估了最新颖的11个大型语言模型（LLM）。通用的大型语言模型（LLM），甚至GPT-4也经常依赖于词汇线索，难以将反思锚定在暂时的游戏中或适应不断变化的策略。相比之下，像DeepSeek-R1这样的增强推理的大型语言模型则显示出早期风格敏感推理的迹象。这些发现揭示了当前大型语言模型在个性化适应性推理方面的关键局限性，并将InMind定位为朝着认知对齐的人机交互迈出的一步。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.16072v2">PDF</a> EMNLP 2025 MainConference</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在人类中心推理任务中表现优异，但在处理社会推理中的个性化推理风格方面存在局限性。为了评估LLMs是否能够在社会推理游戏中捕捉和应用个性化推理风格，我们引入了InMind评估框架。该框架结合了结构化游戏数据、回合级策略痕迹和游戏后反思，支持四种认知驱动的任务，以评估静态对齐和动态适应。以Avalon游戏为例，发现通用LLMs（如GPT-4o）常依赖词汇线索，难以在动态游戏中进行反思和适应变化策略。相比之下，具备推理能力的LLMs（如DeepSeek-R1）展现出早期个性化推理能力。这揭示了当前LLMs在个性化适应性推理方面的关键局限，而InMind为认知对齐的人机交互提供了方向。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLMs在人类中心推理任务上表现出色，但在处理个性化推理风格方面存在局限。</li>
<li>InMind评估框架用于评估LLMs在社交推理游戏中捕捉和应用个性化推理风格的能力。</li>
<li>InMind结合了结构化游戏数据、回合级策略痕迹和游戏后反思。</li>
<li>InMind支持四种认知驱动的任务，以评估LLMs的静态对齐和动态适应能力。</li>
<li>通用LLMs（如GPT-4o）依赖词汇线索，难以在动态环境中进行反思和适应变化策略。</li>
<li>具备推理能力的LLMs（如DeepSeek-R1）展现出个性化推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.16072">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.16072v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.16072v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.16072v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.16072v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Select-to-Know-An-Internal-External-Knowledge-Self-Selection-Framework-for-Domain-Specific-Question-Answering"><a href="#Select-to-Know-An-Internal-External-Knowledge-Self-Selection-Framework-for-Domain-Specific-Question-Answering" class="headerlink" title="Select to Know: An Internal-External Knowledge Self-Selection Framework   for Domain-Specific Question Answering"></a>Select to Know: An Internal-External Knowledge Self-Selection Framework   for Domain-Specific Question Answering</h2><p><strong>Authors:Bolei He, Xinran He, Run Shao, Shanfu Shu, Xianwei Xue, Mingquan Cheng, Haifeng Li, Zhenhua Ling</strong></p>
<p>Large Language Models (LLMs) perform well in general QA but often struggle in domain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces external knowledge but suffers from hallucinations and latency due to noisy retrievals. Continued pretraining internalizes domain knowledge but is costly and lacks cross-domain flexibility. We attribute this challenge to the long-tail distribution of domain knowledge, which leaves partial yet useful internal knowledge underutilized. We further argue that knowledge acquisition should be progressive, mirroring human learning: first understanding concepts, then applying them to complex reasoning. To address this, we propose Selct2Know (S2K), a cost-effective framework that internalizes domain knowledge through an internal-external knowledge self-selection strategy and selective supervised fine-tuning. We also introduce a structured reasoning data generation pipeline and integrate GRPO to enhance reasoning ability. Experiments on medical, legal, and financial QA benchmarks show that S2K consistently outperforms existing methods and matches domain-pretrained LLMs with significantly lower cost. </p>
<blockquote>
<p>大型语言模型（LLM）在一般问答中表现良好，但在特定领域的场景中常常遇到困难。检索增强生成（RAG）引入了外部知识，但由于检索结果中的噪声而产生了虚构和延迟问题。持续预训练可以内化领域知识，但成本高昂且缺乏跨领域灵活性。我们将这一挑战归因于领域知识存在的长尾分布现象，导致部分有用的内部知识未被充分利用。我们进一步认为，知识获取应该是一个循序渐进的过程，模仿人类学习过程：首先理解概念，然后将其应用于复杂推理。为解决这一问题，我们提出了Selct2Know（S2K）框架，这是一个通过内外知识自选策略和选择性监督微调来内化领域知识的成本效益高的框架。我们还引入了一个结构化推理数据生成管道，并整合了GRPO以增强推理能力。在医疗、法律和财务问答基准测试上的实验表明，S2K始终优于现有方法，并以更低的成本匹配领域预训练的LLM。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.15213v2">PDF</a> EMNLP2025 Findings</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）在通用问答任务中表现良好，但在特定领域场景中存在挑战。本文提出了Selct2Know（S2K）框架，通过内外知识自我选择策略和选择性监督微调来内化领域知识，并引入结构化推理数据生成管道和GRPO增强推理能力。实验表明，S2K在医疗、法律和财务问答基准测试中表现优异，具有成本效益。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）在特定领域问答任务中存在挑战。</li>
<li>Retrieval-Augmented Generation（RAG）方法引入外部知识，但存在幻觉和延迟问题。</li>
<li>单纯继续预训练内化领域知识成本高昂，且缺乏跨领域灵活性。</li>
<li>知识获取应该像人类学习一样，先理解概念，再应用于复杂推理。</li>
<li>Selct2Know（S2K）框架通过内外知识自我选择策略和选择性监督微调来解决上述问题。</li>
<li>S2K框架引入结构化推理数据生成管道和GRPO增强技术来提升推理能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.15213">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.15213v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.15213v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.15213v2/page_2_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.15213v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.15213v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.15213v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space"><a href="#MolmoAct-Action-Reasoning-Models-that-can-Reason-in-Space" class="headerlink" title="MolmoAct: Action Reasoning Models that can Reason in Space"></a>MolmoAct: Action Reasoning Models that can Reason in Space</h2><p><strong>Authors:Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, Ranjay Krishna</strong></p>
<p>Reasoning is central to purposeful action, yet most robotic foundation models map perception and instructions directly to control, which limits adaptability, generalization, and semantic grounding. We introduce Action Reasoning Models (ARMs), a class of robotic foundation models that integrate perception, planning, and control through a structured three-stage pipeline. Our model, MolmoAct, encodes observations and instructions into depth-aware perception tokens, generates mid-level spatial plans as editable trajectory traces, and predicts precise low-level actions, enabling explainable and steerable behavior. MolmoAct-7B-D achieves strong performance across simulation and real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching tasks, surpassing closed-source Pi-0 and GR00T N1.5; 86.6% average success on LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks; and in real-world fine-tuning, an additional 10% (single-arm) and an additional 22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines by an additional 23.3% on out-of-distribution generalization and achieves top human-preference scores for open-ended instruction following and trajectory steering. Furthermore, we release, for the first time, the MolmoAct Dataset – a mid-training robot dataset comprising over 10,000 high quality robot trajectories across diverse scenarios and tasks. Training with this dataset yields an average 5.5% improvement in general performance over the base model. We release all model weights, training code, our collected dataset, and our action reasoning dataset, establishing MolmoAct as both a state-of-the-art robotics foundation model and an open blueprint for building ARMs that transform perception into purposeful action through structured reasoning. Blogpost: <a target="_blank" rel="noopener" href="https://allenai.org/blog/molmoact">https://allenai.org/blog/molmoact</a> </p>
<blockquote>
<p>推理是目标行动的核心，然而大多数机器人基础模型直接将感知和指令映射到控制上，这限制了其适应性、通用性和语义基础。我们引入了行动推理模型（ARMs），这是一类机器人基础模型，通过结构化的三阶段管道将感知、规划和控制结合在一起。我们的模型MolmoAct将观察和指令编码为深度感知标记，生成可编辑的轨迹轨迹作为中级空间计划，并预测精确的低级行动，从而实现可解释和可引导的行为。MolmoAct-7B-D在模拟和真实环境设置中表现出强大的性能：在SimplerEnv视觉匹配任务上达到70.5%的零射击准确率，超越封闭式Pi-0和GR00TN1.5；在LIBERO上的平均成功率达到86.6%，其中包括在长远任务上相对于ThinkAct的额外6.3%的收益；在真实世界的微调中，相对于Pi-0-FAST，单臂任务有额外的10%收益，双手协同任务有额外的22.7%收益。此外，它还以额外的23.3%的优势超越基线模型在超出分布泛化上的表现，并在开放式指令遵循和轨迹引导方面达到人类偏好度最高分数。此外，我们首次发布MolmoAct数据集——这是一个中期训练机器人数据集，包含各种场景和任务中的超过1万条高质量机器人轨迹。使用该数据集进行训练相对于基础模型在总体上提高了平均5.5%的性能。我们发布所有模型权重、训练代码、收集的数据集以及我们的行动推理数据集，确立了MolmoAct作为最先进的机器人基础模型，并为构建通过结构化推理将感知转化为有目的行动的ARMs提供了开放蓝图。博客文章：<a target="_blank" rel="noopener" href="https://allenai.org/blog/molmoact">https://allenai.org/blog/molmoact</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.07917v4">PDF</a> Updated GR00T result to N1.5</p>
<p><strong>Summary</strong></p>
<p>基于文中描述，机器人领域存在一个问题：大部分机器人基础模型将感知和指令直接映射到控制上，限制了适应性、泛化能力和语义接地能力。为了解决这个问题，本文提出了一种新的机器人基础模型——行动推理模型（ARMs），其中MolmoAct模型具有深度感知标记、可编辑轨迹跟踪以及精确低级动作生成等特性。MolmoAct模型能够在模拟和真实环境中实现出色的性能，并且具有可解释性和可引导性行为。此外，本文还首次发布了MolmoAct数据集，该数据集包含超过一万条高质量的机器人轨迹数据，用于训练机器人模型。总体来说，MolmoAct是一个先进的机器人基础模型，通过结构化推理将感知转化为有目的性的行动。有关更多信息，请参阅<a target="_blank" rel="noopener" href="https://allenai.org/blog/molmoact%E3%80%82">https://allenai.org/blog/molmoact。</a></p>
<p><strong>Key Takeaways</strong></p>
<p>以下是关键见解：</p>
<p>一、行动推理模型（ARMs）引入机器人基础模型中以解决传统模型的适应性、泛化及语义接地问题。<br>二、MolmoAct模型通过深度感知标记、可编辑轨迹跟踪和精确低级动作生成等技术实现优秀的性能。<br>三、MolmoAct模型在模拟和真实环境中表现良好，具有可解释性和可引导性行为。<br>四、首次发布MolmoAct数据集，包含超过一万条高质量的机器人轨迹数据，用于训练机器人模型，提高模型性能。<br>五、MolmoAct数据集为构建更先进的ARMs提供了开放蓝图，使机器人能够更好地通过结构化推理将感知转化为有目的性的行动。<br>六、MolmoAct在多个任务中实现了突破性的性能，如SimplerEnv视觉匹配任务、LIBERO任务等。相较于其他基准测试模型，其表现尤为出色。</p>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.07917">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.07917v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.07917v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.07917v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.07917v4/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.07917v4/page_5_0.jpg" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="UR-2-Unify-RAG-and-Reasoning-through-Reinforcement-Learning"><a href="#UR-2-Unify-RAG-and-Reasoning-through-Reinforcement-Learning" class="headerlink" title="UR$^2$: Unify RAG and Reasoning through Reinforcement Learning"></a>UR$^2$: Unify RAG and Reasoning through Reinforcement Learning</h2><p><strong>Authors:Weitao Li, Boran Xiang, Xiaolong Wang, Zhinan Gou, Weizhi Ma, Yang Liu</strong></p>
<p>Large Language Models (LLMs) have shown remarkable capabilities through two complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR), which optimizes complex reasoning abilities. However, these two capabilities are often developed in isolation, and existing efforts to unify them remain narrow in scope – typically limited to open-domain QA with fixed retrieval settings and task-specific constraints. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a general framework that unifies retrieval and reasoning through reinforcement learning. UR2 introduces two key contributions: a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries. These components are designed to enable dynamic coordination between retrieval and reasoning, improving adaptability across a diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical, and mathematical reasoning tasks demonstrate that UR$^2$ (built on Qwen-2.5-3&#x2F;7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks. We have released all code, models, and data at <a target="_blank" rel="noopener" href="https://github.com/Tsinghua-dhy/UR2">https://github.com/Tsinghua-dhy/UR2</a>. </p>
<blockquote>
<p>大型语言模型（LLM）通过两种互补的模式展现了显著的能力：增强知识基础的检索增强生成（RAG）和优化复杂推理能力的来自可验证奖励的强化学习（RLVR）。然而，这两种能力往往独立发展，现有的统一它们的尝试仍然局限于开放域问答的固定检索设置和任务特定约束。这种缺乏整合限制了RAG-RL方法在更广泛领域的应用。为了弥补这一差距，我们提出了UR²（统一RAG和推理），这是一个通过强化学习统一检索和推理的一般框架。UR²有两个关键贡献：一个难度感知的课程训练，有选择地为具有挑战性的问题调用检索功能；以及一种混合知识访问策略，结合领域特定的离线语料库和LLM生成的摘要。这些组件的设计旨在实现检索和推理之间的动态协调，提高在各种任务中的适应性。在开放域问答、MMLU-Pro、医疗和数学推理任务上的实验表明，UR²（基于Qwen-2.5-3&#x2F;7B和LLaMA-3.1-8B）显著优于现有的RAG和RL方法，并在一些基准测试中实现了与GPT-4o-mini和GPT-4.1-mini相当的性能。我们已在<a target="_blank" rel="noopener" href="https://github.com/Tsinghua-dhy/UR2%E5%8F%91%E5%B8%83%E6%89%80%E6%9C%89%E4%BB%A3%E7%A0%81%E3%80%81%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E7%9A%84%E3%80%82">https://github.com/Tsinghua-dhy/UR2发布所有代码、模型和数据的。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.06165v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLM）通过两种互补范式展现了显著的能力：增强知识定位的回溯增强生成（RAG）和优化复杂推理能力的可验证奖励强化学习（RLVR）。然而，这两种能力通常孤立发展，现有的统一它们的工作范围狭窄，仅限于开放域问答和固定检索设置的特定任务约束。为弥补这一差距，我们提出UR²（统一RAG和推理），这是一个通过强化学习统一检索和推理的通用框架。UR²引入了两个关键贡献：难度感知课程训练，选择性地对挑战性问题仅使用检索功能；混合知识访问策略，结合领域特定的离线语料库与LLM生成的摘要。这些组件旨在实现检索和推理之间的动态协调，提高在各种任务中的适应性。实验表明，UR²（基于Qwen-2.5-3&#x2F;7B和LLaMA-3.1-8B）显著优于现有的RAG和RL方法，并在多个基准测试上实现了与GPT-4o-mini和GPT-4.1-mini相当的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）通过回溯增强生成（RAG）和可验证奖励强化学习（RLVR）展现出强大的能力。</li>
<li>现有方法往往孤立发展这两种能力，限制了其通用性和应用范围。</li>
<li>UR²框架旨在通过强化学习统一检索和推理，提高模型的适应性。</li>
<li>UR²引入难度感知课程训练和混合知识访问策略，实现检索和推理之间的动态协调。</li>
<li>UR²显著优于现有的RAG和RL方法，并在多个基准测试上实现了与高级模型相当的性能。</li>
<li>UR²框架已发布所有代码、模型和数据的开源地址。</li>
<li>UR²框架有望促进大型语言模型在各个领域的应用和发展。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.06165">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.06165v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.06165v2/page_2_0.jpg" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Uni-cot-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision"><a href="#Uni-cot-Towards-Unified-Chain-of-Thought-Reasoning-Across-Text-and-Vision" class="headerlink" title="Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and   Vision"></a>Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and   Vision</h2><p><strong>Authors:Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, Hao Li</strong></p>
<p>Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large Language Models (LLMs) by decomposing complex tasks into simpler, sequential subtasks. However, extending CoT to vision-language reasoning tasks remains challenging, as it often requires interpreting transitions of visual states to support reasoning. Existing methods often struggle with this due to limited capacity of modeling visual state transitions or incoherent visual trajectories caused by fragmented architectures.   To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought framework that enables coherent and grounded multimodal reasoning within a single unified model. The key idea is to leverage a model capable of both image understanding and generation to reason over visual content and model evolving visual states. However, empowering a unified model to achieve that is non-trivial, given the high computational cost and the burden of training. To address this, Uni-CoT introduces a novel two-level reasoning paradigm: A Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask execution. This design significantly reduces the computational overhead. Furthermore, we introduce a structured training paradigm that combines interleaved image-text supervision for macro-level CoT with multi-task objectives for micro-level CoT. Together, these innovations allow Uni-CoT to perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our design, all experiments can be efficiently completed using only 8 A100 GPUs with 80GB VRAM each. Experimental results on reasoning-driven image generation benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT demonstrates SOTA performance and strong generalization, establishing Uni-CoT as a promising solution for multi-modal reasoning. Project Page and Code: <a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/">https://sais-fuxi.github.io/projects/uni-cot/</a> </p>
<blockquote>
<p>链式思维（CoT）推理被广泛应用于通过分解复杂任务为更简单、连续的子任务来提升大型语言模型（LLM）的性能。然而，将CoT扩展到视觉语言推理任务仍然具有挑战性，因为它通常需要解释视觉状态的过渡以支持推理。现有方法常常在这方面遇到困难，因为它们建模视觉状态过渡的能力有限，或者由于架构碎片化而导致视觉轨迹不一致。为了克服这些局限性，我们提出了Uni-CoT，一个统一的链式思维框架，它能在单个统一模型内进行连贯且基于地面的多模态推理。主要思想是利用能够理解和生成图像的模型来推理视觉内容并模拟不断演变的视觉状态。然而，在一个统一的模型中实现这一点并不简单，因为这会带来很高的计算成本以及训练负担。为了解决这个问题，Uni-CoT引入了一种新型的两级推理模式：用于高级任务规划的宏观层面CoT和用于子任务执行的微观层面CoT。这种设计显著减少了计算开销。此外，我们引入了一种结构化的训练模式，该模式结合了宏观层面CoT的交错图像文本监督以及微观层面CoT的多任务目标。这些创新使Uni-CoT能够进行可扩展且连贯的多模态推理。此外，得益于我们的设计，所有实验都只需使用8个带有80GB VRAM的A100 GPU即可高效完成。在基于推理的图像生成基准测试（WISE）和编辑基准测试（RISE和KRIS）上的实验结果表明，Uni-CoT展现出卓越的性能和强大的泛化能力，成为多模态推理领域的一个有前途的解决方案。项目页面和代码：<a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/">https://sais-fuxi.github.io/projects/uni-cot/</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05606v2">PDF</a> Project Page: <a target="_blank" rel="noopener" href="https://sais-fuxi.github.io/projects/uni-cot/">https://sais-fuxi.github.io/projects/uni-cot/</a></p>
<p><strong>Summary</strong></p>
<p>本文介绍了Chain-of-Thought（CoT）在大型语言模型（LLM）中的广泛应用，并指出其在视觉语言推理任务中的挑战。为克服这些挑战，提出了一种名为Uni-CoT的统一思维链框架，能够在单一模型中实现连贯的多模态推理。其核心思想是利用既能理解图像又能生成图像的模型，对视觉内容和不断变化的视觉状态进行推理。为降低计算成本和训练负担，Uni-CoT引入了两级推理模式，即用于高级任务规划的宏观层次CoT和用于子任务执行的微观层次CoT。此外，还提出了一种结构化的训练模式，将宏观层次的CoT与微观层次的CoT多任务目标相结合。这些创新使Uni-CoT能够进行可扩展和连贯的多模态推理，并在图像生成和编辑基准测试中表现出卓越的性能。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Uni-CoT框架是一种将视觉和语言结合的多模态推理方法，能够在单一模型中实现连贯的推理。</li>
<li>它利用既能理解图像又能生成图像的模型进行视觉状态推理。</li>
<li>Uni-CoT引入了两级推理模式，包括宏观层次的CoT用于任务规划，微观层次的CoT用于子任务执行。</li>
<li>结构化的训练模式结合了宏观和微观层次的CoT目标，提高了模型的性能。</li>
<li>Uni-CoT具有可扩展性，可以在有限的计算资源（仅使用8个A100 GPU）上进行大规模运算。</li>
<li>在图像生成和编辑基准测试中，Uni-CoT表现出卓越的性能，具有强大的泛化能力。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05606">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.05606v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.05606v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Posterior-GRPO-Rewarding-Reasoning-Processes-in-Code-Generation"><a href="#Posterior-GRPO-Rewarding-Reasoning-Processes-in-Code-Generation" class="headerlink" title="Posterior-GRPO: Rewarding Reasoning Processes in Code Generation"></a>Posterior-GRPO: Rewarding Reasoning Processes in Code Generation</h2><p><strong>Authors:Lishui Fan, Yu Zhang, Mouxiang Chen, Zhongxin Liu</strong></p>
<p>Reinforcement learning (RL) has significantly advanced code generation for large language models (LLMs). However, current paradigms rely on outcome-based rewards from test cases, neglecting the quality of the intermediate reasoning process. While supervising the reasoning process directly is a promising direction, it is highly susceptible to reward hacking, where the policy model learns to exploit the reasoning reward signal without improving final outcomes. To address this, we introduce a unified framework that can effectively incorporate the quality of the reasoning process during RL. First, to enable reasoning evaluation, we develop LCB-RB, a benchmark comprising preference pairs of superior and inferior reasoning processes. Second, to accurately score reasoning quality, we introduce an Optimized-Degraded based (OD-based) method for reward model training. This method generates high-quality preference pairs by systematically optimizing and degrading initial reasoning paths along curated dimensions of reasoning quality, such as factual accuracy, logical rigor, and coherence. A 7B parameter reward model with this method achieves state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method that conditions process-based rewards on task success. By selectively applying rewards to the reasoning processes of only successful outcomes, P-GRPO effectively mitigates reward hacking and aligns the model’s internal reasoning with final code correctness. A 7B parameter model with P-GRPO achieves superior performance across diverse code generation tasks, outperforming outcome-only baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further demonstrate the generalizability of our approach by extending it to mathematical tasks. Our models, dataset, and code are publicly available. </p>
<blockquote>
<p>强化学习（RL）在大型语言模型（LLM）的代码生成方面取得了重大进展。然而，当前的模式依赖于测试用例的结果导向奖励，忽视了中间推理过程的质量。虽然直接监督推理过程是一个有前景的方向，但它很容易受到奖励作弊的影响，即政策模型学会利用推理奖励信号而不改善最终结果。为解决此问题，我们引入了一个统一的框架，可以有效地在RL中融入推理过程的质量。首先，为了进行推理评估，我们开发了LCB-RB基准测试，它包括优质和劣质推理过程的偏好对。其次，为了准确评估推理质量，我们引入了基于优化和降级（OD-based）的方法来进行奖励模型训练。该方法通过系统地优化和降级初始推理路径，沿推理质量的精选维度生成高质量偏好对，如事实准确性、逻辑严谨性和连贯性。使用这种方法，一个拥有7B参数的奖励模型在LCB-RB上达到了最先进的性能，并在其他基准测试中具有良好的泛化能力。最后，我们介绍了后置GRPO（P-GRPO），这是一种新型的RL方法，基于任务成功来设定过程奖励。通过有选择地对成功结果的推理过程施加奖励，P-GRPO有效地减轻了奖励作弊问题，并使模型的内部推理与最终的代码正确性相符。使用P-GRPO的7B参数模型在多种代码生成任务中表现优异，相较于只关注结果的基线提高了4.5%，并实现了与GPT-4-Turbo相当的性能。我们还通过将其扩展到数学任务来展示了我们方法的泛化性。我们的模型、数据集和代码已公开可用。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.05170v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>在强化学习（RL）中，过程奖励对于提升大型语言模型（LLM）的代码生成能力至关重要。当前的方法主要依赖于测试案例的结果奖励，忽视了中间推理过程的质量。直接监督推理过程是一个有前景的方向，但它容易受奖励机制滥用的影响，使得策略模型学习如何利用推理奖励信号而不提高最终结果。为了解决这一问题，本文提出一个统一框架，旨在有效地结合推理过程的质量进行RL。首先，为了进行推理评估，我们开发了LCB-RB基准测试，包含优质和劣质推理过程的偏好对。其次，为了准确评估推理质量，我们引入基于优化与降级（OD-based）的方法训练奖励模型。这种方法通过系统地优化和降级初始推理路径的特定维度，如事实准确性、逻辑严谨性和连贯性来生成高质量偏好对。最后，我们引入基于任务成功结果的条件奖励方法——后置GRPO（P-GRPO）。P-GRPO有选择地将奖励应用于成功结果的推理过程，有效减轻了奖励滥用问题并使模型的内部推理与最终代码的正确性相符。使用P-GRPO的7B参数模型在多种代码生成任务上表现优异，相较于仅依赖结果的基线提高了4.5%，性能与GPT-4-Turbo相当。我们的方法还适用于数学任务，相关模型和代码已公开可用。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>强化学习在代码生成中引入了过程奖励来提升大型语言模型性能。</li>
<li>当前方法主要关注结果奖励，忽视了推理过程的质量。</li>
<li>提出一个统一框架来结合推理过程的质量，开发LCB-RB基准测试和基于优化与降级的奖励模型评估方法。</li>
<li>引入Posterior-GRPO（P-GRPO）方法，将奖励应用于成功结果的推理过程，减轻奖励滥用问题。</li>
<li>使用P-GRPO的模型在代码生成任务上表现优异，性能与GPT-4-Turbo相当。</li>
<li>方法具有通用性，可应用于数学任务。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.05170">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.05170v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.05170v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.05170v2/page_3_1.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.05170v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Mitigating-Attention-Hacking-in-Preference-Based-Reward-Modeling-via-Interaction-Distillation"><a href="#Mitigating-Attention-Hacking-in-Preference-Based-Reward-Modeling-via-Interaction-Distillation" class="headerlink" title="Mitigating Attention Hacking in Preference-Based Reward Modeling via   Interaction Distillation"></a>Mitigating Attention Hacking in Preference-Based Reward Modeling via   Interaction Distillation</h2><p><strong>Authors:Jianxiang Zang, Meiling Ning, Shihan Dou, Jiazheng Zhang, Tao Gui, Qi Zhang, Xuanjing Huang</strong></p>
<p>The reward model (RM), as the core component of reinforcement learning from human feedback (RLHF) for large language models (LLMs), responsible for providing reward signals to generated responses. However, mainstream preference modeling in RM is inadequate in terms of token-level interaction, making its judgment signals vulnerable to being hacked by misallocated attention to context. This stems from two fundamental limitations: (1) Current preference modeling employs decoder-only architectures, where the unidirectional causal attention mechanism leads to forward-decaying intra-sequence attention within the prompt-response sequence. (2) The independent Siamese-encoding paradigm induces the absence of token-level inter-sequence attention between chosen and rejected sequences. To address this “attention hacking”, we propose “Interaction Distillation”, a novel training framework for more adequate preference modeling through attention-level optimization. The method introduces an interaction-based natural language understanding model as the teacher to provide sophisticated token interaction patterns via comprehensive attention, and guides the preference modeling to simulate teacher model’s interaction pattern through an attentional alignment objective. Through extensive experiments, interaction distillation has demonstrated its ability to provide more stable and generalizable reward signals compared to state-of-the-art RM optimization methods that target data noise, highlighting the attention hacking constitute a more fundamental limitation in RM. </p>
<blockquote>
<p>奖励模型（RM）作为大型语言模型（LLM）中人类反馈强化学习（RLHF）的核心组件，负责为生成的响应提供奖励信号。然而，RM中的主流偏好建模在令牌级交互方面存在不足，使得其判断信号容易受到对上下文错配注意力的攻击。这源于两个基本局限：（1）当前的偏好建模仅采用解码器架构，其中单向因果注意力机制导致提示-响应序列内的序列内注意力呈现前向衰减。（2）独立的Siamese编码范式导致所选序列和拒绝序列之间缺乏令牌级序列间注意力。为了解决这种“注意力攻击”，我们提出了“交互蒸馏”，这是一种通过注意力层面优化来进行更充分偏好建模的新型训练框架。该方法引入了一种基于交互的自然语言理解模型作为教师，通过全面的注意力提供精细的令牌交互模式，并通过注意力对齐目标指导偏好建模来模拟教师模型的交互模式。通过广泛的实验，交互蒸馏显示出其提供比针对数据噪声的最先进RM优化方法更稳定和可推广的奖励信号的能力，强调了注意力攻击在RM中构成更基本的局限。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.02618v2">PDF</a> This paper is not suitable for this topic, we need to adjust the   context</p>
<p><strong>Summary</strong>：<br>强化学习中的奖励模型（RM）是核心组件，负责为大型语言模型（LLM）的生成响应提供奖励信号。然而，主流的偏好建模在RM中存在对token级别交互的不足，导致判断信号容易受到错误分配的注意力对上下文的影响。这源于两个基本局限：一是当前偏好建模采用单向因果注意力机制的解码器架构，导致提示响应序列内的序列内注意力衰减；二是独立Siamese编码范式导致所选序列和拒绝序列之间的token级别序列间注意力缺失。为解决这一问题，我们提出了“交互蒸馏”这一新型训练框架，通过注意力层面的优化进行更准确的偏好建模。“交互蒸馏”引入基于交互的自然语言理解模型作为教师，通过全面的注意力提供复杂的token交互模式，并引导偏好建模模拟教师模型的交互模式通过注意力对齐目标实现。通过实验证明，相较于针对数据噪声优化的其他RM方法，“交互蒸馏”能够提供更稳定和可泛化的奖励信号。这表明注意力黑客攻击在RM中是一个更基本的局限性。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>强化学习中的奖励模型（RM）是核心组件，负责为大型语言模型提供奖励信号。</li>
<li>当前主流RM在token级别交互方面的不足导致其容易受到错误的注意力影响，进而导致对上下文的判断存在误差。</li>
<li>偏好建模存在两个基本局限：采用单向因果注意力机制的解码器架构和独立Siamese编码范式导致token级别交互不足。</li>
<li>为解决这些问题，提出了“交互蒸馏”这一新型训练框架。这一方法引入了基于交互的自然语言理解模型作为教师来指导偏好建模。</li>
<li>“交互蒸馏”通过全面的注意力提供复杂的token交互模式，并引导偏好建模模拟教师模型的交互模式通过注意力对齐目标实现。这种方法有助于提供更稳定和可泛化的奖励信号。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.02618">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.02618v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.02618v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.02618v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.02618v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.02618v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2508.02618v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DualSG-A-Dual-Stream-Explicit-Semantic-Guided-Multivariate-Time-Series-Forecasting-Framework"><a href="#DualSG-A-Dual-Stream-Explicit-Semantic-Guided-Multivariate-Time-Series-Forecasting-Framework" class="headerlink" title="DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series   Forecasting Framework"></a>DualSG: A Dual-Stream Explicit Semantic-Guided Multivariate Time Series   Forecasting Framework</h2><p><strong>Authors:Kuiye Ding, Fanda Fan, Yao Wang, Ruijie jian, Xiaorui Wang, Luqi Gong, Yishan Jiang, Chunjie Luo, Jianfeng Zhan</strong></p>
<p>Multivariate Time Series Forecasting plays a key role in many applications. Recent works have explored using Large Language Models for MTSF to take advantage of their reasoning abilities. However, many methods treat LLMs as end-to-end forecasters, which often leads to a loss of numerical precision and forces LLMs to handle patterns beyond their intended design. Alternatively, methods that attempt to align textual and time series modalities within latent space frequently encounter alignment difficulty. In this paper, we propose to treat LLMs not as standalone forecasters, but as semantic guidance modules within a dual-stream framework. We propose DualSG, a dual-stream framework that provides explicit semantic guidance, where LLMs act as Semantic Guides to refine rather than replace traditional predictions. As part of DualSG, we introduce Time Series Caption, an explicit prompt format that summarizes trend patterns in natural language and provides interpretable context for LLMs, rather than relying on implicit alignment between text and time series in the latent space. We also design a caption-guided fusion module that explicitly models inter-variable relationships while reducing noise and computation. Experiments on real-world datasets from diverse domains show that DualSG consistently outperforms 15 state-of-the-art baselines, demonstrating the value of explicitly combining numerical forecasting with semantic guidance. </p>
<blockquote>
<p>多元时间序列预测在众多应用中发挥着关键作用。近期的研究工作探索了使用大型语言模型进行MTSF，以利用其推理能力。然而，许多方法将大型语言模型视为端到端的预测器，这往往会导致数值精度的损失，并迫使大型语言模型处理超出其预期设计的模式。另一方面，尝试在潜在空间内对齐文本和时间序列模态的方法经常遇到对齐困难的问题。在本文中，我们提出不应将大型语言模型视为独立的预测器，而是应作为双流框架内的语义指导模块。我们提出了DualSG，这是一个双流框架，提供明确的语义指导，其中大型语言模型充当语义指南，以改进而不是替代传统预测。作为DualSG的一部分，我们引入了时间序列字幕，这是一种明确的提示格式，可以总结自然语言中的趋势模式，并为大型语言模型提供可解释的背景，而不是依赖潜在空间中文本和序列之间的隐含对齐。我们还设计了一个字幕引导融合模块，该模块可以明确地建模变量之间的关系，同时减少噪声和计算量。在来自不同领域的真实数据集上的实验表明，DualSG持续优于15个最新基线，证明了显式结合数值预测和语义指导的价值。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.21830v4">PDF</a> This paper has been accepted by ACM Multimedia 2025 (ACM MM 2025)</p>
<p><strong>Summary</strong>：利用多变量时间序列预测在自然语言处理和时序数据预测中的重要性日益显著。本研究提出一种双流框架DualSG，将大型语言模型作为语义指导模块，用于提高预测的精度和可解释性。通过引入时间序列字幕，该框架能够明确描述趋势模式并提供可理解的上下文环境。实验证明，DualSG在真实数据集上的表现优于其他先进方法。</p>
<p><strong>Key Takeaways</strong>：</p>
<ol>
<li>大型语言模型在多变量时间序列预测中扮演重要角色。</li>
<li>当前方法常常将大型语言模型作为独立预测器，可能导致数值精度损失和处理超出设计范围的图案。</li>
<li>研究提出了一种双流框架DualSG，将大型语言模型作为语义指导模块，而非独立预测器。</li>
<li>引入时间序列字幕，明确描述趋势模式并提供可理解的上下文环境给大型语言模型。</li>
<li>设计了字幕引导融合模块，明确建模变量间关系，减少噪声和计算。</li>
<li>实验证明，DualSG在真实数据集上的表现优于其他先进方法，表明结合数值预测和语义指导的价值。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.21830">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.21830v4/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.21830v4/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.21830v4/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.21830v4/page_5_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.21830v4/page_5_1.jpg" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LoRA-PAR-A-Flexible-Dual-System-LoRA-Partitioning-Approach-to-Efficient-LLM-Fine-Tuning"><a href="#LoRA-PAR-A-Flexible-Dual-System-LoRA-Partitioning-Approach-to-Efficient-LLM-Fine-Tuning" class="headerlink" title="LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient   LLM Fine-Tuning"></a>LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient   LLM Fine-Tuning</h2><p><strong>Authors:Yining Huang, Bin Li, Keke Tang, Meilian Chen</strong></p>
<p>Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit substantially from chain-of-thought (CoT) reasoning, yet pushing their performance typically requires vast data, large model sizes, and full-parameter fine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost, most existing approaches primarily address domain adaptation or layer-wise allocation rather than explicitly tailoring data and parameters to different response demands. Inspired by “Thinking, Fast and Slow,” which characterizes two distinct modes of thought-System 1 (fast, intuitive, often automatic) and System 2 (slower, more deliberative and analytic)-we draw an analogy that different “subregions” of an LLM’s parameters might similarly specialize for tasks that demand quick, intuitive responses versus those requiring multi-step logical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework that partitions both data and parameters by System 1 or System 2 demands, using fewer yet more focused parameters for each task. Specifically, we classify task data via multi-model role-playing and voting, and partition parameters based on importance scoring, then adopt a two-stage fine-tuning strategy of training System 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and intuition and refine System 2 tasks with reinforcement learning (RL) to reinforce deeper logical deliberation next. Extensive experiments show that the two-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while matching or surpassing SOTA PEFT baselines. </p>
<blockquote>
<p>大规模生成模型如DeepSeek-R1和OpenAI-O1在很大程度上受益于思维链（CoT）推理，然而提升它们的性能通常需要大量的数据、庞大的模型规模和全参数微调。虽然参数效率高的微调（PEFT）有助于降低成本，但大多数现有方法主要解决域适应或逐层分配问题，而不是显式地针对数据和参数进行不同的响应需求调整。受《思考，快与慢》的启发，该书描述了两种截然不同的思维模式——系统1（快速、直观、通常自动）和系统2（较慢、更审慎和分析性）——我们进行类比，大型语言模型（LLM）的“不同子区域”参数可能类似于针对需要快速直观反应的任务与需要多步骤逻辑推理的任务进行专业化处理。因此，我们提出了LoRA-PAR双系统LoRA框架，该框架根据系统1或系统2的需求对数据和参数进行分区，为每个任务使用更少但更专注的参数。具体来说，我们通过多模型角色扮演和投票对任务数据进行分类，根据重要性评分对参数进行分区，然后采用两阶段微调策略，用监督微调（SFT）训练系统1任务以增强知识和直觉，用强化学习（RL）对系统2任务进行微调以加强更深入的逻辑思考。大量实验表明，两阶段微调策略——SFT和RL，降低了活动参数的使用量，同时达到或超越了最新的PEFT基准测试。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.20999v3">PDF</a> 12 pages</p>
<p><strong>Summary</strong><br>     大规模生成模型如DeepSeek-R1和OpenAI-O1显著受益于链式思维（CoT）推理，但要提升其性能通常需依赖大量数据、庞大的模型规模和全面的参数微调。尽管参数效率微调（PEFT）有助于降低成本，但现有方法主要集中在领域适应或分层分配上，并未明确针对数据和参数适应不同响应需求。受“快速思考，慢速决策”的启发，我们将LLM参数的不同“子区域”类比为专门执行快速直觉反应与需要多步骤逻辑推理的任务。因此，我们提出了LoRA-PAR双系统框架，根据系统1和系统2的需求对数据和参数进行分区，为每个任务使用更少但更集中的参数。具体而言，我们通过多模型角色扮演和投票对任务数据进行分类，根据重要性评分对参数进行分区，然后采用两阶段微调策略，先用监督微调（SFT）训练系统1任务以增强知识和直觉，再用强化学习（RL）完善系统2任务以强化深度逻辑思考。实验表明，两阶段微调策略SFT和RL在降低活动参数使用的同时，达到了或超越了最新PEFT基准。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型生成模型受益于链式思维（CoT）推理。</li>
<li>参数效率微调（PEFT）对于降低模型成本至关重要。</li>
<li>现有PEFT方法主要关注领域适应和分层分配，缺乏针对数据和参数的灵活适应。</li>
<li>受“快速思考，慢速决策”启发，提出LLM参数的不同“子区域”可专门执行不同任务。</li>
<li>LoRA-PAR框架根据系统1和系统2的需求对数据和参数进行分区。</li>
<li>采用两阶段微调策略：监督微调（SFT）强化知识和直觉，强化学习（RL）强化深度逻辑思考。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.20999">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.20999v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.20999v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.20999v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.20999v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="NIRS-An-Ontology-for-Non-Invasive-Respiratory-Support-in-Acute-Care"><a href="#NIRS-An-Ontology-for-Non-Invasive-Respiratory-Support-in-Acute-Care" class="headerlink" title="NIRS: An Ontology for Non-Invasive Respiratory Support in Acute Care"></a>NIRS: An Ontology for Non-Invasive Respiratory Support in Acute Care</h2><p><strong>Authors:Md Fantacher Islam, Jarrod Mosier, Vignesh Subbian</strong></p>
<p>Objective: Managing patients with respiratory failure increasingly involves non-invasive respiratory support (NIRS) strategies as alternatives to traditional ventilation methods. However, despite the rapidly expanding use of NIRS, there is a significant challenge to its best use under all medical circumstances. It lacks a unified ontological structure, complicating guidance on NIRS modalities across healthcare systems. Our goal is to develop NIRS ontology to support knowledge representation in acute care settings by providing a unified framework that enhances data clarity, interoperability, and clinical decision-making.   Methods: We developed the NIRS ontology using Web Ontology Language (OWL) semantics and Protege to organize clinical concepts and relationships. To enable rule-based clinical reasoning beyond hierarchical structures, we added Semantic Web Rule Language (SWRL) rules. We evaluated logical reasoning by adding 17 hypothetical clinical scenarios. We used SPARQL queries to retrieve and test targeted inferences.   Results: The ontology has 129 classes, 11 object properties, and 17 data properties across 886 axioms that establish concept relationships. To standardize clinical concepts, we added 361 annotations, including descriptive definitions based on controlled vocabularies. SPARQL queries successfully validated all test cases (rules) by retrieving appropriate patient outcomes: for instance, a patient treated with HFNC (high-flow nasal cannula) for 2 hours due to acute respiratory failure may avoid endotracheal intubation.   Conclusion: We developed an ontology that captures NIRS modalities in a unified framework and demonstrated its applicability through the evaluation of hypothetical patient scenarios and alignment with standardized vocabularies, which may need to be expanded to encompass a broader scope. </p>
<blockquote>
<p>目标：随着无创呼吸支持（NIRS）策略的发展，其在呼吸衰竭患者管理中的应用越来越广泛，成为传统通气方法的替代方案。然而，尽管无创呼吸支持的使用正在迅速扩大，但在所有医疗情况下，其最佳使用仍存在重大挑战。缺乏统一的本体结构，导致跨医疗保健系统的无创呼吸支持模式指导变得复杂。我们的目标是开发无创呼吸支持本体，以支持急性护理环境中的知识表示。通过提供统一的框架，增强数据的清晰度、互操作性和临床决策能力。</p>
</blockquote>
<p>方法：我们使用Web本体语言（OWL）语义和Protege来组织临床概念和关系，以开发无创呼吸支持本体。为了超越层次结构实现基于规则的临床推理，我们添加了语义Web规则语言（SWRL）规则。我们通过添加17个假设的临床场景来评估逻辑推理。我们使用SPARQL查询来检索和测试目标推断。</p>
<p>结果：该本体包含129个类、11个对象属性和17个数据属性，跨越886条公理建立概念关系。为了标准化临床概念，我们添加了361个注释，包括基于受控词汇的描述性定义。SPARQL查询成功验证了所有测试用例（规则），通过检索适当的病人结果，例如，因急性呼吸衰竭接受高流量鼻导管治疗2小时的患者可能避免气管插管。</p>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.19992v2">PDF</a> </p>
<p><strong>摘要</strong><br>NIRS本体构建<br>当前患者管理的研究逐渐集中在无创呼吸支持（NIRS）策略上，它是传统通气方法的替代方法。尽管NIRS的应用正在迅速扩大，但在所有医疗情况下其最佳使用仍存在挑战。由于缺乏统一的本体结构，导致在医疗保健系统中对NIRS模式的指导变得复杂。本研究旨在开发一个NIRS本体，以支持急性护理环境下的知识表示，通过提供统一的框架来增强数据的清晰度、互操作性和临床决策能力。本研究使用Web本体语言（OWL）语义和Protege来组织临床概念和关系。为了建立概念之间的关联和临床规则推理的需要，我们在语义网络上引入了SWRL规则，并且构建了评估逻辑的推理假设。使用SPARQL查询进行推理测试，验证其准确性。最终，我们构建了涵盖各种概念和关系的统一框架的NIRS本体。基于标准词汇对临床概念进行了标准化描述，且初步测试成功验证了所有假设案例的推理准确性。例如，对于因急性呼吸衰竭接受高流量鼻导管治疗两小时的患者，可以避免进行气管插管。结论：我们开发了一个统一的框架下的NIRS模式本体，并通过假设患者情景的评估和标准化词汇的匹配来验证其适用性，未来可能需要扩大其范围以涵盖更广泛的领域。</p>
<p><strong>关键见解</strong></p>
<ol>
<li>NIRS策略作为传统通气方法的替代方案在急性护理环境中受到越来越多的关注。</li>
<li>开发一个统一的框架至关重要，以提高NIRS数据清晰度、互操作性和临床决策能力。</li>
<li>使用OWL语义和Protege工具构建NIRS本体以组织临床概念和关系。通过引入SWRL规则实现了基于规则的推理，促进了概念间关系的建立。</li>
<li>本体包含概念与术语的统一框架、标准化的临床概念描述和假设案例验证的推理准确性等关键元素。此外通过SPARQL查询测试证明了其准确性和有效性。这可能有助于提高决策效率和促进临床知识的共享与使用。</li>
<li>本体构建有助于标准化临床实践中的术语和概念，提高医疗决策的质量和效率。未来可能需要进一步扩大应用范围并融入更多内容以提高适用性。对于更复杂的临床情景和需求进一步研究和扩展本体是必要的步骤和方向。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.19992">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.19992v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.19992v2/page_4_0.jpg" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="ThinkAct-Vision-Language-Action-Reasoning-via-Reinforced-Visual-Latent-Planning"><a href="#ThinkAct-Vision-Language-Action-Reasoning-via-Reinforced-Visual-Latent-Planning" class="headerlink" title="ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent   Planning"></a>ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent   Planning</h2><p><strong>Authors:Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, Fu-En Yang</strong></p>
<p>Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks. </p>
<blockquote>
<p>视觉-语言-动作（VLA）推理任务要求智能主体解释多模式指令，执行长期规划，并在动态环境中进行自适应动作。现有方法通常以端到端的方式训练VLA模型，直接将输入映射到动作，没有明确的推理过程，这阻碍了它们进行多步骤规划或适应复杂任务变化的能力。在本文中，我们提出了ThinkAct，这是一个双系统框架，它通过强化视觉潜在规划来桥接高级推理和低级动作执行。ThinkAct训练一个多模式大型语言模型，以生成由与行动一致的视觉奖励引导的实体推理计划，这些奖励基于目标完成和轨迹一致性。这些推理计划被压缩成视觉计划潜在量，以在下游动作模型中执行针对目标环境的稳健动作。关于实体推理和机器人操作基准测试的大量实验表明，ThinkAct能够在复杂的实体AI任务中实现少量适应、长期规划和自我修正行为。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.16815v2">PDF</a> NeurIPS 2025. Project page:   <a target="_blank" rel="noopener" href="https://jasper0314-huang.github.io/thinkact-vla/">https://jasper0314-huang.github.io/thinkact-vla/</a></p>
<p><strong>Summary</strong><br>视觉语言动作（VLA）推理任务需要智能体解释多模式指令，在动态环境中进行长期规划并自适应行动。现有方法通常采用端到端的训练方式，直接将输入映射到行动，缺乏明确的推理过程，这限制了它们在多步骤规划或复杂任务变化中的适应能力。本文提出ThinkAct，一个双系统框架，通过强化视觉潜在规划，将高级推理与低级行动执行相结合。ThinkAct训练多模态大型语言模型生成基于目标完成和轨迹一致性的奖励引导的身体推理计划。这些推理计划被压缩成视觉计划潜在状态，以在目标环境中对下游行动模型进行稳健的行动执行。在身体化推理和机器人操作基准测试上的广泛实验表明，ThinkAct能够在复杂的嵌入式AI任务中实现少样本适应、长期规划和自我修正行为。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VLA推理任务要求智能体解释多模式指令、进行长期规划并在动态环境中自适应行动。</li>
<li>现有方法直接映射输入到行动，缺乏明确推理，限制了多步骤规划和复杂任务变化的适应能力。</li>
<li>ThinkAct是一个双系统框架，结合高级推理和低级行动执行。</li>
<li>ThinkAct通过强化视觉潜在规划来训练多模态大型语言模型生成身体推理计划。</li>
<li>推理计划被压缩成视觉计划潜在状态，以在目标环境中进行稳健的行动执行。</li>
<li>ThinkAct实现了少样本适应、长期规划和自我修正行为。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.16815">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.16815v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.16815v2/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.16815v2/page_3_0.jpg" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="Temporal-Aware-GPU-Resource-Allocation-for-Distributed-LLM-Inference-via-Reinforcement-Learning"><a href="#Temporal-Aware-GPU-Resource-Allocation-for-Distributed-LLM-Inference-via-Reinforcement-Learning" class="headerlink" title="Temporal-Aware GPU Resource Allocation for Distributed LLM Inference via   Reinforcement Learning"></a>Temporal-Aware GPU Resource Allocation for Distributed LLM Inference via   Reinforcement Learning</h2><p><strong>Authors:Chengze Du, Zhiwei Yu, Heng Xu, Haojie Wang, Bo liu, Jialong Li</strong></p>
<p>The rapid growth of large language model (LLM) services imposes increasing demands on distributed GPU inference infrastructure. Most existing scheduling systems follow a reactive paradigm, relying solely on the current system state to make decisions, without considering how task demand and resource availability evolve over time. This lack of temporal awareness in reactive approaches leads to inefficient GPU utilization, high task migration overhead, and poor system responsiveness under dynamic workloads. In this work, we identify the fundamental limitations of these instantaneous-state-only scheduling approaches and propose Temporal Optimal Resource scheduling via Two-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling framework that captures both long-term workload patterns and short-term execution constraints. It adopts a two-layer design: a macro-level scheduler leverages reinforcement learning and optimal transport to coordinate inter-region task distribution, while a micro-level allocator refines task-to-server assignments within each region to reduce latency and switching costs. Experimental results across multiple network topologies show that TORTA reduces average inference response time by up to 15%, improves load balance by approximately 4-5%, and cuts total operational cost by 10-20% compared to state-of-the-art baseline methods. </p>
<blockquote>
<p>大规模语言模型（LLM）服务的快速增长对分布式GPU推理基础设施提出了更高的要求。现有的大多数调度系统都采用反应式范式，仅根据当前系统状态做出决策，而没有考虑到任务需求和资源可用性的时间演变。这种反应式方法中缺乏时间感知导致了GPU利用率低下、任务迁移开销高以及在动态工作负载下系统响应性差的问题。在这项工作中，我们识别出了这些仅基于即时状态的调度方法的基本局限性，并提出了通过两层架构进行时间最优资源调度（TORTA）。TORTA引入了一个时空调度框架，该框架能够捕捉长期工作负载模式和短期执行约束。它采用了两层设计：宏观调度器利用强化学习和最优传输来协调区域间的任务分配，而微观分配器则细化每个区域内的任务到服务器的分配，以减少延迟和切换成本。在多种网络拓扑上的实验结果表明，与最先进的基准方法相比，TORTA将平均推理响应时间减少了高达15%，提高了大约4-5%的负载平衡，并降低了10-20%的总运营成本。</p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.10259v2">PDF</a> 17 pages, 12 figures</p>
<p><strong>Summary</strong></p>
<p>基于大型语言模型（LLM）服务的快速增长对分布式GPU推理基础设施提出了越来越高的要求。现有调度系统主要遵循反应范式，仅根据当前系统状态进行决策，不考虑任务需求和资源可用性的时间演变。这导致GPU利用率低下、任务迁移开销高以及在动态工作负载下的系统响应性差。本研究提出了时空最优资源调度两层架构（TORTA），解决了即时状态调度方法的根本局限性。TORTA采用时空调度框架，既捕捉长期工作负载模式又考虑短期执行约束。它采用两层设计：宏观调度器利用强化学习和最优传输协调跨地区任务分布，微观分配器则细化区域内任务到服务器的分配以降低延迟和切换成本。实验结果显示，与最新基线方法相比，TORTA平均推理响应时间减少15%，负载平衡提升约4-5%，总运营成本降低10-20%。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLM）的增长对分布式GPU推理基础设施的需求增加。</li>
<li>现有调度系统主要基于反应范式，仅考虑即时系统状态，存在时空感知缺失。</li>
<li>时空缺失导致GPU利用率低、任务迁移开销大以及动态工作负载下的系统响应性差。</li>
<li>TORTA框架提出一个时空调度解决方案，结合长期工作负载模式和短期执行约束。</li>
<li>TORTA采用两层设计，宏观调度器利用强化学习和最优传输进行区域间任务协调，微观分配器优化区域内任务分配。</li>
<li>实验结果显示，TORTA在平均推理响应时间、负载平衡和运营成本方面相比现有方法有明显改进。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.10259">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.10259v2/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.10259v2/page_2_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.10259v2/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.10259v2/page_4_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2507.10259v2/page_5_0.jpg" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Chain-of-Thought-Prompting-Obscures-Hallucination-Cues-in-Large-Language-Models-An-Empirical-Evaluation"><a href="#Chain-of-Thought-Prompting-Obscures-Hallucination-Cues-in-Large-Language-Models-An-Empirical-Evaluation" class="headerlink" title="Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models: An Empirical Evaluation"></a>Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language   Models: An Empirical Evaluation</h2><p><strong>Authors:Jiahao Cheng, Tiancheng Su, Jia Yuan, Guoxiu He, Jiawei Liu, Xinqi Tao, Jingwen Xie, Huaxia Li</strong></p>
<p>Large Language Models (LLMs) often exhibit \textit{hallucinations}, generating factually incorrect or semantically irrelevant content in response to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by encouraging step-by-step reasoning, but its impact on hallucination detection remains underexplored. To bridge this gap, we conduct a systematic empirical evaluation. We begin with a pilot experiment, revealing that CoT reasoning significantly affects the LLM’s internal states and token probability distributions. Building on this, we evaluate the impact of various CoT prompting methods on mainstream hallucination detection methods across both instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three key dimensions: changes in hallucination score distributions, variations in detection accuracy, and shifts in detection confidence. Our findings show that while CoT prompting helps reduce hallucination frequency, it also tends to obscure critical signals used for detection, impairing the effectiveness of various detection methods. Our study highlights an overlooked trade-off in the use of reasoning. Code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/ECNU-Text-Computing/cot-hallu-detect">https://github.com/ECNU-Text-Computing/cot-hallu-detect</a> . </p>
<blockquote>
<p>大型语言模型（LLM）常常出现“幻觉”，即生成与提示内容事实上不正确或语义上不相关的内容。链式思维（CoT）提示可以通过鼓励逐步推理来减轻幻觉，但其对幻觉检测的影响仍被忽视。为了弥补这一差距，我们进行了系统的实证研究。我们首先进行了试点实验，揭示了CoT推理对LLM的内部状态和符号概率分布产生了显著影响。在此基础上，我们评估了各种CoT提示方法对主流幻觉检测方法的直接影响，涉及指令微调语言和面向推理的LLM。具体来说，我们研究了三个关键维度：幻觉评分分布的变化、检测准确度的变化以及检测信心的转变。我们的研究结果表明，虽然CoT提示有助于减少幻觉频率，但它也有掩盖用于检测的关键信号的趋势，从而损害了各种检测方法的有效性。我们的研究突出了在使用推理时忽略的权衡。代码公开在：<a target="_blank" rel="noopener" href="https://github.com/ECNU-Text-Computing/cot-hallu-detect%E3%80%82">https://github.com/ECNU-Text-Computing/cot-hallu-detect。</a></p>
</blockquote>
<p><strong>论文及项目相关链接</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.17088v3">PDF</a> Accepted at EMNLP 2025 Findings</p>
<p><strong>Summary</strong></p>
<p>大型语言模型（LLMs）容易出现幻觉，即生成与提示内容事实上不正确或语义上不相关的回应。链式思维（CoT）提示可以通过鼓励逐步推理来减轻幻觉问题，但其对幻觉检测的影响尚未得到充分研究。为了弥补这一空白，我们进行了系统的实证研究。我们首先进行了一项试点实验，发现CoT推理显著影响了LLM的内部状态和令牌概率分布。在此基础上，我们评估了不同的CoT提示方法对主流幻觉检测方法的影响，这些影响涉及指令微调及面向推理的LLMs。我们研究了三个关键方面：幻觉分数分布的变化、检测准确度的变化以及检测信心的变化。研究发现，虽然CoT提示有助于减少幻觉频率，但它也倾向于掩盖用于检测的关键信号，从而损害了各种检测方法的有效性。我们的研究揭示了在使用推理过程中一个被忽视的权衡问题。</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>大型语言模型（LLMs）会生成事实不正确或语义不相关的内容，即出现幻觉。</li>
<li>链式思维（CoT）提示可以通过鼓励逐步推理来减轻LLMs的幻觉问题。</li>
<li>CoT推理对LLM的内部状态和令牌概率分布有显著影响。</li>
<li>CoT提示方法对主流幻觉检测方法的影响涉及指令微调及面向推理的LLMs。</li>
<li>CoT提示有助于减少幻觉频率，但可能掩盖用于检测的关键信号，影响检测方法和准确度。</li>
<li>在使用推理过程中存在一个被忽视的权衡问题。</li>
<li>研究的代码已公开可用。</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.17088">Cool Papers</a></strong> </p>
<details>
  <summary>点此查看论文截图</summary>
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2506.17088v3/page_0_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2506.17088v3/page_1_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2506.17088v3/page_3_0.jpg" align="middle">
<img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_R1_Reasoning/2506.17088v3/page_5_0.jpg" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-09-21/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-09-21/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-21/LLM/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-21\./crop_LLM/2509.15098v1/page_2_1.jpg" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM 方向最新论文已更新，请持续关注 Update in 2025-09-21  TDRM Smooth Reward Models with Temporal Difference for LLM RL and   Inference
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-09-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-09-20/Text-to-Motion/">
                    <div class="card-image">
                        
                        <img src="D:\MyBlog\AutoFX\arxiv\2025-09-20\./crop_Text-to-Motion/2411.14951v3/page_0_0.jpg" class="responsive-img" alt="Text-to-Motion">
                        
                        <span class="card-title">Text-to-Motion</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Text-to-Motion 方向最新论文已更新，请持续关注 Update in 2025-09-20  Morph A Motion-free Physics Optimization Framework for Human Motion   Generation
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-09-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Text-to-Motion/" class="post-category">
                                    Text-to-Motion
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Text-to-Motion/">
                        <span class="chip bg-color">Text-to-Motion</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">28315.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
