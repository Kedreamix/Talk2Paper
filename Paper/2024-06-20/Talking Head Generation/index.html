<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Talking Head Generation">
    <meta name="description" content="Talking Head Generation 方向最新论文已更新，请持续关注 Update in 2024-06-20  Talk With Human-like Agents Empathetic Dialogue Through Perceptible   Acoustic Reception and Reaction">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Talking Head Generation | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-de6a5bba250b1dcfc57b3ad385bdae26.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Talking Head Generation</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                <span class="chip bg-color">Talking Head Generation</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Talking-Head-Generation/" class="post-category">
                                Talking Head Generation
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-06-20
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-12-10
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    6.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    22 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>⚠️ 以下所有内容总结都来自于 Google的大语言模型<a target="_blank" rel="noopener" href="https://ai.google.dev/">Gemini-Pro</a>的能力，如有错误，仅供参考，谨慎使用<br>🔴 请注意：千万不要用于严肃的学术场景，只能用于论文阅读前的初筛！<br>💗 如果您觉得我们的项目对您有帮助 <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ，还请您给我们一些鼓励！⭐️ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFace免费体验</a></p>
</blockquote>
<h1 id="2024-06-20-更新"><a href="#2024-06-20-更新" class="headerlink" title="2024-06-20 更新"></a>2024-06-20 更新</h1><h2 id="Talk-With-Human-like-Agents-Empathetic-Dialogue-Through-Perceptible-Acoustic-Reception-and-Reaction"><a href="#Talk-With-Human-like-Agents-Empathetic-Dialogue-Through-Perceptible-Acoustic-Reception-and-Reaction" class="headerlink" title="Talk With Human-like Agents: Empathetic Dialogue Through Perceptible   Acoustic Reception and Reaction"></a>Talk With Human-like Agents: Empathetic Dialogue Through Perceptible   Acoustic Reception and Reaction</h2><p><strong>Authors:Haoqiu Yan, Yongxin Zhu, Kai Zheng, Bing Liu, Haoyu Cao, Deqiang Jiang, Linli Xu</strong></p>
<p>Large Language Model (LLM)-enhanced agents become increasingly prevalent in Human-AI communication, offering vast potential from entertainment to professional domains. However, current multi-modal dialogue systems overlook the acoustic information present in speech, which is crucial for understanding human communication nuances. This oversight can lead to misinterpretations of speakers’ intentions, resulting in inconsistent or even contradictory responses within dialogues. To bridge this gap, in this paper, we propose PerceptiveAgent, an empathetic multi-modal dialogue system designed to discern deeper or more subtle meanings beyond the literal interpretations of words through the integration of speech modality perception. Employing LLMs as a cognitive core, PerceptiveAgent perceives acoustic information from input speech and generates empathetic responses based on speaking styles described in natural language. Experimental results indicate that PerceptiveAgent excels in contextual understanding by accurately discerning the speakers’ true intentions in scenarios where the linguistic meaning is either contrary to or inconsistent with the speaker’s true feelings, producing more nuanced and expressive spoken dialogues. Code is publicly available at: \url{<a target="_blank" rel="noopener" href="https://github.com/Haoqiu-Yan/PerceptiveAgent%7D">https://github.com/Haoqiu-Yan/PerceptiveAgent}</a>. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.12707v1">PDF</a> 9 pages, 3 figures, ACL24 accepted</p>
<p><strong>Summary</strong><br>大型语言模型增强的代理在人工智能与人类交流中日益普及，提供从娱乐到专业领域的广泛潜力，但当前的多模态对话系统忽视了语音中的声学信息，这对理解人类交流的微妙差异至关重要。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>大型语言模型（LLM）增强的代理在人工智能与人类交流中应用广泛。</li>
<li>目前的多模态对话系统忽视了语音中的声学信息。</li>
<li>声学信息对于理解人类交流的微妙差异至关重要。</li>
<li>这种忽视可能导致对说话者意图的误解。</li>
<li>提出了PerceptiveAgent，旨在通过集成语音模态感知来解决这一问题。</li>
<li>PerceptiveAgent利用LLM作为认知核心，能够准确识别说话者真实意图。</li>
<li>实验结果显示，PerceptiveAgent在理解上下文和表达细微情感方面表现出色。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p>好的，我将根据您提供的摘要和介绍，总结这篇论文的关键信息和核心内容。下面是按照要求的格式输出的答案：</p>
<ol>
<li><p>标题：感知代理：通过感知理解人类对话中的同情心（Talk With Human-like Agents: Empathetic Dialogue Through Perceptible）</p>
</li>
<li><p>作者：Haoqiu Yan（颜浩秋）, Yongxin Zhu（朱永新）, Kai Zheng（郑凯）, Bing Liu（刘斌）, Haoyu Cao（曹浩宇）, Deqiang Jiang（姜德强）, Linli Xu（徐林利）等。</p>
</li>
<li><p>所属机构：中国科学技术大学计算机科学和技术学院、数据科学学院，以及与腾讯优图实验室合作。</p>
</li>
<li><p>关键词：大型语言模型、感知代理、人工智能对话系统、多模态对话系统、语音感知、情感计算。</p>
</li>
<li><p>Urls：论文链接暂未提供；代码仓库链接：<a target="_blank" rel="noopener" href="https://github.com/Haoqiu-Yan/PerceptiveAgent">Github链接</a>（如实际有公开代码的话，请替换为实际链接）。</p>
</li>
<li><p>摘要：</p>
<ul>
<li><p>(1)研究背景：随着人工智能对话系统的普及，人们越来越期待系统能够理解和回应人类的情感和意图。然而，现有的多模态对话系统往往忽略了语音中的声学信息，这对于理解人类沟通的细微差别至关重要。本文旨在通过整合语音感知来弥补这一差距，提出一个能够感知更深层含义的多模态对话系统。</p>
</li>
<li><p>(2)过去的方法与问题：现有的对话系统主要依赖文本交互，忽略了语音中的声学信息，这可能导致对说话人意图的误解。因此，需要一种新的方法来解决这一问题。</p>
</li>
<li><p>(3)研究方法：本文提出了PerceptiveAgent，一个以大型语言模型为核心的多模态对话系统。该系统能够感知输入语音的声学信息，并通过自然语言描述生成基于说话风格的同情回应。实验结果表明，PerceptiveAgent在理解语境和准确识别说话人的真实意图方面表现出色，特别是在语言意义与说话人的真实感受相悖或不一致的情况下。</p>
</li>
<li><p>(4)任务与性能：PerceptiveAgent在多模态对话任务上进行了测试，并通过准确识别说话人的意图和情感，生成更富有同情心的回应，证明了其有效性和性能。这些结果支持了该方法的目标，即提高对话系统的情感智能和语境理解能力。</p>
</li>
</ul>
</li>
</ol>
<p>希望以上总结符合您的要求！<br>7. 方法介绍：</p>
<p>这篇论文的核心方法是设计一个能够感知语音情感和意图的多模态对话系统。这一方法主要包括以下几个步骤：</p>
<ul>
<li>(1)语音感知：通过语音捕获模型（Speech Caption Model）捕捉语音中的声学信息，并将其转化为文本描述。这一模型通过编码语音输入并结合预训练的语言模型生成描述说话风格的文本。这是理解说话人意图的关键步骤。此外，论文还通过微调策略优化该模型，使其更好地适应多模态嵌入对齐和指令调节任务。</li>
<li>(2)意图辨识与理解：利用上一步骤生成的文本描述，结合大型语言模型（LLM），感知对话语境并理解说话人的真实意图。这一过程依赖于大型语言模型的上下文理解能力。设计好的提示可以更有效地利用语言模型的这一功能。这是多模态对话系统的核心环节之一。 </li>
<li>(3)表达性语音合成：利用多说话人多属性合成器（MSMA-Synthesizer）合成富有同情心的音频响应。该合成器根据对话内容和风格描述生成语音响应，实现了语音的精细控制。通过引入多种韵律属性，如音调、速度和能量等，合成器能够生成更自然的语音响应。这一步是多模态对话系统的最终输出环节，使得系统能够模拟人类的情感表达和交流方式。论文中的实验证明了该系统的有效性和性能。</li>
</ul>
<p>总的来说，该方法通过整合语音感知和大型语言模型技术，实现了多模态对话系统的情感智能和语境理解能力提升，为人工智能对话系统的进一步发展提供了新的思路和方法。</p>
<p>结论：</p>
<p>（1）这篇论文的研究意义在于提出了一种感知代理（PerceptiveAgent）的多模态对话系统，该系统能够感知语音中的声学信息并结合大型语言模型（LLM）来理解和回应人类的情感和意图。这项研究推动了人工智能对话系统在情感智能和语境理解能力方面的进展，有助于提高人工智能系统的用户交互体验，使机器更好地理解和模拟人类的情感表达和交流方式。此外，该研究在理解和准确识别说话人意图方面取得了显著成果，特别是在语言意义与说话人的真实感受相悖或不一致的情况下。这些成果对于开发更智能、更人性化的对话系统具有重要的应用价值。</p>
<p>（2）创新点：该论文的创新之处在于整合了语音感知和大型语言模型技术，提出了一个能够感知语音情感和意图的多模态对话系统。该系统通过捕捉语音中的声学信息，结合文本描述和大型语言模型来生成基于说话风格的同情回应。这一创新方法提高了对话系统的情感智能和语境理解能力。<br>性能：实验结果表明，PerceptiveAgent在多模态对话任务上表现出良好的性能。通过准确识别说话人的意图和情感，该系统能够生成更富有同情心的回应。这些结果支持了该方法的目标，即提高对话系统的情感智能和语境理解能力。<br>工作量：该论文在构建PerceptiveAgent系统方面投入了大量的工作，包括设计语音感知模型、意图辨识与理解模块以及表达性语音合成器等。此外，论文还进行了大量的实验验证和性能评估，证明了该系统的有效性和性能。然而，论文未提及代码仓库链接的实际可用性，这可能是一个潜在的工作不足。总体而言，该论文在构建多模态对话系统方面取得了重要的进展和成果。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-6d6ed15644b9de007c349ee10520a26d.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-de6a5bba250b1dcfc57b3ad385bdae26.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-a9d446b3eef7d75be3e920901f7d974e.jpg" align="middle">
</details>




<h2 id="NLDF-Neural-Light-Dynamic-Fields-for-Efficient-3D-Talking-Head-Generation"><a href="#NLDF-Neural-Light-Dynamic-Fields-for-Efficient-3D-Talking-Head-Generation" class="headerlink" title="NLDF: Neural Light Dynamic Fields for Efficient 3D Talking Head   Generation"></a>NLDF: Neural Light Dynamic Fields for Efficient 3D Talking Head   Generation</h2><p><strong>Authors:Niu Guanchen</strong></p>
<p>Talking head generation based on the neural radiation fields model has shown promising visual effects. However, the slow rendering speed of NeRF seriously limits its application, due to the burdensome calculation process over hundreds of sampled points to synthesize one pixel. In this work, a novel Neural Light Dynamic Fields model is proposed aiming to achieve generating high quality 3D talking face with significant speedup. The NLDF represents light fields based on light segments, and a deep network is used to learn the entire light beam’s information at once. In learning the knowledge distillation is applied and the NeRF based synthesized result is used to guide the correct coloration of light segments in NLDF. Furthermore, a novel active pool training strategy is proposed to focus on high frequency movements, particularly on the speaker mouth and eyebrows. The propose method effectively represents the facial light dynamics in 3D talking video generation, and it achieves approximately 30 times faster speed compared to state of the art NeRF based method, with comparable generation visual quality. </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.11259v1">PDF</a> </p>
<p><strong>Summary</strong><br>基于神经辐射场模型的说话头像生成显示了良好的视觉效果，但NeRF的渲染速度过慢严重限制了其应用。</p>
<p><strong>Key Takeaways</strong>  </p>
<ul>
<li>神经光动态场模型（NLDF）旨在通过光段生成高质量的3D说话面部，并显著加快速度。</li>
<li>NLDF使用深度网络一次性学习整个光束的信息，采用知识蒸馏并使用NeRF合成结果指导正确的光段颜色。</li>
<li>提出了新的主动池训练策略，重点关注高频运动，特别是演讲者的嘴部和眉毛。</li>
<li>该方法有效地表现了3D说话视频生成中的面部光动态。</li>
<li>NLDF比NeRF基于方法快大约30倍，并且具有可比较的生成视觉质量。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p>Title: NLDF：基于神经网络光照动态场的3D说话人头像高效生成方法</p>
<p>Authors: 牛冠晨及其他未列出的合作者</p>
<p>Affiliation: 作者的隶属机构未提及。</p>
<p>Keywords: Neural Light Dynamic Fields, Talking Head Generation, NeRF, Light Segmentation, Active Pool Training</p>
<p>Urls: <a target="_blank" rel="noopener" href="https://github.com/XXX/XXX">https://github.com/XXX/XXX</a> （如果没有GitHub代码链接，请填写“GitHub:None”）</p>
<p>Summary:</p>
<p>(1) 研究背景：<br>随着计算机图形学和深度学习技术的发展，3D说话人头像生成已经成为一个热门的研究领域。然而，现有的基于神经网络辐射场（NeRF）的方法虽然生成效果出色，但渲染速度较慢，难以满足实时应用的需求。本文提出的神经网络光照动态场（NLDF）模型旨在解决这一问题，实现高质量3D说话头像的快速生成。</p>
<p>(2) 过去的方法及问题：<br>过去的方法主要基于NeRF模型进行3D说话头像的生成，虽然视觉效果出色，但由于需要对数百个采样点进行繁琐的计算过程来合成一个像素，导致渲染速度较慢。此外，过去的方法未能有效地对光照动态进行建模，影响了生成结果的逼真度。</p>
<p>(3) 研究方法：<br>本文提出了基于光照分段的NLDF模型，使用深度网络一次性学习整个光束的信息。同时，采用知识蒸馏技术进行学习，并使用NeRF合成的结果引导光分段的正确着色。此外，本文还提出了一种新的主动池训练策略，专注于高频移动，特别是说话者的嘴巴和眉毛，以提高生成结果的动态效果。</p>
<p>(4) 任务与性能：<br>本文的方法应用于3D说话视频生成任务，实现了与基于NeRF的方法相比，约30倍的加速效果，同时保持相当的生成视觉质量。通过实验验证，本文方法能够在保证生成质量的同时，显著提高渲染速度，从而满足实时应用的需求。性能结果表明，该方法能够达到研究目标。<br>好的，我将按照您的要求，用中文详细阐述这篇论文的方法论。以下为详细内容：</p>
<ol start="7">
<li>方法论：</li>
</ol>
<p>(1) 研究首先基于现有的神经网络辐射场（NeRF）方法存在渲染速度较慢的问题出发，针对该问题，本文提出了一种基于神经网络光照动态场（NLDF）的模型，用于实现高质量3D说话头像的快速生成。这是解决该问题的核心思路。</p>
<p>(2) 为了构建光照分段模型，本研究引入了光照分段的概念并利用深度网络进行光束信息学习。在此基础上使用知识蒸馏技术进行高效学习。这是其技术的第一步创新点。研究接着引入了NeRF合成结果作为引导，确保光照分段的正确着色。这是其技术的第二步创新点。此外，为了进一步提高生成结果的动态效果，研究还提出了一种新的主动池训练策略，该策略特别关注高频移动区域（如嘴巴和眉毛）。这部分内容是论文的主要贡献和创新点之一。</p>
<p>(3) 在实际应用中，本文的方法被应用于3D说话视频生成任务。通过对比实验验证，该方法的渲染速度相较于基于NeRF的方法实现了约30倍的加速效果，同时保持了相当的视觉质量。这一部分的实验数据和结果证明了该方法的实用性和优越性。最后，通过性能结果分析，验证了该方法达到了研究目标。这部分内容是论文的核心成果和贡献所在。</p>
<ol start="8">
<li>Conclusion:</li>
</ol>
<p>(1) 这项工作的意义在于提出了一种基于神经网络光照动态场（NLDF）的3D说话人头像高效生成方法，解决了现有方法渲染速度慢、光照动态建模不足的问题，为实时应用提供了可能。</p>
<p>(2) 创新点方面，本文引入了光照分段的概念并利用深度网络进行光束信息学习，同时采用知识蒸馏技术和主动池训练策略，提高了生成结果的动态效果和渲染速度。性能上，本文方法实现了与基于NeRF的方法相比约30倍的加速效果，同时保持了相当的视觉质量。工作量方面，本文实现了3D说话视频生成任务的应用，并通过实验验证了方法的有效性。</p>
<p>总体来说，本文提出的基于神经网络光照动态场的3D说话人头像高效生成方法具有显著的创新性和实用性，为相关领域的研究提供了有益的参考和启示。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-0d5f496fb1cc5f2aa7988a95b302e626.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-4f1ec26ae4a93cc432503fa4842a7007.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-9f82dd9ef776058f584bdd8f88b44efc.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://picx.zhimg.com/v2-070e3169781f9800bdeb41a6d90d09de.jpg" align="middle">
</details>




<h2 id="Make-Your-Actor-Talk-Generalizable-and-High-Fidelity-Lip-Sync-with-Motion-and-Appearance-Disentanglement"><a href="#Make-Your-Actor-Talk-Generalizable-and-High-Fidelity-Lip-Sync-with-Motion-and-Appearance-Disentanglement" class="headerlink" title="Make Your Actor Talk: Generalizable and High-Fidelity Lip Sync with   Motion and Appearance Disentanglement"></a>Make Your Actor Talk: Generalizable and High-Fidelity Lip Sync with   Motion and Appearance Disentanglement</h2><p><strong>Authors:Runyi Yu, Tianyu He, Ailing Zhang, Yuchi Wang, Junliang Guo, Xu Tan, Chang Liu, Jie Chen, Jiang Bian</strong></p>
<p>We aim to edit the lip movements in talking video according to the given speech while preserving the personal identity and visual details. The task can be decomposed into two sub-problems: (1) speech-driven lip motion generation and (2) visual appearance synthesis. Current solutions handle the two sub-problems within a single generative model, resulting in a challenging trade-off between lip-sync quality and visual details preservation. Instead, we propose to disentangle the motion and appearance, and then generate them one by one with a speech-to-motion diffusion model and a motion-conditioned appearance generation model. However, there still remain challenges in each stage, such as motion-aware identity preservation in (1) and visual details preservation in (2). Therefore, to preserve personal identity, we adopt landmarks to represent the motion, and further employ a landmark-based identity loss. To capture motion-agnostic visual details, we use separate encoders to encode the lip, non-lip appearance and motion, and then integrate them with a learned fusion module. We train MyTalk on a large-scale and diverse dataset. Experiments show that our method generalizes well to the unknown, even out-of-domain person, in terms of both lip sync and visual detail preservation. We encourage the readers to watch the videos on our project page (<a target="_blank" rel="noopener" href="https://ingrid789.github.io/MyTalk/">https://Ingrid789.github.io/MyTalk/</a>). </p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2406.08096v2">PDF</a> 14 pages of main text, 23 pages in total, 9 figures</p>
<p><strong>Summary</strong><br>我们旨在根据给定的语音编辑说话视频中的唇部运动，同时保留个人身份和视觉细节。</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>任务分解为语音驱动的唇部运动生成和视觉外观合成两个子问题。</li>
<li>目前的解决方案在单一生成模型中处理两个子问题，导致在唇同步质量和视觉细节保留之间存在挑战性的权衡。</li>
<li>我们提议通过分离运动和外观，采用语音到运动扩散模型和运动条件的外观生成模型分别生成它们。</li>
<li>每个阶段仍然存在挑战，如在第一阶段中的运动感知身份保留和第二阶段的视觉细节保留。</li>
<li>为了保留个人身份，我们采用地标来表示运动，并进一步采用基于地标的身份损失。</li>
<li>为了捕捉与运动无关的视觉细节，我们使用单独的编码器对唇部、非唇部外观和运动进行编码，然后用学习的融合模块进行集成。</li>
<li>我们在大规模和多样化数据集上训练了MyTalk模型，实验表明我们的方法在唇同步和视觉细节保留方面对未知、甚至是超领域的人物都具有很好的泛化能力。</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">ChatPaperFree</a></strong></p>
<p>好的，我将会根据您给出的要求来回答。</p>
<ol>
<li>Title: 基于运动与外观解耦的说话人唇动生成技术</li>
<li>Authors: 于润义、何天宇、张艾玲、王宇池、郭俊良、谭煦、刘畅、陈洁和边江</li>
<li>Affiliation: 第一作者于润义是北京大学的。</li>
<li>Keywords: 说话视频生成、唇同步、面部动画、扩散模型</li>
<li>Urls: 论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.08096v2">论文链接</a>。Github代码链接：GitHub:暂无可用的代码链接。</li>
<li>Summary:</li>
</ol>
<p>(1) 研究背景：<br>随着人工智能技术的发展，生成说话视频的需求越来越高。本文旨在编辑说话视频的唇动，使其能够根据给定的语音进行同步，同时保留个人身份和视觉细节。这对于电影特效、虚拟现实和游戏等领域有广泛应用。</p>
<p>(2) 过去的方法及其问题：<br>当前的方法通常在一个生成模型中处理唇动生成和视觉外观合成两个子问题，导致唇同步质量和视觉细节保留之间的权衡困难。此外，这些方法在应对未知或域外人物时表现不佳。</p>
<p>(3) 研究方法：<br>本文提出一种将运动和外观解耦的方法，通过扩散模型和基于运动的外观生成模型来分别生成运动和外观。为解决身份保留和视觉细节捕捉的问题，采用基于特征点的身份损失和融合模块来整合唇部和非唇部区域的视觉细节。</p>
<p>(4) 任务与性能：<br>本文的方法在大型和多样化的数据集上进行训练，实验表明，该方法在未知甚至域外人物上的唇同步和视觉细节保留方面都具有良好的泛化能力。性能评估支持了该方法的有效性。</p>
<p>请注意，由于无法直接观看视频或访问GitHub代码链接，无法进一步验证上述信息的准确性。建议您直接通过提供的链接查看论文和相关资源以获取更多详细信息。<br>好的，根据您的要求，我会按照给定的格式来详细阐述这篇论文的方法论。</p>
<ol start="7">
<li>Methods:</li>
</ol>
<p>(1) 研究背景和意义：随着多媒体技术的发展，生成说话视频的需求越来越高。特别是在电影特效、虚拟现实和游戏等领域，唇动编辑技术具有重要的应用价值。因此，本文旨在开发一种能够编辑说话视频的唇动，使其能够根据给定的语音进行同步，同时保留个人身份和视觉细节的技术。</p>
<p>(2) 对现有技术的问题进行分析：当前的方法大多在一个生成模型中同时处理唇动生成和视觉外观合成两个子问题，这导致在唇同步质量和视觉细节保留之间难以取得平衡。此外，这些方法在应对未知或域外人物时表现不佳。</p>
<p>(3) 方法论的主要思路：针对上述问题，本文提出了一种将运动和外观解耦的方法，通过扩散模型和基于运动的外观生成模型来分别生成运动和外观。首先，利用扩散模型学习唇部运动模式；然后，通过基于运动的外观生成模型来生成与唇部运动相匹配的外观。</p>
<p>(4) 具体实现步骤：</p>
<ul>
<li>a. 数据准备：收集并预处理大量的说话视频数据，包括面部图像和对应的语音信号。</li>
<li>b. 训练和模型构建：采用扩散模型学习唇部运动模式，并构建基于运动的外观生成模型。</li>
<li>c. 唇同步和视觉细节保留：通过融合模块整合唇部和非唇部区域的视觉细节，实现唇同步和视觉细节保留。</li>
<li>d. 评估与测试：在大型和多样化的数据集上进行训练，并通过实验验证该方法在未知甚至域外人物上的泛化能力。</li>
</ul>
<p>(5) 方法和效果评估：实验结果表明，本文提出的方法在唇同步和视觉细节保留方面均表现出良好的性能。通过对比实验和性能评估，验证了该方法的有效性和优越性。</p>
<p>以上就是这篇论文的方法论介绍。</p>
<ol start="8">
<li>Conclusion:</li>
</ol>
<p>(1)这篇工作的意义在于开发了一种能够编辑说话视频的唇动技术，使视频能够根据给定的语音进行同步，同时保留个人身份和视觉细节。这项技术在电影特效、虚拟现实和游戏等领域具有重要的应用价值。</p>
<p>(2)创新点：本文提出了一种基于运动与外观解耦的说话人唇动生成技术，通过扩散模型和基于运动的外观生成模型分别生成运动和外观，实现了唇同步和视觉细节保留。<br>性能：实验结果表明，该方法在唇同步和视觉细节保留方面均表现出良好的性能，并且在未知或域外人物上具有良好的泛化能力。<br>工作量：本文收集并预处理了大量的说话视频数据，构建了基于扩散模型和基于运动的外观生成模型，通过实验验证了方法的有效性。</p>
<p>总体来说，本文提出的说话人唇动生成技术具有创新性和实用性，为电影特效、虚拟现实和游戏等领域的唇动编辑提供了有效的解决方案。</p>
<details>
  <summary>点此查看论文截图</summary>
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-8a6a7c5f91f913dcbb728f71012a7a25.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pica.zhimg.com/v2-ffacd6f931293748617a8f14a08c763e.jpg" align="middle">
<img src="/medias/loading.gif" data-original="https://pic1.zhimg.com/v2-7aef5cf3d8645ae9194bd3559c9139ed.jpg" align="middle">
</details>





                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2024-06-20/Talking%20Head%20Generation/">https://kedreamix.github.io/Talk2Paper/Paper/2024-06-20/Talking%20Head%20Generation/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Talking-Head-Generation/">
                                    <span class="chip bg-color">Talking Head Generation</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-06-20/3DGS/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e47e1891444d84986d52eede4b830aec.jpg" class="responsive-img" alt="3DGS">
                        
                        <span class="card-title">3DGS</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            3DGS 方向最新论文已更新，请持续关注 Update in 2024-06-20  HumanSplat Generalizable Single-Image Human Gaussian Splatting with   Structure Priors
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-06-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/3DGS/" class="post-category">
                                    3DGS
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/3DGS/">
                        <span class="chip bg-color">3DGS</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2024-06-20/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-e47e1891444d84986d52eede4b830aec.jpg" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models 方向最新论文已更新，请持续关注 Update in 2024-06-20  HumanSplat Generalizable Single-Image Human Gaussian Splatting with   Structure Priors
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-06-20
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">4610.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    


        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z\d\-\.\+]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(t.test(e.href)||r.test(e.href))&&(e.href=a.dataset.original)})});</script><script>(r=>{r.imageLazyLoadSetting.processImages=t;var a=r.imageLazyLoadSetting.isSPA,o=r.imageLazyLoadSetting.preloadRatio||1,d=i();function i(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(t){(a||t)&&(d=i());for(var e,n=0;n<d.length;n++)0<=(e=(e=d[n]).getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(r.innerHeight*o||document.documentElement.clientHeight*o)&&(()=>{var t,e,a,o,i=d[n];e=function(){d=d.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).dataset.loaded||(t.hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(a=new Image,o=t.getAttribute("data-original"),a.onload=function(){t.src=o,t.removeAttribute("data-original"),t.setAttribute("data-loaded",!0),e&&e()},a.onerror=function(){t.removeAttribute("data-original"),t.setAttribute("data-loaded",!1),t.src=o},t.src!==o&&(a.src=o)))})()}function e(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",e),r.addEventListener("resize",e),r.addEventListener("orientationchange",e)})(this);</script><script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- 动态标签 -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ诶，页面崩溃了嘛？", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
