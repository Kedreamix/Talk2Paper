<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="R1_Reasoning">
    <meta name="description" content="R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-11  Visual Spatial Tuning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>R1_Reasoning | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-0b8e411842c62e6268f50becfdeb0b48')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">R1_Reasoning</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/R1-Reasoning/">
                                <span class="chip bg-color">R1_Reasoning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                R1_Reasoning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-20
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    21.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    87 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-11-æ›´æ–°"><a href="#2025-11-11-æ›´æ–°" class="headerlink" title="2025-11-11 æ›´æ–°"></a>2025-11-11 æ›´æ–°</h1><h2 id="Visual-Spatial-Tuning"><a href="#Visual-Spatial-Tuning" class="headerlink" title="Visual Spatial Tuning"></a>Visual Spatial Tuning</h2><p><strong>Authors:Rui Yang, Ziyu Zhu, Yanwei Li, Jingjia Huang, Shen Yan, Siyuan Zhou, Zhe Liu, Xiangtai Li, Shuangye Li, Wenqian Wang, Yi Lin, Hengshuang Zhao</strong></p>
<p>Capturing spatial relationships from visual inputs is a cornerstone of human-like general intelligence. Several previous studies have tried to enhance the spatial awareness of Vision-Language Models (VLMs) by adding extra expert encoders, which brings extra overhead and usually harms general capabilities. To enhance the spatial ability in general architectures, we introduce Visual Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with human-like visuospatial abilities, from spatial perception to reasoning. We first attempt to enhance spatial perception in VLMs by constructing a large-scale dataset termed VST-P, which comprises 4.1 million samples spanning 19 skills across single views, multiple images, and videos. Then, we present VST-R, a curated dataset with 135K samples that instruct models to reason in space. In particular, we adopt a progressive training pipeline: supervised fine-tuning to build foundational spatial knowledge, followed by reinforcement learning to further improve spatial reasoning abilities. Without the side-effect to general capabilities, the proposed VST consistently achieves state-of-the-art results on several spatial benchmarks, including $34.8%$ on MMSI-Bench and $61.2%$ on VSIBench. It turns out that the Vision-Language-Action models can be significantly enhanced with the proposed spatial tuning paradigm, paving the way for more physically grounded AI. </p>
<blockquote>
<p>ä»è§†è§‰è¾“å…¥æ•æ‰ç©ºé—´å…³ç³»æ˜¯ç±»äººé€šç”¨æ™ºèƒ½çš„æ ¸å¿ƒã€‚ä¹‹å‰çš„ä¸€äº›ç ”ç©¶è¯•å›¾é€šè¿‡æ·»åŠ é¢å¤–çš„ä¸“ä¸šç¼–ç å™¨æ¥æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„ç©ºé—´æ„è¯†ï¼Œè¿™å¸¦æ¥äº†é¢å¤–çš„å¼€é”€å¹¶é€šå¸¸ä¼šæŸå®³å…¶é€šç”¨èƒ½åŠ›ã€‚ä¸ºäº†å¢å¼ºé€šç”¨æ¶æ„ä¸­çš„ç©ºé—´èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†è§†è§‰ç©ºé—´è°ƒä¼˜ï¼ˆVSTï¼‰è¿™ä¸€å…¨é¢æ¡†æ¶ï¼Œæ—¨åœ¨åŸ¹å…»å…·æœ‰ç±»äººè§†è§‰ç©ºé—´èƒ½åŠ›çš„VLMsï¼Œæ¶µç›–ä»ç©ºé—´æ„ŸçŸ¥åˆ°æ¨ç†çš„å„ä¸ªæ–¹é¢ã€‚æˆ‘ä»¬é¦–å…ˆå°è¯•é€šè¿‡æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†VST-På¢å¼ºVLMsçš„ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›ï¼Œè¯¥æ•°æ®é›†åŒ…å«410ä¸‡æ ·æœ¬ï¼Œæ¶µç›–å•è§†å›¾ã€å¤šå›¾åƒå’Œè§†é¢‘ä¸­çš„19é¡¹æŠ€èƒ½ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å±•ç¤ºäº†VST-Ræ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«13.5ä¸‡æ ·æœ¬ï¼Œç”¨äºæŒ‡å¯¼æ¨¡å‹è¿›è¡Œç©ºé—´æ¨ç†ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ¸è¿›å¼çš„è®­ç»ƒæµç¨‹ï¼šå…ˆè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒä»¥å»ºç«‹åŸºç¡€ç©ºé—´çŸ¥è¯†ï¼Œç„¶åé‡‡ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æé«˜ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚åœ¨ä¸æŸå®³é€šç”¨èƒ½åŠ›çš„æƒ…å†µä¸‹ï¼Œæ‰€æå‡ºçš„VSTåœ¨å¤šä¸ªç©ºé—´åŸºå‡†æµ‹è¯•ä¸Šå§‹ç»ˆè¾¾åˆ°äº†æœ€æ–°æ°´å¹³çš„ç»“æœï¼ŒåŒ…æ‹¬MMSI-Benchä¸Šçš„34.8%å’ŒVSIBenchä¸Šçš„61.2%ã€‚äº‹å®è¯æ˜ï¼Œé€šè¿‡æå‡ºçš„ç©ºé—´è°ƒä¼˜èŒƒå¼ï¼Œè§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹å¯ä»¥æ˜¾è‘—å¢å¼ºï¼Œä¸ºæ›´ç‰©ç†åŒ–çš„AIé“ºå¹³äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05491v1">PDF</a> </p>
<p><strong>Summary</strong><br>è§†è§‰ç©ºé—´æ„ŸçŸ¥å’Œæ¨ç†æ˜¯äººç±»æ™ºèƒ½çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ä¸ºæé«˜è§†è§‰è¯­è¨€æ¨¡å‹çš„ç©ºé—´èƒ½åŠ›ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸ºè§†è§‰ç©ºé—´è°ƒä¼˜ï¼ˆVSTï¼‰çš„ç»¼åˆæ¡†æ¶ï¼Œé€šè¿‡æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†VST-På’ŒVST-Rï¼Œä»¥åŠé‡‡ç”¨ç›‘ç£ç²¾ç»†è°ƒæ•´å’Œå¼ºåŒ–å­¦ä¹ çš„æ¸è¿›è®­ç»ƒç®¡é“ï¼Œå¢å¼ºäº†æ¨¡å‹çš„ç©ºé—´æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚è¯¥æ¡†æ¶åœ¨å¤šä¸ªç©ºé—´åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€æ–°ç»“æœï¼Œè¯æ˜äº†å…¶åœ¨æé«˜è§†è§‰è¯­è¨€æ¨¡å‹ç©ºé—´èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è§†è§‰ç©ºé—´å…³ç³»å¤„ç†æ˜¯äººç±»æ™ºèƒ½çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚</li>
<li>ä¸ºå¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„ç©ºé—´èƒ½åŠ›ï¼Œå¼•å…¥äº†è§†è§‰ç©ºé—´è°ƒä¼˜ï¼ˆVSTï¼‰æ¡†æ¶ã€‚</li>
<li>VSTé€šè¿‡æ„å»ºå¤§è§„æ¨¡æ•°æ®é›†VST-På’ŒVST-Ræ¥åŸ¹å…»VLMsçš„ç©ºé—´æ„ŸçŸ¥å’Œæ¨ç†èƒ½åŠ›ã€‚</li>
<li>VST-Pæ•°æ®é›†åŒ…å«410ä¸‡æ ·æœ¬ï¼Œæ¶µç›–å•è§†å›¾ã€å¤šå›¾åƒå’Œè§†é¢‘çš„ç©ºé—´æ„ŸçŸ¥æŠ€èƒ½ã€‚</li>
<li>VST-Ræ•°æ®é›†åŒ…å«13.5ä¸‡æ ·æœ¬ï¼Œç”¨äºåŸ¹å…»æ¨¡å‹çš„ç©ºé—´æ¨ç†èƒ½åŠ›ã€‚</li>
<li>é‡‡ç”¨ç›‘ç£ç²¾ç»†è°ƒæ•´å’Œå¼ºåŒ–å­¦ä¹ çš„æ¸è¿›è®­ç»ƒç®¡é“æ¥æé«˜æ¨¡å‹çš„ç©ºé—´èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05491">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6e43d4003df2b65308325f4c67bfe44c" align="middle">
<img src="https://picx.zhimg.com/v2-b16b1447e85c58271e4f2b278ceb08a9" align="middle">
<img src="https://picx.zhimg.com/v2-25c0c0e9257740d7578bd2ff89dedc6b" align="middle">
<img src="https://picx.zhimg.com/v2-152a703521d16b3fd8d21f071778a7c0" align="middle">
<img src="https://picx.zhimg.com/v2-642b536c455e159449d7fda0f12df5e5" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="TimeSearch-R-Adaptive-Temporal-Search-for-Long-Form-Video-Understanding-via-Self-Verification-Reinforcement-Learning"><a href="#TimeSearch-R-Adaptive-Temporal-Search-for-Long-Form-Video-Understanding-via-Self-Verification-Reinforcement-Learning" class="headerlink" title="TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding   via Self-Verification Reinforcement Learning"></a>TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding   via Self-Verification Reinforcement Learning</h2><p><strong>Authors:Junwen Pan, Qizhe Zhang, Rui Zhang, Ming Lu, Xin Wan, Yuan Zhang, Chang Liu, Qi She</strong></p>
<p>Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Time-Search/TimeSearch-R">https://github.com/Time-Search/TimeSearch-R</a>. </p>
<blockquote>
<p>æ—¶åºæœç´¢æ—¨åœ¨ä»æ•°ä¸‡å¸§ä¸­è¯†åˆ«å‡ºæœ€å°çš„ç›¸å…³å¸§é›†åˆï¼Œä½œä¸ºå‡†ç¡®ç†è§£é•¿è§†é¢‘çš„åŸºç¡€ã€‚ç°æœ‰å·¥ä½œè¯•å›¾é€æ­¥ç¼©å°æœç´¢èŒƒå›´ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä¾èµ–äºæ‰‹å·¥æœç´¢è¿‡ç¨‹ï¼Œç¼ºä¹ç«¯åˆ°ç«¯çš„ä¼˜åŒ–æ¥å­¦ä¹ æœ€ä½³æœç´¢ç­–ç•¥ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†TimeSearch-Rï¼Œå®ƒå°†æ—¶åºæœç´¢é‡æ–°å®šä¹‰ä¸ºäº¤æ›¿æ–‡æœ¬-è§†é¢‘æ€è€ƒï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ— ç¼é›†æˆè§†é¢‘å‰ªè¾‘çš„æœç´¢è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå°†å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ–¹æ³•ï¼ˆä¾‹å¦‚ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰ï¼‰åº”ç”¨äºè§†é¢‘æ¨ç†å¯èƒ½ä¼šäº§ç”Ÿæ— äººç›‘ç£çš„ä¸­é—´æœç´¢å†³ç­–ã€‚è¿™å¯¼è‡´è§†é¢‘å†…å®¹æ¢ç´¢ä¸è¶³å’Œé€»è¾‘æ¨ç†ä¸ä¸€è‡´ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¸¦æœ‰å®Œæ•´æ€§è‡ªæˆ‘éªŒè¯çš„GRPOï¼ˆGRPO-CSVï¼‰ï¼Œå®ƒä»äº¤æ›¿çš„æ¨ç†è¿‡ç¨‹ä¸­æ”¶é›†å·²æœç´¢çš„è§†é¢‘å¸§ï¼Œå¹¶ä½¿ç”¨ç›¸åŒçš„ç­–ç•¥æ¨¡å‹æ¥éªŒè¯æ‰€æœç´¢å¸§çš„å……åˆ†æ€§ï¼Œä»è€Œæé«˜è§†é¢‘æ¨ç†çš„å®Œæ•´æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸“é—¨ç”¨äºGRPO-CSVçš„SFTå†·å¯åŠ¨å’ŒRLè®­ç»ƒçš„æ•°æ®é›†ï¼Œè¿‡æ»¤æ‰å…·æœ‰å¼±æ—¶é—´ä¾èµ–æ€§çš„æ ·æœ¬ï¼Œä»¥å¢åŠ ä»»åŠ¡éš¾åº¦å¹¶æé«˜æ—¶åºæœç´¢èƒ½åŠ›ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒTimeSearch-Råœ¨æ—¶åºæœç´¢åŸºå‡†æµ‹è¯•ï¼ˆå¦‚Haystack-LVBenchå’ŒHaystack-Ego4Dï¼‰ä»¥åŠé•¿è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ï¼ˆå¦‚VideoMMEå’ŒMLVUï¼‰ä¸Šå–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒTimeSearch-Råœ¨LongVideoBenchä¸Šå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç›¸è¾ƒäºåŸºç¡€æ¨¡å‹Qwen2.5-VLæé«˜äº†4.1%ï¼Œç›¸è¾ƒäºé«˜çº§è§†é¢‘æ¨ç†æ¨¡å‹Video-R1æé«˜äº†2.0%ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Time-Search/TimeSearch-R">https://github.com/Time-Search/TimeSearch-R</a>è·å¾—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05489v1">PDF</a> 22 pages, 17 figures. Official code:   <a target="_blank" rel="noopener" href="https://github.com/Time-Search/TimeSearch-R">https://github.com/Time-Search/TimeSearch-R</a></p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†TimeSearch-Ræ¨¡å‹åœ¨é•¿æ—¶é—´è§†é¢‘ç†è§£ä¸­çš„åº”ç”¨ã€‚è¯¥æ¨¡å‹é‡‡ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’ŒåŸºäºç­–ç•¥çš„å†³ç­–è¿‡ç¨‹æ¥è§£å†³æ—¶ç©ºæœç´¢é—®é¢˜ï¼Œæé«˜æœç´¢æ•ˆç‡å¹¶ä¼˜åŒ–è§†é¢‘å†…å®¹çš„æ¢ç´¢ã€‚é€šè¿‡å¼•å…¥GRPOä¸å®Œæ•´æ€§è‡ªæˆ‘éªŒè¯ï¼ˆGRPO-CSVï¼‰ï¼Œæé«˜äº†è§†é¢‘æ¨ç†çš„å®Œæ•´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTimeSearch-Råœ¨æ—¶ç©ºæœç´¢å’Œé•¿æ—¶é—´è§†é¢‘ç†è§£ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>TimeSearch-Ræ¨¡å‹å°†æ—¶ç©ºæœç´¢é‡æ–°å®šä¹‰ä¸ºæ–‡æœ¬ä¸è§†é¢‘çš„äº¤äº’æ€è€ƒï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ— ç¼é›†æˆè§†é¢‘å‰ªè¾‘çš„æœç´¢è¿‡ç¨‹ã€‚</li>
<li>å¼•å…¥GRPOä¸å®Œæ•´æ€§è‡ªæˆ‘éªŒè¯ï¼ˆGRPO-CSVï¼‰æŠ€æœ¯ï¼Œç¡®ä¿è§†é¢‘å†…å®¹å¾—åˆ°å……åˆ†çš„æ¢ç´¢ï¼Œå¹¶æ”¹è¿›è§†é¢‘æ¨ç†çš„å®Œæ•´æ€§ã€‚</li>
<li>å»ºç«‹äº†ä¸“ä¸ºGRPO-CSVçš„SFTå†·å¯åŠ¨å’ŒRLè®­ç»ƒè®¾è®¡çš„ä¸“ç”¨æ•°æ®é›†ï¼Œæé«˜äº†ä»»åŠ¡éš¾åº¦å¹¶æé«˜äº†æ—¶ç©ºæœç´¢èƒ½åŠ›ã€‚</li>
<li>TimeSearch-Ræ¨¡å‹åœ¨å¤šä¸ªæ—¶ç©ºæœç´¢å’Œé•¿æ—¶é—´è§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼ŒåŒ…æ‹¬Haystack-LVBenchã€Haystack-Ego4Dã€VideoMMEå’ŒMLVUç­‰ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05489">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d349c5b463fa0201e28432599c4c0da8" align="middle">
<img src="https://picx.zhimg.com/v2-fa7a4b634eabf5ef9e309f7475505e63" align="middle">
<img src="https://picx.zhimg.com/v2-b53b5fae4567fd3e97ae6e6382266116" align="middle">
<img src="https://picx.zhimg.com/v2-4ea912590b67831f6147994f21102c0c" align="middle">
<img src="https://picx.zhimg.com/v2-5ca5b3e9d264cfb26f8420a6b97d9a09" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="PreResQ-R1-Towards-Fine-Grained-Rank-and-Score-Reinforcement-Learning-for-Visual-Quality-Assessment-via-Preference-Response-Disentangled-Policy-Optimization"><a href="#PreResQ-R1-Towards-Fine-Grained-Rank-and-Score-Reinforcement-Learning-for-Visual-Quality-Assessment-via-Preference-Response-Disentangled-Policy-Optimization" class="headerlink" title="PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning   for Visual Quality Assessment via Preference-Response Disentangled Policy   Optimization"></a>PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning   for Visual Quality Assessment via Preference-Response Disentangled Policy   Optimization</h2><p><strong>Authors:Zehui Feng, Tian Qiu, Tong Wu, Junxuan Li, Huayuan Xu, Ting Han</strong></p>
<p>Visual Quality Assessment (QA) seeks to predict human perceptual judgments of visual fidelity. While recent multimodal large language models (MLLMs) show promise in reasoning about image and video quality, existing approaches mainly rely on supervised fine-tuning or rank-only objectives, resulting in shallow reasoning, poor score calibration, and limited cross-domain generalization. We propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning framework that unifies absolute score regression and relative ranking consistency within a single reasoning-driven optimization scheme. Unlike prior QA methods, PreResQ-R1 introduces a dual-branch reward formulation that separately models intra-sample response coherence and inter-sample preference alignment, optimized via Group Relative Policy Optimization (GRPO). This design encourages fine-grained, stable, and interpretable chain-of-thought reasoning about perceptual quality. To extend beyond static imagery, we further design a global-temporal and local-spatial data flow strategy for Video Quality Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and 28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5 VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30% and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it produces human-aligned reasoning traces that reveal the perceptual cues underlying quality judgments. Code and model are available. </p>
<blockquote>
<p>è§†è§‰è´¨é‡è¯„ä¼°ï¼ˆQuality Assuranceï¼ŒQAï¼‰æ—¨åœ¨é¢„æµ‹äººç±»å¯¹è§†è§‰ä¿çœŸåº¦çš„æ„ŸçŸ¥åˆ¤æ–­ã€‚è™½ç„¶æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMultimodal Large Language Modelsï¼ŒMLLMsï¼‰åœ¨å›¾åƒå’Œè§†é¢‘è´¨é‡æ¨ç†æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºæœ‰ç›‘ç£çš„å¾®è°ƒæˆ–ä»…æ’åºç›®æ ‡ï¼Œå¯¼è‡´æ¨ç†æµ…æ˜¾ã€åˆ†æ•°æ ¡å‡†ä¸ä½³å’Œè·¨åŸŸæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚æˆ‘ä»¬æå‡ºäº†PreResQ-R1ï¼Œè¿™æ˜¯ä¸€ä¸ªåå¥½-å“åº”åˆ†è§£å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒåœ¨ä¸€ä¸ªå•ä¸€çš„æ¨ç†é©±åŠ¨ä¼˜åŒ–æ–¹æ¡ˆä¸­ç»Ÿä¸€äº†ç»å¯¹åˆ†æ•°å›å½’å’Œç›¸å¯¹æ’åä¸€è‡´æ€§ã€‚ä¸åŒäºå…ˆå‰çš„QAæ–¹æ³•ï¼ŒPreResQ-R1å¼•å…¥äº†ä¸€ç§åŒåˆ†æ”¯å¥–åŠ±å…¬å¼ï¼Œè¯¥å…¬å¼åˆ†åˆ«å»ºæ¨¡æ ·æœ¬å†…å“åº”ä¸€è‡´æ€§å’Œæ ·æœ¬é—´åå¥½å¯¹é½ï¼Œé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGroup Relative Policy Optimizationï¼ŒGRPOï¼‰è¿›è¡Œä¼˜åŒ–ã€‚è¿™ç§è®¾è®¡é¼“åŠ±å¯¹æ„ŸçŸ¥è´¨é‡è¿›è¡Œç²¾ç»†ã€ç¨³å®šã€å¯è§£é‡Šçš„é“¾å¼æ€ç»´æ¨ç†ã€‚ä¸ºäº†è¶…è¶Šé™æ€å›¾åƒï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ç§å…¨å±€æ—¶é—´å’Œå±€éƒ¨ç©ºé—´çš„æ•°æ®æµç­–ç•¥ï¼Œç”¨äºè§†é¢‘è´¨é‡è¯„ä¼°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»…åœ¨6Kå›¾åƒå’Œ28Kè§†é¢‘ä¸Šè¿›è¡Œå¼ºåŒ–å¾®è°ƒåï¼ŒPreResQ-R1åœ¨10ä¸ªå›¾åƒè´¨é‡è¯„ä¼°ï¼ˆIQAï¼‰å’Œ5ä¸ªè§†é¢‘è´¨é‡è¯„ä¼°ï¼ˆVQAï¼‰åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†æœ€å…ˆè¿›çš„æˆæœï¼Œæ— è®ºæ˜¯åœ¨SRCCè¿˜æ˜¯PLCCæŒ‡æ ‡ä¸‹å‡è¡¨ç°ä¼˜å¼‚ï¼Œåœ¨IQAä»»åŠ¡ä¸­åˆ†åˆ«æé«˜äº†5.30%å’Œ2.15%ã€‚é™¤äº†å®šé‡å¢ç›Šä¹‹å¤–ï¼Œå®ƒäº§ç”Ÿä¸äººç±»æ€ç»´è½¨è¿¹ç›¸ç¬¦çš„æ¨ç†ç»“æœï¼Œæ­ç¤ºäº†è´¨é‡åˆ¤æ–­èƒŒåçš„æ„ŸçŸ¥çº¿ç´¢ã€‚ä»£ç å’Œæ¨¡å‹å‡å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05393v1">PDF</a> 27 pages, 14 figures, under review as a conference paper</p>
<p><strong>æ‘˜è¦</strong></p>
<p>è§†è§‰è´¨é‡è¯„ä¼°ï¼ˆQAï¼‰æ—¨åœ¨é¢„æµ‹äººç±»å¯¹è§†è§‰ä¿çœŸåº¦çš„æ„ŸçŸ¥åˆ¤æ–­ã€‚è™½ç„¶æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨å›¾åƒå’Œè§†é¢‘è´¨é‡æ¨ç†æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºç›‘ç£å¾®è°ƒæˆ–ä»…æ’åç›®æ ‡ï¼Œå¯¼è‡´æ¨ç†æµ…æ˜¾ã€åˆ†æ•°æ ¡å‡†ä¸ä½³å’Œè·¨åŸŸæ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚æˆ‘ä»¬æå‡ºPreResQ-R1ï¼Œä¸€ä¸ªåå¥½ååº”åˆ†è§£å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒåœ¨ä¸€ä¸ªå•ä¸€æ¨ç†é©±åŠ¨çš„ä¼˜åŒ–æ–¹æ¡ˆä¸­ç»Ÿä¸€äº†ç»å¯¹åˆ†æ•°å›å½’å’Œç›¸å¯¹æ’åä¸€è‡´æ€§ã€‚ä¸åŒäºä»¥å¾€QAæ–¹æ³•çš„æ˜¯ï¼ŒPreResQ-R1å¼•å…¥äº†ä¸€ç§åŒåˆ†æ”¯å¥–åŠ±å…¬å¼ï¼Œè¯¥å…¬å¼åˆ†åˆ«å»ºæ¨¡æ ·æœ¬å†…å“åº”ä¸€è‡´æ€§å’Œæ ·æœ¬é—´åå¥½å¯¹é½ï¼Œé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œä¼˜åŒ–ã€‚è¿™ç§è®¾è®¡é¼“åŠ±å¯¹æ„ŸçŸ¥è´¨é‡è¿›è¡Œç²¾ç»†ã€ç¨³å®šä¸”å¯è§£é‡Šçš„é“¾å¼æ€ç»´æ¨ç†ã€‚ä¸ºäº†è¶…è¶Šé™æ€å›¾åƒï¼Œæˆ‘ä»¬è¿˜ä¸ºè§†é¢‘è´¨é‡è¯„ä¼°è®¾è®¡äº†ä¸€ç§å…¨å±€æ—¶é—´å’Œå±€éƒ¨ç©ºé—´æ•°æ®æµç­–ç•¥ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»…åœ¨6Kå›¾åƒå’Œ28Kè§†é¢‘ä¸Šè¿›è¡Œå¼ºåŒ–å¾®è°ƒåï¼ŒPreResQ-R1åœ¨10ä¸ªIQAå’Œ5ä¸ªVQAåŸºå‡†æµ‹è¯•ä¸‹è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æˆæœï¼ŒSRCCå’ŒPLCCæŒ‡æ ‡åˆ†åˆ«æé«˜äº†5.3%å’Œ2.15%ã€‚é™¤äº†å®šé‡æ”¶ç›Šå¤–ï¼Œå®ƒäº§ç”Ÿäº†ç¬¦åˆäººç±»æ€ç»´çš„æ¨ç†è½¨è¿¹ï¼Œæ­ç¤ºäº†è´¨é‡åˆ¤æ–­èƒŒåçš„æ„ŸçŸ¥çº¿ç´¢ã€‚ä»£ç å’Œæ¨¡å‹å¯ç”¨ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>è§†è§‰è´¨é‡è¯„ä¼°ï¼ˆVAï¼‰æ—¨åœ¨é¢„æµ‹äººç±»å¯¹è§†è§‰å†…å®¹çš„æ„ŸçŸ¥åˆ¤æ–­ã€‚</li>
<li>å½“å‰çš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨è§†è§‰è´¨é‡æ¨ç†æ–¹é¢å­˜åœ¨å±€é™æ€§ï¼Œå¦‚ä¾èµ–ç›‘ç£å¾®è°ƒã€æ’åç›®æ ‡å¯¼è‡´æµ…æ¨ç†ç­‰ã€‚</li>
<li>PreResQ-R1æ¡†æ¶é€šè¿‡ç»Ÿä¸€ç»å¯¹åˆ†æ•°å›å½’å’Œç›¸å¯¹æ’åä¸€è‡´æ€§æ¥æé«˜è§†è§‰è´¨é‡è¯„ä¼°çš„æ€§èƒ½ã€‚</li>
<li>PreResQ-R1é‡‡ç”¨åŒåˆ†æ”¯å¥–åŠ±å…¬å¼å»ºæ¨¡æ ·æœ¬å†…å“åº”ä¸€è‡´æ€§å’Œæ ·æœ¬é—´åå¥½å¯¹é½ã€‚</li>
<li>é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œå¾®è°ƒä¼˜åŒ–ï¼ŒPreResQ-R1åœ¨å›¾åƒå’Œè§†é¢‘è´¨é‡è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­å–å¾—æ˜¾è‘—æˆæœã€‚</li>
<li>PreResQ-R1ä¸ä»…æé«˜äº†å®šé‡æ€§èƒ½ï¼Œè¿˜æä¾›äº†æ­ç¤ºæ„ŸçŸ¥çº¿ç´¢çš„äººç±»å¯¹é½æ¨ç†è½¨è¿¹ã€‚</li>
<li>PreResQ-R1çš„ä»£ç å’Œæ¨¡å‹å¯ä¾›å…¬ä¼—ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05393">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3c124b6a18d422657e3b069a9d99766" align="middle">
<img src="https://picx.zhimg.com/v2-9a9d8d9df7eab59a2f52b0c02ea49a50" align="middle">
<img src="https://picx.zhimg.com/v2-6f12958db9572a16c3fedd72b7d08458" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="TeaRAG-A-Token-Efficient-Agentic-Retrieval-Augmented-Generation-Framework"><a href="#TeaRAG-A-Token-Efficient-Agentic-Retrieval-Augmented-Generation-Framework" class="headerlink" title="TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation   Framework"></a>TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation   Framework</h2><p><strong>Authors:Chao Zhang, Yuhao Wang, Derong Xu, Haoxin Zhang, Yuanjie Lyu, Yuhao Chen, Shuochen Liu, Tong Xu, Xiangyu Zhao, Yan Gao, Yao Hu, Enhong Chen</strong></p>
<p>Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment Large Language Modelsâ€™ (LLMs) reliability. For flexibility, agentic RAG employs autonomous, multi-round retrieval and reasoning to resolve queries. Although recent agentic RAG has improved via reinforcement learning, they often incur substantial token overhead from search and reasoning processes. This trade-off prioritizes accuracy over efficiency. To address this issue, this work proposes TeaRAG, a token-efficient agentic RAG framework capable of compressing both retrieval content and reasoning steps. 1) First, the retrieved content is compressed by augmenting chunk-based semantic retrieval with a graph retrieval using concise triplets. A knowledge association graph is then built from semantic similarity and co-occurrence. Finally, Personalized PageRank is leveraged to highlight key knowledge within this graph, reducing the number of tokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative Process-aware Direct Preference Optimization (IP-DPO) is proposed. Specifically, our reward function evaluates the knowledge sufficiency by a knowledge matching mechanism, while penalizing excessive reasoning steps. This design can produce high-quality preference-pair datasets, supporting iterative DPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the average Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on Llama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/TeaRAG">https://github.com/Applied-Machine-Learning-Lab/TeaRAG</a>. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯é æ€§ã€‚ä¸ºäº†çµæ´»æ€§ï¼Œè‡ªä¸»å‹RAGé‡‡ç”¨è‡ªä¸»ã€å¤šè½®æ£€ç´¢å’Œæ¨ç†æ¥è§£å†³æŸ¥è¯¢ã€‚å°½ç®¡æœ€è¿‘çš„è‡ªä¸»å‹RAGé€šè¿‡å¼ºåŒ–å­¦ä¹ å¾—åˆ°äº†æ”¹è¿›ï¼Œä½†å®ƒä»¬ç»å¸¸å› ä¸ºæœç´¢å’Œæ¨ç†è¿‡ç¨‹è€Œäº§ç”Ÿå¤§é‡çš„ä»¤ç‰Œå¼€é”€ï¼Œè¿™ç§æƒè¡¡ä¼˜å…ˆè€ƒè™‘å‡†ç¡®æ€§è€Œä¸æ˜¯æ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†TeaRAGï¼Œä¸€ä¸ªé«˜æ•ˆçš„ä»£ç†RAGæ¡†æ¶ï¼Œèƒ½å¤Ÿå‹ç¼©æ£€ç´¢å†…å®¹å’Œæ¨ç†æ­¥éª¤ã€‚é¦–å…ˆï¼Œé€šè¿‡å¢å¼ºåŸºäºåˆ†å—çš„è¯­ä¹‰æ£€ç´¢ä¸ç®€æ´ä¸‰å…ƒç»„çš„çŸ¥è¯†å›¾è°±æ£€ç´¢ï¼Œå¯¹æ£€ç´¢åˆ°çš„å†…å®¹è¿›è¡Œå‹ç¼©ã€‚ç„¶åï¼Œå»ºç«‹ä¸€ä¸ªåŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§å’Œå…±ç°çš„çŸ¥è¯†å…³è”å›¾ã€‚æœ€åï¼Œåˆ©ç”¨ä¸ªæ€§åŒ–PageRankç®—æ³•çªå‡ºæ˜¾ç¤ºè¯¥å›¾ä¸­çš„é‡è¦çŸ¥è¯†ï¼Œå‡å°‘æ¯æ¬¡æ£€ç´¢çš„ä»¤ç‰Œæ•°é‡ã€‚æ­¤å¤–ï¼Œä¸ºäº†å‡å°‘æ¨ç†æ­¥éª¤ï¼Œæˆ‘ä»¬æå‡ºäº†è¿­ä»£è¿‡ç¨‹æ„ŸçŸ¥çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆIP-DPOï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„å¥–åŠ±å‡½æ•°é€šè¿‡çŸ¥è¯†åŒ¹é…æœºåˆ¶æ¥è¯„ä¼°çŸ¥è¯†çš„å……è¶³æ€§ï¼ŒåŒæ—¶æƒ©ç½šè¿‡å¤šçš„æ¨ç†æ­¥éª¤ã€‚è¿™ç§è®¾è®¡å¯ä»¥äº§ç”Ÿé«˜è´¨é‡çš„åå¥½å¯¹æ•°æ®é›†ï¼Œæ”¯æŒè¿­ä»£DPOæ¥æé«˜æ¨ç†ç®€æ´æ€§ã€‚åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šï¼ŒTeaRAGåœ¨Llama3-8B-Instructå’ŒQwen2.5-14B-Instructä¸Šåˆ†åˆ«å°†å¹³å‡ç²¾ç¡®åŒ¹é…åº¦æé«˜äº†4%å’Œ2%ï¼ŒåŒæ—¶å‡å°‘äº†61%å’Œ59%çš„è¾“å‡ºä»¤ç‰Œã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/TeaRAG">https://github.com/Applied-Machine-Learning-Lab/TeaRAG</a>è·å–ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05385v1">PDF</a> 32 pages</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†Retrieval-Augmented Generationï¼ˆRAGï¼‰å¦‚ä½•åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯é æ€§ã€‚ä¸ºå¢å¼ºçµæ´»æ€§ï¼Œagentic RAGé‡‡ç”¨è‡ªä¸»ã€å¤šè½®æ£€ç´¢å’Œæ¨ç†æ¥è§£å†³æŸ¥è¯¢ã€‚å°½ç®¡æœ€è¿‘çš„agentic RAGé€šè¿‡å¼ºåŒ–å­¦ä¹ å¾—åˆ°äº†æ”¹è¿›ï¼Œä½†å®ƒä»¬ä»æœç´¢å’Œæ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿäº†å¤§é‡çš„ä»¤ç‰Œå¼€é”€ï¼Œè¿™ç§æƒè¡¡ä¼˜å…ˆè€ƒè™‘äº†å‡†ç¡®æ€§è€Œä¸æ˜¯æ•ˆç‡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TeaRAGï¼Œä¸€ä¸ªé«˜æ•ˆçš„agentic RAGæ¡†æ¶ï¼Œèƒ½å¤Ÿå‹ç¼©æ£€ç´¢å†…å®¹å’Œæ¨ç†æ­¥éª¤ã€‚é€šè¿‡åŸºäºå—çš„è¯­ä¹‰æ£€ç´¢ä¸å›¾å½¢æ£€ç´¢ç›¸ç»“åˆï¼Œä»¥åŠæ„å»ºçŸ¥è¯†å…³è”å›¾å’Œä½¿ç”¨ä¸ªæ€§åŒ–PageRankæ¥çªå‡ºå…³é”®çŸ¥è¯†ï¼Œä»è€Œå‹ç¼©æ£€ç´¢å†…å®¹ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†è¿­ä»£è¿‡ç¨‹æ„ŸçŸ¥çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆIP-DPOï¼‰æ¥å‡å°‘æ¨ç†æ­¥éª¤ã€‚TeaRAGåœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œå®ƒåœ¨æé«˜ç²¾ç¡®åŒ¹é…ç‡çš„åŒæ—¶ï¼Œå‡å°‘äº†è¾“å‡ºä»¤ç‰Œæ•°é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Retrieval-Augmented Generation (RAG) åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†å¢å¼ºLLMçš„å¯é æ€§ã€‚</li>
<li>Agentic RAGé‡‡ç”¨è‡ªä¸»ã€å¤šè½®æ£€ç´¢å’Œæ¨ç†è§£å†³æŸ¥è¯¢ï¼Œä»¥å¢å¼ºçµæ´»æ€§ã€‚</li>
<li>ç°æœ‰agentic RAGåœ¨å¼ºåŒ–å­¦ä¹ ä¸­è™½æœ‰æ‰€æå‡ï¼Œä½†åœ¨æœç´¢å’Œæ¨ç†è¿‡ç¨‹ä¸­äº§ç”Ÿå¤§é‡ä»¤ç‰Œå¼€é”€ã€‚</li>
<li>TeaRAGæ¡†æ¶æ—¨åœ¨è§£å†³è¿™ä¸€æ•ˆç‡é—®é¢˜ï¼Œé€šè¿‡å‹ç¼©æ£€ç´¢å†…å®¹å’Œæ¨ç†æ­¥éª¤å®ç°æ›´é«˜æ•ˆçš„RAGã€‚</li>
<li>TeaRAGç»“åˆäº†åŸºäºå—çš„è¯­ä¹‰æ£€ç´¢ä¸å›¾å½¢æ£€ç´¢ï¼Œå¹¶åˆ©ç”¨çŸ¥è¯†å…³è”å›¾å’Œä¸ªæ€§åŒ–PageRankæ¥çªå‡ºå…³é”®çŸ¥è¯†ã€‚</li>
<li>æå‡ºè¿­ä»£è¿‡ç¨‹æ„ŸçŸ¥çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆIP-DPOï¼‰ä»¥å‡å°‘æ¨ç†æ­¥éª¤ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTeaRAGåœ¨æå‡ç²¾ç¡®åŒ¹é…ç‡çš„åŒæ—¶å‡å°‘äº†è¾“å‡ºä»¤ç‰Œæ•°é‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4f459ba31f29269d66257d076413a560" align="middle">
<img src="https://picx.zhimg.com/v2-1b29364c956cb4984d6bdcc5267de8fd" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Reasoning-Is-All-You-Need-for-Urban-Planning-AI"><a href="#Reasoning-Is-All-You-Need-for-Urban-Planning-AI" class="headerlink" title="Reasoning Is All You Need for Urban Planning AI"></a>Reasoning Is All You Need for Urban Planning AI</h2><p><strong>Authors:Sijie Yang, Jiatong Li, Filip Biljecki</strong></p>
<p>AI has proven highly successful at urban planning analysis â€“ learning patterns from data to predict future conditions. The next frontier is AI-assisted decision-making: agents that recommend sites, allocate resources, and evaluate trade-offs while reasoning transparently about constraints and stakeholder values. Recent breakthroughs in reasoning AI â€“ CoT prompting, ReAct, and multi-agent collaboration frameworks â€“ now make this vision achievable.   This position paper presents the Agentic Urban Planning AI Framework for reasoning-capable planning agents that integrates three cognitive layers (Perception, Foundation, Reasoning) with six logic components (Analysis, Generation, Verification, Evaluation, Collaboration, Decision) through a multi-agents collaboration framework. We demonstrate why planning decisions require explicit reasoning capabilities that are value-based (applying normative principles), rule-grounded (guaranteeing constraint satisfaction), and explainable (generating transparent justifications) â€“ requirements that statistical learning alone cannot fulfill. We compare reasoning agents with statistical learning, present a comprehensive architecture with benchmark evaluation metrics, and outline critical research challenges. This framework shows how AI agents can augment human planners by systematically exploring solution spaces, verifying regulatory compliance, and deliberating over trade-offs transparently â€“ not replacing human judgment but amplifying it with computational reasoning capabilities. </p>
<blockquote>
<p>äººå·¥æ™ºèƒ½åœ¨åŸå¸‚è§„åˆ’åˆ†ææ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸâ€”â€”ä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ä»¥é¢„æµ‹æœªæ¥æƒ…å†µã€‚ä¸‹ä¸€ä¸ªå‰æ²¿é¢†åŸŸæ˜¯äººå·¥æ™ºèƒ½è¾…åŠ©å†³ç­–ï¼šä»£ç†å¯ä»¥æ¨èåœ°ç‚¹ã€åˆ†é…èµ„æºï¼Œå¹¶åœ¨çº¦æŸå’Œåˆ©ç›Šç›¸å…³è€…ä»·å€¼æ–¹é¢è¿›è¡Œåˆç†é€æ˜çš„æƒè¡¡è¯„ä¼°ã€‚æœ€è¿‘ï¼Œäººå·¥æ™ºèƒ½æ¨ç†æ–¹é¢çš„çªç ´â€”â€”è®¤çŸ¥è§¦å‘(CoT)ã€ååº”(ReAct)å’Œå¤šä»£ç†åä½œæ¡†æ¶â€”â€”ç°åœ¨ä½¿è¿™ä¸€æ„¿æ™¯æˆä¸ºå¯èƒ½ã€‚è¿™ç¯‡ç«‹åœºè®ºæ–‡æå‡ºäº†å…·æœ‰æ¨ç†èƒ½åŠ›çš„è§„åˆ’ä»£ç†çš„åŸå¸‚è§„åˆ’äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é›†æˆäº†ä¸‰ä¸ªè®¤çŸ¥å±‚æ¬¡ï¼ˆæ„ŸçŸ¥ã€åŸºç¡€ã€æ¨ç†ï¼‰å’Œå…­ä¸ªé€»è¾‘ç»„ä»¶ï¼ˆåˆ†æã€ç”Ÿæˆã€éªŒè¯ã€è¯„ä¼°ã€åä½œã€å†³ç­–ï¼‰ï¼Œé€šè¿‡å¤šä»£ç†åä½œæ¡†æ¶å®ç°ã€‚æˆ‘ä»¬è¯´æ˜äº†ä¸ºä»€ä¹ˆè§„åˆ’å†³ç­–éœ€è¦æ˜ç¡®çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™äº›èƒ½åŠ›æ˜¯åŸºäºä»·å€¼çš„ï¼ˆåº”ç”¨è§„èŒƒåŸåˆ™ï¼‰ã€åŸºäºè§„åˆ™çš„ï¼ˆä¿è¯çº¦æŸæ»¡è¶³ï¼‰å’Œå¯è§£é‡Šçš„ï¼ˆäº§ç”Ÿé€æ˜çš„æ­£å½“ç†ç”±ï¼‰â€”â€”è¿™äº›è¦æ±‚ç»Ÿè®¡å­¦ä¹ æ˜¯æ— æ³•å•ç‹¬å®ç°çš„ã€‚æˆ‘ä»¬å°†æ¨ç†ä»£ç†ä¸ç»Ÿè®¡å­¦ä¹ è¿›è¡Œäº†æ¯”è¾ƒï¼Œæå‡ºäº†å…¨é¢çš„æ¶æ„å’ŒåŸºå‡†è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶æ¦‚è¿°äº†å…³é”®çš„ç ”ç©¶æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å±•ç¤ºäº†äººå·¥æ™ºèƒ½ä»£ç†å¦‚ä½•å¢å¼ºäººç±»è§„åˆ’è€…çš„èƒ½åŠ›ï¼Œé€šè¿‡ç³»ç»Ÿåœ°æ¢ç´¢è§£å†³æ–¹æ¡ˆç©ºé—´ã€éªŒè¯æ³•è§„åˆè§„æ€§ä»¥åŠé€æ˜åœ°æƒè¡¡åˆ©å¼Šâ€”â€”å¹¶ä¸æ˜¯å–ä»£äººç±»åˆ¤æ–­ï¼Œè€Œæ˜¯ç”¨è®¡ç®—æ¨ç†èƒ½åŠ›æ¥å¢å¼ºäººç±»åˆ¤æ–­ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05375v1">PDF</a> Submitted to AAAI 2026 Workshop AI4UP</p>
<p><strong>Summary</strong><br>     äººå·¥æ™ºèƒ½åœ¨åŸå¸‚è§„åˆ’åˆ†ææ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œé€šè¿‡æ•°æ®å­¦ä¹ æ¨¡å¼æ¥é¢„æµ‹æœªæ¥çŠ¶å†µã€‚ä¸‹ä¸€é˜¶æ®µæ˜¯AIè¾…åŠ©å†³ç­–åˆ¶å®šï¼Œæ™ºèƒ½ä½“ä¼šæ¨èåœ°ç‚¹ã€åˆ†é…èµ„æºï¼Œå¹¶è¯„ä¼°æƒè¡¡ï¼ŒåŒæ—¶é€æ˜åœ°æ¨ç†çº¦æŸå’Œåˆ©ç›Šç›¸å…³è€…ä»·å€¼ã€‚æœ€æ–°çš„çªç ´å¦‚CoTæç¤ºã€ReActå’Œå¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ä½¿å¾—è¿™ä¸€æ„¿æ™¯æˆä¸ºå¯èƒ½ã€‚æœ¬ç«‹åœºæ–‡ä»¶æå‡ºäº†å…·æœ‰æ¨ç†èƒ½åŠ›çš„åŸå¸‚è§„åˆ’AIæ™ºèƒ½ä½“æ¡†æ¶ï¼Œèåˆäº†ä¸‰ä¸ªè®¤çŸ¥å±‚ï¼ˆæ„ŸçŸ¥ã€åŸºç¡€ã€æ¨ç†ï¼‰å’Œå…­ä¸ªé€»è¾‘ç»„ä»¶ï¼ˆåˆ†æã€ç”Ÿæˆã€éªŒè¯ã€è¯„ä¼°ã€åä½œã€å†³ç­–ï¼‰ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶å®ç°ã€‚æˆ‘ä»¬é˜è¿°äº†è§„åˆ’å†³ç­–ä¸ºä½•éœ€è¦æ˜ç¡®çš„æ¨ç†èƒ½åŠ›ï¼Œè¿™äº›èƒ½åŠ›æ˜¯åŸºäºä»·å€¼ï¼ˆåº”ç”¨è§„èŒƒåŸåˆ™ï¼‰ã€åŸºäºè§„åˆ™ï¼ˆä¿è¯çº¦æŸæ»¡è¶³ï¼‰å’Œå¯è§£é‡Šçš„ï¼ˆäº§ç”Ÿé€æ˜ç†ç”±ï¼‰ï¼Œè¿™äº›æ˜¯ä»…é€šè¿‡ç»Ÿè®¡å­¦ä¹ æ— æ³•å®ç°çš„ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†æ¨ç†æ™ºèƒ½ä½“ä¸ç»Ÿè®¡å­¦ä¹ ï¼Œç»™å‡ºäº†ä¸€ä¸ªç»¼åˆæ¶æ„å’ŒåŸºå‡†è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶æ¦‚è¿°äº†å…³é”®ç ”ç©¶æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å±•ç¤ºäº†AIæ™ºèƒ½ä½“å¦‚ä½•å¢å¼ºäººç±»è§„åˆ’è€…çš„èƒ½åŠ›ï¼Œé€šè¿‡ç³»ç»Ÿåœ°æ¢ç´¢è§£å†³æ–¹æ¡ˆç©ºé—´ã€éªŒè¯æ³•è§„åˆè§„æ€§ä»¥åŠé€æ˜åœ°æƒè¡¡å¾—å¤±ï¼Œè€Œéå–ä»£äººç±»åˆ¤æ–­ï¼Œè€Œæ˜¯ç”¨è®¡ç®—æ¨ç†èƒ½åŠ›æ¥å¢å¼ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>AIåœ¨åŸå¸‚è§„åˆ’åˆ†æé¢†åŸŸå·²ç»å±•ç°å‡ºå·¨å¤§æˆåŠŸï¼Œå¹¶é¢„æµ‹æœªæ¥è¶‹åŠ¿ã€‚</li>
<li>AIè¾…åŠ©å†³ç­–åˆ¶å®šæ˜¯ä¸‹ä¸€é˜¶æ®µçš„é‡è¦å‘å±•æ–¹å‘ï¼Œæ™ºèƒ½ä½“å…·å¤‡æ¨èåœ°ç‚¹ã€åˆ†é…èµ„æºå’Œè¯„ä¼°æƒè¡¡çš„èƒ½åŠ›ã€‚</li>
<li>æœ€è¿‘çš„çªç ´å¦‚CoTæç¤ºã€ReActå’Œå¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ä¸ºå®ç°è¿™ä¸€æ„¿æ™¯æä¾›äº†æŠ€æœ¯æ”¯æŒã€‚</li>
<li>è§„åˆ’å†³ç­–éœ€è¦åŸºäºä»·å€¼ã€è§„åˆ™å’Œå¯è§£é‡Šçš„æ˜ç¡®æ¨ç†èƒ½åŠ›ï¼Œè¿™äº›æ˜¯ä»…é€šè¿‡ç»Ÿè®¡å­¦ä¹ æ— æ³•å®ç°çš„ã€‚</li>
<li>æ¨ç†æ™ºèƒ½ä½“ä¸ç»Ÿè®¡å­¦ä¹ æœ‰é‡è¦åŒºåˆ«ï¼Œå‰è€…èƒ½å¤Ÿæ›´æ·±å…¥åœ°å¤„ç†å¤æ‚çš„è§„åˆ’å†³ç­–ä»»åŠ¡ã€‚</li>
<li>AgenticåŸå¸‚è§„åˆ’AIæ¡†æ¶èåˆäº†è®¤çŸ¥å±‚å’Œé€»è¾‘ç»„ä»¶ï¼Œæä¾›äº†å¤šæ™ºèƒ½ä½“åä½œçš„æœºä¼šã€‚</li>
<li>AIæ™ºèƒ½ä½“å¯å¢å¼ºäººç±»è§„åˆ’è€…çš„èƒ½åŠ›ï¼Œé€šè¿‡æ¢ç´¢è§£å†³æ–¹æ¡ˆç©ºé—´ã€éªŒè¯æ³•è§„åˆè§„æ€§ä»¥åŠé€æ˜åœ°æƒè¡¡å¾—å¤±æ¥è¾…åŠ©å†³ç­–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05375">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-130a1bd82754a7b269242222ac3d6def" align="middle">
<img src="https://picx.zhimg.com/v2-779cd5260e2c72f2a007aa22157473d4" align="middle">
<img src="https://picx.zhimg.com/v2-086d64de9bd162cbd77540d1ff8c208c" align="middle">
<img src="https://picx.zhimg.com/v2-17a80980a26fef06661f019e772075dc" align="middle">
<img src="https://picx.zhimg.com/v2-a45a7f2e87665556825dfa4a4a345b3f" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Reflective-Personalization-Optimization-A-Post-hoc-Rewriting-Framework-for-Black-Box-Large-Language-Models"><a href="#Reflective-Personalization-Optimization-A-Post-hoc-Rewriting-Framework-for-Black-Box-Large-Language-Models" class="headerlink" title="Reflective Personalization Optimization: A Post-hoc Rewriting Framework   for Black-Box Large Language Models"></a>Reflective Personalization Optimization: A Post-hoc Rewriting Framework   for Black-Box Large Language Models</h2><p><strong>Authors:Teqi Hao, Xioayu Tan, Shaojie Shi, Yinghui Xu, Xihe Qiu</strong></p>
<p>The personalization of black-box large language models (LLMs) is a critical yet challenging task. Existing approaches predominantly rely on context injection, where user history is embedded into the prompt to directly guide the generation process. However, this single-step paradigm imposes a dual burden on the model: generating accurate content while simultaneously aligning with user-specific styles. This often results in a trade-off that compromises output quality and limits precise control. To address this fundamental tension, we propose Reflective Personalization Optimization (RPO), a novel framework that redefines the personalization paradigm by decoupling content generation from alignment. RPO operates in two distinct stages: first, a base model generates a high-quality, generic response; then, an external reflection module explicitly rewrites this output to align with the userâ€™s preferences. This reflection module is trained using a two-stage process. Initially, supervised fine-tuning is employed on structured rewriting trajectories to establish a core personalized reasoning policy that models the transformation from generic to user-aligned responses. Subsequently, reinforcement learning is applied to further refine and enhance the quality of the personalized outputs. Comprehensive experiments on the LaMP benchmark demonstrate that RPO, by decoupling content generation from personalization, significantly outperforms state-of-the-art baselines. These findings underscore the superiority of explicit response shaping over implicit context injection. Moreover, RPO introduces an efficient, model-agnostic personalization layer that can be seamlessly integrated with any underlying base model, paving the way for a new and effective direction in user-centric generation scenarios. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸ªäººåŒ–æ˜¯ä¸€ä¸ªè‡³å…³é‡è¦ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºä¸Šä¸‹æ–‡æ³¨å…¥ï¼Œå³å°†ç”¨æˆ·å†å²åµŒå…¥åˆ°æç¤ºä¸­ä»¥ç›´æ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚ç„¶è€Œï¼Œè¿™ç§å•æ­¥èŒƒå¼å¯¹æ¨¡å‹æ–½åŠ äº†åŒé‡è´Ÿæ‹…ï¼šæ—¢è¦ç”Ÿæˆå‡†ç¡®çš„å†…å®¹ï¼Œåˆè¦åŒæ—¶ä¸ç”¨æˆ·ç‰¹å®šçš„é£æ ¼ä¿æŒä¸€è‡´ã€‚è¿™é€šå¸¸ä¼šå¯¼è‡´è¾“å‡ºè´¨é‡å’Œç²¾ç¡®æ§åˆ¶çš„å¦¥åã€‚ä¸ºäº†è§£å†³è¿™ç§åŸºæœ¬ç´§å¼ å…³ç³»ï¼Œæˆ‘ä»¬æå‡ºäº†åå°„ä¸ªæ€§åŒ–ä¼˜åŒ–ï¼ˆRPOï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é‡æ–°å®šä¹‰ä¸ªæ€§åŒ–èŒƒå¼çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡è§£è€¦å†…å®¹ç”Ÿæˆä¸å¯¹é½æ¥å®ç°ã€‚RPOåˆ†ä¸ºä¸¤ä¸ªç‹¬ç«‹é˜¶æ®µè¿è¡Œï¼šé¦–å…ˆï¼ŒåŸºç¡€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡ã€é€šç”¨çš„å“åº”ï¼›ç„¶åï¼Œå¤–éƒ¨åå°„æ¨¡å—æ˜¾å¼åœ°é‡å†™è¾“å‡ºï¼Œä»¥ç¬¦åˆç”¨æˆ·çš„åå¥½ã€‚è¯¥åå°„æ¨¡å—é‡‡ç”¨ä¸¤é˜¶æ®µè¿‡ç¨‹è¿›è¡Œè®­ç»ƒã€‚åˆæœŸï¼Œåœ¨ç»“æ„åŒ–çš„é‡å†™è½¨è¿¹ä¸Šå¯¹ç›‘ç£å¾®è°ƒï¼Œä»¥å»ºç«‹æ ¸å¿ƒä¸ªæ€§åŒ–æ¨ç†ç­–ç•¥ï¼Œè¯¥ç­–ç•¥èƒ½å¤Ÿå¯¹é€šç”¨å“åº”ä¸ç”¨æˆ·å¯¹é½å“åº”è¿›è¡Œå»ºæ¨¡ã€‚éšååº”ç”¨å¼ºåŒ–å­¦ä¹ æ¥è¿›ä¸€æ­¥ä¼˜åŒ–å’Œæé«˜ä¸ªæ€§åŒ–è¾“å‡ºçš„è´¨é‡ã€‚åœ¨LaMPåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒRPOé€šè¿‡è§£è€¦å†…å®¹ç”Ÿæˆä¸ä¸ªæ€§åŒ–ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰åŸºçº¿ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†æ˜¾å¼å“åº”å¡‘é€ ä¼˜äºéšå¼ä¸Šä¸‹æ–‡æ³¨å…¥ã€‚æ­¤å¤–ï¼ŒRPOå¼•å…¥äº†ä¸€ä¸ªé«˜æ•ˆã€æ¨¡å‹æ— å…³çš„ä¸ªäººåŒ–å±‚ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•åŸºç¡€æ¨¡å‹ä¸­ï¼Œä¸ºç”¨æˆ·ä¸ºä¸­å¿ƒçš„ç”Ÿæˆåœºæ™¯å¼€è¾Ÿäº†æ–°çš„æœ‰æ•ˆæ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05286v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ªæ€§åŒ–æ‰€é¢ä¸´çš„æŒ‘æˆ˜ä»¥åŠç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–è¯­å¢ƒæ³¨å…¥çš„é—®é¢˜ï¼Œæå‡ºäº†Reflective Personalization Optimizationï¼ˆRPOï¼‰æ¡†æ¶ï¼Œé€šè¿‡è§£è€¦å†…å®¹ç”Ÿæˆä¸å¯¹é½è¿‡ç¨‹æ¥å®ç°ä¸ªæ€§åŒ–ã€‚RPOåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆç”±åŸºç¡€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡é€šç”¨å“åº”ï¼Œç„¶åç”±å¤–éƒ¨åå°„æ¨¡å—æ˜¾å¼åœ°æ”¹å†™è¾“å‡ºä»¥ç¬¦åˆç”¨æˆ·åå¥½ã€‚åå°„æ¨¡å—é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œé¦–å…ˆä½¿ç”¨ç›‘ç£å¾®è°ƒå»ºç«‹ä¸ªæ€§åŒ–æ¨ç†ç­–ç•¥ï¼Œç„¶åä½¿ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥ä¼˜åŒ–ä¸ªæ€§åŒ–è¾“å‡ºçš„è´¨é‡ã€‚å®éªŒè¡¨æ˜ï¼ŒRPOæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œçªæ˜¾äº†æ˜¾å¼å“åº”å¡‘é€ ç›¸è¾ƒäºéšå¼è¯­å¢ƒæ³¨å…¥çš„ä¼˜åŠ¿ï¼ŒåŒæ—¶ä¸ºä»»ä½•åŸºç¡€æ¨¡å‹æä¾›äº†ä¸€ä¸ªé«˜æ•ˆã€æ¨¡å‹æ— å…³çš„ä¸ªäººåŒ–å±‚ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸ªäººåŒ–æ˜¯ä¸€é¡¹å…³é”®ä¸”å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦é€šè¿‡è¯­å¢ƒæ³¨å…¥æ¥å®ç°ä¸ªæ€§åŒ–ï¼Œä½†è¿™ç§æ–¹æ³•å­˜åœ¨è¾“å‡ºè´¨é‡å’Œæ§åˆ¶ç²¾åº¦çš„å¦¥åé—®é¢˜ã€‚</li>
<li>RPOæ¡†æ¶é€šè¿‡è§£è€¦å†…å®¹ç”Ÿæˆå’Œå¯¹é½è¿‡ç¨‹æ¥å®ç°ä¸ªæ€§åŒ–ï¼Œåˆ†ä¸ºåŸºç¡€æ¨¡å‹ç”Ÿæˆé€šç”¨å“åº”å’Œåå°„æ¨¡å—æ”¹å†™è¾“å‡ºä¸¤ä¸ªé˜¶æ®µã€‚</li>
<li>åå°„æ¨¡å—é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>RPOåœ¨LaMPåŸºå‡†æµ‹è¯•ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>RPOå¼ºè°ƒäº†æ˜¾å¼å“åº”å¡‘é€ ç›¸è¾ƒäºéšå¼è¯­å¢ƒæ³¨å…¥çš„ä¼˜åŠ¿ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05286">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eabbf3fe0cc0efc83c0aee5e82bbc427" align="middle">
<img src="https://picx.zhimg.com/v2-dd8bce5a894dd8be407132f127bc577e" align="middle">
<img src="https://picx.zhimg.com/v2-21186bdde8ced1ba25072a402e890dff" align="middle">
<img src="https://picx.zhimg.com/v2-a5d054589f2d17f5ed747e3596568f36" align="middle">
<img src="https://picx.zhimg.com/v2-2d11baa05d60ba55ef9ca944ff2a092d" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Pluralistic-Behavior-Suite-Stress-Testing-Multi-Turn-Adherence-to-Custom-Behavioral-Policies"><a href="#Pluralistic-Behavior-Suite-Stress-Testing-Multi-Turn-Adherence-to-Custom-Behavioral-Policies" class="headerlink" title="Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to   Custom Behavioral Policies"></a>Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to   Custom Behavioral Policies</h2><p><strong>Authors:Prasoon Varshney, Makesh Narsimhan Sreedhar, Liwei Jiang, Traian Rebedea, Christopher Parisien</strong></p>
<p>Large language models (LLMs) are typically aligned to a universal set of safety and usage principles intended for broad public acceptability. Yet, real-world applications of LLMs often take place within organizational ecosystems shaped by distinctive corporate policies, regulatory requirements, use cases, brand guidelines, and ethical commitments. This reality highlights the need for rigorous and comprehensive evaluation of LLMs with pluralistic alignment goals, an alignment paradigm that emphasizes adaptability to diverse user values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE (PBSUITE), a dynamic evaluation suite designed to systematically assess LLMsâ€™ capacity to adhere to pluralistic alignment specifications in multi-turn, interactive conversations. PBSUITE consists of (1) a diverse dataset of 300 realistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic evaluation framework for stress-testing model compliance with custom behavioral specifications under adversarial conditions. Using PBSUITE, We find that leading open- and closed-source LLMs maintain robust adherence to behavioral policies in single-turn settings (less than 4% failure rates), but their compliance weakens substantially in multi-turn adversarial interactions (up to 84% failure rates). These findings highlight that existing model alignment and safety moderation methods fall short in coherently enforcing pluralistic behavioral policies in real-world LLM interactions. Our work contributes both the dataset and analytical framework to support future research toward robust and context-aware pluralistic alignment techniques. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é€šå¸¸ä¸ä¸€å¥—æ™®éçš„å®‰å…¨æ€§å’Œä½¿ç”¨åŸåˆ™ç›¸ä¸€è‡´ï¼Œæ—¨åœ¨å®ç°å¹¿æ³›çš„å…¬ä¼—æ¥å—æ€§ã€‚ç„¶è€Œï¼ŒLLMåœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨å¾€å¾€å‘ç”Ÿåœ¨ç”±ç‹¬ç‰¹çš„å…¬å¸æ”¿ç­–ã€ç›‘ç®¡è¦æ±‚ã€ä½¿ç”¨æ¡ˆä¾‹ã€å“ç‰ŒæŒ‡å—å’Œé“å¾·æ‰¿è¯ºæ‰€å¡‘é€ çš„ç»„ç»‡ç”Ÿæ€ç³»ç»Ÿå†…ã€‚è¿™ç§ç°å®æƒ…å†µçªæ˜¾äº†å¯¹å…·æœ‰å¤šå…ƒåŒ–å¯¹é½ç›®æ ‡çš„LLMè¿›è¡Œä¸¥è°¨å’Œå…¨é¢è¯„ä¼°çš„å¿…è¦æ€§ï¼Œä»¥åŠå¯¹é½èŒƒå¼éœ€è¦å¼ºè°ƒå¯¹å¤šæ ·åŒ–å’Œç”¨æˆ·ä»·å€¼å’Œéœ€æ±‚çš„é€‚åº”æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¤šå…ƒåŒ–è¡Œä¸ºå¥—ä»¶ï¼ˆPBSUITEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠ¨æ€è¯„ä¼°å¥—ä»¶ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°LLMåœ¨å¤šè½®äº¤äº’å¼å¯¹è¯ä¸­éµå®ˆå¤šå…ƒåŒ–å¯¹é½è§„èŒƒçš„èƒ½åŠ›ã€‚PBSUITEåŒ…æ‹¬ï¼ˆ1ï¼‰åŸºäº30ä¸ªè¡Œä¸šçš„300ä¸ªç°å®LLMè¡Œä¸ºæ”¿ç­–çš„å¤šæ ·æ•°æ®é›†ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªåŠ¨æ€è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºåœ¨æ•Œå¯¹æ¡ä»¶ä¸‹å¯¹æ¨¡å‹éµå®ˆç‰¹å®šè¡Œä¸ºè§„èŒƒè¿›è¡Œå‹åŠ›æµ‹è¯•ã€‚é€šè¿‡ä½¿ç”¨PBSUITEï¼Œæˆ‘ä»¬å‘ç°é¢†å…ˆçš„å¼€æºå’Œé—­æºLLMåœ¨å•è½®è®¾ç½®ï¼ˆå¤±è´¥ç‡ä½äº4%ï¼‰ä¸­èƒ½å¤Ÿä¿æŒå¯¹è¡Œä¸ºæ”¿ç­–çš„ç¨³å¥éµå¾ªï¼Œä½†åœ¨å¤šè½®æ•Œå¯¹äº¤äº’ä¸­ï¼ˆå¤±è´¥ç‡é«˜è¾¾84%ï¼‰ï¼Œå…¶åˆè§„æ€§å¤§å¤§å‡å¼±ã€‚è¿™äº›å‘ç°è¡¨æ˜ï¼Œç°æœ‰çš„æ¨¡å‹å¯¹é½å’Œå®‰å…¨è°ƒåœæ–¹æ³•åœ¨ä¸è¿è´¯åœ°æ‰§è¡Œå¤šå…ƒåŒ–è¡Œä¸ºæ”¿ç­–æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨ç°å®ä¸–ç•Œçš„LLMäº¤äº’ä¸­ã€‚æˆ‘ä»¬çš„å·¥ä½œæ—¢æä¾›äº†æ•°æ®é›†ä¹Ÿæä¾›äº†åˆ†ææ¡†æ¶ï¼Œä»¥æ”¯æŒæœªæ¥ç ”ç©¶å®ç°ç¨³å¥å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¤šå…ƒåŒ–å¯¹é½æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05018v1">PDF</a> Accepted at the Multi-Turn Interactions workshop at the 39th   Conference on Neural Information Processing Systems (NeurIPS 2025)</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸éµå¾ªä¸€å¥—é€šç”¨çš„å®‰å…¨å’Œä½¿ç”¨åŸåˆ™ï¼Œä»¥å¹¿æ³›çš„å…¬ä¼—æ¥å—åº¦ä¸ºè®¾è®¡ç›®æ ‡ã€‚ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­ï¼ŒLLMså¾€å¾€éœ€è¦åœ¨ç»„ç»‡ç”Ÿæ€ç³»ç»Ÿä¸­è¿ä½œï¼Œè¿™äº›ç³»ç»Ÿå—åˆ°ç‹¬ç‰¹çš„å…¬å¸æ”¿ç­–ã€ç›‘ç®¡è¦æ±‚ã€åº”ç”¨åœºæ™¯ã€å“ç‰Œå‡†åˆ™å’Œé“å¾·æ‰¿è¯ºçš„å½±å“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¯¹LLMsè¿›è¡Œå¤šå…ƒåŒ–å¯¹é½ç›®æ ‡çš„ä¸¥è°¨å’Œå…¨é¢è¯„ä¼°ã€‚æœ¬ç ”ç©¶æå‡ºäº†PLURALISTIC BEHAVIOR SUITEï¼ˆPBSUITEï¼‰ï¼Œä¸€ä¸ªåŠ¨æ€è¯„ä¼°å¥—ä»¶ï¼Œæ—¨åœ¨ç³»ç»Ÿåœ°è¯„ä¼°LLMsé€‚åº”å¤šå…ƒåŒ–ç”¨æˆ·ä»·å€¼è§‚å’Œéœ€æ±‚çš„å¯¹é½è§„æ ¼çš„èƒ½åŠ›ã€‚PBSUITEåŒ…æ‹¬ï¼ˆ1ï¼‰ä¸€ä¸ªåŒ…å«æ¥è‡ªä¸‰åä¸ªè¡Œä¸šçš„ä¸‰ç™¾ç§å®é™…LLMè¡Œä¸ºæ”¿ç­–çš„å¤šæ ·æ•°æ®é›†ï¼›ï¼ˆ2ï¼‰ä¸€ä¸ªåŠ¨æ€è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºåœ¨æ•Œå¯¹æ¡ä»¶ä¸‹æµ‹è¯•æ¨¡å‹éµå®ˆå®šåˆ¶çš„è¡Œä¸ºè§„æ ¼çš„èƒ½åŠ›ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨å•è½®è®¾ç½®ä¸­ï¼Œé¢†å…ˆçš„å¼€æºå’Œé—­æºLLMsèƒ½å¤Ÿå¾ˆå¥½åœ°éµå®ˆè¡Œä¸ºæ”¿ç­–ï¼ˆå¤±è´¥ç‡ä¸åˆ°4%ï¼‰ï¼Œä½†åœ¨å¤šè½®æ•Œå¯¹äº’åŠ¨ä¸­ï¼Œå®ƒä»¬çš„åˆè§„æ€§ä¼šå¤§å¤§å‡å¼±ï¼ˆå¤±è´¥ç‡é«˜è¾¾84%ï¼‰ã€‚è¿™è¡¨æ˜ç°æœ‰çš„æ¨¡å‹å¯¹é½å’Œå®‰å…¨è°ƒèŠ‚æ–¹æ³•åœ¨çœŸå®ä¸–ç•Œçš„LLMäº’åŠ¨ä¸­æ— æ³•æœ‰æ•ˆåœ°æ‰§è¡Œå¤šå…ƒåŒ–è¡Œä¸ºæ”¿ç­–ã€‚æˆ‘ä»¬çš„å·¥ä½œä¸ºæ”¯æŒæœªæ¥ç ”ç©¶ç¨³å¥å’Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¤šå…ƒåŒ–å¯¹é½æŠ€æœ¯æä¾›äº†æ•°æ®é›†å’Œåˆ†ææ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç°å®ä¸–ç•Œåº”ç”¨ä¸­éœ€è¦åœ¨ç»„ç»‡ç”Ÿæ€ç³»ç»Ÿä¸­è¿ä½œï¼Œéœ€è¦é€‚åº”ä¸åŒçš„å…¬å¸æ”¿ç­–ã€ç›‘ç®¡è¦æ±‚ç­‰ã€‚</li>
<li>LLMsçš„è¯„ä¼°éœ€è¦å¼ºè°ƒé€‚åº”å¤šå…ƒåŒ–ç”¨æˆ·ä»·å€¼è§‚å’Œéœ€æ±‚çš„å¯¹é½ç›®æ ‡ã€‚</li>
<li>PLURALISTIC BEHAVIOR SUITEï¼ˆPBSUITEï¼‰æ˜¯ä¸€ä¸ªåŠ¨æ€è¯„ä¼°å¥—ä»¶ï¼Œç”¨äºè¯„ä¼°LLMséµå®ˆå¤šå…ƒåŒ–å¯¹é½è§„æ ¼çš„èƒ½åŠ›ã€‚</li>
<li>PBSUITEåŒ…æ‹¬ä¸€ä¸ªåŒ…å«å¤šä¸ªè¡Œä¸šçš„å®é™…LLMè¡Œä¸ºæ”¿ç­–æ•°æ®é›†å’Œä¸€ä¸ªç”¨äºæµ‹è¯•æ¨¡å‹åˆè§„æ€§çš„åŠ¨æ€è¯„ä¼°æ¡†æ¶ã€‚</li>
<li>åœ¨å•è½®è®¾ç½®ä¸­ï¼ŒLLMsçš„åˆè§„æ€§è¾ƒé«˜ï¼Œä½†åœ¨å¤šè½®æ•Œå¯¹äº’åŠ¨ä¸­ï¼Œå…¶åˆè§„æ€§ä¼šæ˜¾è‘—ä¸‹é™ã€‚</li>
<li>ç°æœ‰çš„æ¨¡å‹å¯¹é½å’Œå®‰å…¨è°ƒèŠ‚æ–¹æ³•åœ¨å¤šè½®äº’åŠ¨ä¸­å¯èƒ½æ— æ³•æœ‰æ•ˆæ‰§è¡Œå¤šå…ƒåŒ–è¡Œä¸ºæ”¿ç­–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05018">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e283fae1fa4f2b5187649c1bc6f59e6c" align="middle">
<img src="https://picx.zhimg.com/v2-d39127020b212985863fa53e6ef101ab" align="middle">
<img src="https://picx.zhimg.com/v2-13eeb06be4a7d2a52ace57ce3e0886c5" align="middle">
<img src="https://picx.zhimg.com/v2-f15d6753fea88799e5c71ea29d036867" align="middle">
<img src="https://picx.zhimg.com/v2-ee30389d6c9f23fc38e0d19ae44cc150" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="You-Need-Reasoning-to-Learn-Reasoning-The-Limitations-of-Label-Free-RL-in-Weak-Base-Models"><a href="#You-Need-Reasoning-to-Learn-Reasoning-The-Limitations-of-Label-Free-RL-in-Weak-Base-Models" class="headerlink" title="You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL   in Weak Base Models"></a>You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL   in Weak Base Models</h2><p><strong>Authors:Shuvendu Roy, Hossein Hajimirsadeghi, Mengyao Zhai, Golnoosh Samei</strong></p>
<p>Recent advances in large language models have demonstrated the promise of unsupervised reinforcement learning (RL) methods for enhancing reasoning capabilities without external supervision. However, the generalizability of these label-free RL approaches to smaller base models with limited reasoning capabilities remains unexplored. In this work, we systematically investigate the performance of label-free RL methods across different model sizes and reasoning strengths, from 0.5B to 7B parameters. Our empirical analysis reveals critical limitations: label-free RL is highly dependent on the base modelâ€™s pre-existing reasoning capability, with performance often degrading below baseline levels for weaker models. We find that smaller models fail to generate sufficiently long or diverse chain-of-thought reasoning to enable effective self-reflection, and that training data difficulty plays a crucial role in determining success. To address these challenges, we propose a simple yet effective method for label-free RL that utilizes curriculum learning to progressively introduce harder problems during training and mask no-majority rollouts during training. Additionally, we introduce a data curation pipeline to generate samples with predefined difficulty. Our approach demonstrates consistent improvements across all model sizes and reasoning capabilities, providing a path toward more robust unsupervised RL that can bootstrap reasoning abilities in resource-constrained models. We make our code available at <a target="_blank" rel="noopener" href="https://github.com/BorealisAI/CuMa">https://github.com/BorealisAI/CuMa</a> </p>
<blockquote>
<p>æœ€è¿‘çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›å±•æ˜¾ç¤ºå‡ºæ— ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•åœ¨å¢å¼ºæ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ï¼Œè€Œæ— éœ€å¤–éƒ¨ç›‘ç£ã€‚ç„¶è€Œï¼Œè¿™äº›æ— æ ‡ç­¾RLæ–¹æ³•å¯¹äºå…·æœ‰æœ‰é™æ¨ç†èƒ½åŠ›çš„å°å‹åŸºç¡€æ¨¡å‹çš„é€šç”¨æ€§ä»ç„¶æœªè¢«æ¢ç´¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°ç ”ç©¶äº†æ— æ ‡ç­¾RLæ–¹æ³•åœ¨ä¸åŒæ¨¡å‹å¤§å°å’Œæ¨ç†èƒ½åŠ›æ–¹é¢çš„æ€§èƒ½ï¼Œä»0.5Båˆ°7Bå‚æ•°ã€‚æˆ‘ä»¬çš„å®è¯åˆ†ææ­ç¤ºäº†å…³é”®å±€é™æ€§ï¼šæ— æ ‡ç­¾RLé«˜åº¦ä¾èµ–äºåŸºç¡€æ¨¡å‹å·²æœ‰çš„æ¨ç†èƒ½åŠ›ï¼Œå¯¹äºè¾ƒå¼±çš„æ¨¡å‹ï¼Œå…¶æ€§èƒ½é€šå¸¸ä¼šä½äºåŸºçº¿æ°´å¹³ã€‚æˆ‘ä»¬å‘ç°ï¼Œè¾ƒå°çš„æ¨¡å‹æ— æ³•ç”Ÿæˆè¶³å¤Ÿé•¿æˆ–å¤šæ ·åŒ–çš„æ€ç»´é“¾æ¨ç†ï¼Œä»¥å®ç°æœ‰æ•ˆçš„è‡ªæˆ‘åæ€ï¼Œè€Œä¸”è®­ç»ƒæ•°æ®çš„éš¾åº¦åœ¨å†³å®šæˆåŠŸä¸å¦æ–¹é¢èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•æœ‰æ•ˆçš„æ— æ ‡ç­¾RLæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è¯¾ç¨‹å­¦ä¹ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å¼•å…¥æ›´éš¾çš„é—®é¢˜ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å±è”½æ— å¤šæ•°æ»šåŠ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ•°æ®æ•´ç†æµç¨‹æ¥ç”Ÿæˆå…·æœ‰é¢„å®šéš¾åº¦çš„æ ·æœ¬ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰æ¨¡å‹å¤§å°å’Œæ¨ç†èƒ½åŠ›æ–¹é¢éƒ½è¡¨ç°å‡ºäº†ä¸€è‡´çš„æ”¹è¿›ï¼Œä¸ºæ›´ç¨³å¥çš„æ— ç›‘ç£RLæä¾›äº†é€”å¾„ï¼Œè¯¥RLå¯ä»¥åœ¨èµ„æºå—é™çš„æ¨¡å‹ä¸­å¼•å¯¼æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/BorealisAI/CuMa%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/BorealisAI/CuMaä¸Šè·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04902v1">PDF</a> 39th Conference on Neural Information Processing Systems (NeurIPS   2025) Workshop: MATH-AI</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å±•å±•ç°äº†æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ åœ¨æå‡æ¨ç†èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ï¼Œä½†å¯¹äºè¾ƒå°åŸºç¡€æ¨¡å‹çš„é€šç”¨æ€§å°šæœªæ¢ç´¢ã€‚æœ¬ç ”ç©¶ç³»ç»Ÿåœ°ç ”ç©¶äº†æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ åœ¨ä¸åŒè§„æ¨¡å’Œæ¨ç†èƒ½åŠ›çš„æ¨¡å‹ä¸­çš„åº”ç”¨ï¼Œå‘ç°å…¶ä¸¥é‡ä¾èµ–äºåŸºç¡€æ¨¡å‹çš„é¢„å­˜æ¨ç†èƒ½åŠ›ï¼Œå¯¹è¾ƒå¼±æ¨¡å‹çš„æ€§èƒ½å¸¸ä½äºåŸºçº¿æ°´å¹³ã€‚ä¸ºè§£å†³æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºäº†åˆ©ç”¨è¯¾ç¨‹å­¦ä¹ é€æ­¥å¼•å…¥éš¾é¢˜å’Œè®­ç»ƒæ—¶æ©ç›–æ— å¤šæ•°ç­–ç•¥æ»šåŠ¨çš„ç®€å•æœ‰æ•ˆæ–¹æ³•ï¼Œå¹¶å¼•å…¥æ•°æ®ç®¡é“ç”Ÿæˆé¢„è®¾éš¾åº¦çš„æ ·æœ¬ã€‚æ­¤æ–¹æ³•åœ¨æ‰€æœ‰æ¨¡å‹å’Œæ¨ç†èƒ½åŠ›ä¸Šå‡è¡¨ç°å‡ºä¸€è‡´çš„æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ åœ¨æå‡æ¨ç†èƒ½åŠ›æ–¹é¢å±•ç°å‡ºæ½œåŠ›ï¼Œä½†å¯¹è¾ƒå°åŸºç¡€æ¨¡å‹çš„é€šç”¨æ€§ä»éœ€æ¢ç´¢ã€‚</li>
<li>æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ çš„æ€§èƒ½ä¸¥é‡ä¾èµ–äºåŸºç¡€æ¨¡å‹çš„é¢„å­˜æ¨ç†èƒ½åŠ›ã€‚</li>
<li>è¾ƒå°æ¨¡å‹åœ¨ç”Ÿæˆè¶³å¤Ÿé•¿æˆ–å¤šæ ·åŒ–çš„æ€ç»´é“¾æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œå½±å“è‡ªæˆ‘åæ€èƒ½åŠ›ã€‚</li>
<li>è®­ç»ƒæ•°æ®çš„éš¾åº¦å¯¹æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ çš„æˆåŠŸèµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†åˆ©ç”¨è¯¾ç¨‹å­¦ä¹ çš„æ–¹æ³•ï¼Œé€æ­¥å¼•å…¥éš¾é¢˜ï¼Œæ”¹è¿›äº†æ— ç›‘ç£å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>å¼•å…¥æ•°æ®ç®¡é“ç”Ÿæˆé¢„è®¾éš¾åº¦çš„æ ·æœ¬æœ‰åŠ©äºæé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a3f5981a9c46b74ba5fd200f7f17cb4c" align="middle">
<img src="https://picx.zhimg.com/v2-3f9543a8925d8b39dbb0d13fa5ee586f" align="middle">
<img src="https://picx.zhimg.com/v2-0b8e411842c62e6268f50becfdeb0b48" align="middle">
<img src="https://picx.zhimg.com/v2-50a84dd1437fba3068b8c2dca8117ed5" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Real-Time-Reasoning-Agents-in-Evolving-Environments"><a href="#Real-Time-Reasoning-Agents-in-Evolving-Environments" class="headerlink" title="Real-Time Reasoning Agents in Evolving Environments"></a>Real-Time Reasoning Agents in Evolving Environments</h2><p><strong>Authors:Yule Wen, Yixin Ye, Yanzhe Zhang, Diyi Yang, Hao Zhu</strong></p>
<p>Agents in the real world must make not only logical but also timely judgments. This requires continuous awareness of the dynamic environment: hazards emerge, opportunities arise, and other agents act, while the agentâ€™s reasoning is still unfolding. Despite advances in language model reasoning, existing approaches fail to account for this dynamic nature. We introduce real-time reasoning as a new problem formulation for agents in evolving environments and build Real-Time Reasoning Gym to demonstrate it. We study two paradigms for deploying language models in agents: (1) reactive agents, which employ language models with bounded reasoning computation for rapid responses, and (2) planning agents, which allow extended reasoning computation for complex problems. Our experiments show that even state-of-the-art models struggle with making logical and timely judgments in either paradigm. To address this limitation, we propose AgileThinker, which simultaneously engages both reasoning paradigms. AgileThinker consistently outperforms agents engaging only one reasoning paradigm as the task difficulty and time pressure rise, effectively balancing reasoning depth and response latency. Our work establishes real-time reasoning as a critical testbed for developing practical agents and provides a foundation for research in temporally constrained AI systems, highlighting a path toward real-time capable agents. </p>
<blockquote>
<p>ç°å®ä¸–ç•Œä¸­çš„æ™ºèƒ½ä½“å¿…é¡»åšå‡ºä¸ä»…é€»è¾‘ä¸Šæ­£ç¡®è€Œä¸”åŠæ—¶çš„åˆ¤æ–­ã€‚è¿™éœ€è¦æŒç»­å¯¹åŠ¨æ€ç¯å¢ƒçš„è®¤çŸ¥ï¼šå±é™©å‡ºç°ï¼Œæœºä¼šå‡ºç°ï¼Œå…¶ä»–æ™ºèƒ½ä½“é‡‡å–è¡ŒåŠ¨ï¼Œè€Œæ™ºèƒ½ä½“çš„æ¨ç†ä»åœ¨å±•å¼€ã€‚å°½ç®¡è¯­è¨€æ¨¡å‹æ¨ç†å–å¾—äº†è¿›å±•ï¼Œä½†ç°æœ‰æ–¹æ³•æ— æ³•è§£é‡Šè¿™ç§åŠ¨æ€ç‰¹æ€§ã€‚æˆ‘ä»¬å¼•å…¥å®æ—¶æ¨ç†ä½œä¸ºä¸æ–­å‘å±•çš„ç¯å¢ƒä¸­æ™ºèƒ½ä½“çš„æ–°é—®é¢˜è¡¨è¿°ï¼Œå¹¶æ„å»ºå®æ—¶æ¨ç†ä½“è‚²é¦†è¿›è¡Œæ¼”ç¤ºã€‚æˆ‘ä»¬ç ”ç©¶äº†åœ¨æ™ºèƒ½ä½“ä¸­éƒ¨ç½²è¯­è¨€æ¨¡å‹çš„ä¸¤ç§èŒƒå¼ï¼šï¼ˆ1ï¼‰ååº”å¼æ™ºèƒ½ä½“ï¼Œå®ƒé‡‡ç”¨å…·æœ‰æœ‰é™æ¨ç†è®¡ç®—çš„è¯­è¨€æ¨¡å‹è¿›è¡Œå¿«é€Ÿå“åº”ï¼›ï¼ˆ2ï¼‰è§„åˆ’å¼æ™ºèƒ½ä½“ï¼Œå®ƒå…è®¸è¿›è¡Œæ‰©å±•çš„æ¨ç†è®¡ç®—ä»¥è§£å†³å¤æ‚é—®é¢˜ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œå³ä½¿åœ¨ä¸¤ç§èŒƒå¼ä¸­ï¼Œæœ€å…ˆè¿›çš„æ¨¡å‹åœ¨åšå‡ºé€»è¾‘å’ŒåŠæ—¶åˆ¤æ–­æ—¶ä¹Ÿé¢ä¸´å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†AgileThinkerï¼Œå®ƒåŒæ—¶é‡‡ç”¨è¿™ä¸¤ç§æ¨ç†èŒƒå¼ã€‚éšç€ä»»åŠ¡éš¾åº¦å’Œæ—¶é—´å‹åŠ›çš„å¢åŠ ï¼ŒAgileThinkerå§‹ç»ˆè¡¨ç°å‡ºä¼˜äºä»…é‡‡ç”¨ä¸€ç§æ¨ç†èŒƒå¼çš„æ™ºèƒ½ä½“ï¼Œåœ¨æ¨ç†æ·±åº¦å’Œå“åº”å»¶è¿Ÿä¹‹é—´è¾¾åˆ°äº†æœ‰æ•ˆçš„å¹³è¡¡ã€‚æˆ‘ä»¬çš„å·¥ä½œå»ºç«‹äº†å®æ—¶æ¨ç†ä½œä¸ºå¼€å‘å®ç”¨æ™ºèƒ½ä½“çš„å…³é”®æµ‹è¯•å¹³å°ï¼Œä¸ºæ—¶é—´å—é™çš„AIç³»ç»Ÿç ”ç©¶æä¾›äº†åŸºç¡€ï¼Œä¸ºå¼€å‘å…·å¤‡å®æ—¶èƒ½åŠ›çš„æ™ºèƒ½ä½“æŒ‡æ˜äº†é“è·¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04898v1">PDF</a> 30 pages</p>
<p><strong>Summary</strong>ï¼šç°å®ä¸–ç•Œä¸­çš„æ™ºèƒ½ä½“éœ€è¦åšå‡ºæ—¢é€»è¾‘åˆåŠæ—¶çš„åˆ¤æ–­ï¼Œè¦æ±‚æŒç»­æ„ŸçŸ¥åŠ¨æ€ç¯å¢ƒã€‚é’ˆå¯¹ä¸æ–­å‘å±•çš„ç¯å¢ƒï¼Œæˆ‘ä»¬å¼•å…¥å®æ—¶æ¨ç†ä½œä¸ºæ–°çš„æ™ºèƒ½ä½“é—®é¢˜è¡¨è¿°å½¢å¼ï¼Œå¹¶å»ºç«‹å®æ—¶æ¨ç†gymè¿›è¡Œå±•ç¤ºã€‚ç ”ç©¶å‘ç°å³ä½¿æœ€å…ˆè¿›çš„æ¨¡å‹åœ¨ä¸¤ä¸ªèŒƒå¼ä¸­ä¹Ÿå­˜åœ¨é€»è¾‘å’ŒåŠæ—¶åˆ¤æ–­å›°éš¾çš„é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºAgileThinkerï¼Œèƒ½åŒæ—¶é‡‡ç”¨ä¸¤ç§æ¨ç†èŒƒå¼ï¼Œåœ¨ä»»åŠ¡éš¾åº¦å’Œæ—¶é—´å‹åŠ›å¢å¤§æ—¶è¡¨ç°ä¼˜å¼‚ï¼Œæœ‰æ•ˆå¹³è¡¡æ¨ç†æ·±åº¦å’Œå“åº”å»¶è¿Ÿã€‚è¿™é¡¹å·¥ä½œä¸ºå¼€å‘å®ç”¨æ™ºèƒ½ä½“å»ºç«‹äº†å®æ—¶æ¨ç†çš„é‡è¦æµ‹è¯•å¹³å°ï¼Œå¹¶ä¸ºæ—¶é—´çº¦æŸAIç³»ç»Ÿçš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>æ™ºèƒ½ä½“éœ€è¦é€‚åº”åŠ¨æ€ç¯å¢ƒå¹¶åšå‡ºé€»è¾‘åŠæ—¶çš„åˆ¤æ–­ã€‚</li>
<li>å¼•å…¥å®æ—¶æ¨ç†ä½œä¸ºæ–°çš„æ™ºèƒ½ä½“é—®é¢˜è¡¨è¿°å½¢å¼ã€‚</li>
<li>å»ºç«‹Real-Time Reasoning Gymä»¥å±•ç¤ºå®æ—¶æ¨ç†ã€‚</li>
<li>ç ”ç©¶å‘ç°ç°æœ‰è¯­è¨€æ¨¡å‹åœ¨æ™ºèƒ½ä½“ä¸­å­˜åœ¨é€»è¾‘å’ŒåŠæ—¶åˆ¤æ–­å›°éš¾çš„é—®é¢˜ã€‚</li>
<li>æå‡ºAgileThinkerèƒ½åŒæ—¶é‡‡ç”¨ä¸¤ç§æ¨ç†èŒƒå¼ã€‚</li>
<li>AgileThinkeråœ¨ä»»åŠ¡éš¾åº¦å’Œæ—¶é—´å‹åŠ›å¢å¤§æ—¶è¡¨ç°ä¼˜å¼‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04898">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cf7338c9af725c85e8626ce31f002a1b" align="middle">
<img src="https://picx.zhimg.com/v2-2993f3d9b30640956cf66de2084e7769" align="middle">
<img src="https://picx.zhimg.com/v2-e911b71986394021cf4e9c8c545aeed4" align="middle">
<img src="https://picx.zhimg.com/v2-dc72546b05f58efbb46267e9aff815c2" align="middle">
<img src="https://picx.zhimg.com/v2-4268852069dbf53fafe8734a6fc6cfcb" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="A-Standardized-Benchmark-for-Multilabel-Antimicrobial-Peptide-Classification"><a href="#A-Standardized-Benchmark-for-Multilabel-Antimicrobial-Peptide-Classification" class="headerlink" title="A Standardized Benchmark for Multilabel Antimicrobial Peptide   Classification"></a>A Standardized Benchmark for Multilabel Antimicrobial Peptide   Classification</h2><p><strong>Authors:Sebastian Ojeda, Rafael Velasquez, NicolÃ¡s Aparicio, Juanita Puentes, Paula CÃ¡rdenas, NicolÃ¡s Andrade, Gabriel GonzÃ¡lez, Sergio RincÃ³n, Carolina MuÃ±oz-Camargo, Pablo ArbelÃ¡ez</strong></p>
<p>Antimicrobial peptides have emerged as promising molecules to combat antimicrobial resistance. However, fragmented datasets, inconsistent annotations, and the lack of standardized benchmarks hinder computational approaches and slow down the discovery of new candidates. To address these challenges, we present the Expanded Standardized Collection for Antimicrobial Peptide Evaluation (ESCAPE), an experimental framework integrating over 80.000 peptides from 27 validated repositories. Our dataset separates antimicrobial peptides from negative sequences and incorporates their functional annotations into a biologically coherent multilabel hierarchy, capturing activities across antibacterial, antifungal, antiviral, and antiparasitic classes. Building on ESCAPE, we propose a transformer-based model that leverages sequence and structural information to predict multiple functional activities of peptides. Our method achieves up to a 2.56% relative average improvement in mean Average Precision over the second-best method adapted for this task, establishing a new state-of-the-art multilabel peptide classification. ESCAPE provides a comprehensive and reproducible evaluation framework to advance AI-driven antimicrobial peptide research. </p>
<blockquote>
<p>æŠ—èŒè‚½ä½œä¸ºå¯¹æŠ—æŠ—èŒç´ è€è¯æ€§çš„æœ‰å‰é€”çš„åˆ†å­å·²ç»å´­éœ²å¤´è§’ã€‚ç„¶è€Œï¼Œæ•°æ®ç‰‡æ®µåŒ–ã€æ³¨é‡Šä¸ä¸€è‡´ä»¥åŠç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†æµ‹è¯•é˜»ç¢äº†è®¡ç®—æ–¹æ³•å’Œæ–°å€™é€‰ç‰©çš„å‘ç°ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ç”¨äºæŠ—èŒè‚½è¯„ä¼°çš„æ‰©å±•æ ‡å‡†åŒ–é›†åˆï¼ˆESCAPEï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªå®éªŒæ€§æ¡†æ¶ï¼Œæ•´åˆäº†æ¥è‡ª27ä¸ªéªŒè¯å­˜å‚¨åº“çš„8ä¸‡å¤šä¸ªè‚½ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å°†æŠ—èŒè‚½ä¸é˜´æ€§åºåˆ—åŒºåˆ†å¼€ï¼Œå¹¶å°†å…¶åŠŸèƒ½æ³¨é‡Šçº³å…¥ç”Ÿç‰©å­¦è¿è´¯çš„å¤šæ ‡ç­¾å±‚æ¬¡ç»“æ„ä¸­ï¼Œæ¶µç›–æŠ—èŒã€æŠ—çœŸèŒã€æŠ—ç—…æ¯’å’ŒæŠ—å¯„ç”Ÿè™«ç±»åˆ«çš„æ´»åŠ¨ã€‚åŸºäºESCAPEï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºtransformerçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åˆ©ç”¨åºåˆ—å’Œç»“æ„ä¿¡æ¯æ¥é¢„æµ‹è‚½çš„å¤šç§åŠŸèƒ½æ´»åŠ¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹³å‡ç²¾åº¦ä¸Šç›¸å¯¹äºç¬¬äºŒå¥½çš„æ–¹æ³•å®ç°äº†é«˜è¾¾2.56%çš„ç›¸å¯¹å¹³å‡æ”¹è¿›ï¼Œå»ºç«‹äº†æ–°çš„æœ€å…ˆè¿›çš„å¤šè‚½æ ‡ç­¾åˆ†ç±»ã€‚ESCAPEæä¾›äº†ä¸€ä¸ªå…¨é¢ä¸”å¯é‡å¤çš„è¯„ä»·æ¡†æ¶ï¼Œä»¥æ¨åŠ¨äººå·¥æ™ºèƒ½é©±åŠ¨çš„æŠ—èŒè‚½ç ”ç©¶å‘å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04814v1">PDF</a> 39th Conference on Neural Information Processing Systems (NeurIPS   2025). Camera-ready version. Code: <a target="_blank" rel="noopener" href="https://github.com/BCV-Uniandes/ESCAPE">https://github.com/BCV-Uniandes/ESCAPE</a>.   Dataset DOI: <a target="_blank" rel="noopener" href="https://doi.org/10.7910/DVN/C69MCD">https://doi.org/10.7910/DVN/C69MCD</a></p>
<p><strong>Summary</strong></p>
<p>æŠ—èŒè‚½åœ¨æŠ—å‡»æŠ—å¾®ç”Ÿç‰©è€è¯æ€§æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ã€‚ç„¶è€Œï¼Œä¸ºè§£å†³æ•°æ®ç¢ç‰‡åŒ–ã€æ³¨é‡Šä¸ä¸€è‡´ä»¥åŠç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†ç­‰é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†æ‰©å±•çš„æŠ—èŒè‚½è¯„ä¼°æ ‡å‡†åŒ–é›†åˆï¼ˆESCAPEï¼‰ã€‚è¯¥å®éªŒæ¡†æ¶æ•´åˆäº†è¶…è¿‡8ä¸‡ä¸ªæ¥è‡ª27ä¸ªéªŒè¯åº“çš„è‚½ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å°†æŠ—èŒè‚½ä¸é˜´æ€§åºåˆ—åŒºåˆ†å¼€ï¼Œå¹¶å°†å…¶åŠŸèƒ½æ³¨é‡Šèå…¥ç”Ÿç‰©å­¦è¿è´¯çš„å¤šæ ‡ç­¾å±‚æ¬¡ç»“æ„ä¸­ï¼Œè¦†ç›–æŠ—èŒã€æŠ—çœŸèŒã€æŠ—ç—…æ¯’å’Œé©±è™«ç±»ç­‰æ´»æ€§ã€‚åŸºäºESCAPEï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨åºåˆ—å’Œç»“æ„ä¿¡æ¯çš„åŸºäºtransformerçš„æ¨¡å‹æ¥é¢„æµ‹è‚½çš„å¤šç§åŠŸèƒ½æ´»æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç›¸è¾ƒäºç¬¬äºŒåæ–¹æ³•å¹³å‡ç²¾åº¦æé«˜äº†æœ€é«˜è¾¾2.56%ï¼Œå»ºç«‹äº†æ–°çš„å¤šè‚½å¤šæ ‡ç­¾åˆ†ç±»çš„ä¸šç•Œæ ‡å‡†ã€‚ESCAPEä¸ºAIé©±åŠ¨çš„æŠ—èŒè‚½ç ”ç©¶æä¾›äº†å…¨é¢ä¸”å¯é‡å¤çš„è¯„ä»·æ¡†æ¶ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æŠ—èŒè‚½åœ¨æŠ—å‡»æŠ—å¾®ç”Ÿç‰©è€è¯æ€§æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</li>
<li>å½“å‰é¢ä¸´æ•°æ®ç¢ç‰‡åŒ–ã€æ³¨é‡Šä¸ä¸€è‡´å’Œç¼ºä¹æ ‡å‡†åŒ–åŸºå‡†ç­‰æŒ‘æˆ˜ã€‚</li>
<li>æ¨å‡ºæ‰©å±•çš„æŠ—èŒè‚½è¯„ä¼°æ ‡å‡†åŒ–é›†åˆï¼ˆESCAPEï¼‰ä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>ESCAPEæ•´åˆäº†è¶…è¿‡8ä¸‡ä¸ªæ¥è‡ªå¤šä¸ªæºçš„è‚½ï¼Œå¹¶åŒºåˆ†äº†æŠ—èŒè‚½å’Œé˜´æ€§åºåˆ—ã€‚</li>
<li>ESCAPEæ•°æ®é›†åŒ…å«åŠŸèƒ½æ³¨é‡Šï¼Œè¦†ç›–æŠ—èŒã€æŠ—çœŸèŒã€æŠ—ç—…æ¯’å’Œé©±è™«ç­‰å¤šç§æ´»æ€§ã€‚</li>
<li>åŸºäºESCAPEï¼Œæå‡ºäº†åˆ©ç”¨åºåˆ—å’Œç»“æ„ä¿¡æ¯çš„æ–°çš„è‚½åŠŸèƒ½é¢„æµ‹æ¨¡å‹ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04814">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c4d8dd18569b93ca88d1a9c825f5f5e8" align="middle">
<img src="https://picx.zhimg.com/v2-c3f6d5747abb753be154fb72c7063370" align="middle">
<img src="https://picx.zhimg.com/v2-e6ee93a2d10574547f62e1bc4d45d94f" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="THEval-Evaluation-Framework-for-Talking-Head-Video-Generation"><a href="#THEval-Evaluation-Framework-for-Talking-Head-Video-Generation" class="headerlink" title="THEval. Evaluation Framework for Talking Head Video Generation"></a>THEval. Evaluation Framework for Talking Head Video Generation</h2><p><strong>Authors:Nabyl Quignon, Baptiste Chopin, Yaohui Wang, Antitza Dantcheva</strong></p>
<p>Video generation has achieved remarkable progress, with generated videos increasingly resembling real ones. However, the rapid advance in generation has outpaced the development of adequate evaluation metrics. Currently, the assessment of talking head generation primarily relies on limited metrics, evaluating general video quality, lip synchronization, and on conducting user studies. Motivated by this, we propose a new evaluation framework comprising 8 metrics related to three dimensions (i) quality, (ii) naturalness, and (iii) synchronization. In selecting the metrics, we place emphasis on efficiency, as well as alignment with human preferences. Based on this considerations, we streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as well as face quality. Our extensive experiments on 85,000 videos generated by 17 state-of-the-art models suggest that while many algorithms excel in lip synchronization, they face challenges with generating expressiveness and artifact-free details. These videos were generated based on a novel real dataset, that we have curated, in order to mitigate bias of training data. Our proposed benchmark framework is aimed at evaluating the improvement of generative methods. Original code, dataset and leaderboards will be publicly released and regularly updated with new methods, in order to reflect progress in the field. </p>
<blockquote>
<p>è§†é¢‘ç”Ÿæˆå·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œç”Ÿæˆçš„è§†é¢‘è¶Šæ¥è¶Šé€¼çœŸã€‚ç„¶è€Œï¼Œç”Ÿæˆçš„è¿…é€Ÿå‘å±•è¶…å‡ºäº†å……è¶³è¯„ä¼°æŒ‡æ ‡çš„å¼€å‘é€Ÿåº¦ã€‚ç›®å‰ï¼Œå¯¹äºè¯´è¯äººå¤´éƒ¨ç”Ÿæˆçš„è¯„ä¼°ä¸»è¦ä¾èµ–äºæœ‰é™çš„æŒ‡æ ‡ï¼ŒåŒ…æ‹¬è¯„ä»·ä¸€èˆ¬è§†é¢‘è´¨é‡ã€å˜´å”‡åŒæ­¥å’Œç”¨æˆ·ç ”ç©¶ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«ä¸ä¸‰ä¸ªç»´åº¦ç›¸å…³çš„8ä¸ªæŒ‡æ ‡ï¼šï¼ˆiï¼‰è´¨é‡ï¼Œï¼ˆiiï¼‰è‡ªç„¶åº¦ï¼Œï¼ˆiiiï¼‰åŒæ­¥æ€§ã€‚åœ¨é€‰æ‹©æŒ‡æ ‡æ—¶ï¼Œæˆ‘ä»¬å¼ºè°ƒæ•ˆç‡ä»¥åŠä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬ç®€åŒ–äº†å¯¹å¤´éƒ¨ã€å˜´å·´å’Œçœ‰æ¯›çš„ç²¾ç»†åŠ¨ä½œåˆ†æï¼Œä»¥åŠé¢éƒ¨è´¨é‡ã€‚æˆ‘ä»¬å¯¹ç”±17ç§æœ€æ–°æ¨¡å‹ç”Ÿæˆçš„85000ä¸ªè§†é¢‘è¿›è¡Œçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œè™½ç„¶è®¸å¤šç®—æ³•åœ¨å˜´å”‡åŒæ­¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç”Ÿæˆè¡¨è¾¾åŠ›å’Œæ— ç‘•ç–µçš„ç»†èŠ‚æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚è¿™äº›è§†é¢‘æ˜¯åŸºäºæˆ‘ä»¬æ•´ç†çš„ä¸€ä¸ªæ–°å‹çœŸå®æ•°æ®é›†ç”Ÿæˆçš„ï¼Œæ—¨åœ¨å‡è½»è®­ç»ƒæ•°æ®çš„åè§ã€‚æˆ‘ä»¬æå‡ºçš„åŸºå‡†æ¡†æ¶æ—¨åœ¨è¯„ä¼°ç”Ÿæˆæ–¹æ³•çš„æ”¹è¿›ã€‚åŸå§‹ä»£ç ã€æ•°æ®é›†å’Œæ’è¡Œæ¦œå°†å…¬å¼€å‘å¸ƒå¹¶å®šæœŸæ›´æ–°æ–°çš„æ–¹æ³•ï¼Œä»¥åæ˜ è¯¥é¢†åŸŸçš„è¿›å±•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04520v2">PDF</a> </p>
<p><strong>Summary</strong><br>è§†é¢‘ç”ŸæˆæŠ€æœ¯å‘å±•è¿…é€Ÿï¼Œä½†è¯„ä¼°æŒ‡æ ‡çš„å‘å±•æ»åã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŒ…å«8ä¸ªæŒ‡æ ‡çš„æ–°è¯„ä¼°æ¡†æ¶ï¼Œæ¶‰åŠè´¨é‡ã€è‡ªç„¶åº¦å’ŒåŒæ­¥æ€§ä¸‰ä¸ªç»´åº¦ã€‚æˆ‘ä»¬å¼ºè°ƒè¯„ä¼°æŒ‡æ ‡çš„æ•ˆç‡å’Œä¸äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚é€šè¿‡å¯¹å¤§é‡è§†é¢‘çš„åˆ†æï¼Œæˆ‘ä»¬å‘ç°ç°æœ‰æ¨¡å‹åœ¨å˜´å”‡åŒæ­¥æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨è¡¨è¾¾åŠ›å’Œç»†èŠ‚æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„æ•°æ®é›†æ—¨åœ¨å‡å°‘è®­ç»ƒæ•°æ®çš„åè§ï¼Œè¯„ä¼°æ¡†æ¶æ—¨åœ¨è¯„ä¼°ç”Ÿæˆæ–¹æ³•çš„æ”¹è¿›ã€‚æˆ‘ä»¬å°†å…¬å¼€åŸå§‹ä»£ç ã€æ•°æ®é›†å’Œæ’è¡Œæ¦œå¹¶å®šæœŸæ›´æ–°æ–°æ–¹æ³•ä»¥åæ˜ è¯¥é¢†åŸŸçš„è¿›å±•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>è§†é¢‘ç”ŸæˆæŠ€æœ¯å–å¾—æ˜¾è‘—è¿›å±•ï¼Œä½†ä¸çœŸå®è§†é¢‘ç›¸ä¼¼åº¦ä¸æ–­æé«˜çš„åŒæ—¶ï¼Œè¯„ä¼°æŒ‡æ ‡çš„å‘å±•å´ç›¸å¯¹æ»åã€‚</li>
<li>å½“å‰å¯¹å¤´éƒ¨ç”Ÿæˆè§†é¢‘çš„è¯„ä»·ä¸»è¦ä¾èµ–äºæœ‰é™çš„æŒ‡æ ‡ï¼Œå¦‚è§†é¢‘è´¨é‡ã€å˜´å”‡åŒæ­¥å’Œç”¨æˆ·ç ”ç©¶ç­‰ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä»·æ¡†æ¶ï¼ŒåŒ…æ‹¬8ä¸ªä¸ä¸‰ä¸ªç»´åº¦ï¼ˆè´¨é‡ã€è‡ªç„¶åº¦ã€åŒæ­¥æ€§ï¼‰ç›¸å…³çš„æŒ‡æ ‡ã€‚</li>
<li>åœ¨é€‰æ‹©è¯„ä»·æŒ‡æ ‡æ—¶ï¼Œå¼ºè°ƒæ•ˆç‡å’Œå¯¹äººç±»åå¥½çš„ä¸€è‡´æ€§ã€‚</li>
<li>é€šè¿‡åˆ†æå¤§é‡è§†é¢‘å‘ç°ï¼Œç°æœ‰æ¨¡å‹åœ¨å˜´å”‡åŒæ­¥æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨è¡¨è¾¾åŠ›å’Œç»†èŠ‚æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>è¯¥ç ”ç©¶ä½¿ç”¨äº†ä¸€ä¸ªæ–°æ”¶é›†çš„çœŸå®æ•°æ®é›†æ¥å‡å°‘è®­ç»ƒæ•°æ®çš„åè§é—®é¢˜ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04520">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4c9758f21801c890ccd415042abdb635" align="middle">
<img src="https://picx.zhimg.com/v2-0ec34a24e484e5230672a711d01539b0" align="middle">
<img src="https://picx.zhimg.com/v2-7852cef2e0dbacd0b30030bf7a2efb90" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Monitor-Generate-Verify-MGV-Formalising-Metacognitive-Theory-for-Language-Model-Reasoning"><a href="#Monitor-Generate-Verify-MGV-Formalising-Metacognitive-Theory-for-Language-Model-Reasoning" class="headerlink" title="Monitor-Generate-Verify (MGV): Formalising Metacognitive Theory for   Language Model Reasoning"></a>Monitor-Generate-Verify (MGV): Formalising Metacognitive Theory for   Language Model Reasoning</h2><p><strong>Authors:Nick Oh, Fernand Gobet</strong></p>
<p>Test-time reasoning architectures such as those following the Generate-Verify paradigm â€“ where a model iteratively refines or verifies its own generated outputs â€“ prioritise generation and verification but exclude the monitoring processes that determine when and how reasoning should begin. This omission may contribute to the prefix dominance trap, in which models commit early to suboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy loss. We address this architectural gap by formalising Flavellâ€™s and Nelson and Narensâ€™ metacognitive theories into computational specifications, proposing the Monitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify paradigm by adding explicit monitoring that captures metacognitive experiences (from difficulty assessments to confidence judgements) before generation begins and refines future monitoring through verification feedback. Though we present no empirical validation, this work provides the first systematic computational translation of foundational metacognitive theories, offering a principled vocabulary for understanding reasoning system failures and suggesting specific architectural interventions for future test-time reasoning designs. </p>
<blockquote>
<p>åœ¨éµå¾ªç”Ÿæˆ-éªŒè¯èŒƒå¼çš„æµ‹è¯•æ—¶é—´æ¨ç†æ¶æ„ä¸­ï¼Œæ¨¡å‹ä¼šè¿­ä»£åœ°å®Œå–„æˆ–éªŒè¯å…¶è‡ªèº«ç”Ÿæˆçš„è¾“å‡ºï¼Œè¿™ç§æ¶æ„è™½ç„¶é‡è§†ç”Ÿæˆå’ŒéªŒè¯ï¼Œä½†å¿½ç•¥äº†ç¡®å®šä½•æ—¶ä»¥åŠå¦‚ä½•å¼€å§‹æ¨ç†çš„ç›‘æ§è¿‡ç¨‹ã€‚è¿™ç§é—æ¼å¯èƒ½å¯¼è‡´å‰ç¼€ä¸»å¯¼é™·é˜±ï¼Œå³æ¨¡å‹è¿‡æ—©åœ°é™·å…¥éæœ€ä¼˜æ¨ç†è·¯å¾„ï¼Œå¹¶ä¸”å¾ˆå°‘æ¢å¤ï¼Œå¯¼è‡´å¤§çº¦20%çš„å‡†ç¡®ç‡æŸå¤±ã€‚æˆ‘ä»¬é€šè¿‡å°†å¼—æ‹‰ç»´å°”ä»¥åŠçº³å°”é€Šå’Œçº³ä¼¦æ–¯çš„å…ƒè®¤çŸ¥ç†è®ºå½¢å¼åŒ–ä¸ºè®¡ç®—è§„èŒƒæ¥è§£å†³è¿™ä¸€æ¶æ„å·®è·ï¼Œæå‡ºç›‘è§†-ç”Ÿæˆ-éªŒè¯ï¼ˆMGVï¼‰æ¡†æ¶ã€‚MGVé€šè¿‡æ·»åŠ æ˜ç¡®çš„ç›‘æ§æ¥æ‰©å±•ç”Ÿæˆ-éªŒè¯èŒƒå¼ï¼Œåœ¨ç”Ÿæˆå¼€å§‹ä¹‹å‰æ•è·å…ƒè®¤çŸ¥ä½“éªŒï¼ˆä»éš¾åº¦è¯„ä¼°åˆ°ä¿¡å¿ƒåˆ¤æ–­ï¼‰ï¼Œå¹¶é€šè¿‡éªŒè¯åé¦ˆæ¥å®Œå–„æœªæ¥çš„ç›‘æ§ã€‚å°½ç®¡æˆ‘ä»¬æ²¡æœ‰æä¾›å®è¯éªŒè¯ï¼Œä½†è¿™é¡¹å·¥ä½œæä¾›äº†å¯¹åŸºç¡€å…ƒè®¤çŸ¥ç†è®ºçš„é¦–ä¸ªç³»ç»Ÿæ€§è®¡ç®—ç¿»è¯‘ï¼Œä¸ºç†è§£æ¨ç†ç³»ç»Ÿå¤±è´¥æä¾›äº†æœ‰åŸåˆ™çš„è¯æ±‡ï¼Œå¹¶ä¸ºæœªæ¥çš„æµ‹è¯•æ—¶é—´æ¨ç†è®¾è®¡æå‡ºäº†å…·ä½“çš„æ¶æ„å¹²é¢„æªæ–½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04341v2">PDF</a> To-be presented at the Workshop on the Foundations of Reasoning in   Language Models at NeurIPS 2025 (non-archival)</p>
<p><strong>Summary</strong>ï¼šç”ŸæˆéªŒè¯èŒƒå¼ä¸‹çš„æµ‹è¯•æ—¶é—´æ¨ç†æ¶æ„é€šè¿‡è¿­ä»£å®Œå–„æˆ–éªŒè¯æ¨¡å‹çš„ç”Ÿæˆè¾“å‡ºæ¥å®ç°æ¨ç†ï¼Œä½†å®ƒå¿½è§†äº†å†³å®šä½•æ—¶å¦‚ä½•è¿›è¡Œæ¨ç†çš„ç›‘æ§è¿‡ç¨‹ã€‚æœ¬æ–‡è§£å†³äº†è¿™ä¸€é—®é¢˜ï¼Œç»“åˆå¼—æ‹‰ç»´å°”ã€çº³å°”é€Šå’Œçº³ä¼¦æ–¯çš„å…ƒè®¤çŸ¥ç†è®ºæå‡ºç›‘æ§ç”ŸæˆéªŒè¯ï¼ˆMGVï¼‰æ¡†æ¶ï¼Œé€šè¿‡åœ¨ç”Ÿæˆä¹‹å‰æ•æ‰å…ƒè®¤çŸ¥ä½“éªŒï¼Œå¹¶åœ¨éªŒè¯åé¦ˆä¸­ä¸æ–­å®Œå–„æœªæ¥çš„ç›‘æ§ï¼Œè§£å†³äº†åœ¨ç”ŸæˆéªŒè¯èŒƒå¼ä¸‹å¯èƒ½ä¼šå¯¼è‡´çš„æ—©æœŸæ‰¿è¯ºæ¨ç†è·¯å¾„çš„é”™è¯¯åŠå¤§çº¦ç™¾åˆ†ä¹‹äºŒåçš„å‡†ç¡®æ€§æŸå¤±ã€‚å°½ç®¡ç¼ºä¹å®è¯ç ”ç©¶ï¼Œä½†æœ¬æ–‡é¦–æ¬¡ç³»ç»Ÿåœ°ç¿»è¯‘äº†åŸºç¡€å…ƒè®¤çŸ¥ç†è®ºï¼Œä¸ºç†è§£æ¨ç†ç³»ç»Ÿçš„å¤±è´¥æä¾›äº†åŸåˆ™æ€§çš„è¯æ±‡ï¼Œå¹¶ä¸ºæœªæ¥çš„æµ‹è¯•æ—¶é—´æ¨ç†è®¾è®¡æå‡ºäº†å…·ä½“çš„æ¶æ„å¹²é¢„æªæ–½ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>æµ‹è¯•æ—¶é—´æ¨ç†æ¶æ„ä¸­çš„ç”ŸæˆéªŒè¯èŒƒå¼å¿½ç•¥äº†ç›‘æ§è¿‡ç¨‹ï¼Œå¯èƒ½å¯¼è‡´æ—©æœŸæ‰¿è¯ºæ¨ç†è·¯å¾„çš„é”™è¯¯ã€‚</li>
<li>è¿™ç§å¿½ç•¥å¯èƒ½ä¼šå¼•å‘å‰ç¼€ä¸»å¯¼é™·é˜±ï¼Œä½¿æ¨¡å‹é™·å…¥ä¸ä½³çš„æ¨ç†è·¯å¾„å¹¶éš¾ä»¥æ¢å¤ï¼Œå¯¼è‡´çº¦20%çš„å‡†ç¡®æ€§æŸå¤±ã€‚</li>
<li>ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº†ç›‘æ§ç”ŸæˆéªŒè¯ï¼ˆMGVï¼‰æ¡†æ¶ã€‚</li>
<li>MGVæ¡†æ¶æ‰©å±•äº†ç”ŸæˆéªŒè¯èŒƒå¼ï¼Œé€šè¿‡æ˜ç¡®çš„ç›‘æ§æ•æ‰å…ƒè®¤çŸ¥ä½“éªŒï¼ˆå¦‚éš¾åº¦è¯„ä¼°å’Œä¿¡å¿ƒåˆ¤æ–­ï¼‰ï¼Œå¹¶åœ¨ç”Ÿæˆä¹‹å‰è¿›è¡Œå®Œå–„ã€‚</li>
<li>MGVæ¡†æ¶è¿˜é€šè¿‡éªŒè¯åé¦ˆæ¥å®Œå–„æœªæ¥çš„ç›‘æ§ã€‚</li>
<li>æœ¬æ–‡å°†å…ƒè®¤çŸ¥ç†è®ºè½¬åŒ–ä¸ºè®¡ç®—è§„èŒƒï¼Œä¸ºç†è§£æ¨ç†ç³»ç»Ÿçš„å¤±è´¥æä¾›äº†åŸåˆ™æ€§çš„è¯æ±‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04341">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b733d80614acc8cda8f7771e0bcbb84d" align="middle">
<img src="https://picx.zhimg.com/v2-fc29688c672edacc603f7e734aebb64a" align="middle">
<img src="https://picx.zhimg.com/v2-d17c6ffa241dfd21d8ab4886e8f3c967" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="Med-Banana-50K-A-Cross-modality-Large-Scale-Dataset-for-Text-guided-Medical-Image-Editing"><a href="#Med-Banana-50K-A-Cross-modality-Large-Scale-Dataset-for-Text-guided-Medical-Image-Editing" class="headerlink" title="Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided   Medical Image Editing"></a>Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided   Medical Image Editing</h2><p><strong>Authors:Zhihui Chen, Mengling Feng</strong></p>
<p>Medical image editing has emerged as a pivotal technology with broad applications in data augmentation, model interpretability, medical education, and treatment simulation. However, the lack of large-scale, high-quality, and openly accessible datasets tailored for medical contexts with strict anatomical and clinical constraints has significantly hindered progress in this domain. To bridge this gap, we introduce Med-Banana-50K, a comprehensive dataset of over 50k medically curated image edits spanning chest X-ray, brain MRI, and fundus photography across 23 diseases. Each sample supports bidirectional lesion editing (addition and removal) and is constructed using Gemini-2.5-Flash-Image based on real clinical images. A key differentiator of our dataset is the medically grounded quality control protocol: we employ an LLM-as-Judge evaluation framework with criteria such as instruction compliance, structural plausibility, image realism, and fidelity preservation, alongside iterative refinement over up to five rounds. Additionally, Med-Banana-50K includes around 37,000 failed editing attempts with full evaluation logs to support preference learning and alignment research. By offering a large-scale, medically rigorous, and fully documented resource, Med-Banana-50K establishes a critical foundation for developing and evaluating reliable medical image editing systems. Our dataset and code are publicly available. [<a target="_blank" rel="noopener" href="https://github.com/richardChenzhihui/med-banana-50k]">https://github.com/richardChenzhihui/med-banana-50k]</a>. </p>
<blockquote>
<p>åŒ»å­¦å½±åƒç¼–è¾‘æŠ€æœ¯å·²æˆä¸ºä¸€é¡¹è‡³å…³é‡è¦çš„æŠ€æœ¯ï¼Œåœ¨æ•°æ®å¢å¼ºã€æ¨¡å‹è§£é‡Šæ€§ã€åŒ»å­¦æ•™è‚²å’Œæ²»ç–—æ¨¡æ‹Ÿç­‰é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹é’ˆå¯¹åŒ»å­¦æƒ…å¢ƒçš„å¤§è§„æ¨¡ã€é«˜è´¨é‡ã€å…¬å¼€å¯è®¿é—®çš„ã€ç¬¦åˆä¸¥æ ¼è§£å‰–å’Œä¸´åºŠçº¦æŸçš„æ•°æ®é›†ï¼Œè¯¥é¢†åŸŸçš„è¿›å±•å—åˆ°äº†æå¤§çš„é˜»ç¢ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Med-Banana-50Kæ•°æ®é›†ï¼Œå®ƒåŒ…å«è¶…è¿‡5ä¸‡ä»½ç»è¿‡åŒ»å­¦å®¡æ ¸çš„å½±åƒç¼–è¾‘æ ·æœ¬ï¼Œæ¶µç›–äº†èƒ¸éƒ¨Xå…‰ã€è„‘éƒ¨MRIå’Œçœ¼åº•æ‘„å½±ï¼Œè·¨è¶Šäº†23ç§ç–¾ç—…ã€‚æ¯ä¸ªæ ·æœ¬éƒ½æ”¯æŒåŒå‘ç—…å˜ç¼–è¾‘ï¼ˆå¢åŠ å’Œç§»é™¤ï¼‰ï¼Œå¹¶ä½¿ç”¨åŸºäºçœŸå®ä¸´åºŠå›¾åƒçš„Gemini-2.5-Flash-Imageæ„å»ºã€‚æˆ‘ä»¬æ•°æ®é›†çš„ä¸€ä¸ªå…³é”®åŒºåˆ«åœ¨äºå…¶åŒ»å­¦åŸºç¡€çš„è´¨é‡æ§åˆ¶åè®®ï¼šæˆ‘ä»¬é‡‡ç”¨LLM-as-Judgeè¯„ä¼°æ¡†æ¶ï¼ŒåŒ…æ‹¬æŒ‡ä»¤åˆè§„æ€§ã€ç»“æ„åˆç†æ€§ã€å›¾åƒçœŸå®æ€§å’Œä¿çœŸåº¦ä¿ç•™ç­‰æ ‡å‡†ï¼Œå¹¶è¿›è¡Œæœ€å¤šäº”è½®çš„è¿­ä»£æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒMed-Banana-50Kè¿˜åŒ…æ‹¬çº¦3.7ä¸‡æ¬¡å¤±è´¥çš„ç¼–è¾‘å°è¯•åŠå®Œæ•´çš„è¯„ä¼°æ—¥å¿—ï¼Œä»¥æ”¯æŒåå¥½å­¦ä¹ å’Œå¯¹é½ç ”ç©¶ã€‚é€šè¿‡æä¾›å¤§è§„æ¨¡ã€åŒ»å­¦ä¸¥è°¨ä¸”å®Œæ•´çš„èµ„æºï¼ŒMed-Banana-50Kä¸ºå¼€å‘å¯é çš„åŒ»å­¦å½±åƒç¼–è¾‘ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å·²å…¬å¼€å‘å¸ƒï¼Œå¯ä¾›è®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://github.com/richardChenzhihui/med-banana-50k]%E3%80%82">https://github.com/richardChenzhihui/med-banana-50k]ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.00801v3">PDF</a> </p>
<p><strong>Summary</strong>ï¼šåŒ»ç–—å›¾åƒç¼–è¾‘æŠ€æœ¯å·²é€æ¸æˆä¸ºä¸€é¡¹å…³é”®æŠ€æœ¯ï¼Œå¹¿æ³›åº”ç”¨äºæ•°æ®å¢å¼ºã€æ¨¡å‹è§£é‡Šæ€§ã€åŒ»å­¦æ•™è‚²å’Œæ²»ç–—æ¨¡æ‹Ÿç­‰é¢†åŸŸã€‚ç„¶è€Œï¼Œç¼ºä¹å¤§è§„æ¨¡ã€é«˜è´¨é‡ã€å¼€æ”¾è®¿é—®çš„é’ˆå¯¹åŒ»å­¦æƒ…å¢ƒçš„æ•°æ®é›†ï¼Œä¸¥æ ¼ç¬¦åˆè§£å‰–å­¦å’Œä¸´åºŠçº¦æŸï¼Œæå¤§åœ°é˜»ç¢äº†è¯¥é¢†åŸŸçš„è¿›å±•ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æ¨å‡ºäº†Med-Banana-50Kæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡5ä¸‡ä»½ç»è¿‡åŒ»å­¦å®¡æ ¸çš„å›¾åƒç¼–è¾‘æ ·æœ¬ï¼Œæ¶‰åŠèƒ¸éƒ¨Xå…‰ã€è„‘éƒ¨MRIå’Œçœ¼åº•æ‘„å½±ç­‰23ç§ç–¾ç—…ã€‚æ¯ä¸ªæ ·æœ¬æ”¯æŒåŒå‘ç—…å˜ç¼–è¾‘ï¼ˆå¢åŠ å’Œåˆ é™¤ï¼‰ï¼Œå¹¶ä½¿ç”¨åŸºäºçœŸå®ä¸´åºŠå›¾åƒçš„Gemini-2.5-Flash-Imageæ„å»ºã€‚è¯¥æ•°æ®é›†çš„ä¸€ä¸ªå…³é”®åŒºåˆ«åœ¨äºå…¶åŒ»å­¦åŸºç¡€çš„è´¨é‡æ§åˆ¶åè®®ï¼šæˆ‘ä»¬é‡‡ç”¨LLM-as-Judgeè¯„ä¼°æ¡†æ¶ä½œä¸ºæ ‡å‡†ï¼ŒåŒ…æ‹¬æŒ‡ä»¤åˆè§„æ€§ã€ç»“æ„åˆç†æ€§ã€å›¾åƒçœŸå®æ€§å’Œä¿çœŸåº¦ä¿æŒç­‰æ ‡å‡†ï¼Œå¹¶è¿›è¡Œæœ€å¤šäº”è½®çš„è¿­ä»£æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒMed-Banana-50Kè¿˜åŒ…æ‹¬å¤§çº¦3ä¸‡7åƒæ¬¡çš„ç¼–è¾‘å¤±è´¥å°è¯•è®°å½•ä¸å®Œæ•´çš„è¯„ä¼°æ—¥å¿—è®°å½•æ”¯æŒåå¥½å­¦ä¹ å’Œå¯¹é½ç ”ç©¶ã€‚é€šè¿‡æä¾›å¤§è§„æ¨¡ä¸”ç»è¿‡åŒ»å­¦ä¸¥è°¨éªŒè¯çš„èµ„æºï¼ŒMed-Banana-50Kä¸ºå¼€å‘å¯é çš„åŒ»ç–—å›¾åƒç¼–è¾‘ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å‡å…¬å¼€å¯ç”¨ã€‚å…·ä½“ä¿¡æ¯å¯é€šè¿‡GitHubè®¿é—®ï¼š[<a target="_blank" rel="noopener" href="https://github.com/richardChenzhihui/med-banana-50k]%E3%80%82">https://github.com/richardChenzhihui/med-banana-50k]ã€‚</a> </p>
<p><strong>Key Takeaways</strong>:</p>
<ol>
<li>åŒ»ç–—å›¾åƒç¼–è¾‘æŠ€æœ¯åœ¨æ•°æ®å¢å¼ºã€æ¨¡å‹è§£é‡Šæ€§ç­‰æ–¹é¢å…·æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚ </li>
<li>å½“å‰ç¼ºä¹å¤§è§„æ¨¡çš„åŒ»ç–—å›¾åƒæ•°æ®é›†é˜»ç¢äº†åŒ»ç–—å›¾åƒç¼–è¾‘æŠ€æœ¯çš„è¿›å±•ã€‚ </li>
<li>Med-Banana-50Kæ˜¯ä¸€ä¸ªå…¨é¢çš„åŒ»ç–—å›¾åƒæ•°æ®é›†ï¼ŒåŒ…å«è¶…è¿‡5ä¸‡ä»½ç»è¿‡åŒ»å­¦å®¡æ ¸çš„å›¾åƒç¼–è¾‘æ ·æœ¬ï¼Œæ¶µç›–å¤šç§ç–¾ç—…å’Œæ¨¡æ€ã€‚ </li>
<li>Med-Banana-50Kæ•°æ®é›†æ”¯æŒç—…å˜çš„åŒå‘ç¼–è¾‘ï¼Œå¹¶é‡‡ç”¨çœŸå®çš„ä¸´åºŠå›¾åƒä¸ºåŸºç¡€æ„å»ºã€‚ </li>
<li>è¯¥æ•°æ®é›†å…·æœ‰ä¸¥æ ¼çš„åŒ»å­¦è´¨é‡æ§åˆ¶åè®®å’Œè¯„ä¼°æ ‡å‡†ï¼ŒåŒ…æ‹¬åˆè§„æ€§ã€ç»“æ„åˆç†æ€§ç­‰ã€‚ </li>
<li>Med-Banana-50Kæ•°æ®é›†è¿˜åŒ…æ‹¬å¤§é‡çš„ç¼–è¾‘å¤±è´¥å°è¯•è®°å½•ï¼Œæ”¯æŒåå¥½å­¦ä¹ å’Œå¯¹é½ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.00801">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b22bea946876a9ff6f129c57a4e62b2d" align="middle">
<img src="https://picx.zhimg.com/v2-aba82df67b50d13e66b3cc5a06c0e816" align="middle">
<img src="https://picx.zhimg.com/v2-1fd183790e3414718e82ac83a0f9c4a5" align="middle">
<img src="https://picx.zhimg.com/v2-39aafbb7d0a38a259240fc6806efd230" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="HugAgent-Benchmarking-LLMs-for-Simulation-of-Individualized-Human-Reasoning"><a href="#HugAgent-Benchmarking-LLMs-for-Simulation-of-Individualized-Human-Reasoning" class="headerlink" title="HugAgent: Benchmarking LLMs for Simulation of Individualized Human   Reasoning"></a>HugAgent: Benchmarking LLMs for Simulation of Individualized Human   Reasoning</h2><p><strong>Authors:Chance Jiajie Li, Zhenze Mo, Yuhan Tang, Ao Qu, Jiayi Wu, Kaiya Ivy Zhao, Yulu Gan, Jie Fan, Jiangbo Yu, Hang Jiang, Paul Pu Liang, Jinhua Zhao, Luis Alberto Alonso Pastor, Kent Larson</strong></p>
<p>Simulating human reasoning in open-ended tasks has long been a central aspiration in AI and cognitive science. While large language models now approximate human responses at scale, they remain tuned to population-level consensus, often erasing the individuality of reasoning styles and belief trajectories. To advance the vision of more human-like reasoning in machines, we introduce HugAgent (Human-Grounded Agent Benchmark), which rethinks human reasoning simulation along three dimensions: (i) from averaged to individualized reasoning, (ii) from behavioral mimicry to cognitive alignment, and (iii) from vignette-based to open-ended data. The benchmark evaluates whether a model can predict a specific personâ€™s behavioral responses and the underlying reasoning dynamics in out-of-distribution scenarios, given partial evidence of their prior views. HugAgent adopts a dual-track design: a human track that automates and scales the think-aloud method to collect ecologically valid human reasoning data, and a synthetic track for further scalability and systematic stress testing. This architecture enables low-cost, extensible expansion to new tasks and populations. Experiments with state-of-the-art language models reveal persistent adaptation gaps, positioning HugAgent as the first extensible benchmark for aligning machine reasoning with the individuality of human thought. The benchmark, along with its complete data collection pipeline and companion chatbot, is open-sourced as HugAgent (<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/HugAgent">https://anonymous.4open.science/r/HugAgent</a>) and TraceYourThinking (<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/trace-your-thinking">https://anonymous.4open.science/r/trace-your-thinking</a>). </p>
<blockquote>
<p>æ¨¡æ‹Ÿäººç±»åœ¨æ—¥å¸¸å¼€æ”¾ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›æ˜¯äººå·¥æ™ºèƒ½å’Œè®¤çŸ¥ç§‘å­¦çš„ä¸­å¿ƒè¿½æ±‚ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹å·²ç»åœ¨å¤§è§„æ¨¡ä¸Šè¿‘ä¼¼æ¨¡æ‹Ÿäººç±»çš„å“åº”ï¼Œä½†å®ƒä»¬ä»ç„¶ä»¥ç¾¤ä½“å…±è¯†ä¸ºåŸºç¡€ï¼Œå¸¸å¸¸å¿½ç•¥äº†ä¸ªäººæ¨ç†é£æ ¼å’Œä¿¡å¿µè½¨è¿¹çš„ç‹¬ç‰¹æ€§ã€‚ä¸ºäº†æ¨è¿›æœºå™¨ä¸­æ›´äººæ€§åŒ–æ¨ç†çš„æ„¿æ™¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†HugAgentï¼ˆäººç±»åŸºç¡€ä»£ç†åŸºå‡†æµ‹è¯•ï¼‰ï¼Œå®ƒé‡æ–°æ€è€ƒäº†äººç±»æ¨ç†æ¨¡æ‹Ÿçš„ä¸‰ä¸ªç»´åº¦ï¼šï¼ˆiï¼‰ä»å¹³å‡æ¨ç†åˆ°ä¸ªæ€§åŒ–æ¨ç†ï¼Œï¼ˆiiï¼‰ä»è¡Œä¸ºæ¨¡ä»¿åˆ°è®¤çŸ¥å¯¹é½ï¼Œï¼ˆiiiï¼‰ä»çŸ­ç¯‡æ•…äº‹åˆ°å¼€æ”¾æ•°æ®ã€‚è¯¥åŸºå‡†æµ‹è¯•è¯„ä¼°æ¨¡å‹èƒ½å¦é¢„æµ‹ç‰¹å®šä¸ªä½“åœ¨è¶…å‡ºåˆ†å¸ƒèŒƒå›´åœºæ™¯ä¸­çš„è¡Œä¸ºå“åº”å’Œåº•å±‚æ¨ç†åŠ¨æ€ï¼ŒåŸºäºå¯¹ä»–ä»¬å…ˆå‰è§‚ç‚¹çš„æœ‰é™è¯æ®ã€‚HugAgenté‡‡ç”¨åŒè½¨è®¾è®¡ï¼šäººç±»è½¨é“ç”¨äºè‡ªåŠ¨åŒ–å’Œæ‰©å±•å‡ºå£°æ€è€ƒæ–¹æ³•ä»¥æ”¶é›†ç”Ÿæ€æœ‰æ•ˆçš„äººç±»æ¨ç†æ•°æ®ï¼Œåˆæˆè½¨é“ç”¨äºè¿›ä¸€æ­¥çš„æ‰©å±•æ€§å’Œç³»ç»Ÿå‹åŠ›æµ‹è¯•ã€‚è¿™ç§æ¶æ„èƒ½å¤Ÿå®ç°ä½æˆæœ¬ã€å¯æ‰©å±•çš„æ–°ä»»åŠ¡å’Œäººç¾¤æ‰©å±•ã€‚ä½¿ç”¨æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹è¿›è¡Œçš„å®éªŒæ­ç¤ºäº†æŒç»­çš„é€‚åº”å·®è·ï¼Œè¿™ä½¿å¾—HugAgentæˆä¸ºé¦–ä¸ªå¯ä¸äººç±»æ€ç»´ä¸ªæ€§å¯¹é½çš„æœºå™¨æ¨ç†å¯æ‰©å±•åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æµ‹è¯•åŠå…¶å®Œæ•´çš„æ•°æ®æ”¶é›†ç®¡é“å’Œé…å¥—èŠå¤©æœºå™¨äººå·²ä½œä¸ºHugAgentï¼ˆ<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/HugAgent%EF%BC%89%E5%92%8CTraceYourThinking%EF%BC%88https://anonymous.4open.science/r/trace-your-thinking%EF%BC%89%E5%BC%80%E6%BA%90%E5%8F%91%E5%B8%83%E3%80%82">https://anonymous.4open.science/r/HugAgentï¼‰å’ŒTraceYourThinkingï¼ˆhttps://anonymous.4open.science/r/trace-your-thinkingï¼‰å¼€æºå‘å¸ƒã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.15144v3">PDF</a> To appear in NeurIPS 2025 Workshop on Bridging Language, Agent, and   World Models (LAW)</p>
<p><strong>Summary</strong>ï¼š<br>æ¨¡æ‹Ÿäººç±»åœ¨å¼€æ”¾ä»»åŠ¡ä¸­çš„æ¨ç†èƒ½åŠ›æ˜¯äººå·¥æ™ºèƒ½å’Œè®¤çŸ¥ç§‘å­¦çš„ä¸­å¿ƒç›®æ ‡ã€‚è™½ç„¶å¤§å‹è¯­è¨€æ¨¡å‹å¯ä»¥å¤§è§„æ¨¡åœ°æ¨¡æ‹Ÿäººç±»å“åº”ï¼Œä½†å®ƒä»¬é€šå¸¸é’ˆå¯¹äººç¾¤å…±è¯†è¿›è¡Œè°ƒæ•´ï¼Œå¿½ç•¥äº†ä¸ªäººæ¨ç†é£æ ¼å’Œä¿¡å¿µè½¨è¿¹çš„å¤šæ ·æ€§ã€‚ä¸ºäº†æ¨è¿›æ›´äººæ€§åŒ–çš„æœºå™¨æ¨ç†æ„¿æ™¯ï¼Œæˆ‘ä»¬å¼•å…¥äº†HugAgentï¼ˆHuman-Grounded Agent Benchmarkï¼‰ï¼Œå®ƒé‡æ–°æ€è€ƒäº†äººç±»æ¨ç†æ¨¡æ‹Ÿçš„ä¸‰ä¸ªæ–¹é¢ï¼šï¼ˆiï¼‰ä»å¹³å‡æ¨ç†åˆ°ä¸ªæ€§åŒ–æ¨ç†ï¼Œï¼ˆiiï¼‰ä»è¡Œä¸ºæ¨¡ä»¿åˆ°è®¤çŸ¥å¯¹é½ï¼Œï¼ˆiiiï¼‰ä»åŸºäºæ’å›¾çš„åˆ°å¼€æ”¾çš„æ•°æ®ã€‚HugAgenté‡‡ç”¨åŒè½¨è®¾è®¡ï¼ŒåŒ…æ‹¬è‡ªåŠ¨åŒ–å’Œæ‰©å±•æ€ç»´å‡ºå£°æ–¹æ³•ä»¥æ”¶é›†ç”Ÿæ€æœ‰æ•ˆçš„äººç±»æ¨ç†æ•°æ®çš„äººç±»è½¨é“ï¼Œä»¥åŠç”¨äºè¿›ä¸€æ­¥æé«˜å¯æ‰©å±•æ€§å’Œç³»ç»Ÿæ€§å‹åŠ›æµ‹è¯•çš„åˆæˆè½¨é“ã€‚å®éªŒè¡¨æ˜ï¼Œæœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ä»å­˜åœ¨æŒç»­çš„é€‚åº”å·®è·ï¼Œè¿™å®šä½HugAgentæˆä¸ºç¬¬ä¸€ä¸ªèƒ½ä¸äººç±»æ€ç»´ä¸ªæ€§ç›¸åŒ¹é…çš„æœºå™¨æ¨ç†çš„å¯æ‰©å±•åŸºå‡†æµ‹è¯•ã€‚HugAgentåŠå…¶å®Œæ•´çš„æ•°æ®æ”¶é›†ç®¡é“å’Œä¼´éšèŠå¤©æœºå™¨äººå·²ä½œä¸ºå¼€æºé¡¹ç›®å‘å¸ƒã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>HugAgenté‡æ–°å®šä¹‰äº†äººç±»æ¨ç†æ¨¡æ‹Ÿçš„ä¸‰ä¸ªæ–¹é¢ï¼ŒåŒ…æ‹¬ä¸ªæ€§åŒ–æ¨ç†ã€è®¤çŸ¥å¯¹é½å’Œå¼€æ”¾æ•°æ®çš„ä½¿ç”¨ã€‚</li>
<li>HugAgenté‡‡ç”¨åŒè½¨è®¾è®¡ï¼ŒåŒ…æ‹¬äººç±»è½¨é“å’Œåˆæˆè½¨é“ï¼Œæ—¨åœ¨è‡ªåŠ¨åŒ–å’Œæ‰©å±•äººç±»æ¨ç†æ•°æ®çš„æ”¶é›†ï¼Œå¹¶æ”¯æŒæ–°çš„ä»»åŠ¡å’Œäººç¾¤çš„æ‰©å±•ã€‚</li>
<li>å½“å‰çš„è¯­è¨€æ¨¡å‹åœ¨æ¨¡æ‹Ÿäººç±»æ¨ç†æ—¶å­˜åœ¨é€‚åº”å·®è·ï¼Œå°¤å…¶æ˜¯åœ¨ä¸ªæ€§åŒ–æ–¹é¢ã€‚</li>
<li>HugAgentæ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„åŸºå‡†æµ‹è¯•ï¼Œèƒ½å¤Ÿè¯„ä¼°æ¨¡å‹åœ¨é¢„æµ‹ç‰¹å®šä¸ªäººåœ¨åç¦»åˆ†å¸ƒçš„åœºæ™¯ä¸­çš„è¡Œä¸ºååº”å’Œå†…åœ¨æ¨ç†åŠ¨æ€çš„èƒ½åŠ›ã€‚</li>
<li>HugAgentåŠå…¶æ•°æ®æ”¶é›†ç®¡é“å’Œä¼´éšèŠå¤©æœºå™¨äººå·²ä½œä¸ºå¼€æºé¡¹ç›®å‘å¸ƒï¼Œä»¥ä¿ƒè¿›æ›´å¹¿æ³›çš„ç ”ç©¶å’Œåº”ç”¨ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•ä¸ä»…åœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½æ–¹é¢æœ‰ä»·å€¼ï¼Œè€Œä¸”ä¸ºäººå·¥æ™ºèƒ½å’Œè®¤çŸ¥ç§‘å­¦ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ–°çš„ç ”ç©¶å·¥å…·ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.15144">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-87571cc65d3fbc735cf0e3d819c56460" align="middle">
<img src="https://picx.zhimg.com/v2-d66a29e2fb9865d3612b8ee381768073" align="middle">
<img src="https://picx.zhimg.com/v2-2125f6eb72d4c49a960e1ee1954d2f67" align="middle">
<img src="https://picx.zhimg.com/v2-69e16c145641e271eaf36a64df8f7f93" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward"><a href="#Low-probability-Tokens-Sustain-Exploration-in-Reinforcement-Learning-with-Verifiable-Reward" class="headerlink" title="Low-probability Tokens Sustain Exploration in Reinforcement Learning   with Verifiable Reward"></a>Low-probability Tokens Sustain Exploration in Reinforcement Learning   with Verifiable Reward</h2><p><strong>Authors:Guanhua Huang, Tingqiang Xu, Mingze Wang, Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao Li, Yuhao Jiang, Bo Zhou</strong></p>
<p>Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models in complex reasoning, yet its scalability is often hindered by a training bottleneck where performance plateaus as policy entropy collapses, signaling a loss of exploration. Previous methods typically address this by maintaining high policy entropy, yet the precise mechanisms that govern meaningful exploration have remained underexplored. Our analysis suggests that an unselective focus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies a key issue: the gradual elimination of valuable low-probability exploratory tokens, which we term \textbf{\textit{reasoning sparks}}. We find that while abundant in pre-trained models, these sparks are systematically extinguished during RLVR due to over-penalization, leading to a degeneracy in exploration. To address this, we introduce Low-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a heuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and re-normalizing the distribution over the remaining candidates. The result is a less-noisy proxy where the probability of \textit{reasoning sparks} is amplified, which then serves as a soft regularization target to shield these valuable tokens from elimination via KL divergence. Experiments show that Lp-Reg enables stable on-policy RL, sustaining continuous scaling across $3,000$ training steps and $81,204$ GPU-hours, where baseline entropy-control methods collapse. This sustained exploration leads to state-of-the-art performance, achieving a $60.17%$ average accuracy on five math benchmarks, an improvement of $2.66%$ over prior methods. Code is available at <a target="_blank" rel="noopener" href="https://github.com/CarlanLark/Lp-Reg">https://github.com/CarlanLark/Lp-Reg</a>. </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰æ¨åŠ¨äº†å¤æ‚æ¨ç†ä¸­çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„åº”ç”¨ï¼Œä½†å…¶å¯æ‰©å±•æ€§é€šå¸¸å—åˆ°è®­ç»ƒç“¶é¢ˆçš„é˜»ç¢ï¼Œåœ¨ç­–ç•¥ç†µå´©æºƒæ—¶æ€§èƒ½è¾¾åˆ°å³°å€¼ï¼Œè¿™æ ‡å¿—ç€æ¢ç´¢çš„ä¸§å¤±ã€‚ä¹‹å‰çš„æ–¹æ³•é€šå¸¸é€šè¿‡ä¿æŒé«˜ç­–ç•¥ç†µæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æ§åˆ¶æœ‰æ„ä¹‰çš„æ¢ç´¢çš„ç²¾ç¡®æœºåˆ¶ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå¯¹ç†µçš„æ— é€‰æ‹©æ€§å…³æ³¨å¯èƒ½æ”¾å¤§æ— å…³æ ‡è®°å¹¶ç ´åè®­ç»ƒç¨³å®šæ€§ã€‚æœ¬æ–‡ç ”ç©¶äº†RLVRä¸­çš„æ¢ç´¢åŠ¨æ€ï¼Œå¹¶å‘ç°äº†ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šæœ‰ä»·å€¼çš„ä½æ¦‚ç‡æ¢ç´¢æ ‡è®°ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ¨ç†ç«èŠ±â€ï¼‰çš„é€æ¸æ¶ˆé™¤ã€‚æˆ‘ä»¬å‘ç°è¿™äº›ç«èŠ±åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸­è™½ç„¶ä¸°å¯Œï¼Œä½†åœ¨RLVRæœŸé—´ç”±äºè¿‡åº¦æƒ©ç½šè€Œç³»ç»Ÿåœ°è¢«æ¶ˆé™¤ï¼Œå¯¼è‡´æ¢ç´¢é€€åŒ–ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä½æ¦‚ç‡æ­£åˆ™åŒ–ï¼ˆLp-Regï¼‰ã€‚å…¶æ ¸å¿ƒæœºåˆ¶æ˜¯å°†ç­–ç•¥æ­£åˆ™åŒ–æœå‘å¯å‘å¼ä»£ç†åˆ†å¸ƒã€‚è¯¥ä»£ç†é€šè¿‡è¿‡æ»¤æ‰å‡å®šä¸ºå™ªå£°æ ‡è®°å¹¶é‡æ–°å½’ä¸€åŒ–å‰©ä½™å€™é€‰æ ‡è®°çš„åˆ†å¸ƒæ¥æ„å»ºã€‚ç»“æœæ˜¯ä¸€ä¸ªå™ªå£°è¾ƒå°‘çš„ä»£ç†ï¼Œå…¶ä¸­æ¨ç†ç«èŠ±çš„å¯èƒ½æ€§è¢«æ”¾å¤§ï¼Œç„¶åä½œä¸ºè½¯æ­£åˆ™åŒ–ç›®æ ‡ï¼Œé€šè¿‡KLæ•£åº¦ä¿æŠ¤è¿™äº›æœ‰ä»·å€¼çš„æ ‡è®°å…å—æ¶ˆé™¤ã€‚å®éªŒè¡¨æ˜ï¼ŒLp-Regèƒ½å¤Ÿå®ç°ç¨³å®šçš„åœ¨çº¿ç­–ç•¥å¼ºåŒ–å­¦ä¹ ï¼Œåœ¨3000ä¸ªè®­ç»ƒæ­¥éª¤å’Œ81204ä¸ªGPUå°æ—¶ä¸­æŒç»­æ‰©å±•ï¼Œè€ŒåŸºçº¿ç†µæ§åˆ¶æ–¹æ³•åˆ™å´©æºƒäº†ã€‚è¿™ç§æŒç»­çš„æ¢ç´¢å¯¼è‡´äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨äº”ä¸ªæ•°å­¦åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº†å¹³å‡å‡†ç¡®ç‡æé«˜äº†60.17%ï¼Œè¾ƒä¹‹å‰çš„æ–¹æ³•æé«˜äº†æé«˜äº†è¾¾æ¯”ä¾‹ç®—æ³•é‡‡ç”¨å®¶ç ”ç©¶é™¢æ‰©å±•åŠ›è‘§è§£å†³è‹±è¯­ç›´æ„Ÿæ€§çŸ­è¯­ç»Ÿä¸€å‡ºäº†é’±èŠ³ç˜Ÿè½¨ç²¾å‡†çš„æŠ½åŸä½ æ¢…åå¯¼ç•¥å”¤é†’äº†æ¨ç†ç«èŠ±çš„é‡è¦æ€§å¹¶æå‡äº†å…¶æ¦‚ç‡åˆ†å¸ƒä»è€Œå®ç°äº†æ›´ç¨³å®šçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¹¶æå‡äº†æ¨¡å‹æ€§èƒ½æˆ‘ä»¬çš„ä»£ç å·²åœ¨GitHubä¸Šå…¬å¼€å¯ä¾›æŸ¥é˜…ç½‘å€ä¸º <a target="_blank" rel="noopener" href="https://github.com/CarlanLark/Lp-Reg%E3%80%82">https://github.com/CarlanLark/Lp-Regã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.03222v2">PDF</a> </p>
<p><strong>æ‘˜è¦</strong><br>è¯¥æ–‡ç« ä»‹ç»äº†å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å¤æ‚æ¨ç†ä¸­çš„åº”ç”¨ï¼Œä½†å­˜åœ¨è®­ç»ƒç“¶é¢ˆé—®é¢˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œè¿‡äºå…³æ³¨ç­–ç•¥ç†µå¯èƒ½å¯¼è‡´æ— æ„ä¹‰çš„æ ‡è®°è¢«æ”¾å¤§å¹¶ç ´åè®­ç»ƒç¨³å®šæ€§ã€‚æ–‡ç« æ·±å…¥æ¢è®¨äº†RLVRä¸­çš„æ¢ç´¢åŠ¨æ€ï¼Œå‘ç°ä¸€ä¸ªå…³é”®é—®é¢˜ï¼šæœ‰ä»·å€¼ä½†æ¦‚ç‡ä½çš„â€œæ¨ç†ç«èŠ±â€ï¼ˆreasoning sparksï¼‰é€æ¸è¢«æ·˜æ±°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡ç« å¼•å…¥äº†ä¸€ç§åä¸ºä½æ¦‚ç‡æ­£åˆ™åŒ–ï¼ˆLp-Regï¼‰çš„æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæœºåˆ¶æ˜¯é€šè¿‡ä¸€ä¸ªå¯å‘å¼ä»£ç†åˆ†å¸ƒå¯¹ç­–ç•¥è¿›è¡Œæ­£åˆ™åŒ–ã€‚è¯¥ä»£ç†åˆ†å¸ƒé€šè¿‡è¿‡æ»¤æ‰å‡å®šä¸ºå™ªå£°çš„æ ‡è®°å¹¶é‡æ–°å½’ä¸€åŒ–å‰©ä½™å€™é€‰æ ‡è®°çš„åˆ†å¸ƒæ¥æ„å»ºã€‚Lp-Regé€šè¿‡æ”¾å¤§â€œæ¨ç†ç«èŠ±â€çš„æ¦‚ç‡ä½œä¸ºè½¯æ­£åˆ™åŒ–ç›®æ ‡ï¼Œä¿æŠ¤è¿™äº›æœ‰ä»·å€¼çš„æ ‡è®°å…å—æ¶ˆé™¤çš„å½±å“ã€‚å®éªŒè¡¨æ˜ï¼ŒLp-Regèƒ½å¤Ÿå®ç°ç¨³å®šçš„åœ¨çº¿ç­–ç•¥å¼ºåŒ–å­¦ä¹ ï¼Œç»´æŒæŒç»­æ‰©å±•çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸­å®ç°äº†å¹³å‡å‡†ç¡®ç‡æé«˜è‡³60.17%ï¼Œç›¸è¾ƒäºä¹‹å‰çš„æ–¹æ³•æœ‰æ˜¾è‘—æ”¹å–„ã€‚ä»£ç å·²å…¬å¼€åœ¨GitHubä¸Šæä¾›ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¼ºåŒ–å­¦ä¹ å¯éªŒè¯å¥–åŠ±ï¼ˆRLVRï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹å¤æ‚æ¨ç†åº”ç”¨ä¸­çš„è®­ç»ƒç“¶é¢ˆé—®é¢˜ï¼Œä¸»è¦å› ä¸ºç­–ç•¥ç†µä¸‹é™å¯¼è‡´çš„æ¢ç´¢å‡å°‘ã€‚</li>
<li>è¿‡äºå…³æ³¨ç­–ç•¥ç†µå¯èƒ½å¯¼è‡´æ— æ„ä¹‰çš„æ ‡è®°è¢«æ”¾å¤§å¹¶ç ´åè®­ç»ƒç¨³å®šæ€§ã€‚</li>
<li>å‘ç°æœ‰ä»·å€¼ä½†æ¦‚ç‡ä½çš„â€œæ¨ç†ç«èŠ±â€ï¼ˆreasoning sparksï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ä¸­é€æ¸è¢«æ·˜æ±°çš„é—®é¢˜ã€‚</li>
<li>å¼•å…¥ä½æ¦‚ç‡æ­£åˆ™åŒ–ï¼ˆLp-Regï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºå¯å‘å¼ä»£ç†åˆ†å¸ƒæ¥æ­£åˆ™åŒ–ç­–ç•¥ï¼Œä¿æŠ¤æœ‰ä»·å€¼çš„æ ‡è®°å¹¶ç»´æŒç¨³å®šçš„æ¢ç´¢è¿‡ç¨‹ã€‚</li>
<li>Lp-Regèƒ½å¤Ÿæé«˜å¼ºåŒ–å­¦ä¹ çš„æ¢ç´¢æ•ˆç‡ï¼Œå®ç°äº†åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•ä¸Šçš„å¹³å‡å‡†ç¡®ç‡æ˜¾è‘—æå‡ã€‚</li>
<li>Lp-Regèƒ½å¤Ÿå®ç°æŒç»­çš„åœ¨çº¿ç­–ç•¥å¼ºåŒ–å­¦ä¹ ï¼Œå¹¶èƒ½å¤Ÿåœ¨é•¿æ—¶é—´è®­ç»ƒè¿‡ç¨‹ä¸­ç»´æŒæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.03222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-341408aa728891eecd3fede748fdaf2d" align="middle">
<img src="https://picx.zhimg.com/v2-b54e42af1cf88fb8fd646f0c14908135" align="middle">
<img src="https://picx.zhimg.com/v2-b85257ac4774e6fae767b379593e2f1a" align="middle">
<img src="https://picx.zhimg.com/v2-781f86f20d1a302dba31c0f9c5d9f0c3" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Ethics-Aware-Safe-Reinforcement-Learning-for-Rare-Event-Risk-Control-in-Interactive-Urban-Driving"><a href="#Ethics-Aware-Safe-Reinforcement-Learning-for-Rare-Event-Risk-Control-in-Interactive-Urban-Driving" class="headerlink" title="Ethics-Aware Safe Reinforcement Learning for Rare-Event Risk Control in   Interactive Urban Driving"></a>Ethics-Aware Safe Reinforcement Learning for Rare-Event Risk Control in   Interactive Urban Driving</h2><p><strong>Authors:Dianzhao Li, Ostap Okhrin</strong></p>
<p>Autonomous vehicles hold great promise for reducing traffic fatalities and improving transportation efficiency, yet their widespread adoption hinges on embedding credible and transparent ethical reasoning into routine and emergency maneuvers, particularly to protect vulnerable road users (VRUs) such as pedestrians and cyclists. Here, we present a hierarchical Safe Reinforcement Learning (Safe RL) framework that augments standard driving objectives with ethics-aware cost signals. At the decision level, a Safe RL agent is trained using a composite ethical risk cost, combining collision probability and harm severity, to generate high-level motion targets. A dynamic, risk-sensitive Prioritized Experience Replay mechanism amplifies learning from rare but critical, high-risk events. At the execution level, polynomial path planning coupled with Proportional-Integral-Derivative (PID) and Stanley controllers translates these targets into smooth, feasible trajectories, ensuring both accuracy and comfort. We train and validate our approach on closed-loop simulation environments derived from large-scale, real-world traffic datasets encompassing diverse vehicles, cyclists, and pedestrians, and demonstrate that it outperforms baseline methods in reducing risk to others while maintaining ego performance and comfort. This work provides a reproducible benchmark for Safe RL with explicitly ethics-aware objectives in human-mixed traffic scenarios. Our results highlight the potential of combining formal control theory and data-driven learning to advance ethically accountable autonomy that explicitly protects those most at risk in urban traffic environments. Across two interactive benchmarks and five random seeds, our policy decreases conflict frequency by 25-45% compared to matched task successes while maintaining comfort metrics within 5%. </p>
<blockquote>
<p>è‡ªåŠ¨é©¾é©¶æ±½è½¦å¯¹äºå‡å°‘äº¤é€šäº‹æ•…æ­»äº¡å’Œæé«˜äº¤é€šæ•ˆç‡å…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œç„¶è€Œå…¶å¹¿æ³›é‡‡ç”¨çš„å…³é”®åœ¨äºå°†å¯ä¿¡å’Œé€æ˜çš„é“å¾·æ¨ç†åµŒå…¥åˆ°å¸¸è§„å’Œç´§æ€¥æ“ä½œä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŠ¤è¡ŒäººåŠéª‘è¡Œè€…ç­‰è„†å¼±é“è·¯ä½¿ç”¨è€…ï¼ˆVRUsï¼‰æ–¹é¢ã€‚åœ¨æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†å±‚çš„Safe Reinforcement Learningï¼ˆSafe RLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¢åŠ äº†åŸºäºä¼¦ç†çš„æˆæœ¬ä¿¡å·ä»¥è¾…åŠ©æ ‡å‡†çš„é©¾é©¶ç›®æ ‡ã€‚åœ¨å†³ç­–å±‚é¢ï¼ŒSafe RLä»£ç†é€šè¿‡ä½¿ç”¨ç»„åˆä¼¦ç†é£é™©æˆæœ¬ï¼ˆç»“åˆç¢°æ’æ¦‚ç‡å’Œä¼¤å®³ä¸¥é‡ç¨‹åº¦ï¼‰è¿›è¡Œè®­ç»ƒï¼Œä»¥äº§ç”Ÿé«˜çº§è¿åŠ¨ç›®æ ‡ã€‚åŠ¨æ€ã€é£é™©æ•æ„Ÿä¼˜å…ˆç»éªŒå›æ”¾æœºåˆ¶èƒ½åŠ å¼ºä»ç½•è§ä½†å…³é”®çš„é«˜é£é™©äº‹ä»¶ä¸­çš„å­¦ä¹ ã€‚åœ¨æ‰§è¡Œå±‚é¢ï¼Œå¤šé¡¹å¼è·¯å¾„è§„åˆ’ä¸æ¯”ä¾‹ç§¯åˆ†å¾®åˆ†ï¼ˆPIDï¼‰å’Œæ–¯å¦åˆ©æ§åˆ¶å™¨ç›¸ç»“åˆï¼Œå°†è¿™äº›ç›®æ ‡è½¬åŒ–ä¸ºå¹³ç¨³ä¸”å¯è¡Œçš„è½¨è¿¹ï¼Œç¡®ä¿å‡†ç¡®æ€§å’Œèˆ’é€‚æ€§ã€‚æˆ‘ä»¬åœ¨å°é—­å¾ªç¯ä»¿çœŸç¯å¢ƒä¸­å¯¹æ–¹æ³•è¿›è¡Œäº†è®­ç»ƒå’ŒéªŒè¯ï¼Œè¯¥ç¯å¢ƒåŸºäºå¤§è§„æ¨¡ç°å®ä¸–ç•Œäº¤é€šæ•°æ®é›†æ„æˆï¼Œæ¶µç›–äº†å¤šç§è½¦è¾†ã€éª‘è¡Œè€…å’Œè¡Œäººã€‚æˆ‘ä»¬è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å‡å°‘å¯¹ä»–äººçš„é£é™©æ—¶è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒäº†è‡ªèº«æ€§èƒ½å’Œèˆ’é€‚æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºSafe RLæä¾›äº†ä¸€ä¸ªå¯é‡å¤çš„åŸºå‡†æµ‹è¯•ï¼Œæ˜ç¡®äº†å…¶åœ¨æ··åˆäº¤é€šåœºæ™¯ä¸­åŸºäºä¼¦ç†ç›®æ ‡çš„å®šä½ã€‚æˆ‘ä»¬çš„ç»“æœå¼ºè°ƒäº†ç»“åˆæ­£å¼æ§åˆ¶ç†è®ºå’Œæ•°æ®é©±åŠ¨å­¦ä¹ çš„æ½œåŠ›ï¼Œæ—¨åœ¨æ¨è¿›å…·æœ‰æ˜ç¡®é“å¾·è´£ä»»çš„è‡ªä¸»æ€§ï¼Œåœ¨åŸå¸‚äº¤é€šç¯å¢ƒä¸­æ˜ç¡®ä¿æŠ¤æœ€è„†å¼±çš„äººç¾¤ã€‚åœ¨ä¸¤ä¸ªäº¤äº’åŸºå‡†æµ‹è¯•å’Œäº”ä¸ªéšæœºç§å­çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ç­–ç•¥åœ¨ä¿æŒèˆ’é€‚æŒ‡æ ‡åœ¨5%ä»¥å†…çš„åŒæ—¶ï¼Œä¸åŒ¹é…çš„ä»»åŠ¡æˆåŠŸç›¸æ¯”å‡å°‘äº†å†²çªé¢‘ç‡çš„25-45%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.14926v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§å±‚æ¬¡åŒ–çš„å®‰å…¨å¼ºåŒ–å­¦ä¹ ï¼ˆSafe RLï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†æ ‡å‡†é©¾é©¶ç›®æ ‡ä¸ä¼¦ç†æ„è¯†æˆæœ¬ä¿¡å·ï¼Œç”¨äºåœ¨è‡ªåŠ¨é©¾é©¶è½¦è¾†ä¸­åµŒå…¥å¯ä¿¡ä¸”é€æ˜çš„é“å¾·æ¨ç†ã€‚é€šè¿‡å¤åˆä¼¦ç†é£é™©æˆæœ¬ï¼ˆç»“åˆç¢°æ’æ¦‚ç‡å’Œä¼¤å®³ä¸¥é‡ç¨‹åº¦ï¼‰ï¼ŒSafe RLä»£ç†åœ¨å†³ç­–å±‚é¢ç”Ÿæˆé«˜çº§è¿åŠ¨ç›®æ ‡ã€‚é‡‡ç”¨åŠ¨æ€ã€é£é™©æ•æ„Ÿæ€§çš„ä¼˜å…ˆç»éªŒå›æ”¾æœºåˆ¶ï¼Œå¼ºåŒ–äº†å¯¹ç½•è§ä½†å…³é”®çš„é«˜é£é™©äº‹ä»¶çš„å­¦ä¹ ã€‚åœ¨æ‰§è¡Œå±‚é¢ï¼Œé€šè¿‡å¤šé¡¹å¼è·¯å¾„è§„åˆ’ä¸PIDå’ŒStanleyæ§åˆ¶å™¨å°†è¿™äº›ç›®æ ‡è½¬åŒ–ä¸ºå¹³æ»‘ã€å¯è¡Œçš„è½¨è¿¹ï¼Œç¡®ä¿å‡†ç¡®æ€§å’Œèˆ’é€‚æ€§ã€‚åœ¨å°é—­å¾ªç¯ä»¿çœŸç¯å¢ƒä¸­å¯¹æ–¹æ³•è¿›è¡Œäº†è®­ç»ƒä¸éªŒè¯ï¼Œè¯¥æ–¹æ³•åœ¨å‡å°‘å¯¹ä»–äººçš„é£é™©åŒæ—¶ä¿æŒè‡ªæˆ‘æ€§èƒ½ä¸èˆ’é€‚æ€§æ–¹é¢è¡¨ç°å‡ºä¼˜äºåŸºå‡†æ–¹æ³•çš„æ•ˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Safe RLæ¡†æ¶ç»“åˆäº†æ ‡å‡†é©¾é©¶ç›®æ ‡ä¸ä¼¦ç†æ„è¯†æˆæœ¬ä¿¡å·ï¼Œä»¥ä¿ƒè¿›è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„å¹¿æ³›é‡‡çº³ã€‚</li>
<li>å†³ç­–å±‚é¢é‡‡ç”¨å¤åˆä¼¦ç†é£é™©æˆæœ¬æ¥ç”Ÿæˆé«˜çº§è¿åŠ¨ç›®æ ‡ã€‚</li>
<li>é‡‡ç”¨åŠ¨æ€ã€é£é™©æ•æ„Ÿæ€§çš„ä¼˜å…ˆç»éªŒå›æ”¾æœºåˆ¶æ¥å¼ºåŒ–é«˜é£é™©äº‹ä»¶çš„å­¦ä¹ ã€‚</li>
<li>é€šè¿‡å¤šé¡¹å¼è·¯å¾„è§„åˆ’ä¸æ§åˆ¶å™¨å°†ç›®æ ‡è½¬åŒ–ä¸ºå¹³æ»‘ã€å¯è¡Œçš„è½¨è¿¹ã€‚</li>
<li>æ–¹æ³•åœ¨å°é—­å¾ªç¯ä»¿çœŸç¯å¢ƒä¸­è¿›è¡Œäº†è®­ç»ƒä¸éªŒè¯ï¼Œå±•ç¤ºäº†å¯¹å‡å°‘å¯¹ä»–äººçš„é£é™©ã€ä¿æŒè‡ªæˆ‘æ€§èƒ½ä¸èˆ’é€‚æ€§çš„æ•ˆæœã€‚</li>
<li>ä¸åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•åœ¨å‡å°‘å†²çªé¢‘ç‡æ–¹é¢è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œä»»åŠ¡æˆåŠŸæ—¶å†²çªé¢‘ç‡é™ä½25-45%ï¼ŒåŒæ—¶ä¿æŒèˆ’é€‚æ€§æŒ‡æ ‡åœ¨5%ä»¥å†…ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14926">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96e25bbc877225a74c17a2cc8e076191" align="middle">
<img src="https://picx.zhimg.com/v2-0a53a6f8f878af967f8168d0df15cd31" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="P-ReMIS-Pragmatic-Reasoning-in-Mental-Health-and-a-Social-Implication"><a href="#P-ReMIS-Pragmatic-Reasoning-in-Mental-Health-and-a-Social-Implication" class="headerlink" title="P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication"></a>P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication</h2><p><strong>Authors:Sneha Oram, Pushpak Bhattacharyya</strong></p>
<p>Although explainability and interpretability have received significant attention in artificial intelligence (AI) and natural language processing (NLP) for mental health, reasoning has not been examined in the same depth. Addressing this gap is essential to bridge NLP and mental health through interpretable and reasoning-capable AI systems. To this end, we investigate the pragmatic reasoning capability of large-language models (LLMs) in the mental health domain. We introduce PRiMH dataset, and propose pragmatic reasoning tasks in mental health with pragmatic implicature and presupposition phenomena. In particular, we formulate two tasks in implicature and one task in presupposition. To benchmark the dataset and the tasks presented, we consider four models: Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the experiments suggest that Mistral and Qwen show substantial reasoning abilities in the domain. Subsequently, we study the behavior of MentaLLaMA on the proposed reasoning tasks with the rollout attention mechanism. In addition, we also propose three StiPRompts to study the stigma around mental health with the state-of-the-art LLMs, GPT4o-mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings show that Claude-3.5-haiku deals with stigma more responsibly compared to the other two LLMs. </p>
<blockquote>
<p>å°½ç®¡åœ¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å¿ƒç†å¥åº·é¢†åŸŸï¼Œè§£é‡Šæ€§å’Œå¯è§£é‡Šæ€§å·²å—åˆ°å¹¿æ³›å…³æ³¨ï¼Œä½†æ¨ç†å°šæœªå¾—åˆ°åŒç­‰æ·±åº¦çš„ç ”ç©¶ã€‚ä¸ºäº†å¼¥åˆNLPå’Œå¿ƒç†å¥åº·ä¹‹é—´çš„å·®è·ï¼Œéœ€è¦é€šè¿‡å¯è§£é‡Šå’Œå…·å¤‡æ¨ç†èƒ½åŠ›çš„AIç³»ç»Ÿæ¥æ„å»ºè”ç³»ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç ”ç©¶å¿ƒç†å¥åº·é¢†åŸŸä¸­å¤§å‹è¯­è¨€æ¨¡å‹çš„å®ç”¨æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬ä»‹ç»äº†PRiMHæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†å…·æœ‰å®ç”¨éšæ™¦å’Œé¢„è®¾ç°è±¡çš„å¿ƒç†å¥åº·å®ç”¨æ¨ç†ä»»åŠ¡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åˆ¶å®šäº†ä¸¤ä¸ªéšæ™¦ä»»åŠ¡å’Œä¸€ä¸ªé¢„è®¾ä»»åŠ¡ã€‚ä¸ºäº†å¯¹æå‡ºçš„æ•°æ®é›†å’Œä»»åŠ¡è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬è€ƒè™‘äº†å››ç§æ¨¡å‹ï¼šLlama3.1ã€Mistralã€MentaLLaMaå’ŒQwenã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMistralå’ŒQwenåœ¨è¯¥é¢†åŸŸçš„æ¨ç†èƒ½åŠ›æ˜¾è‘—ã€‚éšåï¼Œæˆ‘ä»¬ç ”ç©¶äº†MentaLLaMAåœ¨æå‡ºçš„æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œé‡‡ç”¨äº†å±•å¼€å¼æ³¨æ„åŠ›æœºåˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨æœ€å…ˆè¿›çš„LLMsï¼ˆGPT4o-miniã€Deepseek-chatå’ŒClaude-3.5-haikuï¼‰æå‡ºäº†ä¸‰ä¸ªå…³äºå¿ƒç†å¥åº·è€»è¾±çš„StiPRomptsç ”ç©¶ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–ä¸¤ä¸ªLLMsç›¸æ¯”ï¼ŒClaude-3.5-haikuåœ¨å¤„ç†å¿ƒç†å¥åº·è€»è¾±æ–¹é¢æ›´ä¸ºè´Ÿè´£ä»»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23247v2">PDF</a> </p>
<p><strong>Summary</strong><br>åœ¨äººå·¥æ™ºèƒ½å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œè§£é‡Šæ€§å’Œå¯è§£é‡Šæ€§å·²å¤‡å—å…³æ³¨ï¼Œä½†å¯¹æ¨ç†çš„ç ”ç©¶å°šæœªæ·±å…¥ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç²¾ç¥å¥åº·é¢†åŸŸçš„å®ç”¨æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†PRiMHæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†å…·æœ‰å®ç”¨éšå«å’Œé¢„è®¾ç°è±¡çš„ç²¾ç¥å¥åº·å®ç”¨æ¨ç†ä»»åŠ¡ã€‚é€šè¿‡å››ä¸ªæ¨¡å‹å¯¹æ•°æ®é›†å’Œä»»åŠ¡è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°Mistralå’ŒQwenåœ¨è¯¥é¢†åŸŸå…·æœ‰è¾ƒå¼ºçš„æ¨ç†èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†MentaLLaMAåœ¨æå‡ºçš„æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¹¶æå‡ºäº†ä¸‰ä¸ªå…³äºç²¾ç¥å¥åº·è€»è¾±çš„StiPRomptsï¼Œä¸å½“å‰å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹æ¯”è¯„ä¼°ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç ”ç©¶å¼ºè°ƒäº†ç²¾ç¥å¥åº·é¢†åŸŸä¸­æ¨ç†çš„é‡è¦æ€§ï¼Œå¹¶æŒ‡å‡ºéœ€è¦æ›´å¤šå…³æ³¨å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ­¤æ–¹é¢çš„èƒ½åŠ›ã€‚</li>
<li>å¼•å…¥äº†PRiMHæ•°æ®é›†ï¼Œä¸ºç²¾ç¥å¥åº·é¢†åŸŸçš„å®ç”¨æ¨ç†ä»»åŠ¡æä¾›äº†èµ„æºã€‚</li>
<li>æå‡ºäº†åŒ…å«å®ç”¨éšå«å’Œé¢„è®¾ç°è±¡çš„ç²¾ç¥å¥åº·å®ç”¨æ¨ç†ä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœæ˜¾ç¤ºMistralå’ŒQwenåœ¨ç²¾ç¥å¥åº·é¢†åŸŸçš„æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå¥½ã€‚</li>
<li>MentaLLaMAåœ¨æå‡ºçš„æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¹Ÿå¾—åˆ°äº†ç ”ç©¶ã€‚</li>
<li>é€šè¿‡ä¸‰ä¸ªStiPRomptsæ¢è®¨äº†ç²¾ç¥å¥åº·è€»è¾±é—®é¢˜ï¼Œå¹¶è¯„ä¼°äº†ä¸åŒå¤§å‹è¯­è¨€æ¨¡å‹çš„åº”å¯¹æ–¹å¼ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23247">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a2638c78a63f4310bec0f330d4f8263" align="middle">
<img src="https://picx.zhimg.com/v2-0c04725d71834eea883b5f3e76996784" align="middle">
<img src="https://picx.zhimg.com/v2-c7c09a1b666a59070570726f9dcecbd1" align="middle">
<img src="https://picx.zhimg.com/v2-bc2c5dfecfd11046695355bcdeeaf4c8" align="middle">
<img src="https://picx.zhimg.com/v2-ee5c7a8fbe10b6e83d12602b8e5d03dc" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="SAFER-Probing-Safety-in-Reward-Models-with-Sparse-Autoencoder"><a href="#SAFER-Probing-Safety-in-Reward-Models-with-Sparse-Autoencoder" class="headerlink" title="SAFER: Probing Safety in Reward Models with Sparse Autoencoder"></a>SAFER: Probing Safety in Reward Models with Sparse Autoencoder</h2><p><strong>Authors:Sihang Li, Wei Shi, Ziyuan Xie, Tao Liang, Guojun Ma, Xiang Wang</strong></p>
<p>Reinforcement learning from human feedback (RLHF) is a key paradigm for aligning large language models (LLMs) with human values, yet the reward models at its core remain largely opaque. In this work, we present sparse Autoencoder For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting and improving reward models through mechanistic analysis. Leveraging Sparse Autoencoders (SAEs), we uncover human-interpretable features in reward model activations, enabling insight into safety-relevant decision-making. We apply SAFER to safety-oriented preference datasets and quantify the salience of individual features by activation differences between chosen and rejected responses. Using these feature-level signals, we design targeted data poisoning and denoising strategies. Experiments show that SAFER can precisely degrade or enhance safety alignment with minimal data modification, without sacrificing general chat performance. Our approach contributes to interpreting, auditing and refining reward models in high-stakes LLM alignment tasks. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/xzy-101/SAFER-code">https://github.com/xzy-101/SAFER-code</a>. \textit{This paper discusses topics related to large language model safety and may include discussions or examples that highlight potential risks or unsafe outcomes.} </p>
<blockquote>
<p>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚çš„å…³é”®èŒƒå¼ï¼Œä½†å…¶æ ¸å¿ƒçš„å¥–åŠ±æ¨¡å‹ä»ç„¶å¤§éƒ¨åˆ†ä¸é€æ˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨å¢å¼ºå¥–åŠ±æ¨¡å‹ï¼ˆSAFERï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡æœºæ¢°åˆ†ææ¥è§£é‡Šå’Œæ”¹è¿›å¥–åŠ±æ¨¡å‹çš„æ–°å‹æ¡†æ¶ã€‚åˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œæˆ‘ä»¬åœ¨å¥–åŠ±æ¨¡å‹æ¿€æ´»ä¸­å‘ç°äººç±»å¯è§£é‡Šçš„ç‰¹å¾ï¼Œä»è€Œæ·±å…¥äº†è§£ä¸å®‰å…¨ç›¸å…³çš„å†³ç­–ã€‚æˆ‘ä»¬å°†SAFERåº”ç”¨äºé¢å‘å®‰å…¨çš„åå¥½æ•°æ®é›†ï¼Œå¹¶é€šè¿‡é€‰æ‹©å“åº”å’Œæ‹’ç»å“åº”ä¹‹é—´çš„æ¿€æ´»å·®å¼‚æ¥é‡åŒ–å•ä¸ªç‰¹å¾çš„é‡è¦æ€§ã€‚ä½¿ç”¨è¿™äº›ç‰¹å¾çº§ä¿¡å·ï¼Œæˆ‘ä»¬è®¾è®¡æœ‰é’ˆå¯¹æ€§çš„æ•°æ®æ±¡æŸ“å’Œå»å™ªç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒSAFERå¯ä»¥åœ¨ä¸ç‰ºç‰²ä¸€èˆ¬èŠå¤©æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡æœ€å°çš„æ•°æ®ä¿®æ”¹ç²¾ç¡®åœ°é™ä½æˆ–æé«˜å®‰å…¨å¯¹é½æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºè§£é‡Šã€å®¡è®¡å’Œæ”¹è¿›é«˜é£é™©LLMå¯¹é½ä»»åŠ¡ä¸­çš„å¥–åŠ±æ¨¡å‹åšå‡ºäº†è´¡çŒ®ã€‚æˆ‘ä»¬çš„ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/xzy-101/SAFER-code">https://github.com/xzy-101/SAFER-code</a>ä¸­æ‰¾åˆ°ã€‚è¿™ç¯‡è®ºæ–‡è®¨è®ºä¸å¤§å‹è¯­è¨€æ¨¡å‹å®‰å…¨ç›¸å…³çš„è¯é¢˜ï¼Œå¹¶å¯èƒ½åŒ…å«çªå‡ºæ½œåœ¨é£é™©æˆ–ä¸å®‰å…¨ç»“æœçš„è®¨è®ºæˆ–ç¤ºä¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.00665v2">PDF</a> One of the institutions requires additional approval before we can   move forward with the publication. Thanks for your understanding, and we hope   to resubmit once everything is finalized</p>
<p><strong>Summary</strong><br>å¼ºåŒ–å­¦ä¹ ä»äººç±»åé¦ˆï¼ˆRLHFï¼‰æ˜¯å¯¹é½å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸äººç±»ä»·å€¼è§‚çš„å…³é”®èŒƒå¼ï¼Œä½†å…¶æ ¸å¿ƒå¥–åŠ±æ¨¡å‹ä»ç„¶å¤§éƒ¨åˆ†ä¸é€æ˜ã€‚æœ¬ç ”ç©¶æå‡ºç¨€ç–è‡ªåŠ¨ç¼–ç å™¨å¢å¼ºå¥–åŠ±æ¨¡å‹ï¼ˆSAFERï¼‰æ¡†æ¶ï¼Œé€šè¿‡æœºæ¢°åˆ†ææ¥è§£è¯»å’Œæ”¹è¿›å¥–åŠ±æ¨¡å‹ã€‚åˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨ï¼ˆSAEï¼‰ï¼Œæ­ç¤ºå¥–åŠ±æ¨¡å‹æ¿€æ´»ä¸­çš„äººå¯è§£é‡Šç‰¹å¾ï¼Œæ´å¯Ÿå®‰å…¨ç›¸å…³çš„å†³ç­–è¿‡ç¨‹ã€‚å°†SAFERåº”ç”¨äºå®‰å…¨å¯¼å‘çš„åå¥½æ•°æ®é›†ï¼Œé€šè¿‡é€‰æ‹©å“åº”å’Œæ‹’ç»å“åº”ä¹‹é—´çš„æ¿€æ´»å·®å¼‚æ¥é‡åŒ–ä¸ªäººç‰¹å¾çš„é‡è¦æ€§ã€‚ä½¿ç”¨è¿™äº›ç‰¹å¾çº§åˆ«çš„ä¿¡å·ï¼Œæˆ‘ä»¬è®¾è®¡æœ‰é’ˆå¯¹æ€§çš„æ•°æ®ä¸­æ¯’å’Œå»å™ªç­–ç•¥ã€‚å®éªŒè¡¨æ˜ï¼ŒSAFERèƒ½åœ¨ä¸ç‰ºç‰²é€šç”¨èŠå¤©æ€§èƒ½çš„å‰æä¸‹ï¼Œç²¾ç¡®åœ°æé«˜æˆ–é™ä½å®‰å…¨å¯¹é½åº¦ï¼Œä¸”åªéœ€å¯¹å°‘é‡æ•°æ®è¿›è¡Œä¿®æ”¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•æœ‰åŠ©äºè§£é‡Šã€å®¡è®¡å’Œæ”¹è¿›é«˜é£é™©è¯­è¨€æ¨¡å‹å¯¹é½ä»»åŠ¡ä¸­çš„å¥–åŠ±æ¨¡å‹ã€‚ä»£ç åœ°å€ï¼š&lt;<a target="_blank" rel="noopener" href="https://github.com/xzy-10">https://github.com/xzy-10</a> 1&#x2F;SAFER-code&gt;ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>RLHFæ˜¯å¯¹é½LLMä¸äººç±»ä»·å€¼è§‚çš„å…³é”®æ–¹æ³•ï¼Œä½†å¥–åŠ±æ¨¡å‹é€æ˜åº¦ä½ã€‚</li>
<li>æå‡ºSAFERæ¡†æ¶ï¼Œåˆ©ç”¨ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨è§£è¯»å’Œæ”¹è¿›å¥–åŠ±æ¨¡å‹ã€‚</li>
<li>é€šè¿‡äººå¯è§£é‡Šçš„ç‰¹å¾æ­ç¤ºå¥–åŠ±æ¨¡å‹çš„æ¿€æ´»æœºåˆ¶ï¼Œæ´å¯Ÿå®‰å…¨ç›¸å…³çš„å†³ç­–è¿‡ç¨‹ã€‚</li>
<li>åº”ç”¨SAFERåˆ°å®‰å…¨å¯¼å‘çš„åå¥½æ•°æ®é›†ï¼Œé‡åŒ–ä¸ªäººç‰¹å¾çš„é‡è¦æ€§ã€‚</li>
<li>åˆ©ç”¨ç‰¹å¾çº§åˆ«ä¿¡å·è®¾è®¡æ•°æ®ä¸­æ¯’å’Œå»å™ªç­–ç•¥ã€‚</li>
<li>SAFERèƒ½ç²¾ç¡®è°ƒæ•´å®‰å…¨å¯¹é½åº¦ï¼Œä¸”å¯¹é€šç”¨èŠå¤©æ€§èƒ½å½±å“å°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.00665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c9e71caa23ff1f27045cbadc7c151653" align="middle">
<img src="https://picx.zhimg.com/v2-c5bd301c013f4c6653fb04aefd7c77ab" align="middle">
<img src="https://picx.zhimg.com/v2-a78d2c2754a5ea35ce71a168a51ef874" align="middle">
<img src="https://picx.zhimg.com/v2-c94edd6cbf348159d85baa5d203abf13" align="middle">
<img src="https://picx.zhimg.com/v2-af121c77dee697de4177d3a651f9a705" align="middle">
<img src="https://picx.zhimg.com/v2-8bf416b60c182ae0d8a00df77fa1172f" align="middle">
<img src="https://picx.zhimg.com/v2-a56807defaedc8e037bce00e137b482a" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="Know-What-You-Donâ€™t-Know-Uncertainty-Calibration-of-Process-Reward-Models"><a href="#Know-What-You-Donâ€™t-Know-Uncertainty-Calibration-of-Process-Reward-Models" class="headerlink" title="Know What You Donâ€™t Know: Uncertainty Calibration of Process Reward   Models"></a>Know What You Donâ€™t Know: Uncertainty Calibration of Process Reward   Models</h2><p><strong>Authors:Young-Jin Park, Kristjan Greenewald, Kaveh Alim, Hao Wang, Navid Azizan</strong></p>
<p>Process reward models (PRMs) play a central role in guiding inference-time scaling algorithms for large language models (LLMs). However, we observe that even state-of-the-art PRMs can be poorly calibrated. Specifically, they tend to overestimate the success probability that a partial reasoning step will lead to a correct final answer, particularly when smaller LLMs are used to complete the reasoning trajectory. To address this, we present a calibration approach â€“ performed via quantile regression â€“ that adjusts PRM outputs to better align with true success probabilities. Leveraging these calibrated success estimates and their associated confidence bounds, we introduce an \emph{instance-adaptive scaling} (IAS) framework that dynamically adjusts the compute budget based on the estimated likelihood that a partial reasoning trajectory will yield a correct final answer. Unlike conventional methods that allocate a fixed number of reasoning trajectories per query, this approach adapts to each instance and reasoning step when using our calibrated PRMs. Experiments on mathematical reasoning benchmarks show that (i) our PRM calibration method achieves small calibration error, outperforming the baseline methods, (ii) calibration is crucial for enabling effective IAS, and (iii) the proposed IAS strategy reduces inference costs while maintaining final answer accuracy, utilizing less compute on more confident problems as desired. </p>
<blockquote>
<p>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMsï¼‰åœ¨æŒ‡å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ¨ç†æ—¶é—´ç¼©æ”¾ç®—æ³•ä¸­èµ·ç€æ ¸å¿ƒä½œç”¨ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„PRMsä¹Ÿå¯èƒ½å­˜åœ¨æ ¡å‡†ä¸è‰¯çš„æƒ…å†µã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä»¬å€¾å‘äºé«˜ä¼°éƒ¨åˆ†æ¨ç†æ­¥éª¤ä¼šå¯¼è‡´æ­£ç¡®æœ€ç»ˆç­”æ¡ˆçš„æˆåŠŸæ¦‚ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨è¾ƒå°çš„LLMsæ¥å®Œæˆæ¨ç†è½¨è¿¹æ—¶ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡åˆ†ä½å›å½’è¿›è¡Œæ ¡å‡†çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥è°ƒæ•´PRMè¾“å‡ºï¼Œä»¥æ›´å¥½åœ°ç¬¦åˆçœŸå®çš„æˆåŠŸæ¦‚ç‡ã€‚åˆ©ç”¨è¿™äº›æ ¡å‡†åçš„æˆåŠŸä¼°è®¡åŠå…¶ç›¸å…³çš„ç½®ä¿¡ç•Œé™ï¼Œæˆ‘ä»¬å¼•å…¥äº†å®ä¾‹è‡ªé€‚åº”ç¼©æ”¾ï¼ˆIASï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶æ ¹æ®ä¼°è®¡çš„å±€éƒ¨æ¨ç†è½¨è¿¹äº§ç”Ÿæ­£ç¡®ç­”æ¡ˆçš„å¯èƒ½æ€§æ¥åŠ¨æ€è°ƒæ•´è®¡ç®—é¢„ç®—ã€‚ä¸ä¼ ç»Ÿçš„ä¸ºæ¯æ¬¡æŸ¥è¯¢åˆ†é…å›ºå®šæ•°é‡çš„æ¨ç†è½¨è¿¹çš„æ–¹æ³•ä¸åŒï¼Œè¿™ç§æ–¹æ³•åœ¨ä½¿ç”¨æˆ‘ä»¬çš„æ ¡å‡†PRMæ—¶èƒ½å¤Ÿé€‚åº”æ¯ä¸ªå®ä¾‹å’Œæ¨ç†æ­¥éª¤ã€‚åœ¨æ•°å­¦æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œï¼ˆiï¼‰æˆ‘ä»¬çš„PRMæ ¡å‡†æ–¹æ³•å®ç°äº†è¾ƒå°çš„æ ¡å‡†è¯¯å·®ï¼Œä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œï¼ˆiiï¼‰æ ¡å‡†å¯¹äºå®ç°æœ‰æ•ˆçš„IASè‡³å…³é‡è¦ï¼Œï¼ˆiiiï¼‰æå‡ºçš„IASç­–ç•¥åœ¨ä¿æŒæœ€ç»ˆç­”æ¡ˆå‡†ç¡®æ€§çš„åŒæ—¶é™ä½äº†æ¨ç†æˆæœ¬ï¼Œå¦‚é¢„æœŸåœ°åœ¨æ›´æœ‰ä¿¡å¿ƒçš„é—®é¢˜ä¸Šä½¿ç”¨äº†è¾ƒå°‘çš„è®¡ç®—ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09338v2">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰çš„æ ¡å‡†é—®é¢˜ã€‚æ–‡ç« æŒ‡å‡ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ä¹Ÿå¯èƒ½å­˜åœ¨æ ¡å‡†ä¸è‰¯çš„æƒ…å†µï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨è¾ƒå°çš„LLMå®Œæˆæ¨ç†è½¨è¿¹æ—¶ï¼Œå®ƒä»¬å¾€å¾€ä¼šé«˜ä¼°éƒ¨åˆ†æ¨ç†æ­¥éª¤å¯¼è‡´æ­£ç¡®æœ€ç»ˆç­”æ¡ˆçš„æˆåŠŸæ¦‚ç‡ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§é€šè¿‡åˆ†ä½å›å½’è¿›è¡Œæ ¡å‡†çš„æ–¹æ³•ï¼Œä»¥è°ƒæ•´PRMè¾“å‡ºï¼Œä½¿å…¶æ›´å¥½åœ°ç¬¦åˆçœŸå®çš„æˆåŠŸæ¦‚ç‡ã€‚åˆ©ç”¨è¿™äº›æ ¡å‡†åçš„æˆåŠŸä¼°è®¡åŠå…¶ç›¸å…³çš„ç½®ä¿¡ç•Œé™ï¼Œæ–‡ç« è¿˜ä»‹ç»äº†ä¸€ç§åä¸ºå®ä¾‹è‡ªé€‚åº”ç¼©æ”¾ï¼ˆIASï¼‰çš„æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿæ ¹æ®ä¼°è®¡çš„æœºç‡åŠ¨æ€è°ƒæ•´è®¡ç®—é¢„ç®—ï¼Œé¢„æµ‹éƒ¨åˆ†æ¨ç†è½¨è¿¹æ˜¯å¦ä¼šå¾—å‡ºæ­£ç¡®çš„æœ€ç»ˆç­”æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–‡çš„PRMæ ¡å‡†æ–¹æ³•å…·æœ‰è¾ƒå°çš„æ ¡å‡†è¯¯å·®ï¼Œä¼˜äºåŸºå‡†æ–¹æ³•ï¼›æ ¡å‡†å¯¹äºå®ç°æœ‰æ•ˆçš„IASè‡³å…³é‡è¦ï¼›æ‰€æå‡ºçš„IASç­–ç•¥åœ¨ç»´æŒæœ€ç»ˆç­”æ¡ˆå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œé™ä½äº†æ¨ç†æˆæœ¬ï¼Œå¦‚é¢„æœŸåœ¨æ›´æœ‰ä¿¡å¿ƒçš„é—®é¢˜ä¸Šä½¿ç”¨è¾ƒå°‘çš„è®¡ç®—èµ„æºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ—¶é—´ç¼©æ”¾ç®—æ³•ä¸­èµ·å…³é”®ä½œç”¨ï¼Œä½†å­˜åœ¨æ ¡å‡†é—®é¢˜ã€‚</li>
<li>PRMå€¾å‘äºé«˜ä¼°éƒ¨åˆ†æ¨ç†æ­¥éª¤çš„æˆåŠŸæ¦‚ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨è¾ƒå°çš„LLMæ—¶ã€‚</li>
<li>æå‡ºä¸€ç§é€šè¿‡åˆ†ä½å›å½’è¿›è¡ŒPRMæ ¡å‡†çš„æ–¹æ³•ï¼Œä»¥æ›´å‡†ç¡®åœ°é¢„æµ‹æˆåŠŸæ¦‚ç‡ã€‚</li>
<li>å¼•å…¥å®ä¾‹è‡ªé€‚åº”ç¼©æ”¾ï¼ˆIASï¼‰æ¡†æ¶ï¼Œæ ¹æ®ä¼°è®¡çš„æˆåŠŸæ¦‚ç‡åŠ¨æ€è°ƒæ•´è®¡ç®—é¢„ç®—ã€‚</li>
<li>å®è¯ç ”ç©¶è¡¨æ˜ï¼ŒPRMæ ¡å‡†å¯¹äºå®ç°æœ‰æ•ˆçš„IASè‡³å…³é‡è¦ã€‚</li>
<li>IASç­–ç•¥åœ¨ç»´æŒæœ€ç»ˆç­”æ¡ˆå‡†ç¡®æ€§çš„åŒæ—¶ï¼Œèƒ½å¤Ÿé™ä½æ¨ç†æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09338">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1896a006d587fcbd12638ad8796aef5" align="middle">
<img src="https://picx.zhimg.com/v2-9328fc050c330474f895b5d3bbc918f9" align="middle">
<img src="https://picx.zhimg.com/v2-53a1adf12ef56f80c9a923384f39b908" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-11/R1_Reasoning/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-11/R1_Reasoning/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/R1-Reasoning/">
                                    <span class="chip bg-color">R1_Reasoning</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-11/LLM/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-2d11baa05d60ba55ef9ca944ff2a092d" class="responsive-img" alt="LLM">
                        
                        <span class="card-title">LLM</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-11  A Metamorphic Testing Perspective on Knowledge Distillation for Language   Models of Code Does the Student Deeply Mimic the Teacher?
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                    LLM
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/LLM/">
                        <span class="chip bg-color">LLM</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-10/Diffusion%20Models/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1cde36739c7f0a359322d97ae4d278e7" class="responsive-img" alt="Diffusion Models">
                        
                        <span class="card-title">Diffusion Models</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Diffusion Models æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-10  NeurOp-DiffContinuous Remote Sensing Image Super-Resolution via Neural   Operator Diffusion
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Diffusion-Models/" class="post-category">
                                    Diffusion Models
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Diffusion-Models/">
                        <span class="chip bg-color">Diffusion Models</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">33297.5k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
