<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM">
    <meta name="description" content="LLM æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-11  A Metamorphic Testing Perspective on Knowledge Distillation for Language   Models of Code Does the Student Deeply Mimic the Teacher?">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-2d11baa05d60ba55ef9ca944ff2a092d')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/LLM/" class="post-category">
                                LLM
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-11
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    18.5k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    76 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-11-æ›´æ–°"><a href="#2025-11-11-æ›´æ–°" class="headerlink" title="2025-11-11 æ›´æ–°"></a>2025-11-11 æ›´æ–°</h1><h2 id="A-Metamorphic-Testing-Perspective-on-Knowledge-Distillation-for-Language-Models-of-Code-Does-the-Student-Deeply-Mimic-the-Teacher"><a href="#A-Metamorphic-Testing-Perspective-on-Knowledge-Distillation-for-Language-Models-of-Code-Does-the-Student-Deeply-Mimic-the-Teacher" class="headerlink" title="A Metamorphic Testing Perspective on Knowledge Distillation for Language   Models of Code: Does the Student Deeply Mimic the Teacher?"></a>A Metamorphic Testing Perspective on Knowledge Distillation for Language   Models of Code: Does the Student Deeply Mimic the Teacher?</h2><p><strong>Authors:Md. Abdul Awal, Mrigank Rochan, Chanchal K. Roy</strong></p>
<p>Transformer-based language models of code have achieved state-of-the-art performance across a wide range of software analytics tasks, but their practical deployment remains limited due to high computational costs, slow inference speeds, and significant environmental impact. To address these challenges, recent research has increasingly explored knowledge distillation as a method for compressing a large language model of code (the teacher) into a smaller model (the student) while maintaining performance. However, the degree to which a student model deeply mimics the predictive behavior and internal representations of its teacher remains largely unexplored, as current accuracy-based evaluation provides only a surface-level view of model quality and often fails to capture more profound discrepancies in behavioral fidelity between the teacher and student models. To address this gap, we empirically show that the student model often fails to deeply mimic the teacher model, resulting in up to 285% greater performance drop under adversarial attacks, which is not captured by traditional accuracy-based evaluation. Therefore, we propose MetaCompress, a metamorphic testing framework that systematically evaluates behavioral fidelity by comparing the outputs of teacher and student models under a set of behavior-preserving metamorphic relations. We evaluate MetaCompress on two widely studied tasks, using compressed versions of popular language models of code, obtained via three different knowledge distillation techniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress identifies up to 62% behavioral discrepancies in student models, underscoring the need for behavioral fidelity evaluation within the knowledge distillation pipeline and establishing MetaCompress as a practical framework for testing compressed language models of code derived through knowledge distillation. </p>
<blockquote>
<p>åŸºäºTransformerçš„ä»£ç è¯­è¨€æ¨¡å‹å·²åœ¨å„ç§è½¯ä»¶åˆ†æä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†ç”±äºè®¡ç®—æˆæœ¬é«˜ã€æ¨ç†é€Ÿåº¦æ…¢ä»¥åŠå¯¹ç¯å¢ƒäº§ç”Ÿé‡å¤§å½±å“ï¼Œå…¶å®è·µéƒ¨ç½²ä»ç„¶å—åˆ°é™åˆ¶ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæœ€è¿‘çš„ç ”ç©¶è¶Šæ¥è¶Šå€¾å‘äºæ¢ç´¢çŸ¥è¯†è’¸é¦ä½œä¸ºä¸€ç§æ–¹æ³•ï¼Œç”¨äºå°†å¤§å‹ä»£ç è¯­è¨€æ¨¡å‹ï¼ˆæ•™å¸ˆæ¨¡å‹ï¼‰å‹ç¼©æˆå°å‹æ¨¡å‹ï¼ˆå­¦ç”Ÿæ¨¡å‹ï¼‰åŒæ—¶ä¿æŒæ€§èƒ½ã€‚ç„¶è€Œï¼Œå­¦ç”Ÿæ¨¡å‹åœ¨å¤šå¤§ç¨‹åº¦ä¸Šæ·±åº¦æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹çš„é¢„æµ‹è¡Œä¸ºå’Œå†…éƒ¨è¡¨ç¤ºåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå°šæœªè¢«æ¢ç´¢ï¼Œå› ä¸ºå½“å‰çš„åŸºäºå‡†ç¡®åº¦çš„è¯„ä¼°åªèƒ½æä¾›æ¨¡å‹è´¨é‡çš„è¡¨é¢è§†å›¾ï¼Œå¹¶ä¸”ç»å¸¸æ— æ³•æ•è·æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹ä¹‹é—´è¡Œä¸ºä¿çœŸåº¦çš„æ›´æ·±å±‚æ¬¡å·®å¼‚ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬å®è¯å‘ç°å­¦ç”Ÿæ¨¡å‹å¾€å¾€æ— æ³•æ·±åº¦æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹ï¼Œå¯¼è‡´åœ¨å¯¹æŠ—æ€§æ”»å‡»ä¸‹æ€§èƒ½ä¸‹é™é«˜è¾¾285%ï¼Œè¿™æ˜¯ä¼ ç»ŸåŸºäºå‡†ç¡®åº¦çš„è¯„ä¼°æ‰€æ— æ³•æ•æ‰åˆ°çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MetaCompressï¼Œä¸€ä¸ªåŸºäºå…ƒå½¢æ€æµ‹è¯•çš„æ¡†æ¶ï¼Œé€šè¿‡æ¯”è¾ƒæ•™å¸ˆå’Œå­¦ç”Ÿåœ¨ä¸€ç»„ä¿æŒè¡Œä¸ºçš„å…ƒå½¢æ€å…³ç³»ä¸‹çš„è¾“å‡ºï¼Œç³»ç»Ÿåœ°è¯„ä¼°è¡Œä¸ºä¿çœŸåº¦ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå¹¿æ³›ç ”ç©¶çš„ä»»åŠ¡ä¸Šè¯„ä¼°äº†MetaCompressï¼Œä½¿ç”¨é€šè¿‡ä¸‰ç§ä¸åŒçš„çŸ¥è¯†è’¸é¦æŠ€æœ¯è·å¾—çš„æµè¡Œä»£ç è¯­è¨€æ¨¡å‹çš„å‹ç¼©ç‰ˆæœ¬ï¼šå‹ç¼©æœºã€åŒ–èº«å’Œå½¢æ€å˜åŒ–ã€‚ç»“æœè¡¨æ˜ï¼ŒMetaCompresså‘ç°äº†å­¦ç”Ÿæ¨¡å‹ä¸­é«˜è¾¾62%çš„è¡Œä¸ºå·®å¼‚ï¼Œè¿™å¼ºè°ƒäº†çŸ¥è¯†è’¸é¦ç®¡é“ä¸­è¡Œä¸ºä¿çœŸåº¦è¯„ä¼°çš„å¿…è¦æ€§ï¼Œå¹¶è¯æ˜äº†MetaCompressä½œä¸ºæµ‹è¯•é€šè¿‡çŸ¥è¯†è’¸é¦å¾—åˆ°çš„å‹ç¼©ä»£ç è¯­è¨€æ¨¡å‹çš„å®ç”¨æ¡†æ¶çš„åœ°ä½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05476v1">PDF</a> The paper is currently under review at a peer-reviewed journal</p>
<p><strong>Summary</strong>ï¼šåŸºäºTransformerçš„ä»£ç è¯­è¨€æ¨¡å‹åœ¨å„ç§è½¯ä»¶åˆ†æä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œä½†å…¶é«˜æ˜‚çš„è®¡ç®—æˆæœ¬ã€ç¼“æ…¢çš„æ¨ç†é€Ÿåº¦å’Œæ˜¾è‘—çš„ç¯å¢ƒå½±å“é™åˆ¶äº†å…¶å®é™…éƒ¨ç½²ã€‚ä¸ºåº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œè¿‘å¹´æ¥è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶é€šè¿‡çŸ¥è¯†è’¸é¦æ–¹æ³•å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆæ•™å¸ˆæ¨¡å‹ï¼‰ä»¥ç”Ÿæˆå°å‹æ¨¡å‹ï¼ˆå­¦ç”Ÿæ¨¡å‹ï¼‰å¹¶ä¿æŒæ€§èƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„çŸ¥è¯†è’¸é¦æŠ€æœ¯åœ¨è¯„ä¼°å­¦ç”Ÿæ¨¡å‹æ˜¯å¦æ·±åº¦æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œç°æœ‰çš„å‡†ç¡®æ€§è¯„ä¼°åªèƒ½æä¾›æ¨¡å‹è´¨é‡çš„è¡¨é¢è§†å›¾ï¼Œæ— æ³•æ•æ‰ä¸¤è€…ä¹‹é—´çš„è¡Œä¸ºå¿ å®åº¦å·®å¼‚ã€‚æœ¬æ–‡é€šè¿‡å®è¯ç ”ç©¶è¯æ˜ï¼Œå­¦ç”Ÿæ¨¡å‹é€šå¸¸æœªèƒ½æ·±åº¦æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹ï¼Œåœ¨æ”»å‡»åœºæ™¯ä¸‹æ€§èƒ½ä¸‹é™é«˜è¾¾285%ï¼Œè¶…è¶Šä¼ ç»Ÿå‡†ç¡®æ€§è¯„ä¼°æ‰€èƒ½è§¦åŠçš„èŒƒå›´ã€‚å› æ­¤ï¼Œæœ¬æ–‡æå‡ºMetaCompressæ¡†æ¶ï¼Œé€šè¿‡æ¯”è¾ƒæ•™å¸ˆå’Œå­¦ç”Ÿåœ¨ä¸€ç³»åˆ—è¡Œä¸ºä¿æŒçš„å…ƒå½¢æ€å…³ç³»ä¸‹çš„è¾“å‡ºï¼Œç³»ç»Ÿåœ°è¯„ä¼°è¡Œä¸ºå¿ å®åº¦ã€‚åœ¨ä¸¤é¡¹å¹¿æ³›ç ”ç©¶çš„ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¯¹ä½¿ç”¨ä¸‰ç§ä¸åŒçŸ¥è¯†è’¸é¦æŠ€æœ¯å‹ç¼©çš„ä»£ç è¯­è¨€æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœè¡¨æ˜MetaCompresså¯å‘ç°å­¦ç”Ÿæ¨¡å‹ä¸­é«˜è¾¾62%çš„è¡Œä¸ºå·®å¼‚ã€‚è¿™è¡¨æ˜åœ¨çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä¸­éœ€è¦è¯„ä¼°è¡Œä¸ºå¿ å®åº¦ï¼Œè€ŒMetaCompressæ¡†æ¶åˆ™ä¸ºè¯„ä¼°é€šè¿‡çŸ¥è¯†è’¸é¦è·å¾—çš„å‹ç¼©è¯­è¨€æ¨¡å‹æä¾›äº†å®ç”¨æµ‹è¯•æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åŸºäºTransformerçš„ä»£ç è¯­è¨€æ¨¡å‹åœ¨è½¯ä»¶åˆ†æä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä½†é¢ä¸´é«˜è®¡ç®—æˆæœ¬ã€æ…¢æ¨ç†é€Ÿåº¦å’Œç¯å¢ƒå½±å“ç­‰æŒ‘æˆ˜ã€‚</li>
<li>çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§æœ‰æ•ˆçš„å‹ç¼©å¤§å‹è¯­è¨€æ¨¡å‹å¹¶ç»´æŒæ€§èƒ½çš„æ–¹æ³•ã€‚</li>
<li>ç°æœ‰å‡†ç¡®æ€§è¯„ä¼°æ— æ³•å…¨é¢åæ˜ å­¦ç”Ÿæ¨¡å‹æ˜¯å¦æ·±åº¦æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹ã€‚</li>
<li>å­¦ç”Ÿæ¨¡å‹åœ¨æ”»å‡»åœºæ™¯ä¸‹æ€§èƒ½ä¸‹é™æ˜¾è‘—ï¼Œè¡¨æ˜éœ€è¦æ›´æ·±å…¥çš„æ¨¡å‹è¡Œä¸ºè¯„ä¼°ã€‚</li>
<li>MetaCompressæ¡†æ¶é€šè¿‡æ¯”è¾ƒæ•™å¸ˆå’Œå­¦ç”Ÿåœ¨è¡Œä¸ºä¿æŒçš„å…ƒå½¢æ€å…³ç³»ä¸‹çš„è¾“å‡ºï¼Œç³»ç»Ÿåœ°è¯„ä¼°è¡Œä¸ºå¿ å®åº¦ã€‚</li>
<li>MetaCompresså¯å‘ç°å­¦ç”Ÿæ¨¡å‹ä¸­é«˜è¾¾62%çš„è¡Œä¸ºå·®å¼‚ï¼Œå¼ºè°ƒåœ¨çŸ¥è¯†è’¸é¦è¿‡ç¨‹ä¸­è¯„ä¼°è¡Œä¸ºå¿ å®åº¦çš„å¿…è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05476">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b2f5250ad483a9c988b9aa7af487b2fb" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="SWE-Compass-Towards-Unified-Evaluation-of-Agentic-Coding-Abilities-for-Large-Language-Models"><a href="#SWE-Compass-Towards-Unified-Evaluation-of-Agentic-Coding-Abilities-for-Large-Language-Models" class="headerlink" title="SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for   Large Language Models"></a>SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for   Large Language Models</h2><p><strong>Authors:Jingxuan Xu, Ken Deng, Weihao Li, Songwei Yu, Huaixi Tang, Haoyang Huang, Zhiyi Lai, Zizheng Zhan, Yanan Wu, Chenchen Zhang, Kepeng Lei, Yifan Yao, Xinping Lei, Wenqiang Zhu, Zongxian Feng, Han Li, Junqi Xiong, Dailin Li, Zuchen Gao, Kun Wu, Wen Xiang, Ziqi Zhan, Yuanxing Zhang, Wuxuan Gong, Ziyuan Gao, Guanxiang Wang, Yirong Xue, Xiaojiang Zhang, Jinghui Wang, Huiming Wang, Wenhao Zhuang, Zhaoxiang Zhang, Yuqun Zhang, Haotian Zhang, Bin Chen, Jiaheng Liu</strong></p>
<p>Evaluating large language models (LLMs) for software engineering has been limited by narrow task coverage, language bias, and insufficient alignment with real-world developer workflows. Existing benchmarks often focus on algorithmic problems or Python-centric bug fixing, leaving critical dimensions of software engineering underexplored. To address these gaps, we introduce SWE-Compass1, a comprehensive benchmark that unifies heterogeneous code-related evaluations into a structured and production-aligned framework. SWE-Compass spans 8 task types, 8 programming scenarios, and 10 programming languages, with 2000 high-quality instances curated from authentic GitHub pull requests and refined through systematic filtering and validation. We benchmark ten state-of-the-art LLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear hierarchy of difficulty across task types, languages, and scenarios. Moreover, by aligning evaluation with real-world developer practices, SWE-Compass provides a rigorous and reproducible foundation for diagnosing and advancing agentic coding capabilities in large language models. </p>
<blockquote>
<p>è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è½¯ä»¶å·¥ç¨‹æ–¹é¢çš„åº”ç”¨ä¸€ç›´å—åˆ°ä»»åŠ¡è¦†ç›–é¢ç‹­çª„ã€è¯­è¨€åè§ä»¥åŠä¸ç°å®ä¸–ç•Œå¼€å‘è€…å·¥ä½œæµç¨‹å¯¹é½ä¸è¶³çš„é™åˆ¶ã€‚ç°æœ‰åŸºå‡†æµ‹è¯•é€šå¸¸ä¾§é‡äºç®—æ³•é—®é¢˜æˆ–Pythonä¸­å¿ƒçš„é”™è¯¯ä¿®å¤ï¼Œä½¿å¾—è½¯ä»¶å·¥ç¨‹çš„å…³é”®ç»´åº¦è¢«å¿½è§†ã€‚ä¸ºäº†å¼¥è¡¥è¿™äº›å·®è·ï¼Œæˆ‘ä»¬å¼•å…¥äº†SWE-Compass1ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„åŸºå‡†æµ‹è¯•ï¼Œå®ƒå°†å„ç§ä»£ç ç›¸å…³çš„è¯„ä¼°æ•´åˆåˆ°ä¸€ä¸ªç»“æ„åŒ–ä¸”ç¬¦åˆç”Ÿäº§å®é™…è¦æ±‚çš„æ¡†æ¶ä¸­ã€‚SWE-Compassæ¶µç›–äº†8ç§ä»»åŠ¡ç±»å‹ã€8ç§ç¼–ç¨‹åœºæ™¯å’Œ10ç§ç¼–ç¨‹è¯­è¨€ï¼Œå…¶ä¸­åŒ…æ‹¬ä»çœŸå®çš„GitHubæ‹‰å–è¯·æ±‚ä¸­ç²¾å¿ƒæŒ‘é€‰çš„2000ä¸ªé«˜è´¨é‡å®ä¾‹ï¼Œå¹¶é€šè¿‡ç³»ç»Ÿçš„è¿‡æ»¤å’ŒéªŒè¯è¿›è¡Œäº†å®Œå–„ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªä»£ç†æ¡†æ¶SWE-Agentå’ŒClaude Codeä¸‹å¯¹åä¸ªæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†ä»»åŠ¡ç±»å‹ã€è¯­è¨€å’Œåœºæ™¯ä¹‹é—´çš„éš¾åº¦å±‚æ¬¡ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¸ç°å®ä¸–ç•Œçš„å¼€å‘è€…å®è·µå¯¹é½è¯„ä¼°ï¼ŒSWE-Compassä¸ºè¯Šæ–­å’Œæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä»£ç†ç¼–ç èƒ½åŠ›æä¾›äº†ä¸¥æ ¼ä¸”å¯å¤åˆ¶çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05459v1">PDF</a> </p>
<p><strong>Summary</strong>ï¼š<br>é’ˆå¯¹è½¯ä»¶å·¥ç¨‹ä¸­å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯„ä¼°å­˜åœ¨ä»»åŠ¡è¦†ç›–é¢ç‹­çª„ã€è¯­è¨€åè§ä»¥åŠä¸çœŸå®ä¸–ç•Œå¼€å‘è€…å·¥ä½œæµç¨‹å¯¹é½ä¸è¶³çš„é—®é¢˜ï¼Œç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦é›†ä¸­åœ¨ç®—æ³•é—®é¢˜æˆ–Pythonä¸ºä¸­å¿ƒçš„bugä¿®å¤ä¸Šï¼Œå¿½è§†äº†è½¯ä»¶å·¥ç¨‹çš„å…³é”®æ–¹é¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¨å‡ºSWE-Compass1ï¼Œå®ƒæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œå°†å„ç§ä»£ç ç›¸å…³çš„è¯„ä¼°å†…å®¹æ•´åˆåˆ°ç»“æ„åŒ–ã€ä¸å®é™…åº”ç”¨ç›¸ç¬¦çš„æ¡†æ¶ä¸­ã€‚SWE-Compassæ¶µç›–8ç§ä»»åŠ¡ç±»å‹ã€8ç§ç¼–ç¨‹åœºæ™¯å’Œ10ç§ç¼–ç¨‹è¯­è¨€ï¼Œå…±ç²¾é€‰äº†2000ä¸ªé«˜è´¨é‡å®ä¾‹ï¼Œè¿™äº›å®ä¾‹æ¥æºäºçœŸå®çš„GitHub pullè¯·æ±‚ï¼Œç»è¿‡ç³»ç»Ÿç­›é€‰å’ŒéªŒè¯ã€‚æˆ‘ä»¬å¯¹åç§æœ€å…ˆè¿›çš„LLMè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œæ­ç¤ºäº†ä»»åŠ¡ç±»å‹ã€è¯­è¨€å’Œåœºæ™¯ä¹‹é—´çš„éš¾åº¦å±‚æ¬¡ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¸çœŸå®ä¸–ç•Œå¼€å‘è€…å®è·µå¯¹é½çš„è¯„ä¼°ï¼ŒSWE-Compassä¸ºè¯Šæ–­å’Œæ”¹è¿›å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ä»£ç†ç¼–ç èƒ½åŠ›æä¾›äº†ä¸¥æ ¼ä¸”å¯å¤åˆ¶çš„åŸºçŸ³ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>SWE-Compassæ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„LLMè¯„ä¼°åŸºå‡†ï¼Œæ¶µç›–äº†å¤šç§ä»»åŠ¡ç±»å‹ã€ç¼–ç¨‹åœºæ™¯å’Œç¼–ç¨‹è¯­è¨€ã€‚</li>
<li>å®ƒè¶…è¶Šäº†ç°æœ‰åŸºå‡†æµ‹è¯•çš„å±€é™ï¼Œè€ƒè™‘äº†çœŸå®ä¸–ç•Œå¼€å‘è€…çš„å·¥ä½œæµç¨‹å’Œç¼–ç¨‹ç¯å¢ƒçš„å¤šæ ·æ€§ã€‚</li>
<li>é€šè¿‡ç²¾é€‰é«˜è´¨é‡å®ä¾‹ï¼ŒSWE-Compassç¡®ä¿äº†è¯„ä¼°çš„æœ‰æ•ˆæ€§å’ŒçœŸå®æ€§ã€‚</li>
<li>åŸºå‡†æµ‹è¯•æ­ç¤ºäº†ä¸åŒä»»åŠ¡ç±»å‹ã€è¯­è¨€å’Œåœºæ™¯åœ¨LLMè¡¨ç°ä¸Šçš„å·®å¼‚å’Œéš¾åº¦å±‚æ¬¡ã€‚</li>
<li>SWE-Compassæä¾›äº†ä»£ç†ç¼–ç èƒ½åŠ›çš„è¯„ä¼°æ¡†æ¶ï¼Œæœ‰åŠ©äºè¯Šæ–­å’Œæ”¹LLMçš„è¿›æ­¥ã€‚</li>
<li>è¯¥åŸºå‡†æµ‹è¯•ä¸ºè¯„ä¼°LLMåœ¨è½¯ä»¶å·¥ç¨‹é¢†åŸŸçš„è¡¨ç°æä¾›äº†å¯å¤åˆ¶çš„åŸºçŸ³ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05459">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-34ed24ef82a8f1d3927da0d72645493d" align="middle">
<img src="https://picx.zhimg.com/v2-406e183b5e7f12e868811f130ded51fb" align="middle">
<img src="https://picx.zhimg.com/v2-4f3b98818578cfa27fcaf66a4c913f21" align="middle">
<img src="https://picx.zhimg.com/v2-97d2b95ea774d67c989c007641b967bd" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Large-Language-Models-for-Explainable-Threat-Intelligence"><a href="#Large-Language-Models-for-Explainable-Threat-Intelligence" class="headerlink" title="Large Language Models for Explainable Threat Intelligence"></a>Large Language Models for Explainable Threat Intelligence</h2><p><strong>Authors:Tiago Dinis, Miguel Correia, Roger Tavares</strong></p>
<p>As cyber threats continue to grow in complexity, traditional security mechanisms struggle to keep up. Large language models (LLMs) offer significant potential in cybersecurity due to their advanced capabilities in text processing and generation. This paper explores the use of LLMs with retrieval-augmented generation (RAG) to obtain threat intelligence by combining real-time information retrieval with domain-specific data. The proposed system, RAGRecon, uses a LLM with RAG to answer questions about cybersecurity threats. Moreover, it makes this form of Artificial Intelligence (AI) explainable by generating and visually presenting to the user a knowledge graph for every reply. This increases the transparency and interpretability of the reasoning of the model, allowing analysts to better understand the connections made by the system based on the context recovered by the RAG system. We evaluated RAGRecon experimentally with two datasets and seven different LLMs and the responses matched the reference responses more than 91% of the time for the best combinations. </p>
<blockquote>
<p>éšç€ç½‘ç»œå¨èƒçš„å¤æ‚æ€§ä¸æ–­å¢é•¿ï¼Œä¼ ç»Ÿå®‰å…¨æœºåˆ¶åœ¨åº”å¯¹ä¸Šæ„Ÿåˆ°å›°éš¾é‡é‡ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ–‡æœ¬å¤„ç†å’Œç”Ÿæˆæ–¹é¢æ‹¥æœ‰å…ˆè¿›çš„èƒ½åŠ›ï¼Œå› æ­¤åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸå…·æœ‰å·¨å¤§çš„æ½œåŠ›ã€‚æœ¬æ–‡æ¢è®¨äº†ä½¿ç”¨å¸¦æœ‰æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„LLMç»“åˆå®æ—¶ä¿¡æ¯æ£€ç´¢å’Œé¢†åŸŸç‰¹å®šæ•°æ®è·å–å¨èƒæƒ…æŠ¥çš„æ–¹æ³•ã€‚æ‰€æå‡ºçš„ç³»ç»ŸRAGReconä½¿ç”¨å¸¦æœ‰RAGçš„LLMå›ç­”æœ‰å…³ç½‘ç»œå®‰å…¨å¨èƒçš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œå®ƒé€šè¿‡ç”Ÿæˆå¹¶å¯è§†åŒ–å‘ˆç°ç»™ç”¨æˆ·æ¯ä¸ªç­”æ¡ˆçš„çŸ¥è¯†å›¾è°±ï¼Œä½¿è¿™ç§å½¢å¼çš„äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å˜å¾—å¯è§£é‡Šã€‚è¿™å¢åŠ äº†æ¨¡å‹çš„æ¨ç†çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ï¼Œä½¿åˆ†æå¸ˆèƒ½å¤ŸåŸºäºRAGç³»ç»Ÿæ¢å¤çš„ä¸Šä¸‹æ–‡æ›´å¥½åœ°ç†è§£ç³»ç»Ÿæ‰€å»ºç«‹çš„è¿æ¥ã€‚æˆ‘ä»¬å®éªŒæ€§åœ°ä½¿ç”¨ä¸¤ä¸ªæ•°æ®é›†å’Œä¸ƒä¸ªä¸åŒçš„LLMå¯¹RAGReconè¿›è¡Œäº†è¯„ä¼°ï¼Œæœ€ä½³ç»„åˆçš„å“åº”ä¸å‚è€ƒå“åº”çš„åŒ¹é…åº¦è¶…è¿‡9.%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05406v1">PDF</a> </p>
<p><strong>Summary</strong><br>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ã€‚é€šè¿‡å®æ—¶ä¿¡æ¯æ£€ç´¢å’Œç‰¹å®šé¢†åŸŸæ•°æ®ç»“åˆï¼ŒRAGReconç³»ç»Ÿèƒ½å¤Ÿå›ç­”å…³äºç½‘ç»œå®‰å…¨å¨èƒçš„é—®é¢˜ï¼Œå¹¶ç”ŸæˆçŸ¥è¯†å›¾è°±è§£é‡ŠAIçš„æ¨ç†è¿‡ç¨‹ï¼Œæé«˜ç³»ç»Ÿçš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒRAGReconçš„å“åº”ä¸å‚è€ƒå“åº”åŒ¹é…åº¦è¶…è¿‡91%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸå…·æœ‰æ˜¾è‘—æ½œåŠ›ã€‚</li>
<li>LLMsç»“åˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ç”¨äºè·å–å¨èƒæƒ…æŠ¥ã€‚</li>
<li>RAGReconç³»ç»Ÿåˆ©ç”¨LLMså’ŒRAGå›ç­”é—®é¢˜ï¼Œå…³äºç½‘ç»œå®‰å…¨å¨èƒçš„ä¿¡æ¯ã€‚</li>
<li>RAGReconé€šè¿‡ç”Ÿæˆå¹¶å±•ç¤ºçŸ¥è¯†å›¾è°±ï¼Œæé«˜AIçš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼ŒRAGReconçš„å“åº”åŒ¹é…åº¦é«˜ã€‚</li>
<li>LLMsåœ¨æ–‡æœ¬å¤„ç†å’Œç”Ÿæˆæ–¹é¢çš„å…ˆè¿›èƒ½åŠ›ï¼Œä½¿å…¶åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸå…·æœ‰å¹¿æ³›åº”ç”¨å‰æ™¯ã€‚</li>
<li>RAGReconç³»ç»Ÿèƒ½å¤Ÿå®æ—¶æ£€ç´¢ä¿¡æ¯ï¼Œå¹¶åŸºäºç‰¹å®šé¢†åŸŸæ•°æ®åšå‡ºåˆ¤æ–­ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05406">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8008c8fc75813617a4bfab9f531cbf23" align="middle">
<img src="https://picx.zhimg.com/v2-bb3fce32feeb66b837fc82bd2a315c12" align="middle">
<img src="https://picx.zhimg.com/v2-251c080356b13bfe6d47e17c20ca4866" align="middle">
<img src="https://picx.zhimg.com/v2-4c33626fa35535d041bac1deb7f44801" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="PreResQ-R1-Towards-Fine-Grained-Rank-and-Score-Reinforcement-Learning-for-Visual-Quality-Assessment-via-Preference-Response-Disentangled-Policy-Optimization"><a href="#PreResQ-R1-Towards-Fine-Grained-Rank-and-Score-Reinforcement-Learning-for-Visual-Quality-Assessment-via-Preference-Response-Disentangled-Policy-Optimization" class="headerlink" title="PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning   for Visual Quality Assessment via Preference-Response Disentangled Policy   Optimization"></a>PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning   for Visual Quality Assessment via Preference-Response Disentangled Policy   Optimization</h2><p><strong>Authors:Zehui Feng, Tian Qiu, Tong Wu, Junxuan Li, Huayuan Xu, Ting Han</strong></p>
<p>Visual Quality Assessment (QA) seeks to predict human perceptual judgments of visual fidelity. While recent multimodal large language models (MLLMs) show promise in reasoning about image and video quality, existing approaches mainly rely on supervised fine-tuning or rank-only objectives, resulting in shallow reasoning, poor score calibration, and limited cross-domain generalization. We propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning framework that unifies absolute score regression and relative ranking consistency within a single reasoning-driven optimization scheme. Unlike prior QA methods, PreResQ-R1 introduces a dual-branch reward formulation that separately models intra-sample response coherence and inter-sample preference alignment, optimized via Group Relative Policy Optimization (GRPO). This design encourages fine-grained, stable, and interpretable chain-of-thought reasoning about perceptual quality. To extend beyond static imagery, we further design a global-temporal and local-spatial data flow strategy for Video Quality Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and 28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5 VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30% and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it produces human-aligned reasoning traces that reveal the perceptual cues underlying quality judgments. Code and model are available. </p>
<blockquote>
<p>è§†è§‰è´¨é‡è¯„ä¼°ï¼ˆQAï¼‰æ—¨åœ¨é¢„æµ‹äººç±»å¯¹è§†è§‰ä¿çœŸåº¦çš„æ„ŸçŸ¥åˆ¤æ–­ã€‚è™½ç„¶æœ€è¿‘çš„å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨å›¾åƒå’Œè§†é¢‘è´¨é‡æ¨ç†æ–¹é¢æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–äºæœ‰ç›‘ç£çš„å¾®è°ƒæˆ–ä»…æ’åç›®æ ‡ï¼Œå¯¼è‡´æ¨ç†è‚¤æµ…ã€åˆ†æ•°æ ¡å‡†ä¸ä½³å’Œè·¨åŸŸæ³›åŒ–æœ‰é™ã€‚æˆ‘ä»¬æå‡ºäº†PreResQ-R1ï¼Œä¸€ä¸ªåå¥½å“åº”åˆ†è§£å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œå®ƒåœ¨ä¸€ä¸ªä»¥æ¨ç†é©±åŠ¨çš„ä¼˜åŒ–æ–¹æ¡ˆä¸­ç»Ÿä¸€äº†ç»å¯¹åˆ†æ•°å›å½’å’Œç›¸å¯¹æ’åä¸€è‡´æ€§ã€‚ä¸åŒäºå…ˆå‰çš„QAæ–¹æ³•ï¼ŒPreResQ-R1å¼•å…¥äº†åŒåˆ†æ”¯å¥–åŠ±å…¬å¼ï¼Œè¯¥å…¬å¼åˆ†åˆ«å»ºæ¨¡æ ·æœ¬å†…å“åº”ä¸€è‡´æ€§å’Œæ ·æœ¬é—´åå¥½å¯¹é½ï¼Œé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œä¼˜åŒ–ã€‚è¿™ç§è®¾è®¡é¼“åŠ±å¯¹æ„ŸçŸ¥è´¨é‡è¿›è¡Œç²¾ç»†ã€ç¨³å®šã€å¯è§£é‡Šçš„é“¾å¼æ€ç»´æ¨ç†ã€‚ä¸ºäº†è¶…è¶Šé™æ€å›¾åƒï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ç§å…¨å±€æ—¶é—´å’Œå±€éƒ¨ç©ºé—´çš„æ•°æ®æµç­–ç•¥ï¼Œç”¨äºè§†é¢‘è´¨é‡è¯„ä¼°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»…åœ¨6Kå›¾åƒå’Œ2ä¸‡8åƒä¸ªè§†é¢‘ä¸Šè¿›è¡Œå¼ºåŒ–å¾®è°ƒåï¼ŒPreResQ-R1åœ¨SRCCå’ŒPLCCæŒ‡æ ‡ä¸‹çš„åä¸ªå›¾åƒè´¨é‡è¯„ä¼°å’Œäº”ä¸ªè§†é¢‘è´¨é‡è¯„ä¼°åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æœ€æ–°æˆæœï¼Œåœ¨å›¾åƒè´¨é‡è¯„ä¼°ä»»åŠ¡ä¸­åˆ†åˆ«æé«˜äº†ç™¾åˆ†ä¹‹äº”ç‚¹ä¸‰é›¶å’Œç™¾åˆ†ä¹‹äºŒç‚¹ä¸€äº”çš„ä¼˜åŠ¿ã€‚é™¤äº†é‡åŒ–æ”¶ç›Šä¹‹å¤–ï¼Œå®ƒäº§ç”Ÿä¸äººç±»å¯¹é½çš„æ¨ç†è½¨è¿¹ï¼Œæ­ç¤ºäº†è´¨é‡åˆ¤æ–­èƒŒåçš„æ„ŸçŸ¥çº¿ç´¢ã€‚ä»£ç å’Œæ¨¡å‹å¯ä¾›ä½¿ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05393v1">PDF</a> 27 pages, 14 figures, under review as a conference paper</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºPreResQ-R1çš„è§†è§‰è´¨é‡è¯„ä¼°ï¼ˆQAï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†ç»å¯¹åˆ†æ•°å›å½’å’Œç›¸å¯¹æ’åä¸€è‡´æ€§ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œä¼˜åŒ–ã€‚PreResQ-R1é‡‡ç”¨åŒåˆ†æ”¯å¥–åŠ±å…¬å¼å»ºæ¨¡æ ·æœ¬å†…å“åº”ä¸€è‡´æ€§å’Œæ ·æœ¬é—´åå¥½å¯¹é½ï¼Œé€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œä¼˜åŒ–ã€‚è¯¥è®¾è®¡é¼“åŠ±å¯¹æ„ŸçŸ¥è´¨é‡è¿›è¡Œç²¾ç»†ã€ç¨³å®šä¸”å¯è§£é‡Šçš„é“¾å¼æ€ç»´æ¨ç†ã€‚åœ¨é™æ€å›¾åƒå’Œè§†é¢‘è´¨é‡è¯„ä¼°æ–¹é¢ï¼ŒPreResQ-R1å–å¾—äº†æœ€å…ˆè¿›çš„æˆæœã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>PreResQ-R1æ˜¯ä¸€ä¸ªåŸºäºå¼ºåŒ–å­¦ä¹ çš„è§†è§‰è´¨é‡è¯„ä¼°æ¡†æ¶ï¼Œç»“åˆäº†ç»å¯¹åˆ†æ•°å›å½’å’Œç›¸å¯¹æ’åä¸€è‡´æ€§ã€‚</li>
<li>è¯¥æ¡†æ¶é‡‡ç”¨åŒåˆ†æ”¯å¥–åŠ±å…¬å¼ï¼Œåˆ†åˆ«å»ºæ¨¡æ ·æœ¬å†…å“åº”ä¸€è‡´æ€§å’Œæ ·æœ¬é—´åå¥½å¯¹é½ã€‚</li>
<li>PreResQ-R1é€šè¿‡ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›è¡Œä¼˜åŒ–ï¼Œé¼“åŠ±å¯¹æ„ŸçŸ¥è´¨é‡è¿›è¡Œç²¾ç»†ã€ç¨³å®šä¸”å¯è§£é‡Šçš„é“¾å¼æ€ç»´æ¨ç†ã€‚</li>
<li>PreResQ-R1åœ¨é™æ€å›¾åƒå’Œè§†é¢‘è´¨é‡è¯„ä¼°æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚</li>
<li>è¯¥æ¡†æ¶å®ç°äº†å¯¹æ„ŸçŸ¥è´¨é‡åˆ¤æ–­çš„æ·±å…¥ç†è§£ï¼Œå¹¶é€šè¿‡äººç±»å¯¹é½çš„æ¨ç†è½¨è¿¹æ­ç¤ºäº†åˆ¤æ–­èƒŒåçš„æ„ŸçŸ¥çº¿ç´¢ã€‚</li>
<li>PreResQ-R1åªéœ€è¦å°‘é‡çš„å›¾åƒå’Œè§†é¢‘æ•°æ®è¿›è¡Œå¼ºåŒ–å¾®è°ƒï¼Œå³å¯å®ç°æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05393">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f3c124b6a18d422657e3b069a9d99766" align="middle">
<img src="https://picx.zhimg.com/v2-9a9d8d9df7eab59a2f52b0c02ea49a50" align="middle">
<img src="https://picx.zhimg.com/v2-6f12958db9572a16c3fedd72b7d08458" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="TeaRAG-A-Token-Efficient-Agentic-Retrieval-Augmented-Generation-Framework"><a href="#TeaRAG-A-Token-Efficient-Agentic-Retrieval-Augmented-Generation-Framework" class="headerlink" title="TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation   Framework"></a>TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation   Framework</h2><p><strong>Authors:Chao Zhang, Yuhao Wang, Derong Xu, Haoxin Zhang, Yuanjie Lyu, Yuhao Chen, Shuochen Liu, Tong Xu, Xiangyu Zhao, Yan Gao, Yao Hu, Enhong Chen</strong></p>
<p>Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment Large Language Modelsâ€™ (LLMs) reliability. For flexibility, agentic RAG employs autonomous, multi-round retrieval and reasoning to resolve queries. Although recent agentic RAG has improved via reinforcement learning, they often incur substantial token overhead from search and reasoning processes. This trade-off prioritizes accuracy over efficiency. To address this issue, this work proposes TeaRAG, a token-efficient agentic RAG framework capable of compressing both retrieval content and reasoning steps. 1) First, the retrieved content is compressed by augmenting chunk-based semantic retrieval with a graph retrieval using concise triplets. A knowledge association graph is then built from semantic similarity and co-occurrence. Finally, Personalized PageRank is leveraged to highlight key knowledge within this graph, reducing the number of tokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative Process-aware Direct Preference Optimization (IP-DPO) is proposed. Specifically, our reward function evaluates the knowledge sufficiency by a knowledge matching mechanism, while penalizing excessive reasoning steps. This design can produce high-quality preference-pair datasets, supporting iterative DPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the average Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on Llama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/TeaRAG">https://github.com/Applied-Machine-Learning-Lab/TeaRAG</a>. </p>
<blockquote>
<p>æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†å¢å¼ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯é æ€§ã€‚ä¸ºäº†çµæ´»æ€§ï¼Œä¸»åŠ¨RAGé‡‡ç”¨è‡ªä¸»ã€å¤šè½®æ£€ç´¢å’Œæ¨ç†æ¥è§£å†³æŸ¥è¯¢ã€‚å°½ç®¡æœ€è¿‘çš„ä¸»åŠ¨RAGé€šè¿‡å¼ºåŒ–å­¦ä¹ å¾—åˆ°äº†æ”¹è¿›ï¼Œä½†å®ƒä»¬å¸¸å¸¸å› ä¸ºæœç´¢å’Œæ¨ç†è¿‡ç¨‹è€Œäº§ç”Ÿå¤§é‡çš„ä»¤ç‰Œå¼€é”€ï¼Œè¿™ç§æƒè¡¡ä¼˜å…ˆè€ƒè™‘å‡†ç¡®æ€§è€Œéæ•ˆç‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†TeaRAGï¼Œä¸€ä¸ªé«˜æ•ˆçš„ä¸»åŠ¨RAGæ¡†æ¶ï¼Œèƒ½å¤Ÿå‹ç¼©æ£€ç´¢å†…å®¹å’Œæ¨ç†æ­¥éª¤ã€‚é¦–å…ˆï¼Œé€šè¿‡ç»“åˆåŸºäºå—çš„è¯­ä¹‰æ£€ç´¢å’ŒåŸºäºç®€æ´ä¸‰å…ƒç»„çš„å›¾æ£€ç´¢æ¥å‹ç¼©æ£€ç´¢å†…å®¹ã€‚ç„¶åï¼Œæ ¹æ®è¯­ä¹‰ç›¸ä¼¼æ€§å’Œå…±ç°æ€§æ„å»ºçŸ¥è¯†å…³è”å›¾ã€‚æœ€åï¼Œåˆ©ç”¨ä¸ªæ€§åŒ–PageRankæ¥çªå‡ºæ˜¾ç¤ºè¯¥å›¾ä¸­çš„å…³é”®çŸ¥è¯†ï¼Œå‡å°‘æ¯æ¬¡æ£€ç´¢çš„ä»¤ç‰Œæ•°é‡ã€‚æ­¤å¤–ï¼Œä¸ºäº†ç®€åŒ–æ¨ç†æ­¥éª¤ï¼Œæå‡ºäº†è¿­ä»£è¿‡ç¨‹æ„ŸçŸ¥çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆIP-DPOï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„å¥–åŠ±å‡½æ•°é€šè¿‡çŸ¥è¯†åŒ¹é…æœºåˆ¶æ¥è¯„ä¼°çŸ¥è¯†çš„å……è¶³æ€§ï¼ŒåŒæ—¶æƒ©ç½šè¿‡å¤šçš„æ¨ç†æ­¥éª¤ã€‚è¿™ç§è®¾è®¡å¯ä»¥äº§ç”Ÿé«˜è´¨é‡çš„é¦–é€‰é…å¯¹æ•°æ®é›†ï¼Œæ”¯æŒè¿­ä»£DPOä»¥æé«˜æ¨ç†çš„ç®€æ´æ€§ã€‚åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šï¼ŒTeaRAGåœ¨Llama3-8B-Instructå’ŒQwen2.5-14B-Instructä¸Šåˆ†åˆ«å°†å¹³å‡ç²¾ç¡®åŒ¹é…åº¦æé«˜äº†4%å’Œ2%ï¼ŒåŒæ—¶å‡å°‘äº†è¾“å‡ºä»¤ç‰Œçš„61%å’Œ59%ã€‚ä»£ç å¯è®¿é—®äº <a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/TeaRAG%E3%80%82">https://github.com/Applied-Machine-Learning-Lab/TeaRAGã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05385v1">PDF</a> 32 pages</p>
<p><strong>Summary</strong></p>
<p>RAGåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†å¢å¼ºLLMçš„å¯é æ€§ï¼Œè€Œagentic RAGåˆ™é€šè¿‡è‡ªä¸»ã€å¤šè½®æ¬¡çš„æ£€ç´¢å’Œæ¨ç†æ¥è§£å†³æŸ¥è¯¢é—®é¢˜ã€‚ç°æœ‰agentic RAGè™½ç»å¼ºåŒ–å­¦ä¹ æ”¹è¿›ï¼Œä½†æœç´¢å’Œæ¨ç†è¿‡ç¨‹ä¼šäº§ç”Ÿå¤§é‡ä»¤ç‰Œå¼€é”€ï¼Œä¼˜å…ˆè€ƒè™‘å‡†ç¡®æ€§è€Œç‰ºç‰²æ•ˆç‡ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæå‡ºToken Efficient agentic RAGï¼ˆTeaRAGï¼‰æ¡†æ¶ï¼Œé€šè¿‡åŸºäºå—çš„è¯­ä¹‰æ£€ç´¢ä¸çŸ¥è¯†å…³è”å›¾ç»“åˆæ¥å‹ç¼©æ£€ç´¢å†…å®¹ï¼Œå¹¶åˆ©ç”¨ä¸ªæ€§åŒ–PageRankçªå‡ºå…³é”®çŸ¥è¯†ï¼Œå‡å°‘æ¯æ¬¡æ£€ç´¢çš„ä»¤ç‰Œæ•°é‡ã€‚åŒæ—¶ï¼Œæå‡ºè¿­ä»£è¿‡ç¨‹æ„ŸçŸ¥çš„ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆIP-DPOï¼‰ï¼Œé€šè¿‡çŸ¥è¯†åŒ¹é…æœºåˆ¶è¯„ä¼°çŸ¥è¯†å……è¶³æ€§å¹¶æƒ©ç½šè¿‡å¤šçš„æ¨ç†æ­¥éª¤ï¼Œä»è€Œæé«˜æ¨ç†çš„ç®€æ´æ€§ã€‚åœ¨å…­ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒTeaRAGåœ¨æé«˜ç²¾ç¡®åŒ¹é…ç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†è¾“å‡ºä»¤ç‰Œæ•°é‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Retrieval-Augmented Generation (RAG)ç»“åˆå¤–éƒ¨çŸ¥è¯†æå‡LLMå¯é æ€§ã€‚</li>
<li>Agentic RAGä½¿ç”¨è‡ªä¸»å¤šè½®æ£€ç´¢å’Œæ¨ç†æé«˜çµæ´»æ€§ã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ è™½æå‡äº†agentic RAGæ€§èƒ½ï¼Œä½†å¼•å‘æ˜¾è‘—çš„ä»¤ç‰Œå¼€é”€é—®é¢˜ã€‚</li>
<li>TeaRAGæ¡†æ¶è¢«æå‡ºä»¥è§£å†‘ä¸Šè¿°é—®é¢˜ï¼Œé€šè¿‡å‹ç¼©æ£€ç´¢å†…å®¹å’Œæ¨ç†æ­¥éª¤å®ç°Tokenæ•ˆç‡ã€‚</li>
<li>TeaRAGç»“åˆå—è¯­ä¹‰æ£€ç´¢ã€çŸ¥è¯†å…³è”å›¾å’Œä¸ªæ€§åŒ–PageRankå‡å°‘æ£€ç´¢ä»¤ç‰Œæ•°é‡ã€‚</li>
<li>æå‡ºIP-DPOæ–¹æ³•ä»¥æé«˜æ¨ç†ç®€æ´æ€§ï¼Œé€šè¿‡çŸ¥è¯†åŒ¹é…æœºåˆ¶è¯„ä¼°çŸ¥è¯†å……è¶³æ€§å¹¶ä¼˜åŒ–è¿‡åº¦æ¨ç†ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05385">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4f459ba31f29269d66257d076413a560" align="middle">
<img src="https://picx.zhimg.com/v2-1b29364c956cb4984d6bdcc5267de8fd" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="Dense-Motion-Captioning"><a href="#Dense-Motion-Captioning" class="headerlink" title="Dense Motion Captioning"></a>Dense Motion Captioning</h2><p><strong>Authors:Shiyao Xu, Benedetta Liberatori, GÃ¼l Varol, Paolo Rota</strong></p>
<p>Recent advances in 3D human motion and language integration have primarily focused on text-to-motion generation, leaving the task of motion understanding relatively unexplored. We introduce Dense Motion Captioning, a novel task that aims to temporally localize and caption actions within 3D human motion sequences. Current datasets fall short in providing detailed temporal annotations and predominantly consist of short sequences featuring few actions. To overcome these limitations, we present the Complex Motion Dataset (CompMo), the first large-scale dataset featuring richly annotated, complex motion sequences with precise temporal boundaries. Built through a carefully designed data generation pipeline, CompMo includes 60,000 motion sequences, each composed of multiple actions ranging from at least two to ten, accurately annotated with their temporal extents. We further present DEMO, a model that integrates a large language model with a simple motion adapter, trained to generate dense, temporally grounded captions. Our experiments show that DEMO substantially outperforms existing methods on CompMo as well as on adapted benchmarks, establishing a robust baseline for future research in 3D motion understanding and captioning. </p>
<blockquote>
<p>å…³äºäººä½“ä¸‰ç»´åŠ¨ä½œå’Œè¯­è¨€èåˆç ”ç©¶çš„æœ€æ–°è¿›å±•ä¸»è¦é›†ä¸­åœ¨åŠ¨ä½œç”Ÿæˆæ–‡æœ¬ä¸Šï¼Œç›¸å¯¹è€Œè¨€åŠ¨ä½œç†è§£çš„ä»»åŠ¡ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ã€‚æˆ‘ä»¬å¼•å…¥å¯†é›†åŠ¨ä½œæè¿°ï¼ˆDense Motion Captioningï¼‰è¿™ä¸€æ–°å‹ä»»åŠ¡ï¼Œæ—¨åœ¨è¯†åˆ«ä¸‰ç»´äººä½“è¿åŠ¨åºåˆ—ä¸­çš„æ—¶é—´ä½ç½®å¹¶å¯¹åŠ¨ä½œè¿›è¡Œæè¿°ã€‚ç°æœ‰çš„æ•°æ®é›†åœ¨æ—¶é—´å±‚é¢æ ‡æ³¨ä¿¡æ¯ä¸å…¨ï¼Œä¸»è¦ç”±åŒ…å«å°‘æ•°åŠ¨ä½œçš„ç®€çŸ­åºåˆ—ç»„æˆã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†å¤æ‚åŠ¨ä½œæ•°æ®é›†ï¼ˆCompMoï¼‰ï¼Œè¿™æ˜¯é¦–ä¸ªåŒ…å«ä¸°å¯Œæ ‡æ³¨ä¿¡æ¯çš„å¤æ‚åŠ¨ä½œæ•°æ®é›†ï¼Œå…·æœ‰ç²¾ç¡®çš„æ—¶é—´è¾¹ç•Œã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ•°æ®ç”Ÿæˆç®¡é“æ„å»ºï¼ŒCompMoåŒ…å«6ä¸‡ä¸ªåŠ¨ä½œåºåˆ—ï¼Œæ¯ä¸ªåºåˆ—åŒ…å«è‡³å°‘ä¸¤åˆ°åä¸ªåŠ¨ä½œï¼Œç²¾ç¡®æ ‡æ³¨äº†æ—¶é—´è·¨åº¦ã€‚æˆ‘ä»¬è¿˜æ¨å‡ºäº†DEMOæ¨¡å‹ï¼Œå®ƒç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œç®€å•åŠ¨ä½œé€‚é…å™¨ï¼Œç»è¿‡è®­ç»ƒèƒ½å¤Ÿç”Ÿæˆå¯†é›†çš„æ—¶é—´æ€§æè¿°ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒDEMOåœ¨CompMoä»¥åŠæ”¹ç¼–çš„åŸºå‡†æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å‡ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºæœªæ¥çš„ä¸‰ç»´åŠ¨ä½œç†è§£å’Œæè¿°ç ”ç©¶å»ºç«‹äº†ç¨³å¥çš„åŸºå‡†ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05369v1">PDF</a> 12 pages, 5 figures, accepted to 3DV 2026</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸä¸‰ç»´äººä½“è¿åŠ¨ä¸è¯­è¨€æ•´åˆæ–¹é¢çš„è¿›å±•ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬åˆ°è¿åŠ¨çš„ç”Ÿæˆï¼Œè€Œè¿åŠ¨ç†è§£çš„ä»»åŠ¡ç›¸å¯¹è¢«å¿½è§†ã€‚æœ¬æ–‡å¼•å…¥å¯†é›†è¿åŠ¨å­—å¹•ï¼ˆDense Motion Captioningï¼‰è¿™ä¸€æ–°ä»»åŠ¡ï¼Œæ—¨åœ¨ä¸´æ—¶å®šä½å’Œå­—å¹•ä¸‰ç»´äººä½“è¿åŠ¨åºåˆ—ä¸­çš„åŠ¨ä½œã€‚å½“å‰æ•°æ®é›†åœ¨æä¾›è¯¦ç»†æ—¶é—´æ³¨é‡Šæ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œä¸»è¦ç”±åŒ…å«å°‘æ•°åŠ¨ä½œç‰¹å¾çš„çŸ­åºåˆ—ç»„æˆã€‚ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼Œæœ¬æ–‡æå‡ºäº†å¤æ‚è¿åŠ¨æ•°æ®é›†ï¼ˆCompMoï¼‰ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåŒ…å«ä¸°å¯Œæ³¨é‡Šçš„å¤§è§„æ¨¡æ•°æ®é›†ï¼Œå…·æœ‰ç²¾ç¡®æ—¶é—´è¾¹ç•Œçš„å¤æ‚è¿åŠ¨åºåˆ—ã€‚é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ•°æ®ç”Ÿæˆç®¡é“æ„å»ºï¼ŒCompMoåŒ…å«6ä¸‡ä¸ªè¿åŠ¨åºåˆ—ï¼Œæ¯ä¸ªåºåˆ—ç”±è‡³å°‘ä¸¤åˆ°åä¸ªåŠ¨ä½œç»„æˆï¼Œå‡†ç¡®æ ‡æ³¨äº†å®ƒä»¬çš„æ—¶é—´èŒƒå›´ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†DEMOæ¨¡å‹ï¼Œå®ƒå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸ç®€å•çš„è¿åŠ¨é€‚é…å™¨ç›¸ç»“åˆï¼Œè®­ç»ƒç”Ÿæˆå¯†é›†ã€æ—¶é—´ä¸Šçš„å­—å¹•ã€‚å®éªŒè¡¨æ˜ï¼ŒDEMOåœ¨CompMoä»¥åŠé€‚åº”çš„åŸºå‡†æµ‹è¯•ä¸Šå¤§å¹…ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºæœªæ¥åœ¨ä¸‰ç»´è¿åŠ¨ç†è§£å’Œå­—å¹•æ–¹é¢çš„ç ”ç©¶å»ºç«‹äº†ç¨³å¥çš„åŸºçº¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å½“å‰è¿›å±•ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬åˆ°è¿åŠ¨çš„ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œè€Œä¸‰ç»´äººä½“è¿åŠ¨ç†è§£çš„ä»»åŠ¡ç›¸å¯¹è¢«å¿½è§†ã€‚</li>
<li>å¼•å…¥Dense Motion Captioningæ–°ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯ç†è§£ä¸‰ç»´äººä½“è¿åŠ¨åºåˆ—ä¸­çš„åŠ¨ä½œã€‚</li>
<li>å½“å‰æ•°æ®é›†åœ¨æä¾›è¯¦ç»†æ—¶é—´æ³¨é‡Šæ–¹é¢å­˜åœ¨ä¸è¶³ã€‚</li>
<li>æå‡ºå¤æ‚è¿åŠ¨æ•°æ®é›†ï¼ˆCompMoï¼‰ï¼ŒåŒ…å«ä¸°å¯Œæ³¨é‡Šçš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚</li>
<li>é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ•°æ®ç”Ÿæˆç®¡é“æ„å»ºCompMoæ•°æ®é›†ï¼ŒåŒ…å«å‡†ç¡®æ ‡æ³¨çš„å¤æ‚è¿åŠ¨åºåˆ—ã€‚</li>
<li>æå‡ºDEMOæ¨¡å‹ï¼Œæ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œç®€å•è¿åŠ¨é€‚é…å™¨è¿›è¡Œå¯†é›†è¿åŠ¨å­—å¹•ç”Ÿæˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05369">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-78e8fbc682059b9c81259472f27b2040" align="middle">
<img src="https://picx.zhimg.com/v2-5eb76e90c9b9e5829ac4153b38da6c6e" align="middle">
<img src="https://picx.zhimg.com/v2-d049688e716af6c02dbc23ebfa99ea81" align="middle">
<img src="https://picx.zhimg.com/v2-e2d14bbf04edfd8a06c1a82bdfe70d5f" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="What-Are-the-Facts-Automated-Extraction-of-Court-Established-Facts-from-Criminal-Court-Opinions"><a href="#What-Are-the-Facts-Automated-Extraction-of-Court-Established-Facts-from-Criminal-Court-Opinions" class="headerlink" title="What Are the Facts? Automated Extraction of Court-Established Facts from   Criminal-Court Opinions"></a>What Are the Facts? Automated Extraction of Court-Established Facts from   Criminal-Court Opinions</h2><p><strong>Authors:KlÃ¡ra BendovÃ¡, TomÃ¡Å¡ Knap, Jan ÄŒernÃ½, VojtÄ›ch Pour, Jaromir Savelka, Ivana KvapilÃ­kovÃ¡, Jakub DrÃ¡pal</strong></p>
<p>Criminal justice administrative data contain only a limited amount of information about the committed offense. However, there is an unused source of extensive information in continental European courtsâ€™ decisions: descriptions of criminal behaviors in verdicts by which offenders are found guilty. In this paper, we study the feasibility of extracting these descriptions from publicly available court decisions from Slovakia. We use two different approaches for retrieval: regular expressions and large language models (LLMs). Our baseline was a simple method employing regular expressions to identify typical words occurring before and after the description. The advanced regular expression approach further focused on â€œsparingâ€ and its normalization (insertion of spaces between individual letters), typical for delineating the description. The LLM approach involved prompting the Gemini Flash 2.0 model to extract the descriptions using predefined instructions. Although the baseline identified descriptions in only 40.5% of verdicts, both methods significantly outperformed it, achieving 97% with advanced regular expressions and 98.75% with LLMs, and 99.5% when combined. Evaluation by law students showed that both advanced methods matched human annotations in about 90% of cases, compared to just 34.5% for the baseline. LLMs fully matched human-labeled descriptions in 91.75% of instances, and a combination of advanced regular expressions with LLMs reached 92%. </p>
<blockquote>
<p>åˆ‘äº‹å¸æ³•è¡Œæ”¿æ•°æ®ä¸­å¯¹çŠ¯ç½ªè¡Œä¸ºçš„æè¿°ä¿¡æ¯éå¸¸æœ‰é™ã€‚ç„¶è€Œï¼Œåœ¨å¤§é™†æ¬§æ´²çš„æ³•é™¢åˆ¤å†³ä¸­å­˜åœ¨å¤§é‡æœªä½¿ç”¨çš„ä¿¡æ¯æ¥æºï¼šå³åˆ¤å†³ä¹¦ä¸­å¯¹çŠ¯ç½ªè¡Œä¸ºçš„æè¿°ï¼Œé€šè¿‡è¿™äº›æè¿°ï¼Œç½ªçŠ¯è¢«åˆ¤æœ‰ç½ªã€‚æœ¬æ–‡ç ”ç©¶äº†ä»æ–¯æ´›ä¼å…‹å…¬å¼€å¯ç”¨çš„æ³•é™¢è£å†³ä¸­æå–è¿™äº›æè¿°çš„å¯è¡Œæ€§ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸¤ç§æ£€ç´¢æ–¹æ³•ï¼šæ­£åˆ™è¡¨è¾¾å¼å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æˆ‘ä»¬çš„åŸºçº¿æ–¹æ³•æ˜¯ä¸€ç§ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¯†åˆ«æè¿°å‰åå‡ºç°çš„å…¸å‹è¯è¯­çš„ç®€å•æ–¹æ³•ã€‚é«˜çº§æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•è¿›ä¸€æ­¥å…³æ³¨â€œé—´éš”â€åŠå…¶å½’ä¸€åŒ–ï¼ˆåœ¨å•ä¸ªå­—æ¯ä¹‹é—´æ’å…¥ç©ºæ ¼ï¼‰ï¼Œè¿™å…¸å‹åœ°ç”¨äºæè¿°è¾¹ç•Œã€‚LLMæ–¹æ³•æ˜¯é€šè¿‡é¢„å…ˆè®¾å®šçš„æŒ‡ä»¤æç¤ºGemini Flash 2.0æ¨¡å‹æ¥æå–æè¿°ã€‚è™½ç„¶åŸºçº¿æ–¹æ³•åªåœ¨40.5%çš„åˆ¤å†³ä¸­è¯†åˆ«åˆ°äº†æè¿°ï¼Œä½†ä¸¤ç§æ–¹æ³•éƒ½æ˜¾è‘—ä¼˜äºå®ƒï¼Œé«˜çº§æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•è¾¾åˆ°97%ï¼ŒLLMæ–¹æ³•è¾¾åˆ°98.75%ï¼Œç»„åˆä½¿ç”¨æ—¶è¾¾åˆ°99.5%ã€‚æ³•å¾‹å­¦ç”Ÿçš„è¯„ä¼°æ˜¾ç¤ºï¼Œåœ¨å¤§çº¦90%çš„æƒ…å†µä¸‹ï¼Œä¸¤ç§é«˜çº§æ–¹æ³•ä¸äººç±»æ³¨é‡Šç›¸åŒ¹é…ï¼Œè€ŒåŸºçº¿æ–¹æ³•ä»…ä¸º34.5%ã€‚LLMsåœ¨91.75%çš„æƒ…å†µä¸‹å®Œå…¨åŒ¹é…äººç±»æ ‡æ³¨çš„æè¿°ï¼Œè€Œé«˜çº§æ­£åˆ™è¡¨è¾¾å¼ä¸LLMsçš„ç»„åˆåˆ™è¾¾åˆ°äº†92%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05320v1">PDF</a> Paper accepted to the proceedings of ASAIL 2025 Workshop under ICAIL   conference for publication. Paper contains 6 pages (references included) and   2 appendices. It contains 8 tables, no figures</p>
<p><strong>Summary</strong>ï¼š</p>
<p>æœ¬æ–‡æ¢è®¨äº†ä»æ–¯æ´›ä¼å…‹å…¬å¼€æ³•é™¢è£å†³ä¸­æå–çŠ¯ç½ªè¡Œä¸ºæè¿°ä¿¡æ¯çš„å¯è¡Œæ€§ã€‚ç ”ç©¶é‡‡ç”¨æ­£åˆ™è¡¨è¾¾å¼å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸¤ç§æ£€ç´¢æ–¹æ³•ã€‚è™½ç„¶åŸºçº¿æ–¹æ³•ä»…èƒ½åœ¨40.5%çš„è£å†³ä¸­æ‰¾åˆ°æè¿°ï¼Œä½†é«˜çº§æ­£åˆ™è¡¨è¾¾å¼å’ŒLLMæ–¹æ³•æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œåˆ†åˆ«è¾¾åˆ°äº†97%ã€98.75%å’Œ99.5%çš„å‡†ç¡®ç‡ã€‚æ³•å¾‹å­¦ç”Ÿçš„è¯„ä¼°æ˜¾ç¤ºï¼Œé«˜çº§æ–¹æ³•ä¸äººç±»æ ‡æ³¨çš„åŒ¹é…åº¦çº¦ä¸º90%ï¼Œè€ŒåŸºçº¿æ–¹æ³•ä»…ä¸º34.5%ã€‚LLMåœ¨å®Œå…¨åŒ¹é…äººç±»æ ‡æ³¨çš„æè¿°æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œè¾¾åˆ°91.75%ï¼Œç»“åˆé«˜çº§æ­£åˆ™è¡¨è¾¾å¼ä¸LLMçš„æ–¹æ³•å‡†ç¡®ç‡æ›´é«˜è¾¾92%ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>åˆ‘äº‹å¸æ³•è¡Œæ”¿æ•°æ®å¯¹çŠ¯ç½ªè¡Œä¸ºçš„æè¿°ä¿¡æ¯æœ‰é™ã€‚</li>
<li>åœ¨æ–¯æ´›ä¼å…‹å…¬å¼€æ³•é™¢è£å†³ä¸­å­˜åœ¨ä¸°å¯Œçš„çŠ¯ç½ªè¡Œä¸ºæè¿°ä¿¡æ¯ã€‚</li>
<li>é‡‡ç”¨æ­£åˆ™è¡¨è¾¾å¼å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸¤ç§æ–¹æ³•æ¥æå–æè¿°ä¿¡æ¯ã€‚</li>
<li>é«˜çº§æ­£åˆ™è¡¨è¾¾å¼å’ŒLLMæ–¹æ³•æ˜¾è‘—æé«˜æå–å‡†ç¡®ç‡ã€‚</li>
<li>LLMåœ¨åŒ¹é…äººç±»æ ‡æ³¨çš„æè¿°æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</li>
<li>ç»“åˆé«˜çº§æ­£åˆ™è¡¨è¾¾å¼ä¸LLMçš„æ–¹æ³•èƒ½å¤Ÿè¿›ä¸€æ­¥æé«˜å‡†ç¡®ç‡ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05320">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6707c17a794bc988058173b9af9e166e" align="middle">
<img src="https://picx.zhimg.com/v2-61740538c413f4439f01c23a6ee9b5f8" align="middle">
<img src="https://picx.zhimg.com/v2-9e54c00273c83c8b742f80ff8deb778e" align="middle">
<img src="https://picx.zhimg.com/v2-42e233c4ff5b7f11df61ebeaab589820" align="middle">
<img src="https://picx.zhimg.com/v2-8d20f3e5cc02fe59109d549afa3fae3d" align="middle">
<img src="https://picx.zhimg.com/v2-29b3fc17294d7091c87c6f278df7fdcd" align="middle">
<img src="https://picx.zhimg.com/v2-09580b010699b563d29043941a2e2e14" align="middle">
<img src="https://picx.zhimg.com/v2-d7a6a83788591de2e9e76947e1ac48c3" align="middle">
<img src="https://picx.zhimg.com/v2-431b83f65551939c318b9b7a890c82b2" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="Code-Review-Automation-using-Retrieval-Augmented-Generation"><a href="#Code-Review-Automation-using-Retrieval-Augmented-Generation" class="headerlink" title="Code Review Automation using Retrieval Augmented Generation"></a>Code Review Automation using Retrieval Augmented Generation</h2><p><strong>Authors:Qianru Meng, Xiao Zhang, Zhaochen Ren, Joost Visser</strong></p>
<p>Code review is essential for maintaining software quality but is labor-intensive. Automated code review generation offers a promising solution to this challenge. Both deep learning-based generative techniques and retrieval-based methods have demonstrated strong performance in this task. However, despite these advancements, there are still some limitations where generated reviews can be either off-point or overly general. To address these issues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages Retrieval-Augmented Generation (RAG) to combine retrieval-based and generative methods, explicitly incorporating external domain knowledge into the code review process. RARe uses a dense retriever to select the most relevant reviews from the codebase, which then enrich the input for a neural generator, utilizing the contextual learning capacity of large language models (LLMs), to produce the final review. RARe outperforms state-of-the-art methods on two benchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively. Its effectiveness is further validated through a detailed human evaluation and a case study using an interpretability tool, demonstrating its practical utility and reliability. </p>
<blockquote>
<p>ä»£ç å®¡æŸ¥å¯¹äºç»´æŠ¤è½¯ä»¶è´¨é‡è‡³å…³é‡è¦ï¼Œä½†å®ƒæ˜¯åŠ³åŠ¨å¯†é›†å‹çš„ã€‚è‡ªåŠ¨ä»£ç å®¡æŸ¥ç”Ÿæˆä¸ºè§£å†³è¿™ä¸€æŒ‘æˆ˜æä¾›äº†æœ‰å‰æ™¯çš„è§£å†³æ–¹æ¡ˆã€‚åŸºäºæ·±åº¦å­¦ä¹ çš„ç”ŸæˆæŠ€æœ¯å’ŒåŸºäºæ£€ç´¢çš„æ–¹æ³•åœ¨æ­¤ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºäº†å¼ºå¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå°½ç®¡å–å¾—äº†è¿™äº›è¿›å±•ï¼Œä»ç„¶å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œç”Ÿæˆçš„è¯„è®ºå¯èƒ½ä¼šåç¦»ä¸»é¢˜æˆ–è¿‡äºç¬¼ç»Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ£€ç´¢å¢å¼ºè¯„å®¡è€…ï¼ˆRAReï¼‰ï¼Œå®ƒåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç»“åˆäº†åŸºäºæ£€ç´¢å’Œç”Ÿæˆçš„æ–¹æ³•ï¼Œæ˜ç¡®åœ°å°†å¤–éƒ¨é¢†åŸŸçŸ¥è¯†çº³å…¥ä»£ç å®¡æŸ¥è¿‡ç¨‹ã€‚RAReä½¿ç”¨å¯†é›†æ£€ç´¢å™¨ä»ä»£ç åº“ä¸­é€‰å–æœ€ç›¸å…³çš„è¯„è®ºï¼Œç„¶åä¸°å¯Œç¥ç»ç”Ÿæˆå™¨çš„è¾“å…¥ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œäº§ç”Ÿæœ€ç»ˆçš„è¯„è®ºã€‚RAReåœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°è¶…è¿‡äº†æœ€å…ˆè¿›çš„æ–¹æ³•ï¼ŒBLEU-4å¾—åˆ†åˆ†åˆ«ä¸º12.32å’Œ12.96ã€‚å…¶æœ‰æ•ˆæ€§é€šè¿‡è¯¦ç»†çš„äººå·¥è¯„ä¼°å’Œæ¡ˆä¾‹ç ”ç©¶å¾—åˆ°äº†è¿›ä¸€æ­¥éªŒè¯ï¼Œä½¿ç”¨äº†è§£é‡Šæ€§å·¥å…·ï¼Œè¯æ˜äº†å…¶å®ç”¨æ€§ã€å¯é æ€§å’Œå®ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05302v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¿™æ˜¯ä¸€ç¯‡å…³äºä»£ç å®¡æŸ¥è‡ªåŠ¨åŒ–çš„ç ”ç©¶è®ºæ–‡ã€‚æ–‡ç« æŒ‡å‡ºè‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥ç”Ÿæˆå¯¹äºç»´æŠ¤è½¯ä»¶è´¨é‡è‡³å…³é‡è¦ï¼Œä½†å­˜åœ¨ç”Ÿæˆè¯„è®ºåç¦»ä¸»é¢˜æˆ–è¿‡äºç¬¼ç»Ÿçš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæ–‡ç« æå‡ºäº†ç»“åˆæ£€ç´¢å’Œç”Ÿæˆæ–¹æ³•çš„Retrieval-Augmented Reviewerï¼ˆRAReï¼‰æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨å¯†é›†æ£€ç´¢å™¨ä»ä»£ç åº“ä¸­é€‰å–æœ€ç›¸å…³çš„è¯„è®ºï¼Œä¸°å¯Œè¾“å…¥å†…å®¹ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ç”Ÿæˆæœ€ç»ˆè¯„è®ºã€‚RAReåœ¨ä¸¤ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œå¹¶é€šè¿‡äººç±»è¯„ä¼°å’Œæ¡ˆä¾‹ç ”ç©¶éªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä»£ç å®¡æŸ¥å¯¹äºç»´æŠ¤è½¯ä»¶è´¨é‡è‡³å…³é‡è¦ï¼Œä½†äººå·¥å®¡æŸ¥åŠ³åŠ¨å¼ºåº¦å¤§ï¼Œéœ€è¦è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆã€‚</li>
<li>æ·±åº¦å­¦ä¹ å’Œæ£€ç´¢æ–¹æ³•éƒ½åœ¨è‡ªåŠ¨åŒ–ä»£ç å®¡æŸ¥ç”Ÿæˆä¸­è¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>ç”Ÿæˆçš„è¯„è®ºå¯èƒ½åç¦»ä¸»é¢˜æˆ–è¿‡äºç¬¼ç»Ÿï¼Œéœ€è¦æ”¹è¿›ã€‚</li>
<li>å¼•å…¥Retrieval-Augmented Reviewerï¼ˆRAReï¼‰æ¨¡å‹ï¼Œç»“åˆæ£€ç´¢å’Œç”Ÿæˆæ–¹æ³•ã€‚</li>
<li>RAReä½¿ç”¨å¯†é›†æ£€ç´¢å™¨ä»ä»£ç åº“ä¸­é€‰å–ç›¸å…³è¯„è®ºï¼Œä¸°å¯Œè¾“å…¥å†…å®¹ã€‚</li>
<li>RAReåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ç”Ÿæˆæœ€ç»ˆè¯„è®ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05302">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2b456d2407a3e0a3af1e5d9e94cc11c8" align="middle">
<img src="https://picx.zhimg.com/v2-908c617f6de460b4536670ad93dac081" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="LiveStar-Live-Streaming-Assistant-for-Real-World-Online-Video-Understanding"><a href="#LiveStar-Live-Streaming-Assistant-for-Real-World-Online-Video-Understanding" class="headerlink" title="LiveStar: Live Streaming Assistant for Real-World Online Video   Understanding"></a>LiveStar: Live Streaming Assistant for Real-World Online Video   Understanding</h2><p><strong>Authors:Zhenyu Yang, Kairui Zhang, Yuhang Hu, Bing Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Weiming Dong, Changsheng Xu</strong></p>
<p>Despite significant progress in Video Large Language Models (Video-LLMs) for offline video understanding, existing online Video-LLMs typically struggle to simultaneously process continuous frame-by-frame inputs and determine optimal response timing, often compromising real-time responsiveness and narrative coherence. To address these limitations, we introduce LiveStar, a pioneering live streaming assistant that achieves always-on proactive responses through adaptive streaming decoding. Specifically, LiveStar incorporates: (1) a training strategy enabling incremental video-language alignment for variable-length video streams, preserving temporal consistency across dynamically evolving frame sequences; (2) a response-silence decoding framework that determines optimal proactive response timing via a single forward pass verification; (3) memory-aware acceleration via peak-end memory compression for online inference on 10+ minute videos, combined with streaming key-value cache to achieve 1.53x faster inference. We also construct an OmniStar dataset, a comprehensive dataset for training and benchmarking that encompasses 15 diverse real-world scenarios and 5 evaluation tasks for online video understanding. Extensive experiments across three benchmarks demonstrate LiveStarâ€™s state-of-the-art performance, achieving an average 19.5% improvement in semantic correctness with 18.1% reduced timing difference compared to existing online Video-LLMs, while improving FPS by 12.0% across all five OmniStar tasks. Our model and dataset can be accessed at <a target="_blank" rel="noopener" href="https://github.com/yzy-bupt/LiveStar">https://github.com/yzy-bupt/LiveStar</a>. </p>
<blockquote>
<p>å°½ç®¡ç¦»çº¿è§†é¢‘ç†è§£çš„è§†é¢‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMï¼‰å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†ç°æœ‰çš„åœ¨çº¿Video-LLMé€šå¸¸åœ¨åŒæ—¶å¤„ç†è¿ç»­å¸§è¾“å…¥å’Œç¡®å®šæœ€ä½³å“åº”æ—¶é—´æ–¹é¢é‡åˆ°å›°éš¾ï¼Œç»å¸¸ç‰ºç‰²å®æ—¶å“åº”èƒ½åŠ›å’Œå™äº‹è¿è´¯æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†LiveStarï¼Œè¿™æ˜¯ä¸€æ¬¾å¼€åˆ›æ€§çš„ç›´æ’­åŠ©ç†ï¼Œé€šè¿‡è‡ªé€‚åº”æµåª’ä½“è§£ç å®ç°å§‹ç»ˆåœ¨çº¿çš„ä¸»åŠ¨å“åº”ã€‚å…·ä½“æ¥è¯´ï¼ŒLiveStarç»“åˆäº†ä»¥ä¸‹å‡ ç‚¹ï¼šï¼ˆ1ï¼‰ä¸€ç§è®­ç»ƒç­–ç•¥ï¼Œå®ç°å¯¹å˜é•¿è§†é¢‘æµçš„å¢é‡è§†é¢‘è¯­è¨€å¯¹é½ï¼Œä¿æŒåŠ¨æ€æ¼”åŒ–å¸§åºåˆ—çš„æ—¶é—´ä¸€è‡´æ€§ï¼›ï¼ˆ2ï¼‰ä¸€ç§å“åº”æ²‰é»˜è§£ç æ¡†æ¶ï¼Œé€šè¿‡å•æ¬¡å‰å‘ä¼ é€’éªŒè¯æ¥ç¡®å®šæœ€ä½³çš„ä¸»åŠ¨å“åº”æ—¶æœºï¼›ï¼ˆ3ï¼‰é€šè¿‡å³°å€¼ç»“æŸå†…å­˜å‹ç¼©å’Œåœ¨çº¿æ¨ç†çš„æµåª’ä½“é”®å€¼ç¼“å­˜ï¼Œå®ç°å¯¹è¶…è¿‡10åˆ†é’Ÿè§†é¢‘çš„å†…å­˜æ„ŸçŸ¥åŠ é€Ÿï¼Œè¾¾åˆ°1.53å€æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬è¿˜æ„å»ºäº†OmniStaræ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–15ç§ä¸åŒç°å®åœºæ™¯å’Œ5ç§åœ¨çº¿è§†é¢‘ç†è§£è¯„ä¼°ä»»åŠ¡çš„å…¨é¢è®­ç»ƒå’ŒåŸºå‡†æµ‹è¯•æ•°æ®é›†ã€‚åœ¨ä¸‰ä¸ªåŸºå‡†æµ‹è¯•ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLiveStarçš„æ€§èƒ½è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œä¸ç°æœ‰çš„åœ¨çº¿Video-LLMç›¸æ¯”ï¼Œè¯­ä¹‰æ­£ç¡®æ€§å¹³å‡æé«˜äº†19.5%ï¼Œæ—¶é—´å·®å¼‚å‡å°‘äº†18.1%ï¼ŒåŒæ—¶åœ¨æ‰€æœ‰äº”ä¸ªOmniStarä»»åŠ¡ä¸Šæé«˜äº†12.0%çš„FPSã€‚æˆ‘ä»¬çš„æ¨¡å‹å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/yzy-bupt/LiveStar%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/yzy-bupt/LiveStarè®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05299v1">PDF</a> NeurIPS 2025 Accepted</p>
<p><strong>æ‘˜è¦</strong></p>
<p>é’ˆå¯¹åœ¨çº¿è§†é¢‘å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆVideo-LLMï¼‰åœ¨å®æ—¶å¤„ç†è¿ç»­å¸§è¾“å…¥å¹¶ç¡®å®šæœ€ä½³å“åº”æ—¶æœºæ–¹é¢çš„å±€é™æ€§ï¼Œæˆ‘ä»¬æ¨å‡ºäº†LiveStarâ€”â€”ä¸€æ¬¾å…·å¤‡è‡ªé€‚åº”æµåª’ä½“è§£ç åŠŸèƒ½çš„å§‹ç»ˆåœ¨çº¿çš„ä¸»åŠ¨å“åº”ç›´æ’­åŠ©æ‰‹ã€‚å®ƒé€šè¿‡è®­ç»ƒç­–ç•¥ã€å“åº”-æ²‰é»˜è§£ç æ¡†æ¶å’Œå†…å­˜æ„ŸçŸ¥åŠ é€ŸæŠ€æœ¯ï¼Œå®ç°äº†å¯¹å¯å˜é•¿åº¦è§†é¢‘æµçš„å¢é‡è§†é¢‘è¯­è¨€å¯¹é½ã€å•å‰å‘ä¼ é€’éªŒè¯çš„æœ€ä½³ä¸»åŠ¨å“åº”æ—¶æœºç¡®å®šï¼Œä»¥åŠé•¿è¾¾10åˆ†é’Ÿä»¥ä¸Šçš„åœ¨çº¿æ¨ç†å†…å­˜å‹ç¼©ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†OmniStaræ•°æ®é›†ï¼Œç”¨äºåŸ¹è®­å’Œè¯„ä¼°åœ¨çº¿è§†é¢‘ç†è§£ä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼ŒLiveStaråœ¨è¯­ä¹‰æ­£ç¡®æ€§å’Œå“åº”æ—¶é—´æ–¹é¢å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ï¼Œå¹³å‡æå‡19.5%ï¼Œä¸”åœ¨æ‰€æœ‰äº”ä¸ªOmniStarä»»åŠ¡ä¸­æé«˜äº†12.0%çš„å¸§ç‡ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>LiveStarè§£å†³äº†åœ¨çº¿Video-LLMåœ¨å¤„ç†è¿ç»­å¸§è¾“å…¥å’Œç¡®å®šæœ€ä½³å“åº”æ—¶æœºæ–¹é¢çš„æŒ‘æˆ˜ï¼Œå®ç°äº†å®æ—¶å“åº”å’Œå™äº‹è¿è´¯æ€§ã€‚</li>
<li>LiveStaré€šè¿‡è®­ç»ƒç­–ç•¥å®ç°äº†å¯¹å¯å˜é•¿åº¦è§†é¢‘æµçš„å¢é‡è§†é¢‘è¯­è¨€å¯¹é½ï¼Œä¿æŒäº†åŠ¨æ€æ¼”åŒ–å¸§åºåˆ—çš„æ—¶é—´ä¸€è‡´æ€§ã€‚</li>
<li>å“åº”-æ²‰é»˜è§£ç æ¡†æ¶é€šè¿‡å•å‰å‘ä¼ é€’éªŒè¯ç¡®å®šäº†æœ€ä½³çš„ä¸»åŠ¨å“åº”æ—¶é—´ã€‚</li>
<li>é€šè¿‡å†…å­˜æ„ŸçŸ¥åŠ é€ŸæŠ€æœ¯ï¼ŒåŒ…æ‹¬å³°å€¼ç»“æŸå†…å­˜å‹ç¼©å’Œæµåª’ä½“é”®å€¼ç¼“å­˜ï¼ŒLiveStarå®ç°äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
<li>OmniStaræ•°æ®é›†çš„æ„å»ºä¸ºåœ¨çº¿è§†é¢‘ç†è§£æä¾›äº†å…¨é¢çš„åŸ¹è®­å’Œè¯„ä¼°èµ„æºï¼Œæ¶µç›–15ä¸ªå¤šæ ·åŒ–çš„çœŸå®åœºæ™¯å’Œ5ä¸ªè¯„ä¼°ä»»åŠ¡ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒLiveStaråœ¨è¯­ä¹‰æ­£ç¡®æ€§å’Œå“åº”æ—¶é—´æ–¹é¢æ˜¾è‘—ä¼˜äºå…¶ä»–åœ¨çº¿Video-LLMï¼Œå¹³å‡æå‡19.5%ã€‚</li>
<li>LiveStaråœ¨æ‰€æœ‰äº”ä¸ªOmniStarä»»åŠ¡ä¸­æé«˜äº†å¸§ç‡ï¼Œå¹³å‡æé«˜12.0%ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05299">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a64ec8bc0adb5a477a7742edadaffbd6" align="middle">
<img src="https://picx.zhimg.com/v2-0edd76fe16126fc84a5783821f0a4433" align="middle">
<img src="https://picx.zhimg.com/v2-dac7b8258b930a5d5c96d342843a64c3" align="middle">
<img src="https://picx.zhimg.com/v2-facea90062605da01048a14755bb55e4" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="Language-Generation-and-Identification-From-Partial-Enumeration-Tight-Density-Bounds-and-Topological-Characterizations"><a href="#Language-Generation-and-Identification-From-Partial-Enumeration-Tight-Density-Bounds-and-Topological-Characterizations" class="headerlink" title="Language Generation and Identification From Partial Enumeration: Tight   Density Bounds and Topological Characterizations"></a>Language Generation and Identification From Partial Enumeration: Tight   Density Bounds and Topological Characterizations</h2><p><strong>Authors:Jon Kleinberg, Fan Wei</strong></p>
<p>The success of large language models (LLMs) has motivated formal theories of language generation and learning. We study the framework of \emph{language generation in the limit}, where an adversary enumerates strings from an unknown language $K$ drawn from a countable class, and an algorithm must generate unseen strings from $K$. Prior work showed that generation is always possible, and that some algorithms achieve positive lower density, revealing a \emph{validityâ€“breadth} trade-off between correctness and coverage. We resolve a main open question in this line, proving a tight bound of $1&#x2F;2$ on the best achievable lower density. We then strengthen the model to allow \emph{partial enumeration}, where the adversary reveals only an infinite subset $C \subseteq K$. We show that generation in the limit remains achievable, and if $C$ has lower density $\alpha$ in $K$, the algorithmâ€™s output achieves density at least $\alpha&#x2F;2$, matching the upper bound. This generalizes the $1&#x2F;2$ bound to the partial-information setting, where the generator must recover within a factor $1&#x2F;2$ of the revealed subsetâ€™s density. We further revisit the classical Goldâ€“Angluin model of \emph{language identification} under partial enumeration. We characterize when identification in the limit is possible â€“ when hypotheses $M_t$ eventually satisfy $C \subseteq M \subseteq K$ â€“ and in the process give a new topological formulation of Angluinâ€™s characterization, showing that her condition is precisely equivalent to an appropriate topological space having the $T_D$ separation property. </p>
<blockquote>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸæ¿€å‘äº†å…³äºè¯­è¨€ç”Ÿæˆå’Œå­¦ä¹ çš„å½¢å¼åŒ–ç†è®ºçš„ç ”ç©¶ã€‚æˆ‘ä»¬ç ”ç©¶äº†â€œæé™è¯­è¨€ç”Ÿæˆâ€çš„æ¡†æ¶ï¼Œå…¶ä¸­å¯¹æ‰‹ä»å¯æ•°ç±»åˆ«ä¸­åˆ—ä¸¾å‡ºä¸€ä¸ªæœªçŸ¥è¯­è¨€Kçš„å­—ç¬¦ä¸²ï¼Œç®—æ³•å¿…é¡»ç”ŸæˆKä¸­æœªè§è¿‡çš„å­—ç¬¦ä¸²ã€‚æ—©æœŸçš„ç ”ç©¶è¡¨æ˜ï¼Œç”Ÿæˆæ€»æ˜¯å¯èƒ½çš„ï¼Œå¹¶ä¸”ä¸€äº›ç®—æ³•èƒ½è¾¾åˆ°æ­£çš„ä¸‹å¯†åº¦ï¼Œè¿™æ­ç¤ºäº†æ­£ç¡®æ€§ï¼ˆæ­£ç¡®æ€§ï¼‰å’Œè¦†ç›–é¢ï¼ˆå¹¿åº¦ï¼‰ä¹‹é—´çš„æœ‰æ•ˆæ€§å’Œå¹¿åº¦æƒè¡¡ã€‚æˆ‘ä»¬è§£å†³äº†è¿™ä¸€é¢†åŸŸçš„ä¸€ä¸ªä¸»è¦å¼€æ”¾é—®é¢˜ï¼Œè¯æ˜äº†æœ€ä½³å¯è¾¾ä¸‹å¯†åº¦çš„ç´§å¯†ç•Œé™ä¸º1&#x2F;2ã€‚ç„¶åæˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œäº†åŠ å¼ºï¼Œå…è®¸â€œéƒ¨åˆ†æšä¸¾â€ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯¹æ‰‹åªæ˜¾ç¤ºKçš„ä¸€ä¸ªæ— é™å­é›†Cã€‚æˆ‘ä»¬è¡¨æ˜åœ¨æé™ä¸­è¿›è¡Œç”Ÿæˆä»ç„¶æ˜¯å¯ä»¥å®ç°çš„ï¼Œå¹¶ä¸”å¦‚æœå­é›†Cåœ¨Kä¸­çš„å¯†åº¦è¾ƒä½Î±ä¸‹Î±è¾ƒä½Î±çš„æƒ…å†µä¸‹å®ç°ä¸‹å¯†åº¦çš„æœ€å°‘äºŒåˆ†ä¹‹ä¸€åŒ¹é…ä¸Šç•Œã€‚è¿™å°†äºŒåˆ†ä¹‹ä¸€çš„ç•Œé™æ¨å¹¿åˆ°éƒ¨åˆ†ä¿¡æ¯è®¾ç½®ï¼Œå…¶ä¸­ç”Ÿæˆå™¨å¿…é¡»åœ¨æ­ç¤ºçš„å­é›†çš„å¯†åº¦çš„ä¸€åŠå†…æ¢å¤ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥é‡æ–°å®¡è§†äº†éƒ¨åˆ†æšä¸¾ä¸‹çš„ç»å…¸Gold-Angluinè¯­è¨€è¯†åˆ«æ¨¡å‹ã€‚æˆ‘ä»¬æè¿°äº†ä½•æ—¶å¯ä»¥å®ç°æé™è¯†åˆ«â€”â€”å½“å‡è®¾M_tæœ€ç»ˆæ»¡è¶³C \subseteq M \subseteq Kæ—¶â€”â€”å¹¶åœ¨è¿‡ç¨‹ä¸­ç»™å‡ºäº†Angluinåˆ»ç”»çš„æ–°æ‹“æ‰‘å…¬å¼ï¼Œè¯æ˜å¥¹çš„æ¡ä»¶æ­£å¥½ç›¸å½“äºä¸€ä¸ªé€‚å½“çš„æ‹“æ‰‘ç©ºé—´å…·æœ‰TDåˆ†ç¦»å±æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05295v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸæ¿€å‘äº†è¯­è¨€ç”Ÿæˆä¸å­¦ä¹ ç†è®ºçš„ç ”ç©¶ã€‚æœ¬æ–‡ç ”ç©¶äº†â€œæé™è¯­è¨€ç”Ÿæˆâ€æ¡†æ¶ï¼Œå…¶ä¸­å¯¹æ‰‹ä»ä¸€ä¸ªå¯æ•°ç±»åˆ«ä¸­åˆ—ä¸¾æœªçŸ¥è¯­è¨€Kçš„å­—ç¬¦ä¸²ï¼Œç®—æ³•å¿…é¡»ç”Ÿæˆæœªè§çš„Kå­—ç¬¦ä¸²ã€‚å…ˆå‰çš„å·¥ä½œè¡¨æ˜ç”Ÿæˆæ€»æ˜¯å¯èƒ½çš„ï¼Œä¸”ä¸€äº›ç®—æ³•èƒ½è¾¾åˆ°æ­£çš„ä¸‹å¯†åº¦ï¼Œæ­ç¤ºäº†æ­£ç¡®æ€§ä¸è¦†ç›–é¢ä¹‹é—´çš„â€œæœ‰æ•ˆæ€§-å¹¿åº¦â€æƒè¡¡ã€‚æœ¬æ–‡è§£å†³äº†è¯¥æ–¹å‘çš„ä¸€ä¸ªä¸»è¦å¼€æ”¾é—®é¢˜ï¼Œè¯æ˜æœ€ä½³å¯è¾¾æˆä¸‹å¯†åº¦çš„ç´§å¯†ç•Œä¸º1&#x2F;2ã€‚éšåï¼Œæœ¬æ–‡å°†æ¨¡å‹æ‰©å±•åˆ°å…è®¸â€œéƒ¨åˆ†åˆ—ä¸¾â€ï¼Œå¯¹æ‰‹åªå±•ç¤ºKçš„ä¸€ä¸ªæ— é™å­é›†Cã€‚æœ¬æ–‡è¯æ˜æé™ç”Ÿæˆä»ç„¶å¯è¡Œï¼Œå¦‚æœCåœ¨Kä¸­çš„ä¸‹å¯†åº¦ä¸ºÎ±ï¼Œåˆ™ç®—æ³•è¾“å‡ºè‡³å°‘è¾¾åˆ°Î±&#x2F;2çš„å¯†åº¦ï¼Œä¸ä¸Šç•Œç›¸åŒ¹é…ã€‚è¿™æ¨å¹¿äº†éƒ¨åˆ†ä¿¡æ¯è®¾ç½®ä¸­çš„1&#x2F;2ç•Œï¼Œåœ¨è¯¥è®¾ç½®ä¸­ï¼Œç”Ÿæˆå™¨å¿…é¡»æ¢å¤åˆ°æ˜¾ç¤ºå­é›†çš„å¯†åº¦çš„1&#x2F;2ä¹‹å†…ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡é‡æ–°å®¡è§†äº†éƒ¨åˆ†åˆ—ä¸¾ä¸‹çš„ç»å…¸Gold-Angluinè¯­è¨€è¯†åˆ«æ¨¡å‹ï¼Œå¹¶æè¿°äº†æé™è¯†åˆ«ä½•æ—¶å¯èƒ½â€”â€”å½“å‡è®¾Mtæœ€ç»ˆæ»¡è¶³CâŠ†MâŠ†Kæ—¶ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œç»™å‡ºäº†Angluinè¡¨å¾çš„æ–°æ‹“æ‰‘å…¬å¼ï¼Œæ˜¾ç¤ºå¥¹çš„æ¡ä»¶æ°å¥½ç­‰åŒäºé€‚å½“çš„æ‹“æ‰‘ç©ºé—´å…·æœ‰TDåˆ†ç¦»å±æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æˆåŠŸæ¨åŠ¨äº†è¯­è¨€ç”Ÿæˆå’Œå­¦ä¹ ç†è®ºçš„ç ”ç©¶ã€‚</li>
<li>å¼•å…¥â€œæé™è¯­è¨€ç”Ÿæˆâ€æ¡†æ¶ï¼Œç ”ç©¶ç®—æ³•åœ¨å¯¹æ‰‹åˆ—ä¸¾æœªçŸ¥è¯­è¨€å­—ç¬¦ä¸²æƒ…å†µä¸‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚</li>
<li>è§£å†³äº†ä¸€ä¸ªä¸»è¦å¼€æ”¾é—®é¢˜ï¼Œå³è¯æ˜äº†æœ€ä½³å¯è¾¾æˆä¸‹å¯†åº¦çš„ç´§å¯†ç•Œä¸º1&#x2F;2ã€‚</li>
<li>æ¨¡å‹è¢«æ‰©å±•åˆ°å…è®¸éƒ¨åˆ†åˆ—ä¸¾çš„æƒ…å†µï¼Œå¹¶è¯æ˜åœ¨ç»™å®šå­é›†Cçš„ä¸‹å¯†åº¦ä¸ºÎ±æ—¶ï¼Œç®—æ³•è¾“å‡ºå¯†åº¦çš„è‡³å°‘ä¸ºÎ±&#x2F;wiseè¿›ä¸€æ­¥æ¢è®¨éƒ¨åˆ†ä¿¡æ¯è®¾ç½®ä¸­çš„è¯­è¨€ç”Ÿæˆé—®é¢˜ã€‚Î±&#x2F;2ã€‚è¿™æ¨å¹¿äº†å…ˆå‰çš„ç»“æœï¼Œå¼ºè°ƒäº†ç®—æ³•éœ€è¦åœ¨æ›´æœ‰é™çš„ä¿¡æ¯ä¸‹è¿›è¡Œæœ‰æ•ˆçš„ç”Ÿæˆã€‚ </li>
<li>é‡æ–°å®¡è§†äº†ç»å…¸Gold-Angluinè¯­è¨€è¯†åˆ«æ¨¡å‹åœ¨éƒ¨åˆ†åˆ—ä¸¾ä¸‹çš„æƒ…å†µï¼Œå¹¶æ¢ç´¢äº†æé™è¯­è¨€è¯†åˆ«çš„å¯èƒ½æ€§ã€‚ </li>
<li>ç»™å‡ºäº†Angluinè¡¨å¾çš„æ–°æ‹“æ‰‘å…¬å¼ï¼Œæ­ç¤ºäº†å…¶ä¸æ‹“æ‰‘ç©ºé—´çš„TDåˆ†ç¦»å±æ€§ä¹‹é—´çš„ç²¾ç¡®å…³ç³»ã€‚è¿™ä¸€å‘ç°æœ‰åŠ©äºæ›´æ·±å…¥åœ°ç†è§£è¯­è¨€è¯†åˆ«çš„æœºåˆ¶å’Œæ¡ä»¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05295">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-991342f7131a0c0b429bbfea1133c7b3" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="Reflective-Personalization-Optimization-A-Post-hoc-Rewriting-Framework-for-Black-Box-Large-Language-Models"><a href="#Reflective-Personalization-Optimization-A-Post-hoc-Rewriting-Framework-for-Black-Box-Large-Language-Models" class="headerlink" title="Reflective Personalization Optimization: A Post-hoc Rewriting Framework   for Black-Box Large Language Models"></a>Reflective Personalization Optimization: A Post-hoc Rewriting Framework   for Black-Box Large Language Models</h2><p><strong>Authors:Teqi Hao, Xioayu Tan, Shaojie Shi, Yinghui Xu, Xihe Qiu</strong></p>
<p>The personalization of black-box large language models (LLMs) is a critical yet challenging task. Existing approaches predominantly rely on context injection, where user history is embedded into the prompt to directly guide the generation process. However, this single-step paradigm imposes a dual burden on the model: generating accurate content while simultaneously aligning with user-specific styles. This often results in a trade-off that compromises output quality and limits precise control. To address this fundamental tension, we propose Reflective Personalization Optimization (RPO), a novel framework that redefines the personalization paradigm by decoupling content generation from alignment. RPO operates in two distinct stages: first, a base model generates a high-quality, generic response; then, an external reflection module explicitly rewrites this output to align with the userâ€™s preferences. This reflection module is trained using a two-stage process. Initially, supervised fine-tuning is employed on structured rewriting trajectories to establish a core personalized reasoning policy that models the transformation from generic to user-aligned responses. Subsequently, reinforcement learning is applied to further refine and enhance the quality of the personalized outputs. Comprehensive experiments on the LaMP benchmark demonstrate that RPO, by decoupling content generation from personalization, significantly outperforms state-of-the-art baselines. These findings underscore the superiority of explicit response shaping over implicit context injection. Moreover, RPO introduces an efficient, model-agnostic personalization layer that can be seamlessly integrated with any underlying base model, paving the way for a new and effective direction in user-centric generation scenarios. </p>
<blockquote>
<p>é»‘ç®±å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ä¸ªæ€§åŒ–æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„ä»»åŠ¡ï¼Œä½†åŒæ—¶ä¹Ÿæ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚ç›®å‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–äºä¸Šä¸‹æ–‡æ³¨å…¥ï¼Œé€šè¿‡å°†ç”¨æˆ·å†å²åµŒå…¥æç¤ºæ¥ç›´æ¥å¼•å¯¼ç”Ÿæˆè¿‡ç¨‹ã€‚ç„¶è€Œï¼Œè¿™ç§å•æ­¥æ¨¡å¼å¯¹æ¨¡å‹æå‡ºäº†åŒé‡æŒ‘æˆ˜ï¼šæ—¢è¦ç”Ÿæˆå‡†ç¡®çš„å†…å®¹ï¼Œåˆè¦ä¸ç”¨æˆ·ç‰¹å®šçš„é£æ ¼ä¿æŒä¸€è‡´ã€‚è¿™é€šå¸¸ä¼šå¯¼è‡´è¾“å‡ºè´¨é‡å’Œç²¾ç¡®æ§åˆ¶ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€åŸºæœ¬çŸ›ç›¾ï¼Œæˆ‘ä»¬æå‡ºäº†åå°„ä¸ªæ€§åŒ–ä¼˜åŒ–ï¼ˆRPOï¼‰è¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œé€šè¿‡è§£è€¦å†…å®¹ç”Ÿæˆä¸å¯¹é½æ¥é‡æ–°å®šä¹‰ä¸ªæ€§åŒ–èŒƒå¼ã€‚RPOåˆ†ä¸ºä¸¤ä¸ªç‹¬ç«‹é˜¶æ®µè¿›è¡Œæ“ä½œï¼šé¦–å…ˆï¼ŒåŸºç¡€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡ã€é€šç”¨çš„å“åº”ï¼›ç„¶åï¼Œå¤–éƒ¨åå°„æ¨¡å—æ˜¾å¼åœ°é‡å†™è¾“å‡ºï¼Œä»¥ç¬¦åˆç”¨æˆ·çš„åå¥½ã€‚è¯¥åå°„æ¨¡å—é‡‡ç”¨ä¸¤é˜¶æ®µè¿‡ç¨‹è¿›è¡Œè®­ç»ƒã€‚é¦–å…ˆï¼Œåˆ©ç”¨ç»“æ„åŒ–é‡å†™è½¨è¿¹è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œä»¥å»ºç«‹æ ¸å¿ƒä¸ªæ€§åŒ–æ¨ç†ç­–ç•¥ï¼Œè¯¥ç­–ç•¥å¯å¯¹é€šç”¨å“åº”åˆ°ç”¨æˆ·å¯¹é½å“åº”çš„è½¬æ¢è¿›è¡Œå»ºæ¨¡ã€‚éšåï¼Œåº”ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æ”¹è¿›å’Œæé«˜ä¸ªæ€§åŒ–è¾“å‡ºçš„è´¨é‡ã€‚åœ¨LaMPåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒRPOé€šè¿‡è§£è€¦å†…å®¹ç”Ÿæˆå’Œä¸ªæ€§åŒ–ï¼Œæ˜¾è‘—ä¼˜äºå½“å‰æœ€ä¼˜åŸºçº¿ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†æ˜¾å¼å“åº”å¡‘é€ ä¼˜äºéšå¼ä¸Šä¸‹æ–‡æ³¨å…¥çš„ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼ŒRPOå¼•å…¥äº†ä¸€ä¸ªé«˜æ•ˆã€æ¨¡å‹æ— å…³çš„ä¸ªäººåŒ–å±‚ï¼Œå®ƒå¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•åŸºç¡€æ¨¡å‹ä¸­ï¼Œä¸ºç”¨æˆ·ä¸ºä¸­å¿ƒçš„ç”Ÿæˆåœºæ™¯å¼€è¾Ÿäº†ä¸€æ¡æ–°çš„æœ‰æ•ˆæ–¹å‘ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05286v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºåå°„ä¸ªæ€§åŒ–ä¼˜åŒ–ï¼ˆRPOï¼‰çš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºè§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ªæ€§åŒ–è¿‡ç¨‹ä¸­çš„æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„ä¸ªæ€§åŒ–æ–¹æ³•ä¸»è¦é€šè¿‡è¯­å¢ƒæ³¨å…¥å®ç°ï¼Œè¿™è¦æ±‚æ¨¡å‹åœ¨ç”Ÿæˆå†…å®¹çš„åŒæ—¶ç¬¦åˆç”¨æˆ·ç‰¹å®šé£æ ¼ï¼Œå¯¼è‡´è¾“å‡ºè´¨é‡å’Œç²¾ç¡®æ§åˆ¶ä¹‹é—´çš„æƒè¡¡ã€‚RPOé€šè¿‡è§£è€¦å†…å®¹ç”Ÿæˆå’Œé£æ ¼å¯¹é½æ¥é‡æ–°å®šä¹‰ä¸ªæ€§åŒ–èŒƒå¼ã€‚é¦–å…ˆï¼ŒåŸºç¡€æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡é€šç”¨å“åº”ï¼Œç„¶åå¤–éƒ¨åå°„æ¨¡å—æ˜¾å¼åœ°é‡å†™è¾“å‡ºä»¥ç¬¦åˆç”¨æˆ·åå¥½ã€‚è¯¥åå°„æ¨¡å—é€šè¿‡ä¸¤é˜¶æ®µè¿‡ç¨‹è¿›è¡Œè®­ç»ƒï¼šé¦–å…ˆï¼Œåˆ©ç”¨ç»“æ„åŒ–é‡å†™è½¨è¿¹è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œå»ºç«‹ä»é€šç”¨åˆ°ç”¨æˆ·å¯¹é½å“åº”çš„æ ¸å¿ƒä¸ªæ€§åŒ–æ¨ç†ç­–ç•¥ï¼›ç„¶åï¼Œåº”ç”¨å¼ºåŒ–å­¦ä¹ è¿›ä¸€æ­¥æé«˜ä¸ªæ€§åŒ–è¾“å‡ºçš„è´¨é‡ã€‚åœ¨LaMPåŸºå‡†æµ‹è¯•ä¸Šçš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒRPOé€šè¿‡è§£è€¦å†…å®¹ç”Ÿæˆå’Œä¸ªæ€§åŒ–ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ã€‚è¿™å¼ºè°ƒäº†æ˜¾å¼å“åº”å¡‘é€ ç›¸å¯¹äºéšå¼ä¸Šä¸‹æ–‡æ³¨å…¥çš„ä¼˜è¶Šæ€§ã€‚æ­¤å¤–ï¼ŒRPOå¼•å…¥äº†ä¸€ç§é«˜æ•ˆã€æ¨¡å‹æ— å…³çš„ä¸ªäººåŒ–å±‚ï¼Œå¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•åŸºç¡€æ¨¡å‹ä¸­ï¼Œä¸ºä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„ç”Ÿæˆåœºæ™¯å¼€è¾Ÿäº†æ–°è€Œæœ‰æ•ˆçš„æ–¹å‘ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸ªæ€§åŒ–æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦ä¾èµ–è¯­å¢ƒæ³¨å…¥ï¼Œä½†å­˜åœ¨è¾“å‡ºè´¨é‡å’Œç²¾ç¡®æ§åˆ¶çš„æƒè¡¡é—®é¢˜ã€‚</li>
<li>åå°„ä¸ªæ€§åŒ–ä¼˜åŒ–ï¼ˆRPOï¼‰æ¡†æ¶è¢«æå‡ºæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå®ƒé€šè¿‡è§£è€¦å†…å®¹ç”Ÿæˆå’Œé£æ ¼å¯¹é½æ¥é‡æ–°å®šä¹‰ä¸ªæ€§åŒ–ã€‚</li>
<li>RPOåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šåŸºç¡€æ¨¡å‹ç”Ÿæˆé€šç”¨å“åº”ï¼Œå¤–éƒ¨åå°„æ¨¡å—è¿›è¡Œæ˜¾å¼é‡å†™ä»¥ç¬¦åˆç”¨æˆ·åå¥½ã€‚</li>
<li>åå°„æ¨¡å—é‡‡ç”¨ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼šæœ‰ç›‘ç£å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜ä¸ªæ€§åŒ–è¾“å‡ºçš„è´¨é‡ã€‚</li>
<li>RPOåœ¨LaMPåŸºå‡†æµ‹è¯•ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯åŸºçº¿ï¼Œå¼ºè°ƒäº†æ˜¾å¼å“åº”å¡‘é€ çš„ä¼˜è¶Šæ€§ã€‚</li>
<li>RPOå¼•å…¥äº†ä¸€ç§æ¨¡å‹æ— å…³çš„ä¸ªäººåŒ–å±‚ï¼Œå¯ä»¥ä¸ä»»ä½•åŸºç¡€æ¨¡å‹æ— ç¼é›†æˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05286">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eabbf3fe0cc0efc83c0aee5e82bbc427" align="middle">
<img src="https://picx.zhimg.com/v2-dd8bce5a894dd8be407132f127bc577e" align="middle">
<img src="https://picx.zhimg.com/v2-21186bdde8ced1ba25072a402e890dff" align="middle">
<img src="https://picx.zhimg.com/v2-a5d054589f2d17f5ed747e3596568f36" align="middle">
<img src="https://picx.zhimg.com/v2-2d11baa05d60ba55ef9ca944ff2a092d" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="GEMMA-SQL-A-Novel-Text-to-SQL-Model-Based-on-Large-Language-Models"><a href="#GEMMA-SQL-A-Novel-Text-to-SQL-Model-Based-on-Large-Language-Models" class="headerlink" title="GEMMA-SQL: A Novel Text-to-SQL Model Based on Large Language Models"></a>GEMMA-SQL: A Novel Text-to-SQL Model Based on Large Language Models</h2><p><strong>Authors:Hari Mohan Pandey, Anshul Gupta, Subham Sarkar, Minakshi Tomer, Schneider Johannes, Yan Gong</strong></p>
<p>Text-to-SQL systems enable users to interact with structured databases using natural language, eliminating the need for specialized programming knowledge. In this work, we introduce GEMMA-SQL, a lightweight and efficient text-to-SQL model built upon the open-source Gemma 2B architecture. Unlike many large language models (LLMs), GEMMA-SQL is fine-tuned in a resource-efficient, iterative manner and can be deployed on low-cost hardware. Leveraging the SPIDER benchmark for training and evaluation, GEMMA-SQL combines multiple prompting strategies, including few-shot learning, to enhance SQL query generation accuracy. The instruction-tuned variant, GEMMA-SQL Instruct, achieves 66.8% Test-Suite accuracy and 63.3% Exact Set Match accuracy, outperforming several state-of-the-art baselines such as IRNet, RYANSQL, and CodeXDavinci. The proposed approach demonstrates that effective prompt design and targeted instruction tuning can significantly boost performance while maintaining high scalability and adaptability. These results position GEMMA-SQL as a practical, open-source alternative for robust and accessible text-to-SQL systems. </p>
<blockquote>
<p>æ–‡æœ¬åˆ°SQLç³»ç»Ÿä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€ä¸ç»“æ„åŒ–æ•°æ®åº“è¿›è¡Œäº¤äº’ï¼Œä»è€Œæ— éœ€å…·å¤‡ä¸“é—¨çš„ç¼–ç¨‹çŸ¥è¯†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¨å‡ºäº†åŸºäºå¼€æºGemma 2Bæ¶æ„çš„è½»é‡çº§ã€é«˜æ•ˆçš„æ–‡æœ¬åˆ°SQLæ¨¡å‹â€”â€”GEMMA-SQLã€‚ä¸è®¸å¤šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸åŒï¼ŒGEMMA-SQLä»¥èµ„æºé«˜æ•ˆã€è¿­ä»£çš„æ–¹å¼è¿›è¡Œå¾®è°ƒï¼Œå¯åœ¨ä½æˆæœ¬ç¡¬ä»¶ä¸Šéƒ¨ç½²ã€‚åˆ©ç”¨SPIDERåŸºå‡†æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ï¼ŒGEMMA-SQLç»“åˆäº†å¤šç§æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬å°æ ·æœ¬å­¦ä¹ ï¼Œä»¥æé«˜SQLæŸ¥è¯¢ç”Ÿæˆå‡†ç¡®æ€§ã€‚æŒ‡ä»¤è°ƒæ•´å‹å˜ä½“GEMMA-SQL Instructè¾¾åˆ°äº†66.8%çš„æµ‹è¯•å¥—ä»¶å‡†ç¡®ç‡å’Œ63.3%çš„ç²¾ç¡®é›†åŒ¹é…å‡†ç¡®ç‡ï¼Œä¼˜äºIRNetã€RYANSQLå’ŒCodeXDavinciç­‰å¤šä¸ªæœ€æ–°å…ˆè¿›åŸºçº¿ã€‚æ‰€æå‡ºçš„æ–¹æ³•è¡¨æ˜ï¼Œæœ‰æ•ˆçš„æç¤ºè®¾è®¡å’Œæœ‰é’ˆå¯¹æ€§çš„æŒ‡ä»¤è°ƒæ•´å¯ä»¥åœ¨ä¿æŒé«˜å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜æ€§èƒ½ã€‚è¿™äº›ç»“æœä½¿GEMMA-SQLæˆä¸ºä¸€ä¸ªå®ç”¨ã€å¼€æºçš„æ›¿ä»£æ–¹æ¡ˆï¼Œç”¨äºæ„å»ºç¨³å¥å’Œå¯è®¿é—®çš„æ–‡æœ¬åˆ°SQLç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.04710v1">PDF</a> </p>
<p><strong>Summary</strong><br>æ–‡æœ¬è½¬SQLç³»ç»Ÿè®©ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡è‡ªç„¶è¯­è¨€ä¸ç»“æ„åŒ–æ•°æ®åº“è¿›è¡Œäº¤äº’ï¼Œæ— éœ€ç‰¹å®šç¼–ç¨‹çŸ¥è¯†ã€‚æœ¬ç ”ç©¶æ¨å‡ºåŸºäºå¼€æºGemma 2Bæ¶æ„çš„è½»é‡çº§é«˜æ•ˆæ–‡æœ¬è½¬SQLæ¨¡å‹â€”â€”GEMMA-SQLã€‚ä¸å…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹ä¸åŒï¼ŒGEMMA-SQLä»¥èµ„æºé«˜æ•ˆçš„æ–¹å¼è¿›è¡Œå¾®è°ƒï¼Œå¯åœ¨ä½æˆæœ¬ç¡¬ä»¶ä¸Šéƒ¨ç½²ã€‚åˆ©ç”¨SPIDERåŸºå‡†è¿›è¡Œæµ‹è¯•å’Œè¯„ä¼°ï¼ŒGEMMA-SQLç»“åˆå¤šç§æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬å°‘æ ·æœ¬å­¦ä¹ ï¼Œæé«˜SQLæŸ¥è¯¢ç”Ÿæˆå‡†ç¡®æ€§ã€‚æŒ‡ä»¤è°ƒæ•´å‹çš„GEMMA-SQL Instructè¾¾åˆ°66.8%çš„æµ‹è¯•å¥—ä»¶å‡†ç¡®ç‡å’Œ63.3%çš„ç²¾ç¡®é›†åŒ¹é…å‡†ç¡®ç‡ï¼Œä¼˜äºIRNetã€RYANSQLå’ŒCodeXDavinciç­‰æœ€æ–°åŸºçº¿ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼Œæœ‰æ•ˆçš„æç¤ºè®¾è®¡å’Œæœ‰é’ˆå¯¹æ€§çš„æŒ‡ä»¤è°ƒæ•´å¯ä»¥åœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜ç³»ç»Ÿçš„å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚è¿™ä½¿GEMMA-SQLæˆä¸ºå®ç”¨ã€å¼€æºçš„æ–‡æœ¬è½¬SQLç³»ç»Ÿçš„ç¨³å¥å’Œå¯è®¿é—®çš„æ›¿ä»£å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>GEMMA-SQLæ˜¯ä¸€ä¸ªåŸºäºå¼€æºGemma 2Bæ¶æ„çš„è½»é‡çº§æ–‡æœ¬è½¬SQLæ¨¡å‹ã€‚</li>
<li>ä¸å¤§å‹è¯­è¨€æ¨¡å‹ä¸åŒï¼ŒGEMMA-SQLä»¥èµ„æºé«˜æ•ˆçš„æ–¹å¼å¾®è°ƒï¼Œé€‚ç”¨äºä½æˆæœ¬ç¡¬ä»¶éƒ¨ç½²ã€‚</li>
<li>GEMMA-SQLç»“åˆå¤šç§æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬å°‘æ ·æœ¬å­¦ä¹ ï¼Œæé«˜SQLæŸ¥è¯¢ç”Ÿæˆå‡†ç¡®æ€§ã€‚</li>
<li>GEMMA-SQL Instructå‹å·å®ç°é«˜å‡†ç¡®ç‡ï¼Œä¼˜äºå…¶ä»–æœ€æ–°åŸºçº¿ã€‚</li>
<li>æœ‰æ•ˆæç¤ºè®¾è®¡å’ŒæŒ‡ä»¤è°ƒæ•´å¯æ˜¾è‘—æé«˜ç³»ç»Ÿæ€§èƒ½ã€å¯æ‰©å±•æ€§å’Œé€‚åº”æ€§ã€‚</li>
<li>GEMMA-SQLä½œä¸ºå®ç”¨ã€å¼€æºçš„æ–‡æœ¬è½¬SQLç³»ç»Ÿï¼Œå…·æœ‰æ›¿ä»£å…¶ä»–ç³»ç»Ÿçš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.04710">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4b396ac74cd270fcc06b982cc99eca8b" align="middle">
<img src="https://picx.zhimg.com/v2-846bde281ffa579f4247aec95dae0821" align="middle">
<img src="https://picx.zhimg.com/v2-b43aaa8b9ba62cc32147714bcf687796" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="MACO-A-Multi-Agent-LLM-Based-Hardware-Software-Co-Design-Framework-for-CGRAs"><a href="#MACO-A-Multi-Agent-LLM-Based-Hardware-Software-Co-Design-Framework-for-CGRAs" class="headerlink" title="MACO: A Multi-Agent LLM-Based Hardware&#x2F;Software Co-Design Framework for   CGRAs"></a>MACO: A Multi-Agent LLM-Based Hardware&#x2F;Software Co-Design Framework for   CGRAs</h2><p><strong>Authors:Zesong Jiang, Yuqi Sun, Qing Zhong, Mahathi Krishna, Deepak Patil, Cheng Tan, Sriram Krishnamoorthy, Jeff Zhang</strong></p>
<p>Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing architecture that can deliver high-performance, energy-efficient acceleration across diverse domains. By supporting reconfiguration at the functional unit level, CGRAs efficiently adapt to varying computational patterns and optimize resource utilization. However, designing CGRAs is highly challenging due to the vast design space, independent architectural parameters, and the time-consuming nature of manual design. Fortunately, the rapid advancement of large language models (LLMs) presents new opportunities to automate this process.   In this work, we propose MACOâ€“ an open-source multi-agent LLM-based framework for Hardware&#x2F;Software (HW&#x2F;SW) co-design of CGRAs. The framework employs LLM reasoning to generate CGRAs across four stages: HW&#x2F;SW co-design, Design error correction, Best design selection, and Evaluation &amp; Feedback. Furthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent reasoning and feedback to achieve higher PPA (that is, power, performance, and area) design points for a given domain. In addition, we introduce an LLM self-learning mechanism that employs LLM-driven decision making to select the optimal CGRA to accelerate the design process.   We evaluate the framework with state-of-the-art LLM-based methods and manual CGRA design, in terms of performance, power consumption, and area. Experimental results show that MACO efficiently generates high-quality CGRA architectures, significantly reducing manual design effort and demonstrating the potential of our framework for real-world CGRA design. </p>
<blockquote>
<p>ç²—ç²’åº¦å¯é‡æ„é˜µåˆ—ï¼ˆCGRAsï¼‰æ˜¯ä¸€ç§æœ‰å‰é€”çš„è®¡ç®—æ¶æ„ï¼Œå¯ä»¥åœ¨ä¸åŒçš„é¢†åŸŸæä¾›é«˜æ€§èƒ½ã€èƒ½æºæ•ˆç‡é«˜çš„åŠ é€Ÿã€‚é€šè¿‡æ”¯æŒåŠŸèƒ½å•å…ƒçº§åˆ«çš„é‡æ„ï¼ŒCGRAsèƒ½å¤Ÿé«˜æ•ˆåœ°é€‚åº”ä¸åŒçš„è®¡ç®—æ¨¡å¼å¹¶ä¼˜åŒ–èµ„æºåˆ©ç”¨ã€‚ç„¶è€Œï¼Œè®¾è®¡CGRAsæ˜¯ä¸€é¡¹æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºè®¾è®¡ç©ºé—´å¤§ã€ç‹¬ç«‹çš„æ¶æ„å‚æ•°ä»¥åŠæ‰‹åŠ¨è®¾è®¡çš„è€—æ—¶æ€§è´¨ã€‚å¹¸è¿çš„æ˜¯ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¿«é€Ÿå‘å±•ä¸ºè‡ªåŠ¨åŒ–è¿™ä¸€è¿‡ç¨‹æä¾›äº†æ–°çš„æœºä¼šã€‚</p>
</blockquote>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†MACOâ€”â€”ä¸€ä¸ªå¼€æºçš„ã€åŸºäºå¤šæ™ºèƒ½ä½“çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„ç¡¬ä»¶&#x2F;è½¯ä»¶ï¼ˆHW&#x2F;SWï¼‰ååŒè®¾è®¡CGRAsçš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›åœ¨å››ä¸ªé˜¶æ®µç”ŸæˆCGRAsï¼šHW&#x2F;SWååŒè®¾è®¡ã€è®¾è®¡é”™è¯¯æ ¡æ­£ã€æœ€ä½³è®¾è®¡é€‰æ‹©ã€è¯„ä¼°ä¸åé¦ˆã€‚æ­¤å¤–ï¼ŒMACOé€šè¿‡åˆ©ç”¨æ™ºèƒ½ä½“çš„æ¨ç†å’Œåé¦ˆæ¥è¿­ä»£ä¼˜åŒ–ç”Ÿæˆçš„CGRAsï¼Œä»¥å®ç°ç»™å®šé¢†åŸŸæ›´é«˜çš„åŠŸç‡æ€§èƒ½é¢ç§¯æ¯”ï¼ˆPPAï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹è‡ªä¸»å­¦ä¹ æœºåˆ¶ï¼Œé‡‡ç”¨å¤§å‹è¯­è¨€æ¨¡å‹é©±åŠ¨çš„å†³ç­–æ¥é€‰æ‹©æœ€ä½³CGRAä»¥åŠ é€Ÿè®¾è®¡è¿‡ç¨‹ã€‚</p>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.13557v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åŸºäºç²—ç²’åº¦å¯é‡æ„é˜µåˆ—ï¼ˆCGRAsï¼‰çš„ç¡¬ä»¶&#x2F;è½¯ä»¶ï¼ˆHW&#x2F;SWï¼‰ååŒè®¾è®¡æ˜¯ä¸€é¡¹æœ‰å‰é€”çš„æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯å¯å®ç°é«˜æ€§èƒ½ä¸”èƒ½æºæ•ˆç‡é«˜çš„è·¨åŸŸåŠ é€Ÿã€‚ç„¶è€Œï¼Œè®¾è®¡CGRAså…·æœ‰å·¨å¤§çš„æŒ‘æˆ˜æ€§å’Œå¤æ‚æ€§ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼€æºå¤šæ™ºèƒ½ä½“æ¡†æ¶MACOï¼Œç”¨äºCGRAsçš„HW&#x2F;SWååŒè®¾è®¡ã€‚MACOæ¡†æ¶åˆ©ç”¨LLMæ¨ç†ç”ŸæˆCGRAså¹¶ä¼˜åŒ–å…¶æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMACOèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡çš„CGRAæ¶æ„ï¼Œæ˜¾è‘—é™ä½æ‰‹åŠ¨è®¾è®¡çš„å·¥ä½œé‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CGRAsæ˜¯ä¸€ç§æœ‰å‰æ™¯çš„è®¡ç®—æ¶æ„ï¼Œå¯å®ç°è·¨åŸŸçš„é«˜æ€§èƒ½ã€èƒ½æºæ•ˆç‡åŠ é€Ÿã€‚</li>
<li>è®¾è®¡CGRAsé¢ä¸´å·¨å¤§çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å·¨å¤§çš„è®¾è®¡ç©ºé—´ã€ç‹¬ç«‹çš„æ¶æ„å‚æ•°å’Œæ—¶é—´æ¶ˆè€—çš„æ‰‹åŠ¨è®¾è®¡ã€‚</li>
<li>MACOæ˜¯ä¸€ä¸ªåŸºäºLLMçš„å¼€æºå¤šæ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºCGRAsçš„HW&#x2F;SWååŒè®¾è®¡ã€‚</li>
<li>MACOåˆ©ç”¨LLMæ¨ç†ç”ŸæˆCGRAså¹¶åœ¨å››ä¸ªä¸åŒé˜¶æ®µè¿›è¡Œä¼˜åŒ–ã€‚</li>
<li>MACOæ¡†æ¶å…·å¤‡è‡ªæˆ‘å­¦ä¹ èƒ½åŠ›ï¼Œèƒ½å¤Ÿé€šè¿‡LLMé©±åŠ¨çš„å†³ç­–é€‰æ‹©æœ€ä½³CGRAæ¥åŠ é€Ÿè®¾è®¡è¿‡ç¨‹ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒMACOèƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆé«˜è´¨é‡çš„CGRAæ¶æ„ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13557">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-4aabc5da16c4dcfe1cbcbd301af5e877" align="middle">
<img src="https://picx.zhimg.com/v2-b2e1232f211530ee219c37e712196061" align="middle">
<img src="https://picx.zhimg.com/v2-b7ffb25990eb22d3ad7280d7f4dfe92e" align="middle">
<img src="https://picx.zhimg.com/v2-166babb66c8b5b07a1fd4cb6d6151e81" align="middle">
<img src="https://picx.zhimg.com/v2-398ad472d42ce34435392c29b3bd8bd1" align="middle">
<img src="https://picx.zhimg.com/v2-f843341f90bc7a406fe3cb5eac0fe9d0" align="middle">
<img src="https://picx.zhimg.com/v2-2ca3b86d2dffc28f0fc2dcc895f42f9c" align="middle">
<img src="https://picx.zhimg.com/v2-b6c5deecd32d6f4f68800d627cadbf07" align="middle">
<img src="https://picx.zhimg.com/v2-b462474d74926fe8b623bbfff12783f7" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="P-ReMIS-Pragmatic-Reasoning-in-Mental-Health-and-a-Social-Implication"><a href="#P-ReMIS-Pragmatic-Reasoning-in-Mental-Health-and-a-Social-Implication" class="headerlink" title="P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication"></a>P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication</h2><p><strong>Authors:Sneha Oram, Pushpak Bhattacharyya</strong></p>
<p>Although explainability and interpretability have received significant attention in artificial intelligence (AI) and natural language processing (NLP) for mental health, reasoning has not been examined in the same depth. Addressing this gap is essential to bridge NLP and mental health through interpretable and reasoning-capable AI systems. To this end, we investigate the pragmatic reasoning capability of large-language models (LLMs) in the mental health domain. We introduce PRiMH dataset, and propose pragmatic reasoning tasks in mental health with pragmatic implicature and presupposition phenomena. In particular, we formulate two tasks in implicature and one task in presupposition. To benchmark the dataset and the tasks presented, we consider four models: Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the experiments suggest that Mistral and Qwen show substantial reasoning abilities in the domain. Subsequently, we study the behavior of MentaLLaMA on the proposed reasoning tasks with the rollout attention mechanism. In addition, we also propose three StiPRompts to study the stigma around mental health with the state-of-the-art LLMs, GPT4o-mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings show that Claude-3.5-haiku deals with stigma more responsibly compared to the other two LLMs. </p>
<blockquote>
<p>å°½ç®¡äººå·¥æ™ºèƒ½å’Œè‡ªç„¶è¯­è¨€å¤„ç†åœ¨å¿ƒç†å¥åº·é¢†åŸŸå·²ç»å¾—åˆ°äº†å¯¹è§£é‡Šæ€§å’Œå¯è§£é‡Šæ€§çš„å¹¿æ³›å…³æ³¨ï¼Œä½†æ¨ç†å°šæœªå¾—åˆ°åŒç­‰æ·±åº¦çš„ç ”ç©¶ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬é€šè¿‡å¯è§£é‡Šå’Œå…·å¤‡æ¨ç†èƒ½åŠ›çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿï¼Œå°†è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¿ƒç†å¥åº·è¿æ¥èµ·æ¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç ”ç©¶å¿ƒç†å¥åº·é¢†åŸŸä¸­å¤§å‹è¯­è¨€æ¨¡å‹çš„å®ç”¨æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å¼•å…¥äº†PRiMHæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†å…·æœ‰å®ç”¨éšå«å’Œé¢„è®¾ç°è±¡çš„å¿ƒç†å¥åº·å®ç”¨æ¨ç†ä»»åŠ¡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åˆ¶å®šäº†ä¸¤ä¸ªéšå«ä»»åŠ¡å’Œä¸€ä¸ªé¢„è®¾ä»»åŠ¡ã€‚ä¸ºäº†è¯„ä¼°æ‰€æå‡ºçš„æ•°æ®é›†å’Œä»»åŠ¡ï¼Œæˆ‘ä»¬è€ƒè™‘äº†å››ç§æ¨¡å‹ï¼šLlama3.1ã€Mistralã€MentaLLaMa å’Œ Qwenã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMistral å’Œ Qwen åœ¨è¯¥é¢†åŸŸçš„æ¨ç†èƒ½åŠ›æ˜¾è‘—ã€‚éšåï¼Œæˆ‘ä»¬ç ”ç©¶äº† MentaLLaMA åœ¨æ‰€æå‡ºæ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œé‡‡ç”¨æ»šåŠ¨æ³¨æ„åŠ›æœºåˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸‰ä¸ª StiPRompts æ¥ç ”ç©¶ä½¿ç”¨æœ€å‰æ²¿çš„å¤§å‹è¯­è¨€æ¨¡å‹ GPT4o-miniã€Deepseek-chat å’Œ Claude-3.5-haiku å›´ç»•å¿ƒç†å¥åº·çš„æ±¡ååŒ–ç°è±¡ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–ä¸¤ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ç›¸æ¯”ï¼ŒClaude-3.5-haiku åœ¨å¤„ç†æ±¡ååŒ–æ–¹é¢æ›´åŠ è´Ÿè´£ä»»ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.23247v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡å…³æ³¨äººå·¥æ™ºèƒ½åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­å¯¹ç²¾ç¥å¥åº·é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ç ”ç©¶ã€‚ä¸ºè§£å†³ç›®å‰è§£é‡Šæ€§å’Œå¯ç†è§£æ€§ç ”ç©¶ä¸­ç¼ºä¹æ·±åº¦çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿé’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®ç”¨æ¨ç†èƒ½åŠ›å±•å¼€ç ”ç©¶ã€‚æ–‡ç« ä»‹ç»äº†PRiMHæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†å…·æœ‰å®ç”¨éšå«å’Œé¢„è®¾ç°è±¡çš„ç²¾ç¥å¥åº·é¢†åŸŸçš„å®ç”¨æ¨ç†ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMistralå’ŒQwenåœ¨è¯¥é¢†åŸŸçš„æ¨ç†èƒ½åŠ›è¾ƒå¼ºã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ç ”ç©¶äº†MentaLLaMAåœ¨æå‡ºçš„æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼Œå¹¶æå‡ºäº†ä¸‰ä¸ªå…³äºç²¾ç¥å¥åº·è€»è¾±çš„StiPRomptsæç¤ºæ¥ç ”ç©¶æœ€å…ˆè¿›çš„LLMsçš„è¡¨ç°ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒClaude-3.5-haikuåœ¨å¤„ç†ç²¾ç¥å¥åº·è€»è¾±æ–¹é¢è¡¨ç°æ›´è´Ÿè´£ä»»ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç²¾ç¥å¥åº·é¢†åŸŸçš„å®ç”¨æ¨ç†èƒ½åŠ›å—åˆ°å…³æ³¨ã€‚</li>
<li>PRiMHæ•°æ®é›†è¢«å¼•å…¥ï¼Œç”¨äºç ”ç©¶å®ç”¨æ¨ç†ä»»åŠ¡ã€‚</li>
<li>å®éªŒä¸­ï¼ŒMistralå’ŒQwenå±•ç°å‡ºè¾ƒå¼ºçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>MentaLLaMAåœ¨æå‡ºçš„æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°è¢«ç ”ç©¶ã€‚</li>
<li>æå‡ºä¸‰ä¸ªå…³äºç²¾ç¥å¥åº·è€»è¾±çš„StiPRomptsæç¤ºã€‚</li>
<li>Claude-3.5-haikuåœ¨å¤„ç†ç²¾ç¥å¥åº·è€»è¾±æ–¹é¢è¡¨ç°æ›´è´Ÿè´£ä»»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.23247">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9a2638c78a63f4310bec0f330d4f8263" align="middle">
<img src="https://picx.zhimg.com/v2-0c04725d71834eea883b5f3e76996784" align="middle">
<img src="https://picx.zhimg.com/v2-c7c09a1b666a59070570726f9dcecbd1" align="middle">
<img src="https://picx.zhimg.com/v2-bc2c5dfecfd11046695355bcdeeaf4c8" align="middle">
<img src="https://picx.zhimg.com/v2-ee5c7a8fbe10b6e83d12602b8e5d03dc" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="Know-What-You-Donâ€™t-Know-Uncertainty-Calibration-of-Process-Reward-Models"><a href="#Know-What-You-Donâ€™t-Know-Uncertainty-Calibration-of-Process-Reward-Models" class="headerlink" title="Know What You Donâ€™t Know: Uncertainty Calibration of Process Reward   Models"></a>Know What You Donâ€™t Know: Uncertainty Calibration of Process Reward   Models</h2><p><strong>Authors:Young-Jin Park, Kristjan Greenewald, Kaveh Alim, Hao Wang, Navid Azizan</strong></p>
<p>Process reward models (PRMs) play a central role in guiding inference-time scaling algorithms for large language models (LLMs). However, we observe that even state-of-the-art PRMs can be poorly calibrated. Specifically, they tend to overestimate the success probability that a partial reasoning step will lead to a correct final answer, particularly when smaller LLMs are used to complete the reasoning trajectory. To address this, we present a calibration approach â€“ performed via quantile regression â€“ that adjusts PRM outputs to better align with true success probabilities. Leveraging these calibrated success estimates and their associated confidence bounds, we introduce an \emph{instance-adaptive scaling} (IAS) framework that dynamically adjusts the compute budget based on the estimated likelihood that a partial reasoning trajectory will yield a correct final answer. Unlike conventional methods that allocate a fixed number of reasoning trajectories per query, this approach adapts to each instance and reasoning step when using our calibrated PRMs. Experiments on mathematical reasoning benchmarks show that (i) our PRM calibration method achieves small calibration error, outperforming the baseline methods, (ii) calibration is crucial for enabling effective IAS, and (iii) the proposed IAS strategy reduces inference costs while maintaining final answer accuracy, utilizing less compute on more confident problems as desired. </p>
<blockquote>
<p>æµç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†æ—¶é—´ç¼©æ”¾ç®—æ³•ä¸­èµ·åˆ°äº†æ ¸å¿ƒä½œç”¨ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°å³ä½¿æ˜¯æœ€å…ˆè¿›çš„PRMä¹Ÿå¯èƒ½å­˜åœ¨æ ¡å‡†ä¸è‰¯çš„æƒ…å†µã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒä»¬å€¾å‘äºé«˜ä¼°éƒ¨åˆ†æ¨ç†æ­¥éª¤ä¼šå¯¼è‡´æ­£ç¡®æœ€ç»ˆç­”æ¡ˆçš„æˆåŠŸæ¦‚ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨è¾ƒå°çš„LLMæ¥å®Œæˆæ¨ç†è½¨è¿¹æ—¶æ›´ä¸ºæ˜æ˜¾ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2506.09338v2">PDF</a> Accepted at NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>åŸºäºè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆPRMï¼‰åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†æ—¶é—´ç¼©æ”¾ç®—æ³•ä¸­çš„æ ¸å¿ƒä½œç”¨ï¼Œæ–‡ç« æŒ‡å‡ºå½“å‰æœ€å…ˆè¿›çš„PRMå­˜åœ¨æ ¡å‡†ä¸è‰¯çš„é—®é¢˜ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæ–‡ç« æå‡ºäº†ä¸€ç§é€šè¿‡åˆ†ä½å›å½’è¿›è¡Œæ ¡å‡†çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯è°ƒæ•´PRMè¾“å‡ºä»¥æ›´å¥½åœ°ä¸çœŸå®æˆåŠŸç‡å¯¹é½ã€‚å¹¶åˆ©ç”¨è¿™äº›æ ¡å‡†åçš„æˆåŠŸä¼°ç®—å’Œç›¸å…³çš„ç½®ä¿¡åŒºé—´ï¼Œå¼•å…¥äº†è‡ªé€‚åº”è®¡ç®—èµ„æºåˆ†é…ç­–ç•¥â€”â€”åŠ¨æ€æ ¹æ®é¢„ä¼°çš„å¯èƒ½æ€§è°ƒæ•´è®¡ç®—é¢„ç®—ï¼Œå†³å®šä¸åŒæ¨ç†è½¨è¿¹çš„åˆ†é…ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç­–ç•¥å®ç°äº†è‰¯å¥½çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>PRMsåœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹æ¨ç†æ—¶é—´ç¼©æ”¾ç®—æ³•ä¸­èµ·æ ¸å¿ƒä½œç”¨ï¼Œä½†å­˜åœ¨æ ¡å‡†ä¸è‰¯çš„é—®é¢˜ã€‚</li>
<li>åˆ†ä½å›å½’æ–¹æ³•ç”¨äºæ ¡å‡†PRMè¾“å‡ºï¼Œä»¥æ›´å¥½åœ°ä¸çœŸå®æˆåŠŸç‡å¯¹é½ã€‚</li>
<li>åˆ©ç”¨æ ¡å‡†åçš„æˆåŠŸä¼°ç®—å’Œç½®ä¿¡åŒºé—´ï¼Œå¼•å…¥äº†è‡ªé€‚åº”è°ƒæ•´è®¡ç®—é¢„ç®—çš„ç­–ç•¥ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„PRMæ ¡å‡†æ–¹æ³•å…·æœ‰è¾ƒå°çš„æ ¡å‡†è¯¯å·®ï¼Œä¼˜äºåŸºå‡†æ–¹æ³•ã€‚</li>
<li>æ ¡å‡†å¯¹äºå®ç°æœ‰æ•ˆçš„è‡ªé€‚åº”ç¼©æ”¾è‡³å…³é‡è¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.09338">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d1896a006d587fcbd12638ad8796aef5" align="middle">
<img src="https://picx.zhimg.com/v2-9328fc050c330474f895b5d3bbc918f9" align="middle">
<img src="https://picx.zhimg.com/v2-53a1adf12ef56f80c9a923384f39b908" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Towards-Explainable-Fake-Image-Detection-with-Multi-Modal-Large-Language-Models"><a href="#Towards-Explainable-Fake-Image-Detection-with-Multi-Modal-Large-Language-Models" class="headerlink" title="Towards Explainable Fake Image Detection with Multi-Modal Large Language   Models"></a>Towards Explainable Fake Image Detection with Multi-Modal Large Language   Models</h2><p><strong>Authors:Yikun Ji, Yan Hong, Jiahui Zhan, Haoxing Chen, jun lan, Huijia Zhu, Weiqiang Wang, Liqing Zhang, Jianfu Zhang</strong></p>
<p>Progress in image generation raises significant public security concerns. We argue that fake image detection should not operate as a â€œblack boxâ€. Instead, an ideal approach must ensure both strong generalization and transparency. Recent progress in Multi-modal Large Language Models (MLLMs) offers new opportunities for reasoning-based AI-generated image detection. In this work, we evaluate the capabilities of MLLMs in comparison to traditional detection methods and human evaluators, highlighting their strengths and limitations. Furthermore, we design six distinct prompts and propose a framework that integrates these prompts to develop a more robust, explainable, and reasoning-driven detection system. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Gennadiyev/mllm-defake">https://github.com/Gennadiyev/mllm-defake</a>. </p>
<blockquote>
<p>å›¾åƒç”ŸæˆæŠ€æœ¯çš„è¿›æ­¥å¼•å‘äº†å…¬ä¼—å®‰å…¨æ–¹é¢çš„é‡å¤§æ‹…å¿§ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œå‡å›¾åƒæ£€æµ‹ä¸åº”åƒâ€œé»‘ç®±â€é‚£æ ·è¿è¡Œã€‚ç›¸åï¼Œç†æƒ³çš„æ–¹æ³•å¿…é¡»ç¡®ä¿æ—¢å¼ºå¤§åˆé€æ˜ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æœ€æ–°è¿›å±•ä¸ºåŸºäºæ¨ç†çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹æä¾›äº†æ–°çš„æœºä¼šã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†MLLMsä¸ä¼ ç»Ÿæ£€æµ‹æ–¹æ³•å’Œäººç±»è¯„ä¼°è€…çš„èƒ½åŠ›ï¼Œçªå‡ºäº†å®ƒä»¬çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†å…­ç§ä¸åŒçš„æç¤ºå¹¶æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†è¿™äº›æç¤ºæ¥å¼€å‘ä¸€ä¸ªæ›´ç¨³å¥ã€å¯è§£é‡Šå’Œä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„æ£€æµ‹ç³»ç»Ÿã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/Gennadiyev/mllm-defake">https://github.com/Gennadiyev/mllm-defake</a>æ‰¾åˆ°ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.14245v2">PDF</a> Accepted to ACM MM 2025; 14 pages including Appendix</p>
<p><strong>Summary</strong><br>     å›¾åƒç”ŸæˆæŠ€æœ¯çš„è¿›æ­¥å¼•å‘äº†å…¬ä¼—å®‰å…¨æ‹…å¿§ã€‚æˆ‘ä»¬ä¸»å¼ å‡å›¾åƒæ£€æµ‹ä¸åº”åƒâ€œé»‘ç®±â€ä¸€æ ·è¿ä½œï¼Œç†æƒ³çš„æ–¹æ³•å¿…é¡»åŒæ—¶ç¡®ä¿å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œé€æ˜åº¦ã€‚å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æœ€æ–°è¿›å±•ä¸ºåŸºäºæ¨ç†çš„AIç”Ÿæˆå›¾åƒæ£€æµ‹æä¾›äº†æ–°çš„æœºä¼šã€‚æˆ‘ä»¬è¯„ä¼°äº†MLLMsä¸ä¼ ç»Ÿæ£€æµ‹æ–¹æ³•å’Œäººç±»è¯„ä¼°è€…çš„èƒ½åŠ›ï¼Œçªå‡ºäº†å®ƒä»¬çš„ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†å…­ç§ä¸åŒçš„æç¤ºï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œå°†è¿™äº›æç¤ºé›†æˆåœ¨ä¸€èµ·ï¼Œä»¥å¼€å‘ä¸€ä¸ªæ›´ç¨³å¥ã€å¯è§£é‡Šã€åŸºäºæ¨ç†çš„æ£€æµ‹ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å›¾åƒç”ŸæˆæŠ€æœ¯çš„è¿›æ­¥å¼•å‘äº†å…¬ä¼—å®‰å…¨å…³æ³¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å‡å›¾åƒæ£€æµ‹æ–¹é¢ã€‚</li>
<li>ç†æƒ³çš„å‡å›¾åƒæ£€æµ‹æ–¹æ³•éœ€è¦å…¼å…·å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›å’Œé€æ˜åº¦ã€‚</li>
<li>å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨AIç”Ÿæˆçš„å›¾åƒæ£€æµ‹æ–¹é¢å…·æœ‰æ½œåŠ›ã€‚</li>
<li>MLLMsåœ¨å‡å›¾åƒæ£€æµ‹æ–¹é¢çš„æ€§èƒ½ä¼˜äºä¼ ç»Ÿæ–¹æ³•ï¼Œä½†ä»å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>è®¾è®¡äº†å…­ç§æç¤ºæ¥å¢å¼ºMLLMsåœ¨å›¾åƒæ£€æµ‹ä¸­çš„ç¨³å¥æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
<li>æå‡ºçš„æ¡†æ¶æ—¨åœ¨é›†æˆè¿™äº›æç¤ºï¼Œä»¥åˆ›å»ºä¸€ä¸ªæ›´ç¨³å¥ã€å¯è§£é‡Šã€åŸºäºæ¨ç†çš„æ£€æµ‹ç³»ç»Ÿã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.14245">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-905e636bea16895844188f91aa30f7f2" align="middle">
<img src="https://picx.zhimg.com/v2-0dc6bf6082980ea620e24d4afc4095e1" align="middle">
<img src="https://picx.zhimg.com/v2-b6a49901de1d188ba3b6d8c3641eedea" align="middle">
<img src="https://picx.zhimg.com/v2-fa55a0f410ca647a640cd71073cb1486" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="MorphTok-Morphologically-Grounded-Tokenization-for-Indian-Languages"><a href="#MorphTok-Morphologically-Grounded-Tokenization-for-Indian-Languages" class="headerlink" title="MorphTok: Morphologically Grounded Tokenization for Indian Languages"></a>MorphTok: Morphologically Grounded Tokenization for Indian Languages</h2><p><strong>Authors:Maharaj Brahma, N J Karthika, Atul Singh, Devaraj Adiga, Smruti Bhate, Ganesh Ramakrishnan, Rohit Saluja, Maunendra Sankar Desarkar</strong></p>
<p>Tokenization is a crucial step in NLP, especially with the rise of large language models (LLMs), impacting downstream performance, computational cost, and efficiency. Existing LLMs rely on the classical Byte-pair Encoding (BPE) algorithm for subword tokenization that greedily merges frequent character bigrams, often leading to segmentation that does not align with linguistically meaningful units. To address this, we propose morphology-aware segmentation as a pre-tokenization step before applying BPE. To facilitate morphology-aware segmentation, we create a novel dataset for Hindi and Marathi, incorporating sandhi splitting to enhance the subword tokenization. Experiments on downstream tasks show that morphologically grounded tokenization improves machine translation and language modeling performance. Additionally, to handle the dependent vowels common in syllable-based writing systems used by Indic languages, we propose Constrained BPE (CBPE), an extension to the standard BPE algorithm incorporating script-specific constraints. In particular, CBPE handles dependent vowels to form a cohesive unit with other characters instead of occurring as a single unit. Our results show that CBPE achieves a 1.68% reduction in fertility scores while maintaining comparable or improved downstream performance in machine translation and language modeling, offering a computationally efficient alternative to standard BPE. Moreover, to evaluate segmentation across different tokenization algorithms, we introduce a new human evaluation metric, \textit{EvalTok}, enabling more human-grounded assessment. </p>
<blockquote>
<p>åˆ†è¯æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€ä¸ªå…³é”®æ­¥éª¤ï¼Œç‰¹åˆ«æ˜¯éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å…´èµ·ï¼Œå®ƒå¯¹ä¸‹æ¸¸æ€§èƒ½ã€è®¡ç®—æˆæœ¬å’Œæ•ˆç‡äº§ç”Ÿäº†å½±å“ã€‚ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹ä¾èµ–äºç»å…¸çš„å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰ç®—æ³•è¿›è¡Œå­è¯åˆ†è¯ï¼Œè¯¥ç®—æ³•è´ªå©ªåœ°åˆå¹¶é¢‘ç¹å‡ºç°çš„å­—ç¬¦åŒå­—èŠ‚ï¼Œé€šå¸¸å¯¼è‡´ä¸è¯­è¨€ä¸Šæœ‰æ„ä¹‰çš„å•ä½ä¸åŒ¹é…çš„åˆ†å‰²ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºå½¢æ€æ„ŸçŸ¥åˆ†å‰²ä½œä¸ºåº”ç”¨BPEä¹‹å‰çš„é¢„åˆ†è¯æ­¥éª¤ã€‚ä¸ºäº†ä¿ƒè¿›å½¢æ€æ„ŸçŸ¥åˆ†å‰²ï¼Œæˆ‘ä»¬ä¸ºå°åœ°è¯­å’Œé©¬æ‹‰åœ°è¯­åˆ›å»ºäº†æ–°å‹æ•°æ®é›†ï¼Œèå…¥æ²™é™€åˆ†è£‚ä»¥å¢å¼ºå­è¯åˆ†è¯ã€‚ä¸‹æ¸¸ä»»åŠ¡çš„å®éªŒè¡¨æ˜ï¼Œå½¢æ€åŸºç¡€åˆ†è¯æé«˜äº†æœºå™¨ç¿»è¯‘å’Œè¯­è¨€å»ºæ¨¡çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¤„ç†å°åœ°æ–‡å­—æ¯ç³»ç»Ÿä¸­å¸¸è§çš„ä¾èµ–å…ƒéŸ³ï¼Œæˆ‘ä»¬å¯¹æ ‡å‡†BPEç®—æ³•è¿›è¡Œäº†æ‰©å±•ï¼Œæå‡ºäº†çº¦æŸBPEï¼ˆCBPEï¼‰ï¼Œå¢åŠ äº†ç‰¹å®šè„šæœ¬çš„çº¦æŸã€‚ç‰¹åˆ«æ˜¯ï¼ŒCBPEå¤„ç†ä¾èµ–å…ƒéŸ³ï¼Œä¸å…¶ä»–å­—ç¬¦å½¢æˆè¿è´¯çš„å•ä½ï¼Œè€Œä¸æ˜¯ä»¥å•ä¸ªå•ä½å‡ºç°ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒCBPEåœ¨ä¿æŒæˆ–æé«˜æœºå™¨ç¿»è¯‘å’Œè¯­è¨€å»ºæ¨¡çš„ä¸‹æ¸¸æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†ç”Ÿè‚²ç‡è¯„åˆ†é™ä½1.68%ï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ç§è®¡ç®—æ•ˆç‡é«˜çš„æ›¿ä»£æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯„ä¼°ä¸åŒåˆ†è¯ç®—æ³•çš„åˆ†å‰²æ•ˆæœï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„äººç±»è¯„ä¼°æŒ‡æ ‡EvalTokï¼Œä½¿äººç±»åŸºç¡€çš„è¯„ä¼°æˆä¸ºå¯èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.10335v2">PDF</a> Accepted at Tokenization Workshop (TokShop), ICML 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„è¯æ³•åˆ†ææ­¥éª¤çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰èƒŒæ™¯ä¸‹ï¼Œå…¶å½±å“äº†ä¸‹æ¸¸æ€§èƒ½ã€è®¡ç®—æˆæœ¬å’Œæ•ˆç‡ã€‚ä¸ºè§£å†³ä¼ ç»Ÿå­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰ç®—æ³•çš„å±€é™æ€§ï¼Œå¦‚æ— æ³•å¯¹é½è¯­è¨€ä¸Šæ„ä¹‰å•å…ƒçš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†å½¢æ€æ„ŸçŸ¥åˆ†å‰²ä½œä¸ºé¢„è¯æ³•åˆ†ææ­¥éª¤ã€‚åŒæ—¶ï¼Œä¸ºäº†å¤„ç†å°åº¦è¯­è¨€ä¸­çš„ä¾å­˜å…ƒéŸ³é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†çº¦æŸBPEï¼ˆCBPEï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨æ ‡å‡†BPEç®—æ³•çš„åŸºç¡€ä¸Šå¢åŠ äº†è„šæœ¬ç‰¹å®šçº¦æŸã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒCBPEåœ¨ä¿è¯ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†ç”Ÿè‚²ç‡å¾—åˆ†çš„é™ä½ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¯„ä¼°ä¸åŒè¯æ³•åˆ†æç®—æ³•çš„åˆ†è¯æ•ˆæœï¼Œæœ¬æ–‡å¼•å…¥äº†å…¨æ–°çš„äººç±»è¯„ä¼°æŒ‡æ ‡EvalTokã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¯æ³•åˆ†æåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é‡è¦æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹èƒŒæ™¯ä¸‹ã€‚</li>
<li>ä¼ ç»Ÿå­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰ç®—æ³•å­˜åœ¨çš„å±€é™æ€§åŠå…¶åœ¨NLPä¸­çš„è¡¨ç°é—®é¢˜ã€‚</li>
<li>æå‡ºå½¢æ€æ„ŸçŸ¥åˆ†å‰²ä½œä¸ºé¢„è¯æ³•åˆ†ææ­¥éª¤æ¥è§£å†³ä¼ ç»Ÿç®—æ³•çš„ä¸è¶³ã€‚</li>
<li>ä¸ºé€‚åº”å°åº¦è¯­è¨€çš„ç‰¹ç‚¹ï¼Œå¼•å…¥äº†çº¦æŸBPEï¼ˆCBPEï¼‰ç®—æ³•å¤„ç†ä¾å­˜å…ƒéŸ³é—®é¢˜ã€‚</li>
<li>CBPEç®—æ³•åœ¨ä¿è¯ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„åŒæ—¶é™ä½äº†ç”Ÿè‚²ç‡å¾—åˆ†ã€‚</li>
<li>å¼•å…¥å…¨æ–°çš„äººç±»è¯„ä¼°æŒ‡æ ‡EvalTokæ¥è¯„ä¼°ä¸åŒè¯æ³•åˆ†æç®—æ³•çš„åˆ†è¯æ•ˆæœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2504.10335">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-02426c7aa501488d56bc1d7f29f88332" align="middle">
<img src="https://picx.zhimg.com/v2-6892bb2d7db0a404388d0c82b491d249" align="middle">
<img src="https://picx.zhimg.com/v2-97a54ff300ec984fbc82dfd098b2e35e" align="middle">
<img src="https://picx.zhimg.com/v2-1e902dbe4448aaa2142c4822159fe8c2" align="middle">
<img src="https://picx.zhimg.com/v2-cb75c545777600c661810c48d1f3f733" align="middle">
<img src="https://picx.zhimg.com/v2-cf858f688d57cce87f2b2683849270b4" align="middle">
<img src="https://picx.zhimg.com/v2-95c9a137a8e2d882a93fe200c4239bf2" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="LEME-Open-Large-Language-Models-for-Ophthalmology-with-Advanced-Reasoning-and-Clinical-Validation"><a href="#LEME-Open-Large-Language-Models-for-Ophthalmology-with-Advanced-Reasoning-and-Clinical-Validation" class="headerlink" title="LEME: Open Large Language Models for Ophthalmology with Advanced   Reasoning and Clinical Validation"></a>LEME: Open Large Language Models for Ophthalmology with Advanced   Reasoning and Clinical Validation</h2><p><strong>Authors:Hyunjae Kim, Xuguang Ai, Sahana Srinivasan, Aidan Gilson, Maxwell B. Singer, Krithi Pushpanathan, Qianqian Xie, Jungwoo Park, Serina Applebaum, Gabriel Dawei Yang, Minjie Zou, David Ziyou Chen, Ke Zou, Soshian Sarrafpour, Ji Liu, Yu Yin, Jimin Huang, Quang Ngoc Nguyen, Erping Long, Peixing Wan, Dianbo Liu, Richard Hintz, W. Jim Zheng, Sophia Y. Wang, Lucila Ohno-Machado, Hua Xu, Ron A. Adelman, Luciano V. Del Priore, Yih-Chung Tham, Qingyu Chen</strong></p>
<p>The rising prevalence of eye diseases poses a growing public health burden. Large language models (LLMs) offer a promising path to reduce documentation workload and support clinical decision-making. However, few have been tailored for ophthalmology, and most evaluations focus mainly on knowledge-based QA without clinically relevant benchmarks or real-world validation. Here, we present LEME, a suite of open-weight LLMs developed through a two-stage process: (1) instruction tuning on 200,000 samples from clinical guidelines, textbooks, and case reports to enhance reasoning and task-following, and (2) reinforcement learning with ~30,000 preference labels to enhance accuracy and informativeness. LEME was evaluated on five curated zero-shot benchmarks spanning tasks such as patient QA, consultation, and treatment planning. It outperformed all seven baselines (all p &lt; 0.004), exceeding GPT-4o by 3.32% (absolute ROUGE-L gain). It was further evaluated on three downstream tasks using deidentified patient data, reviewed by clinicians. In patient QA, LEME received the highest ratings from attending clinicians in 3 out of 4 criteria, with scores of 4.67 for factuality, 4.77 for specificity, 4.79 for completeness, and 4.88 for safety (1-5 scale). Its completeness score surpassed that of expert-written answers (4.79 vs. 4.56; p &#x3D; 0.015). In visual acuity extraction, LEME achieved the highest F1, outperforming LLaMA-3 by 14.1% and Eye-LLaMA by 59.0%. In a pilot evaluation on assessment and treatment planning for diabetic retinopathy, AMD, and glaucoma, LEME received scores of 4.36 for factuality, 4.55 for specificity, 4.42 for completeness, and 4.36 for safety, approaching attending-level performance. All models, data, and code will be released to support further development and clinical translation, laying the groundwork for improved efficiency and patient care </p>
<blockquote>
<p>çœ¼ç–¾çš„æ—¥ç›Šæ™®åŠç»™å…¬å…±å«ç”Ÿå¸¦æ¥äº†è¶Šæ¥è¶Šå¤§çš„è´Ÿæ‹…ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºè§£å†³æ–‡æ¡£å·¥ä½œé‡å¹¶æ”¯æŒä¸´åºŠå†³ç­–åˆ¶å®šæä¾›äº†å‰æ™¯ã€‚ç„¶è€Œï¼Œé’ˆå¯¹çœ¼ç§‘å®šåˆ¶çš„è¯­è¨€æ¨¡å‹å¯¥å¯¥æ— å‡ ï¼Œå¤§å¤šæ•°è¯„ä¼°ä¸»è¦é›†ä¸­åœ¨åŸºäºçŸ¥è¯†çš„é—®ç­”ä¸Šï¼Œç¼ºä¹ä¸´åºŠç›¸å…³çš„åŸºå‡†æµ‹è¯•æˆ–çœŸå®ä¸–ç•ŒéªŒè¯ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†LEMEï¼Œè¿™æ˜¯ä¸€å¥—é€šè¿‡ä¸¤ä¸ªé˜¶æ®µå¼€å‘çš„å¼€æºLLMï¼šé¦–å…ˆæ˜¯åŸºäºä¸´åºŠæŒ‡å—ã€æ•™ç§‘ä¹¦å’Œç—…ä¾‹æŠ¥å‘Šçš„20ä¸‡ä¸ªæ ·æœ¬è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒï¼Œä»¥å¢å¼ºæ¨ç†å’Œä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ï¼›å…¶æ¬¡æ˜¯ä½¿ç”¨çº¦3ä¸‡æ¡åå¥½æ ‡ç­¾è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œä»¥æé«˜å‡†ç¡®æ€§å’Œä¿¡æ¯é‡ã€‚LEMEåœ¨äº”ä¸ªç²¾é€‰çš„é›¶æ ·æœ¬åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶µç›–äº†æ‚£è€…é—®ç­”ã€å’¨è¯¢å’Œæ²»ç–—è®¡åˆ’ç­‰ä»»åŠ¡ã€‚å®ƒè¶…è¶Šäº†æ‰€æœ‰ä¸ƒä¸ªåŸºå‡†ï¼ˆæ‰€æœ‰p &lt; 0.004ï¼‰ï¼Œæ¯”GPT-4oé«˜å‡º3.32%ï¼ˆç»å¯¹ROUGE-Lå¢ç›Šï¼‰ã€‚å®ƒè¿˜åœ¨ä½¿ç”¨åŒ¿åæ‚£è€…æ•°æ®çš„ä¸‰ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šæ¥å—äº†ä¸´åºŠåŒ»ç”Ÿçš„è¯„ä¼°ã€‚åœ¨æ‚£è€…é—®ç­”ç¯èŠ‚ï¼ŒLEMEåœ¨å››é¡¹æ ‡å‡†ä¸­çš„ä¸‰é¡¹è·å¾—äº†ä¸»æ²»åŒ»å¸ˆçš„æœ€é«˜è¯„åˆ†ï¼Œåœ¨äº‹å®æ€§ã€ç‰¹å¼‚æ€§ã€å®Œæ•´æ€§å’Œå®‰å…¨æ€§æ–¹é¢çš„å¾—åˆ†åˆ†åˆ«ä¸º4.67ã€4.77ã€4.79å’Œ4.88ï¼ˆ1-5çº§ï¼‰ã€‚å…¶å®Œæ•´æ€§å¾—åˆ†è¶…è¿‡äº†ä¸“å®¶ç­”æ¡ˆï¼ˆ4.79æ¯”4.56ï¼›p &#x3D; 0.015ï¼‰ã€‚åœ¨è§†åŠ›æå–æ–¹é¢ï¼ŒLEMEçš„F1å¾—åˆ†æœ€é«˜ï¼Œæ¯”LLaMA-3é«˜å‡º14.1%ï¼Œæ¯”Eye-LLaMAé«˜å‡º59.0%ã€‚åœ¨ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ã€AMDå’Œé’å…‰çœ¼è¯„ä¼°å’Œè¯•ç‚¹æ²»ç–—è®¡åˆ’ä¸­ï¼ŒLEMEåœ¨äº‹å®æ€§ã€ç‰¹å¼‚æ€§ã€å®Œæ•´æ€§å’Œå®‰å…¨æ€§æ–¹é¢çš„å¾—åˆ†åˆ†åˆ«ä¸º4.36ã€4.55ã€4.42å’Œ4.36ï¼Œæ¥è¿‘ä¸»æ²»åŒ»å¸ˆæ°´å¹³çš„è¡¨ç°ã€‚æ‰€æœ‰æ¨¡å‹ã€æ•°æ®å’Œä»£ç éƒ½å°†å…¬å¼€å‘å¸ƒï¼Œä»¥æ”¯æŒè¿›ä¸€æ­¥å¼€å‘å’Œä¸´åºŠç¿»è¯‘ï¼Œä¸ºæé«˜æ•ˆç‡å’Œæ‚£è€…æŠ¤ç†å¥ å®šåŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.03740v3">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çœ¼ç–¾é¢†åŸŸå…·æœ‰å·¨å¤§æ½œåŠ›ï¼Œå¯å‡å°‘æ–‡æ¡£å·¥ä½œé‡å¹¶æ”¯æŒä¸´åºŠå†³ç­–ã€‚æœ¬ç ”ç©¶å¼€å‘çš„LEMEæ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒï¼Œåœ¨çœ¼ç–¾ç›¸å…³çš„é—®ç­”ã€å’¨è¯¢å’Œæ²»ç–—è®¡åˆ’ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ï¼Œæ¥è¿‘ç”šè‡³è¶…è¿‡ä¸“å®¶æ°´å¹³ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>çœ¼ç–¾çš„æ™®åŠå¸¦æ¥å…¬å…±å«ç”Ÿè´Ÿæ‹…ï¼ŒLLMå…·æœ‰å‡å°‘æ–‡æ¡£å·¥ä½œé‡åŠæ”¯æŒä¸´åºŠå†³ç­–çš„å·¨å¤§æ½œåŠ›ã€‚</li>
<li>LEMEæ¨¡å‹é€šè¿‡ä¸¤é˜¶æ®µè®­ç»ƒï¼ŒåŒ…æ‹¬åŸºäºä¸´åºŠæŒ‡å—ã€æ•™ç§‘ä¹¦å’Œç—…ä¾‹æŠ¥å‘Šçš„æŒ‡ä»¤è°ƒä¼˜ï¼Œä»¥åŠä½¿ç”¨çº¦30,000ä¸ªåå¥½æ ‡ç­¾çš„å¼ºåŒ–å­¦ä¹ ã€‚</li>
<li>LEMEåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œè¶…è¿‡GPT-4oï¼Œå°¤å…¶æ˜¯åœ¨æ‚£è€…é—®ç­”ã€æ²»ç–—è®¡åˆ’å’Œè§†è§‰æ¸…æ™°åº¦æå–æ–¹é¢ã€‚</li>
<li>åœ¨æ‚£è€…é—®ç­”ä»»åŠ¡ä¸­ï¼ŒLEMEåœ¨äº‹å®æ€§ã€ç‰¹å¼‚æ€§ã€å®Œæ•´æ€§å’Œå®‰å…¨æ€§æ–¹é¢è·å¾—åŒ»ç”Ÿé«˜åº¦è¯„ä»·ã€‚</li>
<li>LEMEåœ¨çœ¼ç–¾è¯„ä¼°å’Œæ²»ç–—è®¡åˆ’ä¸­æ¥è¿‘åŒ»ç”Ÿæ°´å¹³çš„è¡¨ç°ã€‚</li>
<li>æ‰€æœ‰æ¨¡å‹ã€æ•°æ®å’Œä»£ç å°†å…¬å¼€å‘å¸ƒï¼Œæ”¯æŒè¿›ä¸€æ­¥å¼€å‘å’Œä¸´åºŠè½¬åŒ–ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.03740">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bd9ef589e5f61fbf6acabd90496c64bb" align="middle">
<img src="https://picx.zhimg.com/v2-c3ff3ef2d2eb0466e01b828e5298af51" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-11/LLM/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-11/LLM/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-11/Agent/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0cd2d2c7e736cbbb9c6893947f675b53" class="responsive-img" alt="Agent">
                        
                        <span class="card-title">Agent</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Agent æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-11  SWE-Compass Towards Unified Evaluation of Agentic Coding Abilities for   Large Language Models
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-11
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Agent/" class="post-category">
                                    Agent
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Agent/">
                        <span class="chip bg-color">Agent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-11/R1_Reasoning/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0b8e411842c62e6268f50becfdeb0b48" class="responsive-img" alt="R1_Reasoning">
                        
                        <span class="card-title">R1_Reasoning</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            R1_Reasoning æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-11  Visual Spatial Tuning
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-11
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/R1-Reasoning/" class="post-category">
                                    R1_Reasoning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/R1-Reasoning/">
                        <span class="chip bg-color">R1_Reasoning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
