<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="TTS">
    <meta name="description" content="TTS æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>TTS | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-f56b9e6ef23d90258166f86d0b6688f1')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">TTS</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/TTS/">
                                <span class="chip bg-color">TTS</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/TTS/" class="post-category">
                                TTS
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    20k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    81 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-16-æ›´æ–°"><a href="#2025-11-16-æ›´æ–°" class="headerlink" title="2025-11-16 æ›´æ–°"></a>2025-11-16 æ›´æ–°</h1><h2 id="Speech-Audio-Compositional-Attacks-on-Multimodal-LLMs-and-Their-Mitigation-with-SALMONN-Guard"><a href="#Speech-Audio-Compositional-Attacks-on-Multimodal-LLMs-and-Their-Mitigation-with-SALMONN-Guard" class="headerlink" title="Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard"></a>Speech-Audio Compositional Attacks on Multimodal LLMs and Their Mitigation with SALMONN-Guard</h2><p><strong>Authors:Yudong Yang, Xuezhen Zhang, Zhifeng Han, Siyin Wang, Jimin Zhuang, Zengrui Jin, Jing Shao, Guangzhi Sun, Chao Zhang</strong></p>
<p>Recent progress in large language models (LLMs) has enabled understanding of both speech and non-speech audio, but exposing new safety risks emerging from complex audio inputs that are inadequately handled by current safeguards. We introduce SACRED-Bench (Speech-Audio Composition for RED-teaming) to evaluate the robustness of LLMs under complex audio-based attacks. Unlike existing perturbation-based methods that rely on noise optimization or white-box access, SACRED-Bench exploits speech-audio composition mechanisms. SACRED-Bench adopts three mechanisms: (a) speech overlap and multi-speaker dialogue, which embeds harmful prompts beneath or alongside benign speech; (b) speech-audio mixture, which imply unsafe intent via non-speech audio alongside benign speech or audio; and (c) diverse spoken instruction formats (open-ended QA, yes&#x2F;no) that evade text-only filters. Experiments show that, even Gemini 2.5 Pro, the state-of-the-art proprietary LLM, still exhibits 66% attack success rate in SACRED-Bench test set, exposing vulnerabilities under cross-modal, speech-audio composition attacks. To bridge this gap, we propose SALMONN-Guard, a safeguard LLM that jointly inspects speech, audio, and text for safety judgments, reducing attack success down to 20%. Our results highlight the need for audio-aware defenses for the safety of multimodal LLMs. The benchmark and SALMONN-Guard checkpoints can be found at <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench">https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench</a>. Warning: this paper includes examples that may be offensive or harmful.</p>
<blockquote>
<p>è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ä½¿å¾—ç†è§£å’Œå¤„ç†è¯­éŸ³å’Œéè¯­éŸ³éŸ³é¢‘æˆä¸ºå¯èƒ½ï¼Œä½†åŒæ—¶ä¹Ÿæš´éœ²å‡ºå½“å‰ä¿éšœæªæ–½å¯¹å¤æ‚éŸ³é¢‘è¾“å…¥å¤„ç†ä¸å½“æ‰€å¸¦æ¥çš„æ–°å®‰å…¨é£é™©ã€‚æˆ‘ä»¬æ¨å‡ºSACRED-Benchï¼ˆç”¨äºçº¢é˜Ÿå›¢é˜Ÿçš„è¯­éŸ³éŸ³é¢‘ç»„åˆè¯„ä¼°ï¼‰ï¼Œä»¥è¯„ä¼°å¤æ‚éŸ³é¢‘æ”»å‡»ä¸‹LLMçš„ç¨³å¥æ€§ã€‚ä¸åŒäºç°æœ‰çš„åŸºäºæ‰°åŠ¨çš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ä¾èµ–äºå™ªå£°ä¼˜åŒ–æˆ–ç™½ç›’è®¿é—®ï¼ŒSACRED-Benchåˆ©ç”¨è¯­éŸ³éŸ³é¢‘ç»„åˆæœºåˆ¶ã€‚SACRED-Benché‡‡ç”¨ä¸‰ç§æœºåˆ¶ï¼šï¼ˆaï¼‰è¯­éŸ³é‡å å’Œå¤šäººå¯¹è¯ï¼Œåœ¨è‰¯æ€§è¯­éŸ³ä¸‹æ–¹æˆ–æ—è¾¹åµŒå…¥æœ‰å®³æç¤ºï¼›ï¼ˆbï¼‰è¯­éŸ³éŸ³é¢‘æ··åˆï¼Œé€šè¿‡éè¯­éŸ³éŸ³é¢‘å’Œè‰¯æ€§è¯­éŸ³æˆ–éŸ³é¢‘æš—ç¤ºä¸å®‰å…¨æ„å›¾ï¼›ï¼ˆcï¼‰å¤šæ ·çš„å£è¯­æŒ‡ä»¤æ ¼å¼ï¼ˆå¼€æ”¾å¼é—®ç­”ã€æ˜¯éé¢˜ï¼‰ï¼Œä»¥é¿å¼€ä»…æ–‡æœ¬è¿‡æ»¤å™¨ã€‚å®éªŒè¡¨æ˜ï¼Œå³ä½¿æ˜¯ç›®å‰æœ€å…ˆè¿›çš„ä¸“æœ‰LLMâ€”â€”Gemini 2.5 Proï¼Œåœ¨SACRED-Benchæµ‹è¯•é›†ä¸­æ”»å‡»æˆåŠŸç‡ä»é«˜è¾¾66%ï¼Œæ˜¾ç¤ºå‡ºåœ¨è·¨æ¨¡æ€è¯­éŸ³éŸ³é¢‘ç»„åˆæ”»å‡»ä¸‹çš„æ¼æ´ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€å·®è·ï¼Œæˆ‘ä»¬æå‡ºSALMONN-Guardï¼Œä¸€ç§å®‰å…¨ä¿æŠ¤LLMï¼Œå®ƒè”åˆæ£€æŸ¥è¯­éŸ³ã€éŸ³é¢‘å’Œæ–‡æœ¬ä»¥è¿›è¡Œå®‰å…¨åˆ¤æ–­ï¼Œå°†æ”»å‡»æˆåŠŸç‡é™ä½è‡³20%ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœå¼ºè°ƒäº†ä¸ºå¤šåª’ä½“LLMçš„å®‰å…¨æ„è¯†é˜²å¾¡çš„éœ€æ±‚ã€‚è¯¥åŸºå‡†æµ‹è¯•å’ŒSALMONN-Guardæ£€æŸ¥ç‚¹å¯åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench">https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench</a> æ‰¾åˆ°ã€‚è­¦å‘Šï¼šæœ¬è®ºæ–‡åŒ…å«å¯èƒ½å…·æœ‰å†’çŠ¯æ€§æˆ–æœ‰å®³çš„ç¤ºä¾‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10222v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥ï¼Œå¯¹è¯­éŸ³å’Œéè¯­éŸ³éŸ³é¢‘çš„ç†è§£èƒ½åŠ›å¾—åˆ°æå‡ï¼Œä½†åŒæ—¶ä¹Ÿæš´éœ²å‡ºç”±å¤æ‚éŸ³é¢‘è¾“å…¥å¸¦æ¥çš„æ–°å®‰å…¨å¨èƒã€‚æˆ‘ä»¬æ¨å‡ºSACRED-BenchåŸºå‡†æµ‹è¯•å¹³å°ï¼Œç”¨ä»¥è¯„ä¼°LLMåœ¨å¤æ‚éŸ³é¢‘æ”»å‡»ä¸‹çš„ç¨³å¥æ€§ã€‚SACRED-Benché‡‡ç”¨ä¸‰ç§æœºåˆ¶ï¼šè¯­éŸ³é‡å å’Œå¤šäººå¯¹è¯ã€è¯­éŸ³éŸ³é¢‘æ··åˆä»¥åŠå¤šæ ·çš„å£è¯­æŒ‡ä»¤æ ¼å¼ï¼Œèƒ½å¤Ÿåœ¨è‰¯æ€§è¯­éŸ³æˆ–éŸ³é¢‘ä¸­éšå«ä¸å®‰å…¨æ„å›¾ã€‚å®éªŒæ˜¾ç¤ºï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„å¼€æºLLMâ€”â€”Gemini 2.5 Proï¼Œåœ¨SACRED-Benchæµ‹è¯•é›†ä¸Šçš„æ”»å‡»æˆåŠŸç‡ä»é«˜è¾¾66%ï¼Œå‡¸æ˜¾å‡ºåœ¨è·¨æ¨¡æ€ã€è¯­éŸ³éŸ³é¢‘ç»„åˆæ”»å‡»ä¸‹çš„æ¼æ´ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºSALMONN-Guardå®‰å…¨ä¿æŠ¤æ–¹æ¡ˆï¼Œé€šè¿‡è”åˆæ£€æŸ¥è¯­éŸ³ã€éŸ³é¢‘å’Œæ–‡æœ¬è¿›è¡Œå®‰å…¨åˆ¤æ–­ï¼ŒæˆåŠŸå°†æ”»å‡»ç‡é™è‡³20%ã€‚ç»“æœå¼ºè°ƒäº†å¯¹å¤šæ¨¡å¼LLMçš„éŸ³é¢‘æ„ŸçŸ¥é˜²å¾¡éœ€æ±‚ã€‚SACRED-BenchåŸºå‡†æµ‹è¯•å’ŒSALMONN-Guardæ£€æŸ¥ç‚¹å¯åœ¨é“¾æ¥ä¸­æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench">https://huggingface.co/datasets/tsinghua-ee/SACRED-Bench</a>ã€‚è¯·æ³¨æ„ï¼Œæœ¬æ–‡åŒ…å«å¯èƒ½å…·æœ‰å†’çŠ¯æ€§æˆ–æœ‰å®³æ€§çš„ç¤ºä¾‹ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¤Ÿç†è§£è¯­éŸ³å’Œéè¯­éŸ³éŸ³é¢‘ï¼Œä½†é¢ä¸´æ–°çš„å®‰å…¨å¨èƒã€‚</li>
<li>SACRED-Benché‡‡ç”¨ç‹¬ç‰¹çš„æœºåˆ¶è¯„ä¼°LLMåœ¨å¤æ‚éŸ³é¢‘æ”»å‡»ä¸‹çš„ç¨³å¥æ€§ã€‚</li>
<li>SACRED-Benché€šè¿‡è¯­éŸ³é‡å ã€è¯­éŸ³éŸ³é¢‘æ··åˆå’Œå¤šæ ·çš„å£è¯­æŒ‡ä»¤æ ¼å¼æ¥æ¨¡æ‹Ÿæ½œåœ¨çš„å®‰å…¨å¨èƒã€‚</li>
<li>æœ€å…ˆè¿›çš„LLMåœ¨SACRED-Benchæµ‹è¯•é›†ä¸Šçš„æ”»å‡»æˆåŠŸç‡é«˜è¾¾66%ï¼Œæ˜¾ç¤ºå­˜åœ¨æ˜¾è‘—æ¼æ´ã€‚</li>
<li>SALMONN-Guardä½œä¸ºå®‰å…¨ä¿æŠ¤æ–¹æ¡ˆï¼Œé€šè¿‡è”åˆæ£€æŸ¥è¯­éŸ³ã€éŸ³é¢‘å’Œæ–‡æœ¬è¿›è¡Œå®‰å…¨åˆ¤æ–­ï¼ŒæˆåŠŸé™ä½æ”»å‡»ç‡è‡³20%ã€‚</li>
<li>ç»“æœå¼ºè°ƒäº†å¯¹å¤šæ¨¡å¼LLMçš„éŸ³é¢‘æ„ŸçŸ¥é˜²å¾¡éœ€æ±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10222">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eec535246af685e907211e048b9a24e8" align="middle">
<img src="https://picx.zhimg.com/v2-6639799ebe64da9d7aecf974300000fa" align="middle">
<img src="https://picx.zhimg.com/v2-44e23b1b9e1e925cd24c70b552902557" align="middle">
<img src="https://picx.zhimg.com/v2-2a9120299be6e2c693cf76d5dda63146" align="middle">
<img src="https://picx.zhimg.com/v2-b497c205ce19b296f6ab26f0faad2bbd" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="FabasedVC-Enhancing-Voice-Conversion-with-Text-Modality-Fusion-and-Phoneme-Level-SSL-Features"><a href="#FabasedVC-Enhancing-Voice-Conversion-with-Text-Modality-Fusion-and-Phoneme-Level-SSL-Features" class="headerlink" title="FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features"></a>FabasedVC: Enhancing Voice Conversion with Text Modality Fusion and Phoneme-Level SSL Features</h2><p><strong>Authors:Wenyu Wang, Zhetao Hu, Yiquan Zhou, Jiacheng Xu, Zhiyu Wu, Chen Li, Shihao Li</strong></p>
<p>In voice conversion (VC), it is crucial to preserve complete semantic information while accurately modeling the target speakerâ€™s timbre and prosody. This paper proposes FabasedVC to achieve VC with enhanced similarity in timbre, prosody, and duration to the target speaker, as well as improved content integrity. It is an end-to-end VITS-based VC system that integrates relevant textual modality information, phoneme-level self-supervised learning (SSL) features, and a duration predictor. Specifically, we employ a text feature encoder to encode attributes such as text, phonemes, tones and BERT features. We then process the frame-level SSL features into phoneme-level features using two methods: average pooling and attention mechanism based on each phonemeâ€™s duration. Moreover, a duration predictor is incorporated to better align the speech rate and prosody of the target speaker. Experimental results demonstrate that our method outperforms competing systems in terms of naturalness, similarity, and content integrity.</p>
<blockquote>
<p>åœ¨è¯­éŸ³è½¬æ¢ï¼ˆVCï¼‰ä¸­ï¼Œä¿ç•™å®Œæ•´çš„è¯­ä¹‰ä¿¡æ¯ï¼ŒåŒæ—¶å‡†ç¡®å»ºæ¨¡ç›®æ ‡è¯´è¯è€…çš„éŸ³è‰²å’Œè¯­è°ƒè‡³å…³é‡è¦ã€‚æœ¬æ–‡æå‡ºäº†FabasedVCï¼Œä»¥å®ç°VCï¼Œåœ¨éŸ³è‰²ã€è¯­è°ƒå’ŒæŒç»­æ—¶é—´æ–¹é¢ä¸ç›®æ ‡è¯´è¯è€…å…·æœ‰æ›´é«˜çš„ç›¸ä¼¼æ€§ï¼Œå¹¶æé«˜å†…å®¹å®Œæ•´æ€§ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäºVITSç«¯åˆ°ç«¯çš„VCç³»ç»Ÿï¼Œå®ƒé›†æˆäº†ç›¸å…³çš„æ–‡æœ¬æ¨¡å¼ä¿¡æ¯ã€éŸ³ç´ çº§åˆ«çš„è‡ªç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰ç‰¹å¾å’ŒæŒç»­æ—¶é—´é¢„æµ‹å™¨ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨æ–‡æœ¬ç‰¹å¾ç¼–ç å™¨å¯¹æ–‡æœ¬ã€éŸ³ç´ ã€éŸ³è°ƒå’ŒBERTç‰¹å¾ç­‰è¿›è¡Œç¼–ç ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡ä¸¤ç§æ–¹æ³•å°†å¸§çº§åˆ«çš„SSLç‰¹å¾å¤„ç†ä¸ºéŸ³ç´ çº§åˆ«çš„ç‰¹å¾ï¼šå¹³å‡æ± åŒ–å’ŒåŸºäºæ¯ä¸ªéŸ³ç´ çš„æŒç»­æ—¶é—´çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚æ­¤å¤–ï¼Œè¿˜ç»“åˆäº†æŒç»­æ—¶é—´é¢„æµ‹å™¨ï¼Œä»¥æ›´å¥½åœ°å¯¹é½ç›®æ ‡è¯´è¯è€…çš„è¯­é€Ÿå’Œè¯­è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è‡ªç„¶åº¦ã€ç›¸ä¼¼åº¦å’Œå†…å®¹å®Œæ•´æ€§æ–¹é¢ä¼˜äºç«äº‰ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10112v1">PDF</a> Accepted by ACMMM-Asia 2025</p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºFabasedVCçš„è¯­éŸ³è½¬æ¢ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿåœ¨è¿›è¡Œè¯­éŸ³è½¬æ¢æ—¶å¢å¼ºç›®æ ‡è¯´è¯äººçš„éŸ³è‰²ã€è¯­è°ƒåŠæ—¶é•¿çš„ç›¸ä¼¼æ€§ï¼Œå¹¶æ”¹å–„å†…å®¹å®Œæ•´æ€§ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäºVITSçš„ç«¯åˆ°ç«¯ç³»ç»Ÿï¼Œé›†æˆäº†æ–‡æœ¬æ¨¡æ€ä¿¡æ¯ã€åŸºäºéŸ³ç´ çº§åˆ«çš„è‡ªç›‘ç£å­¦ä¹ ç‰¹å¾å’Œæ—¶é•¿é¢„æµ‹å™¨ã€‚æˆ‘ä»¬åˆ©ç”¨æ–‡æœ¬ç‰¹å¾ç¼–ç å™¨ç¼–ç æ–‡æœ¬ã€éŸ³ç´ ã€éŸ³è°ƒä»¥åŠBERTç‰¹å¾ç­‰å±æ€§ã€‚éšåï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸¤ç§å°†å¸§çº§åˆ«çš„SSLç‰¹å¾è½¬æ¢ä¸ºéŸ³ç´ çº§åˆ«ç‰¹å¾çš„æ–¹æ³•ï¼šå¹³å‡æ± åŒ–å’ŒåŸºäºæ¯ä¸ªéŸ³ç´ æ—¶é•¿çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åŠ å…¥äº†ä¸€ä¸ªæ—¶é•¿é¢„æµ‹å™¨ä»¥æ›´å¥½åœ°åŒ¹é…ç›®æ ‡è¯´è¯äººçš„è¯­é€Ÿå’Œè¯­è°ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è‡ªç„¶åº¦ã€ç›¸ä¼¼æ€§å’Œå†…å®¹å®Œæ•´æ€§æ–¹é¢å‡ä¼˜äºå…¶ä»–ç³»ç»Ÿã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>FabasedVCæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„è¯­éŸ³è½¬æ¢ç³»ç»Ÿï¼Œæ—¨åœ¨å¢å¼ºç›®æ ‡è¯´è¯äººçš„éŸ³è‰²ã€è¯­è°ƒåŠæ—¶é•¿çš„ç›¸ä¼¼æ€§ï¼Œå¹¶æ”¹å–„å†…å®¹å®Œæ•´æ€§ã€‚</li>
<li>ç³»ç»Ÿé›†æˆäº†æ–‡æœ¬æ¨¡æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€éŸ³ç´ ã€éŸ³è°ƒå’ŒBERTç‰¹å¾ç­‰ã€‚</li>
<li>é‡‡ç”¨ä¸¤ç§å°†å¸§çº§åˆ«çš„è‡ªç›‘ç£å­¦ä¹ ç‰¹å¾è½¬æ¢ä¸ºéŸ³ç´ çº§åˆ«ç‰¹å¾çš„æ–¹æ³•ï¼šå¹³å‡æ± åŒ–å’Œæ³¨æ„åŠ›æœºåˆ¶ã€‚</li>
<li>æ³¨æ„åŠ›æœºåˆ¶æ˜¯åŸºäºæ¯ä¸ªéŸ³ç´ æ—¶é•¿çš„ï¼Œä»¥æ›´å‡†ç¡®åœ°æ•æ‰è¯­éŸ³ç‰¹å¾ã€‚</li>
<li>ç³»ç»Ÿä¸­åŠ å…¥äº†ä¸€ä¸ªæ—¶é•¿é¢„æµ‹å™¨ï¼Œä»¥åŒ¹é…ç›®æ ‡è¯´è¯äººçš„è¯­é€Ÿå’Œè¯­è°ƒã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼ŒFabasedVCåœ¨è‡ªç„¶åº¦ã€ç›¸ä¼¼æ€§å’Œå†…å®¹å®Œæ•´æ€§æ–¹é¢è¾ƒå…¶ä»–ç³»ç»Ÿæœ‰ä¼˜å¼‚è¡¨ç°ã€‚</li>
<li>è¯¥ç³»ç»Ÿä¸ºè¯­éŸ³è½¬æ¢é¢†åŸŸæä¾›äº†ä¸€ç§æ–°çš„æœ‰æ•ˆæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10112">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c2de3d24aef4720261c7b71542544755" align="middle">
<img src="https://picx.zhimg.com/v2-b77858c17ddefa043e67aa70af0323ff" align="middle">
<img src="https://picx.zhimg.com/v2-3aeb3fd1d8da0ac8e817dbf06b99edb9" align="middle">
<img src="https://picx.zhimg.com/v2-e8f3ee5b07a0395f1e77fffcfb36c8f4" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="Time-Layer-Adaptive-Alignment-for-Speaker-Similarity-in-Flow-Matching-Based-Zero-Shot-TTS"><a href="#Time-Layer-Adaptive-Alignment-for-Speaker-Similarity-in-Flow-Matching-Based-Zero-Shot-TTS" class="headerlink" title="Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS"></a>Time-Layer Adaptive Alignment for Speaker Similarity in Flow-Matching Based Zero-Shot TTS</h2><p><strong>Authors:Haoyu Li, Mingyang Han, Yu Xi, Dongxiao Wang, Hankun Wang, Haoxiang Shi, Boyu Li, Jun Song, Bo Zheng, Shuai Wang</strong></p>
<p>Flow-Matching (FM)-based zero-shot text-to-speech (TTS) systems exhibit high-quality speech synthesis and robust generalization capabilities. However, the speaker representation ability of such systems remains underexplored, primarily due to the lack of explicit speaker-specific supervision in the FM framework. To this end, we conduct an empirical analysis of speaker information distribution and reveal its non-uniform allocation across time steps and network layers, underscoring the need for adaptive speaker alignment. Accordingly, we propose Time-Layer Adaptive Speaker Alignment (TLA-SA), a loss that enhances speaker consistency by jointly leveraging temporal and hierarchical variations in speaker information. Experimental results show that TLA-SA significantly improves speaker similarity compared to baseline systems on both research- and industrial-scale datasets and generalizes effectively across diverse model architectures, including decoder-only language models (LM) and FM-based TTS systems free of LM.</p>
<blockquote>
<p>åŸºäºæµåŒ¹é…ï¼ˆFMï¼‰çš„é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿå±•ç°å‡ºé«˜è´¨é‡çš„è¯­éŸ³åˆæˆå’Œç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œæ­¤ç±»ç³»ç»Ÿçš„è¯´è¯äººè¡¨ç¤ºèƒ½åŠ›ä»ç„¶æœªè¢«å……åˆ†æ¢ç´¢ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºFMæ¡†æ¶ä¸­ç¼ºä¹æ˜ç¡®çš„è¯´è¯äººç‰¹å®šç›‘ç£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹è¯´è¯äººä¿¡æ¯åˆ†å¸ƒè¿›è¡Œäº†å®è¯åˆ†æï¼Œå¹¶æ­ç¤ºäº†å…¶åœ¨æ—¶é—´æ­¥å’Œç½‘ç»œå±‚ä¹‹é—´çš„éå‡åŒ€åˆ†é…ï¼Œè¿™å¼ºè°ƒäº†è‡ªé€‚åº”è¯´è¯äººå¯¹é½çš„éœ€è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶é—´å±‚è‡ªé€‚åº”è¯´è¯äººå¯¹é½ï¼ˆTLA-SAï¼‰æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æŸå¤±å‡½æ•°ï¼Œå®ƒé€šè¿‡è”åˆåˆ©ç”¨è¯´è¯äººä¿¡æ¯ä¸­çš„æ—¶é—´å’Œå±‚æ¬¡å˜åŒ–ï¼Œæé«˜è¯´è¯äººçš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºå‡†ç³»ç»Ÿç›¸æ¯”ï¼ŒTLA-SAåœ¨ç ”ç©¶å’Œå·¥ä¸šè§„æ¨¡çš„æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜è¯´è¯äººç›¸ä¼¼æ€§ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„æ¨¡å‹æ¶æ„ä¸­éƒ½èƒ½æœ‰æ•ˆæ³›åŒ–ï¼ŒåŒ…æ‹¬ä»…è§£ç å™¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰å’ŒåŸºäºFMçš„TTSç³»ç»Ÿç­‰è„±ç¦»LMçš„ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09995v1">PDF</a> Submitted to ICASSP 2026</p>
<p><strong>æ€»ç»“</strong></p>
<p>åŸºäºæµåŒ¹é…ï¼ˆFMï¼‰çš„é›¶æ ·æœ¬æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿå±•ç°å‡ºé«˜è´¨é‡çš„è¯­éŸ³åˆæˆå’Œç¨³å¥çš„æ³›åŒ–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¯¥ç³»ç»Ÿçš„è¯´è¯äººè¡¨å¾èƒ½åŠ›å°šæœªå¾—åˆ°å……åˆ†æ¢ç´¢ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºFMæ¡†æ¶ä¸­ç¼ºä¹æ˜ç¡®çš„è¯´è¯äººç‰¹å®šç›‘ç£ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å®è¯çš„è¯´è¯äººä¿¡æ¯åˆ†å¸ƒåˆ†æï¼Œæ­ç¤ºäº†å…¶åœ¨æ—¶é—´æ­¥å’Œç½‘ç»œå±‚ä¹‹é—´çš„éå‡åŒ€åˆ†é…ï¼Œå¼ºè°ƒäº†è‡ªé€‚åº”è¯´è¯äººå¯¹é½çš„å¿…è¦æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†æ—¶é—´å±‚è‡ªé€‚åº”è¯´è¯äººå¯¹é½ï¼ˆTLA-SAï¼‰æŸå¤±å‡½æ•°ï¼Œé€šè¿‡è”åˆåˆ©ç”¨è¯´è¯äººä¿¡æ¯çš„æ—¶æ€å’Œå±‚æ¬¡å˜åŒ–ï¼Œæé«˜è¯´è¯äººä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åŸºçº¿ç³»ç»Ÿç›¸æ¯”ï¼ŒTLA-SAåœ¨ç ”ç©¶å’Œå·¥ä¸šè§„æ¨¡æ•°æ®é›†ä¸Šçš„è¯´è¯äººç›¸ä¼¼æ€§æ˜¾è‘—æé«˜ï¼Œä¸”åœ¨å¤šç§æ¨¡å‹æ¶æ„ä¸­è¡¨ç°æœ‰æ•ˆï¼ŒåŒ…æ‹¬æ— è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„è§£ç å™¨ä»…è¯­è¨€æ¨¡å‹å’ŒFM-based TTSç³»ç»Ÿã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>åŸºäºæµåŒ¹é…ï¼ˆFMï¼‰çš„TTSç³»ç»Ÿè™½å…·æœ‰é«˜è´¨é‡çš„è¯­éŸ³åˆæˆå’Œæ³›åŒ–èƒ½åŠ›ï¼Œä½†è¯´è¯äººè¡¨å¾èƒ½åŠ›å°šæœªå……åˆ†æ¢ç´¢ã€‚</li>
<li>è¯´è¯äººä¿¡æ¯åœ¨æ—¶é—´å’Œç½‘ç»œå±‚ä¸Šçš„åˆ†å¸ƒä¸å‡ï¼Œéœ€è¦è‡ªé€‚åº”çš„è¯´è¯äººå¯¹é½ã€‚</li>
<li>æå‡ºäº†æ—¶é—´å±‚è‡ªé€‚åº”è¯´è¯äººå¯¹é½ï¼ˆTLA-SAï¼‰æŸå¤±å‡½æ•°ï¼Œè¯¥æŸå¤±å‡½æ•°é€šè¿‡ç»“åˆæ—¶ç©ºå’Œå±‚æ¬¡å˜åŒ–æ¥æé«˜è¯´è¯äººä¸€è‡´æ€§ã€‚</li>
<li>TLA-SAåœ¨ç ”ç©¶å’Œå·¥ä¸šè§„æ¨¡æ•°æ®é›†ä¸Šæ˜¾è‘—æé«˜è¯´è¯äººç›¸ä¼¼æ€§ã€‚</li>
<li>TLA-SAåœ¨å„ç§æ¨¡å‹æ¶æ„ä¸­è¡¨ç°æœ‰æ•ˆï¼ŒåŒ…æ‹¬æ— è¯­è¨€æ¨¡å‹çš„è§£ç å™¨åŠFM-based TTSç³»ç»Ÿã€‚</li>
<li>è¯¥æ–¹æ³•å¼ºè°ƒäº†è‡ªé€‚åº”è¯´è¯äººå¯¹é½åœ¨æå‡TTSç³»ç»Ÿè¯´è¯äººè¡¨å¾èƒ½åŠ›ä¸­çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09995">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-96c72907b7fe2d0e0b0c4c91ff1f316f" align="middle">
<img src="https://picx.zhimg.com/v2-86b0a5cfa55d9cff9b610d5fa8a7358b" align="middle">
<img src="https://picx.zhimg.com/v2-e60de9e2763a108fd4ccb095d805266f" align="middle">
<img src="https://picx.zhimg.com/v2-bdbd3363956bc0cb456d9dc48f640a8b" align="middle">
<img src="https://picx.zhimg.com/v2-63f925c0a59753cc4792d7f163b7c478" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="POTSA-A-Cross-Lingual-Speech-Alignment-Framework-for-Low-Resource-Speech-to-Text-Translation"><a href="#POTSA-A-Cross-Lingual-Speech-Alignment-Framework-for-Low-Resource-Speech-to-Text-Translation" class="headerlink" title="POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation"></a>POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation</h2><p><strong>Authors:Xuanchen Li, Chenrui Cui, Tianrui Wang, Meng Ge, Zikang Huang, Jin Li, Yizhou Peng, Longbiao Wang, Jianwu Dang, Nyima Tashi</strong></p>
<p>Speech Large Language Models (SpeechLLMs) have achieved breakthroughs in multilingual speech-to-text translation (S2TT). However, existing approaches often overlook semantic commonalities across source languages, leading to biased translation performance. In this work, we propose \textbf{POTSA} (Parallel Optimal Transport for Speech Alignment), a new framework based on cross-lingual parallel speech pairs and Optimal Transport (OT), designed to bridge high- and low-resource translation gaps. First, we introduce a Bias Compensation module to coarsely align initial speech representations across languages. Second, we impose token-level OT constraints on a Q-Former using parallel speech pairs to establish fine-grained consistency of representations. Then, we apply a layer scheduling strategy to focus OT constraints on the most semantically beneficial layers. Experiments on the FLEURS dataset show that our method achieves SOTA performance, with +0.93 BLEU on average over five common languages and +5.05 BLEU on zero-shot languages, using only 10 hours of parallel speech per source language.</p>
<blockquote>
<p>è¯­éŸ³å¤§è¯­è¨€æ¨¡å‹ï¼ˆSpeechLLMsï¼‰åœ¨å¤šè¯­ç§è¯­éŸ³åˆ°æ–‡æœ¬çš„ç¿»è¯‘ï¼ˆS2TTï¼‰æ–¹é¢å–å¾—äº†çªç ´ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å¿½è§†äº†æºè¯­è¨€ä¹‹é—´çš„è¯­ä¹‰å…±æ€§ï¼Œå¯¼è‡´ç¿»è¯‘æ€§èƒ½å­˜åœ¨åè§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºè·¨è¯­è¨€å¹³è¡Œè¯­éŸ³å¯¹å’Œæœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰çš„<strong>POTSA</strong>ï¼ˆå¹¶è¡Œæœ€ä¼˜ä¼ è¾“è¯­éŸ³å¯¹é½ï¼‰æ–°æ¡†æ¶ï¼Œæ—¨åœ¨å¼¥é«˜èµ„æºå’Œä½èµ„æºç¿»è¯‘ä¹‹é—´çš„å·®è·ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåè§è¡¥å¿æ¨¡å—ï¼Œç²—ç•¥åœ°å¯¹è·¨è¯­è¨€çš„åˆå§‹è¯­éŸ³è¡¨ç¤ºè¿›è¡Œå¯¹é½ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¯¹ä½¿ç”¨å¹³è¡Œè¯­éŸ³å¯¹çš„Q-Formeræ–½åŠ tokençº§åˆ«çš„OTçº¦æŸï¼Œä»¥å»ºç«‹è¡¨ç¤ºçš„ç²¾ç²’åº¦ä¸€è‡´æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨å±‚è°ƒåº¦ç­–ç•¥ï¼Œå°†OTçº¦æŸé›†ä¸­åœ¨è¯­ä¹‰ä¸Šæœ€æœ‰ç›Šçš„å›¾å±‚ä¸Šã€‚åœ¨FLEURSæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œåœ¨äº”ç§å¸¸è§è¯­è¨€ä¸Šå¹³å‡æé«˜äº†+0.93ä¸ªBLEUåˆ†ï¼Œåœ¨é›¶æ ·æœ¬è¯­è¨€ä¸Šæé«˜äº†+5.05ä¸ªBLEUåˆ†ï¼Œä¸”ä»…ä½¿ç”¨æ¯æºè¯­è¨€10å°æ—¶çš„å¹³è¡Œè¯­éŸ³æ•°æ®ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09232v1">PDF</a> 5 pages, 3 figures, submitted to ICASSP 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹å¤šè¯­è¨€è¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘ï¼ˆS2TTï¼‰é¢†åŸŸå­˜åœ¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè·¨è¯­è¨€å¹³è¡Œè¯­éŸ³å¯¹å’Œæœ€ä¼˜ä¼ è¾“ï¼ˆOTï¼‰çš„æ–°æ¡†æ¶POTSAã€‚è¯¥æ¡†æ¶æ—¨åœ¨è§£å†³é«˜èµ„æºå’Œä½èµ„æºç¿»è¯‘ä¹‹é—´çš„å·®è·ï¼Œé€šè¿‡å¼•å…¥åå·®è¡¥å¿æ¨¡å—è¿›è¡Œåˆæ­¥è¯­éŸ³è¡¨ç¤ºå¯¹é½ï¼Œåœ¨Q-Formerä¸Šæ–½åŠ åŸºäºå¹³è¡Œè¯­éŸ³å¯¹çš„æ ‡è®°çº§OTçº¦æŸï¼Œå¹¶å»ºç«‹ç²¾ç»†è¡¨ç¤ºä¸€è‡´æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨FLEURSæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›æ€§èƒ½ï¼Œäº”ç§å¸¸è§è¯­è¨€çš„BLEUå¾—åˆ†å¹³å‡æé«˜+0.93ï¼Œé›¶é•œå¤´è¯­è¨€æé«˜+5.05 BLEUã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Speech Large Language Models (SpeechLLMs) åœ¨å¤šè¯­è¨€è¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘ (S2TT) å–å¾—çªç ´ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¿½ç•¥æºè¯­è¨€é—´çš„è¯­ä¹‰å…±æ€§ï¼Œå¯¼è‡´ç¿»è¯‘æ€§èƒ½åå·®ã€‚</li>
<li>æå‡ºçš„ POTSA æ¡†æ¶åŸºäºè·¨è¯­è¨€å¹³è¡Œè¯­éŸ³å¯¹å’Œæœ€ä¼˜ä¼ è¾“ (OT)ï¼Œæ—¨åœ¨ç¼©å°é«˜ã€ä½èµ„æºç¿»è¯‘å·®è·ã€‚</li>
<li>POTSA å¼•å…¥åå·®è¡¥å¿æ¨¡å—è¿›è¡Œåˆæ­¥è¯­éŸ³è¡¨ç¤ºå¯¹é½ã€‚</li>
<li>ä½¿ç”¨å¹³è¡Œè¯­éŸ³å¯¹åœ¨ Q-Former ä¸Šæ–½åŠ æ ‡è®°çº§ OT çº¦æŸï¼Œå®ç°ç²¾ç»†è¡¨ç¤ºä¸€è‡´æ€§ã€‚</li>
<li>é‡‡ç”¨å±‚è°ƒåº¦ç­–ç•¥ï¼Œå°† OT çº¦æŸé›†ä¸­åœ¨è¯­ä¹‰ä¸Šæœ€æœ‰ç›Šçš„å›¾å±‚ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09232">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-082f9e928e182446f917d09f855c9e2b" align="middle">
<img src="https://picx.zhimg.com/v2-3dd852fedc20771d6d9622b463ed37ef" align="middle">
<img src="https://picx.zhimg.com/v2-8d11242ac64b01f9bf858960e7c7b93c" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="ParliaBench-An-Evaluation-and-Benchmarking-Framework-for-LLM-Generated-Parliamentary-Speech"><a href="#ParliaBench-An-Evaluation-and-Benchmarking-Framework-for-LLM-Generated-Parliamentary-Speech" class="headerlink" title="ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech"></a>ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech</h2><p><strong>Authors:Marios Koniaris, Argyro Tsipi, Panayiotis Tsanakas</strong></p>
<p>Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them using our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.</p>
<blockquote>
<p>è®®ä¼šæ¼”è®²ç”Ÿæˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹æå‡ºäº†è¶…è¶Šæ ‡å‡†æ–‡æœ¬ç”Ÿæˆä»»åŠ¡çš„ç‰¹å®šæŒ‘æˆ˜ã€‚ä¸åŒäºä¸€èˆ¬çš„æ–‡æœ¬ç”Ÿæˆï¼Œè®®ä¼šæ¼”è®²ä¸ä»…éœ€è¦è¯­è¨€è´¨é‡ï¼Œè¿˜éœ€è¦æ”¿æ²»çœŸå®æ€§å’Œæ€æƒ³ä¸€è‡´æ€§ã€‚å½“å‰çš„è¯­è¨€æ¨¡å‹ç¼ºä¹é’ˆå¯¹è®®ä¼šæƒ…å¢ƒçš„ä¸“é—¨è®­ç»ƒï¼Œç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¾§é‡äºæ ‡å‡†NLPæŒ‡æ ‡ï¼Œè€Œéæ”¿æ²»çœŸå®æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ParliaBenchï¼Œä¸€ä¸ªç”¨äºè®®ä¼šæ¼”è®²ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬æ„å»ºäº†è‹±å›½è®®ä¼šæ¼”è®²æ•°æ®é›†ï¼Œä»¥è¿›è¡Œç³»ç»ŸåŒ–çš„æ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œç»“åˆè®¡ç®—æŒ‡æ ‡å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ³•å®˜çš„è¯„ä¼°ï¼Œæ¥è¡¡é‡ä¸‰ä¸ªç»´åº¦ä¸Šçš„ç”Ÿæˆè´¨é‡ï¼šè¯­è¨€è´¨é‡ã€è¯­ä¹‰è¿è´¯æ€§å’Œæ”¿æ²»çœŸå®æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§åŸºäºåµŒå…¥çš„æ–°æŒ‡æ ‡ï¼Œå³æ”¿æ²»å…‰è°±å¯¹é½å’Œæ”¿å…šå¯¹é½ï¼Œæ¥é‡åŒ–æ„è¯†å½¢æ€å®šä½ã€‚æˆ‘ä»¬å¯¹äº”ç§å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œäº†å¾®è°ƒï¼Œç”Ÿæˆäº†2.8ä¸‡ç¯‡æ¼”è®²ï¼Œå¹¶ä½¿ç”¨æˆ‘ä»¬çš„æ¡†æ¶è¿›è¡Œäº†è¯„ä¼°ï¼Œæ¯”è¾ƒäº†åŸºå‡†æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œå¾®è°ƒåœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šäº§ç”Ÿäº†ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—æ”¹å–„ï¼Œæˆ‘ä»¬æå‡ºçš„æ–°æŒ‡æ ‡åœ¨æ”¿æ²»ç»´åº¦ä¸Šè¡¨ç°å‡ºäº†å¾ˆå¼ºçš„è¾¨åˆ«åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08247v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è®®ä¼šæ¼”è®²ç”Ÿæˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹æå‡ºäº†ç‰¹å®šæŒ‘æˆ˜ï¼Œé™¤äº†æ ‡å‡†æ–‡æœ¬ç”Ÿæˆä»»åŠ¡å¤–ï¼Œè¿˜éœ€è¦æ”¿æ²»çœŸå®æ€§å’Œæ„è¯†å½¢æ€ä¸€è‡´æ€§ã€‚å½“å‰çš„è¯­è¨€æ¨¡å‹ç¼ºä¹è®®ä¼šè¯­å¢ƒçš„ä¸“é—¨è®­ç»ƒï¼Œç°æœ‰è¯„ä¼°æ–¹æ³•ä¾§é‡äºæ ‡å‡†NLPæŒ‡æ ‡è€Œéæ”¿æ²»çœŸå®æ€§ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº†ParliaBenchè®®ä¼šæ¼”è®²ç”ŸæˆåŸºå‡†æµ‹è¯•ã€‚æˆ‘ä»¬ä½¿ç”¨è‹±å›½è®®ä¼šæ¼”è®²æ•°æ®é›†è¿›è¡Œç³»ç»Ÿæ¨¡å‹è®­ç»ƒã€‚æˆ‘ä»¬ç»“åˆè®¡ç®—æŒ‡æ ‡ä¸å¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°æ–¹æ³•ï¼Œä»è¯­è¨€è´¨é‡ã€è¯­ä¹‰è¿è´¯æ€§å’Œæ”¿æ²»çœŸå®æ€§ä¸‰ä¸ªç»´åº¦å¯¹ç”Ÿæˆè´¨é‡è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬æå‡ºä¸¤ç§åŸºäºåµŒå…¥çš„æ–°å‹æŒ‡æ ‡â€”â€”æ”¿æ²»å…‰è°±å¯¹é½å’Œæ”¿å…šå¯¹é½ï¼Œä»¥é‡åŒ–æ„è¯†å½¢æ€å®šä½ã€‚é€šè¿‡å¯¹äº”ç§å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒå¹¶ç”Ÿæˆ2.8ä¸‡ç¯‡æ¼”è®²è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå¾®è°ƒåœ¨å¤§å¤šæ•°æŒ‡æ ‡ä¸Šäº§ç”Ÿäº†ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—æ”¹è¿›ï¼Œæˆ‘ä»¬æå‡ºçš„æ–°å‹æŒ‡æ ‡åœ¨è¡¡é‡æ”¿æ²»ç»´åº¦æ–¹é¢è¡¨ç°å‡ºå¾ˆå¼ºçš„è¾¨åˆ«åŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®®ä¼šæ¼”è®²ç”Ÿæˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹æœ‰ç‰¹å®šæŒ‘æˆ˜ï¼Œéœ€å…¼é¡¾è¯­è¨€è´¨é‡ã€æ”¿æ²»çœŸå®æ€§å’Œæ„è¯†å½¢æ€ä¸€è‡´æ€§ã€‚</li>
<li>å½“å‰è¯­è¨€æ¨¡å‹ç¼ºä¹è®®ä¼šè¯­å¢ƒçš„ä¸“é—¨è®­ç»ƒã€‚</li>
<li>ç°æœ‰è¯„ä¼°æ–¹æ³•ä¸»è¦ä¾§é‡äºæ ‡å‡†NLPæŒ‡æ ‡ï¼Œéœ€è€ƒè™‘æ”¿æ²»çœŸå®æ€§ã€‚</li>
<li>æ¨å‡ºParliaBenchåŸºå‡†æµ‹è¯•ï¼Œä½¿ç”¨è‹±å›½è®®ä¼šæ¼”è®²æ•°æ®é›†è¿›è¡Œç³»ç»Ÿæ¨¡å‹è®­ç»ƒã€‚</li>
<li>è¯„ä¼°æ¡†æ¶ç»“åˆäº†è®¡ç®—æŒ‡æ ‡å’Œå¤§å‹è¯­è¨€æ¨¡å‹è¯„ä¼°æ–¹æ³•ï¼Œæ¶µç›–è¯­è¨€è´¨é‡ã€è¯­ä¹‰è¿è´¯æ€§å’Œæ”¿æ²»çœŸå®æ€§ä¸‰ä¸ªç»´åº¦ã€‚</li>
<li>æå‡ºä¸¤ç§æ–°å‹åµŒå…¥æŒ‡æ ‡â€”â€”æ”¿æ²»å…‰è°±å¯¹é½å’Œæ”¿å…šå¯¹é½ï¼Œä»¥é‡åŒ–æ„è¯†å½¢æ€å®šä½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08247">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b3539bded1e2330929c655f26c78e6a3" align="middle">
<img src="https://picx.zhimg.com/v2-f56b9e6ef23d90258166f86d0b6688f1" align="middle">
<img src="https://picx.zhimg.com/v2-073aacf799311f78eb7066fcd8f03075" align="middle">
<img src="https://picx.zhimg.com/v2-8508825acdd9c18b8da869390314f39d" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="State-of-the-Art-in-Text-Classification-for-South-Slavic-Languages-Fine-Tuning-or-Prompting"><a href="#State-of-the-Art-in-Text-Classification-for-South-Slavic-Languages-Fine-Tuning-or-Prompting" class="headerlink" title="State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?"></a>State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?</h2><p><strong>Authors:Taja Kuzman PungerÅ¡ek, Peter Rupnik, Ivan Porupski, Vuk DiniÄ‡, Nikola LjubeÅ¡iÄ‡</strong></p>
<p>Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.</p>
<blockquote>
<p>ç›´åˆ°æœ€è¿‘ï¼Œç»è¿‡ç²¾ç»†è°ƒæ•´çš„BERTç±»æ¨¡å‹åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæœ€å…ˆè¿›çš„æ€§èƒ½ã€‚éšç€æŒ‡ä»¤å¾®è°ƒè§£ç å™¨æ¨¡å‹ï¼ˆä¹Ÿç§°ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹LLMï¼‰çš„å…´èµ·ï¼Œè¯¥é¢†åŸŸè¶Šæ¥è¶Šå€¾å‘äºé›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºã€‚ç„¶è€Œï¼ŒLLMåœ¨æ–‡æœ¬åˆ†ç±»ä¸Šçš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºè¾ƒå°‘çš„è¯­è¨€ä¸Šï¼Œä»ç„¶è¢«æ¢ç´¢å¾—ä¸å¤Ÿå……åˆ†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å½“å‰çš„è¯­è¨€æ¨¡å‹åœ¨å¤šç§å—æ–¯æ‹‰å¤«è¯­è¨€ä¸Šçš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æ€§èƒ½ã€‚æˆ‘ä»¬å°†å…¬å¼€å¯ç”¨çš„ç»è¿‡ç²¾ç»†è°ƒæ•´çš„BERTç±»æ¨¡å‹ä¸ä¸€ç³»åˆ—å¼€æºå’Œé—­æºçš„å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œè·¨è¶Šäº†ä¸‰ä¸ªé¢†åŸŸçš„ä¸‰ä¸ªä»»åŠ¡ï¼šè®®ä¼šæ¼”è®²çš„æƒ…æ„Ÿåˆ†ç±»ã€æ–°é—»æ–‡ç« å’Œè®®ä¼šæ¼”è®²çš„ä¸»é¢˜åˆ†ç±»ä»¥åŠç½‘ç»œæ–‡æœ¬çš„ä½“è£è¯†åˆ«ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹è¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œé€šå¸¸ä¸ç»è¿‡ç²¾ç»†è°ƒæ•´çš„BERTç±»æ¨¡å‹ç›¸åŒ¹é…ç”šè‡³è¶…è¶Šã€‚æ­¤å¤–ï¼Œåœ¨é›¶æ ·æœ¬è®¾ç½®ä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å—æ–¯æ‹‰å¤«è¯­è¨€å’Œè‹±è¯­ä¸­çš„è¡¨ç°ç›¸å½“ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¹ŸæŒ‡å‡ºäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸»è¦ç¼ºç‚¹ï¼ŒåŒ…æ‹¬è¾“å‡ºæ›´ä¸å¯é¢„æµ‹ã€æ¨ç†é€Ÿåº¦æ˜æ˜¾è¾ƒæ…¢å’Œè®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚ç”±äºè¿™äº›å±€é™æ€§ï¼Œç»è¿‡ç²¾ç»†è°ƒæ•´çš„BERTç±»æ¨¡å‹åœ¨å¤§è§„æ¨¡è‡ªåŠ¨æ–‡æœ¬æ ‡æ³¨æ–¹é¢ä»ç„¶æ˜¯æ›´å®ç”¨çš„é€‰æ‹©ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07989v1">PDF</a> 16 pages; 4 figures; 3 tables. Submitted to the LREC 2026 conference</p>
<p><strong>Summary</strong></p>
<p>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œæœ‰æ—¶èƒ½åŒ¹é…ç”šè‡³è¶…è¶Šå¾®è°ƒè¿‡çš„BERTç±»ä¼¼æ¨¡å‹ã€‚ç„¶è€Œï¼ŒLLMsåœ¨èµ„æºè¾ƒå°‘çš„è¯­è¨€ä¸Šçš„è¡¨ç°å°šå¾…æ¢ç´¢ã€‚æœ¬æ–‡è¯„ä¼°äº†å½“å‰è¯­è¨€æ¨¡å‹åœ¨å—æ–¯æ‹‰å¤«è¯­è¨€ç³»åˆ—ä¸Šçš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æ€§èƒ½ï¼Œç»“æœæ˜¾ç¤ºLLMsçš„é›¶æ ·æœ¬æ€§èƒ½å¼ºåŠ²ï¼Œä½†å­˜åœ¨è¾“å‡ºä¸å¯é¢„æµ‹ã€æ¨ç†é€Ÿåº¦æ…¢å’Œè®¡ç®—æˆæœ¬é«˜ç­‰ç¼ºç‚¹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºå¼ºå¤§çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ã€‚</li>
<li>LLMsåœ¨å—æ–¯æ‹‰å¤«è¯­è¨€ç³»åˆ—ä¸Šçš„è¡¨ç°ä¸è‹±è¯­ç›¸å½“ã€‚</li>
<li>LLMsåœ¨æŸäº›ä»»åŠ¡ä¸Šçš„æ€§èƒ½å¯åŒ¹é…æˆ–è¶…è¶Šå¾®è°ƒè¿‡çš„BERTç±»ä¼¼æ¨¡å‹ã€‚</li>
<li>LLMsçš„è¾“å‡ºè¾ƒä¸å¯é¢„æµ‹ã€‚</li>
<li>LLMsçš„æ¨ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œè®¡ç®—æˆæœ¬è¾ƒé«˜ã€‚</li>
<li>ç›¸è¾ƒäºLLMsï¼Œå¾®è°ƒè¿‡çš„BERTç±»ä¼¼æ¨¡å‹åœ¨å¤§è§„æ¨¡è‡ªåŠ¨æ–‡æœ¬æ ‡æ³¨ä¸­æ›´ä¸ºå®ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07989">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6c80ac5b05d9bac5fd2604313a5ca880" align="middle">
<img src="https://picx.zhimg.com/v2-80ce7e5763872a4be871e698aab9254c" align="middle">
<img src="https://picx.zhimg.com/v2-7a6f77dab2dddfa74f1b781331654202" align="middle">
<img src="https://picx.zhimg.com/v2-a6f555e099ba0904039c312824e529f8" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="SynTTS-Commands-A-Public-Dataset-for-On-Device-KWS-via-TTS-Synthesized-Multilingual-Speech"><a href="#SynTTS-Commands-A-Public-Dataset-for-On-Device-KWS-via-TTS-Synthesized-Multilingual-Speech" class="headerlink" title="SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech"></a>SynTTS-Commands: A Public Dataset for On-Device KWS via TTS-Synthesized Multilingual Speech</h2><p><strong>Authors:Lu Gan, Xi Li</strong></p>
<p>The development of high-performance, on-device keyword spotting (KWS) systems for ultra-low-power hardware is critically constrained by the scarcity of specialized, multi-command training datasets. Traditional data collection through human recording is costly, slow, and lacks scalability. This paper introduces SYNTTS-COMMANDS, a novel, multilingual voice command dataset entirely generated using state-of-the-art Text-to-Speech (TTS) synthesis. By leveraging the CosyVoice 2 model and speaker embeddings from public corpora, we created a scalable collection of English and Chinese commands. Extensive benchmarking across a range of efficient acoustic models demonstrates that our synthetic dataset enables exceptional accuracy, achieving up to 99.5% on English and 98% on Chinese command recognition. These results robustly validate that synthetic speech can effectively replace human-recorded audio for training KWS classifiers. Our work directly addresses the data bottleneck in TinyML, providing a practical, scalable foundation for building private, low-latency, and energy-efficient voice interfaces on resource-constrained edge devices.</p>
<blockquote>
<p>é«˜æ€§èƒ½çš„ç«¯è®¾å¤‡å…³é”®è¯è¯†åˆ«ï¼ˆKWSï¼‰ç³»ç»Ÿåœ¨è¶…ä½åŠŸè€—ç¡¬ä»¶ä¸Šçš„å¼€å‘å—åˆ°ä¸“ä¸šåŒ–å¤šæŒ‡ä»¤è®­ç»ƒæ•°æ®é›†ç¨€ç¼ºçš„ä¸¥é‡åˆ¶çº¦ã€‚ä¼ ç»Ÿçš„é€šè¿‡äººå·¥å½•éŸ³çš„æ•°æ®æ”¶é›†æ–¹å¼æˆæœ¬é«˜ã€é€Ÿåº¦æ…¢ä¸”ç¼ºä¹å¯æ‰©å±•æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†SYNTTS-COMMANDSï¼Œè¿™æ˜¯ä¸€ç§å…¨æ–°çš„å¤šè¯­ç§è¯­éŸ³æŒ‡ä»¤æ•°æ®é›†ï¼Œå®Œå…¨ä½¿ç”¨æœ€å…ˆè¿›çš„æ–‡æœ¬-è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæŠ€æœ¯ç”Ÿæˆã€‚æˆ‘ä»¬å€ŸåŠ©CosyVoice 2æ¨¡å‹å’Œå…¬å…±è¯­æ–™åº“çš„è¯´è¯äººåµŒå…¥æŠ€æœ¯ï¼Œåˆ›å»ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„è‹±è¯­å’Œä¸­æ–‡æŒ‡ä»¤é›†åˆã€‚åœ¨ä¸€ç³»åˆ—é«˜æ•ˆçš„å£°å­¦æ¨¡å‹ä¸Šçš„å¹¿æ³›åŸºå‡†æµ‹è¯•è¡¨æ˜ï¼Œæˆ‘ä»¬çš„åˆæˆæ•°æ®é›†èƒ½å¤Ÿå®ç°å‡ºè‰²çš„å‡†ç¡®æ€§ï¼Œè‹±è¯­å‘½ä»¤è¯†åˆ«ç‡é«˜è¾¾99.5%ï¼Œä¸­æ–‡å‘½ä»¤è¯†åˆ«ç‡è¾¾98%ã€‚è¿™äº›ç»“æœç¨³å¥åœ°éªŒè¯äº†åˆæˆè¯­éŸ³å¯ä»¥æœ‰æ•ˆåœ°æ›¿ä»£äººå·¥å½•éŸ³çš„éŸ³é¢‘ï¼Œç”¨äºè®­ç»ƒKWSåˆ†ç±»å™¨ã€‚æˆ‘ä»¬çš„å·¥ä½œç›´æ¥è§£å†³äº†TinyMLä¸­çš„æ•°æ®ç“¶é¢ˆé—®é¢˜ï¼Œä¸ºåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šæ„å»ºç§æœ‰ã€ä½å»¶è¿Ÿå’ŒèŠ‚èƒ½çš„è¯­éŸ³æ¥å£æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„åŸºç¡€ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07821v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä½¿ç”¨æœ€æ–°æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆæŠ€æœ¯åˆ›å»ºçš„å¤šè¯­ç§è¯­éŸ³å‘½ä»¤æ•°æ®é›†SYNTTS-COMMANDSã€‚é€šè¿‡åˆ©ç”¨CosyVoice 2æ¨¡å‹å’Œå…¬å¼€è¯­æ–™åº“ä¸­çš„è¯´è¯äººåµŒå…¥ï¼ŒæˆåŠŸç”Ÿæˆäº†è‹±è¯­å’Œä¸­æ–‡å‘½ä»¤çš„å¯æ‰©å±•æ•°æ®é›†ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥åˆæˆæ•°æ®é›†åœ¨é«˜æ•ˆå£°å­¦æ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜å¼‚ï¼Œè‹±è¯­å‘½ä»¤è¯†åˆ«ç‡é«˜è¾¾99.5%ï¼Œä¸­æ–‡è¾¾98%ã€‚ç»“æœè¯å®ï¼Œåˆæˆè¯­éŸ³å¯æœ‰æ•ˆæ›¿ä»£äººç±»å½•åˆ¶çš„éŸ³é¢‘ï¼Œç”¨äºè®­ç»ƒå…³é”®è¯è¯†åˆ«åˆ†ç±»å™¨ã€‚è¯¥ç ”ç©¶è§£å†³äº†TinyMLä¸­çš„æ•°æ®ç“¶é¢ˆé—®é¢˜ï¼Œä¸ºåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šæ„å»ºç§å¯†ã€ä½å»¶è¿Ÿå’ŒèŠ‚èƒ½çš„è¯­éŸ³æ¥å£æä¾›äº†å®ç”¨ä¸”å¯æ‰©å±•çš„åŸºç¡€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>SYNTTS-COMMANDSæ˜¯ä¸€ä¸ªå…¨æ–°çš„å¤šè¯­ç§è¯­éŸ³å‘½ä»¤æ•°æ®é›†ï¼Œå®Œå…¨ç”±å…ˆè¿›çš„æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æŠ€æœ¯ç”Ÿæˆã€‚</li>
<li>è¯¥æ•°æ®é›†åŒ…å«è‹±è¯­å’Œä¸­æ–‡å‘½ä»¤ï¼Œè§£å†³äº†ç‰¹å®šå‘½ä»¤è®­ç»ƒæ•°æ®é›†ç¨€ç¼ºçš„é—®é¢˜ã€‚</li>
<li>é€šè¿‡åˆ©ç”¨CosyVoice 2æ¨¡å‹å’Œå…¬å¼€è¯­æ–™åº“ï¼Œå®ç°äº†æ•°æ®é›†çš„åˆ›å»ºï¼Œä½¿å…¶å…·æœ‰å¯æ‰©å±•æ€§ã€‚</li>
<li>å®éªŒè¡¨æ˜ï¼Œè¯¥åˆæˆæ•°æ®é›†åœ¨å£°å­¦æ¨¡å‹ä¸Šçš„è¡¨ç°ä¼˜ç§€ï¼Œè‹±è¯­å’Œä¸­æ–‡å‘½ä»¤è¯†åˆ«ç‡åˆ†åˆ«é«˜è¾¾99.5%å’Œ98%ã€‚</li>
<li>åˆæˆè¯­éŸ³å¯ä»¥æœ‰æ•ˆåœ°æ›¿ä»£äººç±»å½•åˆ¶çš„éŸ³é¢‘ï¼Œç”¨äºè®­ç»ƒå…³é”®è¯è¯†åˆ«åˆ†ç±»å™¨ã€‚</li>
<li>è¯¥ç ”ç©¶è§£å†³äº†TinyMLä¸­çš„æ•°æ®ç“¶é¢ˆé—®é¢˜ï¼Œä¸ºåœ¨èµ„æºå—é™çš„è¾¹ç¼˜è®¾å¤‡ä¸Šæ„å»ºè¯­éŸ³æ¥å£æä¾›äº†åŸºç¡€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07821">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c610255d59a5c99eec6b4950dbb8808e" align="middle">
<img src="https://picx.zhimg.com/v2-e6cb74fd28250359262d951c1fbf5e5b" align="middle">
<img src="https://picx.zhimg.com/v2-63515865902d8d6feaf6ae190e1159e6" align="middle">
<img src="https://picx.zhimg.com/v2-c2d2f2a8521148f1d855b67e039dea40" align="middle">
<img src="https://picx.zhimg.com/v2-4d1d9f4b0ba60c590c4c0c3e0f542ff7" align="middle">
<img src="https://picx.zhimg.com/v2-df876bc8469f2062747aa47e39d05c93" align="middle">
<img src="https://picx.zhimg.com/v2-82c7ae51985b45c94840cbac85a28065" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="E2E-VGuard-Adversarial-Prevention-for-Production-LLM-based-End-To-End-Speech-Synthesis"><a href="#E2E-VGuard-Adversarial-Prevention-for-Production-LLM-based-End-To-End-Speech-Synthesis" class="headerlink" title="E2E-VGuard: Adversarial Prevention for Production LLM-based End-To-End Speech Synthesis"></a>E2E-VGuard: Adversarial Prevention for Production LLM-based End-To-End Speech Synthesis</h2><p><strong>Authors:Zhisheng Zhang, Derui Wang, Yifan Mi, Zhiyong Wu, Jie Gao, Yuxin Cao, Kai Ye, Minhui Xue, Jie Hao</strong></p>
<p>Recent advancements in speech synthesis technology have enriched our daily lives, with high-quality and human-like audio widely adopted across real-world applications. However, malicious exploitation like voice-cloning fraud poses severe security risks. Existing defense techniques struggle to address the production large language model (LLM)-based speech synthesis. While previous studies have considered the protection for fine-tuning synthesizers, they assume manually annotated transcripts. Given the labor intensity of manual annotation, end-to-end (E2E) systems leveraging automatic speech recognition (ASR) to generate transcripts are becoming increasingly prevalent, e.g., voice cloning via commercial APIs. Therefore, this E2E speech synthesis also requires new security mechanisms. To tackle these challenges, we propose E2E-VGuard, a proactive defense framework for two emerging threats: (1) production LLM-based speech synthesis, and (2) the novel attack arising from ASR-driven E2E scenarios. Specifically, we employ the encoder ensemble with a feature extractor to protect timbre, while ASR-targeted adversarial examples disrupt pronunciation. Moreover, we incorporate the psychoacoustic model to ensure perturbative imperceptibility. For a comprehensive evaluation, we test 16 open-source synthesizers and 3 commercial APIs across Chinese and English datasets, confirming E2E-VGuardâ€™s effectiveness in timbre and pronunciation protection. Real-world deployment validation is also conducted. Our code and demo page are available at <a target="_blank" rel="noopener" href="https://wxzyd123.github.io/e2e-vguard/">https://wxzyd123.github.io/e2e-vguard/</a>.</p>
<blockquote>
<p>è¿‘æœŸè¯­éŸ³åˆæˆæŠ€æœ¯çš„è¿›å±•ä¸°å¯Œäº†æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ï¼Œé«˜è´¨é‡ã€äººæ€§åŒ–çš„éŸ³é¢‘å·²åœ¨ç°å®åº”ç”¨ä¸­å¾—åˆ°å¹¿æ³›é‡‡ç”¨ã€‚ç„¶è€Œï¼Œè¯¸å¦‚è¯­éŸ³å…‹éš†æ¬ºè¯ˆç­‰æ¶æ„åˆ©ç”¨è¡Œä¸ºå¸¦æ¥äº†ä¸¥é‡çš„å®‰å…¨é£é™©ã€‚ç°æœ‰çš„é˜²å¾¡æŠ€æœ¯å¾ˆéš¾åº”å¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³åˆæˆã€‚è™½ç„¶ä¹‹å‰çš„ç ”ç©¶å·²ç»è€ƒè™‘äº†åˆæˆå™¨çš„ä¿æŠ¤ï¼Œä½†å®ƒä»¬å‡è®¾äº†æ‰‹åŠ¨æ ‡æ³¨çš„æ–‡æœ¬ã€‚è€ƒè™‘åˆ°æ‰‹åŠ¨æ ‡æ³¨çš„åŠ³åŠ¨å¯†é›†åº¦ï¼Œåˆ©ç”¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ç”Ÿæˆæ–‡æœ¬çš„ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰ç³»ç»Ÿå˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œä¾‹å¦‚é€šè¿‡å•†ä¸šAPIè¿›è¡Œè¯­éŸ³å…‹éš†ã€‚å› æ­¤ï¼Œè¿™ç§ç«¯åˆ°ç«¯çš„è¯­éŸ³åˆæˆä¹Ÿéœ€è¦æ–°çš„å®‰å…¨æœºåˆ¶ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†E2E-VGuardï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸¤ä¸ªæ–°å…´å¨èƒçš„ä¸»åŠ¨é˜²å¾¡æ¡†æ¶ï¼šï¼ˆ1ï¼‰åŸºäºç”Ÿäº§LLMçš„è¯­éŸ³åˆæˆï¼Œï¼ˆ2ï¼‰ç”±ASRé©±åŠ¨E2Eåœºæ™¯äº§ç”Ÿçš„æ–°å‹æ”»å‡»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨ç¼–ç å™¨é›†åˆå’Œç‰¹å¾æå–å™¨æ¥ä¿æŠ¤éŸ³è‰²ï¼ŒåŒæ—¶é’ˆå¯¹ASRçš„å¯¹æŠ—æ€§ç¤ºä¾‹ä¼šç ´åå‘éŸ³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»“åˆäº†å¿ƒç†å£°å­¦æ¨¡å‹æ¥ç¡®ä¿æ‰°åŠ¨çš„ä¸æ˜“å¯Ÿè§‰æ€§ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°ï¼Œæˆ‘ä»¬æµ‹è¯•äº†16ä¸ªå¼€æºåˆæˆå™¨å’Œ3ä¸ªå•†ä¸šAPIçš„ä¸­æ–‡å’Œè‹±æ–‡æ•°æ®é›†ï¼Œè¯å®äº†E2E-VGuardåœ¨éŸ³è‰²å’Œå‘éŸ³ä¿æŠ¤æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚è¿˜è¿›è¡Œäº†ç°å®ä¸–ç•Œçš„éƒ¨ç½²éªŒè¯ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¼”ç¤ºé¡µé¢å¯è®¿é—®ï¼š<a target="_blank" rel="noopener" href="https://wxzyd123.github.io/e2e-vguard/%E3%80%82">https://wxzyd123.github.io/e2e-vguard/ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07099v1">PDF</a> Accepted to NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è¿‘æœŸè¯­éŸ³åˆæˆæŠ€æœ¯çš„è¿›å±•åŠå…¶åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å¹¿æ³›åº”ç”¨ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†æ¶æ„åˆ©ç”¨å¦‚è¯­éŸ³å…‹éš†æ¬ºè¯ˆç­‰å¸¦æ¥çš„ä¸¥é‡å®‰å…¨é£é™©ã€‚ç°æœ‰é˜²å¾¡æŠ€æœ¯éš¾ä»¥åº”å¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³åˆæˆã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºE2E-VGuardçš„ä¸»åŠ¨é˜²å¾¡æ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹ä¸¤å¤§æ–°å…´å¨èƒï¼šåŸºäºLLMçš„è¯­éŸ³åˆæˆå’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰é©±åŠ¨çš„ç«¯åˆ°ç«¯åœºæ™¯çš„æ–°å‹æ”»å‡»ã€‚è¯¥æ¡†æ¶é‡‡ç”¨ç¼–ç å™¨é›†åˆå¹¶ç»“åˆç‰¹å¾æå–å™¨æ¥ä¿æŠ¤éŸ³è‰²ï¼ŒåŒæ—¶é€šè¿‡é’ˆå¯¹ASRçš„å¯¹æŠ—æ€§ä¾‹å­æ¥å¹²æ‰°å‘éŸ³ã€‚åŒæ—¶ï¼Œè¿˜ç»“åˆäº†å¿ƒç†å£°å­¦æ¨¡å‹ç¡®ä¿æ‰°åŠ¨çš„ä¸æ˜“å¯Ÿè§‰æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å¯¹ä¸­æ–‡å’Œè‹±æ–‡æ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›éªŒè¯ï¼Œèƒ½æœ‰æ•ˆä¿æŠ¤éŸ³è‰²å’Œå‘éŸ³ã€‚ç›¸å…³ä»£ç å’Œæ¼”ç¤ºé¡µé¢å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è¿‘æœŸè¯­éŸ³åˆæˆæŠ€æœ¯å‘å±•è¿…é€Ÿï¼Œä½†æ¶æ„åˆ©ç”¨å¦‚è¯­éŸ³å…‹éš†æ¬ºè¯ˆå¸¦æ¥äº†å®‰å…¨é£é™©ã€‚</li>
<li>ç°æœ‰é˜²å¾¡æŠ€æœ¯éš¾ä»¥åº”å¯¹åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­éŸ³åˆæˆå’Œç«¯åˆ°ç«¯åœºæ™¯çš„å¨èƒã€‚</li>
<li>E2E-VGuardæ˜¯ä¸€ç§ä¸»åŠ¨é˜²å¾¡æ¡†æ¶ï¼Œæ—¨åœ¨åº”å¯¹è¿™ä¸¤å¤§æ–°å…´å¨èƒã€‚</li>
<li>E2E-VGuardé‡‡ç”¨ç¼–ç å™¨é›†åˆå¹¶ç»“åˆç‰¹å¾æå–å™¨ä¿æŠ¤éŸ³è‰²ï¼Œé€šè¿‡å¯¹æŠ—æ€§ä¾‹å­å¹²æ‰°å‘éŸ³ã€‚</li>
<li>å¿ƒç†å£°å­¦æ¨¡å‹è¢«ç”¨äºç¡®ä¿æ‰°åŠ¨çš„ä¸æ˜“å¯Ÿè§‰æ€§ã€‚</li>
<li>åœ¨ä¸­æ–‡å’Œè‹±æ–‡æ•°æ®é›†ä¸Šçš„å®éªŒéªŒè¯äº†E2E-VGuardçš„æœ‰æ•ˆæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07099">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1df851e3f56855ab509ba5b887f5dc87" align="middle">
<img src="https://picx.zhimg.com/v2-2d402c2a82596a186957255426ab5c62" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-catalog-of-new-blue-stragglers-in-open-clusters-with-Gaia-DR3"><a href="#A-catalog-of-new-blue-stragglers-in-open-clusters-with-Gaia-DR3" class="headerlink" title="A catalog of new blue stragglers in open clusters with Gaia DR3"></a>A catalog of new blue stragglers in open clusters with Gaia DR3</h2><p><strong>Authors:Songmei Qin, Jing Zhong, Friedrich Anders, Lola Balaguer-NÃºÃ±ez, Chunyan Li, Yueyue Jiang, Guimei Liu, Tong Tang, Li Chen</strong></p>
<p>The high-precision {\it Gaia} data release 3 (DR3) enables the discovery of numerous open clusters in the Milky Way, providing an excellent opportunity to search for blue straggler stars in open clusters and investigate their formation and evolution in these environments. Using the member stars from literature open cluster catalogs, we visually inspected the color-magnitude diagram (CMD) of each cluster and selected cluster candidates that potentially host blue stragglers. We then reassessed cluster memberships using the {\tt pyUPMASK} algorithm with {\it Gaia} DR3 and performed isochrone fitting to derive physical parameters for each cluster, including age, distance modulus, mean reddening, and metallicity. Finally, we empirically identified straggler stars based on their positions relative to the best-fitting isochrone, zero-age main sequence (ZAMS), and equal-mass binary sequence on the CMD. In total, we identified 272 new straggler stars in 99 open clusters, comprising 153 blue stragglers, 98 probable blue stragglers, and 21 yellow stragglers. Compared to the reported blue straggler catalogs based on earlier {\it Gaia} data, our results increase the number of open clusters with stragglers in the Milky Way by 22.2%, and the total number of blue stragglers by 11.2%.</p>
<blockquote>
<p>åˆ©ç”¨é«˜ç²¾åº¦ã€ŠGaiaã€‹æ•°æ®å‘å¸ƒ3ï¼ˆDR3ï¼‰ç‰ˆæœ¬ï¼Œèƒ½å¤Ÿåœ¨é“¶æ²³ç³»ä¸­å‘ç°ä¼—å¤šç–æ•£æ˜Ÿç¾¤ï¼Œè¿™ä¸ºåœ¨ç–æ•£æ˜Ÿç¾¤ä¸­å¯»æ‰¾è“è‰²ç¦»æ•£æ˜Ÿå¹¶ç ”ç©¶è¿™äº›ç¯å¢ƒä¸­å®ƒä»¬çš„å½¢æˆå’Œæ¼”åŒ–æä¾›äº†ç»ä½³æœºä¼šã€‚æˆ‘ä»¬é‡‡ç”¨æ–‡çŒ®ç–æ•£æ˜Ÿç¾¤ç›®å½•ä¸­çš„æˆå‘˜æ˜Ÿï¼Œå¯¹æ¯ä¸ªæ˜Ÿç¾¤çš„æ˜Ÿç­‰-è‰²åº¦å›¾ï¼ˆCMDï¼‰è¿›è¡Œè§†è§‰æ£€æŸ¥ï¼ŒæŒ‘é€‰å‡ºå¯èƒ½åŒ…å«è“è‰²ç¦»æ•£æ˜Ÿå€™é€‰æ˜Ÿç¾¤çš„å€™é€‰æ˜Ÿç¾¤ã€‚æ¥ç€æˆ‘ä»¬ä½¿ç”¨ã€ŠpyUPMASKã€‹ç®—æ³•ç»“åˆã€ŠGaiaã€‹DR3ç‰ˆæœ¬é‡æ–°è¯„ä¼°æ˜Ÿç¾¤æˆå‘˜èº«ä»½ï¼Œå¹¶è¿›è¡ŒåŒåˆ†æ›²çº¿æ‹Ÿåˆä»¥å¾—å‡ºæ¯ä¸ªæ˜Ÿç¾¤çš„ç‰©ç†å‚æ•°ï¼ŒåŒ…æ‹¬å¹´é¾„ã€è·ç¦»æ¨¡æ•°ã€å¹³å‡æ¶ˆå…‰ä»¥åŠé‡‘å±é‡ã€‚æœ€åï¼Œæˆ‘ä»¬æ ¹æ®ç¦»æ•£æ˜Ÿç›¸å¯¹äºæœ€ä½³æ‹ŸåˆåŒåˆ†æ›²çº¿ã€é›¶é¾„ä¸»åºåˆ—ï¼ˆZAMSï¼‰ä»¥åŠç­‰é‡è´¨é‡åŒæ˜Ÿåºåˆ—åœ¨CMDä¸Šçš„ä½ç½®æ¥ç»éªŒæ€§é‰´åˆ«ç¦»æ•£æ˜Ÿã€‚æ€»å…±æœ‰272é¢—æ–°å‘ç°çš„ç¦»æ•£æ˜Ÿå­˜åœ¨äº99ä¸ªç–æ•£æ˜Ÿç¾¤ä¸­ï¼Œå…¶ä¸­åŒ…æ‹¬æœ‰153é¢—è“è‰²ç¦»æ•£æ˜Ÿã€98é¢—ç–‘ä¼¼è“è‰²ç¦»æ•£æ˜Ÿä»¥åŠ21é¢—é»„è‰²ç¦»æ•£æ˜Ÿã€‚ç›¸è¾ƒäºæ—©æœŸåŸºäºã€ŠGaiaã€‹æ•°æ®çš„è“è‰²ç¦»æ•£æ˜Ÿç›®å½•æŠ¥å‘Šï¼Œæˆ‘ä»¬çš„ç»“æœä½¿é“¶æ²³ç³»ä¸­å­˜åœ¨ç¦»æ•£æ˜Ÿçš„æ˜Ÿç¾¤æ•°é‡å¢åŠ äº†22.2%ï¼Œè“è‰²ç¦»æ•£æ˜Ÿçš„æ€»æ•°å¢åŠ äº†11.2%ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07043v1">PDF</a> 24 pages, 15 figures</p>
<p><strong>Summary</strong><br>     åŸºäºé«˜ç²¾åº¦çš„Gaiaæ•°æ®å‘å¸ƒç¬¬ä¸‰ç‰ˆï¼ˆDR3ï¼‰ï¼Œå‘ç°äº†ä¼—å¤šé“¶æ²³ä¸­çš„å¼€æ™®å‹’æ˜Ÿå›¢ï¼Œä¸ºæœå¯»è“ç§»æ˜Ÿå¹¶åœ¨å¼€æ™®å‹’æ˜Ÿå›¢ç¯å¢ƒä¸­ç ”ç©¶å…¶å½¢æˆå’Œæ¼”åŒ–æä¾›äº†æå¥½æœºä¼šã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡æ–‡çŒ®ä¸­çš„å¼€æ™®å‹’æ˜Ÿå›¢ç›®å½•æˆå‘˜æ˜Ÿï¼Œè§†è§‰æ£€æµ‹æ¯ä¸ªæ˜Ÿå›¢çš„è‰²-æ˜Ÿç­‰å›¾ï¼ˆCMDï¼‰ï¼Œç­›é€‰å‡ºå¯èƒ½å«æœ‰è“ç§»æ˜Ÿçš„æ˜Ÿå›¢å€™é€‰è€…ã€‚æ¥ç€ä½¿ç”¨pyUPMASKç®—æ³•é‡æ–°è¯„ä¼°æ˜Ÿå›¢æˆå‘˜èº«ä»½ï¼Œå¹¶è¿›è¡Œæ˜Ÿå›¢ç‰©ç†å‚æ•°ç­‰é¾„è°±æ‹Ÿåˆã€‚æœ€ç»ˆï¼Œå®è¯ç¡®å®šäº†ç›¸å¯¹äºæœ€ä½³æ‹Ÿåˆç­‰é¾„è°±ã€é›¶é¾„ä¸»åºåˆ—å’Œç­‰è´¨é‡åŒæ˜Ÿåºåˆ—åœ¨CMDä¸Šçš„ä½ç½®ï¼Œç¡®å®šäº†272é¢—æ–°çš„è“ç§»æ˜Ÿï¼Œåˆ†å¸ƒåœ¨99ä¸ªå¼€æ™®å‹’æ˜Ÿå›¢ä¸­ã€‚ä¸æ—©æœŸGaiaæ•°æ®çš„è“ç§»æ˜Ÿç›®å½•ç›¸æ¯”ï¼Œæœ¬ç ”ç©¶ä½¿é“¶æ²³ä¸­å…·æœ‰è“ç§»æ˜Ÿçš„å¼€æ™®å‹’æ˜Ÿå›¢æ•°é‡å¢åŠ äº†22.2%ï¼Œè“ç§»æ˜Ÿæ€»æ•°å¢åŠ äº†11.2%ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Gaia DR3æ•°æ®ä¸ºå‘ç°é“¶æ²³ä¸­çš„å¼€æ”¾æ˜Ÿå›¢æä¾›äº†è‰¯å¥½æœºä¼šã€‚</li>
<li>é€šè¿‡æ–‡çŒ®çš„å¼€æ”¾æ˜Ÿå›¢ç›®å½•ç­›é€‰æ½œåœ¨å«æœ‰è“ç§»æ˜Ÿçš„æ˜Ÿå›¢å€™é€‰è€…ã€‚</li>
<li>ä½¿ç”¨pyUPMASKç®—æ³•é‡æ–°è¯„ä¼°æ˜Ÿå›¢æˆå‘˜èº«ä»½å¹¶è¿›è¡Œæ˜Ÿå›¢ç‰©ç†å‚æ•°æ‹Ÿåˆã€‚</li>
<li>é€šè¿‡è‰²-æ˜Ÿç­‰å›¾ç¡®å®šäº†æ–°çš„è“ç§»æ˜Ÿä½ç½®ã€‚</li>
<li>åœ¨99ä¸ªå¼€æ”¾æ˜Ÿå›¢ä¸­å‘ç°äº†æ€»è®¡272é¢—æ–°çš„è“ç§»æ˜Ÿã€‚</li>
<li>ä¸æ—©æœŸGaiaæ•°æ®ç›¸æ¯”ï¼Œæœ¬ç ”ç©¶çš„å‘ç°å¢åŠ äº†é“¶æ²³ä¸­å…·æœ‰è“ç§»æ˜Ÿçš„å¼€æ”¾æ˜Ÿå›¢æ•°é‡å’Œè“ç§»æ˜Ÿæ€»æ•°ã€‚</li>
<li>è“ç§»æ˜Ÿçš„å‘ç°ä¸ºç ”ç©¶å…¶åœ¨å¼€æ”¾æ˜Ÿå›¢ä¸­çš„å½¢æˆå’Œæ¼”åŒ–æä¾›äº†æ›´å¤šç´ æã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07043">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-158bfd8c298388e0f2996c2cf2f4b79c" align="middle">
<img src="https://picx.zhimg.com/v2-57448a653839d89687b0791600bbaef2" align="middle">
<img src="https://picx.zhimg.com/v2-bc9070053d09187bf479b95c8e0d7cc7" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="MedVoiceBias-A-Controlled-Study-of-Audio-LLM-Behavior-in-Clinical-Decision-Making"><a href="#MedVoiceBias-A-Controlled-Study-of-Audio-LLM-Behavior-in-Clinical-Decision-Making" class="headerlink" title="MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making"></a>MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making</h2><p><strong>Authors:Zhi Rui Tam, Yun-Nung Chen</strong></p>
<p>As large language models transition from text-based interfaces to audio interactions in clinical settings, they might introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on 170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age, gender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations. Further analysis uncovered age disparities of up to 12% between young and elderly voices, which persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion was not detected due to poor recognition performance. These results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patientâ€™s voice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We conclude that bias-aware architectures are essential and urgently needed before the clinical deployment of these models.</p>
<blockquote>
<p>éšç€å¤§å‹è¯­è¨€æ¨¡å‹ä»åŸºäºæ–‡æœ¬ç•Œé¢çš„ä¸´åºŠç¯å¢ƒè¿‡æ¸¡åˆ°åŸºäºéŸ³é¢‘çš„äº’åŠ¨ç¯å¢ƒï¼Œå®ƒä»¬å¯èƒ½ä¼šé€šè¿‡éŸ³é¢‘ä¸­çš„å‰¯è¯­è¨€çº¿ç´¢å¼•å…¥æ–°çš„æ¼æ´ã€‚æˆ‘ä»¬å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œæ¶‰åŠ170ä¸ªä¸´åºŠç—…ä¾‹ï¼Œæ¯ä¸ªç—…ä¾‹éƒ½é€šè¿‡æ¶µç›–å¹´é¾„ã€æ€§åˆ«å’Œæƒ…æ„Ÿå˜åŒ–çš„36ç§ä¸åŒè¯­éŸ³ç‰¹å¾åˆæˆè¯­éŸ³ã€‚æˆ‘ä»¬çš„ç ”ç©¶å‘ç°äº†ä¸€ç§ä¸¥é‡çš„æ¨¡å¼åè§ï¼šä¸åŸºäºæ–‡æœ¬çš„è¾“å…¥ç›¸æ¯”ï¼ŒéŸ³é¢‘è¾“å…¥çš„æ‰‹æœ¯å»ºè®®å·®å¼‚é«˜è¾¾35%ï¼Œå…¶ä¸­ä¸€ä¸ªæ¨¡å‹æä¾›çš„å»ºè®®å‡å°‘äº†80%ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¿˜å‘ç°äº†å¹´è½»å’Œè€å¹´å£°éŸ³ä¹‹é—´çš„å·®å¼‚é«˜è¾¾12%ï¼Œå°½ç®¡é‡‡å–äº†è¿è´¯æ¨ç†æç¤ºçš„æ–¹æ³•ï¼Œä½†è¿™ç§å·®å¼‚åœ¨å¤§å¤šæ•°æ¨¡å‹ä¸­ä»ç„¶å­˜åœ¨ã€‚è™½ç„¶æ˜ç¡®çš„æ¨ç†æˆåŠŸåœ°æ¶ˆé™¤äº†æ€§åˆ«åè§ï¼Œä½†ç”±äºè¯†åˆ«æ€§èƒ½ä¸ä½³ï¼Œæƒ…æ„Ÿçš„å½±å“å¹¶æœªè¢«æ£€æµ‹å‡ºæ¥ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒéŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹æ˜“äºæ ¹æ®æ‚£è€…çš„å£°éŸ³ç‰¹å¾è€ŒéåŒ»å­¦è¯æ®åšå‡ºä¸´åºŠå†³ç­–ï¼Œè¿™æ˜¯ä¸€ä¸ªé£é™©æ¼æ´ï¼Œå¯èƒ½ä¼šä½¿åŒ»ç–—ä¿å¥å·®å¼‚æŒç»­å­˜åœ¨ã€‚æˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œåœ¨ä¸´åºŠéƒ¨ç½²è¿™äº›æ¨¡å‹ä¹‹å‰ï¼Œæ„å»ºå…·å¤‡åè§æ„è¯†çš„æ¶æ„æ˜¯è‡³å…³é‡è¦çš„å’Œæ€¥éœ€çš„ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06592v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä»æ–‡æœ¬ç•Œé¢è½¬å‘ä¸´åºŠç¯å¢ƒä¸­çš„éŸ³é¢‘äº¤äº’æ—¶ï¼Œå®ƒä»¬å¯èƒ½ä¼šé€šè¿‡éŸ³é¢‘ä¸­çš„å‰¯è¯­è¨€çº¿ç´¢å¼•å…¥æ–°çš„æ¼æ´ã€‚é€šè¿‡å¯¹170ä¸ªä¸´åºŠç—…ä¾‹çš„è¯„ä¼°å‘ç°ï¼Œä¸åŸºäºæ–‡æœ¬çš„è¾“å…¥ç›¸æ¯”ï¼ŒéŸ³é¢‘è¾“å…¥çš„æ‰‹æœ¯å»ºè®®å·®å¼‚é«˜è¾¾35%ï¼Œå…¶ä¸­ä¸€ä¸ªæ¨¡å‹çš„å»ºè®®æ•°é‡å‡å°‘äº†80%ã€‚åˆ†æè¿˜æ­ç¤ºäº†åœ¨ä¸åŒçš„å£°éŸ³å¹´é¾„ç‰¹å¾é—´å­˜åœ¨é«˜è¾¾12%çš„å·®å¼‚ï¼Œå°½ç®¡å¤§å¤šæ•°æ¨¡å‹é€šè¿‡é“¾å¼æ€ç»´æç¤ºä¹Ÿå­˜åœ¨ç±»ä¼¼å·®å¼‚ã€‚è™½ç„¶æ˜ç¡®çš„æ¨ç†æˆåŠŸæ¶ˆé™¤äº†æ€§åˆ«åè§ï¼Œä½†ç”±äºè¯†åˆ«æ€§èƒ½ä¸ä½³ï¼Œæƒ…æ„Ÿçš„å½±å“å¹¶æœªè¢«æ£€æµ‹åˆ°ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒéŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹æ˜“äºæ ¹æ®æ‚£è€…çš„å£°éŸ³ç‰¹å¾è€ŒéåŒ»å­¦è¯æ®åšå‡ºä¸´åºŠå†³ç­–ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯èƒ½åŠ å‰§åŒ»ç–—ä¿å¥ä¸å¹³ç­‰çš„ç¼ºé™·ã€‚å› æ­¤ï¼Œåœ¨éƒ¨ç½²è¿™äº›æ¨¡å‹ä¹‹å‰ï¼Œéœ€è¦é‡è§†æ¶æ„çš„åè§é—®é¢˜ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†éŸ³é¢‘äº¤äº’æ—¶å¯èƒ½ä¼šå¼•å…¥æ–°çš„æ¼æ´ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡å‰¯è¯­è¨€çº¿ç´¢ã€‚</li>
<li>éŸ³é¢‘è¾“å…¥åœ¨æ‰‹æœ¯å»ºè®®æ–¹é¢ä¸æ–‡æœ¬è¾“å…¥å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œé«˜è¾¾35%ã€‚</li>
<li>åœ¨ä¸åŒå£°éŸ³å¹´é¾„çš„è¯­éŸ³ç‰¹å¾ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„å·®å¼‚ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¸´åºŠå†³ç­–çš„ä¸å‡†ç¡®ã€‚</li>
<li>åœ¨å¤§å¤šæ•°æ¨¡å‹ä¸­ï¼Œå³ä½¿ä½¿ç”¨é“¾å¼æ€ç»´æç¤ºï¼Œå¹´é¾„å·®å¼‚ä¾ç„¶å­˜åœ¨ã€‚</li>
<li>æ€§åˆ«åè§å¯ä»¥é€šè¿‡æ˜ç¡®çš„æ¨ç†æ¶ˆé™¤ï¼Œä½†æƒ…æ„Ÿå› ç´ ç”±äºè¯†åˆ«æ€§èƒ½ä¸ä½³è€Œéš¾ä»¥å½±å“æ¨¡å‹å†³ç­–ã€‚</li>
<li>éŸ³é¢‘å¤§å‹è¯­è¨€æ¨¡å‹æ˜“äºæ ¹æ®æ‚£è€…çš„å£°éŸ³ç‰¹å¾è€ŒéåŒ»å­¦è¯æ®åšå‡ºå†³ç­–ã€‚</li>
<li>åœ¨éƒ¨ç½²ç”¨äºä¸´åºŠçš„å¤§å‹è¯­è¨€æ¨¡å‹ä¹‹å‰ï¼Œéœ€è¦è§£å†³æ¶æ„ä¸­çš„åè§é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06592">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-dd221133846182128b8cac8c3136b3f5" align="middle">
<img src="https://picx.zhimg.com/v2-92b6a70905bf3721516175b0bf31574f" align="middle">
<img src="https://picx.zhimg.com/v2-aed14b5e5e571a3aa78f022be73a1a05" align="middle">
<img src="https://picx.zhimg.com/v2-f3630edcfeaa637ccb0b6fd691a5f5fe" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="TalkSketch-Multimodal-Generative-AI-for-Real-time-Sketch-Ideation-with-Speech"><a href="#TalkSketch-Multimodal-Generative-AI-for-Real-time-Sketch-Ideation-with-Speech" class="headerlink" title="TalkSketch: Multimodal Generative AI for Real-time Sketch Ideation with Speech"></a>TalkSketch: Multimodal Generative AI for Real-time Sketch Ideation with Speech</h2><p><strong>Authors:Weiyan Shi, Sunaya Upadhyay, Geraldine Quek, Kenny Tsu Wei Choo</strong></p>
<p>Sketching is a widely used medium for generating and exploring early-stage design concepts. While generative AI (GenAI) chatbots are increasingly used for idea generation, designers often struggle to craft effective prompts and find it difficult to express evolving visual concepts through text alone. In the formative study (N&#x3D;6), we examined how designers use GenAI during ideation, revealing that text-based prompting disrupts creative flow. To address these issues, we developed TalkSketch, an embedded multimodal AI sketching system that integrates freehand drawing with real-time speech input. TalkSketch aims to support a more fluid ideation process through capturing verbal descriptions during sketching and generating context-aware AI responses. Our work highlights the potential of GenAI tools to engage the design process itself rather than focusing on output.</p>
<blockquote>
<p>è‰å›¾æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºç”Ÿæˆå’Œæ¢ç´¢æ—©æœŸè®¾è®¡æ¦‚å¿µçš„åª’ä»‹ã€‚è™½ç„¶ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰èŠå¤©æœºå™¨äººè¶Šæ¥è¶Šå¤šåœ°ç”¨äºåˆ›æ„ç”Ÿæˆï¼Œä½†è®¾è®¡å¸ˆå¾€å¾€éš¾ä»¥åˆ¶å®šæœ‰æ•ˆçš„æç¤ºï¼Œå¹¶ä¸”å‘ç°å¾ˆéš¾ä»…é€šè¿‡æ–‡å­—æ¥è¡¨è¾¾ä¸æ–­å˜åŒ–çš„è§†è§‰æ¦‚å¿µã€‚åœ¨å½¢æˆæ€§ç ”ç©¶ï¼ˆN&#x3D;6ï¼‰ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è®¾è®¡å¸ˆåœ¨åˆ›æ„äº§ç”Ÿè¿‡ç¨‹ä¸­å¦‚ä½•ä½¿ç”¨GenAIï¼Œå‘ç°åŸºäºæ–‡æœ¬çš„æç¤ºä¼šç ´ååˆ›æ„çš„æµåŠ¨ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†TalkSketchï¼Œè¿™æ˜¯ä¸€ä¸ªåµŒå…¥å¼å¤šæ¨¡å¼AIè‰å›¾ç³»ç»Ÿï¼Œå®ƒå°†è‡ªç”±æ‰‹ç»˜ä¸å®æ—¶è¯­éŸ³è¾“å…¥ç›¸ç»“åˆã€‚TalkSketchæ—¨åœ¨é€šè¿‡æ•è·ç´ æè¿‡ç¨‹ä¸­çš„è¨€è¯­æè¿°å¹¶ç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„AIå“åº”æ¥æ”¯æŒæ›´æµç•…çš„åˆ›æ„ç”Ÿæˆè¿‡ç¨‹ã€‚æˆ‘ä»¬çš„å·¥ä½œçªæ˜¾äº†GenAIå·¥å…·å‚ä¸è®¾è®¡è¿‡ç¨‹æœ¬èº«çš„æ½œåŠ›ï¼Œè€Œä¸æ˜¯ä»…ä»…å…³æ³¨è¾“å‡ºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05817v1">PDF</a> Accepted at AAAI 2026 Workshop on Creative AI for Live Interactive Performances (CLIP). To be published in Springer CCIS series</p>
<p><strong>Summary</strong></p>
<p>è¯¥ç ”ç©¶æ¢è®¨äº†è®¾è®¡å¸ˆåœ¨åˆ›æ„æ„æ€é˜¶æ®µä½¿ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰çš„æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬æ–‡æœ¬æç¤ºå¯¹åˆ›æ„æµç¨‹çš„å¹²æ‰°ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†TalkSketchç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿæ˜¯ä¸€ä¸ªåµŒå…¥å¼çš„å¤šæ¨¡å¼AIç´ æç³»ç»Ÿï¼Œç»“åˆäº†è‡ªç”±æ‰‹ç»˜å’Œå®æ—¶è¯­éŸ³è¾“å…¥ã€‚TalkSketchæ—¨åœ¨é€šè¿‡æ•æ‰ç´ æè¿‡ç¨‹ä¸­çš„è¯­è¨€æè¿°å’Œç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„AIå“åº”æ¥æ”¯æŒæ›´æµç•…çš„åˆ›æ„æ„æ€è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è®¾è®¡å¸ˆåœ¨ä½¿ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰è¿›è¡Œåˆ›æ„æ„æ€æ—¶é¢ä¸´æŒ‘æˆ˜ï¼Œæ–‡æœ¬æç¤ºä¼šå¹²æ‰°åˆ›æ„æµç¨‹ã€‚</li>
<li>TalkSketchç³»ç»Ÿæ˜¯ä¸€ä¸ªåµŒå…¥å¼å¤šæ¨¡å¼AIç´ æç³»ç»Ÿï¼Œç»“åˆäº†è‡ªç”±æ‰‹ç»˜å’Œå®æ—¶è¯­éŸ³è¾“å…¥ã€‚</li>
<li>TalkSketchæ—¨åœ¨æ•æ‰ç´ æè¿‡ç¨‹ä¸­çš„è¯­è¨€æè¿°ï¼Œå¹¶ç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„AIå“åº”ï¼Œä»¥æ”¯æŒæ›´æµç•…çš„åˆ›æ„æ„æ€è¿‡ç¨‹ã€‚</li>
<li>ç ”ç©¶é€šè¿‡ä¸€é¡¹å½¢æˆæ€§ç ”ç©¶ï¼ˆN&#x3D;6ï¼‰å‘ç°ï¼Œè®¾è®¡å¸ˆåœ¨ä½¿ç”¨GenAIæ—¶å­˜åœ¨é—®é¢˜å’Œéœ€æ±‚ï¼ŒéªŒè¯äº†TalkSketchç³»ç»Ÿçš„å¼€å‘èƒŒæ™¯å’Œæ½œåœ¨ä»·å€¼ã€‚</li>
<li>è¯¥ç ”ç©¶çªæ˜¾äº†GenAIå·¥å…·å‚ä¸è®¾è®¡è¿‡ç¨‹æœ¬èº«çš„é‡è¦æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯å…³æ³¨è¾“å‡ºã€‚</li>
<li>TalkSketchç³»ç»Ÿçš„å¼€å‘ä¸ºè§£å†³è®¾è®¡å¸ˆåœ¨ä½¿ç”¨GenAIæ—¶çš„ç—›ç‚¹æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05817">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fb37c56faaf91b060d4f61c5a594843e" align="middle">
<img src="https://picx.zhimg.com/v2-da6978921c351a93ee0a9d0779f26a23" align="middle">
<img src="https://picx.zhimg.com/v2-18be8d8f5d9c8790aed93e01c84d4054" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="Can-Current-Detectors-Catch-Face-to-Voice-Deepfake-Attacks"><a href="#Can-Current-Detectors-Catch-Face-to-Voice-Deepfake-Attacks" class="headerlink" title="Can Current Detectors Catch Face-to-Voice Deepfake Attacks?"></a>Can Current Detectors Catch Face-to-Voice Deepfake Attacks?</h2><p><strong>Authors:Nguyen Linh Bao Nguyen, Alsharif Abuadbba, Kristen Moore, Tingmin Wu</strong></p>
<p>The rapid advancement of generative models has enabled the creation of increasingly stealthy synthetic voices, commonly referred to as audio deepfakes. A recent technique, FOICE [USENIXâ€™24], demonstrates a particularly alarming capability: generating a victimâ€™s voice from a single facial image, without requiring any voice sample. By exploiting correlations between facial and vocal features, FOICE produces synthetic voices realistic enough to bypass industry-standard authentication systems, including WeChat Voiceprint and Microsoft Azure. This raises serious security concerns, as facial images are far easier for adversaries to obtain than voice samples, dramatically lowering the barrier to large-scale attacks. In this work, we investigate two core research questions: (RQ1) can state-of-the-art audio deepfake detectors reliably detect FOICE-generated speech under clean and noisy conditions, and (RQ2) whether fine-tuning these detectors on FOICE data improves detection without overfitting, thereby preserving robustness to unseen voice generators such as SpeechT5.   Our study makes three contributions. First, we present the first systematic evaluation of FOICE detection, showing that leading detectors consistently fail under both standard and noisy conditions. Second, we introduce targeted fine-tuning strategies that capture FOICE-specific artifacts, yielding significant accuracy improvements. Third, we assess generalization after fine-tuning, revealing trade-offs between specialization to FOICE and robustness to unseen synthesis pipelines. These findings expose fundamental weaknesses in todayâ€™s defenses and motivate new architectures and training protocols for next-generation audio deepfake detection.</p>
<blockquote>
<p>ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿå‘å±•ä½¿å¾—è¶Šæ¥è¶Šéšè”½çš„åˆæˆè¯­éŸ³å¾—ä»¥äº§ç”Ÿï¼Œé€šå¸¸è¢«ç§°ä¸ºéŸ³é¢‘æ·±åº¦ä¼ªé€ ã€‚æœ€è¿‘çš„ä¸€é¡¹æŠ€æœ¯FOICE[USENIXâ€™24]å±•ç¤ºäº†ä¸€ç§ç‰¹åˆ«ä»¤äººæ‹…å¿§çš„èƒ½åŠ›ï¼šä»…é€šè¿‡ä¸€å¼ é¢éƒ¨å›¾åƒç”Ÿæˆå—å®³è€…çš„å£°éŸ³ï¼Œè€Œæ— éœ€ä»»ä½•è¯­éŸ³æ ·æœ¬ã€‚FOICEé€šè¿‡åˆ©ç”¨é¢éƒ¨å’Œè¯­éŸ³ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œç”Ÿæˆäº†è¶³ä»¥ç»•è¿‡åŒ…æ‹¬å¾®ä¿¡è¯­éŸ³æ‰“å°å’Œå¾®è½¯Azureåœ¨å†…çš„è¡Œä¸šæ ‡å‡†è®¤è¯ç³»ç»Ÿçš„åˆæˆè¯­éŸ³ã€‚è¿™å¼•å‘äº†ä¸¥é‡çš„å®‰å…¨æ‹…å¿§ï¼Œå› ä¸ºç›¸å¯¹äºè¯­éŸ³æ ·æœ¬ï¼Œé¢éƒ¨å›¾åƒå¯¹äºå¯¹æ‰‹æ¥è¯´æ›´å®¹æ˜“è·å–ï¼Œä»è€Œå¤§å¤§é™ä½äº†å¤§è§„æ¨¡æ”»å‡»çš„é—¨æ§›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸¤ä¸ªæ ¸å¿ƒç ”ç©¶é—®é¢˜ï¼šï¼ˆRQ1ï¼‰æœ€å…ˆè¿›çš„éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨æ˜¯å¦èƒ½åœ¨å¹²å‡€å’Œå˜ˆæ‚çš„æ¡ä»¶ä¸‹å¯é åœ°æ£€æµ‹FOICEç”Ÿæˆçš„è¯­éŸ³ï¼›ï¼ˆRQ2ï¼‰åœ¨FOICEæ•°æ®ä¸Šå¯¹æ£€æµ‹å™¨è¿›è¡Œå¾®è°ƒæ˜¯å¦èƒ½åœ¨ä¸å‡ºç°è¿‡åº¦æ‹Ÿåˆçš„æƒ…å†µä¸‹æé«˜æ£€æµ‹èƒ½åŠ›ï¼Œä»è€Œä¿æŒå¯¹æœªè§è¯­éŸ³ç”Ÿæˆå™¨ï¼ˆå¦‚SpeechT5ï¼‰çš„ç¨³å¥æ€§ã€‚æˆ‘ä»¬çš„ç ”ç©¶åšå‡ºäº†ä¸‰é¡¹è´¡çŒ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯¹FOICEæ£€æµ‹è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿè¯„ä¼°ï¼Œè¡¨æ˜é¢†å…ˆçš„æ£€æµ‹å™¨åœ¨æ ‡å‡†æ¡ä»¶å’Œå˜ˆæ‚æ¡ä»¶ä¸‹éƒ½è¡¨ç°å‡ºæŒç»­çš„å¤±è´¥ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒç­–ç•¥ï¼Œä»¥æ•æ‰FOICEç‰¹å®šçš„ä¼ªè¿¹ï¼Œä»è€Œå¤§å¤§æé«˜äº†å‡†ç¡®æ€§ã€‚ç¬¬ä¸‰ï¼Œæˆ‘ä»¬è¯„ä¼°äº†å¾®è°ƒåçš„æ³›åŒ–èƒ½åŠ›ï¼Œæ­ç¤ºäº†é’ˆå¯¹FOICEçš„ä¸“ä¸šåŒ–ä¸å¯¹æœªè§åˆæˆç®¡é“ç¨³å¥æ€§ä¹‹é—´çš„æƒè¡¡ã€‚è¿™äº›å‘ç°æš´éœ²äº†å½“å‰é˜²å¾¡æ‰‹æ®µçš„æ ¹æœ¬å¼±ç‚¹ï¼Œå¹¶ä¸ºä¸‹ä¸€ä»£éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æä¾›äº†æ–°æ¶æ„å’ŒåŸ¹è®­åè®®çš„åŠ¨æœºã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.21004v2">PDF</a> 8 pages, Accepted at Workshop on AI for Cyber Threat Intelligence, co-located with ACSAC 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åä¸ºFOICEçš„æ–°éŸ³é¢‘æ·±åº¦ä¼ªé€ æŠ€æœ¯ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®å•å¼ é¢éƒ¨å›¾åƒç”Ÿæˆå—å®³è€…çš„å£°éŸ³ï¼Œå¹¶ç»•è¿‡è¡Œä¸šæ ‡å‡†çš„èº«ä»½éªŒè¯ç³»ç»Ÿï¼Œå¼•å‘ä¸¥é‡çš„å®‰å…¨æ‹…å¿§ã€‚ç ”ç©¶å›¢é˜Ÿé’ˆå¯¹è¯¥é—®é¢˜è¿›è¡Œäº†è°ƒæŸ¥ï¼Œå‘ç°ç°æœ‰éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨åœ¨å¹²å‡€å’Œå™ªå£°æ¡ä»¶ä¸‹å‡æ— æ³•å¯é æ£€æµ‹FOICEç”Ÿæˆçš„è¯­éŸ³ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å¼•å…¥äº†æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒç­–ç•¥ï¼Œä»¥æ•è·FOICEç‰¹æœ‰çš„ç‰¹å¾ï¼Œä»è€Œæé«˜æ£€æµ‹å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œç ”ç©¶ä¹Ÿå‘ç°å¾®è°ƒåçš„æ£€æµ‹å™¨åœ¨åº”å¯¹æœªçŸ¥åˆæˆç®¡é“æ—¶å­˜åœ¨æƒè¡¡é—®é¢˜ã€‚å› æ­¤ï¼Œæœ¬æ–‡æ­ç¤ºäº†å½“å‰é˜²å¾¡æœºåˆ¶çš„å¼±ç‚¹ï¼Œå¹¶ä¸ºä¸‹ä¸€ä»£éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹æå‡ºäº†æ–°çš„æ¶æ„å’ŒåŸ¹è®­åè®®ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>FOICEæŠ€æœ¯èƒ½ä»å•å¼ é¢éƒ¨å›¾åƒç”Ÿæˆé€¼çœŸå£°éŸ³ï¼Œç»•è¿‡è¡Œä¸šæ ‡å‡†éªŒè¯ç³»ç»Ÿã€‚</li>
<li>ç°æœ‰éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨æ— æ³•å¯é æ£€æµ‹FOICEç”Ÿæˆçš„è¯­éŸ³ï¼Œæ— è®ºåœ¨å¹²å‡€æˆ–å™ªå£°ç¯å¢ƒä¸‹ã€‚</li>
<li>é€šè¿‡æœ‰é’ˆå¯¹æ€§çš„å¾®è°ƒç­–ç•¥ï¼Œèƒ½æœ‰æ•ˆæé«˜æ£€æµ‹FOICEç”Ÿæˆè¯­éŸ³çš„å‡†ç¡®æ€§ã€‚</li>
<li>å­˜åœ¨ä¸€å®šæƒè¡¡é—®é¢˜ï¼šå¾®è°ƒåçš„æ£€æµ‹å™¨åœ¨é¢å¯¹æœªçŸ¥åˆæˆç®¡é“æ—¶å¯èƒ½ä¸§å¤±ç¨³å¥æ€§ã€‚</li>
<li>å½“å‰é˜²å¾¡æœºåˆ¶å­˜åœ¨å¼±ç‚¹ï¼Œéœ€è¦æ–°çš„æ¶æ„å’ŒåŸ¹è®­åè®®æ¥æé«˜éŸ³é¢‘æ·±åº¦ä¼ªé€ æ£€æµ‹çš„æ•ˆèƒ½ã€‚</li>
<li>å¯¹æŠ—æ·±åº¦ä¼ªé€ æŠ€æœ¯çš„å®‰å…¨åº”å¯¹ç­–ç•¥éœ€è¦è€ƒè™‘é˜²èŒƒæœºåˆ¶çš„å…¨é¢æ€§å’Œç¨³å¥æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.21004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-9cc792f54c21782b0e019c256fdbecbf" align="middle">
<img src="https://picx.zhimg.com/v2-91f1cdbfbca4a049813e2c62ad00f049" align="middle">
<img src="https://picx.zhimg.com/v2-a18fddfba727227943a5884a1f330969" align="middle">
<img src="https://picx.zhimg.com/v2-4b8475bd0234c82d8b55573729ec477c" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="OmniMotion-X-Versatile-Multimodal-Whole-Body-Motion-Generation"><a href="#OmniMotion-X-Versatile-Multimodal-Whole-Body-Motion-Generation" class="headerlink" title="OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation"></a>OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</h2><p><strong>Authors:Guowei Xu, Yuxuan Bian, Ailing Zeng, Mingyi Shi, Shaoli Huang, Wen Li, Lixin Duan, Qiang Xu</strong></p>
<p>This paper introduces OmniMotion-X, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks, including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint&#x2F;trajectory-guided synthesis), as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct OmniMoCap-X, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.</p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†OmniMotion-Xï¼Œè¿™æ˜¯ä¸€ä¸ªé€šç”¨å¤šæ¨¡å¼æ¡†æ¶ï¼Œä»¥ç»Ÿä¸€åºåˆ—åˆ°åºåˆ—çš„æ–¹å¼ï¼Œé‡‡ç”¨è‡ªå›å½’æ‰©æ•£å˜å‹å™¨ï¼Œç”¨äºç”Ÿæˆå…¨èº«äººä½“è¿åŠ¨ã€‚OmniMotion-Xé«˜æ•ˆæ”¯æŒå¤šç§å¤šæ¨¡å¼ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°è¿åŠ¨ã€éŸ³ä¹åˆ°èˆè¹ˆã€è¯­éŸ³åˆ°æ‰‹åŠ¿ï¼Œä»¥åŠå…¨å±€æ—¶ç©ºæ§åˆ¶åœºæ™¯ï¼ˆå¦‚è¿åŠ¨é¢„æµ‹ã€ä¸­é—´å¸§ç”Ÿæˆã€è¡¥å…¨å’Œå…³èŠ‚&#x2F;è½¨è¿¹å¼•å¯¼åˆæˆç­‰ï¼‰ï¼Œä»¥åŠè¿™äº›ä»»åŠ¡çš„çµæ´»ç»„åˆã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨å‚è€ƒè¿åŠ¨ä½œä¸ºæ–°å‹æ¡ä»¶ä¿¡å·ï¼Œè¿™å¤§å¤§æé«˜äº†ç”Ÿæˆå†…å®¹çš„ä¸€è‡´æ€§ã€é£æ ¼å’Œæ—¶ç©ºåŠ¨æ€ï¼Œå¯¹äºç°å®åŠ¨ç”»è‡³å…³é‡è¦ã€‚ä¸ºäº†è§£å†³å¤šæ¨¡å¼å†²çªï¼Œæˆ‘ä»¬å¼•å…¥äº†ä»å¼±åˆ°å¼ºçš„æ¸è¿›å¼æ··åˆæ¡ä»¶è®­ç»ƒç­–ç•¥ã€‚ä¸ºäº†è¿›è¡Œé«˜è´¨é‡çš„å¤šæ¨¡å¼è®­ç»ƒï¼Œæˆ‘ä»¬æ„å»ºäº†è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ç»Ÿä¸€å¤šæ¨¡å¼è¿åŠ¨æ•°æ®é›†OmniMoCap-Xï¼Œé›†æˆäº†10ä¸ªä¸åŒä»»åŠ¡çš„28ä¸ªå…¬å¼€å¯ç”¨çš„MoCapæºï¼Œä»¥æ ‡å‡†åŒ–çš„SMPL-Xæ ¼å¼å’Œ30fpsçš„é¢‘ç‡å‘ˆç°ã€‚ä¸ºäº†ç¡®ä¿è¯¦ç»†å’Œä¸€è‡´çš„æ³¨é‡Šï¼Œæˆ‘ä»¬å°†åºåˆ—å‘ˆç°ä¸ºè§†é¢‘ï¼Œå¹¶ä½¿ç”¨GPT-4oè‡ªåŠ¨ç”Ÿæˆç»“æ„å’Œå±‚æ¬¡åŒ–çš„å­—å¹•ï¼Œæ•æ‰ä½çº§åˆ«åŠ¨ä½œå’Œé«˜çº§åˆ«è¯­ä¹‰ã€‚å¹¿æ³›çš„å®éªŒè¯„ä¼°è¯å®ï¼ŒOmniMotion-Xæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œåœ¨å¤šä¸ªå¤šæ¨¡å¼ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œèƒ½å¤Ÿå®ç°äº¤äº’å¼ç”Ÿæˆé€¼çœŸã€è¿è´¯å’Œå¯æ§çš„é•¿æœŸè¿åŠ¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19789v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>OmniMotion-Xæ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½æ¨¡æ€æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå…¨èº«äººä½“è¿åŠ¨ã€‚å®ƒé‡‡ç”¨è‡ªå›å½’æ‰©æ•£å˜å‹å™¨ï¼Œä»¥ç»Ÿä¸€åºåˆ—åˆ°åºåˆ—çš„æ–¹å¼æ”¯æŒå¤šç§ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ°è¿åŠ¨ã€éŸ³ä¹åˆ°èˆè¹ˆç­‰ã€‚OmniMotion-Xå¼•å…¥å‚è€ƒè¿åŠ¨ä½œä¸ºæ–°å‹æ¡ä»¶ä¿¡å·ï¼Œå¢å¼ºäº†ç”Ÿæˆå†…å®¹çš„è¿è´¯æ€§ã€é£æ ¼å’Œæ—¶ç©ºåŠ¨æ€ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨æ¸è¿›çš„å¼±åˆ°å¼ºæ··åˆæ¡ä»¶è®­ç»ƒç­–ç•¥å¤„ç†å¤šæ¨¡æ€å†²çªï¼Œå¹¶æ„å»ºOmniMoCap-Xæ•°æ®é›†ç”¨äºé«˜è´¨é‡å¤šæ¨¡æ€è®­ç»ƒã€‚å®éªŒè¯æ˜ï¼ŒOmniMotion-Xåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°å“è¶Šï¼Œå¯å®ç°äº¤äº’ç”ŸæˆçœŸå®ã€è¿è´¯ä¸”å¯æ§çš„é•¿æœŸè¿åŠ¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>OmniMotion-Xæ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½æ¨¡æ€æ¡†æ¶ï¼Œç”¨äºå…¨èº«äººä½“è¿åŠ¨ç”Ÿæˆã€‚</li>
<li>æ”¯æŒå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°è¿åŠ¨ã€éŸ³ä¹åˆ°èˆè¹ˆç­‰ã€‚</li>
<li>å¼•å…¥å‚è€ƒè¿åŠ¨ä½œä¸ºæ¡ä»¶ä¿¡å·ï¼Œå¢å¼ºäº†ç”Ÿæˆå†…å®¹çš„è¿è´¯æ€§ã€é£æ ¼å’Œæ—¶ç©ºåŠ¨æ€ã€‚</li>
<li>é‡‡ç”¨æ¸è¿›çš„å¼±åˆ°å¼ºæ··åˆæ¡ä»¶è®­ç»ƒç­–ç•¥å¤„ç†å¤šæ¨¡æ€å†²çªã€‚</li>
<li>æ„å»ºOmniMoCap-Xæ•°æ®é›†ç”¨äºé«˜è´¨é‡å¤šæ¨¡æ€è®­ç»ƒï¼Œé›†æˆäº†å¤šä¸ªMoCapæºã€‚</li>
<li>ä½¿ç”¨GPT-4oè‡ªåŠ¨ç”Ÿæˆç»“æ„åŒ–å±‚æ¬¡åŒ–çš„å­—å¹•ï¼Œç¡®ä¿è¯¦ç»†ä¸”ä¸€è‡´çš„æ³¨é‡Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.19789">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8620922f467f05bcb80397391e2305da" align="middle">
<img src="https://picx.zhimg.com/v2-57532a697cc4f46df80ef75a411a2df3" align="middle">
<img src="https://picx.zhimg.com/v2-8b0d91ef5ff10cd1d2d878dad9bd4218" align="middle">
<img src="https://picx.zhimg.com/v2-fb0a823b10c5bd3402fd3b3c4484f13b" align="middle">
<img src="https://picx.zhimg.com/v2-51703277325e66fd250c8c10191b6214" align="middle">
<img src="https://picx.zhimg.com/v2-53a2c15f6aa1999027940424e088e41e" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="ImaGGen-Zero-Shot-Generation-of-Co-Speech-Semantic-Gestures-Grounded-in-Language-and-Image-Input"><a href="#ImaGGen-Zero-Shot-Generation-of-Co-Speech-Semantic-Gestures-Grounded-in-Language-and-Image-Input" class="headerlink" title="ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input"></a>ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input</h2><p><strong>Authors:Hendric Voss, Stefan Kopp</strong></p>
<p>Human communication combines speech with expressive nonverbal cues such as hand gestures that serve manifold communicative functions. Yet, current generative gesture generation approaches are restricted to simple, repetitive beat gestures that accompany the rhythm of speaking but do not contribute to communicating semantic meaning. This paper tackles a core challenge in co-speech gesture synthesis: generating iconic or deictic gestures that are semantically coherent with a verbal utterance. Such gestures cannot be derived from language input alone, which inherently lacks the visual meaning that is often carried autonomously by gestures. We therefore introduce a zero-shot system that generates gestures from a given language input and additionally is informed by imagistic input, without manual annotation or human intervention. Our method integrates an image analysis pipeline that extracts key object properties such as shape, symmetry, and alignment, together with a semantic matching module that links these visual details to spoken text. An inverse kinematics engine then synthesizes iconic and deictic gestures and combines them with co-generated natural beat gestures for coherent multimodal communication. A comprehensive user study demonstrates the effectiveness of our approach. In scenarios where speech alone was ambiguous, gestures generated by our system significantly improved participantsâ€™ ability to identify object properties, confirming their interpretability and communicative value. While challenges remain in representing complex shapes, our results highlight the importance of context-aware semantic gestures for creating expressive and collaborative virtual agents or avatars, marking a substantial step forward towards efficient and robust, embodied human-agent interaction. More information and example videos are available here: <a target="_blank" rel="noopener" href="https://review-anon-io.github.io/ImaGGen.github.io/">https://review-anon-io.github.io/ImaGGen.github.io/</a></p>
<blockquote>
<p>äººç±»äº¤æµç»“åˆäº†è¨€è¯­å’Œè¡¨è¾¾æ€§çš„éè¨€è¯­çº¿ç´¢ï¼Œå¦‚æ‰‹åŠ¿ç­‰ï¼Œè¿™äº›çº¿ç´¢èµ·ç€å¤šç§äº¤æµåŠŸèƒ½ã€‚ç„¶è€Œï¼Œå½“å‰çš„ç”Ÿæˆæ‰‹åŠ¿ç”Ÿæˆæ–¹æ³•ä»…é™äºä¼´éšè¯´è¯èŠ‚å¥çš„ç®€å•é‡å¤èŠ‚æ‹æ‰‹åŠ¿ï¼Œä½†å¹¶ä¸æœ‰åŠ©äºä¼ è¾¾è¯­ä¹‰æ„ä¹‰ã€‚æœ¬æ–‡è§£å†³äº†ååŒè¯­éŸ³æ‰‹åŠ¿åˆæˆä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼šç”Ÿæˆä¸å£å¤´è¡¨è¾¾è¯­ä¹‰ä¸Šè¿è´¯çš„è±¡å¾æ€§æˆ–æŒ‡ç¤ºæ€§æ‰‹åŠ¿ã€‚è¿™ç§æ‰‹åŠ¿ä¸èƒ½ä»…ä»…ä»è¯­è¨€è¾“å…¥ä¸­å¾—å‡ºï¼Œå› ä¸ºè¯­è¨€è¾“å…¥æœ¬èº«ç¼ºä¹é€šå¸¸ç”±æ‰‹åŠ¿è‡ªä¸»æºå¸¦çš„è§†è§‰æ„ä¹‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªé›¶æ ·æœ¬ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå¯ä»¥æ ¹æ®è¯­è¨€è¾“å…¥ç”Ÿæˆæ‰‹åŠ¿ï¼Œå¹¶é¢å¤–ç”±å›¾åƒè¾“å…¥æä¾›ä¿¡æ¯ï¼Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šæˆ–äººå·¥å¹²é¢„ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ•´åˆäº†ä¸€ä¸ªå›¾åƒåˆ†æç®¡é“ï¼Œå¯ä»¥æå–å…³é”®å¯¹è±¡å±æ€§ï¼Œå¦‚å½¢çŠ¶ã€å¯¹ç§°æ€§å’Œå¯¹é½æ–¹å¼ï¼Œä»¥åŠä¸€ä¸ªè¯­ä¹‰åŒ¹é…æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†è¿™äº›è§†è§‰ç»†èŠ‚ä¸å£è¯­æ–‡æœ¬è”ç³»èµ·æ¥ã€‚ç„¶åï¼Œé€†å‘è¿åŠ¨å­¦å¼•æ“åˆæˆè±¡å¾æ€§å’ŒæŒ‡ç¤ºæ€§æ‰‹åŠ¿ï¼Œå¹¶ä¸å…±åŒç”Ÿæˆçš„è‡ªç„¶èŠ‚æ‹æ‰‹åŠ¿ç›¸ç»“åˆï¼Œå®ç°è¿è´¯çš„å¤šæ¨¡å¼äº¤æµã€‚ä¸€é¡¹å…¨é¢çš„ç”¨æˆ·ç ”ç©¶è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚åœ¨ä»…ä½¿ç”¨è¯­éŸ³çš„åœºæ™¯ä¸­ï¼Œç”±æˆ‘ä»¬çš„ç³»ç»Ÿç”Ÿæˆçš„æ‰‹åŠ¿æ˜¾è‘—æé«˜äº†å‚ä¸è€…è¯†åˆ«å¯¹è±¡å±æ€§çš„èƒ½åŠ›ï¼Œè¯å®äº†å…¶å¯è§£é‡Šæ€§å’Œäº¤æµä»·å€¼ã€‚è™½ç„¶å¯¹è¡¨ç¤ºå¤æ‚å½¢çŠ¶ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†æˆ‘ä»¬çš„ç»“æœçªæ˜¾äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯­ä¹‰æ‰‹åŠ¿å¯¹äºåˆ›å»ºè¡¨è¾¾æ€§å’Œåä½œæ€§è™šæ‹Ÿä»£ç†äººæˆ–åŒ–èº«çš„é‡è¦æ€§ï¼Œæœç€é«˜æ•ˆå’Œç¨³å¥çš„å®ä½“äººç±»ä»£ç†äº¤äº’è¿ˆå‡ºäº†å®è´¨æ€§çš„æ­¥ä¼ã€‚æ›´å¤šä¿¡æ¯å’Œç¤ºä¾‹è§†é¢‘å¯åœ¨ä»¥ä¸‹ç½‘å€æ‰¾åˆ°ï¼š<a target="_blank" rel="noopener" href="https://review-anon-io.github.io/ImaGGen.github.io/">https://review-anon-io.github.io/ImaGGen.github.io/</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.17617v1">PDF</a> </p>
<p><strong>æ‘˜è¦</strong></p>
<p>æœ¬æ–‡æ¢è®¨äº†ç”Ÿæˆä¸è¯­éŸ³ç›¸åè°ƒçš„æ‰‹åŠ¿çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼Œå¹¶ä»‹ç»äº†ä¸€ç§é›¶æ ·æœ¬ç³»ç»Ÿï¼Œèƒ½å¤Ÿä»ç»™å®šçš„è¯­è¨€è¾“å…¥ä¸­äº§ç”Ÿæ‰‹åŠ¿ï¼ŒåŒæ—¶è¿˜èƒ½å¤Ÿä¸å—æ‰‹åŠ¨æ ‡æ³¨æˆ–äººå·¥å¹²é¢„åœ°ç»“åˆå›¾åƒè¾“å…¥ä¿¡æ¯ã€‚é€šè¿‡æ•´åˆå›¾åƒåˆ†æç®¡é“ï¼Œè¯¥ç³»ç»Ÿèƒ½å¤Ÿæå–å…³é”®çš„å¯¹è±¡å±æ€§ï¼ˆå¦‚å½¢çŠ¶ã€å¯¹ç§°æ€§å’Œå¯¹é½æ–¹å¼ï¼‰ï¼Œå¹¶ä¸è¯­ä¹‰åŒ¹é…æ¨¡å—ç»“åˆï¼Œå°†è¿™äº›è§†è§‰ç»†èŠ‚ä¸å£å¤´æ–‡æœ¬è”ç³»èµ·æ¥ã€‚é€šè¿‡é€†å‘åŠ¨åŠ›å­¦å¼•æ“åˆæˆçš„æ ‡å¿—æ€§æˆ–æŒ‡ç¤ºæ€§æ‰‹åŠ¿ä¸å…±åŒç”Ÿæˆçš„å¸¸è§„èŠ‚å¥æ‰‹åŠ¿ç›¸ç»“åˆï¼Œå®ç°äº†è¿è´¯çš„å¤šæ¨¡å¼é€šä¿¡ã€‚ç»¼åˆç”¨æˆ·ç ”ç©¶è¯æ˜äº†è¯¥ç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚åœ¨è¯­éŸ³æ¨¡ç³Šçš„æƒ…å¢ƒä¸­ï¼Œæœ¬ç³»ç»Ÿç”Ÿæˆçš„æ‰‹åŠ¿æ˜¾è‘—æé«˜äº†å‚ä¸è€…è¯†åˆ«å¯¹è±¡å±æ€§çš„èƒ½åŠ›ï¼ŒéªŒè¯äº†å…¶è§£é‡Šèƒ½åŠ›å’Œæ²Ÿé€šä»·å€¼ã€‚è™½ç„¶è¡¨ç¤ºå¤æ‚å½¢çŠ¶ä»å­˜åœ¨æŒ‘æˆ˜ï¼Œä½†æœ¬ç ”ç©¶ç»“æœå¼ºè°ƒäº†åœ¨åˆ›å»ºè¡¨è¾¾æ€§å’Œåä½œæ€§è™šæ‹Ÿä»£ç†æˆ–åŒ–èº«æ—¶ï¼Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯­ä¹‰æ‰‹åŠ¿çš„é‡è¦æ€§ï¼Œæœç€é«˜æ•ˆã€ç¨³å¥çš„æ‹Ÿäººä»£ç†äº¤äº’è¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚</p>
<p><strong>å…³é”®è§è§£</strong></p>
<ol>
<li>å½“å‰çš„æ‰‹åŠ¿ç”Ÿæˆæ–¹æ³•ä¸»è¦å±€é™äºä¼´éšè¯´è¯èŠ‚å¥çš„ç®€å•é‡å¤åŠ¨ä½œï¼Œç¼ºä¹è¯­ä¹‰æ„ä¹‰çš„è´¡çŒ®ã€‚</li>
<li>è®ºæ–‡æå‡ºäº†ä¸€ç§é›¶æ ·æœ¬ç³»ç»Ÿï¼Œç»“åˆäº†è¯­è¨€è¾“å…¥å’Œå›¾åƒè¾“å…¥æ¥ç”Ÿæˆæ‰‹åŠ¿ï¼Œæ—¨åœ¨ç”Ÿæˆä¸å£å¤´è¡¨è¾¾è¯­ä¹‰ä¸Šåè°ƒçš„æ ‡å¿—æ€§æˆ–æŒ‡ç¤ºæ€§æ‰‹åŠ¿ã€‚</li>
<li>ç³»ç»Ÿé€šè¿‡å›¾åƒåˆ†ææå–å¯¹è±¡çš„å…³é”®å±æ€§ï¼Œå¹¶é€šè¿‡è¯­ä¹‰åŒ¹é…æ¨¡å—å°†è¿™äº›å±æ€§ä¸å£å¤´æ–‡æœ¬è”ç³»èµ·æ¥ã€‚</li>
<li>é€†å‘åŠ¨åŠ›å­¦å¼•æ“ç”¨äºåˆæˆæ‰‹åŠ¿ï¼Œå¹¶ç»“åˆç”Ÿæˆçš„è‡ªç„¶èŠ‚å¥æ‰‹åŠ¿ï¼Œä»¥å®ç°è¿è´¯çš„å¤šæ¨¡å¼é€šä¿¡ã€‚</li>
<li>ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œåœ¨è¯­éŸ³æ¨¡ç³Šçš„æƒ…å¢ƒä¸­ï¼Œç³»ç»Ÿç”Ÿæˆçš„æ‰‹åŠ¿æ˜¾è‘—æé«˜äº†å‚ä¸è€…è¯†åˆ«å¯¹è±¡å±æ€§çš„èƒ½åŠ›ã€‚</li>
<li>ç»“æœå¼ºè°ƒäº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯­ä¹‰æ‰‹åŠ¿åœ¨åˆ›å»ºè™šæ‹Ÿä»£ç†æˆ–åŒ–èº«æ—¶çš„é‡è¦æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2510.17617">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-6fc2e5218a86c3f890b46f2b50493a74" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="WEST-LLM-based-Speech-Toolkit-for-Speech-Understanding-Generation-and-Interaction"><a href="#WEST-LLM-based-Speech-Toolkit-for-Speech-Understanding-Generation-and-Interaction" class="headerlink" title="WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction"></a>WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction</h2><p><strong>Authors:Binbin Zhang, Chengdong Liang, Shuai Wang, Xuelong Geng, Zhao Guo, Haoyu Li, Hao Yin, Xipeng Yang, Pengshen Zhang, Changwei Ma, Lei Xie</strong></p>
<p>In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on a large language model (LLM) for speech understanding, generation, and interaction. There are three key features of WEST: 1) Fully LLM-based: Standing on the shoulders of giants by reusing mature architectures, ecosystems (e.g., Hugging Face), and methods (e.g., sequence packing) from large models. 2) Full-stack: Supports tasks such as recognition, synthesis, understanding, dialogue, and multimodal capabilities, with extensibility to incorporate open-source models. 3) Simple and Stupid: A simple and stupid speech toolkit that everyone can Touch. In addition, WEST provides two types of recipes, models, and experimental results. The first is entirely based on open-source models and open-source data, allowing users to fully reproduce the experiments in this paper and serving as a verification system or minimal system baseline. The second is trained on massive data, offering superior performance so the user can directly apply it out of the box. WEST is publicly avilable at <a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/west/">https://github.com/wenet-e2e/west/</a></p>
<blockquote>
<p>æœ¬æ–‡ä»‹ç»äº†WESTï¼ˆWEè¯­éŸ³å·¥å…·åŒ…ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¯­éŸ³å·¥å…·åŒ…ï¼Œç”¨äºè¯­éŸ³ç†è§£ã€ç”Ÿæˆå’Œäº¤äº’ã€‚WESTæœ‰ä¸‰ä¸ªå…³é”®ç‰¹ç‚¹ï¼š1ï¼‰å®Œå…¨åŸºäºLLMï¼šåˆ©ç”¨å¤§å‹æ¨¡å‹çš„æˆç†Ÿæ¶æ„ã€ç”Ÿæ€ç³»ç»Ÿï¼ˆä¾‹å¦‚Hugging Faceï¼‰å’Œæ–¹æ³•ï¼ˆä¾‹å¦‚åºåˆ—æ‰“åŒ…ï¼‰ç«™åœ¨å·¨äººçš„è‚©è†€ä¸Šã€‚2ï¼‰å…¨æ ˆæ”¯æŒï¼šæ”¯æŒè¯†åˆ«ã€åˆæˆã€ç†è§£ã€å¯¹è¯å’Œå¤šæ¨¡å¼åŠŸèƒ½ç­‰ä»»åŠ¡ï¼Œå¯æ‰©å±•ä»¥èå…¥å¼€æºæ¨¡å‹ã€‚3ï¼‰ç®€å•å®ç”¨ï¼šä¸€ä¸ªç®€å•å®ç”¨çš„è¯­éŸ³å·¥å…·åŒ…ï¼Œæ¯ä¸ªäººéƒ½èƒ½è½»æ¾ä½¿ç”¨ã€‚æ­¤å¤–ï¼ŒWESTæä¾›ä¸¤ç§ç±»å‹çš„é…æ–¹ã€æ¨¡å‹å’Œå®éªŒç»“æœã€‚ç¬¬ä¸€ç§å®Œå…¨åŸºäºå¼€æºæ¨¡å‹å’Œå¼€æºæ•°æ®ï¼Œå…è®¸ç”¨æˆ·å……åˆ†å¤ç°æœ¬æ–‡ä¸­çš„å®éªŒï¼Œå¹¶ä½œä¸ºéªŒè¯ç³»ç»Ÿæˆ–æœ€å°ç³»ç»ŸåŸºå‡†ã€‚ç¬¬äºŒç§æ˜¯åœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒçš„ï¼Œæä¾›å“è¶Šæ€§èƒ½ï¼Œç”¨æˆ·å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚WESTå¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/wenet-e2e/west/%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/wenet-e2e/west/å…¬å¼€è·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.19902v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>WESTï¼ˆWEè¯­éŸ³å·¥å…·åŒ…ï¼‰æ˜¯ä¸€æ¬¾åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­éŸ³å·¥å…·åŒ…ï¼Œå…·æœ‰å…¨æ ˆåŠŸèƒ½ï¼Œæ”¯æŒè¯­éŸ³è¯†åˆ«ã€åˆæˆã€ç†è§£å’Œå¯¹è¯ç­‰ä»»åŠ¡ï¼ŒåŒæ—¶æä¾›ç®€å•æ˜“ç”¨çš„ç‰¹æ€§ã€‚å®ƒå»ºç«‹åœ¨æˆç†Ÿçš„æ¶æ„ã€ç”Ÿæ€ç³»ç»Ÿå’Œæ–¹æ³•ä¹‹ä¸Šï¼Œå¹¶æä¾›ä¸¤ç§ç±»å‹çš„æ¨¡å‹ä¸å®éªŒç»“æœä¾›ç”¨æˆ·é€‰æ‹©ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>WESTæ˜¯åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„è¯­éŸ³å·¥å…·åŒ…ã€‚</li>
<li>å®ƒå…·æœ‰å…¨æ ˆåŠŸèƒ½ï¼Œæ”¯æŒè¯­éŸ³è¯†åˆ«ã€åˆæˆã€ç†è§£å’Œå¯¹è¯ç­‰ä»»åŠ¡ã€‚</li>
<li>WESTåˆ©ç”¨ç°æœ‰çš„æˆç†Ÿæ¶æ„ã€ç”Ÿæ€ç³»ç»Ÿå’Œæ–¹æ³•ï¼Œå¦‚Hugging Faceå’Œåºåˆ—æ‰“åŒ…æŠ€æœ¯ã€‚</li>
<li>å®ƒæä¾›ä¸¤ç§ç±»å‹çš„æ¨¡å‹ä¸å®éªŒç»“æœä¾›ç”¨æˆ·é€‰æ‹©ï¼Œä¸€ç§å®Œå…¨åŸºäºå¼€æºæ¨¡å‹å’Œæ•°æ®è¿›è¡ŒéªŒè¯æˆ–ä½œä¸ºæœ€å°ç³»ç»ŸåŸºçº¿ï¼Œå¦ä¸€ç§åœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒï¼Œæä¾›å“è¶Šæ€§èƒ½ã€‚</li>
<li>WESTå…·æœ‰ç®€å•æ˜“æ‡‚çš„ç‰¹ç‚¹ï¼Œæ˜“äºä½¿ç”¨ã€‚</li>
<li>WESTæ˜¯å¼€æºçš„ï¼Œç”¨æˆ·å¯ä»¥åœ¨GitHubä¸Šè·å–å’Œä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19902">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-c3d8b5751e736acfe2c12b1606366ed9" align="middle">
<img src="https://picx.zhimg.com/v2-a7aebdefc68a0afeb8f510e10a338f37" align="middle">
<img src="https://picx.zhimg.com/v2-9b028003bfa661bbe008b2eeec6f9ddc" align="middle">
<img src="https://picx.zhimg.com/v2-7014297b4d924d8489747d92b0aed1d0" align="middle">
<img src="https://picx.zhimg.com/v2-7857ceef61c98497d19470cfe0ceec6e" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="Say-More-with-Less-Variable-Frame-Rate-Speech-Tokenization-via-Adaptive-Clustering-and-Implicit-Duration-Coding"><a href="#Say-More-with-Less-Variable-Frame-Rate-Speech-Tokenization-via-Adaptive-Clustering-and-Implicit-Duration-Coding" class="headerlink" title="Say More with Less: Variable-Frame-Rate Speech Tokenization via Adaptive Clustering and Implicit Duration Coding"></a>Say More with Less: Variable-Frame-Rate Speech Tokenization via Adaptive Clustering and Implicit Duration Coding</h2><p><strong>Authors:Rui-Chen Zheng, Wenrui Liu, Hui-Peng Du, Qinglin Zhang, Chong Deng, Qian Chen, Wen Wang, Yang Ai, Zhen-Hua Ling</strong></p>
<p>Existing speech tokenizers typically assign a fixed number of tokens per second, regardless of the varying information density or temporal fluctuations in the speech signal. This uniform token allocation mismatches the intrinsic structure of speech, where information is distributed unevenly over time. To address this, we propose VARSTok, a VAriable-frame-Rate Speech Tokenizer that adapts token allocation based on local feature similarity. VARSTok introduces two key innovations: (1) a temporal-aware density peak clustering algorithm that adaptively segments speech into variable-length units, and (2) a novel implicit duration coding scheme that embeds both content and temporal span into a single token index, eliminating the need for auxiliary duration predictors. Extensive experiments show that VARSTok significantly outperforms strong fixed-rate baselines. Notably, it achieves superior reconstruction naturalness while using up to 23% fewer tokens than a 40 Hz fixed-frame-rate baseline. VARSTok further yields lower word error rates and improved naturalness in zero-shot text-to-speech synthesis. To the best of our knowledge, this is the first work to demonstrate that a fully dynamic, variable-frame-rate acoustic speech tokenizer can be seamlessly integrated into downstream speech language models.</p>
<blockquote>
<p>ç°æœ‰çš„è¯­éŸ³åˆ†è¯å™¨é€šå¸¸æ¯ç§’åˆ†é…å›ºå®šæ•°é‡çš„æ ‡è®°ï¼Œè€Œä¸è€ƒè™‘è¯­éŸ³ä¿¡å·ä¸­ä¿¡æ¯å¯†åº¦æˆ–æ—¶é—´æ³¢åŠ¨çš„å˜åŒ–ã€‚è¿™ç§ç»Ÿä¸€çš„æ ‡è®°åˆ†é…ä¸è¯­éŸ³çš„å†…åœ¨ç»“æ„ä¸åŒ¹é…ï¼Œè¯­éŸ³ä¸­çš„ä¿¡æ¯åœ¨æ—¶é—´ä¸Šæ˜¯åˆ†å¸ƒä¸å‡çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†VARSTokï¼Œè¿™æ˜¯ä¸€ç§å¯å˜å¸§ç‡è¯­éŸ³åˆ†è¯å™¨ï¼Œå®ƒå¯ä»¥æ ¹æ®å±€éƒ¨ç‰¹å¾ç›¸ä¼¼æ€§æ¥é€‚åº”æ ‡è®°åˆ†é…ã€‚VARSTokå¼•å…¥äº†ä¸¤ä¸ªå…³é”®åˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰ä¸€ç§æ—¶é—´æ„ŸçŸ¥å¯†åº¦å³°å€¼èšç±»ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥è‡ªé€‚åº”åœ°å°†è¯­éŸ³åˆ†å‰²æˆå¯å˜é•¿åº¦çš„å•å…ƒï¼›ï¼ˆ2ï¼‰ä¸€ç§æ–°çš„éšå¼æŒç»­æ—¶é—´ç¼–ç æ–¹æ¡ˆï¼Œå°†å†…å®¹å’Œæ—¶é—´è·¨åº¦åµŒå…¥å•ä¸ªæ ‡è®°ç´¢å¼•ä¸­ï¼Œä»è€Œæ— éœ€è¾…åŠ©æŒç»­æ—¶é—´é¢„æµ‹å™¨ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒVARSTokæ˜¾è‘—ä¼˜äºå¼ºå¤§çš„å›ºå®šé€Ÿç‡åŸºçº¿ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨ä½¿ç”¨é«˜è¾¾23%æ›´å°‘æ ‡è®°çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æ¯”40Hzå›ºå®šå¸§ç‡åŸºçº¿æ›´ä¼˜è¶Šçš„é‡å»ºè‡ªç„¶åº¦ã€‚æ­¤å¤–ï¼ŒVARSTokè¿˜é™ä½äº†é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆçš„è¯é”™è¯¯ç‡ï¼Œå¹¶æé«˜äº†è‡ªç„¶åº¦ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€é¡¹å·¥ä½œï¼Œå±•ç¤ºäº†å¯ä»¥æ— ç¼é›†æˆåˆ°ä¸‹æ¸¸è¯­éŸ³è¯­è¨€æ¨¡å‹ä¸­çš„å®Œå…¨åŠ¨æ€ã€å¯å˜å¸§ç‡çš„å£°å­¦è¯­éŸ³åˆ†è¯å™¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.04685v3">PDF</a> Accepted to AAAI 2026. Project page: <a target="_blank" rel="noopener" href="https://zhengrachel.github.io/VARSTok">https://zhengrachel.github.io/VARSTok</a></p>
<p><strong>Summary</strong><br>è¯­éŸ³ä¿¡å·ä¸­çš„ä¿¡æ¯åˆ†å¸ƒä¸å‡ï¼Œç°æœ‰çš„è¯­éŸ³åˆ†è¯å™¨é€šå¸¸ä»¥å›ºå®šçš„å¸§ç‡åˆ†é…ä»¤ç‰Œï¼Œæ— æ³•åŒ¹é…è¯­éŸ³çš„å†…åœ¨ç»“æ„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†VARSTokâ€”â€”ä¸€ç§åŸºäºå±€éƒ¨ç‰¹å¾ç›¸ä¼¼æ€§çš„å¯å˜å¸§ç‡è¯­éŸ³åˆ†è¯å™¨ã€‚å®ƒå¼•å…¥äº†ä¸¤é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼šè‡ªé€‚åº”åˆ†å‰²è¯­éŸ³ä¸ºå¯å˜é•¿åº¦å•å…ƒçš„åŸºäºæ—¶é—´æ„ŸçŸ¥å¯†åº¦å³°å€¼èšç±»ç®—æ³•ï¼Œä»¥åŠå°†å†…å®¹å’Œæ—¶é—´è·¨åº¦åµŒå…¥å•ä¸ªä»¤ç‰Œç´¢å¼•ä¸­çš„æ–°é¢–éšå¼æŒç»­æ—¶é—´ç¼–ç æ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼ŒVARSTokæ˜¾è‘—ä¼˜äºå›ºå®šå¸§ç‡åŸºçº¿ï¼Œå¹¶åœ¨ä¸ä½¿ç”¨è¾…åŠ©æŒç»­æ—¶é—´é¢„æµ‹å™¨çš„æƒ…å†µä¸‹å®ç°äº†æ›´è‡ªç„¶çš„é‡å»ºæ•ˆæœã€‚æ­¤å¤–ï¼Œå®ƒåœ¨é›¶æ ·æœ¬æ–‡æœ¬åˆ°è¯­éŸ³åˆæˆä¸­é™ä½äº†å•è¯é”™è¯¯ç‡å¹¶æé«˜äº†è‡ªç„¶åº¦ã€‚è¿™æ˜¯é¦–ä¸ªæˆåŠŸå°†å®Œå…¨åŠ¨æ€çš„å¯å˜å¸§ç‡å£°å­¦è¯­éŸ³åˆ†è¯å™¨æ— ç¼é›†æˆåˆ°ä¸‹æ¸¸è¯­éŸ³è¯­è¨€æ¨¡å‹ä¸­çš„å·¥ä½œã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>ç°æœ‰è¯­éŸ³åˆ†è¯å™¨ä»¥å›ºå®šå¸§ç‡åˆ†é…ä»¤ç‰Œï¼Œæ— æ³•åŒ¹é…è¯­éŸ³ä¿¡æ¯çš„ä¸å‡åŒ€åˆ†å¸ƒã€‚</li>
<li>VARSTokæå‡ºåŸºäºå±€éƒ¨ç‰¹å¾ç›¸ä¼¼æ€§çš„å¯å˜å¸§ç‡è¯­éŸ³åˆ†è¯å™¨æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚</li>
<li>VARSTokå¼•å…¥ä¸¤é¡¹å…³é”®æŠ€æœ¯åˆ›æ–°ï¼šè‡ªé€‚åº”åˆ†å‰²è¯­éŸ³å’Œéšå¼æŒç»­æ—¶é—´ç¼–ç æ–¹æ¡ˆã€‚</li>
<li>VARSTokæ˜¾è‘—ä¼˜äºå›ºå®šå¸§ç‡åŸºçº¿ï¼Œå®ç°äº†æ›´è‡ªç„¶çš„é‡å»ºæ•ˆæœï¼Œå¹¶é™ä½äº†å•è¯é”™è¯¯ç‡ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04685">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-01775d3d0f12854b0e5926f2654a93af" align="middle">
<img src="https://picx.zhimg.com/v2-c97fa59f031e855e3a623aef5e3a6ba8" align="middle">
<img src="https://picx.zhimg.com/v2-7040c975bc00c031ea7784b4579b8469" align="middle">
<img src="https://picx.zhimg.com/v2-4ea66b9259a4c96abe334a7df6b592c9" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1><h2 id="Generative-Annotation-for-ASR-Named-Entity-Correction"><a href="#Generative-Annotation-for-ASR-Named-Entity-Correction" class="headerlink" title="Generative Annotation for ASR Named Entity Correction"></a>Generative Annotation for ASR Named Entity Correction</h2><p><strong>Authors:Yuanchang Luo, Daimeng Wei, Shaojun Li, Hengchao Shang, Jiaxin Guo, Zongyao Li, Zhanglin Wu, Xiaoyu Chen, Zhiqiang Rao, Jinlong Yang, Hao Yang</strong></p>
<p>End-to-end automatic speech recognition systems often fail to transcribe domain-specific named entities, causing catastrophic failures in downstream tasks. Numerous fast and lightweight named entity correction (NEC) models have been proposed in recent years. These models, mainly leveraging phonetic-level edit distance algorithms, have shown impressive performances. However, when the forms of the wrongly-transcribed words(s) and the ground-truth entity are significantly different, these methods often fail to locate the wrongly transcribed words in hypothesis, thus limiting their usage. We propose a novel NEC method that utilizes speech sound features to retrieve candidate entities. With speech sound features and candidate entities, we inovatively design a generative method to annotate entity errors in ASR transcripts and replace the text with correct entities. This method is effective in scenarios of word form difference. We test our method using open-source and self-constructed test sets. The results demonstrate that our NEC method can bring significant improvement to entity accuracy. The self-constructed training data and test set is publicly available at github.com&#x2F;L6-NLP&#x2F;Generative-Annotation-NEC.</p>
<blockquote>
<p>ç«¯åˆ°ç«¯çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå¾€å¾€æ— æ³•è½¬å½•ç‰¹å®šé¢†åŸŸçš„å‘½åå®ä½“ï¼Œå¯¼è‡´ä¸‹æ¸¸ä»»åŠ¡å‡ºç°é‡å¤§å¤±è´¥ã€‚è¿‘å¹´æ¥ï¼Œå·²ç»æå‡ºäº†è®¸å¤šå¿«é€Ÿä¸”è½»é‡çº§çš„å‘½åå®ä½“æ ¡æ­£ï¼ˆNECï¼‰æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹ä¸»è¦åˆ©ç”¨éŸ³ç´ çº§ç¼–è¾‘è·ç¦»ç®—æ³•ï¼Œè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ•ˆæœã€‚ç„¶è€Œï¼Œå½“é”™è¯¯è½¬å½•çš„å•è¯ï¼ˆsï¼‰çš„å½¢å¼ä¸åœ°é¢çœŸå®å®ä½“æœ‰å¾ˆå¤§å·®å¼‚æ—¶ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€æ— æ³•æ‰¾åˆ°å‡è®¾ä¸­çš„é”™è¯¯è½¬å½•å•è¯ï¼Œä»è€Œé™åˆ¶äº†å®ƒä»¬çš„ä½¿ç”¨ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°å‹çš„NECæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è¯­éŸ³å£°éŸ³ç‰¹å¾æ¥æ£€ç´¢å€™é€‰å®ä½“ã€‚é€šè¿‡è¯­éŸ³å£°éŸ³ç‰¹å¾å’Œå€™é€‰å®ä½“ï¼Œæˆ‘ä»¬åˆ›æ–°åœ°è®¾è®¡äº†ä¸€ç§ç”Ÿæˆæ–¹æ³•æ¥æ ‡æ³¨ASRè½¬å½•ä¸­çš„å®ä½“é”™è¯¯ï¼Œå¹¶ç”¨æ­£ç¡®çš„å®ä½“æ›¿æ¢æ–‡æœ¬ã€‚è¿™ç§æ–¹æ³•åœ¨å•è¯å½¢å¼å·®å¼‚è¾ƒå¤§çš„æƒ…å†µä¸‹éå¸¸æœ‰æ•ˆã€‚æˆ‘ä»¬ä½¿ç”¨å¼€æºå’Œè‡ªæˆ‘æ„å»ºçš„æµ‹è¯•é›†æµ‹è¯•äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„NECæ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜å®ä½“å‡†ç¡®æ€§ã€‚è‡ªæˆ‘æ„å»ºçš„è®­ç»ƒæ•°æ®å’Œæµ‹è¯•é›†å·²åœ¨github.com&#x2F;L6-NLP&#x2F;Generative-Annotation-NECä¸Šå…¬å¼€å¯ç”¨ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.20700v2">PDF</a> 12 pages, 7 figures, 7 tables, EMNLP 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸæå‡ºçš„è½»é‡çº§å‘½åå®ä½“æ ¡æ­£ï¼ˆNECï¼‰æ¨¡å‹ä¸»è¦åˆ©ç”¨éŸ³ç´ çº§ç¼–è¾‘è·ç¦»ç®—æ³•ï¼Œåœ¨è½¬å½•é”™è¯¯çš„å•è¯ä¸çœŸå®å®ä½“å½¢å¼å·®å¼‚ä¸å¤§æ—¶è¡¨ç°ä¼˜å¼‚ã€‚ä½†å½“ä¸¤è€…å·®å¼‚æ˜¾è‘—æ—¶ï¼Œè¿™äº›æ¨¡å‹å¾€å¾€æ— æ³•æ‰¾åˆ°é”™è¯¯çš„è½¬å½•è¯ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹NECæ–¹æ³•ï¼Œåˆ©ç”¨è¯­éŸ³å£°éŸ³ç‰¹å¾æ£€ç´¢å€™é€‰å®ä½“ï¼Œå¹¶é‡‡ç”¨ç”Ÿæˆæ€§æ–¹æ³•ä¸ºASRè½¬å½•ä¸­çš„å®ä½“é”™è¯¯è¿›è¡Œæ ‡æ³¨å’Œæ›¿æ¢ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å•è¯å½¢å¼å·®å¼‚åœºæ™¯ä¸‹æœ‰æ•ˆï¼Œæ˜¾è‘—æé«˜å®ä½“å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿå¯¹äºç‰¹å®šé¢†åŸŸçš„å‘½åå®ä½“è½¬å½•å­˜åœ¨æŒ‘æˆ˜ã€‚</li>
<li>å½“å‰è½»é‡çº§NECæ¨¡å‹ä¸»è¦åˆ©ç”¨éŸ³ç´ çº§ç¼–è¾‘è·ç¦»ç®—æ³•ï¼Œä½†åœ¨å•è¯å½¢å¼å·®å¼‚å¤§æ—¶æ•ˆæœä¸ä½³ã€‚</li>
<li>æœ¬æ–‡æå‡ºä¸€ç§æ–°å‹NECæ–¹æ³•ï¼Œç»“åˆè¯­éŸ³å£°éŸ³ç‰¹å¾å’Œå€™é€‰å®ä½“è¿›è¡Œå®ä½“é”™è¯¯æ ‡æ³¨å’Œæ›¿æ¢ã€‚</li>
<li>è¯¥æ–¹æ³•é€šè¿‡ç”Ÿæˆæ€§ç­–ç•¥å¤„ç†å®ä½“é”™è¯¯ï¼Œé€‚ç”¨äºä¸åŒåœºæ™¯ã€‚</li>
<li>å…¬å¼€æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜å®ä½“å‡†ç¡®æ€§ã€‚</li>
<li>å…¬å¼€äº†è‡ªæˆ‘æ„å»ºçš„è®­ç»ƒæ•°æ®å’Œæµ‹è¯•é›†ä»¥ä¾›ç ”ç©¶ä½¿ç”¨ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20700">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-cfaaedf7f7767722701b4b1b62fc8c1f" align="middle">
<img src="https://picx.zhimg.com/v2-bde27c5e641ab3c7cd5c2b55a4d901ad" align="middle">
<img src="https://picx.zhimg.com/v2-14be47090bd4e4c9955ceb6e53a0065e" align="middle">
<img src="https://picx.zhimg.com/v2-7a1de5154a4421fb9be5da56f31c02b1" align="middle">
<img src="https://picx.zhimg.com/v2-a879249d63bab16be3c428e6e3a6282b" align="middle">
<img src="https://picx.zhimg.com/v2-bc36b093636486f1405b19faea1c48a5" align="middle">
</details>


<h1 id="-16"><a href="#-16" class="headerlink" title=""></a></h1><h2 id="HM-Talker-Hybrid-Motion-Modeling-for-High-Fidelity-Talking-Head-Synthesis"><a href="#HM-Talker-Hybrid-Motion-Modeling-for-High-Fidelity-Talking-Head-Synthesis" class="headerlink" title="HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis"></a>HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis</h2><p><strong>Authors:Shiyu Liu, Kui Jiang, Xianming Liu, Hongxun Yao, Xiaocheng Feng</strong></p>
<p>Audio-driven talking head video generation enhances user engagement in human-computer interaction. However, current methods frequently produce videos with motion blur and lip jitter, primarily due to their reliance on implicit modeling of audio-facial motion correlationsâ€“an approach lacking explicit articulatory priors (i.e., anatomical guidance for speech-related facial movements). To overcome this limitation, we propose HM-Talker, a novel framework for generating high-fidelity, temporally coherent talking heads. HM-Talker leverages a hybrid motion representation combining both implicit and explicit motion cues. Explicit cues use Action Units (AUs), anatomically defined facial muscle movements, alongside implicit features to minimize phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement Module (CMDM) extracts complementary implicit&#x2F;explicit motion features while predicting AUs directly from audio input aligned to visual cues. To mitigate identity-dependent biases in explicit features and enhance cross-subject generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This module dynamically merges randomly paired implicit&#x2F;explicit features, enforcing identity-agnostic learning. Together, these components enable robust lip synchronization across diverse identities, advancing personalized talking head synthesis. Extensive experiments demonstrate HM-Talkerâ€™s superiority over state-of-the-art methods in visual quality and lip-sync accuracy.</p>
<blockquote>
<p>éŸ³é¢‘é©±åŠ¨çš„è¯´è¯äººè§†é¢‘ç”Ÿæˆæé«˜äº†äººæœºäº¤äº’ä¸­çš„ç”¨æˆ·å‚ä¸åº¦ã€‚ç„¶è€Œï¼Œå½“å‰çš„æ–¹æ³•ç»å¸¸äº§ç”Ÿè¿åŠ¨æ¨¡ç³Šå’Œå”‡éƒ¨æŠ–åŠ¨çš„è§†é¢‘ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬ä¾èµ–äºéŸ³é¢‘é¢éƒ¨è¿åŠ¨å…³è”çš„éšå¼å»ºæ¨¡â€”â€”è¿™ç§æ–¹æ³•ç¼ºä¹æ˜ç¡®çš„å‘éŸ³å…ˆéªŒçŸ¥è¯†ï¼ˆå³ä¸è¯­éŸ³ç›¸å…³çš„é¢éƒ¨è¿åŠ¨çš„è§£å‰–æŒ‡å¯¼ï¼‰ã€‚ä¸ºäº†å…‹æœè¿™ä¸€å±€é™æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†HM-Talkerï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºç”Ÿæˆé«˜ä¿çœŸã€æ—¶é—´è¿è´¯çš„è¯´è¯äººå¤´éƒ¨çš„å…¨æ–°æ¡†æ¶ã€‚HM-Talkeråˆ©ç”¨äº†ä¸€ç§æ··åˆè¿åŠ¨è¡¨ç¤ºæ³•ï¼Œç»“åˆäº†éšå¼å’Œæ˜¾å¼è¿åŠ¨çº¿ç´¢ã€‚æ˜¾å¼çº¿ç´¢ä½¿ç”¨åŠ¨ä½œå•å…ƒï¼ˆAUsï¼‰ï¼Œå³é¢éƒ¨è§£å‰–ä¸Šå®šä¹‰çš„è‚Œè‚‰è¿åŠ¨ï¼Œä»¥åŠéšå¼ç‰¹å¾æ¥æœ€å°åŒ–éŸ³ç´ -è¡¨æƒ…å¤±é…ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„è·¨æ¨¡æ€åˆ†ç¦»æ¨¡å—ï¼ˆCMDMï¼‰æå–äº’è¡¥çš„éšå¼&#x2F;æ˜¾å¼è¿åŠ¨ç‰¹å¾ï¼ŒåŒæ—¶ç›´æ¥ä»ä¸è§†è§‰çº¿ç´¢å¯¹é½çš„éŸ³é¢‘è¾“å…¥ä¸­é¢„æµ‹åŠ¨ä½œå•å…ƒã€‚ä¸ºäº†å‡å°‘æ˜¾å¼ç‰¹å¾ä¸­çš„èº«ä»½ç›¸å…³åè§å¹¶å¢å¼ºè·¨ä¸»ä½“æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ··åˆè¿åŠ¨å»ºæ¨¡æ¨¡å—ï¼ˆHMMMï¼‰ã€‚è¯¥æ¨¡å—åŠ¨æ€åˆå¹¶éšæœºé…å¯¹çš„éšå¼&#x2F;æ˜¾å¼ç‰¹å¾ï¼Œå¼ºåˆ¶å®æ–½èº«ä»½æ— å…³çš„å­¦ä¹ ã€‚è¿™äº›ç»„ä»¶å…±åŒä½œç”¨ï¼Œå®ç°äº†è·¨ä¸åŒèº«ä»½çš„ç¨³å¥å”‡éƒ¨åŒæ­¥ï¼Œæ¨åŠ¨äº†ä¸ªæ€§åŒ–è¯´è¯äººå¤´éƒ¨çš„åˆæˆå‘å±•ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒHM-Talkeråœ¨è§†è§‰è´¨é‡å’Œå”‡éƒ¨åŒæ­¥å‡†ç¡®æ€§æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.10566v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºä¸€ç§åä¸ºHM-Talkerçš„æ–°å‹æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆé«˜ä¿çœŸã€æ—¶é—´è¿è´¯çš„è¯´è¯äººå¤´åƒè§†é¢‘ã€‚è¯¥æ¡†æ¶ç»“åˆäº†éšå¼å’Œæ˜¾å¼è¿åŠ¨çº¿ç´¢çš„æ··åˆè¿åŠ¨è¡¨ç¤ºï¼Œä½¿ç”¨åŠ¨ä½œå•å…ƒï¼ˆAUsï¼‰ç­‰è§£å‰–å®šä¹‰çš„é¢éƒ¨è‚Œè‚‰è¿åŠ¨æ¥æœ€å°åŒ–éŸ³ç´ ä¸è¡¨æƒ…ä¹‹é—´çš„é”™ä½ã€‚é€šè¿‡è·¨æ¨¡æ€è§£è€¦æ¨¡å—ï¼ˆCMDMï¼‰æå–éšå¼å’Œæ˜¾å¼è¿åŠ¨ç‰¹å¾çš„äº’è¡¥ä¿¡æ¯ï¼Œå¹¶ä»éŸ³é¢‘è¾“å…¥ä¸­ç›´æ¥é¢„æµ‹ä¸è§†è§‰çº¿ç´¢å¯¹é½çš„åŠ¨ä½œå•å…ƒã€‚åŒæ—¶ï¼Œå¼•å…¥æ··åˆè¿åŠ¨å»ºæ¨¡æ¨¡å—ï¼ˆHMMMï¼‰æ¥ç¼“è§£èº«ä»½ç›¸å…³çš„åè§ï¼Œå¢å¼ºè·¨ä¸»ä½“æ³›åŒ–èƒ½åŠ›ã€‚è¯¥æ¡†æ¶å¯å¹¿æ³›åº”ç”¨äºä¸ªæ€§åŒ–è¯´è¯äººå¤´åƒåˆæˆï¼Œå®ç°é«˜è´¨é‡çš„è§†é¢‘ç”Ÿæˆå’Œå”‡åŒæ­¥å‡†ç¡®æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>HM-Talkeræ¡†æ¶ç»“åˆäº†éšå¼å’Œæ˜¾å¼è¿åŠ¨çº¿ç´¢ï¼Œä»¥æé«˜è¯´è¯äººå¤´åƒè§†é¢‘ç”Ÿæˆçš„è´¨é‡ã€‚</li>
<li>æ˜¾å¼è¿åŠ¨çº¿ç´¢ä½¿ç”¨åŠ¨ä½œå•å…ƒï¼ˆAUsï¼‰ï¼ŒåŸºäºè§£å‰–å®šä¹‰çš„é¢éƒ¨è‚Œè‚‰è¿åŠ¨ï¼Œä»¥æé«˜å”‡åŒæ­¥çš„å‡†ç¡®æ€§ã€‚</li>
<li>è·¨æ¨¡æ€è§£è€¦æ¨¡å—ï¼ˆCMDMï¼‰èƒ½å¤Ÿæå–éšå¼å’Œæ˜¾å¼è¿åŠ¨ç‰¹å¾çš„äº’è¡¥ä¿¡æ¯ï¼Œä»è€Œä¼˜åŒ–è§†é¢‘ç”Ÿæˆã€‚</li>
<li>HM-Talkeré€šè¿‡é¢„æµ‹ä¸è§†è§‰çº¿ç´¢å¯¹é½çš„éŸ³é¢‘è¾“å…¥ä¸­çš„åŠ¨ä½œå•å…ƒï¼Œæé«˜äº†éŸ³é¢‘é©±åŠ¨çš„è§†é¢‘ç”Ÿæˆçš„è‡ªç„¶åº¦ã€‚</li>
<li>æ··åˆè¿åŠ¨å»ºæ¨¡æ¨¡å—ï¼ˆHMMMï¼‰èƒ½å¤Ÿç¼“è§£èº«ä»½ç›¸å…³çš„åè§ï¼Œå¢å¼ºäº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥å¤„ç†ä¸åŒèº«ä»½çš„è¯´è¯äººå¤´åƒåˆæˆã€‚</li>
<li>HM-Talkeråœ¨è§†è§‰è´¨é‡å’Œå”‡åŒæ­¥å‡†ç¡®æ€§æ–¹é¢ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.10566">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-ca59ea4c6cec848c51ba9e0202a6f93d" align="middle">
<img src="https://picx.zhimg.com/v2-40ba1f350c6a5ffe7bc08e3f4e2e2217" align="middle">
<img src="https://picx.zhimg.com/v2-63ac38f4ef71ce02b0f52513267ddb22" align="middle">
<img src="https://picx.zhimg.com/v2-89c0fba6949d44d082d9bd511e4d327c" align="middle">
<img src="https://picx.zhimg.com/v2-c7f0a8842fca7a54c0a776066ddaef55" align="middle">
</details>


<h1 id="-17"><a href="#-17" class="headerlink" title=""></a></h1><h2 id="MiDashengLM-Efficient-Audio-Understanding-with-General-Audio-Captions"><a href="#MiDashengLM-Efficient-Audio-Understanding-with-General-Audio-Captions" class="headerlink" title="MiDashengLM: Efficient Audio Understanding with General Audio Captions"></a>MiDashengLM: Efficient Audio Understanding with General Audio Captions</h2><p><strong>Authors:Heinrich Dinkel, Gang Li, Jizhong Liu, Jian Luan, Yadong Niu, Xingwei Sun, Tianzi Wang, Qiyang Xiao, Junbo Zhang, Jiahao Zhou</strong></p>
<p>Current approaches for large audio language models (LALMs) often rely on closed data sources or proprietary models, limiting their generalization and accessibility. This paper introduces MiDashengLM, a novel open audio-language model designed for efficient and comprehensive audio understanding through the use of general audio captions using our novel ACAVCaps training dataset. MiDashengLM exclusively relies on publicly available pretraining and supervised fine-tuning (SFT) datasets, ensuring full transparency and reproducibility. At its core, MiDashengLM integrates Dasheng, an open-source audio encoder, specifically engineered to process diverse auditory information effectively. Unlike previous works primarily focused on Automatic Speech Recognition (ASR) based audio-text alignment, our strategy centers on general audio captions, fusing speech, sound and music information into one textual representation, enabling a holistic textual representation of complex audio scenes. Lastly, MiDashengLM provides an up to 4x speedup in terms of time-to-first-token (TTFT) and up to 20x higher throughput than comparable models. Checkpoints are available online at <a target="_blank" rel="noopener" href="https://huggingface.co/mispeech/midashenglm-7b">https://huggingface.co/mispeech/midashenglm-7b</a> and <a target="_blank" rel="noopener" href="https://github.com/xiaomi-research/dasheng-lm">https://github.com/xiaomi-research/dasheng-lm</a>.</p>
<blockquote>
<p>å½“å‰çš„å¤§å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼ˆLALMï¼‰æ–¹æ³•å¾€å¾€ä¾èµ–äºå°é—­çš„æ•°æ®æºæˆ–ä¸“æœ‰æ¨¡å‹ï¼Œè¿™é™åˆ¶äº†å…¶é€šç”¨æ€§å’Œå¯è®¿é—®æ€§ã€‚æœ¬æ–‡ä»‹ç»äº†MiDashengLMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹å¼€æ”¾éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ä½¿ç”¨æˆ‘ä»¬åˆ›æ–°æ€§çš„ACAVCapsè®­ç»ƒæ•°æ®é›†è¿›è¡Œé€šç”¨éŸ³é¢‘å­—å¹•ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆä¸”å…¨é¢çš„éŸ³é¢‘ç†è§£ã€‚MiDashengLMä»…ä¾èµ–äºå…¬å¼€å¯ç”¨çš„é¢„è®­ç»ƒå’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é›†ï¼Œç¡®ä¿å®Œå…¨é€æ˜å’Œå¯é‡å¤æ€§ã€‚å…¶æ ¸å¿ƒç»“åˆäº†Dashengè¿™ä¸€å¼€æºéŸ³é¢‘ç¼–ç å™¨ï¼Œä¸“é—¨ç”¨äºæœ‰æ•ˆå¤„ç†å„ç§å¬è§‰ä¿¡æ¯ã€‚ä¸ä¹‹å‰ä¸»è¦å…³æ³¨åŸºäºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰çš„éŸ³é¢‘æ–‡æœ¬å¯¹é½çš„ç ”ç©¶ä¸åŒï¼Œæˆ‘ä»¬çš„ç­–ç•¥ä¾§é‡äºé€šç”¨éŸ³é¢‘å­—å¹•ï¼Œå°†è¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ä¿¡æ¯èåˆåˆ°ä¸€ä¸ªæ–‡æœ¬è¡¨ç¤ºä¸­ï¼Œå®ç°å¯¹å¤æ‚éŸ³é¢‘åœºæ™¯çš„æ•´ä½“æ–‡æœ¬è¡¨ç¤ºã€‚æœ€åï¼ŒMiDashengLMåœ¨é¦–æ¬¡ä»¤ç‰Œæ—¶é—´ï¼ˆTTFTï¼‰æ–¹é¢æä¾›äº†é«˜è¾¾4å€çš„åŠ é€Ÿï¼Œå¹¶ä¸”ååé‡æ¯”åŒç±»äº§å“é«˜è¾¾20å€ã€‚æ£€æŸ¥ç‚¹æ•°æ®å¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/mispeech/midashenglm-7b%E5%92%8Chttps://github.com/xiaomi-research/dasheng-lm%E5%9C%A8%E7%BA%BF%E8%8E%B7%E5%BE%97%E3%80%82">https://huggingface.co/mispeech/midashenglm-7bå’Œhttps://github.com/xiaomi-research/dasheng-lmåœ¨çº¿è·å¾—ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.03983v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>MiDashengLMæ˜¯ä¸€ä¸ªåŸºäºå…¬å¼€é¢„è®­ç»ƒå’Œç²¾ç»†è°ƒè¯•æ•°æ®é›†çš„æ–°å‹éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆå…¨é¢çš„éŸ³é¢‘ç†è§£ã€‚è¯¥æ¨¡å‹ä½¿ç”¨é€šç”¨çš„éŸ³é¢‘å­—å¹•ACAVCapsè®­ç»ƒæ•°æ®é›†ï¼Œä¸åŒäºä¾èµ–ç‰¹å®šæ•°æ®æºæˆ–ä¸“æœ‰æ¨¡å‹çš„ç°æœ‰æ–¹æ³•ï¼Œå…·æœ‰æ›´å¥½çš„æ³›åŒ–å’Œå¯è®¿é—®æ€§ã€‚å…¶æ ¸å¿ƒç»“åˆäº†å¼€æºéŸ³é¢‘ç¼–ç å™¨Dashengï¼Œèƒ½æœ‰æ•ˆå¤„ç†å„ç§å¬è§‰ä¿¡æ¯ã€‚æ­¤å¤–ï¼ŒMiDashengLMé‡‡ç”¨é€šç”¨çš„éŸ³é¢‘å­—å¹•æ–¹æ³•ï¼Œå°†è¯­éŸ³ã€å£°éŸ³å’ŒéŸ³ä¹ä¿¡æ¯èåˆåˆ°æ–‡æœ¬è¡¨ç¤ºä¸­ï¼Œä¸ºå¤æ‚çš„éŸ³é¢‘åœºæ™¯æä¾›å…¨é¢çš„æ–‡æœ¬è¡¨ç¤ºã€‚æœ€åï¼ŒMiDashengLMæä¾›äº†æ—¶é—´è‡³ç¬¬ä¸€æ ‡è®°çš„æœ€å¤šå¯è¾¾4å€çš„åŠ é€Ÿï¼Œå¹¶å…·æœ‰æ›´é«˜çš„ååé‡ã€‚å…¶ä¸»è¦ç‰¹æ€§åŠå…¶æŠ€æœ¯è´¡çŒ®ç»†èŠ‚å…¬å¼€å¯ä¾›ä½¿ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>MiDashengLMæ˜¯ä¸€ä¸ªé‡‡ç”¨å…¬å¼€æ•°æ®æºå»ºç«‹çš„éŸ³é¢‘è¯­è¨€æ¨¡å‹ï¼Œç¡®ä¿äº†å…¶æ³›åŒ–å’Œå¯è®¿é—®æ€§ã€‚</li>
<li>å®ƒç»“åˆäº†å¼€æºéŸ³é¢‘ç¼–ç å™¨Dashengï¼Œæé«˜äº†å¤„ç†å¤šæ ·å¬è§‰ä¿¡æ¯çš„èƒ½åŠ›ã€‚</li>
<li>è¯¥æ¨¡å‹å¼•å…¥äº†æ–°çš„ACAVCapsè®­ç»ƒæ•°æ®é›†ï¼Œä¸“æ³¨äºé€šç”¨éŸ³é¢‘å­—å¹•çš„åº”ç”¨åœºæ™¯ã€‚</li>
<li>MiDashengLMå®ç°äº†å¤æ‚çš„éŸ³é¢‘åœºæ™¯çš„å…¨é¢æ–‡æœ¬è¡¨ç¤ºã€‚ä¸ä»…èåˆäº†è¯­éŸ³ä¿¡æ¯ï¼Œè¿˜åŒ…æ‹¬å£°éŸ³å’ŒéŸ³ä¹ç­‰ç»†èŠ‚ä¿¡æ¯ã€‚</li>
<li>è¯¥æ¨¡å‹æé«˜äº†éŸ³é¢‘å¤„ç†çš„æ•ˆç‡ï¼Œä¸åŒç±»æ¨¡å‹ç›¸æ¯”ï¼Œæ—¶é—´è‡³ç¬¬ä¸€æ ‡è®°çš„åŠ é€Ÿè¾¾åˆ°æœ€é«˜4å€ï¼Œååé‡æé«˜æœ€é«˜è¾¾20å€ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.03983">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-2e37337f9abb41aa7c4e63b8ae10331c" align="middle">
<img src="https://picx.zhimg.com/v2-2c0e11fb8976c04eb43489994f04d7c9" align="middle">
<img src="https://picx.zhimg.com/v2-9ef6a5f5d7ceb7f73e56b094400723bf" align="middle">
<img src="https://picx.zhimg.com/v2-c0dc06c5f2be19ac784c4dd6086ec922" align="middle">
<img src="https://picx.zhimg.com/v2-b5c1921345c64b85390c1096caba949c" align="middle">
<img src="https://picx.zhimg.com/v2-4db33b1d59e2ab45c1c29d909f9b7e01" align="middle">
</details>


<h1 id="-18"><a href="#-18" class="headerlink" title=""></a></h1><h2 id="Hearing-More-with-Less-Multi-Modal-Retrieval-and-Selection-Augmented-Conversational-LLM-Based-ASR"><a href="#Hearing-More-with-Less-Multi-Modal-Retrieval-and-Selection-Augmented-Conversational-LLM-Based-ASR" class="headerlink" title="Hearing More with Less: Multi-Modal Retrieval-and-Selection Augmented Conversational LLM-Based ASR"></a>Hearing More with Less: Multi-Modal Retrieval-and-Selection Augmented Conversational LLM-Based ASR</h2><p><strong>Authors:Bingshen Mu, Hexin Liu, Hongfei Xue, Kun Wei, Lei Xie</strong></p>
<p>Automatic Speech Recognition (ASR) aims to convert human speech content into corresponding text. In conversational scenarios, effectively utilizing context can enhance its accuracy. Large Language Modelsâ€™ (LLMs) exceptional long-context understanding and reasoning abilities enable LLM-based ASR (LLM-ASR) to leverage historical context for recognizing conversational speech, which has a high degree of contextual relevance. However, existing conversational LLM-ASR methods use a fixed number of preceding utterances or the entire conversation history as context, resulting in significant ASR confusion and computational costs due to massive irrelevant and redundant information. This paper proposes a multi-modal retrieval-and-selection method named MARS that augments conversational LLM-ASR by enabling it to retrieve and select the most relevant acoustic and textual historical context for the current utterance. Specifically, multi-modal retrieval obtains a set of candidate historical contexts, each exhibiting high acoustic or textual similarity to the current utterance. Multi-modal selection calculates the acoustic and textual similarities for each retrieved candidate historical context and, by employing our proposed near-ideal ranking method to consider both similarities, selects the best historical context. Evaluations on the Interspeech 2025 Multilingual Conversational Speech Language Model Challenge dataset show that the LLM-ASR, when trained on only 1.5K hours of data and equipped with the MARS, outperforms the state-of-the-art top-ranking system trained on 179K hours of data.</p>
<blockquote>
<p>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ—¨åœ¨å°†äººç±»è¯­éŸ³å†…å®¹è½¬æ¢ä¸ºç›¸åº”çš„æ–‡æœ¬ã€‚åœ¨å¯¹è¯åœºæ™¯ä¸­ï¼Œæœ‰æ•ˆåˆ©ç”¨ä¸Šä¸‹æ–‡å¯ä»¥æé«˜å…¶å‡†ç¡®æ€§ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å‡ºè‰²çš„é•¿æ–‡æœ¬ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œä½¿å¾—åŸºäºLLMçš„ASRï¼ˆLLM-ASRï¼‰èƒ½å¤Ÿåˆ©ç”¨å†å²ä¸Šä¸‹æ–‡æ¥è¯†åˆ«å¯¹è¯è¯­éŸ³ï¼Œè¿™å¯¹é«˜åº¦ç›¸å…³çš„ä¸Šä¸‹æ–‡å†…å®¹å°¤ä¸ºé‡è¦ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å¯¹è¯å¼LLM-ASRæ–¹æ³•ä½¿ç”¨å›ºå®šæ•°é‡çš„å…ˆè¡Œè¯è¯­æˆ–æ•´ä¸ªå¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œç”±äºå¤§é‡æ— å…³å’Œå†—ä½™çš„ä¿¡æ¯ï¼Œå¯¼è‡´ASRæ··æ·†å’Œè®¡ç®—æˆæœ¬æ˜¾è‘—å¢åŠ ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºMARSçš„å¤šæ¨¡æ€æ£€ç´¢ä¸é€‰æ‹©æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä¸ºå½“å‰è¯è¯­æ£€ç´¢å’Œé€‰æ‹©æœ€ç›¸å…³çš„å£°éŸ³å’Œæ–‡å­—å†å²ä¸Šä¸‹æ–‡ï¼Œå¢å¼ºäº†å¯¹è¯å¼LLM-ASRçš„åŠŸèƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œå¤šæ¨¡æ€æ£€ç´¢è·å–ä¸€ç»„å€™é€‰å†å²ä¸Šä¸‹æ–‡ï¼Œæ¯ä¸ªä¸Šä¸‹æ–‡éƒ½è¡¨ç°å‡ºä¸å½“å‰è¯è¯­é«˜åº¦ç›¸ä¼¼çš„å£°éŸ³æˆ–æ–‡å­—ç‰¹å¾ã€‚å¤šæ¨¡æ€é€‰æ‹©è®¡ç®—æ¯ä¸ªæ£€ç´¢åˆ°çš„å€™é€‰å†å²ä¸Šä¸‹æ–‡çš„è¯­éŸ³å’Œæ–‡å­—ç›¸ä¼¼æ€§ï¼Œå¹¶é‡‡ç”¨æˆ‘ä»¬æå‡ºçš„ç†æƒ³æ’åæ–¹æ³•ç»¼åˆè€ƒè™‘è¿™ä¸¤ç§ç›¸ä¼¼æ€§ï¼Œé€‰æ‹©æœ€ä½³å†å²ä¸Šä¸‹æ–‡ã€‚åœ¨Interspeech 2025å¤šè¯­è¨€å¯¹è¯è¯­éŸ³è¯­è¨€æ¨¡å‹æŒ‘æˆ˜èµ›æ•°æ®é›†ä¸Šçš„è¯„ä¼°æ˜¾ç¤ºï¼Œä»…ç»è¿‡1500å°æ—¶æ•°æ®è®­ç»ƒçš„LLM-ASRï¼Œé…å¤‡MARSåï¼Œå…¶æ€§èƒ½ä¼˜äºç»è¿‡17ä¸‡å°æ—¶æ•°æ®è®­ç»ƒçš„æœ€æ–°é¡¶å°–ç³»ç»Ÿã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.01166v2">PDF</a> AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æŠ€æœ¯åœ¨å¯¹è¯åœºæ™¯ä¸­çš„åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä½¿ç”¨å›ºå®šæ•°é‡çš„å‰é¢å‘è¨€æˆ–æ•´ä¸ªå¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡æ‰€å¸¦æ¥çš„é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ£€ç´¢å’Œé€‰æ‹©æ–¹æ³•ï¼ˆMARSï¼‰ã€‚è¯¥æ–¹æ³•èƒ½å¤Ÿæ£€ç´¢å’Œé€‰æ‹©æœ€ç›¸å…³çš„å£°éŸ³å’Œæ–‡å­—å†å²ä¸Šä¸‹æ–‡ï¼Œä»¥æé«˜å¯¹è¯LLM-ASRçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨MARSæ–¹æ³•çš„LLM-ASRåœ¨ä»…ä½¿ç”¨1.5Kå°æ—¶çš„æ•°æ®è®­ç»ƒæ—¶ï¼Œæ€§èƒ½ä¼˜äºä½¿ç”¨179Kå°æ—¶æ•°æ®è®­ç»ƒçš„ç°æœ‰é¡¶å°–ç³»ç»Ÿã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ä¸­ç”¨äºç†è§£å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œä»¥æé«˜è¯†åˆ«å‡†ç¡®æ€§ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å­˜åœ¨çš„é—®é¢˜ï¼šä½¿ç”¨å›ºå®šæ•°é‡çš„å‰é¢å‘è¨€æˆ–æ•´ä¸ªå¯¹è¯å†å²ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´ASRæ··æ·†å’Œè®¡ç®—æˆæœ¬é«˜æ˜‚ã€‚</li>
<li>æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€æ£€ç´¢å’Œé€‰æ‹©æ–¹æ³•ï¼ˆMARSï¼‰ï¼Œèƒ½å¤Ÿæ£€ç´¢å’Œé€‰æ‹©æœ€ç›¸å…³çš„å£°éŸ³å’Œæ–‡å­—å†å²ä¸Šä¸‹æ–‡ã€‚</li>
<li>MARSæ–¹æ³•é€šè¿‡å¤šæ¨¡æ€æ£€ç´¢è·å–ä¸€ç»„ä¸å½“å‰å‘è¨€é«˜åº¦ç›¸ä¼¼ï¼ˆå£°éŸ³æˆ–æ–‡å­—ï¼‰çš„å†å²ä¸Šä¸‹æ–‡å€™é€‰é›†ã€‚</li>
<li>å¤šæ¨¡æ€é€‰æ‹©æ–¹æ³•è®¡ç®—æ¯ä¸ªæ£€ç´¢åˆ°çš„å€™é€‰å†å²ä¸Šä¸‹æ–‡çš„å£°éŸ³å’Œæ–‡å­—ç›¸ä¼¼æ€§ï¼Œå¹¶æå‡ºè¿‘ç†æƒ³æ’åºæ–¹æ³•æ¥é€‰æ‹©æœ€ä½³å†å²ä¸Šä¸‹æ–‡ã€‚</li>
<li>åœ¨Interspeech 2025å¤šè¯­ç§å¯¹è¯è¯­éŸ³è¯­è¨€æ¨¡å‹æŒ‘æˆ˜æ•°æ®é›†ä¸Šçš„è¯„ä¼°è¡¨æ˜ï¼Œç»“åˆMARSæ–¹æ³•çš„LLM-ASRè¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.01166">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-16f5be1a0dad019970cdc22e38c06708" align="middle">
<img src="https://picx.zhimg.com/v2-ce34ae2941c363b5996f20139cd63ed1" align="middle">
<img src="https://picx.zhimg.com/v2-4eedb0429d9d7288f99daab123ec79e8" align="middle">
<img src="https://picx.zhimg.com/v2-a916004a416ae4350e46e1a76c262466" align="middle">
<img src="https://picx.zhimg.com/v2-606873d63a2b23067cd656c9e6b8ee1e" align="middle">
<img src="https://picx.zhimg.com/v2-6bf208ea47f2b4ef56bfa79ac10f85cc" align="middle">
<img src="https://picx.zhimg.com/v2-b85faaffc2716532775e54bc6fb191b0" align="middle">
</details>


<h1 id="-19"><a href="#-19" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/TTS/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/TTS/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/TTS/">
                                    <span class="chip bg-color">TTS</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/Interactive/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-c5ed64d6b004d1e6854d030fe95a638b" class="responsive-img" alt="Interactive">
                        
                        <span class="card-title">Interactive</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Interactive æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Persona-Aware Alignment Framework for Personalized Dialogue Generation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/Interactive/" class="post-category">
                                    Interactive
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/Interactive/">
                        <span class="chip bg-color">Interactive</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-8732f3f412c53512adc63b28ea5bb7de" class="responsive-img" alt="åŒ»å­¦å›¾åƒ">
                        
                        <span class="card-title">åŒ»å­¦å›¾åƒ</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å›¾åƒ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  From 2D to 3D Without Extra Baggage Data-Efficient Cancer Detection in Digital Breast Tomosynthesis
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/" class="post-category">
                                    åŒ»å­¦å›¾åƒ
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">
                        <span class="chip bg-color">åŒ»å­¦å›¾åƒ</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32562k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
