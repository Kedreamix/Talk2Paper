<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Vision Transformer">
    <meta name="description" content="Vision Transformer æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  LampQ Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Vision Transformer | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-c56eaef7f8ea845a77f30e0137d04b73')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Vision Transformer</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/Vision-Transformer/">
                                <span class="chip bg-color">Vision Transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/Vision-Transformer/" class="post-category">
                                Vision Transformer
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-17
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    70 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-16-æ›´æ–°"><a href="#2025-11-16-æ›´æ–°" class="headerlink" title="2025-11-16 æ›´æ–°"></a>2025-11-16 æ›´æ–°</h1><h2 id="LampQ-Towards-Accurate-Layer-wise-Mixed-Precision-Quantization-for-Vision-Transformers"><a href="#LampQ-Towards-Accurate-Layer-wise-Mixed-Precision-Quantization-for-Vision-Transformers" class="headerlink" title="LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers"></a>LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers</h2><p><strong>Authors:Minjun Kim, Jaeri Lee, Jongjin Kim, Jeongin Yun, Yongmo Kwon, U Kang</strong></p>
<p>How can we accurately quantize a pre-trained Vision Transformer model? Quantization algorithms compress Vision Transformers (ViTs) into low-bit formats, reducing memory and computation demands with minimal accuracy degradation. However, existing methods rely on uniform precision, ignoring the diverse sensitivity of ViT components to quantization. Metric-based Mixed Precision Quantization (MPQ) is a promising alternative, but previous MPQ methods for ViTs suffer from three major limitations: 1) coarse granularity, 2) mismatch in metric scale across component types, and 3) quantization-unaware bit allocation. In this paper, we propose LampQ (Layer-wise Mixed Precision Quantization for Vision Transformers), an accurate metric-based MPQ method for ViTs to overcome these limitations. LampQ performs layer-wise quantization to achieve both fine-grained control and efficient acceleration, incorporating a type-aware Fisher-based metric to measure sensitivity. Then, LampQ assigns bit-widths optimally through integer linear programming and further updates them iteratively. Extensive experiments show that LampQ provides the state-of-the-art performance in quantizing ViTs pre-trained on various tasks such as image classification, object detection, and zero-shot quantization.</p>
<blockquote>
<p>æˆ‘ä»¬å¦‚ä½•å‡†ç¡®åœ°é‡åŒ–é¢„è®­ç»ƒçš„Vision Transformeræ¨¡å‹ï¼Ÿé‡åŒ–ç®—æ³•å°†Vision Transformersï¼ˆViTsï¼‰å‹ç¼©æˆä½ä½æ ¼å¼ï¼Œä»¥æœ€å°çš„ç²¾åº¦æŸå¤±å‡å°‘å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•ä¾èµ–äºç»Ÿä¸€ç²¾åº¦ï¼Œå¿½ç•¥äº†ViTç»„ä»¶åœ¨é‡åŒ–æ–¹é¢çš„ä¸åŒæ•æ„Ÿæ€§ã€‚åŸºäºåº¦é‡çš„æ··åˆç²¾åº¦é‡åŒ–ï¼ˆMPQï¼‰æ˜¯ä¸€ä¸ªæœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½†ä»¥å‰ç”¨äºViTçš„MPQæ–¹æ³•å­˜åœ¨ä¸‰ä¸ªä¸»è¦å±€é™æ€§ï¼š1ï¼‰ç²’åº¦ç²—ç³™ï¼Œ2ï¼‰ç»„ä»¶ç±»å‹ä¹‹é—´åº¦é‡æ ‡å‡†çš„å°ºåº¦ä¸åŒ¹é…ï¼Œä»¥åŠ3ï¼‰é‡åŒ–æ— å…³çš„ä½åˆ†é…ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é’ˆå¯¹Vision Transformersçš„Layer-wiseæ··åˆç²¾åº¦é‡åŒ–ï¼ˆLampQï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºåº¦é‡çš„å‡†ç¡®MPQæ–¹æ³•ï¼Œæ—¨åœ¨å…‹æœè¿™äº›å±€é™æ€§ã€‚LampQæ‰§è¡Œé€å±‚é‡åŒ–ï¼Œä»¥å®ç°ç²¾ç»†çš„ç²’åº¦å’Œé«˜æ•ˆçš„åŠ é€Ÿï¼Œå¹¶ç»“åˆåŸºäºFisherçš„æ„ŸçŸ¥ç±»å‹åº¦é‡æ¥æµ‹é‡æ•æ„Ÿæ€§ã€‚ç„¶åï¼ŒLampQé€šè¿‡æ•´æ•°çº¿æ€§è§„åˆ’æœ€ä¼˜åœ°åˆ†é…ä½å®½ï¼Œå¹¶è¿›ä¸€æ­¥è¿›è¡Œè¿­ä»£æ›´æ–°ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒLampQåœ¨é‡åŒ–åœ¨å„ç§ä»»åŠ¡ï¼ˆå¦‚å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œé›¶æ ·æœ¬é‡åŒ–ï¼‰ä¸Šé¢„è®­ç»ƒçš„ViTsæ–¹é¢æä¾›äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.10004v1">PDF</a> AAAI 2026</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•å‡†ç¡®é‡åŒ–é¢„è®­ç»ƒçš„Vision Transformeræ¨¡å‹ã€‚ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç»Ÿä¸€ç²¾åº¦ï¼Œå¿½ç•¥äº†Vision Transformerç»„ä»¶å¯¹é‡åŒ–çš„ä¸åŒæ•æ„Ÿåº¦ã€‚ä¸ºæ­¤ï¼Œæå‡ºäº†ä¸€ç§åä¸ºLampQçš„åŸºäºåº¦é‡çš„æ··åˆç²¾åº¦é‡åŒ–æ–¹æ³•ï¼Œä»¥å…‹æœç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚LampQå®ç°äº†å¯¹Vision Transformerçš„å±‚çº§æ··åˆç²¾åº¦é‡åŒ–ï¼Œé€šè¿‡ç±»å‹æ„ŸçŸ¥çš„Fisheråº¦é‡æ¥æµ‹é‡æ•æ„Ÿåº¦ï¼Œå¹¶é€šè¿‡æ•´æ•°çº¿æ€§è§„åˆ’æœ€ä¼˜åœ°åˆ†é…ä½å®½ï¼Œè¿›ä¸€æ­¥è¿­ä»£æ›´æ–°ã€‚å®éªŒè¡¨æ˜ï¼ŒLampQåœ¨å¤šç§ä»»åŠ¡é¢„è®­ç»ƒçš„Vision Transformeré‡åŒ–æ–¹é¢è¾¾åˆ°äº†æœ€æ–°æ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é‡åŒ–ç®—æ³•èƒ½å¤Ÿå‹ç¼©Vision Transformerï¼ˆViTsï¼‰æ¨¡å‹ä»¥é™ä½å†…å­˜å’Œè®¡ç®—éœ€æ±‚ï¼ŒåŒæ—¶ä¿æŒæœ€å°çš„ç²¾åº¦æŸå¤±ã€‚</li>
<li>ç°æœ‰æ–¹æ³•ä¸»è¦ä¾èµ–ç»Ÿä¸€ç²¾åº¦è¿›è¡Œé‡åŒ–ï¼Œå¿½ç•¥äº†ViTç»„ä»¶å¯¹é‡åŒ–çš„ä¸åŒæ•æ„Ÿåº¦ã€‚</li>
<li>Metric-based Mixed Precision Quantizationï¼ˆMPQï¼‰æ˜¯ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ³•ï¼Œä½†å­˜åœ¨ä¸‰å¤§å±€é™æ€§ã€‚</li>
<li>LampQï¼ˆLayer-wise Mixed Precision Quantization for Vision Transformersï¼‰å…‹æœäº†è¿™äº›å±€é™æ€§ï¼Œå®ç°äº†å¯¹Vision Transformerçš„å±‚çº§æ··åˆç²¾åº¦é‡åŒ–ã€‚</li>
<li>LampQé€šè¿‡ç±»å‹æ„ŸçŸ¥çš„Fisheråº¦é‡æ¥æµ‹é‡æ•æ„Ÿåº¦ï¼Œå®ç°äº†ç²¾ç»†ç²’åº¦çš„æ§åˆ¶å’Œé«˜æ•ˆåŠ é€Ÿã€‚</li>
<li>é€šè¿‡æ•´æ•°çº¿æ€§è§„åˆ’ï¼ŒLampQèƒ½å¤Ÿæœ€ä¼˜åœ°åˆ†é…ä½å®½ï¼Œå¹¶è¿›ä¸€æ­¥è¿­ä»£æ›´æ–°ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.10004">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d7183d61a7800a1209c899c2832b80e6" align="middle">
<img src="https://picx.zhimg.com/v2-4683014de0c93aa0d650be40c570c9c8" align="middle">
<img src="https://picx.zhimg.com/v2-f951b70935a3e07dd06121aedfdbac33" align="middle">
<img src="https://picx.zhimg.com/v2-985f31f7451cec89ec495e5635620abf" align="middle">
<img src="https://picx.zhimg.com/v2-d0ee59b3f733bc62acae7e4d4a2dc7f2" align="middle">
<img src="https://picx.zhimg.com/v2-64ca28ab03f876155725b48b319892a0" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Test-Time-Spectrum-Aware-Latent-Steering-for-Zero-Shot-Generalization-in-Vision-Language-Models"><a href="#Test-Time-Spectrum-Aware-Latent-Steering-for-Zero-Shot-Generalization-in-Vision-Language-Models" class="headerlink" title="Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models"></a>Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models</h2><p><strong>Authors:Konstantinos M. Dafnis, Dimitris N. Metaxas</strong></p>
<p>Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at <a target="_blank" rel="noopener" href="https://github.com/kdafnis/STS">https://github.com/kdafnis/STS</a>.</p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨é›¶æ ·æœ¬æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æµ‹è¯•æ—¶é—´åŸŸåç§»æ—¶é€šå¸¸æ€§èƒ½ä¸‹é™ã€‚å› æ­¤ï¼Œè¿‘æœŸå‡ºç°äº†é’ˆå¯¹VLMsçš„å³æ—¶æµ‹è¯•æ—¶é—´é€‚åº”ç­–ç•¥ï¼Œè¿™æ˜¯ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œç”¨äºå°†VLMsé€‚åº”å•ä¸ªæœªæ ‡è®°å›¾åƒã€‚ç„¶è€Œï¼Œç°æœ‰çš„é€‚åº”ç­–ç•¥ï¼Œå¦‚æµ‹è¯•æ—¶é—´æç¤ºè°ƒæ•´ï¼Œé€šå¸¸éœ€è¦åå‘ä¼ æ’­å¤§å‹ç¼–ç å™¨æƒé‡æˆ–æ›´æ”¹æ ¸å¿ƒæ¨¡å‹ç»„ä»¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†Spectrum-Aware Test-Time Steeringï¼ˆSTSï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªè½»é‡çº§çš„é€‚åº”æ¡†æ¶ï¼Œå®ƒä»æ–‡æœ¬åµŒå…¥ä¸­æå–é¢‘è°±å­ç©ºé—´æ¥å®šä¹‰ä¸»è¦çš„è¯­ä¹‰æ–¹å‘ï¼Œå¹¶å­¦ä¹ é€šè¿‡é€‚åº”å°‘é‡çš„æ¯ä¸ªæ ·æœ¬åç§»å‚æ•°æ¥ä»¥é¢‘è°±æ„ŸçŸ¥çš„æ–¹å¼æ§åˆ¶æ½œåœ¨è¡¨ç¤ºï¼Œä»¥æœ€å°åŒ–å¢å¼ºè§†å›¾ä¹‹é—´çš„ç†µã€‚STSå®Œå…¨åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„æ¨ç†é˜¶æ®µè¿è¡Œï¼Œæ— éœ€é€šè¿‡å†»ç»“çš„ç¼–ç å™¨è¿›è¡Œåå‘ä¼ æ’­æˆ–ä¿®æ”¹ã€‚åŸºäºæ ‡å‡†è¯„ä¼°åè®®çš„ç»¼åˆå®éªŒè¡¨æ˜ï¼ŒSTSåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šè¶…è¶Šäº†æˆ–è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„æµ‹è¯•æ—¶é—´é€‚åº”æ–¹æ³•ï¼ŒåŒæ—¶åªå¼•å…¥å°‘æ•°é¢å¤–çš„å‚æ•°ï¼Œå¹¶ä¸”ç›¸è¾ƒäºä¼ ç»Ÿçš„æµ‹è¯•æ—¶é—´æç¤ºè°ƒæ•´å®ç°äº†é«˜è¾¾8å€çš„æ¨ç†é€Ÿåº¦æå‡å’Œé«˜è¾¾1 2å€çš„å†…å­˜å ç”¨å‡å°‘ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/kdafnis/STS%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/kdafnis/STSæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09809v1">PDF</a> NeurIPS 2025</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ç« ä»‹ç»äº†ä¸€ç§åä¸ºSpectrum-Aware Test-Time Steeringï¼ˆSTSï¼‰çš„è½»é‡çº§è‡ªé€‚åº”æ¡†æ¶ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿä»æ–‡æœ¬åµŒå…¥ä¸­æå–é¢‘è°±å­ç©ºé—´ï¼Œå®šä¹‰ä¸»è¦è¯­ä¹‰æ–¹å‘ï¼Œå¹¶é€šè¿‡é€‚åº”å°‘é‡æ ·æœ¬åç§»å‚æ•°æ¥å­¦ä¹ åœ¨é¢‘è°±æ„ŸçŸ¥æ–¹å¼ä¸‹å¼•å¯¼æ½œåœ¨è¡¨ç¤ºï¼Œä»è€Œæœ€å°åŒ–å¢å¼ºè§†å›¾ä¹‹é—´çš„ç†µã€‚STSå®Œå…¨åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ¨ç†ï¼Œæ— éœ€é€šè¿‡å†»ç»“ç¼–ç å™¨è¿›è¡Œåå‘ä¼ æ’­æˆ–ä¿®æ”¹ã€‚å®éªŒè¡¨æ˜ï¼ŒSTSåœ¨æµ‹è¯•æ—¶è‡ªé€‚åº”æ–¹æ³•ä¸Šå¤§å¤§è¶…è¶Šäº†æˆ–è¡¨ç°è‰¯å¥½ï¼ŒåŒæ—¶åªå¼•å…¥å°‘é‡é¢å¤–å‚æ•°ï¼Œå¹¶ä¸”ç›¸å¯¹äºä¼ ç»Ÿçš„æµ‹è¯•æ—¶é—´æç¤ºè°ƒæ•´ï¼Œå®ç°äº†é«˜è¾¾8å€çš„æ¨ç†é€Ÿåº¦å’Œé«˜è¾¾12å€æ›´å°çš„å†…å­˜å ç”¨ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision-Language Models (VLMs) åœ¨é›¶æ ·æœ¬æ¨æ–­æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æµ‹è¯•æ—¶é—´åŸŸè½¬ç§»æ—¶æ€§èƒ½ä¼šä¸‹é™ã€‚</li>
<li>æµ‹è¯•æ—¶é—´çš„è‡ªé€‚åº”ç­–ç•¥å·²è¢«è§†ä¸ºé€‚åº”VLMsåˆ°å•ä¸ªæ— æ ‡ç­¾å›¾åƒçš„å¼ºå¤§æŠ€æœ¯ã€‚</li>
<li>ç°æœ‰è‡ªé€‚åº”ç­–ç•¥å¦‚æµ‹è¯•æ—¶é—´æç¤ºè°ƒæ•´é€šå¸¸éœ€è¦åå‘ä¼ æ’­å¤§é‡ç¼–ç å™¨æƒé‡æˆ–æ”¹å˜æ ¸å¿ƒæ¨¡å‹ç»„ä»¶ã€‚</li>
<li>STSæ˜¯ä¸€ä¸ªè½»é‡çº§è‡ªé€‚åº”æ¡†æ¶ï¼Œé€šè¿‡æå–æ–‡æœ¬åµŒå…¥çš„é¢‘è°±å­ç©ºé—´æ¥å®šä¹‰ä¸»è¦è¯­ä¹‰æ–¹å‘ã€‚</li>
<li>STSå­¦ä¹ é€šè¿‡é€‚åº”å°‘é‡æ ·æœ¬åç§»å‚æ•°æ¥å¼•å¯¼æ½œåœ¨è¡¨ç¤ºï¼Œä»¥æœ€å°åŒ–å¢å¼ºè§†å›¾ä¹‹é—´çš„ç†µã€‚</li>
<li>STSå®Œå…¨åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ¨ç†ï¼Œæ— éœ€ä¿®æ”¹æˆ–åå‘ä¼ æ’­å†»ç»“çš„ç¼–ç å™¨ã€‚</li>
<li>å®éªŒè¡¨æ˜STSåœ¨æµ‹è¯•æ—¶è‡ªé€‚åº”æ–¹æ³•ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå…·æœ‰è¾ƒå°‘çš„é¢å¤–å‚æ•°ã€è¾ƒå°çš„å†…å­˜å ç”¨å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09809">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b9ab14608854c3ee42447458ba0b33ff" align="middle">
<img src="https://picx.zhimg.com/v2-d02e50fb4a9734473f1b2b62808b70b1" align="middle">
<img src="https://picx.zhimg.com/v2-9d96fccb587c0170eb085d35d45571e6" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="vMFCoOp-Towards-Equilibrium-on-a-Unified-Hyperspherical-Manifold-for-Prompting-Biomedical-VLMs"><a href="#vMFCoOp-Towards-Equilibrium-on-a-Unified-Hyperspherical-Manifold-for-Prompting-Biomedical-VLMs" class="headerlink" title="vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs"></a>vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs</h2><p><strong>Authors:Minye Shao, Sihan Guo, Xinrun Li, Xingyu Miao, Haoran Duan, Yang Long</strong></p>
<p>Recent advances in context optimization (CoOp) guided by large language model (LLM)-distilled medical semantic priors offer a scalable alternative to manual prompt engineering and full fine-tuning for adapting biomedical CLIP-based vision-language models (VLMs). However, prompt learning in this context is challenged by semantic misalignment between LLMs and CLIP variants due to divergent training corpora and model architectures; it further lacks scalability across continuously evolving families of foundation models. More critically, pairwise multimodal alignment via conventional Euclidean-space optimization lacks the capacity to model unified representations or apply localized geometric constraints, which tends to amplify modality gaps in complex biomedical imaging and destabilize few-shot adaptation. In this work, we propose vMFCoOp, a framework that inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, aligning semantic biases between arbitrary LLMs and CLIP backbones via Unified Semantic Anchors to achieve robust biomedical prompting and superior few-shot classification. Grounded in three complementary constraints, vMFCoOp demonstrates consistent improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions, outperforming state-of-the-art methods in accuracy, generalization, and clinical applicability. This work aims to continuously expand to encompass more downstream applications, and the corresponding resources are intended to be shared through <a target="_blank" rel="noopener" href="https://github.com/VinyehShaw/UniEqui">https://github.com/VinyehShaw/UniEqui</a>.</p>
<blockquote>
<p>è¿‘æœŸï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æç‚¼å‡ºçš„åŒ»å­¦è¯­ä¹‰å…ˆéªŒçŸ¥è¯†æ¥æŒ‡å¯¼ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆCoOpï¼‰çš„æ–¹æ³•ï¼Œä¸ºåŸºäºCLIPçš„è·¨æ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„åº”ç”¨æä¾›äº†æ‰‹åŠ¨æç¤ºå·¥ç¨‹å’Œå®Œå…¨å¾®è°ƒçš„å¯æ‰©å±•æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæç¤ºå­¦ä¹ é¢ä¸´æ¥è‡ªLLMå’ŒCLIPå˜ä½“ä¹‹é—´è¯­ä¹‰ä¸ä¸€è‡´çš„æŒ‘æˆ˜ï¼Œè¿™æ˜¯ç”±äºä¸¤è€…é‡‡ç”¨ä¸åŒçš„è®­ç»ƒè¯­æ–™åº“å’Œæ¨¡å‹æ¶æ„é€ æˆçš„ï¼›æ­¤å¤–ï¼Œå®ƒè¿˜ä¸å…·å¤‡è·¨ä¸æ–­å‘å±•çš„åŸºç¡€æ¨¡å‹å®¶æ—çš„æ‰©å±•æ€§ã€‚æ›´å…³é”®çš„æ˜¯ï¼Œé€šè¿‡ä¼ ç»Ÿçš„æ¬§å‡ é‡Œå¾—ç©ºé—´ä¼˜åŒ–è¿›è¡Œçš„ä¸€å¯¹å¤šæ¨¡æ€å¯¹é½ç¼ºä¹å»ºæ¨¡ç»Ÿä¸€è¡¨ç¤ºæˆ–åº”ç”¨å±€éƒ¨å‡ ä½•çº¦æŸçš„èƒ½åŠ›ï¼Œè¿™å¾€å¾€ä¼šæ”¾å¤§å¤æ‚çš„ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­çš„æ¨¡æ€å·®è·å¹¶ç ´åå°‘é‡æ•°æ®çš„é€‚åº”æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†vMFCoOpæ¡†æ¶ï¼Œå®ƒé€šè¿‡é€†å‘ä¼°è®¡å…±äº«è¶…çƒæµå½¢ä¸Šçš„å†¯ç±³å¡æ–¯è´¹èˆå°”ï¼ˆvMFï¼‰åˆ†å¸ƒæ¥å¯¹é½LLMå’ŒCLIPä¹‹é—´çš„è¯­ä¹‰åå·®ã€‚é€šè¿‡ç»Ÿä¸€è¯­ä¹‰é”šç‚¹å®ç°ç¨³å¥çš„ç”Ÿç‰©åŒ»å­¦æç¤ºå’Œå‡ºè‰²çš„å°‘é‡æ ·æœ¬åˆ†ç±»ã€‚åŸºäºä¸‰é¡¹äº’è¡¥çº¦æŸçš„vMFCoOpåœ¨è·¨14ä¸ªåŒ»å­¦æ•°æ®é›†ã€æ¶‰åŠåŒ»å­¦æˆåƒçš„åäºŒç§æ¨¡æ€ä»¥åŠè§£å‰–å­¦åŒºåŸŸçš„åä¸‰ç§æ–¹é¢æ˜¾ç¤ºå‡ºæŒç»­ä¸€è‡´çš„æ”¹è¿›æ•ˆæœï¼Œåœ¨å‡†ç¡®æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œä¸´åºŠé€‚ç”¨æ€§æ–¹é¢å‡ä¼˜äºç°æœ‰æŠ€æœ¯æ–¹æ³•ã€‚æœ¬å·¥ä½œçš„ç›®æ ‡æ˜¯ä¸æ–­æ‰©å¤§åº”ç”¨èŒƒå›´ä»¥æ¶µç›–æ›´å¤šçš„ä¸‹æ¸¸åº”ç”¨ï¼Œå¹¶é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/VinyehShaw/UniEqui%E5%88%86%E4%BA%AB%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E3%80%82">https://github.com/VinyehShaw/UniEquiåˆ†äº«ç›¸å…³èµ„æºã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09540v2">PDF</a> Accepted as an Oral Presentation at AAAI 2026 Main Technical Track (this version is not peer-reviewed; it is the extended version)</p>
<p><strong>Summary</strong></p>
<p>åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è’¸é¦åŒ»å­¦è¯­ä¹‰å…ˆéªŒçš„ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼ˆCoOpï¼‰è¿›å±•ä¸ºè§£å†³ç”Ÿç‰©åŒ»å­¦CLIPåŸºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„é€‚åº”é—®é¢˜æä¾›äº†å¯ä¼¸ç¼©çš„æ›¿ä»£æ–¹æ¡ˆï¼Œç›¸è¾ƒäºæ‰‹åŠ¨æç¤ºå·¥ç¨‹å’Œå®Œå…¨å¾®è°ƒçš„æ–¹å¼ã€‚ç„¶è€Œï¼Œåœ¨æ­¤èƒŒæ™¯ä¸‹çš„æç¤ºå­¦ä¹ é¢ä¸´LLMså’ŒCLIPå˜ä½“é—´å› ä¸åŒè®­ç»ƒè¯­æ–™åº“å’Œæ¨¡å‹æ¶æ„å¯¼è‡´çš„è¯­ä¹‰ä¸åŒ¹é…æŒ‘æˆ˜ï¼Œä¸”ç¼ºä¹è·¨ä¸æ–­æ¼”å˜çš„å®¶æ—åŸºç¡€æ¨¡å‹çš„æ‰©å±•æ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œé€šè¿‡å¸¸è§„æ¬§å‡ é‡Œå¾—ç©ºé—´ä¼˜åŒ–çš„é…å¯¹å¤šæ¨¡å¼å¯¹é½æ— æ³•å»ºæ¨¡ç»Ÿä¸€è¡¨ç¤ºæˆ–åº”ç”¨å±€éƒ¨å‡ ä½•çº¦æŸï¼Œè¿™åœ¨å¤æ‚çš„ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­ä¼šæ”¾å¤§æ¨¡å¼å·®è·å¹¶ç ´åå°‘é‡æ ·æœ¬é€‚åº”çš„ç¨³å®šæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†vMFCoOpæ¡†æ¶ï¼Œå®ƒé€šè¿‡å…±äº«è¶…çƒé¢æµå½¢ä¸Šåå‘ä¼°è®¡von Mises-Fisherï¼ˆvMFï¼‰åˆ†å¸ƒï¼Œå¹¶é€šè¿‡ç»Ÿä¸€è¯­ä¹‰é”šå¯¹é½ä»»æ„LLMså’ŒCLIPéª¨æ¶ä¹‹é—´çš„è¯­ä¹‰åè§ï¼Œå®ç°ç¨³å¥çš„ç”Ÿç‰©åŒ»å­¦æç¤ºå’Œä¼˜è¶Šçš„å°æ ·æœ¬åˆ†ç±»ã€‚åŸºäºä¸‰é¡¹äº’è¡¥çº¦æŸï¼ŒvMFCoOpåœ¨14ä¸ªåŒ»ç–—æ•°æ®é›†ã€12ç§åŒ»ç–—æˆåƒæ¨¡å¼å’Œ13ä¸ªè§£å‰–åŒºåŸŸä¸Šå®ç°äº†æŒç»­çš„æ”¹è¿›ï¼Œåœ¨å‡†ç¡®æ€§ã€é€šç”¨æ€§å’Œä¸´åºŠé€‚ç”¨æ€§æ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>LLM-distilled medical semantic priors advance context optimization (CoOp) for adapting biomedical CLIP-based vision-language models (VLMs).</li>
<li>Prompt learning faces challenges due to semantic misalignment between LLMs and CLIP variants.</li>
<li>ä¼ ç»Ÿçš„æ¬§å‡ é‡Œå¾—ç©ºé—´ä¼˜åŒ–åœ¨å¤æ‚ç”Ÿç‰©åŒ»å­¦æˆåƒä¸­å­˜åœ¨æ¨¡æ€å·®è·å’Œé€‚åº”ç¨³å®šæ€§é—®é¢˜ã€‚</li>
<li>vMFCoOpæ¡†æ¶é€šè¿‡ä¼°è®¡von Mises-Fisheråˆ†å¸ƒå’Œå¯¹é½è¯­ä¹‰åè§ï¼Œå®ç°äº†ç¨³å¥çš„ç”Ÿç‰©åŒ»å­¦æç¤ºå’Œå°‘é‡æ ·æœ¬åˆ†ç±»ã€‚</li>
<li>vMFCoOpæ¡†æ¶åœ¨åŒ»ç–—æ•°æ®é›†ã€åŒ»ç–—æˆåƒæ¨¡å¼å’Œè§£å‰–åŒºåŸŸä¸Šçš„è¡¨ç°ä¼˜äºæœ€å…ˆè¿›çš„æ–¹æ³•ã€‚</li>
<li>vMFCoOpæ¡†æ¶æ—¨åœ¨ä¸æ–­æ‰©å±•ä»¥æ¶µç›–æ›´å¤šä¸‹æ¸¸åº”ç”¨ï¼Œå¹¶é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/VinyehShaw/UniEqui">https://github.com/VinyehShaw/UniEqui</a>å…±äº«ç›¸å…³èµ„æºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09540">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a12f86f9e44d6d45af5fc33a08ce354e" align="middle">
<img src="https://picx.zhimg.com/v2-d1c51fb95f443e62bf65fe82ae6cb003" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Spatio-Temporal-Data-Enhanced-Vision-Language-Model-for-Traffic-Scene-Understanding"><a href="#Spatio-Temporal-Data-Enhanced-Vision-Language-Model-for-Traffic-Scene-Understanding" class="headerlink" title="Spatio-Temporal Data Enhanced Vision-Language Model for Traffic Scene Understanding"></a>Spatio-Temporal Data Enhanced Vision-Language Model for Traffic Scene Understanding</h2><p><strong>Authors:Jingtian Ma, Jingyuan Wang, Wayne Xin Zhao, Guoping Liu, Xiang Wen</strong></p>
<p>Nowadays, navigation and ride-sharing apps have collected numerous images with spatio-temporal data. A core technology for analyzing such images, associated with spatiotemporal information, is Traffic Scene Understanding (TSU), which aims to provide a comprehensive description of the traffic scene. Unlike traditional spatio-temporal data analysis tasks, the dependence on both spatio-temporal and visual-textual data introduces distinct challenges to TSU task. However, recent research often treats TSU as a common image understanding task, ignoring the spatio-temporal information and overlooking the interrelations between different aspects of the traffic scene. To address these issues, we propose a novel SpatioTemporal Enhanced Model based on CILP (ST-CLIP) for TSU. Our model uses the classic vision-language model, CLIP, as the backbone, and designs a Spatio-temporal Context Aware Multiaspect Prompt (SCAMP) learning method to incorporate spatiotemporal information into TSU. The prompt learning method consists of two components: A dynamic spatio-temporal context representation module that extracts representation vectors of spatio-temporal data for each traffic scene image, and a bi-level ST-aware multi-aspect prompt learning module that integrates the ST-context representation vectors into word embeddings of prompts for the CLIP model. The second module also extracts low-level visual features and image-wise high-level semantic features to exploit interactive relations among different aspects of traffic scenes. To the best of our knowledge, this is the first attempt to integrate spatio-temporal information into visionlanguage models to facilitate TSU task. Experiments on two realworld datasets demonstrate superior performance in the complex scene understanding scenarios with a few-shot learning strategy.</p>
<blockquote>
<p>å¦‚ä»Šï¼Œå¯¼èˆªå’Œæ‹¼è½¦åº”ç”¨ç¨‹åºå·²ç»æ”¶é›†äº†å¤§é‡å¸¦æœ‰æ—¶ç©ºæ•°æ®çš„å›¾åƒã€‚åˆ†æè¿™äº›ä¸æ—¶ç©ºä¿¡æ¯ç›¸å…³çš„å›¾åƒçš„æ ¸å¿ƒæŠ€æœ¯æ˜¯äº¤é€šåœºæ™¯ç†è§£ï¼ˆTSUï¼‰ï¼Œæ—¨åœ¨ä¸ºäº¤é€šåœºæ™¯æä¾›å…¨é¢çš„æè¿°ã€‚ä¸ä¼ ç»Ÿçš„æ—¶ç©ºæ•°æ®åˆ†æä»»åŠ¡ä¸åŒï¼Œå¯¹æ—¶ç©ºæ•°æ®å’Œè§†è§‰æ–‡æœ¬æ•°æ®çš„ä¾èµ–ç»™TSUä»»åŠ¡å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„ç ”ç©¶å¾€å¾€å°†TSUè§†ä¸ºå¸¸è§çš„å›¾åƒç†è§£ä»»åŠ¡ï¼Œå¿½ç•¥äº†æ—¶ç©ºä¿¡æ¯ï¼Œå¹¶å¿½è§†äº†äº¤é€šåœºæ™¯ä¸åŒæ–¹é¢çš„ç›¸äº’å…³ç³»ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºCILPçš„æ–°å‹æ—¶ç©ºå¢å¼ºæ¨¡å‹ï¼ˆST-CLIPï¼‰ç”¨äºTSUã€‚æˆ‘ä»¬çš„æ¨¡å‹ä»¥ç»å…¸çš„è§†è§‰è¯­è¨€æ¨¡å‹CLIPä½œä¸ºéª¨å¹²ç½‘ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§æ—¶ç©ºä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šæ–¹é¢æç¤ºï¼ˆSCAMPï¼‰å­¦ä¹ æ–¹æ³•ï¼Œå°†æ—¶ç©ºä¿¡æ¯èå…¥TSUä¸­ã€‚æç¤ºå­¦ä¹ æ–¹æ³•ç”±ä¸¤ä¸ªç»„ä»¶ç»„æˆï¼šä¸€ä¸ªåŠ¨æ€æ—¶ç©ºä¸Šä¸‹æ–‡è¡¨ç¤ºæ¨¡å—ï¼Œç”¨äºæå–æ¯ä¸ªäº¤é€šåœºæ™¯å›¾åƒçš„æ—¶ç©ºæ•°æ®è¡¨ç¤ºå‘é‡ï¼›ä¸€ä¸ªä¸¤çº§STæ„ŸçŸ¥å¤šæ–¹é¢æç¤ºå­¦ä¹ æ¨¡å—ï¼Œå°†STä¸Šä¸‹æ–‡è¡¨ç¤ºå‘é‡é›†æˆåˆ°CLIPæ¨¡å‹çš„æç¤ºè¯åµŒå…¥ä¸­ã€‚ç¬¬äºŒä¸ªæ¨¡å—è¿˜æå–ä½çº§è§†è§‰ç‰¹å¾å’Œå›¾åƒçº§é«˜çº§è¯­ä¹‰ç‰¹å¾ï¼Œä»¥åˆ©ç”¨äº¤é€šåœºæ™¯ä¸åŒæ–¹é¢çš„äº¤äº’å…³ç³»ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡å°è¯•å°†æ—¶ç©ºä¿¡æ¯æ•´åˆåˆ°è§†è§‰è¯­è¨€æ¨¡å‹ä¸­ï¼Œä»¥ä¿ƒè¿›TSUä»»åŠ¡ã€‚åœ¨ä¸¤ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œåœ¨å¤æ‚çš„åœºæ™¯ç†è§£åœºæ™¯ä¸­ï¼Œé‡‡ç”¨å°æ ·æœ¬å­¦ä¹ ç­–ç•¥å¯ä»¥å–å¾—å“è¶Šçš„æ€§èƒ½ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08978v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡ä¸»è¦æ¢è®¨äº¤é€šåœºæ™¯ç†è§£ï¼ˆTSUï¼‰æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯éœ€è¦åŒæ—¶å¤„ç†æ—¶ç©ºæ•°æ®å’Œè§†è§‰æ–‡æœ¬æ•°æ®ï¼Œä¸ºç°ä»£å¯¼èˆªå’Œæ‹¼è½¦åº”ç”¨æä¾›å¯¹äº¤é€šåœºæ™¯çš„ç»¼åˆæè¿°ã€‚ä¸ºè§£å†³ç°æœ‰ç ”ç©¶å¿½è§†æ—¶ç©ºä¿¡æ¯çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºCLIPçš„æ—¶ç©ºå¢å¼ºæ¨¡å‹ST-CLIPï¼Œå¹¶ç»“åˆæ—¶ç©ºä¸Šä¸‹æ–‡æ„ŸçŸ¥å¤šæ–¹é¢æç¤ºå­¦ä¹ æ–¹æ³•SCAMPï¼Œå®ç°å¯¹äº¤é€šåœºæ™¯çš„æ·±å…¥ç†è§£å’Œåˆ†æã€‚æ­¤æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œçš„ä¸¤ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå…¶åœ¨å¤æ‚åœºæ™¯ç†è§£æ–¹é¢è¡¨ç°ä¼˜è¶Šã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>äº¤é€šåœºæ™¯ç†è§£ï¼ˆTSUï¼‰æ˜¯å¤„ç†å¯¼èˆªå’Œæ‹¼è½¦åº”ç”¨ä¸­çš„æ—¶ç©ºæ•°æ®å’Œè§†è§‰æ–‡æœ¬æ•°æ®çš„æ ¸å¿ƒæŠ€æœ¯ã€‚</li>
<li>ç°æœ‰ç ”ç©¶å¸¸å¸¸å¿½è§†æ—¶ç©ºä¿¡æ¯ä»¥åŠäº¤é€šåœºæ™¯ä¸­ä¸åŒæ–¹é¢çš„ç›¸äº’å…³ç³»ã€‚</li>
<li>æå‡ºäº†åŸºäºCLIPçš„æ—¶ç©ºå¢å¼ºæ¨¡å‹ST-CLIPï¼Œé¦–æ¬¡å°è¯•å°†æ—¶ç©ºä¿¡æ¯èå…¥è§†è§‰è¯­è¨€æ¨¡å‹ä»¥ä¿ƒè¿›TSUä»»åŠ¡ã€‚</li>
<li>ST-CLIPæ¨¡å‹åŒ…å«ä¸¤å¤§æ¨¡å—ï¼šåŠ¨æ€æ—¶ç©ºä¸Šä¸‹æ–‡è¡¨ç¤ºæ¨¡å—å’ŒåŒçº§STæ„ŸçŸ¥å¤šæ–¹é¢æç¤ºå­¦ä¹ æ¨¡å—ã€‚</li>
<li>åŠ¨æ€æ—¶ç©ºä¸Šä¸‹æ–‡è¡¨ç¤ºæ¨¡å—ä¸ºæ¯ä¸ªäº¤é€šåœºæ™¯å›¾åƒæå–æ—¶ç©ºæ•°æ®è¡¨ç¤ºå‘é‡ã€‚</li>
<li>åŒçº§STæ„ŸçŸ¥å¤šæ–¹é¢æç¤ºå­¦ä¹ æ¨¡å—å°†è¿™äº›å‘é‡èå…¥CLIPæ¨¡å‹çš„è¯åµŒå…¥æç¤ºä¸­ï¼Œå¹¶æå–ä½çº§åˆ«è§†è§‰ç‰¹å¾å’Œå›¾åƒçº§é«˜çº§è¯­ä¹‰ç‰¹å¾ï¼Œä»¥åˆ©ç”¨äº¤é€šåœºæ™¯ä¸­ä¸åŒæ–¹é¢çš„äº¤äº’å…³ç³»ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08978">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-604dd447dc7dbfdde53e78b77f98e2a5" align="middle">
<img src="https://picx.zhimg.com/v2-23cb1c7413dccc4bbef63a2ba05d2bb6" align="middle">
<img src="https://picx.zhimg.com/v2-369e8156f83f4f110dec8dd14f081d39" align="middle">
<img src="https://picx.zhimg.com/v2-f0121491a168e21fe0fb73ee03f23454" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Improve-Contrastive-Clustering-Performance-by-Multiple-Fusing-Augmenting-ViT-Blocks"><a href="#Improve-Contrastive-Clustering-Performance-by-Multiple-Fusing-Augmenting-ViT-Blocks" class="headerlink" title="Improve Contrastive Clustering Performance by Multiple Fusing-Augmenting ViT Blocks"></a>Improve Contrastive Clustering Performance by Multiple Fusing-Augmenting ViT Blocks</h2><p><strong>Authors:Cheng Wang, Shuisheng Zhou, Fengjiao Peng, Jin Sheng, Feng Ye, Yinli Dong</strong></p>
<p>In the field of image clustering, the widely used contrastive learning networks improve clustering performance by maximizing the similarity between positive pairs and the dissimilarity of negative pairs of the inputs. Extant contrastive learning networks, whose two encoders often implicitly interact with each other by parameter sharing or momentum updating, may not fully exploit the complementarity and similarity of the positive pairs to extract clustering features from input data. To explicitly fuse the learned features of positive pairs, we design a novel multiple fusing-augmenting ViT blocks (MFAVBs) based on the excellent feature learning ability of Vision Transformers (ViT). Firstly, two preprocessed augmentions as positive pairs are separately fed into two shared-weight ViTs, then their output features are fused to input into a larger ViT. Secondly, the learned features are split into a pair of new augmented positive samples and passed to the next FAVBs, enabling multiple fusion and augmention through MFAVBs operations. Finally, the learned features are projected into both instance-level and clustering-level spaces to calculate the cross-entropy loss, followed by parameter updates by backpropagation to finalize the training process. To further enhance ability of the model to distinguish between similar images, our input data for the network we propose is preprocessed augmentions with features extracted from the CLIP pretrained model. Our experiments on seven public datasets demonstrate that MFAVBs serving as the backbone for contrastive clustering outperforms the state-of-the-art techniques in terms of clustering performance.</p>
<blockquote>
<p>åœ¨å›¾åƒèšç±»é¢†åŸŸï¼Œå¹¿æ³›ä½¿ç”¨çš„å¯¹æ¯”å­¦ä¹ ç½‘ç»œé€šè¿‡æœ€å¤§åŒ–æ­£å¯¹ä¹‹é—´çš„ç›¸ä¼¼æ€§å’Œè´Ÿå¯¹ä¹‹é—´çš„ä¸ç›¸ä¼¼æ€§æ¥æé«˜èšç±»æ€§èƒ½ã€‚ç°æœ‰çš„å¯¹æ¯”å­¦ä¹ ç½‘ç»œï¼Œå…¶ä¸¤ä¸ªç¼–ç å™¨é€šå¸¸é€šè¿‡å‚æ•°å…±äº«æˆ–åŠ¨é‡æ›´æ–°éšå¼åœ°ç›¸äº’äº¤äº’ï¼Œå¯èƒ½æ— æ³•å……åˆ†æ¢ç´¢æ­£å¯¹ä¹‹é—´çš„äº’è¡¥æ€§å’Œç›¸ä¼¼æ€§ï¼Œä»¥ä»è¾“å…¥æ•°æ®ä¸­æå–èšç±»ç‰¹å¾ã€‚ä¸ºäº†æ˜ç¡®èåˆæ­£å¯¹çš„å·²å­¦ä¹ ç‰¹å¾ï¼Œæˆ‘ä»¬åŸºäºè§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰çš„ä¼˜ç§€ç‰¹å¾å­¦ä¹ èƒ½åŠ›ï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹çš„å¤šé‡èåˆå¢å¼ºViTå—ï¼ˆMFAVBsï¼‰ã€‚é¦–å…ˆï¼Œå°†ä¸¤ä¸ªé¢„å¤„ç†çš„å¢å¼ºæ•°æ®ä½œä¸ºæ­£å¯¹åˆ†åˆ«è¾“å…¥åˆ°ä¸¤ä¸ªå…±äº«æƒé‡ViTä¸­ï¼Œç„¶åå°†å®ƒä»¬çš„è¾“å‡ºç‰¹å¾èåˆå¹¶è¾“å…¥åˆ°ä¸€ä¸ªæ›´å¤§çš„ViTä¸­ã€‚å…¶æ¬¡ï¼Œå°†å­¦ä¹ åˆ°çš„ç‰¹å¾åˆ†å‰²æˆä¸€å¯¹æ–°çš„å¢å¼ºæ­£æ ·æœ¬å¹¶ä¼ é€’ç»™ä¸‹ä¸€ä¸ªFAVBsï¼Œé€šè¿‡MFAVBsæ“ä½œå®ç°å¤šæ¬¡èåˆå’Œå¢å¼ºã€‚æœ€åï¼Œå°†å­¦ä¹ åˆ°çš„ç‰¹å¾æŠ•å½±åˆ°å®ä¾‹çº§å’Œèšç±»çº§ç©ºé—´ï¼Œè®¡ç®—äº¤å‰ç†µæŸå¤±ï¼Œç„¶åé€šè¿‡åå‘ä¼ æ’­è¿›è¡Œå‚æ•°æ›´æ–°ä»¥å®Œæˆè®­ç»ƒè¿‡ç¨‹ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¨¡å‹åŒºåˆ†ç›¸ä¼¼å›¾åƒçš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸ºæ‰€æå‡ºçš„ç½‘ç»œä½¿ç”¨é¢„è®­ç»ƒçš„CLIPæ¨¡å‹çš„ç‰¹å¾æå–ä½œä¸ºè¾“å…¥æ•°æ®çš„é¢„å¤„ç†å¢å¼ºã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½œä¸ºå¯¹æ¯”èšç±»çš„éª¨å¹²ç½‘ï¼ŒMFAVBsåœ¨èšç±»æ€§èƒ½ä¸Šè¶…è¶Šäº†æœ€æ–°æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08883v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºå¯¹æ¯”å­¦ä¹ çš„å›¾åƒèšç±»ä¸­ï¼Œé€šè¿‡æœ€å¤§åŒ–æ­£æ ·æœ¬å¯¹é—´çš„ç›¸ä¼¼æ€§å’Œè´Ÿæ ·æœ¬å¯¹é—´çš„å·®å¼‚æ€§æ¥æå‡æ€§èƒ½ã€‚ç°æœ‰å¯¹æ¯”å­¦ä¹ ç½‘ç»œé€šè¿‡å‚æ•°å…±äº«æˆ–åŠ¨é‡æ›´æ–°ä½¿ä¸¤ä¸ªç¼–ç å™¨éšå¼äº¤äº’ï¼Œå¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨æ­£æ ·æœ¬å¯¹çš„ç›¸ä¼¼æ€§å’Œäº’è¡¥æ€§æ¥æå–èšç±»ç‰¹å¾ã€‚ä¸ºæ­¤ï¼Œè®¾è®¡åŸºäºVision Transformerï¼ˆViTï¼‰çš„æ–°å‹å¤šé‡èåˆå¢å¼ºViTå—ï¼ˆMFAVBsï¼‰ï¼Œé€šè¿‡ä¸¤ä¸ªé¢„å¤„ç†å¢å¼ºä½œä¸ºæ­£æ ·æœ¬å¯¹åˆ†åˆ«è¾“å…¥ä¸¤ä¸ªå…±äº«æƒé‡ViTsï¼Œå†èåˆå…¶è¾“å‡ºç‰¹å¾è¾“å…¥æ›´å¤§çš„ViTã€‚æ­¤å¤–ï¼Œå°†å­¦ä¹ åˆ°çš„ç‰¹å¾åˆ†å‰²æˆæ–°çš„å¢å¼ºæ­£æ ·æœ¬å¯¹ï¼Œé€šè¿‡MFAVBsæ“ä½œè¿›è¡Œå¤šæ¬¡èåˆå’Œå¢å¼ºã€‚æœ€ç»ˆï¼Œå°†å­¦ä¹ åˆ°çš„ç‰¹å¾æŠ•å½±åˆ°å®ä¾‹çº§å’Œèšç±»çº§ç©ºé—´ï¼Œè®¡ç®—äº¤å‰ç†µæŸå¤±ï¼Œé€šè¿‡åå‘ä¼ æ’­æ›´æ–°å‚æ•°å®Œæˆè®­ç»ƒè¿‡ç¨‹ã€‚ä½¿ç”¨CLIPé¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾é¢„å¤„ç†è¾“å…¥æ•°æ®ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹åŒºåˆ†ç›¸ä¼¼å›¾åƒçš„èƒ½åŠ›ã€‚åœ¨ä¸ƒä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½œä¸ºå¯¹æ¯”èšç±»çš„éª¨å¹²ç½‘ï¼ŒMFAVBsçš„æ€§èƒ½ä¼˜äºç°æœ‰å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”å­¦ä¹ ç½‘ç»œé€šè¿‡æœ€å¤§åŒ–æ­£æ ·æœ¬å¯¹é—´çš„ç›¸ä¼¼æ€§å’Œè´Ÿæ ·æœ¬å¯¹é—´çš„å·®å¼‚æ€§ï¼Œæå‡äº†å›¾åƒèšç±»çš„æ€§èƒ½ã€‚</li>
<li>ç°æœ‰å¯¹æ¯”å­¦ä¹ ç½‘ç»œåœ¨æå–èšç±»ç‰¹å¾æ—¶ï¼Œå¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨æ­£æ ·æœ¬å¯¹çš„ç›¸ä¼¼æ€§å’Œäº’è¡¥æ€§ã€‚</li>
<li>æå‡ºäº†åŸºäºVision Transformerï¼ˆViTï¼‰çš„æ–°å‹å¤šé‡èåˆå¢å¼ºViTå—ï¼ˆMFAVBsï¼‰ï¼Œä»¥æ›´å¥½åœ°èåˆå’Œæå–ç‰¹å¾ã€‚</li>
<li>MFAVBsé€šè¿‡å¤šæ¬¡èåˆå’Œå¢å¼ºå­¦ä¹ åˆ°çš„ç‰¹å¾ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>ä½¿ç”¨CLIPé¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾è¿›è¡Œè¾“å…¥æ•°æ®çš„é¢„å¤„ç†ï¼Œå¢å¼ºäº†æ¨¡å‹åŒºåˆ†ç›¸ä¼¼å›¾åƒçš„èƒ½åŠ›ã€‚</li>
<li>åœ¨ä¸ƒä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMFAVBsä½œä¸ºå¯¹æ¯”èšç±»çš„éª¨å¹²ç½‘è¡¨ç°å‡ºä¼˜å¼‚æ€§èƒ½ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8da36272a0794a14089401c1ef805036" align="middle">
<img src="https://picx.zhimg.com/v2-721c0321932786121fb877aa0dc1a1be" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="VLMDiff-Leveraging-Vision-Language-Models-for-Multi-Class-Anomaly-Detection-with-Diffusion"><a href="#VLMDiff-Leveraging-Vision-Language-Models-for-Multi-Class-Anomaly-Detection-with-Diffusion" class="headerlink" title="VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion"></a>VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion</h2><p><strong>Authors:Samet Hicsonmez, Abd El Rahman Shabayek, Djamila Aouada</strong></p>
<p>Detecting visual anomalies in diverse, multi-class real-world images is a significant challenge. We introduce \ours, a novel unsupervised multi-class visual anomaly detection framework. It integrates a Latent Diffusion Model (LDM) with a Vision-Language Model (VLM) for enhanced anomaly localization and detection. Specifically, a pre-trained VLM with a simple prompt extracts detailed image descriptions, serving as additional conditioning for LDM training. Current diffusion-based methods rely on synthetic noise generation, limiting their generalization and requiring per-class model training, which hinders scalability. \ours, however, leverages VLMs to obtain normal captions without manual annotations or additional training. These descriptions condition the diffusion model, learning a robust normal image feature representation for multi-class anomaly detection. Our method achieves competitive performance, improving the pixel-level Per-Region-Overlap (PRO) metric by up to 25 points on the Real-IAD dataset and 8 points on the COCO-AD dataset, outperforming state-of-the-art diffusion-based approaches. Code is available at <a target="_blank" rel="noopener" href="https://github.com/giddyyupp/VLMDiff">https://github.com/giddyyupp/VLMDiff</a>.</p>
<blockquote>
<p>æ£€æµ‹å¤šç§å¤šç±»åˆ«çš„ç°å®ä¸–ç•Œä¸­å›¾åƒä¸­çš„è§†è§‰å¼‚å¸¸æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬æ¨å‡ºäº†ä¸€é¡¹æ–°é¢–çš„æ— ç›‘ç£å¤šç±»åˆ«è§†è§‰å¼‚å¸¸æ£€æµ‹æ¡†æ¶â€”â€”oursã€‚å®ƒå°†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion Modelï¼Œç®€ç§°LDMï¼‰ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVision-Language Modelï¼Œç®€ç§°VLMï¼‰ç›¸ç»“åˆï¼Œå¢å¼ºäº†å¼‚å¸¸çš„å®šä½å’Œæ£€æµ‹èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œé€šè¿‡ç®€å•çš„æç¤ºï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„VLMæå–è¯¦ç»†çš„å›¾åƒæè¿°ï¼Œä½œä¸ºLDMè®­ç»ƒçš„é™„åŠ æ¡ä»¶ã€‚å½“å‰çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ä¾èµ–äºåˆæˆå™ªå£°ç”Ÿæˆï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„æ³›åŒ–èƒ½åŠ›å¹¶éœ€è¦æ¯ç±»æ¨¡å‹è®­ç»ƒï¼Œé˜»ç¢äº†å¯æ‰©å±•æ€§ã€‚ç„¶è€Œï¼Œoursåˆ©ç”¨VLMsè·å¾—æ­£å¸¸çš„å­—å¹•ï¼Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šæˆ–é¢å¤–çš„è®­ç»ƒã€‚è¿™äº›æè¿°æ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå­¦ä¹ ç”¨äºå¤šç±»åˆ«å¼‚å¸¸æ£€æµ‹çš„å¼ºå¤§æ­£å¸¸å›¾åƒç‰¹å¾è¡¨ç¤ºã€‚æˆ‘ä»¬çš„æ–¹æ³•å…·æœ‰ç«äº‰åŠ›ï¼Œåœ¨Real-IADæ•°æ®é›†ä¸Šæ¯åŒºåŸŸé‡å ï¼ˆPROï¼‰æŒ‡æ ‡æé«˜äº†é«˜è¾¾25ä¸ªç‚¹ï¼Œåœ¨COCO-ADæ•°æ®é›†ä¸Šæé«˜äº†8ä¸ªç‚¹ï¼Œè¶…è¶Šäº†æœ€æ–°çš„åŸºäºæ‰©æ•£çš„æ–¹æ³•ã€‚ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/giddyyupp/VLMDiff%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/giddyyupp/VLMDiffè·å–ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08173v1">PDF</a> WACV 2026</p>
<p><strong>Summary</strong></p>
<p>ä¸€ç§æ–°å‹çš„æ— ç›‘ç£å¤šç±»è§†è§‰å¼‚å¸¸æ£€æµ‹æ¡†æ¶è¢«æå‡ºï¼Œå®ƒç»“åˆäº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä»¥å¢å¼ºå¼‚å¸¸å®šä½å’Œæ£€æµ‹èƒ½åŠ›ã€‚åˆ©ç”¨é¢„è®­ç»ƒçš„VLMæ¨¡å‹å’Œç®€å•æç¤ºæ¥æå–å›¾åƒè¯¦ç»†æè¿°ï¼Œä¸ºLDMè®­ç»ƒæä¾›é¢å¤–çš„æ¡ä»¶ã€‚è¯¥æ–¹æ³•å…‹æœäº†ç°æœ‰æ‰©æ•£æ–¹æ³•ä¾èµ–åˆæˆå™ªå£°ç”Ÿæˆçš„å±€é™æ€§ï¼Œæ— éœ€æ¯ç±»æ¨¡å‹è®­ç»ƒå³å¯å®ç°è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚åˆ©ç”¨VLMè·å¾—æ­£å¸¸å›¾åƒçš„æè¿°ï¼Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šæˆ–é¢å¤–è®­ç»ƒï¼Œä»è€Œæ¡ä»¶æ‰©æ•£æ¨¡å‹ï¼Œå­¦ä¹ å¤šç±»å¼‚å¸¸æ£€æµ‹çš„é²æ£’æ­£å¸¸å›¾åƒç‰¹å¾è¡¨ç¤ºã€‚è¯¥æ–¹æ³•åœ¨Real-IADå’ŒCOCO-ADæ•°æ®é›†ä¸Šå®ç°äº†å‡ºè‰²çš„æ€§èƒ½æå‡ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æå‡ºäº†ä¸€ç§æ–°å‹çš„æ— ç›‘ç£å¤šç±»è§†è§‰å¼‚å¸¸æ£€æµ‹æ¡†æ¶ã€‚</li>
<li>ç»“åˆäº†æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚</li>
<li>åˆ©ç”¨é¢„è®­ç»ƒçš„VLMæ¨¡å‹æå–å›¾åƒè¯¦ç»†æè¿°ï¼Œä¸ºLDMè®­ç»ƒæä¾›é¢å¤–çš„æ¡ä»¶ã€‚</li>
<li>å…‹æœäº†ç°æœ‰æ‰©æ•£æ–¹æ³•çš„å±€é™æ€§ï¼Œå®ç°äº†è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>åˆ©ç”¨VLMè·å¾—æ­£å¸¸å›¾åƒçš„æè¿°ï¼Œæ— éœ€æ‰‹åŠ¨æ³¨é‡Šæˆ–é¢å¤–è®­ç»ƒã€‚</li>
<li>æ¡ä»¶æ‰©æ•£æ¨¡å‹å­¦ä¹ å¤šç±»å¼‚å¸¸æ£€æµ‹çš„é²æ£’æ­£å¸¸å›¾åƒç‰¹å¾è¡¨ç¤ºã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08173">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-eccdbad2cc56aeb0bac1935a1efbb550" align="middle">
<img src="https://picx.zhimg.com/v2-c5373fc246dfe862dfa2fd3454c8271d" align="middle">
<img src="https://picx.zhimg.com/v2-b5cba8841422fa08ee69d65a31ec3b86" align="middle">
<img src="https://picx.zhimg.com/v2-2ec1659c1c05dff6715bb017cacc58c6" align="middle">
<img src="https://picx.zhimg.com/v2-99675e70b5e66da08fa168f50cc05b48" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="VectorSynth-Fine-Grained-Satellite-Image-Synthesis-with-Structured-Semantics"><a href="#VectorSynth-Fine-Grained-Satellite-Image-Synthesis-with-Structured-Semantics" class="headerlink" title="VectorSynth: Fine-Grained Satellite Image Synthesis with Structured Semantics"></a>VectorSynth: Fine-Grained Satellite Image Synthesis with Structured Semantics</h2><p><strong>Authors:Daniel Cher, Brian Wei, Srikumar Sastry, Nathan Jacobs</strong></p>
<p>We introduce VectorSynth, a diffusion-based framework for pixel-accurate satellite image synthesis conditioned on polygonal geographic annotations with semantic attributes. Unlike prior text- or layout-conditioned models, VectorSynth learns dense cross-modal correspondences that align imagery and semantic vector geometry, enabling fine-grained, spatially grounded edits. A vision language alignment module produces pixel-level embeddings from polygon semantics; these embeddings guide a conditional image generation framework to respect both spatial extents and semantic cues. VectorSynth supports interactive workflows that mix language prompts with geometry-aware conditioning, allowing rapid what-if simulations, spatial edits, and map-informed content generation. For training and evaluation, we assemble a collection of satellite scenes paired with pixel-registered polygon annotations spanning diverse urban scenes with both built and natural features. We observe strong improvements over prior methods in semantic fidelity and structural realism, and show that our trained vision language model demonstrates fine-grained spatial grounding. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/mvrl/VectorSynth">https://github.com/mvrl/VectorSynth</a>.</p>
<blockquote>
<p>æˆ‘ä»¬ä»‹ç»äº†VectorSynthï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„åƒç´ çº§å«æ˜Ÿå›¾åƒåˆæˆæ¡†æ¶ï¼Œå®ƒæ ¹æ®å¸¦æœ‰è¯­ä¹‰å±æ€§çš„å¤šè¾¹å½¢åœ°ç†æ³¨é‡Šè¿›è¡Œæ¡ä»¶åˆæˆã€‚ä¸ä¹‹å‰çš„æ–‡æœ¬æˆ–å¸ƒå±€æ¡ä»¶æ¨¡å‹ä¸åŒï¼ŒVectorSynthå­¦ä¹ å¯†é›†çš„å¤šæ¨¡å¼å¯¹åº”ï¼Œå¯¹é½å›¾åƒå’Œè¯­ä¹‰çŸ¢é‡å‡ ä½•ï¼Œä»è€Œå®ç°ç²¾ç»†çš„ã€ç©ºé—´åŸºç¡€çš„ç¼–è¾‘ã€‚è§†è§‰è¯­è¨€å¯¹é½æ¨¡å—æ ¹æ®å¤šè¾¹å½¢è¯­ä¹‰ç”Ÿæˆåƒç´ çº§åµŒå…¥ï¼›è¿™äº›åµŒå…¥å¼•å¯¼æ¡ä»¶å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œæ—¢å°Šé‡ç©ºé—´èŒƒå›´ä¹Ÿå°Šé‡è¯­ä¹‰çº¿ç´¢ã€‚VectorSynthæ”¯æŒäº¤äº’å·¥ä½œæµç¨‹ï¼Œå°†è¯­è¨€æç¤ºä¸å‡ ä½•æ„ŸçŸ¥æ¡ä»¶ç›¸ç»“åˆï¼Œå…è®¸å¿«é€Ÿæ¨¡æ‹Ÿâ€œå¦‚æœâ€åœºæ™¯ã€ç©ºé—´ç¼–è¾‘å’Œåœ°å›¾ä¿¡æ¯å†…å®¹ç”Ÿæˆã€‚ä¸ºäº†è®­ç»ƒå’Œè¯„ä¼°ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ç³»åˆ—å«æ˜Ÿåœºæ™¯ï¼Œé…ä»¥è¦†ç›–å¤šç§åŸå¸‚åœºæ™¯çš„åƒç´ çº§å¤šè¾¹å½¢æ³¨é‡Šï¼ŒåŒ…æ‹¬äººé€ å’Œè‡ªç„¶ç‰¹å¾ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¯­ä¹‰ä¿çœŸåº¦å’Œç»“æ„é€¼çœŸæ€§éƒ½æœ‰æ˜¾è‘—æ”¹å–„ï¼Œå¹¶ä¸”æ˜¾ç¤ºæˆ‘ä»¬è®­ç»ƒè¿‡çš„è§†è§‰è¯­è¨€æ¨¡å‹å…·æœ‰ç²¾ç»†çš„ç©ºé—´å®šä½èƒ½åŠ›ã€‚ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/mvrl/VectorSynth%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/mvrl/VectorSynthæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07744v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>VectorSynthæ˜¯ä¸€æ¬¾åŸºäºæ‰©æ•£çš„åƒç´ çº§å«æ˜Ÿå›¾åƒåˆæˆæ¡†æ¶ï¼Œæ ¹æ®å¤šè¾¹å½¢åœ°ç†æ³¨é‡Šå’Œå…¶è¯­ä¹‰å±æ€§è¿›è¡Œæ¡ä»¶åˆæˆã€‚ä¸åŒäºä»¥å¾€çš„æ–‡æœ¬æˆ–å¸ƒå±€æ¡ä»¶æ¨¡å‹ï¼ŒVectorSynthå­¦ä¹ å¯†é›†çš„å¤šæ¨¡æ€å¯¹åº”å…³ç³»ï¼Œå¯¹é½å›¾åƒä¸è¯­ä¹‰çŸ¢é‡å‡ ä½•ï¼Œå®ç°ç²¾ç»†çš„ç©ºé—´å®šä½ç¼–è¾‘ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªè§†è§‰è¯­è¨€å¯¹é½æ¨¡å—ï¼Œå¯ä»å¤šè¾¹å½¢è¯­ä¹‰ä¸­äº§ç”Ÿåƒç´ çº§åµŒå…¥ï¼Œè¿™äº›åµŒå…¥å¼•å¯¼æ¡ä»¶å›¾åƒç”Ÿæˆæ¡†æ¶ï¼Œå°Šé‡ç©ºé—´èŒƒå›´å’Œè¯­ä¹‰çº¿ç´¢ã€‚VectorSynthæ”¯æŒäº¤äº’å¼å·¥ä½œæµç¨‹ï¼Œå°†è¯­è¨€æç¤ºä¸å‡ ä½•æ„ŸçŸ¥æ¡ä»¶ç›¸ç»“åˆï¼Œå¯å®ç°å¿«é€Ÿåœºæ™¯æ¨¡æ‹Ÿã€ç©ºé—´ç¼–è¾‘å’Œåœ°å›¾å†…å®¹ç”Ÿæˆã€‚é€šè¿‡è®­ç»ƒå’Œè¯„ä¼°ï¼Œè¯¥æ¡†æ¶åœ¨è¯­ä¹‰ä¿çœŸå’Œç»“æ„ç°å®æ„Ÿæ–¹é¢è¡¨ç°å‡ºå¯¹å…ˆå‰æ–¹æ³•çš„æ˜¾è‘—æ”¹è¿›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>VectorSynthæ˜¯ä¸€ä¸ªåŸºäºæ‰©æ•£çš„å«æ˜Ÿå›¾åƒåˆæˆæ¡†æ¶ï¼Œèƒ½å¤Ÿæ ¹æ®å¤šè¾¹å½¢åœ°ç†æ³¨é‡Šå’Œå…¶è¯­ä¹‰å±æ€§è¿›è¡Œåƒç´ çº§åˆæˆã€‚</li>
<li>è¯¥æ¡†æ¶å­¦ä¹ äº†å¯†é›†çš„å¤šæ¨¡æ€å¯¹åº”å…³ç³»ï¼Œä½¿å›¾åƒä¸è¯­ä¹‰çŸ¢é‡å‡ ä½•å¯¹é½ï¼Œå®ç°äº†ç²¾ç»†çš„ç©ºé—´å®šä½ç¼–è¾‘ã€‚</li>
<li>VectorSynthåŒ…æ‹¬è§†è§‰è¯­è¨€å¯¹é½æ¨¡å—ï¼Œäº§ç”Ÿåƒç´ çº§åµŒå…¥ï¼Œå¼•å¯¼æ¡ä»¶å›¾åƒç”Ÿæˆæ¡†æ¶ã€‚</li>
<li>è¯¥æ¡†æ¶æ”¯æŒäº¤äº’å¼å·¥ä½œæµç¨‹ï¼Œç»“åˆè¯­è¨€æç¤ºå’Œå‡ ä½•æ„ŸçŸ¥æ¡ä»¶ï¼Œå®ç°å¿«é€Ÿåœºæ™¯æ¨¡æ‹Ÿã€ç©ºé—´ç¼–è¾‘å’Œåœ°å›¾å†…å®¹ç”Ÿæˆã€‚</li>
<li>VectorSynthåœ¨è¯­ä¹‰ä¿çœŸå’Œç»“æ„ç°å®æ„Ÿæ–¹é¢è¡¨ç°å‡ºå¯¹å…ˆå‰æ–¹æ³•çš„æ˜¾è‘—æ”¹è¿›ã€‚</li>
<li>è¯¥æ¡†æ¶èƒ½å¤Ÿå°Šé‡ç©ºé—´èŒƒå›´å’Œè¯­ä¹‰çº¿ç´¢ï¼Œåœ¨ç”Ÿæˆå›¾åƒæ—¶å……åˆ†è€ƒè™‘è¿™äº›å› ç´ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07744">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-98ec886cc354e830c96fcc7d04e98a05" align="middle">
<img src="https://picx.zhimg.com/v2-edbe35b44fd59f4f27c664c849aab216" align="middle">
<img src="https://picx.zhimg.com/v2-4905a17e97f42484635683e8234ed15a" align="middle">
<img src="https://picx.zhimg.com/v2-246cddba47cd3bcd172db8efe36545dd" align="middle">
<img src="https://picx.zhimg.com/v2-12c592a409f0ce230c23cb2382b89360" align="middle">
<img src="https://picx.zhimg.com/v2-b52fb669c159b614f533d0dc9c573cb2" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="From-ACR-O-RADS-2022-to-Explainable-Deep-Learning-Comparative-Performance-of-Expert-Radiologists-Convolutional-Neural-Networks-Vision-Transformers-and-Fusion-Models-in-Ovarian-Masses"><a href="#From-ACR-O-RADS-2022-to-Explainable-Deep-Learning-Comparative-Performance-of-Expert-Radiologists-Convolutional-Neural-Networks-Vision-Transformers-and-Fusion-Models-in-Ovarian-Masses" class="headerlink" title="From ACR O-RADS 2022 to Explainable Deep Learning: Comparative Performance of Expert Radiologists, Convolutional Neural Networks, Vision Transformers, and Fusion Models in Ovarian Masses"></a>From ACR O-RADS 2022 to Explainable Deep Learning: Comparative Performance of Expert Radiologists, Convolutional Neural Networks, Vision Transformers, and Fusion Models in Ovarian Masses</h2><p><strong>Authors:Ali Abbasian Ardakani, Afshin Mohammadi, Alisa Mohebbi, Anushya Vijayananthan, Sook Sam Leong, Lim Yi Ting, Mohd Kamil Bin Mohamad Fabell, U Rajendra Acharya, Sepideh Hatamikia</strong></p>
<p>Background: The 2022 update of the Ovarian-Adnexal Reporting and Data System (O-RADS) ultrasound classification refines risk stratification for adnexal lesions, yet human interpretation remains subject to variability and conservative thresholds. Concurrently, deep learning (DL) models have demonstrated promise in image-based ovarian lesion characterization. This study evaluates radiologist performance applying O-RADS v2022, compares it to leading convolutional neural network (CNN) and Vision Transformer (ViT) models, and investigates the diagnostic gains achieved by hybrid human-AI frameworks. Methods: In this single-center, retrospective cohort study, a total of 512 adnexal mass images from 227 patients (110 with at least one malignant cyst) were included. Sixteen DL models, including DenseNets, EfficientNets, ResNets, VGGs, Xception, and ViTs, were trained and validated. A hybrid model integrating radiologist O-RADS scores with DL-predicted probabilities was also built for each scheme. Results: Radiologist-only O-RADS assessment achieved an AUC of 0.683 and an overall accuracy of 68.0%. CNN models yielded AUCs of 0.620 to 0.908 and accuracies of 59.2% to 86.4%, while ViT16-384 reached the best performance, with an AUC of 0.941 and an accuracy of 87.4%. Hybrid human-AI frameworks further significantly enhanced the performance of CNN models; however, the improvement for ViT models was not statistically significant (P-value &gt;0.05). Conclusions: DL models markedly outperform radiologist-only O-RADS v2022 assessment, and the integration of expert scores with AI yields the highest diagnostic accuracy and discrimination. Hybrid human-AI paradigms hold substantial potential to standardize pelvic ultrasound interpretation, reduce false positives, and improve detection of high-risk lesions.</p>
<blockquote>
<p>èƒŒæ™¯ï¼šåµå·¢é™„ä»¶æŠ¥å‘Šå’Œæ•°æ®ç³»ç»Ÿï¼ˆO-RADSï¼‰çš„2022ç‰ˆè¶…å£°åˆ†ç±»ç»†åŒ–äº†é™„ä»¶ç—…å˜çš„é£é™©åˆ†å±‚ï¼Œä½†äººä¸ºè§£è¯»ä»å­˜åœ¨å·®å¼‚å’Œä¿å®ˆé˜ˆå€¼ã€‚åŒæ—¶ï¼Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹åœ¨åŸºäºå›¾åƒçš„åµå·¢ç—…å˜è¡¨å¾æ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚æœ¬ç ”ç©¶è¯„ä¼°äº†æ”¾å°„ç§‘åŒ»ç”Ÿåº”ç”¨O-RADS v2022çš„è¡¨ç°ï¼Œå°†å…¶ä¸é¢†å…ˆçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶ç ”ç©¶äº†æ··åˆäººå·¥æ™ºèƒ½æ¡†æ¶å®ç°çš„è¯Šæ–­å¢ç›Šã€‚æ–¹æ³•ï¼šåœ¨è¿™é¡¹å•ä¸­å¿ƒå›é¡¾æ€§é˜Ÿåˆ—ç ”ç©¶ä¸­ï¼Œå…±çº³å…¥æ¥è‡ª227åæ‚£è€…ï¼ˆå…¶ä¸­110åæ‚£è€…è‡³å°‘æœ‰ä¸€ä¸ªæ¶æ€§å›Šè‚¿ï¼‰çš„512å¼ é™„ä»¶è‚¿å—å›¾åƒã€‚è®­ç»ƒå¹¶éªŒè¯äº†åŒ…æ‹¬DenseNetsã€EfficientNetsã€ResNetsã€VGGsã€Xceptionå’ŒViTsåœ¨å†…çš„16ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚è¿˜æ„å»ºäº†æ··åˆæ¨¡å‹ï¼Œå°†æ”¾å°„ç§‘åŒ»ç”Ÿçš„O-RADSè¯„åˆ†ä¸æ·±åº¦å­¦ä¹ é¢„æµ‹çš„æ¦‚è¿›è¡Œæ•´åˆã€‚ç»“æœï¼šä»…ä½¿ç”¨æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡ŒO-RADSè¯„ä¼°çš„AUCä¸º0.683ï¼Œæ€»ä½“å‡†ç¡®ç‡ä¸º68.0%ã€‚CNNæ¨¡å‹çš„AUCä¸º0.620è‡³0.908ï¼Œå‡†ç¡®ç‡ä¸º59.2%è‡³86.4%ï¼Œå…¶ä¸­ViT16-384è¡¨ç°æœ€ä½³ï¼ŒAUCä¸º0.941ï¼Œå‡†ç¡®ç‡ä¸º87.4%ã€‚æ··åˆäººå·¥æ™ºèƒ½æ¡†æ¶è¿›ä¸€æ­¥æ˜¾è‘—æé«˜äº†CNNæ¨¡å‹çš„æ€§èƒ½ï¼Œä½†å¯¹äºViTæ¨¡å‹çš„æ”¹è¿›å¹¶ä¸å…·æœ‰ç»Ÿè®¡å­¦ä¸Šçš„æ˜¾è‘—æ€§ï¼ˆPå€¼&gt; 0.05ï¼‰ã€‚ç»“è®ºï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹æ˜æ˜¾ä¼˜äºä»…ä½¿ç”¨æ”¾å°„ç§‘åŒ»ç”Ÿè¿›è¡ŒO-RADS v2022è¯„ä¼°ï¼Œå¹¶ä¸”ç»“åˆä¸“å®¶è¯„åˆ†å’Œäººå·¥æ™ºèƒ½å¯å¸¦æ¥æœ€é«˜çš„è¯Šæ–­å‡†ç¡®æ€§å’ŒåŒºåˆ†åº¦ã€‚æ··åˆäººç±»äººå·¥æ™ºèƒ½èŒƒå¼åœ¨æ ‡å‡†åŒ–ç›†è…”è¶…å£°æ£€æŸ¥è§£è¯»ã€å‡å°‘è¯¯æŠ¥å’Œæé«˜é«˜é£é™©ç—…å˜æ£€æµ‹æ–¹é¢æ‹¥æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.06282v1">PDF</a> 18 pages, 4 figures</p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†Ovarian-Adnexal Reporting and Data Systemï¼ˆO-RADSï¼‰çš„è¶…å£°åˆ†ç±»çš„æ›´æ–°åŠå…¶å­˜åœ¨çš„é—®é¢˜ã€‚é€šè¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹åµå·¢ç—…å˜è¿›è¡Œå›¾åƒè¯†åˆ«å±•ç°äº†è‰¯å¥½å‰æ™¯ã€‚æœ¬æ–‡å¯¹æ¯”äº†æ”¾å°„ç§‘åŒ»ç”Ÿè¿ç”¨O-RADS v2022çš„æ€§èƒ½å’Œé¢†å…ˆçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸è§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰æ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶æ¢è®¨äº†æ··åˆäººç±»äººå·¥æ™ºèƒ½æ¡†æ¶çš„è¯Šæ–­ä¼˜åŠ¿ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹æ˜¾è‘—ä¼˜äºæ”¾å°„ç§‘åŒ»ç”Ÿå•ç‹¬ä½¿ç”¨O-RADS v2022çš„è¯„ä¼°ç»“æœï¼Œç»“åˆäººå·¥æ™ºèƒ½çš„ä¸“å®¶è¯„åˆ†å¯å®ç°æœ€é«˜çš„è¯Šæ–­å‡†ç¡®æ€§å’Œé‰´åˆ«åŠ›ã€‚æ··åˆäººç±»äººå·¥æ™ºèƒ½æ¨¡å¼åœ¨æ ‡å‡†åŒ–ç›†è…”è¶…å£°è§£è¯»ã€å‡å°‘è¯¯åˆ¤é˜³æ€§ä»¥åŠæé«˜é«˜é£é™©ç—…å˜æ£€æµ‹æ–¹é¢å…·æœ‰å·¨å¤§æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>O-RADS 2022æ›´æ–°äº†å…¶è¶…å£°åˆ†ç±»ï¼Œå¯¹é™„ä»¶ç—…å˜çš„é£é™©åˆ†å±‚è¿›è¡Œäº†æ”¹è¿›ï¼Œä½†ä»å­˜åœ¨äººä¸ºè§£è¯»çš„å˜å¼‚æ€§é—®é¢˜ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŸºäºå›¾åƒçš„åµå·¢ç—…å˜è¡¨å¾ä¸­æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›ã€‚</li>
<li>æ”¾å°„ç§‘åŒ»ç”Ÿè¿ç”¨O-RADS v2022çš„æ€§èƒ½è¯„ä¼°æ˜¾ç¤ºå…¶AUCä¸º0.683ï¼Œæ•´ä½“å‡†ç¡®ç‡ä¸º68.0%ã€‚</li>
<li>CNNæ¨¡å‹çš„AUCèŒƒå›´åœ¨0.620è‡³0.908ä¹‹é—´ï¼Œå‡†ç¡®ç‡åœ¨59.2%è‡³86.4%ä¹‹é—´ï¼›è€ŒViT16-384è¡¨ç°æœ€ä½³ï¼ŒAUCä¸º0.941ï¼Œå‡†ç¡®ç‡ä¸º87.4%ã€‚</li>
<li>æ··åˆäººç±»äººå·¥æ™ºèƒ½æ¡†æ¶æ˜¾è‘—å¢å¼ºäº†CNNæ¨¡å‹çš„æ€§èƒ½ï¼Œä½†å¯¹ViTæ¨¡å‹çš„æ”¹è¿›æ²¡æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ã€‚</li>
<li>æ·±åº¦å­¦ä¹ æ¨¡å‹æ˜¾è‘—ä¼˜äºæ”¾å°„ç§‘åŒ»ç”Ÿå•ç‹¬ä½¿ç”¨O-RADS v2022çš„è¯„ä¼°ç»“æœã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.06282">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d67632b2c7f876acaa51fad19cb0b2b7" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="A-Dual-Mode-ViT-Conditioned-Diffusion-Framework-with-an-Adaptive-Conditioning-Bridge-for-Breast-Cancer-Segmentation"><a href="#A-Dual-Mode-ViT-Conditioned-Diffusion-Framework-with-an-Adaptive-Conditioning-Bridge-for-Breast-Cancer-Segmentation" class="headerlink" title="A Dual-Mode ViT-Conditioned Diffusion Framework with an Adaptive Conditioning Bridge for Breast Cancer Segmentation"></a>A Dual-Mode ViT-Conditioned Diffusion Framework with an Adaptive Conditioning Bridge for Breast Cancer Segmentation</h2><p><strong>Authors:Prateek Singh, Moumita Dholey, P. K. Vinod</strong></p>
<p>In breast ultrasound images, precise lesion segmentation is essential for early diagnosis; however, low contrast, speckle noise, and unclear boundaries make this difficult. Even though deep learning models have demonstrated potential, standard convolutional architectures frequently fall short in capturing enough global context, resulting in segmentations that are anatomically inconsistent. To overcome these drawbacks, we suggest a flexible, conditional Denoising Diffusion Model that combines an enhanced UNet-based generative decoder with a Vision Transformer (ViT) encoder for global feature extraction. We introduce three primary innovations: 1) an Adaptive Conditioning Bridge (ACB) for efficient, multi-scale fusion of semantic features; 2) a novel Topological Denoising Consistency (TDC) loss component that regularizes training by penalizing structural inconsistencies during denoising; and 3) a dual-head architecture that leverages the denoising objective as a powerful regularizer, enabling a lightweight auxiliary head to perform rapid and accurate inference on smaller datasets and a noise prediction head. Our framework establishes a new state-of-the-art on public breast ultrasound datasets, achieving Dice scores of 0.96 on BUSI, 0.90 on BrEaST and 0.97 on BUS-UCLM. Comprehensive ablation studies empirically validate that the model components are critical for achieving these results and for producing segmentations that are not only accurate but also anatomically plausible.</p>
<blockquote>
<p>åœ¨ä¹³è…ºè¶…å£°å›¾åƒä¸­ï¼Œç²¾ç¡®çš„ç—…ç¶åˆ†å‰²å¯¹äºæ—©æœŸè¯Šæ–­è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œç”±äºå›¾åƒå¯¹æ¯”åº¦ä½ã€æ–‘ç‚¹å™ªå£°å’Œè¾¹ç•Œä¸æ¸…ç­‰é—®é¢˜ï¼Œè¿™ä¸€ä»»åŠ¡å˜å¾—å›°éš¾ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ æ¨¡å‹å·²ç»æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†æ ‡å‡†çš„å·ç§¯æ¶æ„é€šå¸¸éš¾ä»¥æ•æ‰è¶³å¤Ÿçš„å…¨å±€ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´åˆ†å‰²ç»“æœè§£å‰–ä¸Šä¸è¿è´¯ã€‚ä¸ºäº†å…‹æœè¿™äº›ç¼ºç‚¹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§çµæ´»çš„æ¡ä»¶å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†åŸºäºå¢å¼ºå‹UNetçš„ç”Ÿæˆè§£ç å™¨å’ŒVision Transformerï¼ˆViTï¼‰ç¼–ç å™¨ï¼Œç”¨äºå…¨å±€ç‰¹å¾æå–ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸‰ä¸ªä¸»è¦åˆ›æ–°ç‚¹ï¼š1ï¼‰è‡ªé€‚åº”æ¡ä»¶æ¡¥ï¼ˆACBï¼‰ï¼Œç”¨äºé«˜æ•ˆçš„å¤šå°ºåº¦è¯­ä¹‰ç‰¹å¾èåˆï¼›2ï¼‰ä¸€ç§æ–°çš„æ‹“æ‰‘å»å™ªä¸€è‡´æ€§ï¼ˆTDCï¼‰æŸå¤±ç»„ä»¶ï¼Œé€šè¿‡æƒ©ç½šå»å™ªè¿‡ç¨‹ä¸­çš„ç»“æ„ä¸ä¸€è‡´æ€§æ¥æ­£åˆ™åŒ–è®­ç»ƒï¼›3ï¼‰åŒå¤´æ¶æ„ï¼Œåˆ©ç”¨å»å™ªç›®æ ‡ä½œä¸ºå¼ºå¤§çš„æ­£åˆ™åŒ–å™¨ï¼Œä½¿è½»å‹è¾…åŠ©å¤´èƒ½åœ¨è¾ƒå°æ•°æ®é›†ä¸Šè¿›è¡Œå¿«é€Ÿä¸”å‡†ç¡®çš„æ¨ç†ï¼Œä»¥åŠä¸€ä¸ªå™ªå£°é¢„æµ‹å¤´ã€‚æˆ‘ä»¬çš„æ¡†æ¶åœ¨å…¬å…±ä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šè¾¾åˆ°äº†æœ€æ–°æ°´å¹³ï¼Œåœ¨BUSIä¸Šå®ç°äº†0.96çš„Diceå¾—åˆ†ï¼Œåœ¨BrEaSTä¸Šå®ç°äº†0.90çš„Diceå¾—åˆ†ï¼Œä»¥åŠåœ¨BUS-UCLMä¸Šå®ç°äº†0.97çš„Diceå¾—åˆ†ã€‚ç»¼åˆæ¶ˆèç ”ç©¶ç»éªŒéªŒè¯äº†æˆ‘ä»¬æ¨¡å‹ç»„ä»¶å¯¹äºå®ç°è¿™äº›ç»“æœå’Œäº§ç”Ÿæ—¢å‡†ç¡®åˆè§£å‰–ä¸Šåˆç†çš„åˆ†å‰²ç»“æœè‡³å…³é‡è¦ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05989v1">PDF</a> 5 pages, 2 figures, 3 tables, submitted to ISBI 2026</p>
<p><strong>Summary</strong><br>     é’ˆå¯¹ä¹³è…ºè¶…å£°å›¾åƒä¸­çš„ç—…ç¶åˆ†å‰²é—®é¢˜ï¼Œç”±äºä½å¯¹æ¯”åº¦ã€æ–‘ç‚¹å™ªå£°å’Œè¾¹ç•Œä¸æ¸…ç­‰æŒ‘æˆ˜ï¼Œç²¾ç¡®åˆ†å‰²å¯¹æ—©æœŸè¯Šæ–­è‡³å…³é‡è¦ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§çµæ´»çš„ã€æœ‰æ¡ä»¶çš„å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»“åˆäº†å¢å¼ºçš„UNetç”Ÿæˆè§£ç å™¨å’ŒVision Transformerï¼ˆViTï¼‰ç¼–ç å™¨è¿›è¡Œå…¨å±€ç‰¹å¾æå–ã€‚ä¸»è¦åˆ›æ–°åŒ…æ‹¬è‡ªé€‚åº”æ¡ä»¶æ¡¥ï¼ˆACBï¼‰ã€æ‹“æ‰‘å»å™ªä¸€è‡´æ€§ï¼ˆTDCï¼‰æŸå¤±ç»„ä»¶å’ŒåŒå¤´æ¶æ„ã€‚è¯¥ç ”ç©¶åœ¨å…¬å…±ä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šå–å¾—äº†æœ€æ–°æŠ€æœ¯ï¼Œå®ç°äº†BUSIä¸Šçš„Diceå¾—åˆ†0.96ã€BrEaSTä¸Šçš„0.9å’ŒBUS-UCLMä¸Šçš„0.97ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ä¹³è…ºè¶…å£°å›¾åƒä¸­çš„ç—…ç¶åˆ†å‰²å¯¹æ—©æœŸè¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†ç”±äºä½å¯¹æ¯”åº¦ã€æ–‘ç‚¹å™ªå£°å’Œè¾¹ç•Œä¸æ¸…ç­‰æŒ‘æˆ˜ï¼Œåˆ†å‰²å›°éš¾ã€‚</li>
<li>ç°æœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å¦‚æ ‡å‡†å·ç§¯æ¶æ„åœ¨æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œå¯¼è‡´åˆ†å‰²ç»“æœè§£å‰–ä¸Šä¸ä¸€è‡´ã€‚</li>
<li>ç ”ç©¶æå‡ºäº†ä¸€ç§çµæ´»çš„ã€æœ‰æ¡ä»¶çš„å»å™ªæ‰©æ•£æ¨¡å‹ï¼Œç»“åˆUNetç”Ÿæˆè§£ç å™¨å’ŒVision Transformerï¼ˆViTï¼‰ç¼–ç å™¨è¿›è¡Œç‰¹å¾æå–ã€‚</li>
<li>å¼•å…¥ä¸‰ä¸ªä¸»è¦åˆ›æ–°ç‚¹ï¼šè‡ªé€‚åº”æ¡ä»¶æ¡¥ï¼ˆACBï¼‰ã€æ‹“æ‰‘å»å™ªä¸€è‡´æ€§ï¼ˆTDCï¼‰æŸå¤±ç»„ä»¶å’ŒåŒå¤´æ¶æ„ã€‚</li>
<li>è¯¥æ¨¡å‹åœ¨å…¬å…±ä¹³è…ºè¶…å£°æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œè¾¾åˆ°æœ€å…ˆè¿›çš„Diceå¾—åˆ†ã€‚</li>
<li>å…¨é¢çš„æ¶ˆèç ”ç©¶è¯æ˜ï¼Œæ¨¡å‹ç»„ä»¶å¯¹äºå®ç°è¿™äº›ç»“æœå’Œäº§ç”Ÿè§£å‰–ä¸Šåˆç†çš„åˆ†å‰²è‡³å…³é‡è¦ã€‚</li>
<li>è¯¥æ¨¡å‹èƒ½åœ¨è¾ƒå°çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¿«é€Ÿå‡†ç¡®çš„æ¨æ–­ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05989">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bfc1fb58eecbd74b4b2f1df70397dce5" align="middle">
<img src="https://picx.zhimg.com/v2-e9ebbfae91e260c2e641ad9cb67908c3" align="middle">
<img src="https://picx.zhimg.com/v2-6796816b46bd7dcff5228928a50bec80" align="middle">
<img src="https://picx.zhimg.com/v2-a371c8ca0bdff79cd1f4fe2dec66e4aa" align="middle">
<img src="https://picx.zhimg.com/v2-a28b29948c60f73fa27cf18feefa0063" align="middle">
<img src="https://picx.zhimg.com/v2-01572a57df002f872fd868117dc290f0" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1><h2 id="DGL-RSIS-Decoupling-Global-Spatial-Context-and-Local-Class-Semantics-for-Training-Free-Remote-Sensing-Image-Segmentation"><a href="#DGL-RSIS-Decoupling-Global-Spatial-Context-and-Local-Class-Semantics-for-Training-Free-Remote-Sensing-Image-Segmentation" class="headerlink" title="DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation"></a>DGL-RSIS: Decoupling Global Spatial Context and Local Class Semantics for Training-Free Remote Sensing Image Segmentation</h2><p><strong>Authors:Boyi Li, Ce Zhang, Richard M. Timmerman, Wenxuan Bao</strong></p>
<p>The emergence of vision language models (VLMs) bridges the gap between vision and language, enabling multimodal understanding beyond traditional visual-only deep learning models. However, transferring VLMs from the natural image domain to remote sensing (RS) segmentation remains challenging due to the large domain gap and the diversity of RS inputs across tasks, particularly in open-vocabulary semantic segmentation (OVSS) and referring expression segmentation (RES). Here, we propose a training-free unified framework, termed DGL-RSIS, which decouples visual and textual representations and performs visual-language alignment at both local semantic and global contextual levels. Specifically, a Global-Local Decoupling (GLD) module decomposes textual inputs into local semantic tokens and global contextual tokens, while image inputs are partitioned into class-agnostic mask proposals. Then, a Local Visual-Textual Alignment (LVTA) module adaptively extracts context-aware visual features from the mask proposals and enriches textual features through knowledge-guided prompt engineering, achieving OVSS from a local perspective. Furthermore, a Global Visual-Textual Alignment (GVTA) module employs a global-enhanced Grad-CAM mechanism to capture contextual cues for referring expressions, followed by a mask selection module that integrates pixel-level activations into mask-level segmentation outputs, thereby achieving RES from a global perspective. Experiments on the iSAID (OVSS) and RRSIS-D (RES) benchmarks demonstrate that DGL-RSIS outperforms existing training-free approaches. Ablation studies further validate the effectiveness of each module. To the best of our knowledge, this is the first unified training-free framework for RS image segmentation, which effectively transfers the semantic capability of VLMs trained on natural images to the RS domain without additional training.</p>
<blockquote>
<p>è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰çš„å‡ºç°ï¼Œæ¶èµ·äº†è§†è§‰å’Œè¯­è¨€çš„æ¡¥æ¢ï¼Œå®ç°äº†è¶…è¶Šä¼ ç»Ÿä»…è§†è§‰æ·±åº¦å­¦ä¹ çš„å¤šæ¨¡æ€ç†è§£ã€‚ç„¶è€Œï¼Œå°†VLMsä»è‡ªç„¶å›¾åƒé¢†åŸŸè¿ç§»åˆ°é¥æ„Ÿï¼ˆRSï¼‰åˆ†å‰²ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä¸»è¦ç”±äºé¢†åŸŸå·®è·å¤§ä»¥åŠé¥æ„Ÿè¾“å…¥ä»»åŠ¡çš„å¤šæ ·æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ï¼ˆOVSSï¼‰å’Œå¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ï¼ˆRESï¼‰ä¸­ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ç»Ÿä¸€æ¡†æ¶ï¼Œåä¸ºDGL-RSISï¼Œè¯¥æ¡†æ¶è§£è€¦è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºï¼Œå¹¶åœ¨å±€éƒ¨è¯­ä¹‰å’Œå…¨å±€ä¸Šä¸‹æ–‡çº§åˆ«è¿›è¡Œè§†è§‰è¯­è¨€å¯¹é½ã€‚å…·ä½“è€Œè¨€ï¼Œå…¨å±€å±€éƒ¨è§£è€¦ï¼ˆGLDï¼‰æ¨¡å—å°†æ–‡æœ¬è¾“å…¥åˆ†è§£ä¸ºå±€éƒ¨è¯­ä¹‰æ ‡è®°å’Œå…¨å±€ä¸Šä¸‹æ–‡æ ‡è®°ï¼Œè€Œå›¾åƒè¾“å…¥åˆ™è¢«åˆ’åˆ†ä¸ºç±»æœªçŸ¥æ©è†œææ¡ˆã€‚ç„¶åï¼Œå±€éƒ¨è§†è§‰æ–‡æœ¬å¯¹é½ï¼ˆLVTAï¼‰æ¨¡å—è‡ªé€‚åº”åœ°ä»æ©è†œææ¡ˆä¸­æå–ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§†è§‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡çŸ¥è¯†å¼•å¯¼æç¤ºå·¥ç¨‹ä¸°å¯Œæ–‡æœ¬ç‰¹å¾ï¼Œä»å±€éƒ¨è§’åº¦å®ç°OVSSã€‚æ­¤å¤–ï¼Œå…¨å±€è§†è§‰æ–‡æœ¬å¯¹é½ï¼ˆGVTAï¼‰æ¨¡å—é‡‡ç”¨å…¨å±€å¢å¼ºGrad-CAMæœºåˆ¶æ¥æ•è·å¼•ç”¨è¡¨è¾¾å¼çš„ä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œéšåæ˜¯æ©è†œé€‰æ‹©æ¨¡å—ï¼Œè¯¥æ¨¡å—å°†åƒç´ çº§æ¿€æ´»æ•´åˆåˆ°æ©è†œçº§åˆ†å‰²è¾“å‡ºä¸­ï¼Œä»è€Œå®ç°ä»å…¨å±€è§’åº¦çš„RESã€‚åœ¨iSAIDï¼ˆOVSSï¼‰å’ŒRRSIS-Dï¼ˆRESï¼‰åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDGL-RSISåœ¨æ— éœ€è®­ç»ƒçš„æ–¹æ¡ˆä¸­çš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰æ–¹æ³•ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥éªŒè¯äº†æ¯ä¸ªæ¨¡å—çš„æœ‰æ•ˆæ€§ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„é¥æ„Ÿå›¾åƒåˆ†å‰²ç»Ÿä¸€æ¡†æ¶ï¼Œæœ‰æ•ˆåœ°å°†åŸºäºè‡ªç„¶å›¾åƒè®­ç»ƒçš„VLMsçš„è¯­ä¹‰èƒ½åŠ›è½¬ç§»åˆ°é¥æ„Ÿé¢†åŸŸè€Œæ— éœ€é¢å¤–çš„è®­ç»ƒã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2509.00598v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>è¯¥æ–‡æœ¬ä»‹ç»äº†ä¸€ç§åä¸ºDGL-RSISçš„æ— è®­ç»ƒç»Ÿä¸€æ¡†æ¶ï¼Œå®ƒå°†è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºåˆ†ç¦»ï¼Œå¹¶åœ¨å±€éƒ¨è¯­ä¹‰å’Œå…¨å±€ä¸Šä¸‹æ–‡çº§åˆ«è¿›è¡Œè§†è§‰è¯­è¨€å¯¹é½ï¼Œä»è€Œå®ç°äº†é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²å’Œå¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨iSAIDå’ŒRRSIS-DåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ— éœ€é¢å¤–è®­ç»ƒå³å¯å°†è‡ªç„¶è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰èƒ½åŠ›è½¬ç§»åˆ°é¥æ„Ÿé¢†åŸŸã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¼•å…¥æ–°çš„æ— è®­ç»ƒç»Ÿä¸€æ¡†æ¶DGL-RSISï¼Œå®ç°é¥æ„Ÿå›¾åƒåˆ†å‰²ä¸­çš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²å’Œå¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ã€‚</li>
<li>é€šè¿‡åˆ†ç¦»è§†è§‰å’Œæ–‡æœ¬è¡¨ç¤ºï¼Œå¹¶åœ¨å±€éƒ¨è¯­ä¹‰å’Œå…¨å±€ä¸Šä¸‹æ–‡çº§åˆ«è¿›è¡Œè§†è§‰è¯­è¨€å¯¹é½ï¼Œæé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>åˆ©ç”¨å…¨å±€å±€éƒ¨è§£è€¦æ¨¡å—å°†æ–‡æœ¬è¾“å…¥åˆ†è§£ä¸ºå±€éƒ¨è¯­ä¹‰ä»¤ç‰Œå’Œå…¨å±€ä¸Šä¸‹æ–‡ä»¤ç‰Œï¼ŒåŒæ—¶å›¾åƒè¾“å…¥è¢«åˆ’åˆ†ä¸ºç±»æ— å…³çš„æ©è†œææ¡ˆã€‚</li>
<li>é€šè¿‡å±€éƒ¨è§†è§‰æ–‡æœ¬å¯¹é½æ¨¡å—è‡ªé€‚åº”æå–ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è§†è§‰ç‰¹å¾ï¼Œå¹¶é€šè¿‡çŸ¥è¯†å¼•å¯¼æç¤ºå·¥ç¨‹ä¸°å¯Œæ–‡æœ¬ç‰¹å¾ï¼Œå®ç°å±€éƒ¨è§†è§’çš„å¼€æ”¾è¯æ±‡è¯­ä¹‰åˆ†å‰²ã€‚</li>
<li>å…¨çƒè§†è§‰æ–‡æœ¬å¯¹é½æ¨¡å—é‡‡ç”¨å…¨çƒå¢å¼ºGrad-CAMæœºåˆ¶æ•è·å¼•ç”¨è¡¨è¾¾å¼çš„ä¸Šä¸‹æ–‡çº¿ç´¢ï¼Œå¹¶é€šè¿‡æ©è†œé€‰æ‹©æ¨¡å—å°†åƒç´ çº§æ¿€æ´»é›†æˆåˆ°æ©è†œçº§åˆ†å‰²è¾“å‡ºä¸­ï¼Œå®ç°å…¨å±€è§†è§’çš„å¼•ç”¨è¡¨è¾¾å¼åˆ†å‰²ã€‚</li>
<li>åœ¨iSAIDå’ŒRRSIS-DåŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDGL-RSISæ¡†æ¶çš„æ€§èƒ½ä¼˜äºç°æœ‰çš„æ— è®­ç»ƒæ–¹æ³•ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00598">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-214dac5d69ee874e35e792ca9397ca9c" align="middle">
<img src="https://picx.zhimg.com/v2-b57de51f9f68cd42bfee2cf857a43c3b" align="middle">
<img src="https://picx.zhimg.com/v2-f5046e74e4dda7e814f33d6008bdbf05" align="middle">
</details>


<h1 id="-9"><a href="#-9" class="headerlink" title=""></a></h1><h2 id="LPLC-A-Dataset-for-License-Plate-Legibility-Classification"><a href="#LPLC-A-Dataset-for-License-Plate-Legibility-Classification" class="headerlink" title="LPLC: A Dataset for License Plate Legibility Classification"></a>LPLC: A Dataset for License Plate Legibility Classification</h2><p><strong>Authors:Lucas Wojcik, Gabriel E. Lima, Valfride Nascimento, Eduil Nascimento, Rayson Laroca, David Menotti</strong></p>
<p>Automatic License Plate Recognition (ALPR) faces a major challenge when dealing with illegible license plates (LPs). While reconstruction methods such as super-resolution (SR) have emerged, the core issue of recognizing these low-quality LPs remains unresolved. To optimize model performance and computational efficiency, image pre-processing should be applied selectively to cases that require enhanced legibility. To support research in this area, we introduce a novel dataset comprising 10,210 images of vehicles with 12,687 annotated LPs for legibility classification (the LPLC dataset). The images span a wide range of vehicle types, lighting conditions, and camera&#x2F;image quality levels. We adopt a fine-grained annotation strategy that includes vehicle- and LP-level occlusions, four legibility categories (perfect, good, poor, and illegible), and character labels for three categories (excluding illegible LPs). As a benchmark, we propose a classification task using three image recognition networks to determine whether an LP image is good enough, requires super-resolution, or is completely unrecoverable. The overall F1 score, which remained below 80% for all three baseline models (ViT, ResNet, and YOLO), together with the analyses of SR and LP recognition methods, highlights the difficulty of the task and reinforces the need for further research. The proposed dataset is publicly available at <a target="_blank" rel="noopener" href="https://github.com/lmlwojcik/lplc-dataset">https://github.com/lmlwojcik/lplc-dataset</a>.</p>
<blockquote>
<p>åœ¨è‡ªåŠ¨è½¦ç‰Œè¯†åˆ«ï¼ˆALPRï¼‰ä¸­ï¼Œé‡åˆ°ä¸æ¸…æ™°çš„è½¦ç‰Œï¼ˆLPsï¼‰æ˜¯ä¸€å¤§æŒ‘æˆ˜ã€‚è™½ç„¶å‡ºç°äº†è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰ç­‰é‡å»ºæ–¹æ³•ï¼Œä½†è¯†åˆ«è¿™äº›ä½è´¨é‡è½¦ç‰Œçš„æ ¸å¿ƒé—®é¢˜ä»æœªè§£å†³ã€‚ä¸ºäº†ä¼˜åŒ–æ¨¡å‹æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ï¼Œåº”é€‰æ‹©æ€§åœ°å¯¹éœ€è¦æé«˜æ¸…æ™°åº¦çš„æ¡ˆä¾‹è¿›è¡Œå›¾åƒé¢„å¤„ç†ã€‚ä¸ºäº†æ”¯æŒè¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒ…å«10210å¼ è½¦è¾†å›¾åƒå’Œ12687ä¸ªå¸¦æ³¨é‡Šè½¦ç‰Œçš„æ–°æ•°æ®é›†ï¼Œç”¨äºæ¸…æ™°åº¦åˆ†ç±»ï¼ˆLPLCæ•°æ®é›†ï¼‰ã€‚è¿™äº›å›¾åƒæ¶µç›–äº†å„ç§è½¦å‹ã€å…‰ç…§æ¡ä»¶å’Œæ‘„åƒå¤´&#x2F;å›¾åƒè´¨é‡æ°´å¹³ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ç²¾ç»†çš„æ³¨é‡Šç­–ç•¥ï¼ŒåŒ…æ‹¬è½¦è¾†å’Œè½¦ç‰Œçº§åˆ«çš„é®æŒ¡æƒ…å†µï¼Œå››ä¸ªæ¸…æ™°åº¦ç±»åˆ«ï¼ˆå®Œç¾ã€è‰¯å¥½ã€è¾ƒå·®å’Œä¸å¯è¯»ï¼‰ï¼Œä»¥åŠä¸‰ä¸ªç±»åˆ«ï¼ˆä¸åŒ…æ‹¬ä¸å¯è¯»è½¦ç‰Œï¼‰çš„å­—ç¬¦æ ‡ç­¾ã€‚ä½œä¸ºåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨ä¸‰ä¸ªå›¾åƒè¯†åˆ«ç½‘ç»œæ¥ç¡®å®šè½¦ç‰Œå›¾åƒæ˜¯å¦è¶³å¤Ÿå¥½ã€æ˜¯å¦éœ€è¦è¶…åˆ†è¾¨ç‡å¤„ç†æˆ–æ˜¯å¦å®Œå…¨æ— æ³•æ¢å¤ã€‚ä¸‰ä¸ªåŸºå‡†æ¨¡å‹ï¼ˆViTã€ResNetå’ŒYOLOï¼‰çš„æ€»ä½“F1åˆ†æ•°å‡ä½äº80%ï¼Œç»“åˆè¶…åˆ†è¾¨ç‡å’Œè½¦ç‰Œè¯†åˆ«æ–¹æ³•çš„åˆ†æï¼Œçªæ˜¾äº†ä»»åŠ¡çš„éš¾åº¦å¹¶å¼ºè°ƒäº†éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶çš„å¿…è¦æ€§ã€‚æ‰€æå‡ºçš„æ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/lmlwojcik/lplc-dataset%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/lmlwojcik/lplc-datasetå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2508.18425v2">PDF</a> Accepted for presentation at the Conference on Graphics, Patterns and Images (SIBGRAPI) 2025</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è½¦ç‰Œè¯†åˆ«ï¼ˆALPRï¼‰ä¸­é‡åˆ°çš„ä¸æ¸…æ™°è½¦ç‰Œé—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†LPLCã€‚è¯¥æ•°æ®é›†åŒ…å«10,210å¼ è½¦è¾†å›¾åƒå’Œ12,687ä¸ªå¸¦æ³¨é‡Šçš„è½¦ç‰Œï¼Œç”¨äºåˆ†ç±»è½¦ç‰Œçš„å¯è¯»æ€§ã€‚é‡‡ç”¨ç²¾ç»†æ ‡æ³¨ç­–ç•¥ï¼ŒåŒ…æ‹¬è½¦è¾†å’Œè½¦ç‰Œçº§åˆ«çš„é®æŒ¡æƒ…å†µï¼Œä»¥åŠå››ä¸ªå¯è¯»ç±»åˆ«ï¼ˆå®Œç¾ã€è‰¯å¥½ã€ä¸è‰¯å’Œä¸å¯è¯»ï¼‰ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§åŸºäºä¸‰ä¸ªå›¾åƒè¯†åˆ«ç½‘ç»œçš„åˆ†ç±»ä»»åŠ¡åŸºå‡†ï¼Œç”¨äºåˆ¤æ–­è½¦ç‰Œå›¾åƒæ˜¯å¦è¶³å¤Ÿå¥½ã€æ˜¯å¦éœ€è¦è¶…åˆ†è¾¨ç‡å¤„ç†æˆ–å®Œå…¨æ— æ³•æ¢å¤ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ‰€æœ‰åŸºå‡†æ¨¡å‹çš„F1å¾—åˆ†å‡ä½äº80%ï¼Œå¼ºè°ƒäº†ä»»åŠ¡çš„éš¾åº¦å’Œå¯¹è¿›ä¸€æ­¥ç ”ç©¶çš„éœ€è¦ã€‚æ•°æ®é›†å·²åœ¨GitHubä¸Šå…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ALPRåœ¨å¤„ç†ä¸æ¸…æ™°è½¦ç‰Œæ—¶é¢ä¸´æŒ‘æˆ˜ã€‚</li>
<li>é‡å»ºæ–¹æ³•å¦‚è¶…åˆ†è¾¨ç‡ï¼ˆSRï¼‰è™½å·²å‡ºç°ï¼Œä½†æ ¸å¿ƒé—®é¢˜ä»æœªè§£å†³ã€‚</li>
<li>é’ˆå¯¹éœ€è¦æé«˜å¯è¯»æ€§çš„æƒ…å†µï¼Œåº”é€‰æ‹©æ€§åº”ç”¨å›¾åƒé¢„å¤„ç†ã€‚</li>
<li>ä»‹ç»äº†åŒ…å«10,210å¼ è½¦è¾†å›¾åƒå’Œ12,687ä¸ªå¸¦æ³¨é‡Šè½¦ç‰Œçš„LPLCæ•°æ®é›†ï¼Œç”¨äºè½¦ç‰Œå¯è¯»æ€§åˆ†ç±»ã€‚</li>
<li>é‡‡ç”¨ç²¾ç»†æ ‡æ³¨ç­–ç•¥ï¼ŒåŒ…æ‹¬è½¦è¾†å’Œè½¦ç‰Œçº§åˆ«çš„é®æŒ¡ä¿¡æ¯åŠå››ä¸ªå¯è¯»ç±»åˆ«ã€‚</li>
<li>æå‡ºäº†ä¸€ç§åŸºäºä¸‰ä¸ªå›¾åƒè¯†åˆ«ç½‘ç»œçš„åˆ†ç±»ä»»åŠ¡åŸºå‡†ï¼Œç”¨äºåˆ¤æ–­è½¦ç‰Œå›¾åƒçš„çŠ¶æ€ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18425">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-681226dba81606929f100263c1cb9b5b" align="middle">
<img src="https://picx.zhimg.com/v2-ec0393cff386dcf618e24f575fdbdb70" align="middle">
<img src="https://picx.zhimg.com/v2-e3cd80ccde3a3bd9994403bfd265903c" align="middle">
<img src="https://picx.zhimg.com/v2-dd10bba95dcb5b9c05a0a964b994c505" align="middle">
<img src="https://picx.zhimg.com/v2-313d765c945168f850d7bcc3c6c85ec0" align="middle">
<img src="https://picx.zhimg.com/v2-6d2c1933ea7d45ed668e34d5561d3aa4" align="middle">
<img src="https://picx.zhimg.com/v2-77bb1d02297e59a637b0f28f71c6213e" align="middle">
<img src="https://picx.zhimg.com/v2-0122637f7fa2bbd64d5a07ee1c5b4bde" align="middle">
</details>


<h1 id="-10"><a href="#-10" class="headerlink" title=""></a></h1><h2 id="ChestGPT-Integrating-Large-Language-Models-and-Vision-Transformers-for-Disease-Detection-and-Localization-in-Chest-X-Rays"><a href="#ChestGPT-Integrating-Large-Language-Models-and-Vision-Transformers-for-Disease-Detection-and-Localization-in-Chest-X-Rays" class="headerlink" title="ChestGPT: Integrating Large Language Models and Vision Transformers for Disease Detection and Localization in Chest X-Rays"></a>ChestGPT: Integrating Large Language Models and Vision Transformers for Disease Detection and Localization in Chest X-Rays</h2><p><strong>Authors:Shehroz S. Khan, Petar Przulj, Ahmed Ashraf, Ali Abedi</strong></p>
<p>The global demand for radiologists is increasing rapidly due to a growing reliance on medical imaging services, while the supply of radiologists is not keeping pace. Advances in computer vision and image processing technologies present significant potential to address this gap by enhancing radiologistsâ€™ capabilities and improving diagnostic accuracy. Large language models (LLMs), particularly generative pre-trained transformers (GPTs), have become the primary approach for understanding and generating textual data. In parallel, vision transformers (ViTs) have proven effective at converting visual data into a format that LLMs can process efficiently. In this paper, we present ChestGPT, a deep-learning framework that integrates the EVA ViT with the Llama 2 LLM to classify diseases and localize regions of interest in chest X-ray images. The ViT converts X-ray images into tokens, which are then fed, together with engineered prompts, into the LLM, enabling joint classification and localization of diseases. This approach incorporates transfer learning techniques to enhance both explainability and performance. The proposed method achieved strong global disease classification performance on the VinDr-CXR dataset, with an F1 score of 0.76, and successfully localized pathologies by generating bounding boxes around the regions of interest. We also outline several task-specific prompts, in addition to general-purpose prompts, for scenarios radiologists might encounter. Overall, this framework offers an assistive tool that can lighten radiologistsâ€™ workload by providing preliminary findings and regions of interest to facilitate their diagnostic process.</p>
<blockquote>
<p>éšç€å¯¹åŒ»å­¦å½±åƒæœåŠ¡æ—¥ç›Šå¢é•¿çš„ä¾èµ–ï¼Œå¯¹æ”¾å°„ç§‘åŒ»å¸ˆçš„å…¨çƒéœ€æ±‚è¿…é€Ÿå¢åŠ ï¼Œç„¶è€Œæ”¾å°„ç§‘åŒ»å¸ˆçš„ä¾›ç»™å´è·Ÿä¸ä¸Šéœ€æ±‚ã€‚è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†æŠ€æœ¯çš„è¿›å±•åœ¨å¢å¼ºæ”¾å°„ç§‘åŒ»å¸ˆçš„èƒ½åŠ›å’Œæé«˜è¯Šæ–­å‡†ç¡®æ€§æ–¹é¢å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œä¸ºè§£å†³è¿™ä¸€å·®è·æä¾›äº†å¯èƒ½ã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å°¤å…¶æ˜¯é¢„è®­ç»ƒç”Ÿæˆå¼è½¬æ¢å™¨ï¼ˆGPTsï¼‰å·²æˆä¸ºç†è§£å’Œç”Ÿæˆæ–‡æœ¬æ•°æ®çš„ä¸»è¦æ–¹æ³•ã€‚ä¸æ­¤åŒæ—¶ï¼Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTsï¼‰åœ¨å°†è§†è§‰æ•°æ®è½¬æ¢ä¸ºLLMså¯ä»¥é«˜æ•ˆå¤„ç†çš„æ ¼å¼æ–¹é¢è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚æœ¬æ–‡ä»‹ç»äº†ChestGPTï¼Œè¿™æ˜¯ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒå°†EVA ViTä¸Llama 2 LLMç›¸ç»“åˆï¼Œç”¨äºå¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒä¸­çš„ç–¾ç—…è¿›è¡Œåˆ†ç±»å¹¶å®šä½æ„Ÿå…´è¶£åŒºåŸŸã€‚ViTå°†Xå°„çº¿å›¾åƒè½¬æ¢ä¸ºä»¤ç‰Œï¼Œç„¶åè¿åŒå·¥ç¨‹æç¤ºä¸€èµ·è¾“å…¥LLMï¼Œå®ç°äº†ç–¾ç—…çš„è”åˆåˆ†ç±»å’Œå®šä½ã€‚è¯¥æ–¹æ³•ç»“åˆäº†è¿ç§»å­¦ä¹ æŠ€æœ¯ï¼Œæé«˜äº†å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨VinDr-CXRæ•°æ®é›†ä¸Šå®ç°äº†å¼ºå¤§çš„å…¨çƒç–¾ç—…åˆ†ç±»æ€§èƒ½ï¼ŒF1åˆ†æ•°ä¸º0.76ï¼Œå¹¶é€šè¿‡å›´ç»•æ„Ÿå…´è¶£åŒºåŸŸç”Ÿæˆè¾¹ç•Œæ¡†æˆåŠŸåœ°å®šä½äº†ç—…ç†ã€‚æˆ‘ä»¬è¿˜æ¦‚è¿°äº†é™¤é€šç”¨æç¤ºå¤–ï¼Œé’ˆå¯¹æ”¾å°„ç§‘åŒ»ç”Ÿå¯èƒ½é‡åˆ°çš„å„ç§æƒ…æ™¯çš„ç‰¹å®šä»»åŠ¡æç¤ºã€‚æ€»çš„æ¥è¯´ï¼Œè¯¥æ¡†æ¶æä¾›äº†ä¸€ä¸ªè¾…åŠ©å·¥å…·ï¼Œå¯ä»¥é€šè¿‡æä¾›åˆæ­¥å‘ç°å’Œæ„Ÿå…´è¶£åŒºåŸŸæ¥å‡è½»æ”¾å°„ç§‘åŒ»ç”Ÿçš„å·¥ä½œé‡ï¼Œä»è€Œæœ‰åŠ©äºä»–ä»¬çš„è¯Šæ–­è¿‡ç¨‹ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.03739v2">PDF</a> 8 pages, 5 figures, 4 tables</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†å…¨çƒå¯¹æ”¾å°„ç§‘åŒ»ç”Ÿçš„éœ€æ±‚è¿…é€Ÿå¢é•¿ï¼Œè€Œæ”¾å°„ç§‘åŒ»ç”Ÿä¾›åº”ä¸è¶³çš„é—®é¢˜ã€‚è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†æŠ€æœ¯çš„è¿›æ­¥ä¸ºè§£å†³è¿™ä¸€å·®è·æä¾›äº†æ½œåŠ›ã€‚æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºChestGPTçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå®ƒç»“åˆäº†EVA Vision Transformerï¼ˆViTï¼‰å’ŒLlama 2å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”¨äºå¯¹èƒ¸éƒ¨Xå°„çº¿å›¾åƒè¿›è¡Œç–¾ç—…åˆ†ç±»å’Œæ„Ÿå…´è¶£åŒºåŸŸå®šä½ã€‚ViTå°†Xå°„çº¿å›¾åƒè½¬æ¢ä¸ºä»¤ç‰Œï¼Œç„¶åä¸å·¥ç¨‹æç¤ºä¸€èµ·è¾“å…¥LLMï¼Œå®ç°ç–¾ç—…çš„è”åˆåˆ†ç±»å’Œå®šä½ã€‚è¯¥æ–¹æ³•é‡‡ç”¨è¿ç§»å­¦ä¹ æŠ€æœ¯ï¼Œæé«˜äº†å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚åœ¨VinDr-CXRæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å–å¾—äº†è¾ƒå¼ºçš„å…¨çƒç–¾ç—…åˆ†ç±»æ€§èƒ½ï¼ŒF1åˆ†æ•°ä¸º0.76ï¼Œå¹¶èƒ½æˆåŠŸå®šä½ç—…ç†ï¼Œåœ¨æ„Ÿå…´è¶£åŒºåŸŸç”Ÿæˆè¾¹ç•Œæ¡†ã€‚è¯¥æ¡†æ¶ä¸ºæ”¾å°„ç§‘åŒ»ç”Ÿæä¾›äº†ä¸€ç§è¾…åŠ©å·¥å…·ï¼Œé€šè¿‡æä¾›åˆæ­¥å‘ç°å’Œæ„Ÿå…´è¶£åŒºåŸŸï¼Œå‡è½»ä»–ä»¬çš„å·¥ä½œé‡ï¼Œä¿ƒè¿›è¯Šæ–­è¿‡ç¨‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>åŒ»å­¦æˆåƒæœåŠ¡çš„éœ€æ±‚å¢é•¿å¯¼è‡´å¯¹æ”¾å°„ç§‘åŒ»ç”Ÿçš„éœ€æ±‚è¿…é€Ÿå¢åŠ ï¼Œè€Œæ”¾å°„ç§‘åŒ»ç”Ÿçš„ä¾›åº”ä¸è¶³ã€‚</li>
<li>è®¡ç®—æœºè§†è§‰å’Œå›¾åƒå¤„ç†æŠ€æœ¯çš„è¿›æ­¥æœ‰åŠ©äºè§£å†³è¿™ä¸€å·®è·ã€‚</li>
<li>ChestGPTæ˜¯ä¸€ä¸ªç»“åˆEVA Vision Transformerï¼ˆViTï¼‰å’ŒLlama 2å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚</li>
<li>ViTå°†Xå°„çº¿å›¾åƒè½¬æ¢ä¸ºä»¤ç‰Œï¼Œä¸å·¥ç¨‹æç¤ºä¸€èµ·è¾“å…¥LLMï¼Œå®ç°ç–¾ç—…åˆ†ç±»å’Œå®šä½ã€‚</li>
<li>è¯¥æ–¹æ³•é‡‡ç”¨è¿ç§»å­¦ä¹ æŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹çš„å¯è§£é‡Šæ€§å’Œæ€§èƒ½ã€‚</li>
<li>åœ¨VinDr-CXRæ•°æ®é›†ä¸Šï¼Œè¯¥æ–¹æ³•å–å¾—äº†è¾ƒå¼ºçš„ç–¾ç—…åˆ†ç±»æ€§èƒ½ï¼ŒF1åˆ†æ•°ä¸º0.76ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.03739">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8bd42baf4feba77fb3803578d117f2f9" align="middle">
<img src="https://picx.zhimg.com/v2-fc161cb87a1a0e57bce419412e566b9a" align="middle">
<img src="https://picx.zhimg.com/v2-e2977023fd16c38d1958d075f0334fa3" align="middle">
<img src="https://picx.zhimg.com/v2-41bac41a8a4d6837b235153d07ec0071" align="middle">
</details>


<h1 id="-11"><a href="#-11" class="headerlink" title=""></a></h1><h2 id="evMLP-An-Efficient-Event-Driven-MLP-Architecture-for-Vision"><a href="#evMLP-An-Efficient-Event-Driven-MLP-Architecture-for-Vision" class="headerlink" title="evMLP: An Efficient Event-Driven MLP Architecture for Vision"></a>evMLP: An Efficient Event-Driven MLP Architecture for Vision</h2><p><strong>Authors:Zhentan Zheng</strong></p>
<p>Deep neural networks have achieved remarkable results in computer vision tasks. In the early days, Convolutional Neural Networks (CNNs) were the mainstream architecture. In recent years, Vision Transformers (ViTs) have become increasingly popular. In addition, exploring applications of multi-layer perceptrons (MLPs) has provided new perspectives for research into vision model architectures. In this paper, we present evMLP accompanied by a simple event-driven local update mechanism. The proposed evMLP can independently process patches on images or feature maps via MLPs. We define changes between consecutive frames as &#96;&#96;eventsâ€™â€™. Under the event-driven local update mechanism, evMLP selectively processes patches where events occur. For sequential image data (e.g., video processing), this approach improves computational performance by avoiding redundant computations. Through ImageNet image classification experiments, evMLP attains accuracy competitive with state-of-the-art models. More significantly, experimental results on multiple video datasets demonstrate that evMLP reduces computational cost via its event-driven local update mechanism while maintaining output consistency with its non-event-driven baseline. The code and pre-trained models are available at <a target="_blank" rel="noopener" href="https://github.com/i-evi/evMLP">https://github.com/i-evi/evMLP</a>.</p>
<blockquote>
<p>æ·±åº¦ç¥ç»ç½‘ç»œåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚åœ¨æ—©æœŸï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯ä¸»æµæ¶æ„ã€‚è¿‘å¹´æ¥ï¼Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚æ­¤å¤–ï¼Œå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰çš„åº”ç”¨æ¢ç´¢ä¸ºè§†è§‰æ¨¡å‹æ¶æ„çš„ç ”ç©¶æä¾›äº†æ–°çš„è§†è§’ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å¸¦æœ‰ç®€å•äº‹ä»¶é©±åŠ¨å±€éƒ¨æ›´æ–°æœºåˆ¶çš„evMLPã€‚æ‰€æå‡ºçš„evMLPå¯ä»¥é€šè¿‡MLPç‹¬ç«‹å¤„ç†å›¾åƒæˆ–ç‰¹å¾å›¾ä¸Šçš„è¡¥ä¸ã€‚æˆ‘ä»¬å°†è¿ç»­å¸§ä¹‹é—´çš„å˜åŒ–å®šä¹‰ä¸ºâ€œäº‹ä»¶â€ã€‚åœ¨äº‹ä»¶é©±åŠ¨çš„å±€éƒ¨æ›´æ–°æœºåˆ¶ä¸‹ï¼ŒevMLPä¼šé€‰æ‹©æ€§åœ°å¤„ç†å‘ç”Ÿäº‹ä»¶çš„åœ°æ–¹ã€‚å¯¹äºåºåˆ—å›¾åƒæ•°æ®ï¼ˆä¾‹å¦‚è§†é¢‘å¤„ç†ï¼‰ï¼Œè¿™ç§æ–¹æ³•é€šè¿‡é¿å…å†—ä½™è®¡ç®—æ¥æé«˜è®¡ç®—æ€§èƒ½ã€‚é€šè¿‡ImageNetå›¾åƒåˆ†ç±»å®éªŒï¼ŒevMLPçš„å‡†ç¡®ç‡ä¸æœ€å…ˆè¿›çš„æ¨¡å‹å…·æœ‰ç«äº‰åŠ›ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå¤šä¸ªè§†é¢‘æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒevMLPé€šè¿‡å…¶äº‹ä»¶é©±åŠ¨çš„å±€éƒ¨æ›´æ–°æœºåˆ¶é™ä½äº†è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†ä¸éäº‹ä»¶é©±åŠ¨åŸºå‡†çº¿çš„è¾“å‡ºä¸€è‡´æ€§ã€‚ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/i-evi/evMLP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/i-evi/evMLPä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2507.01927v2">PDF</a> </p>
<p><strong>Summary</strong><br>     æ·±ç¥ç»ç½‘ç»œåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æˆæœã€‚è¿‘å¹´æ¥ï¼ŒVision Transformerï¼ˆViTï¼‰è¶Šæ¥è¶Šå—æ¬¢è¿ã€‚æœ¬æ–‡æå‡ºevMLPï¼Œå¹¶é…å¤‡äº†ç®€å•çš„äº‹ä»¶é©±åŠ¨å±€éƒ¨æ›´æ–°æœºåˆ¶ã€‚evMLPå¯ç‹¬ç«‹é€šè¿‡MLPå¤„ç†å›¾åƒæˆ–ç‰¹å¾å›¾çš„patchesã€‚äº‹ä»¶è¢«å®šä¹‰ä¸ºè¿ç»­å¸§ä¹‹é—´çš„å˜åŒ–ã€‚åœ¨äº‹ä»¶é©±åŠ¨å±€éƒ¨æ›´æ–°æœºåˆ¶ä¸‹ï¼ŒevMLPä¼šé€‰æ‹©æ€§åœ°å¤„ç†å‘ç”Ÿäº‹ä»¶çš„patchesã€‚å¯¹äºåºåˆ—å›¾åƒæ•°æ®ï¼ˆå¦‚è§†é¢‘å¤„ç†ï¼‰ï¼Œæ­¤æ–¹æ³•é€šè¿‡é¿å…å†—ä½™è®¡ç®—æé«˜äº†è®¡ç®—æ€§èƒ½ã€‚evMLPåœ¨ImageNetå›¾åƒåˆ†ç±»å®éªŒä¸­è¾¾åˆ°äº†ä¸æœ€æ–°æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨å¤šä¸ªè§†é¢‘æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒevMLPé€šè¿‡å…¶äº‹ä»¶é©±åŠ¨å±€éƒ¨æ›´æ–°æœºåˆ¶é™ä½äº†è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒäº†ä¸éäº‹ä»¶é©±åŠ¨çš„åŸºçº¿ä¸€è‡´çš„è¾“å‡ºã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æ·±ç¥ç»ç½‘ç»œåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°ä¼˜ç§€ï¼Œå…¶ä¸­CNNå’ŒVision Transformerï¼ˆViTï¼‰æ˜¯ä¸»æµæ¶æ„ã€‚</li>
<li>evMLPç»“åˆäº†å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰å’Œäº‹ä»¶é©±åŠ¨å±€éƒ¨æ›´æ–°æœºåˆ¶ï¼Œèƒ½ç‹¬ç«‹å¤„ç†å›¾åƒæˆ–ç‰¹å¾å›¾çš„patchesã€‚</li>
<li>äº‹ä»¶è¢«å®šä¹‰ä¸ºè¿ç»­å¸§ä¹‹é—´çš„å˜åŒ–ï¼ŒevMLPé€šè¿‡äº‹ä»¶é©±åŠ¨é€‰æ‹©æ€§åœ°å¤„ç†å‘ç”Ÿå˜åŒ–çš„patchesã€‚</li>
<li>å¯¹äºåºåˆ—å›¾åƒæ•°æ®ï¼ˆå¦‚è§†é¢‘å¤„ç†ï¼‰ï¼ŒevMLPæé«˜äº†è®¡ç®—æ€§èƒ½ï¼Œé€šè¿‡é¿å…å†—ä½™è®¡ç®—é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
<li>evMLPåœ¨ImageNetå›¾åƒåˆ†ç±»å®éªŒä¸­çš„æ€§èƒ½ä¸æœ€æ–°æ¨¡å‹ç›¸å½“ã€‚</li>
<li>åœ¨å¤šä¸ªè§†é¢‘æ•°æ®é›†ä¸Šï¼ŒevMLPé€šè¿‡äº‹ä»¶é©±åŠ¨å±€éƒ¨æ›´æ–°æœºåˆ¶åœ¨ä¿æŒè¾“å‡ºä¸€è‡´æ€§çš„åŒæ—¶é™ä½äº†è®¡ç®—æˆæœ¬ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.01927">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-15f6f5f936cbf75baa3afe3f29229692" align="middle">
<img src="https://picx.zhimg.com/v2-66ae1b77b9b5e357bb3bb7928570d30c" align="middle">
<img src="https://picx.zhimg.com/v2-9a37afc2def7cb361ec0b189004e7d61" align="middle">
<img src="https://picx.zhimg.com/v2-1a4d48dc4a03b179f62f0ffe80232d68" align="middle">
<img src="https://picx.zhimg.com/v2-286682c61b534ccdbf1de3585460297f" align="middle">
<img src="https://picx.zhimg.com/v2-1172b7177e8220042f3ed4c95baf833c" align="middle">
<img src="https://picx.zhimg.com/v2-c44287cb991af1b782087bfa9b16fcf4" align="middle">
<img src="https://picx.zhimg.com/v2-d17ec6da7b8bb2588a9eee12955e6d77" align="middle">
<img src="https://picx.zhimg.com/v2-1f7dc4cb75bdf55630b9092c01e3b345" align="middle">
</details>


<h1 id="-12"><a href="#-12" class="headerlink" title=""></a></h1><h2 id="FaSDiff-Balancing-Perception-and-Semantics-in-Face-Compression-via-Stable-Diffusion-Priors"><a href="#FaSDiff-Balancing-Perception-and-Semantics-in-Face-Compression-via-Stable-Diffusion-Priors" class="headerlink" title="FaSDiff: Balancing Perception and Semantics in Face Compression via Stable Diffusion Priors"></a>FaSDiff: Balancing Perception and Semantics in Face Compression via Stable Diffusion Priors</h2><p><strong>Authors:Yimin Zhou, Yichong Xia, Bin Chen, Mingyao Hong, Jiawei Li, Zhi Wang, Yaowei Wang</strong></p>
<p>With the increasing deployment of facial image data across a wide range of applications, efficient compression tailored to facial semantics has become critical for both storage and transmission. While recent learning-based face image compression methods have achieved promising results, they often suffer from degraded reconstruction quality at low bit rates. Directly applying diffusion-based generative priors to this task leads to suboptimal performance in downstream machine vision tasks, primarily due to poor preservation of high-frequency details. In this work, we propose FaSDiff (\textbf{Fa}cial Image Compression with a \textbf{S}table \textbf{Diff}usion Prior), a novel diffusion-driven compression framework designed to enhance both visual fidelity and semantic consistency. FaSDiff incorporates a high-frequency-sensitive compressor to capture fine-grained details and generate robust visual prompts for guiding the diffusion model. To address low-frequency degradation, we further introduce a hybrid low-frequency enhancement module that disentangles and preserves semantic structures, enabling stable modulation of the diffusion prior during reconstruction. By jointly optimizing perceptual quality and semantic preservation, FaSDiff effectively balances human visual fidelity and machine vision accuracy. Extensive experiments demonstrate that FaSDiff outperforms state-of-the-art methods in both perceptual metrics and downstream task performance.</p>
<blockquote>
<p>éšç€é¢éƒ¨å›¾åƒæ•°æ®åœ¨ä¼—å¤šåº”ç”¨é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œé’ˆå¯¹é¢éƒ¨è¯­ä¹‰çš„é«˜æ•ˆå‹ç¼©å¯¹äºå­˜å‚¨å’Œä¼ è¾“éƒ½è‡³å…³é‡è¦ã€‚è™½ç„¶åŸºäºå­¦ä¹ çš„é¢éƒ¨å›¾åƒå‹ç¼©æ–¹æ³•å·²ç»å–å¾—äº†æœ‰å‰æ™¯çš„ç»“æœï¼Œä½†åœ¨ä½ç ç‡ä¸‹ï¼Œå®ƒä»¬å¾€å¾€é¢ä¸´é‡å»ºè´¨é‡ä¸‹é™çš„é—®é¢˜ã€‚ç›´æ¥å°†åŸºäºæ‰©æ•£çš„ç”Ÿæˆå…ˆéªŒåº”ç”¨äºæ­¤ä»»åŠ¡ä¼šå¯¼è‡´åœ¨ä¸‹æ¸¸è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸ä½³ï¼Œä¸»è¦æ˜¯å› ä¸ºé«˜é¢‘ç»†èŠ‚çš„ä¿ç•™ä¸ä½³ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†FaSDiffï¼ˆå…·æœ‰ç¨³å®šæ‰©æ•£å…ˆéªŒçš„é¢éƒ¨å›¾åƒå‹ç¼©ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„æ‰©æ•£é©±åŠ¨å‹ç¼©æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜è§†è§‰ä¿çœŸåº¦å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚FaSDiffç»“åˆäº†ä¸€ä¸ªé«˜é¢‘æ•æ„Ÿå‹ç¼©æœºï¼Œç”¨äºæ•æ‰ç²¾ç»†ç»†èŠ‚å¹¶ä¸ºæ‰©æ•£æ¨¡å‹ç”Ÿæˆç¨³å¥çš„è§†è§‰æç¤ºã€‚ä¸ºäº†è§£å†³ä½é¢‘é€€åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å¼•å…¥äº†ä¸€ä¸ªæ··åˆä½é¢‘å¢å¼ºæ¨¡å—ï¼Œè¯¥æ¨¡å—å¯ä»¥åˆ†ç¦»å¹¶ä¿ç•™è¯­ä¹‰ç»“æ„ï¼Œä»è€Œåœ¨é‡å»ºè¿‡ç¨‹ä¸­å®ç°æ‰©æ•£å…ˆéªŒçš„ç¨³å®šè°ƒåˆ¶ã€‚é€šè¿‡è”åˆä¼˜åŒ–æ„ŸçŸ¥è´¨é‡å’Œè¯­ä¹‰ä¿ç•™ï¼ŒFaSDiffæœ‰æ•ˆåœ°å¹³è¡¡äº†äººç±»è§†è§‰ä¿çœŸå’Œè®¡ç®—æœºè§†è§‰å‡†ç¡®æ€§ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼ŒFaSDiffåœ¨æ„ŸçŸ¥æŒ‡æ ‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ–¹é¢éƒ½ä¼˜äºæœ€æ–°æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2505.05870v2">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>é«˜æ•ˆå‹ç¼©é¢éƒ¨å›¾åƒæ•°æ®å¯¹å­˜å‚¨å’Œä¼ è¾“è‡³å…³é‡è¦ã€‚å°½ç®¡åŸºäºå­¦ä¹ çš„é¢éƒ¨å›¾åƒå‹ç¼©æ–¹æ³•å–å¾—äº†ä¸€å®šæˆæœï¼Œä½†åœ¨ä½æ¯”ç‰¹ç‡ä¸‹é‡å»ºè´¨é‡å¾€å¾€ä¸‹é™ã€‚ç›´æ¥å°†æ‰©æ•£å…ˆéªŒåº”ç”¨äºè¯¥ä»»åŠ¡å¯¼è‡´ä¸‹æ¸¸æœºå™¨è§†è§‰ä»»åŠ¡æ€§èƒ½ä¸ä½³ï¼Œä¸»è¦å› ä¸ºé«˜é¢‘ç»†èŠ‚ä¿å­˜ä¸ä½³ã€‚æœ¬æ–‡æå‡ºFaSDiffï¼ˆä¸€ç§å¸¦æœ‰ç¨³å®šæ‰©æ•£å…ˆéªŒçš„é¢éƒ¨å›¾åƒå‹ç¼©æ–¹æ³•ï¼‰ï¼Œæ—¨åœ¨æé«˜è§†è§‰ä¿çœŸåº¦å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚FaSDiffç»“åˆé«˜é¢‘æ•æ„Ÿå‹ç¼©å™¨æ•æ‰ç²¾ç»†ç»†èŠ‚ï¼Œä¸ºæ‰©æ•£æ¨¡å‹ç”Ÿæˆç¨³å¥çš„è§†è§‰æç¤ºã€‚ä¸ºè§£å†³ä½é¢‘é€€åŒ–é—®é¢˜ï¼Œå¼•å…¥æ··åˆä½é¢‘å¢å¼ºæ¨¡å—ï¼Œè§£å¼€å¹¶ä¿ç•™è¯­ä¹‰ç»“æ„ï¼Œåœ¨é‡å»ºè¿‡ç¨‹ä¸­å®ç°ç¨³å®šçš„æ‰©æ•£å…ˆéªŒè°ƒåˆ¶ã€‚é€šè¿‡ä¼˜åŒ–æ„ŸçŸ¥è´¨é‡å’Œè¯­ä¹‰ä¿ç•™ï¼ŒFaSDiffæœ‰æ•ˆå¹³è¡¡äº†äººç±»è§†è§‰ä¿çœŸå’Œæœºå™¨è§†è§‰å‡†ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒFaSDiffåœ¨æ„ŸçŸ¥æŒ‡æ ‡å’Œä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ–¹é¢å‡ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>é«˜æ•ˆå‹ç¼©é¢éƒ¨å›¾åƒæ•°æ®å¯¹å®é™…åº”ç”¨è‡³å…³é‡è¦ã€‚</li>
<li>åŸºäºå­¦ä¹ çš„é¢éƒ¨å›¾åƒå‹ç¼©æ–¹æ³•åœ¨ä½æ¯”ç‰¹ç‡ä¸‹é‡å»ºè´¨é‡ä¸‹é™ã€‚</li>
<li>ç›´æ¥åº”ç”¨æ‰©æ•£å…ˆéªŒäºé¢éƒ¨å›¾åƒå‹ç¼©ä¼šå½±å“ä¸‹æ¸¸æœºå™¨è§†è§‰ä»»åŠ¡æ€§èƒ½ã€‚</li>
<li>FaSDiffç»“åˆäº†é«˜é¢‘æ•æ„Ÿå‹ç¼©å™¨ä»¥æ•æ‰ç²¾ç»†ç»†èŠ‚å¹¶ç”Ÿæˆè§†è§‰æç¤ºã€‚</li>
<li>ä¸ºè§£å†³ä½é¢‘é€€åŒ–é—®é¢˜ï¼Œå¼•å…¥äº†æ··åˆä½é¢‘å¢å¼ºæ¨¡å—æ¥ä¿ç•™è¯­ä¹‰ç»“æ„ã€‚</li>
<li>FaSDiffåœ¨ä¼˜åŒ–æ„ŸçŸ¥è´¨é‡å’Œè¯­ä¹‰ä¿ç•™ä¹‹é—´è¾¾åˆ°äº†å¹³è¡¡ï¼Œæé«˜äº†è§†è§‰ä¿çœŸåº¦å’Œè¯­ä¹‰ä¸€è‡´æ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2505.05870">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-bbef469546b86ff689834ed5636e5a1d" align="middle">
<img src="https://picx.zhimg.com/v2-291592df5057ad82bae0bd41c8d651e4" align="middle">
<img src="https://picx.zhimg.com/v2-5d43e8af8230241b860a8dca3e4b63d5" align="middle">
<img src="https://picx.zhimg.com/v2-c56eaef7f8ea845a77f30e0137d04b73" align="middle">
<img src="https://picx.zhimg.com/v2-bac603af490f203f7b17d8aa1c95a010" align="middle">
</details>


<h1 id="-13"><a href="#-13" class="headerlink" title=""></a></h1><h2 id="ForAug-Recombining-Foregrounds-and-Backgrounds-to-Improve-Vision-Transformer-Training-with-Bias-Mitigation"><a href="#ForAug-Recombining-Foregrounds-and-Backgrounds-to-Improve-Vision-Transformer-Training-with-Bias-Mitigation" class="headerlink" title="ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation"></a>ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation</h2><p><strong>Authors:Tobias Christian Nauen, Brian Moser, Federico Raue, Stanislav Frolov, Andreas Dengel</strong></p>
<p>Transformers, particularly Vision Transformers (ViTs), have achieved state-of-the-art performance in large-scale image classification. However, they often require large amounts of data and can exhibit biases that limit their robustness and generalizability. This paper introduces ForAug, a novel data augmentation scheme that addresses these challenges and explicitly includes inductive biases, which commonly are part of the neural network architecture, into the training data. ForAug is constructed by using pretrained foundation models to separate and recombine foreground objects with different backgrounds, enabling fine-grained control over image composition during training. It thus increases the data diversity and effective number of training samples. We demonstrate that training on ForNet, the application of ForAug to ImageNet, significantly improves the accuracy of ViTs and other architectures by up to 4.5 percentage points (p.p.) on ImageNet and 7.3 p.p. on downstream tasks. Importantly, ForAug enables novel ways of analyzing model behavior and quantifying biases. Namely, we introduce metrics for background robustness, foreground focus, center bias, and size bias and show that training on ForNet substantially reduces these biases compared to training on ImageNet. In summary, ForAug provides a valuable tool for analyzing and mitigating biases, enabling the development of more robust and reliable computer vision models. Our code and dataset are publicly available at <a target="_blank" rel="noopener" href="https://github.com/tobna/ForAug">https://github.com/tobna/ForAug</a>.</p>
<blockquote>
<p>Transformerï¼Œç‰¹åˆ«æ˜¯Vision Transformerï¼ˆViTï¼‰ï¼Œå·²ç»åœ¨å¤§è§„æ¨¡å›¾åƒåˆ†ç±»ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®ï¼Œå¹¶å¯èƒ½è¡¨ç°å‡ºé™åˆ¶å…¶ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›çš„åè§ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ¡ˆForAugï¼Œè¯¥æ–¹æ¡ˆè§£å†³äº†è¿™äº›æŒ‘æˆ˜ï¼Œå¹¶å°†é€šå¸¸ä½œä¸ºç¥ç»ç½‘ç»œæ¶æ„ä¸€éƒ¨åˆ†çš„å½’çº³åè§æ˜¾å¼åœ°çº³å…¥è®­ç»ƒæ•°æ®ä¸­ã€‚ForAugé€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹æ¥åˆ†ç¦»å’Œé‡ç»„å‰æ™¯ç‰©ä½“ä¸ä¸åŒçš„èƒŒæ™¯ï¼Œå®ç°å¯¹è®­ç»ƒè¿‡ç¨‹ä¸­å›¾åƒç»„æˆçš„ç²¾ç»†æ§åˆ¶ã€‚å› æ­¤ï¼Œå®ƒå¢åŠ äº†æ•°æ®å¤šæ ·æ€§å’Œæœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬æ•°é‡ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨ForNetä¸Šåº”ç”¨ForAugå¯¹ImageNetè¿›è¡Œè®­ç»ƒï¼Œæ˜¾è‘—æé«˜äº†ViTå’Œå…¶ä»–æ¶æ„çš„å‡†ç¡®æ€§ï¼Œåœ¨ImageNetä¸Šæé«˜äº†é«˜è¾¾4.5ä¸ªç™¾åˆ†ç‚¹ï¼ˆp.p.ï¼‰ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæé«˜äº†7.3ä¸ªç™¾åˆ†ç‚¹ã€‚é‡è¦çš„æ˜¯ï¼ŒForAugæä¾›äº†æ–°çš„åˆ†ææ¨¡å‹è¡Œä¸ºå’Œé‡åŒ–åè§çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†èƒŒæ™¯ç¨³å¥æ€§ã€å‰æ™¯ç„¦ç‚¹ã€ä¸­å¿ƒåè§å’Œå¤§å°åè§çš„æŒ‡æ ‡ï¼Œå¹¶è¡¨æ˜ä¸åœ¨ImageNetä¸Šè¿›è¡Œè®­ç»ƒç›¸æ¯”ï¼Œåœ¨ForNetä¸Šè¿›è¡Œè®­ç»ƒå¯ä»¥å¤§å¤§å‡å°‘è¿™äº›åè§ã€‚æ€»ä¹‹ï¼ŒForAugä¸ºåˆ†æå’Œç¼“è§£åè§æä¾›äº†æœ‰ä»·å€¼çš„å·¥å…·ï¼Œä½¿å¼€å‘æ›´ç¨³å¥ã€æ›´å¯é çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®é›†å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/tobna/ForAug%E5%85%AC%E5%BC%80%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/tobna/ForAugå…¬å¼€è®¿é—®ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2503.09399v2">PDF</a> v2: added DeiT, added ablation vs simple copy-paste</p>
<p><strong>Summary</strong><br>åŸºäºVision Transformeråœ¨å›¾åƒåˆ†ç±»é¢†åŸŸçš„å‡ºè‰²è¡¨ç°ï¼Œè¯¥æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ¡ˆForAugã€‚å®ƒé€šè¿‡å¼•å…¥å½’çº³åè§å¹¶å°†å…¶çº³å…¥è®­ç»ƒæ•°æ®ï¼Œè§£å†³äº†æ¨¡å‹éœ€è¦å¤§é‡çš„æ•°æ®å’Œå¯èƒ½å‡ºç°çš„åè§é—®é¢˜ã€‚ForAugåˆ©ç”¨é¢„è®­ç»ƒçš„åŸºå‡†æ¨¡å‹åˆ†ç¦»å¹¶é‡æ–°ç»„åˆå‰æ™¯ç‰©ä½“ä¸ä¸åŒçš„èƒŒæ™¯ï¼Œå®ç°å¯¹è®­ç»ƒå›¾åƒç»„æˆçš„ç²¾ç»†æ§åˆ¶ï¼Œæé«˜äº†æ•°æ®å¤šæ ·æ€§å’Œæœ‰æ•ˆè®­ç»ƒæ ·æœ¬æ•°é‡ã€‚åœ¨ImageNetåŠä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ForAugè®­ç»ƒçš„ViTså’Œå…¶ä»–æ¶æ„çš„ç²¾åº¦æ˜¾è‘—æé«˜ã€‚æ­¤å¤–ï¼ŒForAugè¿˜æä¾›äº†åˆ†æå’Œç¼“è§£æ¨¡å‹åè§çš„æ–°æ–¹æ³•ï¼Œæœ‰åŠ©äºå¼€å‘æ›´ç¨³å¥å’Œå¯é çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>Vision Transformers (ViTs) åœ¨å¤§è§„æ¨¡å›¾åƒåˆ†ç±»ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚</li>
<li>ForAugæ˜¯ä¸€ç§æ–°çš„æ•°æ®å¢å¼ºæ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³ViTséœ€è¦å¤§é‡æ•°æ®å’Œå­˜åœ¨åè§çš„é—®é¢˜ã€‚</li>
<li>ForAugé€šè¿‡å°†é¢„è®­ç»ƒçš„åŸºå‡†æ¨¡å‹ç”¨äºåˆ†ç¦»å’Œé‡æ–°ç»„åˆå‰æ™¯ç‰©ä½“ä¸èƒŒæ™¯ï¼Œæé«˜äº†æ•°æ®å¤šæ ·æ€§å’Œæœ‰æ•ˆè®­ç»ƒæ ·æœ¬æ•°é‡ã€‚</li>
<li>ForAugæ˜¾è‘—æé«˜äº†ViTså’Œå…¶ä»–æ¶æ„åœ¨ImageNetåŠä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„ç²¾åº¦ã€‚</li>
<li>ForAugæä¾›äº†åˆ†æå’Œç¼“è§£æ¨¡å‹åè§çš„æ–°æ–¹æ³•ï¼ŒåŒ…æ‹¬èƒŒæ™¯ç¨³å¥æ€§ã€å‰æ™¯ç„¦ç‚¹ã€ä¸­å¿ƒåè§å’Œå¤§å°åè§ç­‰æŒ‡æ ‡ã€‚</li>
<li>ä½¿ç”¨ForAugè®­ç»ƒçš„æ¨¡å‹ç›¸æ¯”åœ¨ImageNetä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œæ˜¾è‘—å‡å°‘äº†åè§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2503.09399">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-e101fb739ecc81a4c96fdb1a98ba8200" align="middle">
<img src="https://picx.zhimg.com/v2-95fc9f3e136c40e5ed943fb1de1273b6" align="middle">
<img src="https://picx.zhimg.com/v2-897f750285f2e115ca5b62d756ba5bf8" align="middle">
<img src="https://picx.zhimg.com/v2-ec48090a0b7b1c015b1222121ffd0036" align="middle">
<img src="https://picx.zhimg.com/v2-88b65cd302ddfe8c0dc30a9cd3db69f9" align="middle">
</details>


<h1 id="-14"><a href="#-14" class="headerlink" title=""></a></h1><h2 id="DeNAS-ViT-Data-Efficient-NAS-Optimized-Vision-Transformer-for-Ultrasound-Image-Segmentation"><a href="#DeNAS-ViT-Data-Efficient-NAS-Optimized-Vision-Transformer-for-Ultrasound-Image-Segmentation" class="headerlink" title="DeNAS-ViT: Data Efficient NAS-Optimized Vision Transformer for Ultrasound Image Segmentation"></a>DeNAS-ViT: Data Efficient NAS-Optimized Vision Transformer for Ultrasound Image Segmentation</h2><p><strong>Authors:Renqi Chen, Xinzhe Zheng, Haoyang Su, Kehan Wu</strong></p>
<p>Accurate segmentation of ultrasound images is essential for reliable medical diagnoses but is challenged by poor image quality and scarce labeled data. Prior approaches have relied on manually designed, complex network architectures to improve multi-scale feature extraction. However, such handcrafted models offer limited gains when prior knowledge is inadequate and are prone to overfitting on small datasets. In this paper, we introduce DeNAS-ViT, a data-efficient NAS-optimized Vision Transformer, the first method to leverage neural architecture search (NAS) for ultrasound image segmentation by automatically optimizing model architecture through token-level search. Specifically, we propose an efficient NAS module that performs multi-scale token search prior to the ViTâ€™s attention mechanism, effectively capturing both contextual and local features while minimizing computational costs. Given ultrasoundâ€™s data scarcity and NASâ€™s inherent data demands, we further develop a NAS-guided semi-supervised learning (SSL) framework. This approach integrates network independence and contrastive learning within a stage-wise optimization strategy, significantly enhancing model robustness under limited-data conditions. Extensive experiments on public datasets demonstrate that DeNAS-ViT achieves state-of-the-art performance, maintaining robustness with minimal labeled data. Moreover, we highlight DeNAS-ViTâ€™s generalization potential beyond ultrasound imaging, underscoring its broader applicability.</p>
<blockquote>
<p>è¶…å£°å›¾åƒçš„ç²¾ç¡®åˆ†å‰²å¯¹äºå¯é çš„åŒ»å­¦è¯Šæ–­è‡³å…³é‡è¦ï¼Œä½†é¢ä¸´ç€å›¾åƒè´¨é‡å·®å’Œæ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„æŒ‘æˆ˜ã€‚ä¹‹å‰çš„æ–¹æ³•ä¾èµ–äºæ‰‹åŠ¨è®¾è®¡å¤æ‚çš„ç½‘ç»œæ¶æ„æ¥æ”¹å–„å¤šå°ºåº¦ç‰¹å¾æå–ã€‚ç„¶è€Œï¼Œå½“å…ˆéªŒçŸ¥è¯†ä¸è¶³æ—¶ï¼Œæ­¤ç±»æ‰‹å·¥æ¨¡å‹æä¾›çš„æ”¶ç›Šæœ‰é™ï¼Œå¹¶ä¸”å®¹æ˜“åœ¨å°æ•°æ®é›†ä¸Šè¿‡åº¦æ‹Ÿåˆã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†DeNAS-ViTï¼Œè¿™æ˜¯ä¸€ç§æ•°æ®é«˜æ•ˆçš„NASä¼˜åŒ–è§†è§‰è½¬æ¢å™¨ï¼Œæ˜¯ç¬¬ä¸€ç§åˆ©ç”¨ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰è¿›è¡Œè¶…å£°å›¾åƒåˆ†å‰²çš„æ–¹æ³•ï¼Œé€šè¿‡tokençº§åˆ«çš„æœç´¢è‡ªåŠ¨ä¼˜åŒ–æ¨¡å‹æ¶æ„ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„NASæ¨¡å—ï¼Œåœ¨ViTçš„æ³¨æ„åŠ›æœºåˆ¶ä¹‹å‰è¿›è¡Œå¤šå°ºåº¦tokenæœç´¢ï¼Œæœ‰æ•ˆåœ°æ•æ‰ä¸Šä¸‹æ–‡å’Œå±€éƒ¨ç‰¹å¾ï¼ŒåŒæ—¶é™ä½è®¡ç®—æˆæœ¬ã€‚è€ƒè™‘åˆ°è¶…å£°æ•°æ®ç¨€ç¼ºå’ŒNASå›ºæœ‰çš„æ•°æ®éœ€æ±‚ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªNASå¼•å¯¼çš„åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¡†æ¶ã€‚è¯¥æ–¹æ³•å°†ç½‘ç»œç‹¬ç«‹æ€§å’Œå¯¹æ¯”å­¦ä¹ çº³å…¥åˆ†é˜¶æ®µä¼˜åŒ–ç­–ç•¥ä¸­ï¼Œæ˜¾è‘—æé«˜äº†åœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹çš„æ¨¡å‹ç¨³å¥æ€§ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼ŒDeNAS-ViTè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œç”¨æœ€å°çš„æ ‡æ³¨æ•°æ®ç»´æŒäº†ç¨³å¥æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çªå‡ºäº†DeNAS-ViTåœ¨è¶…å£°æˆåƒä¹‹å¤–çš„é€šç”¨åŒ–æ½œåŠ›ï¼Œå¼ºè°ƒäº†å…¶æ›´å¹¿æ³›çš„åº”ç”¨æ€§ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2407.04203v3">PDF</a> Accepted by AAAI-26 Main Technical Track</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è¶…å£°å›¾åƒåˆ†å‰²çš„DeNAS-ViTæ–¹æ³•ã€‚è¯¥æ–¹æ³•ç»“åˆäº†ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œé€šè¿‡tokençº§åˆ«çš„æœç´¢è‡ªåŠ¨ä¼˜åŒ–æ¨¡å‹æ¶æ„ã€‚ä¸ºæé«˜æ•°æ®æ•ˆç‡ï¼Œå¼€å‘äº†ä¸€ä¸ªé«˜æ•ˆçš„NASæ¨¡å—ï¼Œå®ç°äº†å¤šå°ºåº¦tokenæœç´¢ä¸ViTæ³¨æ„åŠ›æœºåˆ¶çš„ç»“åˆã€‚æ­¤å¤–ï¼Œå¼€å‘äº†ä¸€ç§NASå¼•å¯¼çš„åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¡†æ¶ï¼Œä»¥åœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹æé«˜æ¨¡å‹çš„ç¨³å¥æ€§ã€‚åœ¨å…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒDeNAS-ViTå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶åœ¨å°‘é‡æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ä¿æŒç¨³å¥æ€§ã€‚æ­¤å¤–ï¼ŒDeNAS-ViTè¿˜å…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>DeNAS-ViTç»“åˆäº†ç¥ç»ç½‘ç»œæ¶æ„æœç´¢ï¼ˆNASï¼‰å’Œè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰ï¼Œä¸ºè¶…å£°å›¾åƒåˆ†å‰²å¸¦æ¥äº†åˆ›æ–°æ–¹æ³•ã€‚</li>
<li>é€šè¿‡tokençº§åˆ«çš„æœç´¢è‡ªåŠ¨ä¼˜åŒ–æ¨¡å‹æ¶æ„ï¼Œæé«˜äº†è¶…å£°å›¾åƒåˆ†å‰²çš„å‡†ç¡®æ€§ã€‚</li>
<li>é«˜æ•ˆçš„NASæ¨¡å—å®ç°äº†å¤šå°ºåº¦tokenæœç´¢ä¸ViTæ³¨æ„åŠ›æœºåˆ¶çš„ç»“åˆï¼Œæå‡äº†ç‰¹å¾æå–çš„æ•ˆæœã€‚</li>
<li>å¼€å‘NASå¼•å¯¼çš„åŠç›‘ç£å­¦ä¹ ï¼ˆSSLï¼‰æ¡†æ¶ï¼Œæé«˜äº†åœ¨æœ‰é™æ•°æ®æ¡ä»¶ä¸‹çš„æ¨¡å‹ç¨³å¥æ€§ã€‚</li>
<li>åœ¨å…¬å…±æ•°æ®é›†ä¸Šï¼ŒDeNAS-ViTå®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†åœ¨å°‘é‡æ ‡æ³¨æ•°æ®ä¸‹çš„ç¨³å¥æ€§ã€‚</li>
<li>DeNAS-ViTå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œä¸ä»…é€‚ç”¨äºè¶…å£°å›¾åƒåˆ†å‰²ï¼Œè¿˜å¯èƒ½åº”ç”¨äºå…¶ä»–é¢†åŸŸã€‚</li>
<li>è¯¥æ–¹æ³•å¯¹äºè§£å†³è¶…å£°å›¾åƒè´¨é‡å·®å’Œæ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„é—®é¢˜å…·æœ‰ç§¯ææ„ä¹‰ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2407.04203">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-a385747ffe60c5662367d1074535c279" align="middle">
<img src="https://picx.zhimg.com/v2-0eaefbf5a0c0ef9c138d4d9983e3b254" align="middle">
<img src="https://picx.zhimg.com/v2-4e9e4ac81a4286d05c119a15f7822e5f" align="middle">
<img src="https://picx.zhimg.com/v2-fc28908dff133e7acaf3493a9b1b9d20" align="middle">
<img src="https://picx.zhimg.com/v2-8045a0f76a453fead38335b44034cea5" align="middle">
</details>


<h1 id="-15"><a href="#-15" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/Vision%20Transformer/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/Vision%20Transformer/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/Vision-Transformer/">
                                    <span class="chip bg-color">Vision Transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/%E6%A3%80%E6%B5%8B_%E5%88%86%E5%89%B2_%E8%B7%9F%E8%B8%AA/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-cc750a82c323d20b480426c96b744d22" class="responsive-img" alt="æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª">
                        
                        <span class="card-title">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  DGFusion Dual-guided Fusion for Robust Multi-Modal 3D Object Detection
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/" class="post-category">
                                    æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E6%A3%80%E6%B5%8B-%E5%88%86%E5%89%B2-%E8%B7%9F%E8%B8%AA/">
                        <span class="chip bg-color">æ£€æµ‹/åˆ†å‰²/è·Ÿè¸ª</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-0a848a00a420f03031281421c1fd90d6" class="responsive-img" alt="è§†é¢‘ç†è§£">
                        
                        <span class="card-title">è§†é¢‘ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            è§†é¢‘ç†è§£ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  MVU-Eval Towards Multi-Video Understanding Evaluation for Multimodal LLMs
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" class="post-category">
                                    è§†é¢‘ç†è§£
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/">
                        <span class="chip bg-color">è§†é¢‘ç†è§£</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32714.1k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
