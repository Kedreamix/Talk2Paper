<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ ">
    <meta name="description" content="æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  NeuroCLIP Brain-Inspired Prompt Tuning for EEG-to-Image Multimodal Contrastive Learning">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹  | Talk2Paper</title>
    <link rel="icon" type="image/png" href="/Talk2Paper/favicon.png">
    
    <style>
        body{
            background-image: url(/Talk2Paper/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/matery.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/my.css">
<link rel="stylesheet" type="text/css" href="/Talk2Paper/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/Talk2Paper/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/Talk2Paper/css/post.css">




    



    <script src="/Talk2Paper/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


    <style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: 
#FFF;
        text-align: center;
        /* loaderé¡µé¢æ¶ˆå¤±é‡‡ç”¨æ¸éšçš„æ–¹å¼*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid 
#49b1f5;
        border-right-color: 
transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: 
#49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: 
#2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px 
rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logoå‡ºç°åŠ¨ç”» */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ä½¿ç”¨æ¸éšçš„æ–¹æ³•æ·¡å‡ºloading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },2500); 
        },1000);//å¼ºåˆ¶æ˜¾ç¤ºloading page 1s  
    };
    loaded();
})()
</script>

 

<body>
    
        <div id="loading-container">
             <p class="loading-text">å˜˜~  æ­£åœ¨ä»æœåŠ¡å™¨å·å–é¡µé¢ . . . </p> 
             <div class="loading-image">
                 <div></div>
                 <div></div>
                 <div></div>
                 <div></div> 
                 <div></div>
             </div>
        </div>
    
    
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/Talk2Paper/" class="waves-effect waves-light">
                    
                    <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Talk2Paper</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>é¦–é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/Talk2Paper/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>å½’æ¡£</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="æ·±è‰²/æµ…è‰²æ¨¡å¼" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/Talk2Paper/medias/talk2paper2.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Talk2Paper</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			é¦–é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/Talk2Paper/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			å½’æ¡£
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Kedreamix/Awesome-Talking-Head-Synthesis" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('https://picx.zhimg.com/v2-4691d4464cbd4344d7ac32906e511f69')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/Talk2Paper/categories/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" class="post-category">
                                æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ 
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-16
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-11-18
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    9.1k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    37 åˆ†
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<p>âš ï¸ ä»¥ä¸‹æ‰€æœ‰å†…å®¹æ€»ç»“éƒ½æ¥è‡ªäº å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œå¦‚æœ‰é”™è¯¯ï¼Œä»…ä¾›å‚è€ƒï¼Œè°¨æ…ä½¿ç”¨<br>ğŸ”´ è¯·æ³¨æ„ï¼šåƒä¸‡ä¸è¦ç”¨äºä¸¥è‚ƒçš„å­¦æœ¯åœºæ™¯ï¼Œåªèƒ½ç”¨äºè®ºæ–‡é˜…è¯»å‰çš„åˆç­›ï¼<br>ğŸ’— å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ© <a target="_blank" rel="noopener" href="https://github.com/Kedreamix/ChatPaperFree">ChatPaperFree</a> ï¼Œè¿˜è¯·æ‚¨ç»™æˆ‘ä»¬ä¸€äº›é¼“åŠ±ï¼â­ï¸ <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/Kedreamix/ChatPaperFree">HuggingFaceå…è´¹ä½“éªŒ</a></p>
</blockquote>
<h1 id="2025-11-16-æ›´æ–°"><a href="#2025-11-16-æ›´æ–°" class="headerlink" title="2025-11-16 æ›´æ–°"></a>2025-11-16 æ›´æ–°</h1><h2 id="NeuroCLIP-Brain-Inspired-Prompt-Tuning-for-EEG-to-Image-Multimodal-Contrastive-Learning"><a href="#NeuroCLIP-Brain-Inspired-Prompt-Tuning-for-EEG-to-Image-Multimodal-Contrastive-Learning" class="headerlink" title="NeuroCLIP: Brain-Inspired Prompt Tuning for EEG-to-Image Multimodal Contrastive Learning"></a>NeuroCLIP: Brain-Inspired Prompt Tuning for EEG-to-Image Multimodal Contrastive Learning</h2><p><strong>Authors:Jiyuan Wang, Li Zhang, Haipeng Lin, Qile Liu, Gan Huang, Ziyu Li, Zhen Liang, Xia Wu</strong></p>
<p>Recent advances in brain-inspired artificial intelligence have sought to align neural signals with visual semantics using multimodal models such as CLIP. However, existing methods often treat CLIP as a static feature extractor, overlooking its adaptability to neural representations and the inherent physiological-symbolic gap in EEG-image alignment. To address these challenges, we present NeuroCLIP, a prompt tuning framework tailored for EEG-to-image contrastive learning. Our approach introduces three core innovations: (1) We design a dual-stream visual embedding pipeline that combines dynamic filtering and token-level fusion to generate instance-level adaptive prompts, which guide the adjustment of patch embedding tokens based on image content, thereby enabling fine-grained modulation of visual representations under neural constraints; (2) We are the first to introduce visual prompt tokens into EEG-image alignment, acting as global, modality-level prompts that work in conjunction with instance-level adjustments. These visual prompt tokens are inserted into the Transformer architecture to facilitate neural-aware adaptation and parameter optimization at a global level; (3) Inspired by neuroscientific principles of human visual encoding, we propose a refined contrastive loss that better model the semantic ambiguity and cross-modal noise present in EEG signals. On the THINGS-EEG2 dataset, NeuroCLIP achieves a Top-1 accuracy of 63.2% in zero-shot image retrieval, surpassing the previous best method by +12.3%, and demonstrates strong generalization under inter-subject conditions (+4.6% Top-1), highlighting the potential of physiology-aware prompt tuning for bridging brain signals and visual semantics.</p>
<blockquote>
<p>æœ€è¿‘ï¼Œè„‘å¯å‘äººå·¥æ™ºèƒ½çš„è¿›å±•é€šè¿‡ä½¿ç”¨CLIPç­‰å¤šæ¨¡æ€æ¨¡å‹æ¥å¯¹é½ç¥ç»ä¿¡å·å’Œè§†è§‰è¯­ä¹‰ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å¾€å¾€å°†CLIPè§†ä¸ºé™æ€ç‰¹å¾æå–å™¨ï¼Œå¿½ç•¥äº†å…¶é€‚åº”ç¥ç»è¡¨å¾çš„èƒ½åŠ›ä»¥åŠè„‘ç”µå›¾ä¸å›¾åƒå¯¹é½ä¸­å›ºæœ‰çš„ç”Ÿç†-ç¬¦å·å·®è·ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†NeuroCLIPï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè„‘ç”µå›¾åˆ°å›¾åƒçš„å¯¹æ¯”å­¦ä¹ è®¾è®¡çš„æç¤ºè°ƒæ•´æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¼•å…¥äº†ä¸‰ä¸ªæ ¸å¿ƒåˆ›æ–°ç‚¹ï¼šï¼ˆ1ï¼‰æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŒæµè§†è§‰åµŒå…¥ç®¡é“ï¼Œç»“åˆåŠ¨æ€è¿‡æ»¤å’Œä»¤ç‰Œçº§èåˆï¼Œç”Ÿæˆå®ä¾‹çº§è‡ªé€‚åº”æç¤ºï¼Œæ ¹æ®å›¾åƒå†…å®¹å¼•å¯¼è¡¥ä¸åµŒå…¥ä»¤ç‰Œçš„è°ƒæ•´ï¼Œä»è€Œåœ¨ç¥ç»çº¦æŸä¸‹å®ç°ç²¾ç»†ç²’åº¦çš„è§†è§‰è¡¨ç¤ºçš„è°ƒåˆ¶ï¼›ï¼ˆ2ï¼‰æˆ‘ä»¬é¦–æ¬¡å°†è§†è§‰æç¤ºä»¤ç‰Œå¼•å…¥åˆ°è„‘ç”µå›¾ä¸å›¾åƒå¯¹é½ä¸­ï¼Œä½œä¸ºå…¨å±€ã€æ¨¡æ€çº§åˆ«çš„æç¤ºï¼Œä¸å®ä¾‹çº§åˆ«çš„è°ƒæ•´ååŒå·¥ä½œã€‚è¿™äº›è§†è§‰æç¤ºä»¤ç‰Œè¢«æ’å…¥åˆ°Transformeræ¶æ„ä¸­ï¼Œä¿ƒè¿›ç¥ç»æ„ŸçŸ¥çš„é€‚åº”æ€§å’Œå…¨å±€å‚æ•°ä¼˜åŒ–ï¼›ï¼ˆ3ï¼‰å—ç¥ç»ç§‘å­¦ä¸­äººç±»è§†è§‰ç¼–ç åŸç†çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç²¾ç»†çš„å¯¹æ¯”æŸå¤±ï¼Œæ›´å¥½åœ°æ¨¡æ‹Ÿè„‘ç”µå›¾ä¿¡å·ä¸­è¯­ä¹‰çš„æ¨¡ç³Šæ€§å’Œè·¨æ¨¡æ€å™ªå£°ã€‚åœ¨THINGS-EEG2æ•°æ®é›†ä¸Šï¼ŒNeuroCLIPåœ¨é›¶æ ·æœ¬å›¾åƒæ£€ç´¢ä¸­å®ç°äº†63.2%çš„Top-1å‡†ç¡®ç‡ï¼Œè¶…è¶Šäº†ä¹‹å‰æœ€ä½³æ–¹æ³•çš„+12.3%ï¼Œå¹¶åœ¨è·¨ä¸»ä½“æ¡ä»¶ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ˆ+4.6% Top-1ï¼‰ï¼Œå‡¸æ˜¾äº†ç”Ÿç†æ„ŸçŸ¥æç¤ºè°ƒæ•´åœ¨å¼¥åˆè„‘ä¿¡å·å’Œè§†è§‰è¯­ä¹‰æ–¹é¢çš„æ½œåŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.09250v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†é’ˆå¯¹è„‘ç”µä¿¡å·ä¸å›¾åƒå¯¹æ¯”å­¦ä¹ çš„ç¥ç»CLIPæŠ€æœ¯ã€‚å®ƒè®¾è®¡äº†ä¸€ä¸ªåŒæµè§†è§‰åµŒå…¥ç®¡é“ï¼Œç»“åˆåŠ¨æ€è¿‡æ»¤å’Œä»¤ç‰Œçº§èåˆç”Ÿæˆå®ä¾‹çº§è‡ªé€‚åº”æç¤ºï¼ŒæŒ‡å¯¼åŸºäºå›¾åƒå†…å®¹çš„è¡¥ä¸åµŒå…¥ä»¤ç‰Œçš„è°ƒæ•´ã€‚æ­¤å¤–ï¼Œé¦–æ¬¡å°†è§†è§‰æç¤ºä»¤ç‰Œå¼•å…¥è„‘ç”µ-å›¾åƒå¯¹é½ä¸­ï¼Œæ’å…¥åˆ°Transformeræ¶æ„ä¸­ï¼Œä¿ƒè¿›ç¥ç»æ„ŸçŸ¥çš„é€‚åº”æ€§å’Œå…¨å±€å‚æ•°ä¼˜åŒ–ã€‚æœ€åï¼Œå—åˆ°ç¥ç»ç§‘å­¦åŸç†çš„å¯å‘ï¼Œæå‡ºäº†ä¸€ç§ç²¾ç»†çš„å¯¹æ¯”æŸå¤±ï¼Œæ›´å¥½åœ°æ¨¡æ‹ŸEEGä¿¡å·ä¸­çš„è¯­ä¹‰æ¨¡ç³Šå’Œè·¨æ¨¡æ€å™ªå£°ã€‚åœ¨THINGS-EEG2æ•°æ®é›†ä¸Šï¼ŒNeuroCLIPå®ç°äº†é›¶æ ·æœ¬å›¾åƒæ£€ç´¢çš„Top-1å‡†ç¡®ç‡63.2%ï¼Œè¶…è¶Šäº†ä¹‹å‰æœ€ä½³æ–¹æ³•+12.3%ï¼Œå¹¶åœ¨è·¨ä¸»ä½“æ¡ä»¶ä¸‹è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼ˆ+4.6% Top-1ï¼‰ï¼Œå±•ç¤ºäº†ç”Ÿç†æ„ŸçŸ¥æç¤ºè°ƒæ•´åœ¨æ¡¥æ¥è„‘ä¿¡å·å’Œè§†è§‰è¯­ä¹‰æ–¹é¢çš„æ½œåŠ›ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>NeuroCLIPåˆ©ç”¨åŒæµè§†è§‰åµŒå…¥ç®¡é“ç»“åˆåŠ¨æ€è¿‡æ»¤å’Œä»¤ç‰Œèåˆç”Ÿæˆè‡ªé€‚åº”æç¤ºï¼ŒæŒ‡å¯¼åŸºäºå›¾åƒå†…å®¹çš„è¡¥ä¸åµŒå…¥ä»¤ç‰Œçš„è°ƒæ•´ã€‚</li>
<li>é¦–æ¬¡å¼•å…¥è§†è§‰æç¤ºä»¤ç‰Œåˆ°EEG-å›¾åƒå¯¹é½ä¸­ï¼Œä¿ƒè¿›ç¥ç»æ„ŸçŸ¥çš„é€‚åº”æ€§å’Œå…¨å±€å‚æ•°ä¼˜åŒ–ã€‚</li>
<li>ç²¾ç»†çš„å¯¹æ¯”æŸå¤±å¯ä»¥æ›´å¥½åœ°æ¨¡æ‹ŸEEGä¿¡å·ä¸­çš„è¯­ä¹‰æ¨¡ç³Šå’Œè·¨æ¨¡æ€å™ªå£°ã€‚</li>
<li>NeuroCLIPåœ¨THINGS-EEG2æ•°æ®é›†ä¸Šå®ç°äº†è¾ƒé«˜çš„é›¶æ ·æœ¬å›¾åƒæ£€ç´¢å‡†ç¡®ç‡ï¼Œè¡¨ç°å‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚</li>
<li>è¯¥æŠ€æœ¯è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ï¼Œå¹¶åœ¨è·¨ä¸»ä½“æ¡ä»¶ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>NeuroCLIPæœ‰åŠ©äºæ¡¥æ¥è„‘ä¿¡å·å’Œè§†è§‰è¯­ä¹‰ï¼Œå±•ç¤ºäº†ç”Ÿç†æ„ŸçŸ¥æç¤ºè°ƒæ•´åœ¨è¿™ä¸€é¢†åŸŸçš„æ½œåŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09250">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-472043f9af3d21706570d1e097526962" align="middle">
<img src="https://picx.zhimg.com/v2-d2fa55162a441a8aef441e2da8622891" align="middle">
<img src="https://picx.zhimg.com/v2-1ad5b0193701a962cb1d25a786dce93a" align="middle">
</details>


<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="Improve-Contrastive-Clustering-Performance-by-Multiple-Fusing-Augmenting-ViT-Blocks"><a href="#Improve-Contrastive-Clustering-Performance-by-Multiple-Fusing-Augmenting-ViT-Blocks" class="headerlink" title="Improve Contrastive Clustering Performance by Multiple Fusing-Augmenting ViT Blocks"></a>Improve Contrastive Clustering Performance by Multiple Fusing-Augmenting ViT Blocks</h2><p><strong>Authors:Cheng Wang, Shuisheng Zhou, Fengjiao Peng, Jin Sheng, Feng Ye, Yinli Dong</strong></p>
<p>In the field of image clustering, the widely used contrastive learning networks improve clustering performance by maximizing the similarity between positive pairs and the dissimilarity of negative pairs of the inputs. Extant contrastive learning networks, whose two encoders often implicitly interact with each other by parameter sharing or momentum updating, may not fully exploit the complementarity and similarity of the positive pairs to extract clustering features from input data. To explicitly fuse the learned features of positive pairs, we design a novel multiple fusing-augmenting ViT blocks (MFAVBs) based on the excellent feature learning ability of Vision Transformers (ViT). Firstly, two preprocessed augmentions as positive pairs are separately fed into two shared-weight ViTs, then their output features are fused to input into a larger ViT. Secondly, the learned features are split into a pair of new augmented positive samples and passed to the next FAVBs, enabling multiple fusion and augmention through MFAVBs operations. Finally, the learned features are projected into both instance-level and clustering-level spaces to calculate the cross-entropy loss, followed by parameter updates by backpropagation to finalize the training process. To further enhance ability of the model to distinguish between similar images, our input data for the network we propose is preprocessed augmentions with features extracted from the CLIP pretrained model. Our experiments on seven public datasets demonstrate that MFAVBs serving as the backbone for contrastive clustering outperforms the state-of-the-art techniques in terms of clustering performance.</p>
<blockquote>
<p>åœ¨å›¾åƒèšç±»é¢†åŸŸï¼Œå¹¿æ³›ä½¿ç”¨çš„å¯¹æ¯”å­¦ä¹ ç½‘ç»œé€šè¿‡æœ€å¤§åŒ–æ­£æ ·æœ¬å¯¹ä¹‹é—´çš„ç›¸ä¼¼æ€§å’Œè´Ÿæ ·æœ¬å¯¹ä¹‹é—´çš„ä¸ç›¸ä¼¼æ€§æ¥æé«˜èšç±»æ€§èƒ½ã€‚ç°æœ‰çš„å¯¹æ¯”å­¦ä¹ ç½‘ç»œï¼Œå…¶ä¸¤ä¸ªç¼–ç å™¨é€šå¸¸é€šè¿‡å‚æ•°å…±äº«æˆ–åŠ¨é‡æ›´æ–°éšå¼åœ°ç›¸äº’äº¤äº’ï¼Œå¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨æ­£æ ·æœ¬å¯¹çš„äº’è¡¥æ€§å’Œç›¸ä¼¼æ€§æ¥ä»è¾“å…¥æ•°æ®ä¸­æå–èšç±»ç‰¹å¾ã€‚ä¸ºäº†æ˜ç¡®åœ°èåˆæ­£æ ·æœ¬å¯¹çš„å­¦ä¹ ç‰¹å¾ï¼Œæˆ‘ä»¬åŸºäºè§†è§‰å˜å‹å™¨ï¼ˆViTï¼‰çš„ä¼˜ç§€ç‰¹å¾å­¦ä¹ èƒ½åŠ›ï¼Œè®¾è®¡äº†ä¸€ç§æ–°å‹çš„å¤šèåˆå¢å¼ºViTå—ï¼ˆMFAVBsï¼‰ã€‚é¦–å…ˆï¼Œå°†ä¸¤ä¸ªé¢„å¤„ç†çš„å¢å¼ºç‰ˆæœ¬ä½œä¸ºæ­£æ ·æœ¬å¯¹åˆ†åˆ«è¾“å…¥åˆ°ä¸¤ä¸ªå…±äº«æƒé‡çš„ViTsä¸­ï¼Œç„¶åå°†å®ƒä»¬çš„è¾“å‡ºç‰¹å¾èåˆå¹¶è¾“å…¥åˆ°ä¸€ä¸ªæ›´å¤§çš„ViTä¸­ã€‚å…¶æ¬¡ï¼Œå°†å­¦ä¹ åˆ°çš„ç‰¹å¾åˆ†å‰²æˆä¸€å¯¹æ–°çš„å¢å¼ºæ­£æ ·æœ¬å¹¶ä¼ é€’ç»™ä¸‹ä¸€ä¸ªFAVBsï¼Œé€šè¿‡MFAVBsæ“ä½œå®ç°å¤šæ¬¡èåˆå’Œå¢å¼ºã€‚æœ€åï¼Œå°†å­¦ä¹ åˆ°çš„ç‰¹å¾æŠ•å½±åˆ°å®ä¾‹çº§å’Œèšç±»çº§ç©ºé—´ï¼Œè®¡ç®—äº¤å‰ç†µæŸå¤±ï¼Œç„¶åé€šè¿‡åå‘ä¼ æ’­è¿›è¡Œå‚æ•°æ›´æ–°ä»¥å®Œæˆè®­ç»ƒè¿‡ç¨‹ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¨¡å‹åŒºåˆ†ç›¸ä¼¼å›¾åƒçš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸ºæ‰€æå‡ºçš„ç½‘ç»œä½¿ç”¨çš„è¾“å…¥æ•°æ®æ˜¯CLIPé¢„è®­ç»ƒæ¨¡å‹æå–çš„ç‰¹å¾çš„é¢„å¤„ç†å¢å¼ºç‰ˆæœ¬ã€‚æˆ‘ä»¬åœ¨ä¸ƒä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œä½œä¸ºå¯¹æ¯”èšç±»çš„éª¨å¹²ç½‘ï¼ŒMFAVBsåœ¨èšç±»æ€§èƒ½ä¸Šè¶…è¿‡äº†æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.08883v1">PDF</a> </p>
<p><strong>Summary</strong><br>     åŸºäºå¯¹æ¯”å­¦ä¹ ç½‘ç»œï¼Œé€šè¿‡æœ€å¤§åŒ–æ­£è´Ÿæ ·æœ¬å¯¹çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§æ¥æå‡å›¾åƒèšç±»çš„æ€§èƒ½ã€‚ä¸ºæ›´å……åˆ†åœ°æå–èšç±»ç‰¹å¾ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ç§åŸºäºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰çš„å¤šèåˆå¢å¼ºViTå—ï¼ˆMFAVBsï¼‰ã€‚é€šè¿‡é¢„å¤„ç†çš„å¢å¼ºæ­£æ ·æœ¬å¯¹è¾“å…¥åˆ°å…±äº«æƒé‡çš„ViTä¸­ï¼Œèåˆç‰¹å¾åå†è¾“å…¥æ›´å¤§çš„ViTã€‚æ­¤å¤–ï¼Œå°†å­¦ä¹ åˆ°çš„ç‰¹å¾åˆ†è£‚æˆæ–°çš„å¢å¼ºæ­£æ ·æœ¬å¯¹ï¼Œå¤šæ¬¡èåˆå’Œå¢å¼ºã€‚æœ€åï¼Œå°†ç‰¹å¾æŠ•å½±åˆ°å®ä¾‹çº§åˆ«å’Œèšç±»çº§åˆ«ç©ºé—´ï¼Œè®¡ç®—äº¤å‰ç†µæŸå¤±ï¼Œé€šè¿‡åå‘ä¼ æ’­æ›´æ–°å‚æ•°æ¥å®Œæˆè®­ç»ƒè¿‡ç¨‹ã€‚ä½¿ç”¨CLIPé¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾é¢„å¤„ç†è¾“å…¥æ•°æ®ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹åŒºåˆ†ç›¸ä¼¼å›¾åƒçš„èƒ½åŠ›ã€‚åœ¨ä¸ƒä¸ªå…¬å¼€æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒMFAVBsä½œä¸ºå¯¹æ¯”èšç±»çš„éª¨å¹²ç½‘ï¼Œåœ¨èšç±»æ€§èƒ½ä¸Šè¶…è¶Šäº†æœ€å…ˆè¿›æŠ€æœ¯ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>å¯¹æ¯”å­¦ä¹ ç½‘ç»œé€šè¿‡æœ€å¤§åŒ–æ­£è´Ÿæ ·æœ¬å¯¹çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚æ€§æ¥æå‡å›¾åƒèšç±»çš„æ€§èƒ½ã€‚</li>
<li>MFAVBsåŸºäºè§†è§‰è½¬æ¢å™¨ï¼ˆViTï¼‰è®¾è®¡ï¼Œèƒ½æ›´å……åˆ†åœ°æå–èšç±»ç‰¹å¾ã€‚</li>
<li>èåˆé¢„å¤„ç†çš„å¢å¼ºæ­£æ ·æœ¬å¯¹ï¼Œæé«˜æ¨¡å‹å­¦ä¹ æ•ˆæœã€‚</li>
<li>é€šè¿‡å¤šæ¬¡èåˆå’Œå¢å¼ºå­¦ä¹ åˆ°çš„ç‰¹å¾ï¼Œè¿›ä¸€æ­¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>ç‰¹å¾è¢«æŠ•å½±åˆ°å®ä¾‹çº§åˆ«å’Œèšç±»çº§åˆ«ç©ºé—´ï¼Œè®¡ç®—äº¤å‰ç†µæŸå¤±ä»¥ä¼˜åŒ–æ¨¡å‹ã€‚</li>
<li>ä½¿ç”¨CLIPé¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾é¢„å¤„ç†è¾“å…¥æ•°æ®ï¼Œå¢å¼ºæ¨¡å‹åŒºåˆ†ç›¸ä¼¼å›¾åƒçš„èƒ½åŠ›ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.08883">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-8da36272a0794a14089401c1ef805036" align="middle">
<img src="https://picx.zhimg.com/v2-721c0321932786121fb877aa0dc1a1be" align="middle">
</details>


<h1 id="-1"><a href="#-1" class="headerlink" title=""></a></h1><h2 id="DI3CL-Contrastive-Learning-With-Dynamic-Instances-and-Contour-Consistency-for-SAR-Land-Cover-Classification-Foundation-Model"><a href="#DI3CL-Contrastive-Learning-With-Dynamic-Instances-and-Contour-Consistency-for-SAR-Land-Cover-Classification-Foundation-Model" class="headerlink" title="DI3CL: Contrastive Learning With Dynamic Instances and Contour Consistency for SAR Land-Cover Classification Foundation Model"></a>DI3CL: Contrastive Learning With Dynamic Instances and Contour Consistency for SAR Land-Cover Classification Foundation Model</h2><p><strong>Authors:Zhongle Ren, Hui Ding, Kai Wang, Biao Hou, Xingyu Luo, Weibin Li, Licheng Jiao</strong></p>
<p>Although significant advances have been achieved in SAR land-cover classification, recent methods remain predominantly focused on supervised learning, which relies heavily on extensive labeled datasets. This dependency not only limits scalability and generalization but also restricts adaptability to diverse application scenarios. In this paper, a general-purpose foundation model for SAR land-cover classification is developed, serving as a robust cornerstone to accelerate the development and deployment of various downstream models. Specifically, a Dynamic Instance and Contour Consistency Contrastive Learning (DI3CL) pre-training framework is presented, which incorporates a Dynamic Instance (DI) module and a Contour Consistency (CC) module. DI module enhances global contextual awareness by enforcing local consistency across different views of the same region. CC module leverages shallow feature maps to guide the model to focus on the geometric contours of SAR land-cover objects, thereby improving structural discrimination. Additionally, to enhance robustness and generalization during pre-training, a large-scale and diverse dataset named SARSense, comprising 460,532 SAR images, is constructed to enable the model to capture comprehensive and representative features. To evaluate the generalization capability of our foundation model, we conducted extensive experiments across a variety of SAR land-cover classification tasks, including SAR land-cover mapping, water body detection, and road extraction. The results consistently demonstrate that the proposed DI3CL outperforms existing methods. Our code and pre-trained weights are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/SARpre-train/DI3CL">https://github.com/SARpre-train/DI3CL</a>.</p>
<blockquote>
<p>å°½ç®¡SARåœŸåœ°è¦†ç›–åˆ†ç±»å·²ç»å–å¾—äº†é‡å¤§è¿›å±•ï¼Œä½†æœ€è¿‘çš„æ–¹æ³•ä»ç„¶ä¸»è¦ä¾§é‡äºç›‘ç£å­¦ä¹ ï¼Œè¿™ä¸¥é‡ä¾èµ–äºå¤§é‡çš„æ ‡è®°æ•°æ®é›†ã€‚è¿™ç§ä¾èµ–ä¸ä»…é™åˆ¶äº†å¯æ‰©å±•æ€§å’Œé€šç”¨æ€§ï¼Œè¿˜é™åˆ¶äº†å…¶åœ¨å„ç§åº”ç”¨åœºæ™¯ä¸­çš„é€‚åº”æ€§ã€‚æœ¬æ–‡å¼€å‘äº†ä¸€ä¸ªç”¨äºSARåœŸåœ°è¦†ç›–åˆ†ç±»çš„é€šç”¨åŸºç¡€æ¨¡å‹ï¼Œä½œä¸ºåŠ é€Ÿå„ç§ä¸‹æ¸¸æ¨¡å‹å¼€å‘å’Œéƒ¨ç½²çš„ç¨³å¥åŸºçŸ³ã€‚å…·ä½“è€Œè¨€ï¼Œæå‡ºäº†ä¸€ç§åŠ¨æ€å®ä¾‹å’Œè½®å»“ä¸€è‡´æ€§å¯¹æ¯”å­¦ä¹ ï¼ˆDI3CLï¼‰é¢„è®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç»“åˆäº†åŠ¨æ€å®ä¾‹ï¼ˆDIï¼‰æ¨¡å—å’Œè½®å»“ä¸€è‡´æ€§ï¼ˆCCï¼‰æ¨¡å—ã€‚DIæ¨¡å—é€šè¿‡å¼ºåˆ¶åŒä¸€åœ°åŒºä¸åŒè§†å›¾ä¹‹é—´çš„å±€éƒ¨ä¸€è‡´æ€§ï¼Œå¢å¼ºäº†å…¨å±€ä¸Šä¸‹æ–‡æ„è¯†ã€‚CCæ¨¡å—åˆ©ç”¨æµ…å±‚ç‰¹å¾å›¾å¼•å¯¼æ¨¡å‹å…³æ³¨SARåœŸåœ°è¦†ç›–å¯¹è±¡çš„å‡ ä½•è½®å»“ï¼Œä»è€Œæé«˜ç»“æ„è¾¨åˆ«èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºé¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ï¼Œæ„å»ºäº†ä¸€ä¸ªåä¸ºSARSenseçš„å¤§è§„æ¨¡å¤šæ ·åŒ–æ•°æ®é›†ï¼ŒåŒ…å«460532å¼ SARå›¾åƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•è·å…¨é¢å’Œä»£è¡¨æ€§çš„ç‰¹å¾ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬åŸºç¡€æ¨¡å‹çš„é€šç”¨æ€§ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¤§é‡å®éªŒï¼Œæ¶µç›–äº†å„ç§SARåœŸåœ°è¦†ç›–åˆ†ç±»ä»»åŠ¡ï¼ŒåŒ…æ‹¬SARåœŸåœ°è¦†ç›–æ˜ å°„ã€æ°´ä½“æ£€æµ‹ã€é“è·¯æå–ç­‰ã€‚ç»“æœä¸€è‡´è¡¨æ˜ï¼Œæ‰€æå‡ºçš„DI3CLä¼˜äºç°æœ‰æ–¹æ³•ã€‚æˆ‘ä»¬çš„ä»£ç å’Œé¢„è®­ç»ƒæƒé‡å¯åœ¨ä»¥ä¸‹ç½‘å€å…¬å¼€è·å–ï¼š<a target="_blank" rel="noopener" href="https://github.com/SARpre-train/DI3CL">https://github.com/SARpre-train/DI3CL</a>ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07808v2">PDF</a> 18 pages, 10 figures;Submitted to IEEE Transactions on Image Processing (TIP); In peer review</p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºSARåœŸåœ°è¦†ç›–åˆ†ç±»çš„é€šç”¨åŸºç¡€æ¨¡å‹ï¼Œè¯¥æ¨¡å‹é‡‡ç”¨åŠ¨æ€å®ä¾‹å’Œè½®å»“ä¸€è‡´æ€§å¯¹æ¯”å­¦ä¹ ï¼ˆDI3CLï¼‰é¢„è®­ç»ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬åŠ¨æ€å®ä¾‹ï¼ˆDIï¼‰æ¨¡å—å’Œè½®å»“ä¸€è‡´æ€§ï¼ˆCCï¼‰æ¨¡å—ï¼Œå¯å¢å¼ºå…¨å±€ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼Œæé«˜ç»“æ„è¾¨åˆ«èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºé¢„è®­ç»ƒæœŸé—´çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†SARSenseã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„DI3CLåœ¨SARåœŸåœ°è¦†ç›–åˆ†ç±»ä»»åŠ¡ä¸Šä¼˜äºç°æœ‰æ–¹æ³•ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>æœ¬æ–‡æå‡ºäº†ä¸€ç§ç”¨äºSARåœŸåœ°è¦†ç›–åˆ†ç±»çš„åŸºç¡€æ¨¡å‹ï¼Œç»“åˆäº†åŠ¨æ€å®ä¾‹ï¼ˆDIï¼‰æ¨¡å—å’Œè½®å»“ä¸€è‡´æ€§ï¼ˆCCï¼‰æ¨¡å—ï¼Œæé«˜äº†æ¨¡å‹çš„å…¨çƒä¸Šä¸‹æ–‡æ„ŸçŸ¥å’Œç»“æ„è¾¨åˆ«èƒ½åŠ›ã€‚</li>
<li>ä¸ºäº†å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ï¼Œæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šæ ·åŒ–çš„æ•°æ®é›†SARSenseã€‚</li>
<li>é‡‡ç”¨å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰æ›´å…¨é¢ã€æ›´å…·ä»£è¡¨æ€§çš„ç‰¹å¾ã€‚</li>
<li>åœ¨å¤šç§SARåœŸåœ°è¦†ç›–åˆ†ç±»ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒéªŒè¯ï¼ŒåŒ…æ‹¬SARåœŸåœ°è¦†ç›–æ˜ å°„ã€æ°´ä½“æ£€æµ‹ã€é“è·¯æå–ç­‰ã€‚</li>
<li>å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„DI3CLæ¡†æ¶åœ¨SARåœŸåœ°è¦†ç›–åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºç°æœ‰æ–¹æ³•ã€‚</li>
<li>è¯¥æ¨¡å‹çš„ä»£ç å’Œé¢„è®­ç»ƒæƒé‡å·²å…¬å¼€å‘å¸ƒåœ¨GitHubä¸Šã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07808">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-b43cc9b3a78a6e5b9b48f4f78d189815" align="middle">
<img src="https://picx.zhimg.com/v2-d8361df55050f5a3edfb78c5058599ee" align="middle">
<img src="https://picx.zhimg.com/v2-d35e12c0d4a24adc5e189eba7526fe9d" align="middle">
<img src="https://picx.zhimg.com/v2-768cca06a7dd52e39d1162fd41bdef25" align="middle">
</details>


<h1 id="-2"><a href="#-2" class="headerlink" title=""></a></h1><h2 id="Semantic-Consistent-Bidirectional-Contrastive-Hashing-for-Noisy-Multi-Label-Cross-Modal-Retrieval"><a href="#Semantic-Consistent-Bidirectional-Contrastive-Hashing-for-Noisy-Multi-Label-Cross-Modal-Retrieval" class="headerlink" title="Semantic-Consistent Bidirectional Contrastive Hashing for Noisy Multi-Label Cross-Modal Retrieval"></a>Semantic-Consistent Bidirectional Contrastive Hashing for Noisy Multi-Label Cross-Modal Retrieval</h2><p><strong>Authors:Likang Peng, Chao Su, Wenyuan Wu, Yuan Sun, Dezhong Peng, Xi Peng, Xu Wang</strong></p>
<p>Cross-modal hashing (CMH) facilitates efficient retrieval across different modalities (e.g., image and text) by encoding data into compact binary representations. While recent methods have achieved remarkable performance, they often rely heavily on fully annotated datasets, which are costly and labor-intensive to obtain. In real-world scenarios, particularly in multi-label datasets, label noise is prevalent and severely degrades retrieval performance. Moreover, existing CMH approaches typically overlook the partial semantic overlaps inherent in multi-label data, limiting their robustness and generalization. To tackle these challenges, we propose a novel framework named Semantic-Consistent Bidirectional Contrastive Hashing (SCBCH). The framework comprises two complementary modules: (1) Cross-modal Semantic-Consistent Classification (CSCC), which leverages cross-modal semantic consistency to estimate sample reliability and reduce the impact of noisy labels; (2) Bidirectional Soft Contrastive Hashing (BSCH), which dynamically generates soft contrastive sample pairs based on multi-label semantic overlap, enabling adaptive contrastive learning between semantically similar and dissimilar samples across modalities. Extensive experiments on four widely-used cross-modal retrieval benchmarks validate the effectiveness and robustness of our method, consistently outperforming state-of-the-art approaches under noisy multi-label conditions.</p>
<blockquote>
<p>è·¨æ¨¡æ€å“ˆå¸Œï¼ˆCMHï¼‰é€šè¿‡å°†æ•°æ®ç¼–ç ä¸ºç´§å‡‘çš„äºŒè¿›åˆ¶è¡¨ç¤ºæ¥ä¿ƒè¿›ä¸åŒæ¨¡æ€ï¼ˆä¾‹å¦‚å›¾åƒå’Œæ–‡æœ¬ï¼‰ä¹‹é—´çš„æœ‰æ•ˆæ£€ç´¢ã€‚è™½ç„¶æœ€è¿‘çš„æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬é€šå¸¸ä¸¥é‡ä¾èµ–äºå®Œå…¨æ ‡æ³¨çš„æ•°æ®é›†ï¼Œè€Œè¿™äº›æ•°æ®é›†çš„è·å–æˆæœ¬é«˜æ˜‚ä¸”åŠ³åŠ¨å¯†é›†ã€‚åœ¨ç°å®ä¸–ç•Œåœºæ™¯ä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šæ ‡ç­¾æ•°æ®é›†ä¸­ï¼Œæ ‡ç­¾å™ªå£°æ™®éå­˜åœ¨ï¼Œä¸¥é‡é™ä½äº†æ£€ç´¢æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„CMHæ–¹æ³•é€šå¸¸å¿½ç•¥äº†å¤šæ ‡ç­¾æ•°æ®å›ºæœ‰çš„éƒ¨åˆ†è¯­ä¹‰é‡å ï¼Œé™åˆ¶äº†å®ƒä»¬çš„ç¨³å¥æ€§å’Œé€šç”¨æ€§ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºè¯­ä¹‰ä¸€è‡´åŒå‘å¯¹æ¯”å“ˆå¸Œï¼ˆSCBCHï¼‰çš„æ–°æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªäº’è¡¥æ¨¡å—ï¼šï¼ˆ1ï¼‰è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´åˆ†ç±»ï¼ˆCSCCï¼‰ï¼Œå®ƒåˆ©ç”¨è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§æ¥ä¼°è®¡æ ·æœ¬å¯é æ€§å¹¶å‡å°‘å™ªå£°æ ‡ç­¾çš„å½±å“ï¼›ï¼ˆ2ï¼‰åŒå‘è½¯å¯¹æ¯”å“ˆå¸Œï¼ˆBSCHï¼‰ï¼Œå…¶åŸºäºå¤šæ ‡ç­¾è¯­ä¹‰é‡å åŠ¨æ€ç”Ÿæˆè½¯å¯¹æ¯”æ ·æœ¬å¯¹ï¼Œä½¿æ¨¡æ€ä¹‹é—´è¯­ä¹‰ç›¸ä¼¼å’Œä¸ç›¸ä¼¼çš„æ ·æœ¬èƒ½å¤Ÿè¿›è¡Œè‡ªé€‚åº”å¯¹æ¯”å­¦ä¹ ã€‚åœ¨å››ä¸ªå¹¿æ³›ä½¿ç”¨çš„è·¨æ¨¡æ€æ£€ç´¢åŸºå‡†æ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œç¨³å¥æ€§ï¼Œåœ¨å™ªå£°å¤šæ ‡ç­¾æ¡ä»¶ä¸‹å§‹ç»ˆä¼˜äºæœ€æ–°æŠ€æœ¯çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.07780v1">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡ä»‹ç»äº†è·¨æ¨¡æ€å“ˆå¸Œï¼ˆCMHï¼‰æŠ€æœ¯åœ¨ä¸åŒæ¨¡æ€ï¼ˆå¦‚å›¾åƒå’Œæ–‡æœ¬ï¼‰ä¹‹é—´çš„æ£€ç´¢åº”ç”¨ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•ä¾èµ–å…¨æ ‡æ³¨æ•°æ®é›†çš„é—®é¢˜ï¼Œä»¥åŠçœŸå®åœºæ™¯ä¸­æ ‡ç­¾å™ªå£°å’Œå¤šæ ‡ç­¾æ•°æ®è¯­ä¹‰é‡å çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶â€”â€”è¯­ä¹‰ä¸€è‡´åŒå‘å¯¹æ¯”å“ˆå¸Œï¼ˆSCBCHï¼‰ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªäº’è¡¥æ¨¡å—ï¼šåˆ©ç”¨è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§è¿›è¡Œæ ·æœ¬å¯é æ€§ä¼°è®¡å’Œå‡å°‘å™ªå£°å½±å“çš„è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´åˆ†ç±»ï¼ˆCSCCï¼‰ï¼Œä»¥åŠåŸºäºå¤šæ ‡ç­¾è¯­ä¹‰é‡å åŠ¨æ€ç”Ÿæˆè½¯å¯¹æ¯”æ ·æœ¬å¯¹çš„åŒå‘è½¯å¯¹æ¯”å“ˆå¸Œï¼ˆBSCHï¼‰ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å™ªå£°å¤šæ ‡ç­¾æ¡ä»¶ä¸‹ï¼Œç›¸è¾ƒäºç°æœ‰æ–¹æ³•æ›´å…·æ•ˆæœå’Œç¨³å¥æ€§ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>è·¨æ¨¡æ€å“ˆå¸Œï¼ˆCMHï¼‰èƒ½å°†ä¸åŒæ¨¡æ€çš„æ•°æ®ç¼–ç æˆç´§å‡‘çš„äºŒè¿›åˆ¶è¡¨ç¤ºï¼Œä»è€Œæé«˜æ£€ç´¢æ•ˆç‡ã€‚</li>
<li>ç°æœ‰æ–¹æ³•å¤šä¾èµ–å…¨æ ‡æ³¨æ•°æ®é›†ï¼Œæˆæœ¬é«˜æ˜‚ä¸”åŠ³åŠ¨å¯†é›†ã€‚</li>
<li>çœŸå®åœºæ™¯ä¸­æ ‡ç­¾å™ªå£°æ™®éï¼Œä¸¥é‡å½±å“æ£€ç´¢æ€§èƒ½ã€‚</li>
<li>å¤šæ ‡ç­¾æ•°æ®ä¸­çš„éƒ¨åˆ†è¯­ä¹‰é‡å è¢«ç°æœ‰CMHæ–¹æ³•æ‰€å¿½è§†ï¼Œé™åˆ¶å…¶ç¨³å¥æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶SCBCHï¼ŒåŒ…å«è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´åˆ†ç±»ï¼ˆCSCCï¼‰å’ŒåŒå‘è½¯å¯¹æ¯”å“ˆå¸Œï¼ˆBSCHï¼‰ä¸¤ä¸ªæ¨¡å—ã€‚</li>
<li>CSCCåˆ©ç”¨è·¨æ¨¡æ€è¯­ä¹‰ä¸€è‡´æ€§ä¼°è®¡æ ·æœ¬å¯é æ€§ï¼Œé™ä½å™ªå£°å½±å“ã€‚</li>
<li>BSCHåŸºäºå¤šæ ‡ç­¾è¯­ä¹‰é‡å åŠ¨æ€ç”Ÿæˆè½¯å¯¹æ¯”æ ·æœ¬å¯¹ï¼Œå®ç°è‡ªé€‚åº”å¯¹æ¯”å­¦ä¹ ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.07780">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-d2a14de4cdc007bbadfb3daaf56895f9" align="middle">
<img src="https://picx.zhimg.com/v2-717dfb8afff73cb3efa077af0c06f8cf" align="middle">
<img src="https://picx.zhimg.com/v2-1f4b357d85b5b7281e902fbbdbd72469" align="middle">
<img src="https://picx.zhimg.com/v2-118cd637151dcbd3e4a2b4d2089c9ac0" align="middle">
<img src="https://picx.zhimg.com/v2-ac018f4a5fc097a0bf21e38711383843" align="middle">
</details>


<h1 id="-3"><a href="#-3" class="headerlink" title=""></a></h1><h2 id="Adapted-Foundation-Models-for-Breast-MRI-Triaging-in-Contrast-Enhanced-and-Non-Contrast-Enhanced-Protocols"><a href="#Adapted-Foundation-Models-for-Breast-MRI-Triaging-in-Contrast-Enhanced-and-Non-Contrast-Enhanced-Protocols" class="headerlink" title="Adapted Foundation Models for Breast MRI Triaging in Contrast-Enhanced and Non-Contrast Enhanced Protocols"></a>Adapted Foundation Models for Breast MRI Triaging in Contrast-Enhanced and Non-Contrast Enhanced Protocols</h2><p><strong>Authors:Tri-Thien Nguyen, Lorenz A. Kapsner, Tobias Hepp, Shirin Heidarikahkesh, Hannes Schreiter, Luise Brock, Dominika Skwierawska, Dominique Hadler, Julian Hossbach, Evelyn Wenkel, Sabine Ohlmeyer, Frederik B. Laun, Andrzej Liebert, Andreas Maier, Michael Uder, Sebastian Bickelhaupt</strong></p>
<p>Background: Magnetic resonance imaging (MRI) has high sensitivity for breast cancer detection, but interpretation is time-consuming. Artificial intelligence may aid in pre-screening. Purpose: To evaluate the DINOv2-based Medical Slice Transformer (MST) for ruling out significant findings (Breast Imaging Reporting and Data System [BI-RADS] &gt;&#x3D;4) in contrast-enhanced and non-contrast-enhanced abbreviated breast MRI. Materials and Methods: This institutional review board approved retrospective study included 1,847 single-breast MRI examinations (377 BI-RADS &gt;&#x3D;4) from an in-house dataset and 924 from an external validation dataset (Duke). Four abbreviated protocols were tested: T1-weighted early subtraction (T1sub), diffusion-weighted imaging with b&#x3D;1500 s&#x2F;mm2 (DWI1500), DWI1500+T2-weighted (T2w), and T1sub+T2w. Performance was assessed at 90%, 95%, and 97.5% sensitivity using five-fold cross-validation and area under the receiver operating characteristic curve (AUC) analysis. AUC differences were compared with the DeLong test. False negatives were characterized, and attention maps of true positives were rated in the external dataset. Results: A total of 1,448 female patients (mean age, 49 +&#x2F;- 12 years) were included. T1sub+T2w achieved an AUC of 0.77 +&#x2F;- 0.04; DWI1500+T2w, 0.74 +&#x2F;- 0.04 (p&#x3D;0.15). At 97.5% sensitivity, T1sub+T2w had the highest specificity (19% +&#x2F;- 7%), followed by DWI1500+T2w (17% +&#x2F;- 11%). Missed lesions had a mean diameter &lt;10 mm at 95% and 97.5% thresholds for both T1sub and DWI1500, predominantly non-mass enhancements. External validation yielded an AUC of 0.77, with 88% of attention maps rated good or moderate. Conclusion: At 97.5% sensitivity, the MST framework correctly triaged cases without BI-RADS &gt;&#x3D;4, achieving 19% specificity for contrast-enhanced and 17% for non-contrast-enhanced MRI. Further research is warranted before clinical implementation.</p>
<blockquote>
<p>ç»“æœï¼š<br><strong>ç¿»è¯‘</strong>ï¼š</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05967v1">PDF</a> 23 pages, 6 figures, 4 tables. Originally submitted to Radiology (RAD-25-2541); under consideration for transfer to Radiology: Artificial Intelligence (RSNA Portfolio Journal)</p>
<p><strong>Summary</strong>ï¼š<br>è¯¥ç ”ç©¶é‡‡ç”¨DINOv2åŸºç¡€çš„åŒ»ç–—åˆ‡ç‰‡è½¬æ¢å™¨ï¼ˆMSTï¼‰å¯¹ä¹³è…ºç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰è¿›è¡Œç®€åŒ–åˆ†æï¼Œä»¥æ’é™¤é‡è¦å‘ç°ï¼ˆBI-RADSâ‰¥4ï¼‰ã€‚ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å¯¹æ¯”å¢å¼ºå’Œéå¯¹æ¯”å¢å¼ºç®€ç•¥ä¹³è…ºMRIä¸­ï¼ŒMSTæ¡†æ¶åœ¨ä¿æŒé«˜çµæ•åº¦ï¼ˆ97.5%ï¼‰çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ­£ç¡®ç­›é€‰ç—…ä¾‹ï¼Œè¾¾åˆ°ä¸€å®šçš„ç‰¹å¼‚æ€§ï¼ˆå¯¹äºå¯¹æ¯”å¢å¼ºMRIä¸º19%ï¼Œå¯¹äºéå¯¹æ¯”å¢å¼ºMRIä¸º17%ï¼‰ã€‚è¯¥ç ”ç©¶ä¸ºäººå·¥æ™ºèƒ½åœ¨ä¹³è…ºç–¾ç—…è¯Šæ–­æ–¹é¢çš„åº”ç”¨æä¾›äº†æ–°ä¾æ®ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ol>
<li>ç ”ç©¶é‡‡ç”¨MSTæ¡†æ¶å¯¹ä¹³è…ºMRIè¿›è¡Œåˆ†æï¼Œæ—¨åœ¨è¯„ä¼°å…¶åœ¨æ’é™¤é‡è¦å‘ç°ï¼ˆBI-RADSâ‰¥4ï¼‰æ–¹é¢çš„æ•ˆèƒ½ã€‚</li>
<li>é‡‡ç”¨äº†å››ç§ç®€ç•¥åè®®è¿›è¡Œæµ‹è¯•ï¼ŒåŒ…æ‹¬T1åŠ æƒæ—©æœŸå‡æ³•ï¼ˆT1subï¼‰ã€æ‰©æ•£åŠ æƒæˆåƒï¼ˆDWI1500ï¼‰ã€T2åŠ æƒç­‰ã€‚</li>
<li>åœ¨é«˜çµæ•åº¦ï¼ˆ97.5%ï¼‰ä¸‹ï¼ŒMSTæ¡†æ¶å…·æœ‰è‰¯å¥½çš„ç‰¹å¼‚æ€§ï¼Œå¯¹äºå¯¹æ¯”å¢å¼ºå’Œéå¯¹æ¯”å¢å¼ºMRIåˆ†åˆ«ä¸º19%å’Œ17%ã€‚</li>
<li>é—æ¼çš„ç—…å˜å¹³å‡ç›´å¾„å°äº10æ¯«ç±³ï¼Œä¸»è¦æ˜¯éè´¨é‡å¢å¼ºç—…å˜ã€‚</li>
<li>å¤–éƒ¨éªŒè¯ç»“æœæ˜¾ç¤ºï¼ŒMSTæ¡†æ¶çš„AUCä¸º0.77ï¼Œå…¶ä¸­88%çš„æ³¨æ„åŠ›å›¾è¢«è¯„ä¸ºè‰¯å¥½æˆ–ä¸­ç­‰ã€‚</li>
<li>ç ”ç©¶è¡¨æ˜ï¼ŒMSTæ¡†æ¶åœ¨ä¹³è…ºç–¾ç—…è¯Šæ–­æ–¹é¢å…·æœ‰ä¸€å®šçš„åº”ç”¨ä»·å€¼ï¼Œä½†éœ€è¦åœ¨ä¸´åºŠå®æ–½å‰è¿›è¡Œè¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05967">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-f1c69d33c143ff6138ae845d5f5e9e4b" align="middle">
<img src="https://picx.zhimg.com/v2-d1d39fa967401c7cc78454c303bcb0dd" align="middle">
</details>


<h1 id="-4"><a href="#-4" class="headerlink" title=""></a></h1><h2 id="C3-Diff-Super-resolving-Spatial-Transcriptomics-via-Cross-modal-Cross-content-Contrastive-Diffusion-Modelling"><a href="#C3-Diff-Super-resolving-Spatial-Transcriptomics-via-Cross-modal-Cross-content-Contrastive-Diffusion-Modelling" class="headerlink" title="C3-Diff: Super-resolving Spatial Transcriptomics via Cross-modal Cross-content Contrastive Diffusion Modelling"></a>C3-Diff: Super-resolving Spatial Transcriptomics via Cross-modal Cross-content Contrastive Diffusion Modelling</h2><p><strong>Authors:Xiaofei Wang, Stephen Price, Chao Li</strong></p>
<p>The rapid advancement of spatial transcriptomics (ST), i.e., spatial gene expressions, has made it possible to measure gene expression within original tissue, enabling us to discover molecular mechanisms. However, current ST platforms frequently suffer from low resolution, limiting the in-depth understanding of spatial gene expression. Super-resolution approaches promise to enhance ST maps by integrating histology images with gene expressions of profiled tissue spots. However, it remains a challenge to model the interactions between histology images and gene expressions for effective ST enhancement. This study presents a cross-modal cross-content contrastive diffusion framework, called C3-Diff, for ST enhancement with histology images as guidance. In C3-Diff, we firstly analyze the deficiency of traditional contrastive learning paradigm, which is then refined to extract both modal-invariant and content-invariant features of ST maps and histology images. Further, to overcome the problem of low sequencing sensitivity in ST maps, we perform nosing-based information augmentation on the surface of feature unit hypersphere. Finally, we propose a dynamic cross-modal imputation-based training strategy to mitigate ST data scarcity. We tested C3-Diff by benchmarking its performance on four public datasets, where it achieves significant improvements over competing methods. Moreover, we evaluate C3-Diff on downstream tasks of cell type localization, gene expression correlation and single-cell-level gene expression prediction, promoting AI-enhanced biotechnology for biomedical research and clinical applications. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/XiaofeiWang2018/C3-Diff">https://github.com/XiaofeiWang2018/C3-Diff</a>.</p>
<blockquote>
<p>ç©ºé—´è½¬å½•å­¦ï¼ˆSTï¼‰å³ç©ºé—´åŸºå› è¡¨è¾¾çš„å¿«é€Ÿå‘å±•ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨åŸå§‹ç»„ç»‡ä¸­æµ‹é‡åŸºå› è¡¨è¾¾ï¼Œä»è€Œå‘ç°åˆ†å­æœºåˆ¶ã€‚ç„¶è€Œï¼Œå½“å‰çš„STå¹³å°ç»å¸¸é¢ä¸´åˆ†è¾¨ç‡ä½çš„é—®é¢˜ï¼Œé™åˆ¶äº†ç©ºé—´åŸºå› è¡¨è¾¾çš„æ·±å…¥ç†è§£ã€‚è¶…åˆ†è¾¨ç‡æ–¹æ³•é€šè¿‡å°†ç»„ç»‡æ–‘ç‚¹çš„åŸºå› è¡¨è¾¾ä¸ç»„ç»‡å­¦å›¾åƒç›¸ç»“åˆï¼Œæœ‰æœ›å¢å¼ºSTå›¾ã€‚ç„¶è€Œï¼Œå¯¹ç»„ç»‡å­¦å›¾åƒå’ŒåŸºå› è¡¨è¾¾ä¹‹é—´çš„ç›¸äº’ä½œç”¨è¿›è¡Œå»ºæ¨¡ï¼Œä»¥æœ‰æ•ˆåœ°å¢å¼ºSTä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºSTå¢å¼ºçš„è·¨æ¨¡æ€è·¨å†…å®¹å¯¹æ¯”æ‰©æ•£æ¡†æ¶ï¼Œç§°ä¸ºC3-Diffï¼Œä»¥ç»„ç»‡å­¦å›¾åƒä¸ºæŒ‡å¯¼ã€‚åœ¨C3-Diffä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹ä¼ ç»Ÿçš„å¯¹æ¯”å­¦ä¹ èŒƒå¼çš„ç¼ºç‚¹è¿›è¡Œåˆ†æï¼Œç„¶åå¯¹å…¶è¿›è¡Œæ”¹è¿›ï¼Œä»¥æå–STå›¾å’Œç»„ç»‡å­¦å›¾åƒçš„æ¨¡æ€ä¸å˜å’Œå†…å®¹ä¸å˜ç‰¹å¾ã€‚æ­¤å¤–ï¼Œä¸ºäº†å…‹æœSTå›¾ä¸­ä½æµ‹åºçµæ•åº¦çš„é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨ç‰¹å¾å•ä½è¶…çƒä½“è¡¨é¢æ‰§è¡ŒåŸºäºå™ªå£°çš„ä¿¡æ¯å¢å¼ºã€‚æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåŠ¨æ€è·¨æ¨¡æ€æ’å€¼çš„è®­ç»ƒç­–ç•¥ï¼Œä»¥ç¼“è§£STæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡å››ä¸ªå…¬å…±æ•°æ®é›†å¯¹C3-Diffçš„æ€§èƒ½è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå®ƒåœ¨ç«äº‰æ–¹æ³•ä¸­å–å¾—äº†æ˜¾è‘—æ”¹è¿›ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ç»†èƒç±»å‹å®šä½ã€åŸºå› è¡¨è¾¾å…³è”å’Œå•ç»†èƒæ°´å¹³åŸºå› è¡¨è¾¾é¢„æµ‹ç­‰ä¸‹æ¸¸ä»»åŠ¡ä¸­è¯„ä¼°äº†C3-Diffçš„è¡¨ç°ï¼Œä¿ƒè¿›äº†äººå·¥æ™ºèƒ½å¢å¼ºç”Ÿç‰©æŠ€æœ¯å’Œç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸ä¸´åºŠåº”ç”¨çš„ç”Ÿç‰©æŠ€æœ¯ã€‚ç›¸å…³ä»£ç å¯åœ¨<a target="_blank" rel="noopener" href="https://github.com/XiaofeiWang2018/C3-Diff%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/XiaofeiWang2018/C3-Diffä¸Šæ‰¾åˆ°ã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2511.05571v1">PDF</a> </p>
<p><strong>Summary</strong><br>     æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¯¹æ¯”æ‰©æ•£çš„è·¨æ¨¡æ€è·¨å†…å®¹å¯¹æ¯”å­¦ä¹ æ¡†æ¶C3-Diffï¼Œç”¨äºä»¥ç»„ç»‡å›¾åƒä¸ºæŒ‡å¯¼å¢å¼ºç©ºé—´è½¬å½•å­¦ï¼ˆSTï¼‰ã€‚é€šè¿‡è·¨æ¨¡æ€è·¨å†…å®¹å¯¹æ¯”å­¦ä¹ ï¼Œç»“åˆè¶…åˆ†è¾¨ç‡æŠ€æœ¯ï¼Œè§£å†³STåœ°å›¾åˆ†è¾¨ç‡ä½çš„é—®é¢˜ã€‚åŒæ—¶ï¼Œè¯¥ç ”ç©¶è¿˜é€šè¿‡ä¿¡æ¯å¢å¼ºå’ŒåŠ¨æ€è·¨æ¨¡æ€è®­ç»ƒç­–ç•¥è§£å†³äº†STæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚åœ¨å››ä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒC3-Diffç›¸è¾ƒäºå…¶ä»–æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>ç©ºé—´è½¬å½•å­¦ï¼ˆSTï¼‰å¯æµ‹é‡åŸå§‹ç»„ç»‡å†…çš„åŸºå› è¡¨è¾¾ï¼Œæœ‰åŠ©äºå‘ç°åˆ†å­æœºåˆ¶ã€‚</li>
<li>å½“å‰STå¹³å°åˆ†è¾¨ç‡è¾ƒä½ï¼Œé™åˆ¶äº†ç©ºé—´åŸºå› è¡¨è¾¾çš„ç†è§£ã€‚</li>
<li>C3-Diffæ¡†æ¶æ—¨åœ¨é€šè¿‡ç»“åˆç»„ç»‡å›¾åƒä¸åŸºå› è¡¨è¾¾æ•°æ®ï¼Œæé«˜STåœ°å›¾çš„åˆ†è¾¨ç‡ã€‚</li>
<li>C3-Diffåˆ†æä¼ ç»Ÿå¯¹æ¯”å­¦ä¹ èŒƒå¼çš„ä¸è¶³ï¼Œå¹¶æå–æ¨¡æ€å’Œå†…å®¹ä¸å˜çš„ç‰¹å¾ã€‚</li>
<li>é€šè¿‡åŸºäºå™ªå£°çš„ä¿¡æ¯å¢å¼ºæŠ€æœ¯è§£å†³STåœ°å›¾ä½æµ‹åºçµæ•åº¦é—®é¢˜ã€‚</li>
<li>C3-Diffé‡‡ç”¨åŠ¨æ€è·¨æ¨¡æ€è®­ç»ƒç­–ç•¥ï¼Œä»¥ç¼“è§£STæ•°æ®ç¨€ç¼ºçš„é—®é¢˜ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.05571">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-fab8f79d5d10ce50261b1c2e3e5c4bbb" align="middle">
<img src="https://picx.zhimg.com/v2-03e2afdf544c37cd7d3d07f8694fd18a" align="middle">
<img src="https://picx.zhimg.com/v2-34897f309d6154e0f915d456b69d39df" align="middle">
<img src="https://picx.zhimg.com/v2-d80762b9b960d4eb04eaee09cba973fe" align="middle">
</details>


<h1 id="-5"><a href="#-5" class="headerlink" title=""></a></h1><h2 id="Graph-Contrastive-Learning-for-Connectome-Classification"><a href="#Graph-Contrastive-Learning-for-Connectome-Classification" class="headerlink" title="Graph Contrastive Learning for Connectome Classification"></a>Graph Contrastive Learning for Connectome Classification</h2><p><strong>Authors:MartÃ­n Schmidt, Sara Silva, Federico Larroca, Gonzalo Mateos, Pablo MusÃ©</strong></p>
<p>With recent advancements in non-invasive techniques for measuring brain activity, such as magnetic resonance imaging (MRI), the study of structural and functional brain networks through graph signal processing (GSP) has gained notable prominence. GSP stands as a key tool in unraveling the interplay between the brainâ€™s function and structure, enabling the analysis of graphs defined by the connections between regions of interest â€“ referred to as connectomes in this context. Our work represents a further step in this direction by exploring supervised contrastive learning methods within the realm of graph representation learning. The main objective of this approach is to generate subject-level (i.e., graph-level) vector representations that bring together subjects sharing the same label while separating those with different labels. These connectome embeddings are derived from a graph neural network Encoder-Decoder architecture, which jointly considers structural and functional connectivity. By leveraging data augmentation techniques, the proposed framework achieves state-of-the-art performance in a gender classification task using Human Connectome Project data. More broadly, our connectome-centric methodological advances support the promising prospect of using GSP to discover more about brain function, with potential impact to understanding heterogeneity in the neurodegeneration for precision medicine and diagnosis.</p>
<blockquote>
<p>éšç€æ— åˆ›æµ‹é‡è„‘æ´»åŠ¨æŠ€æœ¯çš„æœ€æ–°è¿›å±•ï¼Œå¦‚ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ï¼Œé€šè¿‡å›¾ä¿¡å·å¤„ç†ï¼ˆGSPï¼‰ç ”ç©¶ç»“æ„å’ŒåŠŸèƒ½è„‘ç½‘ç»œå·²ç»å˜å¾—å°¤ä¸ºé‡è¦ã€‚GSPæˆä¸ºæ­ç¤ºå¤§è„‘åŠŸèƒ½ä¸ä½œç”¨ä¹‹é—´ç›¸äº’ä½œç”¨çš„å…³é”®å·¥å…·ï¼Œèƒ½å¤Ÿåˆ†æç”±æ„Ÿå…´è¶£åŒºåŸŸä¹‹é—´çš„è¿æ¥æ‰€å®šä¹‰çš„å›¾çš„åˆ†æï¼Œè¿™åœ¨æœ¬æ–‡ä¸­è¢«ç§°ä¸ºè¿æ¥ç»„ã€‚æˆ‘ä»¬çš„å·¥ä½œæœç€è¿™ä¸ªæ–¹å‘è¿ˆå‡ºäº†ä¸€æ­¥ï¼Œæ¢ç´¢äº†å›¾è¡¨ç¤ºå­¦ä¹ é¢†åŸŸå†…çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•çš„ä¸»è¦ç›®æ ‡æ˜¯ç”Ÿæˆä¸»ä½“çº§åˆ«çš„ï¼ˆå³å›¾çº§åˆ«çš„ï¼‰å‘é‡è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºå°†å…±äº«ç›¸åŒæ ‡ç­¾çš„ä¸»ä½“èšé›†åœ¨ä¸€èµ·ï¼ŒåŒæ—¶å°†å…·æœ‰ä¸åŒæ ‡ç­¾çš„ä¸»ä½“åˆ†å¼€ã€‚è¿™äº›è¿æ¥ä½“åµŒå…¥æ¥æºäºå›¾ç¥ç»ç½‘ç»œç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼ŒåŒæ—¶è€ƒè™‘ç»“æ„å’ŒåŠŸèƒ½è¿æ¥ã€‚é€šè¿‡åˆ©ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œæ‰€æå‡ºçš„æ¡†æ¶åœ¨åˆ©ç”¨äººç±»è¿æ¥ç»„é¡¹ç›®æ•°æ®çš„æ€§åˆ«åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚æ›´å¹¿æ³›åœ°è¯´ï¼Œæˆ‘ä»¬çš„ä»¥è¿æ¥ç»„ä¸ºä¸­å¿ƒçš„æ–¹æ³•è®ºè¿›æ­¥æ”¯æŒäº†ä½¿ç”¨GSPæ¥å‘ç°æ›´å¤šå…³äºå¤§è„‘åŠŸèƒ½çš„å¸Œæœ›ï¼Œå¯¹ç²¾ç¡®åŒ»å­¦å’Œè¯Šæ–­ä¸­ç†è§£ç¥ç»å˜å¼‚çš„å¼‚è´¨æ€§å…·æœ‰æ½œåœ¨å½±å“ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2502.05109v2">PDF</a> Presented at Asilomar Conference on Signals, Systems, and Computers 2025</p>
<p><strong>Summary</strong></p>
<p>è¿‘æœŸéä¾µå…¥æ€§æŠ€æœ¯ï¼ˆå¦‚ç£å…±æŒ¯æˆåƒï¼‰çš„è¿›å±•æ¨åŠ¨äº†é€šè¿‡å›¾ä¿¡å·å¤„ç†ï¼ˆGSPï¼‰ç ”ç©¶è„‘ç½‘ç»œå’ŒåŠŸèƒ½çš„é‡è¦æ€§ã€‚æœ¬ç ”ç©¶æ¢ç´¢äº†å›¾è¡¨ç¤ºå­¦ä¹ ä¸­çš„ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨ç”Ÿæˆä¸»ä½“çº§åˆ«çš„å‘é‡è¡¨ç¤ºï¼Œä½¿ç›¸åŒæ ‡ç­¾çš„ä¸»ä½“èšé›†åœ¨ä¸€èµ·ï¼Œä¸åŒæ ‡ç­¾çš„ä¸»ä½“åˆ†å¼€ã€‚åˆ©ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œæ‰€ææ¡†æ¶åœ¨äººç±»è¿æ¥ç»„é¡¹ç›®æ•°æ®ä¸­å®ç°äº†æ€§åˆ«åˆ†ç±»ä»»åŠ¡çš„å“è¶Šæ€§èƒ½ã€‚ç ”ç©¶ä¸ºåˆ©ç”¨GSPæ·±å…¥äº†è§£è„‘åŠŸèƒ½æä¾›äº†å‰æ™¯ï¼Œå¯¹ç²¾ç¡®åŒ»å­¦å’Œè¯Šæ–­çš„ç¥ç»é€€è¡Œæ€§ç–¾ç—…çš„å¼‚è´¨æ€§å…·æœ‰æ½œåœ¨å½±å“ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ul>
<li>å…ˆè¿›çš„éä¾µå…¥æ€§æŠ€æœ¯å¦‚ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ä¿ƒè¿›äº†è„‘ç½‘ç»œå’ŒåŠŸèƒ½çš„ç ”ç©¶ã€‚</li>
<li>å›¾ä¿¡å·å¤„ç†ï¼ˆGSPï¼‰åœ¨è§£æè„‘åŠŸèƒ½ä¸ä½œç”¨ä¸­èµ·åˆ°å…³é”®ä½œç”¨ã€‚</li>
<li>ç ”ç©¶é‡‡ç”¨ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡å›¾è¡¨ç¤ºå­¦ä¹ ç”Ÿæˆä¸»ä½“çº§åˆ«çš„å‘é‡è¡¨ç¤ºã€‚</li>
<li>è¯¥æ–¹æ³•åˆ©ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œå®ç°äº†æ€§åˆ«åˆ†ç±»ä»»åŠ¡çš„å“è¶Šæ€§èƒ½ã€‚</li>
<li>ç ”ç©¶ä¸ºåˆ©ç”¨GSPæ·±å…¥äº†è§£è„‘åŠŸèƒ½æä¾›äº†å‰æ™¯ã€‚</li>
<li>è¯¥ç ”ç©¶å¯¹ç²¾ç¡®åŒ»å­¦å’Œè¯Šæ–­çš„ç¥ç»é€€è¡Œæ€§ç–¾ç—…çš„å¼‚è´¨æ€§å…·æœ‰æ½œåœ¨å½±å“ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2502.05109">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-1210993120948908f0f7f34b218b867b" align="middle">
<img src="https://picx.zhimg.com/v2-154c3e10b64c48e87d2510d12411d69c" align="middle">
<img src="https://picx.zhimg.com/v2-a2b9d4e774c5e845b7f59beb77c07ef3" align="middle">
<img src="https://picx.zhimg.com/v2-dbd3f49588803faec06354cf5fba00ad" align="middle">
<img src="https://picx.zhimg.com/v2-43a9506196efb35d4d9f9deceeb78774" align="middle">
</details>


<h1 id="-6"><a href="#-6" class="headerlink" title=""></a></h1><h2 id="CSPCL-Category-Semantic-Prior-Contrastive-Learning-for-Deformable-DETR-Based-Prohibited-Item-Detectors"><a href="#CSPCL-Category-Semantic-Prior-Contrastive-Learning-for-Deformable-DETR-Based-Prohibited-Item-Detectors" class="headerlink" title="CSPCL: Category Semantic Prior Contrastive Learning for Deformable DETR-Based Prohibited Item Detectors"></a>CSPCL: Category Semantic Prior Contrastive Learning for Deformable DETR-Based Prohibited Item Detectors</h2><p><strong>Authors:Mingyuan Li, Tong Jia, Hao Wang, Bowen Ma, Hui Lu, Shiyi Guo, Da Cai, Dongyue Chen</strong></p>
<p>Prohibited item detection based on X-ray images is one of the most effective security inspection methods. However, the foreground-background feature coupling caused by the overlapping phenomenon specific to X-ray images makes general detectors designed for natural images perform poorly. To address this issue, we propose a Category Semantic Prior Contrastive Learning (CSPCL) mechanism, which aligns the class prototypes perceived by the classifier with the content queries to correct and supplement the missing semantic information responsible for classification, thereby enhancing the model sensitivity to foreground features. To achieve this alignment, we design a specific contrastive loss, CSP loss, which comprises the Intra-Class Truncated Attraction (ITA) loss and the Inter-Class Adaptive Repulsion (IAR) loss, and outperforms classic contrastive losses. Specifically, the ITA loss leverages class prototypes to attract intra-class content queries and preserves essential intra-class diversity via a gradient truncation function. The IAR loss employs class prototypes to adaptively repel inter-class content queries, with the repulsion strength scaled by prototype-prototype similarity, thereby improving inter-class discriminability, especially among similar categories. CSPCL is general and can be easily integrated into Deformable DETR-based models. Extensive experiments on the PIXray, OPIXray, PIDray, and CLCXray datasets demonstrate that CSPCL significantly enhances the performance of various state-of-the-art models without increasing inference complexity. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Limingyuan001/CSPCL">https://github.com/Limingyuan001/CSPCL</a>.</p>
<blockquote>
<p>åŸºäºXå°„çº¿å›¾åƒçš„è¿ç¦å“æ£€æµ‹æ˜¯æœ€æœ‰æ•ˆçš„å®‰å…¨æ£€æŸ¥æ–¹æ³•ä¹‹ä¸€ã€‚ç„¶è€Œï¼ŒXå°„çº¿å›¾åƒç‰¹æœ‰çš„é‡å ç°è±¡å¯¼è‡´çš„å‰æ™¯-èƒŒæ™¯ç‰¹å¾è€¦åˆï¼Œä½¿å¾—é’ˆå¯¹è‡ªç„¶å›¾åƒè®¾è®¡çš„é€šç”¨æ£€æµ‹å™¨è¡¨ç°ä¸ä½³ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºCSPCLï¼ˆç±»åˆ«è¯­ä¹‰å…ˆéªŒå¯¹æ¯”å­¦ä¹ ï¼‰çš„æœºåˆ¶ï¼Œå®ƒå°†åˆ†ç±»å™¨æ„ŸçŸ¥åˆ°çš„ç±»åˆ«åŸå‹ä¸å†…å®¹æŸ¥è¯¢å¯¹é½ï¼Œä»¥çº æ­£å’Œè¡¥å……è´Ÿè´£åˆ†ç±»çš„ç¼ºå¤±è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæé«˜æ¨¡å‹å¯¹å‰æ™¯ç‰¹å¾çš„æ•æ„Ÿæ€§ã€‚ä¸ºäº†å®ç°è¿™ç§å¯¹é½ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç‰¹å®šçš„å¯¹æ¯”æŸå¤±ï¼Œå³CSPæŸå¤±ï¼Œå®ƒåŒ…æ‹¬ç±»å†…æˆªæ–­å¸å¼•æŸå¤±ï¼ˆITAæŸå¤±ï¼‰å’Œç±»é—´è‡ªé€‚åº”æ’æ–¥æŸå¤±ï¼ˆIARæŸå¤±ï¼‰ï¼Œå¹¶ä¼˜äºç»å…¸å¯¹æ¯”æŸå¤±ã€‚å…·ä½“è€Œè¨€ï¼ŒITAæŸå¤±åˆ©ç”¨ç±»åˆ«åŸå‹å¸å¼•ç±»å†…å†…å®¹æŸ¥è¯¢ï¼Œå¹¶é€šè¿‡æ¢¯åº¦æˆªæ–­å‡½æ•°ä¿æŒç±»å†…å¤šæ ·æ€§ã€‚IARæŸå¤±åˆ©ç”¨ç±»åˆ«åŸå‹è‡ªé€‚åº”åœ°æ’æ–¥ç±»é—´å†…å®¹æŸ¥è¯¢ï¼Œæ’æ–¥å¼ºåº¦éšåŸå‹-åŸå‹ç›¸ä¼¼æ€§è€Œç¼©æ”¾ï¼Œä»è€Œæé«˜ç±»é—´å¯åˆ†è¾¨æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›¸ä¼¼ç±»åˆ«ä¹‹é—´ã€‚CSPCLæ˜¯é€šç”¨çš„ï¼Œå¯ä»¥è½»æ¾åœ°é›†æˆåˆ°åŸºäºå¯å˜å½¢DETRçš„æ¨¡å‹ä¸­ã€‚åœ¨PIXrayã€OPIXrayã€PIDrayå’ŒCLCXrayæ•°æ®é›†ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼ŒCSPCLå¯ä»¥æ˜¾è‘—æé«˜å„ç§æœ€æ–°æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œä¸ä¼šå¢åŠ æ¨ç†å¤æ‚åº¦ã€‚ä»£ç å…¬å¼€å¯ç”¨åœ¨<a target="_blank" rel="noopener" href="https://github.com/Limingyuan001/CSPCL%E3%80%82">https://github.com/Limingyuan001/CSPCLã€‚</a></p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2501.16665v2">PDF</a> 22 pages, 5 figures</p>
<p><strong>Summary</strong>ï¼šé’ˆå¯¹Xå…‰å›¾åƒä¸­çš„ç¦æ­¢ç‰©å“æ£€æµ‹é—®é¢˜ï¼Œç”±äºå›¾åƒä¸­ç‰©å“é‡å å¯¼è‡´çš„ç‰¹å¾è€¦åˆç°è±¡ä½¿å¾—é€šç”¨æ£€æµ‹å™¨è¡¨ç°ä¸ä½³ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºç±»åˆ«è¯­ä¹‰å…ˆéªŒå¯¹æ¯”å­¦ä¹ ï¼ˆCSPCLï¼‰çš„æœºåˆ¶ï¼Œé€šè¿‡ä½¿åˆ†ç±»å™¨æ„ŸçŸ¥çš„ç±»åˆ«åŸå‹ä¸å†…å®¹æŸ¥è¯¢å¯¹é½ï¼Œè¡¥å……ç¼ºå¤±çš„è¯­ä¹‰ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹å¯¹å‰æ™¯ç‰¹å¾çš„æ•æ„Ÿæ€§ã€‚è®¾è®¡äº†ç‰¹å®šçš„å¯¹æ¯”æŸå¤±CSPæŸå¤±ï¼ŒåŒ…æ‹¬æˆªæ–­å¸å¼•æŸå¤±å’Œè‡ªé€‚åº”æ’æ–¥æŸå¤±ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯äº†å…¶æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ã€‚ä»£ç å·²å…¬å¼€ã€‚</p>
<p><strong>Key Takeaways</strong>ï¼š</p>
<ul>
<li>Xå…‰å›¾åƒä¸­çš„ç¦æ­¢ç‰©å“æ£€æµ‹æ˜¯æœ‰æ•ˆçš„å®‰å…¨æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>ç”±äºç‰©å“é‡å å¯¼è‡´çš„ç‰¹å¾è€¦åˆç°è±¡ä½¿å¾—é€šç”¨æ£€æµ‹å™¨åœ¨Xå…‰å›¾åƒä¸Šè¡¨ç°ä¸ä½³ã€‚</li>
<li>æå‡ºäº†åŸºäºç±»åˆ«è¯­ä¹‰å…ˆéªŒå¯¹æ¯”å­¦ä¹ ï¼ˆCSPCLï¼‰çš„æœºåˆ¶ï¼Œå¢å¼ºæ¨¡å‹å¯¹å‰æ™¯ç‰¹å¾çš„æ•æ„Ÿæ€§ã€‚</li>
<li>è®¾è®¡äº†CSPæŸå¤±ï¼ŒåŒ…æ‹¬æˆªæ–­å¸å¼•æŸå¤±å’Œè‡ªé€‚åº”æ’æ–¥æŸå¤±ã€‚</li>
<li>CSPCLæœºåˆ¶å¯ä»¥è½»æ˜“é›†æˆåˆ°åŸºäºå¯å˜å½¢DETRçš„æ¨¡å‹ä¸­ã€‚</li>
<li>åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒCSPCLæ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ€§èƒ½ä¸”ä¸ä¼šå¢åŠ æ¨ç†å¤æ‚åº¦ã€‚</li>
</ul>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2501.16665">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-458897963d34dfd783fa0c1bb50e8709" align="middle">
<img src="https://picx.zhimg.com/v2-5369553deed4a085faf556fc814a2ff1" align="middle">
<img src="https://picx.zhimg.com/v2-4691d4464cbd4344d7ac32906e511f69" align="middle">
<img src="https://picx.zhimg.com/v2-5454bdd6d2ca7c158f988ac5d59afe99" align="middle">
</details>


<h1 id="-7"><a href="#-7" class="headerlink" title=""></a></h1><h2 id="Enhancing-Multimodal-Medical-Image-Classification-using-Cross-Graph-Modal-Contrastive-Learning"><a href="#Enhancing-Multimodal-Medical-Image-Classification-using-Cross-Graph-Modal-Contrastive-Learning" class="headerlink" title="Enhancing Multimodal Medical Image Classification using Cross-Graph Modal Contrastive Learning"></a>Enhancing Multimodal Medical Image Classification using Cross-Graph Modal Contrastive Learning</h2><p><strong>Authors:Jun-En Ding, Chien-Chin Hsu, Chi-Hsiang Chu, Shuqiang Wang, Feng Liu</strong></p>
<p>The classification of medical images is a pivotal aspect of disease diagnosis, often enhanced by deep learning techniques. However, traditional approaches typically focus on unimodal medical image data, neglecting the integration of diverse non-image patient data. This paper proposes a novel Cross-Graph Modal Contrastive Learning (CGMCL) framework for multimodal structured data from different data domains to improve medical image classification. The model effectively integrates both image and non-image data by constructing cross-modality graphs and leveraging contrastive learning to align multimodal features in a shared latent space. An inter-modality feature scaling module further optimizes the representation learning process by reducing the gap between heterogeneous modalities. The proposed approach is evaluated on two datasets: a Parkinsonâ€™s disease (PD) dataset and a public melanoma dataset. Results demonstrate that CGMCL outperforms conventional unimodal methods in accuracy, interpretability, and early disease prediction. Additionally, the method shows superior performance in multi-class melanoma classification. The CGMCL framework provides valuable insights into medical image classification while offering improved disease interpretability and predictive capabilities.</p>
<blockquote>
<p>åŒ»ç–—å›¾åƒåˆ†ç±»æ˜¯ç–¾ç—…è¯Šæ–­çš„æ ¸å¿ƒç¯èŠ‚ï¼Œç»å¸¸å€ŸåŠ©æ·±åº¦å­¦ä¹ æŠ€æœ¯æ¥æå‡æ•ˆæœã€‚ç„¶è€Œï¼Œä¼ ç»Ÿæ–¹æ³•é€šå¸¸å…³æ³¨å•æ¨¡æ€åŒ»ç–—å›¾åƒæ•°æ®ï¼Œå¿½ç•¥äº†ä¸åŒéå›¾åƒæ‚£è€…æ•°æ®çš„æ•´åˆã€‚æœ¬æ–‡é’ˆå¯¹æ¥è‡ªä¸åŒæ•°æ®åŸŸçš„å¤šåª’ä½“ç»“æ„æ•°æ®ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„è·¨å›¾æ¨¡æ€å¯¹æ¯”å­¦ä¹ ï¼ˆCGMCLï¼‰æ¡†æ¶ï¼Œä»¥æé«˜åŒ»å­¦å›¾åƒåˆ†ç±»çš„æ•ˆæœã€‚è¯¥æ¨¡å‹é€šè¿‡æ„å»ºè·¨æ¨¡æ€å›¾å¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ æ¥å¯¹é½å¤šæ¨¡æ€ç‰¹å¾åœ¨å…±äº«æ½œåœ¨ç©ºé—´ï¼Œä»è€Œæœ‰æ•ˆåœ°æ•´åˆå›¾åƒå’Œéå›¾åƒæ•°æ®ã€‚è·¨æ¨¡æ€ç‰¹å¾ç¼©æ”¾æ¨¡å—è¿›ä¸€æ­¥ä¼˜åŒ–äº†è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œç¼©å°äº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚æ‰€æå‡ºçš„æ–¹æ³•åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼šå¸•é‡‘æ£®ç—…ï¼ˆPDï¼‰æ•°æ®é›†å’Œå…¬å…±é»‘è‰²ç´ ç˜¤æ•°æ®é›†ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œæ—©æœŸç–¾ç—…é¢„æµ‹æ–¹é¢ï¼ŒCGMCLä¼˜äºä¼ ç»Ÿçš„å•æ¨¡æ€æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç±»é»‘è‰²ç´ ç˜¤åˆ†ç±»æ–¹é¢è¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚CGMCLæ¡†æ¶ä¸ºåŒ»å­¦å›¾åƒåˆ†ç±»æä¾›äº†æœ‰ä»·å€¼çš„è§è§£ï¼ŒåŒæ—¶æé«˜äº†ç–¾ç—…å¯è§£é‡Šæ€§å’Œé¢„æµ‹èƒ½åŠ›ã€‚</p>
</blockquote>
<p><strong>è®ºæ–‡åŠé¡¹ç›®ç›¸å…³é“¾æ¥</strong></p>
<p><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2410.17494v5">PDF</a> </p>
<p><strong>Summary</strong></p>
<p>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„è·¨å›¾æ¨¡æ€å¯¹æ¯”å­¦ä¹ ï¼ˆCGMCLï¼‰æ¡†æ¶ï¼Œç”¨äºå¤šæ¨¡æ€ç»“æ„åŒ–æ•°æ®çš„åŒ»å­¦å›¾åƒåˆ†ç±»ã€‚è¯¥æ¡†æ¶é€šè¿‡æ„å»ºè·¨æ¨¡æ€å›¾å¹¶åˆ©ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œæœ‰æ•ˆæ•´åˆå›¾åƒå’Œéå›¾åƒæ•°æ®ï¼Œåœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­å¯¹é½å¤šæ¨¡æ€ç‰¹å¾ã€‚æ­¤å¤–ï¼Œå…¶è·¨æ¨¡æ€ç‰¹å¾ç¼©æ”¾æ¨¡å—è¿›ä¸€æ­¥ä¼˜åŒ–äº†è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ï¼Œå‡å°‘äº†ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®è·ã€‚åœ¨å¸•é‡‘æ£®ç—…å’Œå…¬å…±é»‘è‰²ç´ ç˜¤æ•°æ®é›†ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒCGMCLåœ¨å‡†ç¡®æ€§ã€å¯è§£é‡Šæ€§å’Œæ—©æœŸç–¾ç—…é¢„æµ‹æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„å•æ¨¡æ€æ–¹æ³•ï¼Œå¹¶åœ¨å¤šç±»é»‘è‰²ç´ ç˜¤åˆ†ç±»ä¸­è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚</p>
<p><strong>Key Takeaways</strong></p>
<ol>
<li>CGMCLæ¡†æ¶ç”¨äºåŒ»å­¦å›¾åƒåˆ†ç±»ï¼Œç»“åˆå¤šæ¨¡æ€ç»“æ„åŒ–æ•°æ®ã€‚</li>
<li>é€šè¿‡æ„å»ºè·¨æ¨¡æ€å›¾ï¼Œæ•´åˆå›¾åƒå’Œéå›¾åƒæ•°æ®ã€‚</li>
<li>åˆ©ç”¨å¯¹æ¯”å­¦ä¹ åœ¨å…±äº«æ½œåœ¨ç©ºé—´ä¸­å¯¹é½å¤šæ¨¡æ€ç‰¹å¾ã€‚</li>
<li>è·¨æ¨¡æ€ç‰¹å¾ç¼©æ”¾æ¨¡å—ä¼˜åŒ–è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>CGMCLåœ¨å¸•é‡‘æ£®ç—…å’Œé»‘è‰²ç´ ç˜¤æ•°æ®é›†ä¸Šçš„è¡¨ç°ä¼˜äºä¼ ç»Ÿæ–¹æ³•ã€‚</li>
<li>CGMCLæé«˜äº†åŒ»å­¦å›¾åƒåˆ†ç±»çš„å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚</li>
</ol>
<p><strong><a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2410.17494">Cool Papers</a></strong> </p>
<details>
  <summary>ç‚¹æ­¤æŸ¥çœ‹è®ºæ–‡æˆªå›¾</summary>
<img src="https://picx.zhimg.com/v2-410f5fe42e807d74fafbac6f284557c0" align="middle">
<img src="https://picx.zhimg.com/v2-6a4a01f6b1bc4d69eb6febff4be443a2" align="middle">
<img src="https://picx.zhimg.com/v2-51f97dc6237561e31b9d33d4e08edcda" align="middle">
<img src="https://picx.zhimg.com/v2-b2c8ae71671b8d771df3d1f6307ddf0f" align="middle">
<img src="https://picx.zhimg.com/v2-7c48da9178bb2a5012fe26cf21e1052d" align="middle">
</details>


<h1 id="-8"><a href="#-8" class="headerlink" title=""></a></h1>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/Talk2Paper/about" rel="external nofollow noreferrer">Kedreamix</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">https://kedreamix.github.io/Talk2Paper/Paper/2025-11-16/%E6%97%A0%E7%9B%91%E7%9D%A3_%E5%8D%8A%E7%9B%91%E7%9D%A3_%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/Talk2Paper/tags/%E6%97%A0%E7%9B%91%E7%9D%A3-%E5%8D%8A%E7%9B%91%E7%9D%A3-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/">
                                    <span class="chip bg-color">æ— ç›‘ç£/åŠç›‘ç£/å¯¹æ¯”å­¦ä¹ </span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/Talk2Paper/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/Talk2Paper/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F_Breast%20Ultrasound/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-a28b29948c60f73fa27cf18feefa0063" class="responsive-img" alt="åŒ»å­¦å½±åƒ/Breast Ultrasound">
                        
                        <span class="card-title">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            åŒ»å­¦å½±åƒ/Breast Ultrasound æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  A Dual-Mode ViT-Conditioned Diffusion Framework with an Adaptive Conditioning Bridge for Breast Cancer Segmentation
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/" class="post-category">
                                    åŒ»å­¦å½±åƒ/Breast Ultrasound
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F-Breast-Ultrasound/">
                        <span class="chip bg-color">åŒ»å­¦å½±åƒ/Breast Ultrasound</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/Talk2Paper/Paper/2025-11-16/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                    <div class="card-image">
                        
                        <img src="https://picx.zhimg.com/v2-1d66bf749c3489cdcfcc5046a33ee5ca" class="responsive-img" alt="äººè„¸ç›¸å…³">
                        
                        <span class="card-title">äººè„¸ç›¸å…³</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            äººè„¸ç›¸å…³ æ–¹å‘æœ€æ–°è®ºæ–‡å·²æ›´æ–°ï¼Œè¯·æŒç»­å…³æ³¨ Update in 2025-11-16  Hybrid second-order gradient histogram based global low-rank sparse regression for robust face recognition
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-11-16
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/Talk2Paper/categories/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/" class="post-category">
                                    äººè„¸ç›¸å…³
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/Talk2Paper/tags/%E4%BA%BA%E8%84%B8%E7%9B%B8%E5%85%B3/">
                        <span class="chip bg-color">äººè„¸ç›¸å…³</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeBlockFuction.js"></script>



<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/Talk2Paper/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/Talk2Paper/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/Talk2Paper/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/Talk2Paper/libs/aplayer/APlayer.min.js"></script>
<script src="/Talk2Paper/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2024-2025</span>
            
            <a href="/Talk2Paper/about" target="_blank">Kedreamix</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;<span
                        class="white-color">32883.8k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- è¿è¡Œå¤©æ•°æé†’. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2024";
                        var startMonth = "1";
                        var startDate = "1";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // åŒºåˆ†æ˜¯å¦æœ‰å¹´ä»½.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                daysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²è¿è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = 'æœ¬ç«™å·²é‹è¡Œ ' + diffYears + ' å¹´ ' + diffDays + ' å¤©';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Kedreamix" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:kedreamix@gmail.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>











    <a href="https://www.zhihu.com/people/kedreamix" class="tooltipped" target="_blank" data-tooltip="å…³æ³¨æˆ‘çš„çŸ¥ä¹: https://www.zhihu.com/people/kedreamix" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">çŸ¥</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/Talk2Paper/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- ç™½å¤©å’Œé»‘å¤œä¸»é¢˜ -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/Talk2Paper/libs/materialize/materialize.min.js"></script>
    <script src="/Talk2Paper/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/Talk2Paper/libs/aos/aos.js"></script>
    <script src="/Talk2Paper/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/Talk2Paper/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/Talk2Paper/js/matery.js"></script>

    

    
    
    
        
        <script type="text/javascript">
            // åªåœ¨æ¡Œé¢ç‰ˆç½‘é¡µå¯ç”¨ç‰¹æ•ˆ
            var windowWidth = $(window).width();
            if (windowWidth > 768) {
                document.write('<script type="text/javascript" src="/Talk2Paper/libs/others/sakura-reduce.js"><\/script>');
            }
        </script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

    <!-- é¼ æ ‡æ˜Ÿæ˜Ÿç‰¹æ•ˆ -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/Talk2Paper/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/Talk2Paper/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/Talk2Paper/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--è…¾è®¯å…”å°å·¢-->
    
    

    

    

    
    <script src="/Talk2Paper/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/Talk2Paper/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>

<!-- åŠ¨æ€æ ‡ç­¾ -->
<script type="text/javascript"> var OriginTitile = document.title, st; 
    document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£è¯¶ï¼Œé¡µé¢å´©æºƒäº†å˜›ï¼Ÿ", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) }) </script>
